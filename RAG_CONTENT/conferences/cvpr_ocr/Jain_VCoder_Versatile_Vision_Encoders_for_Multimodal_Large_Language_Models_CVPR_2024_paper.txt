VCoder: Versatile Vision Encoders for Multimodal Large Language Models
Jitesh Jain1Jianwei Yang2Humphrey Shi1,3
1SHI Labs @ Georgia Tech2Microsoft Research, Redmond3Picsart AI Research (PAIR)
https://github.com/SHI-Labs/VCoder
Abstract
Humans possess the remarkable skill of Visual Perception,
the ability to see and understand the seen, helping them
make sense of the visual world and, in turn, reason. Mul-
timodal Large Language Models (MLLM) have recently
achieved impressive performance on vision-language tasks
ranging from visual question-answering and image caption-
ing to visual reasoning and image generation. However,
when prompted to identify or count (perceive) the entities
in a given image, existing MLLM systems fail. Working
towards developing an accurate MLLM system for percep-
tion and reasoning, we propose using Versatile vision en-
Coders ( VCoder ) as perception eyes for Multimodal LLMs.
We feed the VCoder with perception modalities such as seg-
mentation or depth maps, improving the MLLM’s percep-
tion abilities. Secondly, we leverage the images from COCO
and outputs from off-the-shelf vision perception models to
create our COCO Segmentation Text ( COST ) dataset for
training and evaluating MLLMs on the object perception
task. Thirdly, we introduce metrics to assess the object per-
ception abilities in MLLMs on our COST dataset. Lastly,
we provide extensive experimental evidence proving the
VCoder’s improved object-level perception skills over exist-
ing Multimodal LLMs, including GPT-4V . We open-source
our dataset, code, and models to promote research.
1. Introduction
‘Perception is the soil; reasoning, the seed. With-
out fertile ground, the seed cannot flourish. ’
(GPT-4 [48], 2023)
The ability to think and reason is one of the most remark-
able traits that help humans function daily. Generally, un-
derstanding the environment precedes the act of thinking
and reasoning [28]. Following the success of ChatGPT-like
instruction following AI agents [3, 11, 47, 48, 60] at lan-
guage understanding and reasoning, researchers have lever-
aged LLMs to develop instruct frameworks [14, 40, 68, 74]
that can understand vision and language inputs in an ef-
fort to imitate human perception and reasoning ability. We
Difficulty Level
HIGH
Difficulty LevelUSER
What is happening in the image?
Characters from "Friends" are gathered in a living room, wearing various
Halloween costumes, possibly celebrating or attending a Halloween party .
Chandler stands out in a pink bunny costume.
USER
Can you count the number
of people in the image?
Yes, there are ten
people visible in the
image.
The image shows a group of people gathered in what appears to be a living room
or a dining area, dressed in various silly, quirky costumes . They are sitting at a
dining table with chairs, engaged in a social event or party .
Yes, there are a total of
14 people in the image.
GPT -4V
GPT -4V VCoderVCoderFigure 1. GPT-4V [48] (access date: Dec 16, 2023) returns impres-
sive responses when prompted to describe complex visual scenes.
However, it fails at the simple task of counting in the same scene.
Our VCoder returns the correct count of people.
refer to such systems as Multimodal LLMs (MLLM). Al-
though MLLMs exhibit the ability to perform complex
vision-language tasks like visual captioning [2, 4, 39], im-
age generation [26, 27, 57], visual reasoning and ground-
ing [22, 51, 72], they often display sub-par performance
at simple tasks like counting objects (Fig. 1). As shown
in Fig. 2, MLLMs output incorrect object counts ( people,
chairs ) and hallucinate about the presence ( handbag, couch )
of certain objects when prompted to identify entities in a
visual input. The perception performance is much worse
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27992
GPT -4V
The objects present in the image are:
eleven people , one large potted plant ,
two ceiling lamps , one exit sign , and
various pieces of furniture that appear
to be office desks and chairs.
The objects present in the image
are: 12 people , 1 potted plant , 1 TV, 2
chairs , 1 handbag .
LLaV A-1.5
VCoder
The objects present in the image are:
ceiling, floor, wall, window, cabinet,
door, light, table, rug, thirteen
people, potted plant , three chairs .
OneFormer  (Oracle)USER
What objects can be seen in the image?
GPT -4V
LLaV A-1.5
OneFormer  (Oracle)
The objects present in the image are:
two Corgi dogs , two umbrellas , multiple
chairs , two speakers , and one palm
tree. The setting is an outdoor area
with a beachfront view.
The objects present in the image are:
2 dogs , 3 couches , 3 umbrellas, 1 table ,
and 2 chairs.
The objects present in the image are:
sky, tree, pillow, mountain, water,
grass, bed, two dogs, umbrella, potted
plant,  dining table , six chairs .
What objects can be seen in the image?
GPT -4V
LLaV A-1.5
OneFormer  (Oracle)What objects can be seen in the image?
The objects present in the image are:
two people  (1 male and 1 female), a
table, windows , and buildings  seen
through the windows.
The objects present in the image are:
man, woman , ties (2) , suit (1) , glass wall
(1), b uilding (1), desk (1) .
The objects present in the image are:
window, wall, two people, tie .
The objects present in the image are:
window, two people, tie, wall .
sky, tree, sea, pillow, wall,  bed, two dogs, dining table, six chairs, grass, umbrella,
potted plant, mountain.The objects present in the image are:   GPT -4V
LLaV A-1.5
What is happening in the image?
Characters from "Friends" are gathered in a
living room, wearing various Halloween costumes,
possibly celebrating or attending a Halloween
party. Chandler stands out in a pink bunny
costume.
In the image, a group of people is gathered in a
room, sitting around a table. They are dressed in
costumes, possibly for a party or a themed event.
The image shows a group of people gathered in
what appears to be a living room or a dining area,
dressed in various silly, quirky costumes. They
are sitting at a dining table with chairs, engaged
in a social event or party.
Object Perception General Question-Answering
  Text in red denotes  wrong count .
  Text in pink denotes  hallucination .
  Text in blue denotes  correct object counts .
The objects present in the image are:
ceiling, floor, wall, window, cabinet,
door, light, table, rug, thirteen
people, potted plant, chair, TV .VCoderVCoderVCoderUSER USER USERFigure 2. MLLMs counting and identifying objects. As shown in the first column, GPT-4V [48] (access date: Dec 16, 2023) and LLaV A-
1.5 [39] both fail at counting people . Moreover, LLaV A-1.5 [39] misses background entities like window, wall, etc. and hallucinates about
the presence of a handbag. VCoder can predict the people counts and other background entities accurately except chairs . Similarly, in the
second column, GPT-4V and LLaV A-1.5 fail at counting chairs while the VCoder matches the Oracle’s performance. Notably, all MLLMs
can perceive objects accurately for a non-cluttered image in the third column, with LLaV A-1.5 failing at counting ties. Our VCoder can
also accurately perform general question-answering tasks, as shown in the fourth column. We treat OneFormer [25] as the Oracle for object
perception. Red text represents counting mistakes; pink text represents hallucination; blue text represents correct object perception.
when the scenes are cluttered with many entities. Conse-
quently, a natural question arises: “How to develop MLLM
systems that respond to perception questions accurately?”
This work aims to improve Multimodal LLMs at the sim-
ple yet fundamental object-level perception skills, including
counting. Our motivation stems from the intuition that one
can only describe and reason about a visual scene with the
correct understanding of the entities in the image. In our
effort to develop an accurate Multimodal LLM perception
system, we face three significant challenges: (i) the scarcity
of a vision-language dataset focused on the object percep-
tion task; (ii) existing open-sourced Multimodal LLMs usu-
ally use the ViT from CLIP [52] with an RGB image as
input as the visual component that majorly focuses only on
salient objects, and (iii) the absence of evaluation metrics toquantitatively measure Multimodal LLMs’ object percep-
tion and in particular, counting skills. We list our efforts to
overcome the issues above in the following paragraphs.
The contemporary vision-language models [14, 34, 52]
owe their success to the availability of large-scale image-
text datasets [7, 50, 56]. However, these datasets are more
focused on image captioning [33] and VQA [1] tasks, mak-
ing them unfit for training Multimodal LLMs for basic per-
ception skills like object identification and counting. To
overcome the scarcity of fundamental perception-focused
image-text data, we leverage images from the COCO [36]
dataset and use predictions from off-the-shelf visual percep-
tion models [25, 49, 54] to prepare a COCOSegmentation
Text (COST ) dataset comprising of question-answer pairs
about the objects (background and foreground) present in
27993
each image. We provide more details in Sec. 3.1.
Inspired by diffusion models that add various perception
“control” or “context” images [45, 66, 67, 70] as auxiliary
inputs to aid image generation, we propose feeding extra
perception modalities as control inputs through additional
vision encoders, which we term as our Versatile vision en-
Coders ( VCoder ). In this work, we focus on the task of
object perception and leverage a segmentation map, depth
map, or both as the control inputs; however, the same design
can be extended to other modalities. Our VCoder projects
the control inputs’ information into the LLM’s space as
shown in Fig. 4. We hypothesize that this added control
helps the MLLM improve its object perception ability.
Lastly, owing to the absence of metrics to quantify the
counting ability in MLLMs, we propose computing a count
score ( CS) using one-to-one matching of object words in
the ground truth and MLLM’s answer. We also compute a
hallucination score ( HS) based on the extra objects in the
MLLM’s response that are absent from the ground truth.
Similarly, we introduce a depth score ( DS) to quantify the
object order prediction performance in MLLMs.
Among the open-source MLLMs, we choose LLaV A-
1.5 [39] as our base MLLM. Our extensive experimental
analysis demonstrates the importance of our COST dataset
and VCoder LLaV A-1.5’s improved perception ability. To
summarize, our contributions are as follows:
• We propose using extra (perception) control inputs and
feeding those to a Versatile en Coder (VCoder ) for im-
proved object perception performance.
• We introduce a COCO Segmentation Text ( COST )
dataset to train and evaluate Multimodal LLM systems
on the fundamental object-level perception tasks of ob-
ject identification, counting, and order prediction.
• Furthermore, to quantify the object perception ability
in MLLMs, we propose calculating a count score ( CS),
a hallucination score ( HS) and a depth score ( DS). Our
experiments show that the VCoder-adapted LLaV A-
1.5 outperforms the baseline MLLMs on all metrics
when validated on the COST dataset.
2. Related Work
2.1. Visual Perception
The fundamental nature of visual perception makes it a
critical component in MLLM systems. Perception can be
divided into sub-tasks, including dense prediction tasks
like image segmentation [24, 41, 62] and depth estima-
tion [16, 18, 54], and sparse prediction tasks like object
detection [6, 63] and pose estimation [13, 58]. In the
DL era, initial methods tackled the perception task using
CNN based methods [8, 9, 21, 29, 30, 58] with recent
methods shifting to the use of transformer-based architec-
tures [10, 18, 25, 54, 65, 75]. In this work, we tackle thetask of object-level perception, mainly focusing on predict-
ing names, counts, and order of objects using MLLMs.
2.2. Visual Understanding with LLMs
Using LLMs for vision applications is not a new con-
cept. In a nutshell, developing Multimodal LLMs involves
projecting [2, 4, 34, 61] the features from a vision en-
coder [15, 53] to the embedding space of a language model
(LLM) [11, 59, 60], and, visual instruction-tuning.
LLaV A [40] proposed a pipeline to convert existing
image-text data into dialog format and then finetuned a
CLIP [52] and LLaMA [59] model end-to-end on their
collected dataset showing one of the earliest evidence of
visual-language instruction tuning. Concurrent to LLaV A,
MiniGPT-4 [74] used the visual encoder from BLIP2 [34]
and used a linear layer for projecting visual features into Vi-
cuna’s [11] feature space. InstructBLIP [14] open-sourced
a collection of 16 different datasets covering various vision
tasks like VQA, reasoning, captioning, classification, etc.,
and finetuned a BLIP2 model on their dataset. mPLUG-
Owl [68] proposed using a vision abstractor and finetun-
ing the vision encoder. More recently, LLaV A-1.5 [39]
proposed using an MLP as the projector and finetuned on
academic instruction datasets to achieve state-of-the-art per-
formance on various benchmarks [12, 17, 23, 35]. Among
various open-source MLLMs [5, 22, 31, 32, 69], we chose
LLaV A-1.5 as our baseline due to its superior performance.
2.3. Perception Hallucination in MLLMs
Since the introduction of LLMs, there has been a com-
prehensive study about their ability to hallucinate [71] in
the NLP community. However, the phenomenon of hal-
lucination in Multimodal LLMs has received compara-
tively less attention. LRV-Instruction [38] introduced a new
instruction-tuning dataset containing 400k visual instruc-
tions to prevent hallucination in MLLMs and measured per-
formance treating responses from GPT-4 [48] as ground
truths. More recently, HallusionBench [37] quantitatively
benchmarked various failure modes in MLLMs that lead to
hallucinations based primarily on logical consistency and
reasoning. Unlike these works that tried to benchmark
MLLMs mainly on VQA-type tasks, this paper focuses on
the object-level hallucination in MLLMs.
The two closest works to our objective are POPE [35]
and CHAIR [55]. On the one hand, POPE [35] tried to mea-
sure hallucination in MLLMs using a binary “Yes”-“No”
answer policy in response to questions based on the ab-
sence or presence of an object in the image. On the other
hand, CHAIR [55] focused on measuring hallucination in
image captioning based on only words and not counts for
the objects. In our work, we consider not only object words
but also the corresponding count to compute an object-level
count score and hallucination score.
27994
USER:  <seg>  <image>  What objects can be seen in the
image? Perceive as done for  semantic  segmentation.
ASSISTANT:  The objects present in the image are:
grass, person, tree, house, car, frisbee, fence, windowSemantic Object Identification Instance Object Identification
USER:  <seg>  <image>  What objects can be seen in the
image? Perceive as done for  instance  segmentation.
ASSISTANT:  The objects present in the image are:
two people, car, frisbee.
Panoptic Object IdentificationUSER:  <seg>  <image>  What objects can be seen in the
image? Perceive as done for  panoptic  segmentation.
ASSISTANT:  The objects present in the image are:
grass, tree, house, two people, frisbee, car.
Object Order  PerceptionUSER:  <depth>  <seg>  <image>  Could you list the objects in the image in order
of their perceived distance from the foreground to the background?
ASSISTANT:  The depth order for objects present in the image is: frisbee,
person, person-2, car, tree, house, grass.<seg> from
OneFormer<depth>  from
DINOv2 DPT
<image>  from
COCO dataset
question text  from
prompting GPT -4
answer text  from
OneFormer output
Figure 3. Organization of the COST dataset . We incorporate the images from COCO [36], the questions from GPT-4 [48], and the
segmentation outputs from OneFormer [25] in a question-answer format for training and evaluating MLLMs on the object identification
task. We also extend COST to the object order perception task by incorporating depth map outputs from DINOv2 [49] DPT [54]. COST
can be extended to more object-level tasks by similarly incorporating other modalities (for example, keypoint maps).
3. Object Identification with MLLMs
Suppose you are invited to a Halloween party and want to
bring candies for every person at that party. You ask your
friend to send you a picture (Fig. 1) of the party room so that
you can estimate the number of people and the number of
candies you need to buy. In a hurry, you ask GPT-4V [48]:
“Can you count the number of people in the image? ”, and it
responds: “ Yes, there are ten people visible in the image. ”.
Excited, you arrive at the party with ten candies but wait,
you see fourteen people! Confused, you look at the image
your friend sent you, and you can count fourteen people in
that image, realizing that GPT-4V fails at the simple task of
counting the people in the picture. At the same time, it can
accurately describe the happening of a Halloween party in
the image (Fig. 1). We refer to the phenomenon of Multi-
modal LLMs failing at simple visual perception tasks while
succeeding at complex visual reasoning tasks as Moravec’s
Paradox [44] in perception.
We hypothesize that one of the main reasons for theabove phenomenon is the absence of conversations cover-
ing object identification for the salient objects and the ob-
jects in the background from the instruction-tuning data for
MLLMs. To overcome this issue, we prepare the COCO
Segmentation Text ( COST ) dataset with COCO [36] im-
ages and create sentences using the output from an image
segmentation model [25] to obtain an image-text dataset to
train and evaluate MLLMs for object perception MLLMs.
Moreover, we also introduce a segmentation map as a con-
trol image input to the MLLM for better performance and
quantify object perception performance with a count score
(CS) and a hallucination score ( HS).
3.1. COST to Identify Objects with MLLMs
We find that image segmentation methods [10, 25] can ac-
curately identify salient (foreground objects like people,
cars, etc. ) and background objects (like sky, wall, etc. )
in a given scene. Guided by this finding, we use images
from the COCO [36] dataset and obtain the correspond-
27995
<image><seg><depth >C
O
N
T
R
O
L
I
N
P
U
T
S
MLPImCoder
(CLIP)MLPSegCoder
(CLIP)MLPDepthCoder
(CLIP)
What objects can be
seen in the image?
<query>VCoder
tokenize() + embed()L
L
MThe objects present in
the image are:  wall,
window, two people, tie.
VCoder
USER
Could you sort the objects in this photo
from foreground to background?
The objects present in the image are:
tie, person, person-2, wall, window.
(b) QnA about depth order of
objects(a) VCoder augmented
Multimodal LLMVCoder  
Frozen ParamsTunable Params
📝🖼 
RGB
TextFigure 4. Adapting Multimodal LLMs for accurate object perception with VCoder. (a) We add our VCoder as an adapter to the
LLaV A-1.5 [39] and feed perception modalities as extra control inputs for improved object perception performance. During training, we
freeze the components from LLaV A-1.5 (ImCoder, MLP, and LLM) to retain the original reasoning performance. (b)Using depth map and
segmentation map as the control inputs to VCoder for the object order perception task.
ing segmentation outputs from OneFormer [25], a state-
of-the-art image segmentation model. Next, we extract
the object (class) names and counts from the segmenta-
tion outputs and convert them into a sentence form for the
ground-truth answer: “The objects present in the image are:
[CNT 1] [OBJ 1], [CNT 2] [OBJ 2],. . ., [CNT N] [OBJ N]. ”,
with [OBJ i]representing the object name and [CNT i]rep-
resenting the count (if greater than one) for the ithobject
in the image. We prompt GPT-4 [48] to collect a bucket of
questions for three different object identification tasks: se-
mantic, instance, and panoptic, corresponding to the three
different image segmentation tasks. Finally, as shown in
Fig. 3, we organize the images from COCO, segmentation
maps from OneFormer, questions from GPT-4, and sen-
tences containing object information into a question-answer
format to construct our COCOSegmentation Text (COST )
dataset for training and evaluating MLLMs on the object
identification task.
Statistically, we prompt GPT-4 [48] to return 20 ques-
tions for each question bucket (panoptic, semantic, and
instance). In total, we used 280k images from the
train2017 ,test2017 , and unlabeled2017 splits
of the COCO [36] dataset and corresponding segmentation
outputs from OneFormer [25] to form the visual component
of the COST training dataset. Similarly, we prepare a COST
validation split using the 5k images from the val2017
split of the COCO dataset.
Note that a similar approach can extend the COSTdataset to other perception modalities. In this work, we in-
corporate the depth map modality into our COST dataset
for the object order perception task. Particularly, we lever-
age the publicly available DINOv2 [49] DPT [54] model to
obtain depth maps for COCO images and use the panoptic
mask (from OneFormer [25]) to estimate the depth order of
objects in an image. We format the obtained ordering of
objects into the text with the template: “The depth order
for objects present in the image is: [OBJ 1], [OBJ 2],. . .,
[OBJ J]. ”, with [OBJ j]representing the jthobject name.
To maintain relative ordering among objects belonging to
the same class, we append a count number to the second
and later objects, as shown in the bottom right of Fig. 3 for
person andperson-2 . Similar to the previous setting, we
prompt GPT-4 [48] to return 20 questions for the object or-
der perception task. We provide a detailed flow of obtaining
ground-truth object orders in the appendix.
3.2. VCoder for Multimodal LLMs
We notice that existing open-source Multimodal LLMs gen-
erally use the ViT [15] from CLIP [53] as the image encoder
(ImCoder) during instruction tuning. We reason that the
ViT focuses mainly on salient objects because it is trained
against captions, which leave out information about back-
ground regions. We argue that identifying objects in the
background is critical for a Multimodal LLM to become
skilled at perception. To overcome this limitation, we in-
troduce a segmentation map as a control input [45, 70] into
27996
our Multimodal LLM. Specifically, we use the segmentation
map from OneFormer [25] and project it to the LLM’s em-
bedding space using a pretrained ViT [15] (from CLIP [53])
as a SegCoder and a two-layer MLP [39] which we collec-
tively refer to as our Versatile en Coder (VCoder ). This
extra control from the segmentation map results in consid-
erable performance gains on the object identification task.
As shown in Fig. 4a, our VCoder adapted MLLM takes
three sets of inputs: perception modalities as control in-
puts fed into the VCoder, an RGB image fed into an Im-
age enCoder (and MLP), and the text question. The RGB
image and text are tokenized to the <img> and<query>
tokens, respectively. VCoder is flexible at handling various
perception modalities with a unique token for each modal-
ity. For example, the segmentation map and depth map in-
puts are tokenized to <seg> and<depth> tokens, respec-
tively. Similarly, one can incorporate more modalities with
modality-specific tokens. Finally, all tokenized embeddings
are concatenated and fed into the LLM.
We treat our VCoder as an adapter, added to our base
MLLM, LLaV A-1.5 [39] to obtain the final MLLM frame-
work for experiments. Note that we only train the MLP
components in the VCoder on the COST dataset. We de-
cided to keep all other parameters fixed during training to
keep the reasoning ability unaffected while achieving im-
proved object perception performance.
3.3. Evaluating MLLMs for Object Identification
Despite the availability of various metrics [35, 42, 55] to
measure object hallucination in vision-language models, no
existing metric considers the explicit object counts while
calculating their hallucination scores. We argue that object
counts returned by an MLLM are a critical component that
should not be overlooked while evaluating object identifica-
tion performance. Therefore, we propose evaluating object
identification performance in MLLMs using two metrics:
count-score ( CS) and hallucination-score ( HS).
Gdict={OBJG
1:CNTG
1;···;OBJG
N:CNTG
N}
Pdict={OBJP
1:CNTP
1;···;OBJP
M:CNTP
M}(1)
As shown in Fig. 5, given a ground-truth sentence ( G)
and an MLLM predicted response ( P), we first extract the
object words (nouns) and their corresponding count from
both text samples and represent them in a dictionary form
with keys as the object noun and the value as the corre-
sponding object’s count as shown in Eq. (1) with NandM
representing the number of different object nouns in the G
andPrespectively. Next, we perform one-to-one matching
between the counts for keys with GdictandPdictas the refer-
ence for Count Score ( CS) and Hallucination Score ( HS),
respectively, as shown in Eq. (2).
The objects present in the image are:
wall, table, light, fourteen people, two
ties, bowl, chair, cup, bottle.extract_nouns_with_count():wall: 1; table: 1; light: 1;
bowl: 1; chair: 1; cup: 1;
bottle: 1; people: 14;  ties: 2
The objects present in the image are:
wall, window, floor, six people, two cups,
bowl, two chairs, two bottles, cell phone.extract_nouns_with_count():wall: 1; window: 1; floor: 1;
bowl: 1; cell phone: 1; people: 6;
cups: 2; chairs: 2; bottles: 2CS
count -score for keys in
Ground- Truth
Ground- Truth ( ) Prediction ( )HS
hallucination-scor e for
keys in Pr edictionFigure 5. Evaluation Metrics for Object Identification . We
compare the object counts in the ground truth and prediction to
calculate a count score ( CS) and a hallucination score ( HS).
CS =100
NNX
i=1(min( CNTG
i,CNTP
i)
max( CNTG
i,CNTP
i)ifI(OBJG
i, Pdict)
0 otherwise
HS =100
MMX
j=1

1−min( CNTP
j,CNTG
j)
max( CNTP
j,CNTG
j)ifI(OBJP
j, Gdict)
1 otherwise
I(OBJ, D) =(
True if OBJ is in keys (D)
False otherwise
(2)
Count Score (CS) . It represents the percentage of correct
object counts predicted by the MLLM with respect to the
ground-truth sentence. The higher the CS, the better.
Hallucination Score (HS) . It represents the percentage of
extra object counts predicted by the MLLM that do not exist
in the ground-truth sentence. The lower the HS, the better.
Note that due to the one-to-one word-matching nature of
our evaluation, we manually define a mapping between the
categories in COCO [36] and their synonyms [43, 55]. For
example, we replace words like man, woman, child, boy,
girl,etc.with person in the prediction before evaluation.
4. Experiments
We use LLaV A-1.5 [39] as our base MLLM. LLaV A-1.5
uses CLIP-ViT-L-336px [53] as the image encoder (Im-
Coder) with a two-layer MLP as projection and Vicuna-
1.5 [73] as the LLM. Inside our VCoder, we also use
a CLIP-ViT-L-336px to encode the control inputs and
project the features into the LLM embedding space using
modality-specific two-layer MLPs. We resize the visual
inputs to 336 ×336 resolution (corresponds to 576 tokens)
for our MLLM. During training, we load the instruction-
tuned weights from LLaV A-1.5 and keep those frozen while
only tuning the MLP component of our VCoder. We use
the publicly available OneFormer [25] model trained on
27997
Semantic Instance Panoptic
Method Input Tokens CS (↑) HS ( ↓)CS (↑) HS ( ↓)CS (↑) HS ( ↓)
Closed Model, Open API
GPT-4V [48] ⟨img⟩+⟨query⟩ — — — — 38.4 83.0
Existing Open-Source Multimodal LLMs
MiniGPT-4 LLaMA-2-7b [74] ⟨img⟩+⟨query⟩ 6.2 92.2 5.6 97.7 6.2 94.9
InstructBLIP Vicuna-7b [14] ⟨img⟩+⟨query⟩ 14.2 85.8 25.3 91.9 17.5 91.2
LLaV A-1.5-7b [39] ⟨img⟩+⟨query⟩ 30.6 60.1 50.3 75.9 38.7 67.3
LLaV A-1.5-13b [39] ⟨img⟩+⟨query⟩ 25.0 69.3 49.9 75.0 35.8 68.6
CogVLM-17b [64] ⟨img⟩+⟨query⟩ 33.4 67.5 43.5 86.2 40.6 75.9
Baselines trained on the COST dataset
COST IT LLaV A-1.5-7b ⟨img⟩+⟨query⟩ 78.7 22.1 67.5 30.3 71.9 28.2
Soft-Prompted LLaV A-1.5-7b ⟨prompt ⟩+⟨img⟩+⟨query⟩ 36.2 56.7 18.4 72.2 26.8 63.0
ImCoder LLaV A-1.5-7b ⟨img⟩+⟨img⟩+⟨query⟩ 78.9 22.7 64.0 29.4 70.8 27.9
VCoder augmented LLaVA-1.5
VCoder LLaV A-1.5-7b ⟨seg⟩+⟨img⟩+⟨query⟩ 88.6 10.4 71.1 26.9 86.0 12.8
VCoder LLaV A-1.5-13b ⟨seg⟩+⟨img⟩+⟨query⟩ 89.0 10.0 73.3 25.0 87.2 11.6
Table 1. Comparison to baseline Multimodal LLMs on the COST validation dataset for Object Identification. We compare our
VCoder to existing off-the-shelf baseline MLLMs: MiniGPT-4 [74], InstructBLIP [14], LLaV A-1.5 [39], and CogVLM [64]. We also
train three different variants of LLaV A-1.5 on the COST dataset: COST IT mixes the COST training data with the instruction tuning
data; Soft-Prompted uses a set of learnable tokens, and ImCoder uses an RGB image as the control input. Our VCoder adapted LLaV A-
1.5 performs the best on all three object perception tasks. Note: ⟨·⟩denotes input tokens to LLM with segrepresenting segmentation
map, imgrepresenting RGB image, prompt representing learnable prompt, and query representing the user question. We also evaluate
the performance of GPT-4V [48] on the COST dataset using the publicly accessible paid API released by OpenAI. Our VCoder-adapted
LLaV A-1.5 shows the best performance on object identification among all MLLMs.
COCO [36] with DiNAT-L [19, 20] backbone to obtain the
segmentation map. For getting depth maps, we use the pub-
licly available ViT-L/14 distilled variant of DINOv2 [49]
DPT [54] trained on the NYUd [46] dataset. In this section,
we discuss our results on the object identification task.
4.1. Implementation Details
Training Details. We train our VCoder-adapted LLaV A-
1.5 framework for two epochs on the COST training dataset
with a batch size 256 and a learning rate of 1 e−3. For other
training hyperparameters, we follow the settings used dur-
ing the instruction-tuning stage in LLaV A-1.5 [39]. Fol-
lowing [25], we uniformly sample each object identifica-
tion task (semantic, instance, and panoptic) during training.
We also use the corresponding segmentation map from One-
Former [25] as input to the VCoder during training and in-
ference. On 8 A100 GPUs, it takes 8 and 14 hours to train
our VCoder with the 7b and 13b variants of LLaV A-1.5 as
the base MLLM, respectively.
Evaluation Details. We evaluate all MLLMs on the COST
validation set. We separately evaluate semantic, instance,
and panoptic object identification tasks while randomly
sampling questions from the corresponding task’s questionbucket. Note that for evaluating all off-the-shelf MLLMs,
we experiment with various prompts and finally use the
prompt: “[QUESTION]. Return the answer in the para-
graph format: ‘The objects present in the image are: ... ’
and then list the objects with their count in word format (if
greater than 1) in front of them, like ’two people’. ” , where
[QUESTION] is the randomly sampled question from the
object identification task bucket.
4.2. Main Results
Baselines. We compare the performance of VCoder to
open-source Multimodal LLMs, namely, MiniGPT-4 [74],
InstructBLIP [14], LLaV A-1.5 [39], and CogVLM [64] on
the COST validation set in Tab. 1. Furthermore, we provide
three additional baselines, trained for two epochs:
COST IT LLaV A-1.5 : We mix the COST training data with
the instruction tuning data used in LLaV A-1.5 [39] and fine-
tune a LLaV A-1.5 [39] model from scratch.
Soft-Prompted LLaV A-1.5 : We prepend 576 learnable to-
kens ( ⟨prompt ⟩) to the LLM input and tune only the
⟨prompt ⟩parameters on the COST training dataset.
ImCoder LLaV A-1.5 : We use an RGB image as the control
input and train on the COST training dataset.
As shown in Tab. 1, we notice that all existing MLLMs
27998
Method Depth Score ( ↓)
LLaV A-1.5-7b [39] 166.1
LLaV A-1.5-13b [39] 227.2
VCoder -DS LLaV A-1.5-7b 65.9
VCoder -DS LLaV A-1.5-13b 63.3
Table 2. Performance on Object Order Perception. Our VCoder
LLaV A-1.5 considerably outperforms LLaV A-1.5 [39], owing to
the usage of control inputs and training on the COST dataset.
perform poorly on our COST validation set, demonstrating
their inability to count and identify objects accurately. Note
that existing MLLMs perform relatively better on instance
object identification, reaffirming our claim that MLLMs are
better at detecting salient objects than background objects.
Although the baselines trained on the COST dataset per-
form relatively better, they still lag in performance com-
pared to the VCoder. Notably, a segmentation map per-
forms considerably better than using an RGB image as the
control input, proving the segmentation map’s vitality.
Comparison to GPT-4V [48]. We utilize OpenAI’s newly
released gpt-4-vision-preview1API to obtain re-
sponses from GPT-4V . Our experiments show that GPT-
4V’s responses are consistent across all object identification
tasks, closely aligning with the panoptic identification task.
Therefore, we compare our VCoder to GPT-4V only on the
panoptic object identification to reduce API requests due to
a daily limit of 500 API requests during this project. As
shown in Tab. 1, GPT-4V [48] lags behind our VCoder by
a considerable margin, reaffirming our claim that existing
MLLMs cannot perform accurate object-level perception.
5. Object Order Perception with MLLMs
As shown in Fig. 4, multiple perception modalities can be
leveraged to improve object perception in MLLMs with our
VCoder. This section presents our experiments with our
VCoder using the segmentation and depth maps as the con-
trol inputs. We term the resulting MLLM as VCoder-DS
LLaV A-1.5. Intuitively, predicting the object order implic-
itly means identifying the objects in an image. Therefore,
for the object order perception task (Fig. 4b), we use both
<depth> and<seg> inputs, while only the <seg> input
as the additional control for object identification.
During training, we use a mixture of datasets, including
the object identification and object order perception com-
ponents from the COST dataset. We also use about 200k
image-conversation (along with the corresponding segmen-
tation map obtained using OneFormer [25]) pairs randomly
sampled from the instruction tuning data used in LLaV A-
1.5 [39]. We train our VCoder for one epoch following the
same hyperparameter settings mentioned in Sec. 4.
As shown in Tab. 2, our VCoder-DS LLaV A-1.5 signif-
icantly outperforms the base MLLM, LLaV A-1.5 [39] on
1https://platform.openai.com/docs/guides/visionthe COST validation set. For quantitative evaluation of
MLLMs on the object order perception task, we calculate
a depth score ( DS) using the absolute difference between
the position of objects in the ground truth and prediction.
6. Limitations
Despite the improved object perception performance after
training our VCoder on the COST dataset, certain limi-
tations remain to be addressed for future work. Firstly,
we build our COST dataset using OneFormer [25], which
can only perceive objects belonging to a limited number of
categories due to being trained on a closed-set vocabulary
dataset [36]. For real-world applications, it is imperative to
develop an object perception benchmark for MLLMs cover-
ing many more classes with varying granularity. Secondly,
the count, hallucination, and depth scores use one-to-one
word matching, which requires defining a mapping between
synonymous words. It will be promising to explore ways to
overcome using synonym mappings.
7. Conclusion
This work analyzes the object-level perception skills of
Multimodal Large Language Models (VLMMs). Although
MLLMs are good visual reasoners, they need to improve at
the simple yet fundamental task of object perception. To
improve object perception ability in MLLMs, we propose
the COST dataset for training and evaluating MLLMs at
the object perception task. We benchmark different off-the-
shelf MLLMs and GPT-4V on our COST dataset and ob-
serve their lousy performance. Consequently, we propose
using perception modalities as control inputs and a Versa-
tile vision enCoders ( VCoder ) as an adapter for project-
ing the control inputs to the LLM embedding space. Our
VCoder can easily be extended to leverage various modal-
ities as the control inputs depending on the task. To quan-
tify the object-level perception ability in MLLMs, we intro-
duce a Count-Score ( CS), a Hallucination-Score ( HS), and
a Depth-Score ( DS). We adapted LLaV A-1.5 with VCoder,
only trained the VCoder on our COST dataset, and demon-
strated its improved performance at the object perception
task while retaining the reasoning performance. We hope
our work can inspire the research community to focus on
developing object perception datasets for MLLMs and de-
velop vision systems that are equally good at perception and
reasoning in the future.
Acknowledgements. This work was in part supported by
IARPA under Contract No. 2022-21102100004, NSF CA-
REER Award #2239840, and the National AI Institute for
Exceptional Education (Award #2229873) by National Sci-
ence Foundation and the Institute of Education Sciences,
U.S. Department of Education. We also thank the ML Cen-
ter at Georgia Tech for generously supporting this work.
27999
References
[1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret
Mitchell, C. Lawrence Zitnick, Dhruv Batra, and Devi
Parikh. Vqa: Visual question answering, 2015. 2
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,
Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-
bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
Flamingo: a visual language model for few-shot learning.
InNeurIPS , 2022. 1, 3
[3] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane
Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay,
Quentin Malartic, Badreddine Noune, Baptiste Pannier, and
Guilherme Penedo. Falcon-40B: an open large language
model with state-of-the-art performance. arXiv , 2023. 1
[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf
Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,
Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith,
Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and
Ludwig Schmidt. Openflamingo: An open-source frame-
work for training large autoregressive vision-language mod-
els.arXiv , 2023. 1, 3
[5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xi-
aodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang,
Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Day-
iheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin
Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi
Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei
Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,
Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen
Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan
Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren
Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical
report. arXiv , 2023. 3
[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In ECCV , 2020. 3
[7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12M: Pushing web-scale image-text
pre-training to recognize long-tail visual concepts. In CVPR ,
2021. 2
[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L. Yuille. Semantic image segmen-
tation with deep convolutional nets and fully connected crfs.
InICLR , 2015. 3
[9] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,
Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen.
Panoptic-deeplab: A simple, strong, and fast baseline for
bottom-up panoptic segmentation. In CVPR , 2020. 3
[10] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention masktransformer for universal image segmentation. In CVPR ,
2022. 3, 4
[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-
hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
Xing. Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality, 2023. 1, 3
[12] OpenCompass Contributors. Opencompass: A universal
evaluation platform for foundation models. https://
github.com/open-compass/opencompass , 2023.
3
[13] Timothy F Cootes, Gareth J Edwards, and Christopher J
Taylor. Active appearance models. IEEE Transactions on
pattern analysis and machine intelligence , 23(6):681–685,
2001. 3
[14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning,
2023. 1, 2, 3, 7
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR , 2021. 3, 5, 6
[16] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In Advances in neural information processing systems ,
pages 2366–2374, 2014. 3
[17] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang,
Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A
comprehensive evaluation benchmark for multimodal large
language models. arXiv , 2023. 3
[18] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der
Maaten, Armand Joulin, and Ishan Misra. Omnivore: A Sin-
gle Model for Many Visual Modalities. In CVPR , 2022. 3
[19] Ali Hassani and Humphrey Shi. Dilated neighborhood atten-
tion transformer. arXiv:2209.15001 , 2022. 7
[20] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and
Humphrey Shi. Neighborhood attention transformer. In
CVPR , 2023. 7
[21] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In ICCV , 2017. 3
[22] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,
Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen
Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia
Song, and Furu Wei. Language is not all you need: Aligning
perception with language models. ArXiv , abs/2302.14045,
2023. 1, 3
[23] Drew A Hudson and Christopher D Manning. Gqa: A new
dataset for real-world visual reasoning and compositional
question answering. CVPR , 2019. 3
[24] Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Ji-
achen Li, Steven Walton, and Humphrey Shi. Semask: Se-
28000
mantically masking transformer backbones for effective se-
mantic segmentation. arXiv , 2021. 3
[25] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita
Orlov, and Humphrey Shi. OneFormer: One Transformer to
Rule Universal Image Segmentation. In CVPR , 2023. 2, 3,
4, 5, 6, 7, 8
[26] Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jian-
chao Tan, Yadong Mu, et al. Unified language-vision pre-
training in llm with dynamic discrete visual tokenization.
arXiv , 2023. 1
[27] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-
erating images with multimodal language models. NeurIPS ,
2023. 1
[28] Deanna Kuhn. The Skills of Argument . Cambridge Univer-
sity Press, 1991. 1
[29] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks. In 2016 Fourth
International Conference on 3D Vision (3DV) , pages 239–
248. IEEE, 2016. 3
[30] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE . 3
[31] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi
Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it:
Multi-modal in-context instruction tuning. 2023. 3
[32] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint
arXiv:2305.03726 , 2023. 3
[33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation. In ICML ,
2022. 2
[34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML ,
2023. 2, 3
[35] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, and Ji-Rong Wen. Evaluating object hallucination in
large vision-language models. In EMNLP , 2023. 3, 6
[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir
Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva
Ramanan, C. Lawrence Zitnick, and Piotr Doll ´ar. Microsoft
coco: Common objects in context. In ECCV , 2014. 2, 4, 5,
6, 7, 8
[37] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser
Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusion-
bench: You see what you think? or you think what you see?
an image-context reasoning benchmark challenging for gpt-
4v (ision), llava-1.5, and other multi-modality models. arXiv
preprint arXiv:2310.14566 , 2023. 3
[38] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser
Yacoob, and Lijuan Wang. Aligning large multi-modal
model with robust instruction tuning. arXiv preprint
arXiv:2306.14565 , 2023. 3[39] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning, 2023. 1,
2, 3, 5, 6, 7, 8
[40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. In NeurIPS , 2023. 1, 3
[41] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In
CVPR , 2015. 3
[42] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei
Ji, and Pascale Fung. Negative object presence evaluation
(nope) to measure object hallucination in vision-language
models. arXiv , 2023. 6
[43] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.
Neural baby talk. In CVPR , 2018. 6
[44] H. Moravec. Mind children: The future of robot and human
intelligence. Harvard University Press, 1988. 4
[45] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian
Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-
adapter: Learning adapters to dig out more controllable abil-
ity for text-to-image diffusion models. arXiv , 2023. 3, 5
[46] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob
Fergus. Indoor segmentation and support inference from
rgbd images. In ECCV , 2012. 7
[47] OpenAI. Chatgpt. https://chat.openai.com/ ,
2022. 1
[48] OpenAI. Gpt-4 technical report, 2023. 1, 2, 3, 4, 5, 7, 8
[49] Maxime Oquab, Timoth ´ee Darcet, Theo Moutakanni, Huy V .
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-
sell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-
Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-
las Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,
Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-
janowski. Dinov2: Learning robust visual features without
supervision, 2023. 2, 4, 5, 7
[50] Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
Im2text: Describing images using 1 million captioned pho-
tographs. In NeurIPS , 2011. 2
[51] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-
ing multimodal large language models to the world. ArXiv ,
abs/2306, 2023. 1
[52] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh,
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and
Ilya Sutskever. Learning transferable visual models from nat-
ural language supervision. In ICML , 2021. 2, 3
[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. arXiv , 2021. 3,
5, 6
[54] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In ICCV , 2021. 2, 3,
4, 5, 7
28001
[55] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor
Darrell, and Kate Saenko. Object hallucination in image cap-
tioning. In EMNLP , 2018. 3, 6
[56] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs. In
NeurIPS Workshops 2021 , 2021. 2
[57] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong
Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun
Huang, and Xinlong Wang. Generative pretraining in multi-
modality. arXiv , 2023. 1
[58] Alexander Toshev and Christian Szegedy. Deeppose: Human
pose estimation via deep neural networks. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 1653–1660, 2014. 3
[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-
lien Rodriguez, Armand Joulin, Edouard Grave, and Guil-
laume Lample. Llama: Open and efficient foundation lan-
guage models. arXiv , 2023. 3
[60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer,
Moya Chen, Guillem Cucurull, David Esiobu, Jude Fer-
nandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Vik-
tor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-
renev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut
Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning
Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-
qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,
Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,
Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan
Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. Llama 2: Open foundation and fine-
tuned chat models. arXiv , 2023. 1, 3
[61] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali
Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot
learning with frozen language models. In NeurIPS , 2021. 3
[62] Z. Tu, Xiangrong Chen, Alan Yuille, and Song Zhu. Image
parsing: Unifying segmentation, detection, and recognition.
InIJCV , 2005. 3
[63] Paul Viola and Michael Jones. Rapid object detection using
a boosted cascade of simple features. In CVPR , 2001. 3
[64] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji
Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan
Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming
Ding, and Jie Tang. Cogvlm: Visual expert for pretrained
language models. arXiv , 2023. 7
[65] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,
Jose M. Alvarez, and Ping Luo. Segformer: Simple and ef-ficient design for semantic segmentation with transformers.
InNeurIPS , 2021. 3
[66] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Ir-
fan Essa, and Humphrey Shi. Prompt-free diffusion: Taking”
text” out of text-to-image diffusion models. arXiv preprint
arXiv:2305.16223 , 2023. 3
[67] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and
Humphrey Shi. Versatile diffusion: Text, images and varia-
tions all in one diffusion model. In ICCV , 2023. 3
[68] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming
Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng
Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu,
Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei
Huang. mplug-owl: Modularization empowers large lan-
guage models with multimodality, 2023. 1, 3
[69] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guo-
qiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong. What
matters in training a gpt4-style language model with multi-
modal inputs? arXiv preprint arXiv:2307.02469 , 2023. 3
[70] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV , 2023. 3, 5
[71] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,
Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong
Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi,
and Shuming Shi. Siren’s song in the ai ocean: A survey on
hallucination in large language models. arXiv , 2023. 3
[72] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt-
5: Interleaved vision-and-language generation via generative
vokens. arXiv , 2023. 1
[73] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 6
[74] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understanding with advanced large language models. arXiv ,
2023. 1, 3, 7
[75] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable detr: Deformable transformers
for end-to-end object detection. arXiv , 2020. 3
28002
