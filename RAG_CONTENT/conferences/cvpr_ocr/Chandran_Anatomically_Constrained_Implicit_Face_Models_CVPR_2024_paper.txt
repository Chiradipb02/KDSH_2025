Anatomically Constrained Implicit Face Models
Prashanth Chandran
DisneyResearch |Studios
prashanth.chandran@disneyresearch.comGaspard Zoss
DisneyResearch |Studios
gaspard.zoss@disneyresearch.com
Abstract
Coordinate based implicit neural representations have
gained rapid popularity in recent years as they have been
successfully used in image, geometry and scene modeling
tasks. In this work, we present a novel use case for such
implicit representations in the context of learning anatomi-
cally constrained face models. Actor speciﬁc anatomically
constrained face models are the state of the art in both facial
performance capture and performance retargeting. Despite
their practical success, these anatomical models are slow to
evaluate and often require extensive data capture to be built.
We propose the anatomical implicit face model; an ensem-
ble of implicit neural networks that jointly learn to model
the facial anatomy and the skin surface with high-ﬁdelity,
and can readily be used as a drop in replacement to con-
ventional blendshape models. Given an arbitrary set of skin
surface meshes of an actor and only a neutral shape with
estimated skull and jaw bones, our method can recover a
dense anatomical substructure which constrains every point
on the facial surface. We demonstrate the usefulness of our
approach in several tasks ranging from shape ﬁtting, shape
editing, and performance retargeting.
1. Introduction
Deformable face models are an important tool in the arse-
nal of visual effects artists dealing with facial animation.
As they are ubiquitously used both in high-end production
workﬂows and lightweight consumer applications, build-
ing expressive face models for various applications contin-
ues to remain an active area of research [ 17]. Face mod-
els today can range from simple linear global shape mod-
els [4,27,29] to highly complex local models that incorpo-
rate the underlying facial anatomy through physical simula-
tion [ 15,44,48] or through anatomical constraints [ 47].
In this work, we concern ourselves primarily with the
high-quality facial animation workﬂow where actor spe-
ciﬁc linear blendshape models [ 27] continue to remain
the most commonly used tool for creating facial anima-
tions [ 10,33,47]. We propose a new class of actor speciﬁcshape models named the Anatomical Implicit face Model
(AIM) which provides several unique advantages over the
existing actor speciﬁc face models, and can be used as a
drop-in replacement for traditional blendshape models.
An actor speciﬁc blendshape model is a collection of
3D shapes of the given actor performing a number of fa-
cial expressions, usually created by face scanning [ 2] or by
an artist. While the user-friendliness of such actor speciﬁc
blendshape models contributes to their wide adoption, it is
a well known limitation that such models often require hun-
dreds of shapes to accurately model complex facial defor-
mation [ 27]. To address these shortcomings, local blend-
shape models [ 10,42,47] were proposed. By splitting the
face into regions, and allowing the individual regions to de-
form independently, local shape models are able to capture
complex deformations with a limited number of shapes.
While local models address the lack of expressivity in
global shape models, state-of-the-art methods in facial per-
formance capture [ 47] and retargeting [ 10] often incorpo-
rate anatomical constraints on the facial surface to plausibly
restrict the range of the skin deformations. The anatom-
ical constraints employed by these models [ 10,47] pro-
vide a few hidden advantages that end up contributing to-
wards their practical success. For example, in the context
of facial performance capture, Wu et al.[47] demonstrated
that including anatomical constraints derived from the re-
lationship between the facial skin and underlying bones
(skull and mandible) helps to separate the rigid and non-
rigid components of facial deformation, leading to better
face performance capture. In the context of facial perfor-
mance retargeting, Chandran et al.[10] made use of such an
anatomically constrained local face model to restrict a retar-
geted shape to lie within the space of anatomically plausible
shapes of the target actor.
Despite their practical success, anatomical constraints
are often formulated in practice as regularization terms that
have to be satisﬁed as part of complex optimization prob-
lems involving several objectives. As a result, ﬁtting these
anatomical face models to a target scan or an image for in-
stance, is a computationally intensive procedure taking sev-
eral minutes per frame on a CPU, or requires hand crafted
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2220
GPU solvers [ 20]. Furthermore anatomy constraints are en-
forced only in sparse regions of the face, whereas in reality
the facial skin surface is more densely constrained by the
underlying anatomy, and simulating this dense interaction
between the anatomy and facial skin through physical simu-
lation can be even more computationally intensive [ 39,48].
In this paper, we propose the Anatomical Implicit face
Model ; a framework that allows for a holistic representa-
tion of both the facial anatomy and the skin surface using
simple implicit neural networks and facilitates the learning
of a continuous anatomical structure that densely constrains
the skin surface. Our model formulation, inspired by the
anatomical local model (ALM) of Wu et al.[47], can fur-
ther disentangle deformation arising from rigid bone motion
(jaw motion) and non-rigid deformations created by muscle
activations. Our model also addresses the computational
bottleneck of the ALM model by explicitly deriving the
skin surface from the anatomy, instead of formulating it as a
constrained optimization problem. By ensuring that a point
on the skin surface is always reconstructed through the un-
derlying anatomy, our method provides several unique fea-
tures in comparison to existing implicit face models, such
as anatomy based face manipulation (see Section 5). Be-
fore describing the details of our anatomical formulation in
Section 3, we discuss related work in Section 2.
2. Related Work
3D Morphable Models Facial models used in animation
make up for an extremely well studied body of work with
the earliest works dating back to the late 1970s [ 18]. We
therefore refer to the survey of Egger et al .[17] for an
in-depth review of the state-of-the-art methods, and pro-
vide only a concise summary in this section. Facial blend-
shapes [ 18,27] have been conventionally used as a standard
tool by artists to navigate the geometric space of human
faces. The seminal 3D linear morphable model proposed by
Blanz and Vetter [ 4] used principal component analysis to
describe the variation in facial geometry and texture, which
was later extended to multilinear models, jointly modeling
identity and expression by Vlasic et al.[43] and later by
Cao et al .[7]. Today a very commonly used morphable
face model is the FLAME model [ 29] which incorporates
identity, expression and corrective blendshapes in addition
to modeling bone motion with linear blend skinning. Due to
its ﬂexible nature, the FLAME model is widely used by face
reconstruction algorithms today [ 19]. Finally Chai et al.[8]
recently created the HIFI3D++ morphable model which is
built from a union of scans from several previously pro-
posed models.
In the past few years, numerous face models leverag-
ing the power of deep neural networks to model the non-
linear deformation of the human face have also been pro-
posed. While the initial work in this area by Ranjan etal.[38] focused on the use of specialized graph convo-
lutional networks to operate on shapes, several later ap-
proaches proposed further modiﬁcations to the network ar-
chitecture to improve the accuracy in shape representation
[5,14,22,55]. To make these deep morphable models in-
tuitive to use, Chandran et al .[9] subsequently proposed
the Semantic Deep Face Model which treats a collection of
neural networks like a multilinear model to achieve identity-
expression disentanglement. Extensions of such a semanti-
cally controllable model to deal with topology changes [ 12]
and temporal sequences of geometry [ 11] have also been
proposed. Deep neural models that jointly model the facial
geometry and appearance with semantic controls have also
been proposed [ 28].
Implicit Face Models Owing to the success of coordinate
based neural networks in representing images [ 30,40], 3D
shapes [ 35] and arbitrary scenes [ 31], today’s research on
parametric face models primarily focuses on implicit repre-
sentations. Yenamandra et al.[49] proposed i3DMM as an
initial exploration of using coordinate based networks for
modeling full head geometries. This was followed by IM-
Face [ 51] which disentangled facial geometry into separate
identity and expression embeddings with the help of indi-
vidual deformation ﬁelds. More recently, Neural Paramet-
ric Head Models (NPHM) [ 21] proposed a method which
improves the ﬁdelity of neural implicit representations by
jointly training an ensemble of local neural ﬁelds centered
around anchor points. Implicit neural representations have
also successfully been employed in learning an animat-
able avatar of a human face from only monocular video
as demonstrated by IMAvatar [ 52] and Point Avatar [ 53].
Wang et al.[45] also proposed MoRF, which is a Neural Ra-
diance Field [ 31] conditioned on an identity code allowing
for photorealistic free viewpoint rendering of the full head
in a ﬁxed expression. Recently Buhler et al.[6] also ex-
plored how such multi-identity radiance ﬁelds can be ﬁt to
sparse images to recover a volumetric head model. Finally
coordinate based neural networks have also been success-
fully employed in creating animatable human body mod-
els [3,16,23,34].
Anatomically Constrained Face Models The anatomi-
cal local model proposed in the context of monocular facial
performance capture by Wu et al.[47], ﬁrst introduced the
coupling of the anatomical bone structure to the skin sur-
face and modeled the effect of skin patches sliding over the
bone through soft anatomical constraints. This formulation
was later adapted by Chandran et al.[10] for facial perfor-
mance retargeting. Qiu et al. proposed SCULPTOR [37],
a multi-identity joint morphable model of facial anatomy
and skin learned from a database of computed tomography
(CT) scans. Recently Choi et al. proposed Animatomy [15],
a muscle ﬁber based anatomical basis for animator friendly
face modeling applications. Lastly we recognize several
2221
physically based face models [ 39,41,44,48] which inher-
ently have the ability to model anatomy constraints through
simulation.
We draw inspiration from the three classes of facial mor-
phable models discussed above and propose the Anatomical
Implicit face Model : a blendshape based, implicit, anatom-
ically constrained face model targeted towards high-quality
actor speciﬁc face modeling. Our method can be seen as
general extension of local blendshape models [ 10] to a con-
tinuously evaluable implicit function, and represents a set
of actor blendshapes through a novel anatomical formu-
lation. Unlike traditional patch-based models, our frame-
work allows us to approximate complex shapes without re-
quiring the user to specify patch layouts and other hyper-
parameters. Our solution is based on simple coordinate
based MLPs enabling efﬁcient training and inference, and
provides computational beneﬁts over previous anatomically
formulated face models [ 47]. Finally to the best of our
knowledge, our method is the ﬁrst to explore anatomical
constraints inside an implicit facial blendshape model.
Model LearningModel Fittingframecodetemplateshape
reconstruction
reconstruction
target
target
skinned shape
AIM.........
Figure 1. Our approach consists of a model learning stage (Sec-
tion 4.1) and a model ﬁtting stage (Section 4.2). In the model
learning stage, a set of an actor’s blendshapes are memorized by an
ensemble of MLPs by our Anatomical Implicit face Model (AIM).
In the second model ﬁtting stage, the memorized model can be
used as power shape prior to ﬁt the actor model to target shapes.
3. Anatomical Model Formulation
The core idea of our approach is to formulate a learning
scheme for an implicit neural representation that can repro-
duce an actor blendshape model while automatically learn-
ing the underlying facial anatomy and constraining the skin
surface to this learned anatomy. Crucial to our learning
scheme is our anatomically constrained face model that ge-
ometrically couples the underlying facial anatomy to the en-
closing skin surface which we describe next.
We assume that we are given a set of N3D scans
(S0,S1,S2,. . ,S N 1)of an actor represented as meshes.
Without loss of generality, let S0be the shape with a neu-
learned anatomy neutral geometry skinned geometry residual deformations
}Figure 2. We show the break down of how we anatomically build
up the facial skin surface. Starting from a learned anatomy surface
(left), and learned anatomical properties like the soft tissue thick-
ness, and anatomical surface normals, we reconstruct the neutral
skin geometry. The neutral anatomy is skinned, and non-rigidly
deformed with residual displacements to result in the ﬁnal shape.
tral expression (or the rest pose). Each shape Siconsists
ofVvertices, and all shapes share the same vertex connec-
tivity. For simplicity we exclude the index of the vertex
in a shape in our notation and present our formulation as
operating on surface points s2R3. Let s02R3and
si2R3be corresponding points on the skin surface for the
neutral expression and expression irespectively. In most
previous methods for learning neural face models, a skin
surface point sis learned as a displacement from a base
face surface [ 9,12,21] or simply as points lying in an arbi-
trary 3D space [ 45,51,52]. Contrary to such approaches,
we propose to learn the skin surface susing implicit neural
representations that arrives at the facial skin surface through
a formulation that combines anatomical constraints, linear
blend skinning (LBS), and expression blendshapes into a
single framework.
For our model formulation, we take inspiration from the
anatomical constraints ﬁrst proposed for non-neural face
models [ 1,47], particularly that of Wu et al.[47]. They
establish a link between the skin surface and the anatomical
bones by modeling the thickness di2Rof the soft tissue
between a bone point bi2Rand the skin surface si. These
constraints are deﬁned in sparse regions of the face where a
skin point can be trusted to have bone underneath. We draw
inspiration from their simple formulation and make some
important deviations that enable us to jointly learn both the
surface of the underlying skin anatomy and the enclosing
skin surface for every point on the skin through end-to-end
learning. Speciﬁcally, we arrive at a point on the skin sur-
face as follows
s0=b0+d0n0 (1)
where s0is the position of a surface point corresponding
tosibut on the neutral shape S0,b0,d0,a n d n0are the
bone point, soft tissue thickness and the bone normal at s0.
While Eq. 1allows us to reconstruct points on the neutral
face geometry, to adequately represent skin surfaces under
arbitrary facial expressions, we need to account for surface
deformation arising from the rigid motion of underlying
facial bones (skull and mandible), and the non-rigid skin
motion arising from muscle activations, skin sliding, and
2222
self collisions. To accommodate these additional degrees of
freedom in skin deformation, we incorporate standard lin-
ear blend skinning, and expression blendshapes similar to
the FLAME model [ 29]. Therefore given an anatomically
reconstructed point on the neutral skin surface s0, we can
now compute the position of the same point in an arbitrary
expression sias
si=LBS(s0,Tb,k)+ei (2)
where LBS refers to the standard linear blend skinning op-
erator that rigidly transforms the anatomically reconstructed
neutral surface point s0with a transformation Tband a skin-
ning weight k,ei2R3is the corrective displacement that
is added on top of the skinned result to account for deforma-
tions that cannot be explained by skinning alone. A visual
overview of our approach to anatomically build up the facial
skin surface is shown in Fig. 2.
At this point we have established how to arrive at points
on the skin surface sifor a shape in an arbitrary facial ex-
pression Siby starting from the underlying anatomy bi. It
is important to note that the anatomical constraints as de-
ﬁned by Wu et al.[47] can only be computed on regions
with an underlying bone, and thus, regions like the cheeks
are not anatomically constrained in their approach. An es-
sential feature of our approach that distinguishes it from all
previous works is that we enforce anatomical constraints for
every point on the skin surface; even in regions where there
is no underlying biological bone structure. For this purpose
we redeﬁne the anatomy in our work as a rigidly deforming
region underneath the skin surface that is not restricted to
only the manifold of the skull and mandible bones. Since
this structure does not exist in reality and is, therefore, not
available for supervised learning, we formulate a learning
framework where such rigidly deforming surface can be
learned only from the sparse set of anatomical constraints
computed between the skin and the underlying bones. As
we will see in Section 5, learning this anatomical surface
from data leads to several interesting applications in shape
manipulation and performance retargeting that were previ-
ously challenging to obtain without expensive physical sim-
ulation [ 48] or extensive volumetric data capture [ 37].
4. Anatomical Implicit Face Model
At a high level, our method is comprised of two stages: ﬁrst,
a model learning stage (Section 4.1) and second, a model
ﬁtting stage ( 4.2). In the model learning stage, we bake
a collection of expression blendshapes from an actor into
an implicit neural network that uses the anatomical model
formulation described in Section 3. Our model ﬁtting stage
uses this learned Anatomical Implicit face Model (AIM) and
optimizes for coefﬁcients that deform the model to match
test time constraints like 3D shapes, 2D landmarks and so
on. The overview of our approach is shown in Fig. 1.
...a)b)Figure 3. a) We assume we are given the neutral geometry of an
actor along with an rough estimate of the skull and jaw bone [ 56].
b) We additionally use a collection of N3D shapes of the actor
performing expressions. Unlike Wu et al.[47], we do not require
the tracked anatomy (skull [ 1], jaw [ 57]) for the expression shapes.
4.1. Model Learning
To learn our anatomical implicit face model, we assume we
are given a template shape C, a registered set of Nshapes
(S0,S1,S2,. . ,S N 1)of a single actor in the same topology
of the canonical shape. Additionally we ﬁt a template skull
and jaw only to the neutral shape using the method of Zoss
et al.[56]. The template shape Ccan either be the neutral
shape of the actor or a generic face shape, and the number
of shapes provided can be arbitrary. We use a collection
of 20 shapes in our work. A visual summary of our train-
ing data is shown in Fig. 3. Our objective in the learning
stage is to use a coordinate based neural network to memo-
rize the given shapes through the anatomical formulation in
Section 3. Given the high representation power of periodic
implicit neural networks [ 40], we use the SIREN coordi-
nate network; an MLP with sinusoidal activation functions,
as our base architecture. An ablation study on alternate net-
work choices is provided in our supplemental.
Given a point c2R3on the template shape C, we use
three independent MLPs denoted by B,D, and Nto pre-
dict the anatomy point eb02R3, the soft tissue thickness
ed02R, and the anatomy normal en02R3. These predicted
anatomical properties are then used to reconstruct the posi-
tion of a point on the neutral skin surface es0as
eb0=B(c) (3)
ed0=D(c) (4)
en0=N(c) (5)
es0=eb0+ed0en0. (6)
As discussed in Section 3, to further account for the rigid
and non-rigid deformations of the skin surface, the anatom-
ically constructed neutral skin point es0has to be skinned
and further displaced with residual expression deforma-
tions. We therefore employ two additional MLPs Kand
Ethat predict the skinning weight ek2Rand the corrective
displacements basis Be2R(N 1)⇥3respectively. Note
here that, as an implementation detail, we predict the ex-
pression displacements for all N 1blendshapes (exclud-
ing the neutral) at once from E. The corrective expression
2223
displacement eei2R3for shape ican be extracted from this
output by indexing Beappropriately.
ek=K(c) (7)
Be=E(c) (8)
eei=Be[i] (9)
esi=LBS⇣
es0,eTb,ek⌘
+eei (10)
Here eTb2R9is a 6-DOF jaw bone transformation opti-
mized along with the training of the MLPs to account for
rigid motion of the mandible. Here we parameterize the
jaw bone rotation eTbfollowing the continuous 6D represen-
tation [ 54].
4.1.1 Training Objectives
We next describe the training objectives to learn actor ex-
pression blendshapes along with the underlying anatomy
structure for each skin surface point.
Skin Position Loss The skin position loss penalizes the dif-
ference between the estimated skin point esiand the ground
truth skin point si.
LS= S||esi si||2
2 (11)
We set  S=1.0for all our experiments.
Anatomy Regularizer Since we can roughly estimate the
skull and jaw geometry on the neutral shape using the
method of Zoss et al.[56], we compute sparse anatomical
constraints [ 47] and loosely regularize the learned anatomi-
cal properties to stay close to these estimates only in regions
where the constraints can be accurately computed (i.e. skin
regions with an underlying bone).
LA= b||eb0 b0||2
2+ d||ed0 d0||2
2+ n||en0 ni||2
2(12)
We set  b= d= n=1.0for all our experiments, and
observe that this constraint only regularizes 5-10% of all
the vertices generated by the model on average (see Supple-
mental).
Thickness Regularizer We regularize the soft tissue thick-
ness edpredicted by the model in unconstrained regions to
remain as small unless dictated otherwise by the skin posi-
tion loss.
LD= Reg
D||ed0||2
2 (13)
We set  Reg
D=7.5e 4for all our experiments.
Symmetry Regularizer To exploit the symmetry of the
face, we regularize the predictions of the anatomy MLP B
to be symmetric. We achieve this by requiring that reﬂect-
ing the input points calong the plane of symmetry provides
the same result as reﬂecting the predicted anatomy points ea.
LSym= sym||B(R(c)) R(B(c))||2
2 (14)where Ris an operator that reﬂects a point along the plane
of symmetry. We set  sym= 1e  4for all our experiments.
Note that we do not regularize symmetry on the predicted
thickness or anatomy normals thereby allowing the model
to still be able to represent asymmetric faces.
Optional Skinning Weight Regularizer Finally inspired
by [52], we use an optional loss that encourages the esti-
mated skinning weights ekin regions like the forehead that
are guaranteed to not be affected by the rigid deformation
of the jaw bone to be zero.
LK= k||K(c⇤)||2
2 (15)
here c⇤refers to a small region on the canonical shape C
which includes the forehead. We set  K= 1e2 for all our
experiments.
Our ﬁnal model energy LModel is a summation of the
above losses and is minimized using gradient decent [ 26]
to train our ensemble of coordinate MLPs end-to-end.
LModel =LS+LA+LD+LSym+LK (16)
4.2. Model Fitting
While the aforementioned model can recover interesting
anatomical properties of the face with only sparse supervi-
sion, it is not very useful unless it can be deformed to match
user constraints and serve as a shape prior for an actor facial
geometry.
After training our anatomical implicit face model on a
collection of Nshapes, the coefﬁcients that are required to
deform it include a jaw bone transformation Tb⇤2R9,
coefﬁcients w⇤2RN 1that can be used to blend the cor-
rective expression displacements Be2R(N 1)⇥3, and an
optional global head transformation Tg⇤2R9. Following
Equation ( 10), we can therefore evaluate our anatomical im-
plicit face model as
s⇤=Tg⇤ 
LBS⇣
es0,Tb⇤,ek⌘
+X
N 1w⇤Be!
(17)
where Tg⇤,Tb⇤andw⇤are the only unknowns, and the
rest can be queried from a pre-trained AIM. We consider
two scenarios for model ﬁtting which include i) ﬁtting our
model to a sequence of 3D scans e.g. from a facial perfor-
mance, and ii) ﬁtting our model to 2D landmarks detected
on a video [ 13,46].
For both scenarios, inspired by the state-of-the-art ﬁnd-
ings of Kim et al.[50], we employ neural reparameterized
optimization [ 25] and solve for the weights of a simple MLP
that predicts the unknown parameters instead of directly op-
timizing for them. Speciﬁcally when given a sequence of J
frames with 3D/2D constraints, we optimize for Jframe
codes zj2Rfwhich, when fed as input to a simple 4-layer
2224
MLP FTwith GeLU [ 24] activations, predicts the head Tgj
and jaw Tbjposes for each frame. Additionally as the co-
efﬁcients wjare local and spatially varying depending on
the template query point c, we use a separate 4-layer MLP
FWwhich predicts the coefﬁcients wjby taking both the
frame code zjand the query point cas input.
[Tgj,Tbj]=FT(zj) (18)
wj=FW(zj,c) (19)
Unlike the method of Kim et al.[50] where the reparam-
eterized optimization was used mainly for improved per-
formance, this neural optimization is even necessary in our
case to restrict the number of optimized variables as the
number of spatially varying coefﬁcients w⇤used to evaluate
our anatomical implicit face model can vary drastically de-
pending on the number of constraint points (see Section 5).
4.2.1 Fitting Objectives
3D Position Constraint For ﬁtting our trained model to 3D
constraints coming from a facial performance of an actor,
we minimize the euclidean distance between the estimated
skin point s⇤and the ground truth skin point sGT. However
by constraining only the ﬁnal skin surface, expression dis-
placements could overcompensate for the skinned geome-
try. To prevent this from happening, we additionally require
the skinned shape without corrective displacements s⇤
lbsto
be as close as possible to the ground truth skin point.
L3D
Pos= 3D 
||s⇤ sGT||2
2+||s⇤
lbs sGT||2
2 
(20)
2D Position Constraint For ﬁtting our model to 2D con-
straints such as facial landmarks estimated by a pre-trained
landmark detector [ 13,46], we project the estimated skin
point s⇤to screen space using known camera intrinsics
 and calculate the euclidean distance in 2D between the
project point  (s⇤)and the corresponding landmark.
L2D
Pos= 2D 
|| (s⇤) p||2
2+|| (s⇤
lbs) p||2
2 
(21)
p2R2is a detected landmark corresponding to point s⇤.
Coefﬁcient Regularizer As the complexity of our implicit
anatomical face model can be arbitrarily large, we regular-
ize the estimated blending coefﬁcients w⇤to be small with
a weak L2 regularizer.
LW= w
Reg||w⇤||2
2 (22)
We set  w
Reg=0.75for all our experiments.
Temporal Regularizer Finally when optimizing for coefﬁ-
cients on sequential data, we regularize the optimized frame
codes zjto remain similar between adjacent frames.
LT= t
Reg||zj zj 1||2
2 (23)We set  t
Reg=0.05for all our experiments. Our ﬁnal ﬁt-
ting energy LFitting is therefore
LFitting =L3D
Pos+L2D
Pos+LW+LT (24)
For 3D/2D ﬁtting, we set  2Dand 3Dto 0 respectively.
4.3. Implementation Details
In the model learning stage, we optimize our implicit co-
ordinate networks for 1e4iterations with a learning rate of
2e 3. This takes approximately 10 minutes to converge on
a single Nvidia RTX 3090 for an actor model with 40,000
vertices and 20 blendshapes. In the model ﬁtting stage, we
use a learning rate of 1e 3and optimize the ﬁtting MLPs
FTandFWfor1e4iterations. This process takes 1 second
per frame on a single Nvidia RTX 3090. We implement
all our MLPs in PyTorch [ 36]. In our supplementary ma-
terial we discuss the performance implications of replacing
our current python backend with the well engineered fused
MLP implementation [ 32].
5. Results
We now present several results, applications and evaluations
of our Anatomical Implicit face Model (AIM).
shapes reconstruction
skinning weightstarget neutral reconstructionanatomy
error thickness
skinning weightstarget neutral reconstructionanatomy
error thicknessshapes reconstructionreconstruction error
reconstruction error
Figure 4. We demonstrate the ability of our Anatomical Im-
plicit face Model to recover plausible anatomic features of the
face, while also modeling the skin surface with very high ﬁdelity.
A subset of 3 expressions from 2 different actor speciﬁc mod-
els are shown here. The errors are displayed with a scale of
0mm
 5mm.
5.1. Learning Actor Speciﬁc Anatomy
We begin by showing the reconstruction accuracy of our
AIM on facial blendshapes of multiple actors. As seen in
2225
cheeks soft-tissue volume editingmaskFigure 5. Once the AIM is learned for an actor, it can be used to
intuitively deform a face using the learned anatomic properties, as
demonstrated here by scaling the soft tissue thickness in a hand
painted cheek region, and by propagating the change to the skin
surface thanks to our formulation.
Fig.4on 2 different actors, our method can faithfully rep-
resent facial shapes with high ﬁdelity while capturing both
the low and high frequency features of facial shape and ex-
pression. We also show the anatomic features recovered by
our new formulation which includes the dense underlying
facial anatomy (shown in red), the soft tissue thickness at
every point on the anatomy (visualized as heatmap), and the
optimized subject speciﬁc skinning weights. These results
highlight the new abilities introduced by our method in re-
covering plausible anatomy features while jointly learning
to model surface deformations.
5.2. Anatomy Manipulation
Our ability to estimate the underlying anatomy that densely
constrains the skin surface opens up new, yet computa-
tionally inexpensive ways to edit facial geometry using our
learned anatomic properties. For example, as illustrated in
Fig.5, by simply scaling the learned soft tissue thickness d
in desired regions of the face (denoted by the hand drawn
mask), an artist can interactively sculpt/deform an actor’s
face shape to match their requirements.
5.3. Expression Reconstruction
We next evaluate the expressiveness of our model by ﬁt-
ting it to unseen 3D performances of multiple actors. Given
a sequence of Jdynamic 3D shapes from a studio scan-
ner [ 2], we ﬁrst deform our template mesh Cto match
the scanned shapes using standard mesh registration tech-
niques such that the dynamic 3D scans are in full vertex
correspondence with our AIM. We then follow the ﬁtting
procedure described in Section 4.2and obtain per-frame
transformations [Tgj,Tbj]and shape coefﬁcients wjthat
explain the captured ground truth shape. For this experi-
ment, we use the 3D position constraint from Eq. ( 24) and
setL2Dto 0. We densely constrain the ﬁtting procedure
at every vertex of the ground truth shape. In Fig. 6we pro-
vide both a qualitative and quantitative comparison of ﬁtting
to novel performance from an actor against global blend-
shapes (GBS) [ 27], a patch blendshape model (PBS) [ 13],
and the anatomical local model (ALM) [ 47]. In this exper-Table 1. Average ﬁtting error (in mm) across 819 frames from 5
sequences of 5 different actors.
GBS [27]PBS [13]ALM [47]Ours (G) Ours
0.83 0.51 0.09 0.86 0.31
iment, we use 20 ground truth actor blendshapes to build
the GBS, PBS, and ALM models, and the anatomically re-
constructed blendshapes for our method. Even under this
slight disadvantage, our method outperforms both GBS, and
PBS and provides visually comparable results to the ALM
model. Table 1shows the average ﬁtting error of each
method across 819 frames from 5 sequences of 5 different
actors. Ours (G) refers to a variant of our ﬁtting algorithm
where the expression coefﬁcients are applied globally to ob-
tain a face shape. Our method converges in a few seconds
for each frame, while the ALM algorithm consistently re-
quires several minutes per frame. While the continuous na-
ture of AIM enables us to evaluate it with coefﬁcients of ar-
bitrary locality, it could result in situations where our ﬁtting
is underconstrained in the absence of dense constraints lead-
ing to broken shapes. To illustrate that this does not happen
in our reparameterized optimization, we show the result of
ﬁtting the AIM to sparse constraints in Fig. 7. While in-
creasing the density of constraints improves the ﬁtting ac-
curacy, ﬁtting our model to sparse landmarks also provides
plausible results. Note that we do not compare ﬁtting accu-
racy against generic morphable models like FLAME [ 29] or
NPHM [ 21] as ours is actor speciﬁc and therefore a quanti-
tative comparison might be unfair to the other methods. We
kindly refer to our supplemental material for more results.
5.4. 3D Performance Retargeting
Another important application of our method is in the area
of 3D performance retargeting, where the goal is to trans-
fer a facial animation from a source to a target character
while respecting the identity and anatomical characteris-
tics of the target character. To accomplish this using our
model, we learn two separate instances of our model for the
source and target character respectively from a sparse set of
20 blendshapes in correspondence. We then ﬁt our source
model to the facial animation of the source target character
to obtain per-frame transformations [Tgj,Tbj]and shape
coefﬁcients wj. These coefﬁcients can simply be played
back on the target model to achieve facial performance re-
targeting. In Fig. 8, we provide a qualitative comparison to
the state-of-the-art 3D retargeting algorithm of Chandran et
al.[10] by retargeting the performance from a source to a
target character. Our method provides competitive results
to state of the art, while also allowing users to disentangle
the rigid jaw motion and the soft tissue deformations of the
skin surface. Our method additionally provides a substan-
tial runtime beneﬁt here and retargets each frame in a few
(2-3) seconds, while the method of Chandran et al. requires
2226
TargetOursALMPBSGBSFigure 6. We show qualitative and quantitative comparisons of
ﬁtting 3D performances with various actor speciﬁc models. All the
errors are displayed with a scale of 0mm
 5mm.
several minutes per frame due to a costly anatomical solve.
Finally unlike the approach of Chandran et al., our method
provides all of above beneﬁts without having to manually
choose design parameters such as the patch layout, number
of overlaps etc.
5.5. Limitations
Due to the sparse supervision on the facial anatomy, some-
times artifacts can appear on the learned anatomical sur-
face especially in areas surrounding the lip region. Another
limitation of our work is we current do not skin the facial
64
500
36000Figure 7. Our continuous anatomical face model can be ﬁt to 3D
scans with varying density of constraints and still provide valid
results due to our ﬁtting algorithm.: all the errors are displayed
with a scale of 0mm
 5mm.
inputoursours LBS ours deformations[Chandran et al. 2022]
Figure 8. We show the result of facial performance transfer in 3D
from an input actor (left) to a different actor as produced by our
method (2nd column) and the local retargeting model of Chandran
et al.[10]. While providing qualitatively similar results, our model
implicitly disentangles the performance into rigid jaw motion (3rd
column), and nonrigid soft tissue deformations (4th column).
anatomy to rigidly deform it along with facial expressions.
Addressing these limitations through additional anatomical
regularization or by predicting expression speciﬁc normals
and thickness maps could be interesting future work. Some
temporal jitter could also occur in our ﬁtting step for chal-
lenging performances if the optimization is terminated too
early. Finally extending our model to support facial appear-
ance could be valuable future work.
6. Conclusion
In this paper we propose a new anatomically constrained
implicit face model which provides a holistic representation
of both facial anatomy and the enclosing skin surface using
an ensemble of coordinate neural networks. Given an
arbtrary set of skin surface meshes and only a neutral shape
with estimated skull and jaw bones, our method recovers
a dense anatomical substructure to constrain each point on
the skin surface, and can model complex skin deformations
with high ﬁdelity. While we have explored the use of
such a model in the context of actor speciﬁc blendshape
models, future work could analyze it’s implications as a
generic morphable model, by extending our formulation
to handle multiple identities at once. Our new Anatomical
Implicit face Model (AIM) has applications in shape repre-
sentation and manipulation, retargeting and more, and we
hope that our method encourages exciting future research.
2227
References
[1]Thabo Beeler and Derek Bradley. Rigid stabilization of facial
expressions. ACM TOG , 33(4), 2014. 3,4
[2]Thabo Beeler, Fabian Hahn, Derek Bradley, Bernd Bickel,
Paul Beardsley, Craig Gotsman, Robert W. Sumner, and
Markus Gross. High-quality passive facial performance cap-
ture using anchor frames. ACM Trans. Graphics Proc SIG-
GRAPH , 30, 2011. 1,7
[3]Sourav Biswas, Kangxue Yin, Maria Shugrina, Sanja Fi-
dler, and Sameh Khamis. Hierarchical neural implicit
pose network for animation and motion retargeting. CoRR ,
abs/2112.00958, 2021. 2
[4]V olker Blanz and Thomas Vetter. A morphable model for the
synthesis of 3d faces. In Siggraph , 1999. 1,2
[5]G. Bouritsas, S. Bokhnyak, S. Ploumpis, S. Zafeiriou, and
M. Bronstein. Neural 3d morphable models: Spiral convo-
lutional networks for 3d shape representation learning and
generation. In ICCV , 2019. 2
[6]Marcel C B ¨uhler, Kripasindhu Sarkar, Tanmay Shah,
Gengyan Li, Daoye Wang, Leonhard Helminger, Ser-
gio Orts-Escolano, Dmitry Lagun, Otmar Hilliges, Thabo
Beeler, et al. Preface: A data-driven volumetric prior for
few-shot ultra high-resolution face synthesis. In ICCV , 2023.
2
[7]Chen Cao, Yanlin Weng, Shun Zhou, Y . Tong, and Kun
Zhou. Facewarehouse: A 3d facial expression database for
visual computing. IEEE Transactions on Visualization and
Computer Graphics , 20, 2014. 2
[8]Zenghao Chai, Haoxian Zhang, Jing Ren, Di Kang,
Zhengzhuo Xu, Xuefei Zhe, Chun Yuan, and Linchao Bao.
Realy: Rethinking the evaluation of 3d face reconstruction.
InProceedings of the European Conference on Computer Vi-
sion (ECCV) , 2022. 2
[9]Prashanth Chandran, Derek Bradley, Markus Gross, and
Thabo Beeler. Semantic deep face models. In TDV, 2020.
2,3
[10] Prashanth Chandran, Lo ¨ıc Ciccone, Markus Gross, and
Derek Bradley. Local anatomically-constrained facial per-
formance retargeting. ACM Trans. Graph. , 41(4), 2022. 1,
2,3,7,8
[11] Prashanth Chandran, Gaspard Zoss, Markus Gross, Paulo
Gotardo, and Derek Bradley. Facial Animation with Disen-
tangled Identity and Motion using Transformers. Computer
Graphics Forum , 2022. 2
[12] Prashanth Chandran, Gaspard Zoss, Markus Gross, Paulo
Gotardo, and Derek Bradley. Shape transformers: Topology-
independent 3d shape models using transformers. 41(2),
2022. 2,3
[13] P. Chandran, G. Zoss, P. Gotardo, and D. Bradley. Contin-
uous landmark detection with 3d queries. In CVPR . IEEE
Computer Society, 2023. 5,6,7,1
[14] Zhixiang Chen and Tae-Kyun Kim. Learning feature aggre-
gation for deep 3d morphable models. In CVPR , 2021. 2
[15] Byungkuk Choi, Haekwang Eom, Benjamin Mouscadet,
Stephen Cullingford, Kurt Ma, Stefanie Gassel, Suzi Kim,
Andrew Moffat, Millicent Maier, Marco Revelant, Joe Let-
teri, and Karan Singh. Animatomy: An animator-centric,anatomically inspired system for 3d facial modeling, anima-
tion and transfer. In SIGGRAPH Asia 2022 Conference Pa-
pers, 2022. 1,2
[16] Boyang Deng, J. P. Lewis, Timothy Jeruzalski, Gerard Pons-
Moll, Geoffrey Hinton, Mohammad Norouzi, and Andrea
Tagliasacchi. Nasa neural articulated shape approximation.
InECCV , 2020. 2
[17] Bernhard Egger, William A. P. Smith, Ayush Tewari, Ste-
fanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian
Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani,
Christian Theobalt, V olker Blanz, and Thomas Vetter. 3d
morphable face models - past, present and future. ACM TOG ,
39(5), 2020. 1,2
[18] Paul Ekman and Wallace V . Friesen. Facial action coding
system: a technique for the measurement of facial move-
ment. 1978. 2
[19] Yao Feng, Haiwen Feng, Michael J. Black, and Timo
Bolkart. Learning an animatable detailed 3D face model
from in-the-wild images. TOG , 40(4), 2021. 2
[20] Marco Fratarcangeli, Derek Bradley, Aurel Gruber, Gaspard
Zoss, and Thabo Beeler. Fast Nonlinear Least Squares Op-
timization of Large-Scale Semi-Sparse Problems. Computer
Graphics Forum , 2020. 2
[21] Simon Giebenhain, Tobias Kirschstein, Markos Georgopou-
los, Martin R ¨unz, Lourdes Agapito, and Matthias Nießner.
Learning neural parametric head models. In Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR) ,
2023. 2,3,7
[22] S. Gong, L. Chen, M. Bronstein, and S. Zafeiriou. Spiral-
net++: A fast and highly efﬁcient mesh convolution operator.
InICCV , 2019. 2
[23] Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and Otmar
Hilliges. Vid2avatar: 3d avatar reconstruction from videos
in the wild via self-supervised scene decomposition. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , 2023. 2
[24] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities
and stochastic regularizers with gaussian error linear units.
CoRR , abs/1606.08415, 2016. 6
[25] Stephan Hoyer, Jascha Sohl-Dickstein, and Sam Greydanus.
Neural reparameterization improves structural optimization.
CoRR , abs/1909.04240, 2019. 5
[26] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR , 2015. 5
[27] J. P. Lewis, K. Anjyo, Taehyun Rhee, M. Zhang, Fr ´ed´eric H.
Pighin, and Z. Deng. Practice and theory of blendshape facial
models. In Computer Graphics Forum (Proc. Eurographics ,
2014. 1,2,7
[28] Ruilong Li, Karl Bladin, Yajie Zhao, Chinmay Chinara,
Owen Ingraham, Pengda Xiang, Xinglei Ren, Pratusha
Prasad, Bipin Kishore, Jun Xing, and Hao Li. Learn-
ing formation of physically-based face attributes. CoRR ,
abs/2004.03458, 2020. 2
[29] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and
Javier Romero. Learning a model of facial shape and ex-
pression from 4D scans. ACM , 2017. 1,2,4,7,3,5
2228
[30] Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R.
Chan, Marco Monteiro, and Gordon Wetzstein. ACORN:
adaptive coordinate networks for neural scene representa-
tion. CoRR , abs/2105.02788, 2021. 2
[31] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. In ECCV , 2020. 2
[32] Thomas M ¨uller. tiny-cuda-nn, 2021. 6,2
[33] Ver´onica Orvalho, Pedro Bastos, Frederic Parke, Bruno
Oliveira, and Xenxo Alvarez. A Facial Rigging Survey. In
Eurographics 2012 - State of the Art Reports . The Euro-
graphics Association, 2012. 1
[34] Pablo R. Palafox, Aljaz Bozic, Justus Thies, Matthias
Nießner, and Angela Dai. Npms: Neural parametric mod-
els for 3d deformable shapes. abs/2104.00702, 2021. 2
[35] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.
InCVPR , 2019. 2
[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-
son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An
imperative style, high-performance deep learning library. In
Advances in Neural Information Processing Systems . 2019.
6
[37] Zesong Qiu, Yuwei Li, Dongming He, Qixuan Zhang, Long-
wen Zhang, Yinghao Zhang, Jingya Wang, Lan Xu, Xudong
Wang, Yuyao Zhang, and Jingyi Yu. Sculptor: Skeleton-
consistent face creation using a learned parametric generator.
ACM Trans. Graph. , 41(6), 2022. 2,4
[38] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and
Michael J. Black. Generating 3d faces using convolutional
mesh autoencoders. In ECCV , 2018. 2
[39] Eftychios Sifakis, Andrew Selle, Avram Robinson-Mosher,
and Ronald Fedkiw. Simulating speech with a physics-based
facial muscle model. In Proceedings of the 2006 ACM SIG-
GRAPH/Eurographics Symposium on Computer Animation .
Eurographics Association, 2006. 2,3
[40] Vincent Sitzmann, Julien N.P. Martel, Alexander W.
Bergman, David B. Lindell, and Gordon Wetzstein. Implicit
neural representations with periodic activation functions. In
Proc. NeurIPS , 2020. 2,4
[41] Sangeetha Grama Srinivasan, Qisi Wang, Junior Rojas,
Gergely Kl ´ar, Ladislav Kavan, and Eftychios Sifakis. Learn-
ing active quasistatic physics-based models from data. ACM
Trans. Graph. , 40(4), 2021. 3
[42] J. Rafael Tena, Fernando De la Torre, and Iain Matthews.
Interactive region-based linear 3d face models. ACM Trans.
Graphics Proc SIGGRAPH , 30(4), 2011. 1
[43] Daniel Vlasic, Matthew Brand, Hanspeter Pﬁster, and Jovan
Popovi ´c. Face transfer with multilinear models. ACM TOG ,
24(3), 2005. 2[44] Nicolas Wagner, Mario Botsch, and Ulrich Schwanecke.
Softdeca: Computationally efﬁcient physics-based facial an-
imations. In Proceedings of the 16th ACM SIGGRAPH Con-
ference on Motion, Interaction and Games , 2023. 1,3
[45] Daoye Wang, Prashanth Chandran, Gaspard Zoss, Derek
Bradley, and Paulo Gotardo. Morf: Morphable radiance
ﬁelds for multiview neural head modeling. In ACM SIG-
GRAPH 2022 Conference Proceedings , 2022. 2,3
[46] Erroll Wood, Tadas Baltru ˇsaitis, Charlie Hewitt, Matthew
Johnson, Jingjing Shen, Nikola Milosavljevic, Daniel Wilde,
Stephan Garbin, Chirag Raman, Jamie Shotton, Toby Sharp,
Ivan Stojiljkovic, Tom Cashman, and Julien Valentin. 3d face
reconstruction with dense landmarks, 2022. 5,6
[47] Chenglei Wu, Derek Bradley, Markus Gross, and Thabo
Beeler. An anatomically-constrained local deformation
model for monocular face capture. ACM TOG , 35(4), 2016.
1,2,3,4,5,7
[48] Lingchen Yang, Byungsoo Kim, Gaspard Zoss, Baran
G¨ozc¨u, Markus Gross, and Barbara Solenthaler. Implicit
neural representation for physics-driven actuated soft bodies.
ACM Trans. Graph. , 41(4), 2022. 1,2,3,4
[49] Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-
Peter Seidel, Mohamed Elgharib, Daniel Cremers, and
Christian Theobalt. i3dmm: Deep implicit 3d morphable
model of human heads. In CVPR , 2021. 2
[50] Kim Youwang, Lee Hyun, Kim Sung-Bin, Suekyeong Nam,
Janghoon Ju, and Tae-Hyun Oh. A large-scale 3d face
mesh video dataset via neural re-parameterized optimization.
arXiv , 2023. 5,6
[51] Mingwu Zheng, Hongyu Yang, Di Huang, and Liming Chen.
Imface: A nonlinear 3d morphable face model with implicit
neural representations. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
2022. 2,3
[52] Yufeng Zheng, Victoria Fern ´andez Abrevaya, Marcel C.
B¨uhler, Xu Chen, Michael J. Black, and Otmar Hilliges. Im
avatar: Implicit morphable head avatars from videos. In
Computer Vision and Pattern Recognition (CVPR) , 2022. 2,
3,5
[53] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J.
Black, and Otmar Hilliges. Pointavatar: Deformable point-
based head avatars from videos. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2023. 2
[54] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao
Li. On the continuity of rotation representations in neural
networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2019. 5
[55] Yi Zhou, Chenglei Wu, Zimo Li, Chen Cao, Yuting Ye, Ja-
son Saragih, Hao Li, and Yaser Sheikh. Fully convolutional
mesh autoencoder using efﬁcient spatially varying kernels.
InNeurIPS , 2020. 2
[56] Gaspard Zoss, Derek Bradley, Pascal B ´erard, and Thabo
Beeler. An empirical rig for jaw animation. ACM TOG , 37
(4), 2018. 4,5,1
[57] Gaspard Zoss, Thabo Beeler, Markus Gross, and Derek
Bradley. Accurate markerless jaw tracking for facial per-
formance capture. ACM TOG , 38(4), 2019. 4
2229
