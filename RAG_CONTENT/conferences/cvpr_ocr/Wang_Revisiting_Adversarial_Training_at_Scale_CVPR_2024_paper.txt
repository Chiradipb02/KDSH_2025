Revisiting Adversarial Training at Scale
Zeyu Wang*Xianhang Li*Hongru Zhu Cihang Xie
∗equal contribution
UC Santa Cruz
Abstract
The machine learning community has witnessed a dras-
tic change in the training pipeline, pivoted by those “foun-dation models” with unprecedented scales. However , theﬁeld of adversarial training is lagging behind, predomi-nantly centered around small model sizes like ResNet-50,and tiny and low-resolution datasets like CIF AR-10. Tobridge this transformation gap, this paper provides a mod-ern re-examination with adversarial training, investigatingits potential beneﬁts when applied at scale. Additionally, weintroduce an efﬁcient and effective training strategy to en-
able adversarial training with giant models and web-scale
data at an affordable computing cost. We denote this newlyintroduced framework as AdvXL.
Empirical results demonstrate that AdvXL establishes
new state-of-the-art robust accuracy records under Au-toAttack on ImageNet-1K. F or example, by training onDataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to substantially surpass the previous records of l
∞-
,l2-, andl1-robust accuracy by margins of 11.4% ,14.2%
and 12.9% , respectively. This achievement posits AdvXL as
a pioneering approach, charting a new trajectory for theefﬁcient training of robust visual representations at signif-icantly larger scales. Our code is available at https:
//github.com/UCSC-VLAA/AdvXL .
1. Introduction
The landscape of machine learning, particularly deep learn-
ing, has witnessed a transformative shift with the advent oflarge-scale models and datasets. This paradigmatic shift,exempliﬁed by the inception of “foundation models” such
as Large Language Models (LLMs) [ 6,14,41,55,56], has
redeﬁned the boundaries of what is achievable in various
domains of artiﬁcial intelligence. Excitingly, parallel devel-opments have also been observed in computer vision — re-
cent advancements in scaling datasets and model sizes havemirrored the feasibility of “LLM-like” scaling for buildingexceptionally strong visual recognition models [ 12,16,64].
	




		

(a) Scale comparison.
AdvXL
Prior Best+11.4% +14.2%
+12.9%
(b) Performance comparison.
Figure 1. Our AdvXL increases signiﬁcantly in terms of both
model size and data scale, which brings a substantial boost overprior best results of l
∞,l2, andl1robustness on ImageNet-1K,
even though our model is only trained to be l∞-robust.
However, amidst this evolution, adversarial training [ 19,
39] — a pivotal strategy aimed at securing model robustness
against adversarial attacks — has faced signiﬁcant scala-bility challenges in this foundation model era. Adversar-ial training, typically employed in small models such asResNet-50 [ 23] trained on small datasets like CIFAR-10
[28], involves repeatedly generating adversarial examples
through on-the-ﬂy attacks during the training process. Thisiterative and intensive procedure demands substantial com-putational resources, thus making it challenging to scale up.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
24675
Contrasting with these challenges, recent endeavors in
adversarial training have indeed shown intriguing glimpsesof promise from data scaling by incorporating 50 mil-lion additional images to sustain state-of-the-art robustnessrecords on CIFAR-10 [ 57]. Additionally, other adversar-
ial training works [ 34,52] attain impressive performance
with model scaling using larger models like Swin-L [ 35]
and ConvNeXt-L [ 37] on ImageNet-1K. These observa-
tions, coupled with the burgeoning success of foundationmodels, instigates a critical question: can the principles of
model and data scaling, already proven effective in vanillatraining, be transferable to adversarial training? Moreover,
how effectively does such scaling translate to robustness im-provement in adversarial training?
In response to these questions, we re-examine adver-
sarial training at a previously uncharted foundation-modelscale. In terms of model scaling, we increased the model
parameters from the previously largest 200M size to 1B;
for data scaling, we adversarially train models on vari-ous datasets spanning from the medium-size ImageNet-1Kwith around 1M images to the web-scale dataset compris-ing more than 1Bimages. Additionally, to make the scal-
ing of adversarial training computationally affordable, weintroduce an efﬁcient approach with a straightforward two-stage training schedule, i.e., ﬁrst lightweight pre-training,
then intensive ﬁne-tuning. We name this efﬁcient and scal-able adversarial training framework as AdvXL.
Collectively, extensive experiments showcase that these
scaling endeavors successfully result in substantial im-provements over the previous state-of-the-art methods onadversarial robustness. For example, by training a one-billion parameter model on a one-billion image dataset,we establish a new state-of-the-art record for l
∞-robust
accuracy of 71.0% under AutoAttack on ImageNet-1K,marking a substantial enhancement in model robustness.Notably, AdvXL demonstrates exceptional generalizabilitywhen tested against unseen attacks, improving upon the pre-vious best l
2- andl1-robust accuracy of models trained to
bel∞-robust by margins of ∼14% and ∼13%, respectively.
These results underscore the pivotal role of (signiﬁcantly)scaled adversarial training in enhancing model robustnessagainst diverse adversarial threats.
2. Related Work
2.1. Adversarial Training
Adversarial training has emerged as a pivotal defense mech-anism against adversarial attacks in machine learning. Ini-
tially introduced by Goodfellow et al .[19], this method-
ology involves training models on crafted adversarial ex-amples designed to provoke model misclassiﬁcation. Sub-sequent studies have extended this foundation, examin-ing facets such as the impact of batch size, learning rate,data augmentation, and training duration on model robust-
ness, predominantly on smaller datasets like CIFAR-10[7,20,25,33,40,42]. Other research efforts have explored
deeper nuances of adversarial training recipes tailored forImageNet-1K [ 3,11,49,52,60–62]. Recent works also
investigate the robustness of novel network designs like Vi-sion Transformer (ViT) [ 9,17,21,52]. In particular, Singh
et al.[52] achieve the best generalized robustness by en-
hancing ViT and ConvNeXT with Convolutional Stem.
Despite its effectiveness, adversarial training is notori-
ously resource-intensive, limiting its scalability. To addressthis challenge, researchers have pursued more resource-efﬁcient adversarial training methodologies. Examples in-clude Free Adversarial Training [ 51] and Fast Adversar-
ial Training [ 58], both aimed at reducing training costs
while preserving model robustness. However, these ap-proaches have predominantly focused on smaller networksand datasets, leaving a noticeable gap concerning large-scale models. In this work, we aim to signiﬁcantly ex-pand the horizons of scaling adversarial training to unprece-dented levels of efﬁciency and effectiveness.
2.2. Scaling Vision Foundation Models
Parallel to large-scale language models, exempliﬁed by in-novations like GPT series [ 41], similar efforts have been
made for vision models, particularly with the scaling ofViTs [ 12,16,64]. Liu et al .[36] effectively trained the
SwinV2-G model, housing an astounding 3B parameters,by employing residual-post-norm and scaled cosine atten-tion. Similarly, Dehghani et al.[12] have shown substan-
tial performance enhancements by scaling ViTs to 22B pa-rameters, mirroring the scaling trends witnessed in languagemodels.
Despite the burgeoning scaling efforts in vision founda-
tion models, the exploration of adversarial training has tra-ditionally been limited to small or base model sizes. Recentscaling effort has led to noteworthy performance improve-ments, evidenced by the achievements on RobustBench [ 9]
with larger models like Swin-L and ConvNeXt-L [ 34,52].
Diverging from these antecedent initiatives, our work ex-
plores adversarial training at an even much larger scale, up
to the training of a one-billion-parameter model on one-billion samples, thereby pioneering the frontiers of adver-sarial training into uncharted territory.
3. AdvXL
In this section, we introduce AdvXL, a novel trainingframework designed for adversarially robust visual repre-sentation learning at scale. We ﬁrst revisit the fundamen-tal concept of adversarial attacks and adversarial trainingin Sec. 3.1. Following this, in Sec. 3.2, we present a two-
stage efﬁcient adversarial training pipeline characterized by
a coarse-to-ﬁne, weak-to-strong approach. In Sec. 3.3,w e
24676
showcase how to leverage CLIP [ 47] text encoder as a tool
for enabling us to learn with web-crawled images, wherea precise label is usually missing but with a correspondingtext description, for scaled adversarial training.
3.1. Adversarial Training
Adversarial examples are uniquely crafted inputs that, de-spite their visual similarity to authentic samples withinspeciﬁc norm constraints, are engineered to deceive ma-chine learning models into producing inaccurate predic-tions. These examples play a crucial role in assessing therobustness of a model in scenarios where malicious manip-ulations may occur.
Adversarial Training is central to fortifying a model
against such adversarial inputs. This technique involves astrategic training process designed to enhance the model’srobustness to adversarial attacks. The mathematical foun-
dation of AT is encapsulated as an optimization problem:
min
θ/summationdisplay
(xi,yi)∈Dmax
δ:/bardblδ/bardblp≤/epsilon1pL(fθ(xi+δ),yi), (1)
whereθrepresents the parameters for a network fθ. The ob-
jective is to train the network fθsuch that it maintains con-
sistent predictions under adversarial perturbations δ,i.e.,
within an lp-ball of radius /epsilon1pcentered around each input
samplexi.
Adversarial Training has proven highly effective to safe-
guard models against adversarial threats [ 5,19,53]. In
our approach, we adopt the widely recognized PGD-basedAdversarial Training (PGD-AT) method for the inner max-imization problem, renowned for its robust performanceand computational efﬁciency. For the outer minimizationproblem, we typically employ optimization algorithms likeStochastic Gradient Descent or AdamW [ 38], using cross-
entropy as the loss function L.
3.2. Two-stage Training
Our adversarial training framework hinges on a two-stage
process: a lightweight pre-training stage and an intensiveﬁne-tuning stage. During the pre-training stage, the model
is trained with inputs at reduced token length and weakerattacks, spanning a relatively extended duration. Then, dur-ing the subsequent ﬁne-tuning stage, the model is trained
with inputs at full resolution and stronger attacks, followinga comparatively shorter schedule. Compared to the vanillaone-stage adversarial training pipeline, this coarse-to-ﬁne
(w.r .t. input), weak-to-strong ( w.r .t. adversarial attacker),
two-stage training pipeline signiﬁcantly reduces the over-all training cost, rendering it computationally affordable forfurther scaling up.Coarse-to-ﬁne training. We ﬁrst explore various strategies
for image token reduction in the initial pre-training stage.Following [ 30,31], three distinct approaches are investi-
gated:
•Random Masking . This method, as described in [ 24,32],
involves dividing an image into non-overlapping patches
(e.g., 16 ×16), subsequently masking a random propor-
tion of these patches ( e.g., 75%). The model only pro-
cesses the visible patches, reducing the computationalcost by 50% or 75%, depending on the masking ratio.
•Block Masking . Inspired by [ 4], this approach retains
tokens from a consecutive large block within the image
while discarding others. This method leverages the com-mon placement of objects in the central regions of im-ages, potentially preserving semantic meaningful tokenswhile signiﬁcantly reducing the computational cost fromlengthy inputs.
•Resizing . Image resizing is another method for reducing
the image token length. Compared to masking, resizingretains more image information, especially high-level se-mantics. For instance, resizing an image to 112×112
is computationally akin to applying a 75% masking ratioto an image resized to 224×224. In our approach, we
choose anti-aliasing bilinear interpolation to better pre-
serve the image quality.
A visual comparison illustrating these image token reduc-
tion strategies is presented in Fig. 2. These strategies are
evaluated to discern their efﬁcacy in achieving training ac-celeration while retaining critical image semantics.
Weak-to-strong training. Another critical factor for accel-
erating adversarial training involves managing the numberof gradient steps used to craft adversarial samples. Gen-erally speaking, increasing the number of gradient steps re-sults in stronger attacks and enhances adversarial robustnessbut inevitably inﬂates computational costs. It has been re-
ported that forming a robust network with adversarial train-
ing can take signiﬁcantly longer, ranging from 3 to 30 timesmore than building a non-robust equivalent [ 51]. As a re-
sult, previous studies [ 44,51,59] have proposed strategies
like recycling gradient information or employing a smallgenerator network to mitigate the signiﬁcant computationalburden in adversarial training.
Our exploration reveals that applying a small number
of PGD steps ( e.g., PGD-1) during the pre-training stage
and subsequently increasing these steps during the ﬁne-tuning phase ( e.g., PGD-3) sufﬁciently secure strong robust-
ness, i.e., this method proves effective compared to initiat-
ing training with strong attacks. Importantly, this approach
contributes a notable additional speedup, enhancing the efﬁ-ciency gained from the coarse-to-ﬁne training pipeline ( e.g.,
up to 2×), as solving the inner optimization of adversarial
training often requires optimization with multiple iterationsand is extremely time-consuming.
24677
(a) Original (b) Random Masking (c) Block Masking (d) Resizin g
Figure 2. Illustration of different approaches to image token reduction.
Fine-tuning. Echoing ﬁndings from prior research [ 31,
32], we ﬁnd that further adversarially training our model
with full-resolution inputs and stronger attacks for a shortschedule yields considerable improvement and delivers amore favorable accuracy-to-time trade-off. Compared to the
pre-training stage, the ﬁne-tuning phase is notably shorter,
often reduced by one or two orders of magnitude. There-fore, even though each sample may entail a notably highernumber of image tokens ( e.g.,4×by switching back to full
image resolution) and require more gradient steps ( e.g.,2×
by switching back to the strong PGD-3 attacker) in this ﬁne-tuning phase, the overall computation does not increase sig-niﬁcantly.
3.3. CLIP Embedding for Web-Crawled Images
Previous works have leveraged the zero-shot generaliza-tion capability of pre-trained CLIP text encoder [ 47]t o
aid a range of downstream tasks, including object detec-tion [ 22,66,67] and segmentation [ 29,48] in an open-
vocabulary setting. Similarly, we hereby propose to em-ploy CLIP text encoder to extract classiﬁer weights whentraining on web-crawled large-scale datasets with open textdescriptions, such as LAION-400M [ 50] and DataComp-
1B [ 18]. Moreover, adversarial training on these gigan-
tic datasets enables the model to transcend pre-deﬁnedcategories and directly learn intricate class relationshipsthrough natural language supervision.
Speciﬁcally, we adopt the contrastive loss from [ 47,54],
formulated as:
L/parenleftBig
fI,fT,Ii,Ti/parenrightBig
=
−1
2n/summationdisplay
i⎛
⎝logexp/parenleftbig
hT
i·hI
i/τ/parenrightbig
/summationtext
jexp/parenleftBig
hT
i·hI
j/τ/parenrightBig+l o gexp/parenleftbig
hI
i·hT
i/τ/parenrightbig
/summationtext
jexp/parenleftBig
hI
i·hT
j/τ/parenrightBig⎞
⎠
(2)
wherenrepresents the batch size; τis a learnable tem-
perature parameter; hI
i=fI(Ii)//vextendsingle/vextendsinglefI(Ii)/vextendsingle/vextendsingleandhT
i=
fT(Ti)//vextendsingle/vextendsinglefT(Ti)/vextendsingle/vextendsingledenote the normalized projected fea-
tures of an image-text pair ( Ii,Ti). Note that we opt for
CLIPA-trained text encoder [ 30,31] as the initial fTweight
and keep it frozen during training. In this case, the adver-sarial training framework can be described as the followingoptimization problem,
min
θI/summationdisplay
(Ii,Ti)∈Dmax
δ:/bardblδ/bardblp≤/epsilon1pL/parenleftbig
fI,fT,Ii+δ,Ti/parenrightbig
,(3)
whereθIrepresents the parameters of the image encoder
fI. To elucidate this integration further, Fig. 3provides
a visual representation illustrating the incorporation of theCLIP encoder in adversarial training.
4. Experiment
In this section, we ﬁrst introduce the datasets used for ad-versarial training, along with the details of the training andevaluation setup in Sec. 4.1. In Sec. 4.2, we delve into the
ablation results, exploring key elements in our two-stagetraining pipeline. Furthermore, we investigate the perfor-
mance of adversarial training as the model, data, and sched-
ule scale synergistically in Sec. 4.3. Finally, we compare
and contrast the efﬁciency and efﬁcacy of AdvXL againstprior arts in Sec. 4.5.
4.1. Implementation
Dataset. We utilize four different datasets as the training
set for adversarial training, which are ImageNet-1K andImageNet-21K [ 13] — two well-curated labeled datasets
for supervised training, as well as LAION-400M [ 50] and
DataComp-1B [ 18] — two weakly labeled datasets with
natural language captions crawled from the Internet.
Speciﬁcally, ImageNet-1K comprises approximately
1.28M images from 1000 classes, while ImageNet-21Kconsists of around 13M images from 19k classes. LAION-400M is the ﬁrst publicly available web-scale dataset con-sisting of 400M image-text pairs. It is ﬁltered by CLIPand NSFW criterion, but is still relatively non-curated.DataComp-1B is a more recent dataset with about 1.3Bsamples ﬁltered from a candidate pool of 12.8B image-textpairs from Common Crawl, which has been recorded toyield superior performance for contrastive training.
To summarize, our choices of training datasets cover a
wide range of representative datasets, spanning from
∼1M
to∼1B samples, from well-curated labeled data to non-
curated web data. This diverse selection enables a compre-hensive investigation into the adversarial training concern-ing data scaling behaviors.
24678

	


 	






	

Figure 3. Illustration of leveraging CLIP embedding in adversarial training. The gray line denotes the adversarial example generation ﬂow.
Training. By default, our training initiates with a pre-
training stage utilizing an image size of 112×112 and
PGD-1 with a step size of 4/255. Subsequently, the model
undergoes a ﬁne-tuning stage employing an image size of
224×224and PGD-3 with a step size of 4/255. Our pri-
mary focus centers on ViT [ 15], renowned for its scalability
[15,24,32,47] yet relatively underexplored in the realm of
adversarial training. Note that the current best ViT modelon ImageNet-1K in RobustBench is only ViT-B/16 [ 9], in-
dicating plenty of room for further scaling.
On ImageNet-1K and ImageNet-21K, our recipe closely
follows prior works [ 24], which successfully trains ViTs on
ImageNet at scale from scratch. Speciﬁcally, we adopt theAdamW optimizer [ 38] with a short-term linear learning
rate warmup followed by a cosine learning rate schedule.Our data augmentation strategy integrates RandAug [ 10],
MixUp [ 65] and CutMix [ 63]. Additionally, we incorporate
stochastic depth [ 27] and weight decay for model regular-
ization. On web-scale datasets such as LAION-400M andDataComp-1B, our training recipe aligns with methodolo-gies outlined in [ 31].
The speciﬁcs of our training schedules are tailored to in-
dividual datasets, where the total number of training sam-ples serves as the primary metric, following a paradigm akinto CLIP training [ 31,32,47]. For instance, our default pre-
training schedule on ImageNet-1K spans a total of 256M
samples, which corresponds to 200 epochs of training.
Evaluation. In our analysis, we primarily use robust ac-
curacy under PGD-20 attack with a step size of 1/255 asthe principal metric. When comparing against other state-of-the-art methods, we follow RobustBench [ 9] and use the
robust accuracy evaluated on a subset of selected 5000 im-ages of the ImageNet-1K validation set under AutoAttack.AutoAttack is a standardized adversarial robustness bench-mark that consists of an ensemble of white- and black-box attacks, including APGD for cross-entropy and targetedDLR loss, FAB-attack [ 8] and the black-box Square At-
tack [ 2]. The attack radii are /epsilon1
∞= 4/255, /epsilon12= 2, and /epsilon11
=7 5f o rl∞,l2, andl1attacks, respectively.Approach Ratio/Size Compute Clean PGD-20
baseline 224/0% 1.0 × 75.5 54.5
Random Masking 224/50% 0.5 × 72.0 51.9
Random Masking 224/75% 0.25 × 67.3 46.5
Block Masking 224/50% 0.5 × 72.3 52.0
Block Masking 224/75% 0.25 × 70.6 49.3
Resizing 160/0% 0.5 × 74.7 53.9
Resizing 112/0% 0.25× 73.0 52.5
Resizing 96/0% 0.18 × 70.0 49.9
(a)Image token reduction .
Stage Step Step size Compute Clean PGD-20
Pre-training1 4/255 1.0× 73.0 52.5
2 3/255 1.5 × 72.1 52.6
3 3/255 2.0 × 71.8 52.5
Fine-tuning1 4/255 1.0 × 75.0 50.6
2 4/255 1.5 × 73.2 52.3
3 4/255 2.0× 73.0 52.5
(b)Attack strength .
Approach Ratio/Size Clean PGD-20
w/o Tuning160/0% 74.4 43.2
112/0% 68.5 39.3
160/0% 74.7 53.9w Tuning112/0% 73.0 52.5
(c)Fine-tuning .
Table 1. Ablating design choices with ViT-B/16 on ImageNet-1K.
We report clean and PGD-20 robust accuracy (%). If not speci-
ﬁed, the default setting is: 112×112 image size for pre-training,
224×224 image size for ﬁne-tuning; PGD-1 for pre-training, and
PGD-3 for ﬁne-tuning; 200 epochs for pre-training length, 20epochs for ﬁne-tuning . Default settings are marked in
gray . In ta-
ble (a) and (b), note that full-resolution ﬁne-tuning is included. In
table (b), when tuning the PGD step and step size in pre-training,we ﬁx them to be 3 and 4/255 respectively in ﬁne-tuning; Whentuning the PGD step and step size in ﬁne-tuning, we ﬁx them to be1 and 4/255 respectively in pre-training.
24679
4.2. Design Choices
We ﬁrst conduct an ablation study on the design choices of
AdvXL using ViT-B/16 on ImageNet-1K, with robust accu-racy under PGD-20 serving as the primary metric for adver-sarial robustness. We maintain the default baseline setting(see the caption of Tab. 1). Any alterations are conﬁned to
the speciﬁc factors under examination.
Token Reduction. Our investigation delves into three dis-
tinct strategies for reducing image token length: 1) randommasking, which randomly removes a portion of input to-kens; 2) block masking, which retains a large consecutiveblock of the input grid; 3) resizing, which preserves mosthigh-level semantic information. As shown in Tab. 1a, all
three methods exhibit substantial computational speedups.Notably, image resizing demonstrates superior performanceamong these strategies, presumably because it suffers theleast from loss of information. For instance, resizing the in-put image to 112×112leads to a 75% reduction in total
computation, with only a minor decrease of 2.5% in cleanaccuracy and 2.0% in PGD-20 robust accuracy. We select an
image size of 112×112 for pre-training as the default set-
ting due to its satisfactory balance between efﬁciency andperformance .
Attack Strength. Tab. 1bscrutinizes the impact of vary-
ing attack steps and step sizes during pre-training and ﬁne-tuning. Intriguingly, we observe that the number of PGDsteps for pre-training does not need to align with that for
ﬁne-tuning. For instance, adopting PGD-1 for pre-training
yields nearly equivalent robustness compared to PGD-3,while reducing the computation by 100%. This suggeststhat despite exposure to weaker attacks during pre-training(e.g., with PGD-1), a short-term but stronger adversarial
ﬁne-tuning ( e.g., with PGD-3) is sufﬁcient for the model to
secure strong robustness against adversarial attacks. There-
fore, we opt to use PGD-1 for pre-training and PGD-3 for
ﬁne-tuning in our default setting .
Fine-tuning. Tab. 1coutlines the impact of full resolution
ﬁne-tuning with stronger attacks for an extra 20 epochson the ImageNet-1K dataset. For 112×112PGD-1 pre-
training, a 224×224PGD-3 ﬁne-tuning elevates clean accu-
racy by 4.5% and PGD-20 robust accuracy by 13.2%. Thisﬁne-tuning phase substantially narrows the performancegap between reduced-length pre-training and full-lengthtraining, demanding only around 60% of the pre-trainingcomputational resources. Extending the pre-training sched-ule by the corresponding compute yields signiﬁcantly in-ferior results, highlighting the distinct advantage of ﬁne-tuning in achieving a superior performance-compute trade-off. Therefore, we consistently integrate a short-term ﬁne-
tuning stage post pre-training .4.3. Scaling Behavior
The acceleration outlined previously allows us to delve into
the performance implications of scaling AdvXL within anaffordable computational budget. In particular, we scruti-nize the scaling behavior along three principal axes below,in line with the approach established by Li et al.[32]:
•Model scaling . We substitute the ViT-B/16 model with
ViT-L/16 or ViT-H/14, which has
∼2×or∼4×number of
parameters, respectively.
•Data scaling . We substitute the training set of ImageNet-
1K with three much larger datasets, excessively expand-ing the total number of training samples up to more than
∼1B. These datasets include ImageNet-21K [ 13], a super-
set of ImageNet-1K; LAION-400M [ 50], and DataComp-
1B [18], two web-scale datasets.
•Schedule scaling . To delineate the inﬂuence of large
dataset size from that of extended training duration, weconduct training on ImageNet-21K with the same num-ber of seen samples as training on ImageNet-1K.
By meticulously traversing these three scaling axes, we
scrutinize their individual effects on AdvXL’s performance.The ﬁndings are detailed in Tab. 2, culminating in the fol-
lowing insights.
Model scaling. The evaluation of larger model sizes re-
veals discernible improvements in both clean accuracy andadversarial robustness. For instance, as shown in theﬁrst and the second rows of Tab. 2, ViT-L/16 surpasses
ViT-B/16 by 1.8% clean accuracy (from 73.0% to 74.8%)and 1.8% PGD-20-robust accuracy (from 52.5% to 54.7%)when training on ImageNet-1K. Interestingly, ViT-H/14,despite its superior clean accuracy and tripled computa-tional expense, demonstrates only a slightly better perfor-mance (0.2% higher PGD-20 robustness) compared to ViT-L/16 when training on ImageNet-1K, as shown in the third
row of Tab. 2. However, it notably surpasses ViT-L/16 by
a substantial margin (2.2% in PGD-20 robustness) whentraining on the larger ImageNet-21K dataset (as shown inthe ﬁfth and sixth rows of Tab. 2). This observation suggests
that larger models necessitate a larger training set to fullyleverage their potential. This ﬁnding aligns with conclu-
sions in prior studies [ 26], advocating for equivalent scaling
of model size and the volume of training tokens.
Schedule scaling. Initial experiments demonstrated that
extending the training schedule for ViT-L/16 on ImageNet-
1K yielded diminishing gains, possibly due to the compar-
atively “limited” scale of ImageNet-1K. However, resultsin Tab. 2shows that with larger and more diverse datasets,
training with additional samples yields non-trivial enhance-ments. Even with a 20 ×augmentation in the training sched-
ule using a one-billion sample dataset, such as training a
24680
Case Model Dataset Samples@Resolution Adv. StepsCompute
(1e10)Clean PGD-20
Baseline ViT-B/16 ImageNet-1K 256M@112 + 38.4M@224 1/3 0.5 73.0 52.5
model scaling ViT-L/16 ImageNet-1K 256M@112 + 38.4M@224 1/3 1.7 74.8 54.7
model scaling ViT-H/14 ImageNet-1K 256M@112 + 38.4M@224 2/3 5.7 76.5 54.9
model+data scaling ViT-L/16 + ImageNet-21K 256M@112 + 38.4M@224 1/3 1.7 75.8 56.1
model+data+schedule scaling ViT-L/16 + ImageNet-21K 789M @112 + 38.4M@224 1/3 3.4 77.2 58.3
model+data+schedule scaling ViT-H/14 + ImageNet-21K 789M @84 + 38.4M@224 2/3 8.1 79.0 60.5
model+data+schedule scaling ViT-L/16 + LAION-400M 2.56B @112 + 38.4M@224 1/3 8.8 80.5 62.2
model+data+schedule scaling ViT-H/14 + DataComp-1B 5.12B @84 + 38.4M@224 2/3 38.6 83.3 68.2
Table 2. Scaling behavior of AdvXL . For each model, we report its training set, the number of training samples it used and their resolution,
its PGD number of steps (in pre-training and ﬁne-tuning, respectively), the total training compute (in 1e10 GFLOPS), clean accuracy, andPGD-20-robust accuracy. “+” on the dataset means any additional dataset used during training besides ImageNet-1K. We scale along three
aspects: model, data, and scale, and observe consistent improvement in terms of both clean accuracy and robustness.
Dataset Model Clean PGD-20
ImageNet-1KViT-B/16 73.0 52.5
ConvNeXT-B 73.9 54.2
+LAION-400MViT-L/16 80.5 62.2
ConvNeXT-L 77.9 58.5
Table 3. Architecture comparison between ViT and ConvNeXT.
Text Encoder Clean PGD-20
Base 80.6 62.2
Large 80.5 62.2
Huge 80.6 62.3
Table 4. CLIP text encoder size .
ViT-H/14 model on DataComp-1B for 5.12B samples (the
last row of Tab. 2), there is not an observed saturation point.
Data scaling. Our AdvXL also exhibits favorable
outcomes with web-scale datasets LAION-400M andDataComp-1B. This trend could potentially pave the way
for adversarially trained models to rival foundational
models like CLIP [ 47] and Flamingo [ 1]. Notably, we
ﬁnd that data scaling itself is beneﬁcial, even without aprolonged training schedule. As shown in the second andthe fourth rows of Tab. 2, by substituting ImageNet-1K
with ImageNet-21K to adversarially train ViT-L/16, weobserve an uptick of 1.0% in clean accuracy and a 1.4%increase in robustness, notwithstanding identical trainingdurations. When coupled with our preliminary ﬁndingssuggesting diminished returns from extended schedules onImageNet-1K, we conclude that the richness and diversitybrought by data scaling stand as pivotal elements in thesuccess of adversarial training at scale.
4.4. Architecture Choice
We have also ablated alternative architectures such as Con-vNeXT [ 37] and Swin-Transformer [ 35], two leading back-
bones on RobustBench ImageNet leaderboard [ 34,52].
However, our attempts to train a Swin-Transformer withreduced-size inputs posed challenges as the feature size of
the last stage may even be smaller than the window size.For example, when employing common conﬁgurations likea patch size 4 ×4 and a window size 7 ×7, using a 112 ×112
input would lead to a ﬁnal stage feature size of 3 ×3. This
mismatch hindered effective training without architecturalmodiﬁcations, and thus, we primarily focus on comparingViT and ConvNeXT.
To ensure fair evaluation, we maintain consistency with
the same two-stage training recipe detailed in Sec. 4.3dur-
ing the performance comparison. The results, presented inTab. 3, demonstrate that ConvNeXT does outperform ViT
on a relatively small scale. However, this advantage di-minishes as the scale increases, leading us to keep ViT asthe default backbone for comparisons against other state-of-the-art models.
Also, we could adopt a larger pre-trained CLIP text en-
coder in contrastive learning, as it is frozen and introduceslittle computational overhead. Tab. 4shows the result of
training ViT-L/16 on LAION-400M with various CLIPA
text encoders. As can be seen, the performance is robust to a
wide range of CLIP text encoder choices. Thus, we simplyuse the same-scale text encoder to the image encoder ( i.e.a
ViT-L image encoder with a Large text encoder).
4.5. Comparison with SOTA Models
The comparison presented in Tab. 5evaluates our models
against prior works, focusing on l∞robustness at /epsilon1∞=
4/255. Following [ 52], we include l2robustness at /epsilon12=2
andl1robustness at /epsilon11= 75. Models listed exhibit over
80M parameters and are sorted based on their l∞robust-
ness under AutoAttack.
AdvXL emerges as the top performer owing to its un-
precedented scale in adversarial training. Our highly ef-ﬁcient two-stage training paradigm facilitates this scala-
24681
Model Dataset Samples@Resolution Pre-trained Adv. StepsParams
(M)Compute
(1e10)Source Clean l∞l2l1
RobArch-L
ImageNet-1K128M@224 3 104 1.3 [ 43] 73.5 48.9 39.5 14.7
ViT-B/16 384M@224 2 87 2.7 [ 49] 76.6 53.5 - -
ConvNeXT-B 384M@224 3 89 2.4 [ 34] 76.0 55.8 44.7 21.2
Swin-B 384M@224 3 88 2.4 [ 34] 76.2 56.2 47.9 23.9
ConvNeXt-B+ConvStem 320M@224 /check 3 89 2.0 [ 52] 75.2 56.3 49.4 23.6
ConvNeXt-L+ConvStem 128M@224 /check 3 198 1.8 [ 52] 77.0 57.7 47.0 22.2
ConvNeXt-L+ConvStem 128M@224(320 eval) /check 3 198 1.8 [ 52] 78.2 59.4 56.2 33.8
ConvNeXt-L 384M@224 3 198 5.3 [ 34] 78.0 58.5 - -
Swin-L 384M@224 3 197 5.3 [ 34] 78.9 59.6 - -
ViT-H/14 + DataComp-1B 5.12B@84 + 38.4M@224 + 6.4M@336 2/3 304 39.6 ours 83.9 69.8 69.8 46.0
ViT-g/14 + DataComp-1B 5.12B@84 + 38.4M@224 + 6.4M@336 2/3 1013 63.4 ours 83.9 71.0 70.4 46.7
Table 5. Comparison to SOTA l∞-robust models on ImageNet . For each model we report the training set it used, the number and
resolution of training samples it used, if it uses pre-trained weights or not, the number of PGD steps in AT (in pre-training and ﬁne-tuning,
respectively), the number of parameters of each model, the total training compute (in 1e10GFLOPS), its source, its clean accuracy and
l∞,l2,l1-robust accuracy with /epsilon1∞= 4/255,/epsilon12=2 ,/epsilon11= 75(AutoAttack). Note that for the model initialized with pre-trained weight, the
pre-training compute is not included. For unavailable metrics of those publicly unavailable models, we use “-” to ﬁll in the blank. “+” onthe dataset means any additional dataset used during training besides ImageNet-1K. Our AdvXL successfully secures new state-of-the-artrecords on all three robustness metrics thanks to its unprecedented model and data scale.
bility without incurring excessive computational expenses.
For instance, our largest ViT-g/14 model trained on theDataComp-1B dataset achieves outstanding results with acomputing budget of merely about 12×that of the previ-
ous best results from [ 34]. Despite this relatively modest
computational investment, our model outperforms them byan impressive 11.4% in terms of l
∞-robust accuracy under
AutoAttack. We would like to stress that training with fullresolution and strong attacks on 5.12B samples, without ourefﬁciency design, would incur
∼20×the computational cost
of our approach (equating to ∼250×the compute of the pre-
vious best results), rendering such an endeavor computa-tionally infeasible.
Even more noteworthy is the exceptional generalizabil-
ity showcased by our AdvXL ViT-g/14 models trained onthe web-scale DataComp-1B dataset, securing l
2-robust ac-
curacy of 70.4% and l1-robust accuracy of 46.7%. These
ﬁgures represent an absolute improvement of about 13%
over the best previous results. This observation indicatesthat scaling model, data, and schedule collectively not onlysigniﬁcantly enhances robustness against known attacks butalso fortiﬁes the model against unseen attacks during train-ing. Our ﬁndings on scaling adversarial training illuminatethe path towards the evolution of next-generation robust vi-sual models, potentially propelling the ﬁeld of adversarialtraining into the era of foundation models.
5. Discussion and Conclusion
Adversarial training has traditionally been conﬁned tosmall networks and datasets, predominantly ResNet-50 andCIFAR-10. Until recently, there have been few attemptsto train adversarially robust models on the medium-sizeImageNet-1K dataset. In this work, we break new ground
by scaling adversarial training to web-scale datasets con-
taining over 1B samples. Our AdvXL approach com-
prises two core components: 1) a coarse-to-ﬁne, weak-to-strong, two-stage training paradigm to mitigate the com-putational cost of scaling up; 2) the utilization of a pre-trained CLIP text encoder enabling training on web-scaledatasets. Through scaling along model, data, and sched-
ule dimensions, we successfully establish a new state-of-the-art record of l
∞-robust accuracy under AutoAttack, sur-
passing the previous best by a margin of ∼10%. Addition-
ally, training on those gigantic datasets demonstrates in-creased generalizability against unseen attacks during train-ing, aligning with observations from various foundation
models [ 1,6,45–47]. We envision our work as a stepping
stone for adversarial training to enter the era of foundationmodels, inspiring further large-scale adversarial training en-deavors.
Broad impact. Our method delivers over 5 ×speedup, sig-
niﬁcantly reducing wall-clock time for training models withhundreds of millions or even billions of parameters onbillion-scale datasets ( e.g., on the order of thousands of
TPU/GPU-days). AdvXL not only facilitates rapid proto-
typing and accelerated research cycles but also contributes
to substantial energy and carbon emissions savings, a criti-cal consideration in large-scale model training.
Acknowledge
This work is partially supported by a gift from Open Phi-lanthropy. We thank Center for AI Safety, TPU ResearchCloud (TRC) program, and Google Cloud Research Creditsprogram for supporting our computing needs.
24682
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. NeurIPS , 2022.
7,8
[2] Maksym Andriushchenko, Francesco Croce, Nicolas Flam-
marion, and Matthias Hein. Square attack: a query-efﬁcientblack-box adversarial attack via random search. In ECCV ,
2020. 5
[3] Yutong Bai, Jieru Mei, Alan L Yuille, and Cihang Xie. Are
transformers more robust than cnns? NeurIPS , 2021. 2
[4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit:
BERT pre-training of image transformers. In ICLR , 2022. 3
[5] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nel-
son, Nedim ˇSrndi ´c, Pavel Laskov, Giorgio Giacinto, and
Fabio Roli. Evasion attacks against machine learning at testtime. In ECML PKDD , 2013. 3
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-guage models are few-shot learners. NeurIPS , 2020. 1,8
[7] Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa
Amini, and Zhangyang Wang. Adversarial robustness: Fromself-supervised pre-training to ﬁne-tuning. In CVPR , 2020.
2
[8] Francesco Croce and Matthias Hein. Minimally distorted
adversarial examples with a fast adaptive boundary attack.InICML , 2020. 5
[9] Francesco Croce, Maksym Andriushchenko, Vikash Se-
hwag, Edoardo Debenedetti, Nicolas Flammarion, MungChiang, Prateek Mittal, and Matthias Hein. Robustbench: astandardized adversarial robustness benchmark. In NeurIPS ,
2021. 2,5
[10] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le.
Randaugment: Practical automated data augmentation witha reduced search space. In NeurIPS , 2020. 5
[11] Edoardo Debenedetti, Vikash Sehwag, and Prateek Mittal.
A light recipe to train robust vision transformers. In SaTML ,
2023. 2
[12] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr
Padlewski, Jonathan Heek, Justin Gilmer, Andreas PeterSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-mohsin, et al. Scaling vision transformers to 22 billion pa-
rameters. In ICML , 2023. 1,2
[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In CVPR , 2009. 4,6
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional trans-formers for language understanding. In NAACL , 2019. 1
[15] Al exey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. In ICLR , 2021. 5[16] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,
Xinggang Wang, Tiejun Huang, Xinlong Wang, and YueCao. Eva: Exploring the limits of masked visual representa-tion learning at scale. In CVPR , 2023. 1
,2
[17] Yonggan Fu, Shunyao Zhang, Shang Wu, Cheng Wan, and
Yingyan Lin. Patch-fool: Are vision transformers alwaysrobust against adversarial perturbations? In ICLR , 2022. 2
[18] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan
Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-acomp: In search of the next generation of multimodaldatasets. In NeurIPS , 2022. 4,6
[19] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. In ICLR ,
2015. 1,2,3
[20] Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann,
and Pushmeet Kohli. Uncovering the limits of adversarialtraining against norm-bounded adversarial examples. arXiv
preprint arXiv:2010.03593 , 2020. 2
[21] Jindong Gu, V olker Tresp, and Yao Qin. Are vision trans-
formers robust to patch perturbations? In ECCV , 2022. 2
[22] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
Open-vocabulary object detection via vision and languageknowledge distillation. In ICLR , 2022. 4
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 1
[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In CVPR , 2022. 3,5
[25] Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using
pre-training can improve model robustness and uncertainty.
InICML , 2019. 2
[26] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego
de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan
Clark, Tom Hennigan, Eric Noland, Katherine Millican,George van den Driessche, Bogdan Damoc, Aurelia Guy, Si-mon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals,Jack William Rae, and Laurent Sifre. An empirical analy-sis of compute-optimal large language model training. InNeurIPS , 2022. 6
[27] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q
Weinberger. Deep networks with stochastic depth. In ECCV ,
2016. 5
[28] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Technical report, 2009. 1
[29] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Rene Ranftl. Language-driven semantic seg-mentation. In ICLR , 2022. 4
[30] Xianhang Li, Zeyu Wang, and Cihang Xie. Clipa-v2: Scal-
ing clip training with 81.1 arXiv preprint arXiv:2306.15658 ,
2023. 3,4
[31] Xianhang Li, Zeyu Wang, and Cihang Xie. An inverse scal-
ing law for clip training. In NeurIPS , 2023. 3,4,5
[32] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-
hofer, and Kaiming He. Scaling language-image pre-training
via masking. In CVPR , 2023. 3,4,5,6
24683
[33] Zichao Li, Li Liu, Zeyu Wang, Yuyin Zhou, and Cihang Xie.
Bag of tricks for fgsm adversarial training. arXiv preprint
arXiv:2209.02684 , 2022. 2
[34] Chang Liu, Yinpeng Dong, Wenzhao Xiang, Xiao Yang,
Hang Su, Jun Zhu, Yuefeng Chen, Yuan He, Hui Xue, andShibao Zheng. A comprehensive study on robustness ofimage classiﬁcation models: Benchmarking and rethinking.
arXiv preprint arXiv:2302.14301 , 2023. 2,7,8
[35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. In
ICCV , 2021. 2,7
[36] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.Swin transformer v2: Scaling up capacity and resolution. In
CVPR , 2022. 2
[37] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the2020s. In CVPR , 2022. 2,7
[38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In ICLR , 2019. 3,5
[39] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learning
models resistant to adversarial attacks. In ICLR , 2018. 1
[40] Yichuan Mo, Dongxian Wu, Yifei Wang, Yiwen Guo, and
Yisen Wang. When adversarial training meets vision trans-formers: Recipes from training to architecture. NeurIPS ,
2022. 2
[41] OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774,
2023. 1,2
[42] Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun
Zhu. Bag of tricks for adversarial training. In ICLR , 2021. 2
[43] ShengYun Peng, Weilin Xu, Cory Cornelius, Kevin Li, Rahul
Duggal, Duen Horng Chau, and Jason Martin. Robarch:Designing robust architectures against adversarial attacks.arXiv preprint arXiv:2301.03110 , 2023. 8
[44] Omid Poursaeed, Isay Katsman, Bicheng Gao, and Serge Be-
longie. Generative adversarial perturbations. In CVPR , 2018.
3
[45] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. Improving language understanding by gen-erative pre-training. OpenAI blog , 2018. 8
[46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-pervised multitask learners. OpenAI blog , 2019.
[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In ICML , 2021. 3,4,5,7,8
[48] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong
Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu.Denseclip: Language-guided dense prediction with context-
aware prompting. In CVPR
, 2022. 4
[49] Sylvestre-Alvise Rebufﬁ, Francesco Croce, and Sven Gowal.
Revisiting adapters with adversarial training. In ICLR , 2023.
2,8[50] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, TheoCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:Open dataset of clip-ﬁltered 400 million image-text pairs.
arXiv preprint arXiv:2111.02114 , 2021. 4,6
[51] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi,
Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis,
Gavin Taylor, and Tom Goldstein. Adversarial training for
free! NeurIPS , 32, 2019. 2,3
[52] Naman D Singh, Francesco Croce, and Matthias Hein. Re-
visiting adversarial training for imagenet: Architectures,training and generalization across threat models. In NeurIPS ,
2023. 2,7,8
[53] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. In-triguing properties of neural networks. In ICLR , 2014. 3
[54] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-
trastive multiview coding. In ECCV , 2020. 4
[55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efﬁcient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 1
[56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.Llama 2: Open foundation and ﬁne-tuned chat models. arXiv
preprint arXiv:2307.09288 , 2023. 1
[57] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu,
and Shuicheng Yan. Better diffusion models further improveadversarial training. In ICML , 2023. 2
[58] Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than
free: Revisiting adversarial training. In ICLR , 2020. 2
[59] Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan
Liu, and Dawn Song. Generating adversarial examples withadversarial networks. IJCAI , 2018. 3
[60] Cihang Xie and Alan Yuille. Intriguing properties of adver-
sarial training at scale. In ICLR , 2020. 2
[61] Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L.
Yuille, and Kaiming He. Feature denoising for improvingadversarial robustness. In CVPR , 2019.
[62] Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, and
Quoc V Le. Smooth adversarial training. arXiv preprint
arXiv:2006.14536 , 2020. 2
[63] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-larization strategy to train strong classiﬁers with localizable
features. In ICCV , 2019. 5
[64] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-
cas Beyer. Scaling vision transformers. In
CVPR , 2022. 1,
2
[65] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. In ICLR , 2018. 5
[66] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan
Li, Noel Codella, Liunian Harold Li, Luowei Zhou, XiyangDai, Lu Yuan, Yin Li, et al. Regionclip: Region-basedlanguage-image pretraining. In CVPR , 2022. 4
24684
[67] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
Kr¨ahenb ¨uhl, and Ishan Misra. Detecting twenty-thousand
classes using image-level supervision. In ECCV , 2022. 4
24685
