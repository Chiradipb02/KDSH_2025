MaskPLAN: Masked Generative Layout Planning from Partial Input
Hang Zhang, Anton Savov, Benjamin Dillenburger
Digital Building Technologies, ETH Zurich
{hangzhang, asavov, dillenbb }@ethz.ch
Abstract
Layout planning, spanning from architecture to inte-
rior design, is a slow, iterative exploration of ill-defined
problems, adopting a ”I’ll know it when I see it” ap-
proach to potential solutions. Recent advances in gener-
ative models promise automating layout generation, yet of-
ten overlook the crucial role of user-guided iteration, can-
not generate full solutions from incomplete design ideas,
and do not learn for the inter-dependency of layout at-
tributes. To address these limitations, we propose Mask-
PLAN, a novel generative model based on Graph-structured
Dynamic Masked Autoencoders (GDMAE) featuring five
transformers generating a blend of graph-based and image-
based layout attributes. MaskPLAN lets users generate and
adjust layouts with partial attribute definitions, create al-
ternatives for preferences, and practice new composition-
driven or functionality-driven workflows. Through cross-
attribute learning and the user input as a global conditional
prior we ensure that design synthesis is calibrated at every
intermediate stage, maintaining its feasibility and practi-
cality. Extensive evaluations show MaskPLAN’s superior
performance over existing methods across multiple metrics.
1. Introduction
Layout planning is a ubiquitous task across various do-
mains, including architecture, urban, landscape, and inte-
rior design. It involves balancing functional needs, often
depicted as a bubble diagram, with their compositional ar-
rangement into a cohesive layout [26, 38]. The design of
floor plan layouts is a slow, iterative process exploration of
ill-defined problems, which under the motto ”I’ll know it
when I see it” focuses on embracing possible solutions in-
stead of solving for predefined criteria.
Recent achievements in generative models have shown
significant potential for automating the process of layout
generation [15, 23, 27, 35], specifically in the context of au-
tonomously generating floor plans based on high level func-
tional requirements [2, 7, 29, 31–33, 36, 40, 41, 53].
However, existing approaches predominantly employ
Figure 1. MaskPLAN allows users to influence layout gener-
ation with just the features they prioritize, using partial inputs
in a Graph-structured Dynamic Masked Autoencoder (GDMAE)
equipped with five attribute-specific generative transformers for
predicting layouts from incomplete design ideas.
holistic end-to-end architectures, while overlooking the crit-
ical role of user-guided iteration for the evolving under-
standing of design challenges. Furthermore, previous stud-
ies on user-guided layout generation [16, 21, 34, 40, 44, 50]
exhibit three key shortcomings: (1) inability to accept par-
tial inputs, (2) unsupported relevant attributes for user input,
and (3) failure to combine functional and compositional at-
tributes due to unlearned interdependencies.
Recently, masked autoencoders (MAE) [11, 17] enable
autoregressive image synthesis based on partial informa-
tion with high-fidelity results [5, 6, 17]. In this paper,
we propose MaskPLAN (Fig. 1), a novel, MAE-based user-
guided generative model for layout planning. MaskPLAN
addresses the three shortcomings in the state of the art with:
•Partial Input - we introduce a dynamic masking mecha-
nism in a generative layout planning workflow that takes
partial user input and autocompletes the remaining prop-
erties of the layout (Sec. 3.1). To date, MaskPLAN stands
as the first model to accept such a free range of input.
•Full set of learnable attributes - we demonstrate exhaus-
tive user-AI interactions by enabling users to define func-
tional and compositional attributes in layout design cus-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8964
tomization (Fig. 6).
•Cross-attribute learning - we incorporate the partial
user input as a global conditional prior, calibrating the de-
sign synthesis across every intermediate stage to preserve
the layout’s feasibility and practicality (Sec. 3.4).
•Feasible floor plans - we conduct extensive evaluations
and demonstrate that our approach outperforms existing
methods under various metrics (Sec. 4).
2. Related Work
In the context of our work on MaskPLAN, it is relevant
to take a comprehensive review of the following aspects:
generative floorplan layout synthesis, user-guided genera-
tive modeling and graph-structured masked autoencoders.
Generative Floorplan Layout Synthesis. Layout gen-
eration has recently been widely explored in data-driven
machine learning [15, 23, 27]. Related studies span var-
ious domains, including molecule generation [10, 13, 30],
indoor scene synthesis [47, 48], urban planning [8, 51],
and especially the floor plan generation [1, 7, 16, 21, 32–
34, 36, 40, 41, 44, 50, 53]. Significant prior research
has framed layout generation as a raster image synthesis
task. Promising approaches in such field involve leverag-
ing variational autoencoders (V AEs) [3, 24] and genera-
tive adversarial networks (GANs) [4, 14, 18, 27], which
have demonstrated impressive results in generating floor-
plans [16, 32, 33, 50]. However, pure image-based ap-
proaches are inherently constrained by their inability to cap-
ture spatial relations, which are typically represented as lay-
out graphs [2, 31, 32].
Several graph-structured generative models have shown
promising potential to address this, such as graph neural
networks (GNNs) [25, 39] in generative layout graph cre-
ation [10, 21, 28, 42]. Concurrently, emerging advance-
ments in deep learning are also leveraged for the automation
of urban planning [8] and floorplan generation [29, 44], as
well as text-driven house layout prediction [7, 53]. More re-
cent modern architectures such as vision transformers [12]
and diffusion models [19] have also yielded high fidelity
generative performance in floorplan layout synthesis with
graph-structured input [15, 34, 35, 40]. However, only
graph-based input does not enable the user to control the
compositional aspects of the spatial allocation in a layout.
In this work we propose a feature set that is a mix of image-
based and graph-based information.
User-guided Generative Modeling. Most layout gener-
ation approaches listed above employ an end-to-end archi-
tecture, constraining opportunities for iterative layout cus-
tomization. Only several prior studies have explored user-
guided floorplan generation. Typical approaches are to let
users define room types, locations, and bounding boxes
[16, 50] or to customize a layout graph [21, 32–34]. Pre-
dicted attributes are most often room bounding boxes whichare then post-processed to a layout. Notable exceptions are
the prediction of layout edges in [34] and room polygon out-
lines in [40]. However, these approaches have three main
shortcomings in supporting an iterative design workflow.
First, existing methods cannot generate full solutions
from incomplete design ideas. Early in the design process,
designers frequently work with uncertain details, leading to
initial designs based on incomplete information [9, 38]. A
more adaptable approach is needed for inputs like ”a 16 m2
bedroom next to a bathroom, and a balcony at given loca-
tion,” as illustrated in Fig. 1.
Second, existing layout customization methods often
lack flexibility, notably in adjusting room connectivity [16,
50] and modifying room sizes [16, 34, 40, 50]. An ideal
interactive model should enable users to alter all relevant
layout attributes such as room presence, location, size, ad-
jacency, and shape, addressing these limitations.
Third, current layout generation processes often over-
look the interconnectedness of layout attributes. Typi-
cally, these methods adopt a progressive, multi-stage ap-
proach [21, 33, 34, 44], sequentially addressing room types,
locations, and sizes for example [16, 50], yet fail to recog-
nize their mutual influence. This oversight prevents users
from ”freezing” specific attributes—such as a room’s di-
mensions or purpose—while allowing the model to adjust
its other attributes from earlier stages of the pipeline ac-
cordingly. Therefore, it’s crucial to implement comprehen-
sive mutual relations encoding across all stages of attribute
prediction to achieve practical layout designs.
To address these shortcomings, MaskPLAN is devel-
oped to enable access to all pivotal attributes during training
and customization, encode user partial input as global prior
knowledge, and simultaneously use it to calibrate the layout
synthesis at every intermediate stage.
Graph-structured Masked Autoencoders (GMAE).
MAEs have gained a significant traction in the development
of generative models that are structured on graphs, as ev-
idenced by several recent studies [20, 22, 30]. These in-
vestigations are based on the principles of subgraph [30]
or partial graph [52], wherein the original graph undergoes
stochastic masking on the graph level. This derived masked
information is then subjected to training processes aimed at
reconstructing the source graph [22, 37]. The architecture
of graph-structured MAE resonates strongly with the objec-
tives of this work and forms the basis for MaskPLAN.
3. Method
In MaskPLAN, layout generation is framed as predicting
unobserved layout attributes from a masked attributes ma-
trix, for which we propose a Graph-structured Dynamic
Masked Autoencoder (GDMAE) featuring five generators
that blend graph-based and image-based layout attributes.
8965
Figure 2. Layout representation in MaskPLAN. Room types Tand adjacency Aare passed as binary vectors, while C, areas S, and regions
Rare represented as images and embedded into lower-dimensional visual tokens using a pretrained ADLM before masking.
3.1. Problem Formulation
MaskPLAN aims to reconstruct the source layout attributes
Lfrom the masked matrix Uwhile considering the site con-
dition Bas an additional prior. Therefore, the primary ob-
jective of MaskPLAN is to learn the potential distributions
P(L|U, B), restoring all the unobserved attributes in the
layout. However, predicting all layout attributes simultane-
ously is challenging, so our method decodes them sequen-
tially, with each attribute influencing the next. The joint
probability distribution of the entire generation process is
decomposed as follows:
P(L|U, B) =P(R|S, G, U, B )
P(S|G, U, B )P(G|U, B)(1)
where P(G|U, B)refers to the prediction of the layout
graph G={T, C, A }composed of the predicted room types
T, locations C, and spatial relations A(Fig. 3). At the same
timeP(S|G, U, B )andP(R|S, G, U, B )denote the pro-
cedural forecasting of room areas Sand explicit room shape
regions Rrespectively.3.2. Comparisons with Existing Generators
As discussed in Sec. 2, several existing studies have in-
vestigated the generation of floorplans with user guidance.
RPLAN [50] employs a two-stage prediction strategy. Ini-
tially, the T′andC′are predicted together in a serial-
ized manner, defined as P(T′
i, C′
i|T′
j, C′
j, B), where j=
{0,1, ..., i−1}. Subsequently, the room walls are predicted
which in turn implicitly define the rooms’ bounding boxes.
Graph2Plan [21] retrieves the T′,C′,A′andS′from other
layouts that share a similar boundary condition B. After-
wards, the room bounding boxes estimated. iPLAN [16]
commences with the T′prediction, followed by C′and
bounding boxes simultaneously, in a serialized fashion.
In comparison, MaskPLAN is characterized by innova-
tions as follows: (1) it integrates user partial input to glob-
ally supervise the layout generation, defined as P(L|U, B)
(Eq. 1); (2) instead of representing rooms as mere bound-
ing boxes, it delineates them as explicit regions R′(Fig. 2),
lending higher accuracy to the geometrical representation;
(3) it ensures all pivotal attributes are available both for
training and customization, including T′,C′,A′,S′, and
R′. To our knowledge, MaskPLAN stands as the first model
to integrate these advanced features.
8966
3.3. Layout Representation
We represent each floorplan layout as a combination of site
condition Band layout attributes L(See Fig. 2). The site
condition B∈R128×128×3is represented as a three-channel
image, consisting of the inside mask, boundary mask, and
front door mask. The layout attributes L={T, C, A, S, R }
are designed to capture all the essential geometrical and cat-
egorical features in the layout and are annotated as the fea-
ture matrix and adjacency matrix (Fig. 1). To deal with vari-
able count of rooms (constrained to a maximum of 8 from
the training dataset) we introduce a [Start] and an [End] to-
ken to define every attribute’s sequence making its length
equal to 10. Any non-existing values up to the count of 8
are zero-padded. In detail, the five attributes in Lare rep-
resented as: T- denoting a room’s type ( T∈Z10). We
consolidate room types (13 in RPLAN) down to 8: living
room, bathroom, closet, bedroom, kitchen, dining room,
and balcony; C- representing the room’s central position
by a square of 9×9×3pixels on a 4-channel image
(C∈R10×128×128×4);S- denoting room areas with a
respectively sized square at the center of a one-channel im-
age ( S∈R10×128×128);A- indicating the spatial relations
between rooms as binary matrix where 1 denotes rooms’
adjacency ( A∈Z10×8); and S- representing the shapes
of the rooms by the actual pixels the room occupies, du-
plicated in the initial three channels of a 4-channel image
(R∈R10×128×128×4). The site condition Bis consolidated
into the fourth channel in the images of CandR, where
the front door mask = 255, the boundary mask = 127, and
the inside mask is omitted. This channel only contains site
pixels, ensuring no conflicts between geometrical and site
pixels.
Training on multiple sequences of high-resolution im-
ages is a computationally intensive task. Therefore, we pre-
train an Attribute Discrete Latent Model (ADLM), which
uses VQ-V AE [45] to encode the image information into a
lower dimension as visual tokens. The ADLM encodes to
a latent embedding space d∈RK×V, where Kis the size
of discrete latent space and Vis the dimension of each la-
tent embedding vector. The encoded visual tokens are used
in the training of the masked generative autoencoder. See
further details in the supplementary.
3.4. Masked Generative Autoencoder
The core framework of MaskPLAN adapts the Graph-
structured Dynamic Masked AutoEncoder (GDMAE), with
transformers [12] as its foundational structure. Described
in Fig. 3, MaskPLAN consists of six components, includ-
ing the partial input encoder EUand five mutually related
generators GT,GC,GA,GS, andGR. The encoder EUmaps
the observed attributes Uto the latent representation z. As
shown in recent layout generation approaches the genera-
tion of a floorplan layout is positively enhanced if predi-cated on its graph-based representation. Therefore, the five
autoregressive decoding steps in MaskPLAN are split into
two larger modules: the generator GGto first predict the lay-
out graph G, and the generator GS,Rto predict the final lay-
outL. The generators consistently take the boundary con-
dition Band the partial input Uas conditional factors. Fur-
themore, a novel addition to the transformer generator sub-
architecture is adding cross-attention in the encoder from
the predicted attributes at each stage.
Dynamic Masking. Masking design is crucial in our
task, given that MaskPLAN is tailored to accommodate a
broad range of partial input. We observed optimal values
from 15% in BERT [11] to 75% in MAE [17]. However,
a static masking ratio is inadequate to accommodate partial
input spanning an unrestricted range. Given this challenge,
we experimented with multiple combinations of masking
schedules (Tab. 3), indicating that a dynamic uniform ran-
dom masking between 50 %- 100%achieves optimal per-
formance, while also adeptly meets the demand for exten-
sive adaptability in processing partial user inputs. As illus-
trated top right in Fig. 2, the masking is conducted after the
ADLM-encoding into visual tokens. During training, we
omit the [Start] token in the output to optimize resources
when computing the attention score. At the same time the
input omits the [End] token to avoid accidentally constrain-
ing the model on an arbitrary number of rooms. The input is
then subjected to random masking, uniformly to ensure the
masking behaviors are evenly distributed. Subsequently, the
masked and the rest tokens in sequence are filled with zeros.
Partial Input Encoder. We adopted the encoder ar-
chitecture from Vision Transformers (ViT) [12], using the
same hyperparameters as ViT-Base. Unlike MAE, our par-
tial input encoder EUis applied not simply to the unmasked
attributes, but to the entire sequence of partial input U. De-
spite the dynamic nature of the masked proportion, this em-
bedded vector retains a fixed dimension.
Generator Encoder conditioned with cross-attention.
We augment the partial input Uby observing all existing
priors in the encoder of each generator. As shown in Fig. 3,
the attribute generators are structured with this modified En-
coder, with the exception of GT, as it is the first stage pre-
diction. As a whole, each Generator Encoder takes Uas
input, and computes the cross-attention with the concatena-
tion of all former predicted sequences. Subsequently, the
augmented partial input latent vector is concatenated with
site condition Band the formerly predicted attributes plus
the tokens predicted so far in the current sequence.
Generator Decoder. MaskPLAN utilizes a decoder that
bears resemblance to the autoregressive transformer [46],
employing a procedural and iterative process of sequence-
to-sequence generation. Each Generator Decoder computes
the cross-attention with the hybrid conditions from the Gen-
erator Encoder and generates a distribution over probable
8967
Figure 3. The general framework of MaskPLAN. The partial input is encoded as a global prior, calibrating the design synthesis across five
attribute-specific generators.
values for the succeeding token in its current sequence.
All five layout attributes are represented as integer ma-
trices (See Sec. 3.3 and Fig. 2). Consequently, the objec-
tives of all generators closely mirror the generation tasks
observed in natural language processing, which involves
classifying the current token based on the vocabulary size.
In the stage of GT, the classification size corresponds to the
count of room types. The output of GAadopts a binary for-
mat where the value of 1 signifies adjacency and 0 indicates
non-adjacency. For GC,GS, andGR, which are all repre-
sented as visual tokens I, their classification size aligns with
the dimension of the latent embedding vector V, as defined
in the pretrained ADLM model. Our ablation study on this
hyperparameter identified value 64 as the optimal.
In the last stage, the pretrained ADLM decodes the pre-
dicted visual tokens I′to their corresponding images. For
details on this see the supplementary material.
3.5. Loss Function
To learn the geometric and semantic constraints of a floor-
plan layout, we have a classification and a reconstruction
training loss.
Classification Loss. As our generative transformer
model is formulated to classify probable token values, the
softmax function is applied to the final layer of each individ-
ual generator. Consequently, we first conduct the classifica-
tion loss Lcla, measuring all the trainable features in the lay-
out attributes, between ground truth {T, C, A, S, R }ϵ L
and prediction {T′, C′, A′, S′, R′}ϵ L′. This loss is
summed up as:Lcla=LGG+LGS,R
=5X
n(8X
t(LogP((xn)t|(xn)<t)))(2)
where LGGandLGS,Rdenotes the loss from our dual
generators, respectively. In detail, (xn)tis the predicted
token from the nth attribute in L={T, C, A, S, R }at time
t, andP((xn)t|(xn)<t)is the probability distribution over
the latent space. All of the individual attribute losses in
Eq. 2 are using categorical cross-entropy.
Reconstruction Loss. We additionally measure the loss
of image reconstruction Lrec, evaluated on the difference
between the ground truth image input and its corresponding
image prediction, in the pixel space. This loss is defined as:
Lrec=λ0LCimg+λ1LSimg+λ2LRimg
=3X
jλj(||zd(Ij)−Mj||2
2)(3)
whereLCimg,LCimgandLCimgdenote the combination
of L2 loss results, computed on three pairs of real and fake
images: CimgandC′
img,SimgandS′
img,RimgandR′
img.
In detail, zd(Ij), refers to the images reconstructed by the
pre-trained ADLM decoder from the predicted visual tokens
(Ij∈[Ic, Is, IR]) and Mj∈[Mc, Ms, Mr]denotes the list
of ground truth images in {C, S, R }. Based on experience,
we set λ0as 2,λ1as 1, and λ2as 2. For the loss function of
pre-trained ADLM see the supplementary.
8968
Figure 4. The qualitative comparisons on layout reconstruction reveal that the baselines often generate layouts with missing wanted rooms
or blocked rooms, demonstrated as red dashed lines. Our III delivers consistently good results and the close ground truth alignment of Our
IIhighlights the effectiveness of MaskPLAN’s partial input guided generation.
4. Results
We conduct experiments on the RPLAN dataset [50], which
is widely benchmarked in previous works [16, 21, 32, 33,
40, 50]. RPLAN consists of over 80’000 floor plan images
sampled from real-world residential layouts in Asia. The
training-validation-test split of the dataset is 80%–10%–
10%.
4.1. Metrics
We evaluate the generated layouts against ground truths and
baseline results using two primary metrics: (1) the Frechet
Inception Distance (FID) [18] computed on rendered im-
ages as fidimg, and (2) the Mean Squared Error (MSE) on
{T′, A′, S′}asmseT,mseA, and mseS. We did not use
metrics like Kullback–Leibler Divergence since probabilis-
tic distributions equate different layouts (e.g., 2 bedrooms,
1 bathroom vs. 4 bedrooms, 2 bathrooms). All generated
and ground truth images are standardized in format, scale,
and room type category. To compute the MSE, we vectorize
the predicted rendered image into the layout attribute vector
with a size of 1×8, corresponding to the counts of room
types (Fig. 2). Specifically, we parse these three vectors as
follows: (1) each TiinTvecdenotes the number of rooms
that belong to the ith room type; (2) in Avec, if a room un-
der the ith room type is adjacent to a room under the jth
room type, both AiandAjwill add value 1; (3) for Svec,
each Sirefers to the sum of real-world room sizes under
theith room type, derived from the pixel counts in image S
and scaled in a factor of (20/256)2(described in RPLAN
dataset).4.2. Baselines
We choose four recent studies as baselines: RPLAN [50],
HouseDiffusion [40], iPLAN [16] and Graph2plan [21]. It
is noteworthy that RPLAN, iPLAN, and Graph2plan need
Bfor inference, while Graph2plan requires the integration
ofT,C,S, and Ainputs, constituting the layout graph
G. On the other hand, HouseDiffusion demands TandA
as input conditions. For iPLAN we include two versions:
iPLAN is only provided B, while iPLAN∗is fed with B,T
andC. If necessary, we feed the models all relevant ground
truths for generation.
Given MaskPLAN’s ability to process inputs of varying
completeness across five attributes, we evaluate its perfor-
mance through three input variants. The first, termed Our
I, predicts the layout simply on the given boundary B. The
second variant, Our II , makes the prediction from boundary
Band 25 %random selected partial input. The third as Our
III, derives the layout on the input of site condition B, room
types T, room locations C, and room adjacency A, aligning
the input format with that of layout graph G.
4.3. Quantitative Evaluation
Table 1 shows that Our III outperforms all other models
across all metrics as it bypasses the first generator mod-
uleGGand relies on the ground truth for T,C, and A
as priors for GL.Our II reconstructs designs from only
25% partial input, yet surpasses most baseline metrics. De-
spite only using Bas input, Our I closely follows iPLAN
and Graph2Plan in performance. HouseDiffusion shows
strength in mseTandmseAbut falls short in other met-
rics, lacking the boundary conditioning B. iPLAN performs
8969
Method fidimgmseTmseAmseS
HouseDiffusion 61.724 0.01742 5.486 21.571
RPLAN 7.130 0.24375 13.814 39.264
Our I 4.182 0.28941 10.638 8.662
iPLAN 3.192 0.31722 24.192 11.407
Our II 1.741 0.00492 7.405 2.764
Graph2plan 1.290 0.0001 1 6.942 4.732
iPLAN* 0.241 0.00003 4.710 0.936
Our III 0.139 0.00001 1.947 0.442Table 1. FID scores on rendered images and MSE scores on vec-
torized layout attributes.
fidimgPartial Input 20% 40% 60% 80% 100%
2.314 1.123 0.931 0.417 0.593
Table 2. Generative performance when MaskPLAN is fed with
different ranges of partial input (randomly masked in a fixed ratio).
Ablation Setting
w/o procedural condition 23.103
w/o ADLM pretrained 11.891
VAE instead of VQ-V AE 39.272
w/o per-pixel loss 14.092
0%-100% masking 6.478
25%-100% masking 5.169
75%-100% masking 6.912
Ours best 4.182fidimg
Table 3. Ablation study on various components in our model ar-
chitectures. Quantitative evaluation is calculated on fidimg, from
MaskPLAN simply conditioned on the boundary B.
Figure 5. Generating alternatives. MaskPLAN can generate lay-
out options from the same partial input by varying the order of
attributes in the input matrices. The examples here share the same
partial input for the bedroom in the South-East, the balcony in the
North-West, and a 15m2 kitchen adjacent to the living room.
well in fidimgandmseTbut struggles with mseAdue to
not learning on adjacency A. iPLAN∗, receiving 40% par-
tial input, i.e. TandC, outdoes Our II but not Our III .
Graph2Plan does not learn the T,C,S, and Abut requires
them as input, which explains its very high mseTscore.
We further evaluate the generative performance of Mask-
PLAN with partial input from various fixed masking ratios
in Tab. 2. While the model is trained with dynamic maskingranging from 50 %- 100%, it could still perform layout pre-
diction well on partial input out of this range. Strikingly,
when fed with 100 %input (the complete ground truth),
MaskPLAN doesn’t exhibit any significant improvement in
generative performance over the optimal performance it de-
livers 80 %masking. This might be attributed to the model’s
adaptation during training, which has not seen often such
rich information. Notably, Our III which takes 60 %of the
full input but in essence the complete layout graph is better
than a randomly sampled 60% of input on all five attributes.
This showcases the effectiveness of the layout graph Gas a
strong prior for guiding the layout generation process.
4.4. Qualitative Evaluation
We qualitatively assess whether the models produce lay-
outs that meet the input requirements, and if those layouts
are coherent, feasible, and exhibit appropriate room sizes,
placement, and connectivity. In Fig. 4 we present gener-
ated results from the three variants of MaskPLAN as well
as iPlan, Rplan, Graph2Plan, all conditioned on identical
boundaries. The common occurring mistakes of missing or
blocked rooms are highlighted. In general, Our III deliv-
ers the highest layout quality when compared to all other
methods, which is expected as it receives more information
(layout graph G) as input. Layouts generated with Rplan
often lack rooms, while those generated with Graph2Plan
and iPlan suffer from overlapping and gaps in room place-
ment. In particular, iPlan is not equipped with room adja-
cency information, causing its outcomes to sometimes al-
ter the topology of the layout relations compared to the
ground truth. Graph2Plan maintains the wanted adjacency
between rooms but occasionally struggles to ensure acces-
sibility from all functional rooms to the living room (e.g.,
the bedroom is blocked by the bathroom). On the other
hand, Our I shows significant diversity in layout creation.
At the same time, most of the layouts produced with Our
II– given 25 %partial input – align very closely with the
ground truth, highlighting the effectiveness of our partial
input guided generation.
4.5. Ablation Studies
Our ablation studies (Tab. 3) conditioned solely on the site
boundary Bas in Our I and measure the corresponding ef-
fect in fidimgof different model architecture components.
(1)w/o procedural conditioning : each attribute prediction is
conditioned only on the partial input Uand not previously
predicted attributes as outlined in Eq. 2. It performs much
worse as it lacks the existing features to guide subsequent
predictions. (2) w/o pretrained ADLM : the ADLM model is
integrated into the generative model, which makes it more
challenging to converge and demonstrates the effectiveness
of pertaining the ADLM. (3) VAE instead of VQ-VAE : the
VQ-V AE in the ADLM is replaced with a simple V AE us-
8970
Figure 6. MaskPLAN enables iterative layout customization, a key part of real-world floorplan design, by allowing users to add, edit,
remove, or freeze layout attributes, exploring multiple design paths from vague ideas to complete layouts.
ing the same hyperparameters. This results in a notable de-
cline in accuracy and justifies the use of a quantized model.
(4)w/o per-pixel loss : when the model is trained without
the reconstruction loss ( Lrec), it struggles to reconstruct the
layout accurately due to the absence of spatial pixel infor-
mation. (5) masking designs : we tested various dynamic
masking designs, to determine that the masking ratio from
50%to 100 %(in Ours best) provides optimal accuracy. The
static masking ratio is not considered in the ablations, as
MaskPLAN is designed to support a free range of user par-
tial input. More ablation studies are in the supplementary.
4.6. User-gudied generation
MaskPLAN’s framework introduces novel features that al-
low users to generate and fine-tune layouts based on partial
definition of any of the five attributes, create alternatives for
the same preferences, and employ new design workflows
driven by composition or functionality.
Partial Input Generation. Fig. 6 shows how users
can customize layouts with MaskPLAN by providing par-
tial inputs and iteratively refining them. They can start
with incomplete designs, such as a large unspecified room,
bathroom at a specific location and a kitchen next to it.
Then users iteratively Add,Edit, and Remove room features
to explore several paths of layout customization. More-
over, users can Freeze the predicted attributes of satisfac-
tory rooms allowing for design iterations based on a blend
of fixed and variable inputs, a unique guidance capability
unmatched by current methods.
Generating alternatives. While technically one partial
input leads to one layout, MaskPLAN can generate alterna-tives by altering the order of the wanted attributes specified
in the masked input matrix. Fig. 5 shows layout variations
produced in this manner from the same partial input.
Single-attribute Guidance. MaskPLAN allows users to
control one of five generators at a time enabling new design
workflows through single-attribute guidance which include
(1) providing only a list of preferred room areas; (2) speci-
fying only room adjacencies; and (3) a list of locations for
unspecified rooms. See supplementary for synthetic results.
5. Conclusion
In this paper, we introduced MaskPLAN, a first of a kind
generative model for floorplan layout generation that specif-
ically addresses the challenge of design creation from par-
tial user input. MaskPLAN enables a diverse set of user-AI
interactions by incorporating all essential layout attributes
and outperforms the current state-of-the-art in quantitative
and qualitative metrics. While most recent generative ap-
proaches to floor plan generation focus on the functional
aspect [49], architects also seek compositional clarity by
employing layout typologies [38]. The novelty in Mask-
PLAN is in decoupling and learning the cross-influence of
programmatic and geometric attributes to enable both the
functional and the composition-driven dimensions of the it-
erative design process. Current limitations primarily arise
from the constraints of the training dataset, such as a limit
to 8 rooms and orthogonal wall arrangements. Future re-
search could expand the framework to include newer and
more diverse datasets, like SwissDwellings [43], and extend
its capabilities to multi-floor layout design.
8971
References
[1] Automated floorplan generation in architectural design: A
review of methods and applications. Automation in Con-
struction , 140:104385, 2022. 2
[2] Fan Bao, Dong-Ming Yan, Niloy J Mitra, and Peter Wonka.
Generating and exploring good building layouts. ACM
Transactions on Graphics (TOG) , 32(4):1–10, 2013. 1, 2
[3] Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and
Gang Hua. Cvae-gan: fine-grained image generation through
asymmetric training. In Proceedings of the IEEE inter-
national conference on computer vision , pages 2745–2754,
2017. 2
[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high fidelity natural image synthesis.
arXiv preprint arXiv:1809.11096 , 2018. 2
[5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T
Freeman. Maskgit: Masked generative image transformer.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 11315–11325, 2022.
1
[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In Proceedings of the 37th Interna-
tional Conference on Machine Learning , pages 1691–1703.
PMLR, 2020. 1
[7] Qi Chen, Qi Wu, Rui Tang, Yuhan Wang, Shuai Wang, and
Mingkui Tan. Intelligent home 3d: Automatic 3d-house de-
sign from linguistic descriptions only. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12625–12634, 2020. 1, 2
[8] Hang Chu, Daiqing Li, David Acuna, Amlan Kar, Maria
Shugrina, Xinkai Wei, Ming-Yu Liu, Antonio Torralba, and
Sanja Fidler. Neural turtle graphics for modeling city road
layouts. In Proceedings of the IEEE/CVF international con-
ference on computer vision , pages 4522–4530, 2019. 2
[9] Nigel Cross. Design thinking: Understanding how designers
think and work . Bloomsbury Publishing, 2023. 2
[10] Nicola De Cao and Thomas Kipf. Molgan: An implicit gen-
erative model for small molecular graphs. arXiv preprint
arXiv:1805.11973 , 2018. 2
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 1, 4
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2, 4
[13] Rafael G ´omez-Bombarelli, Jennifer N Wei, David Duve-
naud, Jos ´e Miguel Hern ´andez-Lobato, Benjam ´ın S´anchez-
Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre,
Timothy D Hirzel, Ryan P Adams, and Al ´an Aspuru-Guzik.
Automatic chemical design using a data-driven continuous
representation of molecules. ACS central science , 4(2):268–
276, 2018. 2[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Commu-
nications of the ACM , 63(11):139–144, 2020. 2
[15] Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry S
Davis, Vijay Mahadevan, and Abhinav Shrivastava. Layout-
transformer: Layout generation and completion with self-
attention. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 1004–1014, 2021. 1,
2
[16] Feixiang He, Yanlong Huang, and He Wang. iplan: inter-
active and procedural layout planning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7793–7802, 2022. 1, 2, 3, 6
[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16000–
16009, 2022. 1, 4
[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 2, 6
[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 2
[20] Zhenyu Hou, Xiao Liu, Yuxiao Dong, Chunjie Wang, Jie
Tang, et al. Graphmae: Self-supervised masked graph au-
toencoders. arXiv preprint arXiv:2205.10803 , 2022. 2
[21] Ruizhen Hu, Zeyu Huang, Yuhan Tang, Oliver Van Kaick,
Hao Zhang, and Hui Huang. Graph2plan: Learning floor-
plan generation from layout graphs. ACM Transactions on
Graphics (TOG) , 39(4):118–1, 2020. 1, 2, 3, 6
[22] Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang,
and Yizhou Sun. Gpt-gnn: Generative pre-training of graph
neural networks. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data
Mining , pages 1857–1867, 2020. 2
[23] Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Si-
gal, and Greg Mori. Layoutvae: Stochastic scene layout gen-
eration from a label set. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 9895–
9904, 2019. 1, 2
[24] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 2
[25] Thomas N Kipf and Max Welling. Semi-supervised classi-
fication with graph convolutional networks. arXiv preprint
arXiv:1609.02907 , 2016. 2
[26] Bryan Lawson. How Designers Think . Routledge, 2006. 1
[27] Jianan Li, Jimei Yang, Aaron Hertzmann, Jianming Zhang,
and Tingfa Xu. Layoutgan: Generating graphic layouts with
wireframe discriminators. arXiv preprint arXiv:1901.06767 ,
2019. 1, 2
[28] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will
Hamilton, David K Duvenaud, Raquel Urtasun, and Richard
8972
Zemel. Efficient graph generation with graph recurrent atten-
tion networks. Advances in neural information processing
systems , 32, 2019. 2
[29] Jiachen Liu, Yuan Xue, Jose Duarte, Krishnendra
Shekhawat, Zihan Zhou, and Xiaolei Huang. End-to-
end graph-constrained vectorized floorplan generation with
panoptic refinement. In Computer Vision–ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XV , pages 547–562. Springer, 2022. 1, 2
[30] Omar Mahmood, Elman Mansimov, Richard Bonneau, and
Kyunghyun Cho. Masked graph modeling for molecule gen-
eration. Nature communications , 12(1):3156, 2021. 2
[31] Paul Merrell, Eric Schkufza, and Vladlen Koltun. Computer-
generated residential building layouts. In ACM SIGGRAPH
Asia 2010 papers , pages 1–12. 2010. 1, 2
[32] Nelson Nauata, Kai-Hung Chang, Chin-Yi Cheng, Greg
Mori, and Yasutaka Furukawa. House-gan: Relational gener-
ative adversarial networks for graph-constrained house lay-
out generation. In Computer Vision–ECCV 2020: 16th Eu-
ropean Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part I 16 , pages 162–177. Springer, 2020. 2, 6
[33] Nelson Nauata, Sepidehsadat Hosseini, Kai-Hung Chang,
Hang Chu, Chin-Yi Cheng, and Yasutaka Furukawa. House-
gan++: Generative adversarial layout refinement network to-
wards intelligent computational agent for professional archi-
tects. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 13632–13641,
2021. 1, 2, 6
[34] Wamiq Para, Paul Guerrero, Tom Kelly, Leonidas J Guibas,
and Peter Wonka. Generative layout modeling using con-
straint graphs. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 6690–6700,
2021. 1, 2
[35] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten
Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregres-
sive transformers for indoor scene synthesis. Advances in
Neural Information Processing Systems , 34:12013–12026,
2021. 1, 2
[36] Chi-Han Peng, Yong-Liang Yang, and Peter Wonka. Com-
puting layouts with deformable templates. ACM Transac-
tions on Graphics (TOG) , 33(4):1–11, 2014. 1, 2
[37] Phillip E Pope, Soheil Kolouri, Mohammad Rostami,
Charles E Martin, and Heiko Hoffmann. Explainability
methods for graph convolutional neural networks. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 10772–10781, 2019. 2
[38] Roberto J Rengel. The interior plan: Concepts and exercises .
A&C Black, 2011. 1, 2, 8
[39] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Ha-
genbuchner, and Gabriele Monfardini. The graph neural net-
work model. IEEE transactions on neural networks , 20(1):
61–80, 2008. 2
[40] Mohammad Amin Shabani, Sepidehsadat Hosseini, and Ya-
sutaka Furukawa. Housediffusion: Vector floorplan gen-
eration via a diffusion model with discrete and continuous
denoising. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5466–
5475, 2023. 1, 2, 6[41] Feng Shi, Ranjith K Soman, Ji Han, and Jennifer K Whyte.
Addressing adjacency constraints in rectangular floor plans
using monte-carlo tree search. Automation in Construction ,
115:103187, 2020. 1, 2
[42] Martin Simonovsky and Nikos Komodakis. Graphvae:
Towards generation of small graphs using variational au-
toencoders. In Artificial Neural Networks and Machine
Learning–ICANN 2018: 27th International Conference on
Artificial Neural Networks, Rhodes, Greece, October 4-7,
2018, Proceedings, Part I 27 , pages 412–422. Springer,
2018. 2
[43] Matthias Standfest, Michael Franzen, Yvonne Schr ¨oder,
Luis Gonzalez Medina, Yarilo Villanueva Hernandez,
Jan Hendrik Buck, Yen-Ling Tan, Milena Niedzwiecka, and
Rachele Colmegna. Swiss Dwellings: A large dataset of
apartment models including aggregated geolocation-based
simulation results covering viewshed, natural light, traffic
noise, centrality and geometric analysis, 2022. 8
[44] Jiahui Sun, Wenming Wu, Ligang Liu, Wenjie Min, Gaofeng
Zhang, and Liping Zheng. Wallplan: synthesizing floorplans
by learning to generate wall graphs. ACM Transactions on
Graphics (TOG) , 41(4):1–14, 2022. 1, 2
[45] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. Advances in neural information pro-
cessing systems , 30, 2017. 4
[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 4
[47] Kai Wang, Manolis Savva, Angel X Chang, and Daniel
Ritchie. Deep convolutional priors for indoor scene syn-
thesis. ACM Transactions on Graphics (TOG) , 37(4):1–14,
2018. 2
[48] Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, An-
gel X Chang, and Daniel Ritchie. Planit: Planning and in-
stantiating indoor scenes with relation graph and spatial prior
networks. ACM Transactions on Graphics (TOG) , 38(4):1–
15, 2019. 2
[49] Ramon Elias Weber, Caitlin Mueller, and Christoph Rein-
hart. Automated floorplan generation in architectural design:
A review of methods and applications. Automation in Con-
struction , 140:104385, 2022. 8
[50] Wenming Wu, Xiao-Ming Fu, Rui Tang, Yuhan Wang, Yu-
Hao Qi, and Ligang Liu. Data-driven interior plan genera-
tion for residential buildings. ACM Transactions on Graph-
ics (TOG) , 38(6):1–12, 2019. 1, 2, 3, 6
[51] Linning Xu, Yuanbo Xiangli, Anyi Rao, Nanxuan Zhao,
Bo Dai, Ziwei Liu, and Dahua Lin. Blockplanner: City
block generation with vectorized graph representation. In
2021 IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 5057–5066, 2021. 2
[52] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik,
and Jure Leskovec. Gnnexplainer: Generating explanations
for graph neural networks. Advances in neural information
processing systems , 32, 2019. 2
[53] Hang Zhang. Text-to-form. In Proceedings of the 40th An-
nual Conference of the Association for Computer Aided De-
sign in Architecture , pages 238–247, 2020. 1, 2
8973
