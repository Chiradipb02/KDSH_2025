SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational
Autoencoder
Dihan Zheng1* Yihang Zou1* Xiaowen Zhang2Chenglong Bao1,3,4†
1Yau Mathematical Sciences Center, Tsinghua University, Beijing2Hisilicon, Shanghai
3Yanqi Lake Beijing Institute of Mathematical Sciences and Applications, Beijing
4State Key Laboratory of Membrane Biology, School of Life Sciences, Tsinghua University, Beijing
*Equal contribution†Corresponding author
{zhengdh19,zou-yh21 }@mails.tsinghua.edu.cn clbao@mail.tsinghua.edu.cn
Abstract
The data bottleneck has emerged as a fundamental chal-
lenge in learning based image restoration methods. Re-
searchers have attempted to generate synthesized training
data using paired or unpaired samples to address this chal-
lenge. This study proposes SeNM-VAE, a semi-supervised
noise modeling method that leverages both paired and un-
paired datasets to generate realistic degraded data. Our
approach is based on modeling the conditional distribu-
tion of degraded and clean images with a specially de-
signed graphical model. Under the variational inference
framework, we develop an objective function for handling
both paired and unpaired data. We employ our method to
generate paired training samples for real-world image de-
noising and super-resolution tasks. Our approach excels
in the quality of synthetic degraded images compared to
other unpaired and paired noise modeling methods. Fur-
thermore, our approach demonstrates remarkable perfor-
mance in downstream image restoration tasks, even with
limited paired data. With more paired data, our method
achieves the best performance on the SIDD dataset.
1. Introduction
Image restoration is a fundamental and essential problem
in image processing and computer vision, aiming to re-
store the underlying signal from its corrupted observa-
tion. Traditional methods employ the Maximum a Poste-
riori (MAP) framework, transforming the image restora-
tion problem into an optimization problem. In these ap-
proaches, the objective function comprises a data fidelity
term and a regularization term corresponding to the degra-
dation and prior models, respectively. Over the years, the
prior model has been extensively studied. Before the advent
of deep learning, researchers utilized hand-crafted priors,such as sparsity [45, 48], non-local similarity [6, 12], and
low-rankness [15, 19]. Recently, harnessing the power of
deep neural networks has enabled achieving more accurate
prior models through pre-trained generative models derived
from a plethora of unlabeled clean signals [28, 51].
Deep learning based methods have achieved remarkable
success in image restoration tasks, such as image denois-
ing [20, 65, 66] and super-resolution (SR) [13, 14, 40, 54].
These methods aim to learn an end-to-end mapping for
restoration using paired training data. Owing to the pow-
erful representational capabilities of deep neural networks,
these methods typically outperform traditional approaches.
However, their effectiveness is contingent upon the avail-
ability of high-quality paired training data.
Collecting training data poses its challenges. First, real-
world degradation is highly complex due to the intricate
camera image signal processing (ISP) pipeline [2, 18, 21],
rendering the simulation process challenging. Another ap-
proach entails manual collection, where clean and degraded
pairs are obtained through long and short exposure [5] or
using statistical methods [1, 46]. Nevertheless, these ap-
proaches inevitably suffer from the misalignment problem
between clean and degraded images [57], making the pro-
cess expensive and time-consuming.
We conclude that a key problem in real-world image
restoration is obtaining an accurate degradation model.
With the degradation model, one can tackle the restoration
problem either via a classical optimization based method
with a pre-defined prior model [9, 70] or a supervised learn-
ing based method with synthesized training data [61, 71].
Accordingly, we investigate a scenario where a limited
amount of paired data and a large amount of unpaired data
are available, referred to as a semi-supervised dataset. Our
approach involves learning the unknown degradation model
from this semi-supervised training dataset and synthesizing
more paired data using this degradation model. We then use
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25889
existing image restoration networks to learn a supervised
image restoration model from the synthesized data. To ob-
tain the degradation model, we design a graphical model
that characterizes the relationship between the noisy image
yand the clean image x. We introduce two latent vari-
ables, z, andzn, representing the image content and degra-
dation information, respectively. Furthermore, we assume
thatxis generated by z, andyis generated by zandzn.
Using the idea from V AE [30], we approximate the condi-
tional distribution p(y|x)with encoding and decoding pro-
cesses. To effectively utilize the semi-supervised dataset,
we employ a mixed inference model for q(z|x,y)to fur-
ther decompose the objective function for paired and un-
paired datasets. We apply our SeNM-V AE model to learn
real-world image degradation. Experimental results demon-
strate that the proposed SeNM-V AE model exhibits promis-
ing performance in noise modeling, achieving comparable
results to supervised learning methods trained with fully
paired data. Moreover, we achieve the best performance
by finetuning an existing denoising network on the SIDD
benchmark. Our main contributions are summarized as fol-
lows.
• Leveraging limited paired data and abundant unpaired
data, we propose SeNM-V AE to obtain an effective model
for simulating the degradation process. This is im-
portant for generating high-quality training samples for
real-world image restoration tasks when obtaining train-
ing samples is difficult. Using the variational inference
method, SeNM-V AE is based on a specially designed
graphical model and a hierarchical structure with multi-
layer latent variables.
• Experimental results on the real-world noise modeling
and downstream applications, such as image denoising
and SR, validate the advantages of the proposed SeNM-
V AE, and we achieve the best performance on the SIDD
benchmark.
2. Related work
Semi-supervised image restoration. To alleviate the chal-
lenges of acquiring paired data for image restoration tasks,
researchers have been investigating semi-supervised tech-
niques, including image dehazing [35], deraining [11, 27,
58, 60], and low-light image enhancement [41]. These ap-
proaches typically rely on image priors, such as Total Varia-
tion (TV) [48] and the dark channel prior [22], to formulate
a loss function for unlabeled datasets. Some recent works,
such as [60], have employed CycleGAN [72] loss for un-
paired datasets. However, these methods are often designed
heuristically and lack theoretical rigor. In contrast, our ap-
proach is based on a specially designed graphical model,
and the loss function is derived through variational infer-
ence, enhancing our method’s interpretability.
Deep degradation modeling. Due to the limitations of
Figure 1. a: generation process for (x,y)and the corresponding
inference model. b: degradation generation procedure.
Figure 2. Data flow of the proposed semi-supervised noise mod-
eling method that models three kinds of data: paired domain
(degraded-clean image pairs), source domain (only clean images),
and target domain (only degraded images).
Gaussian noise in capturing the signal-dependence of real-
world noise [46, 70], researchers are exploring data-driven
approaches that utilize either normalizing flow [2, 31] or
GAN [63] to generate realistic noisy images with paired
training data. Furthermore, some studies have employed
unpaired data to learn the unknown degradation process,
which mostly utilizes the GAN model and techniques such
as cycle-consistency [7, 37] proposed in [72] and domain
adversarial [16, 59]. Other unpaired degradation modeling
methods include those based on Flow and V AE [61, 71]. In
contrast, we propose a semi-supervised degradation model-
ing approach for real-world image restoration that leverages
both paired and unpaired datasets.
3. Our methodology
In this section, we present our semi-supervised noise mod-
eling method. Formally, our goal is to estimate the
conditional distribution p(y|x)with one paired dataset
{(xi,yi)}Np
i=1and two unpaired datasets {xi}Ns
i=1,{yi}Nt
i=1,
where Np,Ns, andNtdenote the number of paired, source,
and target training samples, respectively. Subsequently, we
can generate paired training samples using the data from the
source domain. In general, we can learn a conditional gen-
erative model to sample from p(y|x)with paired samples
{(xi,yi)}, whereas those conventional generative models
are incapable of utilizing unpaired samples. Furthermore,
ifNpis small, achieving an accurate generative model be-
comes challenging. In this work, we propose a model that
exploits the information of unpaired data with the assistance
of the provided paired samples.
3.1. Proposed model
To estimate the conditional density function p(y|x),
the traditional Maximal Likelihood (ML) estimation is
25890
Figure 3. The hierarchical structure and network architecture.
to maximize the conditional log-likelihood function:
max θEp(x,y)logpθ(y|x), where θdenotes the model pa-
rameter. Parameterizing p(y|x)in high-dimensional spaces
directly is usually difficult. Thus, we consider the latent
variable model. In particular, we assume there are two la-
tent variables, zandzn, which encode the image content
and degradation information, respectively. Since practical
degradation is signal-dependent, zandznmay be entan-
gled with each other in the latent space. To account for
this entanglement, we employ the factorization p(z,zn) =
p(z)p(zn|z), where we assume that znis generated from z.
Moreover, we assume xis generated by z, andyis gener-
ated by zandzn, see Figure 1a for the generative process
of(x,y). By introducing an inference model q(z,zn|x,y),
we can decompose the conditional log-likelihood as
logp(y|x) =Eq(z,zn|x,y)logp(y,z,zn|x)
q(z,zn|x,y)
+DKL(q(z,zn|x,y)∥p(z,zn|x,y)),(1)
where DKLdenotes the Kullback–Leibler (KL) divergence,
and the expectation term in (1) is called the conditional Ev-
idence Lower BOund (cELBO) [50]. Due to the intractabil-
ity of the original log-likelihood function, we choose to
maximize the cELBO for density estimation. From the
graphical model in Figure 1a, we have
p(y,z,zn|x) =p(z|x)p(zn|z)p(y|z,zn),
p(z,zn|x,y) =p(z|x,y)p(zn|y,z).(2)
To match the factorization of p(z,zn|x,y), we choose
q(z,zn|x,y) =q(z|x,y)q(zn|y,z), (3)
then the cELBO becomes
Eq(z|x,y)q(zn|y,z)logp(y|z,zn)−DKL(q(z|x,y)∥p(z|x))
−Eq(z|x,y)DKL(q(zn|y,z)∥p(zn|z)).
(4)Please refer to the Appendix for the detailed derivation
process. However, the inference model q(z|x,y)requires
paired training data, which hinders the decomposition of
the above cELBO to utilize unpaired data. One approach to
overcome this limitation and further decouple q(z|x,y)is to
use a mixture model [49]. Specifically, we define q(z|x,y)
as a linear combination of two mixture components q(z|x)
andq(z|y),i.e.,
q(z|x,y) =p1q(z|x) +p2q(z|y). (5)
where p1andp2are the mixture weights, and we choose
p1=p2= 0.5in our method. However, this formula-
tion leads to the absence of a closed-form solution for the
KL divergence between q(z|x,y)and the conditional prior
distribution p(z|x)in the cELBO. Fortunately, we have the
following proposition:
Proposition 1. Letq(z|x,y)be a mixture model of q(z|x)
andq(z|y), as described in (5), then:
DKL(q(z|x,y)∥p(z|x))≤p1DKL(q(z|x)∥p(z|x))
+p2DKL(q(z|y)∥p(z|x)).(6)
Moreover, suppose that q(z|x) = p(z|x)by sharing the
same neural network. Then:
DKL(q(z|x,y)∥p(z|x))≤p2DKL(q(z|y)∥q(z|x)).(7)
See the supplemental material file for the proof. Utiliz-
ing proposition 1, we obtain a lower bound for (4):
Eq(z|x,y)q(zn|y,z)logp(y|z,zn)−p2DKL(q(z|y)∥q(z|x))
−Eq(z|x,y)DKL(q(zn|y,z)∥p(zn|z)).
(8)
Furthermore, we introduce a reconstruction term,
Eq(z|x,y)logp(x|z), for the source domain data. This
reconstruction term serves two purposes. Firstly, it facili-
tates the utilization of the source domain data. Secondly,
25891
it aids in regularizing the inference model, q(z|x,y),
ensuring that the image content variable, z, encapsulates
all essential information from the clean image, x, such
that it can be faithfully reconstructed through p(x|z). The
objective function for our method is defined as:
Loss =Eq(z|x,y)DKL(q(zn|y,z)∥p(zn|z))
+λDKL(q(z|y)∥q(z|x))−Eq(z|x,y)logp(x|z)
−Eq(z|x,y)q(zn|y,z)logp(y|z,zn).(9)
In this formulation, we draw inspiration from the β-V AE
framework [24], where we introduce a weight parameter λ
in front of the term DKL(q(z|y)∥q(z|x))to effectively bal-
ance the two KL divergence terms in (9). Consequently, we
can decompose (9) as
Loss =Loss p+Loss s+Loss t, (10)
where
Loss p:=−p1Eq(z|x)q(zn|y,z)logp(y|z,zn)
+p1Eq(z|x)DKL(q(zn|y,z)∥p(zn|z))
−p2Eq(z|y)logp(x|z) +λDKL(q(z|y)∥q(z|x)),
Loss s:=−p1Eq(z|x)logp(x|z),
Loss t:=−p2Eq(z|y)q(zn|y,z)logp(y|z,zn)
+p2Eq(z|y)DKL(q(zn|y,z)∥p(zn|z)).
(11)
It is clear that Loss p, Loss s, and Loss tcorrespond to data
from the paired, source, and target domains, respectively.
Therefore, the loss function described in (9) can be com-
puted with both paired and unpaired datasets. The data flow
for each domain within our approach is depicted in Figure 2.
3.2. Model settings
Degradation generation. Upon completion of the train-
ing process, it becomes feasible to synthesize degraded data
from a clean input. Given a clean image x, we can gener-
ate the corresponding degraded image from p(y|x)using
ancestral sampling. This process involves first deriving the
image content latent variable zfrom q(z|x), followed by
sampling the degradation latent variable znfrom p(zn|z).
Subsequently, the corresponding degraded image yis gen-
erated from p(y|z,zn), as depicted in Figure 1b.
Hierarchical structure. To enhance the quality of our gen-
erative performance, we employ a hierarchical V AE archi-
tecture, as proposed in previous studies [10, 53]. Specifi-
cally, we assume that our latent variable zandznis com-
posed of Llayers:
z= (z1, . . . ,zL),zn= (z1
n, . . . ,zL
n). (12)
Using the chain rule, the probabilistic distribution in (9) canbe decomposed as
q(z|x) =LY
l=1q(zl|x,z>l), q(zn|y,z) =LY
l=1q(zl
n|y,z,z>l
n),
q(z|y) =LY
l=1q(z1|y,z>1), p(zn|z) =LY
l=1p(zl
n|z,z>l
n).
(13)
where z>l
n= (zl+1
n, . . . ,zL
n). Then, the KL divergences
in (9) can be factorized as:
DKL(q(zn|y,z)∥p(zn|z)) =DKL(q(zL
n|y,z)∥p(zL
n|z))
+L−1X
l=1Eq(z>l
n|y,z)
DKL(q(zl
n|y,z,z>l
n)∥p(zl
n|z,z>l
n))
,
DKL(q(z|y)∥q(z|x)) =DKL(q(zL|y)∥q(zL|x))
+L−1X
l=1Eq(z>l|y)
DKL(q(zl|y,z>l)∥q(zl|x,z>l))
,
where the conditional distributions q(zl
n|z>l
n,y,z),
p(zl
n|z,z>l
n),q(zl|y,z>l), and q(zl|x,z>l)are chosen to
be Gaussian distributions, allowing us to calculate the KL
divergence in a closed-form expression.
Model architecture. For the inference model q(z|x), we
choose
q(zl|x,z>l) =N(µl
q(al,bl), σl
q(al,bl)), l= 1, . . . , L,
(14)
where alandblare encoding and decoding features in l-th
layer, respectively, and µl
qandσl
qare networks that convert
(al,bl)to the parameters of a Gaussian distribution. The
encoding features {al}L
l=1are recursively obtained through
a1=f1
θ(x),al=fl
θ(al−1), l= 2, . . . , L, (15)
where fl
θrepresents the basic block in l-th layer. The de-
coding feature blcan be obtained through:
bL=0,bl−1=gl
θ(zl,bl), l= 2, . . . , L, (16)
where zlis sampled from N(µl
q(al,bl), σL
q(al,bl)), and
gl
θis the basic block in l-th decoding layer. We choose the
structure of q(z|y)to be the same as q(z|x).
For the inference model q(zn|y,z), we assume that the
degradation latent variable znis distributed as follows:
q(zl
n|y,z,z>l
n) =N(¯µl
q(al
n,bl
n),¯σl
q(al
n,bl
n)), l= 1, . . . , L,
(17)
where al
nandbl
nare encoding and decoding features, re-
spectively, and ¯µl
qand¯σl
qare Gaussian parameterization
networks in the l-th layer. The encoding feature al
nis com-
puted recursively as follows:
a1
n=f1
θ(y),al
n=fl
θ(al−1
n), l= 2, . . . , L, (18)
25892
and the decoding feature is obtained through
bL
n=zL,bl−1
n=zl−1+gl
θ(zl
n,bl
n), l= 2, . . . , L,
(19)
where zl
nis sampled from (17).
For the conditional prior distribution p(zn|z), we employ
the same architecture as in q(zn|y,z), and assume
p(zl
n|z,z>l
n) =N(¯µl
p(bl
n),¯σl
p(bl
n)), l= 1, . . . , L,
(20)
where the decoding feature bl
nis derived from (19), ¯µl
pand
¯σl
pare Gaussian parameterization networks.
In the case of the generative models, we choose
p(x|z) =N(g1
θ(z1,b1),I), p(y|z,zn) =N(g1
θ(z1
n,b1
n),I).
(21)
We adopt the Residual Dense Block [69] (RDB) as our
basic block for fl
θandgl
θ, see Figure 3.
Degradation level controllable generation. The degra-
dation generation model should be capable of producing
images with varying degradation levels, enabling the gen-
eration of images with different degradation levels using
a single clean input. Therefore, we leverage training data
from the paired domain to train a degradation level predic-
tion network and then utilize this network to estimate the
degradation level of images within the target domain. Dur-
ing training, we concatenate the latent variable znsampled
fromq(zn|y,z)with its corresponding degradation level. In
the generation stage, we concatenate the specified degrada-
tion level to znsampled from p(zn|z)to enable conditional
image generation.
4. Experiment and results
We first evaluate the performance of our model in real-
world noise modeling tasks and then validate the down-
stream performance on image denoising and SR tasks. In
particular, we utilize our model to learn the unknown degra-
dation process, and then we generate synthetic degraded im-
ages from the source domain to augment the original paired
training samples. Finally, we apply an off-the-shelf super-
vised learning network to derive a restoration model from
both the synthetic dataset and the original paired data.
4.1. Datasets
SIDD : The smartphone image denoising dataset (SIDD) [1]
offers a collection of 30,000 noisy images captured by
five representative smartphone cameras across ten diverse
scenes under varying lighting conditions, alongside their
corresponding ground truth images. Here, we utilize the
SIDD-Medium dataset, which includes 320 paired clean
and noisy images. For each image in the dataset, we ran-
domly crop 300 patches of size 256×256, yielding a to-
tal of 96,000 paired data. To establish a semi-superviseddataset, we randomly select 0.01% (10), 0.1% (96), and
1% (960) paired samples from the cropped SIDD-Medium
dataset, serving as the paired domain. These 96,000 paired
images are then divided into two subsets, each containing
48,000 paired images. We utilize the clean images from the
first subset as the source domain and the noisy images from
the second subset as the target domain, resulting in 48,000
unpaired clean and noisy images in each domain. The SIDD
dataset also includes validation and benchmark datasets,
each containing 1,280 image blocks of size 256×256.
DND : The Darmstadt Noise Dataset (DND) [46] comprises
a benchmark dataset containing 1,000 image blocks of size
512×512extracted from 50 real-world noisy images ob-
tained from four commercial cameras. We directly assess
models trained with the SIDD dataset on this benchmark.
AIM19 : Track 2 of the AIM 2019 real-world SR chal-
lenge [38] provides a dataset of unpaired degraded and
clean images. The degraded images are synthesized with
an unknown combination of noise and compression. The
challenge also provides a validation set of 100 paired im-
ages. To construct a semi-supervised dataset, we select the
first 10 paired images from the validation dataset to serve as
the paired domain and leverage the originally provided un-
paired dataset as source and target domains. Performance
evaluation is then conducted on the remaining 90 images
within the validation set.
NTIRE20 : Track 1 of NTIRE 2020 SR challenge [39] fol-
lows the same setting as the AIM19 dataset, inclusive of
an unpaired training set and 100 validation images. Fol-
lowing the methodology of AIM19, we establish the semi-
supervised dataset by amalgamating the original unpaired
training set with the initial 10 paired images from the vali-
dation set. The performance is evaluated on the remaining
90 images within the validation set.
4.2. Implementation details
We train all SeNM-V AE models for 300k iterations using
the Adam optimizer [29]. The initial learning rate is set
to10−4and halved at 150k, 225k, 270k, and 285k itera-
tions. The batch size is set to 8, consisting of randomly
cropped patches of size 64×64. Batches are sampled ran-
domly from the paired, source, and target domains with
equal probability. We apply random flips and rotations to
augment the data. The KL regularization parameter λis
set to 10−7. The number of hierarchical layers Lis 7.
Furthermore, we utilize the KL annealing method [4] for
DKL(q(zn|y,z)∥p(zn|z)). Specifically, we employ a lin-
ear annealing scheme in the first 10k iterations to prevent
posterior collapse. To enhance the generation capacity of
the V AE model, we incorporate the LPIPS [42, 68] loss and
GAN loss [33] to supplement the original L2 loss for noisy
image reconstruction. For the SIDD dataset, SeNM-V AE is
trained with 0.01% (10), 0.1% (96), and 1% (960) of paired
25893
Real noisyKLD=0.215 KLD=0.179 KLD=0.164
SeNM-V AE
10 PDKLD=0.266
SeNM-V AE
96 PDKLD=0.262
SeNM-V AE
960 PDKLD=0.171
SeNM-V AE
Full PDKLD=0.070NeCA-W
Full PDFlow-sRGB
Full PDDANet
Full PDFigure 4. Visual comparison of generated noisy images, ”PD” de-
notes paired data.
Refer ence
 CVF-SID29.71dB
AP-BSN32.25dB
Noisy19.01dB
SCPGabNet33.16dB
SDAP(S)(E)33.17dB
DRUNet
10 PD32.45dB
DRUNet
96 PD33.73dB
DRUNet
960 PD34.39dB
SeNM-V AE
10 PD34.25dB
SeNM-V AE
96 PD34.73dB
SeNM-V AE
960 PD35.15dB
Figure 5. Visual comparison of denoising performance.
34.20dB
 32.16dB
 28.98dB
 29.17dB
 37.07dB
34.42dB
 34.49dB
 34.58dB
 34.50dB
 34.93dB
Clean Uformer MAXIM Restormer NAFNet SeNM-V AE
Figure 6. Visual comparison of denoising performance.
data. In addition, we include all paired data from the SIDD
dataset to train our model, using only Loss pin (11) as our
objective function.
4.3. Noise synthesis
We first validate the performance of our method through
real-world noise modeling tasks on the SIDD dataset.
Compared methods. We compare our SeNM-V AE with
three unpaired noise modeling methods, namely C2N [25],
DeFlow [61], and LUD-V AE [71], as well as three fully
paired noise modeling methods, namely DANet [63], Flow-
sRGB [31], and NeCA-W [17].
Experiment settings and evaluation metrics. All meth-
ods are trained on the SIDD dataset. After training, we
apply the trained models to synthesize noisy images usingMethod # Paired Data FID↓KLD↓PSNR ↑
C2N [25]
033.97 0.169 34.23
DeFlow [61] 39.45 0.205 33.82
LUD-V AE [71] 35.31 0.108 34.91
SeNM-VAE0.01% (10) 17.25 0.036 36.73
0.1% (96) 16.76 0.027 36.92
1% (960) 15.10 0.020 37.28
DANet [63]
100%26.22 0.081 36.25
Flow-sRGB [31] 28.60 0.047 33.24
NeCA-W [17] 19.96 0.030 37.04
SeNM-VAE 13.79 0.011 38.29
Real noise 100% 0 0 38.34
Table 1. Comparison of noise quality on SIDD validation dataset.
DnCNN [65] is used as a downstream denoising model.
clean images from the SIDD validation set. This allows us
to compute the FID [23] score and the KL divergence be-
tween synthetic and real noisy images within the validation
set. Furthermore, using clean images from the SIDD train-
ing dataset, we generate noisy images to create synthesized
training sets. DnCNN [65] models are then trained on these
synthesized paired datasets. The performance of DnCNN
models is evaluated on the SIDD validation dataset. We
employ PSNR to evaluate denoising performance. Higher
PSNR values indicate that the noise models are closer to
the real noise model, signifying better noise quality.
Results. The results are shown in Table 1. Compared with
the unpaired noise modeling approaches, our SeNM-V AE
shows remarkable success across all three metrics, even
when trained with just 10 paired samples. In contrast to
fully paired noise modeling approaches, SeNM-V AE out-
performs the SOTA methodology, NeCA-W [17], utilizing
only 1% of the original SIDD dataset’s paired data. Fur-
thermore, when fully paired data is employed, our method
significantly surpasses other noise modeling methods on all
three metrics. The results demonstrate the effectiveness of
our approach in generating high-quality synthetic noisy im-
ages. The visual results are illustrated in Figure 4, demon-
strating that our method successfully captures the variance
change of real-world noise across different regions within
the image, particularly when using fully paired data.
4.4. Downstream denoising
One significant application of our method is to benefit
downstream denoising tasks. After training, we generate
synthetic paired data using clean images from the source
domain. Then, we employ DRUNet [67] as the downstream
denoising model and train it on both the generated synthetic
dataset and the data from the paired domain.
Compared methods. We compare our semi-supervised de-
noising method with direct training on the paired domain,
and several self-supervised denoising methods, namely
N2V [32], N2S [3], CVF-SID [43], AP-BSN + R3[34],
SCPGabNet [36], SDAP(S)(E) [44], and a fully supervised
25894
SIDD Validation SIDD Benchmark DND BenchmarkMethod # Paired DataPSNR ↑ SSIM↑ PSNR ↑ SSIM↑ PSNR ↑ SSIM↑
N2V [32]
029.35 0.6510 27.68 0.668 - -
N2S [3] 30.72 0.787 29.56 0.808 - -
CVF-SID [43] 34.17 0.872 34.71 0.917 36.50 0.924
AP-BSN + R3[34] 35.91 0.8815 35.97 0.925 38.09 0.937
SCPGabNet [36] 36.53 0.8860 36.53 0.925 38.11 0.939
SDAP(S)(E) [44] 37.55 0.8943 37.53 0.936 38.56 0.940
DRUNet0.01% (10)34.48 0.8658 34.45 0.909 34.37 0.904
SeNM-VAE 37.96 0.9107 37.93 0.949 38.47 0.946
DRUNet0.1% (96)37.68 0.9053 37.63 0.944 38.16 0.942
SeNM-VAE 38.91 0.9134 38.85 0.953 39.32 0.951
DRUNet1% (960)38.93 0.9150 38.89 0.954 38.95 0.949
SeNM-VAE 39.39 0.9176 39.34 0.956 39.47 0.953
DRUNet
100%39.55 0.9187 39.51 0.957 39.52 0.952
VDN [62] 39.29 0.9109 39.26 0.955 39.38 0.952
DeamNet [47] 39.40 0.9169 39.35 0.955 39.63 0.953
Table 2. Comparison of denoising performance on SIDD and DND datasets.
SIDD Validation SIDD BenchmarkMethodPSNR ↑SSIM↑PSNR ↑SSIM↑
Uformer [56] 39.89 0.960 39.74 0.958
MAXIM [52] 39.96 0.960 39.84 0.959
Restormer [64] 40.02 0.960 39.86 0.959
NAFNet [8] 40.30 0.961 40.15 0.960
SeNM-VAE 40.49 0.962 40.38 0.961
Table 3. Comparison of denoising performance on SIDD dataset.
SeNM-V AE is trained using the full SIDD dataset and utilized to
generate synthetic data for finetuning NAFNet.
Method PSNR ↑ SSIM↑ LPIPS ↓
FSSR [16] 20.97 0.5383 0.374
Impressionism [26] 21.93 0.6128 0.426
DASR [59] 21.05 0.5674 0.376
DeFlow [61] 21.43 0.6003 0.349
LUD-V AE [71] 22.25 0.6194 0.341
ESRGAN [54] 21.47 0.5748 0.353
SeNM-VAE 22.48 0.6343 0.333
Table 4. Comparison of SR performance on AIM19. ESRGAN
and SeNM-V AE are trained with 10 paired data.
Method PSNR ↑ SSIM↑ LPIPS ↓
FSSR [16] 21.01 0.4229 0.435
Impressionism [26] 25.24 0.6740 0.230
DASR [59] 22.98 0.5093 0.379
DeFlow [61] 24.95 0.6746 0.217
LUD-V AE [71] 25.78 0.7196 0.220
ESRGAN [54] 25.05 0.6707 0.246
SeNM-VAE 25.91 0.7222 0.216
Table 5. Comparison of SR performance on NTIRE20. ESRGAN
and SeNM-V AE are trained with 10 paired data.
trained DRUNet [67], VDN [62], and DeamNet [47].
Experiment settings and evaluation metrics. The denois-
ing models are trained for 300k iterations with Adam op-
timizer. The initial learning rate is 10−4and halved every
100k iterations. We evaluate the denoising performance ofall the denoising methods on the SIDD validation dataset,
the SIDD benchmark dataset, and the DND benchmark
dataset, and report PSNR and SSIM [55] for each dataset.
Results. The results are presented in Table 2. The table
indicates that our SeNM-V AE approach enhances perfor-
mance compared to the baseline models on the SIDD and
DND datasets. Additionally, our method improves upon the
results of self-supervised denoising methods on the SIDD
dataset, even with access to only 10 paired data samples.
When utilizing 1% paired data, our method yields results
comparable to the fully supervised trained DRUNet model.
As such, SeNM-V AE offers an effective strategy to nar-
row the performance gap between self-supervised and su-
pervised denoising methods. Figure 5 showcases visual re-
sults, where our approach achieves sharper edges and more
thorough noise removal compared to other methods.
4.5. Finetune denoising network
Another application of our method involves generating ad-
ditional training samples to finetune the denoising net-
work. We utilize our SeNM-V AE, trained with all paired
data from the SIDD dataset, to produce extra training data
from clean images in the SIDD dataset. We then finetune
a pre-trained denoising network, NAFNet [8]. These re-
sults are presented in Table 3. The table illustrates that
our method can significantly enhance the denoising per-
formance of NAFNet, leading to superior performance on
the SIDD dataset. Visual results are shown in Figure 6,
where our method notably yields sharper edges and pre-
serves more image information.
4.6. Downstream SR
We apply our method to simulate the degradation process
in real-world SR tasks. Assume the degradation process is
y=D(x)+n, where both the downsample operator Dand
noisenremain unknown. We substitute Dwith the Bicubic
25895
Total Loss FID↓ KLD↓ PSNR ↑
w/o recon. x 24.26 0.070 35.60
w/ recon. x 17.25 0.036 36.73
Table 6. Ablation on the reconstruction loss for the clean image x
on SIDD validation dataset with 10 paired data.
downsample operator Band incorporate the disparity be-
tweenDandBinto the noise term, yielding y=B(x)+n′,
where n′=n+D(x)− B(x). After training, we generate
synthetic low-resolution data and employ ESRGAN [54] to
train a restoration model.
Compared methods. We compare our semi-supervised SR
method with five unpaired degradation modeling methods,
namely FSSR [16], Impressionism [26], DASR [59], De-
Flow [61], LUD-V AE [71], and a supervised trained ESR-
GAN [54].
Experiment settings and evaluation metrics. All the SR
models are trained for 60k iterations with Adam optimizer.
We evaluate each SR method on the final 90 images from
the AIM19 and NTIRE20 validation datasets. Performance
metrics, including PSNR, SSIM, and LPIPS [68], are re-
ported for both datasets.
Results. The results are detailed in Table 4 and Table 5. Our
method surpasses both the supervised ESRGAN model and
the unpaired degradation modeling methods, which high-
lights the effectiveness of our model in leveraging a limited
amount of paired data alongside unpaired data to enhance
the generation of high-quality training samples.
4.7. Ablation study and discussions
Ablation on reconstruction loss for clean image x.We
perform an ablation experiment to the additional loss
−Eq(z|x,y)logp(x|z)in (9) on the SIDD dataset with 10
paired samples, and the results are shown in Table 6. The
table demonstrates an improvement in noise modeling per-
formance with the inclusion of this reconstruction loss. One
reason is that the reconstruction loss acts as Loss sfor source
domain data in (11), allowing effective utilization of source
domain data. Moreover, the reconstruction loss enables the
transformation from noisy image yto the clean image x,
aiding in the disentanglement of image information zfrom
noisy information zn, which further enhances the model’s
ability to learn the noise distribution.
Analysis on the training domains. To demonstrate the ef-
ficacy of our model in utilizing information from unpaired
data domains, we conduct experiments using different num-
bers of unpaired data on the SIDD dataset with 10 paired
data. Specifically, we randomly select 0, 96, and 960 im-
ages for the source and target domains to illustrate our
model’s capacity to exploit information from unpaired data
domains. Then, we conduct ablation studies using only two
domains to understand the roles of the source and target do-# Data
Paired Source TargetFID↓KLD↓PSNR ↑
10 0 0 19.79 0.073 35.97
10 96 96 17.52 0.054 36.67
10 960 960 17.54 0.036 36.78
10 100% 0 22.98 0.043 36.23
10 0 100% 19.11 0.050 36.24
10 100% 100% 17.25 0.036 36.73
Table 7. Comparison of noise quality on SIDD validation dataset
with different numbers of unpaired samples.
PSNR FID KLD 
Figure 7. Parameter analysis of KL weight λon SIDD validation
dataset with 10 paired data.
mains in the training process. The results are summarized
in Table 7. From the table, we find that the noise modeling
performance improves with increasing unpaired data, and
incorporating all three domains yields the best noise model-
ing results. The findings indicate our model’s effectiveness
in leveraging information from unpaired datasets.
Analysis on KL weight λ.We perform a parameter anal-
ysis for the KL weight coefficient λ. The results are pre-
sented in Figure 7. The figure shows that setting λin the
range of 10−6to10−7leads to better noise modeling re-
sults than other cases.
5. Conclusion
This paper presents SeNM-V AE, a semi-supervised noise
modeling approach based on deep variational inference.
The proposed method employs a latent variable model to
capture the conditional distribution between corrupted and
clean images, allowing for the transformation from a clean
image to its corrupted counterpart. Our approach decom-
poses the objective function, enabling training with both
paired and unpaired datasets. We apply SeNM-V AE to real-
world noise modeling and downstream denoising and super-
resolution tasks. Our method further improves upon other
degradation modeling methods and achieves the best per-
formance on the SIDD dataset.
Acknowledgements
This work was supported by the National Key R&D Pro-
gram of China (No. 2021YFA1001300), the National Nat-
ural Science Foundation of China (No. 12271291).
25896
References
[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S
Brown. A high-quality denoising dataset for smartphone
cameras. In CVPR , pages 1692–1700, 2018. 1, 5
[2] Abdelrahman Abdelhamed, Marcus A Brubaker, and
Michael S Brown. Noise flow: Noise modeling with condi-
tional normalizing flows. In CVPR , pages 3165–3173, 2019.
1, 2
[3] Joshua Batson and Loic Royer. Noise2self: Blind denoising
by self-supervision. In ICML , pages 524–533. PMLR, 2019.
6, 7
[4] Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M
Dai, Rafal Jozefowicz, and Samy Bengio. Generating sen-
tences from a continuous space. arXiv:1511.06349 , 2015.
5
[5] Benoit Brummer and Christophe De Vleeschouwer. Natural
image noise dataset. In CVPRW , pages 0–0, 2019. 1
[6] Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local
algorithm for image denoising. In CVPR , pages 60–65. Ieee,
2005. 1
[7] Adrian Bulat, Jing Yang, and Georgios Tzimiropoulos. To
learn image super-resolution, use a gan to learn how to do
image degradation first. In ECCV , pages 185–200, 2018. 2
[8] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun.
Simple baselines for image restoration. In ECCV , pages 17–
33. Springer, 2022. 7
[9] Jun Cheng, Tao Liu, and Shan Tan. Score priors guided deep
variational inference for unsupervised real-world single im-
age denoising. In ICCV , pages 12937–12948, 2023. 1
[10] Rewon Child. Very deep vaes generalize autoregressive mod-
els and can outperform them on images. arXiv:2011.10650 ,
2020. 4
[11] Xin Cui, Cong Wang, Dongwei Ren, Yunjin Chen, and
Pengfei Zhu. Semi-supervised image deraining using knowl-
edge distillation. IEEE Trans Circuits Syst Video Technol , 32
(12):8327–8341, 2022. 2
[12] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and
Karen Egiazarian. Color image denoising via sparse 3d col-
laborative filtering with grouping constraint in luminance-
chrominance space. In ICIP , pages I–313. IEEE, 2007. 1
[13] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Learning a deep convolutional network for image
super-resolution. In ECCV , pages 184–199. Springer, 2014.
1
[14] Chao Dong, Chen Change Loy, and Xiaoou Tang. Acceler-
ating the super-resolution convolutional neural network. In
ECCV , pages 391–407. Springer, 2016. 1
[15] Weisheng Dong, Guangming Shi, and Xin Li. Nonlocal im-
age restoration with bilateral variance estimation: a low-rank
approach. IEEE Trans Image Process , 22(2):700–711, 2012.
1
[16] Manuel Fritsche, Shuhang Gu, and Radu Timofte. Frequency
separation for real-world super-resolution. In ICCVW , pages
3599–3608. IEEE, 2019. 2, 7, 8
[17] Zixuan Fu, Lanqing Guo, and Bihan Wen. srgb real
noise synthesizing with neighboring correlation-aware noise
model. In CVPR , pages 1683–1691, 2023. 6[18] Ryan D Gow, David Renshaw, Keith Findlater, Lindsay
Grant, Stuart J McLeod, John Hart, and Robert L Nicol. A
comprehensive tool for modeling cmos image-sensor-noise
performance. IEEE Trans Electron Devices , 54(6):1321–
1329, 2007. 1
[19] Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu
Feng. Weighted nuclear norm minimization with applica-
tion to image denoising. In CVPR , pages 2862–2869, 2014.
1
[20] Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei
Zhang. Toward convolutional blind denoising of real pho-
tographs. In CVPR , pages 1712–1722, 2019. 1
[21] Samuel W Hasinoff, Fr ´edo Durand, and William T Freeman.
Noise-optimal capture for high dynamic range photography.
InCVPR , pages 553–560. IEEE, 2010. 1
[22] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze
removal using dark channel prior. IEEE Trans. Pattern Anal.
Mach. Intell. , 33(12):2341–2353, 2010. 2
[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. NIPS , 30, 2017. 6
[24] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,
Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and
Alexander Lerchner. beta-vae: Learning basic visual con-
cepts with a constrained variational framework. In ICLR ,
2016. 4
[25] Geonwoon Jang, Wooseok Lee, Sanghyun Son, and Ky-
oung Mu Lee. C2n: Practical generative noise modeling for
real-world denoising. In ICCV , pages 2350–2359, 2021. 6
[26] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li,
and Feiyue Huang. Real-world super-resolution via kernel
estimation and noise injection. In CVPRW , pages 466–467,
2020. 7, 8
[27] Nanfeng Jiang, Jiawei Luo, Junhong Lin, Weiling Chen, and
Tiesong Zhao. Lightweight semi-supervised network for sin-
gle image rain removal. Pattern Recognit , page 109277,
2023. 2
[28] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming
Song. Denoising diffusion restoration models. NIPS , 35:
23593–23606, 2022. 1
[29] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv:1412.6980 , 2014. 5
[30] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv:1312.6114 , 2013. 2
[31] Shayan Kousha, Ali Maleky, Michael S Brown, and Mar-
cus A Brubaker. Modeling srgb camera noise with normal-
izing flows. In CVPR , pages 17463–17471, 2022. 2, 6
[32] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug.
Noise2void-learning denoising from single noisy images. In
CVPR , pages 2129–2137, 2019. 6, 7
[33] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo
Larochelle, and Ole Winther. Autoencoding beyond pixels
using a learned similarity metric. In ICML , pages 1558–
1566. PMLR, 2016. 5
[34] Wooseok Lee, Sanghyun Son, and Kyoung Mu Lee. Ap-bsn:
Self-supervised denoising for real-world images via asym-
25897
metric pd and blind-spot network. In CVPR , pages 17725–
17734, 2022. 6, 7
[35] Lerenhan Li, Yunlong Dong, Wenqi Ren, Jinshan Pan,
Changxin Gao, Nong Sang, and Ming-Hsuan Yang. Semi-
supervised image dehazing. IEEE Trans Image Process , 29:
2766–2779, 2019. 2
[36] Xin Lin, Chao Ren, Xiao Liu, Jie Huang, and Yinjie Lei.
Unsupervised image denoising in real-world scenarios via
self-collaboration parallel generative adversarial branches.
InICCV , pages 12642–12652, 2023. 6, 7
[37] Andreas Lugmayr, Martin Danelljan, and Radu Timofte. Un-
supervised learning for real-world super-resolution. In IC-
CVW , pages 3408–3416. IEEE, 2019. 2
[38] Andreas Lugmayr, Martin Danelljan, Radu Timofte, Manuel
Fritsche, Shuhang Gu, Kuldeep Purohit, Praveen Kandula,
Maitreya Suin, AN Rajagoapalan, Nam Hyung Joon, et al.
Aim 2019 challenge on real-world image super-resolution:
Methods and results. In ICCVW , pages 3575–3583. IEEE,
2019. 5
[39] Andreas Lugmayr, Martin Danelljan, and Radu Timofte.
Ntire 2020 challenge on real-world image super-resolution:
Methods and results. In CVPRW , pages 494–495, 2020. 5
[40] Andreas Lugmayr, Martin Danelljan, Luc Van Gool, and
Radu Timofte. Srflow: Learning the super-resolution space
with normalizing flow. In ECCV , pages 715–732. Springer,
2020. 1
[41] Sameer Malik and Rajiv Soundararajan. Semi-supervised
learning for low-light image restoration through quality as-
sisted pseudo-labeling. In WACV , pages 4105–4114, 2023.
2
[42] Kristina Monakhova, Stephan R Richter, Laura Waller, and
Vladlen Koltun. Dancing under the stars: video denoising in
starlight. In CVPR , pages 16241–16251, 2022. 5
[43] Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son,
and Kyoung Mu Lee. Cvf-sid: Cyclic multi-variate function
for self-supervised image denoising by disentangling noise
from image. In CVPR , pages 17583–17591, 2022. 6, 7
[44] Yizhong Pan, Xiao Liu, Xiangyu Liao, Yuanzhouhan Cao,
and Chao Ren. Random sub-samples generation for self-
supervised real image denoising. In ICCV , pages 12150–
12159, 2023. 6, 7
[45] Pietro Perona and Jitendra Malik. Scale-space and edge
detection using anisotropic diffusion. IEEE Trans. Pattern
Anal. Mach. Intell. , 12(7):629–639, 1990. 1
[46] Tobias Plotz and Stefan Roth. Benchmarking denoising al-
gorithms with real photographs. In CVPR , pages 1586–1595,
2017. 1, 2, 5
[47] Chao Ren, Xiaohai He, Chuncheng Wang, and Zhibo Zhao.
Adaptive consistency prior based deep network for image de-
noising. In CVPR , pages 8596–8606, 2021. 7
[48] Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear
total variation based noise removal algorithms. Physica D ,
60(1-4):259–268, 1992. 1, 2
[49] Yuge Shi, Brooks Paige, Philip Torr, et al. Variational
mixture-of-experts autoencoders for multi-modal deep gen-
erative models. NIPS , 32, 2019. 3[50] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning
structured output representation using deep conditional gen-
erative models. Advances in neural information processing
systems , 28, 2015. 3
[51] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solv-
ing inverse problems in medical imaging with score-based
generative models. arXiv:2111.08005 , 2021. 1
[52] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,
Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxim:
Multi-axis mlp for image processing. In CVPR , pages 5769–
5780, 2022. 7
[53] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical
variational autoencoder. NIPS , 33:19667–19679, 2020. 4
[54] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,
Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-
hanced super-resolution generative adversarial networks. In
ECCVW , pages 0–0, 2018. 1, 7, 8
[55] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P
Simoncelli. Image quality assessment: from error visibility
to structural similarity. IEEE Trans Image Process , 13(4):
600–612, 2004. 7
[56] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang
Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A gen-
eral u-shaped transformer for image restoration. In CVPR ,
pages 17683–17693, 2022. 7
[57] Kaixuan Wei, Jiaolong Yang, Ying Fu, David Wipf, and
Hua Huang. Single image reflection removal exploiting mis-
aligned training data and network enhancements. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 8178–8187, 2019. 1
[58] Wei Wei, Deyu Meng, Qian Zhao, Zongben Xu, and Ying
Wu. Semi-supervised transfer learning for image rain re-
moval. In CVPR , pages 3877–3886, 2019. 2
[59] Yunxuan Wei, Shuhang Gu, Yawei Li, Radu Timofte, Long-
cun Jin, and Hengjie Song. Unsupervised real-world im-
age super resolution via domain-distance aware training. In
CVPR , pages 13385–13394, 2021. 2, 7, 8
[60] Yanyan Wei, Zhao Zhang, Yang Wang, Haijun Zhang,
Mingbo Zhao, Mingliang Xu, and Meng Wang. Semi-
deraingan: A new semi-supervised single image deraining.
InICME , pages 1–6. IEEE, 2021. 2
[61] Valentin Wolf, Andreas Lugmayr, Martin Danelljan, Luc
Van Gool, and Radu Timofte. Deflow: Learning complex im-
age degradations from unpaired data with conditional flows.
InCVPR , pages 94–103, 2021. 1, 2, 6, 7, 8
[62] Zongsheng Yue, Hongwei Yong, Qian Zhao, Deyu Meng,
and Lei Zhang. Variational denoising network: Toward blind
noise modeling and removal. NIPS , 32, 2019. 7
[63] Zongsheng Yue, Qian Zhao, Lei Zhang, and Deyu Meng.
Dual adversarial network: Toward real-world noise removal
and noise generation. In ECCV , pages 41–58. Springer,
2020. 2, 6
[64] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-
nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.
Restormer: Efficient transformer for high-resolution image
restoration. In CVPR , pages 5728–5739, 2022. 7
25898
[65] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and
Lei Zhang. Beyond a gaussian denoiser: Residual learning
of deep cnn for image denoising. IEEE Trans Image Process ,
26(7):3142–3155, 2017. 1, 6
[66] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward
a fast and flexible solution for cnn-based image denoising.
IEEE Trans Image Process , 27(9):4608–4622, 2018. 1
[67] Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc
Van Gool, and Radu Timofte. Plug-and-play image restora-
tion with deep denoiser prior. IEEE Trans. Pattern Anal.
Mach. Intell. , 44(10):6360–6376, 2021. 6, 7
[68] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , pages 586–595,
2018. 5, 8
[69] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and
Yun Fu. Residual dense network for image super-resolution.
InCVPR , pages 2472–2481, 2018. 5
[70] Dihan Zheng, Sia Huat Tan, Xiaowen Zhang, Zuoqiang Shi,
Kaisheng Ma, and Chenglong Bao. An unsupervised deep
learning approach for real-world image denoising. In ICLR ,
2020. 1, 2
[71] Dihan Zheng, Xiaowen Zhang, Kaisheng Ma, and Cheng-
long Bao. Learn from unpaired data for image restoration: A
variational bayes approach. IEEE Trans. Pattern Anal. Mach.
Intell. , 2022. 1, 2, 6, 7, 8
[72] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV , pages 2223–2232,
2017. 2
25899
