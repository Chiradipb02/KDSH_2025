LEAD: Exploring Logit Space Evolution for Model Selection
Zixuan Hu1Xiaotong Li1Shixiang Tang3Jun Liu4Yichun Hu1Ling-Yu Duan1,2*
1School of Computer Science, Peking University, Beijing, China,2Peng Cheng Laboratory, Shenzhen, China,
3The Chinese University of Hong Kong, Hongkong, China,4Singapore University of Technology and Design, Singapore
{hzxuan, hycc, lingyu }@pku.edu.cn, lixiaotong@stu.pku.edu.cn ,
shixiangtang@cuhk.edu.hk, jun liu@sutd.edu.sg
Abstract
The remarkable success of “pretrain-then-finetune”
paradigm has led to a proliferation of available pre-trained
models for vision tasks. This surge presents a signifi-
cant challenge in efficiently choosing the most suitable pre-
trained models for downstream tasks. The critical aspect of
this challenge lies in effectively predicting the model trans-
ferability by considering the underlying fine-tuning dynam-
ics. Existing methods often model fine-tuning dynamics in
feature space with linear transformations, which do not pre-
cisely align with the fine-tuning objective and fail to grasp
the essential nonlinearity from optimization. To this end,
we present LEAD, a finetuning-aligned approach based on
the network output of logits. LEAD proposes a theoretical
framework to model the optimization process and derives an
ordinary differential equation (ODE) to depict the nonlin-
ear evolution toward the final logit state. Additionally, we
design a class-aware decomposition method to consider the
varying evolution dynamics across classes and further en-
sure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabili-
ties derived from the differential equation, our method of-
fers a concise solution to effectively bridge the optimization
gap in a single step, bypassing the lengthy fine-tuning pro-
cess. The comprehensive experiments on 24 supervised and
self-supervised pre-trained models across 10 downstream
datasets demonstrate impressive performances and show-
case its broad adaptability even in low-data scenarios.
1. Introduction
The “pretrain-then-finetune” learning paradigm has exhib-
ited remarkable success in a wide range of computer vision
tasks [6, 28], leading to an increasing number of diverse pre-
trained models open-sourced to the computer vision com-
munity. This surge poses a significant challenge in select-
ing the optimal pre-trained model for a downstream task
*Corresponding Author.
Figure 1. This diagram illustrates the task of model selection,
which aims to rank pre-trained models for choosing the optimal
one. The ground-truth ranking sequence is derived from the mod-
els’ accuracy after fine-tuning. However, due to the time-intensive
nature of this process, it requires to design efficient transferability
metrics for accurate ranking prediction. The performance of this
metric is evaluated using rank correlation, e.g., weighted Kendall’s
tau [42], to compare the predicted and ground-truth rankings.
from a vast model zoo. Considering that the brute-force so-
lution of fine-tuning all models on downstream tasks may
be infeasible in many scenarios due to the substantial com-
putational costs, the model selection task emerges [13, 47],
which ranks the transferability of pre-trained models to the
given task without incurring much cost, as shown in Fig. 1.
Predicting model transferability aims to efficiently esti-
mate the final state after fine-tuning on the downstream task,
without network optimization by backward propagation.
Prior studies only depend on specific properties of the ini-
tial state ( e.g., representation similarity [13, 14], conditional
distribution [33, 47]), to serve as transferability indicators.
Recent studies reveal that modeling the dynamic process of
fine-tuning has become the bottleneck to design more high-
quality transferability metrics. Following this philosophy,
recent works propose to simulate fine-tuning dynamics on
features through multiple stages of linear transformations
[32, 41], achieving some promising results.
Despite recent progress in this field, there are still two
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28664
considerable gaps between existing modeled fine-tuning dy-
namics and the real ones: (1) Insufficient objective align-
ment. The designed objectives in prior works do not seam-
lessly align with fine-tuning, as they often focus more on en-
hancing class separability via feature manipulation, rather
than the classification optimization for both the feature and
output spaces. (2) Disregarding nonlinear modeling. Exist-
ing methods relying solely on linear transformations cannot
well capture the intricate nonlinear changes in the feature
space during optimization. Such constraints contribute to
growing deviations in dynamic modeling and result in un-
reliable predictions of the final state.
Delving deeper into the optimization process, we focus
on modeling the dynamic changes within the output space
(i.e.,logits for classification), which not only more closely
relates to the fine-tuning objective but also embeds the over-
all nonlinear dynamics from the network optimization dur-
ing fine-tuning. Building upon this foundation, we intro-
duce an insightful framework to depict the trajectory of log-
its toward their final state in a dynamical modeling system.
To capture and understand this evolution process of the logit
space, the framework provides a discrete iterative optimiza-
tion equation via gradient analysis and captures nonlinear
logit space changes during optimization.
In contrast to the extensive iterations required in fine-
tuning optimization, we further extend the framework to
propose a new transferability metric “ Logits Evolution
Analysis through Dynamical Equation (LEAD)”, to seize
the final evolutionary state in a single step. We perform
a mathematical derivation to convert the discrete process
into a continuous differential equation and simplify it into
an ordinary differential equation (ODE) by leveraging the
classical analysis tool, Neural Tangent Kernel (NTK) [26].
This ODE concisely models the nonlinear evolution in logit
space through its closed-form solution, allowing for real-
time estimation of logits. Additionally, we introduce a
class-aware decomposition method to enhance prediction
accuracy and break the barrier of theoretical assumption of
NTK. As a result, our method provides a direct estimation
of the evolution by incorporating both the initial model state
and the impact of fine-tuning evolution, serving as an effec-
tive indicator assessing model transferability.
The contributions of this work are three-fold. (1) To the
best of our knowledge, we are the first to explore model
transferability by modeling the evolution process in the
logit space. (2) We propose LEAD, a novel transferabil-
ity metric with two key components: a theoretical frame-
work providing a closed-form solution to depict the log-
its evolution towards the final state, and a class-aware de-
composition method to guarantee the practical applicabil-
ity of theoretical findings and improve predictive precision.
(3) We validate the effectiveness and generalizability of
our method through experiments on 24 different pre-trainedmodels across 10 downstream datasets, achieving SOTA re-
sults with an average of 17% gain in the rank correlation. In
addition, our method can be naturally extended to low-data
scenarios while maintaining competitive performance.
2. Related Work
2.1. Transferability Metric
The challenge of efficiently ranking pre-trained models has
captured much interest of researchers, leading to the devel-
opment of various metrics to evaluate the model transfer-
ability [2, 3, 12, 18, 36, 38, 47, 52]. For example, RSA
[13], DDS [14] and GBC [38] are methods based on the
distance measurements of the feature space. RSA and DDS
assess transferability by evaluating the representation sim-
ilarity between pre-trained and downstream datasets. GBC
employs the Bhattacharyya coefficient to measure class sep-
arability in the feature space. Nleep [33] and Logme [52]
estimate transferability by analyzing the conditional distri-
bution, with Nleep utilizing Gaussian mixture model and
Logme employing Bayesian optimization. However, these
methods treat network outputs as static and overlook the
changes introduced by the training process, leading to less
effectiveness in scenes with larger fine-tuning dynamics.
Recently, SFDA [41] and PED [32] propose to simulate
changes in the feature space, respectively projecting the fea-
tures into a Fisher space and utilizing a physically-inspired
model to enhance the class separability, achieving impacts
similar to fine-tuning. While these methods achieve some
improvements, they solely concentrate on the feature space,
disregarding the dynamic changes introduced by the classi-
fier, which is jointly optimized with the backbone. Addi-
tionally, they utilize linear transformations to simulate the
intricate variations in fine-tuning, resulting in a significant
limitation in simulation precision. Instead, our LEAD em-
ploys the ordinary differential equation to analyze logit dy-
namics, maintaining a closely aligned objective and simu-
lating the nonlinear changes in fine-tuning.
2.2. Neural Tangent Kernel
Studying the behavior of neural network optimization pro-
cesses has been an enduring challenge due to their non-
linearity and nonconvexity. Neural Tangent Kernel (NTK)
[20, 26], serving as a theoretical tool to characterize the op-
timization process, has attracted much research attention.
Some studies have explored its theoretical properties and
applications. Notable works include the NTK’s property of
remaining constant under infinite network width [26], the
explanation of the instability of training [48], the analysis
of its bias and variance under finite width [20], and its ac-
celeration algorithm [35]. While NTK can analyze network
trainability [26, 35], it is constrained by the requirement of
infinite width and neglects the consideration of initial log-
its, resulting in the inability to predict the changes starting
28665
from the initial state for a finite-width network. To tackle
these issues, we innovatively propose an analysis frame-
work leveraging the property of NTK, suitable for model
selection problems to bridge the gap between the initial and
final states of finite-width networks.
3. Methodology
In this section, we first present the problem setup for the
task of model selection. Then we analyze training mech-
anisms to model the fine-tuning optimization in a discrete
process. Finally, we propose a transferability metric that
converts the discrete process into a continuous dynamical
equation with a concise solution to directly seize the evolu-
tion destination of logits, and feed updated logits into Cross-
Entropy loss to assess transferability, as shown in Fig. 2.
3.1. Problem Setup
Consider Mpre-trained models {Ψm}M
m=1and a down-
stream dataset T={xn, yn}N
n=1withKclasses, where
model Ψconsists of its backbone fwhich outputs the en-
coded feature, Nis the number of images in the downstream
task. The purpose of model selection is to design an effi-
cient algorithm to predict the ranking of the performance of
Mpre-trained models after fine-tuning.
Specifically, given the ground-truth performance rank-
ings{Gm}M
m=1of pre-trained models after fine-tuning and
the predicted ranking {Pm}M
m=1by model selection algo-
rithms, we can leverage the commonly-used rank-based cor-
relation, i.e.,the weighted Kendalls’ τw, to evaluate the ef-
fectiveness of the proposed model selection methods, i.e.,
τw=1P
i̸=jwij·X
i̸=jwijsign(Gi−Gj)·sign(Pi−Pj),(1)
where wij=1
i+jand sign (·)is the signum function. A
larger τwindicates more consistencies between the pre-
dicted ranking and the ground truth, which means a better
model selection algorithm.
3.2. Revisiting the Fine-tuning Optimization
When a pre-trained model Ψtransfers to a downstream
taskT:={X,Y}, it undergoes extensive optimization for
evolving from its initial state (before fine-tuning) to the fi-
nal state (after fine-tuning), resulting in complex changes
in both feature space and output space. Since the transfer-
ability metric is designed to evaluate the final state with-
out fine-tuning, it is essential and challenging to model the
optimization process without lengthy computations, bridg-
ing the gap between the initial and the final state. To gain
insights into the underlying optimization mechanisms, we
conduct an analysis to compare the prior arts with fine-
tuning to revisit the evolution process toward the final state.
Recent works SFDA [41] and PED [32] (called simu-
lation methods) aim to approximate fine-tuning proceduresby simulating feature dynamics. They design an objective
function Uto assess the class separability of the feature
space. Then, they apply linear transformations to increase
Uand feed modified features into a classifier to update log-
its, as shown below:
A, b= arg max
A,bU(A·f(X) +b,Y),
∆F(X) =C(A·f(X) +b)− C(f(X)),(2)
where A, b denote linear transformation parameters, Cde-
notes a machine learning classifier and Fdenotes the func-
tion that outputs logits. As a result of the optimization in
Eq. (2), the objective of enhancing class separability is
achieved. However, fine-tuning directly minimizes the gap
between predicted logits and labels and performs gradient
descent on parameters to update the output logits as follows:
∆θ=−η·dL(F(X, θ),Y)
dθ,
∆F(X) =F(X, θ+ ∆θ)− F(X, θ)≈∆θ·dF
dθ
=−η·dL
dθ·dF
dθ=−η·dL
dFdF
dθ·dF
dθ
,(3)
where ηdenotes the learning rate, Ldenotes loss function,
andθdenotes parameters of F. Eq. (3) provides the vari-
ation of logits within one step, capturing the nonlinear dy-
namics based on gradients. When comparing the optimiza-
tion outlined in Eq. (2) and Eq. (3), it becomes evident
that simulation methods differ from fine-tuning in terms of
objectives and optimization approaches.
Despite preliminary exploration, such simulation meth-
ods come with severe limitations due to these inconsisten-
cies, leading to deviations in predicting the final state, as il-
lustrated in Fig. 3. Specifically, simulation methods aim to
enhance class separability within the feature space, whereas
fine-tuning drives logits closer to labels. The insufficient
objective alignment introduces distinct inductive bias com-
pared to fine-tuning, resulting in notable variations in mod-
eling dynamics [27, 34]. Moreover, due to the nonlinear na-
ture of neural networks and complex parameter interactions,
linear approximations in simulation methods often fail to
accurately capture the dynamic changes [21]. Such limita-
tions hinder the effective modeling of nonlinearity, further
constraining modeling precision.
3.3. Modeling Logits Evolution Process through
Dynamical Equation
To address the aforementioned problems, our study delves
deeper into the optimization analysis from Eq. (3). In fact,
unlike features, the logits derived from the optimization out-
put naturally align with the fine-tuning objectives and reflect
the inherent nonlinearity from gradient descent. Therefore,
28666
Figure 2. Comparison of different approaches for modeling the evolution process. (a) Fine-tuning iteratively updates parameters via
gradient descent, revealing authentic logits evolution trajectory and classification accuracy for ground-truth ranking. (b) Recent works
(e.g., SFDA [41], PED [32]) perform linear transformations in feature space to enhance class separability, updating logits with modified
features. The ranking score is determined by Cross-Entropy loss on updated logits. While these approaches simulate fine-tuning impact on
features, they often suffer huge deviations from the authentic results. (c) Our method, LEAD, transforms the discrete logit evolution during
fine-tuning into a gradient-based dynamical equation to capture nonlinear changes and can efficiently predict the final state through the
closed-form solution. It establishes an analysis framework linking the initial and final states, enabling the precise prediction in one step.
it provides a particularly relevant entry point to analyze dy-
namical changes during fine-tuning and assess the model
transferability through its evolution process.
While the dynamical process of logit space analyzed
by Eq. (3) remains consistent with fine-tuning, there still
lacks an efficient approach for capturing logits evolution,
due to the discrete process of iterating over thousands of
steps by traditional optimization. To this end, we propose a
new transferability metric called “Logits Evolution Analy-
sis through Dynamical equation (LEAD)”, that bridges the
evolution process from the initial logit space towards the fi-
nal state. In contrast to extensive optimization iterations,
our method proposes a dynamical analysis framework for
analyzing the nonlinear effects within the logit space, pro-
viding an efficient prediction to capture the final state with
a concise solution in a single step.
Theoretical Background. In our LEAD, we innovatively
introduce the Neural Tangent Kernel (NTK) to simplify the
dynamical equation. Here, we provide a brief introduc-
tion to the definition of the NTK kernel function and the
constant-preserving property that we utilize.
ˆΦθt(X,X)≜∇θtF(X;θt)∇θtF(X;θt)⊤,
Φ(X,X)≜lim
l→∞ˆΦθt(X,X),(4)
where ∇denotes the Laplace Operator, and ldenotes the
root mean square of the widths of all layers in the network.
The kernel function ˆΦθtremains invariant throughout the
training process as lapproaches infinity with an approxi-
mation error of O(1
l2)[20, 26]. For simplicity, in the re-
mainder of this paper, we denote functions at time tas:
Figure 3. Comparison of the average prediction of final state logits
across various methods on VOC2007 [16], where the actual result
stems from fine-tuning. Closer to actual results indicates more
accurate modeling of the final state. Simulation methods (SFDA,
PED) capture the correct trend but exhibit a large deviation from
the actual results, affecting the accuracy of model ranking.
Ft(·)≜F(·, θt),ˆΦt(·)≜ˆΦθt(X,X),Φ≜Φ(X,X).
Dynamical Equation and Closed-form Solution. To by-
pass the complexity of iterative computations, we extend
the analysis in Eq. (3) and theoretically derive a dynamical
equation that is efficient for analysis. Specifically, we seek
for a mathematical derivation through the theory of limit ap-
proximation to convert the discrete process into a continu-
ous differential equation, enabling us to track the trajectory
of logits in the solution space of the equation. Furthermore,
we leverage the property of the NTK in Eq. (4) to simplify
the equation into an Ordinary Differential Equation (ODE)
with an initial value condition. Due to space constraints, we
directly provide the equation and the detailed proof can be
found in the Appendix.
dFt
dt=−η·Φ·dLt
dFt,F0=loginit, (5)
28667
where loginitis the initial state of logits obtained through
feeding features and labels of downstream datasets to a clas-
sification algorithm. Compared to the discrete process, our
proposed ODE depicts the continuous evolution of the fine-
tuning dynamics, allowing for a more flexible modeling ap-
proach. It transforms discrete gradient effects into equiva-
lent differential effects in continuous time, preserving the
capability for modeling nonlinear changes and providing
precise prediction of the evolution process. Furthermore,
unlike employing numerical methods as the ODEsolver, we
can directly obtain its closed-form solution through integra-
tion over time dimension, allowing us to bypass the lengthy
evolution process and reach the optimization destination in
a single step, as shown below:
E(Ft(X)) = 
I−e−ηΦ·t
Y+e−ηΦ·t·loginit.(6)
As depicted in Eq. (6), the solution is derived by interpo-
lating between the one-hot label and the initial observation
of logits, with the interpolation coefficients determined by
the NTK matrix. It comprises two crucial elements: loginit,
representing the value of logits before optimization and de-
termining the start of the evolution process; and Φ, indicat-
ing the convergence rate to the label and determining the
trajectory of logit evolution. Our proposed framework effi-
ciently models the evolution process, enabling a prediction
of real-time changes.
Class-aware Decomposition. While Eq. (6) provides the
theoretical result for the final state, computing the value of
Φunder finite width remains challenging. Prior studies [20,
53] indicate that, even with finite width, the convergence
rate can be assessed by using the eigenvalues of the initial
state NTK matrix ˆΦ0. Inspired by this empirical finding, we
further conduct a decomposition approach to bridge theory
and practice, utilizing the eigenvalue decomposition on ˆΦ0
to replace the convergence rate determined by Φin Eq. (6)
with eigenvalues.
Due to the network exhibiting varying classification abil-
ities across different classes, their convergence behavior and
evolution dynamics also differ during the fine-tuning pro-
cess. To enhance the modeling of behaviors in different
classes, we separately compute NTK for each class, rather
than mixing them together, to decouple the respective opti-
mization effect. Specifically, we extract nsamples of each
class to calculate their respective NTK matrices and per-
form decomposition to obtain eigenvalues. The average
eigenvalue of each class serves as the unified parameter to
determine the interpolation coefficient for this class:
E(Ft(x)) =
I−e−ηλ·t
y+e−ηλ·t·loginit, (7)
where x, yis a given sample and its one-hot label, λis the
average eigenvalue of the NTK matrix associated with the
class of x. Building upon the conclusion of Eq. (6), Eq.(7) avoids the demand of infinite width, enabling practical
computability of the prediction results. As shown in Fig. 2,
we employ the results of Eq. (7) to efficiently obtain pre-
dictions for the final state logits. Subsequently, following
the common practice [32, 33, 41], we feed predictions into
the Cross-Entropy loss to obtain the transferability score for
model ranking.
Summary. Our method explicitly models the fine-tuning
optimization through the theoretical analysis framework
and provides a concise dynamical solution to bypass the
lengthy optimization process in a single step, which seam-
lessly integrates the initial state observations and the evo-
lution trajectory towards the final state. Therefore, we can
efficiently predict the final logit state through this dynami-
cal solution. Through comparing logit prediction accuracy
among different models, it can serve as an effective metric
for predicting model transferability.
4. Experiments
To demonstrate the effectiveness and robustness of our ap-
proach, we evaluate LEAD on diverse pre-trained mod-
els, including both supervised and self-supervised convo-
lutional neural networks. Our evaluation spans 10 widely
used classification benchmarks in transfer learning, em-
ploying weighted Kendall’s τwas rank correlation to evalu-
ate the transferability metric. Additionally, we highlight the
effectiveness of LEAD in the low-data regime, where only
2/5/10samples are available per class.
4.1. Implementation Details
Given a pre-trained model Ψ, we append a random-
initialized [22] MLP has a classification head after its back-
bonef, and combine them as F:=h◦fto produce logits.
To compute the logits in the final state, we only need to de-
rive the NTK matrix of each class and loginitthrough Eq.
(7). We follow the approach in [35] to compute the NTK
matrix and employ a robust and efficient machine learn-
ing classifier, the multi-class SVM [49], to generate loginit.
Due to space constraints, we provide a more detailed expla-
nation and pseudocode in the Appendix.
4.2. Benchmarks
Target Datasets. We employ 10 classification bench-
marks commonly utilized in transfer learning research in-
cluding Caltech-101 [17], Stanford Cars [29], CIFAR10
[30], CIFAR100 [30], DTD [10], Oxford 102 Flowers
[37], Food-101 [4], Oxford-IIIT Pets [39], SUN397 [51],
and VOC2007 [16]. These datasets encompass abun-
dant characteristics, including backgrounds, textures, and
coarse/fine-grained scenes, spanning diverse fields.
Ground Truth Rank. To conduct the ground truth ranking
for the model zoo, we follow the approach in [41] and [52].
Specifically, we employ a grid search strategy to determine
28668
Table 1. Performance comparisons of different transferability metrics on the supervised model zoo. We measure the performance with
weighted Kendall’s τwand a larger τwrepresents a better prediction rank. The best results are in bold and the second-best are in underline.
Method ReferenceDownstream Target Dataset
Food Caltech Flowers Cars CIFAR100 DTD CIFAR10 Pets SUN397 VOC2007
PARC [3] NeurIPS’21 0.363 0.422 0.066 0.135 -0.092 0.536 0.310 0.114 -0.097 0.707
Logme [52] ICML’21 0.336 0.349 0.425 0.538 0.603 0.651 0.783 0.372 0.440 0.673
Nleep [33] CVPR’21 0.504 0.678 0.292 0.435 0.686 0.452 0.656 0.686 0.770 0.645
PACTran [12] ECCV’22 0.709 0.372 0.480 0.151 0.742 0.372 0.757 0.451 0.422 0.536
SFDA [41] ECCV’22 0.534 0.597 0.421 0.472 0.749 0.421 0.817 0.625 0.562 0.728
GBC [38] CVPR’22 0.844 0.430 0.348 0.735 0.830 0.480 0.866 0.358 0.574 0.654
ETran [18] ICCV’23 0.709 0.627 0.534 0.557 0.749 0.605 0.719 0.651 0.577 0.673
PED [32] ICCV’23 0.573 0.597 0.451 0.472 0.749 0.421 0.770 0.625 0.562 0.728
LEAD This paper 0.892 0.698 0.786 0.586 0.835 0.689 0.791 0.841 0.609 0.743
the actual performance of each model in the downstream
task. This strategy involves trying various learning rates
from the set {10−1,10−2,10−3,10−4}and different weight
decay values from the set {0,10−6,10−5,10−4,10−3}. To
ensure reliability, we repeat each experiment 5 times with
different random seeds and take the average as the result.
4.3. Evaluation on Supervised Models
Model Zoo. We construct a Model Zoo that includes 12
CNN models supervised pre-trained on ImageNet [11], cov-
ering widely-used architectures: ResNet-34 [23], ResNet-
50 [23], ResNet-101 [23], ResNet-152 [23], DenseNet-121
[25], DenseNet-161 [25], DenseNet-169 [25], DenseNet-
201 [25], MNet-A1 [45], MobileNetV2 [40], GoogleNet
[43], and InceptionV3 [44]. The accuracy results of these
models after fine-tuning can be found in the Appendix.
Result Analysis. To show the effectiveness of LEAD, we
compare it with prior arts, such as Bayesian-based Logme,
Separability-based GBC, and simulation methods SFDA
and PED. As shown in Tab. 1, simulation methods con-
sider underlying dynamic effects and exhibit promising per-
formances on some datasets. However, as the aforemen-
tioned analysis in Sec. 3.2 shows, the bias in the simu-
lation process causes these methods to have unstable im-
provements, particularly resulting in unsatisfactory perfor-
mance on Flowers, Food, and Pets. Benefiting from a pre-
cise analysis of the evolution, our LEAD yields a perfor-
mance gain of +0.335, +0.319, +0.216 compared to PED on
these three datasets, respectively. Overall, LEAD achieves
the best τwon 7 target datasets and obtains an average τw
of 0.747 which is relatively 18% better than the latest SOTA
ETran [18]. Our experiments highlight the importance of
accurate modeling of dynamics and confirm the effective-
ness of our method to model the evolution process through
the dynamical equation.
4.4. Evaluation on Self-supervised Models
Model Zoo. Self-supervised learning models (SSL mod-
els) have shown their efficacy in transfer learning and of-
ten display superior generalization performance comparedto Supervised models. To illustrate the broad applicabil-
ity of our approach, inspired by [32, 41], we assemble a
self-supervised model zoo trained on ImageNet with BYOL
[19], Infomin [46], PCLv1 [31], PCLv2 [31], Selav2 [1],
InsDis [50], SimCLRv1 [7], SimCLRv2 [8], MoCov1 [24],
MoCov2 [9], DeepClusterv2 [5], and SWA V [6].
Result Analysis. Due to differing pre-training and down-
stream classification objectives, SSL models undergo larger
dynamic changes during the fine-tuning process, leading
to distinct transferability properties compared to supervised
models [15]. As shown in Tab. 1 and 2, methods that disre-
gard dynamics ( e.g.PARC, Nleep) perform competitively
in the supervised model zoo, but fail on some datasets for
SSL models. For example, Nleep and PARC have a neg-
ative small rank correlation on VOC2007 (-0.101) and CI-
FAR100 (-0.136), respectively. To maintain fairness, we
refrain from comparing with GBC [38] in the SSL model
zoo, which is designed for supervised scenarios. On the
contrary, SFDA and PED exhibit better performances due
to consideration of training dynamics. Nevertheless, the
limitation of the simulating process still hinders their per-
formance. Compared to them, LEAD achieves a consistent
improvement on 7 datasets and obtains an average τwof
0.746 which is relatively 15% better than PED (0.648). The
results validate the effectiveness of LEAD for SSL models.
4.5. Evaluation on Low-data Regime
Experimental Settings. In practical application, privacy
and resource constraints may limit our access to the entire
downstream dataset. To explore the utilization boundary,
we evaluate whether LEAD can provide effective model
ranking only using limited samples. We follow three data
settings in PACTran [12] with increasing average number of
samples per class {2,5,10}and divide 10 datasets into two
groups according to the number of classes: 100+ classes,
including Food, Caltech, Flowers, Cars, CIFAR100, and
SUN397; 10-99 classes, including DTD, CIFAR10, Pets,
and VOC2007. For each setting, we repeatedly sample 5
times for evaluation and take the average as the result.
Result Analysis. As shown in Tab. 3, all methods, in-
28669
Table 2. Performance comparisons on the self-supervised model zoo. The best results are in bold and the second-best are in underline.
Method ReferenceDownstream Target Dataset
Food Caltech Flowers Cars CIFAR100 DTD CIFAR10 Pets SUN397 VOC2007
PARC [3] NeurIPS’21 0.359 0.196 0.622 0.424 -0.136 0.447 0.147 0.496 -0.006 0.618
Logme [52] ICML’21 0.570 0.051 0.604 0.375 -0.008 0.627 0.295 0.684 0.217 0.158
Nleep [33] CVPR’21 0.574 0.525 0.534 0.486 0.276 0.641 -0.044 0.792 0.719 -0.101
PACTran [12] ECCV’22 0.720 0.622 0.601 0.474 0.529 0.614 0.477 0.641 0.638 0.620
SFDA [41] ECCV’22 0.685 0.523 0.749 0.515 0.548 0.773 0.619 0.586 0.698 0.568
ETran [18] ICCV’23 0.465 0.405 0.644 0.587 0.650 0.214 0.606 0.338 0.520 0.376
PED [32] ICCV’23 0.581 0.614 0.777 0.649 0.568 0.907 0.673 0.462 0.661 0.583
LEAD This paper 0.860 0.780 0.725 0.663 0.776 0.825 0.713 0.629 0.760 0.723
Table 3. Performance comparisons with different N/K , where Kis the number of classes, Nis the number of samples for computing
the metric. We divide 10 benchmarks into two groups according to the number of classes: 100+ classes and 10-99 classes, and evaluate
the average of τwon the two groups. Additionally, we also provide the average results on the 10 benchmarks, recorded as Avg. The best
results are denoted in bold, the second best results are denoted in underline, and - denotes cannot give a valid ranking.
Supervised / Self-supervised pre-trained Model
MethodN/K = 2 N/K = 5 N/K = 10
100+ classes 10-99 classes Avg 100+ classes 10-99 classes Avg 100+ classes 10-99 classes Avg
GBC 0.17 / 0.32 0.10 / 0.17 0.13 / 0.25 0.45 / 0.40 0.22 / 0.18 0.36 / 0.31 0.59 / 0.42 0.42 / 0.22 0.52 / 0.34
PACTran 0.43 /0.49 0.20 / 0.39 0.33 / 0.45 0.42 / 0.48 0.31 / 0.44 0.38 / 0.47 0.56 / 0.52 0.48 / 0.46 0.53 / 0.50
ETran 0.42 / 0.43 0.12 / 0.41 0.29 / 0.42 0.48 / 0.47 0.31 / 0.38 0.41 / 0.43 0.56 / 0.47 0.56 / 0.48 0.56 / 0.48
PED - / 0.36 - / 0.24 - / 0.31 - / 0.43 - / 0.35 - / 0.40 0.30 / 0.50 0.22 / 0.56 0.27 / 0.52
LEAD 0.45 / 0.48 0.51 /0.45 0.47 /0.46 0.48 /0.49 0.64 /0.53 0.54 /0.51 0.65 /0.53 0.64 /0.60 0.64 /0.56
cluding PACTran designed for this scenario, exhibit signif-
icant performance declines under low-data settings. For in-
stance, ETran’s performance in the supervised model zoo
drops from 0.64 with the entire dataset to 0.29, 0.41, and
0.56 in three low-data settings. Moreover, some methods,
such as SFDA and PED fail to provide prediction rankings
in some settings. For example, when N/K = 2,5, PED
cannot correctly evaluate class separability due to insuffi-
cient data, imposing huge changes to completely separate
different classes. This leads to all models perfectly classi-
fying all samples, resulting in identical scores that cannot be
ranked. In contrast, our LEAD achieves stable results, en-
suring τw≥0.45even in extreme settings like N/K = 2,
and consistently outperforms other methods. For example,
under N/K = 10 , its average τwin the supervised model
zoo reaches 0.64, which is relatively 14% better than the lat-
est SOTA ETran (0.56) and is equivalent to the latest SOTA
on the complete dataset, with less than 5%of data size. The
results demonstrate the robustness of our method in the face
of data limitations.
5. Ablation Study and Efficiency Analysis
In this section, without loss of generality, we conduct abla-
tion studies on Caltech, CIFAR10, and VOC2007, build-
ing upon the evaluation of SSL models in Sec. 4.4 for
saving time. We conduct separate experiments to analyze
the impact of interpolation coefficients, a key component of
LEAD, aiming to delve into the influence of hyperparam-
eters and implementation details, gaining insights into keyfactors contributing to LEAD’s effectiveness. Additionally,
we provide a runtime comparison to verify efficiency, and
more ablation study results are provided in the appendix.
5.1. Interpolation Coefficient
The interpolation coefficients are determined by time coef-
ficient tand the NTK matrix. We conduct ablation experi-
ments on these two key factors:
Time Coefficient t.As we model the discrete process as a
continuous ODE, the continuous variable tis proportional
to the number of updates, determining the progress of the
evolution process. In Fig. 4, we evaluate our method un-
der varied time settings, keeping the unit as 1e6consistent
across all experiments. It is evident that performance im-
proves as the time increases within a specific range. How-
ever, if the dynamic effect persists for an extended period
(e.g., 10.0), all model predictions converge too closely to
the ground-truth label, reducing discrimination for model
ranking. Hence, we set the time coefficient as 1 by default.
Figure 4. The left picture shows the performance gain in τwwith
respect to different time tand the right shows the performance in
different sample sizes for each class NTK.
28670
Sample Size for NTK. To enhance the perception of dif-
ferent dynamic intensities of classes, we propose the class-
aware decomposition that employs Ssamples of each class
to obtain NTK matrix and perform spectrum analysis to de-
rive eigenvalues. In Fig. 4, we conduct ablation experi-
ments to explore the effect of varying S. We observe that
performance generally improves as Sincreases, and gradu-
ally reaches stability. This demonstrates that when the sam-
ple size reaches a certain size, eigenvalues reliably capture
differences in dynamic intensity, offering a refined analysis
result of the dynamics. Therefore, we set S= 64 to ensure
the method’s efficiency and stability in all experiments.
5.2. Running Time Comparison
In experiment results, we have demonstrated the effective-
ness of LEAD in various scenarios. In this section, we
highlight the computational efficiency of LEAD. With the
concise solution in Eq. (7), LEAD predicts the final state
only requiring the NTK matrix and loginit. As shown in
Tab. 4, the running time of LEAD is about half that of
simulation methods (45s compared to 92s), ranking second
after GBC which displays unstable performance. Notably,
LEAD takes 3s to compute the NTK, while feature extrac-
tion takes 420s, resulting in only a marginal 0.7%increase
in GPU time. The time to compute loginitis 42s CPU time,
indicating significant potential for exploring faster classifi-
cation algorithms to further reduce the overall time.
Table 4. The comparisons of average running time on CIFAR10.
Metrics PARC NLEEP PACTran SFDA GBC PED Ours
Running Time 111s 1430s 77s 92s 26s 97s 45s
6. Visualization and Analysis
In this section, we validate our approach through some visu-
alizations and these results clearly illustrate how our model
effectively addresses the model selection problem.
6.1. Logit Prediction
We have conducted a comparison of the predicted logits
distribution on VOC2007 using different methods, as illus-
trated in Fig. 3. Notably, predictions obtained by LEAD
closely match the authentic distribution after fine-tuning.
To provide further quantitative analysis, we visualize the
average bias in predicted logits in Fig. 5. Across three
datasets, our method exhibits a substantial reduction com-
pared to simulation methods, achieving nearly a 50% over-
all reduction. This visualization highlights the precision of
our dynamical equation-based approach in capturing logit
evolution for more accurate ranking sequences.
Figure 5. Comparison of the average difference between logits
predicted by different methods and logits after fine-tuning. The
difference is calculated utilizing the Euclidean distance.6.2. Rank Variation
By employing our method, we can refine models that have
poor initial observation but show better progress during
training. To assess the effectiveness of our approach, we
conduct a visualization comparing the model rankings be-
tween the initial observations and our predicted final state,
as shown in Fig. 6. The results clearly illustrate a substan-
tial enhancement in the calibration of model rankings when
using our refined observations, with the optimal model now
positioned closely to the reference line in the figure. No-
tably, models starting from an initially unfavorable point
can be rapidly elevated to achieve a superior ranking, high-
lighting the effectiveness of our methodology.
Figure 6. The variation in model ranking, utilizing initial states
(first row) and predicted final states of LEAD (second row). Re-
sults span three datasets, from left to right: CIFAR10, Caltech, and
VOC2007. Shaded areas reflect the range of the ranking error.
7. Conclusions and Future Work
In this work, we propose a dynamical equation designed to
capture the evolution process within the logit space, offer-
ing an efficient solution for assessing model transferabil-
ity. We construct a theoretical analysis framework and fur-
ther devise a more practical and precise solution. And we
demonstrate its consistent effectiveness across various sce-
narios including supervised and self-supervised, full-data
and low-data settings. In future work, we intend to ex-
pand the current analysis framework beyond classification
to cover a broader range of downstream tasks. We hope
that our work can enhance the understanding of optimiza-
tion mechanisms in deep neural networks and shed light on
other fields, such as the design of optimization algorithms.
Acknowledgement
This work was supported by the National Natural Science
Foundation of China under Grant 62088102, in part by the
PKU-NTU Joint Research Institute (JRI) sponsored by a do-
nation from the Ng Teng Fong Charitable Foundation and
in part by AI Joint Lab of Future Urban Infrastructure spon-
sored by Fuzhou Chengtou New Infrastructure Group and
Boyun Vision Co. Ltd.
28671
References
[1] Yuki Markus Asano, Christian Rupprecht, and Andrea
Vedaldi. Self-labelling via simultaneous clustering and rep-
resentation learning. arXiv , 2019. 6
[2] Yajie Bao, Yang Li, Shao-Lun Huang, Lin Zhang, Lizhong
Zheng, Amir Zamir, and Leonidas Guibas. An information-
theoretic approach to transferability in task transfer learning.
InICIP , pages 2309–2313, 2019. 2
[3] Daniel Bolya, Rohit Mittapalli, and Judy Hoffman. Scalable
diverse model selection for accessible transfer learning. In
NeurIPS , pages 19301–19312, 2021. 2, 6, 7
[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
Food-101–mining discriminative components with random
forests. In ECCV , 2014. 5
[5] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and
Matthijs Douze. Deep clustering for unsupervised learning
of visual features. In ECCV , pages 132–149, 2018. 6
[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-
otr Bojanowski, and Armand Joulin. Unsupervised learn-
ing of visual features by contrasting cluster assignments. In
NeurIPS , pages 9912–9924, 2020. 1, 6
[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In ICML , 2020. 6
[8] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad
Norouzi, and Geoffrey E Hinton. Big self-supervised mod-
els are strong semi-supervised learners. In NeurIPS , pages
22243–22255, 2020. 6
[9] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
Improved baselines with momentum contrastive learning.
arXiv , 2020. 6
[10] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy
Mohamed, and Andrea Vedaldi. Describing textures in the
wild. In CVPR , pages 3606–3613, 2014. 5
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , pages 248–255, 2009. 6
[12] Nan Ding, Xi Chen, Tomer Levinboim, Soravit Changpinyo,
and Radu Soricut. Pactran: Pac-bayesian metrics for estimat-
ing the transferability of pretrained models to classification
tasks. In ECCV , pages 252–268. Springer, 2022. 2, 6, 7
[13] Kshitij Dwivedi and Gemma Roig. Representation similarity
analysis for efficient task taxonomy & transfer learning. In
CVPR , pages 12387–12396, 2019. 1, 2
[14] Kshitij Dwivedi, Jiahui Huang, Radoslaw Martin Cichy, and
Gemma Roig. Duality diagram similarity: a generic frame-
work for initialization selection in task transfer learning. In
ECCV , pages 497–513, 2020. 1, 2
[15] Linus Ericsson, Henry Gouk, and Timothy M Hospedales.
How well do self-supervised models transfer? In CVPR ,
pages 5414–5423, 2021. 6
[16] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. IJCV , 88:303–338, 2010. 4, 5
[17] Li Fei-Fei. Learning generative visual models from few
training examples. In CVPR workshop , 2004. 5[18] Mohsen Gholami, Mohammad Akbari, Xinglu Wang,
Behnam Kamranian, and Yong Zhang. Etran: Energy-based
transferability estimation. In ICCV , pages 18613–18622,
2023. 2, 6, 7
[19] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,
Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, et al. Bootstrap your own latent-a new approach
to self-supervised learning. In NeurIPS , pages 21271–21284,
2020. 6
[20] Boris Hanin and Mihai Nica. Finite depth and width cor-
rections to the neural tangent kernel. In ICLR , 2020. 2, 4,
5
[21] Hangfeng He and Weijie Su. The local elasticity of neural
networks. In ICLR , 2020. 3
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectifiers: Surpassing human-level perfor-
mance on imagenet classification. In ICCV , 2015. 5
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
pages 770–778, 2016. 6
[24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In CVPR , pages 9729–9738, 2020. 6
[25] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In CVPR , pages 4700–4708, 2017. 6
[26] Arthur Jacot, Franck Gabriel, and Cl ´ement Hongler. Neu-
ral tangent kernel: Convergence and generalization in neural
networks. In NeurIPS , 2018. 2, 4
[27] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task
learning using uncertainty to weigh losses for scene geome-
try and semantics. In CVPR , pages 7482–7491, 2018. 3
[28] Simon Kornblith, Jonathon Shlens, and Quoc V . Le. Do bet-
ter imagenet models transfer better? In CVPR , pages 2656–
2666, 2018. 1
[29] Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei.
Collecting a large-scale dataset of fine-grained cars. 2013. 5
[30] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 5
[31] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi.
Prototypical contrastive learning of unsupervised representa-
tions. arXiv , 2020. 6
[32] Xiaotong Li, Zixuan Hu, Yixiao Ge, Ying Shan, and Ling-
Yu Duan. Exploring model transferability through the lens
of potential energy. In ICCV , pages 5429–5438, 2023. 1, 2,
3, 4, 5, 6, 7
[33] Yandong Li, Xuhui Jia, Ruoxin Sang, Yukun Zhu, Bradley
Green, Liqiang Wang, and Boqing Gong. Ranking neural
checkpoints. In CVPR , pages 2663–2673, 2021. 1, 2, 5, 6, 7
[34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll ´ar. Focal loss for dense object detection. In ICCV ,
pages 2980–2988, 2017. 3
[35] Mohamad Amin Mohamadi and Danica J. Sutherland. A
fast, well-founded approximation to the empirical neural tan-
gent kernel. In ICML , 2022. 2, 5
28672
[36] Cuong Nguyen, Tal Hassner, Matthias Seeger, and Cedric
Archambeau. Leep: A new measure to evaluate transferabil-
ity of learned representations. In ICML , pages 7294–7305,
2020. 2
[37] Maria-Elena Nilsback and Andrew Zisserman. Automated
flower classification over a large number of classes. In 2008
Sixth Indian Conference on Computer Vision, Graphics &
Image Processing , pages 722–729. IEEE, 2008. 5
[38] Michal P ´andy, Andrea Agostinelli, Jasper Uijlings, Vittorio
Ferrari, and Thomas Mensink. Transferability estimation us-
ing bhattacharyya class separability. In CVPR , pages 9172–
9182, 2022. 2, 6
[39] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and
CV Jawahar. Cats and dogs. In CVPR , pages 3498–3505,
2012. 5
[40] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In CVPR , pages 4510–4520,
2018. 6
[41] Wenqi Shao, Xun Zhao, Yixiao Ge, Zhaoyang Zhang, Lei
Yang, Xiaogang Wang, Ying Shan, and Ping Luo. Not all
models are equal: Predicting model transferability in a self-
challenging fisher space. In ECCV , pages 286–302, 2022. 1,
2, 3, 4, 5, 6, 7
[42] Grace S. Shieh. A weighted kendall’s tau statistic. Statistics
Probability Letters , 39(1):17–24, 1998. 1
[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR , pages 1–9, 2015. 6
[44] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR , pages 2818–2826,
2016. 6
[45] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,
Mark Sandler, Andrew Howard, and Quoc V Le. Mnas-
net: Platform-aware neural architecture search for mobile.
InCVPR , pages 2820–2828, 2019. 6
[46] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan,
Cordelia Schmid, and Phillip Isola. What makes for good
views for contrastive learning? In NeurIPS , pages 6827–
6839, 2020. 6
[47] Anh T Tran, Cuong V Nguyen, and Tal Hassner. Transfer-
ability and hardness of supervised classification tasks. In
ICCV , pages 1395–1405, 2019. 1, 2
[48] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and
why pinns fail to train: A neural tangent kernel perspective.
Journal of Computational Physics , 449:110768, 2022. 2
[49] Jason Weston, Chris Watkins, et al. Support vector machines
for multi-class pattern recognition. In Esann , pages 219–
224, 1999. 5
[50] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.
Unsupervised feature learning via non-parametric instance
discrimination. In CVPR , pages 3733–3742, 2018. 6
[51] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
and Antonio Torralba. Sun database: Large-scale scene
recognition from abbey to zoo. In CVPR , pages 3485–3492.
IEEE, 2010. 5[52] Kaichao You, Yong Liu, Jianmin Wang, and Mingsheng
Long. Logme: Practical assessment of pre-trained models
for transfer learning. In ICML , pages 12133–12143, 2021.
2, 5, 6, 7
[53] Gizem Y ¨uce, Guillermo Ortiz-Jim ´enez, Beril Besbinar, and
Pascal Frossard. A structured dictionary perspective on im-
plicit neural representations. In CVPR , pages 19228–19238,
2022. 5
28673
