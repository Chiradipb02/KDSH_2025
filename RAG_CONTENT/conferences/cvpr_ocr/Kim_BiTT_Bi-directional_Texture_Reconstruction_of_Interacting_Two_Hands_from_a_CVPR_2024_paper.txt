BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a
Single Image
Minje Kim1
1KAISTTae-Kyun Kim1,2
2Imperial College London
Free View, Free Pose, Relightable Two Hand ReconstructionInput: Single Image of Interacting Two Hands
Two hands reconstruction via ˝Symmetric information of         left and right hand,˞Hand texture parametric model
Figure 1. Taking a single image input, our method renders the personalized texture of two hands at novel views, poses, and light conditions.
Abstract
Creating personalized hand avatars is important to of-
fer a realistic experience to users on AR / VR platforms.
While most prior studies focused on reconstructing 3D hand
shapes, some recent work has tackled the reconstruction
of hand textures on top of shapes. However, these meth-
ods are often limited to capturing pixels on the visible
side of a hand, requiring diverse views of the hand in a
video or multiple images as input. In this paper, we pro-
pose a novel method, BiTT(Bi-directional Texture recon-
struction of Two hands), which is the ﬁrst end-to-end train-
able method for relightable, pose-free texture reconstruc-
tion of two interacting hands taking only a single RGB im-
age, by three novel components: 1) bi-directional (left $
right) texture reconstruction using the texture symmetry of
left / right hands, 2) utilizing a texture parametric model
for hand texture recovery, and 3) the overall coarse-to-ﬁne
stage pipeline for reconstructing personalized texture of two
interacting hands. BiTT ﬁrst estimates the scene light con-
dition and albedo image from an input image, then recon-
structs the texture of both hands through the texture para-
metric model and bi-directional texture reconstructor. In ex-
periments using InterHand2.6M and RGB2Hands datasets,
our method signiﬁcantly outperforms state-of-the-art hand
texture reconstruction methods quantitatively and qualita-tively. The code is available at https://github.com/
yunminjin2/BiTT .
1. Introduction
3D human reconstruction has been studied in various areas.
With the increasing usage of human-computer interaction,
virtual reality (VR), and augmented reality (AR), recon-
struction of human parts, including the full body, face, and
hand, has been intensively studied for years. In particular,
hand pose estimation, shape, and texture reconstruction are
essential tasks for AR/VR interfaces. 3D hand reconstruc-
tion is still a challenging task due to the highly varied poses
and shapes of hands. Previous works [ 3,4,9,14,23,45] fo-
cused on estimating the 3D pose and shape of a single hand.
A single hand reconstruction has been recently extended to
two interacting hands [ 18,19,21,49] and hand-object in-
teraction [ 2,8,15,16,46] scenarios.
Learning the appearance of objects and humans is cur-
rently in active research for realistic reconstruction. NeRF
[27] represents objects/scenes by a neural radiance ﬁeld
based on volume rendering. Appearance reconstruction
of clothed human bodies [ 1,12,22,30,44] and faces
[20,25,32,38,48] has been intensively studied compared
to hands. For learning hand appearance, LISA [ 7] used a
radiance ﬁeld to learn the shape and color appearance from
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10726
multi-view images. In the latest works [ 5,7,10,17,29] to
reconstruct the hand appearance, they take multi-view im-
ages or a monocular video as input to learn the texture of
mostly single hands [ 5,7,10,17,29]. HandAvatar [ 5] and
HARP [ 17] render relightable hand appearance by estimat-
ing the albedo of a single hand.
3D reconstruction from a single image is also another
challenging task. Non-visible side of an object should be
estimated with given a single image for full 3D reconstruc-
tion. Self-Supervised 3D Mesh Reconstruction (SMR) [ 11]
estimates 3D meshes with texture from a single image in a
self-supervised manner. Wu et al. [43] reconstructed vase
artifacts into 3D mesh with environment lighting, shiny ma-
terial, and texture albedo. Other works [ 9,14,23,45] also
focus on constructing symmetric objects or single-type ob-
jects. Human hands, however, exhibit non-symmetric fea-
tures such as the palm and back. Given the information
on one side of the hand, the other side of the hand texture
should be estimated to fully reconstruct the hand. S2Hand
[6] and AMVUR [ 13] presented a method to reconstruct
both the appearance and geometry of a single hand from
a single image. Nevertheless, their appearance is in blurred
textures, omitting detail texture appearance.
Identity 1Identity 2Identity 3Identity 4Identity 5Figure 2. Symmetrical hand textures of different identities are
shown, taken from pairs of diametrical camera views of Inter-
Hand2.6M [ 28].
In this paper, we propose a novel approach that exploits
texture symmetry of left and right hands through the bi-
directional reconstruction of two hand textures from a sin-
gle image (see Fig. 2). Our method is trained per scene with
only one single image, taking visible texture information
from both hands and the parametric model of hand texture
to reconstruct realistic hand appearances. Given an input
image and a mesh of hands, our method predicts lights and
albedo image (please refer to Sec. 3.1). In the coarse stage,
our model generates full hand textures with estimated vec-
tors using HTML [ 34], the hand texture parametric model.
With the estimated albedo image and coarse stage estimated
texture map, the bi-directional texture reconstructor (BTR)
yields the UV maps of both hand textures by utilizing the
feature maps of a left and right hand. The proposed BiTT
method can render both hands with fully controllable light
conditions, poses, and camera views. We evaluated the
method using the InterHand2.6M [ 28], and RGB2Hands[40] dataset and achieved high-ﬁdelity appearances com-
pared to state-of-the-art methods. We present qualitative
results where the rendered images realistically capture per-
sonalized hand textures (e.g. wrinkles, veins, nails). Fig. 1
shows that our method is capable of controlling light con-
ditions, camera views, and hand poses. To the best of our
knowledge, this is the ﬁrst method to reconstruct both hands
with textures from a given single image input.
In summary, our main contributions are as follows: 1)
we introduce a novel framework BiTT, the ﬁrst method for
rendering two interacting hands from a single image. 2) We
propose the bi-directional texture reconstruction, exploiting
the texture symmetry of left and right hands. 3) We intro-
duce a way to use the texture parametric model for recov-
ering invisible texture. 4) We demonstrate that our frame-
work is an end-to-end trainable for photorealistic two-hand
avatars with controllable poses, views, and light conditions.
2. Related Work
In hand appearance modeling, NIMBLE [ 24] learns factor-
ization of albedo, specular, and normal maps from high-
deﬁnition hand textures. S2HAND [ 6] estimates camera
poses, colored meshes, and lighting conditions in a simul-
taneous way, but its rendering quality does not demonstrate
detailed (or personalized) hand appearances. AMVUR [ 13]
reconstructs a hand using attention-based mesh vertices and
the occlusion-aware texture regression model. However, its
per-vertex texture reconstruction is not adaptable to the re-
construction of realistic hand textures due to its low reso-
lution. In addition, the reconstructed hand textures contain
background colors since the method does not consider ge-
ometric misalignment noise. LISA [ 7] learns an implicit
color ﬁeld with implicit shape representation but still lacks
detailed hand textures despite multi-view inputs.
Neural-Radiance-Field based Representation. In re-
cent years, the application of hand representation by the
neural radiance ﬁeld has been studied extensively. HandA-
vatar [ 5] has improved the hand appearance with the oc-
cupancy ﬁeld and predicted self-occlusion shadows. How-
ever, it requires a large amount of training data and time
for a new instance of hand. HandNeRF [ 10], Livehand
[29] extracts hand texture from multi-view image sequence
with volume rendering in neural radiance ﬁeld for hand
reconstruction. To develop subject-speciﬁc shape and ap-
pearance using NeRF-based methods such as HandAvatar,
HandNeRF, and LiveHand, a substantial volume of images
within a ﬁxed light condition, up to thousands of images for
each single sequence, is typically needed for training. Since
these methods are integrated into an implicit space, they re-
quire additional steps for mesh extraction to further applica-
tions. They also face difﬁculties in controlling illumination,
including rendering shadows and editing textures.
10727
Coarse Stage
!xx
++!(ℎ!)
!(ℎ")%!
%"&!
&"Input Image !
Mesh "!,#, #U'"'!
Bi-directional Texture Reconstruction̂'!
̂'""Scene EstimationLight !
Albedo Image "($)x+=
x+=Texture Map ̂%$,% Mesh &!,#Camera Parameter 'Light (
U: Unwrapping Texture: Residual Connectionx+: Multiplication: Summation: Phong Illumination Rendering)(*,*,*,*))(̂'!,",,!,",-,.)
)(̂'!,",,!,",-,.$)
)(̂'!,",,!,",-$,.)Novel View Image &!&$Novel Light ''
Novel Viewpoint ('ℎ!
ℎ"Fine Stage1−1−#Reconstructed  Image  &!
Relit  Image &!!$
,
Figure 3. The architecture of BiTT. Our method consists of three steps: (1) scene estimation, (2) coarse stage, and (3) ﬁne stage estimation.
The scene estimation understands the scene by predicting the albedo image and lighting conditions with a given input image. Full detailed
textures of both hands are reconstructed from the single image input. The hand texture parametric model is adopted in the coarse stage,
then the bi-directional texture reconstruction reﬁnes the personalized hand textures by the texture symmetry of left-right-hands. Finally,
we render both hands with Phong Illumination [ 33].
UV Texture Map based Representation. Unlike the im-
plicit function-based methods, HARP [ 17] reconstructed a
single hand by mesh rendering with a UV texture map.
HARP reconstructs a personalized single hand using the
UV texture map, normal map, and self-occlusion shadow.
HARP renders detailed hand textures including out-of-
distribution appearance like tattoos and accessories. How-
ever, the HARP method assumes the texture color into a
speciﬁc value, and as a result, the occluded part of the tex-
ture tends to be a ﬁxed value, missing out the detailed tex-
ture.
Positioning BiTT to w.r.t related works. Two-hand tex-
ture rendering based on UV maps has not been explored to
the best of our knowledge. In implicit space, HandNeRF
[10] reconstructed two hands from a multi-view image se-
quence. Reconstruction of interacting two hands from a sin-
gle image is a challenging task, as we have to lift a 2D im-
age to achieve accurate 3D hand representation. Also, two
hands encounter a higher rate of occlusion compared to a
single hand, thus restoring occluded texture is another chal-
lenge. Whereas using multi-views or a video sequence as
input helps reconstruct two interacting hands, taking a sin-
gle image input poses more signiﬁcant challenges. How-
ever, two hands convey more information than a single
hand, which enables us to reconstruct both hands realisti-cally within a single image. In this work, we propose a
novel method that exploits the two-hand symmetric texture
information and employs the hand texture parametric model
as prior. The proposed method demonstrates that only a
single image is enough to reconstruct a realistic two-hand
avatar in contrast to the prior works [ 5,10,17,29].
3. Methods
We propose a texture reconstruction model for two interact-
ing hands with a single image. Fig. 3shows the architecture
of BiTT. Given a single RGB image Iof interacting two
hands, we reconstruct realistic personalized textures for a
two-hand avatar. BiTT is composed of three steps: the scene
estimation, the coarse stage based on the parametric model,
and the ﬁne stage composed with the bi-directional texture
reconstructor.
For reconstructing the full texture of both hands from a
single image, our method relies on utilizing the symmetry
between the left and right hands. While the disparity be-
tween the left and right hand texture is assumed to be not
signiﬁcant (e.g. see Fig. 2), it is beneﬁcial to leverage the
symmetrical data to reconstruct detailed textures even on
occluded hands. For those pixels not visible on both hands,
the texture parametric model is adopted, and its estimation
is further reﬁned.
10728
3.1. Scene Estimation
Scene estimation involves estimating the environment, in-
cluding the light and albedo of hands in the input image. To
estimate the environment, our light network Lestimates the
light parameter lwhich is composed of the ambient light
color, diffuse, specular, and direction. Furthermore, we
also predict an albedo image through our albedo network
A, which represents the surface reﬂectance of objects us-
ing the Lambertian surface. Details of the light and albedo
networks are found in the supplementary material.
3.2. Coarse Stage
Since 2D image lacks the information to reconstruct 3D ob-
jects, non-symmetric object reconstruction [ 1,6,26,31,37,
39,44] suffers from blurry textures. Several major works
[5,7,17,27,29] overcome the lack of information by multi-
view images or a monocular video as input. Instead of in-
creasing the input information, we augment data through
the parametric model. The coarse stage is based on the hand
texture parametric model (HTML) [ 34], which encodes the
hand texture through PCA algorithm and is able to create a
full texture of hand from a single image.
Background on HTML [ 34].HTML is a hand texture
parametric model that can create a full-texture UV map with
a given vector. HTML scanned 51 hands and aligned them
to a canonical space with MANO [ 36] model ﬁtting. Af-
ter the MANO ﬁtting, texture mapping is done with manu-
ally deﬁned UV coordinates. Finally, a parametric model
Tis created by PCA on vectors ⌧i2R618990with the
collection of 2D texture maps where 618,990 is a total
number of 206,330 pixels in texture map with RGB chan-
nels. Given the covariance matrix C2R618990 ⇥618990=
1
n 1Pn
i=1(⌧i ¯⌧)(⌧i ¯⌧)>, where ¯⌧=1
nPn
i=1⌧i, the
principal components  2R618990 ⇥101are obtained by
singular value decomposition of C= ⌃ >, where ⌃2
R101⇥101is a diagonal matrix. With the principal compo-
nents  , we get a full texture eigenvalue T(↵)2R618990
for a given parameter vector ↵2R101with T(↵)=¯⌧+ ↵.
For more details on HTML, please refer to [ 34].
Hand Texture Parametric Model in Coarse Stage. The
HTML [ 34] network Hin Fig. 3is an encoder that esti-
mates both the hand parameter vectors from an input image
I. Given the left-hand parameter vector hland the right-
hand parameter vector hr, we can obtain the full texture
eigenvalues T(hl),T(hr). It can be formally deﬁned as:
T(hi)=¯⌧+ hi, where hl,hr=H(I),i=l, r (1)
ˆIcoarse ={R(T(hi),mi,p ,l)}i=l,r(2)Iis an input image, Ris the differentiable renderer based
on Phong model [ 33], and ˆIcoarse is the reconstructed image
from the coarse stage. With the texture vector T(hi)and
meshes mi,Rrenders two hand meshes with texture on the
2D space at a camera viewpoint pand light condition l.
3.3. Fine Stage
!"!"
!"#"$%!"!"$%
!"#"!"!
!""Bi-directional!-th Decoding BlockBi-directionalDecoding BlockLeft-Hand UV Texture Map
Right-Hand UV Texture Map""#"!#
Figure 4. Detailed architecture of the decoding layer in the bi-
directional texture reconstruction.
To render more realistic and personalized textures, we
use the symmetric features of left/right hands rather than
using features independently. We propose a novel bi-
directional texture reconstructor (BTR) to efﬁciently use
the symmetric texture features of both hands. BTR recon-
structs the full texture from visible pixels in the albedo im-
ageA(I). We create a visible UV texture map and texture
vector by unwrapping hand textures as we know the map-
ping between the UV texture map and image pixels, as well
as the mapping between the UV texture map and the texture
vector. With the visible texture vector for each hand un-
wrapped from the albedo image A(I), denoted as ul,urfor
each hand, and vl,vrwhich are visible mask texture vectors
for each hand, we synthesize texture vector tl,trdeﬁned as:
ti=T(hi)(1 vi)+ui, where i=l, r (3)
BiTT, then, generates full both hand textures through the
BTR.
In addition, with the visible albedo texture ul,ur, we
render ˆIalbedo with the estimated light lto use in loss func-
tion:
ˆIalbedo ={R(ui,mi,p ,l)}i=l,r (4)
Bi-directional Texture Reconstruction. Given the syn-
thesized texture vectors tl,tr, the bi-directional texture re-
constructor (BTR) encodes each texture vector, denoted as
el,er. After encoding each hand texture vector, the decoder
in BTR decodes the embedded feature to create a full texture
vector using skip connections. Fig. 4describes the detailed
10729
description of the BTR decoder. In each bi-directional de-
coding block, the embedded features of the same hand are
concatenated with a skip connection. We also concatenate
the other hand texture embedded features in a bi-directional
way to use the symmetric information of two hands. After-
ward, we obtain the decoded variable from the concatenated
feature at the i-th level, denoted as `fi. The decoded features
`fi
l,`fi
r, which means the i-th level of the left hand and the
right hand, respectively, are deﬁned as:
`fi
l= (N([ei
l,`fi 1
l,`fi 1
r])) (5)
`fi
r= (N([ei
r,`fi 1
r,`fi 1
l])) (6)
where  denotes a ReLU activation function, Nis a con-
volution neural network (CNN), and [] denotes the channel
concatenation operator.
After the decoding, we obtain the full texture vectors
ˆtl,ˆtr2R3⇥206330for each hand. Thus, our network is
formulated as:
ˆI=BiTT (I)= 
R(ˆti,mi,p ,l) 
i=l,r(7)
where ˆIis the ﬁnal rendered image of reconstructed both
hands.
3.4. Loss Functions
The loss functions we use to train our model are as follows.
Reconstruction Loss. As our method aims to represent
the realistic appearance of the input image, we included
the reconstruction loss Lrecto measure the similarity be-
tween the input image Iand three distinct rendered images:
rendered image from ﬁne stage ( ˆI), rendered image from
coarse stage ( ˆIcoarse ), and the rendered image with albedo
visible texture ( ˆIalbedo ). The reconstruction loss Lrecis de-
ﬁned as:
Lrec= reck(I ˆI)k1+ coarse
rec kI ˆIcoarse k1
+ albedo
rec kI ˆIalbedo k1(8)
Reconstruction Loss on Non-visible Pixels. Visible in-
formation itself is not enough to accurately recreate the
complete texture of hands. Even when symmetrical aspects
are taken into account, it is still not sufﬁcient to cover the
entire hand texture. Thereby, we resort to the full texture
obtained in the coarse stage for those pixels not observed
in either of the hands. We measure the L1 loss between the
coarse stage estimated hand texture and ﬁne stage estimated
hand texture with the invisible map mask. Lnvis deﬁned as:
Lnv=X
i=l,rk((T(hi) ˆti)(1 vi)k1 (9)where T(hi)is the reconstructed hand texture in the coarse
stage, ˆtiis the reconstructed hand texture in the ﬁne stage
for each hand. The visible mask of the hand texture is de-
noted as viand thus, 1 virepresents the invisible mask of
the texture. These textures are simultaneously reﬁned with
those of visible and symmetric texture reconstruction and
consistency losses.
Albedo Consistency Loss. The albedo image should be
obtained independent of lighting conditions. To obtain an
accurate albedo image, we augment individually rendered
images ˆIcoarse, ˆliwith different lighting conditions ˆliand
obtain the albedo images through the albedo network A.
Our new albedo loss function Lalbcalculates the L1 loss be-
tween albedo images from different lights. Thus, the albedo
loss term Lalbis deﬁned as:
Lalb=nX
i=1nX
j=i+1[kA(ˆIcoarse, ˆli) A(ˆIcoarse, ˆlj)k1].
(10)
In our experiment, we rendered three different lights in to-
tal: a reconstructed light, a light from a different direction,
and a light with a different color to make the albedo net-
work Aestimate the albedo image consistently even in the
different lighting conditions.
Symmetric Loss. For learning the symmetric feature of
two hands, we apply a symmetric loss term Lsymwhich is
the L1 loss between the left and right hands. The Lsymis
deﬁned as:
Lsym= symk(ˆtl ˆtr)k1 (11)
Total Loss. In summary, our loss function is deﬁned as:
L=Lrec+ nvLnv+ albLalb+ symLsym (12)
where  coarse
rec =0.8, albedo
rec =0.4, nv=0.2, alb=
0.2, sym=0.3are used to train our model and ﬁxed for
all experiments.
4. Experiments
4.1. Experimental Settings
Our training framework follows a per-scene training like
NeRF [ 27]. Given multiple images of the scene, they learn
to generate novel views of the scene. Efforts have been
made to reduce the number of required images, such as Pix-
elNeRF [ 47] which trains NeRF with just one or a few im-
ages. In addition, HARP [ 17], HandAvatar [ 5], HandNeRF
[10], and others [ 7,29] are based on per-scene training, re-
quiring dozens of multiple frames of the same hand (scene).
Our method, utilizing texture symmetry and a parametric
texture model, requires only a single image for training.
10730
Table 1. Quantitative comparisons of BiTT, S2Hand [ 6], HTML [ 34] and HARP [ 17]. Training data are from Interhand2.6M [ 28] including
all identities. For evaluations, novel poses and viewpoints are randomly selected from the same hand identity. In the case when not using
GT mesh, we used IntagHand [ 21] for obtaining meshes.
(a) Using GT mesh in all methods.
Evaluation Method L1 ↓ LPIPS ↓PSNR ↑MS-SSIM ↑
Appearance
ReconstructionS2Hand [ 6] 0.0206 0.1340 26.39 0.8570
HTML [ 34] 0.0256 0.1292 24.72 0.8152
HARP [ 17] 0.0157 0.0696 28.11 0.9061
BiTT(ours) 0.0101 0.1019 30.41 0.9349
Novel PosesS2Hand 0.0221 0.1343 25.70 0.8507
HTML 0.0255 0.1291 24.49 0.8153
HARP 0.0239 0.1266 25.79 0.8546
BiTT(ours) 0.0209 0.1261 26.54 0.8564
Different ViewsS2Hand 0.0217 0.1320 25.73 0.8484
HTML 0.0254 0.1282 24.42 0.8133
HARP 0.0234 0.1189 25.97 0.8346
BiTT(ours) 0.0204 0.1092 27.79 0.8843(b) Without using GT mesh in all methods.
Evaluation Method L1 ↓ LPIPS ↓PSNR ↑MS-SSIM ↑
Appearance
ReconstructionS2Hand [ 6] 0.0264 0.1214 25.72 0.8897
HTML [ 34] 0.0268 0.1207 24.48 0.8545
HARP [ 17] 0.0237 0.1047 25.17 0.8697
BiTT(ours) 0.0131 0.1044 28.40 0.9093
Novel PosesS2Hand 0.0280 0.1525 23.06 0.8092
HTML 0.0310 0.1299 23.46 0.8281
HARP 0.0256 0.1410 24.32 0.8419
BiTT(ours) 0.0223 0.1228 25.12 0.8423
Different ViewsS2Hand 0.0244 0.1512 24.22 0.8335
HTML 0.0291 0.1297 24.22 0.8375
HARP 0.0251 0.1367 24.49 0.8507
BiTT(ours) 0.0210 0.1273 26.34 0.8674
Table 2. Quantitative comparisons between compared methods in RGB2Hands [ 40] dataset. All training and testing images are randomly
selected. As RGB2Hands has no ground truth mesh, we used IntagHand [ 21] as an off-the-shelf model for two hand mesh reconstruction.
Evaluation Method L1 ↓ LPIPS ↓PSNR ↑SSIM ↑MS-SSIM ↑
Appearance ReconstructionS2Hand [ 6] 0.0179 0.0601 25.72 0.9459 0.9286
HTML [ 34] 0.0203 0.0923 24.42 0.8927 0.9075
HARP [ 17] 0.0155 0.0433 25.63 0.9309 0.9344
BiTT(ours) 0.0148 0.0683 26.02 0.9501 0.9323
Novel PosesS2Hand 0.0222 0.0778 24.22 0.9326 0.8991
HTML 0.0233 0.0961 23.25 0.8829 0.8900
HARP 0.0208 0.0758 23.88 0.9043 0.9042
BiTT(ours) 0.0196 0.0774 24.54 0.9352 0.9046
Our model is trained in end-to-end manner. For each
scene, our training process involves 700 epochs with a
learning rate decay by half every 200 steps, starting from
an initial rate of 0.001. Each scene training process is com-
pleted in less than 7 minutes.
Datasets. We mainly use the InterHand2.6M [ 28] dataset
which is composed of interacting two hands for both quan-
titative and qualitative evaluations. Since HARP [ 17] and
NeRF-based approaches [ 5,10,29] require hundreds of im-
ages per scene for training, can only involve a few scenes
to experiments. Requiring a single image, our experiments
are conducted through a total of 378 scenes (images) utiliz-
ing all 26 hand identities of the dataset. After training, we
evaluated its performance with different poses and different
views of the hands from the same identity. Speciﬁcally, we
evaluated with 8 distinct poses and 8 varying viewpoints for
each scene, resulting in a total of 6,048 testing images.
We also evaluated our model with the RGB2Hands [ 40]
dataset, however, since RGB2Hands does not present mul-
tiview images, we evaluated it with different pose images.
Across 300 scenes in the RGB2Hands dataset, we trainedour method and evaluated it with 40 different poses of im-
ages for each scene, thus 12,000 testing images in total.
For each dataset, we present the proportion of visible,
invisible, and usable symmetric texture pixels in each hand
texture at Tab. 3. Using symmetric information, we can ac-
quire up to 60% texture of each hand; otherwise, we have
only about 35% full texture available from the input images.
This demonstrates that using the symmetric information be-
tween both hands is reasonable for reconstructing two hand
textures from a single image.
Metrics. For the quantitative comparisons of different ap-
pearance models, we use a set of metrics that is often ap-
plied to assess the ﬁdelity and quality of rendered images.
We use the L1, learned perceptual image patch similarity
(LPIPS) [ 50], the structural similarity metric (SSIM) [ 41],
the multiscale structural similarity metric (MS-SSIM) [ 42],
and the peak signal-to-noise ratio (PSNR).
Compared Methods. We compare our method with
S2Hand [ 6] which reconstructs a single hand with per-
vertex texture rendering. HARP [ 17] and HTML [ 34] are
10731
Table 3. Pixel ratio comparison between left-hand, right-hand vis-
ible texture, usable symmetric texture, and invisible texture in our
experiment dataset. Colors refer to the Fig. 5mask label.
Dataset InterHand2.6M [ 28]RGB2Hands [ 40]
Left-hand visible texture 35.40% 39.72%
Right-hand visible texture 36.44% 39.06%
Usable symmetric texture 24.29% 10.21%
Invisible texture 19.89% 24.95%
Left Hand Masked UV Texture MapHand palm sideHand back side
Right Hand Masked UV Texture Map: Left-hand visible mask: Right-hand visible mask: Usable symmetric mask: Invisible mask
Hand palm sideHand back sideFigure 5. This ﬁgure shows the visible, invisible, usable symmet-ric texture mask on the UV texture map from an image.the methods that reconstruct a single hand with UV maprendering, and we compare the appearance quality amongthese methods. We modify S2Hand, HTML, and HARP totwo hands for comparison. These methods are extended toestimate each hand texture discretely while learning eachhand texture from a rendered image. Implicit function-based methods [5,7,10,29] are not included in the com-parison, as their methods are not straightforward to extendfor two hands, and their methods require at least hundredsof images per scene for training. For showing robustnesson geometric misalignment, we present two result tables;using ground truth mesh in all the compared methods atTab.1aand without using ground truth mesh in all meth-ods at Tab.1b. Where the ground truth mesh is unavailable,such as RGB2Hands data, we initialize hand meshes usingthe off-the-shelf method, IntagHand [21].4.2. Evaluation on Texture ReconstructionComparing with Prior Arts.We show the qualitative re-sults in Fig.6and the quantitative comparisons in Tab.1and Tab.2. The results show that BiTT signiﬁcantly outper-forms other baselines, especially in reconstructing the in-visible hand parts. HTML [34], S2Hand [6] are not able torepresent detailed appearances like vessels, wrinkles, andhair, whereas BiTT captures detailed personalized appear-ances using symmetric information. HARP [17] excels inthe visible side appearance (regarding Tab.1, Tab.2), but itlacks the capability to reconstruct the invisible sides, merelydisplaying a uniform color in those areas. Notably, BiTT re-mains robust to geometric misalignments, maintaining highGround TruthBiTT(ours)HARPHTMLS2Hand
Figure 6. Qualitative results of HTML [34], S2Hand [6], HARP[17], and BiTT rendered on novel-pose and viewpoint. The lasttwo rows pertain to the RGB2Hands [40] dataset, while the re-maining rows are from the InterHand2.6M [28] dataset.performance even without using GT as having the advan-tages of the parametric model. The performance gain onRGB2Hands Tab.2is relatively less signiﬁcant than on In-terHand2.6M [28]. This is due to the fact that RGB2Handsexhibits a lower percentage of usable symmetric texture, in-dicated in Tab.3. More results of BiTT are shown at Fig.7.We qualitatively compare HandNeRF [10] based on theresults reported in their paper, since there is no detailedexperiment description and the models/codes are not avail-able. As shown in Fig.8, even if our model is trained from asingle image, our model can capture the realistic texture ofboth hands comparably to HandNeRF. Note that HandNeRFis trained over hundreds of images from 10 views. Further-more, BiTT is able to render in different illuminations asshown in Fig.1and Fig.7.4.3. Ablation StudySymmetric information and Albedo Consistency Loss.We perform an ablation study of the use of symmetric in-formation in reconstructing hand texture. We replace the
10732
Reconstructed ImageRelit ImageNovel Pose2Novel ViewpointGround TruthTrained ImageGround Truth
Novel Pose1Ground Truth
Figure 7. Using only a single image input, our method reconstructs realistic detailed textures for both hands. We present some results
having different poses, viewpoints with corresponding ground truth images, and relighted images.
Table 4. Effects of coarse stage estimation, use of symmetric tex-
ture information (Sym. Tex.), and albedo consistency loss. Results
are the mean value evaluated on novel poses and viewpoints.
Coarse Stage Sym. Tex. Lalb LPIPS ↓PSNR ↑SSIM ↑
X 0.1329 25.36 0.8940
XX 0.1230 26.21 0.9163
XX X 0.1176 27.16 0.9199
bi-directional connection with a uni-directional connection
in BTR and Lsymis omitted. As shown in Tab. 4, not using
the symmetric information signiﬁcantly degrades the per-
formance of reconstructing invisible side appearances.
We also perform an ablation study for the albedo consis-
tency loss. In Tab. 4, the albedo consistency loss improves
the overall performance of the model by more precisely es-
timating the albedo image.
5. Conclusions
In this work, we presented a novel two-hand texture recon-
struction method from a single image called BiTT. First,
the bi-directional texture reconstructor is proposed to cre-
ate the full texture of both hands interactively. Second, we
introduce a way to use the texture parametric model for re-
covering texture. The experimental results demonstrate that
our method outperforms existing methods both qualitatively
HandNeRFBiTT (ours)
HandNeRFBiTT (ours)
Ground TruthGround Truth
Figure 8. Qualitative results of HandNeRF [ 10] and BiTT.
and quantitatively. We believe that our work can present a
realistic experience to users by accurately representing their
personalized hands in AR/VR applications.
Limitations and future work. One limitation is that al-
though BiTT is robust to the geometric misalignments
through the texture parametric model, there still exist in-
stances where seriously misaligned meshes cause a signif-
icant level of noise. Future work could involve the learn-
ing method that can reﬁne 3D meshes through detailed tex-
ture reconstruction. Another future work can be extend-
ing our system to detect and incorporate tattoos or acces-
sories on the invisible side of the hands through generative
networks[ 35], enhancing realism.
Acknowledgements. This work was in part supported
by NST grant (CRC 21011, MSIT), KOCCA grant
(R2022020028, MCST), IITP grant (RS-2023-00228996,
MSIT).
10733
References
[1]Thiemo Alldieck, Mihai Zanﬁr, and Cristian Sminchisescu.
Photorealistic monocular 3d reconstruction of humans wear-
ing clothing. In CVPR , 2022. 1,4
[2]Anil Armagan, Guillermo Garcia-Hernando, Seungryul
Baek, Shreyas Hampali, Mahdi Rad, Zhaohui Zhang,
Shipeng Xie, MingXiu Chen, Boshen Zhang, Fu Xiong,
Yang Xiao, Zhiguo Cao, Junsong Yuan, Pengfei Ren, Weit-
ing Huang, Haifeng Sun, Marek Hr ´uz, Jakub Kanis, Zden ˇek
Krˇnoul, Qingfu Wan, Shile Li, Linlin Yang, Dongheui Lee,
Angela Yao, Weiguo Zhou, Sijia Mei, Yunhui Liu, Adrian
Spurr, Umar Iqbal, Pavlo Molchanov, Philippe Weinzaepfel,
Romain Br ´egier, Gr ´egory Rogez, Vincent Lepetit, and Tae-
Kyun Kim. Measuring generalisation to unseen viewpoints,
articulations, shapes and objects for 3d hand pose estimation
under hand-object interaction. In ECCV , 2020. 1
[3]Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim. Push-
ing the envelope for rgb-based dense 3d hand pose estimation
via neural rendering. In CVPR , 2019. 1
[4]Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim.
Weakly-supervised domain adaptation via gan and mesh
model for estimating 3d hand poses interacting objects. In
CVPR , 2020. 1
[5]Xingyu Chen, Baoyuan Wang, and Heung-Yeung Shum.
Hand avatar: Free-pose hand animation and rendering from
monocular video. In CVPR , 2023. 2,3,4,5,6,7
[6]Yujin Chen, Zhigang Tu, Di Kang, Linchao Bao, Ying
Zhang, Xuefei Zhe, Ruizhi Chen, and Junsong Yuan. Model-
based 3d hand reconstruction via self-supervised learning. In
CVPR , 2021. 2,4,6,7
[7]Enric Corona, Tomas Hodan, Minh V o, Francesc Moreno-
Noguer, Chris Sweeney, Richard Newcombe, and Lingni
Ma. Lisa: Learning implicit shape and appearance of hands.
InCVPR , 2022. 1,2,4,5,7
[8]Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul
Baek, and Tae-Kyun Kim. First-person hand action bench-
mark with rgb-d videos and 3d hand pose annotations. In
CVPR , 2018. 1
[9]Shubham Goel, Angjoo Kanazawa, , and Jitendra Malik.
Shape and viewpoints without keypoints. In ECCV , 2020.
1,2
[10] Zhiyang Guo, Wengang Zhou, Min Wang, Li Li, and
Houqiang Li. Handnerf: Neural radiance ﬁelds for animat-
able interacting hands. In CVPR , 2023. 2,3,5,6,7,8
[11] Tao Hu, Liwei Wang, Xiaogang Xu, Shu Liu, and Jiaya Jia.
Self-supervised 3d mesh reconstruction from single images.
InCVPR , 2021. 2
[12] Tao Hu, Tao Yu, Zerong Zheng, He Zhang, Yebin Liu, and
Matthias Zwicker. Hvtr: Hybrid volumetric-textural render-
ing for human avatars. In 3DV, 2022. 1
[13] Zheheng Jiang, Hossein Rahmani, Sue Black, and Bryan M
Williams. A probabilistic attention model with occlusion-
aware texture regression for 3d hand reconstruction from a
single rgb image. In CVPR , 2023. 2
[14] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and
Jitendra Malik. Learning category-speciﬁc mesh reconstruc-
tion from image collections. In ECCV , 2018. 1,2[15] Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael
Black, Krikamol Muandet, and Tang. Siyu. Grasping ﬁeld:
Learning implicit representations for human grasps. In 3DV,
2020. 1
[16] Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar
Hilliges, and Siyu Tang. A skeleton-driven neural occupancy
representation for articulated hands. In 3DV, 2021. 1
[17] Korrawe Karunratanakul, Sergey Prokudin, Otmar Hilliges,
and Siyu Tang. Harp: Personalized hand reconstruction from
a monocular rgb video. In CVPR , 2023. 2,3,4,5,6,7
[18] Jihyun Lee, Junbong Jang, Donghwan Kim, Minhyuk Sung,
and Tae-Kyun Kim. Fourierhandﬂow: Neural 4d hand repre-
sentation using fourier query ﬂow. In NIPS , 2023. 1
[19] Jihyun Lee, Minhyuk Sung, Honggyu Choi, and Tae-Kyun
Kim. Im2hands: Learning attentive implicit representation
of interacting two-hand shapes. In CVPR , 2023. 1
[20] Biwen Lei, Jianqiang Ren, Mengyang Feng, Miaomiao Cui,
and Xuansong Xie. A hierarchical representation network for
accurate and detailed face reconstruction from in-the-wild
images. In CVPR , 2023. 1
[21] Mengcheng Li, Liang An, Hongwen Zhang, Lianpeng Wu,
Feng Chen, Tao Yu, and Yebin Liu. Interacting attention
graph for single image two-hand reconstruction. In CVPR ,
2022. 1,6,7
[22] Ruilong Li, Julian Tanke, Minh V o, Michael Zollhofer, Jur-
gen Gall, Angjoo Kanazawa, and Christoph Lassner. Tava:
Template-free animatable volumetric actors. In ECCV , 2022.
1
[23] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun
Jampani, Ming-Hsuan Yang, and Jan Kautz. Self-supervised
single-view 3d reconstruction via semantic consistency. In
ECCV , 2020. 1,2
[24] Yuwei Li, Longwen Zhang, Zesong Qiu, Yingwenqi Jiang,
Nianyi Li, Yuexin Ma, Yuyao Zhang, Lan Xu, and Jingyi Yu.
Nimble: A non-rigid hand model with bones and muscles.
ACM TOG , 2022. 2
[25] Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Zhen Lei, and Lei
Zhang. Otavatar: One-shot talking face avatar with control-
lable tri-plane rendering. In CVPR , 2023. 1
[26] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and
Andrea Vedaldi. Realfusion: 360 reconstruction of any ob-
ject from a single image. In CVPR , 2023. 4
[27] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. In ECCV , 2020. 1,4,5
[28] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori,
and Kyoung Mu Lee. Interhand2.6m: A dataset and baseline
for 3d interacting hand pose estimation from a single rgb im-
age. In ECCV , 2020. 2,6,7
[29] Akshay Mundra, Mallikarjun B R, Jiayi Wang, Marc Haber-
mann, Christian Theobalt, and Mohamed Elgharib. Live-
hand: Real-time and photorealistic neural hand rendering. In
ICCV , 2023. 2,3,4,5,6,7
[30] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya
Harada. Neural articulated radiance ﬁeld. In ICCV , 2021.
1
10734
[31] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo
Strauss, and Andreas Geiger. Texture ﬁelds: Learning tex-
ture representations in function space. In ICCV , 2019. 4
[32] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Soﬁen
Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo
Martin-Brualla. Nerﬁes: Deformable neural radiance ﬁelds.
InICCV , 2021. 1
[33] Bui Tuong Phong. Illumination for Computer Generated
Pictures . Association for Computing Machinery, 1998. 3,
4
[34] Neng Qian, Jiayi Wang, Franziska Mueller, Florian Bernard,
Vladislav Golyanik, and Christian Theobalt. HTML: A Para-
metric Hand Texture Model for 3D Hand Reconstruction and
Personalization. In ECCV , 2020. 2,4,6,7
[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. arxiv:2112.10752 , 2021.
8
[36] Javier Romero, Dimitrios Tzionas, and Michael J. Black.
Embodied hands: Modeling and capturing hands and bod-
ies together. ACM TOG , 2017. 4
[37] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned
implicit function for high-resolution clothed human digitiza-
tion. In ICCV , 2019. 4
[38] Soumyadip Sengupta, Angjoo Kanazawa, Carlos D. Castillo,
and David W. Jacobs. Sfsnet: Learning shape, refectance and
illuminance of faces in the wild. In CVPR , 2018. 1
[39] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-
3d: Towards single-view anything reconstruction in the wild.
arXiv:2304.10261 , 2023. 4
[40] Jiayi Wang, Franziska Mueller, Florian Bernard, Suzanne
Sorli, Oleksandr Sotnychenko, Neng Qian, Miguel A.
Otaduy, Dan Casas, and Christian Theobalt. Rgb2hands:
Real-time tracking of 3d hand interactions from monocular
rgb video. ACM TOG , 2020. 2,6,7
[41] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P
Simoncelli. Image quality assessment: from error visibility
to structural similarity. In IEEE TIP , 2004. 6
[42] Zhou Wang, Eero P Simoncelli, and Alan C Bovic. Image
quality assessment: from error visibility to structural similar-
ity.IEEE Transactions on Image Processing , 13(4):600–612,
2004. 6
[43] Shangzhe Wu, Ameesh Makadia, Jiajun Wu, Noah Snavely,
Richard Tucker, and Angjoo Kanazawa. De-rendering the
world’s revolutionary artefacts. In CVPR , 2021. 2
[44] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and
Michael J. Black. ICON: Implicit Clothed humans Obtained
from Normals. In CVPR , 2022. 1,4
[45] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. Shelf-
supervised mesh prediction in the wild. In CVPR , 2021. 1,
2
[46] Yufei Ye, Abhinav Gupta, and Shubham Tulsiani. What’s in
your hands? 3d reconstruction of generic objects in hands.
InCVPR , 2022. 1
[47] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance ﬁelds from one or few images. In
CVPR , 2021. 5[48] Chang Yu, Xiangyu Zhu, Xiaomei Zhang, Zhaoxiang Zhang,
and Zhen Lei. Graphics capsule: Learning hierarchical 3d
face representations from 2d images. In CVPR , 2023. 1
[49] Zhengdi Yu, Shaoli Huang, Fang Chen, Toby P. Breckon, and
Jue Wang. Acr: Attention collaboration-based regressor for
arbitrary two-hand reconstruction. In CVPR , 2023. 1
[50] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 6
10735
