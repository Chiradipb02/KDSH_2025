3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting
Zhiyin Qian1Shaofei Wang1,2,3Marko Mihajlovic1Andreas Geiger2,3Siyu Tang1
1ETH Z ¨urich2University of T ¨ubingen3T¨ubingen AI Center
50 FPSrenderingInput: a monocular video30 min.trainingOutput: animatable 3D Gaussian avatarNovel view synthesisNovel pose animation
Figure 1. 3DGS-Avatar. We develop an efficient method for creating animatable avatars from monocular videos, leveraging 3D Gaussian
Splatting [14]. Given a short sequence of dynamic human with a tracked skeleton and foreground masks, our method creates an avatar
within 30 minutes on a single GPU, supports animation and novel view synthesis at over 50 FPS , and achieves comparable or better
rendering quality to the state-of-the-art [57, 58] that requires over 8 GPU days to train, takes several seconds to render a single image, and
relies on pre-training on clothed human scans [57].
Abstract
We introduce an approach that creates animatable hu-
man avatars from monocular videos using 3D Gaussian
Splatting (3DGS). Existing methods based on neural radi-
ance fields (NeRFs) achieve high-quality novel-view/novel-
pose image synthesis but often require days of training, and
are extremely slow at inference time. Recently, the com-
munity has explored fast grid structures for efficient train-
ing of clothed avatars. Albeit being extremely fast at train-
ing, these methods can barely achieve an interactive ren-
dering frame rate with around 15 FPS. In this paper, we use
3D Gaussian Splatting and learn a non-rigid deformation
network to reconstruct animatable clothed human avatars
that can be trained within 30 minutes and rendered at real-
time frame rates (50+ FPS). Given the explicit nature of
our representation, we further introduce as-isometric-as-
possible regularizations on both the Gaussian mean vectors
and the covariance matrices, enhancing the generalization
of our model on highly articulated unseen poses. Experi-
mental results show that our method achieves comparable
and even better performance compared to state-of-the-art
approaches on animatable avatar creation from a monoc-
ular input, while being 400x and 250x faster in training
and inference, respectively. Please see our project page at
https://neuralbodies.github.io/3DGS-Avatar.1. Introduction
Reconstructing clothed human avatars from image inputs
presents a significant challenge in computer vision, yet
holds immense importance due to its applications in virtual
reality, gaming, and e-commerce. Traditional methods of-
ten rely on dense, synchronized multi-view inputs, which
may not be readily available in more practical scenarios.
Recent advances in implicit neural fields [27, 30, 32, 36,
47, 50, 51, 53, 55, 65, 66] have enabled high-quality recon-
struction of geometry [8, 38, 57, 61] and appearance [13,
20, 22, 31, 35, 37, 42, 58, 70] of clothed human bodies
from sparse multi-view or monocular videos. Animation
of such reconstructed clothed human bodies is also possible
by learning the geometry and appearance representations in
a predefined canonical pose [13, 20, 35, 57, 58, 70].
To achieve state-of-the-art rendering quality, existing
methods rely on training a neural radiance field (NeRF) [27]
combined with either explicit body articulation [8, 12, 13,
20, 35, 38, 57, 58, 70] or conditioning the NeRF on hu-
man body related encodings [31, 37, 48, 61]. They often
employ large multi-layer perceptrons (MLPs) to model the
neural radiance field, which are computationally demand-
ing, leading to prolonged training (days) and inference (sec-
onds) time. This computational expense poses a significant
challenge for practical applications of these state-of-the-art
methods in real-time applications.
With recent advances in efficient learning of implicit
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5020
neural fields, training time of NeRFs has been reduced to
minutes [3, 29, 46, 52 ?]. There are also works target-
ing fast inference of pretrained NeRFs [43, 67, 69]. In-
spired by these developments, several avatar reconstruction
methods have been tailored to fast training [7, 12] or fast
inference [6, 17, 39]. However, to the best of our knowl-
edge, there currently exists no method that simultaneously
achieves both fast training and real-time inference for ani-
matable avatar reconstruction from just monocular videos.
Point-based rendering [44, 49, 62, 71, 73 ?, 74] has
emerged as an efficient alternative to NeRFs for fast infer-
ence. With the recently proposed 3D Gaussian Splatting
(3DGS) [14], it is possible to achieve state-of-the-art ren-
dering quality using only a fraction of NeRFs’ inference
time and comparatively fast training for static scene recon-
struction.
Leveraging the capabilities of 3DGS, we demonstrate
its application in modeling animatable clothed avatars us-
ing monocular videos. Our approach effectively integrates
rigid human articulation with a non-rigid deformation field
within the 3DGS framework. We use a small multi-layer
perceptron (MLP) to decode color. This MLP is designed
to be responsive to local non-rigid deformations and dy-
namic lighting conditions, ensuring a more realistic and re-
sponsive rendering of the avatar’s appearance. Furthermore,
we apply as-isometric-as-possible regularizations [15, 41]
toboth the Gaussian mean vectors and the covariance ma-
trices, which helps maintain the geometric consistency and
realistic deformation of the avatar, particularly in dynamic
and varied poses.
Our experimental results show that our method is com-
parable to or better than current state-of-the-art [57, 58] in
animatable avatar creation from monocular inputs, achiev-
ing training speed 400 times faster and inference speed 250
times quicker. Compared to methods that focus on fast
training [7, 12], our method, despite being slower in train-
ing, can model pose-dependent non-rigid deformation and
produce significantly better rendering quality, while being 3
times faster in terms of rendering. We provide an overview
of the comparison to major prior works in Tab. 1. In sum-
mary, our work makes the following contributions:
• We introduce 3D Gaussian Splatting to animatable human
avatars reconstruction from monocular videos.
• We develop a simple yet effective deformation network
as well as regularization terms that effectively drive 3D
Gaussian Splats to handle highly articulated and out-of-
distribution poses.
• Our method is the first, to our knowledge, to simul-
taneously deliver high-quality rendering, model pose-
dependent non-rigid deformation, generalize effectively
to unseen poses, and achieve fast training (less than 30
minutes) and real-time rendering speed (50+ FPS).pose-dependent deformation novel pose animation fast training real-time rendering monocular input
✗ ✗ ✗ ✗ ✓ NeuralBody [37]
✓ ✗ ✗ ✗ ✓ HumanNeRF [58]
✓ ✓ ✗ ✗ ✓ ARAH [57]
✓ ✗ ✓ ✓ ✗ ✓ Instant-NVR [7]
✗ ✓ ✓ ✓ ✗ ✓ InstantAvatar [12]
✓ ✓ ✗ ✗ ✓ MonoHuman [70]
✓ ✓ ✗ ✗ ✗ UV-V olumes [6]
✓ ✓ ✗ ✓ ✗ DELIFFAS [17]
✓ ✓ ✓ ✓ ✓ 3DGS-Avatar (Ours)
Table 1. Comparison to SoTA. Instant-NVR [7] and InstantA-
vatar [12] achieve instant training within 5minutes. For real-time
rendering, we require a frame rate over 30 FPS. Note that while
UV-V olumes [6] claims real-time freeview rendering, they only
achieve 14FPS on novel pose synthesis due to the slow genera-
tion of their UV V olume.
2. Related Works
Neural rendering for clothed human avatars. Since the
seminal work of Neural Radiance Fields (NeRF) [27], there
has been a surge of research on neural rendering for clothed
human avatars. The majority of the works focus on either
learning a NeRF conditioned on human body related encod-
ings [31, 48, 61], or learning a canonical NeRF representa-
tion and warp camera rays from the observation space to the
canonical space to query radiance and density values from
the canonical NeRF [8, 12, 13, 20, 35, 38, 57, 58, 70]. Most
of these works rely on large multi-layer perceptrons (MLPs)
to model the underlying neural radiance field, which are
computationally expensive, resulting in prolonged training
(days) and inference (seconds) time.
With recent advances in accelerated data structures for
neural fields, there has been several works targeting fast in-
ference and fast training of NeRFs for clothed humans. [12]
proposes to use iNGP [29] as the underlying representation
for articulated NeRFs, which enables fast training (less than
5 minutes) and interactive rendering speed (15 FPS) but ig-
nores pose-dependent non-rigid deformations. [7] also uti-
lizes iNGP and represents non-rigid deformations in the UV
space, which enables fast training and modeling of pose-
dependent non-rigid deformations. However, as we will
show in our experiments, [7]’s parametrization of non-rigid
deformations result in blurry renderings. [6] proposes to
generate a pose-dependent UV volume for efficient free-
view synthesis. However, their UV-volume generation pro-
cess is slow (20 FPS), making novel pose synthesis less effi-
cient (only 14 FPS). [17] also employs UV-based rendering
5021
to achieve real-time rendering of dynamic clothed humans,
but only works on dense multi-view inputs. Extending [69],
[54, 72] applied Fourier transform for compressing human
performance capture data, albeit with limitations on dense
multi-view data (60-80 views) and non-generalizability of
the Fourier basis representation to unseen poses beyond the
training dataset. In contrast to all these works, our method
achieves state-of-the-art rendering quality and speed with
less than 30 minutes of training time from a single monoc-
ular video input.
Dynamic 3D gaussians. Point-based rendering [40, 44, 49,
62, 71, 73, 74] has also been shown to be an efficient alter-
native to NeRFs for fast inference and training. Extend-
ing point cloud to 3D Gaussians, 3D Gaussian Splatting
(3DGS) [14] models the rendering process as splatting a
set of 3D Gaussians onto image plane via alpha blending,
achieving state-of-the-art rendering quality with real-time
inference speed and fast training given multi-view inputs.
Given the great performance on both quality and speed
of 3DGS, a rich set of works has further explored the
3D Gaussian representation for dynamic scene reconstruc-
tion. [14] proposed to optimize the position and shape of
each 3D Gaussian on a frame-by-frame basis and simulta-
neously performed 6-DOF dense tracking for free. Their
model size, however, increases with the temporal dimen-
sion. [59, 63] maintain a single set of 3D Gaussians in a
canonical space and deform them to each frame via learn-
ing a time-dependent deformation field, producing state-of-
the-art results in terms of both rendering quality and speed.
[64] augments 3D Gaussians with temporal dimension into
4D Gaussian primitives to approximate the underlying spa-
tiotemporal 4D volume of the dynamic scene. While such
methods show promising results, they are only applicable
to either synthetic datasets with fast camera movement and
slow object motion or forward-facing real scenes with lim-
ited object movements, thus unable to handle the immense
displacement of the articulated human body. To address
this problem, our approach utilizes a statistical human body
model [24] for articulation and applies regularization to re-
duce the overfitting of the deformation field.
Concurrent works. Concurrent with our method, many
recent works also seek to combine 3DGS with human ar-
ticulation prior for avatar reconstruction. We provide a
comparison of our approach to concurrent works in Tab. 2.
D3GA [75] proposed to embed 3D Gaussians in tetrahe-
dral cages and utilize cage deformations for drivable avatar
animation. However, they use dense calibrated multi-view
videos as input and require an additional 3D scan to gener-
ate the tetrahedral mesh template. Li et al. [21] focused on
generating avatars with a detailed appearance from multi-
view videos by post-processing radiance field renderings
with 2D CNNs, which limits their rendering speed. Along
with [11, 28], these works fail to achieve fast training withpose-dependent deformation novel pose animation fast training real-time rendering monocular input
✓ ✓ ✗ ✓ ✗ D3GA [75]
✓ ✓ ✗ ✗ ✗ Liet al. [21]
✓ ✓ ✗ ✓ ✓ SplatArmor [11]
✓ ✓ ✗ ✗ ✗ Moreau et al. [28]
✓ ✓ ✓ ✓ ✗ Yeet al. [68]
✗ ✓ ✓ ✓ ✓ HUGS [16]
✗ ✓ ✓ ✓ ✓ ✓ ✓ GART [18]
✗ ✓ ✓ ✓ ✓ ✓ ✓ Liuet al. [23]
✗ ✗ ✓ ✓ ✓ ✓ ✓ GauHuman [10]
✓ ✓ ✓ ✓ ✓ 3DGS-Avatar (Ours)
Table 2. Comparison to Concurrent Works.
relatively complex pipelines. Similar to our approach, Ye et
al. [68] deforms 3D Gaussians in canonical space via pose-
dependent deformation and rigid articulation, but they still
require 2 hours for training and do not show results on
monocular inputs. HUGS [16] learns a background model
along with the animatable human avatar, but they fail to
take pose-dependent cloth deformation into account. Sev-
eral other works [10, 18, 23] also neglect pose-dependent
cloth deformation to achieve even faster training (in 5 min-
utes) and rendering (150+ FPS). We argue that our method
strikes a good balance between quality and speed compared
to concurrent works, as being the only method simultane-
ously achieving the properties listed in Tab. 2.
3. Preliminaries
Linear Blend Skinning. To model human articulations, a
widely adopted paradigm is to represent geometry and ap-
pearance in a shared canonical space [8, 12, 13, 20, 35,
38, 57, 58] and use Linear Blend Skinning (LBS) [2, 9,
24, 33, 34, 60] to deform the parametric human body un-
der arbitrary poses. Given a point xcin canonical space,
the LBS function takes a set of rigid bone transformations
{Bb}B
b=1and computes its correspondence xoin the obser-
vation space:
xo=LBS σw(xc;{Bb}) (1)
Assuming an underlying SMPL model, we use a total of
B= 24 bone transformations, each represented by a 4×4
rotation-translation matrix, which are then linearly blended
via a set of skinning weights w∈[0,1]B, s.t.PB
b=1wb=
1, modeled by a coordinate-based neural skinning field
fσw(xc)[4, 5, 26, 45, 56]. The forward linear blend skin-
5022
ning function can thus be formulated as:
xo=LBS σw(xc;{Bb}) =XB
b=1fσw(xc)bBbxc(2)
Compared to prior works that search canonical correspon-
dences of points in observation space [12, 57, 58], our
method requires no inverse skinning which is typically diffi-
cult to compute and often leads to multiple solutions [4, 5].
A similar technique has been employed in [73] for face
avatar modeling.
3D Gaussian Splatting. 3DGS [14] utilizes a set of
3D Gaussian primitives {G} as static scene representation
which can be rendered in real-time via differentiable ras-
terization. Each 3D Gaussian Gis defined by its mean x,
covariance Σ, opacity αand view-dependent color repre-
sented by spherical harmonics coefficients f. To ensure pos-
itive semi-definiteness, the covariance matrix is represented
by a scaling matrix Sand rotation matrix R. In practice, we
store the diagonal vector s∈R3of the scaling matrix and
a quaternion vector q∈R4to represent rotation, which can
be trivially converted to a valid covariance matrix.
The 3D Gaussians are projected to the 2D image plane
during the rendering process and accumulated via alpha
blending. Given a viewing transformation Wand the Ja-
cobian of the affine approximation of the projective trans-
formation J, the 2D covariance matrix in camera coordi-
nate [76] is given by Σ′= 
JWΣWTJT
1:2,1:2. The
pixel color Cis thus computed by blending 3D Gaussian
splats that overlap at the given pixel, sorted according to
their depth:
C=X
i
α′
iYi−1
j=1(1−α′
j)
ci (3)
where α′
idenotes the learned opacity αiweighted by the
probability density of i-th projected 2D Gaussian at the tar-
get pixel location. cdenotes the view-dependent color com-
puted from stored SH coefficients f.
The 3D Gaussians {G} are optimized via a photomet-
ric loss. During optimization, 3DGS adaptively controls
the number of 3D Gaussians via periodic densification and
pruning, achieving self-adaptive convergence to an optimal
density distribution of 3D Gaussians that well represents the
scene.
4. Methods
We illustrate the pipeline of our method in Fig. 2. The input
to our method is a monocular video with a calibrated cam-
era, fitted SMPL parameters, and foreground masks. Our
method optimizes a set of 3D Gaussians in canonical space,
which is then deformed to the observation space and ren-
dered from the given camera. For a set of 3D Gaussians
{G(i)}N
i=1, we store the following properties at each point:position x, scaling factor s, rotation quaternion q, opac-
ityαand a color feature vector f. We start by randomly
sampling N= 50 kpoints on the canonical SMPL [24]
mesh surface as initialization of canonical 3D Gaussians
{Gc}. Inspired by HumanNeRF [58], we decompose the
complex human deformation into a non-rigid part that en-
codes pose-dependent cloth deformation, and a rigid trans-
formation controlled by the human skeleton.
4.1. Pose-dependent Non-rigid Deformation
We formulate the non-rigid deformation module as:
{Gd}=Fθnr({Gc};Zp) (4)
where {Gd}represents the non-rigidly deformed 3D Gaus-
sians. θnrrepresents the learnable parameters of the non-
rigid deformation module. Zpis a latent code which en-
codes SMPL pose and shape (θ, β)using a lightweight hi-
erarchical pose encoder [26]. Specifically, the deformation
network fθnrtakes the canonical position xc, the pose latent
codeZpas inputs and outputs the offsets of the Gaussian’s
position, scale, rotation, along with a feature vector z:
(δx, δs, δq,z) =fθnr(xc;Zp) (5)
We use a multi-level hash grid [29] to encode 3D positions
as spatial features, which are then concatenated with the
pose latent code Zpand fed into a shallow MLP with 2 hid-
den layers and a width of 128. The canonical Gaussian is
deformed by:
xd=xc+δx (6)
sd=sc·exp(δs) (7)
qd=qc·[1, δq 1, δq 2, δq 3] (8)
note that the ·operator on quaternions is equivalent to multi-
plying the two rotation matrices derived by the two quater-
nions. Since the quaternion [1,0,0,0]corresponds to the
identity rotation matrix, we have qd=qcwhen δq=0.
4.2. Rigid Transformation
We further transform the non-rigidly deformed 3D Gaus-
sians{Gd}to the observation space via a rigid transforma-
tion module:
{Go}=Fθr({Gd};{Bb}B
b=1) (9)
where a skinning MLP fθris learned to predict skinning
weights at the position xd. We transform the position and
the rotation matrix of 3D Gaussians via forward LBS:
T=XB
b=1fθr(xd)bBb (10)
xo=Txd (11)
Ro=T1:3,1:3Rd (12)
where Rdis the rotation matrix derived from the quaternion
qd.
5023
Non-rigid	Deformationℱ!!"(Sec.	4.1)Canonical	3D	Gaussians	{𝒢"}
Canonical	SpaceObservation	SpaceRigid	Transformationℱ!"(Sec.	4.2)Color	MLPℱ!#(Sec.	4.3)𝐳𝐳Latent	Code	𝒵"Ray	Direction𝐝𝒢#Optimizable	SMPLParameters	𝜃$𝒵${𝚩%}𝛿𝐱,𝛿𝐬,𝛿𝐪𝒢+𝐱"𝐱+𝑐𝐟Diff.	GaussianRasterization	(Eq.	5)𝒢"Rendered	Image
Figure 2. Our framework for creating animatable avatars from monocular videos . We first initialize a set of 3D Gaussians in the
canonical space via sampling points from a SMPL mesh. Each canonical Gaussian Gcgoes through a non-rigid deformation module Fθnr
conditioned on an encoded pose vector Zp(Sec. 4.1) to account for pose-dependent non-rigid cloth deformation. This module outputs a
non-rigidly deformed 3D Gaussian Gdand a pose-dependent latent feature z. The non-rigidly deformed 3D Gaussian Gdis transformed
to the observation space Go(Sec. 4.2) via LBS with learned neural skinning Fθr. The Gaussian feature f, the pose-dependent feature z,
a per-frame latent code Zc, and the ray direction dare propagated through a small MLP Fθcto decode the view-dependent color cfor
each 3D Gaussian. Finally, the observation space 3D Gaussians {Go}and their respective color values are accumulated via differentiable
Gaussian rasterization (Eq. (3)) to render the image.
4.3. Color MLP
Prior works [59, 63, 64] follow the convention of
3DGS [14], which stores spherical harmonics coefficients
per 3D Gaussian to encode the view-dependent color. Treat-
ing the stored color feature fas spherical harmonics coeffi-
cients, the color of a 3D Gaussian can be computed by the
dot product of the spherical harmonics basis and the learned
coefficients: c=⟨γ(d),f⟩, where drepresents the viewing
direction, derived from the relative position of the 3D Gaus-
sian wrt. the camera center and γdenotes the spherical har-
monics basis function.
While conceptually simple, we argue that this approach
does not suit our monocular setting. Since only one camera
view is provided during training, the viewing direction in
the world space is fixed, leading to poor generalization to
unseen test views. Similar to [38], we use the inverse rigid
transformation from Sec. 4.2 to canonicalize the viewing
direction: ˆd=T−1
1:3,1:3d, where Tis the forward transfor-
mation matrix defined in Eq. (10). Theoretically, canoni-
calizing viewing direction also promotes consistency of the
specular component of canonical 3D Gaussians under rigid
transformations.
On the other hand, we observe that the pixel color of
the rendered clothed human avatar also largely depends on
local deformation. Local fine wrinkles on clothes, for in-
stance, would cause self-occlusion that heavily affects shad-
ing. Following [37], we also learn a per-frame latent code
Zcto compensate for different environment light effects
across frames caused by the global movement of the subject.
Hence, instead of learning spherical harmonic coefficients,
we enhance color modeling by learning a neural network
that takes per-Gaussian color feature vector f∈R32, local
pose-dependent feature vector z∈R16from the non-rigid
deformation network, per-frame latent code Zc∈R16, andspherical harmonics basis of canonicalized viewing direc-
tionγ(ˆd)with a degree of 3as input and predicts the color
of the 3D Gaussian:
c=Fθc(f,z,Zc,γ(ˆd)) (13)
In practice, we find a tiny MLP with one 64-dimension hid-
den layer sufficient to model the appearance. Increasing the
size of the MLP leads to overfitting and performance drop.
4.4. Optimization
We jointly optimize canonical 3D Gaussians {Gc}and the
parameters θnr, θr, θcof the non-rigid deformation net-
work, the skinning network and the color network, respec-
tively.
Pose correction. SMPL [24] parameter fittings from im-
ages can be inaccurate. To address this, we additionally
optimize the per-sequence shape parameter as well as per-
frame translation, global rotation, and local joint rotations.
We initialize these parameters θpwith the given SMPL pa-
rameters and differentiably derive the bone transformations
{Bb}as input to the network, enabling direct optimization
via backpropagation.
As-isometric-as-possible regularization. With monocu-
lar video as input, only one view of the human is visible
in each frame, making it extremely hard to generalize to
novel views and novel poses. Considering the sparsity of
input, the non-rigid deformation network is highly under-
constrained, resulting in noisy deformation from the canon-
ical space to the observation space. Inspired by [41], we
leverage the as-isometric-as-possible constraint [15] to re-
strict neighboring 3D Gaussian centers to preserve a similar
distance after deformation. We further augment the con-
5024
straint to Gaussian covariance matrices:
Lisopos =NX
i=1X
j∈Nk(i)d(x(i)
c,x(j)
c)−d(x(i)
o,x(j)
o)
(14)
Lisocov =NX
i=1X
j∈Nk(i)d(Σ(i)
c,Σ(j)
c)−d(Σ(i)
o,Σ(j)
o)
(15)
where Ndenotes the number of 3D Gaussians. Nkdenotes
the k-nearest neighbourhood, and we set kto5. We use
L2-norm as our distance function d(·,·).
Loss function. Our full loss function consists of a RGB loss
Lrgb, a mask loss Lmask , a skinning weight regularization
lossLskin and the as-isometric-as-possible regularization
loss for both position and covariance Lisopos ,Lisocov . For
further details of the loss definition and respective weights,
please refer to the Supp.Mat.
5. Experiments
In this section, we first compare the proposed approach with
recent state-of-the-art methods [7, 12, 37, 57, 58], demon-
strating that our proposed approach achieves superior ren-
dering quality in terms of LPIPS, which is more informa-
tive under monocular setting, while achieving fast training
and real-time rendering speed, respectively 400x and 250x
faster than the most competitive baseline [58]. We then sys-
tematically ablate each component of the proposed model,
showing their effectiveness in better rendering quality.
5.1. Evaluation Dataset
ZJU-MoCap [37]. This is the major testbed for quanti-
tative evaluation. We pick six sequences (377, 386, 387,
392, 393, 394) from the ZJU-MoCap dataset and follow
the training/test split of HumanNeRF [58]. The motion of
these sequences is repetitive and does not contain a suffi-
cient number of poses for meaningful novel pose synthesis
benchmarks. Thus we focus on evaluating novel view syn-
thesis (PSNR/SSIM/LPIPS) and show qualitative results for
animation on out-of-distribution poses. Note that LPIPS in
all the tables are scaled up by 1000 .
PeopleSnapshot [1]. We also conduct experiments on 4
sequences of the PeopleSnapshot dataset, which includes
monocular videos of people rotating in front of a camera.
We follow the data split of InstantAvatar [12] and compare
to [12] on novel pose synthesis. For fair comparison, we
use the provided poses optimized by Anim-NeRF [35] and
do not further optimize it during our training.
5.2. Comparison with Baselines
We compare our approach with NeuralBody [37], Human-
NeRF [58], MonoHuman [70], ARAH [57] and Instant-NVR [7] under monocular setup on ZJU-MoCap. The
quantitative results are reported in Tab. 3. NeuralBody is
underperforming compared to other approaches. Overall,
our proposed approach produces comparable performance
to ARAH on PSNR and SSIM, while significantly outper-
forming all the baselines on LPIPS. We argue that LPIPS
is more informative compared to the other two metrics, as
it is very difficult to reproduce exactly the ground-truth ap-
pearance for novel views due to the monocular setting and
the stochastic nature of cloth deformations. Meanwhile, our
method is also capable of fast training and renders at a real-
time rendering frame rate, being 400 times faster for train-
ing (30 GPU minutes vs.8 GPU days) and 250−500times
faster for inference (50 FPS vs.0.1 FPS for ARAH and
0.2 FPS for HumanNeRF). We also note that Instant-NVR
trains on a refined version of ZJU-MoCap, which provides
refined camera parameters, SMPL fittings, and more accu-
rate instance masks with part-level annotation that is essen-
tial for running their method. Hence their metrics are not
directly comparable to other methods in Tab. 3. We train
our model on the refined dataset for a fair quantitative com-
parison, which clearly shows that our method outperforms
Instant-NVR in most scenarios.
Qualitative comparisons on novel view synthesis can be
found in Fig. 3. We observe that our method preserves
sharper details compared to ARAH and does not produce
fluctuating artifacts as in HumanNeRF caused by noisy de-
formation fields. Instant-NVR produces an oversmooth ap-
pearance and tends to generate noisy limbs. Additionally,
we animate our learned avatars with pose sequences from
AMASS [25] and AIST++ [19], shown in the rightmost col-
umn of Fig. 3. This shows that our model could generalize
to extreme out-of-distribution poses.
For PeopleSnapshot, we report the quantitative compari-
son against InstantAvatar [12] in Tab. 4. Our approach sig-
nificantly outperforms InstantAvatar on PSNR and LPIPS,
while being more than 3x faster during inference.
5.3. Ablation Study
We study the effect of various components of our method
on the ZJU-MoCap dataset, including the color MLP, the
as-isometric-as-possible regularization and the pose correc-
tion module. The average metrics over 6sequences are re-
ported in Tab. 5. We show that all proposed techniques are
required to reach the optimal performance, best reflected by
LPIPS which is the most informative metric for novel view
synthesis evaluation under a monocular setup.
We further show qualitative comparison on out-of-
distribution poses in Fig. 4, which demonstrates that the as-
isometric-as-possible loss helps to constrain the 3D Gaus-
sians to comply with consistent movement during deforma-
tion, hence improving generalization on novel poses. Al-
beit marginally, each individual component contributes to a
5025
GTTraining:Inference:Ours30 min.50+ FPSHumanNeRF>8 days0.2 FPSARAH8 days0.1 FPSInstant-NVR5 min.3 FPSOursNovel view synthesisNovel poseanimation
Figure 3. Qualitative Comparison on ZJU-MoCap [37]. We show the results for both novel view synthesis and novel pose animation of
all sequences on ZJU-MoCap. Our method produces high-quality results that preserve cloth details even on out-of-distribution poses.
5026
Table 3. Quantitative Results on ZJU-MoCap [37]. We outperform both competitive baselines [57, 58] in terms of LPIPS while being
two orders of magnitude faster in training and rendering. Cell color indicates best and second best. Instant-NVR [7] is trained and tested
on a refined version of ZJU-MoCap, thus is not directly comparable to other baselines quantitatively. We train our model on the refined
dataset for fair quantitative comparison to Instant-NVR and the metrics are reported in the last two rows of the table.
Subject: 377 386 387 392 393 394
Metric: GPU ↓FPS↑PSNR ↑SSIM↑LPIPS ↓PSNR ↑SSIM↑LPIPS ↓PSNR ↑SSIM↑LPIPS ↓PSNR ↑SSIM↑LPIPS ↓PSNR ↑SSIM↑LPIPS ↓PSNR ↑SSIM↑LPIPS ↓
NeuralBody [37] 12h 2 29.11 0.9674 40.95 30.54 0.9678 46.43 27.00 0.9518 59.47 30.10 0.9642 53.27 28.61 0.9590 59.05 29.10 0.9593 54.55
HumanNeRF [58] >8d 0.2 30.41 0.9743 24.06 33.20 0.9752 28.99 28.18 0.9632 35.58 31.04 0.9705 32.12 28.31 0.9603 36.72 30.31 0.9642 32.89
MonoHuman [70] 4d 0.1 29.12 0.9727 26.58 32.94 0.9695 36.04 27.93 0.9601 41.76 29.50 0.9635 39.45 27.64 0.9566 43.17 29.15 0.9595 38.08
ARAH [57] 8d 0.1 30.85 0.9800 26.60 33.50 0.9781 31.40 28.49 0.9656 40.43 32.02 0.9742 35.28 28.77 0.9645 42.30 29.46 0.9632 40.76
Ours 0.5h 50 30.64 0.9774 20.88 33.63 0.9773 25.77 28.33 0.9642 34.24 31.66 0.9730 30.14 28.88 0.9635 35.26 30.54 0.9661 31.21
Instant-NVR* [7] 0.1h 3 31.28 0.9789 25.37 33.71 0.9770 32.81 28.39 0.9640 45.97 31.85 0.9730 39.47 29.56 0.9641 46.16 31.32 0.9680 40.63
Ours* 0.5h 50 30.96 0.9778 19.85 33.94 0.9784 24.70 28.40 0.9656 32.96 32.10 0.9739 29.20 29.30 0.9645 34.03 30.74 0.9662 31.00
Table 4. Quantitative Results on PeopleSnapshot [1].
Subject: female-3-casual female-4-casual male-3-casual male-4-casual
Metric: GPU ↓FPS↑PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓
InstantAvatar [12] 5 min. 15 27.66 0.9709 21.00 29.11 0.9683 16.70 29.53 0.9716 15.50 27.67 0.9626 30.7
Ours 45 min. 50 30.57 0.9581 20.86 33.16 0.9678 15.74 34.28 0.9724 14.92 30.22 0.9653 23.05
Full modelw/o ℒ!"#$#%,ℒ!"#&#"
Figure 4. Ablation Study on as-isometric-as-possible regularization, which removes the artifacts on highly articulated poses.
Table 5. Ablation Study on ZJU-MoCap [37]. The proposed
model achieves the lowest LPIPS, demonstrating the effectiveness
of all components.
Metric: PSNR ↑SSIM↑LPIPS↓
Full model 30.61 0.9703 29.58
w/o color MLP 30.55 0.9700 31.24
w/oLisocov 30.61 0.9703 29.84
w/oLisopos,Lisocov 30.59 0.9699 30.25
w/o pose correction 30.60 0.9703 29.87
better novel-view rendering quality and particularly gener-
ates more plausible results with respect to novel pose ani-
mation.
6. Conclusion
In this paper, we present 3DGS-Avatar, one of the first
methods that utilize the explicit representation of 3DGSfor efficient reconstruction of clothed human avatars from
monocular videos. Our method achieves photorealistic ren-
dering, awareness of pose-dependent cloth deformation,
generalization to unseen poses, fast training, and real-time
rendering all at once.
Experiments show that our method is comparable to or
even better than the state-of-the-art methods in terms of ren-
dering quality while being two orders of magnitude faster in
both training and inference. Furthermore, we propose to re-
place spherical harmonics with a shallow MLP to decode
3D Gaussian color and regularize deformation with geo-
metric constraints, both proved to be effective in enhanc-
ing rendering quality. We hope that our new representation
could foster further research in fast, high-quality animatable
clothed human avatar synthesis from a monocular view.
Acknowledgement. SW and AG were supported by the
ERC Starting Grant LEGO-3D (850533) and the DFG EXC
number 2064/1 - project number 390727645. SW and ST
acknowledge the SNSF grant 200021 204840.
5027
References
[1] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian
Theobalt, and Gerard Pons-Moll. Video based reconstruction
of 3d people models. In Proc. of CVPR , 2018. 6, 8
[2] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-
bastian Thrun, Jim Rodgers, and James Davis. Scape: shape
completion and animation of people. ACM Transasctions
Graphics , 24, 2005. 3
[3] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance fields. In Proc. of
ECCV , 2022. 2
[4] Xu Chen, Yufeng Zheng, Michael Black, Otmar Hilliges, and
Andreas Geiger. Snarf: Differentiable forward skinning for
animating non-rigid neural implicit shapes. In Proc. of ICCV ,
2021. 3, 4
[5] Xu Chen, Tianjian Jiang, Jie Song, Max Rietmann, Andreas
Geiger, Michael J. Black, and Otmar Hilliges. Fast-snarf: A
fast deformer for articulated neural fields. Pattern Analysis
and Machine Intelligence (PAMI) , 2023. 3, 4
[6] Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, Xiaoyu Li,
Yu Guo, Jue Wang, and Fei Wang. Uv volumes for real-time
rendering of editable free-view human performance. In Proc.
of CVPR , 2023. 2
[7] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei
Zhou. Learning neural volumetric representations of dy-
namic humans in minutes. In Proc. of CVPR , 2023. 2, 6,
8
[8] Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and Otmar
Hilliges. Vid2avatar: 3d avatar reconstruction from videos in
the wild via self-supervised scene decomposition. In Proc.
of CVPR , 2023. 1, 2, 3
[9] N. Hasler, C. Stoll, M. Sunkel, B. Rosenhahn, and H.-P. Sei-
del. A Statistical Model of Human Pose and Body Shape.
Computer Graphics Forum , 28:337–346, 2009. 3
[10] Shoukang Hu and Ziwei Liu. Gauhuman: Articulated gaus-
sian splatting from monocular human videos. In Proc. of
CVPR , 2024. 3
[11] Rohit Jena, Ganesh Subramanian Iyer, Siddharth Choud-
hary, Brandon Smith, Pratik Chaudhari, and James Gee.
Splatarmor: Articulated gaussian splatting for animat-
able humans from monocular rgb videos. arXiv preprint
arXiv:2311.10812 , 2023. 3
[12] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. In-
stantavatar: Learning avatars from monocular video in 60
seconds. In Proc. of CVPR , 2023. 1, 2, 3, 4, 6, 8
[13] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel,
and Anurag Ranjan. Neuman: Neural human radiance field
from a single video. In Proc. of ECCV , 2022. 1, 2, 3
[14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,
and George Drettakis. 3d gaussian splatting for real-time
radiance field rendering. ACM Transactions on Graphics , 42
(4), 2023. 1, 2, 3, 4, 5
[15] Martin Kilian, Niloy J. Mitra, and Helmut Pottmann. Ge-
ometric modeling in shape space. ACM Transactions on
Graphics (SIGGRAPH) , 26(3), 2007. 2, 5[16] Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel,
Oncel Tuzel, and Anurag Ranjan. Hugs: Human gaussian
splatting. In Proc. of CVPR , 2024. 3
[17] Youngjoong Kwon, Lingjie Liu, Henry Fuchs, Marc Haber-
mann, and Christian Theobalt. Deliffas: Deformable light
fields for fast avatar synthesis. Proc. of NeurIPS , 2023. 2
[18] Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, and
Kostas Daniilidis. Gart: Gaussian articulated template mod-
els. In Proc. of CVPR , 2024. 3
[19] Ruilong Li, Shan Yang, David A. Ross, and Angjoo
Kanazawa. Ai choreographer: Music conditioned 3d dance
generation with aist++. In Proc. of ICCV , 2021. 6
[20] Ruilong Li, Julian Tanke, Minh V o, Michael Zollhoefer,
J¨urgen Gall, Angjoo Kanazawa, and Christoph Lassner.
Tava: Template-free animatable volumetric actors. In Proc.
of ECCV , 2022. 1, 2, 3
[21] Zhe Li, Zerong Zheng, Lizhen Wang, and Yebin Liu. Ani-
matable gaussians: Learning pose-dependent gaussian maps
for high-fidelity human avatar modeling. In Proc. of CVPR ,
2024. 3
[22] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu
Sarkar, Jiatao Gu, and Christian Theobalt. Neural actor:
Neural free-view synthesis of human actors with pose con-
trol. ACM Trans. Graph.(ACM SIGGRAPH Asia) , 2021. 1
[23] Yang Liu, Xiang Huang, Minghan Qin, Qinwei Lin, and
Haoqian Wang. Animatable 3d gaussian: Fast and high-
quality reconstruction of multiple human avatars, 2023. 3
[24] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J. Black. SMPL: A skinned multi-
person linear model. ACM Transasctions Graphics , 34(6),
2015. 3, 4, 5
[25] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-
ard Pons-Moll, and Michael J. Black. AMASS: Archive of
motion capture as surface shapes. In Proc. of ICCV , 2019. 6
[26] Marko Mihajlovic, Yan Zhang, Michael J. Black, and Siyu
Tang. LEAP: Learning articulated occupancy of people. In
Proc. of CVPR , 2021. 3, 4
[27] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In Proc. of ECCV , 2020. 1, 2
[28] Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw,
Yiren Zhou, and Eduardo P ´erez-Pellitero. Human gaussian
splatting: Real-time rendering of animatable avatars. In
Proc. of CVPR , 2024. 3
[29] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding. ACM Transasctions Graphics , 41(4),
2022. 2, 4
[30] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and
Andreas Geiger. Differentiable volumetric rendering: Learn-
ing implicit 3d representations without 3d supervision. In
Proc. of CVPR , 2020. 1
[31] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya
Harada. Neural articulated radiance field. In Proc. of ICCV ,
2021. 1, 2
5028
[32] Michael Oechsle, Songyou Peng, and Andreas Geiger.
Unisurf: Unifying neural implicit surfaces and radiance
fields for multi-view reconstruction. In Proc. of ICCV , 2021.
1
[33] Ahmed A. A. Osman, Timo Bolkart, and Michael J. Black.
Star: Sparse trained articulated human body regressor. In
Proc. of ECCV , 2020. 3
[34] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas
Daniilidis. Learning to estimate 3d human pose and shape
from a single color image. In Proc. of CVPR , 2018. 3
[35] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan
Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani-
matable neural radiance fields for modeling dynamic human
bodies. In Proc. of ICCV , 2021. 1, 2, 3, 6
[36] Songyou Peng, Chiyu ”Max” Jiang, Yiyi Liao, Michael
Niemeyer, Marc Pollefeys, and Andreas Geiger. Shape as
points: A differentiable poisson solver. In Proc. of NeurIPS ,
2021. 1
[37] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,
Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:
Implicit neural representations with structured latent codes
for novel view synthesis of dynamic humans. In Proc. of
CVPR , 2021. 1, 2, 5, 6, 7, 8
[38] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi
Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural
implicit surfaces for creating avatars from videos. ArXiv ,
abs/2203.08133, 2022. 1, 2, 3, 5
[39] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xi-
aowei Zhou. Representing volumetric videos as dynamic
mlp maps. In Proc. of CVPR , 2023. 2
[40] Sergey Prokudin, Michael J. Black, and Javier Romero. SM-
PLpix: Neural avatars from 3D human models. In Proc. of
WACV , 2021. 3
[41] Sergey Prokudin, Qianli Ma, Maxime Raafat, Julien
Valentin, and Siyu Tang. Dynamic point fields. In Proc.
of ICCV , 2023. 2, 5
[42] Amit Raj, Julian Tanke, James Hays, Minh V o, Carsten Stoll,
and Christoph Lassner. Anr-articulated neural rendering for
virtual avatars. In Proc. of CVPR , 2021. 1
[43] Christian Reiser, Richard Szeliski, Dor Verbin, Pratul P.
Srinivasan, Ben Mildenhall, Andreas Geiger, Jonathan T.
Barron, and Peter Hedman. Merf: Memory-efficient radi-
ance fields for real-time view synthesis in unbounded scenes.
ACM TOG , 42(4), 2023. 2
[44] Darius R ¨uckert, Linus Franke, and Marc Stamminger. Adop:
Approximate differentiable one-pixel point rendering. ACM
Transactions on Graphics , 41(4), 2022. 2, 3
[45] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J.
Black. SCANimate: Weakly supervised learning of skinned
clothed avatar networks. In Proc. of CVPR , 2021. 3
[46] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. In Proc. of CVPR ,
2022. 2
[47] Vincent Sitzmann, Semon Rezchikov, William T. Freeman,
Joshua B. Tenenbaum, and Fredo Durand. Light field net-
works: Neural scene representations with single-evaluation
rendering. In Proc. of NeurIPS , 2021. 1[48] Shih-Yang Su, Frank Yu, Michael Zollhoefer, and Helge
Rhodin. A-neRF: Articulated neural radiance fields for learn-
ing human shape, appearance, and pose. In Proc. of NeurIPS ,
2021. 1, 2
[49] Shih-Yang Su, Timur Bagautdinov, and Helge Rhodin. Npc:
Neural point characters from video. In Proc. of ICCV , 2023.
2, 3
[50] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and
Ameesh Makadia. Generalizable patch-based neural render-
ing. In Proc. of ECCV , 2022. 1
[51] Mohammed Suhail1, Carlos Esteves, Leonid Sigal, and
Ameesh Makadia. Light field neural rendering. In Proc.
of CVPR , 2022. 1
[52] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance fields
reconstruction. In Proc. of CVPR , 2022. 2
[53] Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Men-
glei Chai, Yun Fu, and Sergey Tulyakov. R2l: Distilling neu-
ral radiance field to neural light field for efficient novel view
synthesis. In Proc. of ECCV , 2022. 1
[54] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yan-
shun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and
Lan Xu. Fourier plenoctrees for dynamic radiance field ren-
dering in real-time. In Proc. of CVPR , 2022. 3
[55] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
InProc. of NeurIPS , 2021. 1
[56] Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas
Geiger, and Siyu Tang. Metaavatar: Learning animatable
clothed human models from few depth images. In Proc. of
NeurIPS , 2021. 3
[57] Shaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu
Tang. Arah: Animatable volume rendering of articulated hu-
man sdfs. In Proc. of ECCV , 2022. 1, 2, 3, 4, 6, 8
[58] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan,
Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-
mannerf: Free-viewpoint rendering of moving people from
monocular video. In Proc. of CVPR , 2022. 1, 2, 3, 4, 6, 8
[59] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng
Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang.
4d gaussian splatting for real-time dynamic scene rendering.
arXiv preprint arXiv:2310.08528 , 2023. 3, 5
[60] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir,
William T. Freeman, Rahul Sukthankar, and Cristian Smin-
chisescu. Ghum & ghuml: Generative 3d human shape and
articulated pose models. In Proc. of CVPR , 2020. 3
[61] Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu.
H-neRF: Neural radiance fields for rendering and temporal
reconstruction of humans in motion. In Proc. of NeurIPS ,
2021. 1, 2
[62] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu,
Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-
based neural radiance fields. In Proc. of CVPR , 2022. 2,
3
[63] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing
Zhang, and Xiaogang Jin. Deformable 3d gaussians for
5029
high-fidelity monocular dynamic scene reconstruction. arXiv
preprint arXiv:2309.13101 , 2023. 3, 5
[64] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li
Zhang. Real-time photorealistic dynamic scene representa-
tion and rendering with 4d gaussian splatting. arXiv preprint
arXiv 2310.10642 , 2023. 3, 5
[65] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan
Atzmon, Basri Ronen, and Yaron Lipman. Multiview neu-
ral surface reconstruction by disentangling geometry and ap-
pearance. In Proc. of NeurIPS , 2020. 1
[66] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman.
V olume rendering of neural implicit surfaces. In Proc. of
NeurIPS , 2021. 1
[67] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,
Pratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron,
and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-
time view synthesis. In Proc. of SIGGRAPH , 2023. 2
[68] Keyang Ye, Tianjia Shao, and Kun Zhou. Animatable
3d gaussians for high-fidelity synthesis of human motions,
2023. 3
[69] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and
Angjoo Kanazawa. PlenOctrees for real-time rendering of
neural radiance fields. In Proc. of ICCV , 2021. 2, 3
[70] Zhengming Yu, Wei Cheng, xian Liu, Wayne Wu, and Kwan-
Yee Lin. MonoHuman: Animatable human neural field from
monocular video. In Proc. of CVPR , 2023. 1, 2, 6, 8
[71] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz,
and Felix Heide. Differentiable point-based radiance fields
for efficient view synthesis. In SIGGRAPH Asia Conference
Proceedings , 2022. 2, 3
[72] Fuqiang Zhao, Yuheng Jiang, Kaixin Yao, Jiakai Zhang, Liao
Wang, Haizhao Dai, Yuhui Zhong, Yingliang Zhang, Minye
Wu, Lan Xu, and Jingyi Yu. Human performance modeling
and rendering via neural animated mesh. ACM Transactions
on Graphics, (Proc. SIGGRAPH Asia) , 41(6), 2022. 3
[73] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J.
Black, and Otmar Hilliges. Pointavatar: Deformable point-
based head avatars from videos. In Proc. of ECCV , 2023. 2,
3, 4
[74] Zerong Zheng, Han Huang, Tao Yu, Hongwen Zhang, Yan-
dong Guo, and Yebin Liu. Structured local radiance fields
for human avatar modeling. In Proc. of CVPR , 2022. 2, 3
[75] Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito,
Michael Zollh ¨ofer, Justus Thies, and Javier Romero. Driv-
able 3d gaussian avatars. arXiv preprint arXiv:2311.08581 ,
2023. 3
[76] M. Zwicker, H. Pfister, J. van Baar, and M. Gross. Ewa vol-
ume splatting. In Proceedings Visualization, 2001. VIS ’01. ,
pages 29–538, 2001. 4
5030
