MoReVQA: Exploring Modular Reasoning Models
for Video Question Answering
Juhong Min1,2* Shyamal Buch1Arsha Nagrani1Minsu Cho2Cordelia Schmid1
1Google Research2POSTECH†
http://juhongm999.github.io/morevqa
Abstract
This paper addresses the task of video question answer-
ing (videoQA) via a decomposed multi-stage, modular rea-
soning framework. Previous modular methods have shown
promise with a single planning stage ungrounded in visual
content. However, through a simple and effective base-
line, we find that such systems can lead to brittle behav-
ior in practice for challenging videoQA settings. Thus,
unlike traditional single-stage planning methods, we pro-
pose a multi-stage system consisting of an event parser, a
grounding stage, and a final reasoning stage in conjunction
with an external memory. All stages are training-free, and
performed using few-shot prompting of large models, cre-
ating interpretable intermediate outputs at each stage. By
decomposing the underlying planning and task complexity,
our method, MoReVQA, improves over prior work on stan-
dard videoQA benchmarks (NExT-QA, iVQA, EgoSchema,
ActivityNet-QA) with state-of-the-art results, and extensions
to related tasks (grounded videoQA, paragraph captioning).
1. Introduction
The predominant approach for solving video understanding
tasks such as video question answering (videoQA) has long
been end-to-end networks [1, 6,7,38,42,43]. A major
challenge with such methods, however, is their black-box
nature – leading to a lack of interpretability and composi-
tional generalization. For videos in particular, an impor-
tant desired capability is the ability to understand events at
different temporal scales, which is challenging for existing
end-to-end vision-language models (VLMs) that typically
see only a few frames [6, 7,42]. This has led to a recent in-
terest in modular or programmatic approaches [22, 39,40]
to solve such problems, particularly leveraging the suc-
cess of large language models (LLMs) [10, 41,63] which
have shown impressive reasoning and planning capabilities.
1Work done while Student Researcher intern at Google Research.
2Pohang University of Science and Technology
Event 
parsing
LLM
Prediction
LLMEvent parsing
prompt
Grounding
LLMGrounding
prompt
Reasoning
LLMReasoning
prompt
Q
A
LLMA single, large, complex program
(interpretable but does not generalize as well)A Q
Decomposed 
instruction set
specialized at
event parsingDecomposed 
instruction set
specialized at
groundingDecomposed 
instruction set
specialized at
reasoning
Black -box end -to-end model
(difficult to interpret)QAV
V
V(ii) MoReVQA (ours)(b)A single, large, complex prompt(a)
(i) JCEF (ours)
QVASimple, deterministic program
(1 VLM + 1 LLM module from (b))
(consistent toolkit with (b), decomposed)Figure 1. MoReVQA: a new multi-stage, modular reasoning
model for videoQA. Prior work relies on either (a) black-box end-
to-end models that are difficult to interpret, or (b) modular systems
where an interpretable planning step (program generation) is done
in a single, ungrounded stage. (i) In this work, we find that single-
stage planning leads in practice to brittle behavior, underperform-
ing a new simple baseline (JCEF) that captions frames and predicts
an answer (with two modules from (b)). (ii) We then introduce our
new MoReVQA method incorporating both modularity andmulti-
stage planning, providing interpretable, grounded planning and
execution traces, while simultaneously delivering improvements
in overall accuracy by effectively decomposing the underlying task
complexity (still using consistent base models with (b)). Above:
Qis question, Vis video, Ais answer.
These methods generate symbolic programs [39, 40] using
an LLM capable of producing code. They are interpretable
and can be executed directly (leveraging independent visual
or language processing modules). Their advantages are that
they are training-free, compositional, and achieve impres-
sive performance on few-shot vision and language tasks.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
13235
In this paper, we analyze the performance of such meth-
ods in closer detail, particularly for the case of videoQA
(across 4 datasets [33, 47,51,58], representative of a range
of video domains, lengths, and question types) and for sin-
gle stage modular frameworks (such as ViperGPT [40]).
We find that, while recent modular approaches building
on large, state-of-the-art end-to-end networks (LLMs and
VLMs) as modules have shown significant promise [40], a
simple Socratic [59] baseline, which we call Just Caption
Every Frame (JCEF), based on the same underlying mod-
els, can actually outperform these prior approaches by a
significant margin. As the name suggests, JCEF simply cap-
tions every frame in the video using a large vision-language
model (VLM) [8], and then feeds all captions along with
the question to an LLM to produce an answer (Fig. 1(i)
and Fig. 2). We hypothesize that the reason this baseline
outperforms prior work is that these modular frameworks
(Fig. 1(b)) consist of a single planning stage which may
be ungrounded in the video (i.e. the entire program or set
of steps to be executed is determined in a single stage di-
rectly from the language prompt alone), and hence in prac-
tice the single-stage planner must be prompted with a large
space of complex combinations required for answering di-
verse questions in video [27]. While the performance of
JCEF is impressive, it is less interpretable than the previ-
ously mentioned modular approaches, as captions for each
frame tend to be generic and not question-specific (Fig. 2).
In this work, we propose a decomposed, modular, and
multi-stage approach for video question answering to ad-
dress these limitations (Fig. 1(ii) and Fig. 3). Our method
consists of three key planning and execution stages: (1)
event parsing that explicitly decomposes the events in the
question, (2) grounding that identifies corresponding tem-
poral regions in the video that merit further tool use (so
that every single frame does not have to be processed in
detail), and (3) reasoning that gives the final answer af-
ter considering the outputs of composed modules/APIs and
the shared memory. This decomposition of single-stage
planning is motivated by natural sub-tasks for videoQA
and related video-language reasoning tasks. All stages are
training-free, and involve few-shot or zero-shot prompting
of off-the-shelf modules (consistent with the API behav-
ior in single-stage planning methods), in conjunction with
an external read/write memory that maintains state and en-
ables a more flexible design. We call our method Modu-
lar Reasoning for Video Question Answering (MoReVQA),
and show that it outperforms JCEF and other key single-
stage modular baselines, while providing a grounded, inter-
pretable planning and execution trace.
We summarize our key contributions as follows: (1)We
find that existing single-stage code-generation frameworks,
while being modular and interpretable, are not necessarily
well-suited for the complexity of generalizable VideoQA,and can be outperformed by a simple baseline we propose
using a subset of its tool components (e.g. a large VLM
and LLM), (2)we design a multi-stage modular reason-
ing system (MoReVQA) that alleviates this issue by de-
composing the underlying planning sub-tasks effectively,
and(3)we achieve state-of-the-art results on four stan-
dard videoQA benchmarks (NExT-QA, iVQA, EgoSchema,
and ActivityNet-QA) across training-free (zero-shot/few-
shot) methods, in some cases even outperforming fully-
finetuned prior work. We also consider extensions to
grounded videoQA (NExT-GQA [48]) and paragraph cap-
tioning (ActivityNet-Para [28]) with strong performance.
2. Related Work
VideoQA. Video Question-Answering (videoQA) is a key
task for multimodal video understanding systems to assess
their ability to reason about a video [33, 47,49,51,64].
Recent benchmarks have pushed towards assessing reason-
ing for temporal questions [21, 46,47], longer videos [33,
58], and on domains like instructional [51] and egocentric
videos [16, 33]. We evaluate our modular approach on four
diverse and representative videoQA tasks: NExT-QA [47],
iVQA [51], EgoSchema [33], and ActivityNet-QA [58].
End-to-end Models for VideoQA. The recent success of
LLMs [10, 19,41,63] has led to an explosion of multimodal
models that jointly understand vision and text data. Many
works map frozen image encoders [13, 14,37] to the LLM
textual embedding space: e.g., Flamingo [1], via a Perceiver
resampler [25], or BLIP2 [29] and Video-LLaMa [61],
via Q-formers for audio/vision [14, 18]. GIT2 [42] and
PALI [6–8] use simple encoder-decoder style architectures
which are trained for image captioning, while MV-GPT [38]
finetunes a native video backbone [3] for video caption-
ing. Although trained with a generative (captioning) objec-
tive, such models achieve strong results for general vision-
language tasks (cast as auto-regressive generation with
question as prefix). More recent works such as Instruct-
BLIP [11], MiniGPT-4 [66], and VideoBLIP [56] improve
zero-shot results with strong instruction tuning. Generally,
however, end-to-end methods can be difficult to interpret.
For videos in particular, memory limits in end-to-end
models require significant downsampling: e.g. temporally
sampling a few frames with large strides [7, 42], spatially
subsampling each frame to a single token [43, 53,65]. Such
models also tend process each frame with equal importance.
Unlike such works, our model has an explicit grounding
stage which searches for the most relevant video frames
to be processed in more detail. Other grounding works
for videoQA include SeViLa [57], MIST [17], and NExT-
GQA[48]; our model differentiates from these prior works
by incorporating modular multi-stage reasoning.
Visual Programming and Modularity. Visual program-
ming methods [2, 9,22,26,39,40] have shown promise
13236
towards addressing the limitations of end-to-end systems,
by composing multiple sub-task specific modules into an
executable program. Prior (earlier) work on neural modu-
lar networks [2, 26] made initial progress towards this goal,
but were eventually outpaced by developments in large-
scale end-to-end models. Recent work like CodeVQA [39],
ViperGPT [40], and VisProg [22] demonstrated accuracies
on par with some end-to-end systems [29], by replacing the
event/language parsing with a code-finetuned LLM that can
generate an entire python program (which invokes a number
of provided APIs in the prompt). While these approaches
are effective in terms of interpretability and flexibility in
solving VQA, they share common limitations in that they
heavily rely on a ‘single-prompt’ with large, complex code
generation examples [27], which must generate the entire
program without access to the image.
Multistage Planning Models. Recent methods have ex-
plored directly using natural language as the intermedi-
ate representation between large multimodal models. One
emerging class of models are Socratic models [59], which
use few-shot or zero-shot prompting of LLMs and VLMs
to solve video tasks, e.g. VidIL [45] which feeds image
captions, frame attributes and ASR to an LLM to perform
video-language tasks. The closest to our work is A VIS [23],
which also uses multistage LLMs with an external mem-
ory for the task of visual question answering. However un-
like A VIS which works on knowledge focused QA for im-
ages, we focus on the more challenging domain of videos,
where reasoning over multiple frames is required. A key
difference therefore is our grounding stage, which deter-
mines which frames in a (potentially long) video contain
the most relevant information to then deploy additional rea-
soning steps and tools over.
3. Technical Approach
In this section, we contextualize our technical approach for
videoQA (Sec. 3.1) by discussing limitations in the standard
single-stage (Sec. 3.2) paradigm before presenting our main
multistage modular reasoning model MoReVQA(Sec. 3.3).
3.1. Preliminaries: Video Question-Answering
Task. We focus on the task of video question-answering
(VideoQA) as it provides a good testbed for video reasoning
for multimodal systems. Formally, we are given an input
video V={v1, . . . , v l}withlframes and a corresponding
question Qin natural language with a groundtruth answer
A, either directly from the question alone [51, 58], or from
among a set of candidate options A∈Acands [33,47]. The
task is to develop a model Msuch that:
M(V, Q, [Acands])7→A (1)
where Acands are present for closed-set VQA settings [33,
47] and not present for open-ended VQA [51, 58].Design Approaches. The approaches for addressing this
task can vary broadly (Sec. 2and Fig. 1); here, we center our
discussion around two key design principles in state-of-the-
art systems for M: (1) Modularity, where individual, stan-
dalone modules focused on specific sub-tasks are leveraged,
as opposed to a single monolithic black-box model; and
(2)Multi-stage planning, where there are explicit interme-
diate outputs while the system determines which modules
to leverage and how to use them most effectively, provid-
ing a more interpretable chain of execution. In this section,
we focus on contrasting modular methods with single-stage
(prior work) vs. multi-stage planning (our new model).
3.2. Single-Stage Planning
Overview. In Section 2, we discuss the broader space of vi-
sual programming and modular methods [26, 39,40]. Here,
we focus on a specific representative state-of-the-art model
(ViperGPT [40]) and discuss key limitations with its single-
stage planning approach for modular videoQA, using nota-
tion consistent with prior work [26, 40].
ViperGPT. In the context of videoQA, ViperGPT is a
system Mthat consists of a single-stage program gener-
atorπthat takes as input the query Qand a specialized
prompt Pto directly output an intermediate executable pro-
gram z∈Z, where Zrepresents the space of all programs
(Python, natural language, etc.). This program zis then ex-
ecuted on the full input (V, Q, [Acands])to produce the final
answer A. More formally, the full system can be described:
Msingle-stage :π(Q, P )7→z(V, Q, [Acands];L)7→A(2)
where Ldenotes the API module library used to construct
the program z. The program generator πis instantiated
as a code-finetuned LLM [5, 19] conditioned on a well-
engineered prompt file P, consisting of two key compo-
nents: (1) a custom API description with API usage ex-
amples, and (2) a set of dataset-specific program exam-
ples that illustrate how to translate the questions Qfound in
the dataset distribution into a full program zthat composes
these modules together effectively.
Modules for Modular Reasoning. ViperGPT and re-
lated models [22, 39,40] leverage a specialized module
library as described by their API to assemble executable
programs z. We denote this library of API modules as
m∈LAPI: examples include open-vocabulary detec-
tion (OWL-ViT [35]), text-image scoring (CLIP [37], X-
VLM [60]), and captioning (BLIP [29]). The overall pro-
gram zthen describes the modular reasoning of the single-
stage code generation LLM for a given query.
Limitations. While a single-stage approach suggests an
appealing promise of simplicity, in practice, we observe that
this design leads to brittle programs that do not produce reli-
able outputs1. We show a representative example for videos
1Also noted by concurrent analysis [27] in the image-language domain.
13237
A
…
VLM
 sample 𝑛 frames
[frame     0] caption: a group of people are playing a video game together in a living room . 
[frame     2] caption: a group of people are playing a video game together in a living room . 
[frame    4] caption: a group of people are playing a video game in a living room
……
[frame      𝑛] caption: a group of people holding wii remotes in a living room
LLMmemory
QVideo (𝑙 frames )Figure 2. A simple, strong baseline – JCEF. Our proposed
baseline consists of a zero-shot prompted vision-language model
(VLM) which is used to caption nuniformly sampled frames from
a video (n is all frames at 1FPS unless explicitly stated). These
captions are then stored in an external memory, which is passed to
a zero-shot prompted LLM that is used to answer a question about
the video. We show that this baseline outperforms existing visual
programming methods by a large margin and investigate ways to
more effectively improve upon it in a modular, multistage manner.
in Fig. 4, with additional analysis in the supplement.
The core issue of the overall system lies in the difficult
task given to its single-stage planner: before performing vi-
sual reasoning, the model must output a full program with-
out any grounding in the actual video itself. Thus, natu-
ral language ambiguity in the question cannot be resolved
by visual context, important for video / event reasoning
tasks [24, 48]. Furthermore, by expecting the model to
piece together full reasoning programs in one large LLM
inference step, the necessary complexity of examples in the
prompt grows accordingly. In practice, we observe this
leads to the system overfitting on the specific examples pro-
vided (also noted by [27]), falling short of realizing its true
potential for compositional modular generalization.
These limitations naturally beget two key questions:
(Q1) to what extent is brittle single-stage planning impact-
ing accuracy, and (Q2) how well can we overcome these
limitations through a multi-stage decomposition? These
motivate our proposed baseline and model in Sec. 3.3.
3.3. MoReVQA: Multi-stage, Modular Reasoning
Motivation: A Simple, Strong Baseline (JCEF). To
empirically characterize the limitations of single-stage ap-
proaches (per Q1), we create a simple but effective So-
cratic baseline called Just Caption Every Frame (JCEF)
(Figure 2), consisting of two strong modules mV LM [8]
andmLLM [19]. Our baseline involves no training, directly
prompting these large off-the-shelf models in what can be
considered as a very simple, deterministic “program”. For
each video, we sample n≤lframes from the video V, cap-
tioning each frame with an image-language model mV LM .These ncaptions are then combined with frame numbers
(e.g., “[frame 5] caption: a person is throwing a baseball in
a field”) into a prompt Pused to query the LLM mLLM
with the question Qand candidate answers Acand for mul-
tiple choice questions (prompt details in supplement). By
comparing to a state-of-the-art baseline (ViperGPT+), up-
graded with the same modules m∗, we can observe the limi-
tations of single-stage planning designs: surprisingly, JCEF
outperforms this single-stage baseline (Sec. 4).
MoReVQA Overview. We address our second key ques-
tion (Q2) by considering a decomposition of the single-
stage pipeline into multiple stages, in order to effec-
tively improve beyond our JCEF baseline. Our new pro-
posed model, multi-stage modular reasoning for videoQA
(MoReVQA), consists of three stages, rooted in key sub-
tasks that are general to videoQA (and related video-
language reasoning tasks) across benchmarks and domains:
(1)event parsing (understanding what is relevant in the in-
put language), (2) grounding (understanding what is rele-
vant in the input video), and (3) reasoning (understanding
the relevant events, their attributes, and their relationships).
An overview of the pipeline is provided in Fig. 3. Each
stage is distinct yet interconnected, employing an LLM that
generates a set of API calls tailored for the specific sub-
tasks. Importantly, these APIs are backed by the same off-
the-shelf pretrained models [8, 35,37] considered in the
single-stage setting (Sec. 3.2) for consistent comparison.
Central to this process is a shared external memory, man-
aging and storing information across stages, including nat-
ural language events, grounded regions of the video, video
captions, and intermediate tool outputs (details in Sec. 4.3).
Through this decomposition, our MoReVQA model
Mmulti-stage ={M1, M2, M3}relies on smaller focused
prompts {P1, P2, P3}for each stage2; furthermore, inter-
mediate reasoning outputs {z1, z2, z3}are able to handle
different aspects of the overall task, and incorporate ground-
ing in the video itself to resolve ambiguities and inform
new intermediate reasoning steps in a more effective man-
ner than the ungrounded single-stage setting. We describe
each stage Mias follows:
Event parsing stage M1.The first stage focuses on the
initial analysis and processing of the input question Q. Dif-
ferent from traditional language parsing in early modular
systems [2, 26], our M1stage parses at the event -level
rather than word-level, focusing on higher-level video se-
mantics while still decomposing relationships and attributes
for later stages. Our event parsing prompt P1(see supple-
mentary) conditions the LLM to examine the input ques-
tion, perform parsing tasks such as detecting temporal hints
and relationships (“in the beginning of the video”, “before”,
“during”), sub-question types (location, description, expla-
nation), and whether the language would suggest additional
2Please see supplement for prompts and API details.
13238
Event 
parsing 𝑀1
PredictionEvent parsing
prompt 𝑃1
API calls 𝑧1
Execute
Grounding
 𝑀2Grounding
prompt  𝑃2
API calls 𝑧2
Execute
Reasoning
 𝑀3Reasoning
prompt  𝑃3
API calls 𝑧3
ExecuteQ
A
…Video
access access
: Large language model (LLM) 
  with shared parameters
Execute : Tool execution: Prompt with decomposed  
  instruction examples trim(“beginning”)
parse_event (“before”,
  “man standing up”,
  “why is the man
   … his skates?”)
classify (“why”)
require_ocr (“no”)man = localize (“man”)
verify(“is the man
  standing up?”, man)
truncate (“before”)
verify(“is the man
  removing his
  skates?”, man)vqa(“why is the man
removing his skates?”)
vqa([“what surrounds
   the man?”,
   “how is the man
   removing skates?”])
External memory
Video contextQ
Video context : VLM applied on frame subset
update update
updateFigure 3. Modular Reasoning for Video Question-Answering (MoReVQA). To address the limitations of single-stage planning LLMs,
we propose a new multi-stage, modular method Mmulti-stage that decomposes planning and execution into three key steps, motivated by
sub-tasks inherent to videoQA: (i) event parsing M1, (ii) grounding M2, and (iii) reasoning M3. See Section 3.3for additional details.
tool types (e.g. OCR). The LLM then produces a set of API
calls based on these parsing tasks, expressed as z1, which
when executed populates the external memory with relevant
language-only data for later stages.
Grounding stage M2.In this stage, the focus shifts to
grounding identified events, a critical process to help re-
solve ambiguities and direct tool-use in the final reasoning
stage to the temporal regions where they can be most effec-
tive. Here, the prompt P2is constructed with the external
memory state with outputs from M1(e.g., parsed events),
and conditions the LLM to identify candidate frames and
temporal regions in the video with vision-language mod-
ulesmfor entity detection and image-text alignment. The
resulting z2is then executed on the video, and the output
grounding (spatially and temporally) is appended to the ex-
ternal memory. Importantly, this process includes API calls
designed to help verify and resolve event ambiguity through
visual grounding, as illustrated in Fig. 3.
Reasoning stage M3.The final stage of our system per-
forms grounded reasoning before the final prediction. The
LLM prompt P3is based on the memory state after the pre-
vious two stages, and constructs a final z3executable with
API calls (Fig. 3) designed around reasoning sub-questions
to unravel different aspects of the original question, and fo-
cusing vision-language modules on the specific grounded
regions of the video identified previously. This localized,
context-specific information is subsequently combined with
a more general n≤lcaptions from frames sampled uni-
formly (general video context, in Fig. 3) across the video to
form a comprehensive (temporally-sorted) basis for a final
prediction LLM to output the final answer A(in general,
nhere is significantly less than with JCEF). This final APIcall here corresponds to the standard llmquery module
found in prior work [40], typically at the end of the program
to ensure correct formatting and candidate answer selection.
Flexibility and Memory. The modular architecture of
MoReVQA allows it to be dynamically tailored to a wide
range of datasets, question types, and tasks by selectively
engaging different APIs and reasoning strategies based on
the task at hand. In particular, simple questions beget a
“simpler” execution pipeline (stages are equipped with “no-
op” like behavior, if necessary), while more complex ques-
tions are processed with a complex instruction set. This
adaptability is facilitated by the external memory compo-
nent, which not only serves as a repository of information
across stages but also enables the system to iteratively re-
fine its understanding and approach based on the evolving
context. We highlight that each stage (planning and execu-
tion) are informed by previous stages through this memory,
which leads to more robust reasoning behavior.
4. Experiments
Here, we describe the VideoQA datasets and evaluation
metrics used (Sec. 4.1), our key baselines (Sec. 4.2) and
implementation details (Sec. 4.3), and our discussion of re-
sults and analysis (Sec. 4.4).
4.1. Datasets and Evaluation Metrics
We consider four standard videoQA benchmarks to assess
the efficacy of our proposed method, across a range of rep-
resentative video domains, lengths, and question types.
NExT-QA [47] is focused on understanding the ability
of videoQA systems to effectively answer questions across
13239
three types: causal (C), temporal (T) and descriptive (D).
We focus on the same multiple choice (MC) setting reported
in prior single-stage modular reasoning work [40], where
each video clip (avg length, 43s) contains one question and
5 candidate answers; we use 4996 val video-question pairs.
iVQA [51] consists of 7-30s instructional video clips sam-
pled from the HowTo100M dataset [34], with 5615 training
and 1879 testing clips (after removing clips no longer on-
line). Each clip has a question and annotated set of ground
truth answers. We note that iVQA is open-ended (OE)
videoQA, and no candidate answers are provided as input.
EgoSchema [33] is a recent dataset of long egocentric
videos (180s) based on the Ego4D [20] benchmark with
multiple-choice (MC) questions, designed specifically to
assess long video understanding. EgoSchema is focused
entirely on evaluation: the hidden test set consists of 5000
videos via an evaluation server, of which 500 were publicly
released for validation. We report results (accuracy) on the
main (full, 5k) test set for comparison with prior work.
ActivityNet-QA [58] has 5800 videos, each accompa-
nied by 10 annotated question-answer pairs to character-
ize model comprehension of actions, objects, locations, and
events. ActivityNet-QA is open-ended (like iVQA) with
long videos (180s avg., like EgoSchema). We report test set
results using GPT-based evaluation following [30, 32,62].
4.2. Baselines
We compare our method against a key set of baselines:
Single-stage Planning (ViperGPT+). As a representa-
tive state-of-the-art baseline for single-stage planning and
modular reasoning, we reimplement ViperGPT[40], as de-
scribed in Sec. 3.2. We extend the open-source imple-
mentation and upgrade some of the modules/APIs to en-
sure consistent comparisons with our method and to replace
prior module components that are not available (eg. GPT-3
Codex [5]); full description in the supplement. We eval-
uate this baseline on video datasets that were not used in
the original paper (iVQA, EgoSchema, ActivityNet-QA) to
better characterize single-stage planning for videoQA.
Just Caption Every Frame (JCEF). We also consider our
JCEF baseline described in Sec. 3.3as a simple but pow-
erful Socratic model that is a step up in interpretability to
a purely end-to-end system, but lacks the kind of modular
compositionality that is present in more fully fledged mod-
ular reasoning systems. The VLM and LLM models used
here are the same as with ViperGPT+ and our full system,
for consistent comparison (details in Sec. 4.3).
Language-only baseline. We also compare our model with
a language-only baseline, which is an LLM [19] prompted
to answer questions without any visual inputs, as a way to
quantify the amount of non-visual language and/or common
sense bias in each dataset. For consistent comparison, this
language model is used across all modular methods.MethodAccuracy (%)
NExT-QA iVQA
EgoSchema ActivityNet-QA
Random (for MC)
20.0 - 20.0 -
LLM-only [19] 48.5 15.0 41.0 -
ViperGPT [40] 60.0 - - -
ViperGPT+ 64.0
46.6 49.3 37.1
JCEF 66.7 56.9 49.9 43.3
MoReVQA 69.2 60.9 51.7 45.3
Table 1. Comparison to single-stage modular methods.
ViperGPT [40] represents the state-of-the-art single-stage modular
question answering system, and ViperGPT+ is our upgraded reim-
plementation for consistent comparison. Our JCEF strong perfor-
mance highlights the relative weakness in single-stage planning
models, which can lead to brittle programs and outputs. We find
that our MoReVQA model outperforms all key baselines.
4.3. Implementation Details
Across all of our baselines and proposed models (e.g.,
MoReVQA, JCEF, ViperGPT+), our core VLM is PALI-3
(5B) [6] for image captioning and related APIs, and our core
LLM is PaLM-2[19] (e.g., every LLM stage in MoReVQA,
JCEF, the language-only baseline, and for the llmquery
module in ViperGPT+), unless otherwise specified. Our
video context / captioner component for MoReVQA con-
siders n= 16 uniformly sampled video frames as a de-
fault. For JCEF, the default is set to n=l, the number
of frames in the video (at 1 frame per second); we provide
additional JCEF ablations for different values of nin the
supplement. We set decoding temperature to 0to match
prior work [40]; other base models and settings (e.g., OWL-
VIT [35], CLIP [37], etc.) for MoReVQA and ViperGPT+
are in the supplement and are also consistent wherever ap-
plicable. Our implementation relies on JAX/Scenic [4, 12].
MoReVQA stores outputs of each stages in an exter-
nal memory system, backed by global variables for track-
ing and updating information through the model’s process-
ing stages. These stages execute different API calls, e.g.,
event parsing reduces frame data for efficiency, the ground-
ing stage focuses on object localization and action verifi-
cation, and the reasoning stage decomposes and addresses
the question with VQA on selected frames. Additional API,
memory, and LLM prompt details for MoReVQA and other
models are provided in supplementary.
4.4. Results and Discussion
4.4.1 Comparison to Baselines and Analysis
We compare our method to the baselines outlined in Sec. 4.2
in Table 1. The LLM-only baseline performance serves
as a measure of language and commonsense bias: our re-
sults for NExT-QA and iVQA align with prior expectations
(e.g., iVQA was explicitly designed to mitigate language
bias). However, this baseline also does surprisingly well
on EgoSchema, in spite of its explicit emphasis on testing
13240
Ground-truth: playing
Options1.playing2.playing with dog3.tired4.play with spool5.getting her attention
Prediction stage
Question
[frame    42] what is the cat doing?: playing...[frame    48] what surrounds the cat?: a person[frame    48] why was the cat lying on its back?: to be pettedPrediction: playing
✅
Memory after 𝑀!
Question (+ Language Metadata)“why was the cat lying on its back near the end?”
Prediction LLM
Video captionsPrediction: tiredGround-truth: playing
❌Video
CaptionerJCEFVisual Programming
Code generationLLMQuestion
def execute_command(video, question, options):  video_segment = VideoSegment(video)  info = OrderedDict()  for frame_number, frame in enumerate(video_segment.frame_iterator()):    cats = frame.find('cat’)    cat = cats[0] if len(cats) == 1 else best_image_match(cats, 'cat’)   if "yes" in cat.simple_query('is the cat lying?’):      answer = cat.simple_query('why is the cat lying?’)      info[f’[frame %4d] why cat is lying' % frame_number] = answer  answer = video_segment.select_answer(info, question, options)return answerGenerated program 𝑧Prediction: tiredGround-truth: playing
❌
info[frame    3] why cat is lying: sleeping[frame    8] why cat is lying: sleeping[frame    9] why cat is lying: sleeping[frame   12] why cat is lying: playing[frame   19] why cat is lying: sleeping[frame   26] why cat is lying: laying on its back [frame   38] why cat is lying: licking… [frame   60] why cat is lying: scratching[frame   61] why cat is lying: playing[frame   63] why cat is lying: playing[frame   65] why cat is lying: laying on its back
Options1.playing2.playing with dog3.tired4.play with spool5.getting her attentionMoReVQA
Options1.playing2.playing with dog3.tired4.play with spool5.getting her attentionquestion: “why was the cat lying on its back?”frame_ids: [41, 42, 43, 44, …, 65, 66, 67] event_queue: [“cat lying on its back”]conjunction: “none”qa_type: “why”require_ocr: Falsequestion: “why was the cat lying on its back?”frame_ids: [42, 44, 46, 47, 48] (+ event_queue, conjunction, qa_type, ...)
Video context (VLM on frame subset)Event parsing stage 𝑀!Grounding stage 𝑀"Reasoning stage 𝑀#
𝑧!
Memory after 𝑀"Memory after 𝑀#
frame 0frame 1frame 2frame 66frame 67
…Video𝑧"
question: “why was the cat lying on its back?”...reasoning_outputs:𝑧#
QuestionquestionvideoinformationvideogroundedframesgroundedframesreasoningoutputsAPI calls  𝑧"
frame 42frame 44frame 46frame 47frame 48
cat = localize("cat")verify_action("cat lying on its back", [cat])
API calls  𝑧#vqa(“why is the cat lying on its back?”)vqa([“what is the cat doing?”,  “what is the cat’s mood?”,  “what surrounds the cat?”])
[frame     0] caption: a kitten is sitting on a pink carpet looking at the camera ....... [frame    54] caption: a kitten is laying on its back on a bed .[frame    55] caption: a kitten is laying on its back on a bed .……[frame    67] caption: a cat is laying on its back on a bed being petted by a person .Video
frame 42frame 44frame 46frame 47frame 48grounded framesFigure 4. Example qualitative result of MoReVQA on NExT-QA. We observe that the intermediate outputs from our MoReVQA model
are interpretable: event parsing stage parses key events from language, and other tool-use metadata. The grounding stage then determines
which frames contain the ‘cat lying on its back’, and the reasoning stage reasons about relevant sub-questions for the final answer, which
when combined with general video-level context (subset of frame captions), gives us the final correct answer. We observe that JCEF and
ViperGPT+ fail to predict correct answer for the same example (Sec. 4.4.1); we provide additional examples and analysis in the supplement.
Stages NExT-QA iVQA
Event parsing Grounding Reasoning Val Test
✗ ✗ ✗ 66.7 56.9
✓ ✗ ✓ 68.3 56.9
✓ ✓ ✗ 68.7 57.5
✓ ✓ ✓ 69.2 60.9
Table 2. Ablation study of the various stages in MoReVQA.
We show the impact of each of the key stages of our proposed
design, highlighting the improvements between a system without
our proposed stages (top row; defaults to the JCEF baseline) and
our multi-stage reasoning setting (bottom row; all 3 stages). We
observe stages provide complementary ( e.g., NExT-QA) and syn-
ergistic ( e.g., iVQA) gains (additional ablations in supplement).
long form video understanding. We believe this could po-
tentially be an artifact of its dataset construction process,which leveraged automatic LLM generations to form the
question/candidate answer language inputs [33] and may
have introduced unintended language bias.
Next, we highlight our simple Socratic baseline
JCEF outperforms state-of-the-art single-stage program-
ming methods such as ViperGPT+ (our upgraded imple-
mentation) across all datasets, even though both baselines
have access to the same VLM and LLM modules. This
gives us a quantitative assessment of the impact of brit-
tle program generations and tool executions for the single-
stage model, as observed in our qualitative analysis (Fig-
ure 4, bottom + additional examples in supplement); we
also highlight concurrent analysis [27] which found simi-
lar failure modes in image-language settings.
Finally, we note that our model MoReVQA outperforms
all previous training-free baselines across all datasets, while
13241
Method V al
FT
MIST-CLIP [
17] 57.2
✓ HiTeA [54] 63.1
SeViLa [
57] 73.8
ViperGPT [
40] 60.0
✗BLIP-2concat[29] 62.4
BLIP-2voting[29] 62.7
SeViLA [57] 63.6
JCEF 66.7
MoReVQA 69.2
(a) NExT-QA [47]Method Test
FT
VideoCoCa [
50] 39.0
✓ FrozenBiLM [52] 39.7
Text+T
ext [31] 40.2
FrozenBiLM [52
] 27.3
✗BLIP-2 (FlanT5XXL) [29] 45.8
InstructBLIP (FlanT5XL) [11] 53.1
InstructBLIP (FlanT5XXL) [11] 53.8
JCEF 56.9
MoReVQA 60.9
(b) iVQA [51]Method T est
FT
VIOLET [ 15
] 19.9
✗SeViLA [57] 22.7
FrozenBiLM [52] 26.9
mPLUG-Owl [55] 31.1
InternVideo [44] 32.1
∗ShortViViT [36] 31.0
∗LongViViT [36] 33.3
JCEF 50.0
MoReVQA 51.7
(c) EgoSchema [33]Method T est
FT
Video-LLaMA [
61] 12.4
VideoChat [30] 26.5
✗∗LLaMa adapter [62] 34.2
∗Video-ChatGPT [32] 35.2
ViperGPT+ 37.1
JCEF 43.3
MoReVQA 45.3
(d) ActivityNet-QA [58]
Table 3. Comparison to SOTA on the standard video question-answering datasets: (a) NExT-QA, (b) iVQA, (c) EgoSchema, and (d)
ActivityNet-QA. Our method MoReVQA outperforms all training-free prior work or exceeds prior state-of-the-art fine-tuned systems (in
grey), on the main validation datasets [33, 47,51,58]. FT indicates fine-tuned methods. Methods with asterisk∗indicate concurrent work.
at the same time providing intermediate interpretable out-
puts – in Fig. 4, we show a representative example on
NExT-QA. We also show limitations in both JCEF and
ViperGPT given the same example: while JCEF is a strong
baseline, the general-level captions are not always informa-
tive enough to answer the question, while the program gen-
erated by ViperGPT+ does not focus on the frames in the
correct part of the video (specifically, the “if” statement
condition erroneously triggers on irrelevant early frames,
resulting in misleading “info”). See supplement for more.
We also ablate the stages in MoReVQA (Table 2). Our
ablation without any stages (top row) effectively defaults to
the JCEF baseline with only frame-level captions and a final
LLM prediction stage. The absence of grounding indicates
that we simply return a single middle frame for this stage.
For all ablations without the final reasoning stage, we retain
a final LLM prediction on top of the shared memory state,
i.e., the reasoning performs the final VQA only with the in-
put question (without supporting questions). We observe
all three stages of our model (event parsing, grounding, and
reasoning) provide complementary (e.g. on NExT-QA) and
synergistic (e.g. on iVQA) gains, and meaningfully improve
over the JCEF baseline. The added benefit is the inter-
pretability of the intermediate outputs stored in the external
memory. Further, we ablate the impact of key components
in the memory; when the original question is provided to
the reasoning stage, as opposed to a revised version (e.g.,
question in Fig. 4), we note accuracy drops of 1.3% and
3.9% on NExT-QA and iVQA respectively. When grounded
frame locations are only given to prediction stage, instead
of reasoning stage, we observe drops of 1.2% and 3.9% on
the same datasets. Additional examples and analysis are in
the supplement, including specific API usage statistics.
4.4.2 Comparison to State-of-the-Art
Finally, we compare our method to the state-of-the-art
methods on four datasets – NExT-QA, iVQA, EgoSchema,
and ActivityNet-QA (Table 3) – in which the numbers in
bold and underline respectively indicate the best and sec-
ond best. On all datasets we outperform previous zero-shot and few-shot methods by large margins – on NExT-QA
we outperform SeViLA by almost 6%, making progress to-
wards closing the gap to fully finetuned performance. On
iVQA we outperform the nearest method InstructBLIP [11]
by almost 7%, while on EgoSchema the gaps are the largest
(approx. 20%). For EgoSchema, we report results using
n= 30 video frames; we provide results with other val-
ues of nin the supplement. We also show strong results on
ActivityNet-QA, outperforming concurrent work [32, 62]
under consistent evaluation protocols.
Extensions to related tasks. In our supplement, we de-
scribe extensions of our MoReVQA system to other tasks.
We consider grounded videoQA (localizing the relevant
video segment while providing the answer) on the recent
NExT-GQA [48] dataset, and highlight our training-free
MoReVQA achieves strong performance (37.8 mIoP / 39.6
Acc@GQA) vs. prior state-of-the-art SeViLa (29.5 mIoP /
16.6 Acc@GQA; trained with grounding annotations). We
also show on ActivityNet-Para [28] strong performance for
video paragraph captioning (MoReVQA 28.2 CIDEr vs.
finetuned SOTA Vid2Seq [53] with 28.0), even though our
method is training-free. We observe our system’s reasoning
enables diverse long captions of human-centric events.
5. Conclusion
In this work, we have presented a baseline (JCEF) to help
characterize limitations with single-stage planning models,
along with MoReVQA, a new, modular, and decomposed
multi-stage pipeline for video question answering. Our
framework consists of 3stages – event parsing, ground-
ing, and reasoning with an external memory. MoReVQA
achieves state-of-the-art results on popular VideoQA bench-
marks, while producing interpretable intermediate outputs.
We discuss limitations and broader impacts in supplement.
Acknowledgements. We sincerely thank ViperGPT [40]
authors for sharing additional details helpful for the devel-
opment of ViperGPT+, and grateful to Chen S., Jasper U.,
and Lluis C. for discussions. Minsu Cho acknowledges IITP
grant (2022-0-00959: “Few-shot learning of causal infer-
ence in vision and language”) support by Korea (MSIT).
13242
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. Advances in
Neural Information Processing Systems, 35:23716–23736,
2022. 1,2
[2] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan
Klein. Neural module networks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 39–48, 2016. 2,3,4
[3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen
Sun, Mario Lu ˇci´c, and Cordelia Schmid. ViViT: A Video
Vision Transformer. In ICCV, 2021. 2
[4] James Bradbury, Roy Frostig, Peter Hawkins,
Matthew James Johnson, Chris Leary, Dougal Maclau-
rin, George Necula, Adam Paszke, Jake VanderPlas, Skye
Wanderman-Milne, and Qiao Zhang. JAX: composable
transformations of Python+NumPy programs, 2018. 6
[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Hen-
rique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,
Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evalu-
ating large language models trained on code. arXiv preprint
arXiv:2107.03374, 2021. 3,6
[6] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam
Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-
scaled multilingual language-image model. arXiv preprint
arXiv:2209.06794, 2022. 1,2,6
[7] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,
Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-
bastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On
scaling up a multilingual vision and language model. arXiv
preprint arXiv:2305.18565, 2023. 1,2
[8] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,
Jialin Wu, Paul V oigtlaender, Basil Mustafa, Sebastian
Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al.
Pali-3 vision language models: Smaller, faster, stronger.
arXiv preprint arXiv:2310.09199, 2023. 2,4
[9] Jaemin Cho, Abhay Zala, and Mohit Bansal. Visual pro-
gramming for text-to-image generation and evaluation. arXiv
preprint arXiv:2305.15328, 2023. 2
[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 1,2
[11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning.
arXiv preprint arXiv:2305.06500, 2023. 2,8
[12] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab,
Matthias Minderer, and Yi Tay. Scenic: A jax library for
computer vision research and beyond. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 21393–21398, 2022. 6[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 2
[14] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,
Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue
Cao. EV A: Exploring the limits of masked visual represen-
tation learning at scale. In CVPR, 2023. 2
[15] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang
Wang, Lijuan Wang, and Zicheng Liu. Violet : End-to-end
video-language transformers with masked visual-token mod-
eling. arXiv:2111.12681, 2021. 8
[16] Difei Gao, Ruiping Wang, Ziyi Bai, and Xilin Chen. Env-qa:
A video question answering benchmark for comprehensive
understanding of dynamic environments. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV), pages 1675–1685, 2021. 2
[17] Difei Gao, Luowei Zhou, Lei Ji, Linchao Zhu, Yi Yang,
and Mike Zheng Shou. Mist: Multi-modal iterative spatial-
temporal transformer for long-form video question answer-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 14773–14783,
2023. 2,8
[18] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan
Misra. ImageBind: One embedding space to bind them all.
InCVPR, 2023. 2
[19] Google, Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin
Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shak-
eri, Emanuel Taropa, Paige Bailey, et al. Palm 2 technical
report, 2023. 2,3,4,6
[20] Kristen Grauman, Andrew Westbury, Eugene Byrne,
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson
Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Mar-
tin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar
Ramakrishnan, Fiona Ryan, et al. Ego4d: Around the world
in 3,000 hours of egocentric video, 2022. 6
[21] Madeleine Grunde-McLaughlin, Ranjay Krishna, and Ma-
neesh Agrawala. Agqa: A benchmark for compositional
spatio-temporal reasoning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 11287–11297, 2021. 2
[22] Tanmay Gupta and Aniruddha Kembhavi. Visual program-
ming: Compositional visual reasoning without training. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 14953–14962, 2023. 1,
2,3
[23] Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou
Sun, David A Ross, Cordelia Schmid, and Alireza Fathi.
Avis: Autonomous visual information seeking with large
language model agent. In Thirty-seventh Conference on Neu-
ral Information Processing Systems, 2023. 3
[24] De-An Huang*, Shyamal Buch*, Lucio Dery, Animesh
Garg, Li Fei-Fei, and Juan Carlos Niebles. Finding “it”:
Weakly-supervised, reference-aware visual grounding in in-
13243
structional videos. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018. 4
[25] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
Andrew Zisserman, and Joao Carreira. Perceiver: General
perception with iterative attention. In International confer-
ence on machine learning, pages 4651–4664. PMLR, 2021.
2
[26] Justin Johnson, Bharath Hariharan, Laurens Van
Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick,
and Ross Girshick. Inferring and executing programs for
visual reasoning. In Proceedings of the IEEE international
conference on computer vision, pages 2989–2998, 2017. 2,
3,4
[27] Apoorva Khandelwal, Ellie Pavlick, and Chen Sun. Analyz-
ing modular approaches for visual question decomposition.
InConference on Empirical Methods in Natural Language
Processing (EMNLP), 2023. 2,3,4,7
[28] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
Juan Carlos Niebles. Dense-captioning events in videos. In
Proceedings of the IEEE international conference on com-
puter vision, pages 706–715, 2017. 2,8
[29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597, 2023. 2,3,8
[30] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai
Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
Videochat: Chat-centric video understanding, 2024. 6,8
[31] Xudong Lin, Simran Tiwari, Shiyuan Huang, Manling Li,
Mike Zheng Shou, Heng Ji, and Shih-Fu Chang. To-
wards fast adaptation of pretrained contrastive models for
multi-channel video-language retrieval. arXiv preprint
arXiv:2206.02082, 2023. 8
[32] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-
had Shahbaz Khan. Video-chatgpt: Towards detailed video
understanding via large vision and language models, 2023.
6,8
[33] Karttikeya Mangalam, Raiymbek Akshulakov, and Jiten-
dra Malik. Egoschema: A diagnostic benchmark for very
long-form video language understanding. arXiv preprint
arXiv:2308.09126, 2023. 2,3,6,7,8
[34] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
HowTo100M: Learning a Text-Video Embedding by
Watching Hundred Million Narrated Video Clips. In ICCV,
2019. 6
[35] M Minderer, A Gritsenko, A Stone, M Neumann, D Weis-
senborn, A Dosovitskiy, A Mahendran, A Arnab, M De-
hghani, Z Shen, et al. Simple open-vocabulary object de-
tection with vision transformers. European Conference on
Computer Vision (ECCV), 2022. 3,4,6
[36] Pinelopi Papalampidi, Skanda Koppula, Shreya Pathak,
Justin Chiu, Joe Heyward, Viorica Patraucean, Jiajun Shen,
Antoine Miech, Andrew Zisserman, and Aida Nematzdeh.
A simple recipe for contrastively pre-training video-first en-
coders beyond 16 frames, 2023. 8
[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In icml, 2021. 2,
3,4,6
[38] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and
Cordelia Schmid. End-to-end generative pretraining for mul-
timodal video captioning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 17959–17968, 2022. 1,2
[39] Sanjay Subramanian, Medhini Narasimhan, Kushal
Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia
Schmid, Andy Zeng, Trevor Darrell, and Dan Klein. Mod-
ular visual question answering via code generation. arXiv
preprint arXiv:2306.05392, 2023. 1,2,3
[40] D ´ıdac Sur ´ıs, Sachit Menon, and Carl V ondrick. Vipergpt:
Visual inference via python execution for reasoning. arXiv
preprint arXiv:2303.08128, 2023. 1,2,3,5,6,8
[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971, 2023. 1,2
[42] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
Git: A generative image-to-text transformer for vision and
language. arXiv preprint arXiv:2205.14100, 2022. 1,2
[43] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran
Cheng, and Ping Luo. End-to-end dense video captioning
with parallel decoding. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 6847–
6857, 2021. 1,2
[44] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun
Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun
Wang, et al. Internvideo: General video foundation models
via generative and discriminative learning. arXiv preprint
arXiv:2212.03191, 2022. 8
[45] Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou,
Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chen-
guang Zhu, Derek Hoiem, et al. Language models with
image descriptors are strong few-shot video-language learn-
ers.Advances in Neural Information Processing Systems, 35:
8483–8497, 2022. 3
[46] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum,
and Chuang Gan. Star: A benchmark for situated reasoning
in real-world videos. In Thirty-fifth Conference on Neural
Information Processing Systems (NeurIPS), 2021. 2
[47] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.
Next-qa: Next phase of question-answering to explaining
temporal actions. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages
9777–9786, 2021. 2,3,5,8
[48] Junbin Xiao, Angela Yao, Yicong Li, and Tat Seng Chua.
Can i trust your answer? visually grounded video question
answering. arXiv preprint arXiv:2309.01327, 2023. 2,4,8
[49] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,
Xiangnan He, and Yueting Zhuang. Video question answer-
ing via gradually refined attention over appearance and mo-
tion. In ACM Multimedia, 2017. 2
13244
[50] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, So-
ham Ghosh, Yonghui Wu, and Jiahui Yu. Videococa: Video-
text modeling with zero-shot transfer from contrastive cap-
tioners. arXiv preprint arXiv:2212.04979, 2022. 8
[51] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev,
and Cordelia Schmid. Just ask: Learning to answer ques-
tions from millions of narrated videos. In Proceedings of
the IEEE/CVF international conference on computer vision,
pages 1686–1697, 2021. 2,3,6,8
[52] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Zero-shot video question answering via
frozen bidirectional language models. Advances in Neural
Information Processing Systems, 35:124–141, 2022. 8
[53] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, An-
toine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and
Cordelia Schmid. Vid2seq: Large-scale pretraining of a vi-
sual language model for dense video captioning. In CVPR,
2023. 2,8
[54] Qinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu, Qi Qian,
Ji Zhang, and Fei Huang. Hitea: Hierarchical temporal-
aware video-language pre-training. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 15405–15416, 2023. 8
[55] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mplug-owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178, 2023. 8
[56] Keunwoo Peter Yu. VideoBLIP, 2023. 2
[57] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal.
Self-chained image-language model for video localization
and question answering. In NeurIPS, 2023. 2,8
[58] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yuet-
ing Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for
understanding complex web videos via question answering.
InProceedings of the AAAI Conference on Artificial Intelli-
gence, pages 9127–9134, 2019. 2,3,6,8
[59] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choro-
manski, Adrian Wong, Stefan Welker, Federico Tombari,
Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. So-
cratic models: Composing zero-shot multimodal reasoning
with language. ICLR, 2023. 2,3
[60] Yan Zeng, Xinsong Zhang, , and Hang Li. Aligning texts
with visual concepts. arXiv preprint arXiv:2111.08276,
2021. 3
[61] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An
instruction-tuned audio-visual language model for video un-
derstanding. In EMNLP 2023 Demo, 2023. 2,8
[62] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun
Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and
Yu Qiao. Llama-adapter: Efficient fine-tuning of language
models with zero-init attention. ICLR, 2024. 6,8
[63] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,
Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-
former language models. arXiv preprint arXiv:2205.01068,
2022. 1,2[64] Yaoyao Zhong, Junbin Xiao, Wei Ji, Yicong Li, Wei-
hong Deng, and Tat-Seng Chua. Video question answer-
ing: Datasets, algorithms and challenges. arXiv preprint
arXiv:2203.01225, 2022. 2
[65] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher,
and Caiming Xiong. End-to-end dense video captioning with
masked transformer. In CVPR, 2018. 2
[66] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. MiniGPT-4: Enhancing vision-language
understanding with advanced large language models. arXiv
preprint arXiv:2304.10592, 2023. 2
13245
