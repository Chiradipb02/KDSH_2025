A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural
Network
Ruichen Ma, Guanchao Qiao, Yian Liu, Liwei Meng, Ning Ning, Yang Liu, Shaogang Hu*
University of Electronic Science and Technology of China
ruichen.ma@std.uestc.edu.cn, sghu@uestc.edu.cn
Abstract
Binary neural networks utilize 1-bit quantized weights
and activations to reduce both the model’s storage demands
and computational burden. However, advanced binary ar-
chitectures still incorporate millions of inefficient and non-
hardware-friendly full-precision multiplication operations.
A&B BNN is proposed to directly remove part of the mul-
tiplication operations in a traditional BNN and replace the
rest with an equal number of bit operations, introducing the
mask layer and the quantized RPReLU structure based on
the normalizer-free network architecture. The mask layer
can be removed during inference by leveraging the intrinsic
characteristics of BNN with straightforward mathematical
transformations to avoid the associated multiplication op-
erations. The quantized RPReLU structure enables more
efficient bit operations by constraining its slope to be in-
teger powers of 2. Experimental results achieved 92.30%,
69.35%, and 66.89% on the CIFAR-10, CIFAR-100, and Im-
ageNet datasets, respectively, which are competitive with
the state-of-the-art. Ablation studies have verified the effi-
cacy of the quantized RPReLU structure, leading to a 1.14%
enhancement on the ImageNet compared to using a fixed
slope RLeakyReLU. The proposed add&bit-operation-only
BNN offers an innovative approach for hardware-friendly
network architecture.
1. Introduction
Neural networks have made remarkable strides in tasks in-
cluding image classification [1–3], object detection [4, 5],
speech recognition [6, 7], and text generation [8–10], signif-
icantly advancing the development of various fields. How-
ever, as the scale of deep neural networks (DNNs) ex-
pands, the substantial computational and storage require-
ments make them feasible only for running on powerful
but expensive GPUs, leaving edge devices unattainable
[11, 12]. Hardware-efficient network architectures, such
*Corresponding authoras spiking neural networks (SNNs), although they may not
outperform traditional networks in numerous domains, of-
fer the advantage of eliminating multiplication operations
[13, 14]. This reduction in hardware complexity substan-
tially reduces expenses associated with chip design, making
them appealing to chip designers [15–17].
Numerous approaches have been devised to mitigate
hardware overhead in traditional DNNs, albeit often at the
expense of modest performance compromises. Binary neu-
ral networks (BNNs) stand out among these approaches,
aiming to achieve 1-bit quantization of network parame-
ters and activations to reduce storage and computational re-
quirements. Pioneering study [18] has enabled BNNs to
perform inference exclusively through logical operations,
leading to a notable decrease in chip power consumption
and design cost. Recognizing its suboptimal performance
on large datasets such as ImageNet, subsequent studies have
introduced various techniques to improve overall perfor-
mance. These techniques have become nearly indispensable
for advanced BNN models, including scaling factors [19],
BN [20] layers, PReLU [21] layers, and real-value residuals
[22]. However, these layers will unavoidably introduce full-
precision multiplication operations that are not conducive to
hardware efficiency, conflicting with the fundamental goal
of BNN. Although the multiplication operand (MO) intro-
duced is only in the order of millions, it still imposes a sig-
nificant burden on the hardware, and chip designs that cir-
cumvent multipliers are preferable.
This study presents A&B BNN, a binary network archi-
tecture designed to eliminate all multiplication operations
during inference, and was evaluated on three widely used
structures ResNet-18/34 and MobileNet. The accuracies
for various mainstream BNNs and A&B BNN on the Im-
ageNet dataset are presented in Tab. 1, with correspond-
ing results of 61.39%, 65.19%, and 66.89% for the three
structures, respectively. The key to eliminating multiplica-
tion lies in removing the BN layer in the network topology
while minimizing the loss. Studies [26] and [27] introduced
normalizer-free network architecture and study [25] ex-
tended this technique to BNN and proposed BN-Free BNN.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5704
Figure 1. The architecture overview of the (a) baseline BN-Free network and (b) proposed A&B BNN. In contrast to the baseline network,
the proposed A&B BNN eliminates all multiplication operations. The multiplication resulting from βis absorbed into the newly introduced
mask layer and can be removed directly during inference. Multiplications induced by both average pooling and αare substituted by equal
but more efficient bit operations. Additionally, we introduce the quantized RPReLU structure, effectively removing the multiplication
associated with PReLU. Circles represent multiplication operations, diamonds represent bit operations, andLrepresents residual addition.
Binary Network Architecture MO Acc
BNN-ResNet-18 [23] 1.51 M 42.2%
XNOR-ResNet-18 [19] 3.20 M 51.2%
Bi-ResNet-18 [22] 20.86 M 56.4%
ReActNet-18 [24] 22.37 M 65.5%
ReActNet-18 (BN-Free) [25] 4.60 M 61.1%
Bi-ResNet-34 [22] 22.20 M 62.2%
ReActNet-A [24] 10.79 M 69.4%
ReActNet-A (BN-Free) [25] 14.65 M 68.0%
Bi-ResNet-18 (A&B BNN) 0 60.38%
ReActNet-18 (A&B BNN) 0 61.39%
ReActNet-34 (A&B BNN) 0 65.19%
ReActNet-A (A&B BNN) 0 66.89%
Table 1. Top-1 Accuracies of different BNNs evaluated on Ima-
geNet dataset.
While this topology appears to double the multiplication
operand, it can be obviated through characteristics of BNN
and several mathematical transformations. The mask layer
is proposed to execute these multiplication operations, serv-
ing the purpose of gradient scaling during training, and can
be removed during inference. Furthermore, we introduce
the quantized RPReLU structure, replacing the multiplica-
tion operations introduced by PReLU with an equal numberof bit operations. Experiments show that the proposed ap-
proach achieves competitive performance compared to the
state-of-the-art on CIFAR-10, CIFAR-100, and ImageNet
datasets while eliminating multiplication operations, rep-
resenting a valuable trade-off. Our code is available at
https://github.com/Ruichen0424/AB-BNN .
2. Releted Work
Binary neural networks. Binary neural networks binarize
weights and activations through the sign function, which
renders the backpropagation algorithm ineffective due to
its non-differentiable characteristic until [18] proposed the
straight-through estimator (STE) technique. STE passes the
gradient of binary values to full-precision values directly to
disregard the influence of the sign function in the chain rule.
Study [19] proposed using scaling factors to compensate the
loss incurred during binarization, ∥W∥ℓ1/nfor weights and
∥a∥ℓ1/nfor activations. We retain the weight factor as it
can be integrated into the weight matrix while excluding the
activation factor to avoid introducing any multiplication op-
erations. [22] suggested employing real values in the resid-
ual instead of binary to enhance the expressive capacity of
BNN. Since this would introduce multiplication operations,
we did not use this technique. To enforce BNNs to learn
similar distribution as full-precision networks [19, 28, 29],
[24] introduced the ReAct Sign (RSign) and ReAct PReLU
5705
Techniques MO
Scaling factor of activations (c+ 1)·h·w
BN layer c·h·w
PReLU layer c·h·w
Real-value residual cin·cout·hout·wout·kh·kw
Average pooling layer cout·hout·wout
αin BN-Free c·h·w
βin BN-Free c·h·w
Table 2. Multiplication operands introduced by different tech-
niques.
(RPReLU) structures which added some bias layers based
on the original. [25] applies the BN-Free topology pro-
posed in [26, 27] to BNN to eliminate the BN layer, which
is the foundation of this work and is shown in Fig. 1. This
topology employs the scaled weight standardization tech-
nique to regulate the mean and variance of each layer’s
activations and proposes the residual blocks of the form
xℓ+1=xℓ+α·fℓ(xℓ/βℓ).αcontrols the variance growth
rate of the residual block and βregulates the activation dis-
tribution, constituting the primary source of multiplication
in BN-Free BNN.
Efforts to eliminate multiplication. In BNN, the
seminal work [18] introduced XNOR-counts to substitute
full-precision multiplication operations, reducing hardware
overhead by over 200 times. Studies [22, 30–33] proposed
diverse STE functions to enhance training effectiveness,
while [34–38] employed a range of regularization terms to
enhance weights and pre-activation distributions, thereby
augmenting network expression. Additionally, there exist
studies [39, 40] focused on enhancing network performance
through optimizing training strategies and [41–43] dedi-
cated to refining the loss function. Regrettably, though none
of these techniques introduce multiplication operations, al-
most all of these studies employ methods that do. In spiking
neural networks [13, 14], activations manifest as discrete
spikes i.e. {0, 1}, enabling computation solely through
addition operations. [44] proposed AdderNet framework
wherein ℓ1distance is employed instead of convolution as
a metric for assessing the relationship between features and
filters within neural networks, with operations limited to ad-
dition and subtraction. Nevertheless, because of the pres-
ence of the BN layer, there are still millions of multiplica-
tion operations involved, making it not a true multiplication-
operation-free network. Table 2 shows the multiplication
operands introduced by different techniques.
3. Method
This section initially presents the foundational knowledge,
followed by an assessment of the multiplication operands
and their distribution within the BN-Free structure. Sub-sequently, we explore the removable mask layer and then
introduce the bit operation and the quantized RPRuLU unit.
Finally, we discuss the practical hardware benefits.
3.1. Preliminary
Scaled weight standardization. To address the mean shift
and variance explode or vanish on activations resulting from
BN removal, the scaled weight standardization (SWS) tech-
nique from [26] was introduced. Specifically, the weights
are scaled as follows:
ˆWi,j=γ·Wi,j−µi√
Nσi(1)
where µi= (1/N)P
jWi,j,σ2
i= (1/N)P
j(Wi,j−µi)2,
Nis the fan-in, and ˆWi,jis the corresponding standardized
weights. γis related to the activation function and is 1 for
the sign function. SWS technique does not introduce any
multiplication operation during inference.
Adaptive gradient clipping. Gradient clipping is com-
monly used to restrict the norm of gradients [45] to maintain
training stability [46]. [27] proposed the adaptive gradient
clipping (AGC) technique to improve the performance of
the normalizer-free network, which can be described as:
Gl
i→

λ·∥Wl
i∥∗
F
∥Gl
i∥FGl
i,if∥Gl
i∥F
∥Wl
i∥∗
F> λ
Gl
i, otherwise(2)
where lrepresents the corresponding layer and irepre-
sents the ithrow of a matrix.Wl
i∗
F= max {∥Wi∥F, ϵ},
ϵ= 10−3and∥·∥Fis the Frobenius norm. The clipping
threshold λis a crucial hyperparameter that is usually tuned
by grid search.
Distillation loss functions. Study [24] introduced Dis-
tillation loss to enforce the similarity of distribution be-
tween full-precision networks and binary networks to im-
prove performance. The loss is calculated as follows:
LDis=−1
nX
cnX
i=1ρR
c(Xi)×logρB
c(Xi)
ρRc(Xi)
(3)
where nis the batch size, crepresents classes and Xiis the
input image. ρR
cis the softmax output of the full-precision
network and ρB
crepresents the corresponding softmax out-
put of the binary network.
3.2. Multiplication Operations in BN-Free BNN
The components that introduce full-precision multiplication
operations in the BN-Free BNN architecture [25] include α,
β, RPReLU, and the average pooling layer. αandβserve as
manually designed scale factors, normalizing the input and
output to preserve the advantages of the BN layer. To nor-
malize the variance, β=p
Var(xin)is applied before the
5706
Figure 2. The multiplication operand and corresponding ratio
within the BN-Free ReActNet-18 and ReActNet-A structures. For
input images with a resolution of 224×224, the former generates
approximately 4.6 million multiplication operations, while the lat-
ter yields approximately 14.7 million.
convolution operation, and then multiplied by αfor further
computations. Typically, βcorresponds to the expected em-
pirical standard deviation of the activation during initializa-
tion, whereas αis commonly assigned a small value, such
as 0.2. Figure 2 illustrates the operand for the multiplica-
tion operations and the corresponding ratios of BN-Free-
based ReActNet-18 and ReActNet-A on an input picture
with the resolution of 224×224. The former performs 4.6
million multiplication operations, while the latter performs
14.7 million.
3.3. Removable Mask Layer
The mask layer is introduced first to comprehend the gra-
dient transfer through the sign function in BNN. Then we
explain the utilization of mathematical transformation to ef-
ficiently eliminate the multiplication operation caused by β
without incurring any additional cost.
Study [18] proposed the STE technique to realize gradi-
ent transfer of the sign function, and subsequent work pro-
posed various approximation functions fA(·)for optimiza-
tion. This technique involves using the sign function for
binarization during forward propagation and utilizing the
derivative of an approximation function during backpropa-
gation to transfer gradients as shown in Fig. 3a and Eq. (4).
BinaryActivation →Sign(·),forward pass
f′
A(·),backward pass(4)
Mathematically, the gradient approximation technique is
equivalent to the introduction of a mask layer before the
binary activation layer. This layer serves as an additional
activation function, mapping the pre-activations to the bina-
rization layer, and then the binarization layer transfers the
gradient during backpropagation directly. The equivalent
process is depicted in Fig. 3b.
It is crucial to maintain the numerical symbol of the mask
layer unchanged before and after mapping. This layer satis-
fies the relation:
Sign(Mask( x)) = Sign(Mask( k·x))≡Sign( x)(5)
Figure 3. (a) A visualization of gradient approximation tech-
niques. The gradient is transferred through an approximate func-
tion that resembles the impulse function. (b) Introduce the mask
layer to achieve the same effect.
where kis a positive value. This relationship demonstrates
the mathematical properties of the mask layer, enabling the
absorption of a multiplication factor during forward propa-
gation and effectively eliminating the multiplication opera-
tions caused by β. When backpropagating, it satisfies:
∂yout
∂xin=∂yout
∂yML∂yML
∂xin= 1·f′
A(x)≡f′
A(x)(6)
The same gradient transfer effect can be obtained. Figure 4a
illustrates the network structure near β, showcasing the op-
eration of each step. In Fig. 4b, we propose an equivalent
representation that equates one multiplication operation to
two operations. At first glance, it might seem that instead
of eliminating the multiplication operation, it increases it.
However, after completing network training, the product of
ξandβbecomes a constant value that does not require re-
calculation. Furthermore, by leveraging the characteristics
of the mask layer, we can eliminate the second multiplica-
tion, achieving the same effect without any multiplications
during inference. The mathematical characteristics of the
mask layer imply that it does not play a role during infer-
ence and can be directly removed. It must be present during
training as its significance lies in scaling and transforming
values through mask function to regulate gradient transfer
and prevent gradient vanishing. We employ the mask func-
tion described in Eq. (7) and set δ= 3. Its non-zero charac-
teristic improves the activation saturation issue in BNN.
Sigmoid( x, δ) =1
1 +e−δx(7)
5707
Figure 4. (a) The original structure with multiplication. (b) The
equivalent structure, although transforming one multiplication op-
eration into two, can both be eliminated.
3.4. Bit Operations and Quantized RPRuLU Unit
Using bit operations for multiplication by powers of 2
is a straightforward and efficient technique, which relies
on the binary representation of numbers and the proper-
ties of shifting operations. Multiplying a number by a
power of 2 is equivalent to left-shifting its binary repre-
sentation by a specific number of bits. Shifting operations
are highly optimized in many architectures and can be per-
formed quickly, often surpassing traditional multiplication
algorithms. Within the BN-Free structure, the parameter α
is typically assigned a small value, such as 0.2. By conduct-
ing the parameter search, it is possible to set αto a negative
integer power of 2, effectively substituting multiplication
operations with an equal number of bit operations. Simi-
larly, all average pooling layer kernel sizes are set to 2×2,
enabling the replacement of the division operation with a
right shift of two bits.
When dealing with RPReLU, a straightforward approach
is to substitute the PReLU with LeakyReLU and set the
slope to a constant integer power of 2. Nevertheless, the re-
sults of ablation studies indicate a decrease in performance
with this approach. To address this issue, we propose the
quantized RPReLU unit. In this unit, the parameters of
PReLU’s each channel are quantized values and are con-
strained to integer powers of 2, the expression is as follows:
f(yi) =yi, ifyi≥0
2round( ai)·(yi+ξi1) +ξi2,otherwise
(8)
where ai,ξi1, and ξi2are all learnable parameters corre-
spond to the channel i. Figure 5 illustrates the function
graph of PReLU and the proposed quantized RPReLU, re-
spectively. The green area represents the allowed slope
range, which can take any continuous value in RPReLU,
whereas only discrete quantized values are allowed in quan-
tized RPReLU. Ablation studies show that utilizing the
Figure 5. The slope of RPReLU can be any continuous value
greater than 0, while the slope of the proposed quantized RPReLU
is only allowed to be an integer power of 2.
Units LUTs Slice DSPs
Multiplier 47 12 4
Bit-shift 32 ( ↓31.9%) 11 ( ↓8.3%) 0 ( ↓100% )
PReLU 57 17 4
QPReLU 32 ( ↓43.9%) 9 ( ↓47.1%) 0 ( ↓100% )
Table 3. Hardware overhead table for different units.
quantized RPReLU can enhance accuracy by 1.14% when
compared to using RLeakyReLU on the ImageNet dataset.
3.5. Hardware Benefits
The A&B BNN architecture proposed in this study holds
significant practical importance. We conducted synthesis
on various units using the Xilinx Zynq-7000 Z-7045 FPGA,
and the hardware overhead is presented in Tab. 3. The re-
sources integrated on the chip include 218,600 Look-Up Ta-
bles (LUTs), 54,650 slices, and 900 Digital Signal Proces-
sors (DSPs). The synthesis results demonstrate that the 32-
bit full-precision multiplier consumes 47 LUTs, 12 slices,
and 4 DSPs, whereas the bit-operator only necessitates 32
LUTs, 11 slices, and no DSP. Standard PReLU structures
demand 57 LUTs, 17 slices, and 4 DSPs, while quantized
PReLU structures utilize only 32 LUTs, 9 slices, and no
DSP. We achieve this reduction by converting the multipli-
cation operations introduced by the αparameter, the PRePU
layer, and the average pooling layer into an equal number of
bit-shift operations. Additionally, we eliminate the multipli-
cation operations associated with β. Our approach does not
introduce any additional computation or storage overhead.
For one convolution layer without pooling, the reduction in
consumption for three hardware resources is 57.6%, 51.2%,
and 100%, respectively.
In scenarios where the chip lacks built-in multiplication
support, yet the neural network demands it, a viable solution
entails transmitting intermediate results to the host for com-
putation and subsequently sending them back to the chip.
This process results in frequent communication and intro-
5708
duces considerable delays. In contrast, the A&B BNN ar-
chitecture enables the network to perform inference entirely
within the chip, necessitating only a single round-trip com-
munication. This streamlined approach significantly dimin-
ishes latency and improves real-time performance.
4. Experiments
4.1. Experimental Setup
We conducted experiments on the CIFAR-10 [47], CIFAR-
100 [47], and ILSVRC12 ImageNet [48] datasets using the
advanced ReActNet-18/34 and ReActNet-A network struc-
tures in binary networks and presented the results. In line
with most binarization works, the input and output layers’
weights are not quantized to ensure performance. All seeds
were fixed to 2023 to ensure the experiments’ repeatability.
Implementation details for ImageNet. ImageNet is a
widely used benchmark dataset in computer vision, consist-
ing of over 1.28 million training images, 50,000 test im-
ages, and 1,000 classes. Due to the significant success of
ReActNet in the binary domain, we utilized our add&bit-
operation-only network on ReActNet-18/34 and ReActNet-
A, which are enhanced versions of ResNet-18/34 [3] and
MobileNetv1 [49] structures, respectively. Additionally,
we employed SWS and AGC techniques, setting λto 0.02
for stability based on [25]. For training, we employed a
two-step strategy [40], i.e., initially training from scratch
and binarizing only the activations in the first stage, then
fine-tuning based on the previous stage and binarizing both
weights and activations in the second stage. At each stage,
we utilized the Adam optimizer to minimize the Distillation
loss functions [24] and training for 128 epochs, starting with
an initial learning rate of 1e-3 and gradually decreasing to
zero using a linear scheduler. The weight decay is set to
5e-6 in the first stage and 0 in the second stage. We use
the same data augmentation as [24, 25], including random
cropping, lighting, and random horizontal flipping, and use
the same knowledge distillation scheme. For the test set,
the image is resized to 256, center-cropped to 224, and then
inputted into the network.
Implementation details for CIFAR-10 and CIFAR-
100. CIFAR-10 and CIFAR-100 datasets consist of 50,000
training images and 10,000 testing images, divided into 10
and 100 classes, respectively. We conducted experiments
using the ReActNet-18 and ReActNet-A structures, follow-
ing the same two-step training strategy as in the ImageNet
experiments, with each step trained for 256 epochs. To
illustrate the versatility of the proposed algorithm, exper-
iments were also conducted on other two mainstream bi-
nary structures Bi-real BNN and XNOR BNN. Since the
ReActNet-A architecture includes more sub-sampling units
and is not well-suited for handling small-sized datasets such
as CIFAR, we adjusted the stride of the initial layer to 1 like
Figure 6. Top-1 accuracy on the ImageNet dataset, and compari-
son with two baselines. The green background represents the first
training step and the orange represents the second step.
ResNet. Data augmentation included random cropping and
horizontal flipping. Additionally, we adopted the SWS and
AGC techniques, setting λto 0.001 based on [25]. The re-
maining experimental settings align with those used in the
ImageNet experiments.
4.2. Comparison with State-of-the-Arts
Results on ImageNet. We first tested the performance of
the proposed A&B ReActNet-18/34 and ReActNet-A mod-
els on ImageNet to validate the algorithm’s effectiveness.
The Top-1 accuracies for the three network structures were
61.39%, 65.19%, and 66.89%, while the Top-5 accuracies
were 83.06%, 86.03%, and 86.83%, respectively. The com-
parison results are presented in Tab. 1, and Fig. 6 illustrates
the Top-1 training results. Compared to the BN-Free struc-
ture, we achieved a reduction of 14.7 million multiplication
operations with an accuracy loss of 1.11% on ReActNet-
A. We achieved a reduction of 4.6 million multiplication
operations on ReActNet-18 while improving the accuracy
by 0.29%. Considering the actual application scenarios of
BNN, its model complexity is much lower than MobileNet,
and the task difficulty is much lower than ImageNet. The
only 1.1% performance loss on ImageNet is difficult to feel
on edge applications. Conversely, the hardware overhead is
an urgent problem to be solved, the multiplier-less structure
proposed is of great significance in actual production [50].
Results on CIFAR-10 and CIFAR-100. To further
validate the algorithm’s effectiveness and versatility, we
conducted additional experiments using the widely used
smaller datasets CIFAR-10 and CIFAR-100. The compar-
ison results are presented in Tab. 4, while the Top-1 train-
ing results are depicted in Fig. 7. The algorithm demon-
strated competitive results on both datasets. The results of
ReActNet-A on CIFAR validate the intuition of modifying
the structure. Merely reducing the stride of the first layer
convolution can lead to a substantial improvement rang-
ing from 5.53% to 12.93%. Since quantization methods
often have higher variance, providing the mean and stan-
dard deviation would provide a better understanding of the
5709
Figure 7. Top-1 accuracy on the CIFAR-10 and CIFAR-100 datasets, and comparison with two baselines.
BNNsCIFAR10 (%) CIFAR100 (%)
BN BN-Free A&B BN BN-Free A&B
XnorNet18 90.21 79.67 89.94 65.35 53.76 64.51
BiResNet18 89.12 79.59 90.09 63.51 54.34 65.52
ReActNet18 92.31 92.08 92.30 68.78 68.34 69.35
ReActNetA 82.95 83.91 89.44 50.30 55.00 63.23
Table 4. Accuracy on the CIFAR-10 and CIFAR-100 datasets with
two baselines.
experimental results. Experiments with ReActNet-18 on
CIFAR10/100 using five adjacent seeds were conducted,
achieving µ= 92.31%/69.37%,σ= 6.35e−4/5.40e−4.
4.3. Ablation Study
In the previous section, we demonstrated the effectiveness
of the proposed A&B BNN and achieved competitive re-
sults. In this section, we further illustrate the necessity of
the proposed quantized RPReLU and its impact on net-
work performance through ablation studies and then ex-
plore the impact of the hyperparameter α. Due to compu-
tational constraints, all ablation studies are conducted using
the ReActNet-18 structure.
Quantized RPReLU unit. To eliminate the multiplica-
tion operation introduced by RPReLU, we propose replac-
ing PReLU with LeakyReLU or utilizing quantized PReLU.
Although LeakyReLU with a fixed slope is a straightfor-
ward option, it can lead to performance degradation on
complex datasets. To compare the effectiveness of different
options, Tab. 5 provides a comparison between quantized
RPReLU and RLeakyReLU with a fixed slope of 2−3/2−7,
which approximates the commonly used slopes of 0.1 and
0.01. Figure 8 depicts the Top-1 accuracies conducted on
three datasets. The results exhibit a significant disparity in
the impact of the activation function type between the sim-
ple dataset CIFAR and the more complex dataset ImageNet.
In the case of the ImageNet dataset, the activation function
type has only a 0.40% impact during the first stage of ac-TypeImageNet (%) CIFAR-10 (%) CIFAR-100 (%)
Step1 Step2 Step1 Step2 Step1 Step2
RLeakyReLU65.00 60.25 91.67 92.30 69.22 69.31(Slope= 2−3)
RLeakyReLU64.66 60.48 91.61 92.07 67.42 67.90(Slope= 2−7)
Quantized65.40 61.39 91.94 91.94 69.59 69.35RPReLU
Table 5. Ablation studies results of different ReLU structures.
Figure 8. Ablation studies results of different ReLU structures.
The findings demonstrate that employing the proposed quantized
RPReLU architecture enhances performance on the ImageNet
dataset by 1.14% when compared to the fixed-value LeakyReLU.
tivations binarization, but the second-stage impact on the
binarization of both activations and weights reaches 1.14%,
demonstrating the contribution of activation function non-
linearity in enhancing network performance. The complex-
ity of the convolution structure with both weight and acti-
vation binarized is substantially decreased. Weak nonlin-
earity in the activation function can lead to a deterioration
in multi-layer convolutions, reducing the network’s capac-
ity to accommodate the data. Although it introduces only
discrete integer powers of 2, quantized PReLU leads to a
substantial improvement in activation function nonlinear-
ity and enhances network expression. The experimental re-
sults based on the ReActNet-A structure in Tab. 1 employ-
ing BN-Free and A&B BNN also offer supporting evidence
for this claim. The sole difference in network expres-
sion ability between the two resides in whether PReLU is
quantized, leading to a 1.11% improvement, which further
5710
αImageNet (%) CIFAR-10 (%) CIFAR-100 (%)
Step1 Step2 Step1 Step2 Step1 Step2
2−265.40 61.39 91.94 91.94 69.59 69.35
2−364.38 60.55 91.38 91.69 68.81 68.35
Table 6. Ablation studies results of different αvalues.
Figure 9. Ablation studies results using different αvalues. The
results show that the value of 0.25 is consistently better than the
value of 0.125.
highlights the significance of nonlinearity in ReActNet-A
on ImageNet. For ReActNet-18, whether RPReLU is quan-
tized has little effect on the results, which also applies to
the small dataset CIFAR. This implies that network per-
formance in this case is influenced by network complexity,
rather than nonlinearity.
Hyperparameter α.The factor αis utilized after the
convolutional layer to provide feedback to RPReLU, typi-
cally set to a small value such as 0.2. In [26, 27], αis used
to control the growth rate of the variance of the residual
structure, and a large αbrings fast growth while a small α
will weaken the effect of the residual [51], which requires
a trade-off. To explore the impact of α’s value on the net-
work’s performance, we set it to the closest quantized values
to 0.2, specifically 2−2and2−3. The results are presented in
Tab. 6 and Fig. 9 shows the results of Top-1 ablation stud-
ies conducted on three datasets. The experimental results
consistently demonstrate that employing α= 2−2yields
better effects, with the accuracy increasing by 0.25% to 1%
compared to α= 2−3.
4.4. Visualization
The effectiveness of the proposed quantized RPReLU and
its ability to enhance network nonlinearity are demonstrated
through ablation studies. This section presents visualiza-
tions of the range of quantized values and their frequencies
in the quantized RPReLU. Figure 10 illustrates the distribu-
tion of quantization slopes for the trained network, which
quantizes RPReLU on three datasets using ReActNet-18
and ReActNet-A, respectively. The figure reveals that the
quantization values are mainly distributed within the range
of2−19to26. For the small dataset CIFAR, the exponential
distribution ranges from -7 to 2, with a concentration around
-3. For the ImageNet dataset, the quantization distribution
Figure 10. Visualization of the quantified RPReLU architecture
shows the distribution of slopes for ReActNet-18 and ReActNet-A
structures following training on CIFAR-10, CIFAR-100, and Ima-
geNet datasets.
is wider, ranging from -19 to 7, with approximately two
peaks centered around -6 and 2. A broader and more bal-
anced distribution is more representative of non-linearities.
5. Conclusion
This study introduces a novel binary network architecture,
A&B BNN, designed to perform network inference without
any multiplication operation. Building upon the BN-Free
architecture, we introduced the mask layer and the quan-
tized RPReLU structure to completely remove all multi-
plication operations within the traditional binary network.
The mask layer leverages the inherent features of BNNs
and employs straightforward mathematical transformations,
allowing for its direct removal during inference to elimi-
nate associated multiplication operations. Additionally, the
quantized RPReLU structure enhances efficiency by con-
straining its slope to an integer power of 2, employing bit
operations rather than multiplications. Experimental re-
sults show that A&B BNN achieved accuracies of 92.30%,
69.35%, and 66.89% on CIFAR-10, CIFAR-100, and Im-
ageNet datasets, respectively, which is competitive with
state-of-the-art. This study introduces a novel insight for
creating hardware-friendly binary neural networks.
6. Acknowledgment
This work was supported by STI 2030-Major Projects
2022ZD0209700.
References
[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. Advances in neural information processing systems ,
25, 2012.
[2] K Simonyan and A Zisserman. Very deep convolutional
networks for large-scale image recognition. In 3rd In-
ternational Conference on Learning Representations (ICLR
5711
2015) . Computational and Biological Learning Society,
2015.
[3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016.
[4] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Unified, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 779–788, 2016.
[5] Xingkui Zhu, Shuchang Lyu, Xu Wang, and Qi Zhao. Tph-
yolov5: Improved yolov5 based on transformer prediction
head for object detection on drone-captured scenarios. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 2778–2788, 2021.
[6] Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik
McDermott, Stephen Koo, and Shankar Kumar. Transformer
transducer: A streamable speech recognition model with
transformer encoders and rnn-t loss. In ICASSP 2020-2020
IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) , pages 7829–7833. IEEE, 2020.
[7] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals.
Listen, attend and spell: A neural network for large vocab-
ulary conversational speech recognition. In 2016 IEEE in-
ternational conference on acoustics, speech and signal pro-
cessing (ICASSP) , pages 4960–4964. IEEE, 2016.
[8] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. Improving language understanding by gen-
erative pre-training. 2018.
[9] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
pervised multitask learners. OpenAI blog , 1(8):9, 2019.
[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020.
[11] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie.
Model compression and hardware acceleration for neural
networks: A comprehensive survey. Proceedings of the
IEEE , 108(4):485–532, 2020.
[12] Je-Min Hung, Cheng-Xin Xue, Hui-Yao Kao, Yen-Hsiang
Huang, Fu-Chun Chang, Sheng-Po Huang, Ta-Wei Liu,
Chuan-Jia Jhang, Chin-I Su, Win-San Khwa, et al. A four-
megabit compute-in-memory macro with eight-bit precision
based on cmos and resistive random-access memory for ai
edge devices. Nature Electronics , 4(12):921–930, 2021.
[13] Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kher-
adpisheh, Timoth ´ee Masquelier, and Anthony Maida. Deep
learning in spiking neural networks. Neural networks ,
111:47–63, 2019.
[14] Aboozar Taherkhani, Ammar Belatreche, Yuhua Li,
Georgina Cosma, Liam P Maguire, and T Martin McGin-
nity. A review of learning in biologically plausible spiking
neural networks. Neural Networks , 122:253–272, 2020.
[15] Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda.
Towards spike-based machine intelligence with neuromor-
phic computing. Nature , 575(7784):607–617, 2019.[16] Wenqiang Zhang, Bin Gao, Jianshi Tang, Peng Yao, Shi-
meng Yu, Meng-Fan Chang, Hoi-Jun Yoo, He Qian, and
Huaqiang Wu. Neuro-inspired computing chips. Nature
electronics , 3(7):371–382, 2020.
[17] Adnan Mehonic and Anthony J Kenyon. Brain-inspired com-
puting needs a master plan. Nature , 604(7905):255–260,
2022.
[18] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran
El-Yaniv, and Yoshua Bengio. Binarized neural networks:
Training deep neural networks with weights and activations
constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830 ,
2016.
[19] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
and Ali Farhadi. Xnor-net: Imagenet classification using
binary convolutional neural networks. In Computer Vision–
ECCV 2016: 14th European Conference, Amsterdam, The
Netherlands, October 11–14, 2016, Proceedings, Part IV ,
pages 525–542. Springer, 2016.
[20] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In International conference on machine learn-
ing, pages 448–456. pmlr, 2015.
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectifiers: Surpassing human-level perfor-
mance on imagenet classification. In Proceedings of the
IEEE international conference on computer vision , pages
1026–1034, 2015.
[22] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu,
and Kwang-Ting Cheng. Bi-real net: Enhancing the per-
formance of 1-bit cnns with improved representational ca-
pability and advanced training algorithm. In Proceedings of
the European conference on computer vision (ECCV) , pages
722–737, 2018.
[23] Dahyun Kim, Kunal Pratap Singh, and Jonghyun Choi.
Learning architectures for binary networks. In European
conference on computer vision , pages 575–591. Springer,
2020.
[24] Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-
Ting Cheng. Reactnet: Towards precise binary neural net-
work with generalized activation functions. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XIV 16 , pages
143–159. Springer, 2020.
[25] Tianlong Chen, Zhenyu Zhang, Xu Ouyang, Zechun Liu,
Zhiqiang Shen, and Zhangyang Wang. ” bnn-bn=?”: Train-
ing binary neural networks without batch normalization. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4619–4629, 2021.
[26] Andrew Brock, Soham De, and Samuel L Smith. Character-
izing signal propagation to close the performance gap in un-
normalized resnets. In International Conference on Learning
Representations , 2020.
[27] Andy Brock, Soham De, Samuel L Smith, and Karen Si-
monyan. High-performance large-scale image recognition
without normalization. In International Conference on Ma-
chine Learning , pages 1059–1071. PMLR, 2021.
5712
[28] Zhe Xu and Ray CC Cheung. Accurate and compact con-
volutional neural networks with trained binarization. In 30th
British Machine Vision Conference (BMVC 2019) , 2019.
[29] Adrian Bulat and Georgios Tzimiropoulos. Xnor-
net++: Improved binary neural networks. arXiv preprint
arXiv:1909.13863 , 2019.
[30] Charbel Sakr, Jungwook Choi, Zhuo Wang, Kailash
Gopalakrishnan, and Naresh Shanbhag. True gradient-based
training of deep binary activated neural networks via contin-
uous binarization. In 2018 IEEE international conference
on acoustics, speech and signal processing (ICASSP) , pages
2346–2350. IEEE, 2018.
[31] Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin
Gu, Jianzhuang Liu, Rongrong Ji, and David Doermann. Cir-
culant binary convolutional networks: Enhancing the perfor-
mance of 1-bit dcnns with circulant back propagation. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 2691–2699, 2019.
[32] Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen,
Ziran Wei, Fengwei Yu, and Jingkuan Song. Forward and
backward information retention for accurate binary neural
networks. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 2250–2259,
2020.
[33] Peisong Wang, Xiangyu He, Gang Li, Tianli Zhao, and Jian
Cheng. Sparsity-inducing binarized neural networks. In Pro-
ceedings of the AAAI conference on artificial intelligence ,
volume 34, pages 12192–12199, 2020.
[34] Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Mar-
culescu. Regularizing activation distribution for training bi-
narized deep networks. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 11408–11417, 2019.
[35] Tal Rozen, Moshe Kimhi, Brian Chmiel, Avi Mendelson,
and Chaim Baskin. Bimodal-distributed binarized neural
networks. Mathematics , 10(21):4107, 2022.
[36] Sajad Darabi, Mouloud Belbahri, Matthieu Courbariaux, and
Vahid Partovi Nia. Bnn+: Improved binary network training.
2018.
[37] Hyungjun Kim, Jihoon Park, Changhun Lee, and Jae-Joon
Kim. Improving accuracy of binary neural networks us-
ing unbalanced activation distribution. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 7862–7871, 2021.
[38] Yunqiang Li, Silvia-Laura Pintea, and Jan C van Gemert.
Equal bits: Enforcing equally distributed binary network
weights. In Proceedings of the AAAI conference on artifi-
cial intelligence , volume 36, pages 1491–1499, 2022.
[39] Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen,
Dong Huang, and Kwang-Ting Cheng. How do adam and
training strategies help bnns optimization. In International
conference on machine learning , pages 6936–6946. PMLR,
2021.
[40] Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tz-
imiropoulos. Training binary neural networks with real-to-
binary convolutions. In International Conference on Learn-
ing Representations , 2019.[41] Yuzhang Shang, Dan Xu, Bin Duan, Ziliang Zong, Liqiang
Nie, and Yan Yan. Lipschitz continuity retained binary neu-
ral network. In European conference on computer vision ,
pages 603–619. Springer, 2022.
[42] Jiaxin Gu, Junhe Zhao, Xiaolong Jiang, Baochang Zhang,
Jianzhuang Liu, Guodong Guo, and Rongrong Ji. Bayesian
optimized 1-bit cnns. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 4909–4917,
2019.
[43] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong
Chen. Incremental network quantization: Towards lossless
cnns with low-precision weights. In International Confer-
ence on Learning Representations , 2016.
[44] Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao
Xu, Qi Tian, and Chang Xu. Addernet: Do we really
need multiplications in deep learning? In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 1468–1477, 2020.
[45] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On
the difficulty of training recurrent neural networks. In Inter-
national conference on machine learning , pages 1310–1318.
Pmlr, 2013.
[46] Stephen Merity, Nitish Shirish Keskar, and Richard Socher.
Regularizing and optimizing lstm language models. In Inter-
national Conference on Learning Representations , 2018.
[47] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. Master’s thesis, Univer-
sity of Tront , 2009.
[48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115:211–252, 2015.
[49] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 , 2017.
[50] Taylor Simons and Dah-Jye Lee. A review of binarized neu-
ral networks. Electronics , 8(6):661, 2019.
[51] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In Computer
Vision–ECCV 2016: 14th European Conference, Amster-
dam, The Netherlands, October 11–14, 2016, Proceedings,
Part IV 14 , pages 630–645. Springer, 2016.
5713
