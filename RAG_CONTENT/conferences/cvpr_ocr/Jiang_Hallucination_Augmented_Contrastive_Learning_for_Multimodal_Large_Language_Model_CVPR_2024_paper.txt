Hallucination Augmented Contrastive Learning for Multimodal Large Language
Model
Chaoya Jiang1Haiyang Xu2∗Mengfan Dong1Jiaxing Chen1Wei Ye1∗
Ming Yan2Qinghao Ye2Ji Zhang2Fei Huang2Shikun Zhang1
1National Engineering Research Center for Software Engineering, Peking University
2Alibaba Group
{jiangchaoya, wye}@pku.edu.cn, shuofeng.xhy@alibaba-inc.com
Abstract
Multi-modallargelanguagemodels(MLLMs)havebeen
shown to efficiently integrate natural language with visual
informationtohandlemulti-modaltasks. However,MLLMs
still face a fundamental limitation of hallucinations, where
they tend to generate erroneous or fabricated information.
In this paper, we address hallucinations in MLLMs from a
novel perspective of representation learning. We first an-
alyzed the representation distribution of textual and visual
tokensinMLLM,revealingtwoimportantfindings: 1)there
is a significant gap between textual and visual representa-
tions, indicating unsatisfactory cross-modal representation
alignment;2)representationsoftextsthatcontainanddonot
contain hallucinations are entangled, making it challeng-
ing to distinguish them. These two observations inspire us
withasimpleyeteffectivemethodtomitigatehallucinations.
Specifically,weintroducecontrastivelearningintoMLLMs
and use text with hallucination as hard negative examples,
naturallybringingrepresentationsofnon-hallucinativetext
and visual samples closer while pushing way representa-
tionsofnon-hallucinatingandhallucinativetext. Weevalu-
ate our method quantitatively and qualitatively, showing its
effectivenessinreducinghallucinationoccurrencesandim-
proving performance across multiple benchmarks. On the
MMhal-Bench benchmark, our method obtains a 34.66%
/29.5% improvement over the baseline MiniGPT-4/LLaVA.
Our code is available on https:// github.com/ X-
PLUG/mPLUG-HalOwl/tree/main/hacl .
1. Introduction
Large Language Models (LLMs) like GPT-3 [4], LLaMA
[45, 46], and GPT-4 [38] have received significant atten-
tion for their remarkable text understanding and generation
capabilities. Recently, GPT-4V 1[37] has demonstrated im-
∗Corresponding author
1https://openai.com/research/gpt-4v-system-card
(a) Representation distribution of 
LLaVA w/o HACL(b) Representation distribution of 
LLaVA with HACL 
LLaVA with HACL
LLaVA w/o HACL
(c) The blue bar represents overall score on MMhal-Bench and the orange bar represents 
the overall score on MME. Noted biger numbers mean better results. 1.552.08Modality 
Gap
LLaVA with HACL
LLaVA w/o HACL562.58
502.82MMhal-Bench
 MMEFigure 1. Subfigure (a) and subfigure (b) show the distributions
of the last token’s representations yielded by LLM for visual or
textualtokensequences. Blueiconsrepresentimages,greenicons
represent ground truth captions, and red ones represent halluci-
native captions generated by GPT-4. HACL refers to our pro-
posedmethod,HallucinationAugmentedContrastiveLearning. In
subfigure (a), textual and visual representations have cross-model
semantic gaps, while non-hallucinating and hallucinative text rep-
resentations are mixed. This phenomenon is alleviated by HACL,
asshowninsubfigure(b). Subfigure(c)showstheempiricalresults
ofthehallucinationevaluationbenchmarkMMhal-Bench[44]and
the model performance evaluation metric MME [12].
pressive multi-modal abilities in tasks such as image cap-
tion and visual question answering, shedding light on the
vision-language domain and attracting widespread research
interests.
Consequently,anewcategoryofmodels,knownasMulti-
modalLargeLanguageModels(MLLMs)[2,10,26,32,49–
51, 55], has emerged, aiming to enhance LLMs with the
capacity to comprehend and handle visual information. To
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27036
integrate natural language with other modalities, MLLMs
incorporate a learnable interface between pre-trained vi-
sual encodersand LLMs. Suchinterface includes learnable
query tokens [10, 26, 51, 55] or a projection-based linear
model[31,32]thatextractsandintegratesinformationfrom
visual modalities. MLLMs learn this interface to generate
answersformultimodalinstructions,resultinginremarkable
performance in many multimodal tasks.
However, a fundamental limitation of MLLMs is their
tendency to produce erroneous or fabricated information
that doesn’t match the provided visual input, known as hal-
lucination [27, 30, 44, 47]. In this paper, we aim to tackle
the issue from the perspective of representation learning.
We first check the distribution of textual and visual tokens
withintherepresentationspaceofLLMs(Vicuna[54]inour
experiments), in which visual representations are projected
bythelearnedinterface. AsshowninFigure1,wehavetwo
primary findings:
•A significant modality gap remains between the textual
and visual tokens despite visual projection;
•Representations of texts that contain and do not contain
hallucinations are entangled, making it challenging to
differentiate them.
These preliminary observations indicate that the current
learnedinterfacesarenoteffectiveenoughtomapvisualrep-
resentations into the textual representation space of LLMs.
As a result, it is difficult for MLLMs to discriminate be-
tween texts containing minor errors at the level of objects
or attributes and those manifesting typical hallucinative ex-
pressions. This issue potentially heightens the tendency for
MLLMstogeneratemorehallucinations. Therefore,explor-
ingmoreeffectiveapproachestoalignvisualrepresentations
withLLMs’textualrepresentationspacetoaddresshalluci-
nations is crucial.
Inspiredbythefindingsabove,weproposehallucination-
augmentedcross-modalcontrastivelearning(HACL),which
enhancesthealignmentbetweenvisualandtextualrepresen-
tations to alleviate hallucinations. Texts with hallucination
areusedashardnegativeexamplesforimageanchors,natu-
rallypullingcloserrepresentationsofnon-hallucinatingtext
and visual samples while pushing way representations of
non-hallucinating and hallucinative text. Specifically, we
separately feed the visual and textual token sequences into
LLMs to obtain global representations for each modality,
whichisusedforcontrastivelearning. Wegeneratehalluci-
nativeimagecaptionswithGPT-4[38]. Thesehallucinative
textscontainpartialobjectattributeerrorsorintroduceaddi-
tionalnon-existentinformationcomparedtotheoriginalim-
age captions. As shown in Figure 1 (b), introducing HACL
intoLLaVA[32]forcesthevisualrepresentationclosertothe
text representation and makes the correct and hallucinated
text representations more distinguishable. This effective
alignment helps to prevent the generation of hallucinations.Our experiments also show that equipping MLLMs with
HACLnotonlyreducestheoccurrenceofhallucinationsbut
also yields improvements across multiple benchmark eval-
uations. As shown in Subfigure 1 (c), when equipped with
HACL, LLaVA achieves a 29% increase in overall score on
the MMhal-Bench benchmark [44], as well as an 11% im-
provementontheMME[12]benchmark. Inconclusion,this
paper makes the following contributions:
•We underline a significant cross-modal semantic gap be-
tween visual and textual representations and an unex-
pectedrepresentationtanglingamongtextcontainingand
not containing hallucinations in MLLMs. These find-
ings expose the inadequacies of current methodologies
in efficiently bridging the gap between visual and textual
representations.
•Based on these insights, we propose a simple yet ef-
fective method named Hallucination Augmented Cross-
Modal Contrastive Learning (HACL). Introducing con-
trastivelearningintoMLLMsandusinghallucinativetext
as hard negative samples yields a better cross-modal and
morehallucinations-distinguishablerepresentationspace.
•Our experiments show that equipping MLLMs with
HACL not only minigates hallucinations but also effec-
tively improve the performance across multiple bench-
mark evaluations.
2. Related Work
Multi-Modal Large Language Foundation Models. The
successful application of Large Language Models (LLMs)
haspavedthewayfordevelopingseveralapproachesaiming
to augment the perceptual capacities of LLMs with addi-
tional modalities, all within a unified model. There are
three primary methods for constructing multi-modal large
language foundational models, each showing promise for
robust zero-shot generalization capabilities in the vision-
languagedomain. Forinstance,Flamingo[1]isaforerunner
in this area, using a frozen vision encoder and a large lan-
guage model equipped with gated cross-attention for cross-
modality alignment. In contrast, PaLM-E [11] integrates
extracted visual features directly through linear layers into
the pre-trained PaLM [9] model, which boasts 520 billion
parameters, thereby leading to robust performance across
numerous real-world applications. This approach has been
broadlyadoptedbymodelssuchasLLaVA[32],Shikra[7],
etc. One significant limitation of this method, however, is
the creation of lengthy visual sequences. To address this,
BLIP-2[26],drawinginspirationfromDETR[5],developed
a Q-former to reduce the sequence length of visual features
efficiently. This design has been mirrored by Kosmos-1
[17], mPLUG-Owl [51], and MiniGPT-4 [55].
Minigating Hallucination for MLLMs. Toaddresstheis-
sueofhallucinationinMLLMs,researchershavedeveloped
27037
InterfaceLarge Language Model (LLM)
Vision Encoder
A cute dog wearing a 
golden flower on its ears, 
with a wide smile on its 
face. <eos><EOS> <EOS>
Porjected Visual Token SequenceA cute dog, with a green 
leaf on its ears, is smiling 
widely while holding a 
frisbee in its mouth. <eos><EOS>
Ground Truth Caption Hallucinative CaptionAnchorPositive
NegativeAnchorPositive
NegativeHard 
Negative
<EOS> embedding 
of unpaired images
<EOS> embedding 
of paired Image<EOS> embedding 
of unpaired captions
<EOS> embedding 
of hallucinative caption<EOS> embedding 
of ground truth captionsText to Image Contrastive Learning Image to Text Contrastive Learning
Interface
Vision 
Encoder️   Large Language Model 
(LLM)
Interface
Vision 
Encoder  Large Language Model 
(LLM)Stage 1
Stage 2Training Task
1. Text Generation 
2.  Intra-LLM 
Contrastive 
Learning 
1. Self Instruction     
TuningTraining Task
(a) Hallucination Augmented Contrastive Learning (b) Training Paradigm
Figure 2. Subfigure (a) illustrates the proposed HACL. In this framework, we employ GPT-4 [38] to generate the hallucinative captions as
the the hard negative samples in the image-to-text contrastive learning. Subfigure (b) shows the training paradigm of HACL.
variousmethods,whichcanbebroadlycategorizedintotwo
lines. The first line [29, 47] involve limiting the length of
instructiondata,whichtypicallyleadstoareductioninhal-
lucination. Forinstance,LRV-Instruction[29]takesanintu-
itiveapproachbyconstrainingthetextlengthofinstructions
and constructing counterfactual instructions. However, this
may result in less detailed descriptions from the fine-tuned
model. The second line utilizes additional artificial data or
tools to modify hallucinations in the model’s output. For
example, LLaVA-RLHF [44] employs manually annotated
data as reward signals to guide the model in generating less
hallucinative responses. Although effective, this approach
requires extra manual annotation data. In this paper, we
propose a method from the perspective of representation
learning. We introduce hallucinative captions as hard neg-
ative samples in contrastive learning, aiming to narrow the
gap between visual representations and correct textual rep-
resentations, while pushing away from hallucinative textual
representations. This approach effectively addresses the is-
sue of hallucination and also enhances the model’s visual
understanding capability.
3. Method
ThelearnableinterfaceofMLLMsplaysavitalroleinbridg-
ing diverse modalities and mapping visual representationsto the representation space of LLMs. Our goal is to refine
thisinterfacetofacilitatebettermatchingofvisualrepresen-
tationswiththegroundtruthtextintherepresentationspace,
whilealsoincreasingthedistancebetweenthemandhalluci-
nativetext. Toaccomplishthis,weproposeanewapproach
called Hallucination Augmented Cross-modal Contrastive
Learning (HACL). This approach is inspired by contrastive
learning, which is a well-established technique in the fields
ofrepresentationlearning[36]andself-supervisedlearning
[8, 16, 40]. In the following subsection, we first introduce
how to incorporate cross-modal contrastive learning during
training. Next, we describe how to boost contrastive learn-
ingthroughadditionalgeneratedhallucinativecaptions. Fi-
nally,weintroducethehallucinationaugmentedcontrastive
learning training paradigm.
3.1. Cross-modal Contrastive Learning
As shown in Figure 2 (a), our approach can be applied to
any MLLMs that maps or abstracts visual information to
the textual representation space through an learnable inter-
face. Formally, we assume that the MLLM consists of a
visionencoderdenotedas Vθ,alearnableinterfacedenoted
asFα, and a decoder-only based Large Language Model
denoted Lβwhere θ, α, βrepresent the parameters of each
module. Additionally, we also have an unsupervised pre-
training dataset, containing N image-text pairs, denoted as
27038
D={Ii, Ti}, i∈[1,2, . . . , N ].
Assuminganimage Iiisprocessedbythevisionencoder
Vθand the learnable interface Fα, it is transformed into a
visual token sequence of length m. Since most LLMs are
decoder-only models, in order to obtain the representations
that can capture global semantic information. We pass a
< EOS > token through an embedding layer Lβto obtain
the vector representation e∈RDand append it to this se-
quence. Thus,thenewvisualtokensequencebecomes Si
v= 
vi
1, vi
2, . . . , vi
m, ei
v
, where vi
k∈RD, k∈[1,2, /dots, m ].
Similarly,forthecaptionpairedwiththisimage,wealsoap-
pend an <EOS>token to the text token sequence and pass
it through the embedding layer of the LLM to obtain the
text embedding sequence Si
t=
ti
1, ti
2, . . . , ti
n, ei
t
, where
ti
k∈RD, k∈[1,2, /dots, n ]. Subsequently, the visual em-
bedding sequence Svand the text embedding sequence Sv
are individually passed through the LLM Lβto obtain the
final output from the last layer of Lβas following:
Hi
t=Lβ
Si
t
(1)
Hi
v=Lβ
Si
v
(2)
where Hi
v=
ˆvi
1,ˆvi
2, . . . , ˆvi
m,ˆei
v
and Hi
t=ˆti
1,ˆti
2, . . . , ˆti
n,ˆei
t
. Afterwards, we obtain the global repre-
sentation ˆei
vthat captures the overall semantic information
of the image Ii, as well as the global representation ˆei
tthat
capturestheoverallsemanticinformationofthegroundtruth
caption Ti.
Afterwards,similartomanyexistingmethodsinthefield
ofvision-languagepretraining[3,18–20,24,25,48,53],we
introduce the following contrastive learning strategy. As-
suming a batch size of B during the training process, we
compute the text-to-image contrastive learning loss as fol-
lows:
Lt
CL=−X
i=1:B1
Blog
f 
ˆei
t,ˆei
v
f(ˆei
t,ˆeiv) +P
k ̸=if(ˆei
t,ˆekv)
(3)
where f 
ˆei
t,ˆei
v
measures the distance between ˆei
tandˆei
v
in a semantic space. Similar, the image-to-text contrastive
learning loss as follows:
Lv
CL=−X
i=1:B1
Blog
f 
ˆei
v,ˆei
t
f(ˆeiv,ˆei
t) +P
k ̸=if 
ˆeiv,ˆek
t
(4)
3.2. Improving Contrastive Learning with Halluci-
native Captions
Weproposetoimprovetheeffectivenessofcontrastivelearn-
ing by introducing hard negative samples which mimic the
hallucinative text generated by MLLMs.
Generation of Hallucinative Captions Inordertodothis,
weutilizeGPT-4[38]toincorporatesomeelementsintothe
ground truth captions that are either inconsistent with the
image content or completely absent from it. As shown in
Ground Truth Caption: A white bus driving down a small 
street.
Hallucinative Caption: A red bus decorated with yellow and 
green polka dots is driving down a small street.
Ground Truth Caption: A man riding a brown horse in 
uniform next to tall green trees.
Hallucinative Caption: A man riding a brown horse in 
uniform next to tall green trees, holding a silver sword in 
his hand.
Ground Truth Caption: two elephants are touching each 
other in the zoo.
Hallucinative Caption: An elephants and a zebra are 
touching each other in the zoo.Figure 3. This figure showcases a range of hallucinative captions
generated by GPT-4. The hallucinative text is highlighted in red.
Figure 3, these hallucinations can be coarse-grained, focus-
ing on the presence of objects, or fine-grained, focusing on
specific attributes such as quantity, properties, or locations.
Here is our prompt to GPT-4:
Hallucination in Large-scale Visual Language Models
(LVLMs) refers to cases where these models generate de-
scriptions introducing elements that are inconsistent with
the content or completely absent from a provided image.
These hallucinations can be coarse-grained, focusing on
the mere existence of objects, or fine-grained, focusing on
more specific attributes or characteristics such as quantity,
properties, and locations. Your task is to revise a given
captiontocreateamirroredversionthatcloselyalignswith
the original’s content and length but incorporates elements
of hallucination. The first step involves identifying the ob-
jectsinvolvedandtheirassociatedattributeswithinthegiven
caption. Subsequently, combine this insight with the details
concerninghallucinationsprovidedabovetocompleteyour
task.
To improve the generation of more appropriate halluci-
native captions, we also provide some contextual examples
for GPT-4. Please check our appendix for more details.
Hallucination Augmented Contrastive Learning As-
suming that we have generated an hallucinative caption ˙Ti
based on the original caption Tifor the image Ii, and
obtained the global representation ˙ei
tof the hallucinative
caption using the approach described in subsection 3.1, we
cantreatitasanegativesampleintheimage-textcontrastive
learning. Therefore, the new formula for the image-to-text
contrastive learning becomes:
27039
Lv
CL=
−X
i=1:B+11
B+ 1log
f 
ˆei
v,ˆei
t
f(ˆeiv,ˆei
t) +f(ˆeiv,˙ei
t) +P
k ̸=if 
ˆeiv,ˆek
t

(5)
Forthetext-to-imagecontrastivelearning,wehavenotmade
changes and have maintained consistency with the content
presented in subsection 3.1.
3.3. Training Paradigm
As shown in Figure 2 (b) which demonstrates how HACL
is introduced during the training process of MLLMs. Typ-
ically, we incorporate HACL into the first-stage pretraining
ofthemodeltooptimizetheinterface Fαbetter. Therefore,
suppose the loss function of text generation task is denoted
asLGand the optimization object of the first stage can be
defined as follow:
Oα= arg min
αLG+ 
Lv
CL+Lt
CL
/2 (6)
In the second stage, we follow the same approach as other
methods and fine-tune the model using only instructional
data.
4. Experiments
4.1. Implementation
We validatedthe effectiveness ofour methodby applying it
to four different models: miniGPT-4 [55], LLaVA [32] and
LLaVA-1.5 [31].
Data sets For MiniGPT-4, the pre-training phase utilized
significantly large datasets such as LAION[42] (115 mil-
lion), Conceptual Captions [6] (CC3M/CC12M), and oth-
ers. However, generating hallucinative captions for such
enormous datasets is very costly. As a result, for MiniGPT-
4,werandomlysampledabout10milliondata,representing
10% of the total, and didn’t use hallucinative captions for
contrastive learning for the remaining data during training.
Moreover,wediscoveredthatregardlessofnotusinghalluci-
nativecaptionsforenhancement,ourmodelstillsignificantly
enhances models such as MiniGPT-4 [55]. On the other
hand, for the LLaVA [32] and LLaVA1.5 [31], which used
subsets of LAION/CC/SBU datasets with roughly 558K
data, we generated hallucinative captions for every training
datum.
Training Settings We followed the original approach for
MiniGPT-4 [55] and retrained it using the complete pre-
trainingdataset,about10Mdataincludedhallucinativecap-
tions. For LLaVA [32] and LLaVA 1.5 [31], we used the
complete pre-training dataset introduced HACL during the
first stage of pre-training. We keeping the same hyperpa-
rameter settings for all above models. Our experimentswere conducted using 16 NVIDIA A100 GPUs with 80G
of memory. Due to the increased memory usage during
MLLMstraining(whichincludesmodelandgradientdata),
the batch size during contrastive learning was affected. To
address this, we used a queue of size 16,384, similar to the
approaches used for ALBEF [24] and MOCO [8], to store
morenegativesamples. WeusedDeepspeed[41]forLLaVA
and LLaVA 1.5, with a batch size of 64 and 32 on a single
GPU, respectively. For MiniGPT-4, the batch size was 8.
4.2. Effectiveness of HACL on Mitigating Halluci-
nation
Toverifytheefficacyofourproposedmethodinaddressing
hallucination issues, we leveraged two widely used bench-
markevaluationdatasetsthatevaluatethepresenceofhallu-
cinationsinmodels. ThesedatasetsincludedMMHal-Bench
[44] and POPE [27]. MMHal-Bench offers a comprehen-
sive evaluation of models that encompasses multiple per-
spectives, such as attributes, relations, and counting. On
theotherhand,POPEparticularlyfocusesonhallucinations
related to objects. We employed both datasets to measure
the effectiveness of our method in addressing hallucination
across various scenarios.
Evaluation on MMHal-Bench For the MMHal-Bench
[44]. We apply our method to iniGPT-4 [55], LLaVA
[32], LLaVA1.5 [31] and compare the results with other
recentvision-languagemodels,includingMKosmos-2[39],
IDEFICS[21],InstructBLIP[10],andantherLLaVA-RLHF
[44]. Following [44], we use GPT-4 to evaluate the overall
score and hallucination rate of different MLLMs. Table 1
demonstrates a significant improvement in the overall per-
formance of MMHal-Bench after applying our method to
LLaVA[32],MiniGPT-4[55],andLLaVA1.5[31]. Notably,
MiniGPT-4-HACLexhibitedconsiderableperformancegain
over MiniGPT-4 [55]. Moreover, compared with LLaVA-
RLHF[44], a recently proposed method that uses human
feedback and reinforcement learning to address hallucina-
tions, LLaVA-HACL showed an even more significant im-
provement.
Evaluation on POPE In addition, we obtained consistent
results using MMHal-Bench [44] in the POPE evaluation
benchmark[27]. Table2showsthatminiGPT-4-HACLand
LLaVA-HACLbothdemonstratedsignificantimprovements
compared to the original model. Of particular note, the av-
erage F1 score of LLaVA-HACL increased by 17.8% com-
pared to LLaVA [32], while the Yes ratio decreased from
99.55 to 48.25. Furthermore, by applying our method to
LLaVA1.5 [31], LLaVA1.5-HACL easily achieved SOTA
on this benchmark. Noted that LLaVA1.5 [31] is a high-
performingmodelwithalowlikelihoodofgeneratehalluci-
nation, surpassing MiniGPT-4 [55] and LLaVA [32]. This
model’s impressive benchmark scores make it a valuable
27040
MethodOverall Hallucination Score in Each Question Type ↑
Score↑ Rate↓ Attribute Adversarial Comparison Counting Relation Environment Holistic Other
Kosmos-2 [39] 1.69 0.68 2.00 0.25 1.42 1.67 1.67 2.67 2.50 1.33
IDEFICS 9B[21] 1.89 0.64 1.58 0.75 2.75 1.83 1.83 2.50 2.17 1.67
IDEFICS 80B[21] 2.05 0.61 2.33 1.25 2.00 2.50 1.50 3.33 2.33 1.17
InstructBLIP 7B[10] 2.10 0.58 3.42 2.08 1.33 1.92 2.17 3.67 1.17 1.08
InstructBLIP 13B[10] 2.14 0.58 2.75 1.75 1.25 2.08 2.50 4.08 1.50 1.17
LLaVA-RLHF 7B[44] 2.05 0.68 2.92 1.83 2.42 1.92 2.25 2.25 1.75 1.08
LLaVA 7B[32] 1.55 0.76 1.33 0.00 1.83 1.17 2.00 2.58 1.67 1.83
LLaVA 7B-HACL [32] 2.08 (↑0.53)0.62 (↓0.15)2.94 2.01 2.27 1.64 2.35 2.14 1.67 1.63
miniGPT-4 7B[32] 1.39 0.71 0.75 1.83 2.16 0.91 1.25 1.33 0.91 1.91
miniGPT-4 7B-HACL 1.80 (↑0.31)0.65 (↓0.06)1.22 1.85 2.23 1.74 2.13 2.48 1.03 1.58
LLaVA1.5 7B[32] 2.08 0.52 2.75 2.00 2.33 2.08 1.50 1.91 1.91 2.16
LLaVA1.5 7B-HACL 2.13(↑0.05)0.50(↓0.02) 2.95 2.15 2.29 1.97 1.53 1.98 2.02 2.19
Table 1. Evaluation results for different MLLMs on MMHal-Bench.
Datasets Metrics Shikra [7] InstructBLIP [10] MM-GPT [14] mPLUG-Owl [51] MiniGPT-4 [55] w/ HACL LLaVA [32] w/ HACL LLaVA1.5 [31] w/ HACL
RandomAccuracy ( ↑) 86.90 88.57 50.10 53.97 54.64 80.49 ( ↑25.84) 50.97 82.16 ( ↑31.18) 88.17 88.59 ( ↑0.42)
Precision ( ↑) 94.40 84.09 50.05 52.07 57.92 94.32 ( ↑36.39) 50.19 87.30 ( ↑37.11) 97.68 98.62 ( ↑0.93
Recall ( ↑) 79.27 95.13 100.00 99.60 34.65 75.34 ( ↑40.69) 99.13 76.53 ( ↓22.59) 78.93 80.60 ( ↑1.66)
F1-Score ( ↑) 86.19 89.27 66.71 68.39 43.35 83.82 ( ↑40.46) 66.71 81.56 ( ↑14.85) 87.31 88.70 ( ↑1.39)
Yes (→50%) 43.26 56.57 98.90 95.63 31.32 44.33 ( ↑13.01) 99.90 45.19 ( ↓54.71) 41.64 44.43 ( ↑2.78)
PopularAccuracy ( ↑) 83.97 82.77 50.00 50.90 56.67 78.32 ( ↑21.64) 49.87 79.32 ( ↑29.44) 87.46 87.94 ( ↑0.48)
Precision ( ↑) 87.55 76.27 50.00 50.46 58.69 79.23 ( ↑20.54) 49.93 80.34 ( ↑30.41) 95.17 97.23 ( ↑2.06)
Recall ( ↑) 79.20 95.13 100.00 99.40 44.74 74.54 ( ↑29.80) 99.27 76.60 ( ↓22.67) 78.93 79.31 ( ↑0.37)
F1-Score ( ↑) 83.16 84.66 66.67 66.94 50.74 76.85 ( ↑26.11) 66.44 78.43 ( ↑11.99) 86.29 87.36 ( ↑1.07)
Yes (→50%) 45.23 62.37 100.00 98.57 62.20 45.23 ( ↓16.97) 99.40 47.64 ( ↓51.76) 41.46 45.03 ( ↑3.57)
AdversarialAccuracy ( ↑) 83.10 72.10 50.00 50.67 54.50 71.32 ( ↑16.82) 49.70 74.47 ( ↑24.77) 85.93 86.54 ( ↑0.61)
Precision ( ↑) 85.60 65.13 50.00 50.34 57.21 70.53 ( ↑13.32) 49.85 73.55 ( ↑23.70) 91.78 93.01 ( ↑1.23)
Recall ( ↑) 79.60 95.13 100.00 99.33 41.45 73.45 ( ↑32.00) 99.07 76.40 ( ↓22.67) 78.93 79.52 ( ↑0.59)
F1-Score ( ↑) 82.49 77.32 66.67 66.82 48.07 71.96 ( ↑23.89) 66.32 74.95 ( ↑8.63) 84.87 85.73 ( ↑0.86)
Yes (→50%) 46.50 73.03 100.00 98.67 38.32 48.23 ( ↑9.91) 99.37 51.93 ( ↓47.44) 43.00 46.33 ( ↑3.33)
Table 2. Object hallucination benchmark using POPE [27] evaluation pipeline . "Yes" signifies the likelihood of the model producing a
positive response.
foundation to build upon.
4.3. Effectiveness of HACL on Visual Comprehen-
sion
HACL has shown effectiveness in solving the issue of hal-
lucination. Nevertheless,weintendtoexploretheinfluence
of HACL on the model’s abilities of visual comprehension
andgeneration. Toachievethisobjective,wecarriedoutas-
sessmentsoncommonbenchmarks,suchasVisualQuestion
Answering (VQA) [15, 35, 43] after incorporating HACL
into the MLLMs. Furthermore, as MLLMs possess ro-
bust zero-shot capabilities, traditional evaluation metrics
often fail to provide a detailed assessment of their abili-
ties. Additionally, their inability to match the given answer
correctly exacerbates significant robustness issues. To mit-
igate these challenges, the research community introduced
a series of benchmarks. These benchmarks aim to system-
atically structure and evaluate complex multi-modal tasks
fromvariousperspectives. Therefore,wealsoevaluatedthe
model’s performance on recently designed MLLM-focused
Multi-modal Benchmarks including MME [12], MMBench
[33], MM-Vet [52], SEED-Bench [22].
Results on Benchmark Tasks Ourevaluationincludessix
popular benchmarks, as summarized in Table 3. We ap-
plied the HACL to three baselins: MiniGPT-4, LLaVA,
and LLaVA1.5, and compared their performance to otherGeneral VQA General VQA (Zero-shot)
Method #Params VQAv2 GQA VizWizQA TextVQA SciQA
BLIP-2 [26] 8.2B 65.0 41.0 19.6 42.5 61.0
InstructBLIP [10] 8.2B - 49.2 34.5 50.1†60.5
Unified-IO XL[34] 2.9B 77.9 - 57.4‡- -
PaLM-E-12B [11] 12B 76.2 - - - -
Shikra [7] 7.2B 77.4 - - - -
Qwen-VL-Chat [2] 9.6B 78.2 57.5 38.9 61.5‡68.2
LLaVA [31] 7.2B 71.3 41.3 36.7 50.2†61.5
LLaVA-HACL [31] 7.2B 73.342.5 37.4 52.2†62.4
MiniGPT-4 [31] 7.2B 65.2 30.8 30.2 52.3†58.4
MiniGPT-4-HACL [31] 7.2B 68.932.3 31.7 54.2†60.3
LLaVA1.5 [31] 7.2B 78.5 62.0 50.0 58.2†66.8
LLaVA1.5-HACL [31] 7.2B 79.1 62.5 50.5 59.8†67.3
Table 3. Performance comparison on visual question answer-
ing.For VQA, accuracy is reported. Note that specialists are
fine-tuned on each individual dataset. †denotes OCR inputs are
utilized. ‡indicatesthemodelhastrainedonthedataset. Wegray
outthosespecialists’methodswhichareindividuallyfine-tunedon
the dataset as well as those fine-tuned results of generalists.
State-of-the-Art (SOTA) MLLMs such as BLIP2[26], In-
structBLIP [10], Shikra [7], and Qwen-VL-Chat [2]. Our
experimental results show that our approach successfully
enhancestheperformanceoforiginalmodelsacrossarange
of VQA datasets. Notably, LLaVA-HACL outperforms
LLaVA[32]intermsofconsistencyandaccuracyacrossall
VQA datasets. Additionally, when compared to LLaVA1.5
[31], LLaVA1.5-HACL achieves better results in General
27041
Method Vision Encoder Language Model MME MMBench MM-Vet SEED-Bench
BLIP-2 [26] ViT-g (1.3B) Vicuna (7B) 1293.84 - 22.4 46.4
mPLUG-Owl [51] ViT-L (0.3B) LLaMA (7B) 967.34 46.6 - 34.0
InstructBLIP [10] ViT-g (1.3B) Vicuna (7B) 1212.82 36.0 26.2 53.4
LLaMA-Adapter-v2 [13] ViT-L (0.3B) LLaMA (7B) 1328.40 39.5 31.4 32.7
Otter [23] ViT-L (0.3B) LLaMA (7B) 1292.26 48.3 24.6 32.9
Qwen-VL-Chat [2] ViT-G (1.9B) Qwen (7B) 1487.58 60.6 - 58.2
LLaVA [32] ViT-L (0.3B) Vicuna (7B) 502.82 36.2 28.1 33.5
LLaVA-HACL [32] ViT-L (0.3B) Vicuna (7B) 562.58 37.8 28.4 33.9
MiniGPT-4 [55] ViT-g (1.3B) Vicuna (7B) 581.67 23.0 22.1 42.8
MiniGPT-4-HACL [55] ViT-g (1.3B) Vicuna (7B) 653.94 24.5 23.8 42.5
LLaVA-1.5 [31] ViT-L (0.3B) Vicuna (7B) 1510.70 64.3 30.5 58.6
LLaVA-1.5-HACL [31] ViT-L (0.3B) Vicuna (7B) 1530.10 64.5 30.4 58.9
Table 4. Zero-shot multi-modal evaluation on multi-modal benchmarks including MME [12], MMBench [33], MM-Vet [52], SEED-
Bench [22]. The overall scores are reported for evaluation. For MMBench, we report test results.
VQA benchmarks abd zero-shot VQA tasks, implying that
MLLMs may not only mitigate hallucinations but also im-
prove correlations between visual and textual information,
which further refines the generalization ability of models.
MLLM-oriented Multi-modal Benchmarks. We applied
HACL to MiniGPT-4 [55], LLaVA [32], LLaVA1.5 [31]
and evaluate them on five recently popular multi-modal
benchmarks in a zero-shot manner. For a fair compari-
son, we select models with similar language model sizes,
particularly those from the LLaMA [45] family, and detail
their differences in the vision encoder. The results of our
evaluation are listed in Table 4. We discovered that after
implementing HACL, all three models exhibited improve-
mentsacrossmultiplebenchmarks. Notably,forLLaVAand
MiniGPT-4, the enhancement was particularly evident on
the MME [12] benchmark. For instance, after implement-
ing HACL, LLaVA’s MME score improved from 581.67 to
653.94. Theseresultsindicatethatourmethodologycannot
only reduce the instances of model hallucination but also
enhance the model’s visual comprehension capabilities.
4.4. Ablation Study
Impact of Hallucinative Captions To validate the effec-
tiveness of using hallucinative captions as hard negative
samplesincontrastivelearningforresolvinghallucinations,
we conducted the following experiments: In the Stage 1
pre-training phase, we did not introduce any additional hal-
lucinative captions, and the contrastive learning loss was
calculated solely based on the equations 3 and 9 discussed
in subsection 3.1 of our paper. We conducted experiments
on MLLMs including LLaVA [32], MiniGPT-4 [55], and
LLaVA1.5[31],andreportedtheresultsonbenchmarkssuch
asPOPEandMMHal-Bench. Additionally,wealsoreported
results on the MME and VQA benchmarks. As illustrated
in the Table 5, absent the facilitation from hallucinative
captions, the models displayed moderate improvements on
hallucination benchmarks such as MMhal-Bench, yet theseModel w/ CL w/ HC POPE MMHal VQA MME
LLaVA × × 66.48 1.55 71.32 502.82
LLaVA ✓ ×69.23 1.67 72.98 549.04
LLaVA ✓ ✓ 78.31 2.08 73.30 562.58
MiniGP-4 × × 47.38 1.39 65.2 581.67
MiniGP-4 ✓ ×53.54 1.45 67.6 633.21
MiniGP-4 ✓ ✓ 77.54 1.80 68.9 653.94
LLaVA1.5 × × 86.15 2.08 78.5 1510.70
LLaVA1.5 ✓ ×86.31 2.09 78.7 1523.84
LLaVA1.5 ✓ ✓ 87.26 2.13 79.1 1530.10
Table5. Theresultofablationsfortheimpactofhallucinativecap-
tions. We report the text-dev score results of POPE[27], MMhal-
Bench[44],VQAandMME.w/CLreferstotrainingMLLMswith
Contrastive Learning for MLLMs, w/ HC refers to utilize halluci-
nativecaptionstoenhancethecontrastivelearning. ForPOPE,we
report the average F1 score.
model +VE +LLM POPE MMHal VQA MME
LLaVA-HACL ✓ ×63.42 1.43 65.0 324.50
LLaVA-HACL × ✓78.53 2.08 74.2 580.32
LLaVA-HACL × × 78.31 2.08 73.3 562.58
LLaVA1.5-HACL ✓ ×68.89 1.53 69.6 459.34
LLaVA1.5-HACL × ✓87.23 2.13 79.4 1542.48
LLaVA1.5-HACL × × 87.26 2.13 79.1 1530.10
Table 6. Results of models under different training paradigms.
"+VE" denotes training the Visual Encoder during Stage 1 pre-
training, while "+LLM" indicates training the LLM during Stage
1 pretraining.
improvements were somewhat constrained. However, the
subsequent inclusion of hallucinative captions resulted in
a marked enhancement on the same hallucination bench-
mark, thus affirming the potency of the hallucinative cap-
tions. Furthermore, we observed analogous improvements
in the model’s performance on both MME and VQA. Our
hypothesis asserts that hallucinative captions aid MLLMs
in diverting the visual representation from hallucinations
and other textual inaccuracies. This action helps avoid in-
stances of hallucination. Furthermore, contrastive learning
supportsthemodelbyaligningthesemanticsofimage-text,
27042
PC10.6
 0.4
 0.2
 0.0 0.2 0.4 0.6PC2
0.4
0.2
0.00.20.40.6PC3
0.4
0.2
0.00.20.40.6Ground Truth
Hallucination
Image(a) w/o HACL
PC10.6
 0.4
 0.2
 0.0 0.2 0.4 0.6PC2
0.6
0.4
0.2
0.00.20.40.6PC3
0.4
0.2
0.00.20.40.6Ground Truth
Hallucination
Image (b) w/ CL
PC10.6
 0.4
 0.2
0.0 0.2 0.4 0.6 0.8PC2
0.6
0.4
0.2
0.00.20.40.6PC3
0.6
0.4
0.2
0.00.20.40.6Ground Truth
Hallucination
Image (c) w/ HACL
Figure 4. This figure illustrates the visualization of various data distributions. The blue icons represent visual data extracted from images,
thegreeniconsdenotegroundtruthcaptiondata,andtherediconssignifyhallucinativecaptiondata. Thelabel"w/oHACL"representsthe
data distribution obtained from the original model output without employing our proposed method. On the other hand, "w/ CL" indicates
the data distribution resulting from the model output utilizing contrastive learning. Lastly, "w/ HACL " indicates the data distribution
generated by the model output using our proposed method.
which ultimately enhances the model’s effectiveness.
Discussion on Training Paradigm We have observed
that certain Multimodal Language-and-Vision Models
(MLLMs) may not freeze the activity of either the visual
encoder or the Language-and-Vision Models (LLMs) dur-
ing the initial stage of pretraining. To assess the impact
ofourmethodologyundersuchdistincttrainingparadigms,
weindependentlytestedmodelswhereeithertheVisualEn-
coder or the LLMs were active during the first pretrain-
ing phase. These tests were conducted on two platforms:
LLaVA and LLaVA1.5 and subsequently evaluated against
multiplebenchmarkstandards. AsillustratedinTable6,the
modelsexperiencedasignificantperformancedeclinewhen
LLMs are activated. We hypothesize that this downturn
could be linked to low-quality data in the first pretraining
stage and the introduction of additional contrast learning
tasks, both of which affect the LLMs’ representation dis-
tribution. This culminates in the catastrophic forgetting of
the LLMs. Conversely, initiating the Visual Encoder led to
a modest performance boost. This might be attributed to
the fact that the target parameters our model can optimize
extend beyond the learnable interface and incorporate the
visual encoder as well. This expanded scope paves the way
foramoresuccessfulalignmentofvisualandtextrepresen-
tations within the MLLMs.
4.5. Visualization
TheobjectiveofourresearchistheintroductionofHACL,to
furtherenhancethevisualrepresentationoutputofourinter-
face. Theaimistocloselyaligntheoutputtothecorrecttex-
tual representation within the representation space of Lan-
guageModels(LLMs)and,atthesametime,distanceitfrom
hallucinativeandotherincorrecttextualrepresentations. Tosubstantiateourobjective,werandomlyselected200image-
textpairsfromtheCOCO[28]val2017dataset. UsingGPT-
4, we generated hallucination samples and subsequently re-
duced these samples using the hidden state representation
of the last token through LLMs for visualization purposes.
The data distribution under three conditions: without em-
ploying HACL, instigating cross-modal contrastive learn-
ing but without the use of hallucination-enhanced samples,
and usage of hallucination-enhanced sample contrast learn-
ing was visualized respectively. The MLLM utilized in our
studywasLLaVA.AsillustratedinFigure4(a),asubstantial
modality gap is observable in the data distribution without
contrast learning. In Figure 4 (b), after applying contrast
learning, although the modal gap decreased, a differentia-
tioninthedistributionofhallucinationsamplesandground
truthsampleswasunattainable. InFigure4(c),withtheap-
plicationofhallucinationaugmentationincontrastlearning,
not only did the modal gap decrease, but the hallucination
sample distribution was also significantly distanced.
5. Conclusion
This paper addresses the issue of hallucinations in
Multi-modal Large Language Models (MLLMs) and
proposes a method called Hallucination Augmented
Contrastive Learning (HACL) to improve the alignment
between visual and textual representations. By using
contrastive learning on projected text and visual token
sequences, and incorporating hallucinative captions as
hard negative samples, HACL effectively reduces the
occurrence of hallucinations. Experimental results
demonstrate that incorporating HACL enhances the
performance of MLLMs and significantly reduces the
occurrence of hallucinations in benchmark evaluations.
27043
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. Advances in
NeuralInformation Processing Systems, 35:23716–23736,
2022. 2
[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shĳie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-vl: Afrontierlargevision-languagemodelwith
versatile abilities. ArXiv, abs/2308.12966, 2023. 1, 6, 7
[3] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu,
Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som,
SonghaoPiao,andFuruWei. Vlmo: Unifiedvision-language
pre-training with mixture-of-modality-experts. Advances in
NeuralInformation Processing Systems, 35:32897–32912,
2022. 4
[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
PranavShyam,GirishSastry,AmandaAskell,SandhiniAgar-
wal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu,
ClemensWinter,ChristopherHesse,MarkChen,EricSigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-
shot learners. ArXiv, abs/2005.14165, 2020. 1
[5] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-
to-end object detection with transformers. In European
conference oncomputer vision, pages 213–229. Springer,
2020. 2
[6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual12m: Pushingweb-scaleimage-textpre-
trainingtorecognizelong-tailvisualconcepts.In Proceedings
oftheIEEE/CVF Conference onComputer Visionand
PatternRecognition, pages 3558–3568, 2021. 5
[7] Ke Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng
Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s
referentialdialoguemagic. ArXiv,abs/2306.15195,2023. 2,
6
[8] XinleiChen,HaoqiFan,RossGirshick,andKaimingHe.Im-
provedbaselineswithmomentumcontrastivelearning. arXiv
preprintarXiv:2003.04297, 2020. 3, 5
[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann,ParkerSchuh,KensenShi,SashaTsvyashchenko,
Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,
Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif,
Nan Du, Benton C. Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,
Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-
mawat, Sunipa Dev, Henryk Michalewski, Xavier Gar-
cía, Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Bar-
ret Zoph, Alexander Spiridonov, Ryan Sepassi, David Do-han, Shivani Agrawal, Mark Omernick, Andrew M. Dai,
Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor
Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polo-
zov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan
Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav
Petrov, and Noah Fiedel. Palm: Scaling language model-
ingwithpathways. J.Mach.Learn.Res.,24:240:1–240:113,
2022. 2
[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pas-
cale Fung, and Steven C. H. Hoi. Instructblip: Towards
general-purposevision-languagemodelswithinstructiontun-
ing.ArXiv, abs/2305.06500, 2023. 1, 2, 5, 6, 7
[11] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey
Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong
Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
worth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,
Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,
and Peter R. Florence. Palm-e: An embodied multimodal
language model. In International Conference onMachine
Learning, 2023. 2, 6
[12] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,Meng-
danZhang,XuLin,ZhenyuQiu,WeiLin,JinruiYang,Xiawu
Zheng, et al. Mme: A comprehensive evaluation bench-
mark for multimodal large language models. arXivpreprint
arXiv:2306.13394, 2023. 1, 2, 6, 7
[13] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shĳie
Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui He, Xi-
angyuYue,HongshengLi,andYuJiaoQiao. Llama-adapter
v2: Parameter-efficient visual instruction model. ArXiv,
abs/2304.15010, 2023. 7
[14] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,
Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping
Luo, and Kai Chen. Multimodal-gpt: A vision and lan-
guage model for dialogue with humans. arXivpreprint
arXiv:2305.04790, 2023. 6
[15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the V in VQA matter: Ele-
vating the role of image understanding in Visual Question
Answering. In Conference onComputer VisionandPattern
Recognition (CVPR), 2017. 6
[16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
Ross Girshick. Momentum contrast for unsupervised visual
representation learning. In Proceedings oftheIEEE/CVF
conference oncomputer visionandpatternrecognition,pages
9729–9738, 2020. 3
[17] ShaohanHuang,LiDong,WenhuiWang,YaruHao,Saksham
Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan
Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan
Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and
FuruWei. Languageisnotallyouneed: Aligningperception
with language models. ArXiv, abs/2302.14045, 2023. 2
[18] Chaoya Jiang, Haiyang Xu, Chenliang Li, Ming Yan, Wei
Ye, Shikun Zhang, Bin Bi, and Songfang Huang. TRIPS:
Efficient vision-and-language pre-training with text-relevant
imagepatchselection.In Proceedings ofthe2022Conference
27044
onEmpirical MethodsinNaturalLanguage Processing,pages
4084–4096, Abu Dhabi, United Arab Emirates, 2022. Asso-
ciation for Computational Linguistics. 4
[19] Chaoya Jiang, Haiyang Xu, Wei Ye, Qinghao Ye, Chen-
liang Li, Ming Yan, Bin Bi, Shikun Zhang, Fei Huang,
and Songfang Huang. Bus: Efficient and effective vision-
language pre-training with bottom-up patch summarization.
InProceedings oftheIEEE/CVF International Conference
onComputer Vision(ICCV), pages 2900–2910, 2023.
[20] Chaoya Jiang, Haiyang Xu, Wei Ye, Qinghao Ye, Chenliang
Li, Ming Yan, Bin Bi, Shikun Zhang, Fei Huang, and Ji
Zhang. Copa: Efficientvision-languagepre-trainingthrough
collaborativeobject-andpatch-textalignment.In Proceedings
ofthe31stACMInternational Conference onMultimedia,
pages 4480–4491, 2023. 4
[21] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bek-
man, Amanpreet Singh, Anton Lozhkov, Thomas Wang,
Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela,
Matthieu Cord, and Victor Sanh. Obelics: An open web-
scale filtered dataset of interleaved image-text documents,
2023. 5, 6
[22] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-
iao Ge, and Ying Shan. Seed-bench: Benchmarking multi-
modal llms with generative comprehension. arXivpreprint
arXiv:2307.16125, 2023. 6, 7
[23] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model
with in-context instruction tuning. ArXiv, abs/2305.03726,
2023. 7
[24] JunnanLi,RamprasaathSelvaraju,AkhileshGotmare,Shafiq
Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align
before fuse: Vision and language representation learning
withmomentumdistillation. Advances inneuralinformation
processing systems, 34:9694–9705, 2021. 4, 5
[25] Junnan Li, Dongxu Li, Caiming Xiong, and Steven
Hoi. Blip: Bootstrapping language-image pre-training
for unified vision-language understanding and generation.
InInternational Conference onMachine Learning, pages
12888–12900. PMLR, 2022. 4
[26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.
Hoi.Blip-2: Bootstrappinglanguage-imagepre-trainingwith
frozen image encoders and large language models. ArXiv,
abs/2301.12597, 2023. 1, 2, 6, 7
[27] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, and Ji rong Wen. Evaluating object hallucination in
largevision-languagemodels. ArXiv,abs/2305.10355,2023.
2, 5, 6, 7
[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
PietroPerona,DevaRamanan,PiotrDollár,andCLawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014:13thEuropean Conference,
Zurich,Switzerland, September 6-12,2014,Proceedings,
PartV13, pages 740–755. Springer, 2014. 8
[29] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser
Yacoob, and Lĳuan Wang. Aligning large multi-modal
model with robust instruction tuning. arXivpreprint
arXiv:2306.14565, 2023. 3[30] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Ya-
coob, and Lĳuan Wang. Mitigating hallucination in large
multi-modal models via robust instruction tuning, 2023. 2
[31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. ArXiv,
abs/2310.03744, 2023. 2, 5, 6, 7
[32] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
Visual instruction tuning. ArXiv, abs/2304.08485, 2023. 1,
2, 5, 6, 7
[33] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang
Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,
Ziwei Liu, et al. Mmbench: Is your multi-modal model an
all-around player? arXivpreprintarXiv:2307.06281, 2023.
6, 7
[34] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-
taghi, and Aniruddha Kembhavi. Unified-io: A unified
model for vision, language, and multi-modal tasks. ArXiv,
abs/2206.08916, 2022. 6
[35] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and
Anirban Chakraborty. Ocr-vqa: Visual question answering
byreadingtextinimages.In 2019international conference on
document analysisandrecognition (ICDAR),pages947–952.
IEEE, 2019. 6
[36] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv
preprintarXiv:1807.03748, 2018. 3
[37] OpenAI. Gpt-4v(ision) system card. 2023. 1
[38] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774,
2023. 1, 2, 3, 4
[39] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-
ing multimodal large language models to the world. ArXiv,
abs/2306.14824, 2023. 5, 6
[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
AmandaAskell,PamelaMishkin,JackClark,etal. Learning
transferable visual models from natural language supervi-
sion. InInternational conference onmachinelearning,pages
8748–8763. PMLR, 2021. 3
[41] SamyamRajbhandari,JeffRasley,OlatunjiRuwase,andYux-
iongHe.Zero: Memoryoptimizationstowardtrainingtrillion
parameter models, 2020. 5
[42] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for train-
ing next generation image-text models. Advances inNeural
Information Processing Systems, 35:25278–25294, 2022. 5
[43] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,
Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus
Rohrbach. Towardsvqamodelsthatcanread. In Proceedings
oftheIEEE/CVF conference oncomputer visionandpattern
recognition, pages 8317–8326, 2019. 6
[44] ZhiqingSun,ShengShen,ShengcaoCao,HaotianLiu,Chun-
yuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-
XiongWang,YimingYang,KurtKeutzer,andTrevorDarrell.
Aligninglarge multimodalmodelswith factuallyaugmented
rlhf.ArXiv, abs/2309.14525, 2023. 1, 2, 3, 5, 6, 7
27045
[45] HugoTouvron,ThibautLavril,GautierIzacard,XavierMar-
tinet,Marie-AnneLachaux,TimothéeLacroix,BaptisteRoz-
ière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
Lample. Llama: Open and efficient foundation language
models.ArXiv, abs/2302.13971, 2023. 1, 7
[46] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Al-
bert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanielM.
Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen,
GuillemCucurull,DavidEsiobu,JudeFernandes,JeremyFu,
Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui
Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh
Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet,TodorMihaylov,PushkarMishra,IgorMolybog,Yixin
Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,
Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor,
Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan,
IliyanZarov,YuchenZhang,AngelaFan,MelanieKambadur,
Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey
Edunov, and Thomas Scialom. Llama 2: Open foundation
andfine-tunedchatmodels. ArXiv,abs/2307.09288,2023. 1
[47] BinWang,FanWu,XiaoHan,JiahuiPeng,HuapingZhong,
Pan Zhang, Xiao wen Dong, Weĳia Li, Wei Li, Jiaqi Wang,
and Conghui He. Vigc: Visual instruction generation and
correction. ArXiv, abs/2308.12714, 2023. 2, 3
[48] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda,
Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou
Huang. Vision-language pre-training with triple contrastive
learning. In Proceedings oftheIEEE/CVF Conference on
Computer VisionandPatternRecognition, pages 15671–
15680, 2022. 4
[49] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,
YuhaoDan,ChenlinZhao,GuohaiXu,ChenliangLi,Junfeng
Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-docowl:
Modularizedmultimodallargelanguagemodelfordocument
understanding. CoRR, abs/2307.02499, 2023. 1
[50] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,
Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang,
et al. Ureader: Universal ocr-free visually-situated language
understandingwithmultimodallargelanguagemodel. In The
2023Conference onEmpirical MethodsinNaturalLanguage
Processing, 2023.
[51] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mplug-owl: Modularization empowers
large language models with multimodality. arXivpreprint
arXiv:2304.14178, 2023. 1, 2, 6, 7
[52] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
Kevin Lin, Zicheng Liu, Xinchao Wang, and Lĳuan Wang.
Mm-vet: Evaluating large multimodal models for integrated
capabilities. arXivpreprintarXiv:2308.02490, 2023. 6, 7
[53] YanZeng,XinsongZhang,andHangLi.Multi-grainedvisionlanguage pre-training: Aligning texts with visual concepts.
arXivpreprintarXiv:2111.08276, 2021. 4
[54] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gon-
zalez,andIonStoica. Judgingllm-as-a-judgewithmt-bench
and chatbot arena, 2023. 2
[55] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understandingwithadvancedlargelanguagemodels. ArXiv,
abs/2304.10592, 2023. 1, 2, 5, 6, 7
27046
