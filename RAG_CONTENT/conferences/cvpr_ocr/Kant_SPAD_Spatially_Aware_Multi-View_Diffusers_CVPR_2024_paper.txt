SPAD: Spatially Aware Multi-View Diffusers
Yash Kant1,2,4, Aliaksandr Siarohin2, Ziyi Wu1,4, Michael Vasilkovsky2, Guocheng Qian2,3,
Jian Ren2, Riza Alp Guler2, Bernard Ghanem3, Sergey Tulyakov2, Igor Gilitschenski1,4
1University of Toronto,2Snap Research,3KAUST,4Vector Institute
https://yashkant.github.io/spad/
Abstract
We present SPAD, a novel approach for creating con-
sistent multi-view images from text prompts or single im-
ages. To enable multi-view generation, we repurpose a pre-
trained 2D diffusion model by extending its self-attention
layers with cross-view interactions, and fine-tune it on a
high quality subset of Objaverse. We find that a naive exten-
sion of the self-attention proposed in prior work (e.g., MV-
Dream) leads to content copying between views. Therefore,
we explicitly constrain the cross-view attention based on
epipolar geometry. To further enhance 3D consistency, we
utilize Pl ¨ucker coordinates derived from camera rays and
inject them as positional encoding. This enables SPAD to
reason over spatial proximity in 3D well. Compared to con-
current works that can only generate views at fixed azimuth
and elevation (e.g., MVDream, SyncDreamer), SPAD offers
full camera control and achieves state-of-the-art results in
novel view synthesis on unseen objects from the Objaverse
and Google Scanned Objects datasets. Finally, we demon-
strate that text-to-3D generation using SPAD prevents the
multi-face Janus issue.
1. Introduction
3D content generation holds great importance in a wide
range of applications, such as gaming, virtual reality, or
robotics. Yet, the creation of high-quality 3D assets re-
mains a time-consuming endeavor even for seasoned 3D
artists. Recent years have witnessed the emergence of gen-
erative models capable of creating 3D objects from a single
or several 2D images, or just text inputs. Early methods in
this field directly train models such as Variational Auto En-
coders (V AEs) [41] and Generative Adversarial Networks
(GANs) [27] on 3D shapes [1, 49, 96, 98]. These methods
produce results with lower resolution to manage computa-
tional demands and have limited diversity due to the small
scale of training dataset. Later approaches explored differ-
* Equal advisingentiable rendering to learn 3D GANs from monocular im-
ages [7, 8, 14, 26, 51, 55, 76, 77]. These methods improved
resolution, but only show impressive results on relatively
few categories (e.g., ShapeNet [10] furniture).
Recent advances in Diffusion Models (DMs) have rev-
olutionized the field of 2D image generation [21, 34, 54].
Trained on billions of image-text data, state-of-the-art mod-
els [64, 66] learn generic object priors that enable high-
quality and generalizable text-guided image generation. Re-
cent works thus seek to leverage such 2D priors to generate
3D objects. One line of research proposes to optimize a
NeRF [50] model by distilling a pre-trained text-to-image
DM via Score Distillation Sampling (SDS) [59, 90]. This
enables single-view 3D reconstruction [19, 48, 60] and di-
rect text-to-3D synthesis [36, 44, 93]. However, these meth-
ods lack understanding of the underlying object structures.
The 2D prior provided by pre-trained DMs only considers
one view at each optimization step, ignoring the geometric
relationship across views. Even with hand-crafted prompts
specifying explicit viewpoints [59],these methods continue
to exhibit 3D inconsistencies, exemplified by issues such as
the multi-faced Janus problem.
A natural solution is to equip 2D diffusion models with
3D understanding. Zero-1-to-3 [45] proposes to condition
Stable Diffusion [64] with one view and generate another
one given the relative camera pose. However, conditioning
in Zero-1-to-3 is performed by simply concatenating the in-
put view, while disregarding any geometric priors. An al-
ternate approach based on depth warping was proposed in
iNVS [39]. It shows that given an accurate depth map, one
can establish dense correspondences between two views.
This allows DMs to reconstruct high-quality novel views.
Unfortunately, the generation quality of iNVS heavily re-
lies on the precision of depth maps, while monocular depth
estimation in itself is an unsolved problem.
Recent works [83, 102] have observed that Stable Dif-
fusion can be utilized to obtain accurate image correspon-
dences. Self-attention layers of text-to-image DMs can be
directly used for establishing correspondences within the
same image [87]. An interesting question to consider is
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10026
A knight's armored metal helmet with gold trim and holesA cute steampunk elephant
A small robot with a glass container on its head, metal legs, and a glass top
F-15 Eagle, F -16 Fighter Jet, and F/A -18F Super Hornet aircraft
A DSLR photo of a pair of tan cowboy boots, studio lighting, product photography
A wooden chair
Figure 1. Consistent multi-view generation from text with SPAD. Given a text prompt, SPAD is capable of synthesizing many 3D
consistent images of the same object, ranging from daily objects to highly complex machines. SPAD can generate many images from
arbitrary camera viewpoints, while being trained only on four views. Here, we generate eight views sampled uniformly at a fixed elevation.
whether the same layers can also find correspondences be-
tween different views, which can enable 3D geometric un-
derstanding. To this end, we can replace the original self-
attention with multi-view self-attention by applying it over
the concatenated feature maps across views. This approach
trained on orthogonal multi-view images with known cam-
era parameters can generate multiple novel views of the
same object simultaneously, as shown in previous works
such as MVDream [73]. However, we find that such a
model lacks precise camera control across views, and can-
not generate arbitrarily free novel views. When tasked togenerate two views that are close to each other (with signif-
icant overlap), such a model suffers from content copying
problem – where the content of one view is just copied from
another view without modification (see Fig. 5).
Inspired by [79], we design an Epipolar Attention layer,
where feature map positions in one view can only attend
to positions along the epipolar lines in other views. By re-
stricting the cross-view attention maps, these layers enable
better camera control and produce different views at view-
points close to each other. Epipolar Attention alone signifi-
cantly improves 3D consistency, since epipolar lines do not
10027
have a direction. However, it still remains difficult for this
model to disambiguate the direction of the camera ray. This
ambiguity leads to flipping in predicted views, as observed
in iNVS [39] which also used epipolar lines. Motivated by
recent works on Light Field Networks [6, 89], we propose
to represent rays passing through each pixel in Pl ¨ucker co-
ordinates, which assigns unique homogeneous coordinates
for each ray. We use these coordinates as positional em-
beddings inside Epipolar Attention layers. For rays hitting
opposite sides of the object, these embeddings provide a
strong negative bias for self-attention. This prevents it from
utilizing information from the wrong side. Additionally, use
of Pl ¨ucker embeddings also encourages pixels whose rays
are close to each other to have similar representations. This
encourages self-attention to pick features from nearby posi-
tions.
Our method can operate in two modes: text-conditioned
and image-conditioned. In text-conditioned mode, SPAD
can simultaneously denoise several views given a text
prompt. While in image-conditioned mode, given an im-
age, SPAD denoises several other views. In both cases,
the architecture of our method stays the same, and only
the input and output changes. We evaluate SPAD in the
task of text-conditioned multi-view generation and image-
conditioned novel view synthesis on Google Scanned Ob-
jects (GSO) [22] and an unseen subset of Objaverse [63]
datasets. Our results show that SPAD is able to synthesize
high-quality 3D consistent images of objects. Finally, we
enable high-quality text-to-3D generation using SPAD via
a) feed-forward multi-view to 3D triplane generator, and b)
multi-view Score Distillation Sampling similar to [73].
2. Related Works
3D Generative Models. 3D generative modeling is a long-
standing problem in computer vision and graphics. Ear-
lier works directly train generative models such as Vari-
ational Auto Encoders (V AEs) [41] on ground-truth 3D
shapes [1, 25, 49, 81, 96, 98]. However, due to the small
scale of 3D shape datasets, these methods produce less re-
alistic and diverse results compared to their 2D counter-
parts. With the rapid development of Generative Adver-
sarial Networks (GANs) [27] and differentiable rendering,
later works focus on learning 3D GANs from monocular
images, showing impressive generation of multi-view im-
ages [51, 52], radiance fields [7, 8, 20, 28, 55, 100, 105], and
meshes [14, 15, 26, 56, 57]. Nevertheless, GANs still suf-
fer from poor generalizability and training stability, prevent-
ing them from scaling to unconstrained objects and scenes.
Recently, Diffusion Models (DMs) [34, 78] have achieved
great success in general 2D image synthesis, and are also
applied to 3D [23, 24, 38, 40, 42, 53, 74, 91]. Yet, these
methods train 3D DMs from scratch on specific objects such
as human faces or vehicles, limiting their generalization.Closer to ours are methods that adapt large-scale pre-trained
2D DMs [64] for 3D generation, which we will detail next.
Novel View Synthesis (NVS) with 2D Diffusion Models.
Instead of reconstructing the entire 3D shape, NVS aims to
generate 3D consistent images conditioned on a few input
views [67, 95]. Early methods leverage the knowledge of
epipolar geometry to perform interpolation between differ-
ent input views [13, 17, 69, 108]. Since NVS is a 2D image-
to-image translation task, recent works have re-purposed 2D
DMs for it [9, 29, 80, 86, 94, 101]. To achieve 3D consis-
tency, SparseFusion [106] builds a view-conditioned DM on
the latent space of Stable Diffusion [64], and utilizes Epipo-
lar Feature Transformer (EFT) [79] to fuse features from in-
put views. Zero-1-to-3 [45] directly fine-tunes Stable Dif-
fusion on multi-view images rendered from Objaverse [18].
The concurrent work MVDream [73] proposes to de-
noise four views jointly with multi-view self-attention lay-
ers. However, camera pose information is fed in as 1D fea-
tures to these models, discarding the explicit constraint of
3D geometry. Thus this method does not allow accurate
camera control. To cope with this issue MVDream [73]
generates views at fixed camera positions spread apart 90
degrees from each other. However, this approach limits the
maximum number of views that can be generated to only
4. Moreover, it limits the training data to only synthetic
3D model datasets, such as Objaverse [18], since it requires
rendering with the same fixed camera view for each object.
Other works thus study more explicit pose conditioning.
MVDiffusion [84] derives inter-view dense correspondence
from homography transformation, which is used to guide
the attention module in Stable Diffusion. iNVS [39] applies
image wrapping based on depth maps to re-use pixels from
the source view, and thus only needs to inpaint occluded
regions in novel view images. While it can produce pre-
cise reconstructions when good depth maps are available,
the quality of this method degrades drastically when depth
maps are noisy or inaccurate. In addition, the depth ambi-
guity caused by epipolar lines used in iNVS results in the
flipped prediction issue, where the model cannot differenti-
ate two views from opposite directions. SyncDreamer [46]
instead builds a 3D feature volume by up-projecting fea-
tures from each view, and then re-projects it to ensure 3D
consistency among views. However up-projection opera-
tion requires the network to explicitly understand the depth
of each pixel, sharing the same issue with iNVS [39].
Different from prior works, we exploit the internal
properties of large-scale pre-trained text-to-image diffusion
models and enrich self-attention maps with the cross-view
interactions derived from epipolar geometry. In addition,
we use Pl ¨ucker coordinates [37] as positional encodings
to inject 3D priors of the scene into the diffusion model,
further improving camera conditioning and disambiguating
different sides of the object.
10028
𝒐𝒐×𝒅
𝑷𝑖𝑗=𝒐×𝒅,𝒅∈ℝ6 World
Origin
Image Plane𝒅
Camera
(c) Plücker Embedding𝑖
𝑗
(b) Multi -View Diffusion
Plücker
Embedding
(𝑷1,𝑷2)Noisy
Multi -View
(𝒙𝑡1,𝒙𝑡2)Clean
Multi -View
(𝒙1,𝒙2)
Epipolar Attention
𝑠1𝑙1
𝑠2𝑙2
(text 𝒚, camera Δ𝑬, time 𝑡)𝑬1
𝑬2
(a) Multi -View Rendering3D Object
𝑬1
𝑬2Figure 2. Model pipeline. (a) We initialize our multi-view diffusion model from pre-trained text-to-image model, and fine-tune it on
multi-view renders of 3D objects. (b) Our model performs joint denoising on noisy multi-view images {xi
t}N
i=1conditioned on text y
and relative camera poses ∆E. Here, we illustrate the pipeline using N= 2, which can be easily extended to more views. To enable
cross-view interaction, we apply 3D self-attention by concatenating all views, and enforce epipolar constraints on the attention map. We
further add (c) Pl ¨ucker Embedding {Pi}N
i=1to the attention layers as positional encodings, to enable precise camera control and prevent
object flipping artefacts (as shown in Fig. 5).
Lifting 2D Diffusion Models to 3D Generation. Instead
of training a model on 3D data, several works adopt a per-
instance optimization paradigm where pre-trained 2D DMs
provide image priors [59, 90]. Some of them apply it for
single-view 3D reconstruction [19, 48, 60, 62, 72, 82, 99].
More relevant to ours are text-to-3D methods that optimize
a NeRF model [50] by distilling the pre-trained text-to-
image DM. Follow-up works have improved this text-to-3D
distillation process in many directions, including more ef-
ficient 3D representations [12, 44, 71, 85, 97], better dif-
fusion process [16, 36, 70], new loss functions [93, 107],
and prompt design [2]. However, these methods still suffer
from low visual quality and view consistency issues such
as multi-face Janus and content drifting. SPAD generates
multi-view images from a text prompt or a single input view
with better 3D consistency and visual quality, which can
mitigate these issues with multi-view distillation [73].
3. Method
Task Formulation. Our goal is to generate consistent and
many novel views of the same object given a text prompt
or an image, along with relative camera poses as input.
Towards this goal, we train a multi-view diffusion model
that is made spatially aware by explicitly encoding the 3D
knowledge of the scene.
We build upon a state-of-the-art 2D text-to-image dif-
fusion model (Sec. 3.1). Our specific adaptations enable
3D-aware interactions between views (Sec. 3.2), which
include 3D self-attention (Sec. 3.2.1), Epipolar Attention
(Sec. 3.2.2), and Pl ¨ucker Embeddings (Sec. 3.2.3).
3.1. Preliminary: Text-to-Image Diffusion Models
Diffusion models (DMs) [34, 78] are generative models that
learn a target data distribution pθ(x0)by gradually denois-
ing a standard Gaussian distribution, denoted as pθ(x0) =R
pθ(x0:T)dx1:T, where x1:Tare intermediate noisy sam-
ples. DMs leverage a forward process that iteratively addsGaussian noise ϵto the clean data x0, which is controlled
by a pre-defined variance schedule {¯αt}⊤
t=1. During train-
ing, we manually construct noisy samples xt=√¯αtx0+√1−¯αtϵt, and train a denoiser model ϵθ(xt, t)to predict
the added noise conditioned on the denoising time step t:
LDM=||ϵt−ϵθ(xt, t)||2,where ϵt∼ N(0,I).(1)
Generally, the denoiser ϵθis parameterized as a U-Net [65],
which comprises of interleaved residual blocks [31] and
self-attention layers [88]. Within this U-Net, we are inter-
ested primarily in self-attention layers [88], and we refer the
reader to the original paper [64] for the overview of other
blocks. The self-attention layer takes as input a feature map
Fand compute attention of feature in location s with entire
feature map:
˜Fs=SoftMaxQ(Fs)·K(F)⊤
√
d
·V(F), (2)
where Q, K, V are linear projection layers, F∈R(hw)×d
is a flattened feature map obtained from the 2D denoiser ϵθ,
where dis the feature dimension, and h, w are intermediate
spatial dimensions. Fs,˜Fsis the input and output feature
for location srespectively. In practice, the self-attention
operation occurs at multiple resolutions in ϵθ.
3.2. Multi-View Diffusion Models
Inspired by the success of text-to-image DMs, we propose
to generate multi-view images by fine-tuning a pre-trained
2D DM on multi-view rendered images of 3D assets. Fig. 2
shows the overall pipeline of our framework, SPAD. In this
section, we use N= 2 views to explain our method for
brevity. However, note that SPAD is easily extensible to
generate an arbitrary number of views.
3.2.1 Multi-View Self-attention
The goal of our multi-view DM ϵθ(x1
t,x2
t, t,y,∆E)is to
generate 3D consistent images (x1,x2)∈RH×W×3of an
10029
𝑠Feature Map 𝑭𝑖
...
Feature Map 𝑭𝑖+1
𝑙𝑖+1
Feature Map 𝑭𝑖+2𝑙𝑖+2
Feature Map 𝑭𝑁𝑙𝑁
......Figure 3. Epipolar Attention. For each point s(red point) on a
feature map Fi, we compute its epipolar lines {lj}j̸=ion all other
views{Fj}j̸=i. Point swill only attend to features along these
lines plus all the points on itself (blue points).
object guided by a text input yand their relative camera
pose ∆E∈R3×4. To enable cross-view interaction, we
concatenate the feature maps of two views side-by-side as
input to the self-attention layers, denoted as [F1|F2]. This
allows each location sonF1to attend to all locations on
itself and F2, calculated as:
˜F1
s=SoftMaxQ(F1
s)·K([F1|F2])⊤
√
d
·V([F1|F2]).
(3)
Camera conditioning. We embed the relative camera pose
∆Ewith an MLP and fuse it with timestep embedding of
DM to condition the residual blocks as shown in Fig. 4, sim-
ilar to [73].
Issues with vanilla self-attention. We find empirically
that such an unconstrained multi-view self-attention leads
to content-copying between views (shown in Figure 5), i.e.,
the model generates similar images when the camera pose
difference ∆Eis small, ignoring the underlying 3D geome-
try. We hypothesize that this could be the reason concurrent
works such as MVDream [73] opt to generate images with
90 degree view change (along azimuths and fixed elevation)
– as it diminishes the overlap between different views.
3.2.2 Multi-View Epipolar Attention
To enable SPAD to synthesize views at arbitrary relative
poses, and address the above content-copying challenge,
we propose to replace the vanilla self-attention operation
with Epipolar Attention [79]. Epipolar Attention works
by restricting the positions a point in feature map can at-
tend to in other views – by exploiting epipolar geometry.
Fig. 3 presents this mechanism. Specifically, given a source
point son a feature map Fi, we compute its epipolar lines
(implemented as a set of points) {lj}j̸=ion all the other
views {Fj}j̸=i. When computing the attention map be-
tween views, we ignore points that do not lie on these epipo-
lar lines, so that the source point sonly has access to fea-
tures that lie along the camera ray (in other views) as well
Self Attention
Cross Attention text 𝒚Plücker
Embedding 𝑷Multi -View Feature Maps 𝑭
Epipolar Attention
Zero -Init
Projectio nQKV
Projectio n𝑭 𝑷
ResBlocktime 𝑡
camera Δ𝑬Figure 4. Illustration of one block in our multi-view diffusion
model , which consists of a residual block, a self-attention layer,
and a cross-attention layer. The residual block guides the model on
the denoising timestep tand the relative camera pose ∆E, while
the cross-attention layer conditions on text y. We add Pl ¨ucker
Embedding Pto feature maps Fin the self-attention layer by in-
flating the original QKV projection layers with zero projections.
as all points in its own view for denoising:
˜Fi
s=SoftMax 
Q(Fi
s)·K([Fi|Fj
lj])⊤
√
d!
·V([Fi|Fj
lj]).
(4)
In practice, we dilate the epipolar lines with a 3 ×3 filter
to consider neighboring target points for better robustness.
Overall, Epipolar Attention enhances our generalizability to
unseen viewpoint differences and objects.
Issues with Epipolar Attention. However, solely us-
ing epipolar lines to constrain attention masks models can
cause flipped predictions especially under large viewpoint
changes. This happens because in the absence of precise
depth of the object surface, the model can leverage infor-
mation from any point along these lines. Consider the fea-
ture map FNin Fig. 3, the source point smay attend to
the front face of the figure or the back of it. The latter
will cause a flipped prediction. iNVS [39] solves this prob-
lem with a monocular depth estimator. Yet, their imprecise
depth leads to deformed object surfaces and distorted tex-
tures. We instead address this issue with a Pl ¨ucker Ray Em-
bedding which is detailed next.
3.2.3 Pl ¨ucker Ray Embedding
Given a camera with its center placed at o∈R3in the world
coordinate system, we represent a ray passing through a
point on the feature map Fijalong a normalized direction
d∈R3asrij. We embed this ray as the positional encod-
ing to help our model distinguish between different views.
We find the simple ray parametrization rij= (o,d)to be
insufficient here. As an example, consider two rays in the
same direction but with different camera origins which lie
along this ray, r1
ij= (o,d)andr2
ij= (o+td,d), their
embeddings turn out to be notably different, despite them
representing essentially the same ray.
10030
Inspired by recent works in Neural Light Fields [11, 75],
we adopt the Pl ¨ucker Ray Embedding Pij= (o×d,d)
where ×is the cross product. See Fig. 2 (c) for illustration.
This parametrization is able to map r1andr2to the same
embedding as we have:
(o+td)×d=o×d+td×d=o×d. (5)
We simply pass the Pl ¨ucker Embedding P∈R(hw)×6
through a linear projection layer to project it into ddimen-
sion and add it to the multi-view feature maps F, which
serves as the input to the Epipolar Attention layer, shown
in Fig. 4. To avoid disturbing the pre-trained model, the
weights of the projection layer are initialized as zeros and
learned during fine-tuning similar to ControlNet [103].
3D geometric priors in Pl ¨ucker Embedding. With the
Pl¨ucker coordinates, rays that are close in the 3D space
share similar embeddings, which leads to higher values in
the pre-softmax self-attention map Q(F)·K(F)⊤. This en-
courages feature points to look at spatially nearby locations
in other views, enhancing the 3D consistency across views.
On the other hand, two rays passing through the same 3D lo-
cation from opposite cameras will have Pl ¨ucker coordinates
with flipped (positive/negative) sign. Their embeddings will
have a smaller dot product and result in a smaller attention
value. The two pixels will thus attend less to each other,
effectively addressing the flipped prediction problem.
4. Experiments
We conduct extensive experiments to answer the follow-
ing questions: (i)Can SPAD generate high-quality multi-
view images from diverse (non-orthogonal and overlap-
ping) viewpoints that are aligned with input text or image?
(Sec. 4.2) (ii)Are the synthesized views 3D consistent?
(Sec. 4.3) (iii) To what extent do Pl ¨ucker Positional Em-
beddings and Epipolar Attention contribute to the overall
performance? (Sec. 4.3) (iv)Lastly, can SPAD enable high-
quality text-guided 3D asset generation? (Sec. 4.5)
4.1. Experimental Setup
Training Data Curation. Instead of using the entire Obja-
verse [18] which consists of many flat and primitive shapes
that can drift the diffusion model away from high-quality
generation, we filter Objaverse using a few simple heuristics
based on its metadata. We use captions from Cap3D [47].
We select 150K objects with the most like, view, and com-
ment count available in metadata, as well as the top 50K ob-
jects that contain the highest number of mesh polygons and
vertex counts. We use Blender [4] to render 12 multi-view
images for each object at a resolution of 256 ×256. All ob-
jects are centered and re-scaled to a unit cube. We randomly
sample camera positions with elevations in [−90◦,90◦], az-
imuths in [0◦,360◦], and fix the distance to origin as 3.5 and
FOV as 40.26◦.Training Details. We initialize SPAD from the pre-trained
weights of Stable Diffusion v1.5 [64]. We train two versions
of our model: one with text conditioning and another one
with image conditioning for the novel view synthesis task.
In both the variants, we set the number of views Nto 2
while training. For the text-conditioned model, we jointly
denoise both the views. For the image-conditioned model,
we feed in one clean source view image and denoise the
target view. All our baselines and reported numbers follow
this setup. First, we train our models two-view models for
40K iterations on Objaverse, with an effective batch size of
1728 samples per iteration, on eight H100 GPUs. Later we
train a larger text-conditioned model with N= 4views on
sixteen H100 GPUs for 100K steps, and use it to generate
all visuals (except ablation study).
Evaluation Datasets and Metrics. For text-conditioned
multi-view image generation, we follow MVDream [73]
and randomly sample 1,000 Objaverse captions as text
prompts to generate images. We use CLIP-score [61] to
measure the image-text alignment. We also report the
Inception Score (IS) [68] and Fr ´echet Inception Distance
(FID) [32] to evaluate image generation quality. It is impor-
tant to highlight that these metrics only measure generation
quality of individual images, and do not provide any infor-
mation about their multi-view 3D consistency.
For image-conditioned novel view synthesis, we se-
lect 1,000 unseen Objaverse objects for testing. Follow-
ing [39, 45], we use real-world scanned objects from the
Google Scanned Objects (GSO) dataset [22] to evaluate our
method. We render each object from two views, where one
view serves as the model input and another view is the tar-
get novel view image. We report PSNR, SSIM [92], and
LPIPS [104] metrics.
Baselines. Since it is difficult to replicate and control
for training and rendering setups used in prior works, we
choose the following variants of our model as primary base-
lines: Vanilla MV-DM that only adds 3D self-attention on
concatenated multi-view feature maps without Epipolar At-
tention and Pl ¨ucker Embedding; MV-DM (Epipolar) and
MV-DM (Pl ¨ucker) which incorporate the two components,
respectively. We also compare SPAD with two concur-
rent works: MVDream [73] and SyncDreamer [46]. Dif-
ferent from SPAD, both methods can only generate views
at a fixed elevation and azimuth ranges. In the image-
conditioned novel view synthesis task, we compare with
additional baselines Zero-1-to-3 [45] and iNVS [39]. We
used the official codebase and pre-trained weights of these
methods on our testing data to report their results.
4.2. Text-conditioned Multi-View Generation
We use single view quality metrics to compare methods,
similar to MVDream. We evaluate two MVDream variants,
which are fine-tuned from Stable Diffusion v1.5 (same as
10031
Method IS↑ CLIP-score ↑
MVDream (v2.1)†[73] 13.36 ±0.87 30.22±3.83
MVDream (v1.5)†[73] 9.72 ±0.43 28.55±4.05
SyncDreamer‡[46] 11.69 ±0.24 27.76±4.84
Vanilla MV-DM 11.04 ±0.81 28.52±3.69
SPAD ( Ours ) 11.18 ±0.97 29.87±3.33
Table 1. Quantitative results on text-conditioned multi-view
image generation. We randomly sample 1,000 captions from Ob-
javerse, and evaluate the FID, Inception Score (IS), and CLIP-
score.†We ran MVDream’s code on the same captions we
used.‡We first generated single-view images using SD [64] us-
ing captions, and remove their backgrounds. Then, we ran Sync-
Dreamer’s code to generate multi-view images.
Method PSNR ↑SSIM↑LPIPS ↓
Zero-1-to-3†[45] 18.16 0.81 0.201
iNVS [39] 20.52 0.81 0.178
SyncDreamer†[46] 19.51 0.84 0.174
Vanilla MV-DM 17.56 0.81 0.20
MV-DM (Epipolar) 18.90 0.82 0.19
MV-DM (Pl ¨ucker) 17.98 0.81 0.20
SPAD ( Ours ) 20.29 0.84 0.166
Table 2. Quantitative results on image-conditioned novel view
synthesis on Objaverse. We report PSNR, SSIM, and LPIPS on
the generated novel view images of 1,000 unseen Objaverse ob-
jects.†We use official SyncDreamer and Zero-1-to-3 code.
ours) and v2.1, respectively. For SyncDreamer, we follow
the text-to-image-to-3D pipeline described in their paper to
first generate a single-view image from a text prompt using
Stable Diffusion, and then generate multiple views from it.
SPAD is a strong 2D text-to-image generator. The results
on image generation quality are presented in Tab. 1. SPAD
outperforms or matches both baselines on 2D Image Qual-
ity metrics when compared with methods initialized with
Stable Diffusion v1.5. This confirms that our method while
being more 3D consistent, does not compromise either text-
to-image alignment or overall image quality, but rather im-
proves it compared to our baseline MV-DM.
We provide qualitative results in Fig. 1, Fig. B.11, and
Fig. B.12. We put preliminary investigations of training
SPAD with v2.1 base model in Appendix B.4.
4.3. Image-conditioned Novel View Synthesis
Since image quality metrics do not provide any indication
of multi-view consistency or the quality of camera control.
For evaluation of multi-view consistency, we mostly rely on
image-conditioned experiments. For this evaluation, givenMethod PSNR ↑SSIM↑LPIPS ↓
Zero-1-to-3 [45] 16.10 0.82 0.183
iNVS [39] 18.53 0.80 0.180
SyncDreamer†[46] 17.18 0.83 0.178
Vanilla MV-DM 15.98 0.81 0.20
MV-DM (Epipolar) 17.13 0.82 0.19
MV-DM (Pl ¨ucker) 16.15 0.81 0.20
SPAD ( Ours ) 17.99 0.83 0.169
Table 3. Quantitative results on image-conditioned novel view
synthesis on GSO. We report PSNR, SSIM, and LPIPS on the
generated novel view images of GSO objects.†SyncDreamer only
reports results on 30 selected objects from GSO in their paper [46].
We ran their code and test it on all the GSO objects here.
an input view and relative camera pose, we generate the tar-
get view and compare it against ground truth. Tab. 2 and
Tab. 3 present the novel view synthesis results on Objaverse
and GSO, respectively.
SPAD preserves structural and perceptual details. We
find that SPAD outperforms all baselines on LPIPS met-
rics across both datasets, while matching its performance
to SyncDreamer on SSIM. Moreover, we find that adding
each component (Epipolar and Pl ¨ucker) gradually improves
scores across the board, and leads to state-of-the-art perfor-
mance with our full model. This confirms our main hypoth-
esis that imparting 3D understading to MV-DMs is helpful.
We also find that iNVS [39] is able to achieve the high-
est PSNR, since it directly copies pixels from the source
view (via depth-based reprojection). However, it particu-
larly performs worse on SSIM and LPIPS metrics, which
measure the structural and semantic accuracy of the gener-
ated view. This is because of deformations introduced by
reprojection when viewpoint changes are large and monoc-
ular depth from ZoeDepth [3] is inaccurate.
4.4. Qualitative Analysis
We also conduct qualitative analysis to visually understand
the usefulness of each component of our model. The text-
conditioned multi-view generation results of baselines and
SPAD are shown in Fig. 5, where all models are trained
with two views while prompted to generate four views. The
elevation is fixed for all the views, and the azimuth spans
uniformly between [0◦,360◦].
Epipolar Attention promotes better camera control in
SPAD. We find that the vanilla (full) 3D self-attention used
in Vanilla MV-DM and MV-DM (Pl ¨ucker) models often
leads to content copying. This is highlighted in the figure
using blue circles, where the generated dogs face in similar
direction, ignoring the target camera poses. We hypothe-
size that the readaptation of the self-attention layer of SD
originally trained to attend only to itself hinders with gen-
eralization and controllability of this model.
10032
A beagle in a detective's outfit A chihuahua lying in a swimming poolVanilla
MV-DM
MV-DM
(Epipolar )
MV-DM
(Plücker)
OursFigure 5. Qualitative comparison between SPAD and its variants. We prompt models trained on two views to generate four views at 90
degree intervals for clear visual distinctions. The flipped predicted views are highlighted with red circles, while the content-copying issues
are indicated by blue circles.
Additionally, since these models are trained only to gen-
erate two views, we hypothesize that they overfit to predict-
ing only two novel views. In contrast, Epipolar Attention
constrains cross-view interactions to only happen between
spatially related pixels, reducing the search space in estab-
lishing correspondences across images. Despite not being
trained on four views, the model is still able to generate 3D
consistent images by attending to the correct regions.
Pl¨ucker Embeddings help prevent generation of flipped
views. When the difference in camera positions between
two views is large, the epipolar lines introduce ambiguities
in the ray directions. Indeed, Vanilla MV-DM and MV-DM
(Epipolar) sometimes predict image regions that are rotated
by180◦. For example, the dog’s head highlighted by red
circles looks in the opposite direction of the body, which
is inconsistent with other views. Instead, Pl ¨ucker Embed-
dings bias the model to pay less attention to camera views
on opposite sides of the object, while leveraging more in-
formation from spatially closer views.
4.5. Text-to-3D Generation
Multi-view SDS. Inspired by [45, 73], we also adopt the
multi-view Score Distillation Sampling (SDS) [59] to per-
form text-to-3D generation using the four-view SPAD vari-
ant. Concretely, we integrate our model into the state-
of-the-art text-to-3D generation codebase threestudio [30],
and follow the setup similar to MVDream [73] for stable
NeRF [50] distillation. Fig. B.7 shows the multi-view ren-
dered images of the trained NeRF models. We find that
SPAD is able to reconstruct consistent geometry without
Janus problem, while maintaining good visual quality.
Multi-view Triplane Generator. Inspired by concurrent
works [35, 43], we trained a multi-view images to triplane
generator on Objaverse. We follow closely followed the
setup from Instant3D [43], and used four orthogonal viewsfrom SPAD to generate a NeRF in a single feed-forward
pass. Combined together this approach takes roughly 10
seconds to generate a single asset from text prompt, which
is greater than two orders of magnitude faster than SDS
optimization. Fig. B.6 shows the results from this experi-
ment. Thus, we find that SPAD can be used as a faithful
base model to facilitate such generations.
5. Conclusion
In this paper, we propose SPAD, a novel framework for
generating multiple views from text or image input. We
propose to transform the self-attention layers of the pre-
trained text-to-image diffusion model into Epipolar Atten-
tion to promote multi-view interactions and improve cam-
era control. Moreover, we augment self-attention layers
with Pl ¨ucker positional encodings to further improve cam-
era control by preventing flipping view prediction of the ob-
ject. We provide rigorous evaluations of these modifications
and demonstrate state-of-the-art results in terms of image-
conditioned novel view synthesis.
Limitations and Future Work . While our method im-
proves the 3D consistency of multi-view diffusion models,
there still remains lots of scope for improvements. For ex-
ample, a larger Stable Diffusion such as SDXL [58] can
further improve performance while preventing lossy com-
pression of image conditioning. We can use a monocular
depth estimator similar to iNVS [39] to further improve the
correspondences established by epipolar self-attention. Fi-
nally, we plan to explore the usage of SPAD to generate
dynamic 4D assets and multi-object scenes.
Acknowledgments
We would like to thank Xuanchi Ren and Weize Chen for
valuable discussions and support.
10033
References
[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and
Leonidas Guibas. Learning representations and generative
models for 3d point clouds. In Proceedings of the Interna-
tional Conference on Machine Learning , 2018. 1, 3
[2] Mohammadreza Armandpour, Huangjie Zheng, Ali
Sadeghian, Amir Sadeghian, and Mingyuan Zhou. Re-
imagine the negative prompt algorithm: Transform 2d
diffusion into 3d, alleviate janus problem and beyond.
arXiv preprint arXiv:2304.04968 , 2023. 4
[3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter
Wonka, and Matthias M ¨uller. Zoedepth: Zero-shot trans-
fer by combining relative and metric depth. arXiv preprint
arXiv:2302.12288 , 2023. 7
[4] Blender Online Community. Blender - a 3D modelling and
rendering package . Blender Foundation, Blender Institute,
Amsterdam, 2022. 6
[5] Tim Brooks, Aleksander Holynski, and Alexei A. Efros.
Instructpix2pix: Learning to follow image editing instruc-
tions. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 2023. 14
[6] Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav
Shakhrai, Ju Hu, Yun Fu, Denys Makoviichuk, Sergey
Tulyakov, and Jian Ren. Real-time neural light field on mo-
bile devices. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , 2023. 3
[7] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. pi-gan: Periodic implicit generative
adversarial networks for 3d-aware image synthesis. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , 2021. 1, 3
[8] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki
Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,
Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero
Karras, and Gordon Wetzstein. Efficient geometry-aware
3D generative adversarial networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition , 2022. 1, 3
[9] Eric R Chan, Koki Nagano, Matthew A Chan, Alexan-
der W Bergman, Jeong Joon Park, Axel Levy, Miika Ait-
tala, Shalini De Mello, Tero Karras, and Gordon Wetzstein.
Generative novel view synthesis with 3d-aware diffusion
models. In Proceedings of the IEEE International Confer-
ence on Computer Vision , 2023. 3
[10] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
An information-rich 3d model repository. arXiv preprint
arXiv:1512.03012 , 2015. 1
[11] Eric Ming Chen, Sidhanth Holalkere, Ruyu Yan, Kai
Zhang, and Abe Davis. Ray conditioning: Trading photo-
consistency for photo-realism in multi-view image genera-
tion. arXiv preprint arXiv:2304.13681 , 2023. 6
[12] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-
tasia3d: Disentangling geometry and appearance for high-
quality text-to-3d content creation. In Proceedings of theIEEE International Conference on Computer Vision , 2023.
4
[13] Shenchang Eric Chen and Lance Williams. View interpo-
lation for image synthesis. In Special Interest Group on
Computer Graphics and Interactive Techniques , 1993. 3
[14] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith,
Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learn-
ing to predict 3d objects with an interpolation-based differ-
entiable renderer. Advances in Neural Information Process-
ing Systems , 2019. 1, 3
[15] Wenzheng Chen, Joey Litalien, Jun Gao, Zian Wang,
Clement Fuji Tsang, Sameh Khamis, Or Litany, and Sanja
Fidler. Dib-r++: learning to predict lighting and material
with a hybrid differentiable renderer. Advances in Neural
Information Processing Systems , 2021. 3
[16] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,
Gang Yu, Lei Yang, and Guosheng Lin. It3d: Improved
text-to-3d generation with explicit view synthesis. arXiv
preprint arXiv:2308.11473 , 2023. 4
[17] Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik.
Modeling and rendering architecture from photographs: A
hybrid geometry- and image-based approach. In Special In-
terest Group on Computer Graphics and Interactive Tech-
niques , 1996. 3
[18] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:
A universe of annotated 3d objects. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition , 2023. 3, 6
[19] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,
Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.
Nerdi: Single-view nerf synthesis with language-guided
diffusion as general image priors. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition , 2023. 1, 4
[20] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong.
Gram: Generative radiance manifolds for 3d-aware image
generation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , 2022. 3
[21] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in Neural Informa-
tion Processing Systems , 2021. 1
[22] Laura Downs, Anthony Francis, Nate Koenig, Brandon
Kinman, Ryan Hickman, Krista Reymann, Thomas B
McHugh, and Vincent Vanhoucke. Google scanned ob-
jects: A high-quality dataset of 3d scanned household
items. In Proceedings of the IEEE International Confer-
ence on Robotics and Automation , 2022. 3, 6
[23] Ziya Erkoc ¸, Fangchang Ma, Qi Shan, Matthias Nießner, and
Angela Dai. Hyperdiffusion: Generating implicit neural
fields with weight-space diffusion. In Proceedings of the
IEEE International Conference on Computer Vision , 2023.
3
[24] Lin Geng Foo, Jia Gong, Hossein Rahmani, and Jun Liu.
Distribution-aligned diffusion for human mesh recovery. In
Proceedings of the IEEE International Conference on Com-
puter Vision , 2023. 3
10034
[25] Matheus Gadelha, Rui Wang, and Subhransu Maji. Mul-
tiresolution tree networks for 3d point cloud processing. In
Proceedings of the European Conference on Computer Vi-
sion, 2018. 3
[26] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,
Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja
Fidler. Get3d: A generative model of high quality 3d tex-
tured shapes learned from images. Advances in Neural In-
formation Processing Systems , 2022. 1, 3
[27] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative adversarial nets. Advances
in Neural Information Processing Systems , 2014. 1, 3
[28] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.
Stylenerf: A style-based 3d aware generator for high-
resolution image synthesis. In Proceedings of the Interna-
tional Conference on Learning Representations , 2022. 3
[29] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M
Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ra-
mamoorthi. Nerfdiff: Single-image view synthesis with
nerf-guided distillation from 3d-aware diffusion. In Pro-
ceedings of the International Conference on Machine
Learning , 2023. 3
[30] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian
Laforte, Vikram V oleti, Guan Luo, Chia-Hao Chen, Zi-
Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.
threestudio: A unified framework for 3d content generation.
https://github.com/threestudio-project/
threestudio , 2023. 8
[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pat-
tern Recognition , 2016. 4
[32] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in Neural Information Processing Systems ,
2017. 6
[33] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications , 2022. 14
[34] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in Neural Informa-
tion Processing Systems , 2020. 1, 3, 4
[35] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,
Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and
Hao Tan. Lrm: Large reconstruction model for single image
to 3d, 2023. 8, 15
[36] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi,
Zheng-Jun Zha, and Lei Zhang. Dreamtime: An improved
optimization strategy for text-to-3d content creation. arXiv
preprint arXiv:2306.12422 , 2023. 1, 4
[37] Yan-Bin Jia. Pl ¨ucker coordinates for lines in the space.
Problem Solver Techniques for Applied Computer Science,
Com-S-477/577 Course Handout , 2020. 3
[38] Heewoo Jun and Alex Nichol. Shap-e: Generat-
ing conditional 3d implicit functions. arXiv preprint
arXiv:2305.02463 , 2023. 3[39] Yash Kant, Aliaksandr Siarohin, Michael Vasilkovsky,
Riza Alp Guler, Jian Ren, Sergey Tulyakov, and Igor
Gilitschenski. invs: Repurposing diffusion inpainters for
novel view synthesis. In SIGGRAPH Asia 2023 Conference
Papers , 2023. 1, 3, 5, 6, 7, 8, 14
[40] Animesh Karnewar, Andrea Vedaldi, David Novotny, and
Niloy J Mitra. Holodiffusion: Training a 3d diffusion model
using 2d images. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , 2023. 3
[41] Diederik P Kingma and Max Welling. Auto-encoding vari-
ational bayes. arXiv preprint arXiv:1312.6114 , 2013. 1,
3
[42] Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, and Min-
hyuk Sung. Salad: Part-level latent diffusion for 3d shape
generation and manipulation. In Proceedings of the IEEE
International Conference on Computer Vision , 2023. 3
[43] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun
Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg
Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with
sparse-view generation and large reconstruction model.
https://arxiv.org/abs/2311.06214 , 2023. 8, 15
[44] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki
Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja
Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-
resolution text-to-3d content creation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition , 2023. 1, 4
[45] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object. In Proceedings of the
IEEE International Conference on Computer Vision , 2023.
1, 3, 6, 7, 8, 14, 18
[46] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie
Liu, Taku Komura, and Wenping Wang. Syncdreamer:
Generating multiview-consistent images from a single-view
image. arXiv preprint arXiv:2309.03453 , 2023. 3, 6, 7, 14,
15
[47] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-
son. Scalable 3d captioning with pretrained models, 2023.
6
[48] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and
Andrea Vedaldi. Realfusion: 360deg reconstruction of any
object from a single image. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
2023. 1, 4
[49] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy net-
works: Learning 3d reconstruction in function space. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2019. 1, 3
[50] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In Proceedings of the European Conference on Com-
puter Vision , 2020. 1, 4, 8
[51] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian
Richardt, and Yong-Liang Yang. Hologan: Unsupervised
10035
learning of 3d representations from natural images. In Pro-
ceedings of the IEEE International Conference on Com-
puter Vision , 2019. 1, 3
[52] Thu H Nguyen-Phuoc, Christian Richardt, Long Mai,
Yongliang Yang, and Niloy Mitra. Blockgan: Learning 3d
object-aware scene representations from unlabelled images.
Advances in Neural Information Processing Systems , 2020.
3
[53] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751 , 2022. 3
[54] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In Proceedings of
the International Conference on Machine Learning , 2021.
1
[55] Michael Niemeyer and Andreas Geiger. Giraffe: Repre-
senting scenes as compositional generative neural feature
fields. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 2021. 1, 3
[56] Dario Pavllo, Graham Spinks, Thomas Hofmann, Marie-
Francine Moens, and Aurelien Lucchi. Convolutional gen-
eration of textured 3d meshes. Advances in Neural Infor-
mation Processing Systems , 2020. 3
[57] Dario Pavllo, Jonas Kohler, Thomas Hofmann, and Aure-
lien Lucchi. Learning generative models of textured 3d
meshes from real-world images. In Proceedings of the
IEEE International Conference on Computer Vision , 2021.
3
[58] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna, and
Robin Rombach. Sdxl: Improving latent diffusion mod-
els for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952 , 2023. 8
[59] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. In Pro-
ceedings of the International Conference on Learning Rep-
resentations , 2023. 1, 4, 8
[60] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,
Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-
rokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:
One image to high-quality 3d object generation us-
ing both 2d and 3d diffusion priors. arXiv preprint
arXiv:2306.17843 , 2023. 1, 4
[61] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In Proceedings of the International Conference on
Machine Learning , 2021. 6
[62] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,
Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-
man, Michael Rubinstein, Jonathan Barron, et al. Dream-
booth3d: Subject-driven text-to-3d generation. arXiv
preprint arXiv:2303.13508 , 2023. 4
[63] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,
Luca Sbordone, Patrick Labatut, and David Novotny. Com-
mon objects in 3d: Large-scale learning and evaluation ofreal-life 3d category reconstruction. In Proceedings of the
IEEE International Conference on Computer Vision , 2021.
3
[64] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition , 2022. 1, 3, 4, 6, 7
[65] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Proceedings of the International Conference on
Medical Image Computing and Computer Assisted Inter-
vention , 2015. 4
[66] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-
mans, et al. Photorealistic text-to-image diffusion models
with deep language understanding. Advances in Neural In-
formation Processing Systems , 2022. 1
[67] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs
Bergmann, Klaus Greff, Noha Radwan, Suhani V ora, Mario
Luˇci´c, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene
representation transformer: Geometry-free novel view syn-
thesis through set-latent scene representations. In Proceed-
ings of the IEEE Conference on Computer Vision and Pat-
tern Recognition , 2022. 3
[68] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. Advances in Neural Information Process-
ing Systems , 2016. 6
[69] Steven M Seitz, Brian Curless, James Diebel, Daniel
Scharstein, and Richard Szeliski. A comparison and eval-
uation of multi-view stereo reconstruction algorithms. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2006. 3
[70] Hoigi Seo, Hayeon Kim, Gwanghyun Kim, and Se Young
Chun. Ditto-nerf: Diffusion-based iterative text to omni-
directional 3d model. arXiv preprint arXiv:2304.02827 ,
2023. 4
[71] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon
Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,
and Seungryong Kim. Let 2d diffusion model know 3d-
consistency for robust text-to-3d generation. arXiv preprint
arXiv:2303.07937 , 2023. 4
[72] Qiuhong Shen, Xingyi Yang, and Xinchao Wang.
Anything-3d: Towards single-view anything reconstruction
in the wild. arXiv preprint arXiv:2304.10261 , 2023. 4
[73] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie
Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d
generation. arXiv preprint arXiv:2308.16512 , 2023. 2, 3,
4, 5, 6, 7, 8, 14, 15, 16, 17
[74] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,
Jiajun Wu, and Gordon Wetzstein. 3d neural field genera-
tion using triplane diffusion. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
2023. 3
[75] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh
Tenenbaum, and Fredo Durand. Light field networks: Neu-
10036
ral scene representations with single-evaluation rendering.
Advances in Neural Information Processing Systems , 2021.
6
[76] Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Pe-
ter Wonka. Epigraf: Rethinking training of 3d gans. Ad-
vances in Neural Information Processing Systems , 2022. 1
[77] Ivan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian
Ren, Hsin-Ying Lee, Peter Wonka, and Sergey Tulyakov. 3d
generation on imagenet. In Proceedings of the International
Conference on Learning Representations , 2023. 1
[78] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In Proceedings of the In-
ternational Conference on Machine Learning , 2015. 3, 4
[79] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and
Ameesh Makadia. Generalizable patch-based neural ren-
dering. In ECCV , 2022. 2, 3, 5
[80] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea
Vedaldi. Viewset diffusion:(0-) image-conditioned 3d gen-
erative models from 2d data. In Proceedings of the IEEE
International Conference on Computer Vision , 2023. 3
[81] Qingyang Tan, Lin Gao, Yu-Kun Lai, and Shihong Xia.
Variational autoencoders for deforming 3d mesh models.
InProceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition , 2018. 3
[82] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran
Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-
fidelity 3d creation from a single image with diffusion prior.
2023. 4
[83] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng
Phoo, and Bharath Hariharan. Emergent correspondence
from image diffusion. arXiv preprint arXiv:2306.03881 ,
2023. 1
[84] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang,
and Yasutaka Furukawa. Mvdiffusion: Enabling holistic
multi-view image generation with correspondence-aware
diffusion. Advances in Neural Information Processing Sys-
tems, 2023. 3
[85] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,
Michael Niemeyer, and Federico Tombari. Textmesh: Gen-
eration of realistic 3d meshes from text prompts. In Pro-
ceedings of the International Conference on 3D Vision ,
2023. 4
[86] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan,
Jia-Bin Huang, and Johannes Kopf. Consistent view syn-
thesis with pose-guided diffusion models. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition , 2023. 3
[87] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
2023. 1
[88] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances
in Neural Information Processing Systems , 2017. 4[89] Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Men-
glei Chai, Yun Fu, and Sergey Tulyakov. R2l: Distilling
neural radiance field to neural light field for efficient novel
view synthesis. In Proceedings of the European Conference
on Computer Vision , 2022. 3
[90] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich. Score jacobian chaining: Lifting
pretrained 2d diffusion models for 3d generation. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , 2023. 1, 4
[91] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jian-
min Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen,
Fang Wen, Qifeng Chen, et al. Rodin: A generative model
for sculpting 3d digital avatars using diffusion. In Proceed-
ings of the IEEE Conference on Computer Vision and Pat-
tern Recognition , 2023. 3
[92] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.
Image quality assessment: from error visibility to structural
similarity. IEEE Transactions on Image Processing , 2004.
6
[93] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongx-
uan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-
fidelity and diverse text-to-3d generation with variational
score distillation. Advances in Neural Information Process-
ing Systems , 2023. 1, 4
[94] Daniel Watson, William Chan, Ricardo Martin Bru-
alla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad
Norouzi. Novel view synthesis with diffusion models. In
Proceedings of the International Conference on Learning
Representations , 2022. 3
[95] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and
Justin Johnson. Synsin: End-to-end view synthesis from
a single image. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , 2020. 3
[96] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman,
and Josh Tenenbaum. Learning a probabilistic latent space
of object shapes via 3d generative-adversarial modeling.
Advances in Neural Information Processing Systems , 2016.
1, 3
[97] Jinbo Wu, Xiaobo Gao, Xing Liu, Zhengyang Shen, Chen
Zhao, Haocheng Feng, Jingtuo Liu, and Errui Ding. Hd-
fusion: Detailed text-to-3d generation leveraging multiple
noise estimation. arXiv preprint arXiv:2307.16183 , 2023.
4
[98] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang,
Song-Chun Zhu, and Ying Nian Wu. Learning descriptor
networks for 3d shape synthesis and analysis. In Proceed-
ings of the IEEE Conference on Computer Vision and Pat-
tern Recognition , 2018. 1, 3
[99] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,
and Zhangyang Wang. Neurallift-360: Lifting an in-the-
wild 2d photo to a 3d object with 360deg views. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , 2023. 4
[100] Yinghao Xu, Sida Peng, Ceyuan Yang, Yujun Shen, and
Bolei Zhou. 3d-aware image synthesis via learning struc-
tural and textural representations. In Proceedings of the
10037
IEEE Conference on Computer Vision and Pattern Recog-
nition , 2022. 3
[101] Jason J Yu, Fereshteh Forghani, Konstantinos G Derpanis,
and Marcus A Brubaker. Long-term photometric consistent
novel view synthesis with diffusion models. In Proceedings
of the IEEE International Conference on Computer Vision ,
2023. 3
[102] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Pola-
nia Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan
Yang. A tale of two features: Stable diffusion complements
dino for zero-shot semantic correspondence. 2023. 1
[103] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models, 2023.
6
[104] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition , 2018. 6
[105] Xuanmeng Zhang, Zhedong Zheng, Daiheng Gao, Bang
Zhang, Pan Pan, and Yi Yang. Multi-view consistent gener-
ative adversarial networks for 3d-aware image synthesis. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2022. 3
[106] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-
tilling view-conditioned diffusion for 3d reconstruction. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2023. 3
[107] Joseph Zhu and Peiye Zhuang. Hifa: High-fidelity text-
to-3d with advanced diffusion guidance. arXiv preprint
arXiv:2305.18766 , 2023. 4
[108] C Lawrence Zitnick, Sing Bing Kang, Matthew Uytten-
daele, Simon Winder, and Richard Szeliski. High-quality
video view interpolation using a layered representation.
ACM Transactions on Graphics , 2004. 3
10038
