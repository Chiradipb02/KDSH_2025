CAD-SIGNet: CAD Language Inference from Point Clouds
using Layer-wise Sketch Instance Guided Attention
Mohammad Sadil Khan†
mdsadilkhan99@gmail.comElona Dupont†
elona.dupont@uni.luSk Aziz Ali†∗
skaziz.ali@dfki.de
Kseniya Cherenkova‡†
kseniya.cherenkova@uni.luAnis Kacem†
anis.kacem@uni.luDjamila Aouada†
djamila.aouada@uni.lu
†SnT, University of Luxembourg,∗German Research Center for Artificial Intelligence,‡Artec3D
Final
Model
Input point cloud
Conditional Auto Completion
Choose
SketchChoose
Sketch Plane
Step 2 Step 1Full Design History Recovery
CAD-SIGNet
InputUser Controlled Sequence Generation
CAD-SIGNet
User InputCAD-SIGNetInput point cloud
Figure 1. Full design history recovery from an input point cloud (top-left) and CAD-SIGNet - user interaction (bottom-left and right).
Abstract
Reverse engineering in the realm of Computer-Aided De-
sign (CAD) has been a longstanding aspiration, though
not yet entirely realized. Its primary aim is to uncover
the CAD process behind a physical object given its 3D
scan. We propose CAD-SIGNet, an end-to-end trainable
and aeto-regressive architecture to recover the design his-
tory of a CAD model represented as a sequence of sketch-
and-extrusion from an input point cloud. Our model learns
CAD visual-language representations by layer-wise cross-
attention between point cloud and CAD language embed-
ding. In particular, a new Sketch instance Guided Attention
(SGA) module is proposed in order to reconstruct the fine-
grained details of the sketches. Thanks to its auto-regressive
nature, CAD-SIGNet not only reconstructs a unique full de-
sign history of the corresponding CAD model given an in-
put point cloud but also provides multiple plausible design
choices. This allows for an interactive reverse engineer-
ing scenario by providing designers with multiple next step
choices along with the design process. Extensive experi-
ments on publicly available CAD datasets showcase the ef-
fectiveness of our approach against existing baseline mod-
els in two settings, namely, full design history recovery and
conditional auto-completion from point clouds.1. Introduction
Computer-Aided Design (CAD) has become the de facto
method for designing, drafting, and modeling in various
industries [8, 32]. 3D reverse engineering is the process
of inferring a CAD model given a 3D scan. This proce-
dure requires the expertise of designers and can be time-
consuming [9, 43]. Towards the automation of this proce-
dure, several works focused on decomposing point clouds
into parametric primitives allowing the reconstruction of
the final CAD model [9, 16, 23, 25, 35]. However, CAD
modeling consists of a sequential process where designers
draw 2D sketches ( e.g.lines, arcs) and apply CAD opera-
tions ( e.g.extrusion, chamfer) [42, 43]. Recovering these
intermediate design steps is crucial as it enables the ed-
itablity and re-usability of different object parts sharing the
same functionality. For instance, a chair can be composed
of three design steps, legs, seat, and back rest. Retrieving
these steps can allow for editing the legs to be taller, reusing
the back rest in another chair design, etc [19]. Nevertheless,
identifying adequate design steps requires design expertise.
Mohammad Sadil Khan, Elona Dupont, Anis Kacem, and Djamila
Aouada are affiliated to SnT, University of Luxembourg. Sk Aziz Ali is
affiliated to SnT, University of Luxembourg and German Research Cen-
ter for Artificial Intelligence. Most of the work conducted by Sk Aziz Ali
took place when at SnT, University of Luxembourg. Kseniya Cherenkova
is affiliated to both SnT, University of Luxembourg and Artec3D.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
4713
Accordingly, recent methods [31, 41, 42] attempted to learn
this expertise from large-scale CAD datasets [41, 42]. In
particular, the sequential nature of CAD modeling made
language-like representations with adequate grammar an
appealing choice [15, 42, 44]. While such a CAD language-
like representation has been successfully adopted for CAD
generative models [15, 33, 42, 44], it has not been estab-
lished for 3D reverse engineering. As in point cloud cap-
tioning [4, 5], leveraging language-like representations for
reverse engineering requires mechanisms for jointly learn-
ing visual representation from point clouds and correspond-
ing CAD language. Hence, the main question that we ask is:
how to effectively learn CAD visual-language representa-
tions from point cloud and CAD sequences for 3D reverse
engineering?
To answer this question, many challenges need to be ad-
dressed due to the structural disparity between point clouds
in 3D space and language-like representations of CAD se-
quences [27]. In particular, CAD sequences encode both
the chronological order of design steps and their parametric
form [42, 44], while the corresponding point clouds only
encode the geometry of the final design [9]. To the best
of our knowledge, the only works that infer CAD language
from point clouds are DeepCAD [42] and MultiCAD [27].
While DeepCAD [42] focused on learning CAD language
using a feed-forward strategy and presented the point cloud
to CAD language setting as a future application, Multi-
CAD [27] focused on learning the interaction of features
from distinct modalities ( i.e. point cloud and CAD lan-
guage) through a contrastive learning framework. Despite
their promising results, both methods suffer from two main
limitations: (1) Both visual and CAD language represen-
tations are learned separately in the first stage. A mapping
between the two representations is learned afterwards. Nev-
ertheless, this separate learning might result in modality-
specific features that are not relevant for CAD language in-
ference from point clouds [36]; (2) the learning of CAD
language representation is achieved using a feed-forward
strategy where the CAD language of the full design his-
tory is inferred at once. However, in a real-world scenario,
providing input or preferences at each design steps would
allow for tailoring the solution to the requirements of the
designer [44, 45].
To address the aforementioned challenges and limita-
tions, we propose CAD-SIGNet , an end-to-end trainable ar-
chitecture that auto-regressively infers CAD language in
the form of sketch-and-extrusion design steps from point
clouds. Instead of learning separate representations for both
point clouds and CAD language and the mapping between
them, the proposed method jointly learns these representa-
tions through multi-modal transformer blocks. Each block
is composed of layer-wise cross-attention between CAD
language and point cloud embedding. Moreover, other ex-isting works [27, 42] infer sketches from a global represen-
tation of the point cloud. However, we assume that only
a subset of the point cloud is needed to parameterize a
sketch. As shown in the right panel of Figure 1, design-
ers specify a plane in 3D space where the sketch is drawn.
The intersection of the sketch region and the point cloud
(shown in red in the same Figure) is assumed to be suffi-
cient for sketch parameterization. Therefore, this subset,
referred to as Sketch Instance , is first identified and then
considered in the cross-attention to infer sketch parameters.
We refer to this technique as Sketch instance Guided Atten-
tion(SGA). It allows the network to focus its attention on
specific points ( i.e. sketch instance), hence improving fine-
grained sketch inference. Finally, the auto-regressive nature
of CAD-SIGNet allows multiple plausible design choices
to coexist. As shown in the right panel of Figure 1, this en-
ables an interactive reverse engineering scenario, offering
designers various choices throughout the CAD process. An
overview of the proposed approach is provided in Figure 2.
Contributions: The contributions can be summarized to:
• An end-to-end trainable auto-regressive network that in-
fers CAD language given an input point cloud. To the
best of our knowledge, we are the first to propose an auto-
regressive strategy for this problem.
• Multi-modal transformer blocks with a mechanism of
layer-wise cross-attention between point cloud and CAD
language embedding.
• A Sketch instance Guided Attention (SGA) module
which guides the layer-wise cross-attention mechanism to
attend on relevant regions of the point cloud for predicting
sketch parameters.
• A thorough experimental validation in two different re-
verse engineering settings, namely, full CAD history
recovery and conditional auto-completion from point
clouds (see bottom left panel of Figure 1).
2. Related Works
Deep Learning-based CAD Reverse Engineering: CAD
models are well defined 3D objects described by their ge-
ometric and topological properties. As such, some works
address the reverse engineering problem by focusing on re-
covering the geometric features of CAD models from point
clouds. This has been achieved using parametric fitting
techniques either on the edges of the CAD model [7, 26,
28, 39] or on the surfaces [10, 12, 16, 23, 35, 46]. However,
a parametric fitting approach can only provide information
about the final CAD model and it lacks any insight into the
design process and the intermediate steps that were used
to create the CAD model. In order to address these limi-
tations, another line of work [9, 13, 18, 34, 47, 48] mod-
els the CAD construction using Constructive Solid Geome-
try(CSG) [18]. CSG is a sequential method in CAD mod-
eling that combines simple 3D shapes (e.g., cube, sphere)
using boolean operations (e.g., union, intersection). While
4714
MSALFA
MLP
 SGA
FC    Sketch Instance 
Generation  
Point Feature Selection
LFA
MLP
Multi-Modal Transformer Block
Positional
EncodingBxPoint Cloud Encoder
 Multi-Head
Self Attention
Projected
Bounding BoxUnit
Bounding BoxSketch 
InstancePoints
SelectionSketch Guided Attention (SGA)
Output CAD SequenceCAD-SIGNet
i-th Extrusion / Sketch Sequence
Sketch Instance GenerationInput Point Cloud
Input CAD SequenceExtrusion token
Features
Sketch token
FeaturesCross
Attention
(CA)Figure 2. Method Overview . CAD-SIGNet (left) is composed of BMulti-Modal Transformer blocks, each consisting of an LFA [17]
module to extract point features, Fv
b, and a MSA [38] module for token features, Fc
b. A SGA module (top right) combines Fv
bandFc
bfor
CAD visual-language learning. A sketch instance (bottom right), I, obtained from the predicted extrusion tokens is used to apply a mask,
Msgaduring CA to predict sketch tokens.
CSG can allow for the construction of relatively complex
shapes, it is no longer the standard in the CAD industry [43].
Indeed, the feature-based approach has now been adopted
by most CAD software as it allows for the modelling of
more complex shapes using a sequence of sketches and
CAD operations [41]. The work in [37] attempts to retrieve
some of the features of the construction history as extrusion
cylinders, but requires manual input to combine the cylin-
ders into the final shape and does not result into parametric
sketches. Self-supervised [24] and unsupervised [30] ap-
proaches have also been adopted in this context. Neverthe-
less, these approaches strive to infer plausible design steps
approximating the input point cloud, but not necessarily in-
ferring the standard parametric entities and therefore not
reproducing design expertise. CAD-SIGNet goes beyond
these limitations and leverages feature-based sequences of
real design steps to predict CAD history from point clouds.
CAD as a Language: Due to the sequential nature of
feature-based CAD modeling, a common strategy to rep-
resent it is to use language modelling. Inspired by Natural
Language Processing (NLP) [38], some works have focused
on language modeling of CAD sketches [15, 22, 33], others
leveraged it in the context of CAD models [44, 45]. How-
ever, all the aforementioned works present generative mod-
els that allow for the manipulation of a latent space but do
not directly tackle the reverse engineering problem. CAD-
Parser [49] used an intermediate representation of the fi-
nal shape, called Boundary-Representation (B-Rep) [21],
instead of point cloud to relax the problem of CAD lan-
guage inference. Closest to our work are DeepCAD [42]
and MultiCAD [27]. DeepCAD proposed a language-based
sketch-extrusion formulation and predicted the CAD his-
tory from point clouds as a preliminary experiment. Build-
ing on these findings, MultiCAD [27] opted for a two-stage
multimodal contrastive learning strategy. In addition to theseparate modality learning, both [42] and [27] use a feed-
forward strategy limiting the scope of reverse engineering
scenarios. In contrast, CAD-SIGNet presents a joint visual-
language learning strategy and allows designers to interact
with design choices (see Figure 1).
3. Problem and CAD Language Formulation
Given an input point cloud, our objective is to gener-
ate a sequence of tokens representing the design history
of the corresponding CAD model. Formally, let X=
[x1,x2, . . . ,xN]∈RN×3be an input point cloud with
xi∈R3denoting the 3D coordinates of the i-th point and
Nthe number of points. Following recent CAD genera-
tive models [42, 44], the design history of a CAD model
C={Cj}ns
j=1is represented by a sequence of nsdesign
steps, where each step Cj={tk}nj
k=1consists of a se-
quence of njtokens tk∈J0..dtK, with dtdefining the tok-
enization interval. The objective is to learn a mapping,
Φ:RN×3→J0..dtKntss.t.,Φ(X) =C,
where nts=Pns
j=1njdenotes the total number of tokens.
As in [42, 44], the design history is assumed to be composed
ofsketch-and-extrusion sequences. This implies that the se-
quence of tokens {tk}nj
k=1of each design step Cjrepresents
either a parametric sketch Sor an extrusion operation E
and the full design history Ccan be seen as a sequence of
sketch-and-extrusion pairs {(Sl,El)}ns/2
l=1.
Sketch Representation: Similarly to [44], a hierarchical
representation of the sketch is considered. As depicted in
Figure 3, a sketch is created from one or more faces, with a
face being a 2D region bounded by loops. A loop, in turn,
is a closed path that can consist of either a single closed
curve, such as a circle, or multiple curves, e.g., combina-
tion of lines and arcs. The curves are represented by the
tokenized 2D coordinates (px, py)of their parametric for-
mulation (e.g., start and end points for lines). The end of
4715
Sketch Scaling
Factor
Sketch Plane
Translation OrientationExtruded 3D Bounding Box
 Face 
Loops  Curves 
Curve CoordinatesFigure 3. Illustration of sketch and extrusion representations.
a curve, loop, face, and sketch are represented by the end
tokens ec,el,ef, and es, respectively.
Extrusion Representation: The extrusion operation de-
fines the sketch plane and the parameters needed to turn it
into a 3D volume. Following [44], the tokens (θ, ϕ, γ )and
(τx, τy, τz)define the sketch plane orientation and trans-
lation, respectively, with respect to a reference coordinate
system. The token σscales the normalized sketch defined
by the sketch tokens. The pair (d+, d−)represents the ex-
trusion distances along the normal direction of the sketch
plane and its opposite, respectively. The parameter βde-
notes the type of extrusion operation among new,cut,join,
andintersect . Finally, eesets the end of the extrusion to-
kens. Figure 3 shows the different tokens used to represent
the extrusion operation.
In addition to sketch and extrusion tokens, padis used for
padding and clsis considered to indicate the start or end
of the design sequence. More details about CAD sequence
representation are provided in supplementary materials.
4. CAD-SIGNet Architecture
The proposed CAD-SIGNet is an end-to-end trainable
transformer-based architecture that takes a point cloud X
as an input and outputs the corresponding design history se-
quence C. It follows an auto-regressive strategy by consid-
ering the set of previous tokens C<i={tj}j<ias context to
infer the next token ti. For a given point cloud X, the goal
of CAD-SIGNet is to learn its corresponding CAD history
using the following probability distribution,
pθ(C|X) =ntsY
i=1pθ(ti| {tj}j<i,X), (1)
where tiis the i-th sequence token and θdenotes the learned
parameters of the network. As mentioned in Section 3,
the predicted tokens ticorrespond to the representations
of sketch-and-extrusion sequences. Unlike other CAD lan-
guage generative models [42, 44] which infer sketch to-
kensSkfor each design step Ckfollowed by extrusion to-
kensEk+1, CAD-SIGNet first predicts extrusion tokens that
are further used as context to predict sketch tokens. An
overview of our CAD-SIGNet modules is provided in the
left panel of Figure 2.4.1. Point Cloud and CAD Language Embedding
The first module of CAD-SIGNet is responsible for embed-
ding point cloud points and CAD language tokens into the
same de-dimensional space Rde.
Point Cloud Embedding: Given the point cloud
X∈RN×(3+f), where fis the number of additional
per-point estimated features1, a linear layer2followed by
ReLU [14] is firstly applied as follows,
Fp
0= ReLU( XWp
emb), (2)
where Fp
0∈RN×dp0eis the learned embedding,
Wp
emb∈R(3+f)×dp0eis a learnable matrix, and dp0e= 16 .
The per-point features obtained in Fp
0are further enriched
using two Local Feature Aggregation (LFA) [17] modules.
LFA uses k-Nearest Neighbor (k-NN) to aggregate the fea-
tures of neighboring points through a linear combination
weighted by learned attention weights. A linear layer is
applied on the resulting aggregated features followed by
ReLU for each LFA module. The first LFA module results
in the point cloud embedding Fv
0∈RN×dedefined by,
Fv
0= ReLU(LFA( Fp
0)Wlfa), (3)
where Wlfa∈Rdp0e×dedenotes the weight matrix of the
linear projection. The second LFA module is applied on Fv
0
without changing its dimension. For more details about the
operator LFA( .), readers are referred to [17].
CAD Language Embedding: Given an input design se-
quence C={ti}nts
i=1∈J0..dtKnts, a matrix form of the
sequence is adopted. Unlike [44] which maps the sketch co-
ordinates pxandpyinto a 1-dimensional token, we consider
them as a single 2-dimensional token (px, py). To avoid di-
mension mismatch, the other tokens are also considered as
2-dimensional by augmenting them with pad tokens. By
concatenating these tokens and using a one-hot encoding,
a matrix form C∈ {0,1}nts×2dtis used to represent the
sequence C. As in [44], token flags Ctype∈J0..nfKnts×1
andCstep∈J0..ns/2Knts×1are set to indicate token types
and design steps, respectively. The initial embedding of
the CAD language Fc
0∈Rnts×deis obtained by using the
aforementioned token representations within a linear layer
and is given by,
Fc
0= [C+Mseq,Ctype,Cstep]Wc
emb+Cpos,(4)
where (,) is the concatenation operation,
Wc
emb∈R(2dt+2)×deis a learnable weight matrix,
andCpos∈Rnts×dea learned positional encoding. Note
that CAD sequences have a variable number of tokens
˜nts< n tsandMseq∈ {0,−∞}nts×2dtis the padding
mask that sets token embedding beyond ˜ntsto−∞.
1Point normals are extracted using Open3D [1]
2All linear layers used in the paper consist of a weight matrix and a
bias. For notation simplicity, we omit the bias.
4716
4.2. Layer-wise Multi-Modal Transformer Block
Based on the aforementioned embedding, CAD-SIGNet
jointly learns visual-language representations using B
multi-modal transformer blocks of layer-wise cross-
attention between CAD and point cloud embedding.
In particular, let the CAD language embedding Fc
b−1and
the point cloud embedding Fv
b−1be the input of the b-th
block (i.e., the first block receives Fc
0defined in Eq. (4) and
Fv
0defined in Eq. (3)). Firstly, Fc
bis generated from Fc
b−1
using a multi-head scaled dot-product attention [38] ( SA)
and an add-normalization layer [38] ( AddNorm ) as follows
Fc
b= SA( Q,K,V,M), (5)
Fc
b= AddNorm( Fc
b,Fc
b−1), (6)
where query Q, key K, and value Vare extracted from
Fc
b−1andMis the standard self-attention mask [38]. On
the other hand, the point cloud embedding Fv
b−1undergoes
an additional LFA module as described in Eq. (3) to obtain
a point cloud embedding Fv
b∈RN×de.
To enable the information passing between CAD lan-
guage and point cloud embedding within each block, a
cross-attention layer is used on Fv
bandFc
b. This is
achieved by employing linear projections to extract a key
Kv∈RN×deand value Vv∈RN×defrom the point
cloud embedding Fv
b. A query Qc∈Rnts×deis extracted
from the CAD embedding Fc
b. Using Eq. (5), the cross-
attention layer computes a CAD visual-language embed-
dingFvc
b∈Rnts×deas follows,
Fvc
b= SA( Qc,Kv,Vv,0), (7)
where 0is a (nts×N)zero matrix. Furthermore,
the cross and self-attended embedding Fvc
bandFc
bare
added and normalized to help the network learn the
geometric relationship between CAD tokens, yielding,
Fc
b= AddNorm( Fvc
b,Fc
b). Finally, as in [38], a Feed-
Forward Network (FFN) is applied on Fc
band added to it to
form the final CAD embedding, which is passed to the next
block along with Fv
b.
Sketch Instance Guided Attention (SGA): The aforemen-
tioned multi-modal transformer blocks are designed to pass
the information from all point embedding to CAD token
embedding. However, we posit that parameterizing a sketch
requires only cross-attending to a subset of the point cloud.
As shown in the bottom right panel of Figure 2, the inter-
section between the sketch region and the point cloud (de-
picted in red in the same Figure) is considered as adequate
for sketch parameterization. As depicted in Figure 3, the
representation of extrusion tokens defines the sketch plane
and bounding box. Furthermore, CAD-SIGNet predicts ex-
trusion tokens followed by sketch tokens for each design
step. This implies that the predicted extrusion tokens canbe leveraged to define a sketch instance on the point cloud
for cross-attention with sketch token embedding.
Definition 1 A sketch instance I∈Rη×3⊂X, with
η < N , is a subset of the input point cloud X. It is ex-
tracted by selecting points inside the bounding box on the
sketch plane derived from the corresponding predicted ex-
trusion tokens.
The bottom right panel of Figure 2 shows the sketch in-
stance extraction process. Given a set of extrusion to-
kensE, we first project the unit bounding box of the
xy-plane into a bounding box on the sketch plane de-
fined by the extrusion tokens E. In particular, given
the unit bounding on xy-plane defined by the points
U= [(0 ,0,0)T,(0,1,0)T,(1,0,0)T]∈R3×3, the Euler
angles (θ, ϕ, γ ), the translation vector (τx, τy, τz), and the
scaling factor σdefined by the extrusion operation E, the
projected bounding box Up∈R3×3is given by,
Up= (Rxyz(θ, ϕ, γ )(U×σ) + (τx, τy, τz)T), (8)
where Rxyz(θ, ϕ, γ )∈ SO (3)combines the Euler an-
gles in a rotation matrix in the special orthogonal group
SO(3). The sketch instance Iis then defined by
the points of Xlying inside this bounding box, i.e.,
I: = {x∈X|ϕ(x,Up) = True}, where
ϕ(x,Up)is an operator that checks whether an input point
x∈R3is inside the projected bounding box Up. Note that
for training the ground-truth extrusion tokens are used to
define the bounding box Up, while the predicted extrusion
tokens are leveraged at inference time. In order to not pe-
nalize small errors in sketch plane predictions of the extru-
sion tokens and point cloud sampling, the bounding box is
enlarged in the direction of sketch plane normal and its op-
posite by a small margin 0.1×max( d+, d−). The extracted
sketch instances can be then used in the cross-attention de-
fined in Eq. (7) only for sketch token embedding by em-
ploying a suitable mask instead of the zero matrix. In par-
ticular, let Msga∈ {0,−∞}nts×Nbe this mask and msga
ij
be its value for the attention between the i-th token and j-th
point embedding. Msgais introduced to mask the attention
of sketch token embedding to the points lying outside their
corresponding sketch instance. As a result, msga
ijis set to
0if the i-th token embedding is not denoting a sketch. If
thei-th token is representing a sketch, then msga
ijis set to 0
where the j-th point embedding is part of the corresponding
sketch instance and −∞ otherwise. Note that after identi-
fying the sketch instances, 4linear layers are used on the
corresponding subsets of Fv
bto refine their embedding be-
fore extracting the key and value for the cross-attention with
sketch token embedding. The top right panel of Figure 2 vi-
sually describes the SGA module.
4.3. Training and Inference Strategies
After the last multi-modal transformer block, the CAD
embedding Fc
Bis passed to two separate linear lay-
4717
DeepCAD dataset CC3D dataset
 Fusion360 dataset
Point Cloud
DeepCAD
CAD-SIGNet
Ground-TruthFigure 4. Visual results of reconstruction from the CAD sequences predicted from input point clouds. Both DeepCAD [42] and CAD-
SIGNet are trained on DeepCAD dataset [42]. Left: Results on DeepCAD dataset [42]. Middle : Cross-dataset results on CC3D dataset [6],
Right : Cross-dataset results on Fusion360 dataset [41].
ers for predicting the 2D tokens probability matrices
Ox,Oy∈[0,1]nts×dt.
Training: During training, a teacher-forcing strategy [40]
is used to pass the ground-truth as input. The cross-entropy
lossLceis used as an objective function.
Inference: During inference, given the input point
cloud Xand the initial CAD sequence consisting of
C={(cls, pad )}, the next tokens are auto-regressively
generated until the endtoken is predicted.
Hybrid Sampling: The auto-regressive nature of CAD-
SIGNet suggests that different token predictions at a given
time-stamp result in different final CAD sequences. This
allows for generating multiple plausible predictions given
a point cloud. In particular, given the output probabilities
Ox,Oy, one can take the top-1to obtain the predicted to-
kens or opt for a different selection strategy for each token
to have a different final CAD sequence. To showcase this,
we use a hybrid sampling approach during inference by se-
lecting top-5probabilities for the first token, and top-1for
subsequent tokens. This results in 5different final CAD se-
quences given a point cloud. Finally, the optimal CAD se-
quence is chosen by selecting the one that best approximates
the input point cloud. This is assessed by reconstructing
the CAD models3from the predicted sequences, sampling
point clouds on them, and selecting the model that results in
a minimum Chamfer Distance [11] with respect to the input
point cloud.
5. Experimental Results
In this section, the proposed CAD-SIGNet is evaluated on
two reverse engineering scenarios: (1) design history recov-
ery from point clouds, and (2) conditional auto-completion
of design history given user input and point clouds. Addi-
tional preliminary experiments showcasing the applicability
of the proposed method in a realistic scenario of reverse en-
3Opencascade[2] is used to reconstruct a model from a CAD sequence.gineering is also discussed.
Dataset : The DeepCAD dataset [42] is used. The sketch
and extrusion parameters are normalized, ensuring that the
sketches and the CAD models are within a unit-bounding
box starting from the origin. The point clouds are obtained
by uniformly sampling 8192 points from the normalized
CAD model. As in [42], the sketch and extrusion param-
eters are quantized to 8bits.
Implementation Details : We use 8CAD-SIGNet multi-
modal transformer blocks with h= 8 number of heads for
self-attention. The latent dimension is set to de= 128 . The
network has been trained with a batch size of 72for150
epochs using 2NVIDIA RTX A6000 GPUs. We implement
curriculum learning [3] for the first 15epochs, increasingly
sorting CAD sequences by the number of curves. For the
subsequent 135epochs, the samples are shuffled. More de-
tails are provided in supplementary materials.
5.1. Design History Recovery from Point Cloud
In this experiment, the task is to infer CAD language history
given an input point cloud.
Metrics : To evaluate the predicted sequences, a set of met-
rics assessing different levels of the predictions are used.
In particular, the final CAD reconstructions are quantita-
tively evaluated with respect to ground-truth CAD models
using mean and median Chamfer Distances (CD) [11]. As
CAD sequences are predicted as tokens, they do not nec-
essarily result in valid CAD models when reconstructed
using OpenCascade [2]. Accordingly, an Invalidity Ratio
(IR) metric, expressed as a percentage, is the ratio of in-
valid models. F1score is computed to evaluate the pre-
dicted extrusions and different primitive types along with
their occurrences in the sequences. A Hungarian matching
algorithm [20] is used to match the predicted loop and prim-
itive bounding boxes with the ground-truth ones of the same
sketch. More details are provided in the supplementary.
4718
Model IR↓Mean CD ↓
(×10−3)Median CD ↓
(×10−3)
DeepCAD [42] 7.14 42.49 9.640
MultiCAD [27] 11.5 - 8.090
CAD-SIGNet (Ours) 0.88 3.430 0.283
Table 1. Design history recovery from point clouds on Deep-
CAD [42] dataset. Invalidity Ratio (IR), mean and median Cham-
fer Distance (CD) results.
Results: To the best of our knowledge, DeepCAD and Mul-
tiCAD are the only works in literature that perform point
cloud to CAD language inference. Note that DeepCAD re-
sults have been reproduced using its publicly available im-
plementation, while MultiCAD results were taken from the
reported ones in [27] due to unavailability of public im-
plementation. It can be observed in Table 1 that the pro-
posed CAD-SIGNet is outperforming both DeepCAD and
MultiCAD in terms of median CD by a factor of 35and
28, respectively. Moreover, the IR metric shows that the
predicted CAD sequences by CAD-SIGNet results in dras-
tically more valid CAD model reconstructions than both
DeepCAD and MultiCAD. In Table 2, the per-primitive and
extrusion F1 scores of our method are compared to those of
DeepCAD. Our model predicts more accurately the prim-
itive types, and their occurrences in the design sequence
when compared to DeepCAD. Notably, our method records
improvements of more than 14% in F1 score on the arc type
with respect to DeepCAD. In addition, CAD-SIGNet cor-
rectly predicts the extrusions in most cases, showing that
our model can correctly identify the number of needed de-
sign steps. Figure 4 displays several qualitative CAD mod-
els reconstructed from the predicted CAD sequences. Visu-
ally, our method achieves better reconstructions with more
accurate details than DeepCAD [42]. More visual results
are reported in the supplementary materials.
ModelF1 Score
Line↑Arc↑Cricle↑Extrusion ↑
DeepCAD[42] 69.26 13.82 60.14 86.70
CAD-SIGNet (Ours) 77.31 28.65 70.36 92.72
Table 2. Design history recovery from point clouds on Deep-
CAD [42] dataset. F1 scores for lines, arcs, circles, and extrusions.
Ablation Study: The impact of the components proposed
in CAD-SIGNet is assessed in Table 3 in terms of F1 scores
and CAD reconstruction metrics (IR, mean, and median
CD). In the first row of this Table, the hybrid sampling
described in Section 4.3 is ablated, thus selecting tokens
with top-1 probabilities. It can be observed that this re-
sults in a drop of performance in terms of CD distances and
IR while maintaining similar sketch and extrusion scores.
A similar trend is observed for the second row, where the
SGA module is omitted in the layer-wise cross-attention de-
scribed in Section 4.2. This suggests that the hybrid sam-
pling and the SGA module are especially important to ob-
tain valid and accurate final CAD reconstructions. Finally,ModelF1 Score Median CD ↓
(×10−3)Mean CD ↓
(×10−3)IR↓Sketch ↑Extrusion ↑
Ours w/o Hybrid Samp. 75.30 92.97 0.291 6.784 5.02
Ours w/o SGA 75.13 92.49 0.289 4.995 2.18
Ours w/o Layer-wise CA 45.89 84.53 76.40 122.7 0
CAD-SIGNet (Ours) 75.94 92.72 0.283 3.432 0.88
Table 3. Ablation study. Sketch and extrusion F1 scores. Median,
Mean CD, and IR metrics.
the third row reports the results when the layer-wise cross-
attention defined in Eq. (7) is not considered. In this case,
each CAD language embedding Fc
bcross-attends to only
the point cloud embedding Fv
Bfrom the last block B. In
other words, this experiment omits passing the information
from intermediate geometric embedding to the CAD lan-
guage one. Despite generating only valid CAD reconstruc-
tions, we observe a drastic performance drop in all other
metrics using this strategy compared to the proposed layer-
wise cross-attention. Visual results of the ablation experi-
ments are provided in supplementary materials.
5.2. Conditional Auto-Completion from User Input
CAD-SIGNet’s auto-regressive nature enables it to com-
plete the next design steps given a user input and a complete
point cloud. To showcase this scenario, the same model
trained for full design history recovery is used. During in-
ference, the ground-truth tokens of the first extrusion and
sketch are provided, and the task is to predict the next to-
kens of the CAD sequence given the complete point cloud.
Baseline Methods: To the best of our knowledge, there
is no existing method capable of achieving the aforemen-
tioned task. DeepCAD [42] and MultiCAD [27] are not
suitable candidates for this task due to their feed-forward
nature. For the sake of comparison, we adapt two auto-
regressive generative models, namely SkexGen [44], and
HNC [45]. Similarly to DeepCAD [42], we train a Point-
Net++ [29] to encode the point cloud into the latent space
learned by SkexGen [44], and HNC [45] on CAD language.
Note that these adapted baselines were not subject to opti-
mization. More details on how these methods are adapted
are provided in the supplementary materials.
Metrics: The auto-completion performance is evaluated in
terms of final CAD reconstructions. This is measured by
the IR introduced in Section 5.1 and another measure based
on the CD. The latter is given by computing: (1) CDgt
pred
which is the CD between the CAD reconstruction of the
completed sequence and the ground-truth CAD model, (2)
CDgt
inwhich is the CD between the CAD reconstruction of
the user input sequence and the ground-truth, (3) the ratio
of the two distances CDgt
pred/CDgt
in. This ratio allows for as-
sessing whether the completed sequence resulted in a better
final CAD reconstruction with respect to the user input. In
order to reflect the distribution of this measure, the three
quartiles Q1, Q2 (i.e., median), and Q3 are reported.
Results: Table 4 compares the results of CAD-SIGNet
4719
Point Cloud GT HNC SkexGen Input CAD-SIGNetFigure 5. Visual results for auto-completion from user input on
DeepCAD [42] dataset. From left to right, input point cloud,
CAD model reconstruction from user input CAD sequence, Skex-
Gen [44], HNC [45], CAD-SIGNet (ours), and ground-truth.
when using hybrid sampling and without, to the adapted
baselines based on HNC [45] and SkexGen [44]. It can
be observed that all the quartile values of the CD ratio for
SkexGen baseline are very close to 1which indicates that
the completed sequences resulted in a final CAD recon-
struction that is close to the one of the user input. Notably,
CAD-SIGNet achieved low Q1 and Q2 values of 0.054and
0.325, respectively, showing that it improved the user input
by a large margin on half of the testing samples. These ob-
servations are consistent with the visual results reported in
Figure 5. Moreover, discarding the hybrid sampling yields
in a lower performance in all metrics but still outperforms
both HNC and SkexGen baselines. Finally, the SGA mod-
ule provides a noticeable improvement on the median CD
ratio which further highlights its importance.
ModelCD RatioIR↓Q1↓ Q2 (Median) ↓ Q3↓
SkexGen-Baseline [44] 0.987 1.000 1.035 2.04
HNC-Baseline [45] 0.437 1.015 2.589 8.85
Ours w/o Hybrid Samp. 0.096 0.696 1.096 4.40
Ours w/o SGA 0.060 0.458 0.992 0.91
CAD-SIGNet (Ours) 0.054 0.325 0.995 0.65
Table 4. Conditional auto-completion from user input and point
clouds on DeepCAD [42] dataset. Quartiles of CD ratio and IR.
5.3. Applications of CAD-SIGNet
In this section, we highilight the applicability of CAD-
SIGNet in a real-world scenario of reverse engineering.
Cross-Dataset Experiment on Fusion360 : Following the
protocol outlined in MultiCAD [27], a cross-dataset exper-
iment is performed on the Fusion360 dataset [41]. Results
presented in Table 5 shows that CAD-SIGNet outperforms
both MultiCAD [27] and DeepCAD [42] by a significant
margin. Figure 4 (right) shows some visual comparison of
the reconstructed CAD models from the predicted CAD se-
quences of CAD-SIGNet and DeepCAD [42]. The results
indicate that CAD-SIGNet achieves better 3D reconstruc-
tion quality in comparison to DeepCAD [42].
Design History Recovery from Realistic 3D Scans : The
reported results on DeepCAD dataset [42] are obtainedModel IR ↓ Median CD ( ×10−3)↓
DeepCAD [42] 25.17 89.2
MultiCAD [27] 16.52 42.2
CAD-SIGNet (Ours) 1.83 1.15
Table 5. Cross-dataset experiment on design history recovery from
point clouds on Fusion360 [41] dataset.
by applying the model on point clouds sampled on CAD
meshes. However, in a real-world scenario of reverse en-
gineering, we aim to reverse engineer 3D scans which are
prone to 3D scanning artifacts. The CAD-SIGNet model
trained on DeepCAD dataset is tested on this setting by per-
forming a cross-dataset testing. The CC3D dataset consists
of 50k+ realistic 3D scans with their corresponding CAD
models. Figure 4 shows some qualitative results of CAD-
SIGNet compared to DeepCAD. Despite not being trained
on such scan data, CAD-SIGNet succeeds in reconstructing
promising CAD reconstructions. On the test set of CC3D
dataset, composed of 5570 samples, we report a median CD
of2.398and an IR of 2.39compared to a median CD of
263.56and an IR of 12.73achieved by DeepCAD.
User Controlled Reverse Engineering : In a real-world re-
verse engineering scenario, it is not only desirable to gen-
erate the correct CAD sequence from a given point cloud,
but to provide the user with a choice over every design
step [45]. Towards this direction, one can further extend the
hybrid sampling strategy to generate either multiple sketch
planes for the extrusion steps or multiple sketches or loops
from one single sketch plane. Since sketch sequence gener-
ation relies on the points laying close to the predicted sketch
planes, changing sketch planes can result in a new sketch.
The right panel of Figure 1 shows different generated de-
sign paths by our method a user can interactively follow.
6. Conclusion
In this paper, we propose, CAD-SIGNet, an auto-regressive
architecture designed for recovering the design history of
a CAD model given a point cloud. This history is rep-
resented as a sequence of sketch-and-extrusion sequences.
Leveraging its auto-regressive nature, CAD-SIGNet recon-
structs a CAD design history from the input point cloud,
simultaneously offering a range of plausible design alter-
natives. Through multi-modal transformer blocks of layer-
wise cross-attention, the information is passed between
CAD language and point cloud embedding. Notably, the
incorporation of the SGA module enhances the reconstruc-
tion of fine-grained details in the sketches. As future works,
selecting subsets of points using SGA could help overcom-
ing the high computational costs associated with large point
clouds. While our work only considers extrusions, CAD-
SIGNet could be adapted to other operations.
Acknowledgement: The present work is supported
by the National Research Fund, Luxembourg un-
der the BRIDGES2021/IS/16849599/FREE-3D and
IF/17052459/CASCADES projects and Artec3D.
4720
References
[1] Open3d. http://www.open3d.org/ , . 4
[2] Open cascade. https://dev.opencascade.org/ , .
6
[3] Yoshua Bengio, J ´erˆome Louradour, Ronan Collobert, and Ja-
son Weston. Curriculum learning. In Proceedings ofthe26th
Annual International Conference onMachine Learning, page
41–48, New York, NY , USA, 2009. Association for Comput-
ing Machinery. 6
[4] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong
Xu. 3djcg: A unified framework for joint dense captioning
and visual grounding on 3d point clouds. In Proceedings of
theIEEE/CVF Conference onComputer Vision andPattern
Recognition, pages 16464–16473, 2022. 2
[5] Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X
Chang. Scan2cap: Context-aware dense captioning in rgb-
d scans. In Proceedings oftheIEEE/CVF conference on
computer vision andpattern recognition, pages 3193–3203,
2021. 2
[6] Kseniya Cherenkova, Djamila Aouada, and Gleb Gusev.
Pvdeconv: Point-voxel deconvolution for autoencoding cad
construction in 3d. pages 2741–2745, 2020. 6
[7] Kseniya Cherenkova, Elona Dupont, Anis Kacem, Ilya
Arzhannikov, Gleb Gusev, and Djamila Aouada. Sepic-
net: Sharp edges recovery by parametric inference of
curves in 3d shapes. In Proceedings oftheIEEE/CVF
Conference onComputer Vision and Pattern Recognition
(CVPR) Workshops, 2023. 2
[8] Yuanzhe Deng, James Chen, and Alison Olechowski. What
Sets Proficient and Expert Users Apart? Results of a
Computer-Aided Design Experiment. Journal ofMechanical
Design, 146(1):011401, 2023. 1
[9] Tao Du, Jeevana Priya Inala, Yewen Pu, Andrew Spielberg,
Adriana Schulz, Daniela Rus, Armando Solar-Lezama, and
Wojciech Matusik. Inversecsg: Automatic conversion of 3d
models to csg trees. ACM Transactions onGraphics (TOG),
2018. 1, 2
[10] Eric-Tuan, Minhyuk Sung, Duygu Ceylan, Radomir Mech,
Tamy Boubekeur, and Niloy J Mitra. Cpfn: Cascaded prim-
itive fitting networks for high-resolution point clouds. In
Proceedings oftheIEEE/CVF International Conference on
Computer Vision, pages 7457–7466, 2021. 2
[11] H. Fan, H. Su, and L. Guibas. A point set generation
network for 3d object reconstruction from a single image.
In2017 IEEE Conference onComputer Vision andPattern
Recognition (CVPR), pages 2463–2471, Los Alamitos, CA,
USA, 2017. IEEE Computer Society. 6
[12] Martin A Fischler and Robert C Bolles. Random sample
consensus: a paradigm for model fitting with applications to
image analysis and automated cartography. Communications
oftheACM, 24(6):381–395, 1981. 2
[13] Markus Friedrich, Pierre-Alain Fayolle, Thomas Gabor, and
Claudia Linnhoff-Popien. Optimizing evolutionary csg tree
extraction. In Proceedings oftheGenetic andEvolutionary
Computation Conference, pages 1183–1191, 2019. 2
[14] Kunihiko Fukushima. Cognitron: A self-organizing mul-tilayered neural network. Biological Cybernetics, 20:121–
136, 1975. 4
[15] Y . Ganin, S. Bartunov, Y . Li, E. Keller, and S. Saliceti.
Computer-aided design as language. Advances inNeural
Information Processing Systems, 34, 2021. 2, 3
[16] Haoxiang Guo, Shilin Liu, Hao Pan, Yang Liu, Xin Tong,
and Baining Guo. Complexgen: Cad reconstruction by b-rep
chain complex generation. ACM Transactions onGraphics
(TOG), 2022. 1, 2
[17] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan
Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham.
Learning semantic segmentation of large-scale point clouds
with random sampling. IEEE Transactions onPattern
Analysis andMachine Intelligence, 2021. 3, 4
[18] Kacper Kania, Maciej Zieba, and Tomasz Kajdanowicz.
Ucsg-net-unsupervised discovering of constructive solid ge-
ometry tree. Advances inNeural Information Processing
Systems, 33:8776–8786, 2020. 2
[19] Milin Kodnongbua, Benjamin Jones, Maaz Bin Safeer Ah-
mad, Vladimir Kim, and Adriana Schulz. Reparamcad:
Zero-shot cad re-parameterization for interactive manipula-
tion. In SIGGRAPH Asia 2023 Conference Papers, pages
1–12, 2023. 1
[20] Harold W. Kuhn. The Hungarian Method for the Assignment
Problem. Naval Research Logistics Quarterly, 2(1–2):83–97,
1955. 6
[21] Joseph G Lambourne, Karl DD Willis, Pradeep Kumar
Jayaraman, Aditya Sanghi, Peter Meltzer, and Hooman
Shayani. Brepnet: A topological message passing system for
solid models. In Proceedings oftheIEEE/CVF conference
oncomputer vision andpattern recognition, pages 12773–
12782, 2021. 3
[22] Changjian Li, Hao Pan, Adrien Bousseau, and Niloy J Mitra.
Free2cad: Parsing freehand drawings into cad commands.
ACM Transactions onGraphics (TOG), 41(4):1–16, 2022. 3
[23] Lingxiao Li, Minhyuk Sung, Anastasia Dubrovina, L. Yi,
and Leonidas J. Guibas. Supervised fitting of geometric
primitives to 3d point clouds. 2019 IEEE/CVF Conference
onComputer Vision andPattern Recognition (CVPR), pages
2647–2655, 2018. 1, 2
[24] Pu Li, Jianwei Guo, Xiaopeng Zhang, and Dong-Ming Yan.
Secad-net: Self-supervised cad reconstruction by learning
sketch-extrude operations. In Proceedings oftheIEEE/CVF
Conference onComputer Vision andPattern Recognition,
2023. 3
[25] Yuanqi Li, Shun Liu, Xinran Yang, Jianwei Guo, Jie Guo,
and Yanwen Guo. Surface and edge detection for prim-
itive fitting of point clouds. In ACM SIGGRAPH 2023
Conference Proceedings, 2023. 1
[26] Yujia Liu, Stefano D’Aronco, Konrad Schindler, and
Jan Dirk Wegner. Pc2wf: 3d wireframe reconstruction from
raw point clouds. arXiv preprint arXiv:2103.02766, 2021. 2
[27] Weijian Ma, Minyang Xu, Xueyang Li, and Xiangdong
Zhou. Multicad: Contrastive representation learning
for multi-modal 3d computer-aided design models. In
Proceedings ofthe32nd ACM International Conference on
Information andKnowledge Management, page 1766–1776,
4721
New York, NY , USA, 2023. Association for Computing Ma-
chinery. 2, 3, 7, 8
[28] Dimitrios Mallis, Ali Sk Aziz, Elona Dupont, Kseniya
Cherenkova, Ahmet Serdar Karadeniz, Mohammad Sadil
Khan, Anis Kacem, Gleb Gusev, and Djamila Aouada. Sharp
challenge 2023: Solving cad history and parameters recovery
from point clouds and 3d scans. overview, datasets, metrics,
and baselines. In Proceedings oftheIEEE/CVF International
Conference onComputer Vision, pages 1786–1795, 2023. 2
[29] Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Point-
net++: Deep hierarchical feature learning on point sets in
a metric space. In Proceedings ofthe31st International
Conference onNeural Information Processing Systems, page
5105–5114, Red Hook, NY , USA, 2017. Curran Associates
Inc. 7
[30] Daxuan Ren, Jianmin Zheng, Jianfei Cai, Jiatong Li, and
Junzhe Zhang. Extrudenet: Unsupervised inverse sketch-
and-extrude for shape parsing. In ECCV, 2022. 3
[31] Daniel Ritchie, Paul Guerrero, R Kenny Jones, Niloy J Mitra,
Adriana Schulz, Karl DD Willis, and Jiajun Wu. Neurosym-
bolic models for computer graphics. In Computer Graphics
Forum, pages 545–568. Wiley Online Library, 2023. 2
[32] D. Robertson and T.J. Allen. Cad system use and engi-
neering performance. IEEE Transactions onEngineering
Management, 40(3):274–282, 1993. 1
[33] Ari Seff, Wenda Zhou, Nick Richardson, and Ryan P Adams.
Vitruvion: A generative model of parametric cad sketches.
InInternational Conference onLearning Representations,
2021. 2, 3
[34] Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos
Kalogerakis, and Subhransu Maji. Csgnet: Neural shape
parser for constructive solid geometry. In Proceedings
oftheIEEE Conference onComputer Vision andPattern
Recognition (CVPR), 2018. 2
[35] Gopal Sharma, Difan Liu, Subhransu Maji, Evangelos
Kalogerakis, Siddhartha Chaudhuri, and Radom ´ır M ˇech.
Parsenet: A parametric surface fitting network for 3d
point clouds. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, PartVII16. Springer, 2020. 1, 2
[36] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy,
and Cordelia Schmid. Videobert: A joint model for video
and language representation learning. In Proceedings of
theIEEE/CVF international conference oncomputer vision,
pages 7464–7473, 2019. 2
[37] Mikaela Angelina Uy, Yen-Yu Chang, Minhyuk Sung, Purvi
Goel, Joseph G Lambourne, Tolga Birdal, and Leonidas J
Guibas. Point2cyl: Reverse engineering 3d objects from
point clouds to extrusion cylinders. In Proceedings of
theIEEE/CVF Conference onComputer Vision andPattern
Recognition, pages 11850–11860, 2022. 3
[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Il-
lia Polosukhin. Attention is all you need. In Proceedings
ofthe31st International Conference onNeural Information
Processing Systems, page 6000–6010, Red Hook, NY , USA,
2017. Curran Associates Inc. 3, 5[39] Xiaogang Wang, Yuelang Xu, Kai Xu, Andrea Tagliasac-
chi, Bin Zhou, Ali Mahdavi-Amiri, and Hao Zhang. Pie-
net: Parametric inference of point cloud edges. Advances
inneural information processing systems, 33:20167–20178,
2020. 2
[40] Ronald J. Williams and David Zipser. A learning algo-
rithm for continually running fully recurrent neural net-
works. pages 270–280, 1989. 6
[41] Karl DD Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao
Du, Joseph G Lambourne, Armando Solar-Lezama, and Wo-
jciech Matusik. Fusion 360 gallery: A dataset and environ-
ment for programmatic cad construction from human design
sequences. ACM Transactions onGraphics (TOG), 40(4):
1–24, 2021. 2, 3, 6, 8
[42] Rundi Wu, Chang Xiao, and Changxi Zheng. Deepcad: A
deep generative network for computer-aided design models.
InProceedings oftheIEEE/CVF International Conference
onComputer Vision (ICCV), pages 6772–6782, 2021. 1, 2,
3, 4, 6, 7, 8
[43] Xianghao Xu, Wenzhe Peng, Chin-Yi Cheng, Karl DD
Willis, and Daniel Ritchie. Inferring cad modeling se-
quences using zone graphs. In Proceedings oftheIEEE/CVF
Conference onComputer Vision andPattern Recognition,
pages 6062–6070, 2021. 1, 3
[44] Xiang Xu, Karl DD Willis, Joseph G Lambourne, Chin-Yi
Cheng, Pradeep Kumar Jayaraman, and Yasutaka Furukawa.
Skexgen: Autoregressive generation of cad construction
sequences with disentangled codebooks. In International
Conference onMachine Learning (ICML), pages 24698–
24724, 2022. 2, 3, 4, 7, 8
[45] Xiang Xu, Pradeep Kumar Jayaraman, Joseph G. Lam-
bourne, Karl D.D. Willis, and Yasutaka Furukawa. Hier-
archical neural coding for controllable cad model genera-
tion. In Proceedings ofthe40th International Conference
onMachine Learning. JMLR.org, 2023. 2, 3, 7, 8
[46] Siming Yan, Zhenpei Yang, Chongyang Ma, Haibin Huang,
Etienne V ouga, and Qi-Xing Huang. Hpnet: Deep primi-
tive segmentation using hybrid representations. 2021 ieee.
InCVF International Conference onComputer Vision
(ICCV)(2021), pages 2733–2742, 2021. 2
[47] Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi,
Hooman Shayani, Ali Mahdavi-Amiri, and Hao Zhang.
Capri-net: Learning compact cad shapes with adaptive prim-
itive assembly. In Proceedings oftheIEEE/CVF Conference
onComputer Vision andPattern Recognition (CVPR), 2022.
2
[48] Fenggen Yu, Qimin Chen, Maham Tanveer, Ali Mah-
davi Amiri, and Hao Zhang. D2csg: Unsupervised
learning of compact csg trees with dual complements
and dropouts. In Thirty-seventh Conference onNeural
Information Processing Systems, 2023. 2
[49] Shengdi Zhou, Tianyi Tang, and Bin Zhou. Cadparser: A
learning approach of sequence modeling for b-rep cad. 3
4722
