Know Your Neighbors: Improving Single-View Reconstruction
via Spatial Vision-Language Reasoning
Rui Li1Tobias Fischer1Mattia Segu1Marc Pollefeys1
Luc Van Gool1Federico Tombari2, 3
1ETH Z ¨urich2Google3Technical University of Munich
Abstract
Recovering the 3D scene geometry from a single view
is a fundamental yet ill-posed problem in computer vision.
While classical depth estimation methods infer only a 2.5D
scene representation limited to the image plane, recent ap-
proaches based on radiance ﬁelds reconstruct a full 3D rep-
resentation. However, these methods still struggle with oc-
cluded regions since inferring geometry without visual ob-
servation requires (i) semantic knowledge of the surround-
ings, and (ii) reasoning about spatial context. We propose
KYN, a novel method for single-view scene reconstruction
that reasons about semantic and spatial context to predict
each point’s density. We introduce a vision-language mod-
ulation module to enrich point features with ﬁne-grained
semantic information. We aggregate point representations
across the scene through a language-guided spatial atten-
tion mechanism to yield per-point density predictions aware
of the 3D semantic context. We show that KYN improves
3D shape recovery compared to predicting density for each
3D point in isolation. We achieve state-of-the-art results in
scene and object reconstruction on KITTI-360, and show
improved zero-shot generalization compared to prior work.
Project page: https://ruili3.github.io/kyn .
1. Introduction
Humans have the extraordinary ability to estimate the ge-
ometry of a 3D scene from a single image, often includ-
ing its occluded parts. It enables us to reason about where
dynamic actors in the scene might move, and how to best
navigate ourselves to avoid a collision. Hence, estimat-
ing the 3D scene geometry from a single input view is a
long-standing challenge in computer vision, fundamental to
autonomous navigation [ 16] and virtual reality applications
[33]. Since the problem is highly ill-posed due to scale am-
biguity, occlusions, and perspective distortion, it has tradi-
tionally been cast as a 2.5D problem [ 6,31,58], focusing
on areas visible in the image plane and neglecting the non-
Input
 BTS [ 51]
 Ours
Figure 1. Single-view scene reconstruction results. We present
the predicted 3D occupancy grids given a single input image. The
camera is at the bottom left and points to the top right along the z-
aixs. Previous methods like BTS [ 51] struggle to recover accurate
object shapes ( green box) and exhibit trailing effects in unobserved
areas ( blue box). In contrast, KYN recovers more accurate bound-
aries and mitigates the trailing effects prevalent in prior art.
visible parts.
Recently, approaches based on neural radiance
ﬁelds [ 39] have shown great potential in inferring the
true 3D scene representation from a single [ 51,59] or
multiple views [ 50]. For instance, Wimbauer et al . [51]
introduce BTS, a method that estimates a 3D density ﬁeld
from a single view at inference while being supervised only
by photometric consistency given multiple posed views at
training time.
Intuitively, given only a single image at inference, the
model must rely on semantic knowledge from the neighbor-
ing 3D structure to predict the density of occluded points.
However, existing approaches lack explicit semantic mod-
eling and, by modeling density prediction independently for
each point, are unaware of the semantic 3D context of the
point’s surroundings. This results in the clear limitations
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9848
which we illustrate in Fig. 1. Speciﬁcally, prior work [ 51]
struggles with accurate shape recovery ( green ) and further
exhibits trailing effects ( blue) in the absence of visual ob-
servation. We argue that, when considering a single point in
3D, its density highly depends on the semantic scene con-
text, e.g. if there is an intersection, a parking lot, or a side-
walk visible in its proximity. This becomes more critical
as we move further from the camera origin since the degree
of visual coverage decreases with distance, and reconstruct-
ing the increasingly unobserved scene parts requires context
from the neighboring points.
To this end, we present Know Your Neighbors (KYN),
a novel approach for single-view scene reconstruction that
predicts density for each 3D point in a scene by reasoning
about its neighboring semantic and spatial context. We in-
troduce two key innovations. We develop a vision-language
(VL) modulation scheme that endows the representation of
each 3D point in space with ﬁne-grained semantic informa-
tion. To leverage this information, we further introduce a
VL spatial attention mechanism that utilizes language guid-
ance to aggregate the visual semantic point representations
across the scene and predict the density of each individual
point as a function of the neighboring semantic context.
We show that, by injecting semantic knowledge and rea-
soning about spatial context, our method overcomes the
limitations that prior art exhibits in unobserved areas, pro-
ducing more plausible 3D shapes and mitigating their trail-
ing effects. We summarize our contributions as follows:
• We propose KYN, the ﬁrst single-view scene reconstruc-
tion method that reasons about semantic and spatial con-
text to predict each point’s density.
• We introduce a VL modulation module to enrich point
features with ﬁne-grained semantic information.
• We propose a VL spatial attention mechanism that aggre-
gates point representations across the scene to yield per-
point density predictions aware of the neighboring 3D se-
mantic context.
Our experiments on the KITTI-360 dataset [ 34] show that
KYN achieves state-of-the-art scene and object reconstruc-
tions. Furthermore, we demonstrate that KYN exhibits bet-
ter zero-shot generalization on the DDAD dataset [ 18] com-
pared to the prior art.
2. Related Work
Monocular depth estimation. Estimating depth from
a single view has been extensively studied over the last
decade [ 35,36,55–58], both in a supervised and a self-
supervised manner. Supervised methods directly minimize
the loss between the predicted and ground truth depths [ 1,
11]. For these, varying output representations [ 1,2,14],
network architectures [ 1,27,43,61], and loss functions
[1,48,52] have been proposed. Recent methods explore
training uniﬁed depth models on large datasets, tacklingchallenges like varying camera intrinsics [ 12,20,42,56,58]
and dataset bias [ 3]. Self-supervised methods cast the prob-
lem as a view synthesis task and learn depth via photometric
consistency on image pairs. Existing works have investi-
gated how to handle dynamic objects [ 7,13,17,30,31,46],
different network architectures [ 37,53,62,64] and lever-
aging additional constraints [ 19,32,44,47]. Our method
falls in the self-supervised category. However, we estimate
a true 3D representation from a single view, as opposed to
the 2.5D representation produced by traditional depth esti-
mation.
Semantic priors for depth estimation. Previous depth
estimation methods use semantic information to enhance
2D feature representations with different fusion strategies
[8,19,22,32], or to remove dynamic objects [ 5,28] during
training. These methods utilize semantic information in the
2D representation space. On the contrary, we use seman-
tic information to enhance 3D point representations and to
guide our 3D spatial attention mechanism.
Neural radiance ﬁelds. Neural radiance ﬁelds
(NeRFs) [ 39,59] learn a volumetric 3D representa-
tion of the scene from a set of posed input views. In
particular, they use volumetric rendering in order to
synthesize novel views by sampling volume density and
color along a pixel ray. Recent multi-view reconstruction
methods [ 49,54,60] take inspiration from this paradigm,
reformulating the volume density function as a signed
distance function for better surface reconstruction. These
methods are focused on single-scene optimization using
multi-view constraints, often representing the scene with
the weights of a single MLP.
To address the issue of generalization across scenes,
Yuet al . [59] propose PixelNeRF to train a CNN image
encoder across scenes that is used to condition an MLP, pre-
dicting volume density and color without multi-view opti-
mization during inference. However, their approach is lim-
ited to small-scale and synthetic datasets. Recently, Wim-
bauer et al. [51] proposed BTS, an extension of PixelNeRF
to large-scale outdoor scenes. They omit the color predic-
tion and, during training, use reference images to query
color for a 3D point given a density ﬁeld that represents
the 3D scene geometry. While this simpliﬁcation allows
them to scale to large-scale outdoor scenes, their method
falls short in predicting accurate geometry for occluded ar-
eas (Fig. 1). We address this shortcoming by injecting ﬁne-
grained semantic knowledge and reasoning about semantic
3D context when querying the density ﬁeld, leading to bet-
ter shape recovery for occluded regions in particular.
Scene as occupancy. A recent line of work infers 3D scene
geometry as voxelized 3D occupancy [ 4,38] from a single
image. These works predict the occupancy and semantic
class of each 3D voxel based on exhaustive 3D annotations.
Therefore, these methods rely heavily on manually anno-
9849
tated datasets. Further, the predeﬁned voxel resolution lim-
its the ﬁdelity of their 3D representation. In contrast, our
method does not rely on labor-intensive manual annotations
and represents the scene as a continuous density ﬁeld.
Semantic priors for NeRFs. Various works integrate se-
mantic priors into NeRFs. While some utilize 2D semantic
or panoptic semgentation [ 15,26,63], others leverage 2D
VL features [ 13,24,41] and lift these into 3D space by dis-
tilling them into the NeRF. This enables the generation of
2D segmentation masks from new viewpoints, segmenting
objects in 3D space, and discovering or removing particular
objects from a 3D scene. While the aforementioned meth-
ods focus on the classical multi-view optimization setting,
we instead focus on single-view input and leverage seman-
tic priors to improve the 3D representation itself.
3. Method
Problem setup. Given an input image I0, its corresponding
intrinsics K0∈R3×4and pose T0∈R4×4, we aim to
reconstruct the full 3D scene by estimating the density for
each 3D point x∈R3among point set X={xi}M
i=1
σi=f(I0,K0,T0,X,θ), (1)
where the density σiof pointxiis a function of the point set
Xand image I0along with its camera intrinsic/extrinsics.
fdenotes the network and θrepresents its parameters. The
densityσican be further transformed to the binary occu-
pancy score oi∈ {0,1}with a predeﬁned threshold τ. Dur-
ing training (Sec. 3.3), additional images Inare incorpo-
rated with their corresponding intrinsics Knand extrinsics
Tn, withn∈ {1,...,N}, providing multiview supervision.
Overview. We illustrate our method in Fig. 2. Given an
input image I0, we ﬁrst extract image and VL feature maps
FappandFvis. Next, we fuse the image and VL features into
a single feature map Ffusedand further utilize category-wise
text features to compute a segmentation map S. We then
use intrinsics K0to project the 3D point set Xto the image
plane and query Ffused, yielding point-wise visual features.
In parallel, we retrieve point-wise text features by querying
the segmentation map Sand looking up the corresponding
category-wise text features tfor each 3D point. Given the
point-wise visual and text features, we use our VL modu-
lation layers to augment the features with ﬁne-grained se-
mantic information. We aggregate the point-wise features
with VL spatial attention, adding 3D semantic context to
the point-wise features. We guide the attention mechanism
with the VL text features. We ﬁnally predict a per-point
density that is thresholded to yield the 3D occupancy map.
3.1. Vision­Lanugage Modulation
We detail how we extract point-wise visual features and
how we augment the visual features with semantic infor-
mation using our VL modulation module.Point-wise visual feature extraction. Given the input im-
ageI0, we extract image features from standard image en-
coders [ 21] and VL image encoder [ 29]
Fapp=f(I0,θapp),
Fvis=f(I0,θvis),(2)
whereFappandFvisrefer to the appearance features and VL
image features, respectively. We freeze the VL image en-
coder weights θvisto retain the pre-trained semantics. We
then fuse the features by concatenation followed by 2 con-
volutional layers, yielding Ffused. To obtain point-wise fea-
tures for the 3D points X={xi}M
i=1, we extract the fused
featureFfusedw.r.t. the projected 2D coordinates p0(xi)of
each 3D point. Then, we combine each 3D point feature
with a positional embedding γ(·)encoding its position in
normalized device coordinates (NDC). We obtain the fused
point-wise visual feature vi∈R1×Cfor a 3D point xi
vi=Concat(Ffused(p0(xi)),γ(x0
i)), (3)
where Concat (·,·)is the feature concatenation operation,
andx0
iis the 3D position w.r.t.I0’s coordinate.
Point-wise text feature extraction. As the text feature
size does not align with the image space, it can not be ex-
tracted directly by querying projected 2D coordinates. To
this end, we ﬁrst derive the semantic category of each 3D
point through 2D segmentations. Then we use it to asso-
ciate text features with each 3D point. We utilize the cate-
gory names from outdoor scenes [ 9] as prompts to the text
encoder, yielding the text features t∈RQ×C, whereQis
the number of categories and Cis the feature channel. We
then use the visual feature Fvis∈RH×W×Cfrom the VL
image encoder, to obtain the 2D semantic map by comput-
ing cosine similarity between VL image and text features
S= argmax
q∈{1,...,Q}Fvis⊗t⊤
∥Fvis∥∥t∥, (4)
where⊗denotes matrix multiplication, Sdenotes the seg-
mentation map by maximizing the similarity scores be-
tween VL image and text features along the category di-
mension. We then compute the semantic category for each
3D point by querying Swith projected 2D coordinates and
then leverage it to obtain the text feature of each 3D point.
si=S(p0(xi)),
gt
i=t(si),(5)
wheregt
i∈RCis the text feature of the 3D point xi.
VL modulation layers. To augment the 3D point features
with rich semantics from both VL image and text features,
we propose the VL modulation layers, which integrate both
image feature viand text representations gt
iof a 3D point xi
9850
Input
Image VL Image
Encoder       Image 
Encoder
Category-level 
promptsCar, truck,  … VL Text
Encoder
Category text 
features 
Seg. Map  
Project & Query
3D points 
Point-wise
text featuresVision-Language
Spatial AttentionPoint-wise
visual features
Density
EstimationsOccupancies
ThresholdAugmented
point-wise features    Vision-Language 
Modulation layersFigure 2. Overview. Given an input image I0, we use two image encoders to obtain features ( Fapp,Fvis), and fuse these into feature map
Ffused. We further extract category-level text features and a segmentation map S. For a given 3D point set X, we query the extracted
features by projecting them onto the image plane yielding point-wise visual and text features. Next, the VL modulation layers endow the
point representation with ﬁne-grained semantic information. Finally, the VL spatial attention aggregates these point representations across
the 3D scene, yielding density predictions aware of the 3D semantic context.
for better scene geometry reasoning. The module is com-
posed ofL= 4modulation layers, each conducting modu-
lation with multiplication operations
vl+1
i=ReLU(FC(vl
i)⊙FC(gt
i)), (6)
where FC (·)stands for the fully connected layer, ⊙is the
element-wise product between features, FC (gt
i)denotes the
text features encoded by a single fully-connected layer and
is shared across different modulation layers. We set v1
i=vi
at the ﬁrst modulation layer, and iterate through different
layers. We utilize skip connections to inject the initial visual
information v1
iafter the modulated feature vl′+1
i at levell′,
using concatenation followed by one fully-connected layer.
The output feature ˆvi=vL
idenotes the 3D point feature of
xiaugmented with rich image and text semantics.
3.2. VL Spatial Attention
Next, we dive into our VL spatial attention mechanism
that aggregates the extracted point-wise features across the
scene in a global-to-local fashion. First, we combine the
whole set of point-wise visual features {ˆvi}M
i=1and text fea-
tures{gt
i}M
i=1intoV∈RM×C′andCt∈RM×C. Then,
we aggregate these features using a cross-attention opera-
tion in 3D space. Speciﬁcally, we leverage linear atten-
tion [ 23,45] over appropriately split point sets to achieve
memory-efﬁcient spatial context reasoning.
Category-informed cross-attention. We take the point-
wise features Vas queries and values, and leverage text-
based feature Ctas the keys in the linear attention. Speciﬁ-
cally, we project the features with fully connected layers tokeys, queries, and values
FQ=f(V,θQ),
FK=f(Ct,θK),
FV=f(V,θV),(7)
where all features are in RM×ˆC, whereˆCdenotes the fea-
ture dimension before the attention. We then compute the
global context score G∈RˆC×ˆCby attending to the key and
value features, and then correlating with query features by
G=Softmax(F⊤
K)⊗FV,
Fﬁnal=Softmax(FQ/√
D)⊗G,(8)
whereFﬁnaldenotes the spatially aggregated point-wise fea-
tures. The density value is estimated by a single fully con-
nected layer followed by a Softplus (·)function
σ=Softplus(FC(Fﬁnal)). (9)
Reducing the memory footprint. As the point features are
sampled from the entire 3D space, simultaneously process-
ing all point features can hit computational bottlenecks even
with linear attention. To this end, we randomly split the ini-
tial points into chunks, to ensure that each chunk is identi-
cally distributed. Then we conduct the spatial point atten-
tion separately within each chunk and combine the density
estimations afterward. As such, the attention can aggregate
semantic point representations with both spatial awareness
and efﬁciency.
3.3. Training Process
Our method achieves self-supervision by computing the
photometric loss between the reconstructed and target col-
ors. We extract the 2D feature map of I0to get point-wise
9851
Input
 Mono2 [ 17]
 PixelNeRF [ 59]
 BTS [ 51]
 Ours
Figure 3. Qualitative comparisons on KITTI-360 dataset. We illustrate the scene reconstructions as voxel grids, where the camera is on
the left side and points to the right along the z-axis. A lighter voxel color indicates higher voxel positions. Compared to previous methods
that struggle with corrupted and trailing shapes, our method produces faithful scene geometry, especially for occluded areas.
representations, then partition all images {I0} ∪ { In}N
n=1
into a source set Nsource and a loss set Nlossfollowing previ-
ous practice [ 51]. We render the RGB of Nlossfrom the cor-
responding colors of Nsource similar to the self-supervised
depth estimation [ 17]. Instead of resorting to the whole
image, we perform patch-based image supervision [ 51] to
reduce memory footprints. For each pixel on a patch, we
sample the 3D points xion its back-projected ray and con-
duct density estimation. Let xiandxi+1be the adjacent
sampled pixels on a ray, we calculate the RGB information
by volume rendering the sampled color [ 51]
αi= exp(1 −σxiδi)Ti=i−1/productdisplay
j=1(1−αj), (10)
ˆd=S/summationdisplay
i=1Tiαidiˆck=S/summationdisplay
i=1Tiαicxi,k, (11)
whereδidenotes the distance between adjacent sampled
pointsxiandxi+1along the ray, and αirefer to the proba-
bility that the ray ends between xiandxi+1. Note that the
colorcxi,k=Ik(pk(xi))is the sampled RGB value from
the view kin the source set Nsource , to obtain a better ge-
ometry. ˆdandˆckrepresent the terminating depth and the
rendered color.
As we sample the pixels in a patch-wise manner during
training, the rendered RGB and depth are also organizedin patches. Let ˆPkas the rendered patch from view kin
the source Nsource ,Pas the supervisory patch from Nloss,
andd′as the patch depth of P, the loss function is deﬁned
following previous methods [ 17,51]
L=Lph+λeLe, (12)
whereλe= 10−3,LphandLeare photometric loss and
edge-aware smoothness loss [ 17] on patches
Lph= min
k∈Nrender/parenleftBig
λ1L1/parenleftBig
P,ˆPk/parenrightBig
+λ2SSIM/parenleftBig
P,ˆPk/parenrightBig/parenrightBig
,
(13)
Le=|δxd′
i|e−|δxP|+|δyd′
i|e−|δyP|, (14)
whereλ1= 0.15andλ1= 0.85,δx,δydenotes the gradient
along the horizontal and vertical directions.
4. Experiments
To demonstrate the effectiveness of our proposed method,
we compare with existing works [ 17,51,59,62] in single-
view scene reconstruction, including both depth estima-
tion [ 17] and radiance ﬁeld based methods [ 51,59]. We
evaluate both the 3D scene (Sec. 4.4) and object (Sec. 4.5)
reconstruction results on the KITTI-360 dataset in short and
long ranges. We conduct extensive ablations (Sec. 4.6) to
verify the effectiveness of each contribution, compare our
method with existing semantic feature fusion techniques, as
9852
Method O acc↑ IEacc↑ IErec↑
4-20mMonodepth2 [ 17] 0.90 n/a n/a
Monodepth2 [ 17] +4m 0.90 0.59 0.66
PixelNeRF [ 59] 0.89 0.62 0.60
BTS [ 51] 0.92 0.66 0.64
Ours 0.92 0.70 0.72
4-50mMonodepth2 [ 17] 0.82 n/a n/a
Monodepth2 [ 17] +4m 0.81 0.54 0.76
PixelNeRF [ 59] 0.82 0.56 0.68
BTS [ 51] 0.84 0.61 0.53
Ours 0.86 0.63 0.73
Table 1. Comparison of scene reconstruction on KITTI-360 .
Our method achieves the best overall performance in both the near
and far evaluation range.
well as evaluate our method’s performance with the broader
supervision range. Moreover, we demonstrate our method’s
zero-shot generalization ability in Sec. 4.7.
4.1. Datasets
We use the KITTI-360 [ 34] dataset for training and evalu-
ation, as it captures static scenes using cameras deployed
with wide baselines, i.e., two stereo cameras, and two side
cameras (ﬁsheye cameras) at each timestep, which facili-
tate learning full 3D shapes by self-supervision. During the
training phase, we use all cameras from two time steps, i.e.
8 cameras in total, to train our model. We use an input res-
olution of 192 ×640 and choose the left stereo camera from
the ﬁrst time step to extract the image features. We split
all cameras randomly into Nrender andNlossfor sampling
colors and loss computation, as is illustrated in Sec. 3.3.
Furthermore, we use the DDAD [ 18] dataset to evaluate the
zero-shot generalization capability of the models trained on
KITTI-360. We select testing sequences with more than 50
images and use 384 ×640 image resolution.
4.2. Evaluation
We follow the experimental protocol in [ 51] to evaluate
3D occupancy prediction. Speciﬁcally, we sample 3D grid
points on 2D slices parallel to the ground plane. For KITTI-
360, we report scores within distance ranges [4,20]and
[4,50]meters, where the latter provides a more challeng-
ing evaluation scenario. For ground-truth generation, we
follow [ 51] and accumulate 3D LiDAR points across time,
and set 3D points lying out of all depth surfaces as unoccu-
pied, otherwise set to occupied. Unlike [ 51], which accu-
mulate only 20 LiDAR sweeps often leading to inaccurate
occluded scene geometry, we accumulate up to 300 LiDAR
frames. We also provide results with a 20-frame accumu-
lated ground truth in the supplementary material for refer-
ence. For the DDAD dataset, we accumulate up to 100 Li-
DAR frames due to the limited sequence length and evaluate
in the[4,50]meters range.Metrics. We adopt the evaluation metrics in [ 51] and mea-
sure overall reconstruction (O acc) and occluded reconstruc-
tion (IE acc, IE rec) accuracies. Speciﬁcally, O acccomputes
the accuracy between the prediction and the ground truth
in the full area of the evaluation range, thus reﬂecting the
overall performance of the reconstruction. IE acccomputes
the accuracy of the invisible areas speciﬁcally, i.e. without
direct visual observation in I0. IE reccomputes the recall of
both invisible and empty areas, which evaluates the recon-
struction of the occluded empty space. The three metrics
focus on different aspects of the reconstruction quality.
Scene- and object-level evaluation. In addition to evaluat-
ing the performance of the whole scene, we focus on object
reconstruction in particular because they are of particular
interest compared to e.g. the road plane. To this end, we
manually annotate the object areas in the ground-truth oc-
cupancy maps and compute the evaluation metrics on these
object areas. We refer the reader to the supplementary ma-
terial for details.
4.3. Implementation Details
We implement our method using Pytorch [ 40] and train it
on NVIDIA Quadro RTX 6000 GPUs. The appearance
network is similar to [ 17] with pre-trained weights on Im-
ageNet [ 10]. We adopt LSeg [ 29] as the visual-language
network and freeze its parameters. The model is trained us-
ing Adam [ 25] optimizer with a learning rate of 10−4for
25 epochs, which is reduced to 10−5after 120k iterations.
During the training phase, we sample 4096 patches across
loss setNloss, each patch contains 8×8pixels. We further
sample 64 points along each ray following [ 51].
4.4. Scene Reconstruction
We compare our method with recent single-view scene re-
construction methods using self-supervision [ 17,51,59].
Speciﬁcally, we train Monodepth2 [ 17] on our benchmark
as the base depth estimation method. Since the depth net-
work cannot infer occluded geometry, we use a handcraft
criterion to set areas 4mbehind the depth surface as empty
space (Monodepth2 + 4m). We also compare our method
with the NeRF-based methods [ 51,59] following the same
training protocol. As shown in the Tab. 1, compared to pre-
vious methods, our method achieves both the best overall
performance (O acc) and the best occluded area reconstruc-
tion (IE acc, IE rec). Note that Monodepth2 (+ 4m) also yields
a competitive performance in terms of invisible and empty
space reconstruction (IE rec), but it relies on hand-crafted cri-
teria that cannot learn true 3D in the scene. Qualitative com-
parisons are shown in Fig. 3. We show the reconstructed
occupancy grids, where the camera is on the left side and
points to the right along the z-axis within [4, 50m] range.
Our method demonstrates obvious qualitative superiority in
reasoning occluded object shapes against the inherent am-
9853
Method O acc↑ IEacc↑ IErec↑
4-20mMonodepth2 [ 17] 0.69 n/a n/a
Monodepth2 [ 17] +4m 0.70 0.53 0.52
PixelNeRF [ 59] 0.67 0.53 0.49
BTS [ 51] 0.79 0.69 0.60
Ours 0.80 0.69 0.70
4-50mMonodepth2 [ 17] 0.65 n/a n/a
Monodepth2 [ 17] +4m 0.68 0.48 0.59
PixelNeRF [ 59] 0.66 0.56 0.58
BTS [ 51] 0.72 0.61 0.48
Ours 0.75 0.64 0.68
Table 2. Comparison of object reconstruction on KITTI-360 .
Our method outperforms other methods in all metrics in both the
short and long evaluation range.
biguity. Notably, it substantially reduces the trailing effects.
4.5. Object Reconstruction
We evaluate the object reconstruction performance by com-
puting the metrics within the annotated object areas. As
shown in Tab. 2, our method achieves competitive or bet-
ter results in the [4, 20m] evaluation range. Meanwhile,
it achieves obvious improvements for all metrics in the [4,
50m] range, demonstrating its effectiveness in reasoning
ambiguous geometries away from the camera origin. We
further show reconstructions of different categories in Fig.
4. Our method generates faithful object shapes with reason-
able estimates of occluded geometry for different categories
including fences, trees, cars, etc., showing clear improve-
ments over existing methods.
4.6. Ablation Studies
We evaluate the effectiveness of each contribution by sepa-
rately ablating the VL modulation (Sec. 4.6.1 ) and the VL
spatial attention (Sec. 4.6.2 ). We further compare with
existing techniques injecting semantics from related tasks
(Sec. 4.6.3 ). Moreover, we show our method’s improve-
ment under a broader supervision range (Sec. 4.6.4 ).
4.6.1 Ablation study on VL Modulation
We evaluate the effectiveness of VL modulation by adding
different components upon baseline method [ 51], which
uses only appearance feature Fappfrom the generic back-
bone [ 21]. In Tab. 3, we enhance the baseline by incorpo-
rating the fused VL image features ( Ffused) and injecting im-
age/text semantics with the VL Modulation (VL-Mod.). We
ﬁnd that merely using the fused feature Ffusedfrom the VL
image encoder does not contribute to overall improvement.
As a comparison, the proposed VL Modulation signiﬁcantly
improves the performances by properly interacting with im-
age and text features.
Input
 Mono2[ 17]
 PixelNeRF [ 59]
 BTS [ 51]
 Ours
Figure 4. Object reconstruction in the KITTI-360 dataset [ 34].
From left to right: Reconstructions of the fence, tree, and car. Our
method produces more faithful object geometries, in particular in
occluded areas and for various semantic categories.
FappFfused VL-Mod.Scene Recon. Object Recon.
Oacc IEacc IErec Oacc IEacc IErec
✓ 0.84 0.60 0.53 0.72 0.61 0.48
✓ 0.84 0.60 0.55 0.72 0.61 0.48
✓ ✓ 0.85 0.63 0.64 0.73 0.63 0.59
Table 3. Ablations study on VL modulation. We report the per-
formance in the [4, 50m] range. Naively introducing the VL image
feature does not improve performance. Our VL modulation corre-
lating the image and text features yields the best scores.
FappFfused VL-Mod. Attn. VL-Attn.Scene Recon. Object Recon.
OaccIEaccIErecOaccIEaccIErec
✓ 0.84 0.60 0.53 0.72 0.61 0.48
✓ ✓ 0.85 0.61 0.60 0.73 0.61 0.56
✓ ✓ 0.85 0.61 0.67 0.72 0.60 0.62
✓ ✓ 0.85 0.60 0.66 0.73 0.62 0.61
✓ ✓ ✓ 0.86 0.63 0.75 0.74 0.62 0.73
✓ ✓ ✓ 0.86 0.63 0.73 0.75 0.63 0.68
Table 4. Ablation study on spatial attention. We report the per-
formance in the [4, 50m] range. The spatial attention improves in
each variant by enabling the 3D context awareness. Combining
the VL features with spatial attention yields the best performance.
4.6.2 Ablation Study on Spatial Attention
In Tab. 4, we provide experimental support for the effective-
ness of using spatial context, both via spatial attention and
VL spatial attention. We ﬁrst add spatial attention over im-
age appearance features only (row: 1 →2). Without the VL
features, aggregating spatial context still yields notable im-
9854
Semantics Method O acc IEacc IErec
– Baseline 0.84 0.60 0.52
Semantic Feat.Plain Fusion 0.84 0.61 0.51
2D Fusion - LDLS [ 32] 0.84 0.60 0.55
2D Fusion - SAFENet [ 8]0.84 0.60 0.51
VL Feat.Fusing VL image feature 0.84 0.60 0.55
Ours 0.86 0.63 0.73
Table 5. Comparison with semantic feature fusion techniques.
We compare with 2D feature fusion techniques from semantic-
guided depth estimation [ 8,32]. Our method achieves the best
performance with visual and language semantic enhancement and
3D context awareness.
Supervision MethodScene Recon. Object Recon.
Oacc IEacc IErecOacc IEacc IErec
1sin the futureBTS [ 51]0.84 0.61 0.53 0.72 0.61 0.48
Ours 0.86 0.63 0.73 0.75 0.64 0.68
1-4sin the futureBTS [ 51]0.86 0.68 0.72 0.74 0.64 0.75
Ours 0.87 0.69 0.77 0.78 0.68 0.75
Table 6. Evaluation of different supervisory ranges. We report
the performance in [4, 50m] range. Our method with the stan-
dard supervision range (1 sin the future) yields comparable per-
formance with [ 51] using broader supervision (1-4 sin the future).
Meanwhile, it achieves further improvement when using a broader
supervision range.
provement. We further show that the introduction of seman-
tics via VL modulation (VL-Mod) helps independent of the
spatial aggregation mechanism (row 3 →5, row: 4→6), un-
derpinning that adding VL text features outperforms using
VL image features only. When combining both VL modu-
lation and spatial attention mechanisms, we achieve the best
performances (rows 5, 6). Additionally, we observe a small
albeit notable gain when also injecting VL features into the
spatial aggregation (VL-Attn), however, mainly improving
details are not captured in current metrics. Please refer to
the supplementary material for details.
4.6.3 Comparison with Other Semantic Guidances
As semantic cues are vital in other tasks such as depth es-
timation, we investigate whether the techniques used in the
related literature [ 8,32] are useful for single-view scene re-
construction. We use the pre-trained DPT [ 43] semantic
network to provide pre-trained semantic features, and in-
corporate different feature fusion techniques [ 8,32] to our
density prediction pipeline. As shown in Tab. 6, we ﬁnd that
conducting 2D feature fusion does not lead to notable im-
provements over the baseline, which is consistent even with
incorporating VL image feature fusion. However, by appro-
priately interacting with image and text features with 3D
context awareness, our method outperforms related tech-
niques by a notable margin.Method O acc IEacc IErec
PixelNeRF [ 59] 0.55 0.45 0.23
BTS [ 51] 0.48 0.50 0.16
Ours 0.59 0.58 0.42
Table 7. Generalization to DDAD with KITTI-360 trained
model . We evaluate in the [4, 50m] range. Our method demon-
strates better zero-shot ability compared to previous methods.
4.6.4 Improvement with Broader Supervision Range
As single-view reconstruction is supervised by multiple
posed images, the performance can be improved by expand-
ing the supervisory range during training. To this end, we
investigate if our method can achieve consistent improve-
ment using a broader supervisory range. In standard super-
vision, we use ﬁsheye views at the next time step (1 sin
the future). In the broader supervisory range, we randomly
incorporate ﬁsheye views within the next [1-4 s] timestep,
yielding diverse supervisory ranges with a maximum cov-
erage of 40m. We compare with BTS [ 51] in Tab. 6. Our
method with the standard supervisory range produces com-
parable results to [ 51] with a broader supervision range.
Enhanced by a broader range of supervision, our method
achieves further improvement over [ 51].
4.7. Zero­shot Generalization on DDAD
We evaluate the zero-shot generalization of KITTI-360
trained models on the DDAD dataset. We report the [0,50]
meters range scene reconstruction scores in Tab. 7. Our
method outperforms previous work, showing the effective-
ness of the VL guidance for zero-shot generalization.
5. Conclusion
In this paper, we proposed KYN, a new method for
single-view reconstruction that estimates the density of
a 3D point by reasoning about its neighboring semantic
and spatial context. To this end, we incorporate a VL
modulation module to enrich 3D point representations
with ﬁne-grained semantic information. We further pro-
pose a VL spatial attention mechanism that makes the
per-point density predictions aware of the 3D semantic
context. Our approach overcomes the limitations of prior
art [ 51], which treated the density prediction of each
point independently from neighboring points and lacked
explicit semantic modeling. Extensive experiments demon-
strate that endowing KYN with semantic and contextual
knowledge improves both scene and object-level recon-
struction. Moreover, we ﬁnd that KYN better generalizes
out-of-domain, thanks to the proposed modulation of
point representations with strong vision-language features.
The incorporation of VL features not only enhances the
performance of KYN, but also holds the potential to
pave the way towards more general and open-vocabulary
3D scene reconstruction and segmentation techniques.
9855
References
[1] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.
Adabins: Depth estimation using adaptive bins. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 4009–4018, 2021. 2
[2] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.
Localbins: Improving depth estimation by learning local dis-
tributions. In European Conference on Computer Vision ,
pages 480–496. Springer, 2022. 2
[3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter
Wonka, and Matthias M ¨uller. Zoedepth: Zero-shot trans-
fer by combining relative and metric depth. arXiv preprint
arXiv:2302.12288 , 2023. 2
[4] Anh-Quan Cao and Raoul de Charette. Monoscene: Monoc-
ular 3d semantic scene completion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3991–4001, 2022. 2
[5] Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia
Angelova. Depth prediction without the sensors: Leveraging
structure for unsupervised learning from monocular videos.
InProceedings of the AAAI Conference on Artiﬁcial Intelli-
gence , pages 8001–8008, 2019. 2
[6] Junda Cheng, Gangwei Xu, Peng Guo, and Xin Yang. Coa-
trsnet: Fully exploiting convolution and attention for stereo
matching by region separation. International Journal of
Computer Vision , 132(1):56–73, 2024. 1
[7] JunDa Cheng, Wei Yin, Kaixuan Wang, Xiaozhi Chen, Shi-
jie Wang, and Xin Yang. Adaptive fusion of single-view and
multi-view depth for autonomous driving. arXiv preprint
arXiv:2403.07535 , 2024. 2
[8] Jaehoon Choi, Dongki Jung, Donghwan Lee, and Chang-
ick Kim. Safenet: Self-supervised monocular depth estima-
tion with semantic-aware feature extraction. arXiv preprint
arXiv:2010.02893 , 2020. 2,8
[9] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 3213–3223, 2016. 3
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 6
[11] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In Advances in neural information processing systems ,
pages 2366–2374, 2014. 2
[12] Jose M Facil, Benjamin Ummenhofer, Huizhong Zhou,
Luis Montesano, Thomas Brox, and Javier Civera. Cam-
convs: Camera-aware multi-scale convolutions for single-
view depth. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 11826–
11835, 2019. 2[13] Ziyue Feng, Liang Yang, Longlong Jing, Haiyan Wang,
YingLi Tian, and Bing Li. Disentangling object motion and
occlusion for unsupervised multi-frame monocular depth.
arXiv preprint arXiv:2203.15174 , 2022. 2,3
[14] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression net-
work for monocular depth estimation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2002–2011, 2018. 2
[15] Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu,
Lanyun Zhu, Xiaowei Zhou, Andreas Geiger, and Yiyi Liao.
Panoptic nerf: 3d-to-2d label transfer for panoptic urban
scene segmentation. In 2022 International Conference on
3D Vision (3DV) , pages 1–11. IEEE, 2022. 3
[16] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In 2012 IEEE Conference on Computer Vision and
Pattern Recognition , pages 3354–3361. IEEE, 2012. 1
[17] Cl ´ement Godard, Oisin Mac Aodha, Michael Firman, and
Gabriel J Brostow. Digging into self-supervised monocular
depth estimation. In Proceedings of the IEEE International
Conference on Computer Vision , pages 3828–3838, 2019. 2,
5,6,7
[18] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raven-
tos, and Adrien Gaidon. 3d packing for self-supervised
monocular depth estimation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2485–2494, 2020. 2,6
[19] Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, and Adrien
Gaidon. Semantically-guided representation learning for
self-supervised monocular depth. In International Confer-
ence on Learning Representations , 2020. 2
[20] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares ,Ambrus ,,
and Adrien Gaidon. Towards zero-shot scale-aware monocu-
lar depth estimation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 9233–9243,
2023. 2
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 3,7
[22] Hyunyoung Jung, Eunhyeok Park, and Sungjoo Yoo. Fine-
grained semantics-aware representation enhancement for
self-supervised monocular depth estimation. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 12642–12652, 2021. 2
[23] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and
Franc ¸ois Fleuret. Transformers are rnns: Fast autoregressive
transformers with linear attention. In International confer-
ence on machine learning , pages 5156–5165. PMLR, 2020.
4
[24] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo
Kanazawa, and Matthew Tancik. Lerf: Language embedded
radiance ﬁelds. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 19729–19739,
2023. 3
[25] Diederik P Kingma and Jimmy Ba. Adam: A method for
9856
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[26] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Car-
oline Pantofaru, Leonidas J Guibas, Andrea Tagliasacchi,
Frank Dellaert, and Thomas Funkhouser. Panoptic neural
ﬁelds: A semantic object-aware neural scene representation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 12871–12881, 2022.
3
[27] Minhyeok Lee, Sangwon Hwang, Chaewon Park, and
Sangyoun Lee. Edgeconv with attention module for monoc-
ular depth estimation. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision , pages
2858–2867, 2022. 2
[28] Seokju Lee, Sunghoon Im, Stephen Lin, and In So
Kweon. Learning monocular depth in dynamic scenes
via instance-aware projection consistency. arXiv preprint
arXiv:2102.02629 , 2021. 2
[29] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Rene Ranftl. Language-driven semantic seg-
mentation. In International Conference on Learning Rep-
resentations , 2022. 3,6
[30] Rui Li, Xiantuo He, Yu Zhu, Xianjun Li, Jinqiu Sun, and
Yanning Zhang. Enhancing self-supervised monocular depth
estimation via incorporating robust constraints. In Proceed-
ings of the 28th ACM International Conference on Multime-
dia, pages 3108–3117, 2020. 2
[31] Rui Li, Dong Gong, Wei Yin, Hao Chen, Yu Zhu, Kaix-
uan Wang, Xiaozhi Chen, Jinqiu Sun, and Yanning Zhang.
Learning to fuse monocular and multi-view cues for multi-
frame depth estimation in dynamic scenes. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 21539–21548, 2023. 1,2
[32] Rui Li, Danna Xue, Shaolin Su, Xiantuo He, Qing Mao, Yu
Zhu, Jinqiu Sun, and Yanning Zhang. Learning depth via
leveraging semantics: Self-supervised monocular depth es-
timation with both implicit and explicit semantic guidance.
Pattern Recognition , page 109297, 2023. 2,8
[33] Jingyun Liang, Yuchen Fan, Kai Zhang, Radu Timofte, Luc
Van Gool, and Rakesh Ranjan. Movideo: Motion-aware
video generation with diffusion models. arXiv preprint
arXiv:2311.11325 , 2023. 1
[34] Yiyi Liao, Jun Xie, and Andreas Geiger. Kitti-360: A novel
dataset and benchmarks for urban scene understanding in 2d
and 3d. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 45(3):3292–3310, 2022. 2,6,7
[35] Ce Liu, Suryansh Kumar, Shuhang Gu, Radu Timofte, and
Luc Van Gool. Single image depth prediction made better: A
multivariate gaussian take. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 17346–17356, 2023. 2
[36] Ce Liu, Suryansh Kumar, Shuhang Gu, Radu Timofte, and
Luc Van Gool. Va-depthnet: A variational approach to single
image depth prediction. arXiv preprint arXiv:2302.06556 ,
2023. 2
[37] Xiaoyang Lyu, Liang Liu, Mengmeng Wang, Xin Kong,
Lina Liu, Yong Liu, Xinxin Chen, and Yi Yuan. Hr-depth:High resolution self-supervised monocular depth estimation.
arXiv preprint arXiv:2012.07356 , 2020. 2
[38] Ruihang Miao, Weizhou Liu, Mingrui Chen, Zheng Gong,
Weixin Xu, Chen Hu, and Shuchang Zhou. Occdepth:
A depth-aware method for 3d semantic scene completion.
arXiv preprint arXiv:2302.13540 , 2023. 2
[39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. In ECCV , 2020. 1,2
[40] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017. 6
[41] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea
Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al.
Openscene: 3d scene understanding with open vocabularies.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 815–824, 2023. 3
[42] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE transactions on pattern analysis and machine
intelligence , 44(3):1623–1637, 2020. 2
[43] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 12179–12188, 2021. 2,8
[44] Aron Schmied, Tobias Fischer, Martin Danelljan, Marc
Pollefeys, and Fisher Yu. R3d3: Dense 3d reconstruction
of dynamic scenes from multiple cameras. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 3216–3226, 2023. 2
[45] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi,
and Hongsheng Li. Efﬁcient attention: Attention with lin-
ear complexities. In Proceedings of the IEEE/CVF winter
conference on applications of computer vision , pages 3531–
3539, 2021. 4
[46] Jaime Spencer, Chris Russell, Simon Hadﬁeld, and Richard
Bowden. Kick back & relax: Learning to reconstruct the
world by watching slowtv. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 15768–
15779, 2023. 2
[47] Libo Sun, Jia-Wang Bian, Huangying Zhan, Wei Yin,
Ian Reid, and Chunhua Shen. Sc-depthv3: Robust self-
supervised monocular depth estimation for dynamic scenes.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 2023. 2
[48] Zachary Teed and Jia Deng. Deepv2d: Video to depth
with differentiable structure from motion. arXiv preprint
arXiv:1812.04605 , 2018. 2
[49] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
arXiv preprint arXiv:2106.10689 , 2021. 2
[50] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P
Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo
9857
Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-
net: Learning multi-view image-based rendering. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 4690–4699, 2021. 1
[51] Felix Wimbauer, Nan Yang, Christian Rupprecht, and Daniel
Cremers. Behind the scenes: Density ﬁelds for single view
reconstruction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 9076–
9086, 2023. 1,2,5,6,7,8
[52] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin,
and Zhiguo Cao. Structure-guided ranking loss for single im-
age depth prediction. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
611–620, 2020. 2
[53] Gangwei Xu, Junda Cheng, Peng Guo, and Xin Yang. Atten-
tion concatenation volume for accurate and efﬁcient stereo
matching. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 12981–
12990, 2022. 2
[54] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. V ol-
ume rendering of neural implicit surfaces. Advances in Neu-
ral Information Processing Systems , 34:4805–4815, 2021. 2
[55] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. En-
forcing geometric constraints of virtual normal for depth pre-
diction. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 5684–5693, 2019. 2
[56] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus,
Long Mai, Simon Chen, and Chunhua Shen. Learning to
recover 3d scene shape from a single image. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 204–213, 2021. 2
[57] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Si-
mon Chen, Yifan Liu, and Chunhua Shen. Towards accurate
reconstruction of 3d scene shape from a single monocular
image. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 2022.
[58] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu,
Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:
Towards zero-shot metric 3d prediction from a single image.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 9043–9053, 2023. 1,2
[59] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance ﬁelds from one or few images.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 4578–4587, 2021. 1,
2,5,6,7,8
[60] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-
tler, and Andreas Geiger. Monosdf: Exploring monocu-
lar geometric cues for neural implicit surface reconstruc-
tion. Advances in neural information processing systems ,
35:25018–25032, 2022. 2
[61] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and
Ping Tan. New crfs: Neural window fully-connected
crfs for monocular depth estimation. arXiv preprint
arXiv:2203.01502 , 2022. 2
[62] Chaoqiang Zhao, Youmin Zhang, Matteo Poggi, Fabio Tosi,
Xianda Guo, Zheng Zhu, Guan Huang, Yang Tang, andStefano Mattoccia. Monovit: Self-supervised monocular
depth estimation with a vision transformer. In 2022 Inter-
national Conference on 3D Vision (3DV) , pages 668–678.
IEEE, 2022. 2,5
[63] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and An-
drew J Davison. In-place scene labelling and understanding
with implicit scene representation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 15838–15847, 2021. 3
[64] Hang Zhou, David Greenwood, and Sarah Taylor. Self-
supervised monocular depth estimation with internal feature
fusion. arXiv preprint arXiv:2110.09482 , 2021. 2
9858
