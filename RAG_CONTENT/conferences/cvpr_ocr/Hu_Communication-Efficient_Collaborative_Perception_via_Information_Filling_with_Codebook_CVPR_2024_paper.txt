Communication-Efficient Collaborative Perception via
Information Filling with Codebook
Yue Hu1,4Juntong Peng1,4Sifei Liu1,4Junhao Ge1,4Si Liu3Siheng Chen1,2,4
1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University2Shanghai AI Laboratory
3Beihang University4Multi-Agent Governance & Intelligence Crew (MAGIC)
1{18671129361,juntong.peng,hiraeth416,cancaries,sihengc }@sjtu.edu.cn3{liusi }@buaa.edu.cn
Abstract
Collaborative perception empowers each agent to im-
prove its perceptual ability through the exchange of per-
ceptual messages with other agents. It inherently re-
sults in a fundamental trade-off between perception abil-
ity and communication cost. To address this bottleneck
issue, our core idea is to optimize the collaborative mes-
sages from two key aspects: representation and selec-
tion. The proposed codebook-based message representa-
tion enables the transmission of integer codes, rather than
high-dimensional feature maps. The proposed information-
filling-driven message selection optimizes local messages to
collectively fill each agent’s information demand, prevent-
ing information overflow among multiple agents. By in-
tegrating these two designs, we propose CodeFilling ,
a novel communication-efficient collaborative percep-
tion system, which significantly advances the perception-
communication trade-off and is inclusive to both homo-
geneous and heterogeneous collaboration settings. We
evaluate CodeFilling in both a real-world dataset,
DAIR-V2X, and a new simulation dataset, OPV2VH+.
Results show that CodeFilling outperforms previ-
ous SOTA Where2comm on DAIR-V2X/OPV2VH+ with
1,333/1,206 ×lower communication volume. Our code
is available at https://github.com/PhyllisH/
CodeFilling .
1. Introduction
Collaborative perception aims to enhance the perceptual
ability of each individual agent by facilitating the exchange
of complementary perceptual information among multiple
agents [1, 10, 12, 15, 16, 19, 22, 24, 30, 31, 43]. It
fundamentally overcomes the occlusion and long-range is-
sues in single-agent perception [4, 11, 13, 45]. As the
forefront of autonomous systems, collaborative perception
shows significant potential in enormous real-world appli-
Figure 1. CodeFilling avoids redundant messages and
achieves more complete detections by transmitting more critical
perceptual information with the compact code index message.
cations, particularly vehicle-to-everything-communication-
aided autonomous driving [2, 3, 18, 26, 28–30, 33, 38, 39].
In this emerging field, a central challenge lies in opti-
mizing the trade-off between perception performance and
communication cost inherent in agents sharing perceptual
data [10, 12, 16, 19, 20, 30, 41, 42]. Given inevitable practi-
cal constraints of communication systems, efficient utiliza-
tion of communication resources is the prerequisite for col-
laborative perception. To minimize communication over-
head, a straightforward solution is late collaboration, where
agents directly exchange the perception outputs. However,
numerous previous works indicate that late collaboration
yields marginal perception improvements and is vulnera-
ble to various noises [10, 21]. To optimize the perception-
communication trade-off, most studies consider intermedi-
ate collaboration, where the collaborative messages are per-
ceptual features [10, 12, 16, 19, 20, 29, 35, 37, 40]. For
example, When2com [19] proposed the handshake strategy
to limit the number of collaborators; and Where2comm [10]
proposed a pragmatic strategy that only transmits messages
about constrained spatial areas. While these methods miti-
gate certain communication costs, they still necessitate the
transmission of high-dimensional feature maps, which in-
curs substantial communication expenses.
To overcome the limitations of intermediate collabora-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
15481
tion, our core idea is to optimize the collaborative messages
from two key perspectives: representation and selection.
For message representation, we introduce a codebook to
standardize the communication among agents, where each
code is analogous to a word in the human language dictio-
nary. Based on this shared codebook among all agents, we
can use the codes to approximate perceptual features; con-
sequently, only integer code indices need to be exchanged,
eliminating the need for transmitting high-dimensional fea-
tures comprised of floating-point numbers. For message
selection, we propose an information filling strategy, akin
to piecing together a jigsaw puzzle. In this approach, as-
suming an agent’s information demand is upper bounded,
each of its collaborators performs a local optimization to
select non-redundant messages to fill its information gap.
This strategy prevents information overflow among multiple
agents, further significantly reducing communication cost.
Following the above spirit, we propose CodeFilling ,
a novel communication-efficient collaborative 3D detection
system; see Figure 2. The proposed CodeFilling in-
cludes four key modules: i) a single-agent detector, provid-
ing basic detection capabilities; ii) an novel information-
filling-driven message selection , which solves local opti-
mizations for choosing pertinent messages to optimally fill
other agents’ information demands without causing infor-
mation flow; iii) an novel codebook-based message rep-
resentation , which leverages a task-driven codebook to
achieve pragmatic approximation of feature maps, enabling
the transmission of integer code indices; and iv) a message
decoding and fusion module, which integrates the messages
to achieve enhanced collaborative detections.
CodeFilling offers two distinct advantages: i)
it delivers a substantial advancement in the perception-
communication trade-off through the transmission of non-
redundant code indices; and ii) it is inclusive to both settings
of homogeneous and heterogeneous agents by leveraging a
standardized code representation; that is, feature maps ob-
tained from various perception models and diverse sensors
can be aligned to the unified feature space provided by the
proposed codebook.
To evaluate CodeFilling , we conduct extensive ex-
periments on real-world dataset DAIR-V2X, and a new sim-
ulation dataset OPV2VH+ under two homogeneous (Li-
DAR, camera) and one heterogeneous setting. The re-
sults show that i) CodeFilling achieves superior perfor-
mance performances than Where2comm, the current SOTA,
with 1333/1206 ×less communication cost on DAIR-
V2X/OPV2VH+; and ii) CodeFilling maintains supe-
rior trade-off in both homogeneous and heterogeneous set-
tings, establishing an inclusive collaboration system.
To sum up, our main contributions are three-fold:
•We propose CodeFilling , a novel communication-
efficient collaborative 3D detection system, which signif-icantly improves the perception- communication trade-off
and is inclusive to both homogeneous and heterogeneous
collaboration settings;
•We propose two novel methods to optimize collabora-
tive messages: codebook-based message representation and
information-filling-driven message selection;
•We conduct comprehensive experiments to vali-
date that CodeFilling achieves SOTA perception-
communication trade-off across varying communication
bandwidths, on both real-world and simulation datasets in
both homogeneous and heterogeneous settings.
2. Related works
Collaborative Perception. Collaborative perception [10,
12, 16, 17, 19–21, 30, 32, 35, 36, 43] is an emerging
application of multi-agent communication systems to per-
ception tasks, which promote the crucial perception mod-
ule through communication-enabled complementary per-
ceptual information sharing. Several high-quality datasets
have emerged [10, 12, 17, 35, 43, 44] to aid in the algo-
rithm development. Collaborative perception systems have
made remarkable progress in improving perception perfor-
mance [36, 37] and robustness on practical issues, such as
communication bandwidth constraints, pose error [21, 27]
and latency [14, 31]. Here, considering that communication
efficiency is the bottleneck issue for the scale-up of collab-
orative perception, we aim to optimize the performance-
communication trade-off instead of solely promoting the
perception performance regardless of bandwidth costs.
Communication efficiency in collaborative perception.
To address this bottleneck issue, prior methods have made
efforts in two key aspects: message selection and message
representation. For message selection, When2com [19] and
Who2comm [20] employ a handshake mechanism, to se-
lect information from all the relevant collaborators. Fur-
thermore, Where2comm [10] and CoCa3D [12] expand the
selection process to incorporate spatial dimensions. For
message representation, intermediate feature representa-
tion [10, 12, 16, 17, 30, 36, 37] has demonstrated a more
balanced performance-communication trade-off. Source
coding [30] and channel compression [16] techniques are
used to further enhance feature representation efficiency.
However, previous methods accumulate redundant infor-
mation from various collaborators and still transmit high-
dimensional feature vectors, incurring high communication
costs. Here, we facilitate essential supportive information
exchange among agents with compact codebook-based rep-
resentation, efficiently enhancing detection performance.
Codebook compression. Codebook compression, a loss-
less compression technique, effectively captures the essence
of high-dimensional vectors through the combination of
these codes [7]. It has diverse applications, ranging from
digital image compression [8] to the neural network param-
eters compression [9]. Recently, task-adaptive codebooks
15482
Figure 2. CodeFilling is a novel communication-efficient collaborative 3D detection system. The proposed information-filling-driven
message selection and codebook-based message representation contribute to optimizing collaborative messages.
have emerged. Rather than pursuing lossless compression,
it drops task-irrelevant information and focuses on essen-
tial information for specific downstream tasks, further im-
proving representation efficiency [5, 25]. However, existing
task-adaptive codebooks have largely concentrated on 2D
classification tasks [25]. Here, we explore the novel realm
of collaborative 3D object detection, introducing fresh chal-
lenges for codebook compression. This entails preserving
an extensive feature set for precise 3D interpretation and
adapting to fluctuating communication bandwidths, neces-
sitating versatile codebook configurations.
3. Problem Formulation
Consider Nhomogeneous or heterogeneous agents in the
scene, each has its individual perceptual task and unique
sensor setup. To enhance the perception abilities, the agents
exchange complementary perceptual information, forming
a decentralized mutually beneficial collaboration network.
Each agent concurrently acts as both a supporter and a re-
ceiver. In their role as supporters, they contribute perceptual
information to assist their counterparts. Conversely, as re-
ceivers, they gain from the messages provided by others.
Such collaborative perception leads to a holistic enhance-
ment of perceptual capabilities. Here we focus on 3D object
detection. Let Xibe the input collected by the ith agent’s
sensor (LiDAR or camera), and O0
ibe the corresponding
ground-truth detection. The objective is to maximize the
detection performances of all agents given certain commu-
nication budget B; that is,
max
θ,PX
ig
Φθ
Xi,{Pj→i}N
j=1
,O0
i
,s.t.X
i,j,j̸=ib(Pj→i)≤B,
(1)
where g(·,·)is the detection evaluation metric, Φ(·)is a de-
tection model with trainable parameter θ,Pj→iis the mes-
sage transmitted from the ith agent to the jth agent, and
b(·)measures the communication cost of the collaborative
messages. The key challenge is to determine the messagesPj→i, which should be both informative and compact.
4. CodeFilling: Collaborative 3D Detection
To optimize the trade-off between perception ability and
communication cost, we present CodeFilling , a novel
communication-efficient collaborative 3D detection system;
see Fig. 2. It has two parts: i) single-agent 3D detection,
which allows an agent to equip basic detection ability, im-
plementing Φθin (1), and ii) multi-agent collaboration, en-
hancing an agent’s detection ability through the exchange
of efficient perceptual messages Pin (1).
4.1. Single-agent 3D detection
An agent learns to detect 3D objects based on its sen-
sor inputs. It involves an observation encoder and a de-
tection decoder. CodeFilling allows agents to accept
multi-modality inputs, including RGB images and 3D point
clouds. Each agents with its distinct modality projects its
perceptual information to the unified global bird’s eye view
(BEV) coordinate system, better supporting inter-agent col-
laboration and more compatible with both homogeneous
and heterogeneous settings.
Observation encoder. The observation encoder extracts
feature maps from the sensor data. For the ith agent, given
its input Xi, the BEV feature map is Fi= Φ enc(Xi)∈
RH×W×C,where Φenc(·)is the encoder and H, W, C are
its height, weight and channel. For image inputs, Φenc(·)is
followed by an additional warping function that transforms
the extracted front-view feature to BEV . The BEV feature is
output to the decoder, and also message selection and fusion
modules when collaboration is established.
Detection decoder. The detection decoder decodes fea-
tures into objects, including class and regression output.
Given the feature map Fi, the detection decoder Φdec(·)
generate the detections of ith agent by Oi= Φ dec(Fi)∈
RH×W×7,where each location of Oirepresents a rotated
box with class (c, x, y, h, w, cosα,sinα), denoting class
confidence, position, size and angle.
15483
4.2. Multi-agent collaboration
In the proposed multi-agent collaboration, each agent acts
in the dual role of supporter and receiver. As a sup-
porter, each agent employs two novel modules, including
information-filling-driven message selection and codebook-
based message representation, to determine compact, yet
supportive collaboration messages to help others. These
two proposed modules enhance communication efficiency
in both spatial and channel dimensions of a feature map, re-
spectively. As a receiver, each agent employs a message de-
coding and fusion module to integrate supportive messages
from other agents, improving its perceptual performance.
4.2.1 Information-filling-driven message selection
To efficiently select compact collaborative messages
that support other agents, each agent employs a novel
information-filling-driven message selection method. The
key idea is to enable each agent to restrainedly select per-
tinent messages to share with other agents; then collec-
tively, these pieces of non-redundant messages mutually
fulfill each other’s information demands. For example,
in occlusion scenarios, extra information from supporters
helps an agent detect missed objects. However, overfilled
information from multiple supporters wastes communica-
tion resources. Thus, collective coordination is essential to
avoid redundancy and enable more beneficial information.
To achieve this, the proposed selection has two key steps:
information disclosure, wherein agents mutually share their
awareness of available information within specific spatial
areas, and filling-driven optimization, wherein each agent
locally optimizes the supportive messages for others.
Information disclosure. In information disclosure, each
agent: i) employs an information score generator to create
its information score map from its feature map, reflecting its
available information at each spatial area, and ii) broadcasts
this map to all other agents, promoting a mutually thorough
awareness of all the available support.
The information score map is implemented with the de-
tection confidence map. Intuitively, the areas containing an
object are likely to offer more useful information for reveal-
ing missed detections and, therefore, should be assigned
higher information scores. Specifically, given a BEV fea-
ture map Fi, its spatial information score map is
Ci= Φ generator (Fi)∈[0,1]H×W, (2)
where Φgenerator (·)is implemented by detection decoder.
When information score map is generated, each agent
broadcasts it to other agents. This initial communication
is efficient because of lightweight information score maps.
Filling-driven optimization. In the role of a supporter,
each agent gathers the other agents’ information score maps
and determines who needs perceptual information at which
spatial areas by locally solving a filling-driven optimization.
Here, filling the information demands means that an agent
Figure 3. The information-filling-driven message selection fulfills
the information demand with non-redundant information.
only requires the necessary information at certain spatial
areas for precise detection, as extra information no longer
provides significant benefits. This requires each supporter
to prioritize non-redundant and informative spatial regions
with higher scores to assist others and halts the selection
once the receiver’s information demands are fulfilled.
Specifically, the optimization is formulated as a proxy-
constrained problem and obtains a binary selection matrix
for each agent to support each receiver. Let Mi→j∈
{0,1}H×Wbe the binary selection matrix supported on the
BEV map. Each element in the matrix indicates whether
Agent ishould send the information to Agent jat a spe-
cific spatial location (1 for sending information, and 0 for
not sending). To solve for the binary selection matrix, the
proxy-constrained problem is formulated as follows,
{M∗
i→j}i,j= argmax
MNX
j=1fmin
Cj+NX
i=1,i̸=jMi→j⊙Ci, u
,
(3a)
whereNX
i,j=1,j̸=iMi→j≤b,Mi→j∈ {0,1}H×W. (3b)
Here⊙denotes element-wise multiplication, and the scalar
uis a hyper-parameter to reflect the upper bound of in-
formation demand. The function fmin(·,·)computes the
element-wise minimum between a matrix and a scalar.
In (3a), Cj+PN
i=1,i̸=jMi→j⊙Ciindicates that each
receiver jaccumulates the information transmitted from all
supporters, combined with its own information, at each lo-
cation, fmin(Cj+PN
i=1,i̸=jMi→j⊙Ci, u)denotes the
utility for each receiver, linearly increasing with the accu-
mulated information scores until reaching the information
demand u. Note that: i) (3a) is solved at the supporter side
for preparing messages to a receiver; ii) the sum-based util-
ity motivates supporters to collectively meet the receiver’s
demand and focus on higher-scoring regions, and iii) the
cutoff point leads to halting selection to prevent redundancy.
Equation (3a) transforms the feature-based collabora-
tion utility in (1) as the sum of the information scores.
This is based on the assumption that the accumulation of
information scores mirrors the benefits of feature aggre-
gation. Equation (3b) addresses the bandwidth limitation
in (1) by quantifying the total number of selected regions.
This approach simplifies the objective in (1) into a proxy-
constrained problem in (3a) and (3b). The optimized selec-
tion solution derived from (1) is expected to yield a superior
outcome in the final feature-based collaboration.
This optimization problem has an analytical solution; see
the theoretical derivation in the appendix. The solving pro-
15484
cess incurs a computational cost of O(log(m)), where m
denotes the number of spatial region candidates. By focus-
ing on the extremely sparse foreground areas, we effectively
reduce the cost to a negligible level, enabling each agent to
provide more targeted support for others with minimal cost.
Based on the optimized selection matrix {M∗
i→j}N
j=1,
each agent supports each collaborator with a sparse yet in-
formative feature map Zi→j=M∗
i→j⊙ F i, promising
superior perception improvements given the limited com-
munication budget. These selected sparse feature maps are
then output to the message representation module.
The proposed message selection offers two key benefits:
i) it avoids redundancy from multiple supporters via col-
lective selection, and ii) it adapts to varying communication
conditions by adjusting information demand, lower demand
for efficiency in limited budgets, and higher demand for su-
perior performance in ample budgets. Compared to existing
selection methods [10, 12, 19, 20], which are based on indi-
vidual supporter-receiver pairs, our collective optimization
further reduces redundancy across various supporters.
4.2.2 Codebook-based message representation
To efficiently transmit the selected feature map Zi→j, each
agent leverages a novel codebook-based message repre-
sentation, reducing communication cost along the chan-
nel dimension. The core idea is to approximate a high-
dimensional feature vector by the most relevant code from a
task-driven codebook; as a result, only integer code indices
need to be transmitted, rather than the complete feature vec-
tors composed of floating-point numbers.
Codebook learning. Analogous to a language dictio-
nary used by humans, our task-driven codebook is shared
among all agents to standardize their communication for
achieving the detection task. This codebook consists of a set
of codes, which are learned to pragmatically approximate
possible perceptual features present in the training dataset.
Here the pragmatic approximation refers to each code serv-
ing as a lossy approximation of a feature vector, while re-
taining essential information necessary for the downstream
detection task within that vector. Specifically, let 𭟋=
{F(i,s)}N,S
i=1,s=1be the collective set of BEV feature maps
extracted by the observation encoders of all Nagents across
allStraining scenes. Let D=d1,d2,···,dnL
∈
RC×nLbe the codebook, where D[ℓ]=dℓ∈RCis the
ℓth code and nLis the number of codes.
The task-driven codebook is learned through feature ap-
proximation at each spatial location; that is,
D∗= arg min
DX
F∈𭟋X
h,wmin
ℓ
Ψ(D[ℓ]) +F[h,w]−D[ℓ]2
2
,(4)
where Ψ(·)denotes the resulting detection performance
achieved by substituting D[ℓ]forF[h,w]. The first term
pertains to the requirements of the downstream detection
Figure 4. The codebook-based message representation depicts the
original feature vector with the most relevant codes.
task and the second term reflects the reconstruction error
between the original feature vector and the code. This ap-
proximation is lossy for reconstruction while lossless for
the perceptual task, enabling the reduction of communica-
tion cost without sacrificing perceptual capacity.
Code index representation. Based on the shared code-
bookD, each agent can substitute the selected sparse fea-
ture map Zi→jby a series of code indices Ii→j. For each
BEV location (h, w), the code index is obtained as,
(Ii→j)[h,w]= arg min
ℓ(Zi→j)[h,w]−D[ℓ]2
2. (5)
The codebook offers versatility in its configuration by ad-
justing both the codebook size nLand the quantity of codes
nRused for representing the input vector. Equation (5)
demonstrates a specific instance where nR= 1, chosen for
simplicity in notation. When nRis larger, the representation
involves a combination of multiple codes.
Overall, the final message sent from the ith agent to the
jth agent is Pi→j=Ii→j, conveying the required com-
plementary information with compact code indices. Agents
exchange these packed messages with each other.
This codebook-based representation offers three advan-
tages: i) efficiency for transmitting lightweight code in-
dices; ii) adaptability to various communication resources
via adjusting code configurations (smaller for efficiency,
larger for superior performance), and iii) extensibility by
providing a shared standardized representation. New het-
erogeneous agents can easily join the collaboration by
adding its effective perceptual feature basis to the codebook.
4.2.3 Message decoding and fusion
Message decoding reconstructs the supportive features
based on the received code indices and the shared codebook.
Given the received message Pj→i=Ij→i, the decoded fea-
ture map’s bZj→i∈RH×W×Celement located at (h, w)is
(bZj→i)[h,w]=D[Ij→i[h,w]]. Subsequently, message fusion
aggregates these decoded feature maps to augment indi-
vidual features, implementing by the non-parametric point-
wise maximum fusion. For the ith agent, given the recon-
structed feature bZj→i. The enhanced BEV feature is ob-
tained as Hi= max
j∈Ni(Fi,bZj→i)∈RH×W×Cwhere Niis
i-th agent’s connected collaborators and max(·)maximizes
the corresponding features from multiple agents at each in-
dividual spatial location. The enhanced feature Hiis de-
coded to generate the upgraded detection bOi.
15485
Figure 5. In DAIR-V2X, CodeFilling achieves the best perception-communication trade-off in homogeneous & heterogeneous settings.
Figure 6. In OPV2VH+, CodeFilling achieves the best perception-communication trade-off in homogeneous & heterogeneous settings.
4.3. Loss functions
To train the overall system, we supervise three tasks: infor-
mation score map generation, object detection, and code-
book learning. The information score map generator reuses
the parameters of the detection decoder. The overall loss is
defined as, L=PN
iLdet
bOi,O0
i
+Fi−Fi2
2,where
Ldet(·)denotes the detection loss [45], O0
iandbOirepre-
sents the ground-truth and predicted objects, and FiandFi
denote the i-th agent’s original feature map and the one ap-
proximated by codes. During the optimization, the network
parameters and the codebook are updated simultaneously.
5. Experimental Results
Our experiments cover two datasets, both real-world and
simulation scenarios, two types of sensors (LiDAR and
cameras), and both homogeneous and heterogeneous set-
tings. Specifically, we conduct 3D object detection in the
setting of V2X-communication-aided autonomous driving
on DAIR-V2X dataset [43] and the extended large-scale
OPV2VH+ dataset. The detection results are evaluated by
Average Precision (AP) at Intersection-over-Union (IoU)
thresholds of 0.30and0.50. The communication volume
follows the standard setting as [10, 12, 33, 35] that counts
the message size by byte in log scale with base 2.
5.1. Datasets and experimental settings
DAIR-V2X [43] is a widely-used real-world collabora-
tive perception dataset. Each scene contains two agents:
a vehicle and a road-side-unit. Each agent is equipped
with a LiDAR and a camera. The perception range is
204.8m ×102.4m. OPV2VH+ is an extended large-scale
version of the original vehicle-to-vehicle camera-only col-laborative perception dataset OPV2V+ [12] with a larger
array of collaborative agents (a total of 10) and addi-
tional LiDAR sensors, co-simulated by OpenCDA [34] and
CARLA [6]. Each agent has a LiDAR, 4 cameras, and 4
depth sensors. The detection range is 281.6m ×80m.
Implementation. We adopt PointPillar [13] and
CaDDN [23] for the LiDAR and camera detector, respec-
tively. Regarding the heterogeneous setup, agents are ran-
domly assigned either LiDAR or camera, resulting in a bal-
anced 1:1 ratio of agents across the different modalities.
Communication volume. Specifically, for feature repre-
sentation, given a selection matrix M, the bandwidth is cal-
culated as log2(H×W×|M|×C×32/8). Here, 32repre-
sents the float32 data type and 8converts bits to bytes. For
code index representation, given codebook D∈RC×nL,
comprised of nLcodes and each vector constructed using
nRcodes, the bandwidth given the selection matrix Mis
calculated as log2(H×W× |M| ×log2(nL)×nR/8).
Here, log2(nL)signifies the data amount required to repre-
sent each code index integer, decided by the codebook size.
5.2. Quantitative evaluation
Benchmark comparison. Fig. 6 and 5 compare the pro-
posed CodeFilling with previous methods in terms
of the trade-off between detection performance and com-
munication bandwidth for DAIR-V2X and OPV2VH+
datasets under homogeneous and heterogeneous settings,
respectively. Detailed values can also be found in
the appendix. Baselines include no collaboration ( Oi),
Where2comm [10], HMViT [33], V2VNet [30], Dis-
coNet [16], V2X-ViT [37], AttFuse [35] and late fusion,
where agents exchange the detected 3D boxes directly. Note
that HMViT is specifically designed for heterogeneous set-
15486
Figure 7. CodeFilling is robust to pose error issue.
 Figure 8. CodeFilling is robust to communication latency issue.
(a) Two components.
 (b) Selection utility design.
 (c) Information demand.
 (d) Codebook size.
 (e) Code quantity.
Figure 9. Both the proposed information-filling-driven message selection and codebook-based representation are effective.
tings by using domain adaption, while CodeFilling is
naturally compatible with heterogeneous settings without
additional cost. We see that CodeFilling : i) achieves
a far-more superior perception-communication trade-off
across all the communication bandwidth choices and var-
ious collaborative perception settings, including camera-
only, lidar-only, and heterogeneous 3D detection; ii) sig-
nificantly improves the detection performance, especially
under extremely limited communication bandwidth, im-
proves the SOTA performance by 11.093/5.271/38.357%,
14.75/28.516/28.372% for LiDAR/camera/heterogeneous
on DAIR-V2X and OPV2VH+ even when the bandwidth
is constrained by a factor of 100K; and iii) outperforms pre-
vious communication-efficient SOTA, Where2comm, with
significantly reduced communication cost: 1333/115/863,
1206/1078/252 times less on DAIR-V2X and OPV2VH+.
Furthermore, for inference speed, CodeFilling
(36/99ms) is comparable to Where2comm (34/94ms), and
significantly faster than HMViT (90/1266ms) on DAIR-
V2X/OPV2VH+. This communication efficiency ensures
that agents are able to actively collaborate with each other.
Robustness to pose error and communication latency.
We validate the robustness against pose error and commu-
nication latency on both OPV2VH+ and DAIR-V2X. The
pose error setting follows CoAlign [21] using Gaussian
noise with a mean of 0m and standard deviations ranging
from 0m to 1.0m. The latency setting follows SyncNet [14],
varying from 0ms to 500ms. Figs. 7 and 8 show the detec-
tion performances as a function of pose error and latency,
respectively. We see: i) while perception performance gen-
erally declines with increasing levels of pose error and la-
tency, CodeFilling consistently outperforms baselines
under all imperfect conditions; ii) CodeFilling consis-tently surpasses No Collaboration, whereas baselines fail
when pose error exceeds 0.6m and latency surpasses 100ms.
5.3. Ablation studies
Effectiveness of our message selection and representa-
tion. Fig. 9a compares CodeFilling , the one without
message selection, and the one without codebook and mes-
sage selection. We see that: i) applying code index rep-
resentation reduces the communication cost by 208 times
while maintaining the same detection performance, as it by-
passes the high channel dimension typical of feature vec-
tors, and an integer index requires less data than the orig-
inal floating-point numbers, and ii) applying information-
filling-driven message selection achieves 4.8% higher de-
tection performance with the same communication cost, as
it reallocates the bandwidth wasted in redundant informa-
tion to more beneficial information.
Ablation of information-filling-driven message selection.
Fig. 9b compares different utility designs in information-
filling optimization: sum, max, and scenarios without se-
lection. The max utility design favors selecting collabora-
tors with the highest score, leading to no selection if the
ego agent has the highest score. We see that the sum-based
utility outperforms both the max and no selection in the
perception-communication trade-off across all communica-
tion bandwidth conditions. This superiority is due to the op-
timized combination of information from different collabo-
rators, which has proved to be more effective than relying
solely on the best-performing single agent. This approach
encourages agents to participate in collaboration; even the
top-performing agents can benefit from collaboration.
Fig. 9b evaluates three different information demands u
(0.5, 1.0, 1.5). We see that: i) lower udemonstrates better
efficiency under limited communication budgets; ii) higher
15487
(a) Ego detections
 (b) Ego feature
 (c) Ego information score
 (d) Collaborator score 1
 (e) Collaborator score 2
(f) Selection matrix 1
 (g) Selection matrix 2
 (h) Filled confidence
 (i) Codebook-based message
 (j) Collaborative detections
Figure 10. Visualization of collaboration in CodeFilling . Green and red denote ground truth and detection, respectively.
(a) No Collaboration
 (b) V2X-ViT
 (c) HMViT
 (d) Where2comm
 (e) CodeFilling
Figure 11. CodeFilling achieves more accurate detections with 1256 times less communication cost. Green and red boxes denote
ground-truth and detection, respectively.
udemonstrates superior performance under ample commu-
nication budgets. By adjusting u,CodeFilling con-
sistently maintains superior performance-communication
trade-off across all communication conditions.
Ablation of codebook-based message representation.
Fig. 9d and Fig. 9e explore different codebook configu-
rations: codebook size and code quantity. We see that:
i) all the codebook configurations demonstrate a supe-
rior perception-communication trade-off under highly con-
strained communication conditions, showing the effective-
ness and robustness of the codebook-based representation;
and ii) larger codebook sizes and quantities yield bet-
ter performance, while smaller sizes and quantities offer
greater communication efficiency. By adapting the config-
uration, CodeFilling maintains superior performance-
communication trade-off across all communication budgets.
5.4. Qualitative evaluation
Visualization of message selection and representa-
tion. Fig. 10 showcases the efficient collaboration in
CodeFilling . The scene features one ego agent and two
collaborators - one with LiDAR and the other with a cam-
era, with all visualizations presented from the ego’s per-
spective. Fig. 10 (a) and (j) compare the detection results
before and after collaboration. We see that through col-
laboration, the ego agent successfully uncovers detections
missed in its individual view. Fig. 10 (d-g) displays the col-
laborators’ information score maps and their corresponding
selection matrices. We see that the redundant information
in the overlapped regions is avoided, promoting communi-
cation efficiency. Fig. 10 (c) and (h) illustrate the evolution
of the information score before and after filling, highlight-ing that the information demands have been met, which is
indicative of improved perception performance. Fig. 10 (f)
presents the codebook-represented message, revealing that:
i) it is spatially sparse, and ii) the selected information from
heterogeneous collaborators is unified in a common format,
facilitating extensibility.
Visualization of detection results. Fig. 11 com-
pares CodeFilling with previous SOTAs. We see
thatCodeFilling qualitatively outperforms previous
SOTAs with 1256 times less communication cost. The rea-
son is that CodeFilling avoids redundant information
from different collaborators and employs efficient code in-
dex representation, thereby transmitting more critical per-
ceptual information even with less communication cost.
6. Conclusions
We propose CodeFilling , a novel communication-
efficient collaborative 3D detection system with two
novel designs: codebook-based message representation and
information-filling-driven message selection. Extensive ex-
periments covering both real-world and simulation scenar-
ios show that CodeFilling not only achieves state-of-
the-art perception-communication trade-off under various
modalities, including LiDAR, camera, and heterogeneous
settings but also is robust to pose error and latency issues.
Limitation and future work. We plan to explore the tem-
poral dimension and determine critical time stamps.
Acknowledgments. This research is supported by the
National Key R&D Program of China under Grant
2021ZD0112801, NSFC under Grant 62171276 and the
Science and Technology Commission of Shanghai Munici-
pal under Grant 21511100900 and 22DZ2229005.
15488
References
[1] Ebtehal Turki Alotaibi, Shahad Saleh Alqefari, and Anis
Koubaa. Lsar: Multi-uav collaboration for search and res-
cue missions. IEEE Access , 7:55817–55832, 2019. 1
[2] Qi Chen. F-cooper: feature based cooperative perception for
autonomous vehicle edge computing system using 3d point
clouds. Proceedings of the 4th ACM/IEEE Symposium on
Edge Computing , 2019. 1
[3] Runjian Chen, Yao Mu, Runsen Xu, Wenqi Shao, Chenhan
Jiang, Hang Xu, Yu Qiao, Zhenguo Li, and Ping Luo. CO3:
Cooperative unsupervised 3d representation learning for au-
tonomous driving. In The Eleventh International Conference
on Learning Representations , 2023. 1
[4] Siheng Chen, Baoan Liu, Chen Feng, Carlos Vallespi-
Gonzalez, and Carl K. Wellington. 3d point cloud processing
and learning for autonomous driving: Impacting map cre-
ation, localization, and perception. IEEE Signal Processing
Magazine , 38:68–86, 2021. 1
[5] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengy-
ong Wu, Yunji Chen, and Olivier Temam. Diannao: a
small-footprint high-throughput accelerator for ubiquitous
machine-learning. Proceedings of the 19th international
conference on Architectural support for programming lan-
guages and operating systems , 2014. 3
[6] Alexey Dosovitskiy, Germ ´an Ros, Felipe Codevilla, Anto-
nio M. L ´opez, and Vladlen Koltun. Carla: An open urban
driving simulator. In Conference on Robot Learning , 2017.
6
[7] Allen Gersho and Robert M. Gray. Vector quantization and
signal compression. In The Kluwer International Series in
Engineering and Computer Science , 1991. 2
[8] Allen Gersho and Bhaskar Ramamurthi. Image coding using
vector quantization. In IEEE International Conference on
Acoustics, Speech, and Signal Processing , 1982. 2
[9] Song Han, Huizi Mao, and William J. Dally. Deep com-
pression: Compressing deep neural network with pruning,
trained quantization and huffman coding. Computer Vision
and Pattern Recognition , 2015. 2
[10] Yue Hu, Shaoheng Fang, Zixing Lei, Yiqi Zhong, and Si-
heng Chen. Where2comm: Communication-efficient collab-
orative perception via spatial confidence maps. Advances
in neural information processing systems , 35:4874–4886,
2022. 1, 2, 5, 6
[11] Yue Hu, Shaoheng Fang, Weidi Xie, and Siheng Chen.
Aerial monocular 3d object detection. IEEE Robotics and
Automation Letters , 8:1959–1966, 2022. 1
[12] Yue Hu, Yifan Lu, Runsheng Xu, Weidi Xie, Siheng Chen,
and Yanfeng Wang. Collaboration helps camera overtake li-
dar in 3d detection. 2023 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2023. 1, 2, 5,
6
[13] Alex H. Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,
Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders
for object detection from point clouds. 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 12689–12697, 2018. 1, 6[14] Zixing Lei, Shunli Ren, Yue Hu, Wenjun Zhang, and Siheng
Chen. Latency-aware collaborative perception. ECCV , 2022.
2, 7
[15] Jinlong Li, Runsheng Xu, Xinyi Liu, Baolu Li, Qin Zou, Ji-
aqi Ma, and Hongkai Yu. S2r-vit for multi-agent coopera-
tive perception: Bridging the gap from simulation to reality.
ArXiv , abs/2307.07935, 2023. 1
[16] Yiming Li, Shunli Ren, Pengxiang Wu, Siheng Chen, Chen
Feng, and Wenjun Zhang. Learning distilled collaboration
graph for multi-agent perception. Advances in Neural Infor-
mation Processing Systems , 34:29541–29552, 2021. 1, 2, 6
[17] Yiming Li, Ziyan An, Zixun Wang, Yiqi Zhong, Siheng
Chen, and Chen Feng. V2X-Sim: A virtual collaborative
perception dataset for autonomous driving. IEEE Robotics
and Automation Letters , 7, 2022. 2
[18] Yiming Li, Qi Fang, Jiamu Bai, Siheng Chen, Felix Juefei-
Xu, and Chen Feng. Among us: Adversarially robust collab-
orative perception by consensus. ICCV , 2023. 1
[19] Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, and Zsolt
Kira. When2com: Multi-agent perception via communica-
tion graph grouping. In Proceedings of the IEEE/CVF Con-
ference on computer vision and pattern recognition , pages
4106–4115, 2020. 1, 2, 5
[20] Yen-Cheng Liu, Junjiao Tian, Chih-Yao Ma, Nathan Glaser,
Chia-Wen Kuo, and Zsolt Kira. Who2com: Collaborative
perception via learnable handshake communication. In 2020
IEEE International Conference on Robotics and Automation
(ICRA) , pages 6876–6883. IEEE, 2020. 1, 2, 5
[21] Yifan Lu, Quanhao Li, Baoan Liu, Mehrdad Dianat, Chen
Feng, Siheng Chen, and Yanfeng Wang. Robust collabora-
tive 3d object detection in presence of pose errors. IEEE In-
ternational Conference on Robotics and Automation (ICRA) ,
2023. 1, 2, 7
[22] Yifan Lu, Yue Hu, Yiqi Zhong, Dequan Wang, Siheng Chen,
and Yanfeng Wang. An extensible framework for open het-
erogeneous collaborative perception. In The Twelfth Interna-
tional Conference on Learning Representations , 2024. 1
[23] Cody Reading, Ali Harakeh, Julia Chae, and Steven L.
Waslander. Categorical depth distribution network for
monocular 3d object detection. 2021 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
8551–8560, 2021. 6
[24] J ¨urgen Scherer, Saeed Yahyanejad, Samira Hayat, Evsen
Yanmaz, Torsten Andre, Asif Khan, Vladimir Vukadinovic,
Christian Bettstetter, Hermann Hellwagner, and Bernhard
Rinner. An autonomous multi-uav system for search and res-
cue. In Proceedings of the First Workshop on Micro Aerial
Vehicle Networks, Systems, and Applications for Civilian
Use, pages 33–38, 2015. 1
[25] Saurabh Singh, Sami Abu-El-Haija, Nick Johnston, Jo-
hannes Ball ´e, Abhinav Shrivastava, and George Toderici.
End-to-end learning of compressible features. In 2020 IEEE
International Conference on Image Processing (ICIP) , pages
3349–3353. IEEE, 2020. 3
[26] Sanbao Su, Yiming Li, Sihong He, Songyang Han, Chen
Feng, Caiwen Ding, and Fei Miao. Uncertainty quantifica-
tion of collaborative detection for self-driving. ICRA , 2023.
1
15489
[27] Nicholas Vadivelu, Mengye Ren, James Tu, Jingkang Wang,
and Raquel Urtasun. Learning to communicate and correct
pose errors. In Conference on Robot Learning , 2020. 2
[28] Binglu Wang, Lei Zhang, Zhaozhong Wang, Yongqiang
Zhao, and Tianfei Zhou. Core: Cooperative reconstruction
for multi-agent perception. ICCV , 2023. 1
[29] Tianhang Wang, Guang Chen, Kai Chen, Zhengfa Liu, Bo
Zhang, Alois Knoll, and Changjun Jiang. Umc: A uni-
fied bandwidth-efficient and multi-resolution based collab-
orative perception framework. 2023 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , 2023.
1
[30] Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang,
Bin Yang, Wenyuan Zeng, and Raquel Urtasun. V2vnet:
Vehicle-to-vehicle communication for joint perception and
prediction. In European Conference on Computer Vision ,
pages 605–621. Springer, 2020. 1, 2, 6
[31] Sizhe Wei, Yuxi Wei, Yue Hu, Yifan Lu, Yiqi Zhong, Si-
heng Chen, and Ya Zhang. Asynchrony-robust collaborative
perception via bird’s eye view flow. In Advances in Neural
Information Processing Systems , 2023. 1, 2
[32] Hao Xiang, Runsheng Xu, Xin Xia, Zhaoliang Zheng, Bolei
Zhou, and Jiaqi Ma. V2xp-asg: Generating adversarial
scenes for vehicle-to-everything perception. 2023 IEEE In-
ternational Conference on Robotics and Automation (ICRA) ,
pages 3584–3591, 2022. 2
[33] Hao Xiang, Runsheng Xu, and Jiaqi Ma. Hm-vit: Hetero-
modal vehicle-to-vehicle cooperative perception with vision
transformer. ICCV , 2023. 1, 6
[34] Runsheng Xu, Yi Guo, Xu Han, Xin Xia, Hao Xiang, and
Jiaqi Ma. Opencda: An open cooperative driving automa-
tion framework integrated with co-simulation. 2021 IEEE
International Intelligent Transportation Systems Conference
(ITSC) , pages 1155–1162, 2021. 6
[35] Runsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Liu,
and Jiaqi Ma. Opv2v: An open benchmark dataset and fu-
sion pipeline for perception with vehicle-to-vehicle commu-
nication. 2022 International Conference on Robotics and
Automation (ICRA) , pages 2583–2589, 2021. 1, 2, 6
[36] Runsheng Xu, Zhengzhong Tu, Hao Xiang, Wei Shao, Bolei
Zhou, and Jiaqi Ma. CoBEVT: Cooperative bird’s eye view
semantic segmentation with sparse transformers. CoRL ,
2022. 2
[37] Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, Ming-
Hsuan Yang, and Jiaqi Ma. V2X-ViT: Vehicle-to-everything
cooperative perception with vision transformer. ECCV ,
2022. 1, 2, 6
[38] Runsheng Xu, Jinlong Li, Xiaoyu Dong, Hongkai Yu, and Ji-
aqi Ma. Bridging the domain gap for multi-agent perception.
ICRA , 2023. 1
[39] Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang,
Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong,
Rui Song, Hongkai Yu, Bolei Zhou, and Jiaqi Ma. V2v4real:
A real-world large-scale dataset for vehicle-to-vehicle coop-
erative perception. In The IEEE/CVF Computer Vision and
Pattern Recognition Conference (CVPR) , 2023. 1
[40] Dingkang Yang, Kun Yang, Yuzheng Wang, Jing Liu, Zhi
Xu, Rongbin Yin, Peng Zhai, and Lihua Zhang. How2comm:Communication-efficient and collaboration-pragmatic multi-
agent perception. Advances in Neural Information Process-
ing Systems , 2023. 1
[41] Kun Yang, Dingkang Yang, Jingyu Zhang, Mingcheng Li,
Y . Liu, Jing Liu, Hanqi Wang, Peng Sun, and Liang Song.
Spatio-temporal domain awareness for multi-agent collabo-
rative perception. Proceedings of the 31st ACM International
Conference on Multimedia , 2023. 1
[42] Kun Yang, Dingkang Yang, Jingyu Zhang, Hanqi Wang,
Peng Sun, and Liang Song. What2comm: Towards
communication-efficient collaborative perception via feature
decoupling. Proceedings of the 31st ACM International Con-
ference on Multimedia , 2023. 1
[43] Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang,
Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui
Yuan, et al. DAIR-V2X: A large-scale dataset for vehicle-
infrastructure cooperative 3d object detection. In Proceed-
ings of the IEEE/CVF Conference on computer vision and
pattern recognition (CVPR) , 2022. 1, 2, 6
[44] Haibao Yu, Wenxian Yang, Hongzhi Ruan, Zhenwei Yang,
Yingjuan Tang, Xu Gao, Xin Hao, Yifeng Shi, Yifeng Pan,
Ning Sun, Juan Song, Jirui Yuan, Ping Luo, and Zaiqing
Nie. V2x-seq: A large-scale sequential dataset for vehicle-
infrastructure cooperative perception and forecasting. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , 2023. 2
[45] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb ¨uhl. Ob-
jects as points. In arXiv preprint arXiv:1904.07850 , 2019. 1,
6
15490
