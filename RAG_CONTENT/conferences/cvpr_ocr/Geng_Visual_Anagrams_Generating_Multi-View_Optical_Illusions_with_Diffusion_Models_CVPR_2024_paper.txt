Visual Anagrams: Generating Multi-View
Optical Illusions with Diffusion Models
Daniel Geng Inbum Park Andrew Owens
University of Michigan
https://dangeng.github.io/visual_anagrams/
a painting of 
waterfallsa painting 
of a rabbit
a painting of 
a teddy bearFlips“Polymorphic” Jigsaw PuzzlesInner Rotations
Three Views
Negatives
a painting of 
houseplantsa painting 
of einstein
a painting 
of elvisFour Views
an oil painting 
of a teddy bear
an oil painting 
of a giraffe
an oil painting 
of a rabbitan oil painting 
of a bird
a painting of 
houseplants
a painting of 
marilyn monroe
cf. “My Wife and My 
Mother-in-Law.”  1888.
an oil painting 
of a fruit bowl
an oil painting 
of a monkeycf. “Reversible Head with Basket of Fruit.”  (
Giuseppe Arcimboldo, 1590.
Skewscf. “Sky and Water I.”  
M. C. Escher, 1938.
Ambigrams
180°the word "happy", 
cursive writingthe word "holiday", 
cursive writing
a painting of 
kitchenware
a painting of 
a red panda
a lithograph 
of a fish
a lithograph 
of a duck
an oil painting 
of people at a 
campfire
an oil painting  
of an old man
a painting 
of vases
a painting 
of a sloth
a drawing of a 
giraffe
a drawing of a 
penguin
an oil painting 
of a ship
an oil painting 
of a bird
a painting of 
wine and cheese
a painting of 
a turtle
a photo of a 
young lady
a photo of 
an old woman
a pop art of 
albert einstein
a pop art of 
marilyn monroe
cf. “Hybrid Images.”  
Aude Oliva et al. 2006.
an oil painting of 
a tudor portraitan oil painting 
of a skullSkew
cf. “The Ambassadors.”  Hans Holbein, 1533.
an oil painting 
of a soldier
an oil painting 
of a houseplantSkew
a photo of a 
man
a photo of a 
woman
a lithograph 
of a landscapea lithograph of 
houseplants
a lithograph of 
a teddy beara lithograph 
of a rabbita watercolor 
of a kittena watercolor 
of a puppy
Figure 1. Generating Multi-View Illusions. We propose a method for generating optical illusions from an off-the-shelf text-to-image
diffusion model. We create images that match different prompts after undergoing a transformation. Our approach supports a variety of
transformations, including ﬂips, rotations, skews, color inversions, and jigsaw rearrangements. All images are hand selected. For random
samples, please see Fig. 8and Appendix D.For easier viewing, please see our webpage for animated versions of these illusions.
Abstract
We address the problem of synthesizing multi-view opti-
cal illusions: images that change appearance upon a trans-
formation, such as a ﬂip or rotation. We propose a simple,zero-shot method for obtaining these illusions from off-the-
shelf text-to-image diffusion models. During the reverse dif-
fusion process, we estimate the noise from different views
of a noisy image, and then combine these noise estimates
together and denoise the image. A theoretical analysis sug-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
24154
gests that this method works precisely for views that can
be written as orthogonal transformations, of which permu-
tations are a subset. This leads to the idea of a visual
anagram —an image that changes appearance under some
rearrangement of pixels. This includes rotations and ﬂips,
but also more exotic pixel permutations such as a jigsaw
rearrangement. Our approach also naturally extends to il-
lusions with more than two views. We provide both quali-
tative and quantitative results demonstrating the effective-
ness and ﬂexibility of our method. Please see our project
webpage for additional visualizations and results: https:
//dangeng.github.io/visual_anagrams/
1. Introduction
Images that change their appearance under a transforma-
tion, such as a rotation or a ﬂip, have long fascinated stu-
dents of perception, from Salvador Dal ´ı to M. C. Escher.
The appeal of these multi-view optical illusions lies partly
in the challenge of arranging visual elements such that
they may be understood in multiple different ways. Creat-
ing these illusions requires accurately modeling—and then
subverting—visual perception.
In this paper, we propose a simple, zero-shot method
for creating multi-view illusions with off-the-shelf text-
to-image diffusion models. In contrast to most previous
work on computationally generating optical illusions [ 3–
5,10,12,15,18,20,28,31,32,38], our method does
not require an explicit model of human perception. Rather,
our approach builds on work that suggests generative mod-
els may process optical illusions in a way similar to hu-
mans [ 14,23,29]. In this way, our method is similar to
recent work that uses diffusion models to create optical il-
lusions by Burgert et al.[2] and Tancik [ 42].
Our method can generate many types of classic illusions,
such as images that change appearance when ﬂipped or ro-
tated (Fig. 1), as well as a new class of illusions which we
term visual anagrams . These are images that change ap-
pearance under a permutation of their pixels. Image ﬂips
and rotations are a subset of these, as they can both be ex-
pressed as a permutation of pixels, but we also consider
more exotic permutations. For example, we generate jigsaw
puzzles that can be solved in two different ways, which we
call “polymorphic jigsaws.” In addition, we successfully ap-
ply our approach to generating illusions with three and four
views (Fig. 1).
Our method works by using a diffusion model to denoise
an image from multiple views, obtaining multiple noise es-
timates. These noise estimates are then combined to form a
single noise estimate which is used to perform a step in the
reverse diffusion process. However, we show that care must
be taken in choosing these views. For one, the transforma-
tion must keep the statistics of the noise intact, as the dif-
fusion model is trained under the assumption of i.i.d. Gaus-sian noise. We provide an analysis of these conditions and
give an exact speciﬁcation of the class of transformations
supported. Our contributions are as follows:
• We present a simple yet effective method for generating
multi-view optical illusions using diffusion models.
• We derive a precise description of the set of views that
our method supports and provide empirical evidence that
these views work.
• We consider practical design decisions, crucial to opti-
mizing the quality of generated illusions, and report abla-
tions on our choices.
• We provide quantitative and qualitative results, showcas-
ing both the efﬁcacy and ﬂexibility of our method.
2. Related Work
Diffusion Models. Diffusion models [ 6,17,22,35–37,
39–41] are a class of powerful generative models that itera-
tively convert a sample from a noise distribution to a sample
from some data distribution. These models work by esti-
mating the noise in a noisy sample, and removing the esti-
mated noise following some update rule such as DDPM [ 22]
or DDIM [ 40]. A prominent application of diffusion models
has been text-conditioned image synthesis [ 24,30,36,37].
In addition to a noisy image and a timestep, these models
take a language model embedding of a text prompt as condi-
tioning. Our approach is closely related to recent works that
experiment with composing energy-based models and dif-
fusion models [ 7–9,13,26,27]. These approaches [ 9,27]
have shown that noise estimates from multiple conditional
distributions can be combined together to obtain samples
from compositions of the learned distributions. Our method
uses a similar approach, and we apply it to the problem of
multi-view illusion generation.
Computational Optical Illusions. Optical illusions serve
as a testbed for understanding both human and machine per-
ception [ 14,19,23,29,45]. We focus on generating illu-
sions computationally, an area which has primarily relied
on models of how our brains process external stimuli. Free-
man et al.[12] create the illusion of constant motion in a de-
sired direction by locally applying a ﬁlter with continuously
shifting phase, relying on the observation that local phase-
shifts are interpreted as global movement. Oliva et al .[31]
propose a method to make “hybrid images,” which change
appearance depending on the distance they are viewed from.
This method takes advantage of the multiscale nature of hu-
man perception by blending high frequencies of one image
with low frequencies from another. Chu et al .[5] camou-
ﬂage objects in a scene through re-texturing, with additional
constraints on luminance as to preserve salient features of
the object, and other work camouﬂages objects from multi-
ple viewpoints in 3D scenes [ 18,32]. Recently, Chandra et
al.[3] design color-constancy, size constancy, and face
24155
perception illusions by differentiating through a Bayesian
model of human vision. Our method likewise generates il-
lusions, but does not depend on an explicit model of human
perception. Instead, our method works by leveraging visual
priors in diffusion models learned implicitly through data.
This aligns with observations [ 14,23,29] that generative
models process illusions similarly to humans, and predict
the same ambiguities. From this perspective, we can view
our method as leveraging generative, rather than discrimina-
tive, models to synthesize adversarial examples [ 16] against
humans [ 11].
Illusions with Diffusion Models. Very recently, artists
and researchers have taken steps that show the potential
of using diffusion models to create illusions. An artist un-
der the pseudonym MrUgleh [ 43] repurposed a model ﬁne-
tuned for generating QR codes [ 25,47] to create images
whose global structure subtly matches a given template im-
age. In contrast, we study multi-view illusions that can be
created zero-shot from off-the-shelf diffusion models, and
our illusions are speciﬁed via text rather than images. Burg-
ertet al .[2] use score distillation sampling (SDS) [ 33,44]
to create images that align with different prompts from dif-
ferent views. While in principle this approach supports a
superset of our views, the use of SDS results in signiﬁcantly
lower quality results, and the need for explicit optimization
leads to long sampling times. Our method is most similar to
a proof-of-concept by Tancik [ 42], which creates rotation
illusions by sampling from a latent diffusion model [ 36]
while alternating noise estimates between different views
and prompts. While our technical approach is similar, by
contrast we systematically study multi-view illusions, both
by experimentally evaluating many different types of illu-
sions and by providing a theoretical analysis of which views
are (and are not) supported. In doing so, we go beyond just
rotation views. We also make a number of improvements
that result in qualitatively and quantitatively better illusions,
such as by identifying a source of artifacts from latent dif-
fusion, and by adding support for an arbitrary number of
views. To our knowledge, we are the ﬁrst to systematically
evaluate illusions generated by these approaches.
3. Method
Our goal is to produce multi-view optical illusions using
a pretrained diffusion model. That is, we seek to synthe-
size images that change appearance or identity when trans-
formed, such as when ﬂipped or rotated.
3.1. Text-conditioned Diffusion Models
Diffusion models [ 22,39,41] take i.i.d. Gaussian noise,
xT, and iteratively denoise it to produce a sample, x0,
from some data distribution. These models are parame-
terized by a neural network which estimates the noise inDiffusion 
Model
Diffusion 
Model
<latexit sha1_base64="7/pfzAjCi55rJc1nxhxjeMaBX/Q=">AAAB+HicbVDLSsNAFJ3UV62PRl26GSyCq5KIr41QdOOygn1AG8JkMm2HTiZh5qZYQ7/EjQtF3Pop7vwbp20W2nrgwuGce7n3niARXIPjfFuFldW19Y3iZmlre2e3bO/tN3WcKsoaNBaxagdEM8ElawAHwdqJYiQKBGsFw9up3xoxpXksH2CcMC8ifcl7nBIwkm+XR76Lr3EX2CNkPJz4dsWpOjPgZeLmpIJy1H37qxvGNI2YBCqI1h3XScDLiAJOBZuUuqlmCaFD0mcdQyWJmPay2eETfGyUEPdiZUoCnqm/JzISaT2OAtMZERjoRW8q/ud1UuhdeRmXSQpM0vmiXiowxHiaAg65YhTE2BBCFTe3YjogilAwWZVMCO7iy8ukeVp1L6rn92eV2k0eRxEdoiN0glx0iWroDtVRA1GUomf0it6sJ+vFerc+5q0FK585QH9gff4ABfOStA==</latexit>v1=i d
<latexit sha1_base64="XUTmavTxhrYgJQ2jkrc/UcGqz3Q=">AAAB+nicbVDLSsNAFJ3UV62vVJduBovgqiTF10YounFZwT6gDWUynbRDJ5Mwc1MtsZ/ixoUibv0Sd/6N0zYLbT1w4XDOvdx7jx8LrsFxvq3cyura+kZ+s7C1vbO7Zxf3GzpKFGV1GolItXyimeCS1YGDYK1YMRL6gjX94c3Ub46Y0jyS9zCOmReSvuQBpwSM1LWLo24FX+EOsEdIA8HjSdcuOWVnBrxM3IyUUIZa1/7q9CKahEwCFUTrtuvE4KVEAaeCTQqdRLOY0CHps7ahkoRMe+ns9Ak+NkoPB5EyJQHP1N8TKQm1Hoe+6QwJDPSiNxX/89oJBJdeymWcAJN0vihIBIYIT3PAPa4YBTE2hFDFza2YDogiFExaBROCu/jyMmlUyu55+ezutFS9zuLIo0N0hE6Qiy5QFd2iGqojih7QM3pFb9aT9WK9Wx/z1pyVzRygP7A+fwCtcpOn</latexit>v2=ﬂ i p
an oil painting of 
people at a campfire
an oil painting of 
an old man<latexit sha1_base64="Pg+YjzBd2Lp9F8Gpq72kug91mDk=">AAAB83icbVDLSgMxFM3UV62vqks3wSK4KjPia1l047KCfUBnLJn0tg3NJCHJCGXob7hxoYhbf8adf2PazkJbD1w4nHMv994TK86M9f1vr7Cyura+UdwsbW3v7O6V9w+aRqaaQoNKLnU7JgY4E9CwzHJoKw0kiTm04tHt1G89gTZMigc7VhAlZCBYn1FinRSGoAzjUjwGXdstV/yqPwNeJkFOKihHvVv+CnuSpgkISzkxphP4ykYZ0ZZRDpNSmBpQhI7IADqOCpKAibLZzRN84pQe7kvtSlg8U39PZCQxZpzErjMhdmgWvan4n9dJbf86yphQqQVB54v6KcdW4mkAuMc0UMvHjhCqmbsV0yHRhFoXU8mFECy+vEyaZ9Xgsnpxf16p3eRxFNEROkanKEBXqIbuUB01EEUKPaNX9Oal3ov37n3MWwtePnOI/sD7/AELzZG0</latexit>/epsilon11
t
<latexit sha1_base64="NJnUwCbGKesjxqKJlxFHwwqLwsg=">AAAB83icbVDLSgMxFM34rPVVdekmWARXZab4WhbduKxgH9AZSya904ZmkpBkhFL6G25cKOLWn3Hn35i2s9DWAxcO59zLvffEijNjff/bW1ldW9/YLGwVt3d29/ZLB4dNIzNNoUEll7odEwOcCWhYZjm0lQaSxhxa8fB26reeQBsmxYMdKYhS0hcsYZRYJ4UhKMO4FI/Vru2Wyn7FnwEvkyAnZZSj3i19hT1JsxSEpZwY0wl8ZaMx0ZZRDpNimBlQhA5JHzqOCpKCicazmyf41Ck9nEjtSlg8U39PjElqzCiNXWdK7MAselPxP6+T2eQ6GjOhMguCzhclGcdW4mkAuMc0UMtHjhCqmbsV0wHRhFoXU9GFECy+vEya1UpwWbm4Py/XbvI4CugYnaAzFKArVEN3qI4aiCKFntErevMy78V79z7mrStePnOE/sD7/AENU5G1</latexit>/epsilon12
tAvg.<latexit sha1_base64="72ptzsjmGgGW72mpACwhPsdTFyE=">AAAB73icbVDJSgNBEK1xjXGLevTSGAQvhhlxOwa9eIxgFkjG0NPpSZr09IzdNYEw5Ce8eFDEq7/jzb+xsxw08UHB470qquoFiRQGXffbWVpeWV1bz23kN7e2d3YLe/s1E6ea8SqLZawbATVcCsWrKFDyRqI5jQLJ60H/duzXB1wbEasHHCbcj2hXiVAwilZqDNreY3bqjdqFoltyJyCLxJuRIsxQaRe+Wp2YpRFXyCQ1pum5CfoZ1SiY5KN8KzU8oaxPu7xpqaIRN342uXdEjq3SIWGsbSkkE/X3REYjY4ZRYDsjij0z743F/7xmiuG1nwmVpMgVmy4KU0kwJuPnSUdozlAOLaFMC3srYT2qKUMbUd6G4M2/vEhqZyXvsnRxf14s38ziyMEhHMEJeHAFZbiDClSBgYRneIU358l5cd6dj2nrkjObOYA/cD5/AGIPj44=</latexit>v−1
1
<latexit sha1_base64="7fGUNfuAWgcs0cw/HiAvHF/vD3w=">AAAB73icbVDLTgJBEOzFF+IL9ehlIjHxItklvo5ELx4xkUcCK5kdZmHCzOw6M0tCNvyEFw8a49Xf8ebfOMAeFKykk0pVd7q7gpgzbVz328mtrK6tb+Q3C1vbO7t7xf2Dho4SRWidRDxSrQBrypmkdcMMp61YUSwCTpvB8HbqN0dUaRbJBzOOqS9wX7KQEWys1Bp1K4/pmTfpFktu2Z0BLRMvIyXIUOsWvzq9iCSCSkM41rrtubHxU6wMI5xOCp1E0xiTIe7TtqUSC6r9dHbvBJ1YpYfCSNmSBs3U3xMpFlqPRWA7BTYDvehNxf+8dmLCaz9lMk4MlWS+KEw4MhGaPo96TFFi+NgSTBSztyIywAoTYyMq2BC8xZeXSaNS9i7LF/fnpepNFkcejuAYTsGDK6jCHdSgDgQ4PMMrvDlPzovz7nzMW3NONnMIf+B8/gBjmI+P</latexit>v−1
2<latexit sha1_base64="W20EH+DopBjKIwkiea4YmOygIo4=">AAAB+XicbVDJSgNBEO2JW4zbqEcvjUHwFGbE7Rj04jGCWSAzhJ6eStKkZ6G7JhCG/IkXD4p49U+8+Td2kjlo4oOCx3tVVNULUik0Os63VVpb39jcKm9Xdnb39g/sw6OWTjLFockTmahOwDRIEUMTBUropApYFEhoB6P7md8eg9IiiZ9wkoIfsUEs+oIzNFLPtj0UMgQPUi2kEbBnV52aMwddJW5BqqRAo2d/eWHCswhi5JJp3XWdFP2cKRRcwrTiZRpSxkdsAF1DYxaB9vP55VN6ZpSQ9hNlKkY6V39P5CzSehIFpjNiONTL3kz8z+tm2L/1cxGnGULMF4v6maSY0FkMNBQKOMqJIYwrYW6lfMgU42jCqpgQ3OWXV0nrouZe164eL6v1uyKOMjkhp+ScuOSG1MkDaZAm4WRMnskrebNy68V6tz4WrSWrmDkmf2B9/gAEBJPs</latexit>˜/epsilon1t
<latexit sha1_base64="im1e/4T9eS0sJniQeODaVSCpcus=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiTia1l047KCfUATymQ6aYdOHszciCX0N9y4UMStP+POv3HSZqGtBwYO59zLPXP8RAqNtv1tlVZW19Y3ypuVre2d3b3q/kFbx6livMViGauuTzWXIuItFCh5N1Gchr7kHX98m/udR660iKMHnCTcC+kwEoFgFI3kuiHFkR9kT9M+9qs1u27PQJaJU5AaFGj2q1/uIGZpyCNkkmrdc+wEvYwqFEzyacVNNU8oG9Mh7xka0ZBrL5tlnpITowxIECvzIiQz9fdGRkOtJ6FvJvOMetHLxf+8XorBtZeJKEmRR2x+KEglwZjkBZCBUJyhnBhCmRImK2EjqihDU1PFlOAsfnmZtM/qzmX94v681rgp6ijDERzDKThwBQ24gya0gEECz/AKb1ZqvVjv1sd8tGQVO4fwB9bnD5Mskg0=</latexit>xtFigure 2. Algorithm Overview. Our method works by simul-
taneously denoising multiple views of an image. Given a noisy
image xt, we compute noise estimates, /epsilon1i
t, conditioned on differ-
ent prompts, after applying views vi. We then apply the inverse
view v−1
ito align estimates, average the estimates, and perform a
reverse diffusion step. The ﬁnal output is an optical illusion.
some intermediate, partially denoised data point xt, denoted
as/epsilon1θ(xt,y ,t), where yis some conditioning such as text
prompts and tis the timestep in the diffusion process. The
estimated noise is then used in an update rule [ 22,40], from
which xt−1is computed from xt.
To condition the diffusion model on another input, such
as a text prompt, a common approach is to use classiﬁer-free
guidance [ 21]. With this method, unconditional noise esti-
mates (usually obtained by passing the null text prompt as
conditioning) and conditional noise estimates are combined
together:
/epsilon1CFG
t=/epsilon1θ(xt,t ,∅)+γ(/epsilon1θ(xt,t ,y)−/epsilon1θ(xt,t ,∅)).(1)
Here, ∅denotes the embedding of the empty string and γ
is a parameter that controls the strength of the guidance.
Classiﬁer-free guidance acts to sharpen the distribution of
generated images to produce higher quality results. It also
enables negative prompting [1], in which the empty text
prompt embedding, ∅, is replaced by a text prompt that we
would like to discourage the model from generating.
3.2. Parallel Denoising
We produce multi-view illusions by using a diffusion model
to simultaneously denoise multiple views of an image. Con-
cretely, we take a set of Nprompts, yi, each associated
with a view function vi(·), which applies a transformation
to an image. These transformations may be, for example,
the identity function, an image ﬂip, or a permutation of pix-
els. Then given a diffusion model, /epsilon1θ(·), and a partially
denoised image, xt, we combine noise estimates from dif-
ferent views into a single noise estimate by averaging:
˜/epsilon1t=1
N/summationdisplay
iv−1
i(/epsilon1θ(vi(xt),yi,t)). (2)
Effectively, we use each view vito transform the noisy im-
agext, estimate the noise in the transformed images, and
then apply v−1
ito the estimates in order to transform them
back to the original view. Taking an average of these noise
24156
estimates gives us our combined noise estimate, which we
can then use with our choice of diffusion sampler. We note
that this technique of combining noise estimates is similar to
previous work on compositionality [ 7–9,13,26,27], where
the idea is studied in further detail. In order to incorpo-
rate classiﬁer-free guidance we simply replace the estimates
/epsilon1θ(vi(xt),yi,t)with their classiﬁer-free estimates, /epsilon1CFG
t.
3.3. Conditions on Views
One straightforward condition for the views is that they
must be invertible. But diffusion models also implicitly im-
pose other conditions on the views vi(·). We describe two
such conditions below. We ﬁnd that if these conditions are
not satisﬁed, the denoising process produces poor results.
Linearity. The diffusion model, /epsilon1θ, acts on noisy images,
xt. That is, speciﬁcally images of the form:
xt=wsignal
t x0/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
signal+wnoise
t /epsilon1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
noise. (3)
The exact values of wsignal
t andwnoise
tdepend on model im-
plementation details such as the variance schedule, but are
unimportant for our work, so we omit them for clarity. What
is important is that xtis a linear combination of pure sig-
nal, x0, and pure noise, /epsilon1, for some speciﬁc wsignal
t and
wnoise
t. Therefore our view vimust take a noisy image xt
and transform it into a new noisy image vi(xt)that is also
a linear combination of pure signal and pure noise with the
same weighting . This can be achieved by requiring vito be
a linear transformation, of the form
vi(xt)=Aixt, (4)
for some matrix Ai, and some ﬂattened noisy image xt.
By linearity, we are effectively applying the view vito the
signal and the noise separately:
vi(xt)=Ai(wsignal
tx0+wnoise
t/epsilon1) (5)
=wsignal
t Aix0/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
new signal+wnoise
t Ai/epsilon1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
new noise. (6)
This results in a linear combination of transformed signal,
Aix0, and transformed noise, Ai/epsilon1, weighted with the cor-
rect scaling factors. For further discussion, please see Ap-
pendix H.
Statistical Consistency. In addition to expecting a linear
combination of signal and noise at a speciﬁc weighting, the
diffusion model also expects the noise to have a precise dis-
tribution. In particular, most diffusion networks are trained
with /epsilon1∼N(0,I). Therefore, we must ensure that our trans-
formed noise, Ai/epsilon1, is likewise drawn from N(0,I). This is
true if and only if Aiis an orthogonal matrix. We pro-
vide a proof in Appendix I, but intuitively this fact reﬂects
the spherical symmetry of the standard Gaussian density.
Orthogonal transformations, being generalizations of rota-tions and ﬂips to higher dimensions, preserve this spheri-
cally symmetric density. Note that these are rotations in the
pixel values as opposed to spatial rotations.
3.4. Views Considered
The vast majority of orthogonal transformations applied to
an image will not correspond to an intuitive image trans-
formation. However, a number of these transformations
do. Below, we enumerate the orthogonal transformations
which we consider, all of which can be seen in the illusions
in Fig. 1unless otherwise speciﬁed.
Identity. The simplest transformation we consider is the
identity transformation. Using this view allows us to op-
timize the untransformed image to align with a chosen
prompt.
Standard Image Manipulations. We also consider spa-
tial rotations of an image, which can be viewed as permuta-
tions of pixels. This works because permutations are in turn
orthogonal. However, caution must be exercised when ap-
plying a rotation view, as common anti-aliasing operations
such as bilinear sampling will modify the statistics of the
noise. We discuss this further in Sec. 4.4.Spatial reﬂec-
tions are also permutations of pixels. As such we can use
these views to generate illusions. Finally, we implement an
approximation to skewing by rolling columns of pixels by
different displacements.
General Permutations. We have already considered the
special cases of spatial rotation, reﬂection, and skews but
we can also consider other permutations. For example, we
can divide an image into jigsaw pieces and rearrange these
pieces to generate jigsaw puzzles with two solutions—what
we call “polymorphic” jigsaw puzzles . Implementation
details can be found in Appendix F.
We also consider the extreme case of sampling a com-
pletely random permutation of pixels and treating it as our
view. Additionally, we can reduce the complexity of this by
considering permutations of square patches , rather than
pixels. Examples of these illusions can be found in Fig. 6
and are discussed in Sec. 4.3.
Finally, we consider rotating a circle within an image
while leaving the rest of the image stationary, which we
term inner rotations . Note that the permutations we con-
sider are certainly not exhaustive, and many clever transfor-
mations exist which we do not study.
Color Inversion. Negation is an orthogonal transform; it
is intuitively a 180 degree rotation generalized to higher di-
mensions. This allows us to generate illusions that change
appearance upon color inversion, assuming pixel values are
centered at 0 ( e.g., in the range [-1, 1]).
24157
a cartoon sketch 
of a bird
a cartoon sketch 
of a frog
Figure 3. Latent-Based Artifacts. Manipulating the location
of latent codes does not change the orientation of the blocks for
which they encode. Therefore, when using latent diffusion mod-
els we see artifacts as shown above, in which straight lines are
thatched under a rotation.
Arbitrary Orthogonal Transformations. An arbitrary
rotation of an image in pixel space is uninterpretable. Nev-
ertheless, we demonstrate that our method works for these
transformations as well. While these “illusions” are in-
scrutable to the human eye, they serve as conﬁrmation that
any orthogonal transformation works as a view with our
method. These can be found in Fig. 7and are discussed
in Sec. 4.3.
3.5. Design Decisions
Beyond the core method, we also consider various design
decisions aimed at maximizing illusion quality.
Pixel Diffusion Model. Previous work [ 42] performed
multi-view denoising using Stable Diffusion [ 36], a latent
diffusion model. However, the latent representation effec-
tively encodes patches of pixels. This leads to artifacts un-
der rotations or ﬂips, where the location of latents change,
but the content and orientation of these blocks do not.
We show a qualitative example of this in Fig. 3, in which
the model is forced to generate thatched lines to produce
straight lines under a 90◦rotation.
To ameliorate this issue, we implement our method using
a pixel-based diffusion model, DeepFloyd IF [ 24]. Deep-
Floyd denoises directly on pixels, effectively side-stepping
the problem of orientation in latent code blocks.
Combining Noise Estimates. In addition to taking a
mean of noise estimates from different views, we also con-
sider alternating through them by timestep, using the esti-
mate
˜/epsilon1t=v−1
tmod N(/epsilon1θ(vtmod N(xt),t ,y)). (7)
This is the reduction strategy used by [ 42], but we show in
ablations in Sec. 4.2that it performs worse than averaging.
Negative Prompting. We experiment with negative
prompting [ 1] in the 2-view case by using one view’s
prompt as a negative for the other view, and vice versa.
This encourages the model to hide the other view’s prompt
for a given view. For a discussion, please see the ablations
in Sec. 4.2.Table 1. Quantitative Results. We report the alignment score, A,
and the concealment score, C, as well as quantiles of these scores.
For a discussion, please see Sec. 4.1.
Prompt Pair Method A↑A0.9↑A0.95↑C↑C0.9↑C0.95↑
CIFARBurgert et al.[2] 0.225 0.253 0.260 0.501 0.526 0.537
Tancik [ 42] 0.278 0.310 0.316 0.595 0.692 0.712
Ours 0.287 0.321 0.327 0.624 0.717 0.739
OursBurgert et al.[2] 0.233 0.270 0.283 0.501 0.526 0.538
Tancik [ 42] 0.256 0.294 0.309 0.545 0.621 0.655
Ours 0.275 0.315 0.326 0.574 0.668 0.694
Table 2. Ablations. We ablate negative prompting, reduction
methods, and guidance scales on our dataset.
Ablation A↑A0.9↑A0.95↑C↑C0.9↑C0.95↑
Negative Prompting 0.24 0.27 0.276 0.576 0.659 0.683
No Negative Prompting 0.255 0.285 0.295 0.567 0.643 0.679
Alternating Reduction 0.252 0.286 0.292 0.560 0.639 0.664
Mean Reduction 0.255 0.285 0.295 0.567 0.643 0.679
γ=3.0 0.239 0.271 0.285 0.537 0.610 0.629
γ=7.0 0.255 0.285 0.295 0.567 0.643 0.679
γ= 10 .0 0.259 0.290 0.297 0.576 0.664 0.702
4. Results
We provide quantitative and qualitative results, and quan-
titative ablations. If not speciﬁed, qualitative results have
been hand picked for quality. For random samples please
see Fig. 8and Appendix D. All implementation details can
be found in Appendix A.
4.1. Quantitative Results
Metrics. We use CLIP [ 34] to measure how well views
align with the desired prompts. We consider two metrics
derived from a score matrix S∈RN×N, deﬁned as
Sij=φimg(vi(x))Tφtext(pj), (8)
where φimgandφtextare the CLIP visual and textual en-
coders respectively, returning a unit-norm vector embed-
ding. xis our generated illusion, and viare our views
with associated prompts pi. A higher dot product indicates
higher similarity between the image and text.
The ﬁrst metric we consider is min diag( S), which intu-
itively measures the worst alignment of all the views. We
term this metric A, the alignment score . However, this
metric does not account for the possibility of seeing prompt
piin view vjfori/negationslash=j. This is an occasional failure case
of our method and to quantify this we propose a second
derived metric which we term C, the concealment score ,
computed as
1
Ntr(softmax( S/τ)), (9)
where τis the temperature parameter of CLIP. In computing
this metric we average both directions of the softmax, so
that this metric measures how well CLIP can classify a view
24158
Burgert et al.TancikOursOursCIFAR
UnﬂippedUnﬂippedFlippedFlippedFigure 4. Flip View CLIP Score Distribution. We visualize
trade-offs between ﬂipped and unﬂipped views by plotting the dis-
tribution of CLIP scores on the datasets. Note that the quality of
the ﬂipped image is as good as the unﬂipped image, with parity
indicated by the dashed line.
as one of the Nprompts and vice versa.
Dataset. To evaluate our method and baselines we com-
pile two datasets of prompt pairs for 2-view illusions. One
dataset uses the 10 classes from CIFAR-10 and contains a
prompt per pair of classes, for a total of 45 prompt pairs.
We refer to this as CIFAR . The other dataset we compile
by hand, with the process documented in Appendix B. This
dataset consists of 50 prompt pairs, which we refer to as
Ours .
Baselines. We use two baselines that generate illusions
using off-the-shelf diffusion models. One, which we denote
“Burgert et al .[2],” uses Score Distillation Sampling. The
other, which we denote “Tancik [ 42],” is an earlier version
of our method, with differences discussed in more detail
in Sec. 2
Results. We show results comparing our method to base-
lines on both datasets in Tab. 1using vertical ﬂips. We use
vertical ﬂips because it is a transformation supported by our
method as well as the baselines. We use 10 samples per
prompt, for a total of 450 and 500 samples for the CIFAR
dataset and our dataset respectively. It is hard to perform a
fair comparison with more samples because the Burgert et
al. method uses SDS, which is quite slow1. Because we are
particularly interested in the “best-case” performance, we
also report quantiles of metrics, which we denote as A0.9
for the 90th percentile, for example. As can be seen, our
method performs consistently better than the baselines, in
both the alignment score and the concealment score.
In order to give a clearer understanding of trade-offs
when optimizing two views, we show density plots which
plot the CLIP scores of each of the two views of an illusion
in Fig. 4. As can be seen, we do better than the baselines
1Sampling just 10 images per prompt already takes more than a week
of GPU-hours.Burgert et al. Tancik Ours Ours (Flipped)
an ink drawing 
of a housean ink drawing 
of a castle
a watercolor 
painting 
of an owla watercolor 
painting 
of a dog
a street art 
of a rabbita street art 
of a violinOursa painting  
of a trucka painting  
of a deera painting 
of a horsea painting 
of a birda painting 
of a doga painting  
of an airplaneCIFAR
Figure 5. Qualitative Comparisons. We compare illusions gen-
erated by baselines to our illusions. We show examples from both
our prompt dataset and the CIFAR prompt dataset.
on average and in the best-case. Moreover, ﬂipping during
denoising does not hurt performance. The quality of the
ﬂipped images is as high as the unﬂipped images.
4.2. Ablations
We ablate out the noise estimate reduction strategy, negative
prompting, and the guidance scale in Tab. 2. We use our
dataset, with 10 samples for each prompt for a total of 500
illusions.
Reduction Strategy. We ﬁnd that mean reduction does
better than alternating. Our hypothesis is that alternating
the noise estimates results in “thrashing,” causing poor con-
vergence. Moreover, we ﬁnd that the alternating strategy
gives poor results on illusions with more than 2 views, as
each view has fewer denoising steps. Qualitative examples
of this can be found in Appendix G.
Negative Prompting. When using negative prompting,
care must be taken to omit any overlap between the negative
and positive prompt. For example, given the two prompts
"oil painting of a dog" and"oil painting
of a cat" , using one prompt as the negative for the other
would simultaneously encourage and discourage the style
"oil painting" . Rather, the negative prompts should
be"a cat" and"a dog" respectively. We ﬁnd that neg-
ative prompting can improve the concealment score, indi-
24159
an oil painting 
of a young man
an oil painting 
of an old man
a pencil sketch 
of a lemura pencil sketch 
of a kangaroo
a mosaic of  
a ducka mosaic of  
a rabbit
<latexit sha1_base64="hm5dP47wWBxXWFRBYgMt2/zfyMQ=">AAAB8nicbVBNS8NAEJ34WetX1aOXxSJ4KonU6rHoxWMF+wFpKJvtpl26yYbdiVBKf4YXD4p49dd489+4bXPQ1gcDj/dmmJkXplIYdN1vZ219Y3Nru7BT3N3bPzgsHR23jMo0402mpNKdkBouRcKbKFDyTqo5jUPJ2+Hobua3n7g2QiWPOE55ENNBIiLBKFrJr1W7KGJuSK3aK5XdijsHWSVeTsqQo9ErfXX7imUxT5BJaozvuSkGE6pRMMmnxW5meErZiA64b2lC7Z5gMj95Ss6t0ieR0rYSJHP198SExsaM49B2xhSHZtmbif95fobRTTARSZohT9hiUZRJgorM/id9oTlDObaEMi3srYQNqaYMbUpFG4K3/PIqaV1WvFrl6qFart/mcRTgFM7gAjy4hjrcQwOawEDBM7zCm4POi/PufCxa15x85gT+wPn8Ae/zkGQ=</latexit>64×64
<latexit sha1_base64="xbLDfzgSzVk41wFPGL7ElByXmkM=">AAAB8nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEq8eiF48V7AekoWy2m3bpZhN2J0Ip/RlePCji1V/jzX/jts1BWx8MPN6bYWZemEph0HW/ncLa+sbmVnG7tLO7t39QPjxqmSTTjDdZIhPdCanhUijeRIGSd1LNaRxK3g5HdzO//cS1EYl6xHHKg5gOlIgEo2gl36t1UcTcEK/WK1fcqjsHWSVeTiqQo9Erf3X7CctirpBJaozvuSkGE6pRMMmnpW5meErZiA64b6midk8wmZ88JWdW6ZMo0bYUkrn6e2JCY2PGcWg7Y4pDs+zNxP88P8PoJpgIlWbIFVssijJJMCGz/0lfaM5Qji2hTAt7K2FDqilDm1LJhuAtv7xKWhdVr1a9eris1G/zOIpwAqdwDh5cQx3uoQFNYJDAM7zCm4POi/PufCxaC04+cwx/4Hz+AOa2kF4=</latexit>16×16
<latexit sha1_base64="XS1ECujoW4Iskkd9FAiBdVvpFOE=">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKjxyDXjxGMA9JljA7mU2GzM4uM71CWPIVXjwo4tXP8ebfOEn2oNGChqKqm+6uIJHCoOt+OYWV1bX1jeJmaWt7Z3evvH/QMnGqGW+yWMa6E1DDpVC8iQIl7ySa0yiQvB2Mb2Z++5FrI2J1j5OE+xEdKhEKRtFKD7UeiogbUuuXK27VnYP8JV5OKpCj0S9/9gYxSyOukElqTNdzE/QzqlEwyaelXmp4QtmYDnnXUkXtGj+bHzwlJ1YZkDDWthSSufpzIqORMZMosJ0RxZFZ9mbif143xbDmZ0IlKXLFFovCVBKMyex7MhCaM5QTSyjTwt5K2IhqytBmVLIheMsv/yWts6p3Wb24O6/Ur/M4inAEx3AKHlxBHW6hAU1gEMETvMCro51n5815X7QWnHzmEH7B+fgGCXWP7A==</latexit>8×8
cf. “Kaninchen und Ente.” Fliegende Blätter , 1892.
a watercolor 
of a rabbita watercolor 
of a duck
an oil painting 
of a young manan oil painting 
of an old man
<latexit sha1_base64="ktc58PAP07iDpoCY6OtWXKm3xH0=">AAAB8nicbVDLSgNBEJyNrxhfUY9eBoPgKewmvo5BLx4jmAdsljA7mU2GzM4sM71CWPIZXjwo4tWv8ebfOEn2oIkFDUVVN91dYSK4Adf9dgpr6xubW8Xt0s7u3v5B+fCobVSqKWtRJZTuhsQwwSVrAQfBuolmJA4F64Tju5nfeWLacCUfYZKwICZDySNOCVjJr9d6wGNmcL3WL1fcqjsHXiVeTiooR7Nf/uoNFE1jJoEKYozvuQkEGdHAqWDTUi81LCF0TIbMt1QSuyfI5idP8ZlVBjhS2pYEPFd/T2QkNmYSh7YzJjAyy95M/M/zU4hugozLJAUm6WJRlAoMCs/+xwOuGQUxsYRQze2tmI6IJhRsSiUbgrf88ipp16reVfXy4aLSuM3jKKITdIrOkYeuUQPdoyZqIYoUekav6M0B58V5dz4WrQUnnzlGf+B8/gDgmJBa</latexit>32×32Figure 6. Permutation Illusions. We synthesize images whose
appearance changes upon permutation of patches. Even in the dif-
ﬁcult case of a 64×64grid of patches, in which every pixel is
effectively shufﬂed, we are able to generate meaningful images.
cating that it is working as intended. But this comes at the
cost of worse alignment score. This is because the neg-
ative and positive prompt may have fundamental similari-
ties. For example, using "a cat" as the negative prompt
for the prompt "an oil painting of a dog" may
discourage the model from synthesizing anything remotely
cat-like—such as fur, four legs, or a tail—even if it helps in
producing a dog. For this reason we opt not to use negative
prompting with our method.
Guidance Scale. We also ablate out various guidance
scales, γ, for our method. We ﬁnd that a higher guidance
scale tends to do better. This is presumably because a higher
guidance scale results in a sharper sampling distribution.
4.3. Qualitative Results
We show qualitative results in Fig. 1, Fig. 5, Fig. 6, and
Fig. 7. Again, random samples may be found in Fig. 8
and Appendix D. Additional qualitative samples can be
found in Appendix C. Overall, we ﬁnd that our method
can produce very high quality optical illusions for a wide
range of views. Interestingly, our method often ﬁnds
clever ways of reusing elements from one view for another,
such as in the "waterfalls"/"rabbit"/"teddy
bear" three-view illusion in Fig. 1, in which the nose of
the teddy bear is the eye of the rabbit, and a rock on the
waterfalls.
Baselines. We provide qualitative comparisons of our
method to baselines in Fig. 5, where we pick the best im-
ages out of 100 samples for each method. As can be seen,
<latexit sha1_base64="M2GCvy+2YqBjcWtHuxXEIFqnNPo=">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPia1l147KCfcB0KJk004ZmkiG5I5Shn+HGhSJu/Rp3/o2ZdhbaeiBwOOdecu4JE8ENuO63U1pZXVvfKG9WtrZ3dveq+wdto1JNWYsqoXQ3JIYJLlkLOAjWTTQjcShYJxzf5X7niWnDlXyEScKCmAwljzglYCW/FxMYhVF2M630qzW37s6Al4lXkBoq0OxXv3oDRdOYSaCCGON7bgJBRjRwKti00ksNSwgdkyHzLZUkZibIZpGn+MQqAxwpbZ8EPFN/b2QkNmYSh3Yyj2gWvVz8z/NTiK6DjMskBSbp/KMoFRgUzu/HA64ZBTGxhFDNbVZMR0QTCralvARv8eRl0j6re5f1i4fzWuO2qKOMjtAxOkUeukINdI+aqIUoUugZvaI3B5wX5935mI+WnGLnEP2B8/kD5QuRAw==</latexit>A
<latexit sha1_base64="HLO9iZ2r1nrevDP92QrFWXs4ZuY=">AAAB+HicbVC7TsMwFL0pr1IeDTCyWFRILFQJ4jUWWBiLRB9SGyrHdVqrjhPZDlKJ8iUsDCDEyqew8Tc4bQdoOZKlo3Pu1T0+fsyZ0o7zbRWWlldW14rrpY3Nre2yvbPbVFEiCW2QiEey7WNFORO0oZnmtB1LikOf05Y/usn91iOVikXiXo9j6oV4IFjACNZG6tnlboj10A/Sq+whPXaznl1xqs4EaJG4M1KBGeo9+6vbj0gSUqEJx0p1XCfWXoqlZoTTrNRNFI0xGeEB7RgqcEiVl06CZ+jQKH0URNI8odFE/b2R4lCpceibyTymmvdy8T+vk+jg0kuZiBNNBZkeChKOdITyFlCfSUo0HxuCiWQmKyJDLDHRpquSKcGd//IiaZ5U3fPq2d1ppXY9q6MI+3AAR+DCBdTgFurQAAIJPMMrvFlP1ov1bn1MRwvWbGcP/sD6/AGKIJMG</latexit>A−1an oil painting 
of an old womanan oil painting 
of a young ladya photo of a rabbita photo of a duck<latexit sha1_base64="M2GCvy+2YqBjcWtHuxXEIFqnNPo=">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPia1l147KCfcB0KJk004ZmkiG5I5Shn+HGhSJu/Rp3/o2ZdhbaeiBwOOdecu4JE8ENuO63U1pZXVvfKG9WtrZ3dveq+wdto1JNWYsqoXQ3JIYJLlkLOAjWTTQjcShYJxzf5X7niWnDlXyEScKCmAwljzglYCW/FxMYhVF2M630qzW37s6Al4lXkBoq0OxXv3oDRdOYSaCCGON7bgJBRjRwKti00ksNSwgdkyHzLZUkZibIZpGn+MQqAxwpbZ8EPFN/b2QkNmYSh3Yyj2gWvVz8z/NTiK6DjMskBSbp/KMoFRgUzu/HA64ZBTGxhFDNbVZMR0QTCralvARv8eRl0j6re5f1i4fzWuO2qKOMjtAxOkUeukINdI+aqIUoUugZvaI3B5wX5935mI+WnGLnEP2B8/kD5QuRAw==</latexit>A
<latexit sha1_base64="HLO9iZ2r1nrevDP92QrFWXs4ZuY=">AAAB+HicbVC7TsMwFL0pr1IeDTCyWFRILFQJ4jUWWBiLRB9SGyrHdVqrjhPZDlKJ8iUsDCDEyqew8Tc4bQdoOZKlo3Pu1T0+fsyZ0o7zbRWWlldW14rrpY3Nre2yvbPbVFEiCW2QiEey7WNFORO0oZnmtB1LikOf05Y/usn91iOVikXiXo9j6oV4IFjACNZG6tnlboj10A/Sq+whPXaznl1xqs4EaJG4M1KBGeo9+6vbj0gSUqEJx0p1XCfWXoqlZoTTrNRNFI0xGeEB7RgqcEiVl06CZ+jQKH0URNI8odFE/b2R4lCpceibyTymmvdy8T+vk+jg0kuZiBNNBZkeChKOdITyFlCfSUo0HxuCiWQmKyJDLDHRpquSKcGd//IiaZ5U3fPq2d1ppXY9q6MI+3AAR+DCBdTgFurQAAIJPMMrvFlP1ov1bn1MRwvWbGcP/sD6/AGKIJMG</latexit>A−1Figure 7. Orthogonal Illusions. We show that our method works,
even when the view is a randomly sampled orthogonal transfor-
mation A. While these “illusions” are incomprehensible to hu-
man perception, they serve as a conﬁrmation for our mathematical
analysis.
images generated using our method match the prompts in
both views equally well and are of higher quality.
Permutations. Pixel and patch permutations, being a sub-
set of orthogonal transformations, should work with our
method. We show that this is indeed the case in Fig. 6,
where we have results on patch grids of various sizes under
randomly sampled permutations. The 64×64case is quite
hard, yet our method is able to generate images that satisfy
the constraint, albeit at lower quality.
Arbitrary Orthogonal Transformations. As discussed
in Sec. 3.3, our method works for any orthogonal transfor-
mation. So far, we have shown illusions based on a sub-
set of orthogonal views that correspond to intuitive image
transformations. In Fig. 7, we show “illusions” using an
arbitrary orthogonal transformation as a view. We use Sta-
ble Diffusion [ 36] and sample a random orthogonal matrix
A∈R16384 ×16384by projecting an i.i.d. random Gaus-
sian matrix with an SVD. These dimensions correspond to
the size of the Stable Diffusion latent space. We note that
this is an incredibly hard and unnatural transformation of an
image, and results are accordingly of lower quality, but our
method is still able to produce reasonable images.
Random Samples. We show random samples for selected
prompts in Fig. 8. As can be seen, these random samples,
while not as good as those in Fig. 1, are still very high qual-
ity. Some failure cases can be seen where the model prefers
one prompt over another. We add further discussion and
present more random samples in Appendix D.
4.4. Failures
We highlight three interesting failure cases of our method
in Fig. 9.
Independent Synthesis. The ﬁrst of these cases involves
the model synthesizing prompts separately, without com-
bining elements of the two to form an illusion. Empiri-
cally, this happens surprisingly rarely, especially given that
it seems to be such an easy shortcut solution. We hypothe-
size that this is because the diffusion model is biased toward
centering its content, resulting in far more images with con-
tent that is integrated and centered as opposed to separate
and off-center.
24160
an oil painting of 
people at a 
campfirean oil painting of 
an old manView: Vertical Flip
an oil painting of 
a fruit bowlan oil painting of 
a monkeyView: Vertical Flip
an oil painting of 
a snowy mountain 
villagean oil painting of 
a horseView: 90° Rotation
a painting of 
houseplantsa painting of 
marilyn monroeView: Jigsaw
a painting of  
wine and cheesea painting of  
a turtleView: Jigsaw
an oil painting of 
a tudor portraitan oil painting of  
a skullView: Skew
Figure 8. Random Samples. We show random samples, along with their corresponding view, for selected prompts. For more random
samples please see Appendix D.For best quality, view digitally and zoom-in.
Noise Shift. Using views that preserve noise statistics is
critical to our method’s success. For example, we attempted
to recreate the “Dress” illusion [ 46], in which a dress can be
seen as either “blue and black” or “white and gold.” We used
simple white balancing as our view, in which pixel values
were scaled by a constant factor. While this transformation
is linear, it does not preserve the statistics of Gaussian noise.
As a result, we see artifacts in the forms of spots, which we
hypothesize is the result of the model interpreting the scaled
Gaussian noise as signal and actively denoising peaks in the
scaled noise.
Correlated Noise. While our method supports rotations
as transformations, as demonstrated with the “3-view,” “4-
view,” and “Inner Rotation” illusions in Fig. 1, care must be
taken that the rotation does not introduce correlations in the
noise, such as through anti-aliasing. For example, bilinear
sampling introduces signiﬁcant correlations in the noise, as
it is a linear combination of four adjacent pixels. There-
fore, seemingly innocuous rotations may result in divergent
samples if transformations are not carefully kept correlation
free, as shown with the 45 degree bilinear rotation in Fig. 9.
5. Limitations and Conclusions
We present a method to produce compelling and diverse op-
tical illusions. Our method is simple and straightforward to
implement, and additionally amenable to theoretical anal-
ysis. We prove that our method works for a broad set of
transformations, and qualitatively show that it can gener-
ate a wide array of optical illusions. However, at the same
time many possible illusions and transformations are still
an oil painting of a bowl of fruitan oil painting of a monkeyView: Vertical FlipFailure:  Independent Synthesis
a photo of a black and blue dressa photo of a white and gold dressView:  White BalancingFailure:  Noise Shift
a sketch of an elephanta sketch of a mouseView:  Rotate Inner Circle 45 Degrees (Bilinear)Failure:  Correlated NoiseFigure 9. Failures. We highlight three interesting failure cases,
which are discussed in Sec. 4.4.
not possible using our method, such as color constancy il-
lusions, homographies, stretches, and more generally non-
volume-preserving deformations. We leave implementation
of these views for future work. Moreover, our method does
not consistently produce perfect illusions. This may be a
symptom of the difﬁculty of producing good illusions, but
may indicate future work to be done to improve consistency.
Acknowledgements We thank William Henning,
Trenton Chang, Kimball Strong, Jeongsoo Park,
Patrick Chao, Kurtland Chua, and Mohamed El Ba-
nani for their feedback on early drafts. Daniel is
supported by the National Science Foundation Grad-
uate Research Fellowship under Grant No. 1841052.
24161
References
[1] AUTOMATIC1111. Negative prompt. https :
/ / github . com / AUTOMATIC1111 / stable -
diffusion - webui / wiki / Negative - prompt ,
2022. Accessed: November 7, 2023. 3,5
[2] Ryan Burgert, Xiang Li, Abe Leite, Kanchana Ranasinghe,
and Michael Ryoo. Diffusion illusions: Hiding images in
plain sight. https://ryanndagreat.github.io/
Diffusion-Illusions , 2023. 2,3,5,6
[3] Kartik Chandra, Tzu-Mao Li, Joshua Tenenbaum, and
Jonathan Ragan-Kelley. Designing perceptual puzzles by
differentiating probabilistic programs. In ACM SIGGRAPH
2022 Conference Proceedings , pages 1–9, 2022. 2
[4] Ming-Te Chi, Chih-Yuan Yao, Eugene Zhang, and Tong-Yee
Lee. Optical illusion shape texturing using repeated asym-
metric patterns. The Visual Computer , 30:809–819, 2014.
[5] Hung-Kuo Chu, Wei-Hsin Hsu, Niloy J Mitra, Daniel
Cohen-Or, Tien-Tsin Wong, and Tong-Yee Lee. Camouﬂage
images. ACM Trans. Graph. , 29(4):51–1, 2010. 2
[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 2
[7] Yilun Du and Igor Mordatch. Implicit generation and
generalization in energy-based models. arXiv preprint
arXiv:1903.08689 , 2019. 2,4
[8] Yilun Du, Shuang Li, and Igor Mordatch. Compositional vi-
sual generation with energy based models. Advances in Neu-
ral Information Processing Systems , 33:6637–6647, 2020.
[9] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenen-
baum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein,
Arnaud Doucet, and Will Sussman Grathwohl. Reduce,
reuse, recycle: Compositional generation with energy-based
diffusion models and mcmc. In International Conference on
Machine Learning , pages 8489–8510. PMLR, 2023. 2,4
[10] Werner Ehm. A variational approach to geometric-optical
illusions modeling. Proceedings of Fechner Day , 27(1):41–
46, 2011. 2
[11] Gamaleldin Elsayed, Shreya Shankar, Brian Cheung, Nico-
las Papernot, Alexey Kurakin, Ian Goodfellow, and Jascha
Sohl-Dickstein. Adversarial examples that fool both com-
puter vision and time-limited humans. Advances in neural
information processing systems , 31, 2018. 3
[12] William T Freeman, Edward H Adelson, and David J
Heeger. Motion without movement. ACM Siggraph Com-
puter Graphics , 25(4):27–30, 1991. 2
[13] Timur Garipov, Sebastiaan De Peuter, Ge Yang, Vikas
Garg, Samuel Kaski, and Tommi Jaakkola. Compositional
sculpting of iterative generative processes. arXiv preprint
arXiv:2309.16115 , 2023. 2,4
[14] Alexander Gomez-Villa, Adrian Martin, Javier Vazquez-
Corral, and Marcelo Bertalm ´ıo. Convolutional neural net-
works can be deceived by visual illusions. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 12309–12317, 2019. 2,3
[15] Alex Gomez-Villa, Adri ´an Mart ´ın, Javier Vazquez-Corral,
Marcelo Bertalm ´ıo, and Jes ´us Malo. On the synthesis ofvisual illusions using deep generative models. Journal of
Vision , 22(8):2–2, 2022. 2
[16] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572 , 2014. 3
[17] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and
Dimitris Samaras. Diffusion models as plug-and-play pri-
ors. Advances in Neural Information Processing Systems ,
35:14715–14728, 2022. 2
[18] Rui Guo, Jasmine Collins, Oscar de Lima, and Andrew
Owens. Ganmouﬂage: 3d object nondetection with texture
ﬁelds. Computer Vision and Pattern Recognition (CVPR) ,
2023. 2
[19] Aaron Hertzmann. Visual indeterminacy in gan art. In ACM
SIGGRAPH 2020 Art Gallery , pages 424–428. 2020. 2
[20] Elad Hirsch and Ayellet Tal. Color visual illusions: A
statistics-based computational model. Advances in neural
information processing systems , 33:9447–9458, 2020. 2
[21] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance, 2022. 3
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. arXiv preprint arxiv:2006.11239 ,
2020. 2,3
[23] Priyank Jaini, Kevin Clark, and Robert Geirhos. Intrigu-
ing properties of generative classiﬁers. arXiv preprint
arXiv:2309.16779 , 2023. 2,3
[24] Mikhail Konstantinov, Alex Shonenkov, Daria Bakshan-
daeva, and Ksenia Ivanova. If by deepﬂoyd lab at stabilityai,
2023. GitHub repository. 2,5,11
[25] Monster Labs. Controlnet qr code monster v2 for sd-1.5,
2023. 3
[26] Nan Liu, Shuang Li, Yilun Du, Josh Tenenbaum, and
Antonio Torralba. Learning to compose visual relations.
Advances in Neural Information Processing Systems , 34:
23166–23178, 2021. 2,4
[27] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and
Joshua B Tenenbaum. Compositional visual generation with
composable diffusion models. In European Conference on
Computer Vision , pages 423–439. Springer, 2022. 2,4
[28] Dominique Makowski, Zen J Lau, Tam Pham, W
Paul Boyce, and SH Annabel Chen. A parametric frame-
work to generate visual illusions using python. Perception ,
50(11):950–965, 2021. 2
[29] Jerry Ngo, Swami Sankaranarayanan, and Phillip Isola. Is
clip fooled by optical illusions? 2023. 2,3
[30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models, 2021. 2
[31] Aude Oliva, Antonio Torralba, and Philippe G. Schyns. Hy-
brid images. ACM Trans. Graph. , 25(3):527–532, 2006. 2
[32] Andrew Owens, Connelly Barnes, Alex Flint, Hanumant
Singh, and William Freeman. Camouﬂaging an object from
many viewpoints. 2014. 2
[33] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv ,
2022. 3
24162
[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In Proceedings
of the 38th International Conference on Machine Learning ,
pages 8748–8763. PMLR, 2021. 5
[35] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821–8831. PMLR, 2021.
2
[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2022. 2,3,5,7
[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,
Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J
Fleet, and Mohammad Norouzi. Photorealistic text-to-image
diffusion models with deep language understanding, 2022. 2
[38] Troy Shinbrot, Miguel Vivar Lazo, and Theo Siu. Network
simulations of optical illusions. International Journal of
Modern Physics C , 28(02):1750018, 2017. 2
[39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In Proceedings of the
32nd International Conference on Machine Learning , pages
2256–2265, Lille, France, 2015. PMLR. 2,3
[40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. arXiv:2010.02502 , 2020. 2,
3
[41] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In International Conference on Learning Represen-
tations , 2021. 2,3
[42] Matthew Tancik. Illusion diffusion. https://github.
com/tancik/Illusion-Diffusion , 2023. 2,3,5,6,
12
[43] Ugleh. Spiral town - different approach to qr
monster. https : / / www . reddit . com / r /
StableDiffusion/comments/16ew9fz/spiral_
town_different_approach_to_qr_monster/ ,
2023. 3
[44] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich. Score jacobian chaining: Lifting
pretrained 2d diffusion models for 3d generation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 12619–12629, 2023. 3
[45] Xi Wang, Zoya Bylinskii, Aaron Hertzmann, and Robert
Pepperell. Toward quantifying ambiguities in artistic images.
ACM Transactions on Applied Perception (TAP) , 17(4):1–10,
2020. 2
[46] Wikipedia contributors. The dress. https://en.wikipedia . org / wiki / The _ dress . Accessed:
November 9, 2023. 8
[47] Lvmin Zhang and Maneesh Agrawala. Adding conditional
control to text-to-image diffusion models, 2023. 3
24163
