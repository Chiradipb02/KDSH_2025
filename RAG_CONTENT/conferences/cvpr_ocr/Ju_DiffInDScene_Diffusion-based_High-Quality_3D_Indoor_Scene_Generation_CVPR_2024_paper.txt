DiffInDScene: Diffusion-based High-Quality 3D Indoor Scene Generation
Xiaoliang Ju1*, Zhaoyang Huang1*, Yijin Li2, Guofeng Zhang2, Yu Qiao3, Hongsheng Li1,4
1CUHK MMLab2Zhejiang University3Shanghai AI Laboratory4CPII under InnoHK
{akira,drinkingcoder }@link.cuhk.edu.hk, hsli@ee.cuhk.edu.hk
Stage 1Stage 2Stage 3Pure Noise VolumeRough OccupancyDelicate OccupancyMesh from TSDF
Indoor Scene Geometry Generated by DiffInDSceneTexture added by DreamSpace
Texture added by DreamSpace
Figure 1. Coarse-to-ﬁne indoor scene geometry generation using a sparse diffusion framework. For better visualization, the texture is
produced by DreamSpace [46] after the scene geometry is generated by our DiffInDScene.
Abstract
We present DiffInDScene, a novel framework for tack-
ling the problem of high-quality 3D indoor scene gener-
ation, which is challenging due to the complexity and di-
versity of the indoor scene geometry. Although diffusion-
based generative models have previously demonstrated im-
pressive performance in image generation and object-level
3D generation, they have not yet been applied to room-
level 3D generation due to their computationally intensive
costs. In DiffInDScene, we propose a cascaded 3D diffu-
sion pipeline that is efﬁcient and possesses strong genera-
tive performance for Truncated Signed Distance Function
(TSDF). The whole pipeline is designed to run on a sparse
occupancy space in a coarse-to-ﬁne fashion. Inspired by
KinectFusion’s incremental alignment and fusion of local
TSDF volumes, we propose a diffusion-based SDF fusion
⇤Joint ﬁrst authorship
Please visit our project page for the latest updates: https://
akirahero.github.io/diffindscene/approach that iteratively diffuses and fuses local TSDF vol-
umes, facilitating the generation of an entire room environ-
ment. The generated results demonstrate that our work is
capable to achieve high-quality room generation directly in
three-dimensional space, starting from scratch. In addition
to the scene generation, the ﬁnal part of DiffInDScene can
be used as a post-processing module to reﬁne the 3D re-
construction results from multi-view stereo. According to
the user study, the mesh quality generated by our DiffInD-
Scene can even outperform the ground truth mesh provided
by ScanNet.
1. Introduction
3D scene production is a fundamental task in 3D computer
vision with many applications, such as Augmented Real-
ity (AR), game development and embodied AI [ 28], where
the quality of 3D scene geometry plays a paramount role.
While the 3D reconstruction from multi-view stereo [ 1,41]
can recover scenes from real-world, the quality of the re-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
4526
sultant meshes is far from satisfactory, for the major mesh
details might be lost during iterative fusion. Recently, diffu-
sion models [ 11,39] have shown their great ability in gen-
erating images and objects of high quality. Here we want to
ask a question: Can we exploit diffusion models to produce
3D scenes ? In this paper, we propose a novel framework
DiffInDScene, which not only helps optimize the results of
3D reconstruction, but also generates high-quality indoor
spatial geometry from scratch (see Fig. 1).
Diffusion models are a class of generative models de-
signed for synthesizing data by iterative denoising. The
popular Denoising Diffusion Probabilistic Model (DDPM)
training paradigm starts the denoising from pure Gaussian
noise. Training diffusion models for room-level Truncated
Signed Distance Function (TSDF) volumes is challenging
because of its large size. Previous 3D diffusion models only
focus on object-level 3D generations. As reported by In-
stantNGP [ 24], only 2.57% voxels are informative in com-
mon 3D scenes.
To deal with the large scale of indoor scenes, we pro-
pose a coarse-to-ﬁne sparse diffusion pipeline consists of
multiple stages. The ﬁrst few stages are used to generate
the occupancy volume in a coarse-to-ﬁne manner, and the
last one generates the TSDF values in the sparsely occu-
pied voxels. For the ﬁrst few occupancy generation stages,
a multi-scale auto-encoder is designed to encode occupancy
to latent space, providing feature guidance for occupancy
generation. This approach allows us to employ latent diffu-
sion and further compress the size of input volumes. Addi-
tionally, we propose a sparse 3D diffusion model denoising
only on the sparsely occupied voxels of TSDF or the occu-
pancy latent volumes, which saves two orders of computa-
tional and memory costs.
Although the cascaded sparse diffusion pipeline signif-
icantly reduces the required computational resources, the
considerable variation in room sizes still poses a challenge
when attempting to directly train on a room-level TSDF vol-
ume, at the ﬁnal stage of our cascaded diffusion process.
To increase the data variation within each mini-batch, we
randomly crop local TSDF volumes of smaller sizes from
the original large volume for training. During inference,
we design a stochastic TSDF fusion algorithm that gener-
ates the entire room by iterative denoising and fusing local
TSDF volumes. The fusion method allows for the genera-
tion of a complete and uniﬁed TSDF within a large scene
while efﬁciently decomposing the scene into smaller crops,
thereby conserving computing resources. Our proposed dif-
fusion method can be also utilized to reﬁne indoor scene
meshes such as the reconstruction results from multi-view
stereo, such as NeuralRecon [ 41]. Given the TSDF volume
of reconstructed scene as occupancy condition, our diffu-
sion model can effectively reﬁne and optimize the TSDF
volume towards the ground truth.Our contributions can be summarized as four-fold: 1) We
propose a novel framework DiffInDScene for room-level in-
door scene generation with a sparse diffusion model that
saves two orders of resource consumption. 2) We design a
multi-scale auto-encoder to provide feature guidance for the
scene occupancy generation. 3) We propose a novel algo-
rithm that fuses diffusion-based local TSDF volumes, which
enables large-scale indoor scene generation. 4) DiffInD-
Scene exhibits a promising capability in producing high-
quality room-level geometry through both generation from
scratch and reﬁnement of existing reconstructions.
2. Related Works
Diffusion Models. The diffusion model [ 11,38,39] has
emerged as a promising class of generative models for
learning data distributions through an iterative denoising
process. They have shown impressive visual quality in
diverse applications of 2D image synthesis, encompass-
ing image inpainting [ 21], super-resolution [ 13,34], edit-
ing [ 23], text-to-image synthesis [ 25,33], and video gen-
eration [ 12,14]. Nevertheless, the application of diffusion
models in the 3D domain has received limited attention in
comparison to the extensive exploration seen in the 2D do-
main. In the 3D domain, existing research has focused on
the generation of individual objects [ 16,22,27,48], while
less attention has been paid to the synthesis of entire scenes,
which possess signiﬁcantly higher levels of semantic and
geometric complexity, as well as the expansive spatial ex-
tent present in 3D scene synthesis.
3D Shape Generation. Extensive exploration has been
conducted on various 3D representation methods, such
as voxel [ 43,45], point cloud [ 26,44,47], and implicit
ﬁeld [ 19,37], in conjunction with different generation mod-
els such as diffusion models [ 37] and GANs [ 43]. While
these approaches have shown success in object-level gener-
ation, transferring them to scene generation at a larger scale
is still challenging. Firstly, the large scale leads to exponen-
tial growth in computation resources consumption in train-
ing and inference processes. Additionally, object-level gen-
eration is comparatively easier due to simpler geometries
and less diversity.
3D Scene Synthesis. In recent decades, the ﬁeld of
3D scene synthesis has experienced extensive investiga-
tion, particularly driven by the proliferation of 3D indoor
scene datasets [ 4,6] and advancements in 3D deep learn-
ing [20,29,30]. However, current methods mainly focus on
synthesizing plausible 3D scene arrangements [ 3,8,10,31].
They usually learn to synthesize the scene graph as the in-
termediate scene representation and retrieve objects from
available dataset. In contrast to these methods, we aim to
simultaneously synthesize both the scene arrangement and
the detailed geometry. Text2Room [ 15] is the most related
work to ours in recent years, which leverages pre-trained
4527
2D text-to-image models to synthesize a sequence of im-
ages and then conduct an iterative reconstruction. While
such method can produce room-scale geometry, the results
are often fragmentary and distorted, limiting their practical
applications in areas such as gaming or AR.
3. Methodology
3.1. Overview
To generate room-level 3D geometry, the greatest challenge
is the large scale, as it requires substantial computing re-
sources. We employ a cascaded diffusion model to generate
the whole room in a coarse-to-ﬁne manner. The ﬁrst stage
is to generate the coarse structure of the whole room. The
following stages further reﬁne the rough shape to a 3D oc-
cupancy ﬁeld with higher resolutions. At the ﬁnal stage, the
resolution increases to the highest level, and we crop the
whole scene to overlapped pieces to generate the ﬁnal de-
tailed Truncated Signed Distance Function (TSDF) volume.
In every stage, we use a separate sparse diffusion model
to reduce the resource consumption, which exclusively de-
noises on sparsely distributed occupancy. In our implemen-
tation, we use 3 stages to create indoor geometry up to size
of512⇥512⇥128.
Such cascaded solution has three advantages. First,
the computation resource consumption is constrained in all
stages. Second, compared with piece-wise generation or in-
cremental generation methods, the ﬁrst stage of our model
is able to sketch the global structure of the scene, which
helps to generate a complete layout with uniﬁed and de-
tailed structure. Third, every stage can be trained indepen-
dently, and the generation process can be stopped in ad-
vance when generating an unsatisﬁed layout at early stage.
3.2. Cascaded Diffusion for Indoor Geometry Gen-
eration
We propose a sparse cascaded diffusion model as shown in
Fig.2(a). In our implementation, a 3-stage diffusion pro-
cess is utilized to generate a complete indoor scene starting
from noise. The ﬁrst 2 stages are utilized to generate and
reﬁne the 3D binary scene occupancy volume, and the ﬁ-
nal is used to generate TSDF value within the occupancy.
As TSDF volume only retains information near the object
surface, we simply deﬁne all voxels containing valid TSDF
values as the binary scene occupancy.
Assume we have multi-scale occupancy embeddings
z(1),z(2)of a TSDF volume xwith increasing resolutions,
and their binary occupancy masks are Mz(1),Mz(2),Mx,
satisfying
Mz(2)=G1⇣
z(1),Mz(1)⌘
, (1)
Mx=G2⇣
z(1),z(2),Mz(2)⌘
, (2)where G1,G2are occupancy decoders, and we will explain
them in detail in Section 3.3together with the occupancy
latents z(1),z(2). Then the 3-stage diffusion processes
{D1,D2,D3}can be established as follows.
•Stage 1 generates the occupancy latent code z(1)of the
lowest resolution. Given a ﬁxed volume z(1)
Tand its oc-
cupancy mask Mz(1)
T, the diffusion process performs de-
noising operation in Ttimesteps to obtain z(1)
0as
z(1)
0=D1(z(1)
T,Mz(1)
T), (3)
where z(1)
Tis ﬁlled with Gaussian noise, and Mz(1)
Tserves
as a bounding box to specify the scene size of generation.
If we set the block Mz(1)
T[0 :W,0:H,0:L]=1 and
keep all other elements equal to 0, the model can generate
a scene with a maximum size of nW⇥nH⇥nL, where
ndenotes compression ratio of z(1)
Tcompared to x.
•Stage 2 generates latent code of higher resolution z(2)
conditioned on z(1)as
z(2)
0=D2(z(2)
T,Mz(2)
T;z(1)
0), (4)
where Mz(2)
Tis obtained utilizing Eq. ( 1), and z(2)
Tis ﬁlled
with another Gaussian noise volume.
•Stage 3 generates the ﬁnal TSDF volume with all those
generated latent codes z(1),z(2)as input conditions
x0=D3(xT,MxT;z(1)
0,z(2)
0), (5)
where MxTis obtained by Eq. ( 2), and xTis ﬁlled with
Gaussian noise.
Compared with generating occupancy directly, the occu-
pancy embedding can guide the reﬁnement of the occu-
pancy throughout the diffusion process, particularly at the
initial stage of generation.
Sparse Diffusion. We follow DDPM [ 11,38] to implement
the sparse diffusion. Each stage of our model employs a
separate sparse diffusion, with the only distinction lies in
the types of input and output. In this section, we use vto
represent any kind of volumes such as TSDF xand latent
codes z(1),z(2), and yto denote the diffusion condition with
Mas their shared sparsity mask.
DDPM [ 11,38] transforms a sample volume v0, to a
white Gaussian noise vT⇠N (0,1)inTsteps. In each step
t, the sample vtis obtained by adding i.i.d. Gaussian noise
with variance  tand scaling the sample in the previous step
vt 1withp1  t:
q(vt|vt 1)=N⇣
vt;p
1  tvt 1, tI⌘
,(6)
which is also called the forward direction. On the other
hand, the reverse process can be depicted as:
p✓(vt 1|vt,y,M )=N(vt 1;µ✓,⌃✓), (7)
4528
(a) Sparse Cascaded Diffusion Pipeline (3-stage Implementation)  SparseDMSparseDMOccupancy DecoderUp-samplerOccupancy DecoderUp-samplerUp-samplerSparseDMVolume with Arbitrary Size Stage 1Stage 2Stage 3!(")!($)
EncodeQuantizeDecodeEncodeQuantizeDecodeUp-sampleTSDF Volume CropLatent Code(b) Learning the Latent Space for Multi-scale Occupancy
Latent Code!(")!($)!%(")!%($)!!(")"(#), up-sampled!%"(&), up-sampled"""$"&
Sparsely ReconstructedTSDF Volume CropSparse TSDF Volume
Figure 2. Sparse cascaded diffusion with a multi-scale occupancy embedding.
where ydenotes the extra condition. Diffusion models are
trained to reverse the forward process, predicting µ✓as
µ✓(vt,y,M,t )=1p↵t✓
vt  tp1 ¯↵t✏✓(vt,y,M,t )◆
,
(8)
where ↵t:= 1   t,¯↵t:=Qt
s=0↵s, and ✏✓is the
neural network with the parameter set ✓, which has a
UNet-like structure. With the occupancy mask M,✏✓de-
noises only sparsely occupied voxels with sparse convolu-
tions and attentions. We implement ✏✓using the engine of
TorchSparse [ 42]. The details of network architecture are
provided in the supplementary materials.
A mean square error loss masked by Mis used to super-
vise the noise prediction as
Ldi↵=Et,v0,✏,y,M⇣
M h
k✏ ✏✓(vt,y,M,t )k2
2i⌘
.
(9)
3.3. Learning the Latent Space for Multi-scale Oc-
cupancy
To obtain hierarchical occupancy embeddings and their de-
coders, we design a multi-scale Patch-VQGAN as Fig. 2(b),
inspired by VQ-V AE-2 [ 32]. The encoder takes TSDF
volume xas input, and outputs the occupancy embedding
z(1),z(2). These embeddings are expected to be decoded to
the occupancy mask Mz(2)andMxas Eq. ( 1) and Eq. ( 2),
where ground truth of Mz(2)is obtained by downsampling
from Mxvia maxpooling. To capture more shape details,
we also add a TSDF decoding head. After training, the en-
coder is no longer utilized, and only the occupancy decoders
are employed to convert the latent code to occupancy.Formally, we deﬁne the encoder as E, the decoders as
G, and the element-wise quantization of latent volume as
q(·). Any ground truth TSDF volume x2RH⇥W⇥Lcan
be encoded progressively into (z(1),z(2))= E(x), where
z(1),z(2)✓Rd. The quantization is formulated as
q(z) := (arg min
zp2Zkzijk zpk)2Rh⇥w⇥l⇥d,(10)
where Z={zk}K
k=1⇢Rddenotes the discrete codebook
of size K, and zijkrepresents the latent vector at coordinate
i, j, k of the latent volume. For simplicity, we deﬁne z(1)
q:=
q(z(1)),z(2)
q:=q(z(2))in Fig. 2(b).
During the training process, we randomly crop cubes of
96⇥96⇥96from the original TSDF volumes as data sam-
ples, so that diverse crops from different scenes are con-
tained in each mini-batch. The training loss is deﬁned in
Eq. ( 11), where Lrec,Lvq,LGANdenote the reconstruction
loss, vector quantization loss and the adversarial loss from
a simple multi-layer discriminator,
L=Lrec+ 1Lvq+ 2LGAN, (11)
where  1and 2are hyper parameters to weight losses of
different types. For the encoder-decoder training, we use
aL1loss to supervise the TSDF value, and binary cross-
entropy (BCE) to supervise the occupancy masks, as shown
in Eq. ( 12).
Lrec=B C E ( ˆMx,Mx)+B C E ( ˆMz(2),Mz(2))+kx ˆxk1
(12)
TheLvq,LGANare deﬁned similar to [ 7].
3.4. Local Fusion for Global Diffusion
The volume cropping operation in the training of the ﬁnal
stage diffusion raises a question: how to infer on a complete
4529
0.10.00.20.70.20.5-0.60.40.4-0.10.20.20.10.9-0.10.70.2-0.20.70.50.30.4-0.60.10.3-0.10.20.60.00.8-0.10.9Arbitrary Sliding Direction0.10.00.20.70.20.5-0.60.40.4-0.10.20.20.10.9-0.10.7-0.60.70.50.3-0.60.10.20.60.00.8-0.10.90.30.70.4-0.2Noise Prediction at Same Timestep
Diffusion Inference in Overlapped Sliding Windows Stochastic Local FusionLarge SceneFigure 3. Stochastic TSDF Fusion.
scene using a model trained on crops? The independent
generation in a crop-by-crop manner as image generators
such as [ 21,33] may cause inconsistent results between
adjacent crops. To address this issue, a fusion algorithm is
proposed to perform the joint diffusion process concurrently
on the overlapping local volumes.
During inference, we split the indoor space into Kover-
lapping 3D crops that cover the entire room, and generate
the TSDF for the entire room by concurrently diffusing the
Kcrops with stochastic fusion. We denote the global TSDF
at the timestep tasxtand the k-th crop at the timestep
tasxk
tand the global TSDF at the timestep tasxt. At
time step t, we need to obtain the global TSDF xtby fus-
ing local TSDFs xk
tand then update local TSDFs from the
global TSDF. After synchronizing local TSDFs with fusion,
each crop steps to the next time step independently. Specif-
ically, for a voxel grid p, suppose G(p)is a set of crops
that cover p, we need to obtain xt(p)by fusing the crops
{xk
t(p)|k2G(p)}overlapping on p.
Stochastic TSDF Fusion. A straightforward fusion al-
gorithm is taking the average TSDFs of the local crops:
xt(p)=1
|G(p)|P
k2Gxk
t(p), which is also adopted by the
classical KinectFusion [ 17]. However, average fusion sig-
niﬁcantly reduces the variance of the sample distribution.
To ensure the generation quality and global consistency, we
propose stochastic fusion to keep the distribution and fuse
TSDFs in the reverse process. An example of 2-windows
fusion is shown in Fig. 3. Speciﬁcally, we randomly sample
an index kfrom G(p)in a uniform distribution to update
the global TSDF xt(p)=xk
t(p),
xt(p)⇠N (µk
t(p),⌃k
t(p)),k= RandomSelect ( G(p)).
(13)
3.5. Extension to Reﬁning 3D Reconstruction
Given a rough geometry represented by the occupancy vol-
ume of a scene, the TSDF volume can be generated by
directly adopting the ﬁnal stage of our cascaded diffusion
framework.
The most direct application is to recover/reﬁne scenes
produced by multi-view stereo methods or even LiDAR8Optional Condition (e.g.SDF value)MVS
SparseDMLocalFusion
Rough OccupancyCropsComplete SceneFigure 4. Reﬁne or recover scene from MVS methods.
mappers as depicted in Fig. 4, as obtaining an occupancy
of the scene is relatively straightforward using Multi-view
Stereo (MVS) techniques. If the MVS method provides a
TSDF result, it can be utilized as the diffusion condition to
our cascaded diffusion network. The diffusion process is
applied to the crops of the occupancy volume as inputs, and
the results are fused using our local fusion module at each
timestep, so that we can obtain a complete and reﬁned scene
reconstruction.
4. Experiment
DiffInDScene is a versatile tool capable of generating de-
tailed indoor scene geometry at the room level. It is not only
capable of creating scenes from scratch but also has the abil-
ity to reﬁne or recover scenes using occupancy ﬁelds pro-
duced by multi-view stereo methods. To train DiffInDScene
for scene generation, we utilized the 3D-FRONT dataset [ 9]
provided by Alibaba, which consists of 6813 furnished
houses with meshes. For training purposes, we selected
5913 houses with dimensions smaller than 512⇥512⇥128,
with a voxel size of 0.04m. For scene reﬁnement or recov-
ery on multi-view stereo (MVS), we trained DiffInDScene
using the training split of the ScanNet dataset, with the Neu-
ralRecon [ 41] as the MVS module. The ScanNet dataset
contains 1613 indoor scenes, each accompanied by ground-
truth camera poses and surface reconstructions. To ensure
consistency, we adopted the same data split as NeuralRecon
for training and evaluation purposes.
4.1. Indoor 3D Scene Generation
In this section, we will present the results of our indoor
scene generation, both quantitatively and qualitatively. To
the best of our knowledge, Text2Room stands as the state-
of-the-art solution to generate 3D geometry of room-level
indoor scenes. Since there are few other works that can di-
rectly generate scene structures and output the correspond-
ing mesh models, here we also compare with ”Text2Room
+ Poisson” to enrich the experimental comparison, which
means a Poisson reconstruction [ 18] is added as a reﬁne-
ment of Text2Room.
Metrics. We employ mesh quality as a measure to reﬂect
the overall quality of the generated 3D scene geometry. Re-
garding mesh quality, we noticed that noisy meshes and
4530
Table 1. Geometry quality comparison, including mesh quality
(Aspect Ratio, Circularity, and Shape Regularity) and user study
on the completeness and perceptual quality.
Text2Room [ 15]Text2Room
+ Poisson [ 18]Ours
Aspe. mean " 0.416 0.443 0.473
Aspe. var # 0.022 0.029 0.009
Circ. mean " 0.674 0.709 0.781
Circ. var # 0.052 0.057 0.022
Shap. mean " 0.716 0.730 0.816
Shap. var # 0.045 0.060 0.023
Completeness " 2.532 3.228 4.856
Perceptual " 2.472 2.812 4.836
high-quality meshes exhibit distinct distributions of triangle
shapes. Noisy meshes and problematic regions are charac-
terized by triangles with low aspect ratio, circularity, and
shape regularity, as introduced in [ 2,5]. Therefore, these 3
factors are selected as the objective metric of mesh quality.
In addition, we perform a user study similar to the one
conducted in Text2Room [ 15] as the subjective metric on
the mesh quality, including the completeness and perceptual
quality. The scores range from 1 to 5, with higher scores
indicating a better alignment with the evaluation metrics.
Quantitative Results. For the objective metrics, We ran-
domly select 11 scenes from different methods, and calcu-
late the average value of all triangular mesh faces. For the
user study, we randomly choose 5 scenes generated by each
method and gather ratings from a group of 40 users with ba-
sic knowledge in 3D modeling. We summarize the evalua-
tion results as Table 1shows. Our method demonstrates su-
perior performance on those metrics compared to the other
approaches. While employing Poisson reconstruction as a
reﬁnement step enhances the results of Text2Room, it does
not lead to a signiﬁcant improvement.
Qualitative Results. The generated scene samples from
different methods are listed in Fig. 5. We compare both un-
textured meshes and textured scene renderings in our eval-
uation. While Text2Room is capable of reconstructing the
scene from images, it often results in serious distortions and
fragmentation. As indicated by our quantitative evaluation,
applying Poisson reconstruction helps in ﬁlling the holes,
but it has limited impact on improving the overall structure
of the scene. Our method can produce larger rooms with
clearer structures and more complex layout. Furthermore,
by incorporating DreamSpace as a post-processing step, we
can achieve high-quality scene renderings. This combina-
tion allows for enhanced visual output and improved overall
scene representation.
Compared with the middle-size house showed in Fig. 5,
two larger and more complex samples are shown in Fig. 6.
Due to limited space, we demonstrate additional samplesTable 2. Resource consumption comparison.
TFLOPs Parameters(M) GPU Memory(GB)
Batch Size 1 1 1 2 4
Sparse 0.008 161.5 11.8 15.3 22.8
Dense 3.290 161.5 22.8 - -
showcasing larger views and conditional Sketch-to-Scene
generation in the supplementary materials.
4.2. Ablation Study
Sparse or Dense. To compare the dense diffusion and
the sparse diffusion, we implement two networks using
sparse and dense convolution respectively, with exactly the
same structures. Several randomly cropped TSDF volumes
(96⇥96⇥96)from ScanNet dataset are fed into these
two models. The resource consumption of the two strate-
gies are shown in Table 2. This experiment is conducted
on the platform equipped with RTX3090 GPU with 24GB
memory, with the diffusion model running in training sta-
tus. The sparse diffusion requires fewer resources and runs
faster with fewer parameters, due to the characteristic of the
occupancy distribution. In our test data, the largest occu-
pancy rate of the TSDF crops are less than 20%.
Diffusion with Fusion. To compare the different fusion
methods mentioned in Section 3.4, we perform the ﬁnal
stage of our cascaded diffusion model on the given occu-
pancy from a scene of ScanNet dataset. The results are
shown in Fig. 7. Individual diffusion presents signiﬁcant
inconsistency between adjacent crops. Average fusion gen-
erates meshes of lower quality because the sample distribu-
tion during diffusion is disturbed. Our stochastic diffusion
generates high-quality meshes with global consistency.
4.3. Reﬁnement and Recovery on MVS
Metrics. Our objective is to obtain high-quality 3D scenes
from MVS, aiming for results comparable to the ground
truth in various aspects, including details, completeness,
tightness, and sharpness. As a generative model, the pro-
posed method operates within a relaxed occupancy, which
may cause drift from the ground truth. As a result, conven-
tional correspondence-based metrics for 3D reconstruction
may be not suitable, while we include such evaluation [ 36]
in the supplementary materials. Instead, we employ metrics
such as normal error distribution and mesh quality.
Normal error is used to evaluate the similarity of sur-
face orientation between the reconstructed and ground truth
meshes. Smaller normal errors indicate better alignment to
the ground truth. We ﬁlter out outliers with errors larger
than 90 and analyze the percentage of inlier normals be-
low a threshold ( <T ratio) and the mean normal error
of those inliers. We use the same mesh quality metrics as
Section 4.1.
4531
OursText2RoomText2Room + PoissonGeometryTexture Rendering Texture Rendering Geometry
Ours
Figure 5. Comparison of indoor scene generation. The texture renderings of our results are produced by DreamSpace [ 46]. The ceilings
are removed from the original meshes for a better visualization.
Figure 6. Samples with more complex structures generated by
DiffInDScene. Note that the ﬂoating objects in this ﬁgure are ceil-
ing lamps, which are reserved after we remove the ceilings for
visualization.
Results. The samples of scene reconstruction are shown in
Fig.8. As Table 3shows, our method signiﬁcantly outper-
forms the other three methods on all thresholds of normalerror. As for the mesh quality, we compute the mean and
variance of the three scores described above as shown in Ta-
ble4. Our results present signiﬁcantly better performance
than the other three methods, i.e., higher scores with smaller
score variance. Moreover, we also include ground truth
mesh for comparison. Interestingly, our results also out-
perform the ground truth except for the aspect ratio, which
indicates the high quality of our reconstructed meshes.
User Study. The user study is conducted to rank scene
quality, providing subjective evaluations to complement the
objective metrics. We randomly select 10 scenes from the
test split of the ScanNet dataset and generate reconstructed
meshes using four methods. Users are asked to rank the
methods based on details, completeness, plane quality, and
edge quality. We collect ranking results from 51 users. As
shown in Table 5, our reconstructed mesh signiﬁcantly out-
performs NeuralRecon, “NeuralRecon+Laplacian Denois-
4532
IndividualAverageStochastic
Figure 7. Comparison of different fusion methods in the ﬁnal stage
of our cascaded diffusion.
Table 3. Normal error comparison. “NR”, “Lap”, and “SR” denote
NeuralRecon, Laplacian and SimpleRecon, respectively.
NR [ 41]NR + Lap [ 40]SR [35] NR + Ours
<90 mean # 34.65 37.12 35.44 30.4
<90 ratio" 100% 100% 100% 100%
<45 mean # 10.27 12.34 12.51 8.09
<45 ratio" 59.97% 57.97% 60.20% 65.05%
<30 mean # 6.45 8.17 8.31 5.05
<30 ratio" 52.88% 49.89% 51.67% 59.27%
Table 4. Mesh quality comparison. We compare the mean and
variance of three scores: Aspect Ratio, Circularity, and Shape Reg-
ularity. NR(occ) means only the occupancy is used, without the
conditional TSDF depicted by the dash line in Fig. 4.
NR [ 41]NR +
Lap [ 40]SR [35]NR(occ)
+ OursNR +
OursGT
Aspe. mean " 0.459 0.437 0.436 0.469 0.457 0.477
Aspe. var # 0.022 0.023 0.024 0.020 0.016 0.022
Circ. mean " 0.740 0.712 0.708 0.742 0.763 0.758
Circ. var # 0.041 0.052 0.054 0.041 0.030 0.034
Shap. mean " 0.772 0.746 0.739 0.793 0.797 0.793
Shap. var # 0.041 0.052 0.055 0.041 0.030 0.031
ing” (denoted as ”NR + Lap”), and even surpasses the qual-
ity of the ground truth meshes.
5. Conclusion & Acknowledgement
We have presented DiffInDScene as a novel framework
for diffusion-based high-quality indoor scene generation.
DiffInDScene mainly consists of three modules: 1) a sparse
diffusion network that efﬁciently denoises 3D volumes on
(a) Scene samples refined on NeuralRecon(NR).
(b) Comparison of scene quality from different methods.NR + OursNRGround TruthFigure 8. Sample scene reconstructions on ScanNet dataset. The
meshes are colored according to curvatures in sub-ﬁgure(b), where
green regions denote lower curvatures.
Table 5. User study on the reﬁnement of scene reconstruction.
NR [ 41]NR + Lap [ 40]NR + Ours GT
Details " 12.26 6.80 17.41 25.50
Completeness " 11.15 8.84 21.30 20.76
Tight Plane " 5.48 12.10 25.85 18.17
Sharp Edge " 8.22 10.20 22.58 21.19
Overall (Sum) " 37.10 37.98 87.14 85.62
occupied voxels, 2) a multi-scale Patch-VQGAN for occu-
pancy decoding, 3) a cascaded diffusion framework to gen-
erate room-level scene from scratch, and 4) a stochastic fu-
sion algorithm for diffusion-based local TSDFs, which en-
ables large-scale indoor scene generation.
This project is funded in part by National Key R&D Pro-
gram of China Project 2022ZD0161100, by the Centre for
Perceptual and Interactive Intelligence (CPII) Ltd under the
Innovation and Technology Commission (ITC)’s InnoHK,
by General Research Fund of Hong Kong RGC Project
14204021. Hongsheng Li is a PI of CPII under the InnoHK.
We would like to thank the authors of DreamSpace [ 46] for
supporting us to produce vivid visualization. We also ap-
preciate the participation of users in our user study.
4533
References
[1]Aljaz Bozic, Pablo Palafox, Justus Thies, Angela Dai, and
Matthias Nießner. Transformerfusion: Monocular rgb scene
reconstruction using transformers. Advances in Neural In-
formation Processing Systems , 34:1403–1414, 2021. 1
[2]Jan Brandts, Sergey Korotov, and Michal K ˇr´ıˇzek. On the
equivalence of regularity criteria for triangular and tetrahe-
dral ﬁnite element partitions. Computers & Mathematics
with Applications , 55(10):2227–2233, 2008. Advanced Nu-
merical Algorithms for Large-Scale Computations. 6
[3]Angel Chang, Manolis Savva, and Christopher D Manning.
Learning spatial knowledge for text to 3d scene generation.
InProceedings of the 2014 conference on empirical methods
in natural language processing (EMNLP) , pages 2028–2038,
2014. 2
[4]Angel Chang, Angela Dai, Thomas Funkhouser, Maciej
Halber, Matthias Niessner, Manolis Savva, Shuran Song,
Andy Zeng, and Yinda Zhang. Matterport3d: Learning
from rgb-d data in indoor environments. arXiv preprint
arXiv:1709.06158 , 2017. 2
[5]Paolo Cignoni, Marco Callieri, Massimiliano Corsini, Mat-
teo Dellepiane, Fabio Ganovelli, Guido Ranzuglia, et al.
Meshlab: an open-source mesh processing tool. In
Eurographics Italian chapter conference , pages 129–136.
Salerno, Italy, 2008. 6
[6]Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 5828–5839, 2017. 2
[7]Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 12873–12883, 2021. 4
[8]Matthew Fisher, Manolis Savva, Yangyan Li, Pat Hanrahan,
and Matthias Nießner. Activity-centric scene synthesis for
functional 3d scene modeling. ACM Transactions on Graph-
ics (TOG) , 34(6):1–13, 2015. 2
[9]Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming
Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Bin-
qiang Zhao, et al. 3d-front: 3d furnished rooms with layouts
and semantics. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 10933–10942,
2021. 5
[10] Qiang Fu, Xiaowu Chen, Xiaotian Wang, Sijia Wen, Bin
Zhou, and Hongbo Fu. Adaptive synthesis of indoor scenes
via activity-associated object relation graphs. ACM Transac-
tions on Graphics (TOG) , 36(6):1–13, 2017. 2
[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840–6851, 2020. 2,3
[12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High deﬁnition video generation with diffusion mod-
els.arXiv preprint arXiv:2210.02303 , 2022. 2[13] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high ﬁdelity image generation. J. Mach. Learn.
Res., 23(47):1–33, 2022. 2
[14] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J Fleet. Video dif-
fusion models. arXiv preprint arXiv:2204.03458 , 2022. 2
[15] Lukas H ¨ollein, Ang Cao, Andrew Owens, Justin Johnson,
and Matthias Nießner. Text2room: Extracting textured
3d meshes from 2d text-to-image models. arXiv preprint
arXiv:2303.11989 , 2023. 2,6
[16] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural
wavelet-domain diffusion for 3d shape generation. In SIG-
GRAPH Asia 2022 Conference Papers , pages 1–9, 2022. 2
[17] Shahram Izadi, David Kim, Otmar Hilliges, David
Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie
Shotton, Steve Hodges, Dustin Freeman, Andrew Davison,
et al. Kinectfusion: real-time 3d reconstruction and inter-
action using a moving depth camera. In Proceedings of the
24th annual ACM symposium on User interface software and
technology , pages 559–568, 2011. 5
[18] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe.
Poisson surface reconstruction. In Proceedings of the fourth
Eurographics symposium on Geometry processing , page 0,
2006. 5,6
[19] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-
sdf: Text-to-shape via voxelized diffusion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12642–12651, 2023. 2
[20] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-
voxel cnn for efﬁcient 3d deep learning. Advances in Neural
Information Processing Systems , 32, 2019. 2
[21] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting
using denoising diffusion probabilistic models. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11461–11471, 2022. 2,5
[22] Shitong Luo and Wei Hu. Diffusion probabilistic models for
3d point cloud generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 2837–2845, 2021. 2
[23] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. In International Conference on Learning Representa-
tions , 2021. 2
[24] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 41(4):1–15, 2022. 2
[25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 2
[26] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
4534
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751 , 2022. 2
[27] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv
preprint arXiv:2209.14988 , 2022. 2
[28] Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dal-
laire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai,
Alexander William Clegg, Michal Hlavac, So Yeon Min,
et al. Habitat 3.0: A co-habitat for humans, avatars and
robots. arXiv preprint arXiv:2310.13724 , 2023. 1
[29] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcation
and segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 652–660,
2017. 2
[30] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. Advances in neural information
processing systems , 30, 2017. 2
[31] Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, and
Song-Chun Zhu. Human-centric indoor scene synthesis us-
ing stochastic grammar. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
5899–5908, 2018. 2
[32] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-
ating diverse high-ﬁdelity images with vq-vae-2. Advances
in neural information processing systems , 32, 2019. 4
[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684–10695, 2022. 2,5
[34] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J Fleet, and Mohammad Norouzi. Image super-
resolution via iterative reﬁnement. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 2022. 2
[35] Mohamed Sayed, John Gibson, Jamie Watson, Victor
Prisacariu, Michael Firman, and Cl ´ement Godard. Simplere-
con: 3d reconstruction without 3d convolutions. In Com-
puter Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIII ,
pages 1–19. Springer, 2022. 8
[36] Steven M Seitz, Brian Curless, James Diebel, Daniel
Scharstein, and Richard Szeliski. A comparison and evalua-
tion of multi-view stereo reconstruction algorithms. In 2006
IEEE computer society conference on computer vision and
pattern recognition (CVPR’06) , pages 519–528. IEEE, 2006.
6
[37] Jaehyeok Shim, Changwoo Kang, and Kyungdon Joo.
Diffusion-based signed distance ﬁelds for 3d shape gener-
ation. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 20887–20897,
2023. 2
[38] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning , pages 2256–2265. PMLR, 2015.
2,3[39] Yang Song and Stefano Ermon. Improved techniques for
training score-based generative models. Advances in neural
information processing systems , 33:12438–12448, 2020. 2
[40] Olga Sorkine, Daniel Cohen-Or, Yaron Lipman, Marc Alexa,
Christian R ¨ossl, and H-P Seidel. Laplacian surface editing.
InProceedings of the 2004 Eurographics/ACM SIGGRAPH
symposium on Geometry processing , pages 175–184, 2004.
8
[41] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and
Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruc-
tion from monocular video. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 15598–15607, 2021. 1,2,5,8
[42] Haotian Tang, Zhijian Liu, Xiuyu Li, Yujun Lin, and Song
Han. Torchsparse: Efﬁcient point cloud inference engine.
Proceedings of Machine Learning and Systems , 4:302–315,
2022. 4
[43] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and
Josh Tenenbaum. Learning a probabilistic latent space of
object shapes via 3d generative-adversarial modeling. Ad-
vances in neural information processing systems , 29, 2016.
2
[44] Zijie Wu, Yaonan Wang, Mingtao Feng, He Xie, and Ajmal
Mian. Sketch and text guided diffusion model for colored
point cloud generation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 8929–
8939, 2023. 2
[45] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang,
Song-Chun Zhu, and Ying Nian Wu. Learning descriptor
networks for 3d shape synthesis and analysis. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 8629–8638, 2018. 2
[46] Bangbang Yang, Wenqi Dong, Lin Ma, Wenbo Hu, Xiao Liu,
Zhaopeng Cui, and Yuewen Ma. Dreamspace: Dreaming
your room space with text-driven panoramic texture propa-
gation. arXiv preprint arXiv:2310.13119 , 2023. 1,7,8
[47] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge
Belongie, and Bharath Hariharan. Pointﬂow: 3d point cloud
generation with continuous normalizing ﬂows. In Proceed-
ings of the IEEE/CVF international conference on computer
vision , pages 4541–4550, 2019. 2
[48] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation
and completion through point-voxel diffusion. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 5826–5835, 2021. 2
4535
