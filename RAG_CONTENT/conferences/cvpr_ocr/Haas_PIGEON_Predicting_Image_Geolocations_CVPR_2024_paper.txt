PIGEON: Predicting Image Geolocations
Lukas Haas Michal Skreta Silas Alberti Chelsea Finn
Stanford University
Abstract
Planet-scale image geolocalization remains a challeng-
ing problem due to the diversity of images originating from
anywhere in the world. Although approaches based on
vision transformers have made signiﬁcant progress in ge-
olocalization accuracy, success in prior literature is con-
strained to narrow distributions of images of landmarks,
and performance has not generalized to unseen places. We
present a new geolocalization system that combines seman-
tic geocell creation, multi-task contrastive pretraining, and
a novel loss function. Additionally, our work is the ﬁrst
to perform retrieval over location clusters for guess reﬁne-
ments. We train two models for evaluations on street-level
data and general-purpose image geolocalization; the ﬁrst
model, PIGEON, is trained on data from the game of Ge-
oGuessr and is capable of placing over 40% of its guesses
within 25 kilometers of the target location globally. We also
develop a bot and deploy PIGEON in a blind experiment
against humans, ranking in the top 0.01% of players. We
further challenge one of the world’s foremost professional
GeoGuessr players to a series of six matches with millions
of viewers, winning all six games. Our second model, PI-
GEOTTO, differs in that it is trained on a dataset of images
from Flickr and Wikipedia, achieving state-of-the-art re-
sults on a wide range of image geolocalization benchmarks,
outperforming the previous SOTA by up to 7.7 percentage
points on the city accuracy level and up to 38.8 percent-
age points on the country level. Our ﬁndings suggest that
PIGEOTTO is the ﬁrst image geolocalization model that ef-
fectively generalizes to unseen places and that our approach
can pave the way for highly accurate, planet-scale image
geolocalization systems. Our code is available on GitHub.1
1. Introduction
The online game GeoGuessr has recently reached 65 mil-
lion players [ 22], attracting a worldwide crowd of users try-
1https://github.com/LukasHaas/PIGEON .ing to solve a single problem: given a Street View image
taken somewhere in the world, identify its location. The
problem of uncovering geographical coordinates from vi-
sual data is more formally known in computer vision as im-
age geolocalization, and, just like the game of GeoGuessr,
remains notoriously challenging. The scale and diversity
of our planet, seasonal appearance disturbance, and climate
change impacts are some among the many reasons why im-
age geolocalization remains an unsolved problem.
Over the past decade, researchers have advanced the
ﬁeld by casting image geolocalization as a classiﬁcation
task [ 38], developing hierarchical approaches to problem
modeling [ 7,25,27], as well as leveraging vision transform-
ers [7,27] and contrastive pretraining [ 23]. Yet despite this
progress, the most capable models have been highly depen-
dent on distributional alignments between training and test-
ing data, failing to generalize to more diverse datasets that
predominantly include unseen locations [ 7].
In this work, we present a two-pronged multi-task mod-
eling approach that both exhibits world-leading perfor-
mance in the game of GeoGuessr and achieves state-of-the-
art performance on a wide range of image geolocalization
benchmark datasets. First, we present PIGEON , a model
trained exclusively on planet-scale Street View data, tak-
ing a four-image panorama as input. PIGEON is the ﬁrst
computer vision model to reliably beat the most experi-
enced players in the game GeoGuessr, comfortably ranking
within the top 0.01% of players while also beating one of
the world’s best professional players in six out of six games
with millions of viewers. Our model achieves impressive
image geolocalization results on outdoor street-level images
globally, placing 40.4% of its geographic coordinate predic-
tions within a 25-kilometer radius of the correct location.
Subsequently, we evolve our model to PIGEOTTO
which differs from PIGEON in that it takes a single image
as input and is trained on a larger, highly diverse dataset of
over 4 million photos derived from Flickr and Wikipedia
and no Street View data. PIGEOTTO achieves state-of-
the-art results across a wide range of benchmark datasets,
including IM2GPS [ 14], IM2GPS3k [ 37], YFCC4k [ 37],
YFCC26k [ 25], and GWS15k [ 7]. The model slashes the
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12893
Figure 1. Prediction pipeline and main contributions of PIGEON . Administrative boundary and training set metadata are hierarchically
ranked, clustered, and V oronoi tessellated to create semantic geocells. The geocell labels are then used to create continuous labels via
haversine smoothing. Additionally, we pretrain CLIP via geographic synthetic captions in a multi-task setting. The pretrained CLIP
model together with an OPTICS clustering model are employed to generate location cluster representations. During inference, an image
embedding is computed and ﬁrst passed to a linear layer to create geocell predictions and to identify the topK geocell candidates. The
embedding is also used in our reﬁnement pipeline to reﬁne predictions within and across geocells. This is achieved by minimizing the
embedding L2-distance between the inference image embedding and all location cluster representations across the topK geocells. Finally,
predictions are reﬁned within the top identiﬁed cluster to generate geographic coordinates as outputs.
median distance error roughly in half on three benchmark
datasets and more than ﬁve times reduces the median er-
ror on GWS15k which includes images from predominantly
unseen locations. PIGEOTTO is the ﬁrst model that is ro-
bust to location and image distribution shifts by picking up
general locational cues in images as evidenced by the of-
ten double-digit percentage-point increase in performance
on larger evaluation radii. By performing well on out-
of-distribution datasets, PIGEOTTO closes a major gap in
prior literature that is essential for solving the problem of
image geolocalization.
As PIGEON and PIGEOTTO only differ in the training
data and hyperparameter settings, the efﬁcacy of our ap-
proach has important implications for planet-scale image
geolocalization. Our contributions of semantic geocells,
multi-task contrastive pretraining, a new loss function, and
downstream guess reﬁnement all contribute to minimizing
distance errors, as shown in our ablation studies in Sec-
tion4. Still, it is important that future research addresses the
safety of image geolocalization technologies, ensuring re-
sponsible progress in developing computer vision systems.2. Related work
2.1. Image geolocalization problem setting
Image geolocalization refers to the problem of mapping an
image to coordinates that identify where it was taken. This
problem, especially if planet-scale, remains a very challeng-
ing area of computer vision. Not only does a global problem
formulation render the problem intractable, but accurate im-
age geolocalization is also difﬁcult due to changes in day-
time, weather, seasons, time, illumination, climate, trafﬁc,
viewing angle, and many more factors.
The ﬁrst modern attempt at planet-scale image geolo-
calization is attributed to IM2GPS (2008) [ 14], a retrieval-
based approach based on hand-crafted features. Depen-
dence on nearest-neighbor retrieval methods [ 43] using
hand-crafted visual features [ 8] meant that an enormous
database of reference images would be necessary for accu-
rate planet-scale geolocalization, which is infeasible. Con-
sequently, subsequent work decided to restrict the geo-
graphic scope, focusing instead on speciﬁc cities [ 40] like
Orlando and Pittsburgh [ 42] or San Francisco [ 5]; speciﬁc
countries like the United States [ 32]; and even mountain
ranges [ 3,29,34], deserts [ 35], and beaches [ 6].
12894
2.2. Vision transformers and multi-task learning
With the advent of deep learning, methods in image ge-
olocalization shifted from hand-crafted features to end-
to-end learning [ 24]. In 2016, Google released the
PlaNet [ 38] paper that ﬁrst applied convolutional neural net-
works (CNNs) [ 19] to geolocalization. It also ﬁrst cast the
problem as a classiﬁcation task across “geocells” as a re-
sponse to research demonstrating that it was difﬁcult for
deep learning models to directly predict geographic coor-
dinates via regression [ 9,33]. This was due to the subtleties
in geographic data distributions and the complex interde-
pendence between latitudes and longitudes. The improve-
ments realized with deep learning led researchers to revisit
IM2GPS [ 37], apply CNNs to massive datasets of mobile
images [ 16], and deploy their models in the game of Ge-
oGuessr against human players [ 23,32]. Prior literature has
also combined classiﬁcation and retrieval approaches [ 18];
our work modernizes this approach via a hierarchical re-
trieval mechanism over location clusters, equivalent to pro-
totypical networks [ 31] with ﬁxed parameters.
Following the success of transformers [ 36] in natural lan-
guage processing, the transformer architecture found its ap-
plication in computer vision. Pretrained vision transform-
ers (ViT) [ 17] and multi-modal derivatives such as Ope-
nAI’s CLIP [ 28] and GPT-4V [ 26] have successfully been
deployed to image geolocalization [ 1,23,26,27,40,44].
Our approach is novel in that in pretrains CLIP speciﬁcally
for the task of image geolocalization in a multi-task fashion
via auxiliary geographic, demographic, and climate data.
Auxiliary data had previously been shown to aid in image
geolocalization [ 14,27], but our work is the ﬁrst to use aux-
iliary data for contrastive pretraining, retaining CLIP’s ex-
ceptional in-domain generalized zero-shot capabilities that
are critical for geolocalization performance [ 13].
2.3. Geocell partitioning
With image geolocalization framed as a classiﬁcation prob-
lem, the chosen method of partitioning the world into ge-
ographical classes, or “geocells“, can have an enormous
effect on downstream performance. Previous approaches
rely on geocells that are either plainly rectangular, rect-
angular while respecting the curvature of the Earth and
being roughly balanced in class size [ 25] (as is the case
of Google’s S2 library2), or geocells that are effectively
arbitrary as a result of combinatorial partitioning, initial-
izing cells randomly but adjusting their shapes based on
the training dataset distribution [ 30]. Hierarchical ap-
proaches to geocell creation like in individual scene net-
works (ISNs) [ 25,33] can help preserve semantic infor-
mation and exploit the hierarchical knowledge at different
2https://code.google.com/archive/p/s2-geometry-
library .geospatial resolutions, for instance by categorizing the geo-
cells at the city, region, and country levels.
While the semantic construction of geocells has been
found to be of high importance to image geolocaliza-
tion [ 33], even recently published papers continue to use the
S2 library [ 7,18,27]. One of the possible reasons for this
design choice is that for larger datasets, even the most gran-
ular semantic geocells contain too many data points, caus-
ing the classiﬁcation problem to be very imbalanced. Our
work addresses this limitation with a novel semantic geo-
cell creation method, combining hierarchical approaches
with clustering based on the training data distribution and
V oronoi tesselation as the missing link between the two. For
the ﬁrst time, our approach renders semantic geocells useful
for any dataset size and geographic distribution.
2.4. Additional work
Other notable academic work cites the efﬁcacy of cross-
view image geolocalization, especially for rural regions
with sparse, ground-level geo-tagged photos. Cross-view
approaches can combine land cover attributes and ground-
level and overhead imagery to increase robustness through
transfer learning [ 21,41,44]. Using land maps in par-
ticular is an important avenue for future research; in our
work, however, we aim to demonstrate our models’ perfor-
mance relying solely on ground-level images from diverse
settings.
3. Predicting image geolocations
Our image geolocalization system consists of both para-
metric and non-parametric components. This section ﬁrst
explains our data pre-processing pipeline and then walks
through how we frame geolocalization as a distance-aware
classiﬁcation problem. We then delineate our pretraining
and training stages, and ﬁnally describe how we reﬁne loca-
tion predictions to improve street-level guess performance.
3.1. Geocell creation
Contemporary methods all frame image geolocalization as
a classiﬁcation exercise, relying on geocells to discretize
the Earth’s surface into a set number of classes. Our work
experiments with two types of geocell creation methods.
Naive geocells. We ﬁrst employ naive, rectangular geo-
cells inspired by the S2 library, subdividing every geocell
until roughly balanced class sizes are reached. In contrast
to S2 partitioning, our rectangular geocells are not of equal
geographic size, creating even more balanced classes.
Semantic geocells. One limitation of the S2 library and
our naive geocells is that the geocell boundaries are com-
pletely arbitrary and thus meaningless in the context of
12895
image geolocalization. Ideally, each geocell should cap-
ture the distinctive characteristics of its enclosed geographic
area. Political and administrative boundaries serve this pur-
pose well as they often not only capture country or region-
speciﬁc information (i.e. road markings and street signs)
but also follow natural boundaries, such as the ﬂow of rivers
and mountain ranges which encode geological information.
Similar to Theiner et al. [ 33], we rely on planet-scale
open-source administrative data for our semantic geocell
design, drawing on non-overlapping political shapeﬁles of
three levels of administrative boundaries (country, admin 1,
and admin 2 levels) obtained from GADM [ 10]. Starting
at the most granular level (admin 2), our algorithm merges
adjacent admin 2 level polygons such that each geocell con-
tains at least a minimum number of training samples. Our
method attempts to preserve the hierarchy given by admin
1 level boundaries, never merges cells across country bor-
ders (deﬁned by distinct ISO country codes) and, in contrast
to Theiner et al. [ 33], allows for more granular hierarchies.
Figure 2shows an example of our semantic geocell design
preserving the semantics of urban and surrounding Paris.
(a) With naive, rectangular geocells.
 (b) With our semantic geocells.
Figure 2. Geocell speciﬁcations around Paris, France.
OPTICS clustering & Voronoi tessellation. We further
address a major limitation in the semantic geocell design
of Theiner et al. [ 33] which is that some admin 2 areas are
not ﬁne-grained enough to result in a balanced classiﬁca-
tion dataset. This is especially the case for large training
datasets where the number of training examples for a sin-
gle, urban admin 2 area might greatly exceed the minimum
class size, requiring admin 2 areas to be meaningfully split
further. An important observation is that the geographic dis-
tribution of our training data already gives us an indication
of how to meaningfully subdivide our geocells because it
clusters around popular places and landmarks. We extract
these clusters using the OPTICS clustering algorithm [ 2].
Finally, we assign all yet unassigned data points to their
nearest clusters and employ V oronoi tessellation to deﬁne
contiguous geocells for every extracted cluster.3.2. Hierarchical image geolocalization using
distance-based label smoothing
By discretizing the problem of image geolocalization, a
trade-off is created between the granularity of geocells and
predictive accuracy. More granular geocells enable ﬁne-
grained predictions but also result in the classiﬁcation prob-
lem becoming more difﬁcult due to a higher cardinality.
Prior literature addresses this problem by generating sep-
arate geolocalization predictions across multiple levels of
geographic granularity, reﬁning guesses at every subsequent
level [ 7,25,27]. Pramanick et al. [ 27] and Clark et al. [ 7]
further propose architectures that share some model param-
eters between different hierarchy levels, improving geolo-
calization performance. Surprisingly, all prior work suffers
from the same limitation: models ﬁguratively guess in the
blind as they do not know which geocells are located next
to each other, learning their representations in isolation.
Our approach addresses this major limitation and im-
proves upon prior work by sharing allparameters between
multiple, implicit levels of geographic hierarchies. We
achieve this through a new loss function that relates adja-
cent geocells to each other, biasing the label based on the
haversine distance which calculates the distance between
two points on the Earth’s surface in kilometers. Given two
points, p1=( 1, 1)andp2=( 2, 2)with longitude
 and latitude  , and the earth’s radius rin kilometers, we
deﬁne the haversine distance Hav (p1,p2)as follows:
Hav(p1,p2)=2 rarcsin0
@vuutsin2  2  1
2!
+c o s (  1)c o s (  2)s i n2  2  1
2!1
A
(1)
We then “haversine smooth” the original one-hot geocell
classiﬁcation label using this distance metric according to
the following equation for a given sample nand geocell i:
yn,i=e x p (  [Hav(gi,xn) Hav(gn,xn)]/⌧)(2)
where giare the centroid coordinates of the geocell polygon
of cell i,gnare the centroid coordinates of the true geo-
cell, xnare the true coordinates of the example for which
the label is computed, and ⌧is a temperature parameter
which is set to 75 for PIGEON and to 65 for PIGEOTTO
in our experiments. It is important to note that our “haver-
sine smoothing” is distinct from classical “label smoothing”
because labels are not decayed using a constant factor but
based on both the distance to the correct geocell and the true
location. Since for every training example, multiple geo-
cells will have a target yn,ithat is signiﬁcantly larger than
zero, our model simultaneously learns to predict the cor-
rect geocell as well as an even coarser level of geographic
granularity. We design the following loss function based on
haversine smoothing for a particular training sample n:
Ln= X
gi2Glog (pn,i)·exp✓
 Hav(gi,xn) Hav(gn,xn)
⌧◆
(3)
12896
where pn,iis the probability our model assigns to geocell
ifor sample n. An added beneﬁt of using the loss of Equa-
tion ( 3) is that it aids generalization because hierarchy deﬁ-
nitions vary across every training sample. Additionally, if a
sample lies close to the boundary of two geocells, this fact
will be reﬂected through approximately equal target labels
for these two geocells. This is especially helpful for larger,
often rural, geocells. Furthermore, because every target la-
belyn,iis now continuous and the difﬁculty of the classiﬁ-
cation problem can be freely adjusted using ⌧, an arbitrary
number of geocells can be employed as long as geocells are
still contextually meaningful and contain a minimum num-
ber of samples. Finally, we observe that our classiﬁcation
loss is now directly based on the distance to the true loca-
tionxnof a given sample while circumventing the regres-
sion difﬁculties encountered in prior literature [ 9,33].
(a) Without haversine smoothing.
 (b) With haversine smoothing.
Figure 3. Impact of applying haversine smoothing over neighbor-
ing geocells for a location in Accra, Ghana.
3.3. Contrastive pretraining for geolocalization
To generate visual representations to then project onto our
geocells, our architecture uses OpenAI’s CLIP ViT-L/14
336 model as a backbone which is a multi-modal model that
was pretrained on a dataset of 400 million images and cap-
tions [ 28]. The reason why we employ CLIP is that it has
been shown to perform exceptionally well in generalized
zero-shot learning setups [ 28], which is a desirable property
for image geolocalization of both seen andunseen places.
In our experiments, we add a linear layer on top of
CLIP’s vision encoder to predict geocells. For model ver-
sions with multiple image inputs (i.e. four-image panorama
for PIGEON), we average the embeddings of all images.
Averaging embeddings resulted in a superior performance
compared to combining multiple embeddings via multi-
head attention or additional transformer layers.
In Haas et al. [ 13], the authors demonstrate that contin-
uing the pretraining of CLIP using domain-speciﬁc, syn-
thetic captions derived from caption templates improves the
generalized zero-shot performance on image geolocaliza-
tion tasks. We further improve upon their method through
the continued pretraining of CLIP in a multi-task fashion.To this end, we augment our training datasets with geo-
graphic, climate, and directional auxiliary data, used to cre-
ate synthetic image captions by sampling caption compo-
nents from different category templates and concatenating
them. For PIGEOTTO, we use caption components based
on the location, climate, and trafﬁc direction. Meanwhile,
for PIGEON, the Street View metadata allows us to addi-
tionally infer compass directions and the season, the latter
included to avoid shortcut learning [ 12] (i.e. snow !polar
latitudes). Examples of caption components include:
•Location : “A photo I took in the region of Gauteng in South Africa.”
•Climate : “This location has a temperate oceanic climate.”
•Compass direction : “This photo is facing north.”
•Season (month) : “This photo was taken in December.”
•Trafﬁc : “In this location, people drive on the left side of the road.”
All the above caption components contain information
relevant for the geolocalization of an image. Consequently,
our continued contrastive pretraining creates an implicit
multi-task setting and ensures the model learns rich repre-
sentations of the data while learning features that are rele-
vant to the task of image geolocalization.
3.4. Multi-task learning with climate data
We also experiment with making our multi-task setup ex-
plicit by creating task-speciﬁc prediction heads for auxil-
iary labels, and adapt our loss function according to Equa-
tion ( 4), where Ln,loccorresponds to the loss in Equa-
tion ( 3). Our multi-task setup further includes a cross-
entropy classiﬁcation task ( Ln,climate ) of the 28 different
K¨oppen-Geiger climate zones [ 4], a cross-entropy month
(season) classiﬁcation task ( Ln,month), and six mean squared
error (MSE) regression tasks (combined into Ln,reg) that at-
tempt to predict values related to the temperature, precipita-
tion, elevation, and population density of a given location.
Ln=Ln,loc+↵Ln,climate + Ln,month + Ln,reg (4)
We unfreeze the last CLIP layer to allow for parame-
ter sharing across tasks with the goal of observing a posi-
tive transfer from our auxiliary tasks to our geolocalization
problem and to learn more general image representations
reducing the risk of overﬁtting to the training dataset. Ad-
justing ↵,  , and  , our loss function weighs the geolocal-
ization task as much as all auxiliary tasks combined consid-
ering each task’s loss magnitude. A novel contribution of
our work is that we use a total of eight auxiliary prediction
tasks instead of just two compared to prior research [ 27].
3.5. Reﬁnement via location cluster retrieval
To further reﬁne our model’s guesses within a geocell and to
improve street- and city-level performance, instead of sim-
ply predicting the mean latitude and longitude of all points
within a geocell [ 27], we perform intra-geocell reﬁnement.
12897
To this end, we design a hierarchical retrieval mechanism
over location clusters akin to prototypical networks [ 31]
with ﬁxed parameters. We again use the OPTICS clustering
algorithm [ 2] to cluster all points within a geocell gand thus
propose location clusters Cgwhose representation is the av-
erage of all corresponding image embeddings. To compute
all image embeddings, we use our pretrained CLIP model
f(·)described in Section 3.3, mapping each image lin a
cluster cto its embedding f(l).
c⇤= arg min
c2Cg     f(x) 1
|c|X
l2cf(l)     
2(5)
During inference, we predict the location cluster c⇤of
an input image xby selecting the cluster with the mini-
mum Euclidean image embedding distance to the input im-
age embedding f(x). Once the cluster c⇤is determined, we
further reﬁne our guess by choosing the single best location
within the cluster, again via minimizing the Euclidean em-
bedding distance. The retrieval over location clusters and
within-cluster reﬁnement add two additional levels of pre-
diction hierarchy to our system, with the number of unique
potential guesses equaling the training dataset size.
While hierarchical reﬁnement via retrieval is in itself a
novel idea, our work goes one step further. Instead of re-
ﬁning a geolocalization prediction within a single cell, our
mechanism optimizes across multiple cells which further
increases performance. During inference, our geocell clas-
siﬁcation model outputs the topK predicted geocells (5 for
PIGEON, 40 for PIGEOTTO) as well as the model’s associ-
ated probabilities for these cells. The reﬁnement model then
picks the most likely location within each of the topK pro-
posed geocells, after which a softmax is computed across
thetopK Euclidean image embedding distances. We use
a temperature softmax with a temperature that is carefully
calibrated on the validation datasets to balance probabilities
across different geocells. Finally, these reﬁnement proba-
bilities are multiplied with the initial topK geocell probabil-
ities to determine a ﬁnal location cluster and within-cluster
reﬁnement is performed as illustrated in Figure 1.
4. Experimental results and analysis
4.1. Experimental setting
Training PIGEON and PIGEOTTO. Based on our tech-
nical methodology outlined in Section 3, we train two mod-
els for distinct downstream evaluation purposes.
First, inspired by GeoGuessr, we train PIGEON (Predict-
ing Image Geolocations). We collect an original dataset
of 100,000 randomly sampled locations from GeoGuessr
and download a set of four images spanning an entire
“panorama” in a given location, or a 360-degree view, for
a total of 400,000 training images. For each location, westart with a random compass direction and take four images
separated by 90 degrees, carefully creating non-overlapping
image patches.
Second, motivated by PIGEON’s image geolocalization
capabilities, we train PIGEOTTO (Predicting Image Geolo-
cations with Omni-Terrain Training Optimizations). Un-
like PIGEON, PIGEOTTO is not a Street View photo lo-
calizer but rather a general image geolocator. To that end,
we access the MediaEval 2016 dataset [ 20] consisting of
geo-tagged Flickr images from all over the world and ob-
tain 4,166,186 images, considering that some images have
become unavailable since 2016. Additionally, recognizing
the importance of geolocating landmarks for general image
geolocalization capabilities, we add 340,579 images from
the Google Landmarks v2 dataset [ 39] to our training mix
which are all derived from Wikipedia. Importantly, there is
no overlap in the training data we use between PIGEON and
PIGEOTTO, as the models serve different downstream pur-
poses. Unlike PIGEON, PIGEOTTO takes a single image
per location as input, as obtaining a four-image panorama is
often infeasible in general image geolocalization settings.
Evaluation datasets and metrics. Our work deﬁnes the
median distance error to the correct location as the primary
and composite metric. In line with the prior literature on
image geolocalization, we further evaluate the “% @ km”
statistic in our analysis as a more ﬁne-grained metric. For
a given dataset, the “% @ km” statistic determines the per-
centage of guesses that fall within a given kilometer-based
distance from the ground-truth location. Just as in the prior
work, we evaluate ﬁve distance radii: 1 km (roughly street-
level accuracy), 25 km (city-level), 200 km (region-level),
750 km (country-level), and 2,500 km (continent-level).
For PIGEON, we run evaluations on a holdout dataset
collected from GeoGuessr consisting of 5,000 Street View
locations. We separately conduct extensive blind exper-
iments in GeoGuessr deploying PIGEON against human
players with varying degrees of expertise as well as a sep-
arate match against a world-class professional player. To
quantify which parts of our modeling setup impact perfor-
mance, we further run eight separate ablation studies.
For PIGEOTTO, we focus our evaluations squarely on
the benchmark datasets that are established in the litera-
ture. Namely, we look at IM2GPS [ 14], IM2GPS3k [ 37],
YFCC4k [ 37] and YFCC26k [ 25] (based on the MediaEval
2016 dataset [ 20]), and GWS15k [ 7]. As the last dataset
has not been publicly released by the time of this writing,
we reconstruct the dataset by exactly replicating the dataset
generation procedure outlined in Clark et al. [ 7].
4.2. Street View evaluation with PIGEON
We present the results of our evaluations of PIGEON and
ablations of our contributions in Table 1and Table 2.
12898
Table 1. Cumulative ablation study of our image geolocalization
system on a holdout dataset of 5,000 Street View locations.
Country Mean Median GeoGuessr
Ablation Accuracy Error Error Score
% km km points
PIGEON 91.96 251.6 44.35 4,525
 Freezing Last CLIP Layer After Pretraining 91.82 255.1 45.47 4,531
 Hierarchical Guess Reﬁnement 91.14 251.9 50.01 4,522
 Contrastive CLIP Pretraining 89.36 316.9 55.51 4,464
 Semantic Geocells 87.96 299.9 60.63 4,454
 Multi-task Prediction Heads 87.90 312.7 61.81 4,442
 Fine-tuning Last CLIP Layer 87.64 315.7 60.81 4,442
 Four-image Panorama 74.74 877.4 131.1 3,986
 Haversine Smoothing 72.12 990.0 148.0 3,890
Table 2. Cumulative ablation study using ﬁve common distance
radii on a holdout dataset of 5,000 Street View locations.
Distance (% @ km)
Ablation Street City Region Country Continent
1 km 25 km 200 km 750 km 2,500 km
PIGEON 5.36 40.36 78.28 94.52 98.56
 Freezing Last CLIP Layer After Pretraining 4.84 39.86 78.98 94.76 98.48
 Hierarchical Guess Reﬁnement 1.32 34.96 78.48 94.82 98.48
 Contrastive CLIP Pretraining 1.24 34.54 76.36 93.36 97.94
 Semantic Geocells 1.18 33.22 75.42 93.42 98.16
 Multi-task Prediction Heads 1.10 32.74 75.14 93.00 97.98
 Fine-tuning Last CLIP Layer 1.10 32.50 75.32 92.92 98.00
 Four-image Panorama 0.92 24.18 59.04 82.84 92.76
 Haversine Smoothing 1.28 24.08 55.38 80.20 92.00
As evidenced by our results, each subsequent ablation
deteriorates most metrics, pointing to the synergistic nature
of the ensemble of methods in our geolocalization system.
Starting from the very bottom of both tables, correspond-
ing to a simple CLIP vision encoder plus a geocell predic-
tion head, we can see that with the introduction of haver-
sine smoothing, the mean distance error decreases by 112.6
kilometers from 990.0 to 877.4 kilometers. The bulkiest
performance lift, however, comes from the introduction of a
four-image panorama instead of a single image, increasing
our country accuracy by 12.9 percentage points and more
than halving our median kilometer error from 131.1 to 60.8
kilometers. While ﬁne-tuning the last CLIP layer and shar-
ing parameters in a multi-task setting slightly improves the
performance of our model, the uplift is much more palpa-
ble with the introduction of our semantic geocells, reduc-
ing the median error from 60.6 to 55.5 kilometers. When
we additionally pretrain CLIP via our synthetic captions,
we gain another 1.7 percentage points in long-range coun-
try accuracy. Complemented by our hierarchical location
cluster reﬁnement, we improve short-range street-level ac-
curacy from 1.3% to 4.8%. Finally, we freeze the last CLIP
layer again and thus prevent parameter sharing between our
geocell and multi-task prediction heads, given that our pre-
training procedure already incorporates multi-task training.
This results in PIGEON’s ﬁnal metrics of a 92.0% country
accuracy and a median distance error of 44.4 kilometers.
Beyond our ablations, we compare PIGEON’s perfor-
mance to humans in the game of GeoGuessr. To do so,
we develop a Chrome extension bot that has access to PI-
Figure 4. Geolocalization error of PIGEON against human play-
ers of various in-game skill levels across 458 multi-round matches.
The Champion Division consists of the top 0.01% of players. PI-
GEON’s error is higher than in Table 1because GeoGuessr round
difﬁculties are adjusted dynamically, increasing with every round.
GEON as an API and deploy our system in a blind ex-
periment across 458 matches, each consisting of multiple
rounds. PIGEON comfortably outperforms players in Ge-
oGuessr’s Champion Division, consisting of the top 0.01%
of human players. The results are shown in Figure 4, under-
scoring PIGEON’s ability to beat players of all skill levels.
Notably, top GeoGuessr players perform orders of magni-
tudes better than the players evaluated in Seo et al. [ 30].
For our ﬁnal evaluation, we challenge one of the world’s
foremost professional GeoGuessr players to a match and
win six out of six planet-scale, multi-round games.3PI-
GEON is the ﬁrst model to reliably beat a GeoGuessr pro-
fessional.
4.3. Benchmark evaluation with PIGEOTTO
The results of our evaluations of PIGEOTTO on benchmark
datasets are displayed in Table 3. PIGEOTTO achieves
state-of-the-art (SOTA) performance on every single bench-
mark dataset and on the majority of distance-based gran-
ularities. On IM2GPS, it is able to improve the state of
the art on both country-level and continent-level accuracy
by 2 percentage points or more. Its relative underper-
formance on smaller granularities can be attributed to the
landmark-only nature of IM2GPS and its small size of 237
images. On a larger and more general dataset, IM2GPS3k,
PIGEOTTO performs much better, achieving SOTA perfor-
mance on all but the street-level metric, with an impressive
11.4 percentage-point improvement on the country level
and a much lower median error of 147.3 kilometers. Mean-
while, on YFCC4k and YFCC26k, PIGEOTTO is able to
outperform the current state of the art on 9 out of 10 metrics,
including by 12.2 percentage points on the country level
on YFCC4k and by 13.6 percentage points on YFCC26k,
more than halving the previous SOTA median error. Fi-
nally, we see very signiﬁcant improvements on the most re-
cently released benchmark, GWS15k, consisting entirely of
3https://www.youtube.com/watch?v=ts5lPDV--cU .
12899
Table 3. Comparison of PIGEOTTO’s results against other models
on benchmark datasets. PIGEOTTO reduces the median kilometer
error by 2-5x on benchmarks not solely focused on landmarks.
Benchmark MethodMedian Distance (% @ km)
Error Street City Region Country Continent
km 1 km 25 km 200 km 750 km 2,500 km
IM2GPS [14]PlaNet [ 38] >200 8.4 24.5 37.6 53.6 71.3
CPlaNet [ 30] >200 16.5 37.1 46.4 62.0 78.5
ISNs(M, f⇤,S3)[25] >25 16.9 43.0 51.9 66.7 80.2
Translocator [ 27] >25 19.9 48.1 64.6 75.6 86.7
GeoDecoder [ 7] ⇠25 22.1 50.2 69.0 80.0 89.1
PIGEOTTO (Ours) 70.5 14.8 40.9 63.3 82.3 91.1
 (%points ) -7.3 -9.3 -5.7 +2.3 +2.0
IM2GPS3k [37]PlaNet [ 38] >750 8.5 24.8 34.3 48.4 64.6
CPlaNet [ 30] >750 10.2 26.5 34.6 48.6 64.6
ISNs(M, f⇤,S3)[25] ⇠750 10.5 28.0 36.6 49.7 66.0
Translocator [ 27] >200 11.8 31.1 46.7 58.9 80.1
GeoDecoder [ 7] >200 12.8 33.5 45.9 61.0 76.1
PIGEOTTO (Ours) 147.3 11.3 36.7 53.8 72.4 85.3
 (%points ) -1.5 +3.2 +7.9 +11.4 +9.2
YFCC4k [37]PlaNet [ 38] >750 5.6 14.3 22.2 36.4 55.8
CPlaNet [ 30] >750 7.9 14.8 21.9 36.4 55.5
ISNs(M, f⇤,S3)[25] >750 6.7 16.5 24.2 37.5 54.9
Translocator [ 27] >750 8.4 18.6 27.0 41.1 60.4
GeoDecoder [ 7] ⇠750 10.3 24.4 33.9 50.0 68.7
PIGEOTTO (Ours) 383.0 10.4 23.7 40.6 62.2 77.7
 (%points ) +0.1 -0.7 +6.7 +12.2 +9.0
YFCC26k [25]PlaNet [ 38] >2,500 4.4 11.0 16.9 28.5 47.7
ISNs(M, f⇤,S3)[25]⇠2,500 5.3 12.3 19.0 31.9 50.7
Translocator [ 27] >750 7.2 17.8 28.0 41.3 60.6
GeoDecoder [ 7] ⇠750 10.1 23.9 34.1 49.6 69.0
PIGEOTTO (Ours) 333.3 10.5 25.8 42.7 63.2 79.0
 (%points ) +0.4 +1.9 +8.6 +13.6 +10.0
GWS15k [7]ISNs(M, f⇤,S3)[25]>2,500 0.05 0.6 4.2 15.5 38.5
Translocator [ 27] >2,500 0.5 1.1 8.0 25.5 48.3
GeoDecoder [ 7] ⇠2,500 0.7 1.5 8.7 26.9 50.5
PIGEOTTO (Ours) 415.4 0.7 9.2 31.2 65.7 85.1
 (%points ) +0.0 +7.7 +22.5 +38.8 +34.6
Street View images. Crucially, GWS15k is the most difﬁ-
cult dataset in the benchmark set. If we deﬁne images to
be taken in the same location if they are less than 100 me-
ters apart, 92% of locations in GWS15k are not taken in the
same location as any MediaEval 2016 [ 20] training data on
which prior SOTA models and our system were trained. For
comparison, this number ranges from 23% to 42% for the
other four benchmark datasets, underscoring the unique dif-
ﬁculty of GWS15k. Noting that PIGEOTTO was not trained
on any Street View images, this suggests that PIGEOTTO is
truly planet-scale in nature, exhibits robust behavior to dis-
tribution shifts, and is the ﬁrst geolocalization model that
effectively generalizes to unseen places.
5. Ethical considerations
Image geolocalization represents a sub-discipline of com-
puter vision that comes with both potential beneﬁts to soci-
ety as well as with risks of misuse. While prior work in the
ﬁeld addresses ethical implications scantily, we believe that
the potential misuse and negative downstream implications
of image geolocalization systems afford a separate discus-
sion section in this paper.
On the one hand, accurate geo-tagging of images opens
up possibilities for various beneﬁcial applications, far be-
yond the game of GeoGuessr, including helping to under-
stand changes to particular locations over time. Image ge-olocalization has found use cases in autonomous driving,
navigation, geography education, open-source intelligence,
and visual investigations in journalism.
On the other hand, however, applications of image ge-
olocalization may come with risks, especially if the preci-
sion of such systems signiﬁcantly improves in the future. To
our knowledge, this is the ﬁrst state-of-the-art image geolo-
calization paper in the last ﬁve years that is not funded by
military contracts. Recently published work has been sup-
ported by grants from the Department of Defense [ 27] and
the US Army [ 7]. Any attempts to develop image geolocal-
ization technology for military use cases should come under
particular scrutiny. There are also privacy risks involved;
for instance, some methods using Street View images have
been shown to be capable of inferring local income, race,
education, and voting patterns [ 11].
Image geolocalization technologies come with dual-use
risks [ 15], and efforts need to be made to minimize harmful
consequences. To that end, we decide not to release model
weights publicly and only release our code for academic
validation. While a major limitation of today’s image ge-
olocalization technologies (including ours) is that they are
unable to make street-level predictions reliably, researchers
ought to carefully consider the risk of potential misuse of
their work as such technologies get increasingly precise.
6. Conclusion
We propose a novel deep multi-task approach for planet-
scale image geolocalization that achieves state-of-the-art
benchmark results while being robust to distribution shifts.
To conﬁrm the efﬁcacy of our approach, we train and
evaluate two distinct image geolocalization models. First,
we gather a global Street View dataset to train PIGEON, a
multi-task model that places into the top 0.01% of human
players in the game of GeoGuessr. On a holdout dataset of
5,000 Street View locations, 40.4% of PIGEON’s predic-
tions of geographic coordinates land within a 25-kilometer
radius of the ground-truth location. Subsequently, we as-
semble a planet-scale dataset of over 4 million images de-
rived from Flickr and Wikipedia to train the more general
PIGEOTTO, improving the state of the art on a wide range
of geolocalization benchmark datasets by a large margin.
Going forward, it remains to be seen whether ap-
plied image geolocalization technologies will be truly
planet-scale or focused on a well-deﬁned narrow distri-
bution. In any case, our ﬁndings about the importance
of semantic geocell creation, multimodal contrastive
pretraining, and precise intra-geocell reﬁnement, among
others, point to important building blocks for such sys-
tems. Nevertheless, deployment of any downstream
image geolocalization technology will need to balance
potential beneﬁts with possible risks, ensuring the re-
sponsible development of future computer vision systems.
12900
References
[1]Sandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Rad-
ford, Jong Wook Kim, and Miles Brundage. Evaluating
CLIP: Towards Characterization of Broader Capabilities and
Downstream Implications, 2021. 3
[2]Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel,
and J ¨org Sander. OPTICS: Ordering Points to Identify the
Clustering Structure. In Proceedings of the 1999 ACM SIG-
MOD International Conference on Management of Data ,
page 49–60, New York, NY , USA, 1999. Association for
Computing Machinery. 4,6
[3]Georges Baatz, Olivier Saurer, Kevin K ¨oser, and Marc Polle-
feys. Large Scale Visual Geo-Localization of Images in
Mountainous Terrain. 2
[4]Hylke E. Beck, Niklaus E. Zimmermann, Tim R. McVicar,
Noemi Vergopolan, Alexis Berg, and Eric F. Wood. Present
and future K ¨oppen-Geiger climate classiﬁcation maps at 1-
km resolution. Scientiﬁc Data , 5(1):180214, 2018. 5,3,4
[5]Gabriele Berton, Carlo Masone, and Barbara Caputo. Re-
thinking Visual Geo-localization for Large-Scale Applica-
tions, 2022. 2
[6]Liangliang Cao, John R. Smith, Zhen Wen, Zhijun Yin, Xin
Jin, and Jiawei Han. BlueFinder: Estimate Where a Beach
Photo Was Taken. In Proceedings of the 21st International
Conference on World Wide Web , page 469–470, New York,
NY , USA, 2012. Association for Computing Machinery. 2
[7]Brandon Clark, Alec Kerrigan, Parth Parag Kulkarni, Vi-
cente Vivanco Cepeda, and Mubarak Shah. Where We Are
and What We’re Looking At: Query Based Worldwide Im-
age Geo-localization Using Hierarchies and Scenes, 2023. 1,
3,4,6,8
[8]David Crandall, Lars Backstrom, Daniel Huttenlocher, and
Jon Kleinberg. Mapping the World’s Photos. In WWW ’09:
Proceedings of the 18th International Conference on World
Wide Web , pages 761–880, 2009. 2
[9]Alexandre de Br ´ebisson, ´Etienne Simon, Alex Auvolat, Pas-
cal Vincent, and Yoshua Bengio. Artiﬁcial Neural Networks
Applied to Taxi Destination Prediction, 2015. 3,5
[10] GADM. GADM Version 4.1, 2022. 4,1,3
[11] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen,
Jia Deng, Erez Lieberman Aiden, and Li Fei-Fei. Using
deep learning and Google Street View to estimate the demo-
graphic makeup of neighborhoods across the United States.
Proceedings of the National Academy of Sciences , 114(50):
13108–13113, 2017. 8
[12] Robert Geirhos, J ¨orn-Henrik Jacobsen, Claudio Michaelis,
Richard Zemel, Wieland Brendel, Matthias Bethge, and Fe-
lix A. Wichmann. Shortcut learning in deep neural networks.
Nature Machine Intelligence , 2(11):665–673, 2020. 5
[13] Lukas Haas, Silas Alberti, and Michal Skreta. Learning gen-
eralized zero-shot learners for open-domain image geolocal-
ization, 2023. 3,5
[14] James Hays and Alexei A. Efros. IM2GPS: estimating geo-
graphic information from a single image. In Proceedings of
the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) , 2008. 1,2,3,6,8[15] Peter Henderson, Eric Mitchell, Christopher D. Manning,
Dan Jurafsky, and Chelsea Finn. Self-destructing models:
Increasing the costs of harmful dual uses of foundation mod-
els, 2023. 8
[16] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. MobileNets: Efﬁcient Con-
volutional Neural Networks for Mobile Vision Applications,
2017. 3
[17] Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weis-
senborn, Georg Heigold, Jakob Uszkoreit, Lucas Beyer,
Matthias Minderer, Mostafa Dehghani, Neil Houlsby, Syl-
vain Gelly, Thomas Unterthiner, and Xiaohua Zhai. An im-
age is worth 16x16 words: Transformers for image recogni-
tion at scale. 2021. 3
[18] Giorgos Kordopatis-Zilos, Panagiotis Galopoulos, Symeon
Papadopoulos, and Ioannis Kompatsiaris. Leveraging Efﬁ-
cientNet and Contrastive Learning for Accurate Global-scale
Location Estimation, 2021. 3
[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-
ageNet Classiﬁcation with Deep Convolutional Neural Net-
works. In Advances in Neural Information Processing Sys-
tems. Curran Associates, Inc., 2012. 3
[20] Martha Larson, Mohammad Soleymani, Guillaume Gravier,
Bogdan Ionescu, and Gareth J.F. Jones. The Benchmarking
Initiative for Multimedia Evaluation: MediaEval 2016. IEEE
MultiMedia , 24(1):93–96, 2017. 6,8,2,3
[21] Tsung-Yi Lin, Serge Belongie, and James Hays. Cross-View
Image Geolocalization. In 2013 IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 891–898, 2013.
3
[22] Jessica Lucas. A Geography Game Has Its First Superstar.
Can It Survive Its First Player Revolt?, 2023. 1
[23] Grace Luo, Giscard Biamby, Trevor Darrell, Daniel Fried,
and Anna Rohrbach. Gˆ3: Geolocation via Guidebook
Grounding. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2022 , pages 5841–5853, Abu
Dhabi, United Arab Emirates, 2022. Association for Com-
putational Linguistics. 1,3
[24] Carlo Masone and Barbara Caputo. A Survey on Deep Visual
Place Recognition. IEEE Access , 9:19516–19547, 2021. 3
[25] Eric M ¨uller-Budack, Kader Pustu-Iren, and Ralph Ewerth.
Geolocation Estimation of Photos Using a Hierarchical
Model and Scene Classiﬁcation. In Computer Vision – ECCV
2018 , pages 575–592, Cham, 2018. Springer International
Publishing. 1,3,4,6,8,9
[26] OpenAI. GPT-4V(ision) System Card, 2023. 3
[27] Shraman Pramanick, Ewa M. Nowara, Joshua Gleason, Car-
los D. Castillo, and Rama Chellappa. Where in the World is
this Image? Transformer-based Geo-localization in the Wild,
2022. 1,3,4,5,8
[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning Transferable Visual
Models From Natural Language Supervision, 2021. 3,5
[29] Olivier Saurer, Georges Baatz, Kevin K ¨oser, L’ubor Ladick ´y,
and Marc Pollefeys. Image Based Geo-localization in the
12901
Alps. International Journal of Computer Vision , 116(3):
213–225, 2016. 2
[30] Paul Hongsuck Seo, Tobias Weyand, Jack Sim, and Bohyung
Han. CPlaNet: Enhancing Image Geolocalization by Com-
binatorial Partitioning of Maps, 2018. 3,7,8
[31] Jake Snell, Kevin Swersky, and Richard S. Zemel.
Prototypical Networks for Few-shot Learning. CoRR ,
abs/1703.05175, 2017. 3,6
[32] Sudharshan Suresh, Nathaniel Chodosh, and Montiel Abello.
DeepGeo: Photo Localization with Deep Neural Network,
2018. 2,3
[33] Jonas Theiner, Eric Mueller-Budack, and Ralph Ewerth. In-
terpretable Semantic Photo Geolocation, 2021. 3,4,5,1
[34] Jan Tome ˇsek, Martin ˇCad´ık, and Jan Brejcha. CrossLocate:
Cross-Modal Large-Scale Visual Geo-Localization in Natu-
ral Environments Using Rendered Modalities. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision (WACV) , pages 3174–3183, 2022. 2
[35] Eric Tzeng, Andrew Zhai, Matthew Clements, Raphael
Townshend, and Avideh Zakhor. User-Driven Geolocation
of Untagged Desert Imagery Using Digital Elevation Mod-
els. In 2013 IEEE Conference on Computer Vision and Pat-
tern Recognition Workshops , pages 237–244, 2013. 2
[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention Is All You Need, 2017. 3
[37] Nam V o, Nathan Jacobs, and James Hays. Revisiting
IM2GPS in the Deep Learning Era, 2017. 1,3,6,8
[38] Tobias Weyand, Ilya Kostrikov, and James Philbin. PlaNet -
Photo Geolocation with Convolutional Neural Networks. In
European Conference on Computer Vision (ECCV) , 2016. 1,
3,8
[39] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.
Google Landmarks Dataset v2 – A Large-Scale Benchmark
for Instance-Level Recognition and Retrieval, 2020. 6,3
[40] Meiliu Wu and Qunying Huang. IM2City: Image Geo-
Localization via Multi-Modal Learning. In Proceedings of
the 5th ACM SIGSPATIAL International Workshop on AI for
Geographic Knowledge Discovery , page 50–61, New York,
NY , USA, 2022. Association for Computing Machinery. 2,3
[41] Hongji Yang, Xiufan Lu, and Yingying Zhu. Cross-view
Geo-localization with Layer-to-Layer Transformer. In Ad-
vances in Neural Information Processing Systems , pages
29009–29020. Curran Associates, Inc., 2021. 3
[42] Amir Roshan Zamir and Mubarak Shah. Accurate Image Lo-
calization Based on Google Maps Street View. In Computer
Vision – ECCV 2010 , pages 255–268, Berlin, Heidelberg,
2010. Springer Berlin Heidelberg. 2
[43] Amir Roshan Zamir and Mubarak Shah. Image Geo-
Localization Based on Multiple Nearest Neighbor Feature
Matching UsingGeneralized Graphs. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 36(8):1546–
1558, 2014. 2
[44] Sijie Zhu, Mubarak Shah, and Chen Chen. TransGeo:
Transformer Is All You Need for Cross-view Image Geo-
localization, 2022. 3
12902
