AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute
Decomposition and Indexing
Fan Yang1Tianyi Chen2Xiaosheng He1Zhongang Cai1,3
Lei Yang3Si Wu2Guosheng Lin1*
1S-Lab, Nanyang Technological University
2School of Computer Science and Engineering, South China University of Technology
3SenseTime Research
fan007@e.ntu.edu.sg,csttychen@mail.scut.edu.cn, gslin@ntu.edu.sg
Abstract
Editable 3D-aware generation, which supports user-
interacted editing, has witnessed rapid development re-
cently. However, existing editable 3D GANs either fail to
achieve high-accuracy local editing or suffer from huge
computational costs. We propose AttriHuman-3D, an ed-
itable 3D human generation model, which address the
aforementioned problems with attribute decomposition and
indexing. The core idea of the proposed model is to generate
all attributes (e.g. human body, hair, clothes and so on) in
an overall attribute space with six feature planes, which are
then decomposed and manipulated with different attribute
indexes. To precisely extract features of different attributes
from the generated feature planes, we propose a novel at-
tribute indexing method as well as an orthogonal projec-
tion regularization to enhance the disentanglement. We also
introduce a hyper-latent training strategy and an attribute-
specific sampling strategy to avoid style entanglement and
misleading punishment from the discriminator. Our method
allows users to interactively edit selected attributes in the
generated 3D human avatars while keeping others fixed.
Both qualitative and quantitative experiments demonstrate
that our model provides a strong disentanglement between
different attributes, allows fine-grained image editing and
generates high-quality 3D human avatars.
1. Introduction
As the increasing demand for digital content creation in
various domains such as gaming, virtual reality and e-
commerce, the need for realistic and customizable 3D hu-
man avatars has grown significantly. Traditional 2D ed-
itable generation models control the generation style with
*Guosheng Lin is the corresponding author.the manipulation of high-level style latent codes [18–20],
which is ambiguous and hard to achieve precise local edit-
ing, limiting its application in the real-world scenario. Re-
cently, several approaches have been proposed to enable
user manipulation on the free-view generation results of 3D-
aware GANs for rigid objects [22, 34, 35] (e.g. human faces
and cars). However, editable 3D-aware generation for hu-
man avatars could be more challenging due to its high vari-
ance in human geometry and appearance.
Some existing editable 3D-aware GANs for human por-
traits represent the overall regions of the generated objects
with a single NeRF and perform local editing by asking
users to manually edit the generated semantic masks [34,
35]. Although being very efficient, these methods suffer
from the high variance of human shapes, poses and out-
fits. Directly manipulating the semantic masks requires
strong painting skills for the users and involve a lot of te-
dious manual works, making it hard to be applied in the
real-world scenarios. One possible solution is to model and
manipulate different semantic regions independently. CN-
eRF [22] proposes to model each semantic region with dif-
ferent generators. Although generating different semantic
parts independently, training several deep generators in par-
allel will extremely increase the memory footprint, hinder-
ing their applications in real-world scenarios. Moreover,
currently available human datasets are limited and almost
all of them suffer from the style entanglement between dif-
ferent attributes, which means the style of one attribute may
be highly influenced by the style of another attribute. For
example, some pairs always show in the existing datasets:
dresses always with high-heel shoes, t-shirts always with
sports shoes. Men rarely with dresses. Such correlation
may also be learned by the discriminator, leading to mis-
leading punishment in the training process and artifacts in
the editing stage.
To address these problems, we propose AttriHuman-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10596
Figure 1. Our AttriHuman-3D achieves strong disentanglement between different attributes, generates high-quality view-consistent 3D
human avatars which allows fine-grained editing. From left to right, we show the generation results by editing different attributes, try-on
results by modifying the selected attribute sets and generation results of view-consistent images.
3D, an editable 3D human generation model with attribute
decomposition and indexing. Specifically, we propose to
generate all attributes (e.g. hair, clothes, main body and
so on) in an overall feature space with six feature planes
and then extract independent attribute-specific features with
different attribute indexes. Different from CNeRF [22]
that generates each attribute with an independent genera-
tor, our method benefits from the ”all-to-one” design, fa-
cilitating the information sharing across different attributes
and achieving much higher computational efficiency. Our
method is inspired by the recently proposed tensor decom-
position technologies in 4D dynamic NeRFs [3, 9], we for-
mulate the generation space as a 4D space-attribute field
which could be decomposed into six feature planes and effi-
ciently generate with a single 2D CNN-based feature gener-
ator. However, trivially extending the tensor decomposition
technologies in space-time field into space-attribute domain
leads to degraded generation results. Different from time
dimension which is continuous and has clear definition, the
attribute dimension could be disjoint and ambiguous, mak-
ing it hard to define the index of different attributes. To
address this problem, we further propose an implicit index-
ing module to learn the index of each human attribute in the
attribute dimension. To enhance the disentanglement of dif-
ferent attributes, we further propose an orthogonal projec-
tion regularization to enforce the orthogonality of differentindexes, which enables us to extract independent attribute
features from the generated feature planes.
To address the implicit style entanglement between dif-
ferent attributes in the existing datasets, we introduce a
hyper-latent training strategy which conditions the overall
training progress with hyper-latent label to avoid the mis-
leading punishment from the discriminator and an attribute-
specific sampling strategy to split different attributes with
pre-defined sampling bounding boxes to remove the influ-
ence of other attributes defined in different bounding boxes.
Qualitative and quantitative experiments conducted on
the fashion datasets demonstrate that our model provides a
strong disentanglement between different attributes, allows
fine-grained image editing and generates high-quality view-
consistent 3D human avatars. In summary, our main contri-
butions can be listed as follows: 1) We propose a novel ed-
itable 3D human avatar generation model, which achieves
fully disentangled control over the generated human avatars
with attribute decomposition and indexing. 2) We propose a
novel implicit indexing method with an orthogonal projec-
tion regularization to enhance the disentanglement of dif-
ferent attributes. 3) We introduce a hyper-latent training
strategy and an attribute-specific sampling strategy to ad-
dress the style entanglement between different attributes in
the existing human datasets, leading to better editing per-
formance.
10597
2. Related Work
2.1. 3D-Aware GANs
Generative Adversarial Networks (GANs) [11] have
achieved remarkable success in the field of 2D image syn-
thesis [2, 17–19, 37]. Recently, the implicit neural repre-
sentation have been widely explored to be combined with
GANs to learn 3D-aware image synthesis with only 2D
images as supervision [5, 6, 25, 27, 32, 33]. Particu-
larly, GRAF [31] and Pi-GAN [5] propose to learn 3D-
aware image and geometry generation with implicit neu-
ral radiance fields as generators. However, these meth-
ods are limited to synthesis only low-resolution images due
to the large computational costs of the MLP querying in
the volumne rendering process. To solve this problem,
recent methods tend to adopt a two-stage generation pro-
cess by firstly rendering low-resolution images and then
passing it to 2D CNN-based decoders to generate images
with higher resolution [6, 25, 27]. Although providing
high-quality generation of 3D-aware images, these meth-
ods fail to support user-interacted local editings. In the task
of 3D human avatar generation, most of the recent works
generate the 3D human avatars with predefined param-
etic human templates [1, 8, 14, 16, 26, 39–41]. Among
them, EV A3D [14] proposes a compositional human NeRF
representation which divides the human body into differ-
ent local parts and achieves high-resolution human image
generation. However, all of them do not support user-
interacted editing of generated human avatars. Some recent
works [4, 12, 15, 28] also explore using text-image embed-
ding to edit the generation results. Although achieving good
performance, these methods can not achieve fully disentan-
gled control over different semantic parts and suffer from
long optimization time.
2.2. Spatial decomposition
NeRF proposes to use fully implicit networks to represent
3D density and color fields, which is slow in querying and
computational infeasible to generate high-resolution images
directly. A recent trend is to reduce the rendering time
of NeRF with explicit geometric representations, including
sparse 3D grids [38], point clouds [36] and so on. How-
ever, there is a tradeoff between the rendering speed and the
memory costs of different geometric representations. Al-
though explicit methods reduce the optimization time, it re-
quires more memory footprint.
Hybrid representation [3, 6, 7, 9, 24] has been proposed
to improve the computational efficiency of NeRF by com-
bining the explicit representations with implicit layers. In
particular, TensoRF [7] proposes to decompose the voxel
tensor into feature planes and vectors, which greatly im-
proves the computational efficiency. While all these meth-
ods focus on the decomposition of 3D feature volumes.Hexplane [3] further extends it into 4D dynamic scenes.
They decompose the 4D space-time volume into six fea-
ture planes spanning each pair of coordinate axes (e.g. XY,
ZT).
3. Method
3.1. Efficient 4D Space-Attribute Decomposition
we formulate the generated human avatars in a 4D space-
attribute field (e.g. hair, clothes, body shapes and so
on), which could be denoted as (x, y, z, a )∈R4where
a∈ {clothes, hair, shoes... }denotes the attribute dimen-
sion. Given a selected attribute attriand its attribute index
ai=Index (attri), we are capable to extract the corre-
sponding attribute features independently from the gener-
ated feature planes. Recently, CNeRF [22] propose to gen-
erate the 4D volumes (x, y, z, a i), i∈ {1,2, ..., N}withN
independent generators. However, training several indepen-
dent generators in parallel will exponentially increase the
computational cost. Inspired by the tensor decomposition
technologies in 4D dynamic NeRFs [3], we propose to de-
compose the 4D space-attribute field into six feature planes.
Specifically, we decompose the 4D space-attribute volume
V∈RXY ZAFinto six planes : PXY
r,PY Z
r,PXZ
r,PXA
r,
PY A
r,PZA
r, and represent the space-attribute fields Das:
D(x, y, z, a ) = (PXY R 1
xy ⊙PZAR 1
za )VR1F
+(PXZR 2
xz ⊙PY AR 2
ya )VR2F
+(PY ZR 3
yz⊙PXAR 3
xa )VR3F,(1)
where ⊙denotes the element-wise product, PZAR 1za denotes
a 3D tensor with shape Z×A×R1andVR1Fdenotes
a 2D tensor with shape R1×F. In our experiments, we
find that relaxing Eq 1 by setting VR1F,VR2FandVR3F
to be constant leading to faster convergence without losing
performance. Therefore, we set VR1F,VR2FandVR3Fto
be one in our all experiments.
Leveraging the proposed tensor decomposition technol-
ogy, we are capable to efficiently generate the decomposed
feature planes with a single 2D CNN-based feature genera-
tor (like StyleGan2 [19]), which greatly reduces the compu-
tational cost as well as facilitates information sharing across
different attributes and encourages consistency among the
generated features of different attributes.
3.2. Implicit Attribute Indexing
We propose implicit attribute indexing module to predict
the indexes of different attributes. As the attribute dimen-
sion could be disjoint and ambiguous, trivially adopting the
identical mapping indexing method as dynamic NeRFs [3]
leads to degraded generation results (Figure 5). To address
this problem, we propose an implicit indexing module to
learn the index of each predefined attribute in the attribute
10598
Figure 2. The overall framework of our model. We generate the decomposed feature plane with StyleGANv2-based generator and predict
the indexes of each attribute with implicit indexing module. We model the deformation between canonical space and target space with
deformer module and synthesis final image with compositional volume rendering and super-resolution module. Detailed structure of the
attribute decompose module and deformer module is shown at the bottom, where Bs(β),BP(θ)represents the SMPL parameters randomly
sampled from the dataset, n denotes the total number of selected attributes.
dimension. In detail, the implicit indexing module Iis for-
mulated as an MLP network with eight layers. The input
for the indexing module is a one-hot label oi∈RN, where
Nis the number of total attributes. To stabilize the train-
ing, we normalize the predicted index before using it for at-
tribute indexing. The total function of the implicit indexing
module can be simply formulated as ai=norm (I(oi)),
where norm denotes the L2normalization. The indepen-
dent feature Difor attribute ican then be extracted from
the generated feature fields Das:
Di= (PXY R 1
xy ⊙(PZAR 1
za×aAR1
i))
+(PXZR 2
xz ⊙(PY AR 2
ya×aAR2
i))
+(PY ZR 3
yz⊙(PXAR 3
xa ×aAR3
i)).(2)
Note we omit V∗here for simplicity. During our experi-
ments, we found that learning the indexing module in an
unsupervised style fails to achieve strong disentanglement
between different attributes, leading to collapsed geometry
and chaotic textures in the editing stage. To address this
problem, we further propose an orthogonal projection regu-
larization to enforce the orthogonality between indexes for
different attributes.
Orthogonal Projection Regularization. Given a predicted
index ai, the orthogonal projection regularization could bedefined as:
Lorth=X
i,j∈B
i̸=j|⟨ai, aj⟩|,⟨xi, xj⟩=xi·xj
∥xi∥2· ∥xj∥2,
(3)
which involves no additional learnable parameters and can
be applied directly on the mini-batch level for each iteration.
In our experiments, we found that the orthogonal projec-
tion regularization is crucial to enforce the implicit index-
ing module to learn orthogonal attribute indices, facilitates
the disentanglement between different attributes and leads
to better editing results.
3.3. Overall Framework
Our framework is built on EG3D [6], involving a
StyleGAN2-based [19, 34] feature generator, an implicit at-
tribute indexing module, a compositional volume renderer
and a 2D CNN-based up-sampler. The overall framework
of our model is shown in Figure 2.
In detail, we adopt StyleGAN2-based generator to gen-
erate the overall feature planes. To achieve better genera-
tion quality and efficiency, we adopt an improved version
of generator structure following IDE3d [34]. We then sam-
ple a set of attributes Oifrom the real-world dataset, where
Oi= (o0, ..., o k), k < N ,Nrepresents the total number
10599
Figure 3. Qualitative Comparisons of our methods with EG3D, StyleSDF, EV A3D, CNeRF. The RGB images, generated segmentation
masks and 3D meshes demonstrated that our method achieves high-quality human avatar generation. Moreover, the main contribution of
our model is to support interactive user editing, which is not supported by EG3D, StyleSDF and EV A3D.
of attribute categories predefined on the dataset. In our ex-
periments, we set 11 attribute categories including the hu-
man body, hair, shirts, pants, skirts, dress and so on. Dur-
ing the generation process, each attribute is embedded as
a one-hot label oiand is passed into the implicit indexing
module to get the learned index in the attribute dimension.
The features of each attribute are then extracted from the
generated feature planes. Given a sampled point on a cer-
tain camera ray, we query its feature vectors by projecting
the points onto the axis-aligned extrated feature planes and
decode the residual signed distance values ∆di, semantic
masks mi, RGB features and values fi, ciwith a multi-head
MLP, which shares the weights across different attributes.
We disentangle the pose and shape of human by shap-
ing the generated space-attribute fields in a common canon-
ical space and find the deformation between the canonical
space with each observation posed space leveraging the pre-
defined skinning weights on the SMPL human templates. In
detail, we define the transformation of a point xtfrom ob-
servation space tto canonical space as:
x0
1
=X
vi∈N(x)ωiPωM(β0, θ0)(M(βt, θt))−1
, M(β, θ) =I B S(β) +BP(θ)
0T1
,(4)
where Nrepresents the nearest kpoints that are found
among the vertices of SMPL posed mesh M(βt, θt),wi=1/∥xt−vj∥is the transformation weight, BS(β)and
BP(θ)denote the shape/pose blend shape for vertexes
on the SMPL mesh, respectively. Similar to Avatar-
Gen [40], we further involve a per-attribute non-rigid de-
formation network, which predicts the non-rigid offsets
∆xt, to accommodate the non-rigid deformation between
the observation and the canonical spaces. Denote the
transformation defined in E.q. 4 as T, the final deforma-
tion can be formulated as x0=T(xt) + ∆ xt,∆xt=
MLP (embed (x0), BS(β), BP(θ), mi), where mirepre-
sents the semantic mask for attribute i.
As we formulate all the generation in the canonical
space, it’s easy for us to conduct editing by directly ex-
changing the feature planes. To edit certain attribute oi, we
could change the generated feature planes of oiwith an-
other ones generated from new sampled latent codes. In
this way, we support precise editing on certain selected at-
tributes while keeping others fixed. We show more details
in the supplemental materials.
3.4. Compositional Volume rendering
We fuse and render the output of all attributes with com-
positional volume rendering. Specifically, we first fuse the
outputs of all selected attributes with the predicted seman-
tic masks. Given a sampled points xin attribute iwith pre-
dicted residual signed distance values ∆di, semantic mask
mi, RGB features and values fi, and ci, the fusion step can
10600
be formulated as:
d(x) =dt(x) +NX
im′
i(x)·∆di(x), (5)
f(x) =NX
im′
i(x)·fi(x), c (x) =NX
im′
i(x)·ci(x),
(6)
where m′
i(x) =exp(mi(x))PN
jexp(mj(x))denotes the softmax op-
eration, dtrepresents the template SDF queried from the
SMPL template, N denotes the total number of semantic
categories. We then adopt volume rendering [23] to ren-
der the low-resolution RGB image and semantic mask. The
rendered RGB feature along with the segmentation masks
are then fed into the super-resolution module to generate
the final high-resolution images.
3.5. Training
Hyper-latent Training Strategy. During our experiments,
we found that the style entanglement existing in the current
human datasets [21]. One of the most significant problems
is that almost all datasets have few or no images contain-
ing men wearing dresses. However, as attribute is defined
according to semantic areas without definition of human
genders. Problems come when we generate a body area
of men with attributes of dress. In this condition, the gen-
erated image would become out-of-distribution to the dis-
criminator and it would give misleading punishment. As a
result, the feature plane of attribute dress would try to in-
fluence the feature of attribute human body, making it to
look like women (Figure 7), leading to slower convergency
and artifacts in the editing stage. To address this problem,
we condition the linear mapping module of StyleGAN [10]
with hyper-label to control the gender of the generated hu-
man bodies and sample reasonable attribute sets according
to different genders. In this way, we avoid the misleading
punishment from the discriminator and achieves better gen-
eration results.
Attribute-specific Sampling Strategy. While hyper-latent
training strategy is adopted to address the entanglement be-
tween attributes with large spatial overlap, like body and
dress. We propose attribute-specific sampling strategy to
address the entanglement between attributes with little over-
lap areas. For example, the style entanglement of dresses al-
ways with high-heel shoes, T-shirts always with snipers. In
detail, we define attribute-specific bounding boxes for each
attribute and sample corresponding attribute feature only in-
side the bounding box. In this way, we limit the influence of
each attribute strictly inside the bounding box, enhance the
disentanglement between attributes with little or no over-
laps and further improve the computational efficiency by
sampling much fewer points for each attribute. We presentmore details of the pre-defined bounding boxes in the sup-
plementary material.
Loss. We use the non-saturating GAN loss Lganwith
R1 regularization LGreg. We regularize the learned SDFs
by pushing the derivation of delta SDF values to be zero:
Leik=P
x(∥∇di(x)∥)−1)2, and further prevent false or
non-visible surfaces with Lsurf=P
xexp(−100|di(x)|).
Moveover, to constrain the generated residual signed dis-
tance fields to be consistent with the SMPL template mesh,
we guide the predicted residual signed distance fields with a
minimum regularization: Lrsd f=P
x(∥∆d(x)∥). To pre-
vent the learned non-rigid deformation from collapsing, we
add a non-rigid regularization to regularize the learned non-
rigid deformation to be small: Lnonrig =P
x(∥∆xt∥). The
overall loss function is formulated as:
Loverall =Lgan+λGregLGreg+λeikLeik+λsurfLsurf
+λrsd fLrsd f+λnonrig Lnonrig +λorthLorth,
(7)
where λ∗denotes the weights of each loss item.
4. Experiments
Datasets. We train and evaluate our model mainly on the
Deepfashion dataset [21], which contains single-view hu-
man images with different clothes and the corresponding
semantic masks. We select 11903 images that contain the
full human bodies, crop and resize all the images to coarsely
align the human bodies. All images are resized to 512×512
for training. We use an off-the-shelf pose estimator [29] to
estimate SMPL parameters and camera parameters of each
image in the dataset. For each human image, we convert its
corresponding semantic masks into attribute sets by check-
ing whether the semantic region is above zero. we adopt 11
attributes for training, including Outer, Top, Skirts, Dress,
Pants, Rompers, Hats, Glasses, Body, Shoes and Haircut.
4.1. Comparisons.
Baselines. We compare our AttriHuman-3D model
with several state-of-the-art 3D-aware GANs, including
EG3D [6], StyleSDF [27], CNeRF [22] and EV A3D [14].
EG3D, STyleSDF and CNeRF are designed for 3D gen-
eration of rigid / half-rigid objects such as human faces.
Among them, CNeRF is also capable to support user-
interacted editing for the generation results, which is sim-
ilar to our task. EV A3D is also designed for human avatar
generation, but it does not support interactive editing, which
is the main contribution of our model.
Quantitative Evaluations. The quantitative comparisons
between our methods with baselines are shown in Ta-
ble 1. In detail, we adopt three commonly used image
quality evaluation metrics including Frechet Inception Dis-
tance (FID) [13] to evaluate the visual quality and diver-
sity of the rendered images, Percentage of Correct Key-
10601
Methods FID ↓FIDedit↓PCK↑Depth↓E
EG3D [6] 25.90 - 79.63 0.0331 ×
StyleSDF [27] 26.32 - 75.13 0.0376 ×
CNeRF [22] 27.69 30.31 71.56 0.0421 ✓
EV A3D [14] 15.91 - 87.50 0.0272 ×
fixed indexing 21.20 23.58 88.41 0.0357 ✓
w/o OPR 19.81 22.31 88.72 0.0322 ✓
w/o HL 17.24 18.11 89.63 0.0311 ✓
w/o ASP 17.02 17.32 89.97 0.0305 ✓
Ours(full) 16.85 17.43 89.91 0.0302 ✓
Table 1. Quantitative comparisons of our method with other meth-
ods, where OPR denotes orthogonal projection regularization, GS
denotes Hyper-Latent training strategy and ASP denotes Attribute-
specific Sampling strategy. E represents editable ability.
Methods Memory Parameters Time(s)
EG3D 6G 31M 0.06
StyleSDF 6G - 0.13
CNeRF 16G - 0.23
EV A3D 7G - 0.16
11G-base Out of Mem 311M -
Ours(full) 9G 59M 0.09
Table 2. Efficiency comparisons of our method with other meth-
ods. Our model is much more efficient compared to CNeRF and
11G-base model without using the proposed tensor decomposition
technique.
points (PCK) [1] to evaluate the effectiveness of the pose
controllability. Pseudo Depth [30] to evaluate the consis-
tency between the generated geometry and RGB images. As
shown in Table 1, our model achieves comparable results to
the SOTA method EV A3D, while capable to support user-
interacted editing. We also compare the FID of the edited
images (FID edit) with CNeRF and the result demonstrates
our model achieves better editing results with a little drop
in the FID for the edited images. Moreover, we also com-
pare the computational efficiency of our model with others
in Table 2, which shows our model only involves limited ex-
tra parameters to achieve editing capability. Benefiting from
the proposed tensor decomposition technology our model is
much higher efficient compared to CNeRF and 11G-base
model which directly uses 11generators for the generation.
Qualitative Evaluations. Figure 3 shows the qualitative
comparisons of our methods with other baselines. Specif-
ically, we show the rendered images, segmentation masks
and the corresponding meshes. 3D GANs designed for
rigid/half-rigid objects including EG3D, StyleSDF and CN-
eRF fail to learn high-quality generation of human avatars
due to the high variance of human poses and appearance.
While EV A3D generates realistic 3D human avatars, it fails
to support user-interacted editing. Our model achieves com-
parable generation results to EV A3D while capable to sup-
port semantic part-level editing that other methods do not
Figure 4. Qualitative comparisons of the editing results between
our methods and CNeRF. From left to right we show the editing
RGB and residual of changing the Top, Pants and Haircut. Benefit
from our hyper-latent and attribute-specific training strategy, our
model achieves better disentanglement and more precise control
over the target semantic region compared to CNeRF.
Figure 5. Ablation of the implicit indexing module. Figure (a) and
(b) compare the ablation results of our implicit mapping module
with fixed identical mapping as dynamic NeRFs [3]. Figure (c)
shows cosine similarity of the predicted indexes with or without
the proposed orthogonal projection regularization.
support. We also compare the editing results of our model
with CNeRF. As shown in Figure 4, benefit from our hyper-
latent and attribute-specific training strategy, our model
achieves better disentanglement and more precise control
over the target semantic region compared to CNeRF.
4.2. Ablations Studies.
Implicit Indexing Module. Trivially adopting the fixed
identical indexing method in dynamic NeRFs [3] leads to
degraded generation results as shown in Figure 5. In our
implicit indexing module, the proposed orthogonal projec-
tion regularization serves an important role in encouraging
the predicted implicit attribute indexes to be orthogonal to
each other, facilitating the disentanglement between differ-
ent attributes. As shown in Figure 5(c) , Figure 6, with-
10602
Figure 6. Ablation of the orthogonal projection regularization. we
achieve better editing results with the proposed OPR loss.
Figure 7. Ablation of the Hyper-latent training strategy and
Attribute-specific Sampling training strategy.
out the orthogonal projection regularization, the learned at-
tribute indexes tend be entangled with each other, leading
to collapsed results in the editing stage. When we remove
the original feature of the top clothes and change it into an-
other one, the network fails to generate satisfactory edited
images. We also conduct quantitative comparisons in Ta-
ble 1. Removing the implicit indexing module or orthog-
onal projection regularization leads to a significant perfor-
mance drop in all metrics, which indicates the effectiveness
of the proposed implicit module and orthogonal projection
regularization.
Training Strategies. During training, we adopt the hyper-
latent training strategy and attribute-specific sampling strat-
egy to address the implicit style entanglement between dif-
ferent attributes in the existing dataset. As shown in Fig-
ure 7 (a), without the hyper-latent training strategy, the
discriminator tend to give misleading penalty to push the
attribute features of dresses or rompers to influence the
body attribute and try to change the original appearance
of the generated bodies to make them visually more suit-
able for a female subject. With the proposed hyper-latent
training strategy, we avoid the misleading punishment from
the discriminator and achieve more accurate editing results
Figure 8. Application of AttriHuman-3D. (a) Interpolation on the
latent code of Top Clothes gives smooth transformations between
two samples. (b) Pose animation results in a generated avatar. (c)
Inversion of real image
without changing the appearance of the generated human.
The Attribute-specific sampling strategy enforces the style
disentanglement between attributes with little or no over-
lay such as dress and shoes. As shown in Figure 7, with
attribute-specific sampling strategy, we avoid the style influ-
ence from rompers on the shoes, and achieve more precise
editing results. Quantitative results shown in Table 1 also
indicate that the proposed training strategies lead to better
generation results.
5. Conclusion
In this paper, we propose AttriHuman-3D, a novel ed-
itable 3D human avatar generation model.Our method al-
lows users to interactively edit selected attributes in the gen-
erated 3D human avatars while keeping others fixed. Both
qualitative and quantitative experiments demonstrate that
our model provides a strong disentanglement between dif-
ferent attributes, allows fine-grained image editing and gen-
erates high-quality 3D human avatars
6. Acknowledgement
This study is supported under the RIE2020 Industry Align-
ment Fund - Industry Collaboration Projects (IAF-ICP)
Funding Initiative, as well as cash and in-kind contribu-
tion from the industry partner(s). This research is also sup-
ported by the MOE AcRF Tier 2 grant (MOE-T2EP20220-
0007).
10603
References
[1] Alexander Bergman, Petr Kellnhofer, Wang Yifan, Eric
Chan, David Lindell, and Gordon Wetzstein. Generative neu-
ral articulated radiance fields. Advances in Neural Informa-
tion Processing Systems , 35:19900–19916, 2022. 3, 7
[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high fidelity natual image synthesis.
InProc. International Conference on Learning Representa-
tion, 2019. 3
[3] Ang Cao and Justin Johnson. Hexplane: A fast representa-
tion for dynamic scenes. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 130–141, 2023. 2, 3, 7
[4] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-
Yee K Wong. Dreamavatar: Text-and-shape guided 3d hu-
man avatar generation via diffusion models. arXiv preprint
arXiv:2304.00916 , 2023. 3
[5] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. pi-gan: Periodic implicit generative
adversarial networks for 3d-aware image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 5799–5809, 2021. 3
[6] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient
geometry-aware 3d generative adversarial networks. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 16123–16133, 2022. 3, 4, 6,
7
[7] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance fields. In Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv, Is-
rael, October 23–27, 2022, Proceedings, Part XXXII , pages
333–350. Springer, 2022. 3
[8] Zijian Dong, Xu Chen, Jinlong Yang, Michael J Black, Ot-
mar Hilliges, and Andreas Geiger. Ag3d: Learning to gen-
erate 3d avatars from 2d image collections. arXiv preprint
arXiv:2305.02312 , 2023. 3
[9] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk
Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:
Explicit radiance fields in space, time, and appearance. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 12479–12488, 2023. 2,
3
[10] Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen
Qian, Chen Change Loy, Wayne Wu, and Ziwei Liu.
Stylegan-human: A data-centric odyssey of human genera-
tion. In Computer Vision–ECCV 2022: 17th European Con-
ference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
Part XVI , pages 1–19. Springer, 2022. 6
[11] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Proc. Neural
Information Processing Systems , 2014. 3
[12] Ayaan Haque, Matthew Tancik, Alexei A Efros, Alek-
sander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf:Editing 3d scenes with instructions. arXiv preprint
arXiv:2303.12789 , 2023. 3
[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 6
[14] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and
Ziwei Liu. Eva3d: Compositional 3d human generation from
2d image collections. arXiv preprint arXiv:2210.04888 ,
2022. 3, 6, 7
[15] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang
Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-
driven generation and animation of 3d avatars. arXiv preprint
arXiv:2205.08535 , 2022. 3
[16] Suyi Jiang, Haoran Jiang, Ziyu Wang, Haimin Luo, Wen-
zheng Chen, and Lan Xu. Humangen: Generating hu-
man radiance fields with explicit priors. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12543–12554, 2023. 3
[17] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of GANs for improved quality, stability,
and variation. In Proc. International Conference on Learning
Representation , 2018. 3
[18] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProc. IEEE Conference on Computer Vision and Pattern
Recognition , 2019. 1
[19] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of StyleGAN. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition , 2020. 3, 4
[20] Tero Karras, Miika Aittala, Samuli Laine, Erik H ¨ark¨onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. Advances in Neural Infor-
mation Processing Systems , 34:852–863, 2021. 1
[21] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou
Tang. Deepfashion: Powering robust clothes recognition
and retrieval with rich annotations. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1096–1104, 2016. 6
[22] Tianxiang Ma, Bingchuan Li, Qian He, Jing Dong, and Tie-
niu Tan. Semantic 3d-aware portrait synthesis and manipu-
lation based on compositional neural radiance field. arXiv
preprint arXiv:2302.01579 , 2023. 1, 2, 3, 6, 7
[23] B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ra-
mamoorthi, and R Ng. Nerf: Representing scenes as neural
radiance fields for view synthesis. In European conference
on computer vision , 2020. 6
[24] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 41(4):1–15, 2022. 3
[25] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-
ing scenes as compositional generative neural feature fields.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 11453–11464, 2021.
3
10604
[26] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya
Harada. Unsupervised learning of efficient geometry-aware
neural articulated representations. In Computer Vision–
ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part XVII , pages 597–
614. Springer, 2022. 3
[27] Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shecht-
man, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.
Stylesdf: High-resolution 3d-consistent image and geome-
try generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 13503–
13513, 2022. 3, 6, 7
[28] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
and Dani Lischinski. Styleclip: Text-driven manipulation of
stylegan imagery. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 2085–2094,
2021. 3
[29] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and
Michael J Black. Expressive body capture: 3d hands,
face, and body from a single image. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10975–10985, 2019. 6
[30] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE transactions on pattern analysis and machine
intelligence , 44(3):1623–1637, 2020. 7
[31] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas
Geiger. Graf: Generative radiance fields for 3d-aware im-
age synthesis. Advances in Neural Information Processing
Systems , 33:20154–20166, 2020. 3
[32] Chaoyue Song, Jiacheng Wei, Ruibo Li, Fayao Liu, and Gu-
osheng Lin. 3d pose transfer with correspondence learning
and mesh refinement. Advances in Neural Information Pro-
cessing Systems , 34:3108–3120, 2021. 3
[33] Chaoyue Song, Jiacheng Wei, Ruibo Li, Fayao Liu, and Gu-
osheng Lin. Unsupervised 3d pose transfer with cross consis-
tency and dual reconstruction. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 2023. 3
[34] Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue
Wang, and Yebin Liu. Ide-3d: Interactive disentangled edit-
ing for high-resolution 3d-aware portrait synthesis. ACM
Transactions on Graphics (TOG) , 41(6):1–10, 2022. 1, 4
[35] Jingxiang Sun, Xuan Wang, Yong Zhang, Xiaoyu Li, Qi
Zhang, Yebin Liu, and Jue Wang. Fenerf: Face editing in
neural radiance fields. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
7672–7682, 2022. 1
[36] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin
Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:
Point-based neural radiance fields. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5438–5448, 2022. 3
[37] Fan Yang and Guosheng Lin. Ct-net: Complementary trans-
fering network for garment transfer with arbitrary geomet-
ric changes. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition , pages 9899–
9908, 2021. 3
[38] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenox-
els: Radiance fields without neural networks. arXiv preprint
arXiv:2112.05131 , 2021. 3
[39] Jichao Zhang, Enver Sangineto, Hao Tang, Aliaksandr Siaro-
hin, Zhun Zhong, Nicu Sebe, and Wei Wang. 3d-aware
semantic-guided generative model for human synthesis. In
Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XV ,
pages 339–356. Springer, 2022. 3
[40] Jianfeng Zhang, Zihang Jiang, Dingdong Yang, Hongyi Xu,
Yichun Shi, Guoxian Song, Zhongcong Xu, Xinchao Wang,
and Jiashi Feng. Avatargen: a 3d generative model for an-
imatable human avatars. In Computer Vision–ECCV 2022
Workshops: Tel Aviv, Israel, October 23–27, 2022, Proceed-
ings, Part III , pages 668–685. Springer, 2023. 5
[41] Xuanmeng Zhang, Jianfeng Zhang, Rohan Chacko, Hongyi
Xu, Guoxian Song, Yi Yang, and Jiashi Feng. Getavatar:
Generative textured meshes for animatable human avatars.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 2273–2282, 2023. 3
10605
