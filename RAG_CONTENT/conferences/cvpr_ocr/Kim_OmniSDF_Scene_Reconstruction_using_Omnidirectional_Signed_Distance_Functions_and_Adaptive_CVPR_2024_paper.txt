OmniSDF: Scene Reconstruction using
Omnidirectional Signed Distance Functions and Adaptive Binoctrees
Hakyeong Kim Andreas Meuleman Hyeonjoong Jang James Tompkin Min H. Kim
KAIST INRIA KAIST Brown University KAIST
Abstract
We present a method to reconstruct indoor and outdoor
static scene geometry and appearance from an omnidirec-
tional video moving in a small circular sweep. This set-
ting is challenging because of the small baseline and large
depth ranges, making it difficult to find ray crossings. To
better constrain the optimization, we estimate geometry as
a signed distance field within a spherical binoctree data
structure and use a complementary efficient tree traver-
sal strategy based on a breadth-first search for sampling.
Unlike regular grids or trees, the shape of this structure
well-matches the camera setting, creating a better memory-
quality trade-off. From an initial depth estimate, the binoc-
tree is adaptively subdivided throughout the optimization;
previous methods use a fixed depth that leaves the scene
undersampled. In comparison with three neural optimiza-
tion methods and two non-neural methods, ours shows de-
creased geometry error on average, especially in a detailed
scene, while significantly reducing the required number of
voxels to represent such details.
1. Introduction
When reconstructing the geometry of a static unbound
scene, say an outdoor space, most image-based multi-view
reconstruction pipelines use perspective cameras. Given
their limited field of view, they suffer from high data ac-
quisition costs as many images must be captured and cali-
brated. But, given their physically-compact size, it is pos-
sible to move to different spatial sampling positions and
capture many light ray crossings from which to accurately
constrain the surface geometry location. To solve for the
location, modern auto-differentiation systems let us flexi-
bly implement algorithms like sphere tracing with neural
signed-distance functions (SDFs).
Given enough ray crossings, these algorithms show
promise in capturing geometry from real-world perspec-
tive images, both in geometry detail [15] and in large-scale
structures [24], thanks to the robust fitting properties of neu-
ral networks. If memory is not a concern, we can exploit
COLMAP Egocentric Ours NeuS -factoOur 3D reconstruction Input 360 videoFigure 1. We introduce a memory-efficient neural 3D reconstruc-
tion method tailored to work with short egocentric omnidirectional
video inputs. The geometry is estimated using a signed distance
field and a novel adaptive spherical binoctree data structure sub-
divided through iterative optimization. We show that our method
outperforms other state-of-the-art 3D reconstruction methods in
balancing detail and memory cost [8, 21, 33].
voxel grid structures for rapid position indexing [15, 24],
but unbound scenes require careful memory consideration.
To help overcome the acquisition cost, one alternative is
to use an omnidirectional camera that captures a view of
all angles of the surroundings. This is a critical capabil-
ity when comprehensive spatial understanding is necessary,
such as in virtual reality and robotic navigation. But, now
a wider field of view is represented within a limited camera
sensor resolution, such that the angular extent of each pixel
is typically greater than in a perspective camera ( ≈5×).
To gain parallax for distance estimation, suppose we ad-
ditionally consider a limited spatial sampling setting, e.g.,
an omnidirectional video of a circular camera motion—this
would be convenient for data capture [7]. But, as the base-
line between spatial positions is small with respect to the
large range of scene distances, and as each pixel covers a
greater angular extent, it is difficult to accurately constrain
the location of the geometry or to reproduce its fine detail.
These factors decrease the effectiveness of ray marching
techniques and, within an optimization, often necessitate an
impractical number of samples for successful training con-
vergence. This issue is further compounded by rapid and
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20227
large changes in depth in outdoor scenes—a difficult func-
tion to fit—that typically leads to a loss of detail in near-
field geometry. In terms of memory, naive grids do not scale
well to unbound scenes, and approaches tailored to small-
baseline omnidirectional cameras should be more efficient.
To address these challenges, we propose a neural 3D re-
construction method specifically designed for short egocen-
tric omnidirectional video inputs. Central to our approach is
anadaptive spherical binoctree formed of spherical voxels
(‘sphoxels’). This dynamically subdivides from coarse to
fine granularity through an iterative optimization, such that
both ray samples and memory can focus on areas with more
detail. Within the spatially-varying subdivisions of the leaf
nodes, each sphoxel is fine-tuned to optimize spatial resolu-
tion and prioritize sampling in areas closer to the observed
surfaces. Beyond this, we also provide two practical solu-
tions: an efficient tree traversal strategy that manages the
inconsistent number of child nodes based on breadth-first
search, and an intersection test method to accommodate un-
even shapes of the sphoxels in the binoctree structure.
Binoctrees were originally designed to subdivide a
spherical space while compensating for voxel elongation
that occurs at far distances [8]. However, this previous
approach does not adapt the structure: not by the achiev-
able depth accuracy, which can be estimated given camera
poses, nor by any potential initial depth value, nor by the
depth during optimization itself. We show that our approach
improves upon an existing binoctree-based reconstruction
method, reducing error by 38% on average. We also show
improvement over a recent neural-field-based SDF recon-
struction method that does not efficiently partition space
within large-scale scenes [28]. Overall, our approach al-
lows for more efficient and accurate 3D surface reconstruc-
tion of large-scale unbounded scenes using omnidirectional
video inputs. Our method more effectively reconstructs the
3D environment from egocentric video inputs, addressing
challenges in neural geometry with omnidirectional cam-
eras. Our code is freely available for research purposes1.
2. Related Work
360◦photography. Scene understanding from 360◦im-
ages has been actively explored together with the growth
of the VR/AR industry. Using VR as a motivating applica-
tion, several works focus on image-based rendering of 360◦
images from novel viewpoints [2, 4, 5]. These works adopt
convolutional neural networks to construct multi-sphere im-
ages, proxy geometry from learned optical flow, and neu-
ral radiance fields. However, these works do not explicitly
aim to understanding scene geometry and, therefore, suffer
distortion when rendering trajectory deviates from training
trajectories. Other work estimates dense depth and normal
1https://vclab.kaist.ac.kr/cvpr2024p2/maps for corresponding 360◦RGB images for inverse ren-
dering [14]. However, this is limited to indoor scenes, and
output depth maps require post-processing steps for explicit
mesh extraction from obtained point clouds [9, 10]. On the
other hand, our work focuses more on the geometrical un-
derstanding of spherical scenes by directly estimating the
accurate signed distance function (SDF).
Jang et al. [8] share our goal in reconstructing a mesh
of large-scale outdoor scenes from omnidirectional video.
They use a binoctree structure to efficiently subdivide the
unbounded space, then fuse truncated SDF (TSDF) values
directly. This makes the output mesh resolution depend
upon the voxel grid resolution, which is fixed from an ini-
tial depth estimate (e.g., from a pre-trained depth estimation
network). Any errors in the initial depth also cause errors
in fusion. Our work adopt a neural implicit function for
surface representation, using an adaptive binoctree to guide
sampling during reconstruction. This lets us refine any in-
correct initial depth estimates, and does not require param-
eter tuning for fusion weights and truncation thresholds.
Scene-scale surface reconstruction. Structure-from-
motion (SfM) and multi-view stereo (MVS) techniques
have successfully reconstructed city-scale scenes from
multi-view images [1, 11, 21]. However, these meth-
ods represent the 3D geometry with sparse point clouds
that require extra steps to extract a surface mesh [9, 10].
With the emergence of radiance fields as a 3D scene rep-
resentation [6], follow-up multi-view reconstruction stud-
ies have tackled outdoor scenes from object-centric un-
bounded scenes [3, 34] to larger scenes approaching urban
scale [6, 17, 18, 20, 25, 29]. Radiance-field-based methods
work well for novel view synthesis but often fail to estimate
correct geometries from sparse views. Tang et al. [26] re-
fine an extracted surface mesh to acquire delicate geometry
and a texture map from radiance fields. However, they do
not aim to reconstruct scene-scale geometry.
Neural implicit representations using SDFs show po-
tential for image-based 3D reconstruction through several
works [12, 19, 28, 30, 31]. These methods optimize an
implicit function within the volumetric rendering pipeline
by designing a density function derived from the estimated
SDF. Recent works [7, 15, 24] extend the reconstruction
scale to unbounded outdoor scenes. To handle optimiza-
tion difficulties and preserve high-fidelity details, Sun et
al. [24] use a sparse voxel grid built with SfM depth for
the sampling, Li et al. [15] adopt numerical gradients and
level-wise optimization of hash grid features [18], and Guo
et al. [7] used cuboid-shaped hash grids and long-trajectory
initialization schemes to handle unbounded street scenes.
Our work is built upon NeuS [28] to take advantage of a
neural implicit representation; this embeds the geometry re-
construction optimization procedure inside a differentiable
image-based rendering pipeline. However, our work dif-
20228
fers from previous research in how we interpret the recon-
struction space. To handle narrow-baseline outward-facing
scenes, we use an adaptive binoctree within spherical space
for sampling. This is different from common approaches
that use Cartesian coordinates for space sampling.
Grid-based training strategies. V oxel grids are widely
used to accelerate rendering at the cost of memory.
NSVF [16] uses a sparse voxel grid that adaptively prunes
itself to narrow the sampling range. DVGO [23] and
Plenoctrees [32] store density and appearance encodings
to speed up training time and rendering. Instant-NGP [18]
uses a hash table across multiple grid levels. These meth-
ods build a grid in NDC space based on Cartesian coordi-
nates. Cartesian voxel grids assume that regions of interest
are spread evenly within a cube and that sampling of the
space with cameras follows similarly. However, for narrow-
baseline outward-facing sampling (sometimes called ego-
centric), we need a novel design that can reduce sampling
to plausible regions while encompassing unbounded scene
points. EgoNeRF [5] uses a spherical feature grid as a neu-
ral scene representation to optimize a distant environment
but focuses on novel view synthesis and does not recon-
struct a surface. Our adaptive spherical binoctree method
for omnidirectional inputs recovers 3D geometry with im-
proved details and convergence speed over Cartesian grids.
3. Omnidirectional SDF Reconstruction
Given an omnidirectional video captured in a circular
sweep, our goal is to optimize a neural SDF within a spher-
ical space from which to reconstruct a 3D surface mesh.
Our reconstruction algorithm starts from training NeuS [28]
along with an initial binoctree constructed by the per-frame
initial depth from a pre-trained spherical depth estimation
network [8]. Then, we use step-wise sampling strategies
with adaptive binoctree subdivision to optimize both the
spherical grid and the neural networks with an image-based
reconstruction loss. The core ideas of our approach are: 1)
voxel-guided sampling with a sphere-shaped grid that sub-
divides the reconstruction space, which is specialized for
memory efficiency, and 2) online and iterative refinement
of grid structures based on the intermediate results.
3.1. Preliminaries
Implicit surface rendering. We define a surface Sas the
zero-level set of a signed distance function f:R3→R:
S=
x∈R3|f(x) = 0	
. (1)
We use two neural networks to estimate S[28, 30]: 1)
Given a 3D point x,FSDF
θ : (x)→(d, ζ)estimates the
signed distance dto the surface at scene point x, and ad-
ditionally outputs a 256-dimensional feature vector ζ. 2)Fc
θ: (x,n,v, ζ)→(c)estimates the view-dependent ap-
pearance cat scene point xusing the normal n(t)to the
SDF as its derivative. To render an image, we sample points
along a ray {x(t) =o+tv|t⩾0}, where ois the ray ori-
gin and vis the ray direction. Then, we compute Wang et
al.’s [28] unbiased and occlusion-aware weights w(t)from
the estimated SDFs to accumulate the final output color C
by volume rendering:
C(o,v) =Zfar
nearw(t)c(x(t),n(t),v, ζ(t))dt. (2)
Binoctree. The spherical space is defined by a spherical
binoctree [8]: a sphere-shaped octree structure that subdi-
vides a sphere in radial ( r), polar ( θ) and azimuthal ( ϕ) di-
rections. We call each cell a sphoxel. This structure allows
binary subdivisions in the radial direction to prevent spho-
xel elongation as rincreases, and is bound by near and far
spheres. Each sphoxel stores its boundary in spherical coor-
dinates [ rmin, rmax, θmin, θmax, ϕmin, ϕmax] and its index. We
store sphoxel indices for each sampling stage and traverse
the binoctree to efficiently fetch intersecting sphoxels.
Preprocessing. Following Jang et al. [8], this stage esti-
mates camera poses and initializes the spherical binoctree
from a depth estimate. We estimate depth for the initial
tree construction using Jang et al.’s public trained model
and code. We use structure from motion to solve for per-
frame camera poses from the omnidirectional video (Open-
VSLAM [22]). Given the poses, we extract a dense 3D
point cloud for each frame using spherical rectification [13]
and a fine-tuned disparity estimation network [27].
Using per-frame camera poses and calibrated depth maps
as input, we form a global coordinate frame in metric units
for the 3D point cloud. We place the origin at the center
of the cameras and set the near sphere bound to contain all
the camera positions. To represent the color of scene ele-
ments at infinity, such as the sky, we place a final sphere
at the far plane with view-independent color decoded by a
background MLP. We set the far sphere bound per scene
to fit the recovered point cloud inside the sphere. Before
binoctree construction and the neural network training, we
scale both binoctree and point cloud coordinates into a unit
sphere, and store the coefficient for later metric re-scaling.
For the initial binoctree structure, we use the scaled ini-
tial 3D point cloud to subdivide the tree to its finest level
until all sphoxels reach a minimum solid angle αmin. We
derive this size per scene from the baseline and image sizes,
and set αminbetween 5e−4and1e−3.
3.2. Sphoxel Intersection and Surface Existence
Unlike the previous binoctree method that directly inte-
grates the SDF from depth maps [8], we optimize a hybrid
binoctree/MLP SDF representation. We use the spherical
20229
Rd(a) (b)Figure 2. (a) Cuboid approximation of a sphoxel for intersection
testing. Ray-triangle intersection tests occur for all triangles that
comprise cuboid faces. (b) Illustration of rubrics for surface de-
tection in sphoxels. A surface exists in a sphoxel if ∥d∥< R.
binoctree only to guide sampling within the optimization,
providing an efficient subdivision for large spherical spaces.
Given the irregular shape of sphoxels, we cannot adopt con-
ventional Cartesian grid-based algorithms for ray intersec-
tion and occupancy search [16, 32]. Thus, we suggest a
novel intersection and surface existence test for sphoxels.
Each sphoxel is formed of two curved surfaces that each
lie on a sphere defined by a near and far distance along ra-
diusr, and four surfaces by the intersection of those spheres
with bounding planes formed at θmin, θmax, ϕmin, ϕmax(Fig-
ure 2). Only two of these surfaces are simple in that they
can be approximated by two triangles. Therefore, we can-
not adopt common AABB cube intersection algorithms for
sphoxel intersections. We re-define intersection for spho-
xels by approximating them as trapezoidal prisms (square-
based frusta) that share the same vertices (Figure 2(a)). Us-
ing the triangle-ray intersection algorithm, a ray intersects
a sphoxel if we can detect intersections between a ray and
a pair of triangles—one front-facing, one back-facing—that
form the faces. We implement a CUDA kernel that fetches
all intersecting sphoxels denoted by V. Since the number of
marked voxels easily exceeds 10 k, we traverse the binoc-
tree in a breadth-first order to relieve the bottleneck caused
by unnecessary intersection computations.
When an angular subdivision spans a pole, then the
binoctree around the pole forms a spherical sector (a cone
and spherical cap) that contains the polar axis. We divide
the spherical sector around the polar axis and approximate
these sphoxel segments as triangle-based frusta.
Computing sphoxel intersections lets us detect where the
current surface is, but we need another way to mark sphox-
els with likely surface-crossings to be sampled in future it-
erations. For this, we define sphoxel center and radii. Since
we compute intersections from the square-frusta approxi-
mation, we also define the center and radii from approxi-
mate geometry. A sphoxel’s center is the mean position of
its constituent eight vertices, and its radius is the distance
from its center to the farthest vertex. Figure 2(b) compares
the inferred absolute SDF value ∥d∥at the sphoxel center
with the corresponding sphoxel radius R. We assume the
(a) Inward-lookingsphere sampling(b) Outward-looking sphere sampling
(c) Coarse sphoxelsampling(d) Adaptive fine sphoxelsampling
Figure 3. Sampling strategies on a unit sphere. (a) Sphere sam-
pling range for the exocentric inward-looking model. (b)–(d) Sam-
pling for egocentric data using a coarse-to-fine binoctree.
surface passes through the sphoxel if ∥d∥is smaller than R.
3.3. Spatial Sampling Types
We use three different levels of space sampling.
Whole-space spherical sampling. This occurs along each
input camera ray between its intersections with the near and
far bounding spheres. Since we assume that all cameras
are located inside the near-bound sphere, rays uniquely in-
tersect with both spheres. Figure 3(a) illustrates how this
sampling differs from common ray-sphere intersection due
to differing observation directions. We distribute sample
points along each ray; we retain these samples throughout
training and subsequent sphoxel-based samplings to prevent
the optimization from failing to escape local minima.
Coarse sphoxel sampling. We select a set of coarse spho-
xelsVcoarse for sampling. Given the initial binoctree subdivi-
sion, we mark a sphoxel as ∈Vcoarse if it is a parent or grand-
parent of a leaf sphoxel. Given the small baseline and large
scene depth, this dilation step helps to resolve the sparse
distribution of leaf sphoxels to better optimize plausible sur-
face regions. We compute ray entry and exit points for all
marked sphoxels and uniformly sample in between [16].
One issue is the sky: if sampled, this background ap-
pearance will be optimized into the binoctree at an incorrect
depth. Sky regions typically produce few points. Another is
outlier points in the initial 3D point cloud. To handle both,
we additionally prune sphoxels from Vcoarse if the number
of contained initial points is lower than a threshold, which
varies per scene based on the point density from 3 to 20.
Fine sphoxel sampling via adaptive subdivision. Coarse
sphoxel sampling provides a geometrical guide in the initial
20230
Coarse sphoxel
vertices (Blue)Adaptive fine sphoxel
vertices (Green)Points sampled on ray
(Top: coarse, Bottom: fine samples)Figure 4. Adaptive sampling sphoxel bounds. Left: Coarse (blue)
sphoxel vertices for sampling. Center: A sample of points from
coarse (top) and fine (bottom) sphoxel intersections. Right: Fine
(green) sphoxel vertices, which are denser near the surface.
optimization iterations. However, sampling between all in-
tersecting coarse sphoxels samples a larger region than the
true surface location. Therefore, to create Vfine, we period-
ically reassess Vfinemembership and then adaptively sub-
divide member sphoxels during optimization. First, we ini-
tialize VfinewithVcoarse. Then, as optimized SDF values may
push a surface outside of Vfine, we periodically fetch all leaf
sphoxels in Vcoarse, infer the SDF d, and mark a sphoxel as
being in Vfineif∥d∥is smaller than each sphoxel’s radius.
Then, we subdivided all marked sphoxels as Vfinesuch that
surface optimization occurs at finer subdivisions. Figure 4
shows sphoxel vertices in coarse and fine levels. We can
see that adaptive subdivision detects the surface success-
fully and reduces the sampling range appropriately.
3.4. Implementation Details
Neural networks. We optimize three multi-linear percep-
trons (MLPs): one each to decode the SDF and color fea-
tures within the binoctree and one for the background. The
SDF network has eight layers of 256 neurons, and the color
network has four layers of 256 neurons. We use standard
positional encoding with six frequency bands for the SDF
network and ten for the background NeRF.
Sample counts. We use 32 samples for each of the whole-
space sphere, coarse voxel, and adaptive fine voxel stages.
We used 32 samples for the background. Within the spho-
xels, we also conduct importance sampling along the ray
based on the PDF along it, using another 32 samples.
Subdivision schedule. The binoctree is subdivided every
10 k iterations until the leaf node size reaches a threshold
angular size ranging 0.001–0.0001.
Supervision. The rendered color is supervised with an
L1 photometric loss, and the SDFs are supervised with an
eikonal loss. We add a loss to manually mask out the tripod
that supports the camera and to mask initial depth values
near the epipoles where depth cannot be estimated. There-
fore, the total loss is defined as:
Ltotal=LRGB+ωeikLeik+ωmaskLmask. (3)Iterations and computation. We optimize each scene on
a computer equipped with a single NVIDIA A6000 GPU
and an Intel Xeon Silver 4214R 2.40 GHz CPU with 256
GB RAM. It takes ∼20 hours for 300 k iterations. For the
synthetic scenes, we evaluate the model after 100–200 k it-
erations depending on their complexity: 200 k for Sponza ,
200 k for San Miguel , and 150 k for Lone-monk .
4. Results
Evaluation Method. We compare to five methods for
reconstructing omnidirectional images. First, we use the
classical structure-from-motion method COLMAP [21] af-
ter converting omnidirectional images to a six-face per-
spective cube map. Second, we compare to EgocentricRe-
con [8], an omnidirectional reconstruction method designed
for the same kind of input as ours, and also using a binoc-
tree. Third, fourth, and fifth, we compare our reconstruc-
tion results with three neural surface reconstruction meth-
ods: original NeuS [28], NeuS-facto [28, 33], and Neu-
ralangelo [15]. NeuS-facto is a neural SDF reconstruction
method that adapts NeuS by using the proposal network
from mip-NeRF360 for sampling points along the ray. This
comparison is suitable for unbounded scenes, whereas the
original NeuS does not support these. Neuralangelo trains
coarse to fine hash table grids with numerical gradients for
higer-order derivatives.We use each method’s implementa-
tion from SDF studio [33]. Approximate runtime took 10
hours for COLMAP, 30 minutes for EgocentricRecon, 5.5
hours for NeuS, 25 minutes for NeuS-Facto, and 31 hours
for Neuralangelo to process the Sponza scene (Fig. 5) with
200 images. Our method took 12.5 hours.
Dataset. We use synthetic scenes for ground truth quanti-
tative evaluation, using 3D scenes from the Blender Online
Community and the McGuire Computer Graphics Archive.
We render equirectangular RGB-D video with Blender at
2048×1024 pixels. The camera has a circular trajec-
tory within a central region inside the scenes, spanning 200
frames. These three scenes— Sponza ,Lone-monk , and San
Miguel —have fine detail, especially San Miguel , which has
many detailed trees that are challenging to reconstruct. We
also show real-world reconstruction from the Omniphotos
dataset captured with a swinging selfie stick [4].
Space subdivision efficiency. Our binoctree subdivision is
an efficient method to reduce the number of voxels and tree
depths when compared to the naive Cartesian subdivision.
The number of uniform Cartesian voxels required to fill a
unit sphere with the same size as the smallest sphoxel in
our binoctree is great (Table 1): our spherical subdivision
requires about 10 k ×fewer voxels.
Surface reconstruction accuracy. Since only a portion of
the complete ground truth mesh is reconstructed within the
omnidirectional video sweep setting, a direct mesh accu-
20231
Table 1. The number of voxels in each scene for a Cartesian
subdivision grid is more than in our adaptive sphoxel grid. Our
method is significantly more efficient while still achieving high-
frequency details in reconstructed results.
Scene Method Number of voxels Minimum sphoxel size
SponzaDense regular grid 33,335,054,331
Ours 4,346,041 1.25e-10
Lone-monkDense regular grid 2,234,638,740
Ours 231,237 1.87e-9
San MiguelDense grid 1,953,273,076
Ours 951,703 2.14e-9
Table 2. Quantitative evaluation of surface mesh accuracy for
both classical and neural methods. We measure MAE and RMSE
by rendering the inverse depth of each mesh and comparing to the
ground truth mesh. Methods in bold achieve the best accuracy.
Sponza Lone-monk San Miguel Average
RMSE MAE RMSE MAE RMSE MAE RMSE MAE
COLMAP 0.88 0.28 0.40 0.15 1.24 0.41 0.84 0.28
EgocentricRecon 0.75 0.27 1.05 0.43 0.82 0.29 0.88 0.33
NeuS 2.88 1.43 1.39 1.24 – – 2.13 1.33
Neus-facto 2.44 1.12 2.25 1.68 4.02 2.84 2.91 1.88
Neuralangleo 0.99 0.44 1.91 1.71 0.58 0.29 1.16 0.81
Ours 0.82 0.35 0.69 0.36 0.56 0.27 0.69 0.32
racy metric such as Hausdorff distance is inappropriate for
evaluation. Instead, to evaluate the accuracy of surface re-
construction, we render the depth of the reconstructed mesh
from the center of the camera trajectory for all methods.
Then, we compare each to the rendered depth of the ground
truth mesh from the same position.
We measure the depth MSE and RMSE of valid (e.g.,
non-sky) ground truth mesh surfaces in Table 2. Our ap-
proach can optimize an SDF with comparable results to
classical reconstruction methods while greatly exceeding
the reconstruction quality of existing neural SDF methods.
NeuS-facto shows improved accuracy in the Sponza scene
and reconstructs the surface of San Miguel, which NeuS
completely fails to reconstruct. As such, we leave out the
San Miguel reconstruction result of NeuS since the recon-
struction fails to create a mesh and so we cannot align and
measure its accuracy. Neuralangelo shows the best recon-
struction results among previous neural methods. However,
our method achieves a higher accuracy still. We provide the
qualitative comparison with neural methods in Figure 5, and
with classical methods in the supplemental material. This
validates that our method can successfully reconstruct de-
tailed surfaces of large and complex outdoor scenes that
current neural methods could not—simple methods based
only on MLPs cannot easily scale to large scenes.
Figure 6 shows our qualitative reconstruction accuracy
on a real scene. Our method can reconstruct both the scene
details and smooth surfaces within the geometry. COLMAP
reconstructs details without filling in holes; EgocentricRe-
con is overly smooth, and NeuS-facto is often in error.
Comparison with Depth Supervision. We compare our
approach, which uses a binoctree pre-built from depth inputTable 3. Quantitative comparison of binoctree-based geometry
guidance and depth supervision for geometry reconstruction qual-
ity evaluation using RMSE depth accuracy.
Method Sponza Lone-monk San Miguel Average
NeuS-facto [33] 2.44 2.25 4.03 2.91
NeuS-facto-D [33] 1.46 4.75 1.80 2.67
Ours 0.82 0.69 0.56 0.69
Table 4. Number of samplings for sampling strategies ablations
Sampling techniques Nsphere .Ncoarse Nfine Nimportance / steps
Sphere sampling 64 0 0 64 / 4
Sphere + coarse voxel 32 32 0 64 / 4
Sphere + coarse + fine voxel 32 32 32 32 / 2
as a sampling guide, to NeuS-facto-D, a method that uses
depth supervision directly to guide surface reconstruction.
Our findings suggest that using the binoctree as a guide is
superior to direct depth supervision. To incorporate depth
supervision, NeuS-facto-D adds a depth loss component to
the loss with a weight of 0.1. The depth loss is calculated
using L1 loss between the rendered depth and input depth.
Depth supervision can improve surface reconstruction on
average but our approach yields more significant improve-
ments (Table 3). One reason why is because depth supervi-
sion can often be inaccurate. Our approach conservatively
sets bounds upon the depth and then relies upon other losses
to optimize the surface location.
Ablation. To demonstrate the effectiveness of our sam-
pling method, we display the training progress using normal
maps for three different approaches: sphere sampling only,
sphere and coarse sphoxel sampling, and hybrid (sphere,
coarse, and fine sphoxel) sampling at 5 k, 10 k, and 100 k
training iterations. For fairness, we keep the total number
of samples per ray constant at 128. Our baseline setting
(sphere sampling only) follows the default sampling strate-
gies from NeuS except that it uses outward-looking sphere
sampling instead(Fig 3 (b)). Then, we sample more be-
tween coarse sphoxels instead of sphere bounds. Lastly, we
sample fine sphoxels instead of importance sampling. Ta-
ble 4 shows the number of samples used for this ablation.
Figure 7 (bottom right) shows that scene details in con-
cave regions are estimated with accuracy only if ray sam-
pling is strongly guided by both coarse and fine sphoxels.
Figure 8 qualitatively demonstrates that our adaptive fine
voxel correctly converges to the surface location.
5. Discussion and Conclusions
We have presented an adaptive subdivision of spherical
space via a binoctree with novel sampling techniques. This
subdivision and sampling are useful to optimize a neural im-
plicit representation of geometry from an omnidirectional
video. When such a representation is optimized for large
unbounded scenes, the continuous property of the represen-
20232
San Miguel Lone -monk SponzaNeuS NeuS -facto GT Ours Input Neuralangelo
Not available
Not availableFigure 5. We compare our method with traditional and neural methods using ground-truth geometry. We present qualitative results of
NeuS [28], NeuS-facto [33] and Neuralangelo [15] here; our method produce higher-quality 3D geometry. Please refer to Table 2 for
complementary quantitative evaluation and to supplemental material for further qualitative comparisons of omitted traditional methods,
including COLMAP [21], and EgocentricRecon [8].
20233
Ours
EgocentricNeuS-factoCOLMAP
Input
Figure 6. Qualitative comparisons of our reconstruction on a real-world scene from the Omniphotos dataset. Please refer to the supple-
mental video for more results.
5K 10K 100K
Sphere
Sampling onlyCoarse sphoxel +
Sphere samplingSpher + coarse +
adaptive fine sampl .
Figure 7. As an ablation, we show the progress of geometry optimization with the omnidirectional normal map of the sponza scene.
Normal maps are rendered per training iterations of 5K, 10K, and 100K for each sampling strategy. The geometry in the hollow region was
recovered only when both coarse and fine sampling was adopted.
Figure 8. Adaptive fine voxel bounds (red point samples) have
correctly converged to the surface location.
tations often causes smoothed surfaces that lack detail. To
overcome this issue, we use an adaptive subdivision strategy
that densifies samples near the surface. This strategy sub-
divides the sphoxel towards the surface and moves samples
similarly so that density is placed where it is needed.
To address the challenges posed by using a neural func-
tion to estimate abrupt changes in disparity and sparse ob-
servations from omnidirectional input, we propose con-
structing coarse space bounds from depth input to provide
a geometrical guide during optimization. The spherical
binoctree efficiently subdivides the omnidirectional space,
taking into account the rendering resolution and memory
space. We subdivide the space based on solid angle mea-
surement and distance from the camera, using a spatially
variant tree depth, using orders of magnitude fewer vox-
els than a Cartesian grid with an equivalent minimum voxel
size. This leads to improved surface detail without requir-
ing significant memory. Our model performance is com-parable to traditional surface reconstruction methods; con-
current neural methods using MLPs alone cannot achieve
this quality for our inputs, according to our comparisons.
Overall, quantitative, qualitative, and ablation experiments
confirm the effectiveness of our approach as a promising
method for small-baseline omnidirectional data.
Limitations. Our approach uses an initial depth map to
initialize the coarse voxel before subdividing them into finer
levels. Should the estimation of the coarse voxels be wrong,
it can lead to inaccurate guidance for the coarse geometry.
This also may cause inaccuracies when samples are esti-
mated at scene regions that should belong to the background
but have nearer initial depths. We attempt to lessen these ef-
fects by dilating and pruning the voxel before optimization,
but it may be necessary to perform postprocessing to ensure
better final meshes without spurious artifacts.
Further, some scene geometry details still remain chal-
lenging to reconstruct, e.g., concave region reconstruction
may be sensitive to particular minima in the optimization
landscape. Further, effects that should be explained by fine-
scale texture variation may be baked into the geometry.
Acknowledgements
Min H. Kim acknowledges the MSIT/IITP of Korea (RS-
2022-00155620, 2022-0-00058, and 2017-0-00072), LIG,
and Samsung Electronics. James Tompkin acknowledges
US NSF CAREER 2144956.
20234
References
[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Si-
mon, Brian Curless, Steven M Seitz, and Richard Szeliski.
Building rome in a day. Communications of the ACM , 54
(10):105–112, 2011. 2
[2] Benjamin Attal, Selena Ling, Aaron Gokaslan, Christian
Richardt, and James Tompkin. Matryodshka: Real-time 6dof
video view synthesis using multi-sphere images. In Euro-
pean Conference on Computer Vision (ECCV) , pages 441–
459. Springer, 2020. 2
[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance fields. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5470–5479, 2022. 2
[4] Tobias Bertel, Mingze Yuan, Reuben Lindroos, and Christian
Richardt. Omniphotos: casual 360 vr photography. ACM
Transactions on Graphics (TOG) , 39(6):1–12, 2020. 2, 5
[5] Changwoon Choi, Sang Min Kim, and Young Min Kim. Bal-
anced spherical grid for egocentric view synthesis. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 16590–16599, 2023. 2, 3
[6] Kyle Gao, Yina Gao, Hongjie He, Denning Lu, Linlin Xu,
and Jonathan Li. Nerf: Neural radiance field in 3d vision,
a comprehensive review. arXiv preprint arXiv:2210.00379 ,
2022. 2
[7] Jianfei Guo, Nianchen Deng, Xinyang Li, Yeqi Bai, Bo-
tian Shi, Chiyu Wang, Chenjing Ding, Dongliang Wang,
and Yikang Li. Streetsurf: Extending multi-view im-
plicit surface reconstruction to street views. arXiv preprint
arXiv:2306.04988 , 2023. 1, 2
[8] Hyeonjoong Jang, Andreas Meuleman, Dahyun Kang,
Donggun Kim, Christian Richardt, and Min H Kim. Ego-
centric scene reconstruction from an omnidirectional video.
ACM Transactions on Graphics (TOG) , 41(4):1–12, 2022. 1,
2, 3, 5, 7
[9] Michael Kazhdan and Hugues Hoppe. Screened poisson sur-
face reconstruction. ACM Transactions on Graphics (ToG) ,
32(3):1–13, 2013. 2
[10] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe.
Poisson surface reconstruction. In Proceedings of the fourth
Eurographics symposium on Geometry processing , page 0,
2006. 2
[11] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen
Koltun. Tanks and temples: Benchmarking large-scale scene
reconstruction. ACM Transactions on Graphics (ToG) , 36
(4):1–13, 2017. 2
[12] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao,
and Guofeng Zhang. V ox-surf: V oxel-based implicit sur-
face representation. IEEE Transactions on Visualization and
Computer Graphics , 2022. 2
[13] Shigang Li. Binocular spherical stereo. IEEE Transactions
on intelligent transportation systems , 9(4):589–600, 2008. 3
[14] Zhen Li, Lingli Wang, Xiang Huang, Cihui Pan, and Jiaqi
Yang. Phyir: Physics-based inverse rendering for panoramic
indoor images. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition , pages 12713–
12723, 2022. 2
[15] Zhaoshuo Li, Thomas M ¨uller, Alex Evans, Russell H Tay-
lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.
Neuralangelo: High-fidelity neural surface reconstruction. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 8456–8465, 2023. 1, 2,
5, 7
[16] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and
Christian Theobalt. Neural sparse voxel fields. Advances
in Neural Information Processing Systems , 33:15651–15663,
2020. 3, 4
[17] Andreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang,
Changil Kim, Min H. Kim, and Johannes Kopf. Progres-
sively optimized local radiance fields for robust view synthe-
sis. In CVPR , 2023. 2
[18] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 41(4):1–15, 2022. 2, 3
[19] Michael Oechsle, Songyou Peng, and Andreas Geiger.
Unisurf: Unifying neural implicit surfaces and radiance
fields for multi-view reconstruction. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 5589–5599, 2021. 2
[20] Konstantinos Rematas, Andrew Liu, Pratul P Srini-
vasan, Jonathan T Barron, Andrea Tagliasacchi, Thomas
Funkhouser, and Vittorio Ferrari. Urban radiance fields. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 12932–12942, 2022. 2
[21] Johannes L Schonberger and Jan-Michael Frahm. Structure-
from-motion revisited. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
4104–4113, 2016. 1, 2, 5, 7
[22] Shinya Sumikura, Mikiya Shibuya, and Ken Sakurada.
Openvslam: A versatile visual slam framework. In Proceed-
ings of the 27th ACM International Conference on Multime-
dia, pages 2292–2295, 2019. 3
[23] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance fields
reconstruction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5459–
5469, 2022. 3
[24] Jiaming Sun, Xi Chen, Qianqian Wang, Zhengqi Li, Hadar
Averbuch-Elor, Xiaowei Zhou, and Noah Snavely. Neural
3d reconstruction in the wild. In ACM SIGGRAPH 2022
Conference Proceedings , pages 1–9, 2022. 1, 2
[25] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-
han, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,
and Henrik Kretzschmar. Block-nerf: Scalable large scene
neural view synthesis. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
8248–8258, 2022. 2
[26] Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Er-
rui Ding, Jingdong Wang, and Gang Zeng. Delicate textured
mesh recovery from nerf via adaptive surface refinement.
arXiv preprint arXiv:2303.02091 , 2023. 2
20235
[27] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow. In Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–
28, 2020, Proceedings, Part II 16 , pages 402–419. Springer,
2020. 3
[28] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
arXiv preprint arXiv:2106.10689 , 2021. 2, 3, 5, 7
[29] Xiuchao Wu, Jiamin Xu, Xin Zhang, Hujun Bao, Qix-
ing Huang, Yujun Shen, James Tompkin, and Weiwei Xu.
Scanerf: Scalable bundle-adjusting neural radiance fields for
large-scale scene rendering. ACM Transactions on Graphics
(TOG) , 2023. 2
[30] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan
Atzmon, Basri Ronen, and Yaron Lipman. Multiview neu-
ral surface reconstruction by disentangling geometry and ap-
pearance. In Advances in Neural Information Processing
Systems , pages 2492–2502. Curran Associates, Inc., 2020.
2, 3
[31] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. V ol-
ume rendering of neural implicit surfaces. Advances in Neu-
ral Information Processing Systems , 34:4805–4815, 2021. 2
[32] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,
and Angjoo Kanazawa. Plenoctrees for real-time rendering
of neural radiance fields. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 5752–
5761, 2021. 3, 4
[33] Zehao Yu, Anpei Chen, Bozidar Antic, Songyou Peng Peng,
Apratim Bhattacharyya, Michael Niemeyer, Siyu Tang,
Torsten Sattler, and Andreas Geiger. Sdfstudio: A unified
framework for surface reconstruction, 2022. 1, 5, 6, 7
[34] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Koltun. Nerf++: Analyzing and improving neural radiance
fields. arXiv preprint arXiv:2010.07492 , 2020. 2
20236
