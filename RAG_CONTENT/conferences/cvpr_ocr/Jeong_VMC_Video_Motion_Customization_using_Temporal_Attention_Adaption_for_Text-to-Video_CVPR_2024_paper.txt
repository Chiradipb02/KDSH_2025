VMC: Video Motion Customization using Temporal Attention Adaption for
Text-to-Video Diffusion Models
Hyeonho Jeong1* Geon Yeong Park2* Jong Chul Ye1,2
1Kim Jaechul Graduate School of AI,2Bio and Brain Engineering
Korea Advanced Institute of Science and Technology (KAIST)
* Indicates co-first authors
{hyeonho.jeong, pky3436, jong.ye }@kaist.ac.kr
Figure 1. Using only a single video portraying any type of motion, our VideoMotion Customization framework allows for generating a
wide variety of videos characterized by the same motion but in entirely distinct contexts and better spatial/temporal resolution. 8-frame
input videos are translated to 29-frame videos in different contexts while closely following the target motion. The visualized frames for the
first video are at indexes 1, 9, and 17. A comprehensive view of these motions in the form of videos can be explored at our project page.
Abstract
Text-to-video diffusion models have advanced video gener-
ation significantly. However, customizing these models to
generate videos with tailored motions presents a substan-
tial challenge. In specific, they encounter hurdles in ( a) ac-
curately reproducing motion from a target video, and ( b)
creating diverse visual variations. For example, straight-
forward extensions of static image customization methodsto video often lead to intricate entanglements of appear-
ance and motion data. To tackle this, here we present the
Video Motion Customization ( VMC ) framework, a novel
one-shot tuning approach crafted to adapt temporal atten-
tion layers within video diffusion models. Our approach
introduces a novel motion distillation objective using resid-
ual vectors between consecutive noisy latent frames as
a motion reference. The diffusion process then preserve
low-frequency motion trajectories while mitigating high-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9212
frequency motion-unrelated noise in image space. We val-
idate our method against state-of-the-art video generative
models across diverse real-world motions and contexts.
Our code and data can be found at https://video-motion-
customization.github.io/.
1. Introduction
The evolution of diffusion models [12, 26, 29] has signif-
icantly advanced Text-to-Image (T2I) generation, notably
when paired with extensive text-image datasets [3, 23].
While cascaded diffusion pipelines [2, 9, 13, 25, 31, 34, 36]
have extended this success to Text-to-Video (T2V) gener-
ation, current models lack the ability to replicate specific
motions or generate diverse variations of the same motion
with distinct visual attributes and backgrounds. Address-
ing this, we tackle the challenge of Motion Customiza-
tion [35]—adapting pre-trained Video Diffusion Models
(VDM) to produce motion-specific videos in different con-
texts, while maintaining the same motion patterns of target
subjects.
Given a few subject images for reference, appearance
customization [8, 17, 21, 22, 24, 32] in generative models
aims to fine-tune models to generate subject images in di-
verse contexts. However, these approaches, despite varying
optimization objectives, commonly strive for faithful im-
age (frame) reconstruction by minimizing the ℓ2-distance
between predicted and ground-truth noise. This may lead to
theentangled learning of appearance and motion.
To tackle this, we present VMC , a new framework aimed
at adapting pre-trained VDM’s temporal attention layers
via our proposed Motion Distillation objective. This ap-
proach utilizes residual vectors between consecutive (la-
tent) frames to obtain the motion vectors that trace mo-
tion trajectories in the target video. Consequently, we fine-
tune VDM’s temporal attention layers to align the ground-
truth image-space residuals with their denoised estimates,
which equivalently aligns predicted and ground-truth source
noise differences and motion vectors within VDM. This
enables lightweight and fast one-shot training. To further
facilitate the appearance-invariant motion distillation, we
transform faithful text prompts into appearance-invariant
prompts, e.g. "A bird is flying above a lake
in the forest" Ñ"A bird is flying" in Fig.
1. This encourages the modules to focus on the motion in-
formation and ignore others, such as appearance, structure,
background, etc. During inference, our procedure initiates
by sampling key-frames using the adapted key-frame gener-
ation U-Net, followed by temporal interpolation and spatial
super-resolution. To summarize, VMC makes the following
key contributions:
• We introduce a novel fine-tuning strategy which focuses
solely on temporal attention layers in the key-frame gen-eration module. This enables lightweight training (15GB
vRAM) and fast training ( ă5 minutes).
• To our knowledge, we mark a pioneering case of fine-
tuning only the temporal attention layers in video dif-
fusion models, without optimizing spatial self or cross-
attention layers, while achieving successful motion cus-
tomization.
• We introduce a novel motion distillation objective that
leverages the residual vectors between consecutive (la-
tent) frames as motion vectors.
• We present the concept of appearance-invariant prompts,
which further facilitates the process of motion learning
when combined with our motion distillation loss.
2. Preliminaries
Diffusion Models. Diffusion models aim to generate
samples from the Gaussian noise through iterative denois-
ing processes. Given a clean sample x0„pdatapxq, the
forward process is defined as a Markov chain with forward
conditional densities
ppxt|xt´1q“Npxt|βtxt´1,p1´βtqIq
ptpxt|x0q“Npxt|?
¯αx0,p1´¯αqIq,(1)
where xtPRdis a noisy latent variable at a timestep tthat
has the same dimension as x0, andβtdenotes an increasing
sequence of noise schedule where αt:“1´βtand¯αt:“
Πt
i“1αi. Then, the goal of diffusion model training is to
obtain a residual denoiser ϵθ:
min
θExt„ptpxt|x0q,x0„pdatapx0q,ϵ„Np0,Iq“
∥ϵθpxt, tq´ϵ∥‰
.
(2)
It can be shown that this epsilon matching in (2) is equiva-
lent to the Denoising Score Matching (DSM [14, 28]) with
different parameterization:
min
θExt,x0,ϵ“st
θpxtq´∇xtlogptpxt|x0q‰
,(3)
where sθ˚pxt, tq » ´xt´?¯αtx0
1´¯α“ ´1?1´¯αtϵθ˚pxt, tq.
The reverse sampling from qpxt´1|xt,ϵθ˚pxt, tqqis then
achieved by
xt´1“1?αt´
xt´1´αt?1´¯αtϵθ˚pxt, tq¯
`˜βtϵ,(4)
where ϵ„Np0,Iqand˜βt:“1´¯αt´1
1´¯αtβt. To accelerate
sampling, DDIM [27] further proposes another sampling
method as follows:
xt´1“?¯αt´1ˆx0ptq`b
1´¯αt´1´η2˜βt2ϵθ˚pxt, tq`η˜βtϵ,
(5)
where ηP r0,1sis a stochasticity parameter, and ˆx0ptqis
the denoised estimate which can be equivalently derived us-
ing Tweedie’s formula [6]:
ˆx0ptq:“1?¯αtpxt´?
1´¯αtϵθ˚pxt, tqq. (6)
9213
Figure 2. Overview . The proposed Video Motion Customization ( VMC ) framework distills the motion trajectories from the residual
between noisy latent frames, namely motion vector δvn
t. Specifically, we fine-tune only the temporal attention layers of the key-frame gen-
eration model by aligning the ground-truth and predicted motion vectors. After training, the customized key-frame generator is leveraged
for target motion-driven video generation with new appearances context, e.g. "A chicken is walking in a city" .
For a text-guided Diffusion Model, the training objective is
often given by:
min
θExt,x0,ϵ,c“
∥ϵθpxt, t, cq´ϵ∥‰
, (7)
where crepresents the textual embedding. Throughout this
paper, we will often omit cfromϵθpxt, t, cqif it does not
lead to notational ambiguity.
Video Diffusion Models. Video diffusion models [11, 13,
34] further attempt to model the video data distribution.
Specifically, Let pvnqnPt1,...,Nurepresents the N-frame in-
put video sequence. Then, for a given n-th frame vnPRd,
letv1:NPRNˆdrepresents a whole video vector. Let
vn
t“?¯αtvn`?1´¯αtϵn
trepresents the n-th noisy frame
latent sampled from ptpvn
t|vnq, where ϵn
t„Np0, Iq. We
similarly define pvn
tqnP1,...,N ,v1:N
t, andϵ1:N
t. The goal of
video diffusion model training is then to obtain a residual
denoiser ϵθwith textual condition cand video input that
satisfies:
min
θEv1:N
t,v1:N,ϵ1:N
t,c“ϵθpv1:N
t, t, cq´ϵ1:N
t‰
,(8)
where ϵθpv1:N
t, t, cq,ϵ1:N
tPRNˆd. In this work, we denote
the predicted noise of n-th frame as ϵn
θpv1:N
t, t, cqPRd.
In practice, contemporary video diffusion models of-
ten employ cascaded inference pipelines for high-resolution
outputs. For instance, [34] initially generates a low-
resolution video with strong text-video correlation, further
enhancing its resolution via temporal interpolation and spa-
tial super-resolution modules.
In exploring video generative tasks through diffusion
models, two primary approaches have emerged: leverag-
ing foundational Video Diffusion Models (VDMs) [7, 16,
30, 35] or pre-trained Text-to-Image (T2I) models [4, 10,15, 32]. To extend image diffusion models to videos, sev-
eral architectural modifications are made. Typically, U-Net
generative modules integrate temporal attention blocks after
spatial attentions [11]. Moreover, 2D convolution layers are
inflated to 3D convolution layers by altering kernels [11].
3. Video Motion Customization
Given an input video, our main goal is to ( a) distill
the motion patterns M˚of target subjects, and ( b) cus-
tomize the input video in different contexts while main-
taining the same motion patterns M˚, e.g. Sharks w/
motion M˚ÑAirplanes w/ motion M˚, with
minimal computational costs.
To this end, we propose a novel video motion customiza-
tion framework, namely VMC , which leverages cascaded
video diffusion models with robust temporal priors. One
notable aspect of the proposed framework is that we per-
form fine-tuning only on the key-frame generation module,
also referred to as the T2V base model, within the cas-
caded VDMs, which guarantees computational and mem-
ory efficiency. Specifically, within the key-frame genera-
tion model, our fine-tuning process only targets the tempo-
ral attention layers. This facilitates adaptation while pre-
serving the model’s inherent capacity for generic synthesis.
Notably, we freeze the subsequent frame interpolation and
spatial super-resolution modules as-is (Fig. 2).
3.1. Temporal Attention Adaptation
In order to distill the motion M˚, we first propose a new
objective function for temporal attention adaptation using
residual cosine similarity. Our intuition is that residual vec-
tors between consecutive frames may include information
about the motion trajectories.
LetpvnqnPt1,...,Nurepresents the N-frame input video
9214
Figure 3. Training. The proposed framework aims to learn motion
byδϵn
t-alignment using (16) or (17). Note that we only fine-tune
the temporal attention layers in the key-frame generation U-Net.
The blue circle represents the diffusion forward process.
sequence. As defined in Section 2, for a given noisy video
latent vector v1:N
twithϵ1:N
t, letvn
trepresents the n-th
noisy frame latent sampled from ptpvn
t|vnqwithϵn
t. We
will interchangeably use vnandvn
0for notational simplic-
ity. Likewise, vn`c
t is defined as vn
t, with cą0repre-
senting the fixed frame stride. Then, we define the frame
residual vector at time tě0as
δvn
t:“vn`c
t´vn
t, (9)
where we similarly define the epsilon residual vector δϵn
t.
In the rest of the paper, we interchangeably use frame resid-
ual vector and motion vector .
We expect that these motion vectors may encode infor-
mation about motion patterns, where such information may
vary depending on the time tand its corresponding noise
level. The difference vector δvn
tcan be delineated as:
δvn
t“?¯αtpvn`c
0´vn
0q`?
1´¯αtpϵn`c
t´ϵn
tq
“?¯αtδvn
0`?
1´¯αtδϵn
t,(10)
where δϵn
tis normally distributed with zero mean and 2I
variance. In essence, δvn
tcan be acquired through the fol-
lowing diffusion kernel:
ppδvn
t|δvn
0q“Npδvn
t|?¯αtδvn
0,2p1´¯αtqIq.(11)
In light of this, our goal is to transfer motion information
to the temporal attention layers by leveraging the motion
vectors. For this, we first simulate the motion vectors using
video diffusion models. Specifically, as similarly done in
(6), the denoised video vector estimates ˆv01:Nptqcan be
derived by applying Tweedie’s formula:
ˆv1:N
0ptq:“1?¯αt`
v1:N
t´?
1´¯αtϵθpv1:N
t, tq˘
,(12)
where ˆv1:N
0ptqis an empirical Bayes optimal posterior
expectation Erv1:N
0|v1:N
ts. Then, the denoised motion
vector estimate δˆvn
0can be defined in terms of δvn
tand
δϵn
θpv1:N
t, tqby using (12):
δˆvn
0ptq:“1?¯αt´
δvn
t´?
1´¯αtδϵn
θ,t¯
, (13)where δϵn
θpv1:N
t, tq:“ϵn`c
θpv1:N
t, tq´ϵn
θpv1:N
t, tqis ab-
breviated as δϵn
θ,tfor notational simplicity. Similarly, one
can obtain ground-truth motion vector δvn
0by using (10):
δvn
0“1?¯αt´
δvn
t´?
1´¯αtδϵn
t¯
. (14)
Then, our objective is to finetune θbyaligning the mo-
tion vector δvn
0and its denoised estimate δˆvn
0ptq:
min
θEt,n,ϵt,n,ϵt,n`c”
ℓalign`
δvn
0, δˆvn
0ptq˘ı
, (15)
with a loss function ℓalign:RdˆRdÑR. By using ℓ2-
distance for ℓalign, this is equivalent to matching δϵn
θ,tand
δϵn
t:
ℓalign`
δvn
0, δˆvn
0ptq˘
“1´¯αt
¯αtδϵn
t´δϵn
θ,t2. (16)
Notably, aligning the ground-truth and predicted motion
vectors translates into aligning epsilon residuals.
While this objective demonstrates effective empirical
performance, our additional observations indicate that using
ℓcospδϵn
t, δϵn
θ,tqmay further improve the distillation, where
ℓcospx,yq “1´xx,yy
}x}}y}forx,yPRd(more analysis in
section 4.3). Accordingly, our optimization framework is
finally defined as follows:
min
θEt,n,ϵt,n,ϵt,n`crℓcospδϵn
t, δϵn
θ,tqs. (17)
In other words, the proposed optimization framework aims
tomaximize the residual cosine similarity between δϵn
tand
δϵn
θ,t. Hence, this optimization approach can be seamlessly
applied to video diffusion models trained using epsilon-
matching. Practically, we exclusively fine-tune the temporal
attention layers θTAĂθ, originally designed for dynamic
temporal data assimilation [32]. The frame stride remains
fixed at c“1across all experiments.
3.2. Appearance-invariant Prompts
In motion distillation, it is crucial to filter out disruptive
variations that are unrelated to motion. These variations
may include changes in appearance and background, dis-
tortions, consecutive frame inconsistencies, etc. To achieve
this, we further utilize appearance-invariant prompts . Di-
verging from traditional generative customization frame-
works [21, 22, 32, 35] that rely on text prompts that “faith-
fully” describe the input image or video during model fine-
tuning, our framework purposedly employs “unfaithful”
text prompts during the training phase. Specifically, our
approach involves the removal of background information.
For instance, the text prompt ‘a cat is roaring on the grass
under the tree’ is simplified to ‘a cat is roaring’ as presented
in Fig. 4. This reduces background complexity as in Fig. 4a
comapred to Fig. 4b, facilitating the application of new ap-
pearance in motion distillation.
9215
Figure 4. Appearance-invariant Prompt . Comparison of input
reconstruction with and without appearance-invariant prompt: (a)
and (b) depict sampled low-resolution (64x40) keyframes. For
(a), the training prompt used was “A cat is roaring,” while for
(b), the training prompt was “A cat is roaring on the grass under
the tree.” Our appearance-invariant prompt enables the removal of
background information that can disturb motion distillation.
3.3. Inference Pipeline
Once trained, in the inference phase, our process be-
gins by computing inverted latents from the input video
through DDIM inversion. Subsequently, the inverted latents
are fed into the temporally fine-tuned keyframe generation
model, yielding short and low-resolution keyframes. These
keyframes then undergo temporal extension using the un-
altered frame interpolation model. Lastly, the interpolated
frames are subjected to spatial enlargement through the spa-
tial super-resolution model. Refer to Fig. 2 for overview.
4. Experiments
4.1. Implementation Details
In our experiments, we choose Show-1 [34] as our VDM
backbone and its publicly available pre-trained weights. All
experiments were conducted using a single NVIDIA RTX
6000 GPU. VMC with Show-1 demonstrates efficient re-
source usage, requiring only 15GB of vRAM during mixed-
precision training [18], which is completed within 5 min-
utes. During inference, generating a single video compris-
ing 29 frames at a resolution of 576 x 320 consumes 18GB
of vRAM and takes approximately 12 minutes.
4.2. Baseline Comparisons
Dataset Selection. In our experiments, we draw upon a
dataset that comprises 24 videos. These videos encompass
a broad spectrum of motion types occurring in various con-
texts, encompassing vehicles, humans, birds, plants, diffu-
sion processes, mammals, sea creatures, and more. This di-
versity provides a comprehensive range of motion scenarios
for our assessment. Out of these 24 videos, 13 are sourced
from the DA VIS dataset [19], 10 from the WebVid dataset
[1], and 1 video is obtained from LAMP [33].
Baselines. Our method is compared against four contem-
porary baselines that integrate depth map signals into the
diffusion denoising process to assimilate motion informa-
tion. Notably, our approach operates without the necessity
of depth maps during both training and inference, in con-
trast to these baseline methods.Specifically, VideoComposer (VC) [30] is an open-
source latent-based video diffusion model tailored for com-
positional video generation tasks. Gen-1 [7] introduces a
video diffusion architecture incorporating additional struc-
ture and content guidance for video-to-video translation. In
contrast to our targeted fine-tuning of temporal attention,
Tune-A-Video (TA V) [32] fine-tunes self, cross, and tem-
poral attention layers within a pre-trained, but inflated T2I
model on input videos. Control-A-Video (CA V) [5] in-
troduces a controllable T2V diffusion model utilizing con-
trol signals and a first-frame conditioning strategy. Notably,
while closely aligned with our framework, Motion Director
[35] lacks available code at the time of our research.
Qualitative Results. We offer visual comparisons of our
method against four baselines in Fig. 5. The compared
baselines face challenges in adapting the motion of the input
video to new contexts. They exhibit difficulties in applying
the overall motion, neglecting the specific background indi-
cated in the target text (e.g., “underwater” or “on the sand”).
Additionally, they face difficulties in deviating from the
original shape of the subject in the input video, leading to
issues like a shark-shaped airplane, an owl-shaped seagull,
or preservation of the shape of the ground where a seagull
is taking off. In contrast, the proposed framework succeeds
in motion-driven customization, even for difficult composi-
tional customization, e.g. Two sharks are moving.
ÑTwo airplanes are moving in the sky .
Quantitative Results. We further quantitatively demon-
strate the effectiveness of our method against the baselines
through automatic metrics and user study.
Automatic Metrics. We use CLIP [20] for automatic met-
rics. For textual alignment, we compute the average co-
sine similarity between the target prompt and the generated
frames. In terms of frame consistency, we obtain CLIP im-
age features within the output video and then calculate the
average cosine similarity among all pairs of video frames.
For methods that generate temporally interpolated frames,
we utilized the keyframe indexes to calculate the metric for
a fair evaluation. To illustrate, in the case of VMC, which
takes an 8-frame input and produces a 29-frame output, we
considered the frames at the following indexes: 1, 5, 9, 13,
17, 21, 25, 29 . As shown in Table 1, VMC outperforms
baselines in both text alignment and temporal consistency.
User Study. We conducted a survey involving a total of 27
participants to assess four key aspects: the preservation of
motion between the input video and the generated output
video, appearance diversity in the output video compared to
the input video, the text alignment with the target prompt,
and the overall consistency of the generated frames. The
survey utilized a rating scale ranging from 1 to 5. For
assessing motion preservation, we employed the question:
9216
Figure 5. Qualitative comparison against state-of-the-art baselines. In contrast to other baselines, the proposed framework succeeds in
motion-driven customization, even for difficult compositional customization.
9217
Figure 6. Comparative analysis of the proposed frameworks with fine-tuning ( a) temporal attention and ( b) self- and cross-attention layers.
Figure 7. Comparative analysis of the proposed frameworks with ( a)ℓcosand (b)ℓ2loss functions.
“To what extent is the motion of the input video retained in
the output video?” To evaluate appearance diversity, partic-
ipants were asked: “To what extent does the appearance of
the output video avoid being restricted on the input video’s
appearance?” Tab. 1 shows that our method surpasses the
baselines in all four aspects.
Text Temporal Motion Appearance Text Temporal
Alignment Consistency Preservation Diversity Alignment Consistency
VC 0.798 0.958 3.45 3.43 2.96 3.03
Gen-1 0.780 0.957 3.46 3.17 2.87 2.73
TA V 0.758 0.947 3.50 2.88 2.67 2.80
CA V 0.764 0.952 2.75 2.45 2.07 2.00
Ours 0.801 0.959 4.42 4.54 4.56 4.57
Table 1. Quantitative evaluation using CLIP ( left) and user study
(right ). VMC significantly outperforms the other baselines.
4.3. Ablation Studies
Comparisons on attention layers. We conducted a com-
parative study evaluating the performance of fine-tuning:
(a) temporal attention layers and ( b) self- and cross-
attention layers. Illustrated in Fig. 6, both frameworks
exhibit proficient motion learning capabilities. Notably,
the utilization of customized temporal attention layers ( a)
yields smoother frame transitions, indicating the effective-
ness of the optimization framework (17) in encouraging mo-
tion distillation, with a slight preference observed for cus-
tomized temporal attention layers.This observation stems from the premise that integrat-
ing the proposed motion distillation objective (17) may
autonomously and accurately embed motion information
within temporal attention layers [11, 13]. This suggests a
potential application of the motion distillation objective for
training large-scale video diffusion models, warranting fur-
ther exploration in future research endeavors.
Choice of loss functions. In addition, we conducted a
comparative analysis on distinct training loss functions in
(17): the ℓ2-distance and ℓcosas delineated in (17). As
depicted in Fig. 7, the δϵ-matching process in (15) and
(17) demonstrates compatibility with generic loss functions.
While both ℓ2pδϵn
t, δϵn
θ,tqandℓcospδϵn
t, δϵn
θ,tqare promis-
ing objectives, the marginal superiority of ℓcospδϵn
t, δϵn
θ,tq
led to its adoption for visualizations in this study.
Importance of adaptation. To assess the importance of
temporal attention adaptation, we conducted a visualization
of customized generations without temporal attention adap-
tation, as detailed in Section 3.1. Specifically, from our
original architecture in Fig. 2, we omitted attention adap-
tation and performed inference by maintaining the U-Net
modules in a frozen state. The outcomes depicted in Fig. 9
indicate that while DDIM inversion guides the generations
9218
Figure 8. Left: Style transfer on two videos. Right : Motion customization results on the video of “A seagull is walking backward .”
Figure 9. Ablation study on temporal attention adaptation. With-
out temporal attention adaptation, motion distillation fails.
to mimic the motion of the input video, it alone does not en-
sure successful motion distillation. The observed changes
in appearance and motion exhibit an entangled relationship.
Consequently, this underlines the necessity of an explicit
motion distillation objective to achieve consistent motion
transfer, independent of any alterations in appearance.
4.4. Additional results
Video Style Transfer. We illustrate video style transfer ap-
plications in Fig. 8- Left. We incorporate style prompts at the
end of the text after applying appearance-invariant prompt-
ing (see Section 3.2). Target styles are fluidly injected while
preserving the distilled motion of an input video.
Learning Backward Motion. To further verify our video
motion customization capabilities, we present a challeng-
ing scenario: extracting backward motion from a reversedvideo sequence where frames are arranged in reverse or-
der. This scenario, an exceedingly rare event in real-world
videos, is highly improbable within standard training video
datasets [1]. Illustrated in Fig. 8, our VMC framework
showcases proficiency in learning “a bird walking back-
ward” motion and generating diverse videos with distinct
subjects and backgrounds. This capability not only enables
leveraging the distilled motion but also offers prospects for
further contextual editing.
5. Conclusion
This paper introduces Video Motion Customization
(VMC), addressing challenges in adapting Text-to-Video
(T2V) models to generate motion-driven diverse visual cus-
tomizations. Existing models struggle with accurately repli-
cating motion from a target video and creating varied visual
outputs, leading to entanglements of appearance and mo-
tion data. To overcome this, our VMC framework presents a
novel one-shot tuning approach, focusing on adapting tem-
poral attention layers within video diffusion models. This
framework stands out for its efficiency in time and memory,
ease of implementation, and minimal hyperparameters.
Ethics Statement. Our work employs a generative model
that necessitates caution due to its potential for misuse in
creating deceptive content with negative societal impacts.
This research was supported by Field-oriented Technology Devel-
opment Project for Customs Administration through National Research
Foundation of Korea (NRF) funded by the Ministry of Science & ICT
and Korea Customs Service (NRF-2021M3I1A1097938), Institute of In-
formation & communications Technology Planning & Evaluation (IITP)
grant funded by the Korea government (MSIT, Ministry of Science and
ICT) (No. 2022-0-00984, Development of Artificial Intelligence Tech-
nology for Personalized Plug-and-Play Explanation and Verification of
Explanation), Korea Medical Device Development Fund grant funded
by the Korea government (the Ministry of Science and ICT, the Min-
istry of Trade, Industry and Energy, the Ministry of Health & Welfare,
the Ministry of Food and Drug Safety) (Project Number: 1711137899,
KMDF PR20200901 0015), Institute of Information & communications
Technology Planning & Evaluation (IITP) grant funded by the Korea
government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate
School Program (KAIST)).
9219
References
[1] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 1728–1738,
2021. 5, 8
[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
22563–22575, 2023. 2
[3] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun
Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m:
Image-text pair dataset. https://github.com/
kakaobrain/coyo-dataset , 2022. 2
[4] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.
Pix2video: Video editing using image diffusion. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 23206–23217, 2023. 3
[5] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,
Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video:
Controllable text-to-video generation with diffusion models.
arXiv preprint arXiv:2305.13840 , 2023. 5
[6] Bradley Efron. Tweedie’s formula and selection bias. Jour-
nal of the American Statistical Association , 106(496):1602–
1614, 2011. 2
[7] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
Jonathan Granskog, and Anastasis Germanidis. Structure
and content-guided video synthesis with diffusion models.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 7346–7356, 2023. 3, 5
[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618 , 2022. 2
[9] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew
Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-
Yu Liu, and Yogesh Balaji. Preserve your own correlation:
A noise prior for video diffusion models. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 22930–22941, 2023. 2
[10] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.
Tokenflow: Consistent diffusion features for consistent video
editing. arXiv preprint arXiv:2307.10373 , 2023. 3
[11] J Ho, T Salimans, A Gritsenko, W Chan, M Norouzi, and DJ
Fleet. Video diffusion models. arxiv 2022. arXiv preprint
arXiv:2204.03458 . 3, 7
[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 2
[13] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High definition video generation with diffusion mod-
els.arXiv preprint arXiv:2210.02303 , 2022. 2, 3, 7[14] Aapo Hyv ¨arinen and Peter Dayan. Estimation of non-
normalized statistical models by score matching. Journal
of Machine Learning Research , 6(4), 2005. 2
[15] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zero-
shot grounded video editing using text-to-image diffusion
models. arXiv preprint arXiv:2310.01107 , 2023. 3
[16] Hyeonho Jeong, Jinho Chang, Geon Yeong Park, and
Jong Chul Ye. Dreammotion: Space-time self-similarity
score distillation for zero-shot video editing. arXiv preprint
arXiv:2403.12002 , 2024. 3
[17] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu.
Subject-diffusion: Open domain personalized text-to-image
generation without test-time fine-tuning. arXiv preprint
arXiv:2307.11410 , 2023. 2
[18] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory
Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael
Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed
precision training. arXiv preprint arXiv:1710.03740 , 2017.
5
[19] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-
bel´aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017
davis challenge on video object segmentation. arXiv preprint
arXiv:1704.00675 , 2017. 5
[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 5
[21] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500–
22510, 2023. 2, 4
[22] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,
Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,
and Kfir Aberman. Hyperdreambooth: Hypernetworks for
fast personalization of text-to-image models. arXiv preprint
arXiv:2307.06949 , 2023. 2, 4
[23] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. Advances in Neural In-
formation Processing Systems , 35:25278–25294, 2022. 2
[24] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-
booth: Personalized text-to-image generation without test-
time finetuning. arXiv preprint arXiv:2304.03411 , 2023. 2
[25] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. arXiv preprint arXiv:2209.14792 ,
2022. 2
[26] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
9220
ence on machine learning , pages 2256–2265. PMLR, 2015.
2
[27] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 2
[28] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. Advances in neural
information processing systems , 32, 2019. 2
[29] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. arXiv preprint arXiv:2011.13456 , 2020. 2
[30] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,
Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,
and Jingren Zhou. Videocomposer: Compositional video
synthesis with motion controllability. arXiv preprint
arXiv:2306.02018 , 2023. 3, 5
[31] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,
Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo
Yu, Peiqing Yang, et al. Lavie: High-quality video gener-
ation with cascaded latent diffusion models. arXiv preprint
arXiv:2309.15103 , 2023. 2
[32] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu
Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning
of image diffusion models for text-to-video generation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7623–7633, 2023. 2, 3, 4, 5
[33] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi
Li, and Xiangyu Zhang. Lamp: Learn a motion pat-
tern for few-shot-based video generation. arXiv preprint
arXiv:2310.10769 , 2023. 5
[34] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,
Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and
Mike Zheng Shou. Show-1: Marrying pixel and latent dif-
fusion models for text-to-video generation. arXiv preprint
arXiv:2309.15818 , 2023. 2, 3, 5
[35] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao
Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng
Shou. Motiondirector: Motion customization of text-to-
video diffusion models. arXiv preprint arXiv:2310.08465 ,
2023. 2, 3, 4, 5
[36] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
generation with latent diffusion models. arXiv preprint
arXiv:2211.11018 , 2022. 2
9221
