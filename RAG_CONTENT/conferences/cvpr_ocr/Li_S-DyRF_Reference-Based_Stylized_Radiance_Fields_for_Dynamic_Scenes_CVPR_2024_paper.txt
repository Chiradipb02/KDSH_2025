S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes
Xingyi Li1,2Zhiguo Cao1Yizheng Wu1,2Kewei Wang1,2Ke Xian1,2*Zhe Wang3Guosheng Lin2*
1School of AIA, Huazhong University of Science and Technology
2S-Lab, Nanyang Technological University3SenseTime Research
{xingyi li,zgcao,kxian }@hust.edu.cn, gslin@ntu.edu.sg
https://xingyi-li.github.io/s-dyrf
Figure 1. Our method can stylize dynamic 3D scenes, maintaining semantic consistency with the given reference image across both spatial
and temporal dimensions. Besides novel time synthesis (the leftmost one), our stylized dynamic radiance field can synthesize novel views
(the second one) or perform space-time view synthesis (the third one) [28, 30, 41]. Furthermore, our method can also stylize synthetic
objects (the rightmost one). We encourage readers to experience the animations by viewing them with Adobe Acrobat or KDE Okular.
Abstract
Current 3D stylization methods often assume static
scenes, which violates the dynamic nature of our real world.
To address this limitation, we present S-DyRF , a reference-
based spatio-temporal stylization method for dynamic neu-
ral radiance fields. However, stylizing dynamic 3D scenes
is inherently challenging due to the limited availability of
stylized reference images along the temporal axis. Our key
insight lies in introducing additional temporal cues besides
the provided reference. To this end, we generate temporal
pseudo-references from the given stylized reference. These
pseudo-references facilitate the propagation of style infor-
mation from the reference to the entire dynamic 3D scene.
For coarse style transfer, we enforce novel views and times
to mimic the style details present in pseudo-references at the
feature level. To preserve high-frequency details, we create
a collection of stylized temporal pseudo-rays from temporal
pseudo-references. These pseudo-rays serve as detailed and
explicit stylization guidance for achieving fine style trans-
fer. Experiments on both synthetic and real-world datasets
demonstrate that our method yields plausible stylized re-
sults of space-time view synthesis on dynamic 3D scenes.
*Corresponding author.1. Introduction
Style transfer [10, 17, 20, 23, 26, 29, 43] involves the pro-
cess of reimagining an image’s content by emulating the
artistic style of another. While it can produce high-quality
stylized images, it confines the results to an identical view-
point as the content image. In recent years, there has
been a burgeoning demand for the stylization and editing
of 3D scenes and objects across a range of industries, en-
compassing the realms of gaming, cinematic experiences,
and mixed reality applications. Previous methods primarily
concentrate on stylizing 3D objects and scenes using point
clouds [2, 16, 34, 39], meshes [14, 21], or applying style
transfer to both the geometry and texture [50].
As neural radiance fields continue to evolve at a rapid
pace [1, 3, 27, 33, 35], recent advancements [5–7, 11, 16,
18, 32, 36, 40, 46, 47] have incorporated neural radiance
fields into 3D stylization, significantly easing the process of
transferring style from any 2D style reference to 3D scenes.
While these methods excel at producing high-quality ge-
ometrically consistent 3D content, they often fall short in
providing users, including artists and designers, with fine-
grained control over the stylization process, which can be
limiting in achieving the precise aesthetic effects desired.
To address this challenge, several methods have emerged
as promising solutions. For example, Ref-NPR [54] intro-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20102
duces a reference-guided, controllable approach to scene
stylization. This method empowers users to create stylized
views while ensuring both geometric and semantic consis-
tency with a provided stylized reference image. Despite
a significant step towards enhancing the controllability of
stylization in 3D scenes, these methods typically rely on
the assumption that the scene is static. However, in our
dynamic world, scenes often feature diverse dynamic con-
tent, underscoring the increasing need to develop stylization
methods suitable for dynamic 3D scenes.
In this paper, our objective is to take the first step towards
reference-based stylized neural radiance fields for dynamic
3D scenes. The challenge at hand involves not only styl-
izing the 3D scenes but also addressing the temporal di-
mension, a factor that distinguishes it from the stylization
of static 3D scenes. One intuitive approach to address this
challenge is directly applying video-based stylization meth-
ods [4, 15, 19, 45]. While these methods excel at generat-
ing stylized videos that faithfully preserve the texture of the
original style exemplar, they may deviate from the desired
style when applied to novel view synthesis and can suffer
from flickering issues.
To tackle the challenges outlined above, we introduce a
novel paradigm S-DyRF for stylizing dynamic 3D scenes,
utilizing either a single or a small number of stylized refer-
ence images. Considering the scarcity of stylized reference
images along the time axis, our key insight lies in the intro-
duction of additional temporal cues to propagate style in-
formation throughout different timestamps. To achieve this,
our method starts by generating temporal pseudo-references
that act as effective guidance to ensure style consistency
across the temporal domain. Next, we establish a mapping
of content and style, followed by transferring the style infor-
mation contained within those pseudo-references to novel
views and times at the feature level. To preserve details,
we also draw inspiration from Ref-NPR [54] and introduce
the Temporal Reference Ray Registration process. Through
this process, a collection of stylized temporal pseudo-rays
is generated from the pseudo-references. These pseudo-
rays then act as a powerful tool for offering detailed and
explicit stylization guidance for achieving fine style trans-
fer. Bringing everything together, our S-DyRF is capable
of synthesizing stylized novel views and times that uphold
semantic consistency with the given reference image across
both spatial and temporal dimensions.
In summary, our main contributions are:
• We propose a new task of stylizing dynamic 3D scenes
using one or a small number of stylized reference images.
• We introduce S-DyRF, a reference-based spatio-temporal
stylization method for dynamic neural radiance fields.
• Our method showcases superior qualitative and quantita-
tive results, as well as providing controllability for cus-
tomizing dynamic 3D scenes.2. Related Work
Image and video style transfer. Style transfer involves the
process of recomposing the content of an image to emu-
late the style of another. Pioneering endeavors in this field
can be traced back to image analogies [13]. This approach
and follow-up work [8, 12, 31] deviate from manual fil-
ter programming and, instead, automatically learn filters
from examples. However, they necessitate a certain level of
semantic similarity between the content and style images.
Since Gatys et al. [10] initially proposed the idea of utiliz-
ing high-level features extracted from pre-trained deep neu-
ral networks to represent image style, significant progress
[17, 20, 23, 26, 29, 43] has been made in the field of neu-
ral style transfer. In contrast to example-based methods,
neural style transfer exhibits the capability to apply artis-
tic stylization to various images, offering greater versatil-
ity in the stylization process. Although exhibiting similar-
ities with image style transfer, video style transfer meth-
ods [4, 15, 19, 45] primarily concentrate on maintaining
temporal consistency throughout the entire video sequence.
As a representative method of video style transfer, Texler
et al. [45] develop an appearance translation network from
the beginning, utilizing just a handful of stylized exemplars
to extend the desired style across the entire video sequence.
Nonetheless, these approaches primarily target enhancing
short-term consistency between adjacent frames, and they
lack the capability to facilitate novel view synthesis.
3D style transfer. Despite the extensive exploration of
style transfer within the image domain, the field of 3D style
transfer is still in its infancy. Prior methods have mainly
focused on stylizing individual 3D objects, utilizing point
clouds [39], meshes [21], or implementing style transfer on
both geometry and texture [50]. Recent developments have
also directed attention towards stylizing entire 3D scenes.
This can be achieved by explicitly applying style transfer on
point cloud [2, 16, 34] or mesh [14] representations. How-
ever, these methods may encounter issues arising from im-
perfect geometry and texture rendering and are limited to
static scenes.
Stylizing radiance fields. Fueled by the remarkable ad-
vancements in novel view synthesis achieved by neural ra-
diance fields [33, 48], the research community has been en-
ergized to delve deeper into the emerging domain of styl-
izing radiance fields [5–7, 11, 16, 18, 32, 36, 40, 46, 47],
i.e., transferring style to 3D scenes represented as neural
radiance fields. In particular, ARF [51] introduces a near-
est neighbor-based loss that can capture style details while
maintaining multi-view consistency. Although ARF excels
in producing high-quality artistic 3D content, it does not af-
ford explicit control over the stylized results. To control the
stylization, Pang et al. [37] propose employing hash-grid
encoding for learning the appearance and geometry embed-
dings, thereby facilitating local style transfer. For enhanced
20103
Generating Temporal Pseudo -
References
2D Style Transfer
Photorealistic Dynamic 
Radiance Field
Stylized Dynamic Radiance Field
Stylized Space -Time View SynthesisSpati o-Temporal Style Transfer
…ℒfine
ℒcoarse
Temporal Pseudo -RaysCoarse Style Guidance 
Feature MapFigure 2. An overview of our method. Given a pre-trained photorealistic dynamic radiance field, we first render a reference view at time
kfrom a specific reference camera. Following that, the reference view undergoes a 2D style transfer using an appropriate method, e.g.,
manual editing, NNST [23], or ControlNet [52], to produce a stylized reference image. To propagate the style information from the stylized
reference to other timestamps, we generate temporal pseudo-references and apply spatio-temporal style transfer to optimize our dynamic
radiance field. Once this stylization is done, we can yield plausible stylized results of space-time view synthesis on dynamic 3D scenes.
user control over the stylization process, Ref-NPR [54] in-
troduces a reference-guided, controllable scene stylization
method. This method empowers users to generate stylized
novel views that maintain geometric and semantic consis-
tency with a given stylized reference image. However, these
methods are limited to stylizing static scenes. Therefore,
we are interested in taking the first step towards reference-
based stylized neural radiance fields for dynamic 3D scenes.
3. Method
3.1. Overview
We aim to stylize dynamic 3D scenes using one or a few
stylized reference images as input. The expected output of
our method is stylized novel views and times that main-
tain semantic consistency with the given reference image
across both spatial and temporal dimensions. Fig. 2 presents
a schematic depiction of our pipeline. We assume a pre-
trained photorealistic dynamic radiance field FΘ(e.g., Hex-
Plane [1]) that is reconstructed from multiple images or
videos capturing complex, real-world, dynamic 3D scenes,
and denote our stylized dynamic radiance field as GΘ. To
simplify our discussion without sacrificing generality, we
focus on scenarios involving only a single reference image.
We first render a reference view Ik
FΘ,Rat time kfrom a
specific reference camera pR. Following that, the reference
viewIk
FΘ,Rundergoes a 2D stylization process using an ap-propriate method, e.g., manual editing, NNST [23], or Con-
trolNet [52], to produce a stylized reference image Sk
R.
Given the limited number of stylized reference images
along the time axis, we propose to introduce additional tem-
poral cues and explicitly propagate the style information
from the stylized reference to other timestamps to enable
a more coherent and consistent stylization across the en-
tire time series. To achieve this, our method starts by gen-
erating temporal pseudo-references that serve as effective
sources of supervision for maintaining style consistency
across the temporal domain. We then establish a coarse
content-style mapping and broadcast the style information
contained in pseudo-references to novel views and times at
the feature level, facilitating a coarse style transfer. To pre-
serve high-frequency details, we also draw inspiration from
Ref-NPR [54] and introduce the Temporal Reference Ray
Registration process. This process produces a set of styl-
ized temporal pseudo-rays that are generated from pseudo-
references. These pseudo-rays serve as a means to provide
explicit stylization supervision for fine style transfer.
3.2. Generating Temporal Pseudo-References
We focus on stylizing dynamic 3D scenes by leveraging
stylized reference images, allowing for greater controlla-
bility. However, the challenge arises when dealing with a
limited number of stylized reference images, particularly in
the context of stylizing dynamic radiance fields, mainly be-
20104
cause there is a lack of explicit supervision for the major-
ity of timestamps. For consistent stylization over time, our
key insight involves the introduction of additional temporal
cues and the explicit propagation of style information from
the stylized reference to other timestamps.
To achieve this, we leverage a video style transfer
method to generate temporal pseudo-references. The
process begins by rendering novel times, denoted as
{Ii
FΘ,R}T
i=1that share an identical camera pose pRwith
the stylized reference image Sk
Rusing the pre-trained dy-
namic radiance field FΘ. To train, we randomly sample
small patches from Ik
FΘ,R, pass them through a U-net-based
image-to-image translation framework [9], and generate
their stylized versions. Subsequently, we calculate the loss
by comparing these stylized patches with the corresponding
patches extracted from the stylized reference. This error is
then backpropagated through the network to optimize the
model parameters. Once the model is trained, we can feed
those novel times {Ii
FΘ,R}T
i=1into the network and synthe-
size temporal pseudo-references {ˆSi
R}T
i=1. We end up re-
placing ˆSk
Rin{ˆSi
R}T
i=1withSk
R. These pseudo-references
serve as effective sources of supervision, ensuring consis-
tent stylization across the temporal domain, thereby aiding
in the optimization of the dynamic radiance field.
3.3. Spatio-Temporal Style Transfer
We now have temporal pseudo-references besides the pro-
vided stylized reference. Our next step involves performing
spatio-temporal style transfer that leverages these tempo-
ral pseudo-references to propagate style information from
the stylized reference across both temporal and spatial di-
mensions. This spatio-temporal style transfer encompasses
coarse and fine style transfer.
Coarse style transfer. We first transfer style informa-
tion over time and space by ensuring that the seman-
tic correspondence across the entire dynamic scene re-
mains consistent throughout the stylization process. To
accomplish this, we establish a coarse content-style map-
ping and enforce novel views and times to mimic the
style details contained in pseudo-references at the fea-
ture level. Given the inherent redundancy of style infor-
mation in temporal pseudo-references, we select only N
pseudo-references from {ˆSi
R}T
i=1for coarse style transfer,
denoted as {ˆSj
R}N
j=1. We first render a content domain
image It
FΘfrom the pre-trained dynamic radiance field
FΘ, given a time tand camera pose p. Subsequently, we
inputIt
FΘ,{ˆSj
R}N
j=1and their corresponding content do-
main images {Ij
FΘ,R}N
j=1into a pre-trained feature extrac-
tor, yielding high-level semantic features FIt
FΘ,{FˆSj
R}N
j=1
and{FIj
FΘ,R}N
j=1. To transfer style information both tem-
porally and spatially, we construct a coarse style guidance
feature map FGtfrom{FˆSj
R}N
j=1. Specifically, we computethe feature vector at (m, n)ofFGtas follows:
F(m,n)
Gt=⊕
{FˆSj
R}N
j=1(p∗,q∗)
, (1)
(p∗, q∗) = arg min
p,qd
F(m,n)
It
FΘ,⊕
{FIj
FΘ,R}N
j=1(p,q)
,
(2)
where the concatenation operation ⊕is performed along
the width dimension, dis the cosine distance. The result-
ing style guidance feature map c contains spatio-temporal
style information, which can be employed to optimize the
dynamic radiance field, propagating style information from
the stylized reference across both temporal and spatial di-
mensions. To achieve this, we minimize the cosine distance
dbetween FGtand the extracted feature map FSt
GΘof ren-
dered view St
GΘfrom our stylized dynamic radiance field
GΘ, given a time tand camera pose p. Following prior
works [51, 54], we also incorporate a feature-level content-
preserving loss to ensure that the content remains easily rec-
ognizable. The feature-level stylization loss is defined as:
Lfeat=X
i,j
d
FGt,FSt
GΘ
+λFIt
FΘ− FSt
GΘ2
2
.
(3)
We also add a coarse color-level stylization loss to fur-
ther encourage color-matching. Specifically, we first down-
sample each of {ˆSj
R}N
j=1to the size of FGtand concatenate
them together, denoted as ⊕{ˆSj
R,D}N
j=1, and use the afore-
mentioned content-style mapping to obtain a coarse style
guidance color map St
G,D. We also downsample St
GΘ, de-
noted as St
GΘ,D. The coarse color-level stylization loss is
then computed as follows:
Lcolor=X
i,jSt
G,D− St
GΘ,D2
2. (4)
Bringing everything together, our coarse style transfer loss
can be expressed as follows:
Lcoarse=λfeatLfeat+λcolorLcolor. (5)
As suggested in ARF [51], rendering a full-resolution im-
age at each optimization iteration is impractical due to its
memory inefficiency. To tackle this issue, we adopt the de-
ferred back-propagation technique employed in ARF [51]
to optimize the coarse style transfer loss Lcoarse.
Fine style transfer. While coarse style transfer al-
ready yields a fairly good stylization result, it lacks high-
frequency details. To retain high-frequency details, we draw
inspiration from Ref-NPR [54] and introduce the Temporal
Reference Ray Registration process. Similar to Ref-NPR,
we utilize pseudo-depth rendered from the pre-trained dy-
namic radiance field to obtain reference-related explicit su-
pervision. However, since we deal with dynamic 3D scenes,
20105
we need to account for the temporal dimension as well. This
can be achieved by introducing temporal pseudo-references.
Specifically, given time tand reference camera pose pR,
we first lift the pseudo-reference ˆSt
Rinto 3D space using
corresponding depth values rendered from the pre-trained
dynamic radiance field, and maps these 3D positions to cor-
responding voxels. Formally, we define the temporal refer-
ence dictionary Das
D(x, y, z, t ) ={ri∈ R| Q(x(ri, t)) = ( x, y, z )},(6)
where Ris the set of rays originating from the reference
camera pose pR,x(ri, t)denotes the intersection point of
the ray riwith the surface at time t, and Qis a quantiza-
tion operator responsible for mapping 3D positions x(ri, t)
to their corresponding voxels. Given time t, the notation
D(x, y, z, t )represents the collection of rays that drop into
the voxel located at (x, y, z )and the color of D(x, y, z, t )
is given by pseudo-references ˆSt
R. Following this, our tem-
poral reference ray registration process is to find those rays
rj∈ T emitted from training views a pseudo-ray ˆrj∈ R.
rjshould have an intersection point x(rj, t)falling into its
nearest voxel D(x, y, z, t )and share a similar direction drj
with that of ricontained in D(x, y, z, t ). These pseudo-
rays are then assigned with stylized colors ˆCR(ˆrj, t)from
D(x, y, z, t ). The resulting set of temporal pseudo-rays and
their corresponding stylized colors are defined as:
ΦR(t) ={(rj,ˆCR(ˆrj, t))|rj∈ T ∪ R ,ˆrj̸=∅},(7)
ˆrj= arg min
ri∈D(x,y,z,t )x(ri, t)−x(rj, t)
2, (8)
s.t.∠(dri,drj)< θ, Q (x(rj, t)) = ( x, y, z ).(9)
To maintain details, we minimize the mean squared error
between the rendered color ˆCGΘ(ri, t)of our model and the
color of the corresponding temporal pseudo-ray ˆCR(ˆri, t),
which is given by
Lfine=X
ri∈ϕR(t)ˆCGΘ(ri, t)−ˆCR(ˆri, t)2
2, (10)
where ϕR(t)represents the set of those temporal pseudo-
rays at time t.
3.4. Optimization
This section describes our training scheme. We adopt a
hierarch stylization strategy that gradually propagates the
style information from the stylized reference to the entire
dynamic 3D scene. Specifically, we employ hierarchical
stylization on keyframes and subsequently apply it to the
entire sequence.
For a 4D point represented as (x, y, z, t ), we calculate
its volume density and appearance features using a dynamic
radiance field. Subsequently, we predict the final color byfeeding the appearance feature as inputs to an MLP. We use
standard volume rendering to accumulate colors and densi-
ties into 2D images. To stylize dynamic 3D scenes, our total
optimization objective is defined as:
L=λcLcoarse+λfLfine+λTVLTV, (11)
where LTVis the Total Variational (TV) loss that serves to
enforce the spatio-temporal continuity.
4. Experiments
4.1. Implementation Details
Our dynamic radiance field is based on HexPlane [1], an
explicit representation designed for modeling dynamic 3D
scenes. The choice of HexPlane stems from its simplicity
and effectiveness in representing 4D volumes. Following
prior works [51, 54], in the stylization process, we keep the
density component of the dynamic radiance field frozen and
exclusively optimize the appearance component. To avoid
view-dependent stylization, we set and fix the view direc-
tions to zero when passing them into the MLP and proceed
to finetune the model on training views for a few iterations
before stylization.
We leverage a video style transfer method [45] to gener-
ate temporal pseudo-references. For the feature extraction,
we adopt pre-trained VGG16 [42] as our feature extractor.
As in Ref-NPR [54], we set the spatial resolution of the tem-
poral reference dictionary Dto2563, where each voxel can
at most store 8rays. We train our model using the Adam
optimizer [22] and carry out all experiments on a single
NVIDIA GeForce RTX 4090 GPU. For hyperparameters,
we set λ= 5×10−3,λTV= 5×10−2,λfeat=λc= 1,
λcolor= 5, andλf= 10 .
4.2. Baselines
To the best of our knowledge, we are the first to tackle
the task of stylizing neural radiance fields for dynamic 3D
scenes with a stylized 2D view as a reference. We compare
our method with three representative stylization methods.
One is ARF [51], an arbitrary style transfer method for static
3D scenes. To ensure a fair comparison, we reimplement
ARF and make specific adjustments to cater to dynamic
scenes. This modified version is denoted as ARF*. Ref-
NPR [54] is a reference-based 3D stylization method with
the ability to produce stylized novel views while preserving
both geometric and semantic consistency given a stylized
reference. Similarly, since it was initially designed for static
3D scenes, we also reimplement it and adapt it to dynamic
scenes, which we denote as Ref-NPR*. Finally, we also
include Texler et al. [45], a reference-based video styliza-
tion method that is capable of extending the stylized content
from a stylized reference to the entire video sequence.
20106
Ours Texler etal.ARF* Ref-NPR *
Ours Texler etal.ARF* Ref-NPR *Figure 3. Qualitative comparisons on real-world and synthetic datasets. We compare our method with ARF* [51], Ref-NPR* [54],
and Texler et al. [45]. In each case, the upper left image represents the reference view, generated from the photorealistic dynamic radiance
field, while the lower left image depicts its corresponding stylized version.
4.3. Results
Experimental setup. We assess the performance on two
distinct datasets: the Plenoptic Video dataset [25] and the
D-NeRF dataset [38]. To assess both the quality of styliza-
tion and the consistency between different novel views and
novel timesteps of stylized dynamic 3D scenes, we employ
two distinct settings. In one, we maintain a consistent time
frame while generating novel views, and in the other, we
maintain a consistent camera view while generating novel
timesteps. For the evaluation of reference-based styliza-
tion quality, we follow Wu et al. [49] and adopt Ref-LPIPS.
This metric quantifies the perceptual similarity between the
style reference image and novel views or timesteps using
LPIPS [53]. To test the performance on cross-view consis-
tency and temporal stability, we also adopt short-range con-
sistency and long-range consistency used by Lai et al. [24]
and SNeRF [36]. Specifically, we warp a stylized viewto obtain a new view using the optical flow estimated by
RAFT [44]. We then compute the error between the warped
view and the novel view or timestep rendered from the styl-
ized dynamic scene. To quantify short-range consistency,
we compute the error between the ithand(i+ 1)thvideo
frames, while for long-range consistency, we compute the
error between the ithand(i+ 7)thframes.
Quantitative comparisons. We show the quantitative re-
sults for the Plenoptic Video dataset and the D-NeRF
dataset in Table 1 and Table 2, respectively. The results
indicate that our method achieves higher perceptual similar-
ity in both real-world and synthetic scenarios. In addition,
ARF* [51], Ref-NPR* [54], and our method demonstrate
similar cross-view geometric consistency and temporal sta-
bility while Texler et al. [45] lags behind.
Qualitative comparisons. We present the visual compar-
isons in Fig. 3. Given that ARF* [51] is an arbitrary style
20107
Table 1. Quantitative comparisons on the Plenoptic Video dataset. The best performance is in bold , and the second best is underlined .
MethodFixing camera viewpoint Fixing time
Ref-LPIPS ↓Short-range Long-rangeRef-LPIPS ↓Short-range Long-range
consistency ↓ consistency ↓ consistency ↓ consistency ↓
ARF* [51] 0.513 0.006 0.007 0.657 0.015 0.072
Ref-NPR* [54] 0.374 0.012 0.013 0.602 0.020 0.097
Texler et al. [45] 0.353 0.015 0.016 0.585 0.027 0.132
Ours 0.298 0.012 0.013 0.561 0.018 0.084
Table 2. Quantitative comparisons on the D-NeRF dataset. The best performance is in bold , and the second best is underlined .
MethodFixing camera viewpoint Fixing time
Ref-LPIPS ↓Short-range Long-rangeRef-LPIPS ↓Short-range Long-range
consistency ↓ consistency ↓ consistency ↓ consistency ↓
ARF* [51] 0.656 0.027 0.052 0.676 0.032 0.222
Ref-NPR* [54] 0.645 0.028 0.064 0.669 0.034 0.216
Texler et al. [45] 0.593 0.035 0.072 0.621 0.045 0.243
Ours 0.605 0.025 0.051 0.639 0.030 0.211
(a) Multi-view references
(b) Multi-time references
Single
Single
Multiple
Multiple
Figure 4. Multi-references. Our method is versatile and can also
accept multiple references. We show that including additional ref-
erences in spatial or temporal dimensions enriches the details and
enhances the overall quality of the results.
transfer approach, it introduces color mismatches between
the reference image and the rendered view, leading to a per-
ceptual gap, especially in comparison to reference-based
stylization methods. While Ref-NPR* [54] can generate
stylized novel views that preserve both geometric and se-
mantic consistency with the given reference, it falls short in
rendering novel times that achieve comparable visual qual-
ity to our method. The limitation arises from the fact that
Ref-NPR does not account for the temporal propagation
of style information from the stylized reference. Texler et
al. [45] can generate high-quality stylized video sequences
that retain structural details from the reference, yet it is
plagued by flickering issues. In contrast, our method can
output visually pleasing stylized novel views and times
while maintaining semantic consistency with the provided
reference across both spatial and temporal dimensions.
Controllable stylization. In Fig. 5, we showcase the effi-cacy of our method in terms of controllability. Thanks to
the reference-based nature, our method excels at producing
diverse dynamic 3D scenes, aligning with the provided styl-
ized reference. Besides neural style transfer methods such
as NNST [23], we can also employ ControlNet [52] to pro-
duce or edit the reference image, and subsequently apply
this modified reference to shape our dynamic 3D scenes.
Furthermore, our method allows for localized edits to the
reference image, enabling us to finetune or modify spe-
cific aspects of the scenes. This localized control empowers
users to make precise adjustments to the visual elements,
enhancing the overall customization and creative possibili-
ties of our method.
Multi-references. In Sec. 3, our primary focus is on scenar-
ios involving only a single reference image as input, simpli-
fying the discussion for clarity. Nevertheless, it is worth
noting that our method is versatile and can also accommo-
date multiple references. As illustrated in Fig. 4, we demon-
strate how the inclusion of additional references in spatial or
temporal dimensions enriches the details and enhances the
overall quality of the results.
4.4. User Study
To further evaluate the performance of our method from
a human perspective, we also conduct a user study com-
paring it with baseline methods, namely ARF* [51], Ref-
NPR* [54], and Texler et al. [45]. Each participant is pre-
sented with a reference image, its stylized version, and two
stylized videos: one generated by our method and the other
from a randomly selected approach, with the order random-
ized. We invite a total of 56volunteers to select the method
that exhibits superior perceptual quality, or none if it is hard
to judge. The results are detailed in Table 3. These results
clearly indicate a strong preference for our method.
20108
(b)(a)
Figure 5. Controllable stylization. Our method inherently facilitates controllable stylization. (a) Besides neural style transfer methods
such as NNST [23], we can leverage ControlNet [52] to generate or edit the reference image, and subsequently apply this modified reference
to shape our dynamic 3D scenes. (b) Furthermore, our method enables localized edits to the reference image, making it possible to finetune
or alter specific aspects of the scenes.
(b) w/o fine style transfer(a) w/o coarse style transfer
Full model
Reference
Figure 6. Ablation study on each component of our method.
Table 3. User study. Pairwise comparison results indicate a strong
preference for our method.
Comparison Human preference
ARF* [51] / Ours 12.0% / 88.0%
Ref-NPR* [54] / Ours 22.6% / 77.4%
Texler et al. [45] / Ours 30.0% / 70.0%
4.5. Ablation Study
To validate the effectiveness of each component, we con-
duct an ablation study on the Plenoptic Video dataset [25].
One can observe in Fig. 6 (a) that the removal of the coarse
style transfer module results in a noticeable decline in the
quality of the stylized novel views and times. This de-
cline occurs because the absence of the coarse style transfer
module can render the training process vulnerable to get-
ting stuck in local optima and impede its ability to providemeaningful stylization to occluded regions. In addition, as
illustrated in Fig. 6 (b), when the fine style transfer module
is removed, the generated results tend to lack the preserva-
tion of style details. Together, these findings emphasize the
contributions of both the coarse and fine style transfer mod-
ules to the overall quality and visual appeal of the results.
5. Conclusion
In this paper, we have introduced a novel task of styliz-
ing dynamic 3D scenes with one or a few stylized refer-
ence images as input. To this end, we present S-DyRF, a
reference-based spatio-temporal stylization method for dy-
namic neural radiance fields. Our method effectively trans-
fers the style information from the reference to the entire
dynamic 3D scene, resulting in visually pleasing stylized
novel views and times that maintain semantic consistency
with the provided reference image across both spatial and
temporal dimensions. To validate the effectiveness and su-
periority of our method, we conduct extensive experiments
on both synthetic and real-world datasets. Moreover, thanks
to the reference-based nature, our method empowers users
to produce various stylized dynamic 3D scenes according to
their preferences. We hope that S-DyRF can open up new
avenues for 3D stylization and inspire further research in
the domain of stylizing dynamic 3D scenes.
Acknowledgements. This study is supported under the
RIE2020 Industry Alignment Fund – Industry Collabora-
tion Projects (IAF-ICP) Funding Initiative, as well as cash
and in-kind contribution from the industry partner(s). This
work is also supported by the MOE AcRF Tier 2 grant
(MOE-T2EP20220-0007).
20109
References
[1] Ang Cao and Justin Johnson. Hexplane: A fast representa-
tion for dynamic scenes. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 130–141, 2023. 1, 3, 5
[2] Xu Cao, Weimin Wang, Katashi Nagao, and Ryosuke Naka-
mura. Psnet: A style transfer network for point cloud
stylization on geometry and color. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
vision (WACV) , pages 3337–3345, 2020. 1, 2
[3] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance fields. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
pages 333–350, 2022. 1
[4] Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang
Hua. Coherent online video style transfer. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 1105–1114, 2017. 2
[5] Yaosen Chen, Qi Yuan, Zhiqiang Li, Yuegen Liu, Wei Wang,
Chaoping Xie, Xuming Wen, and Qien Yu. Upst-nerf: Uni-
versal photorealistic style transfer of neural radiance fields
for 3d scene. arXiv preprint arXiv:2208.07059 , 2022. 1, 2
[6] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-
Sheng Lai, and Wei-Chen Chiu. Stylizing 3d scene via im-
plicit representation and hypernetwork. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV) , pages 1475–1484, 2022.
[7] Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia
Xu, and Zhangyang Wang. Unified implicit neural styliza-
tion. In Proceedings of the European Conference on Com-
puter Vision (ECCV) , pages 636–654, 2022. 1, 2
[8] Jakub Fi ˇser, Ond ˇrej Jamri ˇska, Michal Luk ´aˇc, Eli Shecht-
man, Paul Asente, Jingwan Lu, and Daniel S `ykora. Stylit:
illumination-guided example-based stylization of 3d render-
ings. ACM Transactions on Graphics (TOG) , 35(4):1–11,
2016. 2
[9] D Futschik, M Chai, C Cao, C Ma, A Stoliar, S Korolev,
S Tulyakov, M Ku ˇcera, and D S `ykora. Real-time patch-
based stylization of portraits using generative adversarial
network. In Proceedings of the 8th ACM/Eurographics Ex-
pressive Symposium on Computational Aesthetics and Sketch
Based Interfaces and Modeling and Non-Photorealistic Ani-
mation and Rendering , pages 33–42, 2019. 4
[10] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.
Image style transfer using convolutional neural networks.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 2414–2423,
2016. 1, 2
[11] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander
Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Edit-
ing 3d scenes with instructions. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , 2023. 1, 2
[12] Mingming He, Jing Liao, Dongdong Chen, Lu Yuan, and Pe-
dro V Sander. Progressive color transfer with dense semantic
correspondences. ACM Transactions on Graphics (TOG) , 38
(2):1–18, 2019. 2[13] Aaron Hertzmann, Charles E Jacobs, Nuria Oliver, Brian
Curless, and David H Salesin. Image analogies. In Proceed-
ings of the 28th annual conference on Computer graphics
and interactive techniques , pages 327–340, 2001. 2
[14] Lukas H ¨ollein, Justin Johnson, and Matthias Nießner.
Stylemesh: Style transfer for indoor 3d scene reconstruc-
tions. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 6198–
6208, 2022. 1, 2
[15] Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wen-
hao Jiang, Xiaolong Zhu, Zhifeng Li, and Wei Liu. Real-
time neural style transfer for videos. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 783–791, 2017. 2
[16] Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh
Singh, and Ming-Hsuan Yang. Learning to stylize novel
views. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV) , pages 13869–13878,
2021. 1, 2
[17] Xun Huang and Serge Belongie. Arbitrary style transfer in
real-time with adaptive instance normalization. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV) , pages 1501–1510, 2017. 1, 2
[18] Yi-Hua Huang, Yue He, Yu-Jie Yuan, Yu-Kun Lai, and Lin
Gao. Stylizednerf: consistent 3d scene stylization as styl-
ized nerf via 2d-3d mutual learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 18342–18352, 2022. 1, 2
[19] Ond ˇrej Jamri ˇska, ˇS´arka Sochorov ´a, Ond ˇrej Texler, Michal
Luk´aˇc, Jakub Fi ˇser, Jingwan Lu, Eli Shechtman, and Daniel
S`ykora. Stylizing video by example. ACM Transactions on
Graphics (TOG) , 38(4):1–11, 2019. 2
[20] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. In
Proceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 694–711, 2016. 1, 2
[21] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neu-
ral 3d mesh renderer. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 3907–3916, 2018. 1, 2
[22] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[23] Nicholas Kolkin, Michal Kucera, Sylvain Paris, Daniel
Sykora, Eli Shechtman, and Greg Shakhnarovich. Neural
neighbor style transfer. arXiv preprint arXiv:2203.13215 ,
2022. 1, 2, 3, 7, 8
[24] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman,
Ersin Yumer, and Ming-Hsuan Yang. Learning blind video
temporal consistency. In Proceedings of the European Con-
ference on Computer Vision (ECCV) , pages 170–185, 2018.
6
[25] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon
Green, Christoph Lassner, Changil Kim, Tanner Schmidt,
Steven Lovegrove, Michael Goesele, Richard Newcombe,
et al. Neural 3d video synthesis from multi-view video.
InProceedings of the IEEE/CVF Conference on Computer
20110
Vision and Pattern Recognition (CVPR) , pages 5521–5531,
2022. 6, 8
[26] Xueting Li, Sifei Liu, Jan Kautz, and Ming-Hsuan Yang.
Learning linear transformations for fast image and video
style transfer. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
3809–3817, 2019. 1, 2
[27] Xingyi Li, Chaoyi Hong, Yiran Wang, Zhiguo Cao, Ke Xian,
and Guosheng Lin. Symmnerf: Learning to explore sym-
metry prior for single-view view synthesis. In Proceedings
of the Asian Conference on Computer Vision (ACCV) , pages
1726–1742, 2022. 1
[28] Xingyi Li, Zhiguo Cao, Huiqiang Sun, Jianming Zhang, Ke
Xian, and Guosheng Lin. 3d cinemagraphy from a single im-
age. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 4595–
4605, 2023. 1
[29] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu,
and Ming-Hsuan Yang. Universal style transfer via feature
transforms. Advances in Neural Information Processing Sys-
tems (NeurIPS) , 30, 2017. 1, 2
[30] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.
Neural scene flow fields for space-time view synthesis of dy-
namic scenes. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
6498–6508, 2021. 1
[31] Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, and Sing Bing
Kang. Visual attribute transfer through deep image analogy.
ACM Transactions on Graphics (TOG) , 36(4):1–15, 2017. 2
[32] Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang,
Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, and
Eric P Xing. Stylerf: Zero-shot 3d style transfer of neural
radiance fields. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
8338–8348, 2023. 1, 2
[33] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In Proceedings of the European Conference on Com-
puter Vision (ECCV) , 2020. 1, 2
[34] Fangzhou Mu, Jian Wang, Yicheng Wu, and Yin Li. 3d photo
stylization: Learning to generate stylized novel views from a
single image. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
16273–16282, 2022. 1, 2
[35] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(TOG) , 41(4):1–15, 2022. 1
[36] Thu Nguyen-Phuoc, Feng Liu, and Lei Xiao. Snerf: Stylized
neural implicit representations for 3d scenes. ACM Transac-
tions on Graphics (TOG) , 41(4):1–11, 2022. 1, 2, 6
[37] Hong-Wing Pang, Binh-Son Hua, and Sai-Kit Yeung. Lo-
cally stylized neural radiance fields. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 307–316, 2023. 2
[38] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
Francesc Moreno-Noguer. D-nerf: Neural radiance fields fordynamic scenes. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 10318–10327, 2021. 6
[39] Mattia Segu, Margarita Grinvald, Roland Siegwart, and Fed-
erico Tombari. 3dsnet: Unsupervised shape-to-shape 3d
style transfer. arXiv preprint arXiv:2011.13388 , 2020. 1,
2
[40] Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng,
Boyao Zhou, Hongwen Zhang, and Yebin Liu. Con-
trol4d: Dynamic portrait editing by learning 4d gan from
2d diffusion-based editor. arXiv preprint arXiv:2305.20082 ,
2023. 1, 2
[41] Liao Shen, Xingyi Li, Huiqiang Sun, Juewen Peng, Ke Xian,
Zhiguo Cao, and Guosheng Lin. Make-it-4d: Synthesizing
a consistent long-term dynamic scene video from a single
image. In Proceedings of the 31st ACM International Con-
ference on Multimedia (ACM MM) , pages 8167–8175, 2023.
1
[42] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 5
[43] Jan Svoboda, Asha Anoosheh, Christian Osendorfer, and
Jonathan Masci. Two-stage peer-regularized feature recom-
bination for arbitrary image style transfer. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 13816–13825, 2020. 1, 2
[44] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow. In Proceedings of the European
Conference on Computer Vision (ECCV) , pages 402–419,
2020. 6
[45] Ond ˇrej Texler, David Futschik, Michal Ku ˇcera, Ond ˇrej
Jamri ˇska, ˇS´arka Sochorov ´a, Menclei Chai, Sergey Tulyakov,
and Daniel S `ykora. Interactive video stylization using few-
shot patch-based training. ACM Transactions on Graphics
(TOG) , 39(4):73–1, 2020. 2, 5, 6, 7, 8
[46] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,
and Jing Liao. Clip-nerf: Text-and-image driven manip-
ulation of neural radiance fields. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 3835–3844, 2022. 1, 2
[47] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He,
Dongdong Chen, and Jing Liao. Nerf-art: Text-driven neural
radiance fields stylization. IEEE Transactions on Visualiza-
tion and Computer Graphics , 2023. 1, 2
[48] Zijin Wu, Xingyi Li, Juewen Peng, Hao Lu, Zhiguo Cao, and
Weicai Zhong. Dof-nerf: Depth-of-field meets neural radi-
ance fields. In Proceedings of the 30th ACM International
Conference on Multimedia (ACM MM) , pages 1718–1729,
2022. 2
[49] Zijie Wu, Zhen Zhu, Junping Du, and Xiang Bai. Ccpl: con-
trastive coherence preserving loss for versatile style transfer.
InProceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 189–206, 2022. 6
[50] Kangxue Yin, Jun Gao, Maria Shugrina, Sameh Khamis, and
Sanja Fidler. 3dstylenet: Creating 3d shapes with geometric
and texture style variations. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
12456–12465, 2021. 1, 2
20111
[51] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu,
Eli Shechtman, and Noah Snavely. Arf: Artistic radiance
fields. In Proceedings of the European Conference on Com-
puter Vision (ECCV) , pages 717–733, 2022. 2, 4, 5, 6, 7,
8
[52] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 3836–3847, 2023. 3, 7, 8
[53] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 586–595, 2018. 6
[54] Yuechen Zhang, Zexin He, Jinbo Xing, Xufeng Yao, and Ji-
aya Jia. Ref-npr: Reference-based non-photorealistic radi-
ance fields for controllable scene stylization. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 4242–4251, 2023. 1, 2, 3,
4, 5, 6, 7, 8
20112
