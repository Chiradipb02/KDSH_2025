LAKE-RED: Camouflaged Images Generation by Latent Background
Knowledge Retrieval-Augmented Diffusion
Pancheng Zhao1,2Peng Xu3* Pengda Qin4Deng-Ping Fan2,1
Zhicheng Zhang1,2Guoli Jia1Bowen Zhou3Jufeng Yang1,2
1VCIP & TMCC & DISSec, College of Computer Science, Nankai University
2Nankai International Advanced Research Institute (SHENZHEN· FUTIAN)
3Department of Electronic Engineering, Tsinghua University4Alibaba Group
pc.zhao99@gmail.com, peng xu@tsinghua.edu.cn, pengda.qpd@alibaba-inc.com, dengpfan@gmail.com
gloryzzc6@sina.com, exped1230@gmail.com, zhoubowen@tsinghua.edu.cn, yangjufeng@nankai.edu.cn
(a) Camouflaged images generated by our method.
camouflagedimageobject extractionvariantgenerationgeneralimagecamouflagedimageobject extraction
(b) Generating variants for existing camouflaged images. (c) Transferring images from general to camouflaged.
Figure 1. LAKE-RED synthesizes realistic camouflaged images for a given foreground object by a knowledge retrieval-augmented
diffusion model. Without any human-specified background; the model automatically generates a background sufficient to conceal the
foreground objects. (b) and (c) shows the image generation process of our method in two application scenarios.
Abstract
Camouflaged vision perception is an important vision
task with numerous practical applications. Due to the ex-
pensive collection and labeling costs, this community strug-
gles with a major bottleneck that the species category of
its datasets is limited to a small number of object species.
However, the existing camouflaged generation methods re-
quire specifying the background manually, thus failing to
extend the camouflaged sample diversity in a low-cost man-
ner. In this paper, we propose a Latent Background Knowl-
edge Retrieval-Augmented Diffusion (LAKE-RED) for cam-
ouflaged image generation. To our knowledge, our contri-
*Corresponding Author.butions mainly include: (1) For the first time, we propose a
camouflaged generation paradigm that does not need to re-
ceive any background inputs. (2) Our LAKE-RED is the first
knowledge retrieval-augmented method with interpretabil-
ity for camouflaged generation, in which we propose an
idea that knowledge retrieval and reasoning enhancement
are separated explicitly, to alleviate the task-specific chal-
lenges. Moreover, our method is not restricted to specific
foreground targets or backgrounds, offering a potential for
extending camouflaged vision perception to more diverse
domains. (3) Experimental results demonstrate that our
method outperforms the existing approaches, generating
more realistic camouflage images. Our source code is re-
leased on https://github.com/PanchengZhao/LAKE-RED.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
4092
1. Introduction
•Background. Camouflaged vision perception [14]
is a challenging problem ( e.g., camouflaged object detec-
tion [13]) aiming to perceive the concealed complex pat-
terns and extensively applied in various fields such as pest
detection [11], healthcare [19, 22, 51], and autonomous
driving [3, 20, 28, 32, 44]. It has made significant progress
in recent years. However, these kinds of overly complex vi-
sual scenes and patterns make it extremely time-consuming
and labor-intensive to annotate the pixel-wise masks. There
is a fact that an instance-level annotation in the COD10K
dataset took an average of 60 minutes [12], far longer than
the 3 minutes in the COCO-Stuff dataset [5], clearly illus-
trating this issue. Thus, this community struggles with a
major bottleneck in that the species category of its datasets
is limited to a small number of object species, e.g., animals.
•Existing Technical Limitations. Recently, the rapid
development in the AIGC community, particularly genera-
tive models based on GAN [8] and Diffusion [18], has re-
vealed the potential of using synthetic data to address data
scarcity. DatasetGAN [55] and BigDatasetGAN [27] train
a shallow decoder to generate pixel-level annotations from
the feature space of pre-trained GANs. DiffuMask [49] is
inspired by the attention map in the Diffusion Model and
obtains pixel-level annotations from the cross-attention pro-
cess of the text and image. However, the above method is
designed for generic scenarios, and the generated data has a
significant domain gap with the training data for the camou-
flage vision perception task. Moreover, as shown in Fig. 2,
the existing camouflaged generation methods require spec-
ifying the background manually, thus failing to extend the
camouflaged sample diversity in a low-cost manner.
•Motivation. Our idea is to make full use of the
domain-specific traits of camouflaged scenes to implement
a low-cost solution. As shown in Fig. 2, the level of tar-
get camouflage depends largely on its surrounding environ-
mental context. Furthermore, we observed that a majority of
camouflaged images utilize a background-matching percep-
tual deception strategy, where the concealed object blends
seamlessly into the surrounding background. In this sce-
nario, the foreground and background regions of the cam-
ouflaged image exhibit remarkable visual perceptual consis-
tency. For instance, the frog concealed in the grass surface
displays a mottled pattern of green and brown just like the
grass and ground. This feature convergence between fore-
ground and background makes it possible to retrieve and
reason about background through foreground features.
•Method Overview. Inspired by the above motiva-
tion, we introduce LAKE-RED, a pipeline that automati-
cally generates high-quality camouflage images and pixel-
level segmentation masks. The model accepts a foreground
object input to achieve object-to-background image inpaint-
ing. Specifically, the model first perceives features from
Camoflaged SalientInput×
Background Inpainting Stream
Inpainting 
Module
Style transfer 
Module
Style-transfer Stream
Manual Selected BackgroundFigure 2. Comparison of Frameworks for Camouflage Image
Generation. Existing methods rely on manually specified back-
grounds, which not only receive limitations in diversity and scope
from the human’s own cognition but also result in expensive im-
age generation on a large scale. Without changing the texture of
itself, the same target can be camouflaged to different degrees in
different environments. Inspired by it, we synthesize camouflaged
images through a background inpainting stream, hiding by auto-
matically choosing a suitable background for the object.
the foreground and utilizes them as queries to retrieve la-
tent background knowledge from a pre-constructed exter-
nal knowledge base. Then, the model learns to reason
from foreground objects to background scenes via using
the retrieved knowledge to conduct the camouflaged back-
ground reconstruction. This helps the model achieve a
richer condition-guided background generation. Simultane-
ously, this synthesis preserves the precise foreground an-
notation and prevents boundary blurring caused by mask
generation. Fig. 1 illustrates pairs of camouflaged images
generated by our LAKE-RED, along with examples of two
application scenarios. Without the need for manually spec-
ified background inputs, the proposed model can efficiently
produce high-quality camouflaged images at a low cost.
•Contribution. (1) For the first time, we propose a
camouflaged generation paradigm without any background
inputs. (2) Our LAKE-RED is the first knowledge retrieval-
augmented method with interpretability for camouflaged
generation, in which we propose an idea that knowledge
retrieval and reasoning enhancement are separated explic-
itly, to alleviate the task-specific challenges. Moreover, our
method is not restricted to specific foregrounds or back-
grounds, offering a potential for extending camouflaged vi-
sion perception to more diverse domains. (3) Experimen-
tal results demonstrate our method outperforms the existing
approaches, generating more realistic camouflage images.
4093
2. Related Work
•Synthetic Dataset Generation. Synthetic data
has gained significant attention as one of the primary ap-
proaches to tackle data bottlenecks in deep learning meth-
ods due to its low cost [24, 40]. Previous research on
synthetic datasets has mainly focused on producing high-
quality simulated scenes in 3D environments and generat-
ing data from them, which has been extensively employed
for tasks such as recognition [23, 46, 47, 62], segmenta-
tion [6, 34, 48, 57], object tracking [33, 58], image and
video understanding [52, 56, 59–61, 63], optical flow esti-
mation [4, 35], and 3D reconstruction [66–68]. The consid-
erable disparity between the distribution of synthetic data
through simulated scenarios and real data restricts their va-
lidity. Significant progress in generative modeling has re-
cently enabled the reduction of the domain gap between
synthetic and real data. With realistic image data generated
by advanced generative models ( e.g., GAN, DALL-E2, and
Stable Diffusion), some research has attempted to investi-
gate the potential of synthetic data as a replacement for real
data [15, 16, 30]. Specifically, DatasetGAN [55] and Big-
DatasetGAN [27] excel in generating a significant quantity
of synthetic images with segmentation masks with limited
labeled data. On the other hand, Diffumask [49] relies ex-
clusively on textual supervision to extract semantic labels
from the cross-attention maps of text and images.
•Camouflage Image Generation. Camouflage im-
ages are different from regular images as they contain one
or more concealed objects [12]. Although the concept of
camouflage can be traced back to Darwin’s theory of evolu-
tion [9, 39, 42] and has long been used in various fields, the
task of camouflage image generation was not proposed until
2010 by Chu et al. [7]. The proposed model gets a specified
foreground and background as input and uses hand-crafted
features to give the foreground textural details similar to the
background, making the concealed objects difficult for hu-
mans to recognize. Recent advancements in deep learning
methods for style transfer and image composing have pro-
vided new ideas for generating camouflage images. Sub-
sequent models, such as Zhang’s [53] and Li’s [29], have
further improved camouflage image generation by compos-
ing the foreground with the background through style trans-
fer and structure alignment. However, the use of artificially
specified backgrounds increases the cost of data acquisition
and limits the diversity of generated images due to human
cognitive limitations. These limitations make it impossible
to generate large-scale datasets, greatly reducing the appli-
cation value of the generated images.
3. Methodology
Our objective is to generate camouflaged images by auto-
matically complementing the background region for a spe-cific foreground object, resulting in a realistic image where
the object is concealed in the generated background. While
there have been advancements in camouflage image gener-
ation methods, manually specifying the background is not
practical due to the high human cost and limited cognitive
range. Through our observation of the camouflage phe-
nomenon, we have noticed that the background region of
a camouflaged image often shares similar image features
with the surface of the foreground object. This suggests that
a suitable camouflage background may already exist within
the foreground image itself. Formally, given a source im-
agexs∈RH×W×3, containing an object with an irregular
shape. The object’s location is precisely indicated by a bi-
nary mask mwith the same size as the original image xs,
where mi,j= 0, with i∈[0, H]andj∈[0, W], repre-
sents the object region that needs to be maintained in sub-
sequent operations, and mi,j= 1 represents the editable
background region. The model takes {xs,m}as input, and
outputs a camouflaged image xc. The objective is to obtain
a prior from the foreground xs⊙¯ mto generate a suitable
background that replaces the original one. The foreground
should harmoniously match the new background.
3.1. Preliminaries
•Revisiting Latent Diffusion Models. Aiming to gen-
erate high-quality camouflage images, our proposed method
is based on classic Latent Diffusion Models (LDM) [41].
Similar to other probabilistic models, LDM learns the prob-
ability distribution p(x)of a given image set xthrough self-
supervised training and achieves high-quality image gener-
ation by reversing a Markov forward process. Specifically,
the forward process adds a sequence noise to the original
images y0=xsto obtain a noisy image {yt|t∈[1, T]},
where yt=αty0+ (1−αt)ϵ. As αtdecreases with
time step t, more Gaussian noise ϵis introduced into y0.
The generation process can be described as a sequence of
denoising autoencoders ϵθ(yt,c, t)to predict a denoised
variant of input yt. Furthermore, in order to decrease the
computational demands of high-resolution image synthe-
sis for the model, a pre-trained autoencoder εis employed
to encode yinto a latent representation z=ε(y), where
z∈Rh×w×c. So the training objective can be defined as
the following loss function:
L=Et,ε(y),ϵ∥ϵθ(zt,c, t)−ϵ∥2
2. (1)
For the inpainting stream, the condition cincludes xs⊙
¯ mto indicate the remaining area. Once Tsteps have been
completed, the model predicts the latent representation z′
0,
of which the noise ϵhas been entirely removed.
Finally, to reconstruct a high-resolution image from the
latent representation, a VQV AE [43] based decoder Dis
utilized in the final stage. The visual information from the
code book eis embedded into the latent representation by
incorporating a quantization layer νinto the decoder, which
4094
Encoder
DecoderBKRM
U-NetLMP
Gaussiannoise𝒙!
𝒎𝒎⨀𝒙!512×512×3128×128×3𝑻−𝟏𝜖
𝑠×3×1𝒛"#$𝒄%*𝒄%𝒛&
𝒛'reverse+𝒄(
512×512×3Down-sampleCode Book
Element-wise MultiplicationReverse
𝑠×3×1𝒄(DEgcondD
MLPURCEM
EgEg
UUp-sample
BackgroundKnowledge RetrievalReasoning EnhancementCompressionDenoisingFigure 3. The pipeline of our camouflaged images generation framework LAKE-RED. Our framework mainly includes three steps:
(1) Extracting visual representations of foreground areas by Localized Masked Pooling (LMP). (2) The Background Knowledge Retrieval
Module (BKRM) is utilized to retrieve background-related features from the codebook. (3) The Reasoning-Driven Condition Enhancement
module (RCEM) allows the model to learn foreground-to-background reasoning through a background reconstruction.
can be yielded as:
y′
0=D(ν(e,z′
0)), (2)
where e∈RK×D,K, andDdenote the size of the discrete
latent space and the dimensionality of each latent embed-
ding vector, respectively.
3.2. Model Designs
Current image inpainting methods accept a conditional in-
putcthat includes known image regions and indicates ed-
itable regions, which can be defined as:
cf,cm=ε(Iknown ), downsample (m, f),
c=Concat 
cf,cm
,(3)
where Iknown =xs⊙¯ m, and mis down sampled by a
factor f= 2n, with n∈N. However, they tend to pri-
oritize preserving the structural continuity of the object in
the image and infer to fill in the missing areas. The in-
ference of the model is constrained when the non-edited
region forms a complete object that lacks structural con-
tinuity with the background. This means that the current
condition is not enough to facilitate the model in making
accurate inferences from the foreground object to the back-
ground scene. To mitigate the negative impact of this per-
formance bottleneck on the results, as shown in Fig. 3,
we focus on retrieving richer background knowledge and
develop a reasoning-based background reconstruction task
that enables the model to explicitly learn the relationship
between the foreground and background of a camouflaged
image. The reconstructed features can then be used to en-
hance existing conditions and provide the model with richer
guidance information.
3.2.1 Background Knowledge Retrieval
As mentioned before, inferring from object to background
is a significant challenge for image inpainting models.However, unlike general images, camouflage images are
primarily characterized by background matching, where the
background and the object exhibit a high degree of consis-
tency in terms of texture. This implies that it becomes fea-
sible to retrieve background knowledge using foreground
features. The training framework for reconstructing back-
grounds through masked ground truth (GT) implicitly mod-
els the relationship between the object and background,
which results in the model paying insufficient attention to
the texture consistency of the object and background. Ex-
plicitly retrieving background features aligned with the ob-
ject features is a viable option to provide richer guidance
for the denoising process. In order to obtain feature repre-
sentations about the background texture, we take inspiration
from the autoencoder and decoder used by LDM, which is
based on VQ-V AE.
VQ-V AE constructs a code book ein the embedding
space between the encoder and the decoder during the train-
ing process. The codebook can be injected with features
into the representation of the latent space by vector quan-
tized operation before the decoder to obtain a better re-
construction performance. To address the issue of missing
background features of the condition, the pre-trained code-
book is replicated and shifted to the denoising process as a
global visual embedding Eg=eT∈RD×K. The process
of obtaining background features xbusing a latent space
codebook Egcan be summarized as:
xb=Concat (h1,h2, . . . ,hH)Wf→b,
hi=aixfWV
i,
ai=softmax
h
xfWQ
ii
·
EgWK
iT
√dk
.(4)
We feed the foreground feature xfinto the Multi-Head At-
4095
tention (MHA) layer with Hheads, as the query, for retriev-
ing the related background content from codebook Eg, and
obtain the background aligned visual feature xb.
3.2.2 Localized Masked Pooling
We introduce a simple and efficient latent background
knowledge retrieval module, denoted as B 
xf,Eg
, that
retrieves background-aligned visual features xbfrom code-
bookEgusing foreground features xf. The richness of the
feature representation xfextracted from cfdirectly impacts
the validity of features that can be retrieved from the code-
book. Thus, the foreground feature representation xfcan
become another potential performance bottleneck. To ex-
clude features in the background region during feature ex-
traction, a straightforward approach is to follow [54] using
Masked Averaged Pooling (MAP), to obtain representative
vectors of foreground features as:
xf
i= Φ
cf
i,cm
=Pw,h
x=1,y=1cf
i,x,y∗¯cm
x,yPw,h
x=1,y=1¯cm
x,y,(5)
where i∈ {1,2, . . . , ϑ }indicates the channel number. The
MAP treats the foreground as a whole and compresses it
into a unified representation, which can lead to a significant
loss of information. In particular, the encoder ε(·)main-
tains the channel number of the feature to be 3, resulting
inxf∈R3×1. This simple representation is insufficient to
capture the rich features of the foreground and can limit the
effectiveness of latent background knowledge retrieval.
Foreground objects in camouflaged images often display
intricate visual features, which we define as a combination
ofssub-features. The higher the value of s, the more in-
tricate and detailed the corresponding feature is. To extract
richer foreground features, we shift our focus from global
to local and employ the SLIC algorithm [1] to cluster the
foreground regions into ssuperpixels. The above process
can be reformulated as:
pi
1,pi
2,···,pi
s=S(cf
i,cm),
xf
i,j= Φ s 
pi
j,cm
=Pw,h
x=1,y=1cf
i,x,y∗pi
j,x,yPw,h
x=1,y=1pi
j,x,y.
(6)
3.2.3 Reasoning-Driven Condition Enhancement
Additionally, we upsample the obtained background knowl-
edge features xband combine them with the foreground
features cfto reconstruct the GT image features z0=
ε(y0),z0∈Rh×w×c. The reconstruction feature can be
computed as:
zrec=MLP (Concat (cf, upsample (xb, f))). (7)
Then, zrecis utilized to refine the initial condition of the
input. To emphasize the background features, we created a
feature reconstruction task that enhances the model’s abilityto reason about real background features using background
knowledge. Specifically, we populate the background re-
gion of cfwith the reconstructed zrecto strengthen the in-
formation embedded in the condition while reserving the
foreground areas. The strategy for enhancing the condition
can be formulated as:
˜cf=cf·(1−cm) +zrec·cm,
˜ c=Concat ˜cf,cm
.(8)
For the loss of background reconstruction, we have:
Lbgrec =1
h×whX
i=1wX
j=1(zrec·cm−z0·cm)2. (9)
Then, the overall loss can be reformulated as:
L=Ldiff+Lbgrec
∝ ∥ϵθ(zt,˜ c, t)−ϵ∥2
2+∥zrec·cm−z0·cm∥2.(10)
By leveraging the properties of the camouflaged image, we
refine and enhance the input condition c. While defining the
image features of the foreground area, the enhanced condi-
tion˜ cguides the generation of background. The implicit
and explicit constraints work together to help the model
learn the texture consistency between the foreground object
and the background, resulting in high-quality camouflage
image generation.
4. Experiments
4.1. Experimental Setups
•Datasets. Following the previous works [13] for
COD, 4,040 images (3,040 from COD10K [12], 1,000 from
CAMO [26]) are used as real data for training the model.
To verify the generative performance, we collected image-
mask pairs from various fields to construct a test data set,
including three subsets: Camouflaged Objects (CO), Salient
Objects (SO), and General Objects (GO). In CO, there are
6,473 pairs of images from CAMO [26], COD10K [12],
and NC4K [38]. Then we randomly selected 6473 im-
ages from the well-known salient object detection datasets
(DUTS [45], DUT-OMRON [50], etc.) and the segmenta-
tion dataset (COCO2017 [31]) to evaluate the performance
of the model on open domain data.
•Metrics. Following the good practices of AIGC
[27, 41] and COD [25, 37], we choose the InceptionNet-
based metrics FID [2] and KID [17] to measure the qual-
ity of generated camouflaged images. Once the image fea-
tures are extracted by InceptionNet, the distance between
them is computed to indicate the level of resemblance be-
tween the model’s output and the target dataset. Different
from the general images, well-synthesized camouflaged im-
ages should not include easily identifiable objects, and it
is more challenging to extract discriminative features [37].
A smaller value indicates that the generated image is more
similar to the real camouflaged image.
4096
Ours Repaint [36] LDM [41] TFill [64] LCGNet [29] DCI [53] AdaIN [21] CI [7] AB [10] Mask Image
Figure 4. Comparison with existing methods in transferring general images into camouflaged images. The first two columns are the
input images and we provide camouflaged images generated by nine methods for the comparison. Note that the methods in columns 3 to 7
additionally share a randomly sampled background image as input.
•Implementation Details. To generate camouflaged
images by given foreground images, we utilize a powerful
Latent Diffusion Model [41] pre-trained in the inpainting
task as initialization. The model is designed to handle im-
ages and masks of size 512×512 and is compressed to
a latent space of 128×128×3using a pre-trained VQ-
V AE [43]. During training, we focus on training the denois-
ing U-Net and do not fine-tune the auto-encoder and de-
coder components. We refine and enhance the existing con-
dition through the proposed module in this paper. The pa-
rameters optimization such as initialization, data augmen-
tation, and batch size are set similar to the original paper.
Finally, the model generates the camouflaged image and re-
sizes it to align with the original input. We conduct all the
experiments by PyTorch and GeForce RTX 3090 GPUs are
used for all experiments.
4.2. Comparison with the State-of-the-art Methods
Previous camouflage image generation methods are based
on image blending or style transfer, which differ funda-
mentally from the method proposed in this paper. Thus,
for each solution, we select cutting-edge methods for
comparison. For the image blending and style transfer
schemes, the model requires a manually specified back-
ground image when accepting a foreground input. We used
Places365 [65], a large-scale scene dataset, as the source
of background images. For a given foreground input, we
randomly sampled a background image from Places365, re-
sized it, and then performed image synthesis process. To fa-
cilitate comparison between different methods, all methods
shared the same background image for a given foreground
input. For the image inpainting scheme, the model onlyaccepts one foreground input and generates a camouflaged
image as output.
•Qualitative analysis. Fig. 4 presents a comparison of
the quality of camouflaged images generated by our method
and other methods from a general image. The results show
that methods such as AB and CI are highly influenced by
the background image input, despite the foreground fea-
tures being processed to align with the background. As a re-
sult, the foreground exhibits conflicts with the background
scenes and objects, such as the eagle and turtle in the room
(2nd and 3rd rows), and the larger-than-life frog (4th row).
LCGNet performs the best in hiding the objects, with their
features being barely visible. Camouflaged objects in na-
ture are seamlessly embedded in the background rather than
being completely invisible. On the other hand, image in-
painting methods only require foreground object input and
adaptive background generation can meet the requirements
of large-scale generation. However, existing methods suffer
from issues such as lack of authenticity of the background
(TFill), low degree of camouflage (LDM), and failure of
background complementation (Repaint-L). In contrast, our
method naturally integrates the given target into the gener-
ated background, preserving all the target’s features while
achieving overall camouflage of the image.
•Quantitative analysis. A large-scale test set is con-
structed to evaluate the quality of camouflage image gener-
ation, which includes three types of foreground objects to
assess the model’s adaptability to different image domains.
The salient objects subset and the general objects subset are
sampled from datasets in the salient object detection and
4097
Table 1. Quantitative performance. The proposed camouflaged image generation method is subjected to a quantitative evaluation,
wherein it is compared with state-of-the-art (SOTA) methods. The evaluation involved specific foreground objects sampled from camou-
flaged images, salient images, and general images. The proposed method shows excellent performance.
Methods InputCamouflaged Objects Salient Objects General Objects Overall
FID↓ KID↓ FID↓ KID↓ FID↓ KID↓ FID↓ KID↓
Image
BlendingAB [10] 03 F+B 117.11 0.0645 126.78 0.0614 133.89 0.0645 120.21 0.0623
CI [7] 10 F+B 124.49 0.0662 136.30 0.7380 137.19 0.0713 128.51 0.0693
AdaIN [21] 17 F+B 125.16 0.0721 133.20 0.0702 136.93 0.0714 126.94 0.0703
DCI [53] 20 F+B 130.21 0.0689 134.92 0.0665 137.99 0.0690 130.52 0.0673
LCGNet [29] 22 F+B 129.80 0.0504 136.24 0.0597 132.64 0.0548 129.88 0.0550
Image
InpaintingTFill [64] 22 F 63.74 0.0336 96.91 0.0453 122.44 0.0747 80.39 0.0438
LDM [41] 22 F 58.65 0.0380 107.38 0.0524 129.04 0.0748 84.48 0.0488
RePaint-L [36] 22 F 76.80 0.0459 114.96 0.0497 136.18 0.0686 96.14 0.0498
Ours 23 F 39.55 0.0212 88.70 0.0428 102.67 0.0625 64.27 0.0355
image segmentation domains, respectively, with the num-
ber of images kept consistent with the COD test set. The
distance between the generated results and the real COD
benchmarks is measured using FID and KID, and the re-
sults are presented in Tab. 1.
The results on the three subsets display a step-wise distri-
bution, indicating that the model performance was strongly
influenced by the image domain gap, with general objects
being more challenging to transform than salient objects.
The image blending-based methods produce large results
because they mechanically shift the foreground features to-
wards being consistent with the background features, result-
ing in image visual features that are primarily determined
by the background image. When the background image is
randomly sampled, the related indexes also exhibit some de-
gree of randomness. On the other hand, image inpainting-
based schemes tended to generate a suitable background for
the object and generally show better performance.
In addition, we observe outliers in the validation results
of LCGNet on the subset of General Objects, which are
caused by a combination of the following reasons. First,
the difficulty of synthesizing increases in three subsets. The
camouflaged object comes from a concealed scene and is
easy to hide. The salient object is of moderate size and
position and usually has a complete structure. The general
object has a rich variety of classes and diverse sizes, making
it challenging to find suitable camouflage environments for
it. As the complexity rises, these approaches progressively
struggle to conceal general objects flawlessly, leading to a
decline in performance within that particular subset. In this
case, LCGNet maximally discards foreground features, and
the results mainly depend on the randomly sampled back-
grounds (Fig. 4). It is least affected by the negative in-
fluence from the foreground and is introduced to random-
ness by the background, thus resulting in anomalous results.
However, our method achieved optimal performance on the
overall test set.
•User Study. Since both image generation quality
and camouflage effectiveness require human perception, weconducted user studies to obtain subjective human judg-
ments on the generated results. To this end, we followed
the previous work on camouflage image generation to ran-
domly select 20 sets of foreground images and applied vari-
ous methods to generate the results. For style transfer-based
methods, we used an additional image randomly sampled
from Places365 as the background input, which was kept
consistent for all methods. We invited 20 participants to
rate the results based on three questions:
-Q#1: Which result is the hardest to find?
-Q#2: Which is the most visually natural result?
-Q#3: Which result appears closest to the real camou-
flaged image dataset?
For each question, participants need to select their top 3
choices, with 1 being the highest. The results of the user
survey are presented in Fig. 5. Although LCGNet received
more votes in Q#1 due to the almost invisible foreground in
the generated results, our method was considered to produce
more natural and visually closer results to the real dataset in
terms of visual presentation.
Table 2. Quantitative Ablation study. We progressively add each
module to the base model to compare their impact on the quality of
the generated results and costs. The result shows that the method
we proposed is effective and almost cost-free.
ModulePrams(M) ↓MAC(G) ↓FPS(Hz) ↑Overall
BKRM RCEM LMP FID↓ KID↓
✗ ✗ ✗ 440.46 577.97 0.2482 96.14 0.0498
✓ ✗ ✗ 440.47 577.99 0.2442 69.80 0.0417
✓ ✓ ✗ 440.47 577.99 0.2438 69.52 0.0412
✓ ✓ ✓ 440.47 577.99 0.2008 64.27 0.0355
4.3. Ablation Study
We conduct the ablation study by gradually adding mod-
ules to the base LDM to evaluate the effectiveness of each
component in our proposed method. As shown in Tab. 2,
the quality of the generated camouflage images gradually
improves with the introduction of the modules proposed
in this paper, demonstrating the effectiveness. When all
three modules are applied simultaneously, the model perfor-
mance reaches its peak, achieving improvements of 33.14%
4098
Ours
RePaint-L
LDM 
TFill 
LCGNet 
DCI 
AdaIN 
CI 
AB Ours
RePaint-L
LDM 
TFill 
LCGNet 
DCI 
AdaIN 
CI 
AB Ours
RePaint-L
LDM 
TFill 
LCGNet 
DCI 
AdaIN 
CI 
AB 
Q#1: Which result is the hardest to find?0 100  0 200  100  
Q#2: Which is the most visually natural result ? Q#3: Which result is closest to the real dataset?1
2
3109111102131218
3377102
8012393
1363532
472
31118
811
12
1413
211807194101237
5275130
7017965
622831
2100
2612
8107
14924140135 89
382132
6786162
10010563
272726471
4513
988
116 26
0 100  200  Figure 5. User study about subjective ratings of the camouflaged image generated by 9 different methods. Our method is considered
to produce the most natural and visually closest results to the real camouflage image.
Foreground Image Base +BKRM +BKRM+RCEM +BKRM+RCEM+LMP
Figure 6. The visualization of ablation study. We visualize the samples during the ablation experiments to visualize the effectiveness of
the modules we proposed.
and 28.71% in the FID and KID metrics, respectively. At
this point, the introduction of the three modules only adds
about 0.01M parameters and 0.02G of computation to the
model, with the inference speed reduced by only 0.04Hz.
These results clearly indicate that our method is effective
and comes at almost no additional cost.
We further visualize the samples during the ablation ex-
periments to show the effectiveness of these modules. Two
sets of results are shown in Fig. 6. The LDM faces chal-
lenges in focusing on the camouflage properties during in-
painting from the foreground object to the background. It
also struggles to generate the background in certain regions,
resulting in black color blocks due to the complexity of the
task. By incorporating a latent background knowledge re-
trieval module (BKRM), the model is explicitly constrained
to learn foreground and background similarity, resulting
in a closer alignment of the generated background with
the foreground. Furthermore, the reasoning-driven condi-
tion enhancement module (RCEM) enhances the realism of
scenes by incorporating a background reconstruction loss
that compels the model to reason and reconstruct the back-
ground features accurately. Finally, the introduction of lo-
calized masked pooling (LMP) shifted the model’s attentionfrom global to local foreground features, enhancing the tex-
ture diversity of the generated background.
5. Conclusion
We propose a latent background knowledge retrieval-
augmented diffusion (LAKE-RED) for camouflaged im-
age generation. Unlike existing methods, our generation
paradigm is background-free. By knowledge retrieval and
reasoning enhancement, we get a strong background condi-
tion from the foreground, resulting in synthetic images that
surpass those generated by other SOTA camouflaged im-
age generation methods. Our approach is not restricted to
specific foreground targets or human-selected backgrounds.
This enables us to generate camouflage images on a large
scale and offers the potential for extending camouflaged vi-
sion perception to more diverse domains in the future.
6. Acknowledgments
This work was supported by the Natural Science Founda-
tion of Tianjin, China (NO.20JCJQJC00020), the National
Natural Science Foundation of China (NO.62306162), Fun-
damental Research Funds for the Central Universities, and
Supercomputing Center of Nankai University (NKSC).
4099
References
[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien
Lucchi, Pascal Fua, and Sabine S ¨usstrunk. Slic superpixels
compared to state-of-the-art superpixel methods. TPAMI , 34
(11):2274–2282, 2012. 5
[2] Mikołaj Bi ´nkowski, Danica J Sutherland, Michael Arbel, and
Arthur Gretton. Demystifying mmd gans. In ICLR , 2018. 5
[3] Keenan Burnett, Sepehr Samavi, Steven Waslander, Timo-
thy Barfoot, and Angela Schoellig. autotrack: A lightweight
object detection and tracking system for the sae autodrive
challenge. In CRV, 2019. 2
[4] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and
Michael J Black. A naturalistic open source movie for opti-
cal flow evaluation. In ECCV , 2012. 3
[5] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-
stuff: Thing and stuff classes in context. In CVPR , 2018.
2
[6] Yajie Chen, Xin Yang, and Xiang Bai. Confidence-weighted
mutual supervision on dual networks for unsupervised cross-
modality image segmentation. SCIS , 66(11):210104, 2023.
3
[7] Hung-Kuo Chu, Wei-Hsin Hsu, Niloy J Mitra, Daniel
Cohen-Or, Tien-Tsin Wong, and Tong-Yee Lee. Camouflage
images. TOG , 29(4):51–1, 2010. 3, 6, 7
[8] Antonia Creswell, Tom White, Vincent Dumoulin, Kai
Arulkumaran, Biswa Sengupta, and Anil A Bharath. Gener-
ative adversarial networks: An overview. SPM , 35(1):53–65,
2018. 2
[9] IC Cuthill. Camouflage. J ZOOL , 308(2):75–92, 2019. 3
[10] J. Mat ´ıas Di Martino, Gabriele Facciolo, and Enric
Meinhardt-Llopis. Poisson Image Editing. IPOL , 6:300–
325, 2016. 6, 7
[11] MA Ebrahimi, Mohammad Hadi Khoshtaghaza, Saeid Mi-
naei, and Bahareh Jamshidi. Vision-based pest detection
based on svm classification method. COMPAG , 137:52–58,
2017. 2
[12] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng,
Jianbing Shen, and Ling Shao. Camouflaged object detec-
tion. In CVPR , 2020. 2, 3, 5
[13] Deng-Ping Fan, Ge-Peng Ji, Ming-Ming Cheng, and Ling
Shao. Concealed object detection. TPAMI , 44(10):6024–
6042, 2021. 2, 5
[14] Deng-Ping Fan, Ge-Peng Ji, Peng Xu, Ming-Ming Cheng,
Christos Sakaridis, and Luc Van Gool. Advances in deep
concealed scene understanding. VI, 1(1):16, 2023. 2
[15] Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Laurent Itti, and
Vibhav Vineet. Dall-e for detection: Language-driven con-
text image synthesis for object detection. arXiv preprint
arXiv:2206.09592 , 2022. 3
[16] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing
Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic
data from generative models ready for image recognition? In
ICLR , 2023. 3
[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NeurIPS , 2017. 5[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 2
[19] Duojun Huang, Xinyu Xiong, De-Jun Fan, Feng Gao,
Xiao-Jian Wu, and Guanbin Li. Annotation-efficient
polyp segmentation via active learning. arXiv preprint
arXiv:2403.14350 , 2024. 2
[20] Duojun Huang, Xinyu Xiong, Jie Ma, Jichang Li, Zequn Jie,
Lin Ma, and Guanbin Li. Alignsam: Aligning segment any-
thing model to open context via reinforcement learning. In
CVPR , 2024. 2
[21] Xun Huang and Serge Belongie. Arbitrary style transfer in
real-time with adaptive instance normalization. In ICCV ,
2017. 6, 7
[22] Ge-Peng Ji, Jing Zhang, Dylan Campbell, Huan Xiong, and
Nick Barnes. Rethinking polyp segmentation from an out-
of-distribution perspective. MIR, pages 1–9, 2024. 2
[23] Guoli Jia and Jufeng Yang. S 2-ver: Semi-supervised visual
emotion recognition. In ECCV , 2022. 3
[24] Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci,
Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba,
and Sanja Fidler. Meta-sim: Learning to generate synthetic
datasets. In ICCV , 2019. 3
[25] Hala Lamdouar, Weidi Xie, and Andrew Zisserman. The
making and breaking of camouflage. In ICCV , 2023. 5
[26] Trung-Nghia Le, Tam V Nguyen, Zhongliang Nie, Minh-
Triet Tran, and Akihiro Sugimoto. Anabranch network for
camouflaged object segmentation. CVIU , 184:45–56, 2019.
5
[27] Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis,
Sanja Fidler, and Antonio Torralba. Bigdatasetgan: Synthe-
sizing imagenet with pixel-wise annotations. In CVPR , 2022.
2, 3, 5
[28] Jiaming Li, Jiacheng Zhang, Jichang Li, Ge Li, Si Liu, Liang
Lin, and Guanbin Li. Learning background prompts to dis-
cover implicit knowledge for open vocabulary object detec-
tion. In CVPR , 2024. 2
[29] Yangyang Li, Wei Zhai, Yang Cao, and Zheng-Jun Zha.
Location-free camouflage generation network. TMM , 25:
5234–5247, 2023. 3, 6, 7
[30] Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng
Wang, and Weidi Xie. Open-vocabulary object segmentation
with diffusion models. In ICCV , 2023. 3
[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C. Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV , 2014. 5
[32] Gengxin Liu, Oliver van Kaick, Hui Huang, and Ruizhen Hu.
Active self-training for weakly supervised 3d scene semantic
segmentation. CVMJ , pages 1–14, 2024. 2
[33] Xin Liu and Jufeng Yang. Progressive neighbor consistency
mining for correspondence pruning. In CVPR , 2023. 3
[34] Xianglong Liu, Shihao Bai, Shan An, Shuo Wang, Wei Liu,
Xiaowei Zhao, and Yuqing Ma. A meaningful learning
method for zero-shot semantic segmentation. SCIS , 66(11):
210103, 2023. 3
[35] Xin Liu, Guobao Xiao, Riqing Chen, and Jiayi Ma. Pgfnet:
Preference-guided filtering network for two-view correspon-
dence learning. TIP, 32:1367–1378, 2023. 3
4100
[36] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting
using denoising diffusion probabilistic models. In CVPR ,
2022. 6, 7
[37] Xue-Jing Luo, Shuo Wang, Zongwei Wu, Christos Sakaridis,
Yun Cheng, Deng-Ping Fan, and Luc Van Gool. Camd-
iff: Camouflage image augmentation via diffusion. AIR, 2:
9150021, 2023. 5
[38] Yunqiu Lv, Jing Zhang, Yuchao Dai, Aixuan Li, Bowen Liu,
Nick Barnes, and Deng-Ping Fan. Simultaneously localize,
segment and rank the camouflaged objects. In CVPR , 2021.
5
[39] Sami Merilaita, Nicholas E Scott-Samuel, and Innes C
Cuthill. How camouflage works. Philos T R Soc B , 372
(1724):20160341, 2017. 3
[40] Alhassan Mumuni, Fuseini Mumuni, and Nana Kobina Ger-
rar. A survey of synthetic data augmentation methods in ma-
chine vision. MIR, pages 1–39, 2024. 3
[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 3, 5, 6,
7
[42] Martin Stevens and Sami Merilaita. Animal camouflage:
current issues and new perspectives. Philos T R Soc B , 364
(1516):423–427, 2009. 3
[43] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. In NeurIPS , 2017. 3, 6
[44] Junyi Wang and Yue Qi. Multi-task learning and joint re-
finement between camera localization and object detection.
CVMJ , pages 1–19, 2024. 2
[45] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,
Dong Wang, Baocai Yin, and Xiang Ruan. Learning to de-
tect salient objects with image-level supervision. In CVPR ,
2017. 5
[46] Lijuan Wang, Guoli Jia, Ning Jiang, Haiying Wu, and Jufeng
Yang. Ease: Robust facial expression recognition via emo-
tion ambiguity-sensitive cooperative networks. In ACM MM ,
2022. 3
[47] Changsong Wen, Guoli Jia, and Jufeng Yang. Dip: Dual
incongruity perceiving network for sarcasm detection. In
CVPR , 2023. 3
[48] Magnus Wrenninge and Jonas Unger. Synscapes: A pho-
torealistic synthetic dataset for street scene parsing. arXiv
preprint arXiv:1810.08705 , 2018. 3
[49] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou,
and Chunhua Shen. Diffumask: Synthesizing images with
pixel-level annotations for semantic segmentation using dif-
fusion models. In ICCV , 2023. 2, 3
[50] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and
Ming-Hsuan Yang. Saliency detection via graph-based man-
ifold ranking. In CVPR , 2013. 5
[51] Li Yuan, Xinyi Liu, Jiannan Yu, and Yanfeng Li. A full-
set tooth segmentation model based on improved pointnet++.
VI, 1(1):21, 2023. 2
[52] Yingjie Zhai, Guoli Jia, Yu-Kun Lai, Jing Zhang, Jufeng
Yang, and Dacheng Tao. Looking into gait for perceiving
emotions via bilateral posture and movement graph convolu-
tional networks. TAFFC , 2024. 3[53] Qing Zhang, Gelin Yin, Yongwei Nie, and Wei-Shi Zheng.
Deep camouflage images. In AAAI , 2020. 3, 6, 7
[54] Xiaolin Zhang, Yunchao Wei, Yi Yang, and Thomas S
Huang. Sg-one: Similarity guidance network for one-shot
semantic segmentation. TCYB , 50(9):3855–3865, 2020. 5
[55] Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-
Francois Lafleche, Adela Barriuso, Antonio Torralba, and
Sanja Fidler. Datasetgan: Efficient labeled data factory with
minimal human effort. In CVPR , 2021. 2, 3
[56] Zhicheng Zhang and Jufeng Yang. Temporal sentiment lo-
calization: Listen and look in untrimmed videos. In ACM
MM, 2022. 3
[57] Zhicheng Zhang, Song Chen, Zichuan Wang, and Jufeng
Yang. Planeseg: Building a plug-in for boosting planar re-
gion segmentation. TNNLS , pages 1–15, 2023. 3
[58] Zhicheng Zhang, Shengzhe Liu, and Jufeng Yang. Multiple
planar object tracking. In ICCV , 2023. 3
[59] Zhicheng Zhang, Lijuan Wang, and Jufeng Yang. Weakly
supervised video emotion detection and prediction via cross-
modal temporal erasing network. In CVPR , 2023. 3
[60] Zhicheng Zhang, Junyao Hu, Wentao Cheng, Danda Paudel,
and Jufeng Yang. Extdm: Distribution extrapolation diffu-
sion model for video prediction. In CVPR , 2024.
[61] Zhicheng Zhang, Pancheng Zhao, Eunil Park, and Jufeng
Yang. Mart: Masked affective representation learning via
masked temporal distribution distillation. In CVPR , 2024. 3
[62] Sicheng Zhao, Guoli Jia, Jufeng Yang, Guiguang Ding, and
Kurt Keutzer. Emotion recognition from multiple modalities:
Fundamentals and methodologies. SPM , 38(6):59–73, 2021.
3
[63] Sicheng Zhao, Xingxu Yao, Jufeng Yang, Guoli Jia,
Guiguang Ding, Tat-Seng Chua, Bj ¨orn W. Schuller, and Kurt
Keutzer. Affective image content analysis: Two decades
review and new perspectives. TPAMI , 44(10):6729–6751,
2022. 3
[64] Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai, and Dinh
Phung. Bridging global context interactions for high-fidelity
image completion. In CVPR , 2022. 6, 7
[65] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. TPAMI , 40(6):1452–1464, 2018. 6
[66] Shihao Zhou, Mengxi Jiang, Qicong Wang, and Yunqi Lei.
Towards locality similarity preserving to 3d human pose es-
timation. In ACCV , 2020. 3
[67] Shihao Zhou, Mengxi Jiang, Shanshan Cai, and Yunqi Lei.
Dc-gnet: Deep mesh relation capturing graph convolution
network for 3d human shape reconstruction. In ACM MM ,
2021.
[68] Shihao Zhou, Duosheng Chen, Jinshan Pan, Jinglei Shi, and
Jufeng Yang. Adapt or perish: Adaptive sparse transformer
with attentive feature refinement for image restoration. In
CVPR , 2024. 3
4101
