Dual-View Visual Contextualization for Web Navigation
Jihyung Kil Chan Hee Song Boyuan Zheng Xiang Deng Yu Su Wei-Lun Chao
The Ohio State University
{kil.5,song.1855,zheng.2372,deng.595,su.809,chao.209}@osu.edu
Abstract
Automatic web navigation aims to build a web agent that
can follow language instructions to execute complex and di-
verse tasks on real-world websites. Existing work primarily
takes HTML documents as input, which define the contents
and action spaces ( i.e., actionable elements and operations)
of webpages. Nevertheless, HTML documents may not pro-
vide a clear task-related context for each element, making it
hard to select the right (sequence of) actions. In this paper,
we propose to contextualize HTML elements through their
“dual views” in webpage screenshots: each HTML element
has its corresponding bounding box and visual content in
the screenshot. We build upon the insight— web developers
tend to arrange task-related elements nearby on webpages
to enhance user experiences —and propose to contextualize
each element with its neighbor elements, using both tex-
tual and visual features. The resulting representations of
HTML elements are more informative for the agent to take
action. We validate our method on the recently released
Mind2Web dataset, which features diverse navigation do-
mains and tasks on real-world websites. Our method con-
sistently outperforms the baseline in all the scenarios, in-
cluding cross-task, cross-website, and cross-domain ones.
1. Introduction
We study automatic web navigation with natural language
instructions [8, 35]. This problem is crucial as it can poten-
tially streamline and automate a wide range of tasks in our
increasingly web-centric world, from online shopping to ac-
cessing information. Successfully solving this problem can
also broadly advance artificial intelligence as it requires un-
derstanding and executing various tasks by interacting with
dynamic and complex real-world (web) environments.
Existing work primarily takes HTML documents as the
web agent’s input [8, 10, 30], which define the meaning and
layout of webpage content. Written partially in natural lan-
guage, HTML documents enable the use of large language
models (LLMs) [1, 4–6, 15, 28, 32, 33] to ground language
instructions ( e.g., “Find one-way flights from New York to
[button] Pick-up Mar22[combobox][button] Return location
[button] Mar23[searchbox] Code
Encoder{ele}{ele}{ele}{ele}{ele}HTML ElementsVisual FeaturesDual-VCR Representation of {ele}1Positional Embeddings2345DocumentHTMLScreenshotWebpage
ElementsHTMLFigure 1. Overview of our proposed Dual-View Contextualized
Representation (D UAL-VCR). HTML elements ( e.g., “[com-
bobox]”) may not have clear contexts for solving web navigation
tasks ( e.g., “Find the lowest rent truck with a pick-up time at 11
am on March 27.”). D UAL-VCR contextualizes each element with
its neighbors in the screenshot ( e.g., “[button] Pick-up Mar22”) to
obtain more informative representations for decision-making.
Toronto.”) in web environments. Moreover, elements in
HTML documents directly define the space of actions ( e.g.,
element “[button] Search” with operation “click”), prevent-
ing the agent from hallucinating infeasible actions.
With that being said, HTML documents may lack a clear
task-related context for each element, impeding the agent
from selecting the right (sequence of) actions to complete a
task. HTML is quite flexible for web developers to arrange
their code. Even semantically related elements, such as an
actionable element ( e.g., “drop-down box”) and its label el-
ement ( e.g., “Number of Passengers”), may not be located
nearby in the document or the DOM tree. This problem also
applies to elements relevant to solving a task. While LLMs
may learn to capture the context, a raw HTML document
of real-world webpages is often quite huge, consisting of
tens of thousands of tokens, making it either infeasible or
cost-prohibitive to be directly fed into LLMs [8, 10, 30].
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14445
In this paper, we propose to enhance the context of each
HTML element by leveraging its “dual view” in the screen-
shot of the rendered webpage: many of the HTML elements
(including the actionable ones) are visible in the screenshot
and have their corresponding bounding boxes1. Taking the
insight— semantically related and task-related HTML ele-
ments are often located nearby on the webpage to facilitate
user experiences—we propose to contextualize each HTML
element with its neighbors in the screenshot. Concretely,
when encoding each HTML element, we 1) append its spa-
tially adjacent elements with positional embeddings and 2)
incorporate both the visual and textual features (Figure 1).
While simple, our method, which we name Dual-View
Contextualized Representation (D UAL-VCR) , has sev-
eral compelling properties that benefit web navigation fun-
damentally. First, DUAL-VCR uses the built-in feature
of HTML documents to align textual and visual content,
making it robust to complex and diverse websites. Second,
DUAL-VCR effectively leverages visual cues on the web-
pages, which are designed to ease users’ efforts in under-
standing and completing tasks. Specifically, DUAL-VCR
connects visually proximate elements that are often seman-
tically related and task-related , providing the agent with
more explicit contexts to take not only individual actions
but also the sequence of actions. Last but not least, DUAL-
VCR can potentially be integrated into any web navigation
algorithms that take HTML documents as input.
We validate D UAL-VCR on the Mind2Web dataset [8],
the largest web navigation benchmark with over 2,000 tasks
curated from 137 real-world websites across 31 domains,
including restaurants, airlines, public services, etc. Con-
cretely, we implement DUAL-VCR on top of the MindAct
algorithm [8], which was proposed to tackle huge HTML
documents. In short, at each action, MindAct first applies
a small LM to rank each HTML element to shrink the doc-
ument; it then uses an LLM to predict the action. We in-
tegrate D UAL-VCR into both steps to enhance the con-
text for element ranking and decision-making. D UAL-VCR
consistently improves MindAct across all three scenarios
(cross-task, cross-website, and cross-domain), leading to a
3.7% absolute gain on average over nine evaluation metrics.
Moreover, D UAL-VCR notably outperforms baselines that
use entire HTML documents or screenshots as input, offer-
ing significant advantages in computation and accuracy.
Our contributions are three-folded:
• We propose D UAL-VCR, a simple and effective dual-view
representation of HTML elements for web navigation.
• D UAL-VCR consistently outperforms baselines on the real-
world web navigation benchmark Mind2Web [8].
• We conduct comprehensive analyses to understand the ef-
fect of our design choices on web navigation performance.
1These bounding boxes can be directly inferred from the HTML docu-
ment without the need to detect them.2. Related Work
Web navigation datasets . Several prior studies [2, 16, 23,
31, 35] have introduced promising benchmarks for assess-
ing agents in web navigation tasks. However, these bench-
marks are often limited to a narrow range of website do-
mains or confined to simplified simulated environments.
For instance, MiniWob++ [16] and WebShop [35] collected
a set of websites including daily tasks ( e.g., shopping), but
each website only has fewer than fifty HTML elements on
average. Some other studies [2, 23, 31] instead explored
other domains, including mobile applications, but their ac-
tion spaces are often simpler than web navigation. Recently,
Mind2Web [8] released the first large-scale web navigation
benchmark consisting of over 2K tasks from various real-
world websites. This enables a comprehensive understand-
ing of web agent’s behaviors in “real-world” scenarios.
The use of HTML documents . Most earlier work [16, 18,
25, 35] focused on simple navigation scenarios like Mini-
Wob++ [16]. Due to the brevity of its HTML documents,
they input whole HTML documents into LLMs to complete
the web navigation tasks. A few studies represented HTML
documents in a more dense format. For instance, ASH [30]
summarized the HTML document using LLMs with hier-
archical prompting. DOM-Q-NET [18] leveraged a graph
neural network to represent a document as a graph. For
real-world web navigation ( e.g., Mind2Web), HTML docu-
ments are often overly lengthy and complex. Thus, recent
studies [8–10] applied text-based filtering to first identify
key HTML elements within the document and only used the
selected elements to complete the task. While all these prior
methods are promising, the HTML document alone may not
provide a clear task-related context for each element, mak-
ing it challenging to select the right actions. Our approach
instead enhances the context of each HTML element based
on their dual view in the screenshot.
The use of webpage screenshots . Beyond using HTML
documents, several studies [9, 11, 14, 16, 17, 21, 29, 35, 36]
have explored the incorporation of screenshots for web nav-
igation. Some of them [9, 11, 14, 16, 17, 36] utilized both
screenshots and HTML documents to learn their joint repre-
sentations during decision-making. Some others [3, 21, 29]
solely relied on screenshots, bypassing the use of HTML
documents. We note that all prior methods primarily fo-
cused on utilizing “whole” screenshots. In contrast, we shift
the focus to neighboring elements within the screenshot,
providing significant benefits in computation and accuracy.
3. Approach: D UAL-VCR
We introduce Dual-View Contextualized Representation
(DUAL-VCR) for enhanced web navigation. To begin with,
we provide a brief background about web navigation.
14446
t=3
MindAct: GT: Dual-VCR:[combobox]SELECT 11:00 am[button] VehiclesCLICK✘[combobox]SELECT 11:00 am✓t=4
[button] 03/27/2023CLICK[button] 03/27/2023CLICK[button] 03/27/2023CLICK✓✓
t=8
[checkbox] 4+CLICK[button] Extra 4CLICK✘CLICK[checkbox] 4+✓MindAct: GT: Dual-VCR:[button] ReviewCLICK✘[button] SelectCLICK[button] SelectCLICK✓t=9Find the lowest rent truck for 4 people, pick up from JFK airport at 11 am on March 27 and return at noon on March 30.Web Navigation Task
Figure 2. Example of real-world web navigation. Top : the web navigation task described in natural language. Left: the sequence of
HTML elements (visualized on webpages, not HTML documents) to interact with to complete the task. We superimpose bounding boxes
and arrows to locate the target elements and indicate their order. Right : the detail at each time step (we showed t={3,4,8,9}for brevity).
GT: ground-truth action (Element with Operation). We compare the predicted actions by MindAct [8] and our DUAL-VCR . The bounding
box and bounding box indicate the target element and one of its neighbors encoded by DUAL-VCR . As shown, DUAL-VCR correctly
predicts the elements and operations at “all” time steps, taking advantage of the much richer task-related dual-view context it encodes.
3.1. Background: web navigation
A web navigation task consists of a website S(e.g., an air-
line website) and an instruction q(“Find one-way flights
from New York to Toronto.”). Given (S, q), a web agent
fneeds to decide and perform a sequence of actions a=
{a1, a2,···, at,···} on the website to complete the task.
Figure 2 (left) gives an illustration.
At time step t, the website has an HTML document Ht,
composed of a list of elements Ht={et,1, et,2,···, et,N}.
These HTML elements jointly define 1) the layout and con-
tent on the rendered webpage It, and 2) the action space at
timet: each candidate action is a pair of an actionable el-
ement ( e.g., “[textbox] To”) and an operation ( e.g., “Type
Toronto”). After taking action at, both the HTML docu-
ment and webpage will be updated into (Ht+1, It+1). For
example, clicking the “[checkbox] One way” on the air-
line webpage removes the “[textbox] Return date” from the
webpage. Namely, the web environment is dynamic, and
the agent must take this into account to decide its actions.
Because of the rich content in the HTML document Ht,
existing work primarily takes it, together with the instruc-
tionqand the action history ( e.g.,Type New York in theFrom box ), as the agent’s input at time tto decide the next
action ( e.g.,Type Toronto in the To box ),
at+1=f(q, Ht,{a1, a2,···, at}). (1)
One excellent candidate for fis LLMs [1, 4–6, 15, 28,
32, 33], which have shown straggering sucesses in ques-
tion answering [34] and logical reasoning [7]. For example,
[16, 19] applied LLMs to simplified web navigation.
However, for real-world webpages that easily contain
thousands of HTML elements (amounting to tens of thou-
sands of tokens), directly applying LLMs is neither efficient
nor effective. As such, recent work [8, 10, 30] employed a
two-stage framework: first summarizing the HTML docu-
ment and then predicting the action. For instance, given the
instruction qand the action history at time t, the MindAct
algorithm [8] first ranks each HTML element using a small
LM. Only the top- KHTML elements are fed into an LLM
to predict the next action. (See Figure 3 for an illustration.)
3.2. Context enhancement
We identify one critical pitfall in the two-stage framework.
Since HTML documents may not provide a clear context for
each element, the element ranker and the subsequent action
14447
HTML DocumentRanking LM
+Target ElementOperationPrediction LLM…Candidate ElementsTop-k ElementsHTML Snippet
Task DescriptionPrevious Actions
Webpage ScreenshotDual-VCRDual-VCRFigure 3. The web navigation pipeline with D UAL-VCR, built
on top of the MindAct algorithm [8]. MindAct uses a small rank-
ing LM to select candidate HTML elements and a prediction LLM
to decide actions. Blocks and arrows in NavyBlue indicate the in-
sertion of DUAL-VCR for enhanced element representations.
predictor may not perform as effectively as expected. Fig-
ure 1 illustrates one such issue: the element “[combobox]”
should be paired with “[button] Pick-up Mar22” to fully de-
scribe its role, i.e., time for pick-up. However, these two el-
ements are not necessarily nearby in the HTML document.
To resolve this issue, we propose to leverage the “dual
view” of each HTML element et,n∈Htin the rendered
webpage Itto enhance its context. In essence, many HTML
elements (including the actionable ones) are visible in It.
Further, their visual location ( e.g., bounding boxes) can be
inferred from HTML documents. Since a webpage (specif-
ically, its screenshot) is designed for users to interact with
the website visually, we hypothesize that incorporating the
visual cues into HTML element representations would ben-
efit the web agent in understanding and completing tasks.
To this end, we propose Dual-View Contextualized
Representation (D UAL-VCR) . In the screenshot view, we
identify the bounding box of each HTML element using
a web automation testing tool2. Taking the insight—web
developers tend to arrange semantically relevant and task-
related elements in proximity to each other on the screen-
shot to enhance user experiences—we contextualize each
element with its “visual” neighbors. Concretely, we calcu-
late the center points of all elements using their bounding
boxes and measure their pairwise distances. For each can-
didate element to be ranked by MindAct, we search for the
closest Melements to form its context jointly.
We consider both the visual and textual information to
encode the candidate element and its visual neighbors. We
extract each element’s visual feature using the Pix2Struct
Vision Transformer (ViT) [20], which is pre-trained on
webpage screenshots. Specifically, we input the whole
screenshot Itinto the ViT and apply ROI Align [12, 24]
on top of the output embeddings to obtain the feature vec-
tor corresponding to each element’s bounding box. In the
HTML document view, we extract each element’s corre-
sponding “HTML text” following MindAct [8].
2https://playwright.dev/
Webpage ScreenshotHTML Document
VisionModelRoIAlignVisual featVisual featVisual featVisual featElementElementElementElement12341234Task DescriptionPrevious Actions00Ranking LMClassifier[0, 1]ScoreProjectionFigure 4. DUAL-VCR-enhanced element ranker. . We contex-
tualize the candidate element (denoted by ⋆) with its neighbors in
the screenshot, using both the visual features (by [20]) and textual
features (extracted from the HTML document). Positional embed-
dings are added to specify neighbor elements, learning their spatial
relationships and pairing the textual features with visual features.
This dual-view contextualized representation is used to rank the
candidate element, measuring its relevance to the current task.
3.3.DUAL-VCR -enhanced element ranker
In MindAct, a small ranking LM is built to predict each
element’s importance for action prediction. At each time
step, the ranking LM takes the element’s HTML text tokens,
the task description q, and the previous actions as input.
We propose to expand the ranking LM to integrate 1)
both visual features and textual features and 2) both the can-
didate element and its neighbor elements. (See Figure 4 for
an illustration.) We make the following design choices. To
align the visual embedding and textual embedding, we fol-
low the recent practice of vision-and-language models ( e.g.,
BLIP-2 [22], LLaV A [27], LLaV A-1.5 [26]) to learn a linear
projection layer to project ViT visual features into the same
dimensionality as the token embeddings in the ranking LM.
To pair each of the projected visual vectors with its corre-
sponding text tokens and specify each neighbor element in
the context, we add positional encoding. Concretely, we
sort the neighbors based on their spatial distances from the
candidate element and add a learnable positional embed-
ding (unique for each rank) to the neighbor element’s visual
and text token embeddings. These positionally encoded vi-
sual and text token embeddings (of the candidate and the
neighbor elements) are fed into the ranking LM; the pro-
jected visual features are prepended to the text embeddings,
serving as soft visual prompts. In training, we only learn
the linear projection layer, the positional embeddings, and
the LM while keeping the ViT frozen. This training scheme
has been shown to effectively enhance the alignment be-
tween vision and language components and improve the
pre-trained LM’s adaptability to downstream tasks. Please
see more details in the supplementary materials.
3.4.DUAL-VCR -enhanced action predictor
After obtaining the top- Kelements from the ranker (§3.3),
MindAct combines them into an HTML snippet as the input
14448
Webpage ScreenshotHTML Document
ElementElementElementElementElementElementElementElementPrediction LLMElementTask Description + Previous ActionsTarget ElementOperationFigure 5. DUAL-VCR-enhanced action predictor . Given the
top-Kcandidate elements (three in the figure, marked with ⋆),
DUAL-VCR appends each with its neighbor elements. The result-
ing HTML snippet, together with the task description and previous
actions, is then fed into an LLM for predicting the next action.
to LLMs. The objective is to predict the action for the cur-
rent time step, including the target element ( e.g., “[textbox]
To”) and its associated operation ( e.g., “Type Toronto”).
Specifically, MindAct converts the target element predic-
tion problem into multiple-choice question-answering.
We apply D UAL-VCR to contextualize each of the an-
swer candidates. Similarly to §3.3, we find the Mclosest
neighbors for each candidate element on the screenshot. We
then append the HTML text tokens of these Mneighbors to
the candidate element; we add specific tokens to separate
between elements. Figure 5 gives an illustration. Please see
the supplementary material for more details.
3.5. Why DUAL-VCR ?
DUAL-VCR leverages and encodes visual cues on the web-
page, offering valuable contexts for the HTML elements in
element ranking and action prediction. We show two cases.
First, as shown in Figure 1, some HTML elements ( e.g.,
“[combobox]”) are quite generic and must be paired with
spatially nearby elements ( e.g., “[button] Pick-up Mar22”)
to specify their meanings ( i.e., time for pick-up). Similar
examples can be found in Figure 2. At t= 8, there are two
seemingly similar candidates “[checkbox] 4+” and “[but-
ton] Extra 4”. Nevertheless, the former is spatially closer
to the element “Number of passengers”, indicating its relat-
edness to the task “... truck for 4 people ...” (see the top
of Figure 2). At t= 9, two identical “[button] Select” ele-
ments exist. The only way to differentiate them is through
their visual neighbors: one is associated with a lower price
than the other. Our D UAL-VCR offers an explicit way to
enforce these spatial contexts in the screenshots.
Second, as shown in the left panel of Figure 2, consec-
utive steps to solve a task often involve spatially nearby el-
ements. Completing one step thus introduces a prior that
its nearby elements may be the next to take action upon.
As both the ranking LM and prediction LLM take the task
description q,past actions , and our D UAL-VCR represen-
tation as input, the models could potentially capture such
prior information to increase the success rate for the follow-
ing action. For example, at t= 4, DUAL-VCR successfully
takes the action “Select 11:30 am”, likely attributing to its
capability to recognize that the previously completed taskwas the spatially nearby “Select 03/27/2023”.
4. Experimental Results
Dataset . We validate D UAL-VCR on Mind2Web [8], a
comprehensive benchmark for real-world web navigation.
Unlike other benchmarks based on simulated websites with
only a few HTML elements, Mind2Web uses over 100 real-
world websites with thousands of HTML elements. Con-
cretely, they provide over 2K open-ended tasks collected
from 137 real-world websites across 31 different domains,
including travel, shopping, public service, etc (Table 1).
Please see more details in the supplementary material.
Evaluation Tasks . Followed by Mind2Web [8], we evalu-
ate models at three different test splits. In Cross-Domain ,
we evaluate the model’s generalizability to a new domain
where it has not seen any websites or tasks associated with
that domain during training. This split contains 912 tasks
in total. In Cross-Website (177 tasks), while the model is
not exposed to test websites, it is trained on websites from
the same domain and potentially with similar tasks. This
configuration enables us to evaluate the model’s capacity to
adapt to entirely new websites within familiar domains and
tasks. Similar to the conventional training/test split, Cross-
Task (252 tasks) randomly splits 20% of the data as a test
set, regardless of the domains and the websites. Please see
the supplementary material for more details.
Evaluation Metrics . We use the Mind2Web’s official met-
rics. The ranker performance is measured by Recall@ K,
where Kis the number of top HTML candidate elements.
Element Accuracy (Ele. Acc) compares the selected el-
ement with the ground-truth elements. Operation F1
(Op. F1) calculates the token-level F1 score for the pre-
dicted operation. Step Success Rate (Step SR) measures
the success of each step; A step is considered successful
only if both the selected element and the predicted operation
are correct. For each step, they provide previous “ground-
truth” actions with the assumption that the model success-
fully completes all previous steps.
Baselines . D UAL-VCR is based on MindAct [8], which
has a ranking LM and a prediction LLM. Our main base-
lines are thus its ranker and action predictor, denoted by
MINDACTRANKandMINDACTPRED. M INDACTRANKuses
DeBERTa base[13], a small encoder-only LM to rank el-
ements. For action prediction, M INDACTPREDuses Flan-
T5base[6], an instruction fine-tuned LLM.
Our Models . Aligned with MindAct, we use the same
DeBERTa base[13] / Flan-T5 base[6] for our ranker / action
predictor, repsectively. For visual features extraction, we
utilize Pix2Struct [20]’s ViT (pre-trained on screenshots) as
the visual backbone and apply ROI Align [12] on the ele-
ment’s region. We use two linear layers to project visual
14449
Dataset # Websites Website # TasksAvg # HTML
Type Elements Tokens
MiniWoB++ [16] 100 Simplified 100 28 500
Mind2Web [8] 137 Real-world 2,350 1,135 44,402
Table 1. Statistics of Mind2Web [8]. Min2Web, the largest web
navigation benchmark, collects real-world websites across various
domains. The significant volume of content on the webpage ( e.g.,
an average of 1K/44K HTML elements/tokens) poses challenges
for LLMs in both computational and learning aspects.
RankerRecall
@1 @5 @10 @50
MINDACTRANK 25.4 61.0 73.5 88.9
DUAL-VCR VNEI -TXT 37.3 70.8 79.3 89.2
DUAL-VCR VIS 37.1 70.2 79.2 89.1
DUAL-VCR VNEI -TXT+VIS 38.4 71.6 79.7 90.1
Table 2. Ranking performance. Visual neighbors’ HTML text
(DUAL-VCR VNEI -TXT) consistently outperforms M INDACTRANK.
Moreover, D UAL-VCR VNEI -TXT+VIS, using both visual neighbors’
HTML text and visual features, performs best, showing the
strength of dual-view contextualization in element ranking.
features into textual embedding space. Please see the sup-
plementary materials for details on the model training.
Notation of D UAL-VCR . D UAL-VCR has several varia-
tions to understand the effect of each of its components in
detail. We denote them as follows:
•DUAL-VCR VIS: Ranker w/ candidate’s visual features.
•DUAL-VCR VNEI -TXT: Ranker w/ neighbors’ HTML text.
•DUAL-VCR VNEI -TXT+VIS: Ranker w/ candidate’s visual features
and its neighbors’ visual features and HTML text.
•DUAL-VCR PRED: Action predictor w/ neighbors’ HTML text.
4.1. Effectinvess of DUAL-VCR
The main goal of our experiments is to show that our dual-
view contexutalization is beneficial in (i) finding promising
top-Kcandidates from entire HTML documents ( i.e., rank-
ing peformance), and (ii) predicting the action, including
both element selection and operation prediction.
Ranking performance . Table 2 summarizes the ranking
results across different top- Kcandidate elements. First,
we see that incorporating the visual neighbor elements’
HTML text (D UAL-VCR VNEI -TXT) consistently and signifi-
cantly outperforms M INDACTRANKon all Recall@ Ks (e.g.,
37.3% vs. 25.4% on Recall@1, 79.3% vs. 73.5% on Re-
call@10), suggesting that contextualizing the element with
its neighbors indeed helps find the target element. Second,
the candidate element’s visual features (D UAL-VCR VIS)
lead to notable improvements over M INDACTRANK(e.g.,
70.2% vs. 61.0% on Recall@5). This implies that the visual
features offer additional context in differentiating HTMLelements, compared to using only its HTML text. Lastly,
DUAL-VCR VNEI -TXT+VISachieves a further boost by lever-
aging both visual neighbors’ HTML text and visual features
(e.g., 38.4%/90.1% on Recall@1/@50).
Action prediction performance . Table 3 shows the re-
sults of action prediction. Compared to the baseline (the
combination of M INDACTRANKand M INDACTPRED), us-
ing the visual neighbors’ HTML texts (D UAL-VCR VNEI -TXT
→DUAL-VCR PRED) notably improves across all metrics.
For instance, we achieve gains of 3.4% on Step SR in Cross-
Task, 1.3% on Ele. Acc in Cross-Webiste, and 6.3% on
Op. F1 in Cross-Domain. These consistent improvements
demonstrate the advantages of incorporating visual neigh-
bor information during the model’s decision-making pro-
cess. Moreover, aligning with the ranking result, integrating
the visual neighbors’ visual features into the ranker (D UAL-
VCR VNEI -TXT+VIS) shows its effectiveness in action predic-
tion as well. Concretely, it achieves the best performance
on all nine metrics, along with a 5% maximum gain on each
type of metric against the baseline ( e.g., Ele. Acc: 47.0%
vs. 42.0% on Cross-Task, Op. F1: 72.0% vs. 67.0% on
Cross-Website, Step SR: 46.0% vs. 41.1% on Cross-Task).
4.2. Analysis
We aim to understand D UAL-VCR in detail. We show a) a
more in-depth analysis of the main table, b) the interaction
between the ranker and the action predictor, c) its effective-
ness compared to whole input data and random elements,
and d) the effect of different sizes of visual neighbors.
Detailed ablation . Table 4 provides more details about the
main table to better understand the impact of each compo-
nent in D UAL-VCR. First, we keep the action predictor as
MINDACTPREDand focus on the pure effects of our rankers
on the action prediction task ( i.e., 1st to 4th rows). We see
that incorporating the candidate element’s visual features
(DUAL-VCR VIS) achieves a slight but significant improve-
ment over M INDACTRANKacross all metrics ( e.g., 42.5%
vs. 42.0% on Ele. Acc). Furthermore, our ranker with
the visual neighbors’ HTML text (D UAL-VCR VNEI -TXT)
outperforms M INDACTRANK by a notable margin of
+2.6%/+0.8%/+2.1% on Ele. Acc/Op. F1/Step SR, respec-
tively. Besides, D UAL-VCR VNEI -TXT+VIS, which encodes
the visual neighbors’ visual features, further improves the
model’s decision-making ability ( e.g., 46.0% vs. 44.6% on
Ele. Acc). In short, we consistently demonstrate the effec-
tiveness of each component in our ranker.
Second, conversely, we fix the ranker and examine the
benefit of encoding visual neighbors’ HTML text features
into the action predictor (D UAL-VCR PRED). Compared to
MINDACTPRED, DUAL-VCR PRED achieves consistent gains
across all rankers. For instance, M INDACTRANK→DUAL-
VCR PRED outperforms M INDACTRANK→MINDACTPRED
14450
Ranker ActionCross-Task Cross-Website Cross-Domain
Predictor Ele. Acc Op. F1 Step SR Ele. Acc Op. F1 Step SR Ele. Acc Op. F1 Step SR
MINDACTRANK MINDACTPRED 42.0 74.9 41.1 30.7 67.0 30.0 31.5 66.6 31.0
DUAL-VCR VNEI -TXTDUAL-VCR PRED45.3 78.4 44.5 32.0 71.5 31.5 32.4 72.9 32.0
DUAL-VCR VNEI -TXT+VIS 47.0 78.7 46.0 32.7 72.0 32.5 33.2 73.3 32.5
Table 3. Results of action prediction. Our D UAL-VCR VNEI -TXT→DUAL-VCR PRED, leveraging visual neighbors’ HTML text information,
notably improves over the baseline (M INDACTRANK→MINDACTPRED) on all nine metrics. Adding visual neighbors’ visual features
(DUAL-VCR VNEI -TXT+VIS) leads to further improvements, highlighting the benefit of dual-view context on real-world web navigation.
Ranker ActionCross-Task
Predictor Ele. Acc Op. F1 Step SR
MINDACTRANK
MINDACTPRED42.0 74.9 41.1
DUAL-VCR VIS 42.5 75.1 41.5
DUAL-VCR VNEI -TXT 44.6 75.7 43.2
DUAL-VCR VNEI -TXT+VIS 46.0 78.6 44.8
MINDACTRANK
DUAL-VCR PRED44.4 75.2 43.1
DUAL-VCR VIS 44.6 76.8 43.8
DUAL-VCR VNEI -TXT 45.3 78.4 44.5
DUAL-VCR VNEI -TXT+VIS 47.0 78.7 46.0
Table 4. Ablation studies for validating the importance of each
component in D UAL-VCR. See §4.2 for a detailed discussion.
(e.g., 44.4% vs. 42.0% on Ele. Acc). Similarly, when fixing
the ranker with D UAL-VCR VNEI -TXT+VIS, DUAL-VCR PRED
improves over M INDACTPRED(e.g., 46.0% vs. 44.8% on
Step SR). This shows directly encoding the visual neigh-
bor’s HTML text into the action predictor is beneficial.
Finally, D UAL-VCR VNEI -TXT+VISand D UAL-VCR PRED
are complementary; we achieve the best performance across
all metrics when leveraging both ( e.g., 47.0%/78.7%/46.0%
on Ele. Acc/Op. F1/Step SR). Please see more ablation stud-
ies in the supplementary materials.
Ranker-action predictor relationship . We analyze the re-
lationship between the ranker and the action predictor in
Table 5. We observe a linear connection between the two.
Concertely, improving the ranker ( e.g., 25.4% vs. 37.3% on
Recall@1) correlates with improved action prediction re-
sults ( e.g., 24.0% vs. 35.5% on Ele. Acc). Aligned with
results in §4.2, this again highlights the importance of im-
proving the model’s ranking ability in web navigation.
Comparison to whole input data . Since HTML docu-
ments contain a significant amount of content, such as thou-
sands of HTML elements, conducting experiments with
whole data is computationally challenging. Nevertheless,
we do our best to report the associated results on Ta-
ble 6 to give more context on the effect of D UAL-VCR.
First, instead of asking the ranker to prune HTML docu-
ments, we directly pass the whole HTML documents into
the action predictor (W HOLE HTML PRED). We see that
WHOLE HTML PREDperforms notably less against the base-line (M INDACTPRED) (i.e., 38.6% vs. 42.0% on Ele. Acc).
We attribute this to the difficulty of finding the target el-
ement among all thousands of elements. In contrast, our
DUAL-VCR PREDachieves a much better result ( i.e., 44.4%)
with significantly less amount of input elements.
Second, D UAL-VCR outperforms the utilization of
whole images. We first use the entire image for the ranker
(WHOLE IMAGE RANK). To extract the image features, we use
the same procedure mentioned in §3.2, except for providing
the region of the whole image instead of that of specific ele-
ments. We then use these whole image features, along with
the same HTML text input used in M INDACTPRED, to train
WHOLE IMAGE RANK. Although the entire image features are
shown effective over the baseline ( i.e., 43.9% vs. 42.0%),
it performs notably less than our approach using the vi-
sual neigbhor ’s visual information ( i.e., 46.0% of D UAL-
VCR VNEI -TXT+VIS). In addition, we conducted a study apply-
ing the whole image to the action predictor. Specifically,
similar to recent vision-and-language models [22, 26, 27],
we extract whole image features using fine-tuned ViT [20]
and prepend them to the top-50 candidate elements ex-
tracted from M INDACTRANKas the input to the LLM (Flan-
T5base[6]). Similar to the result of W HOLE IMAGE RANK, this
action predictor (W HOLE IMAGE PRED) performs worse than
DUAL-VCR PRED, which only uses visual neighbors ’ HTML
text. Overall, this highlights the advantages of our approach
in terms of computational efficiency and performance. See
additional results in the supplementary materials.
Visual neighbors offer meaningful contexts . We exam-
ine whether visual neighbors provide meaningful context
for element ranking and action prediction. To assess this,
we compare visual neighboring elements with random el-
ements (Table 7). Specifically, We randomly select (five)
elements from HTML documents and use them to train ei-
ther the ranker or the action predictor. While our ranker
(e.g., D UAL-VCR VNEI -TXT) notably improves the ranking
performance over M INDACTRANK(e.g., 89.2% vs. 88.9%),
the “random” ranker performs less than M INDACTRANK
(e.g., 86.7% vs. 88.9%). This, in turn, leads to a sig-
nificant performance drop in the action prediction ( e.g.,
42.0% vs. 40.6% on Ele. Acc). Similarly, compared to
the M INDACTPRED, including random elements in the ac-
14451
Ranker ActionTop-1 Top-5 Top-10 Top-50
Predictor Recall Ele. Acc Op. F1 Recall Ele. Acc Op. F1 Recall Ele. Acc Op. F1 Recall Ele. Acc Op. F1
MINDACTRANKMINDACTPRED25.4 24.0 23.7 61.0 39.2 52.1 73.5 41.4 62.8 88.9 42.0 74.9
DUAL-VCR VNEI -TXT 37.3 35.5 33.5 70.8 43.1 54.1 79.3 43.9 63.0 89.2 44.6 75.7
Table 5. Relationship between ranker and action predictor on Cross-Task. The ranker has a linear correlation with the action predictor,
suggesting the importance of improving its ranking capabilities for decision-making.
RankerAction Cross-Task
Predictor Ele. Acc
MINDACTRANKMINDACTPRED 42.0
WHOLE IMAGE PRED 43.6
DUAL-VCR PRED 44.4
WHOLE IMAGE RANK
MINDACTPRED43.9
DUAL-VCR VNEI -TXT 44.6
DUAL-VCR VNEI -TXT+VIS 46.0
- W HOLE HTML PRED 38.6
Table 6. Visual neighbor vs. whole input data. Using visual
neighbors notably outperforms the use of whole data, offering ad-
vantages regarding computational efficiency and performance.
Ranker Recall ActionCross-Task
@50 Predictor Ele. Acc Op. F1
MINDACTRANK 88.9MINDACTPRED 42.0 74.9
RANDOM PRED 41.5 73.6
DUAL-VCR PRED 44.4 75.2
RANDOM RANK 86.7MINDACTPRED40.6 72.0
DUAL-VCR VNEI -TXT 89.2 44.6 75.7
Table 7. Visual neighbors vs. random elements. Visual neigh-
bors provide meaningful contexts for web navigation, notably out-
performing elements randomly extracted from HTML documents.
Ranker Cross-Task
Method # neighbors Recall@50 Ele. Acc Op. F1
DUAL-VCR VIS 0 89.1 42.5 75.1
DUAL-VCR VNEI -TXT+VIS3 89.7 45.5 77.3
5 90.1 46.0 78.6
10 89.5 45.2 77.0
Table 8. Effects of the number of neighbors on ranker. Choos-
ing the right size of visual neighbors is important for element
ranking, and the size of five is found to be most effective for
Mind2Web [8]. We fix the action predictor with M INDACTPRED.
tion predictor hurts the action prediction performance ( e.g.,
74.9% vs. 73.6 on Op. F1) while visual neighbors are bene-
ficial ( e.g., 75.2%). In sum, we empirically demonstrate the
benefits of context in visual neighbors for web navigation.
Effects of the number of visual neighbors . We ablate
the impact of varying sizes of visual neighbors, starting
with Table 8, which shows its effect on the ranker whileAction Predictor Cross-Task
Method # neighbors Ele. Acc Op. F1
MINDACTPRED 0 46.0 78.6
DUAL-VCR PRED3 46.4 78.7
5 47.0 78.7
10 46.2 78.6
Table 9. Effects of the number of neighbors on action predictor.
Similar to Table 8, the size of five is most appropriate for the action
prediction. We use D UAL-VCR VNEI -TXT+VISfor the ranker.
maintaining the same action predictor (M INDACTPRED).
We observe a linear correlation between the size of vi-
sual neighbors and their ranking/action prediction perfor-
mance. For instance, increasing the size of neighbors up to
five shows consistent improvements ( e.g., 89.1% →90.1%
on Recall@50 and 75.1% →78.6% on Op. F1). However,
considering too many neighbors ( e.g., the size of ten) hurts
the performance. For example, increasing the size from
five to ten decreases the element accuracy from 46.0% to
45.2%. We also see a similar pattern when ablating the
effect of the visual neighbor size on the action predic-
tor (Table 9). Concretely, while keeping the same ranker
(DUAL-VCR VNEI -TXT+VIS), the action performance increases
up to the size of five ( e.g., 46.0% →47.0% on Ele. Acc)
but decreases when the size becomes ten ( e.g., 46.2% on
Ele. Acc). Overall, this suggests that choosing an appro-
priate number of neighbors is necessary for both element
ranking and action prediction.
5. Conclusion
We introduce D UAL-VCR to effectively represent HTML
elements for web navigation. D UAL-VCR contextualizes
each element with its visual neighbor elements, leveraging
both textual and visual features. D UAL-VCR consistently
improves real-world web navigation in the Mind2Web
benchmark, supported by comprehensive analyses.
Acknowledgments
This research is supported in part by grants from the Na-
tional Science Foundation (IIS-2107077, OAC-2112606)
and ARL W911NF2220144. We are thankful for the gen-
erous support of the computational resources by the Ohio
Supercomputer Center.
14452
References
[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. In NeurIPS , 2020. 1, 3
[2] Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Ku-
mar, Kate Saenko, and Bryan A Plummer. A dataset for inter-
active vision-language navigation with unknown command
feasibility. In ECCV , 2022. 2
[3] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yan-
tao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Har-
nessing gui grounding for advanced visual gui agents. arXiv
preprint arXiv:2401.10935 , 2024. 2
[4] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-
hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
Xing. Vicuna: An open-source chatbot impressing gpt-
4 with 90%* chatgpt quality. https://lmsys.org/
blog/2023-03-30-vicuna/ , 2023. 1, 3
[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311 , 2022.
[6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, et al. Scaling
instruction-finetuned language models. arXiv preprint
arXiv:2210.11416 , 2022. 1, 3, 5, 7
[7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark
Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry
Tworek, Jacob Hilton, Reiichiro Nakano, et al. Train-
ing verifiers to solve math word problems. arXiv preprint
arXiv:2110.14168 , 2021. 3
[8] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel
Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web:
Towards a generalist agent for the web. In NeurIPS , 2023. 1,
2, 3, 4, 5, 6, 8
[9] Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Mat-
suo, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web
navigation with instruction-finetuned foundation models. In
ICLR , 2024. 2
[10] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari,
Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-
world webagent with planning, long context understanding,
and program synthesis. In ICLR , 2024. 1, 2, 3
[11] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong
Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. We-
bvoyager: Building an end-to-end web agent with large mul-
timodal models. arXiv preprint arXiv:2401.13919 , 2024. 2
[12] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Gir-
shick. Mask r-cnn. In ICCV , 2017. 4, 5
[13] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu
Chen. Deberta: Decoding-enhanced bert with disentangled
attention. In ICLR , 2021. 5
[14] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu,
Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, YuxiaoDong, Ming Ding, et al. Cogagent: A visual language model
for gui agents. arXiv preprint arXiv:2312.08914 , 2023. 2
[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. In
ICLR , 2022. 1, 3
[16] Peter C. Humphreys, David Raposo, Tobias Pohlen, Gregory
Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abram-
son, Petko Georgiev, Alex Goldin, Adam Santoro, and Tim-
othy P. Lillicrap. A data-driven approach for learning to con-
trol computers. In ICML , 2022. 2, 3, 6
[17] Taichi Iki and Akiko Aizawa. Do berts learn to
use browser user interface? exploring multi-step tasks
with unified vision-and-language berts. arXiv preprint
arXiv:2203.07828 , 2022. 2
[18] Sheng Jia, Jamie Kiros, and Jimmy Ba. Dom-q-net:
Grounded rl on structured language. In ICLR , 2019. 2
[19] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Lan-
guage models can solve computer tasks. In NeurIPS , 2023.
3
[20] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu,
Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandel-
wal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova.
Pix2struct: Screenshot parsing as pretraining for visual lan-
guage understanding. In ICML , 2023. 4, 5, 7
[21] Gang Li and Yang Li. Spotlight: Mobile ui understanding
using vision-language models with a focus. In ICLR , 2023.
2
[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 4, 7
[23] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason
Baldridge. Mapping natural language instructions to mobile
ui action sequences. In ACL , 2020. 2
[24] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.
Exploring plain vision transformer backbones for object de-
tection. In ECCV , 2022. 4
[25] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin
Shi, and Percy Liang. Reinforcement learning on web in-
terfaces using workflow-guided exploration. In ICLR , 2018.
2
[26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. arXiv
preprint arXiv:2310.03744 , 2023. 4, 7
[27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. In NeurIPS , 2023. 4, 7
[28] OpenAI. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774 , 2023. 1, 3
[29] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant,
Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Ken-
ton Lee, and Kristina Toutanova. From pixels to ui actions:
Learning to follow instructions via graphical user interfaces.
In NeurIPS , 2023. 2
[30] Abishek Sridhar, Robert Lo, Frank F Xu, Hao Zhu, and
Shuyan Zhou. Hierarchical prompting assists large language
model on web navigation. In EMNLP , 2023. 1, 2, 3
14453
[31] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen
Zhu, and Kai Yu. Meta-gui: Towards multi-modal conversa-
tional agents on mobile gui. In EMNLP , 2022. 2
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto. Stanford alpaca: An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca , 2023. 1, 3
[33] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288 , 2023. 1, 3
[34] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi,
Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Ar-
jun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David
Stap, et al. Benchmarking generalization via in-context in-
structions on 1,600+ language tasks. In EMNLP , 2022. 3
[35] Shunyu Yao, Howard Chen, John Yang, and Karthik
Narasimhan. Webshop: Towards scalable real-world web in-
teraction with grounded language agents. In NeurIPS , 2022.
1, 2
[36] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu
Su. Gpt-4v (ision) is a generalist web agent, if grounded.
arXiv preprint arXiv:2401.01614 , 2024. 2
14454
