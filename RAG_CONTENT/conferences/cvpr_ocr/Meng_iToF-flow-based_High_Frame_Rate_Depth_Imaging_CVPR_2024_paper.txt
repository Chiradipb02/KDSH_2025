iToF-Ô¨Çow-based High Frame Rate Depth Imaging
Yu Meng1, Zhou Xue2, Xu Chang3, Xuemei Hu1*, Tao Yue1
1Nanjing University, Nanjing, China,2Li Auto,3Bytedance Inc.
mengyu@smail.nju.edu.cn, xuezhou08@gmail.com, changxu.21@bytedance.com,
{xuemeihu,yuetao }@nju.edu.cn
Abstract
iToF is a prevalent, cost-effective technology for 3D per-
ception. While its reliance on multi-measurement com-
monly leads to reduced performance in dynamic environ-
ments. Based on the analysis of the physical iToF imag-
ing process, we propose the iToF Ô¨Çow, composed of cross-
mode transformation and uni-mode photometric correction,
to model the variation of measurements caused by differ-
ent measurement modes and 3D motion, respectively. We
propose a local linear transform (LLT) based cross-mode
transfer module (LCTM) for mode-varying and pixel shift
compensation of cross-mode Ô¨Çow, and uni-mode photomet-
ric correct module (UPCM) for estimating the depth-wise
motion caused photometric residual of uni-mode Ô¨Çow. The
iToF Ô¨Çow-based depth extraction network is proposed which
could facilitate the estimation of the 4-phase measure-
ments at each individual time for high framerate and ac-
curate depth estimation. Extensive experiments, including
both simulation and real-world experiments, are conducted
to demonstrate the effectiveness of the proposed methods.
Compared with the SOTA method, our approach reduces the
computation time by 75 %while improving the performance
by38%. The code and database are available at https://
github.com/ComputationalPerceptionLab/iToF_
flow.
1. Introduction
Time of Ô¨Çight (ToF) imaging is a cornerstone technol-
ogy for depth imaging, renowned for its broad application
across numerous Ô¨Åelds [ 17,20,36,38]. The basic prin-
ciple of ToF is to estimate the time difference between
emitting and receiving signals to retrieve depth informa-
tion [ 22]. Commonly, ToF imaging technology can be cat-
egorized into direct-ToF (dToF) imaging and indirect-ToF
(iToF) imaging. In dToF imaging, single photon avalanche
diodes (SPAD) or avalanche photodiode arrays (APD) are
commonly utilized and the time delay can be directly mea-
sured [ 35]. However, the dToF system is constrained by
two main limitations, i.e. high hardware costs and low spa-
*Corresponding author.
Uni-mode Cross-mode
Predicted 
measurementMapped 
measurementùúÉùë°
(a)ùúÉùë°
(b)Figure 1. Overview of the proposed iToF Ô¨Çow-based high frame
rate depth reconstruction. (a) Captured alternating iToF measure-
ments, (b) iToF Ô¨Çow-based propagation of full-mode measure-
ments, and reconstructed depths.
tial resolution. In iToF imaging with amplitude-modulated
continuous wave (AMCW), depth information is encoded
in multiple modes (e.g., 4 modes with 90‚ó¶phase shifts re-
spectively), which correspond to the cross-correlation in-
tegrals of the receiving and emitting signal with different
phase shifts [ 22]. The indirect working principle allows for
lower hardware costs and higher spatial resolution [ 38].
Nevertheless, the reliance on multiple measurements
leads to errors in the depth estimation of dynamic scenes
and limited framerate of depth imaging. To overcome the
problem, manually designed constraint rules and supple-
mentary information were proposed to correct pixel val-
ues for speciÔ¨Åc motion scenes [ 5,10,21,29]. Besides,
motion compensation methods based on optical Ô¨Çow esti-
mation were also proposed for iToF measurements align-
ment [ 11,28]. However, the realistic 3D motion can not be
represented losslessly with 2D optical Ô¨Çow (OF) [ 33]. Such
a dilemma can be tackled with an explicit 3D motion com-
pensation (e.g. scene Ô¨Çow [ 33]) on the iToF measurement
while overcoming the photometric inconsistencies caused
by 3D motion and different modes. However, direct estima-
tion of scene Ô¨Çow usually relies on explicit 3D reconstruc-
tion [ 32,33], and the 3D motion estimation is also highly ill-
posed and computationally complex. Besides, photometric
inconsistency due to different modes of iToF measurements
also exacerbates the ill-posedness of OF estimation.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
4929
Based on the analysis of the iToF imaging process, we
propose iToF-Ô¨Çow to model the variation caused by the
3D motion and alternation of measurement mode, and de-
velop an iToF-Ô¨Çow-based depth extraction neural network
for high frame rate depth estimation. SpeciÔ¨Åcally, from a
physics-inspired perspective, as shown in Fig. 1, we decom-
pose the variation of iToF measurements into cross-mode
Ô¨Çow, which models the photometric variation and pixel shift
among different modes, and the uni-mode Ô¨Çow, which mod-
els the photometric residuals caused by depth-wise motion
of the same mode. As for the cross-mode Ô¨Çow, we ob-
serve the motion-insensitive local linear transformation be-
tween different modes of the measurements and propose
the LLT-based cross-mode transfer module (LCTM). As for
uni-mode Ô¨Çow, we derive the depth-dependent photometric
residual formulation and propose the uni-mode photometric
compensation module (UPCM). With the end-to-end pro-
cessing, the 3D motion is separated into the 2D plane of OF
due to space shift and luminance residuals due to depth-wise
motion, which can be extracted sequentially and separately.
Compared with other methods, the advantages in runtime
and accuracy of our method are demonstrated with exten-
sive experiments. In all, our contributions are concluded as:
‚Ä¢ We propose an iToF Ô¨Çow model, composed of cross-
mode Ô¨Çow and uni-mode Ô¨Çow, to comprehensively
model the variation of measurements caused by dif-
ferent demodulation and 3D motion in iToF imaging.
‚Ä¢ We build an iToF Ô¨Çow-based depth extraction network
based upon LCTM and UPCM, for high-frame-rate
and accurate depth imaging from iToF measurements.
‚Ä¢ We provide an extension database with different mod-
ulation parameters and scenes to augment the existing
iToF databases [ 27,28].
‚Ä¢ Extensive experiments with simulation and real-world
data are conducted and demonstrate the effectiveness
of the proposed method.
2. Related Work
iToF for Time-varying Scene . Depth estimation of iToF
depends on multiple exposure measurements [ 22]. Scene
and camera motions can induce misalignment between
measurements, resulting in errors in depth estimation. Mod-
eling motion-induced pixel misalignment as a noisy time se-
ries, Kalman Ô¨Åltering was employed to mitigate the effects
of transverse motion [ 29]. Jan et al. [30] proposed a model-
based tracking approach using iToF raw measurement to ob-
tain a10√óhigher depth frame rate. Chen et al. [5] proposed
an alignment method based on extra data from a highly dy-
namic sensor using a short exposure. Gao et al. [10] pro-
posed Ô¨Åner categorizations of motion-introduced errors and
designed different correction methods respectively based on
the neighboring pixel. Lee et al. [21] designed rules for de-
tecting moving regions with regular electric charge relations
and proposed a replacement method with adjacent pixels.Database Type GT Size Motion
FLAT [ 11] Syn. Yes 1.2k Yes
CB-ToF [ 27] Syn. Yes 21.4k No
CB-ToF-Extension [ 28] Syn. Yes 2.1k Yes
MF-ToF [ 12] Syn. Yes 155k No
Ours Syn. Yes 2k Yes
Table 1. Summary of public iToF datasets.
Furthermore, learning-based method [ 3,4] are proposed
to recover depth information from extremely low signal-
to-noise measurements with short exposure and reduced
motion-induced error. Guo et al . [11] proposed a neural
network-based encoder-decoder architecture to output ve-
locity maps, which can be used to align the raw iToF mea-
surements. Similarly, Michael et al. [28] used the optical
Ô¨Çow estimation network as their baseline and proposed mul-
tiple loss regularization terms to help overcome the photo-
metric gap between iToF measurements. It‚Äôs worth noting
that the majority of the mentioned methods primarily focus
on aligning pixel positions, neglecting the photometric er-
rors induced by depth-wise motion.
Dynamic Cross Modality Imaging . To capture multi-
modality information with a single camera, imaging tech-
niques that alternately capture different modalities at differ-
ent times are proposed. SpeciÔ¨Åcally, in the Ô¨Åeld of real-time
hyperspectral imaging, Hu et al. [14] propose a complex op-
tical Ô¨Çow (COF)-based method to reconstruct hyperspectral
video information from spectral-sweep video sequences. In
the Ô¨Åeld of high dynamic range (HDR) video imaging, cap-
turing and fusion low dynamic ranges (LDR) frames with
different exposures is commonly adopted [ 2,6,19]. In [ 19],
a CNN-based method was proposed to estimate the mo-
tion Ô¨Çow between two frames explicitly. Chen et al . [2]
proposed a multi-stage spatial temporal pixel alignment for
HDR video reconstruction. In addition to pixel domain
alignment, feature domain alignment was proposed [ 6]. Pu
et al . [26] proposed a Pyramidal Alignment and Masked
merging network (PAMnet) to align the pyramid multi-scale
feature of LDR images to synthesize HDR images. These
works, based on the physical principle of different measure-
ment modes, showing elegant performance in reconstruct-
ing multi-mode video from cross-mode measurements.
iToF Simulation and Database . Data-driven approaches
typically rely on extensive, high-quality datasets paired
with accurate ground truth. Obtaining detailed 3D ground
truth data from real-world iToF photography is both chal-
lenging and expensive. Therefore, simulation has become
the primary way of acquiring iToF dataset. Mitsuba is a
physically-based rendering system developed for research
in computer graphics and physically-based modeling [ 16].
Based on Mitsuba, MitsubaToFRenderer [ 25] was proposed
for rendering ToF data speciÔ¨Åcally. Besides, common de-
velopment tools, such as Blender [ 7] or Unity [ 13], are able
to support importing ToF cameras in simulation. Tab. 1
summarized the publicly accessible large datasets of iToF
4930
(a)
(e) Local K (f) Local B(b) Local        at t (c) Local        at t+1 ùëöùúÉ0
(d)Mapped ùëöùúÉ1ùëöùúÉ0ùëöùúÉ0Figure 2. Demonstration of LLT robustness to motion edges. Lo-
cal area in (b), (c), (e) and (f) is marked with red box in (a).
imaging. As shown, there is still a shortage of adequate
datasets featuring motion. We propose to supplement the
iToF dataset with around 2K motion samples.
3. iToF Flow-Based Depth Extraction Network
In alternating 4-mode iToF imaging, measurements at
different times exhibit varying phase-shifts. Groups of four
measurements, each with a 90‚ó¶phase-shift, are convention-
ally used to extract depth, reÔ¨Çectance, and environmental
illumination. For dynamic scenes with fast-moving objects,
the asynchrony of these 4-mode measurements introduces
ghost and blur artifacts in the moving edges, compromising
the accuracy of the extracted depth. To solve the problem, it
is natural to propagate the three absent mode information of
a single moment from previous measurements with optical
Ô¨Çow [ 28]. However, the photometric inconsistency intro-
duced by the heterogeneous phase shift and depth-wise mo-
tion at different times prevents the direct utilization of the
conventional optical Ô¨Çow-based methods, leading to errors
and artifacts which is hard-to-ignore in practice.
In this paper, we propose an iToF Ô¨Çow model that builds
the relationship between the measurement at different times
and phase shifts, and decomposes the iToF Ô¨Çow depth esti-
mation network into two modules: the cross-mode transfer
and uni-mode photometric correction modules. Building
upon these two modules, we propose the iToF Ô¨Çow-based
depth estimation neural network that accurately estimates
depth of each moment in an end-to-end manner by prop-
agating absent phase information from previous measure-
ments to the target time and correcting depth-wise motion-
induced photometric-intensity-bias with the uni-mode Ô¨Çow.
Details of the iToF model and the depth extraction network
are illustrated in this section.
3.1. LLT¬≠based Cross¬≠mode Transformation
In order to tackle the measurement absence problem, we
propose an LLT-based cross-mode transformation for prop-
agating the measurements with the corresponding mode at
the previous moments to the target times. In the following,we give a detailed analysis of the LLT-based cross-mode
transfer and the corresponding network structure design.
Cross-mode LLT Mapping . The challenging aspect of
information propagation lies in accurately aligning the 2D
motion Ô¨Çow between two modes at different times, known
as cross-mode Ô¨Çow estimation. Thus we propose a motion-
insensitive local linear transfer model that could formu-
late the mapping between different modes. Thanks to its
motion-insensitive property, the model remains invariant for
slightly moved frames within a short time range, such as a
single iToF acquisition period with 4-mode frames. The
LLT model relies on a universal image property: for each
sufÔ¨Åciently small local region around the edge, there are two
types of features corresponding to the piece-wise smooth
regions on the two sides of the edge. The features of pix-
els on the edges can be represented as a linear combina-
tion of the features of the two sides [ 37], which still holds
for iToF measurements regardless of the measured depth.
Based on this property, the 4-mode iToF measurements
within a local region could be represented by the points
along the line determined by the two 4-mode iToF mea-
surement vectors of two regions around the edge. Denoting
the 4-mode vectors on the two sides of the edges as m‚Ä≤=
[m‚Ä≤
Œ∏1,m‚Ä≤
Œ∏2,m‚Ä≤
Œ∏3,m‚Ä≤
Œ∏4]Tandm‚Ä≤‚Ä≤= [m‚Ä≤‚Ä≤
Œ∏1,m‚Ä≤‚Ä≤
Œ∏2,m‚Ä≤‚Ä≤
Œ∏3,m‚Ä≤‚Ä≤
Œ∏4]T,
the vector mpof a certain pixel pin the local region Scould
be represented by
mp=Œ≤pm‚Ä≤+(1‚àíŒ≤p)m‚Ä≤‚Ä≤, (1)
wheremp= [mp
Œ∏1,mp
Œ∏2,mp
Œ∏3,mp
Œ∏4],Œ≤pdenotes the combi-
nation factor of pixel p. Thus, the formulation among ele-
ments of the 4-mode vectors could be represented as linear
mapping based model, i.e.
mp
Œ∏i=kj‚Üíimp
Œ∏j+bj‚Üíi,‚àÄp‚àà S,
kj‚Üíi=m‚Ä≤
Œ∏i‚àím‚Ä≤‚Ä≤
Œ∏i
m‚Ä≤
Œ∏j‚àím‚Ä≤‚Ä≤
Œ∏j,
bj‚Üíi=m‚Ä≤‚Ä≤
Œ∏i‚àíkj‚Üíim‚Ä≤‚Ä≤
Œ∏j, i,j‚àà {Œ∏1,Œ∏2,Œ∏3,Œ∏4}.(2)
It is obvious that for a certain region where the two-feature
property is valid, the parameters of LLT, i.e., kj‚Üíiandbj‚Üíi,
are identical for all the pixels in the local region. In other
words, for a small local region, the mapping parameters
ki‚Üíjandbi‚Üíjat different pixels are uniform and do NOT
change in edge position. Therefore, if the edge movement in
a relatively small time period does not exceed the range of
the region, the LLT could remain unchanged, demonstrating
a valuable insensitive property in our scenario. As shown in
the Fig. 2, we calculated the LLT maps between alternating
modes at different times, and it can be seen that kandb
present an extended area around the edges, which conforms
to our assessment of the motion insensitive property of the
LLT transformation in the edge region. BeneÔ¨Åting from this
motion-insensitive characteristic, we could easily comple-
4931
UPCM3D Conv, Stride=2
3D Conv, Stride=2
3D DeConv, Stride=23D DeConv, Stride=23D Conv, Stride=2
Concat
PS, Scale=2ConcatD
P
RSFEDownsample
2√ó(Conv, Stride=2)
8√ó(Conv, Stride=1)
TranspodeConv+
UpsampleWarp
ConcatEDB0
EDB2
++EDB1
Flow2√ó(Conv, Stride=2)
4√ó(Conv, Stride=1)
PS, Scale=44√ó(Conv, Stride=1)ConcatùëöùúÉ1ùë°‚àí1, ùëöùúÉ2ùë°‚àí2, ùëöùúÉ3ùë°‚àí3, ùëöùúÉ0ùë°‚àí4, ùëöùúÉ0ùë°LMM
+
BK
Flow0
ÔºàdÔºâ ÔºàcÔºâ ÔºàbÔºâLLT-based Cross-mode Transfer
B
KùëöùúÉ0ùë°‚àí4
ùëöùúÉ3ùë°‚àí3
ùëöùúÉ2ùë°‚àí2
ùëöùúÉ1ùë°‚àí1
ùëöùúÉ0ùë°X+‡¥• ùëöùúÉ0ùë°‚àí3
‡¥• ùëöùúÉ0ùë°‚àí2
‡¥• ùëöùúÉ0ùë°‚àí1
Frequency Concatùêπt‚àí1‚Üíùë°
ùêπùë°‚àí3‚Üíùë°ùêπt‚àí2‚Üíùë°
Warp‡∑ùùëöùúÉ1ùë°
‡∑ùùëöùúÉ2ùë°
‡∑ùùëöùúÉ3ùë°
D
P
R
+ùëöùúÉ1ùë°
ùëöùúÉ2ùë°
ùëöùúÉ3ùë°
LMMSFE UPCM
ÔºàaÔºâ
Measured 
Queue Uni-mode Photometric CorrectionInput WindowiToF
Camera
ùëöùúÉùëñùë°‚àíùëñ, ùëñ ‚àà 0,1,2,3 ; ùëöùúÉ0ùë°‚àí4, ùëöùúÉ0ùë°; ‡¥• ùëöùúÉ0ùë°‚àíùëó, ùëó ‚àà 1,2,3 ; f
ùëöùúÉùëñùë°‚àíùëñ, ùëñ ‚àà 1,2,3 ; ùëöùúÉ0ùë°‚àí4; ùëöùúÉ0ùë°;‡∑ùùëöùúÉjùë°,j‚àà1,2,3 ;f, FlowFigure 3. (a) Framework of the proposed network, composed of cross-mode transfer module and uni-mode correction module. (b) Structure
of submodule LMM. (c) Cascade Schematic of submodule SPE. (d) Illustration of the structure of UPCM.
ment the absent modes from the current measurement and
the LLT maps estimated from the previous frames.
LLT-based Cross-mode Transfer Module . We propose an
LLT-based Cross-mode Transfer Module (LCTM) to com-
pute the LLT maps from the previous measurements and
transfer the absent modes at the current moment from previ-
ous measurements. Instead of computing the depth by using
the measurements computed by LLT directly, we introduce
an optical Ô¨Çow-based measurement propagation framework
to prevent the imperfections caused by the invalid cases
of the two-feature property in very few regions, i.e., esti-
mate the optical Ô¨Çow between the LLT mapped measure-
ments at time t‚àíi,i‚àà1,2,3and the real captured mea-
surements at current time t, and then compute the absent
measurements of time tby warping the previous measure-
ments according to the optical Ô¨Çows. As shown in Fig. 3
(a) and (b), the measurements at the aligned target mo-
menttismt
Œ∏0. The submodule LLT-based mapping mod-
ule (LMM) computes the K= [KŒ∏1‚ÜíŒ∏0,KŒ∏2‚ÜíŒ∏0,KŒ∏3‚ÜíŒ∏0]
andB= [BŒ∏1‚ÜíŒ∏0,BŒ∏2‚ÜíŒ∏0,BŒ∏3‚ÜíŒ∏0]from the previous al-
ternating measurements by
LMM(mt‚àí4
Œ∏0,mt‚àí3
Œ∏3,mt‚àí2
Œ∏2,mt‚àí1
Œ∏1,mt
Œ∏0) = [K;B].(3)
As shown in Fig. 3(b), we use a pithy Convolutional Neu-
ral Network(CNN) structure to construct the LMM(¬∑). Two
convolutional layers with a stride of 2 downsample the in-
put to a quarter of the original resolution. Eight convolu-
tional layers with 1-pixel stride extract features. The outputis upsampled by pixelshufÔ¨Çe. Since we do NOT use the
simple patch-based closed form LLT in Eq. ( 2), there is no
explicit hyper-parameters or constraints on motion sizes are
required. To supervise the LMM, we evaluate the perfor-
mance of the LMM with mean absolute error (MAE).
LLMM=1
33/summationdisplay
i=1MAE(mt‚àíi
Œ∏iKŒ∏i‚ÜíŒ∏0+BŒ∏i‚ÜíŒ∏0,mt‚àíi
Œ∏0).(4)
After generating the absent mode ¬Ømt‚àíi
Œ∏0,i‚àà {1,2,3}at
three previous moments by LMM, we use a spatial Ô¨Çow
estimation (SFE) submodule with the hierarchical encoder-
decoder block (EDB) cascaded architecture [ 9,15] to trans-
fer the corresponding modes to current time t, complet-
menting the three absent modes. The structure of the SFE
is shown in Fig. 3(c), we used three EDB cascaded in our
approach. The proposed SFE can be formed as,
SFE(mt‚àí4
Œ∏0,mt‚àí3
Œ∏3,mt‚àí2
Œ∏2,mt‚àí1
Œ∏1,mt
Œ∏0,f) =Flow, (5)
wherefis the modulation frequency, Flow =
[Ft‚àí3‚Üít,Ft‚àí2‚Üít,Ft‚àí1‚Üít]is the optical Ô¨Çows from time
t‚àíi,i‚àà1,2,3tot. Our cross-mode and uni-mode frame-
work is preserved in SFE by feeding the mt‚àí4
Œ∏0into the SFE.
The long-term extra information gives additional character-
istics of motion and texture for better short-term motion es-
timation, which has been proven in video processing [ 8].
At each EDB, the input is downsampled with differ-
ent scale ratios, (i.e., [0.25, 0.5, 1] for EDB0-2). Such a
4932
pyramid-like process can utilize coarse-to-Ô¨Åne bias estima-
tion. As shown in Fig. 3(c), we also use the output of each
block as the input to the next blocks. The MAE loss be-
tween warped measurements and ground truth is used to su-
pervise the optical Ô¨Çow estimation.
LSFE=1
33/summationdisplay
i=1MAE(warp(mt‚àíi
Œ∏i),mt
Œ∏i), (6)
wherewarp(¬∑)is the warping function with optical Ô¨Çow.
3.2. iToF Uni¬≠mode Photometric Correction
Based on the LCTM, the absent modes of time tcould be
transferred from the three previous measurements mt‚àíi
Œ∏j,i‚àà
{1,2,3}. Then, the photometrical inconsistency of the same
modes introduced by the depth-wise motion needs to be cor-
rect for accurate depth estimation. Here, we construct the
iToF uni-mode photometric compensation model to correct
the inconsistency. SpeciÔ¨Åcally, we approximate the iToF
measurement model with its Ô¨Årst-order Taylor expansion,
formulating the relationship between the measurement dis-
turbance to the phase disturbance. Based on this, we build a
Uni-mode Photometric Correction Module (UPCM) to cor-
rect the uni-mode photometric inconsistency from a set of
previous uni-mode measurement disturbances. Some theo-
retical analysis of the iToF uni-mode photometric compen-
sation model and the UPCM is described in the following.
iToF Uni-mode Photometric Compensation Model . For
the iToF measurement with the same demodulation phase
of dynamic scenes, the measurement model is
mŒ∏(u;t) =I(u;t)+A(u;t)cos(œï(u;t)+Œ∏), (7)
whereudenotes the pixel coordinate (x,y).I(u;t)and
A(u;t)represent the environment illumination and scene
reÔ¨Çectance ratio respectively. œï(u;t) =4œÄfd
cis the phase
difference corresponding to the time of Ô¨Çight. dis depth.
cdenotes the light speed. Œ∏‚àà {0‚ó¶,90‚ó¶,180‚ó¶,270‚ó¶}is the
phase of demodulation function.
When measuring a 3D scene, a moving 3D point P(t) =
(X,Y,Z)with a 3D displacement [‚àÜX,‚àÜY,‚àÜZ]fromtto
t+‚àÜtcan be projected to iToF measurement with the pixel
location shift ‚àÜu= (‚àÜx,‚àÜy)and the variation of depth
map‚àÜd. Further, ‚àÜdleds to the change ‚àÜœï,
‚àÜœï=œï(u+‚àÜu;t+‚àÜt)‚àíœï(u;t) =4œÄf‚àÜd
c.(8)
As for the uni-mode cases, although the photometric in-
consistency caused by slight depth-wise motion exists, the
optical Ô¨Çow ‚àÜubetween the uni-mode measurements still
can be estimated with relatively high precison. Besides,
the environment illumination keep constant after warping
with the optical Ô¨Çow, i.e., I(u;t) =I(u+ ‚àÜu;t+ ‚àÜt).
For reÔ¨Çection-dependent parameter A(u;t), considering the
light travel distance and the projected size on sensor, therelationship between A(u;t)and the depth disA(u;t) =
R(u;t)S
d2with the inverse-square law [ 23], whereSis the
amplitude of emitted signal, R(u;t)is reÔ¨Çection rate. When
depth changes, we can get the approximate expression by
Ô¨Årst-order Taylor equation,
‚àÜA(u;t) =‚àí2R(u;t)S
d3‚àÜd, (9)
Considering the fact that d‚â´‚àÜd, we can simplify such
tiny variation and get A(u;t) =A(u+‚àÜu;t+‚àÜt). Then,
the photometric residual could be represented by
mŒ∏i(u+‚àÜu;t+‚àÜt)‚àímŒ∏j(u;t) =
A(u;t)(cos(œï(u;t)+‚àÜœï+Œ∏i)‚àícos(œï(u;t)+Œ∏j)).(10)
For uni-mode with same Œ∏, to simplify the model, we ex-
pand Eq. ( 10) by Ô¨Årst-order Taylor equation as
‚àÜmŒ∏=‚àí‚àÜœïA(u;t)sin(œï(u;t)+Œ∏), (11)
then we build the relationship between the depth-wise mo-
tion caused photometric bias ‚àÜmŒ∏and the phase variation
‚àÜœïwith a linear equation. Considering that the modes
are different as time varying, we can get ‚àÜm(t+‚àÜt)‚Üít
Œ∏i‚Üíj=
mŒ∏i(u+ ‚àÜu;t+ ‚àÜt)‚àímŒ∏j(u;t)with the estimated
optical Ô¨Çow ‚àÜu. According to Eq. ( 10), based on lin-
ear motion assumption of ‚àÜœïwithin 4 alternating frames,
we can get the mapping (œï(u;t),A(u;t),‚àÜt‚àÜœï)‚Üí
‚àÜm(t+‚àÜt)‚Üít
Œ∏i‚Üíj. Based on at least three consecutive pixel-
aligned frames from LCTM, we get unknown independent
variables (œï(u;t),A(u;t),‚àÜœï). According to Eq. ( 11), the
uni-mode photometric compensation variations for depth-
wise motion could be estimated.
In brief conclusion, from the above derivation, we Ô¨Ånd
that from previous alternating 4-mode measurements, the
uni-mode measurement variation for correcting the depth-
wise motion caused photometric residual of the comple-
mented modes at time tcould be recovered. In the follow-
ing, we propose the UPCM network to correct the photo-
metric residual of the absent modes transferred from previ-
ous measurements so that the accurate depth at time tcould
be extracted.
Uni-mode Photometric Correction Module . Here, we in-
troduce the UPCM to compensate the depth-wise motion-
induced photometric residuals. The UPCM module is de-
signed based upon 3D CNN [ 18]. As shown in Fig. 3(d),
our UPCM module uses 3D CNN only for three downsam-
pling and two upsampling, which is much simpler com-
pared to other 3D CNN modules or Unet-like structures.
Based upon the above discussion, we take the previ-
ous consecutive measurements mt‚àí4
Œ∏0,mt‚àí3
Œ∏3,mt‚àí2
Œ∏2,mt‚àí1
Œ∏1,
warped measurements ÀÜmt
Œ∏3,ÀÜmt
Œ∏2,ÀÜmt
Œ∏1and optical Ô¨Çows
Flow from SFE and the intermediate measurements mt
Œ∏0
as the inputs. The UPCM can be presented by
UPCM(mt‚àí4
Œ∏0,mt
Œ∏0,mt‚àíi
Œ∏i,ÀÜmt
Œ∏i,Flow,f ) =Rdp,(12)
4933
1 2 3 4 -0.1 0 0.1
 1 2 3 4 -0.1 0 0.1
 5 6 7 -0.24 0 0.24
 3456 -0.24 0 0.24
RAFT MOM
FFN Ours
RAFT MOM
FFN Ours
RAFT MOM
FFN Ours
RAFT MOM
FFN OursDepth Error Map Depth Error Map Depth Error Map Depth Error MapRAFT MOM FFN Ours Local  Error
(a) Scene#1@20MHz (d) Scene#4@30MHz (c) Scene#3@20MHz (b) Scene#2@20MHzFigure 4. Qualitative comparison. The error map of local details is cropped out and enlarged for visualization below.
wherei‚àà(1,2,3),Rdp= [r1,r2,r3]is the output of
the UPCM, i.e., the estimated depth-dependent photomet-
ric residual (DPR). We add residual to corresponding SFE-
aligned measurements to generate the Ô¨Ånal retrieved mea-
surements at time t. For supervision, we utilize MAE loss
to supervise the DPR estimation,
LUPCM=1
33/summationdisplay
i=1MAE(ÀÜmt
Œ∏i+ri,mt
Œ∏i). (13)
Loss Function . In our method, we train all the modules in
an end-to-end way, and the total loss function is,
Ltotal=LLMM+ŒªSFELSFE+ŒªUPCMLUPCM,(14)
whereŒªSFE andŒªUPCM is the balancing coefÔ¨Åcient and
empirically chosen as 1.4. Experiment
In this section, we Ô¨Årst introduce the proposed supple-
mental dataset. Then, we present the results of the proposed
method in comparison with the State-of-the-Art (SOTA)
methods [ 11,28] and a representative optical Ô¨Çow estima-
tion method RAFT [ 31]. Besides, ablation experiments
are conducted to analyze the effectiveness of the proposed
LCTM, and UPCM modules. Lastly, we further validate the
generalizability of the proposed method to higher speeds
and noisey real-world data.
4.1. Supplement Database
For training, testing and validation, the utilized database
includes both the database proposed in the [ 27,28] and
our proposed database. Our proposed supplement contains
20 LiDAR-scanned real scenes of Matterport3D [ 1] with
complexer background textures compared with the Cornell-
4934
(a) (d) (c) (b)Figure 5. Measurements comparison of our proposed supplement
(a) and (b) with the previous database [ 27,28] (c) and (d).
Box-like scene in previous databases [ 27,28] as shown in
Fig. 5. At least 5 objects of YCB-V dataset [ 34] are im-
ported to each scene at varying distances. Each object has
100 random 6 degrees of freedom (6DoF) moving steps.
Further, modulation frequency that differ from the previ-
ous works [ 27,28], 30 MHz is added. Our supplement
database provides additional features to construct the data-
driven methods. SpeciÔ¨Åcally, we implement the iToF imag-
ing model with ray tracing in Blender [ 7] for simulation.
4.2. Comparisons with State¬≠of¬≠the¬≠Art Methods
In this subsection, we show the comparison of our pro-
posed method with FFN [ 28] and MOM [ 11]. Besides, we
compare our methods with the pre-trained RGB Ô¨Çow net
RAFT [ 31]. We adopt the mean absolute error (MAE) of
depth MAE dand MAE of predicted four mode measure-
ment value MAE pto evaluate the performance. The unit of
depth error in this section is centimeters (cm). We remap all
the measurements to [0,1024] to truncate the overexposure
pixel value before error estimation.
Dataset . For training, validation, and test datasets, 20 MHz
data in [ 27,28] and the proposed 30 MHz data are utilized.
We construct the dynamic scenes by extracting measure-
ments of alternating phases at successive viewpoints corre-
sponding to successive timestamps. SpeciÔ¨Åcally, 124 scenes
at 20MHz and 16 scenes at 30MHz are used for training. 15
scenes at 20MHz and 2 scenes at 30MHz are used for val-
idation. For testing, 15 scenes at 20MHz and 2 scenes at
30MHz are utilized. Data enhancement with randomized
cropping and rotation is used during the training process.
Implementation Details . We train the proposed network
with a batch size of 24 for 60 epochs. Cosine annealing [ 24]
is used to decay the learning rate from 2√ó10‚àí4to2√ó10‚àí6.
For fair comparison, we retrain FFN [ 28] and MOM [ 11]
with the same settings in [ 28] on the same database. All
runtime tests are performed on RTX 2080 Ti. We set the
batch size to 1, record the forward runtime, and average to
the mean runtime in seconds (s). Number of parameters is
recorded in millions (M). The patch spatial resolution for
training, validation, and test is 448√ó448.
Overall Performance Comparison . As the baseline to all
the evaluations, the standard depth estimation (SDE) with
unaligned measurements is demonstrated. The quantita-
tive comparison results are shown in Tab. 2. Our method
achieves the best performance in both photometric and
depth reconstruction. Compared with the SOTA methodMetric SDE RAFT [ 31] FFN [ 28] MOM [ 11] ours
MAEp 7.31 6.96 3.79 6.72 1.51
MAEd 16.72 15.40 7.58 14.28 4.72
Mask rate - 0.82 % 0.80% 0.45% 0%
Para. (M) - 5.26 1.37 9.03 7.75
Time (s) - 0.27 0.32 0.028 0.081
Table 2. Quantitative comparison results.
FNN [ 28], the depth reconstruction error of our method is
reduced by 37 %and the runtime of our method is reduced
to a quarter. The mask rate [ 28] is introduced, which can in-
dicate the number of warping-failed pixels (e.g., out of the
image coordinate plane). As shown, our method presents
the smallest mask rate. Qualitative comparisons are shown
in Fig. 4, and the reconstructed depth maps and error maps
for each method are shown. From the comparison, our
method shows excellent performance in motion compensa-
tion and eliminates most of the artifacts in depth reconstruc-
tion. As for runtime comparison, our method has a distinct
advantage in terms of speed, being on the same order of
magnitude as the best methods MOM [ 11], which demon-
strates the efÔ¨Åciency of our approach. Note that although
our method has a relatively higher number of parameters,
compared to RAFT [ 31] and FFN [ 28], the efÔ¨Åciency of the
algorithm is not affected due to the high degree of paral-
lelism inherent in the proposed network.
4.3. Ablation Study
In this subsection, we conduct ablation experiments
to further demonstrate the effectiveness of the proposed
LCTM, and UPCM. The submodule LMM of LCTM is pro-
posed to predict the LLT maps, which facilitate the estima-
tion of the optical Ô¨Çow. As shown in the Tab. 2, through
comparing the method with solely SFE for optical Ô¨Çow es-
timation, the performance improvement introduced through
combining LMM and SFE demonstrates the efÔ¨Åciency of
introducing LLT-based cross-mode photometric correction.
With SFE and UPCM, the introduction of LMM could sig-
niÔ¨Åcantly improve the performance, further demonstrating
the pivotal role of LMM in the overall framework. As
shown in Fig. 6(a) and (b), the coefÔ¨Åcients KandBpre-
dicted by LMM effectively map the pixel intensity. Further,
we verify the local validity of the linear transformation re-
lationship predicted by LMM. As shown in Fig. 6(f), the
linear mapping relationship between pixel intensities within
the16√ó16sized region marked by the red box can all be
formed by kandbfrom the center pixel. The UPCM is pro-
posed to estimate the photo residual caused by depth-wise
motion. As shown in Tab. 3, UPCM demonstrates signiÔ¨Å-
cant performance gains in the Ô¨Ånal MAE dand MAE p. Note
that we introduce a few 3D CNNs in UPCM, which is the
primary source of computational complexity, and achieve
the performance beneÔ¨Åts from 3D CNNs while not intro-
ducing an excessive computational load.
4935
         
                                 
(a) (b) Mapped (c)
(d) K ÔºàfÔºâLLT Visualization (e) BùëöùúÉ0ùëöùúÉ1ùëöùúÉ1
ùëöùúÉ1
ùëöùúÉ0
Figure 6. LLT visualization. The pixel values of the 16√ó16
region, marked by the red box in (a), are plotted in (f) along with
the LLT predicted by LCTM for the center of that region.
LMM SFE UPCM MAE pMAEdPara.(M) Time (s)
‚úì 1.94 6.24 6.83 0.014
‚úì ‚úì 1.82 5.83 7.54 0.017
‚úì ‚úì 1.74 5.51 7.04 0.073
‚úì ‚úì ‚úì 1.51 4.72 7.75 0.081
Table 3. Ablation experiments of different modules, showing the
effectiveness of the proposed LCTM and UPCM.
In summary, through the sophisticated integration of
LCTM and UPCM based on the iToF Ô¨Çow model, the pro-
posed depth extraction network architecture can efÔ¨Åciently
eliminate the inÔ¨Çuences of mode change and depth-wise
motion, achieving high-precision depth estimation.
4.4. Generalization
Validation of Higher-speed Motion . We Ô¨Årst validate how
the performance of each method changes over different mo-
tion speeds. The FFN [ 28], MOM [ 11], and our method
are trained only on the original database with a maximum
step span of 4 corresponding to the 4-mode measurements
of successive moving step in the simulated measurement se-
quence. We select three scenes corresponding to ego motion
and scene motion at 20 MHz and scene motion at 30 MHz.
The maximum step span in the selected scenes is increased
for simulating different multiplicative speeds, i.e., maxi-
mum step span 5, 6, 7 and 8 for speed ratio 1.25 √ó, 1.5√ó,
1.75√óand 2√ó. As shown in Fig. 7, as the speed increases,
the error of each method becomes larger. Our method main-
tains the best performance in the tests with different speeds,
fully demonstrating the advantages of our method in high-
speed motion scenarios.
Validation of Real-world Data . To further demonstrate
our method, we compare the depth reconstruction perfor-
mance on real-world data. We capture real-world data at
30MHz from the Sony IMX518 with spatial resolution at
QVGA. The exposure time for each measurement is 500
¬µs. As shown in Fig. 8, two scenes are captured, i.e.
the hand gesture and box-throwing scene. The ‚ÄùRaw data‚Äù
     
                                          
   
    
    
     
                                MAEd
MAEp
Speed Ratio1√ó1.25√ó1.5√ó1.75√ó2√ó
Speed Ratio1√ó1.25√ó1.5√ó1.75√ó2√óFigure 7. Effect of the speed on the performance.
19
5Scene#1: Gesture.MOM FFN Ours
7
5
3
Raw dataScene#2: Throwing box.
Figure 8. Performance comparison on real-world data.
image is synthesized by fusing two consecutive measure-
ments, which can visualize the real motion blur and noise.
The results of MOM [ 11], FFN [ 28] and ours are shown
in Fig. 8. For gesture scenes, our method successfully elim-
inates most of the motion blur and realistically restores the
gap between Ô¨Ångers. For box-throwing scenes at a faster
speed, our method locates the position and contour of the
box more accurately than the other methods. This proves
the generalizability of our method.
5. Conclusion
In this paper, we introduced the iToF-Ô¨Çow to model the
raw iToF measurement variation due to the measurement
mode change and 3D motion, which can be categorized
into uni-mode and cross-mode Ô¨Çow. Based on this model,
we proposed the iToF-Ô¨Çow-based depth extraction network,
comprising LCTM and UPCM. With extensive experiments
on both simulated and real-world data, we demonstrate the
efÔ¨Åcacy of the proposed method.
6. Acknowledgements
This work was supported in part by the National Key
Research and Development Program of China under Grant
2022YFA1207200, in part by the NSFC Projects under
Grant 61971465.
4936
References
[1] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niebner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d
data in indoor environments. In 2017 International Confer-
ence on 3D Vision (3DV) , pages 667‚Äì676, 2017. 6
[2] Guanying Chen, Chaofeng Chen, Shi Guo, Zhetong Liang,
Kwan-Yee K Wong, and Lei Zhang. Hdr video reconstruc-
tion: A coarse-to-Ô¨Åne network and a real-world benchmark
dataset. In 2021 IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 2502‚Äì2511, 2021. 2
[3] Yan Chen, Keyuan Qian, Xuanye Cheng, and Jingkun Zhou.
A learning method to optimize depth accuracy and frame
rate for time of Ô¨Çight camera. In 2019 IOP Conference Se-
ries: Materials Science and Engineering , volume 563, page
042067, 2019. 2
[4] Yan Chen, Jimmy Ren, Xuanye Cheng, Keyuan Qian,
Luyang Wang, and Jinwei Gu. Very power efÔ¨Åcient neu-
ral time-of-Ô¨Çight. In 2023 IEEE/CVF Winter Conference on
Applications of Computer Vision (WACV) , pages 2257‚Äì2266,
2020. 2
[5] Zhuo Chen, Peilin Liu, Fei Wen, Jun Wang, and Rendong
Ying. Restoration of motion blur in time-of-Ô¨Çight depth im-
age using data alignment. In 2020 International Conference
on 3D Vision (3DV) , pages 820‚Äì828, 2020. 1,2
[6] Haesoo Chung and Nam Ik Cho. Lan-hdr: Luminance-based
alignment network for high dynamic range video reconstruc-
tion. In 2023 IEEE/CVF International Conference on Com-
puter Vision (ICCV) , pages 12760‚Äì12769, 2023. 2
[7] Blender Online Community. Blender - a 3D modelling and
rendering package . Blender Foundation, Stichting Blender
Foundation, Amsterdam, 2018. 2,7
[8] Duolikun Danier, Fan Zhang, and David Bull. St-mfnet: A
spatio-temporal multi-Ô¨Çow network for frame interpolation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 3521‚Äì3531, 2022. 4
[9] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van
Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
Learning optical Ô¨Çow with convolutional networks. In 2015
IEEE international conference on computer vision (ICCV) ,
pages 2758‚Äì2766, 2015. 4
[10] Jing Gao, Xueqiang Gao, Kaiming Nie, Zhiyuan Gao, and
Jiangtao Xu. A deblurring method for indirect time-of-
Ô¨Çight depth sensor. IEEE Sensors Journal , 23(3):2718‚Äì2726,
2023. 1,2
[11] Qi Guo, Iuri Frosio, Orazio Gallo, Todd Zickler, and Jan
Kautz. Tackling 3d tof artifacts through learning and the Ô¨Çat
dataset. In 2018 Proceedings of the European Conference on
Computer Vision (ECCV) , pages 368‚Äì383, 2018. 1,2,6,7,8
[12] Felipe Gutierrez-Barragan, Huaijin Chen, Mohit Gupta, An-
dreas Velten, and Jinwei Gu. itof2dtof: A robust and
Ô¨Çexible representation for data-driven time-of-Ô¨Çight imag-
ing.IEEE Transactions on Computational Imaging , 7:1205‚Äì
1214, 2021. 2
[13] John K Haas. A history of the unity game engine. 2014. 2[14] Xuemei Hu, Xing Lin, Tao Yue, and Qionghai Dai. Multi-
spectral video acquisition using spectral sweep camera. Op-
tics Express , 27(19):27088‚Äì27102, 2019. 2
[15] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,
Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu-
tion of optical Ô¨Çow estimation with deep networks. In 2017
IEEE conference on computer vision and pattern recognition
(CVPR) , pages 2462‚Äì2470, 2017. 4
[16] Wenzel Jakob, S ¬¥ebastien Speierer, Nicolas Roussel, and De-
lio Vicini. Dr.jit: A just-in-time compiler for differentiable
rendering. IEEE Transactions on Graphics (TOG) , 41(4),
July 2022. 2
[17] Daniel S Jeon, Andr ¬¥eas Meuleman, Seung-Hwan Baek, and
Min H Kim. Polarimetric itof: Measuring high-Ô¨Ådelity depth
through scattering media. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 12353‚Äì12362, 2023. 1
[18] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-
tional neural networks for human action recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence
(TPAMI) , 35(1):221‚Äì231, 2012. 5
[19] Nima Khademi Kalantari and Ravi Ramamoorthi. Deep hdr
video from sequences with alternating exposures. In Com-
puter graphics forum , volume 38, pages 193‚Äì205. Wiley On-
line Library, 2019. 2
[20] Araya Kongpecth, Natavut Kwankeo, and Visuttha Man-
thamkarn. 360 degrees object detection using multiple tof
sensors for educational robot. In 2022 International Con-
ference on Electrical Engineering/Electronics, Computer,
Telecommunications and Information Technology (ECTI-
CON) , pages 1‚Äì4. IEEE, 2022. 1
[21] Seungkyu Lee. Time-of-Ô¨Çight depth camera motion blur
detection and deblurring. IEEE Signal Processing Letters ,
21(6):663‚Äì666, 2014. 1,2
[22] Larry Li et al. Time-of-Ô¨Çight camera‚Äîan introduction. Tech-
nical white paper , (SLOA190B), 2014. 1,2
[23] Chung Ping Liu, Bo Han Cheng, Pei Ling Chen, and
Tsun Ren Jeng. Study of three-dimensional sensing by us-
ing inverse square law. IEEE Transactions on Magnetics ,
47(3):687‚Äì690, 2011. 5
[24] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient
descent with warm restarts. In International Conference on
Learning Representations , 2016. 7
[25] Adithya Pediredla, Ashok Veeraraghavan, and Ioannis
Gkioulekas. Ellipsoidal path connections for time-gated ren-
dering. ACM Transactions on Graphics (TOG) , 38(4):1‚Äì12,
2019. 2
[26] Zhiyuan Pu, Peiyao Guo, M Salman Asif, and Zhan Ma. Ro-
bust high dynamic range (hdr) imaging with complex motion
and parallax. In 2020 Proceedings of the Asian Conference
on Computer Vision (ACCV) , pages 134‚Äì149, 2020. 2
[27] Michael Schelling, Pedro Hermosilla, and Timo Ropinski.
Radu: Ray-aligned depth update convolutions for tof data
denoising. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 671‚Äì680,
2022. 2,6,7
[28] Michael Schelling, Pedro Hermosilla, and Timo Ropinski.
Weakly-supervised optical Ô¨Çow estimation for time-of-Ô¨Çight.
4937
In2023 IEEE/CVF Winter Conference on Applications of
Computer Vision (WACV) , pages 2135‚Äì2144, 2023. 1,2,3,
6,7,8
[29] Lee Streeter. Time-of-Ô¨Çight range image measurement in the
presence of transverse motion using the kalman Ô¨Ålter. IEEE
Transactions on Instrumentation and Measurement (TIM) ,
67(7):1573‚Äì1578, 2018. 1,2
[30] Jan Stuhmer, Sebastian Nowozin, Andrew Fitzgibbon,
Richard Szeliski, Travis Perry, Sunil Acharya, Daniel Cre-
mers, and Jamie Shotton. Model-based tracking at 300hz
using raw time-of-Ô¨Çight observations. In 2015 IEEE Interna-
tional Conference on Computer Vision (ICCV) , pages 3577‚Äì
3585, 2015. 2
[31] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs Ô¨Åeld
transforms for optical Ô¨Çow. In 2020 Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV) , pages 402‚Äì
419, 2020. 6,7
[32] Zachary Teed and Jia Deng. Raft-3d: Scene Ô¨Çow using rigid-
motion embeddings. In 2021 IEEE/CVF conference on com-
puter vision and pattern recognition (CVPR) , pages 8375‚Äì
8384, 2021. 1
[33] Sundar Vedula, Simon Baker, Peter Rander, Robert Collins,
and Takeo Kanade. Three-dimensional scene Ô¨Çow. In 1999
IEEE International Conference on Computer Vision (ICCV) ,
volume 2, pages 722‚Äì729, 1999. 1
[34] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and
Dieter Fox. Posecnn: A convolutional neural network for
6d object pose estimation in cluttered scenes. arXiv preprint
arXiv:1711.00199 , 2017. 7
[35] Augusto Ronchini Ximenes, Preethi Padmanabhan, Myung-
Jae Lee, Yuichiro Yamashita, Dun-Nian Yaung, and Edoardo
Charbon. A modular, direct time-of-Ô¨Çight depth sensor in
45/65-nm 3-d-stacked cmos technology. IEEE Journal of
Solid-State Circuits , 54(11):3203‚Äì3214, 2019. 1
[36] Tao Yang, You Li, Cheng Zhao, Dexin Yao, Guanyin Chen,
Li Sun, Tomas Krajnik, and Zhi Yan. 3d tof lidar in mobile
robotics: A review. arXiv preprint arXiv:2202.11025 , 2022.
1
[37] Tao Yue, Ming-Ting Sun, Zhengyou Zhang, Jinli Suo, and
Qionghai Dai. Deblur a blurred rgb image with a sharp nir
image through local linear mapping. In 2014 IEEE Interna-
tional Conference on Multimedia and Expo (ICME) , pages
1‚Äì6, 2014. 3
[38] Simone Zennaro, Matteo Munaro, Simone Milani, Pietro
Zanuttigh, Andrea Bernardi, Stefano Ghidoni, and Emanuele
Menegatti. Performance evaluation of the 1st and 2nd gen-
eration kinect for multimedia applications. In 2015 IEEE
International Conference on Multimedia and Expo (ICME) ,
pages 1‚Äì6, 2015. 1
4938
