Temporally Consistent Unbalanced Optimal Transport
for Unsupervised Action Segmentation
Ming Xu
Australian National University
mingda.xu@anu.edu.auStephen Gould
Australian National University
stephen.gould@anu.edu.au
Abstract
We propose a novel approach to the action segmentation
task for long, untrimmed videos, based on solving an opti-
mal transport problem. By encoding a temporal consistency
prior into a Gromov-Wasserstein problem, we are able to
decode a temporally consistent segmentation from a noisy
affinity/matching cost matrix between video frames and ac-
tion classes. Unlike previous approaches, our method does
not require knowing the action order for a video to attain
temporal consistency. Furthermore, our resulting (fused)
Gromov-Wasserstein problem can be efficiently solved on
GPUs using a few iterations of projected mirror descent.
We demonstrate the effectiveness of our method in an unsu-
pervised learning setting, where our method is used to gen-
erate pseudo-labels for self-training. We evaluate our seg-
mentation approach and unsupervised learning pipeline on
the Breakfast, 50-Salads, YouTube Instructions and Desk-
top Assembly datasets, yielding state-of-the-art results for
the unsupervised video action segmentation task.
1. Introduction
While action recognition is a well-studied topic in the
area of video understanding, datasets are typically com-
prised of short videos arising from distinctive action cate-
gories with tightly cropped temporal boundaries [16,17]. In
this paper, we study the less-explored setting of segment-
ing long, untrimmed videos of multi-stage activities, where
each video contains multiple actions. A fine grained tempo-
ral understanding of videos is required for this task, which
can be attained in a supervised learning setting given dense,
per-frame annotations. Developing techniques to address
this task in an unsupervised manner will allow large video
collections to be utilized for learning without requiring ex-
pensive frame-level annotations.
The action segmentation task can be formulated as a
dense classification problem, where each frame within a
video is assigned to an action class. Specific to this task,
Figure 1. High-level overview of our action segmentation method,
ASOT. Given a (noisy) cost/affinity matrix between video frames
and actions, ASOT solves an optimal transport (OT) problem to
yield temporally consistent segmentations.
however, is that individual actions occupy temporal blocks
of frames. We call this phenomena the temporal consistency
property. Current approaches in the unsupervised setting do
not provide temporally consistent predictions “out-of-the-
box”, necessitating post-processing methods based on hid-
den Markov models (HMMs) [11, 22, 23, 26, 36, 40, 42] to
decode a segmentation from the learned representations. A
critical assumption for these HMM methods is that the ac-
tion order present within a video is known.
Furthermore, successful approaches to unsupervised
learning for image data based on joint representation learn-
ing and clustering [6, 45] have been adapted to the action
segmentation task [23, 40], yielding state-of-the-art results.
These methods use (regularized) optimal transport (OT) [9]
to generate pseudo-labels for self-training, and jointly learn
a video encoder and a set of action class embeddings. How-
ever, the OT formulation used in [23, 40] does not account
for temporal consistency, which is inherent to the action
segmentation task. In addition, the balanced assignment
assumption is imposed, which yields pseudo-labels that
are uniformly distributed among action classes. We argue
this is unreasonable for action segmentation since datasets
used [2, 21, 35] exhibit long-tailed class distributions [11].
Motivated by these observations, we propose Action
Segmentation Optimal Transport (ASOT), a novel method
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14618
for decoding a temporally consistent segmentation from a
noisy affinity/matching cost matrix between video frames
and action classes. ASOT involves solving a fused, unbal-
anced Gromov-Wasserstein (GW) optimal transport (OT)
problem, which fuses visual information from the cost ma-
trix with a structure-aware GW component that encourages
temporal consistency. Unbalanced OT allows for only a
subset of actions to be represented within a video. Fig. 1
illustrates the mechanics of ASOT.
Finally, we show that ASOT is generally applicable as
a post-processing step for action segmentation pipelines.
Unlike prior HMM-based approaches, ASOT handles order
variations and repeated actions without knowing the action
ordering. While we primarily evaluate ASOT in an unsuper-
vised learning setting, we also show how ASOT is effective
for post-processing of supervised methods. We thoroughly
evaluate ASOT on the unsupervised action segmentation
task, yielding state-of-the-art results. We provide an im-
plementation of ASOT with training and evaluation code1.
2. Related Work
Fully Supervised Action Segmentation. Recent ap-
proaches for the action segmentation task in the fully-
supervised setting have focused on developing appropriate
model architectures. In particular, the temporal convolu-
tional network (TCN) [24,25] is widely used in the literature
since it handles long-term dependencies through a tempo-
ral convolution layer. MS-TCN [13, 28] uses a multi-scale
TCN so videos can be processed at a high temporal res-
olution. Recently, transformer style architectures are also
being investigated [3, 5, 44], yielding promising results. Fi-
nally, some works propose additional segmentation refine-
ment blocks on top of existing architectures [1, 7, 15]. Ad
hoc smoothness losses are commonly included during train-
ing to encourage temporal consistency [7, 13, 14, 44]. We
show how ASOT can be used as a post-processing step to
further improve the performance of MS-TCN [28].
Unsupervised Action Segmentation. In the unsuper-
vised setting, learning by solving a proxy task is a common
approach. Kukleva et al. [22] proposed CTE, which uses
timestamp prediction as a proxy task. The output from in-
termediate network layers is used for the frame embeddings.
UDE [36] and VTE [42] extend CTE with a discriminative
embedding loss and visual reconstruction loss. These meth-
ods have two stages; representations are learned through the
proxy task, and a clustering algorithm is subsequently used
to recover actions. ASAL [26] alternate between represen-
tation learning and HMM parameter estimation using a gen-
eralized EM approach. Notably, TOT [23] and UFSA [40]
1https://github.com/mingu6/action_seg_otare similar to our approach, performing joint representation
learning and clustering using temporal OT.
However, TOT [23] suffers from three limitations: 1)
a balanced assignment assumption is enforced on pseudo-
labels, which we argue is unreasonable for long-tailed ac-
tion class distributions encountered in action segmentation
datasets [11], 2) temporal consistency is not addressed and
3) a single, fixed action ordering for all videos is assumed.
UFSA [40] extends TOT [23] by relaxing 1) and 3) through
two learned modules, an action transcript prediction mod-
ule and a frame to transcript alignment module. In contrast,
our method addresses all three limitations of TOT, han-
dling order variations and repeated actions, by expanding
the OT formulation, with no significant increase in learn-
able parameters or network architecture complexity from
UFSA [40]. Furthermore, TOT [23] and UFSA [40] use
a HMM approach to decode segmentations given a fixed
(TOT) or estimated (UFSA) action ordering. In contrast,
we can use ASOT for both pseudo-labelling and decoding.
We refer the reader to Ding et al. [11] for a comprehen-
sive survey on the temporal action segmentation task.
Optimal Transport for Structured Prediction. Optimal
transport has been used for many alignment tasks in com-
puter vision and machine learning, with too many applica-
tions to be listed here. See Khamis et al. [18] for a recent
survey. Recent examples in computer vision include key-
point matching across image pairs [29, 32], point set regis-
tration [34] and training object detectors [10]. GW OT in
particular, has been used for structured data, e.g., represen-
tation learning on graphs [43] and recently, aligning brains
using fMRI scans [38]. For the first time, we use GW to ex-
ploit problem structure for a segmentation task. Our method
has strong connections to post-processing methods for im-
age segmentation problems, e.g., dense conditional random
fields [20] and bilateral filtering [4], which is worth explor-
ing in more detail in future work.
3. Optimal Transport on Structured Data
Our proposed method, ASOT, formulates the post-
processing step of the temporal action segmentation task
as an optimal transport problem. Specifically, we adopt a
fused, unbalanced, Gromov-Wasserstein (FUGW OT) for-
mulation. In this section, we will provide some background
on optimal transport and specifically, the FUGW problem.
Preliminaries. First, let ⟨A,B⟩:=P
i,jAij, Bijfor
A,B∈Rn×mand let 1nbe a length nvector of ones.
Denote a dataset comprised of Bvideos as D:={Vb}B
b=1.
Each video is assumed to be comprised of Nframes, de-
noted Vb:={Ib
1, . . . ,Ib
N}. Furthermore, let Xb:=
fθ(Vb)∈RN×Ddenote frame-level embeddings for a sin-
14619
gle video, extracted using a deep network parameterized
byθ. Let (learnable) cluster centroids, which represent
actions, be denoted A:= [a1, . . . ,aK]∈RD×K. Let
[n]:={1, . . . , n }denote a discrete set with nelements, and
finally, let ∆K⊂RKbe the K−1dimensional probabil-
ity simplex and ∆N
K⊂RK×Ndenote the Cartesian product
space comprised of Nsuch simplexes.
3.1. Kantorovich Optimal Transport
The classic optimal transport formulation is commonly
known as the Kantorovich (KOT) formulation [37], and we
focus on the discrete setting in this paper. Given histograms
p∈∆nandq∈∆m, and a ground cost Ck∈Rn×m
+ ,
the KOT problem solves for the minimum cost coupling T⋆
between pandq. Specifically, the problem is given by
minimize
T∈Tp,qFKOT(Ck,T):=⟨Ck,T⟩,(1)
where Tp,q:={T∈Rn×m
+|T1m=p,T⊤1n=q}is
commonly referred to as the transportation polytope . The
coupling Tcan be interpreted as a soft assignment between
elements in the support of pandq, namely, two discrete sets
denoted [n]and[m]. In the context of action segmentation,
coupling T∈RN×Kcan be interpreted as an assignment
between frames and actions.
3.2. Gromov-Wasserstein Optimal Transport
Gromov-Wasserstein (GW) optimal transport is an ex-
tension to the Kantorovich formulation, and is used for
comparing histograms defined over incomparable spaces.
Concretely, let (Cv,p)∈Rn×n×∆nand(Ca,q)∈
Rm×m×∆mbe two metric-measure pairs. The distance
matrices, Cv(resp. Ca)fully describe a metric defined over
supports [n](resp. [m]). In the GW setting, there is no met-
ric/cost defined between [n]and[m]. The GW OT prob-
lem [30] replaces the KOT objective function in Eq. 1 with
FGW(Cv,Ca,T):=X
i,k∈[n]
j,l∈[m]L(Cv
ik, Ca
jl)TijTkl,(2)
where L:R×R→Ris a cost function penalizing devi-
ations between distance matrix elements . GW OT is com-
monly used to compare structured objects, e.g., graphs [30].
In our work, we use this GW formulation to encode struc-
tural priors over the transport map desirable for the video
segmentation task (i.e., temporal consistency). This is
achieved by setting CvandCain a particular way, which
will be described in detail in Sec. 4.2.
3.3. Fused GW Optimal Transport
The fused Gromov-Wasserstein (FGW) problem [27,38,
39, 41] combines the KOT and GW OT formulations into asingle optimization problem. The FGW formulation is use-
ful when both a ground cost and a structural prior is avail-
able. FGW OT has been used previously in machine learn-
ing for graph classification and clustering [41] and aligning
fMRI data [38]. Given parameter α∈[0,1], and letting
C:={Ck,Cv,Ca}, the FGW objective is given by
FFGW(C,T):=αFGW(Cv,Ca,T)+(1−α)FKOT(Ck,T).
(3)
For the temporal segmentation problem, the KOT compo-
nent encodes visual similarity between the cluster/action
representations and video frame embeddings. The GW
component on the other hand, encodes desirable structural
properties for resulting segmentations, i.e., temporal consis-
tency. We will describe how parameters Care determined
for action segmentation in Sec. 4.
3.4. Unbalanced Optimal Transport
Recently, there has been interest in unbalanced transport
problems [8, 33, 38], where the constraint for Tto lie in the
transportation polytope Tp,q(often referred to as the bal-
anced assignment property ) is relaxed. This can be achieved
by replacing the marginal constraints on Twith penalty
terms in the objective function. Concretely, in this work,
we will solve the optimization problem given by
minimize
T∈TpFFGW(C,T) +λDKL(T⊤1n∥q),(4)
where D KL(a∥b):=P
iailog(ai/bi)is the Kullback-
Leibler divergence and Tp:={T∈Rn×m
+|T1K=p}
is apartial polytope constraint over the row-sum marginal
ofT. A penalty, weighted by λ > 0, is applied between
the column-sum marginal of Ttoq. We will explain in
Sec. 4.3 why the unbalanced formulation is important for
the temporal segmentation task.
4. Action Segmentation Optimal Transport
In this section, we describe our proposed post-processing
approach ASOT, for the action segmentation problem.
First, we explain how to extract a temporally consistent seg-
mentation from a (noisy) cost matrix between video frames
and actions using optimal transport. These cost matrices
are easily computed using learned video frame and action
embeddings. Second, we introduce the fused, unbalanced
Gromov-Wasserstein OT problem underlying ASOT. The
importance of the unbalanced and GW components for the
action segmentation problem will be described in detail. Fi-
nally, we discuss including entropy regularization, which
allows us to develop a fast, iterative solution based on scal-
ing algorithms [8, 9, 30, 33, 38].
4.1. Optimal Transport for Action Segmentation
Letp=1
N1Nandq=1
K1Kbe histograms over the
set of Nvideo frames and Kactions, denoted [N]and[K],
14620
respectively. An (optimal) coupling T⋆∈RN×Kbetween
[N]and[K]can be interpreted as a (soft) assignment be-
tween video frames and actions. For frame i, we can pre-
dict the corresponding action using j⋆=arg maxjT⋆
ij. Let
Cbe the set of cost matrices for the KOT and GW sub-
problems, which will be defined in Sec. 4.2. We solve the
optimization problem in Eq. 4 for the coupling T⋆.
4.2. Objective Function Formulation
Visual Information. Given frame embeddings Xand ac-
tion embeddings A, we can derive the matching cost matrix
asCk
ij:= 1−x⊤
iaj
∥xi∥2∥aj∥2. This defines the KOT compo-
nent of Eq. 4, and incorporates visual information from the
videos through the frame encoder. However, the KOT com-
ponent alone does not provide temporal consistency.
Structural Priors. The GW component encourages tem-
poral consistency over T. Define the GW cost function as
L(a, b):=ab, radius parameter r∈[0,1]and furthermore,
letCv∈RN×NandCa∈RK×Kbe costs over video
frames and action categories, respectively, given by
Cv
ik:=(
1/r1≤ |i−k| ≤Nr
0 o/w,Ca
jl:=(
0j=l
1o/w.
(5)
This GW formulation penalizes couplings with low tem-
poral consistency. Specifically, assigning temporally adja-
cent frames (within a radius of Nrframes) to different ac-
tions incurs a cost in the GW objective in Eq. 2. To see
this, let TijandTklrepresent two assignment probabilities
with|i−k| ≤Nr,i̸=k(adjacent) and j̸=l(differ-
ent actions). A cost of TijTklis incurred in Eq. 2, since
L(Cp
ik, Cq
jl) = 1 . On the other hand, action changes out-
side of a temporal interval ( |i−k|> Nr ) incur no cost nor
does mapping adjacent frames to the same action ( j=l).
Fig. 2b and 2c illustrates the effect of the GW component.
Finally, note that we can compute Eq. 2 efficiently as
FGW(Cv,Ca,T) =⟨CvTCa,T⟩since the loss L(a, b) =
abcan be factorized, following Peyr ´e et al. [30].
4.3. Importance of Unbalanced Transport
For the temporal action segmentation task, a balanced
assignment necessitates that each video frame is assigned
to an action and furthermore, that each action is represented
equally across the frames of a video. While assigning each
video frame to an action is required for the segmentation
task, not every action may occur within a video. The par-
tial polytope constraint in Sec. 3.4, i.e., T∈ Tp, ensures
that every frame is assigned to an action. However, we do
not force every action to be evenly represented over video
frames. Instead, we regularize the distribution of actions
observed within a video towards uniformity using a KL-
divergence penalty weighted by λ >0. Setting lower values
(a) Affinity matrix 1−Ck.
 (b) ASOT solution.
(c) ASOT w/no GW ( α= 0).
(d) ASOT w/out unbalanced OT.
Figure 2. A raw frame/action affinity matrix in a) is decoded using
ASOT into a temporally consistent segmentation in b). Removing
GW from b) destroys temporal consistency, shown in c). Forcing a
balanced assignment to actions in b) yields temporally consistent,
but unintuitive results, shown in d).
forλallows ASOT to find a segmentation that faithfully
replicate the learned embeddings, whereas a high λem-
phasises that the segmentation is balanced across actions.
Fig. 2b and 2d illustrates balanced and unbalanced OT.
4.4. Fast Numerical Solver for ASOT
We add an entropy regularization term −ϵH(T), where
H(T):=−P
i,jTijlogTijandϵ >0, into the objective
function in Eq. 4. We can solve the FUGW problem using
projected mirror descent similar to [30], which is amenable
for computation on GPUs. Our solver typically converges
within 25 iterations, with each iteration having time com-
plexity O(NK)after exploiting the sparsity structure of Cv
andCa. We provide more details and psuedocode in the
appendix. The largest video we encountered (in the 50Sal-
ads [35] dataset) had N= 16kframes and K= 19 action
classes and takes 26.1ms on an Nvidia RTX 4090 GPU.
5. Unsupervised Learning Pipeline
In this section, we will describe our simple represen-
tation learning pipeline for unsupervised action segmenta-
tion. Core to our method is a self-training approach; we use
ASOT described in Sec. 4 to generate pseudo-labels, which
are then used to train a video frame encoder. Fig. 3 illus-
trates the representation learning pipeline.
Learning Problem. Our unsupervised problem involves
learning the parameters θof a video frame encoder by min-
imizing the cross-entropy (CE) loss between frame/action
embedding similarities and (soft) pseudo-labels computed
by ASOT. Concretely, let Pb∈∆N
Kbe normalized similar-
ities for batch element b, defined element-wise as
Pb
ij:=exp(XbA⊤/τ)ijP
lexp(XbA⊤/τ)il, (6)
14621
Figure 3. The unsupervised training pipeline. Orange are learnable
parameters, and arrows indicate computation/gradient flow.
where τ >0is a temperature scaling parameter. Next, soft-
pseudo labels Tb∈RK×Nare derived from the solution
to the optimization problem in Eq. 4, parameterized using a
KOT cost matrix given by ˆCk=Ck+ρRwhere ρ≥0.
HereRis a temporal prior , introduced in [23], de-
fined element-wise as Rij=|i/N−j/K|.Rregularizes
the coupling towards a banded diagonal and encourages a
canonical ordering of actions across videos. We find set-
tingρ > 0encourages correspondences between adjacent
frames across videos , improving clustering performance.
Our representation learning loss is given by
Ltrain(θ) =−1
BBX
b=1NX
i=1kX
j=1Tb
ijlogPb
ij. (7)
Same as in prior works [6, 23], we apply a stop-gradient
through pseudo-labels Tb. The learnable parameters in-
clude the feature extractor θas well as action embeddings
A. We parameterize fθas a multi-layer perceptron (MLP)
applied per frame, consistent with prior works [22, 23, 26].
6. Experimental Setup
Implementation Details. The encoder MLP has one hid-
den layer and we use the Adam optimizer [19] with a learn-
ing rate of 10−3and weight decay of 10−4. We use k-means
to initialize the action embeddings. During training, we
sample 256 frames randomly from uniformly spaced bins
within each video, similar to [23]. A full description of hy-
perparameter settings is provided in the appendix. We set K
to be equal to the ground truth number of actions per activity
category/dataset, consistent with prior works [22,23,26,40].
Datasets. We briefly describe the four video datasets we
use to evaluate our method. All unsupervised methods are
trained and evaluated on the same set of videos. While
videos are subsampled during training for our method, we
use full videos during testing. For Breakfast and YouTube
Instructions, we train and evaluate our method per activity
category and aggregate results.
• The Breakfast (BF) dataset [21] consists of approx.
1,700 videos, where each video captures a subject
preparing a breakfast item. The dataset is split acrossactivity categories, where each activity category cor-
responds to a particular item (e.g., scrambled egg,
juice). Within each video, multiple actions are ob-
served (e.g., crack egg, pour flour). Breakfast contains
10 activity categories with 48 actions across all activ-
ities, and videos range from a few seconds to several
minutes. We use pre-computed Fisher vector features
from images, consistent with prior works [22, 23, 26].
• The YouTube Instructions (YTI) dataset [2] includes
150 instructional videos belonging to 5 activity cat-
egories. The average video lasts approx. 2 minutes,
and this dataset also has a large number of background
frames. We use pre-computed image features provided
by [2], consistent with prior works [22, 23, 26].
• The 50 Salads (FS) dataset [35] contains 50 videos
of actors performing a cooking activity, totalling 4.5
hours in length. Consistent with prior works, we report
results at two action granularity levels, i.e., Mid with
19 action classes and Eval with 12 action classes. The
Eval level aggregates some actions in the Mid level
into a single action. We use pre-computed features
which were used in prior works [22, 23, 26].
• The Desktop Assembly (DA) dataset [23] includes 76
videos of actors performing an assembly activity. The
activity comprises 22 actions conducted in a fixed or-
der. Each video is approx. 1.5 minutes long, and we
use the pre-computed features provided by [23].
Evaluation Metrics. We follow an evaluation protocol
consistent with prior works on the unsupervised segmen-
tation task [22, 23, 26, 36, 42], which we will now describe.
Since no ground truth action labels are used during training,
we perform Hungarian matching to match learned action
clusters to ground truth actions using frame labels and pre-
dictions. We evaluate methods under two settings, where
the Hungarian matching is performed per video (Unsup.
per) and across the full dataset (Unsup. full). Matching per
video does not require clusters to match to the same ground
truth action across videos and yields more favorable results.
We comprehensively evaluate all methods using mean-
over-frames (MoF), F1-score and mean intersection-over-
union (mIoU). MoF measures the percentage of correct per-
frame predictions and is susceptible to class imbalance is-
sues in a dataset. The F1-score is defined on a per-segment
basis, where a true positive is defined as having over 50%
of the frames in a ground truth segment predicted correctly.
The F1-score is complementary to MoF since large seg-
ments (from dominant classes) are treated a single segment.
Finally, mIoU computes the IoU averaged over all classes,
and is again complementary to both MoF and F1 because
because it explicitly handles class imbalance.
14622
(a) Example videos from the “salat” activity class
within the Breakfast dataset, exhibiting order varia-
tions and repeated actions. Both cases are handled by
ASOT. Actions classes shown are cut fruit ,peel fruit ,
fruit to bowl ,background .
(b) Example videos from YTI (“repot” category). Ac-
tions classes shown are tap pot ,take plant ,loosen
root,put soil ,add top ,water plant ,place plant .
Figure 4. Example action segmentations for videos in the same
activity category with differing action orderings. Different colors
correspond to different actions. Videos of complex, multi-stage
activities can be exhibit markedly different action orderings.
Comparison Methods. We compare our approach
against multiple prior works on unsupervised action
segmentation [22, 23, 26, 36, 40, 42]. Common to all of
these prior approaches is that post-processing involves
solving an inference problem in some underlying hidden
Markov model (HMM). The states in the HMM correspond
to learned action clusters, and observation likelihoods
correspond to frame/cluster similarities. A core assumption
around the HMM is that the action ordering is known for all
videos, which informs the structure of the transition matrix.
All approaches apart from [40] assume a fixed action or-
dering for all videos a priori, whereas [40] estimates the
action ordering per video using an action transcript predic-
tion module. However, our unsupervised learning pipeline
uses ASOT for post-processing and does not need these re-
strictive assumptions or additional learned modules.
We also add a comparison to unsupervised methods
which perform Hungarian matching and evaluate per video
(Unsup. per) [12, 31]. For our method, we can use the
clusters and frame features learned over the full dataset but
evaluate metrics using Hungarian matching per video. TW-
FINCH (TWF) [31] applies clusters time-weighted frame
features within a single video and uses cluster assignments
for segmentation. ABD [12] uses a change detection ap-
proach applied over image similarities computed from ad-
jacent video frames. As discussed previously, per video
matching tends to yield (much) more favorable results.7. Results
7.1. State-of-the-Art Comparison
Our experimental results are presented in Tab. 1. For the
Unsup. (full) setting, our method consistently outperforms
all relevant prior works across all evaluation metrics. For
the Breakfast dataset, our method yields comparable results
to ASAL [26] and the recent joint representation learning
and clustering approach UFSA [40], where UFSA (T) in
Tab. 1 refers to the authors’ full model with a transformer
frame encoder. However, we show major improvements
over UFSA (T) for the FS (Mid) and DA datasets.
We note that our method uses a simple MLP frame en-
coder, whereas UFSA uses a more complex transformer en-
coder, on top of multiple additional learned modules. For a
fairer comparison, we also present results for UFSA with a
comparable MLP encoder, displayed as UFSA (M) in Tab. 1.
Compared to UFSA (M), we show significant improve-
ments for YTI and FS (Eval). Similar to UFSA, we believe
our method’s ability to handle out-of-order alignments dur-
ing segmentation contributes to the improved performance.
Fig. 4 contains qualitative examples where actions are seen
in different orders across videos within a dataset.
For the per video evaluation protocol, our method out-
performs TWF [31] and ABD [12] on the BF and YTI
datasts for MoF and F1, however for BF it performs
poorly on mIoU. We believe this is because our method
is more successfully finding segmentation boundaries for
large classes, at the cost of failing to identify shorter, more
underrepresented actions. In addition, our method appears
to underperform on FS (Mid and Eval) and yield compara-
ble results for DA. We emphasize that the results were opti-
mized for the Unsup. (full) setting, where actions should be
identified correctly across videos. An interesting avenue for
future work is to investigate how to estimate action clusters
using frame features from a single video for our method.
7.2. Ablation Study
In this section, we analyze the effects of various choices
in our learning pipeline and present the results in Table 2.
Temporal prior and k-means. First, we ablate the effect
of initialization of action clusters by switching k-means to
random sampling ( Nok-means ). We find that overall, using
k-means is beneficial because the clusters are initialized to
be more evenly represented within videos, yielding higher
quality pseudo-labels at the start of training. Second, we
remove the temporal prior ( No temp. prior ) described in
Sec. 5. The prior appears to yield a positive effect overall.
Effect of (un)balanced assignment. For ASOT, we first
analyze the effect of forcing a balanced assignment to ac-
tions (Balanced OT), ignoring the unbalanced OT formu-
14623
Breakfast YouTube Instr. 50 Salads (Mid) 50 Salads (Eval) Desktop Ass.
MoF / F1 / mIoU MoF / F1 / mIoU MoF / F1 / mIoU MoF / F1 / mIoU MoF / F1 / mIoU
Per Video TWF [31] 62.7 / 49.8 / 42.3 56.7 / 48.2 / - 66.8 /56.4 /48.7 71.7 / - / - 73.3 / 67.7 /57.7
ABD [12] 64.0 / 52.3 / - 67.2 / 49.2 / - 71.8 / - / - 71.2 / - / - - / - / -
ASOT (Ours) 63.3 /53.5 / 35.9 71.2 /63.3 / 47.8 64.3 / 51.1 / 33.4 64.5 / 58.9 / 33.0 73.4 /68.0 / 47.6
Full CTE∗[22] 41.8 / 26.4 / - 39.0 / 28.3 / - 30.2 / - / - 35.5 / - / - 47.6 / 44.9 / -
CTE†[22] 47.2 / 27.0 / 14.9 35.9 / 28.0 / 9.9 30.1 / 25.5 / 17.9 35.0 / 35.5 / 21.6 - / - / -
VTE [42] 48.1 / - / - - / 29.9 / - 24.2 / - / - 30.6 / - / - - / - / -
UDE [36] 47.4 / 31.9 / - 43.8 / 29.6 / - - / - / - 42.2 / 34.4 / - - / - / -
ASAL [26] 52.5 / 37.9 / - 44.9 / 32.1 / - 34.4 / - / - 39.2 / - / - - / - / -
TOT [23] 47.5 / 31.0 / - 40.6 / 30.0 / - 31.8 / - / - 47.4 / 42.8 / - 56.3 / 51.7 / -
TOT+ [23] 39.0 / 30.3 / - 45.3 / 32.9 / - 34.3 / - / - 44.5 / 48.2 / - 58.1 / 53.4 / -
UFSA (M) [40] - / - / - 43.2 / 30.5 / - - / - / - 47.8 / 34.8 / - - / - / -
UFSA (T) [40] 52.1 / 38.0 / - 49.6 / 32.4 / - 36.7 / 30.4 / - 55.8 / 50.3 / - 65.4 / 63.0 / -
ASOT (Ours) 56.1 /38.3 /18.6 52.9 /35.1 /24.7 46.2 /37.4 /24.9 59.3 /53.6 /30.1 70.4 /68.0 / 45.9
Table 1. Summary of experimental results. For all evaluation metrics, higher is better and bold (respectively, underline ) indicates the
best (respectively second best) performing methods. A “-” indicates that a metric was not reported in the original paper. Note that “Full”
and “Per Video” relate to the evaluation metrics being computed by applying Hungarian matching on a whole dataset and per video basis,
respectively. For CTE [22], ∗and†indicate results reported and reproduced using model checkpoints provided by the authors, respectively.
Results were generated using one run, consistent with prior works, however we provide results for 5 runs for FS and DA in the appendix.
Breakfast YouTube Instr. 50 Salads (Mid) 50 Salads (Eval) Desktop Ass.
MoF / F1 / mIoU MoF / F1 / mIoU MoF / F1 / mIoU MoF / F1 / mIoU MoF / F1 / mIoU
Base 56.1 / 38.3 / 18.6 52.9 / 35.1 / 24.7 46.2 / 37.4 / 24.9 59.3 / 53.6 / 30.1 70.4 / 68.0 / 45.9
Nok-means 57.7 / 36.0 / 17.1 49.5 / 34.1 / 21.3 42.1 / 34.5 / 22.1 55.2 / 52.7 / 28.2 56.8 / 61.7 / 35.6
No temp. prior 48.5 / 24.9 / 16.3 46.9 / 26.2 / 14.6 43.1 / 36.5 / 21.9 59.7 / 54.7 / 26.3 31.4 / 18.7 / 10.6
Balanced OT 29.7 / 29.3 / 17.8 39.4 / 31.4 / 14.6 39.7 / 39.8 / 25.3 35.7 / 41.8 / 24.9 56.5 / 72.7 / 37.8
No GW (train) 34.4 / 25.9 / 14.4 41.1 / 24.9 / 11.7 29.0 / 22.6 / 14.3 35.1 / 38.6 / 22.5 49.5 / 49.4 / 30.4
No GW (train & test) 32.6 / 21.3 / 10.4 38.0 / 21.8 / 11.9 17.3 / 4.2 / 8.9 25.8 / 21.3 / 14.8 45.4 / 42.0 / 26.5
Table 2. Ablation study results for the Unsup (full) setting. Effects are not additive. For all evaluation metrics, higher is better.
lation described in Sec. 4.3. For the BF, YTI and FS
datasets, we observed that dominant classes are common
in the ground truth annotations, and forcing a balanced as-
signment to actions severely degrades performance overall.
However for DA, enforcing a balanced assignment actually
slightly improves performance. This is because the ground
truth actions are (relatively) evenly represented across the
DA dataset. Compare Figures 6b and 4 to Figure 6a to see
class balance discrepancies across datasets.
Effect of GW structural prior. Finally, we remove the
GW problem described in Sec. 3.2 at training time only ( No
GW (train) ), as well as at both training and test time ( No
GW (train & test) ). We observe that removing the temporal
consistency of the pseudo-labels alone is enough to com-
promise the performance of our method. As expected, also
removing the temporal consistency of the test time segmen-
tations further reduces performance.7.3. Sensitivity Analysis
In addition to the ablation studies, we performed a sensi-
tivity analysis on the hyperparameters of our action segmen-
tation OT method during training. These hyperparameters
strongly influence the behavior of the pseudo-labels and re-
sultant learned representations. We illustrate the results in
Fig. 5 and perform this analysis using the desktop assembly
(DA) and 50 Salads (FS) datasets at both the Eval and Mid
granularity, using MoF as the metric.
Effect of λ,randα.As discussed in Sec. 7.2, a (more)
balanced assignment benefits datasets such as DA and FS
(Mid) with a large number of action classes. Unsurpris-
ingly, MoF improves for DA and FS (Mid) for increasing
λ, whereas FS (Eval), which has more dominant action
classes, has an optimal point around λ= 0.11. Further-
more, the GW radius ris positively correlated to the re-
sulting segment length. We can see that a small value of
r= 0.02is enough for temporal consistency in FS (Mid
14624
λMoF
0.00.20.40.60.8
0.100 0.125 0.150 0.175 0.200DA FS (Mid) FS (Eval)(a) Balanc. assign. weight λ
rMoF
00.20.40.60.8
0.02 0.04 0.06 0.08 0.10DA FS (Mid) FS (Eval) (b) GW radius r
αMoF
0.00.20.40.60.8
0.2 0.4 0.6 0.8DA FS (Mid) FS (Eval)
(c) GW weight α
ρMoF
0.00.20.40.60.8
0.10 0.15 0.20 0.25 0.30DA FS (Mid) FS (Eval) (d) Global temporal prior ρ
ε (train)MoF
0.00.20.40.60.8
0.04 0.06 0.08 0.10DA FS (Mid) FS (Eval)
(e) Entropy reg. ϵtrain
KMoF
0.00.20.40.60.8
0.50 0.75 1.00 1.25 1.50DA FS (Mid) FS (Eval) (f) Ratio of learned/true actions
Figure 5. Sensitivity analysis on FUGW OT hyperparameters.
(a) Desktop Assembly
(b) 50 Salads (Eval)
Figure 6. Example segmentations from DA and FS (Eval). Note
the dominant red class in FS, whereas DA is (roughly) balanced.
and Eval) and DA, yielding the best MoF. Finally, MoF is
relatively insensitive to the GW structure weight α, except
at the extremes. We find a value of α∈[0.2,0.5]to work
well across all evaluated datasets.
Effect of ρ,ϵtrainandK.Finally, we observe that higher
values for the global temporal prior weight ρbenefits DA.
We believe this is because a (banded) diagonal coupling is a
reasonable solution for datasets such as DA with relatively
balanced action classes and identical action orderings across
videos. However, FS (Eval and Mid) seem to benefit from
removing the global prior altogether. The pseudo-labels at
initialization appear to be high quality, and do not need ad-
ditional guidance from the global structure.
Furthermore, we observe that the optimal value for en-
tropy regularization for the OT problem, denoted ϵtrain,
should be set between 0.06 and 0.08. Low values for ϵtrainmay result in numerical instability arising from the solver,
while high values may result in incoherent segmentations.
Finally, for datasets large number of classes (DA and FS
Mid), setting number the of clusters Kto be equal to true
number of actions Kgtappears to be optimal. For FS (Eval)
where dominant classes as present, performance actually
improves by setting fewer clusters.
7.4. Post-Processing for Supervised Methods
We show the generality of ASOT as a post-processing
method by applying it to the outputs of the supervised
method MS-TCN++ [28] for the 50 Salads dataset. We use
the recommended multi-scale configuration (MS) with three
refinement layers as well as without refinement layers, i.e.,
single-scale (SS). [28] observed that the primary function of
the refinement layers is to improve the temporal consistency
of the predictions. Our results verify this finding, since SS
with ASOT yields comparable performance with MS. Fur-
thermore, applying ASOT to MS yields significant further
improvements. See the appendix for hyperparameter set-
tings and more details on this experiment.
Accuracy ED F1@10 F1@25 F1@50
MS 83.5 72.6 80.1 77.2 69.4
SS 76.0 36.3 44.8 42.3 34.9
MS + ASOT 83.6 77.6 84.4 83.1 74.9
SS + ASOT 78.3 72.5 80.3 78.5 69.9
Table 3. ED and F1@ Nrefers to the segmental edit distance and
F1 score at overlapping threshold of N%, respectively. Acc. is the
MoF accuracy. Higher is better for all metrics.
8. Discussion and Conclusion
In this paper we present a novel method for the un-
supervised action segmentation task on long, untrimmed
videos. Our post-processing approach ASOT, yields tempo-
rally consistent segmentations without prior knowledge of
the action ordering, required by previous approaches. Fur-
thermore, we show that in the unsupervised setting, ASOT
produces pseudo-labels suitable for self-training. Future
work will investigate the semi and fully-supervised action
segmentation settings, which will require backpropagating
through our ASOT problem. Finally, developing methods
for unsupervised learning over all activity categories for
challenging datasets such as Breakfast is an important direc-
tion for future work, and will require a more sophisticated
learning pipeline than the one proposed in Sec. 5.
Acknowledgements. This work was supported by an
Australian Research Council (ARC) Linkage grant (project
number LP210200931). Ming Xu thanks Akshay Asthana
and Liang Zheng for helpful discussions about this work.
14625
References
[1] Hyemin Ahn and Dongheui Lee. Refining Action Segmenta-
tion With Hierarchical Video Representations. In Int. Conf.
Comput. Vis. , 2021. 2
[2] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal,
Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Un-
supervised Learning From Narrated Instruction Videos. In
IEEE Conf. Comput. Vis. Pattern Recog. , June 2016. 1, 5
[3] Nicolas Aziere and Sinisa Todorovic. Multistage temporal
convolution transformer for action segmentation. Image and
Vision Computing , 2022. 2
[4] Jonathan T. Barron and Ben Poole. The Fast Bilateral Solver.
InEur. Conf. Comput. Vis. , 2016. 2
[5] Nadine Behrmann, S. Alireza Golestaneh, Zico Kolter, Juer-
gen Gall, and Mehdi Noroozi. Unified Fully and Timestamp
Supervised Temporal Action Segmentation via Sequence to
Sequence Translation. In Eur. Conf. Comput. Vis. , 2022. 2
[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-
otr Bojanowski, and Armand Joulin. Unsupervised Learning
of Visual Features by Contrasting Cluster Assignments. In
Adv. Neural Inform. Process. Syst. , volume 33, 2020. 1, 5
[7] Min-Hung Chen, Baopu Li, Yingze Bao, Ghassan AlRegib,
and Zsolt Kira. Action Segmentation With Joint Self-
Supervised Temporal Domain Adaptation. In IEEE Conf.
Comput. Vis. Pattern Recog. , 2020. 2
[8] Lenaic Chizat, Gabriel Peyr ´e, Bernhard Schmitzer, and
Franc ¸ois-Xavier Vialard. Scaling Algorithms for Unbal-
anced Transport Problems. Mathematics of Computation , 87,
2016. 3
[9] Marco Cuturi. Sinkhorn Distances: Lightspeed Computation
of Optimal Transport. In Adv. Neural Inform. Process. Syst. ,
volume 26, 2013. 1, 3
[10] Henri De Plaen, Pierre-Franc ¸ois De Plaen, Johan A. K.
Suykens, Marc Proesmans, Tinne Tuytelaars, and Luc
Van Gool. Unbalanced Optimal Transport: A Unified Frame-
work for Object Detection. In IEEE Conf. Comput. Vis. Pat-
tern Recog. , 2023. 2
[11] Guodong Ding, Fadime Sener, and Angela Yao. Temporal
Action Segmentation: An Analysis of Modern Techniques.
IEEE Trans. Pattern Anal. Mach. Intell. , 2023. 1, 2
[12] Zexing Du, Xue Wang, Guoqing Zhou, and Qing Wang. Fast
and Unsupervised Action Boundary Detection for Action
Segmentation. In IEEE Conf. Comput. Vis. Pattern Recog. ,
2022. 6, 7
[13] Yazan Abu Farha and Jurgen Gall. MS-TCN: Multi-Stage
Temporal Convolutional Network for Action Segmentation.
InIEEE Conf. Comput. Vis. Pattern Recog. , 2019. 2
[14] Dayan Guan, Jiaxing Huang, Aoran Xiao, and Shijian Lu.
Domain Adaptive Video Segmentation via Temporal Con-
sistency Regularization. In Int. Conf. Comput. Vis. , 2021.
2
[15] Yuchi Ishikawa, Seito Kasai, Yoshimitsu Aoki, and Hi-
rokatsu Kataoka. Alleviating Over-segmentation Errors by
Detecting Action Boundaries. In IEEE Wint. Conf. Applic.
Comput. Vis. , 2021. 2
[16] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale VideoClassification with Convolutional Neural Networks. In IEEE
Conf. Comput. Vis. Pattern Recog. , 2014. 1
[17] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman,
and Andrew Zisserman. The Kinetics Human Action Video
Dataset, 2017. 1
[18] Abdelwahed Khamis, Russell Tsuchida, Mohamed Tarek,
Vivien Rolland, and Lars Petersson. Earth Movers in The
Big Data Era: A Review of Optimal Transport in Machine
Learning, 2023. 2
[19] Diederick P Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. In Int. Conf. Learn. Represent. ,
2015. 5
[20] Kr ¨ahenb ¨uhl, Philipp and Koltun, Vladlen. Efficient inference
in fully connected crfs with gaussian edge potentials. In Adv.
Neural Inform. Process. Syst. , 2011. 2
[21] Hilde Kuehne, Ali Arslan, and Thomas Serre. The Language
of Actions: Recovering the Syntax and Semantics of Goal-
Directed Human Activities. In IEEE Conf. Comput. Vis. Pat-
tern Recog. , 2014. 1, 5
[22] Anna Kukleva, Hilde Kuehne, Fadime Sener, and Jurgen
Gall. Unsupervised Learning of Action Classes With Con-
tinuous Temporal Embedding. In IEEE Conf. Comput. Vis.
Pattern Recog. , June 2019. 1, 2, 5, 6, 7, 12
[23] Sateesh Kumar, Sanjay Haresh, Awais Ahmed, Andrey
Konin, M. Zeeshan Zia, and Quoc-Huy Tran. Unsuper-
vised Action Segmentation by Joint Representation Learning
and Online Clustering. In IEEE Conf. Comput. Vis. Pattern
Recog. , June 2022. 1, 2, 5, 6, 7, 12
[24] Colin Lea, Michael D. Flynn, Rene Vidal, Austin Reiter, and
Gregory D. Hager. Temporal Convolutional Networks for
Action Segmentation and Detection. In IEEE Conf. Comput.
Vis. Pattern Recog. , 2017. 2
[25] Peng Lei and Sinisa Todorovic. Temporal Deformable Resid-
ual Networks for Action Segmentation in Videos. In IEEE
Conf. Comput. Vis. Pattern Recog. , 2018. 2
[26] Jun Li and Sinisa Todorovic. Action Shuffle Alternating
Learning for Unsupervised Action Segmentation. In IEEE
Conf. Comput. Vis. Pattern Recog. , June 2021. 1, 2, 5, 6, 7,
12
[27] Mengyu Li, Jun Yu, Hongteng Xu, and Cheng Meng. Ef-
ficient Approximation of Gromov-Wasserstein Distance Us-
ing Importance Sparsification. Journal of Computational and
Graphical Statistics , 2023. 3
[28] Shijie Li, Yazan Abu Farha, Yun Liu, Ming-Ming Cheng,
and Juergen Gall. MS-TCN++: Multi-Stage Temporal Con-
volutional Network for Action Segmentation. IEEE Trans.
Pattern Anal. Mach. Intell. , 2023. 2, 8, 12
[29] Yanbin Liu, Linchao Zhu, Makoto Yamada, and Yi Yang.
Semantic correspondence as an optimal transport problem.
InIEEE Conf. Comput. Vis. Pattern Recog. , 2020. 2
[30] Gabriel Peyr ´e, Marco Cuturi, and Justin Solomon. Gromov-
Wasserstein Averaging of Kernel and Distance Matrices. In
Int. Conf. Mach. Learn. , Jun 2016. 3, 4, 11
[31] Saquib Sarfraz, Naila Murray, Vivek Sharma, Ali Diba, Luc
Van Gool, and Rainer Stiefelhagen. Temporally-Weighted
14626
Hierarchical Clustering for Unsupervised Action Segmenta-
tion. In IEEE Conf. Comput. Vis. Pattern Recog. , 2021. 6,
7
[32] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
and Andrew Rabinovich. SuperGlue: Learning Feature
Matching With Graph Neural Networks. In IEEE Conf. Com-
put. Vis. Pattern Recog. , 2020. 2
[33] Thibault Sejourne, Francois-Xavier Vialard, and Gabriel
Peyr´e. The Unbalanced Gromov Wasserstein Distance:
Conic Formulation and Relaxation. In Adv. Neural Inform.
Process. Syst. , 2021. 3
[34] Zhengyang Shen, Jean Feydy, Peirong Liu, Ariel H Curiale,
Ruben San Jose Estepar, Raul San Jose Estepar, and Marc
Niethammer. Accurate Point Cloud Registration with Robust
Optimal Transport. In Adv. Neural Inform. Process. Syst. ,
2021. 2
[35] Sebastian Stein and Stephen J. McKenna. Combining Em-
bedded Accelerometers with Computer Vision for Recogniz-
ing Food Preparation Activities. In ACM Int. Joi. Conf. Per-
vasive Ubiq. Comp. , 2013. 1, 4, 5
[36] Sirnam Swetha, Hilde Kuehne, Yogesh S Rawat, and
Mubarak Shah. Unsupervised Discriminative Embedding
For Sub-Action Learning in Complex Activities. In IEEE
Int. Conf. Image Process. , 2021. 1, 2, 5, 6, 7, 12
[37] Matthew Thorpe. Introduction to optimal transport, March
2018. 3
[38] Alexis Thual, Quang Huy Tran, Tatiana Zemskova, Nico-
las Courty, R ´emi Flamary, Stanislas Dehaene, and Bertrand
Thirion. Aligning individual brains with Fused Unbalanced
Gromov-Wasserstein. In Adv. Neural Inform. Process. Syst. ,
2022. 2, 3
[39] Vayer Titouan, Nicolas Courty, Romain Tavenard, Chapel
Laetitia, and R ´emi Flamary. Optimal transport for structured
data with application on graphs. In Int. Conf. Mach. Learn. ,
Jun 2019. 3
[40] Quoc-Huy Tran, Ahmed Mehmood, Muhammad Ahmed,
Muhammad Naufil, Anas Zafar, Andrey Konin, and M. Zee-
shan Zia. Permutation-Aware Action Segmentation via Un-
supervised Frame-to-Segment Alignment, 2023. 1, 2, 5, 6,
7, 12
[41] Titouan Vayer, Laetitia Chapel, Remi Flamary, Romain
Tavenard, and Nicolas Courty. Fused Gromov-Wasserstein
Distance for Structured Objects. Algorithms , 13, 2020. 3
[42] Rosaura G. VidalMata, Walter J. Scheirer, Anna Kuk-
leva, David Cox, and Hilde Kuehne. Joint Visual-
Temporal Embedding for Unsupervised Learning of Actions
in Untrimmed Sequences. In IEEE Wint. Conf. Applic. Com-
put. Vis. , 2021. 1, 2, 5, 6, 7, 12
[43] Hongteng Xu, Dixin Luo, Hongyuan Zha, and
Lawrence Carin Duke. Gromov-Wasserstein Learning
for Graph Matching and Node Embedding. In Int. Conf.
Mach. Learn. , 2019. 2
[44] Fangqiu Yi, Hongyu Wen, and Tingting Jiang. Asformer:
Transformer for action segmentation. In Brit. Mach. Vis.
Conf. , 2021. 2
[45] Asano YM., Rupprecht C., and Vedaldi A. Self-labelling via
simultaneous clustering and representation learning. In Int.
Conf. Learn. Represent. , 2020. 1
14627
