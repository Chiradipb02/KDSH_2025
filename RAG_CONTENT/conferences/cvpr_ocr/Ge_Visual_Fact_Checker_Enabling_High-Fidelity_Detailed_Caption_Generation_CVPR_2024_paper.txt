Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation
Yunhao Ge Xiaohui Zeng Jacob Samuel Huffman Tsung-Yi Lin Ming-Yu Liu Yin Cui
NVIDIA
https://research.nvidia.com/labs/dir/vfc/
Ahappylittlegirlisstandinginagreenfield,wearingaplaidshirtandholdingontoastringofpinkballoons.Theballoonsarefloatingintheair,creatingaplayfulandjoyfulatmosphere.Thegirlappearstobeenjoyingthemoment,withasmileonherface.Inthebackground,thereisavastexpanseofgreengrass,stretchingoutasfarastheeyecansee.Theskyaboveisabrightblue,withafewwhitecloudsscatteredabout.Theoverallmoodofthesceneisoneofcarefreehappinessandjoy,withthepinkballoonsaddingatouchofwhimsyandplayfulnesstotheatmosphere.Theimageshowsayounggirlinafieldwithherarmsjoyfullyraised.Sheisholdingabunchofpinkballoonsinherrighthand.She'swearingaplaidshirtandjeans,andherhairisstyledinwhatappearstobeaponytail.Thegirlissmilingwidely,andherposesuggestssheisinmid-celebrationorplay.Thefieldisfilledwithwildflowers,andtreescanbeseeninthebackgroundunderablueskywithsomeclouds.Thesettingappearstobeasunnyday,perfectforoutdooractivities.Theimagecapturesamomentofcarefreechildhoodhappiness.
A3Dmodelofathree-level,intricatelydesignedwoodentowerwithacontrastingblueroofanddoor,standingonawoodenbase.Thetower,brownincolor,resemblesafusionofahouse,atower,andacastle.Attheverytopofthetower,thereisacrescentmoondesign.Theoveralldesignaddsatouchoffantasytothescene.A3Dmodelofasmallwoodentowerwithablueroof.VFC
GPT-4V
Cap3D
VFC
Figure 1. Comparison of VisualFactChecker (VFC) with GPT-4V and Cap3D. VFC can generate high-ﬁdelity detailed captions that closely
match GPT-4V’s quality for 2D images and offer signiﬁcantly more details for 3D objects than Cap3D. VFC used a pre-trained Llama-2 as
the LLM when generating the caption for the above 2D image.
Abstract
Existing automatic captioning methods for visual con-
tent face challenges such as lack of detail, content hallu-
cination, and poor instruction following. In this work, we
propose VisualFactChecker (VFC), a ﬂexible training-free
pipeline that generates high-ﬁdelity and detailed captions
for both 2D images and 3D objects. VFC consists of three
steps: 1) proposal, where image-to-text captioning models
propose multiple initial captions; 2) veriﬁcation, where a
large language model (LLM) utilizes tools such as object
detection and VQA models to fact-check proposed captions;
3) captioning, where an LLM generates the ﬁnal caption by
summarizing caption proposals and the fact check veriﬁca-tion results. In this step, VFC can ﬂexibly generate cap-
tions in various styles following complex instructions. We
conduct comprehensive captioning evaluations using four
metrics: 1) CLIP-Score for image-text similarity; 2) CLIP-
Image-Score for measuring the image-image similarity be-
tween the original and the reconstructed image generated
by a text-to-image model using the caption. 3) human study
on Amazon Mechanical Turk; 4) GPT-4V for ﬁne-grained
evaluation. Evaluation results show that VFC outperforms
state-of-the-art open-sourced captioning methods for 2D
images on the COCO dataset and 3D assets on the Obja-
verse dataset. Our study demonstrates that by combining
open-source models into a pipeline, we can attain caption-
ing capability comparable to proprietary models such as
GPT-4V , despite being over 10 ⇥smaller in model size.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14033
1. Introduction
Image captioning is a pivotal challenge in computer vision
and natural language processing. Its central goal is to en-
capsulate visual data within a textual description, which re-
quires a nuanced understanding of both modalities. The
recent advent of multimodal large language models (MM-
LLMs), such as GPT-4V [ 26], and text-to-image genera-
tion models, such as DALLE-3 [ 3], has marked signiﬁcant
progress in this ﬁeld. These proprietary models could lever-
age expansive human-labeled data and enormous comput-
ing resources to learn to generate detailed and contextually
appropriate image descriptions. On the other hand, exist-
ing open-sourced captioning methods in the community still
face signiﬁcant challenges. Methods such as BLIP-2 [ 17]
and OFA [ 35] often yield overly succinct captions that ne-
glect essential visual information. Conversely, systems like
Mini-GPT4 [ 39], InstructBLIP [ 8], and LLaV A [ 20,21] can
suffer from hallucination, producing long descriptions that
do not align with the actual content of the images.
In light of this, we propose VisualFactChecker (VFC),
a ﬂexible training-free pipeline designed to produce accu-
rate and comprehensive captions for both 2D images and
3D objects. Fig. 1shows examples of captions generated
by VFC and their comparisons with captions generated by
GPT-4V [ 26] and Cap3D [ 23]. Captions generated by VFC
are faithful textural representations of the visual contents.
This can also be veriﬁed by reconstructing images and 3d
objects from captions using state-of-the-art text-to-image
and text-to-3d models, as shown in Fig. 2.
VFC focuses on tackling hallucinations and insufﬁcient
details in generated captions and is structured around three
core components: Proposer , serving as the system’s “eye”,
creating detailed caption proposals as preliminary captions
by using image-to-text captioning models; Large Lan-
guage Model , acting as the “brain”, calling and summariz-
ing information from other components, and leveraging its
advanced generalization capabilities to steer the captioning
process following speciﬁed captioning instructions; Detec-
tor and VQA models , functioning as “tools” utilized by
the LLM for fact-checking caption proposals, ensuring the
ﬁdelity of the ﬁnal generated caption. VFC is versatile and
effectively handles captioning for both 2D images and 3D
objects through a uniﬁed pipeline. Fig. 3shows an overview
of the pipeline. The details of each component and their in-
terplay are explained in Sec. 3.
To comprehensively evaluate the generated captions,
other than leveraging the commonly used CLIP-Score that
primarily gauges the image-caption similarity, we propose
a new metric: the CLIP-Image-Score. This metric assesses
the similarity between the input image and a reconstructed
image created by a text-to-image model from the caption,
offering a complementary measure. Furthermore, we con-
ducted a human study on Amazon Mechanical Turk for cap-tion evaluation. Finally, we also performed a ﬁne-grained
evaluation by asking GPT-4V to compare and judge cap-
tions with detailed reasoning. The combination of CLIP-
Score, CLIP-Image-Score, GPT-4V , and human study pro-
vides a more robust evaluation of captions.
We summarize our main contributions as follows: 1) We
propose VisualFactChecker (VFC), a training-free pipeline
to generate high-ﬁdelity detailed 2D and 3D captions, effec-
tively mitigating the challenge of hallucination in long cap-
tions. (2) CLIP-Image-Score: A novel caption evaluation
metric that measures the similarity between the input image
and a reconstructed image from the caption. (3) Our evalua-
tion shows that VisualFactChecker achieves state-of-the-art
results in 2D and 3D captioning tasks compared with open-
sourced models. (4) Our work shows that using an LLM to
chain open-source models can achieve captioning capability
on par with proprietary models such as GPT-4V .
2. Related Work
2.1. Image Captioning
Image captioning has made signiﬁcant progress with the ad-
vent of deep learning. Pioneering works [ 2,10,14] primar-
ily focus on integrating deep neural networks for enhanced
image understanding and language generation.
Recent strides have been made with the introduction of
Multimodal-Large Language Models (MM-LLMs), which
are trained on extensive vision and language data. The gen-
eral approach involves leveraging a pre-trained large lan-
guage model (LLM) and a vision encoder with a projector
to align with the LLM’s embeddings, thus enhancing vi-
sual understanding. Several models have emerged as sig-
niﬁcant contributors in this domain. BLIP [ 16], BLIP-
2[17], OFA [ 35], Flamingo [ 1], Kosmos-2 [ 27], MiniGPT-
4[39], InstructBLIP [ 8], LLaV A [ 20,21] have demon-
strated impressive performance in single-view image cap-
tioning tasks. However, they exhibit varying limitations.
For instance, BLIP-2 and OFA often generate overly con-
cise captions, while others, like InstructBLIP, can produce
detailed captions that often include inaccurate or hallucina-
tory content. Our method aims to address these limitations
by combining different models into a pipeline via an LLM,
striking a better balance between accuracy and detailedness
in generated captions while mitigating hallucinations.
2.2. Large Language Models for Captioning
Recent advancements in large language models (LLMs) like
GPT-3 [ 5], LAMDA [ 30], PALM [ 7], Llama [ 32], GPT-
4[26] have demonstrated exceptional zero-shot capabilities
in language analysis and summarization tasks. This pro-
ﬁciency has naturally extended to the multimodal domain,
particularly in image-language contexts, where LLMs can
summarize multimodal information in a zero-shot manner.
14034
Input imageDALLE-3 text-to-imageVisualFactChecker (Ours)LLaVA-1.5BLIP-2MVDreamtext-to-3dInput 3D shapeVisualFactChecker (Ours)Cap3D
Figure 2. We use DALLE-3 [ 3] as a text-to-image model to reconstruct 2D images using generated captions from different captioning
methods (BLIP-2, LLaV A-1.5 and ours). Similarly, we use MVDream [ 29] as a text-to-3D model to reconstruct 3D objects using different
3D captions (generated by Cap3D [ 23] and ours). From the results, we can see that the reconstructed images or 3D objects using BLIP-2
or Cap3D captions are less similar than the input ones, suggesting their captions may not contain sufﬁcient information or incorrectly
describe the visual contents; the reconstructed images using LLaV A-1.5 captions contain objects or scenes that are not present in the
original images (top: people in the background, bottom: pedestrians and cars on the street), suggesting there might be hallucinations in
LLaV A-1.5 captions. Images or 3D objects reconstructed using our captions are more similar to the inputs.
Vision-blind LLMs are prominent in multimodal appli-
cations, often utilizing language-only preﬁxes generated by
pre-trained tools. Clipcap [ 25] demonstrates this by us-
ing a continuous embedding as a prompt for a GPT-style
language model, achieving notable performance in single-
viewpoint image captioning. Similarly, Promptcap [ 13]
and PNP-VQA [ 31] leverage natural language prompts with
GPT models to excel in visual question answering.
Recent methods have employed LLMs to generate im-
age captions by summarizing initial captions or keywords
from Vision-Language models. For instance, Socratic mod-
els [37] use a CLIP-based model to extract key tags from
images, followed by GPT-3 with specialized prompts to
create stylized captions. ChatCaptioner [ 38] builds upon
this by integrating ChatGPT and BLIP-2 [ 17] in a conver-
sational approach for question-answering about the image,
and summarizing them into a caption. Visual Clues [ 36]
uses similar tags to generate a paragraph-caption. IC3 [ 6]
and LLM-Fusion [ 4] use LLMs to summarize captions from
existing models augmented with temperature-based sam-
pling. Cap3D [ 24] extends this concept to 3D object.
Our method differentiates itself in two critical ways:
First, we focus on reducing hallucinations in captions by
employing visual grounding tools, such as object detection,
to fact-check captions for enhanced accuracy. Second, our
pipeline can be used for captioning both 2D images and
3D objects. Unlike previous methods that rely on a single
captioning model, we integrate multiple captioning sources
from different models, ensuring a more comprehensive cov-
erage of visual content to generate captions.2.3. Hallucination in MM-LLM
There are two popular topics on the hallucination of MM-
LLMs. (1) Hallucination evaluation: Detection approaches
such as Gunjal et al.[11] train classiﬁcation models to iden-
tify hallucination. They focus on distinguishing between
accurate and hallucinated content. Ground truth comparison
methods [ 18,34] compare model outputs with ground truth
data to detect hallucinations. These techniques assess the
alignment of generated captions with actual image content.
(2) Mitigation Strategies [ 22]: Data optimization methods
such as Liu et al.[19] address hallucination by creating neg-
ative instances in training datasets to reduce model over-
conﬁdence. Iterative generation methods such as Wang et
al.[33] adopt an iterative process for caption generation,
where brief answers are generated in succession and amal-
gamated, aiming to improve accuracy and relevance.
Our VisualFactChecker is a training-free pipeline mit-
igating hallucination in image captioning. Our method
utilizes visual grounding tools for improved accuracy,
thereby actively reducing the hallucination and offering
high-ﬁdelity captions for both 2D images and 3D objects.
3. Visual Fact Checker
This section introduces the key components of Visual-
FactChecker as shown in Fig. 3in detail and explains their
interplay in generating accurate and detailed captions. The
following sections delve into speciﬁcs. First, we detail the
pipeline for 2D image captioning (Sec. 3.1), with Fig. 3
(top) illustrating this process. Then, we explore how our
14035
VQADetectorCaptioner-1Captioner-2InitialCaptionsDetectionResults
Describe this image in detail.Large Language Model
2D Caption
Describe this 3D object in detail.parse the caption and list all objects that could  be detected with an object detection model...Object Checklistparse and modify caption using the results from an object detection model...
Captioner-1Captioner-2InitialCaptionsAnswers
Large Language Model
3D Captionask at most 5 most important and concrete questions that I need to double check...Questionscorrect the descriptionbased on the VQA...distill descriptions about the same 3D object from different camera view into one concise caption...
Multi-view Summary
Figure 3. Pipeline of the VisualFactChecker for captioning 2D images (top) and 3D objects (bottom). The process begins with the input
being captioned by two multimodal captioning models (Captioner-1 and Captioner-2) to generate preliminary captions. These captions are
then veriﬁed using a Large Language Model (LLM) to call object detection (Detector) and VQA models for fact-checking the captions.
Finally, the LLM incorporates all the results and summarizes the ﬁnal caption by following instructions.
approach is adapted for 3D object captioning as shown in
Fig. 3(bottom), underscoring both shared methodologies
and unique aspects relevant to 3D contexts (Sec. 3.2).
3.1. 2D Image Captioning
The caption generation takes three steps: 1) proposal, 2)
veriﬁcation, and 3) captioning. Each step is detailed below.
Proposal : The Proposal step serves as the cornerstone of
the captioning process that generates initial captions. This is
achieved through the utilization of advanced image-to-text
models, speciﬁcally “LLaV A” and “Kosmos2”. These mod-
els are trained on expansive datasets, enabling them to com-
prehend and interpret visual content effectively. By analyz-
ing the input image, they suggest various preliminary cap-
tions, each reﬂecting different facets and interpretations of
the image (Fig. 3top). The rationale behind using multiple
image-to-text multimodal LLMs lies in the complexity of
adequately capturing an image’s essence in a single attempt.
Since an image can be accurately described in numerous
ways, different models bring unique perspectives, thereby
encompassing a broader range of information present in the
image. Although the initial captions proposed may not pos-
sess perfect ﬁdelity, the primary objective at this stage is
to generate captions that are as comprehensive as possible.
Fig.3displays the speciﬁc prompts we used for each step,
with more details in Appendix A.Veriﬁcation and Captioning : The goal of the veriﬁcation
step is to scrutinize and rectify any inaccuracies or hallu-
cinations in the captions during the proposal step. It em-
ploys a combination of a Large Language Model (LLM)
and grounding tools, including an open-vocabulary object
detection model and/or a visual question answering (VQA)
model. Here the LLM can be GPT-4 or Llama2. As shown
in Fig. 3(top), the process involves the following steps:
Step 1: LLM ﬁrst summarizes the initial detailed descrip-
tions from different MM-LLMs into a single, detailed cap-
tion. While this caption is comprehensive, it may not al-
ways be accurate. Step 2: The LLM then analyzes this syn-
thesized caption, identifying all objects that could be veri-
ﬁed by object detection and summarizing an object check-
list. In 2D image captioning, the focus is on eliminating
hallucinations, particularly descriptions of non-existent ob-
jects in the image. Identifying these objects is crucial for
the subsequent veriﬁcation process. Step 3: Taking the ob-
ject checklist as input, an open-vocabulary object detection
model examines candidate objects in the checklist and de-
termines their presence in the image. This step is pivotal in
validating the existence of objects mentioned in the caption,
thus supporting the ﬁdelity of the caption.
After veriﬁcation, we go to the last captioning step:
Based on the object detection results, the LLM revises the
summarized single detailed caption. Each object described
14036
in the caption is cross-checked; if detected, it remains un-
changed, while undetected objects are considered potential
hallucinations and are removed from the caption. This step
results in a ﬁnal caption that is both detailed and reliable.
The underlying assumption is that the detection model,
serving as an object grounding expert, provides more re-
liable results than a general image descriptor.
In the veriﬁcation and captioning steps, the LLM plays a
pivotal role as a “brain”. It starts by parsing the initial cap-
tion and identifying key objects for detailed examination.
The LLM then meticulously assesses whether each object
mentioned actually appears in the image based on detec-
tion results. Following this thorough analysis, it reﬁnes and
revises the initial captions, transforming them into ﬁnal ver-
sions that are both coherent and richly detailed. The LLM
is instrumental in guaranteeing linguistic ﬂuency, ensuring
that the captions not only accurately represent the image but
also maintain the necessary level of detail for high-ﬁdelity
captioning. Moreover, the LLM can follow complex in-
structions to write the captions in a speciﬁed style, such as
a caption that only mentions the foreground objects with-
out mentioning the background. Fig. 3displays the speciﬁc
prompts used for each step.
3.2. 3D Object Captioning
The 3D object captioning process follows a similar struc-
tural pipeline to that of 2D images, with a few key distinc-
tions in certain steps, as depicted in Fig. 3(bottom). In 3D
captioning, an object may present multiple views, each of-
fering unique information. The comprehensive caption for a
3D object is derived by integrating the perspectives from all
these views. For each view, VisualFactChecker is employed
to create a detailed, high-ﬁdelity description. Subsequently,
the LLM (GPT-4 or Llama-2) is used to amalgamate the in-
formation from all views, producing a uniﬁed caption for
the 3D object. In particular, for each view’s captioning,
we have the same three-step approach akin to 2D image
captioning. In the proposal step, LLaV A-1.5 and Instruct-
BLIP are utilized for generating initial detailed descriptions.
We opt out of using Kosmos2 for single 3D objects due to
its less effective performance in providing detailed descrip-
tions, possibly linked to its reliance on an implicit detection
model. Additionally, a slightly modiﬁed prompt is used (see
Fig.3bottom), which incorporates 3D-speciﬁc considera-
tions. In the veriﬁcation and captioning step, we primar-
ily address hallucinations related to the attributes of 3D ob-
jects, such as shape and color. To mitigate these inaccura-
cies, rather than enumerating potential objects, we employ
the LLM to generate ﬁve critical questions that could inﬂu-
ence a text-to-3D generation model in reconstructing the 3D
model. Following this, we utilize VQA models (speciﬁcally
LLaV A-1.5) to respond to these questions based on the in-
put 3D object view image. Subsequently, the LLM amendsCaptioning modelText-to-ImagemodelXCaptionX’CLIP-Image-ScoreCLIP image encoderCLIP image encoder!!"!!!!!×!!!
Figure 4. The CLIP-Image-Score pipeline evaluates caption accu-
racy by encoding an original image Xinto a feature representation
IXusing a CLIP image encoder. A captioning model generates a
caption that is then input into a text-to-image model to reconstruct
an image X0, which is encoded to IX0. The score is computed by
assessing the cosine similarity between IXandIX0, providing a
measure of the caption’s ﬁdelity and hallucination detection.
the initial caption in accordance with the answers provided
by the VQA model. We operate under the assumption that
answering targeted questions results in fewer hallucinations
compared to generating a general description. Once the cap-
tion for each individual view is complete, the LLM synthe-
sizes these multiple perspectives into a singular, compre-
hensive caption for the entire 3D object. The prompts used
for the LLM at each stage are detailed in Appendix A.
4. CLIP-Image-Score
Accurate evaluation of caption correctness and detailed-
ness is paramount in determining the performance of an
image captioning model. Traditional metrics like the
CLIP-Score [ 12] have served as a standard for measuring
the alignment between generated captions and their corre-
sponding images. However, our CLIP-score may lack the
sensitivity needed to detect the speciﬁc issue of hallucina-
tion within captions.
We present the CLIP-Image-Score, an alternative met-
ric speciﬁcally developed to reﬂect the subtleties of cap-
tion quality. This metric is different from CLIP-Score by
introducing an additional reconstruction step. Speciﬁcally,
the CLIP-Image-score evaluates the similarity between the
original image and a reconstructed version of the image
generated by a ﬁxed text-to-image model using the caption
as a prompt. By comparing the raw image to its recon-
structed image, the metric is able to detect discrepancies in-
dicative of hallucination, thus providing a different perspec-
tive of the caption quality assessment. The underlying prin-
ciple of the CLIP-Image-Score is the recognition that mul-
tiple “correct” captions may exist for a single image. How-
ever, it’s only when a caption is both “detail” and “correct”
that the reconstructed image closely resembles the original.
Moreover, any hallucinations present in the caption become
evident in the reconstructed image. Fig. 2presents exam-
ples of such reconstructions. For instance, consider the re-
sults from LLaV A-1.5 shown in the third column. The cap-
14037
tion generated for the ﬁrst image falsely mentions “several
other people in the background”. This error is clearly re-
ﬂected in the image reconstructed by the text-to-image gen-
erator. In essence, comparing the two images indirectly en-
sures alignment between the image and its caption, thereby
providing a complementary method to assess the quality of
the caption than directly comparing the image and caption.
The CLIP-Image-Score evaluation process is depicted in
the following steps:
•Caption Generation : An original image Xis input into
a captioning model, which generates a caption.
•Caption-to-Image Reconstruction : This generated cap-
tion is then used as input for a text-to-image model, which
creates a reconstructed image X0that visually represents
the textual description.
•Raw Image Encoding : The original image Xis pro-
cessed through a CLIP image encoder, translating the vi-
sual content into an encoded representation IX.
•Reconstructed Image Encoding : The reconstructed im-
age is also processed through the CLIP image encoder to
obtain its encoded representation IX0.
•Score Calculation : Finally, the encoded representations
of the original and reconstructed images are compared to
calculate the CLIP-Image-Score. The score is given by
the cosine similarity, which assesses the congruence be-
tween IXandIX0:
CLIP-Image-Score =IX·IX0
kIXk⇥kIX0k(1)
Most notably, CLIP-Image-Score offers a sensitive mea-
sure for detecting hallucinations. In scenarios where the
generated caption includes elements that are not in the orig-
inal image, the reconstructed image will also likely con-
tain these discrepancies. By comparing the original and re-
constructed images, the CLIP-Image-Score can effectively
highlight these differences, offering a clearer insight into
the ﬁdelity and accuracy of the generated caption.
Furthermore, CLIP-Image-Score turns a cross-modality
comparison into a more intuitive comparison in the same
image modality (as shown in Fig. 4). CLIP-Image-Score
represents a new complementary perspective for image cap-
tioning evaluation. By leveraging the capabilities of text-
to-image models and focusing on the congruence between
the original and reconstructed images, it provides an ac-
curate assessment of caption quality, particularly in identi-
fying and measuring hallucinations, thereby enhancing the
overall reliability of caption generation systems.
5. Experiments
This section presents a thorough evaluation of captioning
models across both 2D and 3D visual content, employing a
variety of datasets and methodologies. Table 1provides a
summary of our comprehensive evaluation experiments.Eval Input pairs for evaluation Method Reference
2D Raw image CaptionCLIP-Score Table 2
Human evaluation Fig. 6
CPT4V evaluation Fig. 7
Raw image Image(recon) CLIP-Image-Score Table 2
3DMulti-view (raw) CaptionCLIP-Score Table 3
GPT4V evaluation Fig. 7
Multi-view (raw) Multi-view (recon) CLIP-Image-Score Table 3
Table 1. Summary of evaluation methods and results.
5.1. Overall: CLIP-Score and CLIP-Image-Score
2D image captioning. Dataset : Our evaluation utilized
5,000 COCO test images from the Karpathy split. Baseline
methods : We benchmarked against state-of-the-art caption-
ing models, including BLIP-2 [ 17], InstructBLIP [ 8], and
LLaV A-1.5 [ 20]. The evaluation focused on each model’s
ability to produce accurate, detailed, and coherent captions
that effectively encapsulate the essence of the images. Eval-
uation Metric : We employed two metrics: CLIP-Score
[12] and CLIP-Image-Score (Sec. 4). The CLIP-Score, a
prevalent metric in image caption quality assessment, in-
volves processing the raw image through the CLIP image
encoder and the caption through the CLIP text encoder. The
resultant embeddings are then compared for cosine similar-
ity, with a higher score indicating greater semantic resem-
blance between the image and the caption. For our analysis,
we ﬁrst calculated the CLIP-Score for each image-caption
pair, then averaged these scores across all 50,000 text/image
pairs, scaling the result by a factor of 100. Table 2displays
the comparative performance of various image captioning
methods on the 5,000 COCO test set images. The results
demonstrate that our VisualFactChecker surpasses all base-
line methods in performance.
Captioning Method CLIP-Score (%) "CLIP-Image-Score (%) "
Human Label (COCO GT) 30.36 (-2.54) 71.21 (-2.40)
BLIP2 30.11 (-2.79) 70.79 (-2.82)
InstructBLIP 31.45 (-1.45) 72.95 (-0.66)
LLaV A-1.5 32.08 (-0.82) 73.24 (-0.37)
Kosmos-2 32.32 (-0.58) 73.28 (-0.33)
VisualFactChecker (Ours) 32.90 73.61
Table 2. Image captioning comparison with different metrics on
5000 COCO test set in Karpathy split, we use raw image and cap-
tion as input pairs for evaluation.
As outlined in Sec. 4, the CLIP-Image-Score provides
a complementary view to assess the quality of image cap-
tions. This metric is derived by comparing the cosine sim-
ilarity between the CLIP embeddings of two images: the
original image and a reconstructed image, which is gen-
erated using the provided caption through a text-to-image
generation model. A higher CLIP-Image-Score signiﬁes a
more accurate and effective image caption. For this process,
Stable Diffusion XL (SDXL) [ 28] is utilized as the desig-
nated text-to-image model to reconstruct images based on
14038
Captioning Method CLIP-Score (%)"CLIP-Image-Score (%)"Cap3D 33.44(-0.57)79.88(-0.44)VisualFactChecker (Ours)34.01 80.32Table 3. 3D object captioning comparison with different metrics
on 1000 objects in Objaverse. For CLIP-Score, we use the aver-
age score of two views for evaluation. For CLIP-Image-Score, we
use an off-the-shelf text-to-3D model, MVDream, to generate 3D
models from 3D captions. We compare two views of the raw ob-
ject and the same views of generated 3D object for evaluation.
the generated captions. Table 2presents the CLIP-Image-
Scores obtained for the 5000 images in the COCO test set,
where our method outperforms all baseline methods.
3D object captioning. Dataset : 1,000 3D objects sam-
pled from Objaverse dataset [ 9].Baseline methods :W e
use state-of-the-art 3D object captioning model Cap3D [ 23]
as the baseline. Cap3D uses 8 view images to generate
the ﬁnal object caption, our VisualFactChecker uses only
2 views to generate the object caption. Evaluation Met-
ric: CLIP-Score and CLIP-Image-Score on multiple views
rendered from 3D objects. To evaluate the similarity of a
3D object and the generated caption, we evaluate the sim-
ilarity of the caption with the multi-view images used to
generate the caption. Speciﬁcally, we evaluate the similar-
ity of the generated caption with the two views that were
used to generate the caption and use the average score to
represent the CLIP-Score. Table. 3shows the performance
of 3D object captioning methods on 1,000 3D objects from
Objaverse dataset. VisualFactChecker outperforms Cap3D.
We also use CLIP-Image-Score to evaluate the 3D cap-
tion quality. CLIP-Image-Score needs reconstructed images
to compare with the raw images. We treat the two views that
were used to generate the 3D object caption as the raw im-
age. To obtain the reconstructed image, we use an off-the-
shelf text-to-3D generation model, MVDream, to generate
a 3D object given the generated 3D object caption. We then
render the same two views of images based on the generated
3D object, and we calculate the CLIP-Image-Score between
the raw image and the rendered image. Table. 3shows the
CLIP-Image-Score on 1000 objects in Objaverse dataset.
5.2. Per Image Evaluation: Wining Rate
CLIP-Score and CLIP-Image-Score indicate an overall
performance comparison, which shows an average score
among all 5000 images. The average score may be dom-
inated by a small group of images that have extremely high
or low scores. To zoom in and show a more detailed com-
parison, we try to answer the following question: Given an
image, what is the probability that one method performs
better than another method on caption generation? To an-
swer this question, we need to go over each image and cal-
culate the winning rate for a pair of methods.
Speciﬁcally, for each image, we compare the CLIP-
53.153.454.664.260.762.064.470.781.177.2Figure 5. 2D image captioning comparison with pair-wise winning
rate. VisualFactChecker ( VFC ) outperforms all baseline methods
on both CLIP-Score (top) and CLIP-Image-Score (bottom).
Score of our VisualFactChecker caption against the captions
generated from different baselines respectively, and calcu-
late the wining probability of our method and the baselines.
Fig.5shows the results, for example, we can see that in the
pair-wise comparison, our VisualFactChecker performs bet-
ter (higher CLIP-Score) than LLaV A-1.5 on 64.4% of 5000
images (3220 images).
Calculating the winning rate over all images provides a
more detailed analysis that zooms in on the comparison of
each image, which shows a complementary view than over-
all average CLIP-Score.
5.3. Fine-grained Evaluation: Human and GPT-4V
The CLIP-Score and CLIP-Image-Score offer a general
comparison of overall performance. A pairwise per-image
winning rate provides a more speciﬁc analysis, evaluating
performance on individual images. However, the research
highlighted in related studies [ 15] indicates that the CLIP-
Score may not be ideally suited for image-to-image com-
parison tasks. Furthermore, relying on a single score fails
to provide a nuanced comparison across criteria, such as ac-
curacy and level of detail. We use Human evaluation and
GPT-4V to provide a more ﬁne-grained evaluation.
Human evaluation using Amazon Mechanical Turk
(AMT). We employed a pairwise comparison strategy.
From the COCO dataset, we randomly selected 100 images
out of 5000. For each image, our caption was compared
against 5 baseline captions respectively. To reduce variance,
each comparison was done by 3 different AMT workers and
we used their majority voting as the ﬁnal selection. This re-
sulted in a total of 1500 comparisons collected on AMT.
AMT UI is shown in the appendix. The workers were pre-
sented with two competing captions — one from a baseline
method and one from our VisualFactChecker, in random-
ized order. They were instructed to select the better caption
describing the image based on 3 aspects: correctness, de-
14039
tailness, and ﬂuency. Results in Fig. 6show our captions
are more preferred by humans. The human evaluation in-
struction and web UI is shown in Appendix B.
10069585892
Figure 6. Amazon Mechanical Turk human evaluation results.
GPT-4V evaluation. Our study applied GPT-4V for eval-
uating captions in a manner akin to the caption evaluation
process used in DALLE-3. We use the same randomly se-
lected 100 images from COCO as in Human evaluation.
For each image, we considered the captions generated by 5
baseline methods alongside the caption produced by our Vi-
sualFactChecker. We then presented GPT-4V with the raw
image, our reference caption, and the four baseline captions.
Our designed prompt instructed GPT-4V to compare each
baseline caption against our reference caption, focusing on
two primary aspects: correctness and detail. GPT-4V was
tasked with providing a pairwise, detailed comparison for
each pair, including justiﬁcations for its assessments. Based
on these comparative insights, GPT-4V classiﬁed each base-
line method caption as either “better” or “worse” than our
VisualFactChecker. Fig. 5shows the comprehensive results.
More details about the GPT-4V evaluation prompt and ex-
amples are shown in Appendix B.
9468898710098Figure 7. GPT-4V evaluation results. Our captions are signiﬁ-
cantly better than baselines.
5.4. Ablation Study
In our ablation study, we explore the impact of various com-
ponents on performance. For 2D captioning tasks, we as-
sess the efﬁcacy of initial captioning models, LLaV A-1.5
and Kosmos-2, using the CLIP-Score metric for the cap-
tions they generate on the same 5000 COCO test images.
Additionally, we ablate our method’s performance in the
absence of the veriﬁcation (fact checker) step, which aims
to mitigate hallucinations through detection grounding. Ta-
ble4shows the detailed results. Likewise, in the context of
3D object captioning, we evaluate the individual contribu-Methods or Steps CLIP-Score
2DLLaV A-1.5 32.08 (-0.33)
Kosmos-2 32.32 (-0.09)
VisualFactChecker (w/o fact check) 32.41
VisualFactChecker 32.90 (+0.49)
3DLLaV A-1.5 32.05 (-0.66)
InstructBLIP 32.51 (-0.20)
VisualFactChecker (w/o fact check) 32.71
VisualFactChecker 34.01 (+1.30)
Table 4. Ablation study on captioning 2D images (5000 COCO
test dataset) and 3D objects (1000 Objaverse).
tions of initial captioners, namely LLaV A-1.5 and Instruct-
BLIP on the same 1000 Objaverse 3D objects. We further
investigate the performance of our methodology without the
fact checker, which in this case operates by leveraging a
VQA model to reduce hallucinations. Table 4shows the
detailed results. These results highlight the signiﬁcance of
fact checker in our approach.
5.5. Qualitative Results and Prompt Following
Other than quantitative evaluation results, we show more
qualitative examples of VisualFactChecker for 2D and 3D
captions in Appendix C.
By leveraging an LLM, VisualFactChecker can follow
complex instructions to write captions in various styles. Ex-
amples are shown in Appendix D.
6. Conclusion
We propose the VisualFactChecker (VFC), a training-free
pipeline to generate high-ﬁdelity and detailed captions. By
utilizing an LLM to chain multimodal models and object
detection and VQA models, VFC reduces hallucination in
long captions. We conducted a comprehensive caption eval-
uation using different metrics, including 1) image-text simi-
larity using CLIP-Score, 2) image-reconstructed image sim-
ilarity using our proposed CLIP-Image-Score, 3) human
study, and 4) ﬁne-grained evaluation using GPT-4V . Com-
pared with open-sourced captioning models, our method
achieves state-of-the-art in both 2D and 3D captioning. Our
work shows combining open-sourced models into a pipeline
can signiﬁcantly close the captioning performance gap with
proprietary models like GPT-4V . In the future, we plan to
improve our pipeline further by including more components
for fact-checking and making it more automatic in deciding
which components to use.
Acknowledgments We would like to thank Siddharth Gu-
rurani for helping with our human evaluation using Ama-
zon Mechanical Turk; Haochen Wang for his help in pre-
processing 3D data. We also thank Qinsheng Zhang, Yo-
gesh Balaji, and Yen-Chen Lin for their helpful discussion.
14040
References
[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. In NeurIPS ,
2022. 2
[2]Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and
visual question answering. In CVPR , 2018. 2
[3]James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng
Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,
Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,
and Yunxin Jiao. Improving image generation with better
captions. Technical report, OpenAI, 2023. 2,3,11
[4]Simone Bianco, Luigi Celona, Marco Donzella, and Paolo
Napoletano. Improving image captioning descriptive-
ness by ranking and llm-based fusion. arXiv preprint
arXiv:2306.11593 , 2023. 3
[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. In NeurIPS , 2020. 2
[6]David M Chan, Austin Myers, Sudheendra Vijaya-
narasimhan, David A Ross, and John Canny. Ic3: Im-
age captioning by committee consensus. arXiv preprint
arXiv:2302.01328 , 2023. 3
[7]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311 , 2022. 2
[8]Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning.
arXiv preprint arXiv:2305.06500 , 2023. 2,6
[9]Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:
A universe of annotated 3d objects. In CVPR , 2023. 7
[10] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,
Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko,
and Trevor Darrell. Long-term recurrent convolutional net-
works for visual recognition and description. In CVPR , 2015.
2
[11] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and
preventing hallucinations in large vision language models.
arXiv preprint arXiv:2308.06394 , 2023. 3
[12] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
and Yejin Choi. Clipscore: A reference-free evaluation met-
ric for image captioning. arXiv preprint arXiv:2104.08718 ,
2021. 5,6
[13] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A
Smith, and Jiebo Luo. Promptcap: Prompt-guided task-
aware image captioning. arXiv preprint arXiv:2211.09699 ,
2022. 3[14] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei.
Attention on attention for image captioning. In ICCV , 2019.
2
[15] Klemen Kotar, Stephen Tian, Hong-Xing Yu, Daniel LK
Yamins, and Jiajun Wu. Are these the same apple? com-
paring images based on object intrinsics. arXiv preprint
arXiv:2311.00750 , 2023. 7
[16] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uniﬁed
vision-language understanding and generation. In ICML ,
2022. 2
[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 2,3,6
[18] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, and Ji-Rong Wen. Evaluating object hallucina-
tion in large vision-language models. arXiv preprint
arXiv:2305.10355 , 2023. 3
[19] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser
Yacoob, and Lijuan Wang. Aligning large multi-modal
model with robust instruction tuning. arXiv preprint
arXiv:2306.14565 , 2023. 3
[20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. arXiv
preprint arXiv:2310.03744 , 2023. 2,6
[21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 2
[22] Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo,
Yawen Zhang, Baochen Sun, Carl Yang, and Jie Yang. Eval-
uation and mitigation of agnosia in multimodal large lan-
guage models. arXiv preprint arXiv:2309.04041 , 2023. 3
[23] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-
son. Scalable 3d captioning with pretrained models. In
NeurIPS , 2023. 2,3,7,17
[24] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-
son. Scalable 3d captioning with pretrained models. arXiv
preprint arXiv:2306.07279 , 2023. 3
[25] Ron Mokady, Amir Hertz, and Amit H Bermano. Clip-
cap: Clip preﬁx for image captioning. arXiv preprint
arXiv:2111.09734 , 2021. 3
[26] OpenAI. Gpt-4 technical report, 2023. 2
[27] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-
ing multimodal large language models to the world. arXiv
preprint arXiv:2306.14824 , 2023. 2
[28] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna, and
Robin Rombach. Sdxl: Improving latent diffusion mod-
els for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952 , 2023. 6
[29] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,
and Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-
eration. arXiv preprint arXiv:2308.16512 , 2023. 3,17
[30] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia
14041
Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda:
Language models for dialog applications. arXiv preprint
arXiv:2201.08239 , 2022. 2
[31] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio
Savarese, and Steven CH Hoi. Plug-and-play vqa: Zero-shot
vqa by conjoining large pretrained models with zero training.
arXiv preprint arXiv:2210.08773 , 2022. 3
[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efﬁcient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 2
[33] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong,
Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang,
et al. Vigc: Visual instruction generation and correction.
arXiv preprint arXiv:2308.12714 , 2023. 3
[34] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi,
Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji
Zhang, Jihua Zhu, et al. Evaluation and analysis of hal-
lucination in large vision-language models. arXiv preprint
arXiv:2308.15126 , 2023. 3
[35] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. Ofa: Unifying architectures, tasks, and
modalities through a simple sequence-to-sequence learning
framework. In ICML , 2022. 2
[36] Yujia Xie, Luowei Zhou, Xiyang Dai, Lu Yuan, Nguyen
Bach, Ce Liu, and Michael Zeng. Visual clues: Bridging
vision and language foundations for image paragraph cap-
tioning. In NeurIPS , 2022. 3
[37] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choro-
manski, Adrian Wong, Stefan Welker, Federico Tombari,
Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. So-
cratic models: Composing zero-shot multimodal reasoning
with language. arXiv preprint arXiv:2204.00598 , 2022. 3
[38] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen,
Wenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks,
blip-2 answers: Automatic questioning towards enriched vi-
sual descriptions. arXiv preprint arXiv:2303.06594 , 2023.
3
[39] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understanding with advanced large language models. arXiv
preprint arXiv:2304.10592 , 2023. 2
14042
