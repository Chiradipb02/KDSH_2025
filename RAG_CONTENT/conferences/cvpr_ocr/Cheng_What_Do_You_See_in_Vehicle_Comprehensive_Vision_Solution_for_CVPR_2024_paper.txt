What Do Y ou See in Vehicle? Comprehensive Vision Solution
for In-Vehicle Gaze Estimation
Yihua Cheng1Yaning Zhu2Zongji Wang3Hongquan Hao4Y ongwei Liu4
Shiqing Cheng4Xi Wang4Hyung Jin Chang1
University of Birmingham1, Huazhong University of Science and Technology2
NIST, Chinese Academy of Sciences3, CalmCar4
y.cheng.2@bham.ac.uk, h.j.chang@bham.ac.uk
Abstract
Driver’s eye gaze holds a wealth of cognitive and inten-
tional cues crucial for intelligent vehicles. Despite its sig-
niﬁcance, research on in-vehicle gaze estimation remainslimited due to the scarcity of comprehensive and well-annotated datasets in real driving scenarios. In this pa-per , we present three novel elements to advance in-vehiclegaze research. Firstly, we introduce IVGaze, a pioneering
dataset capturing in-vehicle gaze, collected from 125 sub-
jects and covering a large range of gaze and head poseswithin vehicles. In this dataset, we propose a new vision-based solution for in-vehicle gaze collection, introducinga reﬁned gaze target calibration method to tackle annota-tion challenges. Second, our research focuses on in-vehicle
gaze estimation leveraging the IVGaze. In-vehicle face im-
ages often suffer from low resolution, prompting our in-troduction of a gaze pyramid transformer that leveragestransformer-based multilevel features integration. Expand-ing upon this, we introduce the dual-stream gaze pyramidtransformer (GazeDPTR). Employing perspective transfor-
mation, we rotate virtual cameras to normalize images, uti-
lizing camera pose to merge normalized and original im-ages for accurate gaze estimation. GazeDPTR shows state-of-the-art performance on the IVGaze dataset. Thirdly,we explore a novel strategy for gaze zone classiﬁcation
by extending the GazeDPTR. A foundational tri-plane and
project gaze onto these planes are newly deﬁned. Leverag-ing both positional features from the projection points andvisual attributes from images, we achieve superior perfor-mance compared to relying solely on visual features, sub-stantiating the advantage of gaze estimation. The project is
available at https://yihua.zone/work/ivgaze .
1. Introduction
Understanding driver intention and behavior based on driver
gaze is in high demand in intelligent vehicles, facilitating
Figure 1. In-vehicle gaze estimation illustration. The driver’s gaze
direction is estimated based on the facial images captured by thecamera behind the steering wheels.
diverse applications such as in-vehicle interaction [ 1,2,25]
and driver monitor systems [ 17,21,22]. Recent advances in
vehicle gaze estimation concentrate primarily on gaze zoneestimation [ 16,18,31,32]. These approaches deﬁne mul-
tiple coarse regions, such as side mirrors and windshields,
and conduct classiﬁcation based on face images.
Gaze estimation
1serves as an upstream task of gaze zone
estimation and can offer more precise information to under-stand driver attention. However, these methods typicallyrequire a large-scale dataset for training. Although thereare numerous gaze datasets, collected in indoor [ 39,40]o r
outdoor [ 20] environments, their applicability to the vehi-
cle environment is limited due to the different environmentsand camera settings, resulting in suboptimal performance.Creating an in-vehicle gaze dataset proves challenging dueto the conﬁned and irregular nature of the vehicular environ-ment. Constructing in-vehicle gaze collection systems re-mains an unsolved issue, as traditional gaze collection sys-tems are impractical for use within vehicles. The absenceof in-vehicle gaze datasets acts as a signiﬁcant barrier to theprogress of in-vehicle gaze estimation.
In this paper, we present a comprehensive vision-based
1Our work focuses on gaze direction estimation. We abbreviate gaze
direction as gaze in the rest.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1556
in-vehicle gaze estimation research: a novel vision-based
gaze collection system for vehicles, offering a ﬁrst-of-its-kind in-vehicle gaze dataset, a dual-stream gaze pyramidtransformer for accurate in-vehicle gaze estimation, and itsextension to gaze zone classiﬁcation, showcasing its effec-tiveness in enhancing gaze estimation.
First, we introduce IVGaze, an in-vehicle gaze dataset
collected from 125 subjects. IVGaze provides a dense dis-
tribution of gaze directions, covering a wide range within
in-vehicle environments. It contains various conditions, in-cluding diverse head poses, eye movements, illuminationvariations, and the presence of face accessories such asglasses, sunglasses, and masks. To collect IVGaze, we pro-pose a vision-based gaze collection system. The system
does not require dedicated eye-tracking devices and is easy
to reproduce. We use a single camera for facial appearancecapture and paste stickers in vehicles as gaze targets. How-ever, a signiﬁcant challenge lies in calibrating the 3D posi-tion of gaze targets. This challenge arises because gaze tar-gets are out of the camera’s ﬁeld of view (out-of-FoV). Toaddress this issue, we present a reﬁned gaze target calibra-tion method, which leverages an auxiliary camera to capturegaze targets and a transparent chessboard for calibration.
Secondly, we explore in-vehicle gaze estimation using
the IVGaze. Face images often suffer from low resolutiondue to the inherent limitations of cameras in vehicles. Wepropose a gaze pyramid transformer that utilizes a trans-former to integrate multilevel features. Expanding uponthis, we propose a dual-stream gaze pyramid transformer(GazeDPTR). We rotate virtual cameras via perspectivetransformation to normalize images, and leverage camerapose to merge normalized and original images. GazeDPTRshows state-of-the-art results on IVGaze.
Thirdly, we extend GazeDPTR for the downstream gaze
zone classiﬁcation task. It is challenging to compute theintersection of gaze and the vehicle. We deﬁne a founda-tional tri-plane and project gaze to the tri-plane. We ex-tract positional features from intersection points and predictgaze zones using both positional features and visual fea-tures from face images. Our experiment demonstrates thatthe gaze zone classiﬁcation can be further enhanced by po-sitional features, showing the advantage of gaze estimation.
2. Related Works
2.1. Gaze Data Collection
The human gaze is inherently implicit and poses a chal-lenge for objective measurement, making gaze annotationcomplex in gaze data collection. Some methods capture thehuman gaze through intrusive devices such as eye-trackingglasses [ 13,19]. However, the eye-tracking glasses have
a notable impact on the quality of the captured facial im-ages. Fischer et al .[13] attempted to mitigate this impactTable 1. The comparison of gaze estimation datasets. Gaze anno-
tation is challenging in the vehicle environment, which results inexisting in-vehicle datasets only providing gaze zone annotation.Our work addresses this issue and contributes the ﬁrst in-vehiclegaze dataset containing gaze annotation and natural face images.
Datasets Env. #Sub.Annotation
Gaze Zone
Gaze360 [ 20] Outdoor 238 /enc-33 -
MPIIGaze [ 39]
Indoor15 /enc-33 -
EyeDiap [ 15]1 6 /enc-33 -
EVE [ 27]5 4 /enc-33 -
ETH-XGaze [ 40] 110 /enc-33 -
V ora et al .[31]
V ehicle10 /enc-37/enc-33
Jha & Busso [ 18]1 6 /enc-37/enc-33
Wang et al .[32]3 /enc-37/enc-33
Rangesh et al .[28]1 3 /enc-37/enc-33
Ghosh et al .[16] 338 /enc-37/enc-33
IVGaze (Ours) 125 /enc-33/enc-33
using GAN, but there are also some artifacts in the resulting
face images. Vision-based gaze collection systems typicallydeﬁne gaze as direction vectors originating from facial cen-ters towards speciﬁc gaze targets [ 27,39,40]. However,
these gaze targets often remain outside the camera’s ﬁeld of
view. This out-of-ﬁeld view challenge complicates the cal-ibration of gaze target positions. Zhang et al .[39,40] sets
screen points as gaze targets and uses a mirror to calibratethe screen plane. Kellnhofer et al .[20] uses 360
◦panoramic
cameras to capture gaze targets and human faces simultane-ously where the panoramic camera is pre-calibrated. How-ever, these strategies are not applicable to vehicles.
In-vehicle gaze dataset usually deﬁnes different region
such as windshield and left/right mirror in vehicles and per-form gaze zone classiﬁcation [ 12,14,16,18,23,30,34].
Kasahara et al .[19] collects an in-vehicle gaze dataset but
subjects are required to wear eye-tracking glasses, whichmeans the dataset is not applicable in the real world. Ourgaze collection system does not require dedicated devices
and produces natural face images.
2.2. Appearance-based Gaze Estimation
Appearance-based gaze estimation directly learns mappingfunction from facial appearance to human gaze [ 10]. Con-
ventional gaze estimation methods extract eye features fromeye images [ 7,9,26,36]. They concatenate the head pose
vector with eye features for gaze estimation. Recent meth-ods directly learn gaze from face images [ 3,4,11,33,35].
Face images provide both eye region and head pose in-formation, resulting in superior performance compared tomethods relying solely on eye images. However, the sub-tlety of the human eye in face images poses a challengeas it can be easily overlooked by the network. Zhang et
al.[37] utilize a learnable attention map to guide the net-
1557
A
BC
Depth camera
(for calibration)DMS camera
(for capture)
Transparentchessboard
(a) (b) (c) (d)Vehicle-mounted
infrared camera
Intel RealSense D435
Figure 2. We construct a vision-based in-vehicle gaze collection system comprising a DMS camera, a depth camera, and strategically
placed gaze targets, as depicted in (c). The DMS camera is positioned behind the steering wheel to capture drivers’ facial appearances,while the gaze targets, positioned beyond the DMS camera’s ﬁeld-of-view (FoV), such as on the windshield, remain unobserved. Thedepth camera, utilized for calibration purposes, is temporarily installed for capturing gaze target positions in 3D with respect to its owncoordinates, and it is removed during data collection. To facilitate the calibration of the depth camera’s pose relative to the DMS camera,
we propose employing a transparent chessboard , which is placed between the two cameras.
work’s focus speciﬁcally on the eye. Cheng et al .[11]
crop eye images from face images and construct a cascadenetwork to leverage them. In recent developments, trans-formers have showcased remarkable capabilities in gazeestimation. Cheng et al .[5] demonstrate that transform-
ers can achieve state-of-the-art performance across variousbenchmarks. They apply transformers to address dual-viewgaze estimation and show that dual-view images outperformsingle-view images [ 6].
3. In-Vehicle Gaze Data Collection System
In this section, we introduce our vision-based system for in-
vehicle gaze data collection, eliminating the need for dedi-
cated eye-tracking devices. Our system includes a neat gazetarget calibration method that effectively addresses the corechallenge in gaze annotation. Leveraging this system, wecollect the ﬁrst in-vehicle gaze dataset from 125 subjects.The dataset spans a diverse range of gaze, facilitating and
advancing future research in in-vehicle gaze estimation.
3.1. System Setup
We place gaze targets within the vehicle, and subjects areinstructed to look at each designated target during the datacollection process. Simultaneously, we record their facialappearance along with corresponding gaze targets.
Camera. Our system uses one camera of a driver moni-
tor system (DMS) for facial appearance capture. The DMScamera is an infrared camera with a capture resolution of1280 ×800. The camera is located behind the steering wheel
and directly points at the head region of drivers.
Gaze targets. We mark gaze targets with red points on
stickers, and each target is uniquely labeled with a distinctnumber printed on stickers to aid differentiation. We strate-gically position these stickers within various gaze regions
within the vehicle, including the windshield, left and right-side mirrors, rear-view mirror, center console, speedometer,handbrake, and etc. These targets cover a large gaze region,satisfying the gaze estimation requirements in vehicles.
3.2. Collection Procedure
We meticulously design the collection procedure to mini-mize error. During the collection process, participants are
instructed to look at speciﬁc gaze targets in a predeﬁned se-
quence. They are also required to speak the correspondingnumber associated with each target to conﬁrm their focusaccuracy. Participants must maintain focus on the target for3 seconds to facilitate image capture. We conduct a pre-liminary check on the captured images and discard any er-
roneous ones. Once this veriﬁcation is complete, the cur-
rent data collection phase concludes, and participants areinstructed to shift their gaze to the next gaze targets.
We design three postures for each participant. Partic-
ipants should complete the collection process three timeswith different postures. The ﬁrst posture requires partici-pants not to change head pose. They should preserve thesame head pose and only move eyeballs to focus a gaze tar-
get. We aim to collect sufﬁcient eye movement data with
this posture. We do not require participants to focus on thetarget where participants think it is hard to focus with onlyeye movement, e.g., the target in the side mirror. Partici-
pants can rotate their heads to look at these points but theyare also required to preserve the new head pose until they
cannot look at any targets with the current head pose. Thesecond posture requires participants not to move their headposition but they can rotate their heads to focus a gaze tar-
get. We do not set any constraints in the third posture. Par-ticipants can freely perform head movements in this pos-
1558
(a) IVGaze dataset samples0 20 40 60 80 100 120 140 160 180 2000200400600800Number of Images
(b) Dataset statistics across illumination intensity13756Sunglasses
Face Mask
Glasses
Glasses&Mask
w/oAccessories
2291848562689486
(c) Dataset statistics across face accessories
Figure 3. Our dataset is collected using IR cameras in the vehicle environment. (a) We present image samples of IVGaze, highlighting the
challenges posed by realistic in-vehicle conditions, including cases with sunglasses and reﬂections in glasses. (b) We categorize the imagecount based on their mean pixel value, showing the diversity of illumination conditions. (c) The image count is analyzed based on faceaccessories including glasses, sunglasses, and masks.
ture. The last two postures bring head pose variation for
IVGaze. We do not use devices such as headrests to con-strain participants. Participants only need to preserve thespeciﬁc posture and focus on targets in a comfortable way.
3.3. Gaze Annotation
Vision-based gaze collection systems deﬁne gaze as unit di-
rection vectors originating from the face center oand ex-
tending toward gaze targets t[10]. Our system follows
this framework and the gaze annotation can be computed asg=(t−o)/||t−o||
2. The gaze annotation is decomposed
into 3D face center estimation and gaze target calibration.
3D face center estimation has been effectively addressed
in previous research. We ﬁrst detect facial landmarks andﬁt a 3D morphable face model for 3D facial landmarks [ 24,
40]. We select the 3D position of the nose as gaze origin o.
However, the calibration of gaze target positions presents
a challenge because gaze targets are out-of-FoV . It meansthat the DMS camera cannot capture any images of the gazetargets, complicating the calibration problem.
3.4. Gaze Target Calibration
We need the 3D gaze target position tfor gaze annotation.
However, a key problem is all gaze targets are out-of-FoV .Conventional gaze datasets often present gaze targets on ascreen and employ a mirror to solve the out-of-FoV prob-lem, where the mirror reﬂects the content of the screen [ 36].
However, these methods are not suitable for vehicle scenar-ios and can lead to a substantial accumulation of errors.
In this section, we propose a neat and efﬁcient gaze target
calibration method. Our basic idea is to employ an auxiliarycamera to capture gaze targets. This allows a straightfor-ward calculation of the 3D gaze target positions w.r.t. the
auxiliary camera coordinate system. However, it also in-troduces a new challenge: how to calibrate extrinsic matrix
of the auxiliary cameras w.r .t. DMS camera ? Stereo cali-
bration is typically used to compute the pose of two cam-eras. However, it requires two captured images sharing cor-
responding points. In our system, gaze targets are locatedbehind the DMS camera which means the DMS camera andauxiliary camera should be oriented in opposite directions,making traditional stereo calibration infeasible.
To solve this problem, we propose to use a transparent
chessboard for the pose calibration. We set a transparent
chessboard between the two cameras and the two camerasrespectively capture each side of the chessboard. We ﬁrstcalibrate the two camera poses w.r.t. the chessboard co-ordinate system and then compute the pose between twocameras. It is worth noting that there are two differentchessboard coordinate systems corresponding to each sideof the chessboard. We can derive the transformation ma-trix between the two chessboard coordinate systems whereR
chess =d i a g ( 0 ,0,−1)andtchess =( 0 ,0,−d). The dis
the thickness of the chessboard.
In detail, we use an Intel RealSense D435 depth cam-
era as the auxiliary camera since it can provide accurate3D positions. We show the camera layout in Fig. 2(b).
We can obtain the transformation matrix R
dms,tdms and
Rdepth ,tdepth which can transfer a point from chessboard
coordinate systems to camera coordinate systems. We have
Rrot=RdmsRchessR−1
depth , (1)
trot=−RdmsRchessR−1
depth tdepth +Rdmstchess +tdms.
(2)
We use these matrices to convert points from the depth cam-
era coordinate system into the DMS camera coordinate sys-tem. Please refer to the supplementary material for details.
3.5. In-Vehicle Gaze Dataset
We collect the ﬁrst in-vehicle gaze dataset IVGaze whichprovides dense gaze annotation and natural face images .We show the dataset samples in Fig. 3(a).
Dataset Statistics. We collect 44,705 images from 125
subjects. The number of images per subject ranges from
1559
Figure 4. We show the distribution of data for gaze (left) and head
movements (right). Brighter regions denote higher data density.
169 to 946. Most subjects provide around 300 images per
person, i.e., they repeat the collection procedure three times
with three different postures. 40 subjects are female and 85subjects are male, with ages spanning from 20 to 50 years.
Collection Conditions. The dataset was collected be-
tween 9 am and 7 pm in outdoor environments, covering a
wide range of lighting conditions. We also collect images
in indoor environments, i.e., a garage. The dataset statistics
across pixel intensity are shown in Fig. 3(b).
Face Accessories. We consider two face accessories
during the collection: glasses and masks. For subjects whodid not wear glasses, we provided glasses and asked themto repeat the collection wearing glasses. We provided whiteand black masks for some subjects and asked them to re-peat the collection wearing masks. We also required a fewsubjects to wear sunglasses to facilitate future research. Weshow the statistics in Fig. 3(c).
Data Distribution. We plot the angular distribution
of gaze and head pose. Note that the data normalizationmethod (introduced in the next section) changes gaze direc-
tions and head poses with a rotation matrix. Fig. 4shows
the distribution in normalization space. The horizontal gazeis from −50
◦to90◦, and the vertical gaze is from −40◦to
40◦. It is worth noting that the vehicle environment has lim-
ited space, and our dataset already covers almost all regions.Our dataset is collected in the real vehicle environment, andthe driver seat in the vehicle is located on the left. There-fore, our dataset has a relatively small distribution on theleft. The right ﬁgure in Fig. 4shows the head pose distribu-
tion. Our dataset contains a large range of head poses in the
yaw axis, as subjects need to look at gaze targets from theleft-side mirror to the right-side mirror.
4. In-Vehicle Gaze Estimation
The in-vehicle environment brings new settings and chal-lenges. In this section, we systematically explore in-vehiclegaze estimation. We introduce a revised data preprocess-ing method and describe a novel gaze estimation network.We also extend the network for an application and prove theadvantage of in-vehicle gaze estimation.4.1. Revisiting Data Normalization in Vehicle
Data normalization rotates and translates cameras via per-spective transformation in face images to reduce head posevariation. Conventional methods usually deﬁne the rotatedcamera coordinate system C
rbased on head pose [ 29,38].
However, these methods cannot bring performance im-provement in the vehicle environment.
The x-axis of C
ris typically deﬁned as the x-axis of
head coordinate systems. It means they will rotate imagesto keep the head straight. We observed that the designedx-axis will produce unstable results, especially in the ex-
treme head pose. Therefore, we propose that do not rotatevirtual cameras based on the x-axis of head coordinate sys-
tems. In detail, we compute the z-axis of C
ras direction
vectors from cameras to face centers. We use the x-axis of
the virtual camera, i.e.,(1,0,0)rather than the head coor-
dinate system. We compute the y-axis as y=z×xand
also recompute the x-axis as x=y×z. Finally, we have
a rotation matrix R=[x;y;z]and use a scale matrix S
to maintain the user-camera distance. We warp the imagebased on matrix SR. The human gaze is also changed as
g
n=Rgo, where gnandgodenotes gaze in the normal-
ization and origin space.
4.2. Gaze Pyramid Transformer
Gaze estimation methods typically learn gaze from face im-ages. However, face images captured in vehicles often suf-fer from low resolution owing to the limited capabilitiesof the camera. This low resolution can lead conventionalmethods to overlook subtle details, especially in the eye re-gion. Some techniques address this challenge by croppingeye images and processing them separately [ 8], but these
approaches encounter difﬁculties when faced with extremehead poses, as one of the eye images may not be visible.
We propose a gaze pyramid transformer (GazePTR) in
this section. We build a feature pyramid and input multi-level features into a transformer for gaze estimation. As
shown in Fig. 5, our network uses a convolution network
to extract features from facial images. We collect featuremaps from different levels rather than the last level. We use1×1convolution layers and global average pooling layers
to preserve the same feature dimension. Finally, we input
the multi-level feature into a transformer. A learnable tokenis used to aggregate multi-level features for gaze estimation.
However, the network has the same performance
achieved by directly utilizing the last feature map for gazeestimation. This result can be attributed to the network’s
tendency to overlook low-level features. To address this is-
sue, we introduce additional constraints by estimating gazefrom each level of features and calculating correspondingloss. It can be understood that we ﬁrst extract multi-levelfeatures, with each feature being potentially useful. A trans-
former is used to effectively ensemble these features.
1560
*D]H
375
+HDG3RVLWLRQ&URSSLQJ
3HUVSHFWLYH
7UDQVIRUPDWLRQ5*D]H375
FRQY
FRQY
FRQY
FRQYൈͳ
ൈʹ
ൈͶ
ൈͺ7UDQVIRUPHU
*D]HJ
&DP3RVH 5&&DP3RVH &
3RVLWLRQDO (QFRGLQJ3RVLWLRQDO (QFRGLQJ
*D]H'LUHFWLRQ
Output
3URMHFWLRQ8QNQRZQ 3ODQH *D]H=RQH
7UDQVIRUPHU 7UDQVIRUPHU
ିଵො
%DVLV7UL3ODQH3RVLWLRQDO
(QFRGLQJ3RVLWLRQDO)HDWXUH9LVXDO)HDWXUH
7UDQVIRUPHU
3RV(PEHGGLQJOutput
Figure 5. The GazeDPTR directly crop face for origin images and rotates virtual cameras via perspective transformation for normalized
images. It builds a dual-stream network to extract features from the two images based on the GazePTR for feature extraction whichintegrates multi-level features via transformers. To further merge the features from two streams, we leverage a transformer where camerapose is used as the positional feature in the transformer. We deﬁne the original camera pose as C=diag(1,1,1)and the camera pose
in normalization space is RC . We also extend the network for gaze zone classiﬁcation. We deﬁne a tri-plane and project gaze into them.
We extract positional features from three intersection points via a transformer. We also extract visual features from images and predict thegaze zone based on both visual features and positional features. The whole network is trained in an end-to-end manner.
4.3. Dual-Stream Gaze Pyramid Transformer
Dual-view images have been demonstrated to outperform
single-view images in gaze estimation [ 6]. In our work,
we rotate cameras in data normalization through perspec-tive transformation. This inspired us to formulate the dual-camera setting leveraging both original and normalized im-ages. However, dual-view images improve accuracy sincethey contain more visual information. It remains uncertainwhether the combination of normalized and original imagescan provide additional insights beyond what each offers.
To validate our hypothesis, we executed an oracle base-
line. We separately train GazePTR on the original and the
normalized datasets, selecting the best result from each im-age pair. The result reveals a signiﬁcant performance im-provement, validating the inherent advantages of utilizingboth images. Please refer to the supplementary for details.
Therefore, we propose a dual-stream gaze pyramid trans-
former (GazeDPTR). Our method leverages both normal-ized images and original images for gaze estimation. We
employ GazePTR to extract features from the two imagesand a transformer is used to integrate the two features forgaze estimation. Note that, the two features are in differ-
ent camera coordinate systems and correspond to differentgaze. To establish the connection, we use the camera pose
as the positional information in the transformer. However,it is impossible the calibrate a virtual camera. We deﬁne thecamera pose of original images as C=diag (1,1,1)and
the camera pose of normalized images is RC . We extract
thez-axis and use positional encoding for camera pose [ 6].
4.4. Strategy for Gaze Zone Classiﬁcation
Detecting driver attention is essential in driver monitoring
systems. In this section, we introduce a novel strategy toextend GazeDPTR for gaze zone classiﬁcation. GazeDPTR
estimates gaze direction from face images. While the con-ventional solution involves projecting gaze direction andcomputing intersections within the vehicle, it is not prac-tical without a 3D vehicle model. Zhang et al . use a tri-
plane for self-supervised loss function [ 35]. Our alternative
strategy deﬁnes a foundational tri-plane and computes theintersection of gaze vectors with this tri-plane. We extractpositional features from three intersection points to facili-
tate gaze zone classiﬁcation.
In detail, we deﬁne a tri-plane with normal vectors
(1,0,0),(0,1,0), and (0,0,1), intersecting at the origin
(0,0,0). Given the estimated gaze vector g
n, we convert
it back to the original space using go=R−1gn. We then
project goonto the tri-plane with the face center oand ob-
tain three intersection points. To enhance the positional fea-tures, we apply positional encoding to these points and inputthem into a 2-layer transformer. The positional encodingnot only increases feature dimensions but also normalizesthem within the range of -1 to 1. Additionally, we use an-other learnable token to aggregate visual features from theoriginal images. Gaze zone classiﬁcation is performed us-ing both visual features and positional features.
We train the whole network in an end-to-end manner. We
use L1 loss for gaze estimation and cross-entropy loss forgaze zone classiﬁcation. Please refer to the supplementaryfor implementation details.
5. Experiment
Dataset: IVGaze contains 44,705 images of 125 subjects.
To perform within-dataset evaluation, we divide the datasetinto three subsets based on subjects. The image numbersof the three subsets are 15,165, 14,674, and 14,866. We
1561
Table 2. Performance comparison in gaze estimation. Our meth-
ods achieve better performance than the compared methods.
Angular
ErrorAverage Precision (AP)
<2◦<4◦<6◦<8◦
FullFace [ 37] 13.67◦2.3% 8 .8% 17 .8% 28%
DWG [ 16] 8.82◦6.6% 21 .7% 38 .1% 53 .2%
Gaze360 [ 20] 8.15◦9.2% 27 .3% 44 .6% 58 .9%
FullFace+[37]7.48◦14.2% 31 .1% 46 .7% 63 .1%
GazeTR [ 5] 7.33◦17% 32 .8% 47 .5% 64 .7%
XGaze [ 40] 7.06◦11.7% 32 .7% 51 .5% 66 .7%
GazePTR 7.04◦17.6% 34% 49 .3% 66 .7%
GazeDPTR 6.71◦22.1% 36% 50.3% 68.4%
perform three-fold cross-validation on our dataset.
Evaluation Metric: We use the angular error as the evalua-
tion metric of gaze estimation as most of the methods [ 10],
where a smaller value represents a better model. However,we notice the angular error only shows the overall perfor-mance while cannot give deep insights. Therefore, we de-ﬁne the average precision (AP) where AP of <k
◦means an
estimation is considered correct if the angular error is lower
than k◦. Regarding the gaze zone classiﬁcation, average
precision is used as a common multi-class classiﬁcation.
5.1. Comparison with SOTA Methods
We ﬁrst compared our methods with SOTA methods in the
in-vehicle gaze estimation task. We compared our methods
with FullFace [ 37], Gaze360 [ 20], XGaze [ 40], DWG [ 16]
and GazeTR [ 5]. We replaced the backbone of the FullFace
method from AlexNet to ResNet18 for a more convincingcomparison. We denote the new method as FullFace
+.
The result is shown in Tab. 2. GazePTR and GazeDPTR
both show better performance than the compared methods.GazePTR and GazeTR have the same backbone and trans-
former architecture. However, GazePTR outperforms 0.29
◦
thanGazeTR since GazePTR uses multi-level feature maps.
GazeDPTR builds a dual-stream network and further brings0.33
◦improvement than GazePTR. These results show the
advantage of our methods.
Interestingly, XGaze has a similar angular error to
GazePTR. However, it is signiﬁcantly worse than GazePTRin AP of <2
◦. It is because that XGaze uses ResNet50 as the
backbone. The deep backbone enhances the feature extrac-
tion ability but also easily overlooks the small eye region,making it challenging to achieve precise gaze estimation.
5.2. The Impact of Face Accessories
IVGaze provides rich samples of wearing face accessories,including glasses, masks, and sunglasses. We also evalu-ate their impacts on the accuracy of gaze estimation andshow in Tab. 3. The result shows that eyeglasses have aTable 3. We show the impacts of different face accessories on per-
formance. The sunglasses bring a signiﬁcant performance drop.
Glasses Mask Sunglasses
with w/o with w/o with
FullFace [ 37]14.43◦12.40◦15.20◦13.35◦21.39◦
DWG [ 16] 9.20◦8.19◦9.43◦8.69◦17.43◦
Gaze360[ 20] 8.30◦7.91◦8.95◦7.99◦17.99◦
FullFace+[37]7.59◦7.30◦8.37◦7.30◦16.50◦
XGaze [ 40] 7.07◦7.03◦7.80◦6.90◦15.15◦
GazeTR [ 5] 7.40◦7.22◦8.12◦7.17◦17.49◦
GazePTR 7.13◦6.90◦7.78◦6.89◦16.54◦
GazeDPTR 6.77◦6.63◦7.44◦6.57◦16.41◦
0-20 20-40 40-60 60-805678Angular Error (Deg.)
Gaze Range (Deg.)GazeTR
GazePTR
GazeDPTR
0-20 20-40 40-60 60-80678Angular Error (Deg.)
Head Range (Deg.)GazeTR
GazePTR
GazeDPTR
Figure 6. We compute the angular degree between gaze direc-
tion/head orientation with the frontal direction, i.e., (0, 0, -1), and
count the mean gaze estimation accuracy in different ranges. In-terestingly, GazePTR brings signiﬁcant performance improvementto GazeTR in the 0
◦−20◦gaze range. This is because subjects
prefer to move their eyeball rather than their head when the gazetarget is located at a close range. GazePTR utilizes multilevel fea-tures and can accurately capture eye movement.
large impact if the basic performance is not good. However,
as the basic performance improves, the performance differ-ence reduces. This is because most of glasses are transpar-ent, enabling a robust model to extract features effectively.Sunglasses have a signiﬁcant impact on performance, withaverage performance exceeding 15
◦. Masks have a substan-
tial impact on performance, as they obscure facial regions,complicating the extraction of head-related features.
5.3. Performance Distribution
We show the performance distribution to provide deep in-sight. We ﬁrst show the performance distribution in differ-ent gaze ranges. We introduce a novel concept, the gazerange of a
◦-b◦, which is deﬁned as a set of samples satis-
fying the requirement that the angular degree between thegaze direction and the frontal gaze direction is in the range.The result is shown in Fig. 6. Interestingly, GazePTR and
GazeDPTR have better performance in 0
◦-20◦. This is be-
cause subjects prefer to move their eye rather than head tolook at objects in this range. Our methods leverage multi-
level features and easily capture eye movement. We alsoshow the performance distribution in different head ranges.
1562
Table 4. We present the performance of GazePTR at each level of
the feature. The higher-level feature achieves better performance.Our method integrates features from all levels and achieves thebest performance.
1st 2nd 3rd 4th Ours
GazePTR 10.33◦8.59◦7.61◦7.35◦7.04◦
Table 5. We present the performance of GazePTR separately train-
ing on the original images and normalized images. GazeDPTRleverages camera pose to establish a connection between the twoimages and use both two images for gaze estimation. We also re-moved the camera pose to perform an ablation study.
Original
imagesNormalized
imagesGazeDPTR
(w/o camera pose)GazeDPTR
Acc 7.44◦7.04◦7.03◦6.71◦
Our methods are robust in different head ranges where the
maximum performance difference is only 1◦.
5.4. Ablation Study
GazePTR leverages multi-level features for gaze estima-
tion. We ﬁrst conduct an ablation study to demonstrate the
advantage. We obtain the gaze performance of GazePTR
at each level of feature and show it in Tab. 4. The re-
sult demonstrates a higher-level feature achieves better per-formance. GazePTR integrates multi-level features andachieves the best performance. The result proves the ad-vantage of our design.
GazeDPTR leverages both original and normalized images
along with their corresponding camera poses. To demon-strate the effectiveness of our design, we perform an abla-tion study in Tab. 5. Training GazePTR separately on orig-
inal and normalized images yields performances of 7.44
◦
and 7.04◦respectively. GazeDPTR, which incorporates
both image types, achieves a performance improvement of0.34
◦. We also experimented by excluding the positional
encoding of the camera pose in GazeDPTR. Interestingly,the result is comparable to using only normalized images.
5.5. Additional Experiments
Data Normalization. We compare our method with the
data normalization method [ 38] in the left of Tab. 6. The
previous method cannot work well in vehicle environmentswhile our method brings 0.3
◦performance improvement.
Case Study. We visual gt and our prediction in Fig. 7.
5.6. Gaze Zone Classiﬁcation
We propose a basis tri-plane to acquire positional feature
and combine both positional feature and visual features forgaze zone classiﬁcation. We respectively evaluate each fea-ture and show the result in the right of Tab. 6. The visual
feature achieves better performance than the positional fea-ture since the positional feature is obtained from gaze pro-Table 6. We evaluate different data normalization methods in the
left table. Our method outperforms the previous method [ 38]. In
the right table, we evaluate different features in the gaze zone clas-siﬁcation. Our method uses both positional feature and visual fea-tures, achieving 2.4%improvement over visual features.
NormalizationGazeTR GazePTR[38] ours
×× 7.77◦7.44◦
/check 8.64◦8.53◦
/check 7.33◦7.04◦Visual
FeaturePositional
FeatureAP
/check 79.4%
/check 75.3%
/check /check 81.8%
Figure 7. We illustrate GT with green lines and Prediction with
red lines. The number shows the angular error in degree. The
right two ﬁgures are failure cases since the eye is hard to capture.
jection. The projection increases the model interpretabil-
ity but decreases the ﬁtting ability. Our method uses thetwo features and gets 2.4%performance improvement. The
result demonstrates that the gaze zone classiﬁcation taskcould be enhanced by gaze direction cues, highlighting theadvantage of our gaze estimation work.
6. Conclusion
In this work, we provide systematical research in in-vehiclegaze estimation, including dataset, algorithm, and extensiveapplication. We ﬁrst solve the data collection issue in a ve-hicle where a gaze target calibration method is proposed.We collect the ﬁrst in-vehicle gaze dataset containing densegaze annotation and natural face images. We further explorethe algorithm for in-vehicle gaze estimation. Our workbrings two deep insights 1) multi-level feature is useful tocapture eye region information. 2) simultaneously leverag-ing original images and normalized images could achievebetter performance. We also extend our work for the down-stream gaze zone classiﬁcation task. We demonstrate thatthe gaze direction cues could bring performance improve-ment for gaze zone classiﬁcation.
7. Acknowledgments:
This work was supported by Institute of Information &communications Technology Planning & Evaluation (IITP)grant funded by the Korea government (MSIT) (No.2022-0-00608, Artiﬁcial intelligence research about multi-modalinteractions for empathetic conversations with humans)
1563
References
[1] Abdul Rafey Aftab. Multimodal driver interaction with ges-
ture, gaze and speech. In 2019 International Conference on
Multimodal Interaction , pages 487–492, 2019. 1
[2] Aya Ataya, Won Kim, Ahmed Elsharkawy, and SeungJun
Kim. Gaze-head input: Examining potential interaction withimmediate experience sampling in an autonomous vehicle.
Applied Sciences , 10(24), 2020. 1
[3] Yiwei Bao, Y unfei Liu, Haofei Wang, and Feng Lu. Gen-
eralizing gaze estimation with rotation consistency. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 4207–4216, 2022. 2
[4] Xin Cai, Jiabei Zeng, Shiguang Shan, and Xilin Chen.
Source-free adaptive gaze estimation by uncertainty reduc-tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 22035–
22045, 2023. 2
[5] Yihua Cheng and Feng Lu. Gaze estimation using trans-
former. ICPR , 2022. 3,7
[6] Yihua Cheng and Feng Lu. Dvgaze: Dual-view gaze estima-
tion. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision (ICCV) , pages 20632–20641, 2023.
3,6
[7] Yihua Cheng, Feng Lu, and Xucong Zhang. Appearance-
based gaze estimation via evaluation-guided asymmetric re-gression. In The European Conference on Computer Vision ,
2018. 2
[8] Yihua Cheng, Shiyao Huang, Fei Wang, Chen Qian, and
Feng Lu. A coarse-to-ﬁne adaptive network for appearance-based gaze estimation. In Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence , 2020. 5
[9] Yihua Cheng, Xucong Zhang, Feng Lu, and Y oichi Sato.
Gaze estimation by exploring two-eye asymmetry. IEEE
Transactions on Image Processing , 29:5259–5272, 2020. 2
[10] Yihua Cheng, Haofei Wang, Yiwei Bao, and Feng Lu.
Appearance-based gaze estimation with deep learning: A
review and benchmark. arXiv preprint arXiv:2104.12668 ,
2021. 2,4,7
[11] Yihua Cheng, Yiwei Bao, and Feng Lu. Puregaze: Purifying
gaze feature for generalizable gaze estimation. AAAI , 2022.
2,3
[12] In-Ho Choi, Sung Kyung Hong, and Y ong-Guk Kim. Real-
time categorization of driver’s gaze zone using the deeplearning techniques. In 2016 International conference on
big data and smart computing (BigComp) , pages 143–148.
IEEE, 2016. 2
[13] Tobias Fischer, Hyung Jin Chang, and Yiannis Demiris. Rt-
gene: Real-time eye gaze estimation in natural environments.InThe European Conference on Computer Vision , 2018. 2
[14] Lex Fridman, Philipp Langhans, Joonbum Lee, and Bryan
Reimer. Driver gaze region estimation without use of eyemovement. IEEE Intelligent Systems , 31(3):49–56, 2016. 2
[15] Kenneth Alberto Funes Mora, Florent Monay, and Jean-
Marc Odobez. Eyediap: A database for the developmentand evaluation of gaze estimation algorithms from rgb andrgb-d cameras. In Proceedings of the ACM Symposium on
Eye Tracking Research & Applications , 2014. 2[16] Shreya Ghosh, Abhinav Dhall, Garima Sharma, Sarthak
Gupta, and Nicu Sebe. Speak2label: Using domain knowl-
edge for creating a large scale driver gaze zone estima-
tion dataset. In The IEEE/CVF International Conference
on Computer Vision (ICCV) Workshops , pages 2896–2905,
2021. 1,2,7
[17] Amie C Hayley, Brook Shiferaw, Blair Aitken, Frederick
Vinckenbosch, Timothy L Brown, and Luke A Downey.Driver monitoring systems (dms): The future of impaired
driving management? Trafﬁc injury prevention , 22(4):313–
317, 2021. 1
[18] Sumit Jha and Carlos Busso. Probabilistic estimation of the
gaze region of the driver using dense classiﬁcation. In 2018
21st International Conference on Intelligent TransportationSystems (ITSC) , pages 697–702. IEEE, 2018. 1,2
[19] Isaac Kasahara, Simon Stent, and Hyun Soo Park. Look
both ways: Self-supervising driver gaze estimation and road
scene saliency. In European Conference on Computer Vision ,
pages 126–142. Springer, 2022. 2
[20] Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Ma-
tusik, and Antonio Torralba. Gaze360: Physically uncon-strained gaze estimation in the wild. In The IEEE Interna-
tional Conference on Computer Vision , 2019. 1,2,7
[21] Muhammad Qasim Khan and Sukhan Lee. A comprehensive
survey of driving monitoring and assistance systems. Sen-
sors , 19(11), 2019. 1
[22] Muhammad Qasim Khan and Sukhan Lee. Gaze and eye
tracking: Techniques and applications in adas. Sensors ,1 9
(24), 2019. 1
[23] Sung Joo Lee, Jaeik Jo, Ho Gi Jung, Kang Ryoung Park,
and Jaihie Kim. Real-time gaze estimator based on driver’s
head orientation for forward collision warning system. IEEE
Transactions on Intelligent Transportation Systems , 12(1):
254–267, 2011. 2
[24] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua.
Epnp: An accurate o (n) solution to the pnp problem. Inter-
national journal of computer vision , 81(2):155–166, 2009.
4
[25] Prajval Kumar Murali, Mohsen Kaboli, and Ravinder
Dahiya. Intelligent in-vehicle interaction technologies. Ad-
vanced Intelligent Systems , 4(2):2100122, 2022. 1
[26] Seonwook Park, Adrian Spurr, and Otmar Hilliges. Deep
pictorial gaze estimation. In The European Conference on
Computer Vision , 2018. 2
[27] Seonwook Park, Emre Aksan, Xucong Zhang, and Otmar
Hilliges. Towards end-to-end video-based eye-tracking. In
The European Conference on Computer Vision , pages 747–
763. Springer, 2020. 2
[28] Akshay Rangesh, Bowen Zhang, and Mohan M Trivedi.
Driver gaze estimation in the real world: Overcoming theeyeglass challenge. In The IEEE Intelligent V ehicles Sympo-
sium (IV) , pages 1054–1059. IEEE, 2020. 2
[29] Y usuke Sugano, Yasuyuki Matsushita, and Y oichi Sato.
Learning-by-synthesis for appearance-based 3d gaze estima-tion. In The IEEE Conference on Computer Vision and Pat-
tern Recognition , 2014. 5
[30] Ashish Tawari, Kuo Hao Chen, and Mohan M Trivedi.
Where is the driver looking: Analysis of head, eye and
1564
iris for robust gaze zone estimation. In 17th Interna-
tional IEEE conference on intelligent transportation systems
(ITSC) , pages 988–994. IEEE, 2014. 2
[31] Sourabh V ora, Akshay Rangesh, and Mohan Manubhai
Trivedi. Driver gaze zone estimation using convolutional
neural networks: A general framework and ablative anal-ysis. IEEE Transactions on Intelligent V ehicles , 3(3):254–
265, 2018. 1,2
[32] Yafei Wang, Guoliang Y uan, Zetian Mi, Jinjia Peng, Xueyan
Ding, Zheng Liang, and Xianping Fu. Continuous driver’sgaze zone estimation using rgb-d camera. Sensors , 19(6):
1287, 2019. 1,2
[33] Yaoming Wang, Yangzhou Jiang, Jin Li, Bingbing Ni, Wen-
rui Dai, Chenglin Li, Hongkai Xiong, and Teng Li. Con-trastive regression for domain adaptation on gaze estima-tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 19376–
19385, 2022. 2
[34] Guoliang Y uan, Yafei Wang, Huizhu Yan, and Xianping Fu.
Self-calibrated driver gaze estimation via gaze pattern learn-ing. Knowledge-Based Systems , 235:107630, 2022. 2
[35] Mingfang Zhang, Y unfei Liu, and Feng Lu. Gazeonce:
Real-time multi-person gaze estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) , pages 4197–4206, 2022. 2,6
[36] Xucong Zhang, Y usuke Sugano, Mario Fritz, and Andreas
Bulling. Appearance-based gaze estimation in the wild.InThe IEEE Conference on Computer Vision and Pattern
Recognition , 2015. 2,4
[37] Xucong Zhang, Y usuke Sugano, Mario Fritz, and Andreas
Bulling. It’s written all over your face: Full-face appearance-based gaze estimation. In The IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops , pages
2299–2308, 2017. 2,7
[38] Xucong Zhang, Y usuke Sugano, and Andreas Bulling. Re-
visiting data normalization for appearance-based gaze es-
timation. In Proceedings of the ACM Symposium on Eye
Tracking Research & Applications , 2018. 5,8
[39] Xucong Zhang, Y usuke Sugano, Mario Fritz, and Andreas
Bulling. Mpiigaze: Real-world dataset and deep appearance-based gaze estimation. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence , 41(1):162–175, 2019. 1,2
[40] Xucong Zhang, Seonwook Park, Thabo Beeler, Derek
Bradley, Siyu Tang, and Otmar Hilliges. Eth-xgaze: A largescale dataset for gaze estimation under extreme head poseand gaze variation. In The European Conference on Com-
puter Vision , 2020. 1,2,4,7
1565
