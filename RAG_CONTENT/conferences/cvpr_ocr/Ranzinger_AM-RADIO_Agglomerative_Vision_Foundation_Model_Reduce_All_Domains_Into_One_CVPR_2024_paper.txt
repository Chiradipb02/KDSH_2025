AM-RADIO: Agglomerative Vision Foundation Model
Reduce All Domains Into One
Mike Ranzinger˚, Greg Heinrich˚, Jan Kautz, Pavlo Molchanov
NVIDIA
{mranzinger,gheinrich,jkautz,pmolchanov }@nvidia.com
Figure 1. PCA feature visualization of different models. Our proposed RADIO model can process any resolution and aspect ratio, and
produces semantically rich dense encodings. Not only is the model capable of producing state-of-the-art feature representations at arbitrary
resolutions, but the adaptor heads may be used to simulate other models at resolutions they don’t support, such as SAM at larger than
1024px.
Abstract
A handful of visual foundation models (VFMs) have re-
cently emerged as the backbones for numerous downstream
tasks. VFMs like CLIP , DINOv2, SAM are trained with dis-
tinct objectives, exhibiting unique characteristics for vari-
ous downstream tasks. We ﬁnd that despite their conceptual
differences, these models can be effectively merged into a
uniﬁed model through multi-teacher distillation. We name
this approach AM-RADIO (Agglomerative Model – Reduce
All Domains Into One). This integrative approach not only
surpasses the performance of individual teacher models
but also amalgamates their distinctive features, such as
zero-shot vision-language comprehension, detailed pixel-
level understanding, and open vocabulary segmentation ca-
pabilities. Additionally, in pursuit of the most hardware-
efﬁcient backbone, we evaluated numerous architectures in
*Equal contributionour multi-teacher distillation pipeline using the same train-
ing recipe. This led to the development of a novel architec-
ture (E-RADIO) that exceeds the performance of its prede-
cessors and is at least 6x faster than the teacher models at
matched resolution. Our comprehensive benchmarking pro-
cess covers downstream tasks including ImageNet classiﬁ-
cation, semantic segmentation linear probing, COCO ob-
ject detection and integration into LLaVa-1.5.
Code: https://github.com/NVlabs/RADIO .
1. Introduction
Knowledge Distillation [ 24] has been a very successful
and popular technique for transferring the knowledge of a
“teacher” model (or ensemble of models) into a typically
smaller “student” model. In the original formulation, both
the student and the teacher operate on the same in-domain
dataset, and the student simultaneously matches the logits
of the teacher, and the ground truth labels. Instead of us-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12490
ModelParams Resol-ThroughputImageNet1KSegmentation (linear)Vision-Language (LLaVa-1.5 [36])SAM [32](M)utionZero-shot k-NNADE20k VOCGQA POPE TextVQA VQAv2COCOOpenCLIP-H/14 [10] 632 224 503 77.19 81.10 40.04 68.03 57.94 83.61 50.48 72.24 -MetaCLIP-H/14 [60] 632 224 486 80.51 82.12 35.39 62.62 60.57 84.76 53.65 75.71 -SigLIP-M/14 [69] 428 384 241 82.61 85.16 40.53 70.31 57.70 84.85 56.65 71.94 -Intern-ViT-6B [9] 5,902 224 63 83.20::78.43 47.20 76.85 60.18 84.02 52.45 76.75 -5,537 448 14::68.64 42.78 74.43 61.1987.23 60.3678.83 -*DFN CLIP-H/14 [17] 633 378 17083.9085.27 39.00 70.29 61.73 85.91 56.78 78.78 -*OpenAI CLIP-L/14 [47] 305 336 414 75.54 79.80 36.51 67.04 62.20 86.09 57.92 78.49 -*DINOv2-g/14-reg [13] 1,137 224 294:- 83.41 48.68 82.78 61.88 85.62 47.18 76.23 -*SAM-H/16 [32] 637 1024 12 - 22.12 28.08 34.34 49.92 81.76 43.91 57.6577.18E-RADIO-L(Ours)391 512 468 80.73 83.89 48.22 81.64 61.70 85.07 51.47 76.73 76.31RADIO-ViT-H/16(Ours)653 432 158 82.9386.06 51.34 84.71 63.0186.20 56.3279.2876.23Table 1. Comparison of vision foundation and RADIO models. ”Zero-Shot” and k-NN are computed on ImageNet-1K. ADE20K [ 72] and
VOC (PascalVOC2012) refer to linear probe semantic segmentation mIOU. GQA, POPE (popular), TextVQA, and VQAv2 are obtained
via LLaVa 1.5 [ 36] by replacing the vision encoder. COCO is the instance segmentation metric introduced by [ 7] to evaluate SAM [ 32]
distillation. RADIO attains the best metrics on most benchmarks, and is competitive with the rest, while E-RADIO enables high quality
results in resource constrained settings. Note that Zero-Shot and COCO use teacher’s decoder head that is not ﬁnetuned. Throughput
computed using NVIDIA A100 GPU, stated resolution, and TensorRT v8601. * Denotes teachers used to train our ﬁnal RADIO.:We failed
to export DINOv2-g-reg to TensorRT, so we report DINOv2-g here, which should be fairly close.::We were unable to get zero shot working
using their model code.
ing labeled images, an alternative approach is to train the
student model to match the features of the teacher model
[1,23,26,49,52,57,67].
Instead of using a smaller student model, [ 59] employ
an iterative learning procedure with a high-capacity model
where a student of equal or greater capacity than the teacher
is trained with heavy augmentation applied to the student.
Once trained, they expand the dataset by pseudo-labeling
new data using the trained student. They then make the stu-
dent become the teacher, and repeat the process. An im-
portant ﬁnding in this work is that the student is capable of
surpassing the performance of the teacher.
The authors of [ 24] explore the concept of ensemble dis-
tillation, where there are multiple teachers, each of which
having restricted domain knowledge. [ 73] provides an
overview of multi-teacher distillation, and proposes that in-
stead of matching the summary of an ensemble of teachers,
the student can match the features of each individual teacher
via some learned non-shared mapping from the representa-
tion space of the student to each teacher. Of interest in their
approach is that the student and teacher don’t need to share
the same architecture, and also that treating teachers indi-
vidually yields improved performance.
Recently, the concept of Foundation Models (FMs) [ 3]
has emerged, with the general understanding that these
models are large, general, and expensive to train. Through
training on very large datasets they are broadly applicable
to numerous downstream tasks. A seminal example of such
models is CLIP [ 47], which trains on web-scale weakly su-
pervised (image, caption) pairs, and results in exceptional
zero-shot performances on a wide array of computer vision
benchmarks. While CLIP is ﬁrmly a FM, another model,
DINOv2 [ 44] has emerged with broad capabilities, often
surpassing CLIP on dense tasks that require strong spatialfeatures, such as ADE20k [ 72] and Pascal VOC [ 16]. Sep-
arately, SAM (Segment Anything) [ 32] is gaining popular-
ity for its excellent open-vocabulary instance segmentation
abilities, whose vision encoder we hypothesize has strong
dense feature representations.
We introduce AM-RADIO with the goal of learning from
multiple foundational models simultaneously. We observe
that, when given a student model of sufﬁcient capacity,
it is often able to exceed any of its teachers on impor-
tant axes. In addition to performing well on representative
foundational benchmarks, by virtue of the training frame-
work, our student models are able to mimic their teacher
models, and thus are able to perform downstream tasks
that are otherwise performed by the teachers. Examples
of this include CLIP-ZeroShot applications, since the lan-
guage model trained by CLIP is compatible with our stu-
dent, and also Segment-Anything tasks, as the student is
able to replace the vision encoder and interface with the
already-trained mask decoders.
We also study the effect of using a more hardware-
efﬁcient model architecture. Most works on efﬁciency
are not directly comparable as they use different training
recipes, even when evaluated on the same dataset such as
ImageNet-1k, and may be over-tuned. To this end, we eval-
uate more than 10 promising architectures under the same
training recipe for a direct comparison. We reveal that
CNN-like architectures are faster but struggle to distill ViT
VFMs. This led us to the development of a novel hybrid
architecture, E-RADIO, that exceeds the performance of its
predecessors and is at least 6x faster than teacher models at
matched resolution.
Our main contributions are as follows:
•We describe a general methodology for distilling multi-
ple distinct foundation models into one, including models
12491
Results
VisionFoundationmodel(student)Studenthead 2CLIPStudenthead 1DINOv2
Studenthead 3SAMTeacher 1:DINOv2Multi componentdistillation lossTeacher 2:CLIPMulti componentdistillation lossTeacher 3:SAMMulti componentdistillation loss
A vintage radioon white sandybeach, a colorfulbeach ballnearbyPixel-levelvisual tasks:
Text grounding:
Semanticsegmentation:
(from scratch)
Frozen weights
TrainableImage only data:
Spatial featuresSummary tokenSpatial featuresSummary token
Spatial featuresFramework
Figure 2. AM-RADIO - is a multi-teacher distillation framework
that efﬁciently trains new vision foundation models of arbitrary ar-
chitecture. It uniﬁes unique attributes (like zero-shot text ground-
ing, dense correspondence) of each teacher into a single model that
even outperforms them on a majority of the tasks.
with incompatible input resolutions.
•We show that these student models are able to outperform
their teachers on representative benchmarks.
•We demonstrate that these student models can either
drop-in replace their teachers, or their features can be
used directly in downstream applications such as provid-
ing visual encoding for LLaV A [ 36,37].
•We benchmark a number of efﬁcient architectures and
propose a new architecture (E-RADIO) that allows for
similar model quality at signiﬁcant speedups.2. Related Work
Knowledge Distillation The underpinning of our work is
based on the method of Knowledge Distillation [ 4,5,24,
31,43] which aims to train a “student” model using soft
targets produced by an already-trained “teacher” model, us-
ing the the teacher’s output logits as “soft” labels. Alterna-
tively, distillation can be performed using intermediate net-
work activations [ 1,23,26,49,52,57,67]. In general, due
to the heterogeneous nature of the different teacher founda-
tion models that we employ, we ignore any potential labels
coming from the data, and we ignore the logits of teachers,
and simply opt to match the feature representations of the
teachers before any task-speciﬁc processing stages.
Multi-Teacher Distillation There is also a body of work
that studies distilling a student model jointly from multi-
ple teacher models simultaneously [ 2,18,24,33,38,46,
62,64,66,70,73]. Because of the heterogeneous domains
that our teacher models cover, we don’t apply approaches
that marginalize teachers into a uniﬁed label, and instead
map students to each teacher independently using teacher-
speciﬁc projection heads from the uniﬁed student represen-
tation. Although the reason behind this method in [ 73] is
different, we ﬁnd the same overall strategy to be effective.
While [ 57] doesn’t study matching the features of multi-
ple teachers simultaneously, we are able to extend their
paradigm via the different projection heads. To preserve
drop-in compatibility with teacher frameworks, we elimi-
nate the feature normalization in the loss function.
Distilling Foundation Models Foundation Models [ 3] are
meant to be generalist models that are trained on massive
amounts of data, and are typically resource intensive to train
from scratch. In the vein of single-teacher distillation, [ 44]
employ self-distillation to train their smaller variants from
the larger teacher. [ 57] distills their model from a CLIP
[47] teacher. Instead of focusing our energy on one teacher
in particular , we instead grab high-quality versions of CLIP
[47] (using OpenCLIP [ 28]), DINOv2 [ 44], and SAM [ 32].
Concurrently with our work, [ 56] describe a methodology
for merging a CLIP model into a pretrained SAM model via
distillation, which is, in spirit, quite similar to our approach.
In contrast to theirs, we include DINOv2 and also simplify
the objective to straightforward feature matching. Since we
don’t rely on the student model to be pre-trained, it also
gives us the ﬂexibility to have the student be an architecture
distinct from any teacher.
3. Knowledge Agglomeration
We propose a framework to train a vision foundation model
from scratch via multi-teacher distillation as shown in Fig-
ure2. We demonstrate that each teacher brings unique prop-
erties to the foundational vision model, and the resulting
trained model will agglomerate these attributes.
12492
3.1. Overview
As an initial assumption, we expect that the teacher mod-
els are capable of representing a broad swath of images
found on the internet, coming from datasets such as Ima-
geNet (1k or 21k) [ 14], LAION-400M [ 50] or DataComp-
1B [19]. With this in mind, we choose to study 3 seminal
teacher model families: CLIP [ 47], DINOv2 [ 44], and SAM
[32] as they have demonstrated outstanding performance
over a broad range of tasks (as in CLIP), or speciﬁcally
strong performance on downstream dense tasks, such as se-
mantic segmentation under linear probe (as in DINOv2), or
open-vocabulary segmentation (as in SAM). Because these
teacher models come from such diverse domains, we omit
any form of supplemental ground truth guidance and treat
the aforementioned datasets simply as sources of images.
To assess the quality of our models, we adopt a set of repre-
sentative metrics across a few broad domains.
•Image level reasoning: (i) k-NN Top-1 accuracy on
ImageNet-1K, and (ii) Zero-Shot accuracy using the
CLIP teacher’s language model [ 47]. k-NN [ 8,44,58]
embeds the model’s summary feature vector for every im-
age in the training set, and then for each validation image,
it uses a weighted sum of the knearest training vectors to
elect a label.
•Pixel-level visual tasks: segmentation mIOU on (i)
ADE20K and (ii) Pascal VOC - under the linear probe
setting, details in Section 5.3.
•Large Vision-Language Models: we plug our frozen
vision encoder model into LLaV A-1.5 [ 36] and evaluate
it on a wide set of tasks including GQA [ 27], TextVQA
[51], ScienceQA [ 42] and VQAv2 [ 21]. Details in Sec-
tion5.4.
•SAM-COCO instance segmentation: From [ 7], we
adopt their COCO instance segmentation methodology to
evaluate our ability to replicate SAM visual features.
Results on these tasks, both for teacher models and our AM-
RADIO variants, are summarized in Table 1.
3.2. Adaptor Heads
We opt for simplicity in design of the adaptor heads, and
leave alternative architectures as future work. To this end,
we employ a simple 2-layer MLP, with a LayerNorm and
GELU in between. The input dimension is the student em-
bedding dimension, the intermediate dimension is the max-
imum embedding dimension of all teachers, and the output
dimension matches the speciﬁc teacher. For each teacher,
we employ two heads, one for the summary vector, and one
for the spatial features.
3.3. Distillation Dataset Choice
In table 2we study the effect of different datasets on down-
stream metrics. While the highest image classiﬁcation met-
rics are achieved using ImageNet-1K as the training dataset,Dataset k-NN Zero Shot ADE20K
ImageNet 1K 84.79 80.44 48.11
ImageNet 21K 84.61 80.10 48.65
LAION-400M 83.77 77.46 48.6
DataComp-1B 83.91 78.51 49.01
Table 2. Ablation study on the choice of training dataset. We use
MetaCLIP ViT-H/14 [ 15] and DINOv2 ViT-g/14 teachers, and a
ViT-L/14 student model with CPE [ 30]. Both “k-NN” and “Zero
Shot” are for ImageNet-1k. ADE20k refers to mIOU linear probe
on ADE20k.
we argue that it doesn’t fairly measure “zero shot” perfor-
mance as the student directly learns the teacher features
in the evaluation domain. For this reason, we opt for the
DataComp-1B dataset.
3.4. Loss Formulation
Because we don’t have ground truth data for each teacher
for each image, we instead opt to match the features coming
from each teacher’s vision encoder. In particular, we distin-
guish between the summary feature vector and the spatial
feature vectors for each teacher. The summary feature is
computed differently based on the model. For CLIP and
DINOv2, we use the “class token” as the summary feature
vector, and we don’t match a summary for SAM.
Letfpx|⇥0qbe the student vision encoder with parame-
ters⇥0, and ys
i“hpsq
ipx1|⇥psq
iqbe the learned student head
matching teacher summary features zpsq
i“tpsq
ipx| iqwith
student adaptor parameters ⇥psq
iand teacher parameters  i.
x1“fpx|⇥0q; ypsq
i“hpsq
i´
x1|⇥psq
i¯
;
zpsq
i“tpsq
ipx| iq,(1)
Lsummary pxq“ÿ
i iLcospypsq
i,zpsq
iq(2)
We found empirically that cosine distance loss produced
better models compared to L1, MSE, Smooth-L1 [ 20]. Ad-
ditionally, supervising the spatial features of the model by
matching the teacher was not only important for down-
stream dense tasks, but also improved the holistic quality
of our model.
For matching the spatial features, we employ a combi-
nation of cosine similarity and smooth L1. Similar to equa-
tion ( 2) where we found that cosine similarity produced the
best results, we found the same to be true for the spatial
features. However, we want to allow our student model to
be a drop-in replacement in the teacher frameworks, thus
it’s important that we match the magnitude of the teacher
vectors, and so we include smooth L1. In ( 3) we show the
formulation of this loss. Let hpvq
ipx1|⇥pvq
iqbe the learned
12493
TeachersZero Shot k-NN ADE20KNone75.7782.59 41.18CLIP75.64 82.60 44.42DINOv274.6883.0247.05Both74.85 82.9648.13Table 3. Ablation over which teachers we supervise the spatial fea-
tures. We use a ViT-L/14 student model and train on the LAION-
400M dataset. Adding this loss term is always beneﬁcial. DINOv2
appears to provide better spatial features than CLIP, but training
the student to match both teachers produces the best results. We
don’t ablate SAM as we solely want it for its spatial features.
Method Zero Shot k-NN ADE20K
Naive 70.63 79.50 44.71
Uncertainty [ 11] 70.92 79.37 44.57
AdaLoss [ 25] 71.31 79.77 44.36
Table 4. Loss term balancing methods comparison. We use a
ViT-B/14 student, and CLIP+DINOv2 teachers. We found that
AdaLoss produces the best results on the ImageNet tasks, but the
worst on ADE20K.
student head for matching teacher feature vectors, and cor-
responding tpvq
ipx| pvq
iqbe the teacher feature vectors, with
x1“fpx|⇥0q, then the spatial feature loss is:
Lmatchpx, yq“↵L cospx, yq` L smooth-l1 px, yq
Lfeatures pxq“ÿ
i iLmatch´
hpvq
ipx1|⇥pvq
iq,tpvq
ipx| pv
iqq¯
(3)
We choose ↵“0.9and “0.1to mostly rely on the
empirically better cosine distance, but to also match vector
magnitudes.
3.4.1 Loss Balancing
Due to the number of possible combinations of loss weights
between the different teachers, and even which teachers,
and possible formulations of loss functions, we mostly
opted toward naive loss balancing with all teachers equally
weighted for spatial features (  i“1). For summary fea-
tures, we have  CLIP“ DINO“1and SAM“0.
We did experiment with automatic loss balancing using
predicted uncertainty [ 11], AdaLoss [ 25] (momentum 0.99)
and separately with AMTML-KD [ 38], as ways to learn the
balance of  iand i. In the case of AMTML-KD, the model
would always collapse its entire weight around the CLIP
teacher and would yield worse results than naive manual
balancing. Based on the results in table 4, there is very little
advantage to the more exotic balancing schemes, so we opt
for the ”Naive” method throughout the rest of the paper.4. Implementation Details
Performing heterogeneous multi-teacher distillation is not
trivial due to a mismatch in feature dimensions, input res-
olutions, concepts for loss computation, and downsampling
ratios, as well as challenges in ﬁtting multiple teachers into
a single GPU.
General. We train all student models using the AdamW
[41] optimizer, batch size 1024, cosine annealing learning
rate schedule and base learning rate of 0.001. We train for
600k steps, resulting in 614M total examples seen. For our
best student model, we train using DFN CLIP ViT-H/14
378px, OpenAI CLIP ViT-L/14 336px, DINOv2 ViT-g/14
224px, and SAM ViTDet-H 1024px. We apply random
scale + cropping to both student and teacher inputs. We
chose the DataComp-1B dataset due to it having the highest
quality results of the web-scale datasets we had access to.
We train in two stages, ﬁrst with CLIP+DINOv2 for 300k
steps at 256px, and second with CLIP+DINOv2 at 432px
plus SAM at 1024px for 300k steps.
Student architecture. We study two settings for student
model architecture:
•Standard ViT [ 15] architecture to match the architecture
of teachers. Our best model is a ViT-H/16.
•Efﬁcient architecture variants prioritizing high through-
put on GPUs. See Section 5.1.
Multi-scale Teachers. We choose ViT-H/16 architecture
for our student model. To match resolution of SAM fea-
tures, we feed the expected resolution of 10242. Given that
our CLIP and DINOv2 teachers are patch-14 models, we
opt to feed the student 4322inputs, as that is the same ef-
fective resolution as 3782for patch-14. We found that in-
terpolating DINOv2 features doesn’t degrade results, so the
teacher operates at 224px and we upsample the outputs to
match the student.
Rank/Teacher Partitioning. We group teacher models
by (batch size, student resolution), and then distribute the
groups to different GPUs, such that each GPU processes a
consistent batch size and input resolution. We also sample
groups at different rates. For our training setups that in-
clude SAM, we train with 64 GPUs, half of which get the
CLIP+DINOv2 group with batch size 32 per GPU and in-
put resolution 432, and the other half get SAM with batch
size 2 per GPU and input resolution 1024. This results in an
effective batch size of 1,152. For CLIP+DINOv2 training,
we use 32 GPUs, resulting in batch size 1024.
Multi-Resolution ViTs. Many of our student models use
ViT [ 15] as the base vision architecture. Traditionally, ViTs
use a learned position embedding for each input patch in an
image, which in turn enforces that the model always oper-
ates at a constant resolution. We employ the Cropped Posi-
tion Embedding (CPE) [ 30] augmentation with the number
of positions being equal to 1282. The position embeddings
are then randomly cropped and interpolated to match the
12494
Method k-NN ADE20KNon-CPE82.9647.30CPE 82.8448.52Table 5. Comparing identical ViT-L/14 student models, with and
without CPE [ 30] formulation. While the student only ever trains
at2242resolution, CPE allows us to generalize to 5182resolu-
tion, not only improving over non-CPE, but even outperforming
DINOv2-g itself.
Zero Shot k-NN ADE20K VOC VQAv2
CLS token 78.55 83.91 49.01 83.51 77.66
Avgpool 80.12 83.83 38.36 77.04 78.28
Table 6. Comparing identical ViT models, with CLS token and
average pooling summarization.
number of input patches for the student model. Even when
training with CLIP+DINOv2 at 224 resolution, we found
that this technique results in a negligible drop (Table 5) in
summary metrics, but improved semantic segmentation lin-
ear probing mIOU. For heterogeneous-resolution students,
this is a seamless technique that allows ViT to operate at
arbitrary resolutions within some envelope.
High-Resolution ViT Student. In SAM, they employ the
ViTDet [ 34] architecture as a way to reduce the computa-
tional and memory burden of ViT models at high-resolution.
We reformulate this arch instead into a training augmenta-
tion, where we sample a window size from a set of possible
window sizes. This allows us to reduce the computational
burden of training the student model with the SAM teacher,
and, as we make the window size ﬂexible, it provides an
additional throughput scaling mechanism during inference.
Table 8demonstrates our ability to replace SAM’s encoder.
Separately, we found that high resolution training was un-
stable, so we apply spectral reparametrization [ 68] and a
weight decay of 0.02to prevent attention entropy collapse.
Student/Teacher Resolution Mismatch. When the student
and teacher downsample images through their processing
stack at different rates, it results in the output feature vec-
tors having different resolutions. For example, if the teach-
ers use a ViT-H/14 architecture and student a ViT-H/16, it
means that the student outputs a 142feature map, and the
teachers a 162feature map. For Lfeatures we bilinearly in-
terpolate the outputs to match the larger resolution between
the student and teacher features.
Feature Summarization. In3.4we explained how teacher
summary features are extracted using the “class token” of
their respective ViT models. We now turn our attention to
the summarization of student features. ViTs have 2 options:
(i) a separate summarization “CLS” token or (ii) average
pooling patch tokens. We evaluate both options in Table 6.
We observe that average pooling improves summary loss,
but has a more signiﬁcant detrimental effect on the feature
loss. Given the importance of the latter we choose to useBackbone Param. Through- Zero k-NN ADE20k FD loss
Count put Shot
Teachers
DINOv2 G/14 1.14B 313 N/A 83.41 47.53
OpenCLIP H/14 632M 556 77.19 81.10 40.04
Existing Efﬁcient Models
EfﬁcientNetV2-S 21M 9017 65.37 70.72 27.75 0.415
ResNetv2-101 44M 7283 69.58 75.32 29.61 0.405
RegNetY-064 30M 6573 69.84 74.59 28.9 0.394
EfﬁcientViT-L1 38M 6048 71.73 79.90 33.12 0.376
ConvNext-B 88M 1805 75.43 81.73 38.95 0.358
NFNet-F3 254M 1777 76.93 80.50 38.31 0.340
SwinV2-S 49M 1497 74.70 81.12 35.57 0.364
MaxViT-B 119M 1486 77.49 79.34 38.46 0.340
PoolformerV2-M36 56M 1194 74.46 80.49 35.05 0.377
MViTV2-B 51M 975 75.92 81.39 41.39 0.345
Proposed architecture
E-RADIO-B 118M 6422 75.19 82.21 44.03 0.319
ëw/o upsample 113M 7040 75.45 82.05 41.26 0.353
E-RADIO-L 265M 3472 77.87 83.73 45.5 0.265
Table 7. Comparison of backbones. Throughput is measured using
TensorRT 9.0.1 on A100 in mixed FP16/FP32 precision at batch
size 128 on 2242px resolution. Sorted by descending through-
put order. FD loss is the Feature Distillation training loss against
the DINOv2 teacher, it exhibits high correlation with the ADE20k
mIoU. Bolded models form the speed/quality Pareto front.
separate CLS tokens.
5. Results
In this section, we analyze models obtained with the pro-
posed AM-RADIO framework. First, we touch upon back-
bone efﬁciency, then compare with the original teachers
(CLIP, DINOv2, SAM), and benchmark models under vi-
sion question answering in the LLaVa framework. We will
see that the proposed models outperform the original teach-
ers in multiple metrics, including throughput. Results are
shown in Figure 2and Table 1.
5.1. Efﬁcient Students
We aim to ﬁnd an efﬁcient model architecture to speed up
the inference of VFM. There are a number of architectural
designs aimed at high throughput on GPU devices. We
use our distillation framework to evaluate several backbones
with no change in training hyperparameters.
Upon reviewing the literature on efﬁcient vision back-
bones focused for high GPU throughput, we pick the fol-
lowing list of architectures: EfﬁcientNetV2 [ 54], ResNetv2
[53], RegNetY [ 48], FasterViT [ 22], EfﬁcientViT [ 7], Con-
vNext [ 40], NFNet [ 6], SwinV2 [ 39], MaxViT [ 55], Pool-
formerV2 [ 65] and MViTV2 [ 35]. We train all the back-
bones via distillation on the ImageNet-21k dataset, using
OpenCLIP ViT-H/14 (laion2B-s32B-b79K) and DINOv2
g/14 as teachers. Results are compiled in Table 7.
We observe that many models lag behind teachers. Addi-
tionally, CNN-like models are signiﬁcantly faster than ViTs,
12495
Figure 3. All models followed the same training protocol. The results from three benchmarks show that RADIO and E-RADIO models
outperform others in efﬁciency. This under-performance in other models might be due to overﬁtting architectures on supervised ImageNet-
1K training. E-RADIO notably delivers results 10 times faster and with a 20% improvement over teacher models. We study E-RADIO at
224px resolution, with a window size of 7.
while the latter are more accurate. The relatively low perfor-
mance of existing efﬁcient backbones on the dense ADE20k
segmentation task is not unexpected since all of them apply
a spatial dimension reduction factor of 32 for ﬁnal feature
maps of size 72for input resolution of 2242px, thus hardly
capable of capturing ﬁne-grain spatial information.
E-RADIO: To overcome this issue, we propose a novel
hybrid architecture, named E-RADIO (Efﬁcient RADIO).
This design borrows ideas from existing literature and in-
cludes an input stem with strided convolutions to downsam-
ple the input image by 4x. It then proceeds with 2 stages
of YOLOv8 C2f convolution blocks and 2 stages of trans-
former. For the transformer variant we pick windowed at-
tention (like in SWIN [ 39]), and interleave local windowed
attention with “global” windowed attention as done in [ 22]
and ViTDet [ 34]. To perform “global” attention we ﬁrst
downsample the feature map by 2x, apply windowed atten-
tion, and then upsample the feature maps back to the origi-
nal resolution. Up-/down-sampling is performed by strided
convolution with a kernel size 3x3 and a stride of 2. The
last idea is borrowed from EdgeViT [ 45], which uses local-
global-local attention. See Appendix for details. Finally,
E-RADIO upsamples ﬁnal feature maps by 2x via a decon-
volutional layer and adds them to feature maps from the
third stage, resulting in only a 16x spatial resolution re-
duction. Such upsampling gives an improvement in dense
task while being only 10% slower. Results of E-RADIO in
Table 7demonstrate that the proposed architecture signiﬁ-
cantly outperforms the competition, and can be seen as an
efﬁcient replacement for the much slower full ViT.5.2. Comparison with teachers
A comprehensive set of results is presented in Table 1.W e
notice that MetaCLIP is better than OpenCLIP, and DFN
CLIP better than MetaCLIP. DINOv2 provides important
properties for dense tasks: ADE20k and VOC. Our E-
RADIO-L model is signiﬁcantly faster than all ViT mod-
els. At the same time, it strongly outperforms MetaCLIP
on most metrics at matched throughput, while also enabling
Zero-shot capability that is absent in DINOv2 and SAM.
Our full model, ViT-H/16, is as fast as the teachers but out-
performs them on 6 out of 9 tasks, demonstrating the efﬁ-
ciency of the proposed distillation framework.
Drop-In SAM Replacement. Following [ 7], we use their
evaluation harness to compute the mIOU for instance seg-
mentation using pretrained SAM with vision encoder re-
placed by our model. Table 8shows the results of the
COCO Instance Segmentation task using the baseline SAM
models and RADIO.
5.3. Semantic Segmentation Linear Probing
We train a linear head on top of the frozen features of the
teachers and students alike and evaluate performance in the
MMSeg [ 12] framework using the mIoU metric on ADE20k
and PascalVOC2012 datasets. We use a training and eval-
uation crop size of 512 for RADIO, 518 for DINOv2, and
the native resolution for the others. We use the “slide” eval-
uation mode with a stride of2
3the crop size. We train the
linear head for 160k steps using a total batch size of 16, a
base learning rate of 10´3and the AdamW optimizer.
12496
COCO 2017 drop-in SAM replacement at 1024x1024Family Arch mIOU ThroughputSAMBase 75.78 50.94Large 77.02 20.62Huge 77.18 11.83E-RADIO (ours) Large 76.31 121.74RADIO (ours)ViTDet-H/16-W8:76.09 29.09ViTDet-H/16-W16:76.23 27.91Table 8. We substitute SAM’s vision encoder with our RADIO
model. RADIO aligns with SAM’s features just before the en-
coder’s neck layer. We also examine the impact of varying ViT-
Det window sizes. Differences in throughput owe to the fact that
RADIO doesn’t use relative positional embeddings and we re-
duced shufﬂing with our patch reordering algorithm (in appendix).
Throughput is computed on an NVIDIA A100 GPU using Ten-
sorRT and batch size 16.:This is the same model, just with a
different window size setting.
5.4. Visual Question Answering
We replace the vision encoder in a LLaV A 1.5[ 36] setup
with our own encoder. A 2-layer MLP is used to project
frozen visual features into the language token space. Under
the default LLaV A 1.5 settings, we pretrain a multimodal
projection MLP and then run instruction tuning to ﬁnetune
a Vicuna 7B-1.5 model[ 71]. We evaluate models using the
validation sets of GQA [ 27], TextVQA [ 51], POPE [ 63]
(popular), and we score the model on the Test-Dev set of
VQAv2 [ 21] using EvalAI[ 61]. We use the vision encoder’s
native input resolution, resizing the long edge and padding
the short edge. Experimental results are compiled in Ta-
ble1. Owing to the increased input resolution ﬂexibility of
RADIO, we resize the long edge of the image to 432px as-
pect preserving, only padding to the nearest multiple of the
patch size. This results in 462 tokens on average, versus
the576tokens required by the 336px patch-14 encoders, a
20% reduction.
6. Conclusion and Key Insights
Most VFMs have unique properties such as language
grounding (CLIP), dense correspondences (DINOv2), and
detailed segmentation (SAM), but also large holes in capa-
bility. Distillation allows uniting all these properties in a
single model that often outperforms any of the teachers. We
have also observed that better teachers yield better students,
which allows RADIO to absorb and challenge the current
SOTA foundation models at a given point in time.
Feature distillation loss. We observe the crucial impor-
tance of full feature distillation to boost the performance of
the teacher in dense image understanding tasks, such as an
18% relative improvement on ADE20K.
SAM vs DINOv2. We ﬁnd that, out of the box, SAM is not
well-suited for downstream tasks, whereas DINOv2 signif-
icantly outperforms in zero- and few-shot tasks. For exam-
Figure 4. RADIO “mode switches” when resolution is increased.
In the plot, we show the MSE error between the RADIO features
coming from its DINOv2 head at different resolutions, versus the
features actually produced by DINOv2 at 518px. We bilinearly
interpolate the RADIO features to match the DINOv2 feature res-
olution. At 720px, there is a sudden jump in the error, which cor-
responds with a complete change in color space in the image.
ple, ADE20K segmentation via linear probing is 1.7x bet-
ter with the latter, and the ImageNet1k k-NN metric is 4x
better. SAM excels in detecting edges and segmenting ob-
jects but performs poorly in high-level object description
and combining the semantics of multiple objects (Figure 3).
Dense features. As seen in ﬁgure 1, RADIO is capable of
producing high resolution and low-noise features. Given the
adaptor heads, it is able to reasonably reproduce the features
generated by the teachers, and the backbone features them-
selves are highly coherent. An issue we identiﬁed, however,
shown in ﬁgure 4is that RADIO appears to have a latent
‘low resolution’ and ‘high resolution’ mode, likely due to
the partitioned training between CLIP+DINO and SAM ob-
jectives, which we intend to ﬁx in future work.
Efﬁcient backbone. Based on our analysis of distilling ef-
ﬁcient backbones, we conclude that most model designs are
overly tailored towards supervised training on ImageNet1K,
and as a result, do not scale well to VFM settings. We
designed a new vision backbone, E-RADIO, with a hy-
brid CNN-Transformer architecture that improves upon the
Pareto frontier.
12497
References
[1]S. Ahn, S. Hu, A. Damianou, N. D. Lawrence, and Z. Dai.
Variational information distillation for knowledge transfer.
In2019 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 9155–9163, Los Alamitos,
CA, USA, 2019. IEEE Computer Society. 2,3
[2]Umar Asif, Jianbin Tang, and Stefan Harrer. Ensemble
knowledge distillation for learning improved and efﬁcient
networks. In European Conference on Artiﬁcial Intelligence ,
2019. 3
[3]Muhammad Awais, Muzammal Naseer, Salman Khan,
Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah,
Ming-Hsuan Yang, and Fahad Shahbaz Khan. Foundational
models deﬁning a new era in vision: A survey and outlook,
2023. 2,3
[4]Jimmy Ba and Rich Caruana. Do deep nets really need to
be deep? In Advances in Neural Information Processing
Systems , pages 2654–2662, 2014. 3
[5]L. Beyer, X. Zhai, A. Royer, L. Markeeva, R. Anil, and A.
Kolesnikov. Knowledge distillation: A good teacher is pa-
tient and consistent. In 2022 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 10915–
10924, Los Alamitos, CA, USA, 2022. IEEE Computer So-
ciety. 3
[6]Andrew Brock, Soham De, Samuel L. Smith, and Karen Si-
monyan. High-performance large-scale image recognition
without normalization, 2021. 6
[7]Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han.
Efﬁcientvit: Multi-scale linear attention for high-resolution
dense prediction, 2023. 2,4,6,7
[8]Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´eJ´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers, 2021. 4
[9]Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,
Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,
Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng
Dai. Internvl: Scaling up vision foundation models and
aligning for generic visual-linguistic tasks. arXiv preprint
arXiv:2312.14238 , 2023. 2
[10] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell
Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-
mann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-
ing laws for contrastive language-image learning, 2022. 2
[11] R. Cipolla, Y . Gal, and A. Kendall. Multi-task learning using
uncertainty to weigh losses for scene geometry and seman-
tics. In 2018 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 7482–7491, Los Alami-
tos, CA, USA, 2018. IEEE Computer Society. 5,7
[12] MMSegmentation Contributors. MMSegmentation:
Openmmlab semantic segmentation toolbox and
benchmark. https : / / github . com / open -
mmlab/mmsegmentation , 2020. 7
[13] Timoth ´ee Darcet, Maxime Oquab, Julien Mairal, and Piotr
Bojanowski. Vision transformers need registers, 2023. 2
[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE Conference on Computer Vision and
Pattern Recognition , pages 248–255, 2009. 4
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale, 2021. 4,5
[16] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.
Williams, J. Winn, and A. Zisserman. The pascal visual ob-
ject classes challenge: A retrospective. International Journal
of Computer Vision , 111(1):98–136, 2015. 2
[17] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig
Schmidt, Alexander Toshev, and Vaishaal Shankar. Data ﬁl-
tering networks, 2023. 2
[18] Takashi Fukuda, Masayuki Suzuki, Gakuto Kurata, Samuel
Thomas, Jia Cui, and Bhuvana Ramabhadran. Efﬁcient
knowledge distillation from an ensemble of teachers. In In-
terspeech , 2017. 3
[19] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan
Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,
Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Or-
gad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek
Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen
Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna,
Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran
Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,
Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon,
Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In
search of the next generation of multimodal datasets, 2023.
4
[20] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision , pages 1440–1448,
2015. 4
[21] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the V in VQA matter: Ele-
vating the role of image understanding in Visual Question
Answering. In Conference on Computer Vision and Pattern
Recognition (CVPR) , 2017. 4,8
[22] Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao,
Jose M. Alvarez, Jan Kautz, and Pavlo Molchanov. Fastervit:
Fast vision transformers with hierarchical attention, 2023. 6,
7
[23] B. Heo, J. Kim, S. Yun, H. Park, N. Kwak, and J. Choi.
A comprehensive overhaul of feature distillation. In 2019
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 1921–1930, Los Alamitos, CA, USA, 2019.
IEEE Computer Society. 2,3
[24] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 1,2,3
[25] Hanzhang Hu, Debadeepta Dey, Martial Hebert, and J. An-
drew Bagnell. Learning anytime predictions in neural net-
works via adaptive loss balancing. In Proceedings of the
Thirty-Third AAAI Conference on Artiﬁcial Intelligence and
Thirty-First Innovative Applications of Artiﬁcial Intelligence
Conference and Ninth AAAI Symposium on Educational Ad-
vances in Artiﬁcial Intelligence . AAAI Press, 2019. 5,8
12498
[26] Zehao Huang and Naiyan Wang. Like what you like:
Knowledge distill via neuron selectivity transfer. CoRR ,
abs/1707.01219, 2017. 2,3
[27] Drew A. Hudson and Christopher D. Manning. GQA: a
new dataset for compositional question answering over real-
world images. CoRR , abs/1902.09506, 2019. 4,8,9,10,
11
[28] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-
naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-
clip, 2021. 3
[29] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics
yolov8, 2023. 2
[30] Dahun Kim, Anelia Angelova, and Weicheng Kuo. Region-
aware pretraining for open-vocabulary object detection with
vision transformers, 2023. 4,5,6
[31] Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphrasing
complex network: Network compression via factor transfer.
InProceedings of the 32nd International Conference on Neu-
ral Information Processing Systems , page 2765–2774, Red
Hook, NY , USA, 2018. Curran Associates Inc. 3
[32] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and
Ross Girshick. Segment anything, 2023. 2,3,4
[33] Xu Lan, Xiatian Zhu, and Shaogang Gong. Knowledge dis-
tillation by on-the-ﬂy native ensemble, 2018. 3
[34] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.
Exploring plain vision transformer backbones for object de-
tection, 2022. 6,7
[35] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man-
galam, Bo Xiong, Jitendra Malik, and Christoph Feichten-
hofer. Mvitv2: Improved multiscale vision transformers for
classiﬁcation and detection, 2022. 6
[36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning, 2023. 2,
3,4,8
[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning, 2023. 3
[38] Yuang Liu, Wei Zhang, and Jun Wang. Adaptive multi-
teacher multi-level knowledge distillation. Neurocomputing ,
415:106–113, 2020. 3,5
[39] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu
Wei, and Baining Guo. Swin transformer v2: Scaling up
capacity and resolution, 2022. 6,7,2
[40] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s, 2022. 6
[41] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. In International Conference on Learning
Representations , 2019. 5
[42] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei
Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and
Ashwin Kalyan. Learn to explain: Multimodal reasoning
via thought chains for science question answering. In The36th Conference on Neural Information Processing Systems
(NeurIPS) , 2022. 4
[43] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir
Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Im-
proved knowledge distillation via teacher assistant. In AAAI
Conference on Artiﬁcial Intelligence , 2019. 3
[44] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah-
moud Assran, Nicolas Ballas, Wojciech Galuba, Russell
Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael
Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv ´e Je-
gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr
Bojanowski. Dinov2: Learning robust visual features with-
out supervision, 2023. 2,3,4
[45] Junting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz
Dudziak, Hongsheng Li, Georgios Tzimiropoulos, and Brais
Martinez. Edgevits: Competing light-weight cnns on mobile
devices with vision transformers. In ECCV , 2022. 7,2
[46] Seonguk Park and Nojun Kwak. Feature-level ensemble
knowledge distillation for aggregating knowledge from mul-
tiple networks. In European Conference on Artiﬁcial Intelli-
gence , 2020. 3
[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In Proceedings
of the 38th International Conference on Machine Learning ,
pages 8748–8763. PMLR, 2021. 2,3,4
[48] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,
Kaiming He, and Piotr Doll ´ar. Designing network design
spaces, 2020. 6
[49] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets. CoRR , abs/1412.6550, 2014. 2,3
[50] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-ﬁltered 400 million image-text pairs,
2021. 4
[51] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,
Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus
Rohrbach. Towards vqa models that can read. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , 2019. 4,8,12,13,14
[52] X. Sun, R. Panda, C. Chen, A. Oliva, R. Feris, and K.
Saenko. Dynamic network quantization for efﬁcient video
inference. In 2021 IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 7355–7365, Los Alamitos,
CA, USA, 2021. IEEE Computer Society. 2,3
[53] Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke.
Inception-v4, inception-resnet and the impact of residual
connections on learning. CoRR , abs/1602.07261, 2016. 6
[54] Mingxing Tan and Quoc V . Le. Efﬁcientnetv2: Smaller mod-
els and faster training. CoRR , abs/2104.00298, 2021. 6
12499
[55] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,
Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit:
Multi-axis vision transformer, 2022. 6
[56] Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash
Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin
Mehta, Mohammad Rastegari, Oncel Tuzel, and Hadi
Pouransari. Sam-clip: Merging vision foundation models
towards semantic and spatial understanding, 2023. 3,7,8
[57] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao,
Jianmin Bao, Dong Chen, and Baining Guo. Contrastive
learning rivals masked image modeling in ﬁne-tuning via
feature distillation, 2022. 2,3
[58] Zhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua Lin.
Unsupervised feature learning via non-parametric instance
discrimination. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , 2018. 4
[59] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V .
Le. Self-training with noisy student improves imagenet clas-
siﬁcation. In 2020 IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 10684–10695,
2020. 2
[60] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang,
Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh,
Luke Zettlemoyer, and Christoph Feichtenhofer. Demystify-
ing clip data. 2023. 2
[61] Deshraj Yadav, Rishabh Jain, Harsh Agrawal, Prithvijit Chat-
topadhyay, Taranjeet Singh, Akash Jain, Shiv Baran Singh,
Stefan Lee, and Dhruv Batra. Evalai: Towards better evalua-
tion systems for ai agents, 2019. 8
[62] Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin
Jiang. Model compression with two-stage multi-teacher
knowledge distillation for web question answering system.
InProceedings of the 13th International Conference on Web
Search and Data Mining , page 690–698, New York, NY ,
USA, 2020. Association for Computing Machinery. 3
[63] Kun Zhou Jinpeng Wang Wayne Xin Zhao Yifan Li, Yi-
fan Du and Ji-Rong Wen. Evaluating object hallucination
in large vision-language models. In The 2023 Conference on
Empirical Methods in Natural Language Processing , 2023.
8
[64] Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning
from multiple teacher networks. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge Dis-
covery and Data Mining , page 1285–1294, New York, NY ,
USA, 2017. Association for Computing Machinery. 3
[65] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou,
Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer
is actually what you need for vision, 2022. 6
[66] Fei Yuan, Linjun Shou, Jian Pei, Wutao Lin, Ming Gong,
Yan Fu, and Daxin Jiang. Reinforced multi-teacher selection
for knowledge distillation, 2020. 3
[67] Sergey Zagoruyko and Nikos Komodakis. Paying more at-
tention to attention: Improving the performance of convolu-
tional neural networks via attention transfer. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Pro-
ceedings . OpenReview.net, 2017. 2,3[68] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan
Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and
Joshua M Susskind. Stabilizing transformer training by pre-
venting attention entropy collapse. In International Con-
ference on Machine Learning , pages 40770–40803. PMLR,
2023. 6
[69] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
Lucas Beyer. Sigmoid loss for language image pre-training.
arXiv preprint arXiv:2303.15343 , 2023. 2
[70] Haoran Zhao, Xin Sun, Junyu Dong, Changrui Chen, and
Zihe Dong. Highlight every step: Knowledge distillation via
collaborative teaching. IEEE Transactions on Cybernetics ,
52(4):2070–2081, 2022. 3
[71] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 8
[72] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Barriuso, and Antonio Torralba. Scene parsing through
ade20k dataset. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 5122–5130,
2017. 2
[73] Konrad Zuchniak. Multi-teacher knowledge distillation as an
effective method for compressing ensembles of neural net-
works, 2023. 2,3
12500
