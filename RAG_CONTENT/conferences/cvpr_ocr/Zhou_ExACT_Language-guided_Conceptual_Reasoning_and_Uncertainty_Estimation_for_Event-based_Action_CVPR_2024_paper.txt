ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation
for Event-based Action Recognition and More
Jiazhou Zhou1Xu Zheng1Yuanhuiyi Lyu1Lin Wang1,2*
1AI Thrust, HKUST(GZ)2Dept. of CSE, HKUST
{jiazhouzhou,yuanhuiyilv }@hkust-gz.edu.cn, zhengxu128@gmail.com, linwang@ust.hk
Project Page: https://vlislab22.github.io/ExACT/
Abstract
Event cameras have recently been shown beneficial for
practical vision tasks, such as action recognition, thanks
to their high temporal resolution, power efficiency, and re-
duced privacy concerns. However, current research is hin-
dered by 1) the difficulty in processing events because of
their prolonged duration and dynamic actions with complex
and ambiguous semantics and 2) the redundant action de-
piction of the event frame representation with fixed stacks.
We find language naturally conveys abundant semantic in-
formation, rendering it stunningly superior in reducing se-
mantic uncertainty. In light of this, we propose ExACT, a
novel approach that, for the first time, tackles event-based
action recognition from a cross-modal conceptualizing per-
spective. Our ExACT brings two technical contributions.
Firstly, we propose an adaptive fine-grained event (AFE)
representation to adaptively filter out the repeated events
for the stationary objects while preserving dynamic ones.
This subtly enhances the performance of ExACT without
extra computational cost. Then, we propose a conceptual
reasoning-based uncertainty estimation module, which sim-
ulates the recognition process to enrich the semantic rep-
resentation. In particular, conceptual reasoning builds the
temporal relation based on the action semantics, and uncer-
tainty estimation tackles the semantic uncertainty of actions
based on the distributional representation. Experiments
show that our ExACT achieves superior recognition accu-
racy of 94.83%(+2.23%), 90.10%(+37.47%) and 67.24%
on PAF , HARDVS and our SeAct datasets respectively.
1. Introduction
Action recognition is a crucial vision task with many ap-
plications, such as robot navigation [33, 43], and abnor-
mal human behavior recognition [25, 35]. Many frame-
based learning approaches have been presented, leading
to impressive performance improvements [24, 43]. How-
*Corresponding author.
Sitting?
Standing?Squatting?Uncertainty 
Estimation 
Temporal RelationConceptual 
Reasoning Contrastive 
Alignment(c)Our AEF representation VS.
t
Previous event frame rep. (b)
tStationary Object: 
DragonflyDynamic  Action:  
Sit down
1) Duration:
2) Semantic:Prolonged Avg. 5s
Ambiguous 
&ComplexShort Avg. 0.1s
Limited(a)
ComparisonFigure 1. (a)Unlike stationary objects, e.g.,‘Dragonfly’ with
short duration (0.1s) and limited semantics, dynamic actions like
‘Sit down’ have the prolonged duration (5s) with ambiguous
and complex semantics. (b)Compared with previous event rep-
resentation, stacking events with fixed counts, we adaptively filter
out events recording stationary actions while preserving dynamic
ones; (c)We introduce language guidance to stimulate the recogni-
tion process, particularly focusing on conceptually reasoning tem-
poral relations and estimating uncertain semantics.
ever, these methods may not be ideal solutions for power-
constrained scenarios, e.g., surveillance [3, 24]. RGB cam-
eras also degrade due to environment bias [36] like motion
blur and lighting variations. Moreover, frame-based cam-
eras trigger considerable privacy concerns as they directly
capture users’ appearance.
Recently, bio-inspired event cameras are gaining popu-
larity [1, 14, 27], which ignore the background and only
record moving objects. This leads to sensing efficiency and
resilience to rapid motion and illumination changes with
low power consumption. Also, event cameras mostly reflect
objects’ edges, which alleviates users’ privacy concerns like
skin color and gender. Owing to these advantages, event-
based action recognition offers more pragmatic solutions
for real-world implementations. This has inspired research
endeavors [14, 27, 36, 39, 49, 54, 55] in event-based action
recognition areas with plausible performance.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18633
However, the above methods are deficient for two rea-
sons: 1) They have a limited capacity for recognizing a
large number of different human actions, as demonstrated
by experiments on the HARDVS [48] dataset with 300 cate-
gories in Tab. 1. This probably arises from the complex and
ambiguous semantics induced by the dynamic actions and
prolonged duration (around 5s [48]), compared to objects
with short duration (around 0.1s [48]) and limited seman-
tics. For example, as shown in Fig. 1 (a), ‘Dragonfly’
vs.‘Sit down’ . 2) The lack of tailored event representa-
tionas the raw events are directly stacked into event frames
with fixed stacks, resulting in event frames with either over-
lapped or vague edge information depicting the same action,
see Fig. 3 and Fig. 1 (b).
Recent advancements in vision-language models
(VLMs) [6, 11, 37] have pioneered ideas that incorporate
semantic concepts across text and vision modalities, aiming
at simulating the human processes of conceptualization and
reasoning [19, 41, 42]. The key insight is that language
naturally conveys inherent semantic richness, which can
be beneficial for modeling semantic uncertainty and
establishing complex semantic relations . Inspired by this,
we introduce language as guidance for event-based action
recognition. As the first exploration, the research hurdles
include: 1) How to represent events to depict dynamic
actions in detail without redundant event frames? 2) How
to integrate text embeddings with event embeddings to help
reason complex semantics of dynamic actions and reduce
semantic uncertainty?
To this end, we propose a novel ExACT framework to
tackle event-based action recognition from a cross-modal
conceptual reasoning perspective, as depicted in Fig. 1 (b)
and (c). To address the first challenge, an Adaptive Fine-
grained Event (AFE) representation (Sec. 3.1) is inspired
by the ‘overlapped action regions’ observed in Fig. 3. These
regions indicate an excessive stack of events in one frame,
which is inevitable for previous frame-based representa-
tions with fixed stacks. Differently, our AFE recursively and
offline find the dividing line of different actions based on the
overlapped regions. It eliminates repeated events and pre-
serves dynamic actions, thus enhancing model performance
without extra computational costs (Tab. 2).
For the second challenge, we propose a novel Conceptual
Reasoning-based Uncertainty Estimation (CRUE) module
(Sec. 3.3) to simulate the action recognition process of hu-
mans. Concretely, CRUE initially establishes the tempo-
ral relation of event frames by leveraging the text embed-
dings to reason each frame’s semantics and then obtain
the fused event embeddings. Subsequently, CRUE con-
verts event and text embeddings from discrete representa-
tion to distributional representation, where the distribution
variance quantifies semantic uncertainty. In this way, our
proposed CRUE module establishes a semantic-abundantand uncertainty-aware embedding space to enhance model
performance (Tab. 3).
Meanwhile, as existing datasets solely provide category-
level labels, we propose the SeAct dataset, consisting of
58 categories of actions, with semantic-abundant caption-
level labels. Our dataset serves as the first dataset for event-
text action recognition (67.24% accuracy). We also con-
duct extensive experiments to show that our ExACT out-
performs previous methods , e.g., [14, 26] on the public
datasets, PAF [34] 94.83% accuracy (+2.23%) and espe-
cially on HARDVS [48] 90.10% accuracy (+37.47%) by a
large margin. Beyond action recognition, our ExACT can
be flexibly applied to the event-text retrieval task.
In summary, our main contributions are: (I) We propose
the ExACT– the first framework utilizing language guid-
ance for event-based action recognition; (II) We propose the
CRUE module to mimic human action recognition, creating
a rich, uncertainty-aware cross-modal embedding space for
action recognition. Also, our AFE representation adaptively
filters redundant events, yielding effective representation
for dynamic actions. (III) We introduce the SeAct dataset
with detailed text captions for evaluating the recognizing
ability of actions composed of multiple sub-actions with
different semantics. Extensive experiments demonstrate the
superiority of our ExACT framework on our SeAct dataset
and public datasets.
2. Related Works
Event-based Action Recognition methods can be catego-
rized into the event-only and event-other-modality meth-
ods. For event-only frameworks, the most widespread
techniques involve stacking the event stream into com-
pact frames, followed by the utilization of off-the-shelf
Convolutional Neural Networks (CNNs) [14] or Vision
Transformers (ViTs) [39, 55] for effective feature extrac-
tion. This approach currently exhibits state-of-the-art per-
formance owing to the excellent CNN/ViT backbones per-
formance. Meanwhile, considering the asynchronous char-
acteristics of event data, the research community has ex-
plored the applicability of bio-inspired Spiking Neural Net-
works (SNNs) [27] and the spatiotemporal capabilities of
Graph Convolutional Networks (GCNs) [54] to more aptly
resonate with the unique structure of event data. However,
these approaches have yielded suboptimal performance and
exhibited limited adaptability, partly due to the specialized
hardware requisites inherent to SNNs.
Concurrently, there have been endeavors to integrate
event data with additional modalities. For example, incor-
porating the abundance of color and texture information in
RGB [49] data with event information or utilizing the sup-
plementary motion knowledge in optical flow [36]. In sum-
mary, most of these approaches rely on dense consecutive
event frames, inevitably resulting in redundant frames with
18634
32Event 
Encoder 
Conceptual 
Reasoning -based 
Uncertainty 
Estimation 
Module
Trainable
FrozenEvent stream
Event [ CLS] 
Embedding
Text [ CLS] 
EmbeddingJoint Event -Text 
Embedding SpaceAdaptive Fine -grained 
Event Representation
Sec. 3.1 TSampled Event Frames…
Text 
Encoder 
𝑉1𝑉2𝑉3 𝑉𝑀 …
Learnable Prompt 𝑃lLabelSit Down. Hand -crafted Prompt 𝑃ℎA series of photos 
recoding human 
action for
𝑓𝑡ℎ
𝑓𝑡𝑙𝑓𝑡𝑓𝑒
Sec. 3.3 
t
T Figure 2. Overall framework of our proposed ExACT framework. It consists of four components: (1) the AFE representation re-
cursively eliminates repeated events and generates event frames depicting dynamic actions (Sec. 3.1); (2) the event encoder and (3) the
text encoder, responsible for the event and text embedding, respectively (Sec. 3.2); (4) the CRUE module simulates the action recognition
process to establish the complex semantic relations for sub-actions and reduce the semantic uncertainty. (Sec. 3.3)
overlapped actions and uncertain semantics. To represent
events to depict detailed dynamic actions, the AFE rep-
resentation is proposed to sample event frames adaptively
without introducing extra computation costs.
Vision-laguage Models (VLMs) Recently, there has been
a growing interest in large-scale pre-trained VLMs [6, 11]
for multimodal representations. Inspired by it, several pi-
oneering works [7, 52, 58] have investigated the poten-
tial for transferring VLMs’ capability to the event modal-
ity, thereby revitalizing the best performance of objection
recognition. In addition, the remarkable zero-shot capabil-
ity of VLMs has motivated researchers to explore event-
based label-free [7] or zero (few)-shot applications [58],
thus addressing the scarcity of high-quality event datasets.
Nevertheless, prior event-text methods focus on recognizing
objects with limited semantics but fail to recognize events
recording actions with prolonged time duration and com-
plex and ambiguous semantics. Therefore, Our ExACT
aims at enhancing event-based action recognition from a
cross-modal conceptual reasoning perspective.
3. The Proposed ExACT Framework
Overview. An overview of our ExACT framework is de-
picted in Fig. 2. The key idea of ExACT is to introduce
language as guidance for estimating semantic uncertainty
and establishing semantic relations for event-based action
recognition. The following subsections explain the techni-
cal details of 1) the proposed Adaptive Fine-grained Event
(AFE) representation (Sec. 3.1); 2) the event encoder and
the text encoder (Sec. 3.2); 3) the Conceptual Reasoning-
based Uncertainty Estimation (CRUE) module (Sec. 3.3).
Besides, in Sec. 3.5, we introduce our proposed semantic-
abundant event-based action recognition ( SeAct ) dataset as
the first dataset for event-text action recognition.
33Sec. 3.2.1  Adaptive Fine -grained Event Frame Representation
Event Stream  𝐸00
…… ……
(a) Study CaseRepetitive frames
The observed 
‘overlapped 
actions regions’ Informative frames with dynamic actions 
𝐼10−𝐼11Diff Map:
M Times Recursion
(b) Algorithm Flow
Motivating
𝐼𝑀0𝐼𝑀4
t
𝐼𝑀1𝐼𝑀2𝐼𝑀3𝐼𝑀5
t
Event Stream 𝐸10𝐼10
t
Event Stream 𝐸11𝐼11
t
R≥Δ&&𝑁𝑠𝑢𝑏≥𝑁min ? 
√❓
××
×××Figure 3. (a)Unlike existing methods often lead to repetitive event
frames, our AFE representation adaptively filters out repetitive
events for the same action based on the observed overlapped ac-
tion regions; (b)Illustration of the AFE representation.
3.1. The AFE Representation
Most event-based action recognition models [14, 39, 55]
primarily rely on event frame representations [31, 56],
which is compatible with off-the-shelf CNN/ViT back-
bones. For these models, the event stream is spatially in-
tegrated into frames with fixed event counts or time dura-
tion [57], as shown in Fig. 1 (b). However, the high tempo-
ral resolution of event data inevitably leads to a profusion of
repetitive event frames displaying the same stationary ob-
jects (refer to Fig. 3 (a) blue circles). Consequently, it is ar-
duous for such representation to depict dynamic actions. To
this end, we seek to answer the following question: Can we
adaptively filter out repetitive events for stationary objects
while preserving events recording the dynamic actions?
Accordingly, we visualize the previous event frame rep-
resentation [57] in Fig. 3 (a). A salient observation is the
‘overlapped action region’, as marked by the red square,
18635
which is a byproduct of the frame transformation process
where consecutive actions overlap due to excessive event
stacks. This ‘overlapped action region’ thus serves as a piv-
otal indicator for inappropriate event sampling intervals. In
light of this, we propose the AFE representation.
Specifically, to find the most appropriate dividing line of
different actions based on the ‘overlapped action regions’,
we adopt the classic binary search and implement it offline
and recursively with efficient O(logn)algorithm complex-
ity. As illustrated in Fig. 3 (b), the search algorithm can be
seen as finding leaf nodes of a binary tree. To begin with,
we equally divide the event stream E0
0(root) into two sub-
streams E0
1(node) and E1
1(node) and thus generate their
corresponding event count images I0
1andI1
1. Then, we
subtract I0
1withI1
1to obtain the difference map. Next, to
measure the proportion of overlapped actions based on the
difference map, we define a factor named deference rate R,
given by:
R=sum(abs(I0
1−I1
1))/(sum(E0
0)/2), (1)
where the sum(.)andabs(.)functions indicate the event
counts and absolute value operations, respectively. Intu-
itively, the high value of Rindicates a high probability
of stacking events recording two different actions into one
frame. In this case, we need to divide the event sub-stream
recursively.
For the recursive algorithm, the boundary conditions are
important. In our cases, if the difference rate Ris higher
than the lowest sampling threshold ∆, we repeat the above
division process until it’s lower than ∆or the event count
number of the sub-stream Nsubis less than the minimum
aggregating event count number Nmin. Note that hyper-
parameters Nminand∆vary with different datasets. More
discussions about selecting Nminand∆refer to the Sup-
plmat. After the above searching process with Mtimes
recursion, we finally obtain a series of fine-grained event
frames IT
MwithTevent frames (all leaf nodes).
3.2. Feature Encoding
With the AFE representation, the event stream is processed
into a series of fine-grained event frames IT
M. Then, the
event encoder and text encoder from the pre-trained event-
text model of [58] are utilized to establish a unified event-
text embedding space.
Event Encoder. As shown in Fig. 2, it inputs an RGB event
frame Ii
M∈RH×W×3, i= 1,2, ..., T of the spatial size
H×Wand outputs the event embedding fi
e. Given Tevent
frames, the event encoder processes Ttimes and generates
the event [ CLS] embeddings fe∈RT×He×We×Ne.
Text Encoder. It takes two different kinds of text prompts
as input: 1)the hand-crafted text prompt ‘A series
of photos recording action for [CLASS ].’,
where [ CLASS ] represents the category name. After encod-
ing, each word is converted into the Dp-dimension word
Uncertainty Estimation𝑓𝑒𝑓𝑢𝑠𝑒Text
𝑓𝑡Event
𝑓𝑒
Softmax
𝑓𝑒𝑝
𝑓𝑡𝑝
Conceptual 
Reasoning 
WeightsMLP MLP
…Conceptual Reasoning -based Fusion
Distributional 
Representation
Discrete
Representation
Point
MultiplicationCross
Multiplication𝑁𝑒𝜇𝑒,𝜎𝑒2 𝑁𝑡𝜇𝑡,𝜎𝑡2
𝑓𝑠𝑎𝑚𝑝𝑙𝑒,𝑒𝑛 𝑓𝑠𝑎𝑚𝑝𝑙𝑒,𝑡𝑛
Joint Event -Text 
Embedding Space𝑇
𝑇Figure 4. The proposed CRUE module consists of 1) conceptual
reasoning for frame fusion based on the temporal relation among
events and 2) uncertainty estimation of sub-actions for both text
and event embeddings utilizing distributional representation.
embedding and formed into the final text token Ph2)
the learnable text prompt Pl= [P1, P2, ..., P n, PCLASS .],
where Pi, i= 1,2, ..., n l, is a random initialized parameter
withDpdimensions; nldenotes the number of the learn-
able text prompts; PCLASS represents the encoded word
embeddings of [ CLASS ] and[.]means the concatenation op-
eration. Then, the text encoder transforms hand-crafted text
prompt Phand learnable text prompt Plinto corresponding
text embeddings fh
tandfl
t. We finally obtain the text [ CLS]
embeddings ftby averaging fh
tandfl
t.
3.3. CRUE Module
Previous event-based action recognition methods [14, 26]
fail to consider the following aspects: 1) Temporal Re-
lation : Unlike stationary objects, dynamic actions unfold
over time. The event data’s temporal information is vital
for understanding the meaning of an action. For instance, in
Fig. 1, ‘Sit down’ and‘Stand up’ involve similar
sub-actions with the converse temporal occurrence, namely
‘Standing’ →‘Squatting’ →‘Sitting’ vs.
‘Sitting’ →‘Squatting’ →‘Standing’ , thus
resulting in different semantics. 2) Semantic Uncertainty :
Actions, comprising various sub-actions, present greater se-
mantic complexity than that of stationary objects. Take
‘Sit down’ as an example. It includes sub-actions:
‘Standing’ ,‘Squatting’ , and ‘Sitting’ . Each
sub-action owns a specific meaning. Thus, using any sub-
action is insufficient anduncertain to express the meaning
of entire action ‘Sit down’ . Motivated by these two as-
pects, we propose the CRUE module to emulate the action
recognition process of humans through the proposed con-
ceptual reasoning-based fusion and uncertainty estimation.
18636
Conceptual Reasoning-based Fusion: Unlike the sim-
ple average fusion of event frames [14, 36, 39, 55], the
proposed CRUE module leverages the text embeddings to
guide the event frame fusion. Specifically, as depicted in
Fig. 4, given the event [ CLS] embeddings feand text [ CLS]
embeddings ft, a two-layer MLP network is employed for
dimension projection. In this way, we can obtain the pro-
jected event embeddings fp
eand text embeddings fp
t. Then,
we multiply fp
eandfp
tfollowed by the softmax function to
generate the conceptual reasoning weights. Next, the con-
ceptual reasoning weights are multiplied with the original
projected event embeddings fp
eto obtain the fused event
embeddings ffuse
e. Intuitively, the conceptual reasoning
weights are used as semantic weights generated based on
event frames’ temporal sequence for the frame fusion.
Uncertainty Estimation: Semantic uncertainty refers to
the obtained messages that tend to present multiple tar-
gets [19]. To model the semantic uncertainty of actions,
we borrow the idea of distributional representation applied
both in Natural Language Processing (NLP) [2, 8, 47] and
Computer Vision (CV) [5, 41, 42]. Unlike the methods that
extract features as the discrete representation, we utilize the
probability distribution encoder [19] for events and text em-
bedding. Thus, the semantic uncertainty can be quantified
by the variance of the probability distribution.
Specifically, as shown in Fig. 4, the fused event [ CLS]
embeddings ffuse
e∈RHe×We×Neare equally split into
ffuse
e,1andffuse
e,2at the channel dimension [19]. Then,
ffuse
e,1andffuse
e,2are fed into two standard self-attention
modules [46]. Next, we can predict a mean vector µe∈
RHe×We×Neand a variance vector σe∈RHe×We×Ne.
Here, µeandσeare the estimated parameters of the mul-
tivariate Gaussian distribution ffuse
e∼Ne(µe, σ2
e). We
conduct the same operation on the text CLS embeddings ft
to estimate its corresponding multivariate Gaussian distri-
bution fp
t∼Nt(µt, σ2
t). Overall, the above operations can
be formulated as follows:
µi=Att1(fi,1), σi=Att2(fi,2), (2)
fi= [fi,2, fi,1]∼Ni(µi, σ2
i), (3)
where i=e, tdenotes the event and text embeddings re-
spectively; Attrepresents the self-attention module and [.]
indicates the concatenation operation.
With estimated distributional representation Ni(µi, σ2
i),
i=t, e, we now quantify the semantic uncertainty of
event and text embeddings. Then, we sample arbitrary dis-
crete representation by employing the re-parameterization
method [23] to ensure smooth back-propagation. That is,
we first sample a random noise δ∼N(0, I)from the stan-
dard normal distribution rather than sample directly from
fi∼Ni(µi, σ2
i), i=t, e. Next, we obtain the sampled dis-
crete embeddings fn
sample,i byfn
sample,i =µi+δσi, wheren= 1,2, ..., N andNis a hyper-parameter denoting the
number of sampled discrete embeddings. (More discussion
about Nrefer to Sec. 4.3.) The obtained fn
sample,i observes
the estimated distribution Ni, and thus it can be used for
estimating semantic uncertainty.
However, the above random sampling process increases
the complexity of training, especially given the spatial spar-
sity of event data. To accelerate model convergence, we
introduce the Smooth L1 loss [15] between the normalized
CLS embeddings fi, i=t, eand the sampled embeddings
fn
sample,i , i=t, e:
LsmoothL 1=SmoothL 1(fn
sample,i ,fi−mean (fi)
std(fi)),(4)
where n= 1,2, ..., N andNis a hyper-parameter denoting
the number of sampled discrete embeddings, mean (.)and
std(.)denote the mean and variance of the input embed-
dings. Besides, to reduce the semantic uncertainty of dis-
tributional representation, we introduce regularization loss:
Lreg=sum(σ2
i) +sum(σ2
i), i=t, e, (5)
where sum(.)andabs(.)indicate the summation of in-
put embeddings and absolute value operation, respectively.
Note that the experiment results show the final Lregis
higher than zero when the model converges. This indi-
cates the model doesn’t degenerate from distributional rep-
resentation into discrete representation as the variances are
greater than zero.
3.4. Training Objectives
To establish a joint event-text representation space for ac-
tion recognition, we utilize the contrastive loss L(f1
b, f2
b)
between two modal embeddings f1
bandf2
bas follows:
Lcontrastive (f1
b, f2
b)
=−1
BX
b∈Blogexp 
f1
b×f2
b/τ
exp (f1
b×f2
b/τ) +P
b̸=bexp
f1
b×f2
b/τ,
(6)
where τis the temperature coefficient, Brepresents the size
of the mini-batch, bandbdenote the b-th and the b-th data
among the mini-batch.
We calculate the contrastive loss among all sampled
event embeddings fn
sample,e and text embeddings fn
sample,t .
Finally, the whole training objective is composed of the con-
trastive loss, the Smooth L1 loss, and regularization loss
combined with different rate hyperparameters:
Lfinal =α×Lcontrastive (fn
sample,e , fn
sample,t )
+β×LsmoothL 1+θ×Lreg, (7)
where we set the default values of α,βandθto 1 after
considering their numerical range.
18637
sit down: Human action for sit down refers to a person 
lowering their body to rest in a seated position.
Human -object 
Interactioncatch and throw a ball: Human action for catching and 
throwing a ball involves coordination of motor skills, 
timing, and visual perception.
Health -care 
MonitoringHuman -human 
Interaction
vomit: Human action for vomit refers to the act of 
forcefully expelling stomach contents through the mouth.
handshake: A handshake is a human action symbolizing 
agreement, friendship, respect, or conclusion of a deal.Body -motion 
Onlyt
t
t
tFigure 5. Examples of our SeAct dataset.
3.5. SeAct Dataset
Since the category-level labels provided in the previous
event action dataset [1, 4, 34, 48] only use several words
to describe each action, they fail to stimulate the ability
of our ExACT framework to process complex language in-
formation. To this end, we propose the first semantic-
abundant SeAct dataset for event-text action recognition,
where the detailed caption-level label of each action is pro-
vided. SeAct is collected with a DA VIS346 event camera
whose resolution is 346 × 260. It contains 58 actions under
four themes, as presented in Fig. 5. Each action is accom-
panied by an action caption of less than 30 words generated
by GPT-4 [38] to enrich the semantic space of the original
action labels. We split 80% and 20% of each category for
training and testing (validating), respectively. We believe
our SeAct dataset will be a better evaluation platform for
event-text action recognition and inspire more relevant re-
search in the future. Please refer to the Supplmat. for more
dataset introduction.
4. Experiments
4.1. Dataset and Experimental Settings
Dataset In this work, four datasets are adopted for the
evaluation of our proposed model, including PAF [34],
HARDVS [48], DVS128 Gesture [1] and our newly pro-
posed SeAct. PAF [34], also named Action Recognition, is
a dataset recorded indoors, containing ten action categories
with 450 recordings. HARDVS [48] is a recently released
dataset for event-based action recognition, currently hav-
ing the largest action classes, namely 107,646 recordings
for 300 action categories. Both of the above two datasets
have an average time duration of 5 seconds with 346 × 260
resolution [48]. DVS128 Gesture [1] is collected using a
DVS128 camera with 128 × 128 resolution, dividing into
11 hand and arm gestures. Refer to Sec. 3.5 for the intro-
duction of our SeAct dataset.
Experimental Settings In the AFE representation, the low-Dataset ModelAccuracy (%)
Top-1 Top-5
PAFHMAX SNN [53] 55.00 -
STCA [16] 71.20
Motion SNN [27] 78.10 -
MST [51] 88.21 -
Swin-T (BN) [51] 90.14 -
EV-ACT [14] 92.60 -
ExACT (Ours) 94.83 98.28
HARDVSX3D [12] 45.82 52.33
SlowFast [13] 46.54 54.76
ACTION-Net [50] 46.85 56.19
R2Plus1D [45] 49.06 56.43
ResNet18 [17] 49.20 56.09
TAM [28] 50.41 57.99
C3D [44] 50.52 56.14
ESTF [48] 51.22 57.53
Video-SwinTrans [29] 51.91 59.11
TSM [26] 52.63 60.56
ExACT (Ours) 90.10 96.69
DVS128 GestureTime-surfaces [32] 90.62 -
SNN eRBP [20] 92.70 -
Slayer [40] 93.64 -
DECOLLE [21] 95.54 -
EvT [39] 96.20 -
TBR [18] 97.73 -
EventTransAct [9] 97.92 -
ExACT (Ours) 98.86 98.86
SeActEventTransAct [9] 57.81 64.22
EvT [39] 61.30 67.81
ExACT-category 66.07 70.54
ExACT-caption 67.24 75.00
Table 1. An overall comparison with SoTA models for event-based
action recognition task on the PAF, HARDVS, DVS128 Gesture,
and our proposed SeAct dataset. The best scores are in bold, and
the second scores are underlined. ‘ExACT-category’ and ‘ExACT-
caption’ represent that ExACT is trained with category-level and
caption-level labels, respectively.
est sampling rate ∆is set as 50%, 40%, and 40% while
the minimum aggregating event number Nminis chosen at
100,000, 150,000, and 100,000 on the PAF, HARDVS our
SeAct datasets, respectively. The event encoder and text en-
coder of the event-image-text model ECLIP [58] are utilized
for feature encoding. The number of sampled discrete em-
beddings Nis set to 5 based on the hyperparameter search.
The initial learning rates are set to 1e−5with Adam op-
timizer [22] and weight decay equal to 2e−4. CosineAn-
nealingLR [30] learning rate schedule is employed with a
minimum learning rate of 1e-6. Our model is trained for
100 epochs for PAF and SeAct datasets, and 25 epochs for
HARDVS. Please refer to the Supplmat. for additional ex-
perimental settings.
4.2. Comparison with SOTA Methods
As shown in Tab. 1, our proposed ExACT demonstrates
superior performance on the PAF and HARDVS datasets.
Specifically, ExACT brings +2.23% improvements on the
PAF dataset with ten classes compared with SOTA re-
18638
Sample strategy Frame number Aggregated methodAccuracy
Top-1 Top-5
Event histogram3122 every 80,000 events 89.29 91.07
2720 every 90,000 events 94.21 97.14
2405 every 100,000 events 93.10 95.24
Event voxel [10]3122 every 80,000 events 88.88 90.46
2720 every 90,000 events 92.45 94.89
2405 every 100,000 events 90.85 92.31
TBR [18] 2758 every 2000 ms events 92.79 95.16
AFE (Ours) 2894 adaptive events number 94.83 98.28
Table 2. The comparison of AFE representation with previous
event frame representation.
MethodAccuracy
Top-1 ∆ Top-5 ∆
Contrastive 92.86 - 94.64 -
+ CR 93.64 +0.78 96.43 +1.79
+ CR, UE 94.83 +1.97 98.28 +3.64
Table 3. The ablation study of the CRUE module, where Con-
trastive, CR, and UE denote the contrastive learning loss function,
proposed conceptual reasoning-based fusion and uncertainty esti-
mation, respectively.
MethodAccuracy
Top-1 ∆ Top-5 ∆
Sum 89.14 - 90.97 -
Mean Pooling 92.52 +3.38 95.04 +5.07
CR 94.83 +5.69 98.28 +8.31
Table 4. Impact of CR operation proposed in the CRUE module.
sults. Notably, ExACT brings remarkable +37.47% im-
provements on the HARDVS with 300 classes, showcas-
ing ExACT’s excellent potential in classifying complex and
diverse actions. Moreover, upon evaluation using our pro-
posed SeAct dataset, ExACT exhibits a 67.24% Top-1 and
75.00% Top-5 recognition accuracy in real-world scenar-
ios involving 58 dynamic actions with caption-level labels.
These results demonstrate the effectiveness of our ExACT
framework in action recognition.
4.3. Ablation Studies
In this section, we ablate the key components of ExACT,
training objectives, and important hyper-parameters to ex-
plore their effectiveness. Unless otherwise stated, experi-
ments are performed on the PAF dataset.
Effectiveness of the AFE representation. Tab. 2 shows
that, with a comparatively lower sampled number of
2816, our AFE representation obtains the best accuracy of
94.83%. This result indicates that our AFE representation
can achieve better performance by filtering out the repetitive
frames portraying the same action.
CRUE module vs. Contrastive learning. As shown
in Tab. 3, the baseline model is trained using the con-
trastive learning loss, which is widely adopted in previ-
ous methods [52, 58]. Results indicate that the Concep-Training MethodAccuracy
Top-1 ∆ Top-5 ∆
Lcontrastive 92.86 - 94.64 -
+Lreg 93.96 +1.10 95.82 +1.18
+LsmoothL 1 94.41 +1.55 97.89 +3.25
+Lreg,LsmoothL 1 94.83 +1.97 98.28 +3.46
Table 5. Impact of different training objectives.
2 4 6 8
Number	of	sampled	discrete	embeddings,	N9092949698100Accuracy	(% ）
92.4393.9494.83 94.8394.3495.8398.28 98.28
Top-1
Top-5
Figure 6. Impact of different numbers of sampled discrete embed-
dings proposed in the CRUE Module.
tual Reasoning-based fusion (CR) and CR with Uncertainty
Estimation (UE) lead to an increase in accuracy of +0.78%
and +1.97%, respectively. This implies that the CRUE mod-
ule enhances the model’s ability to comprehend actions by
conceptual reasoning-based fusing event frames based on
their temporal relations and estimating the uncertainty of
action semantics during training, thus achieving better per-
formance than simply employing contrastive learning.
Conceptual Reasoning-based fusion (CR) vs. Other
frame fusion methods To evaluate the effectiveness of the
CR. We compare the CR with two other designs, namely,
the sum and average pooling of all event frames, as shown in
Tab. 3. The results demonstrate the effectiveness of our pro-
posed CR, as it improves recognition accuracy by +5.69%
and +2.31% when compared to the sum operation and the
mean pooling operation, respectively.
Performance comparison of different training objec-
tives. Tab. 5 shows the impact of different training ob-
jectives on model performance. Model trained using the
contrastive learning loss Lcontrastive (Eq. 6) exhibits the
lowest performance compared to other combinations of
training objectives. Both the Lreg(Eq. 5) and LsmoothL 1
(Eq. 4) enhance the model performance, bringing +1.10%
and +1.55% accuracy improvements respectively. The com-
bination of all training objectives brings the largest perfor-
mance improvement of +1.97%, which further validates the
effectiveness of the proposed CRUE module.
Impact of the different number of point sampling. As
shown in Fig. 6, we explore the effect of the number of
sampled discrete embeddings N. We find that as Nin-
creases from 1 to 4, the accuracy increases from 92.43%
to 94.83%. When Nincreases from 4 to 8, the accuracy re-
18639
47
48: In Place Hip Kick
276:Hip Kick Jump
206:Hip Up Kick Jump
16: Alternate Front Kick Jump
242:Standing Right Leg Lift
244:Standing Left Leg Lift
Standing Right Leg Lift
Standing Left Leg LiftHip Kick Jump
Hip Up Kick JumpIn Place Hip Kick
Alternate Front 
Kick Jump(a) (b) (c)Figure 7. t-SNE visualization of event embeddings on the
HARDVS dataset. (a) Before training; (b) Training without CRUE
module; (c) Training with CRUE module.
mains constant, indicating that increasing Nyields dimin-
ishing performance accuracy improvement. Intuitively, this
comes from the fact that distributional representation intro-
duces disturbance during training while more sampled dis-
crete embeddings mitigate this disturbance. Consequently,
we set the hyperparameter Nas5during training.
The t-SNE visualization of event embeddings for the
CRUE module. We select 144 event examples belonging to
six categories from the HARDVS dataset. These categories
include a simple pair with different semantics, as well
as two challenging pairs with similar semantics, namely
‘Hip Kick Jump’ vs.‘Hip Up Kick Jump’ and
‘Standing Right Leg Lift’ vs.‘"Standing
Left Leg Lift"’ . Fig. 7 displays the alteration of
event embedding distribution before training and training
with/without the CRUE module. Comparing those hard
pairs in Fig. 7 (b) and (c), we can find that event embeddings
in different categories are widely distributed while our pro-
posed CRUE module advances in distinguishing those hard
pairs with similar semantic meanings. Intuitively, visual-
ization results prove that the CRUE module enhances the
model’s performance by expressing the uncertainty of se-
mantics, especially for those with similar semantics. See
more results in Supplmat.
4.4. Extension to Other Tasks
In this section, we transfer our EXACT model to both text-
to-event and event-to-text retrieval tasks. Retrieval refers
to searching and retrieving required data that matches a
given query. The query can be of different modalities like
events or texts. It has several practical applications, such as
matching a real-world abnormal action with its correspond-
ing event data in surveillance scenarios. For action retrieval
using EXACT, we feed the query and events/texts into cor-
responding encoders to obtain their embeddings. Next, we
calculate the similarity score between the query embedding
and event/text embeddings, then select those events/texts
with the highest similarity scores as the output. All retrieval
experiments are conducted on our SeAct dataset. Refer to
Supplmat. for more retrieving results.
Text-to-event action retrieval In Fig. 8 (a), we present
the Top-3 retrieved event streams utilizing the action
t
t
t
(1)
(2)
(3)
(a) Text -to-eventText query: Falling down: Falling down refers to the involuntary 
action of losing balance and suddenly collapsing to the ground.
(1)  Hurdle start: Human action for hurdle start refers to the 
physical movements a person undertakes to begin a hurdle race .
(2)  Long jump: Human action for long jump involves running, 
leaping, and landing to achieve maximum horizontal distance.
(3)  Running: Running is a human action involving swift movement 
on foot where both feet leave the ground simultaneously.
tEvent 
Query: 
(b) Event -to-textFigure 8. Action retrieval results.
‘Falling down’ with captions. All retrieved event data
exhibit a high degree of similarity to the input text query,
providing further evidence of EXACT’s effectiveness.
Event-to-text action retrieval In Fig. 8 (b), we display
the Top-3 retrieved text captions queried by the event
stream recording action ‘Hurdle start’ . The retrieved
captions include actions of ‘Hurdle start’ ,‘Long
jump’ and‘Running’ , which share several sub-actions
of similar semantics, proving the effectiveness of ExACT.
5. Conclusion and Future Work
We presented the ExACT, as the first exploration utilizing
language guidance for event-based action recognition. We
proposed a CRUE module to simulate the action recognition
of human beings, especially focusing on conceptual reason-
ing of the temporal relations among event frames and es-
timating the uncertainty of actions. Besides, we proposed
the AFE representation, which adaptively eliminated repet-
itive events to generate detailed event frames for dynamic
actions. To evaluate models’ understanding of the complex
semantics of actions involving multiple sub-actions with
different semantics, we presented the SeAct dataset with
semantic-abundant action captions as the first benchmark
for event-text action recognition. Our ExACT framework
achieved SOTA results on the PAF and HARDV A datasets
and achieved plausible performance on our SeAct dataset.
Furthermore, we extended ExACT to event-text retrieval
tasks, proving its flexible transferability.
Future Work: In the future, we will enhance the concep-
tual reasoning and uncertainty estimation module in various
event vision tasks and experiments on larger event action
datasets with semantic-abundant caption-level labels.
Acknowledgement This paper is supported by the Na-
tional Natural Science Foundation of China (NSF) un-
der Grant No. NSFC22FYT45 and the Guangzhou
City, University and Enterprise Joint Fund under Grant
No.SL2022A03J01278.
18640
References
[1] Arnon Amir, Brian Taba, David Berg, Timothy Melano, Jef-
frey McKinstry, Carmelo Di Nolfo, Tapan Nayak, Alexander
Andreopoulos, Guillaume Garreau, Marcela Mendoza, et al.
A low power, fully event-based gesture recognition system.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 7243–7252, 2017. 1, 6
[2] Ben Athiwaratkun and Andrew Gordon Wilson. Multimodal
word distributions. arXiv preprint arXiv:1704.08424 , 2017.
5
[3] Djamila Romaissa Beddiar, Brahim Nini, Mohammad
Sabokrou, and Abdenour Hadid. Vision-based human ac-
tivity recognition: a survey. Multimedia Tools and Applica-
tions , 79(41-42):30509–30555, 2020. 1
[4] Yin Bi, Aaron Chadha, Alhabib Abbas, Eirina Bourtsoulatze,
and Yiannis Andreopoulos. Graph-based spatio-temporal
feature learning for neuromorphic vision sensing. IEEE
Transactions on Image Processing , 29:9084–9098, 2020. 6
[5] Jie Chang, Zhonghao Lan, Changmao Cheng, and Yichen
Wei. Data uncertainty learning in face recognition. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 5710–5719, 2020. 5
[6] Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi
Chen, Jing Shi, Shuang Xu, and Bo Xu. Vlp: A survey
on vision-language pre-training. Machine Intelligence Re-
search , 20(1):38–56, 2023. 2, 3
[7] Hoonhee Cho, Hyeonseong Kim, Yujeong Chae, and Kuk-
Jin Yoon. Label-free event-based object recognition via joint
learning with image reconstruction from events. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 19866–19877, 2023. 3
[8] Shib Dasgupta, Michael Boratko, Dongxu Zhang, Luke Vil-
nis, Xiang Li, and Andrew McCallum. Improving local iden-
tifiability in probabilistic box embeddings. Advances in Neu-
ral Information Processing Systems , 33:182–192, 2020. 5
[9] Tristan de Blegiers, Ishan Rajendrakumar Dave, Adeel
Yousaf, and Mubarak Shah. Eventtransact: A video
transformer-based framework for event-camera based action
recognition. In 2023 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS) , pages 1–7. IEEE,
2023. 6
[10] Yongjian Deng, Hao Chen, Hai Liu, and Youfu Li. A voxel
graph cnn for object classification with event cameras. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 1172–1181, 2022. 7
[11] Yifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao. A
survey of vision-language pre-trained models. arXiv preprint
arXiv:2202.10936 , 2022. 2, 3
[12] Christoph Feichtenhofer. X3d: Expanding architectures for
efficient video recognition. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 203–213, 2020. 6
[13] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
Kaiming He. Slowfast networks for video recognition. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 6202–6211, 2019. 6[14] Yue Gao, Jiaxuan Lu, Siqi Li, Nan Ma, Shaoyi Du, Yipeng
Li, and Qionghai Dai. Action recognition and benchmark
using event cameras. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 2023. 1, 2, 3, 4, 5, 6
[15] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision , pages 1440–1448,
2015. 5
[16] Pengjie Gu, Rong Xiao, Gang Pan, and Huajin Tang. Stca:
Spatio-temporal credit assignment with delayed feedback in
deep spiking neural networks. In IJCAI , pages 1366–1372,
2019. 6
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 6
[18] Simone Undri Innocenti, Federico Becattini, Federico Per-
nici, and Alberto Del Bimbo. Temporal binary representa-
tion for event-based action recognition. In 2020 25th Inter-
national Conference on Pattern Recognition (ICPR) , pages
10426–10432. IEEE, 2021. 6, 7
[19] Yatai Ji, Junjie Wang, Yuan Gong, Lin Zhang, Yanru Zhu,
Hongfa Wang, Jiaxing Zhang, Tetsuya Sakai, and Yujiu
Yang. Map: Multimodal uncertainty-aware vision-language
pre-training model. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
23262–23271, 2023. 2, 5
[20] Jacques Kaiser, Alexander Friedrich, J Tieck, Daniel Re-
ichard, Arne Roennau, Emre Neftci, and R ¨udiger Dillmann.
Embodied neuromorphic vision with event-driven random
backpropagation. arXiv preprint arXiv:1904.04805 , 2019.
6
[21] Jacques Kaiser, Hesham Mostafa, and Emre Neftci. Synap-
tic plasticity dynamics for deep continuous local learning
(decolle). Frontiers in Neuroscience , 14:515306, 2020. 6
[22] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[23] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 5
[24] Yu Kong and Yun Fu. Human action recognition and predic-
tion: A survey. International Journal of Computer Vision ,
130(5):1366–1401, 2022. 1
[25] Athanasios Lentzas and Dimitris Vrakas. Non-intrusive hu-
man activity recognition and abnormal behavior detection on
elderly people: A review. Artificial Intelligence Review , 53
(3):1975–2021, 2020. 1
[26] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift
module for efficient video understanding. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 7083–7093, 2019. 2, 4, 6
[27] Qianhui Liu, Dong Xing, Huajin Tang, De Ma, and Gang
Pan. Event-based action recognition using motion informa-
tion and spiking neural networks. In IJCAI , pages 1743–
1749, 2021. 1, 2, 6
[28] Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and
Tong Lu. Tam: Temporal adaptive module for video recog-
nition. In Proceedings of the IEEE/CVF international con-
ference on computer vision , pages 13708–13718, 2021. 6
18641
[29] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,
Stephen Lin, and Han Hu. Video swin transformer. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 3202–3211, 2022. 6
[30] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas-
tic gradient descent with warm restarts. arXiv preprint
arXiv:1608.03983 , 2016. 6
[31] Jacques Manderscheid, Amos Sironi, Nicolas Bourdis, Da-
vide Migliore, and Vincent Lepetit. Speed invariant time
surface for learning to detect corner points with event-based
cameras. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 10245–
10254, 2019. 3
[32] Jean-Matthieu Maro, Sio-Hoi Ieng, and Ryad Benosman.
Event-based gesture recognition with dynamic background
suppression using smartphone computational capabilities.
Frontiers in neuroscience , 14:501775, 2020. 6
[33] Christoforos Mavrogiannis, Francesca Baldini, Allan Wang,
Dapeng Zhao, Pete Trautman, Aaron Steinfeld, and Jean
Oh. Core challenges of social robot navigation: A survey.
ACM Transactions on Human-Robot Interaction , 12(3):1–
39, 2023. 1
[34] Shu Miao, Guang Chen, Xiangyu Ning, Yang Zi, Kejia
Ren, Zhenshan Bing, and Alois Knoll. Neuromorphic vision
datasets for pedestrian detection, action recognition, and fall
detection. Frontiers in neurorobotics , 13:38, 2019. 2, 6
[35] Preksha Pareek and Ankit Thakkar. A survey on video-based
human action recognition: recent updates, datasets, chal-
lenges, and applications. Artificial Intelligence Review , 54:
2259–2322, 2021. 1
[36] Chiara Plizzari, Mirco Planamente, Gabriele Goletto, Marco
Cannici, Emanuele Gusso, Matteo Matteucci, and Bar-
bara Caputo. E2 (go) motion: Motion augmented event
stream for egocentric action recognition. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 19935–19947, 2022. 1, 2, 5
[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2
[38] Partha Pratim Ray. Chatgpt: A comprehensive review on
background, applications, key challenges, bias, ethics, lim-
itations and future scope. Internet of Things and Cyber-
Physical Systems , 2023. 6
[39] Alberto Sabater, Luis Montesano, and Ana C Murillo. Event
transformer. a sparse-aware solution for efficient event data
processing. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2677–
2686, 2022. 1, 2, 3, 5, 6
[40] Sumit B Shrestha and Garrick Orchard. Slayer: Spike layer
error reassignment in time. Advances in neural information
processing systems , 31, 2018. 6
[41] Yukun Su, Guosheng Lin, and Qingyao Wu. Self-supervised
3d skeleton action representation learning with motion con-
sistency and continuity. In Proceedings of the IEEE/CVFinternational conference on computer vision , pages 13328–
13338, 2021. 2, 5
[42] Jennifer J Sun, Jiaping Zhao, Liang-Chieh Chen, Florian
Schroff, Hartwig Adam, and Ting Liu. View-invariant prob-
abilistic embedding for human pose. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part V 16 , pages 53–70.
Springer, 2020. 2, 5
[43] Zehua Sun, Qiuhong Ke, Hossein Rahmani, Mohammed
Bennamoun, Gang Wang, and Jun Liu. Human action recog-
nition from various data modalities: A review. IEEE trans-
actions on pattern analysis and machine intelligence , 2022.
1
[44] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatiotemporal features with
3d convolutional networks. In Proceedings of the IEEE inter-
national conference on computer vision , pages 4489–4497,
2015. 6
[45] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotemporal
convolutions for action recognition. In Proceedings of the
IEEE conference on Computer Vision and Pattern Recogni-
tion, pages 6450–6459, 2018. 6
[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 5
[47] Luke Vilnis and Andrew McCallum. Word representations
via gaussian embedding. arXiv preprint arXiv:1412.6623 ,
2014. 5
[48] Xiao Wang, Zongzhen Wu, Bo Jiang, Zhimin Bao, Lin Zhu,
Guoqi Li, Yaowei Wang, and Yonghong Tian. Hardvs: Re-
visiting human activity recognition with dynamic vision sen-
sors. arXiv preprint arXiv:2211.09648 , 2022. 2, 6
[49] Xiao Wang, Zongzhen Wu, Yao Rong, Lin Zhu, Bo Jiang,
Jin Tang, and Yonghong Tian. Sstformer: Bridging spiking
neural network and memory support transformer for frame-
event based recognition. arXiv preprint arXiv:2308.04369 ,
2023. 1, 2
[50] Zhengwei Wang, Qi She, and Aljosa Smolic. Action-net:
Multipath excitation for action recognition. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 13214–13223, 2021. 6
[51] Ziqing Wang, Yuetong Fang, Jiahang Cao, Qiang Zhang,
Zhongrui Wang, and Renjing Xu. Masked spiking trans-
former. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 1761–1771, 2023. 6
[52] Ziyi Wu, Xudong Liu, and Igor Gilitschenski. Eventclip:
Adapting clip for event-based object recognition. arXiv
preprint arXiv:2306.06354 , 2023. 3, 7
[53] Rong Xiao, Huajin Tang, Yuhao Ma, Rui Yan, and Garrick
Orchard. An event-driven categorization model for aer im-
age sensors using multispike encoding and learning. IEEE
transactions on neural networks and learning systems , 31
(9):3649–3657, 2019. 6
[54] Bochen Xie, Yongjian Deng, Zhanpeng Shao, Hai Liu, and
Youfu Li. Vmv-gcn: V olumetric multi-view based graph cnn
18642
for event stream classification. IEEE Robotics and Automa-
tion Letters , 7(2):1976–1983, 2022. 1, 2
[55] Bochen Xie, Yongjian Deng, Zhanpeng Shao, Hai Liu, Qing-
song Xu, and Youfu Li. Event voxel set transformer for spa-
tiotemporal representation learning on event streams. arXiv
preprint arXiv:2303.03856 , 2023. 1, 2, 3, 5
[56] Fang Xu, Lei Yu, Bishan Wang, Wen Yang, Gui-Song Xia,
Xu Jia, Zhendong Qiao, and Jianzhuang Liu. Motion deblur-
ring with real events. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 2583–2592,
2021. 3
[57] Xu Zheng, Yexin Liu, Yunfan Lu, Tongyan Hua, Tianbo Pan,
Weiming Zhang, Dacheng Tao, and Lin Wang. Deep learning
for event-based vision: A comprehensive survey and bench-
marks. arXiv preprint arXiv:2302.08890 , 2023. 3
[58] Jiazhou Zhou, Xu Zheng, Yuanhuiyi Lyu, and Lin Wang. E-
clip: Towards label-efficient event-based open-world under-
standing by clip. arXiv preprint arXiv:2308.03135 , 2023. 3,
4, 6, 7
18643
