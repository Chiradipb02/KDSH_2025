Unlocking Pre-trained Image Backbones for Semantic Image Synthesis
Tariq Berrada1,2Jakob Verbeek1Camille Couprie1Karteek Alahari2
1FAIR, Meta2Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK
OASIS ‚Äì0.03sSDM ‚Äì260sPITI ‚Äì17sOurs ‚Äì0.04sLabel map
Groundtruth
Figure 1. Images generated with models trained on COCO-Stuff. We compare our approach to state-of-the-art methods OASIS, SDM, and
PITI, along with inference times to generate a single image. Our approach combines high-quality samples with low-latency sampling.
Abstract
Semantic image synthesis, i.e., generating images from
user-provided semantic label maps, is an important condi-
tional image generation task as it allows to control both the
content as well as the spatial layout of generated images.
Although diffusion models have pushed the state of the art
in generative image modeling, the iterative nature of their
inference process makes them computationally demanding.
Other approaches such as GANs are more efficient as they
only need a single feed-forward pass for generation, but the
image quality tends to suffer when modeling large and di-
verse datasets. In this work, we propose a new class of GAN
discriminators for semantic image synthesis that generates
highly realistic images by exploiting feature backbones pre-
trained for tasks such as image classification. We also intro-
duce a new generator architecture with better context mod-
eling and using cross-attention to inject noise into latent
variables, leading to more diverse generated images. Our
model, which we dub DP-SIMS, achieves state-of-the-art re-
sults in terms of image quality and consistency with the in-
put label maps on ADE-20K, COCO-Stuff, and Cityscapes,
surpassing recent diffusion models while requiring two or-
ders of magnitude less compute for inference.1. Introduction
Conditional image synthesis aims to generate images based
on information such as text, categories, sketches, label
maps, etc. While text-based generation has seen impressive
advances in recent years with diffusion models [41, 49], it
lacks precise control over the location and the boundaries of
objects, which are important properties for creative content
generation tasks like photo editing, inpainting, and for data
augmentation in discriminative learning [1, 4, 18, 68]. Con-
sequently, in this work we focus on semantic image syn-
thesis [24, 44, 53, 59‚Äì61], where the goal is to produce
an image, given a segmentation map, with every pixel as-
signed to a category, as input. Due to the one-to-many na-
ture of the mapping, prior works have tackled this problem
in a conditional GAN [17] framework by exploring different
conditioning mechanisms in GANs to do stochastic gener-
ations that correspond to the input label map [24, 44, 60].
Others developed conditional discriminator models, which
avoid image-to-image reconstruction losses that compro-
mise diversity in generated images [53]. Diffusion mod-
els [59, 61] have also been investigated for this problem.
SDM [61] adds spatially adaptive normalization layers for
conditioning, while PITI [59] replaces the text encoder of
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
7840
a pre-trained text-to-image diffusion model. In compari-
son to GANs, diffusion models often result in improved im-
age quality, but suffer from lower consistency with the input
segmentation maps, and are slower during inference due to
the iterative sampling process [7].
To improve the image quality and consistency of GAN-
based approaches, we explore the use of pre-trained image
backbones in discriminators for semantic image synthesis.
Although leveraging pre-trained image models is common
in many other vision tasks, such as classification, segmen-
tation, or detection, and more recently for class-conditional
GANs [52], to our knowledge this has not been explored
for semantic image synthesis. To this end, we develop a
UNet-like encoder-decoder architecture where the encoder
is a fixed pre-trained image backbone, which leverages the
multi-scale feature representations embedded therein, and
the decoder is a convolutional residual network. We also
propose a novel generator architecture, building on the dual-
pyramid modulation approach [34] with an improved label
map encoding through attention mechanisms for better di-
versity and global coherence among the images generated.
Finally, we add contrastive and diversity losses to further
improve the quality and diversity of generated images.
We validate our contributions with experiments on the
ADE-20K, COCO-Stuff, and Cityscapes datasets. Our
model, termed DP-SIMS for ‚ÄúDiscriminator Pre-training for
Semantic IMage Synthesis‚Äù, achieves state-of-the-art per-
formance in terms of image quality (measured by FID) and
consistency with the input segmentation masks (measured
by mIoU) across all three datasets. Our results not only sur-
pass recent diffusion models on both metrics, but also come
with two orders of magnitude faster inference.
In summary, our main contributions are the following:
‚Ä¢ We develop an encoder-decoder discriminator that lever-
ages feature representations from pre-trained networks.
‚Ä¢ We propose a generator architecture using attention
mechanisms for noise injection and context modeling.
‚Ä¢ We outperform state-of-the-art GAN and diffusion-based
methods in image quality, input consistency, and speed.
2. Related work
Generative image modeling. Several frameworks have
been explored in deep generative modeling, including
GANs [17, 22, 24, 25, 28, 44, 53], V AEs [30, 47, 58],
flow-based models [14, 15, 29] and diffusion-based mod-
els [13, 21, 49, 59, 61]. GANs consist of generator and
discriminator networks which partake in a mini-max game
that results in the generator learning to model the target
data distribution. GANs realized a leap in sample qual-
ity, due to the mode-seeking rather than mode-covering na-
ture of their objective function [38, 42]. More recently,
breakthrough results in image quality have been obtainedusing text-conditioned diffusion models trained on large-
scale text-image datasets [2, 16, 41, 46, 49]. The relatively
low sampling speed of diffusion models has triggered re-
search on scaling GANs to training on large-scale datasets
to achieve competitive image quality while being orders of
magnitude faster to sample [25].
Semantic image synthesis. Early approaches for semantic
image synthesis leveraged cycle-consistency between gen-
erated images and conditioning masks [24, 60] and spatially
adaptive normalization (SPADE) layers [44]. These ap-
proaches combined adversarial losses with image-to-image
feature-space reconstruction losses to enforce image qual-
ity as well as consistency with the input mask [67]. OA-
SIS [53] uses a UNet discriminator model which labels pix-
els in real and generated images with semantic classes and
an additional ‚Äúfake‚Äù class, which overcomes the need for
feature-space losses that inherently limit sample diversity,
while also improving consistency with the input segmen-
tation maps. Further improvements have been made by
adopting losses to learn image details at varying scales [34],
by exploiting intermediate representations such as edges to
guide the generation process [57], or through multi-modal
approaches which leverage data from different modalities
like text, sketches and segmentations [22].
Several works have explored diffusion models for se-
mantic image synthesis. SPADE layers were incorporated
in the denoising network of a diffusion model in SDM [61]
to align the generated images with semantic input maps.
PITI [59] replaced the text-encoder of pre-trained text-to-
image diffusion models, with a label map encoder, and fine-
tuned the resulting model. FLIS [63] propose a rectified
cross-attention module which integrates unseen semantic
masks into the diffusion process of large-scale text-to-image
pre-trained diffusion models. In our work, rather than re-
lying on generative pre-training as in PITI and FLIS, we
leverage discriminative pre-training.
Another line of work considers generating images from
segmentation maps with free-text annotations [3, 12, 63].
These diffusion approaches, however, exhibit relatively
poor consistency with the input label maps while also be-
ing slower to sample from than GAN-based models.
Pre-trained backbones in GANs. Pre-trained feature rep-
resentations have been explored in various ways in GAN
training. When the model is conditioned on detailed inputs,
such as sketches or segmentation maps, pre-trained back-
bones are used to define a reconstruction loss between the
generated and training images [67]. Another line of work
leverages these backbones as fixed encoders in adversar-
ial discriminators [48, 52]. Naively using a pre-trained en-
coder with a fixed decoder yields suboptimal results, thus
the projected GANs model [52] uses a feature conditioning
strategy based on random projections to make the adversar-
ial game more balanced. While this approach is successful
7841
DecoderEncoder
ConvolutionBatch NormLeaky RELUSigmoidConcatenateUpsample √ó2
F"!
F""
F"#
F!
F"
F#‚Ñ±ùúΩ
Figure 2. Architecture of our discriminator model. The encoder consists of a pre-trained feature backbone FŒ∏(left), residual blocks at
full-image resolution (top), and trained feature decoder that aggregates the multi-scale features from the frozen backbone (right).
with some backbones, the method worked best with small
pre-trained models such as EfficientNets [56], while larger
models resulted in lower performance. A related line of
work [32] uses an ensemble of multiple pre-trained back-
bones to obtain a set of discriminators from which a subset
is selected at every step for computing the most informa-
tive gradients. This produced impressive results but has the
following significant overheads which make it inefficient:
(i) all the discriminators and their associated optimizers are
stored in memory, (ii) there is a pre-inference step to quan-
tify the suitability of each discriminator for any given batch,
and (iii) the main discriminator is trained from scratch. Our
work is closely related to projected GANs, but to our knowl-
edge the first one to leverage pre-trained discriminative fea-
ture networks for semantic image synthesis.
Attention in GANs. While most of the popular GAN
frameworks, such as the StyleGAN family, relied exclu-
sively on convolutions [26‚Äì28], some other works explored
the use of attention in GANs to introduce a non-local
parametrization that operates beyond the receptive field of
the convolutions in the form of self-attention [5, 23, 25, 33,
66], as well as cross-attention to incorporate information
from different modalities (text-to-image). To the best of our
knowledge, our work is the first to explore cross-attention
layers in semantic image synthesis models.
3. Method
Semantic image synthesis aims to produce realistic RGB
images g‚ààRW√óH√ó3that are consistent with an input la-
bel map t‚ààRW√óH√óC, where Cis the number of semantic
classes and W√óHis the spatial resolution. A one-to-many
mapping is ensured by conditioning on a random noise vec-
torz‚àº N(0,I)of dimension dz.
In this section, we present our GAN-based approach,
starting with our method to leverage pre-trained feature
backbones in the discriminator (Sec. 3.1). We then describeour noise injection and label map modulation mechanisms
for the generator (Sec. 3.2), and detail the losses we use to
train our models (Sec. 3.3).
3.1. Pre-trained discriminator backbones
Our discriminator is an encoder-decoder model where the
decoder is made of residual blocks with skip connections
similar to [50, 53], while the encoder is a fixed and pre-
trained feature backbone network followed by a feature con-
ditioning module. The discriminator is trained to classify
pixels as belonging to their semantic category or an addi-
tional ‚Äúfake‚Äù class for synthetic images.
LetFŒ∏be a pre-trained feature backbone with parame-
tersŒ∏. We use this backbone, frozen, as part of the ‚Äúen-
coder‚Äù in the UNet discriminator. Let Fl‚ààRCl√óWl√óHl
denote the features extracted by the backbone at levels
l= 1, . . . , L , which generally have different spatial reso-
lutions Wl√óHland number of channels Cl. These fea-
tures are then processed by the UNet ‚Äúdecoder‚Äù, which is
used to predict per-pixel labels spanning the semantic cate-
gories present in the input label map, as well as the ‚Äúfake‚Äù
label. Additionally, to exploit high-frequency details in the
image, we add a fully trainable path at the full-image reso-
lution with two relatively shallow residual blocks. The full
discriminator architecture is illustrated in Fig. 2.
Feature conditioning. An important problem with us-
ing pre-trained backbones is feature conditioning. Typical
backbones are ill-conditioned, meaning that some features
are much more prominent than others. This makes it diffi-
cult to fully exploit the learned feature representation of the
backbone as strong features overwhelm the discriminator‚Äôs
decoder and result in exploring only certain regions in the
feature representation of the encoder. Previously, [52] tried
to alleviate this problem by applying cross-channel mix-
ing (CCM) and cross-scale mixing (CSM) to the features,
while [32] average the signals from multiple discriminators
7842
Label map encodingConditional Image generation
ùê≥‚ààùëÖ!!
ùê∞‚ààùëÖ""√ó!#
ùê∞ùê∞
ùê∞
ConvolutionConv + SPADE + RELUCross-AttentionSelf-AttentionConv + BN + RELULinear + Leaky RELU
Noise Mappingùê≠ùê†ùê°ùüèùê°ùüêùê°ùüëFigure 3. Our generator architecture consist of two compo-
nents. (i) A conditional image generation network (top) that
takes a low-resolution label map as input and produces the full-
resolution output image. (ii) A semantic map encoding network
(bottom) that takes the full resolution label map as input and pro-
duces multi-scale features that are used to modulate the interme-
diate features of the image generation network.
to obtain a more diluted signal. Empirically, the first ap-
proach underperforms in many of our experiments, as the
strong features still tend to mask out their weaker, yet po-
tentially relevant, counterparts. On the other hand, the sec-
ond introduces a large overhead from the multiple models
being incorporated in training. In our work, we develop a
method that better exploits the feature representation from
the encoder. We achieve this by aiming to make all features
have a comparable contribution to the downstream task.
Consider a feature map Fl‚ààRCl√óWl√óHlat scale lfrom
the pre-trained backbone. First, we apply a contractive non-
linearity (CNL) such as sigmoid to obtain F‚Ä≤
l=œÉ(Fl).
Next, we normalize the features to ensure they have a sim-
ilar contribution in the following layers. We choose batch
normalization, yielding ÀúFl= (F‚Ä≤
l‚àí¬µl)/œÉl, where ¬µland
œÉlare the batch statistics. In this manner, all features are in
a similar range and therefore the decoder does not prioritize
features with a high variance or amplitude.
3.2. Generator architecture
Our generator architecture is based on DP-GAN [34], but
offers two main novelties: a revisited noise injection mech-
anism and improved modeling of long-range dependencies
through self-attention. Following DP-GAN, we use a mask
encoding network to condition the SPADE blocks, rather
than conditioning the SPADE blocks on the label maps
via a single convolution layer, which cannot take into ac-
count longer-range dependencies encoded in the label map.Each block of the label map encoding pyramid is made of
a single convolution layer with downsampling followed by
batch norm, GELU activation [19], attention modules, and
a pointwise convolution layer. For every scale, we obtain
a modulation map hi, i‚àà {1, . . . , L }which, concatenated
with a resized version of the ultimate map hL, will serve as
conditioning for the SPADE block at the same resolution.
While [53] argued that concatenating a spatial noise map
to the label map was enough to induce variety in the gener-
ated images, since the noise is present in all SPADE blocks,
and therefore hard to ignore, the same cannot be said for
the architecture of DP-GAN [34]. The noise is injected
only at the first layer of the label map encoding network,
hence it is much easier to ignore. Consequently, we propose
a different mechanism for noise injection, making use of
cross-attention between the learned representations at dif-
ferent scales and the mapping noise obtained by feeding
zto a three-layer MLP, w=MLP(z)‚ààRnk√ódw. Let
hi‚ààRCi√óHi√óWibe the downsampled feature represen-
tation from the previous scale. This feature hifirst goes
through a convolution to provide an embedding of the label
map, then the spatial dimensions are flattened and projected
via a linear layer to obtain the queries Q‚ààRHiWi√ódq. The
transformed noise vector wis projected via two linear lay-
ers to obtain the keys and the values K, V‚ààRnk√ódq, then
the cross-attention is computed as:
A=SoftMax
QK‚ä§/p
dq
V. (1)
The noise injection blocks at spatial resolutions 64√ó64
and lower use residual cross-attention block
a(hi,w) =hi+Œ∑i¬∑A(hi,w), (2)
where Œ∑i‚ààRis a trainable gating parameter initialized at
0. Noise injection is followed by a residual self-attention
block, before having a convolution output the conditioning
at scale i. For higher resolutions where attention modules
are too expensive, we use convolutional blocks only. The
generator architecture is illustrated in Fig. 3.
3.3. Training
We train our models by minimizing a weighted average of
three loss functions which we detail below.
Pixel-wise focal loss. Our main loss is based on a pixel-
wise GAN loss [53], where the discriminator aims to assign
pixels in real images to the corresponding class in the con-
ditioning label map, and those in generated images to an ad-
ditional ‚Äúfake‚Äù class. To improve the performance on rare
classes, we replace the weighted cross-entropy of [53] with
a weighted focal loss [35], while keeping the same weight-
ing scheme as in [53]. Let p(x)‚àà[0,1]H√óW√ó(C+1)denote
the output class probability map of the discriminator for a
real RGB image x, and p(g)‚àà[0,1]H√óW√ó(C+1)be the
7843
probability map for a generated image g=G(z,t), where
the label index C+ 1is used for the ‚Äúfake‚Äù class. Then, the
discriminator loss is:
LD=‚àíE(x,t)CX
c=1Œ±cH√óWX
i=1ti,c(1‚àíp(x)i,c)Œ≥logp(x)i,c
‚àíE(g,t)H√óWX
i=1(1‚àíp(g)i,C+1)Œ≥logp(g)i,C+1,(3)
where Œ±c‚Äôs are the class weighting terms and Œ≥is a hyper-
parameter of the focal loss. The standard cross-entropy is
recovered for Œ≥= 0, and for Œ≥ > 0the loss puts more
weight on poorly predicted labels.
The pixel-wise loss for the generator then takes the form:
LG=‚àíE(g,t)CX
c=1Œ±cH√óWX
i=1ti,c(1‚àíp(g)i,c)Œ≥logp(g)i,c.
(4)
Using the focal loss, both the generator and discriminator
put more emphasis on pixels that are incorrectly classified.
These often belong to rare classes which helps to improve
performance for these under-represented classes. To pre-
vent the discriminator output probabilities from saturating
and thus leading to vanishing gradients, we apply one-sided
label smoothing [51] by setting the cross-entropy targets to
1‚àíœµfor the discriminator loss, where œµis a hyper-parameter.
Contrastive loss. We define a patch-wise contrastive loss
that encourages the generated images to be globally coher-
ent. Our contrastive framework is based on InfoNCE [43],
which aims to bring matching patch features closer together,
and push them further from non-matching features. Given
a pair (x,t)of image and label map, we generate a corre-
sponding image g=G(z,t), and use HxandHgthe cor-
responding multi-scale features obtained from a pre-trained
VGG network [54]. For every scale, we sample matching
features z,z+from the same spatial coordinates in Hgand
Hxrespectively. Additionally, we sample Nnon-matching
features z‚àí
nat randomly selected coordinates from Hx.
The features are then projected into an embedding space
using a convolution followed by a two-layer MLP to obtain
v,v+,v‚àí
n‚ààRdvbefore computing the InfoNCE loss as
LNCE(v,v+,v‚àí) =‚àílog 
ev‚ä§v+/œÑ
ev‚ä§v+/œÑ+PN
n=1ev‚ä§v‚àí
n/œÑ!
,
(5)
where œÑis a temperature parameter controlling the sharp-
ness in the response of the loss. We apply the loss at feature
scales 1/4,1/8,1/16, and take their sum. This is similar
to the contrastive losses used for image-to-image transla-
tion [45], with the main difference being the feature rep-
resentation from which the loss is calculated. While other
methods reuse the encoder features from their translationnetwork, we obtain the feature pyramid from a VGG net-
work [54] and process it by a simple module made of a con-
volution block followed by a projection MLP.
Diversity loss. To promote diversity among the generated
images we introduce a loss, similar to [39, 64], that encour-
ages two images generated with the same mask, but differ-
ent latents z, to be sufficiently distinct from each other:
LDiv= max"
0, œÑdiv‚àíGf(z1,t)‚àíGf(z2,t)
1
‚à•z1‚àíz2‚à•1#
,(6)
where Gfis the feature output of the generator before the
final convolution. We adopt a cutoff threshold œÑdivon the
loss in order to not overly constrain the generator, and apply
this loss only for similar samples given the same label map.
4. Experiments
We present our experimental setup in Sec. 4.1, followed by
our main results in Sec. 4.2, and ablations in Sec. 4.3.
4.1. Experimental setup
Datasets. We consider three popular datasets to
benchmark semantic image synthesis: COCO-Stuff [6],
Cityscapes [11], ADE-20K [69]. COCO-Stuff provides
118k training images and 5k validation images, labeled
with 183 classes. Cityscapes contains 2,975 training im-
ages along with a validation set of 500 images, and uses 35
labels. ADE-20K holds 26k images with object segmen-
tations across 151 classes. Similar to [44, 60, 61], we use
instance-level annotations when available. For COCO-Stuff
and Cityscapes, we use instance segmentations as in [9], by
creating vertical and horizontal offset maps of every fore-
ground pixel w.r.t. its object center of mass, and concatenate
these to the semantic label maps as input for the model. For
ADE-20K, there are no instance segmentations available.
We generate images at a resolution of 256√ó256for ADE-
20K and COCO-Stuff, and 256√ó512for Cityscapes. We
blurred faces of people in the datasets before use; see the
supplementary material for more details.
Metrics. We compute FID [20] to assess image quality,
and report the mean intersection-over-union score (mIoU)
to measure the consistency with the input segmentation
maps. For a fair comparison with previous work [34, 44,
53], we used the segmentation models from these works
for inferring label maps of the generated images: Uper-
Net101 [62] for ADE-20K, multi-scale DRN-D-105 [65]
for Cityscapes, and DeepLabV2 [8] for COCO-Stuff. We
refer to the scores obtained with these models as mIoU. In
addition, we infer label masks using Mask2Former [10],
which is more accurate than other segmentation models,
thus yielding a more meaningful comparison to the ground-
truth masks. We denote the resulting scores as mIoU MF. See
the supplementary material for more detail.
7844
COCO ADE20k Cityscapes
FID (‚Üì) mIoU MF(‚Üë) mIoU (‚Üë) FID (‚Üì) mIoU MF(‚Üë) mIoU (‚Üë) FID (‚Üì) mIoU MF(‚Üë) mIoU (‚Üë)
Pix2pixHD [60] 111.5 ‚Äî 14.6 73.3 ‚Äî 22.4 104.7 ‚Äî 52.4
SPADE [44] 22.6 ‚Äî 37.4 33.9 ‚Äî 38.5 71.8 ‚Äî 62.3
OASIS [53] 17.0 52.1 44.1 28.3 53.5 48.8 47.7 72.0 69.3
DP-GAN [34] ‚Äî ‚Äî ‚Äî 26.1 ‚Äî 52.7 44.1 ‚Äî 73.6
PoE-GAN [22] 15.8 ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî
ECGAN++ [57] 14.9 ‚Äî 47.9 24.7 ‚Äî 52.7 42.2 ‚Äî 73.3
SDM [61] 15.9 40.3 36.8 27.5 51.9 44.0 42.1 72.8 69.1
PITI [59] 15.5 31.2 29.5 ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî
FLIS [63] 14.4 ‚Äî 40.7 25.0 ‚Äî 41.9 ‚Äî ‚Äî ‚Äî
DP-SIMS (ours) 13.6 65.2 57.7 22.7 67.8 54.3 38.2 78.5 76.3
Table 1. Comparison of DP-SIMS to state-of-the-art GAN-based (first block) and diffusion-based methods (second block). Results taken
from the original papers. We computed the mIoU MFmetric for methods where pre-trained checkpoints or generated images are available.
Backbone Prms. FLOPS Acc@1 FID ( ‚Üì) mIoU MF(‚Üë)
Swin-B 107 M 15.4G 86.4 29.5 55.4
ResNet-50 44 M 4.1G 76.2 24.6 60.5
EfficientNet-Lite1 3 M 631M 83.4 24.5 63.1
ConvNeXt-B [37] 89 M 15.4G 85.1 23.5 63.5
ConvNeXt-L [37] 198 M 34.4G 85.5 22.7 67.8
Table 2. Comparison of backbone architectures on ADE-20K. We
report the ImageNet-1k top-1 accuracy (Acc@1) for reference.
Implementation details. We counter the strong class im-
balance in the datasets used in our experiments with a sam-
pling scheme favoring rare classes. Let fcbe the fraction of
training images where class cappears, then each image is
sampled with a probability proportional to f‚àí1/2
kwithkthe
sparsest class present in the image.
Each of our models is trained on one or two machines
with eight V100 GPUs. We set the total batch size at 64and
use ADAM optimizer in all our experiments with a learning
rate of 10‚àí3and momentums Œ≤1= 0, Œ≤2= 0.99. For pre-
trained Swin backbones, we found it necessary to use gra-
dient clipping to stabilize training. Following prior work,
we track an exponential moving average of the generator
weight and set the decay rate to Œ±= 0.9999 . For the con-
trastive loss, we set the weighting factor ŒªC= 100 , the
temperature œÑ= 0.3and select N= 128 negative samples.
We set ŒªGAN= 1 for the GAN loss and ŒªD= 10 for the
diversity loss. For the focal loss, we set Œ≥= 2.
4.2. Main results
Comparison to the state of the art. In Tab. 1, we report
the results obtained with our model in comparison to the
state of the art. Our DP-SIMS method (with ConvNext-L
backbone) achieves the best performance across metrics and
datasets. On COCO-Stuff, we improve the FID of 14.4 from
FLIS [63] to 13.6, while improving the mIoU MFof 52.1
from OASIS to 65.2, and mIoU of 47.9 from ECGAN++ to
57.7. For ADE-20K, we observe a similar trend with an im-
provement of 2.0 FID points w.r.t. ECGAN++, an improve-Pre-training Acc@1 FID ( ‚Üì) mIoU MF(‚Üë)
Random Init. ‚Äî 52.9 40.7
IN-1k@224 84.3 22.7 62.8
IN-21k@224 86.6 23.6 64.1
IN-21k@384 87.5 22.7 67.8
Table 3. Influence of discriminator pre-training on the overall per-
formance for ADE-20K using a ConvNext-L backbone.
ment of 14.5 points in mIoU MFw.r.t. OASIS, and improv-
ing mIoU by 1.6 points w.r.t. ECGAN++ and DP-GAN. For
Cityscapes, we obtain improvements of 3.9 FID points w.r.t.
Semantic Diffusion, 5.5 points in mIoU MF, and 3.0 points in
mIoU. See Fig. 1 and Fig. 4 for qualitative comparisons of
model trained on COCO-Stuff and Cityscapes. Please re-
fer to the supplementary material for additional examples,
including ones for ADE-20K.
Encoder architecture. We experiment with different
pre-trained backbone architectures for the discriminator in
Tab. 2. All the encoders were trained for ImageNet-1k clas-
sification. We find that the attention-based Swin architec-
ture [36] has the best ImageNet accuracy, but compared
to convolutional models performs worse as a discrimina-
tor backbone for semantic image synthesis, and tends to
be more unstable, often requiring gradient clipping to con-
verge. For the convolutional models, better classification
accuracy translates to better FID and mIoU MF.
Pre-training dataset. In Tab. 3, we analyze the impact of
pre-training the ConvNext-L architecture in different ways
and training our models on top of these, with everything else
being equal. We consider pre-training on ImageNet-1k (IN-
1k@224) and ImageNet-21k (IN-21k@224) at 224√ó224
resolution, and also on ImageNet-21k at 384√ó384resolu-
tion (IN-21k@384). In terms of mIoU MF, the results are in
line with those observed for different architectures: discrim-
inators trained with larger datasets (IN-21k) and on higher
resolutions perform the best. On the other hand, we find that
for FID, using the standard ImageNet (IN-1k@224) results
in better performance than its bigger IN-21k@224 coun-
7845
Label mapOASISSDMOursFigure 4. Qualitative comparison with prior work on the Cityscapes dataset. We show the results of OASIS [53], SDM [61], and our
approach along with the corresponding label map used for generating each image. Note that our method generates more coherent objects
with realistic textures in comparison.
Model Gen. steps Ups. steps ‚àÜtgen ‚àÜtups ‚àÜttot
PITI 250 27 14.3 3.1 17.4
PITI 27 27 1.5 3.1 4.6
SDM 1000 ‚Äî 260.0 ‚Äî 260.0
DP-SIMS ‚Äî ‚Äî ‚Äî ‚Äî 0.04
Table 4. Comparison of inference time (in seconds) of PITI, SDM
and our GAN-based model. We show the time taken by the gener-
ative ( ‚àÜtgen) and the upsampling ( ‚àÜtups) models in addition to the
total time ( ‚àÜttot) for these steps.
terpart, and performs as well as IN-21k@384 pre-training.
This is likely due to the use of the same dataset in the In-
ception model [55], which is the base for calculating FID,
thus introducing a bias in the metric.
Inference speed. An important advantage of GAN mod-
els over their diffusion counterparts is their fast inference.
While a GAN only needs one forward pass to generate an
image, a diffusion model requires several iterative denois-
ing steps, resulting in slower inference, which can hamper
the practical usability of the model. In Tab. 4 we report the
inference speed for generating a single 256√ó256image, av-
eraged over 50different runs. PITI uses 250 denoising steps
for the generative model at 64√ó64resolution and 27 steps
for the upsampling model by default, while SDM uses 1000
steps at full resolution. We also benchmark using 27 steps
for the PITI generative model. Our generator is two to threeEfficientNet-Lite1 ConvNeXt-L
FID mIoU MF FID mIoU MF
Baseline - no normalization 27.8 58.6 24.4 63.6
CCM + CSM (PG) 28.9 59.1 25.4 66.0
BatchNorm + CCM + CSM 29.4 56.4 24.6 66.4
DP-SIMS w/o sigmoid 24.9 62.7 23.3 65.7
DP-SIMS w/o BatchNorm 26.0 61.6 23.6 64.0
DP-SIMS (ours) 24.5 63.1 22.7 67.8Table 5. Abla-
tion on feature con-
ditioning shown on
ADE-20K with two
backbones.
orders of magnitude faster than its diffusion counterparts.
4.3. Ablations
Feature Conditioning. We perform an ablation to vali-
date our feature conditioning mechanisms on ADE-20K in
Tab. 5. We compare to: (i) a baseline that does not nor-
malize the backbone features, (ii) the cross-channel and
scale mixing approach of Projected GAN (PG) [52], (iii)
using our BatchNorm layer with cross-channel and scale
mixing, (iv) DP-SIMS without the sigmoid normlization,
(v) DP-SIMS without the BatchNorm layers. For a fair
comparison with [52], these experiments are conducted on
their best reported backbone, EfficientNet-Lite1 [56]. We
also conducted this experiment with a ConvNeXt-L back-
bone. Compared to the baseline, Projected GAN improves
mIoU MFbut degrades FID, while our feature conditioning
(last line) improves both metrics for both backbones. More-
over, the ablations show that both the sigmoid and Batch-
Norm contribute, and that adding BatchNorm for Project-
edGAN leads to inferior performance.
7846
FID (‚Üì) mIoU MF(‚Üë)
DP-SIMS 22.7 67.8
Generator architecture
OASIS disc + our gen 29.3 49.0
OASIS gen + our disc 25.6 63.6
Ours w/o self-attention 23.7 65.4
Ours w/o cross-attention 23.6 64.5
Training
Ours w/o label smoothing 23.0 66.3
Ours w/o contrastive loss 25.1 66.0Table 6. Ab-
lations on the
architectural
design and
training losses,
shown on
ADE-20K with
ConvNext-L
backbone.
œÑ 0.07 0.3 0.7 2.0
FID 25.7 22.7 24.1 26.4
mIoU MF 62.6 67.8 66.3 61.4Table 7. Influence of
the contrastive loss eval-
uated on ADE-20K.
Cityscapes ADE-20K
FID (‚Üì) mIoU MF(‚Üë) FID (‚Üì) mIoU MF(‚Üë)
Weighted CE 39.8 75.9 23.2 65.5
Focal 39.3 75.0 22.8 64.7
Weighted focal 38.2 78.5 22.7 67.8
Table 8. Comparison of pixel-wise losses on Cityscapes and ADE-
20K with ConvNext-L backbone.
Architectural modifications. In Tab. 6, we perform an ab-
lation on our proposed architectural modifications. Swap-
ping out our generator or discriminator with the ones from
OASIS, suggests that most of the gains are due to our dis-
criminator design. Using the OASIS discriminator instead
of ours deteriorates mIoU MFby 18.8 points and FID by
6.6 points. We also experiment with removing the cross-
attention noise injection mechanism and replacing it with
the usual concatenation instead, as well as leaving out the
self-attention layers. Both of these contribute to the final
performance in a notable manner. Finally, we present an
ablation on label smoothing, which deteriorates FID by 0.3
and mIoU MFby 1.4 points when left out.
Contrastive loss. To assess the importance of the con-
trastive loss, we perform an ablation in the last row of Tab. 6
where we remove it during training. This substantially im-
pacts the results: worsening FID by 2.4 and mIoU MFby
1.8 points. In Tab. 7, we evaluate different values for the
temperature parameter œÑ, and find an optimal temperature
parameter œÑC= 0.3, using Œªc= 100 .
Focal loss. In Tab. 8, we consider the impact of the focal
loss by comparing it to the weighted cross-entropy loss, as
used in OASIS, and the effect of class weighting in the focal
loss. We find that for both datasets switching from weighted
cross-entropy to the focal loss improves FID but worsens
mIoU MF. The weighted focal loss, however, improves both
metrics on both datasets.
Diversity. We study the effect of the diversity loss on the
variability of generated images. Following [53], we report
the mean LPIPS distance across 20 synthetic images fromModel 3D noise LPIPS ( ‚Üë)
SPADE+ ‚úì 0.16
SPADE+ ‚úó 0.50
OASIS ‚úì 0.35
DP-SIMS ‚úó 0.47Table 9. Evaluation of the
diversity of images gener-
ated. Results on ADE-20K
for SPADE+ and OASIS are
taken from [53].
Figure 5. Images generated by varying the noise vector with DP-
SIMS trained on COCO-Stuff and using a ConvNext-L backbone.
the same label map, averaged across the validation set, in
Tab. 9. A qualitative example is provided in Fig. 5 showing
a clear variety in the images generated. In comparison with
OASIS, we generate more diverse images, with an LPIPS
score similar to that of SPADE, but with a much higher
quality, as reported in Tab. 1, in terms of FID and mIoU MF.
5. Conclusion
We introduced DP-SIMS that harnesses pre-trained back-
bones in GAN-based semantic image synthesis models. We
achieve this by using them as an encoder in UNet-type dis-
criminators, and introduce a feature conditioning approach
to maximize the effectiveness of pre-trained features. More-
over, we propose a novel generator architecture which uses
cross-attention to inject noise in the image generation pro-
cess, and introduce new loss terms to boost sample diversity
and input consistency. We experimentally validate our ap-
proach and compare it to state-of-the-art prior work based
on GANs as well as diffusion models on three standard
benchmark datasets. Compared to these, we find improved
performance in terms of image quality, sample diversity,
and consistency with the input segmentation maps. Impor-
tantly, with our approach inference is two orders of magni-
tude faster than diffusion-based methods.
In our experiments we found that transformer-based
models, such as Swin, can lead to instability when used as
discriminator backbones. Given their strong performance
for dense prediction tasks, it would be worthwhile to fur-
ther study and mitigate this issue in future work, hopefully
bringing additional improvements.
7847
References
[1] Mohamed Akrout, B ¬¥alint Gyepesi, P ¬¥eter Holl ¬¥o, Adrienn
Po¬¥or, Bl ¬¥aga Kincs Àùo, Stephen Solis, Katrina Cirone, Jeremy
Kawahara, Dekker Slade, Latif Abid, M ¬¥at¬¥e Kov ¬¥acs, and
Istv¬¥an Fazekas. Diffusion-based data augmentation for
skin disease classification: Impact across original medical
datasets to fully synthetic images. In MICCAI workshop ,
2023.
[2] Chitwan Saharia amd William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,
Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J
Fleet, and Mohammad Norouzi. Photorealistic text-to-image
diffusion models with deep language understanding. In
NeurIPS , 2022.
[3] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,
Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried,
and Xi Yin. Spatext: Spatio-textual representation for con-
trollable image generation. In CVPR , 2023.
[4] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-
hammad Norouzi, and David J. Fleet. Synthetic data from
diffusion models improves imagenet classification. TMLR ,
2023.
[5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high fidelity natural image synthesis.
InICLR , 2019.
[6] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-
Stuff: Thing and stuff classes in context. In CVPR , 2018.
[7] Marl `ene Careil, Jakob Verbeek, and St ¬¥ephane Lathuili `ere.
Few-shot semantic image synthesis with class affinity trans-
fer. In CVPR , 2023.
[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L. Yuille. DeepLab: Semantic im-
age segmentation with deep convolutional nets, atrous con-
volution, and fully connected CRFs. PAMI , 40(4):834‚Äì848,
2018.
[9] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,
Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen.
Panoptic-deeplab: A simple, strong, and fast baseline for
bottom-up panoptic segmentation. In CVPR , 2020.
[10] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In CVPR ,
2022.
[11] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
Cityscapes dataset for semantic urban scene understanding.
InCVPR , 2016.
[12] Guillaume Couairon, Marl `ene Careil, Matthieu Cord,
St¬¥ephane Lathuili `ere, and Jakob Verbeek. Zero-shot spatial
layout conditioning for text-to-image diffusion models. In
ICCV , 2023.
[13] Prafulla Dhariwal and Alex Nichol. Diffusion models beat
GANs on image synthesis. In NeurIPS , 2021.
[14] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
Density estimation using Real NVP. In ICLR , 2017.[15] Weichen Fan, Jinghuan Chen, Jiabin Ma, Jun Hou, and Shuai
Yi. Styleflow for content-fixed image to image translation.
arXiv , 2207.01909, 2022.
[16] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-
based text-to-image generation with human priors. In ECCV ,
2022.
[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NeurIPS ,
2014.
[18] Reyhane Askari Hemmat, Mohammad Pezeshki, Florian
Bordes, Michal Drozdzal, and Adriana Romero-Soriano.
Feedback-guided data synthesis for imbalanced classifica-
tion. arXiv , 2310.00158, 2023.
[19] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities
and stochastic regularizers with Gaussian error linear units.
arXiv , 1606.08415, 2016.
[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. GANs trained by
a two time-scale update rule converge to a local Nash equi-
librium. In NeurIPS , 2017.
[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020.
[22] Xun Huang, Arun Mallya, Ting-Chun Wang, and Ming-Yu
Liu. Multimodal conditional image synthesis with product-
of-experts GANs. In ECCV , 2022.
[23] Drew A. Hudson and C. Lawrence Zitnick. Generative ad-
versarial transformers. In ICML , 2021.
[24] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. CVPR , 2017.
[25] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,
Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up
GANs for text-to-image synthesis. In CVPR , 2023.
[26] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
CVPR , 2019.
[27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of StyleGAN. In CVPR , 2020.
[28] Tero Karras, Miika Aittala, Samuli Laine, Erik H ¬®ark¬®onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. In NeurIPS , 2021.
[29] Diederik Kingma and Prafulla Dhariwal. Glow: Generative
flow with invertible 1 √ó1 convolutions. In NeurIPS , 2018.
[30] Diederik Kingma and Max Welling. Auto-encoding varia-
tional Bayes. In ICLR , 2014.
[31] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ¬¥ar, and
Ross Girshick. Segment anything. arXiv preprint , 2023.
[32] Nupur Kumari, Richard Zhang, Eli Shechtman, and Jun-Yan
Zhu. Ensembling off-the-shelf models for GAN training. In
CVPR , 2022.
[33] Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang,
Zhuowen Tu, and Ce Liu. ViTGAN: Training GANs with
vision transformers. In ICLR , 2022.
7848
[34] Shijie Li, Ming-Ming Cheng, and Juergen Gall. Dual pyra-
mid generative adversarial networks for semantic image syn-
thesis. In BMVC , 2022.
[35] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll ¬¥ar. Focal loss for dense object detection. In ICCV ,
2017.
[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , 2021.
[37] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A ConvNet for the
2020s. In CVPR , 2022.
[38] Thomas Lucas, Konstantin Shmelkov, Karteek Alahari,
Cordelia Schmid, and Jakob Verbeek. Adaptive density esti-
mation for generative models. In NeurIPS , 2019.
[39] Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and
Ming-Hsuan Yang. Mode seeking generative adversarial net-
works for diverse image synthesis. In CVPR , 2019.
[40] Stanislav Morozov, Andrey V oynov, and Artem Babenko.
On self-supervised image representations for gan evaluation.
InICLR , 2021.
[41] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. GLIDE: Towards photorealistic image genera-
tion and editing with text-guided diffusion models. In ICML ,
2022.
[42] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-
GAN: Training generative neural samplers using variational
divergence minimization. In NeurIPS , 2016.
[43] A ¬®aron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv ,
1807.03748, 2019.
[44] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan
Zhu. Semantic image synthesis with spatially-adaptive nor-
malization. In CVPR , 2019.
[45] Taesung Park, Alexei A. Efros, Richard Zhang, and Jun-
Yan Zhu. Contrastive learning for unpaired image-to-image
translation. In ECCV , 2020.
[46] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with CLIP latents. arXiv preprint , 2204.06125, 2022.
[47] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Gen-
erating diverse high-fidelity images with VQ-V AE-2. In
NeurIPS , 2019.
[48] Stephan R. Richter, Hassan Abu AlHaija, and Vladlen
Koltun. Enhancing photorealism enhancement. IEEE
TPAMI , 45(2):1700‚Äì1715, 2022.
[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022.
[50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
Net: Convolutional networks for biomedical image segmen-
tation. In MICCAI , 2015.
[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training GANs. In NeurIPS , 2016.[52] Axel Sauer, Kashyap Chitta, Jens M ¬®uller, and Andreas
Geiger. Projected GANs converge faster. In NeurIPS , 2021.
[53] Edgar Sch ¬®onfeld, Vadim Sushko, Dan Zhang, Juergen Gall,
Bernt Schiele, and Anna Khoreva. You only need adversarial
supervision for semantic image synthesis. In ICLR , 2021.
[54] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR ,
2015.
[55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathan Shlens, and Zbigniew Wojna. Rethinking the in-
ception architecture for computer vision. In CVPR , 2016.
[56] Mingxing Tan and Quoc V . Le. EfficientNet: Rethinking
model scaling for convolutional neural networks. In ICML ,
2019.
[57] Hao Tang, Guolei Sun, Nicu Sebe, and Luc van Gool. Edge
guided GANs with multi-scale contrastive learning for se-
mantic image synthesis. PAMI , 45:14435‚Äì14452, 2023.
[58] Arash Vahdat and Jan Kautz. NV AE: A deep hierarchical
variational autoencoder. In NeurIPS , 2020.
[59] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong
Chen, Qifeng Chen, and Fang Wen. Pretraining is all you
need for image-to-image translation. arXiv , 2205.12952,
2022.
[60] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional GANs. In
CVPR , 2018.
[61] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong
Chen, Dong Chen, Lu Yuan, and Houqiang Li. Seman-
tic image synthesis via diffusion models. arXiv preprint ,
2207.00050, 2022.
[62] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Unified perceptual parsing for scene understand-
ing. In ECCV , 2018.
[63] Han Xue, Zhiwu Huang, Qianru Sun, Li Song, and Wenjun
Zhang. Freestyle layout-to-image synthesis. In CVPR , 2023.
[64] Dingdong Yang, Seunghoon Hong, Yunseok Jang, Tianchen
Zhao, and Honglak Lee. Diversity-sensitive conditional gen-
erative adversarial networks. In ICLR , 2019.
[65] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated
residual networks. In CVPR , 2017.
[66] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus-
tus Odena. Self-attention generative adversarial networks. In
ICML , 2019.
[67] R. Zhang, P. Isola, A. Efros, E. Shechtman, and O. Wang.
The unreasonable effectiveness of deep features as a percep-
tual metric. In CVPR , 2018.
[68] Hanqing Zhao, Dianmo Sheng, Jianmin Bao, Dongdong
Chen, Dong Chen, Fang Wen, Lu Yuan, Ce Liu, Wenbo
Zhou, Qi Chu, Weiming Zhang, and Nenghai Yu. X-Paste:
Revisiting scalable copy-paste for instance segmentation us-
ing CLIP and StableDiffusion. In ICML , 2023.
[69] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Barriuso, and Antonio Torralba. Scene parsing through
ADE20K dataset. In CVPR , 2017.
7849
