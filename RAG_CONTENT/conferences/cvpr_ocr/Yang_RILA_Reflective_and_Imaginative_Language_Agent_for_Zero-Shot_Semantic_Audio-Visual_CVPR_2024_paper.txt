RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic
Audio-Visual Navigation
Zeyuan Yang1*,Jiageng Liu2*,Peihao Chen3,Anoop Cherian4,
Tim K. Marks4,Jonathan Le Roux4,Chuang Gan5,6
1Tsinghua University,2Zhejiang University,3South China University of Technology
4Mitsubishi Electric Research Laboratories (MERL),5UMass Amherst,6MIT-IBM AI Lab
yangzeyu21@mails.tsinghua.edu.cn
Abstract
We leverage Large Language Models (LLM) for zero-
shot Semantic Audio Visual Navigation (SAVN). Existing
methods utilize extensive training demonstrations for rein-
forcement learning, yet achieve relatively low success rates
and lack generalizability. The intermittent nature of au-
ditory signals further poses additional obstacles to infer-
ring the goal information. To address this challenge, we
present the Reflective and Imaginative Language Agent
(RILA). By employing multi-modal models to process sen-
sory data, we instruct an LLM-based planner to actively ex-
plore the environment. During the exploration, our agent
adaptively evaluates and dismisses inaccurate perceptual
descriptions. Additionally, we introduce an auxiliary LLM-
based assistant to enhance global environmental compre-
hension by mapping room layouts and providing strategic
insights. Through comprehensive experiments and analy-
sis, we show that our method outperforms relevant base-
lines without training demonstrations from the environment
and complementary semantic information.
1. Introduction
Intelligent agents are anticipated to navigate intricate envi-
ronments, leveraging both auditory and visual stimuli [31,
38]. Considering a scenario that a vase falls and breaks,
a robot must swiftly pinpoint a target within a room, rely-
ing primarily on transient auditory cues. This need under-
pins our focus on the Semantic Audio-Visual Navigation
(SA VN) task [11]. In SA VN, the target object within the
scene emits intermittent sounds, which the agent must use,
in conjunction with visual information, to find the object.
In addition to the ambiguous goal information conveyed
through sporadic sounds, intricate room layouts and com-
plex navigation trajectories also present significant chal-
*Equal Contribution
Sink can’t be in the Lobby here
Perception might be wrong
Sink might be in the 
imagined toilet region
Sink from left
 Perception:
Reflective 
Planner:No other information
Follow the perception Reflective 
Planner:
Imaginative
Assistant:Imagine a toilet on the 
right side because 
there’s a mirror. 
Find goalImagined toiletFigure 1. An illustration of our agent’s strategy for semantic audio-
visual navigation. The Reflective Planner initiates navigation by
relying on perceptual information for exploration. When explo-
ration leads to an incorrect region, it subsequently discounts the
perceptual descriptions, redirecting its focus. Throughout this pro-
cess, the Imaginative Assistant persistently contributes spatial in-
sights and suggestions, thereby assisting in reasoning.
lenges [44], rendering the SA VN task notably difficult. Pre-
vious research [11] concentrated on the end-to-end train-
ing of reinforcement learning models, yielding inadequate
performance despite the use of extensive training trajec-
tories. Recent approaches enhance performance by inte-
grating auxiliary modules [44] or employing oracle instruc-
tions [31, 38], which may not be feasible in real-world ap-
plications.
Large language models (LLMs) [35, 36] have shown re-
markable progress [30, 47]. Beyond the promising perfor-
mance on natural language tasks [37, 40], the integration
of LLMs into embodied robotics applications has also re-
sulted in substantial improvements [2, 15, 16, 48, 50]. Re-
cent methods [55, 56] equip LLMs with multi-modal mod-
els [28, 29] that provide perception and feedback from the
environment, either explicitly [49, 51] or implicitly [20, 24],
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
16251
in vision-and-language navigation tasks [4]. However, these
applications also fail on SA VN due to their reliance on pre-
cise perception information and explicit goal descriptions.
Consequently, realizing zero-shot SA VN, as anticipated for
intelligent agents, remains a formidable challenge.
Therefore, we propose our Reflective and Imaginative
Language Agent (RILA), leveraging the inherent common-
sense reasoning capabilities of LLMs to perform zero-shot
SA VN. Practically, we design distinct perception models
that process audio and visual signals, which further guide
a frozen LLM in strategic planning. Through active ex-
ploration of the environment, our agent adaptively identi-
fies and deprioritizes misleading goal descriptions. Fur-
thermore, we introduce an LLM-based imaginative assis-
tant, which extracts room layouts and provides high-level
guidance. Incorporating this assistant enables our agent
to achieve comprehensive environmental understanding and
navigate toward the target object in a zero-shot manner.
Fig. 1 provides an illustration of our agent’s navigation.
To validate our approach, we conduct experiments
within the SoundSpaces framework. Experimental results
show that our method surpasses relevant baselines without
reliance on training demonstrations or complementary mod-
ules. Notably, our agent exhibits a success rate exceeding
60% when paired with oracle perceptions, highlighting the
strong planning capability of LLMs. Additionally, we con-
duct a thorough analysis of the bottleneck of the current task
configuration. We summarize our contributions as follows:
• We propose RILA for zero-shot SA VN, exploiting the
commonsense reasoning capabilities of LLMs to navigate
effectively without precise goal descriptions.
• We introduce an imaginative assistant, designed to deduce
the environment’s room layout and provide comprehen-
sive suggestions, thereby enhancing the navigation.
• Experiments substantiate that RILA surpasses previous
baselines, which require training, in a zero-shot manner.
We also conduct a thorough analysis of the SA VN task.
2. Related Work
2.1. Semantic Audio Visual Navigation
Semantic audio-visual navigation is defined in Habitat [33,
41] with the SoundSpaces dataset [10, 13]. Previous re-
search [9, 52] extract features from RGB-D images and
two-channel spectrograms using pre-trained encoders sep-
arately [3, 10], and then train an end-to-end policy network
by reinforcement learning to predict the next action. How-
ever, these methods lack generalizability, failing in unsu-
pervised scenes [44] despite necessitating extensive training
demonstrations. Recent methods [31, 38] query for human
instructions during the navigation. K-SA VEN [44] further
constructs a knowledge graph to provide spatial comprehen-
sion. Instead of training on massive demonstrations, ourmethod exploits the commonsense reasoning capabilities of
LLMs to perform solve the task in a zero-shot manner.
2.2. Navigation with Large Language Models
LLMs have recently demonstrated impressive reasoning
abilities across a range of tasks [21, 39], including embod-
ied tasks [17]. Recent studies [43, 54] investigate visual-
language navigation with LLMs. For instance, ESC [56]
employs LLMs to deduce relationships between objects,
thereby aiding navigation. Chen et al. [14] and Szot et al.
[42], on the other hand, utilize visual foundation models
to convert perceptions into natural language instructions.
However, the application of LLMs in SA VN remains un-
derexplored, especially since prior methods often rely on
ground-truth goal descriptions. In contrast, RILA reflec-
tively navigates toward the target, handling potentially mis-
leading goal descriptions.
2.3. Layout Complementary
Spatial understanding, particularly regarding room lay-
out, is crucial for comprehending complex environments.
LGD[27] employs a room-type codebook to conceptual-
ize room layouts from image clips. Text2Room [23], con-
versely, creates entire rooms guided by textual instructions.
Recent LayoutGPT [18] taps into the visual planning ca-
pabilities of LLMs to produce plausible layouts for visual
generation. In our work, RILA utilizes LLMs to progres-
sively deduce the room layout and type, thereby achieving
a global understanding of the environment.
3. Method
In this work, we consider solving the Semantic Audio Vi-
sual Navigation (SA VN) task [11] in a zero-shot manner,
challenging agents to locate the sounding object within an
intricate and unseen environment. Notably, the audio sig-
nals here are sporadic and often absent, posing a signifi-
cant challenge to the agent’s decision-making process. In-
stead of training on trajectories from the simulated environ-
ment or incorporating additional semantic information, we
leverage the intrinsic commonsense reasoning capabilities
of LLMs for navigation planning.
3.1. Overview
In this section, we provide an overview of our RILA frame-
work, illustrated in Fig. 2. RILA consists of three parts:
the perception module, the Imaginative Assistant, and the
Reflective Planner, which we will introduce separately.
The perception module transforms the sensory data into
natural language descriptions. Visual perceptions ov
tare di-
rectly processed via a pre-trained visual-language model,
which discerns and catalogs the observed objects, thereby
facilitating the construction of a semantic top-down map.
16252
[Thoughts]: You are probably in the living room. There are several surrounding rooms, including a lobby, a hallway, etc. [Suggestion]: The sinkseems in the kitchen.
[Goal Localization]: The audio sounds from the rear right.[Observations]: You have observed several objects:1. A Sofa at <524,874>2....[Goal Prediction]: The audio sounds from a Sink.[Thoughts]: The perceptual information might be misleading.[Planning]: Navigate to <640,740>Visual
Auditory
ImaginativeAssistantSemantic MapObject Grounding
Audio Map
History
ReflectivePlanner
[Action]: Turn LeftDeterministic Policy
Perception ModuleImaginative Assistant(ImaAssistant)Reflective Planner (RefPlanner)Audio Classification
PretrainModelsPromptTemplatesFigure 2. The architecture of our agent comprises three primary components. Firstly, the perception module transforms sensory inputs
into text-based descriptions. Secondly, the Imaginative Assistant analyzes regional information and offers strategic guidance from a global
perspective. Lastly, by integrating the two components, our Reflective Planner assesses perceptual data and navigates toward the target.
We develop distinct modules for auditory perceptions oa
tto
pinpoint the goal location and identify pertinent semantic
cues, given the intermittent nature. Both perceptions are
then synthesized into a text-based format for planning. A
detailed description is illustrated in Section 3.2.
Extending beyond individual objects, we integrate an
LLM-based Imaginative Assistant (ImaAsssistant) to de-
duce room layouts, thereby enriching the spatial compre-
hension of intricate environments. ImaAsssistant then uti-
lizes the layout of both explored and partially observed ar-
eas to provide strategic planning guidance, aiding in navi-
gation. A thorough explanation is provided in Section 3.3.
By amalgamating insights from the perception module and
ImaAsssistant, our Reflective Planner (RefPlanner) lever-
ages inherent commonsense reasoning abilities to explore
the environment and identify misleading auditory descrip-
tions, circumventing the need for exact sound localization.
Detailed explanations are shown in Section 3.4.
3.2. Audio Visual Perception
Following [44], we use the same pre-trained audio classifi-
cation model Ma
objto infer the target object. Considering the
transient nature of audio signals, which presents a consid-
erable obstacle in precise identification, we employ a pro-
gressive strategy. Upon an audio signal oa
tat time step t,
we make a prediction Ma
obj({oa
1,...,t})by amalgamating the
current audio with the accumulative history, facilitating a
refined accuracy. The object ˆgtwith the highest cumulative
prediction score at time tis thus designated as the currentgoal object:
ˆgt= argmax
g tX
i=11Ma
obj({oa
1,...,i})=g!
, (1)
where 1denotes the indicator function. Guided by the pre-
diction ˆgt, we aim to further localize it, thereby improving
the distinction of the target from analogous entities in the
environment. Nonetheless, the complex reverberation of the
simulation poses a significant challenge for localization, as
evidenced by an error margin of about 8 meters [11].
Therefore, we partition the localization into indepen-
dent estimations of distance and direction. To quantify
sound distance, we collected 10,000 unheard auditory sam-
ples from the training environment to delineate the simu-
lation’s dimensional attributes. A pre-trained ResNet-18
model fine-tuned on this dataset demonstrates commend-
able accuracy in estimating distances. Predicting direction,
however, is substantially more arduous.
Instead of ascertaining the precise angle, we shift to
identify the binary directionality, greatly simplified by the
dual-sensor configuration. Nonetheless, techniques such
as Interaural Time Difference (ITD) [6, 22] and fine-tuned
models fall short of the task, which is further discussed in
Section 5. Consequently, we employ weighted predictions
based on the Root Mean Square (RMS) intensity of audi-
tory signals from the dual channels, denoted by Rt
landRt
r.
Practically, we consider the audio source to be from the side
with the larger RMS intensity. For each point pand time t,
16253
the confidence Ct
pis calculated as:
Ct
p=tX
i=1wa
i· 1RMS(p, oa
i), (2)
where 1RMS(p, o)is an indicator function which is equal
to1ifpis located, with respect to the agent, in the side cor-
responding to the larger RMS intensity given observation
o, and the weight wa
tis calculated as wa
t=|Rt
l−Rt
r|
max ( Rt
l,Rtr).
Through iteratively accumulating the weighted predictions,
we construct an audio map that facilitates an approximate
localization of the goal.
To transform visual signals into linguistic representa-
tions, we employ the pre-trained GroundingDINO for both
delineating bounding boxes and identifying the objects
within the RGB observation, thereby furnishing a rudimen-
tary environmental understanding. Besides, we separately
prompt to detect the predicted goal object in case the tar-
get is missed. Simultaneously, a semantic top-down map is
constructed from the Depth observations, with the map seg-
mented into distinct regions demarcated by detected walls,
enabling the assistant to provide a region-level comprehen-
sion. A more detailed illustration of our perception modules
is further provided in Appendix.
3.3. Imaginative Assistant
Given the restricted information from the perception mod-
ule, the planning relies mainly on discrete objects. How-
ever, a global environmental understanding substantially
benefits planning, especially for distant goals requiring
multi-room navigation. To address this, we integrate an
auxiliary LLM-based Imaginative Assistant (ImaAsssis-
tant), offering strategic suggestions to bolster navigation.
In practice, ImaAsssistant infers room layouts. By par-
titioning the semantic map into regions using the detected
walls, we instruct ImaAsssistant to determine closed room
types from observed objects. Yet, as a comprehensive ex-
ploration of a room rarely occurs, partially observed rooms
are more frequently encountered. Therefore, we utilize the
spatial imagination capabilities of LLMs to conceptualize
the layout of these rooms, subsequently directing it to de-
duce room types by interior objects and adjacent rooms. We
present below simplified versions of the prompts.
/* Task Description */
Please infer the room type and precise layout of the
provided interested region.
/* Room Layouts */
Observed Rooms: living room , etc.
Partially Observed Room: wall 1,wall 2, etc.
Internal Objects: chair 1,chair 2,table , etc.Through iterative deduction of both observed and par-
tially observed rooms, RILA attains a comprehensive un-
derstanding of the environment, which yields additional in-
sights beyond the scope of individual objects. To augment
the planning, ImaAsssistant is further instructed to provide
strategic navigation advice. Rather than specific waypoints,
ImaAsssistant reasons about the potential goal locations,
considering spatial layouts and semantic attributes. These
insights enable ImaAsssistant to make suggestions that as-
sist in selecting waypoints more effectively. A simplified
version of the prompt template is presented below.
/* Task Description */
Given the room layout, infer where the Counter is.
Give your advice about which room to explore.
/* Information */
Current room: living room
Surrounding rooms: kitchen ,hallway , etc.
3.4. Reflective Planner
By incorporating layouts and suggestions from ImaAsssis-
tant, our LLM-based Reflective Planner (RefPlanner) har-
nesses the inherent commonsense reasoning capabilities in
planning based on perceptions. At each time step t, au-
dio and visual perceptions are formatted as Goal Descrip-
tionandObservation , respectively. Additionally, a Task De-
scription is articulated at the outset. A simplified template
for the perception prompt is as follows:
/* Task Description */
You are performing a navigation task.
/* Goal Description */
Navigate to the object that sounds like a Counter .
/* Observations */
You have observed the following objects.
With a natural language synopsis of the environment and
the designated navigational objective, we commission Ref-
Planner to strategize high-level planning. Rather than spec-
ifying actions outright, we implement a heuristic method,
frontier-based exploration (FBE), which discerns the junc-
tures between explored and uncharted territories as poten-
tial waypoints for environmental reconnaissance. Instead of
determining specific action, RefPlanner is directed to rea-
son and select an exploration frontier based on current per-
ceptions in a zero-shot manner. The navigation history of
perceptions and reasonings is also provided. Practically, we
implement a deterministic policy for decomposing the way-
point into action sequences. Utilizing a connected graph de-
rived from the semantic top-down map, we apply Dijkstra’s
algorithm to determine the shortest path to the waypoint.
Moreover, as outlined in Section 3.2, the perception de-
16254
scriptions, particularly the goal location, are often ambigu-
ous and may lead to misconceptions, while an intelligent
agent is anticipated to actively interact with the environment
to make judgments about uncertain perceptions. Therefore,
along with the localization confidence of the frontier from
the perception module, we hint to RefPlanner about the po-
tential inaccuracy, which empowers it to explore the en-
vironment adaptively and reflect the reliability of percep-
tion, thus enhancing its proficiency in locating the target
object. The layouts and suggestions from ImaAsssistant are
included as well. We present below a simplified version of
the template used for the navigation prompt.
/* Agent Position */
You are at ⟨x, y⟩
/* Hint */
The perceptual confidence is not always accurate.
/* Frontier Candidates */
Frontier 1: ⟨x, y⟩in the living room
Perceptual confidence: c
Surrounding objects: chair 1,chair 2,table , etc.
/* Suggestions */
The goal object may be in the kitchen .
As shown in Fig. 1, RefPlanner adaptively selects appro-
priate waypoints from a global perspective. When RefPlan-
ner fails to find the target after exploring an area based on
perceptions, it identifies perceptual inaccuracies and navi-
gates using object characteristics. The full prompt scheme
and a detailed example of the navigation are provided in
Appendix. In practice, we implement all LLMs using the
March 2023 version of gpt-3.5-turbo , leveraging the
OpenAI LLM API service1with a temperature of 0.0.
4. Experiments
4.1. Experimental Setup
4.1.1 Datasets
We use SoundSpaces [10, 13] from Habitat [33, 41] envi-
ronment to simulate navigation in 3D environments. We
adopt the Matterport3D (MP3D) dataset for its ground-truth
region layout labels and object labels. In particular, we
evaluate our RILA on 1,000 test episodes within 10 unseen
scenes with unheard sounds from 21 goal objects.
4.1.2 Baselines
We compare our model with several baselines:
•AudioGoal [10] uses a GRU state encoder to acquire the
following action with an end-to-end RL policy network.
1https://platform.openai.com/docs/models•A V-WAN [12] designs a waypoint predictor and leverages
a local path planner to navigate to the waypoint.
•SA Vi [11] uses a goal descriptor network to predict both
the classification and location of the sounding object.
•A VLEN [38] adopts a hierarchical RL policy with goal
predictor and memory unit, and queries oracle instruc-
tions from humans if necessary.
•K-SA VEN [44] proposes an end-to-end policy network
with a knowledge graph constructed on the training data,
presenting the relationship between regions and objects.
In addition, we incorporate two zero-shot methods based
on foundation models to facilitate a more comprehensive
comparison. The ground truth goal object is provided here.
•ImageBind-LLM [20] is a novel multi-modality
model that aggregates ImageBind [19] and LLaMA-
Adapter [53] and we use the perfect stop strategy.
•ESC [56] leverages LLMs and Probabilistic Soft Logic
(PSL) [5] to choose a frontier for a visual-language navi-
gation task. We provide our audio goal description.
4.1.3 Metrics
Following previous work [9, 38, 44], we report agent per-
formance with the following metrics: Success Rate (SR),
Success Rate weighted by Path Length (SPL), Success Rate
weighted by Number of Actions (SNA), and Success When
Silent (SWS), all in percentage (%). We also report the av-
erage Distance To Goal (DTG) in meters at episode end.
4.1.4 Implementation Details
Consistent with previous studies, the agent is provided with
RGB and depth images at a resolution of 256 ×256. It
also receives two-channel audio clips in the form of 65 ×
26 spectrograms. The action space includes MoveForward ,
TurnRight ,TurnLeft , and Stop, with a movement step set at
1 meter. Additionally, the agent obtains its GPS location
at each time step. Detailed implementation details are pro-
vided in Appendix.
4.2. Experimental Results
The comparative results are presented in Table 1. We derive
the results of major baselines from their respective papers.
For ESC and ImageBind-LLM, we incorporate ground-truth
audio descriptions for the SA VN task. Implementation de-
tails are provided in the Supplementary Material. Accord-
ing to Table 1, our agent surpasses baselines that utilize
end-to-end reinforcement learning training, such as SA Vi,
in a zero-shot manner. Even when juxtaposed with base-
lines that utilize additional information, RILA achieves a
higher success rate. Besides, we notice that Imagebind-
LLM fails on the SA VN task, despite incorporating ground-
truth audio descriptions, reflecting the limited performance
16255
Method SR (%) ↑SPL (%) ↑SNA (%) ↑DTG (m) ↓SWS (%) ↑
SupervisedAudioGoal [10] 16.5 15.5 10.4 12.8 5.6
A V-WAN [12] 17.2 13.2 12.7 11.0 6.9
SA Vi [11] 24.8 17.2 13.2 9.9 14.7
A VLEN [38] 26.2 17.6 14.2 9.2 15.8
K-SA VEN [44] 34.4 23.4 21.7 6.6 14.3
Zero-ShotImagebind-LLM†[20] + Audio∗2.4 1.5 1.1 22.6 1.4
ESC [56] + Audio∗23.6 8.0 4.8 17.7 14.2
Ours w/o Assistant 31.4 9.6 6.8 12.2 15.3
Ours 35.4 11.8 8.7 11.4 20.4
Table 1. Comparison with relevant baselines on SoundSpaces Matterport3D test dataset. A VLEN incorporates extra oracle instructions. †
denotes the perfect stop strategy and Audio∗indicates that the ground-truth audio description is provided. In contrast, our method requires
no training trajectories or additional semantic information.
Method SR (%) ↑SPL (%) ↑SWS (%) ↑
Random†19.8 11.8 16.2
Nearest†9.8 22.6 6.4
Llama-2 7B 39.4 22.2 35.4
Ours 60.8 39.6 56.6
Table 2. Ablation study on RefPlanner by replacing it with heuris-
tic frontier selection methods and replacing the ChatGPT with
Llama-2.†indicates using oracle stop.
of open-source multi-modality foundation models on com-
plex embodied tasks. Notably, our approach significantly
outperforms previous works in terms of SWS, with over
40% improvement over K-SA VEN. This underscores the
exceptional efficacy of our method in scenarios involving
long distances and intermittent sounds, thereby highlight-
ing the potential of harnessing the commonsense reasoning
abilities of LLMs for navigation in physical environments.
We observe a relatively lower SPL of our method, at-
tributed to the fact that RILA requires holistic exploration of
the environment to ascertain the target object due to the ab-
sence of end-to-end training. Additionally, given the vague
nature of the goal descriptions, RILA adopts a more cau-
tious strategy for navigation, often traversing longer dis-
tances before reaching the objective. For better illustration,
we provide two cases of snapshots of the navigation pro-
cess using RILA in Fig. 3. As demonstrated in the left case
study, RILA initially explores the living room, guided by er-
roneous perceptual cues. Upon realizing the absence of the
goal object, RILA shifts its navigation toward the bathroom,
utilizing object characteristics to locate the toilet. This pro-
cess highlights RILA’s ability to effectively reflect on poten-
tially misleading goal descriptions, a factor that inevitably
results in a lower SPL. We posit that enhancing audio lo-
calization, perhaps through the well-established Neural Ra-diance Fields (NeRF) [34], could further improve the SPL.
Moreover, as depicted in the right case of Fig. 3, when the
RefPlanner encounters unexplored areas, the ImaAssistant
supplies conjectural room layouts. The spatial insight di-
rects the RefPlanner to explore the kitchen instead of the
dining room in search of the sink, underscoring the ImaAs-
sistant’s utility. Overall, RILA demonstrates the capacity to
adaptively navigate complex environments.
4.3. Ablation Study
Ablation on ImaAssistant. As shown in Table 1, the in-
tegration of ImaAssistant markedly improves performance,
underscoring the impact of strategic guidance. We also ob-
served considerable advancements in SWS, demonstrating
the crucial role of comprehensive layout understanding for
long-distance navigation in intricate settings.
Ablation on RefPlanner. We replace our frontier selec-
tion RefPlanner with two heuristic frontier-based explo-
ration methods, namely Random which selects a frontier
randomly, and Nearest which selects the nearest frontier.
We also compare the ability of GPT-3.5 and Llama-2 for
frontier selection by replacing GPT-3.5 in RefPlanner with
Llama-2. To eliminate the effect from perception, we use
ground-truth perceptions ( i.e., acoustic object, audio map)
in these experiments. In the two heuristic approaches, we
automatically execute the Stop action when the distance to
the goal is less than 1m. As illustrated in Table 2, despite
access to ground-truth perceptions, these heuristic methods
exhibit poor performance. Notably, Llama-2 7B [45] also
struggles to locate the goal object, indicating the lack of
spatial reasoning ability of Llama-2 for navigation tasks.
Ablation on Perception Module. Furthermore, we con-
ducted a comprehensive evaluation of the perception mod-
ules across 500 episodes from 10 scenes. Results are shown
16256
Kitchen
Region layout Agent path Imagine region Goal objectStep
Agent
Step Case2: Sink Case1: ToiletSemantic map Region split Observation
Shortest pathUnknown
Living roomLobby
Living room
Living roomLobby
Living room
Living roomLobby HallwayBathroom
Kitchen
Dining roomKitchen
Kitchen
Dining roomKitchen
Predicted sound locationFigure 3. Visualization of two navigation trajectories, including region layouts and egocentric observations. The left case demonstrates how
RefPlanner reflects on a misleading perception, whereas the right case illustrates that ImaAssistant makes imagination and suggestions,
guiding RefPlanner in waypoint selection based on semantic relevance.
Perception Accuracy ( ↑)
Object Recognition 83.9%
Audio Classification 93.0%
Audio Distance 83.8%
Audio Direction 73.7%
Table 3. Accuracy results of different perception modules. Object
recognition accuracy represents the probability that the detected
item is correctly classified. Audio distance prediction is deemed
accurate within a 4-meter error range.
in Table 3. GroundingDINO achieves an 85.0% recall rate
on object recognition, indicating only a 15.0% error rate in
goal object identification. For all recognized objects, the
accuracy also reaches a notable 83.9%. Similarly, the audio
classifier distinguishes among 21 classes with an accuracy
rate of up to 93.0%. By progressively refining the predic-
tion, RILA made correct predictions in almost all episodes.
The accuracy of audio distance prediction is also commend-
able, reaching 83.8% within a 4-meter margin of error, and
has an average distance error of 2.8 meters. Conversely, the
accuracy of binary judgments on audio direction is limited
to 73.7%, indicating a significant likelihood of error accu-
mulation over steps. To investigate whether the direction
judgment is impacted by complex reverberations in intri-
cate environments, we further separately evaluate episodes
based on whether the goal distance is less or more than 15
meters. Accuracy reached 85.6% for shorter distances, inMethod SR ↑SPL↑DTG↓
Ours 30.2 9.0 11.8
+ GT Audio Semantic 30.2 11.2 11.6
+ GT Audio Localization 52.4 24.6 6.4
+ GT Visual Perception 62.0 39.2 4.8
Table 4. Comparison of incorporating different ground-truth per-
ceptions on the validation dataset. Experiments in each row in-
clude the ground-truth information from all previous rows.
contrast to only 59.5% for longer distances. These findings
underscore the difficulty of making binary direction deter-
minations in SA VN, particularly over extended distances.
In conclusion, each component of RILA demonstrates
competitive performance, with the exception of direction
classification, which tends to be less reliable. To delve
deeper into the capabilities of RILA, we present a compre-
hensive analysis in Section 5.
5. Analysis and Discussion
In this section, we focus on the following research ques-
tions: (i) Are LLMs adequate for completing complex nav-
igation tasks? (ii) Does the sensory data provided by the
SoundSpaces simulation offer clarity and sufficiency for ef-
fective navigation? (iii) Are there any inappropriate sce-
nario settings within the current task configuration?
16257
Relative Angle (rad)
ITD (ms)ILD (dB)Relative Angle (rad)(a)(b)Figure 4. ILT and ITD of the sampled data points. We present the
linear regression and the corresponding confidence intervals.
LLMs excel in intricate language-based navigation with
inherent commonsense reasoning capabilities. By inte-
grating ground-truth perceptual information, we investigate
the navigational planning capabilities of LLMs. Rather than
specifying precise goal locations, we provide only a rough
area. According to the results in Table 4, our agent achieves
a success rate exceeding 60% with a DTG under 5 on the
validation dataset. Failures typically arise from encounter-
ing similar objects in the target area or due to the inherent
limitations of FBE in long-distance navigation. These find-
ings further confirm the adequacy of LLMs’ planning abili-
ties for navigational tasks.
Besides, we observe that providing only ground-truth au-
ditory data yields commendable performance. Conversely,
the success rate markedly decreases in the absence of pre-
cise audio location information, consistent with the experi-
mental results of the perception modules. Although RILA
can effectively utilize potentially imprecise perceptual de-
scription, it remains vulnerable to misdirection caused by
similar objects, thereby constraining the overall perfor-
mance. These observations suggest that the current bottle-
neck in the SA VN task lies in sound source localization.
The auditory sensory data is inadequate for precise lo-
calization. To further investigate the audio localization,
we sampled 4,000 dual-channel audio data points from the
environment and computed two metrics: Interaural Level
Difference (ILD) [46] and ITD. These metrics, crucial for
sound source identification in dual-channel audio [1, 32],
measure differences in sound intensity and arrival time, re-
spectively. The results are depicted in Fig. 3, where the x-
axis represents the sound source angle relative to the agent.
Ideally, these metrics should display a pronounced negative
correlation with the angle [25]. Our analysis reveals that
while ILD demonstrates some negative correlation, serv-
ing as the basis for our direction classification, ITD does
not effectively indicate the sound’s relative direction. This
underlines the constraints of the current audio input con-
figuration [26], complicating precise localization based on
auditory inputs. Detailed analysis is provided in Appendix.
Agent path Goal object Agent Shortest path
 Wrong region
Figure 5. An example of an episode where the goal object is in-
distinguishable. In this case, the target is far from the agent and
surrounded by similar, incorrect items.
Some cases could be further improved. Even in the ab-
sence of precise localization, semantic cues are expected to
guide the agent to the target. However, our observations re-
veal situations where both audio localization is imprecise
and semantic information fails to sufficiently differentiate
between objects. For instance, as illustrated in Figure 5,
the sounding object is distant from the agent, surrounded
by numerous similar items, such as eight chairs in this case.
In SA VN, where sounds are intermittent, the agent must se-
mantically discern the correct stopping point. In this ex-
ample, only two positions would lead to success. Lacking
adequate reasoning cues, the agent resorts to random selec-
tion, leading to failure without exact goal location details.
We postulate that these episodes could be improved by in-
troducing distinct visual differences in target objects, such
as overturning chairs, thus providing definitive cues for the
agent to accurately identify the target.
6. Conclusion
In this work, we propose RILA, a reflective and imaginative
agent for zero-shot semantic audio-visual navigation. By
utilizing distinct models for sensory data processing, RILA
guides an LLM-based reflective planner in active environ-
mental exploration. During this exploration process, RILA
reflectively assesses and disregards erroneous sensory per-
ceptions, especially the goal descriptions. Besides, we inte-
grate an LLM-based auxiliary imaginative assistant to gen-
erate room layouts and offer strategic guidance, thereby im-
proving global understanding of the environment. Compre-
hensive experimental results show the efficacy of RILA.
Acknowledgement
We thank Weiye Du, Jie Yin, and Jiaben Chen for their in-
sightful help. We thank the anonymous reviewers for the
helpful suggestions. This work is funded in part by grants
from Microsoft Accelerating Foundation Models Research
and Mitsubishi Electric Research Laboratories.
16258
References
[1] Neil Aaronson and William Hartmann. Testing, correcting,
and extending the Woodworth model for interaural time dif-
ference. The Journal of the Acoustical Society of America ,
135, 2014. 8
[2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Cheb-
otar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu,
Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,
Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Ir-
pan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally
Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov,
Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu,
Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao,
Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Ser-
manet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vin-
cent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,
Mengyuan Yan, and Andy Zeng. Do as I can and not as I say:
Grounding language in robotic affordances. arXiv preprint
arXiv:2204.01691 , 2022. 1
[3] Ziad Al-Halah, Santhosh K. Ramakrishnan, and Kristen
Grauman. Zero experience required: Plug & play modu-
lar transfer learning for semantic visual navigation. arXiv
preprint arXiv:2202.02440 , 2022. 2
[4] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark
Johnson, Niko S ¨underhauf, Ian Reid, Stephen Gould, and
Anton Van Den Hengel. Vision-and-language navigation:
Interpreting visually-grounded navigation instructions in real
environments. In CVPR , pages 3674–3683, 2018. 2
[5] Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise
Getoor. Hinge-loss markov random fields and probabilistic
soft logic. arXiv preprint arXiv:1505.04406 , 2017. 5
[6] Joshua G. W. Bernstein, Olga A. Stakhovskaya, Ger-
ald I. Schuchman, Kenneth Kragh Jensen, and Matthew J.
Goupell. Interaural time-difference discrimination as a mea-
sure of place of stimulation for cochlear-implant users with
single-sided deafness. Trends in Hearing , 22, 2018. 3
[7] G.C. Carter. Coherence and time delay estimation. Proceed-
ings of the IEEE , 75(2):236–255, 1987. 1
[8] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Nießner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d
data in indoor environments, 2017. 1
[9] Changan Chen, Ziad Al-Halah, and Kristen Grau-
man. Semantic audio-visual navigation. arXiv preprint
arXiv:2012.11583 , 2020. 2, 5
[10] Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vi-
cenc Amengual Gari, Ziad Al-Halah, Vamsi Krishna Ithapu,
Philip Robinson, and Kristen Grauman. SoundSpaces:
Audio-visual navigation in 3D environments. In ECCV ,
pages 17–36. Springer, 2020. 2, 5, 6, 1
[11] Changan Chen, Ziad Al-Halah, and Kristen Grauman. Se-
mantic audio-visual navigation. In CVPR , pages 15516–
15525, 2021. 1, 2, 3, 5, 6
[12] Changan Chen, Sagnik Majumder, Ziad Al-Halah, Ruohan
Gao, Santhosh Kumar Ramakrishnan, and Kristen Grauman.
Learning to set waypoints for audio-visual navigation. In
ICLR , 2021. 5, 6[13] Changan Chen, Carl Schissler, Sanchit Garg, Philip
Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra,
Philip W Robinson, and Kristen Grauman. SoundSpaces 2.0:
A simulation platform for visual-acoustic learning. NeurIPS
Datasets and Benchmarks Track , 2022. 2, 5
[14] Peihao Chen, Dongyu Ji, Kunyang Lin, Runhao Zeng,
Thomas H. Li, Mingkui Tan, and Chuang Gan. Weakly-
supervised multi-granularity map learning for vision-and-
language navigation. arXiv preprint arXiv:2210.07506 ,
2022. 2
[15] Peihao Chen, Xinyu Sun, Hongyan Zhi, Runhao Zeng,
Thomas H Li, Gaowen Liu, Mingkui Tan, and Chuang Gan.
aˆ 2nav: Action-aware zero-shot robot navigation by exploit-
ing vision-and-language ability of foundation models. arXiv
preprint arXiv:2308.07997 , 2023. 1
[16] Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha
Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador,
Dustin Schwenk, Eli VanderBilt, Matthew Wallingford, Luca
Weihs, Mark Yatskar, and Ali Farhadi. RoboTHOR: An open
simulation-to-real embodied AI platform. In CVPR , 2020. 1
[17] Vishnu Sashank Dorbala, James F. Mullen Jr. au2, and Di-
nesh Manocha. Can an embodied agent find your ”cat-
shaped mug”? LLM-guided exploration for zero-shot object
navigation. arXiv preprint arXiv:2303.03480 , 2023. 2
[18] Weixi Feng, Wanrong Zhu, Tsu jui Fu, Varun Jampani, Ar-
jun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and
William Yang Wang. LayoutGPT: Compositional visual
planning and generation with large language models. arXiv
preprint arXiv:2305.15393 , 2023. 2
[19] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan
Misra. ImageBind: One embedding space to bind them all,
2023. 5
[20] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng
Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu
Guo, Xudong Lu, Shuai Ren, Yafei Wen, Xiaoxin Chen,
Xiangyu Yue, Hongsheng Li, and Yu Qiao. ImageBind-
LLM: Multi-modality instruction tuning. arXiv preprint
arXiv:2309.03905 , 2023. 1, 5, 6
[21] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen
Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with
language model is planning with world model. arXiv
preprint arXiv:2305.14992 , 2023. 2
[22] David B. Hawkins, Lamar L. Young, and Cheryl Parker. An
investigation of the interaural time difference threshold for
speech. Perception & Psychophysics , 24:168–170, 1978. 3
[23] Lukas H ¨ollein, Ang Cao, Andrew Owens, Justin Johnson,
and Matthias Nießner. Text2Room: Extracting textured 3D
meshes from 2D text-to-image models. In ICCV , 2023. 2
[24] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng,
Yilun Du, Zhenfang Chen, and Chuang Gan. 3D-LLM:
Injecting the 3D world into large language models. arXiv
preprint arXiv:2307.12981 , 2023. 1
[25] Maike Klingel, Norbert Kop ˇco, and Bernhard Laback.
Reweighting of binaural localization cues induced by later-
alization training. JARO: Journal of the Association for Re-
search in Otolaryngology , 22:551 – 566, 2020. 8
16259
[26] Christine K ¨oppl and Catherine Emily Carr. Maps of interau-
ral time difference in the chicken’s brainstem nucleus lami-
naris. Biological Cybernetics , 98:541–559, 2008. 8
[27] Mingxiao Li, Zehao Wang, Tinne Tuytelaars, and Marie-
Francine Moens. Layout-aware dreamer for embod-
ied referring expression grounding. arXiv preprint
arXiv:2212.00171 , 2022. 2
[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 1
[29] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun
Zhu, et al. Grounding DINO: Marrying DINO with grounded
pre-training for open-set object detection. arXiv preprint
arXiv:2303.05499 , 2023. 1
[30] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng,
Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang.
WebGLM: Towards an efficient web-enhanced question an-
swering system with human preferences. arXiv preprint
arXiv:2306.07906 , 2023. 1
[31] Xiulong Liu, Sudipta Paul, Moitreya Chatterjee, and
Anoop Cherian. Active sparse conversations for im-
proved audio-visual embodied navigation. arXiv preprint
arXiv:2306.04047 , 2023. 1, 2
[32] Louise Loiselle, Michael Dorman, William Yost, Sarah Na-
tale, and Ren ´e Gifford. Using ILD or ITD cues for sound
source localization and speech understanding in a complex
listening environment by listeners with bilateral and with
hearing-preservation cochlear-implants. Journal of Speech
Language and Hearing Research , 59:1, 2016. 8
[33] Manolis Savva*, Abhishek Kadian*, Oleksandr
Maksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain,
Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi
Parikh, and Dhruv Batra. Habitat: A platform for embodied
AI research. In ICCV , 2019. 2, 5
[34] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021.
6
[35] OpenAI. Introducing ChatGPT, 2022. (Accessed on Jun 18,
2023). 1
[36] OpenAI. GPT-4 technical report. arXiv preprint
arXiv:2303.08774 , 2023. 1
[37] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Pad-
makumar, Jason Phang, Jana Thompson, Phu Mon Htut, and
Samuel Bowman. BBQ: A hand-built bias benchmark for
question answering. In Findings of the Association for Com-
putational Linguistics (ACL) , pages 2086–2105, Dublin, Ire-
land, 2022. Association for Computational Linguistics. 1
[38] Sudipta Paul, Amit K Roy-Chowdhury, and Anoop Cherian.
A VLEN: Audio-visual-language embodied navigation in 3d
environments. In NeurIPS , 2022. 1, 2, 5, 6
[39] Dhruv Shah, Bła ˙zej Osi ´nski, Sergey Levine, et al. LM-
Nav: Robotic navigation with large pre-trained models of
language, vision, and action. In Conference on Robot Learn-
ing, pages 492–504. PMLR, 2023. 2[40] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu
Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R.
Brown, Adam Santoro, Aditya Gupta, Adri `a Garriga-
Alonso, et al. Beyond the imitation game: Quantifying
and extrapolating the capabilities of language models. arXiv
preprint arXiv:2206.04615 , 2023. 1
[41] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans,
Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam,
Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan,
Vladimir V ondrus, Sameer Dharur, Franziska Meier, Wo-
jciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun,
Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat
2.0: Training home assistants to rearrange their habitat. In
NeurIPS , 2021. 2, 5
[42] Andrew Szot, Max Schwarzer, Bogdan Mazoure, Harsh
Agrawal, Walter Talbott, Katherine Metcalf, Natalie Mack-
raz, Devon Hjelm, and Alexander Toshev. Large language
models as generalizable policies for embodied tasks. arXiv
preprint arXiv:2310.17722 , 2023. 2
[43] Yujin Tang, Wenhao Yu, Jie Tan, Heiga Zen, Aleksan-
dra Faust, and Tatsuya Harada. SayTap: Language to
quadrupedal locomotion, 2023. 2
[44] Gyan Tatiya, Jonathan Francis, Luca Bondi, Ingrid Navarro,
Eric Nyberg, Jivko Sinapov, and Jean Oh. Knowledge-driven
scene priors for semantic audio-visual embodied navigation.
arXiv preprint arXiv:2212.11345 , 2022. 1, 2, 3, 5, 6
[45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288 , 2023. 6
[46] Balemir Uragun and Ramesh Rajan. The discrimination of
interaural level difference sensitivity functions: development
of a taxonomic data template for modelling. BMC Neuro-
science , 14:114 – 114, 2013. 8
[47] Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchun-
shu Zhou, Shaochun Hao, Guangzheng Xiong, Yizhi Li,
Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, Zhenzhu
Yang, Adam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo
Liu, Wenhu Chen, Ke Xu, Dayiheng Liu, Yike Guo, and Jie
Fu. Interactive natural language processing. arXiv preprint
arXiv:2305.13246 , 2023. 1
[48] Justin Wasserman, Karmesh Yadav, Girish Chowdhary, Ab-
hinav Gupta, and Unnat Jain. Last-mile embodied visual
navigation. In Conference on Robot Learning , 2022. 1
[49] Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chen-
guang Zhu, and Julian McAuley. Small models are valu-
able plug-ins for large language models. arXiv preprint
arXiv:2305.08848 , 2023. 1
[50] Karmesh Yadav, Santhosh Kumar Ramakrishnan, John
Turner, Aaron Gokaslan, Oleksandr Maksymets, Rishabh
Jain, Ram Ramrakhya, Angel X Chang, Alexander Clegg,
Manolis Savva, Eric Undersander, Devendra Singh Chap-
lot, and Dhruv Batra. Habitat challenge 2022. https:
//aihabitat.org/challenge/2022/ , 2022. 1
[51] Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan,
Madhavan Iyengar, David F Fouhey, and Joyce Chai.
16260
LLM-Grounder: Open-vocabulary 3D visual grounding
with large language model as an agent. arXiv preprint
arXiv:2309.12311 , 2023. 1
[52] Abdelrahman Younes, Daniel Honerkamp, Tim
Welschehold, and Abhinav Valada. Catch me if you
hear me: Audio-visual navigation in complex unmapped
environments with moving sounds. arXiv preprint
arXiv:2111.14843 , 2023. 2
[53] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun
Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and
Yu Qiao. LLaMA-Adapter: Efficient fine-tuning of language
models with zero-init attention, 2023. 5
[54] Zhen Zhang, Anran Lin, Chun Wai Wong, Xiangyu Chu, Qi
Dou, and KW Au. Interactive navigation in environments
with traversable obstacles using large language and vision-
language models. arXiv preprint arXiv:2310.08873 , 2023.
2
[55] Gengze Zhou, Yicong Hong, and Qi Wu. NavGPT: Explicit
reasoning in vision-and-language navigation with large lan-
guage models. arXiv preprint arXiv:2305.16986 , 2023. 1
[56] Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen,
Hongxia Jin, Lise Getoor, and Xin Eric Wang. ESC: Ex-
ploration with soft commonsense constraints for zero-shot
object navigation. arXiv preprint arXiv:2301.13166 , 2023.
1, 2, 5, 6
16261
