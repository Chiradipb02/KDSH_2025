Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for
Audio-Visual Segmentation
Qi Yang1,2Xing Nie1,2Tong Li3Pengfei Gao3Ying Guo3
Cheng Zhen3Pengfei Yan3Shiming Xiang1,2
1School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS)
2Institute of Automation, Chinese Academy of Sciences (CASIA)3Meituan
Abstract
Recently, an audio-visual segmentation (AVS) task has
been introduced, aiming to group pixels with sounding ob-
jects within a given video. This task necessitates a first-
ever audio-driven pixel-level understanding of the scene,
posing significant challenges. In this paper, we propose
an innovative audio-visual transformer framework, termed
COMBO, an acronym for COoperation of Multi-order Bi-
lateral relatiOns. For the first time, our framework ex-
plores three types of bilateral entanglements within AVS:
pixel entanglement, modality entanglement, and temporal
entanglement. Regarding pixel entanglement, we employ
a Siam-Encoder Module (SEM) that leverages prior knowl-
edge to generate more precise visual features from the foun-
dational model. For modality entanglement, we design a
Bilateral-Fusion Module (BFM), enabling COMBO to align
corresponding visual and auditory signals bi-directionally.
As for temporal entanglement, we introduce an innovative
adaptive inter-frame consistency loss according to the in-
herent rules of temporal. Comprehensive experiments and
ablation studies on AVSBench-object (84.7 mIoU on S4,
59.2 mIou on MS3) and AVSBench-semantic (42.1 mIoU on
AVSS) datasets demonstrate that COMBO surpasses previ-
ous state-of-the-art methods. Project page is available at
https://yannqi.github.io/AVS-COMBO.
1. Introduction
Human visual attention is often hear-guided ,i.e., we
tend to focus on the object with sounds [4]. For example,
when we hear a cat meow, we pay more attention to the
cat than other objects due to the strong association between
the meow and the cat. Inspired by this potential interac-
tion of auditory and visual signals, the cross-modal studies
of vision and hearing have attracted numerous researchers,
such as the audio-visual correspondence [1, 2], which only
aims to match visual images and audio signals to the same
Modality EntanglementTemporal Entanglement
Pixel Entanglement
CATR [ACM MM'23]
BAVS [Arxiv'23]
AVS-BiGen [Arxiv'23]AVSBench [ECCV'22]
AV-SAM [Arxiv'23]
AVSegFormer [Arxiv'23]ECMV AE [ICCV'23]
Hear2Seg [Arxiv'23]
CSMF [Arxiv'23]
Ours: COMBO
Figure 1. Comparison between the proposed COMBO and ex-
isting state-of-the-art methods. Our COMBO is the first work to
simultaneously explore multi-order bilateral relations in modality,
temporal and pixel levels.
scene, and sound source localization [5, 18, 34] which fur-
ther seeks to locate the vocal visible regions. However,
they have only focused on audio-visual tasks at the image
or region levels, lacking pixel-level annotations. Recently,
A VSBench [44, 45] integrates audio signals into video seg-
mentation, called Audio-Visual Segmentation (A VS), which
comprises two benchmarks: 1) AVSBench-object , which
includes single source sound segmentation (S4) and mul-
tiple sound sources segmentation (MS3); 2) AVSBench-
semantic , which further extends audio-visual semantic seg-
mentation (A VSS) based on A VSBench-object.
Given that audio-visual segmentation is a burgeoning
field that spans both audio and visual modalities, it presents
a non-trivial task. Generally, when performing A VS, a
cross-modal segmentation task involving video, there are
mainly three challenges: (1) A VS contains audio and visual
modalities, thus demanding explicit alignment of sequential
audio features to spatial pixel-level activations; (2) A VS in-
volves temporal information where the state of the current
frame is dynamically affected by historical frames, there-
fore, exploring the correlation between adjacent frames is
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27134
essential; (3) A VS includes an image segmentation task;
compared to 1D audio signals, 2D image signals have more
redundant information, which is prone to be affected by
background noise, thus requiring precise extraction of fea-
tures from the image. To resolve the issue (1), the prevail-
ing methods [13, 25, 26, 31, 33] employ matrix multiplica-
tion and modified cross-attention module to encode pixel-
wise audio-visual interaction. Though impressive, these de-
signs ignore the temporal dependence of adjacent frames
that have been proven to be important for A VS. To solve
this problem, some methods [15, 23, 44] partition tempo-
ral relations into consideration to explore both issues (1)
and (2) simultaneously since A VS is a cross-modal video
task. Nevertheless, their approaches rely too much on im-
plicit inter-frame relations, leading to inaccurate associa-
tions. Regarding issue (3), CSMF [3] leverages frozen large
foundation models to extract pure visual features for A VS.
However, it independently tackles the audio and visual sig-
nals by naively combining several existing foundation mod-
els, resulting in sub-optimal performance.
To this end, we present COMBO , a novel audio-visual
transformer framework for A VS. According to the three is-
sues mentioned above, COMBO simultaneously considers
modality, temporal, and pixel levels by introducing their
bilateral entanglements, as shown in Fig. 1. Specifically,
nature itself has many bilateral relations. For example, in
electricity and magnetism, due to their intrinsic correlation,
the change of current causes the change of magnetic field,
and vice versa. Motivated by this, we refer to this bilateral
relationship of mutual influence as entanglement.
In this work, we explore three potential bilateral entan-
glements: pixel entanglement, modality entanglement, and
temporal entanglement. Pixel entanglement refers to the in-
terdependent relationship between an image and its corre-
sponding mask. Since background noise in the image leads
to inaccuracies in the image-to-mask prediction process, it
is essential to utilize external masks from the foundation
models to entangle the input image to assist the model.
Therefore, we construct a Siam-Encoder Module (SEM) as
a visual feature extractor to facilitate more precise visual
features, which can liberate from the constraints of founda-
tion models than other methods [26, 33, 37]. Besides, as
for the alignments of the audio and visual signals, we ex-
plore the modality entanglement between audio and visual
components to amplify the efficiency of cross-modal match-
ing. Contrary to existing single-fusion methods [13, 44],
we believe that the cooperation between the two modali-
ties can produce a positive effect. Inspired by [24], we ini-
tially propose a potent and memory-efficient bidirectional
audio-visual fusion module called Bilateral-Fusion Mod-
ule (BFM). Our BFM amplifies the spatial awareness of
visual features relevant to sounding objects and strength-
ens the attention of audio signals embodying visual targets.Moreover, the audio-visual tasks contain a solid temporal
entanglement. Thus, we design an adaptive inter-frame con-
sistency loss to better harness this inherent characteristic.
Our main contributions can be summarized as follows:
• We propose a Siam-Encoder Module (SEM) that transfers
the knowledge of the foundation model for mining the
potential pixel entanglement.
• We propose a Bilateral-Fusion Module (BFM) to take full
advantage of the potential of both audio and visual modal-
ities by exploring the modality entanglement.
• We propose an adaptive inter-frame consistency loss
based on the inherent coherence of audio-visual tasks for
enhanced temporal entanglement.
• We show that COMBO significantly outperforms existing
state-of-the-art approaches in the challenging A VSBench-
object and A VSBench-semantic datasets.
2. Related Work
2.1. Sound Source Localization
Sound source localization aims to estimate the position
of a sound source in a video sequence, which is the most
related task to the audio-visual segmentation task. LVS [5]
utilizes a hard-mining strategy and a contrastive learning
mechanism to discriminate challenging image fragments.
DSOL [18] executes class-aware sounding object localiza-
tion from mixed sound, which initially focuses on learning
robust object representations from single-source localiza-
tion. MSSL [34] localizes multiple sound sources in un-
constrained videos without pairwise sound-object annota-
tions. This approach involves the development of a two-
stage learning framework, followed by the execution of
cross-modal feature alignment. The pioneering methods in
these areas have significantly inspired our research on A VS.
2.2. Semantic Segmentation
Semantic segmentation is a fundamental task that re-
quires pixel-level classification. Early researchers take the
Fully Convolutional Networks (FCN) [27] as the dominant
approach and focus on aggregating long-range context in
the feature map. PSPNet [42] performs spatial pyramid
pooling at several grid scales. DeepLab [7, 8] utilizes atrous
convolutions with different atrous rates. Furthermore, some
methods [35, 38, 43] replace traditional convolutional back-
bone with transformer-based architectures. MaskFormer [9]
and Mask2Former [11] propose a mask classifier with learn-
able queries and specialized designs for mask prediction.
OneFormer [19] presents a universal image segmentation
framework that unifies segmentation with a multi-task train-
once design. Recently, a series of SAM models [20–22, 40]
propose to build a foundation model for promptable seg-
mentation with strong generalization. Given that the A VS
task entails segmentation, these studies have significantly
27135
Bilateral-Fusion ModuleAdaptive Inter-Frame
Consistency LossSegmentation Module
MaskClass
Pixel
Decoder
1/32 Stage1/16 Stage1/8 Stage
Transformer Decoder
 
Audio
Encoder
❄Siam-Encoder Module
Image 
Encoder
Maskige 
Encoder
❄Proposal
Generator
MLP
GAPInput Frames Maskiges AudioBi-AttnFigure 2. Overview of the proposed COMBO. COMBO adopts a novel audio-visual transformer framework specifically for audio-visual
segmentation. Aiming at multi-oder bilateral entanglement, our method is composed of three independent modules. (1) We introduce the
Siam-Encoder Module, which is designed for the exploration of pixel entanglement. (2) To integrate the entanglement of audio and visual
signals, we propose a Bilateral-Fusion Module. (3) Given the inherent characteristics of temporal entanglement, we construct an adaptive
inter-frame consistency loss in the segmentation module to enhance the consistency of the output.
contributed to our work.
2.3. Audio-Visual Segmentation
A VS is an emerging task that aims to locate sounding
sources by predicting pixel-wise maps and attracts many re-
searchers [13, 23, 26, 30, 31, 33, 37, 45]. A VSBench [45]
first constructs the audio-visual segmentation benchmark
and proposes a temporal pixel-wise audio-visual interaction
module (TPA VI) to inject audio semantics as guidance for
the visual segmentation process. A VSegformer [13] pro-
poses a transformer architecture that introduces audio fea-
tures into the transformer decoder, enabling the network to
attend to interested visual features selectively. CATR [23]
proposes a combinatorial dependence fusion approach that
comprehensively accounts for the spatial-temporal depen-
dencies of audio-visual combination. Some methods [15,
30, 31] take advantage of the generative manners with la-
tent diffusion model or variational auto-encoder to address
A VS task. In addition, A V-SAM [33], GA VS [37], and
BA VS [26] utilize the large foundation model to bootstrap
audio-visual segmentation. Different from the above meth-
ods [13, 26, 37, 44], our proposed COMBO rethinks A VS
from bilateral relations of three entanglements, which en-
hances the model’s representation ability by exploring the
pixel, modality and temporal inherent relationships.
3. Method
3.1. Bilateral Visual Features Extraction
As illustrated in Fig. 2, our method initiates with the
extraction of visual features primarily because the audio-
visual segmentation (A VS), as a dense prediction task, ex-hibits extensive pixel entanglement in visual perception.
Recent studies [21, 22, 40] have demonstrated that the Seg-
ment Anything Model [20] exhibits robust generalization
performance in segmentation tasks. Consequently, trans-
ferring the impressive capabilities of the foundation model
to more complex visual tasks, such as A VS, presents an
intriguing and valuable research question. The extension,
however, is not straightforward. Although some meth-
ods [3, 33] attempt to fine-tune or concatenate the pre-
trained SAM model for A VS, the limited capacity of the
frozen foundation model restricts its performance to address
the A VS task. Additionally, the A VS task aims to predict all
sound targets per pixel, whereas the SAM model is only ca-
pable of generating class-agnostic masks without any audio
guidance, thus demonstrating a significant disparity. There-
fore, transferring the foundation model’s knowledge to the
A VS task presents a tough challenge.
Maskige as Prior Knowledge. To solve the aforemen-
tioned issues, we believe that a feasible strategy is to in-
corporate the knowledge of the foundation model into vi-
sual features as a pixel entanglement, which is memory-
efficient. Specifically, as depicted in the left area of Fig. 2,
we introduce a Maskige generator. Given the input image
x∈R3×H×Wand the frozen foundation model, one can
derive the class-agnostic masks c∈RK×H×W, where K
represents the number of potential targets. It is essential
to highlight that Kis dynamic and varies with input im-
ages. Thus, we first amplify the quantity of class-agnostic
masks from KtoNwith zero masks and obtain a series of
binary masks, where Nis predetermined. However, given
that the output of SAM is a series of binary masks, it is dif-
ficult to integrate them into visual features. Consequently,
27136
Linear
Reshape
Audio FeaturesAdd ＆  Norm
Linear Linear Linear
Spatial Positional
EncodingsLearnable Positional 
EncodingsSoftmax.T SoftmaxLinear LinearAdd ＆  Norm
Reshape
Visual FeaturesFigure 3. Illustration of Bilateral-Fusion Module (BFM). We input
both visual and image signals, which are subsequently processed
through bilateral attention to yield the fused visual and image fea-
tures respectively. We omit the subscripts of HandWfor better
understanding. For enhanced visibility, the dashed line indicates a
skip connection. Best viewed in color.
inspired by [6], we introduce Maskige m∈R3×H×W,
which shares the exact dimensions as the input image, to
integrate prior knowledge better. To efficiently generate
Maskige m, we employ a random color encoding function
X(·) :RN→R3that is capable of transforming the binary
masks c∈RN×H×Winto Maskige m∈R3×H×Wwithout
extra training. Specifically, Xis designed to enhance the
distinguishability of the Maskige and can be regarded as a
linear layer X(c) =cA, where A∈RN×3. To facilitate
offline inference using the Maskige generator, the value of
Ais manually set appropriately without additional training.
More details are in Appendix.
Siam-Encoder Module (SEM) . To incorporate the image-
like Maskiges as prior knowledge into input frames, we
propose intertwining the features of Maskige and visual
elements during the feature extraction stage. Accord-
ingly, we design a Siam-Encoder Module, as depicted in
Fig. 2. This module encompasses an Image encoder Ev
and a Maskige encoder Em, sharing a common frame-
work. More precisely, for a short video clip with Tframes
I∈RT×3×H×W, the Maskiges can be generated using the
Maskige generator, resulting in M∈RT×3×H×W. Sub-
sequently, we extract multiple output features from both
the image encoder and Maskige encoder, respectively. This
process can be defined as follows:
Fvisual =Ev(I;θv), Fvisual∈ {Fvi}4
i=1, (1)Fmaskige =Em(M;θm), Fmaskige ∈ {Fmi}4
i=1,(2)
in which FviandFmi∈RT×H
2i+1×W
2i+1×Ci.Cirepresents
the dimension of the i-th stage output features. To integrate
the Maskige features into COMBO, we introduce channel-
weighted blocks that augment the original visual features.
The formula can be written as follows:
Fvi=Fmi(GAP(Fmi)W) +Fvi, i={1,2,3,4},(3)
where GAP stands for global average pooling, and W∈
RCi×Cirepresents the linear weight. For simplicity, the
bias is omitted in this context. After obtaining the Mask-
iges as prior information to boost pixel-level entanglement
with visual features, the next critical aspect is exploring the
modality entanglement between audio and visual signals.
3.2. Audio-Visual Bilateral Fusion
The relationship between any two modalities can be
characterized as bilateral entanglement. For instance, an
image can be described in text, and sound is inextrica-
bly linked with its visual counterpart. This entanglement
among these distinct modalities provides an invaluable re-
source for researchers tackling multimodal tasks. Prior
studies [38, 44] have overemphasized the influence of audio
on visual features, thereby underestimating the significance
of visual information to audio features. To address this im-
balance, we propose a Bilateral-Fusion Module (BFM) in
COMBO that surpasses a mere single fusion effect.
Audio Feature Extraction. For an audio clip correspond-
ing to the input frames, we adopt VGGish [17] to ex-
tract audio features following [44]. Firstly, the audio clip
is resampled to yield a 16kHz mono output Amono ∈
RNsamples ×96×64, where Nsamples is related to the dura-
tion of the audio. Then, a short-time Fourier transform is
performed to yield a mel spectrum, denoted as Amel∈
RT×96×64. Finally, the mel spectrum is subsequently fed
into the VGGish model, resulting in the extraction of au-
dio features Fa∈RT×D, where Tdenotes the number of
frames and Drepresents dimension of the audio.
Bilateral-Fusion Module (BFM). We initially employ the
pixel decoder [46] to gradually upsample visual features Fvi
derived from the SEM to further generate high-resolution
per-pixel embeddings Pi∈RT×H
2i+1×W
2i+1×C, i∈
{1,2,3,4}, where Cdenotes the output channel. Then, we
design a Bilateral-Fusion Module (BFM) for constructing a
bidirectional audio-visual mapping to assist with segment-
ing the sounding objects.
As shown in Fig. 3, our BFM utilizes audio features
Fa∈RT×D, in conjunction with the largest pixel-level em-
beddings P1∈RT×H1×W1×Cas inputs which can propa-
gate ample fine-grained semantic information to audio fea-
27137
0.968 0.936 0.919 0.975
0.986 0.985 0.980 0.991FramesFigure 4. Illustration of the impact on Adaptive Inter-frame Con-
sistency Loss. We visualize the heat map of the predicted masks
without and with the consideration of Ladabased on the S4 subset.
The results indicate that implementing Ladapromotes superior in-
terframe consistency. Best viewed in color.
tures. Here, H1=H/4,;W1=W/4. In order to incorpo-
rate both two signals, bilateral attention is designed within
our BFM. Specifically, we initially add fixed sine spatial
positional encodings and learnable positional encodings to
P1andFa, respectively. Next, in order to integrate audio-
visual modalities in a memory-efficient way, our BFM com-
prises four point-wise linear layers that map P1andFa
to intermediate representations with dimension d. These
representations share queries and keys with queries Q=
P1WQ, keys K=FaWK, visual values Vv=P1Wv
V, and
audio values Va=P1Wa
V. Following the mapping process,
the bilateral attention is as follows:
P1=Softmax (QKT/√
d)Va+P1, (4)
Fa=Softmax ((QKT/√
d)T)Vv+Fa, (5)
where dis the embedding dimension. And (QKT/√
d)is
calculated only once, which is more efficient.
After BFM, we proceed by expanding fused audio fea-
turesFaadded with learnable embeddings within the trans-
former decoder as object queries. Additionally, following
[10], we generate the output classes Oclsby incorporating
the per-pixel embeddings P4, P3, P2into the transformer
decoder. We acquire the predicted masks Omaskby multi-
plying the output embeddings from the transformer decoder
with the fused embedding P1.
3.3. Mining Temporal Relationships
Adaptive Inter-frame Consistency Loss. Temporal al-
ways implies a bilateral relationship in nature. For instance,
in video clips, one can deduce the scenario of the current
frame based on the past frame. Similarly, it is also feasi-
ble to predict the future frame based on the current frame.
This interactive relations among frames can be construed as
a type of temporal entanglement. To take advantage of thispotential temporal entanglement, we introduce an adaptive
inter-frame consistency loss for A VS. The similarity score
for each successive frame can be calculated as follows:
St:t+1=cos(Omask
t, Omask
t+1), (6)
where Omask
t refers to the predicted masks at frame t, and
cos(·)symbolizes the cosine similarity function. The term
St:t+1represents the similarity score between frames tand
t+ 1. As illustrated in Fig. 4, it is evident that a signif-
icant similarity exists between distinct frames. Therefore,
to leverage this prior information, we propose an adaptive
inter-frame consistency loss, formulated as follows:
Lada=T−1X
t=1exp(St:t+1−1)(1− St:t+1), (7)
where exp(St:t+1−1)represents adaptive weight. When
the disparity between adjacent frames is substantial, the
adaptive weight item is minimal, aligning with intuition.
3.4. Training and Inference
Overall Training Loss. The comprehensive training loss
comprises three components: classification loss, mask loss,
and adaptive inter-frame consistency loss, as previously dis-
cussed. The classification loss is formulated by a cross-
entropy loss, denoted as Lcls=Lce. The mask loss in-
tegrates the binary cross-entropy loss and the dice loss [32],
and is depicted as Lmask =Lbce+Ldice. Considering that
in the A VS task, the ratio of segmented objects to the total
image area is relatively small, employing dice loss allows
the model to better concentrate on the foreground and mini-
mizes distraction from the background. The overall training
loss is expressed as follows:
L=λclsLcls+λmaskLmask +λadaLada, (8)
where λcls,λmask , and λadaare hyperparameters. More
details about the λparameters are available in Sec. 4.2.
Semantic Inference. After obtaining the predicted embed-
dings Ocls∈RT×Nq×(Kc+1)and binary masks Omask∈
RT×Nq×H×W, where Kcrepresents the total number of ob-
ject classes and Nqis the number of object queries, we em-
ploy the identical post-processing as in [10] to yield the final
semantic segmentation outputs. Specifically, we first calcu-
late the output mask with classes O=Ocls×Omask∈
RT×(Kc+1)×H×W. Then, we execute arg max and discard
theno object class to obtain the ultimate results.
4. Experiments
4.1. A VSBench Datasets
We evaluate our proposed method on the A VSBench
dataset [45], which consists of two scenarios: A VSBench-
object and A VSBench-semantic.
27138
Method BackboneS4 MS3
MJMFMJMF
LVS [5] ResNet-18 37.9 51.0 29.5 33.0
MSSL [34] ResNet-18 44.9 66.3 26.1 36.3
3DC [28] ResNet-152 57.1 75.9 36.9 50.3
SST [12] ResNet-101 66.3 80.1 42.6 57.2
iGAN [29] ResNet-50 61.6 77.8 42.9 54.4
LGVT[41] Swin-B 74.9 87.3 40.7 59.3
A VSBench [45] ResNet-50 72.8 84.8 47.9 57.8
PVT-v2 78.7 87.9 54.0 64.5
CSMF [3] ViT-B 58.0 67.0 34.0 44.0
A VS-BiGen [15] ResNet-50 74.1 85.4 45.0 56.8
PVT-v2 81.7 90.4 55.1 66.8
CATR [23] ResNet-50 74.8 86.6 52.8 65.3
PVT-v2 81.4 89.6 59.0 70.0
DiffusionA VS [30] ResNet-50 75.8 86.9 49.8 58.2
PVT-v2 81.4 90.2 58.2 70.9
ECMV AE [31] ResNet-50 76.3 86.5 48.7 60.7
PVT-v2 81.7 90.1 57.8 70.8
BA VS [26] ResNet-50 78.0 85.3 50.2 62.4
PVT-v2 82.0 88.6 58.6 65.5
A VSegFormer [13] ResNet-50 76.5 85.9 49.5 62.8
PVT-v2 82.1 89.9 58.4 69.3
COMBO (ours) ResNet-50 81.7 90.1 54.5 66.6
(+3.7) (+4.8) (+2.7) (+1.3)
PVT-v2 84.7 91.9 59.2 71.2
(+2.6) (+2.0) (+0.2) (+1.2)
Table 1. Quantitative comparison results of different methods on
A VSBench-object (Single-source, S4; Multi-source, MS3). We
use the same backbones (ResNet-50 and PVT-v2) to demonstrate
that our method outperforms other methods significantly.
Method BackboneA VSS
MJ MF
3DC [28] ResNet-18 17.3 21.6
AOT [39] ResNet-50 25.4 31.0
A VSBench [45] ResNet-50 20.2 25.2
PVT-v2 29.8 35.2
BA VS [26] ResNet-50 24.7 29.6
PVT-v2 32.6 36.4
A VSegFormer [13] ResNet-50 24.9 29.3
PVT-v2 36.7 42.0
COMBO (ours) ResNet-50 33.3 37.3
(+8.4) (+8.0)
PVT-v2 42.1 46.1
(+5.4) (+4.1)
Table 2. Quantitative comparison results on A VSBench-semantic.
A VSBench-object. A VSBench-object [44] is an audio-
visual dataset specifically designed for sound target seg-
mentation. A VSBench-object includes two scenarios based
on the number of audio sources in each frame: single
sound source segmentation (S4) and multiple sound source
segmentation (MS3). The S4 scenario incorporates 4,932ModuleS4 A VSS
MJMFMJMF
COMBO 81.7 90.1 33.3 37.3
w/o Siam-Encoder 80.6 88.7 31.9 35.7
w/o Bilateral-Fusion 81.1 89.9 33.1 36.7
w/o Inter-Frame Loss 81.0 89.8 33.0 37.1
Table 3. Ablation study of the various modules included in
COMBO. We sequentially remove our proposed modules and
compare their performance.
videos, with the ratio of train/validation/test split ratio con-
figured at 70/15/15. This scenario is trained in a semi-
supervised manner, wherein each video comprises five
frames, but annotation during training is limited to the first
frame only. Conversely, the MS3 scenario is characterized
by multiple sound sources, including 424 videos, and main-
tains the same split ratios as in the S4 scenario. This sce-
nario, unlike S4, employs a fully supervised training ap-
proach with all five frames being annotated.
A VSBench-semantic. A VSBench-semantic [45] is an ex-
tension to A VSBench-object that incorporates additional se-
mantic labels for the purpose of enhancing audio-visual
semantic segmentation (A VSS). A VSBench-semantic in-
cludes a set of new multi-source videos as well as the origi-
nal A VSBench-object videos, cumulatively accounting for a
total of 11,356 videos spanning 70 categories. These videos
are allocated as follows: 8,498 for training, 1,304 for valida-
tion, and 1,554 for testing. Additionally, the original videos
have only been enhanced with semantic information, main-
taining the same frames as prior. However, the new videos
have been extended to 10frames, increasing the difficulty
due to the inclusion of extended audio-visual sequences.
4.2. Experimental Setup
Implementation Details. For fair comparison, we
adopt the ImageNet pre-trained ResNet-50 [16] and Pyra-
mid Vision Transformer (PVT-v2) [36] as the visual siam-
encoders. All input frames are resized to 224×224. In
terms of the proposal generator, we leverage the Semantic-
SAM [22] to obtain class-agnostic masks. As for audio in-
put, we take the Vggish encoder [17] pre-trained on Au-
dioSet [14] to extract audio features. Following [10], the
Multi-Scale Deformable Attention Transformer (MSDefor-
mAttn) is our default pixel decoder. Besides, we adopt the
standard transformer decoder, with L= 3 (i.e., a total of 9
layers) and Nq= 100 as the default. The hyperparameters
are set as λcls= 2andλmask = 5. For the inter-frame con-
sistency loss, we set λada= 10 for the A VSBench-object
dataset, λada= 5 for the A VSBench-semantic dataset due
to longer frames per video. We calculate the inter-frame
consistency loss only in the intermediate transformer de-
coder layer to prevent an over-dependence on temporal in-
formation. All models are trained using the Adam optimizer
27139
SEM ModuleS4 A VSS#Params
MJMFMJMF
w.shared weights 81.5 90.2 31.1 34.4 135.5
w/o shared weights 81.7 90.1 33.3 37.3 158.9
Table 4. Ablation study of Siam-Encoder Module (SEM). We ex-
plore two configurations: using shared weights and using separate
weights. #Params denotes the model parameters (M).
Fusion ModeS4 A VSS
MJ MF MJ MF
w/o fusion 81.1 89.9 33.1 36.7
visual fusion only 81.3 89.6 32.6 36.4
audio fusion only 81.3 90.2 33.2 36.7
bilateral fusion 81.7 90.1 33.3 37.3
Table 5. Ablation study of fusion mode. We conduct among four
modes: no fusion, visual fusion only, audio fusion only, and our
fusion mode.
with a learning rate of 1e−4and weight decay of 0.05. We
train the S4 and A VSS subsets for 90k and MS3 for 20k it-
erations with a batch size of 8 on a single A100 40GB GPU.
Metrics. Folloing [44], we adopt two metrics to verify
the effectiveness, namely, Jaccard index Jand F-score F.
Jcomputes the intersection over union (IoU) of the pre-
dicted segmentation and the ground truth mask. Fcon-
siders both precision and recall, which is represented as
Fβ=(1+β2)×presion×recall
β2×precision +recall, where β2is set at 0.3. In our
experiment, we use MJandMFto denote the mean met-
rics across the entire dataset.
4.3. Main Results
We conduct experiments on A VSBench-object and
A VSBench-semantic datasets. As A VS is an emerging pro-
posed problem recently introduced by [44], we compare our
COMBO with some state-of-the-art methods from other re-
lated tasks, such as sound source localization (SSL) [5, 34],
video object segmentation (VOS) [12, 28, 39], and salient
object detection (SOD) [29, 41], all of which provide
a comparative benchmark for our experiments. As evi-
denced in Tab. 1, COMBO demonstrates a substantial per-
formance gap ( +9.8mIoU in S4; +16.3mIoU in MS3)
over other related methods, principally attributable to vari-
ances in setting specific task scenarios. We also com-
pare our method against some recent state-of-the-art meth-
ods [13, 15, 23, 26, 30, 31, 44] that have been explicitly
designed for audio-visual segmentation settings. On the
A VSBench-object dataset, COMBO-R50 outperforms the
current best performance by achieving 3.7 mIoU and 4.8
F-score improvements for S4 subset and 2.7 mIoU and 1.3
F-score improvements for MS3, while COMBO-PVT sur-
passes the top-performing method by 2.6 mIoU and 2.0 F-
score for S4 and 0.2 mIoU and 1.2 F-score for MS3.
Besides, we compare the A VSBench-semantic dataset asQueriesS4 A VSS
MJ MF MJ MF
all 80.9 89.9 30.7 34.2
add 81.7 90.1 33.3 37.3
Table 6. Ablation study of learnable queries.
λadaS4 A VSS
MJ MF MJ MF
0 81.0 89.8 33.0 37.1
5 81.1 90.1 33.3 37.3
10 81.7 90.1 32.1 35.6
20 81.2 89.7 32.6 36.1
Table 7. Ablation study of the adaptive inter-frame consistency
loss. λadais the hyperparameter, while higher values constrain
the output to be more similar.
displayed in Tab. 2, which presents a more challenging set-
ting. Both COMBO-R50 and COMBO-PVT achieve sig-
nificant results, with 8.4 and 5.4 mIoU improvements and
significant F-score enhancements of 8.0 and 4.1, respec-
tively. These experiments confirm that our COMBO model
surpasses existing state-of-the-art methods across all sub-
tasks, consequently setting a new benchmark for A VS.
4.4. Abaltion Study
In this section, we conduct ablation studies to verify
the effectiveness of each essential design in the proposed
COMBO. Specifically, we adopt ResNet-50 [16] as the
backbone and carry out extensive experiments on the S4 and
A VSS sub-tasks due to more videos in these tasks. Other
training settings remain consistent with Sec. 4.2
Component analysis of COMBO. To validate the impact
of our proposed method, we separately eliminate the Siam-
Encoder Module (SEM), Bilateral-Fusion Module (BFM),
and adaptive inter-frame consistency loss. As demonstrated
in Tab. 3, the results indicate that COMBO has demon-
strated superior influence on SEM with 1.4 mIoU improve-
ment, particularly on the A VSS subset. Concurrently, our
findings indicate that BFM is of substantial significance,
demonstrating a performance enhancement of 0.6 mIoU
over the S4 subset. More analysis are discussed later. In
addition, we examine the effect of inter-frame consistency
loss. The results reveal that our loss function contributes to
performance improvements with 0.7 mIoU on the S4 subset.
More details are provided subsequently.
Effects of Siam-Encoder Module (SEM). We first exam-
ine the significance of our SEM. Two designs are compared
in Tab. 4: one with share weights and another with separate
weights. When comparing shared and separate parameters,
it is found that separable parameters can attain 0.2 mIoU on
the S4 subset, but it is indeed more costly than the others.
Effects of Bilateral-Fusion Module (BFM). Moreover, we
investigate the influence of our BFM. We compared our
27140
V isual
FramesAudiofemale singing female singing female singing female singing female singing
A VSBench
A VSegformer
COMBO
(ours)
Ground
T ruth
male singing
pianomale singing
pianomale singing
pianomale singing
pianomale singing
pianokeyboard keyboard keyboard keyboard keyboard
Figure 5. Comparison of Visual Examples on the A VSBench-object and A VSBench-semantic Datasets with A VSBench [45] and A VSeg-
former [13]. Wherein the leftmost example is derived from the S4 subset, the middle example is from the MS3 subset, and the rightmost
example is from the A VSS subset. Red bounding boxes highlight the specific regions for comparison.
modules using different variants, which include without any
fusion, video-only fusion (inject audio feature into visual),
audio-only fusion (inject visual feature to audio), and our
bilateral fusion. As demonstrated in Tab. 5, bilateral fusion
achieves a performance improvement of 0.6 mIoU com-
pared to the model without any fusion. It is worth noting
that the transformer decoder inherently consists of a cross-
attention function, potentially serving as a fusion process.
Our model also realizes a performance improvement of 0.4
mIoU compared to only audio or visual fusion.
Effects of Audio Queries. As demonstrated in Tab. 6, All
denotes the exclusive use of fused audio queries, and Add
signifies the combination of fused audio queries and learn-
able queries. We first expand the audio features to the exact
dimensions as learnable queries, then compare the experi-
ments with the exclusive use of fused audio queries and the
combination of fused audio queries and learnable queries.
The results show that queries with Add have 0.8 and 2.6
mIoU improvements over Allon S4 and A VSS subsets.
Effects of Adaptive Inter-frame Consistency Loss. To
validate the effect of Lada, we adopt different values of
λada. As shown in Tab. 7, the experiments demonstrate
that appropriate consistency constraints can enhance the
model’s performance. The appropriate value λadacan im-
prove performance by 0.7 mIoU and 0.3 F-score for S4. In
addition, the heat map of the predicted masks has been vi-
sualized in Fig. 4. It is observable that the use of Ladafa-
cilitates the generation of a more distinct boundary output.
Nevertheless, it is essential to note that exceedingly high
values may lead to a decline in performance, given that the
video does not exhibit complete consistency.
4.5. Qualitative Analysis
We provide a qualitative comparison between A VS-
Bench [45], A VSegformer [13] and our proposed methodon A VSBench-object and A VSBench-semantic datasets. As
depicted in Fig. 5, our method, COMBO, exhibits superior
audio-temporal and spatial localization quality, leading to
better visualization and segmentation performance. For in-
stance, in the case of the middle samples, our model ac-
curately segments the singing man despite the presence of
other sounds. Moreover, our method achieves more precise
segmentation for background noise handling and provides
richer details of the foreground in other examples.
5. Conclusion
We introduce a novel audio-visual transformer frame-
work, termed COMBO, that archives state-of-the-art per-
formance on A VSBench-object and A VSBench-semantic
datasets. Contrary to previous methodologies that only
factor in modality or temporal relations individually, our
method explores multi-order bilateral relations for the first
time, combining pixel entanglement, modality entangle-
ment, and temporal entanglement. For these three kinds of
entanglement, we propose Siam-Encoder Module (SEM),
Bilateral-Fusion Module (BFM), and adaptive inter-frame
consistency loss, respectively. Extensive experimental re-
sults verify the effectiveness of our proposed framework.
We hope that our work will inspire further research in this
significant and worthwhile field.
Acknowledgements
This research was supported by the Strategic Priority Re-
search Program of Chinese Academy of Sciences (Grant
No. XDB0500103), the National Natural Science Foun-
dations of China (Grants No. 62076242, 62376267), the
Pre-Research Project on Civil Aerospace Technologies
(No. D030312), the National Defense Basic Scientific Re-
search Program of China (No. JCKY2021203B063) and the
innoHK project.
27141
References
[1] Relja Arandjelovic and Andrew Zisserman. Look, listen and
learn. In Proceedings of the IEEE international conference
on computer vision , pages 609–617, 2017. 1
[2] Yusuf Aytar, Carl V ondrick, and Antonio Torralba. Sound-
net: Learning sound representations from unlabeled video.
Advances in neural information processing systems , 29,
2016. 1
[3] Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, and
Xiatian Zhu. Leveraging foundation models for un-
supervised audio-visual segmentation. arXiv preprint
arXiv:2309.06728 , 2023. 2, 3, 6
[4] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew
Zisserman. Vggsound: A large-scale audio-visual dataset.
InICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pages
721–725. IEEE, 2020. 1
[5] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Na-
grani, Andrea Vedaldi, and Andrew Zisserman. Localizing
visual sounds the hard way. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 16867–16876, 2021. 1, 2, 6, 7
[6] Jiaqi Chen, Jiachen Lu, Xiatian Zhu, and Li Zhang. Genera-
tive semantic segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 7111–7120, 2023. 4
[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic im-
age segmentation with deep convolutional nets, atrous con-
volution, and fully connected crfs. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 40(4):834–848,
2018. 2
[8] Liang-Chieh Chen, George Papandreou, Florian Schroff,
and Hartwig Adam. Rethinking atrous convolution for se-
mantic image segmentation. arxiv 2017. arXiv preprint
arXiv:1706.05587 , 2, 2019. 2
[9] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classification is not all you need for semantic segmen-
tation. Advances in Neural Information Processing Systems ,
34:17864–17875, 2021. 2
[10] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In 2022
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 1280–1289, 2022. 5, 6
[11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 1290–1299, 2022. 2
[12] Brendan Duke, Abdalla Ahmed, Christian Wolf, Parham
Aarabi, and Graham W. Taylor. Sstvos: Sparse spatiotem-
poral transformers for video object segmentation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 5912–5921, 2021.
6, 7[13] Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, and Tong
Lu. Avsegformer: Audio-visual segmentation with trans-
former. arXiv preprint arXiv:2307.01146 , 2023. 2, 3, 6,
7, 8
[14] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren
Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal,
and Marvin Ritter. Audio set: An ontology and human-
labeled dataset for audio events. In 2017 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Process-
ing (ICASSP) , pages 776–780, 2017. 6
[15] Dawei Hao, Yuxin Mao, Bowen He, Xiaodong Han, Yuchao
Dai, and Yiran Zhong. Improving audio-visual seg-
mentation with bidirectional generation. arXiv preprint
arXiv:2308.08288 , 2023. 2, 3, 6, 7
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 770–778, 2016. 6, 7
[17] Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis,
Jort F. Gemmeke, Aren Jansen, R. Channing Moore, Manoj
Plakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Malcolm
Slaney, Ron J. Weiss, and Kevin Wilson. Cnn architectures
for large-scale audio classification. In 2017 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Process-
ing (ICASSP) , pages 131–135, 2017. 4, 6
[18] Di Hu, Rui Qian, Minyue Jiang, Xiao Tan, Shilei Wen, Errui
Ding, Weiyao Lin, and Dejing Dou. Discriminative sounding
objects localization via self-supervised audiovisual match-
ing. Advances in Neural Information Processing Systems ,
33:10077–10087, 2020. 1, 2
[19] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita
Orlov, and Humphrey Shi. Oneformer: One transformer
to rule universal image segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2989–2998, 2023. 2
[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and
Ross Girshick. Segment anything. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 4015–4026, 2023. 2, 3
[21] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui
Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation
via large language model. arXiv preprint arXiv:2308.00692 ,
2023. 3
[22] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu,
Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao.
Semantic-sam: Segment and recognize anything at any gran-
ularity. arXiv preprint arXiv:2307.04767 , 2023. 2, 3, 6
[23] Kexin Li, Zongxin Yang, Lei Chen, Yi Yang, and Jun
Xun. Catr: Combinatorial-dependence audio-queried trans-
former for audio-visual video segmentation. arXiv preprint
arXiv:2309.09709 , 2023. 2, 3, 6, 7
[24] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and
Jianfeng Gao. Grounded language-image pre-training. In
27142
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 10965–10975,
2022. 2
[25] Yuhang Ling, Yuxi Li, Zhenye Gan, Jiangning Zhang, Ming-
min Chi, and Yabiao Wang. Hear to segment: Unmixing the
audio to guide the semantic segmentation, 2023. 2
[26] Chen Liu, Peike Li, Hu Zhang, Lincheng Li, Zi Huang,
Dadong Wang, and Xin Yu. Bavs: Bootstrapping audio-
visual segmentation by integrating foundation knowledge.
arXiv preprint arXiv:2308.10175 , 2023. 2, 3, 6, 7
[27] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In 2015
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , pages 3431–3440, 2015. 2
[28] Sabarinath Mahadevan, Ali Athar, Aljosa Osep, Laura Leal-
Taix´e, Bastian Leibe, and Sebastian Hennen. Making a case
for 3d convolutions for object segmentation in videos. In 31st
British Machine Vision Conference 2020, BMVC 2020, Vir-
tual Event, UK, September 7-10, 2020 . BMV A Press, 2020.
6, 7
[29] Yuxin Mao, Jing Zhang, Zhexiong Wan, Yuchao Dai, Aixuan
Li, Yunqiu Lv, Xinyu Tian, Deng-Ping Fan, and Nick Barnes.
Generative transformer for accurate and reliable salient ob-
ject detection. arXiv preprint arXiv:2104.10127 , 2021. 6,
7
[30] Yuxin Mao, Jing Zhang, Mochu Xiang, Yunqiu Lv, Yi-
ran Zhong, and Yuchao Dai. Contrastive conditional la-
tent diffusion for audio-visual segmentation. arXiv preprint
arXiv:2307.16579 , 2023. 3, 6, 7
[31] Yuxin Mao, Jing Zhang, Mochu Xiang, Yiran Zhong, and
Yuchao Dai. Multimodal variational auto-encoder based
audio-visual segmentation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
954–965, 2023. 2, 3, 6, 7
[32] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In 2016 Fourth International
Conference on 3D Vision (3DV) , pages 565–571, 2016. 5
[33] Shentong Mo and Yapeng Tian. Av-sam: Segment any-
thing model meets audio-visual localization and segmenta-
tion. arXiv preprint arXiv:2305.01836 , 2023. 2, 3
[34] Rui Qian, Di Hu, Heinrich Dinkel, Mengyue Wu, Ning Xu,
and Weiyao Lin. Multiple sound sources localization from
coarse to fine. In Computer Vision – ECCV 2020 , pages 292–
308, Cham, 2020. Springer International Publishing. 1, 2, 6,
7
[35] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia
Schmid. Segmenter: Transformer for semantic segmenta-
tion. In 2021 IEEE/CVF International Conference on Com-
puter Vision (ICCV) , pages 7242–7252, 2021. 2
[36] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt
v2: Improved baselines with pyramid vision transformer.
Computational Visual Media , 8(3):415–424, 2022. 6
[37] Yaoting Wang, Weisong Liu, Guangyao Li, Jian Ding, Di
Hu, and Xi Li. Prompting segmentation with sound is
generalizable audio-visual source localizer. arXiv preprint
arXiv:2309.07929 , 2023. 2, 3[38] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,
Jose M Alvarez, and Ping Luo. Segformer: Simple and
efficient design for semantic segmentation with transform-
ers. Advances in Neural Information Processing Systems ,
34:12077–12090, 2021. 2, 4
[39] Zongxin Yang, Yunchao Wei, and Yi Yang. Associating ob-
jects with transformers for video object segmentation. Ad-
vances in Neural Information Processing Systems , 34:2491–
2502, 2021. 6, 7
[40] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim,
Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong.
Faster segment anything: Towards lightweight sam for mo-
bile applications. arXiv preprint arXiv:2306.14289 , 2023. 2,
3
[41] Jing Zhang, Jianwen Xie, Nick Barnes, and Ping Li. Learn-
ing generative vision transformer with energy-based latent
space for saliency prediction. In Advances in Neural Infor-
mation Processing Systems , pages 15448–15463. Curran As-
sociates, Inc., 2021. 6, 7
[42] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. In 2017
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , pages 6230–6239, 2017. 2
[43] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,
Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao
Xiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic
segmentation from a sequence-to-sequence perspective with
transformers. In 2021 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 6877–6886,
2021. 2
[44] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun,
Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong,
Meng Wang, and Yiran Zhong. Audio–visual segmentation.
InEuropean Conference on Computer Vision , pages 386–
403. Springer, 2022. 1, 2, 3, 4, 6, 7
[45] Jinxing Zhou, Xuyang Shen, Jianyuan Wang, Jiayi Zhang,
Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Ling-
peng Kong, Meng Wang, et al. Audio-visual segmentation
with semantics. arXiv preprint arXiv:2301.13190 , 2023. 1,
3, 5, 6, 8
[46] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable {detr}: Deformable transform-
ers for end-to-end object detection. In International Confer-
ence on Learning Representations , 2021. 4
27143
