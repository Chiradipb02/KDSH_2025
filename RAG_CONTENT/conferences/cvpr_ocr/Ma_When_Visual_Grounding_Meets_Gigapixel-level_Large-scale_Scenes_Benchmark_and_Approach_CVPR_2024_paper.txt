When Visual Grounding Meets Gigapixel-level Large-scale Scenes:
Benchmark and Approach
Tao Ma1*, Bing Bai1*, Haozhe Lin1*, Heyuan Wang2, Yu Wang1, Lin Luo2, Lu Fang1†
1Tsinghua University2Peking University
fanglu@tsinghua.edu.cn
gigapixel-level
(b) GigaGrounding(a) RefCOCOi. lower left white carii. womanin a green top with an umbrellabeneath the middle lawn25479 px14335 px30001400
640 px480 pxmegapixel-level
160380woman on right
110330
Figure 1. Comparison of the proposed GigaGrounding and the RefCOCO benchmark. GigaGrounding presents distinctive challenges,
including large-scale scene understanding, high-resolution with significant scale variation, and multi-hop expressions.
Abstract
Visual grounding refers to the process of associating
natural language expressions with corresponding regions
within an image. Existing benchmarks for visual grounding
primarily operate within small-scale scenes with a few ob-
jects. Nevertheless, recent advances in imaging technology
have enabled the acquisition of gigapixel-level images, pro-
viding high-resolution details in large-scale scenes contain-
ing numerous objects. To bridge this gap between imaging
and computer vision benchmarks and make grounding more
practically valuable, we introduce a novel dataset, named
GigaGrounding, designed to challenge visual grounding
models in gigapixel-level large-scale scenes. We extensively
analyze and compare the dataset with existing benchmarks,
demonstrating that GigaGrounding presents unique chal-
lenges such as large-scale scene understanding, gigapixel-
level resolution, significant variations in object scales, and
the “multi-hop expressions”. Furthermore, we introduced
a simple yet effective grounding approach, which employs
a “glance-to-zoom-in” paradigm and exhibits enhanced
capabilities for addressing the GigaGrounding task. The
dataset is available at www.gigavision.ai .
*These authors contributed equally to this work.
†Lu Fang is the corresponding author ( www.luvision.net ).1. Introduction
Visual grounding (VG), also known as referring expression
comprehension [1, 2], phrase localization [3, 4], and natural
language object retrieval [5, 6], aims to identify the region
within an image that corresponds to a given natural lan-
guage expression. This field offers considerable potential
for enhancing our capacity to connect the human language
and visual elements of the tangible world [7]. The appli-
cation of VG in scenarios such as rescue and robotics can
prove to be exceedingly valuable, for example, in searching
for a missing child in crowds.
While the VG community has made significant progress
in recent years, the benchmarks employed thus far have
been predominantly limited to small-scale scenes [1–4]. As
illustrated in Figure 1a, the number of objects is limited, and
the objects cover the dominant portion of the image. How-
ever, recent advances in imaging technology, such as ar-
ray cameras [8] and even consumer-grade smartphones [9],
have enabled the capture of gigapixel-level photos, pro-
viding high-resolution detail information while covering
large-scale scenes up to km2-level [10], as shown in Fig-
ure 1b. The characteristics of conventional VG benchmarks
have restricted current models’ application to relatively sim-
ple and vanilla settings, hindering the development of new
technologies and limiting the applicability of grounding in
broader contexts [11].
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
22119
To address this limitation, we introduce a novel bench-
mark dataset, named GigaGrounding, for visual ground-
ing in large-scale scenes. The dataset is constructed based
on PANDA [11], a gigapixel-level human-centric video
dataset. An illustration of the GigaGrounding dataset is
displayed in Figure 1b. In contrast to conventional vi-
sual grounding benchmarks such as RefCOCO [2], Giga-
Grounding poses unique and non-trivial challenges:
•Large-scale scene understanding . Large-scale scenes
require models to deal with a much larger spatial and se-
mantic context and a significantly larger number of ob-
jects in the image. Hundreds of objects can be distributed
over a vast area with possible occlusions, clutter, and
other distractions, making it a difficult task to recognize
and understand large-scale scenes.
•High resolution with significant scale variation . Pro-
cessing high-throughput data up to gigapixel-level im-
ages poses new challenges to models in terms of ef-
ficiency [10]. Moreover, objects may be presented at
drastically different scales. For instance, as shown in
Figure 1b, the ground-truth bounding box (B-box) for
Case i spans approximately 3000 ×1400 pixels, whereas,
forCase ii , it spans only about 110 ×330 pixels. This sig-
nificant variation necessitates models to be adaptable in
scaling to accommodate diverse object sizes.
•Multi-hop expressions . In addition to direct expressions ,
which refer to descriptions of independent objects as il-
lustrated in Case i , GigaGrounding also includes a con-
siderable number of multi-hop expressions . These expres-
sions challenge grounding models to locate target objects
by first identifying some other reference objects . For ex-
ample, in Case ii , the model needs first to identify “the
middle lawn” and then zoom in to locate the correct per-
son based on the relationship with the lawn.
We benchmark the performance of cutting-edge VG
models with the GigaGrounding dataset. Empirical find-
ings reveal that while existing VG models have demon-
strated satisfactory performance in small-scale scenes, they
face distinct and intricate challenges when being applied
to GigaGrounding. To address these challenges, we pro-
pose GlaZing (GLAnce-to-Zoom-IN Grounding), a simple
yet effective model inspired by human cognitive processes.
GlaZing employs a “glance-to-zoom-in” cascading strategy
to ground target bounding boxes in high-resolution images.
It begins by taking a global view of the thumbnail image, to
identify the region of interest based on the given expression.
Subsequently, it zooms in on the local region with adap-
tive scaling for the final grounding process. Experiments
demonstrated that GlaZing outperformed existing models
in the GigaGrounding dataset, effectively addressing major
challenges such as high resolution, significant scale varia-
tion, and multi-hop expressions.In conclusion, this paper presents several contributions:
• We highlight the significance of deploying visual ground-
ing models in large-scale scenes, and introduce a novel
benchmark dataset, GigaGrounding, characterized by
unique properties that set it apart from existing datasets.
• We empirically assess current VG models using the pro-
posed GigaGrounding dataset, highlighting the limita-
tions of state-of-the-art models in this context.
• We introduce GlaZing, a tailored model for GigaGround-
ing, which leverages a “glance-to-zoom-in” approach.
Our experimental results demonstrate the efficacy of this
approach in addressing the current challenges, while also
indicating room for further improvements.
2. Related Work
In this section, we review two relevant research domains:
visual grounding and high-resolution deep learning.
2.1. Visual Grounding
Visual grounding is a field focused on predicting the lo-
cation within an image that corresponds to a natural lan-
guage expression. Benchmark datasets for VG have pre-
dominantly centered on small-scale scenes. For exam-
ple, the Flickr30K Entities dataset [4] extends the original
Flickr30K dataset [12] by incorporating annotations that es-
tablish correspondence between short region phrases and
images. ReferItGame [3] comprises 20,000 images col-
lected from the SAIAPR-12 dataset [13]. RefCOCO [2],
RefCOCO+ [2], and RefCOCOg [1] are all built upon the
MS COCO dataset [14]. Notably, the image resolutions in
these datasets do not exceed 640 ×480. In addition, there are
related benchmarks in fields such as remote sensing [15, 16]
and biomedical science [17, 18], with their resolutions also
limited to 1024 ×1024 or lower.
For VG models, existing work can be broadly catego-
rized into two-stage and one-stage methods [7, 19]. Two-
stage methods [20–24] initiate the process by generating
a set of region proposals using object detectors. Subse-
quently, they leverage natural language expressions to rank
these proposals and identify the best matching proposal. In
contrast, one-stage methods [7, 19, 25–29] densely inte-
grate both visual and textual information and directly output
bounding boxes.
2.2. High-resolution Deep Learning
In recent years, significant progress has been made in the
field of imaging, leading to gigapixel-level images with
both a wide FoV and high resolution [8, 30, 31]. Employ-
ing high-resolution images and videos as direct inputs for
deep learning models introduces challenges in both train-
ing and inference stages [11, 32–34]. To address these
22120
Dataset Typical res. # Images # B-boxes # Expressions
Flickr30K Entities [4] 500 ×375 31,783 276k 427k
ReferItGame [3] 480 ×360 19,894 96,654 130,525
RefCOCO [2] 640 ×480 19,994 50,000 142,210
RefCOCO+ [2] 640 ×480 19,992 49,856 141,564
RefCOCOg [1] 640 ×480 25,799 49,822 95,010
GigaGrounding >25k×14k 3,775 61,353 61,353
Table 1. Statistics of existing benchmarks and GigaGrounding.
challenges, several approaches have been explored. Rudi-
mentary strategies like “uniform downsampling (UD)” and
“cutting into patches (CIP)” may be employed, but they
may result in the loss of detailed information and relational
cues, respectively [10]. Alternatively, more sophisticated
techniques, including non-uniform downsampling [35–37],
selective zooming and skipping [38–40], and the use of
lightweight scanner networks [41, 42], have been proposed.
Bakhtiarnia et al. [10] have provided a comprehensive re-
view of these methods, their relevant applications, and
datasets pertaining to high-resolution deep learning.
3. GigaGrounding Benchmark
This section offers a comprehensive delineation of the Gi-
gaGrounding benchmark, including an overview, an eluci-
dation of the data collection strategy, and analyses of its
unique properties.
3.1. Overview of GigaGrounding
Dataset overview GigaGrounding is built upon images
pre-extracted from PANDA-Video dataset by the dataset
providers [11]. A summary of key statistics comparing con-
ventional VG benchmarks with GigaGrounding is provided
in Table 1. The analysis reveals a significant limitation in
conventional VG datasets, where image resolutions are con-
strained to the megapixel level. In contrast, GigaGrounding
ambitiously extends its scope to gigapixel-level images, en-
compassing intricate details and large-scale scenes. Con-
sequently, the total number of images naturally decreases.
In terms of bounding boxes, GigaGrounding offers a quan-
tity comparable to other benchmark datasets. Additionally,
in traditional benchmarks, multiple expressions may corre-
spond to a single bounding box, while in GigaGrounding,
each bounding box is associated with only one expression.
We illustrate representative samples from conventional
VG datasets in Figure 2, and also provide examples of Gi-
gaGrounding annotations in Figure 3. Specifically, Giga-
Grounding includes two types of annotated expressions:
•Direct expressions : These expressions directly describe
each target object without introducing dependencies on
other objects. Position modifiers, such as “in the top left
of the image”, may be used when describing small and
challenging-to-locate objects, but only when necessary.
In total, there are 41,040 direct expressions.•Multi-hop expressions : These expressions require
grounding models to respect additional restrictive con-
straints dependent on their relationship with other refer-
ence objects . In other words, the model needs to locate
objects by first identifying specific reference objects and
then finding the target objects based on constraints de-
fined by relative positions ( e.g., “to the left”) or relation-
ships ( e.g., “watching”, “holding”) with those reference
objects. There are 20,313 multi-hop expressions in to-
tal. It is noteworthy that we did not identify a design
intentionally analogous to multi-hop expressions in tra-
ditional VG benchmarks. Notably, RefCOCO+ was ex-
plicitly designed to “focus on purely appearance-based
description” [2].
For more details regarding the PANDA-Video dataset
and additional examples of GigaGrounding, please refer to
the Supplementary Material.
Privacy GigaGrounding is built based on the publicly
available PANDA [11] dataset. PANDA is collected in
public areas where photography is officially approved and
is published under the Creative Commons Attribution-
NonCommercial-ShareAlike 4.0 License [43]. Giga-
Grounding has been created by further processing the data
in PANDA with additional annotations. To protect indi-
vidual privacy, PANDA has already anonymized the faces
in the images. However, it is necessary to acknowledge
that certain individuals may still be identifiable based on
other less invasive characteristics, such as clothing or body
shape [44].
3.2. Data Collection Strategy
We have devised the following annotation procedure
and quality control strategies to diligently uphold non-
ambiguity and maintain high annotation quality.
Annotation protocol The annotation protocol, as depicted
in Figure 4, comprises the following steps:
1. Given the gigapixel images, Group A of workers was
asked to write natural language expressions and draw
bounding boxes for the corresponding objects.
2. Given the gigapixel images and natural language expres-
sions from Step 1, Group B of workers was then asked to
draw corresponding bounding boxes within 40 seconds
independently.
3. If the Intersection over Union (IoU) rate of bounding
boxes from Steps 1 and 2 was greater than 0.5, the sam-
ple were deemed valid. Then the valid expressions were
manually translated into English with the highest possi-
ble fidelity.
This procedure tries to ensure non-ambiguity, by exclud-
ing data instances with object selection discrepancies be-
tween Steps 1 and 2.
22121
Amanwithpiercedearsiswearingglassesandanorangehat(a) Flickr30K Entities
1.white shirt on right2.white shirt man3.man on right (b) ReferItGame
1.woman on right in white shirt2.woman on right3.right women (c) RefCOCO
1.yellow shirt and black shorts2.guy in yellow dibbling ball3.yellow shirt in focus (d) RefCOCO+
1.a woman wearing red playing tennis2.a woman in a red shirt holding a tennis racquet (e) RefCOCOg
Figure 2. Example data from conventional visual grounding datasets.
bluebillboardinthetoprightcornerofthepicture
greensphericalplantintheflowerbedontherightofthepicture
redandwhitebus
taxiclosesttotheredandwhitebus
maninaredshirtpushingasuitcasenearasubwaystation
womaninayellowtopridingatricycleinthemiddleofthepicture
maninwhitejacketandblackbaginfrontofthemallontheleftofthepicture
maninawhitecoatridingabluebikeatthebottomleftofthepicture
Figure 3. Example annotation results of GigaGrounding. Blue boxes indicate direct expressions and orange ones are multi-hop expressions.
ExpressionB-Boxes #1
Group BGroup A
B-Boxes #2Output avalid sampleDiscardGigapixel ImageIoU> 0.5?YesNo
Figure 4. The overview of our annotation procedure.
Quality control strategies We employ the following
strategies for quality control.
•Terminology standardization . We introduced a stan-
dardized lexicon to encourage uniform terminology usage
among annotators, reducing the prevalence of ambiguous
or domain-specific terms. For example, the use of specific
color names like “gamboge” and the adoption of special-
ized terminology such as “coupe” should be avoided.
•Sampling inspection and repair . We performed a sam-
pling inspection for each annotation batch to assess its
quality. Specifically, for expression and bounding box an-
notations, we established a sampling pass rate threshold
of 96%, whereas, for expression translation, the thresh-
old was set to 99%. The sampling inspection processincluded no fewer than 100 samples per round, and the
batch of data was accepted once it met the pass rate
threshold or underwent repair.
•Box ensemble and de-duplication . In Step 3 of the an-
notation procedure, we obtained the ground truth by aver-
aging the valid bounding boxes annotated by both groups
of annotators. Any expressions that did not result in a
valid bounding box were discarded. Furthermore, as data
annotation is carried out in parallel among many annota-
tion workers, we performed a deduplication procedure to
eliminate objects with overlapping bounding boxes and
identical expressions across multiple images to enhance
the data variance, with an IoU threshold empirically es-
tablished at 0.3.
Despite potential undetected ambiguities, the imple-
mented methodologies are believed to substantially mitigate
their effects, thereby enhancing the dataset’s overall quality.
3.3. Analyses of GigaGrounding
This section analyzes the salient characteristics differenti-
ating GigaGrounding from existing VG benchmarks. Our
analysis encompasses the following viewpoints.
Quantities of objects per image Among conventional
datasets, Flickr30K Entities in Figure 2a requires the model
22122
GigaGrounding Flickr30K Ents.ReferItGameRefCOCO RefCOCO+ RefCOCOg05001000150020002500B-Box Scale in # Pixels(a) Pixel-count distribution of bound-
ing box scales.
GigaGrounding Flickr30K Ents.ReferItGameRefCOCO RefCOCO+ RefCOCOg107
106
105
104
103
102
101
100B-Box Area (%)(b) Relative area of bounding boxes
w.r.t. the images.
GigaGrounding Flickr30K Ents.ReferItGameRefCOCO RefCOCO+ RefCOCOg0510152025Expression Length in # Words(c) Word-count distribution of ex-
pression lengths.
0 5 10 15 20 25 30 350.000.020.040.060.080.100.12direct
multihop(d) Word-count distribution of direct
and multi-hop expressions.
Figure 5. A comparative analysis of GigaGrounding against established benchmarks RefCOCO, RefCOCO+, and RefCOCOg. (a) and
(b) elucidate the distribution patterns of bounding box scale (in terms of the max value of height and width) and relative area with respect
to the image, (c) presents the distribution of expression lengths, and (d) shows the lengths’ distribution of both direct and multi-hop
expressions within the GigaGrounding dataset. GigaGrounding exhibits a notably elevated level of complexity, primarily stemming from
its pronounced scale variation and intricate expression structures.
to identify and locate multiple objects corresponding to
short phrases within a single sentence. However, we ob-
serve that the model may not need to fully understand the
global semantics before grounding the corresponding ob-
ject, particularly when there exists only one object of that
type. On the other side, RefCOCO and RefCOCO+ include
an average of 3.9 same-type objects per image, while Re-
fCOCOg displays a relatively lower average of 1.63 same-
type objects per image [2].
Scales of bounding boxes In Figures 5a and 5b, we respec-
tively illustrate the absolute scale of bounding boxes and
their relative area with respect to the images. It is evident
that GigaGrounding exhibits greater scale variance in com-
parison to other datasets. Furthermore, bounding box scale
in GigaGrounding adheres to a long-tail distribution, while
other datasets generally show a more concentrated distri-
bution. On average, the RefCOCO dataset features bound-
ing boxes with dimensions of 212 ×268 pixels, accounting
for approximately 18.49% of the total image area. In con-
trast, the GigaGrounding dataset exhibits an average bound-
ing box size of 478 ×978 pixels, representing just 0.12% of
the image area. This statistic reveals the inherent challenges
associated with grounding models tasked with large-scale
scenes. We also checked the distribution of direct and multi-
hop bounding boxes and noticed that they share a similar
distribution pattern, while the bounding boxes correspond-
ing to multi-hop expressions are slightly smaller, with an
average resolution of 425 ×938, compared to 503 ×997 of
direct expression bounding boxes.
Lengths of expressions We demonstrate the word count
distribution of expression lengths in Figure 5c. We note that
the average expression length in RefCOCOg is 8.46 words,
while in RefCOCO and RefCOCO+, it is only 3.50 and
3.53, respectively. In contrast, in order to locate objects in
large-scale scenes without ambiguity, GigaGrounding pro-
vides significantly longer expressions, and the average ex-
pression length is 14.78 words. Flickr30K Entities presents
comparatively lengthy expressions, while it is tasked withground objects referred by short region phrases. Figure 5d
further illustrates the length distribution of direct expres-
sions and multi-hop expressions, and the average lengths
are 12.33 and 18.74, respectively. Longer expressions may
accommodate more details about the objects’ location, ap-
pearance, and properties, which can increase the semantic
space that the model needs to consider, leading to chal-
lenges in both textual and imagery comprehension.
Other statistics We notice that GigaGrounding is cer-
tainly not offering the most number of images, bound-
ing boxes, and annotated expressions among the compared
datasets (Table 1). However, we believe that GigaGroud-
ing’s unique characteristics have opened the door for new
modeling paradigms with practically useful and broader
real-life applications.
4. Approach
In this section, we present an initial step towards effec-
tive visual grounding in the complex context of gigapixel-
level large-scale scenes. We introduce a simple yet effective
model, referred to as GlaZing.
4.1. Architecture Overview
In the context of GigaGrounding, human operators com-
monly begin with an initial coarse search across the image
to identify regions of interest. Following this initial search,
they then allocate their attention to the selected regions,
subjecting them to thorough scrutiny. This methodical nar-
rowing of focus significantly increases search efficacy. Our
GlaZing model is conceived by analogizing this human cog-
nitive heuristic.
An overview of GlaZing is presented in Figure 6. Given
an input image, it initiates by inputting the downsampled
thumbnail and the corresponding expression into the Glance
Grounding Module (GGM) for preliminary localization and
grounding. Subsequently, the Adaptive Cropping Mod-
ule(ACM) dynamically extracts the high-resolution patch
22123
Down SamplethewomaninaredplaiddresscarryingrollpaperinlefthandinthemiddleofthepictureGlance GroundingModule(tx, ty, tw, th, conf)(x, y, w, h)Glance Grounding PredictionGround-truth Bounding Box
thewomaninaredplaiddresscarryingrollpaperinlefthandinthemiddleofthepictureAdaptive Cropping Moduletraininginference
Zoomed-in Grounding Module
Figure 6. Overview of the proposed GlaZing model.
of interest. This extracted patch, along with the expres-
sion, is then forwarded to the Zoomed-in Grounding Mod-
ule(ZGM) to generate the final prediction.
4.2. Detailed Implementations
The architectural framework expounded in Section 4.1 al-
lows for a multitude of implementations. Notably, GGM
and ZGM can be seamlessly integrated with a wide array of
conventional grounding models. In this section, we present
our specific implementation, which yielded favorable re-
sults in our experiments.
Glance grounding module We design GGM based on
ReSC [28], which adopts an anchor-based grounding
paradigm that divides the image into multiple grids and de-
termines the grid to which the target box belongs. This
design effectively locates the target-relevant region in the
thumbnail image.
Adaptive cropping module This module is devised for the
adaptive extraction of a region of interest from the original
image, guided by the GGM’s output. More precisely, ACM
initiates this process by first computing the coordinates for
a patch of specific size located at the center of the bounding
box predicted by the GGM. It subsequently proceeds to crop
the minimum rectangular region that encloses both this des-
ignated patch and the bounding box predicted by the GGM.
During model training, this process also encapsulates the
ground-truth bounding box. Finally, the extracted region is
resized to a specified target size. Through the utilization of
ACM, our model can adaptively focus on regions of varying
scales and locations.
Zoomed-in grounding module After the first glance and
zooming-in, the target bounding box should account for
a significant proportion of the input patch of ZGM, so
we employ a coordinate regression-based visual grounding
method, i.e., TransVG [7] to accomplish this task.
Training objectives The overall training objective can be
defined as L=LGGM+LZGM. We follow the training objec-
tives employed in ReSC and TransVG for LGGM andLZGM
respectively. Specifically, LGGM comprises the YOLOv3loss [45] designed to optimize anchor association confi-
dence and prediction offset, and a diversity loss [28] in-
tended to guide sub-queries in each iteration to concentrate
on distinct elements within the query. Concerning LZGM,
given that ZGM directly generates a 4-dimensional vector
representing the relative coordinates of the grounded target
box with respect to the input patch, we employ both the
smooth ℓ1 loss and the generalized IoU loss [46] to mini-
mize the disparity between the predicted and target values.
4.3. Discussion
As an initial stride for GigaGrounding, GlaZing tackles the
challenges posed by high-resolution input and scale varia-
tion through a “glance-to-zoom-in” cascading strategy cou-
pled with adaptive cropping. Empirical findings in Sec-
tion 5.3 attest to its effectiveness. Nevertheless, substan-
tial opportunities for further refinement remain, which we
leave as future work, including: (a) more tailored training
objectives for GGM and ZGM, (b) end-to-end pipelines for
seamless glancing and zooming-in, and (c) the feasibility of
multiple iterations of glance and zoom adjustments.
5. Experiments
This section delineates the evaluation protocols employed,
followed by results’ presentation and analysis.
5.1. Baselines
We conducted an evaluation of several publicly available
baselines. As discussed in Section 2, two-stage methods
rely on object detectors to generate region proposals. Given
that object detection in large-scale scenes remains a chal-
lenge in its own right [40], we conducted brief experiments
involving a representative open-source two-stage method,
namely, MAttNet [47]. On the other hand, one-stage meth-
ods directly provide bounding box predictions without re-
lying on separate object detection. In this regard, GlaZ-
ing is more aligned with one-stage methods. We evaluated
state-of-the-art open-source one-stage methods, including
ReSC [28], TransVG [7],RefTR [48], SeqTR [29], QR-
Net [49], and SimREC [50]. A concise introduction to
these baselines is provided in the Supplementary Material.
In addition to visual grounding models, we also con-
ducted a human evaluation to assess the ability of human
subjects to complete the task when presented with down-
sampled images as used by deep learning models.
5.2. Evaluation Protocol
Dataset split We randomly selected 70% distinct expres-
sions for training, 10% for validation, and 20% for testing.
This process resulted in 42,641 samples for the training set,
6,254 for the validation set, and 12,458 for the testing set.
22124
Metric Following the standard protocol, we assessed the
performance using Precision@0.5, where the prediction is
deemed correct if its IoU with the ground-truth box is larger
than 0.5. We also provide information on inference time
and GFLOPs to demonstrate the computational efficiency
of each model.
Image preprocessing In our empirical analysis, for the
two-stage baseline MAttNet, features were extracted with
8-times downsampled images. We followed Yu et al. [47],
who used ground-truth object bounding boxes to elimi-
nate region proposal bottlenecks from the object detector. *
On the other side, one-stage baseline models were evalu-
ated using two distinct image resolutions: 640 ×640 and
1536×1536. The first resolution was in accordance with es-
tablished benchmarks, while the latter was the feasible up-
per limit for processing by most baseline models given our
computational constraints. At 640 ×640, the mean bound-
ing box dimensions were roughly 12 ×25 pixels. This size
expanded to 30 ×60 pixels at 1536 ×1536 resolution. To en-
sure fair comparison, for GlaZing, we employed a 640 ×640
thumbnail image for the GGM, and the ACM cropped the
region of interest from 1536 ×1536 images. The cropped
region was subsequently resized to 640 ×640 for the ZGM
to identify the referred region.
Implementation details For all baseline methods, we con-
ducted experiments based on their official code. However,
for the object detection component of MAttNet, due to
the lack of maintenance, we substituted it with the corre-
sponding implementation of Faster R-CNN [51] from De-
tectron2 [52]. The substitution did not hurt the performance
on RefCOCO based on our experiments. For one-stage
methods, we tried to optimized their performance within
a certain hyper-parameter tuning budget. For instance, for
models with official weights on RefCOCO provided, we ex-
perimented with whether or not to use these weights for ini-
tialization and reported the better results. At the resolution
of 1536 ×1536, we also tried initializing with weights from
GigaGrounding at 640 ×640, and reported the better out-
comes. We also adjusted some important hyperparameters
including learning rate, etc. As for GlaZing, we trained the
model with similar settings as ReSC. For all experiments,
we set the maximum query length to 30, and the computa-
tions were performed on a server equipped with 8 NVIDIA
RTX 3090 GPUs. Please refer to the Supplementary Mate-
rial for more details.
5.3. Overall Performance
We present the evaluation results of our approach and the
baselines in Table 2. Several key observations and analyses
are as follows:
*Consequently, it is inappropriate to directly compare the results of
one-stage and two-stage methods.•Baseline models’ limited efficacy in GigaGrounding:
None of the baseline models achieved Precision@0.5
scores surpassing 50%. In contrast, human testers con-
sistently achieved scores exceeding 60%, even on im-
ages of size 640 ×640. It is also noteworthy that the per-
formance on GigaGrounding dramatically diverged from
that on other benchmark datasets, such as RefCOCO. For
instance, the classical ReSC model outperformed other
baselines on GigaGrounding, even surpassing models ini-
tialized with pretrained weights.
•Substantial advantages of GlaZing: GlaZing demon-
strated a significant performance advantage over all base-
line models. When compared to the highest-performing
deep learning baseline at a resolution of 1536 ×1536 ( i.e.,
ReSC), GlaZing achieveed an impressive relative im-
provement of 28.6%. This substantial gain can be at-
tributed to its glance-to-zoom-in design.
•Incompatibility of existing two-stage methods: We
noticed that MAttNet exhibited marginal improvements
over random guessing (6.8% vs. 6.2%), despite that it
used the ground-truth object bounding boxes as region
proposals. Our hypothesis is that the bottleneck lies in
the feature extractors. Two-stage methods typically use
frozen feature extractors pretrained on MS COCO or Im-
ageNet [53], which may not be effective enough for large-
scale scene understanding.
Additionally, our study reveals the following insights:
•Influence of resolution on performance: Our findings
highlight the critical role of high-resolution input in en-
hancing the performance of both deep learning models
and human evaluators.
•Performance variation with expression complexity:
Notably, the GlaZing model demonstrated consistent per-
formance across both direct and multi-hop expressions.
In contrast, baseline models exhibited a marked per-
formance decline when handling multi-hop expressions.
This disparity underscores the need for models to adapt
to expressions with differing levels of complexity.
5.4. In-depth Analysis
Effective high-resolution processing of GlaZing. GlaZ-
ing’s adaptive cropping mechanism allows for higher-
resolution image processing without significant storage and
computation overhead. Experiments with 8-times down-
sampled images in ACM and ZGM showed promising re-
sults: 66.5% on the test set, with 66.2% for direct and
67.5% for multi-hop expressions. These findings suggest
that processing at higher resolutions may further enhance
the model’s performance.
Benefits of RefCOCO initialization. We report the re-
sults of TransVG, RefTR, and SeqTR with and without Re-
fCOCO weight initialization in Table 3. The adoption of
22125
Category Method BackboneVal Test Infer
TimeGFLOPsTrainable
Params. overall direct multi-hop overall direct multi-hop
Two-stage MAttNet ResNet-101 11.4% 11.6% 10.7% 6.8% 6.4% 7.6% 268.6ms 1592.6 13.0M
One-stage
(640×640)ReSC DenseNet-53 30.8% 35.5% 21.2% 29.0% 33.3% 20.3% 53.4ms 101.3 158.2M
QRNet Swin-S 16.2% 19.5% 9.4% 13.8% 16.8% 7.9% 54.8ms 79.2 247.1M
SimREC CSPDarkNet-53 23.3% 27.6% 14.4% 20.8% 24.7% 12.7% 16.5ms 48.9 40.2M
TransVG‡ResNet-101 13.1% 15.9% 7.1% 11.6% 13.7% 7.3% 25.9ms 41.4 122.6M
RefTR‡ResNet-50 25.4% 29.0% 18.1% 21.8% 25.0% 15.3% 45.0ms 23.5 123.4M
SeqTR‡Darknet-53 17.6% 20.1% 12.4% 15.9% 18.1% 11.6% 39.8ms 48.9 100.9M
Human – – – – – – 62%§– – – – – – – – – –
One-stage
(1536×1536)ReSC DenseNet-53 52.0% 57.5% 40.8% 49.7% 56.0% 37.0% 96.6ms 566.3 158.2M
QRNet Swin-S 28.8% 33.3% 19.5% 26.5% 31.0% 17.3% 141.6ms 435.0 247.1M
SimREC CSPDarkNet-53 35.4% 41.7% 22.5% 33.3% 39.5% 20.8% 48.4ms 280.9 40.2M
TransVG‡ResNet-101 26.3% 31.2% 16.5% 24.6% 29.1% 15.4% 72.6ms 225.3 122.6M
RefTR‡ResNet-50 42.4% 48.4% 30.0% 41.0% 46.7% 29.5% 53.0ms 119.7 123.4M
SeqTR‡Darknet-53 35.6% 41.4% 23.5% 33.2% 38.3% 22.8% 51.2ms 263.7 100.9M
Human – – – – – – 84%§– – – – – – – – – –
GlaZing (Ours) 65.0% 65.3% 64.3% 63.9% 64.1% 63.7% 110.2ms 172.1 280.8M
‡We used the released weights pretrained on RefCOCO for initialization.§Estimated with 100 randomly selected samples from the testing set.
Table 2. Performance Evaluation on the GigaGrounding Dataset. Both accuracy-related and efficiency-related metrics are reported.
TransVG RefTR SeqTR
w/o pretraining 5.1% 0.0% 5.3%
w/ pretraining 11.6% 21.8% 15.9%
Absolute Gain +6.5% +21.8% +10.6%
Table 3. Performance improvements on 640 ×640 input images
achieved through pretraining. We initiated each model with pub-
licly available weights trained on the RefCOCO dataset.
Expr. length & proportion ReSC TransVG GlaZing
1-10 (16.99%) 65.9% 42.4% 66.8%
11-20 (62.77%) 47.8% 22.6% 63.5%
21+ (20.24%) 39.7% 13.8% 62.6%
Table 4. Performance stratification by expression length.
B-box scale ReSC TransVG GlaZing
s < Q 1 29.4% 4.5% 48.1%
Q1≤s < Q 250.2% 19.3% 63.6%
Q2≤s < Q 358.2% 31.1% 72.8%
s≥Q3 60.7% 43.6% 70.9%
Table 5. Performance stratification by bounding box scale. Here,
Q1,Q2, andQ3denote the first, second, and third quartiles of
bounding box scale, respectively.
pretrained weights from RefCOCO led to performance en-
hancements for three methods, despite the substantial dif-
ferences between RefCOCO and GigaGrounding.
Performance about expression length and B-box scale.
We conducted an in-depth analysis of the performance
variability about expression length and B-box scale at
1536×1536, specifically focusing on the ReSC, TransVG,
and GlaZing. The results are presented in Tables 4 and 5.
Our findings indicate that GlaZing demonstrated a notable
robustness across varying difficulty levels. Conversely,TransVG exhibited a more pronounced performance degra-
dation, particularly in scenarios involving longer expres-
sions and smaller B-box sizes. Complete results of all base-
lines can be found in the Supplementary Material.
Analysis of the glance-to-zoom-in strategy in GlaZing.
Our investigation reveals that GlaZing’s ZGM effectively
rectifies prediction errors observed in the GGM, underscor-
ing the effectiveness of the glance-to-zoom-in approach for
GigaGrounding. Specifically, we identified that approxi-
mately 54% of errors fall under “ target feature mismatch ”,
with ZGM correcting 48% of these. Additionally, another
20% of errors are categorized as “ over-fuzziness due to
downsampling ”, of which ZGM amends about 50%. Due
to space constraints, detailed explanations and analyses are
provided in the Supplementary Material.
6. Conclusion
This paper presents GigaGrounding, a novel and chal-
lenging benchmark for visual grounding in gigapixel-level
large-scale scenes, accompanied by an efficient solution
strategy. The distinctive challenges posed by GigaGround-
ing establish it as a pioneering benchmark in this domain.
Our goal is to stimulate further research in visual ground-
ing, aiming to propel the development of models that are
both more accurate and efficient. These advancements are
particularly crucial for real-world applications such as res-
cue operations.
Acknowledgements This work is supported in part by
Natural Science Foundation of China (NSFC) under con-
tract No. 62125106 and 62088102, in part by Min-
istry of Science and Technology of China under contract
No. 2021ZD0109901, in part by Tsinghua-Zhijiang joint re-
search center, in part by Young Elite Scientists Sponsorship
Program by CAST under contract No. 2022QNRC001.
22126
References
[1] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan L Yuille, and Kevin Murphy. Generation
and comprehension of unambiguous object descriptions. In
CVPR , 2016. 1, 2, 3
[2] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
and Tamara L Berg. Modeling context in referring expres-
sions. In ECCV , 2016. 1, 2, 3, 5
[3] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
Tamara Berg. ReferItGame: Referring to objects in pho-
tographs of natural scenes. In EMNLP , 2014. 1, 2, 3
[4] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-
nik. Flickr30k Entities: Collecting region-to-phrase corre-
spondences for richer image-to-sentence models. In ICCV ,
2015. 1, 2, 3
[5] Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng,
Kate Saenko, and Trevor Darrell. Natural language object
retrieval. In CVPR , 2016. 1
[6] Jianan Li, Yunchao Wei, Xiaodan Liang, Fang Zhao, Jianshu
Li, Tingfa Xu, and Jiashi Feng. Deep attribute-preserving
metric learning for natural language object retrieval. In ACM
MM, 2017. 1
[7] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang
Zhou, and Houqiang Li. TransVG: End-to-end visual
grounding with transformers. In ICCV , 2021. 1, 2, 6
[8] Xiaoyun Yuan, Mengqi Ji, Jiamin Wu, David J Brady, Qiong-
hai Dai, and Lu Fang. A modular hierarchical array camera.
Light: Sci. Appl. , 2021. 1, 2
[9] Byford Sam. Samsung announces 200-megapixel phone
camera sensor. https://www.theverge.com/2021/
9/2/22653558/samsung- isocell- hp1- gn5-
200 - megapixel - camera - sensor - announced ,
2021. 1
[10] Arian Bakhtiarnia, Qi Zhang, and Alexandros Iosifidis. Effi-
cient high-resolution deep learning: A survey. arXiv preprint
arXiv:2207.13050 , 2022. 1, 2, 3
[11] Xueyang Wang, Xiya Zhang, Yinheng Zhu, Yuchen Guo,
Xiaoyun Yuan, Liuyu Xiang, Zerun Wang, Guiguang Ding,
David Brady, Qionghai Dai, and Lu Fang. PANDA: A
gigapixel-level human-centric video dataset. In CVPR , 2020.
1, 2, 3
[12] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-
maier. From image descriptions to visual denotations: New
similarity metrics for semantic inference over event descrip-
tions. TACL , 2014. 2
[13] Hugo Jair Escalante, Carlos A Hern ´andez, Jesus A Gon-
zalez, Aurelio L ´opez-L ´opez, Manuel Montes, Eduardo F
Morales, L Enrique Sucar, Luis Villasenor, and Michael
Grubinger. The segmented and annotated IAPR TC-12
benchmark. CVIU , 2010. 2
[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
ECCV , 2014. 2
[15] Yuxi Sun, Shanshan Feng, Xutao Li, Yunming Ye, JianKang, and Xu Huang. Visual grounding in remote sensing
images. In ACM MM , 2022. 2
[16] Yang Zhan, Zhitong Xiong, and Yuan Yuan. RSVG: Explor-
ing data and models for visual grounding on remote sensing
data. TGRS , 61:1–13, 2023. 2
[17] Benedikt Boecking, Naoto Usuyama, Shruthi Bannur,
Daniel C Castro, Anton Schwaighofer, Stephanie Hyland,
Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier
Alvarez-Valle, et al. Making the most of text semantics to
improve biomedical vision–language processing. In ECCV ,
2022. 2
[18] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mo-
hammadhadi Bagheri, and Ronald M Summers. Chestx-
ray8: Hospital-scale chest X-ray database and benchmarks
on weakly-supervised classification and localization of com-
mon thorax diseases. In CVPR , 2017. 2
[19] Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing
Huang, Dong Yu, and Jiebo Luo. A fast and accurate one-
stage approach to visual grounding. In ICCV , 2019. 2
[20] Richang Hong, Daqing Liu, Xiaoyu Mo, Xiangnan He, and
Hanwang Zhang. Learning to compose and reason with lan-
guage tree structures for visual grounding. TPAMI , 2019. 2
[21] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor
Darrell, and Kate Saenko. Modeling relationships in refer-
ential expressions with compositional modular networks. In
CVPR , 2017.
[22] Sibei Yang, Guanbin Li, and Yizhou Yu. Dynamic graph
attention for referring expression comprehension. In ICCV ,
2019.
[23] Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, and Anton
Van Den Hengel. Parallel attention: A unified framework
for visual object discovery through dialogs and queries. In
CVPR , 2018.
[24] Peng Wang, Qi Wu, Jiewei Cao, Chunhua Shen, Lianli Gao,
and Anton van den Hengel. Neighbourhood watch: Refer-
ring expression comprehension via language-guided graph
attention networks. In CVPR , 2019. 2
[25] Xinpeng Chen, Lin Ma, Jingyuan Chen, Zequn Jie, Wei
Liu, and Jiebo Luo. Real-time referring expression compre-
hension by single-stage grounding network. arXiv preprint
arXiv:1812.03426 , 2018. 2
[26] Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen
Qian, and Bo Li. A real-time cross-modality correlation fil-
tering method for referring expression comprehension. In
CVPR , 2020.
[27] Arka Sadhu, Kan Chen, and Ram Nevatia. Zero-shot ground-
ing of objects from natural language queries. In ICCV , 2019.
[28] Zhengyuan Yang, Tianlang Chen, Liwei Wang, and Jiebo
Luo. Improving one-stage visual grounding by recursive sub-
query construction. In ECCV , 2020. 6
[29] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia
Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xiaoshuai Sun,
and Rongrong Ji. SeqTR: A simple yet universal network for
visual grounding. In ECCV , 2022. 2, 6
[30] Jianing Zhang, Tianyi Zhu, Anke Zhang, Xiaoyun Yuan, Zi-
han Wang, Sebastian Beetschen, Lan Xu, Xing Lin, Qiong-
hai Dai, and Lu Fang. Multiscale-VR: Multiscale gigapixel
22127
3d panoramic videography for virtual reality. In ICCP , 2020.
2
[31] Xiaoyun Yuan, Lu Fang, Qionghai Dai, David J Brady, and
Yebin Liu. Multiscale gigapixel video: A cross resolution
image matching and warping approach. In ICCP , 2017. 2
[32] Jianing Zhang, Jinzhi Zhang, Shi Mao, Mengqi Ji, Guangyu
Wang, Zequn Chen, Tian Zhang, Xiaoyun Yuan, Qionghai
Dai, and Lu Fang. GigaMVS: a benchmark for ultra-large-
scale gigapixel-level 3d reconstruction. TPAMI , 2021. 2
[33] Guangyu Wang, Jinzhi Zhang, Kai Zhang, Ruqi Huang, and
Lu Fang. GiganticNVS: Gigapixel large-scale neural render-
ing with implicit meta-deformed manifold. TPAMI , 2023.
[34] Xueyang Wang, Xuecheng Chen, Puhua Jiang, Haozhe Lin,
Xiaoyun Yuan, Mengqi Ji, Yuchen Guo, Ruqi Huang, and
Lu Fang. The group interaction field for learning and ex-
plaining pedestrian anticipation. Engineering , 2023. 2
[35] Adria Recasens, Petr Kellnhofer, Simon Stent, Wojciech Ma-
tusik, and Antonio Torralba. Learning to zoom: a saliency-
based sampling layer for neural networks. In ECCV , 2018.
3
[36] Dmitrii Marin, Zijian He, Peter Vajda, Priyam Chatterjee,
Sam Tsai, Fei Yang, and Yuri Boykov. Efficient segmenta-
tion: Learning downsampling near semantic boundaries. In
ICCV , 2019.
[37] Chen Jin, Ryutaro Tanno, Thomy Mertzanidou, Eleftheria
Panagiotaki, and Daniel C Alexander. Learning to down-
sample for segmentation of ultra-high resolution images. In
ICLR , 2022. 3
[38] Mingfei Gao, Ruichi Yu, Ang Li, Vlad I Morariu, and
Larry S Davis. Dynamic zoom-in network for fast object
detection in large images. In CVPR , 2018. 3
[39] Burak Uzkent and Stefano Ermon. Learning when and where
to zoom with deep reinforcement learning. In CVPR , 2020.
[40] Kai Chen, Zerun Wang, Xueyang Wang, Dahan Gong, Lon-
glong Yu, Yuchen Guo, and Guiguang Ding. Towards real-
time object detection in gigapixel-level video. Neurocomput-
ing, 2022. 3, 6
[41] Maria Tzelepi and Anastasios Tefas. Improving the per-
formance of lightweight CNNs for binary classification us-
ing quadratic mutual information regularization. Pattern
Recogn. , 2020. 3
[42] Huangjing Lin, Hao Chen, Simon Graham, Qi Dou, Nasir
Rajpoot, and Pheng-Ann Heng. Fast ScanNet: Fast and
dense analysis of multi-gigapixel whole-slide images for
cancer metastasis detection. TMI, 2019. 3
[43] Using Creative Commons Public Licenses. Attribution-
noncommercial-sharealike 4.0 international. 3
[44] Baowei Jiang, Bing Bai, Haozhe Lin, Yu Wang, Yuchen Guo,
and Lu Fang. DartBlur: Privacy preservation with detection
artifact suppression. In CVPR , 2023. 3
[45] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental
improvement. arXiv preprint arXiv:1804.02767 , 2018. 6
[46] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir
Sadeghian, Ian Reid, and Silvio Savarese. Generalized in-
tersection over union: A metric and a loss for bounding box
regression. In CVPR , 2019. 6[47] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu,
Mohit Bansal, and Tamara L Berg. MAttNet: Modular at-
tention network for referring expression comprehension. In
CVPR , 2018. 6, 7
[48] Muchen Li and Leonid Sigal. Referring transformer: A one-
step approach to multi-task visual grounding. In NeurIPS ,
2021. 6
[49] Jiabo Ye, Junfeng Tian, Ming Yan, Xiaoshan Yang, Xuwu
Wang, Ji Zhang, Liang He, and Xin Lin. Shifting more
attention to visual backbone: Query-modulated refinement
networks for end-to-end visual grounding. In CVPR , 2022.
6
[50] Gen Luo, Yiyi Zhou, Jiamu Sun, Shubin Huang, Xiaoshuai
Sun, Qixiang Ye, Yongjian Wu, and Rongrong Ji. What
goes beyond multi-modal fusion in one-stage referring ex-
pression comprehension: An empirical study. arXiv preprint
arXiv:2204.07913 , 2022. 6
[51] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. In NeurIPS , 2015. 7
[52] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen
Lo, and Ross Girshick. Detectron2. https://github.
com/facebookresearch/detectron2 , 2019. 7
[53] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
lenge. IJCV , 2015. 7
22128
