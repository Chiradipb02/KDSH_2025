Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box
Vision-Language Models for Selective Visual Question Answering
Zaid Khan Yun Fu
Northeastern University
Abstract
The goal of selective prediction is to allow an a model to
abstain when it may not be able to deliver a reliable predic-
tion, which is important in safety-critical contexts. Existing
approaches to selective prediction typically require access to
the internals of a model, require retraining a model or study
only unimodal models. However, the most powerful models
(e.g. GPT-4) are typically only available as black boxes with
inaccessible internals, are not retrainable by end-users, and
are frequently used for multimodal tasks. We study the possi-
bility of selective prediction for vision-language models in a
realistic, black-box setting. We propose using the principle
of neighborhood consistency to identify unreliable responses
from a black-box vision-language model in question answer-
ing tasks. We hypothesize that given only a visual question
and model response, the consistency of the model’s responses
over the neighborhood of a visual question will indicate re-
liability. It is impossible to directly sample neighbors in
feature space in a black-box setting. Instead, we show that
it is possible to use a smaller proxy model to approximately
sample from the neighborhood. We ﬁnd that neighborhood
consistency can be used to identify model responses to vi-
sual questions that are likely unreliable, even in adversarial
settings or settings that are out-of-distribution to the proxy
model.
1. Introduction
Powerful commercial frontier models are sometimes only
available as black boxes accessible through an API [ 2,25].
When using these models in high-risk scenarios, it is prefer-
able that the model defers to an expert or abstains from
answering rather than deliver an incorrect answer [ 7]. Many
approaches for selective prediction [ 7,30] or improving the
predictive uncertainty of a model exist, such as ensembling
[14], gradient-guided sampling in feature space [ 11], retrain-
ing the model [ 27], or training a auxiliary module using
model predictions [ 21]. Selective prediction has typically
been studied in unimodal settings and/or for tasks with a
closed-world assumption, such as image classiﬁcation, and
Figure 1. Identifying unreliable responses from an API-only black-
box vision-language model (VLM) can be challenging because
conﬁdence scores are not always trustworthy, and more sophis-
ticated methods for selective prediction require a level of access
to the model that is unavailable. We explore the idea of model
consistency to identify unreliable model responses in this realistic
scenario: a reliable response is one that is consistent across ques-
tions that are semantically equivalent but different on the surface.
has only recently been studied for multimodal, open-ended
tasks such as visual question answering [6, 29] (VQA).
In existing deployments, training data is private, model
features and gradients are unavailable, retraining is not pos-
sible, the number of predictions may be limited by the API,
training on model outputs is often prohibited, and queries
are open-ended. In a black-box setting with realistic con-
straints, how do we identify unreliable predictions from a
vision-language model?
An intuitive approach is to consider self-consistency: if
a human subject is given two semantically equivalent ques-
tions, we expect the human subject’s answers to the ques-
tions to be identical. A more formal notion of consistency is
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10854
that given a classiﬁer 5(·)and an point x2R#in feature
space, the classiﬁer’s predictions over an n-neighborhood
ofxshould be consistent with 5(x)for a small enough n
[11]. It is not straightforward to operationalize either of these
notions. How can we scalably obtain “semantically equiv-
alent” visual questions to an input visual question? Since
we can’t access the internal representations of a black-box
model, how can we sample from the neighborhood of an
input visual question?
First, we study selective prediction on VQA across in-
distribution, out-of-distribution, and adversarial inputs using
a large VLM. Next, we describe how rephrasings of a ques-
tion can be viewed as samples from the n-neighborhood of
a visual question pair. We propose training a visual ques-
tion generation model as a probing model to scalably and
cheaply produce rephrasings of visual questions given an-
swers and an image, allowing us to approximately sample
from the neighborhood of a visual question pair. To quantify
uncertainty in the answer to a visual question pair, we feed
the rephrasings of the question to the black-box VLM, and
count the number of rephrasings for which the answer of the
VLM remains the same. Surprisingly, we show that consis-
tency over model-generated “approximate rephrasings” is
effective at identifying unreliable predictions of a black-box
vision-language model, even when the rephrasings are not
semantically equivalent and the probing model is an order
of magnitude smaller than the black-box model.
Our approach is analogous to consistency over samples
taken from the neighborhood of an input sample in feature
space, but this method does not require access to the fea-
tures of the vision-language model. Furthermore, it does
not require a held-out validation set, access to the original
training data, or retraining the vision-language model, mak-
ing it appropriate for black-box uncertainty estimates of a
vision-language model. We conduct a series of experiments
testing the effectiveness of consistency over rephrasings for
assessing predictive uncertainty using the task of selective
visual question answering in a number of settings, including
adversarial visual questions, distribution shift, and out of
distribution detection.
Our contributions are:
•We study the problem of black-box selective prediction
for a large vision-language model, using the setting of
selective visual question answering.
•We show that on in-distribution data, a state-of-the-art
large vision-language model is capable of identifying
when it does not know the answer to a question, but this
ability is severely degraded for out-of-distribution and
adversarial visual questions.
•We propose identifying high-risk inputs for visual ques-
tion answering based on consistency over samples in the
neighborhood of a visual question.
•We show that consistency deﬁnes a different ordering thanmodel conﬁdence / uncertainty over instances in a dataset.
•We conduct a series of experiments validating the proposed
method on in-distribution, out-of-distribution and adver-
sarial visual questions, and show that our approach even
works in the likely setting that the black box model being
probed is substantially larger than the probing model.
We show that consistency over the rephrasings of a ques-
tion is correlated with model accuracy on a question and can
select slices of a test dataset on which a model can achieve
lower risk, reject out of distribution samples, and works well
to separate right from wrong answers, even on adversarial
and out of distribution inputs. Surprisingly, this technique
works even though many rephrasings are not literally valid
rephrasings of a question. Our proposed method is a step
towards reliable usage of vision-language models as an API.
Limitations: Due to resource constraints, we study mod-
els that might now be considered relatively small ( 13B
parameters), and our VQG model is “small” ( <700M).
2. Motivating Experiment
We empirically examine the predictive uncertainty of a large
VLM through the lens of selective visual question answering.
In contrast to the classical VQA setting where a model is
forced to answer, a model is allowed to abstain from an-
swering in selective VQA. For safety and reliability, it is
important to examine both out-of-distribution and adversar-
ial inputs, on which we expect that the VLM will have a high
error rate if forced to answer every out-of-distribution or
adversarial question posed to the model. However, because
the VLM is allowed to abstain, in principle the model can
achieve low risk (low error rate) on a slice of the dataset
corresponding to questions that it knows the answer to. In
a black-box setting, only the raw conﬁdence scores for the
answer candidates are likely to be available, so we use the
conﬁdence of the most likely answer as the uncertainty.
In Fig. 3, we plot the selective visual question answer-
ing performance of BLIP [ 16] ﬁnetuned on VQAv2 using
the conﬁdence scores on the validation sets of adversarial
(AdVQA, Sheng et al. [24]), out-of-distribution (OKVQA,
Marino et al. [20]) and in-distribution (VQAv2) datasets. For
the in-distribution dataset (VQAv2), the model is quickly
able to identify which questions it is likely to know the
answer to, achieving nearly perfect accuracy by rejecting
the most uncertain 40% of the dataset. However, for out-of-
distribution and adversarial datasets, the model has a harder
time – after rejecting 50% of the questions, the model still
has an error rate of ⇡40%. The reason for this is evident in
Fig.2, where we plot the distribution of conﬁdence scores
for incorrect and correct answers for OOD, in-distribution,
and adversarial visual questions. For in-distribution visual
questions, the conﬁdence distribution is bimodal, and incor-
rect and correct answers are clearly separated by conﬁdence.
For OOD visual questions, many correctly answered ques-
10855
Figure 2. For out of distribution (OKVQA) and adversarial visual (AdVQA) questions, conﬁdence scores alone do not work well to separate
right from wrong answers — many correct answers are low conﬁdence for OOD data, and many wrong answers are high conﬁdence for
adversarial data. Note: Displayed conﬁdence scores are raw. See Appendix for discussion on calibration.
in-distribution
out-of-distribution
adversarial
Figure 3. Selective VQA performance of a VLM (BLIP) on three
datasets: adversarial (AdVQA), out-of-distribution (OKVQA), and
in-distribution (VQAv2). On OOD and adversarial questions, the
model has a harder time identifying which questions it should
abstain from.
tions are low conﬁdence and difﬁcult to distinguish from
incorrectly answered questions. A similar situation occurs
for adversarial visual questions, in which many questions
are incorrectly answered with high conﬁdence.
Although the strategy of using model conﬁdence alone
to detect questions the model cannot answer is effective for
in-distribution visual questions, this strategy fails on out-of-
distribution and adversarial visual questions.
3. Method
3.1. Task Deﬁnition and Background
Given an image Eand question @, the task of selective visual
question answering is to decide whether a model 5+&𝐴 (E,@)
should predict an answer 0, or abstain from making a predic-
tion. A typical solution to this problem is to train a selection
function 6(·)that produces an abstention score ?rej2[0,1].
The simplest selection function would be to take the rejection
probability ?rej=1−?(0|@,E)where ?(0|@,E)is the modelconﬁdence that 0is the answer, and then use a threshold g
so that the model abstains when ?rej>gand predicts other-
wise. A more complex approach taken by Whitehead et al.
[29] is to train a parametric selection function 6(zE,z@;\)
where zEand z@are the model’s dense representations of
the question and image respectively. The parameters \are
optimized on a held-out validation set, effectively training a
classiﬁer to predict when 5+&𝐴 will predict incorrectly on
an input visual question E,@.
In the black box setting, access to the dense represen-
tations zE,z@of the image Eand question @is typically
forbidden. Furthermore, even if access to the representation
is allowed, a large number of evaluations of 5+&𝐴 would
be needed to obtain the training data for the selection func-
tion. Existing methods for selective prediction typically
assume and evaluate a ﬁxed set of classes, but for VQA,
the label space can shift for each task (differing sets of
acceptable answers for different types of questions) or be
open-set.
1.The approach should not require access to the black-box
model’s internal representations of E,@.
2.The approach should be model agnostic, as the architec-
ture of the black-box model is unknown.
3.The approach should not require a large number of predic-
tions from black-box model to train a selection function,
because each usage of the black-box model incurs a ﬁ-
nancial cost, which can be substantial if large number of
predictions are needed to train an auxiliary model.
4.Similarly, the approach should not require a held-out
validation set for calibrating predictions, because this
potentially requires a large number of evaluations of the
black-box model.
10856
3.2. Deep Structure and Surface Forms
Within the ﬁeld of linguistics, a popular view ﬁrst espoused
by Chomsky [3]is that every natural language sentence has
both a surface form and a deep structure. Multiple surface
forms can be instances of the same deep structure. Simply
put, multiple sentences that have different words arranged
in different orders can mean the same thing. A rephrasing
of a question corresponds to an alternate surface form, but
the same deep structure. We thus expect that the answer to a
rephrasing of a question should be the same as the original
question. If the answer to a rephrasing is inconsistent with
the answer to an original question, it indicates the model
is sensitive to variations in the surface form of the original
question. This indicates the model’s understanding of the
question is highly dependent on superﬁcial characteristics,
making it a good candidate for abstention — we hypothesize
inconsistency on the rephrasings can be used to better quan-
tify predictive uncertainty and reject questions a model has
not understood.
3.3. Rephrasing Generation as Neighborhood Sam-
pling
The idea behind many methods for representation learning
is that a good representation should map multiple surface
forms close together in feature space. For example, in con-
trastive learning, variations in surface form are generated
by applying augmentations to an input, and the distance be-
tween multiple surface forms is minimized. In general, a
characteristic of deep representation is that surface forms of
an input should be mapped close together in feature space.
Previous work, such as Attribution-Based Conﬁdence [ 11]
and Implicit Semantic Data Augmentation [ 28], exploit this
by perturbing input samples in feature space to explore the
neighborhood of an input. In a black-box setting, we don’t
have access to the features of the model, so there is no direct
way to explore the neighborhood of an input in feature space.
An alternate surface form of the input should be mapped
close to the original input in feature space. Thus, a surface
form variation of an input should be a neighbor of the input
in feature space. Generating a surface form variation of a
natural language sentence corresponds to a rephrasing of the
natural language sentence. Since a rephrasing of a question
is a surface form variation of a question, and surface form
variations of an input should be mapped close to the original
input in feature space, a rephrasing of a question is analo-
gous to a sample from the neighborhood of a question. We
discuss this further in the appendix.
3.4. Cyclic Generation of Rephrasings
A straightforward way to generate a rephrasing of a question
is to invert the visual question answering problem, as is done
in visual question generation. Let ?(+),?(&),?(𝐴)be the
distribution of images, questions, and answers respectively.Visual question generation can be framed as approximating
?(&|𝐴,+), in contrast to visual question answering, which
approximates ?(𝐴|&,+). We want to probe the predictive
uncertainty of a black box visual question answering model
5⌫⌫(·)on an input visual question pair E,@where E⇠?(+)
is an image and @⇠?(&)is a question.. The VQA model
5⌫⌫approximates ?(𝐴|&,+). Let the answer 0assigned the
highest probability by the VQA model 5⌫⌫(·)be taken as the
prospective answer. A VQG model 5+&⌧⇡?(&|𝐴,+)can
then be used to generate a rephrasing of an input question @.
To see how, consider feeding the highest probability answer
0from 5⌫⌫(·)⇡?(𝐴|&,+)into 5+&⌧ (·)⇡?(&|𝐴,+)and
then sampling a sentence @0⇠5+&⌧⇡?(&|𝐴,+)from
the visual question generation model. In the case of an ideal
5+&⌧ (·)and perfectly consistent 5⌫⌫(·),@0should be a
generated question for which ?(0|@0,E)≥?(08|@0,E)8082
𝐴, with equality occurring in the case that 08=0. So, @0is a
question having the same answer as @, which is practically
speaking, a rephrasing. We provide an algorithm listing in
Algorithm 1.
To summarize, we ask the black box model for an answer
to a visual question, then give the predicted answer to a
visual question generation model to produce a question @0
conditioned on the image Eand the answer 0by the black
box model, which corresponds to a question the VQG model
thinks should lead to the predicted answer 0. We assume
the rephrasings generated by 5+&⌧ are good enough, 5⌫⌫
should be consistent on the rephrasings, and inconsistency
indicates a problem with 5⌫⌫. In practice, each @0is not
guaranteed to be a rephrasing (see Fig. 4) due to the proba-
bilistic nature of the sampling process and because the VQG
model is not perfect. The VQG model can be trained by
following any procedure that results in a model approximat-
ing?(0|@,E)that is an autoregressive model capable of text
generation conditional on multimodal image-text input. The
training procedure of the VQG model is an implementation
detail we discuss in Sec. 3.5.
3.5. Implementation Details
We initialize the VQG model 5+&⌧ from a BLIP checkpoint
pretrained on 129m image-text pairs, and train it to max-
imize ?(0|@,E)using a standard language modeling loss.
Speciﬁcally, we use
LVQG=−#’
==1log%\(H=|H<=,0,E) (1)
where H1,.2,... H=are the tokens of a question @and0,E
are the ground-truth answer and image, respectively, from a
vqa triplet (E,@,0). We train for 10 epochs, using an AdamW
[19] optimizer with a weight decay of 0.05 and decay the
learning rate linearly to 0 from 2e-5. We use a batch size of
64 with an image size of 480⇥480, and train the model on
10857
Figure 4. Examples showing the use of model-generated rephrasings to identify errors in model predictions with BLIP as the black box
model 5⌫⌫. In the left panel, we show high-conﬁdence answers that wrong, and identiﬁed by their low consistency across rephrasings. In the
right panel, we show low-conﬁdence answers that are actually correct, identiﬁed by their high-conﬁdence across rephrasings.
the VQAv2 training set [ 9]. To sample questions from the
VQG model, we use nucleus sampling [ 10] with a top- ?of
0.9.
Algorithm 1: Probing Predictive Uncertainty of a
Black-Box Vision-Language Model
Input: E,@,:
Data: 5⌫⌫,5+&⌧
Result: 22Q: the consistency of 5⌫⌫over :
rephrasings of E,@
00 − 5⌫⌫(@,E);
2 −0;
for8 0to:do
@0 −=D2;4DB B0<?;4 (5+&⌧ (E,00));
00 − 5⌫⌫(@0,E);
if00=00then
2 − 2+1;
end
end
return 2÷:
4. Experiments
We conduct a series of experiments probing predictive un-
certainty in a black-box visual question answering setting
over two large vision-language models and three datasets.
The primary task we use to probe predictive uncertainty is
selective visual question answering, which we give a detailed
description of in Sec. 3.5.Futher qualitative examples and
results can be found in the appendix.
4.1. Experimental Setup
Black-box Models The experimental setup requires a black-
box VQA model 5⌫⌫and a rephrasing generator 5+&⌧ . Wedescribe the training of the rephrasing generator 5+&⌧ in
Sec. 3.5. We choose ALBEF [ 15], BLIP [ 16], and BLIP-
2[17] as our black-box models. ALBEF and BLIP have
⇡200m parameters, while the version of BLIP-2 we use
is based on the 11B parameter FLAN-T5 [ 4] model. AL-
BEF has been pretrained on 14m image-text pairs, while
BLIP has been pretrained on over 100m image-text pairs,
and BLIP-2 is aligned on 4M images. We use the ofﬁcial
checkpoints provided by the authors, ﬁnetuned on Visual
Genome [ 13] and VQAv2 [ 9] with 1.4mand440k training
triplets respectively.
Datasets We evaluate in three settings: in-distribution,
out-of-distribution, and adversarial. For the in-distribution
setting, we pairs from the VQAv2 validation set following
the selection of [ 23]. For the out-of-distribution setting, we
use OK-VQA [ 20], a dataset for question answering on natu-
ral images that requires outside knowledge. OK-VQA is an
natural choice for a out-of-distribution selective prediction
task, because many of the questions require external knowl-
edge that a VLM may not have acquired, even through large
scale pretraining. On such questions, a model that knows
what it doesn’t know should abstain due to lack of requisite
knowledge. Finally, we consider adversarial visual questions
in the AdVQA [ 24]. We use the ofﬁcial validation splits pro-
vided by the authors. The OK-VQA, AdVQA, and VQAv2
validation sets contain 5k,10k, and 40k questions respec-
tively.
4.2. Properties of Consistency
In this section we, analyze properties of the consistency. We
are interested in:
1.Is increased consistency on rephrasings correlated with
model accuracy on the original question?
2.What does the conﬁdence distribution look like for differ-
ent levels of consistency?
10858
Figure 5. The distribution of conﬁdence scores of 5⌫⌫at each level of consistency. While higher levels of consistency have a larger
proportion of high conﬁdence answers, they also retain a large number of low conﬁdence answers, showing that consistency deﬁnes a
different ordering over questions than conﬁdence scores alone. BLIP is used as the black-box model 5⌫⌫.
Figure 6. The percentage of each dataset at a given level of con-
sistency. On a well-understood, in-distribution dataset (VQAv2), a
large percentage of the questions are at a high consistency level.
3.What is the distribution of consistency across different
datasets?
In Fig. 7we plot the accuracy of the answers when 5⌫⌫
is BLIP by how consistent each answer was over up to 5
rephrasings of an original question. We ﬁnd that consistency
over rephrasings is correlated with accuracy across all three
datasets, through the correlation is weakest on adversarial
data. Increased consistency on the rephrasings of a question
implies lower risk on the original answer to the original ques-
tion. Next, we examine how the distribution of model conﬁ-
dence varies across consistency levels in Fig. 5. Across all
datasets, slices of a dataset at higher consistency levels also
have a greater proportion of high-conﬁdence answers, but re-
tain a substantial proportion of low conﬁdence answers. This
Figure 7. The accuracy of the answers of a VQA model (BLIP)
plotted as a function of how consistent each answer was over up to
5 rephrasings of an original question.
clearly shows that consistency and conﬁdence are not equiv-
alent, and deﬁne different orderings on a set of questions
and answers. Put another way, low conﬁdence on a question
does not preclude high consistency on a question, and simi-
larly, high conﬁdence on a question does not guarantee the
model will be highly consistent on rephrasings of a question.
Finally, plot the percentage of each dataset at a given level
of consistency in Fig. 6. The in-distribution dataset, VQAv2,
has the highest proportion of questions with 5 agreeing neigh-
bors, with all other consistency levels making up the rest of
the dataset. For the out-of-distribution dataset (OKVQA), a
substantial proportion of questions ( ⇡40% ) have ﬁve agree-
ing neighbors, with the rest of the dataset shared roughly
equally between the other consistency levels. On the adver-
10859
Figure 8. Risk-coverage curves at on slices of test datasets at different levels of consistency. A curve labeled =≥:shows the risk-coverage
tradeoff for a slice of the target dataset where the answers of the model are consistent over at least :rephrasings of an original question. The
=≥0curve is the baseline. Higher consistency levels identify questions on which a model can achieve lower risk across all datasets.
5⌫⌫ BLIP ALBEF
Risk 10.0 15.0 20.0 30.0 40.0 10.0 15.0 20.0 30.0 40.0
Consistency
n≥0 0.11 0.18 0.25 0.4 0.61 0.08 0.14 0.21 0.41 0.68
n≥1 0.13 0.22 0.3 0.47 0.74 0.1 0.18 0.29 0.52 0.83
n≥2 0.14 0.23 0.33 0.51 0.78 0.1 0.21 0.32 0.59 0.89
n≥3 0.16 0.26 0.37 0.56 0.84 0.12 0.23 0.37 0.66 0.97
n≥4 0.18 0.28 0.38 0.59 0.88 0.13 0.26 0.42 0.71 1.0
n≥5 0.19 0.31 0.44 0.65 0.95 0.11 0.33 0.47 0.8 1.0
Table 1. OK-VQA coverage at a speciﬁed risk levels, stratiﬁed by
consistency levels. =≥:indicates that the prediction of the model
was consistent over at least :rephrasings of the question.
5⌫⌫ BLIP ALBEF
Risk 20.0 30.0 40.0 50.0 56.0 20.0 30.0 40.0 50.0 60.0
Consistency
n≥0 0.01 0.09 0.51 0.83 0.98 0.0 0.07 0.24 0.75 1.0
n≥1 0.01 0.11 0.58 0.9 1.0 0.01 0.09 0.29 0.86 1.0
n≥2 0.01 0.1 0.61 0.93 1.0 0.01 0.09 0.3 0.89 1.0
n≥3 0.01 0.1 0.58 0.93 1.0 0.02 0.11 0.3 0.89 1.0
n≥4 0.01 0.08 0.55 0.92 1.0 0.02 0.11 0.3 0.87 1.0
n≥5 0.01 0.04 0.53 0.87 1.0 0.04 0.12 0.27 0.84 1.0
Table 2. AdVQA coverage at a speciﬁed risk levels, stratiﬁed by
consistency levels. =≥:indicates that the prediction of the model
was consistent over at least :rephrasings of the question.
sarial dataset (AdVQA), the distribution is nearly ﬂat, with
equal slices of the dataset at each consistency level. One
conclusion from this is that higher consistency is not neces-
sarily rarer, and is highly dependent on how well a model
understands the data distribution the question is drawn from.
4.3. Selective VQA with Neighborhood Consistency
We turn to the question of whether consistency over rephras-
ings is useful in the setting of selective visual question an-
swering.
1.Can consistency select slices of a test dataset which a
model understands well (achieves lower risk), or alterna-tively, identify questions the model doesn’t understand,
and should reject (high risk)?
2.How well does consistency over rephrasings work to
identify low / high risk questions in out-of-distribution
and adversarial settings?
3.What happens when the question generator is much
smaller than the black-box model?
To analyze how useful consistency is for separating low-risk
from high-risk inputs, we use the task of selective visual
question answering. In Fig. 8we plot risk-coverage curves
for in-distribution, out-of-distribution, and adversarial visual
questions. Each curve shows the risk-coverage tradeoff for
questions at a level of consistency. For example, a curve la-
beled as =≥3shows the risk-coverage tradeoff for questions
on which 3 or more neighbors (rephrasings) were consistent
with the original answer. Hence, the =≥0curve is a baseline
representing the risk-coverage curve for any question, regard-
less of consistency. If greater consistency over rephrasings
is indicative over lower risk (and a higher probability the
model knows the answer), we expect to see that the model
should be able to achieve lower risk on slices of a dataset
that the model is more consistent on. On in-distribution vi-
sual questions (VQAv2), the model achieves lower risk at
equivalent coverage for slices of the dataset that have higher
consistency levels. A similar situation holds for the out-of-
distribution dataset, OKVQA, and the adversarial dataset
AdVQA. In general, the model is able to achieve lower risk
on slices of a dataset on which the consistency of the model
over rephrasings is higher. In Tab. 2and Tab. 1, we show
risk-coverage information in tabular form for AdVQA and
OK-VQA at speciﬁc risk levels. Finally, in Fig. 3, we show
that our approach works even when there is a large size
difference between the black-box model and the question
generator.
10860
Figure 9. Risk-coverage curves when 5+&⌧ (200m parameters) is substantially smaller than 5⌫⌫(11B). Even in this scenario, 5+&⌧ can
reliably identify high-risk questions based on consistency.
5. Related Work
5.1. Selective Prediction
Deep models with a reject option have been studied in the
context of unimodal classiﬁcation and regression [ 7,8,30]
for some time, and more recently for the open-ended task of
question answering [ 12]. Deep models with a reject option in
the context of visual question answering were ﬁrst explored
by Whitehead et al. [29]. They take the approach of training
a selection function using features from the model and a held-
out validation set to make the decision of whether to predict
or abstain. Dancette et al. [6]takes an alternate approach by
training models on different dataset slices. The problem of
eliciting truthful information from a language model [ 18]
is closely related to selective prediction for VQA. In both
settings, the model must avoid providing false information
in response to a question.
5.2. Self-Consistency
Jha et al. [11] introduced the idea of using consistency over
the predictions of a model to quantify the predictive uncer-
tainty of the model. Their Attribution Based Conﬁdence
(ABC) metric is based on using guidance from feature at-
tributions, speciﬁcally Integrated Gradients [ 26] to perturb
samples in feature space, then using consistency over the
perturbed samples to quantify predictive uncertainty. Shah
et al. [23] show that VQA models are not robust to linguistic
variations in a sentence by demonstrating inconsistency of
the answers of multiple VQA models over human-generated
rephrasings of a sentence. Similarly, Selvaraju et al. [22]
show that the answers of VQA models to more complex rea-
soning questions are inconsistent with the answers to simpler
perceptual questions whose answers should entail the answer
to the reasoning question. We connect these ideas to hypoth-
esize that inconsistency on linguistic variations of a visual
question is indicative of more superiﬁcal understanding of
the content of the question, and therefore a higher chance of
being wrong when answering the question.
5.3. Robustness of VQA Models
VQA models have been shown to lack robustness, and
severely prone to overﬁtting on dataset-speciﬁc correlationsrather than learning to answer questions. The VQA-CP [ 1]
task showed that VQA models often use linguistic priors to
answer questions (e.g. the sky is usually blue), rather than
looking at the image. Dancette et al. [5]showed that VQA
models often use simple rules based on co-occurences of ob-
jects with noun phrases to answer questions. The existence of
adversarial visual questions has also been demonstrated by
[24], who used an iterative model-in-the-loop process to al-
low human annotators to attack state-of-the-art While VQA
models are approaching human-level performance on the
VQAv2 benchmark [ 9], their performance on more complex
VQA tasks such as OK-VQA [ 20] lags far behind human
performance.
6. Conclusion
The capital investment required to train large, powerful mod-
els on massive amounts of data means that there is a strong
commercial incentive to keep the weights and features of a
model private while making the model accessible through
an API. Using these models in low-risk situations is not
problematic, but using black-box models in situations where
mistakes can have serious consequences is dangerous. At
the same time, the power of these black-box models makes
using them very appealing.
In this paper, we explore a way to judge the reliabil-
ity of the answer of a black-box visual question answering
model by assessing the consistency of the model’s answer
over rephrasings of the original question, which we gener-
ate dynamically using a VQG model. We show that this is
analogous to the technique of consistency over neighbor-
hood samples, which has been used in white-box settings
for self-training as well as predictive uncertainty. We con-
duct experiments on in-distribution, out-of-distribution, and
adversarial settings, and show that consistency over rephras-
ings is correlated with model accuracy, and predictions of a
model that are highly consistent over rephrasings are more
likely to be correct. Hence, consistency over rephrasings
constitutes an effective ﬁrst step for using a black-box visual
question answering model reliably by identifying queries
that a black-box model may not know the answer to.
10861
References
[1]Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Anirud-
dha Kembhavi. Don’t just assume; look and answer: Over-
coming priors for visual question answering. 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 4971–4980, 2017. 8
[2]Axel Brando, Dami `a Torres, Jose A. Rodr ´ıguez-Serrano, and
Jordi Vitri `a. Building uncertainty models on top of black-box
predictive apis. IEEE Access, 8:121344–121356, 2020. 1
[3]N. Chomsky. The Logical Structure of Linguistic Theory.
Springer, 1975. 4
[4]Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay,
William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Sid-
dhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun
Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu,
Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun
Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob De-
vlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason
Wei. Scaling instruction-ﬁnetuned language models. ArXiv,
abs/2210.11416, 2022. 5
[5]Corentin Dancette, R ´emi Cad `ene, Damien Teney, and
Matthieu Cord. Beyond question-based biases: Assessing
multimodal shortcut learning in visual question answering.
2021 IEEE/CVF International Conference on Computer Vi-
sion (ICCV), pages 1554–1563, 2021. 8
[6]Corentin Dancette, Spencer Whitehead, Rishabh Mahesh-
wary, Ramakrishna Vedantam, Stefan Scherer, Xinlei Chen,
Matthieu Cord, and Marcus Rohrbach. Improving selec-
tive visual question answering by learning from your peers.
2023 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 24049–24059, 2023. 1,8
[7]Yonatan Geifman and Ran El-Yaniv. Selective classiﬁcation
for deep neural networks. In NIPS, 2017. 1,8
[8]Yonatan Geifman and Ran El-Yaniv. Selectivenet: A deep neu-
ral network with an integrated reject option. In International
Conference on Machine Learning, 2019. 8
[9]Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra,
and Devi Parikh. Making the V in VQA matter: Elevating the
role of image understanding in Visual Question Answering.
InConference on Computer Vision and Pattern Recognition
(CVPR), 2017. 5,8
[10] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi.
The curious case of neural text degeneration. ArXiv,
abs/1904.09751, 2019. 5
[11] Susmit Jha, Sunny Raj, Steven Fernandes, Sumit K Jha,
Somesh Jha, Brian Jalaian, Gunjan Verma, and Ananthram
Swami. Attribution-Based Conﬁdence Metric For Deep Neu-
ral Networks. In Advances in Neural Information Processing
Systems. Curran Associates, Inc., 2019. 1,2,4,8
[12] Amita Kamath, Robin Jia, and Percy Liang. Selective question
answering under domain shift. In Annual Meeting of the
Association for Computational Linguistics, 2020. 8
[13] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and
Li Fei-Fei. Visual genome: Connecting language and visionusing crowdsourced dense image annotations. International
Journal of Computer Vision, 123:32–73, 2016. 5
[14] Balaji Lakshminarayanan, Alexander Pritzel, and Charles
Blundell. Simple and scalable predictive uncertainty estima-
tion using deep ensembles. In NIPS, 2016. 1
[15] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Got-
mare, Shaﬁq R. Joty, Caiming Xiong, and Steven C. H. Hoi.
Align before fuse: Vision and language representation learn-
ing with momentum distillation. In Neural Information Pro-
cessing Systems, 2021. 5
[16] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.
Hoi. Blip: Bootstrapping language-image pre-training for
uniﬁed vision-language understanding and generation. In
International Conference on Machine Learning, 2022. 2,5
[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. ArXiv,
abs/2301.12597, 2023. 5
[18] Stephanie C. Lin, Jacob Hilton, and Owain Evans. Truthfulqa:
Measuring how models mimic human falsehoods. In Annual
Meeting of the Association for Computational Linguistics,
2021. 8
[19] Ilya Loshchilov and Frank Hutter. Fixing weight decay regu-
larization in adam. ArXiv, abs/1711.05101, 2017. 4
[20] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
Roozbeh Mottaghi. Ok-vqa: A visual question answering
benchmark requiring external knowledge. 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 3190–3199, 2019. 2,5,8
[21] Hussein Mozannar and David A. Sontag. Consistent esti-
mators for learning to defer to an expert. In International
Conference on Machine Learning, 2020. 1
[22] Ramprasaath R. Selvaraju, Purva Tendulkar, Devi Parikh, Eric
Horvitz, Marco Tulio Ribeiro, Besmira Nushi, and Ece Kamar.
Squinting at vqa models: Introspecting vqa models with sub-
questions. 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pages 10000–10008, 2020.
8
[23] Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh.
Cycle-consistency for robust visual question answering. In
2019 Conference on Computer Vision and Pattern Recogni-
tion (CVPR). IEEE. 5,8
[24] Sasha Sheng, Amanpreet Singh, Vedanuj Goswami, Jose Al-
berto Lopez Magana, Wojciech Galuba, Devi Parikh, and
Douwe Kiela. Human-adversarial visual question answering.
2021. 2,5,8
[25] Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang,
and Xipeng Qiu. Black-box tuning for language-model-as-a-
service. ArXiv, abs/2201.03514, 2022. 1
[26] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic
attribution for deep networks. ArXiv, abs/1703.01365, 2017.
8
[27] Joost van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin
Gal. Uncertainty estimation using a single deep deterministic
neural network. 2020. 1
[28] Yulin Wang, Xuran Pan, Shiji Song, Hong Zhang, Cheng Wu,
and Gao Huang. Implicit semantic data augmentation for
10862
deep networks. In Neural Information Processing Systems,
2019. 4
[29] Spencer Whitehead, Suzanne Petryk, Vedaad Shakib,
Joseph E. Gonzalez, Trevor Darrell, Anna Rohrbach, and
Marcus Rohrbach. Reliable visual question answering: Ab-
stain rather than answer incorrectly. In European Conference
on Computer Vision, 2022. 1,3,8
[30] Liu Ziyin, Zhikang T. Wang, Paul Pu Liang, Ruslan Salakhut-
dinov, Louis-Philippe Morency, and Masahito Ueda. Deep
gamblers: Learning to abstain with portfolio theory. ArXiv,
abs/1907.00208, 2019. 1,8
10863
