Curriculum Point Prompting for Weakly-Supervised Referring Image
Segmentation
Qiyuan Dai Sibei Yang†
School of Information Science and Technology, ShanghaiTech University
{daiqy2022,yangsb }@shanghaitech.edu.cn
Abstract
Referring image segmentation (RIS) aims to precisely
segment referents in images through corresponding nat-
ural language expressions, yet relying on cost-intensive
mask annotations. Weakly supervised RIS thus learns from
image-text pairs to pixel-level semantics, which is chal-
lenging for segmenting fine-grained masks. A natural ap-
proach to enhancing segmentation precision is to empower
weakly supervised RIS with the image segmentation foun-
dation model SAM. Nevertheless, we observe that simply
integrating SAM yields limited benefits and can even lead
to performance regression due to the inevitable noise is-
sues and challenges in excessive focus on object parts.
In this paper, we present an innovative framework, Point
PrompTing (PPT), incorporated with the proposed multi-
source curriculum learning strategy to address these chal-
lenges. Specifically, the core of PPT is a point generator
that not only harnesses CLIP’s text-image alignment ca-
pability and SAM’s powerful mask generation ability but
also generates negative point prompts to address the noisy
and excessive focus issues inherently and effectively. In ad-
dition, we introduce a curriculum learning strategy with
object-centric images to help PPT gradually learn from
simpler yet precise semantic alignment to more complex
RIS. Experiments demonstrate that our PPT significantly
and consistently outperforms prior weakly supervised tech-
niques on mIoU by 11.34%, 14.14%, and 6.97% across Re-
fCOCO, RefCOCO+, and G-Ref, respectively.
1. Introduction
Referring image segmentation (RIS) entails segmenting the
referent referred to by a natural language expression in
an image [12, 14, 16, 53, 55, 70]. This task delves into
pixel-level semantic understanding that aligns with free-
form texts, as opposed to pixel classification into closed-set
categories in typical semantic segmentation [8, 9, 40, 62].
†Corresponding author
“person in blue ”(a) Attention Noise
(b) Part Mislead
(3) Ours Framework
“small middle elephant”
“man in green ”
Point 
GeneratorPoint 
Generator
(c) (d)Figure 1. Illustrations of the CLIP attention map on RIS image-
text pairs and corresponding mask outputs through SAM. (a) and
(b) show background noise issues and excessive focus on object
parts that mislead SAM, while (c) and (d) demonstrate the results
of our method, which mitigates these issues.
In recent years, RIS has made significant progress, result-
ing in improved accuracy in the fully supervised learning
setting [12, 14, 55, 70] and demonstrating great potential in
real-world applications, including text-based human-robot
interaction [74, 75] and image editing [3, 78]. However, an-
notating high-quality pairs of natural language expressions
and their corresponding referent masks in images is chal-
lenging and time-consuming, limiting the development of
RIS which relies on full annotation.
Therefore, our objective is to study RIS through weakly
supervised learning in line with some very recent works [18,
27, 37, 53], specifically without the need for any instance-
level or pixel-level semantic supervision, thereby mitigat-
ing the annotation limitation. In light of the absence of
pixel-level semantic annotations, the primary focus of these
weakly supervised RIS studies is to transfer the semantic
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
13711
associations from image-text pairs to the pixel level. This is
achieved by visual entity discovery and gathering [18], text-
to-image response mapping [37], or enhancing Grad-CAM
for an improved saliency map [27]. While it is possible to
approximate the regions of referents, the segmentation re-
sults lack precision.
Lately, the Segment Anything Model (SAM) [22] has
demonstrated its proficiency in generating valid image seg-
mentation masks. A naive approach to improving weakly
supervised RIS involves employing SAM to refine the
coarse localization of referents into precise segmentation
masks. Despite relying on annotated masks, SAM’s training
does not include pixel-level semantic supervision. This su-
pervision aligns with our weakly supervised RIS setting and
offers a promising “free lunch” solution as an image seg-
mentation foundation model. However, unexpectedly, this
refinement fails to enhance, or even drop, the performance
of RIS. We observe that the primary reasons for this are:
1) The pixel-level semantic response obtained from image-
text pairs inherently contains noise or activates other objects
or attributes mentioned in the expression, yet SAM is not
robust when using such noisy responses as input prompts
directly. As shown in Figure 1(a), the attention response
for the expression in the image indeed localizes the refer-
ent. Nevertheless, the presence of certain noisy attention,
like that in Figure 1(a), results in SAM predicting a seg-
mentation that encompasses all these responsive regions. 2)
More crucially, attending salient rather than comprehensive
responses in weakly supervised RIS worsens the problem
of segmentation ambiguity in SAM, where input prompts
can correspond to multiple valid masks. This is due to the
combined effect of SAM’s almost edge-oriented, semantic-
unaware nature and the image-level semantic supervision
inherent to weakly supervised RIS. For example, the green
t-shirt in Figure 1(b) naturally elicits a more noticeable re-
sponse for the expression “man in green”. However, using
the response as the mask, box, or points to prompt SAM
leads to segmenting the t-shirt rather than the man.
In this paper, we propose a novel point prompt learn-
ing framework that collaborates with an innovative multi-
source curriculum learning strategy to tackle these chal-
lenges in weakly supervised RIS. Our framework utilizes
frozen CLIP as the text and image encoders, frozen SAM
as the mask decoder, and a trainable, lightweight point
generator to seamlessly bridge the encoders and the de-
coder. Specifically, the point generator initializes learnable
point queries to represent segmentation masks, which inter-
act with image and text features from the encoders, gen-
erating point prompts for SAM to achieve precise mask
segmentation. This approach harnesses the inherent ad-
vantages of CLIP’s pre-trained image-text alignment and
SAM’s segmentation capabilities, effectively simplifying
the RIS problem to learning point queries and selecting thepositive query for the referent. Notably, point queries natu-
rally handle noisy responses by distinguishing them as neg-
atives, as illustrated in Figure 1(d).
Moreover, to address the semantic-unaware ambiguity
issue, we introduce training on object-centric images like
ImageNet [11], encouraging point queries to shift their at-
tention from salient to comprehensive responses. First, Im-
ageNet boasts a large vocabulary with thousands of classes,
and its rich semantics can assist point queries in learning
semantic-aware point prediction. Additionally, ImageNet
predominantly features object-centric images, where the
objects are often centrally positioned, making it easier to
extract comprehensive class responses using CLIP or unsu-
pervised DINO models [5, 43, 45]. Training on these com-
prehensive responses can correct the initial point prompts
to match those shown in Figure 1(c), resulting in a precise
mask of the “person”. To the best of our knowledge, we are
the first to leverage object-centric images to improve scene-
centric RIS.
Ultimately, in essence, RIS remains a scene-centric task.
It entails segmenting referents in complex scenes, consider-
ing not only class semantics as seen in object-centric scenes
but also their location and relationships with other objects.
Therefore, we introduce a novel curriculum learning strat-
egy that progressively transitions from class-based segmen-
tation to complex referring image segmentation, incorpo-
rating factors such as location and relationships. Addition-
ally, beyond the complexity of expressions, object-centric
images in ImageNet and scene-centric images in RIS have
distinct data distributions. We also strive to mitigate the
domain gap when jointly training on these datasets by in-
troducing a multi-source training strategy.
To evaluate the effectiveness of our PPT and learning
strategy, we conduct expressions on common benchmarks
in RIS, including RefCOCO [71], RefCOCO+ [71], and G-
Ref [41]. In summary, our main contributions are:
• We propose a novel, parameter-efficient point prompting
framework (PPT) for weakly-supervised RIS. PPT’s core
component is a trainable, lightweight point generator to
identify the referent and naturally distinguish noise and
misleading context as negative prompts, thereby enabling
effective integration with frozen CLIP and SAM.
• To the best of our knowledge, we are the first to ef-
fectively utilize object-centric images to facilitate scene-
centric RIS to learn precise and comprehensive dense se-
mantic alignment between text and image.
• We propose an innovative multi-source curriculum learn-
ing strategy to facilitate the gradual learning of the point
generator, starting from simpler semantic alignment and
progressing to more complex RIS, which meanwhile mit-
igates the domain mismatch in multi-source training.
• Although straightforward, our PPT consistently outper-
forms state-of-the-art weakly-supervised RIS by a signif-
13712
icant margin of 11.34%, 14.14%, and 6.97% in terms of
mIoU on the three benchmarks, respectively.
2. Related Work
Weakly Supervised RIS : RIS aims to segment the precise
mask of the referent through a comprehensive understand-
ing of both the image and the text describing the referent,
which has achieved significant performance improvements
in the fully supervised setting by employing multi-modal
fusion from concatenation operation [16, 30, 36, 51, 64, 67]
to recent attention-based approaches [19, 33, 55, 58, 65,
66, 68–70]. However, achieving full supervision demands
pixel-level semantic annotations, which entail significant
expenses. In response, recent approaches [18, 27, 37] begin
exploring weakly supervised RIS, leveraging weak supervi-
sory signals like bounding boxes or image-text pairs, aiming
to align pixel-level semantics utilizing coarse-grained su-
pervision. Among them, Kim et al. [18] divides image fea-
tures into several entities and then employs top-down multi-
modal attention to select entities combined into the referring
segment. Instead of aggregating visual entities, TRIS [37]
extracts rough object positions through text supervision as
pseudo-labels to train a segmentation network. In contrast,
Leeet al. [27] focuses on reasoning about word relation-
ships to predict from salient maps of each word. Neverthe-
less, the masks from these methods are often coarse due to
noisy pseudo-labels or lacking dedicated fine segmentation
decoding. To address this, we introduce a novel framework,
PPT, which uses point prompting to handle noise issues,
thus enabling the effective utilization of SAM’s segmenta-
tion capabilities for high-quality masks.
Curriculum Learning for RIS: Curriculum learning pro-
posed by [1], similar to the natural learning process, grad-
ually increases the complexity of training data during the
training to enhance the model’s capability. Its effectiveness
has been validated in computer vision [6, 52, 72, 76] and has
recently been introduced into the vision-language field. For
visual reasoning, CLIP-VG [59] is the first to introduce cur-
riculum learning into visual grounding. It iteratively selects
high-quality data for training from a pseudo-label set and
uses updated weights for the next round of data selection,
achieving a progressive effect. For the RIS task, MCRES
[61] combines words from expressions into different novel
compositions, enabling the model to learn from word-word,
word-phrase, and ultimately phrase-phrase relationships. In
contrast to previous fully supervised works, we introduce
curriculum learning to mitigate the substantial noise present
in weakly supervised learning, extending the model’s abil-
ity from simpler semantic alignment to more complex RIS.
Visual Foundation Models possess substantial knowledge
capacity, making them versatile for a wide range of tasks.
For example, the CLIP model [45] demonstrates significant
performance improvements in open-vocabulary tasks [13,15, 25, 49, 50, 73]. In image segmentation, the SAM [21]
also opens up promising avenues[10, 35]. In fully super-
vised RIS, while [26, 58, 63] generally treat foundation
models merely as weight initialization or auxiliary tools for
pseudo-label extraction, under-utilizing their intrinsic pow-
erful representation capabilities. Among these, CRIS [58]
builds upon CLIP by adding a vision-language decoder to
extend into a segmentation model. Similarly, LISA [26] em-
ploys SAM’s image encoder and LLaV A [38] to train an ad-
ditional mask decoder. Recent, other works like Grounding
DINO [39] and Grounded SAM [21, 39] require extensive
object detection data during training to provide strong prior
knowledge for RIS. However, they contradict the weakly
supervised setting. In contrast to prior work, our PPT fully
harnesses the capabilities of foundation models in both the
encoder and decoder stages and concatenates them through
a learnable point generator, simultaneously possessing ro-
bust image-text understanding and precise mask generation.
3. Method
The framework of our proposed PPT and its correspond-
ing multi-source curriculum learning strategy are shown in
Figure 2. First, we introduce PPT, our point prompt learn-
ing framework for weakly supervised RIS. Its central fo-
cus is on utilizing a trainable, lightweight point genera-
tor to transfer the text-image semantic alignment capabil-
ity from frozen CLIP to SAM, enabling robust and precise
mask decoding for referents (see Section 3.1). Next, we
propose learning from object-centric images to support the
point generator in generating semantic-aware and compre-
hensive point prompts, as opposed to merely salient ones
(see Section 3.2). Finally, we apply curriculum learning to
enable the progressive learning of the point generator, mov-
ing from simpler class-based segmentation to more complex
referring image segmentation, while also mitigating the do-
main mismatch in multi-source training (see Section 3.3).
3.1. Point Prompt Learning Framework
3.1.1 Point Prompting Architecture
The overall PPT architecture is simple and illustrated in Fig-
ure 2. It comprises three primary components: the image
and text encoders to extract features, a transformer-based
point generator to predict a set of point prompts and their
corresponding confidence scores, and a mask decoder that
predicts the segmentation from point prompts.
Image Encoder and Text Encoder. Follow previous works
[37, 54, 58], we employ CLIP as both the image and text en-
coders. In contrast to [37] that fully fine-tunes the encoders
to adapt weakly supervised RIS, we opt to freeze the en-
coders. This preserves the pre-trained image-text alignment
of CLIP and ensures parameter-efficient tuning for the en-
tire framework. Specifically, given a pair of an image Iand
13713
"zebra"
"zebra"
Semantic-alignment Learning RIS Learning Domain Refinement
"rightest zebra"
Points,
Scor e
Point Generator
Linear
"rightest zebra"
Training Set
Augment
Object-Centric
 SemanticObject-Centric
Augmented Referring
RIS Semantic
 RIS Referring
CLIPCLIP
SAM
SAM"a dog beside a zebra"
"a brown dog near the man"
 "man in green"
Figure 2. The overall framework of our PPT and its curriculum learning strategy . The top left corner demonstrates that a straightfor-
ward concatenation of CLIP and SAM exhibits segmentation capability for simple object-centric images but performs poorly on referring
expression data. The bottom left corner showcases our point generator, which determines the approximate location of objects by learning
a well-distributed set of points, combined with our curriculum learning strategy depicted on the right side of the figure, which transitions
from simple object-centric semantics towards the more complex referring domain.
a natural language expression G, we use the encoders to
extract visual features {Vn}N
n=1atNstages and obtain the
contextual feature representation at word level as T.
Point Generator. Inspired by the DETR-based object de-
tection methods [4, 28, 77, 79], which represents objects
using a set of object queries, we use a set of point queries
to represent segmentation masks. These point queries are
initially randomly initialized and then interact with image
and text features to predict corresponding point prompts,
which are subsequently used as input for the mask decoder.
First, the interactions are performed across multiple encod-
ing stages because image features at different stages may
emphasize different levels of semantic information essential
for RIS, ranging from simple shapes to complex semantics.
At each stage, such as the n-th stage, the visual features Vn
and text features Tare initially fused to update their rep-
resentations to V′
nandTnfollowing [63]. Then, the set of
point queries Qnalternately query the updated image fea-
tures V′
nand text features Tnby a classical cross-attention
layer [57] as follows,
Qn+1= CrossAttn(CrossAttn( Qn, V′
n), Tn).(1)
Here, CrossAttn denotes the cross-attention layer, and the
point queries at the first stage are learnable embeddings that
are initialized randomly.
Next, for each point query qk∈QN+1at the final stage,
we regress its point prompt Pk={(pm
k, rm
k)}M
m=1, consist-
ing of Mpoints, and predict its referent confidence score ck
as follows,
pm
k, rm
k= PointHead( qk), ck= ScoreHead( qk),(2)where PointHead andScoreHead are two independent
multilayer perceptions for predicting the points and confi-
dent scores, respectively. Here, pm
kdenotes the normalized
coordinates of the m-th point of the k-th point query, while
rm
ksignifies the score for classifying as a positive point.
Mask Decoder. We utilize SAM as the mask decoder and
freeze it, similar to our approach with CLIP, to retain its seg-
mentation capability. The mask decoder can be prompted
with a set of points, which can be positive or negative, for
SAM to infer the area to be segmented and output the cor-
responding segmentation mask. Specifically, for the k-th
point query, we employ the point prompt Pkto prompt the
mask decoder, leading to the generation of the segmentation
mask skwithin image I.
To summarize, given an input image Iand an expres-
sionG, our PPT framework outputs a fixed-size of K
predictions {yk}K
k=1, where the k-th query’s prediction
yk= (sk, Pk, ck)includes the segmentation mask sk, point
prompt Pk={(pm
k, rm
k)}M
m=1, and confidence score ck,
which is formulated as follows,
{yk}K
k=1=fθ(I, G),where yk= (sk, Pk, ck)(3)
Here, we denote our framework as fθ, where the θrepre-
sents learnable parameters in the point generator.
3.1.2 Objective Function
Inspired by DETR in object detection, we apply the Hun-
garian algorithm [24] to search for the best bipartite match-
ing between the pseudo labels {ˆye}E
e=1and the predictions
13714
{yk}K
k=1to determine the best assignments σ(e), where
σ(e)represents the index of the prediction matched with
the pseudo label ˆye. The extraction for pseudo labels
from image-text pairs will be discussed in Section 3.2 and
3.3. Specifically, for each pair of the pseudo label ˆye=
( ˆse,ˆPe,ˆce)and the prediction yσ(e)= (sσ(e), Pσ(e), cσ(e)),
we define the objective function by considering the segmen-
tation loss related to the point prompt and the mask predic-
tion, as well as the loss for confidence score as the refer-
ent. The loss ℓ(yσ(e); ˆye)for the prediction yσ(e)given the
pseudo label ˆyeis formulated as follows,
ℓseg(yσ(e); ˆye) =ℓmask(sσ(e); ˆse) +ℓpt(Pσ(e);ˆPe),
ℓ(yσ(e); ˆye) =ℓseg(yσ(e); ˆye) +ℓbce(cσ(e); ˆce),(4)
where ℓmask represents the DICE loss [31] and binary cross-
entropy loss. The point prompt loss ℓptis defined as the
combination of L1loss for point regression and the bi-
nary cross-entropy loss for point classification. Addition-
ally,ℓbcedenotes the binary cross-entropy loss for classify-
ing the prediction as the referent or not.
During inference, we select the segmentation mask
sargmax {ce}corresponding to the highest confidence score
as the segmentation prediction for the referent.
3.2. Learning from Object-centric Images
To train our PPT model using image-text pairs, we gen-
erate pseudo-labels for referents from these pairs lever-
aging the pre-trained image-text alignment of CLIP [45].
However, directly extracting pseudo-labels from image-
expression pairs in RIS datasets [41, 71] for training does
not yield satisfactory results. The main reasons for this
are as follows: 1) The pixel-level semantic responses ob-
tained through image-text alignment are inevitably noisy
or incomplete (particularly in salient regions), as shown in
Figure 1(a)-(b). 2) The complexity of referring expressions
adds to the challenge of extracting their responses as ef-
fective pseudo-labels, primarily because distinguishing the
response corresponding to the referent from other responses
becomes arduous. Referring expressions describe rich con-
tent, including class, attributes, locations, and relationships.
The semantic response to such expressions encompasses in-
formation about the referent but also incorporates additional
contextual details, such as the “a brown dog near the man”
in Figure 2.
We propose to address these challenges by training on
the ImageNet [11], as its object-centric characteristics make
it particularly suitable for extracting high-quality pseudo-
labels. The top left of Figure 2 shows that the pseudo
segmentation mask for the object-centric image is pre-
cise but noisy for the RIS image. In detail, we construct
pseudo datasets for weakly supervised RIS at both the class-
semantic level and the more complex relationship level.Free Data for Simpler Semantic-alignment Learning.
We create a simpler semantic alignment dataset, denoted
asDsem, from ImageNet for weakly supervised RIS. The
dataset is designed to mitigate issues related to noise and
partial segmentation by including comprehensive pseudo-
masks corresponding to object classes.
Given an image and its class in ImageNet, we first cre-
ate an image-expression pair (I, G)by randomly applying
transformations, such as flipping, to the image and defin-
ing the expression as “the [class name] in the middle”.
Then, we automatically obtain the corresponding pseudo-
label{ˆye}E
e=1, where ˆye= (ˆse,ˆPe,ˆce), and ˆseandˆPede-
note the pseudo segmentation mask and point prompt, re-
spectively. Note that for semantic-level data, we only ob-
tain the referent’s pseudo label, which means E= 1 for
the dataset Dsem, and score ˆce= 1. For the segmentation
mask ˆse, we employ the CLIP[32] followed by SAM to di-
rectly obtain a high-quality precision mask, benefiting from
the object-centric characteristics of the image. For the point
prompt ˆPe, we randomly and uniformly sample Mpoints
within the bounding box of the mask ˆse. Points inside the
mask are considered positive, while those outside are con-
sidered negative. Building upon this sampling approach, we
encourage the model to produce points that are evenly dis-
tributed within referents and emphasize the negative points
closely outside the boundary of referents, significantly re-
ducing the ambiguity in SAM’s point prompting.
Augmented Data for More Complex RIS Learning.
Training exclusively on semantic-level data could result in
a limited understanding of the more complex relationships
needed for RIS. As ImageNet itself does not inherently con-
tain any information about relationships between instances,
we propose augmenting the semantic-level dataset Dsem
to create a referring-level dataset Dref. In dataset Dref,
we focus solely on spatial relationships between instances,
with the generalization to semantic relationships discussed
in Section 3.3. Specifically, we mainly employ two types of
relation augmentation as follows: 1) In composition-based
augmentation, where multiple images are aggregated into
one by scaling and pasing through Mosaic-like [2] augmen-
tation, resulting in crowded images containing multiple in-
stances, along with text that describes absolute positions for
the target referent. 2) In fusion-based augmentation, we em-
bed one image within another and create a text for the target
instance that encodes their relative relationship.
Through relation augmentation, our dataset Drefnot
only enriches the instance-level semantic data by introduc-
ing more complex image-expression pairs, but it also pro-
vides pseudo-labels for context objects other than the ref-
erent. These contextual pseudo-labels are crucial for learn-
ing to distinguish the referent from other objects mentioned
in the expressions. For a sample (I, G,{ˆye}E
e=1)∈Dref,
its pseudo-labels {ˆye}E
e=1are related with Econtextual ob-
13715
jects, with each ˆye= (ˆse,ˆPe,ˆce). In these pseudo-labels,
the confidence score is set to one for the referent and zero
for other context objects.
3.3. Multi-source Curriculum Learning Strategy
In this section, we present a multi-source curriculum learn-
ing strategy for jointly training our PPT model fθusing both
multi-source (ImageNet-based and RIS-based) and multi-
level (semantic-level and referring-level) datasets. We first
introduce the pseudo-label extraction in the RIS dataset.
Then, we present the progressively learning stages from the
semantic level to the referring level to domain refinement.
At each stage, we utilize different sets of data samples from
ImageNet-based datasets DsemandDref, as well as the RIS
dataset H.
Pseudo Label Generation. As discussed in Section 3.2,
due to the complexity of referring expressions, extracting
pseudo-label for the referent directly from the entire expres-
sion results in poor label quality. Therefore, for an image-
expression pair (I, G), we extract its pseudo-label {ˆye}E
e=1
by collecting all Ecandidate referents. This is achieved by
extracting candidate pseudo-label separately for each noun
phrase in the expression using the same method employed
in extracting pseudo-label in Dsem. Subsequently, based on
the count of candidate referents in one pair, we partition
the RIS dataset Hinto two subsets, denoted as Hsemand
Href. In dataset Hsem, a sample (I, G,{ˆye}E
e=1)∈Hsem,
with each ˆye= (ˆse,ˆPe,ˆce), contains only one candi-
date referent, which implies E= 1 for the Hsemdataset
and the score ˆc1= 1 for this referent. For each sample
(I, G,{ˆye}E
e=1)∈Href, it contains multiple candidate ref-
erents, i.e.,E > 1. At this stage of pseudo-label extrac-
tion, we set the confidence scores ˆce= 0 for all candidate
referents because the referent cannot be distinguished from
the other candidate referents. Notably, despite the pseudo-
labels being generated in datasets HsemandHref, they are
highly noisy and necessitate collaboration with ImageNet-
based datasets DsemandDreffor model training.
Semantic-alignment Learning Stage. At this learning
stage, our goal is to jointly train the model using multi-
source semantic-level datasets, including HsemandDsem,
to predict semantic-aware and comprehensive segmentation
masks for references. The inclusion of the dataset Dsem
helps mitigate issues related to noise and partial segmenta-
tion pseudo-labels in Hsem. The learning objective is for-
mulated as follows,
θsem= argminθE(I,G,{ˆye})∼Hsem∪Dsem[ℓ(fθ(I, G);{ˆye})],
(5)
where the loss ℓ(fθ(I, G);{ˆye})between the model predic-
tionfθ(I, G)and the pseudo-label {ˆye}E
e=1is defined in
Equation 4. And we denote the learned parameters after
optimization at this stage as θsem.RIS Learning Stage. Furthermore, we enable progres-
sive learning, transitioning from simpler semantic-level seg-
mentation to more complex referring image segmentation
with relation modeling. This is achieved through two con-
secutive learning steps: 1) Training exclusively on the
ImageNet-based referring-level dataset Dref, with training
samples (I, G,{ˆye})sampled from Hsem∪Dsem∪Dref,
leads to the learned parameters as θrefD. 2) Updating the
pseudo-labels in the referring-level RIS dataset Hrefand
conducting joint training on all four datasets. Specifically,
we select pseudo-referents from the candidate referents in
Hrefto update it to H′
refusing the model fθwith model
parameter θrefD. Notably, even though the augmented
ImageNet-based dataset Drefonly considers spatial rela-
tionships, we observe that the model learned on it can gen-
eralize to predict referents in Hrefwith semantic relation-
ships, thanks to the generalization ability of frozen CLIP
encoders. The second training step is formulated as follows,
H′
ref←Select( Href;fθ(·;θrefD)),
θref= argminθE(I,G,{ˆye})∼Dall[ℓ(fθ(I, G;θrefD);{ˆye})],
(6)
where Select represents the selection of pseudo-referents
from the candidate referents in Hrefaccording to the model
fθwith model parameter θrefD, theDallis the union of the
datasets Hsem,Dsem,Dref, and H′
ref. And the θrefD in
fθ(I, G;θrefD)represent the model’s parameters are initial-
ized with θrefD. The parameter after optimization at this
stage is denoted as θref.
Domain Refinement Stage. Despite data augmentation,
disparities in data distribution and domain persist between
the ImageNet-based and RIS-based datasets. Furthermore,
the model fθrefhas already acquired point generation capa-
bilities at both the instance and relationship levels through
the use of ImageNet-based data in previous stages. Hence,
we exclusively fine-tune on the RIS datasets at this stage by
continuously adjusting the selected pseudo-referents. The
optimization objective is formulated as follows,
H(i)
ref←Select( H(i−1)
ref;fθ(·;θ(i−1)), H(i)=H(i)
ref∪Hsem,
θ(i)= argminθE(I,G,{ˆye})∼H(i)[ℓ(fθ(I, G;θ(i−1));{ˆye})].
(7)
Here, H(i)andθ(i)represent the dataset and learned model
parameters after the i-th round of adjustments. At the first
round, the referring-level dataset and parameters are H′
ref
andθref, obtained from the RIS learning stage.
4. Experiments
4.1. Datasets and Implementation Details
Datasets. We conduct experiments on three major datasets:
RefCOCO[71], RefCOCO+[71], and G-Ref [41]. All the
images used in these datasets are from subsets of MSCOCO
13716
Method Published on Sup. Extra Image-Text PairsRefCOCO RefCOCO+ G-Ref
val test A test B val test A test B val-G
RMI [36] ICCV ’17 F - 44.33 44.74 44.63 29.91 30.37 29.43 33.11
DMN[42] ECCV ’18 F - 49.78 54.83 45.13 38.88 44.22 32.29 34.52
Huet al. [17] CVPR ’20 F - 60.98 62.99 59.21 48.17 52.32 42.11 47.57
GroupViT [60] CVPR ’22 W CC12M [7],YFCC [56] 12.97 14.98 12.02 13.21 15.08 12.41 16.84
TSEG [53] arXiv ’22 W ImageNet-21K [11] 25.44 - - 22.01 - - 22.05
ALBEF [29] NeurIPS ’21 W CC [48],SBU [44],COCO [34],VG [23] 23.11 22.79 23.42 22.44 22.07 22.51 24.18
Chunk [27] ICCV ’23 W CC,SBU,COCO,VG 31.06 32.30 30.11 31.28 32.11 30.13 32.88
Shatter [18] ICCV ’23 W ImageNet-21K 34.76 34.58 35.01 28.48 28.60 27.98 28.87
TRIS [37] ICCV ’23 W WebImage Text [45] 31.17 32.43 29.56 30.90 30.42 30.80 36.00
TRIS + SAM - - WebImage Text 25.56 26.56 25.71 26.22 25.75 26.62 29.84
Our PPT CVPR’24 W WebImage Text, ImageNet-1K [47] 46.76 45.33 46.28 45.34 45.84 44.77 42.97
Table 1. Comparison with state-of-the-art models in weakly supervised RIS on RefCOCO, RefCOCO+ and G-Ref datasets.
Method Params P@0.3 P@0.5 P@0.7 mIoU
TRIS 142.09M 46.57 18.69 4.9 31.17
Shatter - 55.02 24.99 6.35 34.76
Chunk - 46.12 23.88 9.02 31.06
Ours 20.7M 61.16 50.19 36.22 46.76
Table 2. Comparison of precision metrics. Params denote the
number of trainable parameters.
[34], and they respectively contain 142,209, 141,564, and
104,560 sentences. RefCOCO+ does not include absolute
directional words, while G-Ref contains longer sentences.
Following previous works [55, 70], we divide RefCOCO
and RefCOCO+ into the training set, validation set, testA,
and testB. As for G-Ref, we use the validation split by
Google.
Implementation Details. We employ the CLIP VIT/B-16
as the encoder following [22, 27] and use the smallest ViT-B
SAM for mask decoder. We adopt the Adam[20] optimizer
with a batch size of 64 and a learning rate of 0.001. The
training is conducted for 50 epochs, of which 10, 30, and
10 epochs are for the semantic alignment, RIS learning and
domain refinement stages, respectively. We randomly se-
lect 2,836 images from the Mini Imagenet [46] as our aux-
iliary data. We follow previous approaches to use the mIoU
(Mean Intersection over Union) and precision at the 0.3, 0.5,
and 0.7 thresholds of mIoU as our main evaluation metrics.
4.2. Comparison with the State-of-the-Art Methods
Table 1 compares our PPT with other state-of-the-art ap-
proaches across all dataset partitions. Our PPT consis-
tently outperforms all other weakly supervised RIS meth-
ods, achieving an average mIoU improvement of 11.34%,
14.14%, and 6.97% on RefCOCO, RefCOCO+, and G-Ref,
respectively, over the previous highest performance.
Compared to TRIS [37], which shares a strategy of
first extracting pseudo-labels and then training with us,
we achieve significant improvements in mIoU on the three
datasets, with gains of 15.59%, 14.44%, and 6.97%, respec-
tively. This indicates that our point prompting framework,which employs a progressive curriculum learning strategy
from multi-source data, is more effective in discovering
and localizing targets. Furthermore, for a fair comparison,
we also integrate SAM into open-sourced TRIS. However,
such integration leads to a decline in performance due to
the inevitable noise issues and challenges in excessive fo-
cus on object parts in weakly supervised RIS, which will
be detailed in the Appendix. In contrast, our approach
outperforms these methods by 20.18%, 19.12%, 16.31%,
respectively, reflecting that our point generator effectively
eliminates noise by utilizing negative prompts. Compared
to Shatter [18], which requires finding various parts of
the instance and aggregating them to prediction, similar to
our point-prompting design, we outperform it by 11.34%,
16.96%, and 17.1%, respectively. This demonstrates that
by leveraging object-centric images from ImageNet, our
method can predict precise semantic alignment regions in-
stead of partial ones.
As shown in Table 2, our PPT significantly improves pre-
cision at different IoU thresholds compared to other weakly
supervised methods. Especially at prec@0.5 and prec@0.7,
we improve state-of-the-art methods by 25.2% and 27.2%,
respectively, which reflects the accuracy of our point gen-
erator in locating referents and the robustness to scattering
points. This is achieved by the curriculum learning strategy
and augmented object-centric data, contributing to improv-
ing fine-grained mask segmentation.
4.3. Ablation Study
We conduct ablation experiments to analyze the effective-
ness of our proposed method, as shown in Table 3.
Point Prompting Framework. (1) We evaluate the perfor-
mance of directly using CLIP + SAM for RIS in a baseline
experiment (row 1). We find that it fails to locate objects
in a significant number of images, and even when CLIP
identifies the correct region, interference caused by the res-
olution issue of attention map and background noise sup-
press SAM mask generation performance. (2) Building this
foundation, we incorporate the point prompting framework
(row 2), achieving a 7.73% mIoU boost. Our point genera-
13717
Image GT TRIS Ours
(a)“far left elephant ”
(d)“guy striped shirt” (c)“zebra on right”
(b)“small elephant in middle”
 : positive points :negative points
:other query points
Image GT TRIS OursFigure 3. Visualization of our PPT’s prediction results and comparison to TRIS [37].
Method P@0.3 P@0.5 P@0.7 mIoU
1CLIP + SAM 28.72 24.84 11.67 26.46
2Pt Prompt w/o IMG 43.85 25.45 18.35 34.19
3Semantic Learning 51.69 39.63 28.44 38.91
43+RIS Learning 59.42 48.53 35.62 44.14
54+Domain Refine (Full) 61.16 50.20 36.22 46.76
65 w/o CL 57.93 44.64 34.27 43.28
75 w/o Augmented IMG 56.56 45.19 33.03 42.29
85 w/o Free IMG 47.29 38.51 27.94 39.33
95 w/o IMG 43.78 27.11 18.77 34.47
Table 3. Ablation study on the validation set of RefCOCO. IMG
represents Object-centric Images from the ImageNet dataset.
tor directly utilizes high-dimensional features to regress ob-
ject positions, thereby circumventing the noise introduced
by the low resolution of activation maps. Moreover, it in-
cludes negative point prompts, which effectively suppress
background interference.
Curriculum Learning. (3) We perform first-stage training
for our prompting framework and gain 4.72% improvement
(row 3), showing the effectiveness of semantic-alignment
learning. (4) Next, we continue by augmenting training
with RIS data (row 4), which aims at enhancing the learn-
ing of relation concepts, achieving a 5.23% mIoU gain. The
augmented RIS-stage learning helps acquire rudimentary,
more complex relation understanding capability. And the
curriculum learning strategy filters out noisy data to en-
hance the training samples’ signal-to-noise ratio. (5) Fi-
nally, we introduce the domain refinement stage into our
full model, resulting in a further 2.62% mIoU improvement
(row 5), which is achieved by eliminating the domain mis-
match during the final fine-tuning phase. (6) Row 6 illus-
trates that without curriculum learning, the mIoU drops by
3.48%, demonstrating the effectiveness of curriculum learn-
ing to select the high-quality pseudo-labels progressively.
Learning from Object-centric Images. (7) To assess the
impact of object-centric data, we remove augmented data
only (row 7), resulting in a 4.47% mIoU drop, underscor-
ing the assistance our constructed dataset provides in the
relation understanding for referring scenarios. (8) In row
8, we remove free semantic-alignment data but still keepthe augmented data, which resulted in a 7.46% mIoU drop.
This is due to the detrimental impact of incorrect pseudo-
label cases on training, which our additional data can mit-
igate. In addition, it demonstrates that a more straightfor-
ward semantic alignment is crucial for follow-up referring
segmentation. (9) Row 9 demonstrates the results obtained
when training without any object-centric images, compared
to row 2 and row 5, highlighting that our method achieves
maximum gains when both augmented data and curriculum
learning are utilized, indicating that curriculum learning can
effectively leverage the knowledge from the object-centric
images to scene-centric RIS.
4.4. Visualization
Figure 3 visualizes our segmentation results. Expression
in (a) requires the segmentation of the leftmost elephant
among the three, demonstrating our framework’s ability to
distinguish instances. In (b), the expression calls for the
segmentation of the middle elephant, showcasing our po-
sitional understanding capability. In (c), we visualize the
points output by other queries as well, and observe that they
also locate the regions of interest within their respective in-
stances. In (d), we show that the point query retrieves ob-
ject position based on textual semantics, rather than relying
solely on spatial pronouns.
5. Conclusion
We introduce a novel point prompting framework (PPT) for
weakly supervised referring image segmentation. It lever-
ages a point generator to connect frozen CLIP and SAM
and incorporates the concept of curriculum learning to this
field. The framework utilizes object-centric images to aid
in learning dense semantic alignment and relationships be-
tween text and images in a weakly supervised setting, which
helps naturally mitigate noise issues and excessive focus on
object parts.
Acknowledgment: This work was supported by the Na-
tional Natural Science Foundation of China (No.62206174)
and MoE Key Laboratory of Intelligent Perception and
Human-Machine Collaboration (ShanghaiTech).
13718
References
[1] Yoshua Bengio, J ´erˆome Louradour, Ronan Collobert, and Ja-
son Weston. Curriculum learning. In Proceedings of the 26th
annual international conference on machine learning , pages
41–48, 2009. 3
[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-
Yuan Mark Liao. Yolov4: Optimal speed and accuracy of
object detection. arXiv preprint arXiv:2004.10934 , 2020. 5
[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18392–18402, 2023.
1
[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In European confer-
ence on computer vision , pages 213–229. Springer, 2020. 4
[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision , pages 9650–9660, 2021. 2
[6] Thibault Castells, Philippe Weinzaepfel, and Jerome Revaud.
Superloss: A generic loss for robust curriculum learning. Ad-
vances in Neural Information Processing Systems , 33:4308–
4319, 2020. 3
[7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12m: Pushing web-scale image-text pre-
training to recognize long-tail visual concepts. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 3558–3568, 2021. 7
[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE transactions on pattern
analysis and machine intelligence , 40(4):834–848, 2017. 1
[9] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 1290–1299, 2022. 1
[10] Haixing Dai, Chong Ma, Zhengliang Liu, Yiwei Li, Peng
Shu, Xiaozheng Wei, Lin Zhao, Zihao Wu, Dajiang Zhu, Wei
Liu, et al. Samaug: Point prompt augmentation for segment
anything model. arXiv preprint arXiv:2307.01187 , 2023. 3
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 2, 5, 7
[12] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang.
Vision-language transformer and query generation for refer-
ring segmentation. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 16321–16330,
2021. 1
[13] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao,
and Guoqi Li. Learning to prompt for open-vocabulary ob-
ject detection with vision-language model. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14084–14093, 2022. 3
[14] Guang Feng, Zhiwei Hu, Lihe Zhang, and Huchuan Lu. En-
coder fusion network with co-attention embedding for refer-
ring image segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 15506–15515, 2021. 1
[15] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
Open-vocabulary object detection via vision and language
knowledge distillation. arXiv preprint arXiv:2104.13921 ,
2021. 3
[16] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Seg-
mentation from natural language expressions. In Computer
Vision–ECCV 2016: 14th European Conference, Amster-
dam, The Netherlands, October 11–14, 2016, Proceedings,
Part I 14 , pages 108–124. Springer, 2016. 1, 3
[17] Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, and
Huchuan Lu. Bi-directional relationship inferring net-
work for referring image segmentation. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 4424–4433, 2020. 7
[18] Dongwon Kim, Namyup Kim, Cuiling Lan, and Suha Kwak.
Shatter and gather: Learning referring image segmentation
with text supervision. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 15547–
15557, 2023. 1, 2, 3, 7
[19] Namyup Kim, Dongwon Kim, Cuiling Lan, Wenjun Zeng,
and Suha Kwak. Restr: Convolution-free referring im-
age segmentation using transformers. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 18145–18154, 2022. 3
[20] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 7
[21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and
Ross Girshick. Segment anything. arXiv:2304.02643 , 2023.
3
[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 2, 7
[23] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. International journal of computer vision ,
123:32–73, 2017. 7
[24] Harold W Kuhn. The hungarian method for the assignment
problem. Naval research logistics quarterly , 2(1-2):83–97,
1955. 4
[25] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and
Anelia Angelova. F-vlm: Open-vocabulary object detec-
tion upon frozen vision and language models. arXiv preprint
arXiv:2209.15639 , 2022. 3
[26] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui
Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation
13719
via large language model. arXiv preprint arXiv:2308.00692 ,
2023. 3
[27] Jungbeom Lee, Sungjin Lee, Jinseok Nam, Seunghak Yu,
Jaeyoung Do, and Tara Taghavi. Weakly supervised refer-
ring image segmentation with intra-chunk and inter-chunk
consistency. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 21870–21881, 2023.
1, 2, 3, 7
[28] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni,
and Lei Zhang. Dn-detr: Accelerate detr training by intro-
ducing query denoising. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 13619–13627, 2022. 4
[29] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.
Align before fuse: Vision and language representation learn-
ing with momentum distillation. Advances in neural infor-
mation processing systems , 34:9694–9705, 2021. 7
[30] Ruiyu Li, Kaican Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan
Qi, Xiaoyong Shen, and Jiaya Jia. Referring image seg-
mentation via recurrent refinement networks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 5745–5753, 2018. 3
[31] Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei
Wu, and Jiwei Li. Dice loss for data-imbalanced nlp tasks.
arXiv preprint arXiv:1911.02855 , 2019. 5
[32] Yi Li, Hualiang Wang, Yiqun Duan, and Xiaomeng Li. Clip
surgery for better explainability with enhancement in open-
vocabulary tasks. arXiv preprint arXiv:2304.05653 , 2023.
5
[33] Liang Lin, Pengxiang Yan, Xiaoqian Xu, Sibei Yang, Kun
Zeng, and Guanbin Li. Structured attention network for re-
ferring image segmentation. IEEE Transactions on Multime-
dia, 24:1922–1932, 2021. 3
[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 7
[35] Zheng Lin, Zhao Zhang, Zi-Yue Zhu, Deng-Ping Fan, and
Xia-Lei Liu. Sequential interactive image segmentation.
Computational Visual Media , 9(4):753–765, 2023. 3
[36] Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu,
and Alan Yuille. Recurrent multimodal interaction for refer-
ring image segmentation. In Proceedings of the IEEE inter-
national conference on computer vision , pages 1271–1280,
2017. 3, 7
[37] Fang Liu, Yuhao Liu, Yuqiu Kong, Ke Xu, Lihe Zhang, Bao-
cai Yin, Gerhard Hancke, and Rynson Lau. Referring image
segmentation using text supervision. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 22124–22134, 2023. 1, 2, 3, 7, 8
[38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 3
[39] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, JunZhu, et al. Grounding dino: Marrying dino with grounded
pre-training for open-set object detection. arXiv preprint
arXiv:2303.05499 , 2023. 3
[40] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition , pages 3431–3440, 2015. 1
[41] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan L Yuille, and Kevin Murphy. Generation
and comprehension of unambiguous object descriptions. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 11–20, 2016. 2, 5, 6
[42] Edgar Margffoy-Tuay, Juan C P ´erez, Emilio Botero, and
Pablo Arbel ´aez. Dynamic multimodal instance segmentation
guided by natural language queries. In Proceedings of the
European Conference on Computer Vision (ECCV) , pages
630–645, 2018. 7
[43] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193 , 2023. 2
[44] Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
Im2text: Describing images using 1 million captioned pho-
tographs. Advances in neural information processing sys-
tems, 24, 2011. 7
[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2, 3, 5, 7
[46] Sachin Ravi and Hugo Larochelle. Optimization as a model
for few-shot learning. In International conference on learn-
ing representations , 2016. 7
[47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115:211–252, 2015. 7
[48] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages
2556–2565, 2018. 7
[49] Cheng Shi and Sibei Yang. Edadet: Open-vocabulary ob-
ject detection using early dense alignment. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 15724–15734, 2023. 3
[50] Cheng Shi and Sibei Yang. The devil is in the object bound-
ary: Towards annotation-free instance segmentation using
foundation models. In The Twelfth International Conference
on Learning Representations , 2024. 3
[51] Hengcan Shi, Hongliang Li, Fanman Meng, and Qingbo Wu.
Key-word-aware network for referring expression image seg-
mentation. In Proceedings of the European Conference on
Computer Vision (ECCV) , pages 38–54, 2018. 3
13720
[52] Yang Shu, Zhangjie Cao, Mingsheng Long, and Jianmin
Wang. Transferable curriculum for weakly-supervised do-
main adaptation. In Proceedings of the AAAI conference on
artificial intelligence , pages 4951–4958, 2019. 3
[53] Robin Strudel, Ivan Laptev, and Cordelia Schmid. Weakly-
supervised segmentation of referring expressions. arXiv
preprint arXiv:2205.04725 , 2022. 1, 7
[54] Sanjay Subramanian, William Merrill, Trevor Darrell, Matt
Gardner, Sameer Singh, and Anna Rohrbach. Reclip: A
strong zero-shot baseline for referring expression compre-
hension. arXiv preprint arXiv:2204.05991 , 2022. 3
[55] Jiajin Tang, Ge Zheng, Cheng Shi, and Sibei Yang. Con-
trastive grouping with transformer for referring image seg-
mentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 23570–
23580, 2023. 1, 3, 7
[56] Bart Thomee, David A Shamma, Gerald Friedland, Ben-
jamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and
Li-Jia Li. Yfcc100m: The new data in multimedia research.
Communications of the ACM , 59(2):64–73, 2016. 7
[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 4
[58] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong
Guo, Mingming Gong, and Tongliang Liu. Cris: Clip-
driven referring image segmentation. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11686–11695, 2022. 3
[59] Linhui Xiao, Xiaoshan Yang, Fang Peng, Ming Yan, Yaowei
Wang, and Changsheng Xu. Clip-vg: Self-paced curriculum
adapting of clip for visual grounding. IEEE Transactions on
Multimedia , 2023. 3
[60] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:
Semantic segmentation emerges from text supervision. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 18134–18144, 2022. 7
[61] Li Xu, Mark He Huang, Xindi Shang, Zehuan Yuan, Ying
Sun, and Jun Liu. Meta compositional referring expression
segmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 19478–
19487, 2023. 3
[62] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xi-
ang Bai. Side adapter network for open-vocabulary semantic
segmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2945–
2954, 2023. 1
[63] Zunnan Xu, Zhihong Chen, Yong Zhang, Yibing Song, Xi-
ang Wan, and Guanbin Li. Bridging vision and language en-
coders: Parameter-efficient tuning for referring image seg-
mentation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 17503–17512, 2023.
3, 4
[64] Sibei Yang, Guanbin Li, and Yizhou Yu. Cross-modal re-
lationship inference for grounding referring expressions. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4145–4154, 2019. 3[65] Sibei Yang, Guanbin Li, and Yizhou Yu. Dynamic graph at-
tention for referring expression comprehension. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 4644–4653, 2019. 3
[66] Sibei Yang, Guanbin Li, and Yizhou Yu. Graph-structured
referring expression reasoning in the wild. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 9952–9961, 2020. 3
[67] Sibei Yang, Guanbin Li, and Yizhou Yu. Propagating over
phrase relations for one-stage visual grounding. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XIX 16 , pages
589–605. Springer, 2020. 3
[68] Sibei Yang, Guanbin Li, and Yizhou Yu. Relationship-
embedded representation learning for grounding referring
expressions. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 43(8):2765–2779, 2020. 3
[69] Sibei Yang, Meng Xia, Guanbin Li, Hong-Yu Zhou, and
Yizhou Yu. Bottom-up shift and reasoning for referring im-
age segmentation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
11266–11275, 2021.
[70] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-
shuang Zhao, and Philip HS Torr. Lavt: Language-aware
vision transformer for referring image segmentation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 18155–18165, 2022. 1, 3, 7
[71] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
and Tamara L Berg. Modeling context in referring expres-
sions. In Computer Vision–ECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11-14,
2016, Proceedings, Part II 14 , pages 69–85. Springer, 2016.
2, 5, 6
[72] Qing Yu, Daiki Ikami, Go Irie, and Kiyoharu Aizawa. Multi-
task curriculum framework for open-set semi-supervised
learning. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part XII 16 , pages 438–454. Springer, 2020. 3
[73] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and
Chen Change Loy. Open-vocabulary detr with conditional
matching. In European Conference on Computer Vision ,
pages 106–122. Springer, 2022. 3
[74] B Zhang and H Soh. Large language models as zero-shot hu-
man models for human-robot interaction. arxiv 2023. arXiv
preprint arXiv:2303.03548 . 1
[75] Bowen Zhang and Harold Soh. Large language models as
zero-shot human models for human-robot interaction. arXiv
preprint arXiv:2303.03548 , 2023. 1
[76] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jin-
dong Wang, Manabu Okumura, and Takahiro Shinozaki.
Flexmatch: Boosting semi-supervised learning with curricu-
lum pseudo labeling. Advances in Neural Information Pro-
cessing Systems , 34:18408–18419, 2021. 3
[77] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun
Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr
with improved denoising anchor boxes for end-to-end object
detection. arXiv preprint arXiv:2203.03605 , 2022. 4
13721
[78] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 1
[79] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang
Wang, and Jifeng Dai. Deformable detr: Deformable trans-
formers for end-to-end object detection. arXiv preprint
arXiv:2010.04159 , 2020. 4
13722
