Driving Everywhere with Large Language Model Policy Adaptation
Boyi Li1Yue Wang1,2Jiageng Mao2Boris Ivanovic1Sushant Veer1Karen Leung1,3Marco Pavone1,4
1NVIDIA2University of Southern California3University of Washington4Stanford University
Abstract
Adapting driving behavior to new environments, cus-
toms, and laws is a long-standing problem in autonomous
driving, precluding the widespread deployment of au-
tonomous vehicles (AVs). In this paper, we present LLaDA,
a simple yet powerful tool that enables human drivers and
autonomous vehicles alike to drive everywhere by adapting
their tasks and motion plans to traffic rules in new loca-
tions. LLaDA achieves this by leveraging the impressive
zero-shot generalizability of large language models (LLMs)
in interpreting the traffic rules in the local driver handbook.
Through an extensive user study, we show that LLaDA’s
instructions are useful in disambiguating in-the-wild unex-
pected situations. We also demonstrate LLaDA’s ability to
adapt AV motion planning policies in real-world datasets;
LLaDA outperforms baseline planning approaches on all
our metrics. Please check our website for more details:
LLaDA.
1. Introduction
Despite the rapid pace of progress in autonomous driving,
autonomous vehicles (A Vs) continue to operate primarily in
geo-fenced areas. A key inhibitor for A Vs to be able to drive
everywhere is the variation in traffic rules and norms across
different geographical regions. Traffic rule differences in
different geographic locations can range from significant
(e.g., left-hand driving in the UK and right-hand driving in
the US) to subtle (e.g., right turn on red is acceptable in San
Francisco but not in New York city). In fact, adapting to
new driving rules and common practice is difficult for hu-
mans and A Vs alike; failure to adapt to local driving norms
can lead to unpredictable and unexpected behaviors which
may result in unsafe situations [15, 35]. Studies have shown
that tourists are more susceptible to accidents [25, 26] that
can sometimes result in injury or death [28]. This calls for
a complete study of policy adaption in current A V systems.
At the same time, LLMs have recently emerged as front-
runners for zero- or few-shot adaptation to out-of-domain
data in various fields, including vision and robotics [6,
16, 17]. Inspired by these works, our goal is to build
Get Driver License from California
AutonomousDrivingDrive Everywhere with Language PolicyFigure 1. LLaDA enables drivers to obtain instructions in any re-
gion all over the world. For instance, the driver gets a driver’s
license in California, USA, our system enables providing prompt
instructions when the driver drives in different regions with differ-
ent situations.
aLarge Language Driving Assistant (LLaDA) that can
rapidly adapt to local traffic rules and customs (Figure 1).
Our method consists of three steps: First, we leverage exist-
ing methods to generate an executable policy; second, when
presented with an unexpected situation described in natu-
ral language (either by a human prompt or a VLM such as
GPT-4V [24] or LINGO-1 [32]), we leverage a Traffic Rule
Extractor (TRE) to extract informative traffic rules relevant
to the current scenario from the local traffic code; finally,
we pass the TRE’s output along with the original plan to a
pre-trained LLM (GPT-4V [24] in this paper) to adapt the
plan accordingly. We test our method on the nuScenes [2]
dataset and achieve improvements in motion planning under
novel scenarios. We also provide extensive ablation studies
and visualizations to further analyze our method.
Contributions. Our core contributions are three-fold:
1. We propose LLaDA, a training-free mechanism to assist
human drivers and adapt autonomous driving policies to
new environments by distilling leveraging the zero-shot
generalizability of LLMs.
2. LLaDA can be immediately applied to any autonomous
driving stack to improve their performance in new loca-
tions with different traffic rules.
3. Our method achieves performance improvements over
previous state-of-the-arts, as verified by user studies and
experiments on the nuScenes dataset.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14948
2. Related Works
Traffic Rules in A V Planning. Researchers have explored
the possibility of embedding traffic rules in the form of met-
ric temporal logic (MTL) formulae [19], linear temporal
logic (LTL) formulae [7, 14, 18], and signal temporal logic
(STL) formulae [29, 33]. Expressing the entire traffic law
as logic formulae is not scalable due to the sheer number of
rules and the exceptions that can arise. Furthermore, adapt-
ing to traffic rules in a new region still requires the cumber-
some encoding of new traffic rules in a machine-readable
format. This challenge was highlighted in [20], where the
use of a natural language description of traffic rules was
proposed as a potential solution. There is a dearth of litera-
ture on directly using the traffic rule handbook in its natural
language form for driving adaptation to new locations, and
it is precisely what we aim to achieve in this paper.
LLMs for Robotic Reasoning. Recently, many works
have adopted LLMs to tackle task planning in robotics.
These methods generally leverage LLMs’ zero-shot gener-
alization and reasoning ability to design a feasible plan for
robots to execute. Of note, PaLM-E [4] develops an embod-
ied multi-modal language model to solve a broad range of
tasks including robotic planning, visual question answering,
and captioning; this large model serves as a foundation for
robotic tasks. VLP [5] further enables visual planning for
complex long-horizon tasks by pretraining on internet-scale
videos and images. Code-As-Policies [17] re-purposes a
code-writing LLM to generate robot policy code given nat-
ural language commands; it formulates task planning as
an in-context code generation and function call problem.
ITP [16] further proposes a simple framework to perform
interactive task planning with language models, improv-
ing upon Code-As-Policies. Inspired by these works, our
method also leverages LLMs for autonomous driving. How-
ever, the key difference is that our method focuses on policy
adaptation via LLMs rather than the wholesale replacement
of modules with LLMs.
LLMs for Autonomous Driving. Most autonomous
driving pipelines consist of perception, prediction, plan-
ning, and control, which have been significantly advanced
by machine learning and deep neural networks in recent
years. Despite such tremendous progress, both percep-
tion and planning are generally non-adaptive, preventing
A Vs from generalizing to any in-the-wild domain. Recent
works leverage foundation models to provide autonomous
driving pipelines with common sense reasoning ability.
Wang et al. [31] proposes a method to extract nuanced spa-
tial (pixel/patch-aligned) features from Transformers to en-
able the encapsulation of both spatial and semantic fea-
tures. GPT-Driver [21] finetunes GPT-3.5 to enable mo-
tion planning and provide chain-of-thought reasoning for
autonomous driving. DriveGPT4 [34] further formulates
driving as an end-to-end visual question answering prob-lem. Most recently, MotionLM [27] represents continuous
trajectories as sequences of discrete motion tokens and casts
multi-agent motion prediction as a language modeling task
over this domain. Our work also leverages LLMs for policy
adaption, however, we do not fine-tune or train a new foun-
dation model. Instead, our method capitalizes on GPT-4 to
perform direct in-context reasoning.
In parallel, there has been a plethora of literature on A V
out-of-domain (OoD) generalization and detection [1, 8–
10, 12, 13, 22, 30]. However, the vast majority of such
works focus on low-level tasks (e.g., transferring percep-
tion models to data from different sensor configurations [1],
adapting prediction methods to behaviors from different re-
gions [13], etc.) and less on higher-level semantic general-
ization [6], which our work focuses on.
3. Driving Everywhere with Large Language
Model Policy Adaptation
In this section, we will introduce our method, LLaDA, for
adapting motion plans to traffic rules in new geographical
areas and discuss all its building blocks.
LLaDA receives four inputs, all in the form of natu-
ral language: (i) a nominal execution plan, (ii) the traffic
code of the current location, (iii) a description of the current
scene from the ego’s perspective, and (iv) a description of
any “unexpected” scenario that may be unfolding. LLaDA
ingests these four inputs and outputs a motion plan – also
represented in natural language – that addresses the scenario
by leveraging the local traffic rules. The nominal execution
plan can be generated by a human driver. Similarly, the
scene description and the unexpected scenario description
can be generated by a human or a VLM. The unique traffic
code in the current location is the text of the entire driver
handbook that describes the rules of the road for that loca-
tion. Under normal circumstances, the unexpected situation
input defaults to normal status ; however, if something
out-of-the-ordinary unfolds, such as the ego vehicle getting
honked or flashed at, or if the ego driver notices something
unusual in the environment (e.g., an animal on the road),
the appropriate text description of the scenario can be sup-
plied to LLaDA. To make the role of LLaDA more concrete,
consider an example: An A V is operating in New York
City (NYC) and the nominal motion plan for the vehicle
is toturn right at asignalized intersection
with a red light . The A V was honked at by
cross traffic which is unexpected. LLaDA will take
these inputs along with NYC’s driver manual and adapt the
motion plan to no right turn on a red light
because NYC traffic law prohibits right turns on red [23].
In the remainder of this section, we will discuss the build-
ing blocks of LLaDA, illustrated in Figure 2.
Traffic Rule Extractor. Passing the entire driver hand-
book to the LLM is superfluous as we only need the traffic
14949
EnvironmentPlanNew PlanUnique traffic code in current locationUnexpected Situation“Turn right”“As per NYC traffic rules, even if you are in the rightmost lane, you should not turn right on a red light; wait until the light turns green to proceed safely.”“someone honks at me”New York City Driving HandbookLLM Planner 
Traffic Rule Extractor (TRE)“Stopped at a red light on the rightmost lane, starting to turn right.”Scene DescriptionFigure 2. Overview of LLaDA. In this illustration, the driver learned how to drive in California but now needs to drive in New York City.
However, the road situation, traffic code, and unexpected situations are different. In our system, we consider three inputs: initial plan
(“Turn right ”), unique traffic code in current location ( New York City Driving Handbook ), and unexpected situation (“ someone honks at
me”). We will feed these three inputs into a Traffic Rule Extractor (TRE), which aims to organize and filter the inputs and feed the output
into the frozen LLMs to obtain the final new plan. In this paper, we set GPT-4 as our default LLM.
Plan
Unique traffic code in current locationUnexpected Situation“Go straight”“Continue driving safely, ignore the honk unless it's an emergency vehicle or you're causing obstruction.”
“Someone honks at me”
Germany Driving HandbookCollect InformationLLM Planner(GPT-4)
background = "The condition: The driver get the license from {}, but now in {} so we make sure the difference in {} and notify the drivers what they need to do.".format(‘California’, ‘Germany’, ‘Germany’)request = “{}, {}”.format(plan, unexpected situation)response_format = 'Please output the instructions about what should the driver do in one sentence less than 20 words.'prompt = background + '\nThe driver says:' + request + '\n' + response_formatFind Keywordsprompt = "Extract less than 3 common traffic-related phrases from the given sentence, each phrase contains 1 or 2 words: {}".format(prompt)LLMs (GPT-4)
keywordspromptProcess Guidelines with Keywords# Split the content into paragraphsparagraphs = user_guidelines.split("\n\n")# Search for keywords in each paragraphfor paragraph in paragraphs:        for keyword in keywords:                if keyword.lower() in paragraph.lower(): # Case-insensitive searchmatching_paragraphs.append(paragraph)processed_guidelines = ''# Find the paragraphs containing the keywordsfor paragraph in matching_paragraphs:        processed_guidelines +=                           paragraph + '\n' + "_" * 50 + '\n'“Go straight”, “honk”New Plan
Figure 3. Details of Traffic Rule Extractor (TRE). As is shown in the figure, we first organize the information (such as locations, “Turn
right” and “someone honks at me” ) into a prompt. Then we feed the prompt to find the one or two keywords using GPT-4. To guarantee
the search quality, each keyword contains one or two words. Then we find the key paragraphs that contain extracted keywords in the unique
traffic code. In this way, we could filter out the necessary information and only organize the valuable material into GPT-4 to obtain the
final new plan.
rules relevant to the current scenario that the vehicle finds
itself in. In fact, extraneous information of the traffic code
can hurt the LLM Planner’s performance. To achieve this
task-relevant traffic rule extraction we use the Traffic Rule
Extractor (TRE). TRE uses the nominal execution plan and
the description of the unexpected scenario to extract key-words in the traffic code of the current location, which are
further used to extract paragraphs that comprise these key-
words. We use Traffic Rule Extractor (TRE) to identify
the most relevant keywords and paragraph extraction; see
Figure 3 for an illustration of TRE’s operation. We could
observe that TRE is simple yet efficient in extracting key
14950
paragraphs from the unique traffic code, it first generates
a prompt and finds keywords in the organized prompt us-
ing GPT-4. Then we find the keywords in the unique traffic
code in the current location. By organizing the processed
guidelines and prompt, we can obtain a new plan accurately
by using GPT-4 twice. After obtaining the relevant para-
graph, we input the organized information from TRE into
an LLM (GPT-4) to obtain the final new plan, referred to as
the LLM Planner.
4. Applications of LLaDA
LLaDA is a general purpose tool for seamlessly adapting
driving policies to traffic rules in novel locations. We see
two main applications that can benefit from LLaDA:
Traffic Rule Assistance for Tourists. Standalone,
LLaDA can serve as a guide for human drivers in new lo-
cations. We envision an interface wherein a human driver,
when encountered with an unexpected scenario, can query
LLaDA in natural language via a speech-to-text module on
how to resolve it. As described in Section 3, LLaDA can
take this natural language description of the scene, the un-
expected scenario, and the nominal execution plan and pro-
vide a new plan which adheres to the local traffic laws. It
is worth pointing out that in its current form, LLaDA can-
not provide plan corrections unless queried by the human
driver. This limits its usability to scenarios where the hu-
man driver becomes aware that they are in an unexpected
scenario. Extending LLaDA to automatically provide plan
corrections requires the development of an unexpected sce-
nario detector and translator , which is beyond the scope of
this current work and will be explored as part of our future
work. We conducted a survey to garner human feedback
about the usefulness and accuracy of LLaDA in some chal-
lenging traffic rule scenarios – the results are discussed in
Section 5.
A V Motion Plan Adaptation. We can also leverage
LLaDA’s traffic law adaptation ability in an A V planning
stack to automatically adapt A V plans to the rules of a new
geographical location. This can be achieved by interfac-
ing LLaDA with any motion planner capable of generat-
ing high-level semantic descriptions of its motion plan (e.g.,
GPT-driver [21]) and a VLM (e.g., GPT-4V) that can trans-
late the scene and the unexpected scenario into their respec-
tive textual descriptions. LLaDA then adapts the nominal
execution plan and communicates it to a downstream plan-
ner that updates the low-level waypoint trajectory for the
A V . Our approach for using LLaDA to adapt A V motion
plans is summarized in Figure 4. We demonstrate the ben-
efits that LLaDA can deliver to A V planning in our experi-
ments where a nominal planner trained in Singapore is de-
ployed in Boston; more details regarding the experiments
are provided in Section 5.5. Experiments
5.1. Implementation Details
Since LLaDA takes advantage of large pre-trained language
models, our method is training-free and easily be applied
to any existing driving system. LLaDA could be run with
a single CPU. In this paper, we assume the driver obtains
driver’s license from California as the default setting.
5.2. LLaDA Examples
We show a full set of functions of LLaDA in our website.
LLaDA enables the system to provide the most updated in-
structions based on local traffic rules, we show the basic
functions of LLaDA and display how it works when the
drivers are in different places, in diverse unexpected situ-
ations, or with diverse plans under various environments.
We could observe that LLaDA is robust to distinct condi-
tions. We also notice that without the driving handbook,
the model cannot provide accurate information. We assume
this is because GPT-4 may not be able to provide detailed
instructions without the context or complex prompt tuning,
while LLaDA could successfully alleviate this problem and
generate reasonable instructions with emphasis on the spe-
cific local traffic rule and driver’s request.
5.3. Inference on Random Nuscenes/Nuplan Videos
Nuscenes [2] and Nuplan [3] datasets are two of the most
used dataset for autonomous driving. Nuscenes is the first
dataset to carry the full autonomous vehicle sensor suite and
Nuplan is the world’s first closed-loop ML-based planning
benchmark for autonomous driving. However, Nuscenes
only contains 12 simple instructions such as “accelerate”
and Nuplan only has 74 instructions (scenario types) such
as “accelerating at stop sign no crosswalk”, which may not
provide constructive and efficient instructions for drivers in
different locations. LLaDA could successfully address this
problem and can be applied to random videos. We first show
Nuscenes example in Figure 5. We also show Nuplan ex-
amples in Figure 6. It is obvious that LLaDA works for
random videos under diverse scenarios, achieving driving
everywhere with language policy.
5.4. Challenging Situations
To further verify the efficiency of LLaDA, we consider sev-
eral challenging cases and compare the results with and
without our approach. Also, since GPT-4 could translate
different languages at the meanwhile, LLaDA is able to pro-
cess different language inputs and output the corresponding
instructions (See row 5). We display the results in Table 1.
In example 1, in NYC there is no right-turn on red, which
is allowed in San Francisco. In example 2, LLaDA can
point out something relating to Rettungsgasse (move to the
right). This is because in the US the rule is that everyone
14951
EnvironmentGPT-Driver 
LLaDATrajectoryBoston Driving HandbookDriving Plan“Go straight”[(x1, y1), … ,(x6, y6)]GPT-Driver 
“In Boston, drive on the right side and use the left lane for overtaking or turning left.”Re-planned TrajectoryDriving Plan“Go straight”[(x’1, y’1), … ,(x’6, y’6)]Figure 4. Combining LLaDA with GPT-Driver for motion planning on the nuScenes dataset.
PlanOutputs (New Plan)Unexpected Situation
Singapore Driving Handbook
Slow down, signal, and carefully change to the left lane if it's safe and unoccupied.
normal status
Change to the left lane 
Turn left with constant speedGo straight with constant speed
found trash on the road
Stay in your lane, slow down, and safely navigate around the trash without changing lanes or stopping
Continue driving straight at a consistent speed, observing local speed limits and traffic signs.
normal status
a car suddenly appears
Slow down, check mirrors, and prepare to brake if necessary.
Signal in good time, keep to the left in Singapore, and do not swing out to the right while turning left.
normal status
a car suddenly appears
Slow down, signal your intention, and turn left only when it's safe to do so.nuscenesUnique Traffic Code
Figure 5. Inference on a random Nuscenes video in Singapore. Since different from most countries, Singapore has a right-hand traffic
system, which requires distinct behavior in comparison with California’s (left-hand traffic system). For example, when “Turn left with
constant speed” and “normal status”, LLaDA suggest the driver to “keep to the left in Singapore”.
pulls over to the right, but this is not standard in Germany.
In example 3, LLaDA is able to point out that we should
overtake the slow car safely from the right lane, since over-
taking on the left is illegal in London. For example 4, our
system could point out that an unprotected right in England
(left-driving system) requires checking the traffic coming at
you as you will have to cut through upcoming traffic. Both
should mention checking for pedestrians. For example 5,
since the driver is on the Autobahn, where being in the left
lane typically requires driving at very high speeds compared
to American or many other countries’ highway speed lim-
its. On the German Autobahn, the absence of a speed limit
means that drivers instead adopt different ”speed zones” perlane, with the leftmost being the fastest and the rightmost
being the slowest. For example 6, Amish communities ex-
ist in the US and Canada (primarily in the northeast USA),
and they frequently have horse-pulled carriages on roads.
So our system successfully provides the instructions to give
right-of-way to horses.
5.5. Evaluator-based Assessment.
We conducted an evaluator-based assessment to further val-
idate the usefulness of videos generated with LLaDA. Here
we show the corresponding questionnaire, we list the ques-
tions in this Google Form. We provided location, scenario,
unexpected situation, relevant local law as conditions, and
14952
PlanOutputs (New Plan)Unexpected Situation
NevadaDriving Handbook
Ensure to yield to pedestrians and any cross traffic before proceeding through the intersection.
normal status
a car suddenly stopped
The driver should stop, yield to the stopped car, and proceed when it's safe.
Pull up to the stop line and stop at a red light.
normal status
the road is kind of wet
The driver should stop at the red light and maintain a safe distance due to the wet road conditions.
Wait for a green light, then proceed straight, yielding to pedestrians and any traffic already in the intersection.
normal status
Slow down, turn on your headlights, and proceed with caution through the intersection.nuplan
Go straight on intersection
On stopping traffic light
Go straight on traffic light intersectionit suddenly started rainingUnique Traffic CodeFigure 6. Inference on a random Nuplan video in Las Vegas. For this case, we select Nevada driving handbook, the illustration shows that
LLaDA works very well in Nuplan and can provide accurate new plan with various unexpected situations.
Example Original Location Target Location Plan Unexpected Situation Outputs (New Plan)
1 San Francisco NYC Turn right on red normal status Do not turn right on red in NYC unless a sign permitting it is posted.
NYC San Francisco Turn right on red normal status Stop completely, yield for pedestrians and
turn right if there’s no ”No Turn on Red” sign.
2 California Germany Drive straight on the highway an emergency vehicle is approaching from behind Move to the right and allow the emergency vehicle to pass.
3 NYC London Drive straight on the highway the car in front drives very slowly, Overtake the slow car safely from the right lane,
we are in the middle lane of a 3 lane highway as overtaking on the left is illegal in London.
4 California Singapore Unprotected right normal status Yield to all other traffic and pedestrians before making your right turn.
5 California Germany Drive straight on the highway, in the leftmost lane I keep getting honked at by cars behind me Move to the right lane, the leftmost lane in Germany is for overtaking and faster vehicles.
6 California Ontario Driving on a rural two-lane road there’s a horse pulling a carriage The driver should slow down, pass the carriage cautiously, and give plenty of space to the horse.
Table 1. Inference on various challenging examples. We compare the outputs with and without LLaDA, and it is obvious that LLaDA
significantly improves the plan and successfully handles various difficult tasks under diverse scenarios and environments.
LLaDA output as driving assistant instructions. Here is the
relevant local law that indicates what we want to pay atten-
tion to while driving. We asked two questions to 24 partic-
ipants about each of the 8 cases. 54.2%participants have
more than 10 years of driving experience and 20.8%par-
ticipants have 5-10 years driving experience. Also, 75%
participants are from the United States. In our assessment,
we ask two questions: “ Does the instruction follow the rele-
vant local law? ” and “ How useful is the instruction? ”. The
results show that 70.3%participants think the instructions
strictly follow the relevant local law, and 82.8%participants
find the instructions are very or extremely helpful for them.
This highlights that LLaDA brings about a significant en-
hancement in the performance of baseline video diffusion
models in both the alignment and visual quality.5.6. Comparison on Motion Planning
We conduct experiments on the nuScenes dataset to validate
the effectiveness of LLaDA in motion planning. NuScenes
consists of perception and trajectory data collected from
Singapore and Boston, which have different traffic rules
(e.g., driving side difference). Specifically, we first utilize
GPT-Driver [21] to generate an initial driving trajectory for
a particular driving scenario, and then we leverage LLaDA
to generate guidelines for GPT-Driver to re-generate a new
planned driving trajectory. Since LLaDA provides country-
specific guidelines, we fine-tuned the GPT-Driver on the
Singapore subset of the nuScenes dataset and evaluated the
performances of GPT-Driver and LLaDA on the Boston
subset of the nuScenes validation set. We follow [11, 21]
and leverage L2 error (in meters) and collision rate (in per-
centage) as evaluation metrics. The average L2 error is
computed by measuring each waypoint’s distance in the
planned and ground-truth trajectories. It reflects the prox-
14953
Figure 7. Visualization of the motion planning results on the nuScenes Boston subset. Ground truth trajectories are in green . Trajectories
generated by GPT-Driver are in red. Re-planned trajectories by LLaDA are in purple .
imity of a planned trajectory to a human driving trajec-
tory. The collision rate is computed by placing an ego-
vehicle box on each waypoint of the planned trajectory and
then checking for collisions with the ground truth bounding
boxes of other objects. It reflects the safety of a planned tra-
jectory. We follow the common practice in previous works
and evaluate the motion planning result in the 3-second time
horizon. Table 2 shows the motion planning results. With
the guidelines provided by LLaDA, GPT-Driver could adapt
its motion planning capability from Singapore to Boston
and reduce planning errors.
5.7. Ablation Study on Potential Safety Issues
There might be concerns that since LLaDA is based on
LLMs, it might generate prompts that might provide dan-
gerous instructions. To alleviate this concern, we evaluate
LLaDA on three different critical cases with diverse coun-
tries and unexpected situations: 1) when facing a stop sign,
whether LLaDA suggests “stop” or not. 2) When facing the
red light, whether LLaDA suggests “stop”/specific safe no-
tifications or not. 3) When it rains heavily, whether LLaDA
suggests “slow down” or not. 4) When a pedestrian walks
across the street, whether LLaDA suggests to “yield to the
pedestrian” or not. For each case, we evaluate 50 random
examples and report the average score of each case. For
each example, if the answer will cause potentially danger-
ous behavior, we will treat it as an error. We observe that
LLaDA achieves 0%error rate for all 4 cases. In case “stop
sign”, where all the instructions suggest “Wait at the stop
sign until it’s safe”. In case “red light”, where all the in-
structions suggest “come to a complete stop at the red light”
or ”wait until it turns green”. In case “it rains heavily”,
where all the instructions suggest “turn on the headlight”
and ”reduce speed”. In case “A pedestrian walks acrossthe street”, where all the instructions suggest “yield to the
pedestrian”. We didn’t notice any potentially harmful in-
structions that might cause dangerous behavior, which en-
sures user safety by directing them toward appropriate be-
havior.
5.8. Combining with GPT-4V
Our approach can be combined with different systems to
improve the functionality to provide accurate information
for humans. In this section, we study the case by com-
bining LLaDA with GPT-4V . GPT-4V is the vision branch
of GPT-4 which can output corresponding captions to de-
scribe the scenes based on a visual input. We randomly
pick two scenes from Youtube1and ask GPT-4V to pro-
vide additional captions, we add these additional captions
to the user’s request. We show an example in Figure 8,
it could be observed that LLaDA could process the infor-
mation GPT-4V very well and provide accurate instructions
based on given captions.
6. Conclusion, Limitations, and Future Work
Conclusion. In this work, we proposed LLaDA, an LLM-
powered framework that adapts nominal motion plans by
a human driver or an A V to local traffic rules of that re-
gion. The modularity of LLaDA facilitates its use for hu-
man driver assistance as well as A V plan adaptation. To our
knowledge, LLaDA is the first to propose traffic rule-based
adaptation via LLMs. Our results show that human drivers
find LLaDA to be helpful for driving in new locations, and
LLaDA also improves the performance of A V planners in
1Source: https : / / www . youtube . com / watch ? v =
Boh66Pjjiq0 andhttps : / / www . youtube . com / watch ?
v=xn_mSqTrOUo
14954
MethodL2 (m) ↓ Collision (%) ↓
1s 2s 3s Avg. 1s 2s 3s Avg.
GPT-Driver [21] 0.27 0.59 1.04 0.63 0.22 0.45 1.07 0.58
GPT-Driver [21] + LLaDA Re-Planning (Ours) 0.27 0.58 1.02 0.62 0.22 0.41 1.04 0.56
Table 2. Motion planning results. With the guidelines provided by LLaDA, GPT-Driver could adapt its motion planning capability from
Singapore to Boston and reduce planning errors.
PlanOutputs (New Plan)Unique Traffic CodeUnexpected Situation
China Driving Handbook
Continue straight in the left lane, obeying local speed limits and traffic signals.
normal status
Already in the left lane and go straight
Draai linksaf met constante snelheidGo straight with constant speed
a car suddenly appears from right
Slow down, check your mirrors, and yield to the car coming from the right.
Continue driving straight at a consistent speed, observing local speed limits and traffic signs.
normal status
a big truck suddenly appears
Slow down, maintain distance from the truck, and proceed with caution.
The driver should turn left at a constant speed.
normal status
the weather turns cloudy
Turn left at a constant speed, be cautious due to cloudy weather.Random youtubeGPT-4V Scene Description
A bridge stretches ahead with light traffic, leading towards a modern city skyline under a clear sky.
Driving steadily, a car approaches a well-lit tunnel, bordered by concrete walls, under a clear blue sky.
A bustling city street in Amsterdam is Ãlled with pedestrians, vehicles, and cyclists, with the prominent "De Bijenkorf" department store in the background.
Netherlands Driving Handbook
Vertragen en omdraaien
The driver should slow down and turn around.
normal status
a car suddenly appears
Slow down, signal, check mirrors and blind spots, then carefully turn around when safe.
Construction workers operate machinery amid roadwork while pedestrians navigate the busy surrounding streets of a European city.
Figure 8. Combining LLaDA with GPT-4V and inference on random YouTube scenes. In the first two rows, we show the scenes of
Chongqing in China. In the rest two rows, we show the scenes of the crowded Amsterdam Center in Netherlands. As for the plans,
we input Dutch instead of English to verify the capability of LLaDA to interact with drivers from various cultures automatically without
mentioning the language type of the input. “Draai linksaf met constante snelheid” represents “Turn left with constant speed” and “Vertragen
en omdraaien” represents “decelerate and turn around”. We asked the system to output English instructions, we noticed that the system
extracts keywords in English and is able to provide accurate plans.
new locations.
Limitations. Though LLaDA provides various benefits,
it also suffers from two limitations: first, since LLaDA re-
quires running an LLM in the control loop, the runtime for
LLaDA is not yet conducive for closed-loop use in an A V
planning stack – this limitation is shared by all LLM-based
motion planners. Second, as discussed in our results ear-
lier, LLaDA is sensitive to the quality of scene descrip-
tions. Although GPT-4V can provide such descriptions,
they are sometimes not sufficiently accurate. This limitation
points towards the need to develop an A V-specific founda-
tion model that can provide A V-centric scene descriptions.
Broader Impact. As a human driver assistant, we hope
that LLaDA would reduce the number of road accidents in-
duced by tourists oblivious of the local traffic rules. As apolicy adapter for A Vs, we expect LLaDA to pave the way
towards easier traffic rule adaptation for A Vs allowing them
to expand their operations beyond geo-fenced regions.
Future Work. For future work, there are various direc-
tions we are excited to pursue: first, we will explore im-
proving GPT-4V’s scene descriptions by fine-tuning it on
A V datasets. Second, we will explore the development of
an unexpected scenario detector which will allow us to use
LLaDA only when it is needed, thereby significantly alle-
viating the computational burden involved in running an
LLM-based module in the control loop. Finally, we will
work towards furnishing safety certificates for the LLM out-
puts by leveraging recent developments in uncertainty quan-
tification and calibration techniques for ML, such as confor-
mal prediction and generalization theory.
14955
References
[1] Alexander Amini, Tsun-Hsuan Wang, Igor Gilitschenski,
Wilko Schwarting, Zhijian Liu, Song Han, Sertac Karaman,
and Daniela Rus. Vista 2.0: An open, data-driven simulator
for multimodal sensing and policy learning for autonomous
vehicles. In Proceedings of the International Conference on
Robotics and Automation (ICRA) , pages 2419–2426. IEEE,
2022. 2
[2] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
modal dataset for autonomous driving. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11621–11631, 2020. 1, 4
[3] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit
Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom,
and Sammy Omari. nuplan: A closed-loop ml-based plan-
ning benchmark for autonomous vehicles. arXiv preprint
arXiv:2106.11810 , 2021. 4
[4] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey
Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
worth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,
Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,
and Pete Florence. PaLM-E: An embodied multimodal lan-
guage model. In Int. Conf. Mach. Learn. , 2023. 2
[5] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan
Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter
Abbeel, Joshua B. Tenenbaum, Leslie Kaelbling, Andy
Zeng, and Jonathan Tompson. Video language planning,
2023. 2
[6] Amine Elhafsi, Rohan Sinha, Christopher Agia, Edward
Schmerling, Issa AD Nesnas, and Marco Pavone. Semantic
anomaly detection with large language models. Autonomous
Robots , pages 1–21, 2023. 1, 2
[7] Klemens Esterle, Luis Gressenbuch, and Alois Knoll. For-
malizing traffic rules for machine interpretability. In Pro-
ceedings of the Connected and Automated Vehicles Sympo-
sium (CAVS) , pages 1–7. IEEE, 2020. 2
[8] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung,
and Marco Pavone. Task-relevant failure detection for tra-
jectory predictors in autonomous vehicles. arXiv preprint
arXiv:2207.12380 , 2022. 2
[9] Angelos Filos, Panagiotis Tigkas, Rowan McAllister,
Nicholas Rhinehart, Sergey Levine, and Yarin Gal. Can au-
tonomous vehicles identify, recover from, and adapt to dis-
tribution shifts? In Proceedings of the International Con-
ference on Machine Learning (ICML) , pages 3145–3153.
PMLR, 2020.
[10] Dan Hendrycks, Steven Basart, Mantas Mazeika, Moham-
madreza Mostajabi, Jacob Steinhardt, and Dawn Song.
Scaling out-of-distribution detection for real-world settings.
arXiv preprint arXiv:1911.11132 , 2019. 2
[11] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi
Yan, and Dacheng Tao. St-p3: End-to-end vision-based au-tonomous driving via spatial-temporal feature learning. In
European Conference on Computer Vision (ECCV) , 2022. 6
[12] Masha Itkina and Mykel Kochenderfer. Interpretable self-
aware neural networks for robust trajectory prediction. In
Conference on Robot Learning , pages 606–617. PMLR,
2023. 2
[13] Boris Ivanovic, James Harrison, and Marco Pavone. Expand-
ing the deployment envelope of behavior prediction via adap-
tive meta-learning. In Proceedings of the IEEE International
Conference on Robotics and Automation (ICRA) , 2023. 2
[14] Jesper Karlsson and Jana Tumova. Intention-aware motion
planning with road rules. In Proceedings of the Interna-
tional Conference on Automation Science and Engineering
(CASE) , pages 526–532. IEEE, 2020. 2
[15] Markus Koppenborg, Peter Nickel, Birgit Naber, Andy
Lungfiel, and Michael Huelke. Effects of movement speed
and predictability in human–robot collaboration. Human
Factors and Ergonomics in Manufacturing & Service Indus-
tries, 27(4):197–209, 2017. 1
[16] Boyi Li, Philipp Wu, Pieter Abbeel, and Jitendra Malik. In-
teractive task planning with language models. arXiv preprint
arXiv:2310.10645 , 2023. 1, 2
[17] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol
Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code
as policies: Language model programs for embodied control.
InarXiv preprint arXiv:2209.07753 , 2022. 1, 2
[18] Sebastian Maierhofer, Anna-Katharina Rettinger, Eva Char-
lotte Mayer, and Matthias Althoff. Formalization of inter-
state traffic rules in temporal logic. In Proceedings of the
IEEE Intelligent Vehicles Symposium (IV) , pages 752–759.
IEEE, 2020. 2
[19] Kumar Manas and Adrian Paschke. Legal compliance check-
ing of autonomous driving with formalized traffic rule excep-
tions. In Proceedings of the Workshop on Logic Program-
ming and Legal Reasoning in conjunction with 39th Inter-
national Conference on Logic Programming (ICLP) , 2022.
2
[20] Kumar Manas, Stefan Zwicklbauer, and Adrian Paschke. Ro-
bust traffic rules and knowledge representation for conflict
resolution in autonomous driving. In Proceedings of the 16th
International Rule Challenge and 6th Doctoral Consortium
@ RuleML+RR , 2022. 2
[21] Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang.
GPT-Driver: Learning to drive with GPT. arXiv preprint
arXiv:2310.01415 , 2023. 2, 4, 6, 8
[22] Rowan McAllister, Gregory Kahn, Jeff Clune, and Sergey
Levine. Robustness to out-of-distribution inputs via task-
aware generative uncertainty. In Proceedings of the Inter-
national Conference on Robotics and Automation (ICRA) ,
pages 2083–2089. IEEE, 2019. 2
[23] New York City Department of Transportation. Right turn on
red in staten island. https://portal.311.nyc.gov/
article/?kanumber=KA-01354 . 2
[24] OpenAI. Gpt-4 technical report, 2023. 1
[25] Andreas Psarras, Theodore Panagiotidis, and Andreas An-
dronikidis. Covid-19, tourism and road traffic accidents: Ev-
idence from greece. Journal of Transportation Safety & Se-
curity , pages 1–21, 2023. 1
14956
[26] Jaume Rossell ´o and Oscar Saenz-de Miera. Road accidents
and tourism: the case of the balearic islands (spain). Accident
Analysis & Prevention , 43(3):675–683, 2011. 1
[27] Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou,
Nigamaa Nayakanti, Khaled S. Refaat, Rami Al-Rfou, and
Benjamin Sapp. Motionlm: Multi-agent motion forecasting
as language modeling, 2023. 2
[28] Bay City News Service and Embarcadero Staff. Aus-
tralian tourist charged with manslaughter in wrong-
way crash that killed two La Honda residents.
https : / / www . paloaltoonline . com / news /
2023/09/07/australian- tourist- charged-
with- manslaughter- in- wrong- way- crash-
that-killed-two-la-honda-residents , 2023.
1
[29] Sushant Veer, Karen Leung, Ryan Cosner, Yuxiao Chen, Pe-
ter Karkus, and Marco Pavone. Receding horizon planning
with rule hierarchies for autonomous vehicles. arXiv preprint
arXiv:2212.03323 , 2022. 2
[30] Sushant Veer, Apoorva Sharma, and Marco Pavone. Multi-
predictor fusion: Combining learning-based and rule-based
trajectory predictors. arXiv preprint arXiv:2307.01408 ,
2023. 2
[31] Tsun-Hsuan Wang, Alaa Maalouf, Wei Xiao, Yutong Ban,
Alexander Amini, Guy Rosman, Sertac Karaman, and
Daniela Rus. Drive anywhere: Generalizable end-to-end
autonomous driving with multi-modal foundation models.
arXiv preprint arXiv:2310.17642 , 2023. 2
[32] Wayve. Lingo-1: Exploring natural language for au-
tonomous driving. https://wayve.ai/thinking/
lingo - natural - language - autonomous -
driving/ , 2023. 1
[33] Wei Xiao, Noushin Mehdipour, Anne Collin, Amitai Y Bin-
Nun, Emilio Frazzoli, Radboud Duintjer Tebbens, and Calin
Belta. Rule-based optimal control for autonomous driving.
InProceedings ACM/IEEE Int. Conf. on Cyber-Physical Sys-
tems, pages 143–154, 2021. 2
[34] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo,
Kenneth KY Wong, Zhenguo Li, and Hengshuang Zhao.
Drivegpt4: Interpretable end-to-end autonomous driving via
large language model. arXiv preprint arXiv:2310.01412 ,
2023. 2
[35] Yu Zhang, Sarath Sreedharan, Anagha Kulkarni, Tathagata
Chakraborti, Hankz Hankui Zhuo, and Subbarao Kambham-
pati. Plan explicability and predictability for robot task plan-
ning. In Proceedings of the IEEE International Conference
on Robotics and Automation (ICRA) , 2017. 1
14957
