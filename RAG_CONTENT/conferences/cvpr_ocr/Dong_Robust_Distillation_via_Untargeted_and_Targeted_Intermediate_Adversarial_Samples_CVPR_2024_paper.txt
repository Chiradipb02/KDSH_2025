Robust Distillation via Untargeted and Targeted Intermediate Adversarial Samples
Junhao Dong1,2, Piotr Koniusz4,3*, Junxi Chen5, Z. Jane Wang6, and Yew-Soon Ong1,2*
1Nanyang Technological University,2CFAR, IHPC, A*STAR,3Australian National University,
4Data61 CSIRO,5Sun Yat-sen University,6University of British Columbia
{junhao003, asysong }@ntu.edu.sg, piotr.koniusz@data61.csiro.au,
chenjx353@mail2.sysu.edu.cn, zjanew@ece.ubc.ca
Abstract
Adversarially robust knowledge distillation aims to com-
press large-scale models into lightweight models while pre-
serving adversarial robustness and natural performance on
a given dataset. Existing methods typically align probabil-
ity distributions of natural and adversarial samples between
teacher and student models, but they overlook intermediate
adversarial samples along the “adversarial path” formed
by the multi-step gradient ascent of a sample towards the
decision boundary. Such paths capture rich information
about the decision boundary. In this paper, we propose a
novel adversarially robust knowledge distillation approach
by incorporating such adversarial paths into the alignment
process. Recognizing the diverse impacts of intermediate
adversarial samples (ranging from benign to noisy), we
propose an adaptive weighting strategy to selectively em-
phasize informative adversarial samples, thus ensuring ef-
ficient utilization of lightweight model capacity. Moreover,
we propose a dual-branch mechanism exploiting two fol-
lowing insights: (i) complementary dynamics of adversar-
ial paths obtained by targeted and untargeted adversarial
learning, and (ii) inherent differences between the gradient
ascent path from class citowards the nearest class bound-
ary and the gradient descent path from a specific class cj
towards the decision region of ci(i̸=j). Comprehensive
experiments demonstrate the effectiveness of our method on
lightweight models under various settings.
1. Introduction
Deep Neural Networks (DNNs) have advanced image clas-
sification [17, 18, 24, 27, 39, 41] and retrieval [61], gener-
ative models [35, 36, 45], deblurring [59], few-shot learn-
ing [23, 29, 51, 52], medical diagnosis [9], and biometrics
[32]. However, DNNs are vulnerable to adversarial exam-
ples: images with imperceptible perturbations [48]. Despite
their visual similarity to the natural images, adversarial ex-
amples can fool DNNs, leading to incorrect or harmful pre-
*Corresponding author.
 robust
teacherstudent
alignment
student
adversarial
samplenatural
sample1 step1 2 ...n-1intermediate
adversarial
samples
decision
boundary
student
vanillaadv.
robustdistillation types:
student
ours1 2 3
4studentn-2or
or or orFigure 1. Comparison of distillation methods. (i) Vanilla distilla-
tion employs a naturally trained teacher model without adversar-
ial samples, resulting in susceptibility to adversarial attacks. (ii)
Standard adversarially robust distillation incorporates adver-
sary generation from each natural sample x(black frame) to its ad-
versarial counterpart ˆx(red frame). The robust teacher is adversar-
ially pre-trained, and thus the student can distill its responses for
natural and adversarial samples. (iii) Our adversarially robust
knowledge distillation with intermediate adversarial samples
extends upon this by introducing intermediate adversarial sam-
plesˆx(1),···,ˆx(n−1)collected from intermediate steps along the
“adversarial path”, thereby enabling a more nuanced transfer of
robust knowledge based on the comprehensive adversarial land-
scape. Distillation types are further illustrated in Appendix A.
dictions with high confidence. Thus, the adversarial vulner-
abilities undermine public confidence in the reliability of
DNNs and raise trustworthiness concerns [22].
Adversarial training [16] helps models become adversar-
ially robust by augmenting the training set with adversarial
samples. However, adversarial training is computationally
costly, with limited usage in resource-constrained scenar-
ios. Moreover, the robustness achieved by adversarial train-
ing is mainly observed in large models, leaving lightweight
models vulnerable to adversarial attacks [3]. Unlike vanilla
knowledge distillation [19], “adversarially robust knowl-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28432
edge distillation” [15] overcomes this issue by compressing
an adversarially robust large-scale model into a lightweight
one without sacrificing much performance.
Adversarially robust knowledge distillation typically fo-
cuses on transferring the robustness of a large-scale model
to a resource-efficient model by training the student model
to mimic outputs of the robust teacher model using merely
natural and adversarial samples [15, 63, 64]. However, ne-
glecting intermediate adversarial samples from the “adver-
sarial path” obtained during iterative adversary generation
(e.g., multi-step gradient ascent) is suboptimal. Specifi-
cally, weighted intermediate adversarial samples contribute
auxiliary information about the decision boundary, while
appropriate weights can amplify informative samples and
suppress noisy ones. Fig. 1 shows a comparison of our ro-
bust distillation with the adversarial path alongside both the
standard adversarially robust and vanilla distillation.
We also notice that previous methods primarily rely on
untargeted adversarial samples (robustness w.r.t. the near-
est decision boundary of a natural sample). In contrast, our
work extends this by exploiting complex decision bound-
aries between pairs of classes via “targeted adversarial
paths”. Specifically, we propose a dual-branch mechanism
that comprises: (i) an untargeted adversarial branch, where
the predictions of natural samples of class cifrom teacher
and untargeted adversarial samples from student should
align, (ii) a targeted adversarial branch, which aligns the
prediction scores of natural samples of class cj(i̸=j) from
teacher and targeted adversaries from student, and (iii) a di-
vergence criterion that push apart predictions of samples be-
tween both branches to differentiate untargeted and targeted
adversaries based on underlying semantic distinctions.
Extensive experiments and analyses demonstrate the su-
perior performance of our method compared with other ro-
bust knowledge distillation approaches under various sce-
narios. Our contributions can be summarized as follows:
i. We propose a novel adversarially robust knowledge
distillation method that integrates intermediate adver-
saries along the adversarial path. An adaptive weight-
ing mechanism is proposed to calibrate the influence of
each intermediate sample to facilitate the distillation of
adversarial paths, refining a robust “understanding” of
the decision boundary. Our strategy also leads to mini-
mizing an upper bound of the adversarially robust risk.
ii. To capture relations between decision boundaries, we
devise a dual-branch mechanism by harnessing the
complementary characteristics of untargeted and tar-
geted adversarial samples. This inter-class relational
learning facilitates a more effective robustness transfer.
iii. Extensive experiments showcase the superiority of our
method compared with the state-of-the-art approaches
across various settings, including diverse backbones,
auxiliary data, and cross-dataset distillation.Related works. Given the security risks posed by adversar-
ial attacks [11, 47], extensive solutions have been proposed
to improve DNN robustness [7, 30, 56]. Among them, ad-
versarial training [12, 38, 53, 58] has emerged as a power-
ful technique to achieve non-trivial robustness by augment-
ing adversaries into training data. However, its performance
highly relies on the network capacity [3], limiting its practi-
cal applicability for small models. To bridge this gap, recent
studies [15, 54, 62–64] have delved into robust knowledge
distillation to transfer adversarial robustness to lightweight
models. Zi et al. [64] incorporated soft labels from a ro-
bust teacher model as fixed references for distribution align-
ment. Besides the logit-level alignment, Bai et al. [1] intro-
duced contrastive learning to robustness transfer via latent
features. However, previous works primarily focus on the
use of natural and adversarial samples for distillation, over-
looking the significance of untargeted and targeted interme-
diate adversaries (paths) to the decision boundary.
Background. The goal of adversarially robust knowledge
distillation is to transfer adversarial robustness from a large-
scale model ( teacher model) to a lightweight model ( stu-
dent model), where the teacher model is adversarially pre-
trained to obtain robustness. Let a DNN-based classifier
fθ:X → [0,1]Cwith network parameters θ, which out-
puts probabilities of Cclasses. Given a dataset with distri-
bution D, standard adversarial training [30] under the ℓ∞-
norm threat model solves the following minimax problem:
min
θE(x,y)∼Dh
max
∥δ∥∞<ϵLCE(fθ(x+δ), y)i
,(1)
where δdenotes the adversarial perturbation bounded
within the ℓ∞-norm of magnitude ϵ, andLCErepresents the
Cross-Entropy (CE) loss. The outer minimization optimizes
the adversarial empirical risk over the network parameters
θ, while the inner maximization finds the worst-case adver-
sarial examples ˆ x=x+δ. The Projected Gradient Descent
(PGD) [30] is used to optimize the inner maximization:
ˆ x(i+1)=ϑα(ˆ x(i), y)
= Π
B(x,ϵ)
ˆ x(i)+α·sign 
∇ˆ x(i)LCE 
fθ(ˆ x(i)), y
,(2)
where αis the gradient descent step size. ΠB(x,ϵ)(·)is the
projection into the constraints box with ℓ∞radius ϵaround
x. We randomly initialize ˆ x(0)∼x+ 0.001· N(0,I). Af-
ternsteps, one obtains ˆ xand intermediate samples I(x)=
{ˆ x(i)}n−1
i=1that are also adversarial due to the non-linear dy-
namics of decision boundaries. DNNs are also vulnerable to
intermediate adversarial samples, which motivates our idea.
See Appendix C.1 for further details of teacher pre-training.
2. Proposed Approach
In this section, we propose our novel dual-branch (untar-
geted and targeted) adversarially robust knowledge distilla-
tion method based on intermediate adversarial samples.
28433
2.1. Distillation with Intermediate Adversaries
In contrast to vanilla knowledge distillation, which merely
relies on natural samples (non-robust features), robust
knowledge distillation additionally incorporates adversaries
(robust features) for robustness transfer. Existing works
typically utilize predictions of teacher on natural examples
as the only reference points for aligning with predictions of
student for both natural and adversarial samples, yet such a
strategy fails to mimic the responses of teacher to adversar-
ial samples. In our work, we refine this alignment by match-
ing predictions (softmax logits) between the robust teacher
model fθt(·)and the student model fθs(·)on both natural
samples and their adversarial counterparts. We define the
Adversarially Robust Knowledge Distillation (ARKD) as:
LARKD=LKL 
fθt(x)∥fθs(x)
| {z }
alignment of “natural distributions”+β·LKL 
fθt(ˆ x)∥fθs(ˆ x)
| {z }
alignment of “adversarial distributions”,
(3)
where LKLrepresents Kullback–Leibler (KL) divergence,
andβ≥0controls the trade-off between natural perfor-
mance and adversarial robustness. Note that Eq. (3) aligns
prediction scores without relying on ground-truth labels,
which facilitates the adversarially robust knowledge trans-
fer instead of adversarial training from scratch. To obtain
an adversarial sample ˆxfrom its natural counterpart x, one
may use the true label yofxor the prediction pt(x)of
the teacher during the adversary generation process towards
ˆx. Our method performs such an “adversarially consis-
tent alignment” as adversaries generated against the student
model help probe responses of the adversarially pre-trained
teacher, capturing the structure of decision boundaries.
However, Eq. (3) and existing methods only employ nat-
ural samples and their adversarial counterparts–they ignore
the intermediate adversarial samples (see Fig. 1). In con-
trast, to explore and capture well the decision boundary of
the teacher, we propose to augment intermediate adversar-
ial samples Is 
(x, y)
=
(ˆ x(i), y)	n−1
i=1generated from a
sample (x, y)into the distillation process. Intermediate Ad-
versarial Knowledge Distillation (IAKD) is defined as:
LIAKD=n−1X
i=1w 
ˆ x(i)|x
· LKL 
fθt(ˆ x(i))∥fθs(ˆ x(i))
,(4)
where w(·)denotes the instance-wise training weight (nor-
malized within the range of [0,1]) for each intermediate ad-
versarial sample. Weights are designed as interpolating be-
tween (i) an index-based prior, i.e., penalty i/(n−1)gives
stronger weights to intermediate samples closer to the final
adversarial sample ˆx, and (ii) batch-level discrepancy be-
tween the student model’s predictions on intermediate ad-
versaries and the teacher model’s predictions on their corre-
sponding natural samples. Specifically, we define:
w 
ˆ x(i)|x
=(1−γ)i
n+γ 
fθt(x)
y− 
fθs 
ˆ x(i)
y
max
j∈B 
fθt(xj)
yj− 
fθs 
ˆ x(i)
j
yj,(5)
 robust
teacherstudent
attract
student
targeted adv . branch
 robust
teacher studentstudent
untargeted adv . branch
attractrepelFigure 2. The dual-branch mechanism contains (i) untargeted and
(ii) targeted adversary generation branches. The untargeted branch
takes sample xof class y=ciand produces (intermediate) ad-
versarial samples ˆx,ˆx(1),···,ˆx(n−1)by iterative gradient ascent
towards ¬ci(notciclass). In contrast, the targeted branch takes
sample x′of class y′=cjand produces (intermediate) adversar-
ial samples ˆx′,ˆx′(1),···,ˆx′(n−1)by iterative gradient descent to-
wards class ci. Subsequently, in each branch, the student’s predic-
tions for (intermediate) adversarial samples are attracted towards
the teacher’s predictions for the natural sample, i.e.,ˆps→pt
andˆp(1)
s,···,ˆp(n−1)
s→ptfor untargeted adversarial branch and
ˆp′
s→p′
tandˆp′(1)
s,···,ˆp′(n−1)
s →p′
tfor targeted adversarial
branch. Moreover, to improve complementarity of both branches,
in each branch, predictions of the student model for (intermediate)
adversarial samples are repelled from that of the teacher model for
the natural sample of the other branch, e.g.,ˆp′
s↔ptandˆps↔p′
t.
where 0≤γ≤1interpolates between the prior and the pre-
diction discrepancy for a mini-batch B, and 
f(·)
yextracts
they-th coefficient from prediction scores of function f(·).
2.2. Dual-branch Adversarially Robust knoWledge
dIstillatioN (DARWIN)
Several studies have demonstrated the significance of deci-
sion surface modeling to adversarial robustness [8, 28, 34,
60]. The well-established decision boundaries are simulta-
neously applicable to natural samples and their adversarial
counterparts. As existing works typically use untargeted ad-
versarial samples, we propose to use a combination of both
untargeted and targeted adversaries with the goal of more
effectively capturing the structure of decision boundaries of
teacher . Fig. 2 illustrates our dual-branch mechanism.
Untargeted adversarial samples are generated via multi-
step gradient ascent towards the nearest decision boundary
in Eq. (2), where fθ(·)is replaced with fθs(·). Targeted
adversaries are obtained by crossing the decision boundary
towards a chosen class. Specifically, we choose ˆ x′(0)∼x′+
0.001·N(0,I)where x′∈{x:y(x)=cj}and define:
ˆ x′(i+1)=ϑ′
α(ˆ x′(i), y′)
= Π
B(x′,ϵ)
ˆ x′(i)−α·sign 
∇ˆ x′(i)LCE 
fθs(ˆ x′(i)), y′
,(6)
28434
Algorithm 1 Dual-branch Adversarially Robust knoWl-
edge dIstillatioN (DARWIN).
Input: Adversarially pre-trained teacher model fθtwith parameters θt;
student model fθswith parameters θs; dataset D=
(xi, yi)	D
i=1with
Ddata points and Cclasses; batch size m; learning rate τ; maximum ra-
dius of the adversarial perturbation ϵand the attack step size α; mini-batch
sizem; the number of intermediate adversarial samples n−1(iteration
stepsn); hyper-parameters λ1,λ2andβ.
1: Initialize student network parameters θs
2:while not at end of distillation do
3:Form mini-batches for targeted/untargeted adversary generation:
B=
(xj, yj)	m
j=1andB′=
(x′
j, y′
j):y′
j̸=yj	m
j=1
4: Set lARKD =0,lIAKD=0, and lDBKD =0
5: forj= 1,2, . . . , m (in parallel) do
6: Draw ˆx(0)
j∼xj+0.001·N(0,I),ˆx′(0)
j∼x′
j+0.001·N(0,I)
7: fori= 1,2, . . . , n do
8:Generate untargeted/targeted (intermediate) adv. samples:
ˆx(i)
j←ϑα(ˆ x(i−1)
j, yj)andˆx′(i)
j←ϑ′
α(ˆ x′(i−1)
j, y′
j)
9:Use Eq. (8) to accumulate the DBKD loss:
lDBKD←lDBKD
+w 
ˆ x(i)
j|xj
Ltri 
fθt 
x(i)
j
, fθs 
ˆ x(i)
j
, fθs ˆx′(i)
j
+w 
ˆ x′(i)
j|x′
j
Ltri 
fθt 
x′(i)
j
, fθs ˆx′(i)
j
, fθs 
ˆ x(i)
j
10: end for
11:Use Eq. (4) to accumulate the IAKD loss:
lIAKD←lIAKD+n−1P
i=1w 
ˆ x(i)
j|xj
LKL 
fθt(ˆ x(i)
j)∥fθs(ˆ x(i)
j)
12: Set ˆxj=ˆx(n)
j(untargeted adversarial sample)
13:Use Eq. (3) to accumulate the ARKD loss:
lARKD←lARKD
+LKL 
fθt(xj)∥fθs(xj)
+β·LKL 
fθt(ˆ xj)∥fθs(ˆ xj)
14: end for
15:Update student network parameters:
θs←θs−τ∇θs
lARKD +λ1lIAKD+λ2lDBKD
16:end while
17:return Student network parameters θs.
where y′=ci,ci̸=cjandi̸=j. For simplicity of notations,
let adversarial sample ˆxbe appended to intermediate adver-
sarial samplesˆx(i)	n−1
i=1so that ˆx(n)=ˆx. By analogy, we
setˆx′(n)=ˆx′. Hence, our dual-branch mechanism performs
the following attraction and repulsion steps:attract(
fθs(ˆ x(i))	n
i=1→fθt(x)
fθs(ˆ x′(i))	n
i=1→fθt(x′),
repel(
fθs(ˆ x(i))	n
i=1↔fθt(x′)
fθs(ˆ x′(i))	n
i=1↔fθt(x),(7)
where→and↔represent the attraction and repulsion oper-
ations, respectively. The above steps form our Dual-Branch
Knowledge Distillation (DBKD) triplet-based loss:
LDBKD=nX
i=1h
w 
ˆ x(i)|x
Ltri 
fθt(x), fθs 
ˆ x(i)
, fθs ˆx′(i)
+w 
ˆ x′(i)|x′
Ltri 
fθt(x′), fθs 
ˆ x′(i)
, fθs ˆx(i)i
,(8)
where Ltri(p,q,r)with a margin constant mis defined as:
Ltri(p,q,r)=max 
∥p−q∥2
2−∥p−r∥2
2+m,0
.(9)Considering the underlying domain shift between untar-
geted and targeted adversarial samples [14], we further pro-
pose to utilize separate Batch Normalization (BN) [21] lay-
ers for both branches, which facilitates the disentanglement
of the mixed distributions. The main BN layer is primarily
used for natural images and untargeted (standard) adversar-
ial samples, while the auxiliary BN is utilized for the tar-
geted adversarial samples. During the inference stage, only
the primary BN layer is employed.
Objective function. Our final loss is a combination of the
adversarially robust knowledge distillation loss LARKD , the
intermediate adversarial knowledge distillation loss LIAKD,
and the dual-branch knowledge distillation loss LDBKD :
L=LARKD+λ1LIAKD+λ2LDBKD, (10)
where λ1≥0andλ2≥0are weighting hyper-parameters.
The pseudocode of our proposed method is provided in Al-
gorithm 1. During the inference stage, we directly use the
distilled student model for robustness evaluations.
2.3. Label-free Adversarially Robust Distillation
As Eq. (2) and (6) primarily rely on ground-truth labels to
generate untargeted and targeted adversaries, such reliance
can be restrictive when labels are unavailable. To address
this, we propose a label-free robust distillation scheme,
called DARWIN-LF, by leveraging the predictions of the
teacher to simulate ground-truth labels. Hence, untargeted
(intermediate) adversarial samples can be obtained by re-
placing cross-entropy between fθs(ˆ x(i))andyin Eq. (2)
with KL divergence between fθs(ˆ x(i))and˜y=fθt(x):
ˆ x(i+1)=ϑα(ˆ x(i),˜y)
= Π
B(x,ϵ)
ˆ x(i)+α·sign 
∇ˆ x(i)LKL(fθs(ˆ x(i))∥˜y)
.(11)
For targeted adversary generation in Eq. (6), we also replace
the cross-entropy between fθs(ˆ x(i))andy′with KL diver-
gence between fθs(ˆ x(i))and˜y′=fθt(x′)where x′̸=x:
ˆ x′(i+1)=ϑ′
α(ˆ x′(i),˜y′)
= Π
B(x′,ϵ)
ˆ x′(i)−α·sign 
∇ˆ x′(i)LKL 
fθs(ˆ x′(i))∥˜y′
.(12)
Note that the assertion y′̸=yfor Eq. (6) may not al-
ways hold if we randomly draw two samples xandx′.
For a dataset of size Dpartitioned equally into Cclasses,
the chance of them sharing the same label is p 
y(x) =
y(x′)
= 1/C2, which is extremely low for typical C≥
10. Alternatively, one may compare indexes of the top-k
most confident prediction scores fθt(x)andfθt(x′). One
may repeat random sampling for x′if resulting indexes
agree, i.e.,1 
sort(fθt(x))
1:k= 
sort(fθt(x′))
1:k
, or
if∥fθt(x)−fθt(x′)∥1≤hwhere h= 1/C,etc. Here,
1(v=v′)returns 1 if v=v′and(v)1:kreturns the first
kcoefficients of v. By default, we use the “comparison of
indexes” strategy for DARWIN-LF. See also Appendix C.2.
28435
2.4. DARWIN Minimizes an Upper Bound of the
Adversarially Robust Risk
Letf∗
θ(x) = argmaxc[fθ(x)]cbe the index of maximum
predicted probability. Zhang et al. [58] decompose the
so-called robust risk Rrob(fθs;V)of the distilled student
model fθsunder set Vas in the following definition.
Definition 1. The so-called robust, natural, and boundary
risks are defined by Zhang et al. [58] as:
Rrob(fθs;V):=E(x,y)∼V[ 1(∃ˆx∈B(x, ϵ):f∗
θs(ˆx)̸=y)],
Rnat(fθs;V):=E(x,y)∼V[ 1(f∗
θs(x)̸=y)], (13)
Rbdy(fθs;V):=E(x,y)∼V[ 1(∃ˆx∈B(x, ϵ):f∗
θs(ˆx)̸=f∗
θs(x)=y)],
where ϵ≥0is the radius of a ball around xunder the ℓ∞
norm1. Also, Rrob(fθs;V)=Rnat(fθs;V)+Rbdy(fθs;V).
The natural risk corresponds to the error on natural ex-
amples, while the boundary risk represents how close nat-
ural samples are to the decision boundary under ϵ. Typical
adversarially robust classification models minimize the ro-
bust risk, which ensures good performance on natural and
adversarial samples due to low natural and boundary risks.
As DARWIN utilizes intermediate adversarial samples,
our proposed weighting mechanism assigns higher weights
to such samples proportionally to δy 
x,ˆ x(i)
= 
fθt(x)
y− 
fθs 
ˆ x(i)
y, signifying how fast the change of soft-score
for label yis asx→ˆ x(i)based on the following theorem.
Theorem 1. The weighting mechanism in Eq. (5)captures
the localized κ-Lipschitz smoothness of the student network
when fθt(·)≈fθs(·)(reasonably holds when the student is
converging) and γ=1(holds for any γ∈(0,1]). We have:
δy 
x,ˆ x(i)
ϵ≤δy 
x,ˆ x(i)
∥x−ˆ x(i)∥∞≤κ≤δy 
x,ˆ x(i)
α,∀i=1,···,
n−1.(14)
Proof. See Appendix D.3. Here, |·|is the absolute val.
Hence, higher weights for intermediate adversaries indi-
cate higher κand higher weighted boundary risk.
Definition 2. The weighted boundary risk is defined as:
bRbdy(fθs;V):=X
(x,y)∼Vw(ˆ x|x)
ω1(∃ˆx∈B(x, ϵ):f∗
θs(ˆx)̸=f∗
θs(x)=y),
=E
(x,y)∼Vw
1(∃ˆx∈B(x, ϵ):f∗
θs(ˆx)̸=f∗
θs(x)=y)
,(15)
where ω=P
(x,y)∼Vw(ˆ x|x)ensures normalized weights
andVwis the distribution resulting from Vunder weights.
Definition 3. LetIscontain intermediate adversarial sam-
ples for dataset D. LetIs=∪(x,y)∈DIs 
(x, y)
be split
into sets I✗
s={(x, y)∈Is:f∗
θs(x)̸=y}andI✓
s={(x, y)∈
Is:f∗
θs(x)=y}that contain incorrectly and correctly clas-
sified intermediate adversarial samples, respectively.
1Please note that (x, y)is a sample-label pair enumerated from the set
V(which can contain clean samples, adversarial samples, etc.), so the role
and meaning of xandydepend on what set Vcontains.Theorem 1 indicates that applying the IAKD loss from
Eq. (4) (which uses weighting) decreases locally the
Lipschitz constant κ, resulting in a smoother student
function, implying better stability against adversarial at-
tacks. This is signified by Definition 2, which suggests
that with high probability, bRbdy(fθs;V)≥Rbdy(fθs;V)
holds, which explains that we minimize the (more chal-
lenging) upper bound of the robust risk that simultane-
ously improves the smoothness of the student net.
Theorem 2. The difference between the robust risk of D∪I s
(dataset and its intermediate adversarial samples) and the
robust risk of dataset Dalone is given as:
Rrob(D∪I s)− R rob(D)= (16)I✗
s(Rrob(I✗
s)−Rrob(D))
|D|+|Is|+I✓
s(Rrob(I✓
s)− R rob(D))
|D|+|Is|
Proof. See Appendix D.1. Here, |·|is the cardinality.
Theorem 3. DARWIN minimizes the robust risk Rrob(D∪
Is), which is the upper bound of the robust risk of D,i.e.,
Rrob(D ∪ I s)≥ R rob(D)ifτbdy≥ R nat(D)where the
boundary risk gain obtained from introducing intermediate
adversarial samples Isisτbdy=Rbdy(I✓
s)−Rbdy(D)≥0.
Proof. See Appendix D.2.
Theorem 3 tells that if the boundary risk gain τbdy≥
Rnat(D), the use of intermediate adversarial samples Is
with dataset Dleads to minimization of an upper bound
of the robust risk. Moreover, the weighted boundary risk
is “focused” on the most perturbing cases. Thus, we
expect that bRbdy(fθs;I✓
s)≥ R bdy(fθs;I✓
s)with high
probability, yet we cannot guarantee that (the weights do
not capture the decision boundary). We conclude that
the use of the weighted boundary risk increases τbdy,
helping ensure the upper bound we optimize holds.
Ifτbdy<Rnat(fθs;D), DARWIN achieves improve-
ment in adversarial robustness but we expect a small drop
in the natural performance as the boundary risk gain τbdy=
Rbdy(I✓
s)−Rbdy(D)≥0is insufficient to compensate for
Rnat(fθs;D)so that the assertion τbdy≥ R nat(fθs;D)
from Theorem 3 cannot hold. Section 3.2 shows that when
the assertion is violated, a small drop in the natural perfor-
mance occurs (note that this happens in many such pipelines
for adversarially robust self-distillation).
3. Experiments
Below, we provide our experimental setups and compare
DARWIN with other adversarially robust distillation works.
Datasets. We conduct experiments on four datasets:
CIFAR-10, CIFAR-100 [25], ImageNet-100, and TinyIm-
ageNet [42]. See Appendix B.1 for details.
28436
Table 1. CIFAR-10 and CIFAR-100: Comparisons of our DARWIN with other robust knowledge distillation methods when distilled from
the large-scale WRN-34 teacher model . The ℓ∞-norm adversarial perturbations are restricted within ϵ= 8/255. We report both natural
accuracy (%) and robust accuracy (%). “ y” indicates if class labels are required. The best distillation result in each column is in bold .
Type Architecture Method yCIFAR-10 CIFAR-100
Natural PGD-20 CW AA Natural PGD-20 CW AA
Teacher WRN-34 TRADES [58] ✓ 84.92 55.34 54.21 52.55 60.04 31.56 28.64 27.38
StudentResNet-18ARD [15] ✓ 82.95 52.26 51.69 49.46 57.46 30.14 27.11 25.30
IAD [63] ✓ 82.41 53.06 51.79 49.78 56.38 30.61 27.35 25.51
RSLAD [64] ✗ 83.12 53.91 52.84 51.19 57.23 31.08 28.29 26.62
CRDND [54] ✗ 83.92 52.70 50.95 49.05 58.03 30.16 27.02 25.68
GACD [1] ✗ 82.76 53.42 52.26 50.07 56.82 31.19 27.81 26.12
DARWIN ✓ 84.48 55.07 53.85 52.24 59.12 32.30 28.95 27.26
DARWIN-LF ✗ 84.35 55.02 53.99 52.33 59.04 32.18 28.62 27.13
MNV2ARD [15] ✓ 82.44 51.91 50.64 48.40 55.28 30.23 27.05 25.28
IAD [63] ✓ 81.61 52.30 50.19 48.34 54.26 30.46 27.13 25.50
RSLAD [64] ✗ 82.89 52.72 52.04 50.04 57.31 30.48 27.86 25.89
CRDND [54] ✗ 82.77 52.57 50.11 49.28 56.24 29.65 26.68 25.61
GACD [1] ✗ 82.90 52.49 51.40 49.55 56.10 30.49 27.18 25.33
DARWIN ✓ 84.06 53.94 53.11 51.28 58.45 31.53 28.36 26.55
DARWIN-LF ✗ 84.08 53.76 52.80 51.09 58.41 31.44 28.33 26.58
Table 2. CIFAR-10 and CIFAR-100: Comparisons of our DARWIN with other robust knowledge distillation methods under the self-
distillation scenario . The ℓ∞-norm adversarial perturbations are restricted within ϵ= 8/255. We report both natural accuracy (%) and
robust accuracy (%). “ y” indicates whether class labels are required. The best distillation result in each column is in bold .
Type Architecture Method yCIFAR-10 CIFAR-100
Natural PGD-20 CW AA Natural PGD-20 CW AA
Teacher ResNet-18 TRADES [58] ✓ 82.45 52.21 50.29 48.90 56.37 28.68 24.87 23.78
Student ResNet-18ARD [15] ✓ 81.64 52.62 51.35 49.19 57.96 31.34 27.84 26.13
IAD [63] ✓ 80.66 52.63 52.21 48.90 56.45 31.87 28.00 26.66
RSLAD [64] ✗ 81.30 53.80 52.32 50.78 55.17 31.21 27.82 26.46
CRDND [54] ✗ 81.52 52.88 50.85 48.40 56.69 30.13 26.90 26.08
GACD [1] ✗ 81.03 53.26 51.75 49.48 56.95 31.80 28.07 26.30
DARWIN ✓ 82.23 55.15 53.22 51.31 57.74 32.17 28.40 26.89
DARWIN-LF ✗ 82.25 55.12 53.15 51.23 57.82 32.11 28.26 26.73
Teacher MNV2 TRADES [58] ✓ 81.04 50.87 48.46 47.15 54.11 27.28 23.39 22.36
Student MNV2ARD [15] ✓ 81.25 53.02 50.69 48.85 55.64 30.93 27.47 26.05
IAD [63] ✓ 79.36 53.45 50.93 49.14 54.00 31.01 27.59 26.11
RSLAD [64] ✗ 80.01 53.35 51.04 49.74 53.52 29.95 26.66 25.47
CRDND [54] ✗ 80.27 52.92 50.14 49.10 53.70 29.89 26.70 25.67
GACD [1] ✗ 81.17 53.26 50.85 49.18 54.48 31.23 27.38 26.00
DARWIN ✓ 82.18 54.74 52.65 50.97 57.04 31.89 28.19 26.33
DARWIN-LF ✗ 82.12 54.52 52.35 50.58 56.90 31.96 28.16 26.20
Implementation details. Following [15, 63, 64] & Ro-
bustBench [6], we use ResNet-18/34 [18], MobileNetV2
(MNV2) [43], Wide-ResNet-28-10/34-10 (WRN-28/34)
[57] as teacher and student models. We also try Vision
Transformers (ViTs) [13, 49] as a teacher. We adopt regu-
larization factors β=4.0andγ=0.5with margin m=0.1.
In all experiments, loss weighting factors λ1= 1.0and
λ2= 0.5. We use the ℓ∞-norm threat model with pertur-
bation radius ϵ=8/255. See Appendix B.2 for more details
and Appendix E for hyper-parameter evaluations.
3.1. Main Results
DARWIN with the WRN-34 teacher net. Table 1 reports
the classification accuracies on natural examples and theiradversarial counterparts based on three standard adversar-
ial attack methods: PGD [30] of 20iterations with step size
α=2, CW [4], and AutoAttack (AA) [5]. AA is a powerful
parameter-free robustness evaluation with three white-box
(targeted/untargeted) attacks and a black-box attack. For
fair comparisons, all experiments use the WRN-34 teacher
model. Table 1 shows that DARWIN overall achieves the
best natural and adversarial performances. The superior dis-
tillation results with ResNet-18 [18] and MNV2 [43] stu-
dent backbones show the versatility of DARWIN.
DARWIN in the self-distillation setting. Table 2 evalu-
ates DARWIN in the self-distillation setting for which the
teacher and student backbones are identical. As one can see,
DARWIN and its label-free extension, DARWIN-LF, out-
28437
Table 3. ImageNet-100: Robust accuracy (%) of distilled models
when using ResNet-18 and MNV2 student backbones.
Type Architecture Method Natural PGD-20 AA
Teacher ResNet-34 TRADES [58] 72.66 40.88 34.70
StudentResNet-18ARD [15] 65.41 38.34 30.94
RSLAD [64] 66.60 39.12 32.18
IAD [63] 65.62 39.09 32.63
CRDND [54] 65.39 39.33 31.36
GACD [1] 64.85 38.48 31.83
DARWIN 68.76 40.37 33.58
DARWIN-LF 68.91 40.16 33.29
MNV2ARD [15] 65.90 37.28 30.20
RSLAD [64] 65.82 37.86 31.66
IAD [63] 64.96 38.00 31.40
CRDND [54] 65.09 37.54 30.56
GACD [1] 64.20 37.27 31.11
DARWIN 67.93 39.38 32.85
DARWIN-LF 67.86 39.24 32.51
Table 4. Comparison of robust self-distillation methods (ResNet-
18→ResNet-18) on CIFAR-10/CIFAR-100 with auxiliary genera-
tive data. We report the natural & (Auto-Attack) robust accuracies.
Dataset Type DDPM Method Natural Robust
CIFAR-10Teacher ✗ TRADES [58] 82.45 48.90
Student✓ ARD [15] 82.89 53.41
✓ RSLAD [64] 82.05 52.60
✓ IAD [63] 82.95 53.47
✓ DARWIN 84.13 55.92
✓ DARWIN-LF 84.68 56.41
CIFAR-100Teacher ✗ TRADES [58] 56.37 23.78
Student✓ ARD [15] 56.07 26.92
✓ RSLAD [64] 53.40 26.00
✓ IAD [63] 55.82 26.77
✓ DARWIN 58.18 28.24
✓ DARWIN-LF 58.74 28.45
perform other methods and the teacher architecture on the
robust accuracy for CIFAR-10 and CIFAR-100. In terms
of natural performance, DARWIN achieves comparable or
even better results than the teacher model. We attribute
great robust accuracy to the dual-branch mechanism that
helps explore the complex decision boundary.
Robust distillation on ImageNet-100. We here evalu-
ate our approach in the context of larger-scale classifica-
tion with higher-resolution images and a larger number of
classes. Table 3 shows that both DARWIN and DARWIN-
LF retain a significant portion of adversarial robustness in-
herited from the teacher model while simultaneously out-
performing other methods on natural performance.
Robust distillation with auxiliary generative data. Aux-
iliary data generated by the Denoising Diffusion Probabilis-
tic Model (DDPM) [20, 46] has been shown to improve ad-
versarial training [10, 38, 40, 55]. Thus, Table 4 provides
the robustness results on CIFAR-10/100 with an additional
1M generated images [40] in the self-distillation setting.
For CIFAR-10, our DARWIN significantly outperforms the
state-of-the-art method on robust accuracy by a large mar-
gin (2%). In addition, DARWIN improves results on natural
examples for both CIFAR-10 and CIFAR-100.
DARWIN with the ViT teacher. Vision TransformersTable 5. Robust accuracy (%) of models distilled from ViT vari-
ants on CIFAR-10 using ResNet-18 and MNV2 student nets.
Type Architecture Method Natural PGD-20 AA
Teacher ViT-B AT-PRM [33] 83.98 53.10 49.66
Student ResNet-18ARD [15] 82.76 52.95 49.03
RSLAD [64] 82.33 54.89 49.74
IAD [63] 82.27 53.42 49.48
CRDND [54] 82.19 53.16 48.98
GACD [1] 81.64 54.24 49.95
DARWIN 83.75 54.80 51.42
DARWIN-LF 83.73 54.95 51.49
Teacher DeiT-S AT-PRM [33] 82.68 52.47 49.27
Student MNV2ARD [15] 81.59 53.45 49.20
RSLAD [64] 80.86 53.91 50.18
IAD [63] 80.41 54.12 49.62
CRDND [54] 80.27 52.21 48.46
GACD [1] 79.97 54.00 48.91
DARWIN 83.02 54.46 51.19
DARWIN-LF 83.15 54.62 51.13
Table 6. Black-box model extraction. WRN-34 teacher pre-trained
on CIFAR-10/CIFAR-100 is extracted into a ResNet-18 student
with the use of CIFAR-10/CIFAR-100/TinyImageNet. We report
the natural accuracy and (Auto-Attack) robust accuracy.
Pre-training
DatasetDistillation
DatasetMethod Natural Robust
CIFAR-10CIFAR-100RSLAD [64] 70.03 37.68
CRDND [54] 69.30 37.29
GACD [1] 69.22 38.05
DARWIN-LF 72.48 41.79
TinyImageNetRSLAD [64] 64.44 30.29
CRDND [54] 64.95 31.83
GACD [1] 65.78 33.12
DARWIN-LF 66.73 36.20
CIFAR-100 CIFAR-10RSLAD [64] 44.41 18.51
CRDND [54] 45.38 18.65
GACD [1] 44.34 18.37
DARWIN-LF 46.29 21.90
(ViT) [13, 49] enjoy good adversarial robustness [2, 26, 31,
33]. Thus, we evaluate DARWIN to see if the intrinsic ro-
bustness of ViTs can be distilled into lighter student models.
Table 5 shows that DARWIN consistently achieves robust
performance that even surpasses the teacher model. Such
an improvement in robustness is achieved without compro-
mising the natural performance. Hence, DARWIN is also
effective in inheriting robustness from ViT-based teachers.
Black-box model extraction via DARWIN. The black-box
model extraction recovers an online black-box model with
no access to its model parameters or training data [37, 50].
Thus, we extract the teacher model (pre-trained on an inac-
cessible source dataset) by DARWIN with a target dataset
that differs from the source dataset. We aim to recover
the natural performance and adversarial robustness of the
black-box teacher. Table 6 shows that DARWIN-LF out-
performs other adversarially robust models in both natural
and adversarial evaluation metrics upon testing the student
on the source test set. Thus, DARWIN-LF is effective in ex-
tracting knowledge of the black-box model. Appendix C.3
contains more details about this problem.
28438
Table 7. Ablation study (WRN-34 →ResNet-18) of three loss com-
ponents of DARWIN for accuracy (%) on CIFAR-10/CIFAR-100.
ARKD IAKD DBKD Natural PGD-20 AA
1✓ 83.09/57.34 53.95/30.59 50.66/25.32
2✓ ✓ 82.74/56.68 54.52/32.11 51.70/26.95
3✓ ✓ 84.68/59.23 54.36/31.53 51.21/26.18
4✓ ✓ ✓ 84.48/59.12 55.07/32.30 52.24/27.26
2 4 6 8 10 12 14 16 18
Iteration Index (i)525456586062646668Robust Accuracy (%)
Teacher
DARWIN (Ours)
RSLAD
IAD
(a)
Radius (255 •ε)86420 101214161.5
1.0
0.5
0.0
-0.5
-1.0
-1.5Difference of Robust Accuracy  (%) (b)
Figure 3. Experiments on CIFAR-10 (WRN-34 →ResNet-18).
(a) Average robust accuracy (PGD-20) w.r.t. the index number
i= 6, . . . , 14of the intermediate adversarial test samples ( α=
1/255, ϵ= 8/255). (b) Difference of AutoAttack robust accuracy
under different attack strengths (radii ϵ) between DARWIN and
RSLAD [64]. Zoom on tiny pictures: ϵ= 0/255is a clean sam-
ple,ϵ= 8/255, andϵ= 16/255are adversarial samples. We used
ϵ=8/255(indicated by the dashed vertical line) for training.
3.2. Ablation Studies
Loss components. Below, we investigate individual loss
components of DARWIN: (i) our baseline of Adversari-
ally Robust Knowledge Distillation (ARKD) in Eq. (3), (ii)
Intermediate Adversarial Knowledge Distillation (IAKD)
in Eq. (4), and (iii) Dual-Branch Knowledge Distillation
(DBKD) in Eq. (8). Table 7 reports accuracy (CIFAR-10)
for both the natural and adversarially robust performance.
Our baseline ARKD-based alignment of the student with
the teacher obtains a competitive performance, and using
intermediate adversarial samples with the IAKD loss further
improves the distillation. The dual-branch module (DBKD)
also boosts the natural performance/adversarial robustness.
Performance w.r.t. the index number i= 1,···, n−1of
the intermediate adversarial test samples. Figure 3a pro-
vides the average robust accuracy w.r.t. the index number on
DARWIN, RSLAD [64], and IAD [63]. DARWIN shows
the largest gain compared to to RSLAD and IAD on early
intermediate adversarial test samples ( e.g.,i=6).
Performance w.r.t. radius ϵ.Figure 3b shows the robust-
ness gap between DARWIN and RSLAD under several at-
tack radii ϵ(training ϵ= 8/255) . DARWIN achieves bet-
ter natural performance and adversarial robustness under
weaker adversaries ( ϵ≤12). In contrast, RSLAD captures
well the adversarial robustness against strong adversarial
perturbations ( ϵ≥12). Thus, DARWIN captures well small
visually undetectable attacks (zoom tiny pictures in Fig. 3b)
whereas RSLAD only handles visually conspicuous attacks.
Performance w.r.t. τbdyvs.Rnat(D).Theorem 3 indicates
TRADES
(Teacher)
DARWIN
(Student)
RSLAD
(Student)
IAD
(Student)
Figure 4. Attention maps of the teacher (WRN-34 obtained by
TRADES [58]) and several students based on ResNet-18. Notice
the similarity of maps of the teacher and DARWIN (student).
Table 8. The boundary risk gain τbdy vs. the natural risk
Rnat(D;θs)for robust self-distillation (ResNet-18, CIFAR-10).
EpochMNV2 ResNet-18
τbdy Rnat(D;θs) ∆ τbdy Rnat(D;θs) ∆
20-th 0.320 0.306 0.014 0.275 0.261 0.014
40-th 0.358 0.284 0.014 0.237 0.242 -0.005
60-th 0.315 0.272 0.043 0.226 0.240 -0.014
80-th 0.309 0.287 0.022 0.235 0.234 0.001
100-th 0.281 0.253 0.028 0.228 0.230 -0.002
that if the boundary risk gain τbdy<Rnat(D), the use of
intermediate adversarial samples Iswith dataset Dmay vi-
olate the minimization of an upper bound of the robust risk.
Table 8 captures if the boundary risk τbdycompensates for
the natural risk Rnat(fθs;D). For MNV2, the assertion
τbdy≥ R nat(D)holds, and thus Table 2 shows a gain in
the natural performance of the student over the teacher. For
ResNet-18, the assertion fails, leading to a slight drop in the
natural performance of the student model (see Table 2).
3.3. Visualization
Figure 4 shows that the student model distilled by DARWIN
shares similar attention (Grad-CAM [44]) regions with the
teacher model, unlike other methods. Thus, DARWIN cap-
tures the complex decision boundaries of the teacher better
than RSLAD/IAD. Appendix F shows more visualizations.
4. Conclusions
We propose a novel adversarially robust knowledge dis-
tillation approach, DARWIN, that efficiently incorporates
dual-branch intermediate adversarial samples into robust-
ness transfer with the goal of capturing the complex de-
cision boundaries of the teacher model. We demonstrate
that our DARWIN benefits from an instance-wise weighting
scheme, and it minimizes an upper bound of the robust risk.
We make a connection between violating such a theoretical
bound and a slight degradation in the natural performance
exhibited by many adversarially robust distillation methods.
Acknowledgments. This work is partially supported by the
National Research Foundation Singapore under its AI Sin-
gapore Program [Award No.:AISG2-GC-2023-007], CFAR,
IHPC, A*STAR, and SCSE at Nanyang Technological Uni-
versity. PK is supported by CSIRO’s Science Digital.
28439
References
[1] Tao Bai, Jun Zhao, and Bihan Wen. Guided adversarial con-
trastive distillation for robust students. IEEE Transactions
on Information Forensics and Security , 2023. 2, 6, 7
[2] Philipp Benz, Soomin Ham, Chaoning Zhang, Adil Karjauv,
and In So Kweon. Adversarial robustness comparison of vi-
sion transformer and mlp-mixer to cnns. In 32nd British Ma-
chine Vision Conference 2021, BMVC , 2021. 7
[3] S ´ebastien Bubeck, Yuanzhi Li, and Dheeraj M Nagaraj. A
law of robustness for two-layers neural networks. In Confer-
ence on Learning Theory , pages 804–820, 2021. 1, 2
[4] Nicholas Carlini and David Wagner. Towards evaluating the
robustness of neural networks. In 2017 ieee symposium on
security and privacy (sp) , pages 39–57. IEEE, 2017. 6
[5] Francesco Croce and Matthias Hein. Reliable evalua-
tion of adversarial robustness with an ensemble of diverse
parameter-free attacks. In International conference on ma-
chine learning , pages 2206–2216. PMLR, 2020. 6
[6] Francesco Croce, Maksym Andriushchenko, Vikash Se-
hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung
Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a
standardized adversarial robustness benchmark. In Proceed-
ings of the Neural Information Processing Systems Track on
Datasets and Benchmarks , 2021. 6
[7] Zhijie Deng, Xiao Yang, Shizhen Xu, Hang Su, and Jun Zhu.
Libre: A practical bayesian approach to adversarial detec-
tion. In IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 972–982, 2021. 2
[8] Junhao Dong, Yuan Wang, Jian-Huang Lai, and Xiaohua
Xie. Improving adversarially robust few-shot image classifi-
cation with generalizable representations. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9025–9034, 2022. 3
[9] Junhao Dong, Junxi Chen, Xiaohua Xie, Jianhuang Lai, and
Hao Chen. Adversarial attack and defense for medical im-
age analysis: Methods and applications. arXiv preprint
arXiv:2303.14133 , 2023. 1
[10] Junhao Dong, Seyed-Mohsen Moosavi-Dezfooli, Jianhuang
Lai, and Xiaohua Xie. The enemy of my enemy is my
friend: Exploring inverse adversaries for improving adver-
sarial training. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
24678–24687, 2023. 7
[11] Junhao Dong, Yuan Wang, Jianhuang Lai, and Xiaohua Xie.
Restricted black-box adversarial attack against deepfake face
swapping. IEEE Transactions on Information Forensics and
Security , 18:2596–2608, 2023. 2
[12] Junhao Dong, Lingxiao Yang, Yuan Wang, Xiaohua Xie,
and Jianhuang Lai. Toward intrinsic adversarial robustness
through probabilistic training. IEEE Transactions on Image
Processing , 32:3862–3872, 2023. 2
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition atscale. In 9th International Conference on Learning Repre-
sentations, ICLR , 2021. 6, 7
[14] Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geip-
ing, Wojciech Czaja, and Tom Goldstein. Adversarial exam-
ples make strong poisons. Advances in Neural Information
Processing Systems , 34:30339–30351, 2021. 4
[15] Micah Goldblum, Liam Fowl, Soheil Feizi, and Tom Gold-
stein. Adversarially robust distillation. In Proceedings of
the AAAI Conference on Artificial Intelligence , pages 3996–
4003, 2020. 2, 6, 7
[16] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. In Interna-
tional Conference on Learning Representations , 2015. 1
[17] Maryam Haghighat, Peyman Moghadam, Shaheer Mo-
hamed, and Piotr Koniusz. Pre-training with random or-
thogonal projection image modeling. In The Twelfth Inter-
national Conference on Learning Representations , 2024. 1
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 1, 6
[19] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 1
[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840–6851, 2020. 7
[21] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In International conference on machine learn-
ing, pages 448–456. pmlr, 2015. 4
[22] Wei Jiang, Zhiyuan He, Jinyu Zhan, Weijia Pan, and Deepak
Adhikari. Research progress and challenges on application-
driven adversarial examples: A survey. ACM Transactions
on Cyber-Physical Systems (TCPS) , 5(4):1–25, 2021. 1
[23] Dahyun Kang, Piotr Koniusz, Minsu Cho, and Naila Murray.
Distilling self-supervised vision transformers for weakly-
supervised few-shot classification & segmentation. In
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-
24, 2023 , pages 19627–19638. IEEE, 2023. 1
[24] Piotr Koniusz, Fei Yan, Philippe-Henri Gosselin, and Krys-
tian Mikolajczyk. Higher-order Occurrence Pooling on Mid-
and Low-level Features: Visual Concept Detection. Techni-
cal report, 2013. 1
[25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 5
[26] Yanxi Li and Chang Xu. Trade-off between robustness
and accuracy of vision transformers. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7558–7568, 2023. 7
[27] Tsung-Yu Lin, Subhransu Maji, and Piotr Koniusz. Second-
order democratic aggregation. In Computer Vision - ECCV
2018 - 15th European Conference, Munich, Germany,
September 8-14, 2018, Proceedings, Part III , pages 639–656.
Springer, 2018. 1
28440
[28] Feng Liu, Bo Han, Tongliang Liu, Chen Gong, Gang Niu,
Mingyuan Zhou, Masashi Sugiyama, et al. Probabilis-
tic margins for instance reweighting in adversarial training.
Advances in Neural Information Processing Systems , 34:
23258–23269, 2021. 3
[29] Changsheng Lu and Piotr Koniusz. Detect any keypoints:
An efficient light-weight few-shot keypoint detector. Pro-
ceedings of the AAAI Conference on Artificial Intelligence ,
38(4):3882–3890, 2024. 1
[30] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learn-
ing models resistant to adversarial attacks. In International
Conference on Learning Representations , 2018. 2, 6
[31] Kaleel Mahmood, Rigel Mahmood, and Marten Van Dijk.
On the robustness of vision transformers to adversarial ex-
amples. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 7838–7847, 2021. 7
[32] Bla ˇz Meden, Peter Rot, Philipp Terh ¨orst, Naser Damer, Ar-
jan Kuijper, Walter J. Scheirer, Arun Ross, Peter Peer, and
Vitomir ˇStruc. Privacy–enhancing face biometrics: A com-
prehensive survey. IEEE Transactions on Information Foren-
sics and Security , 16:4147–4183, 2021. 1
[33] Yichuan Mo, Dongxian Wu, Yifei Wang, Yiwen Guo, and
Yisen Wang. When adversarial training meets vision trans-
formers: Recipes from training to architecture. Advances in
Neural Information Processing Systems , 2022. 7
[34] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
Jonathan Uesato, and Pascal Frossard. Robustness via
curvature regularization, and vice versa. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9078–9086, 2019. 3
[35] Yao Ni and Piotr Koniusz. NICE: NoIse-modulated Consis-
tency rEgularization for Data-Efficient GANs. In Advances
in Neural Information Processing Systems , pages 13773–
13801. Curran Associates, Inc., 2023. 1
[36] Yao Ni and Piotr Koniusz. CHAIN: Enhancing Generaliza-
tion in Data-Efficient GANs via lipsCHitz continuity con-
strAIned Normalization. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, CVPR 2024 . IEEE,
2024. 1
[37] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.
Knockoff nets: Stealing functionality of black-box models.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4954–4963, 2019. 7
[38] Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Reduc-
ing excessive margin to achieve a better accuracy vs. robust-
ness trade-off. In The Tenth International Conference on
Learning Representations, ICLR , 2022. 2, 7
[39] Saimunur Rahman, Piotr Koniusz, Lei Wang, Luping Zhou,
Peyman Moghadam, and Changming Sun. Learning partial
correlation based deep visual representation for image classi-
fication. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR 2023, Vancouver, BC, Canada,
June 17-24, 2023 . IEEE, 2023. 1
[40] Sylvestre-Alvise Rebuffi, Sven Gowal, Dan Andrei Calian,
Florian Stimberg, Olivia Wiles, and Timothy A Mann. Data
augmentation can improve robustness. Advances in Neural
Information Processing Systems , 34:29935–29948, 2021. 7[41] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. Advances in neural information process-
ing systems , 28, 2015. 1
[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115:211–252, 2015. 5
[43] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 4510–4520, 2018. 6
[44] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
Grad-cam: Visual explanations from deep networks via
gradient-based localization. In IEEE International Confer-
ence on Computer Vision , pages 618–626, 2017. 8
[45] Fatemeh Shiri, Xin Yu, Fatih Porikli, Richard Hartley, and
Piotr Koniusz. Recovering faces from portraits with auxil-
iary facial attributes. In 2019 IEEE Winter Conference on
Applications of Computer Vision (WACV) , pages 406–415,
2019. 1
[46] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning , pages 2256–2265, 2015. 7
[47] Liwei Song, Reza Shokri, and Prateek Mittal. Privacy risks
of securing machine learning models against adversarial ex-
amples. In ACM SIGSAC Conference on Computer and
Communications Security , pages 241–257, 2019. 2
[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus.
Intriguing properties of neural networks. In International
Conference on Learning Representations, ICLR , 2014. 1
[49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Training
data-efficient image transformers & distillation through at-
tention. In International conference on machine learning ,
pages 10347–10357. PMLR, 2021. 6, 7
[50] Florian Tram `er, Fan Zhang, Ari Juels, Michael K Reiter,
and Thomas Ristenpart. Stealing machine learning models
via prediction {APIs}. In25th USENIX security symposium
(USENIX Security 16) , pages 601–618, 2016. 7
[51] Lei Wang and Piotr Koniusz. Uncertainty-dtw for time se-
ries and sequences. In Computer Vision - ECCV 2022 - 17th
European Conference, Tel Aviv, Israel, October 23-27, 2022,
Proceedings, Part XXI , pages 176–195. Springer, 2022. 1
[52] Lei Wang and Piotr Koniusz. Temporal-viewpoint trans-
portation plan for skeletal few-shot action recognition. In
Asian Conference on Computer Vision , page 307–326,
Berlin, Heidelberg, 2023. Springer-Verlag. 1
[53] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun
Ma, and Quanquan Gu. Improving adversarial robustness
requires revisiting misclassified examples. In International
Conference on Learning Representations , 2020. 2
28441
[54] Yuzheng Wang, Zhaoyu Chen, Dingkang Yang, Yang Liu,
Siao Liu, Wenqiang Zhang, and Lizhe Qi. Adversarial con-
trastive distillation with adaptive denoising. In ICASSP
2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages 1–5. IEEE,
2023. 2, 6, 7
[55] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu,
and Shuicheng Yan. Better diffusion models further improve
adversarial training. In International Conference on Machine
Learning, ICML , pages 36246–36263, 2023. 7
[56] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and
Alan Yuille. Mitigating adversarial effects through random-
ization. In International Conference on Learning Represen-
tations , 2018. 2
[57] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. In Proceedings of the British Machine Vision Confer-
ence, 2016. 6
[58] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Lau-
rent El Ghaoui, and Michael Jordan. Theoretically principled
trade-off between robustness and accuracy. In International
conference on machine learning , pages 7472–7482. PMLR,
2019. 2, 5, 6, 7, 8
[59] Hongguang Zhang, Limeng Zhang, Yuchao Dai, Hongdong
Li, and Piotr Koniusz. Event-guided multi-patch network
with self-supervision for non-uniform motion deblurring.
Springer International Journal of Computer Vision (IJCV) ,
131:453–470, 2023. 1
[60] Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi
Sugiyama, and Mohan Kankanhalli. Geometry-aware
instance-reweighted adversarial training. In International
Conference on Learning Representations , 2021. 3
[61] Zhongyan Zhang, Lei Wang, Luping Zhou, and Piotr Ko-
niusz. Learning spatial-context-aware global visual feature
representation for instance image retrieval. In IEEE/CVF
International Conference on Computer Vision, ICCV 2023,
Paris, France, October 1-6, 2023 , pages 11216–11225.
IEEE, 2023. 1
[62] Shiji Zhao, Jie Yu, Zhenlong Sun, Bo Zhang, and Xingxing
Wei. Enhanced accuracy and robustness via multi-teacher
adversarial distillation. In European Conference on Com-
puter Vision , pages 585–602. Springer, 2022. 2
[63] Jianing Zhu, Jiangchao Yao, Bo Han, Jingfeng Zhang,
Tongliang Liu, Gang Niu, Jingren Zhou, Jianliang Xu, and
Hongxia Yang. Reliable adversarial distillation with unre-
liable teachers. In The Tenth International Conference on
Learning Representations, ICLR , 2022. 2, 6, 7, 8
[64] Bojia Zi, Shihao Zhao, Xingjun Ma, and Yu-Gang Jiang. Re-
visiting adversarial robustness distillation: Robust soft labels
make student better. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 16443–
16452, 2021. 2, 6, 7, 8
28442
