TASK2BOX: Box Embeddings for Modeling Asymmetric Task Relationships
Rangel Daroya Aaron Sun Subhransu Maji
University of Massachusetts, Amherst
{rdaroya, aaronsun, smaji }@umass.edu
Class Arachnida 
Family 
Buthidae Family 
Vaejovidae Family 
Ixodidae 
Family 
Pisauridae Family 
Oxyopidae Family 
Salticidae Order Araneae Order Scorpiones Order Ixodida Class Arachnida 
Order 
Araneae Order 
Scorpiones 
Family 
Vaejovidae Family 
Buthidae Family 
Oxyopidae Family 
Pisauridae Family 
Salticidae Order 
Ixodida 
Family 
Ixodidae Order 
Scorpiones 
Order 
Araneae 
Order 
Ixodida 
AA B
C
BC(1) Learned Task Embeddings (2) Learned hierarchy for Class Arachnida 
Figure 1. Box Embeddings of 150 Datasets of iNaturalist + CUB and Corresponding Learned Hierarchy for Class Arachnida . Each
taxonomic category is treated as a separate dataset for which T ASK2BOXembeddings are learned. (1) Shows the learned box embeddings
where datasets from the same group (taxonomic class) have the same color. Datasets naturally cluster to their ground truth groups. (2)
Shows the hierarchy learned through T ASK2BOXfor a specific class. The hierarchy matches the ground truth relationships based on
biological classification. Orders that belong to class Arachnida are learned as boxes ( A,B,C) contained by the larger box for
Arachnida; families under each of the orders are learned as smaller boxes contained by the corresponding orders they belong to.
Abstract
Modeling and visualizing relationships between tasks
or datasets is an important step towards solving various
meta-tasks such as dataset discovery, multi-tasking, and
transfer learning. However, many relationships, such as
containment and transferability, are naturally asymmetric
and current approaches for representation and visualiza-
tion (e.g., t-SNE [44]) do not readily support this. We
propose TASK2BOX, an approach to represent tasks us-
ing box embeddings—axis-aligned hyperrectangles in low
dimensional spaces—that can capture asymmetric relation-
ships between them through volumetric overlaps. We show
that TASK2BOXaccurately predicts unseen hierarchical
relationships between nodes in ImageNet and iNaturalist
datasets, as well as transferability between tasks in the
Taskonomy benchmark. We also show that box embed-
dings estimated from task representations (e.g., CLIP [36],
Task2Vec [4], or attribute based [15]) can be used to pre-
dict relationships between unseen tasks more accurately
than classifiers trained on the same representations, as well
as handcrafted asymmetric distances (e.g., KL divergence).This suggests that low-dimensional box embeddings can
effectively capture these task relationships and have the
added advantage of being interpretable. We use the ap-
proach to visualize relationships among publicly available
image classification datasets on popular dataset hosting
platform called Hugging Face.
1. Introduction
The success of deep learning has led to the proliferation of
datasets for solving a wide range of computer vision prob-
lems. Yet, there are few tools available to enable practition-
ers to find datasets related to the task at hand, and to solve
various meta-tasks related to it. We present T ASK2BOX, a
method to represent tasks using axis-aligned hyperrectan-
gles (or box embeddings). T ASK2BOXis framed as a learn-
able mapping from dataset representation to boxes, and can
be trained to predict various relationships between novel
tasks such as transferability, hierarchy, and overlap.
Box embeddings [48] extend order embeddings [46] by
using volumetric relationships between axis-aligned hyper-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28827
rectangles to encode pairwise relationships. Prior work in
natural language processing has utilized box embeddings to
represent the WordNet [30] hierarchy and to model condi-
tional distributions. To model relationships between novel
datasets, we develop a technique to map from Euclidean
representations of datasets into the space of boxes. We ex-
plore simple image and label embedding from large vision-
language models such as CLIP [36], T ASK2VEC[4], and
attribute-based vectors [15] as base representations of tasks.
We test our framework to model asymmetric relation-
ships between nodes in iNaturalist [45] and Caltech-UCSD
Birds (CUB) [50] and ImageNet [11] datasets, as well as to
predict transferability on the Taskonomy benchmark [52].
Table 1 and 2 show that low-dimensional box embeddings
accurately predict novel relationships between datasets seen
during training, as well as relationships with novel datasets.
Remarkably, T ASK2BOXoutperforms classifiers trained to
directly predict the relationships on the same representa-
tions, suggesting that the box embedding provides a strong
inductive bias for learning hierarchical relationships. We
also outperform simple asymmetric distances proposed in
prior work such as Kullback-Leibler (KL) divergence [4].
To model the heterogeneous tasks in the Taskonomy bench-
mark [52] we map each task to a set of attributes from which
a box embedding is learned. Once again, we obtain signif-
icantly higher correlation between the true and predicted
transferability for both existing and novel datasets com-
pared to standard classifiers (Table 3). Such attribute-based
representations can be readily derived from datasheets [15]
and modelcards [31].
Finally, the low-dimensional box embeddings have the
added advantage of being interpretable. Fig. 1 and Fig. 3
show relationships on the iNaturalist+CUB and ImageNet
categories, respectively. The 2D box representation allows
us to readily visualize the strength and direction of task
relationships based on the overlapping volumes, which is
not possible using symmetric distances with Euclidean rep-
resentations (e.g., t-SNE [44]). At the same time, new
datasets can be embedded in constant time without needing
to retrain or re-optimize. Fig. 5 uses T ASK2BOXto visual-
ize relationships among 131 publicly available datasets on
Hugging Face [1], a popular platform for hosting datasets.
Our main contributions are as follows:
• We introduce a novel method (T ASK2BOX) that uses box
embeddings to learn asymmetric (e.g., hierarchical, trans-
fer learning) dataset relationships.
• We demonstrate that T ASK2BOXcan predict the relation-
ships of new tasks with a collection of existing tasks.
• We illustrate the interpretability of our model, and the
ability to visualize public classification datasets on Hug-
ging Face.
The code for this project is publicly available at https:
//github.com/cvl-umass/task2box .2. Related Work
Task Representations. Given a dataset D={(xi, yi)}n
i=1,
consisting of images xi∈ X and labels yi∈ Y, a range of
approaches have been proposed for dataset representation.
The most straightforward approach involves modeling the
distribution of either the images xor the labels ywithin the
dataset independently, using embeddings referred to in prior
work as “domain” and “label” embeddings [4]. To capture
the joint dependency between images and labels, [4] pro-
posed the use of the Fisher Information Matrix (FIM) de-
rived from a “probe network” trained to minimize a loss
function ℓ(ˆy, y)over the dataset [22, 25, 35]. This approach
leverages the similarity of FIMs to predict task transferabil-
ity and for model selection. However, the utility of the FIM
critically depends on the choice of the probe network and a
pre-defined similarity may not accurately represent the var-
ious relationships between datasets.
We also investigate the use of vision-language mod-
els (VLMs) such as CLIP [36]. This model, trained on
a wide range of visual domains, can generalize to tasks
involving vision and language data. CLIP features have
been effective for image classification [3, 9], semantic
segmentation [18, 24, 26], object detection [16, 47, 51],
and even closing domain gaps for performance improve-
ment [23, 53]. Both images x∈ X and labels y∈ Y repre-
sented as text, can be mapped into a shared space using the
vision encoder ( ϕ) and text encoder ( ψ) of CLIP, allowing
us to model the dataset as a set of image and label embed-
dings{ 
ϕ(xi), ψ(yi)
}n
i=1.
We compare FIMs with representations derived from
CLIP as base representations for tasks and learn box
embeddings to model a variety of relations among tasks.
Task Relations in Computer Vision. Understanding the
relationships between tasks can lead to efficient solutions
to new tasks. Previous work has measured task similarity
by using model gradients [14] or based on their learned
features [21] for grouping tasks for efficient multi-tasking.
Similarly, predicting which pre-trained models will gen-
eralize the best on a new dataset could streamline model
selection. Taskonomy [52] investigates transfer learning
across vision tasks, varying from segmentation to pose
estimation, by computing pairwise transfer distances or
task affinities. These affinities are calculated by evaluating
the extent to which a model trained on a source task
generalizes to a target task [13, 42], though this process is
computationally expensive.
Dataset Visualization. Low-dimensional Euclidean em-
beddings derived from UMAP [28], t-SNE [44], and
LargeVis [43] are widely used to visualize relationships be-
tween datasets. They have been shown to successfully re-
cover clusters of various data modalities by preserving the
28828
relationship of each data point with its neighbors [4, 5, 40].
In low-dimension space, relationships with other data points
are defined by their Euclidean distances. However, these are
commonly used to represent symmetric relations.
Visualizations using tidy trees [37], circle packing [49],
or cone trees [29] organize asymmetric relations as tree-
structured hierarchies in low dimension. However, cyclic
data relationships cannot be properly represented for these
methods (e.g., when a node has two or more parents).
Asymmetric Distances over Datasets. Kullback-Leibler
(KL) divergence between image or label distributions pro-
vides a natural way to represent asymmetric distances be-
tween datasets. T ASK2VEC[4] computes the similarity
between two tasks (e.g., cosine distance), and introduces
asymmetry by using the complexity of the first task as a ref-
erence. The complexity is measured by the similarity of the
task embedding to a “trivial embedding” (embedding of a
task that is easy or has no examples).
Order embeddings on images were first proposed in [46]
to capture tree-structured relationships. Given an dataset of
P= (u, v)drawn from an partially ordered set (X,⪯X),
they frame the problem as learning a mapping f: (X,⪯X
)→(Y,⪯Y)that is order preserving, i.e., u⪯Xv⇐⇒
f(u)⪯Yf(v). The reserved product order was used for
⪯Y, i.e., x⪯y⇐⇒ xi> yi,∀i. Box embeddings [8]
generalized this framework by representing points as axis-
aligned hyper-rectangles and using volumetric relationships
(e.g., intersection over union) to represent asymmetric rela-
tions. They used the framework to model conditional distri-
butions and hypernymy relations (e.g., dog “is a” mammal)
on the WordNet graph [2, 19, 33, 38, 48].
Hyperbolic spaces provide yet another way to model
asymmetric relationships. Examples include the Poincare
disk model which uses hyperbolic cosine (cosh) to mea-
sure distance between points in a disk. Poincare embed-
dings have been similarly used to represent WordNet hierar-
chies [32] and other relations in general graphs. Hyperbolic
representations have also been proposed for representing
images for efficient zero-shot learning given a taxonomic
structure of the labels [27].
To the best of our knowledge, no prior work has explored
the use of these spaces for representing entire datasets and
their effectiveness in capturing various task relationships.
We adopt box embeddings in this work due to the effective-
ness over alternatives in previous work [2, 6, 34, 38], ease
of visualization, as well as due to open-source libraries for
robust learning. However, instead of learning box embed-
dings directly, we learn mappings from task representations.
3. T ASK2BOXFramework
We define the problem as follows: given a collection
of datasets {D1,D2, . . . ,Dm}, and an asymmetric rela-tionship given the pairwise relationship between datasets
d(Di,Dj)∈[0,1], we aim to encode each dataset into
a low-dimension space that preserves the relationships be-
tween datasets and is interpretable.
To achieve this, we propose using box embeddings for
encoding each of the datasets. This process involves two
main steps: (1) deriving the base representation eof each
dataset, and (2) learning a model fθ:e→z, where
z∈R2×krepresents a k-dimensional axis-aligned hyper-
rectangle (i.e., box), denoted by its lower left and upper
right coordinates. These steps are detailed further below.
3.1. Base Task Representations
Each dataset D={(xi, yi)}n
i=1is defined as a collection
of pairs of images xi∈ X and labels yi∈ Y. For obtaining
a base embedding efor each dataset, we utilize methods
such as CLIP [36, 41], T ASK2VEC[4], or attribute-based
approaches [15].
CLIP. Using a pre-trained CLIP model [7, 20, 36, 41], we
compute the mean and variance of the individual sample
embeddings within each dataset. For each data sample,
the image embedding is concatenated with the label em-
bedding, the latter generated from text prompts (e.g., “A
photo of [CLS]”). This concatenation models the joint dis-
tribution of images and labels. Eq. 1 and 2 detail how the
mean and variance embeddings are derived, where [i, j]rep-
resents the concatenation of vectors iandj,ϕis the vision
encoder, and ψis the text encoder. The covariance is ap-
proximated as diagonal for tractability.
µCLIP =1
NNX
i=1[ϕ(xi), ψ(yi)] (1)
σ2
CLIP =1
NNX
i=1([ϕ(xi), ψ(yi)]−µCLIP )2(2)
The base representation is defined as e:=µCLIP∈R2048
ore:= [µCLIP , σ2
CLIP ]∈R4096. A ViT-H/14 [12]
pretrained on LAION-2B [41] was used to extract the
embeddings.
TASK2VEC[4] encodes a dataset using the approximate
Fisher Information Matrix (FIM) of a network trained on
the given dataset. The FIM represents the importance of pa-
rameters in the feature extractor by perturbing the weights ˆw
of a given probe network with Gaussian noise N(0,Λ). The
precision matrix Λis estimated to be close to an isotropic
priorN( ˆw, λ2I)while having a good expected error. Eq. 3
is minimized to find Λwhere His the cross entropy loss,
ˆware the weights of the network, βis the magnitude of the
prior, x∈ X, and y∈ Y.
28829
L( ˆw; Λ) = Ew∼N( ˆw,Λ)[Hpw,ˆpp(y|x)]+
βKL 
N(0,Λ)∥N(0, λ2I)
(3)
The matrix Λprovides an estimate of the FIM and
is approximated as a diagonal matrix. The diagonal
components are used as a base representation of a dataset
(e:=FIM∈R17024). ResNet-34 [17] pretrained on
ImageNet [11] is used as the probe network for all datasets.
Attribute-based. A task can be characterized by a set of t
binary attributes [15] represented as a vector of dimension
t. Some of the attributes explored for representing tasks are:
(1) Is the task generative? (2) Is the task output in 2D? (3)
Does the task involve camera pose estimation? Taking these
3 characteristics, for example, we can represent a 2D seg-
mentation task as the vector e= [0,1,0]as a discriminative
2D task that does not need camera poses.
Tasks in Taskonomy [52] involving multiple modalities
benefit from this attribute-based representation due to its
model independence. This approach also enables general-
ization to unseen tasks by identifying the presence or ab-
sence of various characteristics. Each vision task in Taskon-
omy is represented with 15 attributes, resulting in vectors
e∈R15. The full list of attributes is in Appendix ??.
3.2. Learning Box Embeddings
From a base task representation e, we learn the parameters
θof a model fθ:e→zthat preserves the asymmetric sim-
ilarity function d(Di,Dj)between any two datasets Diand
Dj. In Eq. 4, the learning objective is shown where LEis a
loss function (mean squared error), LDis a distance func-
tion,LRis a regularization term, and λis a hyperparameter.
The embeddings zi, zj∈R2×krepresent the coordinates of
the lower left and the upper right corners of the respective k
dimemsional boxes, with fθ(ei) =ziandfθ(ej) =zj.
ˆθ= argmin
θX
i,j(LE(d(Di,Dj), dbox(zi, zj))
+λLD(zi, zj) +LR)(4)
The asymmetric relationship between box embeddings,
denoted as dbox(zi, zj), is computed in Eq. 5 by calculating
the volume of the intersection between ziandzj, normal-
ized by the volume of zi. Forzito be fully contained inside
zj(zi⊂zj), is it required that dbox(zi, zj) = 1 . Con-
versely, for zjto only partially contain zi,dbox(zj, zi)must
fall within the range (0,1). Fig. 2 illustrates this through
a 2-dimensional example, where z1represents the box em-
bedding for Canidae, and z2for Mammalia.
1
 0
T ask2Box Dim 1123T ask2Box Dim 2Mammalia
Canidae
AmphibiaFigure 2. TASK2BOXEmbeddings in 2D for Mammalia,
Canidae, and Amphibia Datasets from iNaturalist . Each em-
bedding represents the coordinates of the lower left and upper right
corners of each box/rectangle. Since Canidae ( z1) is a proper sub-
set of Mammalia ( z2):dbox(z1, z2) = 1 anddbox(z2, z1) = 0 .1.
dbox(zi, zj) =vol(zi∩zj)
vol(zi)(5)
LDis applied to datasets where d(Di,Dj)>0such that
the Euclidean distance between the center coordinates of zi
andzjis minimized when starting from a non-overlapping
state. This allows non-overlapping embeddings to move
closer to each other, complementing LEto learn relation-
ships. LRencourages solutions with regular-shaped boxes
for better interpretability. The formulation of LR, given in
Eq. 6, applies to a k-dimensional box embedding zi, where
sarepresents the size of the a-th dimension (for example,
width), and α, β are hyperparameters. To prevent the triv-
ial solution of minimizing box volume to zero, the inverse
of the box volume is included. It’s crucial to normalize the
terms with respect to the embedding dimension, as the first
and second terms scale quadratically and exponentially with
dimension increase, respectively.
LR= 
α
k2kX
a=1kX
b=a+1|sa−sb|!
+β(vol(zi))−1/k(6)
The architecture of fconsists of three fully-connected
layers followed by two linear heads: one predicts a k-
dimensional lower-left coordinate, and the other predicts the
k-dimensional sizes of each box dimension.
4. Experiments
We consider the following goals to evaluate the capability
of T ASK2BOXto represent datasets:
1. Given a collection of existing datasets DEand a subset
of pairwise relationships R: can the model generalize on
unseen relationships R′within DEwhere R′∩ R=∅?
2. Given a collection of novel datasets DNnot seen during
training: can the model accurately identify the relation-
ships with the existing datasets DE?
28830
iNaturalist + CUB
MethodExisting Datasets Novel Datasets
µCLIP [µ, σ2]CLIP FIM µCLIP [µ, σ2]CLIP FIM
TASK2BOX(2D) 69.23% 67.84% 39.61% 50.07% 39.66% 10.06%
TASK2BOX(3D) 79.66% 79.35% 57.63% 70.04% 64.53% 20.65%
TASK2BOX(5D) 84.67% 82.41% 79.72% 73.79% 72.11% 34.88%
MLP Classifier 45.25% 61.45% 26.34% 39.06% 44.54% 19.90%
Linear Classifier 4.40% 3.11% 7.06% 4.77% 5.87% 15.92%
KL Divergence - 6.58% 7.94% - 5.90% 0.00%
Asymmetric Cosine 9.29% 11.54% 2.83% 1.47% 1.47% 1.47%
Asymmetric Euclidean 1.71% 1.71% 8.53% 1.47% 1.47% 1.91%
Random 2.06% 1.49%
Table 1. Average F1 Score for Predicting Hierarchical Relationships on iNaturalist + CUB Dataset. For all feature types, T ASK2BOX
outperforms other methods by more than 20% on existing datasets and more than 10% on novel datasets. The best-performing model is in
bold , and the second-best is underlined . Results on Novel Datasets demonstrate that our model can generalize beyond the seen tasks it has
previously seen. The dimensions such as 2D, 3D, and 5D refer to different box dimensionalities. Results using KL divergence for µCLIP
are not shown since it is not a distribution.
The goals are evaluated through two experimental setups,
demonstrating our model’s ability to predict various types
of relationships. The configurations, and their correspond-
ing datasets, relationships, and metrics, are detailed below.
Baseline methods for performance comparison are also re-
viewed. Implementation details are in the Appendix.
4.1. Experimental Setup
4.1.1 Hierarchical Task Relationships
We use a combination of iNaturalist [45] and Caltech-
UCSD Birds (CUB) [50], and instrument-related classes
in ImageNet [11] to evaluate the ability of T ASK2BOXto
represent hierarchical relationships. The first two are com-
posed of images of various species, and the third is com-
posed of images of various objects. The classes naturally
follow a hierarchical form based on biological classification
(taxonomy of iNaturalist+CUB species), and on semantic
relations (hypernymy of objects in ImageNet).
Datasets. For iNaturalist+CUB, the datasets are defined as
the classes, the orders, and the families in the taxonomy
that contain a significant number of samples per dataset as
in [4]. There are 47 classes, 202 orders, and 589 families
for a total of 838 datasets. For ImageNet, the instrument-
related objects were processed using WordNet [30] to obtain
hierarchical information between classes. This resulted in
131 datasets for training and evaluation.
Dataset Relationships. For any two datasets, their relation-
ship is captured as d(Di,Dj)∈ {0,1}, where d(Di,Dj) =
1if and only if Di⊂ D j, and d(Di,Dj) = 0 other-
wise. Fig. 2 provides an example: Canidae ( D1) is a family
within the class Mammalia ( D2); thus, d(D1,D2) = 1 and
d(D2,D1) = 0 . Meanwhile, Amphibia ( D3) is unrelated to
either dataset, resulting in d(D1,D3) = 0Evaluation Metrics. We evaluate the ability of the model
to classify the presence of containment relationships be-
tween datasets using the F1 score due to the imbalance
between positive and negative relationships. To obtain F1
score, precision and recall are first calculated. Precision
is computed as the ratio of true positive pairs predicted to
the total number of positive predictions. Recall is the ra-
tio of true positive pairs predicted to the total number of
true positive pairs. F1 score is then reported as the har-
monic mean between the precision and recall. These cal-
culations are done on both (1) the unseen relationships R′
within existing datasets DEand (2) the relationships be-
tween novel datasets DNand existing datasets DE. For the
latter, we evaluate the relationships in both directions, i.e.,
{(De,Dn)∀ De∈ DE} ∪ {(Dn,De)∀ De∈ DE}.
4.1.2 Transfer Learning Between Datasets
The Taskonomy [52] benchmark is used to evaluate the abil-
ity of T ASK2BOXto predict task affinities.
Datasets. Taskonomy defines a set of 25 visual tasks with
corresponding pairwise task affinities. These tasks range
from object detection, semantic segmentation, pose estima-
tion, and more. We treat each visual task as a dataset. The
tasks are described in Appendix ??.
Dataset Relationships. In contrast to the containment rela-
tionship defined in § 4.1.1, Taskonomy quantifies relation-
ships with task affinity measured by the performance gain
achieved by transfer learning from a source dataset Djto
a target dataset Di. The values are computed using Ana-
lytic Hierarchy Process [39, 52]. Dataset relationships are
computed and normalized based on an ordinal approach and
determined by the percentage of images that transfer well to
a target task given a set of source tasks as detailed in [52].
28831
ImageNet
MethodExisting Datasets Novel Datasets
µCLIP [µ, σ2]CLIP FIM µCLIP [µ, σ2]CLIP FIM
TASK2BOX(2D) 62.72% 63.33% 31.32% 47.76% 48.35% 9.46%
TASK2BOX(3D) 83.12% 82.79% 58.16% 73.06% 66.48% 24.70%
TASK2BOX(5D) 90.58% 88.48% 64.91% 76.95% 78.84% 37.39%
MLP Classifier 54.20% 60.43% 44.85% 62.24% 64.56% 41.22%
Linear Classifier 9.84% 8.33% 25.46% 11.89% 11.87% 22.98%
KL - 5.53% 9.90% - 10.41% 0.00%
Asymmetric Cosine 4.28% 4.28% 6.92% 4.52% 4.52% 0.00%
Asymmetric Euclidean 3.73% 3.73% 7.16% 4.52% 4.52% 4.73%
Random 3.64% 5.02%
Table 2. Average F1 Score for Predicting Hierarchical Relationships on ImageNet Instruments . TASK2BOXis shown to generalize
well on both predicting relationships between existing datasets ( Existing Datasets ), and predicting box embeddings of new datasets and
their relationships with existing datasets ( Novel Datasets ).
For a pair of datasets (Di,Dj), the task affinity is defined as
d(Di,Dj)∈[0,1]. The higher the task affinity to a target
dataset Difrom a source dataset Dj, then d(Di,Dj)gets
closer to 1, where a value of 1 would show Di⊂ D j.
Evaluation Metrics. We evaluate the prediction of the
model based on the Spearman correlation between the
ground truth task affinity values, and the predicted values
based on the box distances in Eq. 5. Similar to § 4.1.1,
we evaluate on both (1) unseen relationships R′for exist-
ing datasets, and (2) on relationships between unseen novel
datasets DNand existing datasets DE.
4.2. Baseline Methods
We compare the performance of T ASK2BOXwith alter-
native models and simple asymmetric distances proposed
in prior work. For hierarchical relationships, given two
datasets Di,Dj, the models predict ˆd(Di,Dj)∈ {0,1}. For
task affinity, the models predict ˆd(Di,Dj)∈[0,1]. The
structure for the various methods are discussed below.
Linear Model. A linear model is trained to predict the re-
lationship value between two datasets. The input is the con-
catenation of the base representation efor the two datasets.
MLP Model. A 4-layer MLP is used instead for prediction
on the same inputs as the linear model.
KL Divergence. Each dataset is treated as a multivariate
Gaussian using the mean and variance of the image and
label features (CLIP) or directly as the FIM. The optimal
threshold tis selected for the minimum distance between
two dataset distributions KL (Di||Dj)< tfor a prediction
ofˆd(Di,Dj) = 1 , and ˆd(Di,Dj) = 0 otherwise. The F1
Score (hierarchical) or the correlation (task affinity) is the
objective for selecting ton the train set.
Asymmetric Cosine Similarity. The cosine similarity dcos
is a symmetric measure between two embeddings ei, ej. An
asymmetric variant was proposed in [4] and shown in Eq. 7
device
containerequipment
implementconveyance
furnishingtoiletry
systemFigure 3. Visualization of Instrument-related Datasets in Ima-
geNet . Datasets that belong to the same superset are shaded in the
same color. T ASK2BOXlearns the hierarchy of various groups,
and clusters similar datasets closer.
by considering the similarity of eiandejrelative to the
complexity of ei. The complexity of is measured as the dis-
tance to the trivial embedding eo, andαis a hyperparameter.
dasym(ei, ej) =dcos(ei, ej)−αdcos(ei, eo) (7)
An optimal threshold tis found on the train set where
dasym(ei, ej)< t results to a prediction ˆd(Di,Dj) = 1 .
The same threshold tis used for test set evaluation.
Asymmetric Euclidean Similarity. The asymmetric sim-
ilarity is computed as in Eq. 7 but uses the Euclidean dis-
tance instead of cosine.
Random. The probability of containment (for hierarchical)
or the value of the task affinity is uniformly random.
5. Results and Discussion
5.1. Hierarchical Relationships
Tables 1 and 2 present the average F1 score for predict-
ing hierarchical relationships on iNaturalist+CUB and Ima-
28832
geNet datasets. Various methods of extracting dataset em-
beddings were evaluated. T ASK2BOXoutperforms base-
line methods, indicating that T ASK2BOXcan both general-
ize across unseen relationships (Existing Datasets), and to
accurately represent relationships with existing datasets for
unseen datasets (Novel Datasets). CLIP features also gen-
eralize better than FIM features, perhaps because the CLIP
multi-modal embedding of images and labels was trained
across a broad set of domains[36]. However, while CLIP
is confined to image and text modalities, FIM can accom-
modate any modality by adapting the probe network’s last
layer for different prediction tasks.
Beyond predicting relationships, our method allows the
visualization of datasets. Fig. 1 illustrates this for a subset
of datasets in iNaturalist and CUB using T ASK2BOX(2D),
and Fig. 3 for ImageNet. The hierarchical organization of
the datasets is apparent in the T ASK2BOXrepresentation –
datasets that contain others appear as larger boxes, while
more specialized datasets are depicted as smaller boxes
neted with borader, more general dataset boxes. However,
while 2D visualization offers insights, the flat surface may
restrict the representation of complex relationships. Repre-
sentations in higher dimensions is explored in § 5.4.
5.2. Task Affinity
Table 3 presents the results of task representations in
Taskonomy. Our method show that, even with attribute-
based embeddings, it can learn box embeddings through
relationship supervision between datasets. T ASK2BOXis
shown to correlate highly with the ground truth task affini-
ties compared to other methods. Given that only 25 tasks are
available in Taskonomy, with 3 held out of training as novel
datasets, there is a higher uncertainty in predicting unseen
datasets. We expect that performance on novel datasets will
improve with more datasets available during training.
Fig. 4 displays the learned representation with
TASK2BOX(2D). Each subfigure in (a)-(c) represents
a subset of tasks identified as having strong transfer rela-
tionships. T ASK2BOXnot only identifies but also visually
represents related tasks suitable for fine-tuning. The small
highlighted boxes indicate target tasks, while the larger
enclosing boxes represent source tasks. Although other
tasks may not be proper subsets, the box distance in Eq. 5
provides an estimate for task affinity. A small box distance
between two datasets, dbox(z1, z2), suggests lower transfer
performance from a source task z2to a target task z1.
5.3. Visualizing Public Datasets
Apart from predicting both hierarchical relationships and
quantified task affinities, our method can also be used to
visualize public datasets that lack available ground truth
relationships. Using a set of vision tasks from Hugging
Face [1], µCLIP + TASK2BOX(2D) were utilized to pre-
DepthJigsaw-Puzzle
Triplet-Fixated-Camera-Pose
(a)
Jigsaw-PuzzleColorization
Denoising
Auto-
encoderInpaintingKeypoint
2D
(b)Jigsaw-Puzzle
Reshading
Pairwise-Fixated-
Camera-PoseSurface
Normal
(c)Figure 4. Visualization of Tasks in Taskonomy showing Source
Tasks that Transfer Well to Target Tasks (shaded) . (a) Jigsaw
and Triplet Fixated Camera Pose estimation are source tasks that
transfer well to Depth estimation. (b) and (c) show different source
tasks (larger boxes) that transfer well to the shaded boxes of De-
noising Autoencoder and Surface Normal, respectively.
MethodExisting Datasets Novel Datasets
Spearman’s ρ Spearman’s ρ
TASK2BOX(2D) 0.85±0.06 0.12±0.21
TASK2BOX(3D) 0.93±0.02 0.48±0.24
TASK2BOX(5D) 0.94±0.03 0.39±0.22
MLP 0.88±0.06 0.31±0.18
Linear 0.75±0.11 0.40±0.24
Random 0.05±0.14 0.15±0.07
Table 3. Spearman Correlation and Standard Deviation be-
tween Predicted and Ground Truth Task Affinities on Taskon-
omy. Our method shows higher correlation with the task affinities
compared to the baseline. Attribute-based embeddings were used.
dict dataset embeddings. A constraint on the box sizes was
added to reflect information about the dataset sizes. Fig. 5
displays the results of T ASK2BOXon 131 datasets from
Hugging Face, where similar datasets, such as sentiments
and documents, are shown to overlap. This approach allows
for the analysis and visualization of dataset variations and
similarities even with only samples of images and labels.
Simultaneously, the sizes of various datasets can be visu-
alized as box sizes – with larger datasets containing more
samples depicted as larger boxes. This tool enables com-
puter vision practitioners to see how their datasets compare
with other existing datasets. Beyond visualizing the varia-
tion of available datasets (based on the number of clusters),
it can expedite the process of finding suitable data sources
by examining embedding overlaps.
5.4. Analysis of TASK2BOX
We discuss properties of T ASK2BOXbelow. Additional in-
sights are also included in Appendix ??.
Using a Box Prior Improves Performance. While
TASK2BOXwas trained with a 3-layer MLP, it achieved sig-
nificantly better performance than the MLP baseline with-
out a box prior. Tables 1, 2, and 3 demonstrate our method
outperforming the baselines. Representing each dataset as
an entity with shape and volume (such as a box), instead
of solely learning relationships from embeddings, proves
effective for generalization on unseen relationships. This
28833
Learned Task Embeddings 
Dataset Size: 56,532 
anger 
surprise 
happy Images Labels Dataset Size: 28,709 
Angry 
Fear
SadImages Labels 
Dataset Size: 2,956 
scientific 
news 
letter Labels Dataset Size: 960 
invoice 
form
passport Images Labels Images 
Figure 5. Visualizing Image Classification Datasets in Hugging Face . The sample data points annotated on the highlighted datasets
show that common tasks overlap with each other (e.g., sentiment classification and document classification datasets). Although labels could
slightly differ between datasets, T ASK2BOXcan infer the level of similarity and represent it as the amount of overlap. The embedding size
(box area) also shows the number of available data.
100101102
T ask2Box Dimension102030405060708090F1 Score (%)
Mean CLIP
[Mean,Var] CLIP
FIM
Figure 6. Effect of T ASK2BOXEmbedding Dimension on the
Accuracy of Predicting Task Relations. As the dimension in-
creases, the performance generally increases.
improvement could be attributed to the explicit modeling of
relationships through physical shapes, which enforces con-
sistency: if zi⊂zjandzjhas no overlap with zk, then
having a physical representation ensures that zi∩zk=∅.
TASK2BOXcan Represent Relations in Varying Dimen-
sions. Fig. 6 illustrates the performance of T ASK2BOXas
the box dimension increases. Representing relationships in
two dimensions results in easily interpretable embeddings;
however, modeling complex task relationships could ben-
efit from expansion to higher dimensions. Projecting to
higher dimensions may yield even better performance, as
relationships between datasets become more accurately rep-
resented, given the model’s increased representation capac-
ity. Nonetheless, as the dimensionality further increases,
learning and visualizing the embedding space also becomes
more challenging.
Using Boxes to Represent Tasks Enables Effective Cal-
culations on Embeddings. Boxes offer the advantage of
being closed under intersection, meaning the intersection of
two boxes results in another box, a property not shared bycircles or ellipses. Among region-based embeddings that
utilize geometric objects (e.g., cones, disks, and boxes) to
represent entities, operations involving boxes are the most
straightforward to calculate [10, 38].
6. Conclusion
We present a novel method that learns low-dimensional, in-
terpretable embeddings of various task relationships. With
box representations, we demonstrate that asymmetric rela-
tionships, such as task affinities and hierarchies, can be ac-
curately modeled using various base representations. While
CLIP embeddings have been shown to outperform FIM
when integrated with our model, future work could investi-
gate how CLIP might be adapted to modalities beyond text
and images. Attribute-based features offer a viable alterna-
tive, and can be extracted from from datasheets using natu-
ral language processing techniques.
The distinct properties of T ASK2BOXwere analyzed, re-
vealing its ability to perform effectively across varying di-
mensions, and to model and visualize overlaps among pub-
lic classification datasets on Hugging Face. This could en-
able computer vision practitioners to assess dataset utility
for a task in hand. Although our model successfully repre-
sents task relationships, it does not incorporate information
about optimal training procedures and model architecture.
Future work could explore the inclusion of additional infor-
mation for a more detailed understanding of the task space.
Acknowledgements. This work was supported by awards
from the National Science Foundation (2329927 and
1749833) and the NASA AIST program. The experiments
were performed on the University of Massachusetts GPU
cluster funded by the Mass. Technology Collaborative.
28834
References
[1] Hugging Face datasets. https://huggingface.
co / datasets ? task _ categories = task _
categories:image-classification . 2, 7
[2] Ralph Abboud, Ismail Ceylan, Thomas Lukasiewicz, and
Tommaso Salvatori. Boxe: A box embedding model for
knowledge base completion. Advances in Neural Informa-
tion Processing Systems , 33:9649–9661, 2020. 3
[3] Rabab Abdelfattah, Qing Guo, Xiaoguang Li, Xiaofeng
Wang, and Song Wang. Cdul: Clip-driven unsupervised
learning for multi-label image classification. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 1348–1357, 2023. 2
[4] Alessandro Achille, Michael Lam, Rahul Tewari, Avinash
Ravichandran, Subhransu Maji, Charless C Fowlkes, Ste-
fano Soatto, and Pietro Perona. Task2vec: Task embedding
for meta-learning. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 6430–6439,
2019. 1, 2, 3, 5, 6
[5] Sanjeev Arora, Wei Hu, and Pravesh K. Kothari. An analysis
of the t-sne algorithm for data visualization. In Proceedings
of the 31st Conference On Learning Theory , pages 1455–
1462. PMLR, 2018. 3
[6] Michael Boratko, Dongxu Zhang, Nicholas Monath, Luke
Vilnis, Kenneth L Clarkson, and Andrew McCallum. Capac-
ity and bias of learned geometric embeddings for directed
graphs. Advances in Neural Information Processing Systems ,
34:16423–16436, 2021. 3
[7] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell
Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-
mann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-
ing laws for contrastive language-image learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 2818–2829, 2023. 3
[8] Tejas Chheda, Purujit Goyal, Trang Tran, Dhruvesh Patel,
Michael Boratko, Shib Sankar Dasgupta, and Andrew Mc-
Callum. Box embeddings: An open-source library for rep-
resentation learning using geometric structures. In Proceed-
ings of the 2021 Conference on Empirical Methods in Nat-
ural Language Processing: System Demonstrations , pages
203–211, 2021. 3
[9] Marcos V Conde and Kerem Turgutlu. Clip-art: Contrastive
pre-training for fine-grained art classification. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 3956–3960, 2021. 2
[10] Shib Dasgupta, Michael Boratko, Siddhartha Mishra, Shriya
Atmakuri, Dhruvesh Patel, Xiang Li, and Andrew McCal-
lum. Word2box: Capturing set-theoretic semantics of words
using box embeddings. In Proceedings of the 60th Annual
Meeting of the Association for Computational Linguistics ,
2022. 8
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 2, 4, 5
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image
is worth 16x16 words: Transformers for image recognition
at scale. International Conference on Learning Representa-
tions (ICLR) , 2021. 3
[13] Kshitij Dwivedi and Gemma Roig. Representation similar-
ity analysis for efficient task taxonomy & transfer learning.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 12387–12396, 2019.
2
[14] Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil,
and Chelsea Finn. Efficiently identifying task groupings for
multi-task learning. Advances in Neural Information Pro-
cessing Systems , 34:27503–27516, 2021. 2
[15] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jen-
nifer Wortman Vaughan, Hanna Wallach, Hal Daum ´e Iii, and
Kate Crawford. Datasheets for datasets. Communications of
the ACM , 64(12):86–92, 2021. 1, 2, 3, 4
[16] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
Open-vocabulary object detection via vision and language
knowledge distillation. In International Conference on
Learning Representations , 2021. 2
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 4
[18] Wenbin He, Suphanut Jamonnak, Liang Gou, and Liu Ren.
Clip-s4: Language-guided self-supervised semantic segmen-
tation. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 11207–
11216, 2023. 2
[19] EunJeong Hwang, Jay-Yoon Lee, Tianyi Yang, Dhruvesh Pa-
tel, Dongxu Zhang, and Andrew McCallum. Event-event re-
lation extraction using probabilistic box embedding. In Pro-
ceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers) , pages
235–244, 2022. 3
[20] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-
naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-
clip, 2021. 3
[21] Zhuoliang Kang, Kristen Grauman, and Fei Sha. Learning
with whom to share in multi-task feature learning. In Pro-
ceedings of the 28th International Conference on Machine
Learning (ICML-11) , pages 521–528, 2011. 2
[22] Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Uni-
versal statistics of fisher information in deep neural net-
works: Mean field approach. In The 22nd International Con-
ference on Artificial Intelligence and Statistics , pages 1032–
1041. PMLR, 2019. 2
[23] Zhengfeng Lai, Noranart Vesdapunt, Ning Zhou, Jun Wu,
Cong Phuoc Huynh, Xuelu Li, Kah Kuen Fu, and Chen-Nee
Chuah. Padclip: Pseudo-labeling with adaptive debiasing in
clip for unsupervised domain adaptation. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 16155–16165, 2023. 2
28835
[24] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan
Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana
Marculescu. Open-vocabulary semantic segmentation with
mask-adapted clip. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 7061–7070, 2023. 2
[25] Zhibin Liao, Tom Drummond, Ian Reid, and Gustavo
Carneiro. Approximate fisher information matrix to char-
acterize the training of deep neural networks. IEEE trans-
actions on pattern analysis and machine intelligence , 42(1):
15–26, 2018. 2
[26] Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, Ke
Li, Binbin Lin, Haifeng Liu, and Xiaofei He. Clip is also
an efficient segmenter: A text-driven approach for weakly
supervised semantic segmentation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 15305–15314, 2023. 2
[27] Shaoteng Liu, Jingjing Chen, Liangming Pan, Chong-Wah
Ngo, Tat-Seng Chua, and Yu-Gang Jiang. Hyperbolic visual
embedding learning for zero-shot recognition. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , 2020. 3
[28] Leland McInnes, John Healy, Nathaniel Saul, and Lukas
Großberger. Umap: Uniform manifold approximation and
projection. Journal of Open Source Software , 3(29):861,
2018. 2
[29] Guy Melancon and Ivan Herman. Circular drawings of
rooted trees . CWI (Centre for Mathematics and Computer
Science), 1998. 3
[30] George A Miller. Wordnet: a lexical database for english.
Communications of the ACM , 38(11):39–41, 1995. 2, 5
[31] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker
Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer,
Inioluwa Deborah Raji, and Timnit Gebru. Model cards
for model reporting. In Proceedings of the conference on
fairness, accountability, and transparency , pages 220–229,
2019. 2
[32] Maximillian Nickel and Douwe Kiela. Poincar ´e embeddings
for learning hierarchical representations. Advances in neural
information processing systems , 30, 2017. 3
[33] Dhruvesh Patel, Shib Sankar Dasgupta, Michael Boratko,
Xiang Li, Luke Vilnis, and Andrew McCallum. Represent-
ing joint hierarchies with box embeddings. In Automated
Knowledge Base Construction , 2020. 3
[34] Dhruvesh Patel, Pavitra Dangati, Jay-Yoon Lee, Michael Bo-
ratko, and Andrew McCallum. Modeling label space inter-
actions in multi-label classification using box embeddings.
InInternational Conference on Learning Representations ,
2022. 3
[35] Jeffrey Pennington and Pratik Worah. The spectrum of the
fisher information matrix of a single-hidden-layer neural net-
work. Advances in neural information processing systems ,
31, 2018. 2
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1, 2, 3, 7
[37] Edward M. Reingold and John S. Tilford. Tidier drawings
of trees. IEEE Transactions on software Engineering , (2):
223–228, 1981. 3
[38] Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box:
Reasoning over knowledge graphs in vector space using box
embeddings. In International Conference on Learning Rep-
resentations , 2019. 3, 8
[39] Roseanna W Saaty. The analytic hierarchy process—what it
is and how it is used. Mathematical modelling , 9(3-5):161–
176, 1987. 5
[40] Saquib Sarfraz, Marios Koulakis, Constantin Seibold, and
Rainer Stiefelhagen. Hierarchical nearest neighbor graph
embedding for efficient dimensionality reduction. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 336–345, 2022. 3
[41] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. Advances in Neural In-
formation Processing Systems , 35:25278–25294, 2022. 3
[42] Astuti Sharma, Tarun Kalluri, and Manmohan Chandraker.
Instance level affinity-based transfer for unsupervised do-
main adaptation. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
5361–5371, 2021. 2
[43] Jian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. Vi-
sualizing large-scale and high-dimensional data. In Proceed-
ings of the 25th international conference on world wide web ,
pages 287–297, 2016. 2
[44] Laurens Van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. Journal of machine learning research , 9
(11), 2008. 1, 2
[45] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,
Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and
Serge Belongie. The inaturalist species classification and de-
tection dataset. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 8769–8778,
2018. 2, 5
[46] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun.
Order-embeddings of images and language. International
Conference on Learning Representations (ICLR) , 2015. 1, 3
[47] Vidit Vidit, Martin Engilberge, and Mathieu Salzmann. Clip
the gap: A single domain generalization approach for ob-
ject detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
3219–3229, 2023. 2
[48] Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCal-
lum. Probabilistic embedding of knowledge graphs with box
lattice measures. In Proceedings of the 56th Annual Meeting
of the Association for Computational Linguistics (Volume 1:
Long Papers) , pages 263–272, 2018. 1, 3
[49] Weixin Wang, Hui Wang, Guozhong Dai, and Hongan Wang.
Visualization of large hierarchical data by circle packing. In
Proceedings of the SIGCHI conference on Human Factors in
computing systems , pages 517–520, 2006. 3
28836
[50] Peter Welinder, Steve Branson, Takeshi Mita, Catherine
Wah, Florian Schroff, Serge Belongie, and Pietro Perona.
Caltech-ucsd birds 200. Technical Report CNS-TR-201, Cal-
tech, 2010. 2, 5
[51] Xiaoshi Wu, Feng Zhu, Rui Zhao, and Hongsheng Li. Cora:
Adapting clip for open-vocabulary detection with region
prompting and anchor pre-matching. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 7031–7040, 2023. 2
[52] Amir R Zamir, Alexander Sax, William Shen, Leonidas J
Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy:
Disentangling task transfer learning. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 3712–3722, 2018. 2, 4, 5
[53] Peihao Zhu, Rameen Abdal, John Femiani, and Peter Wonka.
Mind the gap: Domain gap control for single shot domain
adaptation for generative adversarial networks. In Interna-
tional Conference on Learning Representations , 2021. 2
28837
