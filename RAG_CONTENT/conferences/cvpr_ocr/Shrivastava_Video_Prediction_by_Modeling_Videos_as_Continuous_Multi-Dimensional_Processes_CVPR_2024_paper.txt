Video Prediction by Modeling Videos as Continuous Multi-Dimensional Processes
Gaurav Shrivastava
University of Maryland, College Park
gauravsh@umd.eduAbhinav Shrivastava
University of Maryland, College Park
abhinav@cs.umd.edu
Abstract
Diffusion models have made significant strides in image
generation, mastering tasks such as unconditional image
synthesis, text-image translation, and image-to-image con-
versions. However, their capability falls short in the realm of
video prediction, mainly because they treat videos as a collec-
tion of independent images, relying on external constraints
such as temporal attention mechanisms to enforce temporal
coherence. In our paper, we introduce a novel model class,
that treats video as a continuous multi-dimensional process
rather than a series of discrete frames. Through extensive ex-
perimentation, we establish state-of-the-art performance in
video prediction, validated on benchmark datasets including
KTH, BAIR, Human3.6M, and UCF101.1
1. Introduction
In the evolving landscape of machine learning and genera-
tive models, particularly in the domain of video representa-
tion [ 5,32,34–37], there exists a pivotal challenge in ade-
quately capturing the dynamic transitions between consecu-
tive frames. In this paper, we introduce a novel approach to
video representation that treats the video as a continuous pro-
cess in multi-dimensions. This methodology is anchored in
the observation that transitions between consecutive frames
in a video do not uniformly contain the same amount of mo-
tion. Modeling these transitions with a single-step process
often leads to suboptimal quality in sampling. Our method,
therefore, involves multiple predefined steps between two
consecutive frames, drawing inspiration from recent advance-
ments in diffusion models for image data. This multi-step
diffusion process has been instrumental in better modeling
image data, and we aim to extend this success to video data.
Previous efforts in video modeling with diffusion mod-
els have tended to approach videos as a series of images,
generating separate volumes of video frame sequences and
applying external constraints such as applying temporal at-
tention to maintain the temporal coherence. We argue that
1Navigate to the webpage for video results.
𝒙𝑇−1 𝒙𝑇−2 𝒙2 𝒙1Forward Process: 𝒙𝑡−1~𝑞𝒙𝑡−1|𝒙𝑡
Reverse Process: 𝒙𝑡~𝑝𝜃𝒙𝑡|𝒙𝑡−𝟏𝒙 𝒚𝑡=𝑇 𝑡=0
𝒙𝑇−1 𝒙𝑇−2 𝒙2 𝒙1
𝒙 𝒚
𝒙 𝒚
Intermediate time steps: 𝒙𝑡=1−𝑡𝒙+𝑡𝒚 −𝑡𝑙𝑜𝑔 (𝑡)
√2𝒛Figure 1. The figure is divided into two parts. The top portion
of the figure illustrates the intermediate frames xtbetween two
consecutive frames. x,yrepresents consecutive frames from a
video sequence where y=xj+1andx=xj.xjdenotes some
frame at timestep jin the video sequence V={xi}N
i=1.zdenotes
the white noise. The lower portion of the figure represents the
directed graphical model considered in this work to represent the
continuous video process.
this approach overlooks the inherent continuity in video data,
which can be more naturally conceptualized as a continuous
multi-dimensional process. Our proposed method defines
this continuous process, beginning with two consecutive
frames from a video sequence as endpoints this can be ob-
served in Fig. 1. We delineate the forward process through
interpolation between these endpoints, with a predefined
number of steps guiding the transition from one point to
another. To ensure the existence of p(xt)at all points, we
introduce a novel noise schedule that applies zero noise at
both endpoints.
We approximate each step between these endpoints using
a Gaussian distribution, following the assumptions made in
diffusion models for images by the paper [ 15,21,38,39]. In
defining this forward process, we also lay the groundwork
for estimating a reverse process. This paper presents a novel
lower variational bound for estimating this reverse process.
To summarize, our contribution in this work is as follows:
•We introduce a novel approach for representing videos as
multi-dimensional continuous processes.
•We derive a novel variational bound that efficiently esti-
mates the reverse process in our proposed ‘Continuous
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
7236
Video Process (CVP)’ model.
•Our method employs a unique noise schedule for the con-
tinuous video process, characterized by zero noise at both
endpoints, ensuring the existence of p(xt)at all intermedi-
ate timesteps.
•We demonstrate the efficacy of our approach through state-
of-the-art results in video prediction tasks across four dif-
ferent datasets namely, KTH action recognition, BAIR
robot push, Human3.6M, and UCF101 datasets. Addition-
ally, our model requires 75% fewer sampling steps when
sampling a frame compared to a diffusion-based baseline.
2. Related Works
Understanding and predicting future states based on ob-
served past events is a cornerstone challenge in the domain of
video understanding, crucial for applications where captur-
ing the inherent multi-modality of future states is vital, such
as in autonomous vehicles. Early methods in this field, as
noted by Yuen et al.[ 52] and Walker et al.[ 47], primarily fo-
cused on matching past frames within datasets to extrapolate
future states, although these predictions were constrained
to either symbolic trajectories or directly retrieved future
frames. The advent of deep learning has significantly pro-
pelled advancements in this area. One of the seminal works
by Srivastava et al.[ 41] leveraged a multi-layer LSTM net-
work for deterministic representation learning of video se-
quences. Subsequent studies [ 8,11,17,30,43,45,49], have
expanded the scope of this research by constructing models
that account for the stochastic nature of future states, mark-
ing a notable shift from earlier deterministic approaches.
Recent research in this domain has explored both im-
plicit and explicit probabilistic modeling approaches. Im-
plicit probabilistic modeling, typified by GAN [ 20]-based
models, has a substantial history. Nonetheless, these mod-
els [10,26,28] often grapple with training stability issues
and mode collapse(where model only focuses on a few
modes in the dataset) issues. On the other hand, explicit
probabilistic modeling for video prediction encompasses
a range of methodologies, including Variational Autoen-
coders (V AEs) [ 24], Gaussian processes, and Diffusion mod-
els. V AE-based video prediction methods [ 7,14,26] tend
to average results to align with all potential future scenar-
ios, which undermines the fidelity of predictions. Gaus-
sian process-based models [ 4,35] exhibit proficiency with
smaller datasets but encounter scalability issues owing to
matrix inversion limitations when calculating training like-
lihood. While workarounds exist, they tend to compromise
result fidelity.
Recent advancements in diffusion models [ 12,22,23,
46] have positioned them as the preferred choice for video
prediction tasks. These multi-step models offer superior
sample quality and are resilient to mode collapse. However,
even with such lucrative advantages, modeling videos withthese models tends to have downsides. Majorly methods
falling under this category enforce temporal consistency
using artificial external constraints such as the introduction
of temporal attention blocks. This might be effective but
comes at a cost of significant computing power.
Another class of popular video prediction models is hier-
archical prediction [ 5,6,44,48,50] models. These models
are multistage models that decompose the problems into two
stages. They first predict a high-level structure of a video,
like a human pose, and then leverage that structure to make
predictions at the pixel level. These models generally require
additional annotation for the high-level structure for train-
ing, unlike ours that predicts future frames utilizing only the
pixel-level information of context frames.
We also want to highlight some very recent works like
InDI [ 13], and Cold diffusion [ 3] that provide an alternate
approach to denoising diffusion models that is similar to
our approach. However, their works only explored such
formulation for image-based computational photography and
image generation tasks.
3. Method
Instead of introducing noise iteratively to the frames un-
til they conform to a Gaussian distribution, and adopting
a reverse process such as denoising diffusion, a commonly
employed technique for video prediction, we introduce a
novel model category designed to depict videos as continu-
ous processes. This section delves into the modeling of this
continuous video process.
Suppose we have a video sequence denoted by V=
{xt}N
1where xj∈Rc×h×wis the frame at the timestep
j. We represent this video sequence as a continuous process.
The intermediate frames between x=xjandy=xj+1are
given by the following equation.
xt= (1−t)x+ty−tlog(t)√
2z (1)
Here, z∼ N (0, I)denotes the white noise. From the
above Eqn, it can be seen that at t= 0, we get the frame
xjand at t= 1, we get the frame xj+1. We utilize this
continuous process of evolving xj→xj+1given by Eqn. 1
and derive both the forward and reverse processes. For
defining the forward process, we take steps in the direction
t:T→0instead of the other way, which happens in
denoising diffusion process [ 21]. The reason for this is we
want the reverse process to start from past frame xand
according to the Eqn. 1 xt=xatt= 0.
We can write the forward process, i.e., going from the
start point yatt=Tto endpoint xatt= 0,
xt+∆t=xt+ (y−x)∆t−tlog(t)z (2)
7237
(b) Training Pipeline (c) Sampling Pipeline
U-NET U-NET(a) Finding  in Single step
Current block of 
FramesFuture block of 
FramesNoise Schedule
Figure 2. Fig. (a) demonstrates the methodology for estimating xtin a single step, showcasing the specific computational process involved.
Fig. (b) details the training pipeline of our Continuous Video Process (CVP) model, where xtandtare fed as inputs to the U-Net architecture,
and the anticipated output is ˆy, with ˆy=x1:k+1in this scenario. Fig. (c) provides an overview of the sampling pipeline utilized in our CVP
method, illustrating the sequential steps to predict the next frame of the video sequence given the context frames.
From the above equation, we can write the posterior for
the forward process as q(xt+1|xt,x,y) = N(xt+1:
˜µ(xt,x,y), g2(t)I). Where g(t) =−tlogt . The whole
derivation is provided in the appendix.
For modeling our video diffusion process, we
like to model the likelihood function pθ(xT):=R
pθ(x0:T)dx0:T−1and minimize the negative log-
likelihood to obtain the best fit for our model. Here, pθ(x0:T)
is the probability of the reverse process, and it is defined as
a Markov chain with learned Gaussian transitions starting
atp(x0) =pdata(x). Important note about the notations
x0,xT, unless specified consider x0=xandxT=y
where xis the frame in the video sequence at jthposition
andyis the frame at (j+ 1)thposition. One important
assumption about the continuous video process is we assume
the transition between the frames xandyto follow Markov
chain, i.e., the current state at timestep tonly depends on the
previous state at timestep t−1. Leveraging this assumption
we can define the reverse process as follows,
pθ(x0:T):=p(x0)TY
t=1pθ(xt|xt−1) (3)
where, pθ(xt−1|xt) := N(xt;µθ(xt−1, t−
1),Σθ(xt−1, t−1)). We are interested in learning
the reverse process to perform our video prediction task.
The forward process or the diffusion process is a fixed
Markov chain that gradually transforms the frame yto frame
x.
q(x0:T−1|xT):=TY
t=1q(xt−1|xt), (4)
Training is performed by minimizing the variationalbound on the negative log-likelihood.
E[−logpθ(xT)]≤Eq
−logpθ(x0:T)
q(x0:T−1|xT)
(5)
≤Eq
−logp(x0)−X
t≥1logpθ(xt|xt−1)
q(xt−1|xt)
(6)
=:L(θ) (7)
This variational bound can be simplified to the following
(we refer the readers to the appendix to follow the simplifi-
cation of from Eqn. 7 to the following equation),
L(θ) =:X
t≥1DKL(q(xt|xt−1,x,y)∥pθ(xt|xt−1,x))(8)
In the above Eqn, the KL divergence term utilizes the
comparison of pθ(xt|xt−1,x)with forward process poste-
rior term, which is tractable under the process given by
Eqn. 2. The forward process posterior term is given by
q(xt|xt−1,x,y) =N(xt: ˜µ(xt−1,x,y), g2(t)I)(9)
where, ˜µ(xt,x,y) =xt+ (y−x)andg(t) =−tlog(t).
Consequently, all KL divergences in Eqn. 8 are compar-
isons between Gaussians, so they can be calculated in a
Rao-Blackwellized fashion with closed-form expressions
instead of high-variance Monte Carlo estimates. It is impor-
tant to note while deriving the Eqn. 8, we ignore some terms
that purely involve the forward process posteriors as qhas
no learnable parameters, so such terms are constants during
training.
Now we discuss our choices in pθ(xt|xt−1,x) =
N(xt;µθ(xt−1, t−1,x),Σθ(xt−1, t−1,x))for1< t≤T.
First, we set Σθ(xt−1, t−1) = g2(t)Ito untrained time
dependent constants. Experimentally, the choice of g(t) =
−tlog(t)works the best. This noise function has an inter-
esting property that noise is absent both at the start and end
points, i.e., g(t) = 0 ∀t={0,1}.
7238
Second, to represent the mean µθ(xt, t,x), we propose
a specific parameterization motivated by the forward pro-
cess posterior given by Eqn. 9. With pθ(xt|xt−1,x) =
N(xt;µθ(xt−1, t−1,x), g2(t)I), we can write:
L(θ):=Eq1
2g2(t)∥˜µ(xt,x,y)−µθ(xt, t,x)∥2
+C
(10)
where Cis a constant that does not depend on θ. So, we
see that the most straightforward parameterization of µθis a
model that predicts ˜µt, the forward process posterior mean.
However, we can simplify Eqn. 10 further and obtain a
very simple training loss objective by delving in the term ˜µ.
We further parameterize the term µθas follows,
µθ(xt, t,x) =xt+ (yθ(xt)−x) (11)
When we substitute this µθ(xt, t,x)parameterization in
the Eqn. 10 we get the simplified version of the loss L(θ)as
follows,
Lsimple (θ):=Et,xt1
2g2(t)∥y−yθ((xt, t)∥2
(12)
For training the video prediction model utilizing the above
Eqn. 12 we obtain the xtas a function of tby leveraging the
Eqn. 1. The following equation gives a more generic form of
the final loss function utilized to train the video prediction
model,
arg min
θEt,x,y"
1
2g2(t)y−yθ((1−t)x+ty+g(t)√
2z, t)2#
(13)
The whole training and sampling pipeline is described in the
training Alg. 1, sampling Alg. 2 and depicted in Fig. 2.
4. Experiments
Video prediction task can be defined as given a few context
frames, the model has to predict the subsequent future frames.
In this section, we empirically demonstrate that our approach
yields superior results in modeling the video prediction task.
4.1. Datasets
We chose 4 different types of datasets to demonstrate the
efficacy of our approach. These are standard benchmarks
for video prediction tasks. Dataset lists include KTH action
recognition dataset [ 33], BAIR robot pushing dataset [ 16],
Human3.6M [ 9] and UCF101 [ 40] datasets. Training and
architecture-specific details about the approach are included
in the appendix.
KTH Action Recognition Dataset. The KTH action
dataset [ 33] consists of video sequences of 25 people per-
forming six different actions: walking, jogging, running,Algorithm 1 Training of CVP model
1:repeat
2:x,y∼qdata(x,y)
3:t∼Uniform( {1, . . . , T })
4:z∼ N(0,I)
5:Take gradient descent step on
∇θ1
2g2(t)y−yθ((1−t)x+ty−(tlog(t)/√
2)z, t)2
6:until converged
Algorithm 2 Sampling Algorithm
1:x∼qdata(x)
2:x0=x
3:d=1
N, HereNdenotes number of steps.
4:fort= 1, . . . , N do
5:z∼ N(0,Id)ift >1, elsez=0
6:xt+1=xt+ (ˆy(xt, t)−x)d−tlog(t)z
7:end for
8:return xT
boxing, hand-waving, and hand-clapping. The background
is uniform, and a single person is performing actions in the
foreground. The foreground motion of the person in the
frame is fairly regular. The frames in the video for this
dataset consist of a single channel. The spatial resolution
of the frames in the video is downsampled to the size of
64×64.
BAIR pushing Dataset. The BAIR robot pushing
dataset [ 16] contains the videos of table mounted sawyer
robotic arm pushing various objects around. The BAIR
dataset consists of different actions given to the robotic arm
to perform. The spatial resolution of the frames in the video
is kept to be 64×64.
Human3.6M Dataset. Human3.6M [ 9] dataset consists of
10 subjects performing 15 different actions. The pose in-
formation from the dataset was not used in predicting next
frame. The background is uniform, and a single person is
performing actions in the foreground. The foreground mo-
tion of the person in the frame is fairly regular. The frames
in the video for this dataset consist of ‘RGB’ channels. The
spatial resolution of the frames in the video is downsampled
to the size of 64×64.
UCF101 Dataset. This dataset [ 40] consists of 13,320
videos belonging to 101 different action classes. The video
seems to have a variety of backgrounds and the frames of
the video have three channels, namely ‘RGB’. We reshape
the resolution of frames from the original size of 320×240
down to 128×128for our video prediction tasks. The
downsampling is done utilizing the bicubic downsampling.
4.2. Metrics
We primarily use the FVD [ 42] metric to determine the
best-performing baseline when evaluating a video prediction
7239
GT
GT
GTContext  Frame s
Pred
Frame s
Pred
Frame s
Pred
Frame s1 4 8 12 16 20 24 28Timesteps ProgressionFigure 3. Figure represents qualitative results of our CVP model on
the KTH dataset. The number of context frames used in the above
setting is 4 for all three sequences. Every 4thpredicted future
frame is shown in the figure.
Table 1. Video prediction results on KTH ( 64×64), predicting 30
and 40 frames using models trained to predict kframes at a time.
All models condition on 10 past frames on 256 test videos.
KTH [10→#pred; trained on k]k#pred FVD↓PSNR↑SSIM↑
SVG-LP [14] 10 30 377 28.1 0.844
SA VP [26] 10 30 374 26.5 0.756
MCVD [46] 5 30 323 27.5 0.835
SLAMP [1] 10 30 228 29.4 0.865
SRVP [18] 10 30 222 29.7 0.870
RIVER [12] 10 30 180 30.4 0.86
CVP (Ours) 130 140.6 29.8 0.872
Struct-vRNN [29] 10 40 395.0 24.29 0.766
SVG-LP [14] 10 40 157.9 23.91 0.800
MCVD [46] 5 40 276.7 26.40 0.812
SA VP-V AE [26] 10 40 145.7 26.00 0.806
Grid-keypoints [19] 10 40 144.2 27.11 0.837
RIVER [12] 10 40 170.5 29.0 0.82
CVP (Ours) 140 120.1 29.2 0.841
task. FVD metric evaluates a baseline on both terms, the
reconstruction quality and diversity of the generated samples.
FVD is calculated as the frechet distance between the I3D
embeddings of generated video samples and real samples.
The I3D network used for obtaining the embeddings for real
and generated video is trained on the Kinetics-400 dataset.
5. Setup and Results
Below, we describe in detail how the setup for our experi-
ment looks compared to baselines. We also showcase our
findings about the performance of our method and compari-
son to baselines in this section.
KTH action recognition dataset : For this dataset, we ad-
hered to the baseline setup [ 46], which utilizes the first 10
frames as context frames. In baseline setup, these 10 frames
are utilized to predict the subsequent 30 and 40 frames. Anotable aspect of our experiment is we only used the last 4
frames from this sequence of 10 frames as context frames
in our CVP model, while disregarding the information in
the remaining 6 frames. This decision was taken to main-
tain consistency with the experimental setups used in prior
baseline methodologies. The outcomes of this evaluation are
summarized in Table. 1.
It can be observed from the Table. 1, our model’s unique
approach requires a significantly reduced number of frames
for training. Contrary to other methods that train on an
additional set of kframes (10[context frames]+k[future
frames]), our model uses just one frame (effectively 4[con-
text frames]+1[future frames]). We employ the 4 context
frames to predict the immediate next frame and then autore-
gressively generate either 30 or 40 frames, depending on the
evaluation requirement. This methodology is supported by
our model’s efficient handling of video sequences as con-
tinuous processes, which eliminates the need for external
artificial constraints, such as temporal attention mechanisms.
The results, as shown in Table 1, clearly indicate that
our method delivers state-of-the-art performance when com-
pared to other baseline models. Additionally, the qualitative
results for our CVP model on the KTH dataset can be ob-
served in Fig. 3.
BAIR Robot Push dataset : The BAIR Robot Push dataset
is characterized by highly stochastic video sequences. In our
study, we adhered to a baseline setup [ 46] with three main
experimental settings: 1) using only one context frame to
predict the next 15 frames, 2) employing two context frames
to predict 14 future frames, and 3) utilizing two context
frames to forecast the next 28 frames. The outcomes of these
approaches are summarized in Table 2.
As observed in Table 2, a trend emerges where increasing
the number of frames predicted at a time concurrently results
in a degradation of prediction quality. This phenomenon is
hypothesized to stem from an augmented disparity between
the blocks of context frames and predicted future frames.
Specifically, consider the scenario where two context frames
are designated as x0:2, corresponding to xin the context of
Eqn.1. Under the first experimental condition, where the
model predicts a single frame at a time, the future frame pre-
diction block is represented as x1:3, analogous to yin Eqn.1.
Conversely, in the second condition, where two frames are
predicted simultaneously, the future frame block extends to
x2:4, again paralleling yin the equation. This setup implies
that in the former setting, interpolation occurs between adja-
cent frames (i.e., the transition from x0→x1andx1→x2),
while in the latter, interpolation spans a two-frame interval
(i.e., the transition from x0→x2and from x1→x3). The
expanded interval in the second scenario is posited as the
causative factor for the observed reduction in predictive per-
formance, particularly in configurations where k= 2 and
p= 2.
7240
GTGTContext Frames
Pred
FramesPred
Frames1 6 12 18 24Timesteps ProgressionFigure 4. Figure represents qualitative results of our CVP model on
the BAIR dataset. The number of context frames used in the above
setting is two for both sequences. Every 6thpredicted future frame
is shown in the figure.
The results, as shown in Table 2, clearly indicate that our
method delivers state-of-the-art performance compared to
other baseline models. Additionally, the qualitative results
for our CVP model on the BAIR dataset can be observed in
Fig. 4.
Human3.6M dataset : Similar to the KTH dataset, the Hu-
man3.6M dataset features actors performing distinct actions
against a static background. However, the Human3.6M
dataset distinguishes itself by offering a greater variety of
distinct actions within its videos and providing three-channel
video frames, in contrast to the single-channel frames of the
KTH dataset. For evaluating the Human3.6M dataset, we
employed a similar setup to that used for the KTH dataset,
where 5 frames are provided as context, and the model pre-
dicts the subsequent 30 frames based on these context frames.
The results of this evaluation are summarized in Table 3.
An analysis of Table 3 reveals that our model, with its
unique approach, requires a significantly lower number of
frames for training, needing only a total of 6 frames per
block to yield results that are considerably better than those
of the baselines.
The results, as presented in Table 3, unequivocally demon-
strate that our method outperforms other baseline models, es-
tablishing a new state-of-the-art on the Human3.6M dataset.
Furthermore, the qualitative efficacy of our CVP model on
the Human3.6M dataset is illustrated in Fig. 5, showcasing
the model’s ability to effectively capture and predict the
dataset’s varied actions.
UCF101 dataset : The UCF101 dataset presents a greater
level of complexity compared to the KTH or Human3.6M
datasets, owing to its substantially higher number of ac-
tion categories, diverse backgrounds, and significant camera
movements. Notably, we only use information from the con-
text frames for our frame-conditional generation task. NoTable 2. BAIR dataset evaluation. Video prediction results on
BAIR ( 64×64) conditioning on ppast frames and predicting pred
frames in the future, using models trained to predict kframes at at
time.The common way to compute the FVD is to compare 100 ×256
generated sequences to 256 randomly sampled test videos. Best
results are marked in bold.
BAIR (64×64) p k #pred FVD↓
LVT [31] 1 15 15 125.8
DVD-GAN-FP [10] 1 15 15 109.8
TrIVD-GAN-FP [28] 1 15 15 103.3
VideoGPT [51] 1 15 15 103.3
CCVS [25] 1 15 15 99.0
FitVid [2] 1 15 15 93.6
MCVD [46] 1 5 15 89.5
NÜWA [27] 1 15 15 86.9
RaMViD [23] 1 15 15 84.2
VDM [22] 1 15 15 66.9
RIVER [12] 1 15 15 73.5
CVP (Ours) 1 1 15 70.1
DVG [35] 2 14 14 120.0
SA VP [26] 2 14 14 116.4
MCVD [46] 2 5 14 87.9
CVP (Ours) 2 2 14 68.2
CVP (Ours) 2 1 14 65.1
SA VP [26] 2 10 28 143.4
Hier-vRNN [7] 2 10 28 143.4
MCVD [46] 2 5 28 118.4
CVP (Ours) 2 2 28 95.1
CVP (Ours) 2 1 28 85.1
Table 3. Quantitative comparisons on the Human3.6M dataset. The
best results under each metric are marked in bold.
Human3.6M pk#pred FVD↓
SVG-LP [14] 5 10 30 718
Struct-VRNN [29] 5 10 30 523.4
DVG [35] 5 10 30 479.5
SRVP [18] 5 10 30 416.5
Grid keypoint [19] 8 8 30 166.1
CVP (Ours) 5 1 30 144.5
extra information, like class labels, was used for the predic-
tion task. In evaluating the UCF101 dataset, we adopted an
approach similar to that used for the Human3.6M dataset,
where 5 context frames are provided, and the model is tasked
with predicting the next 16 frames based on these. The
outcomes of this evaluation are detailed in Table. 4.
An examination of Table. 4 reveals that our CVP model
surpasses the performance of other baseline models, thereby
setting a new benchmark for the UCF101 dataset. Addi-
tionally, the qualitative performance of our CVP model on
the UCF101 dataset is depicted in Fig. 6. This illustration
7241
GTContext Frames
1 4 8 12 16 20 24 28Timesteps Progression
GT
GTPred
Frames
Pred
Frames
Pred
FramesFigure 5. Figure represents qualitative results of our CVP model on the Human3.6M dataset. The number of context frames used in the
above setting is 4 for all three sequences. Every 4thpredicted future frame is shown in the figure.
Table 4. Video prediction results on UCF ( 128×128), predicting
16 frames. All models are conditioned on 5 past frames.
UCF101 [5→16]p k #pred FVD↓
SVG-LP [14] 5 10 16 1248
CCVS [25] 5 16 16 409
MCVD [46] 5 5 16 387
RaMViD [23] 5 4 16 356
CVP (Ours) 51 16 245.2
showcases the model’s proficiency in accurately capturing
and predicting the diverse range of actions featured in the
dataset.
6. Limitation
While our method demonstrates promising results in video
prediction, it is important to acknowledge its limitations to
guide future research and application development.
A primary limitation of our approach is its reliance on a
limited context frame window for predicting the next frame.
Specifically, when a context vector, denoted as x0:4, com-prising 4 video frames is used, the prediction of the subse-
quent frame is entirely dependent on this four-frame window.
This model architecture performs adequately in scenarios
involving uniform video sequences. However, its efficacy
diminishes in a setting that requires more context to pre-
dict the future frame. Addressing this limitation requires
a more adaptive approach that can handle varying contex-
tual information, a challenge we have earmarked for future
research.
Another constraint lies in the computational efficiency of
our model. Currently, it necessitates multiple steps to sample
a single frame, which could become a significant bottleneck,
especially when a larger number of frame predictions are
required. Although our method is more efficient in terms of
the number of steps needed for frame sampling compared to
diffusion-based counterparts, further optimization is neces-
sary to reduce the computational overhead associated with
this process.
Additionally, our experimental setup was constrained by
the computational resources available to us. The model was
developed and tested using just two A6000 GPUs. This
limitation raises questions about the potential improvements
7242
Context 
Frames 1 4 8 12 16 20 24 28Timesteps Progression
GT
Pred
Frames
GT
GTPred
Frames
Pred
FramesFigure 6. Figure represents qualitative results of our CVP model on the UCF dataset. The number of context frames used in the above setting
is 5 for all three sequences. Every 4thpredicted future frame is shown in the figure.
that could be achieved with a more powerful computational
setup. A larger model with an increased number of parame-
ters, trained on more advanced hardware, could potentially
unveil further advancements in video prediction capabilities.
We recognize this as an important area for investigation and
encourage labs with more substantial resources to explore
this avenue.
In summary, while our model represents a significant
step forward in video prediction, these limitations highlight
crucial areas for future research and development, paving the
way for more robust and versatile video prediction models.
7. Conclusion
In this work, we have presented a novel model class designed
specifically for video representation, marking a significant
advancement in the field of video prediction tasks. Our com-
prehensive experimental evaluations across various datasets,
including KTH, BAIR, Human3.6M, and UCF101, have not
only validated the effectiveness of our model but also estab-
lished new benchmarks in state-of-the-art performance for
video prediction tasks.A notable aspect of our approach is its efficiency in terms
of the required number of context and future frames for
training. Moreover, our model’s continuous video process
capability uniquely operates without the need for additional
constraints such as temporal attention, which are typically
employed to ensure temporal consistency. This aspect of our
model underscores its inherent ability to maintain temporal
coherence, further simplifying the video prediction process
while enhancing its effectiveness.
In conclusion, the innovations introduced in our model
offer promising directions for future research in video repre-
sentation and prediction. The achievements demonstrated in
this paper not only contribute to the advancement of video
prediction methodologies but also open avenues for explor-
ing more efficient and effective ways of video representation
in various real-world applications.
Acknowledgements. This project was partially funded
by NSF CAREER Award (2238769) to AS. We also thank
Shirley Huang for providing feedback on the manuscript.
7243
References
[1]Adil Kaan Akan, Erkut Erdem, Aykut Erdem, and Fatma
Güney. Slamp: Stochastic latent appearance and motion
prediction. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 14728–14737, 2021.
5
[2]Mohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj
Nair, Sergey Levine, Chelsea Finn, and Dumitru Erhan. Fitvid:
Overfitting in pixel-level video prediction. arXiv preprint
arXiv:2106.13195 , 2021. 6
[3]Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid
Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping,
and Tom Goldstein. Cold diffusion: Inverting arbitrary image
transforms without noise. arXiv preprint arXiv:2208.09392 ,
2022. 2
[4]Sarthak Bhagat, Shagun Uppal, Zhuyun Yin, and Nengli Lim.
Disentangling multiple features in video sequences using
gaussian processes in variational autoencoders, 2020. 2
[5]Navaneeth Bodla, Gaurav Shrivastava, Rama Chellappa, and
Abhinav Shrivastava. Hierarchical video prediction using re-
lational layouts for human-object interactions. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12146–12155, 2021. 1, 2
[6]Haoye Cai, Chunyan Bai, Yu-Wing Tai, and Chi-Keung Tang.
Deep video generation, prediction and completion of human
action sequences. Lecture Notes in Computer Science , page
374–390, 2018. 2
[7]Lluis Castrejon, Nicolas Ballas, and Aaron Courville. Im-
proved conditional vrnns for video prediction. In Proceedings
of the IEEE/CVF international conference on computer vision ,
pages 7608–7617, 2019. 2, 6
[8]Lluís Castrejón, Nicolas Ballas, and Aaron C. Courville.
Improved conditional vrnns for video prediction. CoRR ,
abs/1904.12165, 2019. 2
[9]Cristian Sminchisescu Catalin Ionescu, Fuxin Li. Latent
structured models for human pose estimation. In International
Conference on Computer Vision , 2011. 4
[10] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adver-
sarial video generation on complex datasets. arXiv preprint
arXiv:1907.06571 , 2019. 2, 6
[11] Francesco Cricri, Xingyang Ni, Mikko Honkala, Emre Aksu,
and Moncef Gabbouj. Video ladder networks. CoRR ,
abs/1612.01756, 2016. 2
[12] Aram Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient
video prediction via sparsely conditioned flow matching. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 23263–23274, 2023. 2, 5, 6
[13] Mauricio Delbracio and Peyman Milanfar. Inversion by direct
iteration: An alternative to denoising diffusion for image
restoration. arXiv preprint arXiv:2303.11435 , 2023. 2
[14] Emily Denton and Rob Fergus. Stochastic video generation
with a learned prior, 2018. 2, 5, 6, 7
[15] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in Neural Information
Processing Systems , 34, 2021. 1[16] Frederik Ebert, Chelsea Finn, Alex X. Lee, and Sergey Levine.
Self-supervised visual planning with temporal skip connec-
tions, 2017. 4
[17] N. Elsayed, A. S. Maida, and M. Bayoumi. Reduced-gate
convolutional lstm architecture for next-frame video predic-
tion using predictive coding. In 2019 International Joint
Conference on Neural Networks (IJCNN) , pages 1–9, 2019. 2
[18] Jean-Yves Franceschi, Edouard Delasalles, Mickaël Chen,
Sylvain Lamprier, and Patrick Gallinari. Stochastic latent
residual video prediction. In International Conference on
Machine Learning , pages 3233–3246. PMLR, 2020. 5, 6
[19] Xiaojie Gao, Yueming Jin, Qi Dou, Chi-Wing Fu, and Pheng-
Ann Heng. Accurate grid keypoint learning for efficient video
prediction. In 2021 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS) , pages 5908–5915.
IEEE, 2021. 5, 6
[20] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks, 2014. 2
[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 2020. 1, 2
[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,
Mohammad Norouzi, and David J. Fleet. Video diffusion
models. 2022. 2, 6
[23] Tobias Höppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen,
and Andrea Dittadi. Diffusion models for video prediction
and infilling, 2022. 2, 6, 7
[24] Diederik P. Kingma and Max Welling. Auto-encoding varia-
tional bayes. CoRR , abs/1312.6114, 2013. 2
[25] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. Ccvs:
context-aware controllable video synthesis. Advances in Neu-
ral Information Processing Systems , 34:14042–14055, 2021.
6, 7
[26] Alex X. Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel,
Chelsea Finn, and Sergey Levine. Stochastic adversarial video
prediction. arXiv preprint arXiv:1804.01523 , 2018. 2, 5, 6
[27] Jian Liang, Chenfei Wu, Xiaowei Hu, Zhe Gan, Jianfeng
Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan
Duan. Nuwa-infinity: Autoregressive over autoregressive
generation for infinite visual synthesis. Advances in Neural
Information Processing Systems , 35:15420–15432, 2022. 6
[28] Pauline Luc, Aidan Clark, Sander Dieleman, Diego de Las
Casas, Yotam Doron, Albin Cassirer, and Karen Simonyan.
Transformation-based adversarial video prediction on large-
scale data. arXiv preprint arXiv:2003.04035 , 2020. 2, 6
[29] Matthias Minderer, Chen Sun, Ruben Villegas, Forrester Cole,
Kevin P Murphy, and Honglak Lee. Unsupervised learning
of object structure and dynamics from videos. Advances in
Neural Information Processing Systems , 32, 2019. 5, 6
[30] Marc Oliu, Javier Selva, and Sergio Escalera. Folded re-
current neural networks for future video prediction. CoRR ,
abs/1712.00311, 2017. 2
[31] Ruslan Rakhimov, Denis V olkhonskiy, Alexey Artemov, De-
nis Zorin, and Evgeny Burnaev. Latent video transformer.
arXiv preprint arXiv:2006.10704 , 2020. 6
7244
[32] Nirat Saini, Bo He, Gaurav Shrivastava, Sai Saketh Ramb-
hatla, and Abhinav Shrivastava. Recognizing actions using
object states. In ICLR2022 Workshop on the Elements of
Reasoning: Objects, Structure and Causality , 2022. 1
[33] C. Schuldt, I. Laptev, and B. Caputo. Recognizing human
actions: a local svm approach. In Proceedings of the 17th
International Conference on Pattern Recognition, 2004. ICPR
2004. , pages 32–36 V ol.3, 2004. 4
[34] Gaurav Shrivastava. Diverse Video Generation . PhD thesis,
University of Maryland, College Park, 2021. 1
[35] Gaurav Shrivastava and Abhinav Shrivastava. Diverse video
generation using a gaussian process trigger. arXiv preprint
arXiv:2107.04619 , 2021. 2, 6
[36] Gaurav Shrivastava, Ser-Nam Lim, and Abhinav Shrivastava.
Video dynamics prior: An internal learning approach for
robust video enhancements. In Thirty-seventh Conference on
Neural Information Processing Systems , 2023.
[37] Gaurav Shrivastava, Ser-Nam Lim, and Abhinav Shrivastava.
Video decomposition prior: Editing videos layer by layer. In
The Twelfth International Conference on Learning Represen-
tations , 2024. 1
[38] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. International Conference on
Learning Representations , 2020. 1
[39] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equations.
International Conference on Learning Representations , 2021.
1
[40] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild, 2012. 4
[41] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdi-
nov. Unsupervised learning of video representations using
lstms. In Proceedings of the 32Nd International Conference
on International Conference on Machine Learning - Volume
37, pages 843–852. JMLR.org, 2015. 2
[42] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,
Raphael Marinier, Marcin Michalski, and Sylvain Gelly. To-
wards accurate generative models of video: A new metric and
challenges, 2018. 4
[43] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin,
and Honglak Lee. Decomposing motion and content for
natural video sequence prediction. CoRR , abs/1706.08033,
2017. 2
[44] Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn,
Xunyu Lin, and Honglak Lee. Learning to generate long-term
future via hierarchical prediction, 2017. 2
[45] Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru
Erhan, Quoc V . Le, and Honglak Lee. High fidelity video
prediction with large stochastic recurrent neural networks,
2019. 2
[46] Vikram V oleti, Alexia Jolicoeur-Martineau, and Chris Pal.
Mcvd-masked conditional video diffusion for prediction, gen-
eration, and interpolation. Advances in Neural Information
Processing Systems , 35:23371–23385, 2022. 2, 5, 6, 7[47] Jacob Walker, Abhinav Gupta, and Martial Hebert. Patch
to the future: Unsupervised visual prediction. In Proceed-
ings of the IEEE conference on Computer Vision and Pattern
Recognition , pages 3302–3309, 2014. 2
[48] Jacob Walker, Kenneth Marino, Abhinav Gupta, and Martial
Hebert. The pose knows: Video forecasting by generating
pose futures. In International Conference on Computer Vision ,
2017. 2
[49] Yunbo Wang, Lu Jiang, Ming-Hsuan Yang, Li-Jia Li, Ming-
sheng Long, and Li Fei-Fei. Eidetic 3d LSTM: A model for
video prediction and beyond. In International Conference on
Learning Representations , 2019. 2
[50] Nevan Wichers, Ruben Villegas, Dumitru Erhan, and Honglak
Lee. Hierarchical long-term video prediction without super-
vision, 2018. 2
[51] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srini-
vas. Videogpt: Video generation using vq-vae and transform-
ers.arXiv preprint arXiv:2104.10157 , 2021. 6
[52] Jenny Yuen and Antonio Torralba. A data-driven approach
for event prediction. In European Conference on Computer
Vision , pages 707–720. Springer, 2010. 2
7245
