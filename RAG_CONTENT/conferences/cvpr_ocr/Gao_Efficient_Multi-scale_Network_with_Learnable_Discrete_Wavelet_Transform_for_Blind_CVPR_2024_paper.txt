Efficient Multi-scale Network with Learnable Discrete Wavelet Transform
for Blind Motion Deblurring
Xin Gao∗1,2Tianheng Qiu∗2,3,4
Xinyu Zhang†2Hanlin Bai1Kang Liu1Xuan Huang†4Hu Wei4Guoying Zhang†1Huaping Liu2
∗Equal Contribution†Coresponding Author
1China University of Mining & Technology-Beijing2Tsinghua University
3University of Science and Technology of China
4Hefei Institutes of Physical Science,Chinese Academy of Sciences
Abstract
Coarse-to-fine schemes are widely used in traditional
single-image motion deblur; however, in the context of deep
learning, existing multi-scale algorithms not only require
the use of complex modules for feature fusion of low-scale
RGB images and deep semantics, but also manually gener-
ate low-resolution pairs of images that do not have sufficient
confidence. In this work, we propose a multi-scale network
based on single-input and multiple-outputs(SIMO) for mo-
tion deblurring. This simplifies the complexity of algorithms
based on a coarse-to-fine scheme. To alleviate restoration
defects impacting detail information brought about by us-
ing a multi-scale architecture, we combine the character-
istics of real-world blurring trajectories with a learnable
wavelet transform module to focus on the directional con-
tinuity and frequency features of the step-by-step transi-
tions between blurred images to sharp images. In conclu-
sion, we propose a multi-scale network with a learnable
discrete wavelet transform (MLWNet), which exhibits state-
of-the-art performance on multiple real-world deblurred
datasets, in terms of both subjective and objective quality
as well as computational efficiency. Our code is available
onhttps://github.com/thqiu0419/MLWNet .
1. Introduction
Most current top-performing single-image blind deblurring
algorithms are based on DNNs, which can be structurally
categorized into single-scale [2, 10, 12, 28, 29, 42] and
multi-scale approaches [3, 9, 13, 27, 37]. Compared to
single-scale methods, multi-scale methods are built on the
idea of moving from coarse-to-fine, and thus they decom-
pose the challenging single-image blind deblurring problem
into easier-to-solve sub-problems to restore the blurred im-
age step-by-step.
Earlier DNNs exploiting the coarse-to-fine concept usu-
ally employ estimators of different scales to produce re-
Figure 1. Performance comparison on the RealBlur-J [23] test
dataset in terms of PSNR and GMACs. Our proposed MLWNet
achieves superiority in comparison with other state-of-the-arts.
stored images gradually [20, 21, 27]. However, not only
is the extracted semantic information not sufficiently rep-
resentative, but the use of the same complexity at different
scales may lead to redundant computations; More advanced
multi-scale algorithms [3] use a global encoder-decoder,
which significantly reduces the algorithm’s runtime, mul-
tiple upsampling and downsampling leads to insufficient
restoration of detailed information, and therefore such algo-
rithms require the introduction of additional fusion modules
to encode input images for fusion with deeper semantics.
In addition, existing multi-scale algorithms use multiple-
input and multiple-output (MIMO) architectures, which re-
quire the introduction of manually constructed downsam-
pled image pairs, and usually employ simple interpolation
algorithms to generate low-resolution images for the sake
of efficiency, which is obviously not reliable.
We note that the gradual insertion of images used by ex-
isting coarse-to-fine algorithms is redundant. Referring to
numerous algorithms [6, 8, 15, 17, 26, 31] that are widely
used in downstream tasks, the network starts with a single
input and also extracts features at different scales. These
1
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2733
features can be aggregated by simple feature fusion to ul-
timately obtain competitive results. That is, a DNN itself
possesses the ability to learn effective features at different
scales. This inspired us to design a multi-scale architecture
that retains an original image as input and sequentially re-
stores images at each scale in output stage. This is not only
more theoretically sound, but also eliminates the time re-
quired to generate pairs of images of different resolutions,
as well as the huge complexity increase brought about by
fusion modules between RGB domain and deep features.
The coarse-to-fine algorithm has another inherent defect.
During progressive restoration, the solution of the upper-
level problem takes the solution of the lower-level as ini-
tialization [9]. Although this reduces the difficulty of solv-
ing the upper-level problem, due to the smaller resolution of
lower-level spatial, the features transmitted upwards are se-
mantically precise but spatially ambiguous, thus the restora-
tion ability of multi-scale network in spatial details is lim-
ited. Hence, there is an urgent need to improve the quality
of restoration of high-frequency detail part. A simple ap-
proach is to introduce a frequency domain transform as an
alternative to a spatial domain transform. This would pro-
vide the algorithm with a choice and direct its attention to
different frequencies. In this paper, we consider using the
discrete wavelet transform for the following reasons:
1. Many recent state-of-the-art deblurring algorithms [10,
19, 42] have introduced the discrete Fourier transform
(DFT) as a frequency prior, which provides informa-
tion that helps the algorithm to identify and select high-
and low-frequency components that need to be pre-
served during restoration. Compared to DFT, the dis-
crete wavelet transform (DWT) is better suited to deal
with images containing more abrupt signals [5].
2. Realistic blur and synthetic blur have significant distri-
butional differences [24]. In the real world, due to the
short exposure time of a camera, realistic blur has a spe-
cific directionality [29], i.e., the blur trajectory is region-
ally continuous; conversely, synthetic blur has an unnat-
ural and discontinuous trajectory. To fully utilize the
potential deblurring guidance brought by this trajectory
continuity, we use 2D-DWT to reveal blur directionality,
distinguish changes in the blur signal along different di-
rections, and provide the algorithm with a reliable basis
for deblurring through adaptive learning.
To make 2D-DWT fit the data distribution and feature
layer space more closely, we implemented 2D-DWT with
an adaptive data distribution by using group convolution,
transferring feature space from the spatial domain into the
wavelet domain, generating sub-signals with different fre-
quency features and different directional features, and then
constructing wavelet losses for them in order to constrain
them by self-supervision. Experimental results demonstrate
that the proposed method has advanced performance interms of accuracy and efficiency (Fig. 1).
In summary, our contributions are as follows:
• We propose a single-input and multiple-output(SIMO)
multi-scale baseline for progressive image deblur, which
reduces the complexity of existing multi-scale deblurring
algorithms and improves the overall efficiency of image
restoration networks.
• We construct a learnable discrete wavelet transform
node(LWN) to provide reliable frequency and direction
selection for the algorithm, which promotes the algo-
rithm’s restoration of the high-frequency components of
edges and details.
• For the network to work better, reasonable multi-scale
loss is proposed to guide pixel-by-pixel and scale-by-
scale restoration. We also created reasonable self-
supervised losses for the learnable wavelet transform to
limit the learning direction of the wavelet kernel.
• We demonstrate the effectiveness of our algorithm under
several motion blur datasets, especially real datasets, and
obtain highly competitive results.
2. Related Work
Single-scale Deblurring Algorithm. Image deblurring
have developed rapidly in recent years [2, 10, 11, 14, 39,
40]. To preserve richer image details, Kupyn et al. [11]
viewed deblurring as a special case of image-to-image con-
version, and for the first time utilized a GAN to recover im-
ages with richer details. To provide a high-quality and sim-
ple baseline, Chen et al. [2] proposed a nonlinear activation-
free network that obtained excellent results. Zamir et
al. [38] proposed a transformer with an encoder–decoder
architecture that can be applied to higher-resolution im-
ages and utilizes cross-channel attention. Kong et al. [10]
utilized a domain-based self-attention solver to reduce the
transformer complexity and suppress artefacts. However,
such single-scale algorithms estimate complex restoration
problems directly and may require the design of restorers
of higher complexity, which is suboptimal in terms of effi-
ciency [9].
Multi-scale Deblurring Algorithm. Multi-scale motion
deblur methods aim to achieve progressive recovery using
a multi-stage or multi-sub-network approach. The multi-
scale algorithm previously used in the field of image deblur-
ring is based on MIMO [3, 13, 20, 21, 27]. Recently, Cho
et al. [3] proposed MIMO-UNet, which employs a multi-
output single decoder as well as a multi-input single encoder
to simulate a multi-scale architecture consisting of stacked
networks, thereby greatly simplifying complexity. Zamir et
al. [37] designed a multi-stage progressive image recovery
network to simplify the information flow between multiple
sub-networks. As shown in Sec. 3.1, our proposed multi-
scale approach further simplifies the structure of multi-scale
networks and better balances the accuracy and complexity.
2
2734
Figure 2. The overall architecture of the proposed MLWNet, the SEB is a simple module designed with reference [2], the WFB and
WHB apply the LWN that implements the learnable 2D-DWT. In training phase, supervised learning is performed using Lmulti and self-
supervised restraint of the wavelet kernel is performed using Lwavelet . In testing phase, only the highest scale restored images is output.
Application of Frequency in Deblurring. Frequency is an
important property of images, in recent years frequency do-
main has been introduced into numerous DNNs to be solved
for various tasks [10, 22, 25, 32, 35, 41]. Mao et al. [19]
proposed DeepRFT, which introduced a residual module
based on the Fourier transform into an advanced algorithm.
Kong et al. [10] proposed FFTformer, which introduced the
Fourier transform into a feed-forward network to effectively
utilize different frequency information. Zou et al. [42] pro-
posed SDWNet, which utilized frequency domain informa-
tion as a complement to spatial domain information. Differ-
ent from these methods, we introduce a learnable 2D-DWT
capable of adapting to data distribution and feature space,
more suitable not only for dealing with digital images rich
in mutation signals, but also for dealing with real blurring.
3. Proposed Method
Our proposed MLWNet(Fig. 2) aims to explore an efficient
multi-scale architectural approach to achieve high-quality
blind deblurring of a single image. First, we designed a
novel and highly scalable SIMO multi-scale baseline to ad-
dress the performance bottleneck faced by partially multi-
scale networks. It takes a single image as an input and grad-
ually generates a series of sharp images from the bottom
up. Then, we propose a learnable wavelet transform node
(LWN) for image deblurring, and it enhances the proposed
algorithm’s ability to restore detail information.
3.1. Multi-scale Baseline
Our multi-scale network maintains an encoder-decoder ar-
chitecture, but retains the original image with the highestresolution as input, and restores sharp images of various
scales sequentially in the output stage. In this way, we
eliminate the complexity of the fusion module when dealing
RGB images of different scales and with deep semantics.
For ease of understanding, we will introduce here the En-
coder phase, multi-scale semantic Fusion phase (hereafter
referred to as Fusion phase), and Decoder phase sequen-
tially. As shown in Fig. 2, the Encoder stage consists of sev-
eral gray Simple Encoder Blocks(SEB), where information
flows top-down for feature extraction; the Fusion stage con-
sists of several green Wavelet Fusion Blocks(WFB), where
information flows bottom-up for semantic fusion at differ-
ent scales; and the Decoder stage consists of several pur-
ple Wavelet Head Blocks(WHB), and the bottom-up flow
of information enables gradual restoration. Compared with
WHB, SEB replaces LWN with a 1×1conv for channel
scaling, and a 3×3depth-conv for feature extraction.
The Encoder stage is responsible for full feature extrac-
tion, downsampling feature maps once after each block, and
then passing the feature maps to the Fusion stage or the De-
coder stage as needed, whose outputs Ei
outto block of the
i−thlayer can be denoted as:
Ei
out=ϕi(embed (x)), i =imax
ϕi 
Ei−1
out
, otherwise(1)
where ϕirepresent the block of corresponding layer of En-
coder. The Fusion stage adapts and fuses information from
different scales of semantics produced in Encoder to gen-
erate intermediate representations with deep semantics and
shallow details. The Fusion stage’s outputs Fi
outto the
block of i−thlayer is expressed as in Eq. 2, where δi
3
2735
represents the block of the corresponding layer of Fusion.
Fi
out=δi 
Ei
out+Ei−1
out
, i =imin
δi 
Fi−1
out+Ei
out
, otherwise(2)
The Decoder stage utilizes the information passed by both
the Encoder and Fusion stages to progressively upsample
the feature maps and generating pre-output feature maps at
each scale separately. Output Di
outto the block of the i−th
layer is denoted as in Eq. 3. Here ξirepresents the block of
the corresponding layer of the Decoder.
Di
out=

ξi 
Ei
out
, i =imin
ξi 
Di−1
out+Fi
out
, i > i minandi < i max
ξi 
Di−1
out+Fi−1
out+Ei
out
, i =imax
Each block in the Decoder is followed by a 3×3con-
volution to generate a restored image at each scale. To im-
prove inference efficiency, all but the highest-scale outputs
are generated in the training phase to compute the multi-
scale loss. We note that this design also have a similar role
with auxiliary head [36] in facilitating the model’s learning
of the recovered uniform patterns.
Figure 3. (a)The process of learnable 2D-wavelet convolution.
(b)The construction process of the N×N2D-wavelet kernel.
3.2. Learnable Discrete Wavelet Transform
To alleviate the defects of multi-scale architectures in the
recovery of detail information, we design LWN and con-
structed the WFB and WHB based on it. Unlike the
Fourier transform, the wavelet transform has adaptive time-
frequency resolution, and performs excellently on digi-
tal images rich in mutation signal. Its use in conjunc-
tion with DNNs has gained increasing widespread atten-
tion [7, 16, 18, 34].For ease of understanding, the 1D-DWT case is given
here first. For the input discrete signal t, given the wavelet
function ψj,k(t) = 2j
2ψ(2jt−k)for scaling factor j, time
factor k, and scale function ϕj,k(t) = 2j
2ϕ(2jt−k), the
decomposition of the input signal tatj0is given by:
f(t) =X
j>j 0X
kdj,kψj,k(t) +X
kcj0,kϕj0,k(t) (3)
In this process, dj,k=⟨f(t), ψj,k(t)⟩represents the de-
tail coefficients (i.e., high-frequency components), cj0,k=
⟨f(t), ϕj0,k(t)⟩represents the approximation coefficients
(i.e., low-frequency components) so that the input signal
can be disassembled step by step into a rich wavelet domain
signal. In order to use wavelet transform in DNN, we in-
troduce the analysis vectors ⃗ a0[k] =D
1√
2ϕ(t
2), ϕ(t−k)E
,
⃗ a1[k] =D
1√
2ψ(t
2), ϕ(t−k)E
to represent the high and low
frequency filters respectively, then:
cj+1,p=X
k⃗ a0[k−2p]cj,k
dj+1,p=X
k⃗ a1[k−2p]cj,k(4)
According to Eq. 4, the decomposition of the original
signal over the wavelet basis can be viewed as a recursive
convolution of the original signal with a specific filter using
a step size of 2. Similarly, the inverse wavelet transform can
be realized by transposed convolution of two synthetic vec-
tors⃗ s0and⃗ s1. We extend the above situation to 2D by ef-
ficiently extracting axial high-frequency information using
2D discrete wavelet transform (2D-DWT), and designing
learnable methods to make it adaptive to the data distribu-
tion and feature layers. Fig. 3(b) presents the construction
process of the forward wavelet convolution kernel in 2D,
which can be expressed as:
Fll=⃗ a0×⃗ a0T,Flh=⃗ a0×⃗ a1T
Fhl=⃗ a1×⃗ a0T,Fhh=⃗ a1×⃗ a1T
Kw=cat(Fll,Flh,Fhl,Fhh)(5)
We set ⃗ a0and⃗ a1as learnable filters; Fll,Flh,Fhl, and
Fhhare low-frequency, horizontal high-frequency, vertical
high-frequency, and diagonal high-frequency convolution
operators obtained from vector outer products; The wavelet
kernel Kwis spliced from above convolution operators.
Then we implement learnable wavelet transform as a
group convolution like Fig. 3(a), given a set of input fea-
ture maps Xin∈(C, H, W ), its projection in wavelet
domain Xout∈ 
4C,H
2,W
2
(low-frequency, horizontal
high-frequency, vertical high-frequency, and diagonal high-
frequency component) is generated through the wavelet
4
2736
convolution. Similarly, the construction method of the in-
verse kernel and the forward propagation can be easily de-
duced, since they are inverse processes of each other.
Then, a natural question can then be posed: how to en-
sure the correctness of the wavelet kernel learning and en-
sure that it does not degrade into a general group convolu-
tion and cause performance degradation?
A common practice is to introduce the principle of per-
fect reconstruction [16, 34] to constrain adaptive wavelets,
for a complex z∈C, given a filter ⃗ xthat applies to the
z-transform, its z-transform can be expressed as X(z) =P
n∈Z⃗ x(n)z−n. Then ⃗ a0,⃗ a1,⃗ s0,⃗ s1can be obtained as their
corresponding z-transforms A0,A1,S0,S1, which must then
be satisfied if perfect reconstruction is desired:
A0(−z)S0(z) +A1(−z)S1(z) = 0 (6)
A0(z)S0(z) +A1(z)S1(z) = 2 (7)
Eq. 6 is known as alaising cancellation condition, which
is used to the cancellation of the aliasing effects arising from
the downsampling. We will give a detailed derivation of
Eq. 6 and Eq. 7 in the Appendix.
After wavelet convolution, to achieve frequency restora-
tion in the wavelet domain, the input Xwavelet domain
component was separated into a separate dim, we use
depth-wise convolution with an expansion factor of rfor
wavelet domain feature extraction and transformation af-
ter the wavelet positive transform, and 1×1convolution
for channel expansion and scaling, and finally a learnable
wavelet inverse transform is used to reduce the wavelet do-
main feature maps to the spatial domain for output, which
constitutes the LWN. We follow the rules of [2] and de-
sign structurally similar Wavelet Head Block (WHB) and
Wavelet Fusion Block (WFB), both with LWN as an im-
portant base module, but used for semantic aggregation
and multiscale output at different scales, respectively; thus,
WHB adds the recovery convolution branch after WFB.
3.3. Loss
Multi-scale Loss . Follow PSNR loss [2], we propose multi-
scale loss function as the main loss of the algorithm for
calculating the pixel difference between restored image and
GT at each scale:
Lmulti(x, y) =KX
k=1wk× Lpsnr(xk, yk) (8)
where x,ydenote the output and GT, respectively, kde-
notes the downsampling level, wkrepresents the weights
under the corresponding scale. Our design of wkis based
on the simple finding that we cannot produce absolutely ac-
curate downsampled clear images, and that the accuracy gapis progressively enlarged with decreasing scales. To ensure
that lower scale output layers do not negatively affect the
final output, we empirically set the loss weight wkto1
kfor
the corresponding k-scale.
Wavelet Loss . Both Eq. 6 and Eq. 7 can be easily converted
losses optimized toward a minimum of 0, here we use the
mean square error. Then, based on the convolvable nature
of the ztransform, it is possible to construct convolutionally
equivalent substitutions for polynomial multiplications for
the perfect reconstruction condition:
Lwavelet (θi) = N−1X
k(⟨a0, s0⟩k+⟨a1, s1⟩k)−ˆV⌊N
2⌋!2
= N−1X
kD
(−1)ka0, s0E
k+
(−1)ka1, s1
k!2
(9)
where kdenotes the position of the filter when it performs
the convolution, ˆVis a vector with a center position value
of two, and θidenotes the constructed filter
Overall Loss .Based on the multi-scale loss and wavelet
loss, the loss function used in this study is:
Ltotal(x, y) =Lwavelet (θi) +Lmulti(x, y) (10)
4. Experimental Results
4.1. Datasets and Implementation Details
We evaluate our method on the realistic datasets Real-
Blur [23] and RSBlur [24], as well as the synthetic dataset
GoPro [20], they are all composed of blurry-sharp image
pairs. During training we use AdamW optimizer ( α= 0.9
andβ= 0.9) for a total of 600K iterations. The initial
value of the learning rate is 10−3and is updated with co-
sine annealing schedule. The patch size is set to 256 × 256
pixels and we followed the training strategy of [28, 38]. For
data augmentation, we only use random flips and rotations.
More experimental results can be found in the Appendix.
4.2. Comparison with State-of-the-Art Methods
We have compared our method with several advanced de-
blurring methods and use the PSNR and SSIM to evaluate
the quality of restored images.
Evaluations on the RealBlur dataset. Since our proposed
method is driven by real-world deblurring, we first conduct
comparisons on RealBlur [23]. The quantitative analysis
results are shown in Tab. 1, the PSNR value of our method
is 0.91dB and 0.49dB higher than state-of-the-art GRL on
the RealBlur-J and RealBlur-R datasets respectively. Com-
pared to other multi-scale architectures, our method has
fewer model computational and running time while the per-
formance is better. Fig. 4 shows visual comparison on
RealBlur-J dataset, our method obtains restored text with
5
2737
Figure 4. Visual comparisons on the RealBlur-J dataset [23]. The proposed method generates an image with clearer characters.
MethodRealBlur-J RealBlur-R Avg.
runtime PSNR SSIM PSNR SSIM
DeblurGAN-v2 [12] 29.69 0.870 36.44 0.935 0.04s
SRN [27] 31.38 0.909 38.65 0.965 0.07s
MPRNet [37] 31.76 0.922 39.31 0.972 0.09s
SDWNet [42] 30.73 0.896 38.21 0.963 0.04s
MIMO-UNet+ [3] 31.92 0.919 - - 0.02s
MIMO-UNet++ [3] 32.05 0.921 - - -
DeepRFT+ [19] 32.19 0.931 39.84 0.972 0.09s
BANet [29] 32.00 0.923 39.55 0.971 0.06s
BANet+ [29] 32.42 0.929 39.90 0.972 0.12s
Stripformer [28] 32.48 0.929 39.84 0.974 0.04s
MSSNet [9] 32.10 0.928 39.76 0.972 0.06s
MSDI-Net [13] 32.35 0.923 - - 0.06s
MAXIM-3S [30] 32.84 0.935 39.45 0.962 -
FFTformer [10] 32.62 0.933 40.11 0.973 0.13s
GRL-B [14] 32.82 0.932 40.20 0.974 1.28s
MLWNet-S 33.02 0.933 - - 0.04s
MLWNet-B 33.84 0.941 40.69 0.976 0.05s
Table 1. Quantitative evaluations on the RealBlur dataset [23]. The
experimental results were trained under the corresponding datasets
respectively, and average runtime is tested on 256 ×256 patchs.
richer edge details, while achieving the best contrast, sharp-
ness, brightness, and structural details of the image.
Evaluations on the RSBlur dataset. We further conduct
experiments on the latest real-world deblurring dataset RS-
Blur [24] and follow the protocol of this dataset for fair
comparison. Tab. 2 summarizes the quantitative evalua-
tion results of our method with advanced algorithms. The
proposed MLWNet achieved the highest PSNR and SSIM,
which were 34.94 and 0.880 respectively. In addition, the
model we trained on RSBlur dataset also achieved the best
results on RealBlur-J dataset. We show some visual com-
parisons in Fig. 5. We note that our method outperforms
other methods in low-light blurry scenes, proving that theMethodRSBlur RealBlur-J
PSNR SSIM PSNR SSIM
SRN [27] 32.53 0.840 29.86 0.886
MIMO-Unet [3] 32.73 0.846 29.53 0.876
MIMO-Unet+ [3] 33.37 0.856 29.99 0.889
MPRNet [37] 33.61 0.861 30.46 0.899
Restormer [38] 33.69 0.863 30.48 0.891
Uformer-B [33] 33.98 0.866 30.37 0.899
SFNet [4] 34.35 0.872 30.26 0.897
MLWNet-B 34.94 0.880 30.53 0.905
Table 2. Quantitative evaluations trained on the RSBlur
dataset [24], the RealBlur-J dataset was used for testing only.
Method[23]→[24]MACs(G)PSNR SSIM
DeblurGAN-v2 [12] 30.15 0.766 42.0
MPRNet [37] 29.56 0.785 760.8
MIMO-UNet+ [3] 29.69 0.792 154.4
BANet [29] 30.19 0.806 263.9
BANet+ [29] 30.24 0.809 588.7
MSSNet [9] 29.86 0.806 154.0
FFTformer [10] 29.70 0.787 131.8
MLWNet-B 30.91 0.818 108.2
Table 3. Quantitative evaluation for generalizability shows the re-
sults of models trained on the RealBlur-J dataset and tested on the
RSBlur dataset, MACs are measured on 256 × 256 patches.
proposed method makes textures and structures clearer.
In addition, we evaluate the RSBlur dataset using models
trained only on RealBlur-J to fairly compare the generaliza-
tion of methods to real scenarios. As shown in Tab. 3, our
method produced results with highest PSNR value of 30.91.
The results of Tab. 2 and Tab. 3 show that our model has a
better generalization ability.
Evaluations on the GoPro dataset. We conduct a exten-
6
2738
Figure 5. Visual comparisons on the RSBlur dataset [24]. The deblurring performance of the proposed method in low-light is impressive,
The recovery of characters and texture structures far exceeds other advanced methods.
Figure 6. Visual comparisons on the GoPro dataset [20]. Our method better preserves texture information without sharpening.
sive comparison with advanced algorithms on GoPro [20].
Tab. 4 shows the quantitative evaluation results. The gap
between our method and the state-of-the-art FFTformer is
only 0.001 in SSIM value, while the running time is reduced
by 60%. We show a visual comparison on the GoPro dataset
in Fig. 6. We can see that our method recovers blurred tex-
tures well without sharpening. For the reasons why the pro-
posed MLWNet does not achieve optimal performance in
synthetic blurry, we conducted a detailed analysis in Sec. 5.
5. Analysis and Discussion
In this section, we provide a more in-depth analysis and
show the contribution of each component of the proposed
method. We perform 20w iterations on the RealBlur-J
dataset for abtation with a batch size of 8 to train our method
using a version of the model with a width of 32, and the re-
sults are shown in Tab. 6.
Single-scale vs. Multi-scale . Given that each node of the
baseline network is composed of SEB, we have deformed
the multi-scale and single-scale according to the architec-
ture. Tab. 5 shows that the multi-scale architecture improves
the PSNR and SSIM values to varying degrees, proving thatMethodGOPROAvg.runtimePSNR SSIM
DeblurGAN-v2 [12] 29.55 0.934 0.04s
SRN [27] 30.26 0.934 0.07s
DMPHN [39] 31.20 0.945 0.21s
SDWNet [42] 31.26 0.966 0.04s
MPRNet [37] 32.66 0.959 0.09s
MIMO-UNet+ [3] 32.45 0.957 0.02s
DeepRFT+ [19] 33.23 0.963 0.09s
MAXIM-3S [30] 32.86 0.961 -
Stripformer [28] 33.08 0.962 0.04s
MSDI-net [13] 33.28 0.964 0.06s
Restormer [38] 33.57 0.966 0.08s
NAFNet [2] 33.69 0.967 0.04s
FFTformer [10] 34.21 0.969 0.13s
GRL-B [14] 33.93 0.968 1.28s
MLWNet-B 33.83 0.968 0.05s
Table 4. Quantitative evaluations trained and tested on the GoPro
dataset [20]. Our proposed MLWNet obtains competitive results
with a combination of time efficiency and accuracy.
7
2739
the SIMO strategy we employ helps to eliminate motion
blur to some extent. As shown in Sec. 3.1, we use SIMO
for coarse-to-fine restoration, and only use multiple outputs
to calculate multi-scale losses during training.
WFB and WHB . We default to using WFB and WHB via
Lwavelet statute in this section, we also proved the effec-
tiveness of LWN, because LWN is a subpart of the first
two. After applying them, the PSNR index improved by
0.25dB compared to the multi-scale baseline, which shows
that LWN and its extension modules can easily help multi-
scale algorithms improve detail recovery capabilities. Fig. 7
shows the four types of feature maps generated after for-
ward learnable wavelet convolution. We can see that the
high and low frequency information of the input feature
map are mixed together, and after wavelet group convolu-
tion, the network exerts different attention on the feature
information in different directions or frequencies.
Effectiveness of Lwavelet . By adding unreduced WFB and
WHB to the baseline without using wavelet loss, the per-
formance improvement of the baseline is minimal and does
not break the bottleneck of multiscale detail recovery, which
suggests that the wavelet convolution degenerates into a
general group convolution that only serves to deepen the
network, proving that merely deepening the network is in-
effective for the restoration of detail information.
Method SISO MIMO SIMO
PSNR/SSIM 32.29 /0.924 32.19/0.928 32.37 /0.929
MACs(G) 19.24 21.83 19.29
Table 5. Comparison in various input and output modes.
SIMO WFB WHB Lwavelet PSNR SSIM MACs(G)
✓ 32.37 0.929 19.29
✓ ✓ ✓ 32.40 0.928 28.21
✓ ✓ ✓ 32.49 0.928 25.28
✓ ✓ ✓ 32.57 0.929 22.22
✓ ✓ ✓ ✓ 32.62 0.931 28.21
Table 6. Ablation study on components of the proposed MLWNet.
We set the baseline network to use SEB in its entirety, and models
that do not use SIMO will represent single scales using SISO.
input with Fll with Fhl with Flh with Fhh
Figure 7. Feature maps representing high-and low-frequency com-
ponents generated after learnable wavelet convolution. Zoom in on
the screen for the best view.
Limitations and analysis . It is observed from Tab. 1-3
that our method fails to achieve the same expected high
performance with synthetic blur as it does with realistic
blur. We try to analyze the reasons for this phenomenon
via dataset comparison and experimental results. First, the
average frame synthesis method used in the synthetic blurdataset leads to unnatural discontinuous blur trajectories,
thereby introducing strange high-frequency information in-
terference (see Fig. 8). Second, synthetic data tends to pro-
duce an unnatural mixture of high and low frequencies. It
is easy to mix with the average color area at the edge of the
texture, causing the confusion of high-low frequency infor-
mation.
(a) Realistic Blur (b) Synthetic Blur
Figure 8. The difference between realistic blur (a) and synthetic
blur (b). In the green box, the synthetic blur appears with color
averaging resulting in high and low frequency confusion, and in
the blue box has unnatural discontinuous trajectories.
Noise Level L1 L2 L3 L4 L5
GoPro 34.81 34.66 33.76 33.19 32.63
RealBlur-J 33.92 33.81 33.97 33.93 33.54
Table 7. Performance comparison at different noise difference lev-
els, where L3 contains the noise difference mean.
Finally, there is a large difference in noise levels between
synthetic blur and real blur. We follow [1] to calculate
the noise level of each testset, and divided the noise dif-
ference between clear images and blurred images of GoPro
and RealBlur-J into 5 levels for separate testing. As shown
in Tab. 7, we note that as the noise difference increases, our
method maintains good performance on RealBlur-J, while
the performance drops fast on GoPro. This shows that our
method is robust to the noise of real blur, and that there are
differences in the noise between synthetic blur and real blur.
6. Conclusion
In this paper, we propose a SIMO-based multi-scale archi-
tecture to achieve efficient motion deblurring. We have
developed a learnable discrete wavelet transform module,
which not only improves the algorithm’s ability to recover
details, but is also more applicable to the real world. In ad-
dition, we construct a reasonable multi-scale loss to guide
the recovery of blurred images pixel by pixel and scale by
scale, and constrain the learning direction of the wavelet
kernel with self-supervised loss to achieve better image de-
blurring. Through extensive experiments on multiple real
and synthetic datasets, we demonstrate that it outperforms
existing state-of-the-art methods in terms of quality and ef-
ficiency of image restoration, especially with optimal de-
blurring performance and generalization in real scenes.
8
2740
References
[1] Guangyong Chen, Fengyuan Zhu, and Pheng Ann Heng.
An efficient statistical method for image noise level estima-
tion. In Proceedings of the IEEE International Conference
on Computer Vision , pages 477–485, 2015. 8
[2] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun.
Simple baselines for image restoration. In European Confer-
ence on Computer Vision , pages 17–33. Springer, 2022. 1, 2,
3, 5, 7
[3] Sung-Jin Cho, Seo-Won Ji, Jun-Pyo Hong, Seung-Won Jung,
and Sung-Jea Ko. Rethinking coarse-to-fine approach in sin-
gle image deblurring. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 4641–4650,
2021. 1, 2, 6, 7
[4] Yuning Cui, Yi Tao, Zhenshan Bing, Wenqi Ren, Xinwei
Gao, Xiaochun Cao, Kai Huang, and Alois Knoll. Selective
frequency network for image restoration. In International
Conference on Learning Representations, ICLR , 2023. 6
[5] Ingrid Daubechies. The wavelet transform, time-frequency
localization and signal analysis. IEEE transactions on infor-
mation theory , 36(5):961–1005, 1990. 2
[6] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn:
Learning scalable feature pyramid architecture for object de-
tection. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 7036–7045,
2019. 1
[7] Wooseok Ha, Chandan Singh, Francois Lanusse, Srigokul
Upadhyayula, and Bin Yu. Adaptive wavelet distillation from
neural networks through interpretations. Advances in Neural
Information Processing Systems , 34:20669–20682, 2021. 4
[8] Bumsoo Kim, Jonghwan Mun, Kyoung-Woon On, Minchul
Shin, Junhyun Lee, and Eun-Sol Kim. Mstr: Multi-scale
transformer for end-to-end human-object interaction detec-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 19578–19587,
2022. 1
[9] Kiyeon Kim, Seungyong Lee, and Sunghyun Cho. Mssnet:
Multi-scale-stage network for single image deblurring. In
European Conference on Computer Vision , pages 524–539.
Springer, 2022. 1, 2, 6
[10] Lingshun Kong, Jiangxin Dong, Jianjun Ge, Mingqiang Li,
and Jinshan Pan. Efficient frequency domain-based trans-
formers for high-quality image deblurring. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 5886–5895, 2023. 1, 2, 3, 6, 7
[11] Orest Kupyn, V olodymyr Budzan, Mykola Mykhailych,
Dmytro Mishkin, and Ji ˇr´ı Matas. Deblurgan: Blind mo-
tion deblurring using conditional adversarial networks. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 8183–8192, 2018. 2
[12] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang
Wang. Deblurgan-v2: Deblurring (orders-of-magnitude)
faster and better. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 8878–8887,
2019. 1, 6, 7
[13] Dasong Li, Yi Zhang, Ka Chun Cheung, Xiaogang Wang,
Hongwei Qin, and Hongsheng Li. Learning degradation rep-resentations for image deblurring. In European Conference
on Computer Vision , pages 736–753. Springer, 2022. 1, 2, 6,
7
[14] Yawei Li, Yuchen Fan, Xiaoyu Xiang, Denis Demandolx,
Rakesh Ranjan, Radu Timofte, and Luc Van Gool. Effi-
cient and explicit modelling of image hierarchies for image
restoration. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 18278–
18289, 2023. 2, 6, 7
[15] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyra-
mid networks for object detection. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2117–2125, 2017. 1
[16] Pengju Liu, Hongzhi Zhang, Wei Lian, and Wangmeng Zuo.
Multi-level wavelet convolutional neural networks. IEEE Ac-
cess, 7:74973–74985, 2019. 4, 5
[17] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
Path aggregation network for instance segmentation. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition , pages 8759–8768, 2018. 1
[18] Wei Liu, Qiong Yan, and Yuzhi Zhao. Densely self-guided
wavelet network for image denoising. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops , pages 432–433, 2020. 4
[19] X Mao, Y Liu, W Shen, Q Li, and Y Wang. Deep resid-
ual fourier transformation for single image deblurring. arxiv
2021. arXiv preprint arXiv:2111.11745 , 2021. 2, 3, 6, 7
[20] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene
deblurring. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3883–3891,
2017. 1, 2, 5, 7
[21] Dongwon Park, Dong Un Kang, Jisoo Kim, and Se Young
Chun. Multi-temporal recurrent neural networks for progres-
sive non-uniform single image deblurring with incremental
temporal training. In European Conference on Computer Vi-
sion, pages 327–343. Springer, 2020. 1, 2
[22] Zequn Qin, Pengyi Zhang, Fei Wu, and Xi Li. Fcanet:
Frequency channel attention networks. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 783–792, 2021. 3
[23] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun Cho.
Real-world blur dataset for learning and benchmarking de-
blurring algorithms. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XXV 16 , pages 184–201. Springer, 2020.
1, 5, 6
[24] Jaesung Rim, Geonung Kim, Jungeon Kim, Junyong Lee,
Seungyong Lee, and Sunghyun Cho. Realistic blur synthesis
for learning image deblurring. In European conference on
computer vision , pages 487–503. Springer, 2022. 2, 5, 6, 7
[25] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
Naejin Kong, Harshith Goka, Kiwoong Park, and Victor
Lempitsky. Resolution-robust large mask inpainting with
fourier convolutions. In Proceedings of the IEEE/CVF winter
9
2741
conference on applications of computer vision , pages 2149–
2159, 2022. 3
[26] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet:
Scalable and efficient object detection. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10781–10790, 2020. 1
[27] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji-
aya Jia. Scale-recurrent network for deep image deblurring.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 8174–8182, 2018. 1, 2, 6, 7
[28] Fu-Jen Tsai, Yan-Tsung Peng, Yen-Yu Lin, Chung-Chi Tsai,
and Chia-Wen Lin. Stripformer: Strip transformer for fast
image deblurring. In European Conference on Computer Vi-
sion, pages 146–162. Springer, 2022. 1, 5, 6, 7
[29] Fu-Jen Tsai, Yan-Tsung Peng, Chung-Chi Tsai, Yen-Yu Lin,
and Chia-Wen Lin. Banet: a blur-aware attention network
for dynamic scene deblurring. IEEE Transactions on Image
Processing , 31:6789–6799, 2022. 1, 2, 6
[30] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,
Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxim:
Multi-axis mlp for image processing. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5769–5780, 2022. 6, 7
[31] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-
Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets
new state-of-the-art for real-time object detectors. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 7464–7475, 2023. 1
[32] Luyuan Wang and Yankui Sun. Image classification using
convolutional neural network with wavelet domain inputs.
IET Image Processing , 16(8):2037–2048, 2022. 3
[33] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang
Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general
u-shaped transformer for image restoration. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 17683–17693, 2022. 6
[34] Moritz Wolter and Jochen Garcke. Adaptive wavelet pool-
ing for convolutional neural networks. In International Con-
ference on Artificial Intelligence and Statistics , pages 1936–
1944. PMLR, 2021. 4, 5
[35] Yanchao Yang and Stefano Soatto. Fda: Fourier domain
adaptation for semantic segmentation. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 4085–4095, 2020. 3
[36] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu,
Chunhua Shen, and Nong Sang. Bisenet v2: Bilateral net-
work with guided aggregation for real-time semantic seg-
mentation. International Journal of Computer Vision , 129:
3051–3068, 2021. 4
[37] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Multi-stage progressive image restoration. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 14821–14831, 2021. 1, 2, 6,
7
[38] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-
nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.Restormer: Efficient transformer for high-resolution image
restoration. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 5728–5739,
2022. 2, 5, 6, 7
[39] Hongguang Zhang, Yuchao Dai, Hongdong Li, and Piotr Ko-
niusz. Deep stacked hierarchical multi-patch network for im-
age deblurring. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5978–
5986, 2019. 2, 7
[40] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn
Stenger, Wei Liu, and Hongdong Li. Deblurring by realis-
tic blurring. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2737–
2746, 2020. 2
[41] Yijie Zhong, Bo Li, Lv Tang, Senyun Kuang, Shuang Wu,
and Shouhong Ding. Detecting camouflaged object in fre-
quency domain. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
4504–4513, 2022. 3
[42] Wenbin Zou, Mingchao Jiang, Yunchen Zhang, Liang Chen,
Zhiyong Lu, and Yi Wu. Sdwnet: A straight dilated net-
work with wavelet transformation for image deblurring. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 1895–1904, 2021. 1, 2, 3, 6, 7
10
2742
