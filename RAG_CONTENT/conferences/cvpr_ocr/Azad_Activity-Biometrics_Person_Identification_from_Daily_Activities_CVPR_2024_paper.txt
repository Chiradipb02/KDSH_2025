Activity-Biometrics: Person Identification from Daily Activities
Shehreen Azad Yogesh Singh Rawat
Shehreen.Azad@ucf.edu yogesh@ucf.edu
Center for Research in Computer Vision, University of Central Florida
Abstract
In this work, we study a novel problem which focuses
on person identification while performing daily activities.
Learning biometric features from RGB videos is challeng-
ing due to spatio-temporal complexity and presence of ap-
pearance biases such as clothing color and background. We
propose ABNet, a novel framework which leverages dis-
entanglement of biometric and non-biometric features to
perform effective person identification from daily activi-
ties. ABNet relies on a bias-less teacher to learn biometric
features from RGB videos and explicitly disentangle non-
biometric features with the help of biometric distortion. In
addition, ABNet also exploits activity prior for biometrics
which is enabled by joint biometric and activity learning.
We perform comprehensive evaluation of the proposed ap-
proach across five different datasets which are derived from
existing activity recognition benchmarks. Furthermore, we
extensively compare ABNet with existing works in person
identification and demonstrate its effectiveness for activity-
based biometrics across all five datasets. The code and
dataset can be accessed at: https://github.com/
sacrcv/Activity-Biometrics/
1. Introduction
Person identification is an important task with a wide range
of applications in security, surveillance, and various do-
mains where recognizing individuals across different lo-
cations or time frames is essential [41]. We have seen a
great progress in face recognition [2, 31], however sce-
narios exist where faces may not be visible, such as at
long distances, with uncooperative subjects, under occlu-
sion, or due to mask-wearing. This limitation prompts
the exploration of whole-body-based person identification
methods where most of the existing works are often re-
stricted to image-based approaches [4, 14, 43], overlooking
crucial motion patterns. Video-based methods for person
identification is comparatively recent area where most of
the work is focused on gait recognition; mostly silhouette-
Figure 1. Different approaches for person identification: (left)
samples for existing person identification problems such as face
recognition (top: Celeb-A[30]), whole body recognition (mid-
dle: Market-1501[45]), and gait recognition (bottom: CASIA-
B[42]). (right) we focus on person identification from daily ac-
tivities which presents more challenges beyond learning walking
or facial patterns. We show some samples from datasets we used
to study this problem; (top: NTU RGB-AB, middle: Charades-
AB, bottom: ACC-MM1-Activities1).
based [12, 13, 27] with some recent works on RGB frames
[26, 44]. However these works are mainly focused on walk-
ing style of individuals (see Figure 1).
In this work, we study a novel problem which focuses on
face-restricted person identification during routine activi-
ties. The current landscape of image-based and video-based
whole-body person identification methods predominantly
centers around analyzing human walking patterns from im-
ages or videos [15, 20, 33, 40]. However, in real-world sce-
narios, the individual requiring identification might not al-
ways be engaged in walking; instead, they could be involved
in various daily activities. It is crucial to acknowledge the
significance of capturing and understanding motion cues
that extend beyond simple walking patterns to ensure accu-
rate and reliable identification in diverse and complex situa-
tions. These activities may offer unique cues that can prove
instrumental in identifying individuals even without explicit
facial information, paving the way for diverse applications
in real-world scenarios, like increased surveillance in public
spaces, workplace security and productivity, assistance for
people requiring special needs, and smart home automation.
1The subjects consented to publication
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
287
Learning biometrics from videos of daily activities
presents several inherent challenges. Learning from such
diverse activities amplifies the difficulty in capturing essen-
tial biometrics features. Among the crucial challenges lies
the necessity to prioritize biometrics features while mitigat-
ing appearance biases present in RGB video frames, includ-
ing background variations, clothing color, and other exter-
nal factors. Striking a balance between extracting pertinent
biometrics cues and disregarding irrelevant appearance-
related biases is essential in developing robust and accurate
video-based biometrics identification methods.
We propose a novel framework ABNet, which addresses
some of these challenges and provides effective biometrics
representation for person identification from videos of daily
activities. It relies on two main components; 1) feature
disentanglement , and 2) joint activity-biometrics learning .
Feature disentanglement aims at avoiding appearance biases
while learning the biometrics features. It explicitly learns
biometrics and non-biometrics features with the help of, a)
distillation from bias-less teacher , and b) bias learning us-
ingbiometrics distortion . Joint activity-biometrics learning
provides activity prior for biometrics where the knowledge
of performed activity helps in person identification.
We present extensive evaluations on five different bench-
marks using several metrics comparing the proposed ap-
proach with several state-of-the-art person identification
methods including both image-based and video-based ap-
proaches. This comprehensive evaluation demonstrates the
effectiveness and superiority of our proposed method in
handling diverse datasets and scenarios for activity-based
biometrics. Our main contributions can be summarized as,
• We study a novel problem of person identification from
daily activities using RGB videos.
• We propose a simple and novel strategy to disentangle
biometrics and non-biometrics features from videos for
person identification.
• We show the benefits of activity-prior for biometrics.
• We present several benchmarks to study this problem;
these datasets are dervied from existing activity recogni-
tion datasets specifically curated for person identification.
2. Related work
Image-based identification: Most of the existing person
identification methods use image-based approach [4, 6, 19,
21, 39, 40]. Moreover, most of these methods are designed
towards learning better features in-terms of body shape,
clothes, appearance etc. In recent years, learning cloth in-
variant features is found to be a promising direction in per-
son identification with several works trying to address this
issue. For example, one of the most popular person identi-
fication approach, CAL [15] uses advarsarial loss to learn
cloth invariant features. On the other hand, SCNet[16]
uses a tri-stream network to learn semantically invariantfeatures. Some works also attempt to use multiple modal-
ities (e.g., silhouettes [23], skeletons [34], 3D shape [6])
etc. for better feature representation. Even though the im-
age based methods can have better performance than some
video-based methods, this performance is measured on very
specific datasets, which might or might not generalize to
more complex datasets where the person in consideration is
performing some other activities rather than walking.
Video-based identification: The key for video-based per-
son identification is to extract representations robust to spa-
tial and temporal distractors. These methods incorporate
temporal information in their learned features and gener-
ally have better performance than image based methods.
Several previous works [10, 43] have exploited temporal
cues by aggregating frames features via LSTM network.
However, instead of using aggregated features extracted by
RNNs, 3D CNNs perform better in terms of directly extract-
ing spatio-temporal features that are more robust for person
identification [5, 28]. Following current research direction
[3, 20, 22, 33, 37], our work is also based on 3D CNN.
Gait recognition: Gait recognition is a very active area of
research where the goal is to identify individuals using their
walking style. Existing methods mostly utilize silhouettes
to avoid interference of appearance [12, 13, 27] which lim-
its their applicability on real-world RGB videos. There are
some approaches making use of RGB for gait recognition
[26, 44], but they do require silhouette in addition to RGB
data. In our proposed method we only use silhouette during
training and it is not required for inference.
Knowledge distillation: It is one of the most common tech-
niques to transfer knowledge from a large model (teacher)
to a smaller model (student) for compression and efficient
learning [18]. It has also been found very effective for semi-
supervised learning where the models can learn from unla-
belled samples under student teacher setup [36]. In some
recent efforts it was also explored for person identification
too for effective cross-view [33] and cross-scene [38] repre-
sentation learning. It has been mostly explored within same
modality, whereas we perform a cross-modal distillation to
leverage the teacher’s knowledge of a different data modal-
ity to improve the performance of the student.
3. Method
Our goal is to identify an individual given an RGB video
of that individual performing some activity. We are using
a face restricted setting to perform this task, where the face
of the individual is blurred so as to avoid learning any of
the facial features. Avoiding the explicit learning of facial
features is motivated by acknowledging potential issues like
wearing accessory (masks, sunglasses), privacy concerns,
and individuals’ unwillingness to reveal their faces.
Problem formulation: Given a dataset Dcontaining el-
ements of v, yA, yBwithNsamples, we want to train a
288
Silhouette sequence
Silhouette
encoder
Tθ(.)Silhouette feature FSBias-less distillation
Video
encoder
Sφ(.)Activity
head CA
Actor head
CBDistillation loss LKD
Distortion loss LDisBiometric loss LBio
Spatio-temporal feature
FABActivity feature FAc
Actor feature FBTNegative
PositiveBiometric feature fbb
Appearance feature fbaSilhouette extraction
Biometric distortionInput
RGB
videoActivity prior
Weight sharingActivity loss LAC
Actor
head CDBPositive
Negative
Distorted videoVideo
encoder
Aφ(.)
Distorted spatio-
temporal feature FDABDistorted Actor feature FDBT
Activity
head CDA
Distorted Activity feature FDAcDistorted appearance feature fDbb
Distorted biometric feature
fDbaBias-learningFigure 2. Overview of our proposed method ABNet . RGB video is passed to a video encoder Sφ(·)for spatio-temporal feature FAB
extraction which is passed to the activity head CAand the actor head CB.CBcaptures both biometrics (in red) and appearance (in green)
features in FBT. To disentangle features, bias-less teacher encoder Tθ(·)distills biometrics knowledge from corresponding silhouettes.
The appearance feature bias is learned via a distortion network using encoder Aφ(·)on the distorted video input. Similar to CB,CDBalso
captures both distorted biometrics (in red) and distorted appearance (in green) features in FD
BT. Here, green and red denote positive and
negative feature. Joint training is performed using both CAandCB. During inference, only the dashed box highlighted branch is utilized.
person identification model Mwhich can provide a latent
feature FABfor each video vwhich can be used for match-
ing it with the person id yB. Here v∈RnXCXHXWrep-
resents an RGB video, where nis the number of frames,
C, H, W are the number of channels, height and width of
the video, and yBis its ground truth actor label that is per-
forming some activity yA. Once trained, the model Mwill
be evaluated on a gallery G∈v, yband probe P∈v, yb.
The goal is to match the id of the person ybin probe video
vwith the correct id in videos from gallery.
Overview: We propose ABNet, Activity Biometrics Net-
work, denoted as Mto solve this problem. ABNet performs
biometrics-bias disentanglement and make use of activity
prior to learn a discriminative identity feature for person
identification. Given a video v, the model Mfirst extracts
spatio-temporal features FABwith the help of a video en-
coder Sφ(·). The spatio-temporal feature FABis split into
two segments and are passed to the actor head CBfor per-
son identification as well as the activity head CAfor activity
recognition. Joint biometrics and activity learning enables
the use of activity-prior for biometrics. We get actor fea-
tures FBTfrom CBthat contains both biometrics and ap-
pearance feature entangled with each other. Now to make
the model robust to appearance bias while learning accurate
biometrics features, we introduce two different components
- 1) distillation from a bias-less teacher and learning the
bias using biometrics distortion . The actor feature FBTare
disentangled into biometrics feature fbband appearance fea-turefba. This disentanglement for biometrics feature fbbis
performed using distillation from a bias-less teacher T. On
the contrary, the disentanglement for appearance feature fba
is done by constraining it using a distortion network A. An
overview of the proposed method is shown in Figure 2.
3.1. Biometrics bias disentanglement
Appearance bias in biometrics arises when the models
overly rely on superficial visual cues, such as clothing or
specific accessories for identification. This leads to chal-
lenges such as limited generalization across appearances,
vulnerability to adversarial attacks, and reduced robustness
to environmental variations. This bias can result in biased
matching decisions, and inconsistent performance across
cameras. There has been extensive research done to avoid
clothing features for person reidentification [15, 16, 40],
however, appearance bias can come from features other than
clothes as well. To deal with this issue of appearance bias,
we introduce two different aspects; 1) bias-less distillation
from a teacher network, and 2) learning the bias using neg-
ative mining through biometrics distortion .
Bias-less distillation: One split segment of the extracted
feature FABis fed to the actor head CB, which contains
DB
ωthat is a standard transformer decoder. We get actor
feature FBTfrom DB
ω, which contains biometrics feature
fbband appearance feature fba.DB
ωuses self-attention to
process the input sequence and then projects the attention
output into fbbandfbausing separate linear layers. Now
289
to disentangle the biometrics features from the appearance
features, we propose the use of silhouette features to per-
form bias-less distillation using teacher network T.Tis
termed as bias-less because it is trained on binary silhouette
video bs∈RnXCXHXWthat corresponds to RGB video
v, and thus have no knowledge of appearance based fea-
tures. Tcontains a silhouette encoder Tθ(·)that takes bs
as input and extracts FSfeatures. Following [18] we use
the standard Kullback-Leibler (KL) divergence loss to min-
imize the discrepancy between the probability distributions
of the teacher Tand our model M. The distillation loss
LKDis formulated as below:
LKD=τ2KL(yT||yS), (1)
where, yTandySare the probability distribution of the
teacher Tand our model M.τis the temperature parame-
ter that controls the softness of the teacher’s output. Along
with this distillation loss LKD,CBhas its own biometrics
lossLBioformulated as below:
LBio=Lce+Ltri, (2)
where, LceandLtriare standard triplet and cross-entropy
losses for person identification formulated as below:
Lce=−ylog ˆy, (3)
Ltri= max(( D(fa, fp)−D(fa, fn) +m),0), (4)
where, yandˆyare the ground truth and predicted label, fp
andfnare the positive and negative features for an anchor
feature fawithin the same batch, D(·)is the Euclidean dis-
tance function, and mis the margin of triplet loss.
Bias learning: To make the model robust to appearance
bias, we introduce the distortion network A, which is iden-
tical to Mand shares weights. It contains video encoder
Aφ(·)that takes distorted video ˆv∈RnXCXHXWthat cor-
responds to the original video v. The key idea is to distort
the identity of the person while preserving the appearance.
We rely on elastic transform [1] which randomly transforms
the morphology of objects in images and produces a see-
through-water-like effect in the image still preserving the
appearance. It is used to generate “negative” or “distractor”
samples in the training dataset where the distorted samples
will have the same appearance while changing the identity.
Some sample distorted images are shown in Fig. 3.
Similar to M, this distortion network Aalso extracts
spatio-temporal feature FD
ABusing encoder Aφ(·). Since
this branch is designed for bias-learning, thus the activity
head CDAofAis not utilized. On the contrary, A’s ac-
tor head CDBextracts distorted biometrics feature fD
bband
distorted appearance feature fD
ba. Due to the distortion, fba
andfD
baare treated as positive samples, whereas, fbband
fD
bbas hard negative samples. The goal is to pull togetherpositive pairs (i.e. similar features) and push apart negative
pairs (i.e. dissimilar features). We use this distorted aug-
mentation loss LDisfor bias learning and it is described as,
LDis=max((D(fba, fD
ba)−D(fbb, fD
bb) +m),0),(5)
where D(·)is the Euclidean distance function and mis the
margin for the contrastive loss.
3.2. Joint biometrics and activity learning
Jointly training a network for both activity recognition and
person identification can benefit person identification when
the training data includes activities by enabling the model
to learn shared representations. By learning to understand
contextual cues from activities alongside actor features, the
network can develop richer embeddings, thereby enhancing
the model’s ability to accurately identify individuals across
varying activity contexts. Thus we perform joint learning
of the activity and actor branch of ABNet. One segment
of feature FABis fed to activity head CAthat contains de-
coder DA
Ωthat learns features FAc.CAis trained using LAc
which is a standard cross-entropy loss for the activity labels
regardless of the actor labels. This joint training also en-
ables ABNet to utilize activity priors for biometrics, where
we use knowledge of activity for person identification. This
is accomplished by concatenating the activity features FAC
with biometrics features fbbduring testing.
3.3. Overall learning objective
Finally the model Mis optimized by combining all the
losses which include, biometrics loss LBio, distillation loss
LKD, distortion loss LDisand activity loss LAcand we get
the total loss Lformulated as,
L=LBio+λ1LAc+λ2LKD+λ3LDis (6)
where λi,i∈[1,2,3]are the weights for each of the losses.
4. Experiments and results
Datasets: We perform our experiments on five different
datasets which are derived from existing activity recogni-
tion benchmarks. 1) NTU RGB-AB is derived from NTU
RGB+D [29] which is a large-scale benchmark for activity
recognition. We ignore mutual activities and consider 94 ac-
tivity classes with 88692 samples fro NTU RGB-AB. The
activity classes are divided into daily activities and medical
conditions performed by a total of 106 subjects across 32
different setups, 155 different views which are shown with
3 cameras. We use the official cross-subject split for the
train test separation. 2) PKU MMD-AB is derived from
PKU-MMD [8] which is another large scale benchmark for
activity recognition. Similar to NTU RGB-AB, we ignore
mutual activities from PKU-MMD and PKU MMD-AB has
290
Figure 3. Biometrics distortion: here original samples are shown in the top row and their corresponding distorted samples in the bottom
row. From left to right, every two columns contain samples from NTU RGB-AB, PKU MMD-AB, Charades-AB, ACC-MM1-Activities
and BRIAR-BGC3 dataset respectively. The subjects from BRIAR-BGC3 and ACC-MM1-Activities consented to publication.
41 activity categories with almost 17,000 labeled activity
instances. These activities are performed by 66 actors in 3
different camera views and we use the official cross-subject
split for our experiments. 3) Charades-AB contains all the
9,848 annotated videos from Charades [35] with approx-
imately 6.8 activities per video performed by 267 actors
across 157 activity classes from a single viewpoint. We use
the official train-test split for our experiments. 4) ACC-
MM1-Activities [32] is a recently curated daily activities
dataset which contains 1378 annotated videos where 7 daily
activities are being performed by 200 subjects from a single
view-point. These activities are - enter/exit car, pull/push
door, walk upstairs/downstairs, and texting. We use the of-
ficial train-test split for our experiments. 5) BRIAR-BGC3
[9] is a large-scale, in-the-wild person identification dataset
containing samples across varying distances, environment
conditions. It is mainly focused on walking/standing sce-
nario and consists of 3 different walking conditions (struc-
tured walk, random walk and standing) performed by 1055
subjects in outdoor settings from different ranges and angle
of elevation. BRIAR-BGC3 contains over 1300 hours of la-
beled training videos from 1055 subjects in indoor/outdoor
settings. We use a 20K subset of this dataset for training
with official face-restricted testing set for evaluation.
The videos from all five datasets undergo an arbitrarily cho-
sen value of hue shifting. Training a model on hue-shifted
data, even when appearance features are not explicitly uti-
lized, serves to enhance the model’s robustness and gen-
eralization capabilities. To facilitate face restricted person
identification the faces are blurred using Gaussian blur for
both the test and train split of all datasets.
Implementation and training details: The proposed
method is implemented using Pytorch. We use ResNet3D-
50 [17] as the backbone of the video encoder Sφ(·)and
GaitGL [27] for the teacher’s silhouette encoder Tθ(·).
The silhouettes of the RGB videos are extracted using
Mask2Former [7] to use as input to Tθ(·). We create RGB
video clips from each original video by randomly selecting8 frames with a stride of 4. Every input frame undergoes
resizing to dimensions of 256X128. We train the model
with a batch size of 32 with each batch containing 8 person
and 4 clips for each person. Adam [24] is used as the op-
timizer with weight decay of 5x10−4and learning rate of
3.5X10−4. The model is trained for 150 epochs with a de-
cay factor 0.1after every 40 epochs. The triplet loss margin
mis set to 0.3andλi,i∈[1,2,3]in Eq. (6) is set to 0.01.
During inference the activity feature FAcis concatenated
with the biometrics feature fbbthat acts as the activity prior.
Evaluation protocol: For all datasets except BRIAR-
BGC3, we randomly split the test set into gallery and probe
(more details in supplementary). We use two different eval-
uation protocols; 1) same activity inclusive, and 2) cross-
activity. For the first one, we use all the activities in the
gallery whereas in cross-activity we exclude the activity
in the probe while retrieval. Similarly, we also evaluate
for same-view (View+) and cross-view (View−) for NTU
RGB-AB and PKU MM-AB where view information is
available. For BRIAR-BGC3, we use the official protocol
for face-restricted evaluation.
Evaluation metrics: For a thorough assessment of the
model’s performance, we employ rank 1 accuracy, rank 5
accuracy, mean average precision (mAP), and TAR @ 0.1%
FAR. While the first three evaluation metrics are more pop-
ular to evaluate a person identification model, the latter met-
ric is also crucial to check the model’s ability to minimize
the false acceptance rate.
Baseline methods: We consider ResNet3D-50 [17],
MViTv2 [25] and GaitGL [27] as baselines. To further
demonstrate the effectiveness of our model, we compare
it against several state-of-the-art image based (CAL [15],
PSTR [4], SCNet [16] and AIM [40]) and video based (TSF
[22], VKD [33], BiCnet-TKS [20], STMN [11], PSTA [37],
SINet [3],Video-CAL[15]) person identification methods.
291
Table 1. Comparison with state-of-the-art person identification methods :Evaluation shown on NTU RGB-AB, PKU MMD-AB,
Charades-AB, and ACC-MM1-Activities on same-activity, View+evaluation protocol . †: this model was trained on silhouettes.
Methods VenueNTU RGB-AB PKU MMD-AB Charades-AB ACC-MM1-Activities
Rank 1 mAP Rank 1 mAP Rank 1 mAP Rank 1 mAP
ImageCAL [15] CVPR22 73.79 28.40 81.31 49.45 43.84 25.81 69.83 42.81
PSTR [4] CVPR22 69.14 34.14 84.33 47.52 37.15 24.69 57.41 34.48
SCNet [16] ACM MM23 69.89 31.47 79.53 43.55 31.73 21.89 64.68 39.79
AIM [40] CVPR23 71.37 35.41 82.52 48.89 40.13 28.31 74.79 49.14
VideoTSF [22] AAAI20 71.79 31.80 76.43 37.50 35.38 21.89 49.41 29.73
VKD [33] ECCV20 67.41 35.63 78.35 38.54 36.31 20.71 55.38 29.57
BiCnet-TKS [20] CVPR21 72.71 34.45 80.79 38.52 40.31 27.34 60.44 32.79
STMN [11] ICCV21 72.98 35.08 76.55 47.92 38.72 24.49 59.44 39.68
PSTA [37] ICCV21 67.41 34.78 77.44 50.42 42.89 28.32 71.41 50.31
SINet [3] CVPR22 69.41 30.68 79.58 40.80 40.31 26.90 65.39 45.41
Video-CAL [15] CVPR22 75.49 39.86 79.59 49.42 43.91 28.51 77.48 50.08
BaselinesGaitGL [27] † - 61.51 28.89 65.38 33.78 18.43 6.81 39.41 18.51
ResNet3D-50 [17] - 64.23 26.89 69.70 32.64 32.25 17.42 44.31 22.54
MViTv2 [25] - 63.87 26.41 68.37 28.52 28.51 15.39 40.59 21.52
ABNet (ours) - 78.76 40.31 86.83 57.31 45.84 31.58 80.43 52.71
4.1. Results
In Table 1, we present rank 1 accuracy and mAP metrics
for different baselines and state-of-the-art person identi-
fication methods across NTU RGB-AB, PKU MMD-AB,
Charades-AB, and ACC-MM1-Activities datasets, using
the same activity View+evaluation protocol. ABNet con-
sistently outperforms both the best SOTA models and base-
lines across all four datasets. Table 3 compares ABNet with
top-performing identification methods and baselines on the
BRIAR-BGC3 dataset.
For a detailed evaluation, Table 2 shows ABNet’s perfor-
mance across NTU RGB-AB, PKU MMD-AB, Charades-
AB, and ACC-MM1-Activities datasets. This includes both
same activity and cross activity evaluation protocols, fea-
turing View+and View−settings for NTU RGB-AB and
PKU MMD-AB. As view information is unavailable for
Charades-AB and ACC-MM1-Activities datasets, the eval-
uation focuses solely on same and cross activity protocols.
Comparisons: From Tables 1 and 3, it’s clear that existing
methods are primarily focused on identifying individuals
based on walking patterns in various settings, lacking opti-
mization for diverse activities. Our proposed ABNet consis-
tently outperforms existing models across all datasets. AB-
Net demonstrates approximately 2%to4%higher rank 1
accuracy compared to the best existing method. This con-
sistent superiority highlights ABNet’s effectiveness in per-
son identification across diverse activity scenarios.
In Table 2, ABNet shows relatively stable performance
across different evaluation protocols, except for ACC-
MM1-Activities, which has fewer activity classes leading
to larger performance gaps. The presence of overlapping
activities in Charades-AB video samples reduces its per-formance compared to other datasets. Despite these chal-
lenges, ABNet consistently delivers strong results. Even on
the predominantly walking-focused BRIAR-BGC3 dataset,
ABNet outperforms the best SOTA model by 4%in rank
1 accuracy. Overall, ABNet demonstrates robust perfor-
mance, particularly on datasets with diverse activity classes.
4.2. Ablations
To verify the effectiveness of ABNet and each of its com-
ponents, we perform ablation study on the NTU RGB-AB
dataset in Table 4 on the same activity evaluation protocol.
Refer to the supplementary for ablation study on the cross
activity evaluation protocol. Here, B/L stands for the base-
line which is just the backbone model taking RGB video as
input. K/D stands for bias-less distillation, A/P stands for
activity prior, and lastly F/D stands for the bias learning.
Effect of bias-less distillation: Introducing bias-less dis-
tillation, either independently (row 2) or with an activity
prior (row 4), leads to notable performance improvements
over the baseline. However, combining bias-less distillation
and activity prior demonstrates superior performance over
independent use of distillation, showcasing their synergistic
effect on model enhancement.
Effect of bias learning: Incorporating bias learning
through a distorted video encoder branch boosts model per-
formance even more (row 5). Similar to bias-less distilla-
tion, combining bias learning with an activity prior yields
the best overall performance (row 6), highlighting the im-
portance of their synergy in enhancing model robustness
and disentangling biometrics and appearance information.
Effect of activity prior: Incorporating activity and biomet-
rics features during inference significantly enhances per-
formance compared to using only the baseline model (row
292
Table 2. Comprehensive performance evaluation of ABNet: results shown on NTU RGB-AB, PKU MMD-AB, Charades and ACC-
MM1-Activities. We observe that cross-view and cross-activity setup is the most challenging with some performance drop when compared
with same activity and same view setup.
Dataset Evaluation ProtocolR@1 R@5 mAP TAR @ 0.1% FAR
View+View−View+View−View+View−View+View−
NTU RGB-ABSame activity 78.76 77.81 85.31 82.41 40.31 38.80 39.83 35.68
Cross activity 77.01 76.43 81.37 80.37 37.64 36.14 34.92 33.79
PKU MMD-ABSame activity 86.83 81.41 91.37 87.73 57.31 51.74 42.79 40.31
Cross activity 81.44 79.41 89.31 84.83 51.79 46.30 37.31 34.38
CharadesSame activity 45.84 - 51.04 - 31.58 - 25.39 -
Cross activity 44.82 - 52.01 - 28.78 - 22.61 -
ACC-MM1-ActivitiesSame activity 80.43 - 89.31 - 52.71 - 43.72 -
Cross activity 68.31 - 76.39 - 38.83 - 35.32 -
Table 3. Performance comparison on BRIAR-BGC3 against best
state-of-the-art person identification and baselines.
Model R@1 mAP TAR@ 0.1%FAR
Image-CAL [14] 30.57 17.44 25.38
Video-CAL [14] 28.32 15.43 24.16
PSTA [37] 27.75 13.78 21.54
GaitGL[27] 12.61 9.51 6.44
ResNet3D-50[17] 22.50 12.83 19.71
MViTv2[25] 11.78 10.21 8.44
ABNet (ours) 34.38 18.78 26.42
Table 4. Ablation studies of each component of ABNet on NTU
RGB-AB on same activity evaluation protocol.
B/L K/D A/P F/DView+View−
R@1 mAP R@1 mAP
✓ 64.23 26.89 62.10 22.45
✓ ✓ 69.31 28.01 66.57 24.29
✓ ✓ 69.43 27.97 67.37 24.77
✓ ✓ ✓ 72.89 32.38 70.17 30.68
✓ ✓ ✓ 76.70 36.21 73.82 33.18
✓ ✓ ✓ ✓ 78.76 40.31 77.81 38.80
3). This integration consistently improves model efficacy
across various model configurations demonstrating the role
of activity recognition for biometrics.
4.3. Discussion and analysis
Effect of distortion: Figure 4 presents t-SNE plots of the
biometrics and appearance feature space for ten NTU RGB-
AB individuals at α∈[0,50,100,150,200,250,300,350],
where αrepresents the amount of distortion. For optimal α
we want to find such a value where biometrics feature clus-
ters are overlapped due to being negative, but appearance
feature clusters still remain relatively same due to being
positive. Increasing αcauses more overlap in biometrics
feature clusters; whereas up to α= 250 , appearance fea-
ture clusters remain relatively stable. However, beyond this
point, excessive distortion causes overlapping appearanceTable 5. Effect of distortion on model performance for NTU
RGB-AB on the same activity evaluation protocol
Distortion amountView+View−
R@1 mAP R@1 mAP
α= 200 78.23 38.31 76.81 37.91
α= 250 78.76 40.31 77.81 38.80
α= 300 75.24 31.42 73.17 29.84
Table 6. Effect of face restriction on model performance for NTU
RGB-AB on same activity evaluation protocol
Face RestrictedView+View−
R@1 mAP R@1 mAP
Yes 78.76 40.31 77.81 38.80
No 79.24 41.64 78.87 40.04
clusters. Thus α= 250 is selected as the optimal value.
From the quantitative results presented in Table 5 similar
effect of αis observed on the model’s performance.
Performance analysis across activities: Figure 5 illus-
trates the comparison between our method and the baseline
across selected activities, encompassing the top five best
and bottom five worst instances in person identification per-
formance. Notably, activities posing challenges for person
identification, resulting in lower performance, also exhibit
reduced accuracy in activity recognition, except for a few
exceptional activity classes. This correlation underscores
the consistent relationship between the difficulty of iden-
tifying individuals within activities and the corresponding
accuracy of recognizing those activities.
Effect of face restriction: Table 6 illustrates the model’s
performance on the same activity evaluation protocol, indi-
cating a minimal increase in performance despite the pres-
ence of facial features. This suggests the model’s resilience
to facial variations, showcasing its capability to identify in-
dividuals based on non-facial cues. ABNet demonstrates
stability in performance even after the removal of facial ap-
pearance cues, highlighting its reliance on other distinguish-
ing features, such as activity-related cues.
293
Figure 4. Effect of distortion on feature space: The t-SNE plots illustrate the impact of varying distortion amount α∈
[0,50,100,150,250,300,350] on biometrics (top) and appearance (bottom) features of ABNet for ten random NTU RGB-AB identi-
ties. As αincreases from left to right, the optimal results occur at α= 250 (shown in square) where biometrics changes while appearance
remains consistent. Beyond α= 250 , appearance gets distorted too, making it unsuitable for disentanglement.
Figure 5. Performance analysis across activities: The bar plot on
left axis shows rank 1 identification accuracy for given activity of
ABNet against baseline on 10 activity (5 best and 5 worst) classes
of NTU RGB-AB. The scatter plot with markers on right axis
shows activity recognition accuracy for corresponding classes.
Qualitative results: In addition to the quantitative results,
we show top 4 rank retrieval results in Figure 6. Each row in
this figure corresponds to a probe (left) and the identities re-
trieved (right) by ABNet. The retrieval list shows accurate
person identification across a variety of activities and ap-
pearance, effectively highlighting ABNet’s ability to learn
from activity cues rather than appearance.
5. Conclusion
In this work we study a novel problem of person identi-
fication from videos of daily activities. We propose AB-
Net, a simple approach to solve this problem which re-
lies on feature disentanglement and activity prior for person
identification. This approach incorporates feature disentan-
glement at both biometric and appearance levels, leverag-
ing distinct strategies to enhance accuracy and mitigate bi-
ases. By distilling biometric knowledge from a bias-free
silhouette-trained model and learning appearance biases via
elastic distortion-based transformations, our framework en-
sures a comprehensive understanding of individuals’ inher-
ent biometric traits while accounting for appearance varia-
tions. Moreover, the integration of an activity prior during
inference further enriches the model’s capabilities. Through
Figure 6. Top 4 rank retrieval samples for ABNet on NTU RGB-
AB, Charades-AB, PKU MMD-AB and ACC-MM1-Activities on
row1,2,3,4respectively. The left most column shows the probe
and rest of the columns are the retrieved list. Accurate retrieval is
shown with green box and inaccurate with red. The subjects from
ACC-MM1-Activities consented to publication.
extensive evaluations on five benchmark datasets derived
from large-scale activity recognition datasets, our approach
consistently surpasses several state-of-the-art methods.
6. Acknowledgement
This research is based upon work supported in part by the
Office of the Director of National Intelligence (IARPA) via
2022-21102100001. The views and conclusions contained
herein are those of the authors and should not be interpreted
as necessarily representing the official policies, either ex-
pressed or implied, of ODNI, IARPA, or the US Govern-
ment. The US Government is authorized to reproduce and
distribute reprints for governmental purposes notwithstand-
ing any copyright annotation therein.
294
References
[1] Elastic transform. https://pytorch.org/vision/
main / generated / torchvision . transforms .
ElasticTransform.html . [Online; accessed 08-
November-2023]. 4
[2] Insaf Adjabi, Abdeldjalil Ouahabi, Amir Benzaoui, and Ab-
delmalik Taleb-Ahmed. Past, present, and future of face
recognition: A review. Electronics , 9(8):1188, 2020. 1
[3] Shutao Bai, Bingpeng Ma, Hong Chang, Rui Huang, and
Xilin Chen. Salient-to-broad transition for video person re-
identification. In CVPR , pages 7339–7348, 2022. 2, 5, 6
[4] Jiale Cao, Yanwei Pang, Rao Muhammad Anwer, Hisham
Cholakkal, Jin Xie, Mubarak Shah, and Fahad Shahbaz
Khan. Pstr: End-to-end one-step person search with trans-
formers. In CVPR , pages 9458–9467, 2022. 1, 2, 5, 6
[5] Guangyi Chen, Jiwen Lu, Ming Yang, and Jie Zhou.
Learning recurrent 3d attention for video-based person re-
identification. IEEE TIP , 29:6963–6976, 2020. 2
[6] Jiaxing Chen, Xinyang Jiang, Fudong Wang, Jun Zhang,
Feng Zheng, Xing Sun, and Wei-Shi Zheng. Learning 3d
shape feature for texture-insensitive person re-identification.
InCVPR , pages 8146–8155, 2021. 2
[7] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In CVPR ,
2022. 5
[8] Liu Chunhui, Hu Yueyu, Li Yanghao, Song Sijie, and Liu
Jiaying. Pku-mmd: A large scale benchmark for continu-
ous multi-modal human action understanding. arXiv preprint
arXiv:1703.07475 , 2017. 4
[9] David Cornett, Joel Brogan, Nell Barber, Deniz Aykac, Seth
Baird, Nicholas Burchfield, Carl Dukes, Andrew Duncan,
Regina Ferrell, Jim Goddard, et al. Expanding accurate
person recognition to new altitudes and ranges: The briar
dataset. In WACV , pages 593–602, 2023. 5
[10] Ju Dai, Pingping Zhang, Dong Wang, Huchuan Lu, and
Hongyu Wang. Video person re-identification by temporal
residual learning. IEEE TIP , 28(3):1366–1377, 2018. 2
[11] Chanho Eom, Geon Lee, Junghyup Lee, and Bumsub Ham.
Video-based person re-identification with spatial and tempo-
ral memory networks. In ICCV , pages 12036–12045, 2021.
5, 6
[12] Chao Fan, Yunjie Peng, Chunshui Cao, Xu Liu, Saihui Hou,
Jiannan Chi, Yongzhen Huang, Qing Li, and Zhiqiang He.
Gaitpart: Temporal part-based model for gait recognition. In
CVPR , pages 14225–14233, 2020. 1, 2
[13] Chao Fan, Junhao Liang, Chuanfu Shen, Saihui Hou,
Yongzhen Huang, and Shiqi Yu. Opengait: Revisiting gait
recognition towards better practicality. In CVPR , pages
9707–9716, 2023. 1, 2
[14] Xinqian Gu, Hong Chang, Bingpeng Ma, Hongkai Zhang,
and Xilin Chen. Appearance-preserving 3d convolution for
video-based person re-identification. In ECCV , pages 228–
243. Springer, 2020. 1, 7
[15] Xinqian Gu, Hong Chang, Bingpeng Ma, Shutao Bai,
Shiguang Shan, and Xilin Chen. Clothes-changing personre-identification with rgb modality only. In CVPR , pages
1060–1069, 2022. 1, 2, 3, 5, 6
[16] Peini Guo, Hong Liu, Jianbing Wu, Guoquan Wang, and
Tao Wang. Semantic-aware consistency network for cloth-
changing person re-identification. In ACM MM , 2023. 2, 3,
5, 6
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
pages 770–778, 2016. 5, 6, 7
[18] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 2, 4
[19] Peixian Hong, Tao Wu, Ancong Wu, Xintong Han, and Wei-
Shi Zheng. Fine-grained shape-appearance mutual learning
for cloth-changing person re-identification. In CVPR , pages
10513–10522, 2021. 2
[20] Ruibing Hou, Hong Chang, Bingpeng Ma, Rui Huang,
and Shiguang Shan. Bicnet-tks: Learning efficient spatial-
temporal representation for video person re-identification. In
CVPR , pages 2014–2023, 2021. 1, 2, 5, 6
[21] Yan Huang, Qiang Wu, Jingsong Xu, and Yi Zhong.
Celebrities-reid: A benchmark for clothes variation in long-
term person re-identification. In IJCNN , pages 1–8. IEEE,
2019. 2
[22] Xinyang Jiang, Yifei Gong, Xiaowei Guo, Qize Yang, Feiyue
Huang, Wei-Shi Zheng, Feng Zheng, and Xing Sun. Rethink-
ing temporal fusion for video-based person re-identification
on semantic and time aspect. In AAAI , pages 11133–11140,
2020. 2, 5, 6
[23] Xin Jin, Tianyu He, Kecheng Zheng, Zhiheng Yin, Xu
Shen, Zhen Huang, Ruoyu Feng, Jianqiang Huang, Zhibo
Chen, and Xian-Sheng Hua. Cloth-changing person re-
identification from a single image with gait prediction and
regularization. In CVPR , pages 14278–14287, 2022. 2
[24] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. ICLR , 2015. 5
[25] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man-
galam, Bo Xiong, Jitendra Malik, and Christoph Feichten-
hofer. Mvitv2: Improved multiscale vision transformers for
classification and detection. In CVPR , pages 4804–4814,
2022. 5, 6, 7
[26] Junhao Liang, Chao Fan, Saihui Hou, Chuanfu Shen,
Yongzhen Huang, and Shiqi Yu. Gaitedge: Beyond plain
end-to-end gait recognition for better practicality. In ECCV ,
pages 375–390. Springer, 2022. 1, 2
[27] Beibei Lin, Shunli Zhang, and Xin Yu. Gait recognition via
effective global-local feature representation and local tempo-
ral aggregation. In ICCV , pages 14648–14656, 2021. 1, 2, 5,
6, 7
[28] Chih-Ting Liu, Chih-Wei Wu, Yu-Chiang Frank Wang, and
Shao-Yi Chien. Spatially and temporally efficient non-local
attention network for video-based person re-identification.
arXiv preprint arXiv:1908.01683 , 2019. 2
[29] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,
Ling-Yu Duan, and Alex C Kot. Ntu rgb+ d 120: A large-
scale benchmark for 3d human activity understanding. IEEE
TPAMI , 42(10):2684–2701, 2019. 4
295
[30] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In ICCV , 2015.
1
[31] Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou.
Magface: A universal representation for face recognition and
quality assessment. In CVPR , pages 14225–14234, 2021. 1
[32] K. O’Brien, M. Rybak, J. Huang, A. Stevens, M. Fredriksz,
M. Chaberski, D. Russell, L. Castin, M. Jou, N. Gurrapadi,
and M. Bosch. Accenture-mm1: A multimodal person recog-
nition dataset. In WACVW , 2024. 5
[33] Angelo Porrello, Luca Bergamini, and Simone Calderara.
Robust re-identification by multiple views knowledge distil-
lation. In ECCV , pages 93–110. Springer, 2020. 1, 2, 5, 6
[34] Xuelin Qian, Wenxuan Wang, Li Zhang, Fangrui Zhu,
Yanwei Fu, Tao Xiang, Yu-Gang Jiang, and Xiangyang
Xue. Long-term cloth-changing person re-identification. In
ACCV , 2020. 2
[35] Gunnar A Sigurdsson, G ¨ul Varol, Xiaolong Wang, Ali
Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in
homes: Crowdsourcing data collection for activity under-
standing. In ECCV , pages 510–526, 2016. 5
[36] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. NeurIPS , 30, 2017. 2
[37] Yingquan Wang, Pingping Zhang, Shang Gao, Xia Geng,
Hu Lu, and Dong Wang. Pyramid spatial-temporal aggrega-
tion for video-based person re-identification. In ICCV , pages
12026–12035, 2021. 2, 5, 6, 7
[38] Ancong Wu, Wei-Shi Zheng, Xiaowei Guo, and Jian-Huang
Lai. Distilled person re-identification: Towards a more scal-
able system. In CVPR , pages 1187–1196, 2019. 2
[39] Qize Yang, Ancong Wu, and Wei-Shi Zheng. Person re-
identification by contour sketch under moderate clothing
change. IEEE TPAMI , 43(6):2029–2046, 2019. 2
[40] Zhengwei Yang, Meng Lin, Xian Zhong, Yu Wu, and Zheng
Wang. Good is bad: Causality inspired cloth-debiasing for
cloth-changing person re-identification. In CVPR , pages
1472–1481, 2023. 1, 2, 3, 5, 6
[41] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling
Shao, and Steven CH Hoi. Deep learning for person re-
identification: A survey and outlook. IEEE TPAMI , 44(6):
2872–2893, 2021. 1
[42] Shiqi Yu, Daoliang Tan, and Tieniu Tan. A framework for
evaluating the effect of view angle, clothing and carrying
condition on gait recognition. In ICPR , pages 441–444,
2006. 1
[43] Dongyu Zhang, Wenxi Wu, Hui Cheng, Ruimao Zhang,
Zhenjiang Dong, and Zhaoquan Cai. Image-to-video per-
son re-identification with temporally memorized similarity
learning. IEEE TCSVT , 28(10):2622–2632, 2017. 1, 2
[44] Ziyuan Zhang, Luan Tran, Xi Yin, Yousef Atoum, Xiaoming
Liu, Jian Wan, and Nanxin Wang. Gait recognition via disen-
tangled representation learning. In CVPR , pages 4710–4719,
2019. 1, 2
[45] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV , pages 1116–1124, 2015. 1
296
