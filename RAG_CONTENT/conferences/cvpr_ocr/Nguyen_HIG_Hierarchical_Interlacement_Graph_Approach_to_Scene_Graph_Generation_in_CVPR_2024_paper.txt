HIG: Hierarchical Interlacement Graph Approach to
Scene Graph Generation in Video Understanding
Trong-Thuan Nguyen, Pha Nguyen, Khoa Luu
CVIU Lab, University of Arkansas
{thuann, panguyen, khoaluu}@uark.edu
https://uark-cviu.github.io/ASPIRe/
person-2in the hand
ball-1hitting
partnersperson-1
person-3
in the competitionwhite t-shirt
violet t-shirt
grey t-shirtcatching
person-2ball-1person-1
person-3person-2ball-1person-1
hitting
person-3spinning
person-2ball-1person-1
person-3person-2ball-1person-1
person-3opponentsopponents
in the hand
partnersopponents
in the hand
partnersopponents
black and white
next tonext to
next toopposite
next toopponents
partnersopponentsopponents
opponents
Figure 1. An example from our ASPIRe dataset for Visual Interactivity Understanding. The top row shows keyframes with the bounding
boxes. Appearance ,Situation ,Position ,Interaction , and Relation are attributes presented in the dataset. Best viewed in color .
Abstract
Visual interactivity understanding within visual scenes
presents a significant challenge in computer vision. Exist-
ing methods focus on complex interactivities while lever-
aging a simple relationship model. These methods, how-
ever, struggle with a diversity of appearance, situation, po-
sition, interaction, and relation in videos. This limitation
hinders the ability to fully comprehend the interplay within
the complex visual dynamics of subjects. In this paper, we
delve into interactivities understanding within visual con-
tent by deriving scene graph representations from dense
interactivities among humans and objects. To achieve this
goal, we first present a new dataset containing Appearance-
Situation-Position-Interaction-Relation predicates, named
ASPIRe, offering an extensive collection of videos marked
by a wide range of interactivities. Then, we propose a new
approach named Hierarchical Interlacement Graph (HIG),
which leverages a unified layer and graph within a hierar-
chical structure to provide deep insights into scene changes
across five distinct tasks. Our approach demonstrates supe-
rior performance to other methods through extensive experi-
ments conducted in various scenarios.1. Introduction
Visual interaction and relationship understanding have wit-
nessed significant advancements in computer vision in re-
cent years. Various methods, including deep learning, have
been introduced, particularly in achieving advanced compre-
hension of diverse relationships for a holistic visual under-
standing. Traditional methods span from action recognition
and localization to intricate processes like video caption-
ing [20,47,52], spatio-temporal detection [ 41,57] and video
grounding [ 18,23,33]. However, these tasks often interpret
visual temporal sequences in a constrained, uni-dimensional
way. In addition, relation modeling techniques, including
scene graph generation [ 14,48,50] and visual relationship
detection [ 31,55], adhere to predefined relation categories,
limiting the scope for discovering more diverse relationships.
Delving into the Visual Interactivity Understanding prob-
lem (Fig. 1) [ 14,31,50], we introduce a new dataset,
characterized by 5 ×larger interactivity types, including
Appearance- Situation- Position- Interaction- Relation, named
ASPIRe . To this end, we introduce the Hierarchical Inter-
lacement Graph (HIG), a novel approach to the Interactivity
Understanding problem. The proposed HIG framework inte-
grates the evolution of interactivities over time. It presents
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18384
Table 1. Comparison of available datasets. # denotes the number of the corresponding item. The top sub-block of the table is the summary
of image datasets, and the bottom is video datasets. Single andDouble are the attribute types as defined in Subsec. 4.1. H-H ,H-O ,O-O
indicate the interactivity between Human and Human ,Human and Object ,Object and Object .
Datasets #Videos #Frames #Subjects #RelCls #SettingsAnnotationsAttributes
Single Double
BBox Mask #Annotations H-H H-O O-O
Visual Genome [16] - 108K 33K 42K 1 ✓ ✗ 3.8M ✗ ✗ ✓ ✓
PSG [48] - 49K 80 56 1 ✓ ✓ 538.2K ✗ ✓ ✓ ✓
VidOR [31] 10K - 80 50 1 ✓ ✗ 50K ✗ ✓ ✓ ✓
Action Genome [14] 10K 234K 25 25 1 ✓ ✗ 476.3K ✗ ✗ ✓ ✗
VidSTG [55] 10K - 80 50 1 ✓ ✗ 50K ✗ ✓ ✓ ✓
EPIC-KITCHENS [7] 700 11.5K 21 13 1 ✓ ✗ 454.3K ✗ ✗ ✓ ✗
PVSG [50] 400 153K 126 57 1 ✓ ✓ - ✗ ✓ ✓ ✓
ASPIRe (Ours) 1.5K 1.6M 833 4.5K 5 ✓ ✓ 167.8K ✓ ✓ ✓ ✓
an intuitive modeling technique and lays the groundwork
for enriched comprehension of visual activities and complex
interactivities. HIG operates with a unique unified layer at
every level to jointly process interactivities. This strategy
simplifies operations and eliminates the intricacies of multi-
layers. Instead of perceiving video content as a monolithic
block, HIG models an input video with a hierarchical struc-
ture, promoting a holistic grasp of object interplays. Each
level delves into essence insights, leveraging the strengths
of different levels to capture scene changes over time.
In addition, the proposed HIG framework promotes dy-
namic adaptability andflexibility , empowering the model
to adjust its structure and functions to capture the interac-
tivities throughout video sequences. This adaptability is
further showcased as the HIG framework proficiently tack-
les five distinct tasks, demonstrating its extensive flexibility
in decoding various interactivity nuances. The proposed
HIG framework is not confined to specific tasks or domains,
emphasizing its broad applicability and potential.
The Contributions of this Work. There are three main
contributions to this work. First, we develop a new dataset
named ASPIRe for the Visual Interactivity Understanding
problem, augmented with numerous predicate types to cap-
ture the complex interplay in the real world. Second, we pro-
pose the Hierarchical Interlacement Graph (HIG), standing
out with its hierarchical graph structure and unified layer to
ensure scalability and flexibility, comprehensively capturing
intricate interactivities within video content. Finally, com-
prehensive experiments, including evaluating other methods
on our APSIRe dataset and HIG model on both video and
image datasets, we prove the advantages of the proposed
approach that achieves State-of-the-Art (SOTA) results.
2. Related Work
2.1. Dataset and Benchmarks
Dataset. Action Genome [ 14] introduces a comprehensive
video database with action and spatiotemporal scene graph
annotations. VidOR [ 31] and EPIC-KITCHENS [ 7] focus
on object and relationship detection and egocentric actionrecognition. Ego4D [ 11], VidSTG [ 55], and PVSG [ 50]
further enrich scene understanding and video scene graph
resources. These datasets provide crucial benchmarks for
evaluating scene understanding, detailed in Table 1.
Benchmarks. Current benchmarks primarily rely on rela-
tion classification for identifying inter-object associations.
Action Genome [ 14] integrates spatiotemporal to Visual
Genome [ 16] to establish scene graphs with action recog-
nition using SGFB. VidOR [ 31] provides 10K videos for
benchmarking video object detection and visual relation de-
tection. EPIC-KITCHENS-100 [ 7] offers a varied dataset
with 100 hours of video, 20M frames, and 90K actions.
Ego4D [ 11] focuses on first-person video data, addressing
past, present, and future aspects across nearly 3.6K videos.
VidSTG [ 55] introduces the Video Grounding for Multi-
Form Sentences (STVG) task, augmenting VidOR with addi-
tional sentence annotations. Recently, PVSG [ 50] expanded
PSG [48], advancing video graph generation.
2.2. Interactivity Modeling Approaches
Video Situation Recognition. The VidSitu [ 30] benchmark
provides a collection of events and situations for evaluation,
covering verb prediction, semantic role prediction, and event
relations prediction. In a related approach within this bench-
mark, VideoWhisperer [ 15] adopts a global perspective for
video comprehension, utilizing self-attention across all video
clips. Furthermore, the LVU [ 42] benchmark is tailored for
self-supervised video representation learning, with a strong
focus on hierarchical methodologies.
Video Understanding. This contains a wide range of tasks
and research efforts. Action recognition [ 29,37] has ad-
vanced significantly through graph-based [ 44], few-shot
learning [ 35,40], and transformer-based [ 5] approaches. An-
other area of interest is object retrieval [ 25,51], object track-
ing [27,28], spatio-temporal detection [ 24,41,57], temporal
audio-visual relationships [ 38] which involves object detec-
tion/segmentation, relation detection and moment retrieval
in video content. Additionally, there are challenges such as
visual question answering [ 39,43,43] and video caption-
ing [20,47,52]. Recently, video grounding [ 18,23,33,46]
18385
Appearance: person  in black t-shirt
Situation: person  in conversation
Position: person  standing to the right of person
Relation: person  talking to personAppearance: person  black and blue t-shirt
Situation: person  in conversation
Position: person  standing to the left of person
Interaction: person  holding  cup
Relation: person  talking to person
Appearance: white and red cup
Situation: cup with water 
Position: cup in the handFigure 2. Example and annotations in our ASPIRe dataset. Best viewed in color and zoom in .person
person
cupstanding to the right
talking to
standing to the left
talking to
holdingbelonging toblack t-shirt
conversation
black and blue t-shirt
conversation
white and red
in the hand
with water
(a) A graph representation of the attributes in Fig. 2.
Sj Si Person ObjectPersonPosition ✓ ✓
Interaction ✗ ✗
Relation ✓ ✓ObjectPosition ✗ ✗
Interaction ✓ ✗
Relation ✗ ✗
(b) Summary of annotated double-actor attributes be-
tween two actors in our ASPIRe dataset. appearance
andsituation aresingle-actor attributes as in 4.1.
has provided activities through natural language in visual
content.
Scene Graph Generation. Biswas et al. [ 1] introduce a
Bayesian strategy for debiasing scene graphs in images, en-
hancing recall without retraining. PE-Net [ 58] leveraging
prototype alignment to improve entity-predicate matching
in a unified embedding space, incorporating novel learn-
ing and regularization to reduce semantic ambiguity. PS-
GTR [ 48] and PSGFormer [ 48] introduce recent innova-
tions in scene graph generation, which utilizes a transformer
encoder-decoder to implicitly model scene graph triplets.
Recently, PSG4DFormer [ 49] has been proposed to predict
segmentation masks and then track them to create associated
scene graphs through a relational component.
For dynamic scenes, TEMPURA [ 26] utilizes temporal
consistency and memory-guided training to enhance the de-
tection of infrequent visual relationships in videos. Cho et
al. [6] introduce the Davidsonian Scene Graph (DSG) for
assessing text-to-image alignment, operating a VQA mod-
ule to process atomic propositions from text prompts and
quantifying the alignment between text and image. Further,
advancements by [ 10,17,22,48] have adapted scene graph
techniques to video, focusing on temporal relationships and
advancing comprehensive scene understanding.
2.3. Limitations of Prior Datasets
Existing datasets exhibit notable limitations that hinder a
comprehensive understanding of interactivity within visual
content. Many of these datasets primarily focus on a lim-
ited set of interactivity types , overlooking the complexity of
real-world interactions. This restricted scope has impeded
the development of models capable of handling a variety of
interactivities, thereby limiting their applicability to diverse
scenarios. Moreover, previous datasets predominantly em-
phasize relationships within single connected components
of the relational graph , neglecting complex scenes. Sparseannotations in some datasets further constrain relationship
modeling, often failing to provide comprehensive coverage
and potentially leading to model bias.
To address these limitations, we introduce the new AS-
PIRe dataset to Visual Interactivity Understanding. The
diversity of the ASPIRe dataset is showcased through its
wide range of scenes and settings, distributed in seven sce-
narios. Therefore, ASPIRe distinguishes itself from earlier
datasets, including five types of interactivity, as in Fig. 2.
3. Dataset Overview
3.1. Dataset Collection and Annotation
We introduce a dataset compiled from seven distinct sources,
each contributing unique perspectives to our collection.
The ArgoVerse [ 4] and BDD [ 53] datasets focus on out-
door driving scenes, providing valuable insights into real-
world traffic scenarios. In contrast, the LaSOT [ 8] and
YFCC100M [ 36] datasets consist of in-the-wild videos, cap-
turing a diverse spectrum of human experiences and online
interactions. Additionally, our dataset incorporates content
from the A V A [ 12], Charades [ 32], and HACS [ 56] datasets,
encompassing videos that depict various human interactions,
including interactions between humans and objects. This
compilation results in a diverse scene featuring 833 objects.
Therefore, the ASPIRe dataset enhances the understanding
of activities, surpassing traditional image datasets like Vi-
sual Genome [ 16] and PSG [ 48] by integrating video data.
This crucial integration brings a dynamic dimension to scene
analysis that is conspicuously absent in static datasets. AS-
PIRe stands out for its exceptional detail, demonstrating the
dynamic interactivities over time. ASPIRe has a depth of
interactivities context that is notably comprehensive of other
datasets while only presenting the relationship of humans,
including VidOR [ 31], Action Genome [ 14] and PVSG [ 50],
marking a considerable stride in the scene understanding.
18386
(a) Video sources.
 (b) Interactivity types.
Figure 3. Statistics from the proposed ASPIRe dataset.
To this end, we introduce a structured annotation file
anchored by a primary key named data . This file as-
sembles dictionaries associated with a particular frame and
detailed annotations. Each dictionary contains two cru-
cial lists: segments_info andannotations . The
segments_info list is a collection of dictionaries that
describe the individual segments of the image, and the
annotations list consists of dictionaries that offer bound-
ing boxes and masking details for each segment. Addition-
ally, objects identified within these segments and annota-
tions are assigned the track_id to maintain the identity
within a video. In particular, the annotations within the
ASPIRe dataset are distinguished by five interactivity de-
scriptors: (i) appearances details visual traits of subjects
or objects; (ii) situations describes the environmental
context; (iii) positions identifies the location and orien-
tation; (iv) interactions captures the dynamic actions
between Human-Object ; (v)relations define the con-
nections and associations between Human-Human .
3.2. Dataset Statistics
TheASPIRe dataset is quantitatively analyzed in Table 1 and
visually represented in Fig. 3. ASPIRe contains 1,488 videos
covering 833 object categories and 4,549 interactivities, in-
cluding appearances, situations, positions, interactions, and
relationships. The dataset is especially remarkable for its
videos that depict a comprehensive and intricate variety of
interactivities among subjects, with the number of appear-
ances recorded at 722, situations at 2,902, positions at 130,
interactions at 565, and relations at 230. Furthermore, the
dataset features objects annotated with boxes and masks,
amounting to 167,751 detail annotations.
We provide a detailed analysis of average occurrences
within each video of the ASPIRe dataset. On average, sub-
jects are featured at 4.5 per video, showcasing diversity in
the presence of objects. Both the frequency of appearances
and situations remain steady at an average of 4.5 occurrences
per video, suggesting a uniform representation of visual el-
ements and their contextual narratives. Positions have a
marginally lower average of 4.3 per video. Interactions and
relationships averaged around 4.0 instances per video.
Figure 4. The terminologies used in our proposed ASPIRe dataset
andHierarchical Interlacement Graph .
4. Methodology
4.1. Terminologies
Fig. 4 illustrates our definitions for analyzing interactivities
temporally. Fig. 4a shows the original definition of interactiv-
ities within the subjects as annotated in our proposed ASPIRe
dataset. Interactivities refer to the relationship between sub-
jects. Fig. 4b illustrates a new term Interlacements , which
are interactivities that span across two or sets of nodes in time
or frames. Interlacements is our novel design representing
how the interactivities evolve in our proposed HIG model,
which will be present in the next Section 5. Fig. 4b has two
parts, including double-actor andsingle-actor attribute inter-
lacements. Fig. 4b(i) defines double-actor attributes. double-
actor attributess include position ,interaction , and relation ,
which are attributes that involve two subjects. Fig. 4b(ii) de-
fines single-actor attributes. Single-actor attributes include
appearance andsituation , attributes of individual subjects.
4.2. Problem Formulation
Given a video input ∈RT×H×W×3consisting of Tframes
and frame size of H×W, we identify a set of distinct
subjects, represented as vertices in our graph, Vt={Si}tat
a particular time tand an interactivity set Ias in Eqn. (1).
I(Si, Sj) =n
A(Si),S(Si),
PO(Si, Sj),IN(Si, Sj),RE(Si, Sj)o
(1)
It encapsulates all possible interactivities between subjects.
Each element in Iprovides a fine-grain classification of
the interactivity types. These interactivities are appearance
A(Si), situation S(Si)to express the single-actor attributes,
and position PO(Si, Sj), interaction IN(Si, Sj)and re-
lationRE(Si, Sj)give the double-actor attributes, respec-
tively. The primary objective is to construct a function f.
For each pair of subjects and each frame in the video, f
identifies the most fitting interactivities from the set I. This
function is represented in Eqn. (2).
f:Vt×Vt→I (2)
For every pair of objects drawn from Vt, the function f
learns to predict an interactivity set I, defining the Visual
Interactivity Understanding task.
18387
5. Our Proposed Method
Eqn. (2)is the primary objective in this problem. Our design
of the graph structure, as in Fig. 5, will be described below.
5.1. Hierarchical Interlacement Graph (HIG)
HIG model is designed to capture the complex dynamics
of object interactivity across both spatial and temporal di-
mensions [ 3]. It represents a video as a sequence of graphs
{Gt(Vt, Et)}T
t=1at the first layer, where each graph Gtcor-
responds to a pair of frames. Here, Vtdenotes the set of
nodes, and Etrepresents the set of edges at time t. As the
model progresses through subsequent layers, it combines
graphs from the previous layer to form new, more compre-
hensive graphs, culminating in a single graph cell at the
highest level L, representing the entire video interlacement.
HIG Blocks . The HIG model consists of HIG blocks, each
representing a distinct level of interactivity within the hierar-
chical structure. These blocks function consistently across
all levels l∈ {1, . . . , L }. At each level l, the model inte-
grates graphs from the previous level to enhance the under-
standing of interactivity across spatial and temporal dimen-
sions, as detailed in Algorithm 1.
The feature representation F(l)
t(Si)is dynamically up-
dated for every node Siat each level land time frame t.
This update involves transformations and aggregations of
information from the neighboring nodes of Si. Each node Si
in the graph encapsulates a feature set that evolves through
the hierarchical levels, progressing horizontally across levels
and vertically across time frames, starting from t= 1 to
Tl=T−l+ 1at each level. Specifically, at each level,
the model transitions from processing a larger number of
simpler graphs to fewer, more complex graphs. The feature
representation F(l)
t(Si)at level l, with l >1, is derived by
aggregating transformed features of neighboring nodes from
the previous level l−1as shown in Eqn. (3).
F(l)
t(Si) =X
Sj∈N(Si)F(l−1)
t(Sj) (3)
In Eqn. (3), the feature representation of a node at level lis
the sum of the transformed features of its neighboring nodes
from the previous level. For each node Si, the function N
identifies a set of neighboring nodes that share similar at-
tributes based on similarity scores. This procedure enhances
the comprehensiveness of each node feature set as it ascends
through the hierarchical layers.
Message-Passing Mechanism . In our hierarchical design,
nodes are interconnected through a message-passing mech-
anism. The message m(l)
t(Si, Sj)at level land time tis
influenced by the weight matrix W(l)
ijand the feature vector
F(l−1)
t (Sj)transmitted from SjtoSi. The message from
node SjtoSiis represented as in Eqn. (4).
m(l)
t(Si, Sj) =W(l)
ij· F(l−1)
t(Sj) (4)Algorithm 1 HIG Construction and Feature Embedding
•Input: Frames as graphs {Gt(Vt, Et)}T
t=1; initial features F(0)
t(Si)
for each node Si; number of hierarchical levels L; weight matrices W(l)
ij
for all levels l∈ {1, . . . , L }and node pairs Si, Sj∈Vt.
•Output: I(Si, Sj)
1:forl= 1 toLdo
2: Tl←T−l+ 1
3: fort= 1 toTldo
4: Gl,t(Vl,t, El,t)←ConstructGraph( Gt, l)
5: forSi∈Vl,tdo
6: m(l)
t(Si, Sj)← W(l)
ij· F(l−1)
t (Sj),∀Sj∈ N(Si)
7: F(l)
t(Si)←PTl
t=1F(l−1)
t (Sj),∀Sj∈ N(Si)
8: end for
9: end for
10:end for
11:(V′
t, E′
t)←(V′
L,TL, E′
L,TL)
12:{F′
t(Si)}Si∈V′
t← {F′
L,TL(Si)}Si∈V′
L,TL
13:for(Si, Sj)∈V′
t×V′
tdo
14: I(Si, Sj)← C
m(L)
1(Si, Sj),F(L)
1(Si)
15:end for
In Eqn. (4), the message is a product of the weight matrix
specific to that level and the feature vector of the sending
node. The message m(l)
t(Si, Sj)is transmitted from node
Sjto node Sishaped by the dimensions of the weight matrix
W(l)
ijand the feature vector F(l−1)
t (Sj). The weight matrix
W(l)
ij, critical at level l, typically has a shape of (Dl×Dl−1),
where Dldenotes the feature dimension at level landDl−1
represents the dimension at the preceding level l−1. Si-
multaneously, the feature vector of the node Sjfrom the
previous layer, denoted as F(l−1)
t (Sj), is represented as a
column vector with dimensions of (Dl−1×1).
Hierarchical Aggregation . As the HIG model traverses its
hierarchical structure, it progressively aggregates and refines
node features from the initial to the final level. This tran-
sition involves combining and transforming node features,
ensuring that the intricate details captured at lower levels
are seamlessly integrated into the higher-level context. The
process culminates at the highest level L, where the model
consolidates all the refined features into a single graph cell
att= 1, as represented in Eqn. (5).
F(L)
1(Si) =X
Sj∈N(Si)F(L−1)
1 (Sj) (5)
Eqn. (5)indicates the final feature representation F(L)
1(Si)
at level Lis an aggregation of the transformed features of
its neighboring nodes from the previous level. This final
representation encapsulates the comprehensive interactivity
information from all hierarchical levels.
Interactivity Prediction . For every pair of nodes (Si, Sj),
the function Cis employed to analyze their interactivity. This
function considers both the message m(L)
1(Si, Sj), which
18388
Figure 5. Our proposed Hierarchical Interlacement Graph . The highlighted attributes denote the temporal changes in the graph. Then, all
predicted interactivities are accumulated into the next hierarchy level. A higher-level graph cell covers a bigger portion of video frames .
encapsulates the interactivity between the nodes, and the fea-
ture representation F(L)
1(Si), which reflects the features of
the node Siat the highest hierarchical level. The prediction
function is formulated as in Eqn. (6).
I(Si, Sj) =C
m(L)
1(Si, Sj),F(L)
1(Si)
(6)
In Eqn. (6),I(Si, Sj)represents the predicted interactivi-
ties between nodes SiandSj. The classification function
Coperates on the features and messages at the highest hi-
erarchical level to produce a fine-grained classification on
the edge connecting these nodes. The output of this function
is represented in the set I, where each element provides a
detailed classification of the five interactivity types, includ-
ing appearance ( A), situation ( S), position ( PO), interaction
(IN), and relation ( RE).
Designing a framework as our HIG model, involving data
with varying subjects has distinct advantages. First, graphs
are well-suited for the task, where the number of subjects
can vary. Second, the message-passing mechanism allows
interactivities to be exchanged between neighboring nodes.
Finally, HIG allows for a contextual understanding of where
and when information occurs in the video, which is essential
for tasks that require precise timestamps of events or actions.
5.2. Training Loss
The HIG model employs an integral training loss utilizing
hierarchical weight sharing and sequential unfreezing tech-
niques, with details provided in the following section.
Sequential Training Strategy. The HIG framework em-
ploys a hierarchical weight-sharing strategy to enhance the
efficiency of the training process. By sharing weights across
different levels of the GNN hierarchy, the model takes advan-
tage of a reduction in the total number of parameters, whichoperates as a regularizing mechanism to improve model gen-
eralization. In particular, training within the HIG framework
is conducted through a sequential unfreezing strategy. Ini-
tially, the base level is activated, and subsequent levels are
progressively unfrozen. This strategy allows the network to
adapt to the feature embeddings F(l)
t(Si), which are refined
at each level land time step t.
At each level, the Focal Loss function [ 19] is employed
for edge classification, following [ 14,48,50], as in Eqn. (7).
L(F(l)
t(Si)) =−αt(1−pt(F(l)
t(Si)))γlog(pt(F(l)
t(Si)))(7)
where ptmeasures the probability for the class, αtis a
weighting factor, and γis a parameter that adjusts the rate.
Loss Aggregation. The losses computed at each hierarchical
level are aggregated to determine the total loss for the model
as in Eqn. (8). This aggregation ensures that the training
signal is comprehensive and encapsulates the learning objec-
tives at each hierarchy level. The HIG framework promotes
a nuanced training process, empowering the GNN to model
the inherent hierarchical structures.
Ltotal=X
lL(F(l)
t(Si)) (8)
6. Experiment Results
6.1. Implementation Details
Dataset. The training set comprises 55K subjects and 197K
interactivities across 500 videos. The validation set, which is
used as the test set, comprises 988 videos with 113K subjects
and 400 interactivities. In addition, we use PSG [ 48] to
evaluate our performance on the image data.
Model Configurations. This work uses the PyTorch frame-
work and operates on 8 ×NVIDIA RTX A6000 GPUs. It
18389
Table 2. Comparison against baseline methods on single-actor attributes.
Method Interlacement R/mR@20 R/mR@50 R/mR@100
VanillaAppearance 10.88 / 0.09 12.19 / 0.09 14.16 / 0.08
Situation 2.87 / 0.02 5.29 / 0.03 9.05 / 0.03
HandcraftedAppearance 11.09 / 0.11 12.26 / 0.13 14.27 / 0.17
Situation 3.08 / 0.04 5.36 / 0.07 9.16 / 0.12
ConvolutionAppearance 11.32 / 0.11 12.28 / 0.25 14.32 / 0.22
Situation 3.31 / 0.04 5.38 / 0.19 9.21 / 0.17
TransformerAppearance 12.35 / 0.62 13.89 / 0.64 16.10 / 0.66
Situation 4.54 / 0.55 6.99 / 0.58 10.99 / 0.61
HIG (Our)Appearance 15.02 / 0.60 18.60 /0.64 20.11 / 0.65
Situation 5.01 /0.56 7.02 / 0.55 12.01 /0.63
Table 3. Comparison against previous methods on ASPIRe .
Method Interlacement R/mR@20 R/mR@50 R/mR@100
IMP [45]Position 9.70 / 0.49 9.70 / 0.49 9.70 / 0.49
Interaction 12.79 / 0.08 12.79 / 0.08 12.79 / 0.08
Relation 11.51 / 0.32 11.51 / 0.32 11.51 / 0.32
MOTIFS [54]Position 6.89 / 0.48 8.49 / 0.38 8.70 / 0.40
Interaction 8.83 / 0.12 10.33 / 0.12 10.57 / 0.12
Relation 8.72 / 0.32 10.26 / 0.32 10.55 / 0.32
VCTree [34]Position 4.18 / 0.39 6.75 / 0.40 8.59 / 0.42
Interaction 6.23 / 0.10 9.58 / 0.10 11.63 / 0.10
Relation 6.51 / 0.27 9.82 / 0.28 11.51 / 0.28
GPSNet [21]Position 12.89 / 1.26 12.89 / 1.26 12.89 / 1.26
Interaction 10.89 / 0.11 10.89 / 0.12 10.89 / 0.12
Relation 9.87 / 0.35 9.87 / 0.35 9.87 / 0.35
HIG (Ours)Position 13.02 / 0.09 24.52 /1.33 42.33 / 1.12
Interaction 12.02 /0.11 24.65 /0.12 41.65 /0.14
Relation 10.26 / 0.29 23.72 / 0.34 41.47 /0.39
utilizes a training batch size of 1 and employs the AdamW
Optimizer, starting with an initial learning rate of 0.0001.
We employ PyTorch Geometric [ 9] for constructing graphs
where nodes represent detections and edges signify poten-
tial interactivities. It integrates a ResNet-50 [ 13] backbone
trained with DETR [ 2]. Our framework involves edge prun-
ing using scatter_min andscatter_max for aggre-
gating node features such as bounding box coordinates and
track identification. Then, the framework calculates cosine
similarity and selects the top-k (k= 12 ) nearest neighbors.
Metrics. Inspired by [ 31,48,50], we calculate the recall
metric for the Visual Interactivity Understanding task to pre-
dict a set of triplets that accurately describe the input video.
The model predicts the category labels for the subject, object,
and predicate within each triplet. Each triplet represents a
distinct interactivity in the range time t1andt2. Moreover,
each triplet corresponds to a specific subject in single-actor
scenarios and a pair of subjects in double-actor scenarios
based on a predefined set. To this end, we leverage the
standard metrics used in activity understanding, including
R@KandmR@Kutilized to evaluate the recall of top K
categories and their mean recall, respectively.
6.2. Ablation Study
Baseline Methods. We re-implemented four baseline meth-
ods introduced in [ 50] and presented in Table 2 and Table 4
since the official implementation is unavailable. Table 2
compares all baseline methods and the HIG along single-
actor attributes, and Table 4 compares double-actor attributes.
HIG is designed to analyze videos through a hierarchical
structure that progressively accumulates temporal informa-Table 4. Comparison against baseline methods on double-actor attributes.
Method Interlacement R/mR@20 R/mR@50 R/mR@100
VanillaPosition 10.52 / 0.50 21.97 / 0.55 38.05 / 0.62
Interaction 10.16 / 0.12 22.35 / 0.13 39.91 / 0.14
Relation 9.71 / 0.32 21.96 / 0.36 39.11 / 0.40
HandcraftedPosition 10.73 / 0.52 22.04 / 0.59 38.16 / 0.71
Interaction 10.37 / 0.14 22.42 / 0.17 40.02 / 0.23
Relation 9.92 / 0.34 22.03 / 0.40 39.22 / 0.49
ConvolutionPosition 10.96 / 0.52 22.06 / 0.71 38.21 / 0.76
Interaction 10.60 / 0.14 22.44 / 0.29 40.07 / 0.28
Relation 10.15 / 0.34 22.05 / 0.52 39.27 / 0.54
TransformerPosition 11.04 / 0.83 22.52 / 0.90 38.84 / 1.02
Interaction 10.68 / 0.45 22.90 / 0.48 40.70 / 0.52
Relation 10.23 / 0.65 22.51 / 0.71 39.90 / 0.96
HIG (Ours)Position 13.02 / 0.09 24.52 /1.33 42.33 /1.12
Interaction 12.02 / 0.11 24.65 / 0.12 41.65 / 0.14
Relation 10.26 / 0.29 23.72 / 0.34 41.47 / 0.39
Table 5. Comparison at different video sampling rates of our HIG.
Sampling Rate Interlacement R/mR@20 R/mR@50 R/mR@100 FPS
2 (Half)Appearance 12.13 / 0.59 12.25 / 0.63 7.48 / 0.64
26.4Situation 2.12 / 0.55 5.67 / 0.54 8.62 / 0.62
Position 10.13 / 0.08 18.17 / 1.32 29.7 / 1.11
Interaction 9.13 / 0.10 18.30 / 0.11 29.02 / 0.13
Relation 7.37 / 0.28 17.37 / 0.33 28.84 / 0.38
1 (Full)Appearance 15.02 / 0.60 18.60 / 0.64 20.11 / 0.65
24.2Situation 5.01 / 0.56 7.02 / 0.55 12.01 / 0.63
Position 13.02 / 0.09 24.52 / 1.33 42.33 / 1.12
Interaction 12.02 / 0.11 24.65 / 0.12 41.65 / 0.14
Relation 10.26 / 0.29 23.72 / 0.34 41.47 / 0.39
tion across multiple levels. Instead of getting results for
each frame separately, as is done at level l= 1, we prefer
the predictions made at higher levels, where the confidence
score is greater ≥0.9. A higher hierarchy level covers a
more significant portion of the video frame, as in Fig. 5. This
approach effectively reduces noise and produces a higher
recall rate . In particular, the HIG method is better at recog-
nizing single-actor attributes than other baselines, including
Transformer, Convolution, Handcrafted, and Vanilla. Specif-
ically, the HIG model is 2.67% higher than the Transformer,
the best method in baseline at R@20. HIG is also better for
the double-actor attributes, especially in figuring out interac-
tions and relations. It is 1.34% higher than Transformers at
R@20 when identifying interactions. We visualize keyframe
predictions in a video, as shown in Fig. 6.
Video Sampling Rates. Table 5 explores the influence of
frame sampling rates on the performance of the HIG model
in deployment. Our analysis focuses on evaluating the per-
formance under a reduced number of frames. In the AS-
PIRe dataset, the testing set includes 988 videos, totaling
10,456,48 frames. We address the efficiency of the HIG
model by halving the number of frames in each video. In
particular, we discard one frame out of every two successive
frames. Our experiment reveals a trade-off between recall
score and inference time, where the HIG model experiences
a decrease in recall performance but achieves a 2.2 FPS
increase in inference speed.
6.3. Comparison with State-of-the-Arts
Performance on ASPIRe .We provide the comparative
analysis with SOTAs in Table 3, including IMP [ 45], MO-
18390
dog-1
next to
dog-2in front of
in front of
next toperson-1
in front ofin front of
ball-1dog-1
dog-2hittingplaying with
playing withperson-1
ball-1dog-1
dog-2owner
owner
ownerperson-1
ball-1
dog-1
next to
dog-2in the middle
in the middle
in the leftperson-1
in front ofin front of
ball-1catchingdog-1
running dog-2watchingplaying with
playing withperson-1
ball-1dog-1
dog-2owner
owner
ownerperson-1
ball-1
Interaction Relation Position
Figure 6. Qualitative results of position, interaction, and relation from scene graphs generated from the HIG model.
Table 6. Comparison against previous methods on SGG task.
Method Interlacement R/mR@20 R/mR@50 R/mR@100
IMP [45]Position 0.25 / 0.36 0.29 / 0.35 0.30 / 0.33
Interaction 0.71 / 0.13 0.98 / 0.12 1.15 / 0.13
Relation 0.80 / 0.26 0.81 / 0.25 0.84 / 0.24
MOTIFS [54]Position 0.23 / 0.43 0.23 / 0.43 0.31 /0.38
Interaction 0.39 / 0.11 0.94 / 0.11 1.17 / 0.10
Relation 0.31 / 0.30 0.32 / 0.28 0.53 / 0.32
VCTree [34]Position 0.13 / 0.23 0.14 / 0.22 0.14 / 0.21
Interaction 0.55 / 0.07 0.65 / 0.08 0.72 / 0.08
Relation 0.39 / 0.18 0.39 / 0.20 0.43 / 0.21
GPSNet [21]Position 0.09 / 0.46 1.17 / 0.37 1.32 / 0.46
Interaction 0.99 / 0.09 1.02 / 0.09 1.11 / 0.09
Relation 0.14 / 0.23 0.16 / 0.13 0.29 / 0.23
HIG (Ours)Position 1.00 /0.42 2.40 /0.44 4.87 /0.47
Interaction 1.30 /0.09 3.45 /0.11 6.93 /0.12
Relation 1.26 /0.27 3.43 /0.30 7.02 /0.32
TIFS [ 54], VCTree [ 34], and GPSNet [ 21]. In the ASPIRe
dataset, the HIG method shows impressive results in identi-
fying the position on recall at different top K. In addition,
the HIG model performs well on identifying relations when
it is higher than 1.13% at R@20 compared to GPSNet.
Scene Graph Generation (SGG). We extend the capability
of the HIG model while incorporating image-based scene
graph generation into the training process presented in Ta-
ble 6. Since the prior method was designed for interactions
between pairs of subjects, we focus our comparison on the
double-actor attributes. The HIG method demonstrates su-
perior performance across all interlacement highlighting its
advanced proficiency in attribute recognition within frame-
based scene graph generation scenarios. Compared to the
best-performing previous method, GPSNet, the HIG model
achieves improvements of 3.55%, 5.82%, and 6.73% at
R@100 for position, interaction, and relation.
Performance on PSG. In addition to evaluating our method
on a video dataset, we demonstrate its effectiveness on an
image dataset by comparing it with state-of-the-art methods
on the PSG dataset, as presented in Table 7. When applied to
the PSG dataset, the HIG model treats each image as a single-
frame video, shifting its focus to spatial interactivity rather
than temporal interactivity . Although our model is primarily
designed for video datasets, it achieves comparable resultsTable 7. Comparison against previous methods on PSG [48].
Method R/mR@20 R/mR@50 R/mR@100
IMP [45] 16.5 / 6.52 18.2 / 7.05 18.6 / 7.23
MOTIFS [54] 20.0 / 9.10 21.7 / 9.57 22.0 / 9.69
VCTree [34] 20.6 /9.70 22.1 / 10.2 22.5 / 10.2
GPSNet [21] 17.8 / 7.03 19.6 / 7.49 20.1 / 7.67
PSGFormer [48] 18.6 / 16.7 20.4 / 19.3 20.7 / 19.7
HIG (Ours) 19.4 / 6.42 22.3 / 8.13 26.3 / 9.70
on the image dataset, with only a slight decrease at R@20
compared to state-of-the-art methods. Notably, the HIG
model outperforms VCTree by 3.8% in terms of R@100 ,
highlighting the strength of the graph representation.
7. Conclusion
We addressed the Visual Interactivity Understanding prob-
lem by introducing the ASPIRe dataset and the Hierarchical
Interlacement Graph . APSIRe established a new benchmark
with its extensive predicate types offering nuanced inter-
activity perspectives. Meanwhile, HIG provides a unified
hierarchical structure for capturing complex video interlace-
ments, demonstrating scalability and flexibility in handling
five interactivity types. Additionally, we provided extensive
experiments showcasing the efficiency of HIG and achieving
state-of-the-art results in both video and image datasets.
Limitations. While the HIG approach significantly ad-
vanced the understanding of interactivities, it faced certain
limitations. Computing possible interlacements became a
computational bottleneck, potentially hindering real-time
applications. Also, the framework faced challenges in han-
dling long-duration videos, where the continual learning of
new interactivities could lead to the decay of previously ac-
quired knowledge. As the HIG model was tailored for video
datasets, its image-based performance might not be optimal.
Acknowledgment. This work is partly supported by NSF
Data Science and Data Analytics that are Robust and Trusted
(DART), NSF SBIR Phase 2, and Arkansas Biosciences
Institute (ABI) grants. We also acknowledge the Arkansas
High-Performance Computing Center for providing GPUs.
18391
References
[1]Bashirul Azam Biswas and Qiang Ji. Probabilistic debiasing
of scene graphs. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 10429–
10438, 2023. 3
[2]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-
to-end object detection with transformers. In European con-
ference on computer vision , pages 213–229. Springer, 2020.
7
[3]Orcun Cetintas, Guillem Brasó, and Laura Leal-Taixé. Uni-
fying short and long-term tracking with graph hierarchies.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 22877–22887, 2023. 5
[4]Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet
Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter
Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d
tracking and forecasting with rich maps. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 8748–8757, 2019. 3
[5]Jiawei Chen and Chiu Man Ho. Mm-vit multi-modal video
transformer for compressed video action recognition. In Pro-
ceedings of the IEEE/CVF winter conference on applications
of computer vision , pages 1910–1921, 2022. 2
[6]Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay
Krishna, Jason Baldridge, Mohit Bansal, Jordi Pont-Tuset,
and Su Wang. Davidsonian scene graph: Improving reliability
in fine-grained evaluation for text-image generation. arXiv
preprint arXiv:2310.18235 , 2023. 3
[7]Dima Damen, Hazel Doughty, Giovanni Maria Farinella, An-
tonino Furnari, Evangelos Kazakos, Jian Ma, Davide Molti-
santi, Jonathan Munro, Toby Perrett, Will Price, et al. Rescal-
ing egocentric vision: Collection, pipeline and challenges for
epic-kitchens-100. International Journal of Computer Vision ,
pages 1–23, 2022. 2
[8]Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia
Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.
Lasot: A high-quality benchmark for large-scale single ob-
ject tracking. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pages 5374–5383,
2019. 3
[9]Matthias Fey and Jan E. Lenssen. Fast graph representation
learning with PyTorch Geometric. In ICLR Workshop on
Representation Learning on Graphs and Manifolds , 2019. 7
[10] Arushi Goel, Basura Fernando, Frank Keller, and Hakan Bilen.
Not all relations are equal: Mining informative labels for
scene graph generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 15596–15606, 2022. 3
[11] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary
Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger,
Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the
world in 3,000 hours of egocentric video. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 18995–19012, 2022. 2
[12] Chunhui Gu, Chen Sun, David A Ross, Carl V ondrick, Car-
oline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,
George Toderici, Susanna Ricco, Rahul Sukthankar, et al.Ava: A video dataset of spatio-temporally localized atomic
visual actions. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 6047–6056,
2018. 3
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 7
[14] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos
Niebles. Action genome: Actions as compositions of spatio-
temporal scene graphs. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 10236–10247, 2020. 1, 2, 3, 6
[15] Zeeshan Khan, CV Jawahar, and Makarand Tapaswi.
Grounded video situation recognition. Advances in Neural
Information Processing Systems , 35:8199–8210, 2022. 2
[16] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. International journal of computer vision ,
123:32–73, 2017. 2, 3
[17] Lin Li, Long Chen, Yifeng Huang, Zhimeng Zhang,
Songyang Zhang, and Jun Xiao. The devil is in the labels:
Noisy label correction for robust scene graph generation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 18869–18878, 2022. 3
[18] Mengze Li, Han Wang, Wenqiao Zhang, Jiaxu Miao, Zhou
Zhao, Shengyu Zhang, Wei Ji, and Fei Wu. Winner: Weakly-
supervised hierarchical decomposition and alignment for
spatio-temporal video grounding. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 23090–23099, 2023. 1, 2
[19] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Dollár. Focal loss for dense object detection. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 2980–2988, 2017. 6
[20] Wang Lin, Tao Jin, Ye Wang, Wenwen Pan, Linjun Li, Xize
Cheng, and Zhou Zhao. Exploring group video captioning
with efficient relational approximation. In Proceedings of
the IEEE/CVF International Conference on Computer Vision ,
pages 15281–15290, 2023. 1, 2
[21] Xin Lin, Changxing Ding, Jinquan Zeng, and Dacheng Tao.
Gps-net: Graph property sensing network for scene graph
generation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 3746–3753,
2020. 7, 8
[22] Xin Lin, Changxing Ding, Yibing Zhan, Zijian Li, and
Dacheng Tao. Hl-net: Heterophily learning network for
scene graph generation. In proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , pages
19476–19485, 2022. 3
[23] Zihang Lin, Chaolei Tan, Jian-Fang Hu, Zhi Jin, Tiancai
Ye, and Wei-Shi Zheng. Collaborative static and dynamic
vision-language streams for spatio-temporal video grounding.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 23100–23109, 2023.
1, 2
[24] Chenchen Liu, Yang Jin, Kehan Xu, Guoqiang Gong, and
18392
Yadong Mu. Beyond short-term snippet: Video relation de-
tection with spatio-temporal global context. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 10840–10849, 2020. 2
[25] WonJun Moon, Sangeek Hyun, SangUk Park, Dongchan Park,
and Jae-Pil Heo. Query-dependent video representation for
moment retrieval and highlight detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 23023–23033, 2023. 2
[26] Sayak Nag, Kyle Min, Subarna Tripathi, and Amit K Roy-
Chowdhury. Unbiased scene graph generation in videos. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 22803–22813, 2023. 3
[27] Pha Nguyen, Kha Gia Quach, Chi Nhan Duong, Ngan Le,
Xuan-Bac Nguyen, and Khoa Luu. Multi-camera multiple
3d object tracking on the move for autonomous vehicles.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 2569–2578, 2022. 2
[28] Pha Nguyen, Kha Gia Quach, Kris Kitani, and Khoa Luu.
Type-to-track: Retrieve any object via prompt-based tracking.
Advances in Neural Information Processing Systems , 36, 2024.
2
[29] Kha Gia Quach, Ngan Le, Chi Nhan Duong, Ibsa Jalata,
Kaushik Roy, and Khoa Luu. Non-volume preserving-based
fusion to group-level emotion recognition on crowd videos.
Pattern Recognition , 128:108646, 2022. 2
[30] Arka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia, and
Aniruddha Kembhavi. Visual semantic role labeling for video
understanding. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5589–
5600, 2021. 2
[31] Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang,
and Tat-Seng Chua. Annotating objects and relations in user-
generated videos. In Proceedings of the 2019 on International
Conference on Multimedia Retrieval , pages 279–287, 2019.
1, 2, 3, 7
[32] Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali
Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in
homes: Crowdsourcing data collection for activity under-
standing. In Computer Vision–ECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11–14,
2016, Proceedings, Part I 14 , pages 510–526. Springer, 2016.
3
[33] Chaolei Tan, Zihang Lin, Jian-Fang Hu, Wei-Shi Zheng, and
Jianhuang Lai. Hierarchical semantic correspondence net-
works for video paragraph grounding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 18973–18982, 2023. 1, 2
[34] Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo,
and Wei Liu. Learning to compose dynamic tree structures
for visual contexts. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
6619–6628, 2019. 7, 8
[35] Anirudh Thatipelli, Sanath Narayan, Salman Khan,
Rao Muhammad Anwer, Fahad Shahbaz Khan, and Bernard
Ghanem. Spatio-temporal relation modeling for few-shot
action recognition. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
19958–19967, 2022. 2[36] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin
Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-
Jia Li. Yfcc100m: The new data in multimedia research.
Communications of the ACM , 59(2):64–73, 2016. 3
[37] Thanh-Dat Truong, Quoc-Huy Bui, Chi Nhan Duong, Han-
Seok Seo, Son Lam Phung, Xin Li, and Khoa Luu. Direc-
former: A directed attention in transformer approach to robust
action recognition. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
20030–20040, 2022. 2
[38] Thanh-Dat Truong, Chi Nhan Duong, Hoang Anh Pham,
Bhiksha Raj, Ngan Le, Khoa Luu, et al. The right to talk:
An audio-visual transformer approach. In Proceedings of
the IEEE/CVF International Conference on Computer Vision ,
pages 1105–1114, 2021. 2
[39] Aisha Urooj, Hilde Kuehne, Bo Wu, Kim Chheu, Walid Bous-
selham, Chuang Gan, Niels Lobo, and Mubarak Shah. Learn-
ing situation hyper-graphs for video question answering. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 14879–14889, 2023. 2
[40] Xiang Wang, Shiwei Zhang, Zhiwu Qing, Mingqian Tang,
Zhengrong Zuo, Changxin Gao, Rong Jin, and Nong Sang.
Hybrid relation guided set matching for few-shot action recog-
nition. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 19948–19957,
2022. 2
[41] Haoqian Wu, Keyu Chen, Haozhe Liu, Mingchen Zhuge,
Bing Li, Ruizhi Qiao, Xiujun Shu, Bei Gan, Liangsheng Xu,
Bo Ren, et al. Newsnet: A novel dataset for hierarchical
temporal segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 10669–10680, 2023. 1, 2
[42] Fanyi Xiao, Kaustav Kundu, Joseph Tighe, and Davide Mod-
olo. Hierarchical self-supervised representation learning for
movie understanding. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
9727–9736, 2022. 2
[43] Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei Ji,
and Tat-Seng Chua. Video as conditional graph hierarchy
for multi-granular question answering. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 36, pages
2804–2812, 2022. 2
[44] Jiazheng Xing, Mengmeng Wang, Yudi Ruan, Bofan Chen,
Yaowei Guo, Boyu Mu, Guang Dai, Jingdong Wang, and
Yong Liu. Boosting few-shot action recognition with graph-
guided hybrid matching. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 1740–
1750, 2023. 2
[45] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei.
Scene graph generation by iterative message passing. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 5410–5419, 2017. 7, 8
[46] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Tubedetr: Spatio-temporal video grounding
with transformers. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
16442–16453, 2022. 2
[47] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, An-
toine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and
18393
Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual
language model for dense video captioning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10714–10726, 2023. 1, 2
[48] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou,
Wayne Zhang, and Ziwei Liu. Panoptic scene graph gen-
eration. In European Conference on Computer Vision , pages
178–196. Springer, 2022. 1, 2, 3, 6, 7, 8
[49] Jingkang Yang, CEN Jun, Wenxuan Peng, Shuai Liu,
Fangzhou Hong, Xiangtai Li, Kaiyang Zhou, Qifeng Chen,
and Ziwei Liu. 4d panoptic scene graph generation. In
Thirty-seventh Conference on Neural Information Processing
Systems , 2023. 3
[50] Jingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo,
Liangyu Chen, Bo Li, Zheng Ma, Kaiyang Zhou, Wayne
Zhang, Chen Change Loy, et al. Panoptic video scene graph
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 18675–
18685, 2023. 1, 2, 3, 6, 7
[51] Xitong Yang, Fu-Jen Chu, Matt Feiszli, Raghav Goyal,
Lorenzo Torresani, and Du Tran. Relational space-time query
in long-form videos. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6398–6408, 2023. 2
[52] Hanhua Ye, Guorong Li, Yuankai Qi, Shuhui Wang, Qing-
ming Huang, and Ming-Hsuan Yang. Hierarchical modu-
lar network for video captioning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 17939–17948, 2022. 1, 2
[53] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying
Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell.
BDD100K: A diverse driving dataset for heterogeneous mul-
titask learning. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , June 2020. 3
[54] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi.
Neural motifs: Scene graph parsing with global context. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 5831–5840, 2018. 7, 8
[55] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng
Liu, and Lianli Gao. Where does it exist: Spatio-temporal
video grounding for multi-form sentences. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10668–10677, 2020. 1, 2
[56] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and
Zhicheng Yan. Hacs: Human action clips and segments
dataset for recognition and temporal localization. In Proceed-
ings of the IEEE/CVF International Conference on Computer
Vision , pages 8668–8678, 2019. 3
[57] Jiaojiao Zhao, Yanyi Zhang, Xinyu Li, Hao Chen, Bing Shuai,
Mingze Xu, Chunhui Liu, Kaustav Kundu, Yuanjun Xiong,
Davide Modolo, et al. Tuber: Tubelet transformer for video
action detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 13598–
13607, 2022. 1, 2
[58] Chaofan Zheng, Xinyu Lyu, Lianli Gao, Bo Dai, and Jingkuan
Song. Prototype-based embedding network for scene graph
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22783–
22792, 2023. 3
18394
