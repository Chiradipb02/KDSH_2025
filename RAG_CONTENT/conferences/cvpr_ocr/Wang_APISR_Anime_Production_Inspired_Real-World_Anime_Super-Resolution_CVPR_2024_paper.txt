APISR: Anime Production Inspired Real-World Anime Super-Resolution
Boyang Wang1Fengyu Yang1,2*Xihang Yu1Chao Zhang3Hanbin Zhao3†
1University of Michigan2Yale University3Zhejiang University
Abstract
While real-world anime super-resolution (SR) has
gained increasing attention in the SR community, exist-
ing methods still adopt techniques from the photorealistic
domain. In this paper, we analyze the anime production
workflow and rethink how to use characteristics of it for
the sake of the real-world anime SR. First, we argue that
video networks and datasets are not necessary for anime
SR due to the repetition use of hand-drawing frames. In-
stead, we propose an anime image collection pipeline by
choosing the least compressed and the most informative
frames from the video sources. Based on this pipeline,
we introduce the Anime Production-oriented Image (API)
dataset. In addition, we identify two anime-specific chal-
lenges of distorted and faint hand-drawn lines and un-
wanted color artifacts. We address the first issue by in-
troducing a prediction-oriented compression module in the
image degradation model and a pseudo-ground truth prepa-
ration with enhanced hand-drawn lines. In addition, we in-
troduce the balanced twin perceptual loss combining both
anime and photorealistic high-level features to mitigate un-
wanted color artifacts and increase visual clarity. We evalu-
ate our method through extensive experiments on the public
benchmark, showing our method outperforms state-of-the-
art anime dataset-trained approaches. The code is avail-
able at https://github.com/Kiteretsu77/APISR.
1. Introduction
As an important subdiscipline of real-world super-
resolution (SR), anime SR focuses on restoring and enhanc-
ing low-quality low-resolution (LR) anime visual art images
and videos to high-quality high-resolution (HR) forms. It
has demonstrated significant practical impacts in the fields
of entertainment and commerce [40, 42, 47, 49, 54]. An
emerging line of work has addressed the problem by ex-
tending SR networks to capture multi-scale information or
learning an adaptive degradation model [40, 49]. We argue
these methods lack understanding of the anime domain as
† Corresponding author
* works done at University of Michigan
(a) AnimeSR(b) VQD-SR(c) APISR (Ours)
Figure 1. Comparisons between proposed APISR and other
SOTA anime SR methods. Ours present clearer and sharper
hand-drawn lines, better restoration with more natural details, and
do not present unwanted color artifacts. Zoom in for best view.
their techniques are directly transplanted from the photore-
alistic SR approach.
In this paper, we thoroughly analyze the anime pro-
duction process, exploring ways to leverage its unique as-
pects for practical applications in anime SR. The produc-
tion workflow first starts with hand-drawing sketches on
paper, which are then colorized and enhanced by computer-
generated imagery (CGI) processing [62]. Then, these pro-
cessed sketches are concatenated into a video. Due to the
fact that the drawing process is extremely labor-intensive
and human eyes are not sensitive to motions [9, 32], it is
a standard practice to reuse a single image across multiple
consecutive frames when forming the video. This procedure
in production motivates us to rethink whether it is neces-
sary and efficient to use video networks and video datasets
to train SR networks in the anime domain.
To this end, we explore the use of image-based meth-
ods and datasets as a unified super-resolution and restora-
tion framework for both anime images and videos. Creating
an image dataset allows us more flexibility to exclusively
choose the least-compressed video frames as our potential
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25574
dataset pool, rather than gathering sequential frames that
contain temporal distortions to create a video dataset. Fur-
thermore, by forming an image dataset, we can selectively
focus on the most informative frames, as anime videos typ-
ically possess less information than photorealistic videos.
If we randomly crop a patch from an anime image, there
is a high probability that it is a monochromatic area sig-
nifying a lack of information. In light of these phenom-
ena, we introduce an anime image collection pipeline that
focuses on keyframes in video, along with an image com-
plexity assessment-based selection criteria. This method is
designed to identify and select the least-compressed and
the most informative images from video sources. Using
our pipeline, we propose Anime Production-oriented Image
(API) dataset for SR training.
In addition, we identify two new anime-specific chal-
lenges for real-world SR tasks. First, in anime production,
the clarity of hand-drawn lines is a highly emphasized de-
tail [5, 22, 49] as shown in Fig. 2 a, but hand-drawn lines
are easily weakened due to compression in internet trans-
mission and physical aging in production. This deteriora-
tion at the edges of lines exerts a substantial negative im-
pact on the visual effects. To address this, we start from
the perspective of restoration and enhancement. Concretely,
we propose a prediction-oriented compression module in
the image degradation model to simulate compression in
internet transmission such that the model trained with this
self-supervised method can restore hand-drawn line distor-
tions. In addition, we propose a ground-truth (GT) enhance-
ment approach to enhance faint, aging hand-drawn lines, by
merging hand-drawn lines extracted from the overly sharp-
ened GT images.
Second, we realize an issue of unwanted color artifacts
in anime images, which is a consequence of employing the
GAN-based SR networks [14] (see Fig. 2 b). These arti-
facts are presented as irregularly shaped colored spots with
varying intensities that are scattered randomly across gen-
erated images, which significantly undermines visual per-
ception. We attribute this issue to the reason that image
features of perceptual loss are trained on the photorealistic
image datasets, which is inconsistent in the anime domain.
To mitigate this issue, we conduct a comprehensive study of
perceptual loss and introduce balanced twin perceptual loss,
which assembles perceptual features from both the photore-
alistic domain and the anime domain by a balanced layer
scaling distribution.
Thus, we summarize our contributions as follows:
• We propose a novel anime dataset curation pipeline that
is capable of collecting the least compressed and the most
informative anime images from video sources.
• We propose an image degradation model to deal with
harder compression restoration challenges, especially for
hand-drawn line distortions, and the first methodologies
(a)(b)Figure 2. We identify two new anime-specific challenges :
(a) Distorted and faint hand-drawn lines frequently appear in
real-world anime images. (b) Unwanted color artifacts in Ani-
meSR [49] and VQD-SR [40]. Zoom in for the best view.
in the anime domain to attentively enhance faint hand-
drawn lines.
• We realize and address the unwanted color artifacts in
GAN-based SR network training caused by the domain
inconsistency of the perceptual loss.
• We thoroughly evaluate our method on the real-world
anime SR dataset and show that our method outperforms
state-of-the-art anime dataset-trained SR approaches by a
large margin with only 13.3% training sample complexity
of the prior work.
2. Realated Work
Real-World Super-Resolution. Classical SR methods [6,
12, 50, 51] typically employs a straightforward approach,
using a single bicubic downsampling operation to convert
high-resolution (HR) ground-truth (GT) images into their
low-resolution (LR) counterparts. Classical image restora-
tion methods [25, 26, 52, 53] train different weights for
different tasks. In contrast, real-world SR is dedicated to
implementing a sophisticated degradation model by one
model weight to restore the diverse degradations found in
the real-world scenario, such as blurring, noise, and com-
pression [20, 25, 26, 42, 47, 58].
Generic degradation model design can be broadly classi-
fied into two categories: explicit models [20, 25, 42, 47, 58]
and implicit models [40, 49, 57]. Explicit degradation mod-
els employ kernels and mathematical formulas to simulate
real-world degradation processes. On the other hand, the
implicit degradation models focus on training neural net-
works to capture the distribution of real-world degrada-
tions. Nevertheless, implicit models face challenges of in-
terpretability and scalability. The efficacy of implicit mod-
els lacks a clear rationale, and adapting them to new do-
mains requires the creation of bespoke datasets and extra
training complexity.
Anime Processing. Anime represents a distinctive form
of visual art, often characterized by exaggerated visual rep-
resentation. Creators of anime typically start by sketch-
ing line art, followed by 2D and 3D animation tech-
niques, which include elements like colorization, CGI ef-
25575
(a)
000.20.40.60.81.01234520406080100120123456Number of Samples
DensityImage Complexity ComparisonImage Data Size of Frame Types(b)
Image Data Size (10!Bytes)IC9600 ValueFigure 3. Histogram of (a) the average image data size comparison
between I-Frames and Non-I-Frames (P and B-Frame) in collected
video sources and (b) image complexity [13] comparison between
proposed API and A VC [49] dataset.
HyperIQA↑: 0.519Brisque↓: 20.48
HyperIQA↑: 0.294Brisque↓: 33.78Image Complexity (IC9600) ↑ : 0.097Image Complexity (IC9600) ↑ : 0.613
Figure 4. Image Quality Assessment (IQA) with HyperIQA [38]
and Brisque [28] vs. Image Complexity Assessment (ICA) with
IC9600 [13]. IQA favors simple scenes and gives low scores to
images with strong CGI. However, ICA is the opposite.
fects, and frame interpolation. Notably, recent research in
the realm of anime has garnered substantial attention, e.g.,
AI painting with anime content [15, 18, 60], vectorization of
anime images [56, 61], anime interpolation and inbetween-
ing [9, 35, 37], anime sketch colorization [4, 5, 44, 59], 3D
representation [10, 36], and anime domain adaptation [22].
AnimeSR (NeurIPS 2022) [49] and VQD-SR (ICCV
2023) [40] are two recent representative studies in the do-
main of real-world anime super-resolution tasks. However,
they have not fully addressed the unique challenges of low-
level anime restoration. This includes the faint hand-drawn
lines and domain inconsistency in the training of GAN-
based networks. This paper conducts a comprehensive ex-
ploration of several meticulously crafted approaches to the
anime SR domain.
3. Proposed Method
3.1. Anime Production-Oriented Image SR Dataset
In this section, we present the API (Anime Production-
oriented Image) SR dataset and its curation workflow. This
curation leverages the characteristics of anime videos to se-
lect the least compressed and the most informative frames.
I-Frame-based Image Collection. AnimeSR introduces
A VC-Train, the first video-based anime SR dataset, but they
overlook the impact of compression during the collection
process, which leads VQD-SR to propose a post-processing
technique to enhance the dataset. Instead, we propose a
Figure 5. Samples of API Super-Resolution Dataset. API in-
cludes versatile CGI effects scenes ( e.g., different lightning and
special effects) and presents high image complexity.
novel method to select the least compressed frames from
the source level with minimum effort.
All videos on the internet are compressed and encapsu-
lated with a video compression standard ( e.g., H.264 [31]
and H.265 [39]) for a trade-off between the quality and the
data size. There are numerous video compression standards,
each with a complex engineering system, but they share a
similar backbone design. This characteristic motivates us
to find the pattern that the compression quality assigned
to each frame is different. Video compression designates
some keyframes, known as I-Frames, as individual units
for compression. Empirically, I-Frames are the first frame
of scene-changing scenarios. These I-Frames are allocated
with a high data size budget. On the contrary, a higher com-
pression ratio requires non-I-Frames, namely P-Frames and
B-Frames, to take I-Frames as the reference during com-
pression, which introduces temporal distortions. As shown
in Fig. 3 a, among the anime videos we collect, I-Frames
on average have a much higher data size than other non-I-
Frames, which genuinely stand for higher quality. Thus, we
useffmpeg , a video processing tool, to extract all I-Frames
from the video source as an initial pool.
Image Complexity-based Selection. To further select ide-
alistic images from the I-Frames pool, we need some cri-
teria. A straightforward method involves following A VC-
Train to use the Image Quality Assessment (IQA) to rank
and choose frames with better scores. However, IQA rank-
ing does not prefer anime images with CGI effects but fa-
vors simple scenes with little information (see Fig. 4). Thus,
we argue that image complexity assessment (ICA) is a bet-
ter option in the anime domain.
ICA evaluates the level of intricacy in an image by scor-
ing the amount and variety of details present. Compared to
IQA, ICA demonstrates greater robustness against changes
in saturation, lightning, contrast, and motion blurring. The
ICA metric we use is a recent rising analysis network,
IC9600 [13]. In the anime domain, employing ICA presents
two primary advantages. First, many scenes in anime videos
are characteristically monotonous (as exemplified in Fig. 4
left), where the majority of pixels lack significant informa-
tion in training. IQA favors these simple images and gives
higher score compared to other images, but ICA enables the
25576
BlurNoiseBlurNoisePrediction-OrientedCompressionResizeResizeAPISR Degradation Model
!1LossResNetPL(Anime)VGG PL(PhotoRealistic)GAN LossBalancedTwinPerceptual LossLRGTImage NetworkHand-Drawn LineEnhancementGTLRGENPseudo-GTPrediction-OrientedCompression×"!×""×"#×"!×""×"#
(b)(c)(a)Figure 6. The overview of our proposed methods. (a) We proposed a prediction-oriented compression module in the degradation
model to simulate versatile compression degradations for a single image input (detailed in Sec. 3.2). Proposed shuffled resize module is
randomly positioned to augment the representation of the degradation model. (b) GT images are augmented with proposed hand-drawn
line enhancement to promote the generation of images with sharpened line edge details in training (detailed in Sec. 3.3). (c) Proposed
balanced twin perceptual loss avoids unwanted color artifacts in GAN network training (detailed in Sec. 3.4).
exclusion of these scenes, which, in turn, contributes to a
reduced training sample complexity. Second, ICA is more
adept at identifying meaningful scenes within anime pro-
duction, especially those featuring CGI effects, such as the
dark scene in Fig. 4 right. These are scenarios where IQA
methods typically falter. By collecting versatile scenes, the
network training can become more robust in handling com-
plex real-world anime inputs.
API Dataset. We began by manually sourcing 562 high-
quality anime videos. From these, we extracted all I-Frames
as an initial selection pool. Utilizing the image complex-
ity assessment method mentioned above, we then selected
the top 10 highest-scoring frames from the I-Frames pool of
each video. After discarding inappropriate images ( e.g., nu-
dity, violence, abnormality, and anime images mixed with
photorealistic content), 3,740 high-quality images are ob-
tained as our proposed dataset. Example images are shown
in Fig. 5. Moreover, as shown in Fig.3 b, the density of
high image complexity scored frames of our API dataset is
remarkably superior to that of A VC-Train. More analysis
and data can be found in the supplementary materials.
720P Back-to-Original Production Resolution. While
studying the anime production pipeline, we observed that
most anime productions follow a 720P format (with an im-
age height of 720 pixels). However, in real-world scenarios,
anime is often falsely upscaled to 1080P or other formats,
for the sake of standardizing multimedia formats. We em-
pirically find that rescaling all anime images back to the
original 720P can provide feature density envisioned by the
creators with more compact anime hand-drawn lines and
CGI information.
Ground TruthMulti-FrameVideo CompressionProposed Single-Frame Video Compression
Figure 7. H.264 [31] compression of regular multi-frame video
compression and our proposed single-frame compression. They
exert similar degradations ( e.g., distortion to hand-drawn lines).
3.2. An Anime Practical Degradation model
In the real-world SR, the design of the degradation model is
of great importance. Based on the high-order degradation
model [47] and a recent image-based video compression
restoration model [42], we propose two improvements to re-
store distorted hand-drawn lines and versatile compression
artifacts and to augment the representation of the degrada-
tion model. Our degradation model is shown in Fig. 6 a.
Prediction-Oriented Compression. Utilizing the im-
age degradation model presents a challenge in the anime
restoration of video compression artifacts. This is because
previous real-world image SR methods employ JPEG, an
old but widely-used image compression standard, as the
sole compression module in the image degradation model.
JPEG performs repetitive and independent compression on
all encoding units, without considering the existence of
other units. However, video compression algorithms, for
higher compression ratios, apply prediction algorithms to
search for a reference with similar pixel content and only
compress their differences (residual), thereby reducing in-
25577
formation entropy. Prediction algorithms can search their
reference spatially (intra-prediction) or temporally (inter-
prediction). Regardless of the category, the intrinsic cause
of distortion comes from the misalignment in residual due
to prediction limitation.
Hence, we argue that artifacts equivalent to real-world
video compression artifacts can be synthesized using a sin-
gle image input in conjunction with a prediction-oriented
compression algorithm ( e.g., WebP [33] and H.264). The
need for genuinely sequential frames is not necessary. To
this end, we design a prediction-oriented compression mod-
ule within the image degradation model. This module re-
quires video compression algorithms to compress inputs on
a single-frame basis. Compared to VCISR [42], we don’t
need multiple frames for one turn of execution of com-
pression. This methodology is theoretically reasonable and
practically viable from an engineering perspective. With
a single-frame input, video compression trivially applies
intra-prediction to compress the frame without using its
inter-prediction functionality. Utilizing this approach, the
image degradation model is capable of synthesizing com-
pression artifacts akin to those observed in conventional
multi-frame video compression as shown in Fig. 7. Subse-
quently, by feeding these synthesized images into the image
SR network, the system can effectively learn the patterns of
versatile compression artifacts and engage in the restora-
tion.
Shuffled Resize Module. Degradation models in the real-
world SR domain consider blurring, resize, noise, and com-
pression modules. Blurring, noise, and compression are
real-world artifacts that can be synthesized with clear math-
ematical models or algorithms. However, the logic of the
resize module is entirely different. Resize is not a part of
natural image generation but is introduced solely for SR-
paired dataset purposes. Given this notion, we believe that
previous fixed resize module is not very suitable. We pro-
pose a more robust and effective solution, which involves
randomly placing resize operations at various orders in the
degradation model.
3.3. Anime Hand-Drawn Lines Enhancement
To enhance faint hand-drawn lines, directly employing
global methods, such as modifying the degradation model
or sharpening the entire GT, is not an ideal approach, as
the network cannot learn with attention to hand-drawn line
changes. Thus, we choose to extract sharpened hand-drawn
line information and merge it back with GT to form pseudo-
GT. By introducing this attentively enhanced pseudo-GT
to SR training, the network can generate sharpened hand-
drawn lines without the need to introduce additional neural
network modules or separate post-processing networks.
To extract hand-drawn lines, a direct approach is to apply
a sketch extraction model. However, current learning-based
Outlier Filter&Passive DilateDetectionXDoGMulti-SharpenGT
Pseudo-GTFigure 8. Anime Hand-Drawn Lines Enhancement Pipeline.
sketch extraction is often characterized by a style transfer
to the reference image, which distorts hand-drawn line de-
tails and encompasses unrelated pixel content ( e.g., shad-
ows and edges of CGI effects). Consequently, we need a
more granular, pixel-by-pixel methodology to extract hand-
drawn lines. Thus, we utilize XDoG [48], a pixel-by-
pixel Gaussian kernel-based sketch extraction algorithm, to
extract edge maps from the sharpened GT. Nevertheless,
XDoG edge maps are marred by excessive noise, contain-
ing outlier pixels and fragmented line representations. To
address this ill-posed issue, we propose an outlier filtering
technique coupled with a custom-designed passive dilation
method (detailed in the supplementary materials). In this
way, we yield a more coherent and undisturbed representa-
tion of hand-drawn lines.
We empirically find that overly sharpened pre-processed
GT makes the hand-drawn line margins more noticeable
than other unrelated shadow edge details, which makes the
outlier filter easier to distinguish their differences. Thus, we
propose three rounds of unsharp masking to the GT first. To
sum up, the formula is as follows:
ISharp=fn(IGT), (1)
IMap=h(g(ISharp)), (2)
Ipseudo-GT =ISharp·IMap+IGT·(1−IMap), (3)
where fis the sharpening function that recursively executes
ntimes, gdenotes XDoG edge detection and hstands for
post-processing techniques of passive dilation with outlier
filtering. IMapis a binary value map. The visual pipeline is
shown in Fig. 8.
3.4. Balanced Twin Perceptual Loss for Anime
The existence of unwanted color artifacts is attributed to the
inconsistent dataset domain in training between the genera-
tor and perceptual loss. Currently, most SR models trained
with GAN, including AnimeSR and VQD-SR, use the same
ImageNet [11] pre-trained VGG [34] network as the per-
ceptual loss. However, anime content, particularly those
25578
ResNet50VGG1920x
66thChannel
116thChannelFigure 9. The second middle-layer feature outputs comparison
between VGG19 used by photo-realistic perceptual loss [23] and
ResNet50 used by anime recognition task [2, 3]. With scaling,
ResNet50 presents a similar intensity as the VGG outputs.
mixed with CGI and extensive illustrations, differs signif-
icantly from photorealistic features in ImageNet. To tackle
this problem, we investigate perceptual loss and the subse-
quent improvements made in their following works.
The core idea behind perceptual loss is to utilize high-
level features ( e.g., segmentation, classification, recogni-
tion) to complement low-level pixel features by compar-
ing middle-layer feature outputs. In this regard, we employ
a pre-trained ResNet50 [2, 17] on anime object classifica-
tion task with Danbooru [3] dataset, a substantial and rich
tagging anime illustration database. Since the pre-trained
network is ResNet50 instead of VGG, we propose a simi-
lar middle-layer comparison (detailed in the supplementary
material). Overall, the formula is as follows:
Lϕ
ResNet (ˆy, y) =X
jwj
CjHjWj|ϕj(ˆy)−ϕj(y)|, (4)
where yandˆyare the pseudo-GT by Sec. 3.3 and the gen-
erated images. ϕjrepresents the perceptual function that
returns jth layer output of ResNet50. Cj,Hj, and Wjare
dimensions of the layer output and wjis the scaling fac-
tor for each layer. There are 5 middle-layer feature outputs,
which is the same quantity as VGG-based perceptual loss.
We also observe that the intensity of shallow feature layers
in ResNet50 is very weak (see Fig. 9). To resemble a simi-
lar intensity balance as the VGG, we apply a high wjto the
early layers, which leads to stable training.
Notably, introducing the ResNet-based perceptual loss as
the sole perceptual loss can solve unwanted color artifacts
and lead to quantitative improvements. However, there may
be instances of poor visual results. This is attributed to the
inherent bias in the Danbooru dataset, where most images
are character faces or relatively simple illustrations. Hence,
we seek a tradeoff by using real-world features as an aux-
iliary primer to guide the ResNet-based perceptual loss in
training. This approach results in visually appealing images
and also resolves the unwanted color issue. The overall lossfunction for our GAN training is defined as follows:
L=αL1+βLper+γLadv, (5)
Lper=LResNet +δLV GG, (6)
where L1,LV GG , andLadvare L1 pixel loss, photorealistic
VGG-based perceptual loss, and the adversarial loss. α,β,
γandδare weight parameters.
4. Experiment
4.1. Implementation Details
In our experiment, we employ our proposed API dataset
as the training dataset for the image network. The image
network we utilize is a tiny version of GRL [25] with the
nearest convolution upsample module (detailed in the sup-
plementary).
To train the GAN, we follow the same two-stage train-
ing approach as prior works [7, 46, 47, 49, 58]. In the first
stage, we train the network with L1 pixel loss for 300K it-
erations. In the second stage, we introduce our balanced
twin perceptual loss and the adversarial loss, conducting an
additional 300K iterations. The weights of {α, β, γ, δ }are
{1,0.5,0.2,1}respectively. The layer weight of perceptual
loss is {0.1,20,25,1,1}for ResNet and {0.1,1,1,1,1}
for VGG. Our discriminator is the same three-scale patch
discriminator [19, 30, 45] as in AnimeSR [49] and VQD-
SR [40]. We use the Adam optimizer [24] with a learning
rate of 2×10−4in the first stage and 1×10−4in the second
stage. A learning rate decay is applied every 100K itera-
tions in both stages. The entire training process was carried
out on one Nvidia RTX 4090, with HR patch sizes set at
256x256 and a batch size of 32.
As for the degradation model, we perform degradation
on the whole HR image first rather than directly on a
cropped patch as in previous works [25, 42, 47, 58]. Within
the degradation model, noise and blurring are configured
identically to Real-ESRGAN [47], and the first prediction-
oriented compression is implemented with JPEG [41] and
WebP [33]. The second prediction-oriented compression in-
cludes A VIF [16], JPEG [41], WebP [33], and single-frame
compression of MPEG2 [27], MPEG4 [1], H.264 [31], and
H.265 [39]. The probability of placing the resize module
is equally divided among all positions. Specific parameter
settings can be found in our supplementary materials.
4.2. Comparisons with State-of-the-art Methods
We compare our APISR quantitatively and qualitatively
with other SOTA real-world image and video SR methods,
which include Real-ESRGAN [47], BSRGAN [58], Real-
BasicVSR [7], AnimeSR [49], and VQD-SR [40].
Quantitative Comparison. Following previous real-
world SR works [7, 21, 40, 47, 49], we conduct inference on
low-quality LR datasets to generate high-quality HR images
25579
Figure 10. Qualitative comparisons on A VC-RealLQ [49] for 4×scaling. Zoom in for the best view.
Table 1. Quantitative comparisons on A VC-RealLQ [49]. Bold
text indicates the best performance. (‘ ∗’ denotes fine-tune on ani-
mation videos from [49])
Methods Params ↓ NIQE ↓ MANIQA ↑ CLIPIQA ↑
Real-ESRGAN* [47] 16.70 8.281 0.381 -
BSRGAN* [58] 16.70 8.632 0.376 -
RealBasicVSR* [7] 6.30 8.621 0.362 -
AnimeSR [49] 1.50 8.109 0.462 0.539
VQD-SR [40] 1.47 8.202 0.464 0.567
APISR (Ours) 1.03 6.719 0.514 0.711
and evaluate them using no-reference metrics. The scaling
factor is 4 for all methods. To validate the effectiveness of
our approach, our evaluation is based on A VC-RealLQ [49],
which has 46 video clips each with 100 frames. This
dataset is the only known dataset designed for real-world
anime SR testing. For no-reference metrics, we employ the
same metrics used in VQD-SR and AnimeSR, which are
NIQE [29] and MANIQA [55]. We also incorporate other
SOTA learning-based image quality assessment metrics like
CLIPIQA [43]. All metrics are based on pyiqa[8] library.
As shown in Tab. 1, our model has the smallest network
size, 1.03M parameters, but has SOTA performance in all
metrics among all image and video-based methods. Apart
from the various proposed methods that contribute to oursuccess, special acknowledgment is due to the design of the
prediction-oriented compression model, which enables us
to train image datasets and image networks to restore video
compression degradations. Meanwhile, it is worth mention-
ing that we achieved the result with only 13.3% and 25% of
the training sample complexity of AnimeSR [49] and VQD-
SR [40]. This is especially thanks to the introduction of im-
age complexity assessment in dataset curation which selects
informative images to increase the efficacy of learning the
representation of anime images. Further, we require zero
training on the degradation model due to the explicit degra-
dation model we design.
Qualitative Comparison. As shown in Fig. 10, APISR
greatly improves the visual quality than other methods.
In restoring heavily compressed images, our model ex-
hibits exceptional proficiency than all other methods, as
exemplified in the first row, where we have much fewer
ringing artifacts. Moreover, owing to the proposed hand-
drawn lines enhancement, our generated images manifest
increased line density and clarity as observed in the second
row. In addressing various twisted lines and shadow arti-
facts, our model outperforms others in effective restoration,
evidenced by the third and fourth rows. This is thanks to
25580
Table 2. Ablation study results of different training datasets. IQA
stands for image quality assessment. ICA stands for image com-
plexity assessment.
Dataset NIQE↓MANIQA ↑CLIPIQA ↑
A VC-Train [49] 7.681 0.476 0.658
Random Select 8.006 0.446 0.625
I-Frame + IQA Select 7.876 0.493 0.675
I-Frame + ICA Select 6.912 0.499 0.683
I-Frame + ICA Select + 720P Rescale 6.719 0.514 0.711
Table 3. Ablation study results of different degradation model.
Degradation Model NIQE↓MANIQA ↑CLIPIQA ↑
High-Order [47] 6.667 0.483 0.663
Random Order [58] 6.975 0.491 0.674
Prediction-Oriented Compression 7.133 0.506 0.709
Compression + Shuffled Resize 6.719 0.514 0.711
Table 4. Ablation study results of hand-drawn lines enhancement
denoted as Sharpen and twin perceptual loss denoted as APL .
NIQE↓MANIQA ↑CLIPIQA ↑
Plain 7.351 0.501 0.689
Plain + Sharpen 7.182 0.504 0.707
Plain + Sharpen + APL 6.835 0.512 0.708
Plain + Sharpen + APL + Balanced Scale 6.719 0.514 0.711
our improvement to the image degradation model where we
provide a robust restoration capability on compression and
resize functionality. Meanwhile, due to our proposed bal-
anced twin perceptual loss, images generated by our GAN
network do not show unwanted color artifacts as in Ani-
meSR and VQD-SR, which can be seen in the fifth row.
Further, thanks to the versatile scenes collected in our pro-
posed dataset, we are capable of achieving effective restora-
tion in dark scenes. More visual results can be found in the
supplementary materials.
4.3. Ablation Study
In this section, we conduct ablation studies to evaluate
the substantial impact of our proposed dataset, degradation
model, and hand-drawn lines enhancement with balanced
twin perceptual loss. The inference dataset is still A VC-
RealLQ [49]. Visual comparisons are presented in the sup-
plementary materials.
Impact of the Dataset. As shown in Tab. 2, we substi-
tute our API training dataset with several alternatives for
comparative analysis: A VC-Train [49], frames randomly
selected from the same video source as our API, a collec-
tion of I-Frames with IQA selection, and a collection of I-
Frames with ICA selection. For a fair comparison, we keep
a similar intensity of the training dataset size. If we take
the A VC-Train video training dataset as an image dataset to
train, we include temporal distorted images and less infor-
mative frames, which makes the performance hard to com-
pete with the model trained with API in all metrics. Ran-domly selected image datasets perform poorly because they
lack attention to high-quality frames in videos. With our
I-Frame collection, we take off temporally distorted frames
and choose the least compressed frames, but IQA-based se-
lection limits the performance. With the same training it-
erations and conditions, the dataset selected by ICA-based
criteria leads to an improvement over the dataset by IQA-
based selection. With the 720P rescaling method, anime
images have more compact hand-drawn lines and CGI in-
formation than falsely upscaled versions, and this back-to-
original thinking boosts the performance in all metrics.
Degradation Model. As shown in Tab. 3, to validate
the superiority of our degradation model, we replace our
proposed degradation model with the high-order degrada-
tion model from the Real-ESRGAN [47] and random or-
der degradation model from BSRGAN [58], which share
certain similarity as our methods. Our degradation model
with prediction-oriented compression model reaches an out-
standing improvement in MANIQA [55] and CLIPIQA [43]
metrics. With our shuffled resize design, our network be-
comes more robust to versatile real-world SR scenarios and
the performance can move one step further, especially the
NIQE [29] metrics.
Benefits of proposed Enhancement and Perceptual Loss.
As shown in Tab. 4, we compare our model with the plain
version that is not trained with proposed hand-drawn lines
enhancement and balanced twin perceptual loss. The in-
troduction of our hand-drawn lines enhancement presents
a significant improvement on CLIPIQA [43]. When we
append ResNet perceptual loss in GAN training, it shows
outstanding improvement in NIQE [29]. Further, with the
proposed scaling on the early layers of the ResNet percep-
tual loss part, two perceptual losses have reached a stable
balance and the performance moves one step further. This
proves that a perceptual loss that is compatible with the
anime domain is very insightful and instructive.
5. Conclusion
In this paper, we thoroughly utilize the characteristics of
anime production knowledge and fully leverage it to en-
rich and enhance anime SR. To be specific, we propose a
high-quality and informative anime production-oriented im-
age (API) SR dataset with a novel dataset curation design.
To restore and enhance hand-drawn lines, we propose an
image degradation model to restore video compression ar-
tifacts and a pseudo-GT enhancement strategy. We further
address unwanted color artifacts by introducing a network
trained with high-level anime tasks to construct a balanced
twin perceptual loss. Extensive experiment results demon-
strate our superiority over existing SOTA methods, where
we can restore harder real-world low-quality anime images.
25581
References
[1] Olivier Avaro, Alexandros Eleftheriadis, Carsten Herpel,
Ganesh Rajan, and Liam Ward. Mpeg-4 systems: overview.
Signal Processing: Image Communication , 15(4-5):281–
298, 2000. 6
[2] Matthew Baas. Danbooru2018 pretrained resnet models for
pytorch. https://rf5.github.io , 2019. Accessed:
DATE. 6
[3] Gwern Branwen and Aaron Gokaslan. Danbooru2019:
A large-scale crowdsourced and tagged anime illustration
dataset. Danbooru2017 , 2019. 6
[4] Yu Cao, Xiangqiao Meng, PY Mok, Xueting Liu, Tong-
Yee Lee, and Ping Li. Animediffusion: Anime face line
drawing colorization via diffusion models. arXiv preprint
arXiv:2303.11137 , 2023. 3
[5] Hernan Carrillo, Micha ¨el Cl ´ement, Aur ´elie Bugeau, and
Edgar Simo-Serra. Diffusart: Enhancing line art coloriza-
tion with conditional diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3485–3489, 2023. 2, 3
[6] Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and
Chen Change Loy. Basicvsr: The search for essential compo-
nents in video super-resolution and beyond. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 4947–4956, 2021. 2
[7] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and
Chen Change Loy. Investigating tradeoffs in real-world
video super-resolution. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 5962–5971, 2022. 6, 7
[8] Chaofeng Chen and Jiadi Mo. IQA-PyTorch: Pytorch
toolbox for image quality assessment. [Online]. Avail-
able: https://github.com/chaofengc/IQA-
PyTorch , 2022. 7
[9] Shuhong Chen and Matthias Zwicker. Improving the per-
ceptual quality of 2d animation interpolation. In European
Conference on Computer Vision , pages 271–287. Springer,
2022. 1, 3
[10] Shuhong Chen, Kevin Zhang, Yichun Shi, Heng Wang, Yi-
heng Zhu, Guoxian Song, Sizhe An, Janus Kristjansson,
Xiao Yang, and Matthias Zwicker. Panic-3d: Stylized single-
view 3d reconstruction from portraits of anime characters. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 21068–21077, 2023. 3
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 5
[12] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-
works. IEEE transactions on pattern analysis and machine
intelligence , 38(2):295–307, 2015. 2
[13] Tinglei Feng, Yingjie Zhai, Jufeng Yang, Jie Liang, Deng-
Ping Fan, Jing Zhang, Ling Shao, and Dacheng Tao. Ic9600:
A benchmark dataset for automatic image complexity assess-
ment. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 2022. 3[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems , 27, 2014. 2
[15] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your
personalized text-to-image diffusion models without specific
tuning. arXiv preprint arXiv:2307.04725 , 2023. 3
[16] Jingning Han, Bohan Li, Debargha Mukherjee, Ching-Han
Chiang, Adrian Grange, Cheng Chen, Hui Su, Sarah Parker,
Sai Deng, Urvang Joshi, et al. A technical overview of av1.
Proceedings of the IEEE , 109(9):1435–1462, 2021. 6
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In Computer
Vision–ECCV 2016: 14th European Conference, Amster-
dam, The Netherlands, October 11–14, 2016, Proceedings,
Part IV 14 , pages 630–645. Springer, 2016. 6
[18] Zhengyu Huang, Haoran Xie, Tsukasa Fukusato, and
Kazunori Miyata. Anifacedrawing: Anime portrait explo-
ration during your sketching. In ACM SIGGRAPH 2023
Conference Proceedings , pages 1–11, 2023. 3
[19] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125–1134,
2017. 6
[20] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li,
and Feiyue Huang. Real-world super-resolution via kernel
estimation and noise injection. In The IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR)
Workshops , 2020. 2
[21] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin
Li, and Feiyue Huang. Real-world super-resolution via
kernel estimation and noise injection. In proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition workshops , pages 466–467, 2020. 6
[22] Yuxin Jiang, Liming Jiang, Shuai Yang, and Chen Change
Loy. Scenimefy: Learning to craft anime scene via semi-
supervised image-to-image translation. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 7357–7367, 2023. 2, 3
[23] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Percep-
tual losses for real-time style transfer and super-resolution.
InComputer Vision–ECCV 2016: 14th European Confer-
ence, Amsterdam, The Netherlands, October 11-14, 2016,
Proceedings, Part II 14 , pages 694–711. Springer, 2016. 6
[24] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[25] Yawei Li, Yuchen Fan, Xiaoyu Xiang, Denis Demandolx,
Rakesh Ranjan, Radu Timofte, and Luc Van Gool. Effi-
cient and explicit modelling of image hierarchies for image
restoration. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 18278–
18289, 2023. 2, 6
[26] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc
Van Gool, and Radu Timofte. Swinir: Image restoration us-
25582
ing swin transformer. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 1833–1844,
2021. 2
[27] Joan L Mitchell, William B Pennebaker, Chad E Fogg,
Didier J LeGall, Joan L Mitchell, William B Pennebaker,
Chad E Fogg, and Didier J LeGall. Mpeg-2 overview. MPEG
Video Compression Standard , pages 171–186, 1996. 6
[28] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad
Bovik. No-reference image quality assessment in the spatial
domain. IEEE Transactions on image processing , 21(12):
4695–4708, 2012. 3
[29] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Mak-
ing a “completely blind” image quality analyzer. IEEE Sig-
nal processing letters , 20(3):209–212, 2012. 7, 8
[30] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative ad-
versarial networks. arXiv preprint arXiv:1802.05957 , 2018.
6
[31] Heiko Schwarz, Detlev Marpe, and Thomas Wiegand.
Overview of the scalable video coding extension of the h.
264/avc standard. IEEE Transactions on circuits and sys-
tems for video technology , 17(9):1103–1120, 2007. 3, 4, 6
[32] Wang Shen, Cheng Ming, Wenbo Bao, Guangtao Zhai, Li
Chenn, and Zhiyong Gao. Enhanced deep animation video
interpolation. In 2022 IEEE International Conference on Im-
age Processing (ICIP) , pages 31–35. IEEE, 2022. 1
[33] Zhanjun Si and Ke Shen. Research on the webp image for-
mat. In Advanced graphic communications, packaging tech-
nology and materials , pages 271–277. Springer, 2016. 5, 6
[34] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 5
[35] Li Siyao, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris
Metaxas, Chen Change Loy, and Ziwei Liu. Deep ani-
mation video interpolation in the wild. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 6587–6595, 2021. 3
[36] Li Siyao, Yuhang Li, Bo Li, Chao Dong, Ziwei Liu, and
Chen Change Loy. Animerun: 2d animation visual corre-
spondence from open source 3d movies. Advances in Neural
Information Processing Systems , 35:18996–19007, 2022. 3
[37] Li Siyao, Tianpei Gu, Weiye Xiao, Henghui Ding, Ziwei Liu,
and Chen Change Loy. Deep geometrized cartoon line inbe-
tweening. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 7291–7300, 2023. 3
[38] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge,
Jinqiu Sun, and Yanning Zhang. Blindly assess image qual-
ity in the wild guided by a self-adaptive hyper network. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 3667–3676, 2020. 3
[39] Gary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and
Thomas Wiegand. Overview of the high efficiency video
coding (hevc) standard. IEEE Transactions on circuits and
systems for video technology , 22(12):1649–1668, 2012. 3, 6
[40] Zixi Tuo, Huan Yang, Jianlong Fu, Yujie Dun, and Xuem-
ing Qian. Learning data-driven vector-quantized degradation
model for animation video super-resolution. arXiv preprint
arXiv:2303.09826 , 2023. 1, 2, 3, 6, 7[41] Gregory K Wallace. The jpeg still picture compression stan-
dard. IEEE transactions on consumer electronics , 38(1):
xviii–xxxiv, 1992. 6
[42] Boyang Wang, Bowen Liu, Shiyu Liu, and Fengyu Yang.
Vcisr: Blind single image super-resolution with video com-
pression synthetic data. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision ,
pages 4302–4312, 2024. 1, 2, 4, 5, 6
[43] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Ex-
ploring clip for assessing the look and feel of images. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence ,
pages 2555–2563, 2023. 7, 8
[44] Ning Wang, Muyao Niu, Zhi Dou, Zhihui Wang, Zhiyong
Wang, Zhaoyan Ming, Bin Liu, and Haojie Li. Coloring
anime line art videos with transformation region enhance-
ment network. Pattern Recognition , 141:109562, 2023. 3
[45] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional gans. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 8798–8807, 2018. 6
[46] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,
Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-
hanced super-resolution generative adversarial networks. In
Proceedings of the European conference on computer vision
(ECCV) workshops , pages 0–0, 2018. 6
[47] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.
Real-esrgan: Training real-world blind super-resolution with
pure synthetic data. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 1905–1914,
2021. 1, 2, 4, 6, 7, 8
[48] Holger Winnem ¨oller, Jan Eric Kyprianidis, and Sven C
Olsen. Xdog: An extended difference-of-gaussians com-
pendium including advanced image stylization. Computers
& Graphics , 36(6):740–753, 2012. 5
[49] Yanze Wu, Xintao Wang, Gen Li, and Ying Shan. Animesr:
Learning real-world super-resolution models for animation
videos. arXiv preprint arXiv:2206.07038 , 2022. 1, 2, 3, 6, 7,
8
[50] Zeyu Xiao, Zhiwei Xiong, Xueyang Fu, Dong Liu, and
Zheng-Jun Zha. Space-time video super-resolution using
temporal profiles. In Proceedings of the 28th ACM Inter-
national Conference on Multimedia , pages 664–672, 2020.
2
[51] Zeyu Xiao, Xueyang Fu, Jie Huang, Zhen Cheng, and Zhiwei
Xiong. Space-time distillation for video super-resolution. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 2113–2122, 2021. 2
[52] Zeyu Xiao, Jiawang Bai, Zhihe Lu, and Zhiwei Xiong. A
dive into sam prior in image restoration. arXiv preprint
arXiv:2305.13620 , 2023. 2
[53] Zeyu Xiao, Yutong Liu, Ruisheng Gao, and Zhiwei Xiong.
Cutmib: Boosting light field super-resolution via multi-view
image blending. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
1672–1682, 2023. 2
25583
[54] Shizhuo Xu, Vibekananda Dutta, Xin He, and Takafumi Mat-
sumaru. A transformer-based model for super-resolution of
anime image. Sensors , 22(21):8126, 2022. 1
[55] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan
Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang.
Maniqa: Multi-dimension attention network for no-reference
image quality assessment. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1191–1200, 2022. 7, 8
[56] Chih-Yuan Yao, Shih-Hsuan Hung, Guo-Wei Li, I-Yu Chen,
Reza Adhitya, and Yu-Chi Lai. Manga vectorization and ma-
nipulation with procedural simple screentone. IEEE transac-
tions on visualization and computer graphics , 23(2):1070–
1084, 2016. 3
[57] Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang,
Chao Dong, and Liang Lin. Unsupervised image super-
resolution using cycle-in-cycle generative adversarial net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition workshops , pages 701–710,
2018. 2
[58] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo-
fte. Designing a practical degradation model for deep blind
image super-resolution. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 4791–
4800, 2021. 2, 6, 7, 8
[59] Lvmin Zhang, Chengze Li, Edgar Simo-Serra, Yi Ji, Tien-
Tsin Wong, and Chunping Liu. User-guided line art flat
filling with split filling mechanism. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 9889–9898, 2021. 3
[60] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 3
[61] Song-Hai Zhang, Tao Chen, Yi-Fei Zhang, Shi-Min Hu, and
Ralph R Martin. Vectorizing cartoon animations. IEEE
Transactions on Visualization and Computer Graphics , 15
(4):618–629, 2009. 3
[62] Yang Zhao, Diya Ren, Yuan Chen, Wei Jia, Ronggang Wang,
and Xiaoping Liu. Cartoon image processing: A survey.
IJCV , 2022. 1
25584
