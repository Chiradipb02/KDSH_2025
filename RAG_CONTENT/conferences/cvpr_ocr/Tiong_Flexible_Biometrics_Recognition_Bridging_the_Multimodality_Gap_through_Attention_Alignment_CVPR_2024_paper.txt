Flexible Biometrics Recognition: Bridging the Multimodality Gap through
Attention, Alignment and Prompt Tuning
Leslie Ching Ow Tiong*,1Dick Sigmund*,2Chen-Hui Chan3Andrew Beng Jin Teoh†,4
1Samsung Electronics2AIDOT Inc.3Korea Institute of Science and Technology4Yonsei University
1leslie.tiong@samsung.com2dsigmund@aidot.ai3chchan@kist.re.kr4bjteoh@yonsei.ac.kr
Abstract
Periocular and face are complementary biometrics for
identity management, albeit with inherent limitations, no-
tably in scenarios involving occlusion due to sunglasses or
masks. In response to these challenges, we introduce Flex-
ible Biometric Recognition (FBR), a novel framework de-
signed to advance conventional face, periocular, and mul-
timodal face-periocular biometrics across both intra- and
cross-modality recognition tasks. FBR strategically utilizes
the Multimodal Fusion Attention (MFA) and Multimodal
Prompt Tuning (MPT) mechanisms within the Vision Trans-
former architecture. MFA facilitates the fusion of modali-
ties, ensuring cohesive alignment between facial and peri-
ocular embeddings while incorporating soft-biometrics to
enhance the model’s ability to discriminate between indi-
viduals. The fusion of three modalities is pivotal in explor-
ing interrelationships between different modalities. Addi-
tionally, MPT serves as a unifying bridge, intertwining in-
puts and promoting cross-modality interactions while pre-
serving their distinctive characteristics. The collaborative
synergy of MFA and MPT enhances the shared features of
the face and periocular, with a speciﬁc emphasis on the oc-
ular region, yielding exceptional performance in both intra-
and cross-modality recognition tasks. Rigorous experimen-
tation across four benchmark datasets validates the note-
worthy performance of the FBR model. The source code is
available at https://github.com/MIS-DevWorks/FBR .
1. Introduction
Facial recognition has attained ubiquitous applications
across diverse domains today [ 18,19,35]. However, they
struggle with challenges arising from cosmetic changes,
plastic surgery, and particularly obstructions like face
masks. On the other hand, periocular recognition, which
focuses on the region around the eyes, has gained traction
*The authors have contributed equally to this work.
†Corresponding author.as an alternative to face recognition [ 1,23–25]. Despite the
signiﬁcant progress, however, challenges remain, especially
with glasses or sunglasses, impacting the accuracy of peri-
ocular recognition.
The fusion of facial and periocular [ 15,32,37], holds
promise for enhancing recognition performance. However,
traditional multimodal biometrics present fresh challenges
in managing and storing templates of all biometric modal-
ities, which can result in computational and storage over-
head. Moreover, ensuring all modalities are available for
recognition is critical for seamless deployment. In response
to these challenges, conditional biometrics [ 11,22], along
with cross-modality biometrics recognition [ 20,33], offer
promising avenues to mitigate the constraints of unimodal
biometric systems, i.e., sole face or periocular recognition
as well as multimodal biometric systems. Conditional bio-
metrics enhance a single modality by incorporating infor-
mation from another, such as periocular recognition condi-
tioned by the face or vice versa. On the other hand, cross-
modality biometrics encompasses the task of matching bio-
metric samples across distinct biometric modalities, such as
face vs. periocular matching.
This paper introduces Flexible Biometrics Recognition
(FBR), designed to support intra- and cross-modality bio-
metric matching, as illustrated in Figure 1. The FBR
model is initially trained to align facial, periocular, and soft-
biometric attributes. The latter encompasses social or phys-
ical descriptive traits of individuals such as gender, age,
or ethnicity, which have proven to enhance the discrimi-
native power of the embedding [ 6]. During deployment,
the trained FBR model serves as a feature extractor, ac-
quiring facial or periocular embeddings based on the input
modality. In contrast to unimodal and multimodal biomet-
ric systems, FBR produces modality-invariant embeddings,
facilitating both intra- and cross-modality matching. Ad-
ditionally, FBR can address scenarios where only facial or
periocular data is stored, enabling exclusive reliance on the
available modality during recognition.
The adaptability of FBR ensures robust performance and
operational reliability, even in incomplete or temporarily
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
267
Intra-modality RecognitionCross-modality RecognitionImageEmbedding  FBR ModelEmbedding  pEmbedding  fRecognition
Embedding  pEmbedding  fImageEmbedding 
FBR ModelEnrollment
89.98%85.68%61.79%Intra-modality Face RecognitionIntra-modality Periocular RecognitionCross-modality Face-Periocular Recognition94.82%92.95%80.61%87.89%83.73%0.36%Figure 1. An overview of the Flexible Biometrics Recognition
(FBR). This approach encompasses ﬂexible recognition tasks such
as intra- and cross-modality biometric recognition, demonstrating
its versatility in biometric systems. Our model exhibits enhanced
performance relative to benchmarks, including unimodal biomet-
rics baseline and a competing model [ 10].
unavailable biometric data. As depicted in Figure 1, the
proposed FBR model outperforms the unimodal biomet-
rics baseline and a competing model [ 10], which is also
trained with three modalities and equips with a prompt tun-
ing mechanism. While FBR offers exceptional adaptability,
attaining decent performance in intra- and cross-modality
matching is a non-trivial challenge. Enhancing one facet
could potentially undermine the other and vice versa. Strik-
ing the right balance between optimizing intra- and cross-
modality recognition is crucial.
To substantiate FBR, we introduce a Multimodal Fu-
sion Attention Vision Transformer (MFA-ViT) and a
multimodal-prompt tuning (MPT) mechanism. MFA-ViT
is crafted to establish a cohesive alignment between facial
and periocular features within a ViT. Furthermore, integrat-
ing soft-biometric attributes enhances the model’s ability
to discern differences between identities effectively. The
multimodal fusion attention (MFA) module achieves the fu-
sion of three modalities, which is pivotal in exploring inter-
relationships between different modalities. This endeavor
proves advantageous in mitigating the trade-off between
intra- and cross-modality recognition tasks.
In the realm of robust embedding learning across diverse
modalities, prompt tuning has proven effective for modality
alignment [ 10,39]. Nevertheless, standard prompt tuning
(SPT) often falls short of effectively utilizing multimodalinformation. For FBR problems, SPT lacks proper guidance
for intra- and cross-modality integration, especially when
dealing with multiple modalities. The proposed MPT ad-
dresses this challenge by providing modality-speciﬁc guid-
ance for aligning multimodal information within MFA-ViT.
Unlike SPT, MPT establishes a uniﬁed bridge for three
modalities, intertwining input sequences while promoting
cross-modality interaction while preserving their distinct
characteristics.
To sum up, we leverage the MFA and MPT within the
ViT architecture to address FBR problems. These mech-
anisms are devised for effective multiple modalities align-
ment, especially to accentuate the common distinctive fea-
tures of the face and periocular, with a speciﬁc focus on the
ocular region, as they play a pivotal role in achieving excep-
tional performance in intra- and cross-modality recognition
tasks. Additionally, our approach derives advantages from
incorporating soft-biometric attributes as supplementary in-
formation.
The contributions of this paper are summarized as fol-
lows:
•A novel FBR framework is introduced to address intra-
and cross-modality recognition by integrating face and
periocular modalities with soft-biometric attributes.
•A MFA-ViT is designed to substantiate the FBR no-
tion, enabling the effective fusion of three modalities and
systematically examining the inter-dependencies between
intra- and cross-modality relationships while establishing
a modality-invariant embedding to represent identities.
•The MPT mechanism is proposed, crucially guiding the
integration of multimodal data to produce rich, coher-
ent embeddings, capturing intricate relationships between
different biometric modalities.
2. Related Work
Conditional biometrics has garnered attention as a promis-
ing solution to address the limitations of multimodal bio-
metrics [ 8,9,11]. These studies illustrate the performance
improvements achieved by conditioning the face modality
with soft-biometrics, particularly in challenging environ-
mental variations and occlusions. A previous study has uti-
lized knowledge distillation techniques [ 12] to enhance pe-
riocular modality performance by incorporating face bio-
metrics. In a similar vein, [ 22] introduces an approach
for face-conditioned periocular recognition. This approach
employs a conditional biometrics contrastive loss within a
shared-parameter convolutional network.
Nevertheless, FBR offers a more robust solution due to
its inherent ﬂexibility to handle intra- and cross-modality
recognition tasks without speciﬁc conditioning, making it a
better choice for demanding real-world applications.
Cross-modality biometrics. Prior studies in cross-
modality biometrics have primarily focused on face-voice
268
recognition [ 3,13,20,36], while others like [ 5,34] fo-
cused on bridging visible light face images with alterna-
tive modalities such as infrared or depth images. Most re-
cently, [ 33] introduces HA-ViT, utilizing a face-periocular
contrastive learning approach for cross-modality recogni-
tion. This approach effectively demonstrates how con-
trastive learning can proﬁciently align and differentiate
these modalities, enhancing cross-modality recognition task
performance. However, previous works tend to neglect the
wider utility, especially in tasks related to intra-modality
recognition. Furthermore, while the importance of face and
periocular biometrics in cross-modality recognition is ac-
knowledged, effectively capturing their intricate relation-
ships remains challenging. [ 5,22,33].
Our approach goes beyond the boundaries of cross-
modality biometric recognition by harnessing the MFA and
MPT modules within the ViT architecture. Supplementary
Material Section 8.1highlights that these modules empha-
size the shared salient features of the face and periocular,
particularly the eyes, which are crucial for excelling in intra-
and cross-modality recognition tasks.
Prompt Tuning involves generating task-speciﬁc con-
tinuous vectors using gradient descent [ 16]. These vec-
tors are designed to guide a pre-trained transformer model
to perform speciﬁc tasks without requiring extensive ﬁne-
tuning of the entire model. This technique has recently
been extended to image classiﬁcation tasks, as explored by
[10,39]. In this context, the learnable visual prompt tuning
(VPT) is used to adapt pre-trained ViT models, resulting
in improved performance compared to conventional ﬁne-
tuning.
However, applying VPT directly to FBR poses chal-
lenges. These challenges arise from the inherent limitations
of guiding alignment within VPT methodologies, mainly
when dealing with the complexities of multiple modalities.
To this end, our proposed MPT addresses the alignment of
multiple modality embeddings within a shared embedding
space that captures multimodal information. Importantly,
this uniﬁed alignment is a novel contribution not explored
in existing literature.
3. Methodology
3.1. Network Architecture
MFA-ViT is built upon a shared-parameter network archi-
tecture that accommodates input from three sources: face
(If), periocular ( Ip), and soft-biometric attributes ( Ia), as
illustrated in Figure 2. The MFA-ViT is based on the ViT
architecture [ 4] due to its effectiveness in handling multi-
modality fusion without explicit modiﬁcations [ 38]. Fur-
ther justiﬁcations are provided in Supplementary Material
Section 8.3.
Given a pair of image patches IfandIp, each is tok-enized to yield embeddings ZfandZpwith dimension d,
and it is set to 1,024. To incorporate Iainto the network,
we utilize feature tokenizer [ 7], in order to transform the
input Iainto embeddings Za2R1⇥d. Feature tokenizer
enables the seamless integration of categorical-based soft-
biometric attributes with image-based face and periocular
modalities.
The network takes in biometric token embedding Z⇤
where ⇤denotes f,p, or a, a learnable class token em-
bedding T⇤as well as a learnable prompt token embed-
ding P⇤. Subsequently, these embeddings are directed to
the multimodal fusion attention (MFA) block, Bmwhere
m=1,···,M. Each MFA block comprises MFA layers
Fnwith n=0,1,···,N 1. Each Fnis constructed by
a3⇥3depth-wise Conv (DWS-Conv) layer, a depth-wise
fusion Conv-based multi-head self-attention (DWFC-MSA)
layer, a 1⇥1Conv layer, and a LeakyReLu ( RLeaky) activa-
tion layer. In this paper, M=2andN=4are adopted.
The embeddings T⇤,Z⇤, and P⇤are concatenated and
subsequently fed into the Fnlayer within each Bm, which
is outlined as follows:
Fn+1(K⇤,n)=RLeaky(Conv ([DWS-Conv (K⇤,n),
DWFC-MSA (K⇤,n)])) + K⇤,n,(1)
where K⇤,n=[T⇤,n,Z⇤,n,P⇤,n]2RS⇥H⇥WandS,H,
Wdenote the number of input embeddings, height, width,
respectively. The DWS-Conv layer specializes in extracting
distinct local features from the IfandIppatches while si-
multaneously considering the associations encoded by the
Ia. These associations substantiate the learning of multi-
modal embeddings, particularly facilitated by the P⇤.
The DWFC-MSA layer is tailored to capture the rela-
tionships within and across modalities within the IfandIp
patches. The presence of Iaenriches this layer, allowing
it to achieve a holistic comprehension. The DWFC-MSA
layer collaborates seamlessly with P⇤to support the devel-
opment of this understanding. The DWFC-MSA ( E) layer
is structured as follows:
E0
n+1(K⇤,n)=C-MSA (Norm (K⇤,n)) + K⇤n,(2)
En+1(K⇤,n)=
MLP (Norm (E0
n+1(K⇤,n))) + E0
n+1(K⇤,n),(3)
where Norm (·)denotes layer normalization, C-MSA (·)
refers to 3⇥3depth-wise Conv-based MSA layer, and
MLP (·)represents multi-layer perception. Noted that the
input tokens for DWS-Conv and DWFC-MSA layers are re-
shaped for spatial dimensions.
As the ﬁnal step, the network encodes joint embeddings
J⇤by aggregating the MPT embeddings P0
⇤,Nfrom each
269
!!∈ℝ""#×""#
!!∈ℝ"×$ImageEmbedding
…
…
!%∈ℝ""#×""#
!%∈ℝ"×$ImageEmbedding
…
…!&∈ℝ"×'(
!&∈ℝ"×$AttributeEmbedding
…Multimodal Fusion Attention Layer $#
1 ×1 Conv & LeakyReLuDWFC-MSADWS-ConvAdd & DropoutMultimodal Fusion Attention Layer !!…!∗,#
"∗,$…CLS…!∗,$#∗,$Multimodal Fusion Attention Block %"Multimodal Fusion Attention Block %)"∗,%,#…CLS…!∗,%,#&#∗,%,#
…Multimodal Fusion Attention Layer $*
1 ×1 ConvReLuMultimodal-Prompt TuningJoint Embeddings  &∗∈ℝ"×,AddLarge-marginFace LossLarge-marginPeriocular LossCross Face-periocular and Soft Attributes Contrastive Loss
…!∗,#&"∗,#…CLS#∗,#
"∗,$…CLS…!∗,$#∗,$"∗,%,'…CLS…!∗,%,'&#∗,%,'Softmax'∗
Figure 2. Network architecture of Multimodal Fusion Attention (MFA) Vision Transformer with Multimodal-Prompt Tuning (MPT).
Bmvia addition and followed by an average pooling (Avg-
pool) operation. For classiﬁcation, we utilize J⇤as the input
to the softmax layer, resulting in the softmax vector Y⇤.W e
opt to employ J⇤instead of a class token T⇤as commonly
utilized in ViT, is driven by the hypothesis that the MPT, ex-
plicitly designed for the multimodal fusion task, can offer a
more effective way to capture and leverage cross-modality
information. We explore the impacts of this option in Sec-
tion4.4.4 . The ﬁnal formulation is deﬁned as follows:
J⇤=Avgpool (MX
m=1P0
⇤,N,M), (4)
Y⇤=Softmax (J⇤). (5)
3.2. Multimodal-Prompt Tuning
MPT is incorporated at the input space after the embedding
layers, which are attached to T⇤andZ⇤, as illustrated in
Figure 2. The MPT embeddings ( P0
⇤) play a pivotal role in
guiding the process of multimodal feature prompt learning
in each Fnlayer. Utilizing this guidance enables a subtle
and detailed exploration of relationships among the diverse
modalities and attributes under consideration.
Speciﬁcally, MPT is integrated at the input space of each
layer in Bm. The structure of MPT involves a 1⇥1Conv
layer, followed by a ReLU layer, applied to P⇤,nembed-
dings. We designate the set of learnable P0
⇤,nembeddings
with dimension size of d. Section 4.4.2 emphasizes MPT
embeddings’ role in discerning intricate details within mul-
timodal features, with Figure 5demonstrating their efﬁcacyin capturing eye regions across facial and periocular images.
Additional studies can be found in Supplementary Material
Section 8.2. The input space with MPT embeddings can be
computed as:
[T⇤,n+1,Z⇤,n+1,P0
⇤,n+1]=
Fn+1([T⇤,n+1,Z⇤,n+1,Ln+1(P⇤,n,P⇤,n+1)]),(6)
where P⇤,nandP⇤,n+1represent the previous and current
input prompt embeddings. Ln+1is calculated using a ReLU
activation applied to the output of 1⇥1Conv as deﬁned by:
Ln+1(P⇤,n,P⇤,n+1)=ReLu (Conv ([P⇤,n,P⇤,n+1])).(7)
3.3. Multimodal Contrastive Loss Function
For training, we employ a dual strategy that combines
both large-margin softmax loss ( LLM)[14] and contrastive
loss ( LCL)[33]. This strategy aims to learn intra-
modality relationships within face ( f) and periocular ( p),
and cross-modality relationships encompassing f,p, and
soft-biometric attributes ( a).
Speciﬁcally, the LLMcontributes to shaping the embed-
ding space to better discriminate between different identi-
ties, which is essential for intra-modality recognition. The
LLMis given as follows:
LLM ‡=
 log( ‡)+ 
2X
c6=l
 ‡,c 1
C 1·log( ‡,c) 
,(8)
270
where  denotes softmax function,  ‡,cis to maximize the
margin of discriminated embedding between the identity of
thesame modalities ,Cis the number of identities,  is to
control the degree of degrading labels, and lis the label.  is
set to 0.3 same as [ 14]. The LLM,‡is computed individually
tofandpmodalities, with ‡representing either forp.
On the other hand, the LCLaccentuates the embedding of
the same identity samples, fostering closer proximity in the
embedding space while simultaneously pushing apart sam-
ples from different identities across three modalities, i.e., f,
p, and a. The loss function is implemented following the
formulation introduced by [ 33] and [ 40] to establish signiﬁ-
cant connections between cross-modality relationships. The
LCLcan be expressed as:
LCL=LCM(f,p)+LCM(f,a)+LCM(p, a),(9)
LCM(xu,yu)=
 log (xu,yu)
 (xu,yu)+↵( (xu,xv)) +  (xu,yv),(10)
where  (xu,yu)=e x p (JT
xuJyu
✓)serves to map the embed-
dings J, ofdistinct modalities xandy, yet sharing the same
identity u, into a shared embedding space. The hyperpa-
rameter ✓is introduced to extend the range of JT
xuJyuto
facilitate the model’s convergence.
Furthermore,  (xu,xv)=P
xv2NX
u (xu,xv)repre-
sents pairs of data samples sharing the same modality
but differing in identity v. Here, xv2NX
udesignates
intra-modality pairs , which are pairs of different sam-
ples within the same modality . Similarly,  (xu,yv)=P
yu2NY
v (xu,yv)characterizes pairs of data samples from
different modalities andidentities , thereby ensuring they re-
main distinguishable in the shared embedding space. Here,
yv2NY
urefers to cross-modality pairs between different
samples . Leveraging  (xu,xv)and (xu,yv)empowers the
model to effectively discern between the high similarities in
cross-modalities from different identities.
In our study, ↵is set to 0.8 same as [ 40]. Additionally,
we set ✓to 0.03 for the LCM(f,p)term, and to 0.04 for both
LCM(f,a)andLCM(p, a)terms.
The total loss ( Ltotal) is given as follows:
Ltotal=LLM f+LLM p+LCL. (11)
3.4. Flexible Recognition
To determine a person’s identity, the trained model is de-
signed ﬂexibly to accept inputs from the forpmodalities.
Speciﬁcally, the softmax layer is detached for recognition,
and the modality-invariant embedding is extracted from J‡,where ‡denotes forp. The identity of J‡can be decided
based on
 =max
k[s(Gk,‡,J‡)] (12)
where s(·)calculates a similarity score between the un-
known identity J‡and the gallery sets Gk,‡,krefers to the
number of identities in gallery set.
4. Experiment
4.1. Dataset
4.1.1 Training dataset
The training dataset comprises modalities If,Ip, and Ia,
which are sampled from the VGGFace2 dataset [ 2] and the
MAAD-Face dataset [ 31]. After a comprehensive dataset
review, we have selected 1.49 million samples with 9,131
identities. For each identity, we have paired face Ifand
periocular Ipimages with 47 attributes Iarepresenting the
identities, detailed in [ 31]. We randomly partitioned the
identities, allocating them into training and validation sets
in an 80:20 ratio.
4.1.2 Evaluation dataset
To have a fair comparison, we have selected four public
datasets, namely Ethnic [ 32], FaceScrub [ 21], IMDB [ 26],
and Cross-Modal DB [ 33]. Further details can be found
in Supplementary Material Section 7. These datasets are
benchmarks for evaluating our network’s intra- and cross-
modality matching performance. We adhere to the protocol
in [32], which involves matching a probe from the gallery
sets. In this evaluation, all trained models serve as feature
extractors for IfandIpacross both gallery and probe sets.
The matching process is executed using cosine similarity.
4.2. Experimental Setup
MFA-ViT is trained over 50 epochs with a batch size of 64.
The input sizes for IfandIpare both set to 3⇥112⇥112,
while Iais1⇥47. Each IfandIpimage is divided into 14
patches, and the size of each patch is 8⇥8. We minimize
the total loss using the AdamW Optimizer [ 17]. We employ
a learning rate of 1e-4, a weight decay parameter 1e-5, and
a dropout rate of 0.1.
4.3. Experimental Results
To assess the FBR performance, we conduct a compre-
hensive comparison by re-implementing a baseline model
trained solely on Ifand another solely on Ip. Further-
more, we examined several relevant models designed for
intra- and cross-modality recognition problems, i.e., [ 32],
[22], and [ 5]. We also aim to provide a broader com-
parison encompassing recent studies such as HA-ViT [ 33]
271
and ViT/VPT [ 10] to examine the effectiveness of our pro-
posed model. Noted that we have re-implemented these
models to adapt input embeddings, customizing them for
fair comparisons, while adhering to the same experimental
settings and protocols. In Table 1, we present the perfor-
mance comparisons for intra- and cross-modality recogni-
tion tasks, with the primary metric being rank-1 recogni-
tion accuracy. Speciﬁcally, the cross-modality recognition
tasks encompass scenarios such as face gallery vs. periocu-
lar probe ( f–p) and periocular gallery vs. face probe ( p–f),
while intra-modality recognition focuses on face gallery vs.
face probe ( f–f) and periocular gallery vs. periocular probe
(p–p).
4.3.1 Intra-modality Recognition
As indicated in Table 1, the MFA-ViT/MPT model exhibits
exceptional recognition performance on both the Ethnic
and FaceScrub datasets, achieving high accuracy rates of
94.82% and 95.71% for the f–fand 89.98% and 93.06%
for the p–p, respectively. Furthermore, even when con-
fronted with more challenging datasets such as IMDB and
Cross-Modal DB, MFA-ViT/MPT maintains its competi-
tive advantage, yielding mean accuracy rates of 86.03% and
85.88% for f–fand 80.53% and 76.54% for p–p. These out-
comes underscore the model’s consistent outperformance of
baseline and competing models.
The performance of both the baseline and [ 32] is subop-
timal, primarily attributed to their extensive dependence on
the individual IfandIpmodalities. In contrast, the mod-
els presented by [ 5] and [ 22] have exhibited satisfactory
results by adeptly capturing adaptive relational knowledge
between IfandIpembeddings. Nevertheless, the necessity
for both face and periocular inputs in these models restricts
their adaptability.
Notably, the experiments reveal that models with prompt
tuning, such as ViT/VPT, generally outperform those with-
out, highlighting the efﬁcacy of the prompt tuning. How-
ever, ViT/VPT still exhibits a marginal performance gap
compared to our model. This observation justiﬁes that the
MFA-ViT and MPT are pivotal in exploring complex rela-
tionships among the modalities.
4.3.2 Cross-modality Recognition
In Table 1, the MFA-ViT/MPT model exhibits notable av-
erage recognition accuracies for both the f–pandp–fsce-
narios. Speciﬁcally, in the f–p, the model achieved accu-
racies of 86.70% on the Ethnic dataset, 90.38% on Face-
Scrub, 75.28% on IMDB, and 72.01% on Cross-Modal
DB. Conversely, in the p–fconﬁguration, it attains impres-
sive average accuracies of 89.06% on Ethnic, 92.02% on
FaceScrub, 76.36% on IMDB, and 75.96% on Cross-Modal
DB. The outcomes underscore the consistent superiority ofour model over existing methods, highlighting its effective-
ness in addressing cross-modality recognition across four
datasets.
The baseline among benchmark methods signiﬁcantly
underperformed, scoring below 1% in evaluations, indicat-
ing the distinct nature of face and periocular modalities
despite periocular being a face subset. Hence, modality
alignment is crucial for cross-modal matching. In con-
trast, [ 5] and [ 22] employed contrastive learning for en-
hanced performance, introducing a trade-off between intra-
and cross-modality recognition. As intra-modality perfor-
mance improved, cross-modality recognition degraded, and
vice versa, compared to our model. Their focus on broad
embedding space alignment overshadowed nuances in If
andIpattributes, potentially causing suboptimal recogni-
tion across modalities.
As shown in Table 1, ViT/VPT and HA-ViT, even trained
with Ia, did not perform as effectively as our approach. This
can be attributed to HA-ViT primarily focusing on utiliz-
ing cross-modality loss functions, neglecting the beneﬁts
of prompt tuning for enhancing multimodal features. On
the other hand, ViT/VPT, while utilizing prompt tuning,
encountered challenges in effectively handling multimodal
features.
In summary, results demonstrate MFA adeptly aligns
cross-modality relations, enabling precise matching with-
out contrastive learning trade-offs. Simultaneously, MPT
efﬁciently guides the model, capturing cross-modality and
multimodal dependencies. Collaborative synergy between
MPT and Iaenhances the understanding, contributing to
superior cross-modal recognition. This underscores the ef-
ﬁcacy of our approach to FBR challenges.
4.4. Ablation Study
4.4.1 Effects of Soft-biometric Attributes
We undertook an ablation study to evaluate the effectiveness
ofIausing MFA-ViT and HA-ViT. Notably, both networks
were trained to employ the MPT approach. For a fair com-
parison, soft-biometric attributes are integrated into the in-
put embeddings of HA-ViT with a feature tokenizer during
training. Table 1and Figure 3demonstrate that MFA-ViT
outperforms other models when equipped with Ia. Surpris-
ingly, HA-ViT, whether with or without Ia, lagged behind
MFA-ViT, even though both models employed the identical
Ltotalloss function.
One plausible rationale behind the enhanced utilization
ofIaby MFA-ViT lies in its architectural reﬁnement, de-
signed for seamless integration of features derived from di-
verse modalities with Ia. This empowers the network to
process multiple information sources efﬁciently, equipping
it to capture the rich information encapsulated within Ia. In
contrast, HA-ViT, despite sharing a similar network struc-
ture, appears to lack an effective fusion mechanism, rely-
272
Table 1. FBR performance comparisons on intra-modality ( f–fandp–p) and cross-modality ( f–pandp–f) recognition tasks in terms of
rank-1 recognition (%). The best accuracy is written in bold.
ModelEthnic FaceScrub IMDB Cross-Modal DB
f–f p–p f–p p–f f–f p–p f–p p–f f–f p–p f–p p–f f–f p–p f–p p–f
Model trained independently on Face or Periocular
Baseline 80.61 61.79 0.39 0.33 76.10 63.29 0.47 0.42 62.08 55.31 0.10 0.09 60.43 51.43 0.09 0.07
Model trained with Face and Periocular
Tiong et al. [32] 80.30 65.42 0.44 0.51 76.46 72.13 0.57 0.61 65.31 54.91 0.18 0.19 64.70 50.18 0.11 0.13
George et al. [5] 85.98 66.90 54.18 59.46 89.80 77.76 62.31 66.82 72.58 54.72 34.91 43.57 70.56 47.16 29.50 38.72
Nget al. [22] 92.55 79.29 71.54 75.36 91.82 85.74 75.15 78.36 77.19 71.99 58.56 61.19 76.21 65.63 58.13 61.35
ViT/VPT [ 10] 91.80 83.94 76.68 77.74 93.11 89.26 83.35 85.17 78.57 72.15 60.46 62.32 80.97 69.01 63.13 66.42
HA-ViT [ 33] 91.36 84.32 76.34 77.65 92.13 88.52 80.27 81.80 78.23 71.92 59.49 59.68 78.76 66.87 59.93 61.87
Model trained with Face, Periocular, and Soft-biometric Attributes
ViT/VPT [ 10] 92.95 85.68 83.37 86.10 93.57 90.84 87.43 88.61 81.97 74.59 69.40 70.21 82.76 70.92 65.99 69.11
HA-ViT [ 33] 91.72 85.10 80.03 81.61 92.46 88.70 84.33 85.63 78.81 71.42 64.13 65.49 78.81 67.22 62.34 64.03
MFA-ViT/MPT 94.82 89.98 86.70 89.07 95.71 93.06 90.38 92.02 86.03 80.53 75.28 76.36 85.88 76.54 72.01 75.96
95%Ethnic91%88%84%80%f-fp-pf-pp-f95%FaceScrub91%88%84%80%f-fp-pf-pp-f
88%IMDB80%72%64%56%f-fp-pf-pp-f88%Cross-Modal DB81%74%67%60%f-fp-pf-pp-fMFA-ViTwith !!MFA-ViTwithout !!HA-ViTwith !!HA-ViTwithout !!Figure 3. Performance comparison on the proposed model trained
with or without Iaagainst HA-ViT.
ing primarily on simple concatenation for aggregating mul-
timodal features. This primitive fusion approach in HA-ViT
may account for its comparatively suboptimal utilization of
Iacompared to MFA-ViT.
4.4.2 With and Without MPT
To gauge the efﬁcacy of the MPT strategy, we compare
the MFA-ViT’s performance with and without the incor-
poration of MPT. As illustrated in Figure 4, a progressive
enhancement in performance is observed when MFA-ViT
is integrated with MPT, compared to its operation without
MPT. However, it is crucial to underscore that the perfor-
mance of MFA-ViT without MPT lags in delivering sub-
stantial rank-1 results in intra- and cross-modality recogni-
tion tasks.
Test Accuracy (%)
MFA-ViT/MPTMFA-ViT/VPT
959289868380Ethnic
MFA-ViT
959391898785FaceScrub
IMDB85827976737067
Cross-Modal DB868380746865716277Test Accuracy (%)Figure 4. Performance comparison on MPT, VPT, and no prompt.
This analysis exclusively focuses on the evaluation of MFA-ViT
trained using If,Ip, and Ia.
To gain deeper insights into the advantages of the MPT
strategy, we present visualization results using the Grad-
CAM [ 28]. As shown in Figure 5, MFA-ViT/MPT focuses
intensively on the shared areas of the eye regions in the If
andIpimages. Conversely, MFA-ViT does not employ a
prompt strategy, further limiting the model’s ability to dis-
cern less speciﬁc features compared to the comprehensive
approach offered by the MPT strategy.
These observations vividly showcase the robustness of
MPT. Without the guidance of MPT, MFA-ViT struggles to
discern and accentuate speciﬁc features across modalities
precisely. However, the integration of MPT enables MFA-
ViT to manage input sequences adeptly across its multi-
modal fusion attention layers. This goes beyond just align-
ing different modalities; it is also about discerning and pre-
serving the unique attributes of each while fostering cohe-
sive cross-modality collaboration.
273
Table 2. Performance comparisons on classiﬁcation head inputs (CLS and PRM) in terms of rank-1 recognition (%). The best accuracy is
indicated in bold.
Head Input Ethnic FaceScrub IMDB Cross-Modal DB
CLS PRM f–f p–p f–p p–f f–f p–p f–p p–f f–f p–p f–p p–f f–f p–p f–p p–f
X 94.57 89.18 86.24 88.34 95.26 92.63 89.81 91.51 85.32 79.43 74.52 76.36 85.11 75.53 71.06 74.35
X94.82 89.98 86.70 89.07 95.71 93.06 90.38 92.02 86.03 80.53 75.28 77.37 85.88 76.54 72.01 75.96
Figure 5. Visualization of activation maps for MFA-ViT with
MPT, VPT, and no prompt strategies.
4.4.3 MPT vs VPT
We further investigate the effectiveness of MPT compared
to VPT within the MFA-ViT architecture. Since VPT was
initially designed only for ViT, we extend it to demonstrate
the performance of MPT in utilizing attributes in contrast
to VPT. As illustrated in Figure 4, we observe a gradual
increase in the performance of MFA-ViT/MPT compared
to MFA-ViT/VPT. However, it is essential to note that the
performance of MFA-ViT/VPT still falls short of achieving
signiﬁcant rank-1 results in intra- and cross-modality recog-
nition tasks.
Delving deeper, the visualizations in Figure 5elucidate
that MFA-ViT/MPT is impressive because it adeptly attends
the ocular region on the Ifimage and simultaneously on the
Ipmodalities. In contrast, MFA-ViT/VPT appears to have
difﬁculty differentiating speciﬁc features between IfandIp
modalities. This is because the VPT strategy was initially
designed for task-speciﬁc features, lacking guidance for the
model to understand the intricate association between mul-
timodal features.
These ﬁndings underscore the prowess of MPT in facil-
itating a seamless interplay between IfandIpmodalities
with Ia. The nuanced guidance steers the model towards
a delicate understanding of the modalities, distinguishing it
from VPT. Moreover, the results indicate that VPT’s func-
tionality is impacted by neglecting important multimodal
features within each layer of the learning process.4.4.4 Impacts of Classiﬁcation Head Inputs
In this subsection, we investigate the inﬂuence of different
classiﬁcation head inputs on our model’s performance in
FBR. We consider two types of classiﬁcation head input:
the class token embedding T⇤(CLS) follow the standard
practice as in ViT [ 4] and the joint embeddings J⇤from
multimodal-prompt token embedding ( PRM ). As shown
in Table 2, the result reveals a signiﬁcant performance
degradation when the model is trained with CLS. In con-
trast, training with PRM consistently improves performance
across all benchmark datasets.
The superiority of PRM over CLS is underscored by
these ﬁndings. PRM exhibits effective alignment with FBR
tasks, enabling the model to acquire a more discrimina-
tive embedding. This is achieved through prompt embed-
dings to guide attention toward speciﬁc features and rela-
tionships within multimodal data. In contrast, CLS lacks
cross-modality feature alignment, potentially leading to a
failure in capturing delicate relationships crucial for ad-
dressing FBR problems.
5. Discussion and Conclusion
This paper introduces the MFA-ViT approach to address the
intricate challenges of ﬂexible biometric recognition (FBR).
MFA-ViT leverages an MFA and an MPT to capture cross-
modality and multimodal dependencies in embedding. The
MPT is crucial in facilitating the acquisition of intermodal
associations, which is vital in intra- and cross-modality
recognition tasks. The integration enhances the compre-
hensive understanding of data, particularly in challenging
real-world scenarios, signiﬁcantly boosting recognition per-
formance. Additionally, incorporating soft-biometric at-
tributes provides further contextual insights, strengthening
the discriminative potential of our embeddings. For future
work, we see the potential to extend FBR to encompass
other biometric modalities, paving the way for exceptional
accuracy and efﬁciency in diverse recognition tasks.
6. Acknowledgement
This work was supported by the National Research Founda-
tion of Korea (NRF) grant funded by the Korea government
(MSIP) (NO. NRF-2022R1A2C1010710).
274
References
[1]Elisa Barroso, Gil Santos, Luis Cardoso, Chandrashekhar
Padole, and Hugo Proenc ¸a. Periocular recognition: How
much facial expressions affect performance? Pattern Anal.
Appl. , 19:517–530, 2016.
[2]Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and An-
drew Zisserman. VGGFace2: A dataset for recognising faces
across pose and age. In Int. Conf. Autom. Face Gesture
Recog. , pages 67–74, 2018.
[3]Kai Cheng, Xin Liu, Yiu-ming Cheung, Rui Wang, Xing
Xu, and Bineng Zhong. Hearing like seeing: Improving
voice-face interactions and associations via adversarial deep
semantic matching network. In ACM MM , page 448–455,
2020.
[4]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR , 2021.
[5]Anjith George and Sebastien Marcel. Cross modal focal loss
for RGBD face anti-spooﬁng. In CVPR , pages 7878–7887,
2021.
[6]Ester Gonzalez-Sosa, Julian Fierrez, Ruben Vera-Rodriguez,
and Fernando Alonso-Fernandez. Facial soft biometrics for
recognition in the wild: Recent works, annotation, and cots
evaluation. IEEE Trans. Inf. Forensics Secur. , 13(8):2001–
2014, 2018.
[7]Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and
Artem Babenko. Revisiting deep learning models for tabular
data. In NeurIPS , pages 18932–18943, 2021.
[8]JongWon Hwang, Leslie Ching Ow Tiong, and Andrew
Beng Jin Teoh. Towards face representation learning condi-
tioned on the soft biometrics. In Int. Conf. Mach. Vis. Appl. ,
page 1–7, 2022.
[9]Seyed Mehdi Iranmanesh, Ali Dabouei, and Nasser
Nasrabadi. Attribute adaptive margin softmax loss using
privileged information. In BMVC , pages 1–13, 2020.
[10] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In ECCV , page 709–727, 2022.
[11] Luo Jiang, Juyong Zhang, and Bailin Deng. Robust RGB-D
face recognition using attribute-aware loss. IEEE TPAMI , 42
(10):2552–2566, 2020.
[12] Yoon Gyo Jung, Cheng Yaw Low, Jaewoo Park, and Andrew
Beng Jin Teoh. Periocular recognition in the wild with gen-
eralized label smoothing regularization. IEEE Sign. Process.
Letters , 27:1455–1459, 2020.
[13] Changil Kim, Hijung Valentina Shin, Tae-Hyun Oh, Alexan-
dre Kaspar, Mohamed Elgharib, and Wojciech Matusik. On
learning associations of faces and voices. In ACCV , pages
276–292, 2018.
[14] Takumi Kobayashi. Large margin in softmax cross-entropy
loss. In BMVC , pages 1–12, 2019.
[15] Nagashri N Lakshminarayana, Nishant Sankaran, Sriran-
garaj Setlur, and Venu Govindaraju. Multimodal deep fea-
ture aggregation for facial action unit recognition using vis-ible images and physiological signals. In Int. Conf. Autom.
Face Gesture Recog. , pages 1–4, 2019.
[16] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi-
roaki Hayashi, and Graham Neubig. Pre-train, prompt, and
predict: A systematic survey of prompting methods in nat-
ural language processing. ACM Comput. Surv. , 55(9):1–35,
2023.
[17] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In ICLR , pages 1–10, 2019.
[18] Iacopo Masi, Yue Wu, Tal Hassner, and Prem Natarajan.
Deep face recognition: A survey. In Conf. Graph. Patterns
Images , pages 471–478, 2018.
[19] Shervin Minaee, Amirali Abdolrashidi, Hang Su, Mo-
hammed Bennamoun, and David Zhang. Biometrics recog-
nition using deep learning: A survey. Artif. Intell. Rev. , 56
(8):8647–8695, 2023.
[20] Arsha Nagrani, Samuel Albanie, and Andrew Zisserman.
Learnable PINS: Cross-modal embeddings for person iden-
tity. In ECCV , page 71–88, 2018.
[21] Hong-Wei Ng and Stefan Winkler. A data-driven approach to
cleaning large face datasets. In ICIP , pages 343–347, 2014.
[22] Tiong-Sik Ng, Cheng-Yaw Low, Jacky Chen Long Chai, and
Andrew Beng Jin Teoh. Conditional multimodal biometrics
embedding learning for periocular and face in the wild. In
ICPR , pages 812–818, 2022.
[23] Chandrashekhar N. Padole and Hugo Proenc ¸a. Periocular
recognition: Analysis of performance degradation factors. In
Int. Conf. Biometrics (ICB) , pages 439–445, 2012.
[24] Unsang Park, Arun Ross, and Anil K. Jain. Periocular bio-
metrics in the visible spectrum: A feasibility study. In
Int. Conf. Biometrics Theory Appl. Syst. (BTAS) , pages 1–6,
2009.
[25] Hugo Proenc ¸a and Jo ˜ao C. Neves. Deep-PRWIS: Periocular
recognition without the iris and sclera using deep learning
frameworks. IEEE Trans. Inf. Forensics Secur. , 13(4):888–
896, 2018.
[26] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep ex-
pectation of real and apparent age from a single image with-
out facial landmarks. IJCV , 126(2):144–157, 2018.
[27] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.
Chen. Mobilenetv2: Inverted residuals and linear bottle-
necks. In CVPR , pages 4510–4520, 2018.
[28] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek
Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-
tra. Grad-cam: Visual explanations from deep networks via
gradient-based localization. In ICCV , pages 618–626, 2017.
[29] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR ,
pages 1–14, 2015.
[30] Mingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking
model scaling for convolutional neural networks. In Int.
Conf. Mach. Learn. (ICML) , page 6105–6114, 2019.
[31] Philipp Terh ¨orst, Daniel F ¨ahrmann, Jan Niklas Kolf, Naser
Damer, Florian Kirchbuchner, and Arjan Kuijper. Maad-
face: A massively annotated attribute dataset for face images.
IEEE Trans. Inf. Forensics Secur. , 16:3942–3957, 2021.
275
[32] Leslie Ching Ow Tiong, Seong Tae Kim, and Yong Man Ro.
Multimodal facial biometrics recognition: Dual-stream con-
volutional neural networks with multi-feature fusion layers.
Image Vis. Comput. , 102:103977, 2020.
[33] Leslie Ching Ow Tiong, Dick Sigmund, and Andrew
Beng Jin Teoh. Face-periocular cross-identiﬁcation via con-
trastive hybrid attention vision transformer. IEEE Sign. Pro-
cess. Letters , 30:254–258, 2023.
[34] Hanrui Wang, Xingbo Dong, Zhe Jin, Jean-Luc Dugelay, and
Massimo Tistarelli. Cross-spectrum face recognition using
subspace projection hashing. In ICPR , pages 615–622, 2021.
[35] Mei Wang and Weihong Deng. Deep face recognition: A
survey. Neurocomputing , 429:215–244, 2021.
[36] Rui Wang, Xin Liu, Yiu-ming Cheung, Kai Cheng, Nannan
Wang, and Wentao Fan. Learning discriminative joint em-
beddings for efﬁcient face and voice association. In ACM
SIGIR Conf. Res. Develop. Inf. Retrieval , page 1881–1884,
2020.
[37] Gou Wei, Li Jian, and Sun Mo. Multimodal(audio, facial and
gesture) based emotion recognition challenge. In Int. Conf.
Autom. Face Gesture Recognit. , pages 908–911, 2020.
[38] Peng Xu, Xiatian Zhu, and David A. Clifton. Multimodal
learning with transformers: A survey. IEEE TPAMI , 45(10):
12113–12132, 2023.
[39] Yuhao Zhu, Min Ren, Hui Jing, Linlin Dai, Zhenan Sun, and
Ping Li. Joint holistic and masked face recognition. IEEE
Trans. Inf. Forensics Secur. , 18:3388–3400, 2023.
[40] M. Zolfaghari, Y . Zhu, P. Gehler, and T. Brox. CrossCLR:
Cross-modal contrastive learning for multi-modal video rep-
resentations. In ICCV , pages 1430–1439, 2021.
276
