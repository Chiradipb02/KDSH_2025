Active Domain Adaptation with False Negative Prediction
for Object Detection
Yuzuru Nakamura1Yasunori Ishii1Takayoshi Yamashita2
1Panasonic Holdings Corporation2Chubu University
{nakamura.yuzuru,ishii.yasunori }@jp.panasonic.com takayoshi@isc.chubu.ac.jp
Abstract
Domain adaptation adapts models to various scenes with
different appearances. In this field, active domain adapta-
tion is crucial in effectively sampling a limited number of
data in the target domain. We propose an active domain
adaptation method for object detection, focusing on quan-
tifying the undetectability of objects. Existing methods for
active sampling encounter challenges in considering unde-
tected objects while estimating the uncertainty of model pre-
dictions. Our proposed active sampling strategy addresses
this issue using an active learning approach that simulta-
neously accounts for uncertainty and undetectability. Our
newly proposed False Negative Prediction Module evalu-
ates the undetectability of images containing undetected ob-
jects, enabling more informed active sampling. This ap-
proach considers previously overlooked undetected objects,
thereby reducing false negative errors. Moreover, using
unlabeled data, our proposed method utilizes uncertainty-
guided pseudo-labeling to enhance domain adaptation fur-
ther. Extensive experiments demonstrate that the perfor-
mance of our proposed method closely rivals that of fully
supervised learning while requiring only a fraction of the
labeling efforts needed for the latter.
1. Introduction
Significant differences in appearance due to factors such as
lighting conditions and sensors, as observed in in-vehicle
cameras, demand substantial annotation for each dataset,
resulting in prolonged model deployment. Domain adapta-
tion (DA) is an effective technique in such situations, allow-
ing models to adapt to various scenes with different appear-
ances. Unsupervised domain adaptation (UDA) that deals
with unlabeled target domain data has been widely stud-
ied [2–4, 9, 14, 23, 35, 55, 60, 61]. UDA is an optimal
approach for reducing annotation costs. However, a con-
siderable performance gap persists between UDA and fully
supervised learning, wherein all data are labeled.
Uncertainty
Undetectability
(Ours)
Predicted
Proposals
False Negative
ErrorsFigure 1. Conceptual diagram of the conventional uncertainty
and our proposed undetectability metrics used for the acqui-
sition function. Uncertainty is only estimated from predicted
bounding boxes (represented by blue rectangles). In contrast, un-
detectability is estimated from false negative (FN) errors that the
detection model cannot predict (represented by red dotted rectan-
gles). Our proposed method performs active sampling by utilizing
both uncertainty and undetectability.
Active learning (AL) [5, 10, 16, 22, 24, 26, 30, 42, 45,
46, 53, 56, 58] is a method to select a few effective samples
for training, achieving high accuracy with a limited number
of labeled data. However, most AL methods select sam-
ples from a single pool, assuming an independent and iden-
tically distributed ( i.i.d.) dataset. Consequently, under do-
main shift, AL fails to select samples capable of enhancing
performance.
Active domain adaptation (ADA) is a solution for ef-
fective sample selection under domain shift. This method
has been actively studied [1, 7, 11, 18, 19, 21, 25, 29, 31,
33, 38, 40, 44, 47–51, 57, 59] and, with a limited labeling
budget, achieves accuracy comparable to fully supervised
learning [38]. Most ADA studies focus on image classifi-
cation and semantic segmentation tasks, whereas relatively
less research has been conducted on applying this method
to an object detection task. Classification and segmentation
tasks have a common feature: they can be regarded as clas-
sifying tasks for an image or pixel. In contrast, an object de-
tection task predicts the location and category of an object
bounding box. In ADA, classification uncertainty is taken
as a criterion for sample selection. Therefore, even if ADA,
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28782
designed for classification and segmentation, is applied to
object detection, its contribution pertains only to classifica-
tion, not the prediction or localization of the object region.
Contributions . In this paper, we propose an ADA method
designed for object detection. First, we analyzed the causes
of performance degradation under domain shift to identify
the main challenge in DA for object detection. The analysis
showed that, under domain shift, the increase in false neg-
ative (FN) errors in the target domain considerably impacts
performance.
Existing AL methods for object detection estimate the
uncertainty of predicted object proposals, excluding unde-
tected objects from predictions, rendering uncertainty es-
timation impossible. Hence, designing an AL method that
accounts for undetected objects becomes crucial for enhanc-
ing object detection performance under domain shift.
We propose an active sampling strategy integrating un-
detectability into the acquisition function to incorporate the
aspect of undetected objects in sample selection (Figure 1).
We introduce a framework and model, predicting the possi-
bility of FN errors in images. We call this model the False
Negative Prediction Module (FNPM). FNPM assesses un-
detectability for each image, actively selecting images that
contain numerous undetected objects, thereby training the
object detection model to be robust against FN errors.
Our proposed method initializes the model with UDA
training. As the estimation by the acquisition function be-
comes inaccurate without any DA under domain shift, fea-
ture alignment is performed between source and target do-
mains by adversarial learning [13]. Subsequently, active
sampling using our acquisition function is performed, fol-
lowed by training in a semi-supervised DA manner. This
training includes a few labeled target domain data in ad-
dition to source and unlabeled target domain data. In
addition to feature alignment, our method filters pseudo-
labels by utilizing the localization uncertainty of predicted
bounding boxes. A diverse range of experimental scenar-
ios demonstrates that our proposed method achieves perfor-
mance comparable to fully supervised learning, requiring
only a few percent of the labeling budget. Consequently,
our method, at a low labeling cost, outperforms previous
UDA methods.
Our contributions can be summarized as follows:
• We proposed an ADA method designed for object detec-
tion. To the best of our knowledge, this is the first ADA
method designed for object detection.
• Based on the performance analysis under domain shift,
we identified the issue of undetected objects. Further-
more, we proposed the FNPM focusing on undetected ob-
jects.
• We experimentally demonstrated that even with a low la-
beling budget, the performance of our proposed method
approaches that of fully supervised learning.2. Related Works
Domain Adaptive Object Detection methods adapt mod-
els to various scenes with different appearances. In partic-
ular, unsupervised domain adaptation (UDA), which does
not use target domain labels, has been widely studied [2–
4, 14, 23, 35, 55, 61]. UDA for object detection is frequently
based on adversarial learning or self-training approaches.
Adversarial learning is a method that aligns across
domains in a feature space using the gradient rever-
sal layer and domain discriminator [13]. Adaptive
Teacher [23] combines domain alignment and self-training
to enhance pseudo-labeling accuracy under domain shift.
MGADA [60] aligns domains at various levels, such as
pixel, instance, and category, through integrated multi-
granularity alignment. The self-training method provides
unlabeled target domain data with pseudo-labels for subse-
quent training. Probabilistic Teacher [3] dynamically filters
out noisy pseudo-labels attributed to uncertainty. Unbiased
Mean Teacher [9] uses a generative model to transform tar-
get domain images into a source-like style, thus improving
the accuracy of pseudo-labeling.
Despite extensive studies, UDA’s performance is sub-
stantially deficient compared to that of fully supervised
learning. Conversely, the performance of our proposed
method approaches fully supervised learning while using
only a few percent of the latter approach’s labeling cost.
Active Learning (AL) for Object Detection [5, 10, 22,
24, 26, 30, 42, 45, 46, 53, 56, 58] methods aim to se-
lect effective samples for training within a limited label-
ing budget. Recently, a semi-supervised learning method
using labeled and unlabeled data was proposed. Active
Teacher [26] utilizes a teacher-student structure to generate
pseudo-labels from unlabeled data and incorporates them
into the student model for training. Elezi et al. [10] pro-
posed a method that switches between self-training with
pseudo-labels or inconsistency-based training based on pre-
diction confidence determined by an acquisition function.
These AL methods sample data from a single pool, and
the distribution of the sampled data is assumed to be the
i.i.d. dataset. Therefore, the ability to sample data suitable
for performance improvement is limited under domain shift.
Active Domain Adaptation (ADA) focuses on selecting
effective samples for training under domain shift. This ap-
proach has been actively studied recently [1, 7, 11, 18, 19,
21, 25, 29, 31, 33, 38, 40, 44, 47–51, 57, 59]. LabOR [38]
achieves accuracy comparable to fully supervised learning
on a budget of a few percent of the latter in the semantic
segmentation task. This approach is made possible by an-
notating pixels where uncertainty exists under domain shift.
AADA [40] uses domain discriminator-predicted results for
sampling as a diversity metric combined with uncertainty.
Previous methods mainly focus on an image classifica-
tion task, and some have been applied experimentally to
28783
Backbone
Domain
DiscriminatorGRL Adversarial LossBackboneDomain
DiscriminatorAcquisition
Function
Supervised LossUnsupervised LossUncertainty
Estimation
Pseudo-Labeling
Unlabeled Target Data
Labeled Target Data
Source DataStudentTeacher
EMAAnnotator
Detection
HeadDetection
Head
Active Learning
Supervised Learning
Unsupervised LearningDUT
DLTTop-K
Sampling
LabelingFalse Negative
Prediction Module
Strong
Aug.Weak
Aug.Figure 2. Overview of our proposed method. We propose an active domain adaptation (ADA) framework incorporating the domain
adaptation (DA) and active sampling processes with the False Negative Prediction Module (FNPM). In the active sampling process, the
FNPM estimates the metrics of undetectability, and our acquisition function measures the scores by considering undetectability in addition
to conventional uncertainty and diversity. We then sample unlabeled target domain data based on these scores and label them. Pseudo-
labels are assigned to the unlabeled target data, which are then used for training in the semi-supervised DA process.
an object detection task [40]. However, the contribution
of these methods to object detection is limited because ob-
ject detection involves numerous other factors besides the
uncertainty of category prediction. For example, previous
methods cannot estimate the uncertainty of undetected ob-
jects. Furthermore, category prediction and bounding box
localization are crucial for object detection. Hence, we pro-
pose an ADA method suitable for object detection to ad-
dress these issues.
3. Method
Notation . We have a set of labeled source data DS=
{(xS
i,yS
i)}NS
i=1and unlabeled target data DT={xT
i}NT
i=1,
where xi∈ RW×H×3is the i-th image of width Wand
height Hin a dataset, and yi={bi,j, ci,j}Ni
bbox
j=1 consists
of the j-th bounding box coordinates bi,j∈ {x, y, w, h }
and category index ci,j∈ {1, ..., N c}.DTconsists of a set
of labeled target data DLT={(xLT
i,yLT
i)}NLT
i=1and un-
labeled target data DUT={xUT
i}NUT
i=1. Given DLT=∅
at the beginning of training, we sample images from DUT
that maximize performance through the acquisition function
within a labeling budget. Then, we annotate these images
and incorporate them into DLT.
3.1. Overview
Figure 2 shows the overall framework of our proposed
method. Our proposed method involves the domain adapta-
tion (DA) and active learning (AL) processes. The conven-
tional uncertainty-based AL methods determine whether ornot to sample predicted objects. However, these methods
do not define any criteria for sampling undetected objects.
Therefore, conventional AL methods cannot sufficiently im-
prove the performance for false negative (FN) errors. In this
situation, we propose an active domain adaptation (ADA)
method for object detection using the False Negative Pre-
diction Module (FNPM) to predict the number of FN ob-
jects. Our proposed acquisition function outputs a metric
from the FNPM’s output value, uncertainty, and diversity.
Our proposed method involves three steps: (1) Initial-
izing the model with (DS,DT)in UDA training (Sec-
tion 3.2), (2) active sampling within the budget from DUT
with our acquisition function and incorporating the samples
intoDLTafter labeling (Section 3.3), and (3) training the
model with labeled data DS∪DLTand unlabeled data DUT
in a semi-supervised DA manner (Section 3.4). Five rounds
of steps (2) and (3) are performed. The detailed procedure
is shown in Algorithm 1.
3.2. Model Initialization
We first initialize the common parameters for student and
teacher models with (DS,DT)in a UDA training man-
ner. Active sampling is based on the metrics measured by
the acquisition function for the target domain data. How-
ever, if the model used for the acquisition function does
not have knowledge of the target domain, the acquisition
function cannot correctly measure the metrics for sampling.
Therefore, we first train the model adapted to the target
domain using UDA. Specifically, we perform feature-level
28784
Algorithm 1: Training procedure of proposed method
Input: Source data DS, Target data DT={DLT,DUT},
whereDLT=∅
Output: Model parameters θt, θsadapted on target
domain
1begin
2 Pre-train the student model θs, ϕsbased on Eq. (1)
3 θt←θs, ϕt←ϕs
4 forRRounds do
5 Freeze θtand train FNPM ψbased on Eq. (4)
6 forxUT∈ DUTdo
7 Calculate the acquisition score sallbased on
Eq. (11)
8 end for
9 Select the top-K data Dactive and annotate them
10 DLT← D LT∪ Dactive
11 DUT← D UT\ Dactive
12 Unfreeze θt
13 Train the student model θs, ϕsbased on Eq. (12)
14 Update the teacher model θt, ϕsbased on
Eq. (15)
15 end for
16end
alignment across domains using adversarial learning with
the gradient reversal layer (GRL) and domain discrimina-
tor [13].
Given the detection model and domain discriminator pa-
rameters θsandϕs, respectively, of the student model, the
objective loss function is defined as follows:
min
θsmax
ϕsLinit=LS
sup+λLadv, (1)
where λis the weight of Ladv.LS
supis the supervised loss
in the source domain defined as follows:
Lsup=Lrpn
cls(xi,ci) +Lrpn
reg(xi,bi)
+Lroi
cls(xi,ci) +Lroi
reg(xi,bi),(2)
where Lrpn
clsandLrpn
regare the classification loss and re-
gression loss, respectively, in the region proposal network
(RPN). Lroi
clsandLroi
regare the classification loss and regres-
sion loss, respectively, in the region of interest (RoI) head.
The adversarial loss Ladvis defined as follows:
Ladv=−log(1−D(Fenc(xS
i;θs);ϕs))
−logD(Fenc(xT
i;θs);ϕs),(3)
where FencandDare the backbone of the detection model
and the domain discriminator, respectively. We train the
domain discriminator to discriminate the source domain as
1 and the target domain as 0.
After training with UDA on all the data in (DS,DT), the
parameters of the student model are copied to the parame-
ters(θt, ϕt)of the teacher model (θt←θs, ϕt←ϕs), and
then, we proceed to the active sampling step.
False Positive False Negative0102030405060Number of ErrorsSource
UDA (PT)
OracleFigure 3. Comparison of the number of false positive (FP) er-
rors and FN errors in unsupervised domain adaptation (UDA)
from KITTI to Cityscapes. We used Probabilistic Teacher [3]
as the UDA method in this analysis. Even if UDA reduced the
number of FN errors, the number remained larger than that of FP
errors.
3.3. Active Learning Based on False Negatives
3.3.1 Analysis of Missed Detection under Domain Shift
First, we analyzed the causes of performance degradation
under domain shift to clarify the primary challenge in apply-
ing UDA for object detection. Figure 3 shows a comparison
between false positive (FP) errors and FN errors in UDA.
When comparing Source-only (no adaptation) with Oracle,
the gap in FP errors was 1 pt, whereas, for FN errors, it was
more than double. This result shows that FN errors are the
main cause of performance degradation under domain shift.
Although the UDA method remarkably reduced FN errors
compared with Source-only, the gap in FN errors remained
larger than in FP errors. Therefore, FN errors persist as the
main cause of performance degradation decline under do-
main shift, indicating the potential for further performance
improvement if this issue is resolved.
The results indicate that AL should focus on sampling
images containing undetected objects to effectively reduce
FN errors under domain shift. However, many AL methods
use predicted objects to assess uncertainty when sampling
data. Therefore, uncertainty cannot be estimated for unde-
tected objects that do not appear in the detection proposals.
Hence, we propose an additional metric for the acquisition
function, specifically the number of FN errors per image.
This metric allows sampling to consider the uncertainty and
undetectability of objects.
3.3.2 False Negative Prediction Module
The FNPM predicts the number of FN errors for an image,
intuitively representing the difficulty of detecting objects.
Moreover, as the FNPM outputs the count of FN errors, se-
lected samples based on FNPM metrics contain more infor-
mative data than other samples. By integrating FNPM pre-
dictions into the acquisition function, we can quantify the
undetectability of objects and actively select samples that
28785
GAP
False Negative
Prediction ModuleBackboneFalse Negative
Calculation
False Negative
Prediction LossBounding Box
Prediction
Detection
Head
FCReLUFCReLUFCSigmoidFalse Negative
ErrorFigure 4. Architecture of the FNPM. The FNPM predicts the
number of FN errors. We train the FNPM to predict the number of
FN errors using the domain-adapted backbone.
contribute to reducing FN errors.
Since various factors can cause FN errors and determin-
istic approaches exhibit poor prediction accuracy, deep neu-
ral networks (DNNs) based prediction methods were pro-
posed [32, 52]. We follow these approaches and design a
branch that predicts the number of FN errors using a DNNs
regression model. Figure 4 shows the architecture of the
FNPM. The FNPM receives the output feature map of the
backbone of the detection model and feeds this map to the
global average pooling (GAP) and fully connected (FC) lay-
ers. Subsequently, the FNPM outputs predictions of the
number of FN errors. As corresponding detection results
are obtained from the detection model, the ground truth of
the number of FN errors can be calculated. The FNPM is
trained to minimize errors using the loss function, which is
defined as follows:
Lfn=(G(Fenc(xS
i;θt);ψ)− FN (Fhead (xS
i;θt),yS
i))2
+(G(Fenc(xLT
i;θt);ψ)− FN (Fhead (xLT
i;θt),yLT
i))2,
(4)
where G,ψ, and Fhead are the FNPM, its parameters, and
the head of the detection model, respectively. FN(·,·)cal-
culates the number of FN errors in the detection results by
determining the number of ground truths ynot assigned a
detection result Fhead(x;θ)of the same category with in-
tersection over union (IoU) above a threshold.
During the active sampling process, although the FNPM
predicts the number of FN errors for the unlabeled target
domain, the challenge is that only a few labeled data in the
target domain are available for training the FNPM. To lever-
age labeled data in the source domain, we use the adapted
backbone of the detection model to extract domain-invariant
features and predict these features for the target domain.
Furthermore, we can reduce the number of additional pa-
rameters by using a common feature extractor.
Since the ground truths of the FN errors are calculated
from the prediction results of the detection model, updat-ing the detection model also alters the ground truths, com-
plicating the stable training of the FNPM. Inspired by the
reinforcement learning method [27, 28], we train the de-
tection model and the FNPM alternately. As the FNPM is
used only during the active sampling process, it remains un-
altered during the training of the detection model. Before
active sampling, we freeze the parameters of the detection
model and update only the FNPM. This approach makes it
possible to optimize both the detection model and FNPM
simply and stably.
3.3.3 Uncertainty Estimation with MCDropout
Subsequently, we discuss the measurement of uncertainty
in cases when the detection model identifies the objects
in an image. Many object detection methods measure un-
certainty based on class probability in a manner similar to
that adopted in object recognition tasks. However, in ob-
ject detection, the detection model should stabilize not only
the class probabilities but also the positions of the objects.
Therefore, we use variational inference with Monte Carlo
Dropout (MCDropout) [12] to consider the parameters of
the detection model as a probability distribution. We quan-
tify the variance in localization predictions due to model
perturbations and use it as a metric for uncertainty.
We incorporate the MCDropout layer into the detection
head and reformulate the predicted bounding box coordi-
nates ˆbiand class probabilities ˆpias follows:
{ˆbi(ξ),ˆpi(ξ)}=Fhead(xi;θt, ξ),where ξ∼Ber(η),(5)
where Ber(η)is Bernoulli distribution of dropout rate η.
Variational inference allows us to obtain multiple pre-
dictions sampled under a pseudo-probability distribution.
We use these means (ˆbmean
i,ˆpmean
i )as the prediction re-
sults considering model perturbations and the variance of
the predicted coordinates ˆbvar
ias the localization uncer-
tainty. These means and variances are calculated from
ˆbi,m∼ˆbi(ξ),ˆpi,m∼ˆpi(ξ)when inferred Mtimes.
3.3.4 Acquisition Function
We propose an active sampling strategy that scores a combi-
nation of four metrics, including undetectability and local-
ization uncertainty. In the following text, we explain each
metric and the final combination of the metrics.
Undetectability . We estimate the undetectability of the de-
tection model for images using the FNPM (Section 3.3.2).
The higher the value, the more difficult the sample to predict
and the more informative the sample. The undetectability
metric is defined as follows:
sfn
i=G(Fenc(xUT
i;θt);ψ). (6)
28786
Localization Uncertainty . We quantify the variation in the
predicted coordinates of bounding boxes by variational in-
ference (Section 3.3.3). The localization uncertainty metric
is defined as follows:
sloc
i=1
4Ni
bboxNi
bboxX
j=1X
k∈{x,y,w,h }ˆbvar
i,j,k. (7)
Classification Uncertainty . We use the entropy [43] of
class probabilities to estimate the uncertainty in classifica-
tion. Higher entropy is assumed to be useful for training
because the sample is difficult to classify. The classification
uncertainty metric is defined as follows:
sent
i=−1
Ni
bboxNi
bboxX
j=1NcX
k=1ˆpmean
i,j,k log ˆpmean
i,j,k. (8)
Diversity . We use the metric based on the idea that a high
density of the target domain is more critical under domain
shift. Following [40], we use the domain discriminator to
estimate samples that better represent the distribution of the
target domain. The diversity metric is defined as follows:
sdiv
i=1−D(Fenc(xUT
i;θt);ϕt)
D(Fenc(xUT
i;θt);ϕt). (9)
Final Metric . These four metrics are used to calculate the
final metric for each image. However, the metrics have dif-
ferent ranges of values, and the metrics with a large scale
may become dominant. Therefore, we normalize each met-
ric based on the following expression:
ˆsl
i=max(0,sl
i−(µ(sl)−3σ(sl))
6σ(sl)), (10)
where l∈ {fn, loc, ent, div }, and µ(·)andσ(·)are the
mean and standard deviation, respectively. As each metric
can be assumed unimodal in diverse natural images, it is
scaled by 6σbased on a normal distribution.
The final metric is calculated as the product of the indi-
vidual metrics as follows:
sall
i= ˆsfn
iˆsloc
iˆsent
iˆsdiv
i. (11)
3.4. Semi-Supervised Domain Adaptation
We use a semi-supervised learning framework to train the
model because incorporating a large number of unlabeled
data and a few labeled data can drastically enhance the train-
ing process and improve model accuracy. We conduct su-
pervised learning on source and labeled target domain data
and unsupervised learning on unlabeled target domain data
with uncertainty-guided pseudo-labeling to perform semi-
supervised learning. We use pseudo-labels with low local-
ization uncertainty to achieve more accurate bounding box
localization.The objective loss function is defined as follows:
min
θsmax
ϕsLtotal=LS
sup+LLT
sup+Lunsup +λLadv,(12)
where LLT
supis the supervised loss in the labeled target do-
main, similar to Eq. (2). Lunsup is the unsupervised loss in
the unlabeled target domain defined as follows:
Lunsup =1
Ni
bboxNi
bboxX
j=1Ibbox(ˆbvar
i,j)
[Lrpn
cls(x′UT
i, cPL
i,j) +Lroi
cls(x′UT
i, cPL
i,j)],(13)
where cPL
i,jis the pseudo-label and Ibbox(·)is the indicator
function defined as follows:
Ibbox(ˆbvar
i,j) =(
1,if1
4P
k∈{x,y,w,h }ˆbvar
i,j,k≤γ
0,otherwise ,(14)
where γis the threshold for using pseudo-labels with vari-
ance less than a specific value.
Finally, after the student model is updated once by
Eq. (12), the teacher model is updated by using the expo-
nential moving average (EMA) [41]:
θt←αθt+ (1−α)θs, ϕ t←αϕt+ (1−α)ϕt,(15)
where αis the update ratio.
4. Experiments
4.1. Experimental Settings
4.1.1 Datasets
We evaluated our proposed method in four domain adapta-
tion scenarios using five datasets. The adaptation scenarios
from domain X to Y are represented as X →Y .
Cityscapes (C) [6] is a clear-weather scene dataset captured
using in-vehicle cameras in urban areas. It contains 2,975
training and 500 validation images, with bounding boxes
derived from instance segmentation masks.
Foggy Cityscapes (F) [36] is a pseudo-dense foggy dataset
generated from Cityscapes, maintaining the same number of
images. There are three levels of fog density (0.005, 0.01,
and 0.02). We used a split of 0.02 fog density level.
BDD100k (B) [54] is a large-scale dataset of in-vehicle
camera images and contains 100k images. We used a day-
time subset of this dataset, 36,728 training images and 5,258
validation images.
SIM10k (S) [20] is a dataset of 10,000 images synthesized
by the game engine. We used all 10,000 images in training,
including 58,071 bounding boxes.
KITTI (K) [15] is a dataset of in-vehicle cameras captured
in a different scene from Cityscapes. We used 7,481 images
for training.
28787
4.1.2 Implementation Details
Network Architecture . We used Faster-RCNN [34] as the
detection model, utilizing a BatchNorm-free VGG16 [39]
backbone pre-trained on ImageNet [8] following [3]. The
architecture of the domain discriminator followed that used
in [17].
Pre-processing . The image size was resized to 600 on the
shorter side while maintaining the aspect ratio constant. The
transformation of strong and weak augmentation proceeded
as follows [3].
Optimization . For the detection models, we used SGD
with a momentum of 0.9, a weight decay of 10−4, an ini-
tial learning rate of 0.02 with a warm-up, training for 40k
iterations, and reducing the learning rate by ten at 30k and
35k iterations. For the FNPM, we used SGD with an ini-
tial learning rate of 10−4, training for 2k iterations with a
cosine annealing scheduler before active sampling.
Training Configuration . The batch size was set to four
for each domain data. Five rounds of active sampling were
performed at 5k, 10k, 15k, 20k, and 25k iterations. The
variance threshold γwas set to 0.1, dropout rate ηto 0.1,
number of variational inferences Mto 10, EMA rate αto
0.9996, and weight of adversarial loss λto 0.01.
Evaluation Metrics . We evaluated our method with the
average precision (AP) at the intersection over union (IoU)
threshold of 0.5. In the C→FandC→Bscenarios, we
report the mean AP (mAP) over all classes.
4.2. Comparison with Domain Adaptation Methods
Table 1 shows the quantitative results of comparing our pro-
posed method with the state-of-the-art unsupervised domain
adaptation (UDA) and active domain adaptation (ADA)
methods. Row ”Source-only” and ”Oracle” display the
results of supervised learning using the source domain
dataset and fully labeled target domain dataset, respectively.
Among previous ADA methods, we reproduced AADA as it
focuses on object detection. Our proposed method outper-
formed state-of-the-art UDA methods in most cases while
requiring only 1% of the labeling budget. Particularly, in
theK→Cscenario, the performance improved by more
than 10 pt with only a 1% increase in the labeling budget.
Furthermore, with 5% of the labeling budget, our method
achieved nearly the same performance as Oracle, display-
ing an accuracy difference ratio of 98%.
For a fair comparison of our method with the UDA meth-
ods, we evaluated the model under 0% of the labeling bud-
get setting by solely using pseudo-labels with low localiza-
tion uncertainty for training without active sampling. The
evaluation results show that our method is competitive with
the state-of-the-art methods in the UDA setting. Therefore,
our proposed method can also provide a strong UDA base-
line. Our proposed method achieved the highest perfor-
mance among all the methods in the C→Bscenario.Table 1. Comparison with the state-of-the-art UDA and ADA
methods. Even with only 1% of the labeling budget (Ours (1%)),
the performance of our proposed method exceeded that of almost
all conventional methods in terms of accuracy. Furthermore, our
proposed method achieved the performance nearly equivalent to
that of Oracle while using 5% of the labeling budget (Ours (5%)).
AADA† represents an evaluation through re-implementation.
Methods C→F C →B S→C K →C
Source-only 15.5 26.8 40.3 31.3
UDADA-Faster [4] 27.6 — 39.0 38.5
SWDA [35] 34.3 — 47.7 —
PT [3] 42.7 34.9 55.1 60.2
AT [23] 50.9 — — —
MGADA [60] 44.3 — 49.8 45.2
CMT [2] 50.3 — — 64.3
CSDA [14] 45.0 — 56.9 48.6
NSA-UDA [61] 52.7 35.5 56.3 55.6
Ours (0%) 52.0 36.3 55.0 53.9
ADAAADA† [40] (1%) 37.2 33.1 57.9 47.7
AADA† (5%) 38.5 41.0 58.4 53.7
Ours (1%) 52.2 45.1 61.8 64.2
Ours (5%) 51.8 50.9 65.6 67.9
Oracle 46.3 52.3 68.9 68.9
0.1 0.3 0.5 1 3 5
Budget (%)35404550mAP (%)
Random
MeanEntropy
CoreSet
ActiveT eacher
AADA
Ours
Oracle
Figure 5. Comparison of active sampling strategies in the adap-
tation from Cityscapes to BDD100k. The horizontal axis repre-
sents the labeling budget, and the vertical axis represents mAP.
Our proposed strategy outperformed the conventional strategies
across almost all budgets, achieving a performance close to that
of Oracle with 5% of the labeling budget.
4.3. Comparison with Active Sampling Strategies
We evaluated our proposed method with different ac-
tive sampling strategies and budgets. Figure 5 shows
the performance of the active sampling strategy on bud-
gets in the C→Bscenario, comparing our method with
random sampling, MeanEntropy [43], CoreSet [37], Ac-
tiveTeacher [26], and AADA [40] as the baseline for ADA.
Our proposed strategy consistently outperformed previ-
ous strategies across almost all budgets. In particular, there
was a substantial performance gain against AADA when the
budget was less than or equal to 1%, witnessing 2.5 pt im-
provements at 0.5% of the labeling budget. The reason for
28788
Table 2. Comparison of the performances of our proposed
metric under varying active sampling strategies. The most ac-
curate number is represented in bold. In MeanEntropy [43] and
AADA [40], we underline the results with higher accuracy for the
with and without sfn.
Strategies w/sfn Budget (%)
0.1 0.3 0.5 1 3 5
Random 33.8 35.3 37.4 39.5 45.5 48.3
MeanEntropy36.2 37.1 40.5 44.7 49.2 51.1
✓ 38.4 39.3 41.7 43.9 48.4 51.0
AADA35.8 39.5 40.1 43.5 48.0 49.4
✓ 37.4 38.3 40.7 44.0 48.4 50.4
Ours ✓ 37.4 41.4 42.6 45.1 49.2 50.9
these improvements is that FN errors increased when using
the previous strategies with small budgets, as the model was
not sufficiently adapted to the target domain. Therefore, the
samples that reduce the FN errors are not selected using the
previous strategies. In contrast, our undetectability metric
effectively selects the samples that reduce FN errors. Thus,
our proposed active sampling strategy is particularly effec-
tive under low-budget conditions.
We also evaluated the effectiveness of our undetectabil-
ity metric sfn. Table 2 shows the results of a perfor-
mance comparison when sfnwas incorporated into previ-
ous strategies. In MeanEntropy, our proposed metric im-
proved accuracy when the budget was low. This result in-
dicates that a high budget is needed to sample targets with
low-class probabilities, which leads to the occurrence of FN
errors. In AADA, our proposed metric was generally effec-
tive. This result indicates that the diversity metric used in
AADA is less effective in suppressing FN errors.
4.4. Ablation Studies
4.4.1 Contribution of Components
We investigated the contribution of components within our
framework. Our framework comprises components such
as adversarial learning (AD), pseudo-labeling (PL), mean
teacher (MT), uncertainty-aware PL (UP), and strong aug-
mentation (SA). We compared the performances delivered
by the model trained with a 0.5% budget in the C→Fand
K→Cscenarios (Table 3).
SA was the most beneficial component for enhancing
performance in both scenarios within our framework. AD
and PL followed in terms of impactful contributions in the
C→Fscenario, highlighting the efficacy of feature-level
alignment across domains. MT and UP did not show any
significant differences. This result indicates that Cityscapes
and Foggy Cityscapes share the same information on ob-
ject regions; hence, a simple pseudo-label with a confidence
threshold is sufficient for prediction.
Conversely, PL, MT, and UP, in that order, contributedTable 3. Comparison of the performances of our proposed method
under varying usage conditions of the components
AD PL MT UP SA C→F K →C
✓ ✓ ✓ ✓ 38.2 63.2
✓ ✓ 40.1 53.9
✓ ✓ ✓ ✓ 51.1 54.2
✓ ✓ ✓ ✓ 50.5 60.9
✓ ✓ ✓ ✓ 30.2 49.9
✓ ✓ ✓ ✓ ✓ 51.6 63.1
Table 4. Comparison of the performances of our proposed method
under varying changing each indicator of the acquisition function
sfnslocsentsdivS→C K →C
✓ ✓ ✓ 57.4 61.8
✓ ✓ ✓ 59.2 62.4
✓ ✓ ✓ 60.6 61.2
✓ ✓ ✓ 59.5 62.8
✓ ✓ ✓ ✓ 58.4 63.1
to the K→Cscenario, indicating that the prediction ac-
curacy of pseudo-label is crucial. Implementation of UP in
our framework led to an improvement of the performance
by 3 pt owing to the effectiveness of uncertainty in improv-
ing the prediction accuracy of the pseudo-label. In contrast,
the contribution of AD was low. Pseudo-labels are more ef-
fective than feature-level alignment in this scenario owing
to the large instance-level gaps.
4.4.2 Effectiveness of False Negative Prediction
We investigated the contribution of each metric in the active
sampling strategy (Table 4). For this experiment, we used a
model trained with a 0.5% labeling budget in the S→Cand
K→Cscenarios. In the S→Cscenario, sfnprovides the
largest contribution, as evidenced by the remarkably lower
performance when sfnis omitted, and the notable perfor-
mance gap observed between scenarios with and without
sfn. In the K→Cscenario, we observed a result similar
to that in the S→Cscenario. These results show that this
pattern is effective in various scenarios.
5. Conclusion
We proposed an active domain adaptation method designed
for object detection. Through a performance analysis un-
der domain shift, we identified the challenge of undetected
objects and proposed the False Negative Prediction Module,
focusing on addressing this issue. Experimental verification
in in-vehicle camera scenarios, characterized by significant
domain gaps, demonstrated the effectiveness of our method.
Thus, we believe that our method performs well across var-
ious datasets.
28789
References
[1] Sharat Agarwal, Saket Anand, and Chetan Arora. Reducing
annotation effort by identifying and labeling contextually di-
verse classes for semantic segmentation under domain shift.
InProceedings of the IEEE/CVF Winter Conference on Ap-
plications of Computer Vision (WACV) , pages 5904–5913,
2023. 1, 2
[2] Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, and Yu-Xiong
Wang. Contrastive mean teacher for domain adaptive ob-
ject detectors. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
23839–23848, 2023. 1, 2, 7
[3] Meilin Chen, Weijie Chen, Shicai Yang, Jie Song, Xin-
chao Wang, Lei Zhang, Yunfeng Yan, Donglian Qi, Yueting
Zhuang, Di Xie, and Shiliang Pu. Learning domain adaptive
object detection with probabilistic teacher. In Proceedings
of the 39th International Conference on Machine Learning
(ICML) , pages 3040–3055, 2022. 2, 4, 7
[4] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and
Luc Van Gool. Domain adaptive faster r-cnn for object de-
tection in the wild. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
3339–3348, 2018. 1, 2, 7
[5] Jiwoong Choi, Ismail Elezi, Hyuk-Jae Lee, Clement Fara-
bet, and Jose M. Alvarez. Active learning for deep ob-
ject detection via probabilistic modeling. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 10264–10273, 2021. 1, 2
[6] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 3213–3223, 2016. 6
[7] Antoine de mathelin, Franc ¸ois Deheeger, Mathilde Mougeot,
and Nicolas Vayatis. Discrepancy-based active learning for
domain adaptation. In Proceedings of the International Con-
ference on Learning Representations (ICLR) , 2022. 1, 2
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 248–
255, 2009. 7
[9] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Unbi-
ased mean teacher for cross-domain object detection. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 4091–4101, 2021.
1, 2
[10] Ismail Elezi, Zhiding Yu, Anima Anandkumar, Laura Leal-
Taix´e, and Jose M. Alvarez. Not all labels are equal: Ratio-
nalizing the labeling costs for training object detection. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 14492–14501,
2022. 1, 2
[11] Bo Fu, Zhangjie Cao, Jianmin Wang, and Mingsheng Long.
Transferable query selection for active domain adaptation.
InProceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR) , pages 7272–7281,
2021. 1, 2
[12] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian
approximation: Representing model uncertainty in deep
learning. In Proceedings of the 33rd International Confer-
ence on Machine Learning (ICML) , pages 1050–1059, 2016.
5
[13] Yaroslav Ganin and Victor Lempitsky. Unsupervised do-
main adaptation by backpropagation. In Proceedings of
the 32nd International Conference on Machine Learning
(ICML) , pages 1180–1189, 2015. 2, 4
[14] Changlong Gao, Chengxu Liu, Yujie Dun, and Xueming
Qian. Csda: Learning category-scale joint feature for domain
adaptive object detection. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
11421–11430, 2023. 1, 2, 7
[15] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 3354–3361,
2012. 6
[16] Denis Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi,
and Sotaro Tsukizawa. Deep active learning for biased
datasets via fisher kernel self-supervision. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 9041–9049, 2020. 1
[17] Han-Kai Hsu, Chun-Han Yao, Yi-Hsuan Tsai, Wei-Chih
Hung, Hung-Yu Tseng, Maneesh Singh, and Ming-Hsuan
Yang. Progressive domain adaptation for object detection. In
Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision (WACV) , pages 749–757, 2020.
7
[18] Duojun Huang, Jichang Li, Weikai Chen, Junshi Huang,
Zhenhua Chai, and Guanbin Li. Divide and adapt: Active
domain adaptation via customized learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 7651–7660, 2023. 1, 2
[19] Sehyun Hwang, Sohyun Lee, Sungyeon Kim, Jungseul Ok,
and Suha Kwak. Combating label distribution shift for active
domain adaptation. In Proceedings of the European Confer-
ence on Computer Vision (ECCV) , pages 549–566, 2022. 1,
2
[20] Matthew Johnson-Roberson, Charles Barto, Rounak Mehta,
Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan.
Driving in the matrix: Can virtual worlds replace human-
generated annotations for real world tasks? In Proceedings
of the IEEE International Conference on Robotics and Au-
tomation (ICRA) , pages 746–753, 2017. 6
[21] Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti,
Manoj Ghuhan, Tripti Shukla, and Dinesh Manocha. Salad:
Source-free active label-agnostic domain adaptation for clas-
sification, segmentation and detection. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV) , pages 382–391, 2023. 1, 2
[22] Suraj Kothawade, Saikat Ghosh, Sumit Shekhar, Yu Xiang,
and Rishabh Iyer. Talisman: Targeted active learning for ob-
ject detection with rare classes and slices using submodular
28790
mutual information. In Proceedings of the European Con-
ference on Computer Vision (ECCV) , pages 1–16, 2022. 1,
2
[23] Yu-Jhe Li, Xiaoliang Dai, Chih-Yao Ma, Yen-Cheng Liu,
Kan Chen, Bichen Wu, Zijian He, Kris Kitani, and Peter
Vajda. Cross-domain adaptive teacher for object detection.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 7581–7590,
2022. 1, 2, 7
[24] Mengyao Lyu, Jundong Zhou, Hui Chen, Yijie Huang,
Dongdong Yu, Yaqian Li, Yandong Guo, Yuchen Guo, Liuyu
Xiang, and Guiguang Ding. Box-level active detection. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 23766–23775,
2023. 1, 2
[25] Xinhong Ma, Junyu Gao, and Changsheng Xu. Active uni-
versal domain adaptation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
8968–8977, 2021. 1, 2
[26] Peng Mi, Jianghang Lin, Yiyi Zhou, Yunhang Shen, Gen
Luo, Xiaoshuai Sun, Liujuan Cao, Rongrong Fu, Qiang Xu,
and Rongrong Ji. Active teacher for semi-supervised ob-
ject detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
14482–14491, 2022. 1, 2, 7
[27] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex
Graves, Ioannis Antonoglou, Daan Wierstra, and Martin
Riedmiller. Playing atari with deep reinforcement learning.
InProceedings of the Advances in Neural Information Pro-
cessing Systems Workshops (NeurIPSW) , 2013. 5
[28] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, An-
drei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves,
Martin Riedmiller, Andreas K. Fidjeland, Georg Ostro-
vski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control
through deep reinforcement learning. Nature , 518(7540):
529–533, 2015. 5
[29] Munan Ning, Donghuan Lu, Dong Wei, Cheng Bian,
Chenglang Yuan, Shuang Yu, Kai Ma, and Yefeng Zheng.
Multi-anchor active domain adaptation for semantic seg-
mentation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV) , pages 9112–9122,
2021. 1, 2
[30] Younghyun Park, Wonjeong Choi, Soyeong Kim, Dong-Jun
Han, and Jaekyun Moon. Active learning for object detection
with evidential deep learning and hierarchical uncertainty
aggregation. In Proceedings of the International Conference
on Learning Representations (ICLR) , 2023. 1, 2
[31] Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and
Judy Hoffman. Active domain adaptation via clustering
uncertainty-weighted embeddings. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 8505–8514, 2021. 1, 2
[32] Quazi Marufur Rahman, Niko S ¨underhauf, and Feras Day-
oub. Did you miss the sign? a false negative alarm system
for traffic sign detectors. In Proceedings of the IEEE/RSJInternational Conference on Intelligent Robots and Systems
(IROS) , pages 3748–3753, 2019. 5
[33] Harsh Rangwani, Arihant Jain, Sumukh K Aithal, and
R. Venkatesh Babu. S3vaada: Submodular subset selection
for virtual adversarial active domain adaptation. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV) , pages 7516–7525, 2021. 1, 2
[34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Proceedings of the Advances in Neural
Information Processing Systems (NeurIPS) , 2015. 7
[35] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate
Saenko. Strong-weak distribution alignment for adaptive ob-
ject detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
6956–6965, 2019. 1, 2, 7
[36] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Se-
mantic foggy scene understanding with synthetic data. Inter-
national Journal of Computer Vision (IJCV) , 126:973–992,
2018. 6
[37] Ozan Sener and Silvio Savarese. Active learning for convolu-
tional neural networks: A core-set approach. In Proceedings
of the International Conference on Learning Representations
(ICLR) , 2018. 7
[38] Inkyu Shin, Dong-Jin Kim, Jae Won Cho, Sanghyun Woo,
Kwanyong Park, and In So Kweon. Labor: Labeling only
if required for domain adaptive semantic segmentation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 8588–8598, 2021. 1, 2
[39] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In Pro-
ceedings of the International Conference on Learning Rep-
resentations (ICLR) , 2015. 7
[40] Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu,
Subhransu Maji, and Manmohan Chandraker. Active adver-
sarial domain adaptation. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision
(WACV) , pages 739–748, 2020. 1, 2, 3, 6, 7, 8
[41] Antti Tarvainen and Harri Valpola. Mean teachers are bet-
ter role models: Weight-averaged consistency targets im-
prove semi-supervised deep learning results. In Proceedings
of the Advances in Neural Information Processing Systems
(NeurIPS) , 2017. 6
[42] Huy V . V o, Oriane Sim ´eoni, Spyros Gidaris, Andrei Bursuc,
Patrick P ´erez, and Jean Ponce. Active learning strategies for
weakly-supervised object detection. In Proceedings of the
European Conference on Computer Vision (ECCV) , pages
211–230, 2022. 1, 2
[43] Dan Wang and Yi Shang. A new active labeling method for
deep learning. In Proceedings of the International Joint Con-
ference on Neural Networks (IJCNN) , pages 112–119, 2014.
6, 7, 8
[44] Fan Wang, Zhongyi Han, Zhiyan Zhang, Rundong He, and
Yilong Yin. Mhpl: Minimum happy points learning for
active source free domain adaptation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 20008–20018, 2023. 1, 2
28791
[45] Yuting Wang, Velibor Ilic, Jiatong Li, Branislav Kisa ˇcanin,
and Vladimir Pavlovic. Alwod: Active learning for weakly-
supervised object detection. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
6459–6469, 2023. 1, 2
[46] Jiaxi Wu, Jiaxin Chen, and Di Huang. Entropy-based ac-
tive learning for object detection with progressive diversity
constraint. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
9397–9406, 2022. 1, 2
[47] Tsung-Han Wu, Yi-Syuan Liou, Shao-Ji Yuan, Hsin-Ying
Lee, Tung-I Chen, Kuan-Chih Huang, and Winston H. Hsu.
D2ada: Dynamic density-aware active domain adaptation
for semantic segmentation. In Proceedings of the European
Conference on Computer Vision (ECCV) , pages 449–467,
2022. 1, 2
[48] Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, and
Xinjing Cheng. Towards fewer annotations: Active learn-
ing via region impurity and prediction uncertainty for do-
main adaptive semantic segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 8068–8078, 2022.
[49] Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, Xin-
jing Cheng, and Guoren Wang. Active learning for domain
adaptation: An energy-based approach. In Proceedings of
the AAAI Conference on Artificial Intelligence , pages 8708–
8716, 2022.
[50] Ming Xie, Yuxi Li, Yabiao Wang, Zekun Luo, Zhenye Gan,
Zhongyi Sun, Mingmin Chi, Chengjie Wang, and Pei Wang.
Learning distinctive margin toward active domain adapta-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 7993–
8002, 2022.
[51] Mixue Xie, Shuang Li, Rui Zhang, and Chi Harold Liu.
Dirichlet-based uncertainty calibration for active domain
adaptation. In Proceedings of the International Conference
on Learning Representations (ICLR) , 2023. 1, 2
[52] Qinghua Yang, Hui Chen, Zhe Chen, and Junzhe Su. Intro-
spective false negative prediction for black-box object detec-
tors in autonomous driving. Sensors , 21(8):2819, 2021. 5
[53] Donggeun Yoo and In So Kweon. Learning loss for ac-
tive learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
93–102, 2019. 1, 2
[54] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying
Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-
rell. Bdd100k: A diverse driving dataset for heterogeneous
multitask learning. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 2636–2645, 2020. 6
[55] Jinze Yu, Jiaming Liu, Xiaobao Wei, Haoyi Zhou, Yohei
Nakata, Denis Gudovskiy, Tomoyuki Okuno, Jianxin Li,
Kurt Keutzer, and Shanghang Zhang. Mttrans: Cross-
domain object detection with mean teacher transformer. In
Proceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 629–645, 2022. 1, 2
[56] Weiping Yu, Sijie Zhu, Taojiannan Yang, and Chen Chen.
Consistency-based active learning for object detection. InProceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition Workshops (CVPRW) , pages
3951–3960, 2022. 1, 2
[57] Jiakang Yuan, Bo Zhang, Xiangchao Yan, Tao Chen, Botian
Shi, Yikang Li, and Yu Qiao. Bi3d: Bi-domain active learn-
ing for cross-domain 3d object detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 15599–15608, 2023. 1, 2
[58] Tianning Yuan, Fang Wan, Mengying Fu, Jianzhuang Liu,
Songcen Xu, Xiangyang Ji, and Qixiang Ye. Multiple in-
stance active learning for object detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 5330–5339, 2021. 1, 2
[59] Hao Zhang and Ruimao Zhang. Active domain adaptation
with multi-level contrastive units for semantic segmentation.
InProceedings of the Asian Conference on Computer Vision
(ACCV) , pages 1640–1657, 2022. 1, 2
[60] Wenzhang Zhou, Dawei Du, Libo Zhang, Tiejian Luo, and
Yanjun Wu. Multi-granularity alignment domain adapta-
tion for object detection. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 9581–9590, 2022. 1, 2, 7
[61] Wenzhang Zhou, Heng Fan, Tiejian Luo, and Libo Zhang.
Unsupervised domain adaptive detection with network sta-
bility analysis. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) , pages 6986–
6995, 2023. 1, 2, 7
28792
