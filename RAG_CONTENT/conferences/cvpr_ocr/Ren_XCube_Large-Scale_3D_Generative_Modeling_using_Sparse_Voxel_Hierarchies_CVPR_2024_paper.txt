XCube: Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies
Xuanchi Ren1,2,3Jiahui Huang1Xiaohui Zeng1,2,3Ken Museth1
Sanja Fidler1,2,3Francis Williams1
1NVIDIA2University of Toronto3Vector Institute
Sparse V oxel
Resolution = 10243
Normal Semantics
Figure 1. XCube (X3).Our model generates high-resolution (up to 10243) sparse 3D voxel hierarchies of objects anddriving scenes in
under 30 seconds. The voxels are enriched with arbitrary attributes such as semantics, normals, and TSDF from which mesh could be readily
extracted. Here we show randomly sampled geometries using our model trained on ShapeNet, Objaverse, Karton City, and Waymo.
Abstract
We present XCube (abbreviated as X3), a novel genera-
tive model for high-resolution sparse 3D voxel grids with
arbitrary attributes. Our model can generate millions of
voxels with a ﬁnest effective resolution of up to 10243in a
feed-forward fashion without time-consuming test-time op-
timization. To achieve this, we employ a hierarchical voxel
latent diffusion model which generates progressively higher
resolution grids in a coarse-to-ﬁne manner using a custom
framework built on the highly efﬁcient VDB data structure.
Apart from generating high-resolution objects, we demon-
strate the effectiveness of XCube on large outdoor scenes at
scales of100m×100m with a voxel size as small as 10cm .
We observe clear qualitative and quantitative improvements
over past approaches. In addition to unconditional genera-
tion, we show that our model can be used to solve a variety
of tasks such as user-guided editing, scene completion from
a single scan, and text-to-3D. More results and details can
be found on our project webpage .
1. Introduction
Equipping machines with the ability to create and understand
three-dimensional scenes and objects has long been a tanta-
lizing pursuit, promising a bridge between the digital and
physical worlds. The problem of 3D content generation lies
at the heart of this endeavor. By modeling the distribution ofobjects and scenes, generative models can propose plausible
shapes and their attributes from scratch, from user input, or
from partial observations.
There has been a surge of new and exciting works on
generative models for 3D shapes in recent years. Initial work
in this area, train on datasets of 3D shapes and leverage 3D
priors to perform generation [ 17,21]. While these works
produce impressive results, their diversity and shape quality
is fundamentally bounded by the size of 3D datasets ( e.g.
[5,12]), as well as their underlying 3D representation. To
address the diversity problem, one line of work [ 43,65] pro-
posed an elegant solution that leverages powerful 2D genera-
tive models to produce 3D structures using inverse rendering
and a diffusion score distillation formulation. While they
beneﬁt from the massive corpora of 2D image data and can
generate highly diverse and high-quality shapes, the gener-
ated shapes usually suffer from the Janus face problem, and
the generation process requires test-time optimization that is
lengthy and computationally expensive. More recent works
such as [ 30,32] achieve state-of-the-art 3D generation by
smartly combining a mix of 2D and 3D priors. They gain
diversity from 2D data and spatial consistency from 3D data
and speed up the generation process by operating directly
in 3D. These works motivate a deeper investigation into the
fundamentals of 3D priors for generation. In particular, cur-
rent 3D priors are limited to low resolutions [ 26], and do
not scale well to large outdoor scenes such as those in au-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
4209
tonomous driving and robotics applications. These datasets
of large-scale scenes are abundant and contain more data
than those consisting solely of objects [ 56]. Thus, develop-
ing a scalable 3D generative prior has the potential to unlock
new sources of training data and further push the boundary
of what is possible in 3D generation. In this work, we aim
to explore the limits of purely 3D generative priors, scaling
them to high resolutions and large-scale scenes. Our model
is capable of scaling to high-resolution outputs (e.g. 10243)
by leveraging a novel sparse formulation and can produce
outputs with high geometric complexity, by focusing dense
geometry near the surface of a shape.
Our method, X3, is a novel hierarchical voxel latent dif-
fusion model for generating high-resolution 3D objects and
scenes with arbitrary attributes such as signed distances, nor-
mals, and semantics. Our model generates a latent Sparse
Voxel Hierarchy — a hierarchy of 3D sparse voxel grids with
latent features at each voxel — in a coarse-to-ﬁne manner.
In particular, we model each level of the hierarchy as a la-
tent diffusion model conditioned on the coarser level. The
latent space at each level is encoded using a highly efﬁcient
— both in terms of compute and memory — sparse structure
Variational Autoencoder (V AE). Our generated representa-
tion enjoys several key beneﬁts: (1) it is fully 3D, enabling
it to model intricate details at multiple resolutions, (2) it can
output very high-resolution shapes (up to 10243resolution)
by leveraging sparsity, (3) the distribution at each level is
easy to model since the coarse level need only model a rough
shape, and ﬁner levels are concerned with local details, (4)
our generated shapes support multi-scale user-guided editing
by modifying coarser levels and regenerating ﬁner levels, (5)
since our model leverages a latent diffusion model over a
hierarchy of features, we are able to decode arbitrary multi-
scale attributes ( e.g. semantics, TSDF) from those features.
We demonstrate the effectiveness of our hierarchical
voxel latent diffusion model on standard object datasets such
as Objaverse [ 12] and ShapeNet [ 5] achieving state-of-the-
art results on unconditional and conditional generation from
texts and category labels. We further demonstrate the scala-
bility of our method by demonstrating high-quality uncondi-
tional and conditional (from a single lidar scan) generation
on large outdoor scenes from the Waymo Open Dataset [ 56]
and Karton City [ 1]. Finally, by leveraging a custom sparse
3D deep learning framework built on VDB [ 36], our model
is capable of generating complex shapes at 10243resolution
containing millions of voxels in under 30 seconds.
2. Related Work
Generative Probabilistic Models. Common generative
models include variational autoencoders (V AE) [ 25], gen-
erative adversarial networks (GAN) [ 16], normalizing
ﬂows [ 46], autoregressive models (AR) [ 63], and more re-
cently diffusion models (DM) [ 18,53]. A popular methodfor generative modeling is latent diffusion that has been
found useful in, e.g., images [ 42,48,62] and videos [ 3],
where the diffusion process happens over the latent space
of a simpler generative model ( e.g. a V AE). Latent diffu-
sion models allow for easy decoding of multiple attributes
through different decoders. In our work, we employ a latent
diffusion model over a hierarchy of sparse voxels.
3D Generative Models. The recent surge in the 3D gen-
erative modeling literature mostly focuses on object-level
shape synthesis. One line of work opts for distilling 2D im-
age priors into 3D via inverse rendering [ 28,32,65], while
others [ 14,23,38,40,41,69] focus on directly learning 3D
priors from large-scale 3D datasets [ 11,12]. Recently, hy-
brid 2D-3D approaches [ 31,57] which better leverage both
image priors and large 3D datasets have started to emerge.
Fundamental to these works are good 3D priors that can
instill multiview consistency without the need for expensive
test-time optimization. This is the interest of our work.
Works that tackle large-scale scene generation either
choose a procedural approach that decouples the genera-
tion of different scene components ( e.g. roads, buildings,
etc.) [ 45,55,67], or a more generic approach that gener-
ates the entire scene at once [ 8,24,29]. While the former
approaches usually provide more details, they are limited
to generating a ﬁxed subset of possible scenes and require
specialized data to train. The latter approaches are theoret-
ically more ﬂexible but are currently bounded by their 3D
representation power (hence producing fewer details), which
is a problem we address in this work.
3D Representation for Generative Tasks. Point clouds [2,
34,37,69] are ﬂexible and adaptive, but cannot represent
solid surfaces and pose challenges in architecture design.
Triangle meshes [15,39] are more expressive but are lim-
ited to a ﬁxed topology and hence hard to optimize. Neural
ﬁelds [7,33,38] encode scene geometry implicitly in the
network weights and lack an explicit inductive bias for ef-
fective distribution modeling. Tri-planes [6,14,52] can
represent objects at high resolutions with reduced memory
footprint, but fundamentally lack a geometric bias except for
large axis-aligned planes, posing challenges when modelling
larger scenes with complex geometry. Comparably, voxel
grids [21,51,66] are ﬂexible, expressive for both chunky
and thin structures, and support fast querying and process-
ing.Sparse voxel hierarchies do not store voxel information
for empty regions and hence are more efﬁcient. A popular
approach in the literature is to implement these using oc-
tree variants [ 22,60,61,64] or hash tables [ 58]. However,
previous works either focus only on geometry [ 61,71], are
limited to an effective resolution of 2563[22], do not con-
sider hierarchical generation [ 72], or are not evaluated on
real-world datasets [ 27]. In contrast, our method can gener-
ate high-resolution shapes from a hierarchical latent space,
and is evaluated on large-scale, real-world scene datasets.
4210
Input
 Hierarchical V oxel Latent Diffusion V AE Decoder Generated Hierarchy M esh & Texture Sparse Structure V AE
Next Level …
…
Diffusion Sampling Process V AE Training
Figure 2. Method. Sparse voxel grids within the hierarchy are ﬁrst encoded into compact latent representations using a sparse structure V AE.
The hierarchical latent diffusion model then learns to generate each level of the latent representation conditioned on the coarser level in a
cascaded fashion. The generated high-resolution voxel grids contain various attributes for different applications. Note that technically X1is
a dense latent grid, but illustrated as a sparse one for clarity.
3. Method
Our goal is to learn a generative model of large-scale 3D
scenes represented as sparse voxel hierarchies. The hierarchy
comprises of Llevels of coarse-to-ﬁne voxel grids G=
{G1,...,GL}and their associated per-voxel attributes A=
{A1,...,AL}such as normals and semantics. Finer grids
Gl+1with smaller voxel sizes are strictly contained within
the coarser grids Glforl= 1,...,L−1, and the ﬁnest level
of gridGLcontains the maximum amount of details.
Our method trains a hierarchy of latent diffusion mod-
els over the sparse voxel grids Gencoded by a hierarchy of
sparse structure V AEs, as summarized in Fig. 2. We ﬁrst
introduce the sparse structure variational autoencoder (V AE)
that learns a compact latent representation of voxel grids
in §3.1. Then we describe our full diffusion probabilistic
model that learns the joint distribution of the latent represen-
tation and the sparse voxel hierarchy in § 3.2. The training
and sampling procedures are described in § 3.3, followed by
the implementation details in § 3.4.
3.1. Sparse Structure V AE
Motivation. The sparse structure V AE is designed to learn
a compact latent representation of each voxel grid within the
hierarchy and its associated attributes. Instead of directly
modeling their joint distribution that comprises a mixture of
continuous and discrete random variables, we encode them
into a uniﬁed continuous latent representation, which is not
only friendly to the downstream diffusion models during
training and sampling [ 62,69], but also facilitates the formu-
lation of a hierarchical probabilistic model which we aim to
demonstrate. Additionally, the latent representation, encoded
in a coarser spatial resolution, serves as a compact yet mean-
ingful proxy that saves the computation while preserving the
expressivity [ 17,48]. Represented as X={X1,...,XL},
the latent is also a featurized sparse voxel hierarchy corre-Subdivide Prune Subdivide Prune…
Figure 3. V AE Decoder Architecture. Coarser levels of grids
Glare upsampled to ﬁner grids Gl+1by iteratively subdividing
existing voxels into octants and pruning excessive ones. Each level
may contain many upsampling layers that double the resolution.
sponding to a coarser version of G, with the voxel size of Xl
being the same as Gl−1.
Network Design. We choose to train separate V AEs that op-
erate on each level lof the hierarchy independently. Hence
for the ease of notation, we drop the subscript lin the fol-
lowing discussion. Here, we build the neural networks based
on the operators over sparse voxel grids to model both the
posterior distribution qφ(X|G,A)and the likelihood dis-
tribution pψ(G,A|X), withφ,ψbeing the encoder and
decoder weights respectively.
For the encoder, we utilize the sparse convolutional neural
network to process the input GandAby alternatively apply-
ing sparse convolution and max pooling operations, down-
sampling to the resolution of X. For the decoder, we borrow
the structure prediction backbone from [ 20] that allows us
to predict novel sparse voxels that are not present in the in-
put. It starts from Xand proceeds by progressively pruning
excessive voxels and subdividing existing ones based on the
prediction of a subdivision mask, and ﬁnally reaching the res-
olution of Gafter several upsampling layers. An illustration
of the above decoding scheme is shown in Fig. 3.
3.2. Hierarchical Voxel Latent Diffusion
Probabilistic Modeling. Existing 3D generation litera-
ture [ 17,38] typically uses one level of latent diffusion ( i.e.
L= 1). While this is sufﬁcient to generate intricate scenes
containing one single object, the resolution is still far from
4211
enough to generate large-scale outdoor scenes. The limited
scalability of their underlying 3D representation and the ab-
sence of probabilistic modeling to capture the coarse-to-ﬁne
nature of the data hinder the effectiveness of these methods.
We solve this by marrying a hierarchical latent diffusion
model [ 3,19] with the sparse voxel representation. Specif-
ically, we propose the following factorization of the joint
distribution of grids and latents:
p(G,A,X) =L/productdisplay
l=1pψl(Gl,Al|Xl)pθl(Xl|Cl−1),(1)
whereCl−1is the condition from the coarser level, with:
Cl=/braceleftBigg
c, l = 0
{Gl,Al,c}, l >0, (2)
withcbeing an optional global condition such as a category
label or a text prompt, and pθl(·)instantiated as a diffusion
model with parameter θlwhich we elaborate on later.
The above factorization assumes the Markov process ( i.e.
levellis only conditioned on its coarser level l−1), which
is naturally induced from the geometric nature of the data.
By doing so, we reduce the layers in each level of V AE
and amortize both the computation and the representation
power across multiple levels (see § 4.4for empirical proof).
Additionally, such a factorized modeling endows us with
utmost ﬂexibility, enabling user controls by editing or re-
sampling grids from different levels.
Diffusion Model pθ.Here we omit subscript lagain for
clarity. A diffusion stochastic process transforms a compli-
cated distribution of a random variable into the unit Gaussian
distribution X0∼ N(0,I)by iteratively adding white noise
to it, following a Markov process [ 18]. One commonly used
instantiation is the following:
Xt|Xt−1∼ N(/radicalbig
1−βtXt−1,βtI), (3)
where0< βt≪1controls the amount of noise added for
each step. The reverse process, on the other hand, removes
the noise iteratively and reaches data distribution XTwithin
a discrete number of steps T. It is usually modeled as:
Xt−1|Xt∼ N(µθ(Xt,t),1−¯αt−1
1−¯αtβtI), (4)
withαt= 1−βt,¯αt=/producttextt
s=0αs, andµθa parametrized
learnable module. In practice, we re-parametrize µθas:
µθ=√αtXt−βt/radicalbigg¯αt−1
1−¯αtvθ, (5)
so that the learnable module predicts vinstead. This is in
accordance with the v-parameterization in [ 49] that has been
shown to facilitate training.We instantiate vθ(·)as a 3D sparse variant of the back-
bone used in [ 13], ensuring the grid structure of the de-
coded output from vθmatches the input. To inject condition
Cl−1, we directly concatenate the feature from Al−1with
the network input recalling that Xlalso shares the same grid
structure with Gl−1. Timestep condition is implemented
using AdaGN [ 48] and textual condition cis ﬁrst CLIP-
encoded [ 44] and then injected using cross attention.
3.3. Training and Sampling
Loss Functions. We train the V AE and the diffusion model
level-by-level independently. During the training of the level-
lV AE, we employ the following loss function:
LV AE
l=E{Gl,Al}[EXl∼qφ[BCE(Gl,˜Gl)+
LAttr
l(Al,˜Al)]+λKL(qφ(Xl)∥p(Xl))],(6)
where˜Gl,˜Alis the output of the V AE decoder ψgiven
Xl, andLAttr
lis the loss supervising the attribute predictions
(e.g., TSDF, semantics, etc.) with its speciﬁc form postponed
to the supplementary material. BCE(·)is the binary cross
entropy on the grid structure, making pψa mixed product
distribution. KL(· ∥ ·)is the KL divergence between the
posterior and the prior p(Xl), which we set to unit Gaussian
N(0,I), andλis its weight.
The training loss for the diffusion model is:
LDM
l=Et,Xl,ε∼N(0,I)/bracketleftbig
∥vθl(Xl,t,t)−vref∥2
2/bracketrightbig
,(7)
wherevref=√¯αtε−√1−¯αtXl,t∼[1,T],Xlis sampled
from the V AE posterior, and Xl,t=√¯αtXl+√1−¯αtε.
Sampling. To sample from the joint distribution of Eq(1),
one starts by drawing the coarest latent X1from the diffu-
sion model pθ1. Then, the decoder pψ1is used to generate
the coarsest grid G1and its associated attributes A1(which
is then optionally reﬁned by the reﬁnement network). Con-
ditioned on C1={G1,A1,c}, the diffusion model pθ2is
used to generate the next level of latent X2, and the process
goes on until the highest resolution of {GL,AL}is met. We
include TSDF in ALfor all our experiments, which enables
us to decode high-resolution meshes. For other tasks such
as perception, we further allow for decoding other attributes
such as semantics. We use DDIM [ 54] as our sampler.
3.4. Implementation Details
In practice, we ﬁnd the following implementation details,
specially tuned for the sparse voxel hierarchy generation
case, to be helpful for better results: (1) Early dilation . In
network layers with larger voxel sizes, we dilate the sparse
voxel grids by one, so that the halo regions of the sparse
topology also represent non-zero features. This helps later
layers to better capture the local context and generate smooth
structures. (2) Reﬁnement network . An inherent problem
4212
LION NFD NWD ( Ours)
Figure 4. ShapeNet [ 5] Qualitative Comparison. We show comparison of our method with LION [ 69], NFD [ 52], and NWD [ 21]. Our
method is capable of generating intricate geometry and thin structures. Best viewed with 200% zoom-in.
of our factorized modeling is error accumulation, where
higher-resolution grids cannot easily ﬁx the artifacts from
prior layers. We mitigate this by appending a reﬁnement
network to the output of the V AE decoder that reﬁnes Gland
Al. The architecture of the reﬁnement network is similar to
[20], and its training data is augmented by adding noise to
the posterior of the V AE [ 19] before being decoded. Last but
not least, our architecture and training details can be found
in the supplementary.
Sparse 3D Learning Framework. The use of sparse voxel
grids motivates and enables us to build a custom 3D deep
learning framework for sparse data in order to support higher
spatial resolution and faster sampling. To this end, we lever-
age the VDB [ 36] structure to store our sparse 3D voxel grid.
Thanks to its compact representation (taking only 11MB for
3.4 million of voxels) and fast look-up routine, we are able
to implement common neural operators such as convolution
and pooling in a very efﬁcient manner. Fully operating on the
GPU (including grid building), our framework is able to pro-
cess a 3D scene with 10243in milliseconds, runs ∼3×faster
with∼0.5×of the memory usage than the current state-of-
the-art sparse 3D learning framework TorchSparse [ 58]. All
our architectures are based on this custom framework.
4. Experiments
We conduct comprehensive experiments to evaluate the per-
formance of our model. First, we demonstrate XCube’s
ability to perform unconditional object-level 3D generation
using ShapeNet [ 5] (§4.1), and conditional 3D generation
from category and text using Objaverse [ 12] (§4.2). Next,
we showcase high-resolution outdoor scene-level 3D genera-
tion using both the Karton City [ 1] and Waymo [ 56] datasets
(§4.3), which is one of the ﬁrst results of this kind. Finally,
we conduct ablation studies for our design choices (§ 4.4).
Please refer to the supplementary for further results.Airplane Chair Car
CD EMD CD EMD CD EMD
Point-based method
PVD [ 73] 69.55 60.89 57.68 54.95 64.89 54.61
LION [ 69] 65.10 60.15 56.72 54.28 60.61 54.94
Triplane-based method
NFD [ 52] 57.55 53.47 54.87 54.06 69.49 71.96
Dense voxel-based method
NWD [ 21] 59.78 53.84 56.35 57.98 61.75 58.54
LAS-Diffusion [ 72] 71.29 56.93 55.17 55.02 75.03 72.10
3DShape2VecSet [ 70] 62.75 61.01 54.06 56.79 86.85 80.91
Ours 52.85 49.75 53.99 48.60 57.96 54.43
Table 1. 1-NNA Comparison on ShapeNet [ 5].The lower the
better. Best scores highlighted in bold.
4.1. Object­level 3D Generation on ShapeNet
Dataset. To benchmark XCube against prior methods, we
use the widely-adopted ShapeNet [ 5] dataset. Following
the experimental setup in [ 34,52,68,69], we choose three
speciﬁc categories: Airplane ,Car andChair , containing
4145, 7496, 6778 shapes respectively. To build the ground-
truth voxel hierarchy for training, we voxelize each mesh at
a5123resolution and use the train/val/test split from [ 10].
Evaluation. To evaluate the geometric quality of our syn-
thesized output, we follow previous work [ 21,68,69] and
use1-NNA as our main metric (with both Chamfer distance
(CD) and earth mover’s distance (EMD)). 1-NNA provides
a comprehensive measure of both quality and diversity by
measuring the distributional similarity between the gener-
ated shapes and the validation set [ 68,69]. Please refer to
the supplementary for more details and evaluations.
Baselines. We compare XCube to state-of-the-art 3D gen-
erative models that leverage various latent and shape rep-
resentations: PVD [73],LION [69],NFD [ 52],NWD [21],
LAS-Diffusion [72], and 3DShape2VecSet [70]. PVD and
LION employ point clouds as both latent and output-shape
4213
Figure 5. Close-up View of Our Generated Shape. The voxel
grid is colored by predicted normal. XCube is able to generate a
high level of detail, such as the car interior and airplane propellers.
representations. NFD, NWD, and 3DShape2VecSet all use
mesh as the output shape representation, but with different
latent representations, i.e., triplanes, dense voxel grids, and
unstructured latent vectors, respectively. Although LAS-
Diffusion also uses sparse voxels similar to ours, it does not
allow for generating arbitrary attributes and scale only to
1283resolutions. In contrast, our method uses a sparse voxel
hierarchy as a latent representation and outputs a sparse
voxel grid, which can be readily converted to a mesh.
Results. Tab. 1provides a quantitative comparison of
XCube against baseline approaches and shows the supe-
riority of our approach over past work. The point-based
methods are naturally restricted to generating coarse shapes
(i.e., 2048 points), while our method is able to generate mil-
lions of voxels ( 500×larger). The triplane-based method
(NFD) exhibits decent performance in categories such as
Airplane and Chair with simple geometry. However, its ef-
fectiveness diminishes for the Car category with intricate
geometry (such as the suspension), underscoring the chal-
lenges to generate complex geometric structures using such
representations. 3DShape2VecSet suffers from a similar
issue where its latent representation compresses the infor-
mation too aggressively. While NWD and LAS-Diffusion
are based on voxel grids, they are foundamentally limited
by their low grid resolutions. The inverse wavelet transform
in NWD is also lossy. In contrast, our method is able to
generate high-resolution shapes with ﬁne details, as shown
inFig.5.Fig.4shows a qualitative comparison, which is
consistent with our quantitative results.
User-guided Editing. Our method is based on a sparse
voxel hierarchy, which is a natural representation for user-
guided editing. We demonstrate this ability by allowing
users to edit the coarse-level shapes by adding or removing
voxels in Goxel [ 9], a Minecraft-style 3D voxel editor. Fig.6
shows several examples of user-guided editing.
4.2. Object­level 3D Generation on Objaverse
Dataset. To further demonstrate XCube’s ability to per-
form object-level 3D generation, we evaluate it using the
Figure 6. User-guided Editing. By adding ( green ) and deleting
(red) coarse-level voxels, one can easily control the ﬁner 3D shape.
Objaverse [ 12] dataset, which offers approximately 800K
publicly available 3D models. For the text-to-3D experi-
ment, we use the descriptive text automatically generated by
Cap3D [ 35]. For category-conditional generation, we adopt
the LVIS categorized object subset of [ 12], containing ∼40K
3D objects. We voxelize the 3D objects at a 5123resolution
to build the ground truth voxel hierarchy for training.
Evaluation. We conduct a user study on Amazon Mturk
to compare our method with Shap ·E [23], a state-of-the-art
text-to-3D method. Speciﬁcally, we gather 30prompts from
popular text-to-3D works [ 23,40,41,59]. For each prompt,
we ask30users to pick the 3D object (ours v.s. Shap ·E
w/o texture) that better matches the text prompt and exhibits
higher geometric ﬁdelity. In total, we collect 900pairwise
comparisons. In 79.2%of the comparisons, participants vote
for our generated 3D objects.
Results. We provide qualitative results for text-to-3D in
Fig.7, and category-conditional generation in Fig.9. The
texture of our results is generated by off-the-shelf texture
synthesis methods [ 4,47]. We also compare our method
with Shap ·E [23] inFig.8. Our whole pipeline including
generating the geometry and the texture for one object takes
about 1 minute (30s for objects and 30s for textures). We
observe that our method is able to generate more diverse 3D
objects with higher geometric ﬁdelity and ﬁner details than
Shap·E, as shown in Fig. 9.
4.3. Large­scale Scene­level 3D Generation
Dataset. To demonstrate our model’s scalability and abil-
ity to generate large-scale high-resolution scenes, we train
and evaluate it on the Waymo Open Dataset [ 56] which
contains1000 LiDAR scan sequences capturing different
driving scenarios. Here, we extract the ground-truth dense
scene geometry by accumulating the LiDAR scans and prop-
agating the semantic labels. To construct the ground-truth
sparse voxel hierarchy, we crop each of the extracted scenes
to102.4m×102.4mchunks and voxelize the points and
meshes at a resolution of 10243, resulting in a voxel size of
4214
“A 3D model 
of pineapple”“A 3D model 
of squirrel”
“A 3D model 
of skull”
“An 
eagle head”
Figure 7. Text-to-3D Results on Objaverse [ 12].For each sample we show the input text prompt, the generated sparse voxel grid colored
by normal, the extracted mesh, and the textured mesh (using [ 47]).
“A pair of shorts” “A cactus”(Ours)
“A strong muscular man”Shap!E
Figure 8. Comparison with Shap ·E [23]. We can generate high-
quality shapes that better match the given prompts.
10cm . To demonstrate the superiority of our representation
power, we train a model on Karton City [ 1], a custom syn-
thetic dataset of 20 blocks of synthetic city scenes using the
same resolution settings as Waymo. We divide the 20 blocks
into train/val splits and randomly crop 102.4m×102.4m
chunks in each split to generate 900/100 unique train/val
scenes. Fig.12shows examples from the Karton City model
and we include more results in the supplementary.
Evaluation. To evaluate the quality of our generated results,
we perform a user study using Amazon Mturk. Here we
show 30 users 30 pairs of scenes (totaling 900 pairwise
comparisons) where one scene is sampled from the validation
set and the other is a random output generated by our model.
We ask each user to rank which scene is more realistic. Out
of the 900 comparisons, 66.3%favors our results over the
ground truth, demonstrating that our generated outputs are of
high quality. Fig.10shows several unconditional generations
from our model as well as decoded semantics and normals.
Single-Scan Conditioning. We demonstrate that our model
<Dog>“A chair that looks like a root”
<Lizard>
Figure 9. Diversity of Our Generated Shapes. XCube can gener-
ate diverse shapes under the same text prompt or category label.
can be used to perform conditional generation on large-scale
scenes. In this qualitative experiment, we condition the
model on a single input LiDAR scan and generate a complete
scene with normals and semantics. Fig.11shows several
completions using our method. Note that our input does
not contain semantics, yet our model is able to generate
plausible geometric and semantic completion results. The
supplementary shows additional details and ﬁgures.
4.4. Ablation Study
Progressive Pruning. We replace the progressive prun-
ing part of our pipeline with a single pruning step for the
163→1283V AE on ShapeNet Chairs. We observe that the
4215
Curb & Lane Marker Vehicles Road Sidewalk Vegetation & Trees Building Sign & Pole
Figure 10. Unconditional Samples on the Waymo Open Dataset [ 56].The dashed boxes show a zoomed-in view and the solid boxes show
the normal map for the extracted mesh. Best viewed with 200% zoom-in.
Figure 11. Single-scan-conditioned Generation. The left column
shows the input LiDAR scan and the right column shows our gen-
erated semantic mesh conditioned on the input.
reconstruction accuracy (grid IoU) drops from 92.88% to
89.68%, indicating that progressive pruning is critical for
preserving shape details and injecting 3D inductive bias. Fur-
thermore, the GPU memory usage also increases by a factor
of3×when removing progressive pruning.
Hierarchy Conﬁguration. As shown in Tab. 2, we com-
pare the performance of our model with different hierarchy
conﬁgurations on ShapeNet Chairs. We observe that: (1)
the hierarchical model outperforms the single-level model,
emphasizing the importance of a sparse voxel hierarchy in
3D generative modeling. (2) the model’s performance is
robust to the resolution of the initial hierarchy level. We ﬁnd
163is sufﬁcient for capturing the overall shape of the object.
(3) using two-level and three-level models achieve compa-Model CD (%) EMD (%)
163→512359.31 57.46
163→1283→512353.99 48.60
323→1283→512355.39 51.40
43→163→1283→512352.88 53.62
Table 2. Ablation of Different Resolutions and Depths of the
Hierarchy. Metrics are in 1-NNA. The lower the better.
Figure 12. Unconditional Samples on Karton City [ 1].
rable performance. For unconditional generation, we use a
two-level model for fast sampling. And for the user-editing
setting, we use a three-level model for easier editing.
5. Discussion
Conclusion. We presented X3, a novel generative model for
large-scale 3D scenes represented as a hierarchy of sparse
3D voxel grids. We proposed a hierarchical voxel latent
diffusion model that learns the joint distribution of the latent
representation and the sparse voxel hierarchy. The effective-
ness of our method was demonstrated on both object-level
and scene-level generation, reﬂecting our method’s capabil-
ity of generating high-resolution 3D scenes with ﬁne details.
Limitations and Future Work. Due to current 3D datasets
being still not on par with image datasets (such as [ 50]) , our
text-to-3D model is hard to deal with complex prompts. In
the future, we plan to extend our method in the setting of
image-conditioning, as well as leveraging the learned prior
as a fundamental model to support more downstream tasks.
4216
References
[1]3d karton city model. https://www.turbosquid.
com/3d-models/3d-karton-city-2-model-
1196110 , 2023. Accessed: 2023-08-01. 2,5,7,8
[2]Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and
Leonidas J. Guibas. Learning representations and generative
models for 3d point clouds. In International Conference on
Machine Learning (ICML) , pages 40–49, 2018. 2
[3]Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
22563–22575, 2023. 2,4
[4]Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp,
and Kangxue Yin. Texfusion: Synthesizing 3d textures with
text-guided image diffusion models. In Proceedings of the
IEEE International Conference on Computer Vision (ICCV) ,
2023. 6
[5]Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat
Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis
Savva, Shuran Song, Hao Su, et al. Shapenet: An information-
rich 3d model repository. arXiv preprint arXiv:1512.03012 ,
2015. 1,2,5
[6]Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance ﬁelds. In European
Conference on Computer Vision (ECCV) , pages 333–350,
2022. 2
[7]Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for
generative shape modeling. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR) ,
pages 5939–5948, 2019. 2
[8]Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Scene-
dreamer: Unbounded 3d scene generation from 2d image
collections. arXiv preprint arXiv:2302.01330 , 2023. 2
[9]Guillaume Chereau. Goxel: 3d voxel editor, 2023. Accessed:
2023-11-16. 6
[10] Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin
Chen, and Silvio Savarese. 3d-r2n2: A uniﬁed approach for
single and multi-view 3d object reconstruction. In European
Conference on Computer Vision (ECCV) , pages 628–644,
2016. 5
[11] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong
Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-
tian Laforte, Vikram V oleti, Samir Yitzhak Gadre, et al.
Objaverse-xl: A universe of 10m+ 3d objects. arXiv preprint
arXiv:2307.05663 , 2023. 2
[12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani,
Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe
of annotated 3d objects. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 13142–13153, 2023. 1,2,5,6,7
[13] Prafulla Dhariwal and Alexander Nichol. Diffusion mod-
els beat gans on image synthesis. In Advances in Neural
Information Processing Systems , pages 8780–8794, 2021. 4[14] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,
Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja
Fidler. Get3d: A generative model of high quality 3d tex-
tured shapes learned from images. In Advances in Neural
Information Processing Systems , 2022. 2
[15] Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-
Kun Lai, and Hao Zhang. SDM-NET: deep generative net-
work for structured deformable mesh. ACM Transactions on
Graphics (TOG) , 38(6):243:1–243:15, 2019. 2
[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems , 27, 2014. 2
[17] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and
Barlas O ˘guz. 3dgen: Triplane latent diffusion for textured
mesh generation. arXiv preprint arXiv:2303.05371 , 2023. 1,
3
[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In Advances in Neural Information
Processing Systems , 2020. 2,4
[19] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high ﬁdelity image generation. Journal of Machine
Learning Research , 2022. 4,5
[20] Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, Sanja
Fidler, and Francis Williams. Neural kernel surface recon-
struction. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 4369–
4379, 2023. 3,5
[21] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neu-
ral wavelet-domain diffusion for 3d shape generation. In
SIGGRAPH Asia , 2022. 1,2,5
[22] Moritz Ibing, Gregor Kobsik, and Leif Kobbelt. Octree trans-
former: Autoregressive 3d shape generation on hierarchically
structured sequences. arXiv preprint arXiv:2111.12480 , 2021.
2
[23] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional
3d implicit functions. arXiv preprint arXiv:2305.02463 , 2023.
2,6,7
[24] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten
Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio
Torralba, and Sanja Fidler. Neuralﬁeld-ldm: Scene generation
with hierarchical latent diffusion models. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 8496–8506, 2023. 2
[25] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. In ICLR , 2014. 2
[26] Jumin Lee, Woobin Im, Sebin Lee, and Sung-Eui Yoon. Diffu-
sion probabilistic models for scene-scale 3d categorical data.
arXiv preprint arXiv:2301.00527 , 2023. 1
[27] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-
sdf: Text-to-shape via voxelized diffusion. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 12642–12651, 2023. 2
[28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-
Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-
3d content creation. In Proceedings of the IEEE Conference
4217
on Computer Vision and Pattern Recognition (CVPR) , pages
300–309, 2023. 2
[29] Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei
Chai, Aliaksandr Siarohin, Ming-Hsuan Yang, and Sergey
Tulyakov. Inﬁnicity: Inﬁnite-scale city synthesis. arXiv
preprint arXiv:2301.09637 , 2023. 2
[30] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang,
Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan
Gu, and Hao Su. One-2-3-45++: Fast single image to 3d ob-
jects with consistent multi-view generation and 3d diffusion.
arXiv preprint arXiv:2311.07885 , 2023. 1
[31] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang
Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh
in 45 seconds without per-shape optimization. arXiv preprint
arXiv:2306.16928 , 2023. 2
[32] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,
Sergey Zakharov, and Carl V ondrick. Zero-1-to-3: Zero-shot
one image to 3d object. arXiv preprint arXiv:2303.11328 ,
2023. 1,2
[33] Andrew Luo, Tianqin Li, Wen-Hao Zhang, and Tai Sing Lee.
Surfgen: Adversarial 3d shape synthesis with explicit surface
discriminators. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV) , pages 16218–16228,
2021. 2
[34] Shitong Luo and Wei Hu. Diffusion probabilistic models for
3d point cloud generation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR) ,
pages 2837–2845, 2021. 2,5
[35] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-
son. Scalable 3d captioning with pretrained models. In
Advances in Neural Information Processing Systems , 2023. 6
[36] Ken Museth. VDB: high-resolution sparse volumes with
dynamic topology. ACM Transactions on Graphics (TOG) ,
32(3):27:1–27:22, 2013. 2,5
[37] George Kiyohiro Nakayama, Mikaela Angelina Uy, Jiahui
Huang, Shi-Min Hu, Ke Li, and Leonidas Guibas. Difffacto:
Controllable part-based 3d point cloud generation with cross
diffusion. In Proceedings of the IEEE International Confer-
ence on Computer Vision (ICCV) , 2023. 2
[38] Gimin Nam, Mariem Khliﬁ, Andrew Rodriguez, Alberto
Tono, Linqi Zhou, and Paul Guerrero. 3d-ldm: Neural im-
plicit 3d shape generation with latent diffusion models. arXiv
preprint arXiv:2212.00842 , 2022. 2,3
[39] Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Peter W.
Battaglia. Polygen: An autoregressive generative model of 3d
meshes. In International Conference on Machine Learning
(ICML) , pages 7220–7229, 2020. 2
[40] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751 , 2022. 2,6
[41] Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski,
Chaoyang Wang, Luc Van Gool, and Sergey Tulyakov. Au-
todecoding latent 3d diffusion models. In Advances in Neural
Information Processing Systems , 2023. 2,6
[42] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,
Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.Sdxl: improving latent diffusion models for high-resolution
image synthesis. arXiv preprint arXiv:2307.01952 , 2023. 2
[43] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.
Dreamfusion: Text-to-3d using 2d diffusion. In ICLR , 2023.
1
[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision.
InInternational Conference on Machine Learning (ICML) ,
pages 8748–8763, 2021. 4
[45] Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei,
Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen,
Beining Han, Yihan Wang, Alejandro Newell, Hei Law, Ankit
Goyal, Kaiyu Yang, and Jia Deng. Inﬁnite photorealistic
worlds using procedural generation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , pages 12630–12641, 2023. 2
[46] Danilo Rezende and Shakir Mohamed. Variational inference
with normalizing ﬂows. In International Conference on Ma-
chine Learning (ICML) , pages 1530–1538, 2015. 2
[47] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,
and Daniel Cohen-Or. Texture: Text-guided texturing of 3d
shapes. In SIGGRAPH , 2023. 6,7
[48] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , pages 10674–10685, 2022. 2,3,4
[49] Tim Salimans and Jonathan Ho. Progressive distillation for
fast sampling of diffusion models. In ICLR , 2022. 4
[50] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,
Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick
Schramowski, Srivatsa Kundurthy, Katherine Crowson, Lud-
wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-
5B: an open large-scale dataset for training next generation
image-text models. In Advances in Neural Information Pro-
cessing Systems , 2022. 8
[51] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar
Averbuch-Elor. V ox-e: Text-guided voxel editing of 3d ob-
jects. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV) , 2023. 2
[52] J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,
Jiajun Wu, and Gordon Wetzstein. 3d neural ﬁeld generation
using triplane diffusion. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 20875–20886, 2023. 2,5
[53] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning (ICML) , pages 2256–2265, 2015.
2
[54] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising
diffusion implicit models. In ICLR , 2021. 4
[55] Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zis-
han Qin, and Stephen Gould. 3d-gpt: Procedural 3d modeling
4218
with large language models. arXiv preprint arXiv:2310.12945 ,
2023. 2
[56] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,
Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han,
Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Et-
tinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang,
Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov.
Scalability in perception for autonomous driving: Waymo
open dataset. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
2443–2451, 2020. 2,5,6,8
[57] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea
Vedaldi. Viewset diffusion:(0-) image-conditioned 3d genera-
tive models from 2d data. arXiv preprint arXiv:2306.07881 ,
2023. 2
[58] Haotian Tang, Zhijian Liu, Xiuyu Li, Yujun Lin, and Song
Han. Torchsparse: Efﬁcient point cloud inference engine.
MLSys , 2022. 2,5
[59] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang
Zeng. Dreamgaussian: Generative gaussian splatting for efﬁ-
cient 3d content creation. arXiv preprint arXiv:2309.16653 ,
2023. 6
[60] Jia-Heng Tang, Weikai Chen, Jie Yang, Bo Wang, Songrun
Liu, Bo Yang, and Lin Gao. Octﬁeld: Hierarchical implicit
functions for 3d modeling. arXiv preprint arXiv:2111.01067 ,
2021. 2
[61] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.
Octree generating networks: Efﬁcient convolutional architec-
tures for high-resolution 3d outputs. In Proceedings of the
IEEE International Conference on Computer Vision (ICCV) ,
pages 2107–2115, 2017. 2
[62] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based
generative modeling in latent space. In Advances in Neural
Information Processing Systems , pages 11287–11302, 2021.
2,3
[63] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt,
Oriol Vinyals, Alex Graves, et al. Conditional image genera-
tion with pixelcnn decoders. In Advances in Neural Informa-
tion Processing Systems , pages 4790–4798, 2016. 2
[64] Peng-Shuai Wang, Yang Liu, and Xin Tong. Dual octree
graph networks for learning adaptive volumetric shape rep-
resentations. ACM Transactions on Graphics (TOG) , 41(4):
1–15, 2022. 2
[65] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. Proliﬁcdreamer: High-ﬁdelity and
diverse text-to-3d generation with variational score distilla-
tion. arXiv preprint arXiv:2305.16213 , 2023. 1,2
[66] Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Free-
man, and Joshua B. Tenenbaum. Learning a probabilistic
latent space of object shapes via 3d generative-adversarial
modeling. In Advances in Neural Information Processing
Systems , pages 82–90, 2016. 2
[67] Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu.
Citydreamer: Compositional generative model of unbounded
3d cities. arXiv preprint arXiv:2309.00610 , 2023. 2
[68] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu,
Serge J. Belongie, and Bharath Hariharan. Pointﬂow: 3dpoint cloud generation with continuous normalizing ﬂows. In
Proceedings of the IEEE International Conference on Com-
puter Vision (ICCV) , pages 4540–4549, 2019. 5
[69] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,
Or Litany, Sanja Fidler, and Karsten Kreis. LION: latent
point diffusion models for 3d shape generation. In Advances
in Neural Information Processing Systems , 2022. 2,3,5
[70] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter
Wonka. 3dshape2vecset: A 3d shape representation for neural
ﬁelds and generative diffusion models. ACM Transactions on
Graphics (TOG) , 42(4):92:1–92:16, 2023. 5
[71] Dongsu Zhang, Changwoon Choi, Jeonghwan Kim, and
Young Min Kim. Learning to generate 3d shapes with genera-
tive cellular automata. In ICLR , 2021. 2
[72] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong,
Yang Liu, and Heung-Yeung Shum. Locally attentional SDF
diffusion for controllable 3d shape generation. ACM Transac-
tions on Graphics (TOG) , 42(4):91:1–91:13, 2023. 2,5
[73] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation
and completion through point-voxel diffusion. In Proceedings
of the IEEE International Conference on Computer Vision
(ICCV) , pages 5806–5815, 2021. 5
4219
