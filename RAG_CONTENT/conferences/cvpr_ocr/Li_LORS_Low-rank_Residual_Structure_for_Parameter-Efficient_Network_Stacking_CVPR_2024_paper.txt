LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking
Jialin Li, Qiang Nie, Weifu Fu, Yuhuan Lin, Guangpin Tao, Yong Liu, Chengjie Wang
Youtu Lab, Tencent
{jarenli, ryanwfu, gleelin, guangpintao, choasliu, jasoncjwang }@tencent.com, qnie.cuhk@gmail.com
Abstract
Deep learning models, particularly those based on
transformers, often employ numerous stacked structures,
which possess identical architectures and perform similar
functions. While effective, this stacking paradigm leads to
a substantial increase in the number of parameters, pos-
ing challenges for practical applications. In today’s land-
scape of increasingly large models, stacking depth can even
reach dozens, further exacerbating this issue. To miti-
gate this problem, we introduce LORS (LOw-rank Residual
Structure). LORS allows stacked modules to share the
majority of parameters, requiring a much smaller num-
ber of unique ones per module to match or even surpass
the performance of using entirely distinct ones, thereby
significantly reducing parameter usage. We validate our
method by applying it to the stacked decoders of a query-
based object detector, and conduct extensive experiments
on the widely used MS COCO dataset. Experimental re-
sults demonstrate the effectiveness of our method, as even
with a 70% reduction in the parameters of the decoder, our
method still enables the model to achieve comparable or
even better performance than its original.
1. Introduction
In the current era of prosperity for large models, a common
issue is the significant increase in the number of parame-
ters, which presents challenges for training, inference, and
deployment. Various methods have been proposed to re-
duce the number of parameters in models, such as knowl-
edge distillation [15, 20], which compresses large models
into smaller ones while trying to preserve their performance
but may still lead to a decrease in model capacity; prun-
ing [16, 55], which removes redundant parameters from the
model but can affect the model’s stability; quantization [8],
which reduces the numerical precision of model parameters
to lower storage and computation but may cause model ac-
curacy loss; and parameter sharing [24], which reduces the
number of parameters by sharing them across different lay-
ers but may limit the model’s expressiveness.
Figure 1. The LORS calculation process, which could be adaptive
or static, depending on whether an adaptively generated kernel is
used in the matrix manipulation for private parameters.
Different from the aforementioned methods, we have ob-
served an important fact contributing to the large number of
parameters: the widespread use of stacking in neural net-
works. Stacking refers to those modules that have identi-
cal architectures and perform the same or similar functions,
but possess different parameters due to random initializa-
tion as well as training updates. Examples of stacking can
be found in numerous prominent neural networks, like the
classic ResNet model [18] and Transformers [46]. Partic-
ularly, Transformers heavily rely on stacked structures and
typically employ completely identical multi-layer stacks in
both encoders and decoders. It now serves as an indispens-
able component of many excellent models in fields such as
computer vision and natural language processing.
Although stacking is powerful for enhancing model ca-
pacity, as demonstrated by large language models, it also
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
15866
naturally leads to a sharp increase in the number of param-
eters. For example, GPT-3 [2] utilizes 175 billion parame-
ters and consists of 96 layers of stacked Transformer [46]
layers. How can we enjoy the benefits of stacking while
reducing the required number of parameters? We notice
that stacked decoders have identical structures and similar
functions, indicating that there should be some common-
ality among their parameters. However, since they handle
different input and output distributions, there must also be
unique aspects to their parameters. Therefore, a natural idea
is: it may be possible to represent the shared aspects with
shared parameters while allowing each stacked module to
retain only the parameters that capture its unique character-
istics, thereby reducing the overall parameter usage.
Based on the above considerations, we propose to de-
compose the parameters of stacked modules into two parts:
shared ones representing the commonality and private ones
capturing the specific characteristics. Shared parameters
are available for all modules and trained jointly by them,
while the private parameters are separately owned by each
module. We suppose that the way of sharing parameters
related to the commonality may reduce the number of pa-
rameters while maintaining the performance of the model.
To achieve this goal, we introduce the concept of Low-
rank Residual Structure (LORS), inspired by the approach
of LoRA [21]. LORS essentially adds the unique param-
eters to the shared ones, just like residual connections add
residual information to the features. While LoRA is origi-
nally designed for fine-tuning, we train our LoRA-like op-
eration on parameters from scratch. This approach allows
us to effectively reduce the overall parameter usage while
maintaining the performance of the model, paving the way
for more parameter-efficient network stacking.
To validate our idea, we choose AdaMixer [12], a strong
query-based object detector, as our experimental subject. It
contains a large number of both adaptive and static parame-
ters in its stacked decoders, making it an ideal candidate to
demonstrate LORS’s effectiveness. The difference between
adaptive and static parameters lies in whether they change
with different inputs. Our goal is to show that LORS can
effectively reduce the overall usage of both types of param-
eters while maintaining the model’s performance. We con-
ducted extensive experiments on this detector, illustrating
that our method succeeded in reducing up to 70% of param-
eters in AdaMixer’s decoders, while still enabling the model
to achieve comparable or even superior performance com-
pared to its vanilla version. In summary, our contributions
can be concluded as :
• We propose a novel low-rank residual structure, named
LORS, for network stacking, which can greatly reduce
the number of parameters while maintaining or even im-
proving the performance compared to the vanilla one.
• We introduce effective methodologies for reducing bothstatic and adaptive generated parameters in stacked struc-
tures, which makes our proposed LORS a more versatile
method.
• Our method holds the potential to serve as one of the ba-
sic network structures for large models that are affected
by the issue of excessive parameters due to stacking, ren-
dering them more parameter-efficient, thereby facilitating
an easier implementation in practical applications.
2. Related Work
Models with stacked structures. Many nerual networks
partially or extensively employ stacked modules. CNN-
based models [40], for instance, are widely applied across
various computer vision tasks, such as classification [18,
22, 39, 49], detection [19, 25, 36–38, 43], segmenta-
tion [7, 13, 32, 54], etc. These models often use stacked
smaller modules within larger components. Another kind of
models primarily utilize Multi-Layer Perceptrons (MLPs),
such as the MLP-Mixer [44], whose main body comprises
dozens of completely identical stacked mixer layers. More-
over, since its invention, Transformer [46] has been success-
fully applied to diverse domains, including computer vi-
sion [3, 30, 41, 56], natural language processing [2, 10, 50],
multi-modal learning [6, 27, 35, 47], etc. In the thriv-
ing arena of large language models, Transformer-based
structures are indeed indispensable for many cutting-edge
works [10, 26, 34, 42, 48]. Transformers are usually used
to form stacked multi-layers encoders or decoders, within
each layer being structurally identical.
LoRA and Its Variants. LoRA [21] is a technique pro-
posed for fine-tuning large language models, it introduces
low-rank decomposition matrices in the Transformer [46]
architecture, significantly reducing trainable parameters and
GPU memory requirements. Afterward, a series of works
with modifications as well as improvement have been pro-
posed. AdaLoRA [53] adaptively allocates parameter bud-
gets among weight matrices based on their importance
scores and uses singular value decomposition for incre-
mental updates, leading to an improved fine-tuning perfor-
mance, especially in low budget settings. DyLoRA [45]
introduces a dynamic low-rank adaptation technique that
trains LoRA blocks for a range of ranks instead of a sin-
gle rank, addressing the issues of fixed block size and
rank optimization in LoRA. Delta-LoRA [57] presents a
parameter-efficient approach to fine-tune large language
models by updating not only the low-rank matrices but also
the pre-trained weights using the delta of the product of two
low-rank matrices. LoRA-FA [52] introduces a memory-
efficient fine-tuning method that reduces activation memory
without performance degradation or expensive recomputa-
tion, it achieves this by freezing the projection-down weight
and updating the projection-up weight in each LoRA layer.
ReLoRA [28] is a parameter-efficient method for training
15867
large neural networks using low-rank updates. GLoRA [4],
or Generalized LoRA, is an advanced approach for uni-
versal parameter-efficient fine-tuning tasks, which enhances
the low rank adaptation technique by employing a general-
ized prompt module to optimize pre-trained model weights
and adjust intermediate activations. VeRA [23], or Vector-
based Random Matrix Adaptation, further compresses the
training parameters compared to the vanilla LoRA [21]. It
achieves this by utilizing shared low-rank matrices across
all layers and learning small scaling vectors.
Inspired by the aforementioned LoRA series works, we
propose our method called LORS (Low Rank Residual
Structure) for parameter efficiency. However, our approach
is fundamentally distinct from the LoRA series works.
While LoRA uses different low-rank weights to fine-tune
different pretrained weights, LORS adds different low-rank
weights per layer to the same common weights shared by
all layers, thus creating parameter-efficient stacked models
trained from scratch, requiring no pretrained weights.
3. Approach
In this section, we initially revisit the foundational works
upon which our methodology and experiments are primar-
ily based, encompassing the LoRA mechanism [21], query-
based object detection [3], and the structure of AdaMixer’s
decoders [12]. Then we mathematically formulate our pro-
posed LORS, which comprises two parts: the adaptive one
and the static one. Subsequently, we elaborate on how
the LORS method is applied to reduce the parameters of
AdaMixer’s decoders. Lastly, we provide a quantitative
analysis of the parameter reduction percentage attained by
our proposed method.
3.1. Preliminary
The mechanism of LoRA. The Low-Rank Adaptation
(LoRA) [21] technique is a novel approach designed to
adapt large pre-trained language models for specific tasks.
The key idea of LoRA is to introduce a low-rank parame-
ter matrix which is able to captures task-specific knowledge
while the original pre-trained parameters remain fixed.
Mathematically, given a pre-trained parameter matrix
W∈Rd×h, LoRA uses a low-rank matrix B∈Rd×rand a
projection matrix A∈Rr×hto adapt W, where r≪d, h.
The adapted parameter matrix is then given by
W+ ∆W=W+BA (1)
where BAcaptures the task-specific knowledge.
The key advantage of LoRA is that it can significantly
reduce the number of parameters that need to be fine-tuned,
thereby reducing the computational cost and lowering down
the memory requirement. In some cases, even single-digit
values of rare sufficient to fine-tune the model to the de-
sired state, which is often tens of times less expensive thantraining the parameters in W. Furthermore, by keeping the
original parameters fixed, LoRA avoids catastrophic forget-
ting, a common issue in fine-tuning large models.
Query-based object detection. In the realm of object
detection, query-based detectors have established a new
paradigm [11, 12, 30, 41, 51, 56]. Unlike traditional detec-
tors which rely on anchor boxes or sliding windows, query-
based models utilize a set of learnable queries to interact
with image feature maps. This interaction can be formal-
ized with attention [46] operations as
Qupdated =Attention (Q, K (V)) (2)
where Q,K, andVrepresent the queries, keys, and values.
The learnable queries Qare used to predict object classes
and bounding boxes ultimately, while KandVtypically
originate from encoded image features. It is a common
practice to continuously refine Qthrough interactions with
KandVusing successive decoding layers. These layers
are usually composed of structurally identical decoders.
Decoders of AdaMixer. AdaMixer [12] is a query-based
detector that features an innovative decoder design. This de-
sign includes Adaptive Channel Mixing (ACM) and Adap-
tive Spatial Mixing (ASM) methods, which greatly enhance
its performance [12].
Given a sampled feature x∈RPin×C, where C=
dfeat/g, andgdenotes the number of sampling groups. This
sampled feature is obtained through an operation called
group sampling. This operation divides the feature space
channel dfeatintoggroups and performs individual sam-
pling for each group. Then, ACM (Adaptive Channel Mix-
ing) utilizes a weight adapted by the object query qto trans-
form feature xin the channel dimension, enhancing channel
semantics [12]:
Mc= Linear( q)∈RC×C(3)
ACM( x) = ReLU(LayerNorm( xMc)) (4)
where LayerNorm stands for Layer Normalization [1].
Next is the ASM (Adaptive Spatial Mixing) process,
which aims to enable the adaptability of the object query
qto spatial structures of sampled features [12] by applying
adaptive transformation to the spatial dimension:
Ms= Linear( q)∈RPin×Pout(5)
ASM( x) = ReLU(LayerNorm( xTMs)), (6)
Both ACM and ASM train independent parameters for
each sampling group, and finally the output with the shape
Rg×C×Poutis flattened and transformed to the dqdimension
by a linear layer Loutput to add back to the object query.
ACM, ASM and the output linear transformation Loutput
possess significantly more parameters compared to the de-
coder’s other operations, making them the main contribu-
tors to the number of parameters. Hence, we choose them
15868
as the target components for validating the effectiveness of
our LORS method in parameter reduction.
def non dynamic fused weight(W shared, A, B):
# Wshared: (1, d, h)
# A: (G, r, h)
# B: (G, d, r)
# private weight: (G, d, h)
Wprivate = B @ A
# sum at group dimension to get fused weight: (d, h)
W = concat([W shared, W private], dim=0).sum(axis=0)
return W
Figure 2. Pseudo-code for obtaining a static weight parameter for
one layer.
3.2. Formulation of Our Method
The complete computational process of LORS is illustrated
in Figure 1. Notably, LORS computation is divided into two
types: adaptive and static. In the context of AdaMixer, the
term ”adaptive” indicates whether the transformation matrix
depends on the object query. After completing the formal-
ization of LORS here, we will further elaborate on how it is
applied to the tasks in our setting.
Static Low Rank Residual Structure (LORST). Suppose
there are Nstacked layers of modules with identical archi-
tecture, and Wi∈Rd×hbe a parameter matrix belonging
to the i-th layer. Then, we have:
Wi=Wshared+Wprivate
i (7)
Here, Wshared∈Rd×hrepresents the shared parameters
across all stacked layers, while Wprivate
i denotes the layer-
specific parameters for the i-th layer, which is calculated as
follows:
Wprivate
i =KX
k=1BikAik (8)
Bik∈Rd×r,Aik∈Rr×h, and r≪d, h.Krepresents
the number of parameter groups used to compute Wprivate
i .
Pseudo-code of LORSTcomputing Wprivate
i for a specific
layerican be seen in Figure 2.
Adaptive Low Rank Residual Structure (LORSA). Let
ˆWi∈Rd×hbe an adaptive generated parameter in i-th
stacked layer, it is similarly calculated as :
ˆWi=ˆWshared+ˆWprivate
i (9)
where the cross-layer shared parameter ˆWshared∈Rd×hand
the layer-specific parameter ˆWprivate
i∈Rd×hfor each layerdef dynamic fused weight(q, A, B):
# q: (N, C)
# A: (1, G, r, h)
# B: (1, G, d, r)
# shared weight: (N, 1, d, h)
Wshared = linear1(pro feats).reshape(N, 1, d, h)
# E: (N, G, r, r)
E = linear2(pro feats).reshape(N, G, r, r)
# private weight: (N, G, d, h)
Wprivate = B @ E @ A
# sum at group dimension to get fused weight: (N, d, h)
W = concat([W shared, W private], dim=1).sum(axis=1)
return W
Figure 3. Pseudo-code for obtaining an adaptive weight parameter
for one layer.
are both calculated based on q:
ˆWshared= Linear( q)∈Rd×h(10)
ˆWprivate
i =KX
k=1ˆBikˆEikˆAik (11)
ˆEik= Linear( q)∈Rr×r(12)
where ˆBik∈Rd×randˆAik∈Rr×h,r≪d, h. Pseudo-
code of LORSAcalculating ˆWprivate
i for a specific layer ican
be seen in Figure 3.
3.3. Applying LORS to AdaMixer’s Decoders
We apply LORS to the parameters of linear transformations
belonging to ACM, ASM, and Loutput in each of AdaMixer’s
decoders. These parameters are explained in 3.1.
The overall pipeline of LORS running in AdaMixer is
illustrated in Figure 4. Specifically, for each group of sam-
pling points, LORSA(see 3.2) is used to reduce parameters
inMc(fromRdqtoRC×C) of ACM and Msof ASM (from
RdqtoRPin×Pout), and LORST(see 3.2) to minimize param-
eters in Loutput (fromRC×PouttoRdq).
Thus, the parameter quantities of Mc,Ms, andLoutput are
dq×C×C,dq×Pin×Poutanddq×C×Poutrespec-
tively. When the sampling strategy consists of 2 groups with
64 points each, which is the default setting in our experi-
ments, the values of the variables are as follows: dq= 256 ,
C= 64 ,Pin= 64 , and Pout= 128 . It can be easily
calculated that the number of parameters for each of Mc,
Ms, and Loutput exceeds one million. Indeed, these three
components collectively account for the most of the total
parameter in the AdaMixer model with ResNet-50 as the
backbone, whereas they are also the primary drivers of en-
hanced model performance. This is what motivates us to
conduct LORS experiments on them. The effects of LORS
applied to them will be discussed in detail in 3.4.
15869
Figure 4. The overall pipeline of our proposed LORS, consist-
ing of both adaptive and static parts, each further composed of
shared and private components, works collaboratively. The fig-
ure illustrates the entire computation process within one layer of
the stacked layers, with an enlarged example of the i-th layer for
demonstration.
3.4. Analysis on Parameter Reduction
LetW∈Rd×hbe a weight parameter that exists in every
layer of stacked structures, Nis the number of stacked lay-
ers. If static, it originally has d×hparameters, while using
LORSTrequires1
N×d×h+K×(d×r+r×h)parame-
ters on average per layer; if it is adaptive, generating it by q
with linear transformation requires dq×d×hparameters,
where dqis the dimension of q, and using LORSArequires
1
N×dq×d×h+K×(dq×r2+d×r+r×h)pa-
rameters on average per layer. To more intuitively display
the parameter reduction effect of LORS, we set dq= 256 ,
d= 64 ,h= 128 ,K= 2 for the ASM process and
d= 2×128×128,h= 256 ,K= 1 for the Loutput ,
which is actually the case of AdaMixer’s decoders in our
experiment, and show the parameter reduction for different
rvalues using LORSTand LORSAin Table 1.
r=4 r=8 r=16 r=32
W/ . LORST1.53M 1.66M 1.93M 2.46M
W/O. LORST8.39M 8.39M 8.39M 8.39M
Percentage 18.3% 19.8% 23.0% 29.3%
W/ . LORSA0.36M 0.39M 0.49M 0.89M
W/O. LORSA2.10M 2.10M 2.10M 2.10M
Percentage 17.2% 18.6% 23.3% 42.4%
Table 1. Analysis of the parameter reduction effect of LORSTand
LORSAwith varying rank rvalues. Our default selections are
colored gray. Here the average amount of shared parameters over
each layer has already been taken into account.4. Experiments
In this section, we first explain the implementation details
of our experiments, including the dataset, training specifics,
and different parameter initialization methods for static and
adaptive LORS. Next, we present the main experimental
results. Finally, we perform ablation studies to examine
LORS’s design and its individual components.
4.1. Implementation Details
Dataset. Our experiments were conducted on the widely-
used MS COCO [29] dataset in mmdetection codebase [5],
using the standard metrics for object detection. All mod-
els were trained on the train2017 split (∼118k images) and
then evaluated on the val2017 split (5k images).
Training. We use the AdamW optimizer [33] with a
weight decay of 0.0001 and train all models on 8 Nvidia
V100 GPUs with a batch size of 16 and a learning rate of
2.5×e−5. Models are trained for either 12 or 36 epochs,
with learning rate decreasing by a factor of 10 at epochs 8
and 11 for 12-epoch training, and at epochs 24 and 33 for
36-epoch training. The low-rank values are set as r= 16
for LORSAandr= 8 for LORST. The number of param-
eter groups is set as as K= [1,1,2,2,3,3]for LORSAin
all experiments, applied to ACM and ASM in AdaMixer’s
decoders, and K= [1,1,1,1,1,1]forLoutput in all ex-
periments. We divide the feature channels into 2 groups
with 64 sampling points each instead of AdaMixer’s de-
fault 4 groups with 32 sampling points each, aiming to in-
crease the parameter compressible space for LORS, which
does not improve performance according to both AdaMixer
paper [12] and our experiments. Backbones are initial-
ized with ImageNet-1k [9] pre-trained models, and LORS-
related parameter initialization is detailed below, remaining
parameters are Xavier-initialized [14]. Finally, all other as-
pects about model training, like the data augmentation, the
loss function, etc., just follow AdaMixer’s settings [12].
Initialization Strategies We tried various initialization
methods for each component in LORS and determined the
overall initialization method as follows:
•LORST: For static LORS, we employ Kaiming initial-
ization [17] for Wsharedand each B, and zero initializa-
tion for each A.
•LORSA: For adaptive LORS, we apply Kaiming ini-
tialization [17] to the linear transformation weights form-
ing each ˆWshared, as well as each ˆBandˆA. Addition-
ally, we use zero initialization for the linear transforma-
tion weights forming each ˆE.
4.2. Main Results
To reliably demonstrate that LORS can reduce the number
of parameters in stacked structures while maintaining model
performance, which is measured by AP metric, we follow
15870
Method Queries EpochsDecoder
Params(M)Params(M)Decoder
GFLOPsGFLOPsTraining
HoursAP AP 50AP75APsAPmAPl
AdaMixer 100 12 110 135 12 104 8.5h 42.7 61.5 45.9 24.7 45.4 59.2
AdaMixer + LORS 100 12 35 60 18 110 9.5h 42.6 61.4 46.0 25.0 45.6 58.5
AdaMixer 300 12 113 139 39 132 10h 44.1 63.4 47.4 27.0 46.9 59.5
AdaMixer + LORS 300 12 35 60 56 149 12h 44.1 63.0 47.7 27.8 47.0 59.5
Table 2. 1× training scheme performance on COCO 2017 val set with ResNet-50 as backbone. AdaMixer with LORS can achieve
competitive results while employing a notably reduced number of parameters. FPS is obtained with a single Nvidia V100 GPU.
Method backbone Queries Epochs Params(M) GFLOPs AP AP 50AP75APsAPmAPl
AdaMixer R-50 100 36 135 104 43.2 61.8 46.7 25.0 46.1 58.8
AdaMixer + LORS R-50 100 36 60 110 43.7 62.3 47.3 25.5 46.4 60.0
AdaMixer R-50 300 36 139 132 47.0 66.0 51.1 30.1 50.2 61.8
AdaMixer + LORS R-50 300 36 60 149 47.6 66.6 52.0 31.1 50.2 62.5
AdaMixer R-101 300 36 158 208 48.0 67.0 52.4 30.0 51.2 63.7
AdaMixer + LORS R-101 300 36 79 225 48.2 67.5 52.6 31.7 51.3 63.8
AdaMixer Swin-S 300 36 164 234 51.3 71.2 55.7 34.2 54.6 67.3
AdaMixer + LORS Swin-S 300 36 85 250 51.8 71.6 56.4 35.4 55.0 68.4
Table 3. 3× training scheme performance on COCO 2017 val set, considering different combinations of backbone and query numbers.
Longer training time allows LORS to be fully trained and perform better. The comprehensive improvement in performance and the
significant reduction in parameters demonstrate the effectiveness of LORS. The best results in each category are highlighted in bold.
common practices in the object detection field and present
experimental comparisons in two categories based on train-
ing schemes. The first involves training for 12 epochs using
a weaker data augmentation, and the second involves train-
ing for 36 epochs using a stronger data augmentation, both
of which are identical to those in AdaMixer [12].
Table 2 presents the comparison of performance with and
without the LORS technique, under a 1× training scheme.
Metrics such as the number of parameters, GFLOPs, and
Average Precision (AP) at various scales are evaluated.
When the AdaMixer model is combined with LORS, it
demonstrates a remarkable reduction in the number of pa-
rameters. This reduction is consistently observed across
various query quantities and different training epochs, en-
compassing both the decoder and overall model. For ex-
ample, when trained with 100 queries for 12 epochs, the
AdaMixer+LORS model uses only 35M decoder param-
eters and 60M total parameters, compared to 110M and
135M respectively in the AdaMixer model. This shows re-
duction of approximately 70% in the decoder’s parameters,
which is substantial. However, the AdaMixer+LORS model
shows a slight increase in GFLOPs. In terms of model per-
formance, the AdaMixer+LORS model achieves competi-
tive results, even with a significantly reduced number of
parameters. When trained with 100 queries, although the
AP of 42.6 is slightly lower than the 42.7 AP of the vanilla
AdaMixer, it still slightly outperforms in APsandAPm.
When trained with 300 queries, the use of LORS does notaffect the performance and allows an advantage in all APs,
APmandAPl. These findings suggest that even under con-
ditions of limited model training and data augmentation,
LORS does not impact training result, the model can still
converge quickly to its original limit. Additionally, we also
demonstrate the slight impact of the current LORS method
on inference speed, which might be improved by reducing
the serial and redundant computations within LORS but not
the focus of this paper.
Table 3 showcases the remarkable performance of the
AdaMixer + LORS method under the 3× training scheme
with different backbones and query numbers. It can be ob-
served that the proposed method consistently outperforms
the vanilla AdaMixer across all backbones, query numbers,
and evaluation metrics. This result is somewhat surprising
to us, as the LORS enables the model to use significantly
fewer parameters during both training and inference.
Specifically, when adopting the ResNet-50 backbone and
100 queries, LORS improves the AP value of AdaMixer by
0.5 points (43.7 vs 43.2). It is worth noting that in Table
2, for experiments with the same configuration except for
training 12 epochs, the AP with LORS is 0.1 lower than
without LORS. However, this situation is reversed when
training for 36 epochs. The longer training time under the
3× training scheme allows LORS to be fully trained and
perform better. This emphasizes the importance of suffi-
cient training time for the LORS technique to fully exploit
its potential in enhancing the performance. Furthermore,
15871
when using more powerful backbones and a larger num-
ber of queries, the performance metrics of models utilizing
LORS consistently and comprehensively surpass those of
their counterparts without LORS. Even when employing a
strong backbone such as Swin-S [31] and using 300 queries,
LORS still manages to further improve the model’s perfor-
mance while reducing neary half of the overall parameters.
Much fewer parameters, yet improved performance.
In conclusion, the results in Table 2 and Table 3 show
the effectiveness of the LORS method in reducing the num-
ber of parameters while achieving even better performance.
The method’s adaptability to various backbones and query
numbers further demonstrates its versatility and potential
for broader applications. As for why LORS can achieve bet-
ter performance with fewer parameters, a plausible explana-
tion is that after the model has been sufficiently trained and
stabilized, the parameter matrix within the stacked structure
is inherently sparse and low-rank. Without using LORS,
the model needs to learn such a low-rank structure through
training. In contrast, LORS directly and explicitly makes
the structure sparse, which, to some extent, can be consid-
ered as adding a form of regularization to the model train-
ing, thereby facilitating the model’s learning.
4.3. Ablation Study
To save computational resources, all ablation studies are
conducted using a ResNet-50 backbone and a 1× training
scheme.
LORSALORST Decoder
Params(M)AP
110 42.5†
✓ 79 42.6
✓ 66 42.6
✓ ✓ 35 42.6
Table 4. Effect of LORSAand LORST. ”†” denotes this AP re-
sult was reproduced by ourselves. Both LORSTand LORSAcan
reduce parameters without compromising performance.
Adaptive & Static LORS. We first conduct ablation stud-
ies on the impact of LORSAand LORSTon model param-
eters and performance, as shown in Table 4. In this ta-
ble, a✓indicates that we use the LORS method to replace
the corresponding component in the AdaMixer model men-
tioned earlier, while a blank cell means we use the origi-
nal AdaMixer module. If neither of the two is present, the
model is identical to the original AdaMixer. As can be seen
from the table, both adaptive and static LORS can effec-
tively reduce the parameter size of the decoder in the model
(31M and 44M, respectively) without affecting the model
performance. Note that the symbol † here represents that
the 42.5 AP value is reproduced by us, which is slightly dif-ferent from the 42.7 AP recorded in the original paper. We
believe this discrepancy can be attributed to the randomness
of 1x training and does not affect our conclusions.
Shared and private weights. The second point of inter-
est is to determine which of the shared weight and private
weight has a greater impact on the decoder’s performance.
Experiment in Table 5 is designed to address this question,
which is performed on ACM and ASM of AdaMixer’s de-
coder. Recall that the formula we use to generate the final
weight parameters is W=Wshared+Wprivate. In the ta-
ble, a✓indicates that the corresponding term is present
on the right side of the formula, otherwise, it is removed.
It can be seen that the absence of either term will cause a
nonnegligible decrease in model performance, and the im-
pact of missing Wprivateis even more severe. This suggests
that although the number of parameters in each Wprivateis
smaller than that of Wshared, it could be more important,
possibly because the most informative part of each layer in
the stacked structure is contained within it.
WsharedWprivate Decoder
Params(M)AP
✓ 58 40.9
✓ 43 41.4
✓ ✓ 35 42.6
Table 5. Effect of shared parameters Wsharedand private parame-
tersWprivateDefault choice for our model is colored gray.
Hyperparameters of LORSA.Another aspect we would
like to explore is the optimal number of parameter groups
and the value of rank rfor adaptive LORS. Here, we feel
it is necessary to explain why we use grouped parameters.
Recalling formula 11, the adaptive private parameters are
obtained by continuous multiplication of three matrices, so
the matrix with the smallest rank among them determines
the rank of multiplication, which is the upper limit for the
private parameters. If the task-specific requirement for the
rank of the private parameters matrix unfortunately exceeds
the rank of the smallest component, there will be a bottle-
neck when training the model. Therefore, we designed this
parameter group mechanism, using the sum of the results of
multiple matrix multiplication to break the aforementioned
problem. Another advantage is that the rank of the final ma-
trix possibly increases linearly with the number of parame-
ters, while simply using a larger rwould lead to a quadratic
increase in the number of parameters. In the end, look-
ing at Table 6, we observed that the experimental result for
the 6-layer decoder of the AdaMixer object detector is rela-
tively better when using [1,1,2,2,3,3] groups of parameters
per layer and r= 16 for each group. This slightly strange
parameter group numbers is inspired by AdaLoRA [53]. It
suggests that higher-level neural network parameters may
15872
need to allocate more ranks when performing LoRA fine-
tuning, which we suppose may also hold true for LORS,
and the experiment results confirmed that.
Parameter
GroupsRank rDecoder
Params(M)AP
1 1 1 1 1 1 8 32 41.8
1 1 1 1 1 1 16 34 42.1
2 2 2 2 2 2 16 35 42.4
1 1 2 2 3 3 16 35 42.6
1 1 1 1 1 1 32 38 41.2
Table 6. Validation accuracy with different groups of parameters
and rank rin LORSA. Default choice for our model is colored
gray. Each row’s first six numbers indicate the number of parame-
ter groups sequentially used in each of the six decoder layers.
Hyperparameters of LORST.We further investigate the
optimal configuration settings for LORSTto achieve sat-
isfactory performance, as shown in Table 7. We observed
that the best option is to use one group of low-rank param-
eters with r= 8for the decomposition of each layer Loutput
in the stacked decoder of AdaMixer, which can achieve
a model performance comparable to the original detector.
Since using one group of parameters already yields reason-
ably good results, we did not explore the use of more param-
eter groups. Moreover, we should also note that the value
of rank ris not necessarily better when larger or smaller,
which may indicate that the low-rank structure required by
the model for a specific task has its own particularity.
Parameter
GroupsRank rDecoder
Params(M)AP
1 1 1 1 1 1 2 34 42.3
1 1 1 1 1 1 4 34 41.9
1 1 1 1 1 1 8 35 42.6
1 1 1 1 1 1 16 37 42.1
1 1 1 1 1 1 32 40 41.7
Table 7. Validation accuracy with different rank rin LORST. De-
fault choice for our model is colored gray. Each row’s first six
numbers indicate the number of parameter groups sequentially
used in each of the six decoder layers.
Number of decoders. Finally, we conclude our ablation
studies with experiments on the optimal number of decoder
layers incorporating the LORS structure for better perfor-
mance. Since our method significantly reduces the total
number of model parameters and allows each decoder layer
to train and use shared parameters that capture common fea-
tures, it is natural to consider whether the model’s inher-
ent characteristics are affected and whether using more or
fewer decoding layers would further improve the model’s
performance. With this idea in mind, we conducted sev-eral experiments as shown in Table 8 to explore this issue.
However, similar to the original AdaMixer without LORS,
6 layers still yield the best performance for the AdaMixer
with LORS, having too many or too few layers would de-
grade the model’s performance. This may also suggest that
LORS merely captures the intrinsic structure of the model
without altering its original properties.
Decoder
NumberDecoder
Params(M)AP
3 28 38.9
6 35 42.6
9 42 42.0
12 50 41.0
Table 8. Model performance using different numbers of stacked
decoders when employing LORS. The default choice for our
model is colored gray. This experiment with varying numbers of
decoders demonstrates that six layers still yield the best perfor-
mance for the model, which is not changed by LORS.
5. Conclusion
This research introduced a novel approach to the reduction
of parameters within deep learning models utilizing stacked
structures, i.e., the Low-rank Residual Structure (LORS).
It is potentially an effective methodology in reducing both
static and adaptive parameters. Broadly speaking, LORS
allows the parameters of stacked modules to be largely
shared, while maintaining only a small number of unique
parameters for each module, thereby significantly reducing
the total number of parameters whilst not compromising
performance. We validated our method via object detec-
tion task across various extensive experimental cases on the
widely-used COCO dataset, and the results demonstrates a
superior model performance even with a 50% – 70% reduc-
tion in decoder parameters, surpassing our expectations.
There are some noticeable limitations in our study.
While effective in reducing model parameters without com-
promising performance, enhancing model capabilities re-
quires a comparatively long training process for the method.
Additionally, our method slightly increases the inference
time, as even without a detailed analysis, it can be roughly
seen that LORS has unparalleled and repetitive computa-
tions needing more optimization. Finally, we have only
tested our approach on object detection tasks, specifically
with the AdaMixer model and its decoder structure. It’s
clear that our method can be applied to more tasks (e.g.,
NLP), different models (e.g., language models), and other
neural network components (e.g., backbones). These as-
pects will be the focus of our future work.
15873
References
[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016. 3
[2] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. arXiv preprint
arXiv:2005.14165 , 2020. 2
[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part I 16 , pages 213–229.
Springer, 2020. 2, 3
[4] Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and
Zhiqiang Shen. One-for-all: Generalized lora for parameter-
efficient fine-tuning, 2023. 3
[5] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu
Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,
Jiarui Xu, et al. Mmdetection: Open mmlab detection tool-
box and benchmark. arXiv preprint arXiv:1906.07155 , 2019.
5
[6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
Feng Zhu, and Rui Zhao. Shikra: Unleashing multi-
modal llm’s referential dialogue magic. arXiv preprint
arXiv:2306.15195 , 2023. 2
[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
Proceedings of the European conference on computer vision
(ECCV) , pages 801–818, 2018. 2
[8] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
David. Binaryconnect: Training deep neural networks with
binary weights during propagations. Advances in neural in-
formation processing systems , 28, 2015. 1
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 5
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 2
[11] Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai,
and Hongsheng Li. Fast convergence of detr with spatially
modulated co-attention. In Proceedings of the IEEE/CVF
international conference on computer vision , pages 3621–
3630, 2021. 3
[12] Ziteng Gao, Limin Wang, Bing Han, and Sheng Guo.
Adamixer: A fast-converging query-based object detector.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 5364–5373, 2022. 2,
3, 5, 6
[13] Yanqi Ge, Qiang Nie, Ye Huang, Yong Liu, Chengjie Wang,
Feng Zheng, Wen Li, and Lixin Duan. Beyond proto-types: Semantic anchor regularization for better representa-
tion learning. arXiv preprint arXiv:2312.11872 , 2023. 2
[14] Xavier Glorot and Yoshua Bengio. Understanding the diffi-
culty of training deep feedforward neural networks. In Pro-
ceedings of the thirteenth international conference on artifi-
cial intelligence and statistics , pages 249–256. JMLR Work-
shop and Conference Proceedings, 2010. 5
[15] Jianping Gou, Baosheng Yu, Stephen J Maybank, and
Dacheng Tao. Knowledge distillation: A survey. Interna-
tional Journal of Computer Vision , 129:1789–1819, 2021. 1
[16] Song Han, Jeff Pool, John Tran, and William J Dally. Deep
compression: Compressing deep neural networks with prun-
ing, trained quantization and huffman coding. In Interna-
tional Conference on Learning Representations , 2016. 1
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectifiers: Surpassing human-level perfor-
mance on imagenet classification. In Proceedings of the
IEEE international conference on computer vision , pages
1026–1034, 2015. 5
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 1, 2
[19] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE international
conference on computer vision , pages 2961–2969, 2017. 2
[20] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 1
[21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 2, 3
[22] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 4700–4708, 2017. 2
[23] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus
Asano. Vera: Vector-based random matrix adaptation, 2023.
3
[24] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin
Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite
bert for self-supervised learning of language representations.
arXiv preprint arXiv:1909.11942 , 2019. 1
[25] Hei Law and Jia Deng. Cornernet: Detecting objects as
paired keypoints. In Proceedings of the European confer-
ence on computer vision (ECCV) , pages 734–750, 2018. 2
[26] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-
jad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and
Luke Zettlemoyer. Bart: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and
comprehension. arXiv preprint arXiv:1910.13461 , 2019. 2
[27] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-
fied vision-language understanding and generation. In In-
ternational Conference on Machine Learning , pages 12888–
12900. PMLR, 2022. 2
15874
[28] Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira,
and Anna Rumshisky. Stack more layers differently: High-
rank training through low-rank updates, 2023. 2
[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 5
[30] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi,
Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic
anchor boxes are better queries for detr. arXiv preprint
arXiv:2201.12329 , 2022. 2, 3
[31] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 7
[32] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition , pages 3431–3440, 2015. 2
[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 5
[34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
pervised multitask learners. OpenAI blog , 1(8):9, 2019. 2
[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2
[36] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster,
stronger. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 7263–7271, 2017. 2
[37] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Unified, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 779–788, 2016.
[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. Advances in neural information process-
ing systems , 28, 2015. 2
[39] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 2
[40] Farhana Sultana, Abu Sufian, and Paramartha Dutta. A re-
view of object detection models based on convolutional neu-
ral network. Intelligent computing: image processing based
applications , pages 1–16, 2020. 2
[41] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen-
feng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan
Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end ob-
ject detection with learnable proposals. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 14454–14463, 2021. 2, 3[42] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia
Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda:
Language models for dialog applications. arXiv preprint
arXiv:2201.08239 , 2022. 2
[43] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos:
Fully convolutional one-stage object detection. In Proceed-
ings of the IEEE/CVF international conference on computer
vision , pages 9627–9636, 2019. 2
[44] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-
cas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,
Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al.
Mlp-mixer: An all-mlp architecture for vision. Advances
in neural information processing systems , 34:24261–24272,
2021. 2
[45] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev,
and Ali Ghodsi. Dylora: Parameter efficient tuning of pre-
trained models using dynamic search-free low-rank adapta-
tion, 2023. 2
[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 1, 2, 3
[47] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,
Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu
Qiao, et al. Visionllm: Large language model is also an
open-ended decoder for vision-centric tasks. arXiv preprint
arXiv:2305.11175 , 2023. 2
[48] BigScience Workshop, Teven Le Scao, Angela Fan, Christo-
pher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow, Ro-
man Castagn ´e, Alexandra Sasha Luccioni, Franc ¸ois Yvon,
et al. Bloom: A 176b-parameter open-access multilingual
language model. arXiv preprint arXiv:2211.05100 , 2022. 2
[49] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1492–1500,
2017. 2
[50] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,
Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized
autoregressive pretraining for language understanding. Ad-
vances in neural information processing systems , 32, 2019.
2
[51] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun
Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr
with improved denoising anchor boxes for end-to-end object
detection. arXiv preprint arXiv:2203.03605 , 2022. 3
[52] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu,
and Bo Li. Lora-fa: Memory-efficient low-rank adaptation
for large language models fine-tuning, 2023. 2
[53] Qingru Zhang, Minshuo Chen, Alexander Bukharin,
Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.
Adaptive budget allocation for parameter-efficient fine-
tuning, 2023. 2, 7
[54] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 2881–2890, 2017. 2
15875
[55] Michael Zhu and Suyog Gupta. To prune, or not to prune: ex-
ploring the efficacy of pruning for model compression. arXiv
preprint arXiv:1710.01878 , 2017. 1
[56] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang
Wang, and Jifeng Dai. Deformable detr: Deformable trans-
formers for end-to-end object detection. arXiv preprint
arXiv:2010.04159 , 2020. 2, 3
[57] Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-
Fai Wong, and Lei Zhang. Delta-lora: Fine-tuning high-rank
parameters with the delta of low-rank matrices, 2023. 2
15876
