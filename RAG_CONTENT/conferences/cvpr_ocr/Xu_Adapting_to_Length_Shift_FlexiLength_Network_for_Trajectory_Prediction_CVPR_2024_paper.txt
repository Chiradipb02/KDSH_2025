Adapting to Length Shift: FlexiLength Network for Trajectory Prediction
Yi Xu Yun Fu
Northeastern University, USA
xu.yi@northeastern.edu, yunfu@ece.neu.edu
Abstract
Trajectory prediction plays an important role in various
applications, including autonomous driving, robotics, and
scene understanding. Existing approaches mainly focus on
developing compact neural networks to increase prediction
precision on public datasets, typically employing a stan-
dardized input duration. However, a notable issue arises
when these models are evaluated with varying observation
lengths, leading to a significant performance drop, a phe-
nomenon we term the Observation Length Shift. To address
this issue, we introduce a general and effective framework,
the FlexiLength Network (FLN), to enhance the robustness
of existing trajectory prediction techniques against vary-
ing observation periods. Specifically, FLN integrates tra-
jectory data with diverse observation lengths, incorporates
FlexiLength Calibration (FLC) to acquire temporal invari-
ant representations, and employs FlexiLength Adaptation
(FLA) to further refine these representations for more ac-
curate future trajectory predictions. Comprehensive exper-
iments on multiple datasets, i.e., ETH/UCY, nuScenes, and
Argoverse 1, demonstrate the effectiveness and flexibility of
our proposed FLN framework.
1. Introduction
The goal of trajectory prediction is to predict the future
locations of agents conditioned on their past observed
states, a key step in understanding their motion and be-
havior patterns. This task is essential yet challenging in
many real-world applications such as autonomous driv-
ing [21, 23, 59, 77], robotics [10, 36], and scene under-
standing [11, 69]. Recent advancements in deep learning
have significantly improved the accuracy of trajectory pre-
dictions [2, 4, 8, 9, 17, 27, 38, 44, 47, 50, 52, 65, 66, 76].
However, achieving such promising prediction performance
typically requires complex models and extensive computa-
tional resources. Additionally, many state-of-the-art meth-
ods are developed on public datasets [7, 55, 61, 74] with
a fixed observation length, failing to consider the potential
discrepancies between training conditions and diverse real-
Figure 1. The Observation Length Shift phenomenon is a common
issue in the trajectory prediction task. The AgentFormer model is
trained with a standard observation length of 8 timesteps and tested
at varying lengths to compare with Isolated Training (IT). For each
dataset, the bar groups from left to right correspond to observation
lengths of 2, 4, and 6 timesteps, respectively.
world scenarios. Consequently, the fixed training setups are
often too rigid to meet in practice, especially when faced
with environmental changes or observational variables.
Recently, some studies have explored the issue of
training-testing discrepancy in trajectory prediction. For ex-
ample, work [68] proposes a solution for scenarios where
observations might be incomplete due to occlusions or lim-
ited viewpoints, by developing a method that combines
trajectory imputation and prediction. Other research ef-
forts [5, 24, 26, 67] focus on tackling the domain shift
challenge, where the testing environment differs from the
training. Additionally, studies [34, 40, 54] address situa-
tions with limited observations, such as having only one or
two data points available. These approaches try to mitigate
the training-testing gap from various angles. Acknowledg-
ing these efforts, the importance of addressing the training-
testing gap is highlighted. In our paper, we concentrate on
the less-explored issue of observation length discrepancy.
For certain well-known architectures such as RNN [22]
and Transformer [57], the ability to process inputs of vary-
ing lengths presents a straightforward solution: train the
model with the standard input length and then evaluate at
different input lengths. This solution is contrasted with Iso-
lated Training (IT), where the model is trained at the spe-
cific observation length and evaluated with the same input
length. We conducted experiments using a Transformer-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
15226
based method AgentFormer [73] on dataset ETH/UCY [31,
46] with results illustrated in Fig. 1. The results reveal
a clear performance discrepancy between these two solu-
tions, suggesting performance declines when not evaluated
at the training length. We term this phenomenon the Ob-
servation Length Shift. It should be noted that while the
performance of IT marginally improves, it requires multi-
ple training rounds for different observation lengths. This
observation raises two crucial questions:
1.What are the underlying reasons behind this per-
formance discrepancy?
2.Can this issue be resolved without substantial mod-
ifications or increased computational expense?
Prompted by these questions and given the popularity of
Transformer-based models in the trajectory prediction task,
we conducted an in-depth analysis of a Transformer-based
trajectory prediction method, as detailed in Sec. 3. This
investigation revealed two main causes, which we sum-
marized as positional encoding deviation and normaliza-
tion shift. Building on these findings, we introduce a ro-
bust and adaptable framework, the FlexiLength Network
(FLN), to overcome the Observation Length Shift problem.
FLN effectively merges trajectory data from various ob-
servation lengths, integrates FlexiLength Calibration (FLC)
to create time-invariant representations, and applies Flex-
iLength Adaptation (FLA) to enhance these representations,
thereby improving the trajectory prediction precision. No-
tably, FLN requires only one-time training, without sub-
stantial modifications, but offers the flexibility to different
observation lengths, thus addressing our second question.
Despite some studies focusing on positional encoding
deviation [13, 42] and normalization shift [71, 72], our work
addresses the Observation Length Shift, a commonly over-
looked issue in trajectory prediction. We propose the Flex-
iLength Network (FLN) as a practical and non-trivial so-
lution, especially valuable in real-world scenarios. Unlike
Isolated Training (IT), which is time and memory-expensive
for adapting to various observation lengths, FLN offers a
more resource-efficient alternative. Even though modern
architectures accommodate various input lengths, they of-
ten suffer performance discrepancies, a gap that FLN effec-
tively bridges while preserving the original model design.
Furthermore, FLN can be applied to nearly all existing mod-
els that are based on the Transformer architecture. The key
contributions of our work are organized as follows:
• We identify and thoroughly analyze the Observation
Length Shift phenomenon in trajectory prediction, pin-
pointing the factors that lead to performance degrada-
tion and guiding the direction of our solution.
• We introduce the FlexiLength Framework (FLN), a
robust and efficient framework requiring only one-
time training. It includes FlexiLength Calibration
(FLC) and FlexiLength Adaptation (FLA) for learningtemporal invariant representations, boosting prediction
performance with different observation lengths.
• We validate the effectiveness and generality of
FLN through comprehensive experiments on multi-
ple datasets. It demonstrates consistent superiority
over Isolated Training (IT) with various observation
lengths.
2. Related Work
Trajectory Prediction. The objective is to forecast the fu-
ture trajectories of agents based on their historical obser-
vations. The primary focus has been on understanding the
social dynamics among these agents, leading to the devel-
opment of various methodologies, utilizing Graph Neural
Networks (GNNs) as seen in works like [3, 28, 33, 39,
51, 53, 62]. The inherently uncertain and diverse nature
of future trajectories has led to the development with gen-
erative models, including Generative Adversarial Networks
(GANs) [1, 19, 28, 32, 48], Conditional Variational Autoen-
coders (CV AEs) [25, 37, 49, 63, 64], and Diffusion mod-
els [18, 27, 38]. These models have achieved promising re-
sults on various datasets. However, most of these methods
prioritize prediction accuracy while overlooking the dis-
crepancies that often arise in real-world applications, result-
ing in decreased adaptability under varying conditions. In
contrast, our work focuses on the variation in observation
lengths, a challenge that many existing methods face. It is
noteworthy that some methods based on RNN [12, 25, 49]
and Transformer [16, 56, 70, 73] can handle varying input
lengths. However, these approaches show limitations under
observation length shifts. Our work delves into this specific
challenge, proposing a comprehensive solution that not only
enhances the robustness of existing methods but also im-
proves their performance on varying observation lengths.
Training-Testing Discrepancy. The gap between training
and testing can present in different ways, either in input
types or environmental conditions. Recent studies have be-
gun to uncover various aspects of discrepancy in trajectory
prediction. Some methods [5, 24, 26, 67] have focused on
the environmental shifts between training and testing, offer-
ing innovative solutions. Another approach [60] deals with
the absence of High-Definition (HD) maps during testing,
employing Knowledge Distillation (KD) to transfer map
knowledge from a more informed teacher model to a stu-
dent model. Another aspect recently explored is the com-
pleteness of observations. Studies like [34, 40, 54] assume
only limited observed trajectory data, specifically one or
two frames, is accessible for forecasting. This assumption
has led to the concept of instantaneous trajectory prediction.
Other studies [30, 68] have addressed the challenges posed
by incomplete observations due to occlusions or viewing
limitations in real environments. Another recent work [45]
attempts to diminish internal discrepancies across datasets
15227
Figure 2. ADE 5and FDE 5results for the AgentFormer model,
which is trained on the nuScenes dataset using a standard observa-
tion length of 4 timesteps, and tested at shorter observation lengths
of 2 and 3 timesteps. These results are compared to those obtained
through Isolated Training (IT).
collected in arbitrary temporal configurations. While these
methods have recognized various aspects of training-testing
discrepancies, the issue of observation length shift remains
relatively unexplored. Our proposed framework stands out
in its generality and effectiveness, improving the ability to
handle trajectory data across different observation lengths.
Test-Time Adaptation. The objective of test-time adapta-
tion is to adapt models to new data during testing. A com-
mon approach includes integrating an auxiliary task [14,
15, 43] that employs straightforward self-supervision tech-
niques on the target domain, which typically requires mod-
ifications to the training process in the source domain to in-
clude this auxiliary task. However, recent strategies [35, 58]
enable straightforward adjustments in the target domain,
avoiding the need to modify the training process. Unlike
these methods, our proposed framework only requires one-
time training, eliminating the necessity for adjustments dur-
ing training or additional tuning during inference. More-
over, our framework functions independently, without the
need for evaluation data during the training phase.
3. A Close Look at Performance Degradation
Observation Length Shift. The Observation Length Shift
phenomenon becomes apparent when models, trained with
a specific observation length, are tested using varying
lengths, as illustrated in Fig. 1. To investigate further,
we train the AgentFormer model [73] using the nuScenes
dataset [6] and evaluate its performance across different ob-
servation lengths, with the results presented in Fig. 2. These
results reveal noticeable performance gaps when the model
is evaluated with different observation lengths compared to
Isolated Training. Notably, the performance degradation is
smaller when the testing observation length is close to the
training observation length.
Positional Encoding Deviation. Previous studies [20, 41]
have demonstrated that Transformer models struggle to gen-
eralize to input lengths they haven’t encountered before, due
to a lack of inductive bias. Other works [13, 42] identify
positional encoding as a critical factor that limits the gener-
Mean 𝝁𝝁 Variance 𝝈𝝈𝟐𝟐Figure 3. Layer Normalization statistics in two different layers of
the Transformer encoder within the AgentFormer model, isolat-
edly trained on the Eth dataset at observation lengths of 2, 6, and
8 timesteps.
alization ability to new lengths and suggest model structure
adjustments to enhance the positional encoding. Focusing
on the Transformer-based models, we analyze the positional
encoding in the AgentFormer. The positional encoding fea-
tureτt
n(k)for agent nat timestep tis calculated as follows:
τt
n(k)=⎧⎪⎪⎪⎨⎪⎪⎪⎩sin((t+H)/10000k/dt), kis even
cos((t+H)/10000(k−1)dt), kis odd,(1)
where krepresents the feature index, dtis the feature di-
mension, and His the observation length. It is obvious
that simply removing observations without compensation
will cause the length change, subsequently altering the po-
sitional encoding. Consequently, we hypothesize that the
positional encoding deviation is one crucial factor to the
performance gap. Although other Transformer-based mod-
els [75, 76] use a fully learnable positional embedding, our
experiments in Sec. 5 indicate that this type of positional
embedding remains a key reason.
Normalization Shift. Previous works [71, 72] have stud-
ied the issue of normalization shift in the context of im-
age and video classification. They found that altering net-
work widths impacts Batch Normalization (BN), leading to
variations in the means and variances of aggregated fea-
tures and thereby affecting feature aggregation consistency.
Given that Layer Normalization (LN) is widely used in
Transformer-based models and operates similarly to BN,
we hypothesize that the discrepancy in LN statistics could
be another factor affecting the generalization ability to dif-
ferent lengths. For example, if a model is trained using an
observation length of Hand later evaluated using a length
ofL, the input to the LN would be the intermediate feature
fLduring testing, resulting in the following output:
yL=γHfL−µH
√
σH2+ϵ+βH, (2)
where µH,σH2are derived from the data distribution with
input length H, and γH,βHare parameters learned during
15228
𝑯𝑯=𝑯𝑯𝑺𝑺?Past Trajectory: 𝑿𝑿
Observation Length: 𝑯𝑯
𝑯𝑯=𝑯𝑯𝑳𝑳?
𝑯𝑯=𝑯𝑯𝑴𝑴?𝑿𝑿𝒔𝒔:
𝑿𝑿𝑳𝑳:
𝑿𝑿𝑴𝑴:
Spatial  Encoder
Multi- Head Self -Attention
Feed Forward Layer
Transformer Encoder
Trajectory  Decoder𝒀𝒀𝒔𝒔~𝑫𝑫(𝝋𝝋𝑺𝑺)
𝒀𝒀𝑳𝑳~𝑫𝑫(𝝋𝝋𝑳𝑳)
𝒀𝒀𝑴𝑴~𝑫𝑫(𝝋𝝋𝑴𝑴)Future Trajectory: �𝒀𝒀
KL LossKL Loss
Prediction
Loss


Testing Training: Training Stream
: Inference Stream
: Addition
: Weight Sharing
𝐏𝐏𝐏𝐏𝑺𝑺𝐏𝐏𝐏𝐏𝑳𝑳𝐏𝐏𝐏𝐏𝑴𝑴
𝐋𝐋𝐋𝐋𝑺𝑺𝐋𝐋𝐋𝐋𝑳𝑳𝐋𝐋𝐋𝐋𝑴𝑴
𝐋𝐋𝐋𝐋𝑺𝑺𝐋𝐋𝐋𝐋𝑳𝑳𝐋𝐋𝐋𝐋𝑴𝑴Figure 4. Illustration of our FlexiLength Network (FLN). The map encoding branch is omitted for simplicity. During training, with inputs
of varying observation lengths HS,HM, andHL, we utilize FlexiLength Calibration (FLC) to acquire temporal invariant representations.
Furthermore, FlexiLength Adaptation (FLA) is employed to align these invariant representations with different sub-networks, thereby
augmenting the model capabilities. During inference, the sub-network with the closest match in observation length is activated.
training with the same input length. We train the Agent-
Former model using observation lengths HandLand then
analyze the feature statistics in the Transformer encoder’s
two Layer Normalizations (LNs). Clear statistical differ-
ences at varying observation lengths can be observed in
Fig. 3. Importantly, both µandσare data-dependent, indi-
cating that the differences arise directly from the data itself.
Therefore, we believe that the variations in LN statistics
are another reason for the Observation Length Shift phe-
nomenon in Transformer-based models.
4. FlexiLength Network
4.1. Problem Formulation
Despite some variations in terminology, the fundamental
problem setup for multi-agent trajectory prediction remains
the same across existing methods. Essentially, the objec-
tive is to predict the future trajectories of multiple agents,
Y∈RN×T×F, based on their past observed states X∈
RN×H×F. Here, Nrepresents the number of agents, His
the length of observation, Tis the prediction duration and
Ftypically stands for 2, denoting coordinate dimensions.
Common approaches aim to develop a generative model,
denoted as pθ(Y∣X,I), where Irepresents the contextual
information such as High-definition (HD) maps.
As previously mentioned, these methods often face chal-
lenges when there is a variation in observation lengths dur-
ing evaluation. Hence, our objective is to develop a model
pθ(Y∣X,I)that can be evaluated across a range of input
lengths H, and achieves similar or even better performance
compared to Isolated Training (IT), where Hdenotes a set
of different observation lengths.
4.2. FlexiLength Calibration
As observed in Sec. 3, the performance gap is smaller when
the evaluation length is closer to the training length. This
observation inspired us to incorporate observations of vary-
ing lengths during training and to develop correspondingsub-networks for feature extraction from these sequences.
Given an observation trajectory sequence X, we can gener-
ate sequences of varying lengths either through truncation
or a sliding window approach. In this way, we can col-
lect three sequences: XS,XM, and XL, corresponding
to Short, Medium, and Long lengths, with corresponding
lengths HS,HM, and HL. During training, these three se-
quences are each fed into their corresponding sub-networks
FS(⋅),FM(⋅), and FL(⋅), for processing.
Fig. 4 showcases our FlexiLength Network (FLN) de-
veloped atop the Transformer-based architecture, where the
map branch is omitted for simplicity. We categorize the
components of typical Transformer-based models into sev-
eral key parts: a Spatial Encoder for spatial feature extrac-
tion, a Positional Encoder (PE) for embedding positional in-
formation, a Transformer Encoder for temporal dependency
modeling, and a Trajectory Decoder for generating trajecto-
ries. During training, the three inputs XS,XM, and XL
are each processed by their respective sub-networks FS(⋅),
FM(⋅), and FL(⋅). During inference, the sub-network
matching the observation length is exclusively activated.
This approach sets up a computational stream in the FLN
framework, enabling effective evaluation across various ob-
servation lengths.
Sub-Network Weight Sharing. Given the three trajectory
samples XS,XM, and XL, we employ shared weights for
the spatial encoder, temporal encoder, and trajectory de-
coder across all sub-networks, to find a group of parameters
θto extract the spatio-temporal features as follows:
Y∗∼D(ψ∗),D(ψ∗)=F∗(X∗;θ), (3)
where∗∈{S, M, L}, andD(ψ∗)denotes the distribution
parameterized by ψ∗, that Y∗adheres to, such as a bivariate
Gaussian or Laplace mixture distribution [76].
This design contrasts with Isolated Training (IT) by be-
ing more efficient in terms of parameter usage, as it main-
tains only a single set of weights applicable to various
lengths. Additionally, this shared-weight strategy enhances
15229
performance by implicitly providing the model with the
prior knowledge that these three observed sequences are
part of the same trajectory. This insight makes the model
more resilient to observation length shifts.
Temporal Distillation. Trajectory prediction models gen-
erally perform better with longer observations XLas more
movement information is contained. Consequently, we re-
gardYL∼D(ψL)as the most ’ accurate ’ among the three
predictions. To update the parameters of FL(⋅), we employ
a negative log-likelihood loss, represented as follows:
Lreg=−log(P(ˆY∣ψL)), (4)
where ˆYis the ground truth trajectory. Note that as multi-
ple agents always exist within a single trajectory, the above
equation is a simplified version and omits the calculation
for the average value per agent at each time step.
For updating the parameters of the other two sub-
networks, FS(⋅)andFM(⋅), a direct approach might in-
volve calculating a corresponding negative log-likelihood
loss as shown in Eq. (4). However, it has potential draw-
backs. Since the weights are shared across all three sub-
networks, the optimal parameters for FL(⋅)may not be as
effective for FS(⋅)orFM(⋅). Additionally, optimizing the
log-likelihood loss for ψSandψMcould also hinder the
performance of FL(⋅). This is because their inputs XS
andXM, with less movement information, might lead to
a poorer fit. To address this issue, we employ the KL di-
vergence loss [29] to calibrate and incorporate ψSandψM
into the computational process as follows:
Lkl=KL(D(ψL)∣∣D(ψM))+KL(D(ψL)∣∣D(ψS)).
(5)
Updating the weights of shared sub-networks through
Eq. (5) aims to align the predicted distributions of the ’stu-
dent’ networks ( D(ψM)andD(ψS)) closely with the pre-
dicted distribution of the ’teacher’ network ( D(ψL)). This
process facilitates the transfer of valuable knowledge from
FL(⋅)to both FM(⋅)andFS(⋅). The entire set of param-
eters for our FLN is updated as follows:
LFLN=Lreg+λ1⋅Lkl, (6)
where λserves to balance these two terms during backprop-
agation, we have set λ=1in our implementation.
Considering both Sub-Network Weight Sharing and
Temporal Distillation, Lregoffers guidance for the predic-
tion task, while Lkloffers intra-trajectory knowledge to net-
work training. This will promote D(ψL),D(ψM), and
D(ψS)exhibit high similarity, as the length shift does not
alter the distribution of one specific trajectory. Through this
approach, FLN not only learns temporal invariant represen-
tations but also ensures ease of implementation, as it does
not require any changes to the feature extractors.4.3. FlexiLength Adaptation
We developed FLC to guide our FLN in learning temporal
invariant representations. Additionally, we introduce Flex-
iLength Adaptation (FLA) to optimize the fit of these invari-
ant features across different sub-networks, thereby further
improving their representational capacity.
Independent Positional Encoding. Our analysis in Sec. 3
identifies positional encoding as one factor that can confuse
the model regarding observation length. To counter this, we
implement independent positional encoding for each sub-
network. Taking the positional encoding from the Agent-
Former [73] as an example, we define the positional encod-
ing for inputs with diverse observation lengths as follows:
τt
n(k)=⎧⎪⎪⎪⎨⎪⎪⎪⎩sin((t+H∗)/10000k/dt), kis even
cos((t+H∗)/10000(k−1)dt), kis odd,(7)
where∗∈L, M, S . While recent Transformer-based mod-
els [75, 76] have shifted to a fully learnable positional em-
bedding instead of the traditional sinusoidal pattern, we
adopt this learnable positional embedding across each of
our sub-networks. As a result, for each input sequence XS,
XM, and XL, we define unique learnable positional en-
coding τS,τM, and τL. Note that the implementation of
independent positional encoding for each sub-network is a
resource-efficient solution, introducing only a negligible in-
crease in parameters and computation.
Specialized Layer Normalization. As outlined in Sec. 3,
normalization shift is another reason for the performance
gap when observation length shifts. Denote the interme-
diate features for XS,XM, and XLasfS,fM, and fL,
respectively. We introduce the specialized layer normaliza-
tion for each input sequence as follows:
y∗=γ∗f∗−µ∗
√
σ∗2+ϵ+β∗, (8)
where∗∈{S, M, L}. This specialized normalization al-
lows for the independent learning of γandβ, and the calcu-
lation of µandσspecific to each sequence during training.
Moreover, this approach is efficient, as normalization typi-
cally involves a simple transformation with less than 1%of
total model parameters.
5. Experiments
5.1. Settings
Baselines. Our proposed FLN is a general framework that
is compatible with those Transformer-based methods. We
have chosen two widely recognized open-source methods,
AgentFormer [73] and HiVT [75], to integrate with our
FLN. Furthermore, for a more comprehensive comparison,
15230
Method Note #ParamADE 5/FDE 5↓K = 5 Samples ADE 10/FDE 10↓K = 10 Samples
2 Timesteps 3 Timesteps 4 Timesteps 2 Timesteps 3 Timesteps 4 Timesteps
AFormer [73] - 6.67M 2.26/4.53 2.00/4.10 1.86/3.89 1.72/3.06 1.53/2.95 1.45/2.86
♦AFormer-Mixed ρS=ρM=ρL=0.5 6.67M 2.14/4.38 1.98/4.05 1.94/3.91 1.68/3.06 1.50/2.91 1.57/2.93
♦AFormer-Mixed ρS=0.75, ρM=ρL=0.5 6.67M 2.10/4.38 1.98/4.02 2.05/4.14 1.65/3.01 1.50/2.90 1.66/3.11
♦AFormer-Tuning 4Ts →2Ts 6.67M 2.10/4.32 2.15/4.31 2.28/4.43 1.60/2.98 1.81/3.34 1.79/3.12
♦AFormer-Tuning 4Ts →3Ts 6.67M 2.32/4.67 1.93 /3.94 2.32/4.96 1.98/3.33 1.48/2.92 2.12/3.97
♦AFormer-Joint - 6.67 ×3M 1.98 /4.19 1.95/3.97 1.90/4.02 1.54/3.04 1.50/2.91 1.49/2.96
♦AFormer-IT - 6.67 ×3M 2.02/4.23 1.93 /3.97 1.86 /3.89 1.52/3.00 1.47/2.89 1.45/2.86
AFormer-FLN ρS=ρM=ρL=1 6.68M 1.92/3.91 1.88/3.89 1.83/3.78 1.47/2.90 1.43/2.82 1.32/2.73
Table 1. Comparison with baseline models on nuScenes, evaluated using ADE 5/FDE 5and ADE 10/FDE 10metrics. The best results are
highlighted in bold and the second-best results are underlined.
Method Note #ParamADE 6/FDE 6↓K = 6 Samples
10 Timesteps 20 Timesteps 30 Timesteps
HiVT-64 [75] - 662K 1.23/1.48 0.91/1.37 0.69/1.04
♦HiVT-64-Mixed ρS=ρM=ρL=0.5 662K 1.14/1.57 0.86/1.22 0.80/1.12
♦HiVT-64-Mixed ρS=0.75, ρM=ρL=0.5 662K 1.08/1.49 0.88/1.22 0.84/1.13
♦HiVT-64-Tuning 30Ts →10Ts 662K 0.90 /1.38 1.14/1.48 1.18/1.31
♦HiVT-64-Tuning 30Ts →20Ts 662K 1.33/1.61 0.87/1.20 1.12/1.29
♦HiVT-64-Joint - 662 ×3K 0.98/1.45 0.89/1.24 0.74/1.09
♦HiVT-64-IT - 662 ×3K 0.92/1.43 0.81 /1.17 0.69/1.04
HiVT-64-FLN ρS=ρM=ρL=1 680K 0.81/1.25 0.72/1.08 0.65/0.98
Table 2. Comparison with baseline models on the Argoverse 1 validation set, evaluated using ADE 6/FDE 6metrics. The best results are
highlighted in bold and the second-best results are underlined.
we include four additional baseline models: (1) Mixed
Sampling : In each training iteration, we assign three prob-
abilities ρS,ρM, and ρLto take trajectory data with obser-
vation lengths HS,HM, and HLfor training. (2) Fine-
tuning : The model is trained using the observation length
HL, and then fine-tuned using another length ( HSorHM)
until convergence. (3) Joint : We expand the training dataset
by including trajectory samples from all three observation
lengths ( HS,HM, and HL) and then train the original
model without any structural changes. (4) Isolated Train-
ing: The model is trained exclusively with trajectory data
of a single observation length, HS,HM, orHL.
Datasets. We use three following datasets: (1) The
ETH/UCY dataset [31, 46] is a primary benchmark for
pedestrian trajectory prediction, including five datasets, Eth,
Hotel, Univ, Zara1, and Zara2, with densely interactive tra-
jectories sampled at 2.5Hz. (2) The nuScenes dataset [74]
is a large autonomous driving dataset featuring 1000 scenes,
each annotated at 2Hz, and includes HD maps with 11 se-
mantic classes. (3) Argoverse 1 [7] contains 323,557 real-
world driving sequences sampled at 10Hz, complemented
with HD maps for trajectory prediction.
Evaluation Protocol. Across the three datasets, we use
the metrics Average Displacement Error (ADE K) and Fi-
nal Displacement Error (FDE K) for comparison, where
Kindicates the number of trajectories to be predicted.
Each dataset follows its own evaluation protocol: (1) ForETH/UCY , the leave-one-out setting is standard, with the
task to predict 12 future time steps from 8 observed steps.
Here, Kis commonly set to 20. (2) Within the nuScenes
dataset, as used in AgentFormer, only vehicle data is con-
sidered, predicting 12 future steps from 4 observed steps. K
is usually set to 5 and 10. (3) In Argoverse 1, sequences are
segmented into 5-second intervals, with predictions made
for 30 future steps (3 seconds) based on 20 observed steps
(2 seconds) involving multiple agents. The validation set is
used for our evaluation purpose, with Kbeing 6.
Implementation Details. For each dataset, we define
three different observation lengths (in timestep) H=
{HS, HM, HL}for training, where HLis set as the default
length in the standard evaluation protocol, considering the
potential unavailability of data beyond this length. Specifi-
cally, we use H={2,6,8}for ETH/UCY , H={2,3,4}for
nuScenes, and H={10,20,30}for Argoverse 1. The pre-
diction length Tremains as defined in the standard settings.
The prototype models are trained at observation length HL
and evaluated at HSandHM. Isolated Training (IT) refers
to training and evaluating the model separately at observa-
tion length HS,HM, and HL. More details and experi-
ments are provided in the Supplementary Material.
5.2. Main Results
Comparison with Baselines. Tab. 1 presents the results us-
ing the AgentFormer model on the nuScenes dataset, where
15231
(a) ADE 20on Eth.
 (b) ADE 20on Hotel.
 (c) ADE 20on Univ.
 (d) ADE 20on Zara1.
 (e) ADE 20on Zara2.
Figure 5. Performance on five ETH/UCY datasets using the AgentFormer model, measured with ADE 20. These results are compared with
those of the baseline model and Isolated Training (IT), showcasing notable improvements achieved by our FLN.
(a) FDE 20on Eth.
 (b) FDE 20on Hotel.
 (c) FDE 20on Univ.
 (d) FDE 20on Zara1.
 (e) FDE 20on Zara2.
Figure 6. Performance on five ETH/UCY datasets using the AgentFormer model, measured with FDE 20. These results are compared with
those of the baseline model and Isolated Training (IT), showcasing notable improvements achieved by our FLN.
our FLN framework outperforms all baseline models, show-
casing its robustness in adapting to different observation
lengths. It is noticeable that Mixed Sampling helps miti-
gate the Observation Length Shift issue, as indicated by the
improvement at observation lengths of 2 and 3 timesteps,
compared to the results achieved with the standard train-
ing protocol. However, this improvement comes at the cost
of reduced accuracy at the 4-timestep observation length.
Increasing the probability for length HSimproves the per-
formance at that length, but conversely, the performance at
HLbecomes worse. This pattern suggests a trade-off: im-
proved results at shorter lengths result in diminished per-
formance at longer lengths. One interesting observation is
that the performance at HMremains relatively stable, pos-
sibly because the quantity of sequence data for HMandHL
remains unchanged, thus maintaining a consistent relation-
ship between these two data clusters. Fine-tuning enables
the baseline method to achieve comparable performance
with IT, neglecting information from other lengths. As for
the joint baseline, its performance is similar to IT, and even
better than IT at the 2-timestep length when K=5. How-
ever, a significant issue with both Joint and IT baselines
is the increased complexity of the models due to repeated
training sessions. In contrast, our FLN requires only a sin-gle training session with minimal additional computational
overhead, yet it allows for evaluation at various lengths and
obtains superior results.
Performance across Datasets. Additionally, the evalua-
tion on the ETH/UCY benchmark, as illustrated in Fig. 5
and Fig. 6, reveals that our proposed FLN outperforms IT
at different observation lengths. We also extend our FLN to
the HiVT method, which utilizes learnable embeddings for
positional encoding, and evaluate it on the Argoverse 1 val-
idation set. The results, presented in Tab. 2, show that FLN
consistently achieves superior performance across three dif-
ferent observation lengths, affirming its robustness in adapt-
ing to various observation lengths.
5.3. Generality Study
We have demonstrated that FLN surpasses IT at the train-
ing observation lengths, while not evaluating the model at
lengths outside the training range. Inspired by our findings
in Sec. 3, where the performance gap narrows with observa-
tion length closer to the standard input length, we introduce
an efficient inference approach enabling FLN to be evalu-
ated at any length. When encountering an unseen length
H′, we first calculate its difference from HS,HM, andHL,
and then activate the sub-network with the least difference
in length. In cases where two sub-networks have the same
15232
∆ADE6/∆FDE6↓ 
4Ts*   -0.15/ -0.21
6Ts*   -0.10/ -0.18
10Ts   -0.11/ -0.18
14Ts -0.08/ -0.18
16Ts   -0.08/ -0.10
20Ts   -0.09/ -0.10
24Ts   -0.03/ -0.09
26Ts   -0.02/ -0.02
30Ts   -0.04/ -0.06Figure 7. Performance of FLN and IT at different observation
lengths on the Argoverse 1 validation set. The improvements are
listed in the table. Results from observation lengths outside the
range of 10 to 30 are denoted with a ∗.
length difference, we default to the one with the longer in-
put length. This approach allows for the evaluation of FLN
across a spectrum of unseen observation lengths.
Since we set the longest length HLas the standard ob-
servation length, it is challenging to generate trajectory se-
quences with observation lengths longer than HLdue to
their pre-segmentation. The results on the Argoverse 1 vali-
dation set with observation length shorter than 30 timesteps
are illustrated in Fig. 7. These results clearly demonstrate
that FLN consistently outperforms IT at all observation
lengths, even those beyond the 10-30 range. This validates
the strong generality of our FLN in handling observation
lengths that are not included during training.
5.4. Ablation Study
Length Combination. We implement different combina-
tions of observation lengths on Argoverse 1, with the results
presented in Tab. 3. It is observed that FLN-2 surpasses
IT in terms of ADE 6and FDE 6at the length of 10 and 16
timesteps, and achieves comparable results at the length of
30. However, its performance at 20-timestep length is worse
than IT, likely due to the absence of this sequence length in
the training data. Both FLN-3 and FLN-4 outperform IT at
all observation lengths. This improvement can be attributed
to the existence of trajectory data with intermediate obser-
vation lengths, which helps mitigate the Observation Length
Shift within this range. Although FLN-4 shows better re-
sults than FLN-3 at lengths of 10, 16, and 20 timesteps, it
also costs additional time and resources for training.
Model Design. We conducted a detailed analysis of each
component within our FLN on the nuScenes dataset, with
results presented in Tab. 4. We first remove Weight SharingMethod CombinationADE 6/FDE 6↓K = 6 Samples
10Ts 16Ts 20Ts 30Ts
HiVT-64-IT [75] - 0.92/1.43 0.81/1.20 0.81/1.17 0.69/1.04
HiVT-64-FLN-2 10/30 0.88/1.37 0.80/1.20 0.84/1.21 0.69/1.02
HiVT-64-FLN-3 10/20/30 0.81/1.25 0.73/1.10 0.72/1.08 0.65/0.98
HiVT-64-FLN-4 10/16/20/30 0.79/1.19 0.70 /1.02 0.72 /1.06 0.68/0.99
Table 3. Performance of FLN on the Argoverse 1 validation set
using different combinations of observation lengths. The best re-
sults are highlighted in bold.
Method NoteADE 5/FDE 5↓K = 5 Samples
2Ts 3Ts 4Ts
AFormer-IT [73] - 2.02/4.23 1.93/3.97 1.86/3.89
AFormer-FLN w/o WS 1.94/3.95 1.89/3.92 1.92/3.94
AFormer-FLN w/o TD 2.23/4.39 1.98/4.01 1.85/3.81
AFormer-FLN w/o IPE 2.13/4.33 2.02/4.07 1.95/3.98
AFormer-FLN w/o SLN 2.12/4.30 1.98/4.02 1.93/3.95
AFormer-FLN - 1.92/3.91 1.88/3.89 1.83/3.78
Table 4. Ablation study of FLN on nuScenes. WS, TD, IPE, and
SLN denote Sub-Network Weight Sharing, Temporal Distillation,
Independent Positional Encoding, and Specialized Layer Normal-
ization, respectively. The best results are highlighted in bold.
(w/o WS), creating unique sub-networks for each length.
The results are better than IT at observation lengths of 2
and 3 timesteps, but worse at length of 4. This suggests
that shared weights enable FLN to more effectively capture
temporal-invariant features across diverse input lengths. We
excluded Temporal Distillation (w/o TD) from FLN, mean-
ing not utilizing the KL loss for optimization. This leads to
the performance worse than IT at observation lengths of 2
and 3 timesteps, suggesting the necessity of our TD design.
We also tested shared Positional Encoding (w/o IPE) and
shared Layer Normalization (w/o SLN). Both resulted in a
performance decrease, validating the significance of our In-
dependent Positional Encoding (IPE) and Specialized Layer
Normalization (SLN) designs.
6. Conclusion
In this paper, we tackle the critical challenge of Observa-
tion Length Shift in trajectory prediction by introducing the
FlexiLength Network (FLN). This novel framework, incor-
porating FlexiLength Calibration (FLC) and FlexiLength
Adaptation (FLA), offers a general solution for handling
varying observation lengths and requires only one-time
training. Our thorough experiments on datasets such as
ETH/UCY , nuScenes, and Argoverse 1 demonstrate that
FLN not only improves prediction accuracy and robustness
over a range of observation lengths but also consistently
outperforms Isolated Training (IT).
Limitation and Future Work. One limitation is that
FLN will increase training time due to handling several in-
put sequences per training iteration. Going forward, we will
focus on enhancing the training efficiency of FLN.
15233
References
[1] Javad Amirian, Jean-Bernard Hayet, and Julien Pettr ´e. Social
ways: Learning multi-modal distributions of pedestrian tra-
jectories with GANs. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition Work-
shops , pages 2964–2972, 2019. 2
[2] G ¨orkay Aydemir, Adil Kaan Akan, and Fatma G ¨uney. Adapt:
Efficient multi-agent trajectory prediction with adaptation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 8295–8305, 2023. 1
[3] Inhwan Bae, Jin-Hwi Park, and Hae-Gon Jeon. Learning
pedestrian group representations for multi-modal trajectory
prediction. In Proceedings of the European Conference on
Computer Vision , pages 270–289, 2022. 2
[4] Inhwan Bae, Jean Oh, and Hae-Gon Jeon. Eigentrajectory:
Low-rank descriptors for multi-modal trajectory forecasting.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 10017–10029, 2023. 1
[5] Mohammadhossein Bahari, Saeed Saadatnejad, Ahmad
Rahimi, Mohammad Shaverdikondori, Amir Hossein
Shahidzadeh, Seyed-Mohsen Moosavi-Dezfooli, and
Alexandre Alahi. Vehicle trajectory prediction works,
but not everywhere. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 17123–17133, 2022. 1, 2
[6] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuScenes: A multi-
modal dataset for autonomous driving. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11621–11631, 2020. 3
[7] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-
jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter
Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d
tracking and forecasting with rich maps. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8748–8757, 2019. 1, 6
[8] Guangyi Chen, Zhenhao Chen, Shunxing Fan, and Kun
Zhang. Unsupervised sampling promoting for stochastic hu-
man trajectory prediction. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 17874–17884, 2023. 1
[9] Hao Chen, Jiaze Wang, Kun Shao, Furui Liu, Jianye Hao,
Chenyong Guan, Guangyong Chen, and Pheng-Ann Heng.
Traj-mae: Masked autoencoders for trajectory prediction. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 8351–8362, 2023. 1
[10] Yujiao Cheng, Liting Sun, Changliu Liu, and Masayoshi
Tomizuka. Towards efficient human-robot collaboration with
robust plan recognition and trajectory prediction. IEEE
Robotics and Automation Letters , 5(2):2602–2609, 2020. 1
[11] Hyung-gun Chi, Kwonjoon Lee, Nakul Agarwal, Yi Xu,
Karthik Ramani, and Chiho Choi. Adamsformer for spa-
tial action localization in the future. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 17885–17895, 2023. 1[12] Luigi Filippo Chiara, Pasquale Coscia, Sourav Das, Simone
Calderara, Rita Cucchiara, and Lamberto Ballan. Goal-
driven self-attentive recurrent networks for trajectory predic-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 2518–2527,
2022. 2
[13] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob
Uszkoreit, and Lukasz Kaiser. Universal transformers. In
Proceedings of the International Conference on Learning
Representations , 2019. 2, 3
[14] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried-
miller, and Thomas Brox. Discriminative unsupervised fea-
ture learning with convolutional neural networks. In Pro-
ceedings of the Advances in Neural Information Processing
Systems , 2014. 3
[15] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-
supervised representation learning by predicting image ro-
tations. In Proceedings of the International Conference on
Learning Representations , 2018. 3
[16] Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio
Galasso. Transformer networks for trajectory forecasting. In
Proceedings of the IEEE International Conference on Pat-
tern Recognition , pages 10335–10342, 2020. 2
[17] Junru Gu, Chenxu Hu, Tianyuan Zhang, Xuanyao Chen,
Yilun Wang, Yue Wang, and Hang Zhao. Vip3d: End-to-end
visual trajectory prediction via 3d agent queries. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 5496–5506, 2023. 1
[18] Tianpei Gu, Guangyi Chen, Junlong Li, Chunze Lin, Yong-
ming Rao, Jie Zhou, and Jiwen Lu. Stochastic trajectory
prediction via motion indeterminacy diffusion. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 17113–17122, 2022. 2
[19] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese,
and Alexandre Alahi. Social GAN: Socially acceptable tra-
jectories with generative adversarial networks. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 2255–2264, 2018. 2
[20] Michael Hahn. Theoretical limitations of self-attention in
neural sequence models. Transactions of the Association for
Computational Linguistics , 8:156–171, 2020. 3
[21] Christopher Hazard, Akshay Bhagat, Balarama Raju Bud-
dharaju, Zhongtao Liu, Yunming Shao, Lu Lu, Sammy
Omari, and Henggang Cui. Importance is in your attention:
agent importance prediction for autonomous driving. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 2532–2535, 2022. 1
[22] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term
memory. Neural Computation , 9(8):1735–1780, 1997. 1
[23] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,
Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai
Wang, et al. Planning-oriented autonomous driving. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 17853–17862, 2023. 1
[24] Manh Huynh and Gita Alaghband. Online adaptive temporal
memory with certainty estimation for human trajectory pre-
diction. In Proceedings of the IEEE Winter Conference on
Applications of Computer Vision , pages 940–949, 2023. 1, 2
15234
[25] Boris Ivanovic and Marco Pavone. The trajectron: Proba-
bilistic multi-agent trajectory modeling with dynamic spa-
tiotemporal graphs. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 2375–2384,
2019. 2
[26] Boris Ivanovic, James Harrison, and Marco Pavone. Expand-
ing the deployment envelope of behavior prediction via adap-
tive meta-learning. In Proceedings of the IEEE International
Conference on Robotics and Automation , 2023. 1, 2
[27] Chiyu Jiang, Andre Cornman, Cheolho Park, Benjamin
Sapp, Yin Zhou, Dragomir Anguelov, et al. Motiondiffuser:
Controllable multi-agent motion prediction using diffusion.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 9644–9653, 2023. 1,
2
[28] Vineet Kosaraju, Amir Sadeghian, Roberto Mart ´ın-Mart ´ın,
Ian Reid, Hamid Rezatofighi, and Silvio Savarese. Social-
bigat: Multimodal trajectory forecasting using bicycle-gan
and graph attention networks. In Proceedings of the Ad-
vances in Neural Information Processing Systems , pages
137–146, 2019. 2
[29] Solomon Kullback. Information theory and statistics .
Courier Corporation, 1997. 5
[30] Bernard Lange, Jiachen Li, and Mykel J Kochenderfer.
Scene informer: Anchor-based occlusion inference and
trajectory prediction in partially observable environments.
arXiv preprint arXiv:2309.13893 , 2023. 2
[31] Alon Lerner, Yiorgos Chrysanthou, and Dani Lischinski.
Crowds by example. In Computer graphics forum , pages
655–664. Wiley Online Library, 2007. 2, 6
[32] Jiachen Li, Hengbo Ma, and Masayoshi Tomizuka. Con-
ditional generative neural system for probabilistic trajectory
prediction. In Proceedings of the International Conference
on Intelligent Robots and Systems , 2019. 2
[33] Lihuan Li, Maurice Pagnucco, and Yang Song. Graph-
based spatial transformer with memory replay for multi-
future pedestrian trajectory prediction. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2231–2241, 2022. 2
[34] Rongqing Li, Changsheng Li, Dongchun Ren, Guangyi
Chen, Ye Yuan, and Guoren Wang. Bcdiff: Bidirectional
consistent diffusion for instantaneous trajectory prediction.
InProceedings of the Advances in Neural Information Pro-
cessing Systems , 2023. 1, 2
[35] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste
Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++:
When does self-supervised test-time training fail or thrive?
InProceedings of the Advances in Neural Information Pro-
cessing Systems , pages 21808–21820, 2021. 3
[36] Mohammad Mahdavian, Payam Nikdel, Mahdi TaherAh-
madi, and Mo Chen. Stpotr: Simultaneous human trajectory
and pose prediction using a non-autoregressive transformer
for robot follow-ahead. In Proceedings of the IEEE Interna-
tional Conference on Robotics and Automation , pages 9959–
9965, 2023. 1
[37] Karttikeya Mangalam, Harshayu Girase, Shreyas Agarwal,
Kuan Hui Lee, Ehsan Adeli, Jitendra Malik, and AdrienGaidon. It is not the journey but the destination: End-
point conditioned trajectory prediction. In Proceedings of
the European Conference on Computer Vision , pages 759–
776, 2020. 2
[38] Weibo Mao, Chenxin Xu, Qi Zhu, Siheng Chen, and Yan-
feng Wang. Leapfrog diffusion model for stochastic trajec-
tory prediction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5517–
5526, 2023. 1, 2
[39] Abduallah Mohamed, Kun Qian, Mohamed Elhoseiny, and
Christian Claudel. Social-STGCNN: A social spatio-
temporal graph convolutional neural network for human tra-
jectory prediction. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
14424–14432, 2020. 2
[40] Alessio Monti, Angelo Porrello, Simone Calderara, Pasquale
Coscia, Lamberto Ballan, and Rita Cucchiara. How many
observations are enough? Knowledge distillation for trajec-
tory forecasting. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
6553–6562, 2022. 1, 2
[41] Hyoungwook Nam, Segwang Kim, and Kyomin Jung. Num-
ber sequence prediction problems for evaluating computa-
tional powers of neural networks. In Proceedings of the
AAAI Conference on Artificial Intelligence , pages 4626–
4633, 2019. 3
[42] Hyoungwook Nam, Seung Byum Seo, Vikram Sharma
Mailthody, Noor Michael, and Lan Li. I-bert: Inductive gen-
eralization of transformer to arbitrary context lengths. arXiv
preprint arXiv:2006.10220 , 2020. 2, 3
[43] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of
visual representations by solving jigsaw puzzles. In Proceed-
ings of the European Conference on Computer Vision , pages
69–84. Springer, 2016. 3
[44] Daehee Park, Hobin Ryu, Yunseo Yang, Jegyeong Cho, Ji-
won Kim, and Kuk-Jin Yoon. Leveraging future relationship
reasoning for vehicle trajectory prediction. In Proceedings of
the International Conference on Learning Representations ,
2023. 1
[45] Daehee Park, Jaewoo Jeong, and Kuk-Jin Yoon. Improv-
ing transferability for cross-domain trajectory prediction via
neural stochastic differential equation. In Proceedings of the
AAAI Conference on Artificial Intelligence , 2024. 2
[46] Stefano Pellegrini, Andreas Ess, Konrad Schindler, and Luc
J. Van Gool. You’ll never walk alone: Modeling social
behavior for multi-target tracking. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 261–268, 2009. 2, 6
[47] Luke Rowe, Martin Ethier, Eli-Henry Dykhne, and
Krzysztof Czarnecki. Fjmp: Factorized joint multi-agent
motion prediction over learned directed acyclic interaction
graphs. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 13745–
13755, 2023. 1
[48] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki
Hirose, Hamid Rezatofighi, and Silvio Savarese. SoPhie: An
attentive GAN for predicting paths compliant to social and
15235
physical constraints. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
1349–1358, 2019. 2
[49] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and
Marco Pavone. Trajectron++: Dynamically-feasible trajec-
tory forecasting with heterogeneous data. In Proceedings of
the European Conference on Computer Vision , pages 683–
700, 2020. 2
[50] Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou,
Nigamaa Nayakanti, Khaled S Refaat, Rami Al-Rfou, and
Benjamin Sapp. Motionlm: Multi-agent motion forecast-
ing as language modeling. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 8579–
8590, 2023. 1
[51] Liushuai Shi, Le Wang, Chengjiang Long, Sanping Zhou,
Mo Zhou, Zhenxing Niu, and Gang Hua. Sgcn: Sparse graph
convolution network for pedestrian trajectory prediction. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 8994–9003, 2021. 2
[52] Liushuai Shi, Le Wang, Sanping Zhou, and Gang Hua. Tra-
jectory unified transformer for pedestrian trajectory predic-
tion. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 9675–9684, 2023. 1
[53] Jianhua Sun, Qinhong Jiang, and Cewu Lu. Recursive social
behavior graph for trajectory prediction. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 660–669, 2020. 2
[54] Jianhua Sun, Yuxuan Li, Liang Chai, Hao-Shu Fang, Yong-
Lu Li, and Cewu Lu. Human trajectory prediction with mo-
mentary observation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6467–6476, 2022. 1, 2
[55] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,
Yuning Chai, Benjamin Caine, et al. Scalability in perception
for autonomous driving: Waymo open dataset. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 2446–2454, 2020. 1
[56] Li-Wu Tsao, Yan-Kai Wang, Hao-Siang Lin, Hong-Han
Shuai, Lai-Kuan Wong, and Wen-Huang Cheng. Social-
ssl: Self-supervised cross-sequence representation learning
based on transformers for multi-agent trajectory prediction.
InProceedings of the European Conference on Computer Vi-
sion, pages 234–250, 2022. 2
[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Proceedings of the
Advances in Neural Information Processing Systems , pages
5998–6008, 2017. 1
[58] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol-
shausen, and Trevor Darrell. Tent: Fully test-time adaptation
by entropy minimization. In Proceedings of the International
Conference on Learning Representations , 2021. 3
[59] Jingke Wang, Tengju Ye, Ziqing Gu, and Junbo Chen. Ltp:
Lane-based trajectory prediction for autonomous driving. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 17134–17142, 2022. 1[60] Yuning Wang, Pu Zhang, Lei Bai, and Jianru Xue. Enhanc-
ing mapless trajectory prediction through knowledge distil-
lation. arXiv preprint arXiv:2306.14177 , 2023. 2
[61] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lam-
bert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Rat-
nesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes,
et al. Argoverse 2: Next generation datasets for self-driving
perception and forecasting. In Proceedings of the Neural In-
formation Processing Systems Track on Datasets and Bench-
marks , 2021. 1
[62] Chenxin Xu, Maosen Li, Zhenyang Ni, Ya Zhang, and Si-
heng Chen. Groupnet: Multiscale hypergraph neural net-
works for trajectory prediction with relational reasoning. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 6498–6507, 2022. 2
[63] Chenxin Xu, Weibo Mao, Wenjun Zhang, and Siheng Chen.
Remember intentions: Retrospective-memory-based trajec-
tory prediction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 6488–
6497, 2022. 2
[64] Pei Xu, Jean-Bernard Hayet, and Ioannis Karamouzas. So-
cialvae: Human trajectory prediction using timewise latents.
InProceedings of the European Conference on Computer Vi-
sion, 2022. 2
[65] Yi Xu, Jing Yang, and Shaoyi Du. Cf-lstm: Cascaded
feature-based long short-term networks for predicting pedes-
trian trajectory. In Proceedings of the AAAI Conference on
Artificial Intelligence , pages 12541–12548, 2020. 1
[66] Yi Xu, Dongchun Ren, Mingxia Li, Yuehai Chen, Mingyu
Fan, and Huaxia Xia. Tra2tra: Trajectory-to-trajectory pre-
diction with a global social spatial-temporal attentive neural
network. IEEE Robotics and Automation Letters , 6(2):1574–
1581, 2021. 1
[67] Yi Xu, Lichen Wang, Yizhou Wang, and Yun Fu. Adaptive
trajectory prediction via transferable gnn. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6520–6531, 2022. 1, 2
[68] Yi Xu, Armin Bazarjani, Hyung-gun Chi, Chiho Choi, and
Yun Fu. Uncovering the missing pattern: Unified framework
towards trajectory imputation and prediction. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 9632–9643, 2023. 1, 2
[69] Yiteng Xu, Peishan Cong, Yichen Yao, Runnan Chen, Yue-
nan Hou, Xinge Zhu, Xuming He, Jingyi Yu, and Yuexin
Ma. Human-centric scene understanding for 3d large-scale
scenarios. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 20349–20359, 2023.
1
[70] Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, and Shuai
Yi. Spatio-temporal graph transformer networks for pedes-
trian trajectory prediction. In Proceedings of the European
Conference on Computer Vision , pages 507–523, 2020. 2
[71] Jiahui Yu and Thomas S Huang. Universally slimmable net-
works and improved training techniques. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 1803–1811, 2019. 2, 3
[72] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and
Thomas Huang. Slimmable neural networks. In Proceed-
15236
ings of the International Conference on Learning Represen-
tations , 2019. 2, 3
[73] Ye Yuan, Xinshuo Weng, Yanglan Ou, and Kris M Kitani.
Agentformer: Agent-aware transformers for socio-temporal
multi-agent forecasting. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 9813–
9823, 2021. 2, 3, 5, 6, 8
[74] Wei Zhan, Liting Sun, Di Wang, Haojie Shi, Aubrey
Clausse, Maximilian Naumann, Julius Kummerle, Hendrik
Konigshof, Christoph Stiller, Arnaud de La Fortelle, et al.
Interaction dataset: An international, adversarial and coop-
erative motion dataset in interactive driving scenarios with
semantic maps. arXiv preprint arXiv:1910.03088 , 2019. 1,
6
[75] Zikang Zhou, Luyao Ye, Jianping Wang, Kui Wu, and Ke-
jie Lu. Hivt: Hierarchical vector transformer for multi-agent
motion prediction. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
8823–8833, 2022. 3, 5, 6, 8
[76] Zikang Zhou, Jianping Wang, Yung-Hui Li, and Yu-Kai
Huang. Query-centric trajectory prediction. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 17863–17873, 2023. 1, 3, 4, 5
[77] Yanliang Zhu, Dongchun Ren, Yi Xu, Deheng Qian, Mingyu
Fan, Xin Li, and Huaxia Xia. Simultaneous past and current
social interaction-aware trajectory prediction for multiple in-
telligent agents in dynamic scenes. ACM Transactions on
Intelligent Systems and Technology , 13(1):1–16, 2021. 1
15237
