SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion
for Category-Level Pose Estimation
Yamei Chen1,∗, Yan Di1,∗, Guangyao Zhai1,2,†, Fabian Manhardt3, Chenyangguang Zhang4,
Ruida Zhang4, Federico Tombari1,3, Nassir Navab1and Benjamin Busam1,2,5
1Technical University of Munich2Munich Center for Machine Learning
3Google4Tsinghua University53dwe.ai
https://github.com/NOrangeeroli/SecondPose.git
Abstract
Category-level object pose estimation, aiming to predict
the 6D pose and 3D size of objects from known categories,
typically struggles with large intra-class shape variation.
Existing works utilizing mean shapes often fall short of cap-
turing this variation. To address this issue, we present Sec-
ondPose, a novel approach integrating object-specific ge-
ometric features with semantic category priors from DI-
NOv2. Leveraging the advantage of DINOv2 in provid-
ing SE(3)-consistent semantic features, we hierarchically
extract two types of SE(3)-invariant geometric features to
further encapsulate local-to-global object-specific informa-
tion. These geometric features are then point-aligned with
DINOv2 features to establish a consistent object represen-
tation under SE(3) transformations, facilitating the map-
ping from camera space to the pre-defined canonical space,
thus further enhancing pose estimation. Extensive exper-
iments on NOCS-REAL275 demonstrate that SecondPose
achieves a 12.4% leap forward over the state-of-the-art.
Moreover, on a more complex dataset HouseCat6D which
provides photometrically challenging objects, SecondPose
still surpasses other competitors by a large margin.
1. Introduction
Category-level pose estimation involves estimating the
complete 9 degrees-of-freedom (DoF) object pose, encom-
passing 3D rotation, 3D translation, and 3D metric size, for
arbitrary objects within a known set of categories. This task
has garnered significant research interest due to its essen-
tial role in various applications, including the AR/VR in-
dustry [30, 38, 40, 55], robotics [51, 52, 54], and scene un-
∗Equal contributions.
†Corresponding author (e-mail: guangyao.zhai@tum.de ).
Figure 1. Categorical SE(3)-consistent features. We visualize
our fused features by PCA. Colored points highlight the most cor-
responding parts, where our proposed feature achieves consistent
alignment cross instances (left vs. middle) and maintains consis-
tency on the same instance of different poses (middle vs. right).
derstanding [1, 7, 53]. In contrast to traditional instance-
level pose estimation methods [6,21], which rely on specific
3D CAD models for each target object, the category-level
approach necessitates greater adaptability to accommodate
inherent shape diversity within each category. Effectively
addressing intra-class shape variations has thus become a
central focus, crucial for real-world applications where ob-
jects within a category may exhibit significant differences
in shape while sharing the same general category label.
Mean Shape vs.Semantic Priors. One common ap-
proach to handle intra-class shape variation involves us-
ing explicit mean shapes as prior knowledge [24, 39, 57].
These methods typically consist of two functional modules:
one for reconstructing the target object by slightly deform-
ing the mean shape and another for regressing the 9D pose
based on the reconstructed object [24,39] or enhanced inter-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9959
mediate features [57]. These methods assume that the mean
shape can perfectly encapsulate the structural information
of objects within each category, thus achieving reconstruc-
tion of the target object with minimal deformation is fea-
sible. However, this assumption does not hold in reality.
Objects within the same category, such as chairs, may have
fundamental structural differences, leading to the failure of
such methods.
Recently, self-supervised learning with large vision
models has experienced a significant leap forward, among
which DINOv2 [32], due to its exceptional performance in
providing semantically consistent patch-wise features, has
gained great attention. In particular, various methods [56]
utilize semantic features from DINOv2 as essential priors to
understand the object. In the field of pose estimation, com-
pared to category-specific mean shapes, DINOv2 demon-
strates superior generalization capabilities in object rep-
resentation across each category, thanks to its large-scale
training data and advanced training strategy. ZSP [12] di-
rectly leverage DINOv2 features for zero-shot construction
of semantic correspondences between objects under differ-
ent camera viewpoints, and then estimates the pose with
RANSAC. POPE [10] and CNOS [31] harness DINOv2 to
refine the object detection, thus implicitly boosting the ac-
curacy of pose estimation. However, to our knowledge, cur-
rently there exists no method that explores how to fuse DI-
NOv2 features with object-specific features to directly en-
hance the performance of category-level pose estimation.
In this paper, we present SecondPose , a novel
method that fuses SE(3)-Consistent Dual-stream features
to enhance category-level Pose estimation. Leverag-
ing DINOv2’s patch-wise SE(3)-consistent semantic fea-
tures, we extract two types of SE(3)-invariant geometric
features—pair-wise distance and pair-wise angles—to en-
capsulate object-specific cues. We hierarchically aggregate
geometric features within support regions of increasing ra-
dius to encode local-to-global object structure information.
These features are then point-aligned with DINOv2 features
to establish a unified object representation that is consis-
tent under SE(3) transformations. Specifically, given an
RGB-D image capturing the target object, we first back-
project the depth map to generate the respective point cloud,
which is then fed into our Geometric and Semantic Streams
(Fig. 2.A-B) to extract the corresponding features for our
dual-stream fusion (Fig. 2.C). The fused features denoted
asSECOND are finally fed into an off-the-shelf pose esti-
mator [27] (Fig. 2.D) to regress the 9D pose.
SE(3)-Consistent Fusion vs. Direct Fusion. Alter-
natively, one could think of directly concatenating DI-
NOv2 features with the back-projected point in a point-wise
manner, without extracting SE(3)-invariant geometric fea-
tures. However, our instead proposed SE(3)-consistent fu-
sion holds two important advantages over such a straightfor-ward approach. First, while DINOv2 is trained solely with
RGB images, the incorporation of geometric features from
the point cloud enriches it with valuable local-to-global 3D
structural information. This enrichment proves particularly
advantageous in handling diverse object shapes within a
given category. Second, our SE(3)-consistent object rep-
resentation modifies the underlying pose estimation process
from{point cloud −→ canonical space }to{point cloud
−→SE(3)-consistent representation −→canonical space }.
In this optimized pipeline, the second stage – transitioning
from our object representation to the human-defined canon-
ical space – is consistent under SE(3) transformations. (ap-
proximately invariant, see Fig. 4) This consistency signifi-
cantly simplifies the pose estimation process, as the pose es-
timator only needs to operate within the second stage. Fur-
ther, this streamlined approach not only enhances the accu-
racy of pose estimation but also contributes to the efficiency
of the overall method.
To summarize, our main contributions are threefold:
1. We present SecondPose , the first method to directly
fuse object-specific hierarchical geometric features
with semantic DINOv2 features for category-level
pose estimation.
2. Our SE(3)-consistent dual-stream feature fusion strat-
egy yields a unified object representation that is robust
under SE(3) transformations, better suited for down-
stream pose estimation.
3. Extensive evaluation proves that our SE(3)-consistent
fusion strategy significantly boosts pose estimation
performance even under severe occlusion and clutter,
enabling real-world applications.
2. Related Works
Instance-Level Pose Estimation Instance-level pose es-
timation focuses on determining the 3D rotation and 3D
translation of known objects given their 3D CAD mod-
els. Recent methods can be mainly categorized into three
types: direct pose regression [20, 48], methods that estab-
lish 2D-3D correspondences through keypoint detection or
pixel-wise 3D coordinate estimation [33, 43, 50], and ap-
proaches that learn pose-sensitive embeddings for subse-
quent pose retrieval [37]. While most keypoints based ap-
proaches rely on the P nP algorithm [33, 36, 50] to solve
for pose, some methods instead employ neural networks
to learn the optimization step [43]. As for RGB-D input,
traditional methodologies often rely on hand-crafted fea-
tures [9,16]. Some more recent approaches [4,14,15,42,47]
instead extract features independently from RGB images
and point clouds, using dedicated CNNs and point cloud
networks. These individual features are then fused for di-
rect pose regression [4,42] or keypoint detection [14,15,47].
Despite significant progress, practical applications of these
9960
methods remain limited due to their restriction to a few ob-
jects and the need for 3D CAD models.
Category-Level Pose Estimation In the domain of
category-level pose estimation, the objective encompasses
predicting the 9DoF pose for any object, regardless if pre-
viously seen or novel, from a predefined set of categories.
This task is inherently more complex due to significant
intra-class variations in shape and texture. To address these
challenges, Wang et al. [44] developed the Normalized Ob-
ject Coordinate Space (NOCS), offering a unified represen-
tation framework. This approach involves mapping the ob-
served point cloud to the NOCS system, followed by pose
recovery via the Umeyama algorithm [41]. Alternatively,
CASS [2] introduces a learned canonical shape space, while
FS-Net [5] advocates for a decoupled representation of rota-
tion, focusing on direct pose regression. DualPoseNet [26]
employs dual networks for both explicit and implicit pose
prediction, ensuring consistency for refined pose estima-
tion. GPV-Pose [8] and OPA-3D [35] leverage geometric
insights in bounding box projection to augment the learn-
ing of pose-sensitive features specific to categories. HS-
Pose [59] proposed the HS-layer, a simple network struc-
ture that extends 3D graph convolution to extract hybrid
scope latent features from point cloud data. In contrast,
6-PACK [45] conducts pose tracking by means of seman-
tic keypoints, and CAPTRA [46] combines coordinate pre-
diction with direct regression for enhanced accuracy. Self-
Pose [49] utilizes optical flow to enhance the pose estima-
tion accuracy.
To address the issue of intra-class shape variations, sev-
eral works have focused on the incorporation of additional
shape priors. SPD [39] utilizes a PointNet autoencoder to
derive a prior point cloud for each category, representing
the average shape. This model is then adapted to fit spe-
cific observed instances, assigning the observed point cloud
to the reconstructed shape model. SGPA [3] dynamically
adjusts the shape prior based on structural similarities of
the observed instances. SAR-Net [22], while also employ-
ing shape priors, further leverages geometric attributes of
objects to enhance performance. ACR-Pose [11], instead
utilizes a shape prior-guided reconstruction network paired
with a discriminator to achieve high-quality canonical rep-
resentations.
Furthermore, recent research has introduced prior-free
methods that demonstrate performance comparable to ap-
proaches relying on priors. VI-Net [27] attains high
precision in object pose estimation by separating rota-
tion into viewpoint and in-plane rotations. Additionally,
IST-Net [28] achieves state-of-the-art performance on the
REAL275 benchmark by implicitly transforming camera-
space features to world-space counterparts without depend-
ing on priors.3. Method
The objective of SecondPose is to estimate the 9DoF ob-
ject pose from a single RGB-D image. In particular, given
an RGB-D image capturing the target object from a set of
known categories, our goal is to recover its full 9DoF ob-
ject pose, including the R∈SO(3)and the 3Dtranslation
t∈R3and the 3Dmetric size s∈R3.
3.1. Overview.
As illustrated in Fig 2, SecondPose mainly consists of 3
modules to predict object pose from a single RGB-D input,
i.e.i) the extraction of relevant geometric features Fgand
semantic features Fs, ii) the dual-stream feature fusion to
build our SE(3)-consistent object representation Ff, iii) the
final pose regression from the extracted representation.
3.2. Semantic Category Prior From DINOv2
DINOv2 is an implicit rotation learner We use DINOv2
[32] as our image feature extractor. As shown in [56], DI-
NOv2 can extract semantic-aware information from RGB
images that can be well leveraged to establish zero-shot se-
mantic correspondences, rendering it an excellent method
for rich semantic information extraction.
As for estimating the 3D rotation, such extra semantic-
aware information can provide a noticeable boost in per-
formance. Exemplary, imagine that the z-axis commonly
points to the top side of the object in model space, the y-
axis always points to the front side of the object, and the
x-axis always points to the left side of the object. Harness-
ing the semantic information given by DINOv2, the model
can more easily identify the top, front, and left sides of the
object, thus turning rotation estimation into a much sim-
pler task. Moreover, DINOv2 features additionally contain
global information about the object, including the object
category and pose. Such information can thus serve as a
good global prior to our method.
Deeper DINOv2 features We use the ”token” facet from
the last (11th.) layer as our extracted semantic feature. Es-
sentially, [56] has demonstrated that the features of deeper
layers exhibit optimal semantic matching performance, thus
providing improved consistency in terms of semantic cor-
respondence across different objects. In addition, features
from deeper layers also possess more holistic semantic in-
formation. A visualization piece is shown in Fig. 2.A.
Direct pose estimation from DINOv2 As aforemen-
tioned, the ad-hoc fusion of DINOv2 features with the back-
projected points exhibits several downsides. First, DINOv2
extracts information only from RGB images; hence, the
contained geometric information is limited. Second, as
we make use of deeper-layer features from DINOv2 for a
9961
DINOv2RotationHeadTranslationHeadA. Matching-awareSemanticStream
B. Hierarchical-PanelGeometricStreamFsFgC. Dual-StreamFusion
SECONDFfOutputCropCropSizeHeadRtsHP-PPFCℒ!ℒ"ℒ#ℒ$FcPP,FcP,FcD. PoseEstimatorFigure 2. Illustration of SecondPose. Semantic features are extracted using the DINOv2 model (A), and the HP-PPF feature is computed
on the point cloud (B). These features, combined with RGB values, are fused into our SECOND feature Ff(C) using stream-specific
modules Ls,Lg,Lc, and a shared module Lffor concatenated features. The resulting fused features, in conjunction with the point cloud,
are utilized for pose estimation (D).
b) Hierarchical Panels
a) Point Pair Features!!!"n!n"#!,"$!,"%!,""%
c) HP-PPF"%#%,'#%,(#%,)…
Figure 3. Hierarchical panel-based geometric features. The
inner panel contains points that are close to the point of interest,
and outer panels contain points far from the point of interest.
more holistic representation, the local detailed information
is blurred to some extent. To complement DINOv2 features
in these aspects, we thus need to combine them with geo-
metric features containing local information for better de-
scriptive power.
3.3. Hierarchical Geometric Features
The stream pipeline is shown in Fig. 2.B. Our geomet-
ric embedding in this stream is based on the calculation of
pair-wise SE(3)-equivariant Point Pair Features (PPFs) [9].
We construct our SE(3)-invariant coordinate representation
by aggregating the PPFs between the point of interest and
neighborhood points in the multiple panels centered on
it. We hierarchically concatenate the corresponding SE(3)-
invariant coordinate representations in each panel to enrich
the representation power of our geometric features HP-PPF.
Fig. 3.c provides a visualization of HP-PPF.
Point Pair Features PPFs A comprehensive example is
shown in Fig. 3.a. Given an object point cloud denoted as
P, we consider each pair of points (pi, pj)where pi, pj∈
P. Associated with each point, local normal vectors niand
njare computed at each piandpj, respectively. The finalpairwise feature between piandpjis defined as
fi,j= [di,j, αi,j, βi,j, θi,j], (1)
where di,j=∥pj−pi∥describes the Euclidean distance
between points piandpj.αi,j=∠(ni, pj−pi)rep-
resents the angular deviation between the normal vector
niat point piand the vector extending from pitopj.
βi,j=∠(nj, pj−pi)denotes the angle subtended by the
normal vector njat point pjwith the aforementioned vec-
tor from pitopj.θi,j=∠(nj,ni)denotes the angular
disparity between the normal vectors njandniat points
pjandpi, respectively. Notice that thanks to its locality,
this descriptor is invariant under SE(3).
Geometric Feature Panel Based on PPFs, we propose
panel-based PPFs to construct our geometric representation,
which increases the perception field while maintaining the
merit of the locality. For each point piin the point cloud
P, there is a support panel Si⊆Pwhose cardinality
si=|Si|. For all points pj∈ Si, we calculate the PPF
fi,jbetween pi, pjand the local coordinate representation
fi
lofpiis then obtain as average the average according to
fi
l=1
si(X
jdi,j,X
jαi,j,X
jβi,j,X
jθi,j). (2)
From Single to Hierarchical Panels Even though the
mean aggregation in the panel can take the neighboring
points into account, the inherent local representation lim-
its its representational power, as the features brought by
normals ni,njare noisy when constraining the perception
field. Inspired by CNNs, which extract hierarchical features
from local to global, we hierarchically sample multiple pan-
els from local to global, as shown in Fig. 3.b. Specifi-
cally, for a point set Pwith cardinality |P|, for integers
9962
(k0, k1, k2, ..., k l)satisfying 0 =k0< k 1< k 2< ... <
kl=|P| −1, for each point pi∈Pwe first rank its dis-
tance to any other points in Pfrom smallest to largest:
ri,j=sort(di,j) (3)
and construct support panels:
Si,m={pj∈P|km−1< ri,j≤km},1≤m≤l,(4)
withlbeing the number of employed panels. We then calcu-
late the corresponding pose-invariant coordinate representa-
tions fi,mfor each panel Si,mand concatenate them to get
the point-wise geometric features with
fi
g=fi,1
l⊕fi,2
l⊕...⊕fi,l
l. (5)
Thereby, for smaller k, the support panel is composed of
points that are closer to the point of interest, whereas for
larger k, the support panel consists of points that are farther
from the point of interest. By concatenating features calcu-
lated by panels of different scales, we can harness geomet-
ric features in a way that balances details of local geometric
landscapes and global instance-wise shape information. We
experimentally show in Sec. 4 that our design performs bet-
ter than the usual single-panel descriptor.
3.4. SE(3)-Consistent Feature Fusion
Fusion Strategy We fuse the DINOv2 features, the ge-
ometric feature and RGB values, as shown in Fig. 2.C. In
particular, we use VI-Net [27] as an example of the pose es-
timator, first projecting each feature to each feature stream
Fand 3D point cloud P={pi}to a spherical feature
mapF. To this end, we divide the sphere uniformly into
W×Halong the azimuth and elevation axes, following VI-
Net [27]. We assign the feature of the point with the largest
distance to each bin. When there is no point in the region,
we set 0 in the bin. For each feature map Fi∈ {Fg, Fs, Fc}
representing the geometric feature, the DINOV2 feature,
and the respective RGB value, we employ a separate ResNet
model Lias feature extractor. The outputs of these individ-
ual feature extractors are then concatenated to form the in-
put to another ResNet for feature fusion, obtaining Ffalso
denoted as SECOND ,
Ff=Lf(Lg(Fg)⊕ Ls(Fs)⊕ Lc(Fc)). (6)
Advantages of SE(3)-Consistent Fusion The Design of
a SE(3)-consistent fusion is an integral part of the improved
quality of our method. As for the 3D rotation, we are learn-
ing a mapping from the space of point clouds and its fea-
tures (P, F)∈Rn×3×Rn×Cto space of 3D rotations
R∈SO(3)
Φ :Rn×3×Rn×C7→SO(3). (7)This mapping Φshould ensure rotation-equivariance, mean-
ing that
Φ(RxP, ψRx(F)) =RxΦ(P, F),∀Rx∈SO(3),(8)
where ψRxis the transformation applied to the feature when
rotating the point cloud by Rx. This rotation-equivariance
relation is essential for the learned model to generalize well
on unseen data. Without such equivariance embedded in the
model structure, these relation needs to be learned through
large amounts of data, which is limited by the scale of the
data. Our design of SE(3)-consistent features are approxi-
mately rotation-invariant, hence
ψRx(F)≈F,∀Rx∈SO(3), (9)
eliminating the effect of ψRxin Eq. (8), and thus making
learning of the rotation-equivariance relationship easier.
3.5. SecondPose Training and Inference
Following [27], we leverage a lightweight Point-
Net++ [34] as the translation and size estimation heads.
Given an RGB-D image, we first segment the object of in-
terest using Mask-RCNN [13], similar to [8, 27]. We then
randomly select Npoints from the back-projected 3D point
clouds P∈Rn×3with RGB features Fcand use them to
estimate the translation and size, as shown in Fig. 2.D.
The core of our method is thus developed to focus on the
more challenging task of 3D rotation estimation. We essen-
tially train a separate translation-size network and rotation
network. For the translation-size network, we adopt the L1
loss for both size and translation with
Lts=λt|tpred−tgt|+λs|spred−sgt|. (10)
For the 3D rotation, we instead directly predict the 9D ro-
tation matrix, which we optimize via the L1-loss according
to
LR=|Rpred−Rgt|. (11)
During training, the ground truth translation and size are
used to center and normalize the point cloud before rotation
estimation, while during inference the predicted size and
translation are instead utilized for normalization.
4. Experiment
4.1. Experimental Setup.
Datasets We conduct our experiments on the common
9D pose estimation benchmarks NOCS-REAL275 [44],
NOCS-CAMERA25 [44] as well as HouseCat6D [19]
datasets. NOCS-REAL275 is a real-world dataset with 13
scenes containing objects from 6 different categories; 4,300
images of 7 scenes are used as a training set, while the
other 2,750 images of 6 scenes form the test set. NOCS-
CAMERA25 is a synthetic dataset containing 300k images
9963
Method Mean REAL275
Name Shape Priors IoU 75∗5◦2 cm 5◦5 cm 10◦2 cm 10◦5 cm
SPD [39] ✓ 27.0 19.3 21.4 43.2 54.1
CR-Net [45] ✓ 33.2 27.8 34.3 47.2 60.8
CenterSnap-R [17] ✓ - - 29.1 - 64.3
ACR-Pose [11] ✓ - 31.6 36.9 54.8 65.9
SAR-Net [22] ✓ - 31.6 42.3 50.3 68.3
SSP-Pose [58] ✓ - 34.7 44.6 - 77.8
SGPA [3] ✓ 37.1 35.9 39.6 61.3 70.7
RBP-Pose [57] ✓ - 38.2 48.1 63.1 79.2
SPD + CATRE [29] ✓ 43.6 45.8 54.4 61.4 73.1
DPDN [25] ✓ - 46.0 50.7 70.4 78.4
FS-Net [5] × - - 28.2 - 60.8
DualPoseNet [26] × 30.8 29.3 35.9 50.0 66.8
GPV-Pose [8] × - 32.0 42.9 - 73.3
SS-ConvNet [23] × - 36.6 43.4 52.6 63.5
HS-Pose [59] × - 46.5 55.2 68.6 82.7
IST-Net [28] × - 47.5 53.4 72.1 80.5
VI-Net [27] × 48.3 50.0 57.6 70.8 82.1
SecondPose (Ours) Semantic Priors 49.7 56.2 63.6 74.7 86.0
Table 1. Quantitative comparisons of different methods for category-level 6D object pose estimation on REAL275 [44]. ‘*’ denotes the
CATRE [29] IoU metrics. The best results are in bold, and the second best results are underlined.
with objects from the same categories as NOCS-REAL275.
HouseCat6D is a comprehensive multi-modal real-world
dataset, featuring 194 high-fidelity 3D models of household
items of 10 categories. The collection encompasses trans-
parent and reflective objects situated in 41 scenes, present-
ing a wide range of viewpoints, challenging occlusions, and
devoid of markers.
Evaluation Metrics As for the NOCS-REAL275 dataset,
we report the mean Average Precision (mAP) of
5◦2cm,5◦5cm,10◦2cm,10◦5cm metrics. n◦mcmdenotes
the percentage of prediction with rotation prediction error
within n degrees and translation prediction error within m
centimeters. We also report mAP of 3D Intersection over
Union (IoU) at the threshold of 75%. For the HouseCat6D
dataset, we again report the mAP of 3D IoU under thresh-
olds of 25% and50%.
Efficiency Our method achieves an inference speed of 9
FPS. Excluding the running time of DINOv2, our inference
speed increases to 10 FPS.
Implementation Details We use MaskRCNN [13] to seg-
ment the objects of interest from the input image. We
then combine point-wise radial distances, RGB values, and
semantic-aware features from DINOv2 together with our
proposed local-to-global SE(3)-invariant geometric features
as input for further processing. Next, for the RGB valuesand the point-wise radial distances, we sample 2048 points
from the point cloud. For DINOv2 features, we first crop
the image by the bounding box around the object of inter-
est and then resize the image to a resolution of 210×210.
Finally, for our geometric features, we sample 300 points
from previously sampled 2048 points and estimate point-
wise normal vectors using the 10 nearest neighbors. To train
our model on the NOCS dataset, we use a mixture of 25%
real-world images from the training set of REAL 275 and
75% synthetic images from the CAMERA25 training set,
similar to [44]. For all experiments, we train our models
with batch size 48 on a single NVIDIA 3090 GPU to the
40th. epoch.
4.2. Comparison with State-of-the-Art Methods
In Tab. 1, we compare SecondPose with the state-of-the-
art on NOCS-REAL275 dataset. As can be easily observed,
our method outperforms all state-of-the-art approaches, in-
cluding the recent VI-Net [27], by a large margin on all
metrics. More specifically, our method respectively ex-
ceeds VI-Net for 5◦2 cm and10◦2 cm by 6.2% and 3.9%,
demonstrating the effectiveness of our SE(3)-consistent fea-
ture fusion design. When comparing with DPDN [24], the
best method using mean shape prior, our improvements in
5◦2 cm and5◦5 cm metrics amount to 10.2% and 12.8%.
We show qualitative results in Figure 4. It can be ob-
served that SecondPose is more robust when handling ob-
jects with large intra-class variations, such as camera . In
Tab. 2, we evaluate our method on the HouseCat6D dataset.
9964
VI-NetSecondPoseGroundTruth
Figure 4. Qualitative comparison on REAL275 [44]. We compare our prediction with ground truth and the prediction of our baseline,
VI-Net [27]. Our approach achieves significantly higher precision in rotation estimation.
VI-NetSecondPoseGroundTruth
Figure 5. Qualitative comparison on HouseCat6D [18]. We com-
pare our prediction with ground truth and the prediction of our
baseline, VI-Net [27].
Our method can again exceed current state-of-the-art meth-
ods by a large margin. As for the IoU 50metric, our method
outperforms the second-best method VI-Net by 9.7% on av-
erage. Additional qualitative results can be found in Fig.
4.3. Limitations
Our method’s efficacy is restricted by the constraints of
DINOv2 due to our utilization of its features. When DI-
NOv2 is unable to provide meaningful semantic informa-
tion for specific images, our approach is unable to surpassthis limitation.
4.4. Ablation Studies
To confirm the efficacy of our design choices, we con-
duct several ablation studies on the NOCS-REAL275 [44]
dataset.
[AS-1] Efficacy of employing semantic and geomet-
ric features. To show the effectiveness of our semantic-
geometric-feature-fusion, we train the proposed model in 3
different variations: i) without semantic feature, ii) without
geometric feature, and iii) without both semantic and geo-
metric features. The results are presented in Tab. 3 (B0) -
(B2). When considering the strict 5◦2cm metric, it turns out
that removing semantic features, geometric features or both
always leads to a large decrease in performance. In particu-
lar, the performance respectively drops by 5.1%, 1.1% and
6.3%.
[AS-2] Efficacy of individual geometric feature. We
further run ablations on the four geometric features, d,α,
β,θ. The corresponding results are presented again in Ta-
ble 3 under (C0) - (C3). As can be observed, removing
any component from the geometric feature leads to a strict
drop in performance. Exemplary, for the 5◦2cm metric the
performance drops by at least 1.1%. To summarize, each
geometric feature contributes to the expressiveness of the
geometric representation.
[AS-3] Efficacy of hierarchical panel construction. As
shown in Tab. 3 (D0), when the hierarchical panel is substi-
tuted by KNN with 10 nearest neighbors, the 5◦2cm metric
undergoes a decrease by 0.8%. This demonstrates the im-
9965
Approach IoU 25/ IoU 50 Bottle Box Can Cup Remote Teapot Cutlery Glass Tube Shoe
NOCS [44] 50.0 / 21.2 41.9 / 5.0 43.3 / 6.5 81.9 / 62.4 68.8 / 2.0 81.8 / 59.8 24.3 / 0.1 14.7 / 6.0 95.4 / 49.6 21.0 / 4.6 26.4 / 16.5
FS-Net [5] 74.9 / 48.0 65.3 / 45.0 31.7 / 1.2 98.3 / 73.8 96.4 / 68.1 65.6 / 46.8 69.9 / 59.8 71.0 / 51.6 99.4 / 32.4 79.7 / 46.0 71.4 / 55.4
GPV-Pose [8] 74.9 / 50.7 66.8 / 45.6 31.4 / 1.1 98.6 / 75.2 96.7 / 69.0 65.7 / 46.9 75.4 / 61.6 70.9 / 52.0 99.6 / 62.7 76.9 / 42.4 67.4 / 50.2
VI-Net [27] 80.7 / 56.4 90.6 / 79.6 44.8 / 12.7 99.0 / 67.0 96.7 / 72.1 54.9 / 17.1 52.6 / 47.3 89.2 / 76.4 99.1 / 93.7 94.9 /36.0 85.2 / 62.4
SecondPose (Ours) 83.7 /66.1 94.5 /79.8 54.5 /23.7 98.5/93.2 99.8 /82.9 53.6 / 35.4 81.0 /71.0 93.5 / 74.4 99.3 /92.5 75.6 / 35.6 86.9 /73.0
Table 2. Overall and class-wise evaluation of 3D IoU(at 25%, 50%) on the dataset HouseCat6D [18]. The best results are in bold.
Row Method IoU 75∗5◦2 cm 5◦5 cm 10◦2 cm 10◦5 cm
A0 SecondPose (baseline) 49.7 56.2 63.6 74.7 86.0
B0 w/o semantic 48.0 51.1 58.9 71.6 82.4
B1 w/o geometric 49.5 55.1 62.3 73.7 84.8
B2 w/o semantic+geometric 48.5 49.9 57.4 70.4 80.8
C0 w/o d in Eq. 1 49.1 55.1 63.1 73.7 85.0
C1 w/o αin Eq. 1 49.3 54.7 62.8 73.1 84.7
C2 w/o βin Eq. 1 49.6 54.8 62.7 74.6 86.7
C3 w/o θin Eq. 1 49.5 55.1 63.1 74.2 85.6
D0 KNN Panel (10 nearest neighbors) 49.4 55.4 63.1 73.7 85.5
E0 random rotation 5◦49.7 56.1 63.4 74.6 85.9
E1 random rotation 10◦49.4 55.8 63.5 74.4 85.8
E2 random rotation 15◦48.5 55.4 63.0 73.9 85.4
E3 random rotation 20◦47.9 54.5 62.4 73.2 85.1
F0 manual occlusion n = 16 49.7 56.0 63.6 74.8 86.2
F1 manual occlusion n = 8 49.5 55.7 63.2 74.3 85.6
F2 manual occlusion n = 4 46.7 52.5 60.9 71.5 84.6
G0 random perturbation s = 0.002 49.7 56.1 63.6 74.6 85.8
G1 random perturbation s = 0.005 49.6 55.8 63.4 74.4 86.0
G2 random perturbation s = 0.01 45.9 53.7 62.6 73.4 86.1
Table 3. Ablation Study on REAL275 [44]. ‘*’ denotes the CATRE [29] IoU metrics.
portance of our hierarchical panel construction, as it better
captures finer-grained local and global information.
[AS-4] Robustness under random rotation. To show
the robustness of our method under random rotation ap-
plied on point cloud, we perform experiments on test im-
ages when randomly rotating the entire point cloud by rota-
tion angle A[0◦, n◦], n = 5, 10, 15, 20, see Table 3 (E0) -
(E3). The results show that our method performs well under
these circumstances.
[AS-5] Robustness under manual occlusions We also
perform an additional experiment to show the robustness of
our method under various levels of occlusions. We manu-
ally mask out the object with different scale of rectangular
masks, whose length and width is set to 1/n of the length
and width of the original object bounding box. We further
run tests with n = 16, 8, 4 in Tab. 3 (F0) - (F2). When
undergoing only mild occlusion, i.e.n= 16 , the perfor-
mance is almost identical to the original result. Moreover,
even when dealing with very large occlusions of 1/4 of the
size of the object, the performance is still fairly strong with
only a small decrease of 3% for IoU 75.
[AS-6] Robustness Under Perturbation on PointCloud. Next, we also evaluate the robustness of our method
under random perturbations applied to the point clouds. To
this end, we add random noise sampled from a uniform dis-
tribution ranging from −0.5srto0,5sr, where sis the scale
factor and r is the average distance of the point cloud to the
object center. We test our model with s = 0.002, 0.005,0.01
in Table 3 G0-G2. We observe again that with mild pertur-
bation of s = 0.002, the performance is almost identical to
the original result, while with relatively large perturbation
of s = 0.01, the performance is still fairly strong with only a
small decrease of 3.8% for IoU 75.
5. Conclusion
In this paper, we propose SecondPose designing SE(3)-
consistent fusion of semantic and geometric features for
pose estimation. The two feature streams are proven to
complement each other and jointly contribute to improving
our method. To confirm the efficacy of our method, we ap-
ply our method on the challenging real-world category-level
6D object pose estimation datasets REAL275 and House-
Cat6D and exceed the current SOTA by a large margin.
9966
References
[1] Benjamin Busam, Marco Esposito, Simon Che’Rose, Nas-
sir Navab, and Benjamin Frisch. A stereo vision approach
for cooperative robotic movement therapy. In Proceedings
of the IEEE International Conference on Computer Vision
Workshops , pages 127–135, 2015. 1
[2] Dengsheng Chen, Jun Li, Zheng Wang, and Kai Xu. Learn-
ing canonical shape space for category-level 6d object pose
and size estimation. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
11973–11982, 2020. 3
[3] Kai Chen and Qi Dou. Sgpa: Structure-guided prior adapta-
tion for category-level 6d object pose estimation. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 2773–2782, 2021. 3, 6
[4] Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, and Ales
Leonardis. G2l-net: Global to local network for real-time 6d
pose estimation with embedding vector features. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , June 2020. 2
[5] Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, Linlin
Shen, and Ales Leonardis. Fs-net: Fast shape-based network
for category-level 6d object pose estimation with decoupled
rotation mechanism. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
1581–1590, 2021. 3, 6, 8
[6] Yan Di, Fabian Manhardt, Gu Wang, Xiangyang Ji, Nassir
Navab, and Federico Tombari. So-pose: Exploiting self-
occlusion for direct 6d pose estimation. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 12396–12405, October 2021. 1
[7] Yan Di, Chenyangguang Zhang, Ruida Zhang, Fabian Man-
hardt, Yongzhi Su, Jason Rambach, Didier Stricker, Xi-
angyang Ji, and Federico Tombari. U-red: Unsupervised 3d
shape retrieval and deformation for partial point clouds. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 8884–8895, 2023. 1
[8] Yan Di, Ruida Zhang, Zhiqiang Lou, Fabian Manhardt, Xi-
angyang Ji, Nassir Navab, and Federico Tombari. Gpv-pose:
Category-level object pose estimation via geometry-guided
point-wise voting. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6781–6791, 2022. 3, 5, 6, 8
[9] Bertram Drost, Markus Ulrich, Nassir Navab, and Slobodan
Ilic. Model globally, match locally: Efficient and robust 3d
object recognition. In CVPR , pages 998–1005. Ieee, 2010.
2, 4
[10] Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, De-
jia Xu, Hanwen Jiang, and Zhangyang Wang. Pope: 6-dof
promptable pose estimation of any object, in any scene, with
one reference. arXiv preprint arXiv:2305.15727 , 2023. 2
[11] Zhaoxin Fan, Zhengbo Song, Jian Xu, Zhicheng Wang, Ke-
jian Wu, Hongyan Liu, and Jun He. Acr-pose: Adversarial
canonical representation reconstruction network for category
level 6d object pose estimation, 2021. 3, 6
[12] Walter Goodwin, Sagar Vaze, Ioannis Havoutis, and Ingmar
Posner. Zero-shot category-level object pose estimation. InEuropean Conference on Computer Vision , pages 516–532.
Springer, 2022. 2
[13] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE international
conference on computer vision , pages 2961–2969, 2017. 5,
6
[14] Yisheng He, Haibin Huang, Haoqiang Fan, Qifeng Chen, and
Jian Sun. Ffb6d: A full flow bidirectional fusion network for
6d pose estimation. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 3003–3013, June 2021. 2
[15] Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang
Fan, and Jian Sun. Pvn3d: A deep point-wise 3d keypoints
voting network for 6dof pose estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2020. 2
[16] Stefan Hinterstoisser, Stefan Holzer, Cedric Cagniart, Slobo-
dan Ilic, Kurt Konolige, Nassir Navab, and Vincent Lepetit.
Multimodal templates for real-time detection of texture-less
objects in heavily cluttered scenes. In Proceedings of the
IEEE International Conference on Computer Vision , pages
858–865. IEEE, 2011. 2
[17] Muhammad Zubair Irshad, Thomas Kollar, Michael Laskey,
Kevin Stone, and Zsolt Kira. Centersnap: Single-shot multi-
object 3d shape reconstruction and categorical 6d pose and
size estimation, 2022. 6
[18] HyunJun Jung, Shun-Cheng Wu, Patrick Ruhkamp,
Guangyao Zhai, Hannah Schieber, Giulia Rizzoli, Pengyuan
Wang, Hongcheng Zhao, Lorenzo Garattoni, Sven Meier,
Daniel Roth, Nassir Navab, and Benjamin Busam. House-
cat6d – a large-scale multi-modal category level 6d object
pose dataset with household objects in realistic scenarios,
2023. 7, 8
[19] HyunJun Jung, Guangyao Zhai, Shun-Cheng Wu, Patrick
Ruhkamp, Hannah Schieber, Pengyuan Wang, Giulia Riz-
zoli, Hongcheng Zhao, Sven Damian Meier, Daniel Roth,
Nassir Navab, et al. Housecat6d–a large-scale multi-modal
category level 6d object pose dataset with household ob-
jects in realistic scenarios. arXiv preprint arXiv:2212.10428 ,
2022. 5
[20] Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan
Ilic, and Nassir Navab. Ssd-6d: Making rgb-based 3d detec-
tion and 6d pose estimation great again. In Proceedings of
the IEEE international conference on computer vision , pages
1521–1529, 2017. 2
[21] Yann Labb ´e, Justin Carpentier, Mathieu Aubry, and Josef
Sivic. Cosypose: Consistent multi-view multi-object 6d pose
estimation. In Computer Vision–ECCV 2020: 16th Euro-
pean Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part XVII 16 , pages 574–591. Springer, 2020. 1
[22] Haitao Lin, Zichang Liu, Chilam Cheang, Yanwei Fu,
Guodong Guo, and Xiangyang Xue. Sar-net: Shape align-
ment and recovery network for category-level 6d object pose
and size estimation, 2022. 3, 6
[23] Jiehong Lin, Hongyang Li, Ke Chen, Jiangbo Lu, and Kui
Jia. Sparse steerable convolutions: An efficient learning of
se(3)-equivariant features for estimation and tracking of ob-
9967
ject poses in 3d space. In Neural Information Processing
Systems , 2021. 6
[24] Jiehong Lin, Zewei Wei, Changxing Ding, and Kui Jia.
Category-level 6d object pose and size estimation using self-
supervised deep prior deformation networks. In European
Conference on Computer Vision , pages 19–34. Springer,
2022. 1, 6
[25] Jiehong Lin, Zewei Wei, Changxing Ding, and Kui Jia.
Category-level 6d object pose and size estimation using self-
supervised deep prior deformation networks, 2022. 6
[26] Jiehong Lin, Zewei Wei, Zhihao Li, Songcen Xu, Kui Jia,
and Yuanqing Li. Dualposenet: Category-level 6d object
pose and size estimation using dual pose network with re-
fined learning of pose consistency. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 3560–3569, 2021. 3, 6
[27] Jiehong Lin, Zewei Wei, Yabin Zhang, and Kui Jia. Vi-net:
Boosting category-level 6d object pose estimation via learn-
ing decoupled rotations on the spherical representations. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 14001–14011, 2023. 2, 3, 5, 6, 7, 8
[28] Jianhui Liu, Yukang Chen, Xiaoqing Ye, and Xiaojuan Qi.
Ist-net: Prior-free category-level pose estimation with im-
plicit space transformation, 2023. 3, 6
[29] Xingyu Liu, Gu Wang, Yi Li, and Xiangyang Ji. Catre: It-
erative point clouds alignment for category-level object pose
refinement, 2022. 6, 8
[30] Eric Marchand, Hideaki Uchiyama, and Fabien Spindler.
Pose estimation for augmented reality: a hands-on survey.
IEEE transactions on visualization and computer graphics ,
22(12):2633–2651, 2015. 1
[31] Van Nguyen Nguyen, Thibault Groueix, Georgy Ponimatkin,
Vincent Lepetit, and Tomas Hodan. Cnos: A strong base-
line for cad-based novel object segmentation. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 2134–2140, 2023. 2
[32] Maxime Oquab, Timoth ´ee Darcet, Theo Moutakanni, Huy V .
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-
sell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-
Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-
las Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,
Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-
janowski. Dinov2: Learning robust visual features without
supervision, 2023. 2, 3
[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hu-
jun Bao. Pvnet: Pixel-wise voting network for 6dof pose
estimation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , June
2019. 2
[34] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. Advances in neural information
processing systems , 30, 2017. 5
[35] Yongzhi Su, Yan Di, Guangyao Zhai, Fabian Manhardt, Ja-
son Rambach, Benjamin Busam, Didier Stricker, and Fed-
erico Tombari. Opa-3d: Occlusion-aware pixel-wise aggre-gation for monocular 3d object detection. IEEE Robotics and
Automation Letters , 8(3):1327–1334, 2023. 3
[36] Yongzhi Su, Mahdi Saleh, Torben Fetzer, Jason Rambach,
Nassir Navab, Benjamin Busam, Didier Stricker, and Fed-
erico Tombari. Zebrapose: Coarse to fine surface encod-
ing for 6dof object pose estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6738–6748, 2022. 2
[37] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian
Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3d
orientation learning for 6d object detection from rgb images.
InProceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 699–715, 2018. 2
[38] David Joseph Tan, Nassir Navab, and Federico Tombari. 6d
object pose estimation with depth images: A seamless ap-
proach for robotic interaction and augmented reality. arXiv
preprint arXiv:1709.01459 , 2017. 1
[39] Meng Tian, Marcelo H Ang, and Gim Hee Lee. Shape prior
deformation for categorical 6d object pose and size estima-
tion. In European Conference on Computer Vision , pages
530–546. Springer, 2020. 1, 3, 6
[40] Henning Tjaden, Ulrich Schwanecke, and Elmar Schomer.
Real-time monocular pose estimation of 3d objects using
temporally consistent local color histograms. In Proceedings
of the IEEE International Conference on Computer Vision
(ICCV) , Oct 2017. 1
[41] Shinji Umeyama. Least-squares estimation of transformation
parameters between two point patterns. IEEE Transactions
on Pattern Analysis & Machine Intelligence , 13(04):376–
380, 1991. 3
[42] Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martin-Martin,
Cewu Lu, Li Fei-Fei, and Silvio Savarese. Densefusion: 6d
object pose estimation by iterative dense fusion. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , June 2019. 2
[43] Gu Wang, Fabian Manhardt, Federico Tombari, and Xi-
angyang Ji. Gdr-net: Geometry-guided direct regression net-
work for monocular 6d object pose estimation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 16611–16621, 2021. 2
[44] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin,
Shuran Song, and Leonidas J Guibas. Normalized object co-
ordinate space for category-level 6d object pose and size esti-
mation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 2642–2651, 2019. 3,
5, 6, 7, 8
[45] Jiaze Wang, Kai Chen, and Qi Dou. Category-level 6d ob-
ject pose estimation via cascaded relation and recurrent re-
construction networks, 2021. 3, 6
[46] Yijia Weng, He Wang, Qiang Zhou, Yuzhe Qin, Yueqi
Duan, Qingnan Fan, Baoquan Chen, Hao Su, and Leonidas J
Guibas. Captra: Category-level pose tracking for rigid and
articulated objects from point clouds. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 13209–13218, 2021. 3
[47] Yangzheng Wu, Mohsen Zand, Ali Etemad, and Michael
Greenspan. V ote from the center: 6 dof pose estimation in
9968
rgb-d images by radial keypoint voting. In European Con-
ference on Computer Vision , pages 335–352. Springer, 2022.
2
[48] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and
Dieter . Posecnn: A convolutional neural network for 6d ob-
ject pose estimation in cluttered scenes. Robotics: Science
and Systems , 2018. 2
[49] Michela Zaccaria, Fabian Manhardt, Yan Di, Federico
Tombari, Jacopo Aleotti, and Mikhail Giorgini. Self-
supervised category-level 6d object pose estimation with op-
tical flow consistency. IEEE Robotics and Automation Let-
ters, 8(5):2510–2517, 2023. 3
[50] Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic. Dpod:
6d pose object detector and refiner. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , October 2019. 2
[51] Guangyao Zhai, Xiaoni Cai, Dianye Huang, Yan Di, Fabian
Manhardt, Federico Tombari, Nassir Navab, and Benjamin
Busam. Sg-bot: Object rearrangement via coarse-to-
fine robotic imagination on scene graphs. arXiv preprint
arXiv:2309.12188 , 2023. 1
[52] Guangyao Zhai, Dianye Huang, Shun-Cheng Wu, HyunJun
Jung, Yan Di, Fabian Manhardt, Federico Tombari, Nassir
Navab, and Benjamin Busam. Monograspnet: 6-dof grasping
with a single rgb image. In IEEE International Conference
on Robotics and Automation . IEEE, 2023. 1
[53] Guangyao Zhai, Evin Pinar ¨Ornek, Shun-Cheng Wu, Yan
Di, Federico Tombari, Nassir Navab, and Benjamin Busam.
Commonscenes: Generating commonsense 3d indoor scenes
with scene graphs. In NeurIPS , 2023. 1
[54] Guangyao Zhai, Yu Zheng, Ziwei Xu, Xin Kong, Yong Liu,
Benjamin Busam, Yi Ren, Nassir Navab, and Zhengyou
Zhang. Da2dataset: Toward dexterity-aware dual-arm grasp-
ing. RA-L , 7(4):8941–8948, 2022. 1
[55] Chenyangguang Zhang, Yan Di, Ruida Zhang, Guangyao
Zhai, Fabian Manhardt, Federico Tombari, and Xiangyang
Ji. Ddf-ho: Hand-held object reconstruction via conditional
directed distance field. arXiv preprint arXiv:2308.08231 ,
2023. 1
[56] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Pola-
nia Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan
Yang. A tale of two features: Stable diffusion complements
dino for zero-shot semantic correspondence. arXiv preprint
arXiv:2305.15347 , 2023. 2, 3
[57] Ruida Zhang, Yan Di, Zhiqiang Lou, Fabian Manhardt, Fed-
erico Tombari, and Xiangyang Ji. Rbp-pose: Residual
bounding box projection for category-level pose estimation,
2022. 1, 2, 6
[58] Ruida Zhang, Yan Di, Fabian Manhardt, Federico Tombari,
and Xiangyang Ji. Ssp-pose: Symmetry-aware shape prior
deformation for direct category-level object pose estimation,
2022. 6
[59] Linfang Zheng, Chen Wang, Yinghan Sun, Esha Dasgupta,
Hua Chen, Ales Leonardis, Wei Zhang, and Hyung Jin
Chang. Hs-pose: Hybrid scope feature extraction for
category-level object pose estimation, 2023. 3, 6
9969
