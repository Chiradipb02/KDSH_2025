Rotated Multi-Scale Interaction Network for Referring Remote
Sensing Image Segmentation
Sihan Liu*, Yiwei Ma*, Xiaoqing Zhang*, Haowei Wang, Jiayi Ji†, Xiaoshuai Sun, Rongrong Ji
Key Laboratory of Multimedia Trusted Perception and Efﬁcient Computing,
Ministry of Education of China, Xiamen University, 361005, P .R. China
{liusihan, yiweima, 36920221153149, wanghaowei }@stu.xmu.edu.cn
jjyxmu@gmail.com {xssun, rrji }@xmu.edu.cn
Abstract
Referring Remote Sensing Image Segmentation (RRSIS)
is a new challenge that combines computer vision and natu-ral language processing. Traditional Referring Image Seg-mentation (RIS) approaches have been impeded by the com-plex spatial scales and orientations found in aerial imagery,leading to suboptimal segmentation results. To addressthese challenges, we introduce the Rotated Multi-Scale In-teraction Network (RMSIN), an innovative approach de-signed for the unique demands of RRSIS. RMSIN incorpo-rates an Intra-scale Interaction Module (IIM) to effectivelyaddress the ﬁne-grained detail required at multiple scalesand a Cross-scale Interaction Module (CIM) for integratingthese details coherently across the network. Furthermore,RMSIN employs an Adaptive Rotated Convolution (ARC)to account for the diverse orientations of objects, a novelcontribution that signiﬁcantly enhances segmentation accu-racy. To assess the efﬁcacy of RMSIN, we have curated anexpansive dataset comprising 17,402 image-caption-masktriplets, which is unparalleled in terms of scale and vari-ety. This dataset not only presents the model with a widerange of spatial and rotational scenarios but also estab-lishes a stringent benchmark for the RRSIS task, ensuringa rigorous evaluation of performance. Experimental eval-uations demonstrate the exceptional performance of RM-SIN, surpassing existing state-of-the-art models by a signif-icant margin. Datasets and code are available at https:
//github.com/Lsan2401/RMSIN .
1. Introduction
Referring Remote Sensing Image Segmentation (RRSIS)
stands at the forefront of integrating computer vision withnatural language processing [ 18,36–38], aiming to segment
*These authors contributed equally to this work.
†The corresponding author.Ground 
Truth
LAVT
Ours
“A vehicle on the 
right”“A oval tiny ground track 
field at the bottom”
“Player with bat”
“Red car front”
 “Bus with the 
number 7 on it”
“Dude carrying baby”
(a) Referring Remote Sensing Image 
Segmentation(b) Referring Image 
Segmentation Tiny-Scale Object Rotated Object
“
Regular-Scale Object
Non-Rotated Object
Figure 1. Comparison between the newly constructed RRSIS-D
and conventional RIS datasets [ 62], highlighting the complex spa-
tial scales and orientations prevalent in aerial imagery. (a) Ex-amples from our RRSIS-D, demonstrating the limitations of tradi-tional RIS methods ( e.g.,L A V T[ 58]) in handling such complexi-
ties. (b) Examples from a standard RIS dataset [ 62].
speciﬁc areas from aerial images based on textual descrip-
tions. This sophisticated task goes beyond the capabilitiesof traditional RIS [ 13,25,28], requiring an intricate under-
standing of the spatial and geographic nuances conveyed
from aerial perspectives. RRSIS plays a crucial role in awide range of applications, including land use categoriza-tion [ 9], climate impact studies [ 44], and urban infrastruc-
ture management [ 8]. By pushing the boundaries of seman-
tic understanding in remote sensing data, RRSIS is advanc-
ing the possibilities in these domains. Despite this, the ﬁeld
has been constrained by the limited scale and scope of ex-isting datasets, which are insufﬁcient for training models tothe level of accuracy required for these critical tasks.
In light of these requirements, our research introduces an
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
26658
expansive new benchmark dataset, namely RRSIS-D, de-
signed to propel the development of RRSIS. This datasetsurpasses its predecessors
*[63] in threefold size, encom-
passing not only higher resolution images but also a signif-icantly broader range of geographic diversity. The devel-opment of this dataset is guided by the Segment AnythingModel (SAM) [ 23], which facilitates a semi-automated an-
notation process, thereby mitigating the labor-intensive na-ture of generating accurate pixel-level masks traditionally.This process involves deriving initial segmentation masksfrom bounding box prompts and reﬁning them to ensure
high ﬁdelity to the complex reality of aerial imagery. Theresult is a comprehensive corpus of 17,402 remote sensing
image-caption-mask triplets, an invaluable resource aimedat advancing the precision and utility of RRSIS.
Furthermore, although the existing RIS methodolo-
gies [ 17,25,29,48] have demonstrated effectiveness in
speciﬁc domains [ 39,41,62], they face limitations when
applied to the diverse and intricate nature of remote sensingimagery. As illustrated in Fig. 1, aerial images pose distinct
challenges that are not encountered in conventional imagedatasets, including vast and diverse spatial scales, as wellas objects captured from multiple orientations. Current RISapproaches typically excel in aligning visual and linguisticelements in well-bounded contexts [ 16,61] but falter when
faced with the chaotic and unstructured nature of aerial im-ages. The inability of these methods to grapple with highlevels of spatial variation and rotational diversity results ina notable performance gap in RRSIS tasks, highlighting theneed for a more robust and versatile approach.
To overcome the inherent limitations in existing ap-
proaches, we present the Rotated Multi-Scale Interac-tion Network (RMSIN), a pioneering architectural solution
meticulously designed to tackle the complexities of RRSIS.Our approach introduces a sophisticated Intra-scale Interac-tion Module (IIM) that excels at extracting detailed featureswithin individual layers, as well as a Cross-scale Interac-tion Module (CIM) that facilitates comprehensive featurefusion across the entire network. Furthermore, we integratean Adaptive Rotated Convolution (ARC) into the decoder,empowering the model to effectively handle the intricate ro-tational variations exhibited by objects. By seamlessly in-tegrating these modules, RMSIN proﬁciently extracts andaligns features across diverse scales and orientations, result-ing in remarkable performance enhancements for RRSIS.
To sum up, our key contributions are as follows:
• We introduce RRSIS-D, a novel benchmark dataset tai-
lored for Referring Remote Sensing Image Segmentation(RRSIS). This dataset accommodates substantial varia-tions in both spatial resolution and object orientation.
• We propose the Rotated Multi-Scale Interaction Network
*Known as RefSegRS, as of November 17, 2023, this dataset is not yet
publicly available.(RMSIN) to address the challenges posed by the multiple
spatial scales and orientations prevalent in aerial imagery.
• We propose IIM and CIM to handle ﬁne-grained informa-
tion within and across different scales. Meanwhile, Wedesign ARC to enhance the model’s robustness againstthe ubiquitous rotational phenomena in RRSIS.
2. Related work
Referring Image Detection and Segmentation. Refer-
ring Image Detection aims to predict a bounding box corre-sponding to a given referring expression. Existing workscan be classiﬁed into two-staged methods [ 12,14,65,67]
which are based on region proposal ranking, and one-stage
methods [ 1,24,26,33–35,57,66] which directly predict the
target bounding box. Referring Image Segmentation aims toachieve pixel-level localization of target objects within im-ages based on associated referring expressions, presenting amore complex task [ 13]. Early works [ 25,28,41] leverage
convolution networks and recurrent neural networks to ex-tract vision and language features, respectively. These fea-tures are then fused by simple concatenation to generate ﬁ-nal predictions. Subsequent methods [ 2,3,6,19,20,28,29,
40,47,48,51,54] mainly focus on vision-language align-
ment to enhance predictions. Some employ recurrent reﬁne-ment [ 2,28], while others utilize dynamic ﬁlters [ 3,19,40]
to fuse visual and linguistic features. Recently, leveragingthe Transformer’s outstanding performance, methods can
deviled into two categories: those performing cross-modaldecoder fusion based on Transformer [ 6,20,29,48] and
those incorporating language-aware visual encoding instead
of post-feature fusion [ 22,52,58]. However, due to the spe-
ciﬁc characteristics of aerial images, these methods exhibitlimited performance in the Remote Sensing ﬁeld. Some ap-proaches [ 21,42,60] have introduced extra scale interaction
modules to enhance feature extraction. However, the ex-treme semantic gap between natural images and aerial im-ages still results in suboptimal performance.
Remote Sensing Referring Image Detection and Seg-
mentation. Referring Image Detection in Remote Sensing
ﬁeld is a novel task with limited research. It was ﬁrst intro-
duced by [ 45], where a new dataset and a baseline model
were proposed. Recently, the transformer-based method
RSVG [ 64] has been proposed. RSVG utilized the Vision
Transformer [ 7] and BERT [ 5] as backbones, incorporat-
ing the Multi-Level Cross-Modal feature learning moduleto address multi-scale variations in aerial images. RemoteSensing Referring Image Segmentation (RRSIS) is also anascent ﬁeld owing to the aforementioned challenges. Y uanet al. [ 63] constructed the ﬁrst RRSIS dataset and proposed
a model that utilizes the deep and shallow feature interac-tions to enhance the multi-scale feature extraction. How-ever, this model encounters limitations in handling morecomplex datasets. In an effort to address the existing gaps in
26659
Figure 2. Word cloud for top 100 words within the expressions of
RRSIS-D.
Figure 3. Distribution of image categories of RRSIS-D.
RRSIS, we propose a more extensive and intricate dataset,
RRSIS-D, alongside a novel model named RMSIN and con-duct a comparative evaluation of the performance of Y uan
et al. [ 63]’s model on our dataset.
3. RRSIS-D
We present a new large-scale benchmark, called RRSIS-
D, speciﬁcally designed for the RRSIS task. Fig. 2depicts
the word cloud representation of this dataset. Motivated bythe exceptional segmentation performance achieved by theSegment Anything Model (SAM) [ 23], we adopt a semi-
automatic approach that capitalizes on bounding boxes andSAM to generate pixel-level masks, resulting in cost savingsduring the annotation process. Speciﬁcally, we follow thesteps outlined below to generate pixel-wise annotations forlanguage expressions:
•Step 1 . Pixel-level masks for all images in the dataset are
generated by leveraging the bounding box prompts pro-vided by the RSVGD Dataset [ 64] through the employ-
ment of SAM. It is noteworthy, however, that the perfor-mance of SAM may exhibit variability in accuracy when
applied to partial images, owing to the inherent domaingap between aerial and natural images.
Proportion of Mask SizeNumber of Masks
~૚
૙૚૚૙
૝૝
Ship
Overpass
Ground 
track field
Stadium
Vehicle
Expressway 
toll station 
ࣂࣂ
<<
૙૙
..
૚૚
Airplane
Chimney
ࣂࣂ
<<
૙૙
..
૛૛
 ࣂࣂ
>>
૙૙
..
૟૟
 ࣂࣂ
>>
૙૙
..
૝૝
Figure 4. Distribution of mask sizes, with the horizontal axis
showing mask coverage percentage in images ( θ) and the verti-
cal axis representing total mask count, illustrated with varied-size
ground truth examples.
•Step 2 . We undertake a manual reﬁnement process for
masks associated with problematic aerial images. This re-ﬁnement involves the utilization of a ﬁlling algorithm toaddress hollow problems within the masks. Subsequently,
a meticulous curation of the dataset is conducted to iden-
tify problematic data, and manual annotation is employedto generate masks aligned with annotation standards. Thismanual annotation process is facilitated by the softwaretool [ 59] designed in accordance with the principles of
SAM, ensuring the accurate generation of masks corre-sponding to linguistic expressions.
•Step 3 . To enhance the compatibility of RRSIS-D with
natural RIS models, ﬁnally, the annotations are convertedinto RefCOCO dataset [ 27] format for better usability.
The benchmark statistics, as presented in Tab. 1,e x -
hibit notable distinctions from the existing RefSegRSdataset [ 63]. Our proposed dataset, RRSIS-D, comprises
a comprehensive collection of 17,402 images, accompaniedby their corresponding masks and referring expressions. A
standardized resolution of 800px in height and 800px inwidth has been uniformly applied to all images. Further-more, the semantic labels comprise 20 categories, supple-mented by 7 attributes, thereby enhancing the semantic rich-
ness of the referring expressions. To illustrate the preva-lence of each category, the category distribution is graphi-
cally represented in Fig. 3. For instance, the category “Air-
plane” accounts for 15.6% of the total, ranking highest interms of quantity. It is worth noting that our dataset offersenhanced ﬂexibility in terms of mask resolution, surpassingthat of RefSegRS.
The statistics of the generated masks are depicted in
Fig. 4. Notably, a signiﬁcant portion of the targets is ex-
tremely small, occupying only a fraction of the overall im-age. However, there are also instances of large-scale ob-jects exceeding 400,000 pixels in size. Some examples ofmasks with different sizes are illustrated in the ﬁgure, high-lighting the substantial variability in scale across differentcategories in the dataset. This presents a challenging task,as it involves predicting images with signiﬁcant large-scalevariations and numerous small targets.
26660
DatasetNumber
of imagesImage
sizeSpatial
resolutionAttributes
of expressionMask
generation
RefSegRS [ 63] 4420 512×512 0.13m 3 Manually
RRSIS-D 17402 800×800 0.5m∼30m 7Semi
automatically
Table 1. Compare our dataset with the previous dataset.
4. RMSIN
4.1. Overview
The pipeline of our proposed model is depicted in Fig. 5.
Initially, given the input image I∈RH×W×3and the lan-
guage expression E={ωi},i∈{0,...,N}, whereHand
Wrepresent the height and width of the input image, and N
is the length of the expression, the input language expres-sionEis transformed into the feature space F
/lscript∈RN×Cvia
the backbone f/lscript. The following Compounded Scale Interac-
tion Encoder (CSIE), which is composed of an Intra-scaleInteraction Module (IIM) at each stage, and a Cross-scale
Interaction Module (CIM), is applied to generate the fused
features with sufﬁcient semantics across multiple scales. Fi-nally, we propose an Adaptive Rotated Convolution (ARC)based Oriented-Aware Decoder (OAD) to generate the seg-mentation mask by the parallel inference on the featuresfrom the multiple stages of the CSIE.
4.2. Compounded Scale Interaction Encoder
To effectively locate diverse targets with the guidance of
the referred texts, the information for multi-scale is just as
important as the referring expressions. Given the languagefeaturesF
/lscriptand the input image I∈RH×W×3, the Com-
pounded Scale Interaction Encoder (CSIE) brings about thefusion across vision-language modality in a multi-stage waywith both intra- and inter-perspective.
Speciﬁcally, the CSIE is constructed with two compo-
nents, Intra-scale Interaction Module (IIM) and Cross-scaleInteraction Module (CIM).
4.2.1 Intra-scale Interaction Module
The ﬁrst part of each stage in CSIE, the Intra-scale In-
teraction Module (IIM) is designed to further excavate therich information within each scale and facilitate interac-tion between the vision and language modalities. Basedon a hierarchy of four stages, IIM could be denoted as{φ
i}i∈{1,2,3,4}. After obtaining the language features F/lscript∈
RN×Cthrough the text backbone, where Cdenotes the
number of channels, the output features Fi
eof IIM at stage
icould be described as:
Fi
e=φi(Fi−1
e,F/lscript), (1)
whereF0
eis extracted from the vision backbone fvwith
the input I. More detailed, during the stage i, the inputfeaturesFi−1
e undergo a combination of downsampling and
MLP [ 31] to reduce the scale and unify the dimension of
features, resulting in ˆFi−1
e. The downsampled input ˆFi−1
e is
fed into two branches for enhancing visual priors and fusingcross-modal information individually.
Various Receptive Branch is the ﬁrst branch. The fea-
tureˆF
i−1
e is transformed through multiple branches with
different settings of convolution kernels to yield featureswith various receptive ﬁelds, which could be formulated as:
ω
i=σ⎛
⎝J/summationdisplay
j=0/parenleftBigg
1
CC/summationdisplay
ki
j∗ˆFi−1
e/parenrightBigg⎞
⎠, (2)
where the ki
jmeans the j-th branch of convolution and the σ
is the Sigmoid Function. The above formulation indicatesthat the different convolution setting is utilized to balancethe weight ω
i∈(0,1)H×Wbetween all the pixels. The
weight is taken to enhance the features by:
ˆFi−1
e1=ωi⊗ˆFi−1
e. (3)
In addition, the output is regulated by a Visual Gate,
adding to the raw image features as a complement to lo-cal detail information. The speciﬁc implementation of thegate is:
α= Tanh(Linear(ReLU(Linear( ˆF
i−1
e1)))), (4)
whereLinear(·)denotes the linear projection, and Tanh(·)
andReLU(·)represent the activation functions.
Cross-modal Alignment Branch is designed for multi-
modal alignment, which is the key to enabling the model tocomprehend natural language.
Concretely, taking the input ˆF
i−1
e and language features
F/lscript, the module ﬁrst implements scaled dot-product atten-
tion [ 46] usingˆFi−1
e as the query and F/lscriptas the key and
value to obtain the multi-modal features:
Ai= attention( ˆFi−1
eWi
q,F/lscriptWi
k,F/lscriptWi
v), (5)
whereWi
q,Wi
kandWi
vare the linear projection matrices.
Subsequently, the attention Aiis combined with ˆFi−1
e to
obtain language-guided image features:
ˆFi−1
e2=P r oj (AiWi
w⊗ˆFi−1
eWi
m), (6)
whereWi
wandWi
mare the projection matrices, and ⊗de-
notes element-wise multiplication. The obtained result ispassed through a ﬁnal 1×1convolution, denoted as Proj(·),
to produce the ﬁnal output.
Similar to the operation performed on the output of
ˆF
i−1
e1, the result is regulated by βfrom the Linguistic Gate
shares an identical structure with the Visual Gate and isadded to the raw image features, serving as supplementarylinguistic features. Consequently, the overall output fea-tures of IIM at stage ican be illustrated as:
F
i
e=ˆFi−1
e+αˆFi−1
e1+βˆFi−1
e2. (7)
26661
BERT
Encoder
Layer1
Encoder
Layer2
Encoder
Layer3
Encoder
Layer4
Multi-scale 
Attention
Feed Forward
Scale-aware
Gate瀡An airplane at 
the bottom 瀢
ARC
ARC
ARC
ARC
(b) Cross-scale Interaction Module(c) Adaptive Rotated Convolution
(a) Intra-scale Interaction Module
Various 
Receptive
Transformer 
Layer
Cross-modal 
Alignment
Vision
Gate
Language 
Gate
Depth-wise 
Convolution
Average Pooling
Liner
Convolution
Liner
Rotate
Blockℓࡲ
࢏ࢋࡲି૚ࡰ࢏
ࣂ
ࣅ 
࢏ࡰ∗࢏ࢋࡲ
Figure 5. Overview of the proposed RMSIN.
4.2.2 Cross-scale Interaction Module
While the IIM adequately extracts localized multi-scale
information guided by linguistic features, we additionally
design a Cross-scale Interaction Module (CIM) to further
enhance the interaction between the coarse and ﬁne stages,particularly in response to the scale variation challenge ob-served in aerial images. Speciﬁcally, the module takes fea-tures collected from each stage of the IIM, i.e., the previ-
ously mentioned F
i
e,i∈{1,2,3,4}as input and performs
multi-stage interaction. The structure is depicted schemati-cally in Fig. 5.
Multi-stage Feature Combination is ﬁrst performed,
where the features F
i
eare downsampled to the same size
and concatenated along the channel dimension. The for-mula expression is as follows:
F
i
d= downsample( Fi
e),i∈{1,2,3,4},
F∗
c= concat
c(F1
d,F2
d,F3
d,F4
e),(8)
whereFi
drepresents the downsampled features, and F∗
crep-
resents the multi-stage feature concatenated along the chan-nel dimension. downsample( ·)is typically implemented
through average pooling.
Multi-scale Attention Layer is subsequently imple-
mented. Speciﬁcally, we design various perceptive ﬁeldsfor the concatenated feature F
∗
cto achieve deep multi-
scale interaction. F∗
cis resized to different scales through
the depth-wise convolutions with diverse kernel sizes andstrides, deﬁned as follows:
F
m
c=km∗F∗
c,
hm=⌊h−1
m+1⌋,wm=⌊w−1
m+1⌋,(9)
wherem∈{1,...,M},Mis the number of resized scales,
kmis them-th depth-wise convolution and hmandwmare
the corresponding height and weight of the Fm
c. With theset{Fm
c|m∈{1,...,M}}, we ﬂatten all the elements on
the size dimension and concatenate them as a sequence fea-
turesˆF∗
c∈R(/summationtextM
1hm×wm)×C. Similar to vanilla atten-
tion [ 46], we take the origin feature F∗
cas the query, and
the multi-scale-aware feature ˆF∗
cas the key and value to
perform cross-scale interaction:
/tildewideF∗
c= softmax(F∗
cWq·ˆF∗
cWT
k√
C)·ˆF∗
cWv. (10)
For better preservation of local details, following inspiration
from HRViT [ 10], a local relationship compensation called
LRC is incorporated to regulate the output of the multi-scaleattention. Consequently, the ﬁnal output of the Multi-scale
Attention Layer is expressed as:
F
c=/tildewideF∗
c+DWConv(Hardswish( F∗
c)), (11)
whereDWConv( ·)represents depth-wise convolution, and
Hardswish( ·)is the activity function, implemented in ac-
cordance with [ 10] to enhance the extraction of multi-scale
local information.
The Feed Forward Layer follows the Multi-scale At-
tention layer which is identical to the standard attentionblock [ 46]. The feature F
cis divided into four parts to re-
vert to the size of Fi
eby upsampling and subsequently fed
into the Scale-aware Gate to obtain the ﬁnal output.
Scale-aware Gate is employed to alleviate the semantic
gap before and after multi-scale attention. Speciﬁcally, foreach part from F
c, we implement the corresponding part
fromFeto measure the weight of the cross-scale interac-
tion. This weight is considered as the assistance residualfor the features from IIM. The formulation is as follows:
F
i
o= sigmoid( Fi
eW1)⊗Fi
cW2+Fi
eW3, (12)
wherei∈{1,2,3,4}. The output of the Scale-aware Gate
is utilized in the decoder for ﬁnal mask prediction.
26662
4.3. Oriented-aware Decoder
The set of features {Fi
o|i∈{1,2,3,4}}from the CSIE
are used to generate the segmentation. Considering thatobject instances in aerial images often exhibit various ori-
entations, using static horizontal convolution kernels formask generation may result in a loss of precision. In-spired by oriented object detection, where the problemhas been researched for decades and achieved considerable
progress [ 43,53,55,56], we incorporate the Adaptive Ro-
tated Convolution (ARC) into the segmentation decoder tai-lored for the speciﬁc needs of RRSIS task to achieve bettermask prediction.
4.3.1 Adaptive Rotated Convolution
The ARC captures angle information from input features
and dynamically re-parameterizes the kernel weights to ﬁl-ter out redundant features. Speciﬁcally, it extracts orien-
tation features and predicts n angles θ∈{1,...,n}and
corresponding weights λ∈{1,...,n}based on the input.
For the input X, theθandλare predicted as:
θ, λ= Routing( X), (13)
where the concrete structure of the Routing Block is il-
lustrated in Fig. 5. The static convolution kernel weights
can be viewed as speciﬁc sampling points from the two-
dimensional kernel space. Thus, the rotation of the convolu-tion kernel is the process of rotary resampling. Speciﬁcally,
the convolution kernel weights W
iare re-parameterized ac-
cording to the predicted angels as follows:
Y/prime
i=M−1(θi)Yi,
W/prime
i= interpolation( Wi,Y/prime
i),(14)
whereYiis the coordinates of original sampling points,
M−1(θi)is the inverse matrix of the rotation matrix for
afﬁne transformation by angle θaround the origin, and
interpolation( ·)is implemented as bilinear interpolation.
Finally, the features are ﬁltered by the obtained convolutionkernel and subjected to a weighted sum operation to pro-duce orientation-aware features:
X
∗=X∗n/summationdisplay
i=1λiW/prime
i. (15)
The overall top-down process of mask prediction can be
concluded as follows:
D4=F4
o,
Di= Seg(ARC([ Di+1;Fi
o])),i∈{1,2,3},
D0=P r oj (D1),(16)
whereSeg(·)refers to a nonlinear block comprising a 3×3
convolution layer, a batch normalization layer, and a ReLUactivity function to enhance the nonlinearity of the segmen-
tation feature space. And Proj(·)is implemented as a lin-
ear projection function to map the ﬁnal feature D1into two
class scores. It is notable that half of the convolution layersare replaced by the ARC to leverage orientation informationin the feature space, thereby eliminating redundancy for en-hanced accuracy in boundary details.
5. Experiments
5.1. Implementation Details
Experiment Settings. In our experiments, the visual back-
bone utilizes Swin Transformer [ 31], pre-trained on Ima-
geNet22K [ 4], while the language backbone employs the
base BERT model from HuggingFace’s library [ 49]. The
model is trained for 40 epochs using AdamW [ 32] with a
weight decay of 0.01 and a starting learning rate of 3e-5,
reducing according to polynomial decay. The setup ran on
four RTX 2080 GPUs with a batch size of 8.
Metrics. We utilize Overall Intersection-over-Union
(oIoU), Mean Intersection-over-Union (mIoU), and Preci-sion@X (P@X) as evaluation metrics, similar to prior stud-
ies [ 50,64].
5.2. Comparison with state-of-the-art RIS methods
In our experiments, we compared RMSIN’s performance
with existing state-of-the-art referring image segmentationmethods on the validation and test subsets of our RRSIS-Ddataset (see Tab. 2). For a fair comparison, we adopted the
original implementation details of these competing meth-ods. Notably, RMSIN outperforms its counterparts acrossalmost all metrics on both subsets, marking a signiﬁcantimprovement with a 3.64% and 3.16% increase in mIoU onthe validation and test subsets respectively over the closestcompetitor, LA VT. This leap in performance is particularlyevident in complex scenarios, such as detecting small or ro-tated objects, where it secured over 3.0% gains in Precisionat IoU thresholds of 0.5, 0.6, and 0.7.
5.3. Ablation study
We have performed various ablation experiments on the
validation subset of RRSIS-D to assess the efﬁcacy of thepivotal components within our proposed network.
Efﬁctiveness of IIM and CIM. To validate the efﬁcacy
of our proposed two-scale interaction modules in CSIE, weconduct ablation studies on all the combinations of IIM andCIM. As illustrated in Tab. 3, The introduction of the IIM
brings about discernible improvements in precision at lowerIoU thresholds, while the incorporation of the CIM furtherreﬁnes predictions across various IoU levels. The combinedeffect of both modules demonstrates a synergistic enhance-ment, yielding the highest performance across all evalu-ated metrics, particularly in P@0.5, P@0.7, and mIoU, with
26663
MethodVisual
EncoderText
EncoderP@0.5 P@0.6 P@0.7 P@0.8 P@0.9 oIoU mIoU
Va l Test Va l Test Va l Test Va l Test Va l Test Va l Test Va l Test
RRN [ 25] R-101 LSTM 51.09 51.07 42.47 42.11 33.04 32.77 20.80 21.57 6.14 6.37 66.53 66.43 46.06 45.64
CSMA [ 61] R-101 None 55.68 55.32 48.04 46.45 38.27 37.43 26.55 25.39 9.02 8.15 69.68 69.39 48.85 48.54
LSCM [ 17] R-101 LSTM 57.12 56.02 48.04 46.25 37.87 37.70 26.37 25.28 7.93 8.27 69.28 69.05 50.36 49.92
CMPC [ 16] R-101 LSTM 57.93 55.83 48.85 47.40 38.50 36.94 25.28 25.45 9.31 9.19 70.15 69.22 50.41 49.24
BRINet [ 15] R-101 LSTM 58.79 56.90 49.54 48.77 39.65 39.12 28.21 27.03 9.19 8.73 70.73 69.88 51.14 49.65
CMPC+[30] R-101 LSTM 59.19 57.65 49.36 47.51 38.67 36.97 25.91 24.33 8.16 7.78 70.14 68.64 51.41 50.24
LGCE [ 63] Swin-B BERT 68.10 67.65 60.52 61.53 52.24 51.45 42.24 39.62 23.85 23.33 76.68 76.34 60.16 59.37
LA VT [ 58] Swin-B BERT 69.54 69.52 63.51 63.63 53.16 53.29 43.97 41.60 24.25 24.94 77.59 77.19 61.46 61.04
RMSIN (Ours) Swin-B BERT 74.66 74.26 68.22 67.25 57.41 55.93 45.29 42.55 24.43 24.53 78.27 77.79 65.10 64.20
Table 2. Comparison with state-of-the-art methods on the proposed RRSIS-D dataset. R-101 and Swin-B represent ResNet-101 [ 11] and
base Swin Transfomer [ 31] models, respectively. The best result is bold.LAVT Ours GT
Q:
“The baseball field 
on the upper right”“A vehicle on the 
upper left”
“A vehicle on the 
lower right”
“A vehicle on the lower left”
 “The vehicle at the bottom”
“The basketball court is on the lower right of 
the vehicle on the top”“A baseball field is on the right of the green ground 
track field in the middle”
Large-Scale Object Tiny-Scale Object Rotated Object
Figure 6. Qualitative comparisons between RMSIN and the previous SOTA LA VT. The left part illustrates the predictions of large-
scale objects, while the middle part offers exceedingly diminutive objects amidst a highly noisy background. The right part exhibits the
predictions for scenarios wherein objects are situated at diverse angles.
IIM CIM P@0.5 P@0.7 P@0.9 oIoU mIoU
 69.54 53.16 24.25 77.59 61.46
 71.09 53.45 24.71 77.68 62.27
 73.68 56.67 25.69 77.40 64.25
 74.14 57.59 25.69 77.91 64.91
Table 3. Ablation on the scale interaction modules IIM and CIM.
margins ranging from 3.5% to 4.5%. These ﬁndings afﬁrmthe pivotal role played by the IIM and CIM in capturingmulti-scale features from images, thus substantiating theirefﬁcacy in advancing the overall segmentation capabilities.
Design options of CIM. To further substantiate the effec-
tiveness of CIM, we conduct a detailed analysis of its maincomponents, as outlined in Tab. 4. The most substantial en-
hancement in results is observed upon the inclusion of thecomplete module, showcasing the highest metric enhance-
ment of over 4.14%. This conﬁrms the role of CIM in pre-serving local details and extracting multi-scale information.
Design options of Decoder . We explore the design of the
segmentation decoder structure as demonstrated in Tab. 5.
The CIM yields output features with robust semantics andOptions P@0.5 P@0.7 P@0.9 oIoU mIoU
Default 69.54 53.16 24.25 77.59 61.46
+Multi-scale Attention 68.91 53.68 25.11 77.61 61.46
+Feed Forward 69.83 52.70 25.57 77.85 61.46
+LRC 73.68 56.67 25.69 77.40 64.25
Table 4. Ablation on options design of CIM. Default means the
vanilla self attention and we reintroduce all the designs cumula-tively to demonstrate the effectiveness of each major component.
intricate spatial details. Thus our proposed Oriented-aware
Decoder straightforwardly concatenates the features and ex-tracts angular information through ARC to obtain more ac-curate predictions better suited to RS tasks. We also ex-periment with two alternative decoder structures. The ex-ceptional results of our proposed decoder, surpassing othersacross all metrics, underscore the signiﬁcance of incorpo-rating angle information in the decoding process. This out-come ﬁrmly reafﬁrms the efﬁcacy of our approach in cus-tomizing mask predictions for remote sensing applications,where the inclusion of precise angular information emergesas a critical factor for optimizing segmentation accuracy.
26664
Decoder Design P@0.5 P@0.7 P@0.9 oIoU mIoU
Sum 72.36 55.23 24.89 76.11 62.76
Concat 70.34 53.33 24.66 77.74 61.56
Oriented-aware 73.85 56.84 25.40 77.76 64.15
Table 5. Ablation study examining decoder designs. “Sum ” de-
coder employs summation instead of concatenation for cross-stage
feature connection, while “Concat ” decoder substitutes ARC in
“Oriented-aware ” decoder with static convolutions.
Options P@0.5 P@0.7 P@0.9 oIoU mIoU
(a) Replacement of ARC in the Oriented-aware Decoder
L=1 71.72 54.94 24.43 75.93 62.53
L=2 72.18 55.29 24.66 76.37 62.48
L=3 72.36 55.98 25.00 77.06 63.81
(b) Predicted number of angles in ARC
n=1 72.36 55.98 25.00 77.06 63.81
n=2 72.24 54.48 24.02 77.39 63.14
n=4 73.85 56.84 25.40 77.76 64.15
Table 6. Ablation studies of ARC. L=1 indicates the replacement
of the ﬁrst layer in the decoder with Adaptive Rotated Convo-lution. Experiments on the predicted number of angles are per-formed under L=3.
Design options of ARC. We further investigate the impact
of the Adaptive Rotated Convolution (ARC) replacementstrategy on the results, as demonstrated in Tab. 6(a). We
progressively replace the convolution layers in each stageof the decoder, and the result exhibits a consistent upwardtrend. Consequently, we opt to replace all three layers of thedecoder. Additionally, we explore the inﬂuence of varyingthe number of prediction angles for ARC on the predictionresults illustrated in Tab. 6(b). The decoder showcases a
consistent improvement in performance with an increase inthe predicted number of angles, resulting in a performanceboost of approximately 1% when seated to 4 compared to 1.
5.4. Visualization
5.4.1 Quantitative Results
We qualitatively compare our model with the baseline
to provide a comprehensive understanding. As shown inFig. 6, our model excels at identifying targets across var-
ious scales accurately, even within noisy backgrounds and
at different angles. In contrast, the baseline model exhibits
shortcomings such as missing parts and noticeable shifts inpredicted masks.
5.4.2 Visualization of Features from Encoder
In Fig. 7, we visualize the feature maps from the RMSIN
during training under the ablation of ARC and CSIE. It’s ob-vious that RMSIN can accurately capture boundary infor-
Expression:
“The gray basketball court.”
ࡲࡲ࢕࢕૛૛
Image
Ground truth
࢕
࢕
 Prediction
RMSIN w/o ARC w/o CSIE
ࡲࡲ࢕
࢕࢕૜૜
ࡲࡲ࢕
࢕࢕
࢕࢕࢕૚૚
ࡲࡲ࢕
࢕࢕
࢕࢕࢕૝૝
Figure 7. Visualization of predictions and feature maps across
stages, where Fi
0denotes the feature map for stage i. Each row
shows the outcomes from progressively adding modules.
mation with the assistance of scale interaction and rotated
convolution. With the scale interaction performed by CSIEand the orientation extraction performed by ARC, RMSINcan focus more keenly on the referred targets. Comparedwith the ﬁrst row, CSIE provides more accurate semantics
in the deeper layer, and ARC supplies the space prior, which
is important for rotated object segmentation.
These qualitative comparisons underscore the efﬁcacy of
our approach in addressing challenges related to scale vari-ations and orientation robustness, afﬁrming its capabilitiesin diverse scenarios.
6. Conclusion
In this paper, we propose RMSIN, a novel method adept
at navigating the complex scales and orientations prevalentin RRSIS. By integrating the IIM and CIM, RMSIN capa-bly addresses the wide range of spatial scales encountered
in aerial imagery. Additionally, the implementation of the
ARC offers a solid strategy for tackling various orientations.The construction of our expansive RRSIS-D dataset, featur-ing 17,402 image-caption-mask triplets provides an unpar-alleled resource in terms of scale and variety for rigorousevaluation. The comprehensive validation on the RRSIS-
D dataset not only underscores RMSIN’s superior perfor-mance but also establishes a new benchmark for future re-search in this domain.
Acknowledgement
This work was supported by National Key R&D Program
of China (No.2023YFB4502804), the National Science
Fund for Distinguished Y oung Scholars (No.62025603),
the National Natural Science Foundation of China (No.U21B2037, No. U22B2051, No. 62072389), the NationalNatural Science Fund for Y oung Scholars of China (No.62302411), China Postdoctoral Science Foundation (No.
2023M732948), the Natural Science Foundation of Fujian
Province of China (No.2021J01002, No.2022J06001), andpartially sponsored by CCF-NetEase ThunderFire Innova-tion Research Funding (NO. CCF-Netease 202301).
26665
References
[1] Zhaowei Cai, Gukyeong Kwon, Avinash Ravichandran, Er-
han Bas, Zhuowen Tu, Rahul Bhotika, and Stefan 0 Soatto.X-detr: A versatile architecture for instance-wise vision-language tasks. ArXiv , abs/2204.05626, 2022. 2
[2] Ding-Jie Chen, Songhao Jia, Yi-Chen Lo, Hwann-Tzong
Chen, and Tyng-Luh Liu. See-through-text grouping forreferring image segmentation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision(ICCV) , 2019. 2
[3] Yi-Wen Chen, Yi-Hsuan Tsai, Tiantian Wang, Yen-Y u Lin,
and Ming-Hsuan Yang. Referring expression object segmen-tation with caption-aware consistency. In British Machine
Vision Conference (BMVC) , 2019. 2
[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE Conference on Computer Vision and
Pattern Recognition , pages 248–255, 2009. 6
[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional trans-formers for language understanding. In North American
Chapter of the Association for Computational Linguistics ,
2019. 2
[6] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang.
Vision-language transformer and query generation for refer-
ring segmentation. In Proceedings of the IEEE International
Conference on Computer Vision , 2021. 2
[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. ICLR , 2021. 2
[8] Liuyun Duan and Florent Lafarge. Towards large-scale city
reconstruction from satellites. In European Conference on
Computer Vision , 2016. 1
[9] G.M. Foody. Remote sensing of tropical forest environ-
ments: Towards the monitoring of environmental resources
for sustainable development. International Journal of Re-
mote Sensing , 24:4035–4046, 2003. 1
[10] Jiaqi Gu, Hyoukjun Kwon, Dilin Wang, Wei Ye, Meng Li,
Y u-Hsin Chen, Liangzhen Lai, Vikas Chandra, and David Z.Pan. Multi-scale high-resolution vision transformer for se-mantic segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition(CVPR) , pages 12094–12103, 2022. 5
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 7
[12] Richang Hong, Daqing Liu, Xiaoyu Mo, Xiangnan He, and
Hanwang Zhang. Learning to compose and reason with lan-guage tree structures for visual grounding. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence , 44:684–
696, 2019. 2
[13] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Seg-
mentation from natural language expressions. Proceedingsof the European Conference on Computer Vision (ECCV) ,
2016. 1,2
[14] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor
Darrell, and Kate Saenko. Modeling relationships in ref-erential expressions with compositional modular networks.InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2017. 2
[15] Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, and
Huchuan Lu. Bi-directional relationship inferring networkfor referring image segmentation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) , 2020. 7
[16] Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Y unchao
Wei, Jizhong Han, Luoqi Liu, and Bo Li. Referring im-
age segmentation via cross-modal progressive comprehen-
sion. In
Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2020. 2,7
[17] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Y u,
Faxi Zhang, and Jizhong Han. Linguistic structure guidedcontext modeling for referring image segmentation. In Com-
puter Vision–ECCV 2020: 16th European Conference, Glas-
gow, UK, August 23–28, 2020, Proceedings, Part X 16 , pages
59–75, 2020. 2,7
[18] Jiayi Ji, Yiwei Ma, Xiaoshuai Sun, Yiyi Zhou, Y ongjian Wu,
and Rongrong Ji. Knowing what to learn: a metric-orientedfocal mechanism for image captioning. IEEE Transactions
on Image Processing , 31:4321–4335, 2022. 1
[19] Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, and Tie-
niu Tan. Locate then segment: A strong pipeline for refer-ring image segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition(CVPR) , pages 9858–9867, 2021. 2
[20] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel
Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr - mod-ulated detection for end-to-end multi-modal understanding.InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pages 1780–1790, 2021. 2
[21] Dongwon Kim, Namyup Kim, Cuiling Lan, and Suha Kwak.
Shatter and gather: Learning referring image segmentation
with text supervision. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV) , pages
15547–15557, 2023. 2
[22] Namyup Kim, Dongwon Kim, Cuiling Lan, Wenjun Zeng,
and Suha Kwak. Restr: Convolution-free referring im-age segmentation using transformers. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) , pages 18145–18154, 2022. 2
[23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and
Ross Girshick. Segment anything, 2023. 2,3
[24] Muchen Li and Leonid Sigal. Referring transformer: A
one-step approach to multi-task visual grounding. ArXiv ,
abs/2106.03089, 2021. 2
[25] Ruiyu Li, Kaican Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan
Qi, Xiaoyong Shen, and Jiaya Jia. Referring image seg-mentation via recurrent reﬁnement networks. In Proceed-
26666
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2018. 1,2,7
[26] Y ue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen
Qian, and Bo Li. A real-time cross-modality correlation ﬁl-tering method for referring expression comprehension. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR) , 2020. 2
[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. InComputer Vision–ECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,
P a r tV1 3 , pages 740–755. Springer, 2014. 3
[28] Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, and
Alan Y uille. Recurrent multimodal interaction for referringimage segmentation. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision (ICCV) , 2017. 1,2
[29] Chang Liu, Henghui Ding, Y ulun Zhang, and Xudong Jiang.
Multi-modal mutual attention and iterative interaction for re-ferring image segmentation. IEEE Transactions on Image
Processing , 32:3054–3065, 2023. 2
[30] Si Liu, Tianrui Hui, Shaofei Huang, Y unchao Wei, Bo Li,
and Guanbin Li. Cross-modal progressive comprehension
for referring segmentation. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 44:4761–4775, 2021. 7
[31] Ze Liu, Y utong Lin, Y ue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision , pages 10012–10022, 2021. 4,6,7
[32] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. In International Conference on Learning
Representations , 2017. 6
[33] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin
Wu, Cheng Deng, and Rongrong Ji. Multi-task collabora-tive network for joint referring expression comprehensionand segmentation. In Proceedings of the IEEE/CVF Con-
ference on computer vision and pattern recognition , pages
10034–10043, 2020. 2
[34] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan
Cao, Y ongjian Wu, Feiyue Huang, and Rongrong Ji. To-wards lightweight transformer via group-wise transforma-
tion for vision-and-language tasks. IEEE Transactions on
Image Processing , 31:3386–3398, 2022.
[35] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Y ongjian Wu, Y ue Gao,
and Rongrong Ji. Towards language-guided visual recog-nition via dynamic convolutions. International Journal of
Computer Vision , pages 1–19, 2023. 2
[36] Y unpeng Luo, Jiayi Ji, Xiaoshuai Sun, Liujuan Cao,
Y ongjian Wu, Feiyue Huang, Chia-Wen Lin, and RongrongJi. Dual-level collaborative transformer for image caption-ing. In Proceedings of the AAAI conference on artiﬁcial in-
telligence , pages 2286–2293, 2021. 1
[37] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang,
and Rongrong Ji. X-clip: End-to-end multi-grained con-trastive learning for video-text retrieval. In Proceedings
of the 30th ACM International Conference on Multimedia ,
pages 638–647, 2022.[38] Yiwei Ma, Jiayi Ji, Xiaoshuai Sun, Yiyi Zhou, and Rongrong
Ji. Towards local visual modeling for image captioning. Pat-
tern Recognition , 138:109420, 2023. 1
[39] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, and Kevin Murphy. Generation and comprehen-sion of unambiguous object descriptions. In Computer Vi-
sion and Pattern Recognition , 2016. 2
[40] Edgar Margffoy-Tuay, Juan C. Perez, Emilio Botero, and
Pablo Arbelaez. Dynamic multimodal instance segmentationguided by natural language queries. In Proceedings of the
European Conference on Computer Vision (ECCV) , 2018. 2
[41] V arun K. Nagaraja, Vlad I. Morariu, and Larry S. Davis.
Modeling context between objects for referring expressionunderstanding. In ECCV , 2016. 2
[42] Shuyi Ouyang, Hongyi Wang, Shiao Xie, Ziwei Niu,
Ruofeng Tong, Yen-Wei Chen, and Lanfen Lin. Slvit: Scale-wise language-guided vision transformer for referring imagesegmentation. pages 1294–1302, 2023. 2
[43] Yifan Pu, Yiru Wang, Zhuofan Xia, Yizeng Han, Y ulin
Wang, Weihao Gan, Zidong Wang, Shiji Song, and GaoHuang. Adaptive rotated convolution for rotated object
detection. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV) , pages 6589–6600,
2023. 6
[44] David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly
Kochanski, Alexandre Lacoste, Kris Sankaran, An-drew Slavin Ross, Nikola Milojevic-Dupont, NatashaJaques, Anna Waldman-Brown, Alexandra Sasha Luccioni,Tegan Maharaj, Evan D. Sherwin, Surya Karthik Mukkav-illi, Konrad Paul Kording, Carla P . Gomes, Andrew Y . Ng,Demis Hassabis, John C. Platt, Felix Creutzig, Jennifer T.Chayes, and Y oshua Bengio. Tackling climate change with
machine learning. ACM Computing Surveys (CSUR) , 55:1 –
96, 2019. 1
[45] Y uxi Sun, Shanshan Feng, Xutao Li, Y unming Ye, Jian
Kang, and Xu Huang. Visual grounding in remote sensingimages. Proceedings of the 30th ACM International Confer-
ence on Multimedia , 2022. 2
[46] Ashish V aswani, Noam M. Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, andIllia Polosukhin. Attention is all you need. In Neural Infor-
mation Processing Systems , 2017. 4,5
[47] Haowei Wang, Jiayi Ji, Yiyi Zhou, Y ongjian Wu, and Xi-
aoshuai Sun. Towards real-time panoptic narrative ground-ing by an end-to-end grounding network. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , pages 2528–
2536, 2023. 2
[48] Zhaoqing Wang, Y u Lu, Qiang Li, Xunqiang Tao, Yandong
Guo, Mingming Gong, and Tongliang Liu. Cris: Clip-
driven referring image segmentation. In Proceedings of
the IEEE/CVF conference on computer vision and patternrecognition , 2022. 2
[49] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac, TimRault, R ´emi Louf, Morgan Funtowicz, and Jamie Brew.
Transformers: State-of-the-art natural language processing.InConference on Empirical Methods in Natural Language
Processing , 2019. 6
26667
[50] Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, and
Subhransu Maji. Phrasecut: Language-based image segmen-
tation in the wild. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2020. 6
[51] Changli Wu, Yiwei Ma, Qi Chen, Haowei Wang, Gen Luo,
Jiayi Ji, and Xiaoshuai Sun. 3d-stmn: Dependency-drivensuperpoint-text matching network for end-to-end 3d refer-ring expression segmentation. Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence , 2024. 2
[52] Jianzong Wu, Xiangtai Li, Xia Li, Henghui Ding, Y u Tong,
and Dacheng Tao. Towards robust referring image segmen-
tation. ArXiv , abs/2209.09554, 2022. 2
[53] Y ongchao Xu, Mingtao Fu, Qimeng Wang, Y ukang Wang,
Kai Chen, Gui-Song Xia, and Xiang Bai. Gliding vertex onthe horizontal bounding box for multi-oriented object detec-tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 43(4):1452–1459, 2021. 6
[54] Danni Yang, Jiayi Ji, Xiaoshuai Sun, Haowei Wang, Yinan
Li, Yiwei Ma, and Rongrong Ji. Semi-supervised panop-tic narrative grounding. In Proceedings of the 31st ACM
International Conference on Multimedia , pages 7164–7174,
2023. 2
[55] Xue Yang, Xiaojiang Yang, Jirui Yang, Qi Ming, Wentao
Wang, Qi Tian, and Junchi Yan. Learning high-precisionbounding box for rotated object detection via kullback-leibler divergence. ArXiv , abs/2106.01883, 2021. 6
[56] Xue Yang, Y ue Zhou, Gefan Zhang, Jitui Yang, Wentao
Wang, Junchi Yan, Xiaopeng Zhang, and Qi Tian. The kﬁouloss for rotated object detection. ArXiv , abs/2201.12558,
2022. 6
[57] Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing
Huang, Dong Y u, and Jiebo Luo. A fast and accurate one-
stage approach to visual grounding. In Proceedings of the
IEEE/CVF International Conference on Computer Vision(ICCV) , 2019. 2
[58] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-
shuang Zhao, and Philip H.S. Torr. Lavt: Language-awarevision transformer for referring image segmentation. In2022 IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR) , pages 18134–18144, 2022. 1,2,
7
[59] Alias-z yatengLG and horffmanwang. Isat with segment
anything: Image segmentation annotation tool with segmentanything, 2023. 3
[60] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.
Cross-modal self-attention network for referring image seg-mentation. In 2019 IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 10494–10503,
2019. 2
[61] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.
Cross-modal self-attention network for referring image seg-
mentation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 10502–10511,
2019. 2,7
[62] Licheng Y u, Patrick Poirson, Shan Yang, Alexander C Berg,
and Tamara L Berg. Modeling context in referring expres-sions. In ECCV , pages 69–85, 2016. 1,2[63] Zhenghang Y uan, Lichao Mou, Y uansheng Hua, and
Xiao Xiang Zhu. Rrsis: Referring remote sensing image seg-
mentation, 2023. 2,3,4,7
[64] Yang Zhan, Zhitong Xiong, and Y uan Y uan. Rsvg: Exploring
data and models for visual grounding on remote sensing data.
IEEE Transactions on Geoscience and Remote Sensing , 61:
1–13, 2023.
2,3,6
[65] Hanwang Zhang, Y ulei Niu, and Shih-Fu Chang. Ground-
ing referring expressions in images by variational context.InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2018. 2
[66] Chaoyang Zhu, Yiyi Zhou, Y unhang Shen, Gen Luo, Xingjia
Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xiaoshuai Sun,and Rongrong Ji. Seqtr: A simple yet universal network forvisual grounding. In Computer Vision–ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022,Proceedings, Part XXXV , pages 598–615. Springer, 2022. 2
[67] Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, and Anton
van den Hengel. Parallel attention: A uniﬁed framework forvisual object discovery through dialogs and queries. In Pro-
ceedings of the IEEE Conference on Computer Vision andPattern Recognition (CVPR) , 2018. 2
26668
