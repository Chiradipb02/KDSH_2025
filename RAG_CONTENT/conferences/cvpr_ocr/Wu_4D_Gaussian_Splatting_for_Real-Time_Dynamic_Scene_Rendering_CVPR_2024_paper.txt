4D Gaussian Splatting for Real-Time Dynamic Scene Rendering
Guanjun Wu1*, Taoran Yi2*, Jiemin Fang3†, Lingxi Xie3, Xiaopeng Zhang3,
Wei Wei1, Wenyu Liu2, Qi Tian3, Xinggang Wang2† ‡
1School of CS, Huazhong University of Science and Technology
2School of EIC, Huazhong University of Science and Technology3Huawei Inc.
{guajuwu, taoranyi, weiw, liuwy, xgwang }@hust.edu.cn
{jaminfong, 198808xc, zxphistory }@gmail.com tian.qi1@huawei.com
Figure 1. Our method achieves real-time rendering‡for dynamic scenes at high image resolutions while maintaining high rendering quality.
The right figure is tested on synthetic datasets, where the radius of the dot corresponds to the training time. “Res”: resolution.
‡The rendering speed not only depends on the image resolution but also the number of 3D Gaussians and the scale of deformation fields which are determined by the complexity
of the scene.
Abstract
Representing and rendering dynamic scenes has been an
important but challenging task. Especially, to accurately
model complex motions, high efficiency is usually hard to
guarantee. To achieve real-time dynamic scene rendering
while also enjoying high training and storage efficiency, we
propose 4D Gaussian Splatting (4D-GS) as a holistic rep-
resentation for dynamic scenes rather than applying 3D-GS
for each individual frame. In 4D-GS, a novel explicit rep-
resentation containing both 3D Gaussians and 4D neural
voxels is proposed. A decomposed neural voxel encoding
algorithm inspired by HexPlane is proposed to efficiently
build Gaussian features from 4D neural voxels and then a
lightweight MLP is applied to predict Gaussian deforma-
tions at novel timestamps. Our 4D-GS method achieves
real-time rendering under high resolutions, 82 FPS at an
800×800 resolution on an RTX 3090 GPU while main-
taining comparable or better quality than previous state-
of-the-art methods. More demos and code are available at
*Equal contributions.
†Project Lead.
‡Corresponding author.https://guanjunwu.github.io/4dgs/ .
1. Introduction
Novel view synthesis (NVS) stands as a critical task in the
domain of 3D vision and plays a vital role in many appli-
cations, e.g. VR, AR, and movie production. NVS aims
at rendering images from any desired viewpoint or times-
tamp of a scene, usually requiring modeling the scene ac-
curately from several 2D images. Dynamic scenes are quite
common in real scenarios, rendering which is important but
challenging as complex motions need to be modeled with
both spatially and temporally sparse input.
NeRF [32] has achieved great success in synthesizing
novel view images by representing scenes with implicit
functions. The volume rendering techniques [7] are in-
troduced to connect 2D images and 3D scenes. However,
the original NeRF method bears big training and rendering
costs. Though some NeRF variants [5, 8, 10, 11, 33, 44, 47]
reduce the training time from days to minutes, the rendering
process still bears a non-negligible latency.
Recent 3D Gaussian Splatting (3D-GS) [19] signifi-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20310
cantly boosts the rendering speed to a real-time level by
representing the scene as 3D Gaussians. The cumbersome
volume rendering in the original NeRF is replaced with ef-
ficient differentiable splatting [57], which directly projects
3D Gaussian onto the 2D image plane. 3D-GS not only en-
joys real-time rendering speed but also represents the scene
more explicitly, making it easier to manipulate the scene
representation.
However, 3D-GS focuses on the static scenes. Extend-
ing it to dynamic scenes as a 4D representation is a reason-
able, important but difficult topic. The key challenge lies in
modeling complicated point motions from sparse input. 3D-
GS holds a natural geometry prior by representing scenes
with point-like Gaussians. One direct and effective exten-
sion approach is to construct 3D Gaussians at each times-
tamp [30] but the storage/memory cost will multiply espe-
cially for long input sequences. Our goal is to construct
a compact representation while maintaining both training
and rendering efficiency, i.e. 4D Gaussian Splatting ( 4D-
GS). To this end, we propose to represent Gaussian mo-
tions and shape changes by an efficient Gaussian deforma-
tion field network, containing a temporal-spatial structure
encoder and an extremely tiny multi-head Gaussian defor-
mation decoder. Only one set of canonical 3D Gaussians is
maintained. For each timestamp, the canonical 3D Gaus-
sians will be transformed by the Gaussian deformation field
into new positions with new shapes. The transformation
process represents both the Gaussian motion and deforma-
tion. Note that different from modeling motions of each
Gaussian separately [30, 55], the spatial-temporal structure
encoder can connect different adjacent 3D Gaussians to pre-
dict more accurate motions and shape deformation. Then
the deformed 3D Gaussians can be directly splatted for ren-
dering the according-timestamp image. Our contributions
can be summarized as follows.
• An efficient 4D Gaussian splatting framework with an ef-
ficient Gaussian deformation field is proposed by mod-
eling both Gaussian motion and Gaussian shape changes
across time.
• A multi-resolution encoding method is proposed to con-
nect the nearby 3D Gaussians and build rich 3D Gaussian
features by an efficient spatial-temporal structure encoder.
• 4D-GS achieves real-time rendering on dynamic scenes,
up to 82 FPS at a resolution of 800 ×800 for synthetic
datasets and 30 FPS at a resolution of 1352 ×1014 in real
datasets, while maintaining comparable or superior per-
formance than previous state-of-the-art (SOTA) methods
and shows potential for editing and tracking in 4D scenes.
2. Related Works
In this section, we simply review the difference of dynamic
NeRFs in Sec. 2.1, then discuss the point clouds-based neu-
ral rendering algorithm in Sec. 2.2.
(c) 4D Gaussian Splatting
Gaussian Deformation Field F퐺,푡￿Deformed 3D Gaussians 퐺￿Original 3D Gaussians G
Gaussian Rasterization Paths(a) Canonical Mapping Volume Rendering
Canonical Mapped Points
Original Sampled Points
The Canonical Mapped Ray
The Original Cast Ray
Original Sampled Points
The Original Cast Ray
Time Features of the Points
푡￿푡￿푡￿
(b) Time-aware Volume Rendering Figure 2. Illustration of different dynamic scene rendering meth-
ods. (a) Points are sampled in the casted ray during volume ren-
dering. The point deformation fields proposed in [8, 39] map the
points into a canonical space. (b) Time-aware volume rendering
computes the features of each point directly and does not change
the rendering path. (c) The Gaussian deformation field converts
original 3D Gaussians into another group of 3D Gaussians with a
certain timestamp.
2.1. Novel View Synthesis
Novel view synthesis is a important and challenging task in
3D reconstruction. Much approaches are proposed to rep-
resent a 3D object and render novel views. Efficient repre-
sentations such as light fields [4], mesh [6, 15, 24, 46], vox-
els [16, 18, 23], multi-planes [9] can render high quality im-
age with enough supervisions. NeRF-based approaches [3,
32, 59] demonstrate that implicit radiance fields can ef-
fectively learn scene representations and synthesize high-
quality novel views. [35, 36, 39] have challenged the static
hypothesis, expanding the boundary of novel view syn-
thesis for dynamic scenes. [8] proposes to use an explicit
voxel grid to model temporal information, accelerating the
learning time for dynamic scenes to half an hour and ap-
plied in [17, 29, 56]. The proposed deformation-based
neural rendering methods are shown in Fig. 2 (a). Flow-
based [13, 25, 29, 48, 61] methods adopting warping algo-
rithm to synthesis novel views by blending nearby frames.
[5, 11, 12, 22, 44, 49] represent further advancements in
faster dynamic scene learning by adopting decomposed
neural voxels. They treat sampled points in each timestamp
individually as shown in Fig. 2 (b). [14, 27, 38, 50, 51, 53]
are efficient methods to handle multi-view setups. The
aforementioned methods though achieve fast training speed,
real-time rendering for dynamic scenes is still challenging,
especially for monocular input. Our method aims at con-
structing a highly efficient training and rendering pipeline
in Fig. 2 (c), while maintaining the quality, even for sparse
inputs.
20311
𝑥,𝑦,𝑧
𝑡𝑥𝑡 𝑦𝑡 𝑧𝑡
𝑥𝑦 𝑦𝑧 𝑥𝑧
Spatial-Temporal Structure Encoder
3D Gaussians 𝐺
MLP
Scaling 
Head 𝜑௦Rotation
Head 𝜑௥Position 
Head 𝜑௫
∆𝑟
∆𝑠
Multi-head Gaussian Deformation Decoder
∆𝑥,∆𝑦,∆𝑧
Splatting𝑥,𝑦,𝑧: Position of 3D Gaussians
𝑡: Timestamp
∆𝑥,∆𝑦,∆𝑧 : Deformation of Position
∆𝑟,∆𝑠: Deformation of CovarianceDeformed 3D 
Gaussians 𝐺ᇱ
Figure 3. The overall pipeline of our model. Given a group of 3D Gaussians G, we extract the center coordinate of each 3D Gaussian X
and timestamp tto compute the voxel feature by querying multi-resolution voxel planes. Then a tiny multi-head Gaussian deformation
decoder is used to decode the feature and get the deformed 3D Gaussians G′at timestamp t. The deformed Gaussians are then splatted to
the rendered image.
2.2. Neural Rendering with Point Clouds
Effectively representing 3D scenes remains a challenging
topic. The community has explored various neural repre-
sentations [32], e.g. meshes, point clouds [54], voxels [10],
and hybrid approaches [33, 47]. Point-cloud-based meth-
ods [28, 40, 41, 58] initially target at 3D segmentation and
classification. A representative approach for rendering pre-
sented in [1, 54] combines point cloud representations with
volume rendering, achieving rapid convergence speed even
for dynamic novel view synthesis [34, 61]. [20, 21, 42]
adopt differential point rendering technique for scene recon-
structions.
Recently, 3D-GS [19] is notable for its pure explicit
representation and differential point-based splatting meth-
ods, enabling real-time rendering of novel views. Dy-
namic3DGS [30] models dynamic scenes by tracking the
position and variance of each 3D Gaussian at each times-
tamp ti. An explicit table is utilized to store information
about each 3D Gaussian at every timestamp, leading to a
linear memory consumption increase, denoted as O(tN), in
which Nis num of 3D Gaussians. For long-term scene re-
construction, the storage cost will become non-negligible.
The memory complexity of our approach only depends
on the number of 3D Gaussians and parameters of Gaus-
sians deformation fields network F, which is denoted as
O(N+F). [55] adds a marginal temporal Gaussian distri-
bution into the origin 3D Gaussians, which uplift 3D Gaus-
sians into 4DHowever, it may cause each 3D Gaussian to
only focus on their local temporal space. [26] track each
3D Gaussians individually. Our approach also models 3D
Gaussian motions but with a compact network, resulting in
highly efficient training efficiency and real-time rendering.3. Preliminary
In this section, we simply review the representation and ren-
dering process of 3D-GS [19] in Sec. 3.1 and the formula of
dynamic NeRFs in Sec. 3.2.
3.1. 3D Gaussian Splatting
3D Gaussians [19] is an explicit 3D scene representation in
the form of point clouds. Each 3D Gaussian is characterized
by a covariance matrix Σand a center point X, which is
referred to as the mean value of the Gaussian:
G(X) =e−1
2XTΣ−1X. (1)
For differentiable optimization, the covariance matrix Σcan
be decomposed into a scaling matrix Sand a rotation matrix
R:
Σ =RSSTRT. (2)
When rendering novel views, differential splatting [57]
is employed for the 3D Gaussians within the camera planes.
As introduced by [62], using a viewing transform matrix W
and the Jacobian matrix Jof the affine approximation of
the projective transformation, the covariance matrix Σ′in
camera coordinates can be computed as
Σ′=JWΣWTJT. (3)
In summary, each 3D Gaussian is characterized by the fol-
lowing attributes: position X ∈R3, color defined by spher-
ical harmonic (SH) coefficients C ∈Rk(where krepre-
sents nums of SH functions), opacity α∈R, rotation factor
r∈R4, and scaling factor s∈R3. Specifically, for each
pixel, the color and opacity of all the Gaussians are com-
puted using the Gaussian’s representation Eq. 1. The blend-
ing of Nordered points that overlap the pixel is given by
20312
the formula:
C=X
i∈Nciαii−1Y
j=1(1−αi). (4)
Here, ci,αirepresents the density and color of this point
computed by a 3D Gaussian Gwith covariance Σmulti-
plied by an optimizable per-point opacity and SH color co-
efficients.
3.2. Dynamic NeRFs with Deformation Fields
All the dynamic NeRF algorithms can be formulated as:
c, σ=M(x, d, t, λ ), (5)
whereMis a mapping that maps 8D space (x, d, t, λ )to 4D
space (c, σ). Where xreveals to the spatial point, λis the
optional input as used to build topological and appearance
changes in [36], and dstands for view-dependency.
As is shown in Fig. 2 (a), all the deformation NeRF based
method which estimate the world-to-canonical mapping
by a deformation network ϕt: (x, t)→∆x. Then a net-
work is introduced to compute volume density and view-
dependent RGB color from each ray. The formula for ren-
dering can be expressed as:
c, σ=NeRF (x+ ∆x, d, λ), (6)
where ‘NeRF’ stands for vanilla NeRF pipeline, λis a
frame-dependent code to model the topological and appear-
ance changes [31, 36].
However, our 4D Gaussian splatting framework presents
a novel rendering technique. We successfully compute the
canonical-to-world mapping by a Gaussian deformation
field network Fat the time tdirectly and differential splat-
ting [19] is followed, which enables the skill of computing
backward flow and tracking for 3D Gaussians.
4. Method
Sec. 4.1 introduces the overall 4D Gaussian Splatting
framework. Then, the Gaussian deformation field is pro-
posed in Sec. 4.2. Finally, we describe the optimization
process in Sec. 4.3.
4.1. 4D Gaussian Splatting Framework
As shown in Fig. 3, given a view matrix M= [R, T], times-
tamp t, our 4D Gaussian splatting framework includes 3D
Gaussians Gand Gaussian deformation field network F.
Then a novel-view image ˆIis rendered by differential splat-
ting [57] Sfollowing ˆI=S(M,G′), where G′= ∆G+G.
Specifically, the deformation of 3D Gaussians ∆Gis in-
troduced by the Gaussian deformation field network ∆G=
F(G, t), in which the spatial-temporal structure encoder H
3D Gaussian Initialization Random Point Cloud Input
 4D Gaussian Joint Optimization
Iter 20000 Iter 3000 Iter 0Figure 4. Illustration of the optimization process. With static 3D
Gaussian initialization, our model can learn high-quality 3D Gaus-
sians of the motion part.
can encode both the temporal and spatial features of 3D
Gaussians fd=H(G, t), and the multi-head Gaussian de-
formation decoder Dcan decode the features and predict
each 3D Gaussian’s deformation ∆G=D(f), then the de-
formed 3D Gaussians G′can be introduced.
The rendering process of our 4D Gaussian Splatting is
depicted in Fig. 2 (c). Our 4D Gaussian splatting converts
the original 3D Gaussians Ginto another group of 3D Gaus-
siansG′given a timestamp t, maintaining the effectiveness
of the differential splatting as referred in [57].
4.2. Gaussian Deformation Field Network
The network to learn the Gaussian deformation field in-
cludes an efficient spatial-temporal structure encoder Hand
a Gaussian deformation decoder Dfor predicting the defor-
mation of each 3D Gaussian.
Spatial-Temporal Structure Encoder. Nearby 3D Gaus-
sians always share similar spatial and temporal information.
To model 3D Gaussians’ features effectively, we introduce
an efficient spatial-temporal structure encoder Hincluding
a multi-resolution HexPlane R(i, j)and a tiny MLP ϕdin-
spired by [5, 8, 11, 44]. While the vanilla 4D neural voxel
is memory-consuming, we adopt a 4D K-Planes [11] mod-
ule to decompose the 4D neural voxel into 6 planes. All 3D
Gaussians in a certain area can be contained in the bound-
ing plane voxels and Gaussian’s deformation can also be
encoded in nearby temporal voxels.
Specifically, the spatial-temporal structure encoder H
contains 6 multi-resolution plane modules Rl(i, j)and
a tiny MLP ϕd,i.e.H(G, t) = {Rl(i, j), ϕd|(i, j)∈
{(x, y),(x, z),(y, z),(x, t),(y, t),(z, t)}, l∈ {1,2}}. The
position µ= (x, y, z )is the mean value of 3D Gaussians
G. Each voxel module is defined by R(i, j)∈Rh×lNi×lNj,
where hstands for the hidden dim of features, Ndenotes
the basic resolution of voxel grid and lequals to the upsam-
pling scale. This entails encoding information of the 3D
20313
Ours
 TiNeuV ox Ground Truth K-Planes HexPlane
 3D-GS
Hook
 Jack
Figure 5. Visualization of synthesized datasets compared with other models [5, 8, 11, 17, 19, 49]. The rendering results of [11] are
displayed with a default green background. We have adopted their rendering settings.
Gaussians within the 6 2D voxel planes while considering
temporal information. The formula for computing separate
voxel features is as follows:
fh=[
lY
interp (Rl(i, j)),
(i, j)∈ {(x, y),(x, z),(y, z),(x, t),(y, t),(z, t)}.(7)
fh∈Rh∗lis the feature of neural voxels. ‘interp’ denotes
the bilinear interpolation for querying the voxel features lo-
cated at 4 vertices of the grid. The discussion of the produc-
tion process is similar to [11]. Then a tiny MLP ϕdmerges
all the features by fd=ϕd(fh).
Multi-head Gaussian Deformation Decoder. When all
the features of 3D Gaussians are encoded, we can com-
pute any desired variable with a multi-head Gaussian de-
formation decoder D={ϕx, ϕr, ϕs}. Separate MLPs are
employed to compute the deformation of position ∆X=
ϕx(fd), rotation ∆r=ϕr(fd), and scaling ∆s=ϕs(fd).
Then, the deformed feature (X′, r′, s′)can be addressed as:
(X′, r′, s′) = (X+ ∆X, r+ ∆r, s+ ∆s). (8)
Finally, we obtain the deformed 3D Gaussians G′=
{X′, s′, r′, σ,C}.
4.3. Optimization
3D Gaussian Initialization. [19] shows that 3D Gaus-
sians can be well-trained with structure from motion
(SfM) [43] points initialization. Similarly, 4D Gaussians
should also be fine-tuned in proper 3D Gaussian initializa-
tion. We optimize 3D Gaussians at initial 3000 iterations
for warm-up and then render images with 3D Gaussians
ˆI=S(M,G)instead of 4D Gaussians ˆI=S(M,G′). The
illustration of the optimization process is shown in Fig. 4.Loss Function. Similar to other reconstruction meth-
ods [8, 19, 39], we use the L1 color loss to supervise the
training process. A grid-based total-variational loss [5, 8,
11, 47] Ltvis also applied.
L=|ˆI−I|+Ltv. (9)
5. Experiment
In this section, we mainly introduce the hyperparameters
and datasets of our settings in Sec. 5.1 and the results be-
tween different datasets will be compared with [2, 5, 8, 11,
19, 27, 45, 49, 50] in Sec. 5.2. Then, ablation studies are
proposed to prove the effectiveness of our approaches in
Sec. 5.3 and more discussion about 4D-GS in Sec. 5.4. Fi-
nally, we discuss the limitation of our proposed 4D-GS in
Sec. 5.5.
5.1. Experimental Settings
Our implementation is primarily based on the PyTorch [37]
framework and tested in a single RTX 3090 GPU, and we’ve
fine-tuned our optimization parameters by the configuration
outlined in the 3D-GS [19]. More hyperparameters will be
shown in the appendix.
Synthetic Dataset. We primarily assess the performance
of our model using synthetic datasets, as introduced by D-
NeRF [39]. These datasets are designed for monocular
settings, although it’s worth noting that the camera poses
for each timestamp are close to randomly generated. Each
scene within these datasets contains dynamic frames, rang-
ing from 50 to 200 in number.
Real-world Datasets. We utilize datasets provided by
HyperNeRF [36] and Neu3D’s [22] as benchmark datasets
to evaluate the performance of our model in real-world sce-
narios. The Nerfies dataset is captured using one or two
20314
HyperNeRF
 OursBroom Banana
Chicken 3DPrinter
GT
 TiNeuVox
 3D-GS
 HyperNeRF Ours GT TiNeuVox 3D-GSFigure 6. Visualization of the HyperNeRF [36] datasets compared with other methods [8, 17, 19, 36]. ‘GT’ stands for ground truth images.
Table 1. Quantitative results on the synthesis dataset. The best and the second best results are denoted by pink and yellow. The rendering
resolution is set to 800 ×800. “Time” in the table stands for training times.
Model PSNR(dB) ↑SSIM ↑LPIPS ↓ Time ↓ FPS ↑Storage (MB) ↓
TiNeuV ox-B [8] 32.67 0.97 0.04 28 mins 1.5 48
KPlanes [11] 31.61 0.97 - 52 mins 0.97 418
HexPlane-Slim [5] 31.04 0.97 0.04 11m 30s 2.5 38
3D-GS [19] 23.19 0.93 0.08 10 mins 170 10
FFDNeRF [17] 32.68 0.97 0.04 - <1 440
MSTH [49] 31.34 0.98 0.02 6 mins - -
V4D [12] 33.72 0.98 0.02 6.9 hours 2.08 377
Ours 34.05 0.98 0.02 8 mins 82 18
cameras, following straightforward camera motion, while
the Neu3D’s dataset is captured using 15 to 20 static cam-
eras, involving extended periods and intricate camera mo-
tions. We use the points computed by SfM [43] from the
first frame of each video in Neu3D’s dataset and 200 frames
randomly selected in HyperNeRF’s.
5.2. Results
We primarily assess our experimental results using various
metrics, encompassing peak-signal-to-noise ratio (PSNR),
perceptual quality measure LPIPS [60], structural similar-
ity index (SSIM) [52] and its extensions including structural
dissimilarity index measure (DSSIM), multiscale structural
similarity index (MS-SSIM), FPS, training times and Stor-
age.
To assess the quality of novel view synthesis, we con-
ducted benchmarking against several state-of-the-art meth-
ods in the field, including [5, 8, 11, 12, 17, 19, 27, 49]. The
results are summarized in Tab. 1. While current dynamic
hybrid representations can produce high-quality results,
they often come with the drawback of rendering speed. The
lack of modeling dynamic motion part makes [19] fail to
reconstruct dynamic scenes. In contrast, our method en-joys both the highest rendering quality within the synthesis
dataset and exceptionally fast rendering speeds while keep-
ing extremely low storage consumption and convergence
time.
Additionally, the results obtained from real-world
datasets are presented in Tab. 2 and Tab. 3. It becomes
apparent that some NeRFs [2, 5, 45] suffer from slow
convergence speed, and the other grid-based NeRF meth-
ods [5, 8, 11, 49] encounter difficulties when attempting
to capture intricate object details. In stark contrast, our
methods research comparable rendering quality, fast con-
vergence, and excel in free-view rendering speed in indoor
cases. Though [27] addresses the high quality in compari-
son to ours, the need for multi-cam setups makes it hard to
model monocular scenes and other methods also limit free-
view rendering speed and storage.
5.3. Ablation Study
Spatial-Temporal Structure Encoder. The explicit Hex-
Plane encoder Rl(i, j)possesses the capacity to retain 3D
Gaussians’ spatial and temporal information, which can re-
duce storage consumption in comparison with purely ex-
plicit methods [30]. Discarding this module, we observe
20315
Table 2. Quantitative results on HyperNeRF’s [36] vrig dataset. Rendering resolution is set to 960 ×540.
Model PSNR(dB) ↑MS-SSIM ↑ Times ↓ FPS↑Storage(MB) ↓
Nerfies [35] 22.2 0.803 ∼hours <1 -
HyperNeRF [36] 22.4 0.814 32 hours <1 -
TiNeuV ox-B [8] 24.3 0.836 30 mins 1 48
3D-GS [19] 19.7 0.680 40 mins 55 52
FFDNeRF [17] 24.2 0.842 - 0.05 440
V4D [12] 24.8 0.832 5.5 hours 0.29 377
Ours 25.2 0.845 30 mins 34 61
Table 3. Quantitative results on the Neu3D’s [22] dataset, rendering resolution is set to 1352 ×1014.
Model PSNR(dB) ↑ D-SSIM ↓ LPIPS ↓ Time ↓ FPS↑ Storage (MB) ↓
NeRFPlayer [45] 30.69 0.034 0.111 6 hours 0.045 -
HyperReel [2] 31.10 0.036 0.096 9 hours 2.0 360
HexPlane-all* [5] 31.70 0.014 0.075 12 hours 0.2 250
KPlanes [11] 31.63 - - 1.8 hours 0.3 309
Im4D [27] 32.58 - 0.208 28 mins ∼5 93
MSTH [49] 32.37 0.015 0.056 20 mins 2(15‡) 135
Ours 31.15 0.016 0.049 40 mins 30 90
*: The metrics of the model are tested without “coffee martini” and resolution is set to 1024 ×768.
‡: The FPS is tested with fixed-view rendering.
that using only a shallow MLP ϕdfalls short in model-
ing complex deformations across various settings. Tab. 4
demonstrates that, while the model incurs minimal memory
costs, it does come at the expense of rendering quality.
Gaussian Deformation Decoder. Our proposed Gaus-
sian deformation decoder Ddecodes the features from the
spatial-temporal structure encoder H. All the changes in 3D
Gaussians can be explained by separate MLPs {ϕx, ϕr, ϕs}.
As is shown in Tab. 4, 4D Gaussians cannot fit dynamic
scenes well without modeling 3D Gaussian motion. Mean-
while, the movement of human body joints is typically man-
ifested as stretching and twisting of surface details in a
macroscopic view. If one aims to accurately model these
movements, the size and shape of 3D Gaussians should also
be adjusted accordingly. Otherwise, there may be underfit-
ting of details during excessive stretching, or an inability to
correctly simulate the movement of objects at a microscopic
level.
3D Gaussian Initialization. In some cases without
SfM [43] points initialization, training 4D-GS directly may
cause difficulty in convergence. Optimizing 3D Gaussians
for warm-up enjoys: (a) making some 3D Gaussians stay in
the dynamic part, which releases the pressure of large de-
formation learning by 4D Gaussians as shown in Fig. 4. (b)
learning proper 3D Gaussians Gand suggesting deforma-
tion fields paying more attention to the dynamic part. (c)
avoiding numeric errors in optimizing the Gaussian defor-
mation network Fand keeping the training process stable.
(a) Cook Spinach (b) Coffee MartiniFigure 7. Visualization of tracking with 3D Gaussians. Each line
in the figure of second rows stands for trajectories of 3D Gaussians
Tab. 4 also shows that if we train our model without the
warm-up coarse stage, the rendering quality will suffer.
5.4. Discussions
Tracking with 3D Gaussians. Tracking in 3D is also a
important task. [17] also shows tracking objects’ motion
in 3D. Different from dynamic3DGS [30], our methods
even can present tracking objects in monocular settings with
pretty low storage i.e. 10MB in 3D Gaussians Gand 8 MB
in Gaussian deformation field network F. Fig. 7 shows the
3D Gaussian’s deformation at certain timestamps.
20316
Table 4. Ablation studies on synthetic datasets using our proposed methods.
Model PSNR(dB) ↑SSIM ↑LPIPS ↓ Time ↓ FPS↑Storage (MB) ↓
Ours w/o HexPlane Rl(i, j) 27.05 0.95 0.05 4 mins 140 12
Ours w/o initialization 31.91 0.97 0.03 7.5 mins 79 18
Ours w/o ϕx 26.67 0.95 0.07 8 mins 82 17
Ours w/o ϕr 33.08 0.98 0.03 8 mins 83 17
Ours w/o ϕs 33.02 0.98 0.03 8 mins 82 17
Ours 34.05 0.98 0.02 8 mins 82 18
Figure 8. Visualization of composition with 4D Gaussians.
/uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000029/uni00000033/uni00000036/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni00000013/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000013/uni0000001c/uni00000013/uni00000013/uni00000013/uni00000013/uni00000033/uni00000052/uni0000004c/uni00000051/uni00000057/uni00000056
Figure 9. Visualization of the relationship between rendering
speed and numbers of 3D Gaussians in the rendered screens. All
the tests are finished in the synthesis dataset.
Composition with 4D Gaussians. Similar to dy-
namic3DGS [30], our proposed methods can also propose
editing in 4D Gaussians in Fig. 8. Thanks to the explicit
representation of 3D Gaussians, all the trained models
can predict deformed 3D Gaussians in the same space
following G′={G′
1,G′
2, ...,G′
n}and differential render-
ing [57] can project all the point clouds into viewpoints by
ˆI=S(M,G′).
Analysis of Rendering Speed. As is shown in Fig. 9, we
also test the relationship between points in the rendered
screen and rendering speed at the resolution of 800 ×800.We found that if the rendered points are lower than 30000,
the rendering speed can be up to 90. The config of Gaus-
sian deformation fields are discussed in the appendix. To
achieve render-time rendering speed, we should strike a bal-
ance among all the rendering resolutions, 4D Gaussians rep-
resentation including numbers of 3D Gaussians, and the ca-
pacity of the Gaussian deformation field network and any
other hardware constraints.
5.5. Limitations
Though 4D-GS can indeed attain rapid convergence and
yield real-time rendering outcomes in many scenarios, there
are a few key challenges to address. First, large motions,
the absence of background points, and the unprecise camera
pose cause the struggle of optimizing 4D Gaussians. What
is more, it is still challenging to 4D-GS also cannot split the
joint motion of static and dynamic Gaussiansparts under the
monocular settings without any additional supervision. Fi-
nally, a more compact algorithm needs to be designed to
handle urban-scale reconstruction due to the heavy query-
ing of Gaussian deformation fields by huge numbers of 3D
Gaussians.
6. Conclusion
This paper proposes 4D Gaussian splatting to achieve real-
time dynamic scene rendering. An efficient deformation
field network is constructed to accurately model Gaussian
motions and shape deformations, where adjacent Gaus-
sians are connected via a spatial-temporal structure encoder.
Connections between Gaussians lead to more complete de-
formed geometry, effectively avoiding avulsion. Our 4D
Gaussians can not only model dynamic scenes but also have
the potential for 4D objective tracking and editing.
Acknowledgments
This work was supported by the National Natural Science
Foundation of China (No. 62376102). The authors would
like to thank Haotong Lin for providing the quantitative re-
sults of Im4D [27].
20317
References
[1] Jad Abou-Chakra, Feras Dayoub, and Niko S ¨underhauf. Par-
ticlenerf: Particle based encoding for online neural radiance
fields in dynamic scenes. arXiv preprint arXiv:2211.04041 ,
2022. 3
[2] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael
Zollhoefer, Johannes Kopf, Matthew O’Toole, and Changil
Kim. Hyperreel: High-fidelity 6-dof video with ray-
conditioned sampling. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 16610–16620, 2023. 5, 6, 7
[3] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neu-
ral radiance fields. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 5855–5864,
2021. 2
[4] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erick-
son, Peter Hedman, Matthew Duvall, Jason Dourgarian, Jay
Busch, Matt Whalen, and Paul Debevec. Immersive light
field video with a layered mesh representation. ACM Trans-
actions on Graphics (TOG) , 39(4):86–1, 2020. 2
[5] Ang Cao and Justin Johnson. Hexplane: A fast representa-
tion for dynamic scenes. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 130–141, 2023. 1, 2, 4, 5, 6, 7
[6] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Den-
nis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk,
and Steve Sullivan. High-quality streamable free-viewpoint
video. ACM Transactions on Graphics (ToG) , 34(4):1–13,
2015. 2
[7] Robert A Drebin, Loren Carpenter, and Pat Hanrahan. V ol-
ume rendering. ACM Siggraph Computer Graphics , 22(4):
65–74, 1988. 1
[8] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-
aopeng Zhang, Wenyu Liu, Matthias Nießner, and Qi Tian.
Fast dynamic radiance fields with time-aware neural vox-
els. In SIGGRAPH Asia 2022 Conference Papers , pages 1–9,
2022. 1, 2, 4, 5, 6, 7
[9] John Flynn, Michael Broxton, Paul Debevec, Matthew Du-
Vall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and
Richard Tucker. Deepview: View synthesis with learned gra-
dient descent. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2367–
2376, 2019. 2
[10] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5501–5510, 2022. 1, 3
[11] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk
Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:
Explicit radiance fields in space, time, and appearance. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 12479–12488, 2023. 1,
2, 4, 5, 6, 7[12] Wanshui Gan, Hongbin Xu, Yi Huang, Shifeng Chen, and
Naoto Yokoya. V4d: V oxel for 4d novel view synthesis.
IEEE Transactions on Visualization and Computer Graph-
ics, 2023. 2, 6, 7
[13] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.
Dynamic view synthesis from dynamic monocular video. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 5712–5721, 2021. 2
[14] Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng,
Zicheng Liu, and Xin Tong. Mps-nerf: Generalizable 3d hu-
man rendering from multiview images. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 2022. 2
[15] Kaiwen Guo, Feng Xu, Yangang Wang, Yebin Liu, and
Qionghai Dai. Robust non-rigid motion tracking and sur-
face reconstruction using l0 regularization. In Proceedings
of the IEEE International Conference on Computer Vision ,
pages 3083–3091, 2015. 2
[16] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch,
Xueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-
Escolano, Rohit Pandey, Jason Dourgarian, et al. The re-
lightables: V olumetric performance capture of humans with
realistic relighting. ACM Transactions on Graphics (ToG) ,
38(6):1–19, 2019. 2
[17] Xiang Guo, Jiadai Sun, Yuchao Dai, Guanying Chen, Xiao-
qing Ye, Xiao Tan, Errui Ding, Yumeng Zhang, and Jingdong
Wang. Forward flow for novel view synthesis of dynamic
scenes. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 16022–16033, 2023. 2,
5, 6, 7
[18] Tao Hu, Tao Yu, Zerong Zheng, He Zhang, Yebin Liu, and
Matthias Zwicker. Hvtr: Hybrid volumetric-textural render-
ing for human avatars. In 2022 International Conference on
3D Vision (3DV) , pages 197–208. IEEE, 2022. 2
[19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,
and George Drettakis. 3d gaussian splatting for real-time
radiance field rendering. ACM Transactions on Graphics
(ToG) , 42(4):1–14, 2023. 1, 3, 4, 5, 6, 7
[20] Leonid Keselman and Martial Hebert. Approximate differ-
entiable rendering with algebraic surfaces. In European Con-
ference on Computer Vision , pages 596–614. Springer, 2022.
3
[21] Leonid Keselman and Martial Hebert. Flexible techniques
for differentiable rendering with 3d gaussians. arXiv preprint
arXiv:2308.14737 , 2023. 3
[22] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon
Green, Christoph Lassner, Changil Kim, Tanner Schmidt,
Steven Lovegrove, Michael Goesele, Richard Newcombe,
et al. Neural 3d video synthesis from multi-view video. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 5521–5531, 2022. 2, 5,
7
[23] Zhong Li, Yu Ji, Wei Yang, Jinwei Ye, and Jingyi Yu. Ro-
bust 3d human motion reconstruction via dynamic template
construction. In 2017 International Conference on 3D Vision
(3DV) , pages 496–505. IEEE, 2017. 2
[24] Zhong Li, Minye Wu, Wangyiteng Zhou, and Jingyi Yu. 4d
human body correspondences from panoramic depth maps.
20318
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 2877–2886, 2018. 2
[25] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.
Neural scene flow fields for space-time view synthesis of dy-
namic scenes. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 6498–
6508, 2021. 2
[26] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaus-
sian feature splatting for real-time dynamic view synthesis.
arXiv preprint arXiv:2312.16812 , 2023. 3
[27] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hu-
jun Bao, and Xiaowei Zhou. High-fidelity and real-time
novel view synthesis for dynamic scenes. In SIGGRAPH
Asia Conference Proceedings , 2023. 2, 5, 6, 7, 8
[28] Xingyu Liu, Mengyuan Yan, and Jeannette Bohg. Meteor-
net: Deep learning on dynamic 3d point cloud sequences. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 9246–9255, 2019. 3
[29] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu
Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Jo-
hannes Kopf, and Jia-Bin Huang. Robust dynamic radiance
fields. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 13–23, 2023. 2
[30] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and
Deva Ramanan. Dynamic 3d gaussians: Tracking by per-
sistent dynamic view synthesis. In 3DV, 2024. 2, 3, 6, 7,
8
[31] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,
Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-
worth. Nerf in the wild: Neural radiance fields for uncon-
strained photo collections. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 7210–7219, 2021. 4
[32] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021. 1,
2, 3
[33] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 41(4):1–15, 2022. 1, 3
[34] Byeongjun Park and Changick Kim. Point-dynrf: Point-
based dynamic radiance fields from a monocular video. In
Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision , pages 3171–3181, 2024. 3
[35] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien
Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo
Martin-Brualla. Nerfies: Deformable neural radiance fields.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 5865–5874, 2021. 2, 7
[36] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T
Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-
Brualla, and Steven M Seitz. Hypernerf: A higher-
dimensional representation for topologically varying neural
radiance fields. arXiv preprint arXiv:2106.13228 , 2021. 2,
4, 5, 6, 7[37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Ad-
vances in neural information processing systems , 32, 2019.
5
[38] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xi-
aowei Zhou. Representing volumetric videos as dynamic
mlp maps. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 4252–
4262, 2023. 2
[39] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
Francesc Moreno-Noguer. D-nerf: Neural radiance fields
for dynamic scenes. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
10318–10327, 2021. 2, 5
[40] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classification
and segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 652–660,
2017. 3
[41] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. Advances in neural information
processing systems , 30, 2017. 3
[42] Darius R ¨uckert, Linus Franke, and Marc Stamminger. Adop:
Approximate differentiable one-pixel point rendering. ACM
Transactions on Graphics (ToG) , 41(4):1–14, 2022. 3
[43] Johannes L Schonberger and Jan-Michael Frahm. Structure-
from-motion revisited. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
4104–4113, 2016. 5, 6, 7
[44] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,
Hongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural
4d decomposition for high-fidelity dynamic reconstruction
and rendering. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16632–
16642, 2023. 1, 2, 4
[45] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele
Chen, Junsong Yuan, Yi Xu, and Andreas Geiger. Nerf-
player: A streamable dynamic scene representation with de-
composed neural radiance fields. IEEE Transactions on Visu-
alization and Computer Graphics , 29(5):2732–2742, 2023.
5, 6, 7
[46] Zhuo Su, Lan Xu, Zerong Zheng, Tao Yu, Yebin Liu, and Lu
Fang. Robustfusion: Human volumetric capture with data-
driven visual cues using a rgbd camera. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part IV 16 , pages 246–264.
Springer, 2020. 2
[47] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance fields
reconstruction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5459–
5469, 2022. 1, 3, 5
[48] Fengrui Tian, Shaoyi Du, and Yueqi Duan. Monon-
erf: Learning a generalizable dynamic radiance field from
20319
monocular videos. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 17903–17913,
2023. 2
[49] Feng Wang, Zilong Chen, Guokang Wang, Yafei Song, and
Huaping Liu. Masked space-time hash encoding for efficient
dynamic scene reconstruction. Advances in neural informa-
tion processing systems , 2023. 2, 5, 6, 7
[50] Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, Yafei
Song, and Huaping Liu. Mixed neural voxels for fast multi-
view video synthesis. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 19706–
19716, 2023. 2, 5
[51] Yiming Wang, Qin Han, Marc Habermann, Kostas Dani-
ilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast
learning of neural implicit surfaces for multi-view recon-
struction. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 3295–3306, 2023. 2
[52] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing ,
13(4):600–612, 2004. 6
[53] Qingshan Xu, Weihang Kong, Wenbing Tao, and Marc Polle-
feys. Multi-scale geometric consistency guided and planar
prior assisted multi-view stereo. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence , 45(4):4945–4963,
2022. 2
[54] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin
Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:
Point-based neural radiance fields. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5438–5448, 2022. 3
[55] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li
Zhang. Real-time photorealistic dynamic scene representa-
tion and rendering with 4d gaussian splatting. arXiv preprint
arXiv:2310.10642 , 2023. 2, 3
[56] Taoran Yi, Jiemin Fang, Xinggang Wang, and Wenyu Liu.
Generalizable neural voxels for fast human radiance fields.
arXiv preprint arXiv:2303.15387 , 2023. 2
[57] Wang Yifan, Felice Serena, Shihao Wu, Cengiz ¨Oztireli,
and Olga Sorkine-Hornung. Differentiable surface splatting
for point-based geometry processing. ACM Transactions on
Graphics (TOG) , 38(6):1–14, 2019. 2, 3, 4, 8
[58] Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and
Pheng-Ann Heng. Pu-net: Point cloud upsampling network.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 2790–2799, 2018. 3
[59] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Koltun. Nerf++: Analyzing and improving neural radiance
fields. arXiv preprint arXiv:2010.07492 , 2020. 2
[60] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 6
[61] Kaichen Zhou, Jia-Xing Zhong, Sangyun Shin, Kai Lu,
Yiyuan Yang, Andrew Markham, and Niki Trigoni. Dyn-
point: Dynamic neural point for view synthesis. Advances in
Neural Information Processing Systems , 36, 2024. 2, 3[62] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and
Markus Gross. Surface splatting. In Proceedings of the
28th annual conference on Computer graphics and interac-
tive techniques , pages 371–378, 2001. 3
20320
