GoMAvatar: Efﬁcient Animatable Human Modeling from Monocular Video
Using Gaussians-on-Mesh
Jing Wen Xiaoming Zhao Zhongzheng Ren Alexander G. Schwing Shenlong Wang
University of Illinois Urbana-Champaign
{jw116, xz23, zr5, aschwing, shenlong }@illinois.edu
https://wenj.github.io/GoMAvatar/
……
Single VideoGaussians-on-Mesh
AnimatableReal-timeExplicit Geometry
43FPS
<latexit sha1_base64="4mSRiAOC1HPbUsbyd7QN48TyFAA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsN+3azSbsToQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IJHCoOt+O4W19Y3NreJ2aWd3b/+gfHjUNnGqGW+xWMa6E1DDpVC8hQIl7ySa0yiQ/CEY3878hyeujYjVPU4S7kd0qEQoGEUrNbFfrrhVdw6ySrycVCBHo1/+6g1ilkZcIZPUmK7nJuhnVKNgkk9LvdTwhLIxHfKupYpG3PjZ/NApObPKgISxtqWQzNXfExmNjJlEge2MKI7MsjcT//O6KYbXfiZUkiJXbLEoTCXBmMy+JgOhOUM5sYQyLeythI2opgxtNiUbgrf88ippX1S9y2qtWavUb/I4inACp3AOHlxBHe6gAS1gwOEZXuHNeXRenHfnY9FacPKZY/gD5/MH4xeNAQ==</latexit>t
Figure 1. GoMAvatar takes a monocular RGB video (left) as input to establish an explicit and accurate 4D representation of a dynamic
human. It can render efﬁciently at novel views and poses with state-of-the-art quality. Additionally, it is extremely compact ( 3.63MB per
subject), efﬁcient ( 43FPS), and seamlessly compatible with the graphics pipeline such as OpenGL.
Abstract
We introduce GoMAvatar, a novel approach for real-
time, memory-efﬁcient, high-quality animatable human
modeling. GoMAvatar takes as input a single monocular
video to create a digital avatar capable of re-articulation
in new poses and real-time rendering from novel view-
points, while seamlessly integrating with rasterization-
based graphics pipelines. Central to our method is the
Gaussians-on-Mesh (GoM) representation, a hybrid 3D
model combining rendering quality and speed of Gaussian
splatting with geometry modeling and compatibility of de-
formable meshes. We assess GoMAvatar on ZJU-MoCap,
PeopleSnapshot, and various YouTube videos. GoMA-
vatar matches or surpasses current monocular human mod-
eling algorithms in rendering quality and signiﬁcantly out-
performs them in computational efﬁciency (43 FPS) while
being memory-efﬁcient (3.63 MB per subject).
1. Introduction
High-ﬁdelity, animatable digital avatar modeling is crucial
for various applications such as movie making, healthcare,AR/VR, and simulation. Conventional approaches carried
out in Motion Capture (MoCap) studios are slow, expensive,
and cumbersome, due to costly wearable devices [ 42,52]
and intricate multi-view camera systems [ 28,74]. Hence, to
enable widespread personal use, affordable methods which
only rely on monocular RGB videos for creating digital
avatars are much desired.
Reconstruction of digital humans from monocular
videos has been studied intensively recently [ 16,25,64,
70,81]. The key lies in choosing a suitable 3D represen-
tation, ﬂexible for articulation, efﬁcient for rendering and
storage, and capable of modeling high-quality geometry and
appearance all while being easily integrated into graphics
pipelines. Despite various proposals, no animated 3D rep-
resentation has met all these needs. Neural ﬁelds based
avatars [ 16,27,70,81] offer photorealism, but they are
challenging to articulate and lack explicit geometry, mak-
ing them less compatible with game engines. Mesh-based
methods [ 58] excel in articulation and rendering but fall
short in modeling topological changes and high-quality ap-
pearance. Point-based methods [ 88] are limited by incom-
plete topology and surface geometry. Recent successes of
Gaussian splatting in neural rendering motivate extensions
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2059
to free-form dynamic scenes [ 73], but a knowledge gap ex-
ists in how to leverage Gaussians for articulatable humans .
Besides, the lack of explicit surface modeling of Gaussian
splats hinders their broader use in digital avatar modeling.
To address these challenges, we present GoMAvatar, a
novel digital avatar modeling framework. GoMAvatar op-
erates on a single monocular video and yields an articulated
character that encodes high-ﬁdelity appearance and geom-
etry. It is both articulable and memory-efﬁcient, render-
ing in real-time (see Fig. 1). Central to the framework is
a novel articulated human representation, which we refer
to as Gaussians-on-Mesh (GoM) (Sec. 3.1). GoM com-
bines rendering quality and speed of Gaussian splatting
with geometry modeling and compatibility of deformable
meshes. Speciﬁcally, GoM employs Gaussian splats for
rendering, offering ﬂexibility in modeling rich appearances
and enabling real-time performance (Sec. 3.2). GoM uti-
lizes a skeleton-driven deformable mesh, enabling the cre-
ation of compact, topologically complete digital avatars,
while easing mesh articulation through forward kinematics
(Sec. 3.3). Crucially, to integrate both representations, we
attach a Gaussian to each mesh face. This method differs
from traditional mesh techniques that rely on texturing or
vertex coloring to enhance rendering. It also differs from
standard freeform Gaussian splats, thereby better regulariz-
ing Gaussians for novel poses. Furthermore, to tackle view
dependency, we factorize the ﬁnal RGB color into a pseudo
albedo map rendering and a pseudo shading map prediction.
This entire representation can be inferred from a single in-
put video without additional training data (Sec. 3.5). We
ﬁnd this dual representation to balance performance and ef-
ﬁciency effectively. Importantly, the entire animation and
rendering of GoM are fully compatible with graphics en-
gines, such as OpenGL.
We conducted extensive experiments on the ZJU-MoCap
data [ 54], PeopleSnapshot [ 1] and YouTube videos. Go-
MAvatar matches or surpasses the rendering quality of
the best monocular human modeling algorithms (GoMA-
vatar reaches 30.37 dB PSNR in novel view synthesis and
30.31 dB PSNR in novel pose synthesis). Meanwhile, it
is faster than competing algorithms, reaching a rendering
speed of 43FPS on an NVIDIA A100 GPU and remains
compact in memory, only costing 3.63 MB per subject
(Fig. 2). To summarize, our main contributions are:
•We introduce the Gaussians-on-Mesh representation for
efﬁcient, high-ﬁdelity articulated human reconstruction
from a single video, combining Gaussian splats with de-
formable meshes for real-time, free-viewpoint rendering.
•We design a unique differentiable shading module for
view dependency, splitting color into a pseudo albedo
map from Gaussian splatting and a pseudo shading map
derived from the normal map.Figure 2. Our approach is simultaneously faster (represented
byxcoordinates of circle centers , smaller is better), memory-
efﬁcient (represented by circle size, smaller is better), and renders
at a higher quality (represented by ycoordinates of circle centers,
higher is better). The horizontal brown line denotes our PSNR.
2. Related Work
Representations for novel view synthesis. Several rep-
resentations have been proposed for the task of novel view
synthesis, such as light ﬁelds [ 3,17,36], layered representa-
tions [ 61,62,71,90], voxels [ 41,63], and meshes [ 18,23].
Recently, several works also demonstrated the effective-
ness of an implicit representation, i.e., a neural network,
for a scene [ 12,47,51]. Further, neural radiance ﬁelds
(NeRFs) [ 49] utilize a volume rendering equation [ 29] to
optimize the implicit representation, yielding high-quality
view synthesis. Follow-up works further improve and
demonstrate compelling rendering results [ 4–6,46,57,67,
82]. Meanwhile, other works use volume rendering equa-
tions to optimize more explicit representations [ 7,50,80],
largely accelerating the optimization procedure. Point-
based rendering ( e.g., Gaussian splatting [ 30,45,72]) has
recently been adopted for fast rendering. It models the
scenes as a set of 3D Gaussians, each equipped with ro-
tation, scale, and appearance-related features, and raster-
izes by projecting the 3D Gaussians to the 2D image plane.
To model dynamic scenes, [ 45,72] further extend the 3D
Gaussians, adding a time dependency. To regularize the 3D
Gaussians through time, [ 45] adds physically-based priors
during training, and [ 72] uses a neural network to predict
the deformation of Gaussians. Our approach is inspired
by the recent progress of point-based rendering to facili-
tate fast rendering. More concretely, we also use Gaussian
splatting for rendering. However, different from previous
approaches, we propose the Gaussians-on-Mesh represen-
tation that combines 3D Gaussians with a mesh representa-
tion. By doing so, we obtain fast rendering speed as well as
regularized deformation of 3D Gaussians.
Human modeling. Early works to model humans rely on
templates, e.g., SCAPE [ 2] and SMPL [ 43]. Later, [ 56,
59,60,86] utilize (pixel-aligned) image features to re-
construct human geometry and appearance from a single
2060
image. However, such human modeling is not animat-
able. ARCH [ 19,76] and S3 [ 77] incorporate reanima-
tion capabilities but they fall short in delivering high-quality
rendering. Recently, efforts on human geometry model-
ing exploit implicit representations [ 10,11,24,48,66].
Their use of 3D scans also limits their application. To
address this limitation, human modeling from videos has
received a lot of attention from the community: many
prior efforts utilize implicit representations and differen-
tiable renderers for either non-animatable [ 54] or animat-
able [ 16,22,25,37,39,53,55,64,69,70,75,81,83,89]
scene-speciﬁc human modeling while other efforts focus on
scene-agnostic modeling [ 9,14,21,31,34,35,56,85,87].
In this study, our approach focuses on scene-speciﬁc mod-
eling following prior works. Different from the common
pure implicit representations, we utilize an explicit repre-
sentation termed Gaussians-on-Mesh. The explicit canoni-
cal geometry enables us to apply well-deﬁned forward kine-
matics, such as linear blend skinning, to transform from the
canonical space to the observation space. In contrast, meth-
ods using implicit representations can only perform map-
ping in a backward manner, i.e., from the observation space
to the canonical space, which is inherently ill-posed and am-
biguous.
Real-time rendering of animatable human modeling.
The key to real-time rendering in our approach is the co-
design of an explicit geometry representation and rasteri-
zation: Gaussian splatting and mesh rasterization are faster
than volume rendering in general. This principle has been
explored by prior efforts to accelerate the rendering of
general-purpose NeRFs. Representative approaches pro-
pose to either bake [ 20,78,79] or cache [ 15] the trained
implicit representation. Another line of work exploits mesh-
based rasterization to boost the inference speed [ 13,38,78].
Inspired by the success, concurrent works explore efﬁcient
NeRF rendering for humans [ 16,58]. Note, [ 58] ﬁrstly
trains a NeRF representation and then bakes it into a mesh
for real-time rendering. However, the second baking stage
is shown to harm the rendering quality. In contrast, the pro-
posed Gaussians-on-Mesh representation is trained end-to-
end, achieving a superior quality-speed trade-off.
3. Gaussians-on-Mesh (GoM)
In the following, we present the Gaussians-on-Mesh (GoM)
representation, how to render it, and its articulation. The
goal of the proposed representation is to combine the ben-
eﬁts of both Gaussian splatting and meshes while alleviat-
ing some of their individual shortcomings. Concretely, by
using Gaussian splatting, we attain a high-quality real-time
rendering capability, achieving 43 FPS. By utilizing a mesh,
we conduct effective articulation in a forward manner while
also regularizing the underlying geometry.
Overview. Given a monocular video capturing a human
<latexit sha1_base64="JifhsSvrSYXmUYLoQ6J9mMjCPTw=">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFF7oRKtgHtNOSSdM2NJkMSUYpQ//DjQtF3Pov7vwbM+0stPVA4HDOvdyTE0ScaeO6305uZXVtfSO/Wdja3tndK+4fNLSMFaF1IrlUrQBryllI64YZTluRolgEnDaD8XXqNx+p0kyGD2YSUV/gYcgGjGBjpW5HYDNSIrmRd9Ou7BVLbtmdAS0TLyMlyFDrFb86fUliQUNDONa67bmR8ROsDCOcTgudWNMIkzEe0ralIRZU+8ks9RSdWKWPBlLZFxo0U39vJFhoPRGBnUxT6kUvFf/z2rEZXPoJC6PY0JDMDw1ijoxEaQWozxQlhk8swUQxmxWREVaYGFtUwZbgLX55mTTOyt55uXJfKVWvsjrycATHcAoeXEAVbqEGdSCg4Ble4c15cl6cd+djPppzsp1D+APn8wfTL5K8</latexit>GoMo
<latexit sha1_base64="Ny2xs9CdII+geSDQTigyTaeQ9m4=">AAAB+HicbVDLSsNAFJ34rPXRqEs3g0VwISWRoi6LblxWtA9oQphMJ+20M5MwD6GWfokbF4q49VPc+TdO2yy09cCFwzn3cu89ccao0p737aysrq1vbBa2its7u3sld/+gqVIjMWnglKWyHSNFGBWkoalmpJ1JgnjMSCse3kz91iORiqbiQY8yEnLUEzShGGkrRW4p4CYanMHgnvY4igaRW/Yq3gxwmfg5KYMc9cj9CropNpwIjRlSquN7mQ7HSGqKGZkUA6NIhvAQ9UjHUoE4UeF4dvgEnlilC5NU2hIaztTfE2PElRrx2HZypPtq0ZuK/3kdo5OrcExFZjQReL4oMQzqFE5TgF0qCdZsZAnCktpbIe4jibC2WRVtCP7iy8ukeV7xLyrVu2q5dp3HUQBH4BicAh9cghq4BXXQABgY8AxewZvz5Lw4787HvHXFyWcOwR84nz8JTJKz</latexit>µj,⌃j
<latexit sha1_base64="Ny2xs9CdII+geSDQTigyTaeQ9m4=">AAAB+HicbVDLSsNAFJ34rPXRqEs3g0VwISWRoi6LblxWtA9oQphMJ+20M5MwD6GWfokbF4q49VPc+TdO2yy09cCFwzn3cu89ccao0p737aysrq1vbBa2its7u3sld/+gqVIjMWnglKWyHSNFGBWkoalmpJ1JgnjMSCse3kz91iORiqbiQY8yEnLUEzShGGkrRW4p4CYanMHgnvY4igaRW/Yq3gxwmfg5KYMc9cj9CropNpwIjRlSquN7mQ7HSGqKGZkUA6NIhvAQ9UjHUoE4UeF4dvgEnlilC5NU2hIaztTfE2PElRrx2HZypPtq0ZuK/3kdo5OrcExFZjQReL4oMQzqFE5TgF0qCdZsZAnCktpbIe4jibC2WRVtCP7iy8ukeV7xLyrVu2q5dp3HUQBH4BicAh9cghq4BXXQABgY8AxewZvz5Lw4787HvHXFyWcOwR84nz8JTJKz</latexit>µj,⌃j
<latexit sha1_base64="1ujeAb301Ai+7g8rSYIdQcobxpg=">AAACIXicbZDLSgMxFIYzXut4G3XpJlgEF1JmStEui7pwWcFeoB2HTJppYzPJkGSEMsyruPFV3LhQpDvxZUwvC239IfDznXM4OX+YMKq0635ZK6tr6xubhS17e2d3b985OGwqkUpMGlgwIdshUoRRThqaakbaiSQoDhlphcPrSb31RKSigt/rUUL8GPU5jShG2qDAqSYPIsi6N4RpFGSP526e2wvIW0blPA+coltyp4LLxpubIpirHjjjbk/gNCZcY4aU6nhuov0MSU0xI7ndTRVJEB6iPukYy1FMlJ9NL8zhqSE9GAlpHtdwSn9PZChWahSHpjNGeqAWaxP4X62T6qjqZ5QnqSYczxZFKYNawElcsEclwZqNjEFYUvNXiAdIIqxNqLYJwVs8edk0yyXvolS5qxRrV/M4CuAYnIAz4IFLUAO3oA4aAINn8ArewYf1Yr1Zn9Z41rpizWeOwB9Z3z+8tKPg</latexit>po j,0po j,1po j,2
<latexit sha1_base64="1ujeAb301Ai+7g8rSYIdQcobxpg=">AAACIXicbZDLSgMxFIYzXut4G3XpJlgEF1JmStEui7pwWcFeoB2HTJppYzPJkGSEMsyruPFV3LhQpDvxZUwvC239IfDznXM4OX+YMKq0635ZK6tr6xubhS17e2d3b985OGwqkUpMGlgwIdshUoRRThqaakbaiSQoDhlphcPrSb31RKSigt/rUUL8GPU5jShG2qDAqSYPIsi6N4RpFGSP526e2wvIW0blPA+coltyp4LLxpubIpirHjjjbk/gNCZcY4aU6nhuov0MSU0xI7ndTRVJEB6iPukYy1FMlJ9NL8zhqSE9GAlpHtdwSn9PZChWahSHpjNGeqAWaxP4X62T6qjqZ5QnqSYczxZFKYNawElcsEclwZqNjEFYUvNXiAdIIqxNqLYJwVs8edk0yyXvolS5qxRrV/M4CuAYnIAz4IFLUAO3oA4aAINn8ArewYf1Yr1Zn9Z41rpizWeOwB9Z3z+8tKPg</latexit>po j,0po j,1po j,2
<latexit sha1_base64="1ujeAb301Ai+7g8rSYIdQcobxpg=">AAACIXicbZDLSgMxFIYzXut4G3XpJlgEF1JmStEui7pwWcFeoB2HTJppYzPJkGSEMsyruPFV3LhQpDvxZUwvC239IfDznXM4OX+YMKq0635ZK6tr6xubhS17e2d3b985OGwqkUpMGlgwIdshUoRRThqaakbaiSQoDhlphcPrSb31RKSigt/rUUL8GPU5jShG2qDAqSYPIsi6N4RpFGSP526e2wvIW0blPA+coltyp4LLxpubIpirHjjjbk/gNCZcY4aU6nhuov0MSU0xI7ndTRVJEB6iPukYy1FMlJ9NL8zhqSE9GAlpHtdwSn9PZChWahSHpjNGeqAWaxP4X62T6qjqZ5QnqSYczxZFKYNawElcsEclwZqNjEFYUvNXiAdIIqxNqLYJwVs8edk0yyXvolS5qxRrV/M4CuAYnIAz4IFLUAO3oA4aAINn8ArewYf1Yr1Zn9Z41rpizWeOwB9Z3z+8tKPg</latexit>po j,0po j,1po j,2
Figure 3. Gaussians-on-Mesh (GoM). We learn Gaussians in the
local coordinates of each triangle and transform them to the world
coordinate based on the triangle’s shape. We initialize the rotation
r✓,j2so(3)to zeros and scale s✓,j2R3to ones so that we start
with a Gaussian that’s thin along the normal axis of the triangle.
Meanwhile, the projection of the ellipsoid {x:(x µj)T⌃ 1
j(x 
µj)=1 }on the triangle recovers the Steiner ellipse. See Sec. 3.1
and the appendix for details.
subject of interest, we aim to learn a canonical Gaussians-
on-Mesh representation GoMc
✓such that we can render that
human in real-time given any camera intrinsics K2R3⇥3,
extrinsics E2SE(3), and a human pose P. Note, here and
below, parameters ✓indicate that the corresponding func-
tion or variable is learnable and superscript cindicates the
canonical pose space. To render, we ﬁrst articulate GoMc
✓
to the observation space to obtain
GoMo=Articulator ✓(GoMc
✓,P), (1)
where GoModenotes the Gaussians-on-Mesh representa-
tion in the observation space. To obtain a rendering with
resolution H⇥W, we formulate a neural renderer to yield
the human appearance I2RH⇥W⇥3and the alpha mask
M2RH⇥W⇥1. Formally,
(I,M)=Renderer ✓(K, E, GoMo). (2)
The ﬁnal rendering is obtained from a classical alpha-
composition based on Iand M. We will ﬁrst discuss
the details of the Gaussians-on-Mesh human representation
in Sec. 3.1and the rendering pipeline in Sec. 3.2. Then we
introduce how to articulate the Gaussians-on-Mesh repre-
sentation in Sec. 3.3.
3.1. Gaussians-on-Mesh Representation
The core of our approach is the Gaussians-on-Mesh (GoM)
representation in the canonical space. The design of the rep-
resentation is motivated by the following two key considera-
tions: 1) GoM can be rendered efﬁciently through Gaussian
splatting [ 30] which eliminates the need of dense samples
along rays used in volume rendering; 2) By attaching Gaus-
sians to a mesh, we effectively adapt the shapes of Gaus-
sians to different human poses and enable regularization.
2061
Formally, our canonical Gaussians-on-Mesh representa-
tion is speciﬁed via a collection of points and faces with
associated attributes:
GoMc
✓,{{vc
✓,i}V
i=1,{f✓,j}F
j=1}. (3)
Here, {vc
✓,i}V
i=1and{f✓,j}F
j=1represent Vvertices and F
triangle faces along with their related attributes respectively.
We further deﬁne a vertex as
vc
✓,i=(pc
✓,i,wi), (4)
where pc
✓,i2R3is the vertex coordinate and wi2RJrefers
to the linear blend skinning weights with respect to Jjoints.
We deﬁne the face as
f✓,j=(r✓,j,s✓,j,c✓,j,{ j,k}3
k=1). (5)
r✓,j2so(3)ands✓,j2R3deﬁne the rotation and scale
of the local Gaussian associated with a face. Further,
c✓,j2R3is the color vector. { j,k}3
k=1are the in-
dices of the three vertices belonging to the j-th face, where
 j,k2{1,...,V }. Note that we associate Gaussian pa-
rameters with faces. We will delve into the derivation of the
Gaussian distributions in the world coordinates for render-
ing in the following section.
3.2. Rendering
In contrast to directly computing the ﬁnal color as done by
prior monocular human rendering works [ 70,81], render-
ing of the Gaussians-on-Mesh representation decomposes
the RGB image Iinto the pseudo albedo map IGSand the
pseudo shading map S, i.e., the ﬁnal image Iis given by
I=IGS·S. (6)
Here, IGSis rendered by Gaussian splatting and Sis pre-
dicted from the normal map obtained from mesh rasteriza-
tion. We ﬁnd this combination of Gaussian splatting and
mesh rasterization to better capture view-dependent shad-
ing effects than each individual approach while retaining
efﬁciency. We use ‘pseudo’ because the decomposition is
not perfect. Even though, we will show that the pseudo
shading map encodes lighting effects to some extent.
We emphasize that rendering operates on the GoM rep-
resentation in the observation space (see Eq. ( 2)), i.e., on
GoMo,{{(po
i,wi)}V
i=1,{(r✓,j,s✓,j,c✓,j)}F
j=1}.(7)
Note, the only difference between GoMoand GoMc
✓deﬁned
in Eq. ( 3) is the use of observation space vertex coordinates
po
i. Sec. 3.3will provide more details about how to compute
po
ifrom the vertex coordinates in canonical space pc
✓,i.In greater detail, Gaussian splatting is used to render the
pseudo albedo map IGS, speciﬁed in Eq. ( 6), and the sub-
ject mask M, speciﬁed in Eq. ( 2). To obtain the pseudo
shading map S, speciﬁed in Eq. ( 6), we use the normal
map Nmeshobtained via standard mesh rasterization. Dur-
ing training, we also use the subject mask Mmeshwhich is
obtained through the SoftRasterizer [ 40]. We now discuss
the computation of IGSandS.
Pseudo albedo map IGSrendering. We render IGSand
Mwith Gaussian splatting given FGaussians in the world
coordinate system {Gj,N(µj,⌃j)}F
j=1and the corre-
sponding colors {c✓,j}F
j=1which are deﬁned in Eq. ( 5).F
indicates the number of faces.
Importantly, different from the original 3D Gaussian
splatting that directly learns Gaussian parameters within
the world coordinate system, we acquire these parame-
ters within the local coordinate frame of each triangle
face. Subsequently, we transform these local Gaussians
into the world coordinate system, taking into account the
deformations of the individual faces. This distinctive for-
mulation allows our Gaussian representation to dynami-
cally adapt to the varying shapes of triangles, which can
change across different human poses due to articulation.
Concretely, given a face and its local parameters f✓,j=
(r✓,j,s✓,j,c✓,j,{ j,k}3
k=1), the mean µjof a Gaussian in
world coordinates is the centroid of the face, i.e.,
µj=1
33X
k=1po
 j,k. (8)
po
 j,kis the coordinate of the triangle’s vertex. The Gaus-
sian’s covariance is
⌃j=Aj(RjSjST
jRT
j)AT
j. (9)
RjandSjare the matrices encoding rotation r✓,jand scale
s✓,j.Ajis the transformation matrix from local coordinates
to world coordinates which is a function of the face vertices,
i.e.,Aj=T({po
 j,k}3
k=1). We provide a detailed deriva-
tion of Ajin the supplementary material. Through Eq. ( 8)
and ( 9), Gaussians are dynamically adapted to the shapes of
triangles of different human poses.
Pseudo shading map Sprediction. For view-dependent
shading effects, we predict the pseudo shading map from
the mesh rasterized normal map Nmeshvia
RH⇥W⇥13S=Shading ✓( (Nmesh)).(10)
Here  (·)denotes the positional encoding [ 49].Shading ✓
is a1⇥1convolutional network that maps each pixel to a
scaling factor.
3.3. Articulation
Different from NeRF-based approaches [ 16,70,81] that
require the ill-posed backward mapping from observation
2062
space to canonical space, our articulation follows the mesh’s
forward articulation, i.e., from canonical space to observa-
tion space, taking advantage of our Gaussians-on-Mesh rep-
resentation.
The goal of the articulator deﬁned in Eq. ( 1) is to obtain
the Gaussians-on-Mesh representation in observation space,
i.e., GoMo(see Eq. ( 7)), given the canonical representation
GoMc
✓and a human pose P. Note, we only transform pc
✓,i
topo
ias all the other attributes are shared.
To transform, linear blend skinning (LBS) is applied
to warp the vertices to the observation space. For pose-
dependent non-rigid motion, we utilize a non-rigid motion
module to deform the canonical vertices before applying
LBS. We refer to the space after non-rigid deformation as
‘the non-rigidly transformed canonical space’.
Linear blend skinning. We adhere to the standard linear
blend skinning for the transformation of vertices from the
non-rigidly transformed canonical space into the observa-
tion space as R33po
i=
LBS(pnr
i,wi,P)=PJ
j=1wj
i(Rp
jpnr
i+tp
j)
PJ
k=1wk
i. (11)
In this equation, the human pose P={(Rp
j,tp
j)}J
j=1is rep-
resented by the rotations and translations of Jjoints. Each
vertex is associated with LBS weights denoted as wi. And
pnr
irepresents the coordinates in the non-rigidly transformed
canonical space, which we will elaborate on next.
Non-rigid deformation. To transform to the non-rigidly
transformed canonical space, we model a pose-dependent
non-rigid deformation before LBS. Speciﬁcally, we predict
an offset and add it to the i-th canonical vertex, i.e.,
pnr
i=pc
✓,i+NRDeformer ✓ 
 (pc
✓,i),P 
. (12)
NRDeformer refers to an MLP network.  (·)denotes the
sinusoidal positional encoding [ 49].
3.4. Pose Reﬁnement
Human poses are typically estimated from the image and
hence often inaccurate. Therefore, we follow Human-
NeRF [ 70] to add a pose reﬁnement module that learns to
correct the estimated poses. Speciﬁcally, given a human
pose ˆP={(ˆRp
j,tp
j)}J
j=1estimated from a video frame, we
predict a correction to the joint rotations via
{⇠j}J
j=1=PoseRefiner ✓⇣
{ˆRp
j}J
j=1⌘
. (13)
where ⇠j2SO(3). We obtain the updated pose P=
{(Rp
j,tp
j)}J
j=1={(ˆRp
j·⇠j,tp
j)}J
j=1, which is used
in Eq. ( 11) and ( 12).
It’s important to note that pose reﬁnement occurs only
during novel view synthesis and the training stage to com-
pensate for the inaccuracies in pose estimation from the
videos. It is not needed for animation.3.5. Training
We supervise the predicted RGB image Iand subject mask
Mwith ground-truth IgtandMgt. Our overall loss is
L=LI+↵lpipsLlpips+↵MLM+↵regLreg. (14)
Here, ↵⇤are weights for losses. LIandLMare the L1 loss
on the RGB images and subject masks respectively. Llpips
is the LPIPS loss [ 84] between predicted RGB image Iand
ground-truth Igt. We add additional regularization on the
underlying mesh via Lreg=
Lmask+↵lapLlap+↵normal Lnormal +↵colorLcolor.(15)
Lmask=kMmesh Mgtkis the regularization on the mesh
silhouette. Llap=1
NPN
i=1k ik2is the Laplacian smooth-
ing loss, where  iis the Laplacian coordinate of the i-th ver-
tex. Lnormal is the normal consistency loss that maximizes
the cosine similarity of adjacent face normals. Similar to
the normal consistency, we apply a color smoothness loss
denoted as Lcolor, which penalizes the differences in colors
between two adjacent faces.
We initialize the vertices and faces with SMPL [ 43]. We
initialize the r✓,jands✓,jin Eq. ( 5) to zeros and ones re-
spectively so that we start with a thin Gaussian whose vari-
ance in the face normal axis is small. Meanwhile, the pro-
jection of the ellipsoid {x:(x µj)T⌃ 1
j(x µj)=1 }
on the triangle recovers the Steiner ellipse (see Fig. 3). To
enhance the details, we upsample the canonical GoMc
✓us-
ing GoM subdivision during training. We ﬁrst subdivide
the underlying mesh by introducing new vertices at the cen-
ter of each edge, followed by replacing each face with four
smaller faces. The properties of each face, as described in
Eq. ( 5), are duplicated across the newly generated faces.
4. Experiments
We evaluate GoMAvatar on the ZJU-MoCap dataset [ 54],
the PeopleSnapshot dataset [ 1] and on YouTube videos,
comparing with state-of-the-art human avatar modeling
methods from monocular videos. We showcase our
method’s rendering quality under novel views and poses,
as well as its speed and geometry.
4.1. Experimental setup
Datasets. We validate our proposed approach on ZJU-
MoCap [ 54] data, PeopleSnapshot [ 1] data and Youtube
videos. ZJU-MoCap : The ZJU-MoCap dataset provides
a comprehensive multi-camera, multi-subject benchmark
for human rendering evaluation. It has 9 dynamic human
videos captured by 21 synchronized cameras. In our paper,
to ensure a fair comparison, we adhere to the training/test
split in MonoHuman [ 81] and follow their monocular video
human rendering setting. We validate our approach on six
2063
Novel view synthesisNovel pose synthesisInferencetime (ms)#Memory(MB)#PSNR"SSIM"LPIPS*#PSNR"SSIM"LPIPS*#Neural Body [54]28.72 0.9611 52.2528.54 0.9604 53.91212.316.76HumanNeRF [70]29.61 0.9625 38.4529.74 0.9655 34.791776.7245.73NeuMan [27]28.96 0.9479 60.7428.75 0.9406 62.353412.52.27MonoHuman [81]30.260.969230.9230.050.968431.515970.0280.67Ours30.370.968932.5330.340.968832.3923.23.63Table 1. Quantitative results on ZJU-MoCap dataset. Our results generally provide the best (or second best) quality across both novel
view and novel pose rendering while being the fastest and having the second smallest parameter size. ( best, second best)
CD# NC"
Neural Body [ 54] 5.1473 0.4985
HumanNeRF [ 70]2.8029 0.5039
MonoHuman [ 81]2.6303 0.5205
Ours 2.8364 0.6201
Table 2. Geometry quality evaluation. Our approach provides
the best normal consistency across all methods, and MonoHuman
achieves best quality in surface geometry. ( best, second best)
Novel view synthesis Inference
time (ms) # PSNR "SSIM "LPIPS #
Anim-NeRF [ 8] 28.89 0.9682 0.0206 217.00
InstantAvatar [ 26] 28.61 0.9698 0.0242 71.26
Ours 30.68 0.9767 0.0213 25.82
Table 3. Quantitative results on PeopleSnapshot dataset. Our
approach provides the best results regarding PSNR and SSIM
while being the fastest in inference. ( best, second best)
subjects (377, 386, 387, 392, 393, and 394) in the dataset.
For each subject, the ﬁrst 4/5 frames from Camera 0 are
used for training. We use the corresponding frames in the
remaining cameras to evaluate novel view synthesis, and
the last 1/5 frames from all views to evaluate novel pose
synthesis. PeopleSnapshot : The PeopleSnapshot dataset
provides monocular videos where humans rotate in front
of the cameras. We follow the evaluation protocol in In-
stantAvatar [ 26] to validate our approach. We report results
averaged on four subjects (f3c, f4c, m3c, and m4c) and re-
ﬁne the test poses. Youtube videos : We qualitatively vali-
date our approach on Youtube dancing videos used in Hu-
manNeRF [ 70]. We generate the subject masks with Medi-
aPipe [ 44], and the SMPL poses with PARE [ 33].
Baselines. We compare with state-of-the-art approaches
for single-video articulated human capturing algorithms, in-
cluding NeuralBody [ 54], HumanNeRF [ 70], NeuMan [ 27],
MonoHuman [ 81], Anim-NeRF [ 8] and InstantAvatar [ 26].
Similar to our method, these methods take as input a single
video and 3D skeleton and output an articulated neural hu-
man representation, that can facilitate both novel view and
novel pose synthesis.
Evaluation metrics. We report PSNR, SSIM and LPIPS or
LPIPS* ( =LPIPS ⇥1000 ) for novel view synthesis and
novel pose synthesis. To compare the geometry, we report
Chamfer Distance (CD) and the Normal Consistency (NC)
following the protocol in ARAH [ 69]. For normal consis-tency, we compute 1 L2distance between normals for 1)
each vertex in the ground-truth mesh; and 2) its closest ver-
tex in the predicted mesh. We also benchmark the inference
speed in milliseconds (ms) / frame on an NVIDIA A100
GPU and the memory cost (the size of parameters used in
inference).
4.2. Quantitative results
Tab. 1presents our results on ZJU-MoCap data follow-
ing MonoHuman’s split. In terms of perceptual per-
formance, our approach achieves PSNR/SSIM/LPIPS*
of 30.37/0.9689/32.53 on novel view synthesis and
30.34/0.9688/32.39 on novel pose synthesis, which is on
par with the top-performing competitive methods MonoHu-
man. Notably, in terms of inference time, our approach
achieves a rendering speed of 23.2ms/frame (43 FPS),
which is 257 ⇥faster than MonoHuman, 76 ⇥faster than
HumanNeRF, and more than 9 ⇥faster than any competing
algorithm. These results indicate that our proposed method
enables real-time articulated neural human rendering from a
single video. Meanwhile, our approach is memory-efﬁcient
(3.63 MB parameters), which is smaller than all competitive
methods except NeuMan [ 27].
We also evaluate the Chamfer distance and the nor-
mal consistency between predicted geometry and pseudo
ground-truth geometry in Tab. 2. Note that the pseudo
ground-truths are generated from NeuS [ 68] on all view-
points and are then ﬁltered, following ARAH [ 69]. Our ap-
proach signiﬁcantly outperforms NeRF-based approaches
in terms of normal consistency, which indicates that our
approach can learn meaningful geometry. Note that our
Chamfer distance is slightly worse than HumanNeRF and
MonoHuman. It is possibly due to the use of 3D Gaussians,
which have thickness in the surface normal direction. The
rendered mask is larger than the actual mesh’s silhouette.
Hence, our meshes are a bit smaller than the ‘real’ meshes.
Following InstantAvatar’s split, we evaluate our ap-
proach on four subjects in PeopleSnapshot dataset in Tab. 3.
Our approach achieves the PSNR/SSIM/LPIPS/inference
time of 30.68/0.9767/0.0213/25.82ms, signiﬁcantly out-
performing InstantAvatar’s 28.61/0.9698/0.0242/71.26ms.
Compared to Anim-NeRF’s PSNR/SSIM/LPIPS of
28.89/0.9682/0.0206, our PSNR and SSIM are signiﬁcantly
2064
(a) Ground truth(b) Neural Body(c) HumanNeRF(d) MonoHuman(e) GoMAvatar(ours)
Figure 4. Qualitative comparison to state-of-the-arts. In each pair, we render the RGB image and normal map. The normal map is
rendered from the extracted mesh. We show that our approach can produce realistic details in both rendered images and geometry, while
other approaches struggle to generate a smooth mesh.
(a) Reference (b) HumanNeRF (c) MonoHuman (d) OursFigure 5. Qualitative results on YouTube videos. The ﬁrst image
is the reference image. We compare novel view synthesis in the
ﬁrst row and novel pose synthesis in the second row.
better, while LPIPS is on par. Also, Anim-NeRF renders
at a speed of 217ms/frame on an Nvidia A100, while ours
achieves 25.82ms/frame, being 8.4 ⇥faster.
4.3. Qualitative results
Novel view synthesis. We provide a qualitative compar-
ison with NeuralBody, HumanNeRF and MonoHuman on
rendered images and normal maps in Fig. 4. The nor-
mal maps are rendered from the extracted meshes. As can
be seen from the ﬁgure, our approach captures ﬁne de-
tails, such as facial features and wrinkles, and avoids the
“ghost effect” and “ﬂoaters” observed in HumanNeRF’s
and MonoHuman’s output (see the armpit of the second sub-
ject in HumanNeRF’s rendering and ﬂoaters around Mono-
Human’s rendering). The ghost effect typically occurs when
two body parts come too close, an artifact due to Human-
NeRF’s and MonoHuman’s voxel-based inverse blend skin-
(a) Target pose (b) HumanNeRF (c) MonoHuman (d) OursFigure 6. Novel pose synthesis . Poses are from using poses gen-
erated from MDM [ 65].
ning. Speciﬁcally, limited by the resolution of the LBS
weights, the free space is affected by two unrelated body
parts and thus obtains a large foreground score. The ﬂoaters
are typical volume rendering artifacts as in other NeRF rep-
resentations. In contrast, our approach uses explicit geom-
etry and thus does not suffer from both issues. We addi-
tionally test our approach on YouTube dancing videos in the
ﬁrst row of Fig. 5. Note that the human poses and masks are
predicted and thus inaccurate. However, our method still
renders novel views well, while HumanNeRF and Mono-
Human suffer from imperfect masks and produce ﬂoaters.
Novel pose synthesis. We render novel poses generated
from MDM [ 65], as depicted in Fig. 6. Remarkably, our ap-
proach performs effectively even in extremely challenging
poses characterized by self-penetration, such as sitting. In
contrast, both HumanNeRF and MonoHuman lack the capa-
bility to handle such self-penetration, due to the voxel-based
inverse blend skinning (see the incomplete left hands). We
validate our approach on novel view synthesis using an in-
the-wild YouTube video, as illustrated in the second row
of Fig. 5. Speciﬁcally, when rendering the avatar in a leg-
crossing pose, both HumanNeRF and MonoHuman fail to
produce accurate results, whereas our approach success-
fully renders the pose with ﬁdelity.
2065
Gaussians MeshPSNR"SSIM"LPIPS*#X⇥30.06 0.9673 34.13⇥X28.93 0.9615 38.11XX30.36 0.9690 33.28Table 4. Ablations on scene representation for novel view syn-
thesis. Gaussians-on-Mesh achieves the best results.
4.4. Ablation studies
Canonical representation. We conduct ablation studies
on the Gaussians-on-Mesh (GoM) representation and 3D
Gaussians or meshes alone, as summarized in Tab. 4. In the
3D Gaussians experiment, we only use Gaussian splatting
for rendering, and supervise the rendered image and sub-
ject mask during training. We initialize the Gaussians’ cen-
troids as the vertices of the canonical T-pose SMPL mesh
and directly learn their centroids, rotations and scales in the
world coordinates, which differs from the triangle’s local
coordinates used in our approach. We also compare with
just using a mesh: We initialize using the canonical SMPL
mesh and attach the pseudo-albedo colors to the vertices.
We render the RGB image and subject mask with mesh ras-
terization [ 40]. We supervise the rendered image and sub-
ject mask and apply all regularizations in Eq. ( 15). We also
utilize the color decomposition in Eq. ( 6).
We ﬁnd 3D Gaussians alone suffer from overﬁtting:
without geometry regularization, Gaussians are too ﬂexible
and achieve similar rendering quality on training images,
while the outputs are undesirable during inference. When
using only the mesh, optimization is a known challenge.
In contrast, GoM alleviates these issues and combines the
strengths of both representations. GoM produces the high-
est rendering quality among the three representations.
Local Gaussians vs. world Gaussians. We compare three
choices of attaching Gaussians to the mesh: 1) World
Gaussians : We associate the Gaussian’s centroid with the
face’s centroid (Eq. ( 8)). However, we directly learn the r✓,j
ands✓,jin the world coordinates, i.e., ⌃j=RjSjST
j,RT
j,
where RjandSjare the matrix encodings of r✓,jands✓,j;
2)Local ﬁxed Gaussians : We follow Eqs. ( 8) and ( 9) to
compute a Gaussian’s mean and covariance in the world co-
ordinates. However, r✓,jands✓,jare ﬁxed so that the vari-
ance in the normal axis is small. Meanwhile, the projection
of the ellipsoid {x:(x µj)T⌃ 1
j(x µj)=1 }on the
triangle recovers the Steiner ellipse. 3) Local Gaussians :
We use Eqs. ( 8) and ( 9) to transform the Gaussians and r✓,j
ands✓,jare free variables.
We show the comparison in the top section of Tab. 5. In
terms of rendering quality, world Gaussians and local Gaus-
sians achieve similar performance. But world Gaussians
tend to enlarge the scales instead of stretching the faces, so
the geometry is worse. Local ﬁxed Gaussians can produce
equally good geometry, but lose rendering ﬂexibility.
Shading Module. As shown in the middle section of Tab. 5,PSNR "SSIM "LPIPS* # CD# NC"
World Gaussians 30.34 0.9689 33.99 4.3941 0.6223
Local Fixed Gaussians 30.27 0.9685 34.11 3.0898 0.6247
Local Flex. Gaussians 30.36 0.9690 33.28 3.0728 0.6366
w/o Shading 30.13 0.9684 32.07 3.0177 0.6360
w/ Shading 30.36 0.9690 33.28 3.0728 0.6366
w/o Subdivision 30.36 0.9690 33.28 3.0728 0.6366
w/ Subdivision 30.37 0.9689 32.53 2.8364 0.6201
Table 5. Ablation Studies .Top section : locally deformed Gaus-
sians help improve both geometry and rendering quality. Middle
section : our proposed shading module enhances rendering quality.
Bottom section : subdivision signiﬁcantly improves geometry.
(a)Pseudoshading map(b) Rendered imageFigure 7. Pseudo shading map. We visualize the pseudo shading
map and the rendered image for reference. Our approach learns
view-dependent shading effects as seen in the highlighted regions.
The pseudo shading map is normalized for better visualization.
without the shading module in Eq. ( 6)—that is, by directly
using IGSas the RGB prediction—our model achieves a
PSNR of 30.13. However, with the shading module in-
cluded, the PSNR increases to 30.36. We also visualize the
pseudo shading map, demonstrating that our shading mod-
ule learns lighting effects, as illustrated in Fig. 7.
GoM subdivision. We show in the bottom section of Tab. 5
that GoM subdivision enhances the LPIPS* from 33.28 to
32.53 and reduces the Chamfer distance from 3.0728 to
2.8364. Importantly, the geometry signiﬁcantly improves
with a more ﬁne-grained mesh. Note, this increases infer-
ence time to 23.2ms per frame from 17.5ms.
5. Conclusion
We introduce GoMAvatar, a framework designed for ren-
dering high-ﬁdelity, free-viewpoint images of a human per-
former, using a single input video. At the core of our
method is the Gaussians-on-Mesh representation. Paired
with forward articulation and neural rendering, our method
renders quickly while being memory efﬁcient. Notably, the
method handles in-the-wild videos well.
Acknowledgement
Project supported by Intel AI SRS gift, IBM IIDAI
Grant, Insper-Illinois Innovation Grant, NCSA Faculty Fel-
lowship, NSF Awards #2008387, #2045586, #2106825,
#2331878, #2340254, #2312102, and NIFA award 2020-
67021-32799. We thank NCSA for providing computing
resources. We thank Yiming Zuo for helpful discussions.
2066
References
[1]Thiemo Alldieck, Marcus A. Magnor, Weipeng Xu, Chris-
tian Theobalt, and Gerard Pons-Moll. Video based recon-
struction of 3D people models. In CVPR , 2018. 2,5
[2]Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-
bastian Thrun, Jim Rodgers, and James Davis. SCAPE:
shape completion and animation of people. ACM TOG , 2005.
2
[3]Benjamin Attal, Jia-Bin Huang, Michael Zollhoefer, Jo-
hannes Kopf, and Changil Kim. Learning Neural Light
Fields with Ray-Space Embedding. In CVPR , 2021. 2
[4]Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.
Mip-NeRF: A Multiscale Representation for Anti-Aliasing
Neural Radiance Fields. In ICCV , 2021. 2
[5]Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.
Srinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded
Anti-Aliased Neural Radiance Fields. In CVPR , 2022.
[6]Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.
Srinivasan, and Peter Hedman. Zip-NeRF: Anti-Aliased
Grid-Based Neural Radiance Fields. In ICCV , 2023. 2
[7]Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. TensoRF: Tensorial Radiance Fields. In ECCV ,
2022. 2
[8]Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao
Bao, and Huchuan Lu. Animatable Neural Radiance Fields
from Monocular RGB Video. arXiv , 2021. 6
[9]Jianchuan Chen, Wen Yi, Liqian Ma, Xu Jia, and Huchuan
Lu. GM-NeRF: Learning Generalizable Model-based Neural
Radiance Fields from Multi-view Images. In CVPR , 2023. 3
[10] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,
and Andreas Geiger. SNARF: Differentiable Forward Skin-
ning for Animating Non-Rigid Neural Implicit Shapes. In
ICCV , 2021. 3
[11] Xu Chen, Tianjian Jiang, Jie Song, Max Rietmann, Andreas
Geiger, Michael J. Black, and Otmar Hilliges. Fast-SNARF:
A Fast Deformer for Articulated Neural Fields. TPAMI ,
2022. 3
[12] Zhiqin Chen and Hao Zhang. Learning Implicit Fields for
Generative Shape Modeling. In CVPR , 2019. 2
[13] Zhiqin Chen, Thomas A. Funkhouser, Peter Hedman, and
Andrea Tagliasacchi. MobileNeRF: Exploiting the Polygon
Rasterization Pipeline for Efﬁcient Neural Field Rendering
on Mobile Architectures. In CVPR , 2023. 3
[14] Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng,
Zicheng Liu, and Xin Tong. MPS-NeRF: Generalizable 3D
Human Rendering from Multiview Images. TPAMI , 2022. 3
[15] Stephan J. Garbin, Marek Kowalski, Matthew Johnson,
Jamie Shotton, and Julien P. C. Valentin. FastNeRF: High-
Fidelity Neural Rendering at 200FPS. In ICCV , 2021. 3
[16] Chen Geng, Sida Peng, Zhenqi Xu, Hujun Bao, and Xiaowei
Zhou. Learning Neural V olumetric Representations of Dy-
namic Humans in Minutes. In CVPR , 2023. 1,3,4
[17] Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and
Michael F. Cohen. The lumigraph. In SIGGRAPH , 1996. 2[18] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.
Shape, Light & Material Decomposition from Images using
Monte Carlo Rendering and Denoising. In NeurIPS , 2022. 2
[19] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and
Tony Tung. Arch++: Animation-ready clothed human re-
construction revisited. In ICCV , 2021. 3
[20] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall,
Jonathan T. Barron, and Paul E. Debevec. Baking Neural
Radiance Fields for Real-Time View Synthesis. In ICCV ,
2021. 3
[21] Shou-Yong Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei
Yang, and Ziwei Liu. SHERF: Generalizable Human NeRF
from a Single Image. In ICCV , 2023. 3
[22] T. Hu, Tao Yu, Zerong Zheng, He Zhang, Yebin Liu, and
Matthias Zwicker. HVTR: Hybrid V olumetric-Textural Ren-
dering for Human Avatars. 3DV, 2021. 3
[23] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Let
there be color! ACM TOG , 2016. 2
[24] Timothy Jeruzalski, Boyang Deng, Mohammad Norouzi,
J. P. Lewis, Geo rey E. Hinton, and Andrea Tagliasacchi.
NASA: Neural Articulated Shape Approximation. In ECCV ,
2020. 3
[25] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang.
SelfRecon: Self Reconstruction Your Digital Avatar from
Monocular Video. In CVPR , 2022. 1,3
[26] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. In-
stantavatar: Learning avatars from monocular video in 60
seconds. In CVPR , 2023. 6,2
[27] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel,
and Anurag Ranjan. Neuman: Neural human radiance ﬁeld
from a single video. In ECCV , 2022. 1,6
[28] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei
Tan, Lin Gui, Sean Banerjee, Timothy Scott Godisart, Bart
Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and
Yaser Sheikh. Panoptic Studio: A Massively Multiview Sys-
tem for Social Interaction Capture. TPAMI , 2017. 1
[29] James T. Kajiya and Brian V on Herzen. Ray Tracing V olume
Densities. In SIGGRAPH , 1984. 2
[30] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,
and George Drettakis. 3D Gaussian Splatting for Real-Time
Radiance Field Rendering. ACM TOG , 2023. 2,3
[31] Jaehyeok Kim, Dongyoon Wee, and Dan Xu. You Only
Train Once: Multi-Identity Free-Viewpoint Neural Human
Rendering from Monocular Videos. arXiv , 2023. 3
[32] Diederik P. Kingma and Jimmy Ba. Adam: A Method for
Stochastic Optimization. arXiv , 2014. 2
[33] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,
and Michael J Black. PARE: Part attention regressor for 3D
human body estimation. In ICCV , 2021. 6,3
[34] Youngjoon Kwon, Dahun Kim, Duygu Ceylan, and Henry
Fuchs. Neural Human Performer: Learning Generalizable
Radiance Fields for Human Performance Rendering. In
NeurIPS , 2021. 3
[35] Young Chan Kwon, Dahun Kim, Duygu Ceylan, and Henry
Fuchs. Neural Image-based Avatars: Generalizable Radiance
Fields for Human Avatar Modeling. In ICLR , 2023. 3
2067
[36] Marc Levoy and Pat Hanrahan. Light Field Rendering. In
SIGGRAPH , 1996. 2
[37] Ruilong Li, Julian Tanke, Minh V o, Michael Zollhofer, Jur-
gen Gall, Angjoo Kanazawa, and Christoph Lassner. TA V A:
Template-free Animatable V olumetric Actors. In ECCV ,
2022. 3
[38] Zhi-Hao Lin, Wei-Chiu Ma, Hao-Yu Hsu, Yu-Chiang Frank
Wang, and Shenlong Wang. Neurmips: Neural Mixture of
Planar Experts for View Synthesis. In CVPR , 2022. 3
[39] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu
Sarkar, Jiatao Gu, and Christian Theobalt. Neural actor:
Neural free-view synthesis of human actors with pose con-
trol. ACM TOG , 2021. 3
[40] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft ras-
terizer: A differentiable renderer for image-based 3d reason-
ing. In ICCV , 2019. 4,8
[41] Stephen Lombardi, Tomas Simon, Jason M. Saragih, Gabriel
Schwartz, Andreas M. Lehrmann, and Yaser Sheikh. Neural
V olumes: Learning Dynamic Renderable V olumes from Im-
ages. ACM TOG , 2019. 2
[42] Matthew Loper, Naureen Mahmood, and Michael J Black.
Mosh: Motion and shape capture from sparse markers. ACM
TOG , 2014. 1
[43] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J. Black. SMPL: A skinned multi-
person linear model. ACM TOG , 2015. 2,5,1
[44] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris Mc-
Clanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-
Ling Chang, Ming Guang Yong, Juhyun Lee, et al. Medi-
apipe: A framework for building perception pipelines. arXiv ,
2019. 6
[45] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and
Deva Ramanan. Dynamic 3d gaussians: Tracking by per-
sistent dynamic view synthesis. In 3DV, 2024. 2
[46] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi,
Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duck-
worth. NeRF in the Wild: Neural Radiance Fields for Un-
constrained Photo Collections. In CVPR , 2021. 2
[47] Lars M. Mescheder, Michael Oechsle, Michael Niemeyer,
Sebastian Nowozin, and Andreas Geiger. Occupancy net-
works: Learning 3D reconstruction in function space. In
CVPR , 2019. 2
[48] Marko Mihajlovi ´c, Shunsuke Saito, Aayush Bansal, Michael
Zollhoefer, and Siyu Tang. COAP: Compositional Articu-
lated Occupancy of People. In CVPR , 2022. 3
[49] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing Scenes as Neural Radiance Fields for View
Synthesis. In ECCV , 2020. 2,4,5
[50] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding. ACM TOG , 2022. 2
[51] Jeong Joon Park, Peter Florence, Julian Straub, Richard A.
Newcombe, and Steven Lovegrove. DeepSDF: Learning
continuous signed distance functions for shape representa-
tion. In CVPR , 2019. 2[52] Sang Il Park and Jessica K Hodgins. Capturing and animat-
ing skin deformation in human motion. ACM TOG , 2006.
1
[53] Sida Peng, Junting Dong, Qianqian Wang, Shang-Wei
Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animat-
able Neural Radiance Fields for Modeling Dynamic Human
Bodies. In ICCV , 2021. 3
[54] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,
Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:
Implicit neural representations with structured latent codes
for novel view synthesis of dynamic humans. In CVPR ,
2021. 2,3,5,6
[55] Edoardo Remelli, Timur M. Bagautdinov, Shunsuke Saito,
Chenglei Wu, Tomas Simon, Shih-En Wei, Kaiwen Guo,
Zhe Cao, Fabi ´an Prada, Jason M. Saragih, and Yaser Sheikh.
Drivable V olumetric Avatars using Texel-Aligned Features.
InSIGGRAPH , 2022. 3
[56] Zhongzheng Ren, Xiaoming Zhao, and Alexander G.
Schwing. Class-agnostic Reconstruction of Dynamic Ob-
jects from Videos. In NeurIPS , 2021. 2,3
[57] Zhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexan-
der G. Schwing, and Oliver Wang. Neural volumetric object
selection. In CVPR , 2022. 2
[58] Ignacio Rocco, Iurii Makarov, Filippos Kokkinos, David
Novotn ´y, Benjamin Graham, Natalia Neverova, and Andrea
Vedaldi. Real-time V olumetric Rendering of Dynamic Hu-
mans. arXiv , 2023. 1,3
[59] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-aligned
implicit function for high-resolution clothed human digitiza-
tion. In ICCV , 2019. 2
[60] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul
Joo. PIFuHD: Multi-level pixel-aligned implicit function for
high-resolution 3d human digitization. In CVPR , 2020. 2
[61] Jonathan Shade, Steven J. Gortler, Li wei He, and Richard
Szeliski. Layered depth images. In SIGGRAPH , 1998. 2
[62] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin
Huang. 3D Photography Using Context-Aware Layered
Depth Inpainting. In CVPR , 2020. 2
[63] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias
Nießner, Gordon Wetzstein, and Michael Zollh ¨ofer. Deep-
V oxels: Learning Persistent 3D Feature Embeddings. In
CVPR , 2019. 2
[64] Shih-Yang Su, Frank Yu, Michael Zollhoefer, and Helge
Rhodin. A-NeRF: Articulated Neural Radiance Fields for
Learning Human Shape, Appearance, and Pose. In NeurIPS ,
2021. 1,3
[65] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shaﬁr,
Daniel Cohen-Or, and Amit H Bermano. Human motion dif-
fusion model. In ICLR , 2023. 7
[66] Garvita Tiwari, Nikolaos Saraﬁanos, Tony Tung, and Gerard
Pons-Moll. Neural-GIF: Neural Generalized Implicit Func-
tions for Animating People in Clothing. In ICCV , 2021. 3
[67] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd E. Zick-
ler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF:
Structured View-Dependent Appearance for Neural Radi-
ance Fields. In CVPR , 2022. 2
2068
[68] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
InNeurIPS , 2021. 6
[69] Shaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu
Tang. ARAH: Animatable V olume Rendering of Articulated
Human SDFs. In ECCV , 2022. 3,6
[70] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan,
Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-
manNeRF: Free-viewpoint Rendering of Moving People
from Monocular Video. In CVPR , 2022. 1,3,4,5,6,2
[71] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon
Yenphraphai, and Supasorn Suwajanakorn. NeX: Real-time
View Synthesis with Neural Basis Expansion. In CVPR ,
2021. 2
[72] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng
Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.
4d gaussian splatting for real-time dynamic scene rendering.
arXiv , 2023. 2
[73] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng
Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang.
4D Gaussian Splatting for Real-Time Dynamic Scene Ren-
dering. arXiv , 2023. 2
[74] Cheng-hsin Wuu, Ningyuan Zheng, Scott Ardisson, Rohan
Bali, Danielle Belko, Eric Brockmeyer, Lucas Evans, Timo-
thy Godisart, Hyowon Ha, Alexander Hypes, Taylor Koska,
Steven Krenn, Stephen Lombardi, Xiaomin Luo, Kevyn
McPhail, Laura Millerschoen, Michal Perdoch, Mark Pitts,
Alexander Richard, Jason Saragih, Junko Saragih, Takaaki
Shiratori, Tomas Simon, Matt Stewart, Autumn Trimble,
Xinshuo Weng, David Whitewolf, Chenglei Wu, Shoou-I Yu,
and Yaser Sheikh. Multiface: A Dataset for Neural Face Ren-
dering, 2022. 1
[75] Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu. H-
NeRF: Neural Radiance Fields for Rendering and Temporal
Reconstruction of Humans in Motion. In NeurIPS , 2021. 3
[76] Yuanlu Xu, Bingpeng Ma, and Rui Huang Liang Lin. Person
search in a scene by jointly modeling people commonness
and person uniqueness. In ACM MM , 2014. 3
[77] Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng
Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, and
Raquel Urtasun. S3: Neural shape, skeleton, and skinning
ﬁelds for 3d human modeling. In CVPR , 2021. 3
[78] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,
Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron,
and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-
time view synthesis. In SIGGRAPH , 2023. 3
[79] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and
Angjoo Kanazawa. PlenOctrees for Real-time Rendering of
Neural Radiance Fields. In ICCV , 2021. 3
[80] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance Fields without Neural Networks. In CVPR , 2022.
2
[81] Zhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, and
Kwan-Yee Lin. Monohuman: Animatable human neural
ﬁeld from monocular video. In CVPR , 2023. 1,3,4,5,
6[82] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Koltun. NeRF++: Analyzing and Improving Neural Radi-
ance Fields. arXiv , 2020. 2
[83] Rui Zhang and Jie Chen. NDF: Neural Deformable Fields
for Dynamic Human Modelling. In ECCV , 2022. 3
[84] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 5
[85] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei-Ying Lin,
Yingliang Zhang, Jingyi Yu, and Lan Xu. HumanNeRF: Ef-
ﬁciently Generated Human Radiance Field from Sparse In-
puts. In CVPR , 2022. 3
[86] Xiaoming Zhao, Yuan-Ting Hu, Zhongzheng Ren, and
Alexander G. Schwing. Occupancy Planes for Single-view
RGB-D Human Reconstruction. In AAAI , 2023. 2
[87] Xiaoming Zhao, Alex Colburn, Fangchang Ma, Miguel
´Angel Bautista, Joshua M. Susskind, and Alexander G.
Schwing. Pseudo-Generalized Dynamic View Synthesis
from a Video. In ICLR , 2024. 3
[88] Yufeng Zheng, Yifan Wang, Gordon Wetzstein, Michael J.
Black, and Otmar Hilliges. PointAvatar: Deformable Point-
based Head Avatars from Videos. In CVPR , 2023. 1
[89] Zerong Zheng, Han Huang, Tao Yu, Hongwen Zhang, Yan-
dong Guo, and Yebin Liu. Structured Local Radiance Fields
for Human Avatar Modeling. In CVPR , 2022. 3
[90] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,
and Noah Snavely. Stereo Magniﬁcation: Learning View
Synthesis using Multiplane Images. ACM TOG , 2018. 2
2069
