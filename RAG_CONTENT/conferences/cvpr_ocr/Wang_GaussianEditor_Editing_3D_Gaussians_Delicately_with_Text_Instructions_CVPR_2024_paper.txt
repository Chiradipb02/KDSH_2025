GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions
Junjie Wang*, Jiemin Fang*†, Xiaopeng Zhang, Lingxi Xie, Qi Tian
Huawei Inc.
{is.wangjunjie, jaminfong, zxphistory, 198808xc }@gmail.com tian.qi1@huawei.com
First Edit Second Edit
Gaussian 
EditorGaussian 
Editor3D GaussiansEdited 3D Gaussians
Give him a red nose Make his right face looks like Orc
Make his left face looks like the Tolkien Elf
Make him completely baldMake his right face looks like vampire
Make his right face looks like Lord Voldemort
Figure 1. We propose GaussianEditor, an interactive framework to achieve delicate 3D scene editing following text instructions. As shown
in this figure, our method can precisely control the editing region and achieve multi-round editing.
Abstract
Recently, impressive results have been achieved in 3D
scene editing with text instructions based on a 2D diffusion
model. However, current diffusion models primarily gen-
erate images by predicting noise in the latent space, and
the editing is usually applied to the whole image, which
makes it challenging to perform delicate, especially local-
ized, editing for 3D scenes. Inspired by recent 3D Gaussian
splatting, we propose a systematic framework, named Gaus-
sianEditor, to edit 3D scenes delicately via 3D Gaussians
with text instructions. Benefiting from the explicit prop-
erty of 3D Gaussians, we design a series of techniques to
achieve delicate editing. Specifically, we first extract the
region of interest (RoI) corresponding to the text instruc-
tion, aligning it to 3D Gaussians. The Gaussian RoI is fur-
*Equal contributions.
†Corresponding author.ther used to control the editing process. Our framework
can achieve more delicate and precise editing of 3D scenes
than previous methods while enjoying much faster training
speed, i.e. within 20 minutes on a single V100 GPU, more
than twice as fast as Instruct-NeRF2NeRF (45 minutes –
2 hours)1. The project page is at GaussianEditor.
github.io .
1. Introduction
Creating 3D assets has played a critical role in many appli-
cations and industries, e.g. movie/game production, artis-
tic creation, AR, VR etc. However, this process is usu-
ally expensive and cumbersome, especially for traditional
pipelines. Designers need to take a lot of labor and time to
1The editing time varies in different scenes according to the scene struc-
ture complexity.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20902
finish each step, e.g. sketching, building structures, creat-
ing textures etc. One cheap and effective way of creating
high-quality 3D assets is to start from an existing scene,
capturing, modeling, and editing the scene and obtaining
the wanted one. This approach can be also used for user-
interactive entertainment applications.
Neural radiance field methods [2, 3, 6, 29, 31, 42, 45]
have shown great power in representing 3D scenes and syn-
thesizing novel-view images. Past years have witnessed the
rapid development of NeRF and its variants, from both qual-
ity and efficiency perspectives. Editing a pre-trained NeRF
model becomes a promising way to edit 3D scenes. Rep-
resented by Instruct-NeRF2NeRF [11], researchers propose
to use the image-conditioned 2D diffusion model, e.g. In-
structPix2Pix [4], to edit 3D scenes simply with text instruc-
tions. Notable results have been achieved as real scenes can
be changed following the text instruction. However, current
2D diffusion models face challenges in accurately localiz-
ing editing regions, which hinders the generation of finely
edited scenes due to the change of unintended regions. Even
though some works [30] propose to constrain the editing re-
gion on edited 2D images, the editing region is not accu-
rately localized and hard to apply to the 3D representation.
Besides, NeRF-based methods [9, 48] bear coupling effects
between different spatial positions, e.g. different points are
queried from the same MLP field (for implicit representa-
tions) or voxel vertices (for explicit representations).
Recent 3D Gaussian Splatting [18] (3D-GS) has been
a groundbreaking work in the radiance field, which is the
first to achieve a real sense of real-time rendering while en-
joying high rendering quality and training speed. Besides
its efficiency, we further notice its natural explicit property.
3D-GS has a great advantage for editing tasks as each 3D
Gaussian exists individually. Editing 3D scenes by directly
manipulating 3D Gaussians with desired constraints is easy.
Aiming at editing 3D scenes delicately, we propose to
represent the scene with 3D Gaussians, which can be edited
with text instructions, and name our method as GaussianEd-
itor. GaussianEditor is divided into three main parts to
achieve precise control for editing regions. The first is the
region of interest (RoI) extraction from the given text in-
struction. The instruction may be complex or indirect while
this module helps extract the keywords matching the RoI
for editing. The second part aligns the extracted text RoI
to the 3D Gaussian space through the image space, where a
grounding segmentation module is applied. The last part is
to edit the original 3D Gaussians delicately with constraints
in the obtained 3D Gaussian RoI. With the above processes,
the region for editing can be precisely localized simply from
text instructions, which constrains the 3D Gaussian updat-
ing to obtain a delicately edited new 3D scene. Besides, we
enable interfaces for users to introduce more exact instruc-
tions for more delicate editing, e.g. Gaussian point selectingand 3D boxes for modifying the editing regions2.
Our contributions can be summarized as follows.
• As far as we know, our GaussianEditor is one of the first
systematic methods to achieve delicate 3D scene editing
based on 3D Gaussian splatting.
• A series of techniques are designed and proposed to pre-
cisely localize the editing region of interest, which are
aligned and applied to 3D Gaussians. Though some sub-
modules are from existing works, we believe integrating
these awesome techniques to work effectively is a valu-
able topic, which is what we focus on in this paper.
• Our method achieves a series of more delicate editing
results compared with the previous representative work
Instruct-NeRF2NeRF [11] with much shorter training
time (within 20 minutes v.s.45 minutes – 2 hours).
2. Related Work
2D Image Editing with Diffusion Models. By advance-
ments in diffusion model technology [13, 43], numerous
generative models [40] have attained remarkably impressive
outcomes in image synthesis. Recent progressions in diffu-
sion models have showcased their adeptness in fabricating
lifelike images from arbitrary textual inputs [8, 14, 39, 41,
44]. Harnessing the robust semantic comprehension and im-
age generation capabilities of foundational diffusion mod-
els, an escalating number of research explorations are cur-
rently employing diffusion models as a fundamental frame-
work for implementing text-based image editing function-
alities [33, 36, 37, 40]. Some of these methodologies ne-
cessitate the manual provision of captions for both the orig-
inal and edited images [12], while others mandate specific
scenario-based training for optimization [38]. These req-
uisites have rendered it arduous for ordinary users to avail
themselves of such techniques. Expanding upon this foun-
dation, iP2P [4] introduces instruction-based capabilities to
image editing, enabling users to simply input an image and
apprise the model of the desired alterations. This user-
friendly approach facilitates the democratization of image
editing in a more accessible manner.
3D Scene Editing of Radiance Fields. 3D Scene Edit-
ing of Radiance Fields has gained significant popularity as
a recent research direction [1, 10, 15, 20, 22–25, 28, 34,
46, 47, 51–53]. The main objective of such methods is to
enable the manipulation of both the geometry and appear-
ance of 3D scene representations. However, editing such
scenes poses inherent challenges due to the implicit nature
of traditional NeRF representations, which lack precise lo-
calization capabilities. As a result, previous works have pri-
marily focused on achieving global style transformations of
3D scenes [7, 16, 17, 32, 48, 50, 54]. While some efforts
2These additional instructions are applied to generate the man with two
different edited half faces in Fig. 1.
20903
“A young man with short hair and a 
tan sweater is looking at the camera.”Input Scene
RoI Extraction of 
Text Instruction
“[hair,]” Text RoI“Give him blue hair ”Text InstructionEdited Scene
Delicate Editing within Gaussian RoI
Edited ImageRendered Image
 Noise
LLM Assistant
Additional
Instructions
漑Optional 漒
Select Points
3D Box3D Gaussian RoI Alignment
Image RoI Gaussian RoI
Grounding SegmentationRoI LiftingScene Description Generation
Text description: 
A young man with short hair and a 
tan sweater is looking at the camera.
Edit Instruction: Give him blue hair.Answer:
User MessageRender
conditionText To Image DiffusionFigure 2. Our framework, named GaussianEditor, consists of three key steps. First, a module MDesc is used to get the description of the
input scene, which is put to an LLM assistant MLLM with the text instruction Tprovided by the user to obtain the text RoI TRoI. Second,
a grounding segmentation module MSegis used to convert TRoIto image RoI IRoI, which is then lifted to 3D Gaussians RoI GRoIby RoI
lifting MLift, where additional user instructions Ocan be incorporated. Third, following the user instruction T, rendered image Irender
from randomly chosen views is edited by a diffusion model MDM. The loss between Irender and edited one Ieditis calculated. Finally,
gradient backpropagation and optimization are performed within the Gaussian RoI GRoIto get the edited scene Gedit.
have been made towards object-centric scene editing[55],
keeping the background unchanged has been a persistent
challenge. For example, a recently proposed work, Instruct-
NeRF2NeRF [11], implements text instruction-controlled
3D scene editing capabilities, achieving excellent editing
effects while maintaining user-friendliness. However, it re-
lies on the editing effect of 2D images, which may cause
global changes to the 3D scene. A subsequent work [30]
attempts to compute the relevance map between edited and
unedited images to localize the editing area. The relevance
map may be unreliable when the 2D IP2P [4] model fails.
Other efforts [23] attempt to rely on the 3D coordinates en-
tered by the user to decide the editing area. The introduction
of 3D Gaussians [18] has provided an opportunity to ad-
dress this limitation. Its explicit 3D representation method-
ology enables accurate selection and manipulation of edit-
ing areas. By incorporating LLMs, the whole process can
be more automated.3. Method
In this section, we first review 3D representation methods
in Sec. 3.1. Subsequently, in Sec. 3.2, we overview our
proposed approach, which mainly includes three modules.
Sec. 3.3 delves into the precise Region of Interest (RoI) ex-
traction of text instructions, using scene description gener-
ation module MDesc and LLM assistant MLLM . Sec. 3.4
introduces how to align the instruction RoI with 3D Gaus-
sians, using grounding segmentation module MSegand RoI
lifting module MLift. Finally, Sec. 3.5 describes the deli-
cate editing process within the obtained Gaussian RoI, using
text to image diffusion model MDM.
3.1. Preliminaries
3D Gaussian Splatting. 3D Gaussian splatting [18] is a
recent powerful 3D representation method. It represents the
3D scene with point-like 3D Gaussians G={g1, g2...gN},
20904
where gi={µ,Σ, c, α}andi∈ {1, . . . , N }. Among them,
µ∈R3is the position where the Gaussian centers, Σ∈R7
denotes the 3D covariance matrix, c∈R3is the RGB color
andα∈R1is the opacity. Benefitting from the compact
representation of Gaussians and efficient differentiable ren-
dering approach, 3D Gaussian splatting achieves real-time
rendering with high quality. The splatting rendering process
can be formulated as
C=X
i∈Nciσii−1Y
j=1(1−σj), (1)
where σi=αie−1
2(xi)TΣ−1(xi)represents the influence of
the Gaussian to the image pixel and xiis the distance be-
tween the 3D point and the center of the i-th Gaussian.
3.2. Overall Framework
Given a group of 3D Gaussians Ginput for an input scene
and a text instruction Tfor editing, our Gaussian editor E
can edit the 3D Gaussians delicately into a new one, denoted
asGedit, with the guidance of the instruction. The whole
process can be formulated as
Gedit=E(Ginput,T). (2)
Fig. 2 illustrates the overall framework of our approach,
which consists of three main steps. First, the Region of In-
terest (RoI) is extracted from the text instruction. In this
step, we employ a module named scene description genera-
tionMDexcription to get the description of the input scene.
We then input the scene description Tscene and text instruc-
tionTinto a large language model assistant MLLM to de-
termine where we should make edits in the scene. The out-
put of this step is referred to as the instruction RoI TRoI.
The next step is the 3D Gaussian RoI alignment. We use
a grounding segmentation module MSegto convert the RoI
from text space, i.e.TRoI, to the image space, i.e.IRoI.
Then the image RoI IRoIis lifted to the RoI of 3D Gaus-
siansGRoIthrough RoI lifting module MLift. The Gaus-
sian RoI allows us to control the regions where edits will be
applied precisely.
The last step is delicate editing within the Gaussian RoI.
In this step, we randomly sample the view to obtain the
rendered image Irender . A 2D diffusion model MDM is
used to perform the editing process on the rendered im-
ageIrender , with the user instruction Tand the image
Iinput of input scene as conditions. The resulting edited
image is denoted as Iedit. Subsequently, we calculate the
loss between IeditandIrender and make gradient back-
propagation within GRoI. This implies that only the re-
gions specified by the RoI can receive corresponding gra-
dients during the back-propagation process. Finally, opti-
mization is executed based on these gradients. The final op-
timized scene representation Geditis obtained through sev-
eral rounds of iterative optimization.
“a black park bench”
“ a bicycle is parked on 
the side of the road ”
“ a bicycle is parked on 
a bench in a park ”
"A white bike is leaning against a black bench in a park."ȜȜ
LLM AssistantMultimodal ModelOriginal SceneFigure 3. The process of obtaining scene description.
3.3. RoI Extraction of Text Instruction
The instruction RoI is extracted for the editing regions from
both the input 3D scene Ginput and the text instruction T
provided by the user. To achieve this, we employ a mul-
timodal model MMM in conjunction with the large lan-
guage model assistant MLLM . The first step is the scene
description generation MDesc, which aims to get the scene
description Tscene from 3D Gaussians Ginput :
Tscene =MDesc(Ginput). (3)
The process of the scene description generation MDesc is
shown in Fig. 3. By leveraging the technique of differen-
tiable splatting as shown in Eq. 1, a set of 2D image sam-
ples{Isample}are generated and then inputted into a mul-
timodal model MMM to generate corresponding text de-
scriptions {Tsample}:
Tsample =MMM(PMM,Isample ), (4)
where PMM is a prompt, such as “ What is the
content of the image ”, for multimodal model
MMM to get precise description. Subsequently, these de-
scriptions {Tsample}are fed into a large language model
MLLM , which is specifically instructed by a prompt
Pmerge to merge descriptions of diverse views into one de-
tailed scene description Tscene :
Tscene =MLLM(Pmerge ,{Tsample}). (5)
After that, the scene description Tscene and the user
instruction Tare combined with a predefined tem-
20905
plateTtemplate : “Text description: Tscene Edit
Instruction: TAnswer: ” to form the user message
Tuser=Ttemplate (Tscene,T). The LLM model MLLM is
used to extract the instruction RoI TRoIfrom user message
Tuser with a new prompt Pextract :
TRoI=MLLM(Pextract ,Tuser). (6)
3.4. 3D Gaussian RoI Alignment
To confine the 3D editing region within the instruction RoI,
3D Gaussian RoI GRoIis aligned with the text RoI TRoI.
First, The RoI in the text space is transformed into the image
space via a grounding segmentation module MSeg:
IRoI=MSeg(Iinput,TRoI), (7)
where Iinput is rendered image of the input scene Ginput .
Then we lift the the RoI IRoIin the image space to 3D
Gaussian GRoIthrough training. To achieve this, an ad-
ditional RoI attribute r∈R1was added to 3D Gaussian
gi={µi,Σi, ci, αi, ri}.ris initialized to 0, which means
it is not in the Gaussians RoI, and 1 means it is inside the
RoI. The set of ris denoted as R ∈RN,1, where the Nis
the number of 3D Gaussians Ginput .
Then the color ciin Eq. 1 was rewritten with rito get the
rendered RoI Irender
RoI :
Irender
RoI =X
i∈Nriσii−1Y
j=1(1−σj). (8)
Taking inspiration from SA3D [5], to get the trained
Gaussians RoI Gtrain
RoI , we adopt a similar loss function to
supervise the training process:
Lproj=λ1X
(Irender
RoI·IRoI)+λ2X
((1−Irender
RoI )·IRoI),
(9)
where λ1andλ2are hyperparameters. The rin Eq. 8 is
updated via r←r−η∂Lproj
∂rwith gradient descent, where
ηdenotes the learning rate. Eq. 9 encourages rendered RoI
to cover the Image RoI and not exceed it. Additionally, the
user can modify the trained Gaussian RoI Gtrain
RoI by giving
added Gaussian RoI Gadd
RoI, deleted Gaussian RoI Gdel
RoIand
3D box B3D:
GRoI= (Gtrain
RoI∪ Gadd
RoI− Gdel
RoI)∩ B 3D, (10)
Gadd
RoIrepresents the 3D Gaussians user wants to edit, Gdel
RoI
means 3D Gaussians user wants to keep from editing, B3D
is the coordinates of 3D cuboid it limits RoI to inside the
box.GRoIis the aligned RoI with the text RoI. For exam-
ple, when editing the left face of the man in Fig. 1, ground-
ing segmentation failed to ground “left face”, instead, it
grounded the whole face. In this scenario, the user can use
the interactive interface to set the right face as Gdel
RoIor enterthe rectangular box where the left face is located as B3D.
The lifting process MLift can be represented as:
GRoI=MLift(IRoI,O), (11)
where O={Gadd
RoI,Gdel
RoI,B3D}is optional instructions.
3.5. Delicate Editing within Gaussian RoI
To achieve delicate editing in 3D scenes, we use the Gaus-
sian RoI to constrain the editing area. In particular, we ran-
domly sample viewpoints from the 3D scene and render 2D
image Irender . After that, Irender and noise level tare put
into 2D diffusion model MDM, with the user instruction T
and image Iinput of input scene as conditions, to get edited
image Iedit:
Iedit=D(Irender , t;T,Iinput), (12)
where tis a randomly chosen noise level from [tmin, tmax].
Similar to 3D-GS [18], we apply the L 1and D-SSIM
loss functions during editing.
L= (1−β)L 1+βLD−SSIM . (13)
the two losses are calculated between the 2D edited image
Iedand the rendered image Ird. Then, gradient backpropa-
gation is performed within Gaussian RoI GRoI:
∇G=∂L
∂G· R, (14)
whereRis the set of RoI attributes. That means only Gaus-
sians in RoI can receive gradients. Finally, we utilize the
Adam algorithm to optimize the 3D Gaussians. After many
rounds of training, the edited 3D scene Geditis obtained.
4. Experiments
4.1. Implementation Details
Our method is implemented in PyTorch [35] and CUDA,
based on 3D Gaussian splatting. The multimodal model
we used in our method is BLIP2 [21], and we use GPT-
3.5 Turbo to ground the text ROI. For grounding segmenta-
tion, We use the cascade strategy, i.e. first using Grounding
DINO [26] to get the box on the image corresponding to
the text, and then using SAM [19] to get the corresponding
image RoI. The 2D diffusion model used in our method is
Instruct Pix2Pix [4]. We leave more details in the Appendix.
4.2. Qualitative Evaluation
Visualization Results. In Fig. 1 and Fig. 4, we present
the visual results of GaussianEditor, demonstrating the pre-
cise editing effects while ensuring 3D consistency. Fig. 1
shows the editing capabilities for characters. The first
column displays the original scenes. In the second col-
umn, the first row “ Give him a red nose ” illustrates
20906
Original “Make the road look like the meadow”
“Turn the bench into Turquoise”“Turn the road into river”
“Turn the bench red” “Turn the bench into wood”
Original “Turn the bear into a polar bear” “Turn the bear into a panda” “Turn the bear into a grizzly bear”
First edit Second editFigure 4. Qualitative results on outdoor scenes. Our method supports separate foreground and background editing in real-world scenes.
color-changing ability, while the third row, “ Make him
completely bald ”, showcases capabilities of retextur-
ing and slight geometry editing. The second row in the sec-
ond column demonstrates precise editing ability by exclu-
sively editing the left side of the face. Based on that, we
achieve editing in the third column, focusing on the right
side of the face, showcasing the ability of multi-round edits,
and accurately fulfilling user instructions. Fig. 4 showcases
the precise editing capabilities in open 3D scenes. In the up-
per portion, the bicycle scene allows us to accurately locate
the position of the road and edit its texture, transforming it
into the grass, a river. In the experiment where we change
the texture to a river, our method accurately constructs the
reflection, making it appear realistic. Based on editing the
road into a river, we further edited the bench, proving that
our method can achieve multiple rounds of editing. The
lower portion demonstrates the results of editing the bear,
which fully preserves the original appearance of the back-
ground area and focuses the edits on the bear.
Comparisons with Instruct-NeRF2NeRF. Fig. 5 com-
pares the results of our method with those of IN2N [11],
on the scenes presented in IN2N. From the figure, it is
evident that our method changes the texture of the pants
without affecting the clothes, and vice versa, demonstratingthe effectiveness of our method in distinguishing different
objects within the foreground. Additionally, when editing
the clothes and pants, the background remains unaffected,
indicating our method’s effective separation of foreground
and background. Furthermore, the last column reveals that
IN2N, limited by 2D diffusion, distorts the face, while our
method maintains a superior rendering quality of faces.
Complex Multi-Object Scenes. Furthermore, we present
the results of our editing in a complex scene featuring mul-
tiple objects, as depicted in Fig 6. We specifically selected
three distinct object types for editing purposes. The first
object type is the background. In a particular sub-scene, the
desktop shares similarities with the background. Employing
a caption-based approach, we successfully transformed the
desktop into a wooden material. As evident in the image,
the edited result exhibits a distinct and pronounced wood
texture. The second object type is a foreground object, the
flowerpot. We opted to change the color of the flowerpot
to red, and the outcome was highly successful. Lastly, the
most intricate editing task involved the rolling pin, which
was occluded by multiple objects from various perspectives.
As illustrated in the lower right corner of the picture, we
managed to edit it into a cucumber without any adverse im-
pact on the other objects.
20907
Original “Make his t-shirt out of leather” “Turn his t-shirt into white” “Turn his pants into a bronze statue” “Turn his pants into yellow”
IN2N
Ours
Figure 5. Comparisons with Instruct-NeRF2NeRF (IN2N) [11] on the scene presented in their paper.
Original “Turn the desk into wood” “Turn the rolling pin into cucumber” “Turn the flowerpot red”
Figure 6. Qualitative results on complex multi-object scenes. The background “ desk ”, the foreground “ flower pot ”, and the multi-
view blocked foreground “ rolling pin ” are edited separately.
Table 1. Quantitative evaluation on the bicycle scene of the Mip-
NeRF360 dataset [3].
Methods CTIDS ↑IIS↑FID↓Time↓
IN2N [11] 0.22 0.85 103 51 min
Ours-DVGO [45] 0.11 0.82 148 40 min
Ours-3DGS 0.28 0.95 51 20min
4.3. Quantitative Evaluation
Metric Comparisons. Table 1 shows quantitative results
on the bicycle scene of the Mip-NeRF360 dataset [3], com-
paring with IN2N [11] and Direct V oxel Grid Optimization
(DVGO) [45] as the representation. The metrics include
CLIP Text-Image Direction Similarity (CTIDS), Image-
Image Similarity (IIS), FID, and training time. GaussianEd-
itor achieves the best results in all metrics. The test data is
shown in the supplementary material.
User Study. We perform a user study comparing with
IN2N on the bear scene in Fig. 4 and the human scene in
Fig. 5, involving 21 participants. GaussianEditor gets a87.07% voting percentage, while IN2N gets 12.93%.
4.4. Ablation Study and Analysis
Ablation of Gaussian RoI, Text RoI, RoI Lifting. To
validate the effectiveness of each module in our framework,
we design three variant approaches: (1) w/o Gaussian RoI:
We discontinued the use of Gaussian RoI GRoIto control
the gradients of Gaussian points, as mentioned in Eq. 14.
(2) w/o Text ROI: In this scenario, we ceased the selection
of text ROI TRoIusing LLM assistant MLLM . Instead, all
the words in the user’s instruction are put to MSegto get
IRoI. (3) w/o RoI lifting: Instead of lifting the image RoI
IRoIto 3D Gaussians, the image RoI IRoIis used to gov-
ern the calculation of the loss. That is, only the pixels within
the image RoI IRoIare taken into account for the loss com-
putation. Fig. 7 showcases the outcomes of our ablation
experiment, which aimed to edit the doll based on the in-
struction “ Turn its mouth into red. ” The results
reveal the following findings. (1) When the Gaussian RoI
is not used, the 3D scene is all turned red because the 2D
diffusion fails to control the editing area. (2) In cases where
20908
Original w/o Gaussian RoI w/o Text RoI w/o RoI Lifting Ours
Instruction: “Turn its mouth into red”Figure 7. Ablation experiment of RoI.
text ROI TRoIis not utilized, the grounding segmentation
model tends to segment the entire foreground object, lead-
ing to the doll being entirely edited to red. (3) When RoI
lifting MLift is not employed, the doll’s mouth is success-
fully turned red, but other facial areas are also affected. Be-
cause the grounding segmentation model may fail to parse
specific views, noise exists on the image RoI IRoI. Con-
sequently, leakage occurs during the editing process. Our
proposed RoI lifting module effectively addresses this is-
sue during training. In conclusion, our ablation experiment
demonstrates the effectiveness of several RoI-related mod-
ules in our method.
Ablation of Scene Description Generation. We further
conduct experiments to evaluate the role of scene de-
scription generation, employing three distinct experimen-
tal setups. The first one composes the user message
Tuser, without employing scene description. The second
method randomly samples a view and extracts the corre-
sponding image’s text description as the scene description.
The third one represents the complete version of our ap-
proach. The test scene involves a park where a bike and
a bench are positioned closely together. The editing in-
struction is “ Turn the thing next to the bike
orange ”. The obtained results are presented in Fig. 8.
As shown in the image, when scene description is not em-
ployed, the LLM fails to acquire the Text ROI according to
the user instructions, resulting in editing failure. The second
one randomly samples images to obtain scene descriptions,
resulting in incomplete descriptions and leading to an in-
correct text ROI prediction by the LLM. Consequently, the
final editing result turns the road into an orange color. In
contrast, our method flawlessly executes the editing task.
This success can be attributed to scene description genera-
tion, which obtains an accurate text description encompass-
ing the relative positional relationship between the bicycle
and the bench. This enables the LLM to analyze and deter-
mine the user’s intention to edit the bench. Consequently,
the desired color change of the bench is successfully imple-
mented.
4.5. Limitations
Although our framework has solved some problems inher-
ited from the integrated sub-modules, e.g. noise in the re-
sults of grounding segmentation, there are still some prob-
“[bench]”
Text description: “” Edits: Turn the 
thing next to the bike orange. Answer:
Text description: “A bicycle is parked 
on the side of the road.” Edits: Turn 
the thing next to the bike orange. 
Answer:Text description: “A white bike is 
leaning against a black bench in a 
park.” Edits: Turn the thing next to the 
bike orange. Answer:
“[side of the road]”“To provide a more accurate answer, I'll 
need a text description of the scene you 
want to edit. Could you please provide a 
description of the scene, including the 
object that is next to the bike?”
“Turn the thing next to the bike orange.”Text InstructionOriginal w/o description
Single image description Scene description
LLM LLMLLMFigure 8. Ablation results about the scene description generation.
lems that the current system cannot completely avoid. In
scene description generation, the descriptions from different
views of the same object may differ from each other. When
the differences are large enough, the LLM may misunder-
stand these descriptions as those from multiple objects. This
issue does not affect the results in the current experiment,
but we would like to optimize this in the future. In addition,
our system cannot achieve good editing results in scenes
where the grounding segmentation or diffusion model com-
pletely fails, such as drastic geometric editing.
5. Conclusion
This paper proposes a systematic framework, named Gaus-
sianEditor, for text-guided delicate 3D scene editing. As
we know, GaussianEditor is one of the first works to edit
3D Gaussians, taking advantage of the explicit property of
3D Gaussians and making it easy to control the editing area
precisely. Several techniques are proposed to achieve deli-
cate editing, including extracting instruction RoI from texts,
aligning the RoI to 3D Gaussians, and editing the scene
with the Gaussian RoI. GaussianEditor achieves notably
more delicate editing results than IN2N [11] with much
shorter training time (within 20 minutes v.s. 45 minutes
– 2 hours). Noticing recent works [27, 49] have extended
Gaussian splatting to dynamic scenes, we leave the delicate
editing in dynamic scenes as future work.
20909
References
[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan,
Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng
Cui. Sine: Semantic-driven image-based nerf editing with
prior-guided editing field. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
2023. 2
[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neu-
ral radiance fields. In ICCV , 2021. 2
[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance fields. In CVPR , 2022. 2, 7
[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InCVPR , 2023. 2, 3, 5
[5] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei
Shen, Lingxi Xie, Dongsheng Jiang, Xiaopeng Zhang, and
Qi Tian. Segment anything in 3d with nerfs. In NeurIPS ,
2023. 5
[6] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance fields. In ECCV , 2022.
2
[7] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-
Sheng Lai, and Wei-Chen Chiu. Stylizing 3d scene via im-
plicit representation and hypernetwork. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision , 2022. 2
[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 2021. 2
[9] Shuangkang Fang, Yufeng Wang, Yi Yang, Yi Yang, Yi-
Hsuan Tsai, Wenrui Ding, Ming-Hsuan Yang, and Shuchang
Zhou. Text-driven editing of 3d scenes without retraining.
Arxiv preprint arXiv:2309.04917 , 2023. 2
[10] William Gao, Noam Aigerman, Thibault Groueix, V ova
Kim, and Rana Hanocka. Textdeformer: Geometry manipu-
lation using text guidance. In ACM SIGGRAPH 2023 Con-
ference Proceedings , 2023. 2
[11] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander
Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Edit-
ing 3d scenes with instructions. In CVPR , 2023. 2, 3, 6, 7,
8
[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022. 2
[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. NeurIPS , 2020. 2
[14] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high fidelity image generation. The Journal of
Machine Learning Research , 2022. 2
[15] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang
Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-driven generation and animation of 3d avatars. arXiv preprint
arXiv:2205.08535 , 2022. 2
[16] Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh
Singh, and Ming-Hsuan Yang. Learning to stylize novel
views. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , 2021. 2
[17] Yi-Hua Huang, Yue He, Yu-Jie Yuan, Yu-Kun Lai, and Lin
Gao. Stylizednerf: consistent 3d scene stylization as stylized
nerf via 2d-3d mutual learning. In CVPR , 2022. 2
[18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,
and George Drettakis. 3d gaussian splatting for real-time
radiance field rendering. ACM Transactions on Graphics
(ToG) , 2023. 2, 3, 5
[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 5
[20] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz-
mann. Decomposing nerf for editing via feature field distil-
lation. Advances in Neural Information Processing Systems ,
2022. 2
[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 5
[22] Yuan Li, Zhi-Hao Lin, David Forsyth, Jia-Bin Huang, and
Shenlong Wang. Climatenerf: Physically-based neural ren-
dering for extreme climate synthesis. arXiv e-prints , 2022.
2
[23] Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi
Zhang, Peng Zhou, and Bingbing Ni. Focaldreamer: Text-
driven 3d editing via focal-fusion assembly. arXiv preprint
arXiv:2308.10608 , 2023. 3
[24] Hao-Kang Liu, I Shen, Bing-Yu Chen, et al. Nerf-in:
Free-form nerf inpainting with rgb-d priors. arXiv preprint
arXiv:2206.04901 , 2022.
[25] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard
Zhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional
radiance fields. In Proceedings of the IEEE/CVF interna-
tional conference on computer vision , 2021. 2
[26] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun
Zhu, et al. Grounding dino: Marrying dino with grounded
pre-training for open-set object detection. arXiv preprint
arXiv:2303.05499 , 2023. 5
[27] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and
Deva Ramanan. Dynamic 3d gaussians: Tracking
by persistent dynamic view synthesis. arXiv preprint
arXiv:2308.09713 , 2023. 8
[28] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and
Rana Hanocka. Text2mesh: Text-driven neural stylization
for meshes. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2022. 2
[29] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 2021. 2
20910
[30] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A.
Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G.
Derpanis, and Igor Gilitschenski. Watch your steps: Lo-
cal image and scene editing by text instructions. In arXiv
preprint arXiv:2308.08947 , 2023. 2, 3
[31] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 2022. 2
[32] Thu Nguyen-Phuoc, Feng Liu, and Lei Xiao. Snerf: stylized
neural implicit representations for 3d scenes. arXiv preprint
arXiv:2207.02363 , 2022. 2
[33] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 2
[34] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya
Harada. Neural articulated radiance field. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, 2021. 2
[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zem-
ing Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:
An imperative style, high-performance deep learning library.
NeurIPS , 2019. 5
[36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 2
[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , 2022. 2
[38] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In CVPR , 2023. 2
[39] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In ACM
SIGGRAPH 2022 Conference Proceedings , 2022. 2
[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 2022. 2
[41] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J Fleet, and Mohammad Norouzi. Image super-
resolution via iterative refinement. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 2022. 2
[42] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. In CVPR , 2022. 2
[43] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning usingnonequilibrium thermodynamics. In International confer-
ence on machine learning , 2015. 2
[44] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. Advances in neural
information processing systems , 2019. 2
[45] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance fields
reconstruction. In CVPR , 2022. 2, 7
[46] Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea
Vedaldi. Neural feature fusion fields: 3d distillation of self-
supervised 2d image representations. In 2022 International
Conference on 3D Vision (3DV) , 2022. 2
[47] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,
and Jing Liao. Clip-nerf: Text-and-image driven manip-
ulation of neural radiance fields. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2022. 2
[48] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He,
Dongdong Chen, and Jing Liao. Nerf-art: Text-driven neural
radiance fields stylization. TVCG , 2023. 2
[49] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng
Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang.
4d gaussian splatting for real-time dynamic scene rendering.
arXiv preprint arXiv:2310.08528 , 2023. 8
[50] Qiling Wu, Jianchao Tan, and Kun Xu. Palettenerf:
Palette-based color editing for nerfs. arXiv preprint
arXiv:2212.12871 , 2022. 2
[51] Jiale Xu, Xintao Wang, Yan-Pei Cao, Weihao Cheng, Ying
Shan, and Shenghua Gao. Instructp2p: Learning to edit
3d point clouds with text instructions. arXiv preprint
arXiv:2306.07154 , 2023. 2
[52] Tianhan Xu and Tatsuya Harada. Deforming radiance fields
with cages. In European Conference on Computer Vision ,
2022.
[53] Bangbang Yang, Chong Bao, Junyi Zeng, Hujun Bao, Yinda
Zhang, Zhaopeng Cui, and Guofeng Zhang. Neumesh:
Learning disentangled neural mesh-based implicit field for
geometry and texture editing. In European Conference on
Computer Vision , 2022. 2
[54] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu,
Eli Shechtman, and Noah Snavely. Arf: Artistic radiance
fields. In European Conference on Computer Vision , 2022.
2
[55] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and
Guanbin Li. Dreameditor: Text-driven 3d scene editing with
neural fields. arXiv preprint arXiv:2306.13455 , 2023. 3
20911
