ShapeWalk: Compositional Shape Editing through Language-Guided Chains
Habib Slim
KAUST
habib.slim@kaust.edu.saMohamed Elhoseiny
KAUST
mohamed.elhoseiny@kaust.edu.sa
Figure 1. Visualizing shape chains from ShapeWalk. Our dataset consists of 158K unique shapes connected through 26K
edit chains, with an average length of 14 chained shapes. We illustrate the interpolation process by coloring shapes based on
their proximity to the starting shape ( blue), and the ending shape ( orange ). Each consecutive pair of shapes is associated with
precise language instructions describing the applied edits. For each shape transition, we also provide a precise edit vector θij
describing the parameter changes necessary to transition from one shape to the next.
Abstract
Editing 3D shapes through natural language instructions
is a challenging task that requires the comprehension of
both language semantics and fine-grained geometric de-
tails. To bridge this gap, we introduce ShapeWalk, a care-
fully designed synthetic dataset designed to advance the
field of language-guided shape editing. The dataset consists
of 158K unique shapes connected through 26K edit chains,
with an average length of 14 chained shapes. Each con-
secutive pair of shapes is associated with precise language
instructions describing the applied edits. We synthesize edit
chains by reconstructing and interpolating shapes sampled
from a realistic CAD-designed 3D dataset in the parameterspace of the GeoCode shape program. We leverage rule-
based methods and language models to generate accurate
and realistic natural language prompts corresponding to
each edit. To illustrate the practicality of our contribution,
we train neural editor modules in the latent space of shape
autoencoders, and demonstrate the ability of our dataset to
enable a variety of language-guided shape edits. Finally,
we introduce multi-step editing metrics to benchmark the
capacity of our models to perform recursive shape edits.
We hope that our work will enable further study of composi-
tional language-guided shape editing, and finds application
in 3D CAD design and interactive modeling.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
22574
Figure 2. Comparing ShapeTalk with our work. We compare
the ShapeTalk [3] dataset ( top) with our work ( bottom ). For an
equivalent edit instruction (in green), ShapeTalk provides pairs of
shapes with many factors of variation, while we generate synthetic
pairs of shapes with a single clear varying factor.
1. Introduction
Whether in the realms of computer-aided design, virtual re-
ality environments, or digital content creation, the process
of refining and enhancing 3D visual data often involves in-
tricate adjustments to geometric shapes, textures, and light-
ing. Furthermore, the necessity for precise modifications
adds an additional layer of complexity, as inaccuracies can
have profound implications on the final output. This labor-
intensive nature of 3D data editing not only hinders work-
flow efficiency but also poses a barrier for individuals with-
out specialized skills, limiting the democratization of 3D
content manipulation. As such, there exists a compelling
need for innovative solutions that streamline and democra-
tize the 3D editing process, making it more accessible to a
broader range of users. But how to train models with the
ability to perform these complex edits?
To that end, we introduce ShapeWalk, a dataset aiming at
addressing the challenges inherent in the intricate and skill-
intensive nature of 3D data editing. Our dataset consists
of synthesized chains of 3D shapes, connected through edit
vectors associated with precise language instructions de-
scribing the applied edits. It contains 158K unique shapes
connected through 26K edit chains, with an average length
of 14 chained shapes. Our generation method is scalable
and can yield an indefinite number of realistic shape chains,
and extended to any 3D domain and shape program. To our
knowledge, our dataset is the first to provide a large-scale
collection of realistic 3D shape edit chains with precise
language instructions, and matching ground-truth edited
shapes. Our method yields specific edits undiluted by other
factors of variation, as illustrated in Figure 2.
To validate the usefulness of our dataset, we train neural
editor modules in the latent space of shape autoencoders [1],
and demonstrate the ability of our dataset to train models
capable of performing a variety of shape edits. We learnlatent mappings in the space of a frozen shape autoencoder,
and show that our method can be applied on shapes unseen
during the training of the neural editor. Taking advantage of
the ground-truth shape chains provided by our dataset, we
introduce multi-step editing metrics inspired from the field
of trajectory prediction [4, 26] to evaluate the quality of our
models.
Our contributions can be summarized as follows:
•Chained Shape-Editing Dataset: We introduce a syn-
thetic dataset of chained edits, with associated language
instructions and ground-truth edited shapes. This is the
first dataset of its kind, and is designed to facilitate the
study of compositional shape editing.
•Neural Shape Editor: We introduce a neural editor mod-
ule trained on our dataset, and demonstrate its ability
to perform a variety of shape edits. We show that our
method can be applied on shapes unseen during the train-
ing of the neural editor.
•Chained-Editing Metrics: With the introduction of
ground-truth shape chains, we propose multi-step editing
metrics to evaluate the ability of neural editors to perform
recursively applied edits.
2. Related work
3D Shape Programs. A significant body of work [9, 11,
12, 14, 19] is dedicated to exploring the use of procedu-
ral programs for 3D shape representation. Representing 3D
objects as visual programs has many advantages. Repre-
senting shapes as visual primitives enhances interpretabil-
ity [11, 14], as programs are by definition human-readable
and thus understandable by human experts. Shape programs
are more compact [9, 27] than their usual 3D shape modal-
ities, and can be used to represent shapes more efficiently.
Programs can also be composed to create shapes [11, 14],
or decomposed into smaller programs. Ideally, shape pro-
grams can be edited [13] to modify the underlying geometry
they represent.
Our dataset consists of a collection of synthetic shapes
created utilizing a backbone mesh-generating shape pro-
gram. Specifically, we build from GeoCode [17], a 3D
shape synthesis technique addressing the challenge of
mapping high-fidelity geometry to an editable parameter
space. GeoCode introduces a procedural program enabling
the generation of high-quality mesh outputs with a balanced
blend of interpretability and fine control.
Language-Guided 3D Shape Editing. Various works
have explored the use of natural language instructions
to guide 3D shape editing. Recent efforts [16, 20, 21]
propose to leverage pre-trained CLIP [24] models to align
22575
3D shapes with a given text prompt. Early efforts in that
direction [16] included optimizing meshes to progressively
align them with CLIP embeddings, leading to important
computational overhead. CLIP-Sculptor [21], a more
recent work, leverages a voxelized representation and a
discrete latent space conditioned on CLIP’s image-text
embeddings to perform fast and fidel shape edits without
shape optimization.
Shape Editing Datasets. Other works leverage large text-
aligned 3D shape datasets and require significant manual
annotation effort. ShapeCrafter [8] generates 3D shapes
incrementally from text using a neural network, evolving
with additional phrases. ShapeCrafter is designed for re-
cursive shape edition and utilizes a VQ-V AE [23] model
to represent shapes as discrete codes. This method ex-
hibits consistent shape-text alignment with gradual evolu-
tion. ChangeIt3D [3] introduces ShapeTalk, a large dataset
for describing 3D shape differences. The framework fa-
cilitates language-based editing of 3D models without re-
quiring 2D to 3D conversion methods, and learns a shape
editing model by learning contrasts between sampled shape
pairs.
ShapeTalk [3] is the most similar work to our proposed
dataset, and is the only publicly available dataset specifi-
cally designed for language-guided 3D shape-to-shape edit-
ing. ShapeTalk is a remarkable contribution in the field of
language-guided shape editing, and is one of the first works
to leverage large-scale 3D shape datasets to facilitate the
study of language-guided shape editing. However, this work
has some limitations that we attempt to address in our work.
Collecting a dataset of shape differences is a challenging
task requiring considerable manual annotation effort, and
gathering a large number of 3D shapes may not be feasible
for all domains. Furthermore, ShapeTalk is composed of
edit contexts (i.e. shape pairs and edit instructions) which
are hard to separate into fine-grained, composable edits.
Most of the time, singular shape edits do not have have an
exact ground-truth in the form of a source and target shape
pair. We illustrate this important distinction in Figure 2.
The availability of singular shape edits with an associated
ground-truth would ease benchmarking the quality of shape
editing methods, and facilitate the training process of edit-
ing models. ShapeTalk also does not provide a mechanism
to generate edit chains, which are necessary to study com-
positional shape editing. Finally, ShapeTalk does not easily
enable the study of shape editing in a compositional man-
ner. In contrast, our dataset is 1)synthesized by augmenting
a small set of diverse shapes and can easily be scaled up,
2)composed of fine-grained edits and coarse-grained edits
with exact ground-truths, and 3)separated into edit chains,
which are designed to facilitate the study of compositional
shape editing.3. ShapeWalk
Our dataset contains 158K shapes split into a random and
realistic set. With each edge connecting two consecutive
shapes, we also produce a text instruction generated us-
ing the parameter changes necessary to transition from one
shape to the next. We detail the generation method of our
dataset here, and summarize the process in Figure 3.
3.1. Dataset
Definition. Our dataset can be defined as a collection
of directed graph paths (dipaths), denoted as P(k)=
(S(k), E(k), f(k)). For each shape chain P(k)of length l,
we denote:
•S(k)={s(k)
θ1, . . . , s(k)
θN}, the set of distinct shape nodes
composing the chain. Each shape is defined by a set of
parameters θi⊆Θ, where Θis the linear parameter space
of our shape program.
•E(k)⊆ {(s(k)
θi, s(k)
θj)|s(k)
θi, s(k)
θj∈V(k), j=i+ 1}, the
set of edges linking each consecutive pair of shapes.
•f(k):E(k)7→ {(θij,pij)|θij⊆Θ,pij∈Σ}, a
function mapping each edge to a vector θij⊆Θand
a set of text instructions pij.θijdefines the parameter
changes, or edits necessary, to go from shape itoj.pij
is a set of text instructions describing this edit in natural
language.
Generation. To generate our dataset, we start by recon-
structing a set S⊆ S of realistic 3D CAD shapes into the
space of shapes covered by our shape program SΘ⊂ S. To
that end, we utilize shapes from the 3DC OMPAT++[15, 22]
dataset, a realistic, industry-based 3D CAD dataset. We em-
ploy this dataset to avoid overlap with ShapeNet [5], and to
ensure diversity and visual quality of the shapes.
We define a visual similarity function d:S × S 7→
Rbetween two shapes si, sj∈Sas the feature-wise
mean squared error between the original and reconstructed
meshes’ renderings, in the space of a pre-trained ResNet-
50 [10] encoder ϕR:Rh×w×37→Rd.
d(si, sj) =∥ϕR(fR(si))−ϕR(fR(sj))∥2
Where fR:S7→R3×h×wis a rendering function. The set
of reconstructed shapes can then be defined as:
arg min
SR⊆S
|SR|=(1−α)|S|X
s∈SRd(s,bs)
Wherebsis the reconstructed shape, defined as:
bs=fΘ◦ϕΘ(s)
22576
Figure 3. Detailling the shape generation pipeline of ShapeWalk. Realistic 3D CAD shapes are reconstructed from the
3DC OMPAT++[22] dataset into the GeoCode [17] shape program parameter space, using a mapping function ϕΘ. Reconstructed shapes
with an error over a fixed threshold are discarded using a visual similarity function d, which is based on rendered feature similarity. Filtered
pairs of shapes are then interpolated in the parameter space of the shape program, to generate shape chains P(k).
With fΘ: Θ7→ S the shape program mapping shape pa-
rameters to a 3D mesh, and ϕΘ:S 7→ Θan encoder based
on DGCNN [17, 25] which samples points from a shape
surface and regresses corresponding shape parameters in Θ.
We reconstruct shapes by first sampling pointclouds
from the surface of the original meshes, and then feeding
them into the DGCNN [25] encoder fine-tuned [17] to
regress the parameters of the shape program. We discard
a ratio of α= 0.08of the shapes with the highest re-
construction error, which we measure as the feature-wise
mean squared error between the original and reconstructed
meshes’ renderings, in the space of a pre-trained ResNet-
50 [10] encoder. This threshold is selected empirically
by visually inspecting the shapes in the upper part of the
reconstruction error distribution. In our dataset instance
building on 3DC OMPAT++shapes, we filter out a total
of89shapes during this process, leading to a set of
|SR|= 1113 reconstructed shapes. This process can be
scaled up to any dataset size and any shape domain, as long
as a corresponding shape program is available.
Shape interpolation. After reconstructing a set SRof re-
alistic shapes, we generate a set of shape chains P(k)by
interpolating between the parameters of shape pairs.
To that end, we first sample a set of shape pairs
(sθi, sθj)∈ S P⊆SR×SR, and partition them into
L= 10 levels of proximity. These proximity levels are
based on the feature-wise mean squared error between the
original and reconstructed meshes’ renderings, in the space
of a pre-trained ResNet-50 [10] encoder.
We define the set of shape pairs S(l)
Pat level l∈J1, LKas:
S(l)
P={(sθi, sθj)∈ SP|d(sθi, sθj)∈l−1
L,l
L
}
Where dis a visual similarity function, min-max normal-
ized across all pairs of shapes.For each pair (sθi, sθj)∈ S(l)
P, we then interpolate between
the shape parameters θiandθjto generate a set of interme-
diate shape parameters (θi, . . . , θ i+(N−2), θj)∈ΘN. The
number of intermediate shapes Nis defined as the number
of differing parameters between θiandθj, that is:
N=|{k|θi[k]̸=θj[k]}|
When the proximity level is low, edits necessary to transi-
tion from one shape to another are larger in intensity as the
shapes are more different, and smaller when lis smaller.
The ordering of intermediate edits is randomly sampled us-
ing a dependency-aware algorithm (for example, parame-
ters relating to armrest height or width are sampled after the
addition of armrests).
As illustrated in Figure 1, this generation process leads
to plausible interpolated shapes, and to fine-grained and
realistic sequences of edits with detailed metadata.
We summarize the process of generating realistic shape
chains in Figure 3.
Random set. While the realistic set is based on recon-
structed shapes from the 3DC OMPAT++[15, 22] dataset,
therandom set is based on a collection of random shapes
generated using the shape program. This alternative set
aims at covering a large space of the parameter space of
the GeoCode shape program. This subset is generated by
systematically sampling various combinations of parame-
ters within defined ranges determined by a minimum edit
intensity, but does not necessarily result in realistic shapes.
3.2. Text Instructions
Rule-based generation. Given an edit vector θijde-
scribing the parameter changes necessary to transition
from shape itoj, we generate a set of text instructions
pijdescribing this edit in natural language. We first
map each parameter θi[k]composing the edit vector to
a natural language name, and map the magnitude of the
22577
Figure 4. Dataset chains samples. We feature in this figure shape chains from our dataset with associated generated text instructions. Each
shape chain is composed of a sequence of shapes, with each consecutive pair of shapes associated to a set of text instructions describing the
edits necessary to transition from one shape to the next. Overall, generated text instructions are accurate and provide a detailed description
of the edits applied to the shapes. We include in the supplementary material additional samples of generated shape chains, alongside shape
chains generated for two additional table andvase categories.
parameter change to a natural language intensity depending
on the parameter type. Boolean parameters are also
described appropriately, for example by using the word add
orremove when describing the addition or removal of a part.
Augmentation. This rule-based instruction generation
leads to perfectly accurate instructions, but may lack diver-
sity. To alleviate this issue, we 1)randomly sample adjec-
tives and adverbs to characterize the intensity of edits, 2)
randomly invert parameter names and magnitude directions
(e.g. ” increase armrests straightness ” will be augmented
to ”decrease armrests bend ” for the same edit). As a final
augmentation, we utilize a T5 [18] transformer model fine-
tuned for paraphrasing to generate additional instructions
for each edit vector. We use the Parrot library [6] to fil-
ter generated paraphrases based on a fluency and adequacy
score.
We summarize the process of generating text instructions in
the supplementary material.4. Neural Shape Editing
4.1. Problem
We are concerned with the task of editing a shape sigiven
a sequence of natural language edit instructions. We want
to learn an editing function fE:S ×Σ7→ S , where S
is the space of input shapes, and Σis the space of natu-
ral language edit instructions, able to compose edits from a
starting shape s1to an ending shape sN, given a sequence
of edit instrutions {pt}N
t=1.
csN=fE(ϕT(p12), fE(ϕT(p23), . . . f E(ϕT(pkN), s1)))
Where N=|P|is the length of the edit chain, and ϕT:
Σ7→RDTis a text encoder. At the end of the edit se-
quence, we want to recover a shape csNas close as possible
to the ground-truth shape sN. Our proposed dataset pro-
vides a ground-truth for every intermediate shape sk, which
we leverage to train and evaluate our method.
22578
Figure 5. Visualizing synthesized pairs of edits using our latent editors. We illustrate the diverse range of edits generated by our
proposed latent mapper, showcasing its ability to capture nuanced variations in the input data, for 3DS2VS latents (top two rows) and
PC-AE latents (bottom two rows). Each pair demonstrates the original input on the left and the corresponding synthesized output on the
right, with the associated caption. We abbreviate instructions which are too long to fit in the figure, and indicate when text is omitted with
an ellipsis. For more subtle changes, we circle the areas of interest in both source and target shapes. Overall, edits are consistent with the
input instruction, preserve shape identity while generating plausible output shapes.
4.2. Objective
One of the main advantages of training a shape editing
model with ShapeWalk compared to other works [2, 8] is
the availability of exact ground-truth edited shapes for each
edit instruction (see Section 2). We thus formulate the ob-
jective function of our learning problem as a simple L2loss
between the latents of the synthesized shape and the ground-
truth shape. For all pairs of shapes (si, sj)and their corre-
sponding edit instructions pij, we minimize the following
loss:
L=∥fE(ϕE(si),pij)−ϕE(sj)∥2Where ϕAE=ϕE◦ϕDis a shape autoencoder, and ϕE:
S 7→RDEdenotes its encoder component.
4.3. Models
ForϕAE, we experiment with PC-AE [1] and 3D2VS [28]
models both pre-trained on ShapeNet [5]. Note that our
method is agnostic to the choice of shape autoencoder and
can be adapted to a variety of shape representations. For
ϕT, we use a pretrained BERT [7] model.
Both ϕAEandϕTare frozen during training, and we
only train the parameters of our latent mapper fE.
22579
For PC-AE, we formulate our latent mapper as a neural
module which for each tuple (ϕE(si),pij, ϕE(sj))pre-
dicts an edit vector cθij∈RDEin the feature space of ϕE,
and adds it to the latent of the input shape sito generate the
latent of the output shape sj:
fE(ϕE(si),pij) =cθij+ϕE(si)
We utilize two variants of our latent mapper fE: one in
which the edit vector cθijis predicted directly, and one in
which the edit vector cθijis predicted as a product of a nor-
malized edit direction cvij∈RDEand a magnitude dmij∈R
separately.
For 3D2VS which encodes shapes as latents sets of
higher resolutions, we use a transformer-based latent diffu-
sion model instead to generate edited latents. We concate-
nate the input shape latent ϕE(si)with the noised edited
shape latent ϕE(sj+ϵt)and feed the BERT text embed-
dings to the cross-attention layers of the transformer blocks
to predict the added noise. We illustrate our architecture for
the latent mapper in the supplementary material.
5. Experiments
5.1. Chained Shape Editing
Comparison Models. For PC-AE, we experiment with
three variants of our latent mapper module:
• D IRECT GENdirectly predicts the edited shape latent
without regressing an edit vector.
• L ATEFUSION follows the network proposed in [3], and
first passes the shape embeddings through an encoder.
Encoded shape features are then concatenated with text
embeddings and fed into a neural module which finally
predicts the edit vector.
• O URS is a simple multi-layer perceptron (MLP) which
directly takes the context (shape and text embeddings) as
input to predict an edit vector.
For all of these variants, we experiment with various bottle-
neck dimensions and number of layers (in subscript).
We further decompose these variants into coupled andde-
coupled versions, where we predict a normalized edit direc-
tioncvij∈RDEand a magnitude dmij∈Rseparately. We
provide in appendix the full architecture details and training
hyperparameters employed to train our models.
Note that we do not compare with ChangeIt3D [2] as
their neural-listener distillation method is not applicable
in our context: we train our models with a direct feedback
signal from the ground-truth edited shapes.Single-step metrics. We propose to measure the quality
of our editing steps both before and after shape reconstruc-
tion. Since we experiment on pointclouds, we use the scaled
Chamfer Distance between the reconstructed original shape
siand the reconstructed edited shape bsj:
dCD=1
|bsj|X
x∈bsjmin
y∈si∥x−y∥2+1
|si|X
y∈simin
x∈bsj∥x−y∥2
We also use the L2distance between the latents of the orig-
inal shape siand the edited shape sj:
dL2=∥ϕE(si)−ϕE(sj)∥2
Multi-step metrics. To extend the single-step metrics to
chained shape generation, we take inspiration from the tra-
jectory prediction literature [4, 26] and propose appropriate
metrics for our task.
Given a shape distance function d:S × S 7→ R, a chain
of ground-truth reconstructed shapes {s1, . . . , s N}, and a
set of recursively generated shapes {bs1, . . . ,csN}, we de-
fine the average edit error as the average distance between
the generated shapes and the corresponding ground-truth
shapes:
Ad=1
NNX
t=1d(st,bst)
And the final edit error as the distance between the last
shape in the chain and the corresponding ground-truth
shape:
Fd=d(sN,csN)
We report results with both metrics for d=dCDand
d=dL2.
Results. We provide qualitative results of edited pairs gen-
erated by our latent mapper in Figure 5. Overall, we observe
that our method is able to generate plausible edits that are
consistent with the input instruction, and preserve the iden-
tity of the input shape. However, our method is limited by
the quality of the shape autoencoder. We notice that the PC-
AE editor is unable to perform very fine-grained edits which
are not properly captured by the autoencoder, while visible
in the ground-truth shapes. We explore the possible causes
of these limitations in Section 5.2.
We provide quantitative results for our chained editing met-
rics averaged across |P| ∈ { 10,15,20}in Table 1, and de-
tail per-chain length results in the appendix. Directly pre-
dicting the edited shape latent (D IRECT GEN) leads to poor
edit predictions across all metrics, although increasing the
number of layers may improve the results. Overall, decou-
pling the edit direction and magnitude leads to better results.
Our coupled O URS512×4variant performs better than the
coupled L ATEFUSION 512variant on the Chamfer Distance,
but worse on L2distance.
22580
Model decoupled?Averaged ∀|P|
FCD×1e4ACD×1e4 FL2 AL2
LATEFUSION 1024 /times-circle 2.856 2.621 1.444 1.251
LATEFUSION 512 /times-circle 2.719 2.609 1.462 1.290
LATEFUSION 256 /times-circle 2.874 2.651 1.509 1.283
OURS512×8 /times-circle 3.208 2.822 1.703 1.437
OURS512×4 /times-circle 2.990 2.703 1.589 1.351
LATEFUSION 1024 /check-circle 2.711 2.568 1.405 1.245
LATEFUSION 512 /check-circle 3.002 2.708 1.451 1.254
LATEFUSION 256 /check-circle 3.309 2.848 1.552 1.324
OURS512×8 /check-circle 2.782 2.584 1.497 1.290
OURS512×4 /check-circle 2.670 2.524 1.447 1.266
Table 1. Chained shape editing ablation. We report baseline re-
sults for our proposed chained shape editing task, averaged across
all chain lengths |P| ∈ { 10,15,20}. Both the average final and
average edit error are reported for the Chamfer Distance (CD) and
L2distance ( L2) metrics.
Parameter Accuracy
seat height 1.000
backrest curvature 1.000
object width/height/depth 0.978
seat roundness 0.978
top bar thickness/height 0.961
legs thickness 0.956
legs bending/curvature 0.944
adding/removing handle cushions 0.750
number of legs/backrest rails 0.663
legs roundness/indentation 0.587
AVG(RANDOM ) 0.884
AVG(REALISTIC ) 0.969
Table 2. Parameter-wise accuracy for shape edit recognition.
We report the accuracy of our edit detector for each parameter in
therandom set, alongside the average accuracy for the full random
andrealistic sets. Overall, we observe that our classifier is able to
recognize edits corresponding to high change edits, but fails to
differentiate between subtle changes.
5.2. Recognizing Shape Edits
Problem. To provide additional insight into the quality of
our latent mapper and the difficulty of predicting specific
edits, we train a binary classifier to discriminate from a
pair of shapes (si, sj)on whether the corresponding edit
instruction pijwas applied to siorsj. We train a binary
classifier fC:S × S 7→ { 0,1}, where fC(si, sj) = 1 if the
edit instruction pijwas applied to si, and 0otherwise.
Method. Similarly to our latent mapper architecture for PC-
AE, we use an MLP-based shape latent encoder to encodethe input shape latents into the bottleneck dimension, and
project text features using a single linear layer to the same
dimension. We then concatenate the two feature vectors and
pass them through an MLP predictor outputting probabili-
ties for the two classes siandsj. We train our classifier
using a binary cross-entropy loss.
Training and evaluation. We train our classifier on the
realistic set and evaluate it on the full random shape set,
which has a restricted vocabulary of edit instructions. We
evaluate the accuracy of the classifier on the full set, as well
as on subsets of the data corresponding to specific types of
edits.
Results. In Table 2, we provide accuracy results for shape
edit recognition, depending on the edit type. We also pro-
vide the global accuracy on the random andrealistic sets.
We observe that our classifier is able to recognize edits cor-
responding to shape parameters associated with a high de-
gree of shape variation (like seat height and width/height of
the shape), but struggles with parameters which are more
subtle (like legs roundness, handle cushions). Our classifier
is able to detect edits related to global width/height/depth
adjustments with an accuracy of 97.8%. This accuracy
drops to 94.4% for legs bending/curvature, which is a more
subtle change in the shape, and to 58.7% for legs round-
ness/indentation. We also note that the model does not per-
form well on edits related to the number of parts, i.e. adding
or removing a number kof legs or backrest rails.
We impute these discrepancies to several factors. The
PC-AE shape autoencoder we employ may be unable to
properly reconstruct fine-grained details in the ground-truth
and edited shapes, which renders the task of detecting subtle
changes more difficult. Pointcloud resolution is also a lim-
itation, as fine-grained parameters like leg indentation may
not be properly sampled. Finally, learning a latent mapper
and a classifier that can accurately count the number of parts
to add or remove may also be a difficult task of its own, and
require more domain-specific inductive biases.
6. Conclusion
In conclusion, this paper introduces the ShapeWalk dataset,
designed for advancing compositional shape editing guided
by natural language instructions. The dataset comprises
158K unique shapes connected through 26K edit chains,
synthesized from a realistic CAD-designed 3D dataset.
Language instructions for applied edits are provided, along-
side exact ground-truth edited shapes. Our method requires
zero human annotation effort, and can be scaled up indef-
initely. The usefulness of the dataset is demonstrated by
training neural editor modules in the latent space of shape
autoencoders [1]. Evaluation metrics inspired by trajec-
tory prediction literature [4, 26] are introduced, offering a
quantitative assessment of the quality of recursively edited
shapes.
22581
References
[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and
Leonidas Guibas. Learning representations and generative
models for 3d point clouds. In ICML , 2018. 2, 6, 8
[2] Panos Achlioptas, Ian Huang, Minhyuk Sung, Sergey
Tulyakov, and Leonidas Guibas. Shapetalk: A language
dataset and framework for 3d shape edits and deformations.
In2023 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) . IEEE, 2023. 6, 7
[3] Panos Achlioptas, Ian Huang, Minhyuk Sung, Sergey
Tulyakov, and Leonidas Guibas. ShapeTalk: A language
dataset and framework for 3d shape edits and deformations.
InCVPR , 2023. 2, 3, 7
[4] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan,
Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. So-
cial lstm: Human trajectory prediction in crowded spaces.
In2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 961–971, 2016. 2, 7, 8
[5] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat
Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano-
lis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi,
and Fisher Yu. ShapeNet: An Information-Rich 3D Model
Repository. In arXiv , 2015. 3, 6
[6] Prithiviraj Damodaran. Parrot: Paraphrase generation for
nlu., 2021. 5
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional trans-
formers for language understanding, 2019. 6
[8] Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, and Srinath
Sridhar. Shapecrafter: A recursive text-conditioned 3d shape
generation model, 2023. 3, 6
[9] Aditya Ganeshan, R. Kenny Jones, and Daniel Ritchie. Im-
proving unsupervised visual program inference with code
rewriting families. In Proceedings of the International Con-
ference on Computer Vision (ICCV) , 2023. 2
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep Residual Learning for Image Recognition. In CVPR ,
2016. 3, 4
[11] R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang,
Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel
Ritchie. Shapeassembly: learning to generate programs for
3d shape structure synthesis. ACM Transactions on Graph-
ics, 39(6):1–20, 2020. 2
[12] R. Kenny Jones, David Charatan, Paul Guerrero, Niloy J. Mi-
tra, and Daniel Ritchie. Shapemod: macro operation discov-
ery for 3d shape programs. ACM Transactions on Graphics ,
40(4), 2021. 2
[13] R. Kenny Jones, Homer Walke, and Daniel Ritchie. Plad:
Learning to infer shape programs with pseudo-labels and
approximate distributions. In 2022 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) . IEEE,
2022. 2
[14] R. Kenny Jones, Paul Guerrero, Niloy J. Mitra, and Daniel
Ritchie. Shapecoder: Discovering abstractions for visual
programs from unstructured primitives. ACM Transactions
on Graphics , 42(4), 2023. 2[15] Yuchen Li, Ujjwal Upadhyay, Habib Slim, Ahmed Abdelre-
heem, Arpit Prajapati, Suhail Pothigara, Peter Wonka, and
Mohamed Elhoseiny. 3D CoMPaT: Composition of Materi-
als on Parts of 3D Things. In ECCV , 2022. 3, 4
[16] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,
and Tiberiu Popa. Clip-mesh: Generating textured meshes
from text using pretrained image-text models. In SIGGRAPH
Asia 2022 Conference Papers . ACM, 2022. 2, 3
[17] Ofek Pearl, Itai Lang, Yuhua Hu, Raymond A. Yeh, and Rana
Hanocka. Geocode: Interpretable shape programs. In arXiv
preprint arxiv:2212.11715 , 2022. 2, 4
[18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with a
unified text-to-text transformer. In arXiv , 2023. 5
[19] Daniel Ritchie, Paul Guerrero, R. Kenny Jones, Niloy J. Mi-
tra, Adriana Schulz, Karl D. D. Willis, and Jiajun Wu. Neu-
rosymbolic models for computer graphics. Computer Graph-
ics Forum , 42(2), 2023. 2
[20] Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang,
Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-
shan. Clip-forge: Towards zero-shot text-to-shape genera-
tion. In 2022 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) . IEEE, 2022. 2
[21] Aditya Sanghi, Rao Fu, Vivian Liu, Karl D.D. Willis,
Hooman Shayani, Amir H. Khasahmadi, Srinath Sridhar,
and Daniel Ritchie. Clip-sculptor: Zero-shot generation of
high-fidelity and diverse shapes from natural language. In
2023 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) . IEEE, 2023. 2, 3
[22] Habib Slim, Xiang Li, Mahmoud Ahmed Yuchen Li, Mo-
hamed Ayman, Ujjwal Upadhyay Ahmed Abdelreheem,
Suhail Pothigara Arpit Prajapati, Peter Wonka, and Mo-
hamed Elhoseiny. 3DCoMPaT++: An improved large-scale
3d vision dataset for compositional recognition. In arXiv
preprint arxiv:2212.11715 , 2023. 3, 4
[23] A ¨aron van den Oord, Oriol Vinyals, and Koray
Kavukcuoglu. Neural discrete representation learning.
CoRR , abs/1711.00937, 2017. 3
[24] Suchen Wang, Yueqi Duan, Henghui Ding, Yap-Peng Tan,
Kim-Hui Yap, and Junsong Yuan. Learning transferable
human-object interaction detector with natural language su-
pervision. In 2022 IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) . IEEE, 2022. 2
[25] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma,
Michael M. Bronstein, and Justin M. Solomon. Dynamic
Graph CNN for Learning on Point Clouds. In SIGGRAPH ,
2019. 4
[26] Kaiping Xu, Zheng Qin, Guolong Wang, Kai Huang, Shux-
iong Ye, and Huidi Zhang. Collision-Free LSTM for Human
Trajectory Prediction , pages 106–116. 2018. 2, 7, 8
[27] Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi,
Hooman Shayani, Ali Mahdavi-Amiri, and Hao Zhang.
Capri-net: Learning compact cad shapes with adaptive prim-
itive assembly. In 2022 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) . IEEE, 2022. 2
22582
[28] Biao Zhang, Jiapeng Tang, Matthias Nießner, and Peter
Wonka. 3dshape2vecset: A 3d shape representation for neu-
ral fields and generative diffusion models. ACM Transactions
on Graphics , 42(4):1–16, 2023. 6
22583
