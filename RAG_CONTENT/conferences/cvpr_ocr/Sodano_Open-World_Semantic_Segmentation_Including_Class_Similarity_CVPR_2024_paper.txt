Open-World Semantic Segmentation Including Class Similarity
Matteo Sodano1Federico Magistri1Lucas Nunes1Jens Behley1Cyrill Stachniss12
1Center for Robotics, University of Bonn2Lamarr Institute for Machine Learning and Artiﬁcial Intelligence
ffirstname.lastname g@igg.uni-bonn.de
Abstract
Interpreting camera data is key for autonomously act-
ing systems, such as autonomous vehicles. Vision systems
that operate in real-world environments must be able to un-
derstand their surroundings and need the ability to deal
with novel situations. This paper tackles open-world se-
mantic segmentation, i.e., the variant of interpreting image
data in which objects occur that have not been seen during
training. We propose a novel approach that performs accu-
rate closed-world semantic segmentation and, at the same
time, can identify new categories without requiring any ad-
ditional training data. Our approach1additionally provides
a similarity measure for every newly discovered class in an
image to a known category, which can be useful information
in downstream tasks such as planning or mapping. Through
extensive experiments, we show that our model achieves
state-of-the-art results on classes known from training data
as well as for anomaly segmentation and can distinguish
between different unknown classes.
1. Introduction
Autonomous systems need to understand their surroundings
to operate robustly. To this end, semantic scene understand-
ing based on sensor data is key and numerous variants exist,
such as object detection [17, 49], semantic and instance seg-
mentation [37, 39, 64], and panoptic segmentation [27, 28].
Over the last decade, we witnessed tremendous progress in
scene interpretation for autonomous vehicles using machine
learning. A central challenge for most learning-based sys-
tems is scenes in which novel and previously unseen objects
occur. Such open-world settings, i.e., the fact that not ev-
erything can be covered in the training data, have to be con-
sidered when building vision systems for human-centered
environments and real-world settings. For example, au-
tonomous cars in cities will eventually experience situations
or objects they have not seen before. They should be able to
identify them, for example, to change into a more conserva-
tive mode of operation.
Today, high-quality datasets such as Cityscapes [13] or
1Code: https://github.com/PRBonn/ContMAV
Input RGB Input RGB
Closed-World Prediction Closed-World Prediction
Open-World Prediction Open-World Prediction
road vegetation unknown traffic sign sky truckFigure 1. Given an image containing a previously-unseen object
(top), closed-world methods for semantic segmentation classify
the pixels belonging to that object as one of the known classes
(center, red circle). Our goal is to segment the unknown object and
identify it as a semantic class different to the previously-known
ones (bottom, green circle).
MS COCO [32] allow deep learning methods to achieve
outstanding performance in closed-world scene understand-
ing tasks. A prominent task is semantic segmentation [18],
which aims to assign a semantic category to each pixel in an
image. Systems operating under the closed-world assump-
tion [53] typically cannot correctly recognize an object that
belongs to none of the known categories. Often, they tend
to be overconﬁdent and assign such an object to one of
the known classes. We believe that for applications target-
ing reliability and robustness under varying conditions, the
closed-world assumption has to be relaxed, and we need to
move towards open-world setups. Additionally, a measure
of class similarity can help downstream tasks. For example,
predicting that an area of the image is unknown but similar
to the class car or another type of moving vehicle can be
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3184
used in planning or tracking to estimate the motion of the
object, or in mapping to discard that class from the map.
This paper investigates the problem of open-world se-
mantic segmentation. Given an image at test time, we aim
to have a model that is able to detect any pixel that be-
longs to a category that was unseen at training time and is
also able to distinguish between different new categories.
The ﬁrst problem is called anomaly segmentation [9] and
aims to achieve a binary segmentation between known and
unknown. The second problem, called novel class discov-
ery[20], aims to obtain a pixel-wise classiﬁcation of novel
samples into different classes starting from the knowledge
of previously seen, labeled samples. We aim to investigate
how to solve both tasks jointly in a neural network setting.
We extend best-practice approaches for anomaly detection
for classiﬁcation tasks [2, 15, 66] and provide compelling
results for both, anomaly segmentation and novel class dis-
covery. See Fig. 1 for an example of the targeted output.
The main contribution of this paper is a novel approach
for open-world segmentation based on an encoder-decoder
convolutional neural network (CNN). We propose a new
method that simultaneously performs accurate closed-world
semantic segmentation while constraining all known classes
towards their learned feature descriptor, thanks to a loss
function we introduce. We combine operations on the fea-
ture space with binary anomaly segmentation that allows us
to distinguish between different novel classes and provide
a measure of similarity for every newly discovered class to
a known category. We implemented and thoroughly tested
our approach. In sum, our contributions are the following:
• A fully-convolutional neural network that achieves state-
of-the-art performance on anomaly segmentation while
providing compelling closed-world performance.
• A loss function that allows us to distinguish among dif-
ferent novel classes, and to provide a similarity score for
each novel class to the known categories.
• Extensive experiments on multiple datasets, including the
public benchmark SegmentMeIfYouCan, where we rank
ﬁrst in three out of ﬁve metrics.
2. Related Work
Semantic segmentation under closed-world settings
achieved outstanding performances in different domains,
such as autonomous driving [7, 40, 42, 62], indoor naviga-
tion [24, 30, 56], or agricultural robotics [12, 41, 50, 63].
However, the closed-world assumption should be relaxed
when developing systems for navigating in the wild. In
such cases, we need to move towards open-world setups.
Anomaly Detection and Classiﬁcation . The open-
world setting was initially explored for classiﬁcation, where
anomalous samples had to be recognized and discarded.
This problem was tackled in different ways in the literature.
Simple strategies such as thresholding the softmax activa-tions [8, 22], using a background class for tackling unknown
samples [6, 43, 60], and using model ensembles [31, 61]
represent a solid starting point in theory. In practice, how-
ever, closed-world predictions tend to be overconﬁdent by
showing a peak in the softmax even for unknown sam-
ples [45, 58]. Additionally, it is impossible to train with all
possible examples of unknown objects. To deal with this,
modiﬁcations to the softmax layer have been proposed [2].
Other approaches rely on maximizing the entropy [15] or on
energy scores [35], which are supposedly less susceptible
to the aforementioned overconﬁdence issue. Even though
these approaches can be easily adapted to the segmentation
problem, they are limited in the sense that they rely on the
output of the CNN to be “uncertainty-aware” to some ex-
tent, in order to be able to modify the scores, consider the
output entropies, or similar.
In contrast, we operate on the feature space of the se-
mantic segmentation to not only classify pixels correctly but
also match their feature to a unique class descriptor, in order
to use the distance from it as a measure of “unknown-ness”.
Open-World Segmentation . Open-world or anomaly
segmentation extends the anomaly detection task by trying
to predict whether each individual pixel in an image is an
anomaly or not. Some methods rely on the estimation of
the uncertainty on the prediction with Bayesian deep learn-
ing [16, 31, 54], or on the gradient [34, 38]. Other works
use an additional dataset for out-of-distribution samples, in
order to help the CNN recognize categories that do not be-
long to the standard training set [5, 10]. Recently, gener-
ative models have also been used, since in the reconstruc-
tion phase they will accurately resynthesize only the known
areas, while unknown objects will suffer from a lower re-
construction quality, and can be recognized by looking at
the most dissimilar areas between the input and the out-
put [29, 33, 71]. Due to the limitation of available training
data, many unsupervised approaches use synthetic anomaly
data and train an anomaly detector which is either distance-
based [36, 52, 59] or reconstruction-based [3, 67, 69], with
the latter sharing the same concept as the generative mod-
els mentioned above. Vision-language models based on
CLIP [47, 48, 72] are also gaining interest in the context
of anomaly segmentation [25]. Lately, a lot of research in-
terest is also going in the direction of anomaly segmenta-
tion in video streams because of its application in intelligent
surveillance systems [57, 65, 68].
Differently from these approaches, we do not require
additional data for training and do not rely on uncertainty
estimation or generative models. In contrast, by operat-
ing on the feature space of the semantic segmentation task,
we can deﬁne a distinct region for known and unknown
classes. Furthermore, we leverage the feature descriptors of
the known classes to recognize different unknown classes
and ﬁnd the most similar known class.
3185
Post-processing
Semantic
decoder
Contrastive
decoderEncoder
Class-wise GaussiansClass-wise Gaussians
1 K...
Closed-world semantic
segmentation
Anomaly segmentationInput RGB image
Open-world semantic
segmentationFigure 2. Given an RGB image as input, our network processes it by means of an encoder and two decoders. The semantic decoder
produces a closed-world semantic segmentation and a Gaussian model for each known category. The class Gaussian models are built from
a learned class descriptor (mean) and the variance of all predictions from it. A 3D example is shown in the image. The contrastive decoder
provides an anomaly segmentation output. A post-processing phase ﬁnally achieves open-world semantic segmentation.
3. Our Approach
In this work, we tackle the problem of open-world semantic
segmentation. In addition to handling known classes, we are
particularly interested in segmenting all anomalous areas in
an image, where previously unseen objects appear, and in
differentiating between potentially multiple novel classes.
We propose an approach (see Fig. 2) based on a convolu-
tional neural network with one encoder and two decoders.
The ﬁrst decoder tackles semantic segmentation and oper-
ates on the feature space so that, for each class, features
of pixels belonging to the same class are pushed together.
The mean and variance of each individual class descrip-
tor are stored representing Gaussian distributions that de-
scribe known classes. The second decoder performs binary
anomaly segmentation. Results are ﬁnally merged to obtain
open-world semantic segmentation, i.e. anomaly segmenta-
tion and novel class discovery.
3.1. General Network Architecture
Our network for open-world semantic segmentation is com-
posed of one encoder and two decoders. We use a
ResNet34 [21] encoder, where the basic ResNet block is
replaced with the NonBottleneck-1D block [51], which al-
lows a more lightweight architecture since all 33con-
volutions are replaced by a sequence of 31and13
convolutions with a ReLU in between. For open-world seg-
mentation, contextual information is valuable. Therefore,
we expand the limited receptive ﬁeld of ResNet by incor-
porating contextual information using a pyramid pooling
module [70] after the encoding part. The features produced
will be fed to two separate decoders, that share the same
structural properties. In order to preserve the lightweight
nature of the network, we use three SwiftNet modules [46]
where we incorporate NonBottleneck-1D blocks, and two
ﬁnal upsampling modules based on nearest-neighbor anddepth-wise convolutions, which reduce the computational
load. We use encoder-decoder skip connections after each
downsampling stage of the encoder to directly propagate
more ﬁne-grained features to the decoder.
3.2. Approach for Open-World Segmentation
Our approach for open-world segmentation builds upon the
structure of the CNN we developed, and it exploits the
double-decoder architecture for providing accurate segmen-
tation of unknown regions. The ﬁrst decoder, which we call
“semantic decoder” in the following, targets semantic seg-
mentation. We additionally manipulate the feature space
to create a unique distinct descriptor for each known class.
Our goal is to obtain a correct semantic segmentation for
the known classes, but also produce pre-softmax features
that are similar to the descriptor for each pixel of a certain
class. In this way, we aim to detect as unknown classes
all pixels whose feature vectors are substantially different
from the descriptor of the class they have been assigned
to. The second decoder, which we call “contrastive de-
coder” in the following, leverages the contrastive loss [11]
and objectosphere loss [15] together, to place all features of
known classes on the surface of a hypersphere while push-
ing the ones of unknown classes towards its center. In this
way, the second decoder provides an anomaly segmenta-
tion, where the anomalous regions correspond to previously
unseen classes. The two results are ﬁnally merged using
an automated post-processing operation to obtain the ﬁnal
open-world segmentation.
In the following, we call 
 =f¹11º¹,ºg
the set of pixels in the image, .2 f1 g,the
ground truth mask, and ˆ.2f1 g,the predicted
mask, where and,are the dimensions of the input im-
age. Additionally, we denote with 
:=f?2
j.?=:g
the set of pixels whose ground truth label is :, and with
ˆ
:=f?2
jˆ.?=.?gthe set of pixels that are true posi-
3186
tives for class :,i.e., the set of pixels whose ground truth la-
bel and predicted label are :. Finally, the square of a vector
refers to the element-wise operation (Hadamard product):
v2=E2
1  E2
=>(1)
Semantic Decoder . The aim of semantic segmentation
is to predict a categorical distribution over  classes for all
pixels in an image. We follow best practice and optimize it
with the weighted cross-entropy loss
Lsem= 1
j
jÕ
?2
l:t>
?log f¹f?º (2)
wherel:is a class-wise weight computed via the inverse
frequency of each class in the dataset, t2R, is a
one-hot encoded pixel-wise ground truth label, t?2R 
is a one-hot encoded pixel-wise ground truth label at pixel
?2
,findicates the softmax operation, and f?denotes
the pre-softmax feature predicted for pixel ?.
As mentioned above, we do not only want to perform
standard semantic segmentation but also build a class de-
scriptor to bring all pixels belonging to a certain class to-
wards a certain region in the feature space. To achieve this,
we accumulate the pre-softmax features, also called activa-
tion vectors, of all true positives for each class, where a true
positive is a pixel that is correctly segmented. With this, we
can store a running average class prototype, or mean activa-
tion vector, -:2R for each class :2f1 g:
-:=1
jˆ
:jÕ
?2ˆ
:f? (3)
We also iteratively compute the per-class variance
22
:2R via sum of squares, as
22
:=1
jˆ
:jÕ
?2ˆ
: f? -:2 (4)
At the beginning of epoch 4, we have the means -4 1
:
and variances 24 1
:accumulated in the previous epoch. At
epoch4, we can steer the semantic segmentation to predict,
for each pixel with ground truth class :, a feature vector
equal to -4 1
:. For this, we introduce a feature loss function
Lfeat=1
j
j Õ
:=1Õ
?2
:kf? -4 1
:k
24 1
: (5)
This loss function is not active during the ﬁrst epoch
since there is no accumulated mean yet. Thus, we perform
standard semantic segmentation during the ﬁrst epoch.
The semantic decoder is thus optimized with a weighted
sum of the loss functions introduced above
Lsdec=F1Lsem¸F2Lfeat (6)
A. Objectosphere
B. Contrastive C. Objectosphere + ContrastiveFigure 3. 2D visualization of the expected output of the contrastive
decoder. The behavior of the objectosphere loss is shown in A,
where all points coming from known classes (black) lie around the
red (outer) circle of radius b, see Eq. (9), and the points from un-
known classes lie around the origin. The contrastive loss is shown
in B, where features lie on the unit circle. Together, they lead to a
behavior similar to the one depicted in C.
Contrastive Decoder . The contrastive decoder explic-
itly aims for anomaly segmentation. Given an image of di-
mensions,, where known and unknown classes are
present, the goal of the contrastive decoder is to provide the
basis for a binary prediction where 0corresponds to known
classes and 1to unknown classes. We achieve this by means
of a combination between the contrastive loss [11] and the
objectosphere loss [15]. First, we compute the mean feature
representation f:for class:in the current image as
f:=1
j
:jÕ
?2
:f3
? (7)
where f3
?is the feature predicted at pixel ?from the con-
trastive decoder (the equivalent of f?for the semantic one).
Then, we compute the contrastive loss Lcontsuch that f:
approximates the normalized mean representation ¯-4 1
:of
the corresponding class in the previous epoch -4 1
:and gets
dissimilar from the other classes mean representation:
Lcont=  Õ
:=1logexp¹f:>¯-4 1
:gº
Í 
8=1exp¹f:>¯-4 1
8gº (8)
wheregis a temperature parameter. This way, the loss aims
to make the features from the same class consistent with its
running mean representation -4 1
:, while scattering all  
classes around the unit hypersphere.
At the same time, we use the objectosphere loss Lobj
3187
over each pixel ?2
given by
Lobj=max b kf?k20if?2D:
kf?k2otherwise(9)
whereD:is the set of pixels belonging to known classes.
The remaining pixels, at training time, reduce to the unla-
beled (void) areas of the image. This aims to make the norm
of the feature vector kf?kof pixels belonging to known
classesD:bigger than a threshold b, while the norm of the
features of pixels belonging to unknown classes DDgets re-
duced to 0. These two loss functions LcontandLobjtogether
allows us to optimize towards a situation where the feature
vectors of known classes are distributed along the surface of
the -dimensional hypersphere of radius b, while the fea-
ture vectors of unknown classes gets squashed to 0. A 2D
example of the expected behavior is shown in Fig. 3.
The contrastive decoder is optimized with a weighted
sum of the two losses given by
Lcdec=F3Lcont¸F4Lobj (10)
Post-Processing for Anomaly Segmentation . To obtain
the open-world predictions at test time, we fuse the outputs
of the two decoders. The semantic encoder provides a stan-
dard closed-world semantic segmentation but, thanks to the
loss function that operates directly on the feature space that
we introduced, we can obtain an open-world segmentation.
In fact, we computed mean -:2R and variance 22
:2R 
of each class, meaning that, for each class, we can easily
build a multi-variate normal distribution N -::, where
-:is the mean, and :=diag¹22
:ºis the covariance ma-
trix, which reduces to the diagonalization of the variance 22
:
under the assumption that all classes are independent. Af-
ter building the Gaussian model of each class in the dataset,
given a pixel ?whose predicted feature f?would corre-
spond to class :8:, we compute a ﬁtting score by means
of the squared exponential kernel
B:¹f?º=exp 
 1
2¹f? -:º> 1
:¹f? -:º!
(11)
Then, for each pixel, we take the highest score
B¹?º=max
:B:¹f?º (12)
and, if it is low, then the pixel of interest is in the tail of
the Gaussian, and is considered as a novel class, leading
to an open-world prediction Usemof the semantic decoder.
We can obtain a pixel-wise score Bsem
unk?for being unknown
Bsem
unk?=1 B¹?º
The contrastive decoder leads to a second open-world
predictionUcontby considering as unknown all pixelswhose feature norm is below a certain threshold. In par-
ticular, we can obtain a pixel-wise score Bcont
unk?for being
unknown
Bcont
unk?=max 
0 
1 kf?k2
b!!
 (13)
where f?is the predicted feature at pixel ?. This score is
1when the norm of the feature vector is 0, and 0when the
norm is bigger than b, as described in Eq. (9).
Finally, we fuse the two predictions to obtain a cumula-
tive pixel-wise score for being unknown as
Bunk?=1
2
Bsem
unk?¸Bcont
unk?
 (14)
IfBunk?is above a threshold X, the pixel is considered
belonging to an unknown class.
Post-Processing for Open-World Semantic Segmen-
tation . When a pixel is considered unknown, we need to
store its activation vector and decide whether it belongs to
an already-discovered class or a new one. Given the set of
mean activation vectors for unknown classes discovered
so farF=ff1
D f
Dg, we take the vector f6
Dthat mini-
mizes the distance from the querying vector. If the distance
between f6
Dandf?is below a threshold [, then the pixel
belongs to this class, and the mean activation vector gets
updated, otherwise it creates a new unknown class f6¸1
D.
This allows us to have a virtually unlimited number of novel
classes.
3.3. Class Similarity
As a byproduct of the open-world segmentation, our method
can also predict the most similar known category for each
unknown sample. As explained in Sec. 3.2, it does not suf-
ﬁce for a feature vector to have the highest activation in
the:-th spot for being matched to class :. A sample can
have the highest activation for a certain class :but its score
computed with Eq. (11) is higher for another class ˜:<:,
meaning that the sample is more inside the area of inﬂuence
of class ˜:despite having a higher activation on class :. As
the most similar class, we propose to choose the one that
provides the highest score given by ˜:=argmax:B:¹f?º
4. Experimental Evaluation
The main focus of this work is an approach for open-world
semantic segmentation that also provides a measure of class
similarity. We present experiments to show the capabilities
of our method. The results of our experiments support our
claims, which are: (i) our model achieves state-of-the-art
results for anomaly segmentation while performing com-
petitively on the known classes, (ii) our approach can dis-
tinguish between different unknown classes, and (iii) our
approach can provide a similarity score for each novel class
to the known ones.
3188
(a) Input RGB (b) Closed-world prediction (c) Open-world predictionFigure 4. Results from the validation set of SegmentMeIfYouCan. We show the input RGB overlayed with the ground truth unknown mask
(a), the prediction of our closed-world model (b), and the prediction of our approach for open-world segmentation (c). In the open-world
prediction, the unknown class is shown in red.
Table 1. Left. Comparison between closed-world and open-world model on the known classes of the training datasets. Our OW approach
does not harm closed-world semantic segmentation. Right . Results from the public leaderboard of the SegmentMeIfYouCan benchmark.
We separate methods that use external data, i.e. out of distribution (OoD) data with semantic labels different from the ones in Cityscapes [9],
during training. Our approach ranked overall top 1 for FPR95, PPV and mean F1, and top 6 for AUPR and sIoU (fourth and sixth,
respectively) on January 31st, 2024.
ApproachmIoU [%]"
CityScapes BDDAnomaly
CW 71.1 64.1
OW 70.8 62.8Approach OoDPixel-Level Component-Level
AUPR [%]"FPR95 [%]#sIoU gt [%]"PPV [%]"mean F1 [%]"
DenseHybrid [19] 3 78.0 9.8 54.2 24.1 31.1
RbA [44] 3 94.5 4.6 64.9 47.5 51.9
Maskomaly [1] 7 93.4 6.9 55.4 51.2 49.9
RbA [44] 7 86.1 15.9 56.3 41.4 42.0
ContMA V (ours) 7 90.2 3.8 54.5 61.9 63.6
4.1. Experimental Setup
We use two datasets for validating our method: Segment-
MeIfYouCan [9] and BDDAnomaly [23]. Since ground
truths are available for the test set of BDDAnomaly, we use
it for ablation studies and experiments on class similarity.
We evaluate our methods with the metrics proposed
in the SegmentMeIfYouCan public benchmark for pixel-
level performance: area under the precision-recall curve
(AUPR) and the false positive rate at a true positive rate
of95% (FPR95). For SegmentMeIfYouCan, we report
also component-level metrics provided by the benchmark.
As explained, our approach is not limited to anomaly seg-
mentation, but performs open-world semantic segmenta-
tion. Thus, we also report the mean intersection-over-
union (mIoU) on the known classes, to show that our open-
world segmentation approach does not underperform on the
known classes when compared to the closed-world equiva-
lent (see Tab. 1, left). Finally, we report the mIoU between
the newly-discovered classes and their respective highest-
overlapping ground truth class to be discovered.In all tables, we call our method “ContMA V”, where
“Cont” indicates the contrastive decoder and “MA V” the
mean activation vector of the semantic decoder.
Training details and parameters . In all experiments,
we use the one-cycle learning rate policy [55] with an ini-
tial learning rate of 0.004. We perform random scale, crop,
and ﬂip data augmentations, and optimize with Adam [26]
for 500 epochs with batch size 8. We set b=1,X=06,
g=01,[=05, and loss weights F1=09,F2=01,
F3=05, andF4=05. For SegmentMeIfYouCan, we train
only on Cityscapes. For BDDAnomaly, we train only on the
training set of BDDAnomaly itself.
4.2. Anomaly Segmentation
The ﬁrst set of experiments shows that our model achieves
state-of-the-art results in anomaly segmentation, and thus
also supports our ﬁrst claim. Here, we aim for a binary
segmentation between known classes and previously un-
seen classes. We report results on SegmentMeIfYouCan
in Tab. 1, right and BDDAnomaly in Tab. 2. On Seg-
mentMeIfYouCan, our method outperforms all baselines on
3189
Table 2. Anomaly segmentation results on BDDAnomaly.
Approach AUPR [%]"FPR95 [%]#
MaxSoftmax [22] 3.7 24.5
Background [6] 1.1 40.1
MC Dropout [16] 4.3 16.6
Conﬁdence [14] 3.9 24.5
MaxLogit [23] 5.4 14.0
ContMA V (ours) 96.1 6.9
FPR95 and ranks top 6 on the public leaderboard for AUPR.
On the BDD datasets, our method outperforms all baselines
on both metrics, providing compelling results for the task
of anomaly segmentation. For the BDD datasets, in this ex-
periment, we treat all the unknown categories as the same
unknown class, without focusing on the fact they are, origi-
nally, separate classes. Our approach shows compelling re-
sults for anomaly segmentation, successfully dealing with
challenging situations such as the case in which a known
and an unknown object are overlapping, see Fig. 4. While
SegmentMeIfYouCan is designed speciﬁcally for anomaly
segmentation, having images where the anomalous objects
are prominent, the BDD dataset is more challenging since
objects belonging to bicycle or motorcycle can appear in
very small areas of the image (see related ﬁgures in the sup-
plementary material), making the task of anomaly segmen-
tation more challenging and harder to solve.
4.3. Open-World Semantic Segmentation
The second experiment illustrates that our approach is ca-
pable of distinguishing between different unknown classes,
rather than only stating whether something is known or un-
known. We achieve this thanks to the feature loss function
we introduced in Eq. (7). We conduct this experiment on
BDDAnomaly since the test set is manually generated ex-
cluding images from the training and the validation set and
thus the ground truth labels are available. Our approach
is able to create multiple unknown classes, as explained
in Sec. 3.2. To evaluate it,for each novel class we create we
report the mIoU with the ground truth category that over-
laps the most to it. We report results for our method to-
gether with results we would achieve without the feature
loss function. Since this task is uncommon in the literature,
we report one baseline approach as a performance lower
bound, that uses the background class for the unknowns and
performs K-means clustering in the feature space for this
class. As a performance upper bound, we report the mIoU
of the three classes in closed-world settings on the origi-
nal BDD100K, where there is no unknown but every class
is present at training time. Results are shown in Tab. 3.
Our approach outperforms the baseline and provides satis-
fying results in distinguishing among different classes. Ad-
ditionally, removing the feature loss function also providesTable 3. Open-world semantic segmentation results on BD-
DAnomaly.
ApproachmIoU [%]"
Train Motorcycle Bicycle
Background + cluster 0 32.3 32.8
ContMA V (no feat loss) 48.1 53.8 39.9
ContMA V (with feat loss) 62.4 62.2 56.8
Closed-world 72.3 69.3 60.9
Table 4. Class similarity results on BDDAnomaly.
ApproachAccuracy [%]"
Motorcycle Train
Baseline 12.5 9.8
ContMA V with MA 39.9 27.6
ContMA V 58.9 49.9
good results for open-world segmentation, outperforming
the baseline by a large margin. Thus, this experiment pro-
vides support for our second claim.
4.4. Experiments on Class Similarity
The third experiment shows that our approach successfully
assigns to each novel class its most similar known category,
supporting our third claim. For this experiment, we man-
ually created a lookup table (see supplementary material
for further details) in which each class is assigned a ground
truth label indicating its most similar category. For this ex-
periment, we used the BDDAnomalydataset proposed by
Besnier et al. [4], that is a modiﬁcation of BDDAnomaly
where only train and motorcycle are unknown (we report
anomaly segmentation results on this dataset in the sup-
plement). In the lookup table, the unknown class “motor-
cycle” is reported as similar to “car”, while the unknown
class “train” is reported as similar to “truck”. We report one
baseline that performs semantic segmentation on the known
classes and has a stack of linear layers on the pre-softmax
features that learns the lookup table. We compare with our
same approach but taking the class that has the highest ac-
tivation as most similar. We report pixel-wise accuracy re-
sults in Tab. 4. The results show that the classiﬁer does not
generalize well to the unknown classes. Considering only
the highest activation is better than the “specialized” classi-
ﬁer, but still it is not a reliable measure of class similarity.
4.5. Ablation Studies
Finally, we provide ablation studies to investigate the con-
tribution of the modules we introduced. We refer to each
ablation study in the tables by the letter in the ﬁrst column.
Anomaly Segmentation . First, we perform an ablation
study on the anomaly segmentation pipeline ( Tab. 5). We
3190
Table 5. Ablation study on our anomaly segmentation pipeline on
BDDAnomaly.Lfeatrefers to the feature loss in Eq. (5), and cont
to the contrastive decoder. “PP” indicates the post-processing op-
eration used for obtaining the open-world prediction: “Th” for
softmax thresholding, “MA” for maximum activation, `for the
minimum distance from the mean activation vector, “Gs” for the
Gaussian inference described in Sec. 3.2.
BDDAnomaly
Lfeatcont PP AUPR [%]"FPR95 [%]#
A Th 46.9 93.9
B3 Th 76.4 88.6
C 3 Th 91.8 70.7
D3 3 Th 94.1 54.4
E3 MA 75.9 89.9
F3 3 MA 93.9 57.6
G 3 – 91.8 69.7
H3 ` 94.2 57.0
I3 3 ` 94.8 29.8
J3 Gs 94.2 55.8
K3 3 Gs 96.1 6.9
investigate the contribution of the feature loss Lfeat, of the
Gaussian post-processing described in Sec. 3.2, and of the
contrastive decoder. We ablate different post-processing
strategies. The ﬁrst strategy is a softmax thresholding strat-
egy where we consider a pixel as unknown if it has two or
more activations above a threshold. The second strategy is
based on the maximum softmax activation only and catego-
rizes a pixel as unknown if its maximum activation is below
a certain threshold. These two strategies yield similar per-
formance, which is an expected outcome since they both
rely on the standard ﬁnal output vector. In the table, we can
see that the thresholding strategy alone (A) has poor results,
and its performance with the feature loss (B) is close to the
performance of the maximum activation strategy with fea-
ture loss (E). Additionally, we notice how the thresholding
without the feature loss but with the contrastive decoder (C)
leads to better performance, that is however extremely sim-
ilar to the one of the contrastive decoder only (G), suggest-
ing that the contrastive decoder alone is better than a soft-
max thresholding strategy for this task. A further improve-
ment comes from putting together the feature loss and the
contrastive decoder, which leads to better results with both
thresholding (D) and maximum activation (F). The other
two post-processing strategies we employ are based on the
output of the feature loss. One takes the minimum distance
`of the activation vector from the mean activation vec-
tors we built during training, while the last one is the Gaus-
sian querying. They lead to similar performance when the
contrastive decoder is not used (H and J), and yield the top
2 performance when the contrastive decoder is used (I and
K). The Gaussian querying provides a further improvementTable 6. Ablation study on our class similarity approach on
BDDAnomaly.Lfeatrefers to the feature loss in Eq. (5),
andcontto the contrastive decoder. “PP” indicates the post-
processing operation used for obtaining the open-world predic-
tion: “MA” for maximum activation, `for the minimum dis-
tance from the mean activation vector, “Gs” for the Gaussian in-
ference described in Sec. 3.2.
Accuracy [%]"
Lfeatcont PP Motorcycle Train
L3 MA 38.4 25.9
M 3 3 MA 39.9 27.6
N 3 ` 53.5 41.7
O 3 3 ` 54.3 42.1
P3 Gs 57.8 48.6
Q 3 3 Gs 58.9 49.9
and achieves the best performance for this task.
Class Similarity . The second ablation study targets the
class similarity (Tab. 6). The presence of the contrastive
decoder does not substantially improve the performance,
since the class similarity originates from the semantic de-
coder. Still, numbers when the contrastive decoder is active
(M, O, Q) or inactive (L, N, P) are slightly different since
the contrastive decoder affects the shared encoder via back-
propagation. The performance of class similarity is poor
when we rely on the standard maximum activation (L and
M), while it improves when it is based on the minimum dis-
tance`of the activation vector from the mean activation
vectors built during training (N and O). The Gaussian post-
processing achieves the best performance for both classes
(P and Q), proving the effectiveness of our approach.
5. Conclusions
In this paper, we presented a novel approach for open-
world semantic segmentation on RGB images based on a
double decoder architecture. Our method manipulates the
feature space of the semantic segmentation for identifying
novel classes and additionally indicates the known cate-
gories that are most similar to the newly discovered ones.
We implemented and evaluated our approach on different
datasets and provided comparisons with other existing mod-
els and supported all claims made in this paper. The ex-
periments suggest that our double-decoder strategy achieves
compelling open-world segmentation results. In fact, with
our approach, we are able to detect all anomalous regions in
an image and distinguish between different novel classes.
Acknowledgments . This work has partially been funded by
the Deutsche Forschungsgemeinschaft (DFG, German Research
Foundation) under Germany’s Excellence Strategy, EXC-2070 –
390732324 – PhenoRob and by the European Union’s Horizon
2020 research and innovation programme under grant agreement
No 101017008 (Harmony).
3191
References
[1] Jan Ackermann, Christos Sakaridis, and Fisher Yu. Masko-
maly: Zero-shot mask anomaly segmentation. In Proc. of
British Machine Vision Conference (BMVC) , 2023. 6
[2] Abhijit Bendale and Terrance E. Boult. Towards open set
deep networks. In Proc. of the IEEE/CVF Conf. on Computer
Vision and Pattern Recognition (CVPR) , 2016. 2
[3] Paul Bergmann, Michael Fauser, David Sattlegger, and
Carsten Steger. Uninformed Students: Student-Teacher
Anomaly Detection With Discriminative Latent Embed-
dings. In Proc. of the IEEE/CVF Conf. on Computer Vision
and Pattern Recognition (CVPR) , 2020. 2
[4] Victor Besnier, Andrei Bursuc, David Picard, and Alexandre
Briot. Triggering Failures: Out-Of-Distribution detection by
learning from local adversarial attacks in Semantic Segmen-
tation. In Proc. of the IEEE/CVF Intl. Conf. on Computer
Vision (ICCV) , 2021. 7
[5] Petra Bevandi ´c, Ivan Kreso, Marin Orsi ´c, and Sinisa Segvi ´c.
Simultaneous semantic segmentation and outlier detection in
presence of domain shift. Pattern Recognition , 11824:33–47,
2019. 2
[6] Hermann Blum, Paul-Edouard Sarlin, Juan Nieto, Roland
Siegwart, and Cesar Cadena. The Fishyscapes Benchmark:
Measuring blind spots in semantic segmentation. Intl. Jour-
nal of Computer Vision (IJCV) , 129:3119–3135, 2021. 2, 7
[7] Shubhankar Borse, Ying Wang, Yizhe Zhang, and Fatih
Porikli. Inverseform: A loss function for structured
boundary-aware segmentation. In Proc. of the IEEE/CVF
Conf. on Computer Vision and Pattern Recognition (CVPR) ,
2021. 2
[8] Douglas O. Cardoso, Jo ˜ao Gama, and Felipe M.G. Franc ¸a.
Weightless neural networks for open set recognition. Ma-
chine Learning , 106(9-10):1547–1567, 2017. 2
[9] Robin Chan, Krzysztof Lis, Svenja Uhlemeyer, Hermann
Blum, Sina Honari, Roland Siegwart, Pascal Fua, Mathieu
Salzmann, and Matthias Rottmann. SegmentMeIfYouCan:
A Benchmark for Anomaly Segmentation. In Proc. of the
Conf. on Neural Information Processing Systems (NeurIPS) ,
2021. 2, 6
[10] Robin Chan, Matthias Rottmann, and Hanno Gottschalk.
Entropy maximization and meta classiﬁcation for out-of-
distribution detection in semantic segmentation. In Proc. of
the IEEE/CVF Conf. on Computer Vision and Pattern Recog-
nition (CVPR) , 2021. 2
[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A Simple Framework for Contrastive Learn-
ing of Visual Representations. In Proc. of the Intl. Conf. on
Machine Learning (ICML) , 2020. 3, 4
[12] Thomas A. Ciarfuglia, Ionut M. Motoi, Leonardo Saraceni,
Mulham Fawakherji, Alberto Sanfeliu, and Daniele Nardi.
Weakly and semi-supervised detection, segmentation and
tracking of table grapes with limited and noisy data. Com-
puters and Electronics in Agriculture , 205:107624, 2023. 2
[13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The Cityscapes
dataset for semantic urban scene understanding. In Proc. ofthe IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) , 2016. 1
[14] Terrance DeVries and Graham W. Taylor. Learning conﬁ-
dence for out-of-distribution detection in neural networks.
arXiv preprint , arXiv:1802.04865, 2018. 7
[15] Akshay Raj Dhamija, Manuel G ¨unther, and Terrance Boult.
Reducing network agnostophobia. In Proc. of the Conf. on
Neural Information Processing Systems (NeurIPS) , 2018. 2,
3, 4
[16] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian
Approximation: Representing model uncertainty in deep
learning. In Proc. of the Intl. Conf. on Machine Learning
(ICML) , 2016. 2, 7
[17] Ross Girshick. Fast R-CNN. In Proc. of the IEEE/CVF
Intl. Conf. on Computer Vision (ICCV) , 2015. 1
[18] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proc. of the IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR) , 2014. 1
[19] Matej Grci ´c, Petra Bevandi ´c, and Sinisa ´Segvi ´c. Densehy-
brid: Hybrid anomaly detection for dense open-set recog-
nition. In Proc. of the Europ. Conf. on Computer Vision
(ECCV) , 2022. 6
[20] Kai Han, Andrea Vedaldi, and Andrew Zisserman. Learning
to discover novel visual categories via deep transfer cluster-
ing. In Proc. of the IEEE/CVF Conf. on Computer Vision and
Pattern Recognition (CVPR) , 2019. 2
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proc. of
the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) , 2016. 3
[22] Dan Hendrycks and Kevin Gimpel. A baseline for detect-
ing misclassiﬁed and out-of-distribution examples in neural
networks. In Proc. of the Intl. Conf. on Learning Represen-
tations (ICLR) , 2017. 2, 7
[23] Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou,
Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and
Dawn Song. Scaling out-of-distribution detection for real-
world settings. Proc. of the Intl. Conf. on Machine Learning
(ICML) , 2022. 6, 7
[24] Wenbo Hu, Hengshuang Zhao, Li Jiang, Jiaya Jia, and Tien-
Tsin Wong. Bidirectional projection network for cross di-
mension scene understanding. In Proc. of the IEEE/CVF
Conf. on Computer Vision and Pattern Recognition (CVPR) ,
2021. 2
[25] Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang,
Avinash Ravichandran, and Onkar Dabeer. WinCLIP: Zero-
/Few-Shot Anomaly Classiﬁcation and Segmentation. In
Proc. of the IEEE/CVF Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2023. 2
[26] Diederik P. Kingma and Jimmy Ba. Adam: A Method for
Stochastic Optimization. In Proc. of the Intl. Conf. on Learn-
ing Representations (ICLR) , 2015. 6
[27] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr
Doll´ar. Panoptic Feature Pyramid Networks. In Proc. of the
IEEE/CVF Conf. on Computer Vision and Pattern Recogni-
tion (CVPR) , 2019. 1
3192
[28] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll ´ar. Panoptic Segmentation. In Proc. of
the IEEE/CVF Conf. on Computer Vision and Pattern Recog-
nition (CVPR) , 2019. 1
[29] Shu Kong and Deva Ramanan. Opengan: Open-set recog-
nition via open data generation. In Proc. of the IEEE/CVF
Conf. on Computer Vision and Pattern Recognition (CVPR) ,
2021. 2
[30] Abhijit Kundu, Xiaoqi Yin, Alireza Fathi, David Ross, Brian
Brewington, Thomas Funkhouser, and Caroline Pantofaru.
Virtual multi-view fusion for 3d semantic segmentation. In
Proc. of the Europ. Conf. on Computer Vision (ECCV) , 2020.
2
[31] Balaji Lakshminarayanan, Alexander Pritzel, and Charles
Blundell. Simple and scalable predictive uncertainty esti-
mation using deep ensembles. Proc. of the Conf. on Neural
Information Processing Systems (NeurIPS) , 2017. 2
[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C. Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
Proc. of the Europ. Conf. on Computer Vision (ECCV) , 2014.
1
[33] Krzysztof Lis, Sina Honari, Pascal Fua, and Mathieu
Salzmann. Detecting road obstacles by erasing them.
IEEE Trans. on Pattern Analysis and Machine Intelligence
(TPAMI) , 2024. 2
[34] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Fu-
ture Frame Prediction for Anomaly Detection – A New Base-
line. In Proc. of the IEEE/CVF Conf. on Computer Vision
and Pattern Recognition (CVPR) , 2018. 2
[35] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li.
Energy-based out-of-distribution detection. In Proc. of the
Conf. on Neural Information Processing Systems (NeurIPS) ,
2020. 2
[36] Wenrui Liu, Hong Chang, Bingpeng Ma, Shiguang Shan,
and Xilin Chen. Diversity-Measurable Anomaly Detection.
InProc. of the IEEE/CVF Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2023. 2
[37] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
Convolutional Networks for Semantic Segmentation. In
Proc. of the IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR) , 2015. 1
[38] Kira Maag and Tobias Riedlinger. Pixel-wise gradi-
ent uncertainty for convolutional neural networks ap-
plied to out-of-distribution segmentation. arXiv preprint ,
arXiv:2303.06920, 2023. 2
[39] Elias Marks, Matteo Sodano, Federico Magistri, Louis Wies-
mann, Dhagash Desai, Rodrigo Marcuzzi, Jens Behley, and
Cyrill Stachniss. High precision leaf instance segmentation
for phenotyping in point clouds obtained under real ﬁeld
conditions. IEEE Robotics and Automation Letters (RA-L) ,
2023. 1
[40] Andres Milioto and Cyrill Stachniss. Bonnet: An Open-
Source Training and Deployment Framework for Semantic
Segmentation in Robotics using CNNs. In Proc. of the IEEE
Intl. Conf. on Robotics & Automation (ICRA) , 2019. 2
[41] Andres Milioto, Philipp Lottes, and Cyrill Stachniss. Real-
time Semantic Segmentation of Crop and Weed for PrecisionAgriculture Robots Leveraging Background Knowledge in
CNNs. In Proc. of the IEEE Intl. Conf. on Robotics & Au-
tomation (ICRA) , 2018. 2
[42] Andres Milioto, Leonard Mandtler, and Cyrill Stachniss.
Fast Instance and Semantic Segmentation Exploiting Local
Connectivity, Metric Learning, and One-Shot Detection for
Robotics. In Proc. of the IEEE Intl. Conf. on Robotics &
Automation (ICRA) , 2019. 2
[43] Noam Mor and Lior Wolf. Conﬁdence prediction for
lexicon-free ocr. In Proc. of the IEEE Winter Conf. on Appli-
cations of Computer Vision (WACV) , 2018. 2
[44] Nazir Nayal, Mısra Yavuz, Jo ˜ao F. Henriques, and Fatma
G¨uney. Segmenting unknown regions rejected by all. In
Proc. of the IEEE/CVF Intl. Conf. on Computer Vision
(ICCV) , 2023. 6
[45] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural
networks are easily fooled: High conﬁdence predictions for
unrecognizable images. In Proc. of the IEEE/CVF Conf. on
Computer Vision and Pattern Recognition (CVPR) , 2015. 2
[46] Marin Orsi ´c, Ivan Kreso, Petra Bevandi ´c, and Sinisa Segvi ´c.
In Defense of Pre-Trained ImageNet Architectures for Real-
Time Semantic Segmentation of Road-Driving Images. In
Proc. of the IEEE/CVF Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2019. 3
[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In Proc. of the
Intl. Conf. on Machine Learning (ICML) , 2021. 2
[48] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong
Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen
Lu. DenseCLIP: Language-Guided Dense Prediction With
Context-Aware Prompting. In Proc. of the IEEE/CVF
Conf. on Computer Vision and Pattern Recognition (CVPR) ,
2022. 2
[49] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. In Proc. of the Advances in Neural
Information Processing Systems (NIPS) , 2015. 1
[50] Gianmarco Roggiolani, Matteo Sodano, Tiziano
Guadagnino, Federico Magistri, Jens Behley, and Cyrill
Stachniss. Hierarchical Approach for Joint Semantic, Plant
Instance, and Leaf Instance Segmentation in the Agricultural
Domain. Proc. of the IEEE Intl. Conf. on Robotics &
Automation (ICRA) , 2023. 2
[51] Eduardo Romera, Jos ´e M. Alvarez, Luis M. Bergasa, and
Roberto Arroyo. Erfnet: Efﬁcient residual factorized con-
vnet for real-time semantic segmentation. IEEE Trans. on
Intelligent Transportation Systems (T-ITS) , 19(1):263–272,
2018. 3
[52] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard
Sch¨olkopf, Thomas Brox, and Peter Gehler. Towards To-
tal Recall in Industrial Anomaly Detection. In Proc. of the
IEEE/CVF Conf. on Computer Vision and Pattern Recogni-
tion (CVPR) , 2022. 2
[53] Stuart J. Russell. Artiﬁcial intelligence a modern approach .
Pearson Education, Inc., 2010. 1
3193
[54] Hitesh Sapkota and Qi Yu. Bayesian Nonparametric Sub-
modular Video Partition for Robust Anomaly Detection. In
Proc. of the IEEE/CVF Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2022. 2
[55] Leslie N. Smith and Nicholay Topin. Super-convergence:
Very fast training of neural networks using large learning
rates. Artiﬁcial Intelligence and Machine Learning for Multi-
Domain Operations Applications , 11006:369–386, 2019. 6
[56] Matteo Sodano, Federico Magistri, Tiziano Guadagnino,
Jens Behley, and Cyrill Stachniss. Robust Double-Encoder
Network for RGB-D Panoptic Segmentation. Proc. of the
IEEE Intl. Conf. on Robotics & Automation (ICRA) , 2023. 2
[57] Shengyang Sun and Xiaojin Gong. Hierarchical Semantic
Contrast for Scene-Aware Video Anomaly Detection. In
Proc. of the IEEE/CVF Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2023. 2
[58] Tatiana Tommasi, Novi Patricia, Barbara Caputo, and Tinne
Tuytelaars. A deeper look at dataset bias. Domain Adap-
tation in Computer Vision Applications , pages 37–55, 2017.
2
[59] Chin-Chia Tsai, Tsung-Hsuan Wu, and Shang-Hong Lai.
Multi-scale patch-based representation learning for image
anomaly detection and segmentation. In Proc. of the IEEE
Winter Conf. on Applications of Computer Vision (WACV) ,
2022. 2
[60] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisser-
man. Open-set recognition: A good closed-set classiﬁer is
all you need. In Proc. of the Intl. Conf. on Learning Repre-
sentations (ICLR) , 2021. 2
[61] Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar
Das, Bharat Kaul, and Theodore L Willke. Out-of-
distribution detection using an ensemble of self supervised
leave-out classiﬁers. In Proc. of the Europ. Conf. on Com-
puter Vision (ECCV) , 2018. 2
[62] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang,
Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu,
Hongsheng Li, et al. Internimage: Exploring large-scale vi-
sion foundation models with deformable convolutions. In
Proc. of the IEEE/CVF Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2023. 2
[63] Jan Weyler, Federico Magistri, Elias Marks, Yue Linn
Chong, Matteo Sodano, Gianmarco Roggiolani, Nived Che-
brolu, Cyrill Stachniss, and Jens Behley. PhenoBench–
A Large Dataset and Benchmarks for Semantic Image In-
terpretation in the Agricultural Domain. arXiv preprint ,
arXiv:2306.04557, 2023. 2
[64] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,
Jose M Alvarez, and Ping Luo. Segformer: Simple and efﬁ-
cient design for semantic segmentation with transformers. In
Proc. of the Conf. on Neural Information Processing Systems
(NeurIPS) , 2021. 1
[65] Zhiwei Yang, Jing Liu, Zhaoyang Wu, Peng Wu, and Xi-
aotao Liu. Video Event Restoration Based on Keyframes
for Video Anomaly Detection. In Proc. of the IEEE/CVF
Conf. on Computer Vision and Pattern Recognition (CVPR) ,
2023. 2
[66] Xincheng Yao, Ruoqi Li, Jing Zhang, Jun Sun, and
Chongyang Zhang. Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for Supervised Anomaly Detec-
tion. In Proc. of the IEEE/CVF Conf. on Computer Vision
and Pattern Recognition (CVPR) , 2023. 2
[67] Vitjan Zavrtanik, Matej Kristan, and Danijel Sko ´caj. Draem-
a discriminatively trained reconstruction embedding for
surface anomaly detection. In Proc. of the IEEE/CVF
Intl. Conf. on Computer Vision (ICCV) , 2021. 2
[68] Chen Zhang, Guorong Li, Yuankai Qi, Shuhui Wang, Laiyun
Qing, Qingming Huang, and Ming-Hsuan Yang. Exploit-
ing Completeness and Uncertainty of Pseudo Labels for
Weakly Supervised Video Anomaly Detection. In Proc. of
the IEEE/CVF Conf. on Computer Vision and Pattern Recog-
nition (CVPR) , 2023. 2
[69] Xuan Zhang, Shiyu Li, Xi Li, Ping Huang, Jiulong Shan,
and Ting Chen. DeSTSeg: Segmentation Guided Denois-
ing Student-Teacher for Anomaly Detection. In Proc. of the
IEEE/CVF Conf. on Computer Vision and Pattern Recogni-
tion (CVPR) , 2023. 2
[70] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid Scene Parsing Network. In
Proc. of the IEEE/CVF Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2017. 3
[71] Ying Zhao. OmniAL: A Uniﬁed CNN Framework for Unsu-
pervised Anomaly Localization. In Proc. of the IEEE/CVF
Conf. on Computer Vision and Pattern Recognition (CVPR) ,
2023. 2
[72] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan
Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang
Dai, Lu Yuan, Yin Li, and Jianfeng Gao. RegionCLIP:
Region-Based Language-Image Pretraining. In Proc. of the
IEEE/CVF Conf. on Computer Vision and Pattern Recogni-
tion (CVPR) , 2022. 2
3194
