MELFUSION : Synthesizing M usic from Image and L anguage Cues using
Diffusion Models
Sanjoy Chowdhury∗1,3†Sayan Nag∗2,3†K J Joseph3
Balaji Vasan Srinivasan3Dinesh Manocha1
1University of Maryland, College Park2University of Toronto3Adobe Research
{sanjoyc,dmanocha }@umd.edu sayan.nag@mail.utoronto.ca {josephkj,balsrini }@adobe.com
Project page - https://schowdhury671.github.io/melfusion_cvpr2024/
Abstract
Music is a universal language that can communicate
emotions and feelings. It forms an essential part of the
whole spectrum of creative media, ranging from movies to
social media posts. Machine learning models that can syn-
thesize music are predominantly conditioned on textual de-
scriptions of it. Inspired by how musicians compose music
not just from a movie script, but also through visualizations,
we propose MELFUSION , a model that can effectively use
cues from a textual description and the corresponding im-
age to synthesize music. MELFUSION is a text-to-music
diffusion model with a novel “visual synapse”, which effec-
tively infuses the semantics from the visual modality into the
generated music. To facilitate research in this area, we in-
troduce a new dataset MeLBench, and propose a new eval-
uation metric IMSM. Our exhaustive experimental evalua-
tion suggests that adding visual information to the music
synthesis pipeline significantly improves the quality of gen-
erated music, measured both objectively and subjectively,
with a relative gain of up to 67.98% on the FAD score. We
hope that our work will gather attention to this pragmatic,
yet relatively under-explored research area.
1. Introduction
Music is an essential tool for creative professionals and con-
tent creators. It can complement and set the mood for an
accompanying still image, animation, video, or even text
descriptions while creating a social media post. Finding
music that matches a specific setting, can indeed be an ar-
duous task. A conditional music generation approach, that
can synthesize a music track by analyzing the visual content
and the textual description can find a wide range of practical
∗Equal contribution.
†Work done during internship at Adobe Research.
M E LF USION
(Ours)
Long Caption
A tranquil  scenery is captured with the view of the
night sky just before sunrise. The sky consists of
dynamic spiralling clouds which symbolizes
movement  and aliveness . The stars are bright and
prominent with strokes of yellows and whites
represent a vivid yet peaceful moment. The village
in the scene has houses whose windows emit warm
and glowing light, giving a contrast to the cool,
celestial  tones of sky depicting pleasant  emotions.OVL: 83.86Caption
Generator
A soft musical track of folk
acoustic genre played on violin. OVL: 88.45
We introduce a new
problem setting  and an
approach  to synthesize
music, conditioned on both
visual and textual
modalities .
A soft musical track of folk acoustic genre played
on violin.An
Alternate
ApproachText-to-MusicFigure 1. We present M ELFUSION , a music diffusion model
equipped with a novel “visual synapse”, that can effectively in-
fuse image semantics into a text-to-music diffusion model. This
task indeed requires a detailed understanding of the concepts in
the image. An alternate approach like using a caption generator to
convert image to text space to be further used with existing text-to-
music methods leads to a sub-optimal overall audio quality (OVL)
score. Our approach can knit together complementary information
from both modalities to synthesize high-quality music.
applications in various fields including social media.
Inspired by the progress in generative modeling of im-
ages, music generation has also garnered significant atten-
tion from the community [1, 30, 42]. Recently, Agostinelli
et al. [1], Copet et al. [4] proposed conditioning in the form
of melody or humming. While Sheffer and Adi [43] pursue
image-guided audio generation. Despite these efforts, mu-
sic generation conditioned on multiple modalities like text
and image, is largely uncharted.
Images are more expressive [13] than text-only infor-
mation and capture more fine-grained semantic information
about various visual aspects. For example, as depicted in
Fig 1, to generate a musical track that goes well with a
given image, without indeed using it, one has to make the
tedious effort of producing long, descriptive captions (either
generated by an image captioning model or human annota-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
26826
tors) before employing a typical text-to-music generation
model. Moreover, the model has to be supplied with critical
attributes like ‘tranquil’ ,‘aliveness’ etc (highlighted in fig-
ure) to aptly capture the essence of the image. This poses
a major bottleneck in the scalability of such systems espe-
cially for social media content creators and necessitates di-
rect image conditioning with textual control in music gen-
eration.
Music is indeed different from generic audio. Music con-
tains an arrangement of elements structured to form a coher-
ent and complete entity. These musical elements include
melody, harmony, rhythm, dynamics, and form [41, 45].
Unlike audio, music contains harmonies from different in-
struments forming intricate structures. Prior studies show
[6, 11, 18, 36, 48, 52] that the human brain is extremely
sensitive to disharmony. As a result, the margin of error
especially in producing musical pieces is low compared to
generic audio tracks. This makes music generation a harder
task as the model should be equipped to control the fine-
grained nuances of a composition involving melody, the in-
terplay of the instruments, and genre.
An alternative to generating music would be to retrieve
them. Retrieval-based systems [20, 33] struggle to ‘match’
the right track for a given input prompt thereby limiting
their practical applicability in open-world scenarios primar-
ily because (a) they tend to search from a pre-existing col-
lection of tracks and (b) finding the correct association be-
tween the input prompt and the audio track can be challeng-
ing. The problem is inherently complex due to the mul-
tifaceted nature of music and the abstract associations be-
tween auditory experiences and other sensory modalities.
To overcome these shortcomings, we introduce the first
music generation model that can be conditioned on image
and text instruction. We observe that the features from a
pre-trained text-to-image diffusion model that consumes the
DDIM-inverted latent of the image can guide a text-to-audio
diffusion model. Our key novelty is to facilitate this infor-
mation exchange by incorporating a “visual synapse” to the
text-to-music model, which includes a set of parameters that
learn to combine the signals from both modalities.
We summarise our main contributions below:
(1)We formalize a novel task of generating music that
is consistent with a reference image and an associated text
prompt.
(2)We present M ELFUSION , a novel diffusion model
that can address this pragmatic task.
(3)We introduce MeLBench dataset comprising 11,250
⟨image ,text,music⟩triplets. To the best of our knowledge,
this is the largest collection of these three modalities. Fur-
ther, we extend the MusicCaps [1] dataset by supplement-
ing the text, and music pairs with suitable images extracted
from corresponding YouTube videos or the web.
(4)In order to quantitatively establish the correspon-dence between the image-music pairs we propose a new
metric IMSM. We demonstrate that the score follows hu-
man perception closely, through a user study.
(5)Finally, our exhaustive experimental results reveal
that our approach outperforms existing text-to-music gener-
ation pipelines on both subjective as well as objective eval-
uation with a relative gain of up to 67.98% on FAD score,
thereby setting a new benchmark for multi-modal music
synthesis.
2. Related Works
Music Generation Approaches: Music generation has
garnered significant attention for a considerable amount of
time. While some approaches [8, 35, 50] deploy GANs to
tackle this task, Ycart et al. [51] introduced recurrent neural
networks to model polyphonic music. Bassan et al. [2] pro-
posed an unsupervised segmentation using ensemble tem-
poral prediction errors. Jukebox [7] tackles the long context
of raw audio using a multiscale VQ-V AE to compress it to
discrete codes, modeling those using autoregressive Trans-
formers. Another stream of work [14, 50] that predicts the
MIDI notes to produce music has gained popularity in this
space. However, the scope of these approaches is relatively
limited as they need additional decoders to produce the mu-
sical pieces from the notations.
MusicLM [1] generates high-fidelity music from text
descriptions by casting the process of conditional music
generation as a hierarchical sequence-to-sequence model-
ing task. Mubert [34] is an API-based service that employs
a Transformer backbone. The encoded prompt is used to
match the music tags and the one with the highest similar-
ity is used to query the audio generation API. MusicGen [4]
comprises a single-stage transformer LM together with ef-
ficient token interleaving patterns. This eliminates the need
for hierarchical upsampling. Despite significant progress,
none of these approaches utilize the semantic information
of images to condition the audio generation.
Diffusion Models for Music Generation: With the pro-
lific success of diffusion models in conditional image gen-
eration, there have been recent efforts in music generation
using them. Riffusion [12] base their algorithm on fine-
tuning a stable diffusion model [39] on mel-spectrograms
of music pieces from a paired music-text dataset. This is
one of the first text-to-music generation methods. Mo ˆusai
[42] is a cascading two-stage latent diffusion model that is
equipped to produce long-duration high-quality stereo mu-
sic. Noise2Music [22] introduced a series of diffusion mod-
els, a generator, and a cascade model. The former generates
an intermediate representation conditioned on text, while
the later can produce audio conditioned on the intermediate
representation of the text. MeLoDy [30] pursues an LM-
guided diffusion model by reducing the forward pass bot-
tleneck and applies a novel dual-path diffusion mode. We
26827
find that the visual guidance that is incorporated into our ap-
proach significantly enhances the music generation quality
when compared to all these approaches. We elaborate this
further in Sec. 4.4.
Diffusion Models for Audio Generation: Diffusion-based
methods [23–25, 29, 31, 37] achieve remarkable results in
speech synthesis too. FastDiff [23] deploys time-aware
location-variable convolutions of diverse receptive field pat-
terns to efficiently model long-term time dependencies with
adaptive conditions. AudioLDM [32] is a text-to-audio sys-
tem that is built on a latent space to learn continuous audio
representations from contrastive language-audio pretrain-
ing (CLAP) embeddings. Ghosal et al. [17] simplifies the
architecture of AudioLDM, and uses FLAN-T5 [3] as the
text encoder. Another line of work [15, 49] involves text-
conditional discrete diffusion models to generate discrete
tokens as a representation for spectrograms. However, the
quality of the sound produced by such methods leaves room
for improvements in terms of both subjective and objective
qualities, thereby limiting their practical usability. In con-
trast to these approaches, our method generates music sam-
ples conditioned on visual and textual signals.
3. Synthesizing Music from Image and Text
We propose to learn a conditional distribution M(w|I,Y),
that can generate music waveforms wfrom an image Iand
a paired textual description Y. We materialize Mas M EL-
FUSION , a diffusion model that can succinctly interleave the
semantic cues from the image and textual modality while
generating acoustically pleasing music.
Fig. 2 provides an overview of our approach. On a
high level, our novel methodology consists of two sub-
components: 1) an approach to extract relevant visual in-
formation from the image conditioning Iand 2) a method
to induce this conditioning into the text-to-music generative
model, in a parameter efficient way. We describe each of
these in the subsequent subsections.
3.1. Extracting Visual Guidance
Latent diffusion models (LDMs) for text-to-image genera-
tion [39] have had phenomenal success in generating high-
quality images that are well-grounded in their textual condi-
tioning. We hypothesize that the latent representations and
their transformations encode rich semantic knowledge, that
can guide our audio diffusion model. In our exploration,
we make use of a pre-trained Stable Diffusion model [39].
It contains a VQ-V AE [46] for encoding and decoding the
image to the latent space, a text encoder, and a UNet [40]
that carries out the diffusion process on the latent. The
UNet contains an encoder, a bottleneck layer, and a decoder.
Each encoder and the decoder further contain a set of blocks
with cross-attention layers, self-attention layers, and convo-
lutional layers. Given any intermediate latent image featuref∈R(w×h)×d, a single self-attention [47] operation con-
sist of Q=Wqf,K=Wkf,V=Wvf:
Attention (Q,K,V) =SoftmaxQKT
√dk
V,(1)
where dkis the dimension of the query and key features.
During cross-attention, the key and value matrices operate
on the external text conditioning c∈Rs×dk:K=Wkc,
V=Wvc. Here, Wq,WkandWvare the attention
weight matrices that transform either the image features or
text conditions into the output of each block.
We want to transfer over the semantic information that
is present within these attention layers corresponding to the
image Iinto the music LDM. For this, we first invert I
into the latent space using DDIM Inversion [44] to get zI
T.
This will guarantee that we will be able to generate Ifrom
zI
T. Next, we do the reverse diffusion steps using a pre-
trained text-to-image LDM starting from zI
Tand save the
self-attention features K=Wkf,V=Wvf, to be in-
jected into the music LDM. The intuition behind leverag-
ing the self-attention features is that they control the fea-
ture transformations responsible for generating the visual
semantics of the image. This is mathematically evident
from Eq. (1). In the subsequent section, we elaborate on
how we construct the “synapse” that can transfer the guid-
ance information from Ito the music-diffusion model.
3.2. Text-to-Music LDM with Visual Synapse
Inspired by recent text-to-audio [17, 32] generation ap-
proaches, our text-to-music model is also formulated as a la-
tent diffusion model. During training, the music waveform
wis first converted to a spectrogram s∈RE×F, which is
a visual representation obtained via Fourier Transformation
onw.EandFdenote the number of time slots and fre-
quency slots respectively. Then we encode Susing Audio-
V AE [32] to get a latent representation zM
1∈RC×E/r×F/r,
where Cis the number of channels and ris the compression
level.
The forward diffusion process involves corrupting zM
1
using a Markovian noise process q, which gradually adds
noise to zM
1through zM
ToverTsteps with the following
Gaussian function:
q(zM
t|zM
t−1) =N(zM
t;p
1−βtzM
t−1, βtI), (2)
where βtis a predetermined variance schedule. This itera-
tive sampling process can be approximated by a determin-
istic non-Markovian process as follows [44]:
q(zM
t|zM
1) =N(zM
t;√¯γtzM
1,(1−¯γt)I) (3)
=√¯γtzM
1+ϵp
(1−¯γt), ϵ∼ N(0,I)(4)
where γt= 1−βtand¯γt=Qt
r=0γr.
26828
DDIM
Inversion
Music
EncoderMusic
Decoder❆ ❆
A song of classical
genre played on violin❆
Text-to-Image LDM
Text-to-Music LDM
❆
VocoderSelf-Attention
Residual BlockCross-Attention
Self-AttentionResidual Block
Cross-AttentionTrain OnlyTrain + InferenceTrainable ❆Frozen
A female voice is singing to a digital
drum set along with a bass and old
disco synthesizer sounds. The singer
gets backed by a male singer and the
crowd is singing along. There is
clapping from the crowd. This song
may be playing live at a concert.
A live music festivalA live music festivalText Only
Model
Text Only
Model
VisualBeats
[OVL: 20.1]
[OVL: 25.1]
[OVL: 25.8]
Text
EncoderSA feats from
layer ofCA feats from
layer of
CA Cross-AttnSA Self-AttnFigure 2. Our approach MELFUSION generates music waveform wconditioned on an image Iand a given textual instruction Y. Visual
semantics from Iis instilled into a text-to-music diffusion model (bottom green box) using a pre-trained and frozen text-to-image diffusion
model (top blue box). The image Iis first DDIM inverted into a noisy latent zI
T. The self-attention features from the decoder layers of
the text-to-image LDM that consumes zI
Tis infused into the cross-attention features of text-to-music LDM decoder layers, modulated by
learned αparameters. This fusion operation that happens in the decoder (green stripes) is detailed on the right side of the figure. The music
encoder projects the spectrogram representation of the music to the latent space, and the music decoder retrieves back the spectrograms.
Finally, a vocoder generates the waveform wfrom the spectrograms. Please refer to Sec. 3 for more details.
In the reverse diffusion process, an LDM ϵθ(·,·,·)(im-
plemented as a UNet), learns to de-noise zM
T∼ N(0,I)to
recover zM
1. The architecture of the UNet is kept exactly
similar to the text-to-image UNet described in Sec. 3.1. To
incorporate the additional guidance from image condition-
ing the cross-attention key and value features KM
landVM
l
in each of the decoder layer lof the UNet is modified as fol-
lows:
KM
l=αlKI
l+ (1−αl)KM
l (5)
VM
l=αlVI
l+ (1−αl)VM
l, (6)
where KI
landVI
lare the self-attention features for the cor-
responding layer lof the image conditioning LDM from
Sec. 3.1. Most importantly, the convex combination be-
tween these features is modulated by learned layer spe-
cificαparameters . We find that this simple formulation
elegantly incorporates the image guidance into the text-to-
music diffusion model without hampering its expressivity.
As the αparameters facilitate the information exchange be-
tween the text-to-audio and text-to-image diffusion models,
analogous to how a synapse in a nervous system facilitates
the transfer of electrical and chemical signals between neu-
rons, we refer to this handshake as the visual synapse of a
text-to-music LDM .
Finally, the parameters of the LDM θand the αparame-
ters are trained end-to-end with the following loss function:Algorithm 1 MELFUSION : Training
Input: Image: I; Text: Y; Music: M; Pre-trained Text-to-
Image LDM: ϵψ(·,·,·); Image Encoder: EI(·); Music En-
coder: EM(·); Text Encoder: TM(·); Text-to-Music LDM:
ϵθ(·,·,·); Number of Diffusion Steps: T.
Output: Trained Text-to-Music LDM: ϵθ(·,·,·), Learned mixing
coefficient α, for each decoder layer lof LDM: {αl}.
1:zI
T←DDIM Invert (EI(I)) ▷Initialize Image Latent.
2:{ϵM
1,···ϵM
T} ← Forward Diffusion (EM(M)) ▷Targets.
3:zM
T∼ N (0,I) ▷Initialize Music Latent.
4:c← TM(Y) ▷Encoding Text.
5:fort∈ {T,···,1}do ▷For each denoising step.
6: foreach layer lin decoder of LDM do
7: KI
l,VI
l←Self-attention features of ϵψ(zI
t,∅, t).
8: KM
l,VM
l←Cross-attention features of ϵθ(zM
t,c, t).
9: KM
l←αlKI
l+ (1−αl)KM
l ▷Key update.
10: VM
l←αlVI
l+ (1−αl)VM
l ▷Value update.
11: L ← || ϵM
t−ϵθ(zM
t,c, t)||2▷Eq.(7)
12: Optimize θand all αparameters to reduce L.
13:return ϵθ(·,·,·),{αl}.
L=Et∼[1,T],zM
1,ϵM
t∼N(0,I)∥ϵM
t−ϵθ(zM
t,c, t)∥2(7)
3.3. Overall Framework
We summarize the overall flow of M ELFUSION during
training in Algorithm 1. Our key novelty is to introduce a
26829
Algorithm 2 MELFUSION : Sampling
Input: Image: I; Text: Y; Pre-trained Text-to-Image LDM:
ϵψ(·,·,·); Image Encoder: EI(·); Text Encoder: TM(·);
Trained Text-to-Music LDM: ϵθ(·,·,·); Learned mixing co-
efficient α, for each decoder layer lof LDM: {αl}; Number
of Diffusion Steps: T; Music Decoder: DM(·); V ocoder V(·).
Output: Music Waveform: w
1:zI
T←DDIM Invert (EI(I)) ▷Initialize Image Latent.
2:zM
T∼ N (0,I) ▷Initialize Music Latent.
3:c← TM(Y) ▷Encoding Text.
4:fort∈ {T,···,1}do ▷For each denoising step.
5: foreach layer lin decoder of LDM do
6: KI
l,VI
l←Self-attention features of ϵψ(zI
t,∅, t).
7: KM
l,VM
l←Cross-attention features of ϵθ(zM
t,c, t).
8: KM
l←αlKI
l+ (1−αl)KM
l ▷Key update.
9: VM
l←αlVI
l+ (1−αl)VM
l ▷Value update.
10: zM
t←zM
t−ϵθ(zM
t,c, t) ▷Reverse Diffusion Step.
11:s← DM(zM
0) ▷Generate Spectrograms.
12:w← V (s) ▷Generate Waveform from Spectrograms.
13:return w.
channel through which we can guide the text-to-music dif-
fusion model toward the semantic concepts contained in the
corresponding image conditioning. This “synapse” is de-
tailed in Line 7 to Line 10. The rest of the algorithm follows
the standard LDM training flow.
During inference, we make use of the trained text-to-
image and text-to-music diffusion models, along with the
learned αparameters. As seen in Lines 8 and 9 in Al-
gorithm 2, the cross-attention features of the text-to-music
LDM decoder are updated to incorporate the visual con-
ditioning in each denoising step. Once the denoising
(Line 10) is complete, the latent representation is projected
back into a spectrogram using the decoder of Audio V AE
[32], and then the waveform is generated using HiFi-GAN
vocoder [26] in Lines 11 and 12 respectively.
4. Experiments and Results
To complement our newly introduced problem set-
ting which generates music conditioned on visual
and textual modality, we introduce a new dataset , a
new evaluation metric , and come up with a strong baseline
by extending a state-of-the-art text-to-music method to
consume image modality. We explain each of these in the
subsequent sections.
4.1. Datasets
To the best of our knowledge, there is no publicly avail-
able dataset that contains the ⟨Image ,Text,Music⟩triplets
that are required to train and evaluate M ELFUSION . We
collect a new dataset MeLBench, which contains 11,250
manually annotated triplets of ⟨Image ,Text,Music⟩. Fur-
ther, we extend the MusicCaps [1] dataset which contains
⟨Text,Music⟩pairs by adding the corresponding image.
Figure 3. The distribution of different genres in MeLBench.
Figure 4. Some image and text pairs from MeLBench. We include
more examples in the Appendix.
MeLBench: We hired 18professional annotators to find
10-second snippets of YouTube videos corresponding to 15
predefined genres. The annotators are trained musicians
with at least 5 years of practice. For each of these videos,
they were asked to provide (a) a free-form text description
for up to three sentences, expressing the composition and
(b) any other music-related details such as describing the
genre, mood, tempo, singer voices, instrumentation, disso-
nances, rhythm, etc. A carefully selected frame and mu-
sic from the snippet along with text description from an-
notators forms ⟨Image ,Text,Music⟩triplets. We perform
strict sanity checks to ensure the quality of these triplets
in MeLBench. Fig. 4 shows some image and text samples
from the dataset and Fig. 3 shows the distribution of differ-
ent genres in MeLBench. Before annotating YouTube snip-
pets (containing music-albums, art-performance, ensembles
etc.), they were asked to check for complementary rele-
vance between visuals and music. Further, we perform
manual validation to filter lower-quality samples. We in-
clude more examples and more statistics of the dataset in
the Appendix.
Extended MusicCaps: MusicCaps [1] is a subset of the
AudioSet [16] dataset, which contains music and a cor-
responding textual description of the same. We carefully
choose two images from the web or YouTube that can go
26830
well with each datapoint in MusicCaps, thereby extending
⟨Text,Music⟩pairs to ⟨Image ,Text,Music⟩triplets. We de-
fer more details to the Appendix.
4.2. Evaluation Metrics
We use objective evaluation and human subjective evalua-
tion metrics to measure the efficacy of M ELFUSION .
4.2.1 Objective Evaluation: Following previous works
[17, 28, 32], Fr ´echet Audio Distance (FAD), Fr ´echet Dis-
tance (FD) and KL Divergence scores are used for objec-
tive evaluation. FAD [28] is a perceptual metric that is
adapted from Fr ´echet Inception Distance (FID) for the au-
dio domain. It uses a VGG-like backbone [19] for feature
extraction. FD is similar to FAD but uses PANNs [27] as the
feature extractor. Unlike reference-based metrics, FAD and
FD measure the distance between the generated audio dis-
tribution and the real audio distribution without using any
reference audio samples. On the other hand, KL Diver-
gence [28] is a reference-dependent metric that computes
the divergence between the distributions of the original and
generated audio samples based on the labels generated by
a pre-trained classifier. While FAD is more related to hu-
man perception, KL Divergence captures the similarities
between the original and generated audio signals based on
broad concepts present in them.
FAD, FD, and KL Divergence score captures the ‘good-
ness’ of generated music, while it doesn’t measure whether
the generated music is consistent with the image condition-
ing. We identify this as a gap and propose IMSM metric.
Image Music Similarity Metric (IMSM): CLIP score is
one of the widely used metrics for measuring the similarity
between an image and a corresponding textual description.
Npairs of images and texts are passed through respective
encoders (pre-trained using CLIP loss [38]) to obtain cor-
responding feature embeddings which are used to compute
CLIP score matrix ACLIP∈RN×N. In a very similar fash-
ion, CLAP scores are computed amongst Naudio-text pairs
yielding CLAP score matrix ACLAP∈RN×N[10]. It is
worth noting that in both the matrices the columns repre-
sent text modality. This motivates us to develop a metric
IMSM, which is a measure of the perceptual similarity be-
tween given image-music pairs bridged by the text modal-
ity. In particular, we use CLIP image and text encoders
which are contrastively aligned [38] to compute the image
and text feature embeddings. As a second step, we lever-
age language as the bridging modality by freezing the CLIP
text encoder and aligning the music (audio) encoder via
contrastive training [10]. Finally, for ⟨Image ,Text,Music⟩
pairs we obtain IMSM by suitably combining ACLIP and
ACLAP using the given mathematical expression:
AIMSM =ACLIPAT
CLAP (8)4.2.2 Subjective Evaluation: Following earlier works in
text-to-audio generation [17, 28, 32], we use overall audio
quality (OVL) and relevance to image-text inputs (REL) to
analyze the results of our subjective user study involving
75 participants. They were presented with 100 randomly
generated samples from M ELFUSION . Each of the metrics
(OVL and REL) is a score between 1-100with1being the
lowest. For the OVL score, the users are asked to assign
a score based on how perceptually realistic the generated
audio is, while the REL score requires them to carefully
examine the image and the text prompts before providing
a rating based on their relevance with the generated music.
We add more details of the user study in the Appendix.
4.3. Baseline Methods
We compare M ELFUSION against strong baselines to test
its mettle. To the best of our knowledge, there doesn’t exist
a music diffusion model that is conditioned on visual and
textual modality. Hence, we introduce two baselines: 1)
caption the image with Instruct BLIP [5] and then pass it
along with the caption to MusicLM [1]. We call this base-
lineMusicLM + InstructBLIP . 2) we adapt a recent text-
to-audio diffusion model TANGO [17] into our setting, as
explained next. We call its modified version TANGO++.
Further, we compare ourselves with 7other text-to-music
approaches. We elaborate on them below:
TANGO++: TANGO [17] is a powerful text-to-audio
model based on LDMs. They condition the diffusion model
on text embeddings from FLAN-T5 [3] text encoder ztext.
To facilitate joint conditioning from text and image I, we
embed Ito the latent space as zimage using a ViT [9] based
CLIP encoder, and align them together through an Image-
Text Contrastive loss. Once they are aligned, both the em-
beddings are fused and the LDM is jointly conditioned.
Text-to-Music Baselines: To bring out the utility of con-
ditioning on both visual and textual modality, we compare
MELFUSION with seven other text-to-music methods too:
Riffusion [12], Mubert [34], MusicLM [1], Mo ˆusai [42],
Noise2Music [22], MeLoDy [30] and MusicGen [4]. We
provide more details of each of these in the Appendix.
4.4. Results
We present exhaustive objective and subjective comparison
of M ELFUSION with the baseline approaches in Tab. 1.
When compared with text-to-music approaches in the first
section of the table, our results show the significant utility
of adding extra visual conditioning on the generations. The
fine-grained contextual information from visual modality is
able to supplement the information from the correspond-
ing text, thereby enhancing the quality of music genera-
tion. Further, M ELFUSION is able to consistently outper-
form MusicLM + InstructBLIP and TANGO++ (which has
similar conditioning as ours). This highlights the efficacy
26831
Model Txt ImgMusicCaps MeLBench
Objective metrics Subjective metrics Objective metrics Subjective metrics
FAD↓ KL↓ FD↓ IMSM ↑ OVL↑ REL↑ FAD↓ KL↓ FD↓ IMSM ↑ OVL↑ REL↑
Riffusion [12] ✓ ✗ 13.40 1.19 - - 79.48 75.60 14.06 1.42 32.64 - 80.11 76.26
MuBERT [34] ✓ ✗ 9.60 1.58 - - 77.59 77.93 - - - - - -
MusicLM [1] ✓ ✗ 4.00 1.01 - - 81.51 82.65 3.62 0.93 23.44 - 83.86 84.27
Moˆusai [42] ✓ ✗ 7.50 1.59 - - 75.94 77.33 9.13 1.63 31.51 - 75.11 74.32
Noise2Music [21] ✓ ✗ 2.13 - - - 81.13 79.88 - - - - - -
MeLoDy [30] ✓ ✗ 5.41 - - - 80.61 79.25 - - - - - -
MusicGen [4] ✓ ✗ 3.40 1.23 - - 83.57 83.18 3.28 1.21 23.60 - 84.61 83.25. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
MusicLM + InstructBLIP [5] ✓ ✓ 4.12 1.18 25.68 0.55 80.21 79.85 3.88 1.07 24.96 0.63 81.18 82.42
TANGO++ ✓ ✓ 3.05 1.17 23.91 0.68 84.62 83.96 2.93 1.14 22.16 0.71 85.52 84.81
MELFUSION (Ours) ✓ ✓ 1.12 0.89 22.65 0.76 86.78 85.92 1.05 0.72 20.49 0.83 88.45 87.39. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
∆MELFUSION - MusicGen - - +67.05% +27.64% - - +3.84% +3.29% +67.98% +40.49% +13.17% - +4.53% +4.97%
Table 1. Our proposed approach M ELFUSION offers significant gains over state-of-the-art text-to-music methods (first section), and our
adapted text-and-image conditioned baselines (second section) across multiple objective and subjective metrics on two datasets. IMSM is
applicable only when the model is conditioned on visual modality. We skip comparison with MuBERT, Noise2Music, and MeLoDy on
MeLBench dataset as their codebases are not public. Please refer to Sec. 4.4 for more details.
of our visual synapse, which infuses the right amount of
visual conditioning to enable the model to synthesize per-
ceptually congruent music tracks. Captions from Instruct-
BLIP are superfluous and vague when compared to expert-
annotated, high-quality MusicCaps captions on which Mu-
sicLM is trained. This distributional shift leads to a per-
formance drop as shown in the table. TANGO++ uses con-
trastive loss to align CLIP image features and FLAN-T5 text
features, further, we use simple addition for joint condition-
ing – these design choices can be sub-optimal.
Our subjective human evaluation in Tab. 1 also suggests
that conditioning the music generation on both visual and
textual modality improves its perceptual quality.
5. Discussions and Analysis
5.1 Analyzing the Design Choice of αparameters: α
parameters introduced in Eq. (6) controls how the self-
attention features from the blocks within the text-to-image
diffusion model interact with the cross-attention features
from the text-to-music diffusion model. M ELFUSION has
one alpha parameter per block within the decoders of both
diffusion models. We vary this design choice in Tab. 2.
Attaching the synapse in the decoder offers better perfor-
mance. This is because the decoder controls the major
transformations that contribute to generating the image.
Further, learning different αper block helps to learn block-
specific mixing co-efficient, which slightly improves the
performance.
We also perform a sensitivity analysis on the learning
rate (LR) used while learning αparameters in Tab. 3. Based
on this analysis, we use a LR of 1e−5in our experiments.
5.2 Efficacy of Conditioning on both Modalities: In or-
der to study the contribution of each modality on M ELFU-
SION , we train three different variations of the model by
selectively turning off visual conditioning and textual con-
ditioning. We report the results in Tab. 4. We see significantEncoder DecoderExtended MusicCaps MeLBench
FAD↓KL↓FD↓FAD↓KL↓FD↓
Same αfor all blocks.
✓ ✗ 3.22 1.23 24.01 2.01 1.01 21.96
✓ ✓ 2.71 1.13 23.31 1.27 0.87 21.04
✗ ✓ 2.79 1.14 23.44 1.29 0.87 21.13
Different αfor each block.
✓ ✗ 2.03 1.10 23.36 1.92 0.93 21.28
✓ ✓ 1.13 0.94 22.81 1.07 0.76 20.53
✗ ✓ 1.12 0.89 22.65 1.05 0.72 20.49
Table 2. We systematically analyze our design choice of learnable
αparameters. We vary the position of the synapse: encoder or
decoder and also study whether we need the same or different α
parameters for each block within them.
Learning RateExtended MusicCaps MeLBench
FAD↓KL↓FD↓FAD↓KL↓FD↓
0.5e-6 3.12 1.21 23.26 2.01 1.14 21.95
0.5e-4 1.38 0.95 22.81 1.39 0.88 20.86
1e-6 2.56 1.17 23.11 1.86 1.10 21.71
1e-5 1.12 0.89 22.65 1.05 0.72 20.49
Table 3. Sensitivity analysis on the learning rate for αparameters.
Text ImageExtended MusicCaps MeLBench
FAD↓KL↓IMSM ↑FAD↓KL↓IMSM ↑
✓ ✗ 3.07 1.21 - 3.11 1.19 -
✗ ✓ 5.62 1.54 - 4.16 1.37 -
✓ ✓ 1.12 0.89 0.76 1.05 0.72 0.83
Table 4. Conditioning independently on each of the modalities
leads to inferior music generation performance in this experiment.
improvement when conditioning on both modalities. This
highlights how complementary semantic information from
each modality can compose better music.
5.3 Sensitivity Analysis: Tab. 5 reports the sensitivity anal-
ysis of changing the number of denoising steps Tand the
strength of classifier-free guidance during inference. Simi-
26832
Varying Steps Varying Guidance
Guidance Steps FAD ↓KL↓ FD↓ Steps Guidance FAD ↓KL↓ FD↓
750 2.59 1.97 27.45
4002 1.47 1.13 24.64
200 1.35 1.12 25.22 7 1.12 0.89 22.65
400 1.12 0.89 22.65 20 1.51 1.18 24.92
600 1.09 0.88 22.57 30 1.63 1.29 25.38
800 1.07 0.77 22.48 50 1.58 1.34 24.87
Table 5. Sensitivity analysis on the number of denoising steps T,
and the strength of classifier-free guidance.
Text prompt length
(in words)ImageObjective metrics Subjective metrics
FAD↓KL↓FD↓OVL↑ REL↑
≥8≤13 ✗ 5.28 1.35 25.81 82.86 82.54
≥14≤19 ✗ 3.13 1.20 23.11 85.25 85.16
≥20 ✗ 3.02 1.19 22.65 86.04 85.96
≤7 ✓ 1.86 0.87 21.36 87.13 86.21
Table 6. Performance of M ELFUSION with varying verbosity of
text prompts collected from MeLBench.
lar to the findings from Ghosal et al. [17], increasing Thelps
to generate more pleasing music. This can be attributed to
enhanced refinement from more denoising. CFG strength
of7gives the best result, and we use it in our experiments.
5.4 Verbose Text versus Image Conditioning: The vi-
sual synapse infuses fine-grained semantics from the im-
age into text-to-music diffusion models. Another alterna-
tive to infuse such semantics would be to use verbose text
descriptions. To study this, we remove the visual synapse
from M ELFUSION and train a music generation model con-
ditioned only on text. Then, we vary the length of text
prompts and report results in Tab. 6. Interestingly, we find
that using image conditioning with a small text prompt
outperforms using lengthier prompts. This underscores
the utility of visual conditioning and the ability of visual
synapses to modulate the LDM effectively.
5.5 Effect of visual-cues: We analyse the effect of using a
different image and the same text prompt (please refer to the
project page). When we change from the walkway image to
the blue forest, the music becomes more calm and distant.
We change the image to an abandoned amusement park, car-
nival orchestra, foggy seaside concert and forest at night.
We observe a prevalence of eerie ambient sound echoing
through the deserted park, occasional circus-inspired mo-
tifs, distant sounds of waves and foghorn-like effects and
atmospheric strings imitating rustling leaves respectively.
5.6 Comparison with Text-to-Audio Methods: We in-
clude a comparison of M ELFUSION with text-to-audio gen-
eration approaches in Tab. 7. We finetune their pre-trained
checkpoints on our benchmark datasets for this experiment.
The complementary information from both modalities al-
lows our approach to outperform these methods too.
5.7 Effectiveness of IMSM: We conduct a user study with
64participants to check whether the proposed IMSM metric
is indeed capturing the relatedness between generated mu-
sic and the conditioning image. We randomly choose 300MethodMusicCaps MeLBench
FAD↓KL↓FD↓FAD↓KL↓FD↓
AudioLDM [32] 2.29 1.29 24.07 1.86 1.42 22.49
TANGO [17] 1.96 1.17 23.31 1.93 1.18 21.92
MELFUSION 1.12 0.89 22.65 1.05 0.72 20.49
Table 7. While comparing M ELFUSION with state-of-the-art text-
to-audio approaches, we see significant improvement in quality.
samples from Extended MusicCaps and MeLBench each.
We compute the IMSM score, and also ask users for their
image-music similarity on a scale of [0,1], for these sam-
ples. The average score from IMSM metric and the users
for Extended MusicCaps and MeLBench are ( 0.76,0.71)
and (0.83,0.85) respectively. The high correlation under-
scores the validity and usefulness of IMSM metric.
5.8 Using IMSM to Measure Purity of the Datasets: We
compute the IMSM scores for all image-music pairs present
in both Extended MusicCaps and MeLBench datasets and
obtain a score of 0.91and0.93respectively. The purpose
of this is to quantitatively establish that the curated samples
are perceptually in sync and are meaningful. The high val-
ues of the IMSM scores demonstrate that the curated image
samples are highly perceptually similar and have ample as-
sociation with the musical compositions.
6. Conclusion and Future Works
We explore the utility of infusing image semantics into a
text-to-music diffusion model, enabling us to generate mu-
sic, consistent with both visual and textual semantics in this
work. To the best of our knowledge, ours is the first effort
towards such a multi-conditioned music generation. We de-
velop M ELFUSION with a novel “visual synapse” to effec-
tively infuse the image semantics into music generation, in-
troduce a new dataset MeLBench, and propose a new eval-
uation metric. We conduct exhaustive experimental analy-
sis on MeLBench and a modified version of MusicCaps [1]
and compare M ELFUSION against 7text-to-music meth-
ods, and a modified baseline. The results suggest: 1) the
extra information from the image conditioning significantly
boosts music generation quality 2) our “visual synapse” is
effective in modulating and infusing the required semantic
information into the generative process.
MELFUSION can be an essential tool for a creative pro-
fessional or a social-media content creator who needs to
generate music that can go well with their multi-modal post
(consider a user posting about their recent picnic – their
photos can be the image conditioning while a short descrip-
tion of the trip can be the textual input to M ELFUSION ).
Creating music with semantic lyrics that can go well with a
video can be some interesting open-ended follow-up works.
26833
References
[1] Andrea Agostinelli, Timo I Denk, Zal ´an Borsos, Jesse En-
gel, Mauro Verzetti, Antoine Caillon, Qingqing Huang,
Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al.
Musiclm: Generating music from text. arXiv preprint
arXiv:2301.11325 , 2023. 1, 2, 5, 6, 7, 8
[2] Shahaf Bassan, Yossi Adi, and Jeffrey S Rosenschein. Unsu-
pervised symbolic music segmentation using ensemble tem-
poral prediction errors. arXiv preprint arXiv:2207.00760 ,
2022. 2
[3] Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, et al. Scaling
instruction-finetuned language models. arXiv preprint
arXiv:2210.11416 , 2022. 3, 6
[4] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant,
Gabriel Synnaeve, Yossi Adi, and Alexandre D ´efossez.
Simple and controllable music generation. arXiv preprint
arXiv:2306.05284 , 2023. 1, 2, 6, 7
[5] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning.
2023. 6, 7
[6] Paromita Das, Somsubhra Gupta, and Biswarup Neogi. Mea-
surement of effect of music on human brain and consequent
impact on attentiveness and concentration during reading.
Procedia computer science , 172:1033–1038, 2020. 2
[7] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook
Kim, Alec Radford, and Ilya Sutskever. Jukebox: A gen-
erative model for music. arXiv preprint arXiv:2005.00341 ,
2020. 2
[8] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan
Yang. Musegan: Multi-track sequential generative adversar-
ial networks for symbolic music generation and accompani-
ment. In Proceedings of the AAAI Conference on Artificial
Intelligence , 2018. 2
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 6
[10] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail,
and Huaming Wang. Clap learning audio concepts from nat-
ural language supervision. In ICASSP 2023-2023 IEEE In-
ternational Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP) , pages 1–5. IEEE, 2023. 6
[11] Evelina Fedorenko, Josh H McDermott, Sam Norman-
Haignere, and Nancy Kanwisher. Sensitivity to musical
structure in the human brain. Journal of neurophysiology ,
108(12):3289–3300, 2012. 2
[12] Seth Forsgren and Hayk Martiros. Riffusion-stable diffusion
for real-time music generation, 2022. URL https://riffusion.
com/about , 6. 2, 6, 7
[13] Aviv Gabbay, Niv Cohen, and Yedid Hoshen. An image is
worth more than a thousand words: Towards disentangle-ment in the wild. Advances in Neural Information Process-
ing Systems , 34:9216–9228, 2021. 1
[14] Chuang Gan, Deng Huang, Peihao Chen, Joshua B Tenen-
baum, and Antonio Torralba. Foley music: Learning to gen-
erate music from videos. In Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part XI 16 , pages 758–775. Springer,
2020. 2
[15] Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, and
Bryan Pardo. Vampnet: Music generation via masked acous-
tic token modeling. arXiv preprint arXiv:2307.04686 , 2023.
3
[16] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren
Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal,
and Marvin Ritter. Audio set: An ontology and human-
labeled dataset for audio events. In 2017 IEEE interna-
tional conference on acoustics, speech and signal processing
(ICASSP) , pages 776–780. IEEE, 2017. 5
[17] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish,
and Soujanya Poria. Text-to-audio generation using
instruction-tuned llm and latent diffusion model. arXiv
preprint arXiv:2304.13731 , 2023. 3, 6, 8
[18] Assal Habibi and Antonio Damasio. Music, feelings, and the
human brain. Psychomusicology: Music, Mind, and Brain ,
24(1):92, 2014. 2
[19] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F
Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal,
Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn archi-
tectures for large-scale audio classification. In 2017 ieee in-
ternational conference on acoustics, speech and signal pro-
cessing (icassp) , pages 131–135. IEEE, 2017. 6
[20] Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti,
Judith Yue Li, and Daniel PW Ellis. Mulan: A joint em-
bedding of music audio and natural language. arXiv preprint
arXiv:2208.12415 , 2022. 2
[21] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk,
Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang,
Jiahui Yu, Christian Frank, et al. Noise2music: Text-
conditioned music generation with diffusion models. arXiv
preprint arXiv:2302.03917 , 2023. 7
[22] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk,
Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang,
Jiahui Yu, Christian Frank, et al. Noise2music: Text-
conditioned music generation with diffusion models. arXiv
preprint arXiv:2302.03917 , 2023. 2, 6
[23] Rongjie Huang, Max WY Lam, Jun Wang, Dan Su, Dong Yu,
Yi Ren, and Zhou Zhao. Fastdiff: A fast conditional diffu-
sion model for high-quality speech synthesis. arXiv preprint
arXiv:2204.09934 , 2022. 3
[24] Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou
Zhao. Generspeech: Towards style transfer for generaliz-
able out-of-domain text-to-speech synthesis. arXiv preprint
arXiv:2205.07211 , 2022.
[25] Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye
Cui, and Yi Ren. Prodiff: Progressive fast diffusion model
for high-quality text-to-speech. In Proceedings of the 30th
ACM International Conference on Multimedia , pages 2595–
2605, 2022. 3
26834
[26] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan:
Generative adversarial networks for efficient and high fi-
delity speech synthesis. Advances in Neural Information
Processing Systems , 33:17022–17033, 2020. 5
[27] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,
Wenwu Wang, and Mark D Plumbley. Panns: Large-scale
pretrained audio neural networks for audio pattern recogni-
tion. IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing , 28:2880–2894, 2020. 6
[28] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer,
Alexandre D ´efossez, Jade Copet, Devi Parikh, Yaniv Taig-
man, and Yossi Adi. Audiogen: Textually guided audio gen-
eration. arXiv preprint arXiv:2209.15352 , 2022. 6
[29] Max WY Lam, Jun Wang, Dan Su, and Dong Yu. Bddm:
Bilateral denoising diffusion models for fast and high-quality
speech synthesis. arXiv preprint arXiv:2203.13508 , 2022. 3
[30] Max WY Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan
Feng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo Ma, Xuchen
Song, et al. Efficient neural music generation. arXiv preprint
arXiv:2305.15719 , 2023. 1, 2, 6, 7
[31] Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang
Liu, Qi Meng, Tao Qin, Wei Chen, Sungroh Yoon, and Tie-
Yan Liu. Priorgrad: Improving conditional denoising dif-
fusion models with data-dependent adaptive prior. arXiv
preprint arXiv:2106.06406 , 2021. 3
[32] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,
Danilo Mandic, Wenwu Wang, and Mark D Plumbley. Audi-
oldm: Text-to-audio generation with latent diffusion models.
arXiv preprint arXiv:2301.12503 , 2023. 3, 5, 6, 8
[33] Ilaria Manco, Emmanouil Benetos, Elio Quinton, and
Gy¨orgy Fazekas. Contrastive audio-language learning for
music. arXiv preprint arXiv:2208.12208 , 2022. 2
[34] 2023 Mubert Inc. Mubert. URL https://mubert.com/. Mu-
bert inc. mubert. url https://mubert.com/, 2023,. Mubert Inc.
Mubert. URL https://mubert.com/, 2023, , 2023. 2, 6, 7
[35] Aashiq Muhamed, Liang Li, Xingjian Shi, Suri Yaddana-
pudi, Wayne Chi, Dylan Jackson, Rahul Suresh, Zachary C
Lipton, and Alex J Smola. Symbolic music generation with
transformer-gans. In Proceedings of the AAAI conference on
artificial intelligence , pages 408–417, 2021. 2
[36] Sam V Norman-Haignere, Nancy Kanwisher, Josh H Mc-
Dermott, and Bevil R Conway. Divergence in the functional
organization of human and macaque auditory cortex revealed
by fmri responses to harmonic tones. Nature neuroscience ,
22(7):1057–1060, 2019. 2
[37] Vadim Popov, Ivan V ovk, Vladimir Gogoryan, Tasnima
Sadekova, and Mikhail Kudinov. Grad-tts: A diffusion prob-
abilistic model for text-to-speech. In International Confer-
ence on Machine Learning , pages 8599–8608. PMLR, 2021.
3
[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 6
[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution imagesynthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 2, 3
[40] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 3
[41] Catherine Schmidt-Jones. The basic elements of music . Con-
nexions, 2012. 2
[42] Flavio Schneider, Zhijing Jin, and Bernhard Sch ¨olkopf.
Mo\ˆ usai: Text-to-music generation with long-context la-
tent diffusion. arXiv preprint arXiv:2301.11757 , 2023. 1, 2,
6, 7
[43] Roy Sheffer and Yossi Adi. I hear your true colors: Im-
age guided audio generation. In ICASSP 2023-2023 IEEE
International Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pages 1–5. IEEE, 2023. 1
[44] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 3
[45] David Temperley and Trevor de Clercq. Statistical analysis
of harmony and melody in rock music. Journal of New Music
Research , 42(3):187–204, 2013. 2
[46] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. Advances in neural information pro-
cessing systems , 30, 2017. 3
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 3
[48] Junjie Wu, Junsong Zhang, Xiaojun Ding, Rui Li, and
Changle Zhou. The effects of music on brain functional net-
works: a network analysis. Neuroscience , 250:49–59, 2013.
2
[49] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao
Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete
diffusion model for text-to-sound generation. IEEE/ACM
Transactions on Audio, Speech, and Language Processing ,
2023. 3
[50] Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang. Midinet:
A convolutional generative adversarial network for
symbolic-domain music generation. arXiv preprint
arXiv:1703.10847 , 2017. 2
[51] Adrien Ycart, Emmanouil Benetos, et al. A study on lstm
networks for polyphonic music sequence modelling. ISMIR,
2017. 2
[52] Robert J Zatorre. Music and the brain. Annals of the New
York Academy of Sciences , 999(1):4–14, 2003. 2
26835
