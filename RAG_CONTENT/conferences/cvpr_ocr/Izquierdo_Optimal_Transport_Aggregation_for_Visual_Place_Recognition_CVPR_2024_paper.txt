Optimal Transport Aggregation for Visual Place Recognition
Sergio Izquierdo Javier Civera
I3A, University of Zaragoza, Spain
fizquierdo, jcivera g@unizar.es
Abstract
The task of Visual Place Recognition (VPR) aims to
match a query image against references from an extensive
database of images from different places, relying solely on
visual cues. State-of-the-art pipelines focus on the aggre-
gation of features extracted from a deep backbone, in order
to form a global descriptor for each image. In this con-
text, we introduce SALAD (S inkhorn A lgorithm for L ocally
Aggregated D escriptors), which reformulates NetVLAD‚Äôs
soft-assignment of local features to clusters as an optimal
transport problem. In SALAD, we consider both feature-
to-cluster and cluster-to-feature relations and we also in-
troduce a ‚Äòdustbin‚Äô cluster, designed to selectively discard
features deemed non-informative, enhancing the overall de-
scriptor quality. Additionally, we leverage and Ô¨Åne-tune
DINOv2 as a backbone, which provides enhanced descrip-
tion power for the local features, and dramatically reduces
the required training time. As a result, our single-stage
method not only surpasses single-stage baselines in pub-
lic VPR datasets, but also surpasses two-stage methods that
add a re-ranking with signiÔ¨Åcantly higher cost. Code and
models are available at https://github.com/serizba/salad.
1. Introduction
Recognizing a place solely from images becomes a chal-
lenging task when scenes undergo substantial changes in
their structure or appearance. Such capability is referred
to in the scientiÔ¨Åc and technical literature as visual place
recognition (and by its acronym VPR), and is essential
for agents to navigate and understand their surroundings
autonomously in a wide array of applications, such as
robotics [12‚Äì14, 22, 29] or augmented reality [19]. SpeciÔ¨Å-
cally, it is present in simultaneous localization and mapping
[9, 10] and absolute pose estimation [25, 44] pipelines.
In practice, VPR is framed as an image retrieval problem,
wherein typically a query image serves as the input and the
goal is to obtain an ordered list of top-k matches against
a pre-existing database of geo-localized reference images.
ResNetBaseline
NetVLADOptimal 
TransportOurs
DINOv2 ViT
SOFTMAX
 S
SD
U
S
T
B
I
NAA
LA
AD
D
R@1 R@5 R@10 R@1 R@5 R@10Figure 1. Illustration of a VPR baseline (left) and our contri-
bution (right). The left column outlines a typical VPR baseline, a
ResNet backbone followed by NetVLAD aggregation [4]. On the
right column, we replace ResNet with a partially Ô¨Åne-tuned DI-
NOv2 [41] backbone, and incorporate SALAD, our novel optimal
transport aggregation using the Sinkhorn Algorithm. Our model
achieves unprecedented state-of-the-art results on common VPR
benchmarks.
Images are represented as an aggregation of appearance pat-
tern descriptors, which are subsequently compared via near-
est neighbour. The effectiveness of this matching relies on
generating discriminative per-image descriptors that exhibit
robust performance even for challenging variations such as
Ô¨Çuctuating illumination, structural transformations, tempo-
ral changes, weather and seasonal shifts. Most recent re-
search on VPR have thus focused on the two key compo-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
17658
nents of this general pipeline, namely the deep neural back-
bones for feature extraction and methods for aggregating
such features.
For years, ResNet-based neural networks have been the
predominant backbones for feature extraction [4, 23, 45].
Recently, given the success of Vision Transformer (ViT)
for different computer vision tasks [17, 21, 30, 33], some
methods have introduced ViT in the Ô¨Åeld of VPR [58, 65].
AnyLoc [28] proposed to leverage foundation models, us-
ing DINOv2 [41] as a feature extractor for VPR. However,
AnyLoc uses DINOv2 ‚Äòas is‚Äô, while we show in this paper
that Ô¨Åne-tuning the model for VPR brings a signiÔ¨Åcant in-
crease in performance.
Regarding aggregation, NetVLAD [4], the learned coun-
terpart to the traditional handcrafted VLAD [26], is among
the most popular choices. Alternative methods include
pooling layers like GeM [45] or learned global aggregation,
like the recent MixVPR [2]. In this paper, we propose op-
timal transport aggregation, setting a new state of the art in
VPR.
As a summary, in this work, we present a single-stage
approach to VPR that obtains state-of-the-art results in the
most common benchmarks. To achieve this, we present two
key contributions:
First, we propose SALAD (S inkhorn A lgorithm for
Locally A ggregated D escriptors), a reformulation of the
feature-to-cluster assignment problem through the lens of
optimal transport, allowing more effective distribution of
local features into the global descriptor bins. To further
improve the discriminative power of the aggregated de-
scriptor, we let the network discard uninformative fea-
tures by introducing a ‚Äòdustbin‚Äô mechanism.
Secondly, we integrate the representational power of
foundation models into VPR, using DINOv2 as the back-
bone for feature extraction. Unlike previous approaches
that utilized DINOv2 in its pre-trained form, our method
involves Ô¨Åne-tuning the model speciÔ¨Åcally for the task.
This Ô¨Åne-tuning process converges extremely fast, in just
four epochs, and allows DINOv2 to capture more rele-
vant and distinctive features pertinent to place recogni-
tion tasks.
The fusion of these two novel components results in DI-
NOv2 SALAD, which can be efÔ¨Åciently trained in less than
one hour and sets unprecedented recall in VPR benchmarks,
with 75.0% Recall@1 in MSLS Challenge and 76.0% in
Nordland. All of this with a single-stage pipeline, without
requiring expensive post-processing steps and with an infer-
ence speed of less than 3 ms per image.
2. Related Work
The signiÔ¨Åcant research efforts on VPR have been exhaus-
tively compiled in a number of surveys and tutorials overthe years [19, 36, 37, 49, 64]. Current research addresses a
wide variety of topics, such as novel loss functions [5, 31],
image sequences [20, 60], extreme viewpoint changes [32]
or text features [24]. In this section, we focus on work re-
lated to feature extraction and aggregation, as there lie our
contributions.
Early approaches to VPR used either aggregations of
handcrafted local features [3, 15, 26] or global descrip-
tors [39, 53]. In both cases, geometric [18] and tempo-
ral [18, 38] consistency was sometimes enforced for en-
hanced performance. With the emergence of deep neural
networks, features pre-trained for recognition tasks, with-
out Ô¨Åne-tuning, showed a signiÔ¨Åcant performance boost
over handcrafted ones [54]. However, training or Ô¨Åne-
tuning speciÔ¨Åcally for VPR tasks using contrastive or triplet
losses [40] offers an additional improvement and is standard
nowadays.
NetVLAD [4] is the most popular architecture explicitly
designed for VPR, mimicking the VLAD aggregation [26]
but jointly learning from data both convolutional features
and cluster centroids. Radenovi ¬¥c et al. [45] proposed the
Generalized Mean Pooling (GeM) to aggregate feature ac-
tivations, also a popular baseline due to its simplicity and
competitive performance. In addition to these, several other
alternatives have been proposed in the literature. For exam-
ple, Teichmann et al. [56] aggregates regions instead of lo-
cal features. Recently, MixVPR [2] has presented the best
results in the literature by combining deep features with a
MLP layer.
A notable trend in VPR has been the adoption of a two-
stage approach to enhance retrieval accuracy [11, 23, 47, 50,
55, 65]. After a Ô¨Årst stage with any of the methods presented
in the previous paragraph, the top retrieved candidates are
re-ranked attending to the un-aggregated local features, ei-
ther assessing the geometric consistency to the query image
or predicting their similarity. This re-ranking stage adds a
considerable overhead, which is why it is only applied to
a few candidates, but generally improves the performance.
Re-ranking is out of the scope of our research but, notably,
we outperform all baselines that employ re-ranking even if
our model does not include such stage (and hence it is sub-
stantially faster).
Optimal transport has found a signiÔ¨Åcant number of ap-
plications in graphics and computer vision [8]. Specif-
ically, related to our research, it has been used for im-
age retrieval [43], image matching [61] and feature match-
ing [48, 52]. Recently, Zhang et al. [63] used optimal trans-
port at the re-ranking stage in a retrieval pipeline. However,
ours is the Ô¨Årst work that proposes the formulation of local
feature aggregation from an optimal transport perspective.
17659
Global Descriptor
Score projection
Dimension reduction
Projection Optimal 
Transportclustersfeatures1
1
nm
global tokenfeatures
D
U
S
T
B
I
N
DINOv2 ViTSS
AA
LA
AD
D1
2
nScore projection
Score projectionFigure 2. Overview of our method . First, the DINOv2 backbone extracts local features and a global token from an input image. Then, a
small MLP, score projection, computes a score matrix for feature-to-cluster and dustbin relationships. The optimal transport module uses
the Sinkhorn algorithm to transform this matrix into an assignment, and subsequently, dimensionality-reduced features are aggregated into
the Ô¨Ånal descriptor based on this assignment and concatenated with the global token.
3. Method
DINOv2 SALAD is based on NetVLAD, but we propose to
use and Ô¨Åne-tune the DINOv2 backbone (Sec. 3.1) and pro-
pose a novel module (SALAD) for the assignment (Sec. 3.2)
and aggregation (Sec. 3.3) of features.
3.1. Local Feature Extraction
Effective local feature extraction lies in striking a balance:
features must be robust enough to withstand substantial
changes in appearance, such as those between seasons or
from day to night, yet they should retain sufÔ¨Åcient informa-
tion on local structure to enable accurate matching.
Inspired by the success of ViT architectures in many
computer vision tasks and by AnyLoc [28], that leverages
the exceptional representational capabilities of foundation
models [7], we adopt DINOv2 [41] as our backbone. How-
ever, differently from AnyLoc, we use a supervised pipeline
and include the backbone in the end-to-end training for the
speciÔ¨Åc task, yielding improved performance.
DINOv2 adopts a ViT architecture that initially divides
an input image I2Rhwcintoppcpatches,
withp= 14 . These patches are sequentially projected
with transformer blocks, resulting in the output tokens
ft1;:::;tn;tn+1g;ti2Rd, wheren=hw=p2is the
number of input patches and there is an additional global
token tn+1that aggregates class information. Although
the DINOv2‚Äôs authors reported that Ô¨Åne-tuning the model
only brings dim improvements, we found that at least for
VPR there are substantial gains in selectively unfreezing
and training the last blocks of the encoder.3.2. Assignment
In NetVLAD, a global descriptor is formed by assigning a
set of features to a set of clusters, fC1;:::;C j;:::;C mg,
and then aggregating all features that belong to each clus-
ter. For the assignment, NetVLAD computes a score matrix
S2Rnm
>0, where the element in its ithrow andjthcolumn,
si;j2R>0, represents the cost of assigning a feature to a
clusterCj. In other words, SquantiÔ¨Åes the afÔ¨Ånity of each
feature to each clusters. While SALAD draws inspiration
from NetVLAD, we identify several crucial aspects in their
assignment and propose alternatives to address these.
Reduce assignment priors. When building the score
matrix S, NetVLAD introduces certain priors. SpeciÔ¨Åcally,
it initializes the linear layer that computes Swith centroids
derived from k-means. While this may accelerate the train-
ing, it introduces inductive bias and potentially makes the
model more susceptible to local minima. In contrast, we
propose to learn each row siof the score matrix from scratch
with two fully connected layers initialized randomly:
si=Ws2((Ws1(ti) +bs1)) +bs2 (1)
where Ws1,Ws2andbs1,bs2are the weights and biases
of the layers, and is a non-linear activation function.
Discard uninformative features. Some features, such
as those representing the sky, might contain negligible in-
formation for VPR. NetVLAD does not account for this,
and the contribution of all features is preserved in the Ô¨Ånal
descriptor. Contrary, we follow recent works on keypoint
matching and introduce a ‚Äòdustbin‚Äô where non-informative
features are assigned to. For that, we augment the score ma-
trix, from StoS= [S;si;m+1]2Rnm+1
>0 , by appending
the column si;m+1representing the feature-to-dustbin rela-
tion. As in SuperGlue [48], this score is modeled with a
17660
single learnable parameter z2R:
si;m+1=z1n (2)
being 1n= [1;:::; 1]>2Rnan-dimensional vector of
ones.
Optimal assignment. The original NetVLAD assign-
ment computes a per-row softmax over Sto obtain the dis-
tribution of each feature‚Äôs mass across the clusters. How-
ever, this approach only considers the feature-to-cluster re-
lationship and overlooks the reverse ‚Äìthe cluster-to-feature
relation. For this reason, we reformulate the assignment
as an optimal transport problem where the features‚Äô mass,
=1n, must be effectively distributed among the clusters
or the ‚Äòdustbin‚Äô, = [1>
m;n m]>. We follow Super-
Glue [48] and use the Sinkhorn Algorithm [16, 51] to obtain
the assignment P2Rn(m+1)such that
P1m+1=and P>1n=. (3)
This algorithm Ô¨Ånds the optimal transport assignment be-
tween distributions anditeratively normalizing rows
and columns from exp S
. Finally, we drop the dustbin
column to obtain the assignment P=
p;1;:::; p;m
,
where p;jstands for the jthcolumn of P.
3.3. Aggregation
Once the feature assignment in our SALAD framework is
computed as detailed in Sec. 3.2, we focus on the aggre-
gation of these assigned features to form the Ô¨Ånal global
descriptor. The aggregation process in NetVLAD involves
combining all features assigned to each cluster Cj. How-
ever, we introduce three variations:
Dimensionality reduction. To efÔ¨Åciently manage the
Ô¨Ånal descriptor size, we Ô¨Årst reduce the dimensionality of
the tokens from RdtoRl. This is achieved by processing
the features through two fully connected layers, precisely
adjusting the size of the feature vectors while retaining the
essential information from the task.
fi=Wf2((Wf1(ti) +bf1)) +bf1 (4)
Aggregation. Based on the assignment matrix derived
using the Sinkhorn Algorithm, each feature is aggregated
into its assigned cluster. Differently from NetVLAD, we
do not subtract the centroids to get the residuals. We di-
rectly aggregate these features with a summation, reducing
the incorporated priors about the aggregation. Viewing the
resulting VLAD vector as a matrix V2Rml, each ele-
mentVj;k2Ris computed as follows:
Vj;k=nX
i=1Pi;kfi;k (5)wherefi;kcorresponds to the kthdimension of fi, withk2
f1;:::;lg.
Global token. To include global information about the
scene not easily incorporated into local features, we also
incorporate a scene descriptor gcomputed as:
g=Wg2((Wg1(tn+1) +bg1)) +bg1 (6)
where tn+1is the global token from DINOv2. We then con-
catenate gwith VÔ¨Çattened. Following NetVLAD, we do
an L2 intra-normalization and an entire L2 normalization of
this vector, which yields the Ô¨Ånal global descriptor.
4. Experiments
To rigorously evaluate the effectiveness of our proposed
contributions, we conducted exhaustive experiments fol-
lowing standard evaluation protocols.
4.1. Implementation Details
We ground our training and evaluation setups on the pub-
licly provided framework by MixVPR1.
For the architecture , we opt for a pretrained DINOv2-B
backbone, targeting a balance between computational ef-
Ô¨Åciency and representational capacity. We only Ô¨Åne-tune
the Ô¨Ånal 4 layers of the encoder, which signiÔ¨Åcantly en-
hances the performance without markedly increasing train-
ing time. For the fully connected layers, the weights of the
hidden layers Ws1,Wf1andWg1have 512neurons and
use ReLU for the activation function . To optimize feature
handling, we employ a dimensionality reduction, compress-
ing feature token dimensions from d= 768 tol= 128 ,
and the global to 256. We use m= 64 clusters, result-
ing in a global descriptor of size 12864 + 256 . We also
report results with smaller descriptors, with size 512 + 32
(m= 15; l= 32 ), and 2048 + 64 (m= 32; l= 64 ).
Wetrain on GSV-Cities [1], a large dataset of urban
locations collected from Google Street View. Given the
impressive representation power of DINOv2, our pipeline
achieves training convergence within just 4 epochs. Us-
ing a batch size of 60 places, each represented by 4 im-
ages, the training is completed in 30 minutes on a single
NVIDIA RTX 3090. We use the multi-similarity loss [59]
and AdamW [35] for the optimization, with an initial learn-
ing rate set to 6e 5. To ensure an effective learning rate, we
linearly decay the initial rate at every iteration so at the end
of the training is 20% of the initial value. We use a dropout
rate of 0:3on the score projection and dimensionality reduc-
tion neurons. As our model is agnostic to the image input
size (as long as it can be divided in 1414patches), we
evaluate on images of size 322322but train on 224224
to speedup training time.
1https://github.com/amaralibey/MixVPR
17661
MethodMSLS Challenge MSLS Val NordLand Pitts250k-test SPED
Desc. size Latency (ms) R@1 R@5 R@1 R@5 R@1 R@5 R@1 R@5 R@1 R@5
NetVLAD [4] 32768 1.41 35.1 47.4 82.6 89.6 32.6 47.1 90.5 96.2 78.7 88.3
GeM [45]y 1024 1.14 49.7 64.2 78.2 86.6 21.6 37.3 87.0 94.4 66.7 83.4
Conv-AP [1] 8192 1.22 54.2 66.6 83.1 90.3 42.7 58.9 92.9 97.7 79.2 88.6
CosPlace [5] 2048 2.59 67.2 78.0 87.4 93.0 44.2 59.7 92.1 97.5 80.1 89.6
MixVPR [2] 4096 1.37 64.0 75.9 88.0 92.7 58.4 74.6 94.6 98.3 85.2 92.1
EigenPlaces [6] 2048 2.65 67.4 77.1 89.3 93.7 54.4 68.8 94.1 98.0 69.9 82.9
DINOv2 SALAD 512 + 32 2.33 70.8 83.6 89.3 94.9 61.2 78.9 93.0 97.4 88.5 94.7
DINOv2 SALAD 2048 + 64 2.35 73.7 85.9 90.5 95.4 70.4 85.7 94.8 98.3 89.5 94.9
DINOv2 SALAD 8192 + 256 2.41 75.0 88.8 92.2 96.4 76.0 89.2 95.1 98.5 92.1 96.2
Table 1. Comparison against single-stage baselines. We compare DINOv2 SALAD against two popular baselines [4, 45] and the four
baselines that show best results in recent literature [1, 2, 5, 6]. Our slim version already obtains state-of-the-art results in all metrics. Our
full model outperforms all previous results by a signiÔ¨Åcant margin. Note, in particular, the large improvement in the most challenging
benchmarks, MSLS Challenge and NordLand. yWe reproduced GeM results training during 80 epochs following MixVPR training
pipeline.
MethodDesc. sizeMemory (GB)Latency (ms) MSLS Challenge MSLS Val
Global Local Retrieval Reranking R@1 R@5 R@10 R@1 R@5 R@10
Patch-NetVLAD [23] 4096 28264096 908.30 9.55 8377.17 48.1 57.6 60.5 79.5 86.2 87.7
TransVPR [58] 256 1200256 22.72 6.27 1757.70 63.9 74.0 77.5 86.8 91.2 92.4
R2Former [65] 256 500131 4.7 8.88 202.37 73.0 85.9 88.8 89.7 95.0 96.2
DINOv2 SALAD (ours) 8192 + 256 0.0 0.63 2.41 0.0 75.0 88.8 91.3 92.2 96.4 97.0
Table 2. Comparison against baselines with re-ranking. We compare our single-stage DINOv2 SALAD with methods that perform a re-
ranking stage to improve performance. Without using re-ranking, our DINOv2 SALAD outperforms all other methods while being orders
of magnitude faster and more memory-efÔ¨Åcient. Latency metrics obtained from [65] using a RTX A5000. Latency for DINOv2 SALAD
was computed using a RTX 3090. Memory footprint is calculated on the MSLS Val dataset, which includes around 18;000images.
Method Desc. size SF-XL Test v1 SF-XL Test v2
CosPlace [5] 2048 76.4 88.8
EigenPlaces [6] 2048 84.1 90.8
DINOv2 SALAD 8192 + 256 88.6 94.8
Table 3. Results on SF-XL. (R@1) Our DINOv2 SALAD
achieves unprecedented results on SF-XL despite never seeing any
single image of San Francisco during VPR Ô¨Ånetuning.
Tovalidate our experiments and select the hyperparam-
eters, we monitored the recall in the Pittsburg30k-test [57].
We observed that, in the long run, most conÔ¨Ågurations per-
form similarly, but rapid convergence on a few epochs is
more sensitive to the hyperparameters.
4.2. Results
We benchmarked our model against several single-stage
baselines, namely NetVLAD [4] and GeM [45] as two rep-
resentative tradicional baselines, and Conv-AP [1], Cos-
Place [5], MixVPR [2] and EigenPlaces [6] as the four
most recent and best performing baselines in the literature.
The evaluation spanned a diverse array of well-established
datasets: MSLS Validation and Challenge [60], which are
comprised of dashcam images; Pittsburgh250k-test [57],
featuring urban scenarios; SPED [14], a collection from
surveillance cameras; NordLand, notable for its seasonal
variations from images captured from the front of a train
traversing Norway; and SF-XL [5], a large urban dataset toevaluate VPR at scale. We use Recall@k (R@k) as the met-
ric for all our experiments, as it is standard in related work.
We use evaluation data and code from MixVPR [2], which
considers retrieval as correct if an image at less than 25me-
ters (or two frames for Nordland) from the query is among
the top-k predicted candidates.
As shown in Table 1, our model outperforms all previous
methods on all datasets and all metrics. Even the smaller
512 + 32 version already surpasses previous models with
bigger descriptors on most datasets. It is worth highlighting
the metrics saturation observed in MSLS Val, Pitts250k-test
and SPED, and on the other hand the challenging nature
of MSLS Challenge and NordLand. The MSLS Challenge
dataset, with its diversity, extensive size and closed labels,
and NordLand, with its extreme sample similarity and sea-
sonal shifts, emerge then as key benchmarks for assessing
VPR performance. Although our DINOv2 SALAD shows
a signiÔ¨Åcant improvement on allbenchmarks, it is precisely
in MSLS Challenge and NordLand where we obtain the
most substantial recall increases, with +7:6%;+11:7%and
+17:6%;+14:6%for R@1, R@5 respectively over the sec-
ond best. For SF-XL, as shown in Table 3, our method also
achieves the best results to date. This is remarkable, con-
sidering that the previous state of the art was trained on this
dataset, whereas our method never used any image of San
Francisco when it was Ô¨Åne-tuned.
In Table 2, we compare our DINOv2 SALAD method,
17662
MethodMSLS Challenge MSLS Val NordLand Pitts250k-test SPED
Desc. size R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
ResNet NetVLAD [4] 32768 35.1 47.4 51.7 82.6 89.6 92.0 32.6 47.1 53.3 90.5 96.2 97.4 78.7 88.3 91.4
DINOv2 AnyLoc [28] 49152 42.2 53.5 58.1 68.7 78.2 81.8 16.1 25.4 30.4 87.2 94.4 96.5 85.3 94.4 95.4
ResNet SALAD 8192 57.4 70.8 74.9 83.2 89.5 91.8 33.3 49.6 55.8 91.4 96.9 97.9 75.0 86.7 89.8
ConvNext [34] SALAD 8192 63.9 75.2 80.1 85.5 92.4 94.5 47.8 64.3 70.3 93.9 97.9 98.8 83.5 90.9 92.9
DINOv2 GeM 4096 62.6 78.3 83.0 85.4 93.9 95.0 35.4 52.5 59.6 89.5 96.5 98.0 83.0 92.1 93.9
DINOv2 MixVPR 4096 72.1 85.0 88.3 90.0 95.1 96.0 63.6 80.1 84.6 94.6 98.3 99.3 89.8 94.9 96.1
DINOv2 NetVLAD 24576 75.8 86.5 89.8 92.4 95.9 96.9 71.8 86.5 90.1 95.6 98.7 99.3 90.8 95.7 96.7
DINOv2 NetVLAD (dim. red.) 8192 73.3 85.6 88.3 90.1 95.4 96.8 70.1 86.5 90.2 95.4 98.4 99.1 90.6 95.4 96.7
DINOv2 SALAD (ours) 8192 + 256 75.0 88.8 91.3 92.2 96.4 97.0 76.0 89.2 92.0 95.1 98.5 99.1 92.1 96.2 96.5
Table 4. Ablations . The Ô¨Årst two rows correspond to two baselines in the literature [4, 28], the rest to different aggregations appended to
DINOv2 including our DINOv2 SALAD. Note that only DINO NetVLAD, with a signiÔ¨Åcantly bigger descriptor size than ours, is able to
show competitive results. We outperform all the rest DINOv2 baselines of similar descriptor sizes by a large margin.
which solely operates on a single retrieval stage, against
the leading two-stage Visual Place Recognition (VPR) tech-
niques. In this comparison, we include the best per-
forming models in the literature, namely R2Former [65],
TransVPR [58], and Patch-NetVLAD [23], which incor-
porate a re-ranking reÔ¨Ånement. Note how our DINOv2
SALAD, despite being orders of magnitude faster and
smaller in memory, signiÔ¨Åcantly outperforms all these two-
stage methods on all benchmarks. This Ô¨Ånding not only
highlights the efÔ¨Åciency of our model but also demonstrates
the effectiveness of global retrieval using our novel SALAD
aggregation. Additionally, considering our method‚Äôs re-
liance on local features, we believe that a re-ranking stage
could also be applied, potentially increasing our recall met-
rics but at the price of a higher computational footprint.
4.3. Ablation Studies
Effect of DINOv2 . We assess the impact of the DINOv2
backbone and our optimal transport aggregation SALAD
separately. For this, we compare with the existing baselines
of ResNet NetVLAD or AnyLoc, this last one applying a
VLAD on top of a pretrained DINOv2 encoder. We inte-
grate the DINOv2 backbone with various aggregation mod-
ules, obtaining a handful of performant techniques that im-
prove their respective previous results. As shown in Table 4,
all of these outperform the baselines, even though AnyLoc
already uses DINOv2. This validates the DINOv2‚Äôs inte-
gration in end-to-end Ô¨Åne-tuning to reÔ¨Åne its capabilities.
Effect of SALAD . Our experiments in Table 4 show that
aggregation also matters. Even the recent MixVPR aggre-
gation coupled with DINOv2 does not match the perfor-
mance of DINOv2 NetVLAD and DINOv2 SALAD. We
believe that the DINOv2 backbone is especially suitable
for local feature aggregation, as its features work remark-
ably well in dense visual perception tasks [27, 41, 62].
Although DINOv2 NetVLAD achieves comparable perfor-
mance to SALAD, it employs a descriptor almost three
times as big. Besides, the generalization performance of
DINOv2 NetVLAD is limited, as observed in NordLand
results. We attribute this to NetVLAD‚Äôs priors initializa-
tion with urban scenarios, which constrain the convergenceModel Dim. size # Params. Latency (ms) MSLS Val R@1
S 384 21M 1.30 90.5
B 768 86M 2.41 92.2
L 1024 300M 7.82 92.6
G 1536 1100M 24.93 91.7
Table 5. DINOv2 conÔ¨Ågurations and performances.
of the system. In our experiments we also trained a slim-
mer DINOv2 NetVLAD version, whose features are dimen-
sionally reduced as described in Section 3.3, targetting a Ô¨Å-
nal descriptor of roughly the same size as SALAD. In this
fairer setup, DINOv2 SALAD clearly outperforms DINOv2
NetVLAD. We also evaluate SALAD on top of ResNet
and ConvNext backbones, which improves over baseline
ResNet NetVLAD but is signiÔ¨Åcantly worse than using DI-
NOv2. This indicates that SALAD is specially suited for
high spatial resolution features, like the ones from DINOv2.
Effect of hyperparameters . DINOv2 comes in different
sizes that affect the number of parameters, inference speed,
and representation capabilities. As shown in Table 5, more
parameters do not always result in better performance. Ex-
cessively big models might be harder to train or prone to
overÔ¨Åtting the training set. From these results, we chose
the DINOv2-B backbone, which exhibits a great balance
between performance and size and speed. Regarding de-
scriptor size, we observed (Table 1) that changing mand
lallows to get slimmer versions with competitive perfor-
mance. For the number of blocks to train, as shown in Ta-
ble 6, Ô¨Åne-tuning two or four block report the best results
without signiÔ¨Åcant computation overhead.
Effect of SALAD components . In Table 7, we show
how different components of our SALAD pipeline affect the
Ô¨Ånal performance. Both the global token, which appends
global information not captured in local features, and the
dustbin, which helps in distilling the aggregated features,
contribute to the performance of SALAD. We also trained a
model using a dual-softmax [46] to solve the optimal trans-
port assignment, following LoFTR and Gluestick [42, 52].
Although dual-softmax achieves only slightly worse perfor-
mance, the Sinkhorn Algorithm is theoretically sound and
provides a better acronym to our method.
17663
Figure 3. Heatmap of local features importance . Left images show the original pictures, their right counterparts represent the weights not
assigned to the ‚Äòdustbin‚Äô. Note how the network learns to discard uninformative regions like skies, roads or dynamic objects, and instead
focus on distinctive patterns in buildings and vegetation. We attribute its focus on distant buildings to their invariance to viewpoint change.
MethodMSLS Val
R@1 R@5 R@10
DINOv2 SALAD (frozen) 88.5 95.0 96.2
DINOv2 SALAD (train 2 last blocks) 92.0 96.5 97.0
DINOv2 SALAD (train 4 last blocks) 92.2 96.4 97.0
DINOv2 SALAD (train 6 last blocks) 91.6 96.2 97.0
DINOv2 SALAD (train all blocks) 89.2 95.1 96.1
Table 6. Fine-tuning different number of DINOv2 blocks.
MethodMSLS Val
R@1 R@5 R@10
DINOv2 SALAD w/o dustbin 91.4 95.8 96.2
DINOv2 SALAD w/o global token 91.8 96.0 96.2
DINOv2 SALAD (Dual Softmax) 91.9 95.7 96.5
DINOv2 SALAD 92.2 96.4 97.0
Table 7. Ablation study of the SALAD components.
4.4. Introspective Results
We provide an introspection of our model‚Äôs performance
through a series of illustrative Ô¨Ågures. Figure 3 visualizes
the weights that are not assigned to the ‚Äòdustbin‚Äô, offering
insight into the parts of the input image that the network
considers informative. As the ‚Äòdustbin‚Äô assignment is com-
pletely learnt by the network, some discarded features might
be counter-intuitive. However, we observe that it typically
removes dynamic objects and focuses on the most distinc-
tive and invariant parts of the image. In Figure 4, we dis-
play the assignment distribution of patches from two dif-
ferent images depicting the same place. It demonstrates the
model‚Äôs ability to consistently distribute most of the weights
into the same bins for patches representing similar regions.
Such repeatable and consistent assignment across different
images of the same place is crucial for the reliability andperformance of the system. Finally, in Figure 5, we show-
case various query images alongside their respective top-3
retrievals made by our system. DINOv2 SALAD is able to
retrieve correct predictions even under challenging condi-
tions, such as severe changes in illumination or viewpoint.
5. Conclusions and Limitations
In this paper, we have proposed DINOv2 SALAD, a novel
model for VPR that outperforms previous baselines by a
substantial margin. This achievement is the result of com-
bining two key contributions: a Ô¨Åne-tuned DINOv2 back-
bone for enhanced feature extraction and our novel SALAD
(Sinkhorn Algorithm for Locally Aggregated Descriptors)
module for feature aggregation. Our extensive experiments
demonstrate the effectiveness of these modules, highlight-
ing the model‚Äôs single-stage nature and exceptionally fast
training and inference speed.
While our work brings signiÔ¨Åcant improvements in per-
formance, it is not without limitations. Primarily, the adop-
tion of DINOv2 as our backbone results in slower process-
ing speeds compared to ResNet-based methods. Besides,
although SALAD is a general aggregation module, its ef-
fectiveness is tied to the choice of backbone. It excels with
DINOv2, which offers high spatial resolution features, but it
is less suited for coarser features. Additionally, in SALAD
we use an optimal transport assignment in its simplest form.
More sophisticated constraints could improve the resulting
assignment, a very relevant aspect for our future work.
Acknowledgments
This work was supported by the Spanish Govern-
ment (PID2021-127685NB-I00 and TED2021-131150B-
I00), the Arag ¬¥on Government (DGA T45 23R) and the
scholarship FPU20/02342.
17664
Clusters Clusters
Image 1 Image 2 Cluster assignmentFigure 4. Illustration of feature-to-cluster assignments. See at the leftmost and rightmost part of the Ô¨Ågure two different views of the
same place. Framed by red and blue squares we highlight two corresponding patches in each of the images. The central part of the Ô¨Ågure
shows the feature-to-cluster assignments for these patches. Note how DINOv2 SALAD correctly assigns the features to the same bins for
both views, even with different local texture.
Query Top-1 Top-2 Top-3
Figure 5. DINOv2 SALAD qualitative results at MSLS. The left column shows several queries and the three other ones shows the top-3
candidates retrieved by our DINOv2 SALAD. Candidates are framed in green if they correspond to the same place as the query, and in red
if they do not. Note the correct retrievals under seasonal, weather, viewpoint and day-night changes. Note also a challenging failure case
in the last row, due to non-discriminative image content.
17665
References
[1] Amar Ali-bey, Brahim Chaib-draa, and Philippe Gigu `ere.
Gsv-cities: Toward appropriate supervised visual place
recognition. Neurocomputing , 513:194‚Äì203, 2022.
[2] Amar Ali-Bey, Brahim Chaib-Draa, and Philippe Giguere.
Mixvpr: Feature mixing for visual place recognition. In Pro-
ceedings of the IEEE/CVF Winter Conference on Applica-
tions of Computer Vision , pages 2998‚Äì3007, 2023.
[3] Relja Arandjelovic and Andrew Zisserman. All about vlad.
InProceedings of the IEEE conference on Computer Vision
and Pattern Recognition , pages 1578‚Äì1585, 2013.
[4] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-
jdla, and Josef Sivic. Netvlad: Cnn architecture for weakly
supervised place recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition ,
pages 5297‚Äì5307, 2016.
[5] Gabriele Berton, Carlo Masone, and Barbara Caputo. Re-
thinking visual geo-localization for large-scale applications.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 4878‚Äì4888, 2022.
[6] Gabriele Berton, Gabriele Trivigno, Barbara Caputo, and
Carlo Masone. Eigenplaces: Training viewpoint robust
models for visual place recognition. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 11080‚Äì11090, 2023.
[7] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-
man, Simran Arora, Sydney von Arx, Michael S Bernstein,
Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.
On the opportunities and risks of foundation models. arXiv
preprint arXiv:2108.07258 , 2021.
[8] Nicolas Bonneel and Julie Digne. A survey of optimal trans-
port for computer graphics and computer vision. In Com-
puter Graphics Forum , pages 439‚Äì460. Wiley Online Li-
brary, 2023.
[9] Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif,
Davide Scaramuzza, Jos ¬¥e Neira, Ian Reid, and John J
Leonard. Past, present, and future of simultaneous localiza-
tion and mapping: Toward the robust-perception age. IEEE
Transactions on robotics , 32(6):1309‚Äì1332, 2016.
[10] Carlos Campos, Richard Elvira, Juan J G ¬¥omez Rodr ¬¥ƒ±guez,
Jos¬¥e MM Montiel, and Juan D Tard ¬¥os. Orb-slam3: An accu-
rate open-source library for visual, visual‚Äìinertial, and mul-
timap slam. IEEE Transactions on Robotics , 37(6):1874‚Äì
1890, 2021.
[11] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep
local and global features for image search. In Computer
Vision‚ÄìECCV 2020: 16th European Conference, Glasgow,
UK, August 23‚Äì28, 2020, Proceedings, Part XX 16 , pages
726‚Äì743. Springer, 2020.
[12] Zetao Chen, Adam Jacobson, Niko S ¬®underhauf, Ben Up-
croft, Lingqiao Liu, Chunhua Shen, Ian Reid, and Michael
Milford. Deep learning features at scale for visual place
recognition. In 2017 IEEE international conference on
robotics and automation (ICRA) , pages 3223‚Äì3230. IEEE,
2017.
[13] Zetao Chen, Fabiola Maffra, Inkyu Sa, and Margarita Chli.
Only look once, mining distinctive landmarks from convnetfor visual place recognition. In 2017 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) , pages
9‚Äì16. IEEE, 2017.
[14] Zetao Chen, Lingqiao Liu, Inkyu Sa, Zongyuan Ge, and Mar-
garita Chli. Learning context Ô¨Çexible attention model for
long-term visual place recognition. IEEE Robotics and Au-
tomation Letters , 3(4):4015‚Äì4022, 2018.
[15] Mark Cummins and Paul Newman. Fab-map: Probabilistic
localization and mapping in the space of appearance. The
International journal of robotics research , 27(6):647‚Äì665,
2008.
[16] Marco Cuturi. Sinkhorn distances: Lightspeed computation
of optimal transport. Advances in neural information pro-
cessing systems , 26, 2013.
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representa-
tions , 2021.
[18] Dorian G ¬¥alvez-L ¬¥opez and Juan D Tardos. Bags of binary
words for fast place recognition in image sequences. IEEE
Transactions on Robotics , 28(5):1188‚Äì1197, 2012.
[19] Sourav Garg, Tobias Fischer, and Michael Milford. Where
is your place, visual place recognition? arXiv preprint
arXiv:2103.06443 , 2021.
[20] Sourav Garg, Madhu Vankadari, and Michael Milford. Seq-
matchnet: Contrastive learning with sequence matching for
place recognition & relocalization. In Conference on Robot
Learning , pages 429‚Äì443. PMLR, 2022.
[21] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,
Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chun-
jing Xu, Yixing Xu, et al. A survey on vision transformer.
IEEE transactions on pattern analysis and machine intelli-
gence , 45(1):87‚Äì110, 2022.
[22] Stephen Hausler, Adam Jacobson, and Michael Milford.
Multi-process fusion: Visual place recognition using multi-
ple image processing methods. IEEE Robotics and Automa-
tion Letters , 4(2):1924‚Äì1931, 2019.
[23] Stephen Hausler, Sourav Garg, Ming Xu, Michael Milford,
and Tobias Fischer. Patch-netvlad: Multi-scale fusion of
locally-global descriptors for place recognition. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 14141‚Äì14152, 2021.
[24] Ziyang Hong, Yvan Petillot, David Lane, Yishu Miao, and
Sen Wang. Textplace: Visual place recognition and topolog-
ical localization through reading scene texts. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 2861‚Äì2870, 2019.
[25] Arnold Irschara, Christopher Zach, Jan-Michael Frahm, and
Horst Bischof. From structure-from-motion point clouds to
fast location recognition. In 2009 IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 2599‚Äì2606.
IEEE, 2009.
[26] Herv ¬¥e J¬¥egou, Matthijs Douze, Cordelia Schmid, and Patrick
P¬¥erez. Aggregating local descriptors into a compact image
17666
representation. In 2010 IEEE computer society conference
on computer vision and pattern recognition , pages 3304‚Äì
3311. IEEE, 2010.
[27] Markus K ¬®appeler, K ¬®ursat Petek, Niclas V ¬®odisch, Wol-
fram Burgard, and Abhinav Valada. Few-shot panop-
tic segmentation with foundation models. arXiv preprint
arXiv:2309.10726 , 2023.
[28] Nikhil Keetha, Avneesh Mishra, Jay Karhade, Kr-
ishna Murthy Jatavallabhula, Sebastian Scherer, Madhava
Krishna, and Sourav Garg. Anyloc: Towards universal visual
place recognition. arXiv preprint arXiv:2308.00688 , 2023.
[29] Ahmad Khaliq, Shoaib Ehsan, Zetao Chen, Michael Mil-
ford, and Klaus McDonald-Maier. A holistic visual place
recognition approach using lightweight cnns for signiÔ¨Åcant
viewpoint and appearance changes. IEEE transactions on
robotics , 36(2):561‚Äì569, 2019.
[30] Youngwan Lee, Jonghee Kim, Jeffrey Willette, and Sung Ju
Hwang. Mpvit: Multi-path vision transformer for dense
prediction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 7287‚Äì
7296, 2022.
[31] Mar ¬¥ƒ±a Leyva-Vallina, Nicola Strisciuglio, and Nicolai
Petkov. Data-efÔ¨Åcient large scale place recognition with
graded similarity supervision. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 23487‚Äì23496, 2023.
[32] Tsung-Yi Lin, Yin Cui, Serge Belongie, and James Hays.
Learning deep representations for ground-to-aerial geolocal-
ization. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 5007‚Äì5015, 2015.
[33] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.
Swin transformer v2: Scaling up capacity and resolution. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 12009‚Äì12019, 2022.
[34] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 11976‚Äì11986,
2022.
[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017.
[36] Stephanie Lowry, Niko S ¬®underhauf, Paul Newman, John J
Leonard, David Cox, Peter Corke, and Michael J Milford.
Visual place recognition: A survey. ieee transactions on
robotics , 32(1):1‚Äì19, 2015.
[37] Carlo Masone and Barbara Caputo. A survey on deep visual
place recognition. IEEE Access , 9:19516‚Äì19547, 2021.
[38] Michael J Milford and Gordon F Wyeth. Seqslam: Visual
route-based navigation for sunny summer days and stormy
winter nights. In 2012 IEEE international conference on
robotics and automation , pages 1643‚Äì1649. IEEE, 2012.
[39] Ana C Murillo, Gautam Singh, Jana Kosecka, and Jos ¬¥e Jes ¬¥us
Guerrero. Localization in urban environments using a
panoramic gist descriptor. IEEE Transactions on Robotics ,
29(1):146‚Äì160, 2012.
[40] Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. A met-
ric learning reality check. In Computer Vision‚ÄìECCV 2020:16th European Conference, Glasgow, UK, August 23‚Äì28,
2020, Proceedings, Part XXV 16 , pages 681‚Äì699. Springer,
2020.
[41] Maxime Oquab, Timoth ¬¥ee Darcet, Th ¬¥eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193 , 2023.
[42] R ¬¥emi Pautrat, Iago Su ¬¥arez, Yifan Yu, Marc Pollefeys, and
Viktor Larsson. Gluestick: Robust image matching by
sticking points and lines together. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 9706‚Äì9716, 2023.
[43] OÔ¨År Pele and Michael Werman. Fast and robust earth
mover‚Äôs distances. In 2009 IEEE 12th international confer-
ence on computer vision , pages 460‚Äì467. IEEE, 2009.
[44] No ¬¥e Pion, Martin Humenberger, Gabriela Csurka, Yohann
Cabon, and Torsten Sattler. Benchmarking image retrieval
for visual localization. In 2020 International Conference on
3D Vision (3DV) , pages 483‚Äì494. IEEE, 2020.
[45] Filip Radenovi ¬¥c, Giorgos Tolias, and Ond Àárej Chum. Fine-
tuning cnn image retrieval with no human annotation. IEEE
transactions on pattern analysis and machine intelligence ,
41(7):1655‚Äì1668, 2018.
[46] Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovi ¬¥c, Akihiko
Torii, Tomas Pajdla, and Josef Sivic. Neighbourhood con-
sensus networks. Advances in neural information processing
systems , 31, 2018.
[47] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and
Marcin Dymczyk. From coarse to Ô¨Åne: Robust hierarchical
localization at large scale. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 12716‚Äì12725, 2019.
[48] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
and Andrew Rabinovich. Superglue: Learning feature
matching with graph neural networks. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 4938‚Äì4947, 2020.
[49] Stefan Schubert, Peer Neubert, Sourav Garg, Michael Mil-
ford, and Tobias Fischer. Visual Place Recognition: A Tuto-
rial. IEEE Robotics & Automation Magazine , 2023.
[50] Shihao Shao, Kaifeng Chen, Arjun Karpur, Qinghua Cui,
Andr ¬¥e Araujo, and Bingyi Cao. Global features are all you
need for image retrieval and reranking. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 11036‚Äì11046, 2023.
[51] Richard Sinkhorn and Paul Knopp. Concerning nonnegative
matrices and doubly stochastic matrices. PaciÔ¨Åc Journal of
Mathematics , 21(2):343‚Äì348, 1967.
[52] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and
Xiaowei Zhou. Loftr: Detector-free local feature matching
with transformers. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
8922‚Äì8931, 2021.
[53] Niko S ¬®underhauf and Peter Protzel. Brief-gist-closing the
loop by simple means. In 2011 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems , pages 1234‚Äì1241.
IEEE, 2011.
17667
[54] Niko S ¬®underhauf, Sareh Shirazi, Feras Dayoub, Ben Up-
croft, and Michael Milford. On the performance of convnet
features for place recognition. In 2015 IEEE/RSJ interna-
tional conference on intelligent robots and systems (IROS) ,
pages 4297‚Äì4304. IEEE, 2015.
[55] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea
Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Ak-
ihiko Torii. Inloc: Indoor visual localization with dense
matching and view synthesis. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 7199‚Äì7209, 2018.
[56] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack
Sim. Detect-to-retrieve: EfÔ¨Åcient regional aggregation for
image search. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5109‚Äì
5118, 2019.
[57] Akihiko Torii, Josef Sivic, Tomas Pajdla, and Masatoshi
Okutomi. Visual place recognition with repetitive structures.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 883‚Äì890, 2013.
[58] Ruotong Wang, Yanqing Shen, Weiliang Zuo, Sanping Zhou,
and Nanning Zheng. Transvpr: Transformer-based place
recognition with multi-level attention aggregation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 13648‚Äì13657, 2022.
[59] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong,
and Matthew R Scott. Multi-similarity loss with general
pair weighting for deep metric learning. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 5022‚Äì5030, 2019.
[60] Frederik Warburg, Soren Hauberg, Manuel Lopez-
Antequera, Pau Gargallo, Yubin Kuang, and Javier
Civera. Mapillary street-level sequences: A dataset for
lifelong place recognition. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 2626‚Äì2635, 2020.
[61] Jiankai Xing, Fujun Luan, Ling-Qi Yan, Xuejun Hu, Houde
Qian, and Kun Xu. Differentiable rendering using rgbxy
derivatives and optimal transport. ACM Transactions on
Graphics (TOG) , 41(6):1‚Äì13, 2022.
[62] Jingfeng Yao, Xinggang Wang, Shusheng Yang, and
Baoyuan Wang. Vitmatte: Boosting image matting with pre-
trained plain vision transformers. Information Fusion , 103:
102091, 2024.
[63] Chao Zhang, Stephan Liwicki, and Roberto Cipolla. Beyond
the cls token: Image reranking using pretrained vision trans-
formers. In BMVC , 2022.
[64] Xiwu Zhang, Lei Wang, and Yan Su. Visual place recog-
nition: A survey from deep learning perspective. Pattern
Recognition , 113:107760, 2021.
[65] Sijie Zhu, Linjie Yang, Chen Chen, Mubarak Shah, Xiao-
hui Shen, and Heng Wang. R2former: UniÔ¨Åed retrieval and
reranking transformer for place recognition. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 19370‚Äì19380, 2023.
17668
