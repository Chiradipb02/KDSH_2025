Implicit Motion Function
Yue Gao Jiahao Li Lei Chu Yan Lu
Microsoft Research
{yuegao, li.jiahao, leichu, yanlu }@microsoft.com
Abstract
Recent advancements in video modeling extensively rely
on optical flow to represent the relationships across frames,
but this approach often lacks efficiency and fails to model
the probability of the intrinsic motion of objects. In ad-
dition, conventional encoder-decoder frameworks in video
processing focus on modeling the correlation in the encoder,
leading to limited generative capabilities and redundant in-
termediate representations. To address these challenges,
this paper proposes a novel Implicit Motion Function (IMF)
method. Our approach utilizes a low-dimensional latent
token as the implicit representation, along with the use of
cross-attention, to implicitly model the correlation between
frames. This enables the implicit modeling of temporal cor-
relations and understanding of object motions. Our method
not only improves sparsity and efficiency in representation
but also explores the generative capabilities of the decoder
by integrating correlation modeling within it. The IMF
framework facilitates video editing and other generative
tasks by allowing the direct manipulation of latent tokens.
We validate the effectiveness of IMF through extensive ex-
periments on multiple video tasks, demonstrating superior
performance in terms of reconstructed video quality, com-
pression efficiency and generation ability.
1. Introduction
Recent years have witnessed an upsurge in the field of video
modeling [44], processing [45, 62], compression [31, 38],
prediction [19, 20], and generation [11, 64, 66]. A fun-
damental element in these diverse tasks is the use of opti-
cal flow, which encapsulates the relationship between con-
secutive frames. Existing approaches often employ read-
ily available optical flow estimation models, such as RAFT
[61], to facilitate this process. However, the definition
of optical flow has inherent limitations. It lacks sparsity,
capturing only the positional change of pixels, and fails
to model the probabilistic aspects of object movements,
thereby not comprehending the intrinsic motion and seman-
tics of objects. Furthermore, the non-editability of explicit
Reference frame
Current frame
Reference frame
Current frame
Dense optical flow
TPSMM [81]
Dense optical flow
PECHead [11]
Schematic latent space
IMF
Schematic latent space
IMF
Figure 1. IMF represents the motion in latent space and model
the correlation implicitly. In the diagram, the blue shapes repre-
sent tokens from the reference frame, while the pink shapes rep-
resent tokens from the current frame. Tokens that are unmatched
in the reference frame, indicating new content, are depicted with
pink polygons. Successfully matched tokens, such as changes of
the head pose or the magnification of flower, are indicated with
green arrows. Compared to explicit methods PECHead [11] and
TPSMM [81], our IMF faithfully reconstructs the current frame.
optical flow presents significant challenges in video gener-
ation tasks. This stems from its rigid structure, which lim-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
19278
its the ability to manipulate or alter motion trajectories di-
rectly within the flow field. Consequently, this necessitates
the integration of sophisticated neural networks for down-
stream generation tasks. These identified limitations under-
score the need for an innovative approach in video model-
ing. It is in this context that we propose the Implicit Motion
Function (IMF) method, a novel paradigm designed to ad-
dress the aforementioned shortcomings by offering a more
nuanced and editable representation of motion within video
sequences.
Our design of the IMF is based on the observation that,
contrasting with the aforementioned limitations brought by
the explicit optical flow, modeling the frame relations in
the latent space offers a more nuanced and adaptable ap-
proach. Specifically, by operating in this latent space, it
becomes feasible to effectively distinguish different objects
and new content between two frames. Such capability is
derived from the inherent property of the latent space by
encoding a richer and more abstract representation of mo-
tion and object characteristics. This encoding allows for
a more generalized understanding of object dynamics and
interactions, thereby enabling the IMF to adapt to and ac-
curately represent a broader range of object movements and
behaviors, including the newly appeared objects. It is worth
noting that we generalize the concept of motion to include
the entry and exit of objects from the frame as a form of mo-
tion, rather than solely encompassing pixel displacement.
The design of our pipeline, adopting an encoder-decoder
style, can be highlighted in several aspects: (1) We employ a
dense feature encoder and a latent token encoder to model
dense features and latent tokens independently. This ap-
proach allows us to represent motion between two frames
as a pair of low-dimensional tokens , significantly increas-
ing the sparsity of the representation. (2) We design a la-
tent token decoder to decode the tokens and a implicit mo-
tion alignment to model the correlation between two frames
aligning the features from the reference frame to the cur-
rent frame. They enable the model to understand the re-
lationship between features across frames, thereby grasp-
ing the motion patterns of objects and increasing the spar-
sity of the motion representation. (3) By directly editing
the latent token, we can facilitate editing and other gen-
erative tasks. In particular, the encoder independently ex-
tracts latent tokens from the current frame and the refer-
ence frame, so the frame-to-frame relationship is entirely
modeled by the decoder. As shown in Fig. 1, we provide
a schematic illustration of the correlation modeling process
within the IMF latent space. Compared to explicit methods,
our IMF allows for a faithful motion modeling of non-facial
content ( e.g., flowers) and even new content ( e.g., appear-
ing hand and blooming flower), demonstrating the semantic
completeness is ensured by the proposed implicit modeling,
for both talking head and general videos.In implementing the IMF, two primary challenges
emerge. The first concerns defining and extracting sparse
tokens. Conventional approaches [11, 41, 57, 66, 81] rely
on sparse keypoints transformed into dense optical flow, but
this method proves suboptimal, particularly for data lacking
strong priors. Our approach circumvents this limitation by
allowing the encoder to extract latent tokens without tying
them to physical coordinates, preserving semantic integrity
while maintaining sparsity. The second challenge involves
modeling correlations with latent tokens. Drawing paral-
lels to language model advancements [43, 48], we lever-
age cross-attention [65] mechanism, a sophisticated weight-
based matrix operation that exceeds the capabilities of opti-
cal flow-based grid sampling [41]. By utilizing a latent to-
ken decoder to generate motion feature maps from compact
latent tokens and processing these features through cross-
attention, we model frame relationships and align reference
appearance features to the current frame. This approach en-
ables refined motion feature generation and feature align-
ment, bolstering both the reconstructive and generative abil-
ities of the decoder and offering significant potential when
integrated with foundation models for video applications.
We conducted comprehensive experiments to validate
the superiority of our IMF model in video reconstruction,
video compression, and talking head editing. In video
reconstruction, IMF outperformed existing benchmarks in
both talking head and general datasets, demonstrating its
ability to accurately reconstruct non-facial objects and new
content. For video compression, IMF achieved a superior
rate-distortion trade-off compared to current methods, in-
cluding neural video codecs, highlighting the compactness
of our latent tokens. In talking head editing, IMF surpassed
state-of-the-art (SOTA) explicit editing methods, confirm-
ing the editability and robust generative capabilities of our
latent tokens.
Our contributions can be summarized as follows:
• We propose the Implicit Motion Function ( IMF ), a novel
framework for video modeling and video generation. The
IMF is capable of faithfully reconstructing various videos
at extremely low bit rates, while also enabling high-
quality video generation.
• We incorporate the latent token decoder and the implicit
motion alignment as the implicit motion function for cor-
relation modeling. By integrating this step at the decoder,
the generative capability of the decoder is unleashed.
• By manipulating the latent tokens, the proposed method
can achieve high-quality editing results for talking head
images, which also demonstrates the powerful expressive
capability of our latent tokens.
• Extensive qualitative and quantitative results across sev-
eral talking head and general datasets demonstrate the su-
periority of the our framework for video modeling and
video generation.
19279
2. Related Works
2.1. Image Animation and Video Generation
Image animation, which involves transferring motion be-
tween images, has traditionally relied on methods utiliz-
ing face meshes [13, 49], human keypoints [78], or ac-
tion units [22, 47]. Recent trends, however, are leaning
towards self-supervised techniques that require only video
input. Notable pioneering works such as FOMM [57],
and MRAA [58] introduced motion representation through
sparse keypoints or regions employing local affine transfor-
mations. TPSMM [81] employs thin-plate spline transfor-
mation for motion estimation. LIA [67] uses latent vector
decomposition but relies on optical flow for warping, and its
limited editability leads to artifacts and facial interference.
Furthermore, Mallya et al. [41] proposed a warping method
based on cross-attention mechanisms [65]. However, its re-
liance on explicit keypoints as an intermediate representa-
tion imposes limitations on performance, as its effectiveness
in modeling non-facial data remains relatively limited.
In contrast to methods targeting general data, the field
of talking head video generation, which focuses on creat-
ing realistic videos of talking faces, has advanced signifi-
cantly due to research in various areas. Unlike the image
animation methods that do not need detailed facial models,
talking head generation uses detailed information about the
head to improve video quality. Progress in this field has
been driven by large datasets of face images [8, 72, 73, 82],
3D models of faces [2, 10], face landmark detectors [6, 39],
facial landmark detection [6, 39], neural radiance fields
(NeRF) [14, 18, 28, 32, 42, 53, 75], and diffusion mod-
els [4, 15, 46, 51, 52, 59, 70, 79]. There are several ap-
proaches to generate talking head videos. (1). 3D Face
Model-Based Methods [40, 63]: These methods use 3D
models to track and recreate facial expressions. For exam-
ple, Face2Face [63] tracks expressions from a source video
and applies them to another face. (2). Direct Synthesis-
Based Models [7, 76, 77]: These models create faces by
transforming encoded representations of appearance and
motion into images. They often use latent motion expres-
sions but may also rely on explicit techniques like optical
flow. (3). 3D Mesh-Based Methods [12, 26]: These meth-
ods use detailed 3D models to create lifelike head avatars
from videos. (4). NeRF-Based Methods [14, 18, 18, 32,
53]: These utilize NeRF to represent head geometry and
appearance in a novel 3D format. While effective, they can
be complex and less versatile across different identities. (5).
Warping-Based Methods [17, 50, 66, 69, 73]: These meth-
ods estimate motion fields to warp feature maps and gen-
erate images, often using learned keypoints. (6). Diffusion
Model-Based Methods [9, 54, 60]: These methods create
realistic videos from a single image and an audio sequence,
enhancing lip-sync, video fidelity, and the naturalness of
Implicit Motion
Function Blocks
Encoder Decoderappearance
feature
Latent Token
EncoderFrame
DecoderDense Feature
Encoder
current
latent tokenreference
latent tokenreference frame
current frame
 recon. current frame
Figure 2. The overview of our method, which is encoder-decoder
style. The reference latent token tris obtained with the Latent
Token Encoder. The fl
randtrare shared across the whole video.
head movements and poses. However, they involve complex
training and inference pipelines, which can be a challenge
in terms of computational resources and efficiency. Despite
the successes of these methods, there are still opportuni-
ties for improvement, especially in handling non-facial data
and enhancing editability for talking head videos. This pa-
per aims to explore new approaches in video modeling that
maintain high quality and efficiency while being adaptable
to different models.
2.2. Neural Video Codec
Many neural video codecs [3, 34, 36, 38, 74] adopt the
widely-used residual coding-based framework, where the
predicted frame is firstly generated via motion estima-
tion and motion compensation, and then the residual is
coded. They aim to improve the prediction accuracy as
much as possible, via scale-space flow [3], block-level
prediction[36], and so on. Instead of generating pixel-
domain predicted frame, the emerging conditional coding-
based codecs [27, 29–31, 37, 55] extract feature-domain
temporal context to help encoding, decoding, and the en-
tropy model. However, most of them still rely on optical
flow to generate the prediction or temporal context. Coding
the optical flow requires non-trivial bitrate cost, even with
the help of the learned entropy model [29–31]. By contrast,
our IMF learns sparse latent token, which is naturally com-
pact and efficient.
3. Proposed Method
3.1. Overview
An overview of our framework is shown in Fig. 2, which
follows the encoder-decoder style. The encoder contains
thedense feature encoder EFand the latent token encoder
ET. The decoder is composed of several implicit motion
function (IMF ) blocks and the frame decoder DF. Given
input video {xc}c∈Ωand a reference frame xr, the appear-
ance of the reference frame can be encoded into frbyEF,
and the correlation between xcanxrcan be represented by
19280
Implicit Motion
FunctionImplicit Motion
FunctionImplicit Motion
FunctionCross
AttentionTransformer
BlocksEncoder Decoder
current
latent tokenreference
latent tokenlatent tokenmotion feature
style modulationconst
reference frame
appearance feature
current frame recon. current frame
Latent Token Encoder Frame DecoderDense Feature Encoder Latent Token Decoder Implicit Motion Alignment
flatten unflattenMMM
M fu
ufff
Implicit Motion Function
Res
BlocksRes
Blocks
Figure 3. The illustration of our framework: a) the encoder contains the dense feature encoder EFand the latent token encoder ET; b)
the decoder contains the implicit motion function IMF and the feature decoder DF. IMF includes the latent token decoder IMF Dand the
implicit motion alignment IMF A. The motion features ml
randml
care produced by IMF Dfromtrandtc, independently.
the tokens pair (tc, tr)extracted respectively by ETfrom
xcandxr. The decoder DFaims to faithfully reconstruct
theˆxcwith(tc, tr, fr). The pipeline can be formulated as:
ˆxc=DF(IMF(tc, tr, fr))with
tc=ET(xc), tr=ET(xr), f r=EF(xr).(1)
Note that the reference frame xris not necessarily se-
lected from the input sequence {xc}. Taking the talking
head generation as an example, our pipeline can support the
reference having a different ID with the input. Furthermore,
different from the former works such as optical flow, the
correlation modeling by (tc, tr)does not require the refer-
ence input and the current input has the same dimension,
not even the same modality.
In our framework, to learn sparse yet essential informa-
tion, we will constrain the information flow in tcandtr.
The optimization target can be formulated as:
min L 
ˆxc, xc
s.t.|t|< ϵ, t ∈ {tc, tr},(2)
where Lis the function that measures the similarity between
ˆxcandxc.|t|is the size of the latent tokens, and ϵis the size
limitation. To achieve this objective, we specially design
the IMF at decoder to obtain high-fidelity ˆxc. A detailed
illustration of our full pipeline is shown in Fig. 3. The
remaining part of this section will be organized as follows:
In Sec. 3.2, we briefly introduce the encoder details of EF
andET. In Sec. 3.3, we describe the proposed IMF. Sec. 3.4
demonstrates the editability of the latent tokens on talkinghead generation. In Sec. 3.5, we delve into the design and
underlying rationale of our proposed framework.
3.2. Dense Feature & Latent Token Encoder
Dense Feature Encoder. Given a reference frame xr∈
RH×W×3, the dense feature encoder EFextracts multi-
scale feature fl
r∈Rhl×wl×dl, l∈ {1, L}, where lis the
layer index and Lis number of layers. hl, wlis spatial size
of the feature. dlis the depth dimension of the feature. The
EFis only performed on the reference frame, which means
the features fl
rare shared across the whole video if the ref-
erence frame is constant.
Latent Token Encoder. The latent token encoder ETen-
codes the reference frame and the current frame to trand
tc, independently. It maps the space RH×W×3⇒Rd.Sim-
ilar to fr, when the reference frame is constant, tris also
shared across the entire video. When d=K×2, where
Kis the number of keypoints, our tokens can be viewed as
the explicit coordinates representations from FOMM [57].
However, due to the limited information expressed by coor-
dinates, the expressive capability of this kind of represen-
tation is limited, especially in terms of semantic integrity,
i.e. the completeness to faithfully reconstruct the original
frame. By contrast, we choose d= 1×dm, a latent vector
to ensure both completeness and compactness. We further
discuss in Sec. 3.5 the choice between explicit and implicit
representations of tcand reveal the pros and cons of each in
the ablation experiments.
19281
3.3. Implicit Motion Function
With the latent token trand the appearance feature frex-
tracted from the reference frame, the target of IMF is to ob-
tain the appearance feature fcof the current frame from its
latent token tc. Specifically, the IMF is mainly composed
of two parts. First, we design a latent token decoder IMF D
module to transform the highly compact latent tokens into
spatially aligned motion features that correspond with the
frame. Subsequently, we utilize implicit motion alignment
IMF Amodule to align and refine the appearance feature of
the reference frame to the current frame.
Latent Token Decoder. TheIMF Ddecodes the compact
tokens tr,tcto multiple motion features ml
r∈Rhl×wl×dl
andml
c∈Rhl×wl×dlwhere lis the layer index. The “style
modulation” utilizes Weight Modulation and Demodula-
tion technique of StyleGAN2 [25], scales the convolution
weights with the latent token, and normalizes them to unit
standard deviation. Owing to the varying granularities, our
latent tokens effectively compress multi-scale information,
serving as a comprehensive representation. Furthermore,
the fully implicit nature of our representation allows for
flexible adjustment of the latent token dimension to accom-
modate different scenarios. Different from keypoint-based
explicit representation [41, 57, 66], where the motion fea-
tures are Gaussian heatmaps converted from the keypoints,
our design enjoys better scalability and capability due to the
latent token being directly learned by the encoder instead of
coordinates with a limited value range.
Implicit Motion Alignment. As shown in Fig.3, with the
motion features ml
randml
c, theIMF Awill align the ref-
erence appearance features fl
rto the current frame. The
IMF Acontains two parts including the cross-attention [65]
and transformer. The cross-attention module is imple-
mented with scaled dot-product cross-attention. This atten-
tion module takes motion features ml
c, ml
r, and appearance
features fl
rasQ, K , and V, respectively. The features are
first flattened, then the positional embeddings Pq,Pkare
added to the queries and keys, We compute the dot prod-
ucts of the queries with keys, divide each by√dk, and ap-
ply a softmax function to obtain the weights on the val-
ues. Then the output-aligned values are computed through
matrix multiplication. With the aligned values V′, we fur-
ther refine them using multi-head self-attention and feed-
forward network-based Transformer blocks [65], and finally
obtain the appearance features fl
cof the current frame.
3.4. Token Manipulation
In contrast to the explicit optical flow method, the implicit
representation offers a distinct advantage in terms of ed-
itability. As the latent token is not task-specific, for a new
controllable generation task, the decoder can be fixed and
just train a small adapter with a small cost. Taking talk-
ing head generation as an example, in the training of thelatent space, the full network is only trained to reconstruct
the current frame. After the latent space is trained, we can
edit learned tokens through another independent token ma-
nipulation network. Formally, with an editing module ψ,
source frame xs, and control condition h, we can rewrite
Eq. 1 to obtain the edited feature map ˜fsas:
˜fs=IMF 
ψ(ts|h), ts, fs
. (3)
Passing ˜fsto the frame decoder trained before, we can get
the edited frame ˜xs. In our experiments, we implement ψ
with two MLP encoders to encode the source token and con-
trol condition ( e.g., the 3DMM face coefficients), and one
MLP decoder to output the edited token.
3.5. Discussion
Explicit or Implicit. Based on the foundational works
MonkeyNet [56], FOMM [57], and MRAA [58], intro-
duced by Siarohin et al., several works employ explicit key-
points [17, 66], landmarks [11, 81], regions [58] or vec-
tors [67] as intermediate representations for direct opti-
cal flow estimation. Given the pivotal role of optical flow
in elucidating relationships between video frames, this ex-
plicit approach has become the predominant methodology
in video modeling. However, explicit representations are
inherently constrained. Optical flow, being a dense rep-
resentation, poses optimization challenges due to the per-
pixel operations in grid sampling. While keypoints offer
sparsity, they compromise on detailed motion information.
Additionally, keypoints are effective for quasi-rigid struc-
tures like human heads but falter in general video tasks due
to the diverse shapes and motion patterns, which are diffi-
cult to encapsulate using explicit methods. Another notable
drawback of keypoints is their limited scalability, where in-
creased point usage for finer motion details often leads to
redundancy and artifact-prone outputs.
Encoder-centric (EC) or Decoder-centric (DC). Here, we
discuss the placement of correlation modeling, whether it
is within the encoder or the decoder. Most existing video
compression and video modeling methods belong to EC,
which requires a complex design for encoder. For example,
the functioning of a video codec encoder is contingent on
the decoder output to obtain the prediction or context. Such
intertwined framework necessitates meticulous tuning. For
instance, the previous SOTA codec DCVC-DC [31, 55] in-
corporates upwards of ten distinct complicate strategies dur-
ing training phases. In contrast, recent advancements in
Large Language Models (LLMs) [48] usually utilize a DC
architecture to support diverse tasks. This paradigm shift
prompts reconsideration of similar frameworks for video
codec. Actually, Wyner-Ziv coding [71] has given the theo-
retical support that the rate of DC framework can be identi-
cal with that of the EC framework. Our IMF design aligns
with successful DC-based LLMs, achieving significant en-
hancements in video modeling.
19282
Reference
 Current
 FOMM [57]
 MRAA [58]
 Face-Vid2Vid [66]
 LIA [67]
 IW [41]
 PECHead [11]
 IMF
Figure 4. Comparison of talking head video reconstruction results obtained by the proposed method and previous SOTA approaches. The
main differences between reference frames and current frames are highlighted in circles.
Table 1. Quantitative results of video reconstruction on talking
head datasets.
MethodsCelebV-HQ [82] VFHQ [72]
L1 SSIM PSNR FID LPIPS L1 SSIM PSNR FID LPIPS
MRAA [58] 0.0568 0.777 22.33 64.23 0.1539 0.0454 0.812 22.60 40.17 0.1117
F-Vid2Vid [66] 0.0589 0.746 21.56 67.40 0.1889 0.0491 0.804 21.79 41.95 0.1054
LIA [67] 0.0654 0.754 20.75 65.15 0.1541 0.0537 0.815 21.47 42.27 0.1099
IW [41] 0.0539 0.801 22.67 57.65 0.1287 0.0426 0.862 23.46 34.05 0.0823
PECHead [11] 0.0552 0.803 24.29 56.68 0.1372 0.0435 0.859 23.03 31.20 0.0847
IMF 0.0417 0.849 25.28 53.22 0.1057 0.0283 0.918 26.86 23.35 0.0540
4. Experiments
Datasets. For talking head datasets, CelebVHQ [82] and
VFHQ [72] are used. In addition, to verify the universality
of our model, we also test three general datasets: Flower,
Wavecloth and Foliage1. For visual results of the non-face
datasets, we obtain in-the-wild video from the Internet to
verify the generalization ability.
Implementation details. We use 256×256as the frame
spatial size. The latent token dimension is dm= 32 for all
experiments except for ablation studies. During the train-
ing, the perceptual Loss [23] and GAN loss [21, 33] are
also used. More training details and network structure of
each module are in supplementary materials.
Baselines. We compare four representative works:
FOMM [57], MRAA [58], Face-Vid2Vid [66], and PEC-
Head [11] on talking head data. Besides FOMM [57] and
MRAA [58], we compare TPSMM [81], LIA [67] and
IW [41] on general data. These methods were primarily
proposed for face data, but can also be applied for general
data because the models do not explicitly rely on prior
knowledge of face.
Metrics. We use L1, SSIM [68] and PSNR to evaluate the
1For more information about these datasets, please contact us.Table 2. Quantitative results of video reconstruction on general
datasets.
MethodsFlower Wavecloth Foliage
L1 SSIM LPIPS L1 SSIM LPIPS L1 SSIM LPIPS
FOMM [57] 0.0837 0.620 0.2559 0.0749 0.684 0.2009 0.0801 0.6881 0.1761
MRAA [58] 0.0792 0.632 0.2460 0.0665 0.710 0.1798 0.0757 0.7180 0.1925
TPSMM [81] 0.0642 0.704 0.2097 0.0625 0.716 0.1537 0.0626 0.7771 0.1545
LIA [67] 0.0848 0.589 0.2453 0.0733 0.682 0.1868 0.0826 0.6619 0.1679
IW [41] 0.0639 0.736 0.1983 0.0566 0.790 0.1380 0.0697 0.7427 0.1479
IMF 0.0544 0.761 0.1746 0.0473 0.804 0.1265 0.0548 0.8235 0.1209
low-level similarity. FID [16] and LPIPS [80] are used to
assess the perceptual quality. For video compression, BPP
(bits per pixel) is used to measure the bit-stream size.
4.1. Video Reconstruction
Tab. 1 and Tab. 2 show the quantitative comparisons with
previous SOTA models. Qualitative comparisons are pre-
sented in the Fig. 4 and Fig. 5. More results and videos
are in supplemental material. These results show that our
method significantly outperforms previous SOTA methods.
For talking head data, although previous methods can ap-
proximately reconstruct the correct facial area, only our
model accurately reconstructs the non-facial areas, such
as hair and microphones. Our model also performs best
for local motion changes, like blinking. For general data,
our method is able to precisely reconstruct various con-
tents with complex motion, such as blooming. However,
the methods based on explicit representation and align-
ment [56, 57, 81] perform poorly in cases of drastic con-
tent changes and also have inferior modeling capabilities for
subtle motions, such as minor positional changes in leaves
and flowers. Implicit warping-based method IW [41] is bet-
ter than previous methods, but the keypoints are still used. It
19283
Reference
 Current
 FOMM [57]
 MRAA [58]
 TPSMM [81]
 LIA [67]
 IW [41]
 IMF
Figure 5. Comparison of general video reconstruction results obtained by the proposed method and previous SOTA approaches. The main
differences between reference frames and current frames are highlighted in circles.
0.00 0.01 0.02 0.03 0.04
BPP (bits per pixel)0.800.820.840.860.880.900.920.940.961 - LPIPSUpper left is better
Face-Vid2Vid
PECHead
IW
IMF-16
IMF-32
IMF-64
IMF-128
IMF-16-online
IMF-32-online
IMF-64-online
IMF-128-online
VTM
DCVC-DC
Figure 6. The rate-distortion curve on video compression.
lacks robustness, leading to the appearance of jitter artifacts
in the videos.
4.2. Video Compression
To verify the efficiency and semantic completeness of our
latent tokens, we also test the video compression task us-
ing VFHQ [72] dataset. The rate-distortion curves are pre-
sented in Fig. 6. Besides the existing methods of talking
head generation, we also compare the best standard codec
H.266/VTM [1, 5] and previous SOTA neural video codec
DCVC-DC [31]. For the results of all methods, we only
Reference
 Current
 TPS [81]
 LIA [67]
 IW [41]
 IMF
Figure 7. Comparison with explicit methods on both talking head
and general datasets.
calculate the current BPP (bits per pixel). Our first ap-
proach is directly applying an offline array compressor [35]
to compress the latent tokens, and we test different token
sizes dm∈{16,32,64,128}, denoted as IMF- dm. To verify
the compactness of the latent token, we also follow neu-
ral codec and implement an online learned entropy model
[29, 31], where the token of the previous frame is used to
predict the distribution of that of the current frame and then
the arithmetic coding is applied. It is denoted as IMF- dm-
online. From these results, we can see that our method is
significantly better than all previous methods in terms of
rate-distortion trade-off. Furthermore, the comparison with
the online entropy model also reveal that our latent tokens
are naturally compact, because the online entropy model
only bring a very small bitrate reduction but with non-trivial
quality degradation.
19284
PECHead
 IMF
Input
 Pitch
 Roll
 Yaw
 FOV
 Input
 Pitch
 Roll
 Yaw
 Expression
Figure 8. Comparison of talking head editing results obtained by the proposed method and the previous SOTA PECHead [11].
Table 3. Quantitative results for ablation studies. IR is implicit
motion representation and IA is implicit motion alignment.
Settings Flower VFHQ [72]
IR IA L1 SSIM PSNR LPIPS L1 SSIM FID LPIPS
0.0642 0.704 22.23 0.2097 0.0435 0.859 31.20 0.0847
✓ 0.0604 0.693 21.58 0.1921 0.0396 0.877 29.84 0.0612
✓ 0.0578 0.758 22.46 0.1762 0.0366 0.885 30.08 0.0713
✓ ✓ 0.0544 0.761 23.33 0.1746 0.0283 0.918 23.35 0.0540
4.3. Comparison with Explicit Methods
As shown in Fig. 7, we compare previous SOTA explicit
methods on both talking head and general datasets. The op-
tical flow-based TPS [81] cannot handle the correlation of
facial and non-facial content, or new content, as the micro-
phone and flower are not correctly modeled. The latent mo-
tion decomposition-based LIA [67] containing explicit flow
in the model, fails when the corresponding regions are small
or the motion is complex. The keypoints-based IW [41]
cannot handle the local motion and can also cause jittering
in the videos.
4.4. Token Editing
In our framework, token manipulation facilitates the edit-
ing of facial features, enabling precise control over aspects
such as head pose and facial expression. To demonstrate the
generalization capability of our method, we applied token
editing to images from the FFHQ dataset [24]. We bench-
mark our approach against previous SOTA explicit model
for talking head editing, PECHead [11]. Qualitative results,
as depicted in Fig. 8, demonstrate that our method not only
parallels the editing performance of PECHead [11] but also
surpasses it in terms of realism and naturalness, especially
in intricate areas such as the eyes, mouth, and ears.
4.5. Ablation Studies
We compare the implicit motion representation and align-
ment with the explicit motion representation ( i.e., key-
points) and alignment ( i.e., optical flow-based grid sample).
For explicit motion representation and alignment method on
general dataset Flower, we use the TPSMM [81] as the base-
line model. For the talking head dataset VFHQ [72], we use
the PECHead [11] as the baseline model. For implicit mo-
tion representation and explicit motion alignment, we keep
Reference
 Current
 ER + EA
 IR + EA
 ER + IA
 IR + IA
Figure 9. Ablation studies on both talking head and general
datasets. The IR and ER stand for implicit and explicit motion
representation. The IA and EA stand for implicit and explicit mo-
tion alignment. The IR + IA is our full model.
the latent token encoder and decoder, and replace the im-
plicit motion alignment with an optical flow estimator and
flow-based grid sample to align the motion feature. For ex-
plicit motion representation and implicit motion alignment,
we replace the latent token encoder with a keypoints extrac-
tor and the latent motion decoder with explicit keypoints
to Gaussian heatmap [57], while keeping the implicit mo-
tion alignment. The results are shown in the Tab. 3 and
Fig. 9. The results indicate that explicit representation can
only model the face for facial data and is incapable of mod-
eling hands: it either fails to generate results with hands, or
it does not correctly reconstruct the position of the hands.
The results on flower data also demonstrate the shortcom-
ings of explicit representation and alignment: they either
fail to capture complex motions or are unable to accurately
reconstruct the frames.
5. Conclusion
This paper introduces the Implicit Motion Function (IMF),
an innovative approach that advances beyond traditional op-
tical flow in video modeling and generation. The implicit
methodology of IMF not only achieves high-fidelity video
reconstruction, especially in non-facial and new content re-
gions, but also demonstrates a remarkable compression ra-
tio improvement. Its capacity for directly editable latent to-
kens paves the way for superior editing performance. We
believe IMF represents a meaningful step forward in video
modeling, potentially inspiring further research and devel-
opment in video reconstruction and generation.
19285
References
[1] VTM-17.0. https://vcgit.hhi.fraunhofer.de/
jvet/VVCSoftware_VTM/ . 7
[2] Madhav Agarwal, Rudrabha Mukhopadhyay, Vinay P Nam-
boodiri, and CV Jawahar. Audio-visual face reenactment. In
Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision , pages 5178–5187, 2023. 3
[3] Eirikur Agustsson, David Minnen, Nick Johnston, Johannes
Balle, Sung Jin Hwang, and George Toderici. Scale-space
flow for end-to-end optimized video compression. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 8503–8512, 2020. 3
[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18392–18402, 2023.
3
[5] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle
Chen, Gary J Sullivan, and Jens-Rainer Ohm. Overview of
the versatile video coding (vvc) standard and its applications.
IEEE Transactions on Circuits and Systems for Video Tech-
nology , 31(10):3736–3764, 2021. 7
[6] Adrian Bulat and Georgios Tzimiropoulos. How far are we
from solving the 2d & 3d face alignment problem?(and a
dataset of 230,000 3d facial landmarks). In Proceedings
of the IEEE International Conference on Computer Vision ,
pages 1021–1030, 2017. 3
[7] Egor Burkov, Igor Pasechnik, Artur Grigorev, and Vic-
tor Lempitsky. Neural head reenactment with latent pose
descriptors. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 13786–
13795, 2020. 3
[8] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman.
V oxceleb2: Deep speaker recognition. arXiv preprint
arXiv:1806.05622 , 2018. 3
[9] Chenpeng Du, Qi Chen, Tianyu He, Xu Tan, Xie Chen, Kai
Yu, Sheng Zhao, and Jiang Bian. Dae-talker: High fidelity
speech-driven talking face generation with diffusion autoen-
coder. In Proceedings of the 31st ACM International Con-
ference on Multimedia , pages 4281–4289, 2023. 3
[10] Bernhard Egger, William AP Smith, Ayush Tewari, Stefanie
Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard,
Timo Bolkart, Adam Kortylewski, Sami Romdhani, et al.
3d morphable face models—past, present, and future. ACM
Transactions on Graphics (ToG) , 39(5):1–38, 2020. 3
[11] Yue Gao, Yuan Zhou, Jinglu Wang, Xiao Li, Xiang Ming,
and Yan Lu. High-fidelity and freely controllable talking
head video generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 5609–5619, 2023. 1, 2, 5, 6, 8
[12] Philip-William Grassal, Malte Prinzler, Titus Leistner,
Carsten Rother, Matthias Nießner, and Justus Thies. Neural
head avatars from monocular rgb videos. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 18653–18664, 2022. 3
[13] Ivan Grishchenko, Artsiom Ablavatski, Yury Kartynnik,
Karthik Raveendran, and Matthias Grundmann. Attentionmesh: High-fidelity face mesh prediction in real-time. arXiv
preprint arXiv:2006.10962 , 2020. 3
[14] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun
Bao, and Juyong Zhang. Ad-nerf: Audio driven neural ra-
diance fields for talking head synthesis. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 5784–5794, 2021. 3
[15] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your
personalized text-to-image diffusion models without specific
tuning. arXiv preprint arXiv:2307.04725 , 2023. 3
[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 6
[17] Fa-Ting Hong, Longhao Zhang, Li Shen, and Dan Xu.
Depth-aware generative adversarial network for talking head
video generation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
3397–3406, 2022. 3, 5
[18] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juy-
ong Zhang. Headnerf: A real-time nerf-based parametric
head model. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 20374–
20384, 2022. 3
[19] Xiaotao Hu, Zhewei Huang, Ailin Huang, Jun Xu, and
Shuchang Zhou. A dynamic multi-scale voxel flow network
for video prediction. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6121–6131, 2023. 1
[20] Zhihao Hu, Guo Lu, Jinyang Guo, Shan Liu, Wei Jiang, and
Dong Xu. Coarse-to-fine deep video coding with hyperprior-
guided mode prediction. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 5921–5930, 2022. 1
[21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125–1134,
2017. 6
[22] Geethu Miriam Jacob and Bjorn Stenger. Facial action
unit detection with transformers. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7680–7689, 2021. 3
[23] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. In
European conference on computer vision , pages 694–711.
Springer, 2016. 6
[24] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4401–4410, 2019. 8
[25] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 8110–8119, 2020. 5
19286
[26] Taras Khakhulin, Vanessa Sklyarova, Victor Lempitsky, and
Egor Zakharov. Realistic one-shot mesh-based head avatars.
InComputer Vision–ECCV 2022: 17th European Confer-
ence, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
Part II , pages 345–362. Springer, 2022. 3
[27] Th ´eo Ladune, Pierrick Philippe, Wassim Hamidouche, Lu
Zhang, and Olivier D ´eforges. Conditional coding for flexible
learned video compression. In Neural Compression: From
Information Theory to Applications – Workshop @ ICLR ,
2021. 3
[28] Seoyoung Lee, Seongsu Ha, and Joonseok Lee. Disentan-
gled audio-driven nerf: Talking head generation with de-
tailed identity-specific microexpressions. 3
[29] Jiahao Li, Bin Li, and Yan Lu. Deep contextual video com-
pression. Advances in Neural Information Processing Sys-
tems, 34, 2021. 3, 7
[30] Jiahao Li, Bin Li, and Yan Lu. Hybrid spatial-temporal en-
tropy modelling for neural video compression. In Proceed-
ings of the 30th ACM International Conference on Multime-
dia, 2022.
[31] Jiahao Li, Bin Li, and Yan Lu. Neural video compression
with diverse contexts. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, CVPR 2023, Vancou-
ver, Canada, June 18-22, 2023 , 2023. 1, 3, 5, 7
[32] Weichuang Li, Longhao Zhang, Dong Wang, Bin Zhao, Zhi-
gang Wang, Mulin Chen, Bang Zhang, Zhongjian Wang,
Liefeng Bo, and Xuelong Li. One-shot high-fidelity talking-
head synthesis with deformable neural radiance field. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 17969–17978, 2023. 3
[33] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv
preprint arXiv:1705.02894 , 2017. 6
[34] Jianping Lin, Dong Liu, Houqiang Li, and Feng Wu. M-
LVC: multiple frames prediction for learned video compres-
sion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , 2020. 3
[35] Peter Lindstrom. Fixed-Rate Compressed Floating-Point Ar-
rays, 2014. 7
[36] Bowen Liu, Yu Chen, Rakesh Chowdary Machineni, Shiyu
Liu, and Hun-Seok Kim. Mmvc: Learned multi-mode
video compression with block-based prediction mode selec-
tion and density-adaptive entropy coding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 18487–18496, 2023. 3
[37] Jerry Liu, Shenlong Wang, Wei-Chiu Ma, Meet Shah, Rui
Hu, Pranaab Dhawan, and Raquel Urtasun. Conditional en-
tropy coding for efficient video compression. In European
Conference on Computer Vision , pages 453–468. Springer,
2020. 3
[38] Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei
Cai, and Zhiyong Gao. DVC: an end-to-end deep video com-
pression framework. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
11006–11015, 2019. 1, 3
[39] Jiangjing Lv, Xiaohu Shao, Junliang Xing, Cheng Cheng,
and Xi Zhou. A deep regression architecture with two-stage
re-initialization for high performance facial landmark detec-tion. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 3317–3326, 2017. 3
[40] Luming Ma and Zhigang Deng. Real-time hierarchical fa-
cial performance capture. In Proceedings of the ACM SIG-
GRAPH Symposium on Interactive 3D Graphics and Games ,
pages 1–10, 2019. 3
[41] Arun Mallya, Ting-Chun Wang, and Ming-Yu Liu. Implicit
warping for animation with image sets. Advances in Neural
Information Processing Systems , 35:22438–22450, 2022. 2,
3, 5, 6, 7, 8
[42] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021.
3
[43] OpenAI. Chatgpt. https://chat.openai.com , 2023.
2
[44] Sergiu Oprea, Pablo Martinez-Gonzalez, Alberto Garcia-
Garcia, John Alejandro Castro-Vargas, Sergio Orts-
Escolano, Jose Garcia-Rodriguez, and Antonis Argyros. A
review on deep learning techniques for video prediction.
IEEE Transactions on Pattern Analysis and Machine Intel-
ligence , 44(6):2806–2826, 2020. 1
[45] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Jun-
tao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen,
and Yujun Shen. Codef: Content deformation fields for
temporally consistent video processing. arXiv preprint
arXiv:2308.07926 , 2023. 1
[46] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun
Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image
translation. In ACM SIGGRAPH 2023 Conference Proceed-
ings, pages 1–11, 2023. 3
[47] Albert Pumarola, Antonio Agudo, Aleix M Martinez, Al-
berto Sanfeliu, and Francesc Moreno-Noguer. Ganimation:
Anatomically-aware facial animation from a single image. In
Proceedings of the European conference on computer vision
(ECCV) , pages 818–833, 2018. 3
[48] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
Amodei, and Ilya Sutskever. Language models are unsuper-
vised multitask learners. 2019. 2, 5
[49] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and
Michael J Black. Generating 3d faces using convolutional
mesh autoencoders. In Proceedings of the European confer-
ence on computer vision (ECCV) , pages 704–720, 2018. 3
[50] Yurui Ren, Ge Li, Yuanqi Chen, Thomas H Li, and Shan
Liu. Pirenderer: Controllable portrait image generation via
semantic neural rendering. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 13759–
13768, 2021. 3
[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 3
[52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
19287
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500–
22510, 2023. 3
[53] Shuai Shen, Wanhua Li, Xiaoke Huang, Zheng Zhu, Jie
Zhou, and Jiwen Lu. Sd-nerf: Towards lifelike talking head
animation via spatially-adaptive dual-driven nerfs. IEEE
Transactions on Multimedia , 2023. 3
[54] Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng
Zhu, Jie Zhou, and Jiwen Lu. Difftalk: Crafting diffusion
models for generalized audio-driven portraits animation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 1982–1991, 2023. 3
[55] Xihua Sheng, Jiahao Li, Bin Li, Li Li, Dong Liu, and Yan
Lu. Temporal context mining for learned video compression.
IEEE Transactions on Multimedia , 2022. 3, 5
[56] Aliaksandr Siarohin, St ´ephane Lathuili `ere, Sergey Tulyakov,
Elisa Ricci, and Nicu Sebe. Animating arbitrary objects via
deep motion transfer. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
2377–2386, 2019. 5, 6
[57] Aliaksandr Siarohin, St ´ephane Lathuili `ere, Sergey Tulyakov,
Elisa Ricci, and Nicu Sebe. First order motion model for im-
age animation. Advances in Neural Information Processing
Systems , 32, 2019. 2, 3, 4, 5, 6, 7, 8
[58] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei
Chai, and Sergey Tulyakov. Motion representations for ar-
ticulated animation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
13653–13662, 2021. 3, 5, 6, 7
[59] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 3
[60] Michał Stypułkowski, Konstantinos V ougioukas, Sen He,
Maciej Zieba, Stavros Petridis, and Maja Pantic. Diffused
heads: Diffusion models beat gans on talking-face genera-
tion. arXiv preprint arXiv:2301.03396 , 2023. 3
[61] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow. In European conference on com-
puter vision , pages 402–419. Springer, 2020. 1
[62] A Murat Tekalp. Digital video processing . Prentice Hall
Press, 2015. 1
[63] Justus Thies, Michael Zollhofer, Marc Stamminger, Chris-
tian Theobalt, and Matthias Nießner. Face2face: Real-time
face capture and reenactment of rgb videos. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 2387–2395, 2016. 3
[64] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan
Kautz. Mocogan: Decomposing motion and content for
video generation. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1526–1535,
2018. 1
[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 2, 3, 5
[66] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot
free-view neural talking-head synthesis for video conferenc-ing. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 10039–10049,
2021. 1, 2, 3, 5, 6
[67] Yaohui Wang, Di Yang, Francois Bremond, and Antitza
Dantcheva. Latent image animator: Learning to ani-
mate images via latent space navigation. arXiv preprint
arXiv:2203.09043 , 2022. 3, 5, 6, 7, 8
[68] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing ,
13(4):600–612, 2004. 6
[69] Olivia Wiles, A Koepke, and Andrew Zisserman. X2face: A
network for controlling face generation using images, audio,
and pose codes. In Proceedings of the European conference
on computer vision (ECCV) , pages 670–686, 2018. 3
[70] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu
Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning
of image diffusion models for text-to-video generation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7623–7633, 2023. 3
[71] Aaron Wyner and Jacob Ziv. The rate-distortion function for
source coding with side information at the decoder. IEEE
Transactions on information Theory , 22(1):1–10, 1976. 5
[72] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong,
and Ying Shan. Vfhq: A high-quality dataset and bench-
mark for video face super-resolution. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 657–666, 2022. 3, 6, 7, 8
[73] Kewei Yang, Kang Chen, Yuan-Chen Guo, Daoliang Guo,
Song-Hai Zhang, and Weidong Zhang. Face2faceρ: Real-
time high-resolution one-shot face reenactment. In European
conference on computer vision . Springer, 2022. 3
[74] Ren Yang, Fabian Mentzer, Luc Van Gool, and Radu Tim-
ofte. Learning for video compression with recurrent auto-
encoder and recurrent probability model. IEEE Journal of
Selected Topics in Signal Processing , 15(2):388–401, 2021.
3
[75] Shunyu Yao, RuiZhe Zhong, Yichao Yan, Guangtao Zhai,
and Xiaokang Yang. Dfa-nerf: Personalized talking head
generation via disentangled face attributes neural rendering.
arXiv preprint arXiv:2201.00791 , 2022. 3
[76] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and
Victor Lempitsky. Few-shot adversarial learning of realistic
neural talking head models. In Proceedings of the IEEE/CVF
international conference on computer vision , pages 9459–
9468, 2019. 3
[77] Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya,
and Victor Lempitsky. Fast bi-layer neural synthesis of one-
shot realistic head avatars. In European Conference on Com-
puter Vision , pages 524–540. Springer, 2020. 3
[78] Jing Zhang, Zhe Chen, and Dacheng Tao. Towards high per-
formance human keypoint detection. International Journal
of Computer Vision , 129(9):2639–2662, 2021. 3
[79] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 3
19288
[80] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 6
[81] Jian Zhao and Hui Zhang. Thin-plate spline motion model
for image animation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
3657–3666, 2022. 1, 2, 3, 5, 6, 7, 8
[82] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei
Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebv-
hq: A large-scale video facial attributes dataset. arXiv
preprint arXiv:2207.12393 , 2022. 3, 6
19289
