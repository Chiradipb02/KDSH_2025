SIRA: Scalable Inter-frame Relation and Association for Radar Perception
Ryoma Yataka1,2, Pu Wang1, Petros Boufounos1, Ryuhei Takahashi2
1Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA 02139, USA
2Information Technology R&D Center, Mitsubishi Electric Corporation, Kanagawa 247-8501, Japan
{yataka,pwang,petrosb }@merl.com, Takahashi.Ryuhei@ab.MitsubishiElectric.co.jp
Abstract
Conventional radar feature extraction faces limitations
due to low spatial resolution, noise, multipath reflection,
the presence of ghost targets, and motion blur. Such limita-
tions can be exacerbated by nonlinear object motion, par-
ticularly from an ego-centric viewpoint. It becomes evident
that to address these challenges, the key lies in exploiting
temporal feature relation over an extended horizon and en-
forcing spatial motion consistency for effective association.
To this end, this paper proposes SIRA (Scalable Inter-frame
Relation and Association) with two designs. First, inspired
by Swin Transformer, we introduce extended temporal rela-
tion, generalizing the existing temporal relation layer from
two consecutive frames to multiple inter-frames with tem-
porally regrouped window attention for scalability. Second,
we propose motion consistency track with the concept of a
pseudo-tracklet generated from observational data for bet-
ter trajectory prediction and subsequent object association.
Our approach achieves 58.11mAP@0.5for oriented object
detection and 47.79MOTA for multiple object tracking on
the Radiate dataset, surpassing previous state-of-the-art by
a margin of +4.11mAP@0.5and+9.94MOTA, respec-
tively.
1. Introduction
Automotive perception involves the interpretation of the ex-
ternal driving environment and internal vehicle cabin con-
ditions with an array of perception sensors to achieve ro-
bust safety and driving autonomy [40]. Compared to opti-
cal camera and lidar sensors, radar is cost-effective, friendly
to sensor maintenance and calibration, and has distinct ad-
vantages in providing long-range perception capabilities in
adverse weather and lighting conditions [59].
Nevertheless, a notable limitation of radar-based auto-
motive perception is its low spatial resolution in the azimuth
and elevation domains, and its inherent noise including mul-
tipath reflection, ghost targets, and motion blur. As a re-
sult, its ability to detect and track objects lags behind the
SIRA TempoRadar
ID: 1
ID: 1
ID: 1‚úî‚úî‚úî
ID: 2
ID: 1‚úî
ID: 1ID: 2
ID: 1ID: 2
‚úò
ID: 2Figure 1. Conventional radar perception pipelines such as Tempo-
Radar [27] (Bottom Row) rely on a limited number (one or two) of
frames and the limited time horizon may lead to incorrect feature-
level and object-level association (e.g., t=T‚àí1) and propagate
to subsequent frames (e.g., t=T). In contrast, SIRA (Top Row)
accounts for joint spatio-temporal consistency over an extended
temporal horizon (e.g., all 3frames here), allowing for more ac-
curate association in nonlinear motion scenarios even in an ego-
centric viewpoint.
requirements for fully autonomous driving capabilities. Re-
cently, standalone radar-only perception has been investi-
gated in [1, 14, 27, 28, 38, 39, 60]. Li et al. [27] proposed a
framework called TempoRadar to study temporal attention
to features from 2ego-centric bird-eye-view (BEV) radar
frames. It has shown promising performance gains when
evaluated on the large-scale open Radiate [47] dataset.
However, such limitations can be exacerbated by non-
linear object motion, particularly from an ego-centric BEV .
In particular, low frame rates result in significant influence
from the nonlinearity of object motion, leading to frequent
tracking errors. Conventional radar perception pipelines
such as TempoRadar enables prediction based on informa-
tion from the previous frame, but in the case of objects with
fast and nonlinear motion within radar frames, such infor-
mation is inadequate (Bottom of Fig. 1). Although applying
Kalman filter (KF [24])-based algorithms [4, 8, 12, 62], is
possible, radar perception is difficult to relate accurately due
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
15024
to a complex combination of factors, including the effects of
high-speed nonlinear motion dynamics and the lack of de-
tailed appearance features due to low resolution. To address
these limitations and improve radar perception for object
detection and tracking, we propose a framework called scal-
able inter-frame relation & association (SIRA) . SIRA con-
sists of two modules: extended temporal relation (ETR) and
motion consistency track (MCTrack). The contributions of
this study are as follows:
‚Ä¢ We introduce ETR, generalizing the existing temporal
relation layer from two consecutive frames to multiple
inter-frames with temporally regrouped window attention
for scalability. It emphasizes the temporal consistency
of moving objects by enabling accurate detection while
maintaining computational efficiency over long time hori-
zon. This can facilitate easy detection through consistent
correlations across multiple frames at the object level.
‚Ä¢ We designed MCTrack based on the concept of pseudo-
tracklets, which are generated by using a learnable mod-
ule to predict the arbitral nonlinear motion of an object
between multiple frames, and the association caused by
these pseudo-tracklets enhances spatial consistency dur-
ing inference. Thus, MCTrack enables more reliable po-
sition predictions, even in scenarios with fast-moving ob-
jects and low frame rates.
‚Ä¢ We propose SIRA that adopts a loss function for the end-
to-end learning of these two modules, achieving stable
predictions that capture the spatio-temporal consistency
of nonlinear moving objects.
‚Ä¢ We evaluate SIRA on Radiate [47], a BEV radar dataset.
Our approach achieves 58.11mAP@0.5for oriented ob-
ject detection and 47.79MOTA for multiple object track-
ing on the Radiate dataset, surpassing previous state-
of-the-art by a margin of +4.11mAP@0.5and+9.94
MOTA, respectively.
2. Related Work for Radar Perception
Automotive radar predominantly employs a frequency-
modulated continuous waveform (FMCW) for object detec-
tion, generating point clouds. The fundamental of FMCW
is explained in Appendix 18. In addition, we defer a short
review of recent visual tracking in Appendix 6.
Detection by Radar: For automotive perception, radar-
assisted multimodal approaches were proposed [10, 29, 34,
42, 51, 55]. Compared with multimodals, standalone radar-
only perception has been studied in [1, 13, 14, 27, 28, 38,
39, 60]. A multi-view feature fusion method was proposed
in [14] to combine features from range-Doppler, range-
angle, and angle-Doppler radar heatmaps for object classifi-
cation. As opposed to single-frame radar feature extraction,
Li et al. [27] proposed TempoRadar with 2 frames.Mutiple Object Tracking by Radar: Object tracking
with radar has seen several proposals depending on the spar-
sity or density of the radar points obtained for each ob-
ject [40]. For sparse radar detection points, model-based
tracking algorithms have been explored in the context of
extended object tracking (EOT) [16]. They use Bayesian
filtering [3, 6, 17, 25, 37, 49, 53] to model the spatial distri-
bution of radar detection points across the vehicle‚Äôs range
and predict and update the extended states such as posi-
tion and velocity. Moreover, to address the nonlinearity
problem due to objects deviating from constant linear mo-
tion, algorithms such as extended KF [48] and unscented
KF [23] have been proposed to handle nonlinear motion us-
ing first- and third-order Taylor approximations. However,
these still rely on approximating the Gaussian prior distribu-
tion assumed by the KF, making modeling challenging for
movements where the next position is determined by hu-
man intent, such as in vehicles. Particle filter [18] addresses
nonlinear motion using a sampling-based posterior estima-
tion, which requires exponential computation. For high-
density radar detection points, following [58, 65], Tempo-
Radar extended the achieved strong tracking performance
through learning. Our proposed framework extends KF-
based methods and learning-based approaches by assuming
high-density radar detection points. It explicitly considers
strong object-level consistency by using multiple frames to
capture the nonlinear motion of objects.
3. Scalable Inter-frame Relation & Association
An overview of the SIRA framework is illustrated in Fig. 2
with two main modules: 1) ETR and 2) MCTrack. ETR fo-
cuses on the temporal consistency, while MCTrack captures
the spatial motion consistency, ensuring the continuity and
accuracy of object detection and tracking at the output.
3.1. Preliminary
Encoder: Radar perception pipelines employ an encoder
to transform the radar frame It‚ààR1√óH√óWinto high-level
features and accentuate the position of objects.
Zt:=FŒ∏(It)‚ààRC√óH
s√óW
s, (1)
where C,H,W, and srepresent the number of chan-
nels, height, width, and downsampling ratio over the spa-
tial dimension, respectively. FŒ∏(¬∑)is encoder such as
ResNet [19] with parameters Œ∏. By denoting multiple T
radar frames as I={It}T
t=1‚ààRT√óH√óW, we can obtain
informative features Zt=FŒ∏(I).
Decoder: The decoder estimates the bounding boxes
from the features. To localize objects, the two-dimensional
(2D) center coordinates (xt, yt)of the top- Kpeak values bct
15025
ETR
TWA TRWA
ùëáùëáEnc
 EncPositional
EncoderMCTrack
DEst
Deform 
ConvNormConv
DEst
Deform 
ConvNormConv
DEst
Deform 
ConvNormConv
Reverse Re -grouping & Patch MergingDec Dec Dec
Tracker
PredictionAssociationUpdateDeletionInitialization
Dec
x H1x H1
x L‚ãÆ
‚ãÆWindow grouping
Masked MCANormFNN
Cross 
AttentionMasked MCANormFNN
Cross 
Attention
‚ãÆMasked MCANormFNN
Cross 
Attention
Masked MCANormFNN
Cross 
Attentionx H2
x H2‚ãÆ‚ãÆ
‚ãÆ
‚ãÆ
Sub- frame partition & Re-grouping‚ãÆ
‚ãÆ
Fill Fill Fill Fill
‚ãÆFigure 2. The architecture of SIRA with two modules: 1) extended temporal relation (ETR) capturing the temporal feature consistency
while maintaining computational efficiency, and 2) motion consistency track (MCTrack) estimating pseudo-direction of objects during
training and establishing pseudo-tracklets for better association in inference. The detection loss LBBox
tand pseudo-direction loss LDEstare
used to train the pipeline end-to-end for object detection and tracking.
in the heatmap, corresponding width bwtand length bht, ori-
entation bœët, and 2D offsets (box,t,boy,t)are predicted as the
output bounding box of an object with decoder heads GŒ∏as:

xt, yt,bwt,bht,bœët,box,t,boy,t,bct‚ä§
=GŒ∏(Zt).(2)
One such decoder is the one used in CenterPoint [64].
Exploiting Temporality: For radar perception, it is nec-
essary enhance the feature extraction utilizing additional
properties from the temporal domain. One straightforward
way is to stack multiple frames as the input to the en-
coder, i.e., Zt=FŒ∏(I). To exploiting the feature-level
temporal relation, TempoRadar [27] introduces a tempo-
ral relation layer (TRL) that selects top- Kfeatures Ht‚àà
RC√óKfromZt:=FŒ∏(It,t‚àí1)andHt‚àí1‚ààRC√óKfrom
Zt‚àí1:=FŒ∏(It‚àí1,t), where It‚àí1,tconcatenates two con-
secutive radar frames along the channel dimension in the
order of (t‚àí1, t)with the following feature selector SK:
Ht=SK(Zt), t={t‚àí1, t}. (3)
By concatenating the 2Kselected features as Ht,t‚àí1=
[Ht,Ht‚àí1]‚ä§, TRL further computes masked multi-head
cross-attention (MCA) as
A(V,X) := softmax 
M+q(X)k(X)‚ä§
‚àö
d!
v(V)(4)
where V=Ht,t‚àí1,X=Hpos
t,t‚àí1is the concate-
nated feature Ht,t‚àí1supplemented by the positional encod-
ing,{q(¬∑), k(¬∑), v(¬∑)}are query/keys/values, and dis thequery/key dimension. The masking matrix Mis designed to
turn off the attention between features from the same frame
and allow for only cross-frame feature attention to ensure
temporal feature consistency.
These enhanced features are refilled back to ZtandZt‚àí1
at corresponding spatial coordinates and fed to the decoder
for object detection and tracking. Refer to Appendix 8 for
the top- Kfeature selector SKand the design of M.
3.2. ETR: Extended Temporal Relation
The ETR module borrows the concept of shifted window at-
tention in Swin Transformer [31] but in a deformable tem-
poral fashion. It generalizes the TRL over a longer time
horizon of consecutive frames with a scalable complexity.
In the following, we introduce the two main blocks: tem-
poral window attention (TWA) and temporally regrouped
window attention (TRWA) of ETR shown in Fig. 2.
Temporal Window Attention: Thel-th TWA layer ex-
pands the TRL from 2consecutive frames to a temporal
window of U‚â•2frames and computes masked MCA
within each window. In Fig. 3, we group U= 4 consec-
utive frames into one temporal window (in dash boxes) and
we have 4windows for T= 16 frames.
For each temporal window {t, t‚àí1,¬∑¬∑¬∑, t‚àíU+ 1},
we cyclically shift the frame indices and concatenate the U
shifted radar frames along the channel dimension for the
backbone feature extraction, i.e.,
Zt:=FŒ∏(It,t‚àí1,¬∑¬∑¬∑,t‚àíU+1),
Zt‚àí1:=FŒ∏(It‚àí1,t‚àí2,¬∑¬∑¬∑,t‚àíU+1,t),¬∑¬∑¬∑,
Zt‚àíU+1:=FŒ∏(It‚àíU+1,t,t‚àí1,¬∑¬∑¬∑,t‚àíU+2). (5)
15026
It is easy to see that, when U= 2, this reduces to the TRL.
We then follow the same top- Kfeature selector as the Tem-
poRadar (refer to Appendix 8)
Ht=SK(Zt), t={t, t‚àí1,¬∑¬∑¬∑, t‚àíU+ 1}.(6)
By concatenating features from the temporal window of
Uframes, we have Hl‚àí1
t,¬∑¬∑¬∑,t‚àíU+1= [Hl‚àí1
t,¬∑¬∑¬∑,Hl‚àí1
t‚àíU+1]‚ä§,
where the superindex denotes the layer index in the ETR
model and H0
ttakesHtof (6) as input for the first layer.
We apply the masked MCA of (4) H1times to Hl‚àí1
t,¬∑¬∑¬∑,t‚àíU+1
with a masking matrix Mof dim UK√óUK for cross-
frame feature attention within each window. Collecting
from all windows, the TWA block obtains the features
Hl
t,¬∑¬∑¬∑,Hl
t‚àíT+1from all Tframes at its output.
Temporally Regrouped Window Attention: To allow
for cross-window attention, we regroup the subset features
from different windows in a deformable temporal order.
First, we partition the Kfeatures of each frame into ‚Ñ¶sub-
frame patches with a stride S. Each sub-frame patch con-
sists of Mfeatures. As shown in Fig. 3, one choice for
a non-overlapping sub-frame partition is M=K/2and
S=K/2(assuming Kis even) where each frame is par-
titioned into ‚Ñ¶ = 2 sub-frame patches, as illustrated in
two contrasting colors for each frame in Fig. 3. Alterna-
tively, we may choose S < M for overlapping partition.
The resulting sub-frame patches of frame tare defined as
Hl
t[œâ]‚ààRC√óM, œâ= 1,¬∑¬∑¬∑,‚Ñ¶. For more discussion of
patch size, refer to Appendix 11.
The sub-frame patches are regrouped into a new set of
windows in a deformable temporal order for cross-window
attention. For the newly regrouped window, the features are
aggregated as
Fl
t(œâ) :=
Hl
t[œâ],Hl
t‚àíU[œâ],¬∑¬∑¬∑,Hl
t‚àíT+U[œâ]	‚ä§,(7)
As illustrated in the top right portion of Fig. 3, the regroup-
ing operation extracts one sub-frame patch from each win-
dow and results in U= 4 patches and UM =UK/ 2
features in each new window. Subsequently, we apply the
masked MCAs of (4) H2times over the aggregated fea-
tureFl
t(œâ)in each new window with an affordable cross-
window attention complexity of TM/U √óTM/U .
The cross-window attentive features are re-grouped in
the reverse manner to construct the Kfeatures of each
frame according to the temporal ( t) and patch ( œâ) indices.
In the case of overlapping patch partitioning, i.e., S < M , a
patch merging operation Mis necessary to merge the fea-
turesHl+1
t=M{Hl+1
t[1],¬∑¬∑¬∑,Hl+1
t[‚Ñ¶]}at the overlap-
ping positions. Patch merging operations (mean, sum and
max) will be examined in Section 4.3. The TRWA block
outputs Hl+1
t,¬∑¬∑¬∑,Hl+1
t‚àíT+1for all Tframes, sharing the
same dimension as the input Hl
t,¬∑¬∑¬∑,Hl
t‚àíT+1.
t-4 t-5
t-7 t-6
t-8 t-9
t-11 t-10t-12 t-13
t-15 t-14t t-1
t-3 t-2
tTop-K features A patch              with size M A windowMasked 
MCA
Patch MergingRe-grouping Sub-frame Partition
t-4 t-5
t-7 t-6
t-8 t-9
t-11 t-10t-12 t-13
t-15 t-14t t-1
t-3 t-2
Reverse Re -groupingFigure 3. The TRWA block of the ETR module. Each frame is par-
titioned into sub-frame patches (in two contrasting colors of each
frame in Top Left) and these patches are regrouped into new win-
dows (Top Right) in a deformable temporal order (arrow lines).
Masked multi-head cross-attention (MCA) is applied to new re-
grouped windows for scalable cross-window attention.
Stacking as a Stage: We can stack the TWA and TRWA
blocks as one stage and repeat the stage Ltimes. In be-
tween stages, the output of TRWA block serves the input
to the TWA block in the next stage. Finally, we put these
features Hl+1
t,¬∑¬∑¬∑,Hl+1
t‚àíT+1back to {Zt,¬∑¬∑¬∑,Zt‚àíT+1}at
corresponding spatial coordinates. The effect of Lwill be
examined in Section 4.3.
Complexity Analysis: For a given T,K, and the number
of stages L, the computational complexity expressions for
TempoRadar [27] and the ETR module are shown below
TempoRadar: (TK)2L (8)
ETR: (TWA +TRWA )L=K2TUL +MT2KL/U (9)
where Uis the number of frames in one temporal window
in the TWA block and Mis the number of features for each
sub-frame patch in the TRWA block. Note that, if U=T
andM=K, ETR reduces to the TWA module only, result-
ing in a full-size attention like TempoRadar. In this case,
the ETR complexity in (9) reduces to that of TempoRadar
in (8). Appendix 13 provides numerical comparison of the
complexity in several settings.
3.3. MCTrack: Motion Consistency Track
As shown in Fig. 2, MCTrack takes the temporally en-
hanced features {Zt}from the ETR output, and applies the
decoding heads on each Ztfor bounding box estimation. To
further exploit motion consistency, we introduce two MC
15027
ETR
DEst
Deform 
ConvNormConvDEst
Deform 
ConvNormConvDEst
Deform 
ConvNormConv
Estimated 
Pseudo -Directions
(Red arrows )
Extracted feature maps
Figure 4. Direction Estimation (DEst) decoder head. Each DEst
head takes a pair of 2frames ZTandZT‚àíœÑ, and estimates the
pseudo-direction bdT|T‚àíœÑ(arrow lines in red).
modules: one for training and one for inference, for im-
proved detection and tracking performance.
Motion Consistency for Training: We introduce the
concept of pseudo-direction to improve motion consis-
tency during training. Pseudo-directions are vectors that di-
rectly predict the current object position from each of the
previous frames, using a decoder head with learnable pa-
rameters. It is used to iteratively refine object positions be-
tween frames during learning and the pseudo-direction loss
contributes to the overall training loss in Section 3.4.
To compute the œÑ-step pseudo-direction bdT|T‚àíœÑ1from
the past frame T‚àíœÑto frame T, we design a specific de-
coder head GDEst
Œ∏(¬∑): direction estimation (DEst) with learn-
able parameters Œ∏in Fig. 4,
bdT|T‚àíœÑ=GDEst
Œ∏(ZT,ZT‚àíœÑ) [pzT]‚ààR2, (10)
where ZTandZT‚àíœÑare temporally enhanced features at
frame TandT‚àíœÑ,pzTis a two-dimensional coordinate,
andœÑ= 1,2,¬∑¬∑¬∑, T‚àí1. Fig. 4 shows the DEst head ar-
chitecture, comprising the deformable convolution [9], nor-
malization, and convolution layers. The deformable convo-
lution is particularly used to capture features of objects that
have undergone significant displacement across œÑframes.
The estimated vectors represent the positional differ-
ences of objects across œÑframes. It is essential to address
scenarios where objects move significantly within just one
frame due to low frame rates and ego-vehicle motions.
Motion Consistency for Inference: In inference, we use
a KF-based tracker such as OC-SORT [8] to enforce mo-
tion consistency. As shown in Fig. 2, the tracker consists of
a number of steps with the most crucial one in Association.
1With slightly abused notation, we use Tto denote not only the number
of frames, but also current frame index in this section.
Calculation of Calculation of 
KF predicted direction
Trajectory Forward directionKF predicted statePredicted angleObservation
Estimated observationAssociation
Pseudo directionFigure 5. The calculation of similarity metrics CangleandCtracklet
in MCTrack at inference. A pseudo-tracklet {{bzt}T
t=1,{bvt}T
t=2}
is constructed with bdT|T‚àíœÑestimated with DEst, and is used for
association: (Top) rotating a state xT|T‚àí1to be more correlate the
observation zT, (Bottom) directly correlating the observations zt
withbzt.
To this end, we further introduce the concept of pseudo-
tracklet2, constructed from the above pseudo-direction es-
timation. A pseudo-tracklet consists of a pair of vectors:
{{bzt}T
t=1,{bvt}T
t=2}.bztis an estimated observation with
pseudo-direction bdT|T‚àíœÑandzT(Top of Fig. 5), and bvtis
the forward direction linking between the estimated obser-
vations (Bottom of Fig. 5).
The pseudo-tracklet can only be calculated from obser-
vations that are independent of the state of KF, and explic-
itly contains information about the movement of the object
from the past to the present. We use this pseudo-tracklet to
design the similarity metric in the association:
CMCTrack=ŒªCangle+ (1‚àíŒª)Ctracklet, (11)
Ctracklet=1
T‚àí1T‚àí1X
œÑ=1GIoU 
BzT‚àíœÑ, BbzT‚àíœÑ
,(12)
Cangle= GIoU
BzT, BbxR
T|T‚àí1
, (13)
where Œªis the weighting coefficient, Brepresents the BBox
with subscripts, and GIoU [46] denotes the similarity deter-
mined based on the distance between two BBoxes. In other
words, CtrackletandCanglerepresent the similarity between
the similarity between the pseudo-tracklet and the trajectory
of the KF, and the current observation zTand the rotated
statebxR
T|T‚àí1of the KF, respectively.
2A tracklet is essentially an aggregation of a small number of consec-
utive sensor reports processed by a sensor level tracker [11]. We use the
tracklet as a short trajectory from a set of observations.
15028
As shown in top of Fig. 5, Ctrackletdirectly correlates the
observations zts of the KF trajectory with the estimated ob-
servations bztwith the pseudo-direction. This approach, un-
like the conventional method of correlating with only one
observation value in the current frame, is more robust to
motion. The effectiveness of using both CtrackletandCangle
is reported in Section 4.3. Refer to Algorithm 1 in Ap-
pendix 11 for the pseudo-code of SIRA in inference.
In addition, as shown in bottom of Fig. 5 which repre-
sents the calculation of Cangle, the predicted state bxT|T‚àí1
with KF from the previous state bxT‚àí1is rotated with a
rotation matrix Rof angle œïave. It can be calculated as
pbxR
T|T‚àí1=R(pbxT|T‚àí1‚àípbxT‚àí1)+pbxT‚àí1, where the angle
bœïavecan be calculated as bœïave=1
T‚àí2PT‚àí3
œÅ=0bœïT‚àíœÅsuch
thatbœïT‚àíœÅ= cos‚àí1(bvT‚àíœÅ¬∑bvT‚àíœÅ‚àí1)
‚à•bvT‚àíœÅ‚à•‚à•bvT‚àíœÅ‚àí1‚à•. By using this rotated
statebxR
T|T‚àí1, we can avoid a high correlation between the
predicted state assuming linear motion and the incorrect ob-
servation znoise
T.
Our approach exploits the proposition that the tempo-
rally enhanced features across multiple frames from ETR
allows for more robust estimation of the pseudo-direction
bdT|T‚àíœÑfrom past frame T‚àíœÑto current frame T, com-
pared with conventional single-frame based approaches.
3.4. Learning
A loss function is constructed not only to acquire con-
ventional detection capabilities, but also to provide a clear
guideline to enhance tracking performance. It consists of
two components: a loss between the predicted and the
ground truth BBox ( LBBox
t), and a loss of the pseudo-
direction in which an object has moved between frames and
the actual movement direction ( LDEst
t), as shown in Fig. 2.
LŒ∏:=TX
t=1 
LDEst
t+LBBox
t
. (14)
For each training step, our training procedure calculates LŒ∏
and does the backward for both t= 1 tot=Tandt=T
tot= 1 simultaneously. Therefore, optimization minŒ∏LŒ∏
can be viewed as a bidirectional backward-forward training
through Tframes. For more clear trainig procedure, refer
to Fig. 8 in Appendix 11.
Oriented Bounding Box Loss: We pick the object‚Äôs cen-
ter coordinates from the heatmap, and learn its attributes
from feature representations through regression. Regres-
sion functions, which are heatmap loss Lh
t, width & Length
lossLb
t, orientation loss Lr
t, and offset loss Lo
t, compose the
training objective by a linear combination:
LBBox
t=1
NgtNgtX
k=1 
Lb
t,k+Lr
t,k+Lo
t,k
‚àí1
NNX
i=1Lh
t,i,(15)where Ndenotes the total number of pixels in the heatmap
andNgtis the total number of ground truth bounding boxes.
Refer to Appendix 9 for mathematical definition of each
loss component.
Pseudo-Direction Estimation Loss: LDEstrepresents a
pseudo-direction estimation loss:
LDEst
t=1
NgtNgtX
k=1LDEst
t,k, (16)
LDEst
t,k=1
T‚àí1TX
œÑ=1(
SL1bdt|œÑ‚àídgt
t|œÑ
œÑÃ∏=t
0 œÑ=t,(17)
where bdt|œÑ=GDEst
Œ∏(Zt,ZœÑ)h
pgt
t,ki
denotes a two-
dimensional direction from a position of time œÑto a posi-
tion of time tas mentioned in Section 3.3. pgt
t,kdenotes the
coordinate (xt,k, yt,k)of the center of k-th ground truth ob-
ject and SL1(¬∑)is a smooth L1loss [15]. dgt
t|œÑ=pgt
t,k‚àípgt
œÑ,k
denotes the ground truth direction, which can be calculated
from the difference between the coordinates of the k-th ob-
ject. This loss improves the consistency of the detection
positions between frames, which impacts both the detection
and the tracking performance.
4. Experiments
4.1. Experimental Setup
Due to page limitations, more details on experimental set-
tings are shown in Appendix 12.
Dataset: We use the automotive radar dataset: Radi-
ate[47] in our experiments, the same as TempoRadar
in [27]. The reasons to use this dataset are that it contains
high-resolution radar images, provides well-annotated ori-
ented bounding boxes with tracking IDs for objects, and
records various real driving scenarios in adverse weather,
please refer to Appendix 7 for more details of the rea-
sons. Radiate consists of video sequences recorded in ad-
verse weathers, including sun, night, rain, fog and snow.
We follow the official 3 splits: ‚Äútrain in good weather‚Äù
(22383 frames, only in good weather, sunny or overcast),
‚Äútrain good & bad weather‚Äù (9749 frames, both good & bad
weather conditions), and ‚Äútest‚Äù (11305 frames, all kinds of
weather conditions).
Implementation: Our baseline detectors include: 1) Reti-
naNet [30], 2) CenterPoint [64], 3) BBA Vectors [57], 4)
TempoRadar [27] (referred to as TR in all results). We
also implemented 5) a Sequential TempoRadar (SeTR) that
15029
Table 1. Experimental results of object detection on Radiate . The
number following the model name indicates the # of layers in the
Resnet, and the number in parentheses indicates the # of frames T.
Train good weather Train good & bad weather
mAP@0.3 mAP@0.5 mAP@0.3 mAP@0.5
RetinaNet-18 (1) 52.50 ¬±1.81 37.83¬±1.82 49.44¬±1.32 31.57¬±1.54
CenterPoint-18 (1) 58.69 ¬±3.09 49.41¬±2.94 55.83¬±3.28 44.48¬±3.19
BBA Vectors-18 (1) 59.38 ¬±3.47 50.53¬±2.07 56.84¬±3.45 45.43¬±2.87
TR-18 (2) 62.79 ¬±2.01 53.11¬±1.96 58.87¬±3.31 46.42¬±3.24
TR-18 (4) 66.37 ¬±1.62 53.23¬±1.67 65.10¬±1.67 52.47¬±1.21
SeTR-18 (4) 65.97 ¬±2.03 55.79¬±2.12 64.62¬±1.79 51.78¬±1.81
SIRA-18 (4) 67.28¬±1.47 56.98¬±1.35 65.37¬±1.76 52.88¬±1.60
RetinaNet-34 (1) 50.79 ¬±3.10 35.61¬±3.35 48.09¬±3.85 31.10¬±3.37
CenterPoint-34 (1) 59.42 ¬±1.92 50.17¬±1.91 53.92¬±3.44 42.81¬±3.04
BBA Vectors-34 (1) 60.88 ¬±1.79 51.26¬±1.99 55.87¬±2.90 44.61¬±2.57
TR-34 (2) 63.63 ¬±2.08 54.00¬±2.16 56.18¬±4.27 43.98¬±3.75
TR-34 (4) 67.48 ¬±0.94 57.01¬±1.03 64.60¬±2.08 51.99¬±1.94
SeTR-34 (4) 67.30 ¬±1.80 56.61¬±1.83 65.51¬±1.52 52.43¬±1.51
SIRA-34 (4) 68.68¬±1.12 58.11¬±1.40 66.14¬±0.83 53.79¬±1.14
stacks self-attention for two consecutive frames and sequen-
tially connects them through Tframes. We defer the de-
scription of the SeTR to Appendix 10. We use ResNet-18
and ResNet-34 for the backbone feature extraction.
For MOT, we implemented several trackers that have
been well demonstrated in this task for comparison. These
trackers include the following: CenterTrack [65] and OC-
SORT [8]. For the results of CenterTrack with Tempo-
Radar and ResNet, we copied directly from the paper [27]
except for TempoRadar with 34 layers. And for the KF-
based method, we use the specific parameters and show
the parameters in Appendix 17. We follow [47] and ex-
clude pedestrians and groups of pedestrians from detection
and tracking targets, since only very few reflections are ob-
served in these two kinds of objects. For all numerical re-
sults, we apply a center crop with size 256√ó256upon in-
put images and exclude the targets outside this scope. We
additionaly report the detection results with the full size
(1152√ó1152 ) images in Appendix 15.
Metrics: We adopt the mean average precision (mAP)
with intersection over union (IoU) at 0.3,0.5, and 0.7(re-
ported in Appendix 15) to evaluate detection performance.
The numbers are averaged over 10 random seeds. For MOT,
we adopt MOTA [35] and IDF1 [32] as the main metrics.
MOTA focuses more on the detection performance, while
IDF1 reflects on the performance of association and identity
preservation. Other metrics [35] such as ID switch (IDs),
fragmentation (frag), MT, and PT are also reported. Defini-
tions of these MOT metrics are included in Appendix 14.
4.2. Main Results
Detection: We report the detection results in Table 1. The
benefits of exploiting longer temporal relation for radar ob-Table 2. Experimental results of MOT on Radiate .
Train good weather MOTA‚ÜëIDF1‚ÜëIDs‚ÜìFrag.‚ÜìMT‚ÜëPT‚Üë
ResNet-18 (1) CenterTrack 13.01 - 873 920 269 254
ResNet-34 (1) CenterTrack 14.55 - 802 831 282 279
TR-18 (2) CenterTrack 33.59 - 349 498 145 330
TR-34 (2) CenterTrack 37.85 39.90 457 511 108 246
TR-34 (2) OC-SORT 40.74 45.01 151 291 124 172
TR-18 (4) CenterTrack 42.77 44.91 519 520 244 336
TR-34 (4) CenterTrack 43.64 44.17 503 538 197 326
TR-34 (4) OC-SORT 44.01 44.27 354 497 194 333
SeTR-18 (4) CenterTrack 42.11 50.33 658 561 261 317
SeTR-34 (4) CenterTrack 44.57 48.72 875 602 348 299
SeTR-34 (4) OC-SORT 40.16 28.20 775 689 370 305
ETR-34 (4) CenterTrack 46.06 50.81 1832 613 345 305
ETR-34 (4) OC-SORT 47.11 50.04 540 481 343 313
SIRA-34 (4) CenterTrack*47.30 50.16 1249 566 354 300
SIRA-34 (4) OC-SORT 47.79 51.13 523 488 342 314
*Ctrackletis only used for association since this is not based on SORT.
ject detection are evident in improvements of about +3
mAP@0.3and about +2.5mAP@0.5from single frame of
RetinaNet, CenterPoint, BBA Vectors to two frames of the
TempoRadar, and further more of about +5mAP@0.3and
about +4mAP@0.5from two frames to four frames of the
best among TempoRadar, SeTR, and SIRA. In both train-
ing splits, our SIRA consistently outperforms TempoRadar
and its simple extension SeTR with 4radar frames. The im-
provement margin is more significant in the ‚Äúgood & bad
weather‚Äù training split when ResNet34 is the backbone net-
work. We report the effectiveness of increasing the number
of frames in Appendix 15.
Tracking: Table 2 illustrates the results of MOT. Sim-
ilar conclusions can be made by observing the improve-
ment margins in almost all metrics by using more radar
frames. If we narrow down to the case of 4frames and
with CenterTrack as the tracker, SIRA-34 shows a sig-
nificant improvement of +3.66over TR-34 and +2.83
over SeTR-34 in MOTA. The combination of SIRA+OC-
SORT can further improve the MOT by another +0.49over
SIRA+CenterTrack.
Compared with ETR (without LDEst
tfor training), SIRA
shows consistent improvement in both MOTA and IDF1,
highlighting the effectiveness of modeling consistency in
object movement. For other metrics such as Frag., MT,
and PT, SIRA shows fluctuating but close-to-the-best per-
formance. Full results, including the effectiveness of in-
creasing the number of frames and other indicators, are re-
ported in Appendix 15 due to paper space limitations.
Visualization: We show the visualization results in Fig. 6.
Each set of figures represents ground truth in the upper row
and predictions in the lower row. It is observed that many of
the predictions are made at approximately the same position
15030
City-7-0 Junction -1-10
Fog-6-0SIRA- 34 (4)
OC-SORTGround
TruthSIRA -34 (4)
OC-SORTGround
Truth
Rain -4-0SIRA -34 (4)
OC-SORTGround
TruthSIRA -34 (4)
OC-SORTGround
TruthFigure 6. Visualizations on radar perception on Radiate .4sets of MOT results are shown in radar sequences of city-7-0, fog-6-0, junction-
1-10 and rain-4-0. Each set contains 4frames. Bounding boxes are ground truth or object detection from SIRA. Colors indicate object IDs
and plotted arrows show the motion of detected objects.
M mAP@0.3 mAP@0.5
Mean 65.15 ¬±2.20 55.06¬±2.07
Sum 65.76 ¬±2.15 55.55¬±1.59
Max 67.67¬±1.18 56.47¬±1.54
(a)Operations M. Using the Max
operation works the best.H1H2mAP@0.3 mAP@0.5
1 1 66.95 ¬±1.47 56.65¬±2.38
2 1 67.59 ¬±0.83 57.59¬±0.84
1 2 68.36 ¬±0.94 58.46¬±0.91
2 2 68.68¬±1.12 58.11¬±1.40
(b) # of MCAs . A larger H2con-
tributes more than a larger H1.L mAP@0.3 mAP@0.5
1 68.68 ¬±1.12 58.11¬±1.40
2 68.68 ¬±0.83 58.24¬±1.19
3 69.12 ¬±1.32 58.28¬±1.34
469.16¬±1.06 58.26¬±1.27
(c) # of Stages . More stages
slightly improves the detection.CtrackletCangleMOTA‚ÜëIDF1‚Üë
- - 47.11 50.04
‚úì - 47.11 50.02
- ‚úì 47.00 50.05
‚úì ‚úì 47.79 51.13
(d)Associations C. Using both
CtrackletandCangleworks the best.
Table 3. SIRA ablation experiments onRadiate . If not specified, we used SIRA-34 (4) trained on train good weathter and followed the
experimental settings for other parameters. The best performance is marked in gray.
as the annotations. Furthermore, correct predictions are ob-
served for complex motions, including nonlinear motions.
More visualizations are included in Appendix 16 with more
comparison to other baseline methods.
4.3. Ablation Study
Patch Merging Operator: In the context of patch merg-
ing within ETR, it is essential to merge feature vectors from
overlapping positions. Multiple merging operations, includ-
ing Mean, Sum and Max, can be considered. In the exper-
iment, we use ETR-34 (4) as the model. Table 3a shows
the detection performance. It is seen that the Max operation
works best as the Mean and Sum operations may change the
temporally enhanced features. We use the Max operation as
the default.
Number of Masked MCA ( H1andH2):We investi-
gated the effect of the number of masked MCA H1in
TWA and H2of TRWA. The result in Table 3b shows
that larger Himproves the detection performance. More
masked MCAs H2= 2 in the TRWA contributes to big-
ger improvement margin than using more masked MCAs
H1= 2. We set H1= 2andH2= 2as the default.Number of Stages ( L):We investigated the effect of the
number of stages Lof ETR. Table 3c evaluates the detection
performance when Lvaries from only 1to4. Stacking more
ETR stages slightly improves the detection performance.
Association in MCTrack: In Table 3d, the ablation study
on association reveals that using both CtrackletandCangle
leads to improved tracking performance. These facts in-
dicate that SIRA enforces the spatio-temporal consistency
and can be effective to deal with nonlinear object motion
across consecutive frames. See Appendix 15 for detailed
evaluation results on the performance of Pseudo-Direction
estimation and on the differences in Œª.
5. Conclusion
We overcame the limitations of radar for effective object
detection and tracking in automotive perception by intro-
ducing the SIRA framework, which includes ETR and MC-
Track. SIRA exploits joint spatio-temporal consistency
across multiple frames and enables reliable predictions de-
spite low frame rates and nonlinear motion. Our approach
outperforms previous state-of-the-art by a big margin in
both detection and tracking.
15031
References
[1] Jie Bai, Lianqing Zheng, Sen Li, Bin Tan, Sihan Chen, and
Libo Huang. Radar Transformer: An object classification
network based on 4D MMW imaging radar. Sensors , 21(11),
2021. 1, 2
[2] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang,
Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming
Zhou, and Hsiao-Wuen Hon. UNILMv2: Pseudo-masked
language models for unified language model pre-training. In
Proceedings of the 37th International Conference on Ma-
chine Learning . JMLR.org, 2020. 12
[3] Marcus Baum and Uwe D. Hanebeck. Extended object track-
ing with random hypersurface models. IEEE Transactions on
Aerospace and Electronic Systems , 50(1):149‚Äì159, 2014. 2
[4] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and
Ben Upcroft. Simple online and realtime tracking. In 2016
IEEE International Conference on Image Processing (ICIP) ,
pages 3464‚Äì3468, 2016. 1, 12
[5] Igal Bilik, Oren Longman, Shahar Villeval, and Joseph
Tabrikian. The rise of radar for autonomous vehicles: Sig-
nal processing solutions and future research directions. IEEE
Signal Processing Magazine , 36(5):20‚Äì31, 2019. 26
[6] Peter Bro√üeit, Bharanidhar Duraisamy, and J ¬®urgen Dick-
mann. The volcanormal density for radar-based extended tar-
get tracking. In 2017 IEEE 20th International Conference on
Intelligent Transportation Systems (ITSC) , pages 1‚Äì6, 2017.
2
[7] Jinkun Cao, Hao Wu, and Kris Kitani. Track targets by dense
spatio-temporal position encoding. In the 33rd British Ma-
chine Vision Conference 2022 (BMVC) , 2022. 12
[8] Jinkun Cao, Jiangmiao Pang, Xinshuo Weng, Rawal Khirod-
kar, and Kris Kitani. Observation-Centric SORT: Rethinking
SORT for robust multi-object tracking. In the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 9686‚Äì9696, 2023. 1, 5, 7, 12
[9] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
Zhang, Han Hu, and Yichen Wei. Deformable convolutional
networks. In 2017 IEEE International Conference on Com-
puter Vision (ICCV) , pages 764‚Äì773, 2017. 5
[10] Fangqiang Ding, Andras Palffy, Dariu M. Gavrila, and
Chris Xiaoxuan Lu. Hidden Gems: 4D radar scene flow
learning using cross-modal supervision. In the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 9340‚Äì9349, 2023. 2
[11] Oliver E. Drummond. Track and tracklet fusion filtering. In
Signal and Data Processing of Small Targets 2002 , pages
176 ‚Äì 195. International Society for Optics and Photonics,
SPIE, 2002. 5
[12] Yunhao Du, Zhicheng Zhao, Yang Song, Yanyun Zhao, Fei
Su, Tao Gong, and Hongying Meng. StrongSORT: Make
DeepSORT great again. IEEE Transactions on Multimedia ,
pages 1‚Äì14, 2023. 1, 12
[13] Felix Fent, Philipp Bauerschmidt, and Markus Lienkamp.
RadarGNN: Transformation invariant graph neural network
for radar-based perception. In the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) Work-
shops , pages 182‚Äì191, 2023. 2[14] Xiangyu Gao, Guanbin Xing, Sumit Roy, and Hui Liu.
RAMP-CNN: A novel neural network for enhanced automo-
tive radar object recognition. IEEE Sensors Journal , 21(4):
5119‚Äì5132, 2021. 1, 2
[15] Ross Girshick. Fast R-CNN. In 2015 IEEE International
Conference on Computer Vision (ICCV) , pages 1440‚Äì1448,
2015. 6
[16] Karl Granstr ¬®om and Marcus Baum. Extended object track-
ing: Introduction, overview and applications. CoRR ,
abs/1604.00970, 2016. 2
[17] Karl Granstrom, Maryam Fatemi, and Lennart Svensson.
Poisson multi-Bernoulli mixture conjugate prior for multiple
extended target filtering. IEEE Transactions on Aerospace
and Electronic Systems , 56(1):208‚Äì225, 2020. 2
[18] Fredrik Gustafsson, Fredrik Gunnarsson, Niclas Bergman,
Urban Forssell, Jonas Jansson, Rickard Karlsson, and P-J
Nordlund. Particle filters for positioning, navigation, and
tracking. IEEE Transactions on Signal Processing , 50(2):
425‚Äì437, 2002. 2
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 770‚Äì778, 2016. 2
[20] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen
Wei. Relation networks for object detection. In the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 3588‚Äì3597, 2018. 12
[21] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local
relation networks for image recognition. In 2019 IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
3463‚Äì3472, 2019. 12
[22] Texas Instruments. Short range radar reference design using
AWR1642 (Rev. B), 2018. 25
[23] Simon J Julier and Jeffrey K Uhlmann. New extension of
the kalman filter to nonlinear systems. In Signal processing,
sensor fusion, and target recognition VI , pages 182‚Äì193. In-
ternational Society for Optics and Photonics, 1997. 2
[24] Rudolf Emil Kalman et al. Contributions to the theory of op-
timal control. Bol. Soc. Mat. Mexicana , 5(2):102‚Äì119, 1960.
1
[25] Johann Wolfgang Koch. Bayesian approach to extended ob-
ject and cluster tracking using random matrices. IEEE Trans-
actions on Aerospace and Electronic Systems , 44(3):1042‚Äì
1059, 2008. 2
[26] Jian Li and Petre Stoica. MIMO Radar Signal Processing .
John Wiley &Sons, 2008. 26
[27] Peizhao Li, Pu Wang, Karl Berntorp, and Hongfu Liu.
Exploiting temporal relations on radar perception for au-
tonomous driving. In the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 17050‚Äì
17059, 2022. 1, 2, 3, 4, 6, 7, 12, 13, 17, 19, 26
[28] Yu-Jhe Li, Shawn Hunt, Jinhyung Park, Matthew O‚ÄôToole,
and Kris Kitani. Azimuth super-resolution for FMCW radar
in autonomous driving. In the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
17504‚Äì17513, 2023. 1, 2
15032
[29] Teck-Yian Lim, Amin Ansari, Bence Major, Daniel Fontijne,
Michael Hamilton, Radhika Gowaikar, and Sundar Subrama-
nian. Radar and camera early fusion for vehicle detection in
advanced driver assistance systems. In the Neural Informa-
tion Processing Systems Workshop , 2019. 2
[30] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll ¬¥ar. Focal loss for dense object detection. In 2017
IEEE International Conference on Computer Vision (ICCV) ,
pages 2999‚Äì3007, 2017. 6
[31] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin Transformer:
Hierarchical vision transformer using shifted windows. In
2021 IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 9992‚Äì10002, 2021. 3
[32] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip
H. S. Torr, Andreas Geiger, Laura Leal-Taix ¬¥e, and Bastian
Leibe. HOTA: A higher order metric for evaluating multi-
object tracking. Int. J. Comput. Vision , 129(2):548‚Äì578,
2021. 7, 16
[33] Gerard Maggiolino, Adnan Ahmad, Jinkun Cao, and Kris
Kitani. Deep OC-SORT: Multi-pedestrian tracking by adap-
tive re-identification. In 2023 IEEE International Confer-
ence on Image Processing (ICIP) , pages 3025‚Äì3029, 2023.
12
[34] Yunze Man, Liang-Yan Gui, and Yu-Xiong Wang. BEV-
guided multi-modality fusion for driving perception. In
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 21960‚Äì21969, 2023. 2
[35] Anton Milan, Laura Leal-Taixe, Ian Reid, Stefan Roth, and
Konrad Schindler. MOT16: A benchmark for multi-object
tracking, 2016. 7, 16, 17, 18
[36] Mohammadreza Mostajabi, Ching Ming Wang, Darsh Ran-
jan, and Gilbert Hsyu. High resolution radar dataset for semi-
supervised learning of dynamic objects. In the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
Workshops (CVPRW) , pages 450‚Äì457, 2020. 12
[37] Umut Orguner. A variational measurement update for ex-
tended target tracking with random matrices. IEEE Transac-
tions on Signal Processing , 60(7):3827‚Äì3834, 2012. 2
[38] Arthur Ouaknine, Alasdair Newson, Patrick P ¬¥erez, Florence
Tupin, and Julien Rebut. Multi-view radar semantic seg-
mentation. In 2021 IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 15651‚Äì15660, 2021. 1, 2
[39] Andras Palffy, Ewoud Pool, Srimannarayana Baratam, Julian
F. P. Kooij, and Dariu M. Gavrila. Multi-class road user de-
tection with 3+1D radar in the View-of-Delft dataset. IEEE
Robotics and Automation Letters , 7(2):4961‚Äì4968, 2022. 1,
2
[40] Ashish Pandharipande, Chih-Hong Cheng, Justin Dauwels,
Sevgi Z. Gurbuz, Javier Ibanez-Guzman, Guofa Li, Andrea
Piazzoni, Pu Wang, and Avik Santra. Sensing and machine
learning for automotive perception: A review. IEEE Sensors
Journal , 23(11):11097‚Äì11115, 2023. 1, 2
[41] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li,
Trevor Darrell, and Fisher Yu. Quasi-dense similarity learn-
ing for multiple object tracking. In the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 164‚Äì173, 2021. 12[42] Kun Qian, Shilin Zhu, Xinyu Zhang, and Li Erran Li. Ro-
bust multimodal vehicle detection in foggy weather using
complementary Lidar and Radar signals. In the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 444‚Äì453, 2021. 2
[43] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with a
unified text-to-text transformer. J. Mach. Learn. Res. , 21(1),
2020. 12
[44] Karthik Ramasubramanian and Brian Ginsburg. AWR1243
sensor: Highly integrated 76‚Äì81-GHz radar front-end for
emerging ADAS applications. In Texas Instruments Tech-
nical Report , pages 1‚Äì12, 2017. 26
[45] Julien Rebut, Arthur Ouaknine, Waqas Malik, and Patrick
P¬¥erez. Raw high-definition radar for multi-task learning. In
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 17021‚Äì17030, 2022. 12
[46] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir
Sadeghian, Ian Reid, and Silvio Savarese. Generalized In-
tersection Over Union: A metric and a loss for bounding box
regression. In the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 658‚Äì666, 2019.
5
[47] Marcel Sheeny, Emanuele De Pellegrin, Saptarshi Mukher-
jee, Alireza Ahrabian, Sen Wang, and Andrew Wallace. RA-
DIATE: A radar dataset for automotive perception in bad
weather. In 2021 IEEE International Conference on Robotics
and Automation (ICRA) , pages 1‚Äì7, 2021. 1, 2, 6, 7, 12, 13,
17, 25, 26
[48] Gerald L Smith, Stanley F Schmidt, and Leonard A McGee.
Application of statistical filter theory to the optimal estima-
tion of position and velocity on board a circumlunar vehicle .
National Aeronautics and Space Administration, 1962. 2
[49] Niklas Wahlstrom and Emre Ozkan. Extended target track-
ing using Gaussian processes. IEEE Transactions on Signal
Processing , 63(16):4165‚Äì4178, 2015. 2
[50] Pu Wang, Petros T. Boufounos, Hassan Mansour, and
Philip V . Orlik. Slow-time MIMO-FMCW automotive radar
detection with imperfect waveform separation. In IEEE In-
ternational Conference on Acoustics, Speech, and Signal
Processing (ICASSP) , pages 8634‚Äì8638. IEEE, 2020. 25,
26
[51] Yingjie Wang, Jiajun Deng, Yao Li, Jinshui Hu, Cong Liu,
Yu Zhang, Jianmin Ji, Wanli Ouyang, and Yanyong Zhang.
Bi-LRFusion: Bi-directional LiDAR-Radar fusion for 3D
dynamic object detection. In the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
13394‚Äì13403, 2023. 2
[52] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple
online and realtime tracking with a deep association metric.
In2017 IEEE international conference on image processing
(ICIP) , pages 3645‚Äì3649. IEEE, 2017. 12
[53] Yuxuan Xia, Pu Wang, Karl Berntorp, Lennart Svensson,
Karl Granstr ¬®om, Hassan Mansour, Petros Boufounos, and
Philip V . Orlik. Learning-based extended object tracking
using hierarchical truncation measurement model with au-
15033
tomotive radar. IEEE Journal of Selected Topics in Signal
Processing , 15(4):1013‚Äì1029, 2021. 2
[54] Tetsutaro Yamada, Masato Gocho, Kei Akama, Ryoma
Yataka, and Hiroshi Kameda. Multiple hypothesis tracking
with merged bounding box measurements considering oc-
clusion. IEICE Transactions on Information and Systems ,
E105.D(8):1456‚Äì1463, 2022. 12
[55] Bin Yang, Runsheng Guo, Ming Liang, Sergio Casas, and
Raquel Urtasun. RadarNet: Exploiting radar for robust per-
ception of dynamic objects. In Computer Vision ‚Äì ECCV
2020 , pages 496‚Äì512, 2020. 2
[56] Ryoma Yataka, Pu Wang, Petros Boufounos, and Ryuhei
Takahashi. Radar perception with scalable connective tem-
poral relations for autonomous driving. In 2024 IEEE Inter-
national Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP) , pages 13266‚Äì13270, 2024. 16
[57] Jingru Yi, Pengxiang Wu, Bo Liu, Qiaoying Huang, Hui Qu,
and Dimitris Metaxas. Oriented object detection in aerial im-
ages with box boundary-aware vectors. In 2021 IEEE Win-
ter Conference on Applications of Computer Vision (WACV) ,
pages 2149‚Äì2158, 2021. 6
[58] Tianwei Yin, Xingyi Zhou, and Philipp Kr ¬®ahenb ¬®uhl. Center-
based 3D object detection and tracking. In the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 11779‚Äì11788, 2021. 2
[59] Shuqing Zeng and James N. Nickolaou. Automotive Radar .
CRC Press, 2014. 1
[60] Ao Zhang, Farzan Erlik Nowruzi, and Robert Laganiere.
RADDet: Range-azimuth-doppler based radar object detec-
tion for dynamic road users. In 2021 18th Conference on
Robots and Vision (CRV) , pages 95‚Äì102, 2021. 1, 2
[61] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. FairMOT: On the fairness of detection and
re-identification in multiple object tracking. Int. J. Comput.
Vision , 129(11):3069‚Äì3087, 2021. 12
[62] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng
Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang
Wang. ByteTrack: Multi-object tracking by associating ev-
ery detection box. In Computer Vision ‚Äì ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022,
Proceedings, Part XXII , page 1‚Äì21. Springer-Verlag, 2022.
1, 12
[63] Lianqing Zheng, Zhixiong Ma, Xichan Zhu, Bin Tan, Sen Li,
Kai Long, Weiqi Sun, Sihan Chen, Lu Zhang, Mengyue Wan,
Libo Huang, and Jie Bai. TJ4DRadSet: A 4D radar dataset
for autonomous driving. In the 25th IEEE International Con-
ference on Intelligent Transportation Systems (ITSC) , pages
493‚Äì498, 2022. 12
[64] Xingyi Zhou, Dequan Wang, and Philipp Kr ¬®ahenb ¬®uhl. Ob-
jects as points. CoRR , abs/1904.07850, 2019. 3, 6
[65] Xingyi Zhou, Vladlen Koltun, and Philipp Kr ¬®ahenb ¬®uhl.
Tracking objects as points. In Computer Vision ‚Äì ECCV
2020: 16th European Conference, Glasgow, UK, August
23‚Äì28, 2020, Proceedings, Part IV , page 474‚Äì490. Springer-
Verlag, 2020. 2, 7
15034
