Regressor-Segmenter Mutual Prompt Learning for Crowd Counting
Mingyue Guo1,2, Li Yuan2,3*, Zhaoyi Yan2†, Binghui Chen, Yaowei Wang2, Qixiang Ye1,2†
1University of Chinese Academy of Sciences2Pengcheng Lab3Peking University
guomingyue21@mails.ucas.ac.cn yuanli-ece@pku.edu.cn chenbinghui@bupt.cn
yanzhaoyi@outlook.com wangyw@pcl.ac.cn qxye@ucas.ac.cn
Abstract
Crowd counting has achieved significant progress by
training regressors to predict instance positions. In heav-
ily crowded scenarios, however, regressors are challenged
by uncontrollable annotation variance, which causes den-
sity map bias and context information inaccuracy. In
this study, we propose mutual prompt learning (mPrompt),
which leverages a regressor and a segmenter as guidance
for each other, solving bias and inaccuracy caused by anno-
tation variance while distinguishing foreground from back-
ground. In specific, mPrompt leverages point annotations
to tune the segmenter and predict pseudo head masks in a
way of point prompt learning. It then uses the predicted
segmentation masks, which serve as spatial constraint, to
rectify biased point annotations as context prompt learning.
mPrompt defines a way of mutual information maximization
from prompt learning, mitigating the impact of annotation
variance while improving model accuracy. Experiments
show that mPrompt significantly reduces the Mean Aver-
age Error (MAE), demonstrating the potential to be general
framework for down-stream vision tasks. Code is available
athttps://github.com/csguomy/mPrompt .
1. Introduction
Crowd counting, which estimates the number of people in
images of crowded or cluttered backgrounds, has garnered
increasing attention for its wide-ranging applications in
public security [23, 42], traffic monitoring [13], and agricul-
ture [2, 38]. Many existing methods converted crowd count-
ing as a density map regression problem [3, 27, 28, 62],
i.e., generating density map targets by convolving the point
annotations with the predefined Gaussian kernels and then
training a model to learn from these targets.
Unfortunately, point annotations exhibit considerable
variances, termed label variance, which impedes the accu-
*This work was supported in part by the National Key R&D Program
of China (2022ZD0118101).
†Corresponding author.
Figure 1. Upper: The biased point annotation impedes ac-
curate model learning. mPrompt leverages context prompt
and point prompt to mine spatial context and rectify biased an-
notation for crowd counting. Lower: Illustration of mutual
prompt learning (mPrompt) , which completes pseudo segmen-
tation mask by using point prompt learning. Meanwhile, it lever-
ages the rectified masks as spatial context information to refine bi-
ased point annotations in a way of context prompt learning. (Best
viewed in color)
rate learning of models. As shown in Fig. 1, label vari-
ance is an inherent issue, where the annotated point are
coarsely placed within head regions rather than at precise
center positions. To mitigate the label variance, loss re-
laxation approaches [40, 52, 54] modified the strict pixel-
wise loss constraint via constructing probability density
functions. Segmentation-based approaches [41, 48, 64]
suppressed background noises by introducing an auxiliary
branch to regressor networks [41].
Unfortunately, loss relaxation methods comprise point
position variance, which could introduce background noises
to the regressor. Segmentation-based methods manage to
alleviate label variance using spatial context, but are chal-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28380
lenged by the inaccurate context information. To obtain
accurate context information while alleviating background
noises in a systematic framework remains to be elaborated.
In this study, the pivotal question we seek to address is:
How to obtain precise spatial context information to al-
leviate the impact of label variance for crowd counting?
We propose a simple-yet-effective mutual prompt learning
framework, Fig. 1, which leverages a regressor and a seg-
menter as guidance for each other. This framework com-
prises a head segmenter, a density map regressor, and a
mutual learning module. Specifically, mPrompt leverages
the point annotation to tune a segmenter and predict pseudo
head masks in a way of point prompt learning. As illus-
tration in Fig. 1, the point prompt provides statistical dis-
tribution (random and uncertain locations) of points to re-
fine the object mask. The objective of the segmenter is to
isolate head regions, so as to learn comprehensive and ac-
curate pseudo segmentation masks. Such pseudo segmen-
tation masks are treated as spatial context to rectify biased
point annotations in a way of context prompt learning. This
mutual prompt process fosters information gain between the
segmenter and the density map regressor, driving them to
enhance each other and ultimately reach an optimal state.
The contributions of this study are summarized as fol-
lows:
• We propose a mutual prompt learning (mPrompt) frame-
work, which incorporates a segmenter and a regressor and
maximizes their complementary for crowd counting. To
our best knowledge, this is the first attempt to unify learn-
ing accurate context information and alleviating back-
ground noises using mutual prompt.
• We design feasible point prompt by unifying the pre-
dicted density map with the ground-truth one, and plau-
sible mask prompt by unifying/intersecting the predicted
density map with a segmentation mask.
• Experiments conducted on the popular crowd-counting
datasets, including ShanghaiTechA/B [63], UCF-
QNRF [57] and NWPU [17] demonstrate mPrompt’s ef-
fectiveness when addressing label variance. Particularly,
mPrompt achieves new state-of-the-art performances on
multiple benchmark datasets.
2. Related Work
Density Regression Method. Nowadays, density map re-
gression [27] is widely used in crowd counting [3, 4, 7, 28,
33, 34, 50, 51, 53, 55, 61, 63] due to its simple and effective
learning strategies. Nevertheless, many density regression
approaches neglected scale variation of heads, and thereby
is challenged by the inconsistency between density maps
and features caused by labeling variance.
To tackle scale variance, multi-scale feature fusion lay-
ers [19, 20, 51], attention mechanisms [12, 21, 31, 34,
61], perspective information [47, 58–60], and dilated net-works [4, 58] were proposed.
To mitigate the side effect of inaccurate point annota-
tions, distribution matching [30, 54], generalized localiza-
tion loss [53], and density normalized precision [50] are
proposed to minimize the discrepancy between the pre-
dicted maps and point annotations. For so many approaches
proposed, however, density regression remains challenged
by the label variance issue, which is expected to be tackled
by introducing segmentation-based context information.
Segmentation-based Method. In early years, Chan et
al. [8] and Ryan et al. [46] proposed to segment foreground
objects to distinct clusters, and regress the features of each
cluster to determine the overall object counts. Recent stud-
ies [41, 48, 64] began to incorporate image segmentation
as an auxiliary task to leverage spatial context informa-
tion while mitigating the effects of false regression. These
methods typically utilized the coarse “ground-truth” seg-
mentation maps, which are simply derived from the noisy
point annotation maps. As a result, they lack robust and
precise spatial information, and are prone to label variabil-
ity. In contrast, this study smoothly acquires precise spatial
information about head positions, reducing label variance
through the deployment of mutual prompt learning. The
significant advantage of our approach upon conventional
segmentation-based approaches lies in that it can fully ex-
plore the statistical distribution (random and uncertain loca-
tions) of points to refine the object mask in a way of point
prompt learning.
Prompt Learning. In the era of large language mod-
els [6, 11], prompt learning has been shown to be a powerful
tool for solving various natural language processing (NLP)
tasks [6, 35, 44]. various prompt learning strategies includ-
ing prompt engineering [6, 39], prompt assembling [22],
and prompt tuning [43], are respectively proposed. Inspired
by the success of prompt learning in NLP, vision prompt
learning approaches [5, 18] are proposed. The challenge
lies in how to design plausible prompts which can guide
and enhance the learning of models on specific tasks.
In this study, we take a further step to mutual prompt
learning, with the aim to enhance both the regression and
segmentation models in a unified framework. While the
term “prompt” typically refers to “guidance/hint” embed-
ding into the pretrained large model in the forward pro-
cess, our work extends its application to the realm of back-
ward gradient propagation (via point and context prompt
in this paper). We also extend our method by integrating
pre-trained large-scale models, capitalizing on their exten-
sive knowledge base. This integration enables our model
to achieve robust performance while maintaining parameter
efficiency during training.
28381
Figure 2. mPrompt consists of four components: a shared backbone for feature extraction, a regressor for density map ( ˆy) prediction, a
segmenter for head region ( ˆm) estimation, and a mutual prompt learning module.
3. The Proposed Approach
The proposed approach integrates a regressor and a seg-
menter for density map and segmentation mask prediction.
In what follows, we first unify the segmenter with a regres-
sor to construct a two-branch network. We then introduce
mutual prompt learning to the network, which encompasses
point prompts given by the regressor and context prompts
provided by the segmenter.
3.1. Unifying Segmenter with Regressor
Network Architecture. As shown in Fig. 2(upper),
mPrompt consists of a shared CNN backbone, a density
regressor Rand a head segmenter S, which are trained
in an end-to-end fashion. The shared backbone is derived
from a HRNet by truncating layers from stage4 [56]. To
seamlessly unify the segmenter with the regressor, a self-
attention module applied to them to enhance features of the
regressor, Fig. 2. Denoting S(x)andR(x)as the features
of the regressor and the segmenter for an input image x,
the self-attention operation is applied on S(x)andR(x)
asSigmoid 
S(x)NR(x), whereNis the element-wise
multiplication. With feature self-attention, the regressor
preliminarily incorporates the context information provided
by the segmenter.
The regressor predicts the density map ˆyfor the in-
put image x, and the segmenter predicts the head mask
ˆm. The regressor and segmenter are designed using an
identical architecture, comprising Conv-BN-ReLU blocks.Specifically, three Conv-BN-ReLU blocks are adopted to
decrease the feature channel size progressively from 128
to64, and eventually down to 32followed by a self-
attention operation. A convolution layer of kernel size 1
followed by ReLU/Sigmoid layer squeezes the features to
density/segmentation maps.
Segmenter Learning. Each point annotation is ex-
panded to a density map ( y) and a target mask ( m), which
however are noisy and inaccurate. Fortunately, existing
datasets, such as NWPU [57], provide point and box an-
notations, which can be expanded to pseudo masks for seg-
menter training. A point pseudo mask is derived by ap-
plying dilation to the point density map, which are con-
verted to a segmentation mask after binarization. Follow-
ing [41, 48, 64], we train the segmenter using the cross-
entropy loss function Lsdefined on point pseudo masks.
As elucidated by experiments, using point-based pseudo
masks to train a segmenter exhibits a challenge in assim-
ilating spatial information. This limitation primarily stems
from the fact that the learning targets for both the segmenter
and regressor are manually created from dot annotations,
which intrinsically do not convey any spatial information.
To develop an advanced segmenter, we further leverage the
box annotations provided by the NWPU dataset [57]. A
box pseudo mask is produced by attributing values of 1to
locations within the heads and 0to the background. Ac-
cordingly, the overall loss for the regressor and segmenter
is defined as L=Lden+λsLsegwhere λsis a parameter
28382
Figure 3. Illustration of the generation of prompt information for the segmenter. White boxes highlight key regions for better clarity. The
red-shaded areas represent the head segmentation mask, demonstrating the pseudo mask’s inaccuracy when compared to the more precise
updated target mask. With offline prompt, the prompted segmenter tends to predicted more complete head regions but unfortunately
introduces background noises. With online prompt, background noises are reduced. (Best viewed in color with zoom)
Figure 4. Illustration of K-NN algorithm, which removes back-
ground noises from the target segmentation mask.
to balance the two losses1.
3.2. Segmenter Learning with Point Prompt
As shown in Fig. 2, point prompt defines a procedure to
refine the target mask musing the pseudo mask mp, the
ground-truth density map yand the predicted density map
ˆy. In specific, we utilize the pseudo mask mp(offline ob-
tained via a segmenter pretrained on NWPU box annota-
tions) and ground-truth density map ( y) for offline prompt,
and the density map ˆyfor online prompt, which guarantees
the renewal of the segmentation map via the prompt from
the regressor. When training the segmenter, the binary cross
entropy loss is applied.
Offline Prompt. This is performed by unifying the seg-
mentation pseudo mask mpwith the binarized ground-truth
density map y, as
m=mp∪B(y), (1)
where ∪denotes the union operation performing pixel-wise
OR operation between two matrices. B(·)defines a bi-
narization function: the density map is binarized with a 0
threshold to form a mask. Supervised by training targets m
1Please refer to the supplementary material for details of training a seg-
menter using point and box/pseudo masks.from all the training images, the segmenter tends to absorb
the distribution (random and uncertain locations) of points.
After prompt learning, the prompted segmenter tends to pre-
dict more complete head regions (the top row of Fig. 3)
where the initialized segmenter fails to predict.
Online Prompt. With the offline prompt, the accuracy
of the predicted density map can be improved after κepochs
of training, so that it can be used to improve the target seg-
mentation mask. Following the initial κtraining epochs,
ˆyshould possess credibility and aid in introducing reliable
distributions (Gaussian blobs randomly situated around the
point annotations) of head regions. As a result, integrating
ˆyinto point prompt learning further assists in predicting the
comprehensive head regions. Meanwhile, the union oper-
ation defined in offline/online prompt inevitably introduce
background noises from the density map to the target mask.
To solve, we further leverage a K-NN algorithm to filter
out background noises (the bottom row of Fig. 3) at the end
of online prompt, which defined as interaction operation.
Online prompt defines the following union and intersection
operations, as
m←(m∪(B(ˆy))∩mK, (2)
where mKis a context mask defined by a spatial K-NN al-
gorithm applied on the point annotations Fig. 4. In specific,
for a point annotation, the spatial K-NN algorithm finds
itsKnearest point annotations. The minimum circle area
covering the Knearest point annotations is defined as the
context mask mK.
3.3. Regressor Learning via Context Prompt
With point prompt, the segmenter absorbs distribution of
the annotated points so that it produces more accurate mask
28383
Figure 5. mPrompt with learnable prompt modules based on a pre-
trained model.
predictions. Such mask predictions serve as a spatial infor-
mation to improve the regressor in turn, which is termed as
context prompt. In specific, the context prompt is defined as
a constraint, which encourages the predicted density map ˆy
falling the target mask m. This is implemented by intro-
ducing a context prompt loss to the framework, as
Lcon(ˆy,ˆm) =−Σ 
B(ˆy)∩B(ˆm)
ΣB(ˆy), (3)
where Σaccumulates the values of all pixels. To minimize
the context prompt loss, intersection term in Eq. 3 must be
large, which implies the prediction ˆyof the regressor falling
in the predicted mask area ( ˆm) of the segmenter. In other
words, the segmenter serves as the context prompt of the re-
gressor. When training the regressor, the conventional MSE
constrain is defined as the density map construction loss.
3.4. Mutual Prompt Learning
Given the point prompt defined by Eq. 1 and Eq. 2, and the
context prompt defined by Eq. 3, the mutual prompt learn-
ing is performed in an end-to-end fashion by optimizing the
following loss function,
L=λdLden(ˆy,y) +λsLseg(ˆm,m) +λcLcon(ˆy,ˆm),
(4)
where λd,λsandλcare experimentally defined regulariza-
tion factors.
In summary, our mPrompt comprises three components:
(1) With point prompt learning, the segmenter absorbs sta-
tistical distribution (random and uncertain locations) of
points to predict more accurate target masks. (2) With
context prompt learning, the predicted density map is con-
strained to fall into the target mask regions, which in turn
improve the density regression. (3) Unifying point prompt
learning with context prompt learning in a framework with
shared backbone and training the network parameters in an
end-to-end fashion create mutual prompt learning.
3.5. Extension to Foundation Model
Our mPrompt approach can be further applied to founda-
tion model adaptation. This involves expanding the con-text prompt into a feature insertion strategy, which enhances
the utilization of the extensive knowledge embedded in pre-
trained large models, as demonstrated in Fig. 5. In this pro-
cess, the context prompt is modulated by learnable prompt
modules. Such prompt modules are implemented using
adapter mechanism2[16]. Our primary goal is to integrate
comprehensive context information into foundational mod-
els, specifically for crowd counting. This aims to make ef-
fective use of the representational knowledge in pre-trained
large models by only fine-tuning a small number of param-
eters.
During the inference phase, the learnable prompt mod-
ules, along with the backbone and regressor, are retained,
while the segmenter branch is discarded. These prompt
modules function as context prompts, facilitating the inser-
tion of features into the backbone.
3.6. Interpretive Analysis
The proposed approach is justified from the perspective of
mutual information [1]. mPrompt can be generally inter-
preted as a procedure to maximize the mutual information I
of a regressor ( fr) and a segmenter ( fs). The point prompt
is interpreted as
H(fs, fr) =H(fs|fr) +H(fr) (5)
whereH(·)is information entropy, the H(fs|fr)is condi-
tional and the H(fs, fr)is joint entropy. Denote the pa-
rameters of the model as θ. To minimize Lcon(ˆy,ˆm)is
equivalent to maximize logΣ 
B(ˆy)∩B(ˆm)
ΣB(ˆy). Then the con-
text prompt is interpreted as
arg max
θIθ(fr;fs) = logp(fr|fs)
p(fr), (6)
which maximizes the mutual information between the re-
gressor frand the segmenter fs.
4. Experiment
Dataset: Experiments are carried out on four public crowd
counting datasets including ShanghaiTechA/B [63], UCF-
QNRF [17], and NWPU [57]. ShanghaiTech includes
PartA ( SHA ) and PartB ( SHB ), totaling 1,198images with
330,165annotated heads. SHA comprises 300training im-
ages and 182testing images with crowd sizes from 33to
3,139. SHB includes 400training images and 316testing
images with crowd sizes ranging from 9to578. The im-
ages are captured from Shanghai street views. UCF-QNRF
(QNRF ) encompasses 1,535high-resolution images, 1.25
million annotated heads with extreme crowd congestion,
small head scales, and diverse perspectives. It is divided
into1,201training and 334testing images. NWPU dataset
2Please refer to the supplementary materials for more details.
28384
Method VenueSHA SHB QNRF NWPU(V) NWPU(T)
MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE
GLoss [53] CVPR’21 61.3 95.4 7.3 11.7 84.3 147.5 - - 79.3 346.1
P2PNet [50] ICCV’21 52.8 85.1 6.3 9.9 85.3 154.5 77.4 362.0 83.3 553.9
DKPNet [9] ICCV’21 55.6 91.0 6.6 10.9 81.4 147.2 61.8 438.7 74.5 327.4
SASNeT [51] AAAI’21 53.6 88.4 6.4 9.9 85.2 147.3 - - - -
GauNet [10] CVPR’22 54.8 89.1 6.2 9.9 81.6 153.7 - - - -
CLTR [29] ECCV’22 56.9 95.2 6.5 10.6 85.8 141.3 61.9 246.3 74.3 333.8
DDC [45] CVPR’23 52.9 85.6 6.1 9.6 65.8 126.5 - - - -
PET [32] ICCV’23 49.3 78.8 6.2 9.7 79.5 144.3 58.5 238.0 74.4 328.5
STEERER [14] ICCV’23 54.5 86.9 5.8 8.5 74.3 128.3 54.3 238.3 63.7 309.8
mPrompt ‡(ours) - 52.5 88.9 5.8 9.6 72.2 133.1 50.2 219.0 62.1 293.5
mPrompt ‡∗(ours) - 53.2 85.4 6.3 9.8 76.1 133.4 58.8 240.2 66.3 308.4
Table 1. Performance comparisons. mPrompt ‡∗indicates that we extend the mPrompt to the pre-trained model (SAM-base). The best
results are shown in bold , and the second-best results are underlined .
comprises 5,109images, with 2,133,375annotated heads
and head box annotations. The images are split to a training
set of 3,109images, an evaluation set of 500images, and
a testing set of 1,500images. NWPU(V) and NWPU(T)
denote the validation and testing sets, respectively.
Evaluation Metric: Mean Absolute Error (MAE) and
Root Mean Squared Error (RMSE) [28, 36] are used. They
are defined as MA E =1
NPN
i=1|ˆCi−Ci|andR MSE =q
1
NPN
i=1|ˆCi−Ci|2, where Nis the number of test im-
ages. ˆCiandCirespectively denote the estimated and
ground truth counts of image xi.
Implementation Details: We resize images to a maxi-
mum length of 2,048pixels and a minimum of 416pixels,
keeping the aspect ratio unchanged. Data augmentation in-
cludes random horizontal flipping, color jittering, and ran-
dom cropping with a 400×400pixel patch size. Ground-
truth density maps are generated using a 15×15Gaussian
kernel. The network is trained using Adam [24] optimizer
with learning rate of 1e−4. The batch size is 16and train-
ing on NWPU dataset takes about 25hours on four Nvidia
V100 GPUs. Key parameters include K= 3,λd= 1,
λs= 0.5,λc= 0.5andκ= 50 . The network is constructed
with the backbone HRNet-W40-C [56] pretrained on Ima-
geNet [26] and random initialization of the remaining pa-
rameters. When adopting mPrompt for foundation models,
we utilize SAM-base [25], chosen for its robust segmenta-
tion performance. We train both networks for 700 epochs.
In Table 1, the performance of mPrompt ‡is compared
with state-of-the-art methods across four major datasets.
mPrompt ‡consistently achieves impressive results in terms
of MAE on all four datasets. mPrompt ‡consistently ranks
within the top-2 for MAE performance across the datasets,
highlighting the superior effectiveness of our model.4.1. Visualization Analysis
Fig. 6 visualizes the predicted density maps and the at-
tention map from a test image. mPrompt ‡generates
more precise density maps compared with the baseline
(mPrompt rsg), at both dense and sparse regions. Particu-
larly, after the context prompt learning, mPrompt ‡indeed
isolates the accurate head regions as the regressor absorbs
the context information from the segmenter.
Fig. 7 visualizes the segmentation maps predicted by
mPrompt rsgand mPrompt ‡. Specifically, we identify three
types of regions when comparing these two segmenta-
tion maps. Blue and yellow regions are generated by
mPrompt rsg(baseline) and mPrompt ‡, respectively. Red
regions represent the intersection of these two masks. One
can see that mPrompt ‡improves head region segmentation
by removing areas where the background is mistaken for
a head and adding regions where the head is mistaken for
background, compared to the baseline. In order to evaluate
the enhancement of the segmenter, we conduct an analysis
of the Intersection over Union (IoU) between the head-box
regions and the predicted mask on the NWPU dataset. This
investigation yields IoU scores of 46.5for mPrompt ‡and
38.7for mPrompt rsg, respectively, thus providing quan-
titative evidence of the segmenter’s improvement through
mPrompt ‡. These validate the effect of point prompt learn-
ing, which finally contributes to the superior performance
reported in Table 2.
4.2. Ablation Studies
No Prompt. The baseline mPrompt regconsists only a
regressor. By introducing the segmenter and employ-
ing pseudo mask as supervision, mPrompt regdevelops to
mPrompt rsg. In Table 2, mPrompt regharnesses the robust
features of HRNet (truncated at stage4 ), achieving compet-
itive MAE performances of 59.4,7.8,85.5, and 65.7on
28385
Figure 6. Comparison of the density maps with/without context prompt. (Best viewed in color with zoom)
Figure 7. Comparison of segmentation masks with/without point prompt. (Best viewed in color with zoom)
Methods Regressor SegmenterPoint Prompt ContextSHA SHB QNRF NWPU(V)Offline Online Prompt
mPrompt reg ✓ 59.4 7.8 85.5 65.7
mPrompt rsg ✓ ✓ 58.4 7.1 83.2 64.3
mPrompt p† ✓ ✓ ✓ 54.8 6.2 78.9 59.2
mPrompt p‡ ✓ ✓ ✓ ✓ 53.9 5.9 74.8 52.1
mPrompt c† ✓ ✓ ✓ 55.3 6.4 79.4 62.0
mPrompt † ✓ ✓ ✓ ✓ 54.1 6.1 76.7 56.5
mPrompt ‡ ✓ ✓ ✓ ✓ ✓ 52.5 5.8 72.2 50.2
Table 2. Ablation study of mPrompt components about MAE.
Backbones #Params(M) GFLOPsmPrompt reg mPrompt rsg mPrompt † mPrompt ‡
MAE RMSE MAE RMSE MAE RMSE MAE RMSE
CNN architecture
VGG19 [49] 12.6 19.3 64.0 112.5 62.6 106.5 61.4 100.9 60.9 106.1
HRNet [56] 33.1 62.1 59.4 96.7 58.4 95.8 54.1 92.8 52.5 88.9
Transformer architecture
Swin [37] 7.4 11.6 63.9 105.5 61.8 100.0 61.1 99.3 59.3 98.8
SAM [25] 7.7 13.5 60.4 98.3 59.5 98.8 55.2 89.5 53.2 85.4
Table 3. Comparison of backbones on the SHA dataset is paired with an analysis of learnable parameters and FLOPs for a standard input
size of ( 3×224×224) when training.
SHA, SHB, QNRF, and NWPU(V) datasets, respectively.
mPrompt rsgsurpasses mPrompt reg, highlighting the signif-
icance of introducing the segmenter and signifying the ef-
fective utilization of spatial head information.
Point Prompt. With offline and online point prompt,
mPrompt rsgpromotes to mPrompt p†and mPrompt p‡, re-
spectively. mPrompt p†achieves better performance, reach-ing MAEs of 54.8,6.2,78.9, and 59.2on SHA, SHB,
QNRF, and NWPU(V) datasets, respectively. mPrompt p‡
further reduces the MAEs to 53.9,5.9,74.8, and 52.1on
these four datasets.
Context Prompt. In Table 2, when adopting Lconto
mPrompt rsg, our mPrompt c†delivers a performance gain
on these datasets, indicating the necessity of spatial infor-
28386
NoMutualPromptLearning
MethodSeglabelSHAS  HBQ  NRFN  U(V)pointb     ox
mPtrsg∗✓58.87.584.366.8
mPtrsg✓58.47.183.264.3
MutualPromptLearning
MethodmpSHAS  HBQ  NRFN  U(V)mPtrsg∗m  Ptrsg
mPt‡∅∅54.66.373.952.1
mPt‡∗✓54.36.473.452.7
mPt‡✓52.55.872.250.2
Table4.Performancewhenadoptingdifferentpseudomasks.Due
tospaceconstraints,weuset heabbreviationsNU(V)andmPtt o
respectivelyrefertoNWPU(V)andmPrompt.
mationforregressingimplementedinthisexplicitmanner.
MutualPrompt.I  nTable2 ,b othmPrompt†a nd
mPrompt‡achievessatisfyingperformances,andourfinal
variantmPrompt‡deliversMAEsof52.5,5.8,72.2,and
50.2onSHA,SHB,QNRF,andNWPU(V)datasets,respec-
tively.ComparingwithmPromptreg,asignificantperfor-
mancegaini sachieved,r educingMAEby6.9,2.0,13.3,
and15.5,r espectively.Theseablationstudiesvalidatet he
efficacyofthecomponentsofmPrompt.
Pseudomasks.W   euseas egmenterpretrainedon
NWPUboxannotationst oobtaint heofflinepseudomask
mp.Anaturalquestionarises:Canwegeneratempus-
ingasegmenterpretrainedonlywiththepointannotations,
orevendirectlysetmpas∅?Toexploret his,wepretrain
mPromptrsg∗usingonlythepointannotationsofthecorre-
spondingdatasett ogeneratet hesegmentationmasks(i.e.,
point-basedpseudomask).InTable4,mPromptrsg∗under-
performsmPromptrsgduetotheinaccuracyofthesegmen-
tationlabel.Bysettingmpt o∅andutilizingpseudomasks
generatedf rommPromptrsg∗a ndmPromptrsgi nmutual
promptl earning,weobtainmPrompt‡∅,mPrompt‡∗a nd
mPrompt‡,r espectively.mPrompt‡∗performssimilarlyt o
mPrompt‡∅,asmpi ndeedi ntroducesnoextraspatiali n-
formationwhenonlyutilizingt hepseudomasksgenerated
frommPrompt‡∗.Evenwithmpsett o∅,mPrompt‡∅still
significantlyoutperformsmPromptreg∗,highlightingtheef-
fectivenessofmutualpromptlearning.
BackboneArchitectures.W  er eplaceHRNet-W40-
Cwitho therc ommonly-usedb ackbones( VGG19[ 49],
Swin[37]andSAM[25]).Table3revealst hatmPrompt‡
continuest oo utperformmPromptreg,mPromptrsg,a nd
mPrompt†,a chievings ignificantMAEr eductions.F ur-
thermore,wehaveextendedt hemPromptt of oundational
models,suchast heSAM[ 25]andSwin[ 37].Asshown
inTable3,mPrompt‡( SAMbased)s howsperformance
marginallybelowmPrompt‡,y etwithonlya bout1 4t he
trainingparametersand1 5t heFLOPsoft hel atter.F or
crowdc ounting,g ivent heb ackbonei ss tatica ndo nly
thep romptmodulei sl earnable,t heSwinTransformer,
Figure8.P erformanceonNWPUwhent rainingwithdifferent
annotationvariance.
pretrainedf orc lassification,u nderperformsc omparedt o
SAM[25].ThismainlyattributestoSwin’srepresentational
knowledgei sl essalignedwithcrowdcountingcomparing
withSAM.
Robustnesst oAnnotationVariance.T  oassesst he
robustnessofmutualpromptl earningagainstboxannota-
tionvariance,weconductanexperimentonNWPU,ob-
servingperformancechangeswithvaryingboxannotations.
Specifically,weadduniformr andomnoise,r angingf rom
0t o5 0%,o ft heb oxh eight,t ot heo fficiala nnotated
boxes.F ig.8r evealst hatourmPrompt‡i sonlymildly
affectedbydifferentnoisel evels,whilemPromptrsga nd
STEERER[ 14]s ufferf roms evereperformancedegrada-
tion.Thisdemonstratest herobustnessofourmPrompt‡t o
annotationvariance.
5.Conclusions
Weproposedamutualpromptl earningapproach,t oen-
hancecontexti nformationwhilemitigatingt hei mpactof
pointannotationvariancei ncrowdcounting.mPrompti n-
corporatesasharedbackbone,adensitymapregressorfor
counting,aheadsegmenterforforegroundandbackground
distinction.T hemutualpromptl earnings trategymaxi-
mizedthemutualinformationgainofthesegmenterandre-
gressor.Experimentalresultsonfourpublicdatasetsaffirm
theefficacyandsuperiorityofourmethod.Whilewepri-
marilyfocusoncrowddensitymapsinthisstudy,mPrompt
haspotentialapplicationsi nareaswithscarceornoisyl a-
belingi nformation,suchascrowdl ocalization,objectde-
tection,andvisualtracking.Weaimtoexploretheseappli-
cationsinthefuturework.
Acknowledgments
Thisw orkw ass upportedi np artb yN ationalN at-
uralS  cienceF  oundationo  fC  hina(  NSFC)u  n-
derG rant6 2171431,6  2225208a ndP engC heng
LaboratoryR esearchP rojectN o.P CL2023A08.
28387
References
[1] Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D
Lawrence, and Zhenwen Dai. Variational information dis-
tillation for knowledge transfer. In CVPR , 2019. 5
[2] Shubhra Aich and Ian Stavness. Leaf counting with deep
convolutional and deconvolutional networks. In ICCV , 2017.
1
[3] Deepak Babu Sam, Shiv Surya, and R Venkatesh Babu.
Switching convolutional neural network for crowd counting.
InCVPR , 2017. 1, 2
[4] Shuai Bai, Zhiqun He, Yu Qiao, Hanzhe Hu, Wei Wu, and
Junjie Yan. Adaptive dilated network with self-correction
supervision for counting. In CVPR , 2020. 2
[5] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Glober-
son, and Alexei Efros. Visual prompting via image inpaint-
ing. In NIPS , 2022. 2
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. In NIPS , 2020. 2
[7] Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su. Scale
aggregation network for accurate and efficient crowd count-
ing. In ECCV , 2018. 2
[8] Antoni B Chan, Zhang-Sheng John Liang, and Nuno Vas-
concelos. Privacy preserving crowd monitoring: Counting
people without people models or tracking. In CVPR , 2008.
2
[9] Binghui Chen, Zhaoyi Yan, Ke Li, Pengyu Li, Biao Wang,
Wangmeng Zuo, and Lei Zhang. Variational attention: Prop-
agating domain-specific knowledge for multi-domain learn-
ing in crowd counting. In ICCV , 2021. 6
[10] Zhi-Qi Cheng, Qi Dai, Hong Li, Jingkuan Song, Xiao Wu,
and Alexander G Hauptmann. Rethinking spatial invariance
of convolutional networks for object counting. In CVPR ,
2022. 6
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional trans-
formers for language understanding. In NAACL-HLT , 2019.
2
[12] Li Dong, Haijun Zhang, Jianghong Ma, Xiaofei Xu, Yimin
Yang, and QM Jonathan Wu. Clrnet: A cross locality relation
network for crowd counting in videos. TNNLS , 2022. 2
[13] Ricardo Guerrero-G ´omez-Olmedo, Beatriz Torre-Jim ´enez,
Roberto L ´opez-Sastre, Saturnino Maldonado-Basc ´on, and
Daniel Onoro-Rubio. Extremely overlapping vehicle count-
ing. In IbPRIA , 2015. 1
[14] Tao Han, Lei Bai, Lingbo Liu, and Wanli Ouyang. Steerer:
Resolving scale variations for counting and localization via
selective inheritance learning. In ICCV , 2023. 6, 8
[15] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. Towards a unified view
of parameter-efficient transfer learning. arXiv preprint
arXiv:2110.04366 , 2021. 1
[16] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for NLP. In ICML , 2019. 5[17] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong
Zhang, Somaya Al-Maadeed, Nasir Rajpoot, and Mubarak
Shah. Composition loss for counting, density map estima-
tion and localization in dense crowds. In ECCV , 2018. 2,
5
[18] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In ECCV , 2022. 2
[19] Xiaolong Jiang, Zehao Xiao, Baochang Zhang, Xiantong
Zhen, Xianbin Cao, David Doermann, and Ling Shao.
Crowd counting and density estimation by trellis encoder-
decoder networks. In CVPR , 2019. 2
[20] Xiaoheng Jiang, Li Zhang, Pei Lv, Yibo Guo, Ruijie Zhu,
Yafei Li, Yanwei Pang, Xi Li, Bing Zhou, and Mingliang
Xu. Learning multi-level density maps for crowd counting.
TNNLS , 2019. 2
[21] Xiaoheng Jiang, Li Zhang, Mingliang Xu, Tianzhu Zhang,
Pei Lv, Bing Zhou, Xin Yang, and Yanwei Pang. Attention
scaling for crowd counting. In CVPR , 2020. 2
[22] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neu-
big. How can we know what language models know? TACL ,
2020. 2
[23] Di Kang, Zheng Ma, and Antoni B Chan. Beyond count-
ing: Comparisons of density maps for crowd analysis
tasks—counting, detection, and tracking. TCSVT , 2018. 1
[24] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint , 2014. 6
[25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and
Ross Girshick. Segment anything. arXiv:2304.02643 , 2023.
6, 7, 8, 1
[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. Advances in neural information processing systems ,
25, 2012. 6
[27] Victor Lempitsky and Andrew Zisserman. Learning to count
objects in images. In NIPS , 2010. 1, 2
[28] Yuhong Li, Xiaofan Zhang, and Deming Chen. Csrnet: Di-
lated convolutional neural networks for understanding the
highly congested scenes. In CVPR , 2018. 1, 2, 6
[29] Dingkang Liang, Wei Xu, and Xiang Bai. An end-to-end
transformer model for crowd localization. In ECCV , 2022. 6
[30] Hui Lin, Xiaopeng Hong, Zhiheng Ma, Xing Wei, Yun-
feng Qiu, Yaowei Wang, and Yihong Gong. Direct measure
matching for crowd counting. IJCAI , 2021. 2
[31] Hui Lin, Zhiheng Ma, Rongrong Ji, Yaowei Wang, and Xi-
aopeng Hong. Boosting crowd counting via multifaceted at-
tention. In CVPR , 2022. 2
[32] Chengxin Liu, Hao Lu, Zhiguo Cao, and Tongliang Liu.
Point-query quadtree for crowd counting, localization, and
more. In ICCV , 2023. 6
[33] Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli
Ouyang, and Liang Lin. Crowd counting with deep struc-
tured scale integration network. In ICCV , 2019. 2
[34] Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li
Pan, and Hefeng Wu. Adcrowdnet: An attention-injective
28388
deformable convolutional network for crowd understanding.
InCVPR , 2019. 2
[35] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi-
roaki Hayashi, and Graham Neubig. Pre-train, prompt, and
predict: A systematic survey of prompting methods in nat-
ural language processing. ACM Computing Surveys , 2022.
2
[36] Weizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-
aware crowd counting. In CVPR , 2019. 6
[37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , 2021. 7, 8
[38] Hao Lu, Zhiguo Cao, Yang Xiao, Bohan Zhuang, and Chun-
hua Shen. Tasselnet: counting maize tassels in the wild via
local counts regression network. Plant methods , 2017. 1
[39] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and
Pontus Stenetorp. Fantastically ordered prompts and where
to find them: Overcoming few-shot prompt order sensitivity.
arXiv preprint , 2021. 2
[40] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong.
Bayesian loss for crowd count estimation with point super-
vision. In ICCV , 2019. 1
[41] Davide Modolo, Bing Shuai, Rahul Rama Varior, and Joseph
Tighe. Understanding the impact of mistakes on background
regions in crowd counting. In WCACV , 2021. 1, 2, 3
[42] Daniel Onoro-Rubio and Roberto J L ´opez-Sastre. Towards
perspective-free object counting with deep learning. In
ECCV , 2016. 1
[43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training language
models to follow instructions with human feedback. In NIPS ,
2022. 2
[44] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
pervised multitask learners. OpenAI blog , 2019. 2
[45] Yasiru Ranasinghe, Nithin Gopalakrishnan Nair, Wele
Gedara Chaminda Bandara, and Vishal M Patel. Diffuse-
denoise-count: Accurate crowd-counting with diffusion
models. arXiv preprint , 2023. 6
[46] David Ryan, Simon Denman, Clinton Fookes, and Sridha
Sridharan. Crowd counting using multiple local features.
InDigital Image Computing: Techniques and Applications ,
2009. 2
[47] Miaojing Shi, Zhaohui Yang, Chao Xu, and Qijun Chen. Re-
visiting perspective information for efficient crowd counting.
InCVPR , 2019. 2
[48] Zenglin Shi, Pascal Mettes, and Cees GM Snoek. Counting
with focus for free. In ICCV , 2019. 1, 2, 3
[49] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint , 2014. 7, 8
[50] Qingyu Song, Changan Wang, Zhengkai Jiang, Yabiao
Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and
Yang Wu. Rethinking counting and localization in crowds:
A purely point-based framework. In ICCV , 2021. 2, 6[51] Qingyu Song, Changan Wang, Yabiao Wang, Ying Tai,
Chengjie Wang, Jilin Li, Jian Wu, and Jiayi Ma. To choose or
to fuse? scale selection for crowd counting. In AAAI , 2021.
2, 6
[52] Jia Wan and Antoni Chan. Modeling noisy annotations for
crowd counting. In NIPS , 2020. 1
[53] Jia Wan, Ziquan Liu, and Antoni B Chan. A generalized
loss function for crowd counting and localization. In CVPR ,
2021. 2, 6
[54] Boyu Wang, Huidong Liu, Dimitris Samaras, and Minh
Hoai. Distribution matching for crowd counting. In NIPS ,
2020. 1, 2
[55] Changan Wang, Qingyu Song, Boshen Zhang, Yabiao Wang,
Ying Tai, Xuyi Hu, Chengjie Wang, Jilin Li, Jiayi Ma, and
Yang Wu. Uniformity in heterogeneity: Diving deep into
count interval partition for crowd counting. In ICCV , 2021.
2
[56] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,
Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui
Tan, Xinggang Wang, et al. Deep high-resolution represen-
tation learning for visual recognition. TPAMI , 2020. 3, 6,
7
[57] Qi Wang, Junyu Gao, Wei Lin, and Xuelong Li. Nwpu-
crowd: A large-scale benchmark for crowd counting and lo-
calization. TPAMI , 2020. 2, 3, 5, 1
[58] Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan,
Yezhen Wang, Shilei Wen, and Errui Ding. Perspective-
guided convolution networks for crowd counting. In ICCV ,
2019. 2
[59] Zhaoyi Yan, Ruimao Zhang, Hongzhi Zhang, Qingfu Zhang,
and Wangmeng Zuo. Crowd counting via perspective-guided
fractional-dilation convolution. IEEE Transactions on Mul-
timedia , 2021.
[60] Yifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming Huang,
and Nicu Sebe. Reverse perspective network for perspective-
aware object counting. In CVPR , 2020. 2
[61] Anran Zhang, Jiayi Shen, Zehao Xiao, Fan Zhu, Xiantong
Zhen, Xianbin Cao, and Ling Shao. Relational attention net-
work for crowd counting. In ICCV , 2019. 2
[62] Cong Zhang, Hongsheng Li, Xiaogang Wang, and Xiaokang
Yang. Cross-scene crowd counting via deep convolutional
neural networks. In CVPR , 2015. 1
[63] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao,
and Yi Ma. Single-image crowd counting via multi-column
convolutional neural network. In CVPR , 2016. 2, 5
[64] Muming Zhao, Jian Zhang, Chongyang Zhang, and Wenjun
Zhang. Leveraging heterogeneous auxiliary tasks to assist
crowd counting. In CVPR , 2019. 1, 2, 3
28389
