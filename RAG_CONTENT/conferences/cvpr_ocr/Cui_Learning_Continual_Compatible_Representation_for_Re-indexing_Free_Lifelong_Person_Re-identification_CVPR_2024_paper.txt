Learning Continual Compatible Representation
for Re-indexing Free Lifelong Person Re-identiﬁcation
Zhenyu Cui1, Jiahuan Zhou1, Xun Wang2, Manyu Zhu2, Yuxin Peng1*
1Wangxuan Institute of Computer Technology, Peking University2ByteDance Inc
cuizhenyu@stu.pku.edu.cn, {jiahuanzhou,pengyuxin }@pku.edu.cn, {wangxun.2,zhumanyu }@bytedance.com
Abstract
Lifelong Person Re-identiﬁcation (L-ReID) aims to learn
from sequentially collected data to match a person across
different scenes. Once an L-ReID model is updated usingnew data, all historical images in the gallery are required to
be re-calculated to obtain new features for testing, known as“re-indexing”. However , it is infeasible when raw imagesin the gallery are unavailable due to data privacy concerns,resulting in incompatible retrieval between the query and
the gallery features calculated by different models, which
causes signiﬁcant performance degradation. In this pa-per , we focus on a new task called Re-indexing Free Life-long Person Re-identiﬁcation (RFL-ReID), which requiresachieving effective L-ReID without re-indexing raw imagesin the gallery. To this end, we propose a Continual Com-patible Representation (C
2R) method, which facilitates the
query feature calculated by the continuously updated modelto effectively retrieve the gallery feature calculated by theold model in a compatible manner . Speciﬁcally, we designa Continual Compatible Transfer (CCT) network to con-tinuously transfer and consolidate the old gallery featureinto the new feature space. Besides, a Balanced Compati-ble Distillation module is introduced to achieve compatibil-ity by aligning the transferred feature space with the newfeature space. Finally, a Balanced Anti-forgetting Distilla-tion module is proposed to eliminate the accumulated for-
getting of old knowledge during the continual compatible
transfer . Extensive experiments on several benchmark L-ReID datasets demonstrate the effectiveness of our methodagainst state-of-the-art methods for both RFL-ReID and L-ReID tasks. The source code of this paper is available athttps://github.com/PKU-ICST-MIPL/C2R
CVPR2024 .
1. Introduction
Person re-identiﬁcation (ReID) aims to identify the same
person across different areas at different times [ 42]. Exist-
*Corresponding author.Query Images
Query Features
(a) Traditionial Lifelong ReIDID:1
Re-indexed
Gallery FeaturesID:5
ID:1      ID:2
Old Gallery ImagesReID w/ Re-indexing ReID w/o Re- indexing
ɌɌɁɁ
Query Features
(b) Re-indexing Free Lifelong ReIDOld Gallery FeaturesRe-indexingNew Model
Query Images
ID:1      ID:2
Old Gallery ImagesID:3 ID:5ɁɁɁɁ Data
Privacy
××New ModelOld Gallery FeaturesFeature Replacing
Figure 1. Comparison between (a) the traditional Lifelong Per-
son Re-identiﬁcation (L-ReID) task and (b) the Re-indexing FreeLifelong Person Re-identiﬁcation (RFL-ReID) task.
ing methods [ 26,32,44] have made remarkable progress
based on deep learning methods [ 37] and large-scale
datasets [ 34,41,43]. However, their performance is often
limited when training data are continuously collected froma series of different scenarios due to the well-known catas-trophic forgetting challenge [ 3].
Recently, Lifelong person ReID (L-ReID) has aroused
great concerns involving acquiring knowledge from stream-ing data and performing well across all data [ 5,18,19,38].
Its challenge is to balance the anti-forgetting of old knowl-
edge with the acquisition of new knowledge. To this end,existing L-ReID methods usually adopt the exemplar re-play [ 38] and the knowledge distillation [ 27] to preserve
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
16614
R@1 Performance
w/o Re-indexingR@1 Performance
w/ Re-indexing
mAP Performance
w/o Re-indexingmAP Performance
w/ Re-indexing
 -17.6-9.3
Figure 2. Performance Comparison between the traditional Life-
long Person Re-identiﬁcation (L-ReID) task and the Re-indexingFree Lifelong Person Re-identiﬁcation (RFL-ReID) task.
old knowledge when updating the model. The updated new
model will be sequentially deployed by “re-indexing” [ 24]
features of old data in the gallery. As shown in Fig. 1
(a), the old gallery features are replaced by the re-indexedgallery features to be compatible with the new model. How-
ever, massive incremental images in the gallery bring un-
bearable computational costs and prevent the re-indexingprocess [ 21,39]. Besides, in the context of data privacy
concerns [ 25], re-indexing is legally forbidden due to the
impracticality of storing and re-indexing unauthorized rawimages within privacy-sensitive scenarios [ 1,4,14], also
known as Re-indexing Free Lifelong ReID (RFL-ReID), asshown in Fig. 1(b). In this case, the domain gap between
different datasets leads to the incompatibility between fea-tures calculated by the old and the new models. Conse-quently, as shown in Fig. 2, the performance of existing
methods [ 11,18,27] often degrades signiﬁcantly.
In this paper, we focus on a practical and challeng-
ing task called Re-indexing Free Lifelong Person Re-identiﬁcation (RFL-ReID), which entails deploying L-ReIDmodels continuously without re-indexing raw images in thegallery. It is extremely challenging when considering notonly balancing the acquisition of old and new knowledge
but also achieving the compatibility between the old and
the new feature space. To this end, Compatible Train-ing (CT) [ 16,21,23] becomes a feasible solution which
promotes the compatibility. Unfortunately, most existingCT methods only focus on compatibility within the samedataset. However, the large domain gap [ 5,19] between
different ReID datasets typically leads to the forgetting of
old knowledge, causing the feature shift problem, which se-riously degrades the performance of existing CT methods.
Inspired by the above observation, we propose a Con-
tinual Compatible Representation (C
2R) scheme for RFL-
ReID. The core idea of C2R is to continuously update old
features in the gallery to make it compatible with new queryfeatures. To tackle the domain shift problem, a Contin-ual Compatible Transfer (CCT) network is proposed to up-
date old gallery features continuously, which transfers theold feature into the new feature space by adaptively cap-turing the knowledge from different domains. Correspond-
ingly, to achieve the compatibility between the transferredfeatures and the new ones, a Balanced Compatible Distil-lation (BCD) module is designed to preserve the relation-
ship between the old and the transferred features in a uni-ﬁed feature space. Finally, to eliminate the accumulatedforgetting of old knowledge during the continuous transfer,
a Balanced Anti-forgetting Distillation (BAD) module is in-troduced to balance the old and the new discriminative in-formation. Overall, the main contributions of this paper canbe summarized as follows:
1) This paper proposes a Continual Compatible Repre-
sentation scheme to solve the challenging Re-indexing FreeLifelong Person Re-identiﬁcation task, which prevents re-indexing to raw images in the old gallery.
2) A continual compatible transfer network is designed
to adaptively capture the old and the new knowledge, whichpromotes the updating of old gallery features.
3) To achieve compatibility of the old and the new data, a
balanced compatible distillation module is designed to bal-ance the relationship between the old and the new featuresin a uniﬁed feature space.
4) A balanced anti-forgetting distillation module is in-
troduced to balance the acquisition of the old and the newknowledge without rehearsing the old data.
2. Related Work
2.1. Lifelong Person Re-identiﬁcation
Lifelong Person Re-identiﬁcation (L-ReID) aims to use
streaming data from different domains to train a ReID
model to continuously match the same person across all do-mains [ 18]. It requires continuously accumulating knowl-
edge from new data and preventing catastrophic forgettingof knowledge learned in old data. To this end, rehearsal-based methods rehearsed old data for the anti-forgetting ofold knowledge [ 5,35,38]. Yu et al. [ 38] proposed a knowl-
edge refreshing and consolidation framework to achieveboth positive forward and backward transfers, which si-multaneously promotes person matching in both old andnew domains. However, limited by data privacy concerns,
the old data is usually difﬁcult to rehearse for L-ReID.
Therefore, some methods focus on developing rehearsal-free methods [ 18,19,27,33]. Sun et al. [ 27] proposed a
patch-based knowledge distillation method, which can mit-igate catastrophic forgetting problems by preserving patch-level knowledge within individual patch features and mu-
tual relations. Despite achieving some progress, existing
methods require re-indexing old data in the gallery aftertraining on new data to adapt to the updated model. In thispaper, we focus on a new and challenging task called RFL-ReID, which prohibits re-indexing the old data in the gallerydue to data privacy issues.
16615
MLP
Old Model New ModelForward Mapping Module (FMM)×4
am
1-am
MultiplyMultiply
(a) Training at sth StageContinual Compatible Transfer (CCT)
FC1BNPReLUFC2BNPReLUFC3BNPReLU
(b) Transfer at sthStage
Data Stream zo
zn znzw
Old Gallery 
Feature SetCCT at sth stageDs
ऑsTransferred
Feature SetID: 1 ID: N...
ϕnϕo
v1
v2
vp...
Knowledge Capturing Module (KCM)∑k1
k2
kp...
Softmax
FCFCFCSoftmax
SumBalanced Compatible Distillation (BCD)
Balanced Anti-forgetting Distillation (BAD)znzw
खcaz1ozBo...
z1o
zBo...z1wzBw...
z1w
zBwखcr
Similarity of
Same PersonSimilarity of
Different Person
zwzoOld Classifierखbase
ψoखadग ग
qwqo
खad
ID: 1 ID: N...
ID: N+1 ID: M...
New Model
Extracted
Featureszo
zwznzo
ऐsID: 1...
ID: M
QueryNew 
Model
New Gallery Feature Setϕn
ID: 182ɌɌ
(c) Testing at sthStageOld data
New data
... ...
ϕn
ऐs-1ऐs
Figure 3. The architecture of our proposed Continual Compatible Representation (C2R) method. Our C2R consists of a Continual Com-
patible Transfer (CCT) network, a Balanced Compatible Distillation (BCD) module, and a Balanced Anti-forgetting Distillation (BAD)module. In the training phase, all the above components will be trained, and the CCT network will be used to update the old feature set,
thereby achieving re-indexing free lifelong person re-identiﬁcation.
2.2. Compatible Training
Compatible Training (CT) aims to achieve the compatibil-
ity between the old model and the new model without re-indexing, which encourages the feature calculated by thenew model to be closer to the same object feature calcu-lated by the old model. Existing CT methods can be cate-gorized into two branches: backward CT [ 16,24,39] and
forward CT methods [ 21,30,45]. Pan et al. [ 16] pro-
posed an adversarial learning-based backward CT method,which optimized the distribution discrepancy and improved
the backward compatibility of the updated model. Ra-manujan et al. [ 21] proposed a ﬂexible forward CT method
by employing side-information to facilitate the updating ofthe new model. However, most existing CT methods onlyachieve compatibility between the old and the new modelsupdated using the same dataset, ignoring the requirementto be compatible with multiple different datasets continu-ously. To this end, Wan et al. [ 30] proposed a long-term vi-
sual search framework to realize CT in the continual learn-ing scenario, which allows new classes to occur in the newdata. However, it relies heavily on old data and old fea-tures to achieve the compatibility, which is infeasible forthe privacy-sensitive scenario.
Different from these methods, we propose a contin-
ual compatible representation scheme called C
2R, which
achieves leading L-ReID performance when old data inthe gallery cannot be re-indexed in the practical privacy-sensitive scenario.3. Method
3.1. Problem Formulation
In this paper, we focus on a practical and challenging taskcalled Re-indexing Free Lifelong Person Re-identiﬁcation(RFL-ReID). Formally, given the sequentially collecteddatasetsD={D
1,D2,...,DS}, whereSdenotes the to-
tal number. The sthdatasetDs={Ts,Gs}contains a
training set Tsand the corresponding gallery set Gswith-
out overlapping person identities. Each Ts={xs
i,ys
i}Ns
i=1
containsNstraining images xs
iand the corresponding iden-
tity labels ys
i. Our goal is to train a feature extraction model
φS(·):xi→zito extract the feature zifor each input
imagexi.φS(·)is expected to maximize the similarity be-
tween the images belonging to the same person, therebyperforming well on the overall retrieval performance ofallStasks. Particularly, when training on the s
thdataset
Ds, the previous s−1datasets{D1,...,Ds−1}are abso-
lutely unavailable due to data privacy concerns, includingthe training sets {T
j}s−1
j=1and the gallery sets {Gj}s−1
j=1.
Therefore, each image in Gjcan only be extracted once
based on the model φj(·)within the jthtraining stage.
3.2. Overview
As shown in Fig. 3, our proposed Continual Compatible
Representation (C2R) method consists of a Continual Com-
patible Transfer (CCT) network, a Balanced CompatibleDistillation (BCD) module, and a Balanced Anti-forgetting
16616
Distillation (BAD) module. At the beginning of the sth
training stage ( s≥2), a copy of the model φoand the clas-
siﬁerψoare frozen to serve as the old knowledge. Then, we
train the model with the CCT network based on the BCDand the BAD modules, as shown in Fig. 3(a). After the
training of the s
thstage, we reform a new gallery feature
setFsby updating the transferred features in the old gallery
feature set Fs−1using our proposed CCT network, and ex-
tracting the features of the current gallery set Gsusingφs
(Fig. 3(b)). When testing at the sthstage, we rank each
feature in Fswith the query feature calculated by φs(Fig. 3
(c)) to validate the ReID performance on all sdatasets.
3.3. Lifelong Person Re-identiﬁcation Baseline
In this section, we start by presenting a lifelong person re-
identiﬁcation baseline. The baseline model consists of a
feature extraction model φ(·)and a classiﬁer head ψ(·),
whereφ(·)extracts a feature zifor each input image xi, and
ψ(·)predicts the probability piof each person’s identity.
Considering that the RFL-ReID task aims to continu-
ously match the same person across the sequentially givendatasets, we employ a cross-entropy loss L
ce[13] and a
triplet loss Ltrip[8] to learn a discriminative representation
for each person in the current dataset Ds. However, de-
spite certain performance improvements in Ds, it has been
veriﬁed to cause the catastrophic forgetting problem on theprevious (s−1)datasets [ 3]. To balance the acquisition of
new knowledge and the anti-forgetting of old knowledge,we introduce an anti-forgetting loss L
pkd[27] based on im-
age patch knowledge distillation to build a basic L-ReIDmodel. Therefore, the overall loss function for our baselinemodel is formulated as:
L
base =Lce+Ltrip+Lpkd. (1)
The above baseline method achieves a performance
trade-off between new and old datasets by re-indexing datain old galleries using the new model. However, when theold data is unavailable to the new model due to data privacy
concerns, the performance will be signiﬁcantly degraded
due to the incompatibility of the features output by the oldmodel and the new model. To tackle the above issue, wewill detail present each component of our C
2R scheme in
the following sections.
3.4. Continual Compatible Transfer Network
To tackle the domain shift problem, we design a ContinualCompatible Transfer (CCT) network to transfer the old fea-ture into the new feature space continuously. To this end,our CCT is designed as a two-stream network to capturetransfer-related knowledge from different domains.
Speciﬁcally, we ﬁrst design a Knowledge Capturing
Module (KCM) module to capture knowledge from new do-mains. Let z
o∈RCbe the feature output by the old modelφo(·)based on the sample xs,vp∈RP×Cbe a learnable
new knowledge prototype set with length P. In order to
enable zoto capture new knowledge for compatibility, we
ﬁrst calculate a capturing probability ko∈RPfor each new
knowledge prototype:
ko=δ(gc(/tildewidezo)), (2)
wheregc(·)denotes a three-layer fully connected network,
δ(·)denotes the softmax function, and (/tildewide·)denotes the l2-
normalized function. Then, we combine the captured newknowledge with a weighted summation of the prototypes:
z
t=P/summationdisplay
p=1ko
i·vp. (3)
Sequentially, a parallel Forward Mapping Module
(FMM) is designed to directly map the old feature to the
new one, thereby preserving the old knowledge from old do-
mains. The designed mapping network consists of sequen-tial fully connected layers, Batch Normalization [ 9] layers,
and PReLU [ 6] layers. The ﬁrst fully connected layer re-
duces the channel number to C
0, while the last layer recov-
ers it back to the original number, which keeps the repre-
sentation capability while saving redundant operations:
zm=gs(/tildewidezo). (4)
Finally, CCT takes the input /tildewidezoto generate a factor am
to adaptively balance the above two features ( ztandzm)
obtained by the KCM module and FMM module. Mean-while, the original features are added by the residual skip-connection to alleviate the gradient vanishing issue [ 31].
The transferred result z
wof CCT network can be formu-
lated as follows:
zw=( 1−am)·zt+am·zm+/tildewidezo. (5)
Following [ 14], we leverage 4 cascaded CCT networks
to build up a strong transfer network, which sequentiallytransfers features from previous stages to the current stage.
3.5. Balanced Compatible Distillation
To make the transferred features compatible with the newone, we design a Balanced Compatible Distillation (BCD)module to simultaneously preserve the relationships of theold and the transferred features in the new feature space.
Given a mini-batch of training samples {x
s
i,ys
i}Bi=1,
whereBis the batch size, and a new feature zn
icalculated
by the new model φ(·)n. We start by directly aligning the
above features with l2-loss:
Lca=−1
BB/summationdisplay
i=1||/tildewidezin−/tildewideziw||2. (6)
16617
The above alignment loss promotes compatibility of new
features in the new feature space, while ignoring the rela-tionship between the old features, thus limiting the ReIDperformance on old data. Therefore, we propose a relation-ship distillation loss to overcome the above issue.
Firstly, we construct an old similarity matrix M
o∈
RB×B, which is calculated by the afﬁnity between each two
features in {/tildewidezo
i}Bi=1to represent the relationship between
the corresponding images in the old feature space:
Mo
i,j=e/angbracketleft/tildewidezo
i,/tildewidezo
j/angbracketright
/summationtextB
k=1e/angbracketleft/tildewidezo
i,/tildewidezo
k/angbracketright,(i,j∈[1,B]), (7)
where/angbracketleft·,·/angbracketrightdenotes the cosine similarity. Similarly, a trans-
ferred similarity matrix Mwis also constructed to represent
the relationship between the transferred features {/tildewidezw
i}Bi=1.
Apparently, the alignment in Eq. ( 6) will promote higher
similarity between samples with the same identity in Mw,
which will conﬂict with the corresponding similarity inM
o. Therefore, we remove the similarities between sam-
ples belonging to the same identity to avoid the above con-ﬂicts for relationship distillation:
M
x
i,j=/braceleftBigg
Mx
i,j,ID(i)/negationslash=ID(j)
0,I D (i)=ID(j),(8)
whereID(·)denotes the identity of the given sample, and
x∈o,w. Next, a widely-used knowledge distillation loss
Lcrbased on Kullback-Leibler (KL) divergence is imposed
to distil the relationship between old features as follows:
Lcr=−1
BB/summationdisplay
i=1σ(Mo
i,:)log(σ(Mo
i,:)
σ(Mw
i,:)), (9)
whereσ(·)denotes the row-wise l1-normalized function.
In all, the loss function for the BCD module can be cal-
culated as follows:
Lbcd=μ1Lca+μ2Lcr, (10)
whereμ1andμ2denote two hyper-parameters to balance
our proposed balanced compatible distillation.
3.6. Balanced Anti-forgetting Distillation
Although the proposed BCD module achieves the compati-bility by selectively distilling the relationship, it ignores theaccumulated forgetting of old knowledge during the contin-ual compatible transfer, thereby reducing the inter-class dis-crimination of old data. Therefore, we propose a BalancedAnti-forgetting Distillation (BAD) module to preserve thediscriminative information within the old and new featuresin the new feature space.
To this end, we employ the old classiﬁer head ψ
o(·)to
extract and distil the discriminative information. Therefore,we extract old identity logits qoof the old feature zoas the
theoretical distribution of the discriminative information:
qo=δ(ψo(zo)). (11)
Considering that the transferred feature zwandzocome
from different datasets, it is challenging to extract consistentdiscriminative information directly using ψ
o(·). To tackle
the above problem, we perform an inverse l2-normalizationonz
wso that it can be extracted as the real distribution of
the transferred discriminative information:
qw=δ(ψo(/tildewidezw·||zo||2)). (12)
Sequentially, we use an anti-forgetting loss Ladto pre-
serve the old discriminative knowledge, which can be cal-
culated as follows:
Lad=−1
BB/summationdisplay
i=1qo
ilog(qo
i
qw
i). (13)
Despite achieving the anti-forgetting, the distillation in
Eq. ( 13) will also limit the discrimination of the transferred
feature in the new feature space. Therefore, we introduce
a discriminative consistency-based loss Ldcto balance the
discrimination for both transferred and new features:
Ldc=−1
BB/summationdisplay
i=1(1−cos/angbracketleft/tildewideziw−/tildewidezio
||/tildewideziw−/tildewidezio||2,/tildewidezin−/tildewidezio
||/tildewidezin−/tildewidezio||2/angbracketright).
(14)
The overall loss function for the BAD module can be
calculated as follows:
Lbad=μ3Lad+μ4Ldc, (15)
whereμ3andμ4denote two hyper-parameters when train-
ing the model.
3.7. Objective Function
Finally, the total loss Lof C2R is calculated as follows:
L=Lbase+Lbcd+Lbad. (16)
Through the joint optimizing of L, our C2R achieves the
compatibility between the old and the new models and pre-
vents re-indexing to raw images in the old gallery.
4. Experiments
4.1. Datasets and Evaluation Metrics
Datasets. To verify the effectiveness of our method, we
conduct extensive experiments on ﬁve benchmark lifelongperson ReID datasets, including Market-1501 [ 41], CUHK-
SYSU [ 36], DukeMTMC-ReID [ 43], MSMT17-V2 [ 34]
and CUHK03 [ 12]. For CUHK-SYSU, we follow the
16618
Task MethodMarket-1501 CUHK-SYSU DukeMTMC-ReID MSMT17-V2 CUHK03 Average
mAP R@1 mAP R@1 mAP R@1 mAP R@1 mAP R@1 mAP R@1
L-ReIDJointTrain 68.1 85.2 81.4 83.8 60.4 75.7 24.6 48.9 42.7 43.6 55.4 67.5
SPD [ 28] 35.6 61.2 61.7 64.0 27.5 47.1 5.2 15.5 42.2 44.3 34.4 46.4
LwF [ 11] 56.3 77.1 72.9 75.1 29.6 46.5 6.0 16.6 36.1 37.5 40.2 50.6
CRL [ 40] 58.0 78.2 72.5 75.1 28.3 45.2 6.0 15.8 37.4 39.8 40.5 50.8
AKA [ 18] 58.1 77.4 72.5 74.8 28.7 45.2 6.1 16.2 38.7 40.4 40.8 50.8
MEGE [ 20]39.0 61.6 73.3 76.6 16.9 30.3 4.6 13.4 36.4 37.1 34.0 43.8
PatchKD [ 27]68.5 85.7 75.6 78.6 33.8 50.4 6.5 17.0 34.1 36.8 43.7 53.7
Ours 69.0 86.8 76.7 79.5 33.2 48.6 6.6 17.4 35.6 36.2 44.2 53.7
RFL-ReIDLwF* [ 11] 39.1 58.0 40.0 40.7 7.8 15.3 2.6 7.1 23.3 23.9 22.6 29.0
AKA* [ 18] 36.1 52.2 38.6 37.6 7.6 13.8 3.1 8.3 26.5 26.5 22.4 27.7
CVS* [ 30] 38.8 55.6 49.0 49.7 19.3 30.0 4.6 11.5 24.7 24.7 27.3 34.3
PatchKD* [ 27]61.4 78.4 57.8 59.0 20.8 34.4 5.1 12.8 36.0 37.6 36.2 44.4
Ours 62.7 79.7 64.4 66.3 26.7 41.7 6.8 15.7 37.2 37.6 39.5 48.2
Table 1. Performance on training Order-1 : Market-1501 →CUHK-SYSU →DukeMTMC-ReID →MSMT17-V2 →CUHK03.
Task MethodDukeMTMC-ReID MSMT17-V2 Market-1501 CUHK-SYSU CUHK03 Average
mAP R@1 mAP R@1 mAP R@1 mAP R@1 mAP R@1 mAP R@1
L-ReIDJointTrain 60.4 75.7 24.6 48.9 68.1 85.2 81.4 83.8 42.7 43.6 55.4 67.5
SPD [ 28] 28.5 48.5 3.7 11.5 32.3 57.4 62.1 65.0 43.0 45.2 33.9 45.5
LwF [ 11] 42.7 61.7 5.1 14.3 34.4 58.6 69.9 73.0 34.1 34.1 37.2 48.4
CRL [ 40] 43.5 63.1 4.8 13.7 35.0 59.8 70.0 72.8 34.5 36.8 37.6 49.2
AKA [ 18] 42.2 60.1 5.4 15.1 37.2 59.8 71.2 73.9 36.9 37.9 38.6 49.4
MEGE [ 20]21.6 35.5 3.0 9.3 25.0 49.8 69.9 73.1 34.7 35.1 30.8 40.6
PatchKD [ 27]58.3 74.1 6.4 17.4 43.2 67.4 74.5 76.9 33.7 34.8 43.2 54.1
Ours 59.7 75.0 7.3 19.2 42.4 66.5 76.0 77.8 37.8 39.3 44.7 55.6
RFL-ReIDLwF* [ 11] 15.0 22.9 1.2 3.2 9.5 19.4 38.8 37.5 20.2 19.6 16.9 20.5
AKA* [ 18] 11.1 15.1 1.3 3.2 13.4 27.3 35.9 34.7 25.2 25.6 17.4 21.2
CVS* [ 30] 29.0 41.9 3.5 9.4 30.7 49.6 60.0 61.2 28.5 29.9 30.3 38.4
PatchKD* [ 27]46.5 60.9 4.0 10.4 31.1 50.5 63.0 64.0 35.8 36.6 36.1 44.5
Ours 48.4 63.6 6.2 14.9 37.0 55.6 67.4 68.4 39.2 39.5 39.7 48.4
Table 2. Performance on training Order-2 : DukeMTMC-ReID →MSMT17-V2 →Market-1501 →CUHK-SYSU →CUHK03.
same pre-processing and evaluation of GwFReID [ 35],
which crops the person images with the ground-truth per-son bounding box annotation and re-organized as a ReIDdataset with corresponding identities. To simulate the life-
long person ReID process in the real scenarios, we eval-
uate our method by employing two training orders usedin [18], including Order-1 : Market-1501 →CUHK-SYSU
→DukeMTMC-ReID →MSMT17-V2 →CUHK03 and
Order-2 : DukeMTMC-ReID →MSMT17-V2 →Market-
1501→CUHK-SYSU →CUHK03.
Evaluation Metrics. We employ mean Average Precision
(mAP) [ 41] and Rank@1 accuracy (R@1) [ 15] to evalu-
ate our method on each datasets. The above metrics of ourmethod are reported after sequentially training all datasets.Besides, the corresponding mean accuracy is also reported
to evaluate the overall performance on all tasks. Besides,the Average Forgetting (AF) [ 2] on the above metrics are
reported to evaluate the anti-forgetting performance of ourmethod, which averages the difference between the highestaccuracy and the ﬁnal accuracy on each dataset throughout
the lifelong learning process.
4.2. Implementation Details
Our proposed C2R is implemented with PyTorch [ 17]o n
NVIDIA A40 GPUs. We adopt a ResNet-50 [ 7] network
pre-trained on ImageNet [ 22] as our backbone network. In
each training stage, we train the model for 50 epochs with150 iterations. We use Adam [ 10] optimizer for training.
The learning rate is set to 3.5×10
−4initially, which de-
cayed by 0.1 at the 25thand the 35thepochs. We set the
hyper-parameters μ1,μ2,μ3, andμ4as 50, 1, 0.01 and 0.05,
respectively. The prototype length Pand the channel num-
berC0are empirically set to 16 and 32. Input images are
resized to 256 ×128. The batch size is set to 128, with 4
images for each identity. Following [ 27], each dataset is
randomly sampled with 500 identities to alleviate the prob-lem of unbalanced class numbers among different datasetsfor fairness comparison.
16619
Method LwF AKA CVS PatchKD Ours
AF(mAP) 24.8 26.4 21.0 16.5 13.9
AF(R@1) 30.8 35.0 25.7 18.5 14.5
Table 3. AF performance of C2R compared with existing methods.
(The lower the AF is, the less the model forgets.)
4.3. Comparison with State-of-the-arts Methods
In this section, we compared our C2R with several state-of-
the-art methods on both RFL-ReID and traditional L-ReIDtasks, including SPD [ 28], LwF [ 11], CRL [ 40], AKA [ 18],
MEGE [ 20], PatchKD [ 27], and CVS [ 30], where CSV is a
continual compatible training method and the others are L-ReID methods. The best results are bolded , and the second-
best results are underlined
.
Comparison on the RFL-ReID task. Tab. 1and Tab. 2
summarize the results of our method compared to exist-
ing methods, which are reproduced by the released codeand denoted as ∗. Compared with these methods, our C
2R
achieves the highest average mAP and R@1 accuracy by39.5%/48.2% on Order-1 and 39.7%/48.4% on Order-2 ,
leading PatchKD by 3.3%/3.8% and 3.6%/3.9%, respec-tively. This is because our C
2R method can achieve com-
patible transfer for old gallery features by balancing the
anti-forgetting of old knowledge with the compatibility tothe new model, thereby updating old features without re-indexing old data effectively.
In addition, Tab. 3reports the AF performance of mAP
and R@1 on Order-1 . It can be seen that our C
2R achieves
the lowest AF on both mAP and R@1 accuracy, which islower than PatchKD by 2.6% and 4.0%, respectively. No-tably, compared to CVS, our C2R does not introduce ad-ditional old data while achieving the best RFL-ReID accu-racy and the best AF performance. Speciﬁcally, as shown inFig.4, our proposed C
2R achieves the best average perfor-
mance on the seen datasets in each stage. The above resultsintuitively demonstrate that our proposed C
2R outperforms
SOTA methods on average R@1 and mAP at each stage.
The above results imply that our C2R can adaptively map
old features and capture new knowledge, thus balancing
compatibility with new feature spaces by preserving old dis-
criminative knowledge.
Comparison on the traditional L-ReID task. We fur-
ther evaluate our method on the traditional L-ReID task.As shown in Tab. 1and Tab. 2, our proposed C
2R also
achieves state-of-the-art performance on both two trainingorders, which leads PatchKD on mAP accuracy by 0.5%and 1.5% on Order-1 and Order-2 , respectively. The above
results imply that our method achieves the best RFL-ReIDperformance without compromising the performance of tra-
ditional L-ReID. Therefore, our C
2R has better generaliza-
tion in both general and privacy-sensitive scenarios.Figure 4. Lifelong learning performance comparison in RFL-ReID
task on mAP and R@1 accuracy.
am0.0 0.2 0.4 0.6 0.8 1.0 Ours
mAP 38.2 38.7 39.0 38.4 38.9 38.7 39.5
R@1 46.3 46.9 47.6 47.5 47.8 47.7 48.2
Table 4. Ablation study on different balancing factors in continualcompatible transfer network.
4.4. Ablation Studies
In this section, we conduct ablation studies on Order-1 to
evaluate the effectiveness of each component in our C2R.
Our Base method is the baseline introduced in Sec. 3.3.
Effectiveness of CCT network. We compared the adap-
tively balancing strategy in our continual compatible trans-fer network with a ﬁxed balancing strategy, which intro-
duces ﬁxed values for a
min Eq. ( 5). As shown in Tab. 4, our
method achieves the best mAP and R@1 accuracy amongthe ﬁxed balancing strategies. It indicates that by adaptivelygenerating the most appropriate fusion factor through dif-ferent old features, our CCT network balances the forwardmapping of old knowledge and the knowledge capturing of
new knowledge, verifying its superior effectiveness.
Effectiveness of BCD module. As shown in Tab. 5,w e
evaluate the two important loss functions L
ca(Eq. ( 6)) and
Lcr(Eq. ( 9)) in the balanced compatible distillation mod-
ule. It can be seen that compared with the Base method,L
caimproves the average mAP/R@1 by 1.7% and 1.2%,
and the full version of the BCD module improves by 1.9%and 2.3%. The above results indicate that the alignment in
L
caand the relationship distillation in Lcrare both impor-
tant for RFL-ReID. The former ensures that old features cancontinue to be compatible with the new ones, while the lat-ter preserves the relationship within the old feature space.Finally, the BCD module achieves promising improvement
by modelling the above functions in a balanced manner.
Effectiveness of BAD module. We further evaluate two
main components L
ad(Eq. ( 13)) andLdc(Eq. ( 14)) in the
balanced anti-forgetting module. As shown in Tab. 5,Lad
brings an improvement by 1.1% on average mAP accuracy.
When combined with Ldc, the accuracy further improved
by 0.3%. It indicates that by distilling the discriminative
16620
Base CCTBCD BADmAP R@1LcaLcrLadLdc
/check - - - - - 36.2 44.4
/check /check /check - - - 37.9 45.6
/check /check /check/check - - 38.1 46.7
/check /check /check/check/check - 39.2 47.1
/check /check /check/check/check/check 39.5 48.2
Table 5. Ablation studies of each component in C2Ro n Order-1 .
information within the old feature, BAD balances the anti-
forgetting of the old discrimination knowledge with the ac-quisition of the new knowledge and thus plays an important
role in our C
2R method.
Hyper-parameter Study. There are 4 hyper-parameters for
the training of our C2R method, i.e.μ1,μ2,μ3, andμ4.
Therefore, we evaluate the effect of the above parametersrespectively, as shown in Fig. 6. It can be seen that a slight
or excessive μ
1will eliminate the alignment of the old fea-
ture to the new feature. In addition, as μ2increases, the re-
lationship between old features will prevent the model from
learning new knowledge. Therefore, the moderate μ1(=50)
andμ2(=1) are chosen in the BCD module to balance the
relationship between the old and new features. Similarly,we set a moderate μ
3(=0.01) and μ4(=0.05) to balance the
discriminative information in the old and new features.
4.5. Visualization
To intuitively verify the effectiveness of our C2R, we use
t-SNE [ 29] to visualize and compare the gallery feature
and query feature calculated by our method with the Basemethod. The features are randomly selected from the ﬁvebenchmark datasets. As shown in Fig. 5, the features cal-
culated by our C
2R achieve tighter intra-class representa-
tion while retaining more inter-class discriminability. Theabove results indicate that our C
2R can preserve more dis-
criminative information after transferring old gallery fea-tures, which can be further aligned by new query features toachieve RFL-ReID.
(a) Base (b) our C2R
Figure 5. TSNE results of our C2R compared with Base method.
Different colours represent different identities, while the circlesand the triangles represent gallery and query features, respectively.Figure 6. The inﬂuence of hyper-parameters in our C2R.
4.6. Limitation and Future Work
Our C2R can support L-ReID without re-indexing old data
in the gallery. However, it is difﬁcult to capture enough newknowledge to facilitate feature transfer and updating when
the amount of new data is limited. Therefore, it is worthy
of future research on efﬁcient transfer under a few-shot sce-nario. In addition, when there is noise in new data, howto utilize old knowledge to eliminate such noise to achieveselective transfer should be studied in the future.
5. Conclusion
In this paper, we focus on a practical and challenging task
called Re-indexing Free Lifelong Person Re-identiﬁcation
(RFL-ReID), which prohibits the re-indexing of raw imagesin the gallery due to data privacy concerns. To this end,we propose a Continual Compatible Representation (C
2R)
method, which introduces a Continual Compatible Transfer(CCT) network to continuously transfer old gallery features
to the new feature space. Besides, a Balanced Compatible
Distillation module and a Balanced Anti-forgetting Distil-lation module are proposed to balance the anti-forgettingof old knowledge with the compatibility to the new model.Extensive experiments on ﬁve widely-used benchmark L-ReID datasets verify the effectiveness of our method on the
RFL-ReID task while maintaining the state-of-the-art per-
formance on the general L-ReID scenario.
Ackowledgements
This work was supported by the National Natural ScienceFoundation of China (61925201, 62132001, 62376011).
16621
References
[1] Shaﬁq Ahmad, Gianluca Scarpellini, Pietro Morerio, and
Alessio Del Bue. Event-driven re-id: A new benchmark andmethod towards privacy-preserving person re-identiﬁcation.InProceedings of the IEEE/CVF Winter Conference on Ap-
plications of Computer Vision , pages 459–468, 2022. 2
[2] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan-
than, and Philip HS Torr. Riemannian walk for incremen-tal learning: Understanding forgetting and intransigence. In
Proceedings of the European conference on computer vision
(ECCV) , pages 532–547, 2018. 6
[3] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ale ˇs Leonardis, Gregory Slabaugh, and
Tinne Tuytelaars. A continual learning survey: Defying for-getting in classiﬁcation tasks. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 44(7):3366–3385, 2021.
1,4
[4] Zekeriya Erkin, Martin Franz, Jorge Guajardo, Stefan
Katzenbeisser, Inald Lagendijk, and Tomas Toft. Privacy-preserving face recognition. In Privacy Enhancing Tech-
nologies: 9th International Symposium, PETS 2009, Seattle,WA, USA, August 5-7, 2009. Proceedings 9 , pages 235–253.
Springer, 2009. 2
[5] Wenhang Ge, Junlong Du, Ancong Wu, Yuqiao Xian, Ke
Yan, Feiyue Huang, and Wei-Shi Zheng. Lifelong personre-identiﬁcation by pseudo task knowledge preservation. InProceedings of the AAAI Conference on Artiﬁcial Intelli-gence , pages 688–696, 2022. 1,2
[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level per-formance on imagenet classiﬁcation. In Proceedings of the
IEEE International Conference on Computer Vision , pages
1026–1034, 2015. 4
[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and patternrecognition , pages 770–778, 2016. 6
[8] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-
fense of the triplet loss for person re-identiﬁcation. arXiv
preprint arXiv:1703.07737 , 2017. 4
[9] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-variate shift. In International conference on machine learn-
ing, pages 448–456. pmlr, 2015. 4
[10] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[11] Dangwei Li, Xiaotang Chen, Zhang Zhang, and Kaiqi
Huang. Learning deep context-aware features over body andlatent parts for person re-identiﬁcation. In Proceedings of
the IEEE conference on computer vision and pattern recog-nition , pages 384–393, 2017. 2,6,7
[12] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep-
reid: Deep ﬁlter pairing neural network for person re-identiﬁcation. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 152–159,
2014. 5[13] Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei
Jiang. Bag of tricks and a strong baseline for deep personre-identiﬁcation. In Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition workshops ,
pages 0–0, 2019. 4
[14] Qiang Meng, Chixiang Zhang, Xiaoqiang Xu, and Feng
Zhou. Learning compatible embeddings. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-sion, pages 9939–9948, 2021. 2,4
[15] Hyeonjoon Moon and P Jonathon Phillips. Computational
and performance aspects of pca-based face-recognition al-
gorithms. Perception , 30(3):303–321, 2001. 6
[16] Tan Pan, Furong Xu, Xudong Yang, Sifeng He, Chen Jiang,
Qingpei Guo, Feng Qian, Xiaobo Zhang, Yuan Cheng, LeiYang, et al. Boundary-aware backward-compatible repre-sentation via adversarial learning in image retrieval. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 15201–15210, 2023. 2,3
[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-perative style, high-performance deep learning library. Ad-
vances in neural information processing systems , 32, 2019.
6
[18] Nan Pu, Wei Chen, Yu Liu, Erwin M Bakker, and Michael S
Lew. Lifelong person re-identiﬁcation via adaptive knowl-edge accumulation. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
7901–7910, 2021. 1,2,6,7
[19] Nan Pu, Yu Liu, Wei Chen, Erwin M Bakker, and Michael S
Lew. Meta reconciliation normalization for lifelong personre-identiﬁcation. In Proceedings of the 30th ACM Interna-
tional Conference on Multimedia , pages 541–549, 2022. 1,
2
[20] Nan Pu, Zhun Zhong, Nicu Sebe, and Michael S Lew. A
memorizing and generalizing framework for lifelong personre-identiﬁcation. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 2023. 6,7
[21] Vivek Ramanujan, Pavan Kumar Anasosalu Vasu, Ali
Farhadi, Oncel Tuzel, and Hadi Pouransari. Forward com-patible training for large-scale embedding retrieval systems.InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 19386–19395, 2022.
2,3
[22] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115:211–252, 2015. 6
[23] Seonguk Seo, Mustafa Uzunbas, Bohyung Han, Xuefei Cao,
Joena Zhang, Taipeng Tian, and Ser-Nam Lim. Metric com-patible training for online backﬁlling in large-scale retrieval.InICML Workshop on Localized Learning (LLW) , 2023. 2
[24] Yantao Shen, Yuanjun Xiong, Wei Xia, and Stefano Soatto.
Towards backward-compatible representation learning. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition , pages 6368–6377, 2020. 2,
3
16622
[25] Shupeng Su, Binjie Zhang, Yixiao Ge, Xuyuan Xu, Yexin
Wang, Chun Yuan, and Ying Shan. Privacy-preserving model
upgrades with bidirectional compatible training in image re-
trieval. arXiv preprint arXiv:2204.13919 , 2022. 2
[26] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin
Wang. Beyond part models: Person retrieval with reﬁnedpart pooling (and a strong convolutional baseline). In Pro-
ceedings of the European Conference on Computer Vision
(ECCV) , pages 480–496, 2018. 1
[27] Zhicheng Sun and Yadong Mu. Patch-based knowledge dis-
tillation for lifelong person re-identiﬁcation. In Proceedings
of the 30th ACM International Conference on Multimedia ,
pages 696–707, 2022. 1,2,4,6,7
[28] Frederick Tung and Greg Mori. Similarity-preserving knowl-
edge distillation. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 1365–1374,
2019. 6,7
[29] Laurens Van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. Journal of machine learning research ,9
(11), 2008. 8
[30] Timmy ST Wan, Jun-Cheng Chen, Tzer-Yi Wu, and Chu-
Song Chen. Continual learning for visual search withbackward consistent feature embedding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 16702–16711, 2022. 3,6,7
[31] Chien-Yi Wang, Ya-Liang Chang, Shang-Ta Yang, Dong
Chen, and Shang-Hong Lai. Uniﬁed representationlearning for cross model compatibility. arXiv preprint
arXiv:2008.04821 , 2020. 4
[32] Faqiang Wang, Wangmeng Zuo, Liang Lin, David Zhang,
and Lei Zhang. Joint learning of single-image and cross-image representations for person re-identiﬁcation. In Pro-
ceedings of the IEEE conference on computer vision and pat-tern recognition , pages 1288–1296, 2016. 1
[33] Kai Wang, Chenshen Wu, Andy Bagdanov, Xialei Liu, Shiqi
Yang, Shangling Jui, and Joost van de Weijer. Positivepair distillation considered harmful: Continual meta metriclearning for lifelong object re-identiﬁcation. arXiv preprint
arXiv:2210.01600 , 2022. 2
[34] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-identiﬁcation. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 79–88, 2018.
1,5
[35] Guile Wu and Shaogang Gong. Generalising without forget-
ting for lifelong person re-identiﬁcation. In Proceedings of
the AAAI conference on artiﬁcial intelligence , pages 2889–
2897, 2021. 2,6
[36] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xi-
aogang Wang. End-to-end deep learning for person search.arXiv preprint arXiv:1604.01850 , 2(2):4, 2016. 5
[37] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling
Shao, and Steven CH Hoi. Deep learning for person re-identiﬁcation: A survey and outlook. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 44(6):2872–
2893, 2021. 1[38] Chunlin Yu, Ye Shi, Zimo Liu, Shenghua Gao, and Jingya
Wang. Lifelong person re-identiﬁcation via knowledge re-
freshing and consolidation. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence , pages 3295–3303, 2023. 1,
2
[39] Binjie Zhang, Yixiao Ge, Yantao Shen, Shupeng Su, Fanzi
Wu, Chun Yuan, Xuyuan Xu, Yexin Wang, and YingShan. Towards universal backward-compatible representa-tion learning. arXiv preprint arXiv:2203.01583 , 2022. 2,
3
[40] Bo Zhao, Shixiang Tang, Dapeng Chen, Hakan Bilen, and
Rui Zhao. Continual representation learning for biomet-ric identiﬁcation. In Proceedings of the IEEE/CVF winter
conference on applications of computer vision , pages 1198–
1208, 2021. 6,7
[41] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identiﬁcation:A benchmark. In Proceedings of the IEEE International
Conference on Computer Vision , pages 1116–1124, 2015. 1,
5,6
[42] Liang Zheng, Yi Yang, and Alexander G Hauptmann. Per-
son re-identiﬁcation: Past, present and future. arXiv preprint
arXiv:1610.02984 , 2016. 1
[43] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identiﬁcationbaseline in vitro. In Proceedings of the IEEE International
Conference on Computer Vision , pages 3754–3762, 2017. 1,
5
[44] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-
ranking person re-identiﬁcation with k-reciprocal encoding.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 1318–1327, 2017. 1
[45] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shil-
iang Pu, and De-Chuan Zhan. Forward compatible few-shotclass-incremental learning. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 9046–9056, 2022. 3
16623
