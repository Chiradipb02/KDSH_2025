ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models
Lukas H ¨ollein1,2Aljaˇz Bo ˇziˇc2Norman M ¨uller2David Novotny2Hung-Yu Tseng2
Christian Richardt2Michael Zollh ¨ofer2Matthias Nießner1
1Technical University of Munich2Meta
https://lukashoel.github.io/ViewDiff/
a stuffed 
bear sitting 
on a 
wooden box
Input Multi-view generated images
Figure 1. Multi-view consistent image generation. Our method takes as input a text description, or any number of posed input images, and
generates high-quality, multi-view consistent images of a real-world 3D object in authentic surroundings from any desired camera poses.
Abstract
3D asset generation is getting massive amounts of attention,
inspired by the recent success of text-guided 2D content
creation. Existing text-to-3D methods use pretrained text-
to-image diffusion models in an optimization problem or
fine-tune them on synthetic data, which often results in non-
photorealistic 3D objects without backgrounds. In this paper,
we present a method that leverages pretrained text-to-image
models as a prior, and learn to generate multi-view images in
a single denoising process from real-world data. Concretely,
we propose to integrate 3D volume-rendering and cross-
frame-attention layers into each block of the existing U-Net
network of the text-to-image model. Moreover, we design an
autoregressive generation that renders more 3D-consistent
images at any viewpoint. We train our model on real-world
datasets of objects and showcase its capabilities to generate
instances with a variety of high-quality shapes and texturesin authentic surroundings. Compared to the existing methods,
the results generated by our method are consistent, and have
favorable visual quality ( −30% FID,−37% KID).
1. Introduction
In recent years, text-to-image (T2I) diffusion models [ 29,33]
have emerged as cutting-edge technologies, revolutionizing
high-quality and imaginative 2D content creation guided by
text descriptions. These frameworks have found widespread
applications, including extensions such as ControlNet [ 53]
and DreamBooth [ 32], showcasing their versatility and poten-
tial. An intriguing direction in this domain is to use T2I mod-
els as powerful 2D priors for generating three-dimensional
(3D) assets. How can we effectively use these models to
create photo-realistic and diverse 3D assets?
Existing methods like DreamFusion [ 27], Fantasia3D [ 5],
and ProlificDreamer [ 47] have demonstrated exciting results
by optimizing a 3D representation through score distillation
sampling [ 27] from pretrained T2I diffusion models. The
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5043
3D assets generated by these methods exhibit compelling
diversity. However, their visual quality is not consistently
as high as that of the images generated by T2I models. A
key step to obtaining 3D assets is the ability to generate
consistent multi-view images of the desired objects and their
surroundings. These images can then be fitted to 3D represen-
tations like NeRF [ 25] or NeuS [ 45]. HoloDiffusion [ 19] and
ViewsetDiffusion [ 38] train a diffusion model from scratch
using multi-view images and output 3D-consistent images.
GeNVS [ 4] and DFM [ 41] additionally produce object sur-
roundings, thereby increasing the realism of the generation.
These methods ensure (photo)-realistic results by training
on real-world 3D datasets [ 31,56]. However, these datasets
are orders of magnitude smaller than the 2D dataset used to
train T2I diffusion models. As a result, these approaches pro-
duce realistic but less-diverse 3D assets. Alternatively, recent
works like Zero-1-to-3 [ 23] and One-2-3-45 [ 22] leverage
a pretrained T2I model and fine-tune it for 3D consistency.
These methods successfully preserve the diversity of gener-
ated results by training on a large synthetic 3D dataset [ 7].
Nonetheless, the produced objects can be less photo-realistic
and are without surroundings.
In this paper, we propose a method that leverages the
2D priors of the pretrained T2I diffusion models to produce
photo-realistic and3D-consistent 3D asset renderings. As
shown in the first two rows of Fig. 1, the input is a text
description or an image of an object, along with the cam-
era poses of the desired rendered images. The proposed
approach produces multiple images of the same object in
a single forward pass. Moreover, we design an autoregres-
sive generation scheme that allows to render more images
atanynovel viewpoint (Fig. 1, third row). Concretely, we
introduce projection and cross-frame-attention layers, that
are strategically placed into the existing U-Net architecture,
to encode explicit 3D knowledge about the generated object
(see Fig. 2). By doing so, our approach paves the way to fine-
tune T2I models on real-world 3D datasets, such as CO3D
[31], while benefiting from the large 2D prior encoded in
the pretrained weights. Our generated images are consistent,
diverse, and realistic renderings of objects.
To summarize, our contributions are:
•a method that utilizes the pretrained 2D prior of text-to-
image models and turns them into 3D-consistent image
generators. We train our approach on real-world multi-
view datasets, allowing us to produce realistic and high-
quality images of objects and their surroundings (Sec. 3.1).
•a novel U-Net architecture that combines commonly used
2D layers with 3D-aware layers. Our projection and cross-
frame-attention layers encode explicit 3D knowledge into
each block of the U-Net architecture (Sec. 3.2).
•an autoregressive generation scheme that renders images
of a 3D object from anydesired viewpoint directly with
our model in a 3D-consistent way (Sec. 3.3).2. Related Work
Text-To-2D. Denoising diffusion probabilistic models
(DDPM) [ 14] model a data distribution by learning to invert
a Gaussian noising process with a deep network. Recently,
DDPMs were shown to be superior to generative adversarial
networks [ 8], becoming the state-of-the-art framework for
image generation. Soon after, large text-conditioned models
trained on billion-scale data were proposed in Imagen [ 33]
or Dall-E 2 [ 29]. While [ 8] achieved conditional generation
via guidance with a classifier, [ 13] proposed classifier-free
guidance. ControlNet [ 53] proposed a way to tune the diffu-
sion outputs by conditioning on various modalities, such as
image segmentation or normal maps. Similar to ControlNet,
our method builds on the strong 2D prior of a pretrained
text-to-image (T2I) model. We further demonstrate how to
adjust this prior to generate 3D-consistent images of objects.
Text-To-3D. 2D DDPMs were applied to the generation of
3D shapes [ 28,34,39,42,44,50,58] or scenes [ 10,15,40]
from text descriptions. DreamFusion [ 27] proposed score dis-
tillation sampling (SDS) which optimizes a 3D shape whose
renders match the belief of the DDPM. Improved sample
quality was achieved by a second-stage mesh optimization
[5,21], and smoother SDS convergence [ 35,47]. Several
methods use 3D data to train a novel-view synthesis model
whose multi-view samples can be later converted to 3D, e.g.
conditioning a 2D DDPM on an image and a relative cam-
era motion to generate novel views [ 23,48]. However, due
to no explicit modelling of geometry, the outputs are view-
inconsistent. Consistency can be improved with epipolar
attention [ 43,57], or optimizing a 3D shape from multi-view
proposals [ 22]. Our work fine-tunes a 2D T2I model to gen-
erate renders of a 3D object; however, we propose explicit
3D unprojection and rendering operators to improve view-
consistency. Concurrently, SyncDreamer [ 24] also add 3D
layers in their 2D DDPM. We differ by training on real data
with backgrounds and by showing that autoregressive gener-
ation is sufficient to generate consistent images, making the
second 3D reconstruction stage expendable.
Diffusion on 3D Representations. Several works model
the distribution of 3D representations. While DiffRF [ 26]
leverages ground-truth 3D shapes, HoloDiffusion [ 19] is
supervised only with 2D images. HoloFusion [ 18] extends
this work with a 2D diffusion render post-processor. Im-
ages can also be denoised by rendering a reconstructing 3D
shape [ 1,38]. Unfortunately, the limited scale of existing 3D
datasets prevents these 3D diffusion models from extrapo-
lating beyond the training distribution. Instead, we exploit
a large 2D pretrained DDPM and add 3D components that
are tuned on smaller-scale multi-view data. This leads to im-
proved multi-view consistency while maintaining the expres-
sivity brought by pretraining on billion-scale image data.
5044
MSE
 MSE
predicted 
noisesampled multi-view 
noise & imagesup/downsample
Cross-Attention
Cross-Frame-Attn
Projection Layer
ResNet Block
multi-branch U-Net (shared weights)RT, K, I,“ text“t~U[0,1000]Figure 2. Method Overview. We augment the U-Net architecture of pretrained text-to-image models with new layers in every U-Net
block. These layers facilitate communication between multi-view images in a batch, resulting in a denoising process that jointly produces
3D-consistent images. First, we replace self-attention with cross-frame-attention (yellow) which compares the spatial features of all views.
We condition all attention layers on pose ( RT), intrinsics ( K), and intensity ( I) of each image. Second, we add a projection layer (green)
into the inner blocks of the U-Net. It creates a 3D representation from multi-view features and renders them into 3D-consistent features. We
fine-tune the U-Net using the diffusion denoising objective (Eq. 3) at timestep t, supervised from captioned multi-view images.
3. Method
We propose a method that produces 3D-consistent images
from a given text or posed image input (see Fig. 1 top/mid).
Concretely, given desired output poses, we jointly generate
all images corresponding to the condition. We leverage pre-
trained text-to-image (T2I) models [ 29,33] and fine-tune
them on multi-view data [ 31]. We propose to augment the
existing U-Net architecture by adding new layers into each
block (see Fig. 2). At test time, we can condition our method
on multiple images (see Fig. 1 bottom), which allows us to
autoregressively render the same object from anyviewpoint
directly with the diffusion model (see Sec. 3.3).
3.1. 3D-Consistent Diffusion
Diffusion models [ 14,36] are a class of gener-
ative models that learn the probability distribution
pθ(x0)=R
pθ(x0:T)dx1:Tover data x0∼q(x0)and latent
variables x1:T=x1, . . . , x T. Our method is based on pre-
trained text-to-image models, which are diffusion models
pθ(x0|c)with an additional text condition c. For clarity, we
drop the condition cfor the remainder of this section.
To produce multiple images x0:N
0at once, which are 3D-
consistent with each other, we seek to model their joint
probability distribution pθ(x0:N
0)=R
pθ(x0:N
0:T)dx0:N
1:T. Simi-
larly to concurrent work by Liu et al. [24], we generate one
set of images pθ(x0:N
0)by adapting the reverse process of
DDPMs [ 14] as a Markov chain over all images jointly:
pθ(x0:N
0:T) :=p(x0:N
T)TY
t=1NY
n=0pθ(xn
t−1|x0:N
t),(1)
where we start the generation from Gaussian noise sam-pled separately per image p(xn
T) =N(xn
T;0,I),∀n∈
[0, N]. We gradually denoise samples pθ(xn
t−1|x0:N
t) =
N(xt−1;µn
θ(x0:N
t, t), σ2
tI)by predicting the per-image
mean µn
θ(x0:N
t, t)through a neural network µθthat is shared
between all images. Importantly, at each step, the model uses
the previous states x0:N
tof all images, i.e., there is commu-
nication between images during the model prediction. We
refer to Sec. 3.2 for details on how this is implemented. To
trainµθ, we define the forward process as a Markov chain:
q(x0:N
1:T|x0:N
0) =TY
t=1NY
n=0q(xn
t|xn
t−1), (2)
where q(xn
t|xn
t−1) =N(xn
t;√1−βtxn
t−1, βtI)and
β1, . . . , β Tdefine a constant variance schedule, i.e., we ap-
ply separate noise per image to produce training samples.
We follow Ho et al. [14] by learning a noise predictor ϵθ
instead of µθ. This allows to train ϵθwith an L2loss:
Ex0:N
0,ϵ0:N∼N (0,I),nhϵn−ϵn
θ(x0:N
t, t)2i
. (3)
3.2. Augmentation of the U-Net architecture
To model a 3D-consistent denoising process over all im-
ages, we predict per-image noise ϵn
θ(x0:N
t, t)through a neu-
ral network ϵθ. This neural network is initialized from the
pretrained weights of existing text-to-image models, and is
usually defined as a U-Net architecture [ 29,33]. We seek to
leverage the previous states x0:N
tof all images to arrive at
a 3D-consistent denoising step. To this end, we propose to
add two layers into the U-Net architecture, namely a cross-
frame-attention layer and a projection layer. We note that the
predicted per-image noise needs to be image specific, since
all images are generated starting from separate Gaussian
5045
noise. It is therefore important to keep around 2D layers
that act separately on each image, which we achieve by
finetuning the existing ResNet [ 11] and ViT [ 9] blocks. We
summarize our architecture in Fig. 2. In the following, we
discuss our two proposed layers in more detail.
Cross-Frame Attention. Inspired by video diffusion [ 49,
51], we add cross-frame-attention layers into the U-Net archi-
tecture. Concretely, we modify the existing self-attention lay-
ers to calculate CFAttn (Q, K, V )=softmax QKT
√
d
Vwith
Q=WQhi,K=WK[hj]j̸=i,V=WV[hj]j̸=i,(4)
where WQ, WK, WVare the pretrained weights for feature
projection, and hi∈RC×H×Wis the input spatial feature of
each image i∈[1, N]. Intuitively, this matches features across
all frames, which allows generating the same global style.
Additionally, we add a conditioning vector to all cross-
frame and cross-attention layers to inform the network about
the viewpoint of each image. First, we add pose information
by encoding each image’s camera matrix p∈R4×4into an
embedding z1∈R4similar to Zero-1-to-3 [ 23]. Addition-
ally, we concatenate the focal length and principal point of
each camera into an embedding z2∈R4. Finally, we provide
an intensity encoding z3∈R2, which stores the mean and
variance of the image RGB values. At training time, we set
z3to the true values of each input image, and at test time,
we set z3=[0.5,0]for all images. This helps to reduce the
view-dependent lighting differences contained in the dataset
(e.g., due to different camera exposure). We construct the
conditioning vector as z=[z1, z2, z3], and add it through a
LoRA-linear-layer [ 16]W′Qto the feature projection matrix
Q. Concretely, we compute the projected features as:
Q=WQhi+s·W′Q[hi;z], (5)
where we set s=1. Similarly, we add the condition via W′K
toK, and W′VtoV.
Projection Layer. Cross-frame attention layers are helpful
to produce globally 3D-consistent images. However, the
objects do not precisely follow the specified poses, which
leads to view-inconsistencies (see Fig. 5 and Tab. 3). To this
end, we add a projection layer into the U-Net architecture
(Fig. 3). The idea of this layer is to create 3D-consistent
features that are then further processed by the next U-Net
layers (e.g. ResNet blocks). By repeating this layer across all
stages of the U-Net, we ensure that the per-image features
are in a 3D-consistent space. We do not add the projection
layer to the first and last U-Net blocks, as we saw no benefit
from them at these locations. We reason that the network
processes image-specific information at those stages and
thus does not need a 3D-consistent feature space.
Inspired by multi-view stereo literature [ 3,17,37], we
create a 3D feature voxel grid from all input spatial features
Aggregator MLP
V olume 
Rendererinput posed 
features
output 
features
ScaleNet 
(1x1 CNN)3D CNNExpandNet
(1x1 CNN)CompressNet
(1x1 CNN)Figure 3. Architecture of the projection layer. We produce 3D-
consistent output features from posed input features. First, we
unproject the compressed image features into 3D and aggregate
them into a joint voxel grid with an MLP. Then we refine the voxel
grid with a 3D CNN. A volume renderer similar to NeRF [ 25]
renders 3D-consistent features from the grid. Finally, we apply a
learned scale function and expand the feature dimension.
h0:N
in∈RC×H×Wby projecting each voxel into each image
plane. First, we compress h0:N
inwith a 1×1convolution to
a reduced feature dimension C′=16. We then take the bilin-
early interpolated feature at the image plane location and
place it into the voxel. This way, we create a separate voxel
grid per view, and merge them into a single grid through an
aggregator MLP. Inspired by IBRNet [ 46], the MLP predicts
per-view weights followed by a weighted feature average.
We then run a small 3D CNN on the voxel grid to refine
the 3D feature space. Afterwards, we render the voxel grid
into output features h0:N
out∈RC′×H×Wwith volumetric ren-
dering similar to NeRF [ 25]. We dedicate half of the voxel
grid to foreground and half to background and apply the
background model from MERF [30] during ray-marching.
We found it is necessary to add a scale function after the
volume rendering output. The volume renderer typically uses
asigmoid activation function as the final layer during ray-
marching [ 25]. However, the input features are defined in an
arbitrary floating-point range. To convert h0:N
outback into the
same range, we non-linearly scale the features with 1 ×1 con-
volutions and ReLU activations. Finally, we expand h0:N
outto
the input feature dimension C. We refer to the supplemental
material for details about each component’s architecture.
3.3. Autoregressive Generation
Our method takes as input multiple samples x0:N
tat once
and denoises them 3D-consistently. During training, we set
N=5, but can increase it at inference time up to memory
constraints, e.g., N=30. However, we want to render an
5046
object from anypossible viewpoint directly with our network.
To this end, we propose an autoregressive image generation
scheme, i.e., we condition the generation of next viewpoints
on previously generated images. We provide the timesteps
t0:Nof each image as input to the U-Net. By varying t0:N,
we can achieve different types of conditioning.
Unconditional Generation. All samples are initialized to
Gaussian noise and are denoised jointly. The timesteps t0:N
are kept identical for all samples throughout the reverse pro-
cess. We provide different cameras per image and a single
text prompt. The generated images are 3D-consistent, show-
ing the object from the desired viewpoints (Figs. 4 and 5).
Image-Conditional Generation. We divide the total num-
ber of samples N=nc+nginto a conditional part ncand
generative part ng. The first ncsamples correspond to im-
ages and cameras that are provided as input. The other ng
samples should generate novel views that are similar to the
conditioning images. We start the generation from Gaussian
noise for the ngsamples and provide the un-noised images
for the other samples. Similarly, we set t0:nc=0for all de-
noising steps, while gradually decreasing tng:N.
When nc=1, our method performs single-image recon-
struction (Fig. 6). Setting nc>1allows to autoregressively
generate novel views from previous images (Fig. 1 bottom).
In practice, we first generate one batch of images uncondi-
tionally and then condition the next batches on a subset of
previous images. This allows us to render smooth trajectories
around 3D objects (see the supplemental material).
3.4. Implementation Details
Dataset. We train our method on the large-scale CO3Dv2
[31] dataset, which consists of posed multi-view images of
real-world objects. Concretely, we choose the categories
Teddybear ,Hydrant ,Apple , and Donut . Per cate-
gory, we train on 500–1000 objects with each 200 images
at resolution 256 ×256. We generate text captions with the
BLIP-2 model [ 20] and sample one of 5 proposals per object.
Training. We base our model on a pretrained latent-
diffusion text-to-image model. We only fine-tune the U-Net
and keep the V AE encoder and decoder frozen. In each itera-
tion, we select N=5images and their poses. We sample one
denoising timestep t∼[0,1000] , add noise to the images ac-
cording to Eq. 2, and compute the loss according to Eq. 3. In
the projection layers, we skip the last image when building
the voxel grid, which enforces to learn a 3D representation
that can be rendered from novel views. We train our method
by varying between unconditional and image-conditional
generation (Sec. 3.3). Concretely, with probabilities p1=0.25
andp2=0.25we provide the first and/or second image as
input and set the respective timestep to zero. Similar to Ruiz
et al. [32], we create a prior dataset and use it during training
to maintain the 2D prior (see supplement for details).Table 1. Quantitative comparison of unconditional image gener-
ation. We report average FID [ 12] and KID [ 2] per category and
improve by a significant margin. This signals that our images are
more similar to the distribution of real images in the dataset. We
mask away the background for our method and the real images to
ensure comparability of numbers with the baselines.
CategoryHF [18] VD [38] Ours
FID↓KID↓FID↓KID↓FID↓KID↓
Teddybear 81.93 0.072 201.71 0.169 49.39 0.036
Hydrant 61.19 0.042 138.45 0.118 46.45 0.033
Donut 105.97 0.091 199.14 0.136 68.86 0.054
Apple 62.19 0.056 183.67 0.149 56.85 0.043
We fine-tune the model on 2 ×A100 GPUs for 60K it-
erations (7 days) with a total batch size of 64. We set the
learning rate for the volume renderer to 0.005 and for all
other layers to 5×10−5, and use the AdamW optimizer [ 33].
During inference, we can increase Nand generate up to 30
images/batch on an RTX 3090 GPU. We use the UniPC [ 55]
sampler with 10 denoising steps, which takes 15 seconds.
4. Results
Baselines. We compare against recent state-of-the-art
works for 3D generative modeling. Our goal is to create
multi-view consistent images from real-world, realistic ob-
jects with authentic surroundings. Therefore, we consider
methods that are trained on real-world datasets and select
HoloFusion (HF) [ 18], ViewsetDiffusion (VD) [ 38], and
DFM [ 41]. We show results on two tasks: unconditional gen-
eration (Sec. 4.1) and single-image reconstruction (Sec. 4.2).
Metrics. We report FID [ 12] and KID [ 2] as common met-
rics for 2D/3D generation and measure the multi-view con-
sistency of generated images with peak signal-to-noise ratio
(PSNR), structural similarity index (SSIM), and LPIPS [ 54].
To ensure comparability, we evaluate all metrics on images
without backgrounds, as not every baseline models them.
4.1. Unconditional Generation
Our method can be used to generate 3D-consistent views of
an object from any pose with only text as input by using our
autoregressive generation (Sec. 3.3). Concretely, we sample
an (unobserved) image caption from the test set for the first
batch and generate N=10 images with a guidance scale [ 13]
ofλcfg=7.5. We then set λcfg=0for subsequent batches, and
create a total of 100 images per object.
We evaluate against HoloFusion (HF) [ 18] and Viewset-
Diffusion (VD) [ 38]. We report quantitative results in Tab. 1
and qualitative results in Figs. 4 and 5. HF [ 18] creates di-
verse images that sometimes show view-dependent floating
artifacts (see Fig. 5). VD [ 38] creates consistent but blurry
5047
HoloFusion (HF) [18] ViewsetDiffusion (VD) [38] OursTeddybear
 Hydrant
 Apple
 Donut
Figure 4. Unconditional image generation of our method and baselines. We show renderings from different viewpoints for multiple
objects and categories. Our method produces consistent objects and backgrounds. Our textures are sharper in comparison to baselines. Please
see the supplemental material for more examples and animations.
images. In contrast, our method produces images with back-
grounds and higher-resolution object details.
4.2. Single-Image Reconstruction
Our method can be conditioned on multiple images in or-
der to render any novel view in an autoregressive fashion
(Sec. 3.3). To measure the 3D-consistency of our generated
images, we compare single-image reconstruction againstViewsetDiffusion (VD) [ 38] and DFM [ 41]. Concretely, we
sample one image from the dataset and generate 20 images
at novel views also sampled from the dataset. We follow
Szymanowicz et al. [38] and report the per-view maximum
PSNR/SSIM and average LPIPS across multiple objects and
viewpoints for all methods. We report quantitative results in
Tab. 2 and show qualitative results in Fig. 6. VD [ 38] creates
plausible results without backgrounds. DFM [ 41] creates
5048
0° 340°HF [18]
 VD [38]
Ours(no proj)
Ours(no cfa)
 Ours
Figure 5. Multi-view consistency of unconditional image generation. HoloFusion (HF) [ 18] has view-dependent floating artifacts (the
base in first row). ViewsetDiffusion (VD) [ 38] has blurrier renderings (second row). Without the projection layer, our method has no precise
control over viewpoints (third row). Without cross-frame-attention, our method suffers from identity changes of the object (fourth row). Our
full method produces detailed images that are 3D-consistent (fifth row).
Table 2. Quantitative comparison of single-image reconstruction. Given a single image as input, we measure the quality of novel views
through average PSNR, SSIM, and LPIPS [ 54] per category. We mask away the generated backgrounds to ensure comparability across all
methods. We improve over VD [38] while being on-par with DFM [41].
MethodTeddybear Hydrant Donut Apple
PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓
VD [38] 19.68 0.70 0.30 22.36 0.80 0.19 18.27 0.68 0.14 19.54 0.64 0.31
DFM [41] 21.81 0.82 0.16 22.67 0.83 0.12 23.91 0.86 0.10 25.79 0.91 0.07
Ours 21.98 0.84 0.13 22.49 0.85 0.11 21.50 0.85 0.18 25.94 0.91 0.11
consistent results with backgrounds at a lower image reso-
lution (128 ×128). Our method produces higher resolution
images with similar reconstruction results and backgrounds.
4.3. Ablations
The key ingredients of our method are the cross-frame-
attention and projection layers that we add to the U-Net
(Sec. 3.2). We highlight their importance in Tab. 3 and Fig. 5.
How important are the projection layers? They are nec-
essary to allow precise control over the image viewpoints
(e.g., Fig. 5 row 3 does not follow the specified rotation).
Our goal is to generate a consistent set of images from anyviewpoint directly with our model (Sec. 3.3). Being able
to control the pose of the object is therefore an essential
part of our contribution. The projection layers build up a
3D representation of the object that is explicitly rendered
into 3D-consistent features through volume rendering. This
allows us to achieve viewpoint consistency, as also demon-
strated through single-image reconstruction (Tab. 3).
How important are cross-frame-attention layers? They
are necessary to create images of the same object. Without
them, the teddybear in Fig. 5 (row 4) has the same general
color scheme and follows the specified poses. However, dif-
ferences in shape and texture lead to an inconsistent set of
5049
Input VD [38] DFM [41] Ours Real Image
Figure 6. Single-image reconstruction of our method and base-
lines. Given one image/pose as input, our method produces plausi-
ble novel views that are consistent with the real shape and texture.
We can also produce detailed backgrounds that match the input.sample 1
 sample 2
[C] wearing a green hat [C] sits on a green blanketsample 1
 sample 2
rusty[C] in the grass yellow and white [C]
Figure 7. Diversity of generated results. We condition our method
on text input, which allows to create objects in a desired style.
We show samples for hand-crafted text descriptions that combine
attributes (e.g., color, shape, background) in a novel way. Each
row shows a different generation proposal from our method and we
denote the object category ( Teddybear ,Hydrant ) as[C]. This
showcases the diversity of generated results, i.e., multiple different
objects are generated for the same description.
images. We reason that the cross-frame-attention layers are
essential for defining a consistent object identity .
Does the 2D prior help? We utilize a 2D prior in form
of the pretrained text-to-image model that we fine-tune in a
3D-consistent fashion (Sec. 3.1). This enables our method to
produce sharp and detailed images of objects from different
viewpoints. Also, we train our method on captioned imagesTable 3. Quantitative comparison of our method and ablations.
We report average PSNR, SSIM, LPIPS [ 54], FID [ 12], KID [ 2]
overTeddybear andHydrant categories. We compare against
dropping the projection layer (“no proj”) and cross-frame-attention
(“no cfa”) from the U-Net (see Sec. 3.2). While still producing high-
quality images with similar FID/KID, this shows that our proposed
layers are necessary to obtain 3D-consistent images.
Method PSNR ↑SSIM↑LPIPS↓FID↓KID↓
Ours (no proj) 16.55 0.71 0.29 47.95 0.034
Ours (no cfa) 18.15 0.76 0.25 47.93 0.034
Ours 22.24 0.84 0.11 47.92 0.034
to retain the controllable generation through text descriptions
(Sec. 3.4). We show the diversity and controllability of our
generations in Fig. 7 with hand-crafted text prompts. This
highlights that, after finetuning, our model is still faithful
to text input, and can combine attributes in a novel way, i.e.,
our model learns to extrapolate from the training set.
4.4. Limitations
Our method generates 3D-consistent, high-quality images of
diverse objects according to text descriptions or input images.
Nevertheless, there are several limitations. First, our method
sometimes produces images with slight inconsistency, as
shown in the supplement. Since the model is fine-tuned on a
real-world dataset consisting of view-dependent effects (e.g.,
exposure changes), our framework learns to generate such
variations across different viewpoints. A potential solution is
to add lighting condition through a ControlNet [ 53]. Second,
our work focuses on objects, but similarly scene-scale
generation on large datasets [6, 52] can be explored.
5. Conclusion
We presented ViewDiff, a method that, given text or image
input, generates 3D-consistent images of real-world objects
placed in authentic surroundings. Our method leverages the
expressivity of large 2D text-to-image models and fine-tunes
this 2D prior on real-world 3D datasets to produce diverse
multi-view images in a joint denoising process. The core in-
sight of our work are two novel layers, namely cross-frame-
attention and the projection layer (Sec. 3.2). Our autoregres-
sive generation scheme (Sec. 3.3) allows to directly render
high-quality novel views of a generated 3D object.
6. Acknowledgements
This work was done during Lukas’ internship at Meta Re-
ality Labs Zurich as well as at TU Munich, funded by
a Meta sponsored research agreement. Matthias Nießner
was also supported by the ERC Starting Grant Scan2CAD
(804724). We also thank Angela Dai for the video voice-
over.
5050
References
[1]Titas Anciukevi ˇcius, Zexiang Xu, Matthew Fisher, Paul Hen-
derson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Ren-
derdiffusion: Image diffusion for 3D reconstruction, inpaint-
ing and generation. In CVPR , 2023. 2
[2]Mikołaj Bi ´nkowski, Danica J Sutherland, Michael Arbel, and
Arthur Gretton. Demystifying MMD GANs. In ICLR , 2018.
5, 8
[3]Aljaˇz Bo ˇziˇc, Pablo Palafox, Justus Thies, Angela Dai, and
Matthias Nießner. TransformerFusion: Monocular RGB scene
reconstruction using transformers. In NeurIPS , 2021. 4
[4]Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexan-
der W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala,
Shalini De Mello, Tero Karras, and Gordon Wetzstein. Gen-
erative novel view synthesis with 3D-aware diffusion models.
arXiv:2304.02602, 2023. 2
[5]Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-
tasia3D: Disentangling geometry and appearance for high-
quality text-to-3D content creation. In ICCV , 2023. 1, 2
[6] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber,
Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-
annotated 3D reconstructions of indoor scenes. In CVPR ,
2017. 8
[7]Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani,
Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe
of annotated 3D objects. In CVPR , 2023. 2
[8]Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat GANs on image synthesis. In NeurIPS , 2021. 2
[9]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16 ×16 words: Transformers for image recognition at
scale. In ICLR , 2021. 4
[10] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander
Holynski, and Angjoo Kanazawa. Instruct-NeRF2NeRF: Edit-
ing 3D scenes with instructions. In ICCV , 2023. 2
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR , 2016.
4
[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-
hard Nessler, and Sepp Hochreiter. GANs trained by a two
time-scale update rule converge to a local Nash equilibrium.
InNeurIPS , 2017. 5, 8
[13] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS Workshops , 2021. 2, 5
[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 2, 3
[15] Lukas H ¨ollein, Ang Cao, Andrew Owens, Justin Johnson,
and Matthias Nießner. Text2Room: Extracting textured 3D
meshes from 2D text-to-image models. In ICCV , 2023. 2
[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA:
Low-rank adaptation of large language models. In ICLR ,
2022. 4[17] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hos-
seini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand
Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot
learning with retrieval augmented language models. JMLR ,
24(251):1–43, 2023. 4
[18] Animesh Karnewar, Niloy J. Mitra, Andrea Vedaldi, and
David Novotny. HoloFusion: Towards photo-realistic 3D
generative modeling. In ICCV , 2023. 2, 5, 6, 7
[19] Animesh Karnewar, Andrea Vedaldi, David Novotny, and
Niloy J. Mitra. HoloDiffusion: Training a 3D diffusion model
using 2D images. In CVPR , 2023. 2
[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-
2: Bootstrapping language-image pre-training with frozen im-
age encoders and large language models. arXiv:2301.12597,
2023. 5
[21] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-
Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-
3D content creation. In CVPR , 2023. 2
[22] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexi-
ang Xu, Hao Su, et al. One-2-3-45: Any single image
to 3D mesh in 45 seconds without per-shape optimization.
arXiv:2306.16928, 2023. 2
[23] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,
Sergey Zakharov, and Carl V ondrick. Zero-1-to-3: Zero-shot
one image to 3d object. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
9298–9309, 2023. 2, 4
[24] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie
Liu, Taku Komura, and Wenping Wang. Syncdreamer: Gener-
ating multiview-consistent images from a single-view image.
InThe Twelfth International Conference on Learning Repre-
sentations , 2024. 2, 3
[25] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021. 2,
4
[26] Norman M ¨uller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota
Bul`o, Peter Kontschieder, and Matthias Nießner. DiffRF:
Rendering-guided 3D radiance field diffusion. In CVPR ,
2023. 2
[27] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.
DreamFusion: Text-to-3D using 2D diffusion. In ICLR , 2023.
1, 2
[28] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Ali-
aksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov,
Peter Wonka, Sergey Tulyakov, et al. Magic123: One image
to high-quality 3D object generation using both 2D and 3D
diffusion priors. arXiv:2306.17843, 2023. 2
[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with CLIP latents. arXiv:2204.06125, 2022. 1, 2,
3
[30] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan,
Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hed-
man. MERF: Memory-efficient radiance fields for real-time
5051
view synthesis in unbounded scenes. ACM Transactions on
Graphics (TOG) , 42(4):1–12, 2023. 4
[31] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,
Luca Sbordone, Patrick Labatut, and David Novotny. Com-
mon objects in 3D: Large-scale learning and evaluation of
real-life 3D category reconstruction. In ICCV , 2021. 2, 3, 5
[32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine
tuning text-to-image diffusion models for subject-driven gen-
eration. In CVPR , 2023. 1, 5
[33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay
Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,
Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes,
Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad
Norouzi. Photorealistic text-to-image diffusion models with
deep language understanding. In NeurIPS , 2022. 1, 2, 3, 5
[34] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon
Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung
Lee, and Seungryong Kim. Let 2D diffusion model
know 3D-consistency for robust text-to-3D generation.
arXiv:2303.07937, 2023. 2
[35] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,
and Xiao Yang. MVDream: Multi-view diffusion for 3D
generation. arXiv:2308.16512, 2023. 2
[36] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In ICML , 2015. 3
[37] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and
Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruc-
tion from monocular video. In CVPR , 2021. 4
[38] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea
Vedaldi. ViewSet diffusion: (0-)image-conditioned 3D gen-
erative models from 2D data. In ICCV , 2023. 2, 5, 6, 7,
8
[39] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,
Lizhuang Ma, and Dong Chen. Make-it-3D: High-fidelity 3D
creation from a single image with diffusion prior. In ICCV ,
2023. 2
[40] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and
Yasutaka Furukawa. MVDiffusion: Enabling holistic multi-
view image generation with correspondence-aware diffusion.
InNeurIPS , 2023. 2
[41] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon
Rezchikov, Joshua B. Tenenbaum, Fr ´edo Durand, William T.
Freeman, and Vincent Sitzmann. Diffusion with forward
models: Solving stochastic inverse problems without direct
supervision. In NeurIPS , 2023. 2, 5, 6, 7, 8
[42] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,
Michael Niemeyer, and Federico Tombari. TextMesh: Gener-
ation of realistic 3D meshes from text prompts. In 3DV, 2024.
2
[43] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-
Bin Huang, and Johannes Kopf. Consistent view synthesis
with pose-guided diffusion models. In CVPR , 2023. 2
[44] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich. Score Jacobian chaining: Lifting
pretrained 2D diffusion models for 3D generation. In CVPR ,
2023. 2[45] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. NeuS: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
InNeurIPS , 2021. 2
[46] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini-
vasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-
Brualla, Noah Snavely, and Thomas Funkhouser. IBRNet:
Learning multi-view image-based rendering. In CVPR , 2021.
4
[47] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity and
diverse text-to-3D generation with variational score distilla-
tion. arXiv:2305.16213, 2023. 1, 2
[48] Daniel Watson, William Chan, Ricardo Martin Brualla,
Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi.
Novel view synthesis with diffusion models. In ICLR , 2023.
2
[49] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,
Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and
Mike Zheng Shou. Tune-a-video: One-shot tuning of im-
age diffusion models for text-to-video generation. In ICCV ,
2023. 4
[50] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong.
3D-aware image generation using 2D diffusion models. In
ICCV , 2023. 2
[51] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change Loy.
Rerender a video: Zero-shot text-guided video-to-video trans-
lation. In SIGGRAPH Asia , 2023. 4
[52] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and
Angela Dai. ScanNet++: A high-fidelity dataset of 3D indoor
scenes. In ICCV , 2023. 8
[53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV , 2023. 1, 2, 8
[54] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 5, 7, 8
[55] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and
Jiwen Lu. UniPC: A unified predictor-corrector framework
for fast sampling of diffusion models. In NeurIPS , 2023. 5
[56] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,
and Noah Snavely. Stereo magnification: Learning view syn-
thesis using multiplane images. ACM Transactions on Graph-
ics, 37(4):65:1–12, 2018. 2
[57] Zhizhuo Zhou and Shubham Tulsiani. SparseFusion: Dis-
tilling view-conditioned diffusion for 3D reconstruction. In
CVPR , 2023. 2
[58] Joseph Zhu and Peiye Zhuang. HiFA: High-fidelity text-to-3D
with advanced diffusion guidance. arXiv:2305.18766, 2023.
2
5052
