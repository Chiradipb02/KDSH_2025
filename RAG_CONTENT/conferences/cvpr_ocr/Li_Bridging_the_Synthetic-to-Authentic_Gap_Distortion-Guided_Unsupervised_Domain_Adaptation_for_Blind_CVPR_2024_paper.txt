Bridging the Synthetic-to-Authentic Gap: Distortion-Guided Unsupervised
Domain Adaptation for Blind Image Quality Assessment
Aobo Li Jinjian Wu*Yongxu Liu Leida Li
Xidian University
abli@stu.xidian.edu.cn, jinjian.wu@mail.xidian.edu.cn, {yongxu.liu, ldli }@xidian.edu.cn
Abstract
The annotation of blind image quality assessment
(BIQA) is labor-intensive and time-consuming, especially
for authentic images. Training on synthetic data is expected
to be beneficial, but synthetically trained models often suf-
fer from poor generalization in real domains due to do-
main gaps. In this work, we make a key observation that
introducing more distortion types in the synthetic dataset
may not improve or even be harmful to generalizing au-
thentic image quality assessment. To solve this challenge,
we propose distortion-guided unsupervised domain adap-
tation for BIQA (DGQA), a novel framework that leverages
adaptive multi-domain selection via prior knowledge from
distortion to match the data distribution between the source
domains and the target domain, thereby reducing negative
transfer from the outlier source domains. Extensive exper-
iments on two cross-domain settings (synthetic distortion
to authentic distortion and synthetic distortion to algorith-
mic distortion) have demonstrated the effectiveness of our
proposed DGQA. Besides, DGQA is orthogonal to exist-
ing model-based BIQA methods, and can be used in combi-
nation with such models to improve performance with less
training data.
1. Introduction
Nowadays, countless images are being generated, transmit-
ted, and processed by image enhancement, compression,
and other algorithms all the time. During these processes,
distortions are inevitably introduced that degrade the im-
age quality and may even affect the visual experience of
receivers or the use of downstream algorithms. In most sce-
narios, the reference information required by full-reference
(FR) or reduced-reference (RR) image quality assessment
(IQA) methods [29, 32] is not available. Therefore, it is
crucial to have an accurate and efficient blind image qual-
ity assessment (BIQA) method [22] to monitor the image
*Corresponding author.
Figure 1. The performance on the authentically distorted dataset
LIVEC [7] when the baseline is trained on LIVE [26], KADID-
10k [18], and the sub-set of KADID-10k selected by the proposed
DGQA. Ref. and Dist. denote the number of the reference images
and the number of the distortion in this dataset, respectively.
quality during these processes [14].
With the rapid development of deep learning, many
learning-based BIQA methods [12, 13] have been proposed,
which significantly improve the performance of traditional
BIQA methods. However, since the annotation for the per-
ceptual quality of images (especially authentic distortion
images) is time-consuming and laborious, the amount of la-
beled data in existing BIQA datasets is relatively limited. It
greatly limits the performance of deep learning-based BIQA
methods that rely on a large number of labeled samples.
The acquisition of data and annotations can be much
cheaper for synthetically distorted images generated by
adding artificial distortions than for authentically distorted
images, due to the availability of the corresponding refer-
ence images and the possibility of adjusting the parame-
ters to generate different levels of distortion [31]. If mod-
els trained on images with synthetic distortion could be ef-
fectively generalized to images with authentic distortion, it
would greatly alleviate the pressure on annotation. How-
ever, since authentic distortions are complex and variable, it
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28422
Figure 2. (a) are some representative images in LIVEC, (b) are some typical images in the part domains of KADID-10k whose styles are
somewhat similar to that of LIVEC, and (c) are some typical images in the part domains of KADID-10k whose styles are quite different
from LIVEC.
is difficult to simulate them directly by synthetic distortions.
As a result, there is a large gap between existing synthetic
distortion datasets and authentic distortion datasets which
makes existing methods perform poorly in syn-to-real gen-
eralization.
New synthetic distortion datasets introduce an increas-
ing number of distortion types in an attempt to better cope
with authentic distortion [18]. However, we found that this
did not actually lead to the expected performance improve-
ment. As shown in Fig. 1, compared to LIVE [26], the syn-
thetic distortion dataset KADID-10k [18] with more distor-
tion types and more reference images surprisingly exhibits
significantly worse generalization performance on the au-
thentic distortion dataset LIVEC [7].
Generalizing a model from a synthetically distorted
dataset with multiple artificial distortions to an unlabeled
authentically distorted dataset can be modeled as an un-
supervised multi-source domain adaptation (UMDA) prob-
lem. As shown in Fig. 2, we can observe that the styles
of some domains in the synthetically distorted dataset are
often somewhat similar to that of the synthetically distorted
dataset, while some are quite different. Some studies on do-
main adaptation (DA) [2, 34] have shown that outlier source
domains that differ too much from the target domain will
may not result in performance gains or even negative trans-
fers. Therefore, a natural idea is to try to sieve out the outlier
domains from the multi-source domains, while retaining the
source domains that are similar to the target domain.
In this work, we analyze how to measure the distance
of source and target domains for BIQA under unsupervised
scenarios and propose a distortion-guided unsupervised do-
main adaptation framework for BIQA (DGQA). By per-
forming similar domain selection according to the target
domain, source domains whose distributions differ signifi-cantly from the target domain distribution are filtered out.
The filtered similar source domains are used for training
the model, which reduces the negative transfer from out-
lier source domains to the target domain and improves the
generalization ability of the model. As shown in Fig. 1,
our framework DGQA improves by 13.1%in SRCC and
10.1%in PLCC when tested on LIVEC using only 24% of
the training data. Our contribution is summarized as fol-
lows:
•We make a key observation that the use of a larger syn-
thetic distortion dataset may not improve or even be harm-
ful to the performance of model generalization to authen-
tic distortions, and analyze the reasons for this in the con-
text of some research about domain adaptation.
•We analyze how to measure the distance of source and
target domains for BIQA under unsupervised scenar-
ios and accordingly propose a distortion-guided unsuper-
vised domain adaptation framework that reduces the neg-
ative transfer of the outlier source domains to the target
domain and improves the generalization capacity of the
model.
•Our approach significantly improves the model perfor-
mance with a much smaller amount of training data in the
settings of synthetic distortion to authentic distortion and
synthetic distortion to algorithmic distortion. Besides,
since our method is a sample-based approach, it is fully
compatible with existing model-based approaches.
2. Related Work
In this section, we give a brief overview of recent progress
in BIQA. We then review unsupervised domain adaptation
(UDA) and its applications for BIQA.
28423
2.1. Blind Image Quality Assessment
Early traditional methods typically use handcrafted features
based on natural scene statistics [23, 24, 30]. They extract
these features for images and map them to quality scores to
achieve BIQA. Due to the complexity of the BIQA task and
the limited ability to hand-crafted feature representations,
these methods generally fail to perform satisfactorily.
In recent times, many BIQA approaches [12, 13, 19]
based on deep learning have been proposed. These ap-
proaches have made remarkable progress beyond traditional
methods, as the deep neural network possesses exceptional
ability for representation learning. Zhang et al. [33] design
DB-CNN consisting of two pre-trained convolutional neural
networks which extract features about synthetic and authen-
tic distortions, respectively. MetaIQA [36] utilizes meta-
learning to learn prior knowledge that is shared amongst
diverse types of distortions. In [27], Su et al. propose a self-
adaptive hypernetwork for BIQA. The model first extracts
semantic features from images, subsequently employs a hy-
pernetwork to dynamically learn perception rules, and fi-
nally predicts the quality of the images. Zhao et al. [35] im-
prove the existing degradation process and propose a self-
supervised learning framework for BIQA. Qin et al. [25]
present a new transformer-based BIQA method that effi-
ciently learns quality-aware features with minimal data and
introduces a human-inspired attention panel mechanism.
However, these methods are only applicable in scenarios
where the training and testing data are from similar distri-
butions and exhibit poor performance when trained on the
synthetic distortion dataset and tested on the authentic dis-
tortion dataset since there is a large gap between synthetic
distortions and authentic distortions.
2.2. Unsupervised Domain Adaptation
Unsupervised domain adaptation (UDA) aims to align the
distribution of the labeled source domain and the unlabeled
target domain, thereby improving the generalization perfor-
mance from the source domain to the target domain.
The majority of UDA approaches are feature-based,
which can be classified into two categories: discrepancy-
based methods and adversary-based methods. The former
primarily employ distance measures such as Wasserstein
metric, Kullback-Leibler (KL) divergence [16], maximum
mean discrepancy (MMD) [8], contrastive domain discrep-
ancy (CDD) [11], and correlation alignment (CORAL)
[28] to assess the similarity between domains. The latter
aims to learn transferable and domain-invariant features us-
ing the domain discriminator to encourage domain confu-
sion through an adversarial objective, like DANN [1], and
CDAN [20].
Currently, there are a few works that introduce UDA into
BIQA. UCDA [4] divides the target domain into confident
and non-confident subdomains and subsequently aligns thesource domain and target subdomains from easier to more
challenging. Chen et al. [3] apply the center loss to acquire
domain discriminative features for different domains. They
subsequently aligned two domains using ranked paired fea-
tures to facilitate knowledge transfer from natural IQA to
screen content IQA. Lu et al. [21] utilize the feature
style space for BIQA as the alignment space and propose
Style Mixup to improve the consistency between the fea-
ture styles and their quality scores.
However, BIQA is oriented to images with very large dif-
ferences in style, and imposing conventional feature-based
UDA methods can lead to severe negative transfer due to
the huge gaps between domains. For this reason, we start
from the data level and filter the source domains that are
more similar to the target domain among the set of source
domains to align the source and target domains.
3. Proposed Method
In this section, we first introduce the preliminaries of the
problem formulation and the factors affecting the general-
ization error bound of the target domain. Then we describe
how to implement distortion-guided source domain selec-
tion. Finally, we present the training strategy of our pro-
posed framework. The framework of the proposed DGQA
is shown in Fig. 3.
3.1. Preliminary
The problem of generalizing a synthetically distorted IQA
dataset containing ktypes of artificial distortions to a
specific real scenario can be viewed as an unsupervised
multi-source domain adaptation (UMDA) problem. For
this problem, we have klabeled source domains {DSi=
{xj
Si, yj
Si}n
j=1}k
i=1generated based on different distortion
types and an unlabeled target domain DT={xj
T}n
j=1,
where xj
Siandyj
Sidenote the images and the correspond-
ing quality scores of the source domains, and xj
Tdenotes
the images of the target domain, respectively. We expect to
achieve good performance on the target domain by learning
on these source domains that have different degrees of gap
with the target domain.
The generalization error bound of the model over the tar-
get domain can be used as a measure of the performance of
the task in the target domain, and [34] gives the bound for
multi-source domain adaptation:
Theorem 1. LetHbe a set of real-valued functions from
Xto[0,1]withPdim(H) =d. If{ˆDSi}k
i=1are the empir-
ical distributions generated with m i.i.d. samples from each
domain, and ˆDTis the empirical distribution on the target
domain generated from mksamples without labels, then,
∀α∈Rk
+,P
i∈[k]αi= 1, and for 0< δ < 1, with proba-
28424
Figure 3. The framework of the proposed DGQA. By performing similar domain selection, the source domains with a large gap to the
target domain are sieved out. The sieved similar domains Dsimare used to train the model, which reduces the negative transfer from outlier
source domains to the target domain and improves the model’s performance on the target domain.
bility at least 1−δ, for all h∈H, we have:
ϵT(h)≤X
i∈[k]αi
ˆϵSi(h) +1
2dH(ˆDT;ˆDSi)
+λα
+O r
1
km(log1
δ+dlogkm
d)!
(1)
dH(ˆDT;ˆDSi) = 2 sup
h∈H|Pr
ˆDSi(h)−Pr
ˆDT(h)| (2)
where ϵT(h)is the generalization error of the target do-
main, ˆϵSi(h)is the empirical error in the source domain
Si,dH(ˆDT;ˆDSi)is the difference in the empirical distribu-
tions of the source domain Siand the target domain T,λα
is the risk of the optimal hypothesis on the mixture source
domainP
i∈[k]αiSiandT, andH:={I|h(x)−h′(x)|>t:
h, h′∈ H,0≤t≤1}is the set of threshold functions
induced from H.
According to Thm. 1, we can know that the factors af-
fecting the generalization error bound of the target domain
are source domain empirical error, distance between the dis-
tributions of the source domains and the target domain, opti-
mal joint generalization error, and sample size, respectively.
Since the empirical error can be minimized to an arbitrar-
ily small value by training, the optimal joint generaliza-
tion error does not depend on a specific hypothesis h[1].
Therefore, besides adding training samples, we can reduce
the generalization error of the model on the target domainby narrowing the distance between the distributions of the
source domains and the target domain, thus improving the
performance of the target domain [6].
3.2. Distortion-Guided Source Domain Selection
BIQA is oriented to images with very large differences
in style, and imposing conventional feature-based domain
adaptive methods can lead to severe negative transfer due to
the huge gaps between domains. For this reason, we start
from the data level and filter the source domains that are
more similar to the target domain among the set of source
domains to reduce the negative transfer caused by outlier
source domains.
For BIQA, the data distribution is determined by the
content distribution Dcand distortion distribution Dd[17].
Since images with any content can be selected as the ref-
erence images to generate synthetic distortion images, it
is theoretically possible to achieve a content distribution
that is very similar to an authentic distortion image dataset.
Hence, we assume that the source domain has the same im-
age content as the target domain, i.e. Dc
Si=Dc
T. At this
point, the distance between the distributions Dof the source
domains and the target domain can be translated into the dis-
tance between the distortion distribution Ddof the source
domains and the target domain. In addition, all source do-
mains are sampled uniformly, i.e. αi=1
k. Then the dis-
tance between the distributions of the source domains and
28425
the target domain can be expressed as
dH(ˆDT;ˆDSi) =2
kX
i∈[k]sup
h∈H|Pr
ˆDd
Si(h)−Pr
ˆDd
T(h)|(3)
This distance can be interpreted as the discriminator’s dis-
criminability of source and target domain distortions.
Since our goal is only to filter the source domains that
are relatively more similar to the target domain, for this rea-
son, we only need to compare the relative discriminability
of each source domain to the target domain. To this end, we
integrate the domain discriminators for multiple source do-
mains into a single multi-source domain classifier FD. For
each sample xT
jin the target domain, input the multi-source
domain classifier FDto predict the probability pthat the
sample belongs to each source domain,
[p1
j, p2
j, ..., pk
j] =FD(xT
j) (4)
The relative similarity between the target domain DTand
the source domain DSiis defined as the average probability
that all the images in the target domain belong to this source
domain,
sim(DT,DSi) =1
NNX
j=1pi
j (5)
where Nis the simple size in the target domain. Finally, the
source domains with relative similarity above the threshold
τ(τis set as1
kin this work) are taken as the similar domains
of the target domain,
Dsim={DSj|sim(DT,DSj)> τ, j ∈[k]} (6)
Through the distortion-guided source domain selection,
source domains with significantly large differences in distri-
bution from the target domain are filtered out, thus reducing
the negative transfer of outlier source domains to the tar-
get domain and improving the generalization ability of the
model.
3.3. Training
Domain Classify. We choose the 25 classes of distortion
types in KADID-10k as the domain codes. The tailored
ResNet-50 whose fully connected (FC) layer is replaced
with an FC layer with 25 nodes and a softmax layer is the
multi-source domain classifier FD. To get better classifica-
tion ability, KADIS-700k, which has a larger data size and
the same distortion types as KADID-10k, is used to train
FD. Cross entropy is the loss function used for domain clas-
sification,
LD=MX
j=1(−yD
jlog(FD(xj))−(1−yD
j) log(1 −FD(xj)))
(7)where yD
jis the domain encoding of xj, and Mis the sam-
ple size of train data.
IQA Regression. Similarly, the tailored ResNet-50 whose
FC layer is replaced with an FC layer with one node is the
IQA regressor FR. As shown in Fig. 3, the selected similar
source domains are used as training data for training the
model. We use the L1 norm as the loss function for image
quality score prediction,
LR=X
(xj,yj)∈Dsim||FR(xj)−yj||1 (8)
4. Experiments
In this section, we first describe the experimental setup in
detail. Then the performance comparison is made with
the several state-of-the-art methods and UDA methods on
the synthetic-to-authentic and synthetic-to-algorithmic sce-
nario. A series of subsequent experiments validate the gen-
eralisability of our method, the effectiveness of the pro-
posed core components, and the compatibility with model-
based methods. Finally, we explore the upper bound of the
performance of the method based on source domain selec-
tion on KADID-10k [18] for generalizing to authentic dis-
tortion datasets and experimentally demonstrate the feasi-
bility of generalizing to authentic distortions by generaliz-
ing over synthetic distortions.
4.1. Experimental Setup
We conduct experiments on six IQA datasets, containing
the synthetic distortion datasets KADID-10k [18], KADIS-
700k [18], the authentic distortion datasets LIVEC [7], BID
[5], KonIQ-10k [9], and the dataset PIPAL [10] with both
synthetic and algorithmic distortions. The specific informa-
tion of all datasets is summarised in Table 1. Two typical
criteria, SRCC and PLCC are adopted for measuring pre-
diction monotonicity and prediction accuracy.
Specifically, KADIS-700k is randomly divided into 80%
training data and 20% testing data and used for the pre-
training of the domain classifier. KADID-10k is also par-
titioned into training and validation sets in the same propor-
tions for the training of the IQA regressor, and the remain-
ing four datasets are used as test sets respectively. All split-
ting is performed by reference images to ensure no overlap-
ping content. This random train-val splitting on KADID-
10k for all experiments is repeated five times and the me-
dian SRCC and PLCC are reported.
During the training process, we randomly sample a patch
of size 224×224×3from each training image, and random
horizontal flipping is used for data augmentation. Our mini-
batch size is set at 32, while the learning rate is at 2×10−5.
We use the Adam [15] optimizer coupled with a weight de-
cay of 5×10−4to optimize the model for 15 epochs. In the
testing phase, five patches from each image are randomly
28426
Table 1. Summary of the datasets for training or evaluation.
Dataset KADID-10k KADIS-700k LIVEC BID KonIQ-10k PIPAL
Reference images 81 140000 N/A N/A N/A 250
Distorted images 10125 700000 1162 586 10073 29000
Distortion types 25 25 N/A N/A N/A 40
Annotation DMOS N/A MOS MOS MOS MOS
Scenario Synthetic Synthetic Authentic Authentic Authentic Synthetic+Algorithmic
Table 2. Similar source domain statistics for all target domains, where N.o.S. denotes the number of similar source domains for this target
domain, and N.o.T. denotes the number of occurrences of this source domain.
#1 #2 #3 #4 #7 #9 #10 #11 #12 #16 #17 #18 #19 #21 #22 #23 #25 N.o.S
LIVEC ✓ ✓ ✓ ✓ ✓ ✓ 6
Koniq-10k ✓ ✓ ✓ ✓ ✓ ✓ 6
BID ✓ ✓ ✓ ✓ ✓ ✓ 6
PIPAL-1 ✓ ✓ ✓ ✓ ✓ ✓ ✓ 7
PIPAL-2 ✓ ✓ ✓ ✓ ✓ ✓ 6
PIPAL-3 ✓ ✓ ✓ ✓ ✓ ✓ ✓ 7
PIPAL-4 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 9
PIPAL-5 ✓ ✓ ✓ ✓ ✓ 5
PIPAL-6 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 14
N.o.T 9 5 9 1 1 6 3 1 1 3 7 6 2 2 1 2 7
sampled and the mean of their prediction results is the fi-
nal output. This method is implemented by Pytorch, and all
experiments are carried out on NVIDIA TITAN Xp GPUs.
4.2. Analysis of Similarity Source Domain
For the target domains (LIVEC, KonIQ-10k, BID, and the
algorithmically distorted part of PIPAL) that are used as test
sets in the experiments, we use distortion-guided domain
selection (DGDS) to filter their similar source domains in
the training set KADID-10k. The results are shown in Table
2, where only source domains with at least 1 occurrence are
listed. PIPAL-1, 2, 3, 4, 5, 6 denote traditional SR, PSNR-
originated SR, SR with kernel mismatch, GAN-based SR,
denoising, SR and denoising joint in PIPAL, respectively.
From Table 2, we can see that for most target domains,
similar source domains account for less than 1/3 of the total
source domains. And based on subsequent experimental re-
sults, we will find that the remaining source domains do not
lead to performance improvement, and may even seriously
affect the performance of the model in most cases.
In addition, #1 (Gaussian blur), #3 (Motion blur), #9
(JPEG2000), #17 (Darken), #18 (Mean shift), and #25
(Contrast change) can be found in almost all the similar
source domains of the target domains. It might provide
some reference for the selection of source domains for un-
seen target domains.
These filtered similar source domains will be used in
subsequent experiments for the training of the IQA regres-
sor (denoted DGQA). The IQA regressor trained on allsource domains (i.e., the complete KADID-10k) serves as
the baseline. Besides, the compared methods are trained on
all source domains by default, when not specifically noted.
4.3. Performance Evaluation
1) Performance on the synthetic-to-authentic setting. On
this setting, the proposed DGQA is trained on KADID-10k
and tested on LIVEC, KonIQ-10k, and BID. The perfor-
mance of DGQA is compared against ten BIQA approaches,
including three traditional methods (BRISQUE [23], NIQE
[24] and PIQE [30]), three deep learning-based approaches
(RankIQA [19], MetaIQA [36], DB-CNN [33] and Hy-
perIQA [27]) and four UDA-based methods (DANN [1],
UCDA [4], RankDA [3] and StyleAM [21]). The compar-
isons are summarized in Table 3 and the bolded results im-
ply top performance. All comparison results are from the
publicly available paper [21].
As can be seen, the proposed DGQA uses less than
1/3 of the training data and achieves SOTA performance
when generalized to all authentic distortion datasets. Only
StyleAM is competitive on the ”KADID-10k →KonIQ-10k”
scenario. In all other scenarios, our method achieves sig-
nificant improvements (SRCC increases more than 11.14%
when tested on LIVEC, 13.31% when tested on BID) and
the average SRCC and PLCC increase more than 7.51% and
10.95%, respectively. This demonstrates that our DGQA
framework, which significantly reduces the negative trans-
fer from the outlier domains by reduces the distance be-
tween the target domain and the source domains, and thus
28427
Table 3. Performance comparison on the synthetic-to-authentic setting (KADID-10k →LIVEC, KonIQ-10k, and BID). The average results
are in the last column.
Methods LIVEC KonIQ-10k BID Average
SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC
BRISQUE 0.2433 0.2512 0.1077 0.0991 0.1745 0.1750 0.1752 0.1751
NIQE 0.3044 0.3619 0.4469 0.4600 0.3553 0.3812 0.3689 0.4010
PIQE 0.2622 0.3617 0.0843 0.1995 0.2693 0.3506 0.2053 0.3039
RankIQA 0.4906 0.4950 0.6030 0.5511 0.5101 0.3671 0.5346 0.4711
MetaIQA 0.4639 0.4638 0.5006 0.5035 0.3005 0.4280 0.4217 0.4651
DBCNN 0.2663 0.2897 0.4126 0.4209 0.3179 0.2115 0.3323 0.3074
HyperIQA 0.4903 0.4872 0.5447 0.5562 0.3794 0.2820 0.4715 0.4418
DANN 0.4990 0.4835 0.6382 0.6360 0.5861 0.5102 0.5744 0.5432
UCDA 0.3815 0.3584 0.4958 0.5010 0.3480 0.3907 0.4084 0.4167
RankDA 0.4512 0.4548 0.6383 0.6227 0.5350 0.5820 0.5415 0.5532
StyleAM 0.5844 0.5606 0.7002 0.6733 0.6365 0.5669 0.6404 0.6003
DGQA 0.6958 0.6902 0.6810 0.6866 0.7696 0.7526 0.7155 0.7098
effectively improves the model’s performance in generaliz-
ing from synthetic distortion to authentic distortion.
2) Performance on the synthetic-to-algorithmic set-
ting. To further explore the universality of our pro-
posed framework, we conduct experiments on the ”KADID-
10k→algorithmic distortions on PIPAL” scenario. The ex-
perimental results are shown in Table 4. Our proposed
framework improves the performance of baseline on all al-
gorithmic distortion types in PIPAL except denoising, and
SRCC improves more than 5%on four algorithmic distor-
tion types including traditional SR, PSNR-originated SR,
SR, and denoising joint and even on the very challenging
GAN-based SR. Although our performance on denoising is
slightly lower than the baseline, it should be noted that it
only has used 1/5 labeled data compared to the baseline.
The above experimental results show that our method is
not only effective in generalizing synthetic distortions to
authentic distortions but also helps to improve the perfor-
mance of generalization to other distortions.
4.4. Ablation Study
To explore the contribution of the key component and the
generalisability to different models of our DGQA, a series
of ablation experiments are conducted in this subsection.
The baseline (ResNet-50), HyperIQA and StyleAM trained
on complete KADID-10k are each used as a benchmark to
test the performance gain of our approach. The experimen-
tal results are shown in Table 5, where the results of Hyper-
IQA are obtained by the source code released by the original
authors, and those of StyleAM are obtained by our repro-
duced code.
It can be observed that our method has significantly im-
proved the performance of the baseline by DGDS. The
SRCC and PLCC improve 13.12%,10.92% when tested on
LIVEC, and 5.60%,7.39% when tested on KonIQ-10k. Itshows the effectiveness of the distortion-guided domain se-
lection. Besides, similar improvements in performance can
be seen on HyperIQA and StyleAM. This verifies the gen-
eralisability to different models and the compatibility with
model-based approaches of our DGQA. Besides, the per-
formance of the combination of StyleAM and DGDS on
Koniq-10k shows the potential of combining our method
with feature-based UDA methods to further improve per-
formance.
4.5. Further Study
In this section, we further explore the upper bound of the
performance on generalization to authentic datasets based
on source domain selection on KADID-10k and compare it
with the performance on generalization from an authentic
dataset to other authentic datasets. The performance com-
parisons are shown in Table 6.
Specifically, we adopt a greedy source domain selection
(GDS) strategy to incrementally introduce the source do-
mains in KADID-10k. When tested on LIVEC, the set of
source domains consisting of #1 (Gaussian blur), #3 (Mo-
tion blur), #9 (JPEG2000), #18 (Mean shift), #20 (Non-
eccentricity patch), and #25 (Contrast change) performs
best. And when tested on KonIQ-10k, the source do-
mains consisting of #1 (Gaussian blur), #2 (Lens blur), #9
(JPEG2000), #17 (Darken), #20 (Non-eccentricity patch),
and #25 (Contrast change) perform best. We can observe
that the set of source domains selected by GDS is similar
to ours, which validates the effectiveness of our distortion-
directed source domain selection.
Furthermore, while there is still a large gap compared to
the performance of KonIQ-10k generalized to LIVEC, it is
worth noting that the amount of data for the set of source do-
mains selected via the greedy selection strategy is just less
than 1/6 of that of KonIQ-10k. Compared to LIVEC, which
28428
Table 4. Performance comparison on the synthetic-to-algorithmic setting (KADID-10k →algorithmic distortions on PIPAL). The average
results are in the last row.
Sub-typeBaseline DGQA
SRCC PLCC SRCC PLCC
Traditional SR 0.4756 0.4536 0.5419 +6.63% 0.5404 +8.68%
PSNR-originated SR 0.4758 0.4697 0.5810 +10.52% 0.5956 +12.59%
SR with kernel mismatch 0.1609 0.1121 0.1629 +0.20% 0.1353 +2.32%
GAN-based SR 0.4898 0.4840 0.5393 +4.95% 0.5279 +4.39%
Denoising 0.5705 0.5285 0.5588−1.17% 0.5193−0.92%
SR and Denoising Joint 0.3941 0.3836 0.4470 +5.29% 0.4390 +5.54%
Average 0.4278 0.4053 0.4718 +4.40% 0.4596 +5.43%
Table 5. Ablation studies on the key component and different models.
Methods DomainLIVEC KonIQ-10k
SRCC PLCC SRCC PLCC
BaselineAll 0.5646 0.5810 0.6250 0.6127
DGDS 0.6958 +13.12% 0.6902 +10.92% 0.6810 +5.60% 0.6866 +7.39%
HyperIQAAll 0.5633 0.5890 0.6236 0.5869
DGDS 0.6944 +13.11% 0.6974 +10.84% 0.6853 +6.17% 0.6628 +7.59%
StyleAMAll 0.5174 0.5219 0.7046 0.7060
DGDS 0.6687 +15.13% 0.6671 +14.52% 0.7088 +0.43% 0.7263 +2.03%
Table 6. Performance comparison on generalization to authentic
datasets from the source domain set selected on KADID-10k and
other authentic datasets.
TrainingLIVEC KonIQ-10k
SRCC PLCC SRCC PLCC
DGDS 0.6958 0.6902 0.6810 0.6866
GDS 0.7134 0.7254 0.7255 0.7275
LIVEC - - 0.7074 0.7539
KonIQ-10k 0.8001 0.8096 - -
is of the same magnitude, an even higher SRCC is achieved
for the set of source domains selected via GDS when gen-
eralized to KonIQ-10k. This demonstrates the feasibility
of generalizing to authentic distortions by generalizing over
synthetic distortions.
5. Conclusion
The annotation of BIQA is labor-intensive and time-
consuming, especially for authentic images. Training on
synthetic data is expected to be beneficial, but synthetically
trained models often suffer from poor generalization in au-
thentic distortion datasets due to domain gaps. We make
a key observation that the use of a larger synthetic distor-
tion dataset may not improve or even be harmful to the per-
formance of model generalization to authentic distortions,
and analyze the reasons for this in the context of some re-
search about domain adaptation. To solve this challenge,we analyze how to measure the distance of source and tar-
get domains for BIQA under unsupervised scenarios and
accordingly propose a distortion-guided unsupervised do-
main adaptation framework that reduces the negative trans-
fer of the outlier source domains to the target domain and
improves the generalization ability of the model. Our ap-
proach significantly improves the model performance with
a significantly smaller amount of training data in the set-
tings of synthetic distortion to authentic distortion and syn-
thetic distortion to algorithmic distortion. Besides, since
our method is a sample-based approach, it is fully compat-
ible with existing model-based approaches. Finally, we ex-
plore the upper bound of the performance of the method
based on source domain selection on KADID-10k for gen-
eralizing to authentic distortion datasets and experimentally
demonstrate the feasibility of generalizing to authentic dis-
tortions by generalizing over synthetic distortions.
6. Acknowledgment
This work was partially supported by the Fundamental
Research Funds for the Central Universities (QTZX23038),
Postdoctoral Fellowship Program of China Postdoctoral
Science Foundation (GZC20232034) and Postdoctoral
Research Grant of Shaanxi Province (2023BSHEDZZ165).
References
[1] Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois
Laviolette, and Mario Marchand. Domain-adversarial neural
28429
networks. arXiv preprint arXiv:1412.4446 , 2014. 3, 4, 6
[2] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and
Michael I Jordan. Partial transfer learning with selective ad-
versarial networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 2724–
2732, 2018. 2
[3] Baoliang Chen, Haoliang Li, Hongfei Fan, and Shiqi Wang.
No-reference screen content image quality assessment with
unsupervised domain adaptation. IEEE Transactions on Im-
age Processing , 30:5463–5476, 2021. 3, 6
[4] Pengfei Chen, Leida Li, Jinjian Wu, Weisheng Dong, and
Guangming Shi. Unsupervised curriculum domain adapta-
tion for no-reference video quality assessment. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 5178–5187, 2021. 3, 6
[5] Alexandre Ciancio, Eduardo AB da Silva, Amir Said, Ramin
Samadani, Pere Obrador, et al. No-reference blur assessment
of digital pictures based on multifeature classifiers. IEEE
Transactions on Image Processing , 20(1):64–75, 2010. 5
[6] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas,
and Ben Glocker. Domain generalization via model-agnostic
learning of semantic features. Advances in neural informa-
tion processing systems , 32, 2019. 4
[7] Deepti Ghadiyaram and Alan C Bovik. Massive online
crowdsourced study of subjective and objective picture qual-
ity.IEEE Transactions on Image Processing , 25(1):372–387,
2015. 1, 2, 5
[8] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard
Sch¨olkopf, and Alex Smola. A kernel method for the two-
sample-problem. Advances in neural information processing
systems , 19, 2006. 3
[9] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe.
Koniq-10k: An ecologically valid database for deep learning
of blind image quality assessment. IEEE Transactions on
Image Processing , 29:4041–4056, 2020. 5
[10] Haoyu Chen Xiaoxing Ye Jimmy Ren Chao Dong Jinjin Gu,
Haoming Cai. Pipal: a large-scale image quality assessment
dataset for perceptual image restoration. In European Con-
ference on Computer Vision (ECCV) 2020 , pages 633–651,
Cham, 2020. Springer International Publishing. 5
[11] Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Haupt-
mann. Contrastive adaptation network for unsupervised do-
main adaptation. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
4893–4902, 2019. 3
[12] Le Kang, Peng Ye, Yi Li, and David Doermann. Convolu-
tional neural networks for no-reference image quality assess-
ment. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 1733–1740, 2014. 1,
3
[13] Jongyoo Kim and Sanghoon Lee. Fully deep blind image
quality predictor. IEEE Journal of Selected Topics in Signal
Processing , 11(1):206–220, 2016. 1, 3
[14] Jongyoo Kim and Sanghoon Lee. Deep learning of human
visual sensitivity in image quality assessment framework. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 1676–1684, 2017. 1[15] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[16] Solomon Kullback and Richard A Leibler. On information
and sufficiency. The annals of mathematical statistics , 22(1):
79–86, 1951. 3
[17] Aobo Li, Jinjian Wu, Shiwei Tian, Leida Li, Weisheng Dong,
and Guangming Shi. Blind image quality assessment based
on progressive multi-task learning. Neurocomputing , 500:
307–318, 2022. 4
[18] Hanhe Lin, Vlad Hosu, and Dietmar Saupe. Kadid-10k: A
large-scale artificially distorted iqa database. In 2019 Tenth
International Conference on Quality of Multimedia Experi-
ence (QoMEX) , pages 1–3. IEEE, 2019. 1, 2, 5
[19] Xialei Liu, Joost van de Weijer, and Andrew D Bagdanov.
Rankiqa: Learning from rankings for no-reference image
quality assessment. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 1040–
1049, 2017. 3, 6
[20] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and
Michael I Jordan. Conditional adversarial domain adapta-
tion. Advances in neural information processing systems ,
31, 2018. 3
[21] Yiting Lu, Xin Li, Jianzhao Liu, and Zhibo Chen.
Styleam: Perception-oriented unsupervised domain adaption
for non-reference image quality assessment. arXiv preprint
arXiv:2207.14489 , 2022. 3, 6
[22] Jupo Ma, Jinjian Wu, Leida Li, Weisheng Dong, Xuemei
Xie, Guangming Shi, and Weisi Lin. Blind image quality
assessment with active inference. IEEE Transactions on Im-
age Processing , 30:3650–3663, 2021. 1
[23] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad
Bovik. No-reference image quality assessment in the spa-
tial domain. IEEE Transactions on Image Processing , 21
(12):4695–4708, 2012. 3, 6
[24] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Mak-
ing a “completely blind” image quality analyzer. IEEE Sig-
nal Processing Letters , 20(3):209–212, 2012. 3, 6
[25] Guanyi Qin, Runze Hu, Yutao Liu, Xiawu Zheng, Haotian
Liu, Xiu Li, and Yan Zhang. Data-efficient image quality
assessment with attention-panel decoder. In Proceedings of
the AAAI Conference on Artificial Intelligence , pages 2091–
2100, 2023. 3
[26] H. R. Sheikh, M. F. Sabir, and A. C. Bovik. A statistical
evaluation of recent full reference image quality assessment
algorithms. IEEE Transactions on Image Processing , 15(11):
3440–3451, 2006. 1, 2
[27] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge,
Jinqiu Sun, and Yanning Zhang. Blindly assess image quality
in the wild guided by a self-adaptive hyper network. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 3667–3676, 2020. 3, 6
[28] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation
alignment for unsupervised domain adaptation. Domain
adaptation in computer vision applications , pages 153–171,
2017. 3
[29] Wen Sun, Qingmin Liao, Jing-Hao Xue, and Fei Zhou. Sp-
sim: A superpixel-based similarity index for full-reference
28430
image quality assessment. IEEE Transactions on Image Pro-
cessing , 27(9):4232–4244, 2018. 1
[30] N Venkatanath, D Praneeth, Maruthi Chandrasekhar Bh,
Sumohana S Channappayya, and Swarup S Medasani. Blind
image quality evaluation using perception based features.
In2015 twenty first national conference on communications
(NCC) , pages 1–6. IEEE, 2015. 3, 6
[31] Zhihua Wang, Zhi-Ri Tang, Jianguo Zhang, and Yuming
Fang. Toward a blind image quality evaluator in the wild by
learning beyond human opinion scores. Pattern Recognition ,
137:109296, 2023. 1
[32] Jinjian Wu, Weisi Lin, Guangming Shi, and Anmin Liu.
Reduced-reference image quality assessment with visual in-
formation fidelity. IEEE Transactions on Multimedia , 15(7):
1700–1705, 2013. 1
[33] Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and Zhou
Wang. Blind image quality assessment using a deep bilinear
convolutional neural network. IEEE Transactions on Cir-
cuits and Systems for Video Technology , 30(1):36–47, 2018.
3, 6
[34] Han Zhao, Shanghang Zhang, Guanhang Wu, Jos ´e MF
Moura, Joao P Costeira, and Geoffrey J Gordon. Adversar-
ial multiple source domain adaptation. Advances in neural
information processing systems , 31, 2018. 2, 3
[35] Kai Zhao, Kun Yuan, Ming Sun, Mading Li, and Xing Wen.
Quality-aware pre-trained models for blind image quality
assessment. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22302–
22313, 2023. 3
[36] Hancheng Zhu, Leida Li, Jinjian Wu, Weisheng Dong, and
Guangming Shi. Metaiqa: Deep meta-learning for no-
reference image quality assessment. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14143–14152, 2020. 3, 6
28431
