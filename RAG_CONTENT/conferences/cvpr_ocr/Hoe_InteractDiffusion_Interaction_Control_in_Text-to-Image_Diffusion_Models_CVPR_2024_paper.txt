InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models
Jiun Tian Hoe1Xudong Jiang1Chee Seng Chan2
Yap-Peng Tan1Weipeng Hu1
1School of EEE, Nanyang Technological University, Singapore2CISiP, Universiti Malaya, Malaysia
jiuntian001@e.ntu.edu.sg {exdjiang,eyptan,weipeng.hu }@ntu.edu.sg cs.chan@um.edu.my
GLIGEN Stable DiffusionLayout InputInteraction InputInteractDiffusion
Caption: a personis holdinga bag, another person is talkingon a cell phoneCaption: a personis feedinga cat
Generated Image Generated Image Generated Image
personanother person
personfeedingpersoncell phone
catanother personpersontalkingholding
bagbagcell phone
catFigure 1. Generated samples of size 512x512. Stable Diffusion conditions on text caption only, while GLIGEN conditions on extra layout
input. Our proposed InteractDiffusion conditions on extra interaction label and its location shown by the shaded area.
Abstract
Large-scale text-to-image (T2I) diffusion models have
showcased incredible capabilities in generating coherent
images based on textual descriptions, enabling vast appli-
cations in content generation. While recent advancements
have introduced control over factors such as object local-
ization, posture, and image contours, a crucial gap remains
in our ability to control the interactions between objects
in the generated content. Well-controlling interactions in
generated images could yield meaningful applications,
such as creating realistic scenes with interacting charac-
ters. In this work, we study the problems of conditioning
T2I diffusion models with Human-Object Interaction
(HOI) information, consisting of a triplet label (person,
action, object) and corresponding bounding boxes. We
propose a pluggable interaction control model, called
InteractDiffusion that extends existing pre-trained T2I dif-
corresponding authorsfusion models to enable them being better conditioned on
interactions. Speciﬁcally, we tokenize the HOI information
and learn their relationships via interaction embeddings.
A conditioning self-attention layer is trained to map HOI
tokens to visual tokens, thereby conditioning the visual
tokens better in existing T2I diffusion models. Our model
attains the ability to control the interaction and location on
existing T2I diffusion models, which outperforms existing
baselines by a large margin in HOI detection score, as
well as ﬁdelity in FID and KID. Project page: https:
//jiuntian.github.io/interactdiffusion .
1. Introduction
The advent of diffusion generative models recently opens
up new creative task opportunities. While diffusion mod-
els could generate diverse high quality images that re-
construct the original data distributions, it is important to
control the content generated. Numerous literatures have
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6180
since extensively studied how to control the image gener-
ation of the diffusion models via e.g. class [ 10,37], text
[24,26,27,29], image (including edge, line, scribble and
skeleton) [ 2,16,36] and layout [ 2,7,19,34,38]. However,
these are insufﬁcient to effectively express the nuanced in-
tentions and desired outcomes, especially the interactions
between objects. Our work introduces another important
control in image generation: interaction .
Interaction refers to a reciprocal action between two en-
tities or individuals. Without a doubt, interaction is an inte-
gral part of describing our daily activities. However, we ﬁnd
that existing diffusion models work well on static images
such as paintings or scenic photos but face great challenges
in generating images involving interactions. For instance,
GLIGEN [ 19] adds layout as a condition to help specify the
location of objects, but controlling the relationship or inter-
action between the objects remains an open difﬁcult prob-
lem, as shown in Fig. 1. Control at the interaction level in
text-to-image (T2I) diffusion models has countless applica-
tions, e.g. e-commerce, gaming, interactive storytelling etc.
This paper studies the interaction-conditioned image
generation, i.e. how to specify the interaction in the image
generation process. It faces three main challenges:
a)Interaction representation : How to represent interac-
tion information in a meaningful token representation.
b)Intricate interaction relationship : The relationship
among objects with interaction is complex, and gener-
ating coherent images remains a great challenge.
c)Integrating conditions into existing models : Current
T2I diffusion models excel in image generation quality
but lack interaction control. A pluggable module that
can be seamlessly integrated into them is imperative.
To address the aforementioned issues, we propose an in-
teraction control model called InteractDiffusion as a plug-
gable module to existing T2I diffusion model as illustrated
in Fig. 2, aiming to impose interaction control. First, to pro-
vide conditioning information to the diffusion model, we
treat each interacting pair as a HOI triplet and transform
its information into a meaningful token representation that
contains information about position, size, and category la-
bel. Particularly, we generate three different tokens for each
HOI triplet, i.e.subject ,action , and object tokens. While
both subject andobject tokens contain information about lo-
cation, size, and object category, the action token includes
the location of the interaction and its category label.
Secondly, the challenge of representing intricate interac-
tion lies in encoding the relationship between the tokens of
multiple interactions where tokens are from different inter-
action instances and have different role within an interaction
instance. To address this challenge, we propose instance
embedding and role embedding to group the tokens of the
same interaction and embed their role semantically. Thirdly,
as the existing transformer block consists of a self-attentionand a cross-attention layer [ 27], we add a new Interaction
Self-Attention layer in between to incorporate interaction
tokens into the existing T2I model. This helps to preserve
the original model during training, while simultaneously in-
corporating additional interaction conditioning information.
Our main contributions are summarized as follows:
(i)We address the interaction-mismatch problem in ex-
isting T2I models and raise a new challenge: con-
trolling interaction in T2I diffusion models. Our pro-
posed framework, InteractDiffusion , is pluggable
to existing T2I model. It incorporates interaction
information as additional conditions for training an
interaction-controllable T2I diffusion model, enhanc-
ing the precision of interactions in generated images.
(ii)To effectively capture intricate interaction relation-
ships, we introduce a novel method where we to-
kenize the localization and category information of
〈subject, action, object 〉into three distinct tokens.
These tokens are then grouped together and speci-
ﬁed in their roles of interaction through an embed-
ding framework. This innovative approach enhances
the representation of complex interactions.
(iii) InteractDiffusion signiﬁcantly outperforms the base-
line methods in HOI Detection Scores and maintains
generation quality with slight improvements in both
FID and KID metrics. To the best of our knowledge,
this work is the ﬁrst attempt to introduce interaction
control to diffusion models.
2. Related Work
Human-Object Interactions Recent advancements in
Human-Object Interactions (HOI) have focused on detect-
ing HOIs in images. It aims to locate interacting human and
object pairs via bounding boxes and categorize these objects
and their interactions in a triplet form ( e.g., person, feeding,
cat). While recent HOI detection works [ 6,17,21,32,35]
show promising results, data scarcity hampers detection
performance for rare interactions. Conversely, HOI im-
age synthesis, an inverse task of HOI detection, is rela-
tively underexplored. InteractGAN [ 12] proposed HOI im-
age generation via human pose and reference images of hu-
mans and objects. However, this approach is complicated
as it requires a pose-template pool and reference images of
humans and objects. A more closely related work is the
layout-proposal-based method [ 15], which focuses on scene
layout proposals according to HOI triplets to synthesize im-
ages but is limited to generating ”object placement” propos-
als. Our work focuses on a new problem, namely, control-
ling the interaction in existing T2I diffusion models using
simple bounding box and interaction relations in an end-
to-end manner, without human pose and reference images.
This approach efﬁciently addresses the data scarcity in HOI
detection tasks and opens a wide range of applications.
6181
Text Caption!!!"U-NetGenerated ImageNoiseSICSelf AttentionCross AttentionInteraction ModuleInteraction InformationInteraction Self AttentionSICSICInteraction Module
Interaction TokenizerVisual TokensVisual TokensInteraction EmbeddingInteraction Information
Interaction Entity TokensLegends
another personpersontalkingholdingbagcell phone
another persontalkingholdingcell phonebagpersonInteraction TransformerFigure 2. The overall framework of InteractDiffusion. Our proposed pluggable Interaction Module Iseamlessly incorporates interaction
information into an existing T2I diffusion model (left). The proposed module I(right) consists of Interaction Tokenizer (Sec. 3.2) that
transforms interaction information into meaningful tokens, Interaction Embedding (Sec. 3.3) that incorporates intricate interaction relation-
ship, and Interaction Self-Attention (Sec. 3.4) that integrates interaction control information into Visual Tokens of the existing T2I model.
Diffusion Models The diffusion probabilistic model was
ﬁrst proposed in [ 30], and further improved in training and
sampling methods by [ 14,31]. Training and evaluating dif-
fusion models in pixel space could be costly and slow, and
training on high-resolution images always requires calcu-
lating expensive gradients. Latent Diffusion Model (LDM)
[27] compresses the image into a latent representation of
lower dimensionality [ 11] and carries out the diffusion pro-
cess in latent space to reduce the computation which was
further extended to Stable Diffusion. Our work adds inter-
action control to the Stable Diffusion Model.
Controlling Image Generation T2I diffusion models [ 24,
26,27,29] often utilize a pretrained language model like
CLIP [ 25] to guide the image diffusion process. This al-
lows the generated image’s content to be controlled by a
provided text caption. However, a text caption alone of-
ten provides insufﬁcient control over the generated content,
particularly when aiming to create speciﬁc content such as
object location and layout, scene depth maps, human poses,
boundary lines, and interactions. To address this issue, sev-
eral models have proposed different methods for controlling
the generated content, including object layout [ 19,33,38],
segmentation maps [ 1,3,8,18] and images [ 23,36]. Al-
though controlling image generation via object layout and
images can generally yield better results, one essential as-
pect of image has been largely ignored, namely, the interac-
tion between objects. Our work extends the capabilities of
the current T2I model by strengthening the control of inter-
actions in the generated content.
3. Method
We ﬁrst formulate the problem and then detail our Inter-
actDiffusion model, as illustrated in Fig. 2. It comprisesfour parts: (a) interaction tokenizer that transforms inter-
action conditions into tokens, (b) interaction embedding
that links the relationship between tokens of interacting
triplets, (c) interaction transformer that constructs attention
between image patches and interaction information, and (d)
interaction-conditional diffusion model that generates im-
ages with interaction conditions.
3.1. Preliminary
We study the problem of incorporating interaction condi-
tions dinto existing T2I diffusion model alongside with text
caption condition c. Our aim is to train a diffusion model
f✓(z,c,d)to generate images conditioned on interaction d
and text caption c, where zis the initial noise.
Stable Diffusion, one of the best models, is a scale-up
of the LDM [ 27] with a larger model and data size.
Unlike other diffusion models, LDM splits into two stages
to reduce computational complexity. It ﬁrst learns a bi-
directional projection to project image xfrom pixel space to
a latent space as latent representation zand then trains a dif-
fusion model f✓(z,c)in the latent space with latent z. Our
work focuses on the second stage as we are only interested
in conditioning the diffusion model with interaction.
LDM learns a reverse process of a ﬁxed Markov Chain
of length T. It can be interpreted as an equally weighted se-
quence of denoising autoencoders ✏✓(zt,t);t=1,···,T,
which are trained to predict a denoised version of their input
zt, where ztis a noisy version of the input z.
The unconditional objective can be viewed as
min
✓LLDM=Ez,✏⇠N(0,I),t⇥
k✏ ✏✓(zt,t)k2
2⇤
,(1)
with tuniformly sampled from {1,···,T}. The model
iteratively produces less noisy samples from noise zTto
6182
!!!!!!!!!!!"!"!"!"!"!"!!Figure 3. “Between” operation obtains the action focus area (high-
lighted in orange) between subject and object bounding boxes.
zT 1,zT 2,···,z0, where the model ✏✓(zt,t)is realized
by a UNet [ 28]. The ﬁnal image is obtained by projecting
z0in latent space back into image space in a single pass
through the decoder trained in the ﬁrst stage.
Conditioning In LDM, to condition the diffusion model
with various modalities like text captions, a cross-attention
mechanism was added on top of the UNet backbone. The
conditional input of various modalities is denoted as yand
a domain speciﬁc encoder ⌧✓(·)is used to project yto an
intermediate token representation ⌧✓(y).
In StableDiffusion, text captions represented by yare
used to condition the model. It uses a CLIP encoder de-
noted as ⌧✓(·)to project the text caption yinto 77 text em-
beddings, i.e.⌧✓(y). In particular, the conditioned objective
for StableDiffusion can be viewed as
min
✓LLDM=Ez,✏⇠N(0,I),t⇥
k✏ ✏✓(zt,t ,⌧ ✓(y))k2
2⇤
,(2)
where ⌧✓(·)represents the CLIP text encoder and yrepre-
sents the text caption.
3.2. Interaction Tokenizer (InToken)
We deﬁne interaction das a triplet label consisting of 〈sub-
jects, action a, and object o〉, as well as their corresponding
bounding boxes denoted as 〈bs,ba, and bo〉, respectively.
We use the subject and object bounding boxes to describe
their location and sizes, and introduce an action bounding
box to specify the spatial location of the action. For exam-
ple, a subject ( e.g. women, boy) performing a speciﬁc ac-
tion ( e.g. carrying, kicking) toward a particular object ( e.g.
handbag, ball).
To obtain the action bounding box, we deﬁne a “be-
tween” operation, applied to the subject and object bound-
ing boxes. Suppose bsandbobe speciﬁed by their corner
coordinates [↵i, i],i=1,2,3,4, the “between” operation
onbsandboto obtain bais:
ba=bsbetween bo
=[R2(↵i),R2( i)],[R3(↵i),R3( i)], (3)
where Rk(·)is the kthascending rank of its arguments.
Some examples of the ”between” operation results are
shown in Fig. 3. With this, our interaction condition inputs
of an image is:
D=[d1,..., dN]=[ ( s1,a1,o1,bs1,ba1,bo1),...,
(sN,aN,oN,bsN,baN,boN)],(4)!“person”“feeding”“cat”"!"""##$%&$$%&#$%&$$'&#$%&$$(&)*+ $"!&)*+ $""&)*+ $"#&CLIP Text Encoder #$%&$$,&Fourier Embedder-)*+ $,&.'Object MLP*ActionMLP-.(.)!!Figure 4. Interaction Tokenizer. View bottom-up.
where Nis the number of interaction instances.
Subject and Object tokens We ﬁrst pre-process the text
label and the bounding box into an intermediate representa-
tion. In particular, we use the pre-trained CLIP text encoder
to encode the text of subject, action and object as a repre-
sentative text embedding and use Fourier embedding [ 22] to
encode their respective bounding boxes following GLIGEN
[19]. To generate the subject and object tokens, hs,ho, we
use a multi-layer perceptron ObjectMLP (·)to fuse them as:
hs=ObjectMLP ([ftext(s),Fourier (bs)]) (5)
ho=ObjectMLP ([ftext(o),Fourier (bo)]). (6)
Action token For action token, we train a separate multi-
layer perceptron ActionMLP (·)since action is semantically
apart from the subject and object,
ha=ActionMLP ([ftext(a),Fourier (ba)]). (7)
For each interaction, we transform the interaction condi-
tion input dinto a triplet of tokens h:
h=(hs,ha,ho)=InToken (s, a, o, bs,ba,bo), (8)
where InToken (·)is a combination of Eqs. ( 5) to ( 7) as
shown in Fig. 4.
3.3. Interaction Embedding (InBedding)
Interaction is an intricate relationship between subject, ob-
ject and their action. From Eq. ( 8), tokens hs,ha,hoare in-
dividually embedded (as shown in Fig. 2). For multiple in-
teraction instances, all tokens hs
i,ha
i,ho
i;i=1,···,N, are
individually embedded. Therefore, it is necessary to group
these tokens by interaction instance and specify different
role of tokens within the interaction instance. Segment Em-
bedding [ 9], has demonstrated its effectiveness in captur-
ing relationships between segments in a text sequence by
adding a learnable embedding to tokens to group a sequence
of words into segments. In our work, we extend this con-
cept to group the tokens into triplets. Speciﬁcally, we add
6183
!"!""!#"!$+=!#!"#!##!$!!!!!!+++"""#"$++==!"%""%#"%$+=!#%"#%##%$!%!%!%+++"""#"$++==####!"&""&#"'$+=!#&"#&##&$!&!&!&+++"""#"$++==Initial Interaction Entity TokensInstance EmbeddingsRole EmbeddingsFinal Interaction Entity TokensFigure 5. Interaction Embeddings. Learnable instance embedding
qand role embedding rare added to tokens to represent intricate
interaction relationships between subject s, action aand object o.
a new instance embedding denoted as q2{q1,...,q N}to
interaction instances h2{h1,···,hN}as:
ei=hi+qi, (9)
where all tokens in the same instance share the same in-
stance embedding. This groups all tokens into interaction
instances or triplets.
Besides, each token in the triplet has different role. So,
we embed their roles with three role embeddings r2
{rs,ra,ro}to form ﬁnal entity token ei:
ei=hi+qi+r
=(hs
i+qi+rs,ha
i+qi+ra,ho
i+qi+ro),(10)
where rs,raandrorepresent the role embeddings for sub-
ject, action and object respectively. From Eq. ( 10) we see
that tokens of the same role in all instances share the same
role embedding. Adding instance and role embedding to the
interaction entity token hi(as in Fig. 5) encodes the intri-
cate interaction relationship, i.e. speciﬁes a token’s role and
interaction instance, which results in signiﬁcantly improved
image generation, especially in scenarios with multiple in-
teraction instances.
3.4. Interaction Transformer (InFormer)
Large-scale T2I models such as Stable Diffusion have been
trained on massive-scale image-text pairs and demonstrated
remarkable capabilities in generating highly realistic im-
ages, owing to the knowledge acquired during large-scale
pre-training. In this paper, we aim to incorporate the in-
teraction control into these T2I models with minimal cost.
Therefore, it is crucial to preserve the valuable knowledge
embedded in them.
Lets denote v=[v1,···,vM]as the visual feature
tokens of an image, and cas the caption tokens where
c=⌧✓(y). In LDM models, a Transformer block consists
of two attention layers, i.e. (i) self-attention layer for the
visual tokens and (ii) cross-attention layers that model the
attention between visual tokens and caption tokens:
v=v+SelfAttn (v);v=v+CrossAttn (v,c)(11)!!"!!#!!$!%"!%#!%$"!#!Self-Attention Cross-Attention"&"'⋮"!"&"'⋮⋮"!"&"'⋮Interaction Self-Attention#&#(⋮"!"&"'⋮!"!#!$Interaction Entity Tokens"Visual Tokens#Caption Tokens
Figure 6. Interaction Transformer. An Interaction Self-Attention
is added between the visual token self-attention and the visual-
caption cross-attention to incorporate the interaction conditions.
Interaction Self-Attention Following GLIGEN [ 19], we
freeze the two original attention layers and introduce a new
gated self-attention layer namely Interaction Self-Attention
(see Fig. 6) between them. This is to add the interaction
condition onto the existing Transformer block. Different
from [ 19], we perform self-attention over the concatenation
of visual and interaction tokens [v,es,ea,eo], which fo-
cuses on the relationship of interactions as:
v=v+⌘·tanh  ·TS(SelfAttn ([v,es,ea,eo])),(12)
where TS (·)is a Token Slicing operation to keep only the
output of visual tokens and slice off the others as shown in
Fig.6,⌘is a hyper-parameter for scheduled sampling that
controls the activation of Interaction Self-Attention and  is
a zero-initialized learnable scale that gradually controls the
ﬂow of the gate. Note that Eq. ( 12) performs in between the
two parts of Eq. ( 11). As a summary, our Interaction Self-
Attention layer transforms the interaction information, in-
cluding the interaction, subject and object bounding boxes,
into visual tokens.
Scheduled Sampling We set ⌘=1in Eq. ( 12) during train-
ing and standard inference scheme as to [ 19]. However,
in some occasional situations, the newly added Interaction
Self-Attention layer could cause sub-optimal effects (poor
rendering of non-natural concepts) on existing T2I models.
Thus we include a control on sampling interval on the Inter-
action Self-Attention layer, which can balance out the level
of text caption and interaction control.
Technically, our scheduled sampling scheme is con-
trolled during the inference time by a hyper-parameter !2
[0,1]. It deﬁnes the proportion of diffusion steps inﬂuenced
by the interaction control as follow:
⌘=(
1,t !⇤T# Text + Interaction
0,t > ! ⇤T# Text only(13)
where Tis total number of diffusion steps.
6184
3.5. Interaction-conditional Diffusion Model
We combine InToken, InBedding and InFormer to form the
pluggable Interaction Module, enabling interaction control
in existing T2I diffusion models. The LDM training objec-
tive (Eq. ( 2)) is adopted. Denoting the newly added param-
eters as ✓0, the diffusion model is now deﬁned as ✏✓,✓0(·)
where the extra interaction information is processed by the
interaction tokenizer ⌧✓0(·). As such, the overall training
objective of our model is:
min
✓0LInteractDiffusion = (14)
Ez,✏⇠N(0,I),t⇥
k✏ ✏✓,✓0(zt,t ,⌧ ✓(y),⌧✓0(D))k2
2⇤
.
4. Experiments
We train and evaluate models at 512x512 resolution. We
initialize our model with the pre-trained GLIGEN model
based on StableDiffusion v1.4. Training uses a constant
learning rate of 5e-5 with Adam optimization and a linear
warm-up for the initial 10k iterations. It ran for 500k itera-
tions with a batch size of 8 ( ⇡106 epochs), taking around
160 hours on 2 NVIDIA GeForce RTX 4090 GPUs. We
use a gradient accumulate step of 2, resulting in an effective
batch size of 16. For inference, we employ diffusion sam-
pling steps of 50 with the PLMS [ 20] sampler. More details
are given in Sec. 6of supplementary.
4.1. Datasets
Our experiments were conducted on the widely-used HICO-
DET dataset [ 5], which comprises 47,776 images: 38,118
for training and 9,658 for testing. The dataset includes
151,276 HOI annotations: 117,871 in training and 33,405 in
testing. HICO-DET includes 600 types of HOI triplets con-
structed from 80 object categories and 117 verb classes. We
extracted the annotations in the testing set as input to gen-
erate interaction images and subsequently performed HOI
detection on the generated images using FGAHOI [ 21].
Following the evaluation methodology outlined in
HICO-DET [ 5], we evaluated the generation results in both
Default and Known Object settings. In the Default setting,
the average precision (AP) is computed across all testing
images for each HOI class. The Known Object setting, on
the other hand, calculates the AP of an HOI class solely over
the images containing the object in the corresponding HOI
class (e.g., the AP of the HOI class ’riding bicycle’ is cal-
culated exclusively on the images containing the ’bicycle’
object). We reported the HOI detection results in the Full
and Rare subsets. The Full and Rare subsets consist of 600
and 138 HOI classes, respectively, with a rare class deﬁned
as one represented by less than 10 training samples.4.2. Evaluation Metrics
We evaluate the quality and controllability of interaction in
generation with three metrics.
Fr´echet Inception Distance [13] measures the Fr ´echet dis-
tance in distribution of Inception feature between the real-
images and the generated images (FID).
Kernel Inception Distance [4] measures the squared Maxi-
mum Mean Discrepancy (MMD) between the Inception fea-
tures of the real and generated images using a polynomial
kernel. It relaxes the Gaussian assumption in FID and re-
quires fewer samples.
HOI Detection Score is proposed as a measure of the con-
trollability of interaction in generation models. We utilize
the pretrained state-of-the-art HOI detector, FGAHOI [ 21],
to detect the HOI instances in generated images and com-
pare them against the ground truth from the original anno-
tations in HICO-DET. This process quantiﬁes the models’
controllability in interaction generation. We report the HOI
Detection Score based on the FGAHOI protocol in two cat-
egories, namely Default andKnown Object. Default setting
is more challenging as it requires distinguishing the non-
related images. FGAHOI is implemented with Swin-Tiny
and Swin-Large backbones, and we evaluate with the both.
In summary, FID and KID assess generation quality,
while HOI Det. Score evaluates interaction controllability.
4.3. Qualitative results
Fig.7presents a qualitative comparison with existing meth-
ods. The results demonstrate that our model renders the
interaction relationship between objects better than others,
aligning better with the provided interaction instructions.
Other models often exhibit either mismatched actions or in-
accurate interactions. For instance, while GLIGEN incor-
porates layout control to precisely position objects within an
image, it fails to capture their intricate interactions. Espe-
cially, when multiple interaction instances occur within an
image, GLIGEN’s rendering of interaction relationships is
often mismatched. This challenge persists even in the case
of GLIGEN* which is ﬁne-tuned on HICO-DET. While the
individual placement (location) of objects is accurate, the
interactions between objects appear perplexing.
Our proposed method facilitates improved control over
object interaction in image generation. For instance, in
Fig.7(a)-(c), although the interaction appears to be correct
in existing works, the interaction details are inaccurate. Our
method better renders these details. Moreover, when mul-
tiple interacting pairs are involved, as shown in Fig. 7(d),
only our method is capable of correctly rendering all pairs
of interactions. In Fig. 7(e)-(i), while the interactions ( e.g.
directing airplane, sitting at the dining table, blowing cake,
eating pizza, ﬂushing the toilet) in images were inaccurately
generated in existing works, our InteractDiffusion well ren-
ders these interactions. Our model’s capability stems from
6185
ModelQuality#FGAHOI Swin-Tiny (mAP)"FGAHOI Swin-Large (mAP)"DefaultKnown ObjectDefaultKnown ObjectFID KIDFull RareFull RareFull RareFull RareStableDiffusion35.85 0.012970.63 0.680.66 0.700.64 0.830.65 0.84GLIGEN29.35 0.0127521.73 15.3523.31 17.2423.99 19.5624.99 20.37GLIGEN*18.82 0.0069425.23 17.4526.66 18.7826.45 18.9327.32 19.90InteractDiffusion18.69 0.0067629.53 23.0230.99 24.9331.56 26.0932.52 27.04HICO-DET--29.94 22.2432.48 24.1637.18 30.7138.93 31.93Table 1. Comparison between InteractDiffusion and existing baselines in
terms of generated image quality scores in FID and KID and HOI detection
score in mAP. GLIGEN* is HICO-DET ﬁne-tuned GLIGEN model. The last
row shows the Detection Score from real images.Model Tr. To. Em.Quality Default " Kn. Obj. "
FID#KID #Full Rare Full Rare
StableDiffusion 35.85 0.01297 0.63 0.68 0.66 0.70
GLIGEN X* 29.35 0.01275 21.73 15.35 23.31 17.24
GLIGEN* X* 18.82 0.00694 25.23 17.45 26.66 18.78
InteractDiffusionXX 18.88 0.00686 28.73 21.93 30.15 23.38
XXX 18.69 0.00676 29.53 23.02 30.99 24.93
HICO-DET -- 29.94 22.24 32.48 24.16
Table 2. Ablation study of InteractDiffusion. Tr., To., and
Em. represent Interaction Transformer, Interaction Tok-
enizer, and Interaction Embedding respectively. X* indi-
cate Gated Self-Attention in GLIGEN.Input
 Caption
 GT
 SD
 GLIGEN
 GLIGEN*
 Ours
(a) (b) (c) (d) (e) (f) (g) (h) (i)
Figure 7. Visual comparison with existing baselines. In all methods, we use the text caption format of ”a person {action }a{object }”.
Input and Caption rows represent the interaction conditions, each interaction pair shown by a line link them and is colored differently. GT
represents the ground truth images. Ours gains better control to interaction, and renders images matching the text instructions better.
two key components: the InToken for translating interac-
tion conditions into meaningful tokens, and the InBedding
for modeling complex interaction relationships.
Fig.8shows how InteractDiffusion renders different ac-
tions with the same object, in comparison to StableDiffu-
sion and GLIGEN*. This shows that our model can gen-erate various combinations of interactions that maintain the
coherence and naturalness of interactions between people
and objects. More qualitative results are shown in Sec. 8
and Figs. 13and14of the supplementary, while user pref-
erence study is in Sec. 9.
6186
a person is〈*〉a bottledrinking holding pouringFigure 8. Visualization comparison between StableDiffusion(top), GLIGEN* (middle), and InteractDiffusion (bottom) demon-strates the generation ofdifferent actionsfor the same object.4.4. Quantitative resultsTab.1compares our proposed with existing baselines interms of the quality and interaction controllability, speciﬁ-cally FID, KID, and HOI Detection Score. Compared to theexisting baselines, our proposed achieves the best result.For image generation quality, our proposed producesslightly higher quality than the baselines. It shows thatdespite additional parameters incorporated into the originalmodel to control interactions, the image generation qualityremains unaffected. It is even improved marginally. GLI-GEN* exhibits higher image generation quality than Sta-bleDiffusion and GLIGEN because we ﬁne-tuned it on theHICO-DET dataset in the same way as InteractDiffusion.In the HOI Detection Score benchmark, StableDiffusionperforms poorly because it does not consider the object’slocation and size. Comparing GLIGEN and GLIGEN* thatonly consider the object’s location and size, our method en-codes interaction control information along with localiza-tion information, leading to a signiﬁcant performance gain.Using the Tiny backbone for detection, the slight dispar-ity in mAP between the generated images by our methodand the real image dataset demonstrates that our approachcan generate realistic interactions nearly indistinguishablefrom real-world interactions by a detection algorithm, suchas FGAHOI with a Swin-Tiny backbone. Yet, we have ob-served that the gap between the real dataset and the gen-erated samples widens when a detector of a large model isused. This indicates that although our generation processoutperforms existing baselines, it still has room for furtherimprovement in rendering ﬁner details.Empirically, the results demonstrate that our proposedenhances interaction controllability while maintaining high-quality image generation capability, thereby signiﬁcantlyoutperforming the existing methods in all metrics. This su-perior performance can be attributed to the proposed com-ponents within InteractDiffusion, which include theInTo-kenthat incorporates new interaction conditions,InBed-dingthat encode intricate interaction relationships, and theInFormerthat injects interaction control into the existingtransformer blocks. Collectively, these components consti-tute a pluggable Interaction Module seamlessly integratedinto existing T2I diffusion models.4.5. Ablation studiesThere are three key components that constitute the pro-posed InteractDiffusion, namely, InToken, InBedding, andInFormer. We conducted an ablation study on these com-ponents and tabulated the results in Tab.2. GLIGEN intro-duced a gated self-attention layer into the transformer blockof the Stable Diffusion model to incorporate additional lay-out conditions, resulting in a signiﬁcant performance im-provement from 0.63 to 21.73 in mAP. Upon further ﬁne-tuning on HICO-DET, it achieved an mAP of 25.23.In InteractDiffusion, we include interaction conditions,alongside layout conditions, to enable the interaction con-trol. With InToken, we convert the interaction conditions(consisting of bounding boxes, object labels, action labels,and relationships) into meaningful interaction entity tokens.Compared to GLIGEN, the incorporation of additional ac-tion tokens introduces new information that enhances in-teraction generation and provides greater interaction con-trol. The inclusion of InToken as a key component furtherimproved the detection score from 25.23 to 28.73, therebydemonstrating its effectiveness. Lastly, we include InBed-ding to encode the complex interactions relationship, whichfurther improved detection score from 28.73 to 29.53. Moreablation studies are shown in Sec.7of the supplementary.5. ConclusionThis paper proposes an interaction-conditioned T2I dif-fusion model, namely InteractDiffusion, which addressesproblems of conditioning generated images beyond the textcaption. In existing T2I diffusion models, although severalcontrols (e.g. text, images, layout, etc) have been imposed,controlling the interaction in the generated image remains aformidable challenge. Our contributions can be uniﬁed asa pluggable interaction module being seamlessly integratedinto existing T2I models. The quantitative and qualitativeevaluations demonstrate the effectiveness of our method incontrolling the interaction of generated content, which sig-niﬁcantly outperforms the state-of-the-art approaches.AcknowledgementThis research is supported in part by the Na-tional Research Foundation, Singapore, under the NRF MediumSized Centre Scheme (CARTIN). Any opinions, ﬁndings and con-clusions expressed in this material are those of the authors anddo not reﬂect the views of National Research Foundation, Singa-pore.
6187
References
[1]Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,
Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried,
and Xi Yin. Spatext: Spatio-textual representation for con-
trollable image generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 18370–18380, 2023. 3
[2]Arpit Bansal, Hong-Min Chu, Avi Schwarzschild,
Soumyadip Sengupta, Micah Goldblum, Jonas Geip-
ing, and Tom Goldstein. Universal guidance for diffusion
models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 843–852,
2023. 2
[3]Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.
Multidiffusion: Fusing diffusion paths for controlled image
generation. arXiv preprint arXiv:2302.08113 , 2023. 3
[4]Mikołaj Bi ´nkowski, Danica J Sutherland, Michael Arbel, and
Arthur Gretton. Demystifying mmd gans. arXiv preprint
arXiv:1801.01401 , 2018. 6
[5]Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia
Deng. Learning to detect human-object interactions. In 2018
ieee winter conference on applications of computer vision
(wacv) , pages 381–389. IEEE, 2018. 6
[6]Junwen Chen and Keiji Yanai. Qahoi: Query-based an-
chors for human-object interaction detection. arXiv preprint
arXiv:2112.08647 , 2021. 2
[7]Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free
layout control with cross-attention guidance. arXiv preprint
arXiv:2304.03373 , 2023. 2
[8]Guillaume Couairon, Marl `ene Careil, Matthieu Cord,
St´ephane Lathuili `ere, and Jakob Verbeek. Zero-shot spatial
layout conditioning for text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 2174–2183, 2023. 3
[9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 4
[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 2
[11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 12873–12883, 2021. 3
[12] Chen Gao, Si Liu, Defa Zhu, Quan Liu, Jie Cao, Haoqian He,
Ran He, and Shuicheng Yan. Interactgan: Learning to gen-
erate human-object interaction. In ACM MM , page 165–173,
New York, NY , USA, 2020. Association for Computing Ma-
chinery. 2
[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 6
[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 3
[15] Tianyu Hua, Hongdong Zheng, Yalong Bai, Wei Zhang,
Xiao-Ping Zhang, and Tao Mei. Exploiting relationship
for complex-scene image generation. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , pages 1584–
1592, 2021. 2
[16] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,
and Jingren Zhou. Composer: Creative and controllable im-
age synthesis with composable conditions. arXiv preprint
arXiv:2302.09778 , 2023. 2
[17] Bumsoo Kim, Junhyun Lee, Jaewoo Kang, Eun-Sol Kim,
and Hyunwoo J. Kim. Hotr: End-to-end human-object in-
teraction detection with transformers. In CVPR , 2021. 2
[18] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and
Jun-Yan Zhu. Dense text-to-image generation with attention
modulation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 7701–7711, 2023. 3
[19] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
Gligen: Open-set grounded text-to-image generation. In
CVPR , pages 22511–22521, 2023. 2,3,4,5
[20] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo
numerical methods for diffusion models on manifolds. In In-
ternational Conference on Learning Representations , 2022.
6
[21] Shuailei Ma, Yuefeng Wang, Shanze Wang, and Ying Wei.
Fgahoi: Fine-grained anchors for human-object interaction
detection. arXiv preprint arXiv:2301.04019 , 2023. 2,6
[22] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
representing scenes as neural radiance ﬁelds for view synthe-
sis.Communications of the ACM , 65:99–106, 2022. 4
[23] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu,
Bochen Guan, Yin Li, and Bolei Zhou. Freecontrol:
Training-free spatial control of any text-to-image diffusion
model with any condition. arXiv preprint arXiv:2312.07536 ,
2023. 3
[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 2,3
[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 3,4
[26] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 2,3
[27] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models, 2021. 2,3,4
6188
[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 4
[29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. In NeurIPS , pages 36479–36494,
2022. 2,3
[30] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
ence on machine learning , pages 2256–2265. PMLR, 2015.
3
[31] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In International Conference
on Learning Representations , 2020. 3
[32] Suchen Wang, Yueqi Duan, Henghui Ding, Yap-Peng Tan,
Kim-Hui Yap, and Junsong Yuan. Learning transferable
human-object interaction detector with natural language su-
pervision. In CVPR , pages 939–948, 2022. 2,4
[33] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wen-
tian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff:
Text-to-image synthesis with training-free box-constrained
diffusion. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 7452–7461, 2023. 3
[34] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin
Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael
Zeng, et al. Reco: Region-controlled text-to-image genera-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 14246–14255,
2023. 2
[35] Hangjie Yuan, Jianwen Jiang, Samuel Albanie, Tao Feng,
Ziyuan Huang, Dong Ni, and Mingqian Tang. Rlip: Rela-
tional language-image pre-training for human-object interac-
tion detection. Advances in Neural Information Processing
Systems , 35:37416–37431, 2022. 2
[36] Lvmin Zhang and Maneesh Agrawala. Adding conditional
control to text-to-image diffusion models. arXiv preprint
arXiv:2302.05543 , 2023. 2,3
[37] Guangcong Zheng, Shengming Li, Hui Wang, Taiping Yao,
Yang Chen, Shouhong Ding, and Xi Li. Entropy-driven sam-
pling and training scheme for conditional diffusion genera-
tion. In European Conference on Computer Vision , pages
754–769. Springer, 2022. 2
[38] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi,
Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffu-
sion model for layout-to-image generation. In CVPR , 2023.
2,3
6189
