Robust Self-calibration of Focal Lengths from the Fundamental Matrix
Viktor Kocur1,2Daniel Kyselica1Zuzana Kukelova2
1Faculty of Mathematics, Physics and Informatics, Comenius University in Bratislava
2Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague
{viktor.kocur, daniel.kyselica }@fmph.uniba.sk kukelzuz@fel.cvut.cz
Abstract
The problem of self-calibration of two cameras from a
given fundamental matrix is one of the basic problems in
geometric computer vision. Under the assumption of known
principal points and square pixels, the Bougnoux formula
offers a means to compute the two unknown focal lengths.
However, in many practical situations, the formula yields
inaccurate results due to commonly occurring singularities.
Moreover, the estimates are sensitive to noise in the com-
puted fundamental matrix and to the assumed positions of
the principal points. In this paper, we therefore propose
an efficient and robust iterative method to estimate the focal
lengths along with the principal points of the cameras given
a fundamental matrix and priors for the estimated camera
intrinsics. In addition, we study a computationally efficient
check of models generated within RANSAC that improves
the accuracy of the estimated models while reducing the to-
tal computational time. Extensive experiments on real and
synthetic data show that our iterative method brings signifi-
cant improvements in terms of the accuracy of the estimated
focal lengths over the Bougnoux formula and other state-
of-the-art methods, even when relying on inaccurate priors.
The code for the methods and experiments is available at
https://github.com/kocurvik/robust self calibration
1. Introduction
Camera calibration, i.e., the estimation of camera intrinsic
parameters, is a fundamental problem in computer vision
with many applications. The precision of estimated intrin-
sic parameters, such as focal length and principal point, sig-
nificantly affects the precision of tasks in structure-from-
motion (SfM) [38],visual localization [34], 3D object detec-
tion [45], augmented reality [6], and other applications [39].
Classical calibration methods [46] use known calibra-
tion patterns, e.g. checkerboards, or additional knowledge
of the observed scene to estimate the camera intrinsics. As
such, they are often impractical. In contrast, self-calibration
methods do not require additional knowledge of the scenegeometry and rely on automatic detection of image features.
Thus, they are preferable in many applications.
In this paper, we study the problem of self-calibration of
two cameras, with potentially different intrinsic parameters
that capture the same scene. Although this is a well-studied
problem, with first solutions dated back to the nineties, to
the best of our knowledge, all its existing solutions suffer
from some instabilities and robustness problems.
The geometry of two uncalibrated cameras is captured by
the fundamental matrix [28]. In 1998 Bougnoux [2] showed
that assuming square pixels and known principal points of
both cameras, it is possible to obtain the focal lengths of the
two cameras from the fundamental matrix using a closed-
form formula. This provides a convenient way to determine
the focal length, as the principal point can usually be as-
sumed to lie in the image center. However, the precision of
the estimated focal lengths using the Bougnoux formula [2]
and other similar methods [10, 18, 29] is often marred by in-
accuracies, sometimes even resulting in physically implau-
sible imaginary focal lengths. This stems from singularities
and susceptibility of these methods to noise in fundamental
matrices and assumed positions of principal points. More-
over, singularities occur in very common camera configu-
rations when the principal axes of the cameras are coplanar
(i.e. they intersect), which is common when images of a sin-
gle object of interest are taken from different views.
An alternative approach [15] relies on iterative optimiza-
tion of a multi-term loss, allowing one to estimate both
the focal lengths of the cameras as well as their principal
points. This approach addresses some of the drawbacks of
the aforementioned methods but still relies on the Boug-
noux formula [2] within the iterative process. Thus, it is sus-
ceptible to inaccuracies, especially in the aforementioned
degenerate camera configurations.
Similarly to [15], we formulate the problem of estimat-
ing focal lengths and principal points from a given funda-
mental matrix as a constrained optimization problem us-
ing priors. Constraints ensure that the solution satisfies the
Kruppa equations [27]. To solve the constrained optimiza-
tion problem, we search for the stationary points of its La-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5220
grangian. This results in a complex system of polynomial
equations that we efficiently solve in an iterative way. Each
iteration in our formulation involves solving a system of two
polynomial equations of degree four in two variables, which
is efficiently solved using the Gr ¨obner basis method [24].
Existing approaches, as well as our method, are based on
fundamental matrices that can be estimated using different
variants of RANSAC [1, 7–9, 11, 16]. Some of the matrices
produced by the 7-point algorithm [14] within RANSAC
lead to imaginary focal lengths when decomposed using
the Bougnoux formula [2]. We observed that such ma-
trices rarely lead to the final RANSAC model. Based on
this insight, we propose performing a degeneracy check for
such matrices and rejecting them inside RANSAC, thus sav-
ing computational time on their further processing, such as
scoring and local optimization.
In summary, the contributions of this paper are:
• A novel iterative method for focal length and principal
point estimation from fundamental matrices. Evaluation
on synthetic and large-scale real-world datasets [17, 47]
shows that our method results in superior accuracy of es-
timated focal lengths as well as camera poses.
• A simple and computationally efficient degeneracy check
for fundamental matrices leading to imaginary focal
lengths within RANSAC. Performance on real-world
data [17, 47] shows that performing this check and reject-
ing such models within various RANSAC variants [1, 7–
9] and implementations [3, 16, 23] leads to faster compu-
tations and more accurate pose and focal length estimates.
2. Related Work
The problem of self-calibration of a camera from a given
fundamental matrix is a well-studied problem, with several
solutions that can be divided into two main groups, the di-
rect methods and the iterative methods.
2.1. Direct Methods
Bougnoux [2] showed that under the assumption of square
pixels and known positions of the principal points, it is pos-
sible to calculate the focal lengths of both cameras from
the fundamental matrix in closed form. Kanatani and Mat-
sunuga [18] derived a closed form solution directly from the
elements of the fundamental matrix, avoiding the intermedi-
ate epipole computation required by the Bougnoux formula.
Melanitis and Maragos [29] formulated a linear system, the
solution of which provides the focal lengths.
These approaches are all algebraically equivalent and
thus suffer from the same generic singularity, which oc-
curs whenever the principal axes of the two cameras are
coplanar, i.e. they are intersecting. This is a very common
scenario in practice, as human photographers tend to place
objects of interest in the centers of images. The singular-
ity significantly affects the accuracy of the estimated focallengths even when the axes are close to coplanar.
Similar formulas have been derived for the case when the
focal lengths of the cameras are known to be equal [4, 18,
41]. In case of equal focal lengths, it is possible to avoid es-
timating the full 7-DoF fundamental matrix and instead em-
ploy a 6-point minimal solver [40] within RANSAC to find
the relative poses of the cameras along with the focal length.
These methods also suffer from generic singularities when
the principal axes are parallel or intersecting and the camera
centers are equidistant from the intersection point [42].
When only one of the focal lengths is unknown, the focal
length can be estimated in closed form from the fundamen-
tal matrix [44] or using a 6-point minimal solver [5]. A de-
generate case for these methods only occurs when the center
of the calibrated camera lies on the principal axis of the un-
calibrated camera, a situation rarely occurring in practice.
Since focal lengths appear in squared form in the Kruppa
equations [27], all the previously mentioned direct meth-
ods provide only the squares of focal lengths. This may
result in physically implausible imaginary focal lengths due
to noise. Depending on the level of noise and camera con-
figuration, imaginary focal lengths can occur at a significant
rate (over 20% of samples) [19]. Imaginary focal lengths
are less likely to occur when self-calibration is performed
using three views [20].
2.2. Iterative Methods
Hartley and Silpa-Anan [15] pointed out that errors in the
assumed positions of principal points may significantly af-
fect the focal lengths estimated by the Bougnoux formula.
To remedy this issue, they proposed an iterative method
based on LM optimization [25]. During the optimization,
the fundamental matrix and the positions of the principal
points in both images are optimized to minimize a multi-
term loss. The loss consists of the Sampson error on the
correspondences and the distance of the principal points and
squared focal lengths from predetermined priors. To pre-
vent imaginary focal lengths, the loss also contains a pe-
nalization term which dramatically increases whenever the
square focal lengths are below a given threshold. During
the optimization procedure, the focal lengths are estimated
from the fundamental matrix using the Bougnoux formula.
This approach overcomes many of the issues of using the
formula alone, but may still suffer in the vicinity of degen-
erate camera configurations, as it still relies on the formula
to estimate the focal lengths. Additionally, the procedure is
computationally expensive as it requires computation of the
Sampson error for all correspondences in each iteration.
Iterative methods can also be used to estimate the cam-
era intrinsics using two or more views [10, 12, 27, 31].
When considering only two views with two unknown focal
lengths, these methods should converge to the Bougnoux
formula. However, these methods may result in different
5221
outcomes in the vicinity of degenerate configurations. For
example, the method by Fetzer et al. [10] optimizes for an
energy functional based on the Kruppa equations [27]. In
degenerate configurations, the functional does not have a
single minimum, yet the optimization procedure will be ter-
minated at some point, providing an output different from
the one provided by the Bougnoux formula.
Next, we describe the proposed iterative method that
aims to avoid the aforementioned problems of existing ap-
proaches for self-calibration from the fundamental matrix.
3. Robust focal length estimation
In this paper, similar to Hartley [15], we formulate the prob-
lem of estimating focal lengths from a given fundamental
matrix Fas an optimization problem using priors.
LetF=UDV⊤be the SVD of the fundamental matrix
F, where U= [u1,u2,u3]andV= [v1,v2,v3]are or-
thonormal matrices and D=diag(σ1, σ2,0)is a diagonal
matrix with two non-zero singular values σ1andσ2ofF.
Letω∗
i=KiK⊤
ibe the dual image of the absolute conic for
theithcamera, i= 1,2, with Kibeing the 3×3calibration
matrix of the form
Ki=
fi0ui
0fivi
0 0 1
, (1)
with the focal length fiand principal point ci= [ui, vi]⊤.
For fixed principal points, e.g.,ci= [0,0],i= 1,2,
there is a closed-form solution for the focal lengths ex-
tracted from the given fundamental matrix F,i.e., the Boug-
noux formula [2]. Since this formula can result in unstable
or imaginary estimates, in our formulation, we allow mov-
ing the estimated principal points to avoid such instabilities.
For non-fixed principal points ci, there are infinitely many
decompositions of Finto the essential matrix Eand two
calibration matrices of the form (1). Thus, we formulate
the problem of estimating focal lengths from Fas a con-
strained optimization, where during the optimization, the
focal lengths, along with the positions of principal points in
both images are optimized to minimize the cost function:
min
f1,f2,c1,c2X
i=1,2wf
i(fi−fp
i)2+wc
i∥ci−cp
i∥2
s.t.κ1=σ1(v⊤
1ω∗
1v1)(u⊤
1ω∗
2u2)+
+σ2(v⊤
1ω∗
1v2)(u⊤
2ω∗
2u2) = 0
κ2=σ1(v⊤
1ω∗
1v2)(u⊤
1ω∗
2u1)+
+σ2(v⊤
2ω∗
1v2)(u⊤
1ω∗
2u2) = 0 ,(2)
where fp
iandcp
i= [up
i, vp
i]⊤,i= 1,2are the priors for
the focal lengths and the principal points, wf
iandwc
iare
predetermined weights, and κ1= 0 andκ2= 0 are twoKruppa equations [27]1, which are functions of both focal
lengths and principal points that appear in ω∗
i=KiK⊤
i.
The Kruppa equations ensure that for the input fundamen-
tal matrix Fand the estimated calibration matrices K1and
K2(1), the matrix K⊤
1FK 2is a valid essential matrix.
The cost function in (2) has several advantages over the
cost function used in [15]. (1) It does not contain a term
with the Sampson error on correspondences, which signifi-
cantly slows down the optimization in [15]. (2) It operates
directly on focal lengths fiinstead of squared focal lengths
f2
i. (3) The minimization of (2) does not require compu-
tation or initialization using the Bougnoux formula [2] as
used in [15]. Such an initialization can be very far from the
ground truth focal lengths and in many scenarios can result
inf2
i≤0. Therefore, minimization of (2) does not need a
special penalty term for f2
i≤0as used in [15].
The constrained optimization problem (2) can be trans-
formed into an unconstrained problem using the method of
Lagrange multipliers. This leads to the Lagrange function
L(f1, f2,c1,c2, λ1, λ2):
L=X
i=1,2wf
i(fi−fp
i)2+wc
i∥ci−cp
i∥2−2λ1κ1−2λ2κ2,
(3)
where λ1andλ2are the Lagrange multipliers. The constant
‘−2’ is introduced for an easier subsequent manipulation of
the equations and does not influence the final solution.
In this case, if (f⋆
1, f⋆
2,c⋆
1,c⋆
2)is a point of the minimum
of the original constrained problem (2), then there exist λ⋆
1
andλ⋆
2such that (f⋆
1, f⋆
2,c⋆
1,c⋆
2, λ⋆
1, λ⋆
2)is a stationary point
ofL(3),i.e., a point where all the partial derivatives of L
vanish. The Lagrange function L(f1, f2,c1,c2, λ1, λ2)(3)
is a function of eight unknowns. Thus, to find all stationary
points of Lwe need to solve the following system of eight
polynomial equations in eight unknowns:2
2wf
i(fi−fp
i)−2λ1∂κ1
∂fi−2λ2∂κ2
∂fi= 0,(4)
2wc
i(ci−cp
i)−2λ1∂κ1
∂ci⊤
−2λ2∂κ2
∂ci⊤
=0,(5)
κ1= 0,(6)
κ2= 0.(7)
Unfortunately, the system of eight equations (4)-(7) is too
complex to be efficiently solved using an algebraic method,
e.g. using the Gr ¨obner basis method [22, 24]. Next, we pro-
pose an iterative method to efficiently find a solution of sys-
tem (4)-(7) that minimizes the cost function (2).
1From three Kruppa equations only two are linearly independent.
2Note that (4) represents two equations for i= 1,2and (5) represents
four equations, since there are two vector equations for i= 1,2.
5222
3.1. Iterative method
First, let us denote ∆fi=fi−fp
i,∆ci=ci−cp
i,i= 1,2.
From equations (4)–(5) we have
∆fi=1
wf
i(λ1∂κ1
∂fi+λ2∂κ2
∂fi), (8)
∆ci=1
wc
i
λ1
∂κ1
∂ci⊤
+λ2
∂κ2
∂ci⊤
. (9)
and the cost function in the original constrained optimiza-
tion problem (2) can be rewritten as
e=X
i=1,2wf
i∆f2
i+wc
i∆c⊤
i∆ci. (10)
The proposed iterative solution to equations (4)–(7) fol-
lows the idea of iterative triangulation methods [21, 26].
First, let sk−1=⟨fk−1
1, fk−1
2,ck−1
1,ck−1
2⟩denote the
best current estimate of focal lengths and principal points
after the (k−1)thiteration. The prior values f0
i≡
fp
i,c0
i≡cp
i,i= 1 ,2are used as an initializa-
tion. In the kthiteration the updated estimates sk=
⟨fk
1, fk
2,ck
1,ck
2⟩are obtained by replacing the focal lengths
and principal points ⟨f1, f2,c1,c2⟩on the right-hand side
of equations (8)–(9) by the current best estimates sk−1=
⟨fk−1
1, fk−1
2,ck−1
1,ck−1
2⟩. In this way, we obtain the ex-
pressions for ∆fi=1
wf
i(λ1∂κ1
∂fi(sk−1) +λ2∂κ2
∂fi(sk−1))
and∆ci=1
wc
i(λ1(∂κ1
∂ci(sk−1))⊤+λ2(∂κ2
∂ci(sk−1))⊤),
i= 1 ,2. Here∂κj
∂fi(sk−1)and∂κj
∂ci(sk−1),i, j =
1,2are partial derivatives of κjevaluated at sk−1=
⟨fk−1
1, fk−1
2,ck−1
1,ck−1
2⟩. After this substitution, ∆fiand
∆ciare functions of two Lagrange multipliers λ1andλ2.
Expressions ∆fiand∆ciare, in turn, substituted into
the Kruppa equations (6) and (7). This is done by substi-
tuting ∆fiand∆ci,i= 1,2intoω∗
iusing the relationship
fi= ∆fi+fp
iandci= ∆ci+cp
i.
The updated Kruppa equations (6) and (7), which we de-
noteκk
1= 0 andκk
2= 0, form a system of two equations
of degree four in two unknowns λ1andλ2. This system has
up to 16 real solutions and can be efficiently solved using
the Gr ¨obner basis method [24]. The final solver performs
an elimination of a 20×36matrix and extracts solutions
from the eigenvalues and eigenvectors of a 16×16matrix.
From up to 16 possible solutions, a solution λk
1andλk
2
that minimizes |λ1|+|λ2|is selected [26]. This solution
is used to compute the kthiteration updates ∆fk
iand∆ck
i
using equations (8)–(9) and subsequently the new estimates
of focal lengths and principal points sk=⟨fk
1, fk
2,ck
1,ck
2⟩.
The iterative algorithm stops when the relative change
of error of two consecutive iterations|ek−ek−1|
ek < ϵ, for a
given threshold ϵ. Note that in each iteration, the obtained
solutions satisfy the Kruppa equations κ1= 0andκ2= 0.
This means that for the input fundamental matrix Fand theestimated calibration matrices K1andK2(1), the matrix
K⊤
1FK 2is a valid essential matrix in each iteration.
The situation with f1=f2can be solved using the same
method. In this case, in the cost function (2), as well as in
the Lagrangian and in (4)–(5), we only have i= 1. How-
ever, since we still have two Kruppa equations κ1= 0 and
κ2= 0the final algorithm also relies on solutions to a sys-
tem of two equations of degree four in two unknowns. The
algorithm is summarized in the Supplementary material.
3.2. Real Focal Length Checking within RANSAC
The method proposed in 3.1 is usually applied on a funda-
mental matrix Fthat is obtained using a robust RANSAC-
style estimation [1, 7–9, 11, 16]. Compared to the Boug-
noux formula [2], our method does not suffer from imag-
inary focal length estimates in the presence of noise or an
error in the principal point location. Thus, our method does
not need to check whether the fundamental matrix returned
by the RANSAC is decomposable to real focal lengths or
not. Nevertheless, as we observed in our experiments, fun-
damental matrices that result in imaginary focal lengths
when decompossed with the Bougnoux formula usually do
not provide good estimates and are therefore not selected
inside RANSAC as best models.
Inspired by this observation, we propose a simple modi-
fication of RANSAC that is based on rejecting models that
lead to imaginary focal lengths. Rejection is done before
scoring models on all point correspondences. This reduces
the computational time required to score a model, which is
unlikely to lead to the final output. Thus, this approach is
similar to other degeneracy checking algorithms.
To reject models, we use a modified version of the Boug-
noux formula [2] presented in [32], which has the form of a
ratio of two polynomials in the elements of the fundamental
matrix. The formula returns squared focal lengths. When
the sign of one of the squared focal lengths is negative, we
reject the model. This approach does not require the use
of SVD or other expensive matrix manipulations. This type
of checking can be implemented into different variants of
RANSAC and, as shown later in our experiments, it leads
to reduced computational times while maintaining or even
improving accuracy of pose estimates. Implementation de-
tails can be found in the Supplementary Material (SM).
4. Synthetic Experiments
We perform several synthetic experiments to evaluate vari-
ous aspects of our method and compare it with the state-of-
the-art in controlled configurations. Here, we only include
experiments for two unknown but different focal lengths.
Further experiments for the case where the focal lengths are
assumed to be equal can be found in the SM.
In all our synthetic and real experiments, we set the
weights in (2) to wf
i= 5·10−4andwc
i= 1.0. These
5223
0 2 4 6 8 10
Iterations0.00.20.40.60.81.0Portion of converged samples
102
104
106
108
(a)C(0◦,300)
0 200 400 600 800 1000
Iterations0.00.20.40.60.81.0Portion of converged samples
102
104
106
108
 (b)C(0◦,0)
0 100 200 300 400
Iterations0.00.20.40.60.81.0Portion of converged samples
102
104
106
108
 (c) Phototourism [17]
Figure 1. Plots showing portion of samples for which our algorithm would converge given a threshold for the relative change of errors
in successive iterations|en−en−1|
en < ϵ. For synthetic experiments (a) and (b) we generated 1000 samples with added noise ( σn= 1,
σp= 10 ). We set the priors as fp
1= 660 , fp
2= 440 . For (c) we used the Phototourism benchmark dataset [17]. See Section 5 for details.
weights were obtained by testing different combinations on
the Brandenburg Gate validation scene from the Phototur-
ism dataset [17] and resulted in the best performance on
this scene. However, as we show also in synthetic exper-
iments, for a wide variety of combinations of weights, we
consistently obtain very good estimates and outperform the
state-of-the-art methods. We set the weights for Hartley and
Silpa-Anan’s method [15] in the same manner to 1.0 for wF,
10−4forwcand10−6forwf.
Camera Setup: To perform the experiments, we set up a
scene with two cameras. We assume 640×480image
size and principal points in image centers. The first cam-
era has focal length f1= 600 and the second f2= 400 .
The first camera has center at the origin with the z-axis of
the world coordinate system being its principal axis. We
perform experiments with the second camera in different
configurations denoted as C(θ, y). ForC(0◦, y)the second
camera center is located at coordinates (1200 , y,600) . The
camera is rotated by 60◦around its y-axis. With θ̸= 0◦we
additionally rotate the second camera around the x-axis by
θ. When y= 0 andθ= 0◦the principal axes of the two
cameras intersect and are thus coplanar, which is a degener-
ate configuration for the Bougnoux formula [2].
For a given camera setup, we uniformly generate 100
random points visible by both cameras. After projecting
the points into both images, we add Gaussian noise with
a standard deviation σnto the pixel coordinates. We also
emulate errors in the assumed locations of the principal
points by shifting them cerrpixels in random direction from
the ground truth. The shift cerris either a fixed value or
sampled from a normal distribution with a standard devi-
ation of σc. We estimate the fundamental matrix using
MAGSAC++ [1] as implemented in OpenCV [3].
Convergence: In the first experiment, we evaluate con-
vergence of the proposed iterative method. Different ter-
mination criteria can be considered. We terminate the al-
gorithm when the relative change in error in two succes-
sive iterations|en−en−1|
en is less than a predetermined thresh-
old. Fig. 1a shows that for non-degenerate configurationour method converges quickly even with strict thresholds.
Fig. 1b shows the convergence rates of our method when the
principal axes are coplanar. The degenerate configuration
requires an increased number of iterations to converge with
the same thresholds. However, even in a degenerate config-
uration, the method converges in a few iterations for most
of the samples, even with strict thresholds. We also evalu-
ate convergence on a large scale real-world dataset [17] (see
section 5 for more details). As shown in Fig. 1c, our method
successfully converges for most of the samples even with a
strict threshold. Using these findings, we established the
maximum number of iterations for our method at 50, as fur-
ther iterations demonstrated diminishing returns in terms of
convergence rate.
Accuracy of the Estimated Focal Lengths In the next
experiment, we compare our method with state-of-the-art
methods in terms of accuracy of the estimated focal lengths.
The methods considered are the baseline Bougnoux for-
mula [2] and the iterative approaches of Hartley and Silpa-
Anan [15] and Fetzer et al. [10]. In the experiments, we use
the same priors for Hartley and Silpa-Anan’s method as for
ours. We set the initial values for LM optimization for the
method by Fetzer et al. to the same values as our priors.
Fig. 2a and 2b show how the transition to a degenerate
configuration with coplanar principal axes affects the accu-
racy. Our method performs well even close to and in de-
generate configurations. In comparison, the other methods
fail to provide consistent results when the camera position
approaches the degenerate configuration.
Fig. 2c shows that an increase in cerrresults in a worsen-
ing performance of the Bougnoux formula and the method
by Fetzer et al . Our method is capable of compensating
for the error in the principal point as it is not assumed
to be known exactly. The performance of Hartley and
Silpa-Anan’s method stays consistent for similar reasons,
although it performs worse than ours.
In Fig. 2d, we show the performance of the methods
under varying levels of added image noise. Our method
demonstrates higher robustness to noise, resulting in re-
5224
Ours Hartley and Silpa-Anan[15] Fetzer et al. [10] Bougnoux[2] GTf1 Priorf1
(15°, 0)
(10°, 0)
(5°, 0)
(3°, 0)
(2°, 0)
(1°, 0)
(0°, 0)
Camera Configuration0200400600800100012001400Estimated f 1
(a)
(0°, 300)
(0°, 200)
(0°, 100)
(0°, 50)
(0°, 25)
(0°, 0)
Camera Configuration0200400600800100012001400Estimated f 1
 (b)
0 5 10 20 50 100 200
cerr0200400600800100012001400Estimated f 1
 (c)
0.01 0.1 0.5 1.0 2.0 5.0
n
0200400600800100012001400Estimated f 1
(d)
300 480 540 600 660 720 780 900 1200No Prior
f1Prior0200400600800100012001400Estimated f 1
 (e)
1e-05 0.0001 0.0005 0.001 0.01 0.1 1.0
wf/wc0200400600800100012001400Estimated f 1
 (f)
Figure 2. Synthetic experiments: Box plots for the estimated focal lengths of the first camera. Comparison of the methods as (a, b) the
camera configuration approaches the degeneracy with coplanar principal axes, (c) we vary the error in principal point cerr, (d) we vary the
noise added to the projected points, (e) we vary the prior for f1, (f) we vary the relative weights of focal length and principal point priors.
We use priors fp
1= 700 , fp
2=f2= 400 for (a, b, c, d), fp
1= 1200 , fp
2= 400 for (f), σn= 1for (a, b, c, e, f), σc= 10 for (a, b, d, e, f).
For (c, d, e, f) we randomly sample the configuration C(θ, y)withθ∈[−15◦,15◦]andy∈[−200,200].
duced inter-sample deviation and a more consistent median
estimate across the various noise levels. Fig. 2e shows how
the different methods respond to the priors for focal length,
or in the case of the method by Fetzer et al. to an initializa-
tion for the iterative process. Fig. 2f shows how the setting
of weights for our and Hartley and Silpa-Anan’s method af-
fects the accuracy of the estimated focal lengths.
5. Real-world Experiments
Datasets: We assess the real-world performance of our
method using two extensive datasets. The first dataset, re-
ferred to as the Phototourism dataset , was originally in-
troduced in [17] to serve as a robust benchmark to evalu-
ate structure-from-motion pipelines. The dataset contains
25 scenes of landmarks. Large sets of crowd-sourced im-
ages are available for each scene. For 133of the scenes
a COLMAP [36] reconstruction is provided to serve as
ground truth for camera intrinsics and extrinsics. To evalu-
ate the estimated focal lengths, we randomly sample 1000
pairs of images for each scene. We only consider pairs with
sufficient co-visibility as defined in [17]. We used the Bran-
denburg Gate scene to serve as a validation set to determine
the optimal setting of the weights wf
iandwc
iin (2).
The second dataset, known as the Aachen Day-Night
v1.1 Dataset , is an extension of the previously established
3Originally, reconstructions were available for 16 of the scenes, but 3 of
the scenes were later dropped from the dataset due to data inconsistencies.
101102
Mean Runtime (ms)0.480.500.520.540.560.580.600.620.64mAA p(10°)
VSAC2:
MAGSAC++
PRO+MAGSAC++
PoseLib:
LO-RANSAC
PRO+LO-RANSACOpenCV USAC:
Default
MAGSAC++
Accurate
PROSAC
Original
RFCFigure 3. The plot shows mAA pachieved on the Phototourism
dataset [17] for different RANSAC implementations [3, 16, 23]
with and without real focal length checking (RFC) under varying
number of iterations. To calculate the poses we use the ground
truth focal lengths. Further details are provided in the SM.
Aachen Day-Night dataset [33, 35]. This dataset incorpo-
rates a reference COLMAP [36] model that has been re-
constructed from a comprehensive collection of 6,697 im-
ages captured by a variety of cameras and smartphones. For
evaluation, we randomly sample 1000 pairs of images that
adhere to the same co-visibility criteria as those defined for
the Phototourism dataset. COLMAP ground truth can intro-
duce some bias; however, it is obtained using optimization
5225
Ours Hartley and Silpa-Anan[15] Fetzer et al. [10] Bougnoux[2] Sturm [41] Minimal 6-point [40] Prior RFC Original
0.0 0.2 0.4 0.6 0.8 1.0
Focal length error ferr
i0.00.20.40.60.81.0Portion of samples
(a) Phototourism [17]
0.0 0.2 0.4 0.6 0.8 1.0
Focal length error ferr
i0.00.20.40.60.81.0Portion of samples
 (b) Aachen Day-Night v1.1 [47]
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
Focal length error ferr
i0.00.20.40.60.81.0Portion of samples
 (c) ETH3D Multiview [37]
Figure 4. Plots showing the portion of samples for which the estimated focal lengths were below a given ferr
ithreshold. For (a) and (b)
we assume different focal lengths for the two cameras, for (c) the focal lengths are assumed to be equal.
Phototourism [17] Aachen Day-Night v1.1 [47]
Method RFCMedian mAA p Median mAA f Median mAA p Median mAA f
perr 10◦20◦ferr 0.1 0.2 perr 10◦20◦ferr 0.1 0.2
Ours6.46◦40.37 55.90 0.149 23.95 37.29 9.53◦27.55 46.17 0.232 12.46 23.87
✓ 6.31◦40.97 56.70 0.148 24.23 37.57 8.78◦29.30 48.11 0.214 12.73 24.97
Hartley [15]9.19◦30.15 46.94 0.250 12.73 23.39 12.19◦21.10 38.74 0.336 6.37 12.68
✓ 9.00◦30.61 47.69 0.244 12.88 23.67 11.37◦22.12 40.61 0.312 7.19 14.19
Fetzer [10]9.40◦33.00 47.25 0.238 19.53 29.89 12.33◦24.34 39.69 0.305 11.47 21.50
✓ 8.94◦33.64 48.51 0.221 19.93 30.66 10.66◦25.62 42.16 0.263 11.51 22.63
Bougnoux [2]7.55◦37.17 52.70 0.215 20.65 31.47 10.25◦26.73 45.07 0.295 11.37 21.64
✓ 7.39◦37.57 53.21 0.200 21.05 32.26 9.69◦27.25 45.61 0.259 11.78 22.96
Prior11.17◦22.63 41.98 0.246 8.83 19.81 13.00◦17.06 37.27 0.242 4.11 6.55
✓ 11.05◦22.73 42.31 0.246 8.83 19.81 12.73◦17.68 38.05 0.242 4.11 6.55
GT intrinsics2.85◦59.78 70.80— — —5.45◦45.99 58.66— — —✓ 2.82◦60.05 71.18 4.25◦51.76 64.82
Table 1. Median errors for poses ( perr) and focal lengths ( ferr
i) and mean average accuracy scores for poses (mAA p) and estimated focal
lengths (mAA f) on 12 scenes from the Phototourism dataset [17] and the Aachen Day-Night v1.1 dataset [47]. RFC denotes the real focal
length checking as described in subsection 3.2.
(BA) over many images. This is widely accepted for gener-
ating ground truth and is expected to be more precise than
the estimates obtained from pairs of images.
Metrics: To evaluate the accuracy of the output focal
lengths, we define relative focal length error as
ferr
i=|fest
i−fgt
i|
fgt
i, (11)
withfest
iandfgt
idenoting the estimated and ground truth
focal lengths respectively, with i∈ {1,2}denoting the first
or the second camera. To gauge the accuracy of the poses,
we employ the Mean Average Accuracy (mAA) metric, as
originally proposed in [17]. The basis for this metric is the
pose error (perr)which is calculated as the maximum of the
rotation and translation error in degrees.
Furthermore, we employ the mAA metric to evaluate and
compare the accuracy of the estimated focal lengths, based
on the ferr
icurves as presented in Figure 4. To distinguish
between the two applications of the mAA metric, we denote
them as mAA pfor pose and mAA ffor focal lengths.Fundamental Matrix Estimation: We use LoFTR [43]
to obtain point correspondences. We perform inference
on images resized so that the largest dimension is equal
to 1024 px. To estimate the fundamental matrix, we use
MAGSAC++ [1] with the epipolar threshold set to 3 px.
We also perform experiments with and without utilizing
real focal length checking (RFC) as described in subsec-
tion 3.2. Fig 3 shows that on the Phototourism dataset [17]
the fundamental matrices obtained with RFC generally lead
to more accurate pose estimates, while also reducing com-
putational times across various RANSAC implementations.
More details and further experiments are provided in SM.
Compared Methods: We compare our method with three
state-of-the-art approaches. We evaluate the Bougnoux for-
mula [2], Hartley and Silpa-Anan’s iterative method [15]
and the iterative method by Fetzer et al. [10].
For all images, we consider the principal point to lie in
their center which is a commonly used assumption. With
the exception of the Bougnoux formula, all methods require
priors, or initial estimates, for the focal lengths. Following
common practice, as established in COLMAP [36], we ini-
5226
Method Library RFC Mean Time (ms) Mean iterations
OursMatlab22.42 24.33
✓ 14.95 22.53
Eigen (C++)3.59 23.63
✓ 3.44 22.49
Hartley [15] PyTorch [30]659.61 85.38
✓ 663.98 86.14
Fetzer [10] PyTorch [30]197.89 0.00
✓ 191.75 40.56
Bougnoux [2] NumPy [13]0.27 —
✓ 0.25 —
Table 2. Mean computation times and number of iterations for the
compared methods on the Phototourism dataset [17]. Measure-
ments were performed on Intel i7-11800H. The times shown do
not include the estimation of the fundamental matrices.
MethodMedian mAA p Median mAA f
perr 10◦20◦ferr 0.1 0.2
Ours 16.51◦16.70 32.24 0.299 12.96 23.10
Hartley [15] 21.52◦12.38 24.26 1.090 0.45 0.99
Fetzer [10] 70.40◦2.31 5.20 0.999 6.86 11.64
Sturm [41] 17.98◦20.64 33.30 0.582 10.39 17.55
Minimal [40] 36.18◦16.42 25.74 0.291 14.43 24.29
Prior 24.60◦11.24 20.92 1.119 0.00 0.00
GT intrinsics 6.40◦41.73 54.16 — — —
Table 3. Mean average accuracy scores for poses (mAA p) and
estimated focal lengths (mAA f) on 12 scenes from the ETH3D
High Resolution Multiview dataset [37]. The focal lengths of both
cameras are assumed to be equal. The last row includes reference
results for pose accuracy when ground truth intrinsics are used.
tialize the priors for the focal lengths at 1.2times the maxi-
mum dimension of the image This initial setting implies an
approximate 50-degree field-of-view for the cameras.
Results: The median errors and mean average accuracy
scores for both pose and focal lengths are reported in Ta-
ble 1. On both datasets, our method significantly outper-
forms others in terms of the accuracy of the estimated focal
lengths. The superiority of our method in terms of focal
length accuracy can also be seen in Fig. 4a and 4b. In terms
of pose accuracy, our method also provides more accurate
results than all the compared methods.
The results also show that performing RFC for real fo-
cal lengths within RANSAC as proposed in subsection 3.2
generally leads to improvements in terms of both the pose
and focal length accuracy across the compared methods.
Computational Speed: Table 2 shows the mean compu-
tational times for the various methods. Our method is sig-
nificantly faster than other iterative methods. Note that the
implementation of our method is not optimized and there is
still some space for speed-up.
Equal Focal Lengths: We also evaluate our method for
the case when both focal lengths can be assumed to be
equal. In this case, we use the modified version of our it-
erative algorithm from Section 3.1. To perform the evalua-tion, we use the ETH3D Multiview dataset [37]. We use all
image pairs satisfying the co-visibility criterion for each of
the 12 scenes from the test set, yielding 3830 total pairs.
The dataset combines laser scans and photometric errors
to obtain the ground truth. For comparison, we modified
the method by Fetzer et al. to optimize for a single focal
length. We use Hartley and Silpa-Anan’s method as usual,
but on output we average the two focal lengths produced.
Instead of the Bougnoux formula, we use the formula pro-
posed by Sturm [41] which is specific to the case of equal
focal lengths. We also evaluate the minimal 6-point algo-
rithm [40] implemented within LO-RANSAC [8, 23]. Fur-
ther details and additional experiments are available in SM.
The results are shown in Table 3 and Fig. 4c. Our method
provides superior results to other methods based on the de-
composition of the fundamental matrix, and performs com-
parably to the minimal solver [40] in terms of estimated
focal lengths and shows minor improvements in terms of
pose accuracy. Our method thus provides a good alterna-
tive to the 6-point minimal solver in the case when the focal
lengths of the two cameras are known to be equal.
6. Conclusion
We address the important problem of robust self-calibration
of the focal lengths of two cameras from a given fundamen-
tal matrix. Our new efficient iterative method shows sub-
stantial improvements over existing techniques. Synthetic
experiments show that our method performs reliably even
when the cameras are in or close to degenerate configura-
tions. In real-world evaluations on two large-scale datasets,
our approach demonstrates superior accuracy in terms of
estimated poses and focal lengths, even when we use inac-
curate initial priors, while also being faster than competing
iterative approaches.
We have additionally proposed to perform a compu-
tationally simple check within the standard 7-point al-
gorithm to remove models that lead to imaginary fo-
cal lengths. Real-world experiments show that this ap-
proach not only decreases computational time of the whole
RANSAC pipeline, but, in general, also leads to improved
pose and focal length accuracy across multiple RANSAC
variants and implementations.
Acknowledgments: V . K. was supported by the Slovak Grant
Agency for Science (VEGA), project no. 1/0373/23. and the
TERAIS project, a Horizon-Widera-2021 program of the Euro-
pean Union under the Grant agreement number 101079338. The
results were obtained using the computational resources procured
in the project National competence centre for high performance
computing (project code: 311070AKF2) funded by European Re-
gional Development Fund, EU Structural Funds Informatization of
society, Operational Program Integrated Infrastructure. Z. K. was
supported by the Czech Science Foundation (GA ˇCR) JUNIOR
STAR Grant No. 22-23183M.
5227
References
[1] Daniel Barath, Jana Noskova, Maksym Ivashechkin, and Jiri
Matas. Magsac++, a fast, reliable and accurate robust estima-
tor. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 1304–1312, 2020. 2,
4, 5, 7
[2] Sylvain Bougnoux. From projective to euclidean space under
any practical situation, a criticism of self-calibration. In Sixth
International Conference on Computer Vision , pages 790–
796. IEEE, 1998. 1, 2, 3, 4, 5, 6, 7, 8
[3] Gary Bradski. The opencv library. Dr. Dobb’s Journal: Soft-
ware Tools for the Professional Programmer , 25(11):120–
123, 2000. 2, 5, 6
[4] Michael J Brooks, Lourdes de Agapito, Du Q Huynh, and
Luis Baumela. Towards robust metric reconstruction via a
dynamic uncalibrated stereo head. Image and Vision Com-
puting , 16(14):989–1002, 1998. 2
[5] Martin Bujnak, Zuzana Kukelova, and Tomas Pajdla. 3d re-
construction from image collections with a single known fo-
cal length. In 2009 IEEE 12th International Conference on
Computer Vision , pages 1803–1810. IEEE, 2009. 2
[6] Robert Castle, Georg Klein, and David W. Murray. Video-
rate localization in multiple maps for wearable augmented
reality. In ISWC , 2008. 1
[7] Ondrej Chum and Jiri Matas. Matching with prosac-
progressive sample consensus. In 2005 IEEE computer so-
ciety conference on computer vision and pattern recognition
(CVPR’05) , pages 220–226. IEEE, 2005. 2, 4
[8] Ond ˇrej Chum, Ji ˇr´ı Matas, and Josef Kittler. Locally opti-
mized ransac. In Pattern Recognition: 25th DAGM Sympo-
sium, Magdeburg, Germany, September 10-12, 2003. Pro-
ceedings 25 , pages 236–243. Springer, 2003. 8
[9] Ondrej Chum, Tomas Werner, and Jiri Matas. Two-view
geometry estimation unaffected by a dominant plane. In
2005 IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition (CVPR’05) , pages 772–779.
IEEE, 2005. 2, 4
[10] Torben Fetzer, Gerd Reis, and Didier Stricker. Stable in-
trinsic auto-calibration from fundamental matrices of devices
with uncorrelated camera parameters. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision , pages 221–230, 2020. 1, 2, 3, 5, 6, 7, 8
[11] Martin A Fischler and Robert C Bolles. Random sample
consensus: a paradigm for model fitting with applications to
image analysis and automated cartography. Communications
of the ACM , 24(6):381–395, 1981. 2, 4
[12] Riccardo Gherardi and Andrea Fusiello. Practical autocal-
ibration. In Computer Vision–ECCV 2010: 11th European
Conference on Computer Vision, Heraklion, Crete, Greece,
September 5-11, 2010, Proceedings, Part I 11 , pages 790–
801. Springer, 2010. 2
[13] Charles R. Harris, K. Jarrod Millman, St ´efan J. van der Walt,
Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric
Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith,
Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van
Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern ´andezdel R ´ıo, Mark Wiebe, Pearu Peterson, Pierre G ´erard-
Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser,
Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant.
Array programming with NumPy. Nature , 585(7825):357–
362, 2020. 8
[14] R. Hartley and A. Zisserman. Multiple View Geometry in
Computer Vision . Cambridge, 2nd edition, 2003. 2
[15] Richard Hartley, Chanop Silpa-Anan, et al. Reconstruction
from two views using approximate calibration. In Proc. 5th
Asian Conf. Comput. Vision , pages 338–343, 2002. 1, 2, 3,
5, 6, 7, 8
[16] Maksym Ivashechkin, Daniel Barath, and Ji ˇr´ı Matas. Vsac:
Efficient and accurate estimator for h and f. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 15243–15252, 2021. 2, 4, 6
[17] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,
Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Image
matching across wide baselines: From paper to practice.
International Journal of Computer Vision , 129(2):517–547,
2021. 2, 5, 6, 7, 8
[18] Kenichi Kanatani and Chikara Matsunaga. Closed-form ex-
pression for focal lengths from the fundamental matrix. In
Proc. 4th Asian Conf. Comput. Vision , pages 128–133. Cite-
seer, 2000. 1, 2
[19] Kenichi Kanatani, Atsutada Nakatsuji, and Yasuyuki Sug-
aya. Stabilizing the focal length computation for 3-d recon-
struction from two uncalibrated views. International Journal
of Computer Vision , 66:109–122, 2006. 2
[20] Yasushi Kanazawa, Yasuyuki Sugaya, and Kenichi Kanatani.
Initializing 3-d reconstruction from three views using three
fundamental matrices. In Image and Video Technology–
PSIVT 2013 Workshops: GCCV 2013, GPID 2013, PAES-
NPR 2013, and QACIVA 2013, Guanajuato, Mexico, Octo-
ber 28-29, 2013, Revised Selected Papers 6 , pages 169–180.
Springer, 2014. 2
[21] Zuzana Kukelova and Viktor Larsson. Radial distortion tri-
angulation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 9681–
9689, 2019. 4
[22] Zuzana Kukelova, Martin Bujnak, and Tomas Pajdla. Auto-
matic generator of minimal problem solvers. In Computer
Vision–ECCV 2008: 10th European Conference on Com-
puter Vision, Marseille, France, October 12-18, 2008, Pro-
ceedings, Part III 10 , pages 302–315. Springer, 2008. 3
[23] Viktor Larsson and contributors. PoseLib - Minimal Solvers
for Camera Pose Estimation, 2020. 2, 6, 8
[24] Viktor Larsson, Kalle Astrom, and Magnus Oskarsson. Effi-
cient solvers for minimal problems by syzygy-based reduc-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 820–829, 2017. 2, 3,
4
[25] Kenneth Levenberg. A method for the solution of certain
non-linear problems in least squares. Quarterly of applied
mathematics , 2(2):164–168, 1944. 2
[26] Peter Lindstrom. Triangulation made easy. In 2010 IEEE
Computer Society Conference on Computer Vision and Pat-
tern Recognition , pages 1554–1561. IEEE, 2010. 4
5228
[27] Manolis IA Lourakis and Rachid Deriche. Camera self-
calibration using the Kruppa equations and the SVD of the
fundamental matrix: The case of varying intrinsic parame-
ters. PhD thesis, INRIA, 2000. 1, 2, 3
[28] Quan-Tuan Luong and Olivier D Faugeras. The fundamental
matrix: Theory, algorithms, and stability analysis. Interna-
tional journal of computer vision , 17(1):43–75, 1996. 1
[29] Nikos Melanitis and Petros Maragos. A linear method for
camera pair self-calibration. Computer Vision and Image
Understanding , 210:103223, 2021. 1, 2
[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zem-
ing Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-
son, Andreas Kopf, Edward Yang, Zachary DeVito, Mar-
tin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:
An imperative style, high-performance deep learning library.
InAdvances in Neural Information Processing Systems 32 ,
pages 8024–8035. Curran Associates, Inc., 2019. 8
[31] Marc Pollefeys, Reinhard Koch, and Luc Van Gool. Self-
calibration and metric reconstruction inspite of varying and
unknown intrinsic camera parameters. International journal
of computer vision , 32(1):7–25, 1999. 2
[32] Oleh Rybkin. Robust focal length estimation. Bachelor’s
thesis, Czech Technical University in Prague, 2017. 4
[33] Torsten Sattler, Tobias Weyand, Bastian Leibe, and Leif
Kobbelt. Image Retrieval for Image-Based Localization Re-
visited. In British Machine Vision Conference (BMCV) ,
2012. 6
[34] Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Efficient
& effective prioritized matching for large-scale image-based
localization. IEEE Trans. Pattern Anal. Mach. Intell. , 39(9):
1744–1756, 2017. 1
[35] Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii,
Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi
Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and
Tomas Pajdla. Benchmarking 6DOF Outdoor Visual Local-
ization in Changing Conditions. In Conference on Computer
Vision and Pattern Recognition (CVPR) , 2018. 6
[36] Johannes Lutz Sch ¨onberger and Jan-Michael Frahm.
Structure-from-motion revisited. In Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2016. 6, 7
[37] Thomas Schops, Johannes L Schonberger, Silvano Galliani,
Torsten Sattler, Konrad Schindler, Marc Pollefeys, and An-
dreas Geiger. A multi-view stereo benchmark with high-
resolution images and multi-camera videos. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 3260–3269, 2017. 7, 8
[38] Noah Snavely, Steven M Seitz, and Richard Szeliski. Mod-
eling the world from internet photo collections. IJCV , 80(2):
189–210, 2008. 1
[39] Jakub Sochor, Roman Jur ´anek, Jakub ˇSpaˇnhel, Luk ´aˇs
Marˇs´ık, Adam ˇSirok `y, Adam Herout, and Pavel Zem ˇc´ık.
Comprehensive data set for automatic single camera visual
speed measurement. IEEE Transactions on Intelligent Trans-
portation Systems , 20(5):1633–1643, 2018. 1[40] Henrik Stew ´enius, David Nist ´er, Fredrik Kahl, and Frederik
Schaffalitzky. A minimal solution for relative pose with un-
known focal length. Image and Vision Computing , 26(7):
871–877, 2008. 2, 7, 8
[41] Peter Sturm. On focal length calibration from two views. In
Proceedings of the 2001 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition. CVPR 2001 ,
pages 145–150. IEEE, 2001. 2, 7, 8
[42] Peter Sturm, ZL Cheng, Peter CY Chen, and Aun Neow Poo.
Focal length calibration from two views: method and anal-
ysis of singular cases. Computer Vision and Image Under-
standing , 99(1):58–95, 2005. 2
[43] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and
Xiaowei Zhou. Loftr: Detector-free local feature matching
with transformers. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
8922–8931, 2021. 7
[44] Magdalena Urbanek, Radu Horaud, and Peter Sturm. Com-
bining off-and on-line calibration of a digital camera. In
Proceedings Third International Conference on 3-D Digital
Imaging and Modeling , pages 99–106. IEEE, 2001. 2
[45] Bin Xu and Zhenzhong Chen. Multi-level fusion based 3d
object detection from monocular images. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 2345–2353, 2018. 1
[46] Zhengyou Zhang. A flexible new technique for camera cali-
bration. IEEE Transactions on pattern analysis and machine
intelligence , 22(11):1330–1334, 2000. 1
[47] Zichao Zhang, Torsten Sattler, and Davide Scaramuzza. Ref-
erence pose generation for long-term visual localization via
learned features and view synthesis. International Journal of
Computer Vision , 129:821–844, 2021. 2, 7
5229
