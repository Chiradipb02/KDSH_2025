TexTile: A Differentiable Metric for Texture Tileability
Carlos Rodriguez-Pardo1Dan Casas1Elena Garces1,2Jorge Lopez-Moreno1,2
1Universidad Rey Juan Carlos, Spain2SEDDI, Spain
SSIM LPIPS DISTS BRISQUE TexTile
Figure 1. Existing perceptual metrics, commonly used to evaluate texture synthesis algorithms, typically fail to account for tileability.
Such weakness is depicted in this figure where, for each column, we show tiled versions of textures with (top) and without (bottom) tiling
artifacts. Foreachcolumn,wehighlightusingsaturatedcolordotsthepreferredimage( i.e.,higherscore)accordingtodifferentmetrics. It
canbeseenthatthereisnocorrelationacrossexistingmethods( i.e.,saturateddotsdistributedovertopandbottomrows),whileourmethod
TexTile consistently prefers seamless tiled textures ( i.e.,saturated green dots on the bottom for all columns).
Abstract
We introduce TexTile, a novel differentiable metric to
quantifythedegreeuponwhichatextureimagecanbecon-
catenatedwithitselfwithoutintroducingrepeatingartifacts
(i.e., the tileability). Existing methods for tileable texture
synthesis focus on general texture quality, but lack explicit
analysis of the intrinsic repeatability properties of a tex-
ture. Incontrast,ourTexTilemetriceffectivelyevaluatesthe
tileablepropertiesofatexture,openingthedoortomorein-
formedsynthesisandanalysisoftileabletextures. Underthe
hood, TexTile is formulated as a binary classifier carefully
built from a large dataset of textures of different styles, se-
mantics, regularities, and human annotations. Key to our
method is a set of architectural modifications to baseline
pre-train image classifiers to overcome their shortcomings
at measuring tileability, along with a custom data augmen-
tation and training regime aimed at increasing robustness
and accuracy. We demonstrate that TexTile can be plugged
into different state-of-the-art texture synthesis methods, in-
cludingdiffusion-basedstrategies,andgeneratetileabletex-
tures while keeping or even improving the overall texture
quality. Furthermore, we show that TexTile can objectivelyevaluate any tileable texture synthesis method, whereas the
currentmixofexistingmetricsproducesuncorrelatedscores
which heavily hinders progress in the field.
1. Introduction
The appearance of 3D digital objects plays a fundamental
role in the overall realism of a virtual environment. To
create realistic textures, many strategies have been widely
explored, including procedural algorithms [19, 28], scan-
ning [22, 45] and, more recently, text-to-image generative
pipelines[7‚Äì9]. Amongthedifferentpropertiesthatwewish
for the synthesized textures ( e.g., photorealism, variety in
detail, high resolution), the ability to seamlessly repeat or
tileitselfwithoutnoticeableartifacts‚Äì‚Äìitstileability‚Äì‚Äìises-
peciallyimportantinthefrequentcaseofapplyingatexture
to a large surface. For example, when texturing the facade
of a building or a field of grass.
Many methods exist that focus on the specific case of
tileabletexturesynthesis[1,11,36,45,48,53,54]. Thishas
beenachieved,forexample,bymanipulatingimageborders
[36],maximizingstationaryimageproperties[45],orcondi-
tioninggenerativemodelsonstructuredpatterns[74]. How-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
4439
ever, despite such significantly diverse methodologies used
in existing methods, they typically rely on evaluation us-
ingcommonmetricsbasedongeneraltexturequalitywhich,
unfortunately, do not explicitly account for the intrinsic re-
peatability properties of a texture.
To address this shortcoming, we introduce TexTile, a
novel metric for texture tileability. TexTile is a data-driven
metric that brings two key novel functionalities into tex-
ture synthesis methods: first, it computes a human-friendly
scorethatcapturestheintrinsicrepeatabilityofanytexture;
and second, it provides a differentiable metric that can be
usedasanadditionaldataterminanylearning-basedortest-
timeoptimizationmethodfortileabletexturesynthesis. We
demonstratethatTexTileenablesafactualanalysisofstate-
of-the-art methods for tileable texture synthesis, while pre-
viousmetricsoftenresultinuncorrelatedevaluations( i.e.,a
good tileable texture might have low SSIM [66] score, or a
poortileabletexturemighthaveahighSSIM),asillustrated
in Figure 1. Furthermore, we demonstrate that TexTile can
be used off-the-shelf as an additional loss term in state-of-
the-art methods for texture synthesis, including diffusion-
based models, to output tileable textures while preserving
or even improving the overall image quality.
Under the hood, we formulate TexTile as a binary clas-
sifier built using a carefully designed architecture with an
attention-enhanced convolutional network. The convolu-
tionalfilterscandetectlocaldiscontinuities‚Äìwhicharecom-
mon in borders that are not seamlessly tileable‚Äì and can
deal with images of arbitrary sizes, but they struggle with
global understanding to detect artifacts and repeating pat-
terns. Therefore,weintroduceSelf-Attentionlayersintoour
architecture,whichisknowntocaptureaglobalunderstand-
ing of the input. This, combined with a custom data aug-
mentation policy designed for tileability detection, enables
us to train a novel neural classifier to unleash a new func-
tionality for the state-of-the-art texture synthesis methods.
In summary, we introduce the following contributions:
‚Ä¢Anovellearning-basedmetricfortextureanalysisthatac-
curately quantifies tileability.
‚Ä¢An attention-enhanced convolutional classifier, and a
training configuration aimed at maximizing robustness
and accuracy.
‚Ä¢A differentiable loss function which can be plugged into
texturesynthesisalgorithmstogeneratetileabletextures.
‚Ä¢Open-sourcecodeandtrainedweightsforourmetric. We
believethiswillopenthedoortoquantitativebenchmarks
ontileabletextures,whichiscurrentlynotpossibledueto
the lack of a specific metric for such task.
+
ConvNext Block
ùëìatt+
Self-Attention‚àó
ùúÜInference ( Sec 3.3)
 Input ImageŒô Œôtiled
Model ‚Ñ≥ TexTile
‚ÑíTexTile Synthesis
TexTile  as Loss Function (Sec 5.1)Model Design (Sec 3.2)Figure 2. Our model takes as input a texture image I, which we
tile to form Itiled, and returns an estimation of its tileability. This
metriccanbeusedasalossfunction Óà∏TexTiletoallowsynthesisal-
gorithms to generate tileable textures. Our model, Óàπarchitecture
is comprised of ConvNext [39] and residual self-attention blocks.
2. Related Work
2.1. Image Quality Assessment
Image Quality Assessment (IQA) algorithms can be cat-
egorized into three different groups. Reference-based
IQA methods compare an input and a reference image,
which is the most widely studied strategy for IQA. These
methods have traditionally leveraged pixel-wise differences
(e.g.,PSNR,ùìÅ1orùìÅ2distances) or image statistics ( e.g.,
SSIM [66] or FSIM [69]) to compute the similarity be-
tweentheimages. Neuralreference-basedIQA,whichlever-
age the stronger correlation of deep neural networks with
human perception [71], have been also proposed. These
strategieseitherleveragefeaturesfromuntrained[3]orpre-
trainedconvolutionalneuralnetworks[20],orusedirectsu-
pervision from human judgments, as in LPIPS [71], PIE-
APP [51], DISTS [14], Si-FID [56], or DreamSIM [18].
These methods introduce powerful and differentiable met-
rics, however, they require a reference image and are thus
notsuitableformeasuringglobalproperties,liketileability.
Instead of comparing an input and a reference image,
distribution-based IQA methods compare statistics of two
setsofreferenceimagesandgeneratedimages. Thesemeth-
ods are commonly used to evaluate the perceptual qual-
ity of generative models, typically using metrics based on
neural networks activations [6, 27, 57, 67], nearest neigh-
bors [33], spectral [61], or geometric distances [31]. While
these methods are useful for evaluating the performance of
generative models, they struggle as loss functions [57], and
also require reference images.
Finally,no-reference IQA methods compute the over-
all quality score of an input image without requiring an ex-
plicit reference. These methods rely on image statistics,
as in BIQI [44] amd BRISQUE [43]; or on training deep
neuralnetworksonhumanjudgmentsofimagequality,like
HyperIQA [59], MANIQA [68], VCRNet [49], or CLIP-
IQA [64]. Despite competitive results in image quality as-
sessment, these methods do not incorporate tileabilty anal-
ysis. SeamlessGAN [53] leverages the discriminator of a
single-image generative model to find artifacts in the bor-
dersofgeneratedtextures. However,thediscriminatoronly
4440
measuresseamlessness,ignoringotherfactorsthatinfluence
tileabilityand,mostimportantly,itcannotgeneralizetoany
image outside of its single-image dataset.
Our metric is most closely related to no-reference IQA
methods, as it takes a single image as input. However, in
contrast to existing methods, we assess the image quality
based on tilebility instead of general image quality. We
demonstrate that, when combined with existing IQA met-
rics, our metric successfully captures overall quality and
tileability. To the best of our knowledge, our metric is the
first no-reference tileability metric.
2.2. Tileable Texture Synthesis
Non-parametric tileable texture synthesis methods gener-
ate new tileable images by maximizing image stationar-
ity[45], by manipulating the images with border transfor-
mations using Graphcuts [36], or by patch-based synthe-
sis with histogram-preserving blending [11]. Parametric
alternatives typically leverage deep neural networks in di-
verse ways. Rodriguez-Pardo et al. [54] look for repeating
patterns in images using deep features in pretrained CNNs,
then synthesize tileable images by blending the borders.
Tileability can be also achieved with specific image param-
eterizations and neural network design, as in Neural Cellu-
lar Automata [48], or in the Periodic Spatial GAN [5]. By
manipulating latent spaces in pre-trained GANs, Seamless-
GAN [53] achieve tileability without specific model modi-
fications. Tileable image generation has also been explored
with the goal of material capture, by means of models con-
ditioned on structured patterns [74], or through rolled dif-
fusion[62]. Specialized methods have been proposed for
tileable vector image generation [1]. For a review on tex-
ture synthesis, we refer the reader to the survey in [2].
These methods typically provide quantitative evaluation
using reference-based IQA. As mentioned in Section 2.1,
these metrics measure the perceptual similarity between
generated and input images, however, they do not account
fortiling. Tothebestofourknowledge,thereisnoavailable
metric that can be used to compare these methods in terms
oftileability,hinderingtheprogressofthefieldandlimiting
evaluation to qualitative analyses. Our metric aims to solve
this gap. Being fully differentiable, it can be leveraged as a
lossfunctiontoenableexistingsynthesisalgorithmstopro-
duce seamlessly tileable outputs.
3. TexTile
3.1. Introduction
Our goal is to develop a differentiable no-reference image
metric that measures texture tileability, that is, a single-
imagemetricthatdoesnotrequireasecondimageforcom-
parison purposes. To achieve this, we leverage a convolu-
tional neural network as our differentiable function, which
Input Image
ùê¥ùëàùê∫ùëá‚ÜíùêπPositive 
ExamplesFigure 3. From a tileable texture (left), our data augmentationcan
generate tileable (top row) and non-tileable (bottom) variations.
we train on a dataset of textures on a binary classification
task. Ourmodellearnstoclassifybetweentileableandnon-
tileabletextures,bymeansofacomprehensivedataaugmen-
tation policy and custom architecture design choices. We
explainourmodeldesignchoicesinSec.3.2,validatethem
usingablationstudiesinSec.4.1,andexplainthemodelpre-
dictionsinSec.4.3. WeshowexamplesofresultsofTexTile
as a loss function (Sec. 5.1), as a means of benchmarking
image synthesis algorithms (Sec. 5.2), and applications for
alignment and repeating pattern detection (Sec. 5.3).
3.2. Model Design and Training
Network Design
A model that can measure texture tileability should have at
least three properties. First, it must be able to detect local
discontinuities, which happen when borders are not seam-
lessly tileable. Second, the model should be able to han-
dle images of any dimension or aspect ratios. Finally, it
should have a global understanding of the image, in order
todetectartifactsandrepeatingpatterns. Thefirsttwoprop-
erties can be achieved by fully-convolutional architectures,
which are strongly biased towards textures [26]. However,
problems that require global understanding of images are
typically tackled using attention-based Vision Transform-
ers [23], which are more biased towards shape [46] and are
less flexible in terms of input dimensionality.
We propose an architecture that can benefit from the
properties of both convolutional and attention-based mod-
els. Because of the limited size of our training dataset,
we also want to leverage ImageNet pretraining [12]. We
thus use a state-of-the-art pretrained ConvNext [39] fully-
convolutional model. We further introduce Linear Self-
Attentionmodules[65]toallowittolearnglobalpatternsin
theimages,whilekeepingalimitedcomputationalcost. We
design them as residual layers ùë•‚Üêùë•+ùúÜùëìatt(ùë•), multiplied
byalearnableparameter ùúÜ,whichweinitializeat 1ùëí‚àí6. We
illustrate this module in Figure 2. This way, we can modify
the internal model architecture without disrupting the per-
formance of the pre-trained backbone during early training
iterations. Weonlyusetwoofsuchmodules,whichweplace
onthedeeperlayersoftheConvNextmodel. Previouswork
ontextureestimationandsynthesisalsobenefitfromadding
attention to fully-convolutional backbones [21, 55].
4441
Data Augmentation for Tileable Textures
We leverage a comprehensive data augmentation policy, to
train the model to detect repeating artifacts in images.
Our policy contains several operations which we divide
into:Tileability-preserving policies, which generate vari-
ations of textures without reducing their tileability. These
include global color and gamma changes, random flipping
(horizontalandvertical),translations,equalization,blursor
noise, or rescaling the images with different scale factors
across each dimensions. We also introduce an operation
namedUnFold, which mirrors the input image horizontally
and vertically. Tileability-breaking policies include rota-
tions, shears, random cropping, or thin-plate spline warp-
ing [4]. We apply tileability-preserving policies to tileable
examples, and both kinds of operations to non-tileable ex-
amples. Finally, our last operation is random tiling, by
which we tile the textures a random number of times (1-5),
followedbyarandomrescaling,andrandomcropping. With
this policy, we allow the model to detect tileability regard-
less on the number of repetitions in the input images.
We introduce two additional data augmentation poli-
cies. With ùê¥ùëàùê∫ùëá‚Üíùêπ, we generate non-tileable textures
from tileable textures. We do this by applying a tileability-
breaking operation to a tileable texture. With ùê¥ùëàùê∫ùêπ‚Üíùêπ‚Ä≤,
we create repetition-free texture examples by applying ev-
ery data augmentation operation except random tiling to
non-tileable examples, and assign these textures a positive
tileability label. These two policies are needed to reduce
thedistributionshiftbetweentileableandnon-tileabletrain-
ing datasets, as they come from different sources and their
semantics, feature scales, and contents may differ. With
ùê¥ùëàùê∫ùëá‚Üíùêπandùê¥ùëàùê∫ùêπ‚Üíùêπ‚Ä≤, weforce themodel to learnpat-
ternsexclusivelyrelatedtotileability,ignoringothertexture
properties. We illustrate some of these policies in Figure 3,
including example of UnFolding on the top right image.
3.3. Model Inference
We illustrate the inference process in Figure 2. We tile the
input image I once across each spatial dimension Itiled ‚Üê
ùë°ùëñùëôùëí(I,(2,2)). We observe that this limited number of repe-
titionsisenoughtoaccuratelydetecttileability. Weusethis
processbothintextureevaluationandwhenweuseTexTile
aslossfunction. Itiledisfedtoourmodel Óàπ,whichoutputs
anunboundedprediction,whichcanbetransformedintothe
desired (0,1)range with a Logistic function. However, do-
ing this typically leads to predictions very close to 0 or 1,
making our metric less useful for comparing different tex-
tures. We mitigate this issue by introducing ùúÜ, which con-
trols how far the predicted values are from the boundaries:
TexTile =1
1 + exp(‚àíùúÜ‚ãÖÓàπ(Itiled)) (1)
0.20.4 0.6 0.8 1.0 1.2 Trainable Parameters
1e8 0.100.150.20Validation Error
ConvNext-Base-Att
ConvNextConvNext-Att
ResNetResNet-Att
VGGSwin
Figure 4. Influence on the neural architecture type and size on
itsquantitativeperformance(Cross-entropyerroronthevalidation
dataset). Convolutionalarchitecturesaremarkedwith ‚ñ†,attention-
based models with ‚àô, and our versions of convolutional networks
with embedded attention with ‚úö.
We setùúÜ= 0.25, which preserves discrimination be-
tween clearly tileable and non-tileable samples while am-
biguous cases sit closer to 0.5. This is a strictly monotonic
function, so relative tileability orders are maintained.
3.4. Dataset
Our goal is to make our model robust to textures of any se-
mantics,regularity,stochasticity,andhomogeneity. Wealso
wantourmodeltoworkwithbothnaturalandsynthetictex-
ture maps. To achieve this, we collect a dataset of tileable
and non-tileable textures from a variety of sources.
Wegatheranoveldatasetof 4276tileabletextures,com-
prised of high-resolution photographs of materials, turned
into tileable textures by artist labor. We extend this data
with publicly-available tileable textures, including 285im-
agesfrom[63], 186CC-BYimagesfrom JulioSillet ,includ-
ing both albedo and surface normal maps, and 290royalty-
free textures from ManyTextures . These images, being cu-
rated by human experts, ensure the tileability property.
We also create an additional dataset of non-tileable tex-
tures, for which examples are comparably easier to ob-
tain. To this end, we gather 3922synthetic high-quality
from [13], 1187photographs of a wide variety of tex-
ture types from [10], 506facade photographs from [29],
7265images taken under different illumination conditions
from[72], 760from[73],and 93from[45]. Weextendthis
datasetwith 8675imagesfrom[16,32,34,41],and 86non-
stationary textures from [75]. While we can create nega-
tive examples from tileable textures using ùê¥ùëàùê∫ùëá‚Üíùêπ, this
dataset is important for generalization. We use 504tileable
imagesand 504non-tileableexamplesforvalidation,andthe
restisleftfortraining. Wehaveapproximately5timesmore
non-tileableexamplesthantileableimages,whichmayhin-
derthetaskoflearninganunbiasedclassifier. Weovercome
this with our training and data augmentation policies.
In Section 4 we evaluate the performance of our model,
measuringaveragebinarycross-entropyerroronourtestset,
4442
Configuration Error ‚ÜìAccuracy ‚Üëùêπ1‚ÜëAUC ‚ÜëTrainingW/out Pretraining 0.459 0.808 0.804 0.886
W/out NAdam [15] 0.096 0.965 0.966 0.992
W/out Look-Ahead [70] 0.082 0.971 0.971 0.992Data AugmentationW/out Negative Samples 0.319 0.905 0.909 0.943
W/out Color Aug. 0.119 0.956 0.957 0.992
W/out Rescales 0.105 0.960 0.968 0.990
W/out Flips 0.087 0.972 0.972 0.993
W/out Distortions 0.094 0.966 0.966 0.987
W/outùê¥ùëàùê∫ùëá‚Üíùêπ 0.121 0.960 0.961 0.990
W/outùê¥ùëàùê∫ùêπ‚Üíùêπ‚Ä≤ 0.087 0.969 0.966 0.991
W/out UnFold 0.082 0.970 0.971 0.992
Final Model 0.064 0.982 0.983 0.997
Table1. Ablationstudyondifferentconfigurationsofmodeltrain-
ing configurations and data augmentation policies.
aswellasclassificationmetrics,likeaccuracy, ùêπ1-Scoreand
Area Under the Curve (AUC). We provide more details of
our datasets in the supplementary material.
3.5. Implementation Details
We train our models for 100 epochs using NAdam [15]
Lookahead [70], Automatic Gradient Scaling and Mixed
Precision Training [42], with an initial learning rate of
0.002, halved every 33 epochs. We use PyTorch [50] for
training, and Kornia [52] for data augmentation. We use
batchsizesof24samples,composedofbalancednumberof
positive and negative examples. This process takes approx-
imately6hoursonasingleNvidiaRTX3060GPU.Weuse
images of (384,384)pixels for training and (512,512)for
inference. Hyperparameters are tuned using Bayesian opti-
mization on a validation dataset containing 1000 images.
4. Evaluation
4.1. Ablation Study
Network Architecture Design In Figure 4, we show
the impact of the neural network architecture design on
generalization performance. We evaluate different fully-
convolutionalbackbones,includingResNet[24],VGG[58]
and ConvNext [39]; as well as transformer-based Swin
V2 [38], on different model sizes. We also show the re-
sultsofourvariationsofthefully-convolutionalbackbones,
where we introduce linear Self-Attention [65] modules in
the last layers of the models. Every network is pre-trained
onImageNet[12],thenfinetunedinourtask,andevaluated
onavalidationdataset. Asshown,largernetworkstypically
perform better, with the exception of Swin, which interest-
ingly benefits from fewer parameters. Further, ConvNexts
strongly outperform ResNets and VGGs. By introducing
self-attentionintothesefully-convolutionalmodels,wesig-
nificantly improve their generalization capabilities. In the
rest of our experiments, we will use our custom ConvNext-
Base-Attmodel, as it achieves the lowest error overall.Distance Error ‚ÜìAccuracy ‚Üëùêπ1‚ÜëAUC ‚Üë
FID [27] 0.568 0.703 0.699 0.764
GS [31] 0.694 0.507 0.533 0.510
MSID [61] 0.797 0.582 0.544 0.503
TexTile 0.064 0.982 0.983 0.997
Table 2. Comparison of our metric with distribution-based dis-
tances, on a downstream classification task.
Data Augmentation and Training Configuration
InTable1,weshowanablationstudyofourdataaugmenta-
tionandoptimizationsetups. Fromourfinalmodelconfigu-
ration,weremovedifferentcomponentstoitstrainingpolicy
to study their impact on generalization. We report different
classification metrics measured on our test set, which con-
tains abalanced numberof positiveand negative examples.
First,weshowthatthemodelstronglybenefitsfrompre-
training on ImageNet. The model performance can be en-
hanced by introducing Nesterov momentum [15] into the
optimizer, and further with Lookahead training [70]. We
observe that these results are consistent across architec-
tures, and that other optimizers, such as RAdam [37] or
AdamW [40], performed worse than NAdam or Adam.
Regarding data augmentation, we first tested a model
trainedwithoutnegativeexamples,relyingonsyntheticnon-
tileable textures from tileable ground truths. However,
this yielded limited generalization. Traditional augmenta-
tions(color,geometry,noise,blurs,elastictransformations)
provided incremental improvements. Our custom policies,
which generate negative samples from tileable images and
vice versa, further enhanced model performance. By in-
troducing random unfolding , we achieve small gains. This
comprehensive data augmentation policy mitigates model
and dataset limitations and makes TexTile more robust to
importantfactorslikecolorvariations,sharpness,orscales.
4.2. Comparison with Distribution-Based Metrics
Inthisexperiment,wecompareourmetricwithoff-the-shelf
distribution-basedmetrics. Toachievethis,wecomputethe
latent features of both our tileable and non-tileable training
datasets, using 2x2 tilings as in our setup. Then, for ran-
domly selected subsets from our test set, we compare their
latentfeaturestothoseofbothtrainingdatasets. Finally,we
classify the set according to the which of both distributions
is closest. We show the results on Table 2, where we find
that these metrics fail to capture important characteristics
thatinfluencethetileabilityoftextures. Directlysupervising
fortileabilityisunsurprisinglymoreeffective. Interestingly,
theGS[31]andMSID[61]metricsperformonlymarginally
betterthanrandom,whereasFID[27]bettercapturesthedif-
ferences between the distributions.
4.3. Qualitative Evaluation
We leverage Axion-Based Class-Activation Mappings [17]
to visualize which features are most relevant to our model.
4443
0.059
 0.217
 0.652 0.572
 0.821
 0.013
 0.089
 0.819
 0.969
 0.531Figure5. Ontop,texturessamples(tiled 2 √ó 2)withincreasingpredictedtileability. Belowthem,modelsaliencymapsandTexTilevalues.
In Figure 5, we show saliency maps and predicted TexTile
values for a few representative textures with increasing de-
grees of tileability. As shown on the first two examples,
our model predicts low tileability values for textures with-
outseamlessborders. Inthenextthreeexamples,whichare
seamlessonlyononeoftheiraxes,themodeloutputshigher
tileability values. On the last five examples, which are all
seamlessly tileable, the model is leveraging other features,
likeunevenshadings,unusualartifacts,orrepeatingobjects.
Theseresultsshowthatourmodelcanexploitpatternsother
thanborderdiscontinuityforitsprediction,andthatitcanin-
tegrate distant information for finding repeating elements.
5. Results
5.1. TexTile as a Loss Function
Because TexTile is a fully-differentiable metric, it can be
leveraged as a loss function for synthesizing tileable tex-
tures. We explore this on two different types of algorithms.
First, we extend an optimization-based neural texture
synthesis algorithm [25] to generate tileable textures. To
do so, we simply optimize a joint loss function Óà∏=
ùúÜstyleÓà∏style+ùúÜTexTileÓà∏TexTile, whereùúÜstyleandùúÜTexTilecon-
trol the weight of the each component of the loss and are
selected empirically to ùúÜstyle=ùúÜTexTile = 1. We observed
little sensitivity to these weightings as long as they are on
the same order of magnitude. We show end-to-end synthe-
sisresultsinFigure8. Wecanalsogeneratetileabletextures
in this fashion by optimizing only the texture borders using
outpainting, as we illustrate in Figure 6.
Relatedly, we can also extend single image diffusion
modelsso they can generate tileable textures. While the
goal of these methods is more general image or video syn-
thesis, we leverage them as powerful texture synthesis al-
gorithms. To do so, we use SinFusion models [47] without
anymodificationsinthetrainingprocess. Duringinference,
after each diffusion step, we perform a single optimization
steptothenoisyimageonthedirectionthatmaximizesTex-
Tile. We show qualitative results in Figure 8.
Withthesesimplemodifications,wecantransformthese
methods ‚Äìand potentially any texture generative model‚Äì
into tileable texture synthesis algorithms, without signifi-
cantlossintheperceptualqualityofthegeneratedtextures.
Figure 6. Image outpainting for tileable texture synthesis. On the
left,non-tileable input images with the area to be outpainted in a
solid color; on their sides, outpainted results, obtained by maxi-
mizing tileability, shown in a 2x2 tile composition.
Importantly, this can be achieved without re-training the
generative models. Further implementation details and re-
sults are included in the supplementary material.
5.2. Benchmarking Texture Synthesis Algorithms
In Table 3, we show a quantitative comparison between
different texture synthesis algorithms and generative mod-
els, on the 14-texture dataset used in [53], using reference
and no-reference metrics. For [25, 47], we show results of
their baseline methods and our modifications that generate
tileable textures. The qualitative results on the complete
dataset is present in the supplementary material.
Besides, we observe that powerful generative models
like [47, 53] do not consistently beat non-parametric alter-
natives like [11, 36] across either reference or no-reference
metrics. In terms of tileability, introducing Textile as a
loss function to [25, 47] not only significantly improves the
tileability of their outputs but it does it without reducing
their perceptual quality. Unsurprisingly, methods that per-
form border blending [11, 54] achieve lower tileability val-
ues across different metrics, than methods that synthesize
thetexturesinamoreholisticway,like[5,45,48,53]. These
resultsconfirmthattherearemoreconstituentfactorsintex-
ture tileability than simply seamless borders.
A correlation matrix between these metrics is shown in
Figure 7. Learned reference metrics (LPIPS, DISTS and
PieAPP) correlate strongly between each other but poorly
with other measures. Si-FiD and SSIM are not closely re-
lated with any other metric, while TexTile is slightly corre-
lated with BRISQUE and CLIP-IQA. As we also illustrate
in Figure 1, there is no consensus across metrics on which
algorithmisoutperformingtheothers,showingthatpercep-
tual evaluation for texture synthesis remains a challenge.
4444
Reference-Based Metrics No Reference
SSIM [66] ‚ÜëSi-FID [56] ‚ÜìLPIPS [71] ‚ÜìDISTS [14] ‚ÜìPieAPP [51] ‚ÜìBRISQUE [43] ‚ÜìCLIP-IQA [64] ‚ÜëTexTile ‚Üë
Deloitet al. [11] 0.138 1.707 0.595 0.372 2.289 49.43 0.407 0.639
Liet al. [36] 0.149 0.981 0.559 0.341 1.672 45.84 0.437 0.707
Rodriguez-Pardo et al. [54] 0.171 0.969 0.594 0.361 1.890 43.14 0.661 0.403
Bergmann et al. [5] 0.148 0.981 0.579 0.359 1.789 44.13 0.638 0.675
Niklasson et al. [48] 0.146 0.819 0.623 0.446 2.292 53.99 0.435 0.731
Rodriguez-Pardo et al. [53] 0.166 0.694 0.540 0.336 1.542 51.10 0.452 0.729
Heitzet al. [25] w/out TexTile 0.140 1.741 0.575 0.329 1.687 49.05 0.376 0.431
Heitzet al. [25] with TexTile 0.152 1.764 0.555 0.327 1.653 48.49 0.392 0.781
Nikankin et al. [47] w/out TexTile 0.172 1.314 0.591 0.386 1.963 55.96 0.408 0.388
Nikankin et al. [47] with TexTile 0.189 1.415 0.569 0.387 1.926 56.99 0.396 0.798
Table3. Quantitativeevaluationbetweendifferenttileabletexturesynthesisalgorithmsacrossavarietyofmetrics,includingreferenceand
no-reference measures, and TexTile. Best two results for each columns are marked in bold.
Previous Methods Neural Texture Synthesis [25] Diffusion Models [47]
Input Blending [11] SeamlessGAN[53] Self-Org [48] W/out TexTile With TexTile W/out TexTile With TexTile
Figure8. Comparisonsofdifferenttexturesynthesisalgorithms. Ontheleftmostcolumn,weshowtheinputtexture,ontheright,2x2tilings
of the outputs of different methods. For [25] and [47], we show the original versions our the modifications for tileable texture synthesis.
SSIM Si-FID LPIPS DISTS PieAPP BRISQUE CLIP-IQA TexTileSSIM
Si-FID
LPIPS
DISTS
PieAPP
BRISQUE
CLIP-IQA
TexTile1.00 -0.22 -0.20 0.14 -0.15 0.42 0.10 0.21
-0.22 1.00 -0.04 -0.29 0.08 0.11 -0.53 -0.10
-0.20 -0.04 1.00 0.81 0.88 0.11 0.17 -0.27
0.14 -0.29 0.81 1.00 0.83 0.52 -0.01 0.24
-0.15 0.08 0.88 0.83 1.00 0.31 -0.06 0.03
0.42 0.11 0.11 0.52 0.31 1.00 -0.71 0.55
0.10 -0.53 0.17 -0.01 -0.06 -0.71 1.00 -0.40
0.21 -0.10 -0.27 0.24 0.03 0.55 -0.40 1.00
Figure 7. Pearson correlation matrix between different metrics.
5.3. Alignment and Repeating Pattern Detection
Besidesbenchmarkingandenablingtileabletexturesynthe-
sis, our metric enables additional applications.
PreviousworkontextureanalysisusedtheRadonTrans-
form[30,54]forautomaticallyaligningimageswiththe ùë•ùë¶-axes. Similarly, we can find the optimal rotation angle ùúÉ
for an image I by arg maxùúÉTexTile (Rotate (I,ùúÉ)), maximiz-
ing the tileability of the input image. In Figure 9, we show
the scores of our metric, for the same image, to which we
apply different rotation angles. Our metric provides high
scores for rotation angles which provide seamless borders,
andlowervaluesformisalignments. Interestingly,thereisa
smallpeakat ¬±35‚ó¶,inwhichthelinesintheimageconnect
with each other, but their colors do not match in the bor-
ders. These results indicate that TexTile not only measures
seamlessness,butalsocolorcontinuity,andaxisalignment.
We can also leverage TexTile to compute the size of the
repeating pattern in images. Given an axis-aligned im-
age I, we can find the size ‚Ñé,ùë§of the repeating pattern in
the image by finding the crop that maximizes its tileability:
4445
0¬∞45¬∞90¬∞135¬∞
180¬∞
225¬∞
270¬∞
315¬∞0.500.75
195¬∫ 35¬∫
90¬∫ 135¬∫Figure 9. On the left, TexTile under different rotation angles.
On the right, samples of rotated images on different local peaks.
Scoresbelow0.5,astheredpeak,depictnon-tileabletextureswith
highly-noticeable artifacts. A local maxima for a non-tileable tex-
ture is found at ¬±35‚ó¶, in blue, while highly tileable textures are
found at the gray and green insets.
100150 200 250 100150200250
0.0
0.20.40.60.81.0
Figure 10. On the left, input image. In the middle, TexTile values
for different crop sizes. The optimal crop is highlighted on both
images with a green inset. On the right, the optimal crop, tiled
many times for visualization.
arg max‚Ñé,ùë§TexTile (Crop (I,(‚Ñé,ùë§))). We show a result in
Figure10,wherethisalgorithmfindstheoptimalcropsizeat
theminimumrepeatablepattern,aswellasthreelowerpeaks
atdifferentdiscretescalefactorsofthiscropsize. Notethat
we limit the crops to ‚Ñé,ùë§‚â•64and perform the crop at
thecenteroftheimage. Previouswork[35,54]foundthese
repeating patterns in the activations of pre-trained CNNs.
Because internal neural activations operate at lower reso-
lutions that those of the original image, these methods are
limitedinprecision. Ourmethod,incontrast,operatesatthe
resolution of the original image and may thus be more pre-
cise. More results on these two applications are present in
the supplementary material.
5.4. Failure Cases
As shown in Table 1, our model predictions are accurate,
however, some errors occur. We show some examples of
misclassified textures in Figure 11. On the left, we show a
texture labeled as non-tileable in our test dataset, that our
modelpredictsastileabledespiteitshowingdiscontinuities
in the borders. On the right, we show a texture that is la-
beledastileable,whichourmodelclassifiesasnon-tileable.
Both examples are edge cases and highlight the ambiguity
inwhatconstitutesatileabletexture. Besides,whenusedas
a loss for synthesis models, TexTile cannot compensate for
thelimitationsofthegenerativebackbone. Ifamodelcannot
TexTile : 0.569True label: 0
TexTile : 0.482True label: 1
Figure 11. Examples of misclassifications done by our model. On
the top, ground truth labels, on the bottom, the predicted TexTile
value. Fortheexampleontheleft,weshowaninsetofthecentral
crop of the texture, to highlight that it is not seamlessly tileable.
adequatelysynthesizetexturesthatmatchtheappearanceof
theinput,addingTexTilehelpsreducediscontinuitiesinthe
bordersbutwillnotimproveitsperceptualquality,ascanbe
seen in the textures generated with [25] in Figure 8.
6. Conclusions
Wehavepresented TexTile,thefirstdifferentiablemetricfor
texture tileability. While it is trained on a simple classifi-
cation task, we design custom data augmentation, training
regimes, and neural architectures, all specifically tailored
to accurately measure tileability. We validated our design
choiceswithcomprehensiveablationstudies,andleveraged
saliency maps for model understanding. We showed dif-
ferent applications of our differentiable metric, including
benchmarking texture synthesis algorithms, detecting repe-
titionsandmisalignmentinimages,andtransformingimage
generativemodelsintotileabletexturesynthesisalgorithms.
We will provide code and model weights upon acceptance.
LimitationsandFutureWork Wecouldextendourwork
in several ways. While our method accurately measures
tileability in textures, it is does not quantify their percep-
tual quality, limiting its scope. Combining perceptual met-
rics with TexTile, to measure both perceptual quality and
tileability, is an important research avenue for a more in-
tegrated analysis of texture quality. Besides, our model is
pre-trained on a ImageNet, then fine-tuned on a manually-
curated dataset, and thus may inherit biases. While we be-
lievethedatasetsweusedwerecomprehensive,itispossible
thatsometypeoftextureisunderrepresentedandthemodel
may not perform accurately. Using synthetic data [18] may
help alleviate this issue. Finally, there is no solid under-
standingofhumanperceptionoftexturerepetitiveness[60],
and, while human perceptual validation is out of the scope
of this work, its apparent higher correlation with TexTile,
might add new insights to the elements identified so far.
Acknowledgments This publication is part of the project TaiLOR,
CPP2021-008842 funded by MCIN/AEI/10.13039/501100011033 and the NextGen-
erationEU / PRTR programs. Elena Garces was partially supported by a Juan de la
Cierva - Incorporacion Fellowship (IJC2020-044192-I).
4446
References
[1] Noam Aigerman and Thibault Groueix. Generative escher
meshes.arXiv preprint arXiv:2309.14564 , 2023. 1, 3
[2] Adib Akl, Charles Yaacoub, Marc Donias, Jean-Pierre
Da Costa, and Christian Germain. A survey of exemplar-
basedtexturesynthesismethods. ComputerVisionandImage
Understanding , 172:12‚Äì24, 2018. 3
[3] Dan Amir and Yair Weiss. Understanding and simplifying
perceptual distances. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 12226‚Äì
12235, 2021. 2
[4] Adrien Bartoli, Mathieu Perriollat, and Sylvie Chambon.
Generalized thin-plate spline warps. International Journal
of Computer Vision , 88:85‚Äì110, 2010. 4
[5] UrsBergmann,NikolayJetchev,andRolandVollgraf. Learn-
ing texture manifolds with the periodic spatial gan. pages
469‚Äì477, 2017. 3, 6, 7
[6] Miko≈ÇajBi≈Ñkowski,DanicaJSutherland,MichaelArbel,and
Arthur Gretton. Demystifying mmd gans. In International
Conference on Learning Representations , 2018. 2
[7] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp,
and Kangxue Yin. Texfusion: Synthesizing 3d textures with
text-guided image diffusion models. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 4169‚Äì4181, 2023. 1
[8] Dan Casas and Marc Comino-Trinidad. SMPLitex: A Gen-
erativeModelandDatasetfor3DHumanTextureEstimation
from Single Image. In British Machine Vision Conference
(BMVC), 2023.
[9] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey
Tulyakov,andMatthiasNie√üner. Text2Tex: Text-drivenTex-
ture Synthesis via Diffusion Models. 2023. 1
[10] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A.
Vedaldi. Describing textures in the wild. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, 2014. 4
[11] ThomasDeliotandEricHeitz. Proceduralstochastictextures
by tiling and blending. GPU Zen , 2, 2019. 1, 3, 6, 7
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical im-
agedatabase. In ProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision ,pages248‚Äì255.Ieee,2009.
3, 5
[13] Valentin Deschaintre, Miika Aittala, Fredo Durand, George
Drettakis,andAdrienBousseau.Single-imagesvbrdfcapture
witharendering-awaredeepnetwork. ACMTransactionson
Graphics (TOG) , 37(4):1‚Äì15, 2018. 4
[14] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli.
Image quality assessment: Unifying structure and texture
similarity. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 44(5):2567‚Äì2581, 2020. 2, 7
[15] Timothy Dozat. Incorporating nesterov momentum into
adam. 2016. 5
[16] Mario Fritz, E. Hayman, B. Caputo, and J. Eklundh. The
kth-tips database. 2004. 4[17] Ruigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo,
Yinghui Gao, and Biao Li. Axiom-based grad-cam: To-
wards accurate visualization and explanation of cnns. arXiv
preprint arXiv:2008.02312 , 2020. 5
[18] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy
Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dream-
sim: Learningnewdimensionsofhumanvisualsimilarityus-
ing synthetic data. arXiv preprint arXiv:2306.09344 , 2023.
2, 8
[19] Bruno Galerne, Ares Lagae, Sylvain Lefebvre, and George
Drettakis. Gabor noise by example. ACM Transactions on
Graphics (TOG) , 31(4):73:1‚Äì73:9, 2012. 1
[20] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.
A neural algorithm of artistic style. arXiv preprint
arXiv:1508.06576 , 2015. 2
[21] Shouchang Guo, Valentin Deschaintre, Douglas Noll, and
Arthur Roullier. U-attention to textures: hierarchical hour-
glass vision transformer for universal texture synthesis. In
Proceedings of the ACM SIGGRAPH European Conference
on Visual Media Production , pages 1‚Äì10, 2022. 3
[22] Yu Guo, Cameron Smith, Milo≈° Ha≈°an, Kalyan Sunkavalli,
and Shuang Zhao. Materialgan: reflectance capture using
a generative svbrdf model. ACM Transactions on Graphics
(TOG), 39(6):1‚Äì13, 2020. 1
[23] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,
JianyuanGuo,ZhenhuaLiu,YehuiTang,AnXiao,Chunjing
Xu, Yixing Xu, et al. A survey on vision transformer. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
45(1):87‚Äì110, 2022. 3
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deepresiduallearningforimagerecognition.In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 770‚Äì778, 2016. 5
[25] Eric Heitz, Kenneth Vanhoey, Thomas Chambon, and Lau-
rentBelcour. Aslicedwassersteinlossforneuraltexturesyn-
thesis. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 9412‚Äì9420, 2021. 6, 7,
8
[26] Katherine Hermann, Ting Chen, and Simon Kornblith. The
originsandprevalenceoftexturebiasinconvolutionalneural
networks. Advances in Neural Information Processing Sys-
tems, 33:19000‚Äì19015, 2020. 3
[27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium.Advances in Neural Information Processing Systems ,
30, 2017. 2, 5
[28] Yiwei Hu, Julie Dorsey, and Holly Rushmeier. A novel
framework for inverse procedural texture modeling. ACM
Transactions on Graphics (TOG) , 38(6):1‚Äì14, 2019. 1
[29] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-imagetranslationwithconditionaladversar-
ialnetworks. In ProceedingsoftheIEEE/CVFInternational
Conference on Computer Vision , pages 1125‚Äì1134, 2017. 4
[30] Kourosh Jafari-Khouzani and Hamid Soltanian-Zadeh.
Radontransformorientationestimationforrotationinvariant
textureanalysis. IEEETransactionsonPatternAnalysisand
Machine Intelligence , 27(6):1004‚Äì1008, 2005. 7
4447
[31] Valentin Khrulkov and Ivan Oseledets. Geometry score: A
method for comparing generative adversarial networks. In
InternationalConferenceonMachineLearning ,pages2621‚Äì
2629. PMLR, 2018. 2, 5
[32] Gustaf Kylberg. The kylberg texture dataset v. 1.0. External
report (Blue series) 35, Centre for Image Analysis, Swedish
University of Agricultural Sciences and Uppsala University,
Uppsala, Sweden, 2011. 4
[33] Tuomas Kynk√§√§nniemi, Tero Karras, Samuli Laine, Jaakko
Lehtinen,andTimoAila. Improvedprecisionandrecallmet-
ric for assessing generative models. Advances in Neural In-
formation Processing Systems , 32, 2019. 2
[34] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. A
sparsetexturerepresentationusinglocalaffineregions. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
27(8):1265‚Äì1278, 2005. 4
[35] Louis Lettry, Michal Perdoch, Kenneth Vanhoey, and Luc
Van Gool. Repeated pattern detection using cnn activations.
InIEEEWinterConferenceonApplicationsofComputerVi-
sion, pages 47‚Äì55. IEEE, 2017. 8
[36] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi,
KalyanSunkavalli,andManmohanChandraker. Inverseren-
dering for complex indoor scenes: Shape, spatially-varying
lighting and svbrdf from a single image. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 2475‚Äì2484, 2020. 1, 3, 6, 7
[37] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen,
Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the vari-
anceoftheadaptivelearningrateandbeyond. arXivpreprint
arXiv:1908.03265 , 2019. 5
[38] ZeLiu,HanHu,YutongLin,ZhuliangYao,ZhendaXie,Yix-
uan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.
Swin transformer v2: Scaling up capacity and resolution. In
ProceedingsoftheIEEEConferenceonComputerVisionand
Pattern Recognition , pages 12009‚Äì12019, 2022. 5
[39] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 11976‚Äì11986, 2022. 2,
3, 5
[40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 5
[41] P. Mallikarjuna, Alireza Tavakoli Targhi, Mario Fritz, E.
Hayman,B.Caputo,andJ.Eklundh. Thekth-tips2database.
2006. 4
[42] PauliusMicikevicius,SharanNarang,JonahAlben,Gregory
Diamos,ErichElsen,DavidGarcia,BorisGinsburg,Michael
Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed
precision training. In International Conference on Learning
Representations , 2018. 5
[43] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad
Bovik. No-reference image quality assessment in the spatial
domain. IEEE Transactions on Image Processing , 21(12):
4695‚Äì4708, 2012. 2, 7
[44] Anush Krishna Moorthy and Alan Conrad Bovik. A two-
step framework for constructing blind image quality indices.
IEEE Signal Processing Letters , 17(5):513‚Äì516, 2010. 2[45] Joep Moritz, Stuart James, Tom SF Haines, Tobias Ritschel,
and Tim Weyrich. Texture stationarization: Turning photos
into tileable textures. In Computer Graphics Forum , pages
177‚Äì188. Wiley Online Library, 2017. 1, 3, 4, 6
[46] Muhammad Muzammal Naseer, Kanchana Ranasinghe,
SalmanHKhan,MunawarHayat,FahadShahbazKhan,and
Ming-HsuanYang. Intriguingpropertiesofvisiontransform-
ers.AdvancesinNeuralInformationProcessingSystems ,34:
23296‚Äì23308, 2021. 3
[47] Yaniv Nikankin, Niv Haim, and Michal Irani. Sinfusion:
Training diffusion models on a single image or video. In In-
ternationalConferenceonMachineLearning .PMLR,2023.
6, 7
[48] EyvindNiklasson,AlexanderMordvintsev,EttoreRandazzo,
and Michael Levin. Self-organising textures. Distill, 2021.
https://distill.pub/selforg/2021/textures. 1, 3, 6, 7
[49] Zhaoqing Pan, Feng Yuan, Jianjun Lei, Yuming Fang, Xiao
Shao, and Sam Kwong. Vcrnet: Visual compensation
restoration network for no-reference image quality assess-
ment.IEEE Transactions on Image Processing , 31:1613‚Äì
1627, 2022. 2
[50] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban
Desmaison,LucaAntiga,andAdamLerer. Automaticdiffer-
entiation in pytorch. 2017. 5
[51] Ekta Prashnani, Hong Cai, Yasamin Mostofi, and Pradeep
Sen. Pieapp: Perceptual image-error assessment through
pairwise preference. In Proceedings of the IEEE/CVF Inter-
nationalConferenceonComputerVision ,pages1808‚Äì1817,
2018. 2, 7
[52] Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee,
and Gary Bradski. Kornia: an open source differentiable
computer vision library for pytorch. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision, pages 3674‚Äì3683, 2020. 5
[53] Carlos Rodriguez-Pardo and Elena Garces. Seamlessgan:
Self-supervised synthesis of tileable texture maps. IEEE
TransactionsonVisualizationandComputerGraphics ,2022.
1, 2, 3, 6, 7
[54] Carlos Rodriguez-Pardo, Sergio Suja, David Pascual, Jorge
Lopez-Moreno, and Elena Garces. Automatic extraction
and synthesis of regular repeatable patterns. Computers &
Graphics, 83:33‚Äì41, 2019. 1, 3, 6, 7, 8
[55] Carlos Rodriguez-Pardo, Henar Dominguez-Elvira, David
Pascual-Hernandez, and Elena Garces. Umat: Uncertainty-
aware single image high resolution material capture. Pro-
ceedingsoftheIEEE/CVFInternationalConferenceonCom-
puter Vision , 2023. 3
[56] Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Sin-
GAN: Learning a Generative Model from a Single Natural
Image. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , 2019. 2, 7
[57] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. Advances in Neural Information Process-
ing Systems , 29, 2016. 2
[58] KSimonyanandAZisserman. Verydeepconvolutionalnet-
works for large-scale image recognition. In International
4448
ConferenceonLearningRepresentations .Computationaland
Biological Learning Society, 2015. 5
[59] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge,
Jinqiu Sun, and Yanning Zhang. Blindly assess image qual-
ity in the wild guided by a self-adaptive hyper network. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 3667‚Äì3676, 2020. 2
[60] Hua-Chun Sun, David St-Amand, Curtis L Baker Jr, and
Frederick AA Kingdom. Visual perception of texture reg-
ularity: Conjoint measurements and a wavelet response-
distribution model. PLoS Computational Biology , 17(10):
e1008802, 2021. 8
[61] AntonTsitsulin,MarinaMunkhoeva,DavideMottin,Panagi-
otis Karras, Alex Bronstein, Ivan Oseledets, and Emmanuel
Mueller. Theshapeofdata: Intrinsicdistancefordatadistri-
butions. In InternationalConferenceonLearningRepresen-
tations, 2019. 2, 5
[62] Giuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien
Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy
Boubekeur. Controlmat: A controlled generative approach
tomaterialcapture. arXivpreprintarXiv:2309.01700 ,2023.
3
[63] Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa,
Angel X. Chang, and Manolis Savva. Plan2scene: Convert-
ingfloorplansto3dscenes. In ProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision ,pages10733‚Äì
10742, 2021. 4
[64] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Ex-
ploringclipforassessingthelookandfeelofimages. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence ,
pages 2555‚Äì2563, 2023. 2, 7
[65] SinongWang,BelindaZLi,MadianKhabsa,HanFang,and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768 , 2020. 3, 5
[66] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSi-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE Transactions on Image Process-
ing, 13(4):600‚Äì612, 2004. 2, 7
[67] XudongXie,ZijieWu,ZhiliangXu,andZhenZhu. Learning
inasingledomainfornon-stationarymulti-texturesynthesis.
arXiv preprint arXiv:2305.06200 , 2023. 2
[68] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan
Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang.
Maniqa: Multi-dimensionattentionnetworkforno-reference
image quality assessment. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1191‚Äì1200, 2022. 2
[69] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang.
Fsim: A feature similarity index for image quality assess-
ment.IEEETransactionsonImageProcessing ,20(8):2378‚Äì
2386, 2011. 2
[70] Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E
Hinton. Lookahead optimizer: k steps forward, 1 step
back.Advances in neural information processing systems ,
32, 2019. 5
[71] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 586‚Äì595, 2018. 2, 7
[72] Xilong Zhou and Nima Khademi Kalantari. Adversarial
single-image svbrdf estimation with hybrid training. Com-
puter Graphics Forum , 2021. 4
[73] Xilong Zhou and Nima Khademi Kalantari. Look-ahead
trainingwithlearnedreflectancelossforsingle-imagesvbrdf
estimation. ACM Transactions on Graphics (TOG) , 41(6),
2022. 4
[74] XilongZhou,MilosHasan,ValentinDeschaintre,PaulGuer-
rero, Kalyan Sunkavalli, and Nima Khademi Kalantari. Ti-
legen: Tileable, controllable material generation and cap-
ture. In SIGGRAPH Asia 2022 Conference Papers , pages
1‚Äì9, 2022. 1, 3
[75] Yang Zhou, Zhen Zhu, Xiang Bai, Dani Lischinski, Daniel
Cohen-Or, andHuiHuang. Non-stationary texturesynthesis
by adversarial expansion. ACM Transactions on Graphics
(TOG), 37(4):1‚Äì13, 2018. 4
4449
