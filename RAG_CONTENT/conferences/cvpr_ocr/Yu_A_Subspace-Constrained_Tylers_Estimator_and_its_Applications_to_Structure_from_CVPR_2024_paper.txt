AS u b s p a c e - C o n s t r a i n e dT y l e r ’ sE s t i m a t o r
and its Applications to Structure from Motion*
Feng Yu†
University of Minnesota
fyu@umn.eduTeng Zhang
University of Central Florida
teng.zhang@ucf.eduGilad Lerman‡
University of Minnesota
lerman@umn.edu
Abstract
We present the subspace-constrained Tyler’s estimator (STE)
designed for recovering a low-dimensional subspace within a
dataset that may be highly corrupted with outliers. STE is a
fusion of the Tyler’s M-estimator (TME) and a variant of the fast
median subspace. Our theoretical analysis suggests that, under
ac o m m o ni n l i e r - o u t l i e rm o d e l ,S T Ec a ne f f e c t i v e l yr e c o v e rt h e
underlying subspace, even when it contains a smaller fraction of
inliers relative to other methods in the field of robust subspace
recovery. We apply STE in the context of Structure from Motion
(SfM) in two ways: for robust estimation of the fundamental
matrix and for the removal of outlying cameras, enhancing
the robustness of the SfM pipeline. Numerical experiments
confirm the state-of-the-art performance of our method in these
applications. This research makes significant contributions to
the field of robust subspace recovery, particularly in the context
of computer vision and 3D reconstruction.
1. Introduction
In many applications, data has been collected in large quantities
and dimensions. It is a common practice to represent such data
within a low-dimensional subspace that preserves its essential
information. Principal Component Analysis (PCA) is frequently
employed to identify this subspace. However, PCA faces
challenges when dealing with data contaminated by outliers.
Consequently, the field of Robust Subspace Recovery (RSR)
aims to develop a framework for outlier-robust PCA. RSR is
particularly relevant to problems in computer vision, such as
fundamental matrix estimation, which involves recovering a
hidden subspace associated with “good correspondence pairs”
among highly corrupted measurements.
Various algorithms have been proposed to address RSR, em-
ploying methods such as projection pursuit [ 1,5,14,23,29,35,
38], subspace energy minimization (in particular least absolute
*This work was supported by NSF DMS awards 2124913 and 2318926.
†Supplementary code: https://github.com/alexfengg/STE
‡Corresponding author. All authors equally contributed.deviations and its relaxations) [ 9,24,27,33,34,42,43,51],
robust covariance estimation [ 50], filtering-based methods
[4,8,49]a n de x h a u s t i v es u b s p a c es e a r c hm e t h o d s[ 10,16]. An
in-depth exploration and comprehensive overview of robust sub-
space recovery and its diverse algorithms can be found in [ 25].
Methods based on robust covariance estimators, such as the
Tyler’s M-estimator (TME), offer additional useful information
on the shape of the data within the subspace, similarly to PCA
in the non-robust setting. They also offer maximum-likelihood
interpretation, which is missing in many other methods.
Application of the TME [ 47]t oR S Rh a sb e e ns h o w nt ob e
successful on basic benchmarks [ 25,50]. Moreover, under a
model of inliers in a general position on a subspace and outliers
in general position in the complement of the subspace, TME
was shown to recover the subspace within a desirable fraction
of inliers [ 50]. Below this fraction it was proved to be Small
Set Expansion (SSE) hard to solve the RSR problem [ 16].
One may still succeed with solving the RSR problem with a
computationally efficient algorithm when the fraction of inliers
is lower than the one required by [ 16], considering a more
restricted data model or violating other assumptions made in
[16]. For example, some special results in this direction are
discussed in [ 32]. Also, [ 33]p r o p o s e st h eg e n e r a l i z e dh a y s t a c k
model of inliers and outliers to demonstrate the possibility of
handling lower fractions of inliers by an RSR algorithm. This
model extends the limited standard haystack model [ 27], where
basic methods (such as PCA filtering) can easily work with low
fractions of outliers. Nevertheless, it is unclear how practical
the above theoretical ideas are for applied settings.
One practical setting that requires a fraction of inliers sig-
nificantly lower than the one stated in [ 16]a r i s e si nt h ep r o b l e m
of robust fundamental (or essential) matrix estimation. The
fundamental matrix encompasses the epipolar geometry of two
views in stereo vision systems. It is typically computed using
point correspondences between the two projected images. This
computation requires finding an 8-dimensional subspace within
a9 - d i m e n s i o n a la m b i e n ts p a c e .I nt h i ss e t t i n g ,t h et h e o r e t i c a l
framework of [ 16]r e q u i r e st h a tt h ef r a c t i o no fi n l i e r sb ea tl e a s t
8/9⇡88.9%,w h i c hi sc l e a r l yu n r e a s o n a b l et or e q u i r e .
To date, the RANdom Sample Consensus (RANSAC)
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14575
method [ 10]i st h eo n l yR S Rm e t h o dt h a th a sb e e nh i g h l ys u c -
cessful in addressing this nontrivial scenario, gaining widespread
popularity in computer vision. RANSAC is an iterative method
that randomly selects minimal subsets of the data and fits mod-
els, in particular subspaces, to identify the best consensus set,
that is, the set in most agreement with the hypothesized model.
There are numerous approaches proposed to improve RANSAC,
especially for this particular application, including locally
optimized RANSAC (LO-RANSAC, [ 6]), maximum likelihood
estimator RANSAC (MLESAC) [ 45]), degeneracy-check
enabled RANSAC (DEGENSAC) [ 7]) and M-estimator guided
RANSAC (MAGSAC) [ 2]). A near recovery theory for a variant
of RANSAC under some assumptions on the outliers was sug-
gested in [ 32]. Nevertheless, in general, RANSAC is rather slow
and its application to higher-dimensional problems is intractable.
This work introduces a novel RSR algorithm that is
guaranteed to robustly handle a lower fraction of outliers than
the theoretical threshold proposed by [ 16], under special settings.
Our basic idea is to adapt Tyler’s M-Estimator to utilize the
information of the underlying d-dimensional subspace, while
avoiding estimation of the full covariance. By using less degrees
of freedom we obtain a more accurate subspace estimator
than the one obtained by TME with improved computational
complexity. We show that STE is a fusion of the Tyler’s
M-estimator (TME) and a variant of the fast median subspace
(FMS) [ 24]t h a ta i m st om i n i m i z eas u b s p a c e - b a s e d `0energy.
Our theory shows that our proposed subspace-constrained
Tyler’s estimator (STE) algorithm can effectively recover the
underlying subspace, even when it contains a smaller fraction
of inliers relative to other methods. We obtain this nontrivial
achievement first in a generic setting, where we establish when
an initial estimator for STE is sufficiently well-conditioned to
guarantee the desired robustness of STE. We then assume the
asymptotic generalized haystack model and show that under
this model, TME itself is a well-conditioned initial estimator for
STE, and that unlike TME, STE with this initialization can deal
with a lower fraction of inliers than the theoretical threshold
specified in [ 16].
We demonstrate competitive performance in robust
fundamental matrix estimation, relying solely on subspace
information without additional methods for handling degenerate
scenarios, in contrast to [ 7,12,37]. We also propose a potential
application of RSR for removing bad cameras in order to
enhance the SfM pipeline and show competitive performance
of STE. This is a completely new idea and it may require
additional exploration to make it practical. Nevertheless, it
offers a very different testbed where N=Dis very large and
RANSAC is generally intractable.
The rest of the paper is organized as follows: §2introduces
the STE framework, §3establishes theoretical guarantees
of STE, §4applies STE to two different problems in SfM,
demonstrating its competitive performance relative to existing
algorithms, and § 5provides conclusions and future directions.2. The STE Algorithm
We present our proposed STE. We review basic notation in §2.1
and Tyler’s original estimator in §2.2.W ed e s c r i b eo u rm e t h o d
in§2.3,i t sc o m p u t a t i o n a lc o m p l e x i t yi n §2.4,i t sa l g o r i t h m i c
choices in §2.5and an interpretation for it as a fusion of TME
and FMS with p=0in §2.6.
2.1. Notation
We use bold upper and lower case letters for matrices and
column vectors, respectively. Let Ikdenote the identity matrix
inRk⇥k,w h e r ei f kis obvious we just write I.F o r a m a t r i x
A,w ed e n o t eb y tr(A)andIm(A)the trace and image (i.e.,
column space) of A.W ed e n o t eb y S+(D)andS++(D)the
sets of positive semidefinite and definite matrices in RD⇥D,
respectively. We denote by O(D,d)the set of semi-orthogonal
D⇥dmatrices, i.e., U2O(D,d)if and only if U2RD⇥d
andU>U=Id.W er e f e rt ol i n e a r d-dimensional subspaces as
d-subspaces. For a d-subspace L,w ed e n o t eb y PLtheD⇥D
matrix representing the orthogonal projector onto L.W ea l s o
arbitrarily fix ULinO(D,d)such that ULU>
L=PL(such
ULis determined up to right multiplication by an orthogonal
matrix in O(d,d)). Throughout the paper, X={xi}N
i=1⇢RD
is assumed to be a given centered dataset, that is,PN
i=1xi=0.
2.2. Tyler’s Estimator and its Application to RSR
Tyler’s M-estimator (TME) [ 47]r o b u s t l ye s t i m a t e st h ec o v a r i -
ance ⌃⇤of the dataset X={xi}N
i=1⇢RDby minimizing
D
NNX
i=1log(x>
i⌃ 1xi)+logdet( ⌃) (1)
over ⌃2S++(D)such that tr(⌃)=1 .T h e c o s t f u n c t i o n i n
(1)can be motivated by writing the maximum likelihood of the
multivariate t-distribution and letting its degrees of freedom pa-
rameter, ⌫,a p p r o a c hz e r o[ 31]. This cost function is invariant to
dilations of ⌃,a n dt h ec o n s t r a i n to n tr(⌃),w h o s ev a l u ec a nb e
arbitrarily chosen, fixes a scale. TME also applies to scenarios
where the covariance matrix does not exist. In such cases, TME
estimates the shape (or scatter matrix) of the distribution, which
is defined up to an arbitrary scale. More direct interpretations
of TME as a maximum likelihood estimator can be found in
[11,46]. When Dis fixed and Napproaches infinity, TME
is the “most robust” estimator of the shape matrix for data
i.i.d. sampled from a continuous elliptical distribution [ 47]i na
minimax sense, that is, as a minimizer of the maximal variance.
Tyler [ 47]p r o p o s e dt h ef o l l o w i n gi t e r a t i v ef o r m u l af o r
computing TME:
⌃(k)=NX
i=1xix>
i
x>
i(⌃(k 1)) 1xi/tr NX
i=1xix>
i
x>
i(⌃(k 1)) 1xi!
.
Kent and Tyler [ 22]p r o v e dt h a ti fa n y d-subspace of RD,w h e r e
1dD 1,c o n t a i n sf e w e rt h a n Nd/D data points, then
14576
the above iterative procedure converges to TME. Linear rate
of convergence was proved for the regularized TME in [ 15]a n d
for TME in [ 13].
One can apply the TME estimator to solve the RSR problem
with a given dimension dby forming the subspace spanned
by the top deigenvectors of TME. Zhang [ 50]p r o v e dt h a t
as long as there are more than Nd/D inliers lying on a
subspace, and the projected coordinates of these inliers on the
d-subspace and the projected coordinates of the outliers on the
(D d)-dimensional orthogonal complement of the subspace
are in general position, then TME recovers this subspace. Zhang
[50]a l s os h o w e dt h a ti nt h i ss e t t i n gt h ea b o v ei t e r a t i v ef o r m u l a
converges (note that the condition of convergence in [ 22]d o e s
not apply in this case). The above lower bound of Nd/D on
the number of inliers coincides with the general bound for the
noiseless RSR problem, beyond which the problem becomes
Small Set-Expansion (SSE) hard [ 16].
Numerical experiments in [ 50]a n d[ 25]i n d i c a t e ds t a t e - o f -
the-art accuracy of TME compared to other RSR algorithms
in various settings. The computational complexity of TME is of
order O(K(ND2+D3)),w h e r e Kis the number of iterations.
On the other hand, the cost of faster RSR algorithms is of order
O(KNDd )[24,25,33].
2.3. Motivation and Formulation of STE
We aim to use more cleverly the d-subspace information within
the TME framework to form an RSR algorithm, instead of first
estimating the full covariance. By using less degrees of freedom
we can obtain a more accurate subspace estimator, especially
when the fraction of outliers can be large. Furthermore, our
idea allows us to improve the computational cost to become
state-of-the-art for high-dimensional settings.
Many RSR algorithms can be formulated as minimizing a
best orthogonal projector onto a d-subspace [ 24,25,27,33,51].
We are going to do something similar, but unlike using an
orthogonal projector, we will still use information from TME
to get the shape of the data on the projected subspace. We will
make the rest of the eigenvalues (i.e., bottom D dones) equal
and shrink them by a parameter 0< < 1.W et h u su s ear e g u -
larized version of a reduced-dimension covariance matrix. This
parameter  plays a role in our theoretical estimates. Making
 smaller helps with better subspace recovery, whereas making
 bigger enhances the well-conditioning of the estimator.
Following these basic ideas, we formulate our method, STE.
For simplicity, we utilize covariance matrices and their inverses.
Since these covariance matrices are essentially d-dimensional
and include an additional simple regularizing component, our
overall computations can be expressed in terms of the computa-
tion of the top dsingular values of an N⇥Dmatrix (see § 2.4).
At iteration kwe follow a similar step to that of TME:
Z(k):=NX
i=1xix>
i/(x>
i(⌃(k 1)) 1xi).We compute the eigenvalues { i}D
i=1ofZ(k)and replace each
of the bottom (D d)of them with  · d+1,D,w h e r e
 d+1,D:=1
D dDX
i=d+1 i. (2)
We also compute the eigenvectors of Z(k)and form the matrix
⌃(k)with the same eigenvectors as those of Z(k)and the
modified eigenvalues, scaled to have trace 1. We iteratively
repeat this procedure until the two estimators are sufficiently
close. Algorithm 1summarizes this procedure. Note that it is
invariant to scaling of the data, similarly to TME.
Algorithm 1 STE: Subspace-Constrained Tyler’s Estimator
1:Input: X=[x1,...,xN]2RD⇥N:c e n t e r e dd a t am a t r i x , d:
subspace dimension, K:m a x i m u mn u m b e ro fi t e r a t i o n s ,
⌧, :p a r a m e t e r s .
2:Output: L:d-subspace in RD
3:⌃(0)=ID/D
4:fork=1,2,...do
5: Z(k) PN
i=1xix>
i/(x>
i(⌃(k 1)) 1xi)
6: [U(k),S(k),U(k)] EVD (Z(k))
7:  i [S(k)]iiand d+1,D PD
i=d+1 i/(D d)
8: eS(k) diag(  1,...,  d, · d+1,D,...,  · d+1,D),
9: ⌃(k) U(k)eS(k)(U(k))>/tr 
U(k)eS(k)(U(k))> 
10: Stop if k Kork⌃(k) ⌃(k 1)kF<⌧
11:end for
12:L=S p a no ft h ef i r s t dcolumns of U(k)
2.4. Computational Complexity
Setting w(k)
i=(x>
i(⌃(k 1)) 1xi) 1,w ec a ne x p r e s s Z(k)
asZ(k)=eXeX>,w h e r e eX=[ (w(k)
1)1/2x1,...,(w(k)
N)1/2xN].
With some abuse of notation we denote by  1,. . .,  Dthe
eigenvalues of ⌃(k 1)(and not ⌃(k)). Since they are scaled to
have trace 1,  d+1,D=( 1 Pd
j=1 j)/(D d).W et h u so n l y
need the top deigenvectors and top deigenvalues of ⌃(k 1)to
update ew(k)
i.T h e r e f o r e ,t h ec o m p l e x i t yo fS T Ec a nb eo fo r d e r
O(KNDd )if a special fast algorithm is utilized for computing
only the top deigenvectors.
2.5. Implementation Details
STE depends on the parameters K,⌧and and the initialization
of⌃(0).T h ef i r s tt w op a r a m e t e r sa r er a t h e rs t a n d a r di ni t e r a t i v e
procedures and do not raise any concern.
Our theory sheds some light on possible choices of  and in
particular it indicates that the algorithm can be more sensitive to
choices of  when the quantity defined later in (3)is relatively
small. In this case, it may be beneficial to try several values of  .
We propose here a constructive way of doing it. We first form
as e q u e n c eo f 0< 1,e . g . ,  k=1/k,k=1,...,m.I no r d e rt o
14577
determine the best choice of  ,w ec o m p u t et h ed i s t a n c eo fe a c h
data point xto each subspace Lk,c o r r e s p o n d i n gt ot h ec h o i c e
of k,w h e r e dist( x,Lk)=kx PLkxk.W es e ts e tat h r e s h o l d
⇣,o b t a i n e db yt h em e d i a na m o n ga l lp o i n t sa n da l ls u b s p a c e s
and for each subspace, Lk,w ec o u n tt h en u m b e ro ft h ei n l i e r s
with distance below this threshold. The best  kis determined
according to the subspace yielding the largest number of inliers.
We describe this procedure in Algorithm 2.
For simplicity, we initialize with ⌃(0)=ID/Dand note that
with this choice ⌃(1)reflects the trimmed covariance matrix
and thus reflects the PCA subspace. One can also initialize
with TME or other subspaces (see §3where the theory of STE
is discussed). One can further try several initialization (with
possible random components) and use a strategy similar to
Algorithm 2to choose the best one.
At last, we remark that when computing Z(k)we want to
ensure that x>
i(⌃(k 1)) 1xicannot be zero and we thus add
the arbitrarily small number 10 15to this value.
Algorithm 2 Estimating best  for STE
1:Input: X=[x1,...,xN]2RD⇥N:c e n t e r e dd a t am a t r i x , d:
subspace dimension, { 1,...,  m}:as e to fp r e - s e l e c t e d  ’s.
2:Output:  ⇤:o p t i m a l  among { 1,...,  m}
3:forj=1,2,...,m do
4: L(j) STE( X,d,  j)
5: D(j) {dist( xi,L(j))|xi2X} .
6:end for
7:Set⇣=median( {D(1),. . . ,D(m)})
8:j⇤=argmax1jm|D(j)<⇣|
9: ⇤= j⇤
2.6. STE fuses TME and a Variant of FMS
STE is formally similar to both TME and FMS. Indeed,
at each iteration these algorithms essentially compute
⌃(k+1)=PN
i=1wixix>
i,w h e r e wi⌘wi 
⌃(k) 
.W e s u m m a -
rize the formal weights for FMS (with any choice of pfor
minimizing an `penergy in [ 24]), TME and STE. We ignore
an additional scaling constant for TME and STE, obtained
by dividing wixix>
iabove by its trace, and a regularization
parameter  for FMS. We express these formulas using the
eigenvalues  1,. . .,  Dand eigenvectors u1,. . .,uDof the
weighted sample covariance,PN
i=1wixix>
ifor each method
and := · d+1,D(see ( 2)) as follows:
wFMS
i=1
⇣PD
j=d+1(x>
iuj)2⌘(2 p)/2,
wTME
i=1PD
j=1  1
j(x>
iuj)2,
wSTE
i=1
Pd
j=1  1
j(x>
iuj)2+  1PD
j=d+1(x>
iuj)2.These weights aim to mitigate the impact of outliers in
different ways. For FMS,PD
j=d+1(x>
iuj)2is the squared
distance of a data point xito the subspace L.T h u sf o r p<2,
wFMS
iis smaller for “subspace-outliers”, where the robustness
to such outliers increases when p 0decreases.
The weights of TME are inversely proportional to the
squared Mahalanobis distance of xito the empirical distribution.
They mitigate the effect of “covariance-outliers”. If the dataset
is concentrated on a k-subspace where k<d ,t h e nT M E
can provide smaller weights to points lying away from this
subspace, unlike FMS that does not distinguish between points
within the larger d-subspace.
We note that the weights of STE fuse the above two weights.
Within a d-subspace, they use the shape of the data. They can
thus avoid outliers within this d-subspace. Within the orthogonal
component of this subspace, they use a term proportional to
that of FMS with p=0.W er e m a r kt h a ts u c h `0minimization
has a clear interpretation for RSR, though is generally hard to
guarantee. Indeed, [ 24]h a sn og u a r a n t e e sf o rF M Sw i t h p=0.
It can also yield unwanted spurious stationary points [ 26].
3. Theory
We review a theoretical guarantee for STE, whose proof is given
in [28]. It requires some conditions and we verify they hold
with high probability under the asymptotic generalized haystack
model. We assume a noiseless inliers-outliers RSR model. Let
L⇤denote the underlying d-subspace in RD,Xin=X\L⇤and
Xout=X\ X inbe the set of inliers and outliers, respectively, and
n1=|Xin|andn0=|Xout|be the number of inliers and outliers.
Our first assumption is a mild one on how well-conditioned the
inliers are in L⇤(compare e.g., other assumptions in [ 25,32]).
Assumption 1: Any k-subspace of L⇤,1kd,c o n t a i n sa t
most n1k/dpoints.
Motivation for Assumption 2: The ratio of inliers per
outliers, n1/n0,i nR S Ri so f t e nr e f e r r e dt oa st h eS N R
(signal-to-noise ratio) [ 25,32,33]. The smaller it is, the best
the subspace recovery is. We define the dimension-scaled SNR
(DS-SNR) as the SNR obtained when scaling n1andn0by
their respective dimensions (of L⇤andL?
⇤):
DS-SNR :=n1/d
n0/(D d). (3)
Zhang [ 50]s h o w e dt h a te x a c tr e c o v e r yb yT M Ei sg u a r a n t e e d
whenever DS-SNR >1(assuming general position assumptions
on the inliers and outliers) and Hardt and Moitra [ 16]s h o w e d
that when considering general datasets with general position
assumptions on the inliers and outliers, the RSR problem is
SSE hard if the DS-SNR is lower than 1.W ea i mt os h o wt h a t
under the following weaker generic condition, STE can obtain
exact recovery with DS-SNR, strictly lower than 1.
Assumption 2: DS-SNR > ,w h e r e  <1is the STE parameter.
Our last assumption requires a sufficiently good initialization
for STE, but also implicitly involves additional hidden
14578
assumptions on the inliers and outliers. This is expected, since
Assumption 1 does not require anything from the outliers and
also has a very weak requirement from the inliers. To formulate
the new assumption we define below some some basic condition
numbers for good initialization (which are more complicated
than the one for initialization by PCA suggested by [ 33]a n d
[32]) and also quantities similar to the ones used to guarantee
landscape stability in the theory of RSR [ 25,27,33,51].
Definitions required for Assumption 3: Recall that ⌃(0)
denotes the initial value in Algorithm 1,a n dd e n o t e
⌃(0)
L1,L2=U>
L1⌃(0)UL2.
We define the following condition number
1= d⇣
⌃(0)
L⇤,L⇤ ⌃(0)
L⇤,L?
⇤⌃(0) 1
L?⇤,L?⇤⌃(0)
L?⇤,L⇤⌘
 1⇣
⌃(0)
L?⇤,L?⇤⌘.
To get a better intuition to this primary quantity of Assumption
3, we first express the initial estimator ⌃(0),u s i n gb a s i sv e c t o r s
forL⇤andL?
⇤,a sa 2⇥2block matrix
 
⌃(0)
L⇤,L⇤⌃(0)
L⇤,L?⇤
⌃(0)
L?⇤,L⇤⌃(0)
L?⇤,L?⇤!
.
Defining ⌃0=⌃(0)
L⇤,L?⇤⌃(0) 1
L?⇤,L?⇤⌃(0)
L?⇤,L⇤,w ed e c o m p o s et h i s
block matrix as
 
⌃0⌃(0)
L⇤,L?⇤
⌃(0)
L?⇤,L⇤⌃(0)
L?⇤,L?⇤,!
+ 
⌃(0)
L⇤,L⇤ ⌃00
00!
.
We note that the numerator of 1is the d-th eigenvalue of the
second matrix in the above sum. We show in [ 28]t h a tt h i s
eigenvalue is positive if ⌃(0)is positive definite, which can be
easily enforced. The condition number is thus the ratio between
the smallest positive eigenvalue of the second matrix of the sum
and the largest eigenvalue of the component of the first matrix
associated with L?
⇤.T h e r e f o r e , 1expresses a ratio between
aq u a n t i f i e ro fa d-dimensional component of ⌃(0),a s s o c i a t e d
with L⇤,a n daq u a n t i f i e ro ft h ep r o j e c t i o no n t o L?
⇤of a full
rank component of ⌃(0).
We also define ⌃in,⇤as the TME solution to the set of the
projected inliers {UL⇤x|x2X in}⇢Rdand the following two
condition numbers
2= 1⇣
⌃(0)
L?⇤,L?⇤⌘
 D(⌃(0))andin= 1(⌃in,⇤)
 d(⌃in,⇤).
We note that inis analogous to the condition number in
(25) of [ 32], where we replace the sample covariance by the
TME estimator. An analog to the alignment of outliers statistic
[27,33]f o rS T Ei s
A=   X
x2Xoutxx>
kUL?⇤xk2   .An analog to the stability statistic [ 27,33]f o rS T Ei s
S= d+1,D⇣X
x2Xxx>
kxk2⌘
,
where  d+1,D(X)was defined in ( 2).
Assumption 3: There exists C=C( ,DS-SNR )>0such that
1 CdinA
n1 
in+A
n1
d  n0
D d+2A
 S(1+in)!
.(4)
The exact technical requirement on Cis specified in [ 28]. In
general, the larger the RHS of (4),t h em o r er e s t r i c t e dt h ec h o i c e
of⌃(0)is. In particular, when 1=1,t h ed e f i n i t i o no f 1im-
plies that Im(⌃(0))=L⇤,s ot h es u b s p a c ei sa l r e a d yr e c o v e r e d
by the initial estimate. Therefore, reducing the lower bound
of1may allow some flexibility, so a marginally suboptimal
initialization could still work out. In [ 28], we show that under
the asymptotic generalized haystack model, Assumption 3 can
be interpreted as an upper bound on the largest principal angle
between the initial and ground truth subspaces.
Generic Theory: The next theorem suggests that under
assumptions 1-3, STE nicely converges to an estimator that
recovers L⇤.T h e m a i n s i g n i f i c a n c e o f t h i s t h e o r y i s t h a t
its assumptions can allow DS-SNR lower than 1 for special
instances of datasets (for which the assumptions hold), unlike
the general recovery theories of [ 16]a n d[ 50].
Theorem 1. Under assumptions 1-3, the sequence ⌃(k)gen-
erated by STE converges to UL⇤⌃in,⇤U>
L⇤,t h eT M Es o l u t i o n
for the set of inliers Xin.I na d d i t i o n ,l e t L(k)be the subspace
spanned by the top deigenvectors of ⌃(k),t h e nt h ea n g l e
between L(k)andL⇤,\(L(k),L⇤)=c o s 1(kU>
L(k)UL⇤k),
converges r-linearly to zero.
We discuss insights of this theory on choices of the
algorithms and further verify the above stated advantage of STE
over TME assuming a common probabilistic model.
Choice of  for subspace recovery: In order to avoid too
large lower bound for 1in(4),w h i c hw em o t i v a t e da b o v e ,i ti s
good to find ✏1and✏2>0,s u c ht h a t  lies in (✏1,DS-SNR  ✏2)
(to notice this, observe the terms involving  in the denomina-
tors of the last two additive terms in (4)). We thus note that if
the DS-SNR is expected to be sufficiently larger than 1, we can
use, e.g.,  =0.5,b u tw h e nt h eD S - S N Rc a nb ec l o s et o1o r
lower (e.g., in fundamental matrix estimation), it is advisable
to choose small values of  according to Algorithm 2and their
sizes may depend on the expected value of the DS-SNR.
Possible ways of Initialization: If one expects an initial es-
timated subspace ˆLto have a sufficiently small angle ✓withL⇤,
where ✓=\(ˆL,L ⇤),t h e nf o r ⌃(0):= ⇧ˆL+✏Iit can be shown
that1>O(1/(✏+✓))and2<O(1+✓
✏).T h u so n em a yu s ea
trusted RSR method, e.g., FMS. As discussed in §2.5,t h ec h o i c e
⌃(0)=I(or a scaled version of it) corresponds to ˆLbeing the
14579
PCA subspace (obtained at iteration 1). Also, using the TME
solution for ⌃(0)corresponds to using the TME subspace as ˆL.
Theory under a probabilistic model: We show that under
ac o m m o np r o b a b i l i s t i cm o d e l ,t h ea s s u m p t i o n so fT h e o r e m
1,w h e r e ⌃(0)is obtained by TME, hold. Moreover, we show
that STE (initialized by TME) can recover the correct subspace
in situations with DS-SNR <1,w h e r e a sT M Ec a n n o tr e c o v e r
the underlying subspace in such cases. We follow [ 33]a n d
study the Generalized Haystack Model, though for simplicity,
we assume Gaussian instead of sub-Gaussian distributions and
an asymptotic setting. We assume n1inliers i.i.d. sampled from
aG a u s s i a nd i s t r i b u t i o n N(0,⌃(in)/d),w h e r e ⌃(in)2S+(D)
andL⇤=I m ( ⌃(in))(so⌃(in)hasdnonzero eigenvalues),
andn0outliers are i.i.d. sampled from a Gaussian distribution
N(0,⌃(out)/D),w h e r e ⌃(out)/D2S++(D).W e d e f i n e t h e
following condition numbers of inliers (in L⇤)a n do u t l i e r s :
in= 1(⌃(in))
 d(⌃(in))andout= 1(⌃(out))
 D(⌃(out)).
Clearly, Assumption 1 holds under this model, and
Assumption 2 constrains some of its parameters. Our next
theorem shows that Assumption 3 holds under this model when
the initial estimate ⌃(out)for STE is obtained by TME. It also
shows that in this case STE can solve the RSR problem even
when DS-SNR <1,u n l i k eT M E .F o rs i m p l i c i t y ,w ef o r m u l a t e
the theory for the asymptotic case, where N!1and the
theorem holds almost surely. It is possible to formulate it for
av e r yl a r g e Nwith high probability, but it requires stating
complicated constants depending on various parameters.
Theorem 2. Assume data generated from the above generalized
haystack model. Assume further that for 0<µ< 1,w h i c h
can be arbitrarily small, d<(1 µ)D 2.T h e n , f o r a n y
chosen 0<c0<1,w h i c hi sal o w e rb o u n df o r  ,t h e r ee x i s t s
⌘:=⌘(in,out,c0,µ)<1such that if DS-SNR  ⌘and⌃(0)is
obtained by TME, then Assumption 3 for ⌃(0)is satisfied with
c0< <⌘  c0almost surely as N!1.C o n s e q u e n t l y , t h e
output of the STE algorithm, initialized by TME and with the
choice of c0< <⌘  c0,r e c o v e r s L⇤.O n t h e o t h e r h a n d ,i f
⌃(out)
L⇤,L?⇤6=0and DS-SNR <1,t h e nt h et o p deigenvectors of
TME do not recover L⇤.
There are three different regimes that the theorem covers.
When DS-SNR  1,b o t hT M E + S T E( i . e . ,S T Ei n i t i a l i z e d
by TME) and TME solve the RSR problem. When ⌘DS-
SNR <1,T M E + S T Es o l v e st h eR S Rp r o b l e ma n dT M E
generally fails. When  DS-SNR <⌘,T M E + S T Em i g h ta l s o
fail, but STE with extremely good initialization (that satisfies
Assumption 3) can still solve the problem.
To get a basic idea of the dependence of ⌘on its parameters,
we remark that ⌘!1if either c0!0,in!1,out!1
orµ!0,w h e r et h ep a r a m e t e r µis somewhat artificial and
might be removed with a tighter proof. Therefore, successfulperformance of TME+STE requires a DS-SNR that is close to 1
when  is close to either 0or⌘(so that c0is very small) or when
either the inlier or outlier distribution is highly non-symmetric,
that is, either inoroutis large.
4. Applications to Structure from Motion
We apply STE to problems relevant to SfM: robust estimation
of fundamental matrices (see §4.1), and initial screening of
undesirable cameras (see § 4.2).
4.1. Robust Fundamental Matrix Estimation
Fundamental matrix estimation from noisy and inexact keypoint
matches is a core computer vision problem. It provides a
challenging setting for applying RSR methods.
We review this setting as follows. Let (x,x0)2R3⇥R3
be a correspondence pair of two points in different images
that are projections of the same 3D point in the scene, where
xand x0are expressed by homogeneous coordinates of
planar points. The fundamental matrix F2R3⇥3relates these
corresponding points and the epipolar lines they lie on as
follows: x0>Fx=0[17], or equivalently,
vec(F)·vec(xx0>)=0. (5)
where vec(·)denotes the vectorized form of a matrix. Therefore,
ideally, the set of all vectors in R9of the form vec(xx0>),w h e r e
(x,x0)2R3⇥R3is a correspondence pair, lies on an 8-subspace
inR9and its orthogonal complement yields the fundamental
matrix. In practice, the measurements of correspondence pairs
can be highly corrupted due to poor matching. Moreover, some
choices of correspondence pairs and the corruption mechanism
may lead to concentration on low-dimensional subspaces within
the desired 8-subspace. Furthermore, the corruption mechanism
can lead to nontrivial settings of outliers. Lastly, since d=8and
D=9,t h et h e o r e t i c a lt h r e s h o l do f[ 16]t r a n s l a t e st oh a v i n gt h e
fraction of inliers among all data points be at least 8/9⇡88.9%.
Therefore, this application is often a very challenging setting
for direct RSR methods. The best performing RSR methods
to date for fundamental matrix estimation are variants of
RANSAC [ 10]. RANSAC avoids any subspace-modeling as-
sumptions, but estimates the susbspace based on testing myriads
of samples, each having 7 or 8 point correspondences [ 17].
We test the performance of STE in estimating the fundamen-
tal matrix on the Photo Tourism database [ 41], where the image
correspondences are obtained by SIFT feature similarities [ 30].
We compare STE with the following 3 top RSR performers
according to [ 25]: FMS [ 24], spherical FMS (SFMS) [ 24]a n d
TME [ 47,50]. We also compared with vanilla RANSAC [ 10]
and two of its specialized extensions, which are state-of-the-art
performers for estimating fundamental matrices: locally
optimized RANSAC (LO-RANSAC) [ 6]a n dd e g e n e r a c y - c h e c k
enabled RANSAC (DEGENSAC) [ 7]. For the RSR methods
we used codes from the supplementary material of [ 25]
14580
Figure 1. Median (relative) rotation errors obtained by seven algorithms for the 14 datasets of Photo Tourism.
with their default options. We further used the Python
package pydegensac for implementing LO-RANSAC and
DEGENSAC with the inlier threshold ⌘=0.75.F o rS T E ,w e
used Algorithm 2to estimate the best  from {(2i) 1}5
i=1.
We measure the accuracy of the results according to the me-
dian and mean errors of relative rotation and direction vectors
directly obtained by the fundamental matrices for each method.
For computing these errors, we compared with ground-truth
values provided by [ 41,48]. Figure 1describes the result of the
mean errors for relative rotation per dataset of Photo Tourism,
where the other three errors and mAA( 10 )a r ei nt h es u p p l e m e n -
tal material. STE is significantly better than top RSR performers
(TME, FMS and SFMS). Overall, it appears that STE performs
better than vanilla RANSAC, except for the Ellis Island and
Vienna Cathedral datasets, where RANSAC outperforms STE.
STE is still competitive when compared with LO-RANSAC and
DEGENSAC, except for Notre Dame and the latter two datasets.
4.2. Initial Camera Removal for SfM
We propose a novel application of RSR for SfM and test STE for
this application. Even though our framework is not sufficiently
practical at this point, it allows testing STE in a different setting
where N=Dis very large and d=6.O u ri d e ai st ou s eR S R
within the SfM pipeline right after estimating the fundamental
matrices, in order to remove some cameras that result in inaccu-
rate estimated fundamental matrices. The hope is that eventually
such methods may reduce corruption and speed up the costly
later computationally intensive stages of the global SfM pipeline.
There are two main reasons to question such a process. One
may first question the gain in improving accuracy. Indeed,
since the rest of the pipeline already identifies corrupted
pairwise measurements, this process may not improve accuracy
and may even harm it as it removes whole cameras and not
pairs of cameras. That is, it is possible that a camera, which
results in bad pairwise measurement, also contributes to some
other accurate pairwise estimates that can improve the overall
accuracy. The second concern is in terms of speed. In general,the removal of cameras may result in higher or comparable
speed. Indeed, the LUD global pipeline [ 36], which we follow,
examines the parallel rigidity of the viewing graph and extracts
the maximal parallel rigid subgraph. Thus earlier removal
of cameras may worsen the parallel rigidity of the graph and
increase the computation due to the need of finding a maximal
parallel rigid subgraph. For example, [ 40]r e m o v e sc a m e r a si n
an earlier stage of the LUD pipeline, but results in higher com-
putational cost than the LUD pipeline. Therefore, improvement
of speed for the LUD pipeline by removing cameras is generally
non-trivial. Moreover, currently we use scale factors obtained by
first running LUD, so we do not get a real speed improvement.
Nevertheless, the proposed method is insightful whenever it may
indicate clear improvement in accuracy for a dataset, since one
can then infer that the current pipeline is not effective enough
in handling corrupted measurements, which can be easily
recognized by a simple method. Furthermore, improvement
in “speed” can be indicative of maintaining parallel rigidity.
Our RSR formulation is based on a fundamental observation
by Sengupta et al. [ 39]o nt h el o w - r a n ko ft h e n-view essential
(or fundamental) matrix. The n-view essential matrix Eof
size3n⇥3nis formed by stacking all n
2 
essential matrices,
while being appropriately scaled. That is, the ij-th block of
Eis the essential matrix for the i-th and j-th cameras, where
each Eijis scaled by a factor  ijin accordance with the global
coordinate system (see [ 20,21,39]). It was noticed in [ 39]t h a t
Ehas rank 6. Moreover, [ 39]c h a r a c t e r i z e dt h es e to f n-view
essential matrices whose camera centers are not all collinear by
the satisfaction of a few algebraic conditions, where the major
one is rank( E)=6.F u r t h e re x p l a n a t i o na p p e a r si n[ 20].
We propose a straightforward application of RSR, utilizing
these ideas to initially eliminate cameras that introduce
significant corruption to the essential matrices. For this purpose,
we compute the essential matrices (by computing first the
fundamental matrices and then using the known camera
calibration) and scale each matrix according to the factor
obtained by the LUD pipline [ 36]( n o t et h a tt h i si st h ei n i t i a l
14581
Figure 2. Mean (absolute) rotation errors (in degrees, left) and mean translation errors (in degrees, right) of LUD and four RSR methods used
to initially screen bad cameras within LUD applied to the 14 datasets of Photo Tourism.
scaling applied in [ 20,21,39]b e f o r ea p p l y i n gan o n - c o n v e x
and nontrivial optimization procedure that refines such scales).
Using these appropriately scaled essential matrices, we form the
n-view essential matrix Eof size 3n⇥3n.W ed e n o t et h e 3n⇥3
column blocks of EbyE:,1,...,E:,n(since Eis symmetric they
are the same as the row blocks transposed). We treat Eas a data
matrix with D=N=3n,w h e r et h ec o l u m n so f Eare the data
points. We apply RSR with d=6,r e c o v e ra d-dimensional ro-
bust subspace and identify the outlying columns whose distance
is largest from this subspace. To avoid heuristic methods for the
cutoff of outliers we assume a fixed percentage of 20% outlying
columns. If a column block, E:,icontains an outlying column,
we remove its corresponding camera i.C o n s e q u e n t l y ,as m a l l e r
percentage of cameras (about 10 15%)w i l lb ee l i m i n a t e d .
We use the Photo Tourism database [ 41]w i t hp r e c o m p u t e d
pairwise image correspondences provided by [ 39]( t h e yw e r e
obtained by thresholding SIFT feature similarities). To compute
scale factors for the essential matrices we use the output of
the LUD pipeline [ 36]a sf o l l o w s( f o l l o w i n ga ni d e ap r o p o s e d
in [39]f o ri n i t i a l i z i n gt h e s ev a l u e s ) :G i v e nt h ee s s e n t i a lm a t r i x
for cameras iandjcomputed at an early stage of our pipeline,
Eij,a n dt h eo n eo b t a i n e db yt h ef u l lL U Dp i p e l i n e , ELUD
ij,t h e
scaling factor is  ij=hEij,[ELUD
ij]i/k[ELUD
ij]k2
F.S i n c e m a n y
values of Eijare missing, we also apply matrix completion.
We compare the LUD pipeline with the LUD pipeline com-
bined with the filtering processes achieved by STE, FMS, SFMS,
and TME. For STE we fix  =1/3,t h o u g ha n yo t h e rv a l u ew e
tried yielded the same result. We report both mean and median
errors of rotations and translations and runtime of the standard
LUD and the RSR+LUD methods with initial screening of cam-
eras. Figure 2shows the mean rotation and translation errors,
where the rest of the figures and a summarizing table are in the
supplementary material. In general, STE demonstrates slightly
higher accuracy compared to other RSR methods. Improved
accuracy is particularly notable when matrix completion is not
utilized, as demonstrated in the supplementary material. We
observe that LUD+STE generally improves the estimation of
camera parameters (both rotations and translations) over LUD.The improvement of LUD+STE is noticeable in Roman Forum
and Gendarmenmarkt. In the supplementary material we show
further improvement for Gendarmenmarkt with the removal of
45% outlying columns. While the resulting errors are still large,
their improvement shows some potential in dealing with difficult
SfM structure by initially removing cameras in a way that may
help eliminate some scene ambiguities, which are prevalent in
Gendarmenmarkt. In terms of runtime, both LUD+STE and
LUD+SFMS demonstrate significant improvements, where
LUD+SFMS is even faster than LUD+STE. While this does
not yet imply faster handling of the datasets (as we use initial
scaling factors obtained by LUD), it indicates the efficiency of
the removal of outliers in maintaining parallel rigidity.
5. Conclusions
We introduce STE, a meticulously crafted adaptation of
TME designed to address challenges within RSR. Theoretical
guarantees demonstrate its ability to recover the true underlying
subspace reliably, even with a smaller fraction of inliers
compared to the well-known theoretical threshold. Under the
generalized haystack model, we show that this initialization
can be chosen as TME itself, leading to improved handling of
as m a l l e rf r a c t i o no fi n l i e r sc o m p a r e dt oT M E .O u re x p l o r a t i o n
extends to practical applications, where STE proves effective
in two 3D vision tasks: robust fundamental matrix estimation
and screening of bad cameras for improved SfM.
Several avenues for future research include: •Exploring
adaptations of other robust covariance estimation methods to
RSR. •Studying effective initialization for STE both in theory
and in practice. •In-depth theoretical exploration of the optimal
choice of the parameter  .•Study of alternative ways of adapt-
ing TME to RSR problems. •Improving STE for fundamental
matrix estimation following ideas similar to those in [ 7,12,37]
for addressing challenging degeneracies. •Enhancing our initial
idea of initial removal of bad cameras, specifically attempting
to use it to rectify challenging scene ambiguities. •Testing our
methods for SfM using more recent feature matching algorithms.
14582
References
[1]Larry P. Ammann. Robust singular value decompositions: A
new approach to projection pursuit. Journal of the American
Statistical Association ,8 8 ( 4 2 2 ) : p p .5 0 5 – 5 1 4 ,1 9 9 3 . 1
[2]Daniel Barath, Jiri Matas, and Jana Noskova. MAGSAC:
marginalizing sample consensus. In Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition ,p a g e s1 0 1 9 7 – 1 0 2 0 5 ,2 0 1 9 . 2
[3]Jian-Feng Cai, Emmanuel J. Cand `es, and Zuowei Shen. A
singular value thresholding algorithm for matrix completion.
SIAM Journal on optimization ,2 0 ( 4 ) : 1 9 5 6 – 1 9 8 2 ,2 0 1 0 . 17
[4]Yeshwanth Cherapanamjeri, Prateek Jain, and Praneeth Netrapalli.
Thresholding based outlier robust PCA. In Conference on
Learning Theory ,p a g e s5 9 3 – 6 2 8 .P M L R ,2 0 1 7 . 1
[5]Vartan Choulakian. L1-norm projection pursuit principal
component analysis. Computational Statistics & Data Analysis ,
50(6):1441–1451, 2006. 1
[6]Ondˇrej Chum, Ji ˇr´ıM a t a s ,a n dJ o s e fK i t t l e r . L o c a l l yo p t i m i z e d
RANSAC. In Pattern Recognition: 25th DAGM Symposium,
Magdeburg, Germany, September 10-12, 2003. Proceedings 25 ,
pages 236–243. Springer, 2003. 2,6,13
[7]Ondrej Chum, Tomas Werner, and Jiri Matas. Two-view geom-
etry estimation unaffected by a dominant plane. In 2005 IEEE
Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR’05) ,p a g e s7 7 2 – 7 7 9 .I E E E ,2 0 0 5 . 2,6,8,13
[8]Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry
Li, Ankur Moitra, and Alistair Stewart. Robustly learning a
gaussian: Getting optimal error, efficiently. In Proceedings of
the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete
Algorithms ,p a g e2 6 8 3 – 2 7 0 2 ,U S A ,2 0 1 8 .S o c i e t yf o rI n d u s t r i a l
and Applied Mathematics. 1
[9]Chris Ding, Ding Zhou, Xiaofeng He, and Hongyuan Zha.
R1-PCA: rotational invariant L1-norm principal component
analysis for robust subspace factorization. In ICML ’06:
Proceedings of the 23rd international conference on Machine
learning ,p a g e s2 8 1 – 2 8 8 ,N e wY o r k ,N Y ,U S A ,2 0 0 6 .A C M . 1
[10] Martin A. Fischler and Robert C. Bolles. Random sample
consensus: a paradigm for model fitting with applications to
image analysis and automated cartography. Communications
of the ACM ,2 4 ( 6 ) : 3 8 1 – 3 9 5 ,1 9 8 1 . 1,2,6,13
[11] Gabriel Frahm and Uwe Jaekel. A generalization of Tyler’s
M-estimators to the case of incomplete data. Computational
Statistics & Data Analysis ,5 4 ( 2 ) : 3 7 4 – 3 9 3 ,2 0 1 0 . 2
[12] J-M Frahm and Marc Pollefeys. RANSAC for (quasi-)
degenerate data (QDEGSAC). In 2006 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition
(CVPR’06) ,p a g e s4 5 3 – 4 6 0 .I E E E ,2 0 0 6 . 2,8
[13] William Cole Franks and Ankur Moitra. Rigorous guarantees
for Tyler’s M-estimator via quantum expansion. In Conference
on Learning Theory ,p a g e s1 6 0 1 – 1 6 3 2 .P M L R ,2 0 2 0 . 3
[14] Jerome H. Friedman and John W. Tukey. A projection pursuit
algorithm for exploratory data analysis. IEEE Transactions on
Computers ,C - 2 3 ( 9 ) : 8 8 1 – 8 9 0 ,1 9 7 4 . 1
[15] John Goes, Gilad Lerman, and Boaz Nadler. Robust sparse
covariance estimation by thresholding Tyler’s M-estimator. The
Annals of Statistics ,4 8 ( 1 ) : 8 6–1 1 0 ,2 0 2 0 . 3[16] Moritz Hardt and Ankur Moitra. Algorithms and hardness for
robust subspace recovery. In Conference on Learning Theory ,
pages 354–375. PMLR, 2013. 1,2,3,4,5,6
[17] Richard Hartley and Andrew Zisserman. Multiple view geometry
in computer vision .C a m b r i d g eu n i v e r s i t yp r e s s ,2 0 0 3 . 6,11,12,
13
[18] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,
Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Image matching
across wide baselines: From paper to practice. International
Journal of Computer Vision ,1 2 9 ( 2 ) : 5 1 7 – 5 4 7 ,2 0 2 1 . 13
[19] Arman Karimian and Roberto Tron. Essential matrix estimation
using convex relaxations in orthogonal space. In Proceedings
of the IEEE/CVF International Conference on Computer Vision ,
pages 17142–17152, 2023. 15
[20] Yoni Kasten, Amnon Geifman, Meirav Galun, and Ronen
Basri. Algebraic characterization of essential matrices and their
averaging in multiview settings. In Proceedings of the IEEE/CVF
International Conference on Computer Vision ,p a g e s5 8 9 5 – 5 9 0 3 ,
2019. 7,8
[21] Yoni Kasten, Amnon Geifman, Meirav Galun, and Ronen
Basri. Gpsfm: Global projective sfm using algebraic constraints
on multi-view fundamental matrices. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition ,p a g e s3 2 6 4 – 3 2 7 2 ,2 0 1 9 . 7,8
[22] John T. Kent and David E. Tyler. Maximum likelihood estimation
for the wrapped Cauchy distribution. Journal of Applied
Statistics ,1 5 ( 2 ) : 2 4 7 – 2 5 4 ,1 9 8 8 . 2,3
[23] Nojun Kwak. Principal component analysis based on L1-norm
maximization. IEEE transactions on pattern analysis and
machine intelligence ,3 0 ( 9 ) : 1 6 7 2 – 1 6 8 0 ,2 0 0 8 . 1
[24] Gilad Lerman and Tyler Maunu. Fast, robust and non-convex
subspace recovery. Information and Inference: A Journal of the
IMA,7 ( 2 ) : 2 7 7 – 3 3 6 ,2 0 1 8 . 1,2,3,4,6,13
[25] Gilad Lerman and Tyler Maunu. An overview of robust subspace
recovery. Proceedings of the IEEE ,1 0 6 ( 8 ) : 1 3 8 0 – 1 4 1 0 ,2 0 1 8 .
1,3,4,5,6
[26] Gilad Lerman and Teng Zhang. lp-recovery of the most
significant subspace among multiple subspaces with outliers.
Constr. Approx. ,4 0 ( 3 ) : 3 2 9 – 3 8 5 ,2 0 1 4 . 4
[27] Gilad Lerman, Michael B. McCoy, Joel A. Tropp, and Teng
Zhang. Robust computation of linear models by convex
relaxation. Found. Comput. Math. ,1 5 ( 2 ) : 3 6 3 – 4 1 0 ,2 0 1 5 . 1,3,5
[28] Gilad Lerman, Feng Yu, and Teng Zhang. Theoretical guarantees
for the subspace-constrained Tyler’s estimator, 2024. 4,5,11
[29] Guoying Li and Zhonglian Chen. Projection-pursuit approach
to robust dispersion matrices and principal components: primary
theory and monte carlo. Journal of the American Statistical
Association ,8 0 ( 3 9 1 ) : 7 5 9 – 7 6 6 ,1 9 8 5 . 1
[30] David G. Lowe. Distinctive image features from scale-invariant
keypoints. International journal of computer vision ,6 0 : 9 1 – 1 1 0 ,
2004. 6
[31] Ricardo A. Maronna and V ´ıctor J. Yohai. Robust estimation
of multivariate location and scatter. Wiley StatsRef: Statistics
Reference Online ,p a g e s1 – 1 2 ,2 0 1 4 . 2
[32] Tyler Maunu and Gilad Lerman. Robust subspace recovery with
adversarial outliers, 2019. 1,2,4,5
14583
[33] Tyler Maunu, Teng Zhang, and Gilad Lerman. A well-tempered
landscape for non-convex robust subspace recovery. J. Mach.
Learn. Res. ,2 0 ( 1 ) : 1 3 4 8 – 1 4 0 6 ,2 0 1 9 . 1,3,4,5,6
[34] Tyler Maunu, Chenyu Yu, and Gilad Lerman. Stochastic and
private nonconvex outlier-robust PCAs. In Proceedings of
Mathematical and Scientific Machine Learning ,p a g e s1 7 3 – 1 8 8 .
PMLR, 2022. 1
[35] Michael McCoy and Joel A. Tropp. Two proposals for robust
PCA using semidefinite programming. Electronic Journal of
Statistics ,5 ( n o n e ) : 1 1 2 3–1 1 6 0 ,2 0 1 1 . 1
[36] Onur Ozyesil and Amit Singer. Robust camera location
estimation by convex programming. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,p a g e s
2674–2683, 2015. 7,8
[37] Rahul Raguram, Ondrej Chum, Marc Pollefeys, Jiri Matas, and
Jan-Michael Frahm. USAC: A universal framework for random
sample consensus. IEEE transactions on pattern analysis and
machine intelligence ,3 5 ( 8 ) : 2 0 2 2 – 2 0 3 8 ,2 0 1 2 . 2,8
[38] Elvezio M. Ronchetti and Peter J. Huber. Robust statistics .J o h n
Wiley & Sons Hoboken, NJ, USA, 2009. 1
[39] Soumyadip Sengupta, Tal Amir, Meirav Galun, Tom Goldstein,
David W Jacobs, Amit Singer, and Ronen Basri. A new
rank constraint on multi-view fundamental matrices, and its
application to camera location recovery. In Proceedings of the
IEEE conference on computer vision and pattern recognition ,
pages 4798–4806, 2017. 7,8,16
[40] Yunpeng Shi, Shaohan Li, Tyler Maunu, and Gilad Lerman.
Scalable cluster-consistency statistics for robust multi-object
matching. In International Conference on 3D Vision, 3DV 2021,
London, United Kingdom, December 1-3, 2021 ,p a g e s3 5 2 – 3 6 0 .
IEEE, 2021. 7
[41] Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo
tourism: exploring photo collections in 3d. In ACM siggraph
2006 papers ,p a g e s8 3 5 – 8 4 6 .A s s o c i a t i o nf o rC o m p u t i n g
Machinery, 2006. 6,7,8
[42] Nathan Srebro and Tommi Jaakkola. Weighted low-rank approx-
imations. In Proceedings of the 20th international conference
on machine learning (ICML-03) ,p a g e s7 2 0 – 7 2 7 ,2 0 0 3 . 1
[43] Jacob Steinhardt, Moses Charikar, and Gregory Valiant.
Resilience: A criterion for learning in the presence of arbitrary
outliers. In 9th Innovations in Theoretical Computer Science
Conference, ITCS 2018, January 11-14, 2018, Cambridge, MA,
USA,p a g e s4 5 : 1 – 4 5 : 2 1 .S c h l o s sD a g s t u h l-L e i b n i z - Z e n t r u m
f¨ur Informatik, 2018. 1
[44] Ben Tordoff and David W. Murray. Guided sampling and con-
sensus for motion estimation. In Computer Vision—ECCV 2002:
7th European Conference on Computer Vision Copenhagen,
Denmark, May 28–31, 2002 Proceedings, Part I 7 ,p a g e s8 2 – 9 6 .
Springer, 2002. 15
[45] Philip H.S. Torr and Andrew Zisserman. MLESAC: A new robust
estimator with application to estimating image geometry. Com-
puter vision and image understanding ,7 8 ( 1 ) : 1 3 8 – 1 5 6 ,2 0 0 0 . 2
[46] David E. Tyler. Statistical analysis for the angular central gaussian
distribution on the sphere. Biometrika ,7 4 ( 3 ) : 5 7 9 – 5 8 9 ,1 9 8 7 . 2
[47] David E. Tyler. A distribution-free m-estimator of multivariate
scatter. The Annals of Statistics ,p a g e s2 3 4 – 2 5 1 ,1 9 8 7 . 1,2,6,13[48] Kyle Wilson and Noah Snavely. Robust global translations
with 1dsfm. In Computer Vision–ECCV 2014: 13th European
Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part III 13 ,p a g e s6 1 – 7 5 .S p r i n g e r ,2 0 1 4 . 7
[49] Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust
PCA via outlier pursuit. Advances in neural information
processing systems ,2 3 ,2 0 1 0 . 1
[50] Teng Zhang. Robust subspace recovery by Tyler’s M-estimator.
Information and Inference: A Journal of the IMA ,5 ( 1 ) : 1 – 2 1 ,
2016. 1,3,4,5,6,13
[51] Teng Zhang and Gilad Lerman. A novel M-estimator for robust
PCA. The Journal of Machine Learning Research ,1 5 ( 1 ) :
749–808, 2014. 1,3,5
14584
