SceneFun3D:
Fine-Grained Functionality and Affordance Understanding in 3D Scenes
Alexandros Delitzas1Ayc ¸a Takmaz1Federico Tombari2,3Robert Sumner1
Marc Pollefeys1,4Francis Engelmann1,2
1ETH Zurich2Google3TU Munich4Microsoft
scenefun3d.github.io
High-resolution laser scanFunctional interactive
element annotationsNatural language
task descriptionsMotion
annotations
"Turn on the
ceiling light"/g2927 
"Change the
room's
temperature using
the radiator's
thermostat""Open the fridge"
tip_push
rotatehook_pull
Figure 1. The SceneFun3D dataset. We introduce the ﬁrst large-scale 3D dataset with ﬁne-grained functional interaction annotations
in real-world indoor spaces. SceneFun3D contains more than 14.8k annotations of functional interactive elements in 710 high-ﬁdelity
3D scenes accompanied by 9 affordance categories. Additionally, it provides motion annotations that describe how to interact with the
functional elements and diverse natural language descriptions of tasks that involve manipulating them in the scene context.
Abstract
Existing 3D scene understanding methods are heavily fo-
cused on 3D semantic and instance segmentation. How-
ever, identifying objects and their parts only constitutes an
intermediate step towards a more ﬁne-grained goal, which
is effectively interacting with the functional interactive ele-
ments ( e.g., handles, knobs, buttons) in the scene to accom-
plish diverse tasks. To this end, we introduce SceneFun3D,
a large-scale dataset with more than 14.8k highly accu-
rate interaction annotations for 710 high-resolution real-
world 3D indoor scenes. We accompany the annotations
with motion parameter information, describing how to in-
teract with these elements, and a diverse set of natural lan-
guage descriptions of tasks that involve manipulating them
in the scene context. To showcase the value of our dataset,
we introduce three novel tasks, namely functionality seg-
mentation, task-driven affordance grounding and 3D mo-
tion estimation, and adapt existing state-of-the-art methodsto tackle them. Our experiments show that solving these
tasks in real 3D scenes remains challenging despite recent
progress in closed-set and open-set 3D scene understanding
methods.
1. Introduction
Datasets of 3D indoor environments have been extensively
used for computer vision, robotics, embodied AI and mixed
reality [ 5,7,11]. To perceive 3D environments, 3D instance
segmentation has served as a fundamental task to provide
object-level knowledge to agents enabling scene interaction.
Going a step further, a line of work [ 46,49] has studied the
task of part-object segmentation focusing on the lower-level
object parts, e.g., the drawers of a cabinet. However, these
two tasks serve only as a proxy since in the real-world set-
ting, agents need to successfully detect and interact with the
functional interactive elements (e.g., knobs, handles, but-
tons) of the objects in the scene. Detecting these elements
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14531
is an under-explored area, mainly due to the fact that most
existing datasets, which are based on commodity RGB-D
reconstructions, often fail to accurately capture the 3D ge-
ometry of small details in the scene (see Fig. 2).
To successfully interact with functional elements in a
scene, agents should be capable of understanding visual af-
fordances . The concept of affordance was ﬁrst deﬁned by
Gibson [ 22], as those actions or behaviors afforded due to
the physical structure and design of an object ( e.g., a button
affords pressing, a drawer knob affords pulling). To encour-
age research in this direction, prior works [ 13,72] create 3D
datasets which include dense label annotations on the object
parts of 3D CAD models in the PartNet dataset [ 49]. While
these datasets are very helpful for identifying affordances
on the object-level, there is no dataset providing geometri-
cally ﬁne-grained annotations of visual affordances in real-
world 3D scenes, to the best of our knowledge.
Although the Gibsonian notion [ 22] of affordance might
be sufﬁcient in object-level, it lacks to inform about the pur-
pose or the speciﬁc function of an interactive element in
the context of a scene. 3D environments are characterized
by complex inter- and intra-object functional relationships.
For instance, if an agent is instructed to turn on the ceiling
light, knowing that the buttons of the scene can be pressed
(Gibsonian affordance) does not offer enough information
about what will happen when the button is pressed. To ad-
dress this limitation, Pustejovsky [ 55] introduced the notion
oftelic affordance which is deﬁned as the action or behav-
ior conventionalized due to an object’s typical use or pur-
pose [ 26]. For example, while the Gibsonian affordance of
a light switch button is that it can be pressed, its telic affor-
dance is turning on the ceiling light. Interestingly, we see
that recent open-vocabulary models [ 17,38,54,63] display
promising results towards understanding telic affodances of
functionalities in 3D scenes by leveraging the knowledge of
foundation models, such as CLIP [ 58] and OpenSeg [ 21].
However, there is no benchmark to assess and compare their
visual affordance understanding capability.
In this work, we build the ﬁrst large-scale dataset,
namely SceneFun3D (Fig. 1), containing more than 14.8k
high-ﬁdelity annotations of functional interactive elements
in 3D scenes along with nine Gibsonian-inspired affor-
dances. These are complemented by accurate motion pa-
rameters, outlining how to manipulate these elements and
diverse natural language descriptions of tasks that involve
interacting with them. With the introduction of this dataset
we hope to encourage future research on the following ques-
tions.
Where are the functionalities located in 3D indoor envi-
ronments and what actions they afford? We construct a
dataset of 710 scenes captured with a Faro laser scanner by
leveraging the ARKitScenes data assets [ 5]. This provides
us with high-resolution 3D geometry, compared to previ-
Faro Laser Data RGB-D 3D Reconstruction
Figure 2. Details in laser scans and RGB-D reconstructions.
Laser scans capture a higher level of detail, which is required for
the geometrically ﬁne-grained annotation of small interactive ele-
ments. In datasets with commodity-level RGB-D reconstructions
(e.g., ScanNet [ 11], MultiScan [ 46], Matterport [ 7]) these details
are not visible.
ous datasets that comprise commodity-level RGB-D recon-
structions, which is essential to capture small interactive el-
ements in the scene (Fig. 2). We develop a lightweight web-
based interface that enables the ﬁne-grained annotation of
high-resolution point clouds. We utilize it to collect anno-
tations of the functional interactive elements of the scenes
accompanied by a Gibsonian-inspired affordance category
(e.g., rotate, hook pull, tip push).
What purpose do the functionalities serve in the scene
context? We argue that telic affordances are crucial to un-
derstand the purpose of interactive elements in the scene
and propose a natural approach to study them. We show
functionality annotations to human annotators and ask them
to provide free-form language descriptions of tasks that in-
volve interacting with the displayed functionality ( e.g., the
task description “turn on the ceiling light” involves inter-
acting with the light switch). To the best of our knowledge,
we are the ﬁrst to link Gibsonian and telic affordances for
enhanced 3D scene understanding.
Which motions are involved to interact with the func-
tional elements? To further investigate how an agent can
interact with the functional elements in the scene, we col-
lect 3D motion annotations. For example, pressing a but-
ton involves a motion perpendicular to the button’s surface
to be performed, while opening a cabinet’s door involves
a rotational motion around its hinge by pulling its handle.
In contrast to previous works [ 42,46] that focus on the
motion of articulated parts for a limited set of object cat-
egories, we study motions from the actor’s perspective with
an interaction-centric approach which better resembles the
real-world setting.
We introduce three challenging tasks and leverage our
SceneFun3D dataset for systematic benchmarking. We pro-
pose techniques to adapt state-of-the-art methods on closed-
set and open-set 3D scene understanding to tackle the pro-
posed tasks and perform extensive experiments.
14532
2. Related Work
Semantic understanding of indoor 3D scenes. Existing
3D indoor datasets [ 5,7,11,59,76] primarily focus on
recognizing scene semantics and object instances. Bene-
ﬁting from these large-scale annotated datasets, a plethora
of methods for 3D scene understanding including seman-
tic segmentation [ 1,3,9,30–33,41,43,44,56,57,65,
66,69,74], instance segmentation [ 15,16,24,27,36,40,
61,64,67,78], ﬂoorplan reconstruction [ 4,77] and panop-
tic segmentation [ 10,39] have been developed. While
these 3D segmentation models work well for recognizing
object classes, they fall short on providing detailed ob-
ject interaction information necessary in real-world appli-
cations, such as robotics [ 80]. Earlier large-scale 3D indoor
datasets like S3DIS [ 2], ScanNet [ 11] or Matterport3D [ 7]
leverage consumer-grade RGB-D reconstructions, whereas
more recent efforts including ARKitScenes [ 5] and Scan-
Net++ [ 76] employ professional laser scanners, which yield
high-ﬁdelity reconstructions that even capture small geo-
metrical details such as handles and knobs. In this work,
we focus on identifying those small functional elements and
how to interact with them.
Affordance understanding. Understanding scene affor-
dances has been a long-standing goal in vision and robotics.
Prediction of affordances has been ﬁrst explored through
the lens of rule-based approaches [ 70]. Subsequently, sev-
eral learning-based methods have addressed the prediction
of functionalities from images and videos [ 14,19,45,52],
and in 3D [ 13,50,51,60,71,72]. Another line of work tar-
gets language grounding in 3D scenes [ 8,12,53,79], and
3D scene understanding guided by open-vocabulary queries
[38,54,63]. However, existing methods are largely limited
to point-level or object-level predictions. Our dataset fo-
cuses speciﬁcally on functional interactive elements, and
provides a benchmark consisting of a rich set of natural
language task descriptions. This enables the extension of
object-level open-vocabulary approaches, such as Open-
Mask3D [ 63], to identify ﬁne-grained functional elements
based on complex affordance descriptions.
3D motion estimation. A line of work [ 28,29,35,37,42,
46,62,68,73,75] explores the estimation of 3D motion
and mobility of interactable elements. MultiScan [ 46] fo-
cuses on scenes with articulated objects, and estimates ob-
ject part mobility. OPD [ 35] and OPDMulti [ 62] address
openable part detection and motion parameter estimation.
Hsu et al. [28] explore the inference of articulation prop-
erties of scene objects. These datasets focus on articulated
objects and address a limited set of interaction categories.
In our work, we instead study 3D motion estimation from
an interaction-centric perspective, addressing a larger vari-
ety of interaction cases.3. Task Deﬁnitions
We address three novel 3D scene understanding tasks:
Task 1: Functionality segmentation. Given an input point
cloudP={(pi,fi)}, wherepi∈R3are the point coordi-
nates and fiare the additional point features, such as RGB
color and normals, the task is to predict the instance masks
{mi}K
i=1of the functional interactive elements of the scene
as well as the associated affordance label {ℓi}K
i=1for each
instance, where Kis the number of instances in the scene.
We deﬁne CGibsonian-inspired affordance categories to
describe interactions afforded by common functionalities in
indoor scenes ( e.g., “rotate”). We highlight that this task is
conceptually different from traditional 3D instance segmen-
tation. The model needs to understand visual affordances in
an object-class-agnostic manner and infer the action that a
functional element affords from the 3D geometry. We con-
sider functional interactive elements as the object compo-
nents in the scene that humans and agents interact with to
perform speciﬁc actions ( e.g., turning a handle to open a
door, rotating a dial to control the temperature).
Task 2: Task-driven affordance grounding. Given an in-
put point cloud Pand a free-form task description D(e.g.,
“open the door”, “turn on the ceiling light”), the goal is to
predict the instance mask mof the functional interactive el-
ement referred to by the task description as well as the asso-
ciated affordance label ℓ. To tackle this task, models need to
display understanding of the telic affordance, i.e., the pur-
pose, of the functionalities in the context of the scene.
Task 3: Motion estimation. Given an input point cloud P,
this task complements Task 1 and aims to identify the mo-
tion parameters {ϕi}K
i=1, which describe the action that the
agent should perform, to interact with the predicted func-
tionality. Following the same notation as [ 35,46,62], we
represent the motion parameters as ϕi={ti,ai,oi}, where
ti={rotation,translation }is the motion type, ai∈R3is
the motion axis direction and oi∈R3is the motion origin.
To address this task, the model should be able to understand
how the functionality works and what motion is required to
interact with the corresponding functional element.
4. Building the SceneFun3D dataset
In this section, we describe our approach towards building
the SceneFun3D dataset.
4.1. Laser scans
To study the functional interactable elements in 3D scenes,
high-resolution point clouds are necessary so that the de-
tails and the 3D geometry are of high quality (Fig. 2). To
this end, we construct a dataset consisting of high-quality
3D indoor scenes by leveraging the data assets provided by
ARKitScenes [ 5] as follows.
14533
ARKitScenes provides multiple laser scans per scene
(four on average) by placing a Faro Focus S70 laser scanner
in different positions in the scene. We use the provided laser
scanner’s poses for each scene and combine the laser scans
under the same coordinate system to increase the scene cov-
erage. Afterwards, we downsample the combined laser scan
with a voxel size of 5mm, which is sufﬁcient to preserve the
details of the functional interactive elements of the scene
(e.g., small buttons, knobs, handles, etc.), while enabling
processing by machine learning models.
Next, we visually verify the output XYZRGB point
cloud. We exclude scenes where the laser scanner’s poses
are incorrect, small scenes without interaction spots as well
as scenes for which high-resolution RGB frames are not
available. After following this selection process, we con-
struct a dataset of 710 scenes. We highlight that our pipeline
is scalable, presenting the potential to expand the number of
scenes by leveraging high-resolution datasets released con-
currently with our work, such as ScanNet++ [ 76].
4.2. RGB images and camera poses
We accompany the scans from our dataset with posed RGB
images and video. ARKitScenes provides on average three
video sequences for each scene recorded with a 2020 iPad
Pro. These video sequences come with RGB images from
the Wide and Ultra Wide cameras, depth maps from the on-
device LiDAR scanner, ARKit camera trajectory as well as
an ARKit mesh reconstruction of the scene based on the
low-resolution frames. However, the aforementioned iPad
data and the Faro laser scans are expressed in a different co-
ordinate system and the transformations are not provided by
ARKitScenes. To enable scene understanding with multi-
ple sensor data, we register the laser scans in the coordinate
system of the RGB images as described in Sec. 4.3.
Furthermore, the camera poses in the ARKit camera tra-
jectory are not synced with the iPad’s RGB frames. To
help the registration process as well as utilization in down-
stream tasks, we extract and provide accurate camera poses
for each frame by performing rigid body motion interpola-
tion inSO(3)×R3[23].
4.3. Registration and alignment
We perform a series of steps to register the laser scans in
the coordinate system of the iPad’s camera poses. First,
we reconstruct a high-resolution point cloud using the
high-resolution RGB-D frames and the interpolated camera
poses. This high-resolution point cloud is used as a proxy
for registering the laser scan in the coordinate system of the
camera poses. To help the registration process, we remove
extraneous points from the laser scan due to transparent sur-
faces, such as windows, by using the DBSCAN clustering
algorithm [ 18]. Consequently, we align the laser scan to
the proxy point cloud using Predator [ 34] and then reﬁneLabel Description
rotate functionalities that are adjusted by a ro-
tary switch knob, e.g. thermostat
keypress surfaces that consist of keys that can be
pressed, e.g. remote control, keyboard
tippush functionalities that can be triggered by
the tip of the ﬁnger, e.g. light switch
hookpull surfaces that can be pulled by hooking
up ﬁngers, e.g. fridge handle
pinchpull surfaces that can be pulled through a
pinch movement, e.g. drawer knob
hookturn surfaces that can be turned by hooking
up ﬁngers, e.g. door handle
footpush surfaces that can be pushed by foot, e.g.
foot pedal of a trash can
plugin surfaces that comprise electrical power
sources
unplug removing a plug from a socket
Table 1. Affordance label descriptions.
the alignment by performing Multi-Scale Iterative Closest
Point (ICP). As a ﬁnal step, we visually inspect the align-
ment by projecting the color of the RGB frames to the laser
scan. In rare cases when the registration result is not suc-
cessful, we use manual correspondences for initialization.
4.4. Semantic annotation and data collection
For the data collection process, we have created a
lightweight web-based tool to facilitate the ﬁne-grained an-
notation on large and dense point clouds. Our tool presents
three main advantages compared to existing open-source
tools which were used for annotating existing datasets [ 5,
7,11,46,76]. First, previous works annotate decimated
meshes after performing over-segmentation [ 20] which re-
duces the annotation accuracy. Instead, we directly anno-
tate the high-resolution point cloud and allow an annota-
tion accuracy of up to a single 3D point. Second, our tool
enables the annotation of high-resolution laser scans with
minimum hardware requirements (no GPU required). To do
this, we utilize an accelerated ray-casting algorithm based
on Bounding V olume Hierarchies (BVH) [ 47]. More specif-
ically, we group the 3D points into bounding volumes in a
recursive fashion which speeds up the spatial queries signif-
icantly during the annotator’s clicks. Lastly, during annota-
tion, annotators can see videos of the scene. This not only
helps annotators to identify the scene functionalities and af-
fordances more accurately and faster but also provides fur-
ther information which might not be clearly visible in the
3D point cloud.
Functionalities. We use our annotation UI to collect anno-
tations of the functional interactive elements in the scenes
which include an instance mask as well as an affordance
14534
hook_pull hook_turn
key_press plug_in
tip_push
unplug
rotate
pinch_pull
foot_push
Figure 3. Examples of functional interactive element annotations illustrating the nine affordance categories in SceneFun3D.
label (Fig. 3). We compile a list of nine Gibsonian-
inspired affordance labels, drawing inspiration from prior
research [ 13,25,45], to represent the interaction with com-
mon functional interactable elements of in indoor environ-
ments. A short description of each label can be found in
Tab. 1. Annotators are tasked with detecting functionalities
in the scene, selecting the affordance label that describes
the interaction with the functionality and then annotate the
instance mask. To facilitate the annotation process, they are
allowed to freely navigate in the 3D scene using our UI’s
controls as well as watch the video sequences of the scene.
Additionally, we annotate functionalities whose geome-
try or the parent object’s geometry is not well-represented
in the laser scans. This may occur in cases where the func-
tional part ( e.g. a knob, handle, etc.) or the parent object
(e.g. a fridge) is built of a reﬂective material. We catego-
rize these samples under the label “exclude” and we exclude
these cases from evaluation in our experiments.
Natural language task descriptions. To study the telic af-
fordance or purpose of the collected functionalities in the
scene context, we collect natural language descriptions of
tasks that involve interacting with the corresponding func-
tionalities (Fig. 4). First, functionalities are displayed to
annotators in the 3D scene. We ask them to provide natu-
ral language descriptions for tasks that uniquely involve the
displayed functionality annotation. For example, if the dis-
played functionality is a light switch under the affordance
category “tip push”, then the associated task descriptioncould be “Turn on the ceiling light”. We omit collecting
descriptions for functionalities whose purpose is not clear
in the context of the scene ( e.g., buttons on an unknown
electronic device).
Inspired by [ 79], we augment our collected language de-
scriptions by rephrasing them to increase diversity. We uti-
lize the ChatGPT model gpt-3.5-turbo-instruct for sentence
rephrasing. During the veriﬁcation phase, we ensure the
rephrased task descriptions are well-written and correctly
correspond to the functionalities in the scene context.
3D Motions. We collect the motions (Fig. 5) needed to in-
teract with the annotated functionalities as follows. Initially,
functionality annotations are displayed to annotators in the
3D scene. By observing the high-quality 3D point cloud as
well as the associated scene videos, human annotators can
easily infer the motion required to interact with the element.
For each functional interactive element, the annotators se-
lect the motion type (translational or rotational), the motion
axis origin by selecting a point in the scene as well as the
motion axis direction by setting the direction of the 3D vec-
tor using our UI’s helper tools.
5. The SceneFun3D Dataset
We present statistics on the SceneFun3D dataset concerning
the scenes, functionality annotations, collected language
task descriptions and motion annotations.
Train/Validation/Test splits. Following the standard prac-
tice, we split our data into training, validation and test splits.
14535
"Open the top oven door"
"Turn on the TV using 
the remote on the table"
"Open the window"
"Adjust the toaster's
intensity"
hook_turn
key_press rotatehook_pull
Figure 4. Examples of collected natural language task descriptions.
Figure 5. Examples of 3D motion annotations.
Since the test set of ARKitScenes is not publicly available,
we use the scenes in its validation set as our test set. To con-
struct the validation set, we randomly draw scenes from the
training set of ARKitScenes and use the rest as the training
split. Overall, our dataset consists of 545, 80 and 85 scenes
for training, validation and testing respectively.
Dataset statistics. Our dataset offers the total of 14,867
annotations of functional interactive elements along with
their affordance class for 710 scenes. Furthermore, we pro-
vide motion annotations for 14,279 interactive elements out
of which 8325 require translational motion and 6542 rota-
tional motion. Last, we offer natural language task descrip-
tions for 10,913 interactive elements. After the automated
rephrasing augmentation process, we receive 6,220 addi-
tional descriptions, which results in the total of 17,133. We
refer the reader to the supplementary material for additional
statistics.
6. Baselines and Experiments
We leverage the SceneFun3D dataset to introduce bench-
marks for the novel tasks of functionality segmentation,
task-driven affordance grounding and 3D motion estima-
tion. For each task, we ﬁrst describe the baselines and eval-
uation metrics and then we show quantitative and qualitative
results on our test set. For further implementation details,
we refer the reader to the supplementary material.
6.1. Functionality segmentation
For this task, we adapt two state-of-the-art methods for
3D object instance segmentation, Mask3D [ 61] and Soft-
Group [ 67]. We also report results on the open-vocabulary
LERF model [ 38].Method AP AP 50AP25
SoftGroup-F 3.6 8.4 17.2
LERF [ 38] 4.8 12.3 18.1
Mask3D-F 7.9 18.3 26.6
Table 2. Quantitative results on functionality segmentation.
Mask3D-F. Since instance masks of the functional interac-
tive elements are smaller in size than the masks of object
instances, state-of-the-art methods on object instance seg-
mentation do not work well out-of-the-shelf. As a ﬁrst step,
we substitute the distribution-based BCE loss in the loss
function with a region-based Dice loss which can handle
better the background/foreground class imbalance. We train
Mask3D with the overall loss Lseg=¼diceLdice+¼ceLce,
whereLdiceis the dice loss to supervise the masks and Lce
is the multi-label cross entropy loss.
SoftGroup-F. Following [ 67], we ﬁrst train the U-Net back-
bone on the semantic masks. Similar to Mask3D-F, we sub-
stitute the cross-entropy loss with a weighted multi-class
dice loss Lm-dice . We train the backbone using the com-
bined loss Lbackbone=¼m-diceLm-dice+¼offsetLoffset, where
Loffsetis the offset loss [ 67] used to supervise the offset vec-
tors. Next, we freeze the backbone and train the top-down
reﬁnement module on the instance masks. For this stage,
we use the combined loss L=Lbackbone+Ltop-down . We
utilize the loss Ltop-down=¼ceLce+¼diceLdice+¼scoreLscore,
whereLceis the multi-label cross-entropy loss, Ldiceis the
dice loss and Lscoreis the mask score loss [ 67].
LERF. LERF [ 38] is a method for grounding language em-
beddings from CLIP [ 58] into NeRF [ 48], enabling open-
ended language queries in 3D. We evaluate the zero-shot
capabilities of LERF on our dataset.
14536
Mask3D-F instance prediction Ground-truth affordance masks Input Point Cloud Mask3D-F affordance prediction Ground-truth instance masks
pinch_pull pinch_pull
hook_turn hook_turn hook_turn hook_turn
Figure 6. Qualitative results on the Mask3D-F predictions for the task of functionality segmentation.
Coarse-to-ﬁne learning based on a curriculum. For train-
ing Mask3D-F and SoftGroup-F, we propose a curriculum
learning technique to boost the performance. Since these
methods were originally designed to work with larger in-
stance masks of objects, they may struggle with detecting
the smaller masks of interactive elements in the scene. In-
spired by the concept of curriculum learning [ 6], we start
training with coarse instance masks, which are easier for
the network to detect. Then, during training, we gradu-
ally start feeding the network with more ﬁne-grained masks
closer to the ground-truth. To generate the coarser masks,
we expand the ground-truth instance masks by considering
all the points within a certain radius from the mask’s points.
Speciﬁcally, denoting the point cloud as P, the ground-truth
instance mask as Q, and the mask expansion radius as rn,
the expanded instance mask is calculated as follows
Qrn
expand={p| ∥p−q∥2< rn, p∈ P,q∈ Q} (1)
The mask expansion radius is gradually reduced during
training using step decay
rn=r0d+n
α,(2)
wherernis the mask expansion radius in epoch n,r0is the
initial expansion radius at the beginning of training, dis the
decay rate and ³is the decay interval. After rnbecomes
smaller than a threshold rthr, we disable mask expansion
and we reﬁne the network using the ground-truth masks.
Metrics. As our main metrics, we report the mean Average
Precision at the IoU thresholds of 0.25 and 0.50, AP25and
AP50respectively. We also report AP, the average over
different IoU thresholds from 0.5 to 0.95 with a step of 0.05.
Results. We report the performance on Tab. 2. We
observe that Mask3D-F achieves better performance than
SoftGroup-F on functionality segmentation, which is in ac-
cordance with previous ﬁndings on object instance segmen-
tation benchmarks such as ScanNet [ 11] and S3DIS [ 2].
LERF achieves better scores than SoftGroup-F but failsMethod AP50AP25
OpenMask3D [ 63] 0.0 0.0
LERF [ 38] 4.9 11.3
OpenMask3D-F 8.0 17.5
Table 3. Quantitative results on task-driven affordance grounding.
to match the performance of Mask3D-F. Furthermore, all
methods are effective at segmenting distinctive elements
such as handles that are easily observable but struggle with
very small structures, such as knobs on an electrical device.
In Fig. 6, we show qualitative results on the Mask3D-F se-
mantic and instance predictions.
We also perform an ablation study on the effect of the
initial mask expansion radius (Tab. 4, left). We observe that
setting the mask expansion radius to r0= 0.1leads to op-
timal performance and increasing it further does not yield
any performance gains. Our results demonstrate that if we
disable coarse-to-ﬁne training ( r0= 0) the model fails to
detect interactive elements in the scene.
r0AP50AP25
0.2 18.3 26.2
0.1 18.3 26.6
0.05 9.8 18.6
None 0.0 0.0kexpAP50AP25
0.1 4.5 11.2
0.5 8.3 16.2
1.0 8.0 17.5
2.0 8.0 16.5
Table 4. Ablation studies. Left: Initial mask expansion radius
(r0) used for coarse-to-ﬁne learning on Mask3D-F for the task
of functionality segmentation. Right: Expansion ratio ( kexp) pa-
rameter of OpenMask3D-F for the task of task-driven affordance
grounding.
6.2. Task­driven affordance grounding
For this task, we adapt OpenMask3D [ 63] to per-
form language-guided segmentation of functional elements,
driven by complex descriptions. We also use the LERF [ 38]
model as a baseline.
OpenMask3D-F. OpenMask3D [ 63] is an instance based
approach, which relies on object mask proposals obtained
from Mask3D [ 61]. This object-level representation does
14537
OpenMask3D-F prediction OpenMask3D-F prediction OpenMask3D-F prediction OpenMask3D prediction
"Open the left cabinet door under the sink" "Change the room's temperature using the radiator's
thermostat located next to the terrace door""Unplug the electric kettle from the power supply"
Figure 7. Qualitative results on the OpenMask3D-F predictions for the task of task-driven affordance grounding. OpenMask3D-F uses func-
tional element-level masks, which enables more ﬁne-grained segmentation compared to object-level approaches such as OpenMask3D [ 63].
Method AP25 +M +MA +MAO
Mask3D-FM (rgb) 26.6 23.8 9.8 7.9
Mask3D-FM (rgb + n) 26.5 24.0 10.2 8.1
Table 5. Quantitative results on motion estimation.
not allow the segmentation of functional elements such as
buttons, handles and switches. We extend OpenMask3D to
use mask proposals from our adapted Mask3D-F which pro-
poses masks for functional elements, and we refer to this
approach as OpenMask3D-F. For this task, we compute a
CLIP-based [ 58] embedding for each proposed functional
element-mask. Then we encode the task-description queries
for each scene using the CLIP text encoder. We measure
the similarity between the mask-embeddings and query-
embeddings, and retrieve the mask for a given description
text, if the similarity score is above a certain threshold. As
OpenMask3D relies on multi-scale image crops, it is sensi-
tive to the crop-expansion ratio, kexp(details in [ 63]). We
also experiment with varying kexpvalues to investigate its
effect on task-driven affordance grounding. This ablation
study is presented in Tab. 4(right).
Metrics. For this task, we use instance segmentation met-
rics, and report the AP50andAP25.
Results. Scores are presented in Tab. 3. We observe that
OpenMask3D-F outperforms LERF by a signiﬁcant mar-
gin. The results on OpenMask3D-F also highlight the im-
portance of having ﬁne-grained functional element masks in
order to successfully identify how a certain task can be per-
formed. Fig. 7shows qualitative results of OpenMask3D-F.
6.3. Motion estimation
Mask3D-FM. We extend the Mask3D-F baseline to addi-
tionally predict the per-instance motion parameters along
with the segmentation mask and affordance label. To this
end, we enhance the mask module [ 61] of the architecture
to jointly predict the motion type, motion axis and motion
origin by adding a per-instance prediction head for each mo-
tion parameter. For training, we utilize the overall loss of
L=Lseg+Lmotion , whereLmotion is a combined loss to su-
pervise the motion parameters, inspired by [ 35,62]. Specif-
ically, it is deﬁned as Lmotion=¼typeLtype+¼axisLaxis+¼originLorigin, whereLmotion is a cross-entropy loss for the
motion type, Laxisis a smooth L1 loss for the motion axis
andLorigin is a smooth L1 loss for the motion origin.
Metrics. To evaluate the motion prediction performance,
we follow [ 46,62] and extend the AP25metric for motion
parameter accuracy. More speciﬁcally, we further constrain
mask prediction by whether the model accurately predicted
the motion type ( +M), the motion type and the motion axis
direction ( +MA ) and the motion type, motion axis direction,
and motion origin ( +MAO ). We consider the motion axis
matched if the angle between ground-truth and predicted
axis does not exceed 15◦and the motion origin matched if
the minimum distance between the axis is lower than 0.25.
Results. Quantitative results can be seen on Tab. 5. We
report two variants of our baseline, one that uses only the
rgb color information of the point cloud and one that addi-
tionally uses the estimated normal information. We observe
that the normal information helps the model to predict the
motion parameters more accurately.
7. Conclusion
In this work, we present SceneFun3D, the ﬁrst large-scale
dataset that leverages laser scans to provide geometrically
ﬁne-grained masks along with affordance labels of func-
tional interactive elements in 3D real-world indoor scenes,
followed by motion parameter information and a diverse set
of natural language descriptions of tasks that require inter-
action with them. To investigate multi-task and holistic 3D
scene understanding, we introduce the three novel tasks of
functionality segmentation, task-driven affordance ground-
ing and 3D motion estimation. We adapt state-of-the-art
methods on closed-set and open-set 3D scene understand-
ing and report promising results. We believe that our dataset
will stimulate advancements in embodied AI, robotics and
realistic human-scene interaction modelling.
Acknowledgements. This work is supported by a Career
Seed Award funded by the ETH Zurich Foundation and In-
nosuisse grant (48727.1 IP-ICT). AD is supported with a
scholarship by HELLENiQ ENERGY . We thank Johanna
Wald and Maria Parelli for proofreading.
14538
References
[1] Abhishek Anand, Hema Swetha Koppula, Thorsten
Joachims, and Ashutosh Saxena. Contextually Guided Se-
mantic Labeling and Search for 3D Point Clouds. In Inter-
national Journal on Robotics Research (IJRR) , 2011. 3
[2] Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioan-
nis Brilakis, Martin Fischer, and Silvio Savarese. 3D Se-
mantic Parsing of Large-Scale Indoor Spaces. In Interna-
tional Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2016. 3,7
[3] Matan Atzmon, Haggai Maron, and Yaron Lipman. Point
Convolutional Neural Networks by Extension Operators.
ACM Transactions On Graphics (TOG) , 2018. 3
[4] Armen Avetisyan, Tatiana Khanova, Christopher Choy, Den-
ver Dash, Angela Dai, and Matthias Nießner. SceneCAD:
Predicting Object Alignments and Layouts in RGB-D Scans.
InEuropean Conference on Computer Vision (ECCV) , 2020.
3
[5] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry,
Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe,
Daniel Kurz, Arik Schwartz, et al. ARKitScenes: A Diverse
Real-world Dataset for 3D Indoor Scene Understanding Us-
ing Mobile RGB-D Data. International Conference on Neu-
ral Information Processing Systems (NeurIPS) , 2021. 1,2,
3,4
[6] Yoshua Bengio, J ´erˆome Louradour, Ronan Collobert, and Ja-
son Weston. Curriculum learning. In International Confer-
ence on Machine Learning (ICML) , 2009. 7
[7] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Nießner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-
D Data in Indoor Environments. In International Conference
on 3D Vision (3DV) , 2017. 1,2,3,4
[8] Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner.
ScanRefer: 3D Object Localization in RGB-D Scans using
Natural Language. In European Conference on Computer
Vision (ECCV) , 2020. 3
[9] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4D
Spatio-Temporal ConvNets: Minkowski Convolutional Neu-
ral Networks. In International Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , 2019. 3
[10] Manuel Dahnert, Ji Hou, Matthias Nießner, and Angela Dai.
Panoptic 3D Scene Reconstruction from a Single RGB Im-
age. In International Conference on Neural Information Pro-
cessing Systems (NeurIPS) , 2021. 3
[11] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. ScanNet:
Richly-Annotated 3D Reconstructions of Indoor Scenes. In
International Conference on Computer Vision and Pattern
Recognition (CVPR) , 2017. 1,2,3,4,7
[12] Alexandros Delitzas, Maria Parelli, Nikolas Hars, Geor-
gios Vlassis, Sotirios-Konstantinos Anagnostidis, Gregor
Bachmann, and Thomas Hofmann. Multi-clip: Contrastive
vision-language pre-training for question answering tasks in
3d scenes. In British Machine Vision Conference (BMVC) ,
2023. 3[13] Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, and
Kui Jia. 3D AffordanceNet: A Benchmark for Visual Object
Affordance Understanding. In International Conference on
Computer Vision and Pattern Recognition (CVPR) , 2021. 2,
3,5
[14] Thanh-Toan Do, Anh Viet Nguyen, Ian D. Reid, Dar-
win Gordon Caldwell, and Nikos G. Tsagarakis. Affor-
danceNet: An End-to-End Deep Learning Approach for Ob-
ject Affordance Detection. In International Conference on
Robotics and Automation (ICRA) , 2018. 3
[15] Cathrin Elich, Francis Engelmann, Theodora Kontogianni,
and Bastian Leibe. 3D-BEVIS: Birds-Eye-View Instance
Segmentation. In German Conference on Pattern Recogni-
tion (GCPR) , 2019. 3
[16] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian
Leibe, and Matthias Nießner. 3D-MPA: Multi Proposal Ag-
gregation for 3D Semantic Instance Segmentation. In Inter-
national Conference on Computer Vision and Pattern Recog-
nition (CVPR) , 2020. 3
[17] Francis Engelmann, Fabian Manhardt, Michael Niemeyer,
Keisuke Tateno, and Federico Tombari. OpenNeRF: Open
Set 3D Neural Scene Segmentation with Pixel-Wise Features
and Rendered Novel Views. In International Conference on
Learning Representations (ICLR) , 2024. 2
[18] Martin Ester, Hans-Peter Kriegel, J ¨org Sander, and Xiaowei
Xu. A Density-Based Algorithm for Discovering Clusters in
Large Spatial Databases with Noise. In International Con-
ference on Knowledge Discovery and Data Mining (KDD) ,
1996. 4
[19] Kuan Fang, Te-Lin Wu, Daniel Yang, Silvio Savarese, and
Joseph J. Lim. Demo2Vec: Reasoning Object Affordances
From Online Videos. In International Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2018. 3
[20] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efﬁcient
graph-based image segmentation. International Journal on
Computer Vision (IJCV) , 2004. 4
[21] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin.
Scaling Open-V ocabulary Image Segmentation with Image-
Level Labels. In European Conference on Computer Vision
(ECCV) , 2022. 2
[22] James J. Gibson. The Ecological Approach to Visual Percep-
tion: Classic Edition . Houghton Mifﬂin, 1979. 2
[23] Adrian Haarbach, Tolga Birdal, and Slobodan Ilic. Survey of
Higher Order Rigid Body Motion Interpolation Methods for
Keyframe Animation and Continuous-Time Trajectory Esti-
mation. In International Conference on 3D Vision (3DV) ,
2018. 4
[24] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. OccuSeg:
Occupancy-aware 3D Instance Segmentation. In Interna-
tional Conference on Computer Vision and Pattern Recog-
nition (CVPR) , 2020. 3
[25] Mohammed Hassanin, Salman Khan, and Murat Tahtali. Vi-
sual Affordance and Function Understanding: A Survey.
ACM Computing Surveys , 2021. 5
[26] Alexander Henlein, Anju Gopinath, Nikhil Krishnaswamy,
Alexander Mehler, and James Pustejovsky. Grounding
Human-object Interaction to Affordance Behavior in Mul-
14539
timodal Datasets. Frontiers in Artiﬁcial Intelligence , 2023.
2
[27] Ji Hou, Angela Dai, and Matthias Nießner. 3D-SIS: 3D Se-
mantic Instance Segmentation of RGB-D Scans. In Interna-
tional Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2019. 3
[28] Cheng-Chun Hsu, Zhenyu Jiang, and Yuke Zhu. Ditto in
the House: Building Articulation Models of Indoor Scenes
through Interactive Perception. In International Conference
on Robotics and Automation (ICRA) , 2023. 3
[29] Ruizhen Hu, Wenchao Li, Oliver Van Kaick, Ariel Shamir,
Hao Zhang, and Hui Huang. Learning to Predict Part Mo-
bility from a Single Static Snapshot. ACM Transactions On
Graphics (TOG) , 2017. 3
[30] Zeyu Hu, Xuyang Bai, Jiaxiang Shang, Runze Zhang, Ji-
ayu Dong, Xin Wang, Guangyuan Sun, Hongbo Fu, and
Chiew Lan Tai. VMNet: V oxel-Mesh Network for Geodesic-
Aware 3D Semantic Segmentation. In International Confer-
ence on Computer Vision (ICCV) , 2021. 3
[31] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Point-
wise Convolutional Neural Network. In International
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2018.
[32] Jing Huang and Suya You. Point Cloud Labeling Using 3D
Convolutional Neural Network. In International Conference
on Pattern Recognition (ICPR) , 2016.
[33] Rui Huang, Songyou Peng, Ayca Takmaz, Federico Tombari,
Marc Pollefeys, Shiji Song, Gao Huang, and Francis Engel-
mann. Segment3D: Learning Fine-Grained Class-Agnostic
3D Segmentation without Manual Labels. arXiv preprint
arXiv:2312.17232 , 2023. 3
[34] Shengyu Huang, Zan Gojcic, Mikhail Usvyatsov, and Kon-
rad Schindler Andreas Wieser. PREDATOR: Registration of
3D Point Clouds with Low Overlap. In International Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2021. 4
[35] Hanxiao Jiang, Yongsen Mao, Manolis Savva, and Angel X
Chang. OPD: Single-view 3D openable part detection. In
European Conference on Computer Vision (ECCV) , 2022. 3,
8
[36] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-
Wing Fu, and Jiaya Jia. PointGroup: Dual-Set Point Group-
ing for 3D Instance Segmentation. In International Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2020. 3
[37] Yuki Kawana, Yusuke Mukuta, and Tatsuya Harada. Un-
supervised Pose-aware Part Decomposition for Man-Made
Articulated Objects. In European Conference on Computer
Vision (ECCV) , 2022. 3
[38] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo
Kanazawa, and Matthew Tancik. LERF: Language Embed-
ded Radiance Fields. In International Conference on Com-
puter Vision (ICCV) , 2023. 2,3,6,7
[39] Lars Kreuzberg, Idil Esen Zulﬁkar, Sabarinath Mahadevan,
Francis Engelmann, and Bastian Leibe. 4D-STOP: Panop-
tic Segmentation of 4D Lidar using Spatio-Temporal Object
Proposal Generation and Aggregation. In European Confer-
ence on Computer Vision (ECCV) Workshops , 2022. 3[40] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Mar-
tin R. Oswald. 3D Instance Segmentation via Multi-task
Metric Learning. In International Conference on Computer
Vision and Pattern Recognition (CVPR) , 2019. 3
[41] Loic Landrieu and Martin Simonovsky. Large-scale Point
Cloud Semantic Segmentation with Superpoint Graphs. In
International Conference on Computer Vision and Pattern
Recognition (CVPR) , 2018. 3
[42] Xiaolong Li, He Wang, Li Yi, Leonidas Guibas, A. Lynn Ab-
bott, and Shuran Song. Category-Level Articulated Object
Pose Estimation. In International Conference on Computer
Vision and Pattern Recognition (CVPR) , 2020. 2,3
[43] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan
Di, and Baoquan Chen. PointCNN: Convolution on X-
transformed Points. In International Conference on Neural
Information Processing Systems (NeurIPS) , 2018. 3
[44] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
Convolutional Networks for Semantic Segmentation. In
International Conference on Computer Vision and Pattern
Recognition (CVPR) , 2015. 3
[45] Timo L ¨uddecke and F. W ¨org¨otter. Learning to Segment Af-
fordances. In International Conference on Computer Vision
(ICCV) Workshops , 2017. 3,5
[46] Yongsen Mao, Yiming Zhang, Hanxiao Jiang, Angel X
Chang, and Manolis Savva. MultiScan: Scalable RGBD
scanning for 3D environments with articulated objects. In
International Conference on Neural Information Processing
Systems (NeurIPS) , 2022. 1,2,3,4,8
[47] Daniel Meister, Shinji Ogaki, Carsten Benthin, Michael
Doyle, Michael Guthe, and Jiri Bittner. A Survey on Bound-
ing V olume Hierarchies for Ray Tracing. Computer Graph-
ics Forum , 2021. 4
[48] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing Scenes as Neural Radiance Fields for View
Synthesis. In International Conference on Computer Vision
and Pattern Recognition (CVPR) , 2020. 6
[49] Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna
Tripathi, Leonidas J. Guibas, and Hao Su. PartNet: A Large-
Scale Benchmark for Fine-Grained and Hierarchical Part-
Level 3D Object Understanding. In International Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2019. 1,2
[50] Kaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, and
Leonidas Guibas. O2O-Afford: Annotation-free large-scale
object-object affordance learning. In Conference on Robot
Learning (CoRL) , 2021. 3
[51] Tushar Nagarajan and Kristen Grauman. Learning Affor-
dance Landscapes for Interaction Exploration in 3D Environ-
ments. In International Conference on Neural Information
Processing Systems (NeurIPS) , 2020. 3
[52] Tushar Nagarajan, Christoph Feichtenhofer, and Kristen
Grauman. Grounded Human-Object Interaction Hotspots
from Video. In International Conference on Computer Vi-
sion (ICCV) , 2019. 3
[53] Maria Parelli, Alexandros Delitzas, Nikolas Hars, Geor-
gios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, and
14540
Thomas Hofmann. Clip-guided vision-language pre-training
for question answering in 3d scenes. In International Con-
ference on Computer Vision and Pattern Recognition (CVPR)
Workshops , 2023. 3
[54] Songyou Peng, Kyle Genova, Chiyu ”Max” Jiang, An-
drea Tagliasacchi, Marc Pollefeys, and Thomas Funkhouser.
OpenScene: 3D Scene Understanding with Open V ocabular-
ies. In International Conference on Computer Vision and
Pattern Recognition (CVPR) , 2023. 2,3
[55] James Pustejovsky. The Generative Lexicon . MIT press,
1998. 2
[56] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas.
PointNet: Deep Learning on Point Sets for 3D Classiﬁcation
and Segmentation. In International Conference on Computer
Vision and Pattern Recognition (CVPR) , 2017. 3
[57] Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Point-
Net++: Deep Hierarchical Feature Learning on Point Sets
in a Metric Space. In International Conference on Neural
Information Processing Systems (NeurIPS) , 2017. 3
[58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning Transferable Visual
Models From Natural Language Supervision. In Interna-
tional Conference on Machine Learning (ICML) , 2021. 2,
6,8
[59] David Rozenberszki, Or Litany, and Angela Dai. Language-
Grounded Indoor 3D Semantic Segmentation in the Wild. In
European Conference on Computer Vision (ECCV) , 2022. 3
[60] Manolis Savva, Angel X Chang, Pat Hanrahan, Matthew
Fisher, and Matthias Nießner. SceneGrok: Inferring Action
Maps in 3D Environments. ACM Transactions On Graphics
(TOG) , 2014. 3
[61] Jonas Schult, Francis Engelmann, Alexander Hermans, Or
Litany, Siyu Tang, and Bastian Leibe. Mask3D: Mask Trans-
former for 3D Semantic Instance Segmentation. In Inter-
national Conference on Robotics and Automation (ICRA) ,
2023. 3,6,7,8
[62] Xiaohao Sun, Hanxiao Jiang, Manolis Savva, and An-
gel Xuan Chang. OPDMulti: Openable Part Detection for
Multiple Objects. International Conference on 3D Vision
(3DV) , 2024. 3,8
[63] Ayc ¸a Takmaz, Elisabetta Fedele, Robert W. Sumner, Marc
Pollefeys, Federico Tombari, and Francis Engelmann. Open-
Mask3D: Open-V ocabulary 3D Instance Segmentation. In
International Conference on Neural Information Processing
Systems (NeurIPS) , 2023. 2,3,7,8
[64] Ayc ¸a Takmaz, Jonas Schult, Irem Kaftan, Mertcan Akc ¸ay,
Bastian Leibe, Robert Sumner, Francis Engelmann, and Siyu
Tang. 3D Segmentation of Humans in Point Clouds with
Synthetic Data. In International Conference on Computer
Vision (ICCV) , 2023. 3
[65] Lyne P. Tchapmi, Christopher B. Choy, Iro Armeni, JunY-
oung Gwak, and Silvio Savarese. SEGCloud: Semantic Seg-
mentation of 3D Point Clouds. In International Conference
on 3D Vision (3DV) , 2017. 3
[66] Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud,
Beatriz Marcotegui, Franc ¸ois Goulette, and Leonidas J.Guibas. KPConv: Flexible and Deformable Convolution for
Point Clouds. In International Conference on Computer Vi-
sion (ICCV) , 2019. 3
[67] Thang Vu, Kookhoi Kim, Tung M. Luu, Xuan Thanh
Nguyen, and Chang D. Yoo. SoftGroup for 3D Instance Seg-
mentation on 3D Point Clouds. In International Conference
on Computer Vision and Pattern Recognition (CVPR) , 2022.
3,6
[68] Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qin-
ping Zhao, and Kai Xu. Shape2Motion: Joint Analysis of
Motion Parts and Attributes from 3d Shapes. In Interna-
tional Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2019. 3
[69] Silvan Weder, Hermann Blum, Francis Engelmann, and
Marc Pollefeys. LabelMaker: Automatic Semantic Label
Generation from RGB-D Trajectories. In International Con-
ference on 3D Vision (3DV) , 2024. 3
[70] Patrick H. Winston. Learning Physical Descriptions from
Functional Deﬁnitions, Examples, and Precedents. In Asso-
ciation for the Advancement of Artiﬁcial Intelligence (AAAI) ,
1983. 3
[71] Ruihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian
Wang, Tianhao Wu, Qingnan Fan, Xuelin Chen, Leonidas
Guibas, and Hao Dong. V AT-Mart: Learning Visual Action
Trajectory Proposals for Manipulating 3D ARTiculated Ob-
jects. arXiv preprint arXiv:2106.14440 , 2021. 3
[72] Chao Xu, Yixin Chen, He Wang, Song-Chun Zhu, Yixin
Zhu, and Siyuan Huang. PartAfford: Part-level Affordance
Discovery from 3D Objects. In European Conference on
Computer Vision (ECCV) Workshops , 2022. 2,3
[73] Xianghao Xu, Yifan Ruan, Srinath Sridhar, and Daniel
Ritchie. Unsupervised Kinematic Motion Detection for Part-
segmented 3D Shape Collections. In ACM SIGGRAPH ,
2022. 3
[74] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao.
SpiderCNN: Deep Learning on Point Sets with Parameter-
ized Convolutional Filters. In European Conference on Com-
puter Vision (ECCV) , 2018. 3
[75] Zihao Yan, Ruizhen Hu, Xingguang Yan, Luanmin Chen,
Oliver Van Kaick, Hao Zhang, and Hui Huang. RPM-Net:
Recurrent Prediction of Motion and Parts from Point Cloud.
ACM Transactions On Graphics (TOG) , 2019. 3
[76] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner,
and Angela Dai. ScanNet++: A High-Fidelity Dataset of 3D
Indoor Scenes. In International Conference on Computer
Vision (ICCV) , 2023. 3,4
[77] Yuanwen Yue, Theodora Kontogianni, Konrad Schindler,
and Francis Engelmann. Connecting the Dots: Floorplan
Reconstruction Using Two-Level Queries. In International
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2023. 3
[78] Yuanwen Yue, Sabarinath Mahadevan, Jonas Schult, Francis
Engelmann, Bastian Leibe, Konrad Schindler, and Theodora
Kontogianni. AGILE3D: Attention Guided Interactive
Multi-object 3D Segmentation. In International Conference
on Learning Representations (ICLR) , 2023. 3
[79] Yiming Zhang, ZeMing Gong, and Angel X Chang.
Multi3DRefer: Grounding Text Description to Multiple 3D
14541
Objects. In International Conference on Computer Vision
(ICCV) , 2023. 3,5
[80] Ren ´e Zurbr ¨ugg, Yifan Liu, Francis Engelmann, Suryansh
Kumar, Marco Hutter, Vaishakh Patil, and Fisher Yu.
ICGNet: A Uniﬁed Approach for Instance-Centric Grasp-
ing. In International Conference on Robotics and Automa-
tion (ICRA) , 2024. 3
14542
