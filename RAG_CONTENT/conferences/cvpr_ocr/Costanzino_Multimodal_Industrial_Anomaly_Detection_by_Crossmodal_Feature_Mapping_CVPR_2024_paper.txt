Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping
Alex Costanzino∗Pierluigi Zama Ramirez∗Giuseppe Lisanti Luigi Di Stefano
CVLAB, Department of Computer Science and Engineering (DISI) – University of Bologna, Italy
https://cvlab-unibo.github.io/CrossmodalFeatureMapping/
Abstract
Recent advancements have shown the potential of lever-
aging both point clouds and images to localize anomalies.
Nevertheless, their applicability in industrial manufactur-
ing is often constrained by significant drawbacks, such as
the use of memory banks, which leads to a substantial in-
crease in terms of memory footprint and inference times.
We propose a novel light and fast framework that learns
to map features from one modality to the other on nominal
samples and detect anomalies by pinpointing inconsisten-
cies between observed and mapped features. Extensive ex-
periments show that our approach achieves state-of-the-art
detection and segmentation performance in both the stan-
dard and few-shot settings on the MVTec 3D-AD dataset
while achieving faster inference and occupying less memory
than previous multimodal AD methods. Furthermore, we
propose a layer pruning technique to improve memory and
time efficiency with a marginal sacrifice in performance.
1. Introduction
Industrial Anomaly Detection (AD) aims to identify un-
usual characteristics or defects in products, serving as a vital
component within quality inspection processes. Collecting
data to exemplify anomalies is challenging due to their rar-
ity and unpredictability. Therefore, most works focus on
unsupervised approaches, i.e., algorithms trained only on
samples without defects, also referred to as nominal sam-
ples. Currently, most existing AD methods are geared to-
ward analyzing RGB images. However, in many industrial
settings, anomalies are hard to recognize effectively based
solely on colour images, e.g., due to varying light conditions
conducive to false detection and surface deviations that may
not appear as unlikely colours. Deploying colour images
and surface information acquired by 3D sensors can tackle
the above issues and substantially improve AD.
Recently, researchers have started to explore novel av-
enues thanks to the introduction of benchmark datasets for
*These authors contributed equally to this work .
Figure 1. Performance, speed and memory occupancy of Mul-
timodal Anomaly Detection methods. The chart reports defect
segmentation performance (AUPRO@30%) vs inference speed
(Frame Rate on an NVIDIA 4090 GPU).
3D anomaly detection, such as MVTec 3D-AD [5] and
Eyecandies [6]. Indeed, both provide RGB images along-
side pixel-registered 3D information for all data samples,
thereby fostering the development of new, multimodal AD
approaches [17, 34, 39]. Unsupervised multimodal AD
methods like BTF [17] and M3DM [39] rely on large mem-
ory banks of multimodal features. They achieve excel-
lent performance (AUPRO@30% metric in Fig. 1) at the
cost of extensive memory requirements and slow inference
(Fig. 1). In particular, M3DM outperforms BTF by leverag-
ing frozen feature extractors trained by self-supervision on
large datasets, i.e., ImageNet and Shapenet, for 2D and 3D
features, respectively. Another recent multimodal method,
AST [34], follows a teacher-student paradigm conducive to
a faster architecture (Fig. 1). Yet, AST does not exploit
the spatial structure of the 3D data but employs this infor-
mation just as an additional input channel in a 2D network
architecture. This results in inferior performance compared
to M3DM and BTF (Fig. 1).
In this paper, we propose a novel paradigm to exploit
the relationship between features extracted from different
modalities and improve multimodal AD. The core idea be-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
17234
hind our method, described in Fig. 2, is to learn two cross-
modal mapping functions, M2D→3DandM3D→2D, be-
tween the latent spaces of frozen 2D and 3D feature extrac-
tors,F2DandF3D, respectively. Thus, given a 2D feature
computed by the 2D extractor, M2D→3Dlearns to predict
the corresponding 3D feature calculated by the 3D extractor,
and, likewise, M3D→2Dlearns to predict a 2D feature for
a given 3D feature. As we learn the two mapping functions
on nominal data, we expect them to capture crossmodal re-
lationships peculiar to good samples, while anomalies, by
their quintessential nature, realize mappings unseen at train-
ing time, such as a 2D feature never observed in conjunction
with a certain 3D feature, or vice versa. Hence, at inference
time, we compute an anomaly map Ψby estimating and
aggregating the discrepancies (Ψ3D,Ψ2D)between the ac-
tual features provided by the two frozen extractors and those
predicted by the crossmodal mapping functions.
This framework is amenable to realising multimodal AD
effectively and efficiently. Indeed, no obvious, trivial solu-
tions would lead the crossmodal mapping networks to gen-
eralize to defective samples. For instance, as input and out-
put features are extracted from different modalities, the net-
works cannot learn identity mappings, as may have hap-
pened in previous reconstruction-based AD methods [22].
Moreover, as we will discuss in Sec. 3, modelling the rela-
tionship between 2D and 3D features in nominal data pro-
vides high sensitivity toward all kinds of anomalies. Fi-
nally, the feature mapping functions can be implemented
as lightweight neural networks, such as small and shallow
MLPs. This yields very fast inference alongside limited
memory occupancy.
As shown in Fig. 1, our novel AD approach based
on crossmodal mapping functions achieves state-of-the-art
performance on MVTec 3D-AD, outperforming the best
resource-intensive method based on memory banks (Ours
vs M3DM), while delivering much faster inference. Addi-
tionally, we have observed that learning mappings between
features from shallower layers of the frozen extractors can
yield massive gains in terms of memory requirements and
inference speed with a relatively limited impact on the ef-
fectiveness of our method. Thus, we can prune the deepest
layers of both the 2D and 3D feature extractors to obtain
Small andTiny variants of our framework (Fig. 1: Ours-
S, Ours-T) that require much less memory and run faster.
Remarkably, the Small architecture still provides state-of-
the-art performance on MVTec 3D-AD while requiring less
than half memory compared to the full model, whereas the
Tiny architecture runs almost twice as fast and outperforms
BTF and AST. Finally, we point out that our method can be
trained even with a few nominal samples. To properly eval-
uate our approach in this challenging scenario, we build the
first few-shot multimodal AD benchmark from MVTec 3D-
AD, and we note that our method achieves state-of-the-artanomaly segmentation performance.
Our contributions can be summarized as follows:
• We propose a novel framework for unsupervised multi-
modal AD based on mapping features across modalities;
• By using modality-specific features extracted from frozen
2D and 3D extractors, we attain state-of-the-art detection
and segmentation performance on MVTec 3D-AD, while
reaching performance comparable to the state-of-the-art
on Eyecandies.
• Our method is capable of very fast inference and requires
less memory than state-of-the-art solutions.
• We reach state-of-the-art performance on the proposed
few-shot AD benchmark built on top of MVTec 3D-AD;
• We develop a strategy to prune networks without overly
compromising performance. In this way, we achieve re-
markably faster inference and large memory savings.
2. Related Work
Unsupervised Image Anomaly Detection. Unsupervised
AD approaches [22] analyzing RGB Images can be di-
vided into two broad categories. The general idea be-
hind the first is to learn how to reconstruct images of
nominal samples using auto-encoders [2, 18, 31, 46], in-
painting [27], or diffusion models [40]. Then, at test time,
as the trained model cannot correctly reconstruct anoma-
lous images, a per-pixel anomaly map can be computed
by analyzing the discrepancy between the input and recon-
structed image. The second category of approaches focuses
instead on the feature space defined by deep neural net-
works [4, 7, 10, 12, 13, 15, 21, 23, 29, 30, 33, 36–38, 42–
45, 47]. Deep Feature Reconstruction (DFR) [41] trains an
auto-encoder on the features extracted from nominal sam-
ples. Then, similarly to image reconstruction methods, it
identifies anomalies in the test samples by analysing the dif-
ference between reconstructed and original features. The
increasing availability of effective, general-purpose feature
extractors [8, 16, 24], has fostered interest in anomaly de-
tection methods that deploy features extracted by frozen
models [1, 11, 32]. At training time, the features computed
from nominal samples by a frozen extractor are stored in
a memory bank. At inference time, the features extracted
from the input image by the frozen model are compared
to those stored in the bank to identify anomalies. These
approaches achieve remarkable performance, albeit at the
cost of slow inference — since each feature vector extracted
from the input image has to be compared to all the nominal
ones stored in the bank — and significant memory occu-
pancy — since larger memory banks better capture the vari-
ability of nominal features.
Multimodal RGB-3D Anomaly Detection. Multimodal
approaches exploit both RGB images and 3D data to en-
hance the robustness and effectiveness of anomaly detec-
tion. Following the influential work on benchmarking
17235
Figure 2. Proposed pipeline. Given an RGB Image I2Dand a Point Cloud P3D: a pair of feature extractors, F2D,F3D, extract pixel-
aligned feature maps, E2D,E3D, by Transformer architectures. Then, a pair of crossmodal feature mappings, M2D→3D,M3D→2D, map
the extracted features from one modality to the other, processing the features at each pixel independently. Lastly, extracted, E2D,E3Dand
mapped, ˆE3D,ˆE3D, features are compared through a discrepancy function D, to create modality-specific anomaly maps, Ψ2D,Ψ3D, that
are then combined by an aggregation function, A, to obtain the final anomaly map Ψ.
image-based AD [3], a recent paper [5] has introduced the
MVTec 3D-AD dataset, alongside an experimental valida-
tion including several baselines, such as distribution map-
ping techniques based on GANs and variational models
(i.e., V AEs), as well as auto-encoders. Inspired by Patch-
Core [32], BTF [17] investigates the use of memory banks
for 3D anomaly detection. The authors propose to add 3D
features to the 2D features provided by a frozen convolu-
tional model to enhance anomaly detection performance.
They test several 3D features and achieve the best re-
sults using hand-crafted descriptors extracted from Point
Clouds [35]. M3DM [39] improved over BTF by em-
ploying rich and distinctive 2D and 3D features extracted
by frozen Transformer-based foundation models trained by
self-supervision on large datasets. The authors also propose
a learned function to fuse 2D and 3D features into mul-
timodal features stored in memory banks alongside those
computed from the individual modalities. However, re-
liance on large feature banks renders M3DM overly ex-
pensive in terms of memory and time (Fig. 1). Similarly
to M3DM [39], our method deploys 2D and 3D features
computed by frozen Transformer-based models. Yet, we do
not employ any memory bank and, instead, propose a novel
crossmodal feature mapping paradigm that can be realized
by two lightweight neural networks. Using the same feature
extractors as M3DM [39], we achieve better performance
on MVTec 3D-AD while requiring way less memory and
running remarkably faster (Fig. 1).
3. Method
Our multimodal AD approach relies on learning crossmodal
mappings between features extracted from nominal sam-
ples to pinpoint anomalies based on the discrepancy be-
tween predicted and observed features. As depicted in
Fig. 2, this is realized by (i)a pair of frozen feature ex-
tractors F2D,F3D;(ii)a pair of feature mappings networksM2D→3D,M3D→2D; and (iii)an aggregation module.
3.1. Feature Extraction
The initial step in our pipeline involves extracting features
for every pixel in a 2D image denoted as I2Dand for each
point in a 3D Point Cloud represented by P3D. As explained
in Sec. 1, in our framework, both feature extractors have
been trained on large external datasets and are kept frozen,
i.e., their weights will never be updated.
2D Feature Extraction and Interpolation. Given an im-
ageI2Dwith dimensions H×W×C, we process it with
a 2D feature extractor, denoted as F2D, yielding a feature
map with dimensions Hf×Wf×D2D. Since the dimen-
sions HfandWfare smaller than the original HandW,
we apply a bilinear upsampling operation to obtain E2D,
which is a feature map with dimensions H×W×D2D,
thereby obtaining a feature vector for each pixel location.
3D Feature Extraction and Interpolation. Given a point
cloud of dimensions N×3, we process it with a 3D fea-
ture extractor, F3D, obtaining a set of Nffeature vectors
of size D3D. Each feature vector, fc, is associated with a
specific point within the original point cloud, c∈P3D. In-
deed, many 3D feature extractors ( e.g., [25]) do not estimate
features for each input point but only for a subset of them,
i.e.,Nf< N . Thus, to obtain a feature vector, fp, for each
point of the cloud, p∈P3D, we follow a procedure simi-
lar to [39]. Here, fpis computed as a weighted sum of the
three feature vectors that, among the Nfextracted by F3D,
have the closest centres to p. In this way, we obtain E′
3D, a
set of Ninterpolated feature vectors of size D3D.
Feature Alignment. According to the standard setting in
multimodal AD [5, 6], we assume pixel-registered 3D data
and images. Thus, we know the corresponding pixel lo-
cation associated with each 3D point. As E2DandE′
3D
have been interpolated to match the original image and
point cloud resolutions, we can project E′
3Dinto the 2D
image plane, obtaining E3D, a feature map of dimensions
17236
H×W×D3D. In this process, we set to zero the vectors
at the pixel locations where we do not have a correspond-
ing 3D feature. Finally, we apply a 3×3smoothing kernel
onE3D. At the end of this procedure, we obtain E2Dand
E3D, two feature maps aligned at the pixel level.
3.2. Crossmodal Feature Mapping
Once E2DandE3Dhave been obtained, we deploy two
Feature Mapping functions, implemented as lightweight
MLPs, M2D→3DandM3D→2D.M2D→3Dmaps a fea-
ture vector of size D2Dinto another one of size D3D, while
M3D→2Ddoes the opposite. Each network predicts fea-
tures of one modality from the other, processing each pixel
location independently. Thus, given a pixel location i, and
the corresponding 2D and 3D feature, Ei
2DandEi
3D, we
can obtain the predicted feature of the other modality as:
ˆEi
3D=M2D→3D(Ei
2D)ˆEi
2D=M3D→2D(Ei
3D)(1)
When processing pixel locations without a 3D point asso-
ciated with it, we set to zero the corresponding predicted
feature. By processing all pixels, we obtain the predicted
feature maps ˆE3D,ˆE2D, of dimensions H×W×D2Dand
H×W×D3D, respectively.
Training. At training time, M2D→3DandM3D→2Dare
jointly optimized on all the nominal samples of a dataset by
minimizing the cosine distance between the feature maps
computed from the input data of both modalities and the
predicted ones. Thus, the per-pixel loss is:
Li= 
1−Ei
2D·ˆEi
2D
∥Ei
2D∥∥ˆEi
2D∥!
+ 
1−Ei
3D·ˆEi
3D
∥Ei
3D∥∥ˆEi
3D∥!
(2)
Rationale. As pointed out in Sec. 1, this novel paradigm
offers high sensitivity toward all kinds of anomalies. Let us
conceptualize this property with the toy examples presented
in Fig. 3. At training time (top left), we observe red 2D pat-
terns on flat 3D surfaces and blue 2D patterns on curved 3D
surfaces: M2D→3DandM3D→2Dlearn to predict the re-
lationships between the features extracted from these data.
At inference time, if an anomalous, e.g., yellow, 2D pattern
appears on a curved surface (top right), M3D→2Dpredicts
the 2D feature corresponding to a blue pattern, whilst the
observed 2D feature concerns a yellow pattern. Moreover,
M2D→3Dreceives an input feature unseen at training time,
which would unlikely yield as output the 3D feature of the
actual curved surface. Thus, our method senses a discrep-
ancy between prediction and observation for the 2D and the
3D features. Similar considerations apply to a nominal 2D
pattern on an anomalous 3D surface (bottom left): both pre-
dictions disagree with the observations. This is the case
also when both modalities exhibit anomalies (not shown in
Fig. 3): both inputs are unseen at training time, so both
Figure 3. Toy example of anomaly scenarios with correspond-
ing behaviour of cross-modal mappings. Top left: Nominal sam-
ples. Top right: 2D-Only with an RGB anomaly. Bottom left: 3D-
Only with an anomalous shape. Bottom right: Multimodal-Only,
with nominal RGB and 3D data but anomalous correlation.
crossmodal predictions are unlikely to match the observa-
tions. Finally, we highlight the case mandating multimodal
AD: the individual modalities comply with the nominal dis-
tributions, but their co-occurrence is anomalous. This may
be exemplified by a red pattern on a curved surface (bot-
tom right): again, as M2D→3Doutputs the 3D feature of
the flat patch and M3D→2Dthe 2D feature of the blue one,
both predictions disagree with the observations.
It is worth pointing out that, due to the variability of the
nominal samples, the mappings between 2D and 3D fea-
tures may not be unique . For instance, in Fig. 3, there might
be both flat and curved surfaces coloured in red, and this
one-to-many mapping makes it hard for M2D→3Dto learn
the correct 3D feature to be associated to the 2D feature
of a red patch. Consequently, when presented with a red
patch, M2D→3Dmay predict the wrong 3D feature or an
unlikely one, causing a discrepancy between the predicted
and observed 3D features. Yet, M3D→2Dcan predict the
2D feature of the red patch, due to the 3D→2Dmapping
being many-to-one . Thus, we may avoid a false detection by
pinpointing anomalies only when both predictions disagree
with the observations. Of course, due to even higher vari-
ability across nominal samples, we may also face one-to-
many 3D→2Dmappings, e.g., considering again Fig. 3,
both blue and red image patches on curved 3D patches. In
such a case, when presented with a red patch on a curved
surface, M2D→3Dmay wrongly predict the feature of a flat
patch and M3D→2Dthat of a blue patch, ending up in a
false anomaly detection due to both predictions disagreeing
with the observations.
Nonetheless, in our framework, we can address the issue
17237
of potential one-to-many feature mappings across modali-
ties by leveraging on the highly contextualized 2D and 3D
features provided by Transformer architectures [8, 25]. In-
deed, a contextualized 2D feature, e.g., describing a red
patch surrounded by blue and purple patches, tends to cor-
respond to a specific contextualized 3D feature, e.g., rep-
resenting a flat patch just to the right of a rippling surface
area. In other words, the highly contextualized 2D and 3D
features extracted by Transformers are less prone to realize
one-to-many crossmodal mappings. For the above reasons,
we employ Transformers for both F2DandF3D.
3.3. Aggregation
At inference time, test samples are forwarded to the
Feature Extraction and Mapping networks to obtain
two pairs of extracted and predicted feature maps
(E2D,ˆE2D),(E3D,ˆE3D). After ℓ2-normalization of all the
individual feature vectors, the extracted and predicted maps
are compared pixel-wise by a discrepancy function, D, to
obtain modality-specific anomaly maps Ψ2D,Ψ3D:
Ψ2D=D(E2D,ˆE2D) Ψ 3D=D(E3D,ˆE3D)(3)
We employ the Euclidean distance as discrepancy D.
The above anomaly maps are then combined using an
aggregation function Ato get the final anomaly map Ψ =
A(Ψ2D,Ψ3D). As discussed in Sec. 3.2 with the help
of Fig. 3, pinpointing anomalies only when both predic-
tions disagree with the observations provides high sensi-
tivity across all kinds of anomalies and good robustness
toward false detection. Therefore, we use the pixel-wise
product as aggregation function: Ψ = Ψ 2D·Ψ3D, which
can be thought of as a logical AND: the anomaly score
at any pixel location is high only if this is so for both the
modality-specific scores, i.e., anomaly detection must be
corroborated by both modalities.
The aggregated anomaly map is finally smoothed by a
Gaussian of kernel with σ= 4, similarly to common prac-
tice [11, 32, 39]. The global anomaly score required to
perform sample-level anomaly detection is obtained as the
maximum value of the anomaly map Ψ.
3.4. Layers Pruning
The Feature Extractors employed in our solution [8, 25]
are based on Transformer encoders composed of mlayers.
The distinguishing factor between features at different lay-
ers lies in the varying degree of self-attention processing
applied to the original input. As the input features descend
the encoder layers, they exhibit an increased contextualiza-
tion. We observed that learning mappings between features
from shallower layers of the frozen extractors can yield re-
markable gains in terms of memory requirements and infer-
ence speed with a limited impact on effectiveness. Thus,
Layer 1
Layer l
Layer l +1Layer 1
Layer l
Layer l + 1RGB Image Point Cloud
Feature Mappings at Layer
Pruned Layers
Layer m
Layer m
Pruned LayersFigure 4. Layers Pruning. The Feature Mapping networks can be
fed with features from different layers of the two Transformers.
as shown in Fig. 4, we perform layer-pruning by choos-
ing an intermediate layer lin the 2D and 3D frozen fea-
ture extractors ( F2D,F3D) and discarding those from l+ 1
to the last. Consequently, crossmodal networks, Ml
2D→3D
andMl
3D→2D, map features of layer lof the 2D encoder
into those of layer lof the 3D encoder and vice versa. For
instance, in Ours-T ( l= 1), we trim both encoders after
the first layer, discarding those from second to last, and ap-
ply the crossmodal mappings on the features extracted by
the first layers. In contrast, our reference model learns the
crossmodal mapping networks between features from the
last layer of both encoders.
4. Experimental Settings
Datasets and Metrics. We evaluate our framework on
two multimodal AD benchmarks. MVTec 3D-AD [5] con-
sists of 10categories of industrial objects, totalling 2656
train samples, 294validation samples and 1197 test sam-
ples. Eyecandies [6] is a synthetic dataset featuring photo-
realistic images of 10categories of food items in an indus-
trial conveyor scenario. It contains 10ktrain samples, 1k
validation samples and 4ktest samples. Both datasets pro-
vide RGB images alongside pixel-registered 3D informa-
tion for each sample. Thus, we have RGB information at
each pixel location paired with (x, y, z )coordinates. We
employ the evaluation metrics proposed by MVTec 3D-AD.
Thus, we assess image anomaly detection performance by
the Area Under the Receiver Operator Curve (I-AUROC)
computed on the global anomaly score. We estimate the
anomaly segmentation performance by the pixel-level Area
Under the Receiver Operator Curve (P-AUROC) and the
Area Under the Per-Region Overlap (AUPRO). All previous
works employ 0.3as the False Positive Rate (FPR) integra-
tion threshold to calculate the AUPRO. We reckon such a
value may often turn out too loose for real industrial appli-
cations, allowing too many false positives. Hence, we also
compute AUPRO based on the tighter 0.01threshold. We
denote AUPROs with integration thresholds 0.3and0.01as
AUPRO@30%, and AUPRO@1%, respectively. We report
results with additional thresholds in the Supplementary.
Implementation Details. We employ the same frozen
17238
Method Bagel Cable Gland Carrot Cookie Dowel Foam Peach Potato Rope Tire MeanI-AUROCDepthGAN [5] 0.538 0.372 0.580 0.603 0.430 0.534 0.642 0.601 0.443 0.577 0.532
DepthAE [5] 0.648 0.502 0.650 0.488 0.805 0.522 0.712 0.529 0.540 0.552 0.595
DepthVM [5] 0.513 0.551 0.477 0.581 0.617 0.716 0.450 0.421 0.598 0.623 0.555
V oxelGAN [5] 0.680 0.324 0.565 0.399 0.497 0.482 0.566 0.579 0.601 0.482 0.517
V oxelAE [5] 0.510 0.540 0.384 0.693 0.446 0.632 0.550 0.494 0.721 0.413 0.538
V oxelVM [5] 0.553 0.772 0.484 0.701 0.751 0.578 0.480 0.466 0.689 0.611 0.609
BTF [17] 0.918 0.748 0.967 0.883 0.932 0.582 0.896 0.912 0.921 0.886 0.865
AST [34] 0.983 0.873 0.976 0.971 0.932 0.885 0.974 0.981 1.000 0.797 0.937
M3DM [39] 0.994 0.909 0.972 0.976 0.960 0.942 0.973 0.899 0.972 0.850 0.945
Ours 0.994 0.888 0.984 0.993 0.980 0.888 0.941 0.943 0.980 0.953 0.954
Ours-M 0.988 0.875 0.984 0.992 0.997 0.924 0.964 0.949 0.979 0.950 0.960AUPRO@30%DepthGAN [5] 0.421 0.422 0.778 0.696 0.494 0.252 0.285 0.362 0.402 0.631 0.474
DepthAE [5] 0.432 0.158 0.808 0.491 0.841 0.406 0.262 0.216 0.716 0.478 0.481
DepthVM [5] 0.388 0.321 0.194 0.570 0.408 0.282 0.244 0.349 0.268 0.331 0.335
V oxelGAN [5] 0.664 0.620 0.766 0.740 0.783 0.332 0.582 0.790 0.633 0.483 0.639
V oxelAE [5] 0.467 0.750 0.808 0.550 0.765 0.473 0.721 0.918 0.019 0.170 0.564
V oxelVM [5] 0.510 0.331 0.413 0.715 0.680 0.279 0.300 0.507 0.611 0.366 0.471
BTF [17] 0.976 0.969 0.979 0.973 0.933 0.888 0.975 0.981 0.950 0.971 0.959
AST [34] 0.970 0.947 0.981 0.939 0.913 0.906 0.979 0.982 0.889 0.940 0.944
M3DM [39] 0.970 0.971 0.979 0.950 0.941 0.932 0.977 0.971 0.971 0.975 0.964
Ours 0.979 0.972 0.982 0.945 0.950 0.968 0.980 0.982 0.975 0.981 0.971
Ours-M 0.980 0.966 0.982 0.947 0.959 0.967 0.982 0.983 0.976 0.982 0.972
Table 1. I-AUROC and AUPRO@30% on MVTec 3D-AD for multimodal AD methods. Best results in bold , runner-ups underlined .
Method Bagel Cable Gland Carrot Cookie Dowel Foam Peach Potato Rope Tire Mean
BTF [17] 0.428 0.365 0.452 0.431 0.370 0.244 0.427 0.470 0.298 0.345 0.383
AST [34] 0.388 0.322 0.470 0.411 0.328 0.275 0.474 0.487 0.360 0.474 0.398
M3DM [39] 0.414 0.395 0.447 0.318 0.422 0.335 0.444 0.351 0.416 0.398 0.394
Ours 0.459 0.431 0.485 0.469 0.394 0.413 0.468 0.487 0.464 0.476 0.455
Ours-M 0.480 0.398 0.490 0.467 0.413 0.408 0.481 0.494 0.468 0.488 0.459
Table 2. AUPRO@1% on MVTec 3D-AD for multimodal AD methods. Best results in bold , runner-ups underlined .
Transformers as M3DM [39] to realize the F2DandF3D
feature extractors, i.e., DINO ViT-B/8 [8, 20] trained on Im-
ageNet [14] and Point-MAE [25] trained on ShapeNet [9],
respectively. Thus, F2Dprocesses 224×224 RGB im-
ages and outputs 28×28×768feature maps, which are
bi-linearly up-sampled to 224×224×768before feeding
the features to M2D→3D.F3Dprocesses 1024 groups of
32points obtained with FPS [28], yielding a feature vec-
tor of dimensionality 1152 for each group. As described in
in Sec. 3.1, these features are interpolated and aligned to
224×224×1152 before being fed to M3D→2D.
BothM2D→3DandM3D→2Dconsist of just three lin-
ear layers, each but the last one followed by GeLU activa-
tions. The number of units per layer is 768,960,1152 for
M2D→3Dand1152,960,768forM3D→2D. The two net-
works are trained jointly for 250epochs using Adam [19]
with a learning rate of 0.001.
As done in [17, 34, 39], we fit a plane with RANSAC
on the 3D point cloud and consider a point as background
if the distance to the plane is less than 0.005. Background
points are discarded from the in input to F3D. This proce-
dure accelerates the processing of 3D features and mitigates
background noise in anomaly maps.
Moreover, as described in Sec. 3.4 to obtain lighter ver-
sions of our framework we prune both feature extractors at
layer lequal to 1,4,8, to obtain Tiny,Small andMediumMethod I-AUROC P-AUROC AUPRO@30% AUPRO@1%
AST [34] 0.758 0.902 0.878 0.224
M3DM [39] 0.897 0.977 0.882 0.331
Ours 0.881 0.974 0.887 0.335
Ours-M 0.865 0.973 0.880 0.330
Table 3. Eyecandies Results. Average metrics of 10 classes on
the test set. Best results in bold , runner-ups underlined .
architectures referred to as Ours-T, Ours-S, and Ours-M.
We conducted experiments using both our and the origi-
nal code from the authors of other multimodal AD methods
on a single NVIDIA GeForce RTX 4090.
5. Experiments
Anomaly Detection and Segmentation. Following the
setups of [39], we evaluate our proposal on MVTec 3D-
AD and Eyecandies, reporting results in Tab. 1, Tab. 2
and Tab. 3. Our method achieves the best results in de-
tection and segmentation on MVTec 3D-AD, outperform-
ing the previous state-of-the-art method, M3DM, in all the
three mean metrics, namely I-AUROC, AUPRO@30% and
AUPRO@1%, as well as in most of the individual cate-
gories. Comparison between Tab. 1 and Tab. 2 shows how
the performance of current AD methods turn out dramati-
cally inferior when the evaluation sets a more challenging
bar in terms of tolerable FPR. As mentioned in Sec. 4, we
17239
Method
BTF [17]
AST [34]
M3DM [39]
Ours5-shot 10-shot 50-shot Full
I-AUROC
0.671 0.695 0.806 0.865
0.680 0.689 0.794 0.937
0.822 0.845 0.907 0.945
0.811 0.845 0.906 0.9545-shot 10-shot 50-shot Full
P-AUROC
0.980 0.983 0.989 0.992
0.950 0.946 0.974 0.976
0.984 0.986 0.989 0.992
0.986 0.987 0.991 0.9935-shot 10-shot 50-shot Full
AUPRO@30%
0.920 0.928 0.947 0.959
0.903 0.835 0.929 0.944
0.937 0.943 0.955 0.964
0.949 0.954 0.965 0.9715-shot 10-shot 50-shot Full
AUPRO@1%
0.288 0.308 0.356 0.383
0.158 0.174 0.335 0.398
0.330 0.355 0.387 0.394
0.382 0.398 0.431 0.455
Table 4. Few-shot Anomaly Detection and Segmentation on the MVTec 3D-AD dataset. Best results in bold , runner-ups underlined .
Bagel Carrot Dowel Peach RopeRGB
 PC
 GT
 M3DM
 Ours
Figure 5. MVTec-3D AD Qualitative Results. From top to bot-
tom: input RGB images, point clouds, GT anomaly segmentations,
anomaly maps from M3DM, and anomaly maps with our method.
believe that such a challenge better matches the require-
ments of many real industrial AD applications. Therefore,
we posit that the MVTec 3D-AD benchmark is far from
saturated, and there exist vast margins of improvements in
multimodal AD. As for Eyecandies, Tab. 3, we achieve per-
formance comparable to M3DM, with two winning met-
rics for each method. Moreover, we highlight that the P-
AUROC metric seems almost saturated while also in Eye-
candies there is substantial room for improvement in the
AUPRO@1% metric. In Fig. 5, we show some qualita-
tive results on the MVTec 3D-AD dataset. Compared to
M3DM, our method provides remarkably sharper anomaly
maps, well localized relatively to the ground-truth defect
segmentation, thereby motivating the larger performance
gap in terms of AUPRO@1%. More extensive qualitative
results are reported in the Supplementary Material.
Few-shot Anomaly Detection and Segmentation. In rele-
vant industrial scenarios, collecting many nominal samples
is extremely expensive or even unfeasible. Thus, a desirable
property of AD methods is the ability to model the distribu-
tion of nominal data by only a few samples. To address
this scenario, we define the first benchmark for few-shot
multimodal AD based on the MVTec 3D-AD dataset. WeMethod Frame Rate Memory I-AUROC P-AUROC AUPRO@30% AUPRO@1%
BTF [17] 3.197 381.06 0.865 0.992 0.959 0.383
AST [34] 4.966 463.94 0.937 0.976 0.944 0.398
M3DM [39] 0.514 6526.12 0.945 0.992 0.964 0.394
Ours 21.755 437.91 0.954 0.993 0.971 0.455
Ours-M 24.146 295.81 0.960 0.994 0.972 0.459
Ours-S 24.527 211.09 0.948 0.994 0.972 0.451
Ours-T 42.818 48.12 0.899 0.990 0.961 0.419
Table 5. Inference Speed, Memory Footprint and AD Perfor-
mance on MVTec 3D-AD . Frame Rate in fps and Memory in MB.
randomly select 5, 10, and 50 images from each category
as training data. We train the best multimodal methods,
BTF [17], M3DM [39], and AST [34] on these samples, and
we test them on the entire MVTec 3D-AD test set, reporting
the results in Tab. 4. As for detection, our method achieves
an I-AUROC comparable to M3DM [39] while outperform-
ing the other approaches. We obtain the best segmenta-
tion performance for all metrics (P-AUROC, AUPRO@1%,
and AUPRO@30%) in all the few-shot settings, signifi-
cantly improving the most challenging segmentation met-
ric (+0.052 AUPRO@1% on 5-shot). These results show
that our framework enables learning general crossmodal re-
lationships even from a few nominal samples.
Frame Rate and Memory Occupancy. Computational
efficiency is key to industrial AD. Thus, we investigate
the memory footprint and inference speed w.r.t. AD per-
formance for the best multimodal approaches, BTF [17],
M3DM [39], and AST [34], as well as our method. In
addition, we report the performance of our framework by
pruning the feature extractors at various levels using the
technique described in Sec. 3.4. The results are reported
in Tab. 5. We compute inference speed in frames per sec-
ond on the same machine equipped with an NVIDIA 4090
and Pytorch 1.13, reporting the average across all the test
samples of MVTec 3D-AD. For each method, we include
the time for each step of its inference pipeline, from input
pre-processing to the computation of anomaly scores, syn-
chronizing all GPU threads before estimating the total in-
ference time. We do not include training-only steps such
as the memory bank creation. Regarding memory occu-
pancy during inference, we consider network parameters,
activations, and memory banks. As expected, memory-bank
methods (BTF [17] and M3DM [39]) exhibit the lowest
frame rate and the highest memory footprint. AST [34]
requires only 26 MB more than our model, as it is based
on two feed-forward networks. However, it is still rela-
17240
Anomaly Map I-AUROC P-AUROC AUPRO@30% AUPRO@1%
Ψ2D 0.895 0.985 0.950 0.401
Ψ3D 0.885 0.987 0.956 0.403
Ψ2D+Ψ3D 0.939 0.988 0.959 0.430
max(Ψ 2D,Ψ3D) 0.895 0.985 0.950 0.400
Ψ2D·Ψ3D 0.954 0.993 0.971 0.455
Table 6. Analysis of Aggregation Functions. Results on MVTec
3D-AD. Best results in bold .
Modality Anomaly Map I-AUROC P-AUROC AUPRO@30% AUPRO@1%
Intra Φ2D 0.860 0.980 0.932 0.361
Intra Φ3D 0.816 0.970 0.900 0.348
Intra Φ2D·Φ3D 0.898 0.989 0.963 0.426
Cross Ψ2D 0.865 0.982 0.944 0.382
Cross Ψ3D 0.885 0.985 0.952 0.391
Cross Ψ2D·Ψ3D 0.944 0.993 0.970 0.450
Table 7. Crossmodal vs Intramodal. Results on MVTec 3D-AD.
Best results in bold . Networks are trained for 50 epochs.
tively slow ( 4.966 fps) since it is based on Normalizing
Flow [26]. Our method has the highest frame rate ( 21.755
fps) and the lowest memory occupancy ( 437.91MB) while
outperforming competitors across all metrics. The pruned
models Ours-M, Ours-S, and Ours-T are even more effi-
cient with a marginal sacrifice in accuracy. For instance,
Ours-S occupies half of the memory of our full model and
yet achieves state-of-the-art results on MVTec 3D-AD on
all metrics. Remarkably, Ours-T obtains state-of-the-art
anomaly segmentation performance according to the most
challenging metric (AUPRO@1%= 0.419) while running in
real-time ( 48.12fps).
Aggregation Analysis. We investigate on the impact
of the proposed product-based aggregation discussed in
Sec. 3.3. In Tab. 6, we report the results obtained on
MVTec 3D-AD by using the anomaly maps before aggrega-
tion,Ψ2DandΨ3D, or combined using different functions,
such as pixel-wise sum Ψ2D+ Ψ 3D, pixel-wise maximum
max(Ψ 2D,Ψ3D), and pixel-wise product Ψ2D·Ψ3D. It
is possible to note how the product performs best in both
detection and segmentation. Indeed, considering as anoma-
lous only points in which both Ψ2DandΨ3Dhave high
scores, enables discarding false positives that may occur
when nominal relationships between RGB and 3D features
are not unique, as discussed in Sec. 3.2.
Features Visualization. In Fig. 6 we show the spa-
tially aligned 2D and 3D feature maps before ( E2D, E3D)
and after ( E3D→2D, E2D→3D) crossmodal mappings, as
well as the 2D ( Ψ2D), 3D ( Ψ3D) and final anomaly maps
(Ψ2D·Ψ3D), for a nominal (top) and an anomalous (bot-
tom) test sample of MVTec 3D-AD. Comparing the two
rows, we note that while E2Dfeature maps of the nomi-
nal and anomalous samples look similar, E3Dbetter high-
lights the hole anomaly. We can relate this to a 3D-only
anomaly depicted in Fig. 3. After mapping, the hole is vis-
ible in E3D→2Dyet not in E2D→3D. This visualization
agrees with the rationale discussed in Sec. 3.2 and Fig. 3:RGB PC E2D E3D→2D Ψ2D E3D E2D→3D Ψ3D Ψ2D·Ψ3DNominal
 Anomalous
Figure 6. Features Visualization . Channels average of feature
maps before and after crossmodal mapping ( viridis colormap).
nominal 2D features are mapped into nominal 3D features,
anomalous 3D features into anomalous 2D features. Thus,
by computing the discrepancy between mapped and original
features, we obtain 2D and 3D anomaly maps, both high-
lighting the defect. Remarkably, though the hole occupies a
very small portion of the image, it is detected accurately.
Crossmodal Mapping vs Intramodal Reconstruction.
The authors of DFR [41] argued that learning a reconstruc-
tion network in feature space from nominal samples makes
it possible to detect anomalies in RGB images by analyzing
the reconstruction error. As our method may be thought of
as performing a Crossmodal reconstruction in feature space,
we investigate the impact of learning Crossmodal vs. Intra-
modal feature mapping functions. Tab. 7 compares the re-
sults of our approach (Cross) to those obtained by modify-
ing the input layers of both our mapping networks so as to
learn to reconstruct features within the same modality (In-
tra). The results obtained by reconstructing each modality
independently show that our proposed crossmodal feature
mapping sets forth a more effective modality-specific learn-
ing objective w.r.t. intra-modal feature reconstruction (rows
1 vs. 4, 2 vs. 5). This yields better results also by the aggre-
gated maps obtained by pixel-wise product (rows 3 vs. 6).
6. Conclusions and Limitations
We have developed an effective and efficient multimodal
AD framework based on the core idea of mapping fea-
tures extracted by Transformer architectures across modal-
ities. This novel paradigm outperforms previous resource-
intensive methods on the MVTec 3D-AD benchmark while
delivering substantially faster inference speed. Addition-
ally, we have proposed a layer-pruning strategy for frozen
Transformer encoders that can vastly reduce the memory
footprint and yield even faster inference without compro-
mising AD performance. Lastly, we outperform competi-
tors in the challenging few-shot scenario, achieving state-
of-the-art performance on the proposed multimodal few-
shot AD benchmark. A limitation of our approach lies in
itsmultimodal-only nature, i.e., our paradigm cannot be ap-
plied to 2D AD or 3D AD, as it mandates data from both
modalities at training and test times.
Acknowledgements We gratefully acknowledge the sup-
port of SACMI Imola.
17241
References
[1] Liron Bergman, Niv Cohen, and Yedid Hoshen. Deep
nearest neighbor anomaly detection. arXiv preprint
arXiv:2002.10445 , 2020. 2
[2] Paul Bergmann, Sindy L ¨owe, Michael Fauser, David Sattleg-
ger, and Carsten Steger. Improving unsupervised defect seg-
mentation by applying structural similarity to autoencoders.
arXiv preprint arXiv:1807.02011 , 2018. 2
[3] Paul Bergmann, Michael Fauser, David Sattlegger, and
Carsten Steger. Mvtec ad – a comprehensive real-world
dataset for unsupervised anomaly detection. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , 2019. 3
[4] Paul Bergmann, Michael Fauser, David Sattlegger, and
Carsten Steger. Uninformed students: Student-teacher
anomaly detection with discriminative latent embeddings. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4183–4192, 2020. 2
[5] Paul Bergmann, Jin Xin, David Sattlegger, and Carsten Ste-
ger. The mvtec 3d-ad dataset for unsupervised 3d anomaly
detection and localization. In Proceedings of the 17th Inter-
national Joint Conference on Computer Vision, Imaging and
Computer Graphics Theory and Applications , pages 202–
213, 2022. 1, 3, 5, 6
[6] Luca Bonfiglioli, Marco Toschi, Davide Silvestri, Nicola
Fioraio, and Daniele De Gregorio. The eyecandies dataset
for unsupervised multimodal anomaly detection and local-
ization. In Proceedings of the 16th Asian Conference on
Computer Vision (ACCV2022 , 2022. ACCV . 1, 3, 5
[7] Yunkang Cao, Qian Wan, Weiming Shen, and Liang Gao.
Informative knowledge distillation for image anomaly seg-
mentation. Knowledge-Based Systems , 248:108846, 2022.
2
[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Pro-
ceedings of the International Conference on Computer Vi-
sion (ICCV) , 2021. 2, 5, 6
[9] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat
Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano-
lis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi,
and Fisher Yu. ShapeNet: An Information-Rich 3D Model
Repository. Technical Report arXiv:1512.03012 [cs.GR],
Stanford University — Princeton University — Toyota Tech-
nological Institute at Chicago, 2015. 6
[10] Li-Ling Chiu and Shang-Hong Lai. Self-supervised normal-
izing flows for image anomaly detection and localization. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 2926–2935, 2023. 2
[11] Niv Cohen and Yedid Hoshen. Sub-image anomaly detection
with deep pyramid correspondences. ArXiv , 2020. 2, 5
[12] Thomas Defard, Aleksandr Setkov, Angelique Loesch, and
Romaric Audigier. Padim: a patch distribution modeling
framework for anomaly detection and localization. In Inter-
national Conference on Pattern Recognition , pages 475–489.
Springer, 2021. 2[13] Hanqiu Deng and Xingyu Li. Anomaly detection via reverse
distillation from one-class embedding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9737–9746, 2022. 2
[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 6
[15] Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka.
Cflow-ad: Real-time unsupervised anomaly detection with
localization via conditional normalizing flows. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision , pages 98–107, 2022. 2
[16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 16000–
16009, 2022. 2
[17] Eliahu Horwitz and Yedid Hoshen. Back to the feature: clas-
sical 3d features are (almost) all you need for 3d anomaly
detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2967–
2976, 2023. 1, 3, 6, 7
[18] Jinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shil-
iang Pu, and Hong Zhou. Divide-and-assemble: Learning
block-wise memory for unsupervised anomaly detection. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 8791–8800, 2021. 2
[19] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In The International Conference on
Learning Representations (ICLR) , 2015. 6
[20] Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weis-
senborn, Georg Heigold, Jakob Uszkoreit, Lucas Beyer,
Matthias Minderer, Mostafa Dehghani, Neil Houlsby, Syl-
vain Gelly, Thomas Unterthiner, and Xiaohua Zhai. An im-
age is worth 16x16 words: Transformers for image recogni-
tion at scale. In International Conference on Learning Rep-
resentations , 2021. 6
[21] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas
Pfister. Cutpaste: Self-supervised learning for anomaly de-
tection and localization. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 9664–9674, 2021. 2
[22] Jiaqi Liu, Guoyang Xie, Jingbao Wang, Shangnian Li,
Chengjie Wang, Feng Zheng, and Yaochu Jin. Deep in-
dustrial image anomaly detection: A survey. arXiv preprint
arXiv:2301.11514 , 2, 2023. 2
[23] Fabio Valerio Massoli, Fabrizio Falchi, Alperen Kantarci,
S ¸eymanur Akti, Hazim Kemal Ekenel, and Giuseppe Amato.
Mocca: Multilayer one-class classification for anomaly de-
tection. IEEE Transactions on Neural Networks and Learn-
ing Systems , 33(6):2313–2323, 2021. 2
[24] Maxime Oquab, Timoth ´ee Darcet, Theo Moutakanni, Huy V .
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-
sell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-
Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-
las Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,
17242
Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-
janowski. Dinov2: Learning robust visual features without
supervision, 2023. 2
[25] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu,
Yonghong Tian, and Li Yuan. Masked autoencoders for point
cloud self-supervised learning. In Computer Vision–ECCV
2022: 17th European Conference, Tel Aviv, Israel, October
23–27, 2022, Proceedings, Part II , pages 604–621. Springer,
2022. 3, 5, 6
[26] George Papamakarios, Eric Nalisnick, Danilo Jimenez
Rezende, Shakir Mohamed, and Balaji Lakshminarayanan.
Normalizing flows for probabilistic modeling and inference.
The Journal of Machine Learning Research , 22(1):2617–
2680, 2021. 8
[27] Jonathan Pirnay and Keng Chai. Inpainting transformer for
anomaly detection. In International Conference on Image
Analysis and Processing , pages 394–406. Springer, 2022. 2
[28] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. In Advances in Neural Infor-
mation Processing Systems . Curran Associates, Inc., 2017.
6
[29] Tal Reiss, Niv Cohen, Liron Bergman, and Yedid Hoshen.
Panda: Adapting pretrained features for anomaly detection
and segmentation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
2806–2814, 2021. 2
[30] Oliver Rippel, Patrick Mertens, and Dorit Merhof. Model-
ing the distribution of normal data in pre-trained deep fea-
tures for anomaly detection. In 2020 25th International Con-
ference on Pattern Recognition (ICPR) , pages 6726–6733.
IEEE, 2021. 2
[31] Nicolae-C ˘at˘alin Ristea, Neelu Madan, Radu Tudor Ionescu,
Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas B Moes-
lund, and Mubarak Shah. Self-supervised predictive convo-
lutional attentive block for anomaly detection. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 13576–13586, 2022. 2
[32] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard
Sch¨olkopf, Thomas Brox, and Peter Gehler. Towards total re-
call in industrial anomaly detection. In Proceedings of 2022
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 14298–14308, 2022. 2, 3, 5
[33] Marco Rudolph, Bastian Wandt, and Bodo Rosenhahn. Same
same but differnet: Semi-supervised defect detection with
normalizing flows. In Proceedings of the IEEE/CVF winter
conference on applications of computer vision , pages 1907–
1916, 2021. 2
[34] Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bas-
tian Wandt. Asymmetric student-teacher networks for indus-
trial anomaly detection. In Winter Conference on Applica-
tions of Computer Vision (WACV) , 2023. 1, 6, 7
[35] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. Fast
point feature histograms (fpfh) for 3d registration. In 2009
IEEE International Conference on Robotics and Automation ,
pages 3212–3217, 2009. 3
[36] Mohammadreza Salehi, Niousha Sadjadi, Soroosh
Baselizadeh, Mohammad H Rohban, and Hamid R Ra-biee. Multiresolution knowledge distillation for anomaly
detection. In Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition , pages
14902–14912, 2021. 2
[37] Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, and
Tomas Pfister. Learning and evaluating representations for
deep one-class classification. In International Conference
on Learning Representations , 2021.
[38] Guodong Wang, Shumin Han, Errui Ding, and Di Huang.
Student-teacher feature pyramid matching for anomaly de-
tection. In The British Machine Vision Conference (BMVC) ,
2021. 2
[39] Yue Wang, Jinlong Peng, Jiangning Zhang, Ran Yi, Yabiao
Wang, and Chengjie Wang. Multimodal industrial anomaly
detection via hybrid fusion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 8032–8041, 2023. 1, 3, 5, 6, 7
[40] Julian Wyatt, Adam Leach, Sebastian M Schmon, and
Chris G Willcocks. Anoddpm: Anomaly detection with de-
noising diffusion probabilistic models using simplex noise.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 650–656, 2022. 2
[41] Jie Yang, Yong Shi, and Zhiquan Qi. Dfr: Deep feature re-
construction for unsupervised anomaly segmentation. arXiv
preprint arXiv:2012.07122 , 2020. 2, 8
[42] Minghui Yang, Peng Wu, and Hui Feng. Memseg: A semi-
supervised method for image surface defect detection using
differences and commonalities. Engineering Applications of
Artificial Intelligence , 119:105835, 2023. 2
[43] Jihun Yi and Sungroh Yoon. Patch svdd: Patch-level svdd
for anomaly detection and segmentation. In Proceedings of
the Asian conference on computer vision , 2020.
[44] Seungdong Yoa, Seungjun Lee, Chiyoon Kim, and Hyun-
woo J Kim. Self-supervised learning for anomaly detection
with dynamic local augmentation. IEEE Access , 9:147201–
147211, 2021.
[45] Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu,
Rui Zhao, and Liwei Wu. Fastflow: Unsupervised anomaly
detection and localization via 2d normalizing flows. arXiv
preprint arXiv:2111.07677 , 2021. 2
[46] Vitjan Zavrtanik, Matej Kristan, and Danijel Sko ˇcaj. Draem-
a discriminatively trained reconstruction embedding for sur-
face anomaly detection. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 8330–
8339, 2021. 2
[47] Zheng Zhang and Xiaogang Deng. Anomaly detection using
improved deep svdd model with data structure preservation.
Pattern Recognition Letters , 148:1–6, 2021. 2
17243
