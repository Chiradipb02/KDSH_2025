Diffusion Model Alignment Using Direct Preference Optimization
Bram Wallace1∗Meihua Dang2Rafael Rafailov2Linqi Zhou2Aaron Lou2
Senthil Purushwalkam1Stefano Ermon2Caiming Xiong1Shafiq Joty1
Nikhil Naik1
1Salesforce AI,2Stanford University
bram@openai.com {spurushwalkam,cxiong,sjoty}@salesforce.com naik@alum.mit.edu
{mhdang,rafailov,lzhou907,aaronlou}@stanford.edu ermon@cs.stanford.edu
Abstract
Large language models (LLMs) are fine-tuned using hu-
man comparison data with Reinforcement Learning from
Human Feedback (RLHF) methods to make them better
aligned with users’ preferences. In contrast to LLMs,
human preference learning has not been widely explored
in text-to-image diffusion models; the best existing ap-
proach is to fine-tune a pretrained model using carefully
curated high quality images and captions to improve vi-
sual appeal and text alignment. We propose Diffusion-
DPO, a method to align diffusion models to human pref-
erences by directly optimizing on human comparison data.
Diffusion-DPO is adapted from the recently developed Di-
rect Preference Optimization (DPO) [36], a simpler al-
ternative to RLHF which directly optimizes a policy that
best satisfies human preferences under a classification ob-
jective. We re-formulate DPO to account for a diffusion
model notion of likelihood, utilizing the evidence lower
bound to derive a differentiable objective. Using the Pick-
a-Pic dataset of 851K crowdsourced pairwise preferences,
we fine-tune the base model of the state-of-the-art Stable
Diffusion XL (SDXL)-1.0 model with Diffusion-DPO. Our
fine-tuned base model significantly outperforms both base
SDXL-1.0 and the larger SDXL-1.0 model consisting of an
additional refinement model in human evaluation, improv-
ing visual appeal and prompt alignment. We also develop
a variant that uses AI feedback and has comparable perfor-
mance to training on human preferences, opening the door
for scaling of diffusion model alignment methods.
1. Introduction
Text-to-image diffusion models have been the state-of-the-
art in image generation for the past few years. They are typ-
ically trained in a single stage, using web-scale datasets of
text-image pairs by applying the diffusion objective. This
∗Work done while at Salesforce AIstands in contrast to the state-of-the-art training method-
ology for Large Language Models (LLMs). The best per-
forming LLMs [31, 51] are trained in two stages. In the
first (“pretraining”) stage, they are trained on large web-
scale data. In the second (“alignment”) stage, they are
fine-tuned to make them better aligned with human pref-
erences. Alignment is typically performed using super-
vised fine-tuning (SFT) and Reinforcement Learning from
Human Feedback (RLHF) using preference data. LLMs
trained with this two-stage process have set the state-of-the-
art in language generation tasks and have been deployed in
commercial applications such as ChatGPT and Bard.
Despite the success of the LLM alignment process, most
text-to-image diffusion training pipelines do not incorporate
learning from human preferences. [11, 38, 39] perform two-
stage training, following large-scale pretraining with fine-
tuning on a high-quality text-image pair dataset. This ap-
proach is much more rudimentary than the final-stage align-
ment methods of LLMs. [7, 13] develop more advanced
alignment methods, but have not demonstrated the ability to
stably generalize to a fully open-vocabulary setting across
an array of feedback. Other methods use the pixel-level gra-
dients from reward models on generations to tune diffusion
models, but suffer from mode collapse and can only incor-
porate a relatively narrow set of feedback types [9, 34].
We address this gap in diffusion model alignment for the
first time, developing a method to directly optimize diffu-
sion models on human preference data. We generalize Di-
rect Preference Optimization (DPO) [36], where a gener-
ative model is trained on paired human preference data to
implicitly estimate a reward model. We define a notion of
data likelihood under a diffusion model in a novel formula-
tion and derive a simple but effective loss resulting in stable
and efficient preference training, dubbed Diffusion-DPO.
We connect this formulation to a multi-step RL approach
in the same setting as existing work [7, 13].
We demonstrate the efficacy of Diffusion-DPO by fine-
tuning state-of-the-art text-to-image diffusion models, such
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8228
Figure 1. We develop Diffusion-DPO, a method based on Direct Preference Optimization (DPO) [36] for aligning diffusion models to
human preferences by directly optimizing the model on user feedback data. After fine-tuning on the state-of-the-art SDXL-1.0 model, our
method produces images with exceptionally high visual appeal and text alignment, samples above.
as Stable Diffusion XL (SDXL)-1.0 [33]. Human eval-
uators prefer DPO-tuned SDXL images over the SDXL-
(base + refinement) model 69% of the time on the Par-
tiPrompts dataset, which represents the state-of-the-art in
text-to-image models as measured by human preference.
Example generations shown in Fig. 1. Finally, we show
that learning from AI feedback (instead of human prefer-
ences) using the Diffusion-DPO objective is also effective,
a setting where previous works have been unsuccessful [9].
In sum, we introduce a novel paradigm of learning from
human preferences for diffusion models and present the re-
sulting state-of-the-art model.
2. Related Work
Aligning Large Language Models LLMs are typically
aligned to human preferences using supervised fine-tuning
on demonstration data, followed by RLHF. RLHF con-sists of training a reward function from comparison data on
model outputs to represent human preferences and then us-
ing reinforcement learning to align the policy model. Prior
work [5, 29, 32, 50] has used policy-gradient methods [30,
41] to this end. These methods are successful, but expen-
sive and require extensive hyperparameter tuning [37, 63],
and can be prone to reward hacking [12, 14, 44]. Alter-
native approaches sample base model answers and select
based on predicted rewards [4, 6, 16] to use for super-
vised training [3, 18, 53]. Methods that fine-tune the pol-
icy model directly on feedback data [2, 12], or utilize a
ranking loss on preference data to directly train the pol-
icy model [36, 52, 60, 62] have emerged. The latter set of
methods match RLHF in performance. We build on these
fine-tuning methods in this work, specifically, direct pref-
erence optimization [36] (DPO). Finally, learning from AI
feedback, using pretrained reward models, is promising for
efficient scaling of alignment [5, 25].
8229
Aligning Diffusion Models Alignment of diffusion mod-
els to human preferences has so far been much less ex-
plored than in LLMs. Multiple approaches [33, 39] fine-
tune on datasets scored as highly visually appealing by an
aesthetics classifier [40], to bias the model to visually ap-
pealing generations. Emu [11] finetunes a pretrained model
using a small, curated image dataset of high quality pho-
tographs with manually written detailed captions to improve
visual appeal and text alignment. Other methods [17, 42] re-
caption existing web-scraped image datasets to improve text
fidelity. Caption-aware human preference scoring models
are trained on generation preference datasets [24, 55, 58],
but the impact of these reward models to the generative
space has been limited. DOODL [54] introduces the task of
aesthetically improving a single generation iteratively at in-
ference time. DRAFT [9] and AlignProp [34], incorporate a
similar approach into training: tuning the generative model
to directly increase the reward of generated images. These
methods perform well for simple visual appeal criteria, but
lack stability and do not work on more nuanced rewards
such as text-image alignment [9]. DPOK and DDPO [7, 13]
are RL-based approaches to maximize the scored reward
(with distributional constraints) over a relatively limited vo-
cabulary set; the performance of these methods degrades
as the number of train/test prompts increases. Diffusion-
DPO is unique among alignment approaches in effectively
increasing measured human appeal across an open vocab-
ulary (DPOK, DDPO), without increased inference time
(DOODL) while maintaining distributional guarantees and
improving generic text-image alignment in addition to vi-
sual appeal (DRAFT, AlignProp). See Tab. 1, Supp. S1).
Equal
Open Inference Divergence
Methods V ocabulary Cost Control
DPOK [13] ✗ ✓ ✓
DDPO [7] ✗ ✓ ✗
DOODL [54] ✓ ✗ ✗
DRaFT [9], AlignProp [34] ✓ ✓ ✗
Diffusion-DPO (ours) ✓ ✓ ✓
Table 1. Method class comparison. Existing methods fail in one
or more of: Generalizing to an open vocabulary, maintaining the
same inference complexity, avoiding mode collapse/providing dis-
tributional guarantees. Diffusion-DPO addresses these issues.
3. Background
3.1. Diffusion Models
Given samples from a data distribution q(x0), noise
scheduling function αtandσt(as defined in [39]), denois-
ing diffusion models [19, 45, 49] are generative models
pθ(x0)which have a discrete-time reverse process with aMarkov structure pθ(x0:T) =QT
t=1pθ(xt−1|xt)where
pθ(xt−1|xt) =N(xt−1;µθ(xt), σ2
t|t−1σ2
t−1
σ2
tI).(1)
Training is performed by minimizing the evidence lower
bound (ELBO) associated with this model [23, 48]:
LDM=Ex0,ϵ,t,xt
ω(λt)∥ϵ−ϵθ(xt, t)∥2
2
, (2)
withϵ∼ N (0,I),t∼ U (0, T),xt∼q(xt|x0) =
N(xt;αtx0, σ2
tI). λt=α2
t/σ2
tis a signal-to-noise ra-
tio [23], ω(λt)is a pre-specified weighting function (typ-
ically chosen to be constant [19, 47]).
3.2. Direct Preference Optimization
Our approach is an adaption of Direct Preference Optimiza-
tion (DPO) [36], an effective approach for learning from
human preference for language models. Abusing notation,
we also use x0as random variables for language.
Reward Modeling Estimating human partiality to a gen-
eration x0given conditioning c, is difficult as we do not
have access to the latent reward model r(c,x0). In our set-
ting, we assume access only to ranked pairs generated from
some conditioning xw
0≻xl
0|c, where xw
0andxl
0denot-
ing the “winning” and “losing” samples. The Bradley-Terry
(BT) model stipulates to write human preferences as:
pBT(xw
0≻xl
0|c) =σ(r(c,xw
0)−r(c,xl
0)) (3)
where σis the sigmoid function. r(c,x0)can be parame-
terized by a neural network ϕand estimated via maximum
likelihood training for binary classification:
LBT(ϕ) =−Ec,xw
0,xl
0
logσ 
rϕ(c,xw
0)−rϕ(c,xl
0)
(4)
where prompt cand data pairs xw
0,xl
0are from a static
dataset with human-annotated labels.
RLHF RLHF aims to optimize a conditional distribution
pθ(x0|c)(conditioning c∼ D c) such that the latent reward
model r(c,x0)defined on it is maximized, while regulariz-
ing the KL-divergence from a reference distribution pref
max
pθEc∼Dc,x0∼pθ(x0|c)[r(c,x0)]
−βDKL[pθ(x0|c)∥pref(x0|c)](5)
where the hyperparameter βcontrols regularization.
DPO Objective In Eq. (5) from [36], the unique global
optimal solution p∗
θtakes the form:
p∗
θ(x0|c) =pref(x0|c) exp ( r(c,x0)/β)/Z(c)(6)
8230
where Z(c) =P
x0pref(x0|c) exp ( r(c,x0)/β)is the par-
tition function. Hence, the reward function is rewritten as
r(c,x0) =βlogp∗
θ(x0|c)
pref(x0|c)+βlogZ(c) (7)
Using Eq. (4), the reward objective becomes:
LDPO(θ)=−Ec,xw
0,xl
0"
logσ 
βlogpθ(xw
0|c)
pref(xw
0|c)−βlogpθ(xl
0|c)
pref(xl
0|c)!#
(8)
By this reparameterization, instead of optimizing the reward
function rϕand then performing RL, [36] directly optimizes
the optimal conditional distribution pθ(x0|c).
4. DPO for Diffusion Models
In adapting DPO to diffusion models, we consider a setting
where we have a fixed dataset D={(c,xw
0,xl
0)}where
each example contains a prompt cand a pairs of images
generated from a reference model prefwith human label
xw
0≻xl
0. We aim to learn a new model pθwhich is aligned
to the human preferences, with preferred generations to pref.
The primary challenge we face is that the parameterized dis-
tribution pθ(x0|c)is not tractable, as it needs to marginalize
out all possible diffusion paths (x1, ...xT)which lead to x0.
To overcome this challenge, we utilize the evidence lower
bound (ELBO). Here, we introduce latents x1:Tand define
R(c,x0:T)as the reward on the whole chain, such that we
can define r(c,x0)as
r(c,x0) =Epθ(x1:T|x0,c)[R(c,x0:T)]. (9)
As for the KL-regularization term in Eq. (5), following
prior work [19, 45], we can instead minimize its upper
bound joint KL-divergence DKL[pθ(x0:T|c)∥pref(x0:T|c)].
Plugging this KL-divergence bound and the definition of
r(c,x0)(Eq. (9)) back to Eq. (5), we have the objective
max
pθEc∼Dc,x0:T∼pθ(x0:T|c)[r(c,x0)]
−βDKL[pθ(x0:T|c)∥pref(x0:T|c)].(10)
This objective has a parallel formulation as Eq. (5) but de-
fined on path x0:T. It aims to maximize the reward for re-
verse process pθ(x0:T), while matching the distribution of
the original reference reverse process. Paralleling Eqs. (6)
to (8), this objective can be optimized directly through the
conditional distribution pθ(x0:T)via objective:
LDPO-Diffusion (θ) =−E(xw
0,xl
0)∼Dlogσ
βExw
1:T∼pθ(xw
1:T|xw
0)
xl
1:T∼pθ(xl
1:T|xl
0)
logpθ(xw
0:T)
pref(xw
0:T)−logpθ(xl
0:T)
pref(xl
0:T)
(11)We omit cfor compactness (details included in Supp. S2).
To optimize Eq. (11), we must sample x1:T∼pθ(x1:T|x0).
Despite the fact that pθcontains trainable parameters, this
sampling procedure is both (1) inefficient asTis usually
large ( T= 1000 ), and (2) intractable since pθ(x1:T)rep-
resents the reverse process parameterization pθ(x1:T) =
pθ(xT)QT
t=1pθ(xt−1|xt). We solve these two issues next.
From Eq. (11), we substitute the reverse decompositions
forpθandpref, and utilize Jensen’s inequality and the con-
vexity of function −logσto push the expectation outside.
With some simplification, we get the following bound
LDPO-Diffusion (θ)≤ −E(xw
0,xl
0)∼D,t∼U(0,T),
xw
t−1,t∼pθ(xw
t−1,t|xw
0),
xl
t−1,t∼pθ(xl
t−1,t|xl
0)
logσ
βTlogpθ(xw
t−1|xw
t)
pref(xw
t−1|xw
t)−βTlogpθ(xl
t−1|xl
t)
pref(xl
t−1|xl
t)
(12)
Efficient training via gradient descent is now possible.
However, sampling from reverse joint pθ(xt−1,xt|x0,c)
is still intractable and rof Eq. (9) has an expectation
overpθ(x1:T|x0). So we approximate the reverse process
pθ(x1:T|x0)with the forward q(x1:T|x0)(an alternative
scheme in Supp. S2). With some algebra, this yields:
L(θ) =−E(xw
0,xl
0)∼D,t∼U(0,T),xw
t∼q(xw
t|xw
0),xl
t∼q(xl
t|xl
0)
logσ(−βT(
+DKL(q(xw
t−1|xw
0,t)∥pθ(xw
t−1|xw
t))
−DKL(q(xw
t−1|xw
0,t)∥pref(xw
t−1|xw
t))
−DKL(q(xl
t−1|xl
0,t)∥pθ(xl
t−1|xl
t))
+DKL(q(xl
t−1|xl
0,t)∥pref(xl
t−1|xl
t))).
(13)
Using Eq. (1) and algebra, the above loss simplifies to:
L(θ) =−E(xw
0,xl
0)∼D,t∼U(0,T),xw
t∼q(xw
t|xw
0),xl
t∼q(xl
t|xl
0)
logσ(−βTω(λt) (
∥ϵw−ϵθ(xw
t, t)∥2
2− ∥ϵw−ϵref(xw
t, t)∥2
2
− 
∥ϵl−ϵθ(xl
t, t)∥2
2− ∥ϵl−ϵref(xl
t, t)∥2
2
(14)
where x∗
t=αtx∗
0+σtϵ∗,ϵ∗∼ N (0, I)is a draw from
q(x∗
t|x∗
0)(Eq. (2)). λt=α2
t/σ2
tis the signal-to-noise ratio,
ω(λt)a weighting function (constant in practice [19, 23]).
We factor the constant Tintoβ. This loss encourages ϵθ
to improve more at denoising xw
tthanxl
t, visualization in
Fig. 2. We also derive Eq. (14) as a multi-step RL approach
in the same setting as DDPO and DPOK [7, 13] (Supp. S3)
but as an off-policy algorithm, which justifies our sampling
choice in Eq. 13. A noisy preference model perspective
yields the same objective (Supp. S4).
8231
xw0 Error Change.
0.4
0.00.4
xl
0 Error Change.
||/epsilon1l−/epsilon1θ(xl
t,t)||2
2−||/epsilon1l−/epsilon1ref(xl
t,t)||2
20.4
0.00.4Figure 2. Loss surface visualization. Loss can be decreased by
improving at denoising xw
0andworsening forxl
0. A larger βin-
creases surface curvature.
5. Experiments
5.1. Setting
Models and Dataset: We demonstrate the efficacy of
Diffusion-DPO across a range of experiments. We use
the objective from Eq. (14) to fine-tune Stable Diffusion
1.5 (SD1.5) [39] and the state-of-the-art open-source model
Stable Diffusion XL-1.0 (SDXL) [33] base model. We
train on the Pick-a-Pic [24] dataset , which consists of pair-
wise preferences for images generated by SDXL-beta and
Dreamlike, a fine-tuned version of SD1.5. The prompts and
preferences were collected from users of the Pick-a-Pic web
application (see [24] for details). We use the larger Pick-a-
Pic v2 dataset. After excluding the ∼12% of pairs with ties,
we end up with 851,293 pairs, with 58,960 unique prompts.
Hyperparameters We use AdamW [27] for SD1.5 exper-
iments, and Adafactor [43] for SDXL to save memory. An
effective batch size of 2048 (pairs) is used; training on 16
NVIDIA A100 GPUs with a local batch size of 1 pair and
gradient accumulation of 128 steps. We train at fixed square
resolutions. A learning rate of2000
β2.048·10−8is used with
25% linear warmup. The inverse scaling is motivated by the
norm of the DPO objective gradient being proportional to β
(the divergence penalty parameter) [36]. For both SD1.5
and SDXL, we find β∈[2000 ,5000] to offer good per-
formance (Supp. S5). We present main SD1.5 results with
β= 2000 and SDXL results with β= 5000 .
Evaluation We automatically validate checkpoints with
the 500 unique prompts of the Pick-a-Pic validation set:
measuring median PickScore reward of generated images.
Pickscore [24] is a caption-aware scoring model trained on
Pick-a-Pic v1 to estimate human-perceived image quality.
For final testing, we generate images using the baseline and
Diffusion-DPO-tuned models conditioned on captions fromthe Partiprompt [59] and HPSv2 [55] benchmarks (1632
and 3200 captions respectively). While DDPO [7] is a re-
lated method, we did not observe stable improvement when
training from public implementations on Pick-a-Pic. We
employ labelers on Amazon Mechanical Turk to compare
generations under three different criteria: Q1: General Pref-
erence ( Which image do you prefer given the prompt? ), Q2:
Visual Appeal (prompt not considered) ( Which image is
more visually appealing? ) Q3: Prompt Alignment ( Which
image better fits the text description? ). Five responses are
collected for each comparison with majority vote (3+) being
considered the collective decision.
5.2. Primary Results: Aligning Diffusion Models
First, we show that the outputs of the Diffusion-DPO-
finetuned SDXL model are significantly preferred over the
baseline SDXL-base model. In the Partiprompt evaluation
(Fig. 3-top left), DPO-SDXL is preferred 70.0% of the time
for General Preference (Q1), and obtains a similar win-
rate in assessments of both Visual Appeal (Q2) and Prompt
Alignment (Q3). Evaluation on the HPS benchmark (Fig. 3-
top right) shows a similar trend, with a General Preference
win rate of 64.7%. We also score the DPO-SDXL HPSv2
generations with the HPSv2 reward model, achieving an av-
erage reward of 28.16, topping the leaderboard [56].
We display qualitative comparisons to SDXL-base in
Fig. 3 (bottom). Diffusion-DPO produces more appealing
imagery, with vivid arrays of colors, dramatic lighting, good
composition, and realistic people/animal anatomy. While
all SDXL images satisfy the prompting criteria to some de-
gree, the DPO generations appear superior, as confirmed by
the crowdsourced study. We do note that preferences are
not universal, and while the most common shared prefer-
ence is towards energetic and dramatic imagery, others may
prefer quieter/subtler scenes. The area of personal or group
preference tuning is an exciting area of future work.
After this parameter-equal comparison with SDXL-base,
we compare SDXL-DPO to the complete SDXL pipeline
(base + refiner) in Fig. 4. The refinement model is an image-
to-image diffusion model that improves visual quality of
generations, and is especially effective on detailed back-
grounds and faces. In our experiments with PartiPrompts
and HPSv2, SDXL-DPO (3.5B parameters, SDXL-base ar-
chitecture only), handily beats the complete SDXL model
(6.6B parameters). In the General Preference question, it
has a benchmark win rate of 69% and 64% respectively,
comparable to its win rate over SDXL-base alone. This is
explained by the ability of the DPO-tuned model (Fig. 4,
bottom) to generate fine-grained details and its strong per-
formance across different image categories. While the re-
finement model is especially good at improving the genera-
tion of human details, the win rate of Diffusion-DPO on the
People category in Partiprompt dataset over the base + re-
8232
A galaxy-colored figurine is floating  
over the sea at sunset, photorealistic A smiling beautiful sorceress  
wearing a high necked blue suit  
surrounded by swirling rainbow  
aurora, hyper-realistic, cinematic,  
post-production 
A monk in an orange robe by a round  
window in a spaceship in dramatic  
lighting Concept art of a mythical sky  
alligator with wings, nature  
documentary 
Anthropmorphic koala teaching a  
college class SDXL DPO-SDXL DPO-SDXL 
SDXL Figure 3. (Top) DPO-SDXL significantly outperforms SDXL in human evaluation. (L) PartiPrompts and (R) HPSv2 benchmark results
across three evaluation questions, majority vote of 5 labelers. (Bottom) Qualitative comparisons between SDXL and DPO-SDXL. DPO-
SDXL demonstrates superior prompt following and realism. DPO-SDXL outputs are better aligned with human aesthetic preferences,
favoring high contrast, vivid colors, fine detail, and focused composition. They also capture fine-grained textual details more faithfully.
finer model is still an impressive 67.2% (compared to 73.4%
over the base). Further evaluations, including comparisons
to Emu [11] and DDPO [61] are in Supp. S1.
5.3. Image-to-Image Editing
Image-to-image translation performance also improves af-
ter Diffusion-DPO tuning. We test DPO-SDXL on TEd-
Bench [22], a text-based image-editing benchmark of
100 real image-text pairs, using SDEdit [28] with noise
strength 0.6. Labelers are shown the original image and
SDXL/DPO-SDXL edits and asked “Which edit do you pre-
fer given the text?” DPO-SDXL is preferred 65% of the
time, SDXL 24%, with 11% draws. We show qualitative
SDEdit results on color layouts (strength 0.98) in Fig. 5.
5.4. Learning from AI Feedback
In LLMs, learning from AI feedback has emerged as a
strong alternative to learning from human preferences [25].Diffusion-DPO can admit learning from AI feedback by
directly ranking generated pairs into (yw, yl)using a pre-
trained scoring network. We use HPSv2 [55] for an al-
ternate prompt-aware human preference estimate, CLIP
(OpenCLIP ViT-H/14) [21, 35] for text-image alignment,
Aesthetic Predictor [40] for non-text-based visual appeal,
and PickScore. We run all experiments on SD 1.5 ( β=
5000 , 1000 steps, 2048 batch size). Training on PickScore
and HPS rankings increase the win rate for both raw vi-
sual appeal and prompt alignment (Fig. 6). We note that
PickScore feedback is interpretable as pseudo-labeling the
Pick-a-Pic dataset—a form of data cleaning [57, 64]. Train-
ing for Aesthetics and CLIP improves those capabilities
more specifically, in the case of Aesthetics at the expense of
CLIP. The ability to train for text-image alignment via CLIP
is a noted improvement over prior work [9]. Moreover,
training SD1.5 on the pseudo-labeled PickScore dataset
(β= 5000 , 2000 steps, batch size 2048) outperforms
8233
SDXL Base SDXL Base + Refiner DPO-SDXL 
DPO-SDXL 
SDXL 
Base+ 
Refiner Figure 4. DPO-SDXL (base only) significantly outperforms the much larger SDXL-(base+refinement) model pipeline in human evaluations
on the PartiPrompts and HPS datasets. While the SDXL refinement model is used to touch up details from the output of SDXL-base, the
ability to generate high quality details has been naturally distilled into DPO-SDXL by human preference. Among other advantages,
DPO-SDXL shows superior generation of anatomical features such as teeth, hands, and eyes. Prompts: close up headshot, steampunk
middle-aged man, slick hair big grin in front of gigantic clocktower, pencil sketch /close up headshot, futuristic young woman with glasses,
wild hair sly smile in front of gigantic UFO, dslr, sharp focus, dynamic composition /A man and woman using their cellphones, photograph
training on the raw labels. On the General Preference
Partiprompt question, the win-rate of DPO increases from
59.8%to63.3%, indicating that learning from AI feedback
can be a promising direction for diffusion model alignment.
5.5. Analysis
Implicit Reward Model As a consequence of the theo-
retical framework, our DPO scheme implicitly learns a re-
ward model and can estimate the differences in rewards be-tween two images by taking an expectation over the inner
term of Eq. (14) (details in Supp. S4.1). We estimate over
10 random t∼ U{ 0,1}Our learned models (DPO-SD1.5
and DPO-SDXL) perform well at binary preference classi-
fication (Tab. 2), with DPO-SDXL exceeding all existing
recognition models on this split. These results shows that
the implicit reward parameterization in the Diffusion-DPO
objective has comprable expressivity and generalization as
the classical reward modelling objective/architecture.
8234
Original SDXL DPO-SDXL
Figure 5. Diffusion-DPO generates more visually appealing im-
ages in the downstream image-to-image translation task. Com-
parisons of using SDEdit [28] from color layouts. Prompts are "A
fantasy landscape, trending on artstation" (top) , "High-resolution
rendering of a crowded colorful sci-fi city" (bottom).
PickScoreTraining Ranker Automated Win Rate vs. SD1.5
HPS
CLIP
PickScore HPS CLIP Aesthetics0.30.51.0
Aesthetics
Metric Model:
Figure 6. Automated head-to-head win rates under reward models
(xlabels, columns) for SD1.5 DPO-tuned on the “preferences”
of varied scoring networks ( ylabels, rows). Example: Tuning
onAesthetics preferences (bottom row) achieves high Aesthetics
scores but has lower text-image alignment as measured by CLIP.
Model PS HPS CLIP Aes. DPO-SD1.5 DPO-SDXL
Acc. 64.2 59.3 57.1 51.4 60.8 72.0
Table 2. Preference accuracy on the Pick-a-Pic (v2) validation set.
The v1-trained PickScore has seen the evaluated data.
Training Data Quality Fig. 7 shows that despite SDXL
being superior to the training data (including the yw), as
measured by Pickscore, DPO training improves its perfor-
mance substantially. In this experiment, we confirm that
Diffusion-DPO can improve on in-distribution preferences
as well, by training ( β= 5k, 2000 steps) the Dreamlike
model on a subset of the Pick-a-Pic dataset generated by
the Dreamlike model alone. This subset represents 15% of
the original dataset. Dreamlike-DPO improves on the base-
line model, though the performance improvement is limited,
y: Dreamlike
Dreamlike
DPO-Dreamlike*yw: Dreamlike
y: SDXL-
yw: SDXL-
SDXL
DPO-SDXL0.210.220.23Median PickscoreFigure 7. Diffusion-DPO improves on the baseline Dreamlike and
SDXL models, when finetuned on both in-distribution data (in case
of Dreamlike) and out-of-distribution data (in case of SDXL). yl
andywdenote the Pickscore of winning and losing samples.
perhaps because of the small size of the dataset.
Supervised Fine-tuning (SFT) is beneficial in the LLM
setting as initial pretraining prior to preference training.
To evaluate SFT in our setting, we fine-tune models on
the preferred (x, yw)pairs of the Pick-a-Pic dataset. We
train for the same length schedule as DPO using a learning
rate of 1e−9and observe convergence. While SFT im-
proves vanilla SD1.5 ( 55.5%win rate over base model), any
amount of SFT deteriorates the performance of SDXL, even
at lower learning rates. This contrast is attributable to the
much higher quality of Pick-a-Pic generations vs. SD1.5,
as they are obtained from SDXL-beta and Dreamlike. In
contrast, the SDXL-1.0 base model is superior to the Pick-
a-Pic dataset models. See Supp. S6 for further discussion.
6. Conclusion
In this work, we introduce Diffusion-DPO: a method that
enables diffusion models to directly learn from human feed-
back in an open-vocabulary setting for the first time. We
fine-tune SDXL-1.0 using the Diffusion-DPO objective and
the Pick-a-Pic (v2) dataset to create a new state-of-the-art
for open-source text-to-image generation models as mea-
sured by generic preference, visual appeal, and prompt
alignment. We additionally demonstrate that DPO-SDXL
outperforms even the SDXL base plus refinement model
pipeline, despite only employing 53% of the total model
parameters. Dataset cleaning/scaling is a promising future
direction as we observe preliminary data cleaning improv-
ing performance (Sec. 5.4). While DPO-Diffusion is an of-
fline algorithm, we anticipate online learning methods to be
another driver of future performance. There are also excit-
ing application variants such as tuning to the preferences of
individuals or small groups.
Finally, any effort in text-to-image generation
presents ethical risks, particularly when data are web-
collected. We discuss these risks in detail in Supp. S7
8235
References
[1] https://huggingface.co/carperai/sd-2-1-pickscore-
450epochs. 2
[2] Model index for researchers, 2023. 2
[3] Thomas Anthony, Zheng Tian, and David Barber. Thinking
fast and slow with deep learning and tree search. Neural
Information Processing Systems , 2017. 2
[4] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,
Deep Ganguli, Tom Henighan, Andy Jones, Nicholas
Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac
Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Ka-
mal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown,
Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan.
A general language assistant as a laboratory for alignment,
2021. 2
[5] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda
Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna
Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol
Chen, Catherine Olsson, Christopher Olah, Danny Her-
nandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-
Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey
Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite,
Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby,
Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,
Sheer El Showk, Stanislav Fort, Tamera Lanham, Timo-
thy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan
Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann,
Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom
Brown, and Jared Kaplan. Constitutional ai: Harmlessness
from ai feedback, 2022. 2
[6] Michiel A. Bakker, Martin J. Chadwick, Hannah R. Shea-
han, Michael Henry Tessler, Lucy Campbell-Gillingham, Jan
Balaguer, Nat McAleese, Amelia Glaese, John Aslanides,
Matthew M. Botvinick, and Christopher Summerfield. Fine-
tuning language models to find agreement among humans
with diverse preferences. Neural Information processing sys-
tems, 2022. 2
[7] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and
Sergey Levine. Training diffusion models with reinforce-
ment learning. arXiv preprint arXiv:2305.13301 , 2023. 1, 3,
4, 5
[8] Kevin Black et al. Training diffusion models with reinforce-
ment learning. arXiv preprint arXiv:2305.13301 , 2023. 2
[9] Kevin Clark, Paul Vicol, Kevin Swersky, and David J Fleet.
Directly fine-tuning diffusion models on differentiable re-
wards, 2023. 1, 2, 3, 6
[10] Katherine Crowson, Stella Biderman, Daniel Kornis,
Dashiell Stander, Eric Hallahan, Louis Castricato, and Ed-
ward Raff. Vqgan-clip: Open domain image generation and
editing with natural language guidance, 2022. 1
[11] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang
Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-
aofang Wang, Abhimanyu Dubey, et al. Emu: Enhanc-ing image generation models using photogenic needles in a
haystack. arXiv preprint arXiv:2309.15807 , 2023. 1, 3, 6, 2
[12] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,
Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. Alpacafarm: A simulation
framework for methods that learn from human feedback,
2023. 2
[13] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu,
Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Moham-
mad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok:
Reinforcement learning for fine-tuning text-to-image diffu-
sion models. arXiv preprint arXiv:2305.16381 , 2023. 1, 3,
4, 5
[14] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for
reward model overoptimization, 2022. 2
[15] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming
Song, Matthieu Geist, and Stefano Ermon. Iq-learn: Inverse
soft-q learning for imitation. Neural Information Processing
Systems , 2021. 5
[16] Amelia Glaese, Nat McAleese, Maja Tr˛ ebacz, John
Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,
Laura Weidinger, Martin Chadwick, Phoebe Thacker,
Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen
Huang, Ramona Comanescu, Fan Yang, Abigail See,
Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz,
Jaume Sanchez Elias, Richard Green, So ˇna Mokrá, Nicholas
Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason
Gabriel, William Isaac, John Mellor, Demis Hassabis, Ko-
ray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irv-
ing. Improving alignment of dialogue agents via targeted
human judgements, 2022. 2
[17] Gabriel Goh, James Betker, Li Jing, Aditya Ramesh, Tim
Brooks, Jianfeng Wang, Lindsey Li, Long Ouyang, Juntang
Zhuang, Joyce Lee, Prafulla Dhariwal, Casey Chu, Joy Jiao,
Jong Wook Kim, Alex Nichol, Yang Song, Lijuan Wang, and
Tao Xu. Improving image generation with better captions.
2023. 3, 11, 13
[18] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Kse-
nia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya
Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolf-
gang Macherey, Arnaud Doucet, Orhan Firat, and Nando de
Freitas. Reinforced self-training (rest) for language model-
ing, 2023. 2
[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. pages 6840–6851, 2020. 3, 4
[20] Kaiyi Huang et al. T2i-compbench: A comprehensive bench-
mark for open-world compositional inage generation, 2023.
9
[21] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-
naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-
clip, 2021. If you use this software, please cite it as below.
6
[22] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models. In Pro-
8236
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 6007–6017, 2023. 6
[23] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan
Ho. Variational diffusion models. 2021. 3, 4
[24] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-
tiana, Joe Penna, and Omer Levy. Pick-a-pic: An open
dataset of user preferences for text-to-image generation.
arXiv preprint arXiv:2305.01569 , 2023. 3, 5, 9
[25] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu,
Thomas Mesnard, Colton Bishop, Victor Carbune, and Ab-
hinav Rastogi. Rlaif: Scaling reinforcement learning from
human feedback with ai feedback, 2023. 2, 6
[26] Sergey Levine. Reinforcement learning and control as prob-
abilistic inference: Tutorial and review, 2018. 4
[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 5
[28] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. arXiv preprint arXiv:2108.01073 , 2021. 6, 8
[29] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John
Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Su-
sannah Young, Lucy Campbell-Gillingham, Geoffrey Irving,
and Nat McAleese. Teaching language models to support
answers with verified quotes, 2022. 2
[30] V olodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza,
Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver,
and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning, 2016. 2
[31] OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774,
2023. 1
[32] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob
Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan
Lowe. Training language models to follow instructions with
human feedback, 2022. 2
[33] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and
Robin Rombach. Sdxl: Improving latent diffusion models
for high-resolution image synthesis, 2023. 2, 3, 5, 1
[34] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and
Katerina Fragkiadaki. Aligning text-to-image diffusion
models with reward backpropagation. arXiv preprint
arXiv:2310.03739 , 2023. 1, 3
[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 6, 1
[36] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Er-
mon, Christopher D. Manning, and Chelsea Finn. Direct
preference optimization: Your language model is secretly a
reward model, 2023. 1, 2, 3, 4, 5, 8[37] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté
Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Han-
naneh Hajishirzi, and Yejin Choi. Is reinforcement learning
(not) for natural language processing: Benchmarks, base-
lines, and building blocks for natural language policy opti-
mization, 2022. 2
[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. Stable diffusion
2.https://huggingface.co/stabilityai/
stable-diffusion-2 . Accessed: 2023 - 11 - 16. 1
[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 10684–10695, 2022. 1, 3, 5
[40] Christoph Schuhmann. Laion-aesthetics. https://
laion.ai/blog/laion-aesthetics/ , 2022. Ac-
cessed: 2023 - 11- 10. 3, 6, 1
[41] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms, 2017. 2
[42] Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias,
and Yaniv Leviathan. A picture is worth a thousand words:
Principled recaptioning improves image generation, 2023. 3
[43] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive
learning rates with sublinear memory cost. In International
Conference on Machine Learning , pages 4596–4604. PMLR,
2018. 5
[44] Joar Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov,
and David Krueger. Defining and characterizing reward
hacking. Neural Information Processing Systems , 2022. 2
[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
ence on machine learning , pages 2256–2265, 2015. 3, 4
[46] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 6
[47] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. Advances in neural
information processing systems , 32, 2019. 3
[48] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.
Maximum likelihood training of score-based diffusion mod-
els. In Neural Information Processing Systems , 2021. 3
[49] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In International Conference on Learning Represen-
tations , 2021. 3
[50] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,
Ryan Lowe, Chelsea V oss, Alec Radford, Dario Amodei, and
Paul Christiano. Learning to summarize from human feed-
back. Neural Information Processing Systems , 18, 2020. 2
[51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer,
8237
Moya Chen, Guillem Cucurull, David Esiobu, Jude Fer-
nandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Vik-
tor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-
renev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut
Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning
Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-
qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,
Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,
Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan
Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. Llama 2: Open foundation and fine-
tuned chat models, 2023. 1
[52] Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi
Huang, Leandro von Werra, Clémentine Fourrier, Nathan
Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M.
Rush, and Thomas Wolf. Zephyr: Direct distillation of lm
alignment, 2023. 2
[53] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis
Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey
Irving, and Irina Higgins. Solving math word problems with
process- and outcome-based feedback, 2022. 2
[54] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil
Naik. End-to-end diffusion latent optimization improves
classifier guidance. arXiv preprint arXiv:2303.13703 , 2023.
3, 1
[55] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng
Zhu, Rui Zhao, and Hongsheng Li. Human preference score
v2: A solid benchmark for evaluating human preferences of
text-to-image synthesis. arXiv preprint arXiv:2306.09341 ,
2023. 3, 5, 6
[56] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng
Zhu, Rui Zhao, and Hongsheng Li. Hpsv2 github. https:
//github.com/tgxs002/HPSv2/tree/master ,
2023. Accessed: 2023 - 11 - 15. 5
[57] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V
Le. Self-training with noisy student improves imagenet
classification. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 10687–
10698, 2020. 6
[58] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai
Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagere-
ward: Learning and evaluating human preferences for text-
to-image generation. arXiv preprint arXiv:2304.05977 ,
2023. 3
[59] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,
Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and
Yonghui Wu. Scaling autoregressive models for content-rich
text-to-image generation, 2022. 5
[60] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Song-
fang Huang, and Fei Huang. Rrhf: Rank responses to alignlanguage models with human feedback without tears. Neural
Information Processing Systems , 2023. 2
[61] Yinan Zhang et al. Large-scale reinforcement learning for
diffusion models, 2024. 6, 1
[62] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mo-
hammad Saleh, and Peter J. Liu. Slic-hf: Sequence likeli-
hood calibration with human feedback, 2023. 2
[63] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei
Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao
Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin
Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang
Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang
Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang.
Secrets of rlhf in large language models part i: Ppo, 2023. 2
[64] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanx-
iao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-
training and self-training. Advances in neural information
processing systems , 33:3833–3845, 2020. 6
8238
