Compressing Large Language Models using Low Rank
and Low Precision Decomposition
Rajarshi Saha
Stanford UniversityNaomi Sagan
Stanford UniversityVarun Srivastava
Stanford University
Andrea J. Goldsmith
Princeton UniversityMert Pilanci
Stanford University
Abstract
The prohibitive sizes of Large Language Models (LLMs) today make it diffi-
cult to deploy them on memory-constrained edge devices. This work introduces
CALDERA – a new post-training LLM compression algorithm that harnesses the
inherent low-rank structure of a weight matrix Wby approximating it via a low-
rank, low-precision decomposition as W≈Q+LR. Here, LandRare low rank
factors, and the entries of Q,LandRare quantized. The model is compressed by
substituting each layer with its Q+LRdecomposition, and the zero-shot perfor-
mance of the compressed model is evaluated. Additionally, LandRare readily
amenable to low-rank adaptation, consequently enhancing the zero-shot perfor-
mance. CALDERA obtains this decomposition by formulating it as an optimization
problem minQ,L,R∥(Q+LR−W)X⊤∥2
F, where Xis the calibration data, and
Q,L,Rare constrained to be representable using low-precision formats. Theoreti-
cal upper bounds on the approximation error of CALDERA are established using
a rank-constrained regression framework, and the tradeoff between compression
ratio and model performance is studied by analyzing the impact of target rank and
quantization bit budget. Results illustrate that compressing LlaMa- 2 7B/13B/70B
and LlaMa- 3 8B models using CALDERA outperforms existing post-training LLM
compression techniques in the regime of less than 2.5bits per parameter. The
implementation is available at: https://github.com/pilancilab/caldera.
1 Introduction
Large Language Models (LLMs) stand out due to their remarkable ability to generate human-like
text, thereby supporting a diverse range of applications ranging from writing assistance to code
generation. These models leverage vast datasets and significant computational resources to achieve
their impressive functionality. The architecture of LLMs typically includes multiple layers, each with
weight matrices essential for encoding various aspects of the training data – from simple syntactic
patterns to complex semantic relationships. However, the substantial size of these trained models
leads to high computational costs and considerable energy consumption during inference, which can
be challenging for deployment in resource-constrained environments. As LLMs continue to expand in
scale, compression techniques to reduce the memory and computational requirements of the models
are becoming crucial to ensure their broad accessibility.
Due to the correlated nature of language syntax and semantics learned during training, often, the
weight matrices of LLMs exhibit redundancy, which manifests as a low-rank structure. This re-
dundancy suggests the potential for compression without substantial loss in performance. This
work introduces CALDERA: Calibration Aware Low-Precision DEcomposition with Low- Rank
Adaptation, which compresses LLMs by leveraging the approximate low rank structure inherent in
38th Conference on Neural Information Processing Systems (NeurIPS 2024).these weight matrices. Given a matrix W∈Rn×d, CALDERA approximates it as W≈Q+LR,
where Q∈Rn×d,L∈Rn×kandR∈Rk×d. Here, the leftandright low rank factors, respectively
LandR, are tall and wide matrices, and kis the target rank. Furthermore, the entries of Q,LandR
are represented using low-precision formats with BQ,BLandBRbits per entry, respectively.
0 1000 2000 3000 4000
Index104
103
102
101
100101MagnitudeSingular values of LlaMA-2 7B Query matrices
Query
Figure 1: Decaying spectrum of weight ma-
trices (aka, "approximate low-rank")
W
dnFull precision
≈ L
mnBLbits
Rm
dBRbits
+ Q n
dBQbitsFigure 2: CALDERA decomposes a full-precision weight ma-
trix into a low-rank component ( LR), which captures the con-
tribution of the top singular values using BL,BRbits, and Qfor
the trailing singular values with BQbits, enabling flexible pre-
cision settings for each component. Typically, BQ<BL,BR.
Since the singular value profile (aka spectrum) of the weight matrices of an LLM follow a decaying
profile as shown in Fig. 1, the low-rank factors LandRcapture the effect of the large singular
components of Wwith high fidelity. Moreover, the backbone Q, which is quantized aggressively
– for instance, using BQ= 2 bits, coarsely captures the essence of the moderately decaying and
low singular components of W. CALDERA substitutes each weight matrix Win an LLM, with
its approximate low-precision and low-rank decomposition Q+LR, resulting in a post-training
quantization strategy that delivers state-of-the-art zero-shot performance. In addition, since usually
k≪min{n, d}, implying that the total number of parameters in LRis much smaller compared to
the number of entries in W(i.e.,k(n+d)≪nd), CALDERA can readily fine-tune (or "adapt" ) the
low rank factors LandRin order to boost the zero-shot results.
1.1 Significance and Related Works
Recent efforts have explored various avenues for compression, including but not limited to weight
pruning, quantization, and the use of parameter-efficient training methods – each approach offering
distinct advantages and tradeoffs. This section briefly reviews the current methodologies, highlighting
the contributions and limitations of some studies closely related to this work.
LLM Compression and Outlier Mitigation : Recent studies like SmoothQuant [ 42], OPTQ [ 8],
QuIP [ 3], AQLM [ 7], and QuIP# [ 36] consider the challenging regime of sub- 4bit post-training
LLM quantization. These works collectively emphasize the need to manage the impact of outliers,
i.e., weights with unusually high magnitudes. Accommodating outliers necessitates choosing the
dynamic range (or scale) of a quantizer to be high, consequently increasing the quantization error.
QuIP equalizes (and reduces) the weight matrices by using a randomized matrix transform, and
subsequently, QuIP# employs E8 lattice to make the weights more amenable to vector quantization.
Both QuIP and QuIP# use a refined variant of the column-wise quantization method proposed in
OPTQ, wherein error feedback from previously quantized columns of a matrix is used to compensate
for the error incurred while quantizing subsequent columns. CALDERA utilizes this diverse arsenal
of strategies and builds on top of QuIP#, while capitalizing on the approximate low-rank structure of
LLM weight matrices. While it is possible to obtain even more aggressively compressed LLMs [ 24],
this approach requires training from scratch, which is computationally demanding.
Parameter Efficient Fine-Tuning (PEFT) : In a related yet distinct vein of work, PEFT methods have
gained significant momentum, aiming to adapt LLMs to specific tasks without extensive computational
overhead. Recent studies such as QLoRA [ 5], LoftQ [ 22], and LQ-LoRA [ 13] have explored the
intersection of PEFT and quantization, demonstrating that fine-tuning through low-rank updates, as
originally proposed in LoRA [ 16], can mitigate the performance losses due to quantization. Given
that CALDERA yields a decomposition Q+LR, the low-rank components are particularly suitable
for fine-tuning with any existing PEFT methods, thereby enhancing the zero-shot capabilities.
Low Rank Approximation : The rank- kapproximation of a matrix A∈Rn×dcan be represented
by the factorization A≈LR, with L∈Rn×kandR∈Rk×d, where k≤min{n, d}. Known
as the Burer-Monteiro factorization, this method substantially decreases the number of parameters,
2thus reducing computational demands. Recent studies such as LoRD [ 18], ASVD [ 44], FWSVD
[15], LASER [ 33], LQER [ 45], and ZeroQuant-V2 [ 43] have explored the efficacy of low-rank
structures in LLM weights, treating low-rank factorization and quantization independently. In
contrast, LPLR [ 31] approaches this by uniquely formulating a joint optimization problem for generic
matrices, while simultaneously leveraging the equalization property of randomized transforms,
as in [ 3]. CALDERA formally leverages this inherent low-rank structure for LLM compression
alongside existing frameworks such as QuIP# [ 36] and LoftQ [ 22], providing additional flexibility
for compression. Furthermore, rigorous theoretical guarantees are derived using a rank-constrained
regression framework for obtaining a low precision and low-rank decomposition, thereby also
analytically demonstrating its superiority over rank-agnostic strategies.
2 Problem Formulation
In a neural network layer, a weight matrix Wtransforms an input activation xinto an output activation
given by Wx. This transformation can be succinctly described using the matrix’s singular value
decomposition (SVD). For any matrix A∈Rn×d, the SVD is A=P
iσiuivi, where σi,ui,viare
theithsingular value and the corresponding left and right singular vectors, respectively. The impact
of each singular component uivion the matrix’s transformation is determined by the magnitude
ofσi. Given that weight matrices exhibit a decaying singular value profile (Fig. 1), indicating an
approximate low-rank structure, lesser contributing singular components can be pruned with minimal
impact on the functionality of the matrix, ensuring minimal distortion in the output activations.
CALDERA approximates the weight matrix of a neural network, W, as a low-precision, low-rank
decomposition, W≈Q+LR, with all components Q,L,Rin low-precision format. Unlike
previous works such as [ 13,22,44,45], which represent the low-rank factors LandRin high-
precision (16 or 32-bit floating point), this work extends their representation to low-precision. This
further reduces the memory footprint while preserving performance. Alternatively, for the same
memory footprint, it allows the target rank kto be higher, thereby capturing the low rank structure
with higher fidelity by including more of the higher singular value components. The following
paragraph formalizes this as a constrained optimization problem.
For a given quantizer, let Qdenote the set of discrete quantization points in R. ForB-bit quantization,
the cardinality of Qsatisfies log2|Q| ≤B. Consider a matrix W∈Rn×d. The goal of this work is
to obtain an decomposition W≈Q+LRby approximately solving the minimization problem
min
Q,L,R(Q+LR−W)X⊤2
Fsubject to Q∈Qn×d
Q,L∈Qn×k
L,andR∈Qk×d
R.(1)
Here,QQ,QLandQRdenote the lattice codebooks used to quantize Q,LandR, using BQ,BL
andBRbits, respectively. Furthermore, X∈Rm×dis a calibration matrix that aims to preserve the
Frobenius norm error of the compressed layer output activations. If Wis the first layer’s weight
matrix, Xincludes input embeddings from a calibration dataset, such as a subset of RedPajama
[34], with the ithrow representing the ithdatapoint. For intermediate layers, Xcontains the input
activations, which are the output activations of the preceding layer.
Using the Frobenius norm of the output of a layer as a proxy objective for quantizing the weight
matrices of an LLM is a popular strategy, and was used in prior work of Nagel et al. [27] . This proxy
objective function is particularly useful for post-training quantization of LLMs because their large
size makes it difficult to apply sophisticated compression methods.
3 Proposed Algorithm: Calibration-Aware Low-Precision Decomposition
with Low Rank Adaptation
This section introduces CALDERA to approximately solve (1)and get a Q+LRdecomposition
of a weight matrix Wusing the calibration matrix X. The pseudocode is provided in Alg. 1. It
consists of a nested loop for alternately optimizing the variables Q,LandR. Suppose QQ,QLand
QR, respectively, denote quantizers used for quantizing Q,LandR. For instance, they can refer to
uniformly dithered scalar quantizers, as described in App. G.2. Initially, the low-rank factors are set
to0, andWis quantized using the LDLQ quantizer proposed in [ 3, §3.1]. LDLQ is an adaptive
3quantization method that iteratively quantizes [8] each column of Wusing QQto get Qas
Q(k)= Q Q(W(k)+ (W(1:k−1)−Q(1:k−1))ak), (2)
where Q(k),W(k)denote the kthcolumn, W(1:k−1)denotes the first kcolumns, QQhas a bit-budget
BQ, andak∈Rk−1is a learnable sequence of vectors. Update Eq. (2)incorporates linear feedback
from already quantized columns, it can be seen that Qsatisfies Q= Q Q(W+ (W−Q)M), where
the feedback matrix Mis a strictly upper triangular matrix with columns ak. Defining H≜1
mX⊤X
to be the (scaled) Hessian of the least squares objective in (1), [3] show that the optimal feedback
matrix is the Mobtained from the LDL decomposition of mH, given by mH= (M+I)D(M+I)⊤.
Subsequently, Qis fixed and the Low-Precision Low-Rank (LPLR) factorization of the residual,
(W−Q), is computed. This is done by the LPLRF ACTORIZE submodule (Alg. 2), which is a
refined version of the LPLR algorithm proposed in [31]. For a given matrix A, Alg. 2 minimizes
min
L,R(LR−A)X⊤2
Fsubject to L∈Qn×k
L,andR∈Qk×d
R, (3)
where QLandQRuseBLandBRbits, respectively. In contrast to [ 31], the objective in (3)is
calibration-data aware. Therefore, the update equations are derived using a rank-constrained regres-
sion framework, as described in App. B. Moreover, lines 7to14inLPLRF ACTORIZE iteratively
refine the estimates of LandR, and can only yield a smaller Frobenius norm error. The left and right
low-rank factor update equations are described as follows.
Initialization : In the absence of quantization constraints, a globally optimal solution to the optimiza-
tion problem (3)can be found as described later in lemma 4.2. Consequently, the low-rank factors are
initialized using rank-constrained regression in lines 2–4. Since subsequent quantization disrupts
optimality of the solution, the factors are iteratively updated to minimize this distortion.
Updating L: To update the left factor L, lines 5and9of Alg. 2 solves minZ∥(ZR−A)X⊤∥2
F.
For a fixed R, this is a least squares minimization, whose solution is available is closed form as
`L= 
AX⊤ 
RX⊤†=AHR⊤(RHR⊤)−1, as derived in App. C.1.
Updating R: Line 8of Alg. 2, updates the right factor Rby keeping Lfixed and solving
minZ∥(LZ−A)X⊤∥2
F. As this is an under-determined linear system, there exist multiple solutions
forZ, all attaining the same objective function value. It is shown in App. C.1 that `R=L†AHH†is
a solution. The corresponding error is also obtained, which is used in the derivation of Thm. 4.1.
Computational Complexity : A high-level calculation is provided here, and detailed discussions can
be found in App. D. It is worthwhile to note that the closed form expressions of `Land`R, which are
iteratively quantized, are functions of the Hessian H=1
mX⊤X. Therefore, Hcan be computed
offline initially, per LLM, by doing a single forward pass, and subsequently used for all model
quantization experiments. For each layer, this pre-processing includes computing Hand its LDL
decomposition, along with computing HH†, requiring a total of O(md2+2d3)multiplications. Each
outer iteration involves an LDLQ quantization. Quantizing the kthcolumn has complexity O(nk),
since feedback from kalready quantized columns need to be incorporated. Hence, quantizing a matrix
inRn×dentails O(n2+ 3n)complexity. Moreover, LPLRF ACTORIZE requires O(m2(n+d))to
initialize, and subsequently, each inner iteration entails O(ndk). Assuming n, d≥m≫k, and
keeping only the dominant terms, the total complexity of CALDERA, not including the complexity
of the pre-processing discussed earlier, is O 
Tout 
n2+m2(n+d) +ndkTin
.
Fine tuning via Low-Rank Adaptation : Once the weight matrices of each layer are replaced by
itsQ+LRapproximation, the zero-shot performance of (post-training) quantized model can be
evaluated. §5 shows that CALDERA quantized models outperform existing strategies. Additionally, if
desired, the low-rank factors LandRcan be further fine-tuned using low-rank adaptation [ 13,16,22]
on a small task-specific dataset. While the initialization of the fine-tuning step has quantized Q,L
andR, the fine-tuned factors are represented using 16-bits ( BF16 format). Although this leads to a
slight increase in the memory footprint, the performance gains from fine-tuning are substantial.
4 Approximation Error Analysis
The approximation error upper bounds are derived via a rank-constrained regression framework. Thm.
4.1 below (formally stated and proved in App. C.4) is an informal version of the main theoretical
result of this paper, and provides an upper bound on the Frobenius norm error when CALDERA
4approximates a weight matrix Wis asW≈Q+LRby solving the optimization problem (1)using
Alg. 1. For convenience of analysis, it is assumed that the dynamic range of QQ, denoted as R,
is chosen to be high enough, ensuring it remains unsaturated. Consequently, for a scalar input the
quantization error from QQhas zero mean and bounded variance, given by∆2
4=R2
(2BQ−1)2.
Theorem 4.1. Approximation error of CALDERA (Informal) Given W∈Rn×dandX∈Rm×d
withm≤d, letDbe obtained from the LDL decomposition X⊤X=mH= (M+I)D(M+I)⊤,
andλmax,λmindenote the max and min eigenvalues of H. Additionally, let Q≜LDLQ (W,QQ),
where QQhas dynamic range Rand bit-budget BQ, the quantization error be η≜QQ(Q+ (W−
Q)M)−(Q+ (W−Q)M), and σ1≥. . .≥σk. . .be the singular values of X(W−Q)⊤. If the
target rank ksatisfies 0.25λ1/2
min(mσ1)−1λ−3/2
maxP
i>kσ2
i≤k≤m, and the dynamic ranges of QL
andQRare set as RL=2σ1
σk√mλminandRR=σ1, then Q,LandRreturned by Alg. 1 satisfy
1
nmE(Q+LR−W)X⊤2
F≤1
nX
i>kEλi(ηDη⊤) +ϵ≲4dλmaxR2
π(2BQ−1)2
1−k
2n2
+ϵ,
while utilizing an average budget of1
2log2
kσ3
1
mϵσ kλmax
λminp
d/n
bits per parameter for the low-rank
factors LandR, when n≈d. Here, the expectation is over the stochasticity of the quantizers.
An informal version of the main result is provided here, and the formal version including specific
constant values, along with the derivation, can be found in App. C.4. The requirement m≤dis not
restrictive, because when His positive definite, (1)can be rewritten as ∥(Q+LR−W)X⊤∥2
F=
∥(Q+LR−W)H1/2∥2
F, ensuring m=d. This is detailed further in App. C.5. The approximation
error upper bound given by Thm. 4.1 can be directly compared with the result of Chee et al. [3, Thm.
1], which states that for vanilla LDLQ without LPLRF ACTORIZE ,
E(Q−W)X⊤2
F≤E
Tr 
ηDη⊤
=nX
i=1Eλi(ηDη⊤). (4)
Evidently, Alg. 1 yields a smaller error provided,P
i>kEλi(ηDη⊤)<Pk
i=1Eλi(ηDη⊤)−ϵ,
where ϵcan be chosen to be arbitrarily small. Furthermore, since the expression in Thm. 4.1 consists
of two terms, namely, the rank-constrained regression error, which depends on the target rank k, and
the additive quantization error of ϵ, which is dictated by the bit-budgets used for LandR, this upper
Algorithm 1: CALDERA :Calibration Aware Low-Precision DEcomposition with Low- RankAdaptation
Input: Matrix: W∈Rn×d, Target rank: k, Calibration matrix: X∈Rm×d, Outer and inner
iterations: Tout,Tin, Quantizers: QQ,QL,QR, Flag: EnableLoRA, Fine-tune rank: r
Output: LPLR decomposition: Q∈Qn×d
Q,L∈Qn×k
L,R∈Qk×d
Rs.t.WX⊤≈(Q+LR)X⊤
1Initialize: t←0,L0←0,R0←0,MinError ← ∞
2while t <Toutdo
3 Update Q:Qt+1←LDLQ (W−LtRt,QQ)
4 Update low-rank factors:
Lt+1,Rt+1←LPLRF ACTORIZE (W−Qt+1, k,X,QL,QR,Tin)
5 if∥(Qt+1+Lt+1Rt+1−W)X⊤∥2
F<MinError then
6 Qbest←Qt+1,Lbest←Lt+1,Rbest←Rt+1,
MinError ← ∥(Qt+1+Lt+1Rt+1−W)X⊤∥2
F
7 end
8 t←t+ 1
9end
10ifEnableLoRA is TRUE then
11 Further Fine-tune top- rsingular components of LbestandRbestto16-bit precision using
Low-Rank Adaptation (LoRA) (as in [13, 16, 22])
12end
13return Qbest,Lbest,Rbest
5Algorithm 2: LPLRF ACTORIZE (A, k,X,QL,QR, Tin): LPLR factorization submodule
Input: Matrix: A∈Rn×d, Target rank: k, Calibration matrix: X∈Rm×d, Iterations: Tin,
Quantizers: QL,QR
Output: Low precision Low Rank factors: L∈Qn×k,R∈Qk×ds.t.AX⊤≈LRX⊤
1Initialize: Iteration counter :i←0
2Compute SVD of XasUeΣV⊤.
3Compute SVD of U⊤XA⊤as`U`Σ`V⊤
4Get right low-rank factor: R0←QR(I⊤
k`Σ`V⊤)
5Get left low-rank factor: L0≜QL(`L0), where `L0= arg minZ∈Rk×d(ZR0−A)X⊤2
F
6Lbest←L0,Rbest←R0,MinError ←(L0R0−A)X⊤2
F.
7while i <Tindo
8 Update right: Ri+1←QR(`Ri+1), where `Ri+1= arg minZ∈Rk×d(LiZ−A)X⊤2
F
9 Update left: Li+1←QL(`Li+1), where `Li+1= arg minZ∈Rn×k(ZRi−A)X⊤2
F
10 if(Li+1Ri+1−A)X⊤2
F<MinError then
11 Lbest←Li+1,Rbest←Ri+1,MinError ←(Li+1Ri+1−A)X⊤2
F
12 end
13 i←i+ 1
14end
15return Lbest,Rbest
bound can be made arbitrarily small by ensuring that the two terms are approximately equal, i.e.,
E∥(Q+LR−W)X⊤∥2
Fis upper bounded by 2ϵ. This is apparent in the following regimes:
(i)k≪n: In this regime, kis treated as a constant as ngrows. Then, if the bit-budget BQsatisfies
BQ≥log2
2R(πϵ)−1/2p
nmd λ max+ 1
,then E(Q+LR−W)X⊤2
F≤2ϵ.
(ii)k= O( n): For a fixed BQ, ifkis allowed to grow with dimension n, then choosing kto satisfy
k≥2n−(2BQ−1)R−1(πϵ)1/2(mdλ−1/2
max)√nensures E(Q+LR−W)X⊤2
F≤2ϵ.
This implies that the upper bound can be made arbitrarily small by either (i)increasing the bit-budget
of the backbone, i.e., BQ, for a fixed rank k, or(ii)increasing the rank kfor a fixed BQ, for example,
BQ= 2. Alternatively stated, this provides a tunable knob for controlling the error by trading off the
allocated bit-budget between the backbone Qand the low-rank factors L,R.
4.1 Analysis Outline
In this section, a brief proof sketch is presented, highlighting the major challenges in the proof and
how they are addressed. For analysis, Qis assumed to be updated prior to L,Rin Alg. 1. However,
in practice, the update order is inconsequential, and can be swapped, depending on whichever yields
a smaller error. The complete derivation of the approximation error is provided in App. C. A key
ingredient of the proof is the solution of the rank-constrained regression problem, which is defined as,
min
rank(Z)≤k∥XZ−Y∥2
F. (5)
Although this problem is non-convex, it can be solved to global optimality via two SVDs [ 41]. The
following lemma characterizes the solution to the optimization problem in (5).
Lemma 4.2. Given Y∈Rm×n, and full rank X∈Rm×d, where m≤d. LetX=UeΣV⊤and
U⊤Y=`U`Σ`V⊤denote full SVDs of XandU⊤Y. Then, for k≤m, the solution of (5)is given by
Z∗≜arg min
rank(Z)≤k∥XZ−Y∥2
F=
VImΣ−1`UIk
I⊤
k`Σ`V⊤
,
6where Σ:=eΣIm∈Rm×mis a diagonal matrix consisting of the non-zero singular values of X.
Moreover, denoting the non-zero singular values of Yas{σi(Y)}m
i=1, the optimal value of (7)is
min
rank(Z)≤k∥XZ−Y∥2
F=∥XZ∗−Y∥2
F=mX
i=k+1σ2
i(Y). (6)
The complete lemma (with the case m > d ), and the derivation, are provided in App. B. Using
lemma 4.2, the approximation error of LPLRF ACTORIZE is analyzed in App. C.3. Specifically,
lemma C.3 shows that for any input matrix A, Alg. 2 with suitably chosen BLandBR, ensures that
E(LR−A)X⊤2
F, as in (3), can be upper bounded by twice the sum of squared trailing singular
values, (ref. (6)). While proving lemma C.3, it is assumed that if QLorQRgets saturated, a trivial
output of L=0,R=0is returned. Therefore, lemmas C.1 and C.2 specify choosing the dynamic
ranges RRandRLto be sufficiently high so that saturation happens with a very low probability. The
proof of Thm. 4.1 is completed by using the LDL decomposition of mHas proposed in [ 3], along
with an application of Marchenko-Pastur approximation to bound the expected eigenvalues of the
quantization error, i.e., Eλi 
ηη⊤
, yielding the final inequality.
5 Numerical Simulations
The efficacy of CALDERA is assessed by using it to compress four popular open source LLMs
from Meta AI, namely, LLaMa-2 7B, LLaMa-2 13B, LLaMa-2 70B [ 35] and LLaMa-3 8B [ 26].
The framework is built in PyTorch on top of the QuIP# [ 36] and LoftQ [ 22], and is available at
https://github.com/pilancilab/caldera.
Baselines . The full-rank matrix Q, also referred to as the backbone, is quantized to 2-bits using the
LDLQ procedure from QuIP [ 3,36], employing an E8 lattice quantizer [ 39]. For CALDERA, which
allows even the low-rank factors, LandR, to be represented in low-precision, the quantization is
also performed with an E8 lattice. Prior to running Alg. 1, a randomized Hadamard transform (RHT)
is applied to the left and the right of the input weight matrix, as the incoherence pre-processing step,
to equalize the magnitude of the entries making them more robust to quantization. In other words,
CALDERA decomposition is performed on fW≜H⊤
LWH R, where HLandHRare Hadamard
matrices, right-multiplied by a diagonal matrix with i.i.id. {±1}entries. In addition, the Hessian
matrix obtained from the calibration data is substituted by eH≜H⊤
RHH R. As described in [ 3], this
improves the quantization error incurred by LDLQ. Further details are provided in App. E.2.
Metrics . The performance of CALDERA is evaluated using perplexity on the test splits of the
Wikitext2 [ 25] and C4 [ 6] datasets, as well as task-specific goodness-of-fit metrics such as zero-
shot accuracy for sequence classification. Specifically, zero-shot accuracy was measured on the
Winogrande [ 19], RTE [ 1,40], PiQA [ 2], ARC-Easy, and ARC-Challenge [ 4] tasks. App. E.3
provides more details regarding these benchmarks. Perplexity was measured using a sequence length
equal to the model’s maximum context length, i.e., 4096 for LLaMa-2, and 8192 for LLaMa-3.
Zero-shot experiments were performed using EleutherAI’s Language Model Evaluation Harness [ 9].
5.1 Zero-shot Results
Tables 1 and 2 report the perplexities and accuracies for CALDERA with varying target rank (k)ofL
andR. A smaller value is better for perplexity, which is defined as the exp(·)of the training objective,
while zero-shot accuracies are reported as percentages. Per-parameter bit budgets range from 2.1
(e.g., rank- 64factors in 4-bit precision) to 2.4bits (e.g., rank- 64factors in half precision or rank- 256
factors in 4-bit precision). For comparison, the Q+LRdecomposition of weight matrices found in
the QuIP# codebase was performed on each model. For the sake of direct comparison, fine-tuning
of the diagonal matrices in RHT was omitted. As QuIP# does not support quantized factors, Land
Rare rank-64 in order to ensure that the per-parameter bit-budget remains in the 2−2.4range. As
another baseline comparison, each model is quantized using QuIP# without any low-rank factors.
Results for the unquantized models are also provided.
For all models, the rank- 256CALDERA decomposition with 4-bit factors had the lowest perplexity
and generally had the highest accuracies. As CALDERA supports quantizing low-rank factors
with minimal performance loss, more singular components can be captured compared to using
half-precision factors while employing the same number of bits. Consequently, the low-rank factors
7Table 1: Zero-shot perplexities (denoted by ↓) and accuracies ( ↑) for LLaMa- 2.BQ= 2bits throughout.
Method Rank BL(= B R)Avg Bits Wiki2 ↓C4↓Wino ↑RTE ↑PiQA ↑ArcE ↑ArcC ↑
CALDERA (7B) 64 16 2.4 7.36 9.47 64.6 66.4 73.7 60.8 31.7
CALDERA (7B) 64 4 2.1 7.37 9.74 63.7 62.1 72.3 60.9 31.7
CALDERA (7B) 128 4 2.2 6.76 8.83 63.8 59.9 75.1 65.1 34.6
CALDERA (7B) 256 4 2.4 6.19 8.14 66.0 60.6 75.6 63.6 34.0
QuIP# (7B, No FT) 64 16 2.4 7.73 10.0 63.1 66.8 71.7 63.2 31.7
QuIP# (7B, No FT) 0 — 2 8.23 10.8 61.7 57.8 69.6 61.2 29.9
CALDERA (13B) 64 4 2.08 6.04 7.98 66.9 61.0 76.0 69.5 37.2
CALDERA (13B) 128 4 2.16 5.72 7.66 67.9 58.5 76.0 68.5 38.7
CALDERA (13B) 256 4 2.32 5.41 7.21 66.9 62.1 76.2 70.3 40.4
QuIP# (13B, No FT) 0 — 2 6.06 8.07 63.6 54.5 74.2 68.7 36.2
CALDERA (70B) 128 4 2.1 4.11 5.95 75.5 69.3 79.8 76.9 47.7
CALDERA (70B) 256 4 2.2 3.98 5.76 77.6 71.5 79.8 79.5 47.4
QuIP# (70B, No FT) 0 — 2 4.16 6.01 74.2 70.0 78.8 77.9 48.6
Unquantized (7B) 0 — 16 5.12 6.63 67.3 63.2 78.5 69.3 40.0
Unquantized (13B) 0 — 16 4.57 6.05 69.5 61.7 78.8 73.2 45.6
Unquantized (70B) 0 — 16 3.12 4.97 77.0 67.9 81.1 77.7 51.1
Table 2: Zero-shot perplexities (denoted by ↓) and accuracies ( ↑) for LLaMa- 3 8B.BQ= 2bits throughout.
Method Rank BL(= B R)Avg Bits Wiki2 ↓C4↓Wino ↑RTE ↑PiQA ↑ArcE ↑ArcC ↑
CALDERA 64 16 2.4 9.22 10.5 68.9 63.9 72.9 69.9 36.5
CALDERA 64 4 2.1 10.6 11.8 66.9 58.5 71.8 68.2 34.3
CALDERA 128 4 2.2 9.21 10.5 67.6 69.7 74.4 71.8 36.3
CALDERA 256 4 2.4 8.22 9.56 69.7 65.0 75.1 73.2 40.0
QuIP# (No FT) 64 16 2.4 10.9 11.8 66.5 57.0 69.6 63.8 31.0
QuIP# (No FT) 0 — 2 13.8 15.6 63.2 52.7 67.6 57.6 28.2
Unquantized 0 — 16 5.54 7.01 73.5 68.6 79.7 80.1 50.2
can regain the performance that was compromised when the backbone Qwas quantized to 2bits.
Since zero-shot experiments have some inherent randomness and low-rank regularization effects [ 33],
the zero-shot accuracies reported here are not as directly indicative of quantization performance as
the perplexity results. In addition, §5.3, demonstrates that degradation in zero-shot accuracy can be
recovered via LoRA fine-tuning. It is worthwhile to note these results substantiate the claims of [ 17],
which report that low-bit quantization of LLaMa- 3 8B, significantly deteriorates model performance
across various post-training quantization techniques, more so than with the LLaMa- 2series.
5.2 Fine-tuning of Randomized Hadamard Transform (RHT) Parameters
As CALDERA presents a general optimization framework for matrix decompositions of the form
Q+LR, it can easily be extended with additional heuristics to improve performance. This section
serves as a proof of concept, by examining one such heuristic: Fine-tuning of randomized Hadamard
transform parameters. This technique, proposed in QuIP# [ 36], involves fine-tuning the diagonal
Rademacher matrices with ±1entries in the RHT to minimize the cross-entropy loss between the
output of the original and quantized models on the calibration dataset. Subsequently, RHT fine-tuning
is performed on the models quantized using CALDERA in §5.1.1Details on specific fine-tuning
hyperparameters can be found in App. E.4.
Perplexity and zero-shot results in Tables 3 and 4 match the trends in §5.1, i.e., CALDERA with
rank- 256factors typically performs best, with the exception of RTE. In addition, perplexities are
substantially lower than without the fine-tuning of randomized Hadamard transform parameters.
1Since this is primarily a proof of concept, the fine-tuning is not as extensive as in [ 36] due to computational
limits. While [ 36] performs fine-tuning layer-by-layer prior to doing so on the end-to-end objective, experiments
in this section only include the end-to-end fine-tuning. Furthermore, the fine-tuning is performed with a sequence
length of 512, as opposed to 4096 as in [ 36]. As such, the QuIP# numbers reported here are different from [ 36].
8Table 3: Zero-shot perplexities and accuracies for LLaMa-2 7B, with end-to-end fine-tuning of randomized
Hadamard transform parameters. BQ= 2bits throughout.*See Footnote 1.
Method Rank BL(= B R)Avg Bits Wiki2 ↓C4↓Wino ↑RTE ↑PiQA ↑ArcE ↑ArcC ↑
CALDERA 64 16 2.4 6.22 8.23 64.2 63.2 76.1 63.4 34.7
CALDERA 64 4 2.1 6.30 8.32 64.6 65.7 75.4 63.3 35.4
CALDERA 128 4 2.2 6.09 8.06 65.1 61.0 76.5 65.1 35.6
CALDERA 256 4 2.4 5.84 7.75 65.7 60.6 76.5 64.6 35.9
QuIP#*64 16 2.4 6.32 8.31 64.9 66.4 75.0 65.2 34.5
QuIP#*0 — 2 6.58 8.62 64.4 53.4 75.0 64.8 34.0
Table 4: Zero-shot perplexities and accuracies for LLaMa-3 8B, with end-to-end fine-tuning of randomized
Hadamard transform parameters. BQ= 2bits throughout.*See Footnote 1.
Method Rank BL(= B R)Avg Bits Wiki2 ↓C4↓Wino ↑RTE ↑PiQA ↑ArcE ↑ArcC ↑
CALDERA 64 16 2.4 7.63 8.9 70.3 70.8 75.4 72.4 39.0
CALDERA 64 4 2.1 8.06 9.34 69.5 64.3 76.0 71.5 40.0
CALDERA 128 4 2.2 7.76 9.02 69.4 63.9 76.0 73.7 41.8
CALDERA 256 4 2.4 7.34 8.68 70.3 70.4 76.5 73.6 42.3
QuIP#*64 16 2.4 7.92 9.15 68.4 58.1 74.9 72.3 40.4
QuIP#*0 — 2 8.44 9.75 67.5 57.8 72.9 67.6 37.3
5.3 Low Rank Adaptation (LoRA) Fine-tuning Results
In addition to RHT FT as described above, once the Q+LRdecomposition with target rank kis
obtained, and ktakes values 64,128and256, fine-tuning the top r(≤k)singular components on a
specific downstream datasets can recover the performance lost due to quantization. We consider three
such tasks – (i)language modeling on Wikitext (Wiki2), (ii)recognizing textual entailment (RTE),
and(iii)commonsense reasoning (WinoGrande). Throughout all experiments in Table 5, r= 64 is
chosen and those singular components are fine-tuned to 16-bit precision, i.e., BF16 format. The tasks
(ii)and(iii) are sequence classification tasks, and the pre-trained LLaMa model is augmented with a
linear classification head, which is fine-tuned along with the low-rank factors [ 29]. In other words,
the approximation is written as W≈Q+L1R1+L2R2, where L1∈Rn×r,L2∈Rn×(k−r),
R1∈Rr×d,R2∈R(k−r)×d,L= [L1|L2],R⊤= [R⊤
1|R⊤
2]. The value of ris set to 64and
L2,R2are fined-tuned to Lbf16,Rbf16using low-rank adaptation similar to [ 13,16,22]. Doing this
significantly on a small task-specific dataset like WikiText2, RTE, or Winogrande, can noticeably
boost the zero-shot accuracy, as can be seen from Table 5.2
Experimental details can be found in App. E.4. For each dataset, ten checkpoints are saved during
the course of fine-tuning, and the best test performance is reported in Table 5. For datasets where test
labels are not available, evaluation performance is reported instead.
For comparison, results from the LoftQ [ 22] and LQ-LoRA [ 13] papers are also reported, where
available. As these papers were published before the release of LLaMa-3, only LLaMa-2 results are
available.3In each case, CALDERA achieves better performance at a lower bit budget.
5.4 Autoregressive Generation Throughput
The low-rank (LR)component in CALDERA can recover some of the accuracy lost due to the
aggressive 2-bit quantization of Q. However, CALDERA also needs to dequantize and multiply the
low-rank factors, which results in a slight (albeit, acceptable) throughput degradation compared to
QuIP# (shown in Table 6). Nevertheless, CALDERA’s throughput is significantly higher than that of
the unquantized model. This is because compressing weight matrices results in a smaller volume of
data transfer from and to the GPU’s SRAM, speeding up forward passes. It is worthwhile to note that
2There is an element of stochasticity in fine-tuning, so the final accuracies listed in Table 5 for different
CALDERA parameters are not directly indicative of the smaller error (i.e., ∥(Q+LR−W)X⊤∥2
F) of the
initialization. Rather, they show that low-rank adaptation can recover accuracy degradation from quantization.
3Some LLaMa-3 results for LoftQ are present on its GitHub repository, but none for the tasks evaluated here.
9Table 5: CALDERA fine-tuning results for LLaMa-2 7B and LLaMa-3 8B. BL,BRare the bit-budgets of L
andRfor the low-rank initialization. Rank- 64fine-tuned factors are represented in BF16 precision.
LLaMa-2 7B LLaMa-3 8B
Method Rank BQBL(= B R)RHT FT Avg Bits Wiki2 ↓RTE ↑Wino ↑Wiki2 ↓RTE ↑Wino ↑
CALDERA 64 2 16 No 2.4 6.06 82.31 84.06 7.91 84.48 85.56
CALDERA 64 2 16 Yes 2.4 5.89 85.19 85.32 7.88 86.28 88.16
CALDERA 64 2 4 No 2.4 6.01 81.23 84.06 8.33 85.56 88.40
CALDERA 64 2 4 Yes 2.4 5.91 85.56 83.42 7.96 87.00 88.40
CALDERA 128 2 4 No 2.5 5.84 83.75 85.32 7.84 84.84 88.63
CALDERA 128 2 4 Yes 2.5 5.77 84.12 85.00 7.68 86.64 88.00
CALDERA 256 2 4 No 2.7 5.61 83.75 85.4 7.44 86.28 88.08
CALDERA 256 2 4 Yes 2.7 5.55 86.28 84.93 7.44 85.20 89.19
LoftQ 64 2 16 — 2.4 7.85 — — — — —
LoftQ 64 2.5 16 — 2.9 5.78 — — — — —
LQ-LoRA 64 2.75 8 — 2.95 5.67 — 72.4 — — —
Llama-2 70B runs into out-of-memory (OOM) as an A10G GPU only has 24GiB of VRAM, which
is not enough for 70B parameters in FP16 format (which approximately requires 140 GiB).
Table 6: Throughputs for meta-llama/Llama-2-{7,70}b-hf on an NVIDIA
A10G GPU for a batch size and sequence length of 1(BQ= 2for all rows)
Method Rank BL(= B R)Throughput (tok/s)
Uncompressed (7B, FP16) — — 31.75
CALDERA (7B) 64 16 61.68
CALDERA (7B) 64 4 46.29
CALDERA (7B) 128 4 46.19
CALDERA (7B) 256 4 45.89
QuIP# (7B) 0 — 87.74
Uncompressed (70B, FP16) – – OOM
CALDERA (70B) 128 4 5.33
CALDERA (70B) 256 4 4.66
QuIP# (70B) – – 18.18Notably, the throughput is
higher when the LR fac-
tors are in 16-bit compared
to when they are in 4-bit.
This is because CALDERA
used QuIP#’s lattice dequan-
tizers for the low-rank fac-
tors as well, adding to the
compute overhead. More-
over, QuIP# also used fused
kernels, and CALDERA’s
throughput can be improved
by leveraging such optimiza-
tions. Since this work
is primarily motivated with
the goal of closing the
gap with respect to uncom-
pressed models in the 2to
2.5bits per parameter regime, throughput improvement using custom kernels, paged attention, etc., is
left for future work. We discuss the broader impacts of our work along with limitations in App. H.
6 Conclusions
In this work, the problem of obtaining a low-precision and low-rank decomposition of an LLM weight
matrix was considered. A Q+LRdecomposition efficiently captures the high singular components
of the weight matrix with sufficient fidelity, while coarsely compressing the less significant moderate-
to-low singular components. An optimization-theoretically motivated algorithm was proposed to
obtain this decomposition, which iteratively optimized the quantized backbone Qand the low-rank
factors L,R. Additionally, it was shown that LandRcan be efficiently fine-tuned using low-rank
adaptation to boost the zero-shot performance of the quantized model. By utilizing a rank-constrained
regression framework, an upper bound was established on the approximation error of the algorithm,
and it was shown that this upper bound can be significantly smaller than prior bounds in the literature.
Finally, the proposed method was empirically evaluated by compressing the LlaMA family of LLMs
in the challenging sub- 2.5bits per parameter regime. The proposed approach can also be used to
complement existing compression strategies; thereby making it efficient to distribute compressed
LLMs and deploy them on regular consumer hardware, making them more accessible to researchers.
10Acknowledgements
This work was supported in part by the National Science Foundation (NSF) under Grant DMS-
2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army
Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of
Naval Research under Grant N00014-24-1-2164.
References
[1]L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and B. Magnini. The Fifth PASCAL
Recognizing Textual Entailment Challenge, 2009.
[2]Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi. PIQA: Reasoning about Physical Com-
monsense in Natural Language. In Thirty-Fourth AAAI Conference on Artificial Intelligence ,
2020.
[3]J. Chee, Y . Cai, V . Kuleshov, and C. D. Sa. QuIP: 2-Bit Quantization of Large Language Models
With Guarantees. In Thirty-seventh Conference on Neural Information Processing Systems ,
2023.
[4]P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord.
Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge.
arXiv:1803.05457v1 , 2018.
[5]T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. QLoRA: Efficient Finetuning of
Quantized LLMs. In Thirty-seventh Conference on Neural Information Processing Systems ,
2023. URL https://openreview.net/forum?id=OUIFPHEgJU .
[6]J. Dodge, M. Sap, A. Marasovi ´c, W. Agnew, G. Ilharco, D. Groeneveld, M. Mitchell, and
M. Gardner. Documenting Large Webtext Corpora: A Case Study on the Colossal Clean
Crawled Corpus, 2021.
[7]V . Egiazarian, A. Panferov, D. Kuznedelev, E. Frantar, A. Babenko, and D. Alistarh. Extreme
Compression of Large Language Models via Additive Quantization, 2024. URL https:
//arxiv.org/abs/2401.06118 .
[8]E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. OPTQ: Accurate Quantization for
Generative Pre-trained Transformers. In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=tcbBPnfwxS .
[9]L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu,
A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds,
H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A
framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/
records/10256836 .
[10] G. H. Golub and C. F. van Loan. Matrix Computations . JHU Press, fourth edition,
2013. ISBN 1421407949 9781421407944. URL http://www.cs.cornell.edu/cv/GVL4/
golubandvanloan.htm .
[11] R. Gray and T. Stockham. Dithered quantizers. IEEE Transactions on Information Theory , 39
(3):805–812, 1993. doi: 10.1109/18.256489.
[12] S. Gugger, L. Debut, T. Wolf, P. Schmid, Z. Mueller, S. Mangrulkar, M. Sun, and B. Bossan.
Accelerate: Training and inference at scale made simple, efficient and adaptable. https:
//github.com/huggingface/accelerate , 2022.
[13] H. Guo, P. Greengard, E. P. Xing, and Y . Kim. LQ-LoRA: Low-rank Plus Quantized Matrix
Decomposition for Efficient Language Model Finetuning. arxiv:2311.12023, 2023. URL
https://arxiv.org/abs/2311.12023 .
11[14] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Review , 53(2):217–288,
2011. doi: 10.1137/090771806. URL https://doi.org/10.1137/090771806 .
[15] Y .-C. Hsu, T. Hua, S. Chang, Q. Lou, Y . Shen, and H. Jin. Language model compression with
weighted low-rank factorization. In International Conference on Learning Representations ,
2022. URL https://openreview.net/forum?id=uPv9Y3gmAI5 .
[16] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. LoRA:
Low-Rank Adaptation of Large Language Models. In International Conference on Learning
Representations , 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9 .
[17] W. Huang, X. Ma, H. Qin, X. Zheng, C. Lv, H. Chen, J. Luo, X. Qi, X. Liu, and M. Magno.
How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study, 2024.
[18] A. Kaushal, T. Vaidhya, and I. Rish. LORD: Low Rank Decomposition Of Monolingual Code
LLMs For One-Shot Compression, 2023.
[19] S. Keisuke, L. B. Ronan, B. Chandra, and C. Yejin. WinoGrande: An Adversarial Winograd
Schema Challenge at Scale, 2019.
[20] A. Krishnamoorthy and D. Menon. Matrix inversion using cholesky decomposition. In 2013
Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA) , pages
70–72, 2013.
[21] H. J. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In Proceedings
of the Thirteenth International Conference on Principles of Knowledge Representation and
Reasoning , KR’12, page 552–561. AAAI Press, 2012. ISBN 9781577355601.
[22] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. LoftQ: LoRA-
Fine-Tuning-Aware Quantization for Large Language Models. arxiv:2310.08659, 2023. URL
https://arxiv.org/abs/2310.08659 .
[23] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen.
Dora: Weight-decomposed low-rank adaptation, 2024.
[24] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue, and F. Wei.
The era of 1-bit llms: All large language models are in 1.58 bits, 2024.
[25] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer Sentinel Mixture Models, 2016.
[26] Meta AI. Introducing Meta Llama 3: The most capable openly available LLM to date. https:
//ai.meta.com/blog/meta-llama-3/ , 2024. Accessed: 2024-05-07.
[27] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort. Up or Down? Adaptive
Rounding for Post-Training Quantization. In Proceedings of the 37th International Conference
on Machine Learning , volume 119, pages 7197–7206, 2020.
[28] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He. Zero: Memory optimizations toward training
trillion parameter models, 2020.
[29] S. Raschka. Building a gpt-style llm classifier from scratch, 2024. URL https://magazine.
sebastianraschka.com/p/building-a-gpt-style-llm-classifier . Blog post.
[30] J. Rasley, S. Rajbhandari, O. Ruwase, and Y . He. Deepspeed: System optimizations enable
training deep learning models with over 100 billion parameters. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , KDD
’20, page 3505–3506, New York, NY , USA, 2020. Association for Computing Machinery.
ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/
3394486.3406703 .
[31] R. Saha, V . Srivastava, and M. Pilanci. Matrix Compression via Randomized Low Rank and
Low Precision Factorization. In Thirty-seventh Conference on Neural Information Processing
Systems , 2023. URL https://openreview.net/forum?id=rxsCTtkqA9 .
12[32] L. Schuchman. Dither Signals and Their Effect on Quantization Noise. IEEE Transactions on
Communication Technology , 12(4):162–165, 1964. doi: 10.1109/TCOM.1964.1088973.
[33] P. Sharma, J. T. Ash, and D. Misra. The Truth is in There: Improving Reasoning in Language
Models with Layer-Selective Rank Reduction, 2023.
[34] Together Computer. Redpajama: an open dataset for training large language models, October
2023. URL https://github.com/togethercomputer/RedPajama-Data .
[35] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra,
P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,
J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn, S. Hosseini,
R. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.
Lachaux, T. Lavril, J. Lee, D. Liskovich, Y . Lu, Y . Mao, X. Martinet, T. Mihaylov, P. Mishra,
I. Molybog, Y . Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.
Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,
I. Zarov, Y . Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and
T. Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023.
[36] A. Tseng, J. Chee, Q. Sun, V . Kuleshov, and C. D. Sa. QuIP#: Even Better LLM Quantization
with Hadamard Incoherence and Lattice Codebooks, 2024.
[37] M. Udell and A. Townsend. Why are big data matrices approximately low rank? SIAM Journal
on Mathematics of Data Science , 1(1):144–160, 2019. doi: 10.1137/18M1183480. URL
https://doi.org/10.1137/18M1183480 .
[38] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data
Science . Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University
Press, 2018. doi: 10.1017/9781108231596.
[39] M. Viazovska. The sphere packing problem in dimension 8.Annals of Mathematics , 185(3),
May 2017. ISSN 0003-486X. doi: 10.4007/annals.2017.185.3.7. URL http://dx.doi.org/
10.4007/annals.2017.185.3.7 .
[40] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A Multi-
Task Benchmark and Analysis Platform for Natural Language Understanding, 2019. In the
Proceedings of ICLR.
[41] S. Xiang, Y . Zhu, X. Shen, and J. Ye. Optimal exact least squares rank minimization. In
Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining , KDD ’12, page 480–488, New York, NY , USA, 2012. Association for
Computing Machinery. ISBN 9781450314626. doi: 10.1145/2339530.2339609. URL https:
//doi.org/10.1145/2339530.2339609 .
[42] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. SmoothQuant: Accurate and
Efficient Post-Training Quantization for Large Language Models. In Proceedings of the 40th
International Conference on Machine Learning , 2023.
[43] Z. Yao, X. Wu, C. Li, S. Youn, and Y . He. ZeroQuant-V2: Exploring Post-training Quantization
in LLMs from Comprehensive Study to Low Rank Compensation, 2023.
[44] Z. Yuan, Y . Shang, Y . Song, Q. Wu, Y . Yan, and G. Sun. ASVD: Activation-aware Singular
Value Decomposition for Compressing Large Language Models, 2023.
[45] C. Zhang, J. Cheng, G. A. Constantinides, and Y . Zhao. LQER: Low-Rank Quantization Error
Reconstruction for LLMs, 2024.
13Contents
1 Introduction 1
1.1 Significance and Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2 Problem Formulation 3
3Proposed Algorithm: Calibration-Aware Low-Precision Decomposition with Low
Rank Adaptation 3
4 Approximation Error Analysis 4
4.1 Analysis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
5 Numerical Simulations 7
5.1 Zero-shot Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
5.2 Fine-tuning of Randomized Hadamard Transform (RHT) Parameters . . . . . . . . 8
5.3 Low Rank Adaptation (LoRA) Fine-tuning Results . . . . . . . . . . . . . . . . . 9
5.4 Autoregressive Generation Throughput . . . . . . . . . . . . . . . . . . . . . . . . 9
6 Conclusions 10
A Notations 15
B Rank-constrained Regression 15
CDerivations for Calibration-Aware Low-Precision and Low-Rank Decomposition:
CALDERA 17
C.1 Update Equations for Low-Rank Factors in LPLRF ACTORIZE submodule . . . . . 17
C.2 Dynamic Ranges of Quantizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C.3 Proof of Lemma C.3: Approximation Error Upper Bound for LPLRF ACTORIZE . . 19
C.4 Proof of Thm. 4.1: Approximation Error Upper Bound for CALDERA . . . . . . . 22
C.5 A simplification for positive definite Hessians . . . . . . . . . . . . . . . . . . . . 24
D Computational Complexity of CALDERA 25
E Additional Experimental Details 25
E.1 System Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E.2 Parameters of CALDERA Quantization . . . . . . . . . . . . . . . . . . . . . . . 26
E.3 Details about Goodness-of-Fit Metrics . . . . . . . . . . . . . . . . . . . . . . . . 26
E.4 Fine-tuning Parameter Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
E.5 Computation of Average Bit Budget Per-Parameter . . . . . . . . . . . . . . . . . 27
E.6 Additional Notes on Numerical Simulations . . . . . . . . . . . . . . . . . . . . . 27
F Additional Numerical Simulations 27
F.1 Ablation Study of CALDERA Parameters with respect to Frobenius Norm Error . . 27
F.2 Experiments on Mistral-7B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
G Auxiliary Results 29
G.1 Useful Results from Linear Algebra and Probability . . . . . . . . . . . . . . . . . 29
G.2 Uniformly dithered scalar quantizer . . . . . . . . . . . . . . . . . . . . . . . . . 30
H Limitations and Further Discussions 31
14A Notations
This section begins by outlining key notations used in both linear algebra and probability theory.
Boldface uppercase and lowercase letters, such as Aanda, represent matrices and vectors respectively.
Idenotes the identity matrix, and its dimension is assumed to be imminent from the context. The first
kcolumns of the identity matrix IasIk, and let Ikbe the submatrix formed by the last kcolumns.
The singular values of Aare denoted by σmax(A) =σ1≥σ2≥. . .≥σr=σmin(A), where
r= rank( A). Similarly, the eigenvalues are denoted as λ1(A), . . . , λ r(A). The max-norm of Ais
defined as ∥A∥max= max i,j|Aij|, the spectral norm of Ais defined as ∥A∥2= sup∥x∥=1∥Ax∥=
σmax(A), and the Frobenius norm is ∥A∥F=P
i,jA2
ij1/2
=Tr 
A⊤A
=P
k∈[r]σ2
k1/2
.
For any matrix X, its Moore-Penrose pseudo-inverse is denoted by X†. The notations ≈,≲and≳
are used to denote approximate equality and inequalities that hold asymptotically in the limit when
dimensions grow to infinity. In other words, for any two functions A(n)andB(n),
A(n)≲B(n)iff lim
n→∞A(n)≤lim
n→∞B(n).
Notations ≈and≳are defined analogously. Wherever relevant, dimension-dependent terms are
highlighted blue.
Table 7: Notations used in this paper
Notation Description Remarks
W LLM weight matrix of any layer W∈Rn×d
Q,L,R Backbone, left and right low rank
factorsQ∈Rn×d,L∈Rn×k, andR∈Rk×d
are represented using BQ,BL, and BRbits,
respectively. Approximation: W≈Q+
LR
X Calibration data matrix X∈Rm×d. Input activation for each layer.
Computed once offline for each LLM.
H H≜1
mX⊤X (Scaled) Hessian of least-squares objectives
(1) and (3). Computed offline once.
k Target rank for low-rank factors k= 64,128,256in our expts.
QQ,BQ,R Quantizer, bit-budget and dynamic
range for the backboneOperates on matrices in Rn×d.
QL,BL,RL Quantizer, bit-budget and dynamic
range for the left low-rank factorOperates on matrices in Rn×k.
QR,BR,RR Quantizer, bit-budget and dynamic
range for the right low-rank factorOperates on matrices in Rk×d.
M Strictly upper triangular matrix from
the LDL decomposition of mHmH= (M+I)D(M+I)⊤
D Diagonal matrix obtained from the
LDL decomposition of mHmH= (M+I)D(M+I)⊤
η Quantization error of LDLQ , given
byη≜QQ(Q+ (W−Q)M)−
(Q+ (W−Q)M)η∈Rn×dis assumed to consist of i.i.d.
entries for analytical tractability.
EL,ER Quantization error matrices from
quantizing left and right factorsEL∈Rn×kandER∈Rk×dconsists of
zero-mean random variables with bounded
variance.
B Rank-constrained Regression
Recall that the submatrix formed by the first mcolumns of the identity matrix Iis denoted as Im,
and let Imbe the submatrix formed by the last mcolumns. The dimension of Iis inferred depending
on context. Consider the following:
min
rank(Z)≤k∥XZ−Y∥2
F. (7)
15Although this problem is non-convex, it can be solved to global optimality via two SVDs. The
following lemma characterizes the solution the rank-constrained regression in (5).
Lemma B.1. (Global optimality of rank-constrained regression) Suppose Y∈Rm×nis given,
and suppose X∈Rm×dis full rank, i.e., rank(X) = min {m, d}, with SVD, X=UeΣV⊤.
Furthermore, let `U`Σ`V⊤denote the full SVD of U⊤Yifm≤d, or the full SVD of (UId)⊤Yif
m > d . Then, for k≤m, the solution of (5)is given by
Z∗:= arg min
rank(Z)≤k∥XZ−Y∥2
F=


VImΣ−1`UIk
I⊤
k`Σ`V⊤
ifm≤d,
VΣ−1`UIk
I⊤
k`ΣV⊤
otherwise.(8)
Here,Σis a diagonal matrix of the non-zero singular values of X, defined as Σ:=eΣIm∈Rm×m
when m≤d, andΣ:=I⊤
deΣ∈Rd×dwhen d < m . Additionally, the optimal value is
min
rank(Z)≤k∥XZ−Y∥2
F
=∥XZ∗−Y∥2
F=( Pm
i=k+1σ2
i(Y), ifm≤d,Pm
i=k+1σ2
i 
(UId)⊤Y
+(UIm−d)⊤Y2
F,otherwise.(9)
Proof. Casem≤d:Since the full SVD of X∈Rm×disX=UeΣV⊤, where U∈Rm×m,
eΣ∈Rm×d, and V∈Rd×d, the last (d−m)columns of eΣwill be zero, i.e., eΣIm−d=0.
LetZ′:=V⊤Z∈Rd×nbe the transformed optimization variable. Since Vis a unitary matrix,
rank(Z)≤kif and only if rank(Z′)≤k. Splitting Z′∈Rd×nintoZ′′:=I⊤
mZ′∈Rm×nand
Z′′:=I⊤
d−mZ′∈R(d−m)×n, it can be seen that eΣZ′=eΣIdZ′′+eΣIm−dZ′′=ΣZ′′, where
Σ:=eΣIm∈Rm×mis a diagonal matrix comprised of the non-zero singular values of X. Then,
min
rank(Z)≤k∥XZ−Y∥2
F≡ min
rank(Z′)≤keΣZ′−U⊤Y2
F≡ min
rank(Z′′)≤kΣZ′′−U⊤Y2
F.(10)
Note that the objective function value is independent of Z′′, asZ′′lies in the null space of X⊤, and
rank(Z′′)≤kfollows from the fact that rank of a submatrix cannot exceed the full matrix. Since X
is full rank, Σis invertible, and the minimization in (10) is equivalent to
min
rank(eZ)≤keZ−U⊤Y2
F=mX
i=k+1σ2
i(Y), (11)
whereeZ:=ΣZ′′∈Rm×n. The equality in (11) follows as a consequence of Eckart-Young-Mirsky
theorem (lemma G.1), which states that ∥eZ−U⊤Y∥2
Fis minimized by taking the best rank- k
approximation of U⊤Y, and the fact that the singular values of U⊤YandYare the same as Uis
unitary. Moreover, as the SVD of U⊤Yis`U`Σ`V⊤,
eZ∗:= arg min
rank(eZ)≤keZ−U⊤Y2
F=
`UIk
I⊤
k`Σ`V⊤
and, Z′′
∗:=Σ−1eZ∗=
Σ−1`UIk
I⊤
k`Σ`V⊤
. (12)
In other words, if Z′
∗denotes the solution of the middle optimization problem in (10),I⊤
mZ∗=Z′′
∗.
Furthermore, note that setting I⊤
d−meZ∗=0ensures rank(Z′
∗) = rank( Z′′
∗)≤k, while keeping the
objective value in (10) unchanged. Hence, an optimal solution is given by
Z′
∗="
Σ−1`UIk
I⊤
k`Σ`V⊤
0(d−m)×n#
and, Z∗=VZ′
∗=
VImΣ−1`UIk
I⊤
k`Σ`V⊤
.(13)
Casem>d:Recalling that the full SVD of X∈Rm×disX=UeΣV⊤, where U∈Rm×m,
eΣ∈Rm×d, andV∈Rd×d, in this case, the last m−drows of eΣwill be zero, i.e., I⊤
m−deΣ=0.
Denote Z′:=V⊤Z. This time, as Σ:=I⊤
deΣ,
min
rank(Z)≤k∥XZ−Y∥2
F≡ min
rank(Z′)≤keΣZ′−U⊤Y2
F
16≡ min
rank(Z′)≤kΣZ′−I⊤
dU⊤Y2
F+I⊤
m−dU⊤Y2
F
≡ min
rank(Z′′)≤kZ′′−I⊤
dU⊤Y2
F+I⊤
m−dU⊤Y2
F
(i)=mX
i=k+1σ2
i 
(UId)⊤Y
+(UIm−d)⊤Y2
F, (14)
where Z′′:=ΣZ′=ΣV⊤Z. Note that the term(UIm−d)⊤Y2
Fis the irreducible error, and (i)is,
once again, a consequence of the fact thatZ′−I⊤
dU⊤Y2
Fis minimized by taking the best rank- k
approximation of I⊤
dU⊤Y. Moreover, since the SVD of I⊤
dU⊤Yis`U`Σ`V⊤,
Z′′
∗:= arg min
rank(Z′′)≤kZ′′−I⊤
dU⊤Y2
F=
`UIk
I⊤
k`ΣV⊤
,
and Z∗=VΣ−1Z′′
∗=
VΣ−1`UIk
I⊤
k`ΣV⊤
. (15)
Remark : It is not necessary for Xto be full rank. An equivalent result can be derived with Σ−1
replaced by Σ†.
Computational complexity of rank-constrained regression : Arriving at the globally optimal
solution in lemma B.1 requires computing two SVDs, namely X∈Rm×d, which entails a complexity
ofO(dm2), andU⊤Y∈Rm×n, with a complexity of O(nm2). Hence, the total computational
complexity is O(m2(n+d)).
C Derivations for Calibration-Aware Low-Precision and Low-Rank
Decomposition: CALDERA
C.1 Update Equations for Low-Rank Factors in LPLRF ACTORIZE submodule
As discussed in §3, the left and right low-rank factors in the LPLRF ACTORIZE sub-module (Alg.
2) are found by solving least squares minimization problem. The closed form expressions can be
obtained by solving the normal equations directly. In what follows, the same expressions are also
derived explicitly via the singular value decomposition, as it gives an expression for the error (for
example, refer to (17)) – which consists of an additive and irreducible error term.
Updating Lwith fixed R: For a fixed R∈Rk×d, the left low rank factor is computed in lines 5and
9of Alg. 2 by solving
min
Z∈Rk×nXR⊤Z−Y2
F,where Y:=XA⊤∈Rm×n. (16)
LetXR⊤=UXReΣXRV⊤
XRbe the full SVD of XR⊤, where UXR∈Rm×m,eΣXR∈Rm×k, and
VXR∈Rk×k. Recall from App. B that Idenotes the submatrix formed by the first mcolumns of the
identity matrix Im, andImis the submatrix formed by the last mcolumns. Denoting ΣXR:=I⊤
keΣXR
andZ′:=V⊤
XRZ, since I⊤
m−keΣXR=0, it can be seen that
min
Z∈Rk×nUXReΣXRV⊤
XRZ−Y2
F= min
X∈Rk×neΣXRV⊤
XRZ−U⊤
XRY2
F
=(UXRIm−k)⊤Y2
F+ min
Z′∈Rk×nΣXRZ′−(UXRIk)⊤Y2
F.(17)
The last term of (17) is minimized by setting Z′←Z′
∗:=Σ−1
XR(UXRIk)⊤Y. Since Z′
∗=V⊤
XRZ∗=
V⊤
XR`L⊤, this yields the left low rank factor to be
`L⊤=VXRΣ−1
XR(UXRIk)⊤XA⊤=⇒`L= (AX⊤)(RX⊤)†(i)=AHR⊤(RHR⊤)−1,(18)
where (i)follows from the explicit expression for the pseudoinverse of the wide matrix RX⊤∈Rk×d.
17Updating Rwith fixed L: For a fixed L∈Rn×k, the right low-rank factor is computed in lines 4
and8of Alg. 2 by solving
min
Z∈Rd×k∥XZL⊤−Y∥2
F,where Y:=XA⊤∈Rm×n. (19)
LetX=UXeΣXV⊤
Xbe the full SVD of X∈Rm×d, where UX∈Rm×m,eΣX∈Rm×d, and
VX∈Rd×d. Then, denoting Z′:=ZL⊤, the minimization (19) is equivalent to
min
Z′∈Rd×n∥eΣXV⊤
XZ′−U⊤
XY∥2
F≡min
Z′′∈Rd×n∥eΣXZ′′−U⊤
XY∥2
F,where Z′′:=V⊤
XZ′. (20)
Note that (20) is an undetermined linear system with multiple solutions. In particular, since eΣX∈
Rm×d, the last (d−m)columns of eΣXconsist of zeros, i.e., eΣXId−m=0. Therefore, a solution of
this system is given by
Z′′
∗=`Σ†U⊤
XY or,V⊤
XZ′
∗=eΣ†
XU⊤
XY or,Z′
∗=VXeΣ†
XU⊤
XY
or,Z∗=VXeΣ†
XU⊤
XY(L⊤)†. (21)
This implies `R=L†AX⊤UX(eΣ†
X)⊤VX.
The proof is completed by noting that X⊤UX(eΣ†
X)⊤VX=HH†, because,
HH†=X⊤X 
X⊤X†=X⊤UXeΣXV⊤
X
VXeΣ⊤
XU⊤
XUXeΣXV⊤
X†
=X⊤UXeΣXV⊤
X
VXeΣ⊤
XeΣXV⊤
X†
=X⊤UXeΣXV⊤
XVX
eΣ⊤
XeΣX†
V⊤
X
=X⊤UXeΣX
eΣ⊤
XeΣX†
V⊤
X=UX
eΣ†
X⊤
V⊤
X. (22)
C.2 Dynamic Ranges of Quantizers
Lemmas C.1 and C.2, stated in this section provide sufficient conditions to ensure that the quantizers
QLandQR, used to quantize the left and right low-rank factors in the LPLRF ACTORIZE submodule,
remain unsaturated.
Lemma C.1. (Dynamic range of right quantizer) Given matrices A∈Rn×dandX∈Rm×d, let
σmaxdenote the maximum singular value of XA⊤. Then, the quantizer QRremains unsaturated if
the dynamic range is chosen to be RR=σmax.
Proof. Note that the input to the right quantizer QR, i.e., I⊤
k`Σ`Vsatisfies ∥I⊤
k`Σ`V⊤∥max≤
∥I⊤
k`Σ`V⊤∥2=∥I⊤
k`Σ∥2=∥`Σ∥2=∥U⊤XA⊤∥2=∥XA⊤∥2. This completes the proof.
Since the input to QLis dependent on the output of QR, the following lemma C.2 provides an upper
bound on the input to QL, provided that QRwas unsaturated.
Lemma C.2. (Dynamic range of left quantizer) Given matrices A∈Rn×dandX∈Rm×n, let
λmindenote the smallest eigenvalue of H=1
mX⊤X, and let σmaxandσkdenote the largest and
thekthsingular values of XA⊤, respectively. For some small ϵ >0and an absolute constant C,
suppose the number of bits for the right quantizer QRsatisfies
BR≥log2 
4Cσmax
σklog 2 
√
d+√
k+s
log8∥XA⊤∥2
F
ϵ!!
. (23)
Then, if the dynamic range of QLis chosen to be
RL=2σmax
σk√mλmin, (24)
thenQLremains unsaturated with probability exceeding 1−0.25ϵXA⊤−2
F.
18Proof. In Line 5of Alg. 2, the input to QLis`L0= 
WX⊤ 
R0X⊤†. In the rest of the proof, the
subscript 0in`L0and`R0is dropped for brevity. Recall the notation for the full SVD of XR⊤, i.e.,
XR⊤=UXReΣXRVXR, where UXR∈Rm×m,eΣXR∈Rm×k, andVXR∈Rk×k. From Eq. (18),
`Lcan be expressed as `L=VXRΣ†
XR(UXRIk)⊤XA⊤. Consequently, ∥`L∥maxis upper bounded as
∥`L∥max≤ ∥`L∥2(i)=Σ†
XR(UXRIk)⊤XA⊤
2(ii)
≤Σ†
XR
2(UXRIk)⊤XA⊤
2
(iii)
≤Σ†
XR
2XA⊤
2. (25)
Here, (i)holds because VXRis a unitary matrix, (ii)follows from submultiplicativity of spectral
norm, and (iii)follows from the fact that (UXRIk) (UXRIk)⊤≼Iand lemma G.4. Furthermore,
Σ†
XR
2=σ−1
min(ΣXR)(i)
≤
(mλmin)1/2σmin(R)−1
(ii)
≤(mλmin)−1/2
σmin
I⊤
k`Σ`V⊤
− ∥ER∥2−1
, (26)
where σmin(ΣXR)is the smallest non-zero singular value of XR⊤,(i)follows from lemma G.4
and(ii)follows from lemma G.5. Here, ER∈Rk×dis the quantization error from quantizing right
low-rank factor, and consist of unbiased random variables with bounded variance, as described in
lemma G.6. Since `Σcontains the singular values of U⊤XA⊤,
σmin
I⊤
k`Σ`V⊤(i)=σmin(I⊤
k`Σ) =σk(`Σ) =σk 
U⊤XA⊤(ii)=σk 
XA⊤
(27)
where (i)and(ii)follow because `VandUare unitary matrices. Substituting (27) in(26) and(26) in
(25),
∥`L∥max≤Σ†
XR
2XA⊤
2≤(mλmin)−1/2 
σk 
XA⊤
− ∥ER∥2−1XA⊤
2(28)
Lemma C.1 suggests choosing the dynamic range of QRto be∥XA⊤∥2to ensure that QRremains
unsaturated. As a result,
∥ER∥max≤∆R:=2∥XA⊤∥2
2BR−1=⇒ ∥ER∥ψ2≤2∥XA⊤∥2
(2BR−1) log 2, (29)
where ∥·∥ψ2denotes the subgaussian norm (refer to lemma G.2). Subsequently, lemma G.2 yields
that for some absolute constant C,
∥ER∥2≤2C∥XA⊤∥2
(2BR−1) log 2√
d+√
k+t
with probability exceeding 1−2e−t2. (30)
Setting t=r
log
8∥XA⊤∥2
F
ϵ
yields
∥ER∥2≤2C∥XA⊤∥2
(2BR−1) log 2 
√
d+√
k+s
log8∥XA⊤∥2
F
ϵ!
, (31)
with probability exceeding 1−ϵ
4∥XA⊤∥2
F. Note that if the bit budget BRis chosen to satisfy
BR≥log2 
4Cσ1
σklog 2 
√
d+√
k+s
log8∥XA⊤∥2
F
ϵ!!
, (32)
then∥ER∥2≤σk/2. Consequently, from (28),
∥`L∥max≤2σ1
σk√mλmin. (33)
This completes the proof.
C.3 Proof of Lemma C.3: Approximation Error Upper Bound for LPLRF ACTORIZE
Lemma C.3. (Approximation error of LPLRF ACTORIZE )Given A∈Rn×dandX∈Rm×dwith
m≤d, letλmaxandλmindenote the maximum and minimum eigenvalues of H=1
mX⊤X, and let
19σ1≥. . .≥σkdenote the singular values of XA⊤. Suppose the target rank ksatisfies
λ1/2
min
4mσ1λ3/2
maxX
i>kσ2
i≤k≤m,
the dynamic ranges of quantizers QLandQRare respectively set as
RL=2σ1
σk√mλminand RR=σ1,
and for some absolute constant Cand arbitrarily small ϵthat satisfies 0< ϵ≤4mkλ2
maxλ−1
minσ1,
suppose the bit-budgets of QLandQRsatisfy
BL≥log2 
4σ2
1
σkr
nk
ϵλmax
λmin+ 1!
,
and BR≥max{B1,B2},where B1:= log2 
2σ1r
kd
ϵλmax
λmin+ 1!
,
and B2:= log2 
4Cσ1
σklog 2 
√
d+√
k+s
log8P
iσ2
i
ϵ!!
.
Then, the factors L,Rreturned by Alg. 1 satisfy E(LR−A)X⊤2
F≤P
i>kσ2
i+ϵ, where the
expectation is over the stochasticity of quantizers QLandQR.
Proof. Firstly, note that upper bounding the error E∥(A−L0R0)X⊤∥2
Fsuffices, since the lines 7
to13in Alg. 1 refine the estimates of LandRand can only yield a smaller Frobenius norm error.
Consider the quantized low rank factorization A≈QL
`L0
QR
I⊤
k`Σ`V⊤
. Let
EL:= Q L
`L0
−`L0,andER:= Q R
I⊤
k`Σ`V⊤
−I⊤
k`Σ`V⊤(34)
denote the quantization error matrices. Furthermore, let
ξ∗≜min
rank(Z)≤k∥(A−Z)X⊤∥2
F (35)
denote the optimal value of the unquantized rank-constrained regression problem. Since the dynamic
ranges of quantizers QRandQLare chosen according to lemmas C.1 and C.2 respectively, the
entries of ELandERare unbiased random variables with bounded variance as in lemma G.6. Let
∆R:= 2R R/(2BR−1), and∆L:= 2R L/(2BL−1)denote the quantization resolutions. Conditioned
on the event that the left quantizer QLis unsaturated, the error matrix satisfies EEL=0. This yields,
E
QL
`L0
QR
I⊤
k`Σ`V⊤
−A
X⊤2
F
=E
`L0+EL
QR
I⊤
k`Σ`V⊤
−A
X⊤2
F
(i)=E
`L0QR
I⊤
k`Σ`V⊤
−A
X⊤2
F| {z }
T1+EELQR
I⊤
k`Σ`V⊤
X⊤2
F| {z }
T2, (36)
where (i) follows from the fact that the error matrix is unbiased.
Since `L0= arg minZ∈Rk×d(ZR0−A)X⊤2
F, term T1can be upper bounded as
E
`L0QR
I⊤
k`Σ`V⊤
−A
X⊤2
F
(i)
≤E
VΣ−1`UIkQR
I⊤
k`Σ`V⊤
−A
X⊤2
F
=E
VΣ−1`UIk
I⊤
k`Σ`V⊤+ER
−A
X⊤2
F
=
VΣ−1`UIkI⊤
k`Σ`V⊤−A
X⊤2
F| {z }
=ξ∗+EVΣ−1`UIkERX⊤2
F, (37)
20where inequality (i)is obtained by replacing the minimizer `L0with a different, but appropriately
chosen matrix. Here, Σis the diagonal matrix containing the non-zero singular values of X.
Since κ2:=λmax/λmin, the second term in (37) is
EVΣ−1`UIkERX⊤2
F(i)=EΣ−1`UIkERX⊤2
F
=Eh
Tr
Σ−1`UIkERX⊤XE⊤
RI⊤
k`U⊤Σ−1i
(ii)
≤mλmaxEh
Tr
Σ−1`UIkERE⊤
RI⊤
k`U⊤Σ−1i
(iii)=mλmaxEh
Tr
Σ−2`UIkERE⊤
RI⊤
k`U⊤i
(iv)
≤κ2E
Tr 
IkERE⊤
RI⊤
k
(v)=κ2E
Tr 
ERE⊤
R
=κ2E∥ER∥2
F≤kd∆2
R
4κ2. (38)
Here, (i)follows since Vis a unitary matrix, (ii)follows from lemma G.4, (iii)follows from the
cyclic property of trace, (iv)follows as `Uis unitary, and (v)follows since I⊤
kIk=Iask≤m.
Moreover, since EER=0, term T2can be upper bounded as
EELQR
I⊤
k`Σ`V⊤
X⊤2
F=EEL
I⊤
k`Σ`V⊤+ER
X⊤2
F
=EELI⊤
k`Σ`V⊤X⊤2
F+EELERX⊤2
F(39)
The first term in (39) is
EELI⊤
k`Σ`V⊤X⊤2
F=Tr
ELI⊤
k`Σ`V⊤X⊤X`V`Σ⊤IkE⊤
L
(i)
≤mλmaxTr
ELI⊤
k`Σ`Σ⊤IkE⊤
L
(ii)
≤mλmaxσ2
1Tr 
ELE⊤
L
≤nk∆2
L
4mλmaxσ2
1, (40)
where (i)and(ii)follow from lemma G.4 and the fact that `Vis a unitary matrix. Similarly, the
second term of (39) is
EELERX⊤2
F=E
Tr 
ELERX⊤XE⊤
RE⊤
L
≤mλmaxE
Tr 
ERE⊤
RE⊤
LEL
=mλmaxE"kX
i=1 
ERE⊤
R
ii 
E
ELE⊤
L
ii#
(i)
≤mλmaxn∆2
L
4E"kX
i=1 
ERE⊤
R
ii#
=mλmaxn∆2
L
4E∥ER∥2
F≤kn∆2
L
4d∆2
R
4mλmax. (41)
Here, (i)follows because E
E⊤
LEL
is a diagonal matrix since
 
E
E⊤
LEL
ij=nX
l=1E[EliElj] =(
nVar 
E2
li
≤n∆2
L
4fori=j,Pn
l=1E[Eli]E[Elj] = 0 fori̸=j.(42)
As a consequence of (37) to (41),
E
QL
`L0
QR
I⊤
k`Σ`V⊤
−A
X⊤2
F
≤ξ∗+kd∆2
R
4κ2+nk∆2
L
4mλmaxσ2
1+kn∆2
L
4d∆2
R
4mλmax. (43)
21Since RR=σ1, the second term of (43) does not exceed 0.25ϵif
BR≥log2 
2σ1κr
kd
ϵ+ 1!
. (44)
This needs to be considered in conjunction with the lower bound on BRin lemma C.1, which yields
the maximum of two quantities in the theorem statement. Since RL=2σ1
σk√mλmin, the third term of
(43) does not exceed 0.25ϵif
BL≥log2 
4σ2
1
σkr
nk
ϵλmax
λmin+ 1!
, (45)
provided QLstays unsaturated. Furthermore, if ϵ≤4kmλ2
maxλ−1
minσ1, choosing BRandBLas
above ensures that the third term of (43) is also upper bounded by 0.25ϵ.
The approximation error upper bound in (43) holds conditioned on the event that QLwas unsaturated,
which according to lemma C.2, is ensured with probability exceeding 1−0.25ϵXA⊤−2
F. For
analysis purposes, it is assumed that when QLgets saturated, Alg. 1 returns L=0andR=
0, as a result of which, the approximation error is upper bounded by ∥XA⊤∥2
F.4Then, since
Pr (Q Lis unsat. )≥1−0.25ϵ∥XA⊤∥2
F, using Cauchy-Schwarz inequality for expectations,
E(LR−A)X⊤2
F=Eh(LR−A)X⊤2
FQLis unsat.i
+Eh(0−A)X⊤2
FQLis sat.i
≤ξ∗+3ϵ
4+ Pr (Q Lis sat. )X
iσ2
i≤ξ∗+ϵ. (46)
This completes the proof.
C.4 Proof of Thm. 4.1: Approximation Error Upper Bound for CALDERA
Theorem C.4. Approximation error of CALDERA (Formal) Given W∈Rn×dandX∈Rm×d
withm≤d, letDbe obtained from the LDL decomposition X⊤X=mH= (M+I)D(M+I)⊤,
andλmax,λmindenote the max and min eigenvalues of H. Additionally, let Q≜LDLQ (W,QQ),
where QQhas dynamic range Rand bit-budget BQ, the quantization error be η≜QQ(Q+ (W−
Q)M)−(Q+ (W−Q)M), and σ1≥. . .≥σk. . .be the singular values of X(W−Q)⊤. If the
target rank ksatisfies
λ1/2
min
4mσ1λ3/2
maxX
i>kσ2
i≤k≤m,
the dynamic ranges of QLandQRare set as
RL=2σ1
σk√mλminand, RR=σ1,
and for some absolute constant Cand arbitrarily small ϵthat satisfies 0< ϵ≤4mkλ2
maxλ−1
minσ1,
suppose the bit-budgets of QLandQRare set so that they satisfy
BL≥log2 
4σ2
1
σkr
nk
ϵλmax
λmin+ 1!
,
and BR≥max{B1,B2},where B1:= log2 
2σ1r
kd
ϵλmax
λmin+ 1!
,
and B2:= log2 
4Cσ1
σklog 2 
√
d+√
k+s
log8P
iσ2
i
ϵ!!
,
thenQ,LandRreturned by CALDERA (Alg. 1) satisfy
E(Q+LR−W)X⊤2
F≤mX
i>kEλi(ηDη⊤) +ϵ≲4mdλ maxR2
π(2BQ−1)2
1−k
n
n−k
2
+ϵ,
4In practice, it can be easily checked if QLwas saturated, and if so, Alg. 1 is repeated with a fresh realization
of the stochastic quantizer QR. Since lemma C.2 guarantees that the saturation probability is low, it implies that
"few" realizations suffice.
22where the expectation is over the stochasticity of the quantizers QQ,QLandQR.
Proof. As in §C.3, an upper bound is obtained on E(Q0+L0R0−W)X⊤2
F, since after the first
iteration, the alternating steps 3and4in Alg. 2 simply refine the estimates of Q,LandR, and can
only yield a smaller error. For convenience of notation, in what follows, the subscript 0is omitted.
SinceL,Ris obtained after applying LPLRF ACTORIZE submodule to (W−Q), lemma C.3 suggests
that if RL,RR,BL, and BRare chosen appropriately,
E∥(Q+LR−W)X⊤∥2
F=E∥(LR−(W−Q))X⊤∥2
F≤X
i>kEσ2
i 
(Q−W)X⊤
+ϵ,(47)
where the expectation in the upper bound on the right hand side is over the randomness in Qdue
to the stochasticity in quantizer QQ. Since Q=LDLQ (W,QQ)is the LDLQ quantizer of Chee
et al. [3](or,BLOCK LDLQ quantizer of Tseng et al. [36], which is a successor of LDLQ proposed
by Chee et al. [3]), it is shown that
Q= Q Q(Q+ (W−Q)M),
where Mis a strictly upper triangular matrix. This is a consequence of the fact that LDLQ quantizes
one column at a time, while simultaneously incorporating a linear feedback from the already quantized
columns. Moreover, if η= Q Q(W+ (W−Q)M)−(W+ (W−Q)M)denotes the quantization
error, then it can be seen that
Q−W=η(M+I)−1(48)
Moreover, since mH=X⊤X= (M+I)D(M+I)⊤,
Eσ2
i 
(Q−W)X⊤
=Eλi 
(Q−W)H(Q−W)⊤
(i)=Eλi 
η(M+I)−1(M+I)D(M+I)⊤(M+I)−⊤η⊤
=Eλi(ηDη⊤), (49)
where (i)follows from (48). From (47),
E∥(Q+LR−W)X⊤∥2
F≤X
i>kEλi(ηDη⊤) +ϵ. (50)
Since ηDη⊤=Pd
j=1Djjηjη⊤
j, where Djjdenotes the jthdiagonal entry of Dandηjis the jth
column of η∈Rn×d,
λi 
ηDη⊤
=λi
dX
j=1Djjηjη⊤
j
≤d(max jDjj)λi
1
ddX
j=1ηjη⊤
j
. (51)
Note that ηconsists of quantization errors, which, for uniformly dithered scalar quantization, are
zero mean random variables with bounded variance upper bounded by∆2
4=R2
(2BQ−1)2. As the
exact error distribution for non-subtractively dithered quantization (which is commonly used in
practice), is not fully understood, a simplification is made for easier analysis, assuming the quan-
tizer is subtractively dithered. For subtractively dithered quantizers, if Schuchman’s conditions
are met [ 11,32], this results in a uniform error distribution in the interval
−∆
2,∆
2
. Therefore,
assuming quantization errors are independent and identically distributed, for large dimensions d, the
eigenvalues of1
dPd
j=1ηjη⊤
jare distributed according to the Marchenko-Pastur distribution, given
byfmp(x) :=2d
πn∆2xp
(λ+−x) (x−λ−), where λ±:=∆2
4 
1±pn
d2. Furthermore, denoting
q:= F−1
mp 
1−k
n
, where F−1
mpis the inverse cumulative distribution function (CDF) of fmp(x), it
can be seen that
X
i>kEλi
1
dX
jηjη⊤
j
≈(n−k)Zq
λ−x
2π(n/d)(∆2/4)p
(λ+−x) (x−λ−)
xdx
(i)
≤√
nd
π
1−k
n
F−1
mp
1−k
n
−λ−
(ii)
≤∆2
π
1−k
n
n−k
2
. (52)
23Here,≈denotes that the equaility holds asymptotically, (i)follows from upper bounding the integral
using the fact that (λ+−x) (x−λ−)is maximized at x=λ++λ−
2. Furthermore, (ii)follows after
substituting the values of λ±and using the following linear upper bound on the inverse CDF of
Marchenko-Pastur distribution,
F−1
mp(x)≤λ+−λ+−λ−
2
(1−x). (53)
It can be verified by inspection that Fmp(x)is lower bounded by
λ+−λ−
2
x−λ2
+−λ2
−
4
, and
inequality (53) is a consequence of this fact. Furthermore, from lemma G.3, max jDjj≤λmax.
Substituting (52) in (51), and substituting ∆2=4R2
(2BQ−1)2, completes the proof.
Informal version of Thm. C.4 : Consider n≈d. From Thm. C.4, a (simplified) asymptotic
dependence on the bit-budgets BLandBRcan be obtained. In what follows, constant factors inside
thelog2(·)have been ignored. Comparing the expressions for B1andB2, it can be seen that the
desired bit-budgets are
BL≥log2 
4σ2
1
σkr
nk
ϵλmax
λmin!
and, BR≥log2 
2σ2r
kd
ϵλmax
λmin!
.
Forn≈d, this yields an average bit-budget of
1
2(BL+ BR) =1
2log28k
ϵσ3
1
σkλmax
λmin√
nd
bits per parameter.
C.5 A simplification for positive definite Hessians
The expressions in (8)are a little involved, but they can be simplified if His positive definite, i.e., all
eigenvalues are strictly greater than 0. LetH=UΛU⊤be the eigenvalue decomposition of H, and
letH1
2=UΛ1
2U⊤be its symmetric square root. Furthermore assume that His positive definite,
i.e., all diagonal entries of Λare strictly positive. In practice, this can be ensured by regularizing H
through the addition of an identity matrix, scaled by a small amount.
Then, consider the following equivalent optimization problems:
min
rank(Z)≤k(A−Z)X⊤2
F≡min
rank(Z)≤kTr 
(A−Z)H(A−Z)⊤
≡min
rank(Z)≤k(A−Z)H1
22
F
≡min
rank(Z)≤k(A−Z)UΛ1
22
F
≡min
rank(Z)≤kY−ZUΛ1
22
F(where Y≜AUΛ1
2)
≡ min
rank(Z′)≤k∥Y−Z′∥2
F(54)
Here,Z′≜ZUΛ1
2, and the constraint rank(Z′)≤kfollow from the fact that multiplying Zby an
invertible matrix, UΛ1
2, keeps its rank unchanged. The final optimization problem can be solved
optimally by considering the SVD of YasY=UΣV⊤, and the optimal solution is
Z′
∗= (UIk) 
I⊤
kΣV⊤
, (55)
where Ikdenotes the first kcolumns of the identity matrix. So the final solution is given by,
Z∗= (UIk)
I⊤
kΣV⊤Λ−1
2U⊤
. (56)
This yields a closed form expression for the optimal solution of the rank-constrained regression
problem for the case of positive definite Hessians.
24D Computational Complexity of CALDERA
Complexity of pre-preprocessing : For a given calibration dataset/activation X∈Rm×d, the
Hessian is computed as H=1
mX⊤X, which requires O(md2)compute. Subsequently, the LDL
decomposition of His computed, which entails O(d3)complexity [ 20]. Moreover, HH†, which is
used in the update equation of `Ris also computed beforehand, with a complexity of O(d3). therefore,
the total complexity of pre-processing is O(md2+ 2d3).
Complexity of LDLQ : Each iteration of CALDERA invokes a single call to LDLQ . An LDLQ call
involves updating each column of an n×diteratively. From (2), updating the kthcolumn requires
multiplying a matrix (of already quantized columns) in Rn×(k−1)with a vector in Rk−1, implying
a complexity of O(n(k−1)). Subsequently, if QQis uniform scalar quantization, quantizing the
(feedback incorporated) kthcolumn entails O(n)compute, implying a total of O(nk)compute for
thekthcolumn. Hence, quantizing all columns from k= 1, . . . , n would require O (nPn
k=1k) =
O
n2
2+3n
2
= O( n2)compute.
Complexity of LPLRF ACTORIZE : Every iteration of CALDERA invokes a call to
LPLRF ACTORIZE , which itself consists of Tininner iterations. Each inner iteration of
LPLRF ACTORIZE , consists of initializing the left and right low rank factors using rank-constrained
regression. As seen in §B, it has a computational complexity of O 
m2(n+d)
. Subsequently,
for any matrix A, the left low rank factor as `L=AHR⊤(RHR⊤)−1can be found using succes-
sive matrix multiplications. While a gradient descent based algorithm can possibly speed up this
computation, in implementation, closed form expressions are computed directly as they are com-
putationally affordable for the hidden dimensions of the LLMs considered. Computing A 
HR⊤
requires O (dk(n+d)), computing (RHR⊤)−1entails O 
dk(k+d) +k3
, and multiplying them
together requires O 
nk2
. Keeping the dominant term, the aggregated computational complexity for
computing the left low-rank factor in each inner iteration is O (ndk).
Additionally, the left low-rank factor can be computed as `R=L†A(HH†), where computing L†Ais
O (ndk). Since HH†is already computed beforehand, multiplying L†AandHH†entails O 
kd2
.
Once again, keeping the dominant term, the complexity is O (ndk). Hence, the total complexity of
each inner iteration of LPLRF ACTORIZE isO (ndk).
Remark : Compared to the two-stage LPLR algorithm proposed in [ 31], the left and right low-rank
factors are iteratively refined in Alg. 2 since this additional compute can be afforded for quantizing
LLM weight matrices. Moreover, SVD computations can be sped up with randomized SVD [14].
E Additional Experimental Details
E.1 System Details
The code for this paper is available at https://github.com/pilancilab/caldera. The framework is
built in PyTorch on top of the QuIP# [ 36] and LoftQ [ 22] repositories, and utilizes HuggingFace
implementations of all datasets and LLMs. Experiments were performed on either NVIDIA RTX
A6000, NVIDIA A10G, or NVIDIA H100 GPUs. Hessian computation for all but LLaMa-2 70B was
performed on four NVIDIA A10 GPUs (provisioned as Amazon AWS EC2 instances), parallelized by
distributing different layers to different GPUs. Hessian computation for LLaMa-2 70B was performed
on a single H100 GPU. Model quantization was performed on four NVIDIA RTX A6000 GPUs,
parallelized in the same manner as Hessian computation. All zero-shot and fine-tuning experiments
were also run on A6000 GPUs, except for LLaMa-2 70B zero-shot experiments, which were run on an
H100 GPU. Low-rank adaptation experiments were parallelized across four GPUs via HuggingFace
Accelerate [ 12], with DeepSpeed ZeRO Level 2 [ 28,30]. The use of Llama family of LLMs for
research is guided by a community license that can be founded at https://llama.meta.com/license/ and
https://llama.meta.com/llama3/license/.
25E.2 Parameters of CALDERA Quantization
For all CALDERA decompositions, the number of alternating iterations between updating QandL,
R(i.e.,Toutin Alg. 1) is 15. For decompositions with quantized low-rank factors, the number of
LPLR iterations (i.e., Tinin Alg. 2) is 10.
For both CALDERA and QuIP# decompositions, the calibration dataset consists of 256random
samples from the RedPajama dataset. The number of tokens in each calibration data point is equal to
the context length of the corresponding model, i.e., 4096 for LLaMa-2 7B and 8192 for LLaMa-3 8B.
Note that this calibration dataset is significantly smaller than the 6144 datapoints used in [36].
An additional heuristic applied during the CALDERA decomposition is an update to the Hessian
matrix based on the subspace spanned by LR:
eH≜H−MVV⊤M⊤, (57)
Where H=MM⊤is the LDL decomposition of the Hessian and LRM =UΣV⊤is the singular
value decomposition of the product LRM . The updated eHis used in all LPLR updates, and the
original Hessian is used for updating the low-rank factors via Alg. 2. This heuristic was found in the
QuIP# codebase, and empirically speeds up convergence of CALDERA, as is discussed in App. F.1.
Additionally, the SVD steps while quantizing 70B models was replaced by randomized SVD [ 14] for
computational speedup.
E.3 Details about Goodness-of-Fit Metrics
In addition to perplexity on the WikiText2 and C4 datasets, CALDERA was evaluated using zero-shot
accuracy on the following sequence classification tasks:
1.Winogrande [19]: a collection of 44k problems problems inspired by the Winograd Schema
Challenge [ 21], specifically designed to be robust to learned biases in model training datasets.
Specifically, Winogrande is a two-choice fill-in-the-blank task that requires significant
commonsense reasoning.
2.RTE [1]: Recognizing Text Entailment is a task in the General Language Understanding
Evaluation (GLUE) benchmark [ 40]. Given two statements, the model must classify whether
or not the second follows from the first.
3.PiQA [2]: Physical Interaction: Question Answering is a question-answering task that
requires understanding about physical relationships between objects.
4.ArcE [4]: In general, the AI2 Reasoning Challenge is a set of multiple-choice questions
that require a grade-school level of knowledge. ArcE is the subset that is labeled Easy.
5.ArcC [4]: The ARC-Challenge problems are in the same format as ARC-Easy, but the
dataset only includes problems that were answered incorrectly by selected algorithms.
The unquantized ( 16-bit) numbers mentioned in Tabs. 1 and 2 are either copied from [ 36] for the tasks
available reported therein. For some ones that could not be easily found, inference was performed
directly on the 16-bit model downloaded from HuggingFace.
E.4 Fine-tuning Parameter Details
Fine-tuning of the diagonal Rademacher matrices in the RHT was performed over a calibration
dataset sampled from the training split of RedPajama. Due to computational constraints, each sample
contains 512tokens, rather than the full context length of the model. The calibration dataset is
256samples in total, with 192data points in the training split and 64in the evaluation split. RHT
fine-tuning was performed for 5epochs with a learning rate of 10−3. The epoch with the lowest
evaluation loss was used for further zero-shot and fine-tuning experiments.
Parameters for low-rank adaptation experiments can be found in Table 8. Overall, the number
of epochs and therefore number of training steps, was determined empirically based on when the
evaluation accuracy plateaued.
For the comparison of CALDERA to LQ-LoRA in Winogrande accuracy, the LQ-LoRA result
reported in [ 13] did not involve fine-tuning directly on the Winogrande dataset, but rather on a subset
of C4 and Wikitext2.
26Table 8: Hyperparameter settings for low-rank adaptation*. Batch size refers to the per-device batch size. All
fine-tuning experiments are parallelized across four GPUs.
Dataset Block Size Batch Size*Grad. Acc. Epochs LR Weight Decay LR Sched. Warmup Steps
Wikitext2 512 2 1 step 3 3e-6 0.001 Linear 42 steps 2091
RTE 128 20 2 steps 10 3e-5 0.01 Linear 100 steps 160
Winogrande 256 10 1 step 3 1e-5 0.01 Linear 100 steps 3030
E.5 Computation of Average Bit Budget Per-Parameter
Assume that the dimensions of the seven weight matrices in each transformer layer have dimensions
ni×di, where i∈ {1, . . . , 7}. Also, for the models we consider, each transformer layer has matrices
of the same dimensions. In general, if the full-rank matrix Qhas a bit budget of BQ, the factors L
andRboth have a bit budget of BLR, and the rank of the factors is k, the average number of bits per
parameter isP7
i=1(BQnidi+kBLR(ni+di))P7
i=1nidi.
For LLaMa-2 7B, all self-attention matrices are 4096×4096 , the gate andupprojections are
11008 ×4096 , and the down projection is 4096×11008 . So, (ni, di) = (4096 ,4096) fori∈
{1, . . . , 4},(n5, d5) = (n6, d6) = (11008 ,4096) , and (n7, d7) = (4096 ,11008) .
For LLaMa-3 8B, (n1, d1) = ( n4, d4) = (4096 ,4096) ,(n2, d2) = ( n3, d3) = (4096 ,1024) ,
(n5, d5) = (n6, d6) = (14336 ,4096) , and (n7, d7) = (4096 ,14336) .
In §5.3, r= 64 of the kfactors are fine-tuned in 16-bit precision. This results in the following
number of bits per parameter:P7
i=1(BQnidi+ (k−r)BLR(ni+di) + 16 r(ni+di))P7
i=1nidi.
E.6 Additional Notes on Numerical Simulations
For LLaMa-2 70B, the zero-shot perplexities and accuracies reported in the QuIP# paper [ 36] could
not be replicated. As such, the QuIP# performance reported in this paper are based on recreations of
those experiments.
F Additional Numerical Simulations
F.1 Ablation Study of CALDERA Parameters with respect to Frobenius Norm Error
The per-iteration relative Frobenius norm error of CALDERA, for a few selected weight matrices of
LLaMa-2 7B, is plotted in Figure 3. The data-aware relative Frobenius norm error is defined as(Q+LR−W)X⊤
F
∥WX⊤∥F,
where Wis the unquantized weight matrix, Xis the calibration data, and Q+LRis the CALDERA
decomposition. The following ablations of CALDERA decomposition parameters are considered:
1.16-Bit Factors :Qis quantized to 2bits via LDLQ. The low-rank factors are kept in half
precision, so no LPLR iterations are required after rank-constrained regression is performed.
The randomized Hadamard transform is performed on the WandHmatrices are prior to
quantization.
2.4-Bit Factors : The low-rank factors are quantized to 4bits of precision using an E8 lattice
quantizer, and 10iterations of LPLRF ACTORIZE (Alg. 2) are run for each update of the
factors (i.e., Tin= 10 in Alg. 1). Otherwise, the decompostion parameters are the same as
in16-Bit Factors .
3.4-Bit Factors (No RHT) : The parameters of CALDERA are the same as in 4-Bit Factors ,
except the randomized Hadamard transform step is omitted.
274.4-Bit Factors (No LDLQ) : The parameters of CALDERA are the same as in 4-Bit Factors ,
except Qis quantized to 2-bit precision using direct E8 lattice quantization instead of the
LDLQ algorithm.
5.16-Bit Factors (Hessian Update) : The parameters of CALDERA are the same as in 16-Bit
Factors , except the Hessian update step described in App. E.2 is performed prior to LDLQ.
For each ablation, the rank of LandRvaries between k∈ {64,128,256}. For comparison, QuIP#
with the same-size low-rank factors is performed, using the same calibration data and the low-rank
decomposition from the QuIP# codebase.
0 10 20 30 40 50
Iteration0.1050.1100.1150.1200.1250.1300.1350.140Relative  F ro. Norm ErrorLLaMa-2 7B Layer 25 Key Proj., Rank-64 F actors
QuIP#
16B F actors
4B F actors4B F actors (No RHT)
4B F actors (No LDLQ)
16B F actors (Hessian Update)
0 10 20 30 40 50
Iteration0.220.230.240.250.260.27Relative Fro. Norm ErrorLLaMa-2 7B Layer 25 Up Proj., Rank-64 Factors
QuIP#
16B F actors
4B F actors4B F actors (No RHT)
4B F actors (No LDLQ)
16B F actors (Hessian Update)
0 10 20 30 40 50
Iteration0.100.110.120.13Relative  F ro. Norm ErrorLLaMa-2 7B Layer 25 Key Proj., Rank-128 F actors
QuIP#
16B F actors
4B F actors4B F actors (No RHT)
4B F actors (No LDLQ)
16B F actors (Hessian Update)
0 10 20 30 40 50
Iteration0.210.220.230.240.250.260.27Relative  Fro. Norm ErrorLLaMa-2 7B Layer 25 Up Proj., Rank-128 Factors
QuIP#
16B F actors
4B F actors4B F actors (No RHT)
4B F actors (No LDLQ)
16B F actors (Hessian Update)
0 10 20 30 40 50
Iteration0.080.090.100.110.120.13Relative  Fro. Norm ErrorLLaMa-2 7B Layer 25 Key Proj., Rank-256 Factors
QuIP#
16B F actors
4B F actors4B F actors (No RHT)
4B F actors (No LDLQ)
16B F actors (Hessian Update)
0 10 20 30 40 50
Iteration0.190.200.210.220.230.240.250.26Relative  Fro. Norm ErrorLLaMa-2 7B Layer 25 Up Proj., Rank-256 Factors
QuIP#
16B F actors
4B F actors4B F actors (No RHT)
4B F actors (No LDLQ)
16B F actors (Hessian Update)
Figure 3: Relative data-aware Frobenius norm error per iteration of CALDERA for selected matrices
of LLaMa-2 7B layer 25. For all experiments, the bit precision of Qis2, and the calibration dataset
is the same as used in §5. The first iteration of CALDERA with the Hessian update is omitted, as it
has a large error, inhibiting plot readability.
Overall, CALDERA with 16-bit factors consistently achieves a lower error than QuIP#. Additionally,
the Hessian update heuristic improves convergence and often, but not always, reduces the final error
achieved after 50iterations of Alg. 1. CALDERA with 4-bit factors has higher Frobenius-norm
error than with 16-bit factors, but the degradation is minor. On the other hand, replacing LDLQ with
a lattice quantizer significantly degrades the Frobenius norm error, and omitting the randomized
Hadamard transform worsens the error for some of the weight matrices considered.
280 10 20 30 40 50
LPLR Iteration0.350.400.450.500.550.600.65Relative Fro. Norm ErrorLLaMa-2 7B Layer 25 Key Proj.
Rank 256
Rank 128
Rank 64
0 10 20 30 40 50
LPLR Iteration0.760.780.800.820.840.86Relative F ro. Norm ErrorLLaMa-2 7B Layer 25 Up Proj.
Rank 256
Rank 128
Rank 64Figure 4: Relative data-aware Frobenius norm error per iteration of LPLRF ACTORIZE , for the
decomposition W≈LR, for two matrices in LLaMa-2 7B layer 25.
To demonstrate the convergence of LPLRF ACTORIZE (Alg. 2), the per-iteration relative data-aware
error of LPLRF ACTORIZE for the factorization W≈LRis plotted in Figure 4. For all curves
plotted, the randomized Hadamard transform is performed on WandHbefore the factorization is
computed, and both factors are quantized to 4bits of precision via an E8 lattice quantizer.
In both cases, the alternating minimization iterations reduce the error. For the Up projection matrix,
this reduction is nominal, whereas, for the Key projection matrix, the alternating iterations result in a
significant improvement.
F.2 Experiments on Mistral-7B
Perplexity and language modeling benchmark accuracy results for quantizing Mistral 7B via
CALDERA and QuIP# are in Table 9. Results are consistent with those in Section 5.1.
Table 9: Evaluations of Wikitext2 and C4 perplexities, as well as percent accuracies on some common language
modeling benchmarks, on CALDERA-compressed Mistral 7B. All quantizations use calibration datasets released
on Huggingface by the authors of QuIP#. BQ= 2 bits throughout, and BL= B R= 4 bits where low-rank
factors are present. For fairness of comparison, QuIP# numbers reported do not include RHT finetuning.
Model Method Rank Avg Bits Wiki2 ↓C4↓Wino ↑ PiQA ↑ ArcE ↑ ArcC ↑
(acc)(acc_norm )(acc_norm )(acc_norm )
Mistral 7B CALDERA 128 2.19 6.01 8.92 71.51 78.02 75.21 46.59
Mistral 7B CALDERA 256 2.38 5.71 8.44 70.24 79.92 74.92 46.93
Mistral 7B QuIP# 0 2 6.19 9.09 69.30 78.45 72.90 44.80
G Auxiliary Results
G.1 Useful Results from Linear Algebra and Probability
This section states some useful linear algebra results as lemmas. Some of them are stated without
proof, which can be easily proved or found in a linear algebra textbook such as [10].
Lemma G.1. (Eckart-Young-Mirsky theorem) For any matrix Awith SVD given by A=UΣV⊤,
the solution of Ak:= arg minrank(bA)≤k∥A−bA∥2
Fis given by Ak=
UΣ1/2Ik
I⊤
kΣ1/2V⊤
,
and∥A−Ak∥2
F=P
i>kσ2
i(A).
Lemma G.2. (Vershynin [38, Thm 4.4.5 ]) LetXbe ad×mrandom matrix whose entries Xij
are independent, zero-mean, subgaussian random variables. Then, for any t > 0,∥X∥2≤
CK√
d+√m+t
with probability exceeding 1−2e−t2, where K= max i,j∥Aij∥ψ2, and
∥·∥ψ2denotes the subgaussian norm, and Cis an absolute constant.
29Remark : Lemma G.2 states a high probability upper bound on the spectral norm of a random matrix
in terms of the subgaussian norm of the entries of the matrix. The subgaussian norm of a subgaussian
random variable Xis defined as ∥X∥ψ2≜inf{t≥0|E[eX2/t2]≤2}. It can be shown that any
bounded random variable Xis subgaussian, and satisfies ∥X∥ψ2≤∥X∥∞
log 2.
Lemma G.3. Suppose the LDL decomposition of a matrix H∈Rd×dis given by H= (M+
I)D(M+I)⊤, where Mis strictly upper triangular and Dis diagonal. Then, max jDjj≤λmax,
where λmaxdenotes the largest eigenvalue of H.
Proof. Letejdenote the jthcanonical basis vector. Then,
e⊤
jHej=e⊤
j(M+I)D(M+I)⊤ej(i)=Djj+X
i>jM2
ji. (58)
Moreover, from properties of eigenvalues, e⊤
jHej≤λmax. Therefore, for any j,Djj≤Djj+P
i>jM2
ji≤λmax. Since it holds true for all j, this completes the proof.
Lemma G.4. (Loewner ordering for matrix products) For any matrix AandB,
σ2
min(A)B⊤B≼B⊤A⊤AB≼σ2
max(A)B⊤B.
Lemma G.5. (Minimum singular value) For matrices AandB,σmin(A+B)≥σmin(A)−∥B∥2.
G.2 Uniformly dithered scalar quantizer
Let us consider quantizing a scalar xwith|x| ≤R. Given Bbits, the scalar quantizer with dynamic
range Ris described by first specifying the M= 2Bquantization points
q1=−R, q2=−R + ∆ , q3=−R + 2∆ , . . . , q M=−R + ( M−1)∆,
where ∆ =2R
M−1is the resolution. The quantizer operation is defined as:
QR,B(x) =qk+1with probability r,
qkwith probability 1−r,(59)
where k= arg maxj{qj≤x}, i.e., x∈[qk, qk+1), and r=x−qk
∆. As shown in the following
lemma G.6, such a quantizer satisfies
E[QR,B(x)] =xandE(QR,B(x)−x)2≤∆2
4=R2
(2B−1)2, (60)
i.e., it is unbiased and the error variance depends on RandB. Here, the E(·)is over the randomness
from dithering in (59). If the input xto the quantizer falls outside this range, i.e., x >Rorx <−R,
the quantizer is said to be saturated . To quantize any matrix X,QR,B(X)is obtained by quantizing
each entry independently, i.e., [QR,B(X)]ij≜QR,B(Xij).
Lemma G.6. For scalar xwith|x| ≤R, denote the quantization error of uniformly dithered B–bit
scalar quantizer as ϵ= Q R,B(x)−x. Then, E[ϵ] = 0 andVar(ϵ)≤R2
(2B−1)2, where the expectation
Eis over the randomness due to dithering in the quantizer operation.
Proof. Suppose x∈[qk, qk+1)andqk+1=qk+ ∆, where ∆ =2R
2B−1. Then,
EQR,B(x) =qk+1x−qk
∆+qk
1−x−qk
∆
=(qk+ ∆) ( x−qk) +qk(∆−x+qk)
∆=x.
Furthermore, the variance can be upper bounded as
Var(QR,B(x)−x)2= (qk+1−x)2(x−qk)
∆+ (qk−x)2
1−x−qk
∆
≤(qk+1−x) (x−qk)
≤ sup
x∈[qk,qk+1)(qk+1−x) (x−qk)
=
qk+1−qk+qk+1
2qk+qk+1
2−qk
=∆2
4=R2
(2B−1)2.
30When the input xto the quantizer exceeds R, the quantizer is said to be saturated , causing the
quantization error to deviate from zero mean and bounded variance. Thus, it is crucial to ensure that
the quantizer operates within the unsaturated regime with high probability.
H Limitations and Further Discussions
Our proposed algorithm CALDERA exploits low-rank structure in weight matrices of LLMs, and is
seen to boost the performance of existing methods in the literature of LLM compression. It does so
by providing an additional degree of freedom to control the compression ratio – namely, the target
rank(k). Theoretical guarantees show improved performance over existing benchmarks, when the
matrix being compressed is inherently low-rank to begin with – something that is observed in LLMs.
This also implies that CALDERA can be used for any other application scenarios that require matrix
compression, as discussed in [ 31]. Its effectiveness largely depends on the presence of an inherent
approximate low-rank structure, which is seen in many real-world matrices [37].
Despite attaining a lower loss for LLM compression, since CALDERA is designed to tackle an
optimization problem through an iterative process, it requires slightly more computational resources.
For instance, compressing Llama-2 7B or Mistral-7B models (with rank- 256) took approximately
34GPU-hours (on NVIDIA A10G GPUs provisioned from an AWS G5 instance), and compressing
Llama-2 13B took 59GPU-hours (on a locally hosted NVIDIA A6000 GPU). Moreover, LLaMa-2
70B can be quantized via CALDERA with rank- 256factors in approximately 90GPU hours (on an
H100 from Lambda labs), which is on par with QuIP# (with RHT finetuning), which reports 100GPU
hours (on A100). It should be noted that these wallclock times can be reasonably afforded, and are
not prohibitively large . Furthermore, since the compression of an LLM is a one-time computational
expense, this cost becomes highly amortized considering the frequent use of LLMs for inference
following the deployment of the compressed model.
Although CALDERA often achieves perplexities and accuracies similar to unquantized models, a
gap remains, as shown in Tables 1 and 2. This indicates there is potential for enhancing quantization
strategies. For example, the target rank (k)can be treated as a hyper-parameter, and adjusted across
different layers. Sharma et al. [33] demonstrate that such a layer-specific rank reduction can improve
generalization capabilities. Additionally, improved fine-tuning strategies such as those proposed by
Liu et al. [23], which reduce the gap between LoRA and full fine-tuning, can be incorporated with
the low-rank adaptation step of CALDERA. Detailed investigations are left for future work.
Broader Impacts : Compressing models enables their deployment in resource-constrained settings,
facilitating educational and technological advancements with limited infrastructure. Deploying
on edge devices for inference also enhances privacy by reducing the need for data to be sent to
centralized servers for inference, thereby enhancing user privacy and data security. Additionally,
lower computational requirements of inference using compressed models is a step towards adoption
of environment-friendly AI strategies.
On the other hand, as the compression is often not always lossless, any technique may result in
reduced accuracy or loss of nuanced understanding, potentially impacting the reliability of the models
in critical applications. Conseuqently, due diligence should be exercised when deploying these
models. From a broader perspective, LLMs are quite powerful, and their easier deployment can
possibly lead to misuse, such as the spread of misinformation or automated generation of harmful
content. Consequently, robust regulatory frameworks are necessary, as LLMs continue becoming
more accessible to the general audience.
31NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract proposes an algorithm for compression large language mod-
els (LLMs) using low-precision and low-rank approximation. The algorithm, named
CALDERA, presented in this work achieves that goal. The performance of CALDERA
is studied theoretically by deriving explicit approximation error guarantees, as well as
numerically by compressing Llama and Mistral families of models for a variety of tasks,
and shows improved performance over existing benchmarks that don’t take into account
low-rankness of the weight matrices.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Limitations are discussed explicitly in Appendix Sec. H. Assumptions made
with regard to theory are stated in the theorem statement. These assumptions hold in practice
as our algorithm is seen to perform well in numerical simulations. Extensive experimentation
were performed on as many setups as allowed within our computational budget.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
32judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All the assumptions are stated in the main Theorem statement of §4. A brief
outline of the analysis (i.e., proof sketch) is provided in §4.1. Complete derivations of the
proof are provided in App. C. The derivation is obtained by providing lemmas that build
up to the proof, and the lemmas are appropriately cross-referenced. Auxiliary lemmas and
well-known results that are used the derivation are stated as auxiliary lemmas in App. G.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Experimental details are provided in §5 and App. F. Open source datasets are
used, and the codebase associated with the paper is also open-sourced.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
33(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The codebase developed for this paper is open-sourced at
https://github.com/pilancilab/caldera. The README file provides necessary instructions to
reproduce experiments from the paper.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Experimental details are provided in Sec. 5 and App. E. Code is open-
sourced at https://github.com/pilancilab/caldera, along with a README file that provides
instructions on how to reproduce the results from the paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
34Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: As LLMs are computationally expensive to process and run inference on, error
bars are not reported. However, since code is open-sourced, results can be repeated with
different configurations.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Details are provided in Sec. 5 and App. E.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conforms, in every respect, with the
NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
35•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Broader societal impacts are discussed in App. H
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [No]
Justification: No new datasets are used in this work. All datasets are already publicly
open-sourced. Models compressed using CALDERA will be open-sourced.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
36Justification: All the creators and original owners of assets used in this paper have been
properly credited, and the associated license and terms of use mentioned and respected.
Details can be found in §5 and App. E.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: Compressed models will be released publicly.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowd-sourcing or research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
37Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowd-sourcing or research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
38