Predicting Ground State Properties:
Constant Sample Complexity and Deep Learning
Algorithms
Marc Wanner
Computer Science and Engineering
Chalmers University of Technology
and University of Gothenburg
wanner@chalmers.seLaura Lewis
Applied Mathematics and Theoretical Physics
University of Cambridge
Cambridge, United Kingdom
llewis@alumni.caltech.edu
Chiranjib Bhattacharyya
Computer Science and Automation
Indian Institute of Science
Bangalore, India
chiru@iisc.ac.inDevdatt Dubhashi
Computer Science and Engineering
Chalmers University of Technology
and University of Gothenburg
dubhashi@chalmers.se
Alexandru Gheorghiu
Computer Science and Engineering
Chalmers University of Technology
and University of Gothenburg
aleghe@chalmers.se
Abstract
A fundamental problem in quantum many-body physics is that of finding ground
states of local Hamiltonians. A number of recent works gave provably efficient
machine learning (ML) algorithms for learning ground states. Specifically, Huang
et al. in [ 1], introduced an approach for learning properties of the ground state of
ann-qubit gapped local Hamiltonian Hfrom only nO(1)data points sampled from
Hamiltonians in the same phase of matter. This was subsequently improved by
Lewis et al. in [ 2], toO(logn)samples when the geometry of the n-qubit system is
known. In this work, we introduce two approaches that achieve a constant sample
complexity, independent of system size n, for learning ground state properties.
Our first algorithm consists of a simple modification of the ML model used by
Lewis et al. and applies to a property of interest known in advance. Our second
algorithm, which applies even if a description of the property is not known, is a
deep neural network model. While empirical results showing the performance of
neural networks have been demonstrated, to our knowledge, this is the first rigorous
sample complexity bound on a neural network model for predicting ground state
properties. We also perform numerical experiments on systems of up to 45qubits
that confirm the improved scaling of our approach compared to [1, 2].
1 Introduction
One of the most important problems in quantum many-body physics is that of finding ground
states of quantum systems. This is due to the fact that the ground state describes the behavior of
electronic systems (e.g., metals, magnets, etc.) at room temperature well. Thus, understanding
38th Conference on Neural Information Processing Systems (NeurIPS 2024).the ground state can provide insights into, for example, chemical properties of molecules, leading
to many potential applications in chemistry and materials science. However, despite extensive
research [ 3,4,5,6,7,8,9,10,11,12,13,14], an efficient classical algorithm solving this problem
in full generality remains out of reach. On the other hand, researchers have successfully leveraged
classical machine learning (ML) techniques to solve (albeit largely heuristically) the ground state
problem and other related quantum many-body problems [ 15,16,17,18,19,20,21,22,23,24,25,
26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46]. Rather than solving
these problems directly from first principles, ML algorithms are given some training data collected
from physical experiments and are asked to generalize it to new inputs. Intuitively, this additional
data can make the problem easier and thus may open the door to obtaining provably efficient classical
ML algorithms for finding ground states. This data-driven approach is in some sense necessary, since
finding the ground state from the Hamiltonian alone is known to be QMA -hard in general [ 47], and
thus out of reach for both efficient classical and quantum algorithms.
In a recent work [ 1], Huang et al. proposed the first provably efficient ML algorithm for predicting
ground state properties of gapped geometrically local Hamiltonians. In particular, the algorithm
in [1] uses an amount of training data (or sample complexity ) that scales as O(n1/ϵ), where nis the
system size and ϵis the prediction error of the ML algorithm. Recently, [ 2] improved this guarantee,
achieving O(log(n)2polylog(1 /ϵ)), anexponential improvement with respect to the system size n. The
same sample complexity was obtained by [ 48] for the task of learning thermal state properties with
exponential decay of correlations. Moreover, [ 49] extended this to Lindbladian phases of matter [ 50]
with local rapid mixing, including both ground states of gapped Hamiltonians and thermal states. The
work of [ 51] obtains a similar guarantee assuming the continuity of quantum states in the parameter
range of interest but focusing on the scaling with respect to 1/ϵrather than system size.
These previous works drastically improve the sample complexity of the original Huang et al. result [ 1],
but none prove sample complexity lower bounds for their respective tasks, leaving open the possibility
of further reducing the sample complexity. In addition, [ 2,48,49] all use fairly simple learning
models, i.e., regularized linear regression or taking empirical averages of classical shadows [ 52],
respectively. With the emergence of neural networks as a popular model in practical ML, one
may wonder if these more powerful ML tools may be useful to predict ground state properties as
well. In fact, recent works [ 37,36] empirically demonstrate a favorable sample complexity using
neural-network-based ML algorithms. However, there are currently no rigorous theoretical guarantees
regarding the amount of training data needed to achieve a desired prediction error. These remarks
lead us to the following two central questions of this work.
Question 1. Can classical ML algorithms predict ground state properties with even less than
O(log(n)2polylog(1 /ϵ))data?
This is especially relevant for systems approaching the thermodynamic limit, where the system size
can be arbitrarily large. Needing fewer samples also means less work for experimentally preparing
ground states of the system. The second question, stated as an open question in [1, 2] is:
Question 2. Can we obtain rigorous sample complexity guarantees for neural-network-based ML
algorithms for predicting ground state properties?
Our results
We give positive answers to both questions. We consider the same assumptions as [ 2] with minimal
additional ones that we mention here. First, we show that a simple modification to the approach in [ 2]
allows us to achieve a sample complexity that is independent of the system size . This does, however
require knowledge of the property we wish to predict in advance, whereas this is not a requirement
in [2]. We view this as a reasonable assumption, since in practice we can imagine preparing ground
states of some system in order to measure a specific property of interest. We show the following
theorem, stated informally here. The formal statement, including all the assumptions required for
proving the result, can be found in Appendix B.
Theorem 1 (Informal) .LetH(x)be an n-qubit gapped, geometrically local Hamiltonian with ground
state ρ(x). Given an observable O, with a known decomposition as a sum of local Pauli operators
and given training data {(xℓ, yℓ)}N
ℓ=1sampled from an arbitrary distribution, with yℓ≈tr(Oρ(xℓ)),
2Figure 1: A deep network model for predicting ground state properties. Given a vector x∈
[−1,1]mthat parameterizes a quantum many-body Hamiltonian H(x), the algorithm uses geometric
structure to create “local” neural network models fθPi
Pi. The ML algorithm then combines the outputs
of these local models to predict a property tr(Oρ(x)), where ρ(x)is the ground state of H(x).
Here, we decompose O=PM
i=1αPiPifor Pauli operators Pi, where the final layer takes a linear
combination of the outputs of the local models weighted by some trainable parameters wPithat
intuitively should approximate the Pauli coefficients αPi.
there is an ML algorithm for predicting ground state properties tr(Oρ(x))to within precision ϵ >0
using N=O 
2polylog(1 /ϵ)
training samples.
Note that the number of samples Ndepends only on the desired prediction error ϵand is independent
of the system size. In particular, this means that for a fixed prediction error our algorithm requires
only a constant amount of training data. Moreover, the computational complexity of our algorithm
improves upon [ 2], having O(n)runtime, compared to the previous O(nlogn). While removing the
lognfactor may seem like a small improvement, in practice this can make a significant difference.
For instance, for a system of n∼1000 qubits, removing the lognfactor would result in a ten-fold
reduction in training data and time.
Much like in [ 2], this result also extends to learning classical representations of ρ(x). In other words,
if the algorithm is instead given classical shadows [52] of the ground state as training data, it can then
predict a classical representation of ρ(x)for new parameters x. This can mitigate the requirement
that the observable is known in Theorem 1, as predicting properties from a classical representation
clearly requires knowledge of the observable.
Our second result shows the same sample complexity guarantee for a neural network ML algorithm
(Figure 1) [ 53], in which one does not need to know the observable being measured in advance. An
additional constraint that we require in this case is that the training data is not sampled according to
an arbitrary distribution, but a distribution satisfying some technical assumptions. We note that these
assumptions are satisfied for common distributions such as uniform and Gaussian.
Theorem 2 (Informal) .LetH(x)be an n-qubit gapped, geometrically local Hamiltonian with ground
stateρ(x). For any observable O, expressible as a sum of local Pauli operators and given training
data{(xℓ, yℓ)}N
ℓ=1, sampled from a distribution satisfying certain assumptions with yℓ≈tr(Oρ(xℓ)),
there is a neural network ML algorithm for predicting ground state properties tr(Oρ(x)), for uniform
x, to within precision ϵ >0using N=O 
2polylog(1 /ϵ)
training samples under mild assumptions
on training.
We prove this result by making use of the Koksma-Hlawka inequality from quasi-Monte Carlo
theory [54, 55, 56, 57, 58] and combining it with the spectral flow formalism [59, 60, 61].
Similar to Theorem 1, we can also extend this result to learning classical representations of ρ(x)when
given classical shadow training data. The formal statement and its proof can be found in Appendix C.
We also remark that, much like the setting in [ 51], a more favorable scaling with respect to ϵcan
be achieved if the number of parameters that the Hamiltonian depends on is constant. In particular,
it was shown in [ 51] that if the number of parameters is constant, the sample complexity scales
3asN=poly(1/ϵ,log(n)).For our results, this similarly yields N=poly(1/ϵ),preserving the
independence on the system size while also achieving a polynomial scaling in 1/ϵ.
Furthermore, we perform numerical experiments on system sizes of up to 45 qubits, which support
our theoretical findings, and show that, in practice, our deep learning algorithm outperforms previous
methods [2]. We describe them in detail in Appendix D, and they are illustrated in Figure 2.
2 Preliminaries
2.1 Problem statement
First, we formally describe the problem setting, which is the same as [ 2]. We consider a family
ofn-qubit Hamiltonians H(x)smoothly parameterized by an m-dimensional vector x∈[−1,1]m.
We assume that these Hamiltonians are gapped for all choices of parameters x∈[−1,1]mand
geometrically local such that they can be written as a sum of local terms
H(x) =LX
j=1hj(⃗ xj), (2.1)
where the parameter vector xis a concatenation of the constant-dimensional vectors ⃗ x1, . . . , ⃗ x L. Each
of these constant-dimensional vectors ⃗ xjparameterizes the local interaction term hj(⃗ xj). Crucially,
we assume that each local term hjonly depends on a constant number of parameters rather than the
entire parameter vector x. We also assume that the geometry of the n-qubit system is known.
Throughout this work, we use ρ(x)to denote the ground state of the Hamiltonian H(x)andOto
denote an observable that can be written as a sum of geometrically local observables with bounded
spectral norm ∥O∥∞≤1. Here, the ground states ρ(x)form a gapped quantum phase of matter.
Given samples of quantum states drawn from this phase, we wish to predict expectation values of
observables Owith respect to other states in the same phase. In other words, we are given training
data{(xℓ, yℓ)}N
ℓ=1, where yℓ≈tr(Oρ(xℓ))approximates the ground state property for a parameter
choice xℓ∈[−1,1]msampled from some distribution Dover the parameter space. We aim to learn a
function h∗(x)that approximates the ground state property tr(Oρ(x))for some unseen parameter
xwhile minimizing the amount of training data, or sample complexity, N. How well we learn the
ground state property is quantified by the average prediction error
E
x∼D|h∗(x)−tr(Oρ(x))|2≤ϵ. (2.2)
We describe the precise conditions under which Theorems 1 and 2 hold in the following sections.
2.2 Review of previous algorithm
In this section, we review the previous algorithm from [ 2], as our proofs rely on similar ideas. For
full details, we refer the reader to [ 2] and our more detailed presentation in Appendix A.1. The
ML algorithm proposed in [ 2] requires some geometric definitions. Fix a geometrically local Pauli
observable P∈ {I, X, Y, Z }⊗n.
Letδ1, B > 0be efficiently-computable hyperparameters that we define in Appendix A.1. Define the
setIPof coordinates csuch that xcparameterizes some local term hj(c)that is close to the Pauli P.
Here, the distance between two observables dobsis defined as the minimum distance between the
qubits that the observables act on, where the distance between qubits is given by the geometry of the
system, which we assume to be known. Formally, we define this set of local coordinates as
IP≜{c∈ {1, . . . , m }:dobs(hj(c), P)≤δ1}, (2.3)
where hj(c)is the local term in the Hamiltonian H(x)whose parameters ⃗ xj(c)include the variable
xc. The intuition behind this set of coordinates is that it indexes the parameters xcthat influence the
ground state property tr(Pρ(x))corresponding to the Pauli P. The algorithm consists of two steps.
First, it maps the parameter space [−1,1]mto a high dimensional space via a nonlinear feature map
ϕ. Second, it runs ℓ1-regularized linear regression (LASSO) [62, 63, 64] over the feature space.
This first step encodes the geometry of the problem. The feature map intuitively projects a given
parameter vector xonto the local parameter space {xc:c∈IP}. We define this precisely in
4Appendix A.1. Following the feature mapping, the ML algorithm uses LASSO [ 62,63,64] to learn
functions of the form {h(x) =w·ϕ(x) :∥w∥1≤B}for a chosen hyperparameter B > 0. We
denote the learned function by h∗(x) =w∗·ϕ(x). For our purposes, we set B= 2O(polylog(1 /ϵ1)).
This algorithm obtains the following rigorous guarantee.
Theorem 3 (Theorem 1 in [ 2]).Given n, δ > 0,1
e> ϵ > 0and a training data set {(xℓ, yℓ)}N
ℓ=1of
size
N= log( n/δ)2polylog(1 /ϵ), (2.4)
where xℓis sampled from an unknown distribution Dand|yℓ−tr(Oρ(xℓ))| ≤ϵ. With a proper choice
of the efficiently computable hyperparameters δ1, δ2, and B, the learned function h∗(x) =w∗·ϕ(x)
satisfies
E
x∼D|h∗(x)−tr(Oρ(x))|2≤ϵ (2.5)
with probability at least 1−δ. The training and prediction time of the classical ML model are
bounded by O(nN) =nlog(n/δ)2polylog(1 /ϵ).
A crucial step in the proof is that ground state properties can indeed be approximated by linear
functions over the feature space. Along the way, [ 2] proves that the ground state property can be
approximated by a linear combination of “local functions,” which are local in that they only depend
on parameters with coordinates in the set IP. We relegate further details to Appendix A.1 and [2].
3 Main results
In this section, we discuss our rigorous guarantees for predicting ground state properties with constant
sample complexity and with neural-network-based ML algorithms.
3.1 Constant sample complexity
In this section, we show that a simple modification of the algorithm from [ 2] can achieve a sample
complexity that is independent of the system size n, under the additional assumption that the
observable Ois known. In practice, a scientist often has a specific ground state property in mind
that they wish to study, so we view this as a natural assumption. Moreover, this is still an interesting
learning problem, as when obtaining the training data via quantum experiments, preparing the ground
stateρ(x)in the laboratory for a new choice of parameters xmay be difficult experimentally. This
in turn means that accurately predicting some property tr(Oρ(x))for a new choice of xmay be
challenging, even if the property of interest, O, is known. ML algorithms can allow us to circumvent
this issue and generalize from the results of few training data points without needing to prepare the
ground state directly.
LetO=P
P∈{I,X,Y,Z }⊗nαPPbe an observable that can be written as a sum of geometrically local
observables. Because Ois assumed to be known, we can find this decomposition of Oin terms
of the Pauli observables P. The overall structure of the algorithm remains the same: perform a
nonlinear feature mapping followed by linear regression. However, there are two key differences
from the previous algorithm [ 2]. First, we change the feature mapping of [ 2] to incorporate the
Pauli coefficients αP. We define this new feature mapping ˜ϕin Appendix B. The second difference
from [ 2] is that we use ridge regression [ 65,66] instead of LASSO [ 62,63,66]. Recall that LASSO
learns hypothesis functions of the form {h(x) =w·˜ϕ(x) :∥w∥1≤B}for some hyperparameter
B > 0. In contrast, ridge regression replaces the ℓ1-norm constraint ∥w∥1≤Bwith an ℓ2-norm
constraint: ∥w∥2≤Λ, for some hyperparameter Λ>0. Namely, for a chosen efficiently-computable
hyperparameter Λ>0, ridge regression finds a vector w∗that minimizes the training error subject to
the constraint that ∥w∥2≤Λ, i.e.,
min
w∈Rmϕ
∥w∥2≤Λ1
NNX
ℓ=1|w·˜ϕ(xℓ)−yℓ|2. (3.1)
Standard results in machine learning theory give sample complexity upper bounds for ridge regression
in terms of Λand the ℓ2-norm of the feature vector ˜ϕ(x)[65,66]. The key idea is that with our
new feature mapping, we can still approximate the ground state property by a linear function over
5the feature space, as in [ 2], to obtain a low training error. Meanwhile, by incorporating the Pauli
coefficients αPinto the feature map, we can bound the ℓ2-norm of ˜ϕ(x)by a quantity independent of
system size, leveraging bounds on the ℓ1-norm of the Pauli coefficients [ 2,67]. We note that naively
applying ridge regression with the feature map from [ 2] does not achieve the same guarantees and
in fact gives worse scaling than [ 2]. Similarly, we can also choose a suitable Λ>0independent of
system size. Thus, we obtain the following guarantee.
Theorem 4 (Constant sample complexity) .Given n, δ > 0,1/e > ϵ > 0and a training data set
{(xℓ, yℓ)}N
ℓ=1of size
N= log(1 /δ)2polylog(1 /ϵ), (3.2)
where xℓis sampled from an unknown distribution Dand|yℓ−tr(Oρ(xℓ))| ≤ϵ. With a proper
choice of the efficiently computable hyperparameters δ1, δ2,Λ, the learned function h∗(x)satisfies
E
x∼D|h∗(x)−tr(Oρ(x))|2≤ϵ (3.3)
with probability least 1−δ. The training and prediction time of the classical ML model are bounded
byO(n)polylog(1 /δ)2polylog(1 /ϵ).
We compare this result to Theorem 3. For a constant prediction error ϵ=O(1), our proposed
algorithm achieves a constant sample complexity N=O(1), compared to the logarithmic sample
complexity N=O(logn)of [2]. Moreover, we also improve the computational complexity,
achieving a linear-in- nruntime, compared to the previous O(nlogn). The scaling with respect to the
prediction error ϵis the same as the previous algorithm [ 2]. This means that regardless of how large
our quantum system is, we need the same amount of samples to predict ground state properties well.
This is especially important for settings in which obtaining training data for large systems is difficult.
Thus far, we have only considered the setting in which we learn a specific ground state property
tr(Oρ(x))for a fixed observable O. Because our training data is given in the form {(xℓ, yℓ)}N
ℓ=1,
where yℓapproximates tr(Oρ(x))for this fixed observable O, if we want to predict a new property
for the same ground state ρ(x), we would need to generate new training data. Thus, it may be
more useful to learn a ground state representation, from which we could predict tr(Oρ(x))for
many different choices of observables Owithout requiring new training data. In this case, suppose
we are instead given training data {xℓ, σT(ρ(xℓ))}N
ℓ=1, where σT(ρ(xℓ))is a classical shadow
representation [ 52,68,69,70,71] of the ground state ρ(xℓ). An immediate corollary of Theorem 4
is that we can predict ground state representations with the same sample complexity. This follows
from the same proof as Corollary 5 in [2].
Corollary 1 (Learning representations of ground states) .Letn, δ > 0,1/e > ϵ > 0andδ >0.
Given training data {(xℓ, σT(ρ(xℓ))}N
ℓ=1of size
N= log(1 /δ)2O(polylog(1 /ϵ)), (3.4)
where xℓis sampled from DandσT(ρ(xℓ)is the classical shadow representation of the ground state
ρ(xℓ)using Trandomized Pauli measurements. For T=˜O(log(n/δ)/ϵ2), with probability at least
1−δ, the ML algorithm will produce a ground state representation ˆρN,T(x)that achieves
E
x∼D|tr(OˆρN,T(x))−tr(Oρ(x))|2≤ϵ (3.5)
for any observable with ∥O∥∞≤1that can be written as a sum of geometrically local observables.
3.2 Rigorous guarantees for neural networks
In this section, we prove the existence of a deep neural network model that can predict ground
state properties using a constant number of training samples. In particular, we prove that after
training on a constant number of samples from a distribution Don[−1,1]msatisfying certain
technical assumptions, our model can achieve a low prediction error under mild assumptions on
training. In this case, for predicting properties tr(Oρ(x)), the observable Oneed not be known
in advance. However, we need to assume that all mixed first order derivatives of the Hamiltonian
∥∂mH(x)/∂x1···∂xm∥∞≤1exist and are bounded. This is not much stronger than [ 2], which
assumes that directional derivatives ∂hj/∂ˆuare bounded by one for any direction ˆu. Moreover, we
6also need the training data to be sampled from a distribution Dwith probability density function
gsatisfying the following assumptions: ghas full support and is continuously differentiable on
[−1,1]m. Also, gis of the form g(x) =QL
j=1gj(⃗ xj). This resembles our assumption on the form of
the Hamiltonian H(x). Furthermore, the average prediction error is measured with respect to the
same distribution D. We note that these assumptions are satisfied for common distributions such as
uniform and Gaussian.
As in the previous algorithm [ 2], we leverage the geometry of the n-qubit system to approximate the
ground state properties by a linear combination of smooth local functions, which only depend on
parameters with coordinates in the local coordinate set IPdefined in Equation (2.3). Crucially, the
size˜m≜|IP|of the domains of these local functions is independent of the system size.
Instead of using a feature map and linear regression to learn the ground state properties, we utilize a
deep neural network model defined as follows. Inspired by the local approximation of ground state
properties, we define “local models” fθP
P: [−1,1]˜m→R, which are neural networks consisting of
three layers of affine transformations and applications of a nonlinear activation function. In particular,
fθP
Phas two hidden layers with the affine transformations given by the trainable weights and biases
denoted by θP. We take hyperbolic tangent, tanh , as the activation function. These local models are
then combined into a model fΘ,w: [−1,1]m→Rgiven by
fΘ,w(x) =X
P∈S(geo)wPfθP
P(x), (3.6)
where wP∈Rare the weights in the last layer and Θ ={θP}P∈S(geo). This model is schematically
illustrated in Figure 1. We refer to Definition 6 in Appendix C for a full description of the model.
Consider training data {(xℓ, yℓ)}N
ℓ=1, where xℓare sampled according to a distribution Dsatisfying
the assumptions described above and |yℓ−tr(Oρ(xℓ))| ≤ϵ. The ML algorithm first initializes the
weights via standard deep learning initialization procedures, e.g., Xavier initialization [ 72]. Then,
the algorithm performs quasi-Monte Carlo training given the training data, e.g., Adam [ 73], to find
weights Θ∗, w∗which minimize the training objective function
1
NNX
ℓ=1|fΘ,w(xℓ)−yℓ|2+λ∥w∥1, (3.7)
where λis some regularization parameter that may depend on ϵ. For this algorithm, we prove the
following theorem bounding the average prediction error of our deep neural network model.
Theorem 5 (Neural network sample complexity guarantee) .Let1/e > ϵ > 0. LetDbe a distribution
with probability density function gsatisfying the properties stated above. Let fΘ∗,w∗: [−1,1]m→R
be a neural network model trained on data {(xℓ, yℓ)}N
ℓ=1of size
N=O
log(1 /δ)2polylog(1 /ϵ)
, (3.8)
where the xℓ’s are sampled from Dand|yℓ−tr(Oρ(xℓ))| ≤ϵ. Suppose that fΘ∗,w∗achieves a
value no larger than O(ϵ)on the training objective (Equation (3.7) ) with λ(ϵ) =O(ϵ). Additionally,
suppose that all parameters Θ∗
ioffΘ∗,w∗satisfy |Θ∗
i| ≤Wmax, for some Wmax>0that is
independent of n. Then
E
x∼D|fΘ∗,w∗(x)−tr(Oρ(x))|2≤ϵ. (3.9)
Similar to Theorem 4, for a constant prediction error ϵ=O(1), the deep neural network algorithm
achieves constant sample complexity N=O(1). In contrast to Theorem 4, we do not require
knowledge about the observable, O. This is a direct consequence of the regularity of wP, which
is achieved when the training objective is small. Theorem 6 guarantees, that a model with such
regularity can yield a small prediction error.
There are, however, some caveats compared to the previous result. First, the training data is restricted
to being sampled from a distribution satisfying our technical assumptions stated previously, in contrast
to Theorem 4 which holds for data sampled from any arbitrary unknown distribution. Second, in
regards to the model, the weights must be bounded by a constant Wmax. Finally, we cannot guarantee
7a priori that the network will indeed achieve a low training error. This is due to the fact that our
training objective is non-convex and thus, globally optimal weights cannot be found efficiently in
general [ 74]. Even so, we are still able to prove the existence of suitable weights such that the
resulting network approximates tr(Oρ(x))for any x∈[−1,1]m(see Theorem 6 in the next section).
However, we view the assumptions made in Theorem 5 as being mild in practice. Small training
objectives are commonly achieved in deep learning so we expect our training algorithm to produce
a model which fulfills the assumptions of Theorem 5 after O(1)training steps and O(n)runtime.
Moreover, it is known that gradient descent provably converges to the global optimum for over-
parametrized deep neural networks, while the weights remain small, when properly initialized [ 75].
We verify that these conditions are satisfied in practice through our numerical experiments in Figure 2
and??. To our knowledge, Theorem 5 is the first rigorous sample complexity bound on a neural
network model for predicting ground state properties.
We also note that if the training data is instead sampled according to a low-discrepancy sequence
(LDS) [ 55,56,76,77,78,57,79,58,80,81], we can obtain better guarantees, but these improvements
are hidden in the polylogarithmic factors in the exponential. We discuss learning given data from a
LDS in Appendix C. Intuitively, a LDS is a collection of points in the parameter space that covers the
space such that there are no large gaps, or discrepancies.
Similar to Corollary 1, if we are instead given training data {xℓ, σT(ρ(xℓ))}N
ℓ=1, where σT(ρ(xℓ))is
a classical shadow representation [ 52,68,69,70,71] of the ground state ρ(xℓ), then we obtain the
following immediate corollary of Theorem 5.
Corollary 2 (Learning representations of ground states with neural networks) .Letn, δ > 0,1/e >
ϵ >0andδ >0. Given training data {(xℓ, σT(ρ(xℓ))}N
ℓ=1of size
N=O
log(1 /δ)2polylog(1 /ϵ)
, (3.10)
where xℓis sampled from a distribution Dsatisfying the same assumptions as Theorem 5 and
σT(ρ(xℓ)is the classical shadow representation of the ground state ρ(xℓ)using Trandomized Pauli
measurements. For T=˜O(log(n/δ)/ϵ2), with probability at least 1−δ, the ML algorithm will
produce a ground state representation ˆρN,T(x)that achieves
E
x∼D|tr(OˆρN,T(x))−tr(Oρ(x))|2≤ϵ (3.11)
for any observable with ∥O∥∞≤1that can be written as a sum of geometrically local observables.
3.2.1 Proof ideas for neural network guarantee
To prove Theorem 5, we first show that our neural network model fΘ,wcan approximate the
ground state properties well. In particular, we show that there exist weights Θ′, w′such that fΘ′,w′
approximates the ground state properties and thus achieves small value for the training objective
(Equation (3.7)). Then, we bound the prediction error using tools from deep learning and quasi-Monte
Carlo theory [54, 55, 56, 57, 58]. We ensure the existence of fΘ′,w′in the following theorem.
Theorem 6. For any 1/e > ϵ > 0and width W, there exist weights Θ′, w′such that the neural
network model fΘ′,w′satisfies
|fΘ′,w′(x)−tr(Oρ(x))|2≤ϵ,∀x∈[−1,1]m. (3.12)
Moreover, each parameter Θiof the network has a magnitude of |Θi|= 2O(polylog(1 /ϵ)).
This implies that for a suitable choice of regularization parameter λ=O(ϵ), the training objective
from Equation (3.7) is also small. We prove this statement by combining results in deep learning
regarding tanh neural networks approximating functions [ 53] with the geometric locality of the
system and smoothness of the ground state properties. We note that the weights Θ′, w′in Theorem 6
are not necessarily the weights Θ∗, w∗found via the neural network training procedure in Theorem 5.
Because the training objective is non-convex, we cannot guarantee convergence to these weights
Θ′, w′. However, assuming that fΘ∗,w∗does indeed achieve a low training error (which is often
satisfied in practice), we are able to rigorously guarantee that the model will generalize well and
achieve a low prediction error in Theorem 5.
8Notice that the guarantee of Theorem 6 holds for all xand, in particular, does not require our
assumptions on the distribution D. The assumption that the network is trained on such data only
becomes relevant when bounding the prediction error. While not explicitly stated here, we also
note that Theorem 6 gives a bound on the number of trainable parameters |Θi|that has a similar
dependence on ϵas the model in [ 2]. Furthermore, the parameters are independent of system size, n.
Additional smoothness assumptions on the Hamiltonian H(x)can yield mild improvements on the
dependence in terms of ϵ, as briefly discussed in Appendix C.1. Moreover, because of this bound on
|Θi|, applying an additional penalty on the ℓ2-norm of the weights Θcan help ensure that the weights
remain small. In practice, this is usually satisfied during training when the weights are initialized
properly and the inputs are regularized, e.g. [ 53]. Thus, the condition that |Θ∗
i| ≤Wmaxis often
satisfied in practice and is not considered a strong assumption in deep learning.
To prove the prediction error bound in Theorem 5 assuming that a low training error is achieved, we
combine techniques from quasi-Monte Carlo theory applied to deep learning [ 54] (see Appendix A.2
for a review) along with our knowledge of the geometry of the n-qubit system. In contrast to [ 54], we
need to characterize the dimension of the input domain in our approach. The reason for doing this is
that the approximation error depends on the size ˜m=|IP|(Equation (2.3)) of our local models fθP
P.
The central result we use here is the Koksma-Hlawka inequality [ 56] (see Theorem 10 in Ap-
pendix A.2) from quasi-Monte Carlo theory. This produces a bound on the prediction error in terms
of the star-discrepancy (see Definition 2 in Appendix A.2) and the Hardy-Krause variation. The
star-discrepancy can be controlled by known bounds on the star-discrepancy of random points [ 82].
We bound the Hardy-Krause variation by explicitly computing the mixed derivatives of the local
models fθP
Pand the ground state properties tr(Oρ(x)). In particular, we bound the latter using tools
from the spectral flow formalism [ 59,60,61], and this is where the assumption that the mixed first
order derivatives of the Hamiltonian are bounded is needed. Putting these steps together, we arrive at
the rigorous guarantee in Theorem 5.
4 Numerical experiments
We conduct numerical experiments to observe the performance of our model in practice. The
results demonstrate that our assumptions in Theorem 5 are often satisfied in practice and that our
deep learning algorithm outperforms the previous best-known method [ 2]. Moreover, we generate
and utilize significantly more training data than in prior works [ 1,2]. The code can be found at
https://github.com/marcwannerchalmers/learning_ground_states.git .
We consider the classical neural network model discussed in the previous section and defined formally
in Definition 6. For each of the local models fθP
P, we use fully connected deep neural networks with
five hidden layers of width 200. We train the model with the AdamW optimization algorithm [ 83].
We measure the training error and prediction error via the root-mean-square error (RMSE). The
model is discussed further in Appendix D.
As in [ 2], we consider the two-dimensional antiferromagnetic random Heisenberg model on between
20to45qubits and predict two-body correlation functions. The corresponding Hamiltonian is
H=X
⟨ij⟩Jij(XiXj+YiYj+ZiZj), (4.1)
where ⟨ij⟩denotes all pairs of neighboring sites on the lattice. The coupling terms Jijcorrespond to
the parameters xof the Hamiltonian and are sampled uniformly from [0,2].
We generate training data similarly to [ 1,2], using the density-matrix renormalization group
(DMRG) [ 9] based on matrix-product-states (MPS) [ 84]. To assess the performance of our model, we
consider both uniformly randomly distributed Jijand coupling parameters, which are distributed as a
Sobol sequence. It is easy to see that the distributions and Hsatisfy the requirements of Theorem 5.
In Figure 2 (Left), we see that our deep learning algorithm consistently outperforms the previous
best-known ML algorithm from [ 2], achieving approximately half the prediction error on the same
training data. The prediction error also exhibits a constant scaling with respect to system size,
agreeing with our rigorous guarantee in Theorem 5. Another noteworthy observation is that the ML
algorithm’s performance on LDS is nearly equivalent to its performance on uniformly random points.
We discuss a potential reason for this in Appendix D.
94x5 5x5 6x5 7x5 8x5 9x5
System size (n)0.000.010.020.030.040.050.060.070.08Average prediction error
Algorithm
Deep Learning, LDS
Regression, LDS
Deep Learning, Random
Regression, Random
409 1228 2048 2867 3686
Training set size (N)0.000.010.020.030.040.050.060.070.08Average prediction error
δ1
0
1
2
3
4
5
4x5 5x5 6x5 7x5 8x5 9x5
System size (n)0.000.010.020.030.040.050.060.070.08Average training error
Metric
Training error
/bardblw/bardbl1
/bardblΘ/bardbl∞
0.000.250.500.751.001.251.501.75
Average norm
Figure 2: Numerical experiments. (Left) Comparison with previous methods. Each point indicates
the prediction error (RMSE) of our deep learning model or the regression model of [ 2], fixing the
training set size N= 3686 and the size of the local neighorbood δ1= 0(Equation (2.3)). We train
both algorithms on either LDS or uniformly random points. (Center) Scaling with training size. Each
point indicates the prediction error of our deep learning model given LDS training data for various δ1
and training data sizes. (Right) Neural network weights and training error. Blue points correspond to
the training error of the neural network model. Red points correspond to the ℓ1norm of parameters in
the last layer or the largest absolute value of the parameters of the neural network, fixing N= 3686
andδ1= 1. This shows that the assumptions in Theorem 5 are achieved in practice. The shaded
areas denote the 1-sigma error bars across the assessed ground state properties.
Figure 2 (Center) illustrates the prediction error scaling with respect to the training set size for various
choices of δ1(size of the local neighborhood from Equation (2.3)). For δ1= 0, the error arising from
approximating the ground state property via local functions dominates. For δ1>0, we observe a
smaller local approximation error and thus achieve a smaller prediction error for sufficiently large
training sets. This is consistent with our theoretical results.
Finally, Figure 2 (Right) illustrates that our assumptions in Theorem 5 are mild in practice. Namely,
the blue points show that a small training error can be achieved. The red points also demonstrate
that the ℓ1-norm of the parameters in the last layer and the largest absolute value of the parmeters
in the trained neural network remain small. In particular, in Figure 2, the weights exhibit a scaling
independent of system size n. Hence, we find that the assumptions needed to guarantee the prediction
error bound in Theorem 5, namely that the training objective is small and the weights of the neural
network are small and independent of system size, are fulfilled in our numerical experiments. We
provide further details of the numerical experiments in Appendix D.
5 Discussion
We have shown that we can construct ML models for predicting ground state properties that require
only a constant number of training samples, for a fixed prediction error. Specifically, we showed
that a simple modification to the linear regression model in [ 2] only requires 2polylog (ϵ−1)samples in
order to achieve a prediction error of ϵ, provided that we know a decomposition of the observable of
interest in terms of Pauli operators. We then showed that a neural network model which is trained on
2polylog (ϵ−1)training samples and which achieves O(ϵ)training error on these samples will also have
a prediction error of at most ϵ.In this case, knowledge of the observable Ois no longer required.
Our work leaves open several avenues for future exploration. First, it would be desirable to understand
the conditions under which we can prove convergence for the training error. For instance, could
the model be changed so as to use a convex objective, thereby avoiding the issues associated with
finding a global optimum in a non-convex landscape? Following [ 49,50], we would also like to know
whether the results obtained for neural networks can be extended to thermal states or Lindbladian
phases of matter. Finally, for both results it would be desirable to improve the scaling with respect to
the error ϵ. Currently, the models have quasipolynomial scaling in 1/ϵand the only case in which we
know how to achieve poly(1/ϵ)scaling is when the number of parameters, m,is constant (as in [ 51]).
10Acknowledgements
MW and DD are supported by SSF (Swedish Foundation for Strategic Research), grant number
FUS21-0063. LL is supported by a Marshall Scholarship. CB is partially supported by a grant from
DIA-COE. AG is supported by the Knut and Alice Wallenberg Foundation through the Wallenberg
Centre for Quantum Technology (WACQT). This work was done in part while a subset of the authors
were visiting the Simons Institute for the Theory of Computing.
References
[1]Hsin-Yuan Huang, Richard Kueng, Giacomo Torlai, Victor V Albert, and John Preskill. Provably
efficient machine learning for quantum many-body problems. Science , 377(6613):eabk3333,
2022.
[2]Laura Lewis, Hsin-Yuan Huang, Viet T Tran, Sebastian Lehner, Richard Kueng, and John
Preskill. Improved machine learning algorithm for predicting ground state properties. Nature
Communications , 15(1):895, 2024.
[3] P. Hohenberg and W. Kohn. Inhomogeneous electron gas. Phys. Rev. , 136:B864–B871, 1964.
[4]W. Kohn. Nobel lecture: Electronic structure of matter—wave functions and density functionals.
Rev. Mod. Phys. , 71:1253–1266, 1999.
[5]Anders W. Sandvik. Stochastic series expansion method with operator-loop update. Phys. Rev.
B, 59:R14157–R14160, 1999.
[6] David Ceperley and Berni Alder. Quantum Monte Carlo. Science , 231(4738):555–560, 1986.
[7]Federico Becca and Sandro Sorella. Quantum Monte Carlo Approaches for Correlated Systems .
Cambridge University Press, 2017.
[8]James Gubernatis, Naoki Kawashima, and Philipp Werner. Quantum Monte Carlo Methods .
Cambridge University Press, 2016.
[9]Steven R White. Density matrix formulation for quantum renormalization groups. Physical
review letters , 69(19):2863, 1992.
[10] Steven R White. Density-matrix algorithms for quantum renormalization groups. Phys. Rev. B ,
48(14):10345, 1993.
[11] Guifré Vidal. Class of quantum many-body states that can be efficiently simulated. Physical
review letters , 101(11):110501, 2008.
[12] Alberto Peruzzo, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J
Love, Alán Aspuru-Guzik, and Jeremy L O’brien. A variational eigenvalue solver on a photonic
quantum processor. Nat. Commun. , 5:4213, 2014.
[13] J Ignacio Cirac, David Perez-Garcia, Norbert Schuch, and Frank Verstraete. Matrix product
states and projected entangled pair states: Concepts, symmetries, theorems. Reviews of Modern
Physics , 93(4):045003, 2021.
[14] Toby S Cubitt. Dissipative ground state preparation and the dissipative quantum eigensolver.
arXiv preprint arXiv:2303.11962 , 2023.
[15] Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby,
Leslie V ogt-Maranto, and Lenka Zdeborová. Machine learning and the physical sciences. Rev.
Mod. Phys. , 91:045002, 2019.
[16] Juan Carrasquilla. Machine learning for quantum matter. Adv. Phys.: X , 5(1):1797528, 2020.
[17] Dong-Ling Deng, Xiaopeng Li, and S. Das Sarma. Machine learning topological states. Phys.
Rev. B , 96:195145, 2017.
11[18] Juan Carrasquilla and Roger G. Melko. Machine learning phases of matter. Nat. Phys. , 13:431,
2017.
[19] Giuseppe Carleo and Matthias Troyer. Solving the quantum many-body problem with artificial
neural networks. Science , 355(6325):602–606, 2017.
[20] Giacomo Torlai and Roger G. Melko. Learning thermodynamics with Boltzmann machines.
Physical Review B , 94(16):165134, 2016.
[21] Yusuke Nomura, Andrew S. Darmawan, Youhei Yamaji, and Masatoshi Imada. Restricted
boltzmann machine learning for solving strongly correlated quantum systems. Phys. Rev. B ,
96:205152, 2017.
[22] Evert P. L. van Nieuwenburg, Ye-Hua Liu, and Sebastian D. Huber. Learning phase transitions
by confusion. Nat. Phys. , 13:435, 2017.
[23] Lei Wang. Discovering phase transitions with unsupervised learning. Phys. Rev. B , 94:195105,
2016.
[24] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. arXiv preprint arXiv:1704.01212 , 2017.
[25] Giacomo Torlai, Guglielmo Mazzola, Juan Carrasquilla, Matthias Troyer, Roger Melko, and
Giuseppe Carleo. Neural-network quantum state tomography. Nat. Phys. , 14(5):447–450, 2018.
[26] Rodrigo A Vargas-Hernández, John Sous, Mona Berciu, and Roman V Krems. Extrapolating
quantum observables with machine learning: inferring multiple phase transitions from properties
of a single phase. Physical review letters , 121(25):255702, 2018.
[27] KT Schütt, Michael Gastegger, Alexandre Tkatchenko, K-R Müller, and Reinhard J Maurer.
Unifying machine learning and quantum chemistry with a deep neural network for molecular
wavefunctions. Nat. Commun. , 10(1):1–10, 2019.
[28] Ivan Glasser, Nicola Pancotti, Moritz August, Ivan D. Rodriguez, and J. Ignacio Cirac. Neural-
network quantum states, string-bond states, and chiral topological states. Phys. Rev. X , 8:011006,
2018.
[29] Matthias C Caro, Hsin-Yuan Huang, Nicholas Ezzell, Joe Gibbs, Andrew T Sornborger, Lukasz
Cincio, Patrick J Coles, and Zoë Holmes. Out-of-distribution generalization for learning
quantum dynamics. arXiv preprint arXiv:2204.10268 , 2022.
[30] Joaquin F Rodriguez-Nieva and Mathias S Scheurer. Identifying topological order through
unsupervised machine learning. Nat. Phys. , 15(8):790–795, 2019.
[31] Zhuoran Qiao, Matthew Welborn, Animashree Anandkumar, Frederick R Manby, and Thomas F
Miller III. Orbnet: Deep learning for quantum chemistry using symmetry-adapted atomic-orbital
features. J. Chem. Phys. , 153(12):124111, 2020.
[32] Kenny Choo, Antonio Mezzacapo, and Giuseppe Carleo. Fermionic neural-network states for
ab-initio electronic structure. Nat. Commun. , 11(1):2368, May 2020.
[33] Hiroki Kawai and Yuya O Nakagawa. Predicting excited states from ground state wavefunc-
tion by supervised quantum machine learning. Machine Learning: Science and Technology ,
1(4):045027, 2020.
[34] Javier Robledo Moreno, Giuseppe Carleo, and Antoine Georges. Deep learning the hohenberg-
kohn maps of density functional theory. Physical Review Letters , 125(7):076402, 2020.
[35] Korbinian Kottmann, Philippe Corboz, Maciej Lewenstein, and Antonio Acín. Unsupervised
mapping of phase diagrams of 2d systems from infinite projected entangled-pair states via deep
anomaly detection. SciPost Physics , 11(2):025, 2021.
[36] Haoxiang Wang, Maurice Weber, Josh Izaac, and Cedric Yen-Yu Lin. Predicting properties of
quantum systems with conditional generative models. arXiv preprint arXiv:2211.16943 , 2022.
12[37] Viet T Tran, Laura Lewis, Hsin-Yuan Huang, Johannes Kofler, Richard Kueng, Sepp Hochreiter,
and Sebastian Lehner. Using shadows to learn ground state properties of quantum hamiltoni-
ans. Machine Learning and Physical Sciences Workshop at the 36th Conference on Neural
Information Processing Systems (NeurIPS) , 2022.
[38] Kyle Mills, Michael Spanner, and Isaac Tamblyn. Deep learning and the schrödinger equation.
Phys. Rev. A , 96:042113, Oct 2017.
[39] N Saraceni, S Cantori, and S Pilati. Scalable neural networks for the efficient learning of
disordered quantum systems. Physical Review E , 102(3):033301, 2020.
[40] Cancan Huang and Brenda M Rubenstein. Machine learning diffusion monte carlo forces. The
Journal of Physical Chemistry A , 127(1):339–355, 2022.
[41] Matthias Rupp, Alexandre Tkatchenko, Klaus-Robert Müller, and O Anatole V on Lilienfeld.
Fast and accurate modeling of molecular atomization energies with machine learning. Physical
review letters , 108(5):058301, 2012.
[42] Felix A Faber, Luke Hutchison, Bing Huang, Justin Gilmer, Samuel S Schoenholz, George E
Dahl, Oriol Vinyals, Steven Kearnes, Patrick F Riley, and O Anatole V on Lilienfeld. Prediction
errors of molecular machine learning models lower than hybrid dft error. Journal of chemical
theory and computation , 13(11):5255–5264, 2017.
[43] Benno S Rem, Niklas Käming, Matthias Tarnowski, Luca Asteria, Nick Fläschner, Christoph
Becker, Klaus Sengstock, and Christof Weitenberg. Identifying quantum phase transitions using
artificial neural networks on experimental data. Nature Physics , 15(9):917–920, 2019.
[44] Xiao-Yu Dong, Frank Pollmann, Xue-Feng Zhang, et al. Machine learning of quantum phase
transitions. Physical Review B , 99(12):121104, 2019.
[45] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth
Lloyd. Quantum machine learning. Nature , 549(7671):195–202, 2017.
[46] Luuk Coopmans and Marcello Benedetti. On the sample complexity of quantum boltzmann
machine learning. arXiv preprint arXiv:2306.14969 , 2023.
[47] Julia Kempe, Alexei Kitaev, and Oded Regev. The complexity of the local hamiltonian problem.
Siam journal on computing , 35(5):1070–1097, 2006.
[48] Cambyse Rouzé, Daniel Stilck França, Emilio Onorati, and James D. Watson. Efficient learning
of ground and thermal states within phases of matter. Nature Communications , 15(1), September
2024.
[49] Emilio Onorati, Cambyse Rouzé, Daniel Stilck França, and James D Watson. Provably efficient
learning of phases of matter via dissipative evolutions. arXiv preprint arXiv:2311.07506 , 2023.
[50] Andrea Coser and David Pérez-García. Classification of phases for mixed states via fast
dissipative evolution. Quantum , 3:174, 2019.
[51] Yanming Che, Clemens Gneiting, and Franco Nori. Exponentially improved efficient ma-
chine learning for quantum many-body states with provable guarantees. arXiv preprint
arXiv:2304.04353 , 2023.
[52] Hsin-Yuan Huang, Richard Kueng, and John Preskill. Predicting many properties of a quantum
system from very few measurements. Nat. Phys. , 16:1050––1057, 2020.
[53] Tim De Ryck, Samuel Lanthaler, and Siddhartha Mishra. On the approximation of functions by
tanh neural networks. Neural Networks , 143:732–750, November 2021.
[54] Siddhartha Mishra and T. Konstantin Rusch. Enhancing accuracy of deep learning algorithms
by training with low-discrepancy sequences. SIAM Journal on Numerical Analysis , 59(3):1811–
1834, 2021.
[55] Stanislaw K Zaremba. The mathematical basis of monte carlo and quasi-monte carlo methods.
SIAM review , 10(3):303–314, 1968.
13[56] Russel E Caflisch. Monte carlo and quasi-monte carlo methods. Acta numerica , 7:1–49, 1998.
[57] Harald Niederreiter. Random number generation and quasi-Monte Carlo methods . SIAM, 1992.
[58] Pierre L’Ecuyer and Christiane Lemieux. Recent advances in randomized quasi-monte carlo
methods. Modeling uncertainty: An examination of stochastic theory, methods, and applications ,
pages 419–474, 2002.
[59] Sven Bachmann, Spyridon Michalakis, Bruno Nachtergaele, and Robert Sims. Automor-
phic equivalence within gapped phases of quantum lattice systems. Commun. Math. Phys. ,
309(3):835–871, 2012.
[60] Matthew B Hastings and Xiao-Gang Wen. Quasiadiabatic continuation of quantum states: The
stability of topological ground-state degeneracy and emergent gauge invariance. Phys. Rev. B ,
72(4):045141, 2005.
[61] Tobias J Osborne. Simulating adiabatic evolution of gapped spin systems. Phys. Rev. A ,
75(3):032321, 2007.
[62] Fadil Santosa and William W. Symes. Linear inversion of band-limited reflection seismograms.
SIAM Journal on Scientific and Statistical Computing , 7(4):1307–1330, 1986.
[63] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society: Series B (Methodological) , 58(1):267–288, 1996.
[64] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning .
The MIT Press, 2018.
[65] Craig Saunders, Alexander Gammerman, and V olodya V ovk. Ridge regression learning algo-
rithm in dual variables. In Proceedings of the Fifteenth International Conference on Machine
Learning , pages 515–521, 1998.
[66] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms . Cambridge university press, 2014.
[67] Hsin-Yuan Huang, Sitan Chen, and John Preskill. Learning to predict arbitrary quantum
processes. PRX Quantum , 4(4):040337, 2023.
[68] Andreas Elben, Richard Kueng, Hsin-Yuan Huang, Rick van Bijnen, Christian Kokail, Marcello
Dalmonte, Pasquale Calabrese, Barbara Kraus, John Preskill, Peter Zoller, and Benoît Vermersch.
Mixed-state entanglement from local randomized measurements. Phys. Rev. Lett. , 125:200501,
2020.
[69] Andreas Elben, Steven T Flammia, Hsin-Yuan Huang, Richard Kueng, John Preskill, Benoît
Vermersch, and Peter Zoller. The randomized measurement toolbox. arXiv preprint
arXiv:2203.11374 , 2022.
[70] Kianna Wan, William J Huggins, Joonho Lee, and Ryan Babbush. Matchgate shadows for
fermionic quantum simulation. arXiv preprint arXiv:2207.13723 , 2022.
[71] Kaifeng Bu, Dax Enshan Koh, Roy J Garcia, and Arthur Jaffe. Classical shadows with pauli-
invariant unitary ensembles. arXiv preprint arXiv:2202.03272 , 2022.
[72] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedfor-
ward neural networks. In Proceedings of the thirteenth international conference on artificial
intelligence and statistics , pages 249–256. JMLR Workshop and Conference Proceedings, 2010.
[73] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[74] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
[75] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International conference on machine learning , pages
1675–1685. PMLR, 2019.
14[76] Ilya M Sobol. Uniformly distributed sequences with an additional uniform property. USSR
Computational mathematics and mathematical physics , 16(5):236–242, 1976.
[77] Il’ya Meerovich Sobol’. On the distribution of points in a cube and the approximate evaluation
of integrals. Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki , 7(4):784–802, 1967.
[78] Harald Niederreiter. Point sets and sequences with small discrepancy. Monatshefte für Mathe-
matik , 104:273–337, 1987.
[79] Harald Niederreiter. Low-discrepancy and low-dispersion sequences. Journal of number theory ,
30(1):51–70, 1988.
[80] John H Halton. On the efficiency of certain quasi-random sequences of points in evaluating
multi-dimensional integrals. Numerische Mathematik , 2:84–90, 1960.
[81] Art B Owen. Monte carlo variance of scrambled net quadrature. SIAM Journal on Numerical
Analysis , 34(5):1884–1910, 1997.
[82] Christoph Aistleitner and Markus Hofer. Probabilistic discrepancy bound for monte carlo point
sets, 2012.
[83] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.
[84] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Mach. Learn. , 20(3):273–297,
1995.
[85] Kjetil O Lye, Siddhartha Mishra, and Deep Ray. Deep learning observables in computational
fluid dynamics. Journal of Computational Physics , 410:109339, 2020.
[86] Art B Owen. Multidimensional variation for quasi-monte carlo. In Contemporary Multivariate
Analysis And Design Of Experiments: In Celebration of Professor Kai-Tai Fang’s 65th Birthday ,
pages 49–74. World Scientific, 2005.
[87] Christoph Aistleitner and Josef Dick. Functions of bounded variation, signed measures, and a
general koksma-hlawka inequality, 2014.
[88] E. Hlawka and R. Mück. Über eine transformation von gleichverteilten folgen ii. Computing ,
9(2):127–138, Jun 1972.
[89] Mohammad Mahdi Bejani and Mehdi Ghatee. A systematic review on overfitting control in
shallow and deep neural networks. Artificial Intelligence Review , pages 1–48, 2021.
[90] George Casella and Roger L Berger. Statistical lnference. Duxbury press , 2002.
[91] Murray Rosenblatt. Remarks on a multivariate transformation. The Annals of Mathematical
Statistics , 23(3):470–472, 1952.
[92] Mei Zhang, Aijun Zhang, and Yongdao Zhou. Construction of Uniform Designs on Arbitrary
Domains by Inverse Rosenblatt Transformation , pages 111–126. 05 2020.
[93] 2. Quasi-Monte Carlo Methods for Numerical Integration , pages 13–22.
[94] Ohad Shamir. A variant of azuma’s inequality for martingales with subgaussian tails. arXiv
preprint arXiv:1110.2392 , 2011.
[95] Howard E Haber. Notes on the matrix exponential and logarithm. Santa Cruz Institute for
Particle Physics, University of California: Santa Cruz, CA, USA , 2018.
[96] Steven R. White. Density matrix formulation for quantum renormalization groups. Phys. Rev.
Lett., 69:2863–2866, 1992.
[97] De Huang, Jonathan Niles-Weed, Joel A Tropp, and Rachel Ward. Matrix concentration for
products. arXiv preprint arXiv:2003.05437 , 2020.
15Appendices
Contents
A Preliminaries 16
A.1 Review of previous algorithm and proof . . . . . . . . . . . . . . . . . . . . . . . 16
A.1.1 ML Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.1.2 Proof Ideas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.2 Deep learning with low-discrepancy sequences . . . . . . . . . . . . . . . . . . . 20
B Constant sample complexity 23
B.1 Training error bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.2 Prediction error bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.3 Computational time for training and prediction . . . . . . . . . . . . . . . . . . . 27
C Rigorous guarantees for neural networks 27
C.1 Approximation of ground state properties by neural networks . . . . . . . . . . . . 30
C.2 Prediction error bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
C.3 Prediction on general distributions . . . . . . . . . . . . . . . . . . . . . . . . . . 39
C.4 Bound on the mixed derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
D Details of numerical experiments 53
D.1 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
D.2 Additional experiments and discussion . . . . . . . . . . . . . . . . . . . . . . . . 54
D.3 Experiments with non-geometrically-local Hamiltonians . . . . . . . . . . . . . . 56
These appendices provide the technical details of the ideas discussed in the main text. In Appendix A,
we review several important concepts for our proofs such as the algorithm and rigorous guarantee
from [ 2] in Appendix A.1 and background on classical deep learning techniques in Appendix A.2. In
Appendix B, we build on [ 2] to obtain a sample complexity upper bound for predicting ground state
properties independent of system size. In Appendix C, we prove our guarantee for predicting ground
state properties using neural networks.
A Preliminaries
A.1 Review of previous algorithm and proof
In this section, we review the previous algorithm from [ 2] along with intermediate results we use
throughout our proofs. For full details, we refer the reader to [ 2]. Throughout this section, let
1/e > ϵ 1, ϵ2, ϵ3>0. One can think of ϵ1as the approximation error caused by the hypothesis of our
ML algorithm not exactly capturing the ground state property; ϵ2represents the noise in the training
data; ϵ3corresponds to the generalization error.
Recall that we consider a family of n-qubit Hamiltonians H(x)smoothly parameterized by an
m-dimensional vector x∈[−1,1]mthat satisfies the following assumptions, which we restate
from [2].
16(a)Physical system: We consider nfinite-dimensional quantum systems that are arranged
at locations, or sites, in a d-dimensional space, e.g., a spin chain ( d= 1), a square lattice
(d= 2), or a cubic lattice ( d= 3). Unless specified otherwise, our big- O,Ω,Θnotation is
with respect to the thermodynamic limit n→ ∞ .
(b)Hamiltonian: H(x)decomposes into a sum of geometrically local terms H(x) =PL
j=1hj(⃗ xj), each of which only acts on an O(1)number of sites in a ball of O(1)
radius. Here, ⃗ xj∈Rq, q=O(1)andxis the concatenation of Lvectors ⃗ x1, . . . , ⃗ x Lwith
dimension m=Lq=O(n). Individual terms hj(⃗ xj)obey∥hj(⃗ xj)∥∞≤1and also have
bounded directional derivative: ∥∂hj/∂ˆu∥∞≤1, where ˆuis a unit vector in parameter
space.
(c)Ground-state subspace: We consider the ground state ρ(x)for the Hamiltonian H(x)to be
defined as ρ(x) = lim β→∞e−βH(x)/tr 
e−βH(x)
. This is equivalent to a uniform mixture
over the eigenspace of H(x)with the minimum eigenvalue.
(d)Observable: Ocan be written as a sum of few-body observables O=P
jOj, where each
Ojonly acts on an O(1)number of sites. Hence, we can also write O=P
P∈S(geo)αPP,
where P∈ {I, X, Y, Z }⊗nandS(geo)is the set of all geometrically local Pauli observables.
We focus on Ogiven as a sum of geometrically local observablesP
jOj, where each Ojonly
acts on an O(1)number of sites in a ball of O(1)radius. Moreover, Ohas∥O∥∞=O(1).
We also assume that the spectral gap of H(x)is bounded from below by some constant γfor all
choices of parameters x∈[−1,1]m.
The ML algorithm is given a training data set {(xℓ, yℓ)}N
ℓ=1, where xℓis sampled from some
distribution Dover the parameter space [−1,1]mandyℓapproximates the ground state property:
|yℓ−tr(Oρ(xℓ))| ≤ϵ2. The goal is to learn some function h∗(x)that achieves a low average
prediction error
E
x∼D|h∗(x)−tr(Oρ(x))|2≤ϵ. (A.1)
A.1.1 ML Algorithm
The ML algorithm proposed in [ 2] requires several geometric definitions. We use S(geo)to denote
the set of all geometrically local Pauli observables throughout.
Letδ1, δ2, B > 0be efficiently-computable hyperparameters that we define later. Then, define
the set IPof coordinates cthat parameterize some local term hj(c)that is close to a Pauli P∈
{I, X, Y, Z }⊗n. Here, the distance between two observables dobsis defined as the minimum distance
between the qubits that the observables act on, where the distance between qubits is given by
the geometry of the system, which we assume to be known. Formally, we define the set of local
coordinates as
IP≜{c∈ {1, . . . , m }:dobs(hj(c), P)≤δ1}, (A.2)
where hj(c)is the local term in the Hamiltonian H(x)whose parameters ⃗ xj(c)include the variable
xc. The intuition behind this set of coordinates is that it indexes the parameters xcthat influence
the ground state property tr(Pρ(x))corresponding to this Pauli P. Using this intuition, because
these parameters xcforc∈IPmatter most for the property we are trying to learn (as [ 2] proves and
we give the ideas for later), then we can define a new effective parameter space in which all other
parameters are set to zero. Moreover, parameters xcforc∈IPcan be discretized to lie on a lattice.
This gives the following set XP
XP≜x∈[−1,1]m:ifc /∈IP, xc= 0
ifc∈IP, xc∈ {0,±δ2,±2δ2, . . . ,±1}
. (A.3)
We can also define a set Tx,Pfor each vector x∈XPwhich is the set of parameters x′that are close
toxfor coordinates in IP:
Tx,P≜
x′∈[−1,1]m:−δ2
2< xc−x′
c≤δ2
2,∀c∈IP
. (A.4)
With these definitions in place, we set the hyperparameters as follows. Define δ1as
δ1≜max
Cmaxlog2(2C/ϵ1), C4, C5,max(5900 , α,7(d+ 11) , θ)
b
, (A.5)
17where b, Cmax, C4, C5, α, θ, C are all constants. We refer to the supplementary information of [ 2]
for a full description of these constants. Moreover, δ2is given by
δ2≜1
2√
C′|IP|
ϵ1, (A.6)
where C′is a constant. Finally, we define an additional hyperparameter B >0as
B≜2O(polylog(1 /ϵ1)). (A.7)
The ML algorithm from [ 2] utilizes these objects to encode the geometric locality of the system. The
algorithm consists of two steps. First, it maps the parameter space [−1,1]mto a high dimensional
space Rmϕfor
mϕ≜X
P∈S(geo)|XP|=O(n)2O(polylog(1 /ϵ1))(A.8)
via a nonlinear feature map ϕ. Second, it runs ℓ1-regularized linear regression (LASSO) over the
feature space.
This first step encodes the geometry of the problem. In particular, the feature map is defined as
follows, where each coordinate of ϕ(x)is indexed by x′∈XPandP∈S(geo)
ϕ(x)x′,P≜1[x∈Tx′,P]. (A.9)
In this way, the feature map ϕ(x)identifies the nearest lattice point to x. The idea is that one can
approximate the ground state property well by only approximating it at these representative points
and summing up. We make this intuition rigorous in the following section.
Following the feature mapping, our ML algorithm uses LASSO [ 62,63,64] to learn functions of the
form{h(x) =w·ϕ(x) :∥w∥1≤B}. In particular, for a chosen hyperparameter B > 0, LASSO
finds a coefficient vector w∗that solves the following optimization problem minimizing the training
error subject to the constraint that ∥w∥1≤B
min
w∈Rmϕ
∥w∥1≤B1
NNX
ℓ=1|w·ϕ(xℓ)−yℓ|2. (A.10)
We denote the learned function by h∗(x) =w∗·ϕ(x). Note that the learned function does not need
to achieve the minimum training error, but can be some amount ϵ3/2above it. For our purposes, we
setB= 2O(polylog(1 /ϵ1)).
This algorithm obtains the following rigorous guarantee.
Theorem 7 (Theorem 5 in [ 2]).Let1/e > ϵ 1, ϵ2, ϵ3>0andδ > 0. Given training data
{(xℓ, yℓ)}N
ℓ=1of size
N= log( n/δ)2O(log(1 /ϵ3)+polylog(1 /ϵ1)), (A.11)
where xℓis sampled from Dandyℓis an estimator of tr(Oρ(xℓ))such that |yℓ−tr(Oρ(xℓ))| ≤ϵ2,
the ML algorithm can produce h∗(x)that achieves prediction error
E
x∼D|h∗(x)−tr(Oρ(x))|2≤(ϵ1+ϵ2)2+ϵ3 (A.12)
with probability at least 1−δ. The training time for constructing the hypothesis func-
tion hand the prediction time for computing h∗(x)are upper bounded by O(nN) =
nlog(n/δ)2O(log(1 /ϵ3)+polylog(1 /ϵ1)).
A.1.2 Proof Ideas
This rigorous guarantee is proven by first showing that the training error
ˆR(h) = min
w1
NNX
ℓ=1|h(xℓ)−yℓ|2(A.13)
is small.
18Lemma 1 (Lemma 15 in [2]) .The function
g(x) =X
P∈S(geo)X
x′∈XPfP(x′)1[x∈Tx′,P] =w′·ϕ(x), (A.14)
achieves training error
ˆR(g)≤(ϵ1+ϵ2)2. (A.15)
The proof of this consists of three different steps. First, one can show that tr(Oρ(x))can be
approximated by a sum of smooth local functions, denoted as f(x) =P
P∈S(geo)fP(x), where
fP(x) =αPtr(Pρ(χP(x)))forO=P
P∈{I,X,Y,Z }⊗nαPPand
χP(x)c=xc, c∈IP
0 c /∈IP(A.16)
for all c∈ {1, . . . , m }. In other words, parameters that parameterize local terms hjfar away a Pauli
P(xcforc /∈IP) do not contribute much to the ground state property, and thus we can simply set
them to zero. Formally, this approximation is given in the following lemma.
Lemma 2 (Corollary 2 in [ 2]).Consider a class of local Hamiltonians {H(x) :x∈[−1,1]m}and
an observable O=P
P∈{I,X,Y,Z }⊗nαPPsatisfying assumptions (a)-(d). There exists a constant
C >0such that for any 1/e > ϵ 1>0,
|tr(Oρ(x))−f(x)| ≤Cϵ1
X
P∈S(geo)|αP|
, (A.17)
where f(x) =P
P∈S(geo)fP(x).
Second, one can also show that this sum of local functions f(x) =P
P∈S(geo)fP(x)can in turn be
approximated by a linear function over the feature space g(x) =w′·ϕ(x), where w′is a vector with
entries indexed by P∈S(geo)andx′∈XPgiven by w′
x′,P=fP(x′).
Lemma 3 (Corollary 3 in [ 2]).Forg(x) =w′·ϕ(x)andf(x) =P
P∈S(geo)fP(x), then writing an
observable O=P
P∈{I,X,Y,Z }⊗nαPP, we have
|g(x)−f(x)|< ϵ1
X
P∈S(geo)|αP|
 (A.18)
for any x.
This tells us that the hypothesis functions of the ML algorithm indeed approximate the ground
state properties well. The final piece needed is a norm inequality bounding the ℓ1-norm of the
Pauli coefficients. This allows us to bound the terms involving |αP|in Lemma 2 and Lemma 3. In
particular, we have the following bound.
Theorem 8 (Corollary 4 in [ 2]).LetO=P
P∈{I,X,Y,Z }⊗nαPPbe an observable that can be
written as a sum of geometrically local observables. Then,X
P|αP|=O(1). (A.19)
Given these results, Lemma 1 follows directly by triangle inequality and rescaling ϵ1when using
Lemma 2 and Lemma 3. Finally, to prove Theorem 7, it remains to bound the generalization error by
ϵ3. This follows directly from known sample complexity guarantees for the LASSO algorithm [ 64],
which learns ℓ1-regularized linear functions. In order to apply this known result, one needs to provide
a regularization parameter, i.e., some B >0such that the ML algorithm learns functions of the form
h(x) =w·ϕ(x)for∥w∥1≤B. To choose such a B, Lewis et al. bound the ℓ1-norm of w′, where
recallw′
x′,P=fP(x′).
Lemma 4 (Lemma 14 in [ 2]).Letw′be the vector of coefficients defined by w′
x′,P=fP(x′). Then,
∥w′∥1=X
P∈S(geo)X
x′∈XP|fP(x′)|= 2O(polylog(1 /ϵ1)). (A.20)
Using B= 2O(polylog(1 /ϵ1))in the known guarantees for LASSO [64] gives Theorem 7.
19A.2 Deep learning with low-discrepancy sequences
In this section, we review results in classical deep learning theory for obtaining rigorous guarantees
when learning from data sampled according to a low-discrepancy sequence (LDS) [ 55,56,76,77,78,
57, 79, 58, 80, 81]. For this discussion, we follow [54, 85].
We consider a neural network ormulti-layer perceptron model as a composition of several layers
of affine transformations and nonlinear activation functions. Namely, let σ:R→Rbe a nonlinear
activation function. Then, a neural network with Llayers is defined as follows. Let d0, . . . , d L∈N
be the dimension (number of neurons or width ) of each layer k∈ {0, . . . , L }. Here, the zeroth layer
is the input layer and the Lth layer is the output layer . At each layer k∈ {0, . . . , L −1}except for
the output layer, we define an affine transformation Wk:Rdk→Rdk+1byWk(x) =Akx+bkfor
a matrix of weights Ak∈Rdk+1×dkand a vector of biases bk∈Rdk+1. Then, a neural network is
defined as
fθ(x) = (WL−1◦σ◦ ··· ◦ σ◦W0)(x), (A.21)
where σis applied element-wise and θ= ((A0, b0), . . . , (AL−1, bL−1)). The hidden layers are the
firstL−1layers. Here, θare the trainable parameters of the neural network, which can be iteratively
updated through training on data. A deep neural network is a neural network with at least three layers:
L≥3. In this work, we consider the activation function
σ(x) = tanh( x) =ex−e−x
ex+e−x. (A.22)
We refer to such neural networks with this activation function as tanh neural networks .
Suppose a neural network fθaims to approximate some target function f, given training data
{(xℓ, f(xℓ)}N
ℓ=1. Then, the training error is defined as
ˆR(θ) =1
NNX
ℓ=1|f(xℓ)−fθ(xℓ)|2. (A.23)
The prediction error is then defined over the whole domain, including unseen data, as
R(θ) = E
x∼D|f(x)−fθ(x)|2, (A.24)
where the training data is sampled from some distribution D. A canonical result in deep learning
theory [66] is that the generalization error can be bounded by roughly
R(θ)≲ˆR(θ) +O1√
N
, (A.25)
where≲indicates that we only state this result schematically. Importantly, this means that in order
for the neural network fθto approximate fwith high accuracy, many training data points Nare
needed, which is undesirable. In order to fix this issue, [ 54] combines ideas from deep learning with
tools from quasi-Monte Carlo methods [55, 56, 57, 58] to achieve a generalization error bound of
R(θ)≲ˆR(θ) +˜O1
N
, (A.26)
where ˜Oindicates that we are suppressing polylogarithmic factors. The key tool used here is low-
discrepancy sequences [ 55,56,76,77,78,57,79,58,80,81]. Intuitively, this is a collection of points
that covers domain of the function fin such a way that there are no large gaps, or discrepancies. By
filling these gaps, one can ensure that the training data accurately represents the target function, more
so than even uniformly random data. We leverage these ideas to obtain our rigorous guarantee on
the sample complexity of a deep learning algorithm for predicting ground state properties. In the
following, we formally define low-discrepancy sequences and a key inequality in quasi-Monte Carlo
theory for obtaining our generalization bound.
First, we define the discrepancy of a sequence, which is a measure of uniformity.
Definition 1 (Discrepancy [ 56]).Letλbe the Lebesgue measure, N∈N. Let x={xℓ}N
ℓ=1be a
sequence of points with xℓ∈[0,1]dfor all ℓ. The discrepancy of the sequence xis defined as
DN(d) = sup
J∈E|RN(J)|, (A.27)
20where
RN(J) =1
NNX
ℓ=11{xℓ∈J} −λ(J) (A.28)
for a Lebesgue-measurable set J⊆[0,1]d. Also, Eis the set of all rectangular subsets of [0,1]d, i.e.,
E=(dY
i=1[ai, bi) : 0≤ai< bi≤1)
. (A.29)
Intuitively, one can consider the discrepancy as a measure of how well the sequence fills rectangular
subsets of [0,1]d. If the discrepancy is small, this means that the sequence fills these subsets well.
We can similarly define the star-discrepancy, where the supremum is instead taken over rectangular
subsets of [0,1]dsuch that one endpoint is 0.
Definition 2 (Star-discrepancy [ 56]).Letλbe the Lebesgue measure, N∈N. Letx={xℓ}N
ℓ=1be a
sequence of points with xℓ∈[0,1]dfor all ℓ. The star-discrepancy of the sequence xis defined as
D∗
N(d) = sup
J∈E∗|RN(J)|, (A.30)
where RNis defined in Equation (A.28) for a Lebesgue-measurable set J⊆[0,1]d. Also, E∗is the
set of all rectangular subsets of [0,1]d, i.e.,
E∗=(dY
i=1[0, bi) : 0< bi≤1)
. (A.31)
With these definitions, we can define low-discrepancy sequences.
Definition 3 (Low-discrepancy sequence [ 56]).A sequence of points x={xℓ}N
ℓ=1withxℓ∈[0,1]d
for all ℓis alow-discrepancy sequence if
D∗
N(d)≤C(logN)d
N, (A.32)
where Cis a constant that possibly depends on dbut is independent of N.
The value of the constant Cin this definition depends on the construction of the low-discrepancy
sequence. Several constructions of low-discrepancy sequences are known [ 80,77,81,57]. In this
work, we consider Sobol sequences in base 2[57]. For these sequences, we have the following
guarantee
Theorem 9 (Theorem 4.17 in [ 57]).LetN∈N. Ifx={xℓ}N
ℓ=1is a Sobol sequence in base 2with
xℓ∈[0,1]dfor all ℓ, then the star-discrepancy satisfies
D∗
N(d)≤C(d)(logN)d
N, (A.33)
where C(d)is a constant satisfying
C(d)<1
d!d
log(2 d)
. (A.34)
We state this result without proof and refer to [ 57] for details on this construction. Another important
known discrepancy bound that we will use in Appendix C.3 is the following bound on the star-
discrepancy of uniformly random points.
Lemma 5 (Corollary 1 in [ 82]).For any d≥1, N≥1andδ∈(0,1)a (uniformly) randomly
generated d-dimensional point set (x1, . . . , x N)satisfies
D∗
N(d)≤5.7s
4.9 + log1
δ√
d√
N(A.35)
with probability at least 1−δ.
21As discussed earlier, low-discrepancy sequences allow us to obtain better sample complexity guar-
antees for neural networks. The key result in quasi-Monte Carlo theory that enables this is the
Koksma-Hlawka inequality [ 56]. In order to properly state it, we first need to define the Hardy-
Krause variation. A full technical definition can be found in, e.g., [ 54], but for our purposes, it suffices
to consider the following upper bound [ 86]. Let fbe a “sufficiently smooth” function. Then, its
Hardy-Krause variation can be upper bounded by
VHK(f)≤ˆVHK=Z
[0,1]d∂df(y)
∂yi···∂yddy+dX
i=1ˆVHK(f(i)
1), (A.36)
where f(i)
1is the restriction of the function fto the boundary yi= 1. If all of the mixed partial
derivatives are continuous, then this inequality is actually an equality [ 56]. Now, we can state the
Koksma-Hlawka inequality.
Theorem 10 (Koksma-Hlawka inequality) .Letf: [0,1]d→Rbe a function whose mixed derivatives
are absolutely integrable over its domain with bounded Hardy-Krause variation VHK(f)<∞. Let
x={xℓ}N
ℓ=1be a sequence of N d-dimensional points in [0,1]dwith star-discrepancy D∗
N(d). Then
Z
[0,1]df(x)dx−1
NNX
ℓ=1f(xℓ)≤VHK(f)D∗
N(d). (A.37)
This theorem is used in quasi-Monte Carlo methods to estimate the error of approximating an integral
of a function fby the empirical average of fevaluated on a sequence of points. Notice that if the
sequence xis a low-discrepancy sequence, then by definition, we can upper bound the star-discrepancy
D∗
N. Moreover, recalling the definitions of prediction error and training error (Equations (A.23)
and (A.24)), one can see how this relates to our task of bounding the prediction error.
To generalize our results to a wider class of distributions, we need to extend these tools for arbitrary
measures, rather than just the Lebesgue measure. First, we restate the definition of discrepancy and
star-discrepancy [87].
Definition 4 (General Discrepancy [ 88]).Letµbe a normalized Borel measure on [0,1]d. Let
x={xℓ}N
ℓ=1be a sequence of points with xℓ∈[0,1]dfor all ℓ. The discrepancy with respect to µof
the sequence xis defined as
DN(d;µ) = sup
J∈E|RN(J;µ)|, (A.38)
where
RN(J;µ) =1
NNX
ℓ=11{xℓ∈J} −µ(J) (A.39)
for a Borel-measurable set J⊆[0,1]d. Also, Eis the set of all rectangular subsets of [0,1]d, i.e.,
E=(dY
i=1[ai, bi) : 0≤ai< bi≤1)
. (A.40)
Definition 5 (General Star-Discrepancy [ 87]).Letµbe a normalized Borel measure on [0,1]d, and
letN∈N. Letx={xℓ}N
ℓ=1be a sequence of points with xℓ∈[0,1]dfor all ℓ. The star-discrepancy
with respect to µof the sequence xis defined as
D∗
N(d;µ) = sup
J∈E∗|RN(J;µ)|, (A.41)
where RNis defined in Equation (A.39) for a Borel-measurable set J⊆[0,1]d. Also, E∗is the set
of all rectangular subsets of [0,1]d, i.e.,
E∗=(dY
i=1[0, bi) : 0< bi≤1)
. (A.42)
These definitions coincide with Definition 1 and Definition 2 when µis the Lebesgue measure λ.
Moreover, we can define general low-discrepancy sequences similarly to Definition 3 with respect
this general star-discrepancy. There is also a generalized Koksma-Hlawka inequality [ 87], which we
state below.
22Theorem 11 (Generalized Koksma-Hlawka inequality; Theorem 1 in [ 87]).Letf: [0,1]d→Rbe a
measurable function whose mixed derivatives are absolutely integrable over its domain with bounded
Hardy-Krause variation VHK(f)<∞. Let µbe a normalized Borel measure on [0,1]d, and let
x={xℓ}N
ℓ=1be a sequence of N d-dimensional points in [0,1]dwith general star-discrepancy
D∗
N(d;µ). Then,
1
NNX
ℓ=1f(xℓ)−Z
[0,1]df(x)dµ(x)≤VHK(f)D∗
N(d;µ). (A.43)
B Constant sample complexity
In this section, we show that with a simple modification of the algorithm from [ 2], we can reduce the
sample complexity to O(1)for a constant prediction error. We consider all of the same definitions/no-
tation as in Appendix A.1. This section is similar to Section IV in the Supplementary Information
of [2]. As in [ 2], our algorithm first maps the parameter space [−1,1]minto a high-dimensional
feature space Rmϕformϕgiven in Equation (A.8) via a feature map ϕ. Our simple modification is to
use the feature map defined by
˜ϕ(x)x′,P≜sign(αP)p
|αP|1{x∈Tx′,P}, (B.1)
where each coordinate of ϕ(x)is indexed by P∈S(geo), x′∈XP. Note that defining the feature
map in this way requires knowledge of the observable O=P
PαPPcorresponding to the ground
state property to be predicted. However, in practice, this is a natural assumption. The hypothesis class
for our proposed ML algorithm consists of linear functions in this feature space, i.e., functions of
the form h(x) =w·ϕ(x). Then, our algorithm learns these functions via ridge regression [ 65,66].
For a chosen hyperparameter Λ>0, ridge regression finds a vector w∗that solves the following
optimization problem minimizing the training error subject to the constraint that ∥w∥2≤Λ
min
w∈Rmϕ
∥w∥2≤Λ1
NNX
ℓ=1|w·˜ϕ(xℓ)−yℓ|2, (B.2)
where yℓapproximates tr(Oρ(xℓ)). We denote the learned function by h∗(x) =w∗·˜ϕ(x). Note that
the learned function does not need to achieve the minimum training error, but can be some amount say
ϵ3/2above it. For our purposes, we choose the hyperparameter to be Λ = 2O(polylog(1 /ϵ1)), which
we justify in the next section.
Note that there are two main differences from the algorithm in [ 2]. First, recall from Appendix A.1
that the feature map was previously defined as ϕ(x)x′,P=1{x∈Tx′,P}forP∈S(geo), x′∈XP.
Second, instead of using LASSO ( ℓ1-regularized regression), our proposed algorithm uses ridge
regression.
With this algorithm, we obtain the following guarantee.
Theorem 12 (Constant sample complexity; Detailed restatement of Theorem 4) .Let1/e >
ϵ1, ϵ2, ϵ3>0andδ >0. Given training data {(xℓ, yℓ)}N
ℓ=1of size
N= log(1 /δ)2O(log(1 /ϵ3)+polylog(1 /ϵ1)), (B.3)
where xℓis sampled from Dandyℓis an estimator of tr(Oρ(xℓ))such that |yℓ−tr(Oρ(xℓ))| ≤ϵ2,
the ML algorithm can produce h∗(x)that achieves prediction error
E
x∼D|h∗(x)−tr(Oρ(x))|2≤(ϵ1+ϵ2)2+ϵ3 (B.4)
with probability at least 1−δ. The training time for constructing the hypothesis function h∗and the
prediction time for computing h∗(x)are upper bounded by
O(n)polylog(1 /δ)2O(log(1 /ϵ3)+polylog(1 /ϵ1)). (B.5)
Comparing to Theorem 7, notice that our sample complexity guarantee is completely independent of
system size n.
23The theorem in the main text corresponds to ϵ1= 0.2ϵ, ϵ2=ϵ, and ϵ3= 0.4ϵ. In this way,
(ϵ1+ϵ2)2≤1.44ϵ2≤0.53ϵand(ϵ1+ϵ2)2+ϵ3≤ϵ.
So far, we have only considered the setting in which we learn a specific ground state property
tr(Oρ(x))for a fixed observable O. Because our training data is given in the form {(xℓ, yℓ)}N
ℓ=1,
where yℓapproximates tr(Oρ(x))for this fixed observable O, if we want to predict a new property
for the same ground state ρ(x), we would need to generate new training data. Thus, it may be
more useful to learn a ground state representation, from which we could predict tr(Oρ(x))for
many different choices of observables Owithout requiring new training data. In this case, suppose
we are instead given training data {xℓ, σT(ρ(xℓ))}N
ℓ=1, where σT(ρ(xℓ))is a classical shadow
representation [ 52,68,69,70,71] of the ground state ρ(xℓ). An immediate corollary of Theorem 12
is that we can predict ground state representations with the same sample complexity. This follows
from the same proof as Corollary 5 in [2].
Corollary 3 (Learning representations of ground states; detailed restatement of Corollary 1) .Let
1/e > ϵ 1, ϵ2, ϵ3>0andδ >0. Given training data {(xℓ, σT(ρ(xℓ))}N
ℓ=1of size
N= log(1 /δ)2O(log(1 /ϵ3)+polylog(1 /ϵ1)), (B.6)
where xℓis sampled from DandσT(ρ(xℓ)is the classical shadow representation of the ground state
ρ(xℓ)using Trandomized Pauli measurements. For T=O(log(nN/δ )/ϵ2
2) =˜O(log(n/δ)/ϵ2
2), the
ML algorithm can produce a ground state representation ˆρN,T(x)that achieves
E
x∼D|tr(OˆρN,T(x))−tr(Oρ(x))|2≤(ϵ1+ϵ2)2+ϵ3 (B.7)
with probability at least 1−δ, for any observable with eigenvalues between −1and1that can be
written as a sum of geometrically local observables.
We note that the number of measurements Tneeded to generate the training data scales as log(n),
but the amount of training data still remains constant with respect to system size. We do not consider
the number of measurements as contributing to the sample complexity because in our setting, the ML
algorithm is given this training data as input and does not generate it itself.
B.1 Training error bound
To prove Theorem 12, we first derive a bound on the training error. Recall that the training error is
defined as
ˆR(h) = min
w1
NNX
ℓ=1|h(xℓ)−yℓ|2. (B.8)
Define the vector ˜wwith entries indexed by P∈S(geo), x′∈XPby
˜wx′,P≜p
|αP|tr(Pρ(χP(x))), (B.9)
where χP(x)is defined in Equation (A.16). Then, notice that
˜g(x)≜˜w·˜ϕ(x) (B.10)
=X
P∈S(geo)X
x′∈XPsign(αP)|αP|tr(Pρ(χP(x)))1{x∈Tx′,P} (B.11)
=X
P∈S(geo)X
x′∈XPαPtr(Pρ(χP(x)))1{x∈Tx′,P} (B.12)
=w′·ϕ(x) (B.13)
=g(x), (B.14)
where g(x) =w′·ϕ(x)withw′
x′,P=αPtr(Pρ(χP(x))), ϕ(x)x′,P=1{x∈Tx′,P}. By Lemma 1,
we know that g(x)approximates the ground state property with low training error, and thus, in turn,
˜g(x)also approximates the ground state property well. The existence of ˜wsuch that ˜g(x) =˜w·˜ϕ(x)
guarantees that the function h∗(x) =w∗·˜ϕ(x)found by performing via ridge regression will also
yield a small training error. More formally, we have the following guarantee
24Lemma 6 (Training error) .The function
˜g(x) =˜w·˜ϕ(x) =X
P∈S(geo)X
x′∈XPαPtr(Pρ(χP(x)))1{x∈Tx′,P} (B.15)
achieves training error
ˆR(˜g)≤(ϵ1+ϵ2)2. (B.16)
Since ˜g(x) =g(x), this follows directly from Lemma 1. Moreover, we can obtain an ℓ2-norm bound
on˜w. We can utilize this upper bound to choose the hyperparameter Λ>0such that ∥w∥2≤Λ.
Thus, we have the following lemma,
Lemma 7 (ℓ2-Norm bound) .Let˜wbe the vector of coefficients defined in Equation (B.9) . Then, we
have
∥˜w∥2
2=X
P∈S(geo)X
x′∈XP|αP||tr(Pρ(χP(x)))|2= 2O(polylog(1 /ϵ1)). (B.17)
Proof. This is a simple consequence of Lemma 4. Explicitly, we have
∥˜w∥2=X
P∈S(geo)X
x′∈XP|αP||tr(Pρ(χP(x)))|2(B.18)
≤X
P∈S(geo)X
x′∈XP|αP||tr(Pρ(χP(x)))| (B.19)
= 2O(polylog(1 /ϵ1)), (B.20)
where the second line follows because tr(Pρ(χP(x)))≤1and the last line follows by Lemma 4.
This justifies our choice of Λ = 2O(polylog(1 /ϵ1)). Now consider the learned function h∗(x) =
w∗·˜ϕ(x), where w∗is found by minimizing the training error subject to the constraint that ∥w∥2≤Λ.
We do not require the learned function to achieve the minimum training error, but it can be some
amount ϵ3/2above it, i.e.,
ˆR(h∗)≤ϵ3
2+ minw
∥w∥2≤Λ1
NNX
ℓ=1|w·˜ϕ(xℓ)−yℓ|2. (B.21)
Since we chose Λ = 2O(polylog(1 /ϵ1))and we showed in Lemma 7 that ∥˜w∥2≤Λ, then the minimum
training error is at most ˆR(˜g). We also know that this is bounded by (ϵ1+ϵ2)2by Lemma 6. This
then implies
ˆR(h∗)≤ϵ3
2+ˆR(g)≤(ϵ1+ϵ2)2+ϵ3
2. (B.22)
B.2 Prediction error bound
To prove Theorem 12, it remains to bound the prediction error. We can use a standard result from
machine learning theory on the prediction error of ridge regression algorithms [66, 64].
Theorem 13 (Theorem 26.12 in [ 66]).Suppose that Dis a distribution over X × Y such that with
probability 1we have that ∥x∥2≤R. LetH={x7→w·x:∥w∥2≤Λ}and let ℓ:H ×Z→R
be a loss function of the form ℓ(w,(x, y)) = ϕ(w·x, y)such that for all y∈ Y, a7→ϕ(a, y)is
aρ-Lipschitz function and such that max a∈[−ΛR,ΛR]|ϕ(a, y)| ≤c. Then, for any δ∈(0,1), with
probability of at least 1−δover the choice of an i.i.d. sample of size N, for all h∈ H,
R(h)≤ˆRS(h) +2ρΛR√
N+cr
2 log(2 /δ)
N. (B.23)
Here, R(h)denotes the prediction error for the hypothesis h. With this, we can complete the proof of
Theorem 12.
25Proof of Theorem 12. First, let us reframe the theorem in our setting. Consider the input space X
to be the parameter space [−1,1]mand our input variable is x=˜ϕ(x). Since the observables we
consider have spectral norm at most 1, the output space fulfils Y ⊆ [−1,1]. The hypothesis set is
H={x7→w·˜ϕ(x) :∥w∥2≤Λ}, where in the previous section, we set Λ = 2O(polylog(1 /ϵ1)).
It remains to check the conditions of the theorem. We begin by showing that ∥x∥2≤Rfor some
R >0. We have the following computation:
∥x∥2=˜ϕ(x)·˜ϕ(x) =˜ϕ(x)2
2(B.24)
=X
P∈S(geo)X
x′∈XP|sign(αP)p
|αP|1{x∈Tx′,P}|2(B.25)
=X
P∈S(geo)X
x′∈XP|αP|1{x∈Tx′,P} (B.26)
=X
P∈S(geo)|αP| (B.27)
=O(1), (B.28)
where the second to last line follows because for a given P,x∈Tx′,Pfor exactly one x′∈XP.
This is shown in Corollary 3 of [ 2]. Also, the last line follows by Theorem 8. Thus, we can take
R=O(1).
Finally, note that ℓ(w,(x, y)) =|w·x−y|=ϕ(w·x, y). Therefore, ϕ(a, y)is a1-Lipschitz
function and fulfils
max
a∈[−ΛR,ΛR]|ϕ(a, y)|= max
a∈[−ΛR,ΛR]|a−y| ≤ΛR+ 1. (B.29)
Thus, we can consider ρ= 1andc=O(1)·2O(polylog(1 /ϵ1))+ 1.
By Equation (B.22), we know that the learned model h∗(x) =w∗·˜ϕ(x)achieves
ˆR(h∗)≤(ϵ1+ϵ2)2+ϵ3
2. (B.30)
Plugging in R=O(1),ρ= 1,Λ = 2O(polylog(1 /ϵ1))andc=O(1)·2O(polylog(1 /ϵ1))+ 1into
Theorem 13, we have
R(h∗)≤(ϵ1+ϵ2)2+ϵ3
2(B.31)
+1√
N
2O(1)·2O(polylog(1 /ϵ1))+
O(1)·2O(polylog(1 /ϵ1))+ 1p
2 log(2 /δ)
(B.32)
with probability at least 1−δ. In order to bound the prediction error by (ϵ1+ϵ2)2+ϵ3, we need N
to be large enough such that
1√
N
2O(1)·2O(polylog(1 /ϵ1))+
O(1)·2O(polylog(1 /ϵ1))+ 1p
2 log(2 /δ)
≤ϵ3
2.(B.33)
Solving for Nin this inequality and simplifying we have
N=4
ϵ2
32O(polylog(1 /ϵ1))(1 +p
log(1 /δ))2= 2O(log(1 /ϵ3)+polylog(1 /ϵ1))log(1 /δ). (B.34)
Thus, for this N, we can guarantee that R(h∗)≤(ϵ1+ϵ2)2+ϵ3, as claimed.
On another note, when considering a scenario with a fixed number of parameters m=O(1), much
like the setting in [ 51], the expression derived from the result in Lemma 10 exhibits polynomial
dependence on ϵ. One can incorporate the constant number of parameters by setting ˜m=m. Thus,
we recover the exact ground state properties tr(Pρ(x))infPand the approximation error resulting
from applying Lemma 2 vanishes completely. Furthermore, we can slightly adapt the proof of
Lemma 7 and obtain
∥˜w∥2
2=X
P∈S(geo)X
x′∈XP|αP||tr(Pρ(χP(x)))|= max
P∈S(geo)|XP|X
Q∈S(geo)|αP|=O(ϵ−m),(B.35)
where the last step is performed similarly as in the proof of Lemma 4.
26B.3 Computational time for training and prediction
It remains to analyze the computational time for the ML algorithm’s training and prediction.
Proof of computational time in Theorem 12. The training time is dominated by the time required for
ridge regression over the feature space defined by the feature map ϕ. Recall that the optimization
problem under considerations is
min
w∈Rmϕ
∥w∥2≤Λ1
NNX
ℓ=1|w·˜ϕ(xℓ)−yℓ|2. (B.36)
One can show that this is a convex optimization problem so that we can solve its equivalent dual
problem instead. This dual optimization problem is given by
max
α∈RN−α⊺(K+λI)α+ 2α·Y, (B.37)
where the kernel matrix is K=X⊺X, for the feature matrix X∈Rmϕ×Ndefined by X=
(˜ϕ(x1)···˜ϕ(xN))and the response vector Y= (y1, . . . , y N)⊺. Ifκis the maximum time it takes
to compute a kernel entry K(x, x′) =˜ϕ(x)·˜ϕ(x′), then one can show that the time to solve this dual
problem is O(κN2+N3). Moreover, prediction can be executed in O(κN). For more details in this
analysis, we refer the reader to, e.g., Section 11.3.2 of [64].
In our case, κ=O(mϕ)since ˜ϕ(x)∈Rmϕand the kernel is simply the dot product of two of these
vectors. By Equation (A.8), we know that
mϕ=O(n)2O(polylog(1 /ϵ1))(B.38)
so that κ=O(n)2O(polylog(1 /ϵ1)). Moreover, by Theorem 12,
N= log(1 /δ)2O(log(1 /ϵ3)+polylog(1 /ϵ1)). (B.39)
Plugging this into the time required to solve the dual problem for kernel ridge regression, we have
O(κN2+N3) =O(n)polylog(1 /δ)2O(log(1 /ϵ3)+polylog(1 /ϵ1)). (B.40)
Moreover, the prediction time is given by
O(κN) =O(n)polylog(1 /δ)2O(log(1 /ϵ3)+polylog(1 /ϵ1)). (B.41)
C Rigorous guarantees for neural networks
In this section, we derive a rigorous guarantee on the sample complexity of a deep-learning based
model for predicting ground state properties. Similarly to the previous sections, let 1/e > ϵ 1, ϵ2, ϵ3>
0throughout. One can think of ϵ1as the approximation error caused by our neural network model not
exactly capturing the ground state property; ϵ2represents the noise in the training data; ϵ3corresponds
to the generalization error.
Recall again the setup, where we consider a family of n-qubit Hamiltonians H(x) =PL
j=1hj(⃗ xj)
parameterized by an m-dimensional vector x∈[−1,1]m, which satisfies the assumptions (a)-(c) in
Appendix A.1. Let ρ(x)denote the ground state of H(x). We consider the task of predicting ground
state properties tr(Oρ(x))for some observable Othat satisfies assumption (d) in Appendix A.1,
where we are given training data {(xℓ, yℓ)}N
ℓ=1with yℓ≈tr(Oρ(xℓ)). In particular, suppose
|yℓ−tr(Oρ(xℓ))| ≤ϵ2. Furthermore, we also assume that all mixed partial derivatives of order
˜m≜|IP|ofhjare bounded as
∂˜m
∂x1∂x2. . . ∂x ˜mhj(x)
∞≤1, (C.1)
where IPis the set of local coordinates defined in Equation (A.2). Here, we denote the number of
local coordinates by ˜m=|IP|for ease of notation. This is similar in spirit to assumption (b), in
27which we assume that the local terms have bounded directional derivatives: ∥∂hj/∂ˆu∥∞≤1, where
ˆuis a unit vector in parameter space.
LetS(geo)denote the set of geometrically local Pauli observables. Our deep neural network model
consists of |S(geo)|=O(n)“local” multi-layer perceptron models (defined in generally in Ap-
pendix A.2) with two hidden layers and tanh activation functions. Their outputs are combined
through a linear layer without activation function. Formally, our model is defined as follows.
Definition 6 (Deep neural network model) .The neural network model is given by a function
fΘ,w: [−1,1]m→Rdefined by
fΘ,w(x) =X
P∈S(geo)wPfθP
P(x), (C.2)
where the “local models” fθP
P: [−1,1]˜m→Rare given by
fθP
P(x) = (Wout◦tanh◦Whidden◦tanh◦Win◦τ−1)(x), (C.3)
withτ−1(x) = ( x+ 1)/2andθP= [(Win, bin),(Whidden , bhidden ),(Wout, bout)]. Here, Win∈
R˜m×W,bin∈RW,Whidden ∈RW×W,bhidden ∈RW,Wout∈RW×1andbout∈R, where W
denotes the width of the hidden layers. The weights are given by Θ ={θP:P∈S(geo)}in the local
models and w∈Rin the last layer. Furthermore, we denote the individual parameters by Θi∈R.
Using this model, we can establish an objective function that we aim to minimize during the training
process. Specifically, this objective function comprises the mean square error along with a lasso
penalty applied to the weights win the final layer.
Definition 7 (Training objective) .LetfΘ,wbe a neural network model as in Definition 6. Let
{(xℓ, yℓ)}N
ℓ=1be the training data set and λ >0be some regularization parameter that may depend
onϵ1, ϵ2>0. The training objective is given by
1
NNX
ℓ=1|fΘ,w(xℓ)−yℓ|2+λ∥w∥1. (C.4)
Our proposed ML algorithm then operates as in Algorithm 1.
Algorithm 1: Deep learning-based prediction of ground state properties
Sample Nlow-discrepancy points {xℓ}N
ℓ=1;
Collect training labels {yℓ}N
ℓ=1, where yℓ≈tr(Oρ(xℓ));
Data: {(xℓ, yℓ)}N
ℓ=1;
Fix|IP|;
Initialize model architecture according to Definition 6 with appropriate hyperparameter δ1, width
Was in Theorem 16 and weights Θ, wusing an appropriate initialization method (e.g., Xavier
initialization [72]);
Train with respect to the objective in Definition 7 with appropriate hyperparameter λ >0using a
quasi-Monte Carlo training algorithm, e.g., Adam [73] until convergence;
Obtain locally optimal parameters Θ∗, w∗;
Result: Classical representation fΘ∗,w∗;
After training our model using Algorithm 1, we obtain the following rigorous guarantee.
Theorem 14 (Neural network sample complexity guarantee) .Let1/e > ϵ 1, ϵ2, ϵ3>0. LetfΘ∗,w∗:
[−1,1]m→Rbe a neural network model produced from Algorithm 1 trained on data {(xℓ, yℓ)}N
ℓ=1
of size
N= 2O(polylog(1 /ϵ1)+polylog(1 /ϵ3)), (C.5)
where the xℓ’s form a low-discrepancy Sobol sequence and |yℓ−tr(Oρ(xℓ))| ≤ϵ2. Suppose that
fΘ∗,w∗achieves a training error of at most ((ϵ1+ϵ2)2+ϵ3)/2. Additionally, suppose that all
parameters Θ∗
ioffΘ∗,w∗satisfy |Θ∗
i| ≤Wmax, for some Wmax>0that is independent of the system
sizen. Then the neural network fΘ∗,w∗achieves prediction error
E
x∼U[−1,1]m|fΘ∗,w∗(x)−tr(Oρ(x))|2≤2(ϵ1+ϵ2)2+ϵ3, (C.6)
where x∼U[−1,1]mdenotes sampling xfrom a uniform distribution over [−1,1]m.
28We prove this theorem in the next two sections (Appendices C.1 and C.2). As a corollary of this, we
obtain the theorem stated in the main text. We discuss the assumptions that the distribution Dmust
satisfy in depth and prove the corollary in Appendix C.3. The theorem in the main text (Theorem 5)
corresponds to ϵ1=ϵ3= 0.1ϵandϵ2=ϵ. Hence, 2(ϵ1+ϵ2)2≤2.44ϵ2≤0.9ϵfor1/e > ϵ > 0
and so 2(ϵ1+ϵ2)2+ϵ3≤ϵ.
Corollary 4 (Neural network sample complexity guarantee; detailed restatement of Theorem 5) .Let
1/e > ϵ 1, ϵ2, ϵ3>0,Da distribution with PDF gsatisfying the following properties: ghas full
support and is continuously differentiable on [−1,1]m. Moreover, gis of the form
g(x) =LY
j=1gj(⃗ xj). (C.7)
LetfΘ∗,w∗: [−1,1]m→Rbe a neural network model produced from Algorithm 1 trained on data
{(xℓ, yℓ)}N
ℓ=1of size
N= log(1 /δ)2O(polylog(1 /ϵ1)+polylog(1 /ϵ3)), (C.8)
where the xℓ∼ D and|yℓ−tr(Oρ(xℓ))| ≤ϵ2. Suppose that fΘ∗,w∗achieves a training error
of at most ((ϵ1+ϵ2)2+ϵ3)/2. Additionally, suppose that all parameters Θ∗
ioffΘ∗,w∗satisfy
|Θ∗
i| ≤Wmax, for some Wmax>0that is independent of the system size n. Then the neural network
fΘ∗,w∗achieves prediction error
E
x∼D|fΘ∗,w∗(x)−tr(Oρ(x))|2≤2(ϵ1+ϵ2)2+ϵ3, (C.9)
with probability at least 1−δ.
Moreover, while we can show the existence of suitable parameters that achieve a low training error,
quantified by our training objective in Definition 7, in general, we cannot prove that Algorithm 1
converges to parameters close to this optimum because our training objective is not convex. Thus, to
obtain the guarantee in Theorem 14, we need to assume that a low training error is indeed achieved
by Algorithm 1. This is commonly satisfied in practice.
Similar to Corollary 3, if we are instead given training data {xℓ, σT(ρ(xℓ))}N
ℓ=1, where σT(ρ(xℓ))is
a classical shadow representation [ 52,68,69,70,71] of the ground state ρ(xℓ), then an immediate
corollary of Theorem 14 is that we can predict ground state representations with the same sample
complexity. This follows from the same proof as Corollary 5 in [2].
Corollary 5 (Learning representations of ground states with neural networks; detailed restatement
of Corollary 2) .Let1/e > ϵ 1, ϵ2, ϵ3>0andδ >0. Given training data {(xℓ, σT(ρ(xℓ))}N
ℓ=1of
size
N= log(1 /δ)2O(polylog(1 /ϵ3)+polylog(1 /ϵ1)), (C.10)
where xℓis sampled from a distribution Dsatisfying the same assumptions as Corollary 4 and
σT(ρ(xℓ)is the classical shadow representation of the ground state ρ(xℓ)using Trandomized Pauli
measurements. For T=O(log(nN/δ )/ϵ2
2) =˜O(log(n/δ)/ϵ2
2), the ML algorithm can produce a
ground state representation ˆρN,T(x)that achieves
E
x∼D|tr(OˆρN,T(x))−tr(Oρ(x))|2≤2(ϵ1+ϵ2)2+ϵ3 (C.11)
with probability at least 1−δ, for any observable with eigenvalues between −1and1that can be
written as a sum of geometrically local observables.
We review low-discrepancy sequences and techniques in quasi-Monte Carlo theory in Appendix A.2,
which we use in our proof. To prove Theorem 14, we first show that there exists weights Θ′, w′such
that our proposed neural network fΘ′,w′achieves a low training error, i.e., it approximates the ground
state properties tr(Oρ(x))well. We show this using results in classical deep learning theory about
approximating arbitrary functions with neural networks [ 53]. Then, we use the Koksma-Hlawka
inequality (Theorem 10) to bound the prediction error of our model, similarly to [ 54]. As we want to
derive a bound which is independent of the size of the physical system, our approach requires some
additional steps. Since the dimension of the input domain of our model depends on the size of the
physical system, we cannot treat it as constant as in [ 54]. Therefore, we bound the prediction error
with respect to local functions, whose domain size is independent of the system size.
29Moreover, recall that the Koksma-Hlawka inequality produces a bound in terms of the star-discrepancy
(Definition 2) and the Hardy-Krause variation. The star-discrepancy can be bounded by considering
low-discrepancy sequences (Definition 3), and the Hardy-Krause variation can be bounded by
Equation (A.36). We derive explicit bounds for the Hardy-Krause variation of the ground state
properties tr(Oρ(x)), using tools from the spectral flow formalism [ 59,60,61]. To obtain Corollary 4,
we follow a similar proof but use results relating the discrepancy with respect to the Lebesgue measure
to the discrepancy with respect to arbitrary measures and bounds on the discrepancy of uniformly
random points (Appendix A.2).
In Appendix C.1, we prove that our model approximates the ground state properties well. Then, in
Appendix C.2, we use the Koksma-Hlawka inequality to bound the prediction error of our model.
Technical results explicitly bounding the mixed partial derivatives of the ground state properties are
found in Appendix C.4. We use these in Appendix C.2 to bound the Hardy-Krause variation. We
then generalize this to data sampled from different distributions, as in Corollary 4, in Appendix C.3.
C.1 Approximation of ground state properties by neural networks
In this section, we prove that when choosing the number of parameters and width of the model
appropriately, there exists a parameter set for which the deep neural network model approximates
the ground state properties well. This shows the existence of a neural network with low training
error. The proof is a direct application of the main result from [ 53], which proves that tanh neural
networks can approximate sufficiently smooth functions, in combination with the bounds on the
mixed derivative of tr(Pρ(x))we derived in Appendix C.4.
We consider the local functions defined as in Appendix A.1.2. Namely, define f(x) =P
P∈S(geo)αPfP(x), where fP(x) = tr( Pρ(χP(x)))forO=P
P∈{I,X,Y,Z }⊗nαPPand
χP(x)c=xc, c∈IP
0 c /∈IP(C.12)
for all c∈ {1, . . . , m }, forIPdefined in Equation (A.2). Note that here, we slightly alter the definition
from Appendix A.1.2, where we do not include the coefficient αPin the definition of fP(x). Because
all parameters with coordinates not in IPare set to 0, we can view fPas a function taking inputs in
[−1,1]˜m, where recall that we use ˜m=|IP|to denote the number of local coordinates.
We show that there exists a neural network that can approximate these local functions fPwell.
In order to apply the result from [ 53] to approximate tr(Pρ(χP(x))), we need to transform its
inputs, such that it becomes a map [0,1]˜m→R. Therefore, we introduce an appropriate function
τ(x) = 2 x−1. To avoid confusion when considering the different domains [−1,1]mversus [0,1]m,
if an input x∈[−1,1]m, we simply denote it by x. Ifx∈[0,1]m, we denote it by ¯x. We similarly
use this notation for other domain dimensions.
In the following lemma, we use Wk,∞(Ω)forΩ⊆Rmto denote the Sobolev space
Wk,∞(Ω)≜
f∈L∞(Ω) :∂|α|f
∂xα1
1···∂xαmm∈L∞(Ω)for all α∈Nm
0with|α| ≤k
(C.13)
so that the Sobolev norm is defined as
∥f∥Wk,∞(Ω)≜max
|α|=s∂|α|f
∂xα1
1···∂xαmm
L∞(Ω). (C.14)
forα∈Nm
0and the L∞-norm is defined by
∥f∥L∞(Ω)= sup
x∈Ω∥f∥. (C.15)
Lemma 8 (Existence of approximating neural network) .Letϵ1>0,s, M ∈N. Let
∥H(x)∥Ws,∞([−1,1]m)≤1. Define functions fP: [0,1]˜m→RasfP(τ(¯x)) = tr( Pρ(χP(τ(¯x)))),
where τ(¯x) = 2¯ x−1. Then, there exist neural networks ˆfM
P, such that
fP◦τ−ˆfM
P
L∞([0,1]˜m)≤ϵ1 (C.16)
with at most ϵ−˜m+1
s
1 2O( ˜mlog( ˜m))parameters. Furthermore, the weights scale as 2poly(log(1 /ϵ1),˜m,s).
30To prove this, we utilize the main result from [ 53], which states that a neural network with tanh
activation functions can approximate effectively any function.
Theorem 15 (Theorem 5.1 in [ 53]).Letd, s∈N,R >0,d >0andf∈Ws,∞([0,1]d). There exist
constants C(d, k, s, f ),N0(d)>0, such that for every N∈NwithN > N 0(d)there exists a tanh
neural network ˆfNwith two hidden layers, one of width at most 3⌈s
2⌉|Ps−1,d+1|+d(N−1)(where
|Pn,d|= n+d−1
n
) and another of width at most 3⌈d+2
2⌉|Pd+1,d+1|Nd(or3⌈s
2⌉+N−1and6N
ford= 1), such that,
∥f−ˆfN∥L∞([0,1]d)≤(1 +δ)C(d,0, s, f)
Ns, (C.17)
and for k= 1, . . . , s −1,
∥f−ˆfN∥Wk,∞([0,1]d)≤3d(1 +δ)(2(k+ 1))3kmaxn
Rk,lnk 
βNs+d+2oC(d, k, s, f )
Ns−k,(C.18)
where we define
β=k32d√
dmax{1,∥f∥1/2
Wk,∞([0,1]d)}
δmin{1,p
C(d, k, s, f )}. (C.19)
Iff∈Cs([0,1]d), then it holds that
C(d, k, s, f ) = max
0≤l≤k1
(s−l)!3d
2s−l
|f|Ws,∞([0,1]d), N 0(d) =3d
2, (C.20)
and else it holds that
C(d, k, s, f ) = max
0≤l≤kπ1/4√s
(s−l−1)! 
5d2s−l|f|Ws,∞([0,1]d), N 0(d) = 5 d2. (C.21)
In addition, the weights of ˆfNscale as O
C−s/2Nd(d+s2+k2)/2(s(s+ 2))3s(s+2)
.
The proof of Lemma 8 follows by an application of Theorem 15.
Proof of Lemma 8. We directly apply Theorem 15, with the input space dimension ˜m, where ˜mis
the number of local parameters ˜m=|IP|. By Corollary 8, then we have
fP◦τ−ˆfM
P
L∞([0,1]˜m)≤(1 +δ)
s!3 ˜mCs2
2Ms
. (C.22)
We want to show that this is bounded above by ϵ1. By rearranging, we find that this holds when
ϵ−1
s
1(1 +δ)
s!1
s3
2˜mCs2≤M. (C.23)
Note that by composing with fPwithτ, we acquire an extra factor of 2s, which can be considered a
component of C. Using M=O(ϵ−1
s
1˜ms2), this results in the widths of the two layers being
3ls
2ms+ ˜m
˜m+ 1
+ ˜m(M−1)and˜m+ 2
22 ˜m+ 2
d+ 1
M˜m, (C.24)
and therefore at most
(c1˜m)c2˜mϵ−˜m+1
s
1 = 2O( ˜m(log( ˜m)+log(1 /ϵ1)/s))(C.25)
trainable weights in the neural network. The constants c1andc2are independent of ˜m, but may
depend on s. By Theorem 15, the weights scale as
O
C−s/2
ϵ−1
s
1˜ms2˜m( ˜m+s2+k2)/2
(s(s+ 2))3s(s+2)
=ϵ−˜m+1
s
1 2O( ˜mlog( ˜m)). (C.26)
31Although the dependence on sis not relevant for our final statement, it is important to comment on
the effect of the smoothness of H(x). The result states that the dependence of the required parameters
with respect to 1/ϵ1improves with the highest degree for which all mixed derivatives of H(x)are
bounded. When H(x)is analytic, scan be chosen to be very large so that the number of parameters in
the neural network almost scales as O( ˜mslog( ˜m)). The constant scales rather poorly with s; however,
this effect is only be visible for very small ϵ1.
Using Lemma 8, we can show that there exist parameters such that the complete model approximates
tr(Oρ(x))and obtains a small training objective (defined in Definition 7). The theorem (Theorem 6)
in the main text corresponds to ϵ1= 0.2ϵ, ϵ2=ϵso that (ϵ1+ϵ2)2≤1.44ϵ2≤0.53ϵ≤ϵ.
Theorem 16 (Detailed restatement of Theorem 6) .For any 1/e > ϵ 1, ϵ2>0and appropriate width
W, there exist weights Θ′, w′such that the neural network model fΘ′,w′satisfies
|fΘ′,w′(x)−tr(Oρ(x))| ≤ϵ1 (C.27)
for any x∈[−1,1]m. In particular, for any collection of Ntraining data points {(xℓ, yℓ)}N
ℓ=1with
|yℓ−tr(Oρ(xℓ))| ≤ϵ2, we have
1
NNX
ℓ=1|fΘ′,w′(xℓ)−yℓ|2+λ∥w′∥1≤(ϵ1+ϵ2)2(C.28)
for a suitable choice of regularization parameter λ=O(ϵ2
1). Moreover, each parameter Θiof the
network has a magnitude of |Θi|= 2O(polylog(1 /ϵ1)).
Proof. Write O=P
PαPtr(Pρ(x)). By Theorem 8, let D=O(1)be a constant such that
X
P|αP| ≤D. (C.29)
For every Pauli P, then by Lemma 8, there exist weights θ′
Psuch that a neural network ¯fθ′
P:
[0,1]˜m→Rwith two hidden layers as in Definition 6 approximates the local functions fP(τ(¯x)) =
tr(Pρ(χP(τ(¯x)))), where τ(¯x) = 2¯ x−1, up to an error ϵ1/(4D), when the width of their hidden
layers is chosen as W=ϵ−˜m+1
s
1 2O( ˜mlog( ˜m)), where the number of local coordinates is given by
˜m=|IP|and by the smoothness assumption Item (d), s≥1. In other words, we have
¯fθ′
P
P(¯x)−tr(Pρ(χP(τ(¯x))))≤ϵ1
4D, (C.30)
where ¯x∈[0,1]˜m. Because τis simply a coordinate transformation, then we also obtain
fθ′
P
P(x)−tr(Pρ(χP(x)))≤ϵ1
4D, (C.31)
forx∈[−1,1]˜m.
Furthermore, by Lemma 2 in Appendix A.1.2, the sum of the local functions f(x) =P
PαPfP(x)
approximates the ground state property tr(Oρ(x)) =P
PαPtr(Pρ(x))well. In particular, combin-
ing Lemma 2 with Theorem 8, we have
X
PαPtr(Pρ(χP(x)))−X
PαPtr(Pρ(x))≤ϵ1
4. (C.32)
This holds when choosing the local radius δ1(defined in Equation (A.5)) to be δ1= 4Clog2(1/ϵ1)
for some constant C. This implies that ˜m=|IP|=O(polylog(1 /ϵ1))(e.g., Equation (S33) of [ 2]).
Thus, for the model fΘ′,w′with architecture defined in Definition 6 and weights w′
P=αPand
Θ′={θ′
P}P, we have
|fΘ′,w′(x)−tr(Oρ(x))| (C.33)
=X
PαPfθ′
P
P(x)−X
Ptr(Pρ(χP(x))) +X
Ptr(Pρ(χP(x)))−X
Ptr(Pρ(x))(C.34)
32≤X
P|αP|ϵ1
4D+X
Ptr(Pρ(χP(x)))−X
Ptr(Pρ(x))(C.35)
≤ϵ1
4+X
Ptr(Pρ(χP(x)))−X
Ptr(Pρ(x))(C.36)
≤ϵ1
2. (C.37)
Moreover, by definition of the training data, we have |yℓ−tr(Oρ(xℓ))| ≤ϵ2. Thus, by triangle
inequality and choosing regularization parameter λ=ϵ2
1/(2D), we have
1
NNX
ℓ=1|fΘ′,w′(xℓ)−yℓ|2+λ∥w′∥1 (C.38)
=1
NNX
ℓ=1|fΘ′,w′(xℓ)−tr(Oρ(xℓ)) + tr( Oρ(xℓ))−yℓ|2+λ∥w′∥1 (C.39)
≤(ϵ1/2 +ϵ2)2+λ∥w′∥1 (C.40)
≤ϵ1
2+ϵ22
+ϵ2
1
2(C.41)
≤(ϵ1+ϵ2)2. (C.42)
Finally, plugging in ˜m=O(polylog(1 /ϵ1)), by Lemma 8, then we obtain |Θi|= 2O(polylog(1 /ϵ1)),
as required.
C.2 Prediction error bound
In this section, we derive our result on the prediction error to complete the proof of Theorem 14.
The central result we use is the Koksma-Hlawka inequality (Theorem 10) from quasi-Monte Carlo
theory, which produces a bound in terms of the star-discrepancy (Definition 2) and the Hardy-Krause
variation (Equation (A.36)). We review these tools in Appendix A.2. To bound the star-discrepancy,
we consider a specific low-discrepancy sequence with guarantees described in Appendix A.2. The
Hardy-Krause variation can be bounded by considering the mixed derivatives of our target function
tr(Oρ(x))and our neural network model. We relegate the bounds on the mixed derivatives of
tr(Oρ(x))to Appendix C.4, as the discussion is fairly technical. To bound the mixed derivatives of
our model, we consider the following lemma.
Lemma 9 (Bound on mixed derivatives of neural network) .Letk, d∈N. Let ˆf: [−1,1]d→Rbe a
tanh neural network with two hidden layers of width W≥dand maximal weight Wmax. Then
∥ˆf∥Wk,∞([−1,1]d)= 2O(k2log(k)+klog(dWW max)). (C.43)
Proof. Recall that a tanh deep neural network with two hidden layers is defined as a function
ˆf: [−1,1]d→Rsuch that
ˆf(x) = (Wout◦tanh◦Whidden◦tanh◦Win)(x), (C.44)
where the activation function tanh is applied element-wise. Note that this result holds for any tanh
neural network with two hidden layers, where this neural network does not necessarily have to be the
same model as Definition 6.
LetWL∈ {Win, Whidden , Wout}denote the layers of the neural network that perform an affine
transformation for L∈ {in,hidden ,out}. We can also use L∈ {0,1,2}, where 0corresponds to in, 1
corresponds to hidden, and 2 corresponds to out. Let dLdenote the width (number of input neurons) in
each layer, where we define d0=din, d1=d2=W, d 3=dout. In this way, WL:RdL→RdL+1for
L∈ {0,1,2}. Explicitly, we have Win:Rdin→RW,Whidden :RW→RW,Wout:RW→Rdout.
Since WLperforms an affine transformation, we can write it has WL(x) = (f1(x), . . . , f dL+1(x)),
where x∈RdLandfiare linear functions fi(x) =w⊺
ix+biforwi∈RdL, bi∈R. For these linear
33layers, we observe for any function g:Rdg→RdLwith input dimension dgand for L∈ {0,1,2},
we have
max
1≤i≤dL+1∥(WL◦g)i∥Wk,∞([−1,1]d)= max
1≤i≤dL+1∥fi(g(x))∥Wk,∞([−1,1]d) (C.45)
≤dL+1X
i=1∥wT
ig(x) +bi∥Wk,∞([−1,1]d) (C.46)
≤max
1≤j≤dg∥WL∥1∥g(x)j∥Wk,∞([−1,1]d), (C.47)
where we use the notation
∥WL∥1≜dL+1X
i=1
|bi|+dLX
j=1|wi,j|
, (C.48)
where wis a matrix with rows given by the vectors w⊺
i,wi∈RdL. To show this inequality, we used
Hölder’s inequality in the last step. With this, by factoring out one layer WLat a time, we can bound
the Sobolev norm of ˆf. In particular, we have
∥ˆf∥Wk,∞([−1,1]d) (C.49)
=∥(Wout◦tanh◦Whidden◦tanh◦Win)(x)∥Wk,∞([−1,1]d)(C.50)
≤ ∥Wout∥1max
1≤j≤W∥(tanh◦Whidden◦tanh◦Win)j∥Wk,∞([−1,1]d) (C.51)
≤ ∥Wout∥116(e2k4d2)k(2k)k(k+1)max
1≤j≤W∥(Whidden◦tanh◦Win)j∥k
Wk,∞([−1,1]d) (C.52)
≤ ∥Wout∥1∥Whidden∥k
1·16(e2k4d2)k(2k)k(k+1)max
1≤j≤W∥(tanh◦Win)j∥k
Wk,∞([−1,1]d)(C.53)
≤ ∥Wout∥1∥Whidden∥k
1·162(e2k4d2)2k(2k)2k(k+1)∥Win∥k
1. (C.54)
In the second line, we used Equation (C.47). In the third line, we used the two following inequalities:
dm
dxmtanh( x)≤(2m)m+1min{exp(−2x),exp(2 x)} (C.55)
for all x∈R, m∈N(see Lemma A.4 in [53]), and
∥g◦f∥Wn,∞≤16(e2n4md2)n∥g∥Wn,∞max
1≤i≤m∥(f)i∥n
Wn,∞ (C.56)
for any functions f∈Cn(Ω1; Ω2)andg∈Cn(Ω2;R)defined on Ω1⊂Rd,Ω2⊂Rmwith
d, m, n ∈N(see Lemma A.7 in [ 53]). In the fourth and fifth lines, we used Equation (C.47) and
these inequalities again. Furthermore, we used that our inputs are absolutely bounded by 1in the last
step.
We can further bound this term using that Wmaxis the maximal weight of ˆfand the width of the
hidden layers is lower bounded by W≥d.
∥ˆf∥Wk,∞([−1,1]d)≤162(e2k4d2)2k(2k)2k(k+1)W2k+1
maxW3k+1dk= 2O(k(klog(k)+log( dWW max))).
(C.57)
Now we have all the necessary tools in order to derive a bound on the generalization error for our
neural network model of the form given in Definition 6. In our proof, we first bound the prediction
error in terms of functions with 2 ˜m-dimensional domain and on which we can directly apply the
Koksma-Hlawka inequality. Then, we use the previous result to obtain an explicit bound. Due to
the regularity of the parameters αPand our model parameters wP, this prediction error bound is
independent of the system size n.
Before stating the formal result bounding the prediction error, we introduce some notation. We define
the prediction error of a neural network fΘ,wwith weights given by Θ, was
R(Θ)≜ E
x∼U[−1,1]m|fΘ,w(x)−tr(Oρ(x))|2, (C.58)
34where in our case, x∼U[−1,1]mdenotes xsampled from a uniform distribution over [−1,1]m. We
suppress win the notation to avoid cluttering. For a neural network fΘ,wgenerated from training on
some data {(xℓ, yℓ)}N
ℓ=1, we can define the training error as
ˆR(Θ)≜1
NNX
ℓ=1|fΘ,w(xℓ)−yℓ|2. (C.59)
Moreover, as in our analysis in Appendix C.1, we rely on an approximation of the ground state prop-
ertytr(Oρ(x))by a sum of smooth local functionsP
PαPfP(x)(Lemma 2). Namely, combining
Lemma 2 and Theorem 8, we have that for ϵ1>0, then choosing δ1>0as in Equation (A.5), i.e.,
δ1=O(log2(1/ϵ1)),X
PαPfP(x)−tr(Oρ(x))≤ϵ1
2(C.60)
Note that here, again, we slightly alter the definition from Appendix A.1.2, where we do not include
the coefficient αPin the definition of fP(x). With these definitions, we have the following guarantee
on the prediction error.
Lemma 10 (Prediction error bound) .Let1/e > ϵ 1, ϵ2>0. Consider a tanh neural network
fΘ,w: [−1,1]m→Rwith architecture defined in Definition 7 with weights Θi≤Wmaxfor some
Wmax>0independent of the system size nand weights win the last layer. Suppose we train fΘ,won
data{(xℓ, yℓ)}N
ℓ=1of size N, where the xℓ’s form a low-discrepancy sequence with star-discrepancy
D∗
Nand|yℓ−tr(Oρ(xℓ))| ≤ϵ2. Then, we have
R(Θ)≤ˆR(Θ) +ϵ2
1
2+ϵ2
2+ (∥w∥1+∥w∥2
1)D∗
N·2O(polylog( WW max/ϵ1)). (C.61)
where ˜m=|IP|=O(log2(1/ϵ1))forIPdefined in Equation (A.2) .
Proof. Recall the definition of our neural network model in Definition 6. In particular, our model is
given by a function fΘ,w: [−1,1]m→Rdefined by
fΘ,w(x) =X
P∈S(geo)wPfθP
P(x), (C.62)
where we refer to fθP
P: [−1,1]˜m→Ras the local models. For fP(x) = tr( Pρ(χP(x)))as
considered in Equation (C.60), we can define the following quantities. Define the training error with
respect to this local approximation by
ˆRloc(Θ)≜1
NNX
ℓ=1fΘ,w(xℓ)−X
PαPfP(xℓ)2
(C.63)
Also define the prediction error with respect to the local approximation as
Rloc(Θ)≜ E
x∼U[−1,1]mfΘ,w(x)−X
PαPfP(x)2
, (C.64)
where x∼U[−1,1]mmeans that xis sampled according to the uniform distribution.
By Lemma 2, for our choice of δ1, we have Equation (C.60):
X
PαPfP(x)−tr(Oρ(x))≤ϵ1
2. (C.65)
By the triangle inequality, we can bound the prediction error as
R(Θ) = E
x∼U[−1,1]mfΘ,w(x)−X
PαPfP(x) +X
PαPfP(x)−tr(Oρ(x))2
≤Rloc(Θ) +ϵ2
1
4.
(C.66)
35By applying the reverse triangle inequality, we can further bound this as
R(Θ)≤ϵ2
1
4+ˆRloc(Θ) + |Rloc(Θ)−ˆRloc(Θ)| (C.67)
=ϵ2
1
4+ˆRloc(Θ) (C.68)
+E
x∼U[−1,1]mfΘ,w(x)−X
PαPfP(x)2
−1
NNX
ℓ=1fΘ,w(xℓ)−αPfP(xℓ)2(C.69)
=ϵ2
1
4+ˆRloc(Θ) (C.70)
+E
x∼U[−1,1]m X
PwPfθP
P(x)−αPfP(x)!2
−1
NNX
ℓ=1 X
PwPfθP
P(xℓ)−αPfP(xℓ)!2
(C.71)
We can expand the term in the expectation/sum as follows
 X
PwPfθP
P(x)−αPfP(x)!2
(C.72)
= X
PwPfθP
P(x)!2
−2 X
PwPfθP
P(x)! X
PαPfP(x)!
+ X
PαPfP(x)!2
(C.73)
=X
P1,P2wP1fθP1
P1(x)wP2fθP2
P2(x)−2wP1fθP1
P1(x)αP2fP2(x) +αP1fP1(x)αP2fP2(x). (C.74)
Plugging this into the absolute value term in Equation (C.71) and upper bounding it with the triangle
inequality, we have
|Rloc(Θ)−ˆRloc(Θ)| ≤X
P1,P2|wP1||wP2|E
x∼U[−1,1]m[fθP1
P1(x)fθP2
P2(x)]−1
NNX
ℓ=1fθP1
P1(xℓ)fθP2
P2(xℓ)
+ 2|wP1|αP2|E
x∼U[−1,1]m[fθP1
P1(x)fP2(x)]−1
NNX
ℓ=1fθP1
P1(xℓ)fP2(xℓ)
+|αP1||αP2|E
x∼U[−1,1]m[fP1(x)fP2(x)]−1
NNX
ℓ=1fP1(xℓ)fP2(xℓ).
(C.75)
Notice that in the expectation over [−1,1]m, we can replace this by an expectation over the set of
local parameters, i.e., the parameters with coordinates in IP1∪IP2, which we denote by SP1,P2.
This is because the functions in the expectations are local functions that only depend on these local
parameters. The dimension of the domain we integrate over thus becomes independent of the system
sizen, as|SP1,P2| ≤2 ˜m= 2|IP|.
We can now bound this term further using the Koksma-Hlawka inequality (Theorem 10). We apply
a simple variable transformation τ(x) = 2 x−1so that the domain of fP◦τbecomes [0,1]˜m.
Furthermore, we denote the domain associated with SP1,P2byΩP1,P2≜[0,1]|SP1,P2|. Starting with
the first term in Equation (C.75), we obtain
E
x∼U[0,1]m[fθP1
P1(τ(¯x))fθP2
P2(τ(¯x))]−1
NNX
ℓ=1fθP1
P1(τ(¯xℓ))fθP2
P2(τ(¯xℓ))(C.76)
=E
x∼U(ΩP1,P2)[fθP1
P1(τ(¯x))fθP2
P2(τ(¯x))]−1
NNX
ℓ=1fθP1
P1(τ(¯xℓ))fθP2
P2(τ(¯xℓ))(C.77)
36=Z
SP1,P2fθP1
P1(τ(¯x))fθP2
P2(τ(¯x))dx−1
NNX
ℓ=1fθP1
P1(τ(¯xℓ))fθP2
P2(τ(¯xℓ))(C.78)
≤D∗
N(2 ˜m)VHK
(fθP1
P1·fθP2
P2)◦τ
, (C.79)
where ¯xℓ=τ−1(xℓ), such that Equation (C.76) matches the expression referenced in Equation (C.75).
Note that we also applied in the last step that the star-discrepancy is increasing with respect to the
dimension of the sequence. By application of the chain rule and the Cauchy-Schwartz inequality in
the definition of the Hardy-Krause variation (Equation (A.36)), it is easy to see that
VHK
(fθP1
P1·fθP2
P2)◦τ
≤22 ˜mVHK(fθP1
P1·fθP2
P2). (C.80)
For all subsets S′⊆SP1,P2, applying the product rule yields
∂|S′|
∂xS′(fθP1
P1·fθP2
P2)≤X
A⊆S′∂|A|
∂xAfθP1
P1∂|S′\A|
∂xS′\AfθP2
P2= 2O( ˜mlog(WW max)+ ˜m2log( ˜m)),(C.81)
where the last equality follows from applying Lemma 9 from Appendix C.4 with d=k= 2 ˜mand
|{A:A⊆S′}|= 22 ˜m. Here, we are using the notation∂|B|
∂xBto denote the mixed derivative with
respect to all parameters xi∈Bfor some set B. Thus, applying Lemma 9 again, we obtain
VHK(fθP1
P1·fθP2
P2)≤X
S′⊆SP1,P2∂|S′|
∂xS′(fθP1
P1·fθP2
P2)= 2O( ˜mlog(WW max)+ ˜m2log( ˜m)).(C.82)
Thus, putting it all together, we see that
E
x∼U[−1,1]m[fθP1
P1(x)fθP2
P2(x)]−1
NNX
ℓ=1fθP1
P1(xℓ)fθP2
P2(xℓ)(C.83)
≤22 ˜mD∗
N(2 ˜m)2O( ˜mlog(WW max)+ ˜m2log( ˜m)). (C.84)
The remaining terms in Equation (C.75) can be bounded similarly using Lemma 22 from Ap-
pendix C.4. This lemma is applicable to fPbecause the derivatives are with respect to the local
parameters. In this way, we can upper bound Equation (C.75) by
|Rloc(Θ)−ˆRloc(Θ)| ≤X
P1,P2
(|wP1||wP2|+|wP1||αP2|)2O( ˜mlog(WW max)+ ˜m2log( ˜m))(C.85)
+|αP1||αP2|2O( ˜mlog( ˜m))
D∗
N(2 ˜m). (C.86)
Plugging this back in to Equation (C.67), we have
R(Θ)≤ϵ2
1
4+ˆRloc(Θ) +X
P1,P2
(|wP1||wP2|+|wP1||αP2|)2O( ˜mlog(WW max)+ ˜m2log( ˜m))(C.87)
+|αP1||αP2|2O( ˜mlog( ˜m))
D∗
N(2 ˜m). (C.88)
Lastly, we can bound ˆRloc(Θ)≤ϵ2
1/4 +ϵ2
2+ˆR(Θ)in the same way as in Equation (C.66):
ˆRloc(Θ) =1
NNX
ℓ=1fΘ,w(xℓ)−X
PαPfP(χP(xℓ))2
(C.89)
≤1
NNX
ℓ=1fΘ,w(xℓ)−yℓ2+|yℓ−tr(Oρ(xℓ))|2+tr(Oρ(xℓ))−X
PαPfP(χP(xℓ))2
(C.90)
≤ˆR(Θ) + ϵ2
2+ϵ2
1
4. (C.91)
37Inserting ˜m=|IP|=O(polylog (1 /ϵ1))and using thatP
P|αP|=O(1)(Theorem 8) yields
R(Θ)≤ˆR(Θ) +ϵ2
1
2+ϵ2
2+ (∥w∥1+∥w∥2
1)D∗
N(2 ˜m)2O(polylog( WW max/ϵ1)). (C.92)
Using the previous result and the results from low-discrepancy theory (see Appendix A.2 for a
review), we can now show that Algorithm 1 will, under mild assumptions for training, output a model,
which yields low prediction error. Thus, using Lemma 10, we can easily prove Theorem 14.
Proof of Theorem 14. By Theorem 9, we know that for Sobol sequences in base 2with points in
[0,1]d, the star-discrepancy is bounded by
D∗
N(d)≤C(d)log(N)d
N, (C.93)
where C(d)is a constant such that
C(d)<1
d!d
log(2 d)
. (C.94)
Since C(d) =o(1), there exists a constant C, such that C≥C(d)for all d >0. In our case, d= 2 ˜m,
so we have
D∗
N(2 ˜m)≤Clog(N)2 ˜m
N. (C.95)
Using the assumption that the training objective is not larger than ((ϵ1+ϵ2)2+ϵ3)/2, by Lemma 10,
we have
R(Θ∗) = E
x∼U[−1,1]m|fΘ∗,w∗(x)−tr(Oρ(x))|2(C.96)
≤ϵ2
1
2+ϵ2
2+(ϵ1+ϵ2)2+ϵ3
2+C′log(N)polylog(1 /ϵ1)2O(polylog( Wmax/ϵ1))
N(C.97)
≤2(ϵ1+ϵ2)2+ϵ3
2+C′log(N)polylog(1 /ϵ1)2O(polylog( Wmax/ϵ1))
N, (C.98)
where C′is a constant. We also used here that ˜m=|IP|=O(polylog(1 /ϵ1). Since the training
data has size N=O 
2polylog(1 /ϵ1)+polylog(1 /ϵ3)
,Wmaxcan be chosen with respect to ϵ1, ϵ3and
independent of the system size nsuch that
C′log(N)polylog(1 /ϵ1)2O(polylog( Wmax/ϵ1))
N≤ϵ3
4. (C.99)
In this way, we obtain
R(Θ∗)≤2(ϵ1+ϵ2)2+ϵ3. (C.100)
Since the training objective from Definition 7 is non-convex, we cannot guarantee that our algorithm
converges to a neural network with low training error. However, the assumptions made in Theorem 14
are rather mild in practice. Small training errors are a well-known phenomenon in deep learning and
usually come at the expense of a larger prediction error, which is referred to as overfitting . Overfitting
may arise due to excessive model complexity [ 89], i.e. too many trainable parameters. This is
reflected by Lemma 10, since the generalization error increases with the width Wof the layers.
The major challenge in practice lies in finding an appropriate balance between achieving a small
training objective and model complexity, rather than only the latter. Furthermore, when the inputs are
regularized, the weights usually remain small during training when initialized properly. This was for
example observed in [53].
Finally, it is worth noting that in a scenario with a constant number of parameters m=O(1), similar
to the setup in [ 51], the expression derived from the outcome in Lemma 10 exhibits nearly linear
dependence on ϵ. When incorporating the constant number of parameters by setting ˜m=m, we
38recover the exact ground state properties tr(Pρ(x))infP. Thus, ϵ1in Lemma 10 becomes 0. Hence,
the ability of LDS training to overcome the curse of dimensionality can unfold its full potential,
since the domain dimension becomes independent of ϵand expression Equation (C.88) reduces to a
constant multiplied by D∗
N(2m). By Equation (C.95), we obtain R(Θ) = O(ϵ−(1+δ))for any δ >0
andϵsmall enough, when the conditions of Theorem 5 are fulfilled.
C.3 Prediction on general distributions
In this section, we generalize our results to hold for a wider class of distributions. Recall that our
rigorous guarantee proven so far (Theorem 14) holds when the training data is generated according
to a low-discrepancy sequence and the prediction error is measured with respect to the uniform
distribution. We want to extend this result for different choices of both training and prediction error
distributions. Notice that our prediction error bound (Lemma 10) is the only place that requires
these assumptions on the distributions. Thus, in this section, we establish bounds on the expected
prediction error for a more general family of distributions. We consider the following two cases.
1.The training data is generated according to a general low-discrepancy sequence (in the
sense of Definitions 4 and 5), and the prediction error is measured with respect to some
distribution D.
2.The training data consists of independently and identically distributed (i.i.d.) random
samples according to a distribution D, and the prediction error is measured with respect to
the same distribution D.
There are some conditions on the distributions that we discuss shortly. In Case 1, suppose for
example that we want to provide rigorous guarantees on the prediction error when the parameters
x∈[−1,1]mare sampled from a standard normal distribution (restricted to [−1,1]mand normalized
appropriately). As normally distributed test samples are more densely populated around the mean and
more sparse around the boundary of the input domain, we need to predict more accurately around the
mean than close to the boundary. When using a uniform low-discrepancy sequence for training, as in
Algorithm 1, the predictive capabilities of our model are not exploited properly. To remedy this, we
consider the training data to form a general low-discrepancy sequence, where it is low-discrepancy
with respect to a normal distribution. We can relate this general low-discrepancy sequence to an
LDS with respect to the Lebesgue measure , which are the sequences considered in Appendix C.2,
via the probability integral transform (see, e.g., [ 90]). We sometimes refer to LDS with respect to
the Lebesgue measure as uniform low-discrepancy sequences. Formally, for any random variable X,
which follows some probability distribution P(X≥x)≜FX(x), the random variable Y=FX(X)
follows a uniform distribution. It turns out that the same transformation on LDS produces LDS with
respect to other measures than the Lebesgue measure, as illustrated in Figure 3. Moreover, under
some assumptions on the distribution, we can bound the discrepancy with respect to other measures
in terms of the discrepancy with respect to the Lebesgue measure, which we know how to bound as
in Appendix C.2.
In the following, we formalize this argument and adapt it to our problem setting. We refer the reader
to Appendix A.2 to review the necessary concepts of generalized (star-)discrepancy, the Koksma-
Hlawka inequality, and related results. Then, we demonstrate that a generalization of Lemma 10 and
Theorem 14 can be achieved by incorporating these findings with slight adjustments to the proofs.
In Case 2, we consider training data sampled i.i.d. from some distribution Dand prediction error
measured with respect to the same distribution D. To obtain a rigorous guarantee on the prediction
error in this case, we leverage a probabilistic bound on the discrepancy of uniformly random points
from [ 82]. Utilizing the previously established framework from Case 1, we can bound the discrepancy
of points sampled from Din terms of the discrepancy of uniformly random points. This allows us to
establish similar guarantees for Case 2.
Before proving each of these cases, we set up our probabilistic framework and define the Borel
measure with respect to which our low-discrepancy sequence is defined. Let g≜PDF(D)be the
probability density function (PDF) of the data distribution and let G≜CDF(D)be the corresponding
cumulative distribution function (CDF). In the following, assume that the PDF gsatisfies the following
properties.
391.00
 0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
x11.00
0.75
0.50
0.25
0.000.250.500.751.00x2Transformed LDS
x
1(x)
Figure 3: Transformed low-discrepancy sequences. The blue circles correspond to two-dimensional
uniform Sobol points x. The orange triangles indicate the corresponding Sobol points with respect
to the CDF of the standard normal distribution, denoted by Φ. The latter forms a low-discrepancy-
sequence with respect to the Borel measure µ= Φ.
(a)Strict positivity: ghas full support on [−1,1]m, i.e.,g(x)>0ifx∈[−1,1]mandg(x) = 0
otherwise.
(b)Continuity: g(x)is continuously differentiable on [−1,1]m.
(c)Component-wise independence : The (random) variables ⃗ xi, ⃗ xjupon which different local
terms hi(⃗ xi), hj(⃗ xj)of the Hamiltonian depend on, are independent. Hence, the PDF gis
of the form
g(x) =LY
j=1gj(⃗ xj) (C.101)
for PDFs gj.
We implicitly assume that galso satisfies all properties of a probability density function. It should be
noted that Assumptions (a), (b) could technically be relaxed. We expand more on this later. Notice
that if g: [−1,1]m→Rmeets these requirements, the same holds for ¯g≜g◦τ: [0,1]m→R. Here,
we use the notation from the previous section, where a bar denotes that we are working in the domain
[0,1]mas opposed to [−1,1]m, andτ(¯(x)) = 2¯ x−1. Since the available results hold on [0,1]m, we
will mostly work with ¯gand the corresponding CDF ¯G.
We continue to set up the necessary notation to formally state our prediction error bound for Case 1.
LetSP1,P2,ΩP1,P2be as in the proof of Lemma 10. Namely, let SP1,P2be the parameters with
coordinates in IP1∪IP2, where IPis defined in Equation (A.2), and let ΩP1,P2= [0,1]|SP1,P2|.
Additionally, define µP1,P2≜Q
j∈SP1,P2¯Gj(⃗¯xj)as the probability measure of the marginal for all
(random) variables with indices in SP1,P2. Due to Assumption (c), µP1,P2depends on at most 2 ˜m
variables. Furthermore, we define
µ∗≜arg max
µPi,PjDN(|SPi,Pj|;µPi,Pj), (C.102)
and denote by S∗the corresponding coordinate set. S∗forms the domain of µ∗, and we use d∗to
denote the dimension of the domain.
40In both Case 1 and Case 2, the idea is to define a transformation Fthat maps random variables with
an arbitrary distribution to uniformly random variables. Namely, we construct a mapping ϕsuch that
E
x∼D[u(x)] = E
x∼U[−1,1]m[u(ϕ(x))] (C.103)
for any function u. In the following, we introduce the transform F≜ϕ−1, as has been introduced in
[91,88,92].Fcan nicely be characterized using ¯gand¯G, and assumptions on Fare easy to verify
for a given data distribution. In fact, if Fsatisfies a Lipschitz condition, then known results [ 88]
bound the discrepancy with respect to an arbitrary measure in terms of the discrepancy with respect
to the Lebesgue measure, i.e., we can directly upper-bound DN(d∗;µ∗)in terms of DN(d∗). Our
prediction error bound for more general distributions follows from this result and the results from
Appendix C.2.
Letg∗be defined such that dµ∗(x) =g∗(x)dx. Also, let A, B⊆S∗be such that A∩B=∅and
C=S∗\(A∪B). Then, we define the conditional marginal PDF as
g∗(xA|XB=xB)≜R
[0,1]|C|g∗(x)dxC
R
[0,1]|A|+|C|R(xB)1
0···R(xB)|B|
0g∗(x)dx(C.104)
and the corresponding CDF as
G∗(XA=xA|XB=xB)≜Z(xA)1
0···Z(xA)|A|
0g∗(xA|xB)dxA. (C.105)
For convenience, we refer to the indices of xinS∗viax1, x2, . . . , x d∗. We can do this without loss
of generality by permuting the order of the parameters. Using these definitions, we can now define
the reverse transformation as F: [0,1]d∗→[0,1]d∗, where the indices of Fare given by
Fj(x)≜G∗(Xj=xj|X1=x1, . . . , X j−1=xj−1). (C.106)
If random variables are distributed as X∼G∗, then F(X)∼U[−1,1]d∗(or equivalently U[0,1]d∗
under the variable transformation τ(x) = 2 x−1), since X1,X2|X1,. . .,Xd∗|X1, . . . , X d∗−1are
independent andY
jFj(X) =G∗(X). (C.107)
Finally, with this notation set up, we can formally state our result for Case 1.
Corollary 6 (Neural network sample complexity guarantee; generalization of Theorem 14) .Let
1/e > ϵ 1, ϵ2, ϵ3>0, and let Dbe a distribution with PDF gfulfilling assumptions (a)-(c) and F
according to Equation (C.106) . Let fΘ∗,w∗: [−1,1]m→Rbe a neural network model produced
from Algorithm 1 trained on data {(ˆxℓ,ˆyℓ)}N
ℓ=1of size
N= 2O(polylog(1 /ϵ1)+polylog(1 /ϵ3)), (C.108)
where the xℓ’s form a low-discrepancy Sobol sequence, ˆxℓ=F−1(xℓ)and|ˆyℓ−tr(Oρ(ˆxℓ))| ≤ϵ2.
Suppose that fΘ∗,w∗achieves a training error of at most ((ϵ1+ϵ2)2+ϵ3)/2. Additionally, suppose
that all parameters Θ∗
ioffΘ∗,w∗satisfy |Θ∗
i| ≤Wmax, for some Wmax>0that is independent of
the system size n. Then the neural network fΘ∗,w∗achieves prediction error
E
x∼D|fΘ∗,w∗(x)−tr(Oρ(x))|2≤2(ϵ1+ϵ2)2+ϵ3. (C.109)
Similarly, we also have a guarantee for Case 2, which is the version we state in the main text and the
beginning of this appendix.
Corollary 7 (Neural network sample complexity guarantee; generalization of Corollary 6 for random
data) .Let1/e > ϵ 1, ϵ2, ϵ3>0,Da distribution with PDF gsatisfying assumptions (a)-(c). Let
fΘ∗,w∗: [−1,1]m→Rbe a neural network model produced from Algorithm 1 trained on data
{(xℓ, yℓ)}N
ℓ=1of size
N= log(1 /δ)2O(polylog(1 /ϵ1)+polylog(1 /ϵ3)), (C.110)
41where the xℓ∼ D and|yℓ−tr(Oρ(xℓ))| ≤ϵ2. Suppose that fΘ∗,w∗achieves a training error
of at most ((ϵ1+ϵ2)2+ϵ3)/2. Additionally, suppose that all parameters Θ∗
ioffΘ∗,w∗satisfy
|Θ∗
i| ≤Wmax, for some Wmax>0that is independent of the system size n. Then the neural network
fΘ∗,w∗achieves prediction error
E
x∼D|fΘ∗,w∗(x)−tr(Oρ(x))|2≤2(ϵ1+ϵ2)2+ϵ3, (C.111)
with probability at least 1−δ.
We first prove Corollary 6 similarly to how we proved Theorem 14. In particular, we can prove a
generalized version of Lemma 10, where we are given a low-discrepancy sequence with respect to µ∗
and wish to bound the prediction error with respect to D, as in Case 1. Define the prediction error of
a neural network fΘ,wwith weights given by Θ, wwith respect to a distribution Das
RD(Θ)≜E
x∼D|fΘ,w(x)−tr(Oρ(x))|2=Z
[−1,1]m|fΘ,w(x)−tr(Oρ(x))|2dG(x),(C.112)
where x∼ D denotes xsampled from the distribution Dover[−1,1]manddG(x) =g(x)dx. Again,
we suppress win the notation to avoid cluttering. Then, we have the following lemma.
Lemma 11 (Generalized prediction error bound) .Let1/e > ϵ 1, ϵ2>0. Consider a tanh neural
network fΘ,w: [−1,1]m→Rwith architecture defined in Definition 7 with weights Θi≤Wmax
for some Wmax>0independent of the system size nand weights win the last layer. Assume
thatGsatisfies assumptions (a)-(c) and |yℓ−tr(Oρ(xℓ))| ≤ϵ2. Furthermore, suppose we train
fΘ,won data {(xℓ, yℓ)}N
ℓ=1of size N, where the τ−1(xℓ)’s from a set with star-discrepancy at most
D∗
N(d;µ∗)in each dimension d. Then, we have
RD(Θ)≤ˆR(Θ) +ϵ2
1
2+ϵ2
2+ (∥w∥1+∥w∥2
1)D∗
N(d∗;µ∗)·2O(polylog( WW max/ϵ1)).(C.113)
Moreover, if there exist constants b1, b2such that D∗
N(d)≤b1p
b2+ log(1 /δ)q
d
Nwith probability
at least 1−δ, then there exists a constant ˜b1such that
RD(Θ)≤ˆR(Θ)+ϵ2
1
2+ϵ2
2+(∥w∥1+∥w∥2
1)˜b1p
1 + log(1 /δ)r
˜m
N·2O(polylog( WW max/ϵ1))(C.114)
with probability at least 1−δ.
This lemma can be proven in the same fashion as Lemma 10 with two minor adjustments. One change
is the use of a generalization of the Koksma-Hlawka inequality, which we discuss in Theorem 11 in
Appendix A.2. To prove the second part of Lemma 11, we need a technical lemma (Lemma 14) to
handle the probability of failure in the bound on the star-discrepancy. We relegate this to the end of
the section, as it is mainly a technicality.
Proof. We proceed in the same way as in Lemma 10, replacing R(Θ)withRD(Θ)and replacing
Rloc(Θ)with
Rloc,D(Θ)≜E
x∼DfΘ,w(x)−X
PαPfP(x)2
. (C.115)
We follow the proof of Lemma 10 until Equation (C.75). This gives us
RD(Θ)≤ϵ2
1
4+ˆRloc(Θ) + |Rloc,D(Θ)−ˆRloc(Θ)|, (C.116)
where recall that
ˆRloc(Θ) =1
NNX
ℓ=1fΘ,w(xℓ)−X
PαPfP(xℓ)2
, (C.117)
as in Equation (C.63). Moreover, we also have the adjusted version of Equation (C.75)
|Rloc,D(Θ)−ˆRloc(Θ)| ≤X
P1,P2|wP1||wP2|E
x∼D[fθP1
P1(x)fθP2
P2(x)]−1
NNX
ℓ=1fθP1
P1(xℓ)fθP2
P2(xℓ)
42+ 2|wP1||αP2|E
x∼D[fθP1
P1(x)fP2(x)]−1
NNX
ℓ=1fθP1
P1(xℓ)fP2(xℓ)
+|αP1||αP2|E
x∼D[fP1(x)fP2(x)]−1
NNX
ℓ=1fP1(xℓ)fP2(xℓ).(C.118)
To bound the first term, we use the generalized Koksma-Hlawka inequality (Theorem 11) to obtain
E
x∼D[fθP1
P1(x)fθP2
P2(x)]−1
NNX
ℓ=1fθP1
P1(xℓ)fθP2
P2(xℓ)(C.119)
=Z
[−1,1]m|fΘ,w(x)−tr(Oρ(x))|2dG(x)−1
NNX
ℓ=1fθP1
P1(xℓ)fθP2
P2(xℓ)(C.120)
=Z
[0,1]mfθP1
P1(τ(¯x))fθP2
P2(τ(¯x))LY
i=1¯gi(⃗¯xi)d¯x−1
NNX
ℓ=1fθP1
P1(τ(¯xℓ))fθP2
P2(τ(¯xℓ))(C.121)
=Z
ΩP1,P2fθP1
P1(τ(¯x))fθP2
P2(τ(¯x))dµP1,P2(¯x)−1
NNX
ℓ=1fθP1
P1(τ(¯xℓ))fθP2
P2(τ(¯xℓ))(C.122)
≤D∗
N(d∗;µ∗)VHK
(fθP1
P1·fθP2
P2)◦τ
. (C.123)
Here, in the first equality, we use Assumption (c) on the structure of the PDF gand use the variable
transformation τ(x) = 2 x−1. In the second equality, similarly to Lemma 10, we notice that because
the functions in the expectation are local functions that only depend on parameters in SP1,P2, we
can replace the expectation over the whole domain [0,1]mwith an expectation over just the domain
ΩP1,P2. This step also crucially uses Assumption (c), where the factorization of the PDF gdue to
independence is needed. The last line uses the generalized Koksma-Hlawka inequality (Theorem 11).
The remainder of the proof follows in the same way as Lemma 10.
The second part of the statement is a direct consequence of Lemma 14. Specifically, following the
proof of Lemma 10, we use Lemma 14 to bound the term in Equation (C.88). This is necessary
because the upper bound on the star-discrepancy only holds probabilistically, so we must show that we
can still use this upper bound on the sum of several star-discrepancy terms. This is more complicated
than a simple union bound, and we relegate the proof and statement of Lemma 14 to the end of this
section.
In addition to this lemma, we also need a result from [ 88], adapted to our definitions above. In the
following, we use DN(ω;d)to denote the discrepancy with respect to the Lebesgue measure of a
specific sequence ωof length Nand dimension d, as in Definition 1. Similarly, we use DN(ω;d;µ)
to denote the discrepancy with respect to a measure µof a specific sequence ωof length Nand
dimension d, as in Definition 4.
Lemma 12 (Theorem 2 in [ 88]).Letω={xℓ}N
ℓ=1be an arbitrary sequence on the open d-
dimensional unit cube with discrepancy DN(ω, d), and let ˆω={ˆxℓ}N
ℓ=1be the sequence defined by
Fˆxℓ=xℓ, where Fis defined in Equation (C.106) . Moreover, let gbe a strictly positive, d-times
continuously differentiable PDF , such that g(x)≥m > 0for all x. Let Gbe the corresponding
probability measure (i.e., CDF). Furthermore, let Fsatisfy
∥F(x)−F(y)∥ ≤K∥x−y∥. (C.124)
Then, the discrepancy of ˆωwith respect to Gis bounded as
DN(ˆω;d;G)≤c(DN(ω;d))1
d, (C.125)
where c= 2d·3d(K+ 1)d−1.
The authors in [ 88] note that the assumption on Fin Equation (C.124) is certainly fulfilled when gis
continuously differentiable. This is where Assumption (b) is used, where technically, we only require
this Lipschitz condition on F. With Lemma 11 and Lemma 12, we are ready to prove Corollary 6.
43Proof of Corollary 6. We proceed similarly as in the proof of Theorem 14 but this time using
Lemma 11 instead of Lemma 10. First, we bound the nonuniform discrepancy of our training
inputs ˆω={ˆxℓ}N
ℓ=1. Recall that ˆxℓ=F−1(xℓ)forxℓgenerated according to a low-discrepancy
Sobol sequence (i.e., low-discrepancy with respect to the Lebesgue measure). By definition of F,
thenˆωhas star-discrepancy D∗
N(ˆω;d∗;µ∗). By Assumption (b), we can apply Lemma 12 to obtain
D∗
N(ˆω;d∗;µ∗)≤DN(ˆω;d∗;µ∗)≤c(DN(ω;d∗))1
d∗≤2d∗c(D∗
N(ω;d∗))1
d∗. (C.126)
Here, the first inequality follows because D∗
N(d)≤DN(d), and the second follows by Lemma 12.
Finally, the last inequality follows from DN(d)≤2dD∗
N(d)(see, e.g., [ 93]). Because d∗≤2 ˜m, we
can proceed as in the proof of Theorem 14 using the bound above.
By Theorem 9, we know that for Sobol sequences in base 2with points in [0,1]d∗, the star-discrepancy
is bounded by
D∗
N(d∗)≤C(d∗)log(N)d∗
N, (C.127)
where C(d)is a constant such that
C(d)<1
d!d
log(2 d)
. (C.128)
Since C(d) =o(1), there exists a constant C, such that C≥C(d)for all d > 0. Using the
assumption that the training objective is not larger than ((ϵ1+ϵ2)2+ϵ3)/2, by Lemma 11, we have
RD(Θ∗) = E
x∼D|fΘ∗,w∗(x)−tr(Oρ(x))|2(C.129)
≤ϵ2
1
2+ϵ2
2+(ϵ1+ϵ2)2+ϵ3
2+C′log(N)2O(polylog( Wmax/ϵ1))
N1/d∗ (C.130)
≤2(ϵ1+ϵ2)2+ϵ3
2+C′log(N)2O(polylog( Wmax/ϵ1))
N1/polylog(1 /ϵ1), (C.131)
where C′is a constant. We also used here that d∗≤2 ˜mand˜m=|IP|=O(polylog(1 /ϵ1). Since
the training data has size N=O 
2polylog(1 /ϵ1)+polylog(1 /ϵ3)
,Wmaxcan be chosen with respect to
ϵ1, ϵ3and independent of the system size nsuch that
C′′log(N)2O(polylog( Wmax/ϵ1))
N1/polylog(1 /ϵ1)≤ϵ3
4(C.132)
for some constant C′′. In this way, we obtain
RD(Θ∗)≤2(ϵ1+ϵ2)2+ϵ3. (C.133)
Finally, we prove Case 2, where the training data is sampled i.i.d. according to a distribution D
and the prediction error is also measured with respect to D. Note that we can drop the assumption
thatF−1is efficiently computable for this case. The key result we need for this is a bound on the
star-discrepancy for uniformly random points (Lemma 5).
Proof of Corollary 7. Letˆxℓ=Fxℓ, where Fis as in Equation (C.106). As stated in [ 91,92],F
transforms random variables with distribution Dinto standard uniform random variables. Hence,
similarly to the proof of Corollary 6, if ω={xℓ}N
ℓ=1has star-discrepancy D∗
N(ω;d∗;µ∗), we can
bound it with respect to the discrepancy of ˆω={ˆxℓ}N
ℓ=1:
D∗
N(ω;d∗;µ∗)≤DN(ω;d∗;µ∗)≤c(DN(ˆω;d∗))1
d∗≤2d∗c(DN(ˆω;d∗))1
d∗. (C.134)
Here, again we use D∗
N(d)≤DN(d)≤2dD∗
N(d)and Lemma 12. Then, by Lemma 5, for uniformly
random points, i.e., ˆω={ˆxℓ}N
ℓ=1, we have
D∗
N(d)≤5.7p
4.9 + log(1 /δ)r
d
N(C.135)
with probability at least 1−δ. The rest of the proof follows in the same way as Corollary 6, but using
the above discrepancy bound. Note that because this discrepancy bound only holds probabilistically,
we need to use the second part of Lemma 11.
44Our prediction error bounds for Case 1 and Case 2 (in Corollaries 6 and 7, respectively) look rather
similar. However, one can verify that Corollary 6 only requires about square root of the number of
samples Corollary 7 uses to achieve a certain risk bound with small enough ϵ, but this advantage
is hidden in the polylogarithmic factors in the exponent. Hence, low-discrepancy data yields better
theoretical guarantees. However, they did not yield an improvement in our numerical experiments, as
discussed in Appendix D. The size Nof the training set seems to be very large for low-discrepancy
data to have practical effects. In the main text, we present Corollary 7 because it is the more general
theoretical statement.
In fact, one can also improve the polylogarithmic factors in Corollary 6 by imposing stronger
assumptions on the distribution. In particular, the result in Lemma 12 seems surprisingly weak at
first glance. Multidimensional transformations do, however, constitute a major challenge, since they
generally do not preserve properties such as lines remaining straight or parallel. The boxes over
which one optimizes in order to compute the discrepancy can thus change severely in shape, which
can strongly alter the discrepancy and makes it difficult to analyze. This can result in a rather poor
scaling in terms of the discrepancy with respect to the Lebesgue measure and thus N. However, when
gfulfills additional assumptions, we obtain a much better dependence on Nby directly applying the
Koksma-Hlawka inequality (with respect to the Lebesgue measure) to f◦ϕ. Unsurprisingly, this is
possible when the mixed derivative of F−1=ϕis bounded on [0,1]. This follows, when g’s mixed
derivative is bounded [88]. We restate this result below.
Lemma 13 (Theorem 1 in [ 88]).Letω={xℓ}N
ℓ=1of size Nbe an arbitrary sequence on the open
d-dimensional unit cube with discrepancy DN(ω, d)andˆω={ˆxℓ}N
ℓ=1the sequence defined by
Fˆxℓ=xℓ, where Fis defined in Equation (C.106) . Moreover, let gbe a strictly positive, d-times
continuously differentiable PDF , such that g(x)≥m > 0for all x. Let Gbe the corresponding
probability measure (i.e., CDF). Furthermore, let Fsatisfy
∂|A|Fj
∂xA≤M 1≤j≤d, A ⊆ {1, . . . , d }. (C.136)
ThenZ
[0,1d]f(ˆx)dG(ˆx)−1
NNX
ℓ=1f(ˆxℓ)≤d!M
m2d−1
D∗
N(ω;d)VHK(f). (C.137)
On a high level, the proof works via the observation that the Jacobian of Fhasgas its determinant,
which is strictly positive. Since F◦ϕis the identity, one can write the Jacobian of F◦ϕas a linear
system of equations with the derivatives of ϕas solution. Using Cramer’s rule and the assumption on
F, one can upper bound the derivative of ϕ. Applying this iteratively, one can show via induction
that the mixed derivatives of ϕare also bounded when the mixed derivatives of Fare bounded. Note
that (as stated in [ 88]) when Dis a product of independent distributions, i.e. g(x) =Qm
i=1gi(xi)
and fulfills assumptions (a)-(c), the conditions for Lemma 13 are also fulfilled. It is important to
emphasize that the additional assumption used in Lemma 13 yield a much better dependence of ϵon
N. However, this improvement is hidden in the polylogarithmic factors.
We dedicate the last part of this section to the proof the following statement, which we used in the
proof of Lemma 11. At a high level, this shows that when we have a probabilistic upper bound on the
star-discrepancy, we can still upper bound a sum of star-discrepancies with high probability.
Lemma 14. Suppose there exist constants b1, b2such that D∗
N(d)≤b1p
b2+ log(1 /δ)q
d
Nwith
probability 1−δ. Then, there exists a constant ˜b1, such that for any t >0
Pr
X
P1,P2∈S(geo)(c1|αP1||αP2|+c2(|wP1||αP2|+|wP1||wP2|))D∗
N(xP1,P2)≥t
 (C.138)
≤exp 
−Nt2
˜b1(c1∥α∥2
1+c2(∥w∥1∥α∥1+∥w∥2
1))2!
, (C.139)
where recall SP1,P2is the set of parameters with coordinates in IP1∪IP2(Equation (A.2) ),c1=
2O( ˜mlog( ˜m))andc2= 2O( ˜mlog(WW max)+ ˜m2log( ˜m)). Thus D∗
N(xP1,P2)denotes the star-discrepancy
of this set of parameters in the training data.
45First, we introduce two useful tools for the proof.
Theorem 17 (Azuma’s Inequality for Martingales with Subgaussian Tails; Adapted from Theorem
2 in [ 94]).LetZ1, Z2, . . . , Z nbe a martingale difference sequence with respect to a sequence
X1, X2, . . . , X n, and suppose there are constants b >1,c1, . . . , c n>0, such that for any jand any
t >0it holds that
Pr(Zj> t|X1, . . . , X j−1)≤bexp 
−t2
c2
j!
. (C.140)
Then, it holds that
Pr
nX
j=1Zj> t
≤exp 
−t2
28bPn
j=1c2
j!
. (C.141)
Proof of Theorem 17. Following the steps of the proof of Theorem 2 in [ 94], but taking the sum over
Zjinstead of the empirical average, we obtain for any s >0
Pr
nX
j=1Zj> t
≤e−ste7bc2
ns2E
nY
j=1esZjX1, . . . , X n−1
≤e−st+7bs2Pn
j=1c2
j.(C.142)
We refer to [ 94] for further details of this calculation. Choosing s=t/
14bPn
j=1c2
j
, the expres-
sion above equals e−t2/(28bPn
j=1c2
j), and we get the claim:
Pr
nX
j=1Zj> t
≤exp 
−t2
28bPn
j=1c2
j!
. (C.143)
We also need the following two small lemmas.
Lemma 15. Letδ >0. LetX: ΩX→ X andY: ΩY→ Y be independent random variables and
f:X × Y → R+be a function such that PrXY(f(X, Y)≥t)≤δfort >0. Then,
E[f(X, Y)|X]≤t
2(C.144)
with probability at least 1−2δ.
Proof. First, we show that Pr(f(X, Y)≥t|X)≥1/2with high probability. Then, we show that
this implies the claim by Markov’s inequality.
First, suppose for the sake of contradiction that Pr 
E[ 1{f(X, Y)≥t}|X]≥1
2
>2δ. Using the
independence of XandYapplying Markov’s inequality, we obtain
Pr(f(X, Y)≥t) =E[E[ 1{f(X, Y)≥t}|X]]≥1
2Pr
E[ 1{f(X, Y)≥t}|X]≥1
2
>2δ·1
2=δ,
(C.145)
which contradicts our initial assumption. Therefore, with probability at most 2δ(w.r.t. X),
Pr(f(X, Y)≥t|X)≥1
2and hence
1
2≤Pr(f(X, Y)≥t|X)≤E[f(X, Y)|X]
t(C.146)
by Markov’s inequality. The result follows immediately.
Lemma 16. Letj∈[m]be a coordinate of the parameters. Then, |{P∈S(geo):j∈IP}|=tildem ,
where ˜m=O(|IP|).
46Proof. Recall that
IP={c∈ {1, . . . , m }:dobs(hj(c), P)≤δ1}. (C.147)
Fixing cinstead of Palso results in a set of geometrically local terms in a radius δ1around a
geometrically local term. Hence, the size of the set {P∈S(geo):j∈IP}also scales as |IP|, which
is at most ˜m.
Now we are able to provide a partial proof to Lemma 14.
Lemma 17. LetSP1,P2be the set of parameters with coordinates in IP1∪IP2and let xP1,P2≜
{{x∈SP1,P2}ℓ}N
ℓ=1denote the training data set only for these local parameters. If there exist
constants b1, b2such that D∗
N(d)≤b1p
b2+ log(1 /δ)q
d
Nwith probability at least 1−δ, then
Pr
X
P2∈S(geo)|αP2|D∗
N(xP1,P2)≥t
≤exp
−Nt2
224∥α∥2
2( ˜m)2exp(b1)b2
(C.148)
for any P1∈S(geo)and any t >0.
Proof. LetP1∈S(geo). Define
Xj≜E
X
P2∈S(geo)|αP2|D∗
N(xP1,P2)Yj−1, . . . , Y 0
, (C.149)
where we omit the dependence xP1,P2=xP1,P2(Y1, . . . , Y m)andYj≜{(xj)ℓ}N
ℓ=1andxjparam-
eterize hj. We consider all increments, which are not contained in IP1. Hence, with slight abuse
of notation, let index j= 0 refer to all coordinates in IP1andY0≜{{(xj) :j∈IP1}ℓ}N
ℓ=1.
Furthermore, let
X0≜E
X
P2∈S(geo)|αP2|D∗
N(xP1,P2)Y0
 (C.150)
andZ1≜X1−X0. Clearly, X0, . . . , X mis a martingale sequence and Z1, . . . , Z mthe respective
martingale difference sequence. Furthermore, note that Xm=P
P2∈S(geo)|αP2|D∗
N(xP1,P2)and by
definition of Y0,j /∈IP1for all j >0. Now, since D∗
N(xP1,P2)≥0and|αP2|D∗
N(xP1,P2)cancel
out if j /∈IP2,
Zj≤E
X
P2∈S(geo):j∈IP2\IP1|αP2|D∗
N(xP1,P2)Yj−1, . . . , Y 0
 (C.151)
=X
P2∈S(geo):j∈IP2\IP1|αP2|E
D∗
N(xP1,P2)Yj−1, . . . , Y 0
(C.152)
=X
P2∈S(geo):j∈IP2\IP1|αP2|E
D∗
N(xP1,P2)(Yk)k<j,k∈IP1
. (C.153)
Then, for any t >0, we have
Pr(Zj≥t)≤Pr
X
P2∈S(geo):j∈IP2\IP1|αP2|E
D∗
N(xP1,P2)(Yk)k<j,k∈IP1∪IP2
≥t
(C.154)
≤X
P2∈S(geo):j∈IP2Pr
E
D∗
N(xP1,P2)(Yk)k<j,k∈IP1∪IP2
≥t
|αP2|
(C.155)
≤2X
P2∈S(geo):j∈IP2Pr
D∗
N(xP1,P2)≥t
2|αP2|
(C.156)
47≤2X
P2∈S(geo):j∈IP2exp(b1) exp
−Nt2
4|αP2|2b2˜m
(C.157)
≤2|{P2∈S(geo):j∈IP2}|exp(b1) exp 
−Nt2
4b2˜mP
P2∈S(geo):j∈IP2|αP2|2!
(C.158)
≤2 ˜mexp(b1) exp 
−Nt2
4b2˜mP
P2∈S(geo):j∈IP2|αP2|2!
, (C.159)
In the second line, we use a union bound. In the third line, we use Lemma 15 with X=
(Yk)k<j,k∈IP1∪IP2andY= (Y0, . . . , Y j−1). In the fourth line, we use a rearrangement of the
probabilistic upper bound on the star-discrepancy. In the last line, we use the definition of ˜m. Now,
by Theorem 17, for any t >0
Pr
X
P2∈S(geo)|αP2|D∗
N(xP1,P2)≥t
≤exp
−t2
28b′Pm
i=1c2
i
(C.160)
withb′= 2 ˜mexp(b1)andc2
i=4b2˜m
NP
P2∈S(geo):i∈IP2|αP2|2. Furthermore,
mX
i=1X
P2∈S(geo):i∈IP2|αP2|2=X
P2∈S(geo)|αP2|2mX
i=11{i∈IP2}= ˜mX
P2∈S(geo)|αP2|2= ˜m∥α∥2
2.
(C.161)
The result follows from this.
Now, we are finally able to prove Lemma 14.
Proof of Lemma 14. We need to bound the weighted sum of the star-discrepancies we consider.
This requires an extra step, since the star-discrepancy may vary among the sequences in the sum.
Note that simply applying the union bound would result in a log(n)-factor. Luckily, only the sum
needs to be small, rather than all individual terms needing to be small at once. Recall that we
useSP1,P2to denote the set of parameters with coordinates in IP1∪IP2. In the following, we use
D∗
N(xP1,P2)to denote the star-discrepancy of {x∈SP1,P2}N
ℓ=1, i.e., the training data points restricted
to local parameters in SP1,P2. We aim to apply Theorem 17 toP
P1,P2∈S(geo)|αP1||αP2|D∗
N(xP1,P2),P
P1,P2|wP1||αP2|D∗
N(xP1,P2)andP
P1,P2∈S(geo)|wP1||wP2|D∗
N(xP1,P2)).
For illustrative purposes, we only consider the first term for now. We proceed similarly to the proof
of Lemma 17. Define
Xj≜E
X
P1,P2∈S(geo)|αP1||αP2|D∗
N(xP1,P2)Yj−1, . . . , Y 1
, (C.162)
where the expectation is with respect to the inputs Yj≜{(xj)ℓ}N
ℓ=1andxjparametrize hj. Further-
more, let
X0≜E
X
P1,P2∈S(geo)|αP1||αP2|D∗
N(xSP1,P2)
 (C.163)
andZ1≜X1−X0. Clearly, X0, . . . , X mis a martingale sequence and Z1, . . . , Z mthe respective
martingale difference sequence. Furthermore, Xm=P
P1,P2∈S(geo)|αP1||αP2|D∗
N(xP1,P2). Now,
since D∗
N(xP1,P2)≥0and|αP1||αP2|D∗
N(xP1,P2)cancel out if j /∈IP1∪IP2,
Zj≥E
X
P1,P2∈S(geo):j∈IP1∪IP2|αP1||αP2|D∗
N(xP1,P2)Yj−1, . . . , Y 1
 (C.164)
=X
P1,P2∈S(geo):j∈IP1∪IP2|αP1||αP2|E
D∗
N(xP1,P2)Yj−1, . . . , Y 1
(C.165)
48= 2X
P1∈S(geo):j∈IP1X
P2∈S(geo)|αP1||αP2|E
D∗
N(xP1,P2)Yj−1, . . . , Y 1
. (C.166)
In the last step, we used the observation that for jto be contained in IP1∪IP2, it has to be contained
in at least one of the two sets. Hence, we can enumerate the admissible coordinate sets by fixing P1,
such that IP1contains jand combine it with all IP2. The factor two arises from doing the same with
P1when fixing P2.
Now, for any t >0, we have
Pr(Zj≥t)≤Pr
2X
P1∈S(geo):j∈IP1X
P2∈S(geo)|αP1||αP2|E
D∗
N(xP1,P2)Yj−1, . . . , Y 1
≥t

(C.167)
≤2X
P1∈S(geo):j∈IP1Pr
X
P2∈S(geo)|αP2|D∗
N(xP1,P2)≥t
4|αP1|
 (C.168)
≤2X
P1∈S(geo):j∈IP1exp
−Nt2
16·224|αP1|2∥α∥2
2( ˜m)2exp(b1)b2
(C.169)
≤2|{P1∈S(geo):j∈IP1}|exp 
−Nt2
16·224∥α∥2
2( ˜m)2exp(b1)b2P
P1∈S(geo):j∈IP1|αP1|2!
(C.170)
≤2 ˜mexp 
−Nt2
16·224∥α∥2
2( ˜m)2exp(b1)b2P
P2∈S(geo):j∈IP1|αP1|2!
. (C.171)
In second line, we use the union bound and Lemma 15 with X= (Yk)k<j,k∈IP1∪IP2andY=
(Yk)k>j,k∈IP1∪IP2. In the third line, we use Lemma 17. In the last line, we use the definition of ˜m.
Applying Theorem 17 and boundingP
jc2
jexactly as in the proof of Lemma 17 yields
Pr
X
P1,P2∈S(geo)|αP1||αP2|D∗
N(xP1,P2)≥t
≤exp 
−Nt2
˜b1∥α∥4
2!
. (C.172)
One can similarly repeat this argument for the remaining terms of
X
P1,P2∈S(geo)(c1|αP1||αP2|+c2(|wP1||αP2|+|wP1||wP2|))D∗
N(xP1,P2)). (C.173)
Using that ∥α∥2
2≤ ∥α∥2
1and solving for the appropriate δyields the desired result.
C.4 Bound on the mixed derivatives
LetO=P
P∈{I,X,Y,Z }⊗nαPPbe an observable that can be written as a sum of geometrically
local observables. In the following, we derive an expression for the mixed partial derivatives of
tr(Pρ(x)), using tools from the spectral flow formalism [ 59,60,61]. This allows us to bound the
Hardy-Krause variation (Equation (A.36)) in Appendix C.2. Let the spectral gap of H(x)be lower
bounded by some constant γfor all choices of parameters x∈[−1,1]m. Then, by the spectral flow
formalism [ 59,60,61], the directional derivative of a ground state of H(x)in the direction defined
by the parameter unit vector ˆuis given by
∂
∂ˆuρ(x) = ˆu· ∇xρ(x) =i[Dˆu(x), ρ(x)] (C.174)
where
Dˆu(x) =Z+∞
−∞Wγ(t)eitH(x)∂H
∂ˆu(x)e−itH(x)dt, (C.175)
49andWγ(t)is defined by
|Wγ(t)| ≤(1
20≤γ|t| ≤θ,
35e2(γ|t|)4e−2
7γ|t|
log2(γ|t|)γ|t|> θ.(C.176)
The parameter θis chosen to be the largest real solution of 35e2(γ|t|)4exp
−2
7γ|t|
log2(γ|t|)
= 1/2.
This allows to us to obtain an expression of the first order derivative of ρ(x)with respect to some
parameter xk. Consider the unit vector ˆu= ˆek≜(0, . . .0,1,0, . . .0)T, where the 1is in the kth
position. Then, the directional derivative in the direction given by ekis
∂
∂ˆekρ(x) = ˆek· ∇xρ(x) =∂
∂xkρ(x) =i[Dˆek(x), ρ(x)]. (C.177)
Hence, we obtain
∂
∂x1tr(Pρ(x)) = tr
P∂
∂x1ρ(x)
=itr(P[Dˆe1(x), ρ(x)]) = itr([P, D ˆe1(x)]ρ(x)).(C.178)
In order to compute the mixed derivative of second order, we now apply the product rule to this
expression, which yields
∂2
∂x1∂x2αPtr(Pρ(x)) =∂
∂x2iαPtr([P, D ˆe1(x)]ρ(x)) (C.179)
=iαP
tr
P,∂
∂x2Dˆe1(x)
ρ(x)
−tr
[P, D ˆe1(x)]∂
∂x2ρ(x)
(C.180)
=αPtr
i
P,∂
∂x2Dˆe1(x)
ρ(x)
−tr([[P, D ˆe1(x)], Dˆe2(x)]ρ(x)).
(C.181)
Note that the terms of this expression can be treated similarly as the first partial derivative. For each
additional partial derivative, we obtain terms consisting of the product with nested commutators with
ρ(x)under the trace. The nested commutators contain Dˆej(x)or partial derivatives of it, for which
we will later derive an explicit form. Hence, we can apply the same scheme until we arrive at the k-th
partial derivative. In order to formalize this statement, we need to introduce some additional notation.
Throughout the rest of this section, we use the notation∂|B|
∂xBto denote the mixed derivative with
respect to all parameters xi∈Bfor some set B.
Definition 8. Letk∈N. LetA⊆[k]be a set of size |A|=m. Define an ordering l1< l2<···< lm
over the elements l1, l2, . . . , l m∈AofA. Then, we define
‚K
l∈A∂Bll≜imtr ""
···""
P,∂|Bl1|
∂xBl1Dˆel1(x)#
,∂|Bl2|
∂xBl2Dˆel2(x)#
···#
,∂|Blm|
∂xBlmDˆelm(x)#
ρ(x)!
,
(C.182)
where Bj⊂[k]. We refer to the nested commutators under the trace as summands . The set Aand
collection {Bl}l∈A≜BAsatisfy the following conditions:
1. Each summand contains Dˆe1(x).
2. The sets A, B 1, . . . , B msatisfy A∪Sm
j=1Bj= [k]andA∩Bj=∅,Bi∩Bj=∅.
3. For each (Bl, l)pair, it holds that i > l for all i∈Bl.
This notation gives a compact way of expressing the terms of the mixed derivative and allows us
to address each mixed derivative of the terms Dˆejindividually. Each term ‚J
l∈A∂Bllcontains the
product of m+ 1matrix-valued functions (including ρ(x), which depend on x. The set Adenotes the
partial derivatives on the factor ρ(x), which have been differentiated using Equation (C.174) when
50applying the product rule. We will address the partial derivatives on Dˆejlater in this section, when
we derive an upper bound for ‚J
l∈A∂Bll.
The first condition underlines that the first partial derivative on ρ(x)is necessarily computed via
Equation (C.174) and thus contained in each term. The second condition reflects that each partial
derivative operates on exactly one factor in each summand when applying the product rule. The third
condition arises from the order, by which the partial derivatives are computed. For example, when we
apply∂
∂x′
jafter∂
∂xj, the∂
∂xjDˆe′
jcan not occur in any term, since no term contained Dˆe′
jwhen the
partial derivatives∂
∂xjwere computed.
We can show that the mixed partial derivatives of αPtr(Pρ(x))can be written in terms of ‚J
l∈A∂Bll.
Lemma 18 (Mixed derivative) .LetAk={A⊆[k] : 1∈A}andBAbe as in Definition 8. The
mixed derivative of the ground state property tr(Pρ(x))is given by
∂k
∂x1. . . ∂x kαPtr(Pρ(x)) =αPX
A∈AkX
(B1,...,B|A|)∈BA‚K
l∈A∂Bll. (C.183)
Proof. We proceed via induction. First, we verify that∂
∂xk‚J
l∈A∂BllwithA∈ A k−1andA∪Sm
j=1Bj= [k−1]yield summands which fulfill the criteria for summands of the k-th partial
derivative stated in Definition 8. Then, we show that each summand of the k-th derivative stems from
a unique summand from the (k−1)-th partial derivative.
For the first part, let |A|=m. Furthermore, let
Is(j) ={j}ifs=k
∅ else. (C.184)
Then,
∂
∂xk‚K
l∈A∂Bll=mX
j=1‚K
lj∈A∂Blj∪Ij(l)l+ ‚K
l∈A∪{k}∂Bll, (C.185)
where each summand fulfills the properties, since k > l for all l∈A.
Next, let A′∈ Akand let BA′be the corresponding collection. Then, it is easy to see that, if k∈Blj,
it stems from the summand ‚J
l∈A′∂BllwithBl1, . . . , B lj\ {k}, . . . , B lm. Ifk∈A′, it stems from
‚J
l∈A\{k}∂Bll.
With this expression in hand, we can move forward to bounding the mixed derivative, which we need
in order to bound the Hardy-Krause variation. First, we will upper bound the number of terms in
‚J
l∈A∂Bll. Then, we derive an upper bound on the individual terms. For the first step, we exploit
the conditions on the sets defining the mixed derivative. When we drop the third requirement in
definition Definition 8, |BA|corresponds to the number of ways of assigning k−mdistinct balls to
mbins. Thus, we obtain the following result on the number of terms ‚J
l∈A∂Bllin the k-th mixed
derivative.
Lemma 19. LetAdenote the set of all subsets of [k]. Then, the number of summands in the expression
‚J
l∈A∂Bllis upper bounded by
|BA| ≤kX
s=1k
s
sk−s. (C.186)
Proof. There are |A|=Pk
s=1 k
s
different subsets of [k]. For each set Awith|A|=s, there are s
setsBl. Dropping the third condition in Definition 8, we observe that each of the k−selements in
[k]\Acan be in any of the ssets. Thus, we obtain the claimed upper bound.
In the next step, we aim to bound each individual term of the mixed derivative ‚J
l∈A∂Bll. Therefore,
a crucial step is to bound the spectral norm of each factor. We first derive a preliminary result on the
51mixed derivatives of the factors in Dˆej(x), which depend on x. This can be done using Duhamel’s
Formula for the derivative of the exponential map on eH(x), where we exploit that we only compute
the derivative with respect to one parameter at a time, such that we can treat H(x)as a function,
which only depends on one parameter.
Theorem 18 (Derivative of the exponential map; Theorem 3a in [ 95])).LetA(t) :R→Cn×n. Then,
d
dteA(t)=Z1
0e(1−s)A(t)dA(t)
dt
esA(t)ds. (C.187)
Lemma 20. Letk∈[n],B⊆[n]\ {k}, such that∂|C|hj
∂xC
∞≤1∀C⊆B∪ {k}. Then
∂|B|
∂xB
eitH(x)∂hj
∂xk
e−itH(x)
∞≤2|B|+1(|B|+ 1)|B|+1(C.188)
Proof. By Theorem 18, the mixed derivative equals the sum of terms of the form
T=Z1
0···Z1
0Y
lfl(sl)dsl. . . ds 1, (C.189)
where fl(sl)can be any of e(1−sl)iH(x),esliH(x),∂lhj
∂xBlor1. By our assumption and the Cauchy-
Schwartz inequality, each term Tsatisfies ∥T∥∞≤1. Furthermore, by the product rule, the number
of terms is smaller thanQ|B|
j=1(2j+ 1) . Since each term of the lth partial derivative (including k) is
the product of at most 2l+ 1factors depending on x, such that the (l+ 1)th derivative contains at
most 2l+ 1-times as many factors. Thus, the number of terms is bounded above by
|B|Y
j=1(2j+ 1)≤|B|+1Y
j=1(2j) = 2nn!≤2|B|+1(|B|+ 1)|B|+1, (C.190)
as required.
Now we can bound the terms ‚J
l∈A∂Bll.
Lemma 21 (Bound components of the derivative) .Let‚J
l∈A∂Bllbe as in Definition 8. Then
‚K
l∈A∂Bll≤(2Cγ)|A||A|Y
s=12|Bls|+1(|Bls|+ 1)|Bls|+1. (C.191)
Proof. Recall that
Dˆu(x) =Z+∞
−∞Wγ(t)eitH(x)∂H
∂ˆu(x)e−itH(x)dt, (C.192)
where Wγ(t), such that
|Wγ(t)| ≤(1
20≤γ|t| ≤θ,
35e2(γ|t|)4e−2
7γ|t|
log2(γ|t|)γ|t|> θ,(C.193)
where θis chosen to be the largest real solution of 35e2(γ|t|)4exp
−2
7γ|t|
log2(γ|t|)
= 1/2. It is also
useful to note that supt|Wγ(t)|= 1/2.
By definition of the terms ‚J
l∈A∂Bll, the Cauchy-Schwartz inequality, and ∥[A, B]∥∞≤
2∥A∥∞∥B∥∞, we obtain
‚K
l∈A∂Bll≤Z+∞
−∞|Wγ(t)|dt|A|
2|A||A|Y
s=1sup
t∂|Bls|
∂xBls
eitH(x)∂hjs
∂xs
e−itH(x)
∞.
(C.194)
52We bound each term individually. For the first term, we proceed in a similar manner as in [ 2] (Lemma
3). Namely, by Equation (S32) in [2], we can bound this integral by
Z+∞
t∗|Wγ(t)|dt≤245
2e2γ−1
1
1−35 log2(γt∗)
γt∗
(γt∗)10e−2
7γt∗
log2(γt∗)≜C′
γ, (C.195)
by choosing t∗such that γt∗= max(5900 , α,7(d+ 11) , θ)for some constant α. Here, we use C′
γ
to denote a constant that depends only on γ. Moreover, since |Wγ(t)| ≤1
2, we can conclude that
Z+∞
−∞|Wγ(t)|dt≤Zt∗
−t∗1
2dt+ 2Z+∞
t∗|Wγ(t)|dt≤max(5900 , α,7(d+ 11) , θ)
γ+ 2C′
γ≜Cγ,
(C.196)
where Cγis also a constant that only depends on γ. By Lemma 20, we obtain the desired statement.
Lemma 22 (Bounding the k-th mixed derivative) .Thek-th mixed derivative of αPtr(Pρ(x))is
bounded by∂k
∂x1. . . ∂x kαPtr(Pρ(x))≤ |αP|2O(klog(k))(C.197)
Proof. First, we derive an upper bound on the terms ‚J
l∈A∂Bll, which is independent of AandBA.
Proceeding from the result of Lemma 21, we obtain
(2Cγ)|A||A|Y
s=12|Bls|+1(|Bls|+ 1)|Bls|+1≤(2Cγ)|A|2k|A|Y
s=1k|Bls|+1≤(C1k)k, (C.198)
where we used |A| ≤kandP
l(|Bl|+ 1) = kandC1= 4Cγ. Furthermore, from Lemma 19, it is
easy to see that |BA| ≤kk. Thus, we obtain
∂k
∂x1. . . ∂x kαPtr(Pρ(x))≤ |BA||αP|(C1k)k≤ |αP|Ck
1k2k=|αP|2O(klog(k)). (C.199)
Note that when deriving this result, we do not require that the parameters for the mixed derivatives
are distinct. Assuming that ∥H(x)∥Wk,∞([−1,1]m)≤1, we can induce an order recover the above
bound for any mixed derivative of order k.
Corollary 8. If∥H(x)∥Wk,∞([−1,1]m)≤1,then∥αPtr(Pρ(x))∥Wk,∞≤ |αP|2O(klog(k)).
Proof. Note that the bound from Lemma 22 is agnostic to the explicit directions ˆejof the derivatives.
Thus, we can choose any mixed derivative λ∈Nk
0such thatPk
j=1λj=kand fix an order
o: dom( λ)→[k]. Then, we can bound the mixed derivative on o(λ)using the same approach as in
Lemma 22 to obtain the bound.
D Details of numerical experiments
In this section, we discuss the numerical experiments in detail.
D.1 Experimental setup
As in [ 2], we consider the two-dimensional antiferromagnetic Heisenberg model with spin- 1/2
particles placed on sites in a two-dimensional lattice. The corresponding Hamiltonian is
H=X
⟨ij⟩Jij(XiXj+YiYj+ZiZj), (D.1)
53where ⟨ij⟩denotes all pairs of neighboring sites on the lattice. The coupling terms Jijcorrespond to
the parameters xof the Hamiltonian and are sampled uniformly from [0,2](and then mapped to lie
in[−1,1]for our ML algorithm). The goal of the numerical experiment is to predict the two-body
correlation functions, i.e., the expectation value of
Cij=1
3(XiXj+YiYj+ZiZj) (D.2)
for all neighboring sites ⟨ij⟩.
To this end, we generate data similarly to [ 1,2], approximating the ground state and corresponding
correlation functions for the Hamiltonian Equation (D.1) of different lattice sizes and choices of
coupling parameters Jij. We consider lattice sizes of 4×5 = 20 up to 9×5 = 45 . For each
lattice size, we generate two datasets of size 4096 , one with uniformly randomly distributed Jijand
one where the coupling parameters are distributed as a Sobol sequence. We obtained the data by
approximating the ground state using the density-matrix renormalization group (DMRG) [ 96] based
on matrix-product-states (MPS) [ 97], as has been done in [ 1,2]. The simulations were performed on
Nvidia T4 and A40 graphical processing units (GPUs). The former were used for lattice sizes from
4×5up to 7×5while the latter were used for lattice sizes 8×5and9×5. Depending on system
size, we required between ≈50and200hours on the respective hardware component to simulate
one dataset of size 4096 .
Our deep learning model was also trained on Nvidia T4 and A40 GPUs. We trained the models for
all respective correlation terms in parallel, by training a full model fΘ,w
ij(we omit the indices for the
model’s parameters) for each term and minimizing the combined loss function
X
⟨ij⟩NX
ℓ=1|fΘ,w
ij(xℓ)−(Cij)ℓ|2(D.3)
for the sake of time efficiency. For each data point, we trained a combined model for 500epochs.
For the terms of the local models fθP
P, as defined in Definition 6, we used fully connected deep
neural networks with five hidden layers of width 200. For training, we used the AdamW optimization
algorithm [ 83]. Depending on the system size and the amount of training data, this took between 0.5
and20hours. As a baseline, we compared against the best model from [ 2]. The code can be found at
https://github.com/marcwannerchalmers/learning_ground_states.git .
D.2 Additional experiments and discussion
In this section, we discuss the results of the numerical experiments and additional experiments
performed that are not mentioned in the main text.
First, we perform additional experiments that analyze the scaling of the training/prediction error
with respect to various parameters such as system size, local neighborhood size, and training set size
(Figures 4 to 6). Importantly, in each of these, we see that the training error is small, as required by
Theorem 5. Thus, as discussed in the main text, this assumption is satisfied in practice.
Moreover, as shown in Figure 2 (Left), the empirical prediction accuracy (RMSE) of the deep
learning model is approximately constant with respect to the size of the lattice. Figure 4 (Right)
further underlines this statement. The slight increase in prediction error for δ1>0(size of the local
neighborhood in Equation (A.2)) present in Figure 4 (Right) when increasing the system size from
4×5to5×5may occur due to numerical errors in the data. From system size 5×5onwards, we
rather witness random fluctuations in test errors than a systematic increase.
Furthermore, we observe that the deep learning model significantly outperforms the regression model
with random Fourier features from [ 2]. On the one hand, we notice that the performance of the latter
could be improved, since the hyperparameters considered for hyperparameter tuning were selected for
a substantially smaller dataset. This is underlined by the drop in RMSE for the regression model on
Figure 5 for δ1= 1, whereas a smaller RMSE is possible when choosing δ1= 0. On the other hand,
we think that the vast body of deep learning research also offers room for practical improvement of
our deep learning model.
Forδ1= 0, we believe that our model achieves the best possible prediction error. For training set size
larger than 2048 , there is little improvement on the prediction error, as opposed to all experiments
544x5 5x5 6x5 7x5 8x5 9x5
System size (n)0.000.020.040.060.08Average training error
1
0
1
2
3
4
5
4x5 5x5 6x5 7x5 8x5 9x5
System size (n)0.000.020.040.060.08Average prediction error
1
0
1
2
3
4
5Figure 4: Training/Prediction Error vs. System Size. This figure shows the scaling of the training
(left) and prediction (right) RMSE with respect to system size for different values of δ1. All training
sets are distributed as Sobol sequences and were trained on N= 3686 samples. The shaded areas
denote the 1-sigma error bars across the assessed ground state properties.
1 2 3 4 5
1
0.000.010.020.030.040.050.060.07Average training error
Algorithm
Deep Learning, LDS
Regression, LDS
Deep Learning, Random
Regression, Random
1 2 3 4 5
1
0.000.010.020.030.040.050.060.07Average prediction error
Algorithm
Deep Learning, LDS
Regression, LDS
Deep Learning, Random
Regression, Random
Figure 5: Training/Prediction Error vs. Local Neighborhood Size. This figure shows the scaling
of the training (left) and prediction (right) RMSE with respect to the local neighborhood size δ1. All
training sets are of size N= 3686 with system size 9×5. The shaded areas denote the 1-sigma error
bars across the assessed ground state properties.
withδ1>0(see Figure 2 (Center)). Furthermore, the training error remains relatively large compared
to other choices of δ1(see Figure 4). Hence, we conclude that the error arising from approximating
the ground state property via local functions dominates the prediction error.
When increasing δ1, we witness an increase in prediction error, especially for small training sets. This
is consistent with Lemma 10, which states that the bound on the prediction error is a combination
of the training error and a term proportional to the star-discrepancy (and thus increases with the
dimension of the domain of the local models). Our experimental results underline the balance which
must be achieved between the two in order to obtain a small prediction error. This can clearly be
observed in Figure 6. The training error decreases when increasing δ1and increases with the size of
the training set. Meanwhile, the test error increases when increasing δ1and decreases with the size of
the training set.
Another interesting observation is that the ML algorithm’s performance on LDS seem to be almost
the same as that of uniformly random points. We believe this is due to the dominance of the local
approximation error for small δ1and the drastic increase in dimensionality of the local models with
increasing δ1outweighing the benefit of using LDS in practice. The dominance of approximation
error is also a possible explanation for the slight decrease in prediction error with respect to the
system size in Figure 2 (Left) and Figure 5. For our concrete choice of lattice shape and ground state
properties, the local approximation error may be decreasing with respect to system size. However,
we do not expect this to be the case in general.
55409 1228 2048 2867 3686
Training set size (N)0.000.020.040.060.08Average training error
1
0
1
2
3
4
5
409 1228 2048 2867 3686
Training set size (N)0.000.020.040.060.08Average prediction error
1
0
1
2
3
4
5Figure 6: Training/Prediction Error vs. Training Set Size. This figure shows training (left) and
prediction (right) RMSE with respect to training set size for different values of δ1. All training sets
are distributed as Sobol sequences and the grid size is 9×5. The shaded areas denote the 1-sigma
error bars across the assessed ground state properties.
D.3 Experiments with non-geometrically-local Hamiltonians
In this section, we assess the necessity of the geometric locality assumption by conducting numerical
experiments for non-geometrically-local systems. We conclude that geometric locality is necessary
for our theoretical results.
We conduct experiments on for a Hamiltonian given by
H=X
j<iJij(XiXj+YiYj+ZiZj). (D.4)
The difference between this Hamiltonian and Equation (D.1) is that the sites iandjare not required
to be neighboring, thus violating the geomtric locality assumption needed for our rigorous guarantees.
We predict the same ground state properties as in the previous section, i.e., two-body correlation
functions on neighboring sites. Our ML model still uses the local coordinate set IPfrom Equa-
tion (2.3). However, notice that the non-geometric-locality of the terms in the Hamiltonian impacts
the number of parameters used. In other words, a larger number of parameters now affects a site in
the neighborhood of each local Pauli. Furthermore, our adapted ML model assumes observables with
2-local terms1. Hence, the adapted ML model reads
X
P∈S(2-local)fθP
P. (D.5)
Due to the lack of geometric locality and the larger number of terms of the Hamiltonian, the ground
state properties are substantially harder to simulate, compared to the previous ones. We limit ourselves
to uniformly random parameters and lattice shapes 4×5,5×5and6×5. The former two were
simulated on Nvidia T4 GPUs and the latter on Nvidia A40 GPUs, using approximately 100−500
hours per data set of size 4096 . We also notice that the approximation error due to MPS may be
larger in this dataset than in the previous one. As for the previous results, we trained the models for
each ground state property in parallel, by optimizing the sum of their training objectives. For the
local models fθP
P, we used fully connected neural networks with five hidden layers of width 100.
This may not be optimal, but sufficient for the purpose of assessing the scaling of the prediciton
error. We trained the models for different training set sizes using δ1= 0. Since the adapted models
consisted substantially more terms than the previous ones, training them for 500epochs took between
5and35hours on Nvidia T4 GPUs for lattice shapes 4×5and5×5and on Nvidia A40 GPUs on a
6×5-lattice.
In Figure 7 (Right), we witness system size-dependent prediction error for the smallest training set
size we investigate. Since the respective training error is very small, the respective prediction errors
arise due to overfitting. This effect diminishes for larger training sets. This is what one would expect
when directly applying the techniques of our theoretical results to this setting. Since the number
12-local in the sense that Pdoes not act on more than two not necessarily neighboring sites.
564x5 5x5 6x5
System size (n)0.00.10.20.30.40.5Average training error
Training set size (N)
409
1228
2048
2867
3686
4x5 5x5 6x5
System size (n)0.00.10.20.30.40.5Average prediction error
Training set size (N)
409
1228
2048
2867
3686Figure 7: Training/Prediction Error vs. System Size for Non-Geometrically-Local Systems. This
figure shows training (left) and prediction (right) RMSE with respect to the system size for the model
given in Equation (D.4), which violates geometric locality. All training sets are of size N= 3686 and
δ1= 0. The shaded areas denote the 1-sigma error bars across the assessed ground state properties.
of terms increases quadratically in system size, the norm of the weights in the final layer can not
be bounded by a constant anymore. Furthermore, the properties of the local approximation do not
hold true anymore. Hence, the predictive capabilities of a model with δ1= 0may be more limited
here than in the geometrically local case. However, the prediction error may also be impacted by
possible numerical errors in the training data, as well as the architecture of the local deep neural
networks. Overall, these experiments illustrate the necessity of the geometric locality assumption in
our theoretical results.
57NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We state, albeit informally, our results and assumptions in the abstract and
instruction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Section 3 includes a detailed discussion of the assumptions needed in the
paper.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
58Answer: [Yes]
Justification: The assumptions are detailed in Section 3. The appendices contain the full
proofs.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The code and data for the numerical experiments are provided in the supple-
mentary material. The details of the numerical experiments are also described in Section D
of the appendices.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
595.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code and data for the numerical experiments is provided in the supple-
mental material, and the details regarding the experiments are described in the Appendix
D.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: This is detailed in Appendix D.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Error bars are included in the numerical experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
60•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: This is detailed in Appendix D.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: This research conforms with the code of ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: There are no societal impacts of the work performed.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
61•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: When using data or parts of code from previous papers, this is cited.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
62•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
63