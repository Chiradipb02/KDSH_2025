Grounding and Validation of Algorithmic Recourse
in Real-World Contexts:
A Systematized Literature Review
Anonymous Author(s)
Affiliation
Address
email
Abstract
The aim of algorithmic recourse (AR) is generally understood to be the provision 1
of “actionable” recommendations to individuals affected by algorithmic decision- 2
making systems, in an attempt to offer the capacity for taking actions that may 3
lead to more desirable outcomes in the future. Over the past few years, AR 4
literature has largely focused on theoretical frameworks to generate “actionable” 5
counterfactual explanations that further satisfy various desiderata, such as diversity 6
or robustness. We believe that algorithmic recourse, by its nature, should be seen 7
as a practical problem: real-world socio-technical decision-making systems are 8
complex dynamic entities involving various actors (end users, domain experts, 9
civil servants, system owners, etc.) engaged in social and technical processes. 10
Thus, research needs to account for the specificities of systems where it would 11
be applied. To evaluate how authors envision AR “in the wild”, we carry out a 12
systematized review of 127 publications pertaining to the problem and identify the 13
real-world considerations that motivate them. Among others, we look at the ways 14
to make recourse (individually) actionable, the involved stakeholders, the perceived 15
challenges, and the availability of practitioner-friendly open-source codebases. 16
We find that there is a strong disconnect between the existing research and the 17
practical requirements for AR. Most importantly, the grounding and validation of 18
algorithmic recourse in real-world contexts remain underexplored. As an attempt 19
to bridge this gap, we provide other authors with five recommendations to make 20
future solutions easier to adapt to their potential real-world applications. 21
1 Introduction 22
Algorithmic decision-making (ADM) tools are frequently seen as a way to improve decision processes 23
in a variety of high-stakes domains such as public administration [ 47,146] or healthcare [ 45,87]. 24
Deep learning models have attracted much attention due to their perceived high performance, but 25
the predictions of such models cannot be interpreted by humans, hence end users – both individuals 26
subjected to algorithmic decisions and decision-makers operating on them – are placed in a position 27
where they are unable to understand the grounds of a prediction, act on it, or trust it [159]. 28
To help address this problem, a variety of explanation methods has been proposed. Of particular 29
interest for this paper are counterfactual explanations (CEs) that attempt to explain the predictions for 30
individual instances of data, taking the form of conditional statements such as “if the value of feature 31
xwasainstead of b, the model would have predicted class yinstead of z”. They are perceived to be 32
an attractive approach to explanation that does not require “opening the black box” [ 151] and have 33
been argued to align with the ways that humans naturally reason about events [84]. 34
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.CEs are also seen as the go-to method for algorithmic recourse (AR), or the generation of actionable 35
recommendations that provide people with the knowledge needed to achieve more desirable predic- 36
tions in ADM systems. Recourse is distinct from the “explanation” or “justification” of algorithmic 37
decisions, and more closely related to the notion of contestability of Artificial Intelligence [ 7] in that 38
it aims not only to improve the trust in the algorithm, but also embrace human agency [142]. 39
Algorithmic recourse is an inherently practical problem in that it resembles a bureaucratic complaint 40
process: an individual unhappy with some decision engages with a representative of the issuing 41
organization, in an attempt to overturn it. Yet, we observe that much of the existing work is highly 42
theoretical, with little consideration of whether it could be applied in organizational settings [see 43
also 18]. Deploying AR in realistic systems without analyzing its mechanics in a broader context 44
and without knowing what types of dynamics are expected to arise is bound to lead to unanticipated 45
outcomes. Many of them will be undesirable and even potentially unsafe, and impossible to validate 46
with respect to a set of requirements because the requirements for AR are necessarily socio-technical. 47
Societal and institutional components of algorithmic recourse are the focal point of our work , 48
as we look beyond the typical technical considerations to assess the practical aspects of the problem. 49
To that end, we contribute a systematized review of 127 publications that address the goals of 50
algorithmic recourse and we evaluate to what extent they incorporate such practical considerations. 51
We characterize our approach as systematized because we follow a fully systematic approach to the 52
collection of publications, but their selection is not necessarily exhaustive [ 46] as many impactful 53
ideas in computer science are published only in the form of pre-prints. Based on our analysis, we also 54
provide other authors with five recommendations on how to improve the practicality of AR research. 55
The rest of the manuscript is structured as follows. In Section 2 we elaborate on the background of 56
our work. Then, in Section 3 we describe our approach to this review. Next, Section 4 introduces 57
our findings. Section 5 provides a discussion of our results, introduces our recommendations, and 58
addresses the limitations of the current work. Finally, Section 6 forms the conclusion to this paper. 59
2 Background 60
2.1 On algorithmic recourse 61
Algorithmic – or actionable, individual – recourse was introduced in [ 138] as“the ability of a person 62
to change the decision of the model through actionable input variables” , building on the earlier 63
work of [ 151] who argued that CEs are a psychologically-grounded way to (1) help decision-subjects 64
understand an algorithmic decision, (2) provide them with information needed to contest it, and (3) 65
inform about actions that could be taken to overturn it. For instance, consider a person who has 66
unsuccessfully applied for a loan; they may then receive AR such as “if you requested $5000 less, 67
you would qualify for this loan” . The key consideration for AR is “actionability”, which entails that 68
the recipient of the recommendation should be capable of implementing it. If they had been informed 69
“if you were 10 years younger, you would qualify for the loan” , they would have still received a 70
valid CE, but not recourse. More recently [ 69] has recast the problem as reasoning about minimal 71
interventions on the structural causal model. This formulation (at least theoretically) addresses an 72
important shortcoming of “correlational” recourse. Without accounting for the downstream causal 73
effects of actions, an individual may exert more effort than necessary and still fail to achieve the 74
target outcome. Indeed, counterfactuals are an inherently causal concept [103]. 75
We note that problems similar to AR have been studied under a variety of different names: actionable 76
knowledge discovery [e.g., 2],action rules mining [e.g., 110],inverse classification [e.g., 5],why 77
not questions [e.g., 58], or actionable feature tweaking [134]. These alternative formulations have 78
generally focused on “business” knowledge, rather than individual recommendations, but ultimately 79
the goal of all these approaches is to extract information from a (black-box) model that allows the 80
user – an individual or a decision-maker – to act. We highlight them to emphasize that AR does 81
not have to be achieved through the means of CEs. Rather CEs should be seen as one of the means 82
to achieve AR, particularly promising in that they do not require expert-level understanding of the 83
model to be useful. Nonetheless, we decide to distinguish between the literature on AR (commonly 84
equated with actionable CEs), and these alternative formulations in our work. 85
Existing research has generally considered AR in simplistic settings that are far removed from 86
real-world socio-technical decision-making systems, where it would be implemented as a process. 87
2For example, such systems are dynamic [ 113,137], must support the implementation of AR at scale 88
[9,94], and involve various stakeholders beyond the end users [ 17,151]. Moreover, if the intended 89
goal of AR is to help individuals subjected to algorithmic decisions in an effective manner, research 90
must entail a rich understanding of “actionability” to account for the differences between them [ 142]. 91
2.2 On the position of our review 92
Several groups of authors have previously surveyed the landscape of counterfactual explanations in 93
general, and algorithmic recourse specifically. Perhaps the most relevant to our work is [ 71], which 94
discusses five deficits of research on CEs, with a special focus on the (lack of) psychological grounding. 95
Another pertinent publication is [ 70], which attempts to unify the definitions and formulations of 96
AR in existing literature, but the work primarily focuses on technical aspects. Next, [ 143] develops 97
a rubric to compare counterfactual explainers (equated with AR) and identifies 21 research challenges. 98
While these also remain mostly technical, several of them are relevant to our work, for instance, CEs 99
“as an interactive service to the applicants” or reinforcing “the ties between machine learning and 100
regulatory communities” . More recently, [ 48] reviewed and benchmarked a number of CE generators, 101
but AR is only a secondary consideration in the work. We also highlight [ 130], which is the only 102
systematic review of counterfactual and contrastive approaches to date. The authors understand CEs 103
as a way to justify model predictions (i.e., they are different from AR). We agree with this distinction 104
in that CEs can be useful for reasons other than recourse, such as model debugging [e.g., 1,122]. 105
Finally, although not reviews, [ 13] and [ 142] are particularly relevant to our work, offering critical 106
perspectives on AR and addressing multiple shortcomings of recourse literature. 107
3 Methods 108
In this section, we briefly discuss our approach to the literature review following the SALSA – Search, 109
Appraisal, Synthesis, Analysis – framework introduced in [ 46]. We also provide a more detailed 110
description to allow for the reproduction of our process in the supplementary materials. Figure 1 111
presents our process in the form of a PRISMA flow diagram [97]. 112
3.1 Search 113
We make use of three search engines to collect the initial set of studies: ACM Digital Library, IEEE 114
Xplore, and SCOPUS. Given the previously mentioned blurry distinction between AR and CEs, 115
we consider the papers discussing either problem. In a small scoping review, we identify several 116
keywords common to publications on recourse, as well as several equivalent terms to build the query. 117
We search in titles, abstracts, and keywords, arriving at 3092 records after de-duplication. To facilitate 118
the screening process, we employ the open-source ASReview tool, which makes use of an active 119
learning approach to re-order the set of publications, such that the most relevant ones are always 120
“at the top of the stack” [ 139]. The researchers behind the tool suggest employing a stopping rule 121
measured in the number of consecutive irrelevant records, which we set to 30, or 1% of the entire 122
dataset. We accept all papers that focus on algorithmic recourse and counterfactual explanations, 123
completing the screening after evaluating 1040 abstracts, leading to 499 relevant records. 124
We observe that some important publications may be missing from our results. For instance, [ 151] 125
was published in a legal journal that is not indexed by computer science search engines. Thus, we 126
decide to augment the set of records by applying snowballing, which has been shown as a good 127
alternative to databases in systematic reviews in software engineering [ 162]. We collect the references 128
for the top 50 (10%) “most impactful” publications, measured by the number of citations. While this 129
introduces several pre-prints into our result set [ 52,61,91,113,143,150], we decide not to exclude 130
them. Our review remains primarily concerned with peer-reviewed work. After adding the snowballed 131
references to our dataset, we are left with 2018 records for the second screening with ASReview. 132
This time, we look for publications that specifically refer to the problem of AR, “actionable” CEs, or 133
modifying outcomes of automated decision-making systems. We employ a stricter stopping rule to 134
minimize the risk of false negatives, completing the screening after 60 consecutive irrelevant records 135
with 203 records considered for full-text appraisal. To allow for complete reproducibility of the 136
search process, we provide an extended discussion (including queries) in the technical Appendix A. 137
33.2 Appraisal 138
We were able to retrieve all of the remaining 203 documents. For each document, we require that the 139
authors explicitly cite recourse as the center of interest, or look at (1)explanations (2)provided for 140
individual instances (3)with the goal of acting upon them (4)in an attempt to modify the predictions 141
(5)of a classification model. We exclude 51 publications as they are not on topic, primarily because 142
they focus on CEs for the sake of explanation. Four works in this category look at (what they 143
call) recourse but extend the problem to settings beyond the scope of this review: recommender 144
systems [ 31,43,145], text classification [ 37], and anomaly detection [ 27]. Further 15 publications 145
are duplicates, typically pre-prints of other documents that were included in the review. Next, 8 146
documents were published before [ 151] that sparked the research on AR, and thus we exclude them as 147
well. These look at the alternative formulations discussed earlier in Section 2.1. Finally, 2 documents 148
are not publications: one is an abstract of a talk, and the other is a student poster. For each document, 149
we answer a number of questions relating to the practical considerations introduced by the authors. 150
IdentificationRecords identified from:
•ACM Digital Library ( n= 1267 )
•IEEE Xplore ( n= 513 )
•SCOPUS ( n= 2139 )Records removed before screening:
•Duplicates ( n= 783 )
•Only meta-data ( n= 44 )ScreeningFirst screening by abstract ( n= 3092 ),
screening completed after 1041 recordsRecords excluded ( n= 2593 ),
after encountering 30irrelevant records
Records from snowballing ( n= 1519 ),
using top 10% of records by citation countRecords before second screening ( n= 2018 )
Second screening by abstract ( n= 2018 ),
screening completed after 538 recordsRecords excluded ( n= 1815 ),
after encountering 60irrelevant records
Records assessed for eligibility ( n= 203 ),
appraisal using the full textRecords excluded ( n= 76 ):
•Not on topic ( n= 51 )
•Duplicates ( n= 15 )
•Y ear of publication ( n= 8)
•Other formats ( n= 2)IncludedStudies included in the review ( n= 127 ):
3,4,6,8,9,12–16,19–22,24–26,28–30,34–36,38,39,41,42,44,48–57,60–67,69,70,72–83,
85,86,88–96,98–102,104–109,111–118,120,121,123–129,131–133,135–138,141–144,147–
158, 160, 161, 163–165
Figure 1: Identification of studies via databases and snowballing
3.3 Synthesis 151
To compile the results we carry out a standard thematic content analysis following the approach 152
presented in [ 40]. First, we explore the data extracted from the set of publications relevant to each 153
question to find the commonalities, which serves as the grounds for creating the initial set of codes. 154
We evaluate the documents against these codes and keep track of any other considerations. If such 155
considerations appear in multiple documents, we create new codes for them. Afterward, we re- 156
evaluate all documents against the new code. As the coding exercise is carried out by one author, they 157
do a third pass over all documents to double-check for potential errors. Finally, where relevant, we 158
cluster the codes into larger themes. In this analysis we only look at the explicit statements provided 159
by the authors, we do not attempt to infer their understanding of the problem. Thus, the numbers 160
provided in Section 4 should be understood as describing how algorithmic recourse is discussed in 161
the literature. For brevity, we focus our discussion on the main themes, but we still highlight specific 162
publications if we observe that the authors introduce novel, highly relevant considerations that do not 163
fit into other themes. Finally, even though we also evaluated the technical aspects of the proposed 164
solutions – requirements for methods and datasets used in evaluations – they are not covered in this 165
review. Instead, we point the interested readers to [48, 70, 143]. 166
44 Results 167
The following nine sections introduce the results of the thematic analysis. For each question, we 168
explain why it is relevant to the analysis and examine the main themes. We also highlight highly 169
important but underexplored themes. We start with the general points such as contributions and 170
definitions in Sections 4.1 to 4.3. Then, in Sections 4.4 to 4.7 we investigate the societal components 171
of AR research. Finally, in Sections 4.8 and 4.9 we look at the aspects relevant to practitioners. 172
4.1 What types of contributions do the authors choose to make to the AR research? 173
We start by looking at the main goals of the collected publications to validate our assumption that 174
AR literature is primarily concerned with technical solutions. We annotate each entry with at most 175
two codes based on the form of contributions. By far the largest group is propose methods , which 176
applies to 88 (69.3%) out of the 127 publications. These are primarily generators for individual CEs, 177
but we also find 18 (14.2%) documents that propose other methods. Next, 20 (15.7%) publications 178
develop theoretical frameworks , for instance by grounding AR in user studies or providing critical 179
perspectives on the problem. Further, 15 (11.8%) focus on empirical or theoretical analyses of the 180
properties of AR and another 15 publications apply it in a variety of domains. We did not identify 181
any applications evaluated with humans in the loop. Then, 5 (3.9%) publications benchmark existing 182
methods, while 3 (2.4%) review them. We make our annotations available in technical Appendix B. 183
4.2 What are the criteria covered in the authors’ definitions of AR? 184
We also evaluate what is understood as the problem to be addressed by AR mechanisms. In particular, 185
what are the criteria to satisfy authors’ definitions of recourse. A similar question was posed by [ 70] 186
who combined six definitions into “recourse can be achieved by an affected individual if they can 187
understand and accordingly act to alleviate an unfavorable situation, thus exercising temporally- 188
extended agency” , but this approach was far from systematic. Instead, we are interested in the 189
underlying concepts. 74 (58.3%) publications explicitly define AR, 16 (12.6%) mention it but do not 190
include a definition, while 37 (29.1%) do not mention AR, even though they align with its (overall) 191
goals. The most common theme is overturning undesirable decisions , present in 47 definitions (63.5% 192
of all definitions), but specifically overturning algorithmic decisions is mentioned only 43 (58.1%) 193
times. It is generally understood that AR is provided to affected individuals (44, or 59.5%) but 4 (5.4%) 194
definitions consider stakeholders more broadly. Actionability as a requirement for recourse is noted 195
in only 39 (52.7%) definitions. Then, 20 (27.0%) publications specifically mention counterfactual 196
explanations as means to AR, while 26 (35.1%) include various other technical considerations in the 197
definitions, such as “changes to actionable input variables” or “desired classes”. 198
We also point to several themes that are, interestingly, underrepresented. Only 18 (24.3%) documents 199
mention explanation, justification, or understanding of a decision as the pre-requisite for AR. Next, 200
10 (13.5%) highlight future-orientation or other temporal aspects of the provided recommendations. 201
Although “consequential settings” , typically bank lending, are given as examples in nine (12.2%) 202
definitions, they are never explicitly mentioned as the scenarios where recourse ought to be provided, 203
which may be akin to the “enjoyment of recourse” as defined by [ 142] where people are aware that 204
there exists a way to reverse undesirable decisions.18 publications (10.8%) promote AR as an ability . 205
Finally, only 2 (2.7%) publications require that recourse accounts for the preferences of its recipients. 206
4.3 What are the criteria covered in the authors’ definitions of actionability? 207
As we observe, “actionability” is a concept that underpins AR but we discover that, in general, its 208
understanding is limited. 91 (71.6%) publications attempt to define what it means (for a CE) to be 209
actionable. Most commonly, in 48 (52.7%) out of 91 definitions, it is understood as acting only on 210
directly-mutable features , 6 (6.6%) distinguish that features may be indirectly-mutable but still not 211
actionable, while 22 (24.2%) also highlight that feature values may need to be constrained . Next, 19 212
(20.9%) definitions rely on a tautology that actionability means people can take actions , 11 (12.1%) 213
emphasize that these actions must be successful or lead to change , and 3 (3.3%) further require 214
that they are aligned with people’s real-world objectives . Only 14 (15.4%) definitions put users 215
1Financial domain dominates the evaluations as well, with 90 of 116 evaluations on non-synthetic data
making use of at least one finance-related dataset, most commonly German Credit Data [59] with 51 uses.
5at the center stage, indicating that actionability depends on the user or their preferences , while 2 216
(2.2%) highlight the importance of the context [144,156], for instance, that the ability to act on a 217
recommendation may change over time. Importantly, ethical considerations are never mentioned as 218
the pre-requisite for actionability, but we find some broader discussions about this [e.g., 142]. 219
4.4 What is the role of end users? What other stakeholders are envisioned in the AR process? 220
Given that AR is to be implemented in socio-technical systems that include a variety of actors, we 221
are interested in the types of stakeholders acknowledged in the literature. A total of 105 publications 222
provide explicit consideration of this type. In general, end users subject to algorithmic decisions 223
are envisioned to be the recipients of AR, but this is not always the case: it may also be provided to 224
experts [e.g., 21,22,76] or organizations [e.g., 65,72,147], which highlights that in some cases AR 225
may be carried out on behalf of the affected individuals. In any case, 47 (44.8%) publications in the 226
subset agree that end users should inform actionability, but it is rarely clear how these preferences 227
should be specified. User-friendly (interactive) interfaces are a consideration in only 14 (13.3%) 228
documents. A total of 29 (27.6%) publications envision domain experts as someone who inform 229
the recourse process. They are either expected to inform actionability in the AR system or provide 230
other forms of knowledge, typically in the form of a causal structure. Besides the experts, authors 231
of 35 (33.3%) papers have discussed a variety of stakeholders. Most commonly system owners 232
[e.g., 20,34,38,89], but also auditors [e.g., 138,158], data scientists [e.g., 28,82], developers [e.g., 233
22, 131], practitioners [e.g., 100, 156], regulators [e.g., 28, 120], or even potential attackers [102]. 234
4.5 What types of real-world considerations motivate existing research? 235
With the multitude of challenges that stand ahead of real-world AR, we are interested in the considera- 236
tions that motivate existing work. The main theme we find is ensuring proper individual actionability , 237
which is addressed in 46 (37.4%) of 123 publications relevant to this question. This is typically 238
achieved with the encoding of user preferences as constraints, but other means include providing 239
diverse CEs. In fact, tackling specific desiderata for AR (beyond actionability) is the second largest 240
area of research with 28 (22.8%) publications. Various other technical challenges are considered 241
in 24 (19.5%) documents, for example, integrating background knowledge [e.g., 16,62,64,98], or 242
incorporating feature importance [e.g., 4,6,96,116]. We also find 19 (15.4%) publications that 243
discuss the problem of communicating recourse to the end users . 16 (13.0%) focus on the dynamics 244
of real-world systems , typically addressing the robustness of AR [e.g., 75,91,93,137], while 14 245
(11.4%) look at recourse in multi-agent systems . This also relates to performance considerations 246
emphasized in 15 (12.2%) of documents. Causality drives research in 14 (11.4%) cases. We also 247
find several themes that are under-emphasized: only 9 (7.3%) publications are directly motivated by 248
research in psychology , while ethics of AR are emphasized in only 7 (5.7%) documents. 249
4.6 What types of real-world considerations are seen as challenges for future work? 250
While the previous section looked at the considerations that drive existing research, in this section we 251
distill the recommendations for future research going beyond the improvement of own work, which 252
are provided in 74 documents. Causality is highlighted as a challenge in 22 (29.7%) of them, while 253
other technical considerations are given in 20 (27.0%) cases. These range from robustness [e.g., 254
51,117,137], support for categorical features [e.g., 36,157], or distinguishing between valid CEs and 255
adversarial examples [ 101]. Next, 19 (25.6%) documents highlight the importance of ensuring proper 256
individual actionability , which also relates to communicating recourse to the end users (9, or 12.2%) 257
andsupporting realistic cost functions (8, or 10.8%). Ethics of AR are highlighted in 11 (14.9%) 258
publications, for example, that AR research may detract from other obligations of model owners 259
[77,133]. The same number of publications emphasize the need to (1) ground research in user studies , 260
and (2) accommodate for the dynamics of real-world systems .Privacy or security is highlighted in 10 261
(13.5%) documents, while the abuse of recourse , such as strategic behaviors, surfaces in 7 (9.4%) 262
papers. Other challenges include improving performance (8, or 10.8%), considering multi-agent 263
systems (4, or 5.4%), and developing legal frameworks (4, or 5.4%) for recourse. We also highlight 264
several challenges particularly relevant to our work: (the usefulness of) recourse is perceived as 265
difficult to evaluate in practice [ 41,60,115], it must account for individual, contextual, societal, and 266
even cultural factors [ 123], which further means that engagement with recourse mechanisms and the 267
likelihood of its implementation are context-dependent [e.g., 6, 42, 128]. 268
64.7 What types of (emergent) group-level dynamics are addressed in the existing research? 269
Real-world systems entail the implementation of recourse by more than one agent, which may 270
introduce group-level dynamics. Nonetheless, out of 119 documents relevant to this question, 93 271
(78.2%) seem to understand recourse as a purely individual phenomenon. Among the remaining 272
26 documents we find considerations for several different group-level effects. Various perspectives 273
on the problem of fair AR, covering both individual and group formulations are addressed by 274
[12,36,52,120,121,131,149,154]. Next, [ 9] shows that the implementation of AR on a large scale 275
may lead to domain and model shifts, which introduce unexpected costs for the stakeholders.2In [42] 276
the authors focus on another negative consequence of AR at scale, showing that it may reinforce 277
social segregation. The impact of the “right to be forgotten”, where data deletion requests trigger 278
model retraining that may invalidate existing recourses is addressed in [ 75]. Then, [ 94] develop a 279
game-theoretic framework for AR in multi-agent settings, attempting to optimize for “social welfare” 280
rather than the profits of individual agents. We find two further similar perspectives on recourse: 281
[38] proposes auditing and subsidies to minimize the risks of strategic behaviors in a multi-agent 282
setting, while [ 136] attempts to incentivize actual improvement for a population of agents. Finally, 283
[65] provides a framework that generates transparent and consistent recourses for a sub-population. 284
We also note two other lines of research that account for the remaining documents with group-level 285
considerations. First, in a causal setting [e.g., 68,73] subpopulations are necessary to estimate 286
the interventional effects on individuals. Second, several works highlight the importance of global 287
insights into the data [22, 41, 44, 78, 108, 112, 152], such as recourse summaries [78, 112]. 288
4.8 What are the approaches to the realistic evaluation of proposed methods? 289
We now explore the different forms of “real-world” evaluations, going beyond quantitative experi- 290
ments, which are present in 51 publications. Most commonly, in 28 (54.9%) of those, the authors 291
make use of case studies presenting the methods in an end-to-end manner. Among those, the appli- 292
cation of recourse in the Hired.com marketplace goes furthest in simulating real-world conditions 293
for AR [ 89], but the recommendations are still not evaluated with humans in the loop. Further, 9 294
(17.6%) documents include other forms of short walk-through examples . We also identify 14 (27.5%) 295
papers that evaluate the methods with user experiments , 10 of which involve non-expert users and 296
4 involve expert users. While we do not observe any interviews with non-expert users, we find 1 297
(2.0%) publication where experts are interviewed [22].Other involvement of non-experts applies to 298
[116], where they inform the development of methods. Other involvement of experts is featured in 299
two documents where they evaluated the outputs of methods [ 25,132]. Altogether, end users were 300
involved in 17 publications, which is only 13.3% of all publications covered in our study, even more 301
striking than the 21% of CE methods evaluated with user studies as reported in [71]. 302
4.9 What are the open source and documentation practices in AR research? 303
Finally, we note that the lack of availability of well-documented open-source code may be an important 304
obstacle to the application of AR in real-world systems. For all 116 publications that involve some 305
form of computational experiments, we verify whether the source code is publicly available. If the 306
authors do not explicitly link to their code in the paper, we attempt to find it independently. Ultimately, 307
we collect open-source implementations for 64 (55.2%) publications. Then, for each of them, we 308
evaluate the quality of documentation. The instructions on the general usage (such as installation and 309
workflow) are provided with 27 (41.5%) repositories, while instructions on the reproduction of results 310
in 23 (35.4%). In 19 (29.2%) cases we find walk-through tutorials , typically in the form of Jupyter 311
Notebooks, although we note that they differ in quality. For instance, 5 repositories include code-only 312
notebooks with no further textual explanation that could guide the practitioner. Implementations 313
for 4 papers include more “professionalized” documentation [9,86,100,156]. The latter sets a 314
golden standard as it further includes a tutorial video and a live demo. We do not find anyadditional 315
materials for practitioners for 13 (20.0%) of the available implementations. 316
2Such “endogenous dynamics” were postulated earlier in the first version of [ 113] dated December 22nd
2020, but this discussion has been completely removed from the subsequent versions of the pre-print.
75 Discussion 317
Regardless of whether AR can be normatively expected or not [ 77], many systems can genuinely 318
benefit from recourse mechanisms, especially when the interests of the system owner and the end users 319
are aligned [ 72], such as in the healthcare system to improve the well-being of patients [ 76,96,155], 320
or on the online platforms that attempt to improve the experience of their users [ 89,134]. Nonetheless, 321
the values and norms underlying recourse – trust, agency, fairness, safety, and so on – are emergent 322
properties of systems where recourse mechanisms would be introduced. Such norms can only be 323
understood and evaluated when accounting for the technical, social, and institutional components of 324
the system [32], but the latter two remain largely unexplored in the recourse literature. 325
Recourse is not inherently safe or unsafe, butits (incorrect) implementation may lead to the emer- 326
gence of unsafe dynamics, such as the unexpected costs to stakeholders as discussed by [ 9] or the 327
reinforcement of social segregation addressed in [ 42]. While it may be too challenging to provide 328
accurate system-level evaluations at this stage of research, authors can still expand the boundaries 329
of their analyses to account for global effects or look at the position of recourse mechanisms in the 330
broader context of a complete socio-technical AI system [ 33]. As AR is a “reality-centric AI” problem 331
[140] by its nature, working towards its integration into existing systems will require a design-oriented 332
approach, potentially with specific systems in mind. The “Abstraction Traps” discussed by [ 119] in 333
the context of research on fair machine learning apply here: that technical solutions designed for one 334
social context cannot be directly repurposed for another application, that values to which they are 335
expected to adhere to cannot be captured with mathematical formulas, that their insertion into an 336
existing process will impact its behavior, or that the best solutions may not necessarily be technical. 337
It is perhaps most telling that only 12% of surveyed publications attempt to apply recourse in realistic 338
settings. We will discuss two of these settings to highlight the stark differences in system properties. 339
Most of the applications included in our review focus on the provision of actionable individual 340
recommendations to students [ 3,4,24,109,126,135,160]. In this relatively low-stakes domain 341
almost any recourse will be actionable in that following a personalized set of learning activities 342
does not require any resources other than time. Even then, the system involves multiple actors 343
– students, teachers, parents – whose interactions will impact the process, for example, because 344
students may fail to benefit from certain learning activities without additional support. Conversely, 345
we find several publications where authors attempt to provide recourse in the high-stakes medical 346
domain [ 76,96,155]. Here, recommendations must be tailored to the preferences, resources, or 347
lifestyles of patients in order to have a chance of being actionable. Moreover, certain aspects of their 348
implementation fully rely on other actors, such as a clinician prescribing the medications. Finally, it 349
may happen that recourse does not exist at all when the outcomes of a patient cannot be improved. 350
5.1 Recommendations for future research 351
We distill our findings into five key recommendations. First, in Sections 4.2, 4.3 we observed that 352
operational definitions for recourse are still unavailable. Second, Sections 4.4 and 4.8 underlined 353
little consideration for people involved in recourse processes. Third, Sections 4.5, 4.6 highlighted the 354
overwhelmingly technical approaches to recourse. Fourth, Section 4.7 stressed the lack of group-level 355
analyses. Fifth, from Sections 4.8, and 4.9 we learned about the missing consideration of practitioners. 356
1. Broadening the scope of research. AR is generally seen as a service for affected individuals, 357
but this formalization may be unnecessarily limiting. In fact, in many systems, these individuals may 358
be unable to directly act on recommendations [see also 142]. Instead, we propose to operationalize 359
the aim of AR as the provision of recommendations aligned with the preferences ofnon-expert users 360
in an attempt to help them improve outcomes in an ADM setting , which emphasizes that providing 361
easy to understand andindividually actionable recommendations remains the key research problem. 362
2. Engaging end users, affected individuals, and communities. AR solutions are rarely evaluated 363
with humans. Instead, they attempt to satisfy a variety of desiderata formulated by authors and 364
assessed in an automated manner. Sparsity, proximity, or mutability of features are far from perfect 365
proxies for individual actionability. For AR to be truly useful, it must be able to satisfy the preferences 366
of its end users. Research is also necessary to learn about the needs of the affected individuals 367
concerning recourse, and to validate its potential contributions and inherent limitations. Authors may 368
also benefit from the rich literature on human-computer interaction [e.g., 11, 23] or psychology. 369
83. Accepting a socio-technical perspective. A pervasive assumption in the literature is that all 370
challenges of AR require purely technical solutions. For instance, many authors emphasize the 371
importance of causal modeling to guarantee recourse, but the models that aim to be explained are 372
themselves notcausal. Similarly, to improve the performance of CE generators many authors turn to 373
deep generative models [ 35,42,61,67,81,90,99]. Not only do they explain the data rather than the 374
model [ 10], but more importantly they shift the problem from improving the trust in non-interpretable 375
models, to attempting to trust non-interpretable explainers. Although a socio-technical perspective 376
on AR brings its own challenges, such as accounting for the roles of stakeholders involved in the 377
provision of recourse, it creates important opportunities. For example, developing “recourse contracts” 378
[34, 39] or designing feedback processes to account for imperfect robustness. 379
4. Accounting for emergent effects. Decision-making systems involve multiple individuals who 380
may be interested in receiving recourse and may have competing interests. Research on AR should, 381
from the onset, explore group-level effects such as external costs or fairness. While this may require 382
expanding the boundaries of analysis, it is necessary to anticipate the emergent outcomes of recourse. 383
These may even occur due to the multi-system dynamics of AR: recommendations implemented by 384
an individual to improve their outcomes in one system will affect them in other contexts [see also 13]. 385
5. Attending to other operational aspects. Finally, the artifacts of AR research should be 386
practitioner-friendly. On the one hand, this requires being explicit about the position of the proposed 387
methods in a broader system, for example, in the form of end-to-end case studies that allow practi- 388
tioners to better understand the benefits of the proposed solutions. On the other hand, this suggests 389
that authors should attempt to move away from merely providing scripts for experiments, and focus 390
on developing well-documented frameworks that can be adapted to different ADM systems. 391
5.2 Limitations of our work 392
Our review is not without shortcomings. Most importantly, for each paper the extraction and coding 393
of data was performed by a single author, which means that the quantitative results may be imperfect. 394
We account for this by focusing the analysis on the overarching themes represented in existing 395
publications, thus, even if another researcher would have carried out the coding in a somewhat 396
different manner, they should arrive at similar results and our analysis remains valid. Additionally, as 397
our review ultimately looks at the authors’ perception of recourse, we do not want to misconstrue 398
their views. Thus, we do not infer any considerations unless they are provided explicitly. Our reading 399
may be more strict than intended by the authors and the numbers reported in our results may be 400
underestimated. At the same time, we believe that if certain considerations are deemed important 401
by the researchers, they would choose to be explicit about them. Finally, although we followed a 402
systematic process, we cannot claim that we collected AR literature in an exhaustive manner due to 403
the specificities of computer science publishing. Thus, we acknowledge that there may exist some 404
insightful publications addressing recourse that have not been covered in this literature review. 405
6 Conclusions 406
Algorithmic recourse concerns the provision of recommendations aligned with the preferences of 407
non-expert users of algorithmic decision-making systems to help them achieve more desirable out- 408
comes in the future. Existing research on the topic is predominantly theoretical, even though recourse, 409
in expectation, is a real-world problem with strong practical implications. To that end, we conducted 410
a systematized literature review of 127 publications that focus on algorithmic recourse, and more gen- 411
erally on actionable counterfactual explanations. We evaluated the practical considerations provided 412
by the authors. Our findings indicate that, indeed, AR tends to be perceived as a (predominantly) 413
technical problem. Although we think highly of fundamental research, we note that for algorithmic 414
recourse to leave computer science labs, it must be more strongly grounded and validated in the real 415
world, and consider the requirements for systems that include not only technical but also social and 416
institutional components. To help bridge this gap, we synthesize a list of five recommendations for 417
other authors that aim to reinforce recourse as a practical problem. We believe that AR should not be 418
seen as only a simple ad-hoc solution to improve the acceptance of black-box models in consequential 419
domains, but rather as a full-fledged socio-technical mechanism that can benefit many systems and 420
improve the agency of affected individuals and decision-makers across a variety of settings. 421
9References 422
[1]Abubakar Abid, Mert Yuksekgonul, and James Zou. Meaningfully Debugging Model Mistakes 423
using Conceptual Counterfactual Explanations. In Proceedings of the 39th International 424
Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , 425
pages 66–88. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/a 426
bid22a.html . 427
[2]Gediminas Adomavicius and Alexander Tuzhilin. Discovery of Actionable Patterns in 428
Databases: The Action Hierarchy Approach. In Proceedings of the Third International 429
Conference on Knowledge Discovery and Data Mining , KDD’97, page 111–114. AAAI Press, 430
1997. 431
[3]Farzana Afrin, Margaret Hamilton, and Charles Thevathyan. Exploring Counterfactual Ex- 432
planations for Predicting Student Success. In Computational Science – ICCS 2023 , volume 433
14074 LNCS, pages 413–420. Springer Nature Switzerland, 2023. doi: 10.1007/978-3-031-3 434
6021-3_44. 435
[4]Muhammad Afzaal, Jalal Nouri, Aayesha Zia, Panagiotis Papapetrou, Uno Fors, Xiu Wu, 436
Yongchaoand Li, and Rebecka Weegar. Automatic and Intelligent Recommendations to 437
Support Students’ Self-Regulation. In 2021 International Conference on Advanced Learning 438
Technologies (ICALT) , pages 336–338, July 2021. ISBN 2161-377X. doi: 10.1109/ICALT522 439
72.2021.00107. 440
[5]Charu C. Aggarwal, Chen Chen, and Jiawei Han. The Inverse Classification Problem. Journal 441
of Computer Science and Technology , 25:458–468, 2010. 442
[6]Emanuele Albini, Jason Long, Danial Dervovic, and Daniele Magazzeni. Counterfactual 443
Shapley Additive Explanations. In Proceedings of the 2022 ACM Conference on Fairness, 444
Accountability, and Transparency , FAccT ’22, page 1054–1070, New York, NY , USA, 2022. 445
Association for Computing Machinery. ISBN 9781450393522. doi: 10.1145/3531146.3533168. 446
URLhttps://doi.org/10.1145/3531146.3533168 . 447
[7]Kars Alfrink, Ianus Keller, Gerd Kortuem, and Neelke Doorn. Contestable AI by design: 448
Towards a framework. Minds and Machines , pages 1–27, 2022. 449
[8]Hissah Alotaibi and Ronal Singh. Metrics for Evaluating Actionability in Explainable AI. In 450
PRICAI 2023: Trends in Artificial Intelligence , pages 481–487. Springer Nature Singapore, 451
2023. ISBN 978-981-99-7022-3. 452
[9]Patrick Altmeyer, Giovan Angela, Aleksander Buszydlik, Karol Dobiczek, Arie van Deursen, 453
and Cynthia C. S. Liem. Endogenous Macrodynamics in Algorithmic Recourse. In 2023 IEEE 454
Conference on Secure and Trustworthy Machine Learning (SaTML) , pages 418–431, 2023. 455
doi: 10.1109/SaTML54575.2023.00036. 456
[10] Patrick Altmeyer, Mojtaba Farmanbar, Arie van Deursen, and Cynthia C. S. Liem. Faithful 457
Model Explanations through Energy-Constrained Conformal Counterfactuals. In Proceedings 458
of the AAAI Conference on Artificial Intelligence , volume 38, pages 10829–10837, 2024. 459
[11] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. Power to the 460
People: The Role of Humans in Interactive Machine Learning. AI Magazine , 35(4):105–120, 461
2014. 462
[12] André Artelt, Valerie Vaquet, Riza Velioglu, Fabian Hinder, Johannes Brinkrolf, Malte 463
Schilling, and Barbara Hammer. Evaluating Robustness of Counterfactual Explanations. In 464
2021 IEEE Symposium Series on Computational Intelligence (SSCI) , pages 01–09, December 465
2021. doi: 10.1109/SSCI50451.2021.9660058. 466
[13] Solon Barocas, Andrew D. Selbst, and Manish Raghavan. The Hidden Assumptions Behind 467
Counterfactual Explanations and Principal Reasons. In Proceedings of the 2020 Conference on 468
Fairness, Accountability, and Transparency , FAT* ’20, page 80–89, New York, NY , USA, 2020. 469
Association for Computing Machinery. ISBN 9781450369367. doi: 10.1145/3351095.3372830. 470
URLhttps://doi.org/10.1145/3351095.3372830 . 471
10[14] Hosein Barzekar and Susan McRoy. Achievable Minimally-Contrastive Counterfactual 472
Explanations. Machine Learning and Knowledge Extraction , 5(3):922–936, 2023. doi: 473
10.3390/make5030048. 474
[15] Sander Beckers. Causal Explanations and XAI. In Bernhard Schölkopf, Caroline Uhler, and 475
Kun Zhang, editors, Proceedings of the First Conference on Causal Learning and Reasoning , 476
volume 177 of Proceedings of Machine Learning Research , pages 90–109. PMLR, 11–13 Apr 477
2022. URL https://proceedings.mlr.press/v177/beckers22a.html . 478
[16] Alexander Berman, Ellen Breitholtz, Christine Howes, and Jean-Philippe Bernardy. Explaining 479
Predictions with Enthymematic Counterfactuals. In CEUR Workshop Proceedings , volume 480
3319, pages 95–100. CEUR-WS, 2022. 481
[17] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joy- 482
deep Ghosh, Ruchir Puri, José M. F. Moura, and Peter Eckersley. Explainable Machine 483
Learning in Deployment. In Proceedings of the 2020 Conference on Fairness, Accountability, 484
and Transparency , FAT* ’20, page 648–657, New York, NY , USA, 2020. Association for 485
Computing Machinery. ISBN 9781450369367. doi: 10.1145/3351095.3375624. URL 486
https://doi.org/10.1145/3351095.3375624 . 487
[18] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle 488
Bao. The Values Encoded in Machine Learning Research. In Proceedings of the 2022 ACM 489
Conference on Fairness, Accountability, and Transparency , FAccT ’22, page 173–184, New 490
York, NY , USA, 2022. Association for Computing Machinery. ISBN 9781450393522. doi: 491
10.1145/3531146.3533083. URL https://doi.org/10.1145/3531146.3533083 . 492
[19] Miguel Á. Carreira-Perpiñán and Suryabhan Singh Hada. Counterfactual Explanations for 493
Oblique Decision Trees: Exact, Efficient Algorithms. Proceedings of the AAAI Conference 494
on Artificial Intelligence , 35:6903–6911, May 2021. doi: 10.1609/aaai.v35i8.16851. URL 495
https://ojs.aaai.org/index.php/AAAI/article/view/16851 . 496
[20] Yatong Chen, Jialu Wang, and Yang Liu. Strategic Recourse in Linear Classification. In 497
Workshop on Consequential Decision Making in Dynamic Environments , 2020. 498
[21] Ziheng Chen, Fabrizio Silvestri, Gabriele Tolomei, Jia Wang, He Zhu, and Hongshik Ahn. 499
Explain the Explainer: Interpreting Model-Agnostic Counterfactual Explanations of a Deep 500
Reinforcement Learning Agent. IEEE Transactions on Artificial Intelligence , 5(4):1443–1457, 501
2024. doi: 10.1109/TAI.2022.3223892. 502
[22] Furui Cheng, Yao Ming, and Huamin Qu. DECE: Decision Explorer with Counterfactual 503
Explanations for Machine Learning Models. IEEE Transactions on Visualization & Computer 504
Graphics , 27(02):1438–1447, feb 2021. ISSN 1941-0506. doi: 10.1109/TVCG.2020.3030342. 505
[23] Hao-Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O’Connell, Terrance Gray, F. Maxwell 506
Harper, and Haiyi Zhu. Explaining Decision-Making Algorithms through UI: Strategies 507
to Help Non-Expert Stakeholders. In Proceedings of the 2019 CHI Conference on Human 508
Factors in Computing Systems , CHI ’19, page 1–12, New York, NY , USA, 2019. Association 509
for Computing Machinery. ISBN 9781450359702. doi: 10.1145/3290605.3300789. URL 510
https://doi.org/10.1145/3290605.3300789 . 511
[24] Lea Cohausz. Towards Real Interpretability of Student Success Prediction Combining Methods 512
of XAI and Social Science. In Proceedings of the 15th International Conference on Educational 513
Data Mining , pages 361–367, Durham, United Kingdom, July 2022. International Educational 514
Data Mining Society. ISBN 978-1-7336736-3-1. doi: 10.5281/zenodo.6853069. 515
[25] Riccardo Crupi, Alessandro Castelnovo, Daniele Regoli, and Beatriz San Miguel Gonzalez. 516
Counterfactual Explanations as Interventions in Latent Space. Data Mining and Knowledge 517
Discovery , 2022. doi: 10.1007/s10618-022-00889-2. 518
[26] Susanne Dandl, Christoph Molnar, Martin Binder, and Bernd Bischl. Multi-Objective Coun- 519
terfactual Explanations. In Parallel Problem Solving from Nature – PPSN XVI , pages 520
448–469, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58112-1. doi: 521
10.1007/978-3-030-58112-1_3. 522
11[27] Debanjan Datta, Feng Chen, and Naren Ramakrishnan. Framing Algorithmic Recourse for 523
Anomaly Detection. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge 524
Discovery and Data Mining , KDD ’22, page 283–293, New York, NY , USA, 2022. Association 525
for Computing Machinery. ISBN 9781450393850. doi: 10.1145/3534678.3539344. URL 526
https://doi.org/10.1145/3534678.3539344 . 527
[28] Randall Davis, Andrew W. Lo, Sudhanshu Mishra, Arash Nourian, Manish Singh, Nicholas 528
Wu, and Ruixun Zhang. Explainable Machine Learning Models of Consumer Credit Risk. 529
Journal of Financial Data Science , 5(4):9–39, 2022. doi: 10.3905/jfds.2023.1.141. 530
[29] Marcelo de Sousa Balbino, Luis Enrique Zárate Gálvez, and Cristiane Neri Nobre. CSSE 531
- An agnostic method of counterfactual, selected, and social explanations for classification 532
models. Expert Systems with Applications , 228:120373, 2023. ISSN 0957-4174. doi: https: 533
//doi.org/10.1016/j.eswa.2023.120373. URL https://www.sciencedirect.com/scienc 534
e/article/pii/S0957417423008758 . 535
[30] Giovanni De Toni, Bruno Lepri, and Andrea Passerini. Synthesizing explainable counterfactual 536
policies for algorithmic recourse with program synthesis. Machine Learning , 112(4):1389– 537
1409, 2023. ISSN 0885-6125. doi: 10.1007/s10994-022-06293-7. 538
[31] Sarah Dean, Sarah Rich, and Benjamin Recht. Recommendations and User Agency: The 539
Reachability of Collaboratively-Filtered Information. In Proceedings of the 2020 Conference 540
on Fairness, Accountability, and Transparency , FAT* ’20, page 436–445, New York, NY , USA, 541
2020. Association for Computing Machinery. ISBN 9781450369367. doi: 10.1145/3351095. 542
3372866. URL https://doi.org/10.1145/3351095.3372866 . 543
[32] Roel Dobbe and Anouk Wolters. Toward Sociotechnical AI: Mapping Vulnerabilities for 544
Machine Learning in Context. Minds and Machines , 34(2):1–51, 2024. 545
[33] Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. Hard choices in artificial intelligence. 546
Artificial Intelligence , 300:103555, 2021. ISSN 0004-3702. doi: https://doi.org/10.1016/j.arti 547
nt.2021.103555. URL https://www.sciencedirect.com/science/article/pii/S0 548
004370221001065 . 549
[34] Ricardo Dominguez-Olmedo, Amir-Hossein Karimi, and Bernhard Schölkopf. On the Adver- 550
sarial Robustness of Causal Algorithmic Recourse. In Proceedings of the 39th International 551
Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , 552
pages 5324–5342. PMLR, 17–23 2022. 553
[35] Michael Downs, Jonathan L. Chu, Yaniv Yacoby, Finale Doshi-Velez, and Weiwei Pan. 554
CRUDS: Counterfactual Recourse Using Disentangled Subspaces. ICML Workshop on Human 555
Interpretability in Machine Learning , pages 1–23, 2020. 556
[36] Ahmad-Reza Ehyaei, Amir-Hossein Karimi, Bernhard Schoelkopf, and Setareh Maghsudi. 557
Robustness Implies Fairness in Causal Algorithmic Recourse. In Proceedings of the 2023 558
ACM Conference on Fairness, Accountability, and Transparency , FAccT ’23, page 984–1001, 559
New York, NY , USA, 2023. Association for Computing Machinery. ISBN 9798400701924. 560
doi: 10.1145/3593013.3594057. URL https://doi.org/10.1145/3593013.3594057 . 561
[37] Julia El Zini and Mariette Awad. Beyond Model Interpretability: On the Faithfulness and 562
Adversarial Robustness of Contrastive Textual Explanations. In Findings of the Association for 563
Computational Linguistics: EMNLP 2022 , pages 1391–1402. Association for Computational 564
Linguistics, 2022. doi: 10.18653/v1/2022.findings-emnlp.100. 565
[38] Andrew Estornell, Yatong Chen, Sanmay Das, Yang Liu, and Yevgeniy V orobeychik. Incen- 566
tivizing Recourse through Auditing in Strategic Classification. In Proceedings of the Thirty- 567
Second International Joint Conference on Artificial Intelligence, IJCAI-23 , pages 400–408. 568
International Joint Conferences on Artificial Intelligence, 8 2023. doi: 10.24963/ijcai.2023/45. 569
URLhttps://doi.org/10.24963/ijcai.2023/45 . 570
[39] Andrea Ferrario and Michele Loi. The Robustness of Counterfactual Explanations Over Time. 571
IEEE Access , 10:82736–82750, 2022. ISSN 2169-3536. doi: 10.1109/ACCESS.2022.3196917. 572
12[40] Susanne Friese, Jacks Soratto, and Denise Pires de Pires. Carrying out a computer-aided 573
thematic content analysis with ATLAS.ti. IWMI Working Papers , 18, 04 2018. 574
[41] Sainyam Galhotra, Romila Pradhan, and Babak Salimi. Explaining Black-Box Algorithms 575
Using Probabilistic Contrastive Counterfactuals. In Proceedings of the 2021 International 576
Conference on Management of Data , SIGMOD ’21, pages 577–590, New York, NY , USA, 577
2021. Association for Computing Machinery. ISBN 978-1-4503-8343-1. doi: 10.1145/344801 578
6.3458455. 579
[42] Ruijiang Gao and Himabindu Lakkaraju. On the Impact of Algorithmic Recourse on Social 580
Segregation. In Proceedings of the 40th International Conference on Machine Learning , 581
ICML’23. JMLR.org, 2023. 582
[43] Azin Ghazimatin, Oana Balalau, Rishiraj Saha Roy, and Gerhard Weikum. PRINCE: Provider- 583
Side Interpretability with Counterfactual Explanations in Recommender Systems. In Pro- 584
ceedings of the 13th International Conference on Web Search and Data Mining , WSDM ’20, 585
pages 196–204, New York, NY , USA, 2020. Association for Computing Machinery. ISBN 586
978-1-4503-6822-3. doi: 10.1145/3336191.3371824. 587
[44] Oscar Gomez, Steffen Holter, Jun Yuan, and Enrico Bertini. ViCE: Visual Counterfactual Ex- 588
planations for Machine Learning Models. In Proceedings of the 25th International Conference 589
on Intelligent User Interfaces , IUI ’20, pages 531–535, New York, NY , USA, 2020. Association 590
for Computing Machinery. ISBN 978-1-4503-7118-6. doi: 10.1145/3377325.3377536. 591
[45] Crystal Grant. Algorithms Are Making Decisions About Health Care, Which May Only 592
Worsen Medical Racism, October 2022. URL https://www.aclu.org/news/privacy-t 593
echnology/algorithms-in-health-care-may-worsen-medical-racism . Accessed 594
22.05.2024. 595
[46] Maria J. Grant and Andrew Booth. A typology of reviews: an analysis of 14 review types and 596
associated methodologies. Health Information & Libraries Journal , 26(2):91–108, 2009. doi: 597
https://doi.org/10.1111/j.1471-1842.2009.00848.x. 598
[47] Stephan Grimmelikhuijsen and Albert Meijer. Legitimacy of Algorithmic Decision-Making: 599
Six Threats and the Need for a Calibrated Institutional Response. Perspectives on Public 600
Management and Governance , 5(3):232–242, 03 2022. ISSN 2398-4910. doi: 10.1093/ppmg 601
ov/gvac008. URL https://doi.org/10.1093/ppmgov/gvac008 . 602
[48] Riccardo Guidotti. Counterfactual Explanations and How to Find Them: Literature Review 603
and Benchmarking. Data Mining and Knowledge Discovery , 2022. doi: 10.1007/s10618-022 604
-00831-6. 605
[49] Riccardo Guidotti and Salvatore Ruggieri. Ensemble of Counterfactual Explainers. In 606
Discovery Science: 24th International Conference, DS 2021, Halifax, NS, Canada, October 607
11–13, 2021, Proceedings , pages 358–368, Berlin, Heidelberg, 2021. Springer-Verlag. ISBN 608
978-3-030-88941-8. doi: 10.1007/978-3-030-88942-5_28. 609
[50] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Francesca Naretto, Franco Turini, 610
Dino Pedreschi, and Fosca Giannotti. Stable and Actionable Explanations of Black-Box 611
Models through Factual and Counterfactual Rules. Data Mining and Knowledge Discovery , 612
2022. doi: 10.1007/s10618-022-00878-5. 613
[51] Hangzhi Guo, Feiran Jia, Jinghui Chen, Anna Squicciarini, and Amulya Yadav. RoCourseNet: 614
Robust Training of a Prediction Aware Recourse Model. In Proceedings of the 32nd ACM 615
International Conference on Information and Knowledge Management , CIKM ’23, pages 619– 616
628, New York, NY , USA, 2023. Association for Computing Machinery. ISBN 9798400701245. 617
doi: 10.1145/3583780.3615040. 618
[52] Vivek Gupta, Pegah Nokhiz, Chitradeep Dutta Roy, and Suresh Venkatasubramanian. Equaliz- 619
ing Recourse Across Groups. arXiv , 2019. 620
13[53] Victor Guyomard, Françoise Fessant, Tassadit Bouadi, and Thomas Guyet. Post-hoc Coun- 621
terfactual Generation with Supervised Autoencoder. In Communications in Computer and 622
Information Science , volume 1524 CCIS, pages 105–114. Springer Science and Business 623
Media Deutschland GmbH, 2021. doi: 10.1007/978-3-030-93736-2_10. 624
[54] Victor Guyomard, Françoise Fessant, Thomas Guyet, Tassadit Bouadi, and Alexandre Termier. 625
Generating Robust Counterfactual Explanations. In Machine Learning and Knowledge Discov- 626
ery in Databases: Research Track. ECML PKDD 2023 , pages 394–409, Berlin, Heidelberg, 627
2023. Springer-Verlag. ISBN 978-3-031-43417-4. doi: 10.1007/978-3-031-43418-1_24. 628
[55] Suryabhan Singh Hada and Miguel Á. Carreira-Perpiñán. Exploring Counterfactual Ex- 629
planations for Classification and Regression Trees. In Communications in Computer and 630
Information Science , volume 1524 CCIS, pages 489–504. Springer Science and Business 631
Media Deutschland GmbH, 2021. doi: 10.1007/978-3-030-93736-2_37. 632
[56] Aparajita Haldar, Teddy Cunningham, and Hakan Ferhatosmanoglu. RAGUEL: Recourse- 633
Aware Group Unfairness Elimination. In Proceedings of the 31st ACM International Con- 634
ference on Information & Knowledge Management , CIKM ’22, pages 666–675, New York, 635
NY , USA, 2022. Association for Computing Machinery. ISBN 978-1-4503-9236-5. doi: 636
10.1145/3511808.3557424. 637
[57] Ian Hardy, Jayanth Yetukuri, and Yang Liu. Adaptive Adversarial Training Does Not Increase 638
Recourse Costs. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society , 639
AIES ’23, pages 432–442, New York, NY , USA, 2023. Association for Computing Machinery. 640
ISBN 9798400702310. doi: 10.1145/3600211.3604704. 641
[58] Zhian He and Eric Lo. Answering Why-not Questions on Top-k Queries. 2012 IEEE 28th 642
International Conference on Data Engineering , pages 750–761, 2012. doi: 10.1109/ICDE.201 643
2.8. 644
[59] Hans Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository, 1994. DOI: 645
https://doi.org/10.24432/C5NC77. 646
[60] Jacqueline Höllig, Aniek F. Markus, JJef de Slegte, and Prachi Bagave. Semantic Mean- 647
ingfulness: Evaluating Counterfactual Approaches for Real-World Plausibility and Fea- 648
sibility. In Communications in Computer and Information Science , volume 1902 CCIS, 649
pages 636–659. Springer Science and Business Media Deutschland GmbH, 2023. doi: 650
10.1007/978-3-031-44067-0_32. 651
[61] Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 652
Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision 653
Making Systems. arXiv , 2019. 654
[62] Sarathi K, Shania Mitra, Deepak P, and Sutanu Chakraborti. Counterfactuals as Explanations 655
for Monotonic Classifiers. In CEUR Workshop Proceedings , volume 3389, pages 177–188. 656
CEUR-WS, 2022. 657
[63] Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, and Hiroki Arimura. Distribution-Aware 658
Counterfactual Explanation by Mixed-Integer Linear Optimization. Transactions of the 659
Japanese Society for Artificial Intelligence , 36(6), 2021. doi: 10.1527/TJSAI.36-6_C-L44. 660
[64] Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Yuichi Ike, Kento Uemura, and Hiroki 661
Arimura. Ordered Counterfactual Explanation by Mixed-Integer Linear Optimization. In 35th 662
AAAI Conference on Artificial Intelligence, AAAI 2021 , volume 13A, pages 11564–11574. 663
Association for the Advancement of Artificial Intelligence, 2021. 664
[65] Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, and Yuichi Ike. Counterfactual Explanation 665
Trees: Transparent and Consistent Actionable Recourse with Decision Trees. In Proceedings 666
of The 25th International Conference on Artificial Intelligence and Statistics , volume 151, 667
pages 1846–1870. PMLR, 2022. 668
14[66] Amir-Hossein Karimi, Gilles Barthe, Borja Balle, and Isabel Valera. Model-Agnostic Coun- 669
terfactual Explanations for Consequential Decisions. In Proceedings of the Twenty Third 670
International Conference on Artificial Intelligence and Statistics , volume 108, pages 895–905. 671
PMLR, 2020. 672
[67] Amir-Hossein Karimi, Julius V on Kügelgen, Bernhard Schölkopf, and Isabel Valera. Algorith- 673
mic recourse under imperfect causal knowledge: a probabilistic approach. Advances in Neural 674
Information Processing Systems , 33:265–277, 2020. 675
[68] Amir-Hossein Karimi, Julius von Kügelgen, Bernhard Schölkopf, and Isabel Valera. Towards 676
Causal Algorithmic Recourse. In xxAI - Beyond Explainable AI: International Workshop, Held 677
in Conjunction with ICML 2020, July 18, 2020, Vienna, Austria, Revised and Extended Papers , 678
pages 139–166, Cham, 2020. Springer International Publishing. ISBN 978-3-031-04082-5. 679
doi: 10.1007/978-3-031-04083-2_8. 680
[69] Amir-Hossein Karimi, Bernhard Schölkopf, and Isabel Valera. Algorithmic Recourse: From 681
Counterfactual Explanations to Interventions. In Proceedings of the 2021 ACM Conference on 682
Fairness, Accountability, and Transparency , FAccT ’21, pages 353–362, New York, NY , USA, 683
2021. Association for Computing Machinery. ISBN 978-1-4503-8309-7. doi: 10.1145/344218 684
8.3445899. 685
[70] Amir-Hossein Karimi, Gilles Barthe, Bernhard Schölkopf, and Isabel Valera. A Survey of 686
Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations. ACM 687
Computing Surveys , 55(5), December 2022. ISSN 0360-0300. doi: 10.1145/3527848. 688
[71] Mark T. Keane, Eoin M. Kenny, Eoin Delaney, and Barry Smyth. If Only We Had Better 689
Counterfactual Explanations: Five Key Deficits to Rectify in the Evaluation of Counterfactual 690
XAI Techniques. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint 691
Conference on Artificial Intelligence, IJCAI-21 , pages 4466–4474. International Joint Con- 692
ferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/609. URL 693
https://doi.org/10.24963/ijcai.2021/609 . Survey Track. 694
[72] Nwaike Kelechi and Licheng Jiao. Quantifying Actionability: Evaluating Human-Recipient 695
Models. IEEE Access , 11:119811–119823, 2023. ISSN 2169-3536. doi: 10.1109/ACCESS.2 696
023.3324906. 697
[73] Gunnar König, Timo Freiesleben, and Moritz Grosse-Wentrup. Causal Perspective on Mean- 698
ingful and Robust Algorithmic Recourse. ICML Workshop on Algorithmic Recourse , 2021. 699
[74] Gunnar König, Timo Freiesleben, and Moritz Grosse-Wentrup. Improvement-Focused 700
Causal Recourse (ICR). In Proceedings of the Thirty-Seventh AAAI Conference on Arti- 701
ficial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial In- 702
telligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence , 703
AAAI’23/IAAI’23/EAAI’23. AAAI Press, 2023. ISBN 978-1-57735-880-0. doi: 10.1609/aa 704
ai.v37i10.26398. 705
[75] Satyapriya Krishna, Jiaqi Ma, and Himabindu Lakkaraju. Towards Bridging the Gaps between 706
the Right to Explanation and the Right to Be Forgotten. In Proceedings of the 40th International 707
Conference on Machine Learning , ICML’23. JMLR.org, 2023. 708
[76] Anisio Lacerda, Claudio Almeida, Leonardo Ferreira, Adriano Pereira, Gisele L. Pappa, Wag- 709
ner Meira, Debora Miranda, Marco A. Romano-Silva, and Leandro Malloy Diniz. Algorithmic 710
Recourse in Mental Healthcare. In 2023 International Joint Conference on Neural Networks 711
(IJCNN) , pages 1–8, June 2023. ISBN 2161-4407. doi: 10.1109/IJCNN54540.2023.10191158. 712
[77] Derek Leben. Explainable AI as evidence of fair decisions. Frontiers in Psychology , 14, 2023. 713
doi: 10.3389/fpsyg.2023.1069426. 714
[78] Dan Ley, Saumitra Mishra, and Daniele Magazzeni. GLOBE-CE: A Translation Based 715
Approach for Global Counterfactual Explanations. In Proceedings of the 40th International 716
Conference on Machine Learning , ICML’23. JMLR.org, 2023. 717
15[79] Ana Lucic, Harrie Oosterhuis, Hinda Haned, and Maarten de Rijke. FOCUS: Flexible 718
Optimizable Counterfactual Explanations for Tree Ensembles. In Proceedings of the 36th 719
AAAI Conference on Artificial Intelligence, AAAI 2022 , volume 36, pages 5313–5322, 2022. 720
[80] Shucen Ma, Jianqi Shi, Yanhong Huang, Shengchao Qin, and Zhe Hou. Minimal-unsatisfiable- 721
core-driven Local Explainability Analysis for Random Forest. International Journal of 722
Software and Informatics , 12(4):355–376, 2022. doi: 10.21655/ijsi.1673-7288.00280. 723
[81] Divyat Mahajan, Chenhao Tan, and Amit Sharma. Preserving Causal Constraints in Counter- 724
factual Explanations for Machine Learning Classifiers. In NeurIPS 2019 Workshop “Do the 725
right thing”: machine learning and causal inference for improved decision making , 2019. 726
[82] Raphael Mazzine, Sofie Goethals, Dieter Brughmans, and David Martens. Counterfactual 727
Explanations for Employment Services. International workshop on AI for Human Resources 728
and Public Employment Services , 2021. 729
[83] Md Golam Moula Mehedi Hasan and Douglas A. Talbert. Mitigating the Rashomon Effect in 730
Counterfactual Explanation: A Game-theoretic Approach. In Proceedings of the International 731
Florida Artificial Intelligence Research Society Conference, FLAIRS , volume 35. Florida 732
Online Journals, University of Florida, 2022. doi: 10.32473/flairs.v35i.130711. 733
[84] Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. Artificial 734
Intelligence , 267:1–38, 2019. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2018.07.0 735
07. URL https://www.sciencedirect.com/science/article/pii/S00043702183 736
05988 . 737
[85] Jonathan Moore, Nils Hammerla, and Chris Watkins. Explaining Deep Learning Models with 738
Constrained Adversarial Examples. In PRICAI 2019: Trends in Artificial Intelligence: 16th 739
Pacific Rim International Conference on Artificial Intelligence, Cuvu, Yanuca Island, Fiji, 740
August 26–30, 2019, Proceedings, Part I , pages 43–56, Berlin, Heidelberg, 2019. Springer- 741
Verlag. ISBN 978-3-030-29907-1. doi: 10.1007/978-3-030-29908-8_4. 742
[86] Ramaravind K. Mothilal, Amit Sharma, and Chenhao Tan. Explaining Machine Learning 743
Classifiers through Diverse Counterfactual Explanations. In Proceedings of the 2020 Confer- 744
ence on Fairness, Accountability, and Transparency , FAT* ’20, pages 607–617, New York, 745
NY , USA, 2020. Association for Computing Machinery. ISBN 978-1-4503-6936-7. doi: 746
10.1145/3351095.3372850. 747
[87] Madhumita Murgia. Algorithms are deciding who gets organ transplants. Are their decisions 748
fair?, November 2023. URL https://www.ft.com/content/5125c83a-b82b-40c5-8 749
b35-99579e087951 . Accessed 22.05.2024. 750
[88] Philip Naumann and Eirini Ntoutsi. Consequence-Aware Sequential Counterfactual Generation. 751
InMachine Learning and Knowledge Discovery in Databases. Research Track: European 752
Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part II , 753
pages 682–698, Berlin, Heidelberg, 2021. Springer-Verlag. ISBN 978-3-030-86519-1. doi: 754
10.1007/978-3-030-86520-7_42. 755
[89] Daniel Nemirovsky, Nicolas Thiebaut, Ye Xu, and Abhishek Gupta. Providing Actionable 756
Feedback in Hiring Marketplaces using Generative Adversarial Networks. In WSDM 2021 - 757
Proceedings of the 14th ACM International Conference on Web Search and Data Mining , pages 758
1089–1092. Association for Computing Machinery, 2021. doi: 10.1145/3437963.3441705. 759
[90] Daniel Nemirovsky, Nicolas Thiebaut, Ye Xu, and Abhishek Gupta. CounteRGAN: Gener- 760
ating Counterfactuals for Real-Time Recourse and Interpretability using Residual GANs. In 761
Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence, UAI 2022 , pages 762
1488–1497. Association For Uncertainty in Artificial Intelligence (AUAI), 2022. 763
[91] Duy Nguyen, Ngoc Bui, and Viet Anh Nguyen. Distributionally Robust Recourse Action. 764
arXiv , 2023. 765
[92] Duy Nguyen, Ngoc Bui, and Viet Anh Nguyen. Feasible Recourse Plan via Diverse Interpo- 766
lation. In Proceedings of The 26th International Conference on Artificial Intelligence and 767
Statistics , volume 206, pages 4679–4698. PMLR, 2023. 768
16[93] Tuan-Duy H. Nguyen, Ngoc Bui, Duy Nguyen, Man-Chung Yue, and Viet Anh Nguyen. 769
Robust Bayesian Recourse. In Proceedings of the Thirty-Eighth Conference on Uncertainty in 770
Artificial Intelligence , volume 180, pages 1498–1508. PMLR, 2022. 771
[94] Andrew O’Brien and Edward Kim. Toward Multi-Agent Algorithmic Recourse: Challenges 772
From a Game-Theoretic Perspective. In Proceedings of the International Florida Artificial 773
Intelligence Research Society Conference, FLAIRS , volume 35. Florida Online Journals, 774
University of Florida, 2022. doi: 10.32473/flairs.v35i.130614. 775
[95] Andrew O’Brien, Edward Kim, and Rosina Weber. Investigating Causally Augmented Sparse 776
Learning as a Tool for Meaningful Classification. In 2023 IEEE Sixth International Conference 777
on Artificial Intelligence and Knowledge Engineering (AIKE) , pages 33–37, September 2023. 778
ISBN 2831-7203. doi: 10.1109/AIKE59827.2023.00013. 779
[96] Ming Lun Ong, Anthony Li, and Mehul Motani. Explainable and Actionable Machine Learning 780
Models for Electronic Health Record Data. In IFMBE Proceedings , volume 79, pages 91–99, 781
Cham, 2021. Springer International Publishing. doi: 10.1007/978-3-030-62045-5_9. 782
[97] Matthew J. Page, Joanne E. McKenzie, Patrick M. Bossuyt, Isabelle Boutron, Tammy C. 783
Hoffmann, Cynthia D. Mulrow, Larissa Shamseer, Jennifer M. Tetzlaff, Elie A. Akl, Sue E. 784
Brennan, et al. The PRISMA 2020 statement: an updated guideline for reporting systematic 785
reviews. Bmj, 372, 2021. 786
[98] Axel Parmentier and Thibaut Vidal. Optimal Counterfactual Explanations in Tree Ensembles. 787
InProceedings of the 38th International Conference on Machine Learning , volume 139, pages 788
8422–8431. PMLR, 2021. 789
[99] Martin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. Learning Model-Agnostic Counter- 790
factual Explanations for Tabular Data. In The Web Conference 2020 - Proceedings of the World 791
Wide Web Conference, WWW 2020 , pages 3126–3132, 2020. doi: 10.1145/3366423.3380087. 792
[100] Martin Pawelczyk, Sascha Bielawski, Johannes van den Heuvel, Tobias Richter, and Gjergji 793
Kasneci. CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual 794
Explanation Algorithms. In Proceedings of the Neural Information Processing Systems Track 795
on Datasets and Benchmarks 1 (NeurIPS Datasets and Benchmarks 2021) , 2021. 796
[101] Martin Pawelczyk, Chirag Agarwal, Shalmali Joshi, Sohini Upadhyay, and Himabindu 797
Lakkaraju. Exploring Counterfactual Explanations Through the Lens of Adversarial Ex- 798
amples: A Theoretical and Empirical Analysis. In Proceedings of The 25th International 799
Conference on Artificial Intelligence and Statistics , volume 151, pages 4574–4594. PMLR, 800
2022. 801
[102] Martin Pawelczyk, Himabindu Lakkaraju, and Seth Neel. On the Privacy Risks of Algorithmic 802
Recourse. In Proceedings of The 26th International Conference on Artificial Intelligence and 803
Statistics , volume 206, pages 9680–9696. PMLR, 2023. 804
[103] Judea Pearl. Causality . Cambridge University Press, 2 edition, 2009. ISBN 9780511803161. 805
[104] Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. FACE: 806
Feasible and actionable counterfactual explanations. In Proceedings of the AAAI/ACM Con- 807
ference on AI, Ethics, and Society , AIES ’20, pages 344–350, New York, NY , USA, 2020. 808
Association for Computing Machinery. doi: 10.1145/3375627.3375850. 809
[105] Wenting Qi and Charalampos Chelmis. Improving Algorithmic Decision–Making in the 810
Presence of Untrustworthy Training Data. In 2021 IEEE International Conference on Big 811
Data (Big Data) , pages 1102–1108, 2021. doi: 10.1109/BigData52589.2021.9671677. 812
[106] Marcos M. Raimundo, Luis Gustavo Nonato, and Jorge Poco. Mining Pareto-optimal Counter- 813
factual Antecedents with a Branch-and-Bound Model-Agnostic Algorithm. Data Mining and 814
Knowledge Discovery , 2022. doi: 10.1007/s10618-022-00906-4. 815
[107] Goutham Ramakrishnan, Yun Chan Lee, and Aws Albarghouthi. Synthesizing Action Se- 816
quences for Modifying Model Decisions. In Proceedings of the AAAI Conference on Artificial 817
Intelligence , volume 34, pages 5462–5469, 2020. 818
17[108] Natraj Raman, Daniele Magazzeni, and Sameena. Shah. Bayesian Hierarchical Models for 819
Counterfactual Estimation. In Proceedings of The 26th International Conference on Artificial 820
Intelligence and Statistics , volume 206, pages 1115–1128. PMLR, 2023. 821
[109] Gomathy Ramaswami, Teo Susnjak, and Anuradha Mathrani. Supporting Students’ Academic 822
Performance Using Explainable Machine Learning with Automated Prescriptive Analytics. 823
Big Data and Cognitive Computing , 6(4), 2022. doi: 10.3390/bdcc6040105. 824
[110] Zbigniew W. Ras and Alicja Wieczorkowska. Action-Rules: How to Increase Profit of a 825
Company. In Principles of Data Mining and Knowledge Discovery , pages 587–592. Springer 826
Berlin Heidelberg, 2000. ISBN 978-3-540-45372-7. 827
[111] Peyman Rasouli and Ingrid Chieh Yu. CARE: Coherent Actionable Recourse Based on Sound 828
Counterfactual Explanations. International Journal of Data Science and Analytics , 17, 2022. 829
doi: 10.1007/s41060-022-00365-6. 830
[112] Kaivalya Rawal and Himabindu Lakkaraju. Beyond Individualized Recourse: Interpretable 831
and Interactive Summaries of Actionable Recourses. In Proceedings of the 34th International 832
Conference on Neural Information Processing Systems , NIPS’20, Red Hook, NY , USA, 2020. 833
Curran Associates Inc. ISBN 978-1-71382-954-6. 834
[113] Kaivalya Rawal, Ece Kamar, and Himabindu Lakkaraju. Algorithmic Recourse in the Wild: 835
Understanding the Impact of Data and Model Shifts. arXiv , 2021. 836
[114] Annabelle Redelmeier, Martin Jullum, Kjersti Aas, and Anders Løland. MCCE: Monte Carlo 837
sampling of realistic counterfactual explanations. In Data Mining and Knowledge Discovery , 838
pages 421–437. Springer Nature, 2024. doi: 10.1007/s10618-024-01017-y. 839
[115] Alexis Ross, Himabindu Lakkaraju, and Osbert Bastani. Learning models for actionable 840
recourse. In Advances in Neural Information Processing Systems , volume 34, pages 18734– 841
18746, 2021. 842
[116] Pedram Salimi, Nirmalie Wiratunga, David Corsar, and Anjana Wijekoon. Towards Feasible 843
Counterfactual Explanations: A Taxonomy Guided Template-Based NLG Method. In Frontiers 844
in Artificial Intelligence and Applications , volume 372, pages 2057–2064. IOS Press BV , 2023. 845
doi: 10.3233/FAIA230499. 846
[117] Maximilian Schleich, Zixuan Geng, Yihong Zhang, and Dan Suciu. GeCo: Quality Counter- 847
factual Explanations in Real Time. Proc. VLDB Endow. , 14(9):1681–1693, may 2021. ISSN 848
2150-8097. doi: 10.14778/3461535.3461555. URL https://doi.org/10.14778/34615 849
35.3461555 . 850
[118] Jakob Schoeffer, Niklas Kuehl, and Yvette Machowski. “There Is Not Enough Information”: 851
On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness 852
in Automated Decision-Making. In Proceedings of the 2022 ACM Conference on Fairness, 853
Accountability, and Transparency , FAccT ’22, pages 1616–1628, New York, NY , USA, 2022. 854
Association for Computing Machinery. ISBN 978-1-4503-9352-2. doi: 10.1145/3531146.35 855
33218. 856
[119] Andrew D. Selbst, danah boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet 857
Vertesi. Fairness and abstraction in sociotechnical systems. In Proceedings of the Conference on 858
Fairness, Accountability, and Transparency , FAT* ’19, page 59–68, New York, NY , USA, 2019. 859
Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.3287598. 860
URLhttps://doi.org/10.1145/3287560.3287598 . 861
[120] Shubham Sharma, Jette Henderson, and Joydeep Ghosh. CERTIFAI: A Common Framework 862
to Provide Explanations and Analyse the Fairness and Robustness of Black-Box Models. 863
InProceedings of the AAAI/ACM Conference on AI, Ethics, and Society , AIES ’20, pages 864
166–172, New York, NY , USA, 2020. Association for Computing Machinery. ISBN 978-1- 865
4503-7110-0. doi: 10.1145/3375627.3375812. 866
18[121] Shubham Sharma, Alan H. Gee, David Paydarfar, and Joydeep Ghosh. FaiR-N: Fair and Robust 867
Neural Networks for Structured Data. In Proceedings of the 2021 AAAI/ACM Conference on 868
AI, Ethics, and Society , AIES ’21, pages 946–955, New York, NY , USA, 2021. Association for 869
Computing Machinery. ISBN 978-1-4503-8473-5. doi: 10.1145/3461702.3462559. 870
[122] Sunny Shree, Jaganmohan Chandrasekaran, Yu Lei, Raghu N. Kacker, and D. Richard Kuhn. 871
DeltaExplainer: A Software Debugging Approach to Generating Counterfactual Explanations. 872
In2022 IEEE International Conference On Artificial Intelligence Testing (AITest) , pages 873
103–110, 2022. doi: 10.1109/AITest55621.2022.00023. 874
[123] Manan Singh, Sai Srinivas Kancheti, Shivam Gupta, Ganesh Ghalme, Shweta Jain, and 875
Narayanan C. Krishnan. Algorithmic Recourse Based on User’s Feature-Order Preference. 876
InProceedings of the 6th Joint International Conference on Data Science & Management of 877
Data (10th ACM IKDD CODS and 28th COMAD) , CODS-COMAD ’23, pages 293–294, New 878
York, NY , USA, 2023. Association for Computing Machinery. ISBN 978-1-4503-9797-1. doi: 879
10.1145/3570991.3571039. 880
[124] Ronal Singh, Tim Miller, Henrietta Lyons, Liz Sonenberg, Eduardo Velloso, Frank Vetere, 881
Piers Howe, and Paul Dourish. Directive Explanations for Actionable Explainability in 882
Machine Learning Applications. ACM Trans. Interact. Intell. Syst. , 13(4), December 2023. 883
ISSN 2160-6455. doi: 10.1145/3579363. 884
[125] Dylan Slack, Sophie Hilgard, Himabindu Lakkaraju, and Sameer Singh. Counterfactual 885
Explanations Can Be Manipulated. In Advances in Neural Information Processing Systems , 886
volume 34, pages 62–75, 2021. 887
[126] Bevan I. Smith, Charles Chimedza, and Jacoba H. Bührmann. Individualized Help for At-Risk 888
Students Using Model-Agnostic and Counterfactual Explanations. Education and Information 889
Technologies , 27(2):1539–1558, March 2022. ISSN 1360-2357. doi: 10.1007/s10639-021-1 890
0661-6. 891
[127] Jan-Tobias Sohns, Christoph Garth, and Heike Leitte. Decision Boundary Visualization for 892
Counterfactual Reasoning. Computer Graphics Forum , 42(1):7–20, 2023. doi: 10.1111/cgf.14 893
650. 894
[128] Nina Spreitzer, Hinda Haned, and Ilse van der Linden. Evaluating the Practicality of Counter- 895
factual Explanations. In CEUR Workshop Proceedings , volume 3277, pages 31–50. CEUR-WS, 896
2022. 897
[129] Laura State, Salvatore Ruggieri, and Franco Turini. Reason to Explain: Interactive Contrastive 898
Explanations (REASONX). In Explainable Artificial Intelligence , volume 1901 CCIS, pages 899
421–437, Cham, 2023. Springer Nature Switzerland. ISBN 978-3-031-44064-9. 900
[130] Ilia Stepin, Jose M. Alonso, Alejandro Catala, and Martín Pereira-Fariña. A Survey of 901
Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial 902
Intelligence. IEEE Access , 9:11974–12001, 2021. 903
[131] Muhammad Suffian and Alessandro Bogliolo. Investigation and Mitigation of Bias in Ex- 904
plainable AI. In CEUR Workshop Proceedings , volume 3319, pages 89–94. CEUR-WS, 905
2022. 906
[132] Muhammad Suffian, Pierluigi Graziani, Jose M. Alonso, and Alessandro Bogliolo. FCE: 907
Feedback Based Counterfactual Explanations for Explainable AI. IEEE Access , 10:72363– 908
72372, 2022. ISSN 2169-3536. doi: 10.1109/ACCESS.2022.3189432. 909
[133] Emily Sullivan and Philippe Verreault-Julien. From Explanation to Recommendation: Ethical 910
Standards for Algorithmic Recourse. In Proceedings of the 2022 AAAI/ACM Conference on 911
AI, Ethics, and Society , AIES ’22, pages 712–722, New York, NY , USA, 2022. Association for 912
Computing Machinery. ISBN 978-1-4503-9247-1. doi: 10.1145/3514094.3534185. 913
[134] Gabriele Tolomei, Fabrizio Silvestri, Andrew Haines, and Mounia Lalmas. Interpretable 914
Predictions of Tree-Based Ensembles via Actionable Feature Tweaking. In Proceedings of 915
the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 916
pages 465–474, 2017. doi: 10.1145/3097983.3098039. 917
19[135] Maria Tsiakmaki and Omiros Ragos. A Case Study of Interpretable Counterfactual Explana- 918
tions for the Task of Predicting Student Academic Performance. In 2021 25th International 919
Conference on Circuits, Systems, Communications and Computers (CSCC) , pages 120–125, 920
July 2021. doi: 10.1109/CSCC53858.2021.00029. 921
[136] Stratis Tsirtsis and Manuel Gomez-Rodriguez. Decisions, Counterfactual Explanations and 922
Strategic Behavior. In Proceedings of the 34th International Conference on Neural Information 923
Processing Systems , NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. ISBN 924
978-1-71382-954-6. 925
[137] Sohini Upadhyay, Shalmali Joshi, and Himabindu Lakkaraju. Towards Robust and Reliable 926
Algorithmic Recourse. In Advances in Neural Information Processing Systems , volume 20, 927
pages 16926–19937, 2021. 928
[138] Berk Ustun, Alexander Spangher, and Yang Liu. Actionable Recourse in Linear Classification. 929
InProceedings of the Conference on Fairness, Accountability, and Transparency , FAT* ’19, 930
pages 10–19, New York, NY , USA, 2019. Association for Computing Machinery. ISBN 931
978-1-4503-6125-5. doi: 10.1145/3287560.3287566. 932
[139] Rens Van De Schoot, Jonathan De Bruin, Raoul Schram, Parisa Zahedi, Jan De Boer, Fe- 933
lix Weijdema, Bianca Kramer, Martijn Huijts, Maarten Hoogerwerf, Gerbrich Ferdinands, 934
Albert Harkema, Joukje Willemsen, Yongchao Ma, Qixiang Fang, Sybren Hindriks, Lars 935
Tummers, and Daniel L. Oberski. An open source machine learning framework for efficient 936
and transparent systematic reviews. Nature Machine Intelligence , 3(2):125–133, 2021. doi: 937
10.1038/s42256-020-00287-7. 938
[140] Mihaela van der Schaar and Andrew Rashbass. The case for Reality-centric AI, Feb 2023. 939
URLhttps://www.vanderschaar-lab.com/the-case-for-reality-centric-ai/ . 940
Accessed 21.05.2024. 941
[141] Peter M. VanNostrand, Huayi Zhang, Dennis M. Hofmann, and Elke A. Rundensteiner. FACET: 942
Robust Counterfactual Explanation Analytics. Proc. ACM Manag. Data , 1(4), December 2023. 943
doi: 10.1145/3626729. 944
[142] Suresh Venkatasubramanian and Mark Alfano. The Philosophical Basis of Algorithmic Re- 945
course. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , 946
FAT* ’20, pages 284–293, New York, NY , USA, 2020. Association for Computing Machinery. 947
ISBN 978-1-4503-6936-7. doi: 10.1145/3351095.3372876. 948
[143] Sahil Verma, Varich Boonsanong, Minh Hoang, Keegan E. Hines, John P. Dickerson, and 949
Chirag Shah. Counterfactual Explanations and Algorithmic Recourses for Machine Learning: 950
A Review. arXiv , 2022. 951
[144] Sahil Verma, Keegan Hines, and John P. Dickerson. Amortized Generation of Sequential 952
Algorithmic Recourses for Black-Box Models. In Proceedings of the 36th AAAI Conference 953
on Artificial Intelligence, AAAI 2022 , volume 36, pages 8512–8519. Association for the 954
Advancement of Artificial Intelligence, 2022. 955
[145] Sahil Verma, Ashudeep Singh, Varich Boonsanong, John P. Dickerson, and Chirag Shah. 956
RecRec: Algorithmic Recourse for Recommender Systems. In Proceedings of the 32nd 957
ACM International Conference on Information and Knowledge Management , CIKM ’23, 958
pages 4325–4329, New York, NY , USA, 2023. Association for Computing Machinery. ISBN 959
9798400701245. doi: 10.1145/3583780.3615181. 960
[146] Kilian Vieth-Ditlmann. The algorithmic administration: automated decision-making in the 961
public sector, May 2024. URL https://algorithmwatch.org/en/algorithmic-admin 962
istration-explained/ . Accessed 22.05.2024. 963
[147] Marco Virgolin and Saverio Fracaros. On the Robustness of Sparse Counterfactual Explana- 964
tions to Adverse Perturbations. Artificial Intelligence , 316(C), March 2023. ISSN 0004-3702. 965
doi: 10.1016/j.artint.2022.103840. 966
20[148] Vy V o, Trung Le, Van Nguyen, He Zhao, Edwin V . Bonilla, Gholamreza Haffari, and Dinh 967
Phung. Feature-Based Learning for Diverse and Privacy-Preserving Counterfactual Expla- 968
nations. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery 969
and Data Mining , KDD ’23, pages 2211–2222, New York, NY , USA, 2023. Association for 970
Computing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.3599343. 971
[149] Julius V on Kugelgen, Amir-Hossein Karimi, Umang Bhatt, Isabel Valera, Adrian Weller, and 972
Bernhard Scholkopf. On the Fairness of Causal Algorithmic Recourse. In Proceedings of the 973
36th AAAI Conference on Artificial Intelligence, AAAI 2022 , volume 36, pages 9584–9594. 974
Association for the Advancement of Artificial Intelligence, 2022. 975
[150] Julius von Kügelgen, Nikita Agarwal, Jakob Zeitler, Afsaneh Mastouri, and Bernhard 976
Schölkopf. Algorithmic Recourse in Partially and Fully Confounded Settings Through Bound- 977
ing Counterfactual Effects. arXiv , 2021. 978
[151] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without 979
opening the black box: Automated decisions and the GDPR. Harvard Journal of Law & 980
Technology , 31:841, 2017. 981
[152] Paul Y . Wang, Sainyam Galhotra, Romila Pradhan, and Babak Salimi. Demonstration of 982
Generating Explanations for Black-Box Algorithms Using Lewis. Proc. VLDB Endow. , 14 983
(12):2787–2790, July 2021. ISSN 2150-8097. doi: 10.14778/3476311.3476345. 984
[153] Yongjie Wang, Qinxu Ding, Ke Wang, Yue Liu, Xingyu Wu, Jinglong Wang, Yong Liu, and 985
Chunyan Miao. The Skyline of Counterfactual Explanations for Machine Learning Decision 986
Models. In Proceedings of the 30th ACM International Conference on Information & Knowl- 987
edge Management , CIKM ’21, pages 2030–2039, New York, NY , USA, 2021. Association for 988
Computing Machinery. ISBN 978-1-4503-8446-9. doi: 10.1145/3459637.3482397. 989
[154] Yongjie Wang, Hangwei Qian, Yongjie Liu, Wei Guo, and Chunyan Miao. Flexible and Robust 990
Counterfactual Explanations with Minimal Satisfiable Perturbations. In Proceedings of the 991
32nd ACM International Conference on Information and Knowledge Management , CIKM ’23, 992
pages 2596–2605, New York, NY , USA, 2023. Association for Computing Machinery. ISBN 993
9798400701245. doi: 10.1145/3583780.3614885. 994
[155] Zhendong Wang, Isak Samsten, Vasiliki Kougia, and Panagiotis Papapetrou. Style-Transfer 995
Counterfactual Explanations: An Application to Mortality Prevention of ICU Patients. Artif. 996
Intell. Med. , 135(C), January 2023. ISSN 0933-3657. doi: 10.1016/j.artmed.2022.102457. 997
[156] Zijie J. Wang, Jennifer Wortman Vaughan, Rich Caruana, and Duen Horng Chau. GAM 998
Coach: Towards Interactive and User-Centered Algorithmic Recourse. In Proceedings of 999
the 2023 CHI Conference on Human Factors in Computing Systems , CHI ’23, New York, 1000
NY , USA, 2023. Association for Computing Machinery. ISBN 978-1-4503-9421-5. doi: 1001
10.1145/3544548.3580816. 1002
[157] Greta Warren, Mark T. Keane, and Ruth M. J. Byrne. Features of Explainability: How Users 1003
Understand Counterfactual and Causal Explanations for Categorical and Continuous Features 1004
in XAI. In CEUR Workshop Proceedings , volume 3251. CEUR-WS, 2022. 1005
[158] Greta Warren, Barry Smyth, and Mark T. Keane. “Better” Counterfactuals, Ones People 1006
Can Understand: Psychologically-Plausible Case-Based Counterfactuals Using Categorical 1007
Features for Explainable AI (XAI). In Case-Based Reasoning Research and Development: 30th 1008
International Conference, ICCBR 2022, Nancy, France, September 12–15, 2022, Proceedings , 1009
pages 63–78, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3-031-14922-1. doi: 1010
10.1007/978-3-031-14923-8_5. 1011
[159] Daniel S. Weld and Gagan Bansal. The Challenge of Crafting Intelligible Intelligence. Commun. 1012
ACM , 62(6):70–79, may 2019. ISSN 0001-0782. doi: 10.1145/3282486. URL https: 1013
//doi.org/10.1145/3282486 . 1014
[160] Anjana Wijekoon, Nirmalie Wiratunga, Ikechukwu Nkisi-Orji, Kyle Martin, Chamath Pali- 1015
hawadana, and David Corsar. Counterfactual Explanations for Student Outcome Prediction 1016
with Moodle Footprints. In CEUR Workshop Proceedings , volume 2894, pages 1–8. CEUR- 1017
WS, 2021. 1018
21[161] Nirmalie Wiratunga, Anjana Wijekoon, Ikechukwu Nkisi-Orji, Kyle Martin, Chamath Pal- 1019
ihawadana, and David Corsar. DisCERN: Discovering Counterfactual Explanations using 1020
Relevance Features from Neighbourhoods. In 2021 IEEE 33rd International Conference on 1021
Tools with Artificial Intelligence (ICTAI) , pages 1466–1473, November 2021. ISBN 2375-0197. 1022
doi: 10.1109/ICTAI52525.2021.00233. 1023
[162] Claes Wohlin. Guidelines for Snowballing in Systematic Literature Studies and a Replication 1024
in Software Engineering. EASE ’14: Proceedings of the 18th International Conference on 1025
Evaluation and Assessment in Software Engineering , 2014. doi: 10.1145/2601248.2601268. 1026
URLhttps://doi.org/10.1145/2601248.2601268 . 1027
[163] Jingquan Yan and Hao Wang. Self-Interpretable Time Series Prediction with Counterfactual 1028
Explanations. In Proceedings of the 40th International Conference on Machine Learning , 1029
ICML’23. JMLR.org, 2023. 1030
[164] Jayanth Yetukuri, Ian Hardy, and Yang Liu. Towards User Guided Actionable Recourse. 1031
InProceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society , AIES ’23, 1032
pages 742–751, New York, NY , USA, 2023. Association for Computing Machinery. ISBN 1033
9798400702310. doi: 10.1145/3600211.3604708. 1034
[165] Songming Zhang, Xiaofeng Chen, Shiping Wen, and Zhongshan Li. Density-Based Reliable 1035
and Robust Explainer for Counterfactual Explanation. Expert Syst. Appl. , 226(C), September 1036
2023. ISSN 0957-4174. doi: 10.1016/j.eswa.2023.120214. 1037
22A Extended discussion of the search process 1038
While our discussion of the search process in Section 3.1 in the main body of the document is 1039
complete, we also provide an extended version of this discussion to allow for full reproducibility. 1040
We make use of 3 search engines to collect the initial set of studies: ACM Digital Library, IEEE 1041
Xplore, and SCOPUS. Given the blurry distinction between AR and CEs, we consider the papers 1042
discussing either problem. In a small scoping review, we identify several keywords common to 1043
publications on recourse, as well as several equivalent terms to build the query shown below. 1044
(“Machine Learning” OR “Artificial Intelligence”
OR “Algorithmic Decision*” OR “Consequential Decision*”
OR Classif* OR Predict* OR “Explainable AI” OR AI OR XAI)
AND (((Counterfactual OR Contrastive OR Actionable) AND Explanation*)
OR ((Algorithmic OR Individual* OR Actionable) AND Recourse)
OR Counterfactual?)
We modify this query to account for the semantic differences between the search engines. 1045
For ACM Digital Library: 1046
Title:(( "Machine Learning" OR "Artificial Intelligence"
OR "Algorithmic Decision*" OR "Consequential Decision*"
OR classif* OR predict* OR "Explainable AI" OR ai OR xai )
AND ( ( ( counterfactual OR contrastive OR actionable )
AND explanation* ) OR ( ( algorithmic OR individual* OR actionable )
AND recourse ) OR counterfactual? ))
OR Abstract:(( "Machine Learning" OR "Artificial Intelligence"
OR "Algorithmic Decision*" OR "Consequential Decision*"
OR classif* OR predict* OR "Explainable AI" OR ai OR xai )
AND ( ( ( counterfactual OR contrastive OR actionable )
AND explanation* ) OR ( ( algorithmic OR individual* OR actionable )
AND recourse ) OR counterfactual? ))
OR Keyword:(( "Machine Learning" OR "Artificial Intelligence"
OR "Algorithmic Decision*" OR "Consequential Decision*"
OR classif* OR predict* OR "Explainable AI" OR ai OR xai )
AND ( ( ( counterfactual OR contrastive OR actionable )
AND explanation* ) OR ( ( algorithmic OR individual* OR actionable )
AND recourse ) OR counterfactual? ))
For IEEE Xplore: 1047
((("All Metadata":"Machine Learning"
OR "All Metadata":"Artificial Intelligence"
OR "All Metadata":"Algorithmic Decision*"
OR "All Metadata":"Consequential Decision*"
OR "All Metadata":classif* OR "All Metadata":predict*
OR "All Metadata":"Explainable AI" OR "All Metadata":ai
OR "All Metadata":xai )
AND ((("All Metadata":counterfactual OR "All Metadata":contrastive
OR "All Metadata":actionable ) AND "All Metadata":explanation* )
OR ( ("All Metadata":algorithmic OR "All Metadata":individual*
OR "All Metadata":actionable )
AND "All Metadata":recourse )
OR "All Metadata":counterfactual? )))
23For SCOPUS: 1048
TITLE-ABS-KEY ( ( "Machine Learning" OR "Artificial Intelligence"
OR "Algorithmic Decision*" OR "Consequential Decision*"
OR classif* OR predict* OR "Explainable AI" OR ai OR xai )
AND ( ( ( counterfactual OR contrastive OR actionable ) AND explanation* )
OR ( ( algorithmic OR individual* OR actionable ) AND recourse )
OR counterfactual? ) )
The search is carried out on January 12th2024 in titles, abstracts, and keywords, with 1267 results 1049
from ACM Digital Library (The ACM Guide to Computing Literature), 513 results from IEEE Xplore, 1050
and 2139 results from SCOPUS. This leads to a total of 3919 results, which are imported to the 1051
Zotero reference management software for de-duplication. After removing the duplicates, we are left 1052
with 3136 results, 44 of which are the meta-data of conference proceedings that we also remove. 1053
To facilitate the screening process, we employ the open-source ASReview tool, which makes use of 1054
an active learning approach to re-order the set of publications, such that the most relevant ones are 1055
always “at the top of the stack” [139]. We run ASReview on the default settings, i.e.: 1056
Feature extraction technique: TF-IDF
Classifier: Naive Bayes
Query strategy: Maximum
Balance strategy: Dynamic resampling (Double)
The researchers behind the tool suggest employing a stopping rule measured in the number of 1057
consecutive irrelevant records, which we set to 30, or 1% of the entire dataset. We accept all papers 1058
that focus on algorithmic recourse and counterfactual explanations, completing the screening after 1059
evaluating 1040 abstracts (33.67% of the dataset), leading to 504 (16.30%) records among which we 1060
identify further 4 duplicates to remove. This results in the reported number of 499 relevant records. 1061
We observe that some important publications may be missing from our results. For instance, [ 151] 1062
was published in the Harvard Journal of Law & Technology that is not indexed by computer science 1063
search engines. Thus, we decide to augment the set of records by applying snowballing, which has 1064
been shown as a good alternative to databases in systematic reviews in software engineering [162]. 1065
We decide to make use of citation counts as a proxy for impact. Due to the lack of a suitable tool that 1066
would provide unbiased citation counts for allpapers in our dataset, we collect them from Google 1067
Scholar. Unfortunately, citation counts on Google Scholar tend to be inflated, but as we make use of 1068
snowballing purely to enrich the dataset, these does not impact the validity of our study. We manually 1069
collect Google Scholar citation counts for all 499 results from the first screening on January 27th1070
and 28th, order them descendingly, and collect references for the top 50 (10%) “most impactful” 1071
publications. Snowballing results in a total of 1519 new records. Indeed, we observe that [ 151] 1072
(mentioned above) is referenced by 39 of the 50 publications used for snowballing. 1073
While this strategy introduces several pre-prints into our result set [ 52,61,91,113,143,150], we 1074
decide not to exclude them. Our review remains primarily concerned with peer-reviewed work. Here, 1075
we also note that [ 114], which we collected as a pre-print has been published between the search and 1076
appraisal. As such we decided to evaluate its published version and refer to it in this paper. 1077
After adding the snowballed references into our dataset, we are left with 2018 records for the second 1078
screening with ASReview, again on the default settings. This time, we look for publications that 1079
specifically refer to the problem of AR, “actionable” CEs, or modifying outcomes of automated 1080
decision-making systems. We employ a stricter stopping rule to minimize the risk of false neg- 1081
atives, completing the screening after 60 consecutive irrelevant records. We evaluate 538 results 1082
(26.71% of the dataset), with 203 (10.06%) relevant results that are considered for full-text appraisal. 1083
This concludes the extended discussion of the search process. 1084
24B Evaluation of contributions 1085
Table 1: Evaluation of the collected publications on the types of contributions, 2017-2021.
Year ReferencePropose
methodsTheoretical
frameworksAnalyses Apply Benchmark Review
2017 [151] ✓ ✓
2019 [52] ✓
[61] ✓
[81] ✓
[85] ✓
[138] ✓
2020 [35] ✓
[86] ✓
[136] ✓
[20] ✓
[26] ✓
[44] ✓
[67] ✓
[66] ✓
[99] ✓
[104] ✓
[107] ✓
[120] ✓
[112] ✓
[13] ✓
[142] ✓
2021 [69] ✓ ✓
[137] ✓ ✓
[41] ✓
[49] ✓
[53] ✓
[73] ✓
[150] ✓
[105] ✓
[19] ✓
[22] ✓
[63] ✓
[64] ✓
[88] ✓
[98] ✓
[115] ✓
[117] ✓
[153] ✓
[161] ✓
[121] ✓
[55] ✓
[12] ✓
[113] ✓
[125] ✓
[4] ✓
[82] ✓
[89] ✓
[96] ✓
[135] ✓
[152] ✓
[160] ✓
[100] ✓
25Table 2: Evaluation of the collected publications on the types of contributions, 2022.
Year ReferencePropose
methodsTheoretical
frameworksAnalyses Apply Benchmark Review
2022 [39] ✓ ✓
[34] ✓ ✓
[6] ✓
[25] ✓
[50] ✓
[62] ✓
[158] ✓
[83] ✓
[56] ✓
[79] ✓
[80] ✓
[90] ✓
[93] ✓
[106] ✓
[111] ✓
[132] ✓
[131] ✓
[144] ✓
[65] ✓
[101] ✓ ✓
[24] ✓ ✓
[70] ✓ ✓
[15] ✓
[16] ✓
[94] ✓
[118] ✓
[133] ✓
[157] ✓
[128] ✓
[149] ✓
[28] ✓
[109] ✓
[126] ✓
[48] ✓ ✓
[143] ✓ ✓
26Table 3: Evaluation of the collected publications on the types of contributions, 2023-2024.
Year ReferencePropose
methodsTheoretical
frameworksAnalyses Apply Benchmark Review
2023 [36] ✓ ✓
[29] ✓ ✓
[116] ✓ ✓
[9] ✓ ✓
[42] ✓ ✓
[75] ✓ ✓
[147] ✓ ✓
[156] ✓ ✓
[155] ✓ ✓
[54] ✓
[123] ✓
[14] ✓
[72] ✓
[30] ✓
[51] ✓
[91] ✓
[92] ✓
[95] ✓
[108] ✓
[127] ✓
[129] ✓
[141] ✓
[154] ✓
[163] ✓
[164] ✓
[165] ✓
[78] ✓
[148] ✓
[74] ✓
[77] ✓
[124] ✓
[38] ✓
[57] ✓
[102] ✓
[3] ✓
[76] ✓
[8] ✓
[60] ✓
2024 [21] ✓
[114] ✓
27NeurIPS Paper Checklist 1086
1.Claims 1087
Question: Do the main claims made in the abstract and introduction accurately reflect the 1088
paper’s contributions and scope? 1089
Answer: [Yes] 1090
Justification: Our main claim is that existing research on recourse is disconnected from the 1091
practical requirements of systems where it would be applied (see Section 4 and Section 5.1). 1092
Our claim is supported by a systematized literature review which is the contribution of this 1093
work (Section 3). These are reflected in the abstract and the introduction. 1094
Guidelines: 1095
•The answer NA means that the abstract and introduction do not include the claims 1096
made in the paper. 1097
•The abstract and/or introduction should clearly state the claims made, including the 1098
contributions made in the paper and important assumptions and limitations. A No or 1099
NA answer to this question will not be perceived well by the reviewers. 1100
•The claims made should match theoretical and experimental results, and reflect how 1101
much the results can be expected to generalize to other settings. 1102
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 1103
are not attained by the paper. 1104
2.Limitations 1105
Question: Does the paper discuss the limitations of the work performed by the authors? 1106
Answer: [Yes] 1107
Justification: We highlight and discuss the three main limitations of our work in Section 5.2. 1108
Guidelines: 1109
•The answer NA means that the paper has no limitation while the answer No means that 1110
the paper has limitations, but those are not discussed in the paper. 1111
• The authors are encouraged to create a separate "Limitations" section in their paper. 1112
•The paper should point out any strong assumptions and how robust the results are to 1113
violations of these assumptions (e.g., independence assumptions, noiseless settings, 1114
model well-specification, asymptotic approximations only holding locally). The authors 1115
should reflect on how these assumptions might be violated in practice and what the 1116
implications would be. 1117
•The authors should reflect on the scope of the claims made, e.g., if the approach was 1118
only tested on a few datasets or with a few runs. In general, empirical results often 1119
depend on implicit assumptions, which should be articulated. 1120
•The authors should reflect on the factors that influence the performance of the approach. 1121
For example, a facial recognition algorithm may perform poorly when image resolution 1122
is low or images are taken in low lighting. Or a speech-to-text system might not be 1123
used reliably to provide closed captions for online lectures because it fails to handle 1124
technical jargon. 1125
•The authors should discuss the computational efficiency of the proposed algorithms 1126
and how they scale with dataset size. 1127
•If applicable, the authors should discuss possible limitations of their approach to 1128
address problems of privacy and fairness. 1129
•While the authors might fear that complete honesty about limitations might be used by 1130
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 1131
limitations that aren’t acknowledged in the paper. The authors should use their best 1132
judgment and recognize that individual actions in favor of transparency play an impor- 1133
tant role in developing norms that preserve the integrity of the community. Reviewers 1134
will be specifically instructed to not penalize honesty concerning limitations. 1135
3.Theory Assumptions and Proofs 1136
Question: For each theoretical result, does the paper provide the full set of assumptions and 1137
a complete (and correct) proof? 1138
28Answer: [NA] 1139
Justification: Our work, as a literature review, does not rely on theoretical results or proofs. 1140
Nonetheless, we are explicit about the “assumptions” in that we discuss our approach to the 1141
collection and analysis of results in depth in Section 3. 1142
Guidelines: 1143
• The answer NA means that the paper does not include theoretical results. 1144
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 1145
referenced. 1146
•All assumptions should be clearly stated or referenced in the statement of any theorems. 1147
•The proofs can either appear in the main paper or the supplemental material, but if 1148
they appear in the supplemental material, the authors are encouraged to provide a short 1149
proof sketch to provide intuition. 1150
•Inversely, any informal proof provided in the core of the paper should be complemented 1151
by formal proofs provided in appendix or supplemental material. 1152
• Theorems and Lemmas that the proof relies upon should be properly referenced. 1153
4.Experimental Result Reproducibility 1154
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 1155
perimental results of the paper to the extent that it affects the main claims and/or conclusions 1156
of the paper (regardless of whether the code and data are provided or not)? 1157
Answer: [NA] 1158
Justification: Our work does not rely on any experiments, so this question is not applicable. 1159
Nonetheless, we believe that we provide sufficiently in-depth characterization of the review 1160
process where other authors should be able to reproduce it (Section 3 and Appendix A). 1161
Guidelines: 1162
• The answer NA means that the paper does not include experiments. 1163
•If the paper includes experiments, a No answer to this question will not be perceived 1164
well by the reviewers: Making the paper reproducible is important, regardless of 1165
whether the code and data are provided or not. 1166
•If the contribution is a dataset and/or model, the authors should describe the steps taken 1167
to make their results reproducible or verifiable. 1168
•Depending on the contribution, reproducibility can be accomplished in various ways. 1169
For example, if the contribution is a novel architecture, describing the architecture fully 1170
might suffice, or if the contribution is a specific model and empirical evaluation, it may 1171
be necessary to either make it possible for others to replicate the model with the same 1172
dataset, or provide access to the model. In general. releasing code and data is often 1173
one good way to accomplish this, but reproducibility can also be provided via detailed 1174
instructions for how to replicate the results, access to a hosted model (e.g., in the case 1175
of a large language model), releasing of a model checkpoint, or other means that are 1176
appropriate to the research performed. 1177
•While NeurIPS does not require releasing code, the conference does require all submis- 1178
sions to provide some reasonable avenue for reproducibility, which may depend on the 1179
nature of the contribution. For example 1180
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 1181
to reproduce that algorithm. 1182
(b)If the contribution is primarily a new model architecture, the paper should describe 1183
the architecture clearly and fully. 1184
(c)If the contribution is a new model (e.g., a large language model), then there should 1185
either be a way to access this model for reproducing the results or a way to reproduce 1186
the model (e.g., with an open-source dataset or instructions for how to construct 1187
the dataset). 1188
(d)We recognize that reproducibility may be tricky in some cases, in which case 1189
authors are welcome to describe the particular way they provide for reproducibility. 1190
In the case of closed-source models, it may be that access to the model is limited in 1191
some way (e.g., to registered users), but it should be possible for other researchers 1192
to have some path to reproducing or verifying the results. 1193
295.Open access to data and code 1194
Question: Does the paper provide open access to the data and code, with sufficient instruc- 1195
tions to faithfully reproduce the main experimental results, as described in supplemental 1196
material? 1197
Answer: [NA] 1198
Justification: Our work does not rely on any experiments, so this question is not applicable. 1199
Nonetheless, we provide the complete list of publications covered in this review. We will 1200
also release the review data upon acceptance. 1201
Guidelines: 1202
• The answer NA means that paper does not include experiments requiring code. 1203
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/pu 1204
blic/guides/CodeSubmissionPolicy ) for more details. 1205
•While we encourage the release of code and data, we understand that this might not be 1206
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 1207
including code, unless this is central to the contribution (e.g., for a new open-source 1208
benchmark). 1209
•The instructions should contain the exact command and environment needed to run to 1210
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 1211
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 1212
•The authors should provide instructions on data access and preparation, including how 1213
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 1214
•The authors should provide scripts to reproduce all experimental results for the new 1215
proposed method and baselines. If only a subset of experiments are reproducible, they 1216
should state which ones are omitted from the script and why. 1217
•At submission time, to preserve anonymity, the authors should release anonymized 1218
versions (if applicable). 1219
•Providing as much information as possible in supplemental material (appended to the 1220
paper) is recommended, but including URLs to data and code is permitted. 1221
6.Experimental Setting/Details 1222
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 1223
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 1224
results? 1225
Answer: [NA] 1226
Justification: Our work does not rely on any experiments, so this question is not applicable. 1227
Guidelines: 1228
• The answer NA means that the paper does not include experiments. 1229
•The experimental setting should be presented in the core of the paper to a level of detail 1230
that is necessary to appreciate the results and make sense of them. 1231
•The full details can be provided either with the code, in appendix, or as supplemental 1232
material. 1233
7.Experiment Statistical Significance 1234
Question: Does the paper report error bars suitably and correctly defined or other appropriate 1235
information about the statistical significance of the experiments? 1236
Answer: [NA] 1237
Justification: Our work does not rely on any experiments, so this question is not applicable. 1238
Guidelines: 1239
• The answer NA means that the paper does not include experiments. 1240
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 1241
dence intervals, or statistical significance tests, at least for the experiments that support 1242
the main claims of the paper. 1243
30•The factors of variability that the error bars are capturing should be clearly stated (for 1244
example, train/test split, initialization, random drawing of some parameter, or overall 1245
run with given experimental conditions). 1246
•The method for calculating the error bars should be explained (closed form formula, 1247
call to a library function, bootstrap, etc.) 1248
• The assumptions made should be given (e.g., Normally distributed errors). 1249
•It should be clear whether the error bar is the standard deviation or the standard error 1250
of the mean. 1251
•It is OK to report 1-sigma error bars, but one should state it. The authors should 1252
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 1253
of Normality of errors is not verified. 1254
•For asymmetric distributions, the authors should be careful not to show in tables or 1255
figures symmetric error bars that would yield results that are out of range (e.g. negative 1256
error rates). 1257
•If error bars are reported in tables or plots, The authors should explain in the text how 1258
they were calculated and reference the corresponding figures or tables in the text. 1259
8.Experiments Compute Resources 1260
Question: For each experiment, does the paper provide sufficient information on the com- 1261
puter resources (type of compute workers, memory, time of execution) needed to reproduce 1262
the experiments? 1263
Answer: [NA] 1264
Justification: Our work does not rely on any experiments, so this question is not applicable. 1265
Guidelines: 1266
• The answer NA means that the paper does not include experiments. 1267
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 1268
or cloud provider, including relevant memory and storage. 1269
•The paper should provide the amount of compute required for each of the individual 1270
experimental runs as well as estimate the total compute. 1271
•The paper should disclose whether the full research project required more compute 1272
than the experiments reported in the paper (e.g., preliminary or failed experiments that 1273
didn’t make it into the paper). 1274
9.Code Of Ethics 1275
Question: Does the research conducted in the paper conform, in every respect, with the 1276
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 1277
Answer: [Yes] 1278
Justification: We have reviewed the NeurIPS Code of Ethics and we confirm that our work 1279
conforms to it in every respect. 1280
Guidelines: 1281
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 1282
•If the authors answer No, they should explain the special circumstances that require a 1283
deviation from the Code of Ethics. 1284
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 1285
eration due to laws or regulations in their jurisdiction). 1286
10.Broader Impacts 1287
Question: Does the paper discuss both potential positive societal impacts and negative 1288
societal impacts of the work performed? 1289
Answer: [Yes] 1290
Justification: Although this is not covered in a separate section, positive and negative societal 1291
impacts of our work (and algorithmic recourse in general) are a key consideration throughout 1292
this paper. See for instance Section 1 or Section 6. 1293
Guidelines: 1294
31• The answer NA means that there is no societal impact of the work performed. 1295
•If the authors answer NA or No, they should explain why their work has no societal 1296
impact or why the paper does not address societal impact. 1297
•Examples of negative societal impacts include potential malicious or unintended uses 1298
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 1299
(e.g., deployment of technologies that could make decisions that unfairly impact specific 1300
groups), privacy considerations, and security considerations. 1301
•The conference expects that many papers will be foundational research and not tied 1302
to particular applications, let alone deployments. However, if there is a direct path to 1303
any negative applications, the authors should point it out. For example, it is legitimate 1304
to point out that an improvement in the quality of generative models could be used to 1305
generate deepfakes for disinformation. On the other hand, it is not needed to point out 1306
that a generic algorithm for optimizing neural networks could enable people to train 1307
models that generate Deepfakes faster. 1308
•The authors should consider possible harms that could arise when the technology is 1309
being used as intended and functioning correctly, harms that could arise when the 1310
technology is being used as intended but gives incorrect results, and harms following 1311
from (intentional or unintentional) misuse of the technology. 1312
•If there are negative societal impacts, the authors could also discuss possible mitigation 1313
strategies (e.g., gated release of models, providing defenses in addition to attacks, 1314
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 1315
feedback over time, improving the efficiency and accessibility of ML). 1316
11.Safeguards 1317
Question: Does the paper describe safeguards that have been put in place for responsible 1318
release of data or models that have a high risk for misuse (e.g., pretrained language models, 1319
image generators, or scraped datasets)? 1320
Answer: [NA] 1321
Justification: Our work poses no such risks, so this question is not applicable. We do not 1322
introduce any data or models. 1323
Guidelines: 1324
• The answer NA means that the paper poses no such risks. 1325
•Released models that have a high risk for misuse or dual-use should be released with 1326
necessary safeguards to allow for controlled use of the model, for example by requiring 1327
that users adhere to usage guidelines or restrictions to access the model or implementing 1328
safety filters. 1329
•Datasets that have been scraped from the Internet could pose safety risks. The authors 1330
should describe how they avoided releasing unsafe images. 1331
•We recognize that providing effective safeguards is challenging, and many papers do 1332
not require this, but we encourage authors to take this into account and make a best 1333
faith effort. 1334
12.Licenses for existing assets 1335
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 1336
the paper, properly credited and are the license and terms of use explicitly mentioned and 1337
properly respected? 1338
Answer: [NA] 1339
Justification: Our work does not use existing assets (other than the referenced papers), so 1340
this question is not applicable. All papers covered in the review are referenced in sufficient 1341
detail, so that the readers can access them. 1342
Guidelines: 1343
• The answer NA means that the paper does not use existing assets. 1344
• The authors should cite the original paper that produced the code package or dataset. 1345
•The authors should state which version of the asset is used and, if possible, include a 1346
URL. 1347
32• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 1348
•For scraped data from a particular source (e.g., website), the copyright and terms of 1349
service of that source should be provided. 1350
•If assets are released, the license, copyright information, and terms of use in the package 1351
should be provided. For popular datasets, paperswithcode.com/datasets has 1352
curated licenses for some datasets. Their licensing guide can help determine the license 1353
of a dataset. 1354
•For existing datasets that are re-packaged, both the original license and the license of 1355
the derived asset (if it has changed) should be provided. 1356
•If this information is not available online, the authors are encouraged to reach out to 1357
the asset’s creators. 1358
13.New Assets 1359
Question: Are new assets introduced in the paper well documented and is the documentation 1360
provided alongside the assets? 1361
Answer: [NA] 1362
Justification: Our work does not release any new assets, so this question is not applicable. 1363
We release the paper with the most permissible license available for NeurIPS submissions. 1364
Finally, we will release the review data upon acceptance. 1365
Guidelines: 1366
• The answer NA means that the paper does not release new assets. 1367
•Researchers should communicate the details of the dataset/code/model as part of their 1368
submissions via structured templates. This includes details about training, license, 1369
limitations, etc. 1370
•The paper should discuss whether and how consent was obtained from people whose 1371
asset is used. 1372
•At submission time, remember to anonymize your assets (if applicable). You can either 1373
create an anonymized URL or include an anonymized zip file. 1374
14.Crowdsourcing and Research with Human Subjects 1375
Question: For crowdsourcing experiments and research with human subjects, does the paper 1376
include the full text of instructions given to participants and screenshots, if applicable, as 1377
well as details about compensation (if any)? 1378
Answer: [NA] 1379
Justification: Our paper does not involve crowdsourcing or research with human subjects, so 1380
this question is not applicable. The work was in its entirety carried out by the authors. 1381
Guidelines: 1382
•The answer NA means that the paper does not involve crowdsourcing nor research with 1383
human subjects. 1384
•Including this information in the supplemental material is fine, but if the main contribu- 1385
tion of the paper involves human subjects, then as much detail as possible should be 1386
included in the main paper. 1387
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 1388
or other labor should be paid at least the minimum wage in the country of the data 1389
collector. 1390
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 1391
Subjects 1392
Question: Does the paper describe potential risks incurred by study participants, whether 1393
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 1394
approvals (or an equivalent approval/review based on the requirements of your country or 1395
institution) were obtained? 1396
Answer: [NA] 1397
Justification: Our work does not involve crowdsourcing or research with human subjects, so 1398
this question is not applicable. We did not require an IRB approval or equivalent to carry 1399
out this work. 1400
33Guidelines: 1401
•The answer NA means that the paper does not involve crowdsourcing nor research with 1402
human subjects. 1403
•Depending on the country in which research is conducted, IRB approval (or equivalent) 1404
may be required for any human subjects research. If you obtained IRB approval, you 1405
should clearly state this in the paper. 1406
•We recognize that the procedures for this may vary significantly between institutions 1407
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1408
guidelines for their institution. 1409
•For initial submissions, do not include any information that would break anonymity (if 1410
applicable), such as the institution conducting the review. 1411
34