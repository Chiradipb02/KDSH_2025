A Separation in Heavy-Tailed Sampling:
Gaussian vs. Stable Oracles for Proximal Samplers
Ye He
Georgia Institute of Technology
yhe367@gatech.eduAlireza Mousavi-Hosseini
University of Toronto, and Vector Institute
mousavi@cs.toronto.edu
Krishnakumar Balasubramanian
University of California, Davis
kbala@ucdavis.eduMurat A. Erdogdu
University of Toronto, and Vector Institute
erdogdu@cs.toronto.edu
Abstract
We study the complexity of heavy-tailed sampling and present a separation result
in terms of obtaining high-accuracy versus low-accuracy guarantees i.e., samplers
that require only O(log(1 /ε))versus Ω(poly(1/ε))iterations to output a sample
which is ε-close to the target in χ2-divergence. Our results are presented for
proximal samplers that are based on Gaussian versus stable oracles. We show that
proximal samplers based on the Gaussian oracle have a fundamental barrier in
that they necessarily achieve only low-accuracy guarantees when sampling from a
class of heavy-tailed targets. In contrast, proximal samplers based on the stable
oracle exhibit high-accuracy guarantees, thereby overcoming the aforementioned
limitation. We also prove lower bounds for samplers under the stable oracle and
show that our upper bounds cannot be fundamentally improved.
1 Introduction
The task of sampling from heavy-tailed targets arises in various domains such as Bayesian statis-
tics [ GJPS08 ,GLM18 ], machine learning [ CDV09 ,BZ17 ,N¸ SR19 ,¸ SZTG20 ,DKTZ20 ], robust
statistics [ KN04 ,JR07 ,Kam18 ,YŁR22 ], multiple comparison procedures [ GBH04 ,GB09 ], and
study of geophysical systems [ SP15 ,QM16 ,PBEM23 ]. This problem is particularly challenging
when using gradient-based Markov Chain Monte Carlo (MCMC) algorithms due to diminishing
gradients, which occurs when the tails of the target density decay at a slow (e.g. polynomial) rate.
Indeed, canonical algorithms like Langevin Monte Carlo (LMC) have been empirically observed to
perform poorly [LWME19, HMW21, HFBE24] when sampling from such heavy-tailed targets.
Several approaches have been proposed in the literature to overcome these limitations of LMC and
related algorithms. The predominant ones include (i) transformation-based approaches, where a
diffeomorphic (invertible) transformation is used to first map the heavy-tailed density to a light-tailed
one so that a light-tailed sampling algorithm can be used [ JG12 ,YŁR22 ,HBE24 ], (ii) discretizing
general Itô diffusions with non-standard Brownian motion that have heavy-tailed densities as their
equilibrium density [ EMS18 ,LWME19 ,HFBE24 ], and (iii) discretizing stable-driven stochastic
differential equations [ ZZ23 ]. However, the few theoretical results available on the analysis of
algorithms based on approaches (i) and (ii) provide only low-accuracy heavy-tailed samplers; such
algorithms require poly(1/ε)iterations to obtain a sample that is ε-close to the target in a reasonable
metric of choice. Furthermore, quantitative complexity guarantees for the sampling approach used in
(iii) are not yet available; thus, existing comparisons are mainly based on empirical studies.
In stark contrast, when the target density is light-tailed it is well-known that algorithms like proximal
samplers based on Gaussian oracles and the Metropolis Adjusted Langevin Algorithm (MALA) have
high-accuracy guarantees; these algorithms require only polylog (1/ε)iterations to obtain a sample
which is ε-close to the target in some metric. See, for example, the works by [ DCWY19 ,LST21b ,
38th Conference on Neural Information Processing Systems (NeurIPS 2024).ν≥1 ν∈(0,1)
Oracle Gaussian (Alg. 1) Stable (Alg. 2 & 3) Gaussian (Alg. 1) Stable (Alg. 2 & 3)
Complexity ˜Ω(ε−1
ν)(Cor. 2) O(log(ε−1))(Cor. 5) ˜Ω(ε−1
ν)(Cor. 2) ˜O(ε−1
ν+1)(Cor. 5)
Table 1: Separation for Proximal Samplers: Gaussian vs. practical Stable oracles ( α=1):Upper and lower
iteration complexity bounds to generate an ε-accurate sample in χ2-divergence from the generalized Cauchy
target densities with degrees of freedom ν, i.e.πν∝(1 +|x|2)−(d+ν)/2. Here, ˜Ω,˜Ohide constants depending
onνandpolylog (d,1/ε). For the proximal sampler with a general α-Stable oracle (Algorithm 2), the upper
bound for ν∈(0,1)isO(log(1 /ε))when α=ν. The lower bounds are from Corollary 2 via 2TV2≤χ2.
WSC22a ,CCSW22 ,CG23 ]. Specifically, [ LST21b ] analyzed the proximal sampling algorithm
to sample from a class of strongly log-concave densities and obtained high-accuracy guarantees.
[CCSW22 ] established similar high-accuracy guarantees for the proximal sampler to sample from
target densities that satisfy a certain functional inequality, covering a range of light-tailed densities
with exponentially fast tail decay (e.g. log-Sobolev and Poincaré inequalities). However, it is not clear
if the proximal sampler achieves the same desirable performance when the target is not light-tailed.
In light of existing results, in this work, we first consider the following question:
Q1. What are the fundamental limits of proximal samplers under the Gaussian
oracle when sampling from heavy-tailed targets?
To answer this question, we construct lower bounds showing that Gaussian-based samplers necessarily
require poly(1/ε)iterations to sample from a class of heavy-tailed targets. These results complement
the lower bounds on the complexity of sampling from heavy-tailed densities using the LMC algorithm
established in [ MHFH+23]. With this lower bound in hand, we next consider the following question:
Q2. Is it possible to design high-accuracy samplers for heavy-tailed targets?
We answer this in the affirmative by constructing proximal samplers that are based on stable oracles
(see Definition 1 and Algorithm 2) by leveraging the fractional heat-flow corresponding to a class of
stable-driven SDEs. We analyze the complexity of this algorithm when sampling from heavy-tailed
densities that satisfy a fractional Poincaré inequality, and establish that they require only log(1/ε)
iterations. Together, our answers to Q1andQ2provide a clear separation between samplers based on
Gaussian and stable oracles. Our contributions can be summarized as follows.
•Lower bounds for the Gaussian oracle : In Section 2, we focus on Q1and establish in Theorems 1
and 2 respectively that the Langevin diffusion and the proximal sampler based on the Gaussian
oracle necessarily have a fundamental barrier when sampling from heavy-tailed densities. Our
proof technique builds on [ Hai10 ], and provides a novel perspective for obtaining algorithm-
dependent lower bounds for sampling, which may be of independent interest.
•A proximal sampler based on the stable oracle: In Section 3, we introduce a proximal sampler
based on the α-stable oracle, which fundamentally relies on the exact implementations of the
fractional heat flow that correspond to a stable-driven SDE. Here, the parameter αdetermines the
allowed class of heavy-tailed targets which could be sampled with high-accuracy. In Theorem 3
and Proposition 1, we provide upper bounds on the iteration complexity that are of smaller order
than the corresponding lower bounds established for the Gaussian oracle. We provide a rejection-
sampling based implementation of the α-stable oracle for the case α= 1and prove complexity
upper bounds in Corollary 3. Finally, in Theorem 4, considering a sub-class of Cauchy-type
targets, we prove lower bounds showing that our upper bounds cannot be fundamentally improved.
An illustration of our results for Cauchy target densities, πν∝(1 +|x|2)−(d+ν)/2where νis the
degrees of freedom, is provided in Table 1. We specifically consider the practical version of the stable
proximal sampler with α= 1(i.e., Algorithm 2 with the stable oracle implemented by Algorithm 3),
and show that it always outperforms the Gaussian proximal sampler (Algorithm 1). Indeed, when
ν≥1, the separation between these algorithms is obvious. In the case ν∈(0,1), Algorithm 2 & 3
has a poly( 1/ε) complexity, nevertheless, it still improves the complexity of the Gaussian proximal
sampler by a factor of ε. We also show via lower bounds (in Section 3.4) that the poly( 1/ε) complexity
for Algorithm 2 & 3, when ν∈(0,1), can only be improved up to certain factors. We remark that
for the ideal proximal sampler (Algorithm 2), the upper bound when ν∈(0,1)is also O(log(1 /ε)).
These results demonstrate a clear separation between Gaussian and stable proximal samplers.
Related works. We first discuss works analyzing the complexity of heavy-tailed sampling as charac-
terized by a functional inequality assumption. [ CDV09 ] analyzed the connection between sampling
2algorithms for a class of s-concave densities satisfying a certain isoperimetry condition related to
weighted Poincaré inequalities. [ HFBE24 ] undertook a mean-square analysis of discretization of
a specific Itô diffusion that characterizes a class of heavy-tailed densities satisfying a weighted
Poincaré inequality. [ ALPW22 ] and [ ALPW23 ] analyzed the complexity of pseudo-marginal MCMC
algorithms and the random-walk Metropolis algorithm respectively, under weak Poincaré inequalities.
As mentioned before, [ MHFH+23] showed lower bounds for the LMC algorithm when the target
density satisfies a weak Poincaré inequality. [ HBE24 ] and [ YŁR22 ] analyzed a transformation based
approach for heavy-tailed sampling under conditions closely related to the same functional inequality.
This transformation methodology is also used to demonstrate asymptotic exponential ergodicity for
other sampling algorithms like the bouncy particle sampler and the zig-zag sampler, in the heavy-
tailed settings [ DBCD19 ,DGM20 ,BRZ19 ]. These works provide only low-accuracy guarantees for
heavy-tailed sampling and do not consider the use of weak Fractional Poincaré inequalities.
Recent years have witnessed a significant focus on (strongly) log-concave sampling, leading to an
extensive body of work that is challenging to encapsulate succinctly. In the context of (strongly) log-
concave or light-tailed distributions, a plethora of non-asymptotic investigations have been conducted
on LMC variations, including advanced integrators [ SL19 ,LWME19 ,HBE20 ], underdamped LMC
[CCBJ18 ,EGZ19 ,CLW23 ,DRD20 ], and MALA [ DCWY19 ,LST20 ,CLA+21,WSC22b ]. Outside
the realm of log-concavity, the dissipativity assumption, which regulates the growth of the potential,
has been used in numerous studies to derive convergence guarantees [ DM17 ,RRT17 ,EMS18 ,EH21 ,
MFWB22, EHZ22, BCE+22].
While research on upper bounds of sampling algorithms’ complexity has advanced considerably, the
exploration of lower bounds is still nascent. [ CGL+22] explored the query complexity of sampling
from strongly log-concave distributions in one-dimensional settings. [ LZT22 ] established lower
bounds for LMC in sampling from strongly log-concave distributions. [ CBL22 ] presented lower
bounds for sampling from strongly log-concave distributions with noisy gradients. [ GLL20 ] focused
on lower bounds for estimating normalizing constants of log-concave densities. Contributions by
[LST21a ] and [ WSC22b ] provide lower bounds in the metropolized algorithm category, including
Langevin and Hamiltonian Monte Carlo, in strongly log-concave contexts. Finally, [ CGLL22 ]
contributed to lower bounds in Fisher information for non-log-concave sampling.
2 Lower Bounds for Sampling with the Gaussian Oracle
In this section, we focus on Q1for both the Langevin diffusion (in continuous time) and the proximal
sampler (in discrete time), where both procedures have the target density as their invariant measures.
Our results below illustrate the limitation of the Gaussian oracle1for heavy-tailed sampling in both
continuous and discrete time, showing that the phenomenon is not because of the discretization effect,
but is inherently related to the use of Gaussian oracles.
Langevin diffusion. We first start with the overdamped Langevin diffusion (LD):
dXt=−∇V(Xt)dt+√
2dBt. (LD)
LD achieves high-accuracy “sampling” in continuous time, i.e. a polylog(1 /ε)convergence rate in
the light-tailed setting. We make the following dissipativity-type assumption.
Assumption 1. The target density is given by πX(x)∝exp(−V(x)), where V:Rd→Rsatisfies
∀x∈Rd,(d+ν1)|x|2
1 +|x|2≤ ⟨x,∇V(x)⟩ ≤(d+ν2)|x|2
1 +|x|2for some ν2≥ν1≥0.
Remark 1. The upper bound on ⟨x,∇V(x)⟩ensures that Vgrows at most logarithmically in |x|.
Consequently, πXis heavy-tailed and in fact does not satisfy a Poincaré inequality. The lower bound
on⟨x,∇V(x)⟩is only needed for deriving the dimension dependency in our guarantees. If one is
only interested in the εdependency, this condition can be replaced with 0≤ ⟨x,∇V(x)⟩.
A classical example of a density satisfying the above assumption is the generalized Cauchy density
with degrees of freedom ν=ν1=ν2>0, where the potential is given by
Vν(x):=d+ν
2ln(1 + |x|2). (1)
The following result, proved in Appendix A, provides a lower bound on the performance of LD.
1Here, for the sake of unified presentation, we refer the use of Brownian motion in (LD) as Gaussian oracle.
3Algorithm 1 Gaussian Proximal Sampler [LST21b]
Input: Sample x0, and step η >0.fork= 0,1,···, N−1 //Gibbs sampler
Sample yk∼πY|X(·|xk) =N(xk, ηId) //Heat flow
Sample xk+1|yk∼πX|Y(·|yk)∝πX(·) exp −| · − yk|2
2η
//Calls to RGO
return xN
Theorem 1. Suppose πX∝exp(−V)satisfies Assumption 1. Let Xtbe the solution of the Langevin
diffusion, and µt:= Law( Xt). Then, for any δ >0,
TV(πX, µt)≥Cν1,ν2dν1−ν2
2(1+δ)(Cδ(µ0) +κδt)−ν2(1+δ)
2,
where κδ:= 1∨2
d+ν2∨ν2(1+δ)
(d+ν2)δ,Cδ(µ0):=1
d+ν2E[(1 +|X0|2)γ]1/γwithγ=κδ(d+ν2)/2, and
Cν1,ν2is a constant depending only on ν1andν2.
If we assume |X0| ≤ O (√
d)for simplicity, then by choosing δ=2 ln ln t
ν2lnt∧2 ln ln d
(ν2−ν1) lnd, we obtain
TV(πX, µt)≥˜Ων1,ν2(dν1−ν2
2t−ν2
2).
Thus, LD requires at least T=˜Ων1,ν2 
dν1−ν2
ν2(1/ε)2/ν2
to reach εerror in total variation. While
this bound may be small in high dimensions when ν2> ν1, for the canonical model of Cauchy-type
potentials with ν2=ν1=ν, it will be independent of dimension, as stated by the following result.
Note that Assumption 1 can also cover a general scaling by replacing |x|withc|x|for some constant
c, which would introduce a multiplicative factor of 1/c2for the lower bound on T. This is expected
as e.g., mixing to the Gibbs potential c2|x|2can be faster than mixing to |x|2by a factor of 1/c2.
Corollary 1. Consider the generalized Cauchy density πX
ν∝exp(−Vν)where Vνis as in (1).
LetXtbe the solution of the Langevin diffusion, and µt:= Law( Xt). For simplicity, assume the
initialization satisfies |X0| ≤ O (√
d). Then, achieving TV(πX
ν, µT)≤εrequires T≥˜Ων 
ε−2
ν
.
The above lower bound implies that LD is a low-accuracy “sampler” for this target density in the
sense that it depends polynomially on 1/ε; this dependence gets worse with smaller νas the tails
get heavier. It is worth highlighting the gap between the upper bound of [ MHFH+23, Corollary 8],
which is ˜O 
1/ε4/ν
, and the lower bound in Corollary 1.
Gaussian proximal sampler. In the remainder of this section, we prove that the Gaussian proximal
sampler, described in Algorithm 1, also suffers from a poly(1 /ε)rate when the target density is
heavy-tailed. In each iteration of Algorithm 1, the first step involves sampling a standard Gaussian
random variable ykcentered at the current iterate xkwith variance ηI; this is a one-step isotropic
Brownian random walk. Alternatively, since the Fokker-Planck equation of the standard Brownian
motion is the classical heat equation, this step could also be interpreted as an exact simulation of the
heat flow; see, for example, [ CG03 ] and [ Wib18 ]. Specifically, the density of ykis the solution to
the heat flow at time ηwith the initial condition being the density of xk. The second step is called
the restricted Gaussian oracle (RGO) as coined by [ LST21b ]; under which (xk, yk)is a reversible
Markov chain whose stationary density has x-marginal πX.
Assumption 2. For some ν2≥ν1≥0, the target πX(x)∝exp(−V(x))withV:Rd→Rsatisfies
∀x∈Rd(d+ν1)|x|2
1 +|x|2≤ ⟨x,∇V(x)⟩,|∇V(x)| ≤(d+ν2)|x|
1 +|x|2,∆V(x)≤(d+ν2)2
1 +|x|2.
The first condition above also appears in Assumption 1 and the second condition implies the upper
bound of Assumption 1; thus, the above assumption is stronger. Note that the generalized Cauchy
measure (1)satisfies this assumption with ν1=ν2=ν. Under Assumption 2, we state the following
lower bound on the Gaussian proximal sampler and defer its proof to Appendix A.
Theorem 2. Suppose πX∝exp(−V)satisfies Assumption 2. Let xkdenote the kthiterate of the
Gaussian proximal sampler (Algorithm 1) with step ηand let ρX
k:= Law( xk). Then, for any δ >0,
TV(πX, ρX
k)≥Cν1,ν2dν1−ν2
2(1+δ)(Cδ(µ0) +κδηk)−ν2(1+δ)
2,
where κδ,Cδ(µ0), and Cν1,ν2are defined in Theorem 1.
4Above, assuming |X0| ≤ O (√
d)with the same choice of δas in Theorem 1 yields TV(πX
ν, ρX
k)≥
˜Ων1,ν2 
dν1−ν2
2(kη)−ν2
2
.Note that in order for the RGO step to be efficiently implementable, we
need to have a sufficiently small η. The state-of-the-art implementation of RGO requires a step size of
order η=˜O(1/(Ld1/2))when VhasL-Lipschitz gradients [ FYC23 ]. With this choice of step size,
the above lower bound requires at least N=˜Ων1,ν2 
Ld1/2+(ν1−ν2)/ν2(1/ε)2/ν2
iterations. The
assumptions in Theorem 2 once again cover the canonical examples of generalized Cauchy densities,
where we have L=d+ν, which simplifies the lower bound as follows.
Corollary 2. Consider the generalized Cauchy density πX
ν∝exp(−Vν)where Vνis as in (1).
Letxkdenote the kthiterate of the Gaussian proximal sampler, and define ρX
k:= Law( xk), and
choose the step size η=˜O(1/(Ld1/2)). If we assume |X0| ≤ O (√
d)for simplicity, then achieving
TV(πX
ν, ρX
N)≤εrequires N≥˜Ων 
d3
2ε−2
ν
iterations.
We emphasize that the above lower bound is of order poly(1/ε)as advertised. Thus, the RGO-based
proximal sampler can only yield a low-accuracy guarantee in this setting.
3 Stable Proximal Sampler and the Restricted α-Stable Oracle
Having characterized the limitations of Gaussian oracles for heavy-tailed sampling, thereby answering
Q1, in what follows, we will focus on Q2and construct proximal samplers based on the α-stable
oracle, and prove that they achieve high-accuracy guarantees when sampling from heavy-tailed targets.
First, we provide a basic overview of α-stable processes and fractional heat flows.
Isotropic α-stable process. Fort≥0, letX(α)
tbe the isotropic stable Lévy process in Rd, starting
from x∈Rd, with the index of stability α∈(0,2], defined uniquely via its characteristic function
Exei⟨ξ,X(α)
t−x⟩=e−t|ξ|α. When α= 2,X(2)
tis a scaled Brownian motion, and when 0< α < 2, it
becomes a pure Lévy jump process in Rd. The transition density of X(α)
tis then given by
p(α)(t;x, y) =p(α)
t(y−x)with p(α)
t(y) = (2 π)−dZ
Rdexp(−t|ξ|α)e−i⟨ξ,y⟩dξ, (2)
where the second equation above is the inverse Fourier transform of the characteristic function, thus
returns the density. The transition kernel and the density in (2)have closed-form expressions for the
special cases α= 1,2. In particular, when α= 1,p(1)
treduces to a Cauchy density with degrees of
freedom ν= 1, i.e. p(1)
t(y)∝(|y|2+t2)−(d+1)/2. We finally note that the isotropic stable Lévy
process X(α)
tdisplays self-similarity like the Brownian motion; the processes X(α)
atanda1/αX(α)
t
have the same distribution. This property is crucial in the development of the stable proximal sampler.
Fractional heat flow. The equation ∂tu(t, x) =−(−∆)α/2u(t, x)with the condition u(0, x) =
u0(x)is an extension of the classical heat flow, and is referred to as the fractional heat flow. Here,
−(−∆)α/2is the fractional Laplacian operator with α∈(0,2], which is the infinitesimal generator
of the isotropic α-stable process. For α= 2, it reduces to the standard Laplacian operator ∆.
Stable proximal sampler. Letπ(x, y)be a joint density such that π(x, y)∝πX(x)p(α)(η;x, y),
where πXis the target and p(α)(η;x, y)is the transition density of the α-stable process, introduced in
(2). It is easy to verify that (i) the X-marginal of πisπX, (ii) the conditional density of Ygiven Xis
πY|X(·|x) =p(α)(η;x,·), (iii) the Y-marginal is πY=πX∗p(α)
η, i.e.πYis obtained by evolving
πXalong the α-fractional heat flow for time η, and (iv) the conditional density of Xgiven Yis
πX|Y(·|y)∝πX(·)p(α)(η;·, y).Based on these, we introduce the following stable oracle.
Definition 1 (Restricted α-Stable Oracle) .Given y∈Rd, an oracle that outputs a random vector
distributed according to πX|Y(·|y), is called the Restricted α-Stable Oracle (R αSO).
Note that when α= 2, the R αSO reduces to the RGO of [ LST21b ]. The Stable Proximal Sampler
(Algorithm 2) with parameter αis initialized at a point x0∈Rdand performs Gibbs sampling on
the joint density π. In each iteration, the first step involves sampling an isotropic α-stable random
vector ykcentered at the current iterate xk, which is a one-step isotropic α-stable random walk. This
could also be interpreted as an exact simulation of the fractional heat flow. Indeed, due to the relation
between the fractional heat flow and the isotropic stable process, the density of ykis exactly the
solution to the α-fractional heat flow at time ηwith the initial condition being the density of xk.
5Algorithm 2 Stable Proximal Sampler with parameter α
Input: Sample x0, step η >0, and α∈(0,2).
fork= 0,1,···, N−1 //Gibbs sampler
Sample yk∼πY|X(·|xk) =p(α)(η;xk,·) //Fractional heat flow
Sample xk+1|yk∼πX|Y(·|yk)∝πX(·)p(α)(η;·, yk) //Calls to R αSO
return xN
When α= 2, the first step reduces to an isotropic Brownian random walk and a simulation of the
classical heat flow. The second step calls the R αSO at the point yk.
3.1 Convergence guarantees
We next provide convergence guarantees for the stable proximal sampler in χ2-divergence assuming
access to the R αSO. Similar results for a practical implementation are presented in Section 3.2. To
proceed, we introduce the fractional Poincaré inequality, first introduced in [ WW15 ] to characterize a
class of heavy-tailed densities including the canonical Cauchy class.
Definition 2 (Fractional Poincaré Inequality) .Forϑ∈(0,2), a probability density µsatisfies a
ϑ-fractional Poincaré inequality (FPI) if there exists a positive constant CFPI(ϑ)such that for any
function ϕ:Rd→Rin the domain of E(ϑ)
µ, we have
Varµ(ϕ)≤CFPI(ϑ)E(ϑ)
µ(ϕ). (FPI)
where E(ϑ)
µis a non-local Dirichlet form associated with µdefined as
E(ϑ)
µ(ϕ) :=cd,ϑZZ
{x̸=y}(ϕ(x)−ϕ(y))2
|x−y|(d+ϑ)dxµ(y)dywith cd,ϑ=2ϑΓ((d+ϑ)/2)
πd/2|Γ(−ϑ/2)|.
Remark 2. FPI is a weaker condition than Assumption 2. In fact, any density satisfying the first 2
conditions in Assumption 2 satisfies ϑ-FPI for all ϑ < ν 1[WW15 , Theorem 1.1]. In Proposition 2,
we show that as ϑ→2−, FPI becomes equivalent to the standard Poincaré inequality.
In the sequel, ρX
kdenotes the law of xk,ρY
kdenotes the law of yk, andρk=ρX,Y
kis the joint law of
(xk, yk). We provide the following convergence guarantee under an FPI, proved in Appendix B.2.
Theorem 3. Assume that πXsatisfies the α-FPI with parameter CFPI(α)forα∈(0,2). For any step
sizeη >0and initial density ρX
0, thekthiterate of Algorithm 2, with parameter α, satisfies
χ2(ρX
k|πX)≤exp
−kη 
CFPI(α)+η−1
χ2(ρX
0|πX).
As a consequence of Remark 2 and Proposition 2, we recover the result in [ CCSW22 , Theorem 4], by
letting α→2−. While our results in Theorem 3 are based on Algorithm 2 which requires exact calls
to RαSO, the next result, proved in Appendix B.3, shows that even with an inexact implementation
of RαSO, the error accumulation is at most linear, and Algorithm 2 still converges quickly.
Proposition 1. Suppose the R αSO in Algorithm 2 is implemented inexactly, i.e. there exists a positive
constant εTVsuch that TV(˜ρX|Y
k(·|y), ρX|Y
k(·|y))≤εTVfor all y∈Rdandk≥1, where ˜ρX|Y
k(·|y)
is the density of the inexact R αSO sample conditioned on y. Let ˜ρX
kbe the density of the output of
thekthstep of Algorithm 2 with the inexact R αSO and ρX
kbe the density of the output of kthstep
Algorithm 2 with the exact R αSO. Then, for all k≥0,
TV(˜ρX
k, ρX
k)≤TV(˜ρX
0, ρX
0) +k εTV.
Further, if ˜ρX
0=ρX
0, for any K≥K0, we get TV(˜ρK
X, πX)≤ε, ifεTV≤ε/2K, where the constant
K0= (1 + CFPI(α)η−1) log 
χ2(˜ρX
0|πX)/ε2
withCFPI(α)being the α-FPI parameter of πX.
3.2 A practical implementation of R αSO
In the sequel, we introduce a practical implementation of R αSO when α= 1. For this, we consider
the case when the target density πX∝e−Vsatisfies the 1-FPI with parameter CFPI(1). A more
thorough implementation of R αSO for other values of αwill be investigated in future work.
Assumption 3. There exist constants β, L > 0such that for any minimizer x∗∈arg miny∈RdV(y)
and for all x∈Rd,Vsatisfies V(x)−V(x∗)≤L|x−x∗|β.
6Algorithm 3 RαSO Implementation for α= 1via Rejection Sampling
Input: V,x∗∈arg min V,η >0,y∈Rd.
while TRUE // Rejection sampling
Generate (Z1, Z2, u)∼ N(0, Id)⊗ N(0,1)⊗U[0,1]
x←y+ηZ1/|Z2| //Cauchy random vector
return xifu≤exp(−V(x) +V(x∗)) //Accept-reject step
Algorithm 3 provides an exact implementation of R αSO for α= 1via rejection sampling. Inputs to
this algorithm are the intermediate points ykin the stable proximal sampler (Algorithm 2). Note that
Algorithm 3 requires a global minimizer of V, which is always assumed to exist, which guarantees
that the acceptance probability is non-trivial. It generates proposals with density p(1)(η;·, y)and
utilizes that p(1)is a Cauchy density and Cauchy random vectors can be generated via ratios between
a Gaussian random vector and square-root of a χ2random variable. Finally, the accept-reject step
ensures that the output xhas density πX|Y(·|y)∝e−Vp(1)(η;·, y). This makes Algorithm 3 a
zeroth-order algorithm requiring only access to function evaluations of V. Under Assumption 3, by
choosing a small step-size, we can control the expected number of rejections in Algorithm 3. We now
state the iteration complexity of our stable proximal sampler with this R αSO implementation in the
following result, whose proof is provided in Appendix B.3.
Corollary 3. Assume V satisfies Assumption 3. If we choose the step-size η= Θ( d−1
2L−1
β), then
Algorithm 3 implements the R αSO with α= 1, with the expected number of zeroth-order calls to Vof
orderE[exp( L|yk|β)]. Further assume πXsatisfies 1-FPI with parameter CFPI(1). Suppose we run
Algorithm 2 with R αSO implemented for with α= 1by Algorithm 3. Then, to return a sample which
isε-close in χ2-divergence to the target, the expected number of iterations required by Algorithm 2 is
O 
CFPI(1)d1
2L1
βlog(χ2(ρX
0|πX)/ε)
.
Note that the above result provides a high-accuracy guarantee for the implementable version of the
stable proximal sampler (Algorithm 3) for a class of heavy-tailed targets, overcoming the fundamental
barrier established in Theorem 2 for the Gaussian proximal sampler (i.e., Algorithm 1). A numerical
illustration of this improvement is provided in Appendix D by sampling from student-t distributions.
Remark 3. (1) Finding a global minimizer of the potential Vcan be hard, which could be avoided if a
lower bound on the potential Vis available; see Appendix B.3. (2) A trivial bound for E[exp( L|yk|β)]
isexp(LM)forM=EπX[|X|β] +χ2(ρX
0|πX)EπX[|X|2β]1
2. Since our main focus is high vs low
accuracy samplers, deriving a sharper bound is beyond the scope of the current paper.
3.3 Illustrative examples
To illustrate our results, we now apply the proximal algorithms to sample from Cauchy densities and
discuss the complexity of both the ideal sampler (Algorithm 2) in which we can choose any α∈(0,2)
and the implementable version with α= 1 (Algorithm 3). For the ideal sampler, we can choose
α≤νfor any degrees of freedom ν >0, and apply Theorem 3 since πνsatisfies a α-FPI [WW15].
Corollary 4. For any ν >0, consider the generalized Cauchy target πν∝exp(−Vν)withVνdefined
in(1). For the stable proximal sampler with parameter α∈(0,2)andα≤ν(i.e., Algorithm 2),
suppose we set the step-size η∈(0,1)and draw the initial sample from the standard Gaussian
density. Then, the number of iterations required by Algorithm 2 to produce an ε-accurate sample in
χ2-divergence is O(CFPI(α)η−1log(d/ε)), where CFPI(α)is the α-FPI parameter of πν.
For the implementable sampler, since the parameter αis fixed to be 1, whether a suitable FPI is
satisfied or not depends on the degrees of freedom ν. Specifically, when ν≥1,1-FPI is satisfied
and Corollary 5 applies. When ν∈(0,1), on the other hand, 1-FPI is not satisfied. To tackle this
issue, we prove convergence guarantees for the proximal sampler under a weak fractional Poincaré
inequality; the next corollary, proved in Appendix B.4, summarizes these results.
Corollary 5. For the Cauchy target πν∝exp(−Vν)where Vνis defined in (1), we consider
Algorithm 2 with α= 1, a standard Gaussian initialization, and R αSO implemented by Algorithm 3.
(1)When ν≥1, if we set the step-size η= Θ 
d−1
2(d+ν)−4
, the expected number iterations
required by Algorithm 2 to output a sample which is ε-close in χ2-divergence to the target is of
orderO 
CFPI(1)d1
2(d+ν)4log(d/ε)
, where CFPI(1)is the 1-FPI parameter of πν.
7(2)When ν∈(0,1), if we set the step-size η= Θ 
d−1
2(d+ν)−4
ν
, the expected number of
iterations required by Algorithm 2, to output a sample which is ε-close in χ2-divergence to the
target is of order ˜O 
max
c1
νd1
2ν+4
ν2, cd1
2+4
νε−1
ν+1	
, where cis the positive constant given
in(16). Here, ˜Ohides the polylog factors on dand1/ε.
The stable proximal sampler (Algorithm 2) is a high accuracy sampler for the class of generalized
Cauchy targets, as long as α≤ν, meaning that it achieves log( 1/ε) iteration complexity. The
improvement from poly( 1/ε) to log( 1/ε) separates the stable proximal sampler and the Gaussian
proximal sampler in the task of heavy-tailed sampling. When we use the rejection-sampling imple-
mentation with parameter α= 1(Algorithm 3), iteration complexity goes through a phase transition
as the tails get heavier. When the generalized Cauchy density has a finite mean ( ν >1), we achieve
a high-accuracy sampler with log( 1/ε) iteration complexity. However, without a finite mean (i.e.,
ν∈(0,1)), the algorithm becomes a low-accuracy sampler with poly( 1/ε) complexity. Even in this
low-accuracy regime, the implementable stable proximal sampler outperforms the Gaussian one, as
originally highlighted in Table 1. Last, we claim that the poly( 1/ε) complexity of Algorithms 2 and 3
is not due to a loose analysis, as we show poly(1 /ε)lower bounds in the following section.
3.4 Lower bounds for the stable proximal sampler
We now study lower bounds on the stable proximal sampler to sample from the class of target densities
satisfying Assumption 2, which includes the generalized Cauchy target. Recall that Assumption 2
implies the FPI used in Theorem 3. The result below, proved in Appendix C, complements Theorem 3,
showing the impossibility of achieving log(1/ε)rates for a sufficiently large α.
Theorem 4. Suppose πX∝exp(−V)withVsatisfying Assumption 2 andν2(d+ν2)
d+ν1< α≤2. Let
xkdenote the kthiterate of Algorithm 2 with parameter αand step size η, and let ρX
k:= Law( xk).
Then for any τ∈ ν2(d+ν2)
d+ν1, α
, and g(d, ν1, ν2, τ) =ν2/{τ(d+ν1)−ν2(d+ν2)}, we have
TV(πX, ρX
k)≥Cν1,ν2,αdτ(d+ν1)g(d,ν1,ν2,τ)
2 
E[(1 +|x0|2)τ
2] +m(α)
τkτ
2+1ητ
α−(d+ν2)g(d,ν1,ν2,τ),
where Cν1,ν2,αis a constant depending only on ν1, ν2, α, and m(α)
τis the τthabsolute moment of the
α-stable random variable with density p(α)
1defined in (2).
Remark 4. The parameter τin Theorem 4 can be chosen arbitrarily close to α. Specifically, if we
assume |X0| ≤ O (√
d), then with the choice of τ=α− log(log d)
logd∧log log( η−1)
log(η−1)
, we have
TV(πX, ρX
k)≥˜Ων1,ν2,α 
dτ(d+ν1)g(d,ν1,ν2,α)
2 
dα+m(α)
τkα
2+1η−(d+ν2)g(d,ν1,ν2,α)
,
where ˜Ωhides polylog (d/η)factors.
Theτthabsolute moment of the α-stable random variable depends on the choice of αand the
dimension d. It is hard to find an explicit formula of m(α)
τin general. An explicit formula is only
available in some special cases, such as α= 1,2. Specializing Theorem 4 for the generalized Cauchy
potential (i.e., ν1=ν2) we obtain the following explicit result.
Corollary 6. Letα∈(0,2]. Suppose πν∝exp(−Vν)where Vν(x)is as in (1)for some ν∈(0, α).
Let(xk)k≥0be the output of Algorithm 2 with parameter αand step-size η >0, and ρX
k:= Law( xk)
for all k≥0. Then for any τ∈(ν, α),
TV(ρX
k, πν)≥Cν,αdντ
2(τ−ν) 
E[(1 +|x0|2)τ
2] +m(α)
τkτ
2+1ητ
α−ν
τ−ν.
where m(α)
τis the τthabsolute moment of the α-stable random variable with density p(α)
1as in (2).
For the rejection sampling implementation in Algorithm 3, α= 1andm(1)
τ= Θ( dτ
2)for all τ <1
(see Appendix B.1). Notice that to implement the R αSO in the Stable proximal sampler efficiently,
we need a sufficiently small step-size η. When the target potential satisfies Assumption 3, i.e. V
isβ-Hölder continuous with parameter L, we require η= Θ( d−1
2L−1
β)to ensure R αSO can be
implemented with O(1)queries. Therefore, if we choose η= Θ( d−1
2L−1
β), the minimum number
of iterations we need to get an ε-error in TVis
Ων,τ
ε−2(τ−ν)
(2+τ)νdτ
2+τL2τ
β(2+τ)
.
8For the generalized Cauchy potential with ν∈(0,1), we have β=ν/4andL= (d+ν)/ν, which
leads to the following corollary.
Corollary 7. Suppose πX
ν∝exp(−Vν)is the generalized Cauchy density with ν∈(0,1). Letxk
denote the k-th iterate of the stable proximal sampler with α= 1(Algorithm 3), and ρX
k:= Law( xk).
If we choose the step size η= Θ( L−4
νd−1
2)where L=d+ν
νis the ν/4-Hölder constant of Vν, and as-
sume, for simplicity, |x0| ≤ O (√
d), then, TV(πX
ν, ρX
N)≤εrequires N≥Ων,τ 
dτ+8τ/ν
2+τε−2(τ−ν)
ν(2+τ)
,
for any τ∈(ν,1). Further, by choosing τ= max( ν,1−log(log( d/ε))
log(d/ε)), we obtain
N≥˜Ων
dν+8
3νε−2(1−ν)
3ν
,in order for TV(πX
ν, ρX
N)≤ε.
The above result shows that when implementing the R αSO in Algorithm 2 with Algorithm 3, to
sample from generalized Cauchy targets with ν∈(0,1), we can at best have an iteration complexity
of order poly (1/ε), matching the upper bounds in Corollary 5 up to certain factors.
4 Overview of Proof Techniques
Lower bounds. We build on the techniques developed in [ Hai10 ]. Let µtdenotes the law of LD
along its trajectory. To proceed, we need some G:Rd→Rfor which we can upper bound µt(G):=R
Gdµt, and some f:Rd→Rthat satisfies πX(G≥y)≥f(y)for all y∈R+. After finding the
candidates Gandf, Lemma 1 in Appendix A guarantees TV(πX, µt)≥supy∈R+f(y)−µt(G)/y.
This technique relies on choosing Gsuch that it has heavy tails under πXleading to a large f(y),
while having light tails along the trajectory, thus small µt(G). By picking G= exp( κV)withκ≥1,
one can immediately observe that πX(G) =∞, thus Gindeed has heavy tails under πX.
To control µt(G)along the trajectory, one can use the generator of LD to bound ∂tµt(G). Recall
the generator of LD, LLD(·) = ∆( ·)− ⟨∇ V,∇·⟩. Therefore, with a choice of G= exp( κV),
controlling ∂tµt(G)requires bounding the first and second derivatives of V. To avoid making extra
assumptions for Vin the analysis of LD, we instead construct Gbased on a surrogate potential
˜V(x) =d+ν2
2ln(1 + |x|2), which is an upper bound to the potential V. We then estimate fbased on
this surrogate potential in Lemma 2, and control the growth of µt(G)in Lemma 3. Combined with
Lemma 1, this leads to the proof of Theorem 1, with the details provided in Appendix A.
For the Gaussian proximal sampler, bounding ρX
k(G)requires controlling the expectation of G
along the forward and backward heat flow. For the particular choice of G= exp( κV), we show
in Lemma 4 that the growth of ρX
k(G)can be controlled only by considering a forward heat flow
with the corresponding generator LHF=1
2∆. Therefore, given additional estimates on the second
derivatives of V, we bound the growth of ρX
k(G)in Lemma 5. Once this bound is achieved, we can
invoke Lemma 1 to finish the proof of Theorem 2.
Upper bounds. Our upper bound analysis builds on that by [ CCSW22 ] in the specific ways discussed
next. We consider the change in χ2divergence when we apply the two operations to the law ρX
kto
the iterates and the target πX:(i)evolving the two densities along the α-fractional heat flow for time
ηand(ii)applying the R αSO to the resulting densities. For the step (i), it is required to show that
the solution along the fractional heat flow of the stable proximal sampler at any time, satisfies FPI. To
show this, (a)the convolution property of the FPI is proved in Lemma 6, and (b)the FPI parameter
for the stable process follows from [ Cha04 , Theorem 23]. In Proposition 3, it is then shown that
theχ2divergence decays exponentially fast along the fractional heat flow under the assumption of
FPI. The aforementioned results enable us to prove the exponential decay of χ2divergence along
the fractional heat flow under FPI in Proposition 3. To deal with the step (ii) above, we use the data
processing inequality; see Proposition 3. These two steps together, enable us to derive the stated
upper bounds for the stable proximal sampler.
5 Discussion
We showed the limitations of Gaussian proximal samplers for high-accuracy heavy-tailed sampling,
and proposed and analyzed stable proximal samplers, establishing that they are indeed high-accuracy
algorithms. We now list a few important limitations and problems for future research: (i) It is
important to develop efficiently implementable versions of the stable proximal sampler for all values
ofα∈(0,2), and characterize their complexity in terms of problem parameters, (ii) Gaussian
9proximal samplers can be interpreted as a proximal point method for approximating the entropic
regularized Wasserstein gradient flow of the KL objective [ CCSW22 ]. This leads to the question,
can we provide a variational intepreration of the stable proximal sampler? A potential approach is
to leverage the results by [ Erb14 ] on gradient flow interpretation of jump processes corresponding
to the fractional heat equation, (iii) It is possible to use a non-standard Itô process in the proximal
sampler (in place of the α-stable diffusion); see, for example, [ EMS18 ,LWME19 ,HFBE24 ]. With
this modification, it is interesting to examine the rates under weighted Poincaré inequalities that also
characterize heavy-tailed densities. There are two difficulties to overcome here: (a)How to generate
an exact non-standard Itô process? (b)How to implement the corresponding Restricted non-standard
Gaussian Oracle, which requires the zeroth order information of the transition density of the Itô
process? In certain cases, non-standard Itô diffusion can be interpreted as a Brownian motion on an
embedded sub-manifold; thus, the approach in [GLL+23] might be useful.
Acknowledgements
KB was supported in part by NSF grants DMS-2053918 and DMS-2413426.
References
[ALPW22] Christophe Andrieu, Anthony Lee, Sam Power, and Andi Q Wang, Comparison of
Markov chains via weak Poincaré inequalities with application to pseudo-marginal
MCMC , The Annals of Statistics 50(2022), no. 6, 3592–3618.
[ALPW23] ,Weak Poincaré Inequalities for Markov chains: Theory and Applications ,
arXiv preprint arXiv:2312.11689 (2023).
[App09] David Applebaum, Lévy processes and stochastic calculus , Cambridge university press,
2009.
[BCE+22] Krishnakumar Balasubramanian, Sinho Chewi, Murat A Erdogdu, Adil Salim, and
Shunshi Zhang, Towards a theory of non-log-concave sampling: First-order station-
arity guarantees for Langevin Monte Carlo , Conference on Learning Theory, PMLR,
2022, pp. 2896–2923.
[BHJ08] Krzysztof Bogdan, Wolfhard Hansen, and Tomasz Jakubowski, Time-dependent
Schrödinger perturbations of transition densities , Studia Mathematica 189(2008),
no. 3, 235–254.
[BRZ19] Joris Bierkens, Gareth O Roberts, and Pierre-André Zitt, Ergodicity of the zigzag
process , The Annals of Applied Probability 29(2019), no. 4, 2266–2301.
[BZ17] Maria-Florina F Balcan and Hongyang Zhang, Sample and computationally efficient
learning algorithms under s-concave distributions , Advances in Neural Information
Processing Systems 30(2017).
[CBL22] Niladri S Chatterji, Peter L Bartlett, and Philip M Long, Oracle lower bounds for
stochastic gradient sampling algorithms , Bernoulli 28(2022), no. 2, 1074–1092.
[CCBJ18] Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan, Underdamped
Langevin MCMC: A non-asymptotic analysis , Conference on learning theory, PMLR,
2018, pp. 300–323.
[CCSW22] Yongxin Chen, Sinho Chewi, Adil Salim, and Andre Wibisono, Improved analysis for
a proximal algorithm for sampling , Conference on Learning Theory, PMLR, 2022,
pp. 2984–3014.
[CDV09] Karthekeyan Chandrasekaran, Amit Deshpande, and Santosh Vempala, Sampling
s-concave functions: The limit of convexity based isoperimetry , International Work-
shop on Approximation Algorithms for Combinatorial Optimization, Springer, 2009,
pp. 420–433.
[CG03] Eric A Carlen and Wilfrid Gangbo, Constrained steepest descent in the 2-Wasserstein
metric , Annals of mathematics (2003), 807–846.
10[CG23] Yuansi Chen and Khashayar Gatmiry, A Simple Proof of the Mixing of Metropolis-
Adjusted Langevin Algorithm under Smoothness and Isoperimetry , arXiv preprint
arXiv:2304.04095 (2023).
[CGL+22] Sinho Chewi, Patrik R Gerber, Chen Lu, Thibaut Le Gouic, and Philippe Rigollet, The
query complexity of sampling from strongly log-concave distributions in one dimension ,
Proceedings of Thirty Fifth Conference on Learning Theory, vol. 178, PMLR, 2022,
pp. 2041–2059.
[CGLL22] Sinho Chewi, Patrik Gerber, Holden Lee, and Chen Lu, Fisher information lower
bounds for sampling , arXiv preprint arXiv:2210.02482 (2022).
[Cha04] Djalil Chafaï, Entropies, convexity, and functional inequalities, on Φ-entropies and
Φ-sobolev inequalities , Journal of Mathematics of Kyoto University 44(2004), no. 2,
325–363.
[CLA+21] Sinho Chewi, Chen Lu, Kwangjun Ahn, Xiang Cheng, Thibaut Le Gouic, and Philippe
Rigollet, Optimal dimension dependence of the Metropolis-Adjusted Langevin Algo-
rithm , Conference on Learning Theory, PMLR, 2021, pp. 1260–1300.
[CLW23] Yu Cao, Jianfeng Lu, and Lihan Wang, On explicit L2-convergence rate estimate for
underdamped Langevin dynamics , Archive for Rational Mechanics and Analysis 247
(2023), no. 5, 90.
[DBCD19] George Deligiannidis, Alexandre Bouchard-Côté, and Arnaud Doucet, Exponential
ergodicity of the Bouncy Particle Sampler , Annals of Statistics 47(2019), no. 3.
[DCWY19] Raaz Dwivedi, Yuansi Chen, Martin J Wainwright, and Bin Yu, Log-concave sampling:
Metropolis-Hastings algorithms are fast , Journal of Machine Learning Research 20
(2019), no. 183, 1–42.
[DGM20] Alain Durmus, Arnaud Guillin, and Pierre Monmarché, Geometric ergodicity of the
Bouncy Particle Sampler , Annals of applied probability 30(2020), no. 5, 2069–2098.
[DKTZ20] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis, Learning
halfspaces with Massart noise under structured distributions , Conference on Learning
Theory, PMLR, 2020, pp. 1486–1513.
[DM17] Alain Durmus and Éric Moulines, Nonasymptotic convergence analysis for the un-
adjusted Langevin algorithm , The Annals of Applied Probability 27(2017), no. 3,
1551–1587 (en).
[DRD20] Arnak S Dalalyan and Lionel Riou-Durand, On sampling from a log-concave density
using kinetic Langevin diffusions , Bernoulli 26(2020), no. 3, 1956–1988.
[EGZ19] Andreas Eberle, Arnaud Guillin, and Raphael Zimmer, Couplings and quantitative
contraction rates for Langevin dynamics , The Annals of Probability 47(2019), no. 4,
1982–2010.
[EH21] Murat A Erdogdu and Rasa Hosseinzadeh, On the convergence of Langevin Monte
Carlo: The interplay between tail growth and smoothness , Conference on Learning
Theory, PMLR, 2021, pp. 1776–1822.
[EHZ22] Murat A Erdogdu, Rasa Hosseinzadeh, and Shunshi Zhang, Convergence of Langevin
Monte Carlo in chi-squared and Rényi divergence , International Conference on Artifi-
cial Intelligence and Statistics, PMLR, 2022, pp. 8151–8175.
[EMS18] Murat A Erdogdu, Lester Mackey, and Ohad Shamir, Global non-convex optimization
with discretized diffusions , Advances in Neural Information Processing Systems 31
(2018).
[Erb14] Matthias Erbar, Gradient flows of the entropy for jump processes , Annales de l’IHP
Probabilités et statistiques, vol. 50, 2014, pp. 920–945.
11[FYC23] Jiaojiao Fan, Bo Yuan, and Yongxin Chen, Improved dimension dependence of a
proximal algorithm for sampling , arXiv preprint arXiv:2302.10081 (2023).
[GB09] Alan Genz and Frank Bretz, Computation of multivariate normal and t-probabilities ,
vol. 195, Springer Science & Business Media, 2009.
[GBH04] Alan Genz, Frank Bretz, and Yosef Hochberg, Approximations to multivariate tinte-
grals with application to multiple comparison procedures , Recent Developments in
Multiple Comparison Procedures, Institute of Mathematical Statistics, 2004, pp. 24–32.
[GJPS08] Andrew Gelman, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su, A weakly
informative default prior distribution for logistic and other regression models , The
annals of applied statistics 2(2008), no. 4, 1360–1383.
[GLL20] Rong Ge, Holden Lee, and Jianfeng Lu, Estimating normalizing constants for log-
concave distributions: Algorithms and lower bounds , Proceedings of the 52nd Annual
ACM SIGACT Symposium on Theory of Computing, 2020, pp. 579–586.
[GLL+23] Sivakanth Gopi, Yin Tat Lee, Daogao Liu, Ruoqi Shen, and Kevin Tian, Algorithmic
aspects of the log-Laplace transform and a non-Euclidean proximal sampler , arXiv
preprint arXiv:2302.06085 (2023).
[GLM18] Joyee Ghosh, Yingbo Li, and Robin Mitra, On the use of Cauchy prior distributions
for Bayesian logistic regression , Bayesian Analysis 13(2018), no. 2, 359–383.
[Hai10] Martin Hairer, Convergence of Markov processes , Lecture notes (2010).
[HBE20] Ye He, Krishnakumar Balasubramanian, and Murat A Erdogdu, On the ergodicity,
bias and asymptotic normality of randomized midpoint sampling method , Advances in
Neural Information Processing Systems 33(2020), 7366–7376.
[HBE24] ,An analysis of Transformed Unadjusted Langevin Algorithm for Heavy-tailed
Sampling , IEEE Transactions on Information Theory (2024).
[HFBE24] Ye He, Tyler Farghly, Krishnakumar Balasubramanian, and Murat A Erdogdu, Mean-
square analysis of discretized Itô diffusions for heavy-tailed sampling , Journal of
Machine Learning Research (to appear) (2024).
[HMW21] Lu-Jing Huang, Mateusz B Majka, and Jian Wang, Approximation of heavy-tailed
distributions via stable-driven SDEs , Bernoulli 27(2021), no. 3, 2040–2068.
[JG12] Leif T Johnson and Charles J Geyer, Variable transformation to obtain geometric
ergodicity in the Random-Walk Metropolis algorithm , The Annals of Statistics 40
(2012), no. 6, 3050–3076.
[JR07] Søren Jarner and Gareth Roberts, Convergence of heavy-tailed Monte Carlo Markov
Chain algorithms , Scandinavian Journal of Statistics 34(2007), no. 4, 781–815.
[Kam18] Kengo Kamatani, Efficient strategy for the Markov chain Monte Carlo in high-
dimension with heavy-tailed target probability distribution , Bernoulli 24(2018), no. 4B,
3711–3750.
[KN04] Samuel Kotz and Saralees Nadarajah, Multivariate t-distributions and their applica-
tions , Cambridge University Press, 2004.
[Kwa17] Mateusz Kwa ´snicki, Ten equivalent definitions of the fractional Laplace operator ,
Fractional Calculus and Applied Analysis 20(2017), no. 1, 7–51.
[LST20] Yin Tat Lee, Ruoqi Shen, and Kevin Tian, Logsmooth gradient concentration and
tighter runtimes for Metropolized Hamiltonian Monte Carlo , Conference on learning
theory, PMLR, 2020, pp. 2565–2597.
[LST21a] ,Lower bounds on Metropolized sampling methods for well-conditioned distri-
butions , Advances in Neural Information Processing Systems 34(2021), 18812–18824.
12[LST21b] ,Structured logconcave sampling with a Restricted Gaussian Oracle , Confer-
ence on Learning Theory, PMLR, 2021, pp. 2993–3050.
[LWME19] Xuechen Li, Yi Wu, Lester Mackey, and Murat A Erdogdu, Stochastic Runge-Kutta
accelerates Langevin Monte Carlo and beyond , Advances in neural information pro-
cessing systems 32(2019).
[LZT22] Ruilin Li, Hongyuan Zha, and Molei Tao, Sqrt(d) Dimension Dependence of Langevin
Monte Carlo , The International Conference on Learning Representations, 2022.
[MFWB22] Wenlong Mou, Nicolas Flammarion, Martin J Wainwright, and Peter L Bartlett, Im-
proved bounds for discretization of Langevin diffusions: Near-optimal rates without
convexity , Bernoulli 28(2022), no. 3, 1577–1601.
[MHFH+23]Alireza Mousavi-Hosseini, Tyler K. Farghly, Ye He, Krishna Balasubramanian, and
Murat A. Erdogdu, Towards a Complete Analysis of Langevin Monte Carlo: Beyond
Poincaré Inequality , Proceedings of Thirty Sixth Conference on Learning Theory, vol.
195, 2023, pp. 1–35.
[Nol20] John P Nolan, Univariate stable distributions , Springer, 2020.
[N¸ SR19] Than Huy Nguyen, Umut ¸ Sim¸ sekli, and Gaël Richard, Non-asymptotic analysis of Frac-
tional Langevin Monte Carlo for non-convex optimization , International Conference
on Machine Learning, 2019, pp. 4810–4819.
[PBEM23] Mathieu Le Provost, Ricardo Baptista, Jeff D Eldredge, and Youssef Marzouk, An
adaptive ensemble filter for heavy-tailed distributions: Tuning-free inflation and local-
ization , arXiv preprint arXiv:2310.08741 (2023).
[QM16] Di Qi and Andrew J Majda, Predicting fat-tailed intermittent probability distributions
in passive scalar turbulence with imperfect models through empirical information
theory , Communications in Mathematical Sciences 14(2016), no. 6, 1687–1722.
[RRT17] Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky, Non-convex learning via
stochastic gradient Langevin dynamics: A nonasymptotic analysis , Conference on
Learning Theory, PMLR, 2017, pp. 1674–1703.
[SL19] Ruoqi Shen and Yin Tat Lee, The randomized midpoint method for log-concave
sampling , Advances in Neural Information Processing Systems 32(2019).
[SP15] Prashant D Sardeshmukh and Cécile Penland, Understanding the distinctively skewed
and heavy tailed character of atmospheric and oceanic probability distributions , Chaos:
An Interdisciplinary Journal of Nonlinear Science 25(2015), no. 3.
[¸ SZTG20] Umut ¸ Sim¸ sekli, Lingjiong Zhu, Yee Whye Teh, and Mert Gurbuzbalaban, Fractional
underdamped Langevin dynamics: Retargeting SGD with momentum under heavy-
tailed gradient noise , International Conference on Machine Learning, 2020, pp. 8970–
8980.
[Wib18] Andre Wibisono, Sampling as optimization in the space of measures: The Langevin
dynamics as a composite optimization problem , Conference on Learning Theory,
PMLR, 2018, pp. 2093–3027.
[WSC22a] Keru Wu, Scott Schmidler, and Yuansi Chen, Minimax mixing time of the Metropolis-
adjusted Langevin algorithm for log-concave sampling , The Journal of Machine
Learning Research 23(2022), no. 1, 12348–12410.
[WSC22b] ,Minimax Mixing Time of the Metropolis-Adjusted Langevin Algorithm for
Log-Concave Sampling , Journal of Machine Learning Research 23(2022), no. 270,
1–63.
[WW15] Feng-Yu Wang and Jian Wang, Functional inequalities for stable-like Dirichlet forms ,
Journal of Theoretical Probability 28(2015), no. 2, 423–448.
13[YŁR22] Jun Yang, Krzysztof Łatuszy ´nski, and Gareth Roberts, Stereographic Markov Chain
Monte Carlo , arXiv preprint arXiv:2205.12112 (2022).
[ZZ23] Xiaolong Zhang and Xicheng Zhang, Ergodicity of supercritical SDEs driven by
α-stable processes and heavy-tailed sampling , Bernoulli 29(2023), no. 3, 1933–1958.
14A Lower Bound Proofs for the Langevin Diffusion and the Gaussian
Proximal Sampler
While research on upper bounds of sampling algorithms’ complexity has advanced considerably, the
exploration of lower bounds is still nascent. [ CGL+22] explored the query complexity of sampling
from strongly log-concave distributions in one-dimensional settings. [ LZT22 ] established lower
bounds for LMC in sampling from strongly log-concave distributions. [ CBL22 ] presented lower
bounds for sampling from strongly log-concave distributions with noisy gradients. [ GLL20 ] focused
on lower bounds for estimating normalizing constants of log-concave densities. Contributions by
[LST21a ] and [ WSC22b ] provide lower bounds in the metropolized algorithm category, including
Langevin and Hamiltonian Monte Carlo, in strongly log-concave contexts. Finally, [ CGLL22 ]
contributed to lower bounds in Fisher information for non-log-concave sampling. In what follows,
we take a different approach and rely on the arguments developed in [Hai10].
We begin by stating the following result which drives our lower bound strategy.
Lemma 1 ([Hai10 , Theorem 5.1]) .Suppose µandνare probability measures on Rd. Consider some
G:Rd→R+andf:R+→R+satisfying µ(G≥y)≥f(y)for all y∈R+. Then,
TV(µ, ν)≥sup
y∈R+f(y)−R
Gdν
y.
In particular, suppose Id·f:R+∋y7→yf(y)∈R+is a bijection, then
TV(µ, ν)≥1
2f
(Id·f)−1 
2m
,
for any m≥R
Gdν.
Proof. By the definition of total variation and Markov’s inequality, for any y >0
TV(µ, ν)≥µ(G≥y)−ν(G≥y)≥f(y)−R
Gdν
y.
When Id·fis invertible, choosing y= (Id·f)−1(2m)implies yf(y) = 2 mand yields the desired
result.
To apply Lemma 1 when the target density satisfies Assumption 1, we need to establish tail lower
bounds for this density, which we do so via the following lemma. In the following, let ωd:=
πd/2
Γ((d+2)/2)denote the volume of the unit d-ball.
Lemma 2. Suppose πX(x)∝exp(−V(x))satisfies Assumption 1. Then, for all R >0,
πX(|x| ≥R)≥2de−ν1/d
(d+ν1)Γ(ν1/2)(d/2)ν1/2(1 +R−2)−(d+ν2)/2R−ν2.
When focusing on dependence on Randd, we obtain,
πX(|x| ≥R)≥Cν1dν1/2(1 +R−2)−(d+ν2)/2R−ν2,
where Cν1=21−ν1/2e−ν1
(1+ν1)Γ(ν1/2).
Proof. Without loss of generality assume V(0) = 0 . Via Assumption 1, we have the estimates for V,
V(x) =Z1
t=0⟨x,∇V(tx)⟩dt≤(d+ν)Z1
t=0t|x|2dt
1 +|tx|2=d+ν2
2ln(1 + |x|2),
and similarly
V(x)≥d+ν1
2ln(1 + |x|2).
15Consequently, using the spherical coordinates,
πX(|x| ≥R)≥1
ZZ
|x|≥R(1 +|x|2)−(d+ν2)/2dx
=dωd
ZZ
r≥R(1 +r2)−(d+ν2)/2rd−1dr
≥dωd(1 +R−2)−(d+ν2)/2
ZZ
r≥Rr−ν2−1dr
=dωd(1 +R−2)−(d+ν2)/2
ZdR−ν2.
Next, using the lower bound established on Vand spherical coordinates, we obtain,
Z≤Z
Rd(1 +|x|2)−(d+ν1)/2dx
=dωdZ∞
0(1 +r2)−(d+ν1)/2rd−1dr
=1
2dωdZ∞
0uν1/2−1(1−u)d/2−1du
=1
2dωdB(ν1/2, d/2)
=dωdΓ(ν1/2)Γ(d/2)
2Γ((d+ν1)/2),
where Bdenotes the beta function. Plugging back into our tail lower bound, we obtain,
πX(|x| ≥R)≥2Γ((d+ν1)/2)
Γ(ν1/2)Γ(d/2)(1 +R−2)−(d+ν2)/2R−ν2.
Moreover, by [MHFH+23, Lemma 32] we have
Γ((d+ν1)/2)
Γ(d/2)=d
d+ν1Γ((d+ν1+ 2)/2)
Γ((d+ 2)/2)≥2de−ν1/d
d+ν1(d/2)ν1/2,
which completes the proof.
Another element of Lemma 1 is controlling the growth of E[G(Xt)]throughout the process. The
following lemma achieves such control under the Langevin diffusion.
Lemma 3. Suppose (Xt)t≥0is the solution to the Langevin diffusion starting at X0with the
corresponding potential V(x)satisfying Assumption 1. Let G(x) = exp( κ˜V(x))where ˜V(x) =
d+ν2
2ln(1 + |x|2)andκ≥2
d+ν2∨1. Then,
E[G(Xt)]≤
E[G(X0)]2
κ(d+ν2)+ 4κ(d+ν2)tκ(d+ν2)
2.
Proof. Recall the generator of the Langevin diffusion L(·) = ∆ · −⟨∇ V ,∇·⟩. Then,
dE[G(Xt)]
dt=E[LG(Xt)]
=κEh
κ|∇˜V|2+ ∆˜V− ⟨∇ ˜V ,∇V⟩
Gi
≤κEh
κ|∇˜V|2+ ∆˜V
Gi
(Assumption 1)
≤2κ2(d+ν2)2EG(Xt)
1 +|Xt|2
= 2κ2(d+ν2)2E
G(Xt)1−2
κ(d+ν2)
≤2κ2(d+ν2)2E[G(Xt)]1−2
κ(d+ν2) (Jensen′s Inequality) .
Integrating the above inequality completes the proof.
16With the above lemmas in hand, we are ready to present the proof of Theorem 1.
Proof of Theorem 1. To apply Lemma 1 we choose G(x) = exp( κ˜V(x))where ˜V(x) =d+ν2
2ln(1+
|x|2)withκ≥1∨2
d+ν2. By Lemma 2 we have
πX(G(x)≥y)≥πX
|x| ≥y1
κ(d+ν2)
≥Cν1dν1/2
1 +y−2
κ(d+ν2)−(d+ν2)/2
y−ν2
κ(d+ν2).
Moreover, define
g(t):=
g(0)2
κ(d+ν2)+ 4κ(d+ν2)tκ(d+ν2)
2,
withg(0):=E[G(X0)]. Then by Lemma 3 we have E[G(Xt)]≤g(t)and we can invoke Lemma 1
to obtain
TV(πX, µt)≥sup
y∈R+Cν1dν1/2
1 +y−2
κ(d+ν2)−(d+ν2)/2
y−ν2
κ(d+ν2)−g(t)
y.
≥sup
y∈R+Cν1dν1/2exp 
−(d+ν2)y−2
κ(d+ν2)
2!
y−ν2
κ(d+ν2)−g(t∨1)
y,
where we used the fact that 1 +x≤exfor all x∈Randg(t)is non-decreasing in t. Choose
y∗:=C′
ν1,ν2g(t∨1)
dν1/2κ(d+ν2)
κ(d+ν2)−ν2
,
for a sufficiently large constant C′
ν1,ν2≥1. For simplicity, let
˜g(t):=g(t∨1)2
κ(d+ν2)
4κ(d+ν2),
and notice that
y∗=C′
ν1,ν2dκ(d+ν2)
2·κ(d+ν2)−ν1
κ(d+ν2)−ν2(4κ(1 +ν2/d)˜g(t))κ2(d+ν2)2
2(κ(d+ν2)−ν2). (3)
Using the fact that
y∗≥(4κ)κ2(d+ν2)2
2(κ(d+ν2)−ν2)dκ(d+ν2)
2·κ(d+ν2)−ν1
κ(d+ν2)−ν2,
we have
TV(πX, µt)≥Cν1exp
−1 +ν2/d
8κ·dν1−ν2
κ(d+ν2)−ν2
dν1/2y∗−ν2
κ(d+ν2)−g(t∨1)
y∗
≥˜Cν1,ν2dν1/2y∗−ν2
κ(d+ν2)−g(t∨1)
y∗,
where ˜Cν1,ν2=Cν1e−1+ν2/d
8. By plugging in the value of y∗from (3), we obtain,
TV(πX, µt)
≥
˜Cν1,ν2C′
ν1,ν2−ν2
κ(d+ν2)−C′
ν1,ν2−1n
dν1−ν2
2(2κ(1 +ν2/d)˜g(t))−ν2
2o1+ν2
κ(d+ν2)−ν2.
Thus for sufficiently large C′
ν1,ν2, there exists C′′
ν1,ν2such that
TV(πX, µt)≥C′′
ν1,ν2n
dν1−ν2
2(4κ(1 +ν/d)˜g(t)))−ν2
2o1+ν2
κ(d+ν2)−ν2.
Choosing κaccording to the statement of the theorem completes the proof.
In order to prove a similar theorem for the Gaussian proximal sampler, we control the growth of
E[G(xk)]for the iterates of the proximal sampler via the following lemmas.
17Lemma 4. Suppose (xk, yk)kare the iterates of the Gaussian proximal sampler with step size ηand
target density πX∝exp(−V)for some V:Rd→R. LetG(x) = exp( κV(x))withκ≥1. Then,
for every k≥0,
E[G(xk+1)]≤E[G(xk+p
2ηz)],
where z∼ N(0, Id)is sampled independently from xk.
Proof. Recall that πX|Y(x|y)∝exp 
−V(x)−|x−y|2
2η
. Therefore,
E[G(xk+1)|yk] =CykZexp 
(κ−1)V(x)−|x−yk|2
2η
(2πη)d/2dx
=CykE[G(yk+√ηz1)1−1/κ|yk],
where z1∼ N(0, Id). Furthermore,
Cyk=1
(2πη)d/2Z
exp
−V(x)−|x−yk|2
2η
dx
=E[G(yk+√ηz1)−1/κ|yk].
Therefore,
E[G(xk+1)|yk]
=E[G(yk+√ηz1)1−1/κ|yk]
E[G(yk+√ηz1)−1/κ|yk]
≤E[G(yk+√ηz1)|yk]1−1/κE[G(yk+√ηz1)|yk]1/κ(Jensen’s Inequality)
=E[G(yk+√ηz1)|yk].
Recall yk=xk+√ηz2where z2∼ N(0, Id)is independent from xk. By the towering property of
conditional expectation,
E[G(xk+1)]≤E[G(xk+√ηz1+√ηz2)]
=E[G(xk+p
2ηz)],
where z∼ N(0, Id)is independent from xk, which completes the proof.
In order to provide a more refined control over E[G(xk)], we need additional assumptions on V. In
particular, when considering the generalized Cauchy density, we arrive at the following lemma.
Lemma 5. Suppose (xk, yk)kare the iterates of the Gaussian proximal sampler with step size ηand
target density πX∝exp(−V)satisfies
|∇V(x)| ≤(d+ν2)|x|
1 +|x|2and ∆V(x)≤(d+ν2)2
1 +|x|2,
for all x∈Rd. LetG(x) = exp( κV(x))withκ≥1∨2
d+ν2. Then, for every k≥0,
E[G(xk+1)]2
κ(d+ν2)≤E[G(xk)]2
κ(d+ν2)+ 4κηk(d+ν2).
Proof. From Lemma 4, we have
E[G(xk+1)]≤E[G(xk+p
2ηz)],
where z∼ N(0, Id)is independent from xk. Consider the Brownian motion starting at xk, denoted
byZt=Bt+xkwhere (Bt)is a standard Brownian motion in Rd. Notice that the generator for the
18process dZt= dBtisL=1
2∆. Therefore,
dE[G(Zt)]
dt=E[LG(Zt)]
=κ
2E
G(Zt) 
κ|∇V|2+ ∆V
≤κ(κ+ 1)
2Eh
G(Zt)(d+ν2)2
1 +|Zt|2i
≤2κ2(d+ν2)2E
G(Zt)1−2
κ(d+ν2)
≤2κ2(d+ν2)2E[G(Zt)]1−2
κ(d+ν2) (Jensen’s Inequality) .
Integrating the above inequality yields
E[G(Zt)]2
κ(d+ν2)≤E[G(Z0)]2
κ(d+ν2)+ 2κ(d+ν2)t.
The proof is complete by noticing that Z0=xkandZt=xk+√2ηzfort= 2η.
Proof of Theorem 2. Notice that the statements of Lemmas 3 and 5 are virtually the same by changing
tto2kη. Using this fact, the rest of the proof follows exactly the same as the proof of Theorem 1.
B Proofs for the Stable Proximal Sampler
B.1 Preliminaries
In this section, we introduce additional preliminaries on the isotropic α-stable process, the fractional
Poincaré-type inequalities, the fractional Laplacian and the fractional heat flow.
The Lévy process is a stochastic process that is stochastically continuous with independent and
stationary increments. Due to the stochastic continuity, the Lévy processes have càdlàg trajectories,
which allows jumps in the paths. A Lévy process Ytis uniquely determined by a triple (b, A, ν )
through the following Lévy-Khinchine formula: for all t≥0andξ∈Rd,
E
ei⟨ξ,Yt⟩
= exp
t 
i⟨b, ξ⟩ −ξ⊺Aξ+Z
Rd\{0}(ei⟨ξ,y⟩−1−i⟨ξ, y⟩1{|y|≤1}(y))ν(dy)
,(4)
where b∈Rdis a drift vector. A∈Rd×dis the covariance matrix of the Brownian motion in the
Lévy-Itô decomposition[ App09 , Thereom 2.4.16] and νis the Lévy measure related to the jump parts
in the Lévy-Itô decomposition.
The rotationally invariant(isotropic) stable process is a special case for the Le ´vy process when b= 0,
A= 0andνis the measure given by
ν(dy) =cd,α|y|−(d+α), c d,α= 2αΓ((d+α)/2)/(πd/2|Γ(−α/2)|). (5)
Based on the Lévy-Khinchine formula (4), if we initialize the process at x∈Rd, its characteristic
function is given by
Exei⟨ξ,X(α)
t−x⟩=e−t|ξ|α, x, ξ ∈Rd, t≥0. (6)
The index of stability α∈(0,2]determines the tail-heaviness of the densities: the smaller is α, the
heavier is the tail. The parameter tin(6)measures the spread of Xtaround the center. When α= 2,
the stable process pertains to the Brownian motion running with a time clock twice as fast as the
standard one and hence it has continuous paths. When α∈(0,2), the stable process paths contain
discontinuities, which are often referred as jumps. At each fixed time, unlike the Brownian motion,
theα-stable process density only has a finite pth-moment for p < α , i.e.
E[|X(α)
1|p] =(
+∞ p∈[α,+∞), α∈(0,2),
m(α)
p<+∞ p∈(0, α), α∈(0,2).
19When d= 1, the fractional absolute moment formula for m(α)
pcan be derived explicitly, see [ Nol20 ,
Chapter 3.7]. When d >1, the explicit formula for m(α)
pis only known in some special cases. For
example, when α= 1,m(1)
p=Γ((d+p)/2)Γ((1 −p)/2)
Γ(d/2)Γ(1 /2)for all p <1. Another good property of α-stable
process is the self-similarity. By examining the characteristic functions, it is easy to verify that the
isotropic α-stable process is self-similar with the Hurst index 1/α, i.e.X(α)
atanda1/αX(α)
thave the
same distribution. Or equivalently, p(α)
t(x) =t−d
αp(α)
1(t−1
αx)for all x∈Rdandt >0.
The fractional Laplacian operator in Rdof order αis denoted by −(−∆)α/2forα∈(0,2]. It
was introduced as a non-local generalization of the Laplacian operator to model various physical
phenomenons. In [ Kwa17 ], ten equivalent definitions of the fractional Laplacian operator are
introduced. Here we recall two of them:
(a) Distributional definition: For all Schwartz functions ϕdefined on Rd, we haveZ
Rd−(−∆)α/2f(y)ϕ(y)dy=Z
Rdf(x)
−(−∆)α/2ϕ(x)
dx.
(b) Singular integral definition: For a limit in the space Lp(Rd),p∈[1,∞), we have
−(−∆)α/2f(x) = lim
r→0+2αΓ(d+α
2)
πd/2|Γ(−α
2)|Z
Rd\Brf(x+z)−f(x)
|z|d+αdz.
where Bris the unit ball with radius rcentered at the origin.
The fractional Laplacian can be understood as the infinitesimal generator of the stable Le ´vy process.
More explicitly, the semigroup defined by the transition probability p(α)
tin(2)has the infinitesimal
generator −(−∆)α/2, i.e. the density function p(α)
tsatisfies the following equation in the sense of
distribution, [BHJ08]:
∂tp(α)
t(x) =−(−∆)α/2p(α)
t(x). (7)
(7)is usually referred as the α-fractional heat flow. When α= 2,−(−∆)α/2is the Laplacian
operator and (7) becomes the heat flow.
Proposition 2 (From FPI to PI) .When ϑ→2−, the ϑ-FPI reduces to the classical Poincaré
inequality with Dirichlet form Eµ(ϕ) =R
|∇ϕ(x)|2dxfor any smooth bounded ϕ:Rd→Rd.
Proof. It suffices to prove that E(ϑ)
µ(ϕ)converges to Eµ(ϕ)asϑ→2−for any smooth function ϕ.
Recall the definition of E(ϑ)
µ(ϕ):
E(ϑ)
µ(ϕ) :=cd,ϑZZ
{x̸=y}(ϕ(x)−ϕ(y))2
|x−y|(d+ϑ)dxµ(y)dywith cd,ϑ=2ϑΓ((d+ϑ)/2)
πd/2|Γ(−ϑ/2)|,
where cd,ϑ=O(2−ϑ)asϑ→2−. Now we rewrite the inside integral in E(ϑ)
µ(ϕ)and split the
integral region into a centered unit ball, denoted as B1, and its complement:
Z
x̸=y(ϕ(x)−ϕ(y))2
|x−y|(d+ϑ)dx=Z
z̸=0(ϕ(y+z)−ϕ(y))2
|z|(d+ϑ)dz
=Z
B1(ϕ(y+z)−ϕ(y))2
|z|(d+ϑ)dz
| {z }
I1+Z
Rd\B1(ϕ(y+z)−ϕ(y))2
|z|(d+ϑ)dz
| {z }
I2.
ForI2, we have
I2≤4∥ϕ∥2
∞Z
Rd\B11
|z|d+ϑdz=4∥ϕ∥2
∞dπd
2
Γ(d
2+ 1)Z∞
1r−ϑ+1dr=4∥ϕ∥2
∞dπd
2
ϑΓ(d
2+ 1).
As a result, the term in E(ϑ)
µ(ϕ)that is induced by I2satisfies
cd,ϑZ
RdI2µ(y)dy≤cd,ϑ4∥ϕ∥2
∞dπd
2
ϑΓ(d
2+ 1)→0asϑ→2−.
20ForI1, we have when ϑ >1,
I1−Z
B1|⟨∇ϕ(y), z⟩|2
|z|d+ϑdz
=Z
B1 
ϕ(y+z)−ϕ(y)− ⟨∇ ϕ(y), z⟩ 
ϕ(y+z)−ϕ(y) +⟨∇ϕ(y), z⟩
|z|d+ϑdz
≤ ∥ϕ∥C2(Rd)∥ϕ∥C1(Rd)Z
B1|z|−(d+ϑ−3)dz
=∥ϕ∥C2(Rd)∥ϕ∥C1(Rd)dπd
2
Γ(d
2+ 1)Z1
0rϑ−2dr
=∥ϕ∥C2(Rd)∥ϕ∥C1(Rd)dπd
2
(ϑ−1)Γ(d
2+ 1),
where ∥ϕ∥Ci(Rd):= supx∈Rd|ϕ(i)(x)|fori= 1,2. As a result, the term in E(ϑ)
µ(ϕ)that is induced
byI1satisfies
cd,ϑZ
Rd 
I2−Z
B1|⟨∇ϕ(y), z⟩|2
|z|d+ϑdz
µ(y)dy≤cd,ϑ∥ϕ∥C2(Rd)∥ϕ∥C1(Rd)dπd
2
(ϑ−1)Γ(d
2+ 1)→0asϑ→2−.
Therefore we have E(ϑ)
µ(ϕ)→cd,ϑR
RdR
B1|⟨∇ϕ(y),z⟩|2
|z|d+ϑµ(y)dzdyasϑ→2−. Last, we prove the
limit is equivalent to 2Eµ(ϕ). For i̸=j, we have
Z
B1∂iϕ(y)∂jϕ(y)zizjdz=−Z
B1∂iϕ(y)∂jϕ(y)˜zi˜zjd˜z,
where ˜zk=zkfor all k̸=jand˜zj=−zj. Therefore,R
B1∂iϕ(y)∂jϕ(y)zizjdz= 0. As a result,
Z
B1|⟨∇ϕ(y), z⟩|2
|z|d+ϑdz=Z
B1Pd
i=1(∂iϕ(y))2z2
i
|z|d+ϑdz
=dX
i=1(∂iϕ(y))21
dZ
B1|z|2
|z|d+ϑdz
=|∇ϕ(y)|2 πd
2
(2−ϑ)Γ(d
2+ 1),
and the proof follows from cd,ϑπd
2
(2−ϑ)Γ(d
2+1)→2asϑ→2−.
B.2 χ2convergence under FPI
In this section, we study the decaying property of χ2-divergence from ρX
ktoπX, where ρX
kis the
law of xk. In the following analysis, we denote ρk=ρX,Y
kas the law of (xk, yk),ρY
kthe law of yk.
We will analyze the two steps in the stable proximal sampler separately.
Step 1. In the following proposition, we study the decay of χ2-divergence in step 1.
Proposition 3. Assume that πXsatisfies the α-FPI with parameter CFPI(α), then for each k≥0,
χ2(ρY
k|πY)≤exp
−η 
CFPI(α)+η−1
χ2(ρX
k|πX).
Proof of Proposition 3. For the simplicity of notations, we will write p(α)andp(α)
taspandpt
respectively in this proof. Since xk∼ρX
kandyk|xk∼p(η;x,·), we have
ρY
k(y) =Z
Rdp(η;x, y)ρX
k(x)dx=Z
RdρX
k(x)pη(y−x)dx=ρX
k∗pη(y).
21Therefore, we can view ρY
kasρX
kevolving along the following factional heat flow
∂t˜ρt=−(−∆)α
2˜ρt.
That is if ˜ρ0=ρX
k, then ˜ρη=ρY
k. Similarly, since πY=πX∗pη, if˜ρ0=πX, then ˜ρη=πY. For
anyt∈[0, η], define πX
t=πX∗ptandρX
t=ρX
k∗pt. The derivative of ϕ-divergence from ρX
tto
πX
tcan be calculated as
d
dtZ
Rdϕ(ρX
t
πX
t)πX
tdx
=Z
Rd∂tπX
tϕ(ρX
t
πX
t) +ϕ′(ρX
t
πX
t)
∂tρX
t−∂tπX
tρX
t
πX
t
dx
=−Z
Rdϕ(ρX
t
πX
t)(−∆)α
2πX
tdx+Z
Rdϕ′(ρX
t
πX
t)ρX
t
πX
t(−∆)α
2πX
t−(−∆)α
2ρX
t
dx
=Z
Rd
−ρX
t
πX
t(−∆)α
2ϕ′(ρX
t
πX
t) + (−∆)α
2ρX
t
πX
tϕ′(ρX
t
πX
t)
−(−∆)α
2ϕ(ρX
t
πX
t)
πX
tdx,
where in the second identity we used the distributional definition of the fractional Laplacian. Next according to
the singular integral definition of fractional Laplacian, we have
−(−∆)α
2f(x) :=cd,αlim
r→0+Z
Rd\Brf(x+z)−f(x)
|z|d+αdz, (8)
where Br={x∈Rd:|x| ≤r}andcd,αis given in (5). With (8), we have
d
dtZ
Rdϕ(ρX
t
πX
t)πX
tdx
=cd,αlim
r→0+Z
RdZ
Rd\Brϕ(ρX
t(x+z)
πX
t(x+z))−ϕ(ρX
t(x)
πX
t(x))−ρX
t(x+z)
πX
t(x+z)ϕ′(ρX
t(x+z)
πX
t(x+z)) +ρX
t(x)
πX
t(x)ϕ′(ρX
t(x+z)
πX
t(x+z))
|z|d+αdzπX
t(x)dx.
When ϕ(r) = (r−1)2,R
Rdϕ(ρX
t
πX
t)πX
tdx=χ2(ρX
t|πX
t)and we have
d
dtχ2(ρX
t|πX
t) =−cd,αlim
r→0+Z
RdZ
Rd\Br
ρX
t(x+z)
πX
t(x+z)−ρX
t(x)
πX
t(x)2
|z|d+αdzπX
tdx:=−EπX
t(ρX
t
πX
t).
According to [ Cha04 , Theorem 23], ptsatisfies α-FPI with parameter tfor all t∈(0, η]. Since πXalso satisfies
theα-FPI with parameter CFPI(α), Lemma 6 implies that πX
t=πX∗ptsatisfies the α-FPI with parameter
CFPI(α)+ηfor all t∈(0, η]. Therefore we have
d
dtχ2(ρX
t|πX
t) =−EπX
t(ρX
t
πX
t)≤ − 
CFPI(α)+η−1χ2(ρX
t|πX
t).
Last, according to Gronwall’s inequality we have
χ2(ρY
k|πY) =χ2(ρX
η|πX
η)≤exp
−η 
CFPI(α)+η−1
χ2(ρX
k|πX).
Step 2. In this step, we study the decay of χ2-divergence in step 2. building on the work by
[CCSW22 ]. According to the R αSO, we have ρX
k+1(x) =R
RdπX|Y(x|y)ρY
k(y)dy. Also notice that
πX(x) =R
RdπX|Y(x|y)πY(y)dy. According to the data processing inequalities, χ2divergence
won’t increase after step 2, i.e. χ2(ρX
k+1|πX)≤χ2(ρY
k|πY).
Combining our results in Step 1 andStep 2 , we prove Theorem 3.
Lemma 6. Letµ1, µ2be two probability densities satisfying the ϑ-FPI with parameters C1, C2
respectively. Then µ1∗µ2satisfies the ϑ-FPI with parameter C1+C2.
Proof of Lemma 6. LetX, Y be two independent random variables such that X∼µ1andY∼µ2.
Then X+Y∼µ1∗µ2. According to variance decomposition, we have for any function ϕ,
Varµ1∗µ2(ϕ) =Var(ϕ(X+Y)) =E[Var(ϕ(X+Y)|Y)] + Var(E[ϕ(X+Y)|Y]).
22Since X∼µ1andµ1satisfies the ϑ-FPI with parameter C1, we have
Var(ϕ(X+Y)|Y)≤C1cd,αZZ
{z̸=0}(ϕ(x+Y+z)−ϕ(x+Y))2
|z|(d+ϑ)dzµ1(x)dx,
therefore we have
E[Var(ϕ(X+Y)|Y)]
≤C1cd,αZZZ
{z̸=0}(ϕ(x+y+z)−ϕ(x+y))2
|z|(d+ϑ)dzµ1(x)dxµ2(y)dy.(9)
Since Y∼µ2andµ2satisfies the ϑ-FPI with parameter C2, we have
Var(E[ϕ(X+Y)|Y])
≤C2cd,αZZ
{z̸=0} R
ϕ(x+y+z)µ1(x)dx−R
ϕ(x+y)µ1(x)dx2
|z|(d+ϑ)dzµ2(y)dy
≤C2cd,αZZ
{z̸=0}Z(ϕ(x+y+z)−ϕ(x+y))2
|z|(d+ϑ)µ1(x)dxdzµ2(y)dy (10)
where the last inequality follows from Jensen’s inequality. Combining (9) and (10), we have
Varµ1∗µ2(ϕ)≤C1cd,αZZZ
{z̸=0}(ϕ(x+y+z)−ϕ(x+y))2
|z|(d+ϑ)dzµ1(x)dxµ2(y)dy
+C2cd,αZZ
{z̸=0}Z(ϕ(x+y+z)−ϕ(x+y))2
|z|(d+ϑ)µ1(x)dxdzµ2(y)dy
≤(C1+C2)cd,αZZZ
{z̸=0}(ϕ(x+y+z)−ϕ(x+y))2
|z|(d+ϑ)dzµ1(x)dxµ2(y)dy
= (C1+C2)cd,αZZ
{z̸=0}(ϕ(u+z)−ϕ(u))2
|z|(d+ϑ)dzµ1∗µ2(u)du
= (C1+C2)Eµ1∗µ2(ϕ),
where the second inequality follows from Fatou’s lemma.
B.3 Implementation of the Stable Proximal Sampler
In this section we discuss the implementation of the R αSO step in our stable proximal sampler. We
introduce an exact implementation of the R αSO step without optimizing the target potential and the
proofs for Corollary 3 and Proposition 1.
Rejection sampling without optimization . Suppose a uniform lower bound of the target potential
is known, i.e. there is a constant CLowsuch that infx∈RdV(x)≥CLow>−∞, RαSO at each step
can be implemented exactly via a rejection sampler with proposals ˜xk+1following p(α)
η(· −yk)and
the acceptance probability exp(−V(˜xk+1) +CLow). Then the expected number of rejections, N,
satisfies
N= Z
Rde−V(x)+CLowp(η;x, yk)dx−1and logN=−CLow−log Z
Rde−V(x)p(α)(η;x, yk)dx
.
Without loss of generality, we assume x∗= 0, which always hold if we translate the potential Vby
V(0). Then we have
logN≤ −CLow+Z
Rd 
V(x)−V(0)
p(α)(η;x, yk)dx
≤ −CLow+LZ
Rdx+ykβp(α)
η(x)dx
≤ −CLow+LEX∼πX[|X|β] +Lηβdβ
2+LEX∼πX[|X|2β]1
2χ2(ρX
0|πX)1
2+Γ(d+1
2)Γ(1−β
2)L
Γ(d+1−β
2)π1
2ηβ,
23where the second inequality follows from Assumption 3 and the last inequality follows from
the proof of Corollary 3. With the above estimation, we can pick η= Θ( C1
β
Lowd−1
2L−1
β)
and the expected number of rejections satisfies logN =O(CLow+LM)with
M=EπX[|X|β] +χ2(ρX
0|πX)EπX[|X|2β]1
2.
Proof of Corollary 3. The expected number of iterations conditioned on ykin the rejection sampling
is
N=Z
Rde−V(x)+V(x∗)p(α)(η;x, yk)dx−1
and logN=−V(x∗)−log Z
Rde−V(x)p(α)(η;x, yk)dx
≤Z
Rd 
V(x)−V(x∗)
p(α)(η;x, yk)dx
=Z
Rd 
V(x+yk)−V(x∗)
p(α)
η(x)dx.
WLOG, assume x∗= 0. Since Vsatisfies Assumption 3, we have
logN≤LZ
Rdx+ykβp(α)
η(x)dx=LΓ(d+1
2)
πd+1
2ηZ
Rdx+ykβ(x2+η2)−d+1
2dx
≤L|yk|β+LΓ(d+1
2)
πd+1
2ηZ
Rdxβ(x2+η2)−d+1
2dx
≤L|yk|β+LΓ(d+1
2)
πd+1
2ηZ
Rd(x2+η2)−d+1−β
2dx
=L|yk|β+Γ(d+1
2)Γ(1−β
2)L
Γ(d+1−β
2)π1
2ηβ.
Therefore, when η= Θ( d−1
2L−1
β), the expected number of rejections N is of order E[exp( L|yk|β].
Since πXsatisfies a 1-FPI with parameter CFPI(1), according to [ Cha04 ],ptsatisfies the 1-FPI with
parameter ηfor any t∈(0, η). Last it follows from Theorem 9 that for any η >0, to achieve a
ε-accuracy in χ2divergence, we need to perform the stable proximal sampler Ksteps with
K≥ 
CFPI(1)η−1+ 1
logχ2(ρX
0|πX)
ε
=O 
CFPI(1)d1
2L1
βlog χ2(ρX
0|πX)
ε
.
Proof of Proposition 1. For all k≥0, we have
TV(˜ρX
k+1, ρX
k+1) = TV Z
˜ρX|Y
k+1(·|y)˜ρY
k(y)dy,Z
ρX|Y
k+1(·|y)ρY
k(y)dy
≤TV Z
˜ρX|Y
k+1(·|y)˜ρY
k(y)dy,Z
ρX|Y
k+1(·|y)˜ρY
k(y)dy
+ TV Z
ρX|Y
k+1(·|y)˜ρY
k(y)dy,Z
ρX|Y
k+1(·|y)ρY
k(y)dy
≤E˜ρY
k[TV(˜ρX|Y
k+1(·, y)], ρX|Y
k+1(·|y)) + TV(˜ ρY
k, ρY
k)
≤εTV+ TV(˜ ρX
k, ρX
k),
where the last two inequalities follow from the data processing inequality. Therefore, TV(˜ρX
k, ρX
k)≤
kεTV+ TV(˜ ρX
0, ρX
0)for all k≥1.
24Next, the iteration complexity of Algorithm 2 with an inexact R αSO can be obtained from Proposition
1. Since ˜ρX
0=ρX
0, according to Pinsker’s inequality, we have
TV(˜ρX
k, πX)≤TV(˜ρX
k, ρX
k) + TV( ρX
k, πX)≤TV(˜ρX
k, ρX
k) +q
χ2(ρX
k|πX)/2
≤kεTV+q
exp(−kη(CFPI(α)+η)−1)χ2(˜ρX
0|πX)/2.
For any ε >0and any Ksatisfies
K≥(CFPI(α)η−1+ 1) ln 
2χ2(˜ρX
0|πX)/ε2
,
if the R αSO can be implemented inexactly with εTV≤ε
2K, the density of the Kthiterate of
Algorithm 2 is ε-close to the target in the total variation distance, i.e. TV(˜ρK
X, πX)≤ε.
B.4 Convergence under Weak Fractional Poincaré Inequality
Our main result for Algorithm 2 in Theorem 3 is proved under the assumption the target satisfying
α-FPI. Furthermore, for the rejection-sampling based implementation of the R αSO in Algorithm 3,
the parameter αis set to be 1. In order to use Theorem 3 for the case of generalized Cauchy targets,
one has to check if the α-FPI is satisfied or not, which depends on the degrees of freedom parameter
νof the generalized Cauchy desity. Specifically, when ν≥1,1-FPI is satisfied and we hence
have Corollary 5, part (i) based on Theorem 3. When ν∈(0,1),1-FPI is not satisfied and hence
Theorem 3 no longer applies.
To tackle this issue, we now introduce a generalization of Theorem 3 to the case when the target
satisfies a weak version of Fractioanl Poincaré inequality (wFPI) and provide convergence guarantees
for the stable proximal sampler in χ2-divergence.
Definition 3 (weak Fractional Poincaré Inequality) .Forϑ∈(0,2), a probability density µsatisfies
aϑ-weak fractional Poincaré inequality if there exists a decreasing function βWFPI(ϑ):R+→R+
such that for any ϕ:Rd→Rin the domain of E(ϑ)
µwithµ(ϕ) = 0 , we have
µ(ϕ2)≤βWFPI(ϑ)(r)E(ϑ)
µ(ϕ) +r∥ϕ∥2
∞,∀r >0, (wFPI)
where E(ϑ)
µis a non-local Dirichlet form associated with µdefined as
E(ϑ)
µ(ϕ) :=cd,ϑZZ
{x̸=y}(ϕ(x)−ϕ(y))2
|x−y|(d+ϑ)dxµ(y)dywith cd,ϑ=2ϑΓ((d+ϑ)/2)
πd/2|Γ(−ϑ/2)|.
The wFPI is satisfied by any probability density that is locally bounded, and is hence extremely
general. Setting the parameter r= 0, wFPI reduces to FPI with CFPI(ϑ)=βWFPI(ϑ)(0).
Theorem 5. Assume that πXsatisfies the α-wFPI with parameter βWFPI(α)(r)for some α∈(0,2).
Then for any step size η >0and initial condition ρX
0such that R∞(ρX
0|πX)<∞, thekthiterate of
the stable proximal sampler with parameter α(Algorithm 2) satisfies
χ2(ρX
k|πX)≤exp 
−(βWFPI(α)(r) +η)−1kη
χ2(ρX
0|πX)
+ 4r 
1−exp 
−(βWFPI(α)(r) +η)−1(k+ 1)η
exp 
2R∞(ρX
0|πX)
.
The proof of Theorem 5 follows the same two-step analysis as it is introduced in the beginning of
Section B.2. The convergence property corresponding to Step 1 is stated in the following Proposition.
Proposition 4. Assume that πXsatisfies the α-wFPI with parameter βWFPI(α)for some α∈(0,2),
then for each k≥0, r > 0,
χ2(ρY
k|πY)≤exp 
−(βWFPI(α)(r) +η)−1η
χ2(ρX
k|πX)
+ 4r 
1−exp 
−(βWFPI(α)(r) +η)−1η
exp 
2R∞(ρX
k|πX)
.(11)
Proof of Proposition 4. In the stable proximal sampler with parameter α, we have ρY
k=ρX
k∗p(α)
η
andπY=πX∗p(α)
η. Therefore we can view ρY
kandπYasρX
kandπXevolving along the fractional
25heat flow by time ηrespectively. For any t∈[0, η], define πX
t=πX∗p(α)
tandρX
t=ρX
k∗p(α)
t. We
have
d
dtχ2(ρX
t|πX
t) =−EπX
t(ρX
t
πX
t) =−EπX
t(ρX
t
πX
t−1).
According to [ Cha04 , Theorem 23], p(α)
tsatisfies α-FPI with parameter ηfor all t∈(0, η]. According
to Lemma 7, πX
tsatisfies the α-wFPI with βWFPI(α)(r) +η. Therefore we get
d
dtχ2(ρX
t|πX
t)≤ 
βWFPI(α)(r) +η−1χ2(ρX
t|πX
t) +r 
βWFPI(α)(r) +η−1ρX
t/πX
t−12
∞
≤ 
βWFPI(α)(r) +η−1χ2(ρX
t|πX
t) + 4r 
βWFPI(α)(r) +η−1exp 
2R∞(ρX
k|πX)
,
where the last inequality follows from the definition of Renyi-divergence and the data processing
inequality. Last, (11) follows from Gronwall’s inequality.
Proof of Theorem 5. According to Proposition 4, the χ2decaying property in step 1 of the algorithm
is as follows,
χ2(ρY
k|πY)≤exp 
−(βWFPI(α)(r) +η)−1η
χ2(ρX
k|πX)
+ 4r 
1−exp 
−(βWFPI(α)(r) +η)−1η
exp 
2R∞(ρX
k|πX)
.
In step 2, we have ρX
k+1=ρY
k∗πX|YandπX=πY∗πX|Y. Therefore according to the data
processing inequality, we get
χ2(ρX
k+1|πX)≤χ2(ρY
k|πY)
≤exp 
−(βWFPI(ϑ)(r) +η)−1η
χ2(ρX
k|πX)
+ 4r 
1−exp 
−(βWFPI(α)(r) +η)−1η
exp 
2R∞(ρX
k|πX)
≤exp 
−k(βWFPI(α)(r) +η)−1η
χ2(ρX
0|πX)
+ +4 r 
1−exp 
−(βWFPI(α)(r) +η)−1(k+ 1)η
exp 
2R∞(ρX
0|πX)
,
where the last inequality follows from the data processing inequality. Last, apply the above iterative
relation ktimes and we prove (11).
Lemma 7. Letµ1be a probability density on Rdsatisfying the ϑ-wFPI with parameter βWFPI(ϑ)(r).
Letµ2be a probability density on Rdsatisfying the ϑ-FPI with parameter CFPI(ϑ). Then µ1∗µ2
satisfies ϑ-wFPI with parameter βWFPI(ϑ)(r) +CFPI(ϑ).
Proof of Lemma 7. LetX, Y be two independent random variables such that X∼µ2andY∼µ1.
According to variance decomposition, we have for any function ϕsuch that µ1∗µ2(ϕ) = 0 ,
Varµ1∗µ2(ϕ) =Var(ϕ(X+Y)) =E[Var(ϕ(X+Y)|Y)] + Var(E[ϕ(X+Y)|Y]).
Since X∼µ2andµ2satisfies the ϑ-FPI with parameter CFPI(ϑ), we have
E[Var(ϕ(X+Y)|Y)] (12)
≤CFPI(ϑ)cd,αZZZ
{z̸=0}(ϕ(x+y+z)−ϕ(x+y))2
|z|(d+ϑ)dzµ2(x)dxµ1(y)dy. (13)
Since Y∼µ1andµ1satisfies the ϑ-wFPI with parameter βWFPI(ϑ), following the proof of Lemma 6,
we have
Var(E[ϕ(X+Y)|Y])
≤βWFPI(ϑ)cd,αZZ
{z̸=0}Z(ϕ(x+y+z)−ϕ(x+y))2
|z|(d+ϑ)µ2(x)dxdzµ1(y)dy
+rZ
ϕ(x+·)µ2(x)dx−ZZ
ϕ(x+y)µ2(x)dxµ1(y)dy2
∞
≤βWFPI(ϑ)cd,αZZ
{z̸=0}Z(ϕ(x+y+z)−ϕ(x+y))2
|z|(d+ϑ)µ2(x)dxdzµ1(y)dy+r∥ϕ∥2
∞,(14)
26where the last inequality follows from the fact that µ1∗µ2(ϕ) = 0 and the convexity ∥·∥∞. Combining (12)
and (14), we have
Varµ1∗µ2(ϕ)
≤CFPI(ϑ)cd,αZZZ
{z̸=0}(ϕ(x+y+z)−ϕ(x+y))2
|z|(d+ϑ)dzµ2(x)dxµ1(y)dy
+βWFPI(ϑ)(r)cd,αZZ
{z̸=0}Z(ϕ(x+y+z)−ϕ(x+y))2
|z|(d+ϑ)µ2(x)dxdzµ1(y)dy+r∥ϕ∥2
∞
= 
βWFPI(ϑ)(r) +CFPI(ϑ)
cd,αZZ
{z̸=0}(ϕ(u+z)−ϕ(u))2
|z|(d+ϑ)dzµ1∗µ2(u)du+r∥ϕ∥2
∞
= 
βWFPI(ϑ)(r) +CFPI(ϑ)
Eµ1∗µ2(ϕ) +r∥ϕ∥2
∞.
Lemma 7 is hence proved.
B.5 Proofs for the Generalized Cauchy Examples
In this section, we provide proofs for the two corollaries in Section 3.2.
Proof of Corollary 4. According to [ WW15 , Corollary 1.2], πνsatisfies α-FPI with parameter
CFPI(ϑ)for any α≤min(2 , ν). Therefore it follows from Theorem 3 that
χ2(ρX
k|πν)≤exp
−kη 
CFPI(α)+η−1
χ2(ρX
0|πν). (15)
According to [ MHFH+23, Corollary 22], when ρX
0=N(0, Id)andd≥2,R∞(ρX
0|πν)≤
ln(2ν/2Γ(ν/2)) + ln(d+ν
2e)which implies χ2(ρX
0|πν) = Θ( d). Therefore Corollary 4 follows
from (15) and η∈(0,1).
Proof of Corollary 5. We prove the two part in the Corollary separately:
(i) When ν≥1, according to [ WW15 , Corollary 1.2] πνsatisfies the 1-FPI with parameter CFPI(1).
Corollary 3 applies with L= 4(d+ν)andβ= 1/4and the iteration complexity of Algorithm 2 is
of order O 
CFPI(1)d1
2(d+ν)4ln(χ2(ρX
0|πν)/ε)
.
(ii) When ν∈(0,1), according to [ WW15 , Corollary 1.2], there exists a positive constant csuch
thatπνsatisfies the 1-wFPI with parameter
βWFPI(1) (r) =c(1 +r−(1−ν)/ν). (16)
Theorem 5 implies that
χ2(ρX
k|πν)≤exp 
−kη
η+c(1 +r−(1−ν)/ν)
χ2(ρX
0|πν)
+r 
1−exp 
−(k+ 1)η
η+c(1 +r−(1−ν)/ν)
exp 
2R∞(ρX
0|πX)
≤exp 
−kη
η+c(1 +r−(1−ν)/ν)
χ2(ρX
0|πν)
+(k+ 1)ηr
η+c(1 +r−(1−ν)/ν)exp 
2R∞(ρX
0|πX)
.
For any ε >0andk≥1, pick r=exp 
−2νR∞(ρX
0|πν)
cνεν
(k+1)νην , we have χ2(ρX
k|πν)≤εif
k≥
1 +c1
νη−1
ν+ 21/νcη−1ε−(1−ν)/νexp 2(1−ν)R∞(ρX
0|πν)
ν
ln1/ν(2χ2(ρX
0|πν)
ε).
Corollary 3 applies with L= (d+ν)/νandβ=ν/4. Therefore, by choosing η= Θ( d−1
2(d+ν)−4
ν), the
iteration complexity in Algorithm 2 is of order
O
max
c1
νd1
2ν+4
ν2, cd1
2+4
νε−1−ν
νexp 2(1−ν)R∞(ρX
0|πν)
ν	
ln1
ν(2χ2(ρX
0|πν)
ε)
.
27C Proofs for the Lower Bounds on the Stable Proximal Sampler
In this section we introduce the proofs for the lower bounds for the stable proximal sampler with
parameter αwhen the target is the generalized Cauchy density with degrees of freedom strictly
smaller than α. The lower bound is proved following the idea introduced in Section 2.
Lemma 8. Suppose (xk, yk)kare the iterates of the stable proximal sampler with parameter α, step
sizeηand target density πX∝exp(−V)for some V:Rd→R. Let G(x) = exp( κV(x))with
κ∈(0,1). Then, for every k≥0,
E[G(xk+1)]≤E[G(xk+ 21
αη1
αzk)],
where zk, with density p(α)
1, is sampled independently from xk.
Proof of Lemma 8. Recall that πX|Y(x|y)∝πX(x)p(α)(η;x, y). We have
E[G(xk+1)] =E
E[G(xk+1)|yk]
=E
Z−1
ykZ
G(x)πX(x)p(α)
η(x−yk)dx
=E
Z−1
ykE[G(yk+η1
αzk)πX(yk+η1
αzk)|yk]
,
where Zyk=R
πX(x)p(α)
η(x−yk)dx=E[πX(yk+η1
αzk)|yk]andzkis the α-stable random
vector with density p(α)
1, which is independent to yk, xk. Let T:R+→RbeT(r) =r−κ. Since
κ∈(0,1),Tis convex and r7→rT(r)is concave. According to the fact that G(x) =T(πX)(x)
and Jensen’s inequality, we have
E[G(xk+1)] =EE
(πXT(πX))(yk+η1
αzk)|yk
E
πX(yk+η1
αzk)|yk]
≤E
T 
E[πX(yk+η1
αzk)|yk]
.
Since Tis convex, apply Jensen’s inequality again and we get
E[G(xk+1)]≤E[G(yk+η1
αzk)] =E
E[G(xk+η1
αz′
k+η1
αzk)|xk]
=E[G(xk+ 21
αη1
α¯zk)|xk],
where z′
kis the α-stable random vector with density p(α)
1, which is independent to xk, zkand the last
identity follows from the self-similarity of α-stable process with ¯zk∼p(α)
1which is independent to
xk.
Lemma 9. Suppose (xk, yk)kare the iterates of the stable proximal sampler with parameter α, step
sizeηand target density πX∝exp(−V)satisfies
|∇V(x)| ≤(d+ν2)|x|
1 +|x|2and ∆V(x)≤(d+ν2)2
1 +|x|2,
for some ν2∈(0, α)and for all x∈Rd. LetG(x) = exp( κV(x))with
κ∈(ν2(d+ν2)−1, α(d+ν2)−1).
Then, for every k≥0and for all r >0,
E[G(xk+1)]≤(1 +r)κ(d+ν2)
2E[G(xk)] + 2κ(d+ν2)
αηκ(d+ν2)
α(1 +r−1)κ(d+ν2)
2m(α)
κ(d+ν2),(17)
where m(α)
κ(d+ν2)=E[|zk|κ(d+ν2)]withzkbeing an α-stable random vector with density p(α)
1.
Moreover, for every N≥0,
E[G(xN)]≲E[G(x0)] +m(α)
κ(d+ν2)Nκ(d+ν2)
2+1ηκ(d+ν2)
α, (18)
where≲is hiding a uniform positive constant factor.
28Proof of Lemma 9. Without loss of generality assume V(0) = 0 . Then, we have that,
V(x) =Z1
0⟨x,∇V(tx)⟩dt≤(d+ν2)Z1
0t|x|
1 +|tx|2dt=d+ν2
2ln(1 + |x|2).
Therefore G(x) = exp( κV(x))≤(1 +|x|2)κ(d+ν2)/2, Since κ∈(ν2(d+ν2)−1, α(d+ν2)−1),
G(x) =O(|x|κ(d+ν2))when|x| ≫1andE[G(xk+ 21
αη1
αzk)]in Lemma 8 is finite. We have
E[G(xk+ 21
αη1
αzk)]
≤E[(1 +|xk+ 21
αη1
αzk|2)κ(d+ν2)
2]
≤E[(1 + (1 + r)|xk|2+ 41
αη2
α(1 +r−1)|zk|2)κ(d+ν2)
2]
≤(1 +r)κ(d+ν2)
2E[G(xk)] + 2κ(d+ν2)
αηκ(d+ν2)
α(1 +r−1)κ(d+ν2)
2E[|zk|κ(d+ν2)]
≤(1 +r)κ(d+ν2)
2E[G(xk)] + 2κ(d+ν2)
αηκ(d+ν2)
α(1 +r−1)κ(d+ν2)
2m(α)
κ(d+ν2),
where the first inequality follows from the Young’s inequality and m(α)
κ(d+ν2)=E[|zk|κ(d+ν2)]with
zkbeing an α-stable random vector with density p(α)
1.(17) follows from Lemma 8. Furthermore, by
induction we have
E[G(xN)]≤(1 +r)κ(d+ν2)N/2E[G(x0)]
+(1 +r)κ(d+ν2)N/2−1
(1 +r)κ(d+ν2)/2−12κ(d+ν2)
αηκ(d+ν2)
α(1 +r−1)κ(d+ν2)
2m(α)
κ(d+ν2).
Pickr=2
κ(d+ν2)Nand (18) is proved.
Proof of Theorem 4. To apply Lemma 1, we choose G(x) = exp( κV(x))with κ∈(ν2(d+
ν2)−1, α(d+ν2)−1)⊂(0,1). Without loss of generality assume V(0) = 0 . Via Assumption 1, we
have the estimates for V,
V(x) =Z1
0⟨x,∇V(tx)⟩dt≥(d+ν1)Z1
0t|x|
1 +|tx|2dt=d+ν1
2ln(1 + |x|2).
By Lemma 2 we have
πX(G(x)≥y)≥πX
|x| ≥y1
κ(d+ν1)
≥Cν1dν1
2
1 +y−2
κ(d+ν1)−d+ν2
2y−ν2
κ(d+ν1).
We then invoke Lemma 1 and Lemma 9 to obtain
TV(ρX
N, πX)
≳sup
y≥1Cν1dν1
2
1 +y−2
κ(d+ν1)−d+ν2
2y−ν2
κ(d+ν1)−E[G(x0)] +m(α)
κ(d+ν2)Nκ(d+ν2)
2+1ηκ(d+ν2)
α
y.
The fact that κ∈(ν2(d+ν1)−1, α(d+ν2)−1)ensures that the supremum on the right side is always
positive. In particular, picking ysuch that
y1−ν2
κ(d+ν1)= Θ
C−1
ν1d−ν2
2 
E[G(x0)] +m(α)
κ(d+ν2)Nκ(d+ν2)
2+1ηκ(d+ν2)
α
,
we obtain that
TV(ρX
N, πX)
≳Cκ(d+ν1)
κ(d+ν1)−ν2ν1 dκ(d+ν1)ν2
2κ(d+ν1)−2ν2 
E[G(x0)] +m(α)
κ(d+ν2)Nκ(d+ν2)
2+1ηκ(d+ν2)
α−ν2
κ(d+ν1)−ν2,
where≳is hiding a uniform positive constant factor. Therefore, for any α∈(ν2(d+ν2)
d+ν1,2]and
δ∈(0, α−ν2(d+ν2)
d+ν1), we can choose κ=α−δ
d+ν2∈(ν2
d+ν1,α
d+ν2)and get that
TV(ρX
N, πX)
≥Cν1,ν2,δdν2(α−δ)(d+ν1)
2(α−δ)(d+ν1)−2ν2(d+ν2) 
E[G(x0)] +m(α)
α−δNα−δ
2+1ηα−δ
α−ν2(d+ν2)
(α−δ)(d+ν1)−ν2(d+ν2).
Theorem 4 then follows by taking τ=α−δ.
29Figure 1: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be one-
dimensional student-t with center 0and4degrees of freedom; initialization is chosen x0= 20 .
C.1 Further Discussions on Lower bounds of the stable proximal sampler
To derive a lower bound for the stable proximal sampler with parameter α, it is worth mentioning
that there is an extra difficulty applying our method when ν≥α. Recall that when ν∈(0, α),πν
has heavier tail than ρX
kdoes. Therefore, when we apply
TV(ρX
k, πν)≥ |πν(G≥y)−ρX
k(G≥y)|, (19)
to study the lower bound, it suffices to derive a lower bound on πν(G≥y), and an upper bound
onρX
k(G≥y)which is smaller than the lower bound on πν(G≥y). Deriving these bounds is not
too hard: the lower bound can be obtained by looking at an explicit integral against πνdirectly and
the upper bound is derived based on the fractional absolute moment accumulation of the isotropic
α-stable random variables along the stable proximal sampler.
However, when ν≥α, we expect that ρX
khas heavier tail than πν. Therefore, to apply (19), we need
to find an upper bound on πν(G≥y), and a lower bound on ρX
k(G≥y)which is smaller than the
upper bound on πν(G≥y). Notice that ρX
k(G≥y)is a quantity varying along the trajectory of the
stable proximal sampler. Deriving a lower bound along the trajectory is essentially more challenging
than deriving an upper bound.
In order to derive a satisfying lower bound in this case, it hence remains to characterize the stable
proximal sampler as an approximation of an appropriate gradient flow, just as that the Brownian-
driven proximal sampler can be interpret as the entropy-regularized JKO scheme in [ CCSW22 ];
see also Section 5. To understand this kind of gradient flow approximations itself is an interesting
future work as it may help us to understand and characterize the class of MCMC samplers that utilize
heavy-tail samples to approximate lighter-tail target densities, which is non-standard compared to
commonly used MCMC samplers such as ULA, MALA, etc.
D Numerical Illustrations
In this section, we present numerical results that illustrate the improved performance of the proximal
sampler with stable oracles ( α= 1) compared to that with Gaussian oracles. We first sample from
the one-dimensional student-t distribution with center zero and 4degrees of freedom by running
the proximal samplers with different oracles in parallel for 100 times. Each individual chain is run
for 100 iterations with step-size η= 0.1. Figures 1,2,3 present the convergence results for different
initializations x0= 20,5,−5respectively. In each figure, the first column shows the means and
variances of the iterates along the trajectories; the center column shows the histograms of the last
iterates and the target density (red curve); the last column shows the convergence of Wasserstein- 2
distance along the trajectories. We also sample from the two-dimensional student-t distribution with
center at the origin and 4degrees of freedom by running the proximal samplers with different oracles
in parallel for 30 times. Each individual chain is run for 20 iterations with step-size η= 0.1with
the initialization at x0= [5,1]. In Figure 4, we present the convergence results, the first column
showing the means and variances of the first-coordinates along the trajectories, the center column
showing the histograms of first-coordinate in the last iterates and the first-coordinate marginal density
of the target distribution (red curve), and the last column showing the convergence of Wasserstein- 2
distance along the trajectories.
30Figure 2: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be one-
dimensional student-t with center 0and4degrees of freedom; initialization is chosen x0= 5.
Figure 3: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be one-
dimensional student-t with center 0and4degrees of freedom; initialization is chosen x0=−5.
Figure 4: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be two-
dimensional student-t with center (0,0)and4degrees of freedom; initialization is chosen x0= [5,1].
31NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The main claim in the abstract and introduction is the separation result between
Gaussian and Proximal Sampler. The rest of the sections are exactly stating (and proving)
the aforementioned separation result.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Please see Section 5.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
32Answer: [Yes]
Justification: The assumptions are listed in the respective theorem. The (correct) proofs are
provided in the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: Our work is primarily theoretical.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
33Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: Our work is primarily theoretical.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: Our work is primarily theoretical.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: Our work is primarily theoretical.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
34• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: Our work is primarily theoretical.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The authors have read the Ethics Guideline and followed it in the paper
preperation.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Our work is primarily theoretical.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
35•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our work is primarily theoretical.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: Our work is primarily theoretical.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
36•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Our work is primarily theoretical.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our work is primarily theoretical.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our work is primarily theoretical.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
37