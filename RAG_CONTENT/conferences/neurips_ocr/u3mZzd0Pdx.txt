Lower Bounds of Uniform Stability in Gradient-Based
Bilevel Algorithms for Hyperparameter Optimization
Rongzhen Wang1,2, Chenyu Zheng1,2, Guoqiang Wu3,
Xu Min4, Xiaolu Zhang4, Jun Zhou4, Chongxuan Li1,2∗
1Gaoling School of Artificial Intelligence, Renmin University of China
2Beijing Key Laboratory of Big Data Management and Analysis Methods
3School of Software, Shandong University4Ant Group
{wangrz,cyzheng,chongxuanli}@ruc.edu.cn; guoqiangwu@sdu.edu.cn;
minxu.mx@antgroup.com; {yueyin.zxl,jun.zhoujun}@antfin.com
Abstract
Gradient-based bilevel programming leverages unrolling differentiation (UD) or im-
plicit function theorem (IFT) to solve hyperparameter optimization (HO) problems,
and is proven effective and scalable in practice. To understand the generalization
behavior, existing works establish upper bounds on the uniform stability of these
algorithms, while their tightness is still unclear. To this end, this paper attempts to
establish stability lower bounds for UD-based and IFT-based algorithms. A central
technical challenge arises from the dependency of each outer-level update on the
concurrent stage of inner optimization in bilevel programming. To address this
problem, we introduce lower-bounded expansion properties to characterize the
instability in update rules which can serve as general tools for lower-bound analysis.
These properties guarantee the hyperparameter divergence at the outer level and the
Lipschitz constant of inner output at the inner level in the context of HO. Guided
by these insights, we construct a quadratic example that yields tight lower bounds
for the UD-based algorithm and meaningful bounds for a representative IFT-based
algorithm. Our tight result indicates that uniform stability has reached its limit in
stability analysis for the UD-based algorithm.
1 Introduction
Hyperparameters significantly influence the convergence behavior of learning algorithms as well
as the efficiency and generalization performance of the trained model [ 1,2,3].Hyperparameter
optimization (HO) algorithms aim to find the best hyperparameters (associated with the optimized
model parameters) on a validation set. Classical approaches for HO include grid search [ 4], random
search [ 5], Bayesian optimization [ 6,7,8], and evolutionary algorithms [ 9,10], which often suffer
from the problem of scaling up. Recently, gradient-based methods have achieved excellent empirical
performance in high-dimensional HO problems [11, 3, 12].
In gradient-based methods, HO is formulated as a bilevel programming problem. The inner level
seeks the best model parameters on the training set given current hyperparameters. In the outer level,
hyperparameters are optimized with gradient descent. However, the gradient is difficult to compute as
it requires differentiating the optimized model parameters w.r.t. the hyperparameters. Two mainstream
strategies have been developed to obtain this Jacobian by explicitly unrolling differentiation (UD) [ 13,
2, 14] or approximately applying the implicit function theorem (IFT) [1, 15, 16, 12].
∗Correspondence to Chongxuan Li.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).To investigate the underlying reason for their success, existing work establishes generalization upper
bounds based on algorithmic stability [ 17,18]. In particular, [ 17] presents a generalization framework
associated with a notion of uniform stability for general bilevel programming in HO and stability
upper bounds for the UD-based algorithm. Despite their efforts, such algorithms have not been fully
understood and one of the key unsolved problems is whether existing stability analyses are tight.
To this end, this paper establishes lower bounds on the stability of gradient-based bilevel programming
algorithms for HO. Technically, we begin by introducing lower-bounded expansion properties which
inherently characterize the instability in general update rules including stochastic gradient descent
(SGD) as detailed in Section 4. Our expansion properties, to a certain degree, mirror those introduced
by [19], with the distinction being our emphasis on lower bounds rather than upper bounds. This
approach not only enables a comparative analysis with upper bounds to evaluate their alignment (i.e.,
tightness) but also lays down a conceptual framework for analyzing the lower bounds of algorithmic
stability, generally applicable to both single-level SGD and various bilevel algorithms.
Building upon these properties, we explore the stability of the UD-based algorithm in Section 5.
We first present a recursive stability lower bound that aligns with the existing upper bound at the
outer level given the expansion properties of the compound validation loss, followed by an analysis
of the Lipschitz constant of the inner output to maximize those expansion coefficients. Guided by
these theoretical insights, we construct a quadratic example that yields a tight lower bound for the
UD-based algorithm with constant step sizes and a nearly tight lower bound with linearly decreasing
step sizes with respect to key factors. Meaningful bounds for a representative IFT-based algorithm are
also provided in Appendix C based on its essential connection to UD-based methods. We highlight
that the example is carefully designed to obtain explicit stability lower bounds by overcoming the
challenges posed by the intricate behavior of the bilevel algorithms, i.e. the dependence of each
outer-level update on the current turn of inner optimization.
We outline our contributions as follows: (1) We introduce lower-bounded expansion properties that
can serve as general tools for analyzing lower bounds of the stability in gradient descent. (2) To
our knowledge, we present the first lower bounds of uniform stability for both the UD-based and
representative IFT-based algorithms, facing the challenge posed by the intricate formulation of the
outer update in bilevel optimization. (3) Our lower bounds match existing upper bounds for the
UD-based algorithm, verifying that uniform stability has reached its limit in characterizing the
generalization of the UD-based algorithm. Detailed results are summarized in Table 1.
2 Related work
Algorithmic stability [ 20,21] measures the change in the model output when a single training
example is replaced. It is shown to be sufficient and necessary for learnability in certain cases [ 22].
Stability-based generalization analysis of an algorithm typically consists of three key elements: a
notion of stability, a stability-based generalization bound, and a stability analysis depending on the
algorithm. Below, we introduce the related work based on these three elements.
Algorithmic stability. [23] introduce uniform stability, which characterizes the worst-case change of
loss and presents a stability-based generalization bound with high probability. Notable efforts [ 24,25,
26,27] have been made to obtain sharper bounds for uniformly stable algorithms in general. Besides
the uniform stability, various notions of stability that characterize the average change [ 22], local
change [28], or change in the hypothesis [29, 30] are investigated for fine-grained analyses.
Stability of stochastic gradient descent (SGD). SGD has been one of the workhorses in deep
learning and therefore attracted much attention. To unravel the mystery behind its success, [ 19]
analyze its (randomized) uniform stability on (strongly) convex and nonconvex losses. [ 31] analyze
the uniform stability of (S)GD for nonsmooth convex losses and provide sharp upper and lower
expectation bounds. [ 32,30] consider the on-average stability for SGD and build data-dependent
generalization bounds to explain the effectiveness of practical techniques like proper initialization.
Recent work establishes lower bounds on the uniform stability of SGD and investigates the tightness
of corresponding upper bounds. [ 33] proves a general minimax optimal lower bound for stability
generalization error together with optimization error on convex and smooth losses. [ 31] finds the
general technique proposed by [ 33] is sub-optimal in convex but nonsmooth cases, and provides
2sharper lower bounds by constructing a special class of loss functions. [ 34] adopts a similar approach
by construction to present lower bounds for smooth and potentially nonconvex loss functions.
In this paper, we focus on the smooth and nonconvex cases in HO and provide stability lower bounds
by construction as [ 31] and [ 34]. Compared with [ 34], we consider a more complicated and nontrivial
bilevel optimization problem, where the interaction between inner and outer processes brings a
significant impact on the analysis. A detailed comparison is provided in Section 5.3 and Appendix E.
Stability for bilevel programming. [17] extend the notion of uniform stability to HO and analyze
stability upper bounds of the UD-based algorithm, while the tightness of their result is largely
open. Recently, it has been extended to the analysis of implicit gradient algorithms [ 18]. This
paper provides the first lower bounds, generally applying to two main categories of gradient-based
HO methods. There are other bilevel optimization algorithms [ 12,35,36,37,38,39,15] and
settings [2, 16, 3, 40, 41, 42, 43] where our framework can potentially be extended in future work.
3 Problem formulation
3.1 Elementary notations and definitions
Scalar, vector, and matrix. We employ lowercase letters (e.g., a), lowercase boldface letters (e.g., a),
and uppercase boldface letters (e.g., A) to denote scalars, vectors, and matrices, respectively. For a
vector a,∥·∥denotes its Euclidean norm. For a matrix A,∥·∥denotes its spectral norm. Additionally,
leta1⊜a2denote a1anda2are collinear by a non-negative factor, namely, ∃a≥0s.t.a1=aa2.
Loss function. A differentiable function ℓ:Ω→RisL-Lipschitz continuous if ∀u,v∈Ω,
∥ℓ(u)−ℓ(v)∥ ≤L∥u−v∥. It is γ-smooth if ∀u,v∈Ω,∥∇ℓ(u)− ∇ℓ(v)∥ ≤γ∥u−v∥.
Twin datasets. A pair of datasets are considered twin datasets if they differ in only a single data point,
denoted by S≃˜S. Throughout this paper, we use a tilde symbol to distinguish their corresponding
notions, e.g., examples zand˜z, output parameters wand˜w.
Asymptotic notations. Denote with an≲bnthatanis bounded above by bnup to a constant factor
for sufficiently large n, and conversely by an≳bn. We say an≍bnifan≲bnandan≳bn.
3.2 HO as bilevel programming
Denote the testing, validation, and training distributions on the data space ZbyDtest,DvalandDtr,
and corresponding losses by ℓtest,ℓvalandℓtr. Since the validation phase is generally regarded as a
rehearsal for testing, Dvalandℓvalare commonly assumed to be consistent with Dtestandℓtest.
Given a validation set Sval={zval
i}m
i=1i.i.d.∼(Dval)mand a training set Str={ztr
j}n
j=1i.i.d.∼(Dtr)n,
HO algorithms seek the best-performing hyperparameter-parameter pair on Sval. Denote by λthe
hyperparameter in space Λ,θthe (model) parameter in space Θ. This process can be formulated as
the following bilevel problem:
ˆλ≈arg min
λ∈Λ1
mmX
i=1ℓval(λ,ˆθ(λ);zval
i),where ˆθ(λ)≈arg min
θ∈Θ1
nnX
j=1ℓtr(λ,θ;ztr
j).
Here, ˆθ(λ)is selected by its training performance under the given λandℓval(λ,ˆθ(λ);z)can be
rewritten as a compound validation loss L(λ;z)considering ˆθ(λ)a function of λ.
Various methods are proposed to solve this nested problem, among which gradient-based algorithms
have recently achieved success in scalability [ 11,3,12]. As shown in Algorithm 1, gradient-based
methods utilize SGD as the optimizer at both levels, where the primary challenge lies in the calculation
of the gradient of the compound validation loss, called hypergradient ,
∇λL(λ;z) =∇λℓval(λ,θK(λ);z) +∇λθK(λ)|{z}
inner Jacobian∇θℓval(λ,θK(λ);z), (1)
3where the inner Jacobian involves differentiating through the inner-level optimization. To this end,
the UD-based methods obtain the exact inner Jacobian by directly unrolling the inner differentiation:2
∇λθK(λ) =−K−1X
k=0ηk+1∇2
θλℓtr(λ,θk)KY
i=k+1(I−ηi+1∇2
θθℓtr(λ,θi)). (2)
While representative IFT-based methods leverage the implicit function theorem and Neumann series
to obtain an alternative estimation [12]:
\∇λθK(λ) =−ηK∇2
θλℓtr(λ,θK)K−1X
k=0h
I−ηK∇2
θθℓtr(λ,θK)ik
. (3)
Please refer to Algorithm 1 for the whole process. Notably, this paper adopts a common theoretical
assumption [19, 17] of constant inner step sizes and decreasing outer step sizes.3
Algorithm 1 Gradient-based bilevel HO
1:Input: Initialization λ0andθ0; training set Strand validation set Sval; step size scheme αandη
2:Output: The hyperparameter λTand hypothesis θK
3:fort= 1toTdo
4: fork= 1toKdo
5: uniformly sampling jkfrom [n]
6: θk←θk−1−ηk∇θℓtr(λt−1,θk−1;ztr
jk)
7: end for
8: uniformly sampling itfrom [m]
9:g← ∇L (λt−1;zval
it)▷UD-based algorithm in Eq. (2), IFT-based algorithm in Eq. (3)
10: λt←λt−1−αtg
11:end for
12:return λTandθK
3.3 Generalization and stability of HO
The generalization behavior of HO algorithms characterizes the selected model’s potential perfor-
mance on the unseen test data. Specifically, denoting the hyperparameter output by a stochastic HO
algorithm AasA(Sval, Str), we are interested in the difference between its expected testing risk and
empirical validation risk, namely generalization error defined as
ϵgen:=EA,Sval,Str
Ez∼Dtest[L(A(Sval, Str);z)]−1
mmX
i=1L(A(Sval, Str);zval
i)
. (4)
Stability-based generalization theory turns this problem into measuring the algorithmic robustness.
We define the notion of uniform argument stability for HO algorithms, which captures the variation
in algorithm outputs when replacing a single validation point.4
Definition 3.1 (Uniformly argument stability on validation) .A stochastic HO algorithm Aisϵarg-
uniformly argument stable on validation where
ϵarg:= sup
Sval≃˜Sval∈Zm,Str∈ZnEA[∥A(Sval, Str)− A(˜Sval, Str)∥]. (5)
Our analysis mainly leverages this notion following [ 31], as it is the key measure for stability bounds
under the Lipschitz continuous condition. ϵargdiffers from the uniform stability ϵstabdefined in [ 17,
Definition 1] only by a Lipschitz constant L(i.e.,ϵstab≤Lϵarg), and our results for ϵstabare also
2For formula neatness, we set an unused ηK+1= 0as a placeholder, and similarly αT+1= 0in Eq. (6).
3Namely, ηk=ηandαt≤c
t, with a constant c >0. The decreasing step size is also widely adopted in the
optimization convergence analysis works, such as SGD [44, 45], AdaGrad [46], Adam [47, 48].
4Definition 3.1 considers perturbing the validation set instead of the training set. The relevant discussion is
provided in Appendix G.1
4provided in Theorem 5.6 and Theorem C.7 for direct comparison with former works. Existing stability-
based generalization bound [ 17, Theorem 1] shows that uniform stability guarantees generalization
in expectation for HO algorithms that ϵgen≤ϵstab.
Former work [ 17] constructs the first stability upper bound for UD-based HO algorithms. This result
is fundamentally based on an upper-bounded recurrence relation of the distance between the outputs
respectively optimized on twin validation sets, denoted by δt:=∥λt−˜λt∥at the t-th step.
Theorem 3.2 (Recursion upper bound for UD, Theorems 2 and 3, [ 17]).Suppose the compound
validation loss L(·;z)isL-Lipschitz continuous and γ-smooth for all z∈Z, and the training
lossℓtr(λ,·;z)isγtr-smooth for all λ∈Λandz∈Z. Then for all 1≤t≤T,EA[δt]≤
1 + (1 −1/m)αtγ
EA[δt−1] +2αtL
m,where L≲(1 +ηγtr)K, γ≲(1 +ηγtr)2K.
Unrolling this recursion, we directly get the stability upper bound in recursion form:
ϵarg≤TX
t=1T+1Y
s=t+1 
1 +αs(1−1/m)γ2αtL
m. (6)
As this result does not explicitly display its order w.r.t. Tunder decreasing step sizes αt≤c/t, [17]
further deforms Eq. (6) with the bounded loss condition to obtain ϵstab≲T(1−1/m)γc
(1−1/m)γc+1/m.
[18] analyzes a specific IFT-based algorithm, which, under certain assumptions, achieves a similar
result of ϵstab≲Tq/mwithq <1. Though these stability upper bounds have been established, their
tightness is rarely explored, and the stability of IFT-based algorithms remains largely open. Therefore,
this paper takes a first step towards establishing stability lower bounds (namely, how unstable an
algorithm can be) for both UD-based and IFT-based HO algorithms.
4 Expansion properties of update rules
This paper endeavors to establish tight lower bounds for uniform (argument) stability as defined
in Definition 3.1, which is fundamentally the supremum of the output divergence. For iterative
algorithms, this divergence accumulates recursively across the whole optimization process. Therefore,
we first introduce lower-bounded expansion properties in Section 4.1 to characterize update rules that
will induce guaranteed divergence at each iteration. This is followed by an analysis in Section 4.2 on
how the objective functions within SGD need to be structured to satisfy these properties. We will see
in Section 5 that, for gradient-based HO algorithms, these properties jointly lead to a lower-bounded
divergence recursion given the outer-level update properties in Theorem 5.1 and a lower-bounded
Lipschitz constant of the inner output given the inner-level update properties in Theorem 5.2.
Our expansion properties correspond, to some extent, with those presented by [ 19] and the key
difference lies in our focus on lower bounds rather than upper bounds. This approach not only
facilitates comparisons with upper bounds to discuss their alignments (i.e. tightness) but also provides
a general framework for analyzing the lower bounds of algorithmic stability.
4.1 Lower-bounded expansion properties of general iterative algorithms
Letwbe a general notation for parameters (or hyperparameters) in space Ω. An update rule is a
function G:Ω→Ωthat maps wto its next state G(w), and an iterative algorithm is composed
of a series of consecutive update rules. We denote two sequences of update rules by {Gt}T
t=1and
{G′
t}T
t=1, and the corresponding outputs by {wt}T
t=1and{w′
t}T
t=1.
Intrinsically, the divergence between wtandw′
tdynamically evolves across the entire process, driven
by two factors: disparity in current update rules, and difference in current parameters resulting from
prior updates. Our goal is to systematically analyze how variations between two update sequences lead
to substantial divergence in outputs. In the following, we introduce Definition 4.1 and Definition 4.2
correspondingly to characterize properties of update rules leading to increasing divergence.
Definition 4.1 (σ-divergent) .Two update rules GandG′areσ-divergent along vif for all w∈Ω,
G(w)−G′(w)⊜v,∥G(w)−G′(w)∥ ≥σ.
Definition 4.2 (ρ-growing) .An update rule Gisρ-growing along vif for all w,w′∈Ωsuch that
w−w′parallel with v,
G(w)−G(w′)⊜w−w′,∥G(w)−G(w′)∥ ≥ρ∥w−w′∥.
5Figure 1: The loss surface of a two-dimensional
example: ℓ(θ) =1
2w⊤Aw−wwhere A=
−1 0
0 1
. The direction v1=
1
0
is highlighted
in red, along which ℓexhibits expansive property.
Figure 2: Practical output distances vs. theoretical
bounds in Theorem 5.5. We implement UD-based
Algorithm 1 on Example 5.3. The output hyper-
parameter distances with increasing Tare plotted
on the horizontal axis. The upper/lower bounds
with corresponding Tare plotted on the vertical
axis. The linear trends suggest these three values
are of almost the same order w.r.t. T.
Intuitively, σ-divergent update rules produce sufficiently divergent output parameters and a ρ-growing
update rule scales the divergence between parameters with a sufficiently large factor. The direction v
is chosen as the most expansive direction, as detailed with a concrete example in Section 5.4.
4.2 Lower-bounded expansion properties of SGD
One-step SGD can be generally formulated as Gℓ,α(w) =w−α∇ℓ(w), where the loss function
directly impacts this gradient-based update rule. We now define the µ-expansive property for the loss
function which leads to the growing property of SGD.
Definition 4.3 (µ-expansive) .A differentiable function ℓ:Ω→Risµ-expansive along vif for all
w,w′∈Ωthatw−w′parallel with v, there exists µw,w′≥µsuch that
∇ℓ(w)− ∇ℓ(w′) =−µw,w′(w−w′).
This paper mainly focuses on the case when µ >0where the loss function is nonconvex. We have
µ≤0for the convex case. When µ >0, Definition 4.3 connects to µ-strongly concavity.5These
concepts are equivalent in the one-dimensional case. In general, µ-strongly concavity imposes uniform
curvature in all directions, while µ-expansiveness restricts concavity in only one direction with an
additional restriction for the colinearity of ∇ℓ(w)− ∇ℓ(w′)and−(w−w′). See Appendix G.2 for
details. We illustrate a simple loss function in Fig. 1 that satisfies Definition 4.3.
Notably, the directional restrictions on the update rules in Definitions 4.1 to 4.3 simplify the lower-
bound calculations as it enables us to focus only on the norm of the divergence at each step and get
rid of directional variation, which aids in a clearer understanding of divergence dynamics. As a first
attempt to establish stability lower bounds for bilevel optimization problems, our work leaves open
whether these conditions can be relaxed. A potential approach might involve requiring the divergence
to exhibit a specific directional component rather than strict alignment as in the current definitions.
The following lemma shows that the expansiveness of the loss function can induce the growing
property of SGD.
Lemma 4.4 (Growing property of SGD with expansive loss function, proof in Appendix B.2) .
Suppose ℓisµ-expansive along vand1 +αµ≥0, then Gℓ,αis(1 +αµ)-growing along v.
5Namely, ∀w,w′∈Ω,⟨∇ℓ(w)− ∇ℓ(w′),w−w′⟩ ≤ − µ∥w−w′∥2.
65 Lower bounds on uniform stability in HO
Based on tools introduced in Section 4, this section proceeds to precisely characterize the stability
of gradient-based bilevel HO algorithms. In Section 5.1, we provide a lower-bounded recursion of
hyperparameter divergence that aligns with Theorem 3.2 given the expansion properties of the outer
optimization, followed by a lower bound of Lipschitz constant of the inner output given the expansion
properties of the inner optimization in Section 5.2. These findings pose insights in the construction of
Example 5.3 at both the inner and outer levels to maximize the instability of HO algorithms. This
quadratic example produces a tight lower bound for UD-based algorithms, detailed in Section 5.4,
and meaningful bounds for IFT-based algorithms, provided in Appendix C.
5.1 Stability lower bound given outer-level expansion properties
We first establish a uniform argument stability lower bound by considering the outer level of the
bilevel programming as a single-level optimization problem w.r.t. the hyperparameters. This approach
takes the compound validation loss Las a whole, temporarily disregarding the dependence of this
loss on the inner-level solution and the inner Jacobian.
Suppose Svaland˜Svalare twin validation sets differing only on the i-th entry, and denote the se-
quences of update rules on Svaland˜Svalas{Gzit,αt}T
t=1and{G˜zit,αt}T
t=16respectively. According
to Definition 3.1, the uniform argument stability is lower-bounded by the hyparameter distance after
Tsteps, which primarily depends on the divergent property of update rules on different examples and
the expansiveness of the compound validation loss. By characterizing these properties and utilizing
Lemmas 4.4 and B.2, we obtain a lower bound of the stability in the following Theorem 5.1.
Theorem 5.1 (Lower bound given outer-level expansion properties, proof in Appendix B.3) .Suppose
there exists a nonzero vector valong which Gzi,αtandG˜zi,αtare2αtL′-divergent and L(·;z)is
γ′-expansive for all z∈Sval. Then we have EA[δt]≥
1 +αt(1−1
m)γ′
EA[δt−1] +2αtL′
mand
ϵarg≥TX
t=1T+1Y
s=t+1 
1 +αs(1−1/m)γ′2αtL′
m.
Theorem 5.1 echos the upper bound formulation in Eq. (6). The distinctions arise solely in the
smooth/expansive coefficients γ/γ′and the continuous/divergent coefficients L/L′. Consequently,
the alignment of these two bounds (i.e., their tightness) hinges on the values of these coefficients.
As detailed later in Section 5.2, we delve deeper into the coefficients present in our lower bound by
unfolding the bilevel problem, focusing on the solution of the inner level and its Jacobian.
Further, Theorem 5.1 not only applies to all HO algorithms that employs outer-level SGD but also
to single-level SGD. In the context of single-level SGD, the expansion properties can be directly
inferred from the loss function, as elaborated in Appendix E.
5.2 Lipschitz lower bound given inner-level expansion properties
Based on Theorem 5.1, our next step towards building stability lower bounds is analyzing the expan-
sive coefficient γ′and divergent coefficient L′of outer-level optimization where the hypergradient
is used for update. As can be observed in Eq. (1), the inner Jacobian ∇λθK(λ)is a key bridge
between the inner and outer level that significantly influence the hypergradient. Here, we measure the
lower bound of its maximum volume, i.e., the Lipschitz continuity coefficient of θK(λ)regarding λ
denoted by LθK, to provide a guarantee for the effect of the hypergradient.
Denote corresponding inner update rules as Gλ,ηandGλ′,ηgiven hyperparameters λandλ′. Apply-
ing them consecutively for Ktimes, we get two sequences of inner updates. Theorem 5.2 presents
how the expansion properties of the inner problem characterize the lower bound of LθK.
Theorem 5.2 (Lower bound of Lipschitz of the inner-level solution, proof in Appendix B.4) .Given
any two hyperparameters λ,λ′∈Λ, suppose there exists a nonzero vector valong which Gλ,ηand
6We slightly abuse the notation in the subscript of the update rule as in Section 4.2, since the loss function in
this contest can be solely distinguished by the selected sample. We use a similar simplification for the inner
update rules in the next section.
7Gλ′,ηare∥λ−λ′∥σtr-divergent and ℓtr(λ,·;z)isµtr-expansive for all z∈Strwithµtr>0. Then
we have
LθK≥σtr
ηµtr[(1 + ηµtr)K−1].
Omitting constants that depend on η,σtr, and µtr, we get LθK≳(1 +ηµtr)K.
It is worth mentioning that our lower bound for LθKis matched with its upper bound in [ 17] (see in
Theorem 3 and Proposition 2), which prepares us to obtain a tight lower bound.
5.3 An example with maximal simplification
Motivated by Theorems 5.1 and 5.2, the following example is carefully constructed, exhibiting all
expansive and divergent properties as required by these theorems to establish tight lower bounds on
uniform argument stability of gradient-based HO algorithms.
Example 5.3. We introduce an HO problem as follows. The validation loss and training loss are
given by:
ℓval(λ,θ;z) =ℓtr(λ,θ;z) =1
2θ⊤Aθ+λ⊤θ−yx⊤θ,
where A∈Rd×dis symmetric. Denote the eigenvalues of Aasγ1≤ ··· ≤ γd. Let γ1<0and
|γ1| ≥ |γd|, andv1be a unit eigenvector for γ1. LetSvaland˜Svalbe a pair of twin validation datasets
differing at the i-th example where
zi= (xi, yi) = (v1,1),˜zi= (˜xi,˜yi) = (−v1,1).
In this example, Adetermines the convexity of the problem. Throughout the main text, we consider
the most common nonconvex case where Ais indefinite and symmetric. See Appendix D for the
results of (strongly) convex losses.
Notably, our example satisfies Assumption B.1 adopted for establishing the stability upper bounds
where the loss functions are Lipschitz continuous and smooth. Hereafter we denote L(·;z)as
L-Lipschitz continuous and γ-smooth, and ℓtr(λ,·;z)asγtr-smooth, where γtr=|γ1|.
Example 5.3 is constructed adhering to the principle of maximal simplification. Specifically, the
quadratic form is essential for inducing nonconvexity. The second bilinear cross term represents the
simplest scenario for interaction between hyperparameters and parameters, ensuring a non-zero inner
Jacobian. The final term provides a connection for parameters and data. ℓvalandℓtrare set to be
identical here for simplicity, and our results do not fundamentally depend on their consistency.
We emphasize the role of the eigenvector (i.e., v1) which corresponds to the smallest eigenvalue.
It represents the least convex direction, thereby offering the greatest expansiveness of the loss
(see Fig. 1), and both the inner and outer optimizations attain the highest level of divergence and
expansiveness in this direction. Consequently, in Example 5.3, the distinct samples in Svaland˜Sval
are set to align reversely with v1to make the HO algorithms unstable.
Remark. The constructed example is required to meet two essential criteria: first, it must reveal the
instability inherent in the algorithms; second, it must allow precise calculation of the smoothness
coefficient γand the expansion coefficient µfor the compound validation loss to verify the alignment
between lower and upper bounds. Simultaneously satisfying these two requirements is challenging
for bilevel algorithms. In Appendix G.3, we provide a ridge regression example to illustrate how the
bilevel structure complicates the analysis of stability lower bounds.
5.4 Lower bounds of UD-based algorithms
The following proposition shows that Example 5.3 induces the expansion of UD-based algorithms.
Proposition 5.4 (Expansion properties of UD-based algorithms, proof in Appendix B.5) .Suppose
we solve Example 5.3 by UD-based Algorithm 1 with constant inner step size ηwhere 1−ηγd≥0
and outer step size αt. Then (1) the outer update rules Gzi,αtandG˜zi,αtare2αtL′- divergent along
v1, and (2) the composite validation loss L(·;z)isγ′-expansive along v1for all z∈Sval, where
L≍L′≍(1 +ηγtr)K, γ=γ′≍(1 +ηγtr)2K.
8Combining the lower bound in Theorem 5.1 with the upper bound in Eq. (6), we instantly get
TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ2αtL′
m≤ϵarg≤TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ2αtL
m,(7)
where the bounds are in the same order w.r.t. T,Kandm. These matching bounds in recursion form
verify the tightness of the existing upper bound [17].
Specifically, for constant step sizes, i.e., αt=cfor all t, Eq. (7) explicitly reveals the scale of ϵarg
regarding T:ϵarg≍ 
1 +c(1−1/m)γT/m. However, for linearly decreasing step sizes αt≤c/t,
additional scaling steps7are necessary and the deformed result is provided below.
Theorem 5.5 (Uniform argument stability of UD-based algorithms, proof in Appendix B.6) .Solving
Example 5.3 by UD-based Algorithm 1 with constant inner step size ηwhere 1−ηγd≥0and
decreasing outer step sizes αt=c/twithcas a positive constant has uniform argument stability that
Tln(1+(1−1
m)cγ′)
m≲ϵarg≲T(1−1
m)cγ
m,
where γ=γ′≍(1 +ηγtr)2Kas in Proposition 5.4.
The scaling steps unavoidably create a discrepancy between the deformed lower and upper bounds,
while their quotient, T(1−1
m)cγ−ln(1+(1−1
m)cγ′), is small given a small c(e.g., 0.01). We compare
the practical output hyperparameter distances and the theoretical bounds in Fig. 2.
Notably, the upper bound in our result is not contradictory to the existing upper bound of ϵarg≲
T(1−1/m)γc
(1−1/m)γc+1/min [17] because we remove the bounded loss assumption, i.e., ∃a, b∈Rs.t.
L ∈[a, b]. This modification is necessary to fairly compare the upper and lower bounds. Detailed
discussion is provided in Appendix E.3.
Based on the results of uniform argument stability, we can further obtain similar results of uniform
stability by introducing additional assumptions as below.
Theorem 5.6 (Uniform stability of UD-based algorithms, proof in Appendix B.7) .Following the
same condition as in Theorem 5.5, and additionally, if the initial points θ0=0,λ0=0, and
v1⊥xval
jfor any j∈[m]\iandv1⊥xtr
jfor any j∈[n], then Algorithm 1 has uniform stability
thatTln(1+(1−1
m)cγ′)
m≲ϵstab≲T(1−1
m)cγ
m,where γ=γ′≍(1 +ηγtr)2Kas in Proposition 5.4.
Technically, we adopt these additional assumptions following [ 34] to simplify the formulation of
L(λT,z)− L(λ′
T,z)by eliminating the quadratic term and reducing it to be colinear with λT−λ′
T.
By doing so, a clear relation can be established between the loss divergence and the hyperparameter
divergence, which leads to a transfer from uniform argument stability to uniform stability.
Remark. For now, we have characterized the stability error as an upper bound on the generalization
error. Let us now examine how this stability-based generalization bound informs the allocation
of data between the validation and training sets. Suppose we have a total of Ndata points, with
m=aNassigned to the validation set Svalandn= (1−a)Nassigned to the training set Str, where
a∈(0,1). The expected population risk can be decomposed into the generalization error and the
empirical validation risk as follows:
EA,Sval,Str,ztesth
L(A(Sval, Str);ztest)i
=ϵgen|{z}
(I)+EA,Sval,Str
1
mmX
i=1L(A(Sval, Str);zval
i)

| {z }
(II).
On one hand, the generalization bound ϵgen≤ϵstab= Θ(1 /aN)(as in Eq. (7)) suggests that a
should be sufficiently large to keep term (I) small. On the other hand, ashould also be sufficiently
small to get a low validation risk in term (II), since a larger training set generally improves validation
performance. Thus, selecting ainvolves a trade-off to optimize the overall population risk.
7For instance, 1 +x≤exis used in [19].
96 Conclusion and discussion
This paper establishes novel lower bounds of the uniform stability for various HO algorithms and
shows the existing upper bound in UD-based algorithms is tight. This result indicates that the notion
of uniform stability has reached its limit in stability analysis for the UD-based algorithm. The
lower-bounded expansion properties proposed in this paper can serve as general tools for analyzing
lower bounds of stability. This paper applies them to both single-level and bilevel optimization. We
also discuss in detail potential extensions of our analysis framework on establishing average stability
lower bounds and generalization lower bounds in Appendix H.
Limitations and social impacts. This paper is constrained in the scope of smooth loss functions,
while non-smooth scenarios [ 31] remain open. Moreover, a uniform stability lower bound does not
directly imply a generalization lower bound. This gap exists as algorithmic stability is inherently
introduced as a theoretical tool for analyzing the generalization upper bound. Alternative approaches
might include directly deriving a generalization lower bound with examples considering the data
distribution. This paper is a purely theoretical work, we have not identified any direct, significant
societal impacts that must be emphasized.
Acknowledgments and Disclosure of Funding
This work was supported by Beijing Natural Science Foundation (L247030); NSF of China (Nos.
62076145, 62206159); Beijing Nova Program (No. 20230484416); Major Innovation & Planning
Interdisciplinary Platform for the “Double-First Class" Initiative, Renmin University of China; the
Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin
University of China (22XNKJ13); the Natural Science Foundation of Shandong Province (Nos.
ZR2022QF117), the Fundamental Research Funds of Shandong University; and the Ant Group
Research Fund. The work was partially done at the Engineering Research Center of Next-Generation
Intelligent Search and Recommendation, Ministry of Education. G. Wu was also sponsored by the
TaiShan Scholars Program.
References
[1]Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation ,
12(8):1889–1900, 2000.
[2]Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil.
Bilevel programming for hyperparameter optimization and meta-learning. In ICML , volume 80,
pages 1563–1572, 2018.
[3]Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search.
InICLR , 2019.
[4]Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning .
Adaptive computation and machine learning. MIT Press, 2012.
[5]James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal
of machine learning research , 13(2), 2012.
[6]Jonas Mo ˇckus. On bayesian methods for seeking the extremum. In Optimization Techniques
IFIP Technical Conference: Novosibirsk, July 1–7, 1974 , pages 400–404. Springer, 1975.
[7]Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. Advances in neural information processing systems , 25, 2012.
[8]Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christo-
pher R Collins, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Tuning hyperparameters
without grad students: Scalable and robust bayesian optimisation with dragonfly. The Journal
of Machine Learning Research , 21(1):3098–3124, 2020.
[9]James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-
parameter optimization. Advances in neural information processing systems , 24, 2011.
10[10] Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali
Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based
training of neural networks. arXiv preprint arXiv:1711.09846 , 2017.
[11] Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based
tuning of continuous regularization hyperparameters. In International conference on machine
learning , pages 2952–2960. PMLR, 2016.
[12] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters
by implicit differentiation. In AISTATS , volume 108, pages 1540–1552, 2020.
[13] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and
reverse gradient-based hyperparameter optimization. In International Conference on Machine
Learning , pages 1165–1173. PMLR, 2017.
[14] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-
propagation for bilevel optimization. In The 22nd International Conference on Artificial
Intelligence and Statistics , pages 1723–1732. PMLR, 2019.
[15] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In ICML , vol-
ume 48, pages 737–746, 2016.
[16] Aravind Rajeswaran, Chelsea Finn, Sham M. Kakade, and Sergey Levine. Meta-learning with
implicit gradients. In NeurIPS , pages 113–124, 2019.
[17] Fan Bao, Guoqiang Wu, Chongxuan Li, Jun Zhu, and Bo Zhang. Stability and generalization
of bilevel programming in hyperparameter optimization. Advances in neural information
processing systems , 34:4529–4541, 2021.
[18] Congliang Chen, Li Shen, zhiqiang xu, Wei Liu, Zhi-Quan Luo, and Peilin Zhao. Exploring the
generalization capabilities of AID-based bi-level optimization, 2024.
[19] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. In International conference on machine learning , pages 1225–1234.
PMLR, 2016.
[20] William H Rogers and Terry J Wagner. A finite sample distribution-free performance bound for
local discrimination rules. The Annals of Statistics , pages 506–514, 1978.
[21] Luc Devroye and Terry Wagner. Distribution-free performance bounds for potential function
rules. IEEE Transactions on Information Theory , 25(5):601–604, 1979.
[22] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability,
stability and uniform convergence. The Journal of Machine Learning Research , 11:2635–2670,
2010.
[23] Olivier Bousquet and André Elisseeff. Stability and generalization. The Journal of Machine
Learning Research , 2:499–526, 2002.
[24] Vitaly Feldman and Jan V ondrak. Generalization bounds for uniformly stable algorithms.
Advances in Neural Information Processing Systems , 31, 2018.
[25] Vitaly Feldman and Jan V ondrak. High probability generalization bounds for uniformly stable
algorithms with nearly optimal rate. In Conference on Learning Theory , pages 1270–1279.
PMLR, 2019.
[26] Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly
stable algorithms. In Conference on Learning Theory , pages 610–626. PMLR, 2020.
[27] Yegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with
convergence rate o(1/n).Advances in Neural Information Processing Systems , 34:5065–5076,
2021.
11[28] Zhun Deng, Hangfeng He, and Weijie Su. Toward better generalization bounds with locally
elastic stability. In International Conference on Machine Learning , pages 2590–2600. PMLR,
2021.
[29] Tongliang Liu, Gábor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and
hypothesis complexity. In International Conference on Machine Learning , pages 2159–2167.
PMLR, 2017.
[30] Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic
gradient descent. In International Conference on Machine Learning , pages 5809–5819. PMLR,
2020.
[31] Raef Bassily, Vitaly Feldman, Cristóbal Guzmán, and Kunal Talwar. Stability of stochastic
gradient descent on nonsmooth convex losses. Advances in Neural Information Processing
Systems , 33:4381–4391, 2020.
[32] Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent.
InInternational Conference on Machine Learning , pages 2815–2824. PMLR, 2018.
[33] Yuansi Chen, Chi Jin, and Bin Yu. Stability and convergence trade-off of iterative optimization
algorithms. arXiv preprint arXiv:1804.01619 , 2018.
[34] Yikai Zhang, Wenjia Zhang, Sammy Bald, Vamsi Pingali, Chao Chen, and Mayank Goswami.
Stability of sgd: Tightness analysis and improved bounds. In Uncertainty in artificial intelli-
gence , pages 2364–2373. PMLR, 2022.
[35] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level
optimization for learning and vision from a unified perspective: A survey and beyond. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 44(12):10045–10067, 2022.
[36] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and
enhanced design. In ICML , volume 139 of Proceedings of Machine Learning Research , pages
4882–4892, 2021.
[37] Mathieu Dagréou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel
optimization that enables stochastic and global variance reduction algorithms. In NeurIPS ,
2022.
[38] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv
preprint arXiv:1802.02246 , 2018.
[39] Xuxing Chen, Minhui Huang, Shiqian Ma, and Krishna Balasubramanian. Decentralized
stochastic bilevel optimization with improved per-iteration complexity. In ICML , volume 202,
pages 4641–4671, 2023.
[40] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. arXiv preprint arXiv:1611.02163 , 2016.
[41] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. On the global optimality of
model-agnostic meta-learning. In ICML , volume 119, pages 9837–9846, 2020.
[42] Hong Li and Li Zhang. A bilevel learning model and algorithm for self-organizing feed-forward
neural networks for pattern classification. IEEE Transactions on Neural Networks and Learning
Systems , 32(11):4901–4915, 2020.
[43] Risheng Liu, Jinyuan Liu, Zhiying Jiang, Xin Fan, and Zhongxuan Luo. A bilevel integrated
model with data-driven layer ensemble for multi-modality image fusion. IEEE Transactions on
Image Processing , 30:1261–1274, 2020.
[44] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex
stochastic programming. SIAM journal on optimization , 23(4):2341–2368, 2013.
[45] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM review , 60(2):223–311, 2018.
12[46] Li Shen, Congliang Chen, Fangyu Zou, Zequn Jie, Ju Sun, and Wei Liu. A unified analysis of
adagrad with weighted aggregation and momentum acceleration. IEEE Transactions on Neural
Networks and Learning Systems , 2023.
[47] Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for
convergences of adam and rmsprop. In Proceedings of the IEEE/CVF Conference on computer
vision and pattern recognition , pages 11127–11135, 2019.
[48] Congliang Chen, Li Shen, Fangyu Zou, and Wei Liu. Towards practical adam: Non-convexity,
convergence theory, and mini-batch acceleration. Journal of Machine Learning Research ,
23(229):1–47, 2022.
[49] Yunwen Lei. Stability and generalization of stochastic optimization with nonconvex and
nonsmooth problems. In The Thirty Sixth Annual Conference on Learning Theory , pages
191–227. PMLR, 2023.
[50] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Generalization of model-agnostic meta-
learning algorithms: Recurring and unseen tasks. Advances in Neural Information Processing
Systems , 34:5469–5480, 2021.
13A Overview of main results
A.1 Overview of main contributions
Table 1: Overview of main contributions. The results presented here are derived without the bounded
loss assumption for fair comparison. Deformed bounds are derived under decreasing step size
αt≤c/twhere c >0is a constant.
Our contributions Comparable results
Expansion
propertiesσ-divergent, ρ-growing (Ours) σ-bounded, η-expansive [19]
UD-based
algorithmRecursive lower bound:PT
t=1QT
s=t+1(1 +αs(1−1/m)γ′)2αtL′
m,
where γ′=γ≍(1 +ηγtr)2K,
L′≍(1 +ηγtr)K(Ours)Recursive upper bound:PT
t=1QT
s=t+1(1 +αs(1−1/m)γ)2αtL
m,
where γ≲(1 +ηγtr)2K,
L≲(1 +ηγtr)K[17]
Deformed lower bound:
≳Tln(1+(1 −1/m)cγ′
m,
where γ′=γ≍(1 +ηγtr)2K(Ours)Deformed upper bound:
≲T(1−1/m)cγ
m,
where γ≲(1 +ηγtr)2K(Ours)
IFT-based
algorithmRecursive lower bound:PT
t=1QT
s=t+1(1 +αs(1−1/m)γ′)2αtL′
m,
where γ′=γ≍(1 +ηγtr)2K,
L′≍(1 +ηγtr)K(Ours)Recursive upper bound:PT
t=1QT
s=t+1(1 +αs(1−1/m)γ)2αtL
m,
where γ≲K(1 +ηγtr)2K,
L≲(1 +ηγtr)K(Ours)
Deformed lower bound
≲Tln(1+(1 −1/m)cγ′)
m,
where γ′≳(1 +ηγtr)2K(Ours)Deformed upper bound
≲T(1−1/m)cγ
m,
where γ≲K(1 +ηγtr)2K(Ours)
14A.2 Dependent graph of main results
Definition 4.1
σ-divergent update rulesDefinition 4.2
ρ-growing update rules
Lemma B.2
Recursion of parameter divergence
given expansion properties of update rulesDefinition 4.3
µ-expansive loss functions
Lemma 4.4
Growing property of SGD
given the expansive loss
Theorem 5.1
Lower bound of uniform argument stability
given expansion properties of
the outer optimizationTheorem 5.2
Lower bound of the Lipschitz constant
of the inner output given expansion
properties of the inner optimization
Example 5.3
Constructed example with Lipschitz
continuous and smooth loss
Proposition 5.4 for UD
(Proposition C.5 for IFT)
Expansion properties of HO
algorithms on Example 5.3
Assumption B.1
Theorem 3.2
Upper bound of uniform argument
stability in recursion form
Theorem 5.5 for UD
(Theorem C.6 for IFT)
Deformed uniform argument stability
bounds with explicit orders
Theorem 5.6 for UD
(Theorem C.7 for IFT)
Deformed uniform stability
bounds with explicit ordersEq. (7) for UD
(Eq. (13) for IFT)
Tight uniform argument stability
bounds in recursion formspecified on SGD
inspire the construction
satisfy
scaled
Figure 3: Dependent graph of our main results. The blue node denotes previous results and others are
our contributions. The solid line represents direct proof dependency. The dashed line is annotated
with text therein.
B Proofs of main theoretical results
B.1 General assumptions
We first list some assumptions in the derivation for upper bounds [ 17], which are common theoretical
conditions for an HO problem. We follow these assumptions throughout Section 5. The constructed
Example 5.3 also satisfies these assumptions.
Assumption B.1. LetΩbe an open set including Λ×Θ×Z, we assume that
151.ΛandΘare compact and convex with non-empty interiors, and Zis compact,
2.ℓval(λ,θ;z)∈C2(Ω), that is, ℓvalis second order continuously differentiable on Ω,
3.ℓtr(λ,θ;z)∈C3(Ω), that is, φiis third order continuously differentiable on Ω,
4.ℓtr(λ,θ;z)isγtr-smooth as a function of θfor all z∈Zandλ∈Λ(the first and third
points imply such a constant γtrexists).
B.2 Proof of Lemma 4.4
Lemma 4.4: Assume ℓisµ-expansive on vand1 +αµ≥0, then Gℓ,αis(1 +αµ)-growing on v.
Proof. Recalling Definition 4.3, for any w−w′colinear with vwe have
Gℓ,α(w)−Gℓ,α(w′)
=w−w′−α 
∇ℓ(w)− ∇ℓ(w′)
=w−w′+αa(w−w′)
=(1 + αµw,w′)(w−w′),
where µw,w′≥µand thus 1 +αµw,w′≥1 +αµ≥0by assumption. Therefore, we have
Gℓ,α(w)−Gℓ,α(w′)⊜w−w′and∥Gℓ,α(w)−Gℓ,α(w′)∥ ≥(1 +αµ)∥w−w′∥, which implies
Gℓ,αis(1 +αµ)-growing on vaccording to Definition 4.2.
B.3 Proof of Theorem 5.1
As the divergence of each step is entwined with prior results and shapes subsequent evolution, we
first provide the following recursion for the parameter distance using the expansion properties.
Lemma B.2 (Recursion of parameter divergence) .Let the initial points be w0=w′
0∈Ω. Suppose
there exists a nonzero vector valong which, for all 1≤t≤T,Gt̸=G′
tareσ-divergent and Gtare
ρ-growing. Then we have wt−w′
t⊜vfor all 1≤t≤Tand recursively,
∥w0−w′
0∥= 0,∥wt−w′
t∥ ≥
ρ∥wt−1−w′
t−1∥+σ, G t̸=G′
t,
ρ∥wt−1−w′
t−1∥, G t=G′
t,t≥1.
Proof. Without loss of generality, assume v1is a unit vector (i.e., ∥v1∥= 1). At the initial point, we
havew0−w′
0=0⊜v. According to Definition 4.1 and Definition 4.2, if wt−1−w′
t−1⊜v, then
wt−w′
t=Gt(wt−1)−G′
t(w′
t−1)
=
Gt(wt−1)−Gt(w′
t−1) +Gt(w′
t−1)−G′
t(w′
t−1)Gt̸=G′
t
Gt(wt−1)−Gt(w′
t−1) Gt=G′
t
=
ρt(wt−1−w′
t−1) +σtv, G t̸=G′
t,
ρt(wt−1−w′
t−1), G t=G′
t,
=
(ρt∥wt−1−w′
t−1∥+σt)v, G t̸=G′
t,
ρt∥wt−1−w′
t−1∥v, G t=G′
t,
where ρt≥ρandσt≥σ. Thus, we have the above recurrence relation for parameter distance, and
all subsequent parameter divergence will be in the direction of v.
Now we are prepared to prove Theorem 5.1: Suppose there exists a nonzero vector valong which
Gzi,αtandG˜zi,αtare2αtL′-divergent and L(·;z)isγ′-expansive for all z∈Sval. Then we have
ϵarg≥TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ′2αtL′
m.
Proof. Using Lemma 4.4, we have Gzit,αtandG˜zit,αtare(1 +αtγ′)-growing for all 1≤t≤T.
Denote δt=∥λt−˜λt∥for each step t. As Algorithm 1 is initialized with the same starting point,
16we know that λ0=˜λ0and thus δ0= 0. For all 1≤t≤T, there is a probability of 1−1
mto select
the same examples and1
motherwise. Consequently, by the law of total probability, we have the
recurrence relation
EA[δt]≥
1−1
m
EA[(1 + αtγ′)δt−1] +1
mEA[δt−1+ 2αtL′](Lemma B.2, the law of total probability)
=
1 +αt(1−1
m)γ′
EA[δt−1] +2αtL′
m(linearity of expectation) ,
By unwinding the recurrence from Tto1, for all Sval≃˜Sval∈Zm, Str∈Znwe have
EA[δT]≥TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ′2αtL′
m, (8)
which implies
ϵarg≥TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ′2αtL′
m.
B.4 Proof of Theorem 5.2
Here we prove a more general version of Theorem 5.2 in the main paper by additionally considering
the cases where µtr≤0. Theorem 5.2 in the main paper can be simply derived by letting µtr>0.
Theorem B.3 (Lower bound of Lipschitz of the inner-level solution, generalized Theorem 5.2) .Given
any two hyperparameters λ,λ′∈Λ, suppose there exists a nonzero vector valong which Gλ,ηand
Gλ′,ηare∥λ−λ′∥σtr-divergent and ℓtr(λ,·;z)isµtr-expansive for all z∈Strwhere 1+ηµtr≥0.
Then we have
LθK=

σtr
ηµtr[(1 + ηµtr)K−1], µtr>0,
σtrK, µtr= 0,
σtr
η|µtr|[1−(1−η|µtr|)K], µtr<0.
Omitting constants that depend on η,σtr, and µtr, we get LθK≳

(1 +ηµtr)K, µtr>0,
K, µtr= 0,
1, µtr<0.
Proof. First, for any λandλ′, we establish a lower bound of ∥θK(λ)−θK(λ′)∥in a recursion way.
Using Lemma 4.4, we have Gλ,ηandGλ′,ηare(1 +ηµtr)-growing. For any inner step 1≤k≤K,
we have
∥θk(λ)−θk(λ′)∥=∥Gλ,η(θk−1(λ))−Gλ′,η(θk−1(λ′))∥
=∥Gλ,η(θk−1(λ))−Gλ,η(θk−1(λ′)) +Gλ,η(θk−1(λ′))−Gλ′,η(θk−1(λ′))∥
≥(1 +ηµtr)∥θk−1(λ)−θk−1(λ′)∥+σtr∥λ−λ′∥∥v∥ (Lemma B.2)
=(1 +ηµtr)∥θk−1(λ)−θk−1(λ′)∥+σtr∥λ−λ′∥ (∥v∥= 1)
= (1 + ηµtr)∥θk−1(λ)−θk−1(λ′)∥+σtr∥λ−λ′∥. (1 +ηµtr≥0)
Using the fact that the algorithm is initialized with the same starting point and unwinding the above
recurrence from Kto1, we obtain
∥θK(λ)−θK(λ′)∥ ≥K−1X
k=0(1 +ηµtr)kσtr∥λ−λ′∥,
which implies that for any λ∈Λ. According to the mean value theorem for vector valued multivari-
able function, there exists a con line segment determined by λandλ′such that ∥∇θK(c)(λ−λ′)∥ ≥
17∥θK(λ)−θK(λ′)∥, and for triangle inequality, we have ∥∇θK(c)∥∥λ−λ′∥ ≥ ∥∇ θK(c)(λ−λ′)∥.
Therefore, by the definition of Lipschitz continuity, it holds that
LθK≥ ∥∇θK(c)∥ ≥∥θK(λ)−θK(λ′)∥
∥λ−λ′∥≥σtrK−1X
k=0(1 +ηµtr)k=

σtr(1+ηµtr)K−1
ηµtr , µtr>0,
σtrK, µtr= 0,
σtr1−(1−η|µtr|)K
η|µtr|, µtr<0,
which completes the proof.
B.5 Proof of Proposition 5.4
Before deriving Proposition 5.4, we present a technical lemma as follows.
Lemma B.4. Suppose that A∈Rd×dis a symmetric matrix. We denote v1, . . . ,vdthe orthogonal
unit eigenvectors of Aandγ1≤ ··· ≤ γdthe corresponding eigenvalues, where we assume that
1−ηγd≥0. Then it holds that
ηK−1X
k=0(I−ηA)k
2I−ηAK−1X
k=0(I−ηA)k
=ηK−1X
k=0(1−ηγ1)k
2−ηγ1K−1X
k=0(1−ηγ1)k

≍

(1−ηγ1)2K, γ 1<0,
K, γ 1= 0,
1, γ 1>0.
Proof. For simplicity, we denote ηPK−1
k=0(I−ηA)k
2I−ηAPK−1
k=0(I−ηA)k
byC, thenC
is symmetric and has eigenvectors v1, . . . ,vdas well. Based on the symmetric of C,∥C∥equals to
its maximum absolute eigenvalue, which can be expressed as
∥C∥= sup
i|∥Cvi∥|
= sup
iηK−1X
k=0(1−ηγi)k
2−ηγiK−1X
k=0(1−ηγi)k

= sup
i

2ηK γ i= 0,ηPK−1
k=0(1−ηγi)k
2−ηγi1−(1−ηγi)K
1−(1−ηγi)γi̸= 0
= sup
i(
2ηK γ i= 0,
ηPK−1
k=0(1−ηγi)k 
1 + (1 −ηγi)K
γi̸= 0.
The last equation for γi̸= 0holds for 1−ηγi≥1−ηγd≥0.
We define h(γ):=ηPK−1
k=0(1−ηγ)k 
1 + (1 −ηγ)K
, which is decreasing on (−∞, γd], achieving
the maximum at the smallest eigenvalue γ1. Therefore,
∥C∥=ηK−1X
k=0(1−ηγ1)k
2−ηγ1K−1X
k=0(1−ηγ1)k
≍

(1−ηγ1)2K, γ 1<0,
K, γ 1= 0,
1, γ 1>0.
Now, we are ready to prove Proposition 5.4. Here we prove a more general version of Proposition 5.4
in the main paper by additionally considering the cases where γ1≥0. Proposition 5.4 in the main
paper can be simply derived by letting γ1<0.
Proposition B.5 (Expansion properties of UD-based algorithms, generalized Proposition 5.4) .Sup-
pose we solve Example 5.3 by UD-based Algorithm 1 with constant inner step size ηwhere 1−ηγd≥0
18and outer step size αt. Then (1) the outer update rules Gzi,αtandG˜zi,αtare2αtL′- divergent along
v1, and (2) the composite validation loss L(·;z)isγ′-expansive along v1for all z∈Sval, where
L≍L′≍

(1 +ηγtr)2K, γ 1<0,
K, γ 1= 0,
1, γ 1>0,andγ=γ′≍

(1 +ηγtr)2K, γ 1<0,
K, γ 1= 0,
1, γ 1>0.
Proof. As Example 5.3 satisfies Assumption B.1, we have L′≤L≲(1 +ηγtr)Kby Theorem 3 in
[17]. We are going to verify that L′≳(1 +ηγtr)Kandγ=γ′≳(1 +ηγtr)2Kin the following.
Given a hyperparameter λ, a constant step size ηand a initial point θ0, at each step 1≤k≤K, we
have an inner update
Gλ,η(θk−1) =θk−1−η∇θℓtr(λ,θk−1;ztr
jk)
=θk−1−η∇θ1
2θ⊤
k−1Aθk−1+λ⊤θk−1−ytr
jkxtr⊤
jkθk−1
=θk−1−η(Aθk−1+λ−ytr
jkxtr
jk)
= (I−ηA)θk−1−η(λ−ytr
jkxtr
jk),
where jkis uniformly sampled from [n]. Recursively, we get
θK(λ) =Gλ,η
Gλ,η 
. . . G λ,η(θ0)
= (I−ηA)Kθ0−ηK−1X
k=0(I−ηA)k(λ−ytr
jkxtr
jk),
so that
∇λθK(λ) =−ηK−1X
k=0(I−ηA)k. (9)
AsL′andγ′describe the expansion properties of SGD on L, we first investigate the gradient of the
compound validation loss
∇L(λ;z) =θK(λ) +∇λθK(λ)⊤ 
AθK(λ) +λ−yx
.
For all λ∈Λ, the outer update divergence when SGD picks the distinct examples is
Gzi,αt(λ)−G˜zi,αt(λ) =αt 
∇L(λ;zi)− ∇L (λ;˜zi)
=αt∇λθK(λ)⊤ 
−yixi−(−˜yi˜xi)
=−2αt∇λθK(λ)⊤v1 (yi= ˜yi= 1,xi=v1,˜xi=−v1)
= 2αtηK−1X
k=0(I−ηA)kv1 (Ais symmetric)
= 2αtηK−1X
k=0(1−ηγ1)kv1 (Av1=γ1v1)
= 2αtηK−1X
k=0(1 +ηγtr)kv1. (γtr=|γ1|=−γ1)
Recalling Definition 4.1, we have Gzi,αtandGz′
i,αtareαtL′-divergent along v1, where
L′=ηK−1X
k=0(1 +ηγtr)k=


(1 +ηγtr)K−1
/γtr≍(1 +ηγtr)K, γ 1<0,
ηK≍K, γ 1= 0,
1−(1 +ηγtr)K
/γtr,≍1 γ1>0.(10)
For the case that γ1<0, we have L′≍(1 +ηγtr)K.
19Next, we are going to clarify that L(λ)isγ′-expansive along v1, andγ′equals to the smooth constant
γofL(λ). AsLis twice differentiable, according to the definition of smoothness, we have
γ= sup
λ∈Λ∥∇2
λL(λ;z)∥
=∥∇λθK(λ) +∇λθK(λ)⊤ 
A∇λθK(λ) +I
∥ (∇2
λθK(λ) =0)
=−ηK−1X
k=0(I−ηA)k
2I−ηAK−1X
k=0(I−ηA)k

=ηK−1X
k=0(1−ηγ1)k
2−ηγ1K−1X
k=0(1−ηγ1)k
 (Lemma B.4)
=ηK−1X
k=0(1 +ηγtr)k
2 +ηγtrK−1X
k=0(1 +ηγtr)k
 (11)
≍

(1 +ηγtr)2K, γ 1<0,
K, γ 1= 0,
1, γ 1>0.
Eq. (11) holds for γtr=|γ1|=−γ1. For all λ,λ′∈Λ, such that λ−λ′=av1⊜v1where
a∈R+, we have
∇L(λ;z)− ∇L (λ′;z) =θK(λ)−θK(λ′)
+∇λθK(λ)⊤ 
AθK(λ) +λ−yx
− ∇λ′θK(λ′)⊤ 
AθK(λ′) +λ′−yx
=−ηK−1X
k=0(I−ηA)k(λ−λ′)−ηK−1X
k=0(I−ηA)k
−AηK−1X
k=0(I−ηA)k+I
(λ−λ′)
=−ηK−1X
k=0(I−ηA)k
2I−ηAK−1X
k=0(I−ηA)k
(λ−λ′)
=−ηK−1X
k=0(I−ηA)k
2I−ηAK−1X
k=0(I−ηA)k
av1
=−ηK−1X
k=0(1−ηγ1)k
2−ηγ1K−1X
k=0(1−ηγ1)k
av1
=−ηK−1X
k=0(1 +ηγtr)k
2 +ηγtrK−1X
k=0(1 +ηγtr)k
(λ−λ′),
:=−γ′(λ−λ′).
According to Definition 4.3, this implies L(λ)isγ′-expansive along v1. Therefore
γ′=γ≍

(1 +ηγtr)2K, γ 1<0,
K, γ 1= 0,
1, γ 1>0.
For the case that γ1<0, we obtain that γ′=γ≍(1 +ηγtr)2K.
B.6 Proof of Theorem 5.5
Here we prove a more general version of Theorem 5.5 in the main paper where γ1<0by additionally
considering the cases where γ1≥0.
20Theorem B.6 (Uniform argument stability of UD algorithm, generalized Theorem 5.5) .Solving
Example 5.3 by UD-based Algorithm 1 with constant inner step size ηwhere 1−ηγd≥0and
decreasing outer step sizes αt=c/twithcas a positive constant has uniform argument stability that
Tln(1+(1−1
m)cγ′)
m≲ϵarg≲T(1−1
m)cγ
m,
where γ=γ′≍

(1−ηγ1)2K, γ 1<0,
K, γ 1= 0,
1, γ 1>0.as in Proposition B.5.
Proof. We first derive the left side of the result (i.e., the lower bound). The derivation is built upon
the resursion form of the lower bound in Theorem 5.1 and utilizes a scaling operation that when
r=ln(1+(1−1/m)cγ′)
(1−1/m)cγ′, it holds that 1 +x≥exp(rx)for any x∈
(1−1/m)cγ′/t|1≤t≤T	
.
According to Theorem 5.1, we have
ϵarg≥TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ′2αtL′
m
≥TX
t=1TY
s=t+1exp"
r
1−1
m
αsγ′#
2αtL′
m
=T−1X
t=1TY
s=t+1exp"
r
1−1
mcγ′
s#
2cL′
tm+2cL′
Tm(αt=c/t)
=T−1X
t=1exp
r
1−1
m
cγ′TX
s=t+11
s
2cL′
tm+2cL′
Tm
≥T−1X
t=1exp"
r
1−1
m
cγ′lnT+ 1
t+ 1#
2cL′
tm+2cL′
Tm(∀t2> t1>0,Pt2
t=t11
t≥lnt2+1
t1))
=TX
t=1T+ 1
t+ 1r(1−1/m)cγ′
2cL′
tm
≥2cL′
m(T+ 1)r(1−1/m)cγ′T+1X
t=2t−r(1−1/m)cγ′−1
≥2cL′
m(T+ 1)r(1−1/m)cγ′ZT+2
2t−r(1−1/m)cγ′−1dt
(∀a >0,PT+1
t=2t−a−1≥RT+2
2t−a−1dt)
=2cL′
m(T+ 1)r(1−1/m)cγ′
2−r(1−1/m)cγ′−(T+ 2)−r(1−1/m)cγ′
r 
1−1/m
cγ′

=2L′
r(m−1)γ′(T+ 1)r(1−1/m)cγ′h
2−r(1−1/m)cγ′−(T+ 2)−r(1−1/m)cγ′i
≥2cL′
mln 
1 + (1 −1/m)cγ′
T+ 1
2ln(1+(1−1/m)cγ′)
−1
. (r=ln(1+(1−1/m)cγ′)
(1−1/m)cγ′)
21Then, we continue to derive the right side of the result (i.e., the upper bound). Based on Eq. (6), we
have
ϵarg≤TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ2αtL
m
≤T−1X
t=1TY
s=t+1exp"
1−1
mγc
s#
2cL
tm+2cL
Tm
=T−1X
t=1exp

1−1
m
γcTX
s=t+11
s
2cL
tm+2cL
Tm
≤T−1X
t=1exp"
1−1
m
γclnT
t#
2cL
tm+2cL
Tm(∀t2> t1>1,Pt2
t=t11
t≤lnt2
t1−1))
=TX
t=1T
t(1−1/m)γc2cL
tm
=2cL
mT(1−1/m)γcTX
t=1t−(1−1/m)γc−1
≤2cL
mT(1−1/m)γc 
1 +ZT
1t−(1−1/m)γc−1dt!
(∀a >0,PT
t=1t−a−1≤1 +RT
1t−a−1dt)
=2cL
m(1−1/m)cγ
1 + 
1−1/m
γc
T(1−1/m)γc−1
=2L
(m−1)γ
1 + 
1−1/m
γc
T(1−1/m)γc−1
.
Therefore, it holds that
2cL′"
T+1
2ln(1+(1−1/m)cγ′)
−1#
mln 
1 + (1 −1/m)cγ′ ≤ϵarg≤2L
1 + 
1−1/m
γc
T(1−1/m)γc−1
(m−1)γ.(12)
Omitting the constants depending on c,γ, and L,γ′andL′, we haveTln(1+(1−1
m)cγ′)
m≲ϵarg≲
T(1−1
m)cγ
m, which completes the proof.
B.7 Proof of Theorem 5.6
Theorem 5.6: Following the same condition as in Theorem 5.5, and additionally, if the initial
points θ0=0,λ0=0, andv1⊥xval
jfor any j∈[m]\iandv1⊥xtr
jfor any j∈[n], then
the order of uniform stability ϵstabw.r.t. TsatisfiesTln(1+(1−1
m)cγ′)
m≲ϵstab≲T(1−1
m)cγ
m,where
γ=γ′≍(1 +ηγtr)2Kas in Proposition 5.4.
Proof. In the following, we show that ϵstabexplicitly shows the same order as ϵargin Theorem 5.5
with additional assumptions for Example 5.3. For the upper bound, it is easy to get with Lipschitz
condition that
ϵstab= sup
Sval≃˜Sval∈Zm,Str∈Zn,z∈ZEA[∥L(A(Sval, Str);z)− L(A(˜Sval, Str);z)∥]
≤ sup
Sval≃˜Sval∈Zm,Str∈ZnEA[L∥A(Sval, Str)− A(˜Sval, Str)∥]
=Lϵarg,
22and according to Theorem 5.5, we have ϵstab≲T(1−1
m)cγ
m.
To obtain the lower bound, we need to explicitly derive the optimization process of A(Sval, Str)and
A(˜Sval, Str)(i.e.,λTand˜λT), and corresponding loss values.
From the proof of Proposition 5.4, we know that
θK(λ) =Gλ,η
Gλ,η 
. . . G λ,η(θ0)
= (I−ηA)Kθ0−ηK−1X
k=0(I−ηA)k(λ−ytr
jkxtr
jk)
=−ηK−1X
k=0(I−ηA)k(λ−ytr
jkxtr
jk). (θ0=0)
=−ηK−1X
k=0(I−ηA)kλ+ηK−1X
k=0(I−ηA)kytr
jkxtr
jk
:=−BKλ+btr
K,
where symmetric matrix BK:=ηPK−1
k=0(I−ηA)kand vector btr
K:=ηPK−1
k=0(I−ηA)kytr
jkxtr
jk.
Building upon θK(λ), we can derive L(λ,z)as
L(λ,z) =1
2θ⊤
K(λ)AθK(λ) +λ⊤θK(λ)−yx⊤θK(λ)
=1
2(−BKλ+btr
K)⊤A(−BKλ+btr
K) +λ⊤(−BKλ+btr
K)−yx⊤(−BKλ+btr
K)
=1
2λ⊤(BKAB K−2BK)λ+ (−btr⊤
KAB K+btr⊤
K+yx⊤BK)λ+1
2btr⊤
KAbtr
K−yx⊤btr
K
=1
2λ⊤BK(AB K−2I)λ+ (−btr⊤
KAB K+btr⊤
K+yx⊤BK)λ+1
2btr⊤
KAbtr
K−yx⊤btr
K,
whose gradient is
∇λL(λ,z) =BK(AB K−2I)λ+ (−BKAbtr
K+btr
K+yBKx)
=BK(AB K−2I)λ+yBKx+btr
K−BKAbtr
K.
Then, the update rule of λcan be expressed as
λt=λt−1−αt∇λL(λt−1,zit)
=λt−1−αth
BK(AB K−2I)λt−1+yitBKxit+btr
K−BKAbtr
Ki
=
I−αtBK(AB K−2I)
λt−1−αtyitBKxit−αt(btr
K−BKAbtr
K).
Now, by unwinding the recurrence from Tto1withλ0=0, we can obtain
λT=TY
t=1
I−αtBK(AB K−2I)
λ0+TX
t=1TY
s=t+1
I−αsBK(AB K−2I)
(−αtyitBKxit)
+TX
t=1TY
s=t+1
I−αsBK(AB K−2I)
(−αt(btr
K−BKAbtr
K))
=TX
t=1TY
s=t+1
I−αsBK(AB K−2I)
(−αtyitBKxit)
| {z }
r
+TX
t=1TY
s=t+1
I−αsBK(AB K−2I)
(−αt(btr
K−BKAbtr
K))
| {z }
a1. (λ0=0)
23Recall that Svaland˜Svalonly differ in the i-th entry where zi= (xi, yi) = (v1,1),˜zi= (˜xi,˜yi) =
(−v1,1).Denote 1[·]as the indicator function. We simplify the term ras follows:
r=TX
t=1TY
s=t+1
I−αsBK(AB K−2I)
−αtBKmX
j=1yjxj 1[it=j]

=mX
j=1TX
t=11[it=j]TY
s=t+1
I−αsBK(AB K−2I) 
−αtBKyjxj
=TX
t=11[it=i]TY
s=t+1
I−αsBK(AB K−2I)
(−αtBKyixi)
+mX
j̸=iTX
t=11[it=j]TY
s=t+1
I−αsBK(AB K−2I) 
−αtBKyjxj
=TX
t=11[it=i]TY
s=t+1
I−αsBK(AB K−2I)
(−αtBKv1)
+mX
j̸=iTX
t=11[it=j]TY
s=t+1
I−αsBK(AB K−2I) 
−αtBKyjxj
=TX
t=11[it=i]TY
s=t+1
I−αsηK−1X
k=0(I−ηA)k(AηK−1X
k=0(I−ηA)k−2I)

−αtηK−1X
k=0(I−ηA)kv1

+mX
j̸=iTX
t=11[it=j]TY
s=t+1
I−αsBK(AB K−2I) 
−αtBKyjxj
=TX
t=11[it=i]TY
s=t+1
1−αsηK−1X
k=0(1−ηγ1)k(γ1ηK−1X
k=0(1−ηγ1)k−2)

−αtηK−1X
k=0(1−ηγ1)k
v1
+mX
j̸=iTX
t=11[it=j]TY
s=t+1
I−αsBK(AB K−2I) 
−αtBKyjxj
=−TX
t=11[i=it]TY
s=t+1
1−αsL′(γ1L′−2) 
αtL′
v1
| {z }
b:=τv1
+mX
j̸=iTX
t=11[it=j]TY
s=t+1
I−αsBK(AB K−2I) 
−αtBKyjxj
| {z }
a2,
where L′=ηPK−1
k=0(1−ηγ1)kas in Proposition B.5. We further define a:=a1+a2, and then
λT=a1+a2−b=a−b. Follow the same process of derivation, we have ˜λT=a+bwhere the
opposite symbol for barise from xi=v1while ˜xi=v1.
Recall that we have assumed that v1⊥xtr
kfor any k∈[n]andv1⊥xval
jfor any j∈[m]thatj̸=i,
thusv⊤
1btr
K= 0andv⊤
1xj= 0, j̸=i∈[m]. Therefore,
a⊤b=TX
t=1(−αt(btr
K−BKAbtr
K))⊤TY
s=t+1
I−αsBK(AB K−2I)
τv1
+mX
j̸=iTX
t=11[it=j] 
−αtBKyjxj⊤TY
s=t+1
I−αsBK(AB K−2I)
τv1.
24AsBK:=ηPK−1
k=0(I−ηA)kandv1is the eigenvector of A, we have a⊤b= 0, i.e.,a⊥b.
Now, we are ready to discuss L(λT,z)− L(˜λT,z).
L(λT,z)− L(˜λT,z)
=1
2λ⊤
TBK(AB K−2I)λT+ (−btr⊤
KAB K+btr⊤
K+yx⊤BK)λT
−1
2˜λ⊤
TBK(AB K−2I)˜λT−(−btr⊤
KAB K+btr⊤
K+yx⊤BK)˜λT
=1
2λ⊤
TBK(AB K−2I)λT−1
2λ′⊤
TBK(AB K−2I)˜λT
| {z }
c+(−btr⊤
KAB K+btr⊤
K+yx⊤BK)(λT−˜λT),
where
c=1
2(a−b)⊤BK(AB K−2I)(a−b)−1
2(a+b)⊤BK(AB K−2I)(a+b)
=−2a⊤BK(AB K−2I)b−2b⊤BK(AB K−2I)a
=−4a⊤BK(AB K−2I)b (by symmetric)
= 4a⊤BK(AB K−2I)τv1
= 4(γ1L′−2)τa⊤v1
=0.
Therefore, L(λT,z)− L(˜λT,z)can be simplified as
L(λT,z)− L(˜λT,z) = (−btr⊤
KAB K+btr⊤
K+yx⊤BK)(λT−˜λT)
=−2(−btr⊤
KAB K+btr⊤
K+yx⊤BK)b (λT−˜λT=−2b)
=−2yx⊤BKb
=−2yL′τx⊤v1. (b=τv1)
Letz∗= (v1,1), we have
|L(λT,z∗)− L(˜λT,z∗)|= 2L′τ∥v1∥2= 2L′τ=L′∥λT−˜λT∥. (∥λT−˜λT∥= 2τ)
Therefore, by the definition of ϵstab, we have
ϵstab≥ |L(λT,z∗)− L(˜λT,z∗)|=L′∥λT−˜λT∥ ≥TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ′2αtL′
m,
where the last inequality holds for Eq. (8) in the proof of Theorem 5.1. Following the proof of
Theorem B.6, we can further derive
ϵstab≥2cL′2"
T+1
2ln(1+(1−1/m)cγ′)
−1#
mln 
1 + (1 −1/m)cγ′ .
Omitting the constants regarding c,γ′andL′, we have ϵstab≳Tln(1+(1−1
m)cγ′)
m,which completes
the proof.
C Deferred results of IFT-based HO algorithm
Based on Example 5.3, we also investigate and establish a stability lower bound for the IFT-based
algorithm. The stability analysis for the IFT-based algorithm is conducted following the same proof
idea as the UD-based algorithm. Similarly to the analysis for the UD-based algorithm, we first
obtain the expansive and divergent properties of the outer level in Proposition C.5 of Appendix C.
These jointly lead to uniform argument stability bounds in Theorem C.6. For completeness, we also
derive an upper bound for the IFT algorithm based on existing techniques [ 17], presented together as
follows.
We first introduce several lemmas useful for the following proofs.
25Lemma C.1 (Lemma 2 in [ 17]).Suppose ΛandΘare convex and compact with non-empty interiors,
Zis compact, Λ×Θ×Zis included in an open set Ωandf(λ,θ;z)∈Ck(Ω), then for all i≤k−1
order partial differential h(λ,θ;z)off(λ,θ;z), we have sup
θ∈Θ,z∈Z||h(λ,θ;z)||λ∈Λ,Lip<∞and
sup
λ∈Λ,z∈Z||h(λ,θ;z)||θ∈Θ,Lip<∞.
Lemma C.1 implies that any i≤1order partial differential of ℓval(λ,θ;z)is Lipschitz and any i≤2
order partial differential of ℓtr(λ,θ,z)is Lipschitz continuous under Assumption B.1. We denote
the maximal Lipschitz constants among them as Q.
Lemma C.2 (Theorem 3 in [ 17]).Denote θK(λ)asLθK-Lipschitz continuous, we have LθK≲
(1 +ηγtr)K.
Lemma C.3. In the case of Example 5.3, the \∇λθK(λ)calculated by the IFT-based algorithm is
exactly ∇λθK(λ).
Proof.
\∇λθK(λ) =−∇2
θλℓtr(λ,θK(λ))ηK−1X
k=0h
I−η∇2
θθℓtr(λ,θK(λ))ik
=−ηK−1X
k=0[I−ηA]k
=∇λθK(λ). (Eq. (9))
Now, we are ready to prove Propositions C.4 and C.5.
Proposition C.4 (Lipshchitz properties of IFT-based algorithm) .Suppose we solve Example 5.3 by
IFT-based Algorithm 1 with constant inner step size ηwhere 1−ηγd≥0and outer step size αt.
Then the composite validation loss L(·;z)isL-Lipschitz continuous and γ-smooth for all z∈Sval,
where
L≲(1 +ηγtr)K, γ≲K(1 +ηγtr)2K.
Proof. According to Lemma 1 in [ 17], the Lipschitz continuous coefficient L=
supλ∈Λ∇λL(λ;z). For all λ∈Λ, we have
∇λL(λ;z)=∇λℓval(λ,θK(λ);z) +\∇λθK(λ)∇θℓval(λ,θK(λ);z)
≤∇λℓval(λ,θK(λ);z)+\∇λθK(λ)∇θℓval(λ,θK(λ);z)
≤Q+Q\∇λθK(λ)
=Q+Q∇2
θλℓtr(λ,θK(λ))ηK−1X
k=0h
I−η∇2
θθℓtr(λ,θK(λ))ik
≤Q+ηQ∇2
θλℓtr(λ,θK(λ))K−1X
k=0I−η∇2
θθℓtr(λ,θK(λ))k
≤Q+ηQ2K−1X
k=0
1 +η∥∇2
θθℓtr(λ,θK(λ))∥k
≤Q+ηQ2K−1X
k=0 
1 +ηγtrk
=Q+Q2 
1 +ηγtrK−1
γtr.
26Omitting the constants depending on Q,η, and γtr, we get L≲( 
1 +ηγtrK).
To obtain the smoothness coefficient γ, we first discuss the Lipschitz continuty coefficient of
\∇λθK(λ). In the following, we use ℓtrto represent ℓtr(λ,θK(λ))when there is no ambiguity.
∇λ\∇λθK(λ) =∇λ
−∇2
θλℓtr(λ,θK(λ))ηK−1X
k=0h
I−η∇2
θθℓtr(λ,θK(λ))ik

=−

∇3
θλλℓtr+∇λθK(λ)∇3
θλθℓtr)
ηK−1X
k=0h
I−η∇2
θθℓtr)ik

| {z }
B1
−
∇2
θλℓtrηK−1X
k=1
−η
∇3
θθλℓtr+∇λθK(λ)∇3
θλθℓtr)
kh
I−η∇2
θθℓtr(λ,θK(λ))ik−1

| {z }
B2.
We bound the spectral norm of B1andB2, respectively.
∥B1∥=
∇3
θλλℓtr+∇λθK(λ)∇3
θλθℓtr)
ηK−1X
k=0h
I−η∇2
θθℓtr)ik
≤
Q+Q∇λθK(λ)
ηK−1X
k=0(1 +ηγtr)k

≤
Q+QLθK
ηK−1X
k=0(1 +ηγtr)k

≲(1 +ηγtr)2K. (Lemma C.2)
In addition,
∥B2∥=∇2
θλℓtrηK−1X
k=1
−η
∇3
θθλℓtr+∇λθK(λ)∇3
θλθℓtr)
kh
I−η∇2
θθℓtr(λ,θK(λ))ik−1
=∇2
θλℓtrη
−η
∇3
θθλℓtr+∇λθK(λ)∇3
θλθℓtr)K−1X
k=1kh
I−η∇2
θθℓtr(λ,θK(λ))ik−1
≤Qη2
Q+Q∇λθK(λ)
K−1X
k=1k(1 +ηγtr)k−1

≤Qη2
Q+QLθK 
K(1 +ηγtr)K−(1 +ηγtr)K−(1 +ηγtr)
ηγtr−1!
≲K(1 +ηγtr)2K.
Denote \∇λθK(λ)to be L\∇λθK-Lipschitz continuous for all λ∈Λ, then we have
L\∇λθK= sup
λ∈Λ∇λ\∇λθK(λ)≤ ∥B1∥+∥B2∥≲K(1 +ηγtr)2K.
27With the above result and Lemma C.3, we have that for all λ,λ′∈Λ,
∇λL(λ;z)− ∇λL(λ′;z)≤∇λℓval(λ,θK(λ);z)− ∇λℓval(λ′,θK(λ′);z)
+\∇λθK(λ)∇θℓval(λ,θK(λ);z)−\∇λθK(λ′)∇θℓval(λ′,θK(λ′);z)
≤∇λℓval(λ,θK(λ);z)− ∇λℓval(λ′,θK(λ);z)
+∇λℓval(λ′,θK(λ);z)− ∇λℓval(λ′,θK(λ′);z)
+\∇λθK(λ)∇θℓval(λ,θK(λ);z)−\∇λθK(λ)∇θℓval(λ′,θK(λ′);z)
+\∇λθK(λ)∇θℓval(λ′,θK(λ′);z)−\∇λθK(λ′)∇θℓval(λ′,θK(λ′);z)
≤Q∥λ−λ′∥+QθK(λ)−θK(λ′)
+\∇λθK(λ)
Q∥λ−λ′∥+QθK(λ)−θK(λ′)
+Q\∇λθK(λ)−\∇λθK(λ′)
≤Q∥λ−λ′∥+QLθK∥λ−λ′∥
+LθK
Q∥λ−λ′∥+QLθK∥λ−λ′∥
+QL\∇λθK∥λ−λ′∥
≲K(1 +ηγtr)2K∥λ−λ′∥.
which implies that γ≲K(1 +ηγtr)2K.
Proposition C.5 (Expansion properties of IFT-based algorithms) .Suppose we solve Example 5.3
by IFT-based Algorithm 1 with constant inner step size ηwhere 1−ηγd≥0and outer step size
αt. Then (1) the outer update rules Gzi,αtandG˜zi,αtare2αtL′- divergent along v1, and (2) the
composite validation loss L(·;z)isγ′-expansive along v1for all z∈Sval, where
L′≳(1 +ηγtr)K, γ′≳(1 +ηγtr)2K.
Proof. According to Lemma C.3, in the case of Example 5.3, the hypergradient calculated with
the IFT-based algorithm is the same as the UD-based algorithm, which implies they achieve the
same parameter divergence in this example. Therefore, we have the same result for L′andγ′as in
Proposition 5.4 that L′=≳(1 +ηγtr)Kandγ′≳(1 +ηγtr)2K, which complete the proof.
C.1 Stability bounds of IFT-based HO algorithm
Combining the lower bound in Theorem 5.1 with the upper bound in Equation (6), we instantly have
TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ′2αtL′
m≤ϵarg≤TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ2αtL
m.(13)
Theorem C.6 (Uniform argument stability of IFT-based algorithm, proof in Appendix C) .Solving
Example 5.3 by IFT-based Algorithm 1 with constant inner step size ηwhere 1−ηγd≥0and
decreasing outer step sizes αt=c/twithcas a positive constantTln(1+(1−1
m)cγ′)
m≲ϵarg≲
T(1−1
m)cγ
m,where γ≲K(1 +ηγtr)2K, γ′≳(1 +ηγtr)2Kas in Proposition C.5.
The upper bound is not limited to Example 5.3, but holds in more general case with the same mild
assumption in [ 17] (see Assumption B.1). Notably, in contrast to the outcomes observed with the
UD-based algorithm, the upper bound incorporates an extra factor of K, leading to a larger upper
bound for the IFT-based algorithm and a misalignment between the lower and upper bounds.
We can further establish a similar guarantee for the uniform stability ϵstabdetailed in Theorem C.7.
28Proof. Based on properties in Propositions C.4 and C.5 the same proof as Theorem 5.5, we get the
result.
Theorem C.7 (Uniform stability of IFT-based algorithm) .Following the same condition as in
Theorem C.6, and additionally, if the initial points θ0=0,λ0=0, and v1⊥xval
jfor any
j∈[m]\iandv1⊥xtr
jfor any j∈[n], then the order of uniform stability ϵstabw.r.t. Tsatisfies
Tln(1+(1−1
m)cγ′)
m≲ϵstab≲T(1−1
m)cγ
m,where γandγ′are the same as in Proposition C.5.
Proof. With the same proof as Theorem 5.6, we can get the result.
D Deferred results of UD-based algorithm on (strongly) convex inner loss
Recalling that in Example 5.3, ℓtr(λ,θ;z) =1
2θ⊤Aθ+λ⊤θ−yx⊤θ,where the smallest eigenvalue
ofAisγ1. Therefore, when γ1≥0(γ1>0),ℓtr(λ,θ;z)is convex (strongly convex) w.r.t. θ
for all z∈Z. Utilizing the case for γ1≥0(γ1>0) in Proposition B.5 and the same proof as in
Theorem B.6 and Theorem 5.6, we can get the stability results for the convex (strongly convex) case
in this section.
Theorem D.1 (Uniform argument stability of UD-based algorithms for (strongly) convex ℓtr).Solving
Example 5.3 by UD-based Algorithm 1 with constant inner step size ηwhere 1−ηγd≥0and
decreasing outer step sizes αt=c/twithcas a positive constant has uniform argument stability that
Tln(1+(1−1
m)cγ′)
m≲ϵarg≲T(1−1
m)cγ
m,
where γ=γ′≍Kwhen γ1= 0andγ=γ′≍1when γ1>0as in Proposition B.5.
Proof. Please refer to the proof of Proposition B.5 and Theorem B.6 where we generalize the results
in the main paper for the (strongly) convex case.
Theorem D.2 (Uniform stability of UD-based algorithms for (strongly) convex ℓtr).Following the
same condition as in Theorem D.1, and additionally, if the initial points θ0=0,λ0=0, and
v1⊥xval
jfor any j∈[m]\iandv1⊥xtr
jfor any j∈[n], then Algorithm 1 has uniform stability
that
Tln(1+(1−1
m)cγ′)
m≲ϵstab≲T(1−1
m)cγ
m,
where γ=γ′≍Kwhen γ1= 0andγ=γ′≍1when γ1>0as in Proposition B.5.
Proof. Please refer to the proof of Theorem 5.6.
E Deferred results of single-level SGD
As discussed in Section 4, deriving a stability lower bound entails constructing an example with
maximum instability, and we need to study two aspects of the constructed example: (1) properties of
the (compound) loss, and (2) stability behavior of the outer SGD corresponding to these properties. For
(2), the outer level of gradient-based bilevel HO algorithms and the single-level SGD have equivalent
formulation observing corresponding relations between λ↔w,L ↔ ℓ,Sval↔Sand stability
definitions Definition 3.1 ↔Eq. (15). As a result, given the smoothness constants γforLandℓ, the
stability upper bounds under the bounded loss condition for the bilevel ( ϵstab≲T(1−1/m)γc
(1−1/m)γc+1/m
in [17]) and single-level ( ϵstab≲Tγc
γc+1/min [19]) algorithms have similar results. Given those
properties, their stability lower bounds can be analyzed in a general framework: construct a well-
designed example, examine its key properties, and derive the stability lower bound in response to
these properties.
Our proposed lower-bounded expansion properties in Section 4 and provable stability lower bound
given these properties in Theorem 5.1 are generally applicable for both bilevel and single-level
29analysis. Building upon these tools, we also establish stability lower bounds for single-level SGD in
addition to our main results regarding bilevel algorithms. Notably, while the technique of stability
analysis for the outer level of bilevel problems can be adapted to single-level ones, the stability
behavior of bilevel and single-level problems are not directly comparable .
In this Section, we introduce basic concepts corresponding to stability analysis of single-level SGD
in Appendix E.1 introduced by [ 19]. Based on this, Appendix E.2 leverages the lower-bounded
expansion properties established in Section 4 to provide a stability lower bound for single-level
SGD, which is tighter than the existing result in [ 34, Theorem 4]. An upper bound is established in
Appendix E.3 for a fair comparison between the lower and upper bounds without the bounded loss
condition. Detailed comparison with existing works is provided in Table 2.
Algorithm 2 Single-level SGD
1:Input: Initialization w0; dataset S; step size scheme α
2:Output: The parameter wT
3:fort= 1toTdo
4: uniformly sampling itfrom [m]
5:g← ∇ℓ(wt−1;zit)
6:wt←wt−1−αtg
7:end for
8:return wT
E.1 Problem formulation for the stability analysis of single-level SGD
Suppose we are interested in the distribution Don data space Z, from which we obtain a sample
S={zi}m
i=1i.i.d.∼ Dm. Suppose wis the parameter to optimize in space Ω, and its loss on an example
zisℓ(w;z). The single-level SGD is shown in Algorithm 2. Following [ 19], the generalization error
of single-level SGD is defined as
ϵgen:=EA,S
Ez∼D[ℓ(A(S);z)]−1
mmX
i=1ℓ(A(S);zi)
, (14)
and we say a single-level stochastic algorithm Aisϵarg-uniformly argument stable if,
ϵarg= sup
S≃˜S∈ZmEA[∥A(S)− A(˜S)∥]. (15)
Based on these definitions, [ 19] has shown that stability guarantees generalization in single-level
problems: if a stochastic algorithm Aisϵarg-uniformly argument stable and the loss function ℓ(w;z)
isL-Lipschitz on Ωfor all z∈Z, then we have
ϵgen≤ϵstab≤Lϵarg. (16)
E.2 Proof of uniform stability lower bound
We first present a single-level example following [34].
Example E.1. Suppose Ω ={w:∥w∥ ≤W}where W > 0, andZ=X×Ywhere X={x:
∥x∥ ≤1}andY= [−1,1]. Assume the loss function is ℓ(w;z) =1
2w⊤Aw−yx⊤w,where
A∈Rd×dis a symmetric matrix. Denote the eigenvalues of Aasγ1≤ ··· ≤ γd, where γ1<0and
|γ1| ≥ |γd|, andv1as a unit eigenvector of Aforγ1. Additionally, suppose the twin datasets Sand
˜Sare different at the i-th entry, where zi= (xi, yi) = (v1,1),˜zi= (˜xi,˜yi) = (−v1,1).
Proposition E.2 (Lipschitz continuty and smoothness coefficients) .In Example E.1, the loss function
ℓ(w;z)isL-Lipschitz continuous and γ-smooth on Ωfor all z∈Z, where L≤ |γ1|W+ 1and
γ=|γ1|.
Proof. Asℓis twice differentiable on Ω×Z, we have
L= sup
z∈Zsup
w∈Ω∥∇ℓ(w;z)∥= sup
z∈Zsup
w∈Ω∥Aw−yx∥ ≤ |γ1|W+ 1,
γ= sup
z∈Zsup
w∈Ω∥∇2ℓ(w;z)∥= sup
z∈Zsup
w∈Ω∥A∥=|γ1|.
30Proposition E.3 (Divergent and expansive coefficients) .Suppose we solve Example E.1 by single-
level SGD, then the gradient update rules Gzi,αtandG˜zi,αtare2αtL′-divergent along v1and
ℓ(w;z)isγ′-expansive along v1onΩfor all z∈S, where L′= 1andγ′=|γ1|.
Proof. For all w∈Ω,
Gzi,αt(w)−G˜zi,αt(w) =−αt 
∇ℓ(w;zi)− ∇ℓ(w;˜zi)
=−αt(yixi−˜yi˜xi) = 2 αtv1⊜v1.
Recalling Definition 4.1, this implies Gzi,αtandG˜zi,αtare2αtL′-divergent where L′= 1.Addi-
tionally, for all w,w′∈Ωsuch that w−w′collinear with v1and any z∈Z, we have
∇ℓ(w;z)− ∇ℓ(w′;z) =A(w−w′) =γ1(w−w′) =−|γ1|(w−w′).
Recalling Definition 4.3, this implies ℓ(w;z)isγ′-expansive on Ωfor all z∈Zwhere γ′=|γ1|.
Given Proposition E.3, we can directly leverage Theorem 5.1 to obtain a stability lower bound.
Theorem E.4 (Lower bound of single-level SGD in recursion form) .In th case of Example E.1,
running SGD for Tsteps on a γ-smooth loss function has uniform argument stability with
ϵarg≥TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ2αt
m.
Proof. Using Theorem 5.1 and Proposition E.3, we gets the result.
Following the proof of Theorem 5.5 we can deform the result in Theorem E.4 to display an explicit
order under decreasing step sizes.
Theorem E.5 (Lower bound of single-level SGD in deformed form) .In th case of Example E.1,
running SGD for Tsteps on a γ-smooth loss function with step sizes αt=c/thas uniform argument
stability with
ϵarg≥2c
mln 
1 + (1 −1/m)cγ
T+ 1
2ln(1+(1−1/m)cγ)
−1
.
Omitting constant factors that depends on candγ, we have ϵarg≳Tln(1+(1−1/m)cγ)
m.
Proof. The proof follows the scaling for the lower bound in Theorem 5.5.
Remark. Compared with Theorem 4 in [ 34] our result relaxes the condition and improves its order
w.r.t. m. To see this, we first show that the step-size settings are equivalent. In particular, αt=a
0.99γt
(Lemma 3, [ 34]) is equivalent to αt=c
t(Theorem E.5, ours) with c=a
0.99γ. Based on this
equivalence, we can rewrite the lower bound in [ 34] asϵarg≳T0.99cγ
m1+0.99cγwith assumptions c= 1,
0< γ <0.1
0.99andT > m (detailed in the proof of Theorem 4, [ 34]). In contrast, our lower
bound of ϵarg≳Tln(1+(1 −1/m)cγ)
min Theorem E.5 holds for any c >0,γ >0,T≥1, relaxing the
conditions. Regarding the tightness of the lower bound, our result is sharper concerning mgiven
limm→∞T0.99cγ
m1+0.99cγ
Tln(1+(1 −1/m)cγ)
m= 0 for fixed T. In addition, concerning T, our result is comparable
observing that the ratio of the powers on Tdiffer slightly, namely 0.96≤0.99cγ
ln(1+(1 −1/m)cγ)≤1.06
for all m≥100, under the scope of application of their result (i.e., c= 1and0< γ <0.1
0.99). The
superiority of our lower bound stems from a loose result in Lemma 3 in [ 34]. Denote ∆t:=wt−˜wt.
It states that EA[∥∆T∥|∆t0̸= 0]≥1
2n(T
t0)0.99cγ, while this can be improved into EA[∥∆T∥|∆t0̸=
0]≥1
2m(T
t0)0.99cγ+ (T+1
t0+1)0.99cγ∆t0, which will lead to a sharper lower bound.
31E.3 Proof of uniform stability upper bound
Denote δt:=∥wt−˜wt∥. The proof leverages an intermediate result of Theorem 3.12 in [19].
Lemma E.6 ([19], Theorem 3.12) .Assume ℓ(·;z)isL-Lipschitz and γ-smooth for all z∈Z.
Running SGD with step sizes αt, for all S≃˜S∈Zm, we have the recurrence relation: ∀1≤t≤T,
EA[δt]≤
1−1
m
(1 +αtγ)EA[δt−1] +1
m 
EA[δt−1] + 2αtL
.
Unwinding the recursion we have the stability upper bound.
Theorem E.7 (Upper bound of single-level SGD in recursion form) .Assume ℓ(·;z)isL-Lipschitz
andγ-smooth for all z∈Z. In th case of Example E.1, running SGD with step sizes αthas uniform
argument stability with
ϵarg≤TX
t=1T+1Y
s=t+1 
1 + (1 −1/m)αsγ2αt(γW+ 1)
m.
Proof. As defined, ϵarg= supS≃˜S∈ZmEA[δT]and. Unwinding the recursion in Lemma E.6 and
using the fact that L≤γW+ 1in Proposition E.2, we get the result.
Here we set an additional αT+1= 0for the expression neatness. Recalling Theorem E.4, the upper
and lower bound are in exactly the same formulation with only difference in by a constant (i.e.,
γW+ 1), which means the lower and upper bound tightly match w.r.t. the key factors Tandm.
Considering the case of constant step size, we get ϵarg≍
1+(1−1/m)αγT
m, showing an exploding
rate w.r.t. T. When adopting linearly decreasing step sizes αt≤c/t, the upper bound can also be
deformed to reveal an explicit order w.r.t. key factors.
Theorem E.8 (Upper bound of single-level SGD in defromed form) .Assume ℓ(·;z)isL-Lipschitz
andγ-smooth for all z∈Z. Running SGD for Tsteps with step sizes αt≤c/thas uniform argument
stability of
ϵarg≤2(γW+ 1)
(m−1)γ
1 + 
1−1/m
γc
T(1−1/m)cγ−1
.
Omitting constant factors that depends on c,γandW, we have ϵarg≲T(1−1/m)cγ
m.
Proof. The proof follows the scaling for the upper bound in Theorem 5.5.
Combining Theorem E.8 and Theorem E.5, we haveTln(1+(1−1/m)cγ)
m≲ϵarg≲T(1−1/m)cγ
m. Notably,
the discrepancy between the lower and upper bounds is unavoidable, stemming from the scaling steps
required to obtain an explicit order, and this gap becomes small when we have large mand small cγ.
Concerning the lower and upper bounds in recursion form in Theorem E.5 and Theorem E.7, our
results are tightly matched.
Remark. Notice that Theorem 3.8 in [ 19] presents an upper bound of ϵarg≲Tγc
γc+1
m, which is tighter
than our result of ϵarg≲T(1−1/m)γc
mbut with additional bounded loss assumption that ℓ(w;z)∈[0,1].
Both results are based on the recurrence relation in Lemma E.6. They derive the upper bound with a
hitting time t0and bound the loss divergence after t0with the bounding loss constant (i.e., 1) and
thus get a tighter upper bound. However, to derive lower bounds, we need to explicitly calculate the
divergence between parameters and corresponding loss values, which will inevitably reveal all the
terms in the recursion. In this case, the bounded loss assumption is not applicable and thus we present
an upper bound without this condition as a fair and clear comparison with the lower bound.
We acknowledge that the bounded loss assumption is commonly adopted for upper-bound analysis in
theoretical works. Despite [ 19], several following works also adopt this technique. [ 34] derive the
upper bound of ϵarg≲Tγc
mγc+1in the nonconvex case with a similar approach by bounding the loss
after hitting time t0, with an additional setting for t0=n. However, there appears to be a misuse of
Lemma 4 in their proof of Theorem 5, which leads to their result being tighter compared to [ 19] in
32the case of Tcγ
cγ+1≤m. Specifically, in the proof of Theorem 5, they decompose EA[∥∆T]into two
terms that EA[∥∆T]≤EA[∥∆T∥|∆n= 0]P[∆n= 0] + EA[∥∆T∥|∆n̸= 0]P[∆n̸= 0] to bound
these two terms separately. For the second term, the union bound is used to get EA[∥∆T∥|∆n̸=
0]P[∆n̸= 0]≤1
nPn
t=1EA[∥∆T∥|H=t], where H=tdenotes that tis the first time SGD pick
the different entry in the twin datasets. EA[∥∆T∥|H=t]is further bounded using Lemma 4 that
EA[∥∆T∥|∆t= 0]≤(T
t)a2L
nto getEA[∥∆T∥|H=t]≤(T
t)a2L
n. While this appears to be a
misuse as H=tcan only imply ∆t−1= 0and whether ∆t= 0remains uncertain, where Lemma 4
is not applicable. Another work [ 49] use a large constant to bound the loss divergence from the start
of the evolution of the parameter divergence, which leads to an upper bound of ϵarg≲T
meven with
constant step sizes. A detailed comparison of existing results is listed in Table 2.
Table 2: A detailed comparison of existing results on uniform stability of single-level SGD. We unify
the notations that the loss function is γ-smooth, the dataset is of size mand SGD picks the different
entry for the first time at t0.
SettingsStep size Constant αt=α Decreasing αt≤c/t
Range of iterations
with bounding loss1≤t≤T t 0≤t≤T -
ResultsUpper boundT
m[49]Tγc
γc+1
m[19]T(1−1/m)γc
m(Ours)
Lower bound - -Tln(1+(1 −1/m)cγ)
m(Ours)
T0.99cγ
m1+0.99cγ[34]
F Details of simulations
The implementing code is provided in the supplementary material. All simulations can be conducted
on the CPU of a laptop.
F.1 Hyperparameter distance and stability bounds
To examine the tightness and validity of the upper and lower bounds presented in Theorem 5.5, we
implement UD-based Algorithm 1 on Example 5.3 with linearly decreasing step sizes and compare
the practical output hyperparameter distances with our theoretical bounds under a range of outer
iterations T.
Specifically, we set the loss functions and the twin validation sets as in Example 5.3 with A=
−1 0
0 1
andv1=
1
0
. The optimization is implemented with fixed γtr= 1,K= 100 ,m= 100 ,
n= 100 ,η= 0.01, and c= 0.01.
The comparison is shown in Fig. 2. We plot the output hyperparameter distances with increasing T
from 1000 to5000 on the horizontal axis and the deformed lower bounds and upper bounds with
corresponding Ton the vertical axis. The dashed lines are linear fittings of the hyperparameter
distances and the upper/lower bounds, to examine the linear trends of their relative magnitude.
F.2 Recursive stability bounds and deformed stability bounds
Here we implement additional simulations to examine the tightness between the recursive upper/lower
bounds and deformed upper/lower bounds presented in Eq. (7) and Appendix B.6. Specifically,
the recursive upper bound is calculated byPT
t=1QT
s=t+1 
1 +αs(1−1/m)γ
2αtL′/mand the
recursive lower bound is calculated byPT
t=1QT
s=t+1 
1 +αs(1−1/m)γ
2αtL/m in Eq. (7). The
deformed upper bound is calculated by T(1−1
m)cγ/mand the deformed lower bound is calculated by
Tln(1+(1−1
m)cγ′)/m. As for the coefficients: we set γtr= 1in Example 5.3. L′is calculated with
(1 +ηγtr)K−1
/γtras in Eq. (10) and Lis calculated with L= 0.1+1.1L′as they are of the same
33order of magnitude. γ=γ′is calculated with ηPK−1
k=0(1 +ηγtr)k
2 +ηγtrPK−1
k=0(1 +ηγtr)k
as in Eq. (11).
During the optimization, we fix η= 0.01,n= 100 andc= 0.01. For the simulation regarding T,
we set K= 100 andm= 100 and plot the results of the recursive upper bounds for Tfrom 1000 to
5000 in the horizontal axis with the corresponding other three bounds in the vertical axis, shown in
Fig.4. For the simulation regarding K, we set T= 1000 andm= 100 and plot the results of the
recursive upper bounds for Kfrom25to200in the horizontal axis with the corresponding other three
bounds in the vertical axis, shown in Fig.5. For the simulation regarding m, we set T= 1000 and
K= 100 and plot the results of the recursive upper bounds for mfrom 100to2000 in the horizontal
axis with the corresponding other three bounds in the vertical axis, shown in Fig.6.
All curves exhibit linear trends, indicating these bounds are in the same order w.r.t. T,K, and m.
Figure 4: The relations of bounds for Tfrom
1000 to5000 .
Figure 5: The relations of bounds for Kfrom
25to200.
Figure 6: The relations of bounds for Kfrom 100to2000 .
G Additional discussions
G.1 Additional discussion for the definition of uniform stability on validation
In HO, the model is typically evaluated during the validation phase and is expected to generalize
well on unseen test data based on its validation performance. Therefore, we focus on the impact
of perturbations in the validation set in our current definition of generalization error and uniform
argument stability. However, in the context of meta-learning where both datasets play crucial
roles [ 50], considering the perturbations in the training set may provide additional insights for
generalization analysis, which might be an interesting topic for future work.
34G.2 Additional discussion for expansiveness and existing concepts
We first clarify that the convex loss function corresponds to Definition 4.3 for the case when µ≤0.
When the loss function is convex, we have ⟨∇ℓ(w)− ∇ℓ(w′),w−w′⟩ ≥ 0. According to
Definition 4.3, if the loss is additionally µ-expansive, there exist µw,w′≥µsuch that
⟨∇ℓ(w)− ∇ℓ(w′),w−w′⟩=⟨−µw,w′(w−w′),w−w′⟩
=−µw,w′∥w−w′∥2
≥0,
thus we have µ≤µw,w′≤0.
When µ >0,µ-strongly concavity requires ⟨∇ℓ(w)− ∇ℓ(w′),w−w′⟩ ≤ − µ∥w−w′∥2for all
w,w′∈Ω, and Definition 4.3 restricts that for all w,w′∈Ωthatw−w′parallel with v, there
exists µw,w′≥µsuch that
⟨∇ℓ(w)− ∇ℓ(w′),w−w′⟩=⟨−µw,w′(w−w′),w−w′⟩
=−µw,w′∥w−w′∥2
≤ −µ∥w−w′∥2.
Therefore, µ-expansiveness along vimplies concavity only for w−w′parallel with v.
On the other hand, if we have ⟨∇ℓ(w)− ∇ℓ(w′),w−w′⟩ ≤ − µ∥w−w′∥2forw−w′parallel
withv, then
⟨∇ℓ(w)− ∇ℓ(w′),w−w′⟩ ≤ ∥∇ ℓ(w)− ∇ℓ(w′)∥∥w−w′∥
≤ −µ∥w−w′∥2,
and thus ∥∇ℓ(w)− ∇ℓ(w′)∥ ≤ − µ∥w−w′∥. Therefore, compared with strongly concavity on a
single direction, µ-expansiveness has an additional restriction for the colinearity of ∇ℓ(w)− ∇ℓ(w′)
and−(w−w′).
Additionally, for the one-dimensional case, the condition w−w′parallel with some scalar vis equiv-
alent to ∀w, w′∈Ω. Therefore, µ-expansiveness along vimplies µ-strongly concavity. Conversely,
µ-strongly concavity implies that there exists a µw,w′≥µsuch that
(∇ℓ(w)− ∇ℓ(w′))(w−w′) =−µw,w′(w−w′)2
≤ −µ(w−w′)2,
thusµ-strongly concavity conversely also implies µ-expansiveness along v. Therefore, these concepts
are equivalent in the one-dimensional case.
G.3 Technical challenges of stability lower bound analysis for bilevel algorithms
The nested optimization in bilevel algorithms poses challenges to the stability analysis as the insta-
bility and simplicity of the constructed example are both crucial for deriving a tight lower bound.
Specifically, to examine the alignment of the lower and upper bounds, we need to precisely calcu-
late the smooth coefficient γand expansive coefficient µof the compound validation loss for the
constructed example. While the implicit and intricate formulation of ∇L(λ)in bilevel optimization
makes γandµdifficult to obtain. In the following, we take ridge regression as an example to illustrate
how the bilevel structure hinders the stability analysis.
Example G.1 (Regularization coefficient in ridge regression) .The validation loss and training
loss are given by ℓval(λ,θ) =1
2(y−θTx)2, ℓtr(λ,θ) =1
2(y−θ⊤x)2+λ
2θ⊤θ.Solving it with
UD-based Algorithm 1, we have the inner output as θK(λ) =QK
k=1(I−ηλI−ηxjkxjk⊤)θ0+PK
i=1QK
l=k+1(I−ηλI−ηxjlxjl⊤)ηyjkxjkand a far more complex inner Jacobian ∇λθK(λ),
resulting in a unmeasurable hypergradient ∇L(λ) =∇λθK(λ)(y−θK(λ)⊤x)(−x).
These complexities obstacle us to precisely examine the divergence dynamics at each step. There-
fore, we introduce expansion properties in Section 4 and the general lower bounded guarantees
in Theorems 5.1 and 5.2 to jointly contribute to the careful construction of Example 5.3. As a
result, Example 5.3 exhibits the maximum instability while having a relatively simple outer gradient
update feasible for lower bound analysis, which will lead to tight stability lower bounds presented in
Section 5.4.
35H Potential extension of our framework
H.1 Extension on average stability lower bounds
Considering the similarities between average stability [ 32, Definition 2] and uniform stability, our
techniques may be adapted to the data-dependent setting for average stability with some modifications.
In this section, we present a preliminary proof sketch for establishing the stability lower bound of
single-level SGD based on the variant Example E.1.
To account for the randomness in the sampled datasets, we first define some notations. Let Sidenote
a copy of Swith the i-th element replaced by z′
i,GS,t/GSi,tandwS,t/wSi,tdenote the SGD update
rules and the updated parameters optimized on SandSiat the step t. With a slight adjustment
Section 4.2 and a similar proof as in the current paper, Theorem 5.1 can be modified as: Suppose
there exists a nonzero vector v(S, Si)along which GS,tandGSi,tare2αtL′(S, Si)-divergent and
L(·;z)isγ′
t(S, Si)-expansive for all wS,t−wSi,tthat parallel with v(S, Si). Then we have
EA[δt(S, Si)]≥[1 +αt(1−1/m)]γ′
t(S, Si)EA[δt−1(S, Si)] + 2 αtL′(S, Si)/m.
This recursion closely corresponds with the recursive upper bound in [32, Eq.(19)]:
EA[δt(S, Si)]≤[1 +αt(1−1/m)]ψt(S, Si)EA[δt−1(S, Si)] + 2 αtL(S, Si)/m,
and the matching of γ′
t&ψt,L′&Lwill further guide the design of the constructed example.
Therefore, our analysis framework can be adapted for the average stability with suitable modifications.
Specifically for average stability, a core challenge is calculating the expectation over SandSifor the
above recursive formula, which is beyond the scope of our paper. In the following, we provide a proof
sketch for establishing the average argument stability lower bound to clarify a possible approach to
extend our framework.
Assume that the data follows a distribution Dthatp(z) =
0.5ifz= (v1,1),
0.5ifz= (−v1,1).Si.i.d.∼ Dm
andz′
i∼ D are independent of each other. The validation loss and training loss follow Example E.1.
Under these assumptions, we derive that L′(S, Si) =∥yixi−y′
ix′
i∥/2andµt(S, Si) = 0 forzi=z′
i,
µt(S, Si) =|γ1|forzi̸=z′
i. This leads to the average argument stability lower bound:
ϵarg≥TX
t=1TY
s=t+1(1 +αs(1−1/m|γ1|))αt/m.
H.2 Extension on generalization lower bounds
In this section, we discuss a possible approach to extend our framework on the analysis of gener-
alization lower bounds. We first present a lemma clarifying the fundamental equivalence between
generalization and stability. Then a lower bound on the expected hyperparameter divergence is
established by slightly modifying Example 5.3 with additional design on the data distribution, which
will imply the generalization lower bound under certain conditions.
Lemma H.1. LetSval= (zval
1, . . . ,zval
m)andSval′= (zval
1′, . . . ,zval
m′)be two independent samples
drawn i.i.d. from Dval. Let ˜Sval
i= (zval
1, . . . ,zval
i′, . . . ,zval
m)denote the twin validation set of Sval
differing in the i-th example. Consider ϵgenas the generalization error defined in Eq. (4). Then, we
have
ϵgen=1
mmX
i=1EA,Str,Sval,zval
i′[L(A(Sval, Str);zval
i′)− L(A(˜Sval
i, Str);zval
i′)].
Proof. According to the definition of generalization error in Eq. (4) and the linearity of expectation,
ϵgen=EA,Sval,Strh
Ez∼Dtest[L(A(Sval, Str);z)]i
| {z }
(a)−1
mmX
i=1EA,Sval,Strh
L(A(Sval, Str);zval
i)i
| {z }
(b).
36As it is assumed Dval=DtestandSval′is i.i.d. sampled from Dvalindependent from Sval, term (a)
can be rewritten as
(a)=EA,Sval,Str
1
mmX
i=1Ezval
i′[L(A(Sval, Str);zval
i′)]
.
Under the expectation, Svaland˜Sval
iis exchangeable, then term (b) can be rewritten as
(b)=1
mmX
i=1EA,˜Sval
i,Strh
L(A(˜Sval
i, Str);zval
i′)i
=1
mmX
i=1EA,Sval,zval
i′,Strh
L(A(˜Sval
i, Str);zval
i′)i
.
Combining (a) and (b) leads to the equation in the theorem.
Lemma H.1 shows a fundamental relation between generalization and stability: The generalization
error equals the expected loss divergence when replacing a single example in the validation set.
Stability-based generalization analysis typically takes the supremum on Str, Sval,zval
i′to obtain a
distribution-agnostic upper bound of generalization error as
ϵgen≤ϵstab:= sup
Str,Sval,zval
i′EA[L(A(Sval, Str);zval
i′)− L(A(˜Sval
i, Str);zval
i′)],
where ϵstabis commonly upper bounded assuming L-Lipshcitz of the loss as
ϵstab≤Lϵarg:=L sup
Str,Sval≃˜SvalEA[∥A(Sval, Str)− A(˜Sval, Str)∥].
This paper attempts to derive lower bounds of ϵargwhich will not directly imply the generalization
lower bounds because ϵargis fundamentally a distribution-agnostic upper bound for the generalization
error. In order to obtain a generalization lower bound, a promising way is to extend Example 5.3 with
additional assumption on the validation distribution rather than directly specifying Svaland˜Sval.
We present a primary result below for the lower bound of the expected hyperparameter divergence,
which indicates a way to extend our methods on the analysis of generalization lower bounds.
Example H.2. We introduce an HO problem as follows. Let the validation loss and the training loss
be:
ℓval(λ,θ;z) =ℓtr(λ,θ;z) =1
2θ⊤Aθ+λ⊤θ−yx⊤θ,
where A∈Rd×dis symmetric. Denote the eigenvalues of Aasγ1≤ ··· ≤ γd. Letγ1<0,γd≤0,
andv1be a unit eigenvector for γ1. Suppose the validation distribution follows:
p(z) =
0.5ifz= (v1,1),
0.5ifz= (−v1,1).
Theorem H.3. Suppose we solve Example H.2 by UD-based Algorithm 1 with constant inner step size
ηwhere 1−ηγd≥0and outer step size αt. Denote that the expected hyperparameter divergence
asϵgen,arg:=1
mPm
i=1EA,Str,Sval,zval
i′[∥A(Sval, Str)− A(˜Sval
i, Str)∥].Then, we have
ϵgen,arg≥TX
t=1TY
s=t+1 
1 +αs(1−1/m)γαtL′
m,
where
L≍L′≍(1 +ηγtr)K, γ=γ′≍(1 +ηγtr)2K.
37Proof. We first decompose ϵgen,argconditioned on the difference of zval
iandzval
i′as
ϵgen,arg
=1
mmX
i=1P[zval
i=zval
i′]EA,Str,Sval,zval
i′[∥A(Sval, Str);−A(˜Sval
i, Str)∥|zval
i=zval
i′]
+1
mmX
i=1P[zval
i̸=zval
i′]EA,Str,Sval,zval
i′[∥A(Sval, Str)− A(˜Sval
i, Str)∥|zval
i̸=zval
i′]
=1
mmX
i=1P[zval
i̸=zval
i′]EA,Str,Sval,zval
i′[∥A(Sval, Str)− A(˜Sval
i, Str)∥|zval
i̸=zval
i′]
=1
2mmX
i=1EA,Str,Sval,zval
i′[∥A(Sval, Str)− A(˜Sval
i, Str)∥|zval
i̸=zval
i′].
The last equation holds for that as zval
iandzval
i′are sampled from Dvalspecified in Example H.2
independently, which leads to P[zval
i̸=zval
i′] = 1/2.
According to Proposition 5.4 and Theorem 5.1, for any Str,i∈[m], and Sval≃˜Sval
iwhere
zval
i̸=zval
i′∈ {(−v1,1),(v1,1)}, we have
∥A(Sval, Str)− A(˜Sval
i, Str)∥ ≥TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ2αtL′
m.
Therefore, it holds that
ϵgen,arg≥1
2mmX
i=1TX
t=1TY
s=t+1 
1 +αs(1−1/m)γ2αtL′
m=TX
t=1TY
s=t+1 
1 +αs(1−1/m)γαtL′
m.
This result sheds light on the analysis of the generalization lower bound by establishing the lower
bound on the expected hyperparameter divergence since it will induce a generalization lower bound if
there exists a positive real constant Lsuch that |ϵgen| ≥Lϵgen,arg. One possible situation is that the
designed compound validation loss satisfies for all z∈Z,|L(λ;z)− L(λ′;z)| ≥L∥λ−λ′∥. As
the generalization lower bound is beyond the main scope of this paper, further design and derivation
may be left for future research.
38NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our claims accurately match our theoretical results and reflect the paper’s
contribution and scope.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See Section 6.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
39Justification: All theoretical results include complete proofs with clearly stated assumptions
in the Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide a simulation code in the supplemental material. It is sufficient for
reproducing our experimental results.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
40Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide a simulation code in the supplemental material. It is sufficient for
reproducing our experimental results.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide a simulation code with training details in the supplemental material.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: The simulation experiments in our paper validate the theoretical results with
the trend of the curves, where randomness does not affect the validity.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
41•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: See Appendix F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in our paper conforms with the NeurIPS Code of
Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: See Section 6.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
42•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper does not release data or model.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: This paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
43•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing or research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing or research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
44