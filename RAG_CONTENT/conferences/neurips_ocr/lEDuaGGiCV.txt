Lucy: Think and Reason to Solve Text-to-SQL
Submission #14824
Abstract
Large Language Models ( LLMs) have made significant progress in assisting 1
users to query databases in natural language. While LLM-based techniques 2
provide state-of-the-art results on many standard benchmarks, their perfor- 3
mance significantly drops when applied to large enterprise databases. The 4
reason is that these databases have a large number of tables with complex 5
relationships that are challenging for LLMs to reason about. We analyze 6
challenges that LLMs face in these settings and propose a new solution that 7
combines the power of LLMs in understanding questions with automated 8
reasoningtechniquestohandlecomplexdatabaseconstraints. Basedonthese 9
ideas, we have developed a new framework that outperforms state-of-the-art 10
techniques in zero-shot text-to-SQL on complex benchmarks. 11
1 Introduction 12
Large Language Models ( LLMs) have significantly enhanced AI agents’ capacity to assist 13
humans in a variety of important tasks, including co-pilot programming [Chen et al., 2021, 14
GitHub, Inc., 2021], program verification [Wu et al., 2024, Chakraborty et al., 2023], and 15
math problem solving [Zhou et al., 2024]. One of the fastest-growing areas in this space 16
is the development of LLM-based assistants for querying SQL databases. In this task, a 17
user poses a question to a database in natural language. The agent’s goal is to generate an 18
SQL query that, when executed against the database, answers the user’s question. Such 19
assistance enables users with different levels of expertise to effectively analyze their data. 20
Recently, LLM-based solutions have made significant progress in addressing the text-to-SQL 21
problem [Gao et al., 2024, Li et al., 2024a]. While GPT-based methods have quickly reached 22
near-human performance on academic benchmarks, like Spider [Yu et al., 2018], they struggle 23
to provide high-quality user assistance on large industrial databases [Sequeda et al., 2023, Li 24
et al., 2023]. One of the core challenges is that industrial databases model many objects with 25
complex relationships between them. To transform a natural language question into an SQL 26
query, the LLMmust effectively reason about these intricate relationships, which is highly 27
non-trivial for LLMmodels. Interestingly, we found that gpt4can even indicate in some 28
cases that it needs help with logical reasoning on complex databases. Here is a common 29
gpt4output message on a question that requires multiple joins from ACME insurance 30
database [Sequeda et al., 2023]: ‘This join may need adjustment based on the actual logic of 31
relating claims to policy coverage details.’ . While we do provide the database schema as part 32
of the input, it is still challenging for LLMs to formally reason about database logic. 33
In this work, we propose a new text-to-SQL framework, Lucy, designed for large databases 34
with complex relationships between objects. Our main underlying idea is to combine the 35
ability of LLMmodels to effectively relate user questions to database objects with the power 36
of automated reasoning to analyze relationships between these objects. The Lucyworkflow 37
consists of three high-level steps. First, upon receiving a user’s question, we identify the 38
relevant objects and their attributes in the target database. In the second step, we employ 39
an automated reasoner to build a view that joins the relevant tables based on relational 40
constraints defined by the database schema. This view contains all the necessary information 41
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not
distribute.StarA
StarB SnowflakeRetentionClient
[id,name,loc_id]Location
[id]Datacenter
[id,name,loc_id]
Gift Bonus
Payment
[id]
PayAmount
[id,pay_id,amount ]
Tax
[payamt_id ]Supercharge
[payamt_id ]
Income
[payamt_id ]Compute ,
[id,dc_id]
ResourcePool
[id]
Config
[id,rspool_id ]Runtime
[id,rspool_id ]
cMemory
[config_id ,
overheadlimit ]cCPU
[config_id ,
overheadlimit ]rMemory
[runtime_id ,
overallusage ]rCPU
[runtime_id ,
overallusage ]m: mRsPool2Client
Figure 1: Objects and their relations in the database ddo.
to answer the user’s questions. In the third step, we construct a query targeting this view to 42
produce an answer for the user. Our contributions are summarized as follows: 43
•We propose a text-to-SQL framework Lucycapable of querying large industrial 44
databases. To the best of our knowledge, Lucyis the first framework designed to 45
support logical reasoning in the context of the text-to-SQL problem. 46
•Lucyoffers several advantages: 47
–alleviates the need for complex reasoning from a LLM, allowing it to focus on 48
tasks where it currently excels, 49
–supportsmodelingandreasoningaboutcomplex, commonlyuseddesignpatterns 50
to model relationships, like many-to-many, Star, and Snowflake , 51
–its modular workflow allows for effective debugging of failures, 52
–performs zero-shot generation and does not require fine-tuning of LLMs. 53
•Our experimental results demonstrate significant performance improvements on 54
several standard benchmarks as well as introduced large benchmarks. We also 55
demonstrate the debugging capabilities of Lucy. 56
2 Motivation 57
To provide high-quality user assistance in text-to-SQL tasks, we face two types of challenges. 58
The first type of challenge comes from the formulation of the user’s question. A question 59
can be poorly specified, ambiguous, or require additional knowledge that is not present in 60
the question. For example, the user might ask to list clients eligible for a loan; however, the 61
eligibility criteria are not present in the question [Li et al., 2023, 2024b]. The second class is 62
related to the complexity of the queried database that can have a large number of tables 63
with complex relations between them [Sequeda et al., 2023, Li et al., 2023]. In this work, we 64
focus on the second class. One approach to deal with complex relationships is to introduce 65
an intermediate layer, like a knowledge graph or ontology structure, that contains rich 66
information about the underlying database. Then, LLMs generate queries to this knowledge 67
graph using specialized languages, e.g., SPARQL, [Sequeda et al., 2023]. In turn, these 68
queries can be automatically translated to SQL. While this approach does show promise, it 69
does not alleviate the core issue: an LLM is still expected to reason about complex relations 70
between objects in this intermediate representation. Moreover, such a rich intermediate layer, 71
like an ontology, might not be easy to obtain for a database. Other standard techniques, 72
like additional training, multi-shot or fine-tuning, also rely on LLMs to perform constrained 73
reasoning steps [Gao et al., 2023, Pourreza and Rafiei, 2024, Gao et al., 2024]. To the best of 74
our knowledge, dealing with complex relationships in text-to-SQL remains an open problem. 75
In order to isolate the underlying challenges in this problem, we created an example database 76
2Q1: List customers who use datacenters with names
starting with ‘dev’. Output clients and datacenters
names.
/* GPT4 generated SQL */:
select Client .name ,Datacenter .name
from Client
join Location onLocation .id=Client .loc_id
join Datacenter onLocation .id=Datacenter .loc_id
where Datacenter .name like 'dev % '
/* Correct SQL */
select Client .name ,Datacenter .name
from Datacenter
join Compute onDatacenter .id=Compute .dc_id
join ResourcePool on
Compute .id=ResourcePool .compute_id
join RsPool2Client on
ResourcePool .id=RsPool2Client .rspool_id
join Client onClient .id=RsPool2Client .client_id
where Datacenter .name like 'dev % 'Q2: List resource pools names with CPU overhead
limit greater than runtime overall usage by 100.
/* GPT4 generated SQL */:
select ResourcePool .name
from ResourcePool
join rCPU on
ResourcePool .runtime_id =rCPU.runtime_id
join cCPU on
ResourcePool .config_id =cCPU.config_id
where cCPU.overheadlimit >rCPU.overallusage + 100
/* Correct SQL */:
select distinct ResourcePool .name
from ResourcePool
left join Config on
ResourcePool .id=Config .rspool_id
left join cCPU onConfig .id=cCPU.config_id
left join Runtime on
ResourcePool .id=Runtime .rspool_id
left join rCPU onRuntime .id=rCPU.runtime_id
where cCPU.overheadlimit >rCPU.overallusage + 100
Table 1: User’s questions Q1 and Q2. Incorrect parts of the GPTanswer are shown in red.
that covers standard relationship patterns adopted in industry and academia. We identified a 77
set of simple and clearly formulated questions and demonstrated that even on this simplified 78
schema and clear questions, state-of-the-art LLMs struggle to assist the user. 79
2.1 Database description 80
We describe a minimal example database schema that contains basic relations, like 1: 1 81
and 1: m, and more advanced relationship patterns, like m: mandStar, and analyze the 82
performance of LLMs on this schema (See Appendix A for relational database definitions). 83
Suppose a business sells cloud compute resources to customers and uses a database, ddo, 84
to manage its Day-to-Day Operations. Figure 1 shows objects’ corresponding tables, their 85
relationships, and a subset of attributes. In particular, each table has a primary key, e.g., 86
Location .id, and might have foreign keys to refer to another table, e.g., Client refers 87
toLocation using Client .loc_id. All attributes relevant to our examples are shown in 88
Figure 1 with self-explanatory names. ddomanages payments ( Payment ) and marketing 89
retention strategies ( Retention ) for clients ( Client) and resources ( ResourcePool ) 90
in datacenters ( Datacenter ). This example is in part inspired by the VMware vSphere 91
data model (discussed in Section 5). The full data model contains hundreds of types of 92
resources that form deep tree-like structures [Managed Object, 2024]. Next, we consider 93
how relationships between objects are modeled in ddo. Figure 1 already defines basic 94
relationships, including 1: 1(dotted edges) and 1: m(solid edges). 95
Many-to-many ( m: m).ClientandResourcePool are related via a m: mrelationship 96
(the dashed edge) meaning that a client might use multiple resource pools and one resource 97
pool can serve multiple clients. The table RsPool2Client models this relation. 98
Star.AStarpattern is a type of database schema composed of a single, central fact table 99
surrounded by dimension tables. There are two groups of objects connected in a Star 100
patterns in our example. StarA keeps track of retention marketing strategies for each 101
client that can be either Giftor/and Bonus.StarB records clients’ payments ( Payment ). 102
Payments’ amounts are stored in the PayAmount table. Each amount can be exactly one 103
of three types: Tax,Supercharge , and Income. 104
Snowflake. ASnowflake schema consists of one fact table connected to many dimension 105
tables, which can be connected to other dimension tables through a many-to-one relationship. 106
Inddo, database resource pools are modeled using the snowflake pattern. Each resource 107
pool has configurations ( Config) and snapshots of the current usage ( Runtime ).Config 108
andRuntime have two children nodes each to define CPU and memory properties. 109
Lookup. Alookup table is a table that contains descriptions and code values used by 110
multiple tables, e.g., zip codes, country names. etc. In ddo,Location is a lookup table 111
that stores geo-location related data for quick access. 112
3Question MatchTables GenerateView QueryViewdbModel
Relevant
tables (RT)Summary
view (V)V,Q
Figure 2: Lucy’s high-level workflow. Red colored boxes indicate phases performed by
LLMs, and a green colored box is a phase performed by an automated reasoner.
2.2 User questions 113
We consider three simple questions to ddothat are well formulated: outputs are explicitly 114
specified, so no additional information is needed to answer them. We use gpt4(‘gpt-4-0125- 115
preview’), and promptB [Sequeda et al., 2023] for these questions. For each question, we 116
present a ground truth answer and a GPTanswer. Table 1 presents both questions (Q3 is 117
presented in Appendix C.1). 118
Question Q1 is ‘List customers who use datacenters with names starting with ‘dev’. Output 119
clients and datacenters names’ . The user asks for information that relates clients and 120
datacenters. Consider GPT’s answer. GPTmisses the core logic of the database: clients 121
and datacenter resources are related via a m: mrelation (modeled with RsPool2Client ). 122
GPToutputs clients and datacenters that share the same location, which is incorrect. 123
Question Q2 is ‘List resource pool names with CPU overhead limit greater than runtime 124
overall usage by 100’ . Here the user asks about resource pool properties. However, the 125
GPTanswer ignores the database’s primary/foreign relations. It performs an inner 126
joinbetween ResourcePool ,cCPU, and rCPUtables, using non-existent attributes 127
ResourcePool .config_id andResourcePool .runtime_id , which is clearly incorrect. 128
In summary, these examples demonstrated that LLMs struggle to handle complex relation- 129
ships between objects. 130
3 Framework design 131
In this section, we present our framework Lucy. Figure 2 illustrates the workflow diagram, 132
and Algorithm 1 shows the main steps of the workflow. There are two inputs to the 133
framework. The first input is a user question Q. The second input is dbModel , which is a 134
description of the database schema that we discuss in the next section (Section 3.1). The 135
workflow consists of three sequential subtasks: MatchTables ,GenerateView , and QueryView . 136
MatchTables identifies the relevant tables and their attributes related to the user question 137
(Section 3.2). GenerateView finds a combined view of relevant tables taking into account 138
database constraints (Section 3.3). The third phase, QueryView , takesVand the user 139
question Qand produces an SQL query QforV(Section 3.4). To simplify notations, we 140
assume that dbModel is a global variable in Algorithm 1. 141
3.1 Database model ( dbModel ) 142
We start with dbModel , ordbmfor short. dbmis a data structure that contains aggregated 143
information about the database, maintained as a JSON structure. dbmshould be constructed 144
once for a database as the structure of the database is relatively stable. dbmcan always be 145
extended if the database requires modifications. Here are the two main blocks of dbm: 146
Database schema. The schema is written using the SQL Data Definition Language (CREATE 147
TABLE statements). It includes table names, names and types of columns in each table, 148
and database constraints such as primary and foreign keys. It can also contain optional user 149
comments associated with each table and column. We refer to tables and constraints as 150
dbm.tablesanddbm.constraints , respectively. We extract this information in the form of 151
JSON. Appendix D.1.1–D.1.2 shows examples of these structures. 152
4Patterns summary. The user can optionally list higher-level design patterns that are not 153
captured by the schema explicitly. This information can help to improve the accuracy of the 154
algorithm. We support m: m,Star,Snowflake , and lookup patterns, but the model is 155
extendable to support other patterns. The user identifies these patterns manually, based on 156
the logic of the target domain. In the future, we envision that the process can be partially 157
automated. Appendix D.1.3 shows the JSON format used to specify pattern structures. 158
Formal notations. We introduce formal notations. dbm.tablescontains a list of tables ti, 159
i∈[1, m]where mis the number of tables. dbm.constraints contains a set of pairs (ti, tj) 160
such that tiandtjare related via 1: 1, 1: mor m: 1relation. We denote dbm. m: mas a 161
set of triplets (ti, tj, tk), where a join table tkmodels a m: mrelation between tables ti 162
andtj. Note that (ti, tk)and(tj, tk)must be in dbm.constraints . Additionally, we denote 163
dbm.lookup as the set of lookup tables. For example, in the ddodatabase, dbm. m: m= 164
{(Client ,ResourcePool ,RsPool2Client )}anddbm.lookup ={Location}. For a 165
tree-like pattern, like StarorSnowflake , we distinguish between root table and inner 166
tables using two predicates, e.g., star_root (t) returns Trueiftis the root table of a Star 167
andstar_inner (t) returns Trueiftis an inner table (not root) of a Star. 168
3.2 The MatchTables phase 169
The first phase, MatchTables , needs to find relevant tables and their attributes to the user 170
question. One approach to achieve that can be to provide the schema and a question to an 171
LLMand ask for this information. However, one of the distinguishing features of real-world 172
databases is their large number of tables and attributes. Hence, feeding all of them along with 173
their descriptions to the prompt might not be feasible for many LLMmodels. Therefore, we 174
build an iterative procedure that takes advantage of database tree-like patterns. In general, 175
this procedure can be customized to best support the structure of a database. 176
Algorithm 1 Lucy
Require: User question Q, database model dbModel
Ensure: Summary viewV, SQL queryQ
1:Phase 1: MatchTables //LLM -based phase
2:// get core tables (these are tables that are not inner tables in StarorSnowflake )
3:core_tables ={t|t∈dbm.tables∧t /∈(snowflake_inner (t)∨star_inner (t))}
4:// identify relevant core tables to the user query
5:_,T=promptA (Q, core_tables, {})
6:RT={}
7:fort∈Tdo
8: ift∈snowflake_root (t)∨t∈star_root (t)then
9: // a breadth-first deepening to identify relevant tables and attributes inside a pattern rooted at t
10:RT=RT∪IterativePrompting (Q, t)
11: else
12:R′
T,_=promptA (Q,{}, t.attributes ),RT=RT∪R′
T// identify t’s relevant attributes
13:Phase 2: GenerateView // constraint reasoner-based phase
14:// formulate a constraint satisfaction problem
15:S=formulate_csp(RT)
16:// solve Sto find a path in Gthat satisfies constraints ( C1)–(C5)
17:P=solve_csp (S)
18:// build a viewVbase onPby joining tables along the path P.
19:V=build_view (P)
20:Phase 3: QueryView //LLM -based phase
21:Q=promptC (Q,V)
22:returnV,Q
Algorithm 1 shows MatchTables in lines 2–12. First, the algorithm focuses on tables that are 177
not inner tables of any patterns. We refer to such tables as core tables (core_tables in line 3). 178
For example, Figure 3 shows core tables for ddo. Next, we ask LLMto find relevant tables 179
among these core tables using promptA in line 5. (Appendix D.2.1 shows a promptA with 180
a few examples.) As a result, we obtain a set of relevant core tables. We explore them one 181
by one in the loop in line 7. If it is a root table of a pattern, we perform a search inside the 182
corresponding pattern to find more relevant tables using a breadth-first deepening procedure, 183
IterativePrompting , in line 10 (Algorithm 2 shows IterativePrompting ’s pseudocode 184
in Appendix D.2). Otherwise, we use promptA to obtain relevant attributes in line 12. 185
Example 3.1.Consider questions Q1andQ2from Table 1. Figure 3 shows ddo’s core 186
tables. For Q1, a LLMidentifies relevant core tables: T = { Client,Datacenter } 187
5Retention Client Location Datacenter
Payment RsPool2Client Compute ResourcePool...
... ...
Figure 3: A part of the abstract schema graph Gforddothat includes core tables.
(line 5). Since none of these tables is a root of a Snowflake or aStar, we prompt 188
for relevant attributes for each table in line 12 to get RT= {Client .name,Client .gender, 189
Datacenter .name}. Now consider Q2.LLMidentifies ResourcePool as a relevant 190
table in line 5. As ResourcePool is the root table of Snowflake (see Figure 1), we begin 191
to explore the pattern tree in a breadth-first order using IterativePrompting in line 10. 192
ResourcePool has two child nodes, Config andRuntime , and several attributes. We 193
query the LLMand find that both Config andRuntime are relevant as well as its attribute 194
ResourcePool .name. Following the breadth-first search order, we consider Config with 195
two descendants cCPUandcMemory and discover cCPUis relevant (Example D.4 in 196
Appendix shows a full version). 197
3.3 The GenerateView phase 198
The MatchTables phase identifies a set of relevant tables and their attributes. Next, we 199
construct a view table that combines relevant tables and attributes into a single table. 200
We build an abstract schema graph Gwhich provides a graph view of dbm, and define a 201
CSP over this graph. For each table tiindbm.tables, we introduce a node in G. We use 202
the names tito refer to the corresponding nodes. For each pair of tables tiandtj, s.t. 203
(ti, tj)∈dbm.constraints , we introduce an edge that connects them. We denote Vthe set of 204
nodes in GandEits edges. Figure 3 illustrates a part of the graph (core tables) for ddo. 205
Algorithm 1 shows three main steps of this phase: build an abstract graph representation G 206
of the schema (line 15); formulate and solve CSP to obtain a path P(line 17); and perform 207
joins along this path to obtain the designed view V(line 19). Next, we describe these steps. 208
Problem Formulation. LetT=tables (RT)be a set of relevant tables returned by 209
MatchTables . We formulate the problem of finding a path PinGthat visits a set of nodes 210
Tand satisfies a set of database constraints. 211
(C1)Pmust be a valid path in G. This ensures that we follow primary/foreign keys 212
relationships, i.e., 1: 1, 1: m, and build a valid view. 213
(C2)Pvisits all relevant tables T. This ensures combining all relevant tables to a view. 214
(C3)Consider (ti, tj, tk)∈dbm. m: m. Ifti∈Pandtj∈Pthentkmust occur inPonce 215
between tiandtj. These constraints enforce m: mrelationships. 216
(C4)Ift∈Pandt∈dbm.lookupthen t’s predecessor equals its successor in P. This 217
ensures that a lookup table serves as a look-up function for each table individually. 218
(C5)Cost function: we minimize the number of occurrences of tables outside of TinP. A 219
shorter path that focuses on the tables in Tallows us to build more succinct views. 220
(C1)–(C5) are common constraints that we encounter in the benchmark sets. In general, the 221
user can specify more constraints to capture the logical relationships of the modeled data. 222
Constraint satisfaction problem (CSP). We define a CSP formulation Sof constraints 223
(C1)–(C5). We start with a basic formulation. Let nbe the maximum length of the path P. 224
For each node tiinGand step r, where r∈[1, n], we introduce a Boolean variable br
i.br
iis 225
true iff tiis the rth node inP. We also introduce a sink-node Boolean variable br
dfor each 226
layer to model paths that are shorter than n.Scontains the following logical constraints: 227
(C5): minimize/summationtext
i,ti/∈Tocci (1)
∀i.ti∈V occi=b1
i+. . .+bn
i(2)
(C1):∀i.ti∈V, r∈[1, n−1] br
i⇒(∨j.(ti,tj)∈Ebr+1
j)∨br+1
d(3)
(C2): ∀i.ti∈T occi≥1(4)
6228
(C3):∀k.(ti, tj, tk)∈dbm. m: m occk= 1(5)
(C3):∀k.(ti, tj, tk)∈dbm. m: m, r∈[2, n−1]br
k⇒(br−1
i∧br+1
j)∨(br−1
j∧br+1
i)(6)
(C4):∀i.ti∈dbm.lookup , r∈[2, n−1] br
i⇒(br−1
j⇒br+1
j)(7)
∀r∈[1, n] br
1+. . .+br
|V|= 1(8)
∀r∈[1, n−1] br
d⇒br+1
d(9)
Consider the encoding S. Equations 2 specify integer variables, occi, fori∈[1, n], that count 229
the occurrences of each table in the path. Equations 8 encode that only one node belongs to 230
a path at each step. Equations 9 encode that if the path visits the sink node, then it must 231
stay there. Other equations encode constraints ( C1)–(C5). By construction, Equations 1–9 232
generate a valid path in Gthat satisfies the constraints ( C1)–(C5). 233
Example 3.2.For Q1, solving Sgives the green path between Datacenter andClient 234
in Figure 3. Srules out the red path as we enforce constraint ( C4) and optimization ( C5). 235
Improvements of CSP. Our basic model Scan be improved to take advantage of Star 236
andSnowflake patterns. Namely, we can leverage the decomposition of Gand find a path 237
Pamong core tables only. Then, for each core table in Pthat is a pattern root, and for each 238
inner relevant table in this pattern, we build a path P′along the corresponding branch. For 239
example, Figure 1 shows two paths from ResourcePool tocCPU(an orange path) and 240
rCPU(a blue path). We use left join to combine tables along each such branch. Finally, 241
we combinePandP’s into a single view. 242
Summary view. Given a pathPin a graph, we jointables along the path using their 243
primary and foreign key relations. We keep the same set of attributes that MatchTables 244
identified. An example of the Vfor Q1 that corresponds to the green path in Figure 3 is 245
shown in the listing in Table 7 in Appendix D.3.1. 246
3.4 The QueryView phase. 247
QueryView takes the summary view Valong with the user question, and prompts an LLM 248
to obtain the final SQL using promptC (line 21 in Algorithm 1). promptC is defined in 249
Appendix D.4.1. The listing in Table 7 shows an SQL Qto answer Q1 (Appendix D.3.1). 250
4 Discussion on strengths and limitations 251
Strengths. Lucyis designed based on the principle of separation of responsibilities between 252
generative tasks and automated reasoning tasks: each step focuses on either an NLP-related 253
subproblem or a constraint reasoning subproblem. This separation allows us to support a 254
number of unique capabilities. First, Lucyshifts the burden of complex reasoning from 255
LLMs to constraint solvers. Second, we support reasoning on complex relationships, like 256
m: m,lookup,StarorSnowflake . Third, our framework is flexible and extensible as 257
it is easy to incorporate domain-specific constraints as soon as they can be expressed by 258
constraint modeling language. This assumes that the user has a data analytics role and 259
understands the logic of the database. Such formal reasoning capability is important, as it is 260
hard to control LLMs via prompts when non-trivial reasoning is required. Fourth, we can 261
evaluate each phase and diagnose Lucyfailure modes. For example, if MatchTables misses 262
relevant tables, this indicates that we need to provide more information about the schema to 263
anLLM. Fifth, based on our evaluation, Lucycan support complex queries that include 264
multiple filtering operators and aggregators, e.g. average or sum. This capability follows 265
from the QueryView phase as the final call to an LLMis performed on a single view table. 266
Limitations. The first limitation is that we cannot guarantee that the SQL query answers 267
the user’s question. Given the current state of the art, providing such guarantees is beyond the 268
reach of any copilot method that takes natural language descriptions and outputs structured 269
text, like code or SQL. However, our solution does guarantee that Vsatisfies database 270
constraints, which is a step forward in this direction. Second, we do not support questions 271
7that require unionoperators in the GenerateView phase. In fact, there are no benchmarks 272
available that require the unionoperator to answer questions. Supporting unionwould 273
require an extension of MatchTables andGenerateView . Third, we observed experimentally 274
thatLucystruggles with certain types of queries that involve a particular interleaving 275
ordering of filtering and aggregate operators or question-specific table dependencies, like a 276
lookup table that has to be used multiple times to answer the user’s question. We further 277
discuss such questions in our experiments. 278
5 Experimental evaluation 279
In our experimental evaluation, we aim to answer the main questions: 280
•IsLucycompetitive with existing LLM-based approaches? 281
•Can we debug Lucyto gain insights about failure modes? 282
•CanLucyhandle complex questions? 283
Setup.We compare with the following zero-shot baselines: gpt4,nsql, and chat2query 284
(c2qfor short). gpt4andc2qmethods are the best zero-shot techniques according to the 285
BIRD leadership board that are accessible for evaluation [Li et al., 2024b]. nsqlis the best 286
open-source large foundation model designed specifically for the SQL generation task [Labs, 287
2023b]. chat2query is closed-source but the authors kindly extended their API that we 288
can run experiments with gpt4. We provide all benchmarks and frameworks’ results in the 289
supplementary materials. For gpt4andLucy, we use the ‘gpt-4-0125-preview’ API without 290
fine-tuning. We use OR-Tools as a constraint solver [Perron and Didier, 2024] (Appendix E.1 291
provides full details of the experimental setup). 292
Evaluation metrics. We use the standard Execution Accuracy ( ex) [Li et al., 2023]. In 293
addition, we consider a relaxation of this metric. We noticed that frameworks often add 294
additional attributes to the output as the exact format of the output is rarely specified. Hence, 295
we extend extoesxmetrics that check if the output of a framework contains the ground 296
truth outputs. To better understand performance characteristics and possible failure modes, 297
we consider the coverage metric that captures whether a framework correctly identified a 298
subset of relevant tables and attributes. Let sqlGbe the ground truth answer and sqlFbe a 299
generated query. Then we assess the percentage of the ground truth content slqFcaptures: 300
covt=|tables (slqF)∩tables (slqG)|
|tables (slqG)|cova=|attributes (slqF)∩attributes (slqG)|
|attributes (slqG)|,(10)
where tables () and attributes () are functions that return a set of tables and attributes. 301
Table 2: The ACME insurance dataset.
gpt4 gpt4ex c2q nsql Lucy dw
covt0.44 0.47 0.82 0.31 0.95 -
cova0.36 0.42 0.81 0.25 0.93 -
ex 9 13 16 2 3024
esx 9 13 16 3 33-Table 3: The Cloud Resources dataset.
gpt4 gpt4ex c2q Lucy
covt0.46 0.44 0.44 0.98
cova0.50 0.44 0.48 0.98
ex 6 4 2 17
esx 9 5 2 18
ACME insurance. We consider the ACME insurance dataset that was recently pub- 302
lished [Sequeda et al., 2023]. The dataset represents an enterprise relational database schema 303
in the insurance domain. The authors focused on a subset of 13 tables out of 200 tables and 304
proposed a set of 45 challenging questions. We identified two Starpatterns in this database. 305
The authors showed that their method ( dw) solved 24 out of 45 problems using intermediate 306
representation of a knowledge graph, while gpt4solved only 8 problems. However, results 307
are not publicly available, so we cannot perform coverage analysis and compute esx. 308
We reran the experiment on gpt4with the same promptB (Appendix C.1.1) and obtained 309
similar results to those reported in [Sequeda et al., 2023]. In addition, we extended the 310
schema with descriptions of table attributes from dbModel in the form of comments, which 311
we called gpt4ex (See Appendix E.2 for examples). Table 2 shows our results. First, we 312
observe that there is a strong correlation between coverage and accuracy metrics in the 313
results. c2qandLucyshow good coverage, meaning that they can correctly identify most of 314
8the required tables and attributes. They also demonstrate better performance compared to 315
other methods. Our framework shows very high coverage and solves about 30 of benchmarks 316
according to the exmetric, which outperforms dwthat solves 24 and other methods. 317
Lucystill cannot solve 13 benchmarks, which is surprising given high coverage. We 318
performed a study to locate where Lucyfails on these benchmarks (See Appendix E.2.1 for 319
all questions where Lucywas unsuccessful). In summary, the majority of failures come from 320
under-specified output attributes or nonstandard aggregators, like specialized formulas to 321
compute an average. In four cases, MatchTables missed a table, and in one case, QueryView 322
missed the attribute to output. The most interesting mode of failure is when we need to 323
perform multiple lookups on the same table. The reason for that is the MatchTables phase 324
identifies only relevant tables but ignores possible relationships between them. Extending 325
MatchTables to retrieve relationships between tables is interesting future work. 326
BIRD datasets. Next, we consider the state-of-the-art dataset BIRD [Li et al., 2023]. 327
From the development set, we chose two datasets with complex relationships between objects: 328
financial (106 instances) and formula 1(174 instances)1. The accuracy of chat2query 329
on the BIRD development set is ∼58%; however, its accuracy on financial andformula 1 330
are much lower,∼45%. We compare with results from gpt4’23 andc2qavailable from [Al- 331
ibabaResearch, 2020] and [TiDBCloud, 2020a], respectively. However, we reran these 332
benchmarks with gpt4andgpt4ex as the gpt4’23 results are nearly one year old. Table 5 333
and Table 4 show results on financial andformula 1, respectively. Lucyandc2qhave 334
higher coverage and good accuracy. Lucyshows the best results in most cases. Again, 335
Lucyhas very good coverage on financial but was able to solve only 68 out of 106 queries 336
based on the esxmetric. We manually performed an questions study on the failed questions. 337
There are two major groups there that are interesting. First, Lucyhas difficulty if there are 338
multiple orderings, especially nested or ordering in different directions. Second, sometimes, 339
MatchTables adds an additional table that is not needed to find the answer. The rest are 340
either ambiguous questions or small mistakes like outputting a wrong attribute, i.e., id 341
instead of name. See Appendix E.3.3 for examples of questions where Lucywas unsuccessful. 342
Table 4: The formula 1dataset.
gpt4’23 gpt4 gpt4ex c2q nsql Lucy
covt0.86 0.78 0.77 0.88 0.52 0.93
cova0.84 0.75 0.75 0.81 0.50 0.94
ex 54 67 65 80 9 83
esx 66 80 79 93 10 103Table 5: The financial dataset.
gpt4’23 gpt4 gpt4ex c2q nsql Lucy
covt0.81 0.84 0.87 0.92 0.50 0.97
cova0.81 0.81 0.85 0.91 0.59 0.96
ex 36 47 52 596 56
esx 38 55 64 62 6 68
343
Cloud resources. Next, we propose a new benchmark based on the vSphere API data 344
model [VMware, Inc., 2024]. We experimented with this publicly available data model of 345
an industrial product, as it is well-documented and easily accessible via a web interface. It 346
describes the state of the system as well as its configuration parameters. States are stored in 347
a database and queried by customers to keep track of performance, maintenance, and data 348
analysis. We extracted the descriptions of main objects in Managed Object [2024], including 349
data centers, resource pools, hosts, and virtual machines and their properties, and built a 350
database that captures these relationships using 52 tables. Overall, we have two Stars, five 351
Snowflake s and two m: ms patterns. For each table and an attribute, we get descriptions 352
from [Managed Object, 2024]. As these can be a lengthy description, we use GPTto shorten 353
it to 15 words (see promptD in Appendix E.3.2) . We generated data randomly using 354
sqlfaker [Kohlegger, 2020]. We create 20 challenging questions for this benchmark. 355
Table 3 shows our results. nsqlcannot process this benchmark due to a limited context 356
window. We again see that Lucyoutperforms other models in both coverage and accuracy. 357
c2qfailed on 6 questions with an error ‘Unable to generate SQL for this database due to its 358
extensive tables’ and it often does not follow instructions on the output columns. In terms of 359
failure mode, Lucyfailed in the third phase as it hallucinated some attribute names when 360
names are long, e.g., ‘Resourcepoolruntimemory’ instead of ‘Resourcepoolruntimememory’. 361
1Recently, Wretblad et al. [2024b] provided a detailed analysis of the BIRD dataset and found a
number of errors of various types. See Appendix E.3 for the discussion.
9References 362
AlibabaResearch. BIRD-SQL: A BIg Bench for Large-Scale Relational Database Grounded 363
Text-to-SQLs. https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird , 364
2020. Accessed: May 2024. 365
A. Beaulieu. Learning SQL . O’Reilly Media, Inc., 2nd edition, 2009. ISBN 9780596520830. 366
S. Chakraborty, S. Lahiri, S. Fakhoury, M. Musuvathi, A. Lal, A. Rastogi, 367
N. Swamy, and R. Sharma. Ranking llm-generated loop invariants for pro- 368
gram verification. In 2023 Empirical Methods in Natural Language Processing , 369
December 2023. URL https://www.microsoft.com/en-us/research/publication/ 370
ranking-llm-generated-loop-invariants-for-program-verification/ . EMNLP- 371
Findings 2023. 372
S. Chang and E. Fosler-Lussier. How to prompt llms for text-to-sql: A study in zero-shot, 373
single-domain, and cross-domain settings. arXiv preprint arXiv:2305.11853 , 2023. 374
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, 375
Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, 376
G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, 377
M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, 378
E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, 379
I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, 380
V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, 381
P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. 382
Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL 383
https://arxiv.org/abs/2107.03374 . 384
datadotworld, Inc. data instances. https://github.com/datadotworld/ 385
cwd-benchmark-data/issues/3 , 2024. Accessed: May 2024. 386
X. Dong, C. Zhang, Y. Ge, Y. Mao, Y. Gao, lu Chen, J. Lin, and D. Lou. C3: Zero-shot 387
text-to-sql with chatgpt, 2023. 388
D. Gao, H. Wang, Y. Li, X. Sun, Y. Qian, B. Ding, and J. Zhou. Text-to-sql empowered by 389
large language models: A benchmark evaluation, 2023. 390
D. Gao, H. Wang, Y. Li, X. Sun, Y. Qian, B. Ding, and J. Zhou. Text-to-sql empowered by 391
large language models: A benchmark evaluation. Proc. VLDB Endow. , 17(5):1132–1145, 392
2024. URL https://www.vldb.org/pvldb/vol17/p1132-gao.pdf . 393
GitHub, Inc. GitHub Copilot. https://copilot.github.com/ , 2021. Accessed: May 2024. 394
B. Hui. Panel of BIRD Annotation Issues. https://github.com/AlibabaResearch/ 395
DAMO-ConvAI/issues/39 , 2024. Accessed: May 2024. 396
IBM, Inc. Dimensional schemas. https://www.ibm.com/docs/en/ida/9.1.2?topic= 397
design-dimensional-schemas , 2021. Accessed: May 2024. 398
M. Kohlegger. sqlfaker. https://pypi.org/project/sqlfaker/#description , 2020. Ac- 399
cessed: May 2024. 400
N. S. Labs. Nstext2sql: An open source text-to-sql dataset for foundation model training, July 401
2023a. URL https://github.com/NumbersStationAI/NSQL?tab=readme-ov-file . 402
N. S. Labs. Nstext2sql: An open source text-to-sql dataset for foundation model training, 403
July 2023b. URL https://github.com/NumbersStationAI/NSQL . 404
H. Li, J. Zhang, H. Liu, J. Fan, X. Zhang, J. Zhu, R. Wei, H. Pan, C. Li, and H. Chen. 405
Codes: Towards building open-source language models for text-to-sql, 2024a. 406
10J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao, R. Geng, N. Huo, 407
X. Zhou, C. Ma, G. Li, K. C. Chang, F. Huang, R. Cheng, and Y. Li. Can LLM 408
already serve as A database interface? A big bench for large-scale database grounded 409
text-to-sqls. CoRR, abs/2305.03111, 2023. doi: 10.48550/ARXIV.2305.03111. URL 410
https://doi.org/10.48550/arXiv.2305.03111 . 411
J.Li, B.Hui, G.Qu, B.Li, J.Yang, B.Li, B.Wang, B.Qin, R.Cao, R.Geng, N.Huo, X.Zhou, 412
C. Ma, G. Li, K. C. Chang, F. Huang, R. Cheng, and Y. Li. BIRD_SQL: Leaderboard 413
- Execution Accuracy (EX). https://bird-bench.github.io/ , 2024b. Accessed: May 414
2024. 415
A. Liu, X. Hu, L. Wen, and P. S. Yu. A comprehensive evaluation of chatgpt’s zero-shot 416
text-to-sql capability, 2023. 417
Managed Object. Managed Object Types Overview, 418
VMware, Inc. https://vdc-download.vmware.com/ 419
vmwb-repository/dcr-public/c476b64b-c93c-4b21-9d76-be14da0148f9/ 420
04ca12ad-59b9-4e1c-8232-fd3d4276e52c/SDK/vsphere-ws/docs/ReferenceGuide/ 421
index.html , 2024. Accessed: May 2024. 422
L. Perron and F. Didier. Cp-sat, 2024. URL https://developers.google.com/ 423
optimization/cp/cp_solver/ . 424
M. Pourreza and D. Rafiei. Dts-sql: Decomposed text-to-sql with small large language 425
models, 2024. 426
F. Rossi, P. van Beek, and T. Walsh, editors. Handbook of Constraint Programming , volume 2 427
ofFoundations of Artificial Intelligence . Elsevier, 2006. ISBN 978-0-444-52726-4. URL 428
https://www.sciencedirect.com/science/bookseries/15746526/2 . 429
J. Sequeda, D. Allemang, and B. Jacob. A benchmark to understand the role of knowledge 430
graphs on large language model’s accuracy for question answering on enterprise sql 431
databases, 2023. 432
L. Silverston, W. H. Inmon, and K. Graziano. The data model resource book: a library of 433
logical data models and data warehouse designs . John Wiley & Sons, Inc., USA, 1997. 434
ISBN 0471153648. 435
TiDBCloud. chat2query bench. https://github.com/tidbcloud/chat2query_bench , 436
2020a. Accessed: May 2024. 437
TiDBCloud. Get Started with Chat2Query API. https://docs.pingcap.com/tidbcloud/ 438
use-chat2query-api , 2020b. Accessed: May 2024. 439
VMware, Inc. VMware vSphere API Reference Documentation. 440
https://vdc-download.vmware.com/vmwb-repository/dcr-public/ 441
c476b64b-c93c-4b21-9d76-be14da0148f9/04ca12ad-59b9-4e1c-8232-fd3d4276e52c/ 442
SDK/vsphere-ws/docs/ReferenceGuide/index.html , 2024. Accessed: May 2024. 443
N. Wretblad, F. G. Riseby, R. Biswas, A. Ahmadi, and O. Holmström. Finan- 444
cial annotations, March 2024a. URL https://github.com/niklaswretblad/ 445
the-effects-of-noise-in-text-to-SQL/blob/main/annotations/financial_ 446
annotations.xlsx . 447
N. Wretblad, F. G. Riseby, R. Biswas, A. Ahmadi, and O. Holmström. Understanding the 448
effects of noise in text-to-sql: An examination of the bird-bench benchmark, 2024b. 449
H. Wu, C. Barrett, and N. Narodytska. Lemur: Integrating large language models in 450
automated program verification. In The Twelfth International Conference on Learning 451
Representations , 2024. URL https://openreview.net/forum?id=Q3YaCghZNt . 452
11T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman, 453
Z.Zhang, andD.Radev. Spider: Alarge-scalehuman-labeleddatasetforcomplexandcross- 454
domain semantic parsing and text-to-SQL task. In E. Riloff, D. Chiang, J. Hockenmaier, 455
and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural 456
Language Processing , pages 3911–3921, Brussels, Belgium, Oct.-Nov. 2018. Association for 457
Computational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology. 458
org/D18-1425 . 459
A. Zhou, K. Wang, Z. Lu, W. Shi, S. Luo, Z. Qin, S. Lu, A. Jia, L. Song, M. Zhan, and H. Li. 460
Solving challenging math word problems using GPT-4 code interpreter with code-based 461
self-verification. In The Twelfth International Conference on Learning Representations , 462
2024. URL https://openreview.net/forum?id=c8McWs4Av0 . 463
12A Background 464
Relational databases. LetD1, . . . , D nbe a set of domains. A relation or table, t, is 465
defined over subset of domains: t(Xi0, . . . , X ik)⊆Di0×. . .×Dik,Xij⊆Dij, j∈[0, k]. In 466
addition, tdefines a set of attributes (or columns) names Xi0, . . . , X ik. Projection is a unary 467
operation on a set of attribute names Y,Y⊆X. The result of such projection is the set 468
of tuples that is obtained when all tuples in tare restricted to attributes Y. Inner join, or 469
simply join, is a binary operator between two tables t1andt2over their common attributes 470
Ythat returns a set of all combinations of tuples in t1andt2that are equal on Y. Left 471
join, left join , is similar to the join but returns all rows of t1filling unmatched rows of 472
t2with null values. A database can support a large set of constraints over tables. The two 473
main constraint types are related to primary and foreign keys. A primary key is the smallest 474
subset of attributes guaranteed to uniquely differentiate each tuple in a table. A foreign key 475
is a subset of attributes Yin a table t1that corresponds with (usually) a primary key of 476
another table t2, with the property that the projection of t1onYis a subset of the projection 477
oft2onY[Beaulieu, 2009]. 478
Design patterns. A database typically represents entities and their interactions in real- 479
world processes, e.g., the financial management of a company. To effectively model these 480
complex entities, several design patterns have been developed [IBM, Inc., 2021, Silverston 481
et al., 1997]. A many-to-one pattern ( m: 1) specifies a relationship when any number of 482
attributes from one table is associated with unique attributes of the same or another table, 483
typically enforced by foreign key and primary key relationships. A many-to-many relationship 484
( m: m) occurs when any number of attributes from one table is associated with any number 485
of attributes from the same or another table. It is typically modeled with an auxiliary join 486
table that refers to the primary keys of the tables in the relationship. The lookup table is 487
a table that contains descriptions and code values used by multiple tables, e.g., zip codes, 488
country names. etc. A Starpattern is a type of relational database schema composed of a 489
single, central fact table surrounded by dimension tables. A Snowflake schema consists 490
of one fact table connected to many dimension tables, which can be connected to other 491
dimension tables through a many-to-one relationship. 492
Constraint satisfaction. A constraint satisfaction problem (CSP) consists of a set of 493
variables, each with a finite domain of values, and a set of constraints specifying allowed 494
combinations of values for subsets of variables [Rossi et al., 2006]. A solution is an assignment 495
of values to the variables satisfying the constraints. In the constraint optimization problem, 496
we are looking for a solution that optimizes a given cost function. Constraint solvers typically 497
explore partial assignments enforcing a local consistency property using either specialized 498
or general-purpose propagation algorithms and employ conflict-driven learning to store 499
information from failures as the search proceeds. We used OR-Tools CP-SAT solver [Perron 500
and Didier, 2024] in our experiments. 501
B Related work 502
We focus on the zero-shot text-to-SQL problem, which has received significant attention 503
in the last few years. Liu et al. [2023] performed a comprehensive evaluation of ChatGPT 504
on the Spider dataset and demonstrated that it shows good performance. In [Dong et al., 505
2023], a new framework based on the GPT model was proposed, involving several techniques 506
for promoting and post-processing the output to get more consistent results. Chang and 507
Fosler-Lussier [2023] proposed several techniques to improve the performance of ChatGPT. 508
[TiDBCloud, 2020b] represents the most recent zero-shot method. According to the API 509
documentation [TiDBCloud, 2020b], the authors construct a data summary object that 510
contains ‘AI exploration information of the given database.’ This method performs very well 511
on the BIRD dataset. However, it relies on LLMs to reason about database relationships. 512
Sequeda et al. [2023] performed an interesting investigation of the performance of LLMs on 513
large industrial databases. They identified that GPTdoes not perform well when it needs 514
to reason about complex relationships. The authors proposed a two-step approach to tackle 515
this problem. As a knowledge graph is available for these benchmarks, the authors proposed 516
13Q3: What are the total tax payment, which is the sum of Tax and Supercharge?
/* GPT4 generated SQL */: select sum ( Tax .amount +Supercharge .amount )
from Tax
join PayAmount onPayAmount .id=Tax .payamt_id
join Supercharge onPayAmount .id=Supercharge .payamt_id
/* Correct SQL */:
select sum( ifnull ( payTax .amount , 0)) + sum ( ifnull ( paySupercharge .amount ,0))
from Payment
join PayAmount aspayTax onPayment .id=payTax .pay_id
left join Tax onpayTax .id=Tax .payamt_id
join PayAmount aspaySupercharge onPayment .id=paySupercharge .pay_id
left join Supercharge onpaySupercharge .id=Supercharge .payamt_id
Table 6: A user’s question Q3. Incorrect parts of the GPTanswer are highlighted in red.
using the knowledge graph as an intermediate representation. Namely, the user’s question is 517
answered using the KGstructure with SPARQL, and this answer is automatically translated 518
to SQL using a given mapping from ontology to SQL (R2RML). However, while reasoning 519
on a knowledge graph can be easier for LLMs, it is still challenging to take all complex 520
relationships into account. 521
C Motivation (additional materials) 522
C.1 User’s questions 523
The third question Q3, ‘What are the total tax payments, which is the sum of Tax and 524
Supercharge?’ , asks about the total amount of taxes paid from all payments (Table 6). There 525
are a few issues with the GPTanswer. First, it outputs all payment amounts that are both 526
tax and supercharge. We reminded that that each payment amount can be of one type, so 527
the result will be empty. Second, it hallucinates as there are no amountcolumns in the Tax 528
orSupercharge tables. 529
C.1.1 Definition of promptB 530
Inputs: DB_SCHEMA, Question
promptB Given the database described by the following DDL: <DB_SCHEMA>.
Write a SQL query that answers the following question. Do not explain the query.
Return just the query, so it can be run verbatim from your response. Here’s the
question: <Question>. [Sequeda et al., 2023]
Returns : SQL
531
D Framework design (additional materials) 532
D.1 Database model ( dbModel ) 533
D.1.1 Example of a table from dbm.tables 534
Here is a JSON structure for the Client table from the financial dataset [Li et al., 2023]. 535
It contains the table name, primary keys, attributes, their types, and descriptions. This 536
information is available in the dataset. The description of the table is generated by gpt4 537
using the prompt promptD . 538
539
1" Client ": { 540
2 " type ": " ManagedObject ", 541
3 " primary ": [ 542
4 " client_id " 543
5 ], 544
146 " path ": "<path -to >/ Client . json ", 545
7 " path_to_types ": ""< path -to >/ Client_types . json " 546
8} 547548
Here is the JSON structure for Client.json: 549
550
1{ 551
2 " NameField ": " Client ", 552
3 " DescriptionField ": " Focuses on client information , 553
encompassing unique client identifiers , gender , birth 554
dates , and the location of the branch with which they 555
are associated .", 556
4 " client_id ": "the unique number ", 557
5 " gender ": " Description : 'F: female ; M: male '", 558
6 " birth_date ": " birth date ", 559
7 " district_id ": " location of branch " 560
8} 561562
Here is the JSON structure for Client_types.json: 563
564
1{ 565
2 " NameField ": { 566
3 " type ": " varchar (100)", 567
4 " default ": " DEFAULT NULL " 568
5 }, 569
6 " DescriptionField ": { 570
7 " type ": " varchar (5000)", 571
8 " default ": " DEFAULT NULL " 572
9 }, 573
10 " client_id ": { 574
11 " type ": " bigint ", 575
12 " default ": "NOT NULL " 576
13 }, 577
14 " gender ": { 578
15 " type ": " varchar (46)", 579
16 " default ": "NOT NULL " 580
17 }, 581
18 " birth_date ": { 582
19 " type ": " date ", 583
20 " default ": "NOT NULL " 584
21 }, 585
22 " district_id ": { 586
23 " type ": " bigint ", 587
24 " default ": "NOT NULL " 588
25 } 589
26} 590591
D.1.2 Example of a m: 1relation from dbm.constraints 592
Here is the JSON structure for the Client and District relation from the financial dataset [Li 593
et al., 2023]. 594
595
1 " Client , District ": { 596
2 " type ": " Relationships ", 597
3 " sqlrelation ": "M:1", 598
4 " foreign_relation ": { 599
5 " FOREIGN ": [ 600
6 " district_id " 601
7 ], 602
158 " foreign_relation_ref_table ": " District ", 603
9 " foreign_relation_ref_table_keys ": [ 604
10 " district_id " 605
11 ] 606
12 } 607
13 } 608609
D.1.3 Example of a m: mpattern from dbm.patterns 610
Here is the JSON structure for the Account and District m: mrelation from the financial 611
dataset [Li et al., 2023]. 612
613
1{ 614
2 " Account , Client ": { 615
3 " type ": " Relationships ", 616
4 " description ": "", 617
5 " sqlrelation ": "M:M", 618
6 "m2 m_relation ": { 619
7 "m2 m_middle_table ": " Disp ", 620
8 "m2 m_side_tables ": [ 621
9 " Client ", 622
10 " Account " 623
11 ], 624
12 "m2 m_relation_one ": [ 625
13 " Disp ", 626
14 " Client " 627
15 ], 628
16 "m2 m_relation_two ": [ 629
17 " Disp ", 630
18 " Account " 631
19 ] 632
20 } 633
21 } 634
22} 635636
Here is the JSON structure for the Snowflake pattern rooted ta ResourcePool (Cloud 637
Resources dataset). 638
639
1{ 640
2 " NameField ": " ResourcePool ", 641
3 " config ": { 642
4 " cpualloc ", 643
5 " memalloc " 644
6 }, 645
7 " runtime ": { 646
8 "cpu", 647
9 " memory " 648
10 } 649
11} 650651
D.2 MatchTables 652
D.2.1promptA 653
promptA requires three inputs: a user question, a set of tables (can be empty), and a set of 654
attributes for a given table t(can be empty). 655
16Inputs: Question, Tables, Attributes
promptA: Here is a json schema. Please treat json schema objects as a description
of tables in a database <JSON(Tables, Attributes)>. The user has a query to answer
<Question>. What are all relevant json elements to a user query from the list [<list
of json elements>]? Output is a list of elements, [element, element, element,...]. Do
not explain.
Returns: We post-process the output to extract a set of tables and their attributes
(RT) and relevant tables T
656
In the prompt, we provide description of tables and attributes from dbm. We show a few 657
examples of <JSON(Tables, Attributes)> and the corresponding <list of json elements>. 658
Example D.1.Here is an example of a JSON(Tables, {}) used in line 5 in Algorithm 1 for 659
financial dataset. The goal is to determine relevant core tables. 660
661
1{ 662
2 " Account ": " Manages financial accounts , tracking each 663
account 's unique identification , the location of the 664
associated bank branch , the frequency of account 665
servicing , and the account 's creation date . It 666
categorizes the servicing frequency with options like 667
monthly , weekly , and post - transaction issuances 668
Properties of Account : account_id , district_id , 669
frequency , date . ", 670
3 " Card ": " Manages of credit cards , incorporating unique 671
identifiers for each card and the related 672
dispositions . It also categorizes credit cards into 673
various classes , such as junior , standard , and 674
high - level , reflecting their tier and associated 675
benefits . Properties of Card : card_id , disp_id , type , 676
issued . ", 677
4 " Client ": " Focuses on client information , encompassing 678
unique client identifiers , gender , birth dates , and 679
the location of the branch with which they are 680
associated . Properties of Client : client_id , gender , 681
birth_date , district_id . ", 682
5 " Disp ": " Manage dispositions in financial accounts . It 683
contains a unique identifier for each record , links 684
each disposition to specific clients and accounts , 685
and categorizes the nature of each disposition into 686
types like 'OWNER ','USER ', or 'DISPONENT '. 687
Properties of Disp : disp_id , client_id , account_id , 688
type . ", 689
6 " District ": " Provides a detailed overview of 690
district - level data , essential for regional analysis 691
and decision - making . It includes a unique identifier 692
for each district , along with the district 's name and 693
its broader region . The table delves into 694
demographic , economic data and economic indicators , 695
records crime statistics . Properties of District : 696
district_id , A2, A3, A4, A5, A6, A7, A8, A9, A10, 697
A11, A12, A13, A14, A15, A16. ", 698
7 " Loan ": " Manages loan - related data , offering insights 699
into each loan 's unique identifier , associated 700
account details , approval dates , amounts , durations , 701
and monthly payments . Properties of Loan : loan_id , 702
account_id , date , amount , duration , payments , status . 703
", 704
178 " Order_ ": " Manages payment orders , detailing unique 705
identifiers for each order , linked account numbers , 706
and recipient bank details . It captures the bank and 707
account number , the debited amount for each order and 708
categorizes the purpose of each payment . Properties 709
of Order_ : order_id , account_id , bank_to , account_to , 710
amount , k_symbol . ", 711
9 " Trans ": " Includes transaction management , encompassing 712
details such as transaction identifiers , associated 713
account numbers , and dates of transactions , 714
categorizes transactions , covering a range of 715
activities from insurance payments and statement fees 716
to interest credits , sanctions for negative balances , 717
household payments , pension disbursements , and loan 718
payments ; and details about the transaction partner 's 719
bank , identified by a unique two - letter code , and 720
their account number . Properties of Trans : trans_id , 721
account_id , date , type , operation , amount , balance , 722
k_symbol , bank , account . " 723
10} 724725
The list of JSON elements is as follows 726
727
1['Account ','Card ','Client ','Disp ','District ','Loan ', 728
'Order_ ','Trans '] 729730
Example D.2.Here is an example of a JSON({}, Attributes) used in line 11 in Algorithm 1 731
for the table District to determine relevant attributes (from the financial dataset). 732
733
1{ 734
2 " DescriptionField ": " Provides a detailed overview of 735
district - level data , essential for regional analysis 736
and decision - making . It includes a unique identifier 737
for each district , along with the district 's name and 738
its broader region . The table delves into 739
demographic , economic data and economic indicators , 740
records crime statistics .", 741
3 " district_id ": " location of branch ", 742
4 "A2": " district_name ", 743
5 "A3": " region ", 744
6 "A4": "", 745
7 "A5": " municipality < district < region ", 746
8 "A6": " municipality < district < region ", 747
9 "A7": " municipality < district < region ", 748
10 "A8": " municipality < district < region ", 749
11 "A9": " Description : not useful ", 750
12 "A10": " ratio of urban inhabitants ", 751
13 "A11": " average salary ", 752
14 "A12": " unemployment rate 1995", 753
15 "A13": " unemployment rate 1996", 754
16 "A14": "no. of entrepreneurs per 1000 inhabitants ", 755
17 "A15": "no. of committed crimes 1995", 756
18 "A16": "no. of committed crimes 1996" 757
19} 758759
The list of json elements is as follows 760
761
1[ district_id , A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, 762
A13, A14, A15, A16] 763764
18Moreover, if a table is a root table of a pattern, we provide inner tables and their attribute 765
names so that an LLMcan determine the relevance of Snowflake to the user question. 766
Example D.3.Here is an example of the Snowflake summary rooted at ResourcePool 767
from the Cloud Resources benchmark. 768
769
1" ResourcePool ": " Resource pools manage VM resources within a 770
hierarchy , ensuring efficient allocation through 771
configurable settings and states . Properties of 772
ResourcePool : namespace , name , owner , summary , config , 773
config , config . changeVersion , config . entity , 774
config . lastModified , config . scaleDescendantsShares , 775
config . cpualloc , config . cpualloc , 776
config . cpualloc . expandableReservation , 777
config . cpualloc . limit_ , config . cpualloc . overheadLimit , 778
config . cpualloc . reservation , config . cpualloc . shares , 779
config . cpualloc , config . memalloc , config . memalloc , 780
config . memalloc . expandableReservation , 781
config . memalloc . limit_ , config . memalloc . overheadLimit , 782
config . memalloc . reservation , config . memalloc . shares , 783
config . memalloc , config , runtime , runtime , 784
runtime . overallStatus , runtime . sharesScalable , 785
runtime .cpu , runtime .cpu, runtime .cpu. maxUsage , 786
runtime .cpu . overallUsage , runtime .cpu. reservationUsed , 787
runtime .cpu . reservationUsedForVm , 788
runtime .cpu . unreservedForPool , 789
runtime .cpu . unreservedForVm , runtime .cpu, runtime . memory , 790
runtime . memory , runtime . memory . maxUsage , 791
runtime . memory . overallUsage , 792
runtime . memory . reservationUsed , 793
runtime . memory . reservationUsedForVm , 794
runtime . memory . unreservedForPool , 795
runtime . memory . unreservedForVm , runtime . memory , runtime , 796
ResourcePool_id . " 797798
D.2.2 Description of the IterativePrompting algorithm. 799
Algorithm 2 IterativePrompting
Require: Q, t
Ensure: Relevant tables and attributes in a tree-like pattern rooted at t
1:stack_tables = [t]
2:RT={}
3:while stack_tables do
4: r=stack_tables.pop ()
5:// check if ris a leaf in a tree-like pattern
6: ifleaf(r)then
7:R′
T,_=promptA (Q,{}, r.attributes )
8: else
9: // find children of rin a tree-like pattern
10: children_tables ={t|t∈dbm.tables∩children (r)}// children(r) returns descendants of rin the pattern.
11:R′
T, T=promptA (Q,children_tables , r)
12: stack_tables.push (T)
13:RT=RT∪R′
T
Example D.4(Full version of Example 3.1 for the question Q2) .Consider Q2from Table 1. 800
Figure 3 shows ddo’s core tables. LLMidentifies ResourcePool as a relevant table 801
in line 5, along with its attribute ResourcePool .name. Since ResourcePool is the 802
root table of a Snowflake pattern, we begin to explore the pattern tree in a breadth-first 803
order using IterativePrompting in line 10. See Figure 1 for the structure of the the 804
Snowflake pattern. ResourcePool has two child nodes, Config andRuntime , and 805
several attributes. We then query an LLMand find that both Config andRuntime are 806
relevant as well its attribute ResourcePool .name. Following the breadth-first search order, 807
19/* -- Summary view V--*/
create view Vas select
Client .idasClient _id,
Client .name asClient _name ,
Client .gender asClient _gender ,
Datacenter .name asDatacenter _name ,
Datacenter .idasDatacenter _id
from Datacenter
join Compute onDatacenter .id=Compute .dc_id
join ResourcePool onCompute .id=ResourcePool .compute_id
join RsPool2Client onResourcePool .id=RsPool2Client .rspool_id
join Client onClient .id=RsPool2Client .client_id
/* --Final query Q--*/
select Client _name ,
Datacenter _name
from Vwhere Datacenter _id> 1;
Table 7: GenerateView andQueryView results for Q1.
we next consider Config which has descendants cCPUandcMemory and a few attributes. 808
We discover that only one of them, cCPU, is relevant. We then move to the next table in 809
order, Runtime . It has two descendants rCPUandrMemory and a few attributes. We 810
discover that only one of them, rCPU, is relevant. Next, we identify relevant attributes of 811
cCPUin line 7 (Algorithm 2) and find that cCPU.overheadlimit is relevant to the user 812
query. Finally, we identify relevant attributes of rCPUin line 7 in (Algorithm 2) and find 813
thatrCPU.overallusage is relevant to the user query. 814
D.3 The GenerateView phase 815
D.3.1 Summary view. 816
Consider again the question Q1 from Example 3.1. The view Vthat corresponds to the 817
green path in Figure 3 is shown in the listing in Table 7. We keep the same set of attributes 818
that MatchTables identified. In addition, we also perform renaming of all attributes, as we 819
can control the length of the aliases (in case they are too long). For example, Client.name 820
gets an alias Client_name,Client.gendergetsClient_gender, so on. 821
D.4 The QueryView phase 822
D.4.1promptC 823
Here is promptC that we use in the final phase QueryView (Algorithm 1, line 21). The 824
function name() returns name of the view V. 825
Inputs: Question,V
promptC I created a view table <name( V)> with all relevant information. Here
is a view <V>. Please write MySQL query to name( V) view to answer the following
question: <Question>. Use only name( V) columns in the query. Absolutely NO
columns renaming. Absolutely NO HAVING operators. Absolutely NO COUNT(*).
Output query that I can run via python interface. Output ’“‘sql...’. Do not explain.
Returns: SQL
826
We used a few assertive statements that we discuss next. ’Absolutely NO column renaming’ 827
means that we want to use aliases in the view table to form a valid SQL query. The statement 828
’Absolutely NO HAVING operators.’ reflects our observation that gpt4cannot generate 829
valid SQL when using HAVING in combination with GROUP BY. It is a subject of future 830
research to deal with MySQL constraints, so we encourage QueryView to avoid this operator. 831
20Finally, we discourage the use of COUNT(*), ‘Absolutely NO COUNT(*)’, to ensure that 832
gpt4focuses on counting the entities specified in the user’s question. 833
We noticed that better results are obtained if we provide a description of tables that are used 834
to generate this view together with their relevant attributes. Here is an extended version of 835
promptC where we provide relevant tables and their attributes that are used to obtain the 836
V. We also provide an evidence if available. 837
Inputs: Question,V, DB_SCHEMA
promptC’ (with evidence and a part of the schema) Here is a SQL schema
for in MySQL: <DB_SCHEMA> I created a view table <name( V)> with all relevant
information. Here is a view < V>. Please write MySQL query to name( V) view
to answer the following question: <Question>. Additional knowledge to answer:
<Evidence> Use only name( V) columns in the query. Absolutely NO columns
renaming. Absolutely NO HAVING operators. Absolutely NO COUNT(*). Output
query that I can run via python interface. Output ’“‘sql...’. Do not explain.
Returns: SQL
838
E Experimental evaluation (additional materials) 839
E.1 Setup 840
We run experiments on a laptop Intel(r) Core 2.40Hz and 32GB of memory. For nsqlwe use 841
the largest model with 7B parameters (NumbersStation/nsql-llama-2-7B [Labs, 2023a]). For 842
gpt4andLucy, we use the ‘gpt-4-0125-preview’ model as a LLMand set the temperature 843
to 0.2 . We do not fine-tune a LLM. We require 20 answers from gpt4for each question. If 844
the number of correct answers is more than 5, then we count that benchmark as solved. 845
In the case of Lucy, we require 5 answers for each GPTcall for the MatchTables phase. 846
We sort tables based on the number of occurrences in these answers and take at most 8 847
candidates among relevant tables from each promptA output. Similarly to gpt4, we require 848
20 answers from QueryView and decide on the success as described above. We use ORTools 849
as a constraint solver [Perron and Didier, 2024]. 850
We support MySQL as a relational database. However, BIRD uses SQLite. We automatically, 851
converted queries from sqlite to MySQL. 852
We provide all benchmarks and their results in the supplementary materials. 853
E.2 ACME insurance 854
Note on the database. There are a few issues with broken relational constraints due to 855
missing tables, as reported [datadotworld, Inc., 2024], which we fixed by adding the missing 856
tables from the original database. 857
Extended schema examples. Example of tables extended with comments that describe 858
each attribute for the ACME insurance benchmark. 859
860CREATE TABLE Claim_Amount 861
( 862
Claim_Amount_Identifier bigint NOT NULL COMMENT Claim Amount 863
Identifier is the unique identifier of the financial 864
amount reserved , paid , or collected in connection with a 865
claim . The money being paid or collected for settling a 866
claim and paying the claimants , reinsurers , other 867
insurers , and other interested parties . Claim amounts are 868
classified by various attributes ., 869
Claim_Identifier int NOT NULL COMMENT Claim Identifier 870
is the unique identifier for a Claim ., 871
Claim_Offer_Identifier int NULL COMMENT Claim Offer 872
Identifier is the unique identifier for a Claim Offer ., 873
21Amount_Type_Code varchar (20) NULL COMMENT Amount Type 874
Code defines the category to which a monetary amount will 875
be applied . Example : premium , commission , tax , 876
surcharge ., 877
Event_Date datetime NULL COMMENT Event Date is the 878
date on which a transaction or insurance - related 879
happening takes place ., 880
Claim_Amount decimal (15 ,2) NULL COMMENT The money 881
being paid or collected for settling a claim and paying 882
the claimants , reinsurers , other insurers , and other 883
interested parties . Claim amounts are classified by 884
various attributes ., 885
Insurance_Type_Code char (1) NULL COMMENT Insurance Type 886
Code represents the category under which risk is assumed . 887
Examples : Direct for policies directly issued by a 888
company ; Assumed for risks assumed from another company ; 889
Ceded for portions of risk ceded to another insurer ., 890
PRIMARY KEY ( Claim_Amount_Identifier ASC ), 891
FOREIGN KEY ( Claim_Offer_Identifier ) REFERENCES 892
Claim_Offer ( Claim_Offer_Identifier ), 893
FOREIGN KEY ( Claim_Identifier ) REFERENCES Claim ( Claim_Identifier ) 894
) 895896
897
898
CREATE TABLE Claim_Reserve 899
( 900
Claim_Amount_Identifier bigint NOT NULL COMMENT Claim Amount 901
Identifier is the unique identifier of the financial 902
amount reserved , paid , or collected in connection with a 903
claim . The amount of expected loss over the life of the 904
Claim ., 905
PRIMARY KEY ( Claim_Amount_Identifier ASC), 906
FOREIGN KEY ( Claim_Amount_Identifier ) REFERENCES 907
Claim_Amount ( Claim_Amount_Identifier ) 908
) 909910
E.2.1 Challenging questions 911
In this section, we present 13 questions that Lucyfound challenging to answer and identify 912
reasons for these failures. 913
Question1: What are the loss payment, loss reserve, expense payment, expense
reserve amount by claim number and corresponding policy number, policy holder,
premium amount paid, the catastrophe it had, and the agent who sold it?
Reason: Multiple lookups. "policy holder" and "agent" require a look up to the
same table Agreement_Party_Role.
914
Question2: What are the total loss, which is the sum of loss payment, loss reserve,
expense payment, expense reserve amount by claim number and corresponding policy
number, policy holder and premium amount paid?
Reason: Phase 1 issue. Phase 1 misses the relevant table Agreement_Party_Role.
915
Question3: What is the total amount of premiums that a policy holder has paid?
Reason: Phase 3 issue. Phase 3 makes a mistake in the group by clause.
916
22Question4: What are the total loss, which is the sum of loss payment, loss reserve,
expense payment, expense reserve amount by catastrophe and policy number?
Reason: Ambiguous question. By "by catastrophe", the user means to output
Catastrophe’s attribute Name. However, Phase 1 identifies Catastrophe’s attribute
Identifier as relevant instead of Name.
917
Question5: What is the average policy size which is the the total amount of
premium divided by the number of policies?
Reason: Ambiguous question. The definition of average is not standard, as the
same policy can have multiple amountvalues.
918
Question6: What are the loss payment, loss reserve, expense payment, expense
reserve amount by claim number and corresponding policy number, policy holder,
premium amount paid and the agent who sold it?
Reason: Multiple lookups.
919
Question7: Return agents and the policy they have sold that have had a claim
and the corresponding catastrophe it had.
Reason: Ambiguous question. The output includes Company_Claim_Number,
although this information is not specified in the question.
920
Question8: What is the loss ratio of each policy and agent who sold it by policy
number and agent id?
Reason: Ambiguous question. "the loss ratio" is a complex formula here, making it
hard to guess without its proper specification.
921
Question9: What are all the premiums that have been paid by policy holders?
Reason: Ambiguous question. Policy.Policy_Number and Party_Identifier should
be included in the output. But they are not specified in the question.
922
Question10: What are the loss payment, loss reserve, expense payment, expense
reserve amount by claim number and corresponding policy number, policy holder and
premium amount paid?
Reason: Phase 1 issue. Phase 1 misses the relevant table Agreement_Party_Role.
923
Question11: What is the loss ratio, number of claims, total loss by policy number
and premium where total loss is the sum of loss payment, loss reserve, expense
payment, expense reserve amount and loss ratio is total loss divided by premium?
Reason: Phase 1 issue. Phase 1 misses the relevant table Policy.
924
Question12: What are the total loss, which is the sum of loss payment, loss
reserve, expense payment, expense reserve amount by claim number, catastrophe and
corresponding policy number?
Reason: Phase 1 issue. Phase 1 misses the relevant table Catastrophe.
925
23Question13: What is the total amount of premiums that a policy holder has paid
by policy number?
Reason: Ambiguous question. Party_Identifier is included in the output. But it is
not specified in the question.
926
E.3 BIRD datasets 927
E.3.1 Additional notes on the dataset. 928
Note on dbModel. We used attribute descriptions available in BIRD in dbModel . We 929
also build table descriptions in the following way. We provided the description from BIRD to 930
anLLMto generate a short summary description using promptD defined in Section E.3.2. 931
Note on datasets. It has been shown that there are a number of incorrect ground truth 932
SQLs in BIRD datasets [Hui, 2024, Wretblad et al., 2024b]. For example, Wretblad et al. 933
[2024b] found that 72 out of 106 benchmark questions in financial have errors of various 934
types. Most of the issues have been reported to the authors from multiple sources, and we 935
also reported additional problems via private communication. The authors acknowledge 936
these issues and are working on them. To provide an example we reported from formula 1: 937
•Question: ‘Where can the introduction of the races held on Circuit de Barcelona- 938
Catalunya be found?’ 939
•Ground truth SQL: select distinct circuits.url fromcircuits inner join races 940
onraces.circuitId = circuits.circuitId wherecircuits.name = ’Circuit de Barcelona- 941
Catalunya’. 942
•The issue is that selectshould be on race.url rather than circuits.url as the user 943
requests information about the race, not the circuit. 944
On top of that, there are logical inconsistencies in ground truth answers for the financial 945
dataset. Often, users ask for information about clients’ accounts. Client and account tables 946
have a m: mrelationship modeled using an additional table disp. At the same time, they are 947
both related to a lookup table district. Unfortunately, many ground truth SQL statements 948
perform a join between clients and accounts via the district table, which is incorrect. Let’s 949
consider an example. 950
•Question: ‘Please provide the IDs of the 3 female clients with the largest loans.’ 951
•Ground truth SQL: selectT1.client_id fromclient asT1inner join account as 952
T2onT1.district_id = T2.district_id inner join loan asT3onT2.account_id = 953
T3.account_id whereT1.gender = ’F’ order by T3.amount DESC LIMIT 3 954
•The issue is that the answer relates clients and accounts that have the same dis- 955
trict_id. However, this does not mean that the client is related to the account. 956
As the authors are working on corrections, we analyzed the reported issues and manually 957
corrected the ground truth. We only adjusted the SQL ground truth values to match 958
the user questions; we did not alter the questions or evidences. We provide the corrected 959
benchmarks in the supplementary material. Specifically, we corrected the financial and 960
formula 1benchmarks and used the correct answers to evaluate all methods. Interestingly, 961
the performance of all frameworks improved by a few percentage points when we fixed these 962
ground truth SQL statements. 963
E.3.2promptD 964
Here is promptD that we use to generate tables summaries for financial andformula 1 965
datasets. 966
promptD Give me a very brief description of the <NAME> table.
967
24Example for the Loan table (the financial database)
Prompt: Give me a very brief description of the Loan table.
original_column_name,column_name,column_description,data_format,
value_description
loan_id„the id number identifying the loan data,integer,
account_id„the id number identifying the account,integer,
date„the date when the loan is approved,date,
amount„approved amount,integer,unit: US dollar
duration„loan duration,integer,unit: month
payments,monthly payments,monthly payments,real,unit: month
status„repayment status,text,"’A’ stands for contract finished, no problems; ’B’ stands
for contract finished, loan not paid; ’C’ stands for running contract, OK so far; ’D’
stands for running contract, client in debt"
Return: The Loan table manages loan-related data, offering insights into each
loan’s unique identifier, associated account details, approval dates, amounts, durations,
and monthly payments.
968
E.3.3 Challenging questions 969
We discuss three major groups of challenging questions with examples. 970
The first group contains ambiguous questions. Here are a few examples. 971
Question: List out the no. of districts that have female average salary is more
than 6000 but less than 10000?
Reason: Ambiguous question. ‘no. of districts’ refers to the district number based
on the ground truth. However, Lucycounts the number of districts.
972
Question: W that the client whose card was opened in 1996/10/21 made?
Reason: Ambiguous question. Lucyfilters on ‘card issued date’, while ground
truth filters on ‘account opened date’. However, the user is indeed asking about ‘card
open date’ in this question. This issue was also independently observed in [Wretblad
et al., 2024a].
973
The second group contains complex filtering, ordering, and/or formulas to compute. Here 974
are a few examples. 975
Question: List out the account numbers of clients who are youngest and have
highest average salary?
Reason: Phase 3 issue. There are two filtering conditions that have to be applied
in order. First, we find the youngest clients, then select the one with the highest
average salary among them. Lucytreats these conditions as a conjunction, resulting
in an empty output.
976
Question: List out the account numbers of female clients who are oldest and has
lowest average salary, calculate the gap between this lowest average salary with the
highest average salary?
Reason: Phase 3 issue. Two filtering conditions are required: first, in descending
order, and then in ascending order. However, Lucyfails to perform them in this
sequence.
977
25Question: For the client who applied the biggest loan, what was his/her first
amount of transaction after opened the account.
Reason: Phase 3 issue. Two filtering conditions are required: first, in ascending
order, and then in descending order. However, Lucyfails to perform them in this
sequence.
978
The third group contains questions where the MatchTables phase either adds an extra table, 979
or occasionally misses a table or attributes. Here is an example. 980
Question: How many accounts have an owner disposition and request for a state-
ment to be generated upon a transaction?
Reason: Phase 1 issue. Lucyidentifies "Tran" (transaction) as a relevant table,
but it is not needed to answer the query.
981
E.4 Cloud resources 982
Note on the cost of running. One note here is that GPTandc2qmodels are costly to 983
run. For example, in the Cloud Resources experiment, the costs are as follows: c2qcosts 984
$15,gpt4$2, and gpt4ex $5, while Lucycosts $0.5. 985
26NeurIPS Paper Checklist 986
1.Claims 987
Question: Do the main claims made in the abstract and introduction accurately 988
reflect the paper’s contributions and scope? 989
Answer: [Yes] 990
Justification: Yes, we do. 991
Guidelines: 992
•The answer NA means that the abstract and introduction do not include the 993
claims made in the paper. 994
•The abstract and/or introduction should clearly state the claims made, including 995
the contributions made in the paper and important assumptions and limitations. 996
A No or NA answer to this question will not be perceived well by the reviewers. 997
•The claims made should match theoretical and experimental results, and reflect 998
how much the results can be expected to generalize to other settings. 999
•It is fine to include aspirational goals as motivation as long as it is clear that 1000
these goals are not attained by the paper. 1001
2.Limitations 1002
Question: Does the paper discuss the limitations of the work performed by the 1003
authors? 1004
Answer: [Yes] 1005
Justification: Yes, see Section 4 1006
Guidelines: 1007
•The answer NA means that the paper has no limitation while the answer No 1008
means that the paper has limitations, but those are not discussed in the paper. 1009
•The authors are encouraged to create a separate "Limitations" section in their 1010
paper. 1011
•The paper should point out any strong assumptions and how robust the results 1012
are to violations of these assumptions (e.g., independence assumptions, noiseless 1013
settings, model well-specification, asymptotic approximations only holding 1014
locally). The authors should reflect on how these assumptions might be violated 1015
in practice and what the implications would be. 1016
•The authors should reflect on the scope of the claims made, e.g., if the approach 1017
was only tested on a few datasets or with a few runs. In general, empirical 1018
results often depend on implicit assumptions, which should be articulated. 1019
•The authors should reflect on the factors that influence the performance of the 1020
approach. For example, a facial recognition algorithm may perform poorly when 1021
image resolution is low or images are taken in low lighting. Or a speech-to-text 1022
system might not be used reliably to provide closed captions for online lectures 1023
because it fails to handle technical jargon. 1024
•The authors should discuss the computational efficiency of the proposed algo- 1025
rithms and how they scale with dataset size. 1026
•If applicable, the authors should discuss possible limitations of their approach 1027
to address problems of privacy and fairness. 1028
•While the authors might fear that complete honesty about limitations might 1029
be used by reviewers as grounds for rejection, a worse outcome might be that 1030
reviewers discover limitations that aren’t acknowledged in the paper. The 1031
authors should use their best judgment and recognize that individual actions in 1032
favor of transparency play an important role in developing norms that preserve 1033
the integrity of the community. Reviewers will be specifically instructed to not 1034
penalize honesty concerning limitations. 1035
3.Theory Assumptions and Proofs 1036
Question: For each theoretical result, does the paper provide the full set of assump- 1037
tions and a complete (and correct) proof? 1038
27Answer: [Yes] 1039
Justification: We model a part of the problem as an optimization problem and 1040
provide formal encoding. See Section 3.3. 1041
Guidelines: 1042
•The answer NA means that the paper does not include theoretical results. 1043
•All the theorems, formulas, and proofs in the paper should be numbered and 1044
cross-referenced. 1045
•All assumptions should be clearly stated or referenced in the statement of any 1046
theorems. 1047
•The proofs can either appear in the main paper or the supplemental material, 1048
but if they appear in the supplemental material, the authors are encouraged to 1049
provide a short proof sketch to provide intuition. 1050
•Inversely, any informal proof provided in the core of the paper should be 1051
complemented by formal proofs provided in appendix or supplemental material. 1052
•Theorems and Lemmas that the proof relies upon should be properly referenced. 1053
4.Experimental Result Reproducibility 1054
Question: Does the paper fully disclose all the information needed to reproduce 1055
the main experimental results of the paper to the extent that it affects the main 1056
claims and/or conclusions of the paper (regardless of whether the code and data are 1057
provided or not)? 1058
Answer: [Yes] 1059
Justification: Yes, we describe all algorithms and an optimization model. 1060
Guidelines: 1061
•The answer NA means that the paper does not include experiments. 1062
•If the paper includes experiments, a No answer to this question will not be 1063
perceived well by the reviewers: Making the paper reproducible is important, 1064
regardless of whether the code and data are provided or not. 1065
•If the contribution is a dataset and/or model, the authors should describe the 1066
steps taken to make their results reproducible or verifiable. 1067
•Depending on the contribution, reproducibility can be accomplished in various 1068
ways. For example, if the contribution is a novel architecture, describing the 1069
architecture fully might suffice, or if the contribution is a specific model and 1070
empirical evaluation, it may be necessary to either make it possible for others 1071
to replicate the model with the same dataset, or provide access to the model. In 1072
general. releasing code and data is often one good way to accomplish this, but 1073
reproducibility can also be provided via detailed instructions for how to replicate 1074
the results, access to a hosted model (e.g., in the case of a large language model), 1075
releasing of a model checkpoint, or other means that are appropriate to the 1076
research performed. 1077
•While NeurIPS does not require releasing code, the conference does require all 1078
submissions to provide some reasonable avenue for reproducibility, which may 1079
depend on the nature of the contribution. For example 1080
(a)If the contribution is primarily a new algorithm, the paper should make it 1081
clear how to reproduce that algorithm. 1082
(b)If the contribution is primarily a new model architecture, the paper should 1083
describe the architecture clearly and fully. 1084
(c)If the contribution is a new model (e.g., a large language model), then there 1085
should either be a way to access this model for reproducing the results or a 1086
way to reproduce the model (e.g., with an open-source dataset or instructions 1087
for how to construct the dataset). 1088
(d)We recognize that reproducibility may be tricky in some cases, in which 1089
case authors are welcome to describe the particular way they provide for 1090
reproducibility. In the case of closed-source models, it may be that access to 1091
the model is limited in some way (e.g., to registered users), but it should be 1092
28possible for other researchers to have some path to reproducing or verifying 1093
the results. 1094
5.Open access to data and code 1095
Question: Does the paper provide open access to the data and code, with sufficient 1096
instructions to faithfully reproduce the main experimental results, as described in 1097
supplemental material? 1098
Answer: [Yes] 1099
Justification: We provide the data in supplementary materials and describe prompts. 1100
We will make code publicly available. 1101
Guidelines: 1102
•The answer NA means that paper does not include experiments requiring code. 1103
•Please see the NeurIPS code and data submission guidelines ( https://nips. 1104
cc/public/guides/CodeSubmissionPolicy ) for more details. 1105
•While we encourage the release of code and data, we understand that this might 1106
not be possible, so “No” is an acceptable answer. Papers cannot be rejected 1107
simply for not including code, unless this is central to the contribution (e.g., for 1108
a new open-source benchmark). 1109
•The instructions should contain the exact command and environment needed 1110
to run to reproduce the results. See the NeurIPS code and data submis- 1111
sion guidelines ( https://nips.cc/public/guides/CodeSubmissionPolicy ) 1112
for more details. 1113
•The authors should provide instructions on data access and preparation, in- 1114
cluding how to access the raw data, preprocessed data, intermediate data, and 1115
generated data, etc. 1116
•The authors should provide scripts to reproduce all experimental results for 1117
the new proposed method and baselines. If only a subset of experiments are 1118
reproducible, they should state which ones are omitted from the script and why. 1119
•At submission time, to preserve anonymity, the authors should release 1120
anonymized versions (if applicable). 1121
•Providing as much information as possible in supplemental material (appended 1122
to the paper) is recommended, but including URLsto data and code is permitted. 1123
6.Experimental Setting/Details 1124
Question: Does the paper specify all the training and test details (e.g., data splits, 1125
hyperparameters, how they were chosen, type of optimizer, etc.) necessary to 1126
understand the results? 1127
Answer: [Yes] 1128
Justification: We specified parameters of prompts. We do not train new models. 1129
Guidelines: 1130
•The answer NA means that the paper does not include experiments. 1131
•The experimental setting should be presented in the core of the paper to a level 1132
of detail that is necessary to appreciate the results and make sense of them. 1133
•The full details can be provided either with the code, in appendix, or as 1134
supplemental material. 1135
7.Experiment Statistical Significance 1136
Question: Does the paper report error bars suitably and correctly defined or other 1137
appropriate information about the statistical significance of the experiments? 1138
Answer: [NA] 1139
Justification: We provide details for Lucyandgpt4. Existing methods either 1140
provide their results as a single answer [Li et al., 2024b] or are too costly to run 1141
multiple times. 1142
Guidelines: 1143
29•The answer NA means that the paper does not include experiments. 1144
•The authors should answer "Yes" if the results are accompanied by error bars, 1145
confidence intervals, or statistical significance tests, at least for the experiments 1146
that support the main claims of the paper. 1147
•The factors of variability that the error bars are capturing should be clearly 1148
stated (for example, train/test split, initialization, random drawing of some 1149
parameter, or overall run with given experimental conditions). 1150
•The method for calculating the error bars should be explained (closed form 1151
formula, call to a library function, bootstrap, etc.) 1152
•The assumptions made should be given (e.g., Normally distributed errors). 1153
•It should be clear whether the error bar is the standard deviation or the standard 1154
error of the mean. 1155
•It is OK to report 1-sigma error bars, but one should state it. The authors 1156
should preferably report a 2-sigma error bar than state that they have a 96% 1157
CI, if the hypothesis of Normality of errors is not verified. 1158
•For asymmetric distributions, the authors should be careful not to show in 1159
tables or figures symmetric error bars that would yield results that are out of 1160
range (e.g. negative error rates). 1161
•If error bars are reported in tables or plots, The authors should explain in the 1162
text how they were calculated and reference the corresponding figures or tables 1163
in the text. 1164
8.Experiments Compute Resources 1165
Question: For each experiment, does the paper provide sufficient information on the 1166
computer resources (type of compute workers, memory, time of execution) needed 1167
to reproduce the experiments? 1168
Answer: [Yes] 1169
Justification: Yes, we describe the experimental setup. 1170
Guidelines: 1171
•The answer NA means that the paper does not include experiments. 1172
•The paper should indicate the type of compute workers CPU or GPU, internal 1173
cluster, or cloud provider, including relevant memory and storage. 1174
•The paper should provide the amount of compute required for each of the 1175
individual experimental runs as well as estimate the total compute. 1176
•The paper should disclose whether the full research project required more 1177
compute than the experiments reported in the paper (e.g., preliminary or failed 1178
experiments that didn’t make it into the paper). 1179
9.Code Of Ethics 1180
Question: Does the research conducted in the paper conform, in every respect, with 1181
the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 1182
Answer: [Yes] 1183
Justification: 1184
Guidelines: 1185
•The answer NA means that the authors have not reviewed the NeurIPS Code 1186
of Ethics. 1187
•If the authors answer No, they should explain the special circumstances that 1188
require a deviation from the Code of Ethics. 1189
•The authors should make sure to preserve anonymity (e.g., if there is a special 1190
consideration due to laws or regulations in their jurisdiction). 1191
10.Broader Impacts 1192
Question: Does the paper discuss both potential positive societal impacts and 1193
negative societal impacts of the work performed? 1194
Answer: [Yes] 1195
30Justification: We believe it has a positive impact as we enhance users with new 1196
capabilities. 1197
Guidelines: 1198
•The answer NA means that there is no societal impact of the work performed. 1199
•If the authors answer NA or No, they should explain why their work has no 1200
societal impact or why the paper does not address societal impact. 1201
•Examples of negative societal impacts include potential malicious or unintended 1202
uses (e.g., disinformation, generating fake profiles, surveillance), fairness consid- 1203
erations (e.g., deployment of technologies that could make decisions that unfairly 1204
impact specific groups), privacy considerations, and security considerations. 1205
•The conference expects that many papers will be foundational research and 1206
not tied to particular applications, let alone deployments. However, if there 1207
is a direct path to any negative applications, the authors should point it out. 1208
For example, it is legitimate to point out that an improvement in the quality 1209
of generative models could be used to generate deepfakes for disinformation. 1210
On the other hand, it is not needed to point out that a generic algorithm for 1211
optimizing neural networks could enable people to train models that generate 1212
Deepfakes faster. 1213
•The authors should consider possible harms that could arise when the technology 1214
is being used as intended and functioning correctly, harms that could arise when 1215
the technology is being used as intended but gives incorrect results, and harms 1216
following from (intentional or unintentional) misuse of the technology. 1217
•If there are negative societal impacts, the authors could also discuss possible 1218
mitigation strategies (e.g., gated release of models, providing defenses in addition 1219
to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a 1220
system learns from feedback over time, improving the efficiency and accessibility 1221
of ML). 1222
11.Safeguards 1223
Question: Does the paper describe safeguards that have been put in place for 1224
responsible release of data or models that have a high risk for misuse (e.g., pretrained 1225
language models, image generators, or scraped datasets)? 1226
Answer: [NA] . 1227
Justification: 1228
Guidelines: 1229
•The answer NA means that the paper poses no such risks. 1230
•Released models that have a high risk for misuse or dual-use should be released 1231
with necessary safeguards to allow for controlled use of the model, for example 1232
by requiring that users adhere to usage guidelines or restrictions to access the 1233
model or implementing safety filters. 1234
•Datasets that have been scraped from the Internet could pose safety risks. The 1235
authors should describe how they avoided releasing unsafe images. 1236
•We recognize that providing effective safeguards is challenging, and many papers 1237
do not require this, but we encourage authors to take this into account and 1238
make a best faith effort. 1239
12.Licenses for existing assets 1240
Question: Are the creators or original owners of assets (e.g., code, data, models), 1241
used in the paper, properly credited and are the license and terms of use explicitly 1242
mentioned and properly respected? 1243
Answer: [Yes] 1244
Justification: 1245
Guidelines: 1246
•The answer NA means that the paper does not use existing assets. 1247
31•The authors should cite the original paper that produced the code package or 1248
dataset. 1249
•The authors should state which version of the asset is used and, if possible, 1250
include a URL. 1251
•The name of the license (e.g., CC-BY 4.0) should be included for each asset. 1252
•For scraped data from a particular source (e.g., website), the copyright and 1253
terms of service of that source should be provided. 1254
•If assets are released, the license, copyright information, and terms of use in 1255
the package should be provided. For popular datasets, paperswithcode.com/ 1256
datasets has curated licenses for some datasets. Their licensing guide can help 1257
determine the license of a dataset. 1258
•For existing datasets that are re-packaged, both the original license and the 1259
license of the derived asset (if it has changed) should be provided. 1260
•If this information is not available online, the authors are encouraged to reach 1261
out to the asset’s creators. 1262
13.New Assets 1263
Question: Are new assets introduced in the paper well documented and is the 1264
documentation provided alongside the assets? 1265
Answer: [Yes] 1266
Justification: We provide all benchmarks in supplementary materials. 1267
Guidelines: 1268
•The answer NA means that the paper does not release new assets. 1269
•Researchers should communicate the details of the dataset/code/model as part 1270
of their submissions via structured templates. This includes details about 1271
training, license, limitations, etc. 1272
•The paper should discuss whether and how consent was obtained from people 1273
whose asset is used. 1274
•At submission time, remember to anonymize your assets (if applicable). You 1275
can either create an anonymized URL or include an anonymized zip file. 1276
14.Crowdsourcing and Research with Human Subjects 1277
Question: For crowdsourcing experiments and research with human subjects, does 1278
the paper include the full text of instructions given to participants and screenshots, 1279
if applicable, as well as details about compensation (if any)? 1280
Answer: [NA] 1281
Justification: 1282
Guidelines: 1283
•The answer NA means that the paper does not involve crowdsourcing nor 1284
research with human subjects. 1285
•Including this information in the supplemental material is fine, but if the main 1286
contribution of the paper involves human subjects, then as much detail as 1287
possible should be included in the main paper. 1288
•According to the NeurIPS Code of Ethics, workers involved in data collection, 1289
curation, or other labor should be paid at least the minimum wage in the 1290
country of the data collector. 1291
15.Institutional Review Board (IRB) Approvals or Equivalent for Research 1292
with Human Subjects 1293
Question: Does the paper describe potential risks incurred by study participants, 1294
whether such risks were disclosed to the subjects, and whether Institutional Review 1295
Board (IRB) approvals (or an equivalent approval/review based on the requirements 1296
of your country or institution) were obtained? 1297
Answer: [NA] 1298
Justification: 1299
32Guidelines: 1300
•The answer NA means that the paper does not involve crowdsourcing nor 1301
research with human subjects. 1302
•Depending on the country in which research is conducted, IRB approval (or 1303
equivalent) may be required for any human subjects research. If you obtained 1304
IRB approval, you should clearly state this in the paper. 1305
•We recognize that the procedures for this may vary significantly between insti- 1306
tutions and locations, and we expect authors to adhere to the NeurIPS Code of 1307
Ethics and the guidelines for their institution. 1308
•For initial submissions, do not include any information that would break 1309
anonymity (if applicable), such as the institution conducting the review. 1310
33