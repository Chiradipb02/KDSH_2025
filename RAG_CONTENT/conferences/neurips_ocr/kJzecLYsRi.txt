On the Saturation Effects of Spectral Algorithms
in Large Dimensions
Weihao Lu
Department of Statistics and Data Science
Tsinghua University
Beijing, China 100084
luwh19@mails.tsinghua.edu.cnHaobo Zhang
Department of Statistics and Data Science
Tsinghua University
Beijing, China 100084
zhang-hb21@mails.tsinghua.edu.cn
Yicheng Li
Department of Statistics and Data Science
Tsinghua University
Beijing, China 100084
liyc22@mails.tsinghua.edu.cnQian Lin∗
Department of Statistics and Data Science
Tsinghua University
Beijing, China 100084
qianlin@tsinghua.edu.cn
Abstract
The saturation effects, which originally refer to the fact that kernel ridge regression
(KRR) fails to achieve the information-theoretical lower bound when the regres-
sion function is over-smooth, have been observed for almost 20 years and were
rigorously proved recently for kernel ridge regression and some other spectral
algorithms over a fixed dimensional domain. The main focus of this paper is to
explore the saturation effects for a large class of spectral algorithms (including the
KRR, gradient descent, etc.) in large dimensional settings where n≍dγ. More
precisely, we first propose an improved minimax lower bound for the kernel regres-
sion problem in large dimensional settings and show that the gradient flow with
early stopping strategy will result in an estimator achieving this lower bound (up to
a logarithmic factor). Similar to the results in KRR, we can further determine the
exact convergence rates (both upper and lower bounds) of a large class of (optimal
tuned) spectral algorithms with different qualification τ’s. In particular, we find
that these exact rate curves (varying along γ) exhibit the periodic plateau behavior
and the polynomial approximation barrier. Consequently, we can fully depict the
saturation effects of the spectral algorithms and reveal a new phenomenon in large
dimensional settings (i.e., the saturation effect occurs in large dimensional setting
as long as the source condition s > τ while it occurs in fixed dimensional setting
as long as s >2τ).
1 Introduction
Let’s assume we have ni.i.d. samples (xi, yi)from a joint distribution supported on Rd×R. The
regression problem, one of the most fundamental problems in statistics, aims to find a function ˆf
based on these samples such that the excess risk ,∥ˆf−f⋆∥2
L2=Ex[(f⋆(x)−ˆf(x))2], is small, where
f⋆(x) =E[Y|x]is the regression function . Many non-parametric regression methods are proposed
to solve the regression problem by assuming that f⋆falls into certain function classes, including
polynomial splines Stone (1994), local polynomials Cleveland (1979); Stone (1977), the spectral
algorithms Caponnetto (2006); Caponnetto and De Vito (2007); Caponnetto and Yao (2010), etc.
∗Corresponding author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Spectral algorithms, as a classical topic, have been studied since the 1990s. Early works treated
certain types of spectral algorithms in their theoretical analysis (Caponnetto (2006); Caponnetto and
De Vito (2007); Raskutti et al. (2014); Lin et al. (2020)). These works often consider das a fixed
constant and impose the polynomial eigenvalue decay assumption under a kernel (i.e., there exist
constants 0<c≤C<∞, such that the eigenvalues of the kernel satisfy cj−β≤λj≤Cj−β,j≥1
for certain β >1depending on the fixed d). They further assume that f⋆belongs to the reproducing
kernel Hilbert space (RKHS) Hassociated with the kernel. Under the above assumptions, they
then showed that the minimax rate of the excess risk of regression over the corresponding RKHS
is lower bounded by n−β/(β+1)and that some (regularized) spectral algorithms, e.g., the kernel
ridge regression (KRR) and the kernel gradient flow, can produce estimators achieving this minimax
optimal rate.
However, subsequent studies have revealed that when higher regularity (or smoothness) of f⋆is
assumed, KRR fails to achieve the information-theoretical lower bound on the excess risk, while
kernel gradient flow can do so. Specifically, let’s assume that f⋆belongs to the interpolation space
[H]sof the RKHS Hwiths >0(see, e.g., Steinwart et al. (2009); Dieuleveut et al. (2017); Dicker
et al. (2017); Pillaud-Vivien et al. (2018); Lin et al. (2020); Fischer and Steinwart (2020); Celisse
and Wahl (2021)). It is then shown that the information-theoretical lower bound on the excess risk
isn−sβ/(sβ+1). When 0< s≤2, Caponnetto and De Vito (2007); Yao et al. (2007); Lin et al.
(2020); Zhang et al. (2023) have already shown that the upper bound of the excess risks of both KRR
and the kernel gradient flow is n−sβ/(sβ+1), and hence they are minimax optimal. On the contrary,
when s >2, Yao et al. (2007); Lin et al. (2020) showed that the upper bound of the excess risks
of kernel gradient flow is n−sβ/(sβ+1)while the best upper bound of the excess risks of KRR is
n−2β/(2β+1)(Caponnetto and De Vito (2007)). Bauer et al. (2007); Gerfo et al. (2008); Dicker et al.
(2017) conjectured that the convergence rate of KRR is bounded below by n−2β/(2β+1)and Li et al.
(2022) rigorously proved it. The above phenomenon is often referred to as the saturation effect of
KRR:
KRR is inferior to certain spectral algorithms, such as kernel gradient flow, when s >2.
In recent years, neural network methods have gained tremendous success in many large-dimensional
problems, such as computer vision He et al. (2016); Krizhevsky et al. (2017) and natural language
processing Devlin (2018). Several groups of researchers tried to explain the superior performance of
neural networks on large-dimensional data from the aspects of "lazy regime" (Arora et al. (2019); Du
et al. (2019, 2018); Li and Liang (2018)). They noticed that, when the width of a neural network is
sufficiently large, its parameters/weights stay in a small neighborhood of their initial position during
the training process. Later, Jacot et al. (2018); Arora et al. (2019); Hu et al. (2021); Suh et al. (2021);
Lai et al. (2023); Li et al. (2024) proved that the time-varying neural network kernel (NNK) converges
(uniformly) to a time-invariant neural tangent kernel (NTK) as the width of the neural network goes
to infinity, and thus the excess risk of kernel gradient flow with NTK converges (uniformly) to the
excess risk of neural networks in the ‘lazy regime’.
Inspired by the concepts of the "lazy regime" and the uniform convergence of excess risk, the
machine learning community has experienced a renewed surge of interest in large-dimensional
spectral algorithms. The earliest works focused on the consistency of two specific types of spectral
algorithms: KRR and kernel interpolation (Liang and Rakhlin (2020); Liang et al. (2020); Ghorbani
et al. (2020, 2021); Mei et al. (2021, 2022); Misiakiewicz and Mei (2022); Aerni et al. (2023); Barzilai
and Shamir (2023)). In comparison, results on large-dimensional kernel gradient flow were somewhat
scarce, and these results largely mirrored those associated with KRR (e.g., Ghosh et al. (2021)).
Recently, Lu et al. (2023) proved that large-dimensional kernel gradient flow is minimax optimal
when s= 1. Then, Zhang et al. (2024) provided upper and lower bounds on the convergence rate on
the excess risk of KRR for any s >0. Surprisingly, they discovered that for s >1, the convergence
rate of KRR did not match the lower bound on the minimax rate. Unfortunately, they didn’t prove that
certain spectral algorithms can reach the lower bound on the minimax rate they provided, and hence
they didn’t rigorously prove that the saturation effect of KRR occurs in large dimensions. Instead,
Zhang et al. (2024) only conjectured that certain spectral algorithms (e.g., kernel gradient flow) can
provide minimax optimal estimators after their main results.
If Zhang et al. (2024)’s conjecture is true, then we can safely conclude that: when the regression
function f⋆is smooth enough, KRR is inferior to kernel gradient flow in large dimensions as well.
Consequently, previous results on large-dimensional KRR may not be directly extendable to large-
2dimensional neural networks, even if the neural networks are in the ‘lazy regime’. The main focus of
this paper is to prove this conjecture by showing that kernel gradient flow is minimax optimal in large
dimensions.
1.1 Related work
Saturation effects of fixed-dimensional spectral algorithms. When the dimension dof the data
is fixed, the saturation effect of KRR has been conjectured for decades and is rigorously proved in
the recent work Li et al. (2022). Suppose f⋆∈[H]swiths >2. It is shown that: (i) the minimax
optimal rate is n−sβ/(sβ+1)(Rastogi and Sampath (2017); Yao et al. (2007); Lin et al. (2020));
and (ii) the convergence rate on the excess risk of KRR is n−2β/(2β+1)(Li et al. (2022)). More
recently, Li et al. (2024) determined the exact generalization error curves of a class of analytic spectral
algorithms, which allowed them to further show the saturation effect of spectral algorithms with finite
qualification τ(see, e.g., Appendix C): suppose f⋆∈[H]swiths >2τ, then the convergence rate on
the excess risk of the above spectral algorithms is n−2τβ/(2τβ+1).
New phenomena in large-dimensional spectral algorithms. In the large-dimensional setting
where n≍dγwithγ > 0, new phenomena exhibited in spectral algorithms are popular topics
in recent machine-learning research. A line of work focused on the polynomial approximation
barrier phenomenon (e.g., Ghorbani et al. (2021); Donhauser et al. (2021); Mei et al. (2022); Xiao
et al. (2023); Misiakiewicz (2022); Hu and Lu (2022)). They found that, for the square-integrable
regression function, KRR and kernel gradient flow are consistent if and only if the regression function
is a polynomial with a low degree. Another line of work considered the benign overfitting of
kernel interpolation (i.e., kernel interpolation can generalize) (e.g., Liang and Rakhlin (2020); Liang
et al. (2020); Aerni et al. (2023); Barzilai and Shamir (2023); Zhang et al. (2024)). Moreover, two
recent work (Lu et al. (2023); Zhang et al. (2024)) discussed two new phenomena exhibited in
large-dimensional KRR and kernel gradient flow: the multiple descent behavior and the periodic
plateau behavior. The multiple descent behavior refers to the phenomenon that the curve of the
convergence rate ( with respect to n) of the optimal excess risk is non-monotone and has several
isolated peaks and valleys; while the periodic plateau behavior refers to the phenomenon that the
curve of the convergence rate ( with respect to d) of the optimal excess risk has constant values when
γis within certain intervals. Finally, Zhang et al. (2024) conjectured that the saturation effect of KRR
occurs in large dimensions. The above works imply that these phenomena occur in many spectral
algorithms in large dimensions, hence encouraging us to provide a unified explanation of these new
phenomena.
1.2 Our contributions
In this paper, we focus on the large-dimensional spectral algorithms with inner product kernels, and
we assume that the regression function falls into an interpolation space [H]swiths >0. We state our
main results as follows:
Theorem 1.1 (Restate Theorem 4.1 and 4.2, non-rigorous) .Lets >0,τ≥1, and γ >0be fixed
real numbers. Denote pas the integer satisfying γ∈[p(s+ 1),(p+ 1)( s+ 1)) . Then under certain
conditions, the excess risk of large-dimensional spectral algorithm with qualification τsatisfies
Eˆfλ⋆−f⋆2
L2X
=(
ΘP 
d−min{γ−p,s(p+1)}
·poly(ln(d)), s≤τ
ΘP
d−min{γ−p,τ(γ−p+1)+ p˜s
τ+1,˜s(p+1)}
·poly(ln(d)), s > τ,
where ˜s= min {s,2τ}.
More specifically, we list the main contributions of this paper as follows:
(1)In Theorem 3.1, we show that the convergence rate on the excess risk of (optimally-tuned)
kernel gradient flow in large dimensions is ΘP(d−min{γ−p,s(p+1)})·poly(ln(d)), which
matches the lower bound on the minimax rate given in Theorem 3.3 (up to a logarithmic
factor). We find that kernel gradient flow is minimax optimal for any s >0and any γ >0,
and KRR is not minimax optimal for s >1and for certain ranges of γ(We provide a visual
illustration in Figure 2). Consequently, we rigorously prove that the saturation effect of
KRR occurs in large dimensions.
3(2)In Theorem 3.3, we enhanced the previous minimax lower bound results given in Lu et al.
(2023) and Zhang et al. (2024). Specifically, we show that the minimax lower bound is
Ω(d−min{γ−p,s(p+1)})/poly(ln(d)). In comparison, the previous minimax lower bound is
Ω(d−min{γ−p,s(p+1)})/dεfor any ε >0, and the additional term dεchanges the desired
convergence rate.
(3)In Section 4, we determine the convergence rate on the excess risk of large-dimensional
spectral algorithms. From our results, we find several new phenomena exhibited in spectral
algorithms in large-dimensional settings. We provide a visual illustration of the above
phenomena in Figure 1: i) The first phenomenon is the polynomial approximation barrier,
and as shown in Figure 1(a), when sis close to zero, the curve of the convergence rate of
spectral algorithm drops when γ≈pfor any integer pand will stay invariant for most of the
other γ; ii) The second one is the periodic plateau behavior, and as shown in Figure 1(b) and
Figure 1(c), when 0< s < 2τandγ∈[p(s+ 1) + s+ (max {s, τ}−τ)/τ,(p+ 1)( s+ 1))
for an integer p≥0, the convergence rate does not change when γvaries; iii) The final
one is the saturation effect, and as shown in Figure 1(c) and Figure 1(d), when s > τ , the
convergence rate of spectral algorithm can not achieve the minimax lower bound for certain
ranges of γ. A detailed discussion about the above three phenomena can be found in Section
4.
(a)
 (b)
 (c)
 (d)
Figure 1: Convergence rates of spectral algorithm with qualification τ= 2in Theorem 4.1, Theorem
4.2, and corresponding minimax lower rates in Theorem 3.3 with respect to dimension d. We
present four graphs corresponding to four kinds of source conditions: s= 0.01,1,3,5. The x-axis
represents asymptotic scaling, γ:n≍dγ; the y-axis represents the convergence rate of excess risk,
r:Excess risk ≍dr.
2 Preliminaries
Suppose that we have observed ni.i.d. samples (xi, yi), i∈[n]from the model:
y=f⋆(x) +ϵ, (1)
where xi’s are sampled from ρX,ρXis the marginal distribution on X ⊂Rd+1,y∈ Y ⊂ R,f⋆is
some function defined on a compact set X, and
E(x,y)∼ρh
ϵ2xi
≤σ2, ρX-a.e.x∈ X,
for some fixed constant σ >0, where ρis the joint distribution of (x, y)onX × Y . Denote the n×1
data vector of yi’s and the n×ddata matrix of xi’s byYandXrespectively.
2.1 Kernel ridge regression and kernel gradient flow
In this subsection, we introduce two specific spectral algorithms, kernel ridge regression and kernel
gradient flow, which produce estimators of the regression function f⋆. A further discussion on general
spectral algorithms will be provided in Section 4.
Throughout the paper, we denote Has a separable RKHS on Xwith respect to a continuous and
positive definite kernel function K(·,·) :X × X → Rand there exists a constant κsatisfying
max
x∈XK(x, x)≤κ2.
4Kernel ridge regression Kernel ridge regression (KRR) constructs an estimator ˆfKRR
λby solving
the penalized least square problem
ˆfKRR
λ= arg min
f∈H 
1
nnX
i=1(yi−f(xi))2+λ∥f∥2
H!
,
where λ > 0is referred to as the regularization parameter. The representer theorem (see, e.g.,
Steinwart and Christmann (2008)) gives an explicit expression of the KRR estimator, i.e.,
ˆfKRR
λ(x) =K(x, X)(K(X, X ) +nλI)−1Y. (2)
Kernel gradient flow The gradient flow of the loss function L=1
2nP
i(yi−f(xi))2induced a
gradient flow in Hwhich is given by
d
dtˆfGF
t(x) =−1
nK(x, X)(ˆfGF
t(X)−Y). (3)
If we further assume that ˆfGF
0(x) = 0 , then we can also give an explicit expression of the kernel
gradient flow estimator
ˆfGF
t(x) =K(x, X)K(X, X )−1(I−e−1
nK(X,X)t)Y. (4)
2.2 The interpolation space
Define the integral operator TKasTK(f)(x) =R
K(x, x′)f(x′)dρX(x′). It is well known that TK
is a positive, self-adjoint, trace-class, and hence a compact operator (Steinwart and Scovel (2012)).
The celebrated Mercer’s theorem further assures that
K(x, x′) =X
jλjϕj(x)ϕj(x′), (5)
where the eigenvalues {λj, j= 1,2, ...}is a non-increasing sequence, and the corresponding
eigenfunctions {ϕj(·), j= 1,2, ...}are orthonormal in L2(X, ρX)function space.
The interpolation space [H]swith source condition sis defined as
[H]s:=nX
jajλs/2
jϕj: (aj)j∈ℓ2o
⊆L2(X, ρX), (6)
with the inner product deduced from
∞X
j=1ajλs/2
jϕj
[H]s=∞X
j=1a2
j1/2
. (7)
It is easy to show that [H]sis also a separable Hilbert space with orthonormal basis {λs/2
jϕj}j.
Generally speaking, functions in [H]sbecome smoother as sincreases (see, e.g., the example of
Sobolev RKHS in Edmunds and Triebel (1996); Zhang et al. (2023).
2.3 Assumptions
In this subsection, we list the assumptions that we need for our main results.
To avoid potential confusion, we specify the following large-dimensional scenario for kernel regres-
sion where we perform our analysis: suppose that there exist three positive constants c1,c2andγ,
such that
c1dγ≤n≤c2dγ, (8)
and we often assume that dis sufficiently large.
In this paper, we only consider the inner product kernels defined on the sphere. An inner product
kernel is a kernel function Kdefined on Sdsuch that there exists a function Φ : [−1,1]→R
independent of dsatisfying that for any x, x′∈Sd, we have K(x, x′) = Φ( ⟨x, x′⟩). If we further
5assume that the marginal distribution ρXis the uniform distribution on X=Sd, then the Mercer’s
decomposition for Kcan be rewritten as
K(x, x′) =∞X
k=0µkN(d,k)X
j=1Yk,j(x)Yk,j(x′), (9)
where Yk,jforj= 1,···, N(d, k)are spherical harmonic polynomials of degree kandµk’s are the
eigenvalues of Kwith multiplicity N(d,0) = 1 ;N(d, k) =2k+d−1
k·(k+d−2)!
(d−1)!(k−1)!, k= 1,2,···.
For more details of the inner product kernels, readers can refer to Gallier (2009).
Remark 2.1.We consider the inner product kernels on the sphere mainly because the harmonic
analysis is clear on the sphere ( e.g., properties of spherical harmonic polynomials are more concise
than the orthogonal series on general domains). This makes Mercer’s decomposition of the inner
product more explicit rather than several abstract assumptions ( e.g., Mei and Montanari (2022)).
We also notice that very few results are available for Mercer’s decomposition of a kernel defined on
the general domain, especially when the dimension of the domain is taking into consideration. e.g.,
even the eigen-decay rate of the neural tangent kernels is only determined for the spheres. Restricted
by this technical reason, most works analyzing the spectral algorithm in large-dimensional settings
focus on the inner product kernels on spheres (Liang et al., 2020; Ghorbani et al., 2021; Misiakiewicz,
2022; Xiao et al., 2023; Lu et al., 2023, etc.). Though there might be several works that tried to
relax the spherical assumption (e.g., Liang et al. (2020); Aerni et al. (2023); Barzilai and Shamir
(2023), we can find that most of them (i) adopted a near-spherical assumption; (ii) adopted strong
assumptions on the regression function, e.g., f⋆(x) =x[1]x[2]···x[L]for an integer L >0, where
x[i]denotes the i-th component of x; or (iii) can not determine the convergence rate on the excess
risk of the spectral algorithm.
To avoid unnecessary notation, let us make the following assumption on the inner product kernel K.
Assumption 1.Φ(t)∈ C∞([−1,1])is a fixed function independent of dand there exists a non-
negative sequence of absolute constants {aj≥0}j≥0, such that we have
Φ(t) =X∞
j=0ajtj,
where aj>0for any j≤ ⌊γ⌋+ 3.
The purpose of Assumption 1 is to keep the main results and proofs clean. Notice that, by Theorem
1.b in Gneiting (2013), the inner product kernel Kon the sphere is semi-positive definite for all
dimensions if and only if all coefficients {aj, j= 0,1,2, ...}are non-negative. One can easily extend
our results in this paper when certain coefficients ak’s are zero (e.g., one can consider the two-layer
NTK defined as in Section 5 of Lu et al. (2023), with ai= 0for any i= 3,5,7,···).
In the next assumption, we formally introduce the source condition, which characterizes the relative
smoothness of f⋆with respect to H.
Assumption 2 (Source condition) .Suppose that f⋆(x) =P∞
i=1fiϕi(x).
(a)f⋆∈[H]sfor some s >0, and there exists a constant Rγonly depending on γ, such that
∥f⋆∥[H]s≤Rγ. (10)
(b)Denote qas the smallest integer such that q > γ andµq̸= 0. Define Id,kas the index set
satisfying λi≡µk, i∈ Id,k. Further suppose that there exists an absolute constant c0>0
such that for any dandk∈ {0,1,···, q}withµk̸= 0, we have
X
i∈Id,kµ−s
kf2
i≥c0. (11)
Assumption 2 is a common assumption when one is interested in the tight bounds on the excess risk
of spectral algorithms (e.g., Caponnetto and De Vito (2007); Fischer and Steinwart (2020), Eq.(8)
in Cui et al. (2021), Assumption 3 in Li et al. (2024), and Assumption 5 in Zhang et al. (2024)).
Assumption 2 implies that the regression function exactly falls into the interpolation space [H]s, that
is,f⋆∈[H]sandf⋆/∈[H]tfor any t > s . For example, from the proof part I of Lemma D.14, one
can check that f⋆withP
i∈Id,pµ−s
pf2
i=P
i∈Id,p+1µ−s
p+1f2
i= 0can have a faster convergence rate
on the excess risk.
6Notations. Let’s denote the norm in L2(X, ρX)as∥ · ∥ L2. For a vector x, we use x[i]to de-
note its i-th component. We use asymptotic notations O(·), o(·),Ω(·)andΘ(·). For instance,
we say two (deterministic) quantities U(d), V(d)satisfy U(d) =o(V(d))if and only if for any
ε > 0, there exists a constant Dεthat only depends on εand the absolute positive constants
σ, κ, s, γ, c 0, c1, c2,C1,···,C8>0, such that for any d > D ε, we have U(d)< εV (d). We also
write an=poly(bn)if there exist a constant θ≥0, such that an= Θ( bθ
n). We use the probability
versions of the asymptotic notations such as OP(·), oP(·),ΩP(·),ΘP(·). For instance, we say the
random variables Xn, Ynsatisfying Xn=OP(Yn)if and only if for any ε >0, there exist constants
CεandNεsuch that P(|Xn| ≥Cε|Yn|)≤ε,∀n > N ε.
2.4 Review of the previous results
The following two results are restatements of Theorem 2 and Theorem 5 in Zhang et al. (2024).
Proposition 2.2. Lets≥1andγ >0be fixed real numbers. Denote pas the integer satisfying
γ∈[p(s+ 1),(p+ 1)( s+ 1)) . Suppose that Assumption 1 and Assumption 2 hold for sandγ. Let
ˆfKRR
λbe the function defined in (2). Define ˜s= min {s,2}, then there exists λ⋆>0, such that we have
EˆfKRR
λ⋆−f⋆2
L2X
= Θ P
d−min{γ−p,γ−p+p˜s+1
2,˜s(p+1)}
·poly(ln(d)),
where ΘPonly involves constants depending on s, σ, γ, c 0, κ, c 1andc2. In addition, the convergence
rates of the generalization error can not be faster than above for any choice of regularization
parameter λ=λ(d, n)→0.
Proposition 2.3 (Lower bound on the minimax rate) .Lets >0andγ >0be fixed real numbers.
Denote pas the integer satisfying γ∈[p(s+ 1),(p+ 1)( s+ 1)) . LetPconsist of all the distributions
ρonX ×Y such that Assumption 1 and Assumption 2 hold for sandγ. Then for any ε >0, we have:
min
ˆfmax
ρ∈PE(X,Y)∼ρ⊗nˆf−f⋆2
L2= Ω
d−min{γ−p,s(p+1)}·d−ε
,
where Ωonly involves constants depending on s, σ, γ, c 0, κ, c 1, c2andε.
From the above two propositions, we can find that when s >1, the convergence rate on the excess
risk of KRR does not always match the lower bound on the minimax optimal rate. Zhang et al. (2024)
further conjectured that the lower bound on the minimax optimal rate provided in Proposition 2.3
is tight (ignoring the additional term d−ε). Hence, they believed that the saturation effect exists for
large-dimensional KRR.
3 Main results
In this section, we determine the convergence rate on the excess risk of kernel gradient flow as
d−min{γ−p,s(p+1)}poly(ln(d)), which differs from the lower bound on the minimax rate provided
in Proposition 2.3 by dεfor any ε >0. We then tighten the lower bound on the minimax rate
tod−min{γ−p,s(p+1)}/poly(ln(d)). Based on the above results, we find that KRR is not minimax
optimal for s >1and for certain ranges of γ. Therefore, we show that the saturation effect of KRR
occurs in large dimensions.
3.1 Exact convergence rate on the excess risk of kernel gradient flow
We first state our main results in this paper.
Theorem 3.1 (Kernel gradient flow) .Lets >0andγ >0be fixed real numbers. Denote pas the
integer satisfying γ∈[p(s+ 1),(p+ 1)( s+ 1)) . Suppose that Assumption 1 and Assumption 2 hold
forsandγ. Let ˆfGF
tbe the function defined in (4). Then there exists t⋆>0, such that we have
EˆfGF
t⋆−f⋆2
L2X
= Θ P
d−min{γ−p,s(p+1)}
·poly(ln(d)), (12)
where ΘPonly involves constants depending on s, σ, γ, c 0, κ, c 1andc2.
7Theorem 3.1 is a direct corollary of Theorem 4.1 and Example 2. Combining with the previous
results in Proposition 2.3, or our modified minimax rate given in Theorem 3.3, we can conclude
that large-dimensional kernel gradient flow is minimax optimal for any s > 0and any γ > 0.
More importantly, the convergence rate of kernel gradient flow is faster than that of KRR given in
Proposition 2.2 when (i) 1< s≤2andγ∈(p(s+ 1) + 1 , p(s+ 1) + 2 s−1)for some p∈N, or
(ii)s >2andγ∈(p(s+ 1) + 1 ,(p+ 1)( s+ 1)) for some p∈N. Therefore, we have proved the
saturation effect of KRR in large dimensions.
Remark 3.2.When p≥1, the logarithm term poly(ln(d))in (12) can be removed. When p= 0, we
have poly (ln(d)) = (ln( d))2in (12). See Appendix D.4 for details.
3.2 Improved minimax lower bound
Recall that Proposition 2.3 gave a lower bound on the minimax rate as d−min{γ−p,s(p+1)}·d−ε. The
following theorem replaces the additional term d−ε(which has changed the convergence rate) into a
logarithm term poly−1(ln(d))(which does not change the desired convergence rate).
Theorem 3.3 (Improved minimax lower bound) .Lets >0andγ >0be fixed real numbers. Denote
pas the integer satisfying γ∈[p(s+ 1),(p+ 1)( s+ 1)) . LetPconsist of all the distributions ρon
X × Y such that Assumption 1 and Assumption 2 hold for sandγ. Then we have:
min
ˆfmax
ρ∈PE(X,Y)∼ρ⊗nˆf−f⋆2
L2= Ω
d−min{γ−p,s(p+1)}.
poly(ln(d)), (13)
where Ωonly involves constants depending on s, σ, γ, c 0, κ, c 1, and c2.
4 Exact convergence rate on the excess risk of spectral algorithms
In this section, we will give tight bounds on the excess risks of certain types of spectral algorithms,
such as kernel ridge regression, iterated ridge regression, kernel gradient flow, and kernel gradient
descent.
Given an analytic filter function φλ(·)with qualification τ≥1(refer to Appendix C for the definitions
of analytic filter function and its qualification), we can define a spectral algorithm in the following
way (see, e.g., Bauer et al. (2007)). For any y∈R, letKx:R→ H be given by Kx(y) =y·K(x,·),
whose adjoint K∗
x:H →Ris given by K∗
x(f) =⟨K(x,·), f⟩H=f(x). Moreover, we denote by
Tx=KxK∗
xandTX=1
nPn
i=1Txi. We also define the sample basis function
ˆgZ=1
nXn
i=1Kxi(yi) =1
nXn
i=1yi·K(xi,·). (14)
Now, the estimator of the spectral algorithm is defined by
ˆfλ=φλ(TX)ˆgZ. (15)
Many commonly used spectral algorithms can be constructed by certain analytic filter functions. We
provide two examples (kernel ridge regression and kernel gradient flow) as follows, and put two more
examples (iterated ridge regression and kernel gradient descent) in Appendix C. We provide rigorous
proof for these examples in Lemma C.3.
Example 1 (Kernel ridge regression) .The filter function of kernel ridge regression (KRR) is well-
known to be
φKRR
λ(z) =1
z+λ, ψKRR
λ(z) =λ
z+λ, τ = 1. (16)
Example 2 (Kernel gradient flow) .The filter function is
φGF
λ(z) =1−e−tz
z, ψGF
λ(z) =e−tz, t=λ−1, τ =∞. (17)
For any analytic filter function φλwith qualification τ≥1and the corresponding estimator of the
spectral algorithm defined in (15), the following two theorems provide exact convergence rates on the
excess risk when (i) the regression function is less-smooth, i.e., we have s≤τ, and (ii) s > τ , where
sis the source condition coefficient of the regression function given in Assumption 2.
8Theorem 4.1. Let0< s≤τandγ >0be fixed real numbers. Denote pas the integer satisfying
γ∈[p(s+ 1),(p+ 1)( s+ 1)) . Suppose that Assumption 1 and Assumption 2 hold for sandγ.
Letφλ(z)be an analytic filter function and ˆfλbe the function defined in (15). Suppose one of the
following conditions holds:
(i)τ=∞,(ii)s >1/(2τ),(iii)γ >((2τ+ 1)s)/(2τ(1 +s));
then there exists λ⋆>0, such that we have
Eˆfλ⋆−f⋆2
L2X
= Θ P
d−min{γ−p,s(p+1)}
·poly(ln(d)),
where ΘPonly involves constants depending on s, σ, γ, c 0, κ, c 1andc2.
Theorem 4.2. Lets > τ andγ > 0be fixed real numbers. Denote pas the integer satisfying
γ∈[p(s+ 1),(p+ 1)( s+ 1)) . Suppose that Assumption 1 and Assumption 2 hold for sandγ. Let
φλ(z)be an analytic filter function and ˆfλbe the function defined in (15). Define ˜s= min {s,2τ},
then there exists λ⋆>0, such that we have
Eˆfλ⋆−f⋆2
L2X
= Θ P
d−min{γ−p,τ(γ−p+1)+ p˜s
τ+1,˜s(p+1)}
·poly(ln(d)),
where ΘPonly involves constants depending on s, σ, γ, c 0, κ, c 1andc2. In addition, the convergence
rates of the generalization error can not be faster than above for any choice of regularization
parameter λ=λ(d, n)→0.
Remark 4.3.These theorems substantially generalize the results on exact generalization error bounds
of analytic spectral algorithms under the fixed-dimensional setting given in Li et al. (2024). Although
the “analytic functional argument” introduced in their proof is still vital for us to deal with the
general spectral algorithms, their proof has to rely on the polynomial eigendecay assumption that
λj≍j−β(Assumption 1), which does not hold in large dimensions since the hidden constant factors
in the assumption vary with d11 (Lu et al. (2023)). Hence, their proof is not easy to generalize to
large-dimensional spectral algorithms.
We provide some graphical illustrations of Theorem 4.1 and Theorem 4.2 in Figure 1 (with τ= 2)
and in Appendix A (with τ= 1,τ= 2,τ= 4, and τ=∞, corresponding to KRR, iterated ridge
regression in Example 3 and kernel gradient flow).
As a direct consequence of Theorem 3.3, Theorem 4.1, and Theorem 4.2, we find that for the spectral
algorithm with estimator defined in (15), it is minimax optimal if s≤τand the conditions in Theorem
4.1 hold. Moreover, these results show several phenomena for large-dimensional spectral algorithms.
Saturation effect of large-dimensional spectral algorithms with finite qualification. In the
large-dimensional setting and for the inner product kernel on the sphere, our results show that the
saturation effect of spectral algorithms occurs when s > τ . As shown in Figure 1(c) and Figure 1(d),
when s > τ , no matter how carefully one tunes the regularization parameter λ, the convergence rate
can not be faster than d−min{γ−p,τ(γ−p+1)+ p˜s
τ+1,˜s(p+1)}, thus can not achieve the minimax lower bound
d−min{γ−p,s(p+1)}.
Periodic plateau behavior of spectral algorithms when s≤2τ.When 0< s≤2τand
γ∈[p(s+ 1) + s+ max {s, τ}/τ−1,(p+ 1)( s+ 1)) for an integer p≥0, from Theorem 4.1 and
Theorem 4.2, the convergence rate on the excess risk of spectral algorithm d−s(p+1). The above rate
does not change when γvaries, which can also be found in Figure 1(b) and Figure 1(c). BIn other
words, if we fix a large dimension dand increase γ(or equivalently, increase the sample size n), the
optimal rate of excess risk of a spectral algorithm stays invariant in certain ranges. Therefore, in order
to improve the rate of excess risk, one has to increase the sample size above a certain threshold.
Polynomial approximation barrier of spectral algorithms when s→0.From Theorem 4.1,
when sis close to zero, the convergence rate d−min{γ−p,s(p+1)}is unchanged in the range γ∈
[p(s+ 1) + s,(p+ 1)( s+ 1)) , and increases in the short range γ∈[p(s+ 1), p(s+ 1) + s). In other
words, the excess risk of spectral algorithms will drop when γexceeds p(s+ 1)≈pfor any integer
pand will stay invariant for most of the other γ. We term the above phenomenon as the polynomial
approximation barrier of spectral algorithms (borrowed from Ghorbani et al. (2021)), and it can be
illustrated by Figure 1(a) with s= 0.01.
9Remark 4.4.Ghorbani et al. (2021) discovered the polynomial approximation barrier of KRR. As
shown by Figure 5 and Theorem 4 in Ghorbani et al. (2021), if s= 0and the true function falls into
L2= [H]0, then with high probability we have
EˆfKRR
λ⋆−f⋆2
L2
−P>pf⋆2
L2≤εf⋆2
L2+σ2
, (18)
where pis the integer satisfying γ∈[p, p+ 1) ,λ⋆is defined as in Theorem 4 in Ghorbani et al.
(2021), P>ℓmeans the projection onto polynomials with degree > ℓ, and εis any positive real
number. Notice that (18) implies that the excess risk of KRR will drop when γexceeds any integer
and will stay invariant for other γ, and is consistent with our results for spectral algorithms.
5 Conclusion
In this paper, we rigorously prove the saturation effect of KRR in large dimensions. Let s >0and
γ >0be fixed real numbers, denote pas the integer satisfying γ∈[p(s+ 1),(p+ 1)( s+ 1)) . Given
that the kernel is an inner product kernel defined on the sphere and that f⋆falls into the interpolation
space [H]s, we first show that the convergence rate on the excess risk of large-dimensional kernel
gradient flow is ΘP 
d−min{γ−p,s(p+1)}
·poly(ln(d))(Theorem 3.1), which is faster than that
of KRR given in Zhang et al. (2024). We then determine the improved minimax lower bound as
Ω 
d−min{γ−p,s(p+1)}
/poly(ln(d))(Theorem 3.3). Combining these results, we know that kernel
gradient flow is minimax optimal in large dimensions, and KRR is inferior to kernel gradient flow in
large dimensions. Our results suggest that previous results on large-dimensional KRR may not be
directly extendable to large-dimensional neural networks if the regression function is over-smooth.
In Section 4, we generalize our results to certain spectral algorithms. We determine the convergence
rate on the excess risk of large-dimensional spectral algorithms (Theorem 4.1 and Theorem 4.2). From
these results, we find several new phenomena exhibited in large-dimensional spectral algorithms,
including the saturation effect, the periodic plateau behavior, and the polynomial approximation
barrier.
In this paper, we only consider the convergence rate on the excess risk of optimal-tuned large-
dimensional spectral algorithms with uniform input distribution on a hypersphere. We believe that
several results in fixed-dimensional settings with input distribution on more general domains (e.g.,
Haas et al. (2024); Li et al. (2024)) can indeed be extended to large-dimensional settings, although we
must carefully consider the constants that depend on d. Furthermore, we believe that by considering
the learning curve of large-dimensional spectral algorithms (i.e., the convergence rate on the excess
risk of spectral algorithms with any regularization parameter λ >0) or the convergence rate on the
excess risk of large-dimensional kernel interpolation (i.e., KRR with λ= 0), further research can
find a wealth of new phenomena compared with the fixed-dimensional setting.
Acknowledgments and Disclosure of Funding
Lin’s research was supported in part by the National Natural Science Foundation of China (Grant
92370122, Grant 11971257). The authors are grateful to the reviewers for their constructive comments
that greatly improved the quality and presentation of this paper.
References
Aerni, M., M. Milanta, K. Donhauser, and F. Yang (2023). Strong inductive biases provably prevent
harmless interpolation. arXiv preprint arXiv:2301.07605 .
Arora, S., S. Du, W. Hu, Z. Li, and R. Wang (2019). Fine-grained analysis of optimization and
generalization for overparameterized two-layer neural networks. In International Conference on
Machine Learning , pp. 322–332. PMLR.
Arora, S., S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang (2019). On exact computation
with an infinitely wide neural net. Advances in Neural Information Processing Systems 32 .
Barzilai, D. and O. Shamir (2023). Generalization in kernel regression under realistic assumptions.
arXiv preprint arXiv:2312.15995 .
10Bauer, F., S. Pereverzev, and L. Rosasco (2007). On regularization algorithms in learning theory.
Journal of Complexity 23 (1), 52–72.
Caponnetto, A. (2006, September). Optimal rates for regularization operators in learning theory.
Technical Report CBCL Paper #264/AI Technical Report #062, Massachusetts Institute of Tech-
nology.
Caponnetto, A. and E. De Vito (2007). Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics 7 (3), 331–368.
Caponnetto, A. and Y . Yao (2010). Cross-validation based adaptation for regularization operators in
learning theory. Analysis and Applications 8 (02), 161–183.
Celisse, A. and M. Wahl (2021). Analyzing the discrepancy principle for kernelized spectral filter
learning algorithms. Journal of Machine Learning Research 22 (76), 1–59.
Cleveland, W. S. (1979). Robust locally weighted regression and smoothing scatterplots. Journal of
the American Statistical Association 74 (368), 829–836.
Cui, H., B. Loureiro, F. Krzakala, and L. Zdeborová (2021). Generalization error rates in kernel
regression: The crossover from the noiseless to noisy regime. Advances in Neural Information
Processing Systems 34 , 10131–10143.
Devlin, J. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding.
arXiv preprint arXiv:1810.04805 .
Dicker, L. H., D. P. Foster, and D. Hsu (2017). Kernel ridge vs. principal component regression: Mini-
max bounds and the qualification of regularization operators. Electronic Journal of Statistics 11 (1),
1022 – 1047.
Dieuleveut, A., N. Flammarion, and F. Bach (2017). Harder, better, faster, stronger convergence rates
for least-squares regression. Journal of Machine Learning Research 18 (101), 1–51.
Donhauser, K., M. Wu, and F. Yang (2021). How rotational invariance of common kernels prevents
generalization in high dimensions. In International Conference on Machine Learning , pp. 2804–
2814. PMLR.
Du, S., J. Lee, H. Li, L. Wang, and X. Zhai (2019). Gradient descent finds global minima of deep
neural networks. In International Conference on Machine Learning , pp. 1675–1685. PMLR.
Du, S. S., X. Zhai, B. Poczos, and A. Singh (2018). Gradient descent provably optimizes over-
parameterized neural networks. arXiv preprint arXiv:1810.02054 .
Edmunds, D. E. and H. Triebel (1996). Function Spaces, Entropy Numbers, Differential Operators .
Cambridge: Cambridge University Press.
Fischer, S. and I. Steinwart (2020). Sobolev norm learning rates for regularized least-squares
algorithms. Journal of Machine Learning Research 21 (205), 1–38.
Gallier, J. (2009). Notes on spherical harmonics and linear representations of lie groups. preprint .
Gerfo, L. L., L. Rosasco, F. Odone, E. D. Vito, and A. Verri (2008, 07). Spectral Algorithms for
Supervised Learning. Neural Computation 20 (7), 1873–1897.
Ghorbani, B., S. Mei, T. Misiakiewicz, and A. Montanari (2020). When do neural networks
outperform kernel methods? Advances in Neural Information Processing Systems 33 , 14820–
14830.
Ghorbani, B., S. Mei, T. Misiakiewicz, and A. Montanari (2021). Linearized two-layers neural
networks in high dimension. The Annals of Statistics 49 (2), 1029 – 1054.
Ghosh, N., S. Mei, and B. Yu (2021). The three stages of learning dynamics in high-dimensional
kernel methods. arXiv preprint arXiv:2111.07167 .
11Gneiting, T. (2013). Strictly and non-strictly positive definite functions on spheres. Bernoulli 19 (4),
1327 – 1349.
Haas, M., D. Holzmüller, U. Luxburg, and I. Steinwart (2024). Mind the spikes: Benign overfitting
of kernels and neural networks in fixed dimension. Advances in Neural Information Processing
Systems 36 .
He, K., X. Zhang, S. Ren, and J. Sun (2016). Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778.
Hu, H. and Y . M. Lu (2022). Sharp asymptotics of kernel ridge regression beyond the linear regime.
arXiv preprint arXiv:2205.06798 .
Hu, T., W. Wang, C. Lin, and G. Cheng (2021). Regularization matters: A nonparametric perspective
on overparametrized neural network. In International Conference on Artificial Intelligence and
Statistics , pp. 829–837. PMLR.
Jacot, A., F. Gabriel, and C. Hongler (2018). Neural tangent kernel: Convergence and generalization
in neural networks. Advances in Neural Information Processing Systems 31 .
Krizhevsky, A., I. Sutskever, and G. E. Hinton (2017). Imagenet classification with deep convolutional
neural networks. Communications of the ACM 60 (6), 84–90.
Lai, J., M. Xu, R. Chen, and Q. Lin (2023). Generalization ability of wide neural networks on R.
arXiv preprint arXiv:2302.05933 .
Li, Y ., W. Gan, Z. Shi, and Q. Lin (2024). Generalization error curves for analytic spectral algorithms
under power-law decay. arXiv preprint arXiv:2401.01599 .
Li, Y . and Y . Liang (2018). Learning overparameterized neural networks via stochastic gradient
descent on structured data. Advances in Neural Information Processing Systems 31 .
Li, Y ., Z. Yu, G. Chen, and Q. Lin (2024). On the eigenvalue decay rates of a class of neural-network
related kernel functions defined on general domains. Journal of Machine Learning Research 25 (82),
1–47.
Li, Y ., H. Zhang, and Q. Lin (2022). On the saturation effect of kernel ridge regression. In The
Eleventh International Conference on Learning Representations .
Li, Y ., H. Zhang, and Q. Lin (2024). On the asymptotic learning curves of kernel ridge regression
under power-law decay. Advances in Neural Information Processing Systems 36 .
Liang, T. and A. Rakhlin (2020). Just interpolate: Kernel “Ridgeless” regression can generalize. The
Annals of Statistics 48 (3), 1329 – 1347.
Liang, T., A. Rakhlin, and X. Zhai (2020). On the multiple descent of minimum-norm interpolants
and restricted lower isometry of kernels. In Conference on Learning Theory , pp. 2683–2711.
PMLR.
Lin, J., A. Rudi, L. Rosasco, and V . Cevher (2020). Optimal rates for spectral algorithms with
least-squares regression over hilbert spaces. Applied and Computational Harmonic Analysis 48 (3),
868–890.
Lu, W., H. Zhang, Y . Li, M. Xu, and Q. Lin (2023). Optimal rate of kernel regression in large
dimensions. arXiv preprint arXiv:2309.04268 .
Mei, S., T. Misiakiewicz, and A. Montanari (2021). Learning with invariances in random features
and kernel models. In Conference on Learning Theory , pp. 3351–3418. PMLR.
Mei, S., T. Misiakiewicz, and A. Montanari (2022). Generalization error of random feature and
kernel methods: Hypercontractivity and kernel matrix concentration. Applied and Computational
Harmonic Analysis 59 , 3–84.
12Mei, S. and A. Montanari (2022). The generalization error of random features regression: Precise
asymptotics and the double descent curve. Communications on Pure and Applied Mathemat-
ics 75 (4), 667–766.
Misiakiewicz, T. (2022). Spectrum of inner-product kernel matrices in the polynomial regime and
multiple descent phenomenon in kernel ridge regression. arXiv preprint arXiv:2204.10425 .
Misiakiewicz, T. and S. Mei (2022). Learning with convolution and pooling operations in kernel
methods. Advances in Neural Information Processing Systems 35 , 29014–29025.
Pillaud-Vivien, L., A. Rudi, and F. Bach (2018). Statistical optimality of stochastic gradient descent
on hard learning problems through multiple passes. Advances in Neural Information Processing
Systems 31 .
Raskutti, G., M. J. Wainwright, and B. Yu (2014). Early stopping and non-parametric regression: An
optimal data-dependent stopping rule. Journal of Machine Learning Research 15 (11), 335–366.
Rastogi, A. and S. Sampath (2017). Optimal rates for the regularized learning algorithms under
general source condition. Frontiers in Applied Mathematics and Statistics 3 , 3.
Steinwart, I. and A. Christmann (2008). Support vector machines . Springer Science & Business
Media.
Steinwart, I., D. Hush, and C. Scovel (2009). Optimal rates for regularized least squares regression.
InConference on Learning Theory , pp. 79–93. PMLR.
Steinwart, I. and C. Scovel (2012). Mercer’s theorem on general domains: On the interaction between
measures, kernels, and rkhss. Constructive Approximation 35 , 363–417.
Stone, C. J. (1977). Consistent Nonparametric Regression. The Annals of Statistics 5 (4), 595 – 620.
Stone, C. J. (1994). The Use of Polynomial Splines and Their Tensor Products in Multivariate
Function Estimation. The Annals of Statistics 22 (1), 118 – 171.
Suh, N., H. Ko, and X. Huo (2021). A non-parametric regression viewpoint: Generalization of
overparametrized deep relu network under noisy observations. In International Conference on
Learning Representations .
Xiao, L., H. Hu, T. Misiakiewicz, Y . M. Lu, and J. Pennington (2023). Precise learning curves and
higher-order scaling limits for dot product kernel regression. Journal of Statistical Mechanics:
Theory and Experiment 2023 (11), 114005.
Yang, Y . and A. Barron (1999). Information-theoretic determination of minimax rates of convergence.
The Annals of Statistics 27 (5), 1564 – 1599.
Yao, Y ., L. Rosasco, and A. Caponnetto (2007). On early stopping in gradient descent learning.
Constructive Approximation 26 , 289–315.
Zhang, H., Y . Li, W. Lu, and Q. Lin (2023). On the optimality of misspecified kernel ridge regression.
InInternational Conference on Machine Learning , pp. 41331–41353. PMLR.
Zhang, H., Y . Li, W. Lu, and Q. Lin (2024). Optimal rates of kernel ridge regression under source
condition in large dimensions. arXiv preprint arXiv:2401.01270 .
Zhang, H., W. Lu, and Q. Lin (2024). The phase diagram of kernel interpolation in large dimensions.
arXiv preprint arXiv:2404.12597 .
13A Graphical illustration and numerical experiments of main results
A.1 Graphical illustration of Theorem 3.1, Theorem 4.1, and Theorem 4.2
Recall that Theorem 3.1, Theorem 4.1, and Theorem 4.2 determined the convergence rate on the
excess risk of: (i) large-dimensional kernel gradient flow with s >0; (ii) large-dimensional spectral
algorithm with τ≥1ands≤τ; and (iii) large-dimensional spectral algorithm with τ≥1and
s > τ .
In Figure 1, we have provided a visual illustration of Theorem 4.1 and Theorem 4.2 when τ= 2.
Now, in Figure 2, we provide more visual illustrations of the results of spectral algorithms with
τ= 1,τ= 2,τ= 4, andτ=∞, which correspond to kernel ridge regression (KRR), iterated ridge
regression in Example 3, and kernel gradient flow.
(a)
 (b)
 (c)
 (d)
(e)
 (f)
 (g)
 (h)
(i)
 (j)
 (k)
 (l)
(m)
 (n)
 (o)
 (p)
Figure 2: Convergence rates of spectral algorithms with qualification τ= 1(KRR), τ= 2(iterated
ridge regression), τ= 4(iterated ridge regression), and τ=∞(kernel gradient flow) in Theorem
4.1, Theorem 4.2, and corresponding minimax lower rates in Theorem 3.3 with respect to dimension
d. We present four graphs corresponding to four kinds of source conditions: s= 0.01,1,3,5. The
x-axis represents asymptotic scaling, γ:n≍dγ; the y-axis represents the convergence rate of excess
risk,r:Excess risk ≍dr.
14A.2 Numerical experiments
We conducted two experiments using two specific kernels: the RBF kernel and the NTK kernel.
Experiment 1 was designed to confirm the optimal rate of kernel gradient flow and KRR when s= 1.
Experiment 2 was designed to illustrate the saturation effect of KRR when s >1.
Experiment 1: We consider the following two inner product kernels:
(i) RBF kernel with a fixed bandwidth:
Krbf(x, x′) = exp
−∥x−x′∥2
2
2
, x, x′∈Sd.
(ii) Neural Tangent Kernel (NTK) of a two-layer ReLU neural network:
Kntk(x, x′) := Φ( ⟨x, x′⟩), x, x′∈Sd,
where Φ(t) = [sin (arccos t) + 2( π−arccos t)t]/(2π).
The RBF kernel satisfies Assumption 1. For the NTK, the coefficients of Φ(·),{aj}∞
j=0, satisfy
aj>0, j∈ {0,1}∪{2,4,6, . . .}andaj= 0, j∈ {3,5,7, . . .}(see, e.g., Lu et al. (2023)). As noted
after Assumption 1, our results can be extended to inner product kernels with certain zero coefficients
aj. Specifically, for any γ >0, as long as aj>0forj=⌊γ⌋,⌊γ⌋+ 1, the proof and convergence
rate remain the same. Therefore, for γ <2in our experiments, the convergence rates for NTK will
be the same as for the RBF kernel.
We used the following data generation procedure:
yi=f∗(xi) +ϵi, i= 1, . . . , n,
where each xiis i.i.d. sampled from the uniform distribution on Sd, and ϵii.i.d.∼ N (0,1).
We selected the training sample sizes nwith corresponding dimensions dsuch that n=dγ, γ=
0.5,1.0,1.5,1.8. For each kernel and dimension d, we consider the following regression function f∗:
f∗(x) =K(u1, x) +K(u2, x) +K(u3, x),for some u1, u2, u3∈Sd. (19)
This function is in the RKHS H, and it is easy to prove that, for any u0∈Sd, Assumption 2 (b) holds
forK(u0,·)withs= 1. Therefore, Assumption 2 holds for s= 1. We used logarithmic least squares
to fit the excess risk with respect to the sample size, resulting in the convergence rate r. As shown in
Figure 3 and Figure 4, the experimental results align well with our theoretical findings.
Experiment 2: We use most of the settings from Experiment 1, except that the regression function
is changed to f∗(x) =p
µs
2N(d,2)P2(< ξ, x > )with s= 1.9,P2(t) := ( dt2−1)/(d−1)
the Gegenbauer polynomial, and ξ∈Sd. Notice that the addition formula P2(< ξ, x > ) =
1
N(d,2)PN(d,2)
j=1Y2,j(ξ)Y2,j(x)implies that
∥f∗∥2
[H]s=1
N(d,2)N(d,2)X
j=1Y2
2,j(ξ) =P2(1) = 1 ,
hence f∗∈[H]sand satisfies Assumption 2.
Our experiment settings are similar to those on page 30 of Li et al. (2022). We choose the regulariza-
tion parameter for KRR and kernel gradient flow as λ= 0.05·d−θ. For KRR, since Corollary D.16
suggests that the optimal regularization parameter is λ≍d−0.7, we set θ= 0.7. Similarly, based on
Corollary D.16, we set θ= 0.5for kernel gradient flow. Additionally, we set γ= 1.8. The results
indicate that the best convergence rate of KRR is slower than that of kernel gradient flow, implying
that KRR is inferior to kernel gradient flow when the regression function is sufficiently smooth.
B Proof of Theorem 3.3
We first restate Theorem 3.3.
152.00 2.05 2.10 2.15 2.20 2.25 2.30
log10 n2.10
2.05
2.00
1.95
1.90
1.85
1.80
1.75
1.70
log10 ErrNTK,  =0.5,  theretical rate=-1
KRR, parameters chosen by CV
log10Err  1.03log10n + 0.37
Kernel regression
log10Err  1.09log10n + 0.42
(a)
2.70 2.75 2.80 2.85 2.90 2.95 3.00
log10 n2.1
2.0
1.9
1.8
1.7
log10 ErrNTK,  =0.8,  theretical rate=-1
KRR, parameters chosen by CV
log10Err  0.84log10n + 0.38
Kernel regression
log10Err  0.84log10n + 0.59
 (b)
3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7
log10 n2.2
2.1
2.0
1.9
1.8
1.7
log10 ErrNTK,  =1.5,  theretical rate=-2/3
KRR, parameters chosen by CV
log10Err  0.63log10n + 0.23
Kernel regression
log10Err  0.66log10n + 0.21
(c)
3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7
log10 n2.0
1.9
1.8
1.7
1.6
log10 ErrNTK,  =1.8,  theretical rate=-5/9
KRR, parameters chosen by CV
log10Err  0.54log10n + 0.03
Kernel regression
log10Err  0.56log10n + 0.04
 (d)
Figure 3: Results of Experiment 1. We repeated each experiment 50 times and reported the average
excess risk for (a) kernel gradient flow (labeled as "kernel regression" in our reports) and (b) kernel
ridge regression (KRR) on 1000 test samples. We randomly selected u1, u2, u3and kept them
fixed for each repeat. We choose the stopping time tin kernel gradient flow as C1n0.5, where
C1∈ {0.001,0.01,0.1,1,10,100,1000}. We use 5-fold cross-validation to select the regularization
parameter λin kernel ridge regression. The alternative values of λin cross-validation are C2n−C3,
where C2∈ {0.001,0.005,0.01,0.1,0.5,1,2,5,10,40,100,300,1000}, C3∈ {0.1,0.2, . . . , 1.5}.
Theorem B.1 (Restate Theorem 3.3) .Lets >0andγ >0be fixed real numbers. Denote pas the
integer satisfying γ∈[p(s+ 1),(p+ 1)( s+ 1)) . LetPconsist of all the distributions ρonX × Y
such that Assumption 1 and Assumption 2 hold for sandγ. Then for any d≥C, a sufficiently large
constant only depending on s,γ,c1, and c2, we have the following claims:
(i) When γ∈(p(s+ 1), p+ps+s], we have
min
ˆfmax
ρ∈PE(X,Y)∼ρ⊗nˆf−f⋆2
L2≥ln ln( d)
50(γ−p(s+ 1))(ln( d))2dp−γ.
(ii) When γ∈(p+ps+s,(p+ 1)( s+ 1)] , we have
min
ˆfmax
ρ∈PE(X,Y)∼ρ⊗nˆf−f⋆2
L2= Ω
d−s(p+1)
,
where Ωonly involves constants depending on s, σ, γ, c 0, κ, c 1, and c2.
Proof of Theorem B.1. The item (ii) is a direct corollary of Theorem 5 in Zhang et al. (2024). Now
we begin to proof the item (i). We need the following lemma.
162.00 2.05 2.10 2.15 2.20 2.25 2.30
log10 n2.3
2.2
2.1
2.0
1.9
1.8
log10 ErrRBF,  =0.5,  theretical rate=-1
KRR, parameters chosen by CV
log10Err  1.23log10n + 0.65
Kernel regression
log10Err  1.31log10n + 0.73
(a)
2.70 2.75 2.80 2.85 2.90 2.95 3.00
log10 n2.40
2.35
2.30
2.25
2.20
2.15
2.10
2.05
log10 ErrRBF,  =0.8,  theretical rate=-1
KRR, parameters chosen by CV
log10Err  1.06log10n + 0.76
Kernel regression
log10Err  0.90log10n + 0.32
 (b)
3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7
log10 n2.8
2.7
2.6
2.5
2.4
2.3
2.2
2.1
2.0
log10 ErrRBF,  =1.5,  theretical rate=-2/3
KRR, parameters chosen by CV
log10Err  0.65log10n + -0.09
Kernel regression
log10Err  0.72log10n + -0.09
(c)
3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7
log10 n2.5
2.4
2.3
2.2
2.1
2.0
1.9
log10 ErrRBF,  =1.8,  theretical rate=-5/9
KRR, parameters chosen by CV
log10Err  0.59log10n + -0.13
Kernel regression
log10Err  0.61log10n + -0.22
 (d)
Figure 4: A similar plot as Figure 3, but with the RBF kernel.
Lemma B.2 (Restate Lemma 4.1 in Lu et al. (2023)) .For any δ∈(0,1)and any 0<˜ε1,˜ε2<∞
only depending on n,d,{λj},c1,c2, and γand satisfying
VK(˜ε2,D) +n˜ε2
2+ ln(2)
V2(˜ε1,B)≤δ, (20)
we have
min
ˆfmax
ρ∈PE(X,Y)∼ρ⊗nˆf−f⋆2
L2≥1−δ
4˜ε2
1, (21)
where ρf⋆is the joint-p.d.f. of x, ygiven by (1) with f=f⋆,B:=
f∈ H,∥f∥[H]s≤Rγ	
D:=
ρfjoint distribution of (y, x) where x∼ρX, y=f(x) +ϵ, ϵ∼N(0, σ2), f∈ B
,
andV2,VKare the ε-covering entropies ( as defined in Yang and Barron (1999); Lu et al. (2023)) of
(B, d2=∥ · ∥2
L2)and(D, d2=KL divergence ).
Suppose γ∈(p(s+ 1), p+ps+s]. LetC(p) =C12/10be a constant only depending on γ, where
C12are given in Lemma D.13. Then we introduce
˜ε2
1≜dp−γ/ln(d)and˜ε2
2≜C(p)dp
nln ln( d). (22)
173.0 3.1 3.2 3.3 3.4
log10 n4.5
4.0
3.5
3.0
2.5
2.0
1.5
log10 ErrNTK,  =1.8
Kernel gradient flow, =0.5
log10Err  0.88log10n + -1.37
KRR, =0.7
log10Err  0.52log10n + 0.09
Figure 5: Results of Experiment 2. It can be seen that the best rate of excess risk for KRR is slower
than that of kernel gradient flow.
Let us further assume that d≥C, where Cis a sufficiently large constant only depending on γ,s, and
c1. By Lemma D.11 and Lemma D.13 we have
˜ε2
1=dp−γ/ln(d)<C9
dps≤µs
p
µs
p+1<˜ε2
2=C(p)dp
nln ln( d)≤C(p)
c1dp−γln ln( d)< µs
p
n˜ε2
2Definition of C12≤1
10N(d, p) ln ln( d).(23)
Therefore, for any d≥C, where Cis a sufficiently large constant only depending on s,γ, andc1, we
have
V2(˜ε1,B)Lemma A.5 in Lu et al. (2023)
≥ K(˜ε1)≥1
2N(d, p) lnµs
p
˜ε2
1
Definition of ˜ε2
1≥1
2N(d, p) ln
C9dγ−p(s+1)ln(d)
≥1
2N(d, p)
(γ−p(s+ 1)) ln( d) +1
2ln ln( d)
.(24)
On the other hand, from Lemma D.11, Lemma D.13, and Lemma D.12, one can check the following
claim:
Claim 1. Suppose γ∈(p(s+ 1), p+ps+s]. For any d≥C, where Cis a sufficiently large
constant only depending on s,γ,c1, and c2, we have
K√
2σ˜ε2/6
≤1
2N(d, p) ln18µs
p
σ2˜ε2
2ln ln( d)
.
18Therefore, for any d≥C, where Cis a sufficiently large constant only depending on s,γ,c1, and c2,
we have
VK(˜ε2,D) =V2(√
2σ˜ε2,B)Lemma A.5 in Lu et al. (2023)
≤ K√
2σ˜ε2/6
Claim 1
≤1
2N(d, p) ln18µs
p
σ2˜ε2
2ln ln( d)
Definition of ˜ε2
2≤1
2N(d, p) ln
18C10σ−2[C(p)]−1c2dγ−p(s+1)
≤1
2N(d, p)
(γ−p(s+ 1)) ln( d) +1
5ln ln( d)
.(25)
Combining (23), (24), and (25), we finally have:
VK(˜ε2,D) +n˜ε2
2+ ln(2)
V2(˜ε1,B)≤[10(γ−p(s+ 1)) ln( d) + 4 ln ln( d)]
[10(γ−p(s+ 1)) ln( d) + 5 ln ln( d)]<1,
and from Lemma B.2, we get
min
ˆfmax
f⋆∈BE(X,y)∼ρ⊗n
f⋆ˆf−f⋆2
L2≥ln ln( d)
4 ln(d) [10( γ−p(s+ 1)) ln( d) + 5 ln ln( d)]dp−γ
≥ln ln( d)
50(γ−p(s+ 1))(ln( d))2dp−γ,
finishing the proof. ■
C Definition of analytic filter functions
We first introduce the following definition of analytic filter functions (Bauer et al. (2007); Li et al.
(2024)).
Definition C.1 (Analytic filter functions) .Let
φλ: [0, κ2]→R≥0|λ∈(0,1)	
be a family of
functions indexed with regularization parameter λand define the remainder function
ψλ(z) := 1 −zφλ(z). (26)
We say that {φλ|λ∈(0,1)}(or simply φλ(z)) is an analytic filter function if:
(1)zφλ(z)∈[0,1]is non-decreasing with respect to zand non-increasing with respect to λ.
(2)The qualification of this filter function is τ∈[1,∞]such that ∀0≤τ′≤τ(and also
τ′<∞), there exist positive constants Cionly depending on τ′,i= 1,2,3,4,5, such that
we have
φλ(z)≥C1z−1, ψ λ(z)≤C2(z/λ)−τ′,∀λ∈(0,1), z > λ (27)
C3≤λφλ(z)≤C4, ψ λ(z)≥C5,∀λ∈(0,1), z≤λ. (28)
(3)Ifτ <∞, then there exists a positive constant C6only depending on τandλ1, such that we
have
ψλ(λ1)≥C6λτ, (29)
where λ1is the largest eigenvalue of Kdefined in (5); and there exist positive constants C7
andC8only depending on τ, such that we have
(z/λ)2τψ2
λ(z)≥C7,∀λ∈(0,1), z > λ (30)
(z/λ)2τψ2
λ(z)≤C8zφλ(z),∀λ∈(0,1), z≤λ. (31)
(4) Let
Dλ=
z∈C: Rez∈[−λ/2, κ2],|Imz| ≤Rez+λ/2	
∪
z∈C:z−κ2≤κ2+λ/2,Rez≥κ2	
;
Then φλ(z)can be extended to be an analytic function on some domain containing Dλand
the following conditions holds for all λ∈(0,1):
19(C1)|(z+λ)φλ(z)| ≤˜Efor all z∈Dλ;
(C2)|(z+λ)ψλ(z)| ≤˜Fλfor all z∈Dλ;
where ˜E,˜Fare positive constants.
Remark C.2.We remark that some of the above properties are not essential for the definition of filter
functions in the literature (Bauer et al., 2007; Gerfo et al., 2008), but we introduce them to avoid
some unnecessary technicalities in the proof. The requirements of analytic filter functions are first
considered in Li et al. (2024) and used for their “analytic functional argument”, which will also be
vital in our proof.
The following examples show many commonly used analytic filter functions and their proofs can be
found in Lemma C.3, see also Li et al. (2024).
Example 3 (Iterated ridge regression) .Letq≥1be fixed. We define
φIT,q
λ(z) =1
z
1−λq
(z+λ)q
, ψIT,q
λ(z) =λq
(z+λ)q, τ =q. (32)
Example 4 (Kernel gradient descent) .The gradient descent method is the discrete version of gradient
flow. Let η >0be a fixed step size. Then, iterating gradient descent with respect to the empirical loss
tsteps yields the filter function
φGD
λ(z) =ηt−1X
k=0(1−ηz)k=1−(1−ηz)t
z, λ = (ηt)−1, (33)
ψGD
λ(z) = (1 −ηz)t, τ =∞. (34)
Moreover, when ηis small enough, say η <1/(2κ2), we have Re(1−ηz)>0forz∈Dλ, so we can
take the single-valued branch of (1−ηz)teven when tis not an integer. Therefore, we can extend
the definition of the filter function so that λcan be arbitrary and t= (ηλ)−1.
Lemma C.3. φKRR
λ,φIT,q
λ,φGF
λ, and φGD
λare analytic filter functions.
Proof. Notice that (i) z≤z+λ≤2zwhen z > λ ; and that (ii) λ≤z+λ≤2λwhen z≤λ.
Hence, the constants C1,C2,C3,C4, andC6are given in Li et al. (2024).
ForC5, when z≤λ, we can take C5= min {1/2,2−q, e−1, e−1}>0.
ForC7, when z > λ , we have
(z/λ)2τ(ψKRR
λ(z))2=z
z+λ2
≥1/4
(z/λ)2τ(ψIT,q
λ(z))2=z
z+λ2q
≥2−2q.
ForC8, when z≤λ, we have
z2τ−1(ψKRR
λ(z))2
λ2τφKRR
λ(z)=z
z+λ≤1
2
z2τ−1(ψIT,q
λ(z))2
λ2τφIT,q
λ(z)=z2q
(z+λ)2q−[λ(z+λ)]q≤1
22q−2q.
■
D Proof of Theorem 4.1 and Theorem 4.2
D.1 Bias-variance decomposition
We first apply a standard bias-variance decomposition on the excess risk of spectral algorithms, and
readers can also refer to Zhang et al. (2023, 2024) for more details.
20Recall the definition of ˆgZandˆfλin (14) and (15). Let’s define their conditional expectations as
˜gZ:=E(ˆgZ|X) =1
nnX
i=1Kxif⋆(xi)∈ H; (35)
and
˜fλ:=E
ˆfλ|X
=φλ(TX) ˜gZ∈ H. (36)
Let’s also define their expectations as
g=EˆgZ=Z
XK(x,·)f⋆(x)dρX(x)∈ H, (37)
and
fλ=φλ(T)g. (38)
Then we have the decomposition
ˆfλ−f⋆=1
nφλ(TX)nX
i=1Kxiyi−f⋆
=1
nφλ(TX)nX
i=1Kxi(f∗
ρ(xi) +ϵi)−f⋆
=φλ(TX) ˜gZ+1
nnX
i=1φλ(TX)Kxiϵi−f⋆
=
˜fλ−f⋆
+1
nnX
i=1φλ(TX)Kxiϵi. (39)
Taking expectation over the noise ϵconditioned on Xand noticing that ϵ|Xare independent noise
with mean 0 and variance σ2, we obtain the bias-variance decomposition:
Eˆfλ−f⋆2
L2X
=Bias2(λ) +Var(λ), (40)
where
Bias2(λ) :=˜fλ−f⋆2
L2,Var(λ) :=σ2
n2nX
i=1∥φλ(TX)K(xi,·)∥2
L2. (41)
Given the decomposition (40), we next derive the upper and lower bounds of Bias2(λ)andVar(λ)
in the following two subsections.
Before we close this subsection, let’s introduce some quantities and an assumption that will be used
frequently in our proof later. Denote the true function as f⋆=∞P
i=1fiϕi(x), let’s define the following
quantities:
N1,φ(λ) =∞X
j=1[λjφλ(λj)] ;N2,φ(λ) =∞X
j=1[λjφλ(λj)]2;
M1,φ(λ) = ess sup
x∈X∞X
j=1(ψλ(λj)fjϕj(x));M2,φ(λ) =∞X
j=1(ψλ(λj)fj)2;(42)
moreover, when φλ=φKRR
λ, we denote Nk(λ) =Nk,φKRR(λ)andMk(λ) =Mk,φKRR(λ)for simplic-
ity, where k= 1,2.
Assumption 3.Suppose that
ess sup
x∈X∞X
j=1[λjφλ(λj)]2ϕ2
j(x)≤ N 2,φ(λ); (43)
21and
ess sup
x∈X∞X
j=1[λjφλ(λj)]ϕ2
j(x)≤ N 1,φ(λ); (44)
and
ess sup
x∈X∞X
j=1
λjφKRR
λ(λj)
ϕ2
j(x)≤ N 1(λ). (45)
For simplicity of notations, we denote hx(·) =K(x,·),x∈ X in the rest of the proof. Moreover, we
denote Tλ:= (T+λ)−1andTXλ:= (TX+λ)−1.
D.2 Variance term
The following proposition rewrites the variance term using the empirical semi-norm.
Proposition D.1 (Restate Lemma 9 in Zhang et al. (2024)) .The variance term in (41) satisfies that
Var(λ) =σ2
nZ
X∥φλ(TX)hx(·)∥2
L2,ndρX(x). (46)
The operator form (46) allows us to apply concentration inequalities and establish the following
two-step approximation.
Z
X∥φλ(TX)hx∥2
L2,ndρX(x)A≈Z
X∥φλ(T)hx∥2
L2,ndρX(x)B≈Z
X∥φλ(T)hx∥2
L2dρX(x).
(47)
Approximation B The following lemma characterizes the magnitude of Approximation B in high
probability. Recall the definitions of N1,φ(λ)andN2,φ(λ)in (42).
Lemma D.2 (Approximation B) .Suppose that (43) in Assumption 3 holds. Then, for any fixed
δ∈(0,1), with probability at least 1−δ, we have
1
2Z
X∥φλ(T)hx∥2
L2dρX(x)−R2 (48)
≤Z
X∥φλ(T)hx∥2
L2,ndρX(x) (49)
≤3
2Z
X∥φλ(T)hx∥2
L2dρX(x) +R2, (50)
where
R2=5N2,φ(λ)
3nln2
δ. (51)
Proof. Define a function
f(z) =Z
X(φλ(T)hx(z))2dρX(x)
=Z
X∞X
j=1(λjφλ(λj))2ϕ2
j(x)ϕ2
j(z)dρX(x)
=∞X
j=1(λjφλ(λj))2ϕ2
j(z). (52)
Since (43) in Assumption 3 holds, we have
∥f∥L∞≤ N 2,φ(λ);∥f∥L1=N2,φ(λ).
22Applying Proposition 34 in Zhang et al. (2024) for√fand noticing that ∥√f∥L∞=p
∥f∥L∞=
N2,φ(λ)1
2, we have
1
2p
f2
L2−5N2,φ(λ)
3nln2
δ≤p
f2
L2,n≤3
2p
f2
L2+5N2,φ(λ)
3nln2
δ, (53)
with probability at least 1−δ.
On the one hand, we have
p
f2
L2,n=Z
Xf(z)dPn(z) =Z
XZ
X(φλ(T)hx(z))2dρX(x)
dPn(z)
=Z
XZ
X(φλ(T)hx(z))2dPn(z)
dρX(x)
=Z
X∥φλ(T)hx∥2
L2,ndρX(x).
On the other hand, we have
p
f2
L2=Z
Xf(z)dρX(z)
=Z
XZ
X(φλ(T)hx(z))2dρX(x)
dρX(z)
=Z
X∥φλ(T)hx∥2
L2dρX(x).
Therefore, (53) implies the desired results. ■
Approximation A
Lemma D.3. Suppose that (43) and (45) in Assumption 3 hold. Suppose that there exists a constant ϵ
only depending on sandγ, such that λ=λ(n, d)satisfies nϵ−1N1(λ)→0. Then there exists an
absolute constant C1, such that for any fixed δ∈(0,1), when nis sufficiently large, with probability
at least 1−δ, we have
Z
X∥φλ(TX)hx∥2
L2,ndρX(x)−Z
X∥φλ(T)hx∥2
L2,ndρX(x)(54)
≤C1q
N2,φ(λ) +C1p
vN1(λ) lnλ−1
·p
vN1(λ) lnλ−1, (55)
where v=N1(λ)
nlnn.
Remark D.4.The proof of Lemma D.3 is mainly based on Lemma 4.18 in Li et al. (2024). Notice
that we replace the Assumption 2 in Li et al. (2024) by (45) in Assumption 3 (borrowed from Zhang
et al. (2024)), since both of them can deduce same results given by Lemma 4.2 in Li et al. (2024) or
Lemma 37 in Zhang et al. (2024).
Proof. We start with
D=|∥φλ(TX)hx∥L2− ∥φλ(T)hx∥L2| ≤T1
2[φλ(T)−φλ(TX)]hx
H.
Using operator calculus, we get
T1
2[φλ(T)−φλ(TX)]hx
=T1
21
2πiI
ΓλRTX(z)(T−TX)RT(z)φλ(z)dz
hx
=1
2πiI
ΓλT1
2(TX−z)−1(T−TX)(T−z)−1hxφλ(z)dz
=1
2πiI
ΓλT1
2T−1
2
λ·T1
2
λ(TX−z)−1T1
2
λ·T−1
2
λ(T−TX)T−1
2
λ·T1
2
λ(T−z)−1T1
2
λ·T−1
2
λhxφλ(z)dz.
23Therefore, taking the norms yields
D≤1
2πT1
2T−1
2
λ·T1
2
λ(TX−z)−1T1
2
λ·T−1
2
λ(T−TX)T−1
2
λ·T1
2
λ(T−z)−1T1
2
λ
·T−1
2
λhx
HI
Γλ|φλ(z)dz|
=1
2π·I·II·III·IV·V·I
Γλ|φλ(z)dz|
≤1
2π·1·√
6C·r
N1(λ)
nlnn·C·p
N1(λ)I
Γλ|φλ(z)dz|,
where in the second estimation, we use ( I) operator calculus, ( IIandIV) Proposition E.8, ( III)
Lemma E.7, and ( V) Lemma 37 in Zhang et al. (2024) for each term respectively. Finally, from (63)
in Li et al. (2024), we getI
Γλ|φλ(z)dz| ≤Clnλ−1, (56)
and thus there exists an absolute constant C1, such that we have
D=|∥φλ(TX)hx∥L2− ∥φλ(T)hx∥L2| ≤C1p
vN1(λ) lnλ−1.
On the other hand, combining (52) and (43) in Assumption 3, we have ∥φλ(T)hx∥2
L2≤ N 2,φ(λ),
and hence
∥φλ(TX)hx∥L2+∥φλ(T)hx∥L2≤2∥φλ(T)hx∥L2+D
≤q
N2,φ(λ) +C1p
vN1(λ) lnλ−1.
Finally,∥φλ(TX)hx∥2
L2− ∥φλ(T)hx∥2
L2
=|∥φλ(TX)hx∥L2− ∥φλ(T)hx∥L2|(∥φλ(TX)hx∥L2+∥φλ(T)hx∥L2)
≤C1q
N2,φ(λ) +C1p
vN1(λ) lnλ−1
·p
vN1(λ) lnλ−1,
and henceZ
X∥φλ(TX)hx∥2
L2,ndρX(x)−Z
X∥φλ(T)hx∥2
L2,ndρX(x)
≤1
nnX
i=1∥φλ(TX)hxi∥2
L2− ∥φλ(T)hxi∥2
L2
≤sup
x∈X∥φλ(TX)hx∥2
L2− ∥φλ(T)hx∥2
L2
≤C1q
N2,φ(λ) +C1p
vN1(λ) lnλ−1
·p
vN1(λ) lnλ−1,
■
Final proof of the variance term Now we are ready to state the theorem about the variance term.
Theorem D.5. Suppose that (43) and (45) in Assumption 3 hold. Suppose there exists a constant
ϵ >0only depending on sandγ, such that λ=λ(n, d)satisfies
N1(λ)·nϵ−1→0, (57)
N2
1(λ)
nN2,φ(λ)·ln(n)(lnλ−1)2→0; (58)
then we have
Var(λ) = [1 + oP(1)]σ2
nN2,φ(λ). (59)
24Proof. Recall that Var(λ) =σ2
nR
X∥φλ(TX)hx∥2
L2,ndρX(x). Hence, when nis large enough,
with probability at least 1−δwe have
Z
X∥φλ(TX)hx∥2
L2,ndρX(x)−Z
∥φλ(T)hx∥2
L2dρX(x)
≤Z
X∥φλ(TX)hx∥2
L2,ndρX(x)−Z
X∥φλ(T)hx∥2
L2,ndρX(x)
+Z
X∥φλ(T)hx∥2
L2,ndρX(x)−Z
X∥φλ(T)hx∥2
L2dρX(x)
Lemma D.2
≤Z
X∥φλ(TX)hx∥2
L2,ndρX(x)−Z
X∥φλ(T)hx∥2
L2,ndρX(x)+5N2,φ(λ)
3nln2
δ
Lemma D.3
≤q
N2,φ(λ)·C1p
vN1(λ) lnλ−1+C2
1vN1(λ)(lnλ−1)2
+5N2,φ(λ)
3nln2
δ
Definition of v=r
N2,φ(λ)
nN1(λ)·C1p
ln(n) lnλ−1+N2
1(λ)
n·C2
1ln(n)(lnλ−1)2+N2,φ(λ)
n·5
3ln2
δ
=I·C1p
ln(n) lnλ−1+II·C2
1ln(n)(lnλ−1)2+III·5
3ln2
δ.
When n≥C, a sufficiently large constant only depending on γandC1, we have
I·C1p
ln(n) lnλ−1≤1
6N2,φ(λ).
Furthermore, whenN2
1(λ)
nN2,φ(λ)·nϵ→0, we have I·C1p
ln(n) lnλ−1/N2,φ(λ)→0andII·
C2
1ln(n)(lnλ−1)2/N2,φ(λ)→0.
Finally, from (52) we have
∥φλ(T)hx∥2
L2=∞X
i=1(λjφλ(λj))2ϕ2
i(z),
and thus the deterministic term writes
Z
X∥φλ(T)hx∥2
L2dρX(x) =N2,φ(λ).
■
D.3 Bias term
In this subsection, our goal is to determine the upper and lower bounds of bias under some approxi-
mation conditions.
The triangle inequality implies that
Bias(λ) =˜fλ−f⋆
L2≥ ∥fλ−f⋆∥L2−˜fλ−fλ
L2
Bias(λ)≤ ∥fλ−f⋆∥L2+˜fλ−fλ
L2.(60)
The following lemma characterizes the dominant term of Bias(λ).
Lemma D.6. For any λ >0, we have
∥fλ−f⋆∥L2=M2,φ(λ)1
2. (61)
25Proof. We have
∥fλ−f⋆∥2
L2=∞X
i=1λiφλ(λi)fiϕi(x)−∞X
i=1fiϕi(x)2
L2
=∞X
i=1ψλ(λi)fiϕi(x)2
L2
=∞X
i=1(ψλ(λi)fi)2
=M2,φ(λ).
■
The following lemma bounds the remainder term of Bias(λ)when s≥1.
Lemma D.7. Suppose that (45) in Assumption 3 holds. Suppose that there exist constants ϵandC
only depending on sandγ, such that λ=λ(n, d)satisfies
nϵ−1N1(λ)→0, (62)
N1(λ)M2
1,φ(λ)
n2=o
M2,φ(λ) +σ2
nN2,φ(λ)
, (63)
N1(λ)
nln(n)(lnλ−1)2·∞X
j=1λ2λiφ2
λ(λi)
λ+λif2
i=o
M2,φ(λ) +σ2
nN2,φ(λ)
; (64)
then we have
˜fλ−fλ2
L2=oP
M2,φ(λ) +σ2
nN2,φ(λ)
. (65)
Proof. Do the decomposition,
˜fλ−fλ=φλ(TX)˜gX−(ψλ(TX) +φλ(TX)TX)fλ
=φλ(TX)(˜gX−TXfλ)−ψλ(TX)Tφλ(T)f⋆
=φλ(TX)(˜gX−TXfλ)−φλ(TX)ψλ(T)g+φλ(TX)ψλ(T)g−ψλ(TX)Tφλ(T)f⋆
=φλ(TX) [˜gX−TXfλ−ψλ(T)g] + [φλ(TX)ψλ(T)Tf⋆−ψλ(TX)Tφλ(T)f⋆]
=φλ(TX)(˜gX−TXfλ−g+Tfλ) + (φλ(TX)Tψλ(T)−ψλ(TX)Tφλ(T))f⋆
=I+II.
(66)
Bound on I:For the first term in (66), we have
∥I∥L2=∥φλ(TX)(˜gX−TXfλ−g+Tfλ)∥L2
=T1
2φλ(TX)(˜gX−TXfλ−g+Tfλ)
H
≤T1
2T−1
2
λ·T1
2
λφλ(TX)T1
2
λ·T−1
2
λ[(˜gX−TXfλ)−(g−Tfλ)]
H
(72) in Zhang et al. (2024)
≤T1
2
λφλ(TX)T1
2
λ·T−1
2
λ[(˜gX−TXfλ)−(g−Tfλ)]
H
Proposition E.1
≤ 4T1
2
λT−1
XλT1
2
λ·T−1
2
λ[(˜gX−TXfλ)−(g−Tfλ)]
H
(62) and(73) in Zhang et al. (2024)
≤ 12T−1
2
λ[(˜gX−TXfλ)−(g−Tfλ)]
H,
26Denote ξi=ξ(xi) =T−1
2
λ(Kxif⋆(xi)−Txifλ). To use Bernstein inequality, we need to bound the
m-th moment of ξ(x):
E∥ξ(x)∥m
H=ET−1
2
λKx(f⋆−fλ(x))m
H
≤ET−1
2
λK(x,·)m
HE 
|(f⋆−fλ(x))|mx
. (67)
Note that Lemma 37 in Zhang et al. (2024) shows that
T−1
2
λK(x,·)
H≤ N 1(λ)1
2, µ-a.e.x∈ X;
By definition of M1,φ(λ), we also have
∥fλ−f⋆∥L∞=∞X
i=1ψλ(λi)fiϕi(x)
L∞=M1,φ(λ). (68)
In addition, we have proved in Lemma D.6 that
E|(fλ(x)−f⋆(x))|2=M2,φ(λ).
So we get the upper bound of (67), i.e.,
(67)≤ N 1(λ)m
2· ∥fλ−f⋆∥m−2
L∞·E|(fλ(x)−f⋆(x))|2
=N1(λ)m
2M1,φ(λ)m−2M2,φ(λ)
=
N1(λ)1
2M1,φ(λ)m−2
N1(λ)1
2M2,φ(λ)1
22
.
Using Lemma 36 in Zhang et al. (2024) with therein notations: L=N1(λ)1
2M1,φ(λ)andσ=
N1(λ)1
2M2,φ(λ)1
2, for any fixed δ∈(0,1), with probability at least 1−δ, we have
∥I∥L2≤12·4√
2 log2
δ 
N1(λ)1
2M1,φ(λ)
n+N1(λ)1
2M2,φ(λ)1
2√n!
. (69)
Bound on II:For the second term in (66), we have
∥II∥L2=∥(φλ(TX)Tψλ(T)−ψλ(TX)Tφλ(T))f⋆∥L2
≤T1
2(φλ(TX)Tψλ(T)−ψλ(T)Tφλ(T))f⋆
H
+T1
2(ψλ(TX)Tφλ(T)−ψλ(T)Tφλ(T))f⋆
H.(70)
For the first term in (70), we still employ the analytic functional argument:
T1
2(φλ(TX)Tψλ(T)−ψλ(T)Tφλ(T))f⋆
=T1
2(φλ(TX)−φλ(T))Tψλ(T)f⋆
=1
2πiI
ΓλT1
2(TX−z)−1(TX−T)(T−z)−1φλ(z)Tψλ(T)f⋆dz
=1
2πiI
ΓλT1
2T−1
2
λ·T1
2
λ(TX−z)−1T1
2
λ·T−1
2
λ(T−TX)T−1
2
λ
·T1
2
λ(T−z)−1T1
2
λ·T−1
2
λT1
2·T1
2ψλ(T)f⋆φλ(z)dz.
27Therefore,
2π∥T1
2(φλ(TX)Tψλ(T)−ψλ(T)Tφλ(T))f⋆∥H
≤I
ΓλT1
2T−1
2
λ·T1
2
λ(TX−z)−1T1
2
λ·T−1
2
λ(T−TX)T−1
2
λ
·T1
2
λ(T−z)−1T1
2
λ·T−1
2
λT1
2·T1
2ψλ(T)f⋆
H|φλ(z)dz|
(72) in Zhang et al. (2024)
≤I
ΓλT1
2
λ(TX−z)−1T1
2
λ·T−1
2
λ(T−TX)T−1
2
λ
·T1
2
λ(T−z)−1T1
2
λ·T1
2ψλ(T)f⋆
H|φλ(z)dz|
(45) and Proposition E.8
≤√
6C2I
ΓλT−1
2
λ(T−TX)T−1
2
λ
·T1
2ψλ(T)f⋆
H|φλ(z)dz|
Lemma E.7
≤√
6C2√vI
ΓλT1
2ψλ(T)f⋆
H|φλ(z)dz|
Definition of M2,φ(λ)=√
6C2√vM1/2
2,φ(λ)I
Γλ|φλ(z)dz|
(56)
≤√
6C3√vM1/2
2,φ(λ) lnλ−1,(71)
where v=N1(λ)
nlnn.
For the second term in (70), we have
T1
2(ψλ(TX)Tφλ(T)−ψλ(T)Tφλ(T))f⋆
=T1
21
2πiI
ΓλRTX(z)(T−TX)RT(z)ψλ(z)dz
Tφλ(T)f⋆
=1
2πiI
ΓλT1
2(TX−z)−1(T−TX)(T−z)−1ψλ(z)Tφλ(T)f⋆dz
=1
2πiZ
ΓλT1
2T−1
2
λ·T1
2
λ(TX−z)−1T1
2
λ·T−1
2
λ(T−TX)T−1
2
λ
·T1
2
λ(T−z)−1T1
2
λ·T−1
2
λTφλ(T)f⋆ψλ(z)dz.
Hence, similar to (71), we have
2πT1
2(ψλ(TX)Tφλ(T)−ψλ(T)Tφλ(T))f⋆
H
≤Z
ΓλT1
2T−1
2
λ·T1
2
λ(TX−z)−1T1
2
λ·T−1
2
λ(T−TX)T−1
2
λ
·T1
2
λ(T−z)−1T1
2
λ·T−1
2
λTφλ(T)f⋆
H|ψλ(z)dz|
≤√
6C2√vT−1
2
λTφλ(T)f⋆
HZ
Γλ|ψλ(z)dz|
Definition of analytic filter functions
≤√
6C2√vT−1
2
λTφλ(T)f⋆
HC˜Fλlnλ−1.(72)
28Combining (66), (69), (70), (71), and (72), there exists a constant C1only depending on δand˜F,
such that we have˜fλ−fλ
L2
≤C1 
N1(λ)1
2M1,φ(λ)
n+N1(λ)1
2M2,φ(λ)1
2√n!
+C1√vM1/2
2,φ(λ) lnλ−1+C1√vT−1
2
λTφλ(T)f⋆
Hλlnλ−1
(62)
≤ 
n−1N1(λ)1/2·C1C1/2·(M2,φ(λ))1/2
+ 
n−1N1(λ)1/2·C1·(M2,φ(λ))1/2
+ 
nϵ−1N1(λ)1/2·C1·(M2,φ(λ))1/2
+o
M2,φ(λ) +σ2
nN2,φ(λ)1/2
.(73)
■
When s <1, we can use the following lemma to bound the remainder term of Bias(λ). This lemma
is a modification of Lemma D.7, and its proof is partly based on Lemma 26 in Zhang.
Lemma D.8. Suppose that (45) in Assumption 3 holds. Suppose that there exist constants ϵandC
only depending on sandγ, such that λ=λ(n, d)satisfies
nϵ−1N1(λ)→0, (74)
N1(λ)
nln(n)(lnλ−1)2·∞X
j=1λ2λiφ2
λ(λi)
λ+λif2
i≪
M2,φ(λ) +σ2
nN2,φ(λ)
; (75)
n−1N1(λ)1
2
∥fλ∥L∞+n1−s
2+ϵ
=o
M2,φ(λ) +σ2
nN2,φ(λ)1/2
; (76)
then we have˜fλ−fλ2
L2=oP
M2,φ(λ) +σ2
nN2,φ(λ)
. (77)
Proof. Similar to the proof in Lemma D.7, we have the decomposition ˜fλ−fλ=I+II, with
∥I∥2
L2≤122T−1
2
λ[(˜gX−TXfλ)−(g−Tfλ)]2
H,
∥II∥2
L2=o
M2,φ(λ) +σ2
nN2,φ(λ)
.
Denote ξi=ξ(xi) =T−1
2
λ(Kxif⋆(xi)−Txifλ). Further consider the subset Ω1={x∈ X :
|f⋆(x)| ≤t}andΩ2=X\Ω1, where twill be chosen appropriately later. Decompose ξias
ξiIxi∈Ω1+ξiIxi∈Ω2and we have the following decomposition:
T−1
2
λ[(˜gX−TXfλ)−(g−Tfλ)]
H=1
nnX
i=1ξi−Eξx
H(78)
≤1
nnX
i=1ξiIxi∈Ω1−EξxIx∈Ω1
H+∥1
nnX
i=1ξiIxi∈Ω2∥H+∥EξxIx∈Ω2∥H
:=I+II+III. (79)
Next we choose t=n1−s
2+ϵt, q=2
1−s−ϵqsuch that
ϵt< ϵ;and1−s
2+ϵt>1/2
1−s−ϵq
. (80)
29Then we can bound the three terms in (78) as follows:
(i)For the first term in (78), denoted as I, notice that
∥(fλ−f⋆)Ixi∈Ω1∥L∞≤ ∥fλ∥L∞+n1−s
2+ϵt. (81)
Imitating (67) in the proof of Lemma D.7, we have
I=oP
M2,φ(λ) +σ2
nN2,φ(λ)1/2
. (82)
(ii)For the second term in (78), denoted as II. Since q=2
1−s−ϵq<2
1−s, Lemma 42 in Zhang et al.
(2024) shows that,
[H]s,→Lq(X, µ), (83)
with embedding norm less than a constant Cs,κ. Then Assumption 2 (a) implies that there exists
0< C q<∞only depending on γ, sandκsuch that ∥f⋆∥Lq(X,µ)≤Cq. Using the Markov
inequality, we have
P(x∈Ω2) =P
|f⋆(x)|> t
≤E|f⋆(x)|q
tq≤(Cq)q
tq.
Further, since (80) guarantees tq≫n, we have
τn:=P(II>0) (84)
≤P
∃xis.t.xi∈Ω2,
= 1−P
xi/∈Ω2,∀xi, i= 1,2,···, n
= 1−P
x /∈Ω2n
= 1−P
|f⋆(x)| ≤tn
≤1−
1−(Cq)q
tqn
→0. (85)
(iii)For the third term in (78), denoted as III. Since Lemma 37 in Zhang et al. (2024) implies that
∥T−1
2
λk(x,·)∥H≤ N 1(λ)1
2, µ-a.e.x∈ X,so
III≤E∥ξxIx∈Ω2∥H≤Eh
∥T−1
2
λk(x,·)∥H· 
f⋆−fλ(x)
Ix∈Ω2i
≤ N 1(λ)1
2E 
f⋆−fλ(x)
Ix∈Ω2
≤ N 1(λ)1
2∥f⋆−fλ∥1
2
L2·P(x∈Ω2)1
2
≤ N 1(λ)1
2M2,φ(λ)1
2t−q
2, (86)
where we use Cauchy-Schwarz inequality for the third inequality and Lemma D.6 for the fourth
inequality. Recalling that the choices of t, qsatisfy t−q≪n−1and we have assumed nϵ−1N1(λ)→
0, we have
III=o
M2,φ(λ)1
2
. (87)
Plugging (82), (84) and (87) into (78), we finish the proof. ■
Final proof of the bias term Now we are ready to state the theorem about the bias term.
Theorem D.9 (s≥1).Suppose that (45) in Assumption 3 holds. Suppose that there exist constants ϵ
andConly depending on sandγ, such that λ=λ(n, d)satisfies
nϵ−1N1(λ)→0,
N1(λ)M2
1,φ(λ)
n2≪
M2,φ(λ) +σ2
nN2,φ(λ)
,
N1(λ)
nln(n)(lnλ−1)2·∞X
j=1λ2λiφ2
λ(λi)
λ+λif2
i≪
M2,φ(λ) +σ2
nN2,φ(λ)
;
then we haveBias2(λ)− M 2,φ(λ)=oP
M2,φ(λ) +σ2
nN2,φ(λ)
. (88)
30Theorem D.10 (s <1).Suppose that (45) in Assumption 3 holds. Suppose that there exist constants
ϵandConly depending on sandγ, such that λ=λ(n, d)satisfies
nϵ−1N1(λ)→0,
N1(λ)
nln(n)(lnλ−1)2·∞X
j=1λ2λiφ2
λ(λi)
λ+λif2
i≪
M2,φ(λ) +σ2
nN2,φ(λ)
;
n−1N1(λ)1
2
∥fλ∥L∞+n1−s
2+ϵ
=o
M2,φ(λ) +σ2
nN2,φ(λ)1/2
;
then we haveBias2(λ)− M 2,φ(λ)=oP
M2,φ(λ) +σ2
nN2,φ(λ)
. (89)
D.4 Quantity calculations and conditions verification for the inner product kernels
In the previous two sections, we have successfully bounded the bias and the variance terms by the
quantities M2,φ(λ)andN2,φ(λ). In this subsection, we will focus on the inner product kernels on
the sphere. We will (i) determine the rates for the above quantities, and (ii) verify all the conditions
in Theorem D.5, Theorem D.9 and Theorem D.10.
Recall that µkandN(d, k), defined in (9), are the eigenvalues of the inner product kernel Kdefined
on the sphere and the corresponding multiplicity. The following three lemmas (mainly cited from Lu
et al. (2023)) give concise characterizations of µkandN(d, k), which is sufficient for the analysis in
this paper.
Lemma D.11. For any fixed integer p≥0, there exist constants C,C9andC10only depending on p
and{aj}j≤p+1, such that for any d≥C, we have
C9d−k≤µk≤C10d−k, k= 0,1,···, p+ 1. (90)
Lemma D.12. For any fixed integer p≥0, there exist constants Conly depending on pand
{aj}j≤p+1, such that for any d≥C, we have
µk≤C10
C9d−1µp, k =p+ 1, p+ 2,···
where C9andC10are constants given in Lemma D.11.
Lemma D.13. For any fixed integer p≥0, there exist constants C11,C12andConly depending on p,
such that for any d≥C, we have
C11dk≤N(d, k)≤C12dk, k = 0,1,···, p+ 1. (91)
With these lemmas, we can begin to bound the quantities M2,φ(λ)andN2,φ(λ).
Lemma D.14. Suppose that Assumption 1 and Assumption 2 hold for sand an integer p. Suppose
ℓ≤p,t=λ−1∈(dℓ, dℓ+1]. Then we have the following bound.
M2,φ(λ) =

Θ 
d−s(ℓ+1)
τ=∞
Θ 
t−2τdℓ(2τ−s)+d−s(ℓ+1)
s≤2τ <∞
Θ 
λ2τ
s >2τ
N2,φ(λ)
n= Θdℓ
n+t2
ndℓ+1
∞X
k=0λ2µkφ2
λ(µk)
λ+µkN(d,k)X
j=1f2
k,j=O
λ2dmax{p(2−s),0}+d−s(ℓ+1)
;(92)
and thus Assumption 3 holds. Moreover, when s≥1, We have
M2
1,φ(λ) =

O 
d−(ℓ+1)(s−1)
τ=∞
O 
λ2τ−1dℓ(2τ−s)+d−(ℓ+1)(s−1)
s≤2τ <∞
O 
λ2τ−1
s >2τ(93)
31Proof. I.We begin with M2,φ(λ). Ifs≤2τandτ <∞, then we have
M2,φ(λ) =∞X
k=0ψ2
λ(µk)N(d,k)X
j=1f2
k,j
≤ℓX
k=0C2
2(tµk)−2τ(µk)sN(d,k)X
j=1(µk)−sf2
k,j+∞X
k=ℓ+1ψ2
λ(µk)N(d,k)X
j=1f2
k,j
≤ℓX
k=0C2
2(tµk)−2τ(µk)sN(d,k)X
j=1(µk)−sf2
k,j+∞X
k=ℓ+1(µk)sN(d,k)X
j=1(µk)−sf2
k,j
≤C2
2t−2τ(C9d−ℓ)s−2τℓX
k=0N(d,k)X
j=1(µk)−sf2
k,j+ (C10d−ℓ−1)s∞X
k=ℓ+1N(d,k)X
j=1(µk)−sf2
k,j
=O
t−2τdℓ(2τ−s)+d−s(ℓ+1)
;
and when τ=∞, a similar argument ( taking τ′< τand let τ′→ ∞ , then we have (td−ℓ)−2τ′→0)
shows that M2,φ(λ) =O(d−s(ℓ+1)).
Similarly, if s≤2τ, then we have
M2,φ(λ)≥1{τ <∞}ℓX
k=0C2
7(tµk)−2τ(µk)sN(d,k)X
j=1(µk)−sf2
k,j
+∞X
k=ℓ+1ψ2
λ(µk)N(d,k)X
j=1f2
k,j
≥1{τ <∞}Ω
t−2τdℓ(2τ−s)
+∞X
k=ℓ+1C2
5(µk)sN(d,k)X
j=1(µk)−sf2
k,j
≥1{τ <∞}Ω
t−2τdℓ(2τ−s)
+C2
5(C10d−ℓ−1)sN(d,ℓ)X
j=1(µℓ)−sf2
ℓ,j
=1{τ <∞}Ω
t−2τdℓ(2τ−s)
+ Ω
d−s(ℓ+1)
.
If2τ < s , then
M2,φ(λ) =∞X
k=0ψ2
λ(µk)N(d,k)X
j=1f2
k,j
Lemma E.3
≤ κ2(s−2τ)λ2τ∞X
k=0N(d,k)X
j=1µ−s
kf2
k,j
=O 
λ2τ
.
Similarly, if 2τ < s , then we have
M2,φ(λ)≥ψ2
λ(µ0)f2
0,1
≥C2
6f2
0,1·λ2τ
= Ω 
λ2τ
.
32II.Now let’s bound the second term N2,φ(λ)/n. We have
N2,φ(λ)
n=1
n∞X
k=0N(d, k) [µkφλ(µk)]2
≤1
nℓX
k=0N(d, k) +1
n∞X
k=ℓ+1N(d, k) [µkφλ(µk)]2
≤1
nℓX
k=0N(d, k) +C2
4t2
n∞X
k=ℓ+1N(d, k)(µk)2
≤ℓN(d, ℓ)
n+C2
4t2
nµℓ+1
=Odℓ
n+t2
ndℓ+1
.(94)
Similarly, we have
N2,φ(λ)
n≥C2
1
nℓX
k=0N(d, k) +C2
3t2
n∞X
k=ℓ+1N(d, k)(µk)2
≥C2
1N(d, ℓ)
n+C2
3t2
nµℓ+1
= Ωdℓ
n+t2
ndℓ+1
.(95)
III. For the third term, we have
∞X
k=0λ2µkφ2
λ(µk)
λ+µkN(d,k)X
j=1f2
k,j≤λ2R2
γ
pX
k=0µs
kφ2
λ(µk) +λ−1∞X
k=p+1µs+1
kC2
4λ−2

=O
λ2dmax{p(2−s),0}+λ−1d−(s+1)(ℓ+1)
=O
λ2dmax{p(2−s),0}+d−s(ℓ+1)
IV .Now we show that Assumption 3 holds. Notice that (45) has been verified in Lemma 20 of Zhang
et al. (2024). Similarly, one can prove (43) and (44) hold using a similar proof as that for Lemma 20
of Zhang et al. (2024).
V .For the final term, when s≥1, we have
M2
1,φ(λ) = ess sup
x∈X∞X
i=1(ψλ(λi)fiei(x))2
≤ ∞X
i=1ψλ(λi)
λiφλ(λi)f2
i!
·ess sup
x∈X∞X
i=1 
λiφλ(λi)ei(x)2
Assumption 3
≤ ∞X
i=1ψλ(λi)
λiφλ(λi)f2
i!
·∞X
i=1λiφλ(λi)
:=Q1,φ(λ)· N1,φ(λ). (96)
33ForQ1,φ(λ), when τ≥s/2andτ <∞, we have
Q1,φ(λ) =∞X
k=0ψ2
λ(µk)µs−1
k
φλ(µk)N(d,k)X
j=1µ−s
kf2
k,j
≤C2
2
C1ℓX
k=0λ2τµ−2τ+s
kN(d,k)X
j=1µ−s
kf2
k,j
+ (C3)−1λ∞X
k=ℓ+1µs−1
kN(d,k)X
j=1µ−s
kf2
k,j
=O
λ2τdℓ(2τ−s)+λd−(ℓ+1)(s−1)
.(97)
Similarly, when τ=∞, we can show that Q1,φ(λ) =O(λd−(ℓ+1)(s−1)).
And when τ < s/ 2, we have
Q1,φ(λ) =∞X
k=0ψ2
λ(µk)µs−1
k
φλ(µk)N(d,k)X
j=1µ−s
kf2
k,j
Lemma E.3
≤C2
2κ2(s−2τ)
C1λ2τpX
k=0N(d,k)X
j=1µ−s
kf2
k,j
+∞X
k=p+1ψ2
λ(µk)µs−1
k
φλ(µk)N(d,k)X
j=1µ−s
kf2
k,j
(30)
≤C2
2κ2(s−2τ)
C1λ2τpX
k=0N(d,k)X
j=1µ−s
kf2
k,j
+∞X
k=p+1C8λ2τN(d,k)X
j=1µ−s
kf2
k,j
=O 
λ2τ
.
ForN1,φ(λ), we have
N1,φ(λ) =∞X
k=0N(d, k) [µkφλ(µk)]
≤ℓX
k=0N(d, k) +∞X
k=ℓ+1N(d, k) [µkφλ(µk)]
≤ℓX
k=0N(d, k) +C4t∞X
k=ℓ+1N(d, k)µk
≤ℓN(d, ℓ) +C4t
=O 
dℓ+λ−1
=O 
λ−1
.(98)
Therefore, when s≥1, we have
M2
1,φ(λ) =

O 
d−(ℓ+1)(s−1)
τ=∞
O 
λ2τ−1dℓ(2τ−s)+d−(ℓ+1)(s−1)
s≤2τ <∞
O 
λ2τ−1
s >2τ(99)
■
34From Lemma D.14, we have the following three corollaries.
Corollary D.15. Let1≤s≤τandγ > 0be fixed real numbers. Denote pas the integer
satisfying γ∈[p(s+ 1),(p+ 1)( s+ 1)) . Suppose one of the following cases holds for λ⋆=d−ℓor
λ⋆=d−ℓ·poly(ln(d)):
(1)p≥1,p(s+ 1)≤γ < ps +p+s,ℓ=p+ 1/2
(2)p≥1,ps+p+s≤γ < ps +p+s+ 1,ℓ= (γ−(p+ 1)( s−1))/2
(3)γ < s ,ℓ= min {γ,1}/2
(4)s≤γ < s + 1,ℓ= (γ−(s−1))/2
Then we have
M2,φ(λ⋆)≲N2,φ(λ⋆)
n= Θ
d−s(p+1)+dp
n
, (100)
or
M2,φ(λ⋆)≲N2,φ(λ⋆)
n= Θ
d−s(p+1)+dp
n
·poly(ln(d)). (101)
Corollary D.16. Letτ < s≤2τandγ >0be fixed real numbers. Denote pas the integer satisfying
γ∈[p(s+ 1),(p+ 1)( s+ 1)) . Denote ∆ =γ−p(s+ 1) . Suppose one of the following cases holds
forλ⋆=d−ℓorλ⋆=d−ℓ·poly(ln(d)):
(1)γ≥1,0≤∆≤τ,ℓ=ℓ1:=p+ ∆/(2τ)
(2)γ≥1,τ≤∆≤s+s/τ−1,ℓ=ℓ2:=p+ (∆ + 1) /(2τ+ 2)
(3)γ≥1,∆≥s+s/τ−1,ℓ=ℓ3:=p+ (∆ + 1 −s)/2
(4)γ <1,ℓ=γ/2
Then we have
M2,φ(λ⋆)≍N2,φ(λ⋆)
n= Θ
d−min{γ−p,τ(γ−p+1)+ ps
τ+1,s(p+1)}
, (102)
or
M2,φ(λ⋆)≍N2,φ(λ⋆)
n= Θ
d−min{γ−p,τ(γ−p+1)+ ps
τ+1,s(p+1)}
·poly(ln(d)). (103)
Proof. Denote I=−2ℓτ+ 2pτ−ps,II=−sp−s,III=p−γ, andIV= 2ℓ−γ−p−1. From
Lemma D.14 we have
M2,φ(λ⋆)≍dI+dII,N2,φ(λ⋆)
n≍dIII+dIV.
We can verify that:
(1) When 0≤∆≤τandℓ=p+ ∆/(2τ), we have
II≤I=III≥IVandmin
γ−p,τ(γ−p+ 1) + ps
τ+ 1, s(p+ 1)
=γ−p;
(2) When τ≤∆≤s+s/τ−1andℓ=p+ (∆ + 1) /(2τ+ 2) , we have
II≤I=IV≥IIIandmin
γ−p,τ(γ−p+ 1) + ps
τ+ 1, s(p+ 1)
=τ(γ−p+ 1) + ps
τ+ 1;
(3) When ∆≥s+s/τ−1andℓ=p+ (∆ + 1 −s)/2, we have
I≤II=IV≥IIIandmin
γ−p,τ(γ−p+ 1) + ps
τ+ 1, s(p+ 1)
=s(p+ 1);
35(4) When γ <1andℓ=γ/2, we have
III≥max{I,II,IV}.
■
Corollary D.17. Lets <1andγ >0be fixed real numbers. Denote pas the integer satisfying
γ∈[p(s+ 1),(p+ 1)( s+ 1)) . Suppose one of the following cases holds for λ⋆=d−ℓor
λ⋆=d−ℓ·poly(ln(d)):
(1)τ=∞,p≥1,p(s+ 1)≤γ < ps +p+s,ℓ=p+s/2
(2)τ=∞,p≥1,ps+p+s≤γ < ps +p+s+ 1,ℓ= (γ+p(1−s))/2
(3)τ=∞,γ < s ,ℓ= min {γ,1,2γs}/2
(4)τ=∞,s≤γ < s + 1,ℓ= min {(γ+ (1−s))/2, γ(1 +s)−s, γ/2}
(5)τ <∞,p(s+ 1)≤γ < ps +p+s,ℓ= (γ+ 2τp−sp−p)/(2τ)
(6)τ <∞,ps+p+s≤γ < ps +p+s+ 1,ℓ=p+s/(2τ)
Then we have
M2,φ(λ⋆) +N2,φ(λ⋆)
n= Θ
d−s(p+1)+dp
n
, (104)
or
M2,φ(λ⋆) +N2,φ(λ⋆)
n= Θ
d−s(p+1)+dp
n
·poly(ln(d)). (105)
D.4.1 Verification of variance conditions
Lemma D.18 (Verification of variance conditions for inner-product kernels) .Suppose n≍dγand
s≥1, forγ∈[p(s+ 1),(p+ 1)( s+ 1)) . For any given ℓ≥0, if
λ≥

d−ℓ 
1 + ln2(d)1{γ= 2, s= 1}
p≥1,2ℓ≤max{2p+ 1, γ−(p+ 1)( s−1)}
d−ℓln2(d) p= 0, γ≥1,2ℓ≤max{1, γ−(s−1)}
d−ℓp= 0, γ < 1,2ℓ≤γ;
then there exists a constant ϵ >0only depending on sandγ, such that λ=λ(n, d)satisfies
N1(λ)·nϵ−1→0,
N2
1(λ)
nN2,φ(λ)·ln(n)(lnλ−1)2→0.
Proof. From Lemma 21 in Zhang et al. (2024), we have N1(λ)≍λ−1. When p= 0, we have
γ−ℓ >0. When p≥1, we have γ−p−1/2≥ps−1/2>0. Therefore, there exists a constant
ϵ >0only depending on sandγ, such that we have
N1(λ)·nϵ−1→0.
Denote q:=⌊ℓ⌋. From Lemma D.14, we further have N2,φ(λ) = Ω 
dq+λ−2d−q−1
. Hence, we
have
N2
1(λ)
nN2,φ(λ)·ln(n)(lnλ−1)2=O(ln(d))3
n(λ2dq+d−q−1)
.
Denote ∆ :=(ln(d))3
nλ2dq,∆′:=(ln(d))3
dγ−q−1, then when ∆ =o(1)or∆′=o(1), we have:
N2
1(λ)
nN2,φ(λ)·ln(n)(lnλ−1)2→0.
Now we show that ∆ =o(1):
36•When p≥3andp= 2, s > 1, since γ−2ℓ+q≥(γ−ℓ−1) + ( q+ 1−ℓ)>0, we have
∆ =o(1).
• When p= 2, s= 1, since 2ℓ−q < ℓ + 1<4≤γ, we have ∆ =o(1).
• When p= 2, s= 1, since 2ℓ−q < ℓ + 1<4≤γ, we have ∆ =o(1).
• When p= 1, γ > 2s+ 1, since ℓ <2and hence 2ℓ−q <3≤γ, we have ∆ =o(1).
•When p= 1, s > 1, γ≤2s+ 1, orp= 1, s= 1, γ > 2, since 2ℓ−q≤2< γ, we have
∆ =o(1).
• When p= 1,s= 1,γ= 2, since 2ℓ−q≤2≤γ, we have ∆ =O((ln(d))−1).
• When p= 0, since γ−2ℓ≥0, we have ∆ =O((ln(d))−1).
■
Lemma D.19 (Verification of variance conditions for inner-product kernels: saturation case) .Suppose
τ < s≤2τ. Suppose n≍dγ, forγ∈[p(s+ 1) + τ, p(s+ 1) + s+s/τ−1]. For any given ℓ≥0, if
λ≥d−ℓ, ℓ≤p+ (γ−p(s+ 1) + 1) /(2τ+ 2);
then there exists a constant ϵ >0only depending on sandγ, such that λ=λ(n, d)satisfies
N1(λ)·nϵ−1→0,
N2
1(λ)
nN2,φ(λ)·ln(n)(lnλ−1)2→0.
Proof. From Lemma 21 in Zhang et al. (2024), we have N1(λ)≍λ−1. Notice that we have
2(τ+ 1)( γ−p)≥
ps−1 p≥1
2τ2+ (τ−1)p= 0>0;
Therefore, there exists a constant ϵ >0only depending on τ,s, and γ, such that we have
N1(λ)·nϵ−1→0.
Denote q:=⌊ℓ⌋. From Lemma D.14, we further have N2,φ(λ) = Ω 
dq+λ−2d−q−1
. Hence, we
have
N2
1(λ)
nN2,φ(λ)·ln(n)(lnλ−1)2=O(ln(d))3
n(λ2dq+d−q−1)
=O(ln(d))3
nλ2dq
+O(ln(d))3
dγ−q−1
.
Denote ∆ :=(ln(d))3
nλ2dq,∆′:=(ln(d))3
dγ−q−1. We have:
• When p≥1, since
2(τ+ 1)[γ−2ℓ+q]≥2(τ+ 1)[( γ−ℓ−1) + ( q+ 1−ℓ)]
≥
ps−2 p≥2
2(τ+ 1)( τ−1) + 2[ τs+s−1]p= 1
>0,
we have ∆ =o(1).
• When p= 0, since γ >1, we have ∆′=o(1).
■
37Lemma D.20 (Verification of variance conditions for inner-product kernels: misspecified case) .
Suppose n≍dγand0< s < 1, forγ∈[p(s+ 1),(p+ 1)( s+ 1)) . For any given ℓ≥0, if
λ≥

d−ℓp≥1,2ℓ≤max{2p+s, γ+p(1−s)}
d−ℓp= 0, γ > s, 2ℓ≤γ
d−ℓln(d) p= 0, γ≤s,2ℓ≤γ;
then there exists a constant ϵ >0only depending on sandγ, such that λ=λ(n, d)satisfies
N1(λ)·nϵ−1→0,
N2
1(λ)
nN2,φ(λ)·ln(n)(lnλ−1)2→0.
Proof. When p≥1, it is a direct result of step 2 (the verification of the second condition in
(146) of Zhang et al. (2024)) in the proof of Theorem 3 in Zhang et al. (2024) and the fact that
N2,φ(λ)≍ N 2(λ).
When p= 0, a similar argument as the proof for Lemma D.18 give the desired results. ■
D.4.2 Verification of bias conditions
Lemma D.21 (Verification of bias conditions) .Suppose 1≤s≤τ. Suppose n≍dγ, forγ∈
[p(s+ 1),(p+ 1)( s+ 1)) . For any given ℓ≥0, if
λ≥

d−ℓ 
1 + ln2(d)1{γ= 2, s= 1}
p≥1,2ℓ≤max{2p+ 1, γ−(p+ 1)( s−1)}
d−ℓln2(d) γ∈[1, s+ 1),2ℓ≤max{1, γ−(s−1)}
d−ℓγ∈(0,1),2ℓ≤γ;
then there exists a constant ϵ >0only depending on sandγ, such that λ=λ(n, d)satisfies
N1(λ)M2
1,φ(λ)
n2≪
M2,φ(λ) +σ2
nN2,φ(λ)
,
N1(λ)
nln(n)(lnλ−1)2·∞X
j=1λ2λiφ2
λ(λi)
λ+λif2
i≪
M2,φ(λ) +σ2
nN2,φ(λ)
.(106)
Proof. When 1≤s≤τ, from Lemma D.14, we have
n
M2,φ(λ) +σ2
nN2,φ(λ)
= Ω
dγ−s(q+1)+dq
N1(λ)M2
1,φ(λ)
n=O
λ2(s−1)d−γ+qs+λ−1d−γ−(q+1)(s−1)
N1(λ) ln(n)(lnλ−1)2·∞X
j=1(λ)2λiφ2
λ(λi)
λ+λif2
i=O 
(ln(d))3
·O
λdmax{q(2−s),0}+λ−1d−s(q+1)
,
Denote I=λ2(s−1)d−γ+qs,II=λ−1d−γ−(q+1)(s−1),III=λdmax{q(2−s),0}(ln(d))3, andIV=
λ−1d−s(q+1)(ln(d))3.
For any p≥0and any s≥1:
• From Lemma D.18, we have IV≪dγ−s(q+1).
•When γ≥1, we have γ≥p+ 1, and hence II≪IV≪dγ−s(q+1); when γ <1, we have
II≪dqwithq= 0.
•When p≥1orγ∈(s, s+ 1) , since −ℓs+qs≤0, we have I/dγ−s(q+1)=
O(d−2(γ−ℓ−s/2))≪1; when γ∈(0, s], we have I=O(d−2sℓ+2ℓ−γ) =O(d−2sℓ)≪dq
withq= 0.
38•When s≥2, we have III≪dq; when s <2andp= 0, we have III≪dq; when s <2
andp≥1andq≥1, since γ−ℓ−s >min{(s+ 1)q−ℓ, ps−1/2}>0, we have
III/dγ−s(q+1)=d−(γ−ℓ−s)−2(ℓ−q)≪1orIII/dq≪1; when s <2andp≥1and
q= 0, we have III≪dq.
Combining all these, we get the desired results. ■
Lemma D.22. [Verification of bias conditions: saturation case] Suppose τ < s ≤2τ. Suppose
n≍dγ, forγ∈[p(s+ 1),(p+ 1)( s+ 1)) . For any given ℓ≥0, if
λ≥

d−ℓp≥1, ℓ≤max{ℓ1, ℓ2, ℓ3}
d−ℓln2(d)γ∈[1, s+ 1), ℓ≤max{ℓ1, ℓ2, ℓ3}
d−ℓγ∈(0,1),2ℓ≤γ,
where τ,∆,ℓ1,ℓ2, andℓ3are given in Lemma D.16; then there exists a constant ϵ >0only depending
onsandγ, such that λ=λ(n, d)satisfies
N1(λ)M2
1,φ(λ)
n2≪
M2,φ(λ) +σ2
nN2,φ(λ)
,
N1(λ)
nln(n)(lnλ−1)2·∞X
j=1λ2λiφ2
λ(λi)
λ+λif2
i≪
M2,φ(λ) +σ2
nN2,φ(λ)
.(107)
Proof. When τ < s≤2τ, from Lemma D.14, we have
n
M2,φ(λ) +σ2
nN2,φ(λ)
= Ω
λ2τdq(2τ−s)+dγ−s(q+1)+dq
N1(λ)M2
1,φ(λ)
n=O
λ2(τ−1)d−γ+q(2τ−s)+λ−1d−γ−(q+1)(s−1)
N1(λ) ln(n)(lnλ−1)2·∞X
j=1(λ)2λiφ2
λ(λi)
λ+λif2
i=O 
(ln(d))3
·O
λdmax{q(2−s),0}+λ−1d−s(q+1)
.
Denote I′=λ2(τ−1)d−γ+q(2τ−s),II=λ−1d−γ−(q+1)(s−1),III=λdmax{q(2−s),0}(ln(d))3, and
IV=λ−1d−s(q+1)(ln(d))3.
For any p≥0and any 1≤τ < s≤2τ:
•From Lemma D.18 and Lemma D.19, since N1(λ)·nϵ−1→0, we have IV≪dγ−s(q+1).
•When γ≥1, we have γ≥p+ 1, and hence II≪IV≪dγ−s(q+1); when γ <1, we have
II≪dqwithq= 0.
• When p≥1, since −ℓτ+qτ≤0and
γ−ℓ−s/2
≥maxs(2p−1)
2,(2τ+ 1)( τ+ps)−(τ+ 1)s+ps−1
2(τ+ 1, ps+s(τ+ 1)
2τ−1
>0,
we have I′/dγ−s(q+1)≪1; when p= 0, we have I′=O(d−2τℓ+2ℓ−γ)≪dqwithq= 0.
•When γ−p−ps∈[0, τ]∪[s+s/τ−1, s+ 1], we have ℓ≤max{ℓ1, ℓ3}. Similar to the
proof in Lemma D.21, we can show that III≪dγ−s(q+1)+dq.
•Finally, consider the case γ−p−ps∈[τ, s+s/τ−1]. When s≥2, we have III≪dq;
when s <2, since s >1, we have III/dq=λd−q(s−1)≪0.
Combining all these, we get the desired results. ■
39Lemma D.23 (Verification of bias conditions: misspecified case) .Suppose 0< s < 1. Suppose
n≍dγ, forγ∈[p(s+ 1),(p+ 1)( s+ 1)) . Suppose one of the following holds:
(1)τ=∞.
(2)s >1/(2τ),
(3)γ >((2τ+ 1)s)/(2τ(1 +s)).
Suppose one of the following cases holds for λ=d−ℓorλ=d−ℓ(ln(d))2:
(1)τ=∞,p(s+ 1)≤γ≤ps+p+s,
ℓ∈[p, p+ min {1/2, γs}]
(2)τ=∞,ps+p+s < γ < ps +p+s+ 1,
ℓ∈[p,min{(γ−(p+ 1)( s−1))/2, γ(1 +s)−s(p+ 1)}]
(3)τ <∞,p(s+ 1)≤γ≤ps+p+s,
ℓ= (γ+ 2τp−sp−p)/(2τ)
(4)τ <∞,ps+p+s < γ < ps +p+s+ 1,
ℓ=p+s/(2τ).
then there exists a constant ϵ >0only depending on sandγ, such that λ=λ(n, d)satisfies
N1(λ)
nln(n)(lnλ−1)2·∞X
j=1λ2λiφ2
λ(λi)
λ+λif2
i≪
M2,φ(λ) +σ2
nN2,φ(λ)
;
n−2N1(λ)
∥fλ∥L∞+n1−s
2+ϵ2
=o
M2,φ(λ) +σ2
nN2,φ(λ)
.
Proof. When 0< s < 1, from Lemma D.14, we have
n
M2,φ(λ) +σ2
nN2,φ(λ)
= Ω
dγ−s(p+1)+dp
n−1N1(λ)n1−s=O 
λ−1d−γs
N1(λ) ln(n)(lnλ−1)2·∞X
j=1(λ)2λiφ2
λ(λi)
λ+λif2
i=O 
(ln(d))3
·O
λdmax{p(2−s),0}+λ−1d−s(p+1)
,
and the convergence rate of ∥fλ∥L∞can be attained similar to Lemma 25 in Zhang et al. (2024).
Since τ≥1, similar to the proof of Theorem 3 of Zhang et al. (2024), when 1/2< s < 1, we have
n−2N1(λ)
∥fλ∥L∞+n1−s
2+ϵ2
=o
M2,φ(λ) +σ2
nN2,φ(λ)
,
and when s≤1/2, we have
n−2N1(λ)∥fλ∥2
L∞=o
M2,φ(λ) +σ2
nN2,φ(λ)
.
Denote I=λ−1d−γs,II=λdp(2−s)(ln(d))3, andIII=λ−1d−s(p+1)(ln(d))3.
For any p≥0and any 0< s < 1:
• From Lemma D.20, we have III≪dγ−s(p+1),
•When γ≤ps+p+s, we can show I≪dpwhen: (1) p≥1, or (2) p= 0 and
s >1/(2τ)>0, or (3) τ=∞,
• When γ > ps +p+s, we can show I≪dγ−s(p+1)holds if and only if τ=∞or
γ >(2τ+ 1)s+ 2τ(1 +s)p
2τ(1 +s), τ =τ <∞;
40and the above inequality holds when (1) p > 0or (2) p= 0, s > 1/(2τ)>0, or (3)
p= 0, γ > ((2τ+ 1)s)/(2τ(1 +s));
• When γ≤ps+p+s, since ℓ≥p > p−ps, we have II≪dp;
• When γ > ps +p+s, since ℓ≥p > p−ps, we have II≪dγ−s(p+1).
Combining all these, we get the desired results. ■
D.5 Final proof of Theorem 4.1 and Theorem 4.2
For each case, the proof can be done in the following steps:
(i) When λ≥λ⋆ands≤2τ, where the definition of the balanced parameter λ⋆can be found
in Corollary D.15 and Corollary D.16, we have
M2,φ(λ⋆) +σ2
nN2,φ(λ⋆) = Θ P
d−β⋆
·poly(ln(d))
M2,φ(λ) +σ2
nN2,φ(λ) = Θ P 
d−β
·poly(ln(d)),
where d−β⋆is the desired convergence rate given in Theorem 4.1 or Theorem 4.2 and
β≤β∗. Similarly, when s >2τ, by taking s= 2τin Corollary D.16, we also have
M2,φ(λ⋆) +σ2
nN2,φ(λ⋆) = Θ P
d−β⋆
·poly(ln(d))
M2,φ(λ) +σ2
nN2,φ(λ) = Θ P 
d−β
·poly(ln(d)).
(ii)When λ≥λ⋆, from Lemma D.14, Lemma D.18, Lemma D.19, Lemma D.20, Lemma D.21,
Lemma D.22, and Lemma D.23, we know that conditions in Theorem D.5, Theorem D.9,
and Theorem D.10 are satisfied. Therefore, we have
Eˆfλ⋆−f⋆2
L2X
= Θ P
d−β⋆
·poly(ln(d))
Eˆfλ−f⋆2
L2X
= Θ P 
d−β
·poly(ln(d)).
(iii) Finally, when s > τ , we can further show that: the convergence rates of the generalization
error can not be faster than above for any choice of regularization parameter λ=λ(d, n)→
0. Notice that, when s≥1, for any λ < λ⋆, from the monotonicity of Var(λ)(see, e.g., Li
et al. (2024); Zhang et al. (2024)), we have
Eˆfλ−f⋆2
L2X
≥Var(λ)≥Var(λ⋆)≍Eˆfλ⋆−f⋆2
L2X
,
and hence
Eˆfλ−f⋆2
L2X
= Ω P
d−β⋆
·poly(ln(d)).
E Auxiliary lemmas
Proposition E.1. For any analytic filter function φλ, we have (z+λ)φλ(z)≤4and(z+λ)ψλ(z)≤
4λ.
Proof. From (28), we have (z+λ)φλ(z)≤2 max{z, λ}φλ(z)≤2 max{1,C4} ≤4. From (27),
we have (z+λ)ψλ(z)≤2 max{z, λ}ψλ(z)≤2 max{C2,1}λ≤4λ. ■
Lemma E.2. Letφλbe an analytic filter function defined in Definition C.1. Then, for any s∈[0,1],
we have
sup
z∈[0,κ2]φλ(z)zs≤4λs−1.
41Proof. For any z∈[0, κ2], from Proposition E.1, we have (z+λ)φλ(z)≤4. Therefore, from
Proposition B.3 in Li et al. (2024), we have
φλ(z)zs≤4zs
z+λ≤4λs−1.
■
Lemma E.3. Letψλbe defined in Definition C.1. Then, for any s >2τ, we have
sup
z∈[0,κ2]zsψ2
λ(z)≤C2
2κ2(s−2τ)λ2τ.
Proof. For any z, we have
ψλ(z)≤C2(z/λ)−τ1{z > λ}+1{z≤λ} ≤C2(z/λ)−τ,
hence
zsψ2
λ(z)≤C2
2zsz−2τλ2τ≤C2
2κ2(s−2τ)λ2τ.
■
E.1 Analytic functional calculus
The “analytic functional argument” introduced in Li et al. (2024) is vital in our proof for Theorem
4.1. For readers’ convenience, we collect some of the main ingredients here, see Li et al. (2024) for
details.
Definition E.4.LetAbe a linear operator on a Banach space X. The resolvent set ρ(A)is given by
ρ(A):={λ∈C|A−λis invertible },
and we denote RA(λ):= (A−λ)−1. The spectrum of Ais defined by
σ(A):=C\ρ(A).
A simple but key ingredient in the analytic functional calculus is the following resolvent identity :
RA(λ)−RB(λ) =RA(λ)(B−A)RB(λ) =RB(λ)(B−A)RA(λ). (108)
The resolvent allows us to define the value of f(A)in analog to the form of Cauchy integral formula,
where Ais an operator and fis an analytic function. The following two propositions are well-known
results on operator calculus.
Proposition E.5 (analytic functional calculus) .LetAbe an operator on a Hilbert space Handf
be an analytic function defined on Df⊂C. LetΓbe a contour contained in Dfsurrounding σ(A).
Then,
f(A) =1
2πiI
Γf(z)(z−A)−1dz=−1
2πiI
Γf(z)RA(z)dz, (109)
and it is independent of the choice of Γ.
Now, let Γbe a contour contained in Dfsurrounding both σ(A)andσ(B). Using (108), we get
f(A)−f(B) =−1
2πiI
Γf(z) [RA(z)−RB(z)] dz=1
2πiI
ΓRB(z)(A−B)RA(z)f(z)dz.
(110)
Proposition E.6 (Spectral mapping theorem) .LetAbe a bounded self-adjoint operator and fbe a
continuous function on σ(A). Then
σ(f(A)) ={f(λ)|λ∈σ(A)}. (111)
Consequently, ∥f(A)∥= supλ∈σ(A)|f(λ)| ≤ ∥f∥∞.
42Let us define the contour Γλconsidered in Li et al. (2024) by
Γλ= Γλ,1∪Γλ,2∪Γλ,3
Γλ,1={x±(x+η)i∈C|x∈[−η,0]}
Γλ,2=
x±(x+η)i∈C|x∈(0, κ2)	
Γλ,3=
z∈C|z−κ2=κ2+η,Re(z)≥κ2	
,(112)
where η=λ/2. Then, since TandTXare positive self-adjoint operators with ∥T∥,∥TX∥ ≤κ2,
we have σ(T), σ(TX)⊂[0, κ2]. Therefore, Γλis indeed a contour satisfying the requirement in
Proposition E.5.
Proposition E.7. Suppose that (45) in Assumption 3 holds. Suppose that λ=λ(n, d)satisfies
v:=N1(λ)
nlnn=o(1). Then for any fixed δ∈(0,1), when nis sufficiently large, with probability
at least 1−δ, we have
∥T−1
2
λ(T−TX)T−1
2
λ∥ ≤√v.
T−1
2
λT1
2
Xλ2
≤2 (113)
T1
2
λT−1
2
Xλ2
≤3. (114)
Proof. These inequalities are direct results of (56), (58), and (59) in Zhang et al. (2024). ■
Proposition E.8 (Restate Proposition 4.13 in Li et al. (2024) with only the constant modified) .When
(113) holds, there is an absolute constant that for any z∈Γλ,
∥T1
2
λ(T−z)−1T1
2
λ∥ ≤C
∥T1
2
λ(TX−z)−1T1
2
λ∥ ≤√
6C.(115)
43NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We first propose an improved minimax lower bound for the kernel regression
problem in large dimensional settings in Theorem 3.3 and show that the gradient flow
with early stopping strategy will result in an estimator achieving this lower bound (up to
a logarithmic factor) in Theorem 3.1. We further determine the exact convergence rates
of a large class of (optimal tuned) spectral algorithms with different qualification τ’s, and
provide a discussion on new phenomena we find in Section 4.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We explain the reason for considering spherical data in Remark 2.1. We
point out in the Conclusion section that our work only considers the optimal-tuned spectral
algorithms.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
443.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We list all assumptions we need in the statement of our main theorems. We
provide a complete (and correct) proof in the Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
455.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: The paper does not include experiments requiring code.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
46•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The paper does not include experiments.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: There is no societal impact of the work performed.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
47•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
48•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
49