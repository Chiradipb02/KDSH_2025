CLIPLoss and Norm-Based Data Selection Methods
for Multimodal Contrastive Learning
Yiping Wang∗
University of WashingtonYifang Chen∗
University of WashingtonWendan Yan
University of Washington
Alex Fang
University of WashingtonWenjing Zhou
University of MichiganKevin Jamieson
University of Washington
Simon Shaolei Du
University of Washington
Abstract
Data selection has emerged as a core issue for large-scale visual-language model
pretraining (e.g., CLIP), particularly with noisy web-curated datasets. Three
main data selection approaches are: (1) leveraging external non-CLIP models to
aid data selection, (2) training new CLIP-style embedding models that are more
effective at selecting high-quality data than the original OpenAI CLIP model,
and (3) designing better metrics or strategies universally applicable to any CLIP
embedding without requiring specific model properties (e.g., CLIPScore is one
popular metric). While the first two approaches have been extensively studied,
the third remains under-explored. In this paper, we advance the third approach by
proposing two new methods. Firstly, instead of classical CLIP scores that only
consider the alignment between two modalities from a single sample, we introduce
negCLIPLoss , a method inspired by CLIP training loss that adds the alignment
between one sample and its contrastive pairs as an extra normalization term to
CLIPScore for better quality measurement. Secondly, when downstream tasks are
known, we propose a new norm-based metric, NormSim , to measure the similarity
between pretraining data and target data. We test our methods on the data selection
benchmark, DataComp [ 1]. Compared to the best baseline using only OpenAI’s
CLIP-L/14, our methods achieve a 5.3% improvement on ImageNet-1k and a 2.8%
improvement on 38 downstream evaluation tasks. Moreover, both negCLIPLoss
andNormSim are compatible with existing techniques. By combining our methods
with the current best methods DFN [ 2] and HYPE [ 3], we can boost average
performance on downstream tasks by 0.9%, achieving a new state-of-the-art on the
DataComp-medium benchmark2.
1 Introduction
Curating large-scale visual-language datasets from web-sourced data has become common for
pretraining multi-modal models. However, the quality of these web-curated data pairs remains
a critical bottleneck. Research has shown that the choice of dataset significantly impacts model
performance, irrespective of the models and training techniques employed [ 4–11], and this motivates
∗Equal contribution. Correspondence to ypwang61@cs.washington.edu . Codes are available at
https://github.com/ypwang61/negCLIPLoss_NormSim .
2DataComp benchmark: https://www.datacomp.ai/dcclip/leaderboard.html .
38th Conference on Neural Information Processing Systems (NeurIPS 2024).the development of various data selection strategies. This paper focuses on optimizing subset selection
from a fixed data pool to train a CLIP model [ 4] that achieves superior performance on zero-shot
downstream tasks.
Classical methods rely solely on OpenAI’s (OAI) pretrained CLIP model (i.e., a teacher model) and
focus on better utilizing the embeddings. The most commonly used one is calculating CLIPScore,
which measures the cosine similarity between the visual and language embeddings of the CLIP
model for the same sample, to eliminate low-quality data with mismatches between text and image.
Other works also leverage heuristic distribution alignment techniques to select samples relevant to
downstream tasks, such as image-based filtering [ 1]. These approaches are generally viewed as
providing only limited enhancements. However, we argue that the potential of those embeddings
has been heavily under-explored. This work seeks a universal method to better employ any given
embeddings, not only from OAI CLIP, but also from other CLIP-style models.
On the other hand, recent leading data filtering methods, instead of focusing on improving embedding
utilization stategy itself, mainly follow the other two directions, both employing external resources.
They either (1) use external non-CLIP models that aid in data selection, (2) or use external high-
quality multi-modal data to train a better CLIP-style embedding model than the original OAI CLIP to
filter out low-quality data. Specifically, in the first line of works, HYPE [ 3] leverages embeddings
from hyperbolic models instead of the classical Euclidean-based CLIP to measure how each data point
has semantically overlaps with other data points and filters out data with low specificity. T-MARS [ 12]
removes images where the text is the only feature correlated with the caption using FAST [ 13], an
off-the-shelf OCR text detection model. Devil [ 14] applies fasttext [ 15] to remove non-English texts
and use BLIP-2 [ 16] model for digit recognition to keep useful images with digits. The second
direction, represented by Data Filtering Network (DFN) [ 2], involves training a new CLIP-style
teacher model that uses high-quality datasets like HQITP-350M. Although the embeddings extracted
from this model perform worse than the OAI CLIP in downstream tasks, it is particularly good at
filtering out low-quality data. Notably, some of these methods can be combined and indeed, merging
the selected data from DFN and HYPE achieves current state-of-art as shown in HYPE [3].
Previous works mainly focus on improving the CLIP embedding quality or utilizing an external model
to do filtering but employ the CLIP embedding in a suboptimal way by only using classical methods
like CLIPScore. In contrast, in this work, we focus on improving the filtering methods themselves
for any given CLIP embedding. We show that there are universal and more effective strategies for
utilizing any CLIP teacher model, regardless of its architecture (e.g., B/32 or L/14) or the dataset it
was trained on (e.g., OpenAI-WIT-400M or DFN’s high-quality dataset). These strategies should
always be orthogonal to the use of any newly trained CLIP-style models like DFN and might also be
compatible with methods using external models like FAST and BLIP-2.
Our Contributions. We propose an alternative to CLIPScores that we call negCLIPLoss that more
accurately characterizes data quality. We also introduce a new distribution metric we call the p-Norm
Similarity Score ( NormSim ) when knowledge about downstream tasks is available. Two major
observations directly inform our proposals:
•Firstly, we observe that classical methods measure the quality of a multi-modal sample by com-
puting the cosine similarity between its visual and language embeddings, believing that lower
similarity indicates that the text does not match its image part well. However, we find that some
less informative samples may have a systematic bias, which leads to higher CLIPScores. For
example, the language part containing the word "image" can result in higher similarity with any
visual part, even when the text does not accurately describe its image content. Our proposed
method negCLIPLoss , inspired by the standard CLIPLoss, normalizes the original CLIPScore by
the similarity between a sample and its contrastive pairs. For example, the high score caused by the
word "image" is typically consistent across its contrastive pairs, so our adjustment reduces this bias.
As we have highlighted, such replacement can be universally applied across different embedding
models. See Fig. 2 for illustrations.
•Secondly, if one has access to examples drawn from the same distribution as the target task, it
is natural to assume that this extra knowledge could be leveraged to inform the data filtering
process. We propose the NormSim metric to measure the vision similarity between a training
sample xand the target task dataset Xv
target∈Rn×Ddefined as ∥fv(Xv
target)fv(xv)∥p, where
fv:RD→Rdis the vision encoder of teacher model so that fv(Xv
target)∈Rn×d,fv(xv)∈Rd,
andfv(Xv
target)fv(xv)∈Rn, and∥ · ∥pis the pnorm; effective choices are p= 2or∞. Notably,
unlike previous ImagetNet-based filtering [ 1], which tries to keep the training set as diverse as
2downstream tasks by clustering the training set and finding the nearest neighbor group for every
target sample , our method does not explicitly consider the diversity but select examples as long
as it is close to any target sample (i.e. select high NormSim score). Notably, negCLIPLoss and
NormSim enjoy complementary effect in data selection. See Fig. 3.
To illustrate the effectiveness of our methods, we use a widely used benchmark DataComp [ 1] as
our primary method of evaluating the datasets created by our data filtering methods. We show that,
by simply replacing the CLIPScores with negCLIPLoss and utilizing NormSim we are able to
exceed the best OAI-CLIP(L/14)-based baseline by 5.3% on ImageNet-1k and 2.8% on average
across 38 downstream tasks, which is similar or even better than the performance achieved by many
external-resources-based methods. Notably, even if the target downstream tasks are not available,
using NormSim on a proxy downstream task constructed from the training set, called NormSim 2-D,
combined with negCLIPLoss, can also gain a 1.9% improvement on 38 downstream evaluation.
Moreover, the improvements achieved by our methods are not limited to OAI CLIP-based methods
but can also be obtained by combining our methods with advanced models that require external
resources. By merging the subset selected by negCLIPLoss andNormSim with the subset selected
by current state-of-the-art method “HYPE ∪DFN”, we can further improve it by 0.9% on both
ImageNet-1k and on average 38 downstream tasks. Besides, we can also achieve a 0.8% improvement
on average 38 tasks over "HYPE ∪DFN" using only the data selected by DFN and our strategies.
More importantly, we demonstrate that negCLIPLoss, as a replacement for CLIPScore, can be applied
to any other embedding models like OAI-L/14, OAI-B/32, and DFN-B/32, universally boosting
performance from 0.4% to 3.0% on an average of 38 tasks. This result is not only technically
insightful for understanding the information available in embeddings but also practically significant.
Compared to existing methods, our approach saves a significant amount of computational time on
both reprocessing and new embedding retraining as shown in Table 5.
2 Problem Setup
Data Filtering on Multimodal Dataset. We are given a training dataset Dtrain={xv, xl}, where
(xv, xl)∈RDis the image-text (vision-language) training pair. For convenience, we will let
superscript vldenote either modality so that, for example, xvl∈xv, xl. Our goal is to identify a
subset S⊂Dtrainthat maximizes the zero-shot accuracy of the CLIP model on some downstream
tasks when Sis used to train the CLIP model.
CLIP score and embedding. Recent efforts, such as LAION [ 5] and DataComp [ 1], use Ope-
nAI’s CLIP ViT-L/14 model [ 4] as a teacher model to obtain quality score. Here we denote
this vanilla CLIP model as ¯fvl. For any pair xvl, the model outputs a normalized unit-vector
¯fvl(xvl). If Xvl:={xvl
1, . . . , xvl
m}denotes a dataset containing msamples, then we define
¯fvl(Xvl) = [ ¯fvl(xvl
1), . . . , ¯fvl(xvl
m)]⊤∈Rm×das the embedding matrix. The popular filtering
metric “CLIPScore” is defined as ⟨¯fv(xv),¯fl(xl)⟩ ∈[−1,1].
Dataset and model. Here we follow the pipeline of Datacomp [ 1] to standardize the training and
evaluation process. This is a testbed for dataset experiments aiming to open-source and further
improve the vanilla CLIP model and is widely adopted in previous data selection papers [ 17,18,12,
2, 19, 7]. We will give more details in Sec. 4.
3 Data Filtering Strategy
3.1 negCLIPLoss: A Better Metric than CLIPScore
In this section, we introduce a better and statistically interpretable quality metric called negCLIPLoss,
which directly replaces the common metric CLIPScore. Fig. 1 illustrates how negCLIPLoss works.
This new metric only requires negligible extra computational costs and no additional external data
collection costs. As the name suggested, this metric is inspired by the standard CLIP loss used in the
actual training process of the teacher CLIP model, which is defined as
ℓB∗(xvl
i) =−1
2"
logexp(¯fv(xv
i)⊤¯fl(xl
i)/τ)P
j∈B∗exp(¯fv(xv
i)⊤¯fl(xl
j)/τ)+ logexp(¯fv(xv
i)⊤¯fl(xl
i))/τP
j∈B∗exp(¯fv(xv
j)⊤¯fl(xl
i)/τ)#
(1)
Here B∗is the random batch where i-th sample belongs during a particular training step, and τis
the learnable temperate parameter. Notably, the teacher loss differs from CLIPScore primarily by a
3“Caruba Step -down 
verloopring 67-46”“17596 Green Willow 
Place -Photo 25”
CLIPScore : Top 27%  
negCLIPLoss : Top 39%
𝕹:Top17%
CLIPScore : Top 16%  
negCLIPLoss : Top 30%
𝕹:Top14%
“Listing Image 7”
CLIPScore : Top 36%  
negCLIPLoss : Top 52%
𝕹:Top 7%
CLIPScore can overestimate the qualityCLIPScore  ≔𝒇𝒊𝑻𝒈𝒊Quality Metric for data 𝒊
where the normalization term 𝕹≔𝜏
2[logσ𝑗∈𝐵e𝑓𝑖𝑇𝑔𝑗/𝜏+logσ𝑗∈𝐵e𝑓𝑗𝑇𝑔𝑖/𝜏]negCLIPLoss ≔𝒇𝒊𝑻𝒈𝒊−𝕹∝negative CLIP loss for data 𝒊
𝒇/𝒈: Image/text embedding 𝝉: temperature 𝑩: batch
“San Juan Islands 
Friday Harbor”
CLIPScore : Top   78%  
negCLIPLoss : Top   34%
𝕹:Top 100%“American football”
CLIPScore : Top   56%  
negCLIPLoss : Top   37%
𝕹:Top 99%CLIPScore can underestimate the quality
“CIMG5175 Woolly 
sheep at Dunk's Green”
CLIPScore : Top   83%  
negCLIPLoss : Top   42%
𝕹:Top100%
Figure 1: Illustration of negCLIPLoss. CLIPScore may underestimate (bottom left, where the data
quality is high but CLIPScore is low) or overestimate (bottom right, where the data quality is low
but CLIPScore is high) the quality of image-text pairs. However, this issue can be mitigated by
simply subtracting a normalization term R. negCLIPLoss employs the teacher model to calculate
the negative CLIP loss on training data and serves as a more accurate metric. Here, “Top X%”
denotes that the score represents the top X% high values within the entire dataset (i.e., the (100-X)%
percentile among all the values). For example, “ R:Top100% ” means this data has almost the
smallest Ramong the whole dataset, which represents that it contains highly specific elements in
both images and texts.
normalization term R∗as follows:
−τ·ℓB∗(xvl
i) =¯fv(xv
i)⊤¯fl(xl
i)|{z}
CLIPScore−τ
2"
logX
j∈B∗exp(¯fv(xv
i)⊤¯fl(xl
j)
τ) + logX
j∈B∗exp(¯fv(xv
j)⊤¯fl(xl
i)
τ)#
| {z }
normalization term R∗
In practice, since the training dataset of teacher CLIP models, like OAI-WIT400M [ 4], and the actual
batch divisions B∗is inaccessible, we randomly select Kbatches from the student model’s training
data and use the averaged results from {Bk}K
i=1to estimate the normalization term R∗onB∗:
negCLIPLoss (xvl
i) :=−τ
KKX
k=1ℓBk(xvl
i)≈CLIPScore (xvl
i)− R∗(2)
Here{Bk}K
i=1are some batches randomly selected from the student model’s training data and
xi∈Bk,∀k. We choose K= 10 in our experiments, but any sample size larger than 5 is sufficiently
stable for estimating the original CLIPLoss (Details in Appendix D.1). Besides, in Sec. 4.3.3 we also
show that the computational cost introduced by Rremains negligible compared to other baselines.
The temperature τand batch size |B∗|can be directly obtained from the parameters of the pretrained
teacher model. More details of negCLIPLoss are in Appendix, including the concentration analysis of
R(Appendix A.1), pseudocode (Algorithm 1), and the ablation study of τand|B|(Appendix C.2).
0.2 0.4
Downsampling ratio182022242628ImageNet-1k accuracy
ImageNet-1k
CLIPScore
negCLIPLoss
0.2 0.4
Downsampling ratio26283032Average performance
Average over 38 datasets
CLIPScore
negCLIPLoss
Figure 2: Comparison of negCLIPLoss and
CLIPScore across different downsampling ra-
tios on DataComp-medium.Motivation behind negCLIPLoss. Other existing
works also use loss-guided data selection, such as
LESS [ 20] in NLP, CoDis [ 21] in CV , and RHO [ 22]
in general data scheduling scenarios. However, it is
still unclear whether selecting based on teacher loss
is suitable for multi-modal contrastive learning. Here
we give an affirmative answer as shown in Fig. 2,
where we can see negCLIPLoss performs better than
or on par with CLIPScore consistently.
To illustrate how teacher loss helps our selection, we
demonstrate that the normalization term provided by
negCLIPLoss is crucial for correcting the overestima-
tion or underestimation inherent in CLIPScore. A high normalization term implies that either the
image embedding, text embedding, or both can easily match multiple contrastive pairs beyond their
4“View Vol. 2 No. 11 
(2013):”
“Zdjęcie Woodstock”
“1: CV Focus episode     
1 -Steve Bridge”“An unhurried sense of time 
is in itself a form of wealth”“Ontwerp van finalist 
golightly ”
“Weatherbeeta Wide Tab Long 
Travel Boots -Panda Print”“Corn -and-Tomato -
Saladcs ”“A Red Fox, Vulpes Vulpes , 
Noses a Polar Bear, Ursus 
Maritimus -Norbert 
Rosing -Framed”
Type2:  target -related, high quality
Type4: not target -related, high quality
Type 1:  target -related, low quality
Type3: not target -related, low quality
negCLIPLoss𝐍𝐨𝐫𝐦𝐒𝐢𝐦𝟐(IN-1k)
😊☹ No Filtering
Num(T1) ≈ Num(T2) ≈  Num(T3) ≈  Num(T4)
Quality Filtering
Num(T2) ≈  Num(T4) >>Num(T1) ≈ Num(T3)😐
Norm Similarity Filtering 😑
Num(T1) ≈ Num(T2) >> Num(T3) ≈  Num(T4)
Quality + Norm -Sim Filtering
Num(T2) >> Num(T1) ≈ Num(T3) ≈  Num(T4)NormSim𝑝(𝑋target,𝑥)
=∥ҧ𝑓𝑣𝑋target𝑣ഥ𝑓𝑣𝑥𝑣∥𝑝
(a) Data Type for Different negCLIPLoss and 𝐍𝐨𝐫𝐦𝐒𝐢𝐦𝟐(b) Data Selection Methods 
Comparison 
Top 20% Top 5% Top 80% Top 95% Top 50%Top
95%Top
80%Top
50%Top
20%Top
5%
😊Figure 3: Illustration of NormSim on DataComp. Xtargetis the target prior data. “Top X%” denotes
that the score represents the top X% high values within the entire dataset. (a) Visualization of data
with different NormSim and negCLIPLoss. Here we use NormSim 2(ImageNet-1k) as an example.
Although both Type 2 and Type 4 data have high negCLIPLoss and thus high quality, data with low
NormSim 2(Type 4) are more irrelevant to downstream tasks like ImageNet, VTAB, and MSCOCO.
For example, they contain many images dominated by OCR content and make little contribution to
improving downstream performance. (b) Illustration of a rough comparison of sampling data for
different filtering methods. Using “ negCLIPLoss ∩NormSim ” filtering can balance the quality and
relevance to downstream tasks, thus increasing the proportion of Type 2 data. (Refer to Appendix E
for more visualization.)
corresponding counterparts. For example, in the bottom right of Fig. 1, the text containing “Image”
or “Photo” can be easily matched with any visual content. Similarly, the image of “verloopring” only
contains very simple features and can be matched with many words like “white”, “empty” or “circle”,
etc. Consequently, despite a high absolute CLIPScore, the relative negCLIPLoss within its batch can
be lower. In contrast, the bottom left features highly specific elements in both text and images, such
as "Islands Harbor," "American football", and "sheep at green". These elements are specific and less
likely to match with contrastive pairs, resulting in a higher relative negCLIPLoss.
3.2 NormSim: A New Training-Target Similarity Metric
Our proposed negCLIPLoss is a universal approach to improve filtering performance by estimating
quality better, and it does not rely on any downstream task. Now, if we can access some knowledge
of the downstream tasks, we could further improve the performance by using a vision-only p-norm
similarity to target data metric to measure the relationship between each training sample and the
downstream target data. We will discuss the reason to use vision-only embedding later in this section.
Specifically, we assume access to the target set of downstream tasks and denote them as Xtarget=
{xtarget,(1), . . . , x target,(m)}, where each xtarget,(i)∈Rdisi.i.d.-sampled from the target downstream
distribution Ptarget3, but without overlapping with the test set. Then, for each training sample xvland
the corresponding target set Xtarget, the NormSim is defined as:
NormSim p(Xtarget, x) :=∥¯fv(Xv
target)¯fv(xv)∥p=
X
xt∈Xtarget⟨¯fv(xv
t),¯fv(xv)⟩p
1/p
(3)
We select the subset Sby choosing the samples with top- Nhighest NormSim scores. The choice of
the norm type pcan be based on the data distribution and training process. In this paper, we consider
two instantiations of p:
When p= 2, our data selection method can be regarded as the following equation. It’s equivalent to
selecting a subset that aligns with the principal components of the target set variance (Appendix C.6.1).
3Although out-of-distribution tasks like “WILDS” have distribution shift between training data and test data,
they still provides useful information of the test data.
5S= arg max
|S|=NX
i∈SNormSim 2(xt, xi),NormSim 2(xt, xi) =
X
xt∈Xtarget¯fv(xv
t)⊤¯fv(xv)2
1/2
(4)
When p=∞, the distance metric can be regarded as an even more optimistic measure, such that
a training sample will be selected if it has high similarity to any target sample . Note that this is
different from nearest-neighbor-based method used in image-based filtering [ 1], where they are trying
to find the nearest training sample of every target sample . In this case, it can be regarded as:
S= arg max
|S|=NX
i∈SNormSim ∞(xt, xi), NormSim ∞(xt, xi) = max
xt∈Xtarget¯fv(xv
t)⊤¯fv(xv
i) (5)
In Appendix D.3, we also show that our NormSim ∞can outperform the nearest neighbor selection
on the downstream target tasks. Here, we show an example selected via the NormSim 2(ImageNet-1k)
in Fig. 3, showing that this vision-target-aware method is complementary to the quality-based one.
Choice of Target Data. In the experiment parts, we try two kinds of target data: training data from
ImageNet-1k (1.3M) or training data from all 24 accessible downstream tasks (2.1M)4. We denote
them as NormSim p(IN-1k) andNormSim p(Target) , respectively.
Necessity of using vision-only information We use only the visual information xvinstead of multi-
modal information xvlfor measuring similarity. This is because common crawled text often has
brief captions, making the OAI CLIP language embedding weaker than its visual embedding model
[1,23–25]. Consequently, the language part cannot characterize the pre-training and downstream task
distribution as well as the visual part. This phenomenon is also observed in Gadre et al. [ 1], where
image-based filtering (select data whose image embeddings are similar to that from ImageNet-1k)
outperforms text-based filtering (select data whose captions contain words from ImageNet-21k).
More ablation studies are provided in Appendix D.4.
Generality of NormSim in choosing teacher model. Notably, since we just use image embeddings
in the NormSim metric, we believe it unnecessary to use CLIP model to obtain NormSim. Norm-
Sim can be a general metric for selecting target-related image/image-text data if any good image
representations are given, like the representations obtained from pretrained ResNet-50.
Theoretical justification. Unlike many existing methods that force diversity by selecting training
samples around each xtarget, our strategy maximizes similarity without directly considering data
diversity. For the p= 2 case, we demonstrate that maximizing NormSim 2is optimal under a
linear model ¯fv, as shown in Appendix A.2. Our theorem also provides error guarantees for noisy
embeddings and explains when vision-only embeddings outperform combined vision and language
embeddings. Recent work by Joshi et al. [ 26] provides a similar analysis but focuses on high-quality
data and cross-variance between images and texts. This approach is less effective than image-only
methods for filtering noisy datasets, as discussed above.
Using proxy when downstream Xtarget is inaccessible. Surprisingly, we show that the 2-norm can
also be used when only the pre-training set is available. In this case, we construct a proxy “target” set
from the pre-training set itself. Specifically, let Sibe the selected subset at step i, then we treat the
current Sias the proxy “target” set. To construct the next smaller set, we select the next data batch
Si+1satisfying arg maxSi+1⊂SiP
x∈SNormSim 2(Si, x),until reaching an N size subset. We call
this approach NormSim 2-D(Dynamic) and will specify the algorithm details in Appendix C.3.
4 Experimental Results
In this section, we evaluate the performance of negCLIPLoss and NormSim, aiming to address the
following questions: Q1: Given a fixed CLIP teacher model, can our methods more effectively utilize
CLIP embeddings for data filtering? Q2: Are our methods applicable to diverse CLIP teacher models
with varying architectures or different pretrained datasets? Q3: How does our method compare to
other leading approaches that utilize external models or multimodal datasets? Additionally, could our
method be compatible with these methods and enhance their effectiveness?
4Here we only use the target data for data selection, instead of training on them. The target dataset is
significantly smaller than pretraining set like DataComp-medium (128M) or external datasets like HQITP-350M
utilized by DFN [2].
64.1 Setup
We adhere to the standardized training and evaluation protocols of the DataComp benchmark [ 1].
Training configuration. We employ the medium-scale training configuration of DataComp
(DataComp-medium). It provides a substantial dataset comprising 128 million low-quality, web-
curated image-text pairs to be filtered. Once the data subset is obtained by some data filtering strategy,
it will be used to train a fixed CLIP-B/32 model in a fixed training budget that allows the model to
pass 128 million data points an epoch. Therefore, smaller subsets will be repeated more frequently,
ensuring a fair comparison. We note that the size of the DataComp dataset becomes smaller over time
since some URLs of images become invalid5, and we only successfully downloaded about 110M data.
Therefore, the results of baselines on the leaderboard do not apply to our datasets, and we reproduce
all the top baselines on the leaderboard with their public UIDs of the selected data.
Evaluation. We measured the model performance on 38 downstream datasets including image
classification and retrieval tasks followed by DataComp. The image classification tasks contain
ImageNet-1k [ 27], ImageNet distribution shifts [ 28–31], 11 datasets from the Visual Task Adap-
tation Benchmark (VTAB) [ 32] and 3 datasets from WILDS [ 33,34]. Retrieval datasets contain
Flickr30k [35], MSCOCO [36] and WinoGA ViL [37].
Teacher model architecture. Our experiments utilize two architectures for OpenAI’s CLIP teacher
models: ViT-L/14 and ViT-B/32. Additionally, we use the public version of DFN (DFN-P) proposed
by Fang et al. [2] as a teacher model, and its architecture is also ViT-B/32.
4.2 Baselines
We restate the three current research directions mentioned before based on how much external
resources are employed: (D1) using OAI CLIP alone while optimizing embedding employment
strategies, (D2) training and using a more advanced CLIP embedding model based on external data,
and (D3) utilizing non-CLIP external models to aid data selection. It is important to note that D2
and D3 may also incorporate strategies from D1. For example, CLIPScore (D1) has been used in
almost all the top methods. Therefore, we categorize baselines by the largest possible category they
encompass. According to the above categorization, we summarize the baselines we used in our
experiments as follows. Please refer to Fig. 4 and Appendix C.4 for more details.
D1: OAI CLIP embedding only. The learner can only access the pretraining dataset (like DataComp-
medium), the original OAI CLIP teacher model that is used to extract embeddings, and some target
data of the downstream tasks which is much smaller than the pretraining dataset (like ImageNet-1k).
In this category, we don’t use any existing external non-CLIP models or any newly trained CLIP
model based on external multi-modal dataset. In detail, This category includes (1) CLIPScore [38],
which only uses CLIPScore for filtering as we mentioned before. (2) Image-based filtering [1],
which uses ImageNet-1K training data as the downstream target data for data filtering. It applies
k-means clustering to the image embeddings of training data and selects clusters closest to the
ImageNet-1K embeddings. Gadre et al. [ 1] also try to combine image-based filtering and CLIPScore
together. (3) D2Pruning [18], which represents the dataset as an undirected graph and selects the
data by combining difficulty and diversity. They use the CLIP score to initialize their graph.
D2, D3: Accessible external model and multi-modal data. All the current top baselines enable the
learner to utilize external resources, either to train a better CLIP teacher model or to help filtering
using existing models’ properies. In detail, (1) DFN [2] trains another CLIP data filtering network
via external high-quality data. Their currently public model ( DFN-P ) is trained on CC12M [ 39] +
CC3M [ 40] + SS15M [ 41], while the best DFN is trained on nonpublic HQITP-350M [ 2], which
is even larger than DataComp-medium. (2) HYPE [3] leverages hyperbolic embeddings (different
from CLIP embedding) and the concept of entailment cones to filter out samples with meaningless
or underspecified semantics, enhancing the specificity of each sample. (3) HYPE ∪DFN proposed
by [3] samples subset separately for each method and then merge them. This is the state-of-the-art
method on the DataComp benchmark for medium size. (4) Other methods including T-MARS [12],
Devils [14],MLM [42], which leverage external models such as text detection model FAST [ 13],
BLIP-2 [16] and LLaV A-1.5 [43, 44] to heuristically select data. See details in Appendix C.4.
Cross-setting comparison. We make these separations for fair comparison. Intuitively, performance
should be ranked as D2, D3 >D1. However, our results show that cross-setting comparisons are
possible and our D1 methods can perform similar or even better than most of D3 methods.
5See https://github.com/mlfoundations/datacomp/issues/3. Similar issues are proposed by D2pruning [18].
7Table 2: Results on DataComp-medium from methods that use only OpenAI’s CLIP-L/14 model, i.e.,
all methods are from the D1category. The “dataset size” represents the size of the subset obtained
from different approaches. NormSim(IN-1k) denotes using the training data of ImageNet-1k as the
target while NormSim(Target) represents using that of all 24 available downstream tasks. NormSim-D
refers to the methods that use an iteratively selected subset from the training set as the target proxy.
Filtering StrategyDataset IN-1k IN Dist. Shift VTAB Retrieval Avg.
Size (1 task) (5) (11) (3) (38)
No filtering [1] 110M 17.3 15.0 25.2 21.3 25.6
CLIPScore (20%) [38] 22M 25.4 22.7 31.8 22.0 31.0
CLIPScore (30%) [38] 33M 26.4 23.6 32.6 24.5 32.2
Image-based [1] 24M 25.5 21.9 30.4 24.6 29.9
CLIPScore (30%) ∩Image-based [1] 11M 27.4 23.9 31.9 21.4 30.8
D2Pruning [18] 22M 23.2 20.4 31.4 18.7 29.5
negCLIPLoss (20%) 22M 27.4 23.8 33.7 23.7 32.5
negCLIPLoss (30%) 33M 27.9 24.6 33.2 25.1 32.9
CLIPScore (30%) ∩NormSim 2-D 22M 28.3 25.0 34.5 22.7 32.9
negCLIPLoss (30%) ∩NormSim 2-D 22M 29.8 26.1 34.8 24.6 34.1
CLIPScore (30%) ∩NormSim 2(IN-1k) 22M 29.1 25.4 35.8 24.1 33.4
CLIPScore (30%) ∩NormSim 2(Target) 22M 28.9 25.1 32.7 23.6 32.5
CLIPScore (30%) ∩NormSim ∞(IN-1k) 22M 29.7 25.9 33.7 24.1 33.7
CLIPScore (30%) ∩NormSim ∞(Target) 22M 30.2 26.2 35.0 23.4 33.9
negCLIPLoss (30%) ∩NormSim 2(IN-1k) 22M 30.4 26.4 35.4 25.6 34.3
negCLIPLoss (30%) ∩NormSim 2(Target) 22M 30.6 26.2 35.2 25.5 33.9
negCLIPLoss (30%) ∩NormSim ∞(IN-1k) 22M 31.9 27.3 34.8 25.0 34.4
negCLIPLoss (30%) ∩NormSim ∞(Target) 22M 31.7 27.2 36.0 26.0 35.0
4.3 Main Results and Discussions
4.3.1 Comparision on D1 Category (Q1)Table 1: Results on DataComp-medium from
the top methods that use only OpenAI’s CLIP-
B/32 model or public version of DFN (DFN-P).
“NormSimB/32
∞” represents using OAI CLIP-B/32
to calculate NormSim ∞.
Strategy Size IN-1k VTAB Avg.
OAI CLIP-B/32
CLIPScore (30%) 33M 27.6 33.6 33.2
CLIPScore (20%) 22M 27.0 33.0 32.2
negCLIPLoss (30%) 33M 28.8 33.7 33.6
negCLIPLoss (20%) 22M 28.9 34.3 33.0
negCLIPLoss (30%)22M 32.4 35.9 35.2∩NormSim ∞(Target)
DFN-P
CLIPScore (30%) 33M 28.4 33.2 32.7
CLIPScore (20%) 22M 29.7 33.0 33.1
CLIPScore (17.5%) 19M 30.2 34.1 33.8
CLIPScore (15%) 16M 25.9 32.9 31.6
negCLIPLoss (30%) 33M 28.9 33.4 33.2
negCLIPLoss (20%) 22M 30.7 33.6 33.8
negCLIPLoss (17.5%) 19M 31.2 35.7 34.7
negCLIPLoss (15%) 16M 31.3 35.8 34.6
negCLIPLoss (30%)22M 29.4 33.5 32.5∩NormSim ∞(Target)
negCLIPLoss (17.5%)16M 31.5 34.6 34.4∩NormSim ∞(Target)
negCLIPLoss (17.5%)16M 31.6 37.2 35.7∩NormSimB/32
∞(Target)In Table 2, we compare the D1 methods where
only the OAI CLIP model is allowed to be used.
Our Methods leverage OAI CLIP-L/14 better.
First , negCLIPLoss outperforms CLIPScore on
all metrics , regardless of whether it is used alone
or combined with other methods. These results
support our claim that negCLIPLoss can more
accurately estimate the data quality.
Second , even when target knowledge is unavail-
able, use NormSim 2-Dtogether with negCLI-
PLoss can still improve the filtering performance
by 1.9% on average 38 downstream tasks. Third ,
when target knowledge is available, NormSim 2
andNormSim ∞can improve filtering more sig-
nificantly compared with NormSim 2-D, and in
general, NormSim ∞is the best choice . Espe-
cially, compared with the best baseline ‘CLIP-
Score (30%)’, our best combination ‘negCLI-
PLoss ∩NormSim ∞(Target)’ improves 5.3%
onImageNet-1k and2.8% on average 38 down-
stream tasks , respectively. Later in Table 3 we
will see that this result outperform all the D3
baselines except DFN ∪HYPE. On the other
hand, when using ImageNet-1k as the target
data, the choice of norm has very little influ-
ence.
8Table 3: Results of all D1&D2&D3 top methods on DataComp-medium. The results of MLM [ 42]
are from their paper, while all other baselines are reproduced on our downloaded dataset using their
official UIDs. “Ours (20%)” refers to use “negCLIPLoss (30%) ∩NormSim ∞(Target)” to get 20%
of original data, while “Ours (10%)” denotes applying “negCLIPLoss (20%) ∩NormSim ∞(Target)”
to get 10%. And we use “*” to indicate the case where we choose the intersection of the data selected
by using OAI CLIP-B/32 and OAI CLIP-L/14 separately, which results in about 15M data for “Ours
(20%)*” and 7.4M data for “Ours (10%)*”.
Type Filtering StrategyDataset IN-1k IN Dist. Shift VTAB Retrieval Avg.
Size (1) (5) (11) (3) (38)
D3 T-MARS [12] 22M 30.8 26.3 34.8 25.4 34.1
D3 Devil [14] 20M 31.0 26.7 35.9 24.7 34.5
D3 MLM [42] 38M 30.3 25.6 36.0 29.0 34.5
D3 HYPE [3] 10M 30.3 25.8 34.3 22.2 31.9
D2 DFN [2] 16M 36.0 30.1 36.2 27.0 35.4
D3 DFN ∪HYPE [3] 20M 36.4 30.8 38.5 28.0 36.8
D1 Ours (20%) 22M 32.4 27.4 35.9 26.3 35.2
D3 DFN ∪Ours (20%)* 23M 36.4 30.9 38.6 28.1 37.6
D3 DFN ∪HYPE ∪Ours (10%)* 22M 37.3 31.4 38.5 27.6 37.7
4.3.2 Try Other Teacher Models (Q2)
To evaluate whether our method applies to other CLIP teacher models, we replaced OAI CLIP-L/14
with OAI CLIP-B/32 and DFN-P as embedding models. We compare the best baseline “CLIPScore”
with our “negCLIPLoss” and best strategy “negCLIPLoss ∩NormSim ∞(Target)” as shown in Table 1
and Appendix D.2. Note that the original DFN paper selects a subset comprising 19.2M data points,
which accounts for approximately 17.5%of our dataset and 15% of their dataset, we incorporate
these sampling ratios into our comparison.
negCLIPLoss can be applied to different CLIP embedding models. Our proposed negCLIPLoss,
as a replacement of CLIPScore, not only leads to better performance compared to all the other
baselines using OAI CLIP-L/14 as shown in Table 2, but also achieves universal improvement on the
other two CLIP embedding models, OAI CLIP-B/32 and DFN-P as shown in Table 1. Our methods
can consistently outperform all downstream tasks for different filtering ratios and models, like a
0.5%-5.4% increase on ImageNet-1k.
Embedding required by NormSim should have good downstream performance. When combining
negCLIPLoss with NormSim ∞, OAI CLIP-B/32 and DFN-P exhibit completely different behaviors.
The former obtains results even better than those in Table 2, which uses OAI CLIP-L/14 as the
teacher model, while DFN-P achieves results even worse than using negCLIPLoss alone6. The
reason is that, unlike OAI CLIP-B/32, DFN-P is specially designed for data filtering at the expense of
downstream task performance , as claimed by its authors. For example, the ImageNet-1k accuracy
for DFN-P, OAI CLIP-B/32, and OAI CLIP-L/14 are 45%, 63%, and 75%, respectively. This
indicates that the embeddings obtained from DFN on target data might be highly unreliable, leading
to inaccurate similarity calculations between training and target data. To support this, if we use
DFN-P to evaluate negCLIPLoss but utilize OAI CLIP-B/32 for calculating NormSim, as shown in
"negCLIPLoss (17.5%) ∩NormSimB/32
∞(Target)", we can further improve the results compared to
using negCLIPLoss alone. Its average performance on 38 tasks is even higher than utilizing the best
DFN (trained on HQITP-350M) with CLIPScore, as shown in Table 3.
4.3.3 Comparison with D2 & D3 Categories (Q3)
In this part, we compare all the D2 & D3 baselines mentioned in Sec. 4.2 together with our best
strategy in Table 3. Here we reproduce all the baselines if their official UIDs are available. For “A ∪
B” mentioned in Table 3, we follow the way of “HYPE ∪DFN” in Kim et al. [ 3] to merge the data,
which generates the sampling subset separately for each method and then merge them. This will result
in oversampling the shared data, which is intuitively more important.7We also show the best result
6see "negCLIPLoss (30%) ∩NormSim ∞(Target)" versus "negCLIPLoss (20%)/(30%)" and "negCLIPLoss
(17.5%) ∩NormSim ∞(Target)" versus "negCLIPLoss (17.5%)/(15%)"
7For the dataset size of “A ∪B”, we count the number of the unique data in the dataset followed HYPE [3].
9we obtain by combining our method with DFN [ 2] and HYPE [ 3] on the full DataComp-medium
dataset in Table 4, where the baselines are from DataComp benchmark.
Table 4: Results of the top methods on the full
DataComp-medium dataset (128M data).
Strategy IN-1k Avg.
No filtering 17.6 25.8
CLIPScore [38] 27.3 32.8
T-MARS [12] 33.0 36.1
Devils [14] 32.0 37.1
DFN [2] 37.1 37.3
DFN∪HYPE [3] 38.2 37.9
DFN∪Ours (20%) 37.5 38.6
DFN∪HYPE ∪Ours (10%) 38.2 38.8Our methods can outperform most of the D3
methods. In Table 3, we show that without us-
ing any external models or data, our best combina-
tion, i.e., using OAI CLIP-B/32 for “negCLIPLoss
(30%) ∩NormSim ∞(Target)” ( Ours (20%) ), still
outperforms all methods except DFN and “DFN
∪HYPE”. This answers the first part of Q3 and
further indicates that some external models may be
redundant since CLIP embeddings already contain
necessary information.
We can further improve the SOTA method.
In Table 3, we show that our model can further
boost the performance of the current SOTA method
“HYPE ∪DFN” by 0.9% on both ImageNet-1k and
on average 38 downstream tasks, and close results can be achieved even without combining HYPE
which utilizes the external embedding model MERU [ 45]. And we update the SOTA performance of
the DataComp-medium (full dataset) benchmark as shown in Table 4. Here we use the data selected
by both OAI CLIP-B/32 and L/14, which we found is more robust than using one of them alone.
Our better results answer the second part of Q3, that is, our methods can be compatible with other
D2&D3 methods.
5 Conclusion and Limitation
In this paper, we introduce two metrics, negCLIPLoss and NormSim, to enhance data selection in
multimodal contrastive learning without relying on external resources. negCLIPLoss provides a
more accurate quality metric compared to the commonly used CLIPScore, while NormSim measures
the similarity between pretraining data and target data for known downstream tasks. Experiments
show that our methods achieve results that are competitive with or even better to approaches using
external models or datasets. Additionally, negCLIPLoss and NormSim are compatible with existing
top techniques, allowing us to achieve a new state-of-the-art by combining them.
A notable limitation of our study is the exclusion of larger pretraining datasets, such as the large and
xlarge scales of DataComp. However, DataComp-medium is the most commonly used benchmark for
data selection in CLIP pretraining, and our method has demonstrated both effectiveness (Table 2-3)
and efficiency (Table 5) on it. Future directions include exploring better ways to merge data selected
by different methods and incorporating our methods into data scheduling scenarios.
6 Acknowledgement
We thank Tong Chen, Pang Wei Koh, Xiaochuang Han, Rui Xin, Luyao Ma, Lei Chen, and other
members in the UW ML Group for many insightful discussions and helpful feedback. The research
of Kevin Jamieson and Yifang Chen are partially supported by the NSF through the University of
Washington Materials Research Science and Engineering Center, DMR-2308979, and awards CCF
2007036. SSD acknowledges the support of NSF IIS 2110170, NSF DMS 2134106, NSF CCF
2212261, NSF IIS 2143493, NSF CCF 2019844, and NSF IIS 2229881.
10References
[1]Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao
Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In
search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108 , 2023.
[2]Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal
Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425 , 2023.
[3]Wonjae Kim, Sanghyuk Chun, Taekyung Kim, Dongyoon Han, and Sangdoo Yun. Hype: Hyper-
bolic entailment filtering for underspecified images and texts. arXiv preprint arXiv:2404.17507 ,
2024.
[4]Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning ,
pages 8748–8763. PMLR, 2021.
[5]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,
Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-
5b: An open large-scale dataset for training next generation image-text models. Advances in
Neural Information Processing Systems , 35:25278–25294, 2022.
[6]Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade
Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws
for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 2818–2829, 2023.
[7]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. arXiv preprint arXiv:2308.12966 , 2023.
[8]Huy V V o, Vasil Khalidov, Timothée Darcet, Théo Moutakanni, Nikita Smetanin, Marc
Szafraniec, Hugo Touvron, Camille Couprie, Maxime Oquab, Armand Joulin, et al. Au-
tomatic data curation for self-supervised learning: A clustering-based approach. arXiv preprint
arXiv:2405.15613 , 2024.
[9]Tzu-Heng Huang, Changho Shin, Sui Jiet Tay, Dyah Adila, and Frederic Sala. Multimodal data
curation via object detection and filter ensembles. arXiv preprint arXiv:2401.12225 , 2024.
[10] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. Data selection for
language models via importance resampling. Advances in Neural Information Processing
Systems , 36:34201–34227, 2023.
[11] Amro Abbas, Evgenia Rusak, Kushal Tirumala, Wieland Brendel, Kamalika Chaudhuri, and
Ari S Morcos. Effective pruning of web-scale datasets based on complexity of concept clusters.
arXiv preprint arXiv:2401.04578 , 2024.
[12] Pratyush Maini, Sachin Goyal, Zachary C Lipton, J Zico Kolter, and Aditi Raghunathan. T-
mars: Improving visual representations by circumventing text feature learning. arXiv preprint
arXiv:2307.03132 , 2023.
[13] Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, and Tong Lu. Fast:
Faster arbitrarily-shaped text detector with minimalist kernel representation, 2021.
[14] Haichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, and Heng Wang. The devil is in the details:
A deep dive into the rabbit hole of data filtering. arXiv preprint arXiv:2309.15954 , 2023.
[15] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for
efficient text classification. arXiv preprint arXiv:1607.01759 , 2016.
[16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image
pre-training with frozen image encoders and large language models. In International conference
on machine learning , pages 19730–19742. PMLR, 2023.
11[17] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt.
Improving multimodal datasets with image captioning. arXiv preprint arXiv:2307.10350 , 2023.
[18] Adyasha Maharana, Prateek Yadav, and Mohit Bansal. D2 pruning: Message passing for
balancing diversity and difficulty in data pruning. arXiv preprint arXiv:2310.07931 , 2023.
[19] Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather,
and Ari Morcos. Sieve: Multimodal dataset pruning using image captioning models. arXiv
preprint arXiv:2310.02110 , 2023.
[20] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less:
Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333 ,
2024.
[21] Xiaobo Xia, Bo Han, Yibing Zhan, Jun Yu, Mingming Gong, Chen Gong, and Tongliang
Liu. Combating noisy labels with sample selection by mining high-discrepancy examples. In
Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages
1833–1843, October 2023.
[22] Sören Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch,
Winnie Xu, Benedikt Höltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, and Yarin
Gal. Prioritized training on points that are learnable, worth learning, and not yet learnt. In
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato,
editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of
Proceedings of Machine Learning Research , pages 15630–15649. PMLR, 17–23 Jul 2022.
[23] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang,
Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks? arXiv
preprint arXiv:2107.06383 , 2021.
[24] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning
texts with visual concepts. In International Conference on Machine Learning , pages 25994–
26009. PMLR, 2022.
[25] Yutaro Yamada, Yingtian Tang, and Ilker Yildirim. When are lemons purple? the concept
association bias of clip. arXiv preprint arXiv:2212.12043 , 2022.
[26] Siddharth Joshi, Arnav Jain, Ali Payani, and Baharan Mirzasoleiman. Data-efficient contrastive
language-image pretraining: Prioritizing data quality over quantity. In Sanjoy Dasgupta, Stephan
Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial
Intelligence and Statistics , volume 238 of Proceedings of Machine Learning Research , pages
1000–1008. PMLR, 02–04 May 2024.
[27] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition , pages 248–255. Ieee, 2009.
[28] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global repre-
sentations by penalizing local predictive power. Advances in Neural Information Processing
Systems , 32, 2019.
[29] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet
classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, edi-
tors, Proceedings of the 36th International Conference on Machine Learning , volume 97 of
Proceedings of Machine Learning Research , pages 5389–5400. PMLR, 09–15 Jun 2019.
[30] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural
adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 15262–15271, 2021.
[31] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo,
Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness:
A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 8340–8349, 2021.
12[32] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario
Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A
large-scale study of representation learning with the visual task adaptation benchmark. arXiv
preprint arXiv:1910.04867 , 2019.
[33] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay
Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al.
Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine
Learning , pages 5637–5664. PMLR, 2021.
[34] Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen,
Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, et al. Extending the wilds
benchmark for unsupervised adaptation. arXiv preprint arXiv:2112.05090 , 2021.
[35] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions
to visual denotations: New similarity metrics for semantic inference over event descriptions.
Transactions of the Association for Computational Linguistics , 2:67–78, 2014.
[36] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár,
and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv
preprint arXiv:1504.00325 , 2015.
[37] Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel
Stanovsky, and Roy Schwartz. Winogavil: Gamified association benchmark to challenge
vision-and-language models. Advances in Neural Information Processing Systems , 35:26549–
26564, 2022.
[38] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A
reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718 , 2021.
[39] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing
web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 3558–3568, 2021.
[40] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A
cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Iryna Gurevych
and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages 2556–2565, Melbourne, Australia,
July 2018. Association for Computational Linguistics.
[41] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality
not quantity: On the interaction between dataset design and robustness of clip. Advances in
Neural Information Processing Systems , 35:21455–21469, 2022.
[42] Weizhi Wang, Khalil Mrini, Linjie Yang, Sateesh Kumar, Yu Tian, Xifeng Yan, and Heng Wang.
Finetuned multimodal language models are high-quality image-text data filters. arXiv preprint
arXiv:2403.02677 , 2024.
[43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instruction tuning. arXiv preprint arXiv:2310.03744 , 2023.
[44] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot
impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April
2023) , 2(3):6, 2023.
[45] Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, and Shanmukha Ra-
makrishna Vedantam. Hyperbolic image-text representations. In International Conference on
Machine Learning , pages 7694–7731. PMLR, 2023.
[46] Ryumei Nakada, Halil Ibrahim Gulluk, Zhun Deng, Wenlong Ji, James Zou, and Linjun Zhang.
Understanding multimodal contrastive learning and incorporating unpaired data. In International
Conference on Artificial Intelligence and Statistics , pages 4348–4380. PMLR, 2023.
13[47] Gene H Golub and Charles F Van Loan. Matrix computations . JHU press, 2013.
[48] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint , volume 48.
Cambridge university press, 2019.
[49] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE
Transactions on Big Data , 7(3):535–547, 2019.
[50] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Im-
proving multimodal datasets with image captioning. Advances in Neural Information Processing
Systems , 36, 2024.
[51] Sachin Goyal, Pratyush Maini, Zachary C Lipton, Aditi Raghunathan, and J Zico Kolter.
Scaling laws for data filtering–data curation cannot be compute agnostic. arXiv preprint
arXiv:2404.07177 , 2024.
14A Theoretical Interpretation
A.1 Concentration of Normalization Term in negCLIPLoss
In this section, we construct a theorem using the concentration inequality to show that when the batch
size is sufficiently large, the normalization term RBkobtained from actual batch Bkcan approximate
RB∗calculated using ground truth batch B∗quite well. The details are as follows:
We assume that the pretraining dataset Dis ndependent and identically distributed ( i.i.d.) sampled
from some distribution P. Besides, to use pretraining data batch to approximate the ground truth
batch, one necessary condition is that their distribution is similar. Here for simplicity, we assume that
they are also i.i.d..
Assumption A.1. We assume that the ground-truth batch of data B∗used by the teacher model is
i.i.d. to the pretraining dataset Dwhich is required to be filtered.
For simplicity, we denote sij=¯fv(xv
i)⊤¯fl(xl
j), i, j∈Bto be the cross-image-text similarities in
the batch B. Then the normalization term can be written as
RB
i=τ
2
log(X
j∈Bexp(sij/τ)) + log(X
j∈Bexp(sji/τ))

Heresij∈[−1,1]. We will show that RB
i= (1 + o(1))· RB∗
ifor all iwhen|B|is sufficiently large,
which means that we can use the random batch to approximate the ground-truth batch.
Theorem A.1. If Assumption A.1 holds and the batch size satisfies |B|=|B∗|, then we have
RB
i= Θ(log( |B|))while|RB
i− RB∗
i|=O(1√
|B|)for any i∈B∩B∗.
Proof. Since sij∈[−1,1], It’s obvious that RB
i= Θ(log( |B|)). Let αij:= exp( sij/τ)−
Ej[exp( sij/τ)], then αijis zero-mean. Note that since the data is i.i.d., so does αij, and we
denote γ:=Ej[α2
ij]. Note that |αij| ≤e1/τ=:M, from Bernstein inequality we have
P(|X
j∈Bαij| ≥t)≤2 exp(−1
2t2
|B|γ+1
3Mt)
A similar conclusion holds for B∗. These result that with probability at least 1−η, we have
|X
j∈Bαij| ≤max{2r
|B|γln(2
η),4
3Mln(2
η)}=:t(|B|, γ, η, M )
Thus we have |P
j∈Bexp(sij
τ)−P
j∈B∗exp(sij
τ)| ≤2t(|B|, γ, η). Furthermore, for any x1, x2>1,
it’s easy to prove that |log(x1)−log(x2)| ≤|x1−x2|
min(x1,x2). Therefore, we have |log(P
j∈Bexp(sij
τ))−
log(P
j∈B∗exp(sij
τ))|≲O(1√
|B|). Similar claims hold for |RB
i− RB∗
i|.
A.2 Optimality of NormSim 2Under Linear Assumption
In this section, we give a theoretical justification on the NormSim metric when p= 2under the linear
model assumptions when low quality image and mismatched text has already been removed. In other
words, we mainly focus on the following strategy.
S= arg max
|S|=NX
i∈S¯fv(xv
i)⊤
1
|Xtarget|X
xt∈Xtarget¯fv(xv
t)¯fv(xv
t)⊤

| {z }
¯Σtarget_proxy¯fv(xv
i) (6)
15A.2.1 Theoretical Setup
Training data. For any xv,xl∈Rdobservable image and text training pairs, we define zv,zlto
be the corresponding latent vectors which contain all semantically pertinent information about our
tasks of interest. Similar to previous theoretical work [ 46], we assume each i.i.d pair zvlfollows
zero-mean sub-gaussian distribution whose cross-covariance satisfies
Cov(zv,zl) = Σ train=diag(σ1, σ2, . . .), ∥zvl∥= 1
and each xvlis generated based on a linear model such that
xvl=G∗
vlzvl+ξvl.
HereG∗
vl∈Od×ris the othonormal ground truth representation mapping from the latent vector space
to the input space, and ξvl∼ N(0, Id)arei.i.d. random noise.
Also we denote the cross covariance of any finite dataset S′(e.g. the given train set Dtrain) asΣS′.
Test data. For any zero-shot downstream task, we assume it shares almost same data generation
process as the training set, except its the cross-covariance Σtarget does not necessarily equal Σtrain,
which necessitate the choice of ¯Σtarget_proxy .
CLIP embedding model as teacher. Under the linear model assumption, we have a teacher model
¯fvl=¯Gvl, whose generated clip embedding can partially recover the ground truth hidden vector zvl
with error.
Formally, we say teacher has ϵn
verror if for all possible nbudget subsets S⊂Dtrain,
1
|S|X
xvl∈S¯G⊤
vxv(xv)⊤¯Gv−X
xvl∈Szv(zv)⊤
∗≤ϵn
v
where the same notation applies for the language modal. By the orthonormal assumption on the
ground truth matrix G∗
vl, we see that ¯G⊤
vis aiming to inverting the map. In addition, we say the
teacher has ϵn
v∗lcross modal error
1
|S|∥X
xvl∈S¯G⊤
vxv(xl)⊤¯Gl−X
xvl∈Szv(zl)⊤∥∗≤ϵn
v∗l
When all ϵn
v, ϵn
l, ϵn
v∗l→0asn→ ∞ , then we say the teacher is strong for both modalities. But it
might also be possible that only one modal, for example, visual is strong. That is ϵn
v→0, ϵn
l, ϵn
v∗l≫
ϵn
v.
Model and training. According to Lemma 4.1 in [ 46], using the CLIP loss to optimize the linear
model has approximately the same training dynamics as using the regularized linear loss. Therefore,
here we assume that we are learning Gv, Glby maximizing the clip score gap between the contrastive
pairs, plus a regularizer,
min
Gv,GlLρ
S(Gv, Gl) := min
Gv,GlP
i∈SP
j∈S(sij−sii)
|S|(|S| −1)+ρ
2|S|
|S| −1∥GvG⊤
l∥2
F
where sij:=⟨G⊤
vxv
i, G⊤
lxl
j⟩andρ >0is some regularizer-related constant . Note that this objective
maximizes self-similarity and minimizes similarity between disparate pairs. Note that this “loss” can
be negative, avoiding the trivial null solution of all zeros. We denote this training process from any
given SasGvl=Aρ(S).
Goal and metric. Under the same principle as our training loss function, we measure the per-
formance of any learnt Gv, Glon some downstream task with distribution Dtarget as test loss
Ltarget(Gv, Gl) :=
Exvl∼D target
xvl
2∼D target(⟨G⊤
vxv, G⊤
lxl
2⟩ − ⟨G⊤
vxv, G⊤
lxl⟩)
16This is inspired by the following classification accuracy. Assume that the test data including Cclass,
and the class distribution is C. For every class c, the training data x= (xv,xl)satisfies distribution
Pc. We further assume the corresponding classification templates are {xc}C
c=1. Thus we define
classification accuracy as
AC(Gv, Gl) =Ec,c′∼C×C [Exi∼Pc1[sic> sic′]]
Therefore our goal is to minimize its gap between the best hind-side subset, for any ρ, without budget
constraints,
∆ρ(S) =Ltarget(ˆGvl)−min
S′∈DtrainLtarget(Aρ(S′)),ˆGvl=Aρ(S)
A.2.2 Generalization Guarantees
We now provide theoretical guarantees and postpone our proof into Appendix A.2.3. Firstly, we are
going to prove the intuition behind NormSim 2score.
Lemma A.1 (Intuition behind NormSim 2).With high probability at least 1−1
|S|d, suppose the
hind-side best subset has at least nnumber of samples, then we have
∆ρ(S) =1
ρmax
S′∈Dtrain(Tr (Σ target(ΣS′−ΣS)))
| {z }
NormSim 2related term+O s
dlog(d|S|)
n+s
dlog(d|S|)
|S|!
| {z }
noise
Proof sketch. ❶Under the assumption that both zvl, ξvlis zero-mean, maximizing the clip score gap
is equivalent to maximizing the clip score of the same sample.
Ltarget(ˆGv,ˆGl) :=−Exvl∼Dtarget⟨ˆG⊤
vxv,ˆG⊤
lxl⟩
❷By minimizing the regularized training loss Lρ
S(Gv, Gl)using Eckart-Young-Mirsky Theorem, we
get a closed form solution of ˆGas
ˆGvˆG⊤
l≈1
ρG∗
vΣS·(G∗
l)⊤+noise depend on S
❸Combining the result in ❷and❶, we have
Ltarget(ˆGvl)≈ −1
ρTr (Σ targetΣS)−noise depend on S
The same analysis can be applied on minS′∈DtrainLtarget(A(S′))as well. Rearranging these two
equations gives us the final result.
This lemma shows the the ∆(S)is depend on the NormSim 2-related term and the noise term which
comes from ξ. When nand|S|is large enough, then the NormSim 2-related term will become
dominant. This aligns with our practice experience that the final performance is less sensitive to the
small variation in the number of select data as long as that is sufficient. Moreover, in some special
cases where test distribution has identity cross-variance, then sampling by choosing CLIP score might
be enough.
Now we are ready to give a proof on the choice of ¯Σtarget and visual-only information. Specifically,
the strategy error mainly comes from (1). The unknown test distribution shift from training. (2). The
unobservable ground truth ΣS. To tackle error (1), we assume some prior knowledge on test by using
the proxy test variance ¯Σtarget. To tackle the error (2), there are two possible solutions as shown below.
Based on the theoretical interpretation, we should choose different strategy based on the property of
the teacher embedding model.
Svision+language = arg max
STr
¯Σtarget(X
xvl∈S¯G⊤
vxv(xl)⊤¯Gl)

Svision only = arg max
STr
¯Σtarget(X
xvl∈S¯G⊤
vxv(xv)⊤¯Gv)

17Theorem A.2 (Main) .Under the assumption of Lemma A.1,
∆ρ(S)≤noise +1
ρ∥¯Σtarget−Σtarget∥∥ΣS−Σbest∥∗
+1
ρ(
ϵS
v∗l(vision+language)
ϵS
v+q
1−1
|S|P
i∈[S]⟨zv,zl⟩)(vision only)
Firstly, it is evident that the greater the difference between ¯Σtarget andΣtarget, the less improvement we
can expect. Moreover, in scenarios where ϵlis large (indicating lower accuracy in the language part)
while ϵvis small (indicating higher accuracy in the vision part), it may be advisable to opt for vision-
only embeddings. However, the learner should also consider the termq
1−1
|S|P
i∈S⟨zv,zl⟩, which
represents the alignment between the ground truth visual and language latent vectors, essentially
reflecting the intrinsic quality of the data. If this term is already significant, relying solely on vision
information as a proxy for language information could lead to suboptimal results.
A.2.3 Detailed proofs
Lemma A.2. Let
ˆGv,ˆGl= arg min
Gv,Gl∈Rd×rL(Gv, Gl) (7)
Then we have
ˆGvˆG⊤
l=1
ρG∗
vΣS(G∗
l)⊤+P1+P2+P3+P4 (8)
where noise terms Piare defined in (12) , (13), (14) and (15).
Proof. Note that sij= (xl
j)⊤GlG⊤
vxv
i= Tr( G⊤
vxv
i(xl
j)⊤Gl), like the proof of Corollary B.1. in
[46], we have
L(Gv, Gl) =P
i∈SP
j∈S(sij−sii)
|S|(|S| −1)+ρ
2|S|
|S| −1∥GvG⊤
l∥2
F
=P
i∈SP
j∈Ssij− |S|P
i∈Ssii
|S|(|S| −1)+ρ
2|S|
|S| −1∥GvG⊤
l∥2
F
=−Tr 
G⊤
v"
1
|S| −1X
i∈Sxv
i(xl
i)⊤−|S|
|S| −1¯xv(¯xl)⊤#
Gl!
+ρ
2|S|
|S| −1∥GvG⊤
l∥2
F
=:−Tr(G⊤
vΓGl) +ρ
2|S|
|S| −1∥GvG⊤
l∥2
F
where ¯xvl:= (P
i∈Sxvl
i)/|S|. Then by the Eckart-Young-Mirsky Theorem (For example, Theorem
2.4.8 in Golub et al. [47]), we know that
arg min
Gv∈Rd×r,Gl∈Rd×rL(Gv, Gl)
= arg max
Gv∈Rd×r,Gl∈Rd×rTr(G⊤
vΓGl)−ρ
2|S|
|S| −1∥GvG⊤
l∥2
F
={(Gv, Gl)∈Rd×r×Rd×r:GvG⊤
l=1
ρ|S| −1
|S|SVD r(Γ)} (Eckart-Young-Mirsky Theorem )
where the notation SVD r(Γ)means choosing the first rcomponents of the matrix Γ. Further note
that
Γ =1
|S| −1X
i∈Sxv
i(xl
i)⊤−|S|
|S| −1¯xv(¯xl)⊤(9)
=:P0+P1+P2+P3+P4 (10)
18Here note that ΣS=1
|S|P
i∈Szv
i(zl
i)⊤, we have Pias follows:
P0:=|S|
|S| −1G∗
v·ΣS·(G∗
l)⊤(11)
P1:=1
|S| −1G∗
vX
i∈Szv
i(ξl
i)⊤(12)
P2:=1
|S| −1X
i∈Sξv
i(zl
i)⊤(G∗
l)⊤(13)
P3:=1
|S| −1X
i∈Sξ(1)
i(ξ(2)
i)⊤(14)
P4:=−|S|
|S| −1¯xv(¯xl)⊤(15)
It’s clear that the rank of the matrix P0is no more than r, soSVD r(P0) =P0. And for i∈ {1,2,3,4},
Piare noise terms with E[Pi] =O.
Lemma A.3. For any fixed S, w.h.p 1−δthe noise term can be upper bounded byq
dlog(1/δ)
|S|
Proof. To upper bound the P1 and P2, we have
∥X
izvl
i(ξvl
i)⊤∥2
∗= Tr
X
i,jξvl
i(zvl
i)⊤zvl
jξvl
j
=X
i,j(zvl
i)⊤zvl
j(ξvl
j)⊤ξvl
i
E∥X
izvl
i(ξvl
i)⊤∥2
∗=E"X
i(zvl
i)⊤zvl
i(ξvl
i)⊤ξvl
i#
=|S|d
Regarding each (zvl
i)⊤zvl
j(ξvl
j)⊤ξvl
ias weakly dependent variable, then by using Bernstein inequality,
we have, with high probability 1−δ,
∥X
izvl
i(ξvl
i)⊤∥2
∗≤ |S|d+q
d|S|2σ2
ξlog(1/δ)≤ |S|dp
log(1/δ)
So1
|S|∥P
izvl
i(ξvl
i)⊤∥∗≤q
dlog(1/δ)
|S|. Note that ∥¯xvl∥≲q
log(|S|d)
|S|(like Proposition 2.5 in
Wainwright et al. [48]), it is easy to see that P3 ad P4 are the low order terms if δ≲1
|S|d.
Lemma A.4 (Intuition behind V AS) .With high probability 1−δ, suppose the hind-side best subset
has at least nnumber of samples, then we have
∆(S) =1
ρmax
S′∈Dtrain(Tr (Σ target(ΣS′−ΣS))) +s
dlog(1/δ)
n+s
dlog(1/δ)
|S|
Proof. For any learnt Gv, Glbased on dataset S, we have
Ltest(Gv, Gl) = Tr( G⊤
vExvl∼D target[xv(xl)⊤]Gl)
= Tr(Exvl∼D target[xv(xl)⊤]GlG⊤
v)
=1
ρTr 
Exvl∼D target[xv(xl)⊤]G∗
lΣS(G∗
v)⊤
−Tr 
Exvl∼D target[xv(xl)⊤]noise S
=1
ρTr 
(G∗
v)⊤Exvl∼D target[xv(xl)⊤]G∗
lΣS
−Tr 
Exvl∼D target[xv(xl)⊤]noise S
=−1
ρTr (Σ targetΣS)−Tr 
Exvl∼D target[xv(xl)⊤]noise S
19Here the first equation comes from Theorem A.4 and the third equation comes from Lemma A.2.
Consequently, we have
−min
S′∈DtrainLtest(A(S′)) = max
S′∈Dtrain1
ρTr (Σ targetΣS′) + Tr 
Exvl∼D target[xv(xl)⊤]noise S′
≤1
ρmax
S′∈Dtrain(Tr (Σ targetΣS′)) +∥Exvl∼D target[xv(xl)⊤]∥∥noise S′∥∗
≤1
ρmax
S′∈Dtrain(Tr (Σ targetΣS′)) +O s
dlog(1/δ)
n!
Therefore, we have the final result as
∆(S) =Ltest(ˆGvl)−min
S′∈DtrainLtest(A(S′))
=1
ρmax
S′∈Dtrain(Tr (Σ target(ΣS′−ΣS))) +O s
dlog(1/δ)
n+s
dlog(1/δ)
|S|!
Theorem A.3 (Main) .Under the assumption of Lemma A.1, we have
∆(S)≤noise +∥¯Σtarget−Σtarget∥∥ΣS−Σbest∥∗
+(
ϵS
v∗l(vision+language)
ϵS
v+q
1−1
|S|P
i∈[S]⟨zv,zl⟩)
(vision only)
Proof. Based on Lemma A.1, we will focus on the error cause from selecting subset S, that is,
Tr Σ targetΣS. Since the exact Σtarget is unknown, we assume the access to some proxy ¯Σtarget instead.
Recall that, for any S, we have ground-truth ΣS=Ezvl∈Szv(zl)⊤. Unfortunately, this is not directly
observable by the learner. Instead, the learner is able to observe some proxy ¯ΣSbased on the teacher
model ¯Gvland therefore solving
arg max
STr ¯Σtarget¯ΣS
and therefore, denote Σbest= arg maxS′∈DtrainTr (Σ targetΣS′)
Tr (Σ target(Σbest−ΣS)) = Tr ¯Σtarget(Σbest−¯ΣS)
+ Tr ¯Σtarget(¯ΣS−ΣS)
+ Tr 
(Σtarget−¯Σtarget)(Σ best−ΣS)
≤Tr ¯Σtarget(¯ΣS−ΣS)
+ Tr 
(Σtarget−¯Σtarget)(Σ best−ΣS)
≤ ∥Σtarget∥∥¯ΣS−ΣS∥∗+∥¯Σtarget−Σtarget∥∥ΣS−Σbest∥∗
where the first inequality is by the definition of ¯ΣSand the second inequality comes from holder’s
inequality. Now the key is to upper bound ∥¯ΣS−ΣS∥∗based on our chosen strategy.
In option 1, we use the clip embedding from both visual and language modal. That is, choose
¯ΣS=P
xvl∈S(¯Gv)⊤xv(xl)⊤¯Gl. Then we have
∥¯ΣS−ΣS∥∗≤1
|S|∥X
xvl∈S(¯Gv)⊤xv(xl)⊤¯Gl−X
xvl∈Szv(zl)⊤∥∗≤ϵS
v∗l
In option 2, we use the clip embedding from language model only. That is choose ¯ΣS=P
xvl∈S¯G⊤
vxv(xv)⊤¯Gv. Then, by definition of ϵS, we have
∥¯ΣS−ΣS∥∗≤1
|S|∥X
xvl∈S¯G⊤
vxv(xv)⊤¯Gv−X
xvl∈Szv(zv)⊤∥∗+1
|S|∥X
xvl∈Szv(zv)⊤−ΣS∥∗
≤ϵS
v+1
|S|∥X
xvl∈Szv(zv)⊤−ΣS∥∗
20Now to further bound the second term, we have
1
|S|∥X
xvl∈Szv(zv)⊤−ΣS∥∗≤1
|S|∥Z⊤
v∥∗∥Zv−Zl∥∗
=1
|S|q
TrZvZ⊤vq
Tr(Zv−Zl)⊤(Zv−Zl)
=1
|S|p
Tr(In×n)q
2 Tr 
In×n−ZvZ⊤
l
=1
|S|s
2|S|(|S| −X
i∈[S]⟨zv,zl⟩)
=vuut1−1
|S|X
i∈[S]⟨zv,zl⟩)
Therefore, we finish the proof.
Theorem A.4 (A simplified version of test loss) .Under the assumption that both zvl, ξvlis zero-mean,
maximizing the clip score gap is equivalent to maximize the clip score of the same sample.
Ltarget(Gv, Gl) :=−Exvl∼D target⟨G⊤
vxv, G⊤
lxl⟩
Proof. For any xvl, we have
Ex′
vl∼D target(⟨G⊤
vxv, G⊤
lx′
l⟩ − ⟨G⊤
vxv, G⊤
lxl⟩)
=⟨G⊤
vxv, G⊤
lEx′
vl∼D target(x′
l−xl)⟩
=−⟨G⊤
vxv, G⊤
lxl⟩
21CLIP Teacher Model
(CLIP embedding)External ModelImage -Text Dataset Downstream Target
𝐷2pruning
Direction 1 Direction 2 Direction 3Image -based DFN T-MARS Text-based Devil MLM HYPECLIP 
ScoreAll available 
downstream 
tasks (2.1M)IN-21K, 
captionsIN-1K 
(1.3M)OAI-WIT400M HQITP -350M
FAST fasttext BLIP -2LLaVA -1.5
(GPT -4V)MERURedCups + others (27M)
OursnegCLIPLoss NormSimFigure 4: Illustration of different directions for data selection methods for multimodal contrastive
learning. Here we use four colors to denote the four main resources we can obtain: CLIP teacher
model, downstream target data (which is much smaller than the external multimodal dataset or
pretraining dataset), the external image-text dataset, and the external non-CLIP model. Direction 1
denotes the methods that only use the original OAI CLIP teacher model and the downstream target
data. Direction 2 represents the methods that use external datasets to train a new CLIP teacher model
for improving filtering, like DFN [ 2].Direction 3 denotes the methods that use external non-CLIP
model to select the data that may be heuristically helpful for downstream tasks, like image without too
much text or be more special. In general, D1 method using only CLIP embedding, like negCLIPLoss,
is orthogonal to D2. And both D1 and D2 can be combined with D3 to explore better filtering results.
In the experiments part of the main paper (Sec. 4), we further show that our proposed D1 methods:
NormSim and negCLIPLoss, can outperform all the D3 baselines except the best method “HYPE ∪
DFN”. And we can achieve the new state-of-the-art by combining our methods with that method.
B Illstration of Different Directions for Data Selection in Multimodal
Contrastive Learning
We summarize our main idea of categorizing the current top data selection methods in Figure 4.
C Details of Experiments
C.1 Computation Cost
Our algorithm can significantly reduce the computational cost compared to many existing works as
shown in Table 5. For example, when the CLIP embeddings are obtained (cost about 50 hours for
CLIP-B/32), both T-MARS [ 12] and MLM [ 42] still require more than 900 hours data pre-processing
time to extract the required information from 110M size dataset of DataComp-medium, while we only
need about 5 hours. On the other hand, DFN, although has a similar forward speed (i.e. preprocessing
time), requires retraining a new CLIP teacher model on the HQITP-350M, which is larger than
DataComp-medium.
We give some details in estimating the preprocessing time of other methods:
•ForT-MARS andD2pruning, we run their official code on DataComp-small (11M) data,
and simply scale the preprocessing time by 10 for DataComp-medium, given that the
preprocessing time for T-MARS is proportional to the size of the pretraining dataset, while
D2pruning is no faster than linear.
22Table 5: Comparison of preprocessing time and external resources needed between our method and
other D3 category methods. We skip DFN since it’s orthogonal to our negCLIPLoss method and we
can directly improve it as mentioned in Table 1. Here since all the baselines below except MLM
use a pretrained CLIP model, we only count the time that doesn’t contain that for inferring CLIP
image/text embeddings (about 50 L40 hours for OAI CLIP-B/32), which is also adopted in DataComp
benchmark [ 1]. The external dataset corresponds to the external multimodal dataset used for training
or finetuning the external model. Notably, the preprocessing time for the following methods are all
approximately linearly proportional to the amount of unfiltered pretrained dataset.
TypeFiltering Ext. Model Size of Preprocess TrainingAvg.Strategy Used Ext. Dataset Time Time
D1 D2Pruning [18] NA NA >70 L40 h 65 L40 h 29.5
D3 T-MARS [12] FAST [13] NA 950 L40 h 65 L40 h 34.1
D3 MLM [42] LLaV A-1.5 [43, 44] 50k 1120 A100 h 65 L40 h 34.5
D3 Devil [14] fasttext [15], BLIP-2 [16] NA 510 A100 h 65 L40 h 34.5
D3 HYPE [3] MERU [45] 27M > 120 L40 h 65 L40 h 31.9
D1 Ours (20%) NA NA 5 L40 h 65 L40 h 35.2
•ForMLM , we get the estimated time from their paper. They mention that they need 6.1
minutes to process 10k samples on A100, which results in 1120 A100 hours for our dataset
(110M). We need to mention that their estimation time of calculating CLIP embedding is
inaccurate and we can do it much faster than their claim using the DataComp pipeline.
•ForDevil , it needs to run the k-means clustering algorithm from the faiss library on the
embedding space, which is estimated to cost 120 L40 hours on DataComp-medium. Using
BLIP-2 [ 16] to scan the whole dataset will need about 470 A100 hours from the experimental
details in [ 17]. From https://lambdalabs.com/gpu-benchmarks, we roughly assume that 120
L40 hours are at least comparable to 40 A100 hours for K-means clustering.
•ForHYPE , they claim that MERU is as efficient as CLIP, but they still need at least 120
L40 hours for processing 110M data for their final score, since it uses the image embedding
clusters on DataComp-medium obtained from running k-means clustering algorithm.
C.2 Details of negCLIPLoss
We give the pseudocode of calculating negCLIPLoss in Algorithm 1, which is specially designed
for pytorch-style parallel matrix calculation. It can be fully accelerated and the computation cost
introduced by the normalization term is negligible compared with the training time or preprocessing
time of other top baselines as detailed in Table C.1.
In negCLIPLoss, we need to get the batch size |B|and the value of the learnable temperature
parameter τat the final step of the teacher model pretraining stage. For OAI CLIP-L/14 and OAI
CLIP-B/32, these values are τ= 0.01and|B|= 32768 .
We also have an ablation study about the temperature parameter and batch size chosen for CLIP
teacher models as shown in Table 6. We will see that in general, a larger batch size will result in
better performance, and τ= 0.01, b= 32768 is the best choice for both OAI CLIP-B/32 and DFN-P.
The reason for such a batch size is that a larger batch can contain more contrastive data pairs, which
is also supported by the concentration result of the normalization term proved in Appendix A.1,
and thus it can check the image-text matching between more different data. Therefore, we always
consider the largest batch size 32768 which can fit into a single 24G GPU in the CLIP forward pass,
which is also the OAI CLIP training batch size.
C.3 Details of NormSim 2-D
In this section, we illustrate the details of our NormSim 2-Dalgorithm. The top- Nselection method
is aiming to achieve the object:
S= arg max
|S|=NX
i∈S¯fv(xv
i)⊤
1
|Xtarget|X
xt∈Xtarget¯fv(xv
t)¯fv(xv
t)⊤
¯fv(xv
i) (16)
23Table 6: Ablation study about the temperature parameters τand batch size bfor CLIP teacher model.
The values obtained from the last training step of the teacher models are τ= 0.01, b= 32768 for
OAI CLIP-B/32, OAI CLIP-L/14, and b= 16384 , τ= 0.07for DFN-P. In the main paper, we use
b= 32768 , τ= 0.01for all three kinds of teacher models.
OAI CLIP-B/32 Size IN-1k IN Dist. Shift VTAB Retr. Avg.
CLIPScore (30%) [38] 33M 27.6 24.2 33.6 25.1 33.2
negCLIPLoss (30%)
b= 16384 , τ= 0.01 33M 28.8 25.0 32.5 26.2 33.0
b= 16384 , τ= 0.02 33M 28.6 24.8 33.3 25.3 33.1
b= 16384 , τ= 0.07 33M 28.0 24.2 33.5 25.1 32.6
b= 32768 , τ= 0.001 33M 16.0 13.9 25.1 19.4 24.4
b= 32768 , τ= 0.005 33M 28.5 25.0 33.6 27.0 33.0
b= 32768 , τ= 0.01 33M 28.8 25.1 33.7 26.6 33.6
b= 32768 , τ= 0.02 33M 28.5 24.8 33.6 26.2 32.9
b= 32768 , τ= 0.07 33M 28.2 24.5 32.8 25.2 32.7
negCLIPLoss (30%) ∩NormSim ∞(Target)
b= 16384 , τ= 0.01 22M 32.4 27.4 34.5 26.1 34.7
b= 16384 , τ= 0.02 22M 31.8 26.7 35.0 24.9 34.2
b= 16384 , τ= 0.07 22M 31.0 26.3 35.0 25.5 33.9
b= 32768 , τ= 0.005 22M 32.2 27.2 35.3 26.5 34.8
b= 32768 , τ= 0.01 22M 32.4 27.4 35.9 26.3 35.2
DFN-P Size IN-1k IN Dist. Shift VTAB Retr. Avg.
negCLIPLoss
15%, b= 16384 , τ= 0.07 16M 31.0 27.0 35.2 26.8 34.2
15%, b= 32768 , τ= 0.01 16M 31.3 27.3 35.8 26.4 34.6
17.5%, b= 16384 , τ= 0.07 19M 31.3 27.2 33.5 27.6 33.5
17.5%, b= 32768 , τ= 0.01 19M 31.2 27.5 35.7 27.0 34.7
negCLIPLoss (17.5%) ∩NormSimB/32
∞(Target)
b= 16384 , τ= 0.07 16M 31.1 27.4 34.8 26.1 34.2
b= 32768 , τ= 0.01 16M 31.6 27.3 37.2 25.5 35.7
when the actual Xtargetis unknown. In practice, removing one data at a time is too slow. Therefore,
we remove a batch of data for every step. In detail, if the number of steps is τ, and let ¯Σtest,i=
1
|Si|P
j∈Si¯fv(xv
j)¯fv(xv
j)⊤where Siis the selected subset at step i, then we will remove the data
satisfies the following equation step-by-step until reaching the final subset size:
Si\Si+1= arg min
xl∈Si"
¯fv(xv
l)T· 
1
|Si|X
xt∈Si¯fv(xv
t)¯fv(xv
t)⊤!
·¯fv(xv
l)#
, i∈ {0, . . . , τ −1}
Then we can detail the algorithm process of NormSim 2-Din Algorithm 2. In general, the smaller
the step size, the better the results. But in experiments, we find that it’s already enough to get good
results when τ= 500 .
C.4 Details of Related Works
We add some details about the baselines used in our paper as follows.
•Text-based filtering. [1] proposes a text-based filtering that tries to select the data that contains
caption overlapping with the class name from ImageNet-21K or ImageNet-1K.
•Image-based filtering. [1] also proposes a heuristic way to sample the visual content overlaps
with ImageNet-1K classes. They first apply filtering by language (only choose English caption by
fasttext [ 15]) and caption length (over two words and 5 characters). Then they cluster the image
embeddings from training data to 100K groups using Faiss [ 49], and keep the groups whose cluster
center is the nearest neighbor to at least one image embedding of ImageNet-1K image.
24Algorithm 1 negCLIPLoss
Inputs: image/text embeddings of the pretraining data Fvl= [{¯fvl(xvl
1)}, . . . ,{¯fvl(xvl
N)}]⊤∈
RN×d, batch size b, temperature parameter τ, the number of times negCLIPLoss is random
K(= 10) .
Initialize negCLIPLoss array r= [0, . . . , 0]∈RN
fork= 1toKdo
Get a random batch division Sk={B1, . . . , B s}such that s=⌈N/b⌉. Every Bi∈Skis the
index of a batch of data.
forj= 1tosdo
Get batch of embeddings in batch j:Fvl
j=Fvl[Bj]∈Rb×d
Get the similarity matrix: Ej=Fv
j(Fl
j)⊤∈Rb×b
Get the CLIPScores: cj=diag(Ej)∈Rb
Define Gj= exp( Ej/τ)
Define gv
j∈Rbbe the vector containing the sum of each row vector in Gj(i.e., over image).
Define gl
j∈Rbbe the vector containing the sum of each column vector in Gj(i.e., over text).
Get the negCLIPLoss: r[Bj] =cj−0.5τ·(log(gv
j) + log( gv
j)), here we use element-wise
operation.
end for
end for
Take the mean of each random division as output: negCLIPLoss =r/K
Algorithm 2 NormSim-D strategy
Inputs: image embeddings of the data after CLIP score filtering {¯fv(xv
i)}i∈S, target size N,
number of steps τ
Initialize S0=S, N 0=|S|
fort= 1toτdo
Size at step t:Nt=N0−t
τ(N0−N).
Prior matrix: ¯Σtest,t−1=P
j∈St−1¯fv(xv
j)¯fv(xv
j)⊤
Updated NormSim 2-D for each sample iinSt−1:
NormSim 2-D(xi) =¯fv(xv
i)⊤·¯Σtest,t−1·¯fv(xv
i)
Construct Stsuch that it contains the data with highest NormSim 2-DinSt−1and satisfies
|St|=Nt.
end for
•D2Pruning. [18] tries to represent the dataset as an undirected graph for coreset selection. They
assign the difficulty for each example and use message passing to update the difficulty score
incorporating the difficulty of its neighboring examples, and finally try to keep both diverse and
difficult subsets. For our experiments, we adhere to the default hyperparameters of D2on DataComp
as specified in their official codebase.
•T-MARS [12] uses a text detection model like FAST [ 13] to filter out the data that only contain the
texts of caption in the image and don’t have other useful image features.
•Devils [14] combines many ways for data filtering. At the very first it filter data based on
heuristic rules like text length, frequency of texts, and image size, and it also use CLIPScore
for cross-modality matchment. Then it adopts target distribution alignment methods similar to
image-based filtering, but instead of using ImageNet-1k only, it uses 22 downstream tasks as the
target set. Further, it adopts external models fasttext [ 15] to remove non-English captions and
image-captioning model BLIP-2 [50] to select images with MNIST-style digits.
•MLM [42] prompts GPT-4V to construct instruction data including the image-text data, and use
it to fine-tune a smaller vision-language model like LLaV A-1.5 [ 43,44] into a filtering network.
Nevertheless, the number of parameters of LLaV A-1.5 is still much larger than CLIP, and thus
LLaV A-1.5 has a much longer preprocessing time as mentioned in Table C.1.
25C.5 How to Choose Hyperparameters
The main hyper-parameters of our negCLIPLoss and NormSim are the target numbers for filtering
(refer to Appendix C.2 for the setting of temperature and batch size), which is also the main concerns
for all the top baselines like DFN, MLM, and T-MARS. In the case of DataComp settings, noting that
all the top baselines in DataComp-medium benchmark keep the downsampling ratios ranging from
15% 30% to achieve the best results, we can set the sampling ratio as some previous baselines. Our
method with OAI CLIP teacher model first selects the data with the top 30% negCLIPLoss, and then
selects the top 66.7% NormSim scores to keep 20% of the original pool. We don’t tune the target size
carefully here for fair comparison.
In more general cases, we can recommend some training-dataset-independent thresholds for Norm-
Sim, since the scores only depends on the norm pand target data rather than other data in the pool.
We recommend to set the threshold as 0.7 for NormSim ∞(Target) and 0.15 for NormSim 2(IN-1k) in
general. On the other hand for negCLIPLoss, note that like NormSim, CLIPScore is also training-
dataset-independent, we recommend to first find the percentile of the data with CLIPScore=0.21, and
then downsample the dataset using CLIPLoss until that particular percentile.
Overall, finding optimal filtering ratio for data selection algorithm is always difficult and out of the
scope of this paper. From the paper about the scaling law for data filtering [ 51], downsampling
size even depends on the computation budget. When you have more budget, you should sample
more data for learning. And thus another possible solution is to use their fitting formula to get some
recommended downsampling ratios.
At last, we also note that in data selection problem, visualization is a simple but effective way for
tuning parameters or finding downsampling ratios . People can first randomly select a small subset
(like 1000 data) on some pretraining data subset, and then calculate the target scores (CLIPScore,
negCLIPLoss, NormSim or any other metrics) on them, and fianlly visualize the data corresponding
to scores at different percentiles, like top 10%, 30%, 50% and 70% of the negCLIPLoss. In this
way, we can determine the threshold of filtering directly by observating the data. We also give some
visualization examples of our methods in Appendix E, We believe this is an effective way to give
some guidance on how to roughly select the initial downsampling ratios.
C.6 Discussion of NormSim
C.6.1 How NormSim 2Connects to Selecting the Data in Principal Components.
For convenience, we let f(xt)denote the image embedding of the target data xt∈XT, and f(xs)
denotes the image embeddings of training data xs∈XS. Then the definition of NormSim on a data
xsis
NormSim p(XT, xs) = X
xt∈XT[f(xt)⊤f(xs)]p!1/p
(17)
Then when p= 2, we have
NormSim 2(XT, xs) = X
xt∈XT[f(xs)⊤f(xt)]·[f(xt)⊤f(xs)]!1/2
(18)
= 
f(xs)⊤·X
xt∈XT[f(xt)f(xt)⊤]·f(xs)!1/2
(19)
∝"
f(xs)⊤ 
1
|XT|X
xt∈XTf(xt)f(xt)⊤!
f(xs)#1/2
(20)
26Note that Λ =1
|XT|P
xt∈XTf(xt)f(xt)⊤is the variance matrix of the target image embeddings.
Then using NormSim 2for filtering, we have
S= arg max
|S|=NX
xs∈XSNormSim 2(XT, xs) (21)
NormSim 2(XT, xs) =f(xs)⊤·Λ·f(xs) (22)
=f(xs)⊤U·S·U⊤f(xs) (23)
=rX
j=1sj·[f(xs)⊤uj]2(24)
Here Λ =USU⊤is the eigen decoposition of Λ, where S=diag(s1, . . . , s r)withs1> . . . > s r
are the matrix of eigenvalues, and U= [u1, . . . , u r]∈Rd×rare the corresponding eigenvectors
(i.e., the principal component directions). Note that the column vectors of Uandf(xs)are all unit
vectors, (24) shows that NormSim 2select the data that match with the principal components, i.e.,
eigen directions ujwith large eigen values sj.
C.6.2 Why NormSim works well without explictly considering data diversity.
We answer this question by the following reasons:
•Many top baselines, such as DFN and T-MARS, also don’t explicitly consider diversity, yet
they still provide good performance. Devil even shows that valuable data is worth sampling
multiple times, which they call “quality duplication”. Therefore, one important reason
why NormSim works well without explicitly considering diversity may be that when the
computing budget is limited, as in the DataComp benchmark, the model first needs to learn
the most useful and representative data, which should be similar to some target data.
•Moreover, we chose validation data from 24 downstream tasks ranging from ImageNet
to EuroSet, which may have covered a sufficiently diverse range of target examples for
NormSim to calculate similarity. The diversity of the target data will consequently result in
the diversity of the selected subset. And this also implies the importance of selecting a good
target dataset.
•An additional reason may be that our proposed negCLIPLoss already implicitly selects more
diverse data, as shown in Figure 1 of the main paper. If some training data are diverse, they
will match less with other data and thus have a lower normalization term. This results in a
larger negCLIPLoss and a higher probability of being sampled.
D Additional Results
D.1 Stability Analysis of Batch Sampling Numbers in negCLIPLoss
We show that negCLIPLoss is not sensitive to the number of random select batches Kin Figure 5.
D.2 Universality of negCLIPLoss over Different Teacher Models
We show the complete results of applying our methods to different teacher models like OAI CLIP-B/32
and DFN-P in Table 7. Detail descriptions are in Sec. 4.
D.3 NormSim ∞is Better than Nearest Neighbor Selection
We also try to use near-neighbor selection for aligning downstream distribution. Here, we calculate
the ranks of pretraining data for each target (the higher the rank, the higher the similarity), and then
for each pre-train data, we keep its highest rank. Finally, we select the data with the highest ranks as
the nearest neighbor selected subset.
In Table 8, we show that given the training data of 22 downstream tasks, our NormSim ∞can
outperform near neighbor selection under the same downsampling ratio. The reason may be that the
distribution between the target and pretraining set is not well aligned, so if you force the algorithm to
271 5 10 20 50 100
K20253035negCLIPLoss
25.4
22.731.8
22.031.0Filtering Strategy 20%
IN-1k
IN Dist
VTAB
Retrieval
Average
1 5 10 50
K20253035negCLIPLoss
26.4
23.632.6
24.532.2Filtering Strategy 30%
IN-1k
IN Dist
VTAB
Retrieval
AverageComparison of Methods Across Different MetricsFigure 5: Results of negCLIPLoss with a different number of batch samples (denoted as K) on
DataComp-medium. Solid lines denote negCLIPLoss, while dashed lines denote CLIPScore. Here,
we use OAI CLIP-L/14 as the pretrained model. We can see that once K≥5, negCLIPLoss
consistently outperforms CLIPScore across all subtask metrics. In the main paper, we set K= 10 .
Table 7: Results on DataComp-medium from the top methods that use only OpenAI’s CLIP-B/32
model or public version of DFN (DFN-P).
OAI CLIP-B/32Dataset IN-1k IN Dist. Shift VTAB Retrieval Avg.
Size (1 sub-task) (5) (11) (3) (38)
CLIPScore (20%) 22M 27.0 23.8 33.0 22.9 32.2
CLIPScore (30%) 33M 27.6 24.2 33.6 25.1 33.2
negCLIPLoss (20%) 22M 28.9 24.8 34.3 24.3 33.0
negCLIPLoss (30%) 33M 28.8 25.1 33.7 26.6 33.6
negCLIPLoss (30%) ∩NormSim ∞(Target) 22M 32.4 27.4 35.9 26.3 35.2
DFN-P
CLIPScore (15%) 16M 25.9 23.3 32.9 21.9 31.6
CLIPScore (17.5%) 19M 30.2 26.8 34.1 26.5 33.8
CLIPScore (20%) 22M 29.7 26.8 33.0 27.0 33.1
CLIPScore (30%) 33M 28.4 24.7 33.2 26.8 32.7
negCLIPLoss (15%) 16M 31.3 27.3 35.8 26.4 34.6
negCLIPLoss (17.5%) 19M 31.2 27.5 35.7 27.0 34.7
negCLIPLoss (20%) 22M 30.7 27.4 33.6 27.5 33.8
negCLIPLoss (30%) 33M 28.9 25.5 33.4 27.3 33.2
negCLIPLoss (30%) ∩NormSim ∞(Target) 22M 29.4 23.6 33.5 24.2 32.5
negCLIPLoss (17.5%) ∩NormSim ∞(Target) 16M 31.5 26.4 34.6 25.4 34.4
negCLIPLoss (17.5%) ∩NormSimB/32
∞(Target) 16M 31.6 27.3 37.2 25.5 35.7
find the nearest train data for each target, that train data may be sometimes random and not helpful.
On the other hand, NormSim ∞will not select this kind of data. It will select the data whose best
similarity score exceeds some general threshold, rather than just consider ranks.
D.4 Vision-Only NormSim is Better than Using Both Vision and Language
In DataComp [ 1], they show that image-based filtering is better than text-based filtering. In our paper,
we also do an ablation study to support this. Due to the restriction of computation resources, we run
ourNormSim 2(IN-1k) and NormSim 2-Don DataComp-small as an example. Since ImageNet-1k
28Table 8: Comparison between NormSim ∞and nearest neighbor selection. We use OAI CLIP-L/14
as the teacher model and assume both methods have been intersected with negCLIPLoss (30%). The
size of the selected subset is 22M.
Filtering Strategy IN-1k VTAB Avg.
negCLIPLoss (30%) 27.9 33.2 32.9
Nearest Neibor Selection 31.5 34.9 34.0
NormSim ∞(Target) 31.7 36.0 35.0
only has labels rather than long texts for describing images, we need to generate the caption before
calculating NormSim 2(IN-1k). We select 80 templates as the original CLIP paper [ 4], generate
prompts for each class, and take the mean of their embeddings as the representative text embedding
for images within that class.
The results are in Table 9. We can see that for both metrics, we have “image only” > “image ×text”
> “text only” . We believe the reason for NormSim 2(IN-1k) is that the images themselves can convey
significantly more features than the text prompts generated by labels. For NormSim 2-D, it should be
related to the large amounts of low-quality captions in the web-curated dataset. And “image ×text”
will also be influenced by the informativeness and the quality of captions. In short, for NormSim,
using vision-only embeddings is a best choice.
Table 9: Ablation Study on the NormSim and its variants on DataComp-small (11M). All experiments
first select 45% data based on the CLIP score, then use corresponding approaches to obtain 3.3M
data.“image” or “text” means using the variance of image or text embeddings to represent ¯Σtarget, and
“image ×text” means representing ¯Σtarget with the cross-covariance of image and text embeddings.
Filtering Strategy ∩CLIP score (45%) IN-1k IN Dist. Shift VTAB Retrieval Average
Random Sampling 4.2 4.9 17.2 11.6 15.6
NormSim (IN-1k, image) 5.2 5.5 19.0 12.2 17.4
NormSim (IN-1k, text) 3.9 4.2 16.3 11.3 14.9
NormSim (IN-1k, image ×text) 4.3 4.9 17.5 11.8 15.9
NormSim-D (image) 4.7 5.4 19.7 11.7 17.3
NormSim-D (text) 3.5 4.1 16.7 11.1 15.4
NormSim-D (image ×text) 3.6 4.2 18.4 11.1 15.8
E Additional Visualization
We further visualize8more data with different negCLIPLoss in Figure 6, 7 and 8. And similar for
NormSim ∞(Target) in Figure 9, 10 and 11.
8We use https://github.com/ypwang61/research_tools/blob/main/visualization2.py (Im-
ageCaptionVisualizer) for visualizing the dataset. We also recommend visualizing basic dataset statistics
byhttps://lst627.github.io/visdatacomp.github.io/ .
29Figure 6: Visualization of a small subset whose negCLIPLoss rank top 100% high in DataComp-
medium.
30Figure 7: Visualization of a small subset whose negCLIPLoss rank top 50% high in DataComp-
medium.
31Figure 8: Visualization of a small subset whose negCLIPLoss rank top 10% high in DataComp-
medium.
32Figure 9: Visualization of the images from a small subset whose NormSim ∞(Target) rank top 100%
high in DataComp-medium.
33Figure 10: Visualization of the images from a small subset whose NormSim ∞(Target) rank top 50%
high in DataComp-medium.
34Figure 11: Visualization of the images from a small subset whose NormSim ∞(Target) rank top 10%
high in DataComp-medium.
35F NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Yes we clearly define 1. the benchmark we are using; 2.the methods with its
key insights 3.the empirical improvement.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss this briefly in the last section.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
36Justification: The full version of theory of NormSim results are in Appendix. A and we
provide all the assumptions and proofs. We briefly mentioned this in Sec. 3.2.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The main results are in the Sec. 4. We also provide experiment details in
Appendix C.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
37Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code will be provided according to Neurips code submission guidance.
After got accepted, we will open source that.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The main results are in the Sec.4. We also provide experiment details in
Appendix C
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: Almost all existing works, like DFN, HYPE, and MLM, only run the training
once on DataComp-medium. Training on a 128M size dataset is very costly and relatively
stable, so it is commonly believed that there is no need to rerun experiments with different
training seeds. In the experiments, we fix all the training seeds to be 0 for fair comparison.
For our algorithm, most of them are deterministic. The only one involving randomness
is negCLIPLoss, which requires resampling K=10 times. For it we provide a sensitivity
analysis in Fig. 5.
Guidelines:
• The answer NA means that the paper does not include experiments.
38•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We discuss the computing cost estimation and comparison in Appendix C.1.
We didn’t explicitly calculate the memories since it is quite standard under the DataComp
benchmark.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Yes
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
39Justification: This research focuses on the methodology part of data selection. All experi-
ments are performed under the existing standard dataset. So as long as those datasets itself
maybe harmless, our research will not make any negative impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper will only provide UID’s of selected data from existing datasets
(DataComp-medium [1]). This paper will not release any model or new dataset.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We cite the DataComp [ 1] which introduces the URL for the dataset and the
code/models used to implement the benchmark.
Guidelines:
• The answer NA means that the paper does not use existing assets.
40• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing or research with human subjects.
All metrics are fixed evaluations.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing or research with human subjects.
41Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
42