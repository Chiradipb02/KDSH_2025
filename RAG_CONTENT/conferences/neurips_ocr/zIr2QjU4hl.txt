Bridging Model-Based Optimization
and Generative Modeling via
Conservative Fine-Tuning of Diffusion Models
Masatoshi Uehara1∗Yulai Zhao2∗Ehsan Hajiramezanali1Gabriele Scalia1
Gokcen Eraslan1Avantika Lal1Sergey Levine3†Tommaso Biancalani1†
1Genentech2Princeton University3UC Berkeley
Abstract
AI-driven design problems, such as DNA/protein sequence design, are commonly
tackled from two angles: generative modeling, which efficiently captures the
feasible design space (e.g., natural images or biological sequences), and model-
based optimization, which utilizes reward models for extrapolation. To combine the
strengths of both approaches, we adopt a hybrid method that fine-tunes cutting-edge
diffusion models by optimizing reward models through RL. Although prior work
has explored similar avenues, they primarily focus on scenarios where accurate
reward models are accessible. In contrast, we concentrate on an offline setting
where a reward model is unknown, and we must learn from static offline datasets, a
common scenario in scientific domains. In offline scenarios, existing approaches
tend to suffer from overoptimization, as they may be misled by the reward model
in out-of-distribution regions. To address this, we introduce a conservative fine-
tuning approach, BRAID, by optimizing a conservative reward model, which
includes additional penalization outside of offline data distributions. Through
empirical and theoretical analysis, we demonstrate the capability of our approach to
outperform the best designs in offline data, leveraging the extrapolation capabilities
of reward models while avoiding the generation of invalid designs through pre-
trained diffusion models. The main code is available at https://github.
com/masa-ue/RLfinetuning_Diffusion_Bioseq .
1 Introduction
Computational design involves synthesizing designs that optimize a particular reward function.
This approach finds applications in various scientific domains, including DNA/RNA/protein design
(Sample et al., 2019; Gosai et al., 2023; Wu et al., 2024). While physical simulations are often used in
design problems, lacking extensive knowledge of underlying physical processes necessitates solutions
that solely rely on experimental data. In these scenarios, we need an algorithm that synthesizes
an improved design by utilizing a dataset of past experiments (i.e., static offline dataset ). Existing
research has addressed computational design from two primary angles. The first angle is generative
modeling such as diffusion models (Ho et al., 2020), which aim to directly model the distribution of
valid designs by emulating the offline data. This approach allows us to model the space of “valid”
designs (e.g., natural images, natural DNA sequences, foldable protein sequences (Avdeyev et al.,
2023)). The second angle is offline model-based optimization (MBO), which entails learning the
reward model from static offline data and optimizing it with respect to design inputs (Brookes et al.,
2019; Trabucco et al., 2021; Angermueller et al., 2019; Linder and Seelig, 2021; Fannjiang and
∗Equal contribution: uehara.masatoshi@gene.com ,yz6292@princeton.edu
†Corresponding authors: svlevine@eecs.berkeley.edu ,biancalt@gene.com
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Figure 1: The left figure illustrates our setup with a pre-trained generative model and offline data.
On the right, the motivation of the algorithm is depicted. The region surrounded by the green line
is the original entire design space, with the colored region indicating the valid design space (e.g.,
natural images, human-like DNA sequences). The red region denotes areas with more offline data
available, while the blue region indicates areas with less data available. We aim to add penalties to
the blue regions using conservative reward modeling to prevent overoptimization while imposing a
stricter KL penalty on the non-colored regions to prevent the generation of invalid designs.
Listgarten, 2020; Chen et al., 2022). This class of methods potentially enables us to surpass the best
design observed in the offline data by harnessing the extrapolative capabilities of reward models.
In our work, we explore how the generative modeling and MBO perspectives could be reconciled,
inspired by recent work on RL-based fine-tuning of diffusion models (e.g., Black et al. (2023); Fan
et al. (2023)), which aims to finetune diffusion models by optimizing down-stream reward functions.
Although these studies do not originally address computational design, we can potentially leverage
the strengths of both perspectives. However, these existing studies often focus on scenarios where
online reward feedback can be queried or accurate reward functions are available. Such approaches
are not well-suited for the typical offline setting, where we lack access to true reward functions and
need to rely solely on static offline data (Levine et al., 2020; Kidambi et al., 2020; Yu et al., 2020). In
scientific fields, this offline scenario is common due to the high cost of acquiring feedback data. In
such contexts, existing works for fine-tuning diffusion models may easily lead to overoptimization,
where optimized designs are misled by the trained reward model from the offline data, resulting in
out-of-distribution adversarial designs instead of genuinely high-quality designs.
To mitigate overoptimization, we develop a conservative fine-tuning approach for generate models
aimed at computational design. Specifically, we consider a critical scenario where we have offline
data (with feedback) and a pre-trained diffusion model capable of capturing the space of “valid”
designs, and propose a two-stage method (Figure 1). In the initial stage, we train a conservative
reward model using offline data, incorporating an uncertainty quantification term that assigns higher
penalties to out-of-distribution regions. Subsequently, we finetune pre-trained diffusion models by
optimizing the conservative reward model to obtain high-quality designs and prevent the generation
of out-of-distribution designs. In the fine-tuning process, we also introduce a KL penalization term to
ensure that the generated designs remain within the valid design space.
Our primary contribution lies in the introduction of a novel framework, BRAID (douBly conseRvA-
tive fine-tuning diffusIon moDels). The term “doubly conservative” reflects the incorporation of two
types of conservative terms, both in reward modeling and KL penalization. By properly penalizing
the fine-tuned diffusion model when it deviates significantly from the offline data distribution, we
effectively address overoptimization. Additionally, by framing our fine-tuning procedure within the
context of soft-entropy regularized Markov Decision Processes, we offer theoretical justification for
the inclusion of these conservative terms in terms of regret. This theoretical result shows that fine-
tuned generative models outperform the best designs in the offline data, leveraging the extrapolation
capabilities of reward models while avoiding the generation of invalid designs. Furthermore, through
empirical evaluations, we showcase the efficacy of our approach across diverse domains, such as
DNA/RNA sequences and images.
22 Related Works
We summarize related works. For additional works such as fine-tuning on LLMs, refer to Section A.
Fine-tuning diffusion models via reward functions. Several previous studies have aimed to
improve diffusion models by optimizing reward functions using various methods, including supervised
learning (Lee et al., 2023; Wu et al., 2023), RL (Black et al., 2023; Fan et al., 2023) and control-based
techniques (Clark et al., 2023; Xu et al., 2023; Prabhudesai et al., 2023). In contrast to our work,
their emphasis is not on an offline setting, i.e., their setting assumes online reward feedback is
available or accurate reward functions are known. Additionally, while Fan et al. (2023) include
the KL term in their algorithms, our innovation lies in integrating conservative reward modeling to
mitigate overoptimization and formal statistical guarantees in terms of regret (Theorem 1, 2).
Conditional diffusion models. Conditional diffusion models, which learn conditional distributions
of designs given the rewards, have been extensively studied (Ho and Salimans, 2022; Dhariwal
and Nichol, 2021; Song et al., 2020; Bansal et al., 2023). However, for the purpose of MBO,
these approaches require that the offline data has good coverage on values we want to condition on
(Brandfonbrener et al., 2022). Compared to conditional diffusion models, our approach aims to obtain
designs that can surpass the best design in offline data by leveraging the extrapolation capabilities of
reward models. We compare these approaches with our work in Section 7.
Offline model-based optimization (MBO). Offline MBO is also known as offline black-box
optimization and is closely related to offline contextual bandits and offline RL (Levine et al., 2020).
While conservative approaches have been studied there (e.g., Kidambi et al. (2020); Yu et al. (2020)
and more in Section A); most of the works are not designed to incorporate a diffusion model, unlike
our approach. Hence, it remains unclear how these methods can generate designs that remain within
intricate valid design spaces (e.g., generating natural images).
It is worth noting a few exceptions (Yuan et al., 2023; Krishnamoorthy et al., 2023) that attempt
to integrate diffusion models into MBO. However, the crucial distinctions lie in the fact that we
directly optimize rewards with diffusion models, whereas these prior works focus on using conditional
diffusion models. Additionally, we delve into the incorporation of conservative terms, an aspect not
explored in their works. We compare these methods with ours empirically in Section 7.
3 Preliminaries
We outline our framework for offline model-based optimization with a pre-trained generative model.
Subsequently, we highlight the challenges arising from distributional shift. Additionally, we provide
an overview of diffusion models, as we will employ them as pre-trained generative models.
3.1 Offline Model-Based Optimization with Pre-Trained Generative Model
Our objective is to find a high-quality design within a design space, X. Each design x∈ X is
associated with a reward, r(x), where r:X → [0,1]is an unknown reward function. Then, our aim
is to find a high-quality generative model p∈∆(X), that yields a high r(x). It is formulated as
argmaxp∈∆(X)Ex∼p[r(x)]. (1)
Avoiding invalid designs. In MBO, the design space Xis typically huge. However, in practice, the
valid design space denoted by Xpreis effectively contained within this extensive Xas a potentially
lower-dimensional manifold. For instance, in biology, our focus often centers around discovering
highly bioactive protein sequences. While the raw search space might encompass |20|Bpossibilities
(where Bis the length), the actual design space corresponding to valid proteins is significantly more
constrained. Consequently, our problem can be formulated as:
argmax
p∈∆(Xpre)Ex∼p[r(x)],(eqivaletnly, argmax
p∈∆(X)Ex∼p[r(x)]−Ex∼p[I(x /∈ Xpre)]). (2)
Note supposing that a reward r(·)is0outside of Xpre, this is actually still equivalent to (1).
3Offline data with a pre-trained generative model. Based on the above motivation, we consider
scenarios where we have an offline dataset Doff, used for learning the reward function. More
specifically, the dataset, Doff={x(j), y(j)}noff
j=1contains pairs of designs x∼poff(·)and their
associated noisy reward feedbacks y=r(x) +ϵ, where ϵis noise.
Compared to settings in many existing papers on MBO, we also assume access to a pre-trained
generative model (diffusion model) trained on a large dataset comprising valid designs, in addition to
the offline data Doff. For example, in biology, this is expected to capture the valid design space Xpre
such as human DNA sequences or physically feasible proteins (Avdeyev et al., 2023; Li et al., 2024;
Sarkar et al., 2024; Stark et al., 2024; Campbell et al., 2024). These pre-trained generative models are
anticipated to be beneficial for narrowing down the raw search space Xto the design space Xpre. In
our work, denoting the distribution induced by the pre-trained model by ppre, we regard the support
ofppreasXpre.
3.2 Challenge: Distributional Shift
To understand our challenges, let’s first examine a simple approach for MBO with a pre-trained
generative model. For instance, we can adapt methods from Clark et al. (2023); Prabhudesai et al.
(2023) to our scenario. This approach involves two steps. In the first step, we perform reward
learning: ˆr= argmin˜r∈FPnoff
i=1{˜r(x(i))−y(i)}2,where Frepresents a function class that includes
mappings from Xto[0,1], aiming to capture the true reward function r(·). Then, in the second step,
we fine-tune a pre-trained diffusion model to optimize ˆr.
Despite its simplicity, this approach faces two types of distributional shifts. Firstly, the fine-tuned
generative model might produce invalid designs outside of Xpre. As discussed in Section 3.1, we aim
to prevent this situation. Secondly, the fine-tuned generative model may over-optimize ˆr, exploiting
uncertain regions of the learned model ˆr. Indeed, in regions not covered by offline data distribution
poff, the learned reward ˆrcan easily have higher values, while the actual reward values in terms of r
might be lower due to the higher uncertainty. We aim to avoid situations where we are misled by
out-to-distribution adversarial designs.
3.3 Diffusion Models
We present an overview of denoising diffusion probabilistic models (DDPM) (Song et al., 2020;
Ho et al., 2020; Sohl-Dickstein et al., 2015). Note while the original diffusion model was initially
introduced in Euclidean spaces, it has since been extended to simplex spaces for biological sequences
(Avdeyev et al., 2023), which we will use in Section 7. In diffusion models, the goal is to develop a
generative model that accurately emulates the data distribution from the dataset. Specifically, denoting
the data distribution by ppre∈∆(X), a DDPM aims to approximate using a parametric model
structured as p(x0;θ) =R
p(x0:T;θ)dx1:T, where p(x0:T;θ) =pT+1(xT;θ)Q1
t=Tpt(xt−1|xt;θ).
Here, each ptis considered as a policy, which is a mapping from a design space Xto a distribution
overX. By optimizing the variational bound on the negative log-likelihood, we can obtain a set
of policies {pt}1
t=T+1such that p(x0;θ)≈ppre(x0). For simplicity, in this work, assuming that
pre-trained diffusion models are accurate, we denote the pre-trained policy as {ppre
t(·|·)}1
t=T+1, and
the generated distribution by the pre-trained diffusion model at x0byppre. With slight abuse of
notation, we often denote ppre
T+1(·)byppre
T+1(·|·)
4 Doubly Conservative Generative Models
We’ve discussed how na ¨ıve approaches for computational design may yield invalid designs or over-
optimize reward functions, with both challenges stemming from distributional shift. Our goal in this
section is to develop doubly conservative generative models to mitigate this distributional shift.
4.1 Avoiding Invalid Designs
To avoid invalid designs, we begin by considering the following generative model:
exp(ˆr(·)/α)ppre(·)R
exp(ˆr(x)/α)ppre(x)dx(:= argmax
p∈∆(X)Ex∼p[ˆr(x)]−αKL(p∥ppre)), (3)
4where KL(p∥ppre) =Ex∼p[log(p(x)/ppre(x))]. In this formulation, the generative model is designed
as an optimizer of a loss function composed of two parts: the first component encourages designs
with high rewards, while the second component acts as a regularizer penalizing the generative model
for generating invalid designs. This formulation is inspired by our initial objective in (2), where we
substitute an indicator function with log(p/ppre). This regularizer takes ∞when pis not covered by
ppre, and αgoverns the strength of the regularizer.
4.2 Avoiding Overoptimization
Next, we address the issue of overoptimization. This occurs when we are fooled by the learned
reward model in uncertain regions. Therefore, a natural approach is to penalize generative models
when they produce designs in uncertain regions.
As a first step, let’s consider having an uncertainty oracle ˆg:X → [0,1], which is a random variable
ofDoff. This oracle is expected to quantify the uncertainty of the learned reward function ˆr.
Assumption 1 (Uncertanity oracle) .With probability 1−δ, we have
∀x∈ Xpre;|ˆr(x)−r(x)| ≤ˆg(x) (4)
These calibrated oracles are well-established when using a variety of models such as linear models,
Gaussian processes, and neural networks. We will provide detailed examples of such calibrated
oracles in Section 4.3. Essentially, as long as the reward model is well-specified (i.e., there exists
˜r∈ F such that ∀x∈ Xpre: ˜r(x) =r(x)), we can create such a calibrated oracle.
Doubly Conservative Generative Models. Utilizing the uncertainty oracle defined in Assump-
tion 1, we present our proposal:
ˆπα(·) =exp ((ˆ r−ˆg)(·)/α)ppre(·)R
exp ((ˆ r−ˆg)(x)/α)ppre(x)dx(:= argmax
p∈∆(X)Ex∼p[(ˆr−ˆg)(x)]| {z }
Penalized reward−αKL(p∥ppre)|{z}
KL Penalty).(5)
Here, to combat overoptimization, we introduce an additional penalty term ˆg(x). This penalty term is
expected to prevent ˆπαfrom venturing into regions with high uncertainty because it would take a
higher value in such regions. We refer to ˆπαas a doubly conservative generative model due to the
incorporation of two conservative terms.
An attentive reader might question the necessity of simultaneously introducing two conservative terms.
Specifically, the first natural question is whether KL penalties, intended to prevent invalid designs, can
replace uncertainty-oracle-based penalties. However, this may not hold true because even if we can
entirely avoid venturing outside of Xpre(support of ppre), we may still output designs on uncertain
regions not covered by poff. The second question is whether uncertainty-oracle-based penalties can
substitute KL penalties. While it is partly true in situations where the support of poffis contained
within that of ppre, uncertainty-oracle-based penalties, lacking leverage on pre-trained generative
models, are ineffective in preventing invalid designs. In contrast, KL penalties are considered a more
direct approach to stringently avoid invalid designs by leveraging pre-trained generative models.
4.3 Examples of Uncertainty Oracles
Example 1 (Gaussian processes.) .When we use an RKHS as F(a.k.a. GPs) associated with a kernel
k(·,·) :X × X → R(Srinivas et al., 2009), a typical construction of ˆrandˆgis
ˆr(·) =Y(K+λI)−1k(·),ˆg(·) =c(δ)q
ˆk(·,·),
where c(δ)∈R>0, λ∈R>0,k(x) = [k(x(1), x),···, k(x(noff), x)]⊤,
Y= [y(1),···, y(noff)],{K}p,q=k(x(p), x(q)),ˆk(x, x′) =k(x, x′)−k(x)⊤{K+λI}−1k(x′).
Note that when using deep neural networks, by considering the last layer as a feature map, we can
still create a kernel (Zhang et al., 2022; Qiu et al., 2022).
Example 2 (Bootstrap) .When we use neural networks as F, it is common to use a statistical
bootstrap method. Note many variants have been proposed (Chua et al., 2018), and its theory
has been analyzed (Kveton et al., 2019). Generally, in our context, we generate multiple models
ˆr1,···,ˆrMby resampling datasets, and then consider argminiˆriasˆr−ˆg.
5Algorithm 1 BRAID (douBly conse RvAtive f Ine-tuning Diffusion models)
1:Require : Parameter α∈R+, a set of policy classes {Πt}where Πt⊂[X → ∆(X)], pre-trained
diffusion model {ppre
t}1
t=T+1.
2:Train a conservative reward model ˆr−ˆgusing an offline data Doff.
3:Update a diffusion model as {ˆpt}tby solving the planning problem:
{ˆpt}t= argmax
{pt∈Πt}1
t=T+1E{pt}[ˆr(x0)−ˆg(x0)]| {z }
Penalized reward−αΣ1
t=T+1E{pt}[KL(pt(·|xt)∥ppre
t(·|xt))]| {z }
KL penalty(6)
where the expectation E{pt}[·]is taken with respect toQ1
t=T+1pt(xt−1|xt).
4:Output : A policy {ˆpt}t
5 Conservative Fine-tuning of Diffusion Models
In this section, we consider how to sample from a doubly conservative generative model ˆπα, using
diffusion models as pre-trained generative models. Our algorithm is outlined in Algorithm 1. Initially,
we learn a penalized reward ˆr−ˆgfrom the offline data and set it as a target to prevent overoptimization
in(6). Additionally, we integrate a KL regularization term to prevent invalid designs. The parameter
αgoverns the intensity of this regularization term.
Formally, this phase can be conceptualized as a planning problem in soft-entropy-regularized MDPs
(Neu et al., 2017; Geist et al., 2019). In this MDP formulation:
• The state space Sand action space Acorrespond to the design space X.
• The reward at time t∈[0,···, T](∈ S × A → R) is provided only at Tasˆr−ˆg.
• The transition dynamics at time t(∈[S × A → ∆(S)]) is an identity δ(st+1=at).
• The policy at time t(∈ S → ∆(A)) corresponds to pT+1−t:X → ∆(X).
• The reference policy at tis a policy in the pre-trained model ppre
T+1−t
In these entropy-regularized MDPs, the soft optimal policy corresponds to {ˆpt}. Importantly, we can
analytically derive the fine-tuned distribution in Algorithm 1 and show that it simplifies to a doubly
conservative generative model ˆπα, from which we aim to sample.
Theorem 1. Letˆpα(·)be an induced distribution from optimal policies {ˆpt}1
t=T+1in(6), i.e.,
ˆp(x0) =R
{Q1
t=T+1ˆpt(xt−1|xt)}dx1:Twhen{Πt}is a global policy class ( Πt={X → ∆(X)}).
Then,
ˆpα(x) = ˆπα(x).
We have deferred to the proof in Section 5.1. While similar results are known in the context of
standard entropy regularized RL (Levine, 2018), our theorem is novel because previous studies did
not consider pre-trained diffusion models.
Training algorithms. Based on Theorem 1, to sample from ˆπα, what we need to is to solve
Equation (6). We can employ any off-the-shelf RL algorithms to solve this planning problem.
Given that the transition dynamics are known, and differentiable reward models are constructed in
our scenario, a straightforward approach to optimize (6)is to directly optimize differentiable loss
functions with respect to parameters of neural networks in policies, as detailed in Appendix B. Indeed,
this approach has recently been used in fine-tuning diffusion models (Clark et al., 2023; Prabhudesai
et al., 2023), demonstrating its stability and computational efficiency.
Remark 1 (Novelty of Theorem 1) .A theorem similar to Theorem 1 has been proven for continuous-
time diffusion models in Euclidean space (Uehara et al., 2024, Theorem 1). However, the primary
distinction lies in the fact that while their findings are restricted to Euclidean space, where diffusion
policies take Gaussian polices, our results are not constrained to any specific domain. Hence, for
example, our Theorem 1 can handle scenarios where the domain is discrete or lies on the simplex
space (Avdeyev et al., 2023) in order to model biological sequences as we do in Section 7.
65.1 Sketch of the Proof of Theorem 1
We explain the sketch of the proof of Theorem 1. The detail is deferred to Theorem C.1.
By induction from t= 0tot=T+ 1, we can first show
ˆpt(xt−1|xt) =exp(vt−1(xt−1)/α)ppre
t−1(xt−1|xt)
exp(vt(xt)/α). (7)
Here, vt(xt)is a soft optimal value function:
Eˆp[ˆr(x0)−ˆg(x0)−α1X
k=tKL(pk(·|xk)∥ppre
k(·|xk))|xt],
which satisfies an equation analogous to the soft Bellman equation: v0(x) = ˆr(x)−ˆg(x)and for
t= 1tot=T+ 1,
exp(vt(xt)
α) =Z
expvt−1(xt−1)
α
ppre
t(xt−1|xt)dxt−1. (8)
Now, we aim to calculate a marginal distribution at tdefined by ˆpt(xt) =R
{Qt
k=t+1ˆpk(xk−1|xk)}dxt+1:T. Then, by induction, we can show that
ˆpt(xt) = exp( vt(xt)/α)ppre
t(xt)/C (9)
where Cis a normalizing constant. Indeed, supposing that the above (9)hold at t, the equation (9)
also holds for t−1as follows:Z
ˆpt−1(xt−1|xt)ˆpt(xt)dxt= exp( vt−1(xt−1)/α)ppre
t−1(xt−1)/C= ˆpt−1(xt−1).
Finally, by setting t= 0, the statement in Theorem 1 is concluded.
6 Regret Guarantee
In this section, our objective is to demonstrate that a policy ˆpαfrom Algorithm 1 can provably
outperform designs in offline data by establishing the regret guarantee.
To assess the performance of our fine-tuned generative model, we introduce the soft-value metric:
Jα(p) :=Ex∼p[r(x)]−αKL(p∥ppre).
This metric comprises two components: the expected reward and a penalty term applied when p
produces invalid outputs, as we see in (3). Now, in terms of soft-value Jα(p), our proposal ˆpαoffers
the following guarantee.
Theorem 2 (Per-step regret) .Suppose Assumption 1. Then, with probability 1−δ, we have
∀π∈∆(X);Jα(π)−Jα(ˆpα)|{z }
Per step regret≤2p
Cπ×Ex∼poff[ˆg(x)2]1/2
| {z }
Stat, C π:= max
x∈Xoffπ(x)
poff(x),
where Xoff={x∈ X:poff(x)>0}. As an immediate corollary,
Ex∼π[r(x)]−Ex∼ˆpα[r(x)]≤αKL(π∥ppre) + 2p
Cπ×Ex∼poff[ˆg(x)2]1/2.
In the theorem above, we establish that the per-step regret against a generative model πwe aim to
compete with is small as long as the generative model πfalls within Xoffand the learned model ˆris
calibrated as in Assumption 1. First, the term (Stat) corresponds to the statistical error associated
withˆrover the offline data distribution poff. When the model is well-specified, it is upper-bounded
byp¯d/n, where ¯drepresents the effective dimension of F, as we will discuss shortly. Secondly, the
termCπcorresponds to the coverage between a comparator generative model πand our generative
model ˆpα. Hence, it indicates that the performance of our learned ˆpαis at least as good as that of a
comparator generative model πcovered by poff. While this original coverage term Cπdiverges when
πgoes outside of Xoff, we can refine it using the extrapolation capabilities of a function class F, as
we will discuss shortly. This refined version ensures that we can achieve high-quality designs that
outperform designs in the offline data (i.e., best designs in Xoff).
7Example 3. We consider a scenario where an RKHS is used for F. LetFbe a model represented by
an infinite-dimensional feature ϕ(·). Let ¯ddenote the effective dimension of F(Valko et al., 2013).
Corollary 1 (Informal: Formal characterization is in Section D ) .Assuming that the model is
well-specified, with probability 1−δ, we have:
Jα(π)−Jα(ˆpα)≤p¯Cπ×˜O r¯d3
n!
,¯Cπ:= sup
κ:∥κ∥2=1κ⊤Ex∼π[ϕ(x)ϕ⊤(x)]κ
κ⊤Ex∼poff[ϕ(x)ϕ⊤(x)]κ.
The refinement of the coverage term in ¯Cπis characterized as the relative condition number between
covariance matrices on a generative model πand an offline data distribution poff, which is smaller
thanCπ. This ¯Cπcould still be finite even if Cπis infinite. In this regard, Corollary 1 illustrates that
the trained generative model can outperform the best design in the offline data by harnessing the
extrapolation capabilities of reward models.
7 Experiments
We perform experiments to evaluate (a) the effectiveness of conservative methods for fine-tuning
diffusion models and (b) the comparison of our approach between existing methods for MBO with
diffusion models (Krishnamoorthy et al., 2023; Yuan et al., 2023). We will start by outlining the
baselines and explaining the experimental setups. Regarding more detailed setups, hyperparameters,
architecture of neural networks, and ablation studies, refer to Appendix E.
Methods to compare. We compare the following methods in our evaluation. For a fair comparison,
we always use the same αinBRAID andSTRL .3.
•BRAID (proposed method) : We consider two approaches: (1) Bonus , as in Example 1 by
setting a last layer as a feature map and constructing a kernel, (2) Bootstrap , as in Example 2.
•Standard RL (STRL) : RL-fine-tuning that optimizes the standard ˆrwithout any conservative
term, following existing works on fine-tuning (Clark et al., 2023; Prabhudesai et al., 2023).
•DDOM (Krishnamoorthy et al., 2023) : We train with weighted classifier-free guidance (Ho and
Salimans, 2022) using offline data, conditioning on a class with high yvalues (top 5%) during
inference. Note that this method is training from scratch rather than fine-tuning.
•Offline Guidance (Yuan et al., 2023) : After training a classifier using offline data, we use
guidance (conditional diffusion models) (Dhariwal and Nichol, 2021) on top of pre-trained
diffusion models and condition on classes with high yvalues (top 5%) at inference time.
Evaluation. We assess the performance of each generative model primarily by visualizing the
histogram of true rewards r(x)obtained from the generated samples. For completeness, we include
similar histograms for both the pre-trained model ( Pretrained ) and the offline dataset ( Offline ).
As for hyperparameter selection, such as determining the strengths of conservative terms/epochs,
we adhere to conventional practice in offline RL (e.g., Rigter et al. (2022); Kidambi et al. (2020);
Matsushima et al. (2020)) and choose the best one through a limited number of online interactions.
Remark 2. We omit comparisons with pure MBO methods for two reasons: (i) DDOM , which we
compare against, already demonstrates a good performance across multiple datasets, and (ii) these
methods are unable to model complex valid spaces since they do not incorporate state-of-the-art
generative models (e.g., stable diffusion), thereby lacking the capability to generate valid designs
(e.g., natural images) as we show in Section 7.2.
7.1 Design of Regulatory DNA/RNA Sequences
We examine two publicly available large datasets consisting of enhancers ( n≈700k) (Gosai et al.,
2023) and UTRs ( n≈300k) (Sample et al., 2019) with activity levels collected by massively parallel
reporter assays (MPRA) (Inoue et al., 2019). These datasets have been extensively used in sequence
3Regarding the effectiveness of KL-regularization, it has been discussed in Fan et al. (2023); Uehara et al.
(2024). Hence, in our work, we focus on the effectiveness of conservatism in reward modeling.
8Offline Pretrained DDOM Guidance STRL BRAID-Boot BRAID-Bonus
Method1.5
1.0
0.5
0.00.51.01.5MRL
(a) 5’UTRs
Offline Pretrained DDOM Guidance STRL BRAID-Boot BRAID-Bonus
Method0246810HepG2
 (b) Enhancers
Offline Pretrained Guidance STRL BRAID-Boot BRAID-Bonus
Method23456789Reward
 (c) Images
Figure 2: Barplots of the rewards r(x)for samples generated by each algorithm. It reveals that
proposals consistently outperform baselines.
/uni00000046/uni00000044/uni00000057/uni00000003/uni0000005f/uni00000003/uni0000001c/uni00000011/uni00000015/uni0000001b
 /uni00000047/uni00000052/uni0000004a/uni00000003/uni0000005f/uni00000003/uni0000001c/uni00000011/uni00000015/uni00000019
/uni0000004b/uni00000052/uni00000055/uni00000056/uni00000048/uni00000003/uni0000005f/uni00000003/uni0000001c/uni00000011/uni00000014/uni00000013
 /uni00000050/uni00000052/uni00000051/uni0000004e/uni00000048/uni0000005c/uni00000003/uni0000005f/uni00000003/uni0000001c/uni00000011/uni00000015/uni00000019
(a) Undesirable scenarios
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000034/uni00000058/uni00000048/uni00000055/uni0000004c/uni00000048/uni00000056/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000036/uni00000037/uni00000035/uni0000002f
/uni00000025/uni00000035/uni00000024/uni0000002c/uni00000027/uni00000010/uni00000025/uni00000052/uni00000052/uni00000057
/uni00000025/uni00000035/uni00000024/uni0000002c/uni00000027/uni00000010/uni00000025/uni00000052/uni00000051/uni00000058/uni00000056(b) Training curves in terms of ˆr
(but not r)
/uni00000018/uni00000011/uni0000001c/uni00000013/uni00000032/uni00000049/uni00000049/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000003
/uni00000003/uni0000002a/uni00000058/uni0000004c/uni00000047/uni00000044/uni00000051/uni00000046/uni00000048/uni00000046/uni00000044/uni00000057
/uni00000018/uni00000011/uni00000016/uni0000001a/uni00000047/uni00000052/uni0000004a
/uni00000019/uni00000011/uni00000013/uni00000013/uni00000045/uni00000058/uni00000057/uni00000057/uni00000048/uni00000055/uni00000049/uni0000004f/uni0000005c
/uni00000018/uni00000011/uni0000001c/uni00000016/uni00000053/uni00000048/uni00000044/uni00000046/uni00000052/uni00000046/uni0000004e
/uni00000019/uni00000011/uni0000001b/uni00000014/uni00000036/uni00000037/uni00000035/uni0000002f
/uni0000001a/uni00000011/uni00000017/uni00000016
 /uni00000019/uni00000011/uni00000019/uni00000018
 /uni00000019/uni00000011/uni0000001a/uni00000014
/uni0000001a/uni00000011/uni00000018/uni00000015/uni00000025/uni00000052/uni00000052/uni00000057
/uni0000001a/uni00000011/uni0000001b/uni0000001c
 /uni0000001a/uni00000011/uni00000019/uni00000015
 /uni0000001a/uni00000011/uni0000001b/uni00000013
/uni0000001a/uni00000011/uni00000018/uni00000017/uni00000025/uni00000052/uni00000051/uni00000058/uni00000056
/uni0000001a/uni00000011/uni0000001c/uni00000015
 /uni0000001a/uni00000011/uni00000016/uni00000019
 /uni0000001a/uni00000011/uni00000016/uni00000016 (c) Generated images
Figure 3: Results on Image Generation
optimization for DNA and RNA engineering, particularly for the advancement of cell and RNA
therapy (Castillo-Hair and Seelig, 2021; Ghari et al., 2023; Lal et al., 2024; Ferreira DaSilva et al.,
2024). In the Enhancers dataset, each xis a DNA sequence with a length of 200, while y∈Ris the
measured activity in cell lines. For the UTRs dataset, xis a 5’UTR RNA sequence with a length of
50while y∈Ris the mean ribosomal load (MRL) measured by polysome profiling.
Setting of oracles and offline data. We aim to explore a scenario where we have a pre-trained
model and an offline dataset. Since the true reward function r(·)is typically unknown, we initially
divide the original dataset D={x(i), y(i)}randomly into two subsets: DoraandD′. Then, from D′,
we select datasets below 95% quantiles for enhancers and 60% quantiles for UTRs and define them as
offline datasets Doff. Subsequently, we construct an oracle r(·)by training a neural network on Dora
and use it for testing purposes. Here, we use an Enformer-based model, which is a state-of-the-art
model for DNA sequences (Avsec et al., 2021). Regarding pre-trained diffusion models, we use ones
customized for sequences over simplex space (Avdeyev et al., 2023). In the subsequent analysis, each
algorithm solely has access to the offline data Doffand a pre-trained diffusion model, but not r(·).
Results. The performance results in terms of r(·)are depicted in Fig 2a and b. It is seen that
fine-tuned generative models via RL outperform conditioning-based methods: DDOM andGuidance .
This is expected because conditional models themselves are not originally intended to surpass the
conditioned value ( ≈best value in the offline data). Conversely, fine-tuned generative models via RL
are capable of exceeding the best value in offline data by harnessing the extrapolation capabilities
of reward modeling, as also theoretically supported in Corollary 1. Secondly, both BRAID-boot
andBRAID-bonus demonstrate superior performance compared to STRL .This suggests that con-
servatism aids in achieving fine-tuned generative models with enhanced rewards while mitigating
overoptimization.
7.2 Image Generation
We consider the task of generating aesthetically pleasing images, following prior works (Fan et al.,
2023; Black et al., 2023). We use Stable Diffusion v1.5 as our pretrained diffusion model, which
can generate high-quality images conditioned on prompts such as “cat” and “dog”. We use the A V A
9dataset (Murray et al., 2012) as our offline data and employ a linear MLP on top of CLIP embeddings
to train reward models ( ˆrandˆr−ˆg) from offline data for fine-tuning.
Setting of oracles. To construct r(x), following existing works, we use the LAION Aesthetic
Predictor V2 (Schuhmann, 2022), already pre-trained on a large-scale image dataset. However,
this LAION predictor gives high scores even if generated images are almost identical regardless of
prompts, as in Figure 3b. These situations are undesirable because it means fine-tuned models are
too far away from pre-trained models. Hence, for our evaluation, we define r(x)as follows: (1)
asking vision language models (e.g., LLaV A (Liu et al., 2024)) whether images contain objects in
the original prompts4(e.g., dog, cat), (2) if Yes, outputting the LAION predictor, and (3) if No,
assigning 0. This evaluation ensures that high r(x)still indicates capturing the space of the original
stable diffusion.
Results. We show that our proposed approach outperforms the baselines in terms of r(x), as in
Fig 2c5. We also show the generated images in Figure 3cc. Additionally, we plot the training curve
during the fine-tuning process in terms of the mean of ˆr(x)of generated samples in Fig 3cb. The
results indicate that in STRL , while the learning curve based on the learned reward quickly grows,
fine-tuned models no longer necessarily remain within the space of pre-trained models ( STRL in
Fig 2c). In contrast, in our proposal, by carefully regularizing on regions outside of the offline data,
we can generate more aesthetically pleasing images than STRL , which remain within the space of
pre-trained models. For more images/ ablation studies, refer to Appendix E.
8 Summary
For the purpose of fine-tuning from offline data, we introduced a conservative fine-tuning approach
by optimizing a conservative reward model, which includes additional penalization outside of offline
data distributions. Through empirical and theoretical analysis, we demonstrate the capability of our
approach to outperform the best designs in offline data, leveraging the extrapolation capabilities of
reward models while avoiding the generation of invalid designs through pre-trained diffusion models.
References
Angermueller, C., D. Dohan, D. Belanger, R. Deshpande, K. Murphy, and L. Colwell (2019). Model-
based reinforcement learning for biological sequence design. In International conference on
learning representations .
Avdeyev, P., C. Shi, Y . Tan, K. Dudnyk, and J. Zhou (2023). Dirichlet diffusion score model for
biological sequence generation. In International Conference on Machine Learning , pp. 1276–1301.
PMLR.
Avsec, ˇZ., V . Agarwal, D. Visentin, J. R. Ledsam, A. Grabska-Barwinska, K. R. Taylor, Y . Assael,
J. Jumper, P. Kohli, and D. R. Kelley (2021). Effective gene expression prediction from sequence
by integrating long-range interactions. Nature methods 18 (10), 1196–1203.
Bansal, A., H.-M. Chu, A. Schwarzschild, S. Sengupta, M. Goldblum, J. Geiping, and T. Goldstein
(2023). Universal guidance for diffusion models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pp. 843–852.
Black, K., M. Janner, Y . Du, I. Kostrikov, and S. Levine (2023). Training diffusion models with
reinforcement learning. arXiv preprint arXiv:2305.13301 .
Brandfonbrener, D., A. Bietti, J. Buckman, R. Laroche, and J. Bruna (2022). When does return-
conditioned supervised learning work for offline reinforcement learning? Advances in Neural
Information Processing Systems 35 , 1542–1553.
Brookes, D., H. Park, and J. Listgarten (2019). Conditioning by adaptive sampling for robust design.
InInternational conference on machine learning , pp. 773–782. PMLR.
4The F1 score of LLaV A for detecting objects was 1.0as detailed in Appendix E.2.3. Hence, this part is
considered to be accurate.
5Note we omitted DDOM because it is not a fine-tuning method.
10Campbell, A., J. Yim, R. Barzilay, T. Rainforth, and T. Jaakkola (2024). Generative flows on discrete
state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint
arXiv:2402.04997 .
Castillo-Hair, S. M. and G. Seelig (2021). Machine learning for designing next-generation mrna
therapeutics. Accounts of Chemical Research 55 (1), 24–34.
Chang, J. D., M. Uehara, D. Sreenivas, R. Kidambi, and W. Sun (2021). Mitigating covariate shift in
imitation learning via offline data without great coverage. arXiv preprint arXiv:2106.03207 .
Chen, C., Y . Zhang, J. Fu, X. S. Liu, and M. Coates (2022). Bidirectional learning for offline
infinite-width model-based optimization. Advances in Neural Information Processing Systems 35 ,
29454–29467.
Chua, K., R. Calandra, R. McAllister, and S. Levine (2018). Deep reinforcement learning in a
handful of trials using probabilistic dynamics models. Advances in neural information processing
systems 31 .
Clark, K., P. Vicol, K. Swersky, and D. J. Fleet (2023). Directly fine-tuning diffusion models on
differentiable rewards. arXiv preprint arXiv:2309.17400 .
Dhariwal, P. and A. Nichol (2021). Diffusion models beat gans on image synthesis. Advances in
neural information processing systems 34 , 8780–8794.
Fakoor, R., J. W. Mueller, K. Asadi, P. Chaudhari, and A. J. Smola (2021). Continuous doubly
constrained batch reinforcement learning. Advances in Neural Information Processing Systems 34 ,
11260–11273.
Fan, Y ., O. Watkins, Y . Du, H. Liu, M. Ryu, C. Boutilier, P. Abbeel, M. Ghavamzadeh, K. Lee, and
K. Lee (2023). Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models.
arXiv preprint arXiv:2305.16381 .
Fannjiang, C. and J. Listgarten (2020). Autofocused oracles for model-based design. Advances in
Neural Information Processing Systems 33 , 12945–12956.
Ferreira DaSilva, L., S. Senan, Z. M. Patel, A. J. Reddy, S. Gabbita, Z. Nussbaum, C. M. V . Cordova,
A. Wenteler, N. Weber, T. M. Tunjic, et al. (2024). Dna-diffusion: Leveraging generative models
for controlling chromatin accessibility and gene expression via synthetic regulatory elements.
bioRxiv , 2024–02.
Geist, M., B. Scherrer, and O. Pietquin (2019). A theory of regularized markov decision processes.
InInternational Conference on Machine Learning , pp. 2160–2169. PMLR.
Ghari, P. M., A. Tseng, G. Eraslan, R. Lopez, T. Biancalani, G. Scalia, and E. Hajiramezanali (2023).
Generative flow networks assisted biological sequence editing. In NeurIPS 2023 Generative AI
and Biology (GenBio) Workshop .
G´omez-Bombarelli, R., J. N. Wei, D. Duvenaud, J. M. Hern ´andez-Lobato, B. S ´anchez-Lengeling,
D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik (2018).
Automatic chemical design using a data-driven continuous representation of molecules. ACS
central science 4 (2), 268–276.
Gosai, S. J., R. I. Castro, N. Fuentes, J. C. Butts, S. Kales, R. R. Noche, K. Mouri, P. C. Sabeti, S. K.
Reilly, and R. Tewhey (2023). Machine-guided design of synthetic cell type-specific cis-regulatory
elements. bioRxiv .
Ho, J., A. Jain, and P. Abbeel (2020). Denoising diffusion probabilistic models. Advances in neural
information processing systems 33 , 6840–6851.
Ho, J. and T. Salimans (2022). Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 .
Inoue, F., A. Kreimer, T. Ashuach, N. Ahituv, and N. Yosef (2019). Identification and massively
parallel characterization of regulatory elements driving neural induction. Cell stem cell 25 (5),
713–727.
11Kidambi, R., A. Rajeswaran, P. Netrapalli, and T. Joachims (2020). Morel: Model-based offline
reinforcement learning. Advances in neural information processing systems 33 , 21810–21823.
Kong, L., Y . Du, W. Mu, K. Neklyudov, V . De Bortol, H. Wang, D. Wu, A. Ferber, Y .-A. Ma, C. P.
Gomes, et al. (2024). Diffusion models as constrained samplers for optimization with unknown
constraints. arXiv preprint arXiv:2402.18012 .
Krishnamoorthy, S., S. M. Mashkaria, and A. Grover (2023). Diffusion models for black-box
optimization. arXiv preprint arXiv:2306.07180 .
Kumar, A., A. Zhou, G. Tucker, and S. Levine (2020). Conservative q-learning for offline reinforce-
ment learning. Advances in Neural Information Processing Systems 33 , 1179–1191.
Kveton, B., C. Szepesvari, S. Vaswani, Z. Wen, T. Lattimore, and M. Ghavamzadeh (2019). Garbage
in, reward out: Bootstrapping exploration in multi-armed bandits. In International Conference on
Machine Learning , pp. 3601–3610. PMLR.
Lal, A., D. Garfield, T. Biancalani, and G. Eraslan (2024). reglm: Designing realistic regulatory dna
with autoregressive language models. bioRxiv , 2024–02.
Lee, K., H. Liu, M. Ryu, O. Watkins, Y . Du, C. Boutilier, P. Abbeel, M. Ghavamzadeh, and S. S. Gu
(2023). Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192 .
Levine, S. (2018). Reinforcement learning and control as probabilistic inference: Tutorial and review.
arXiv preprint arXiv:1805.00909 .
Levine, S., A. Kumar, G. Tucker, and J. Fu (2020). Offline reinforcement learning: Tutorial, review,
and perspectives on open problems. arXiv preprint arXiv:2005.01643 .
Li, Z., Y . Ni, W. A. Beardall, G. Xia, A. Das, G.-B. Stan, and Y . Zhao (2024). Discdiff: Latent
diffusion model for dna sequence generation. arXiv preprint arXiv:2402.06079 .
Linder, J. and G. Seelig (2021). Fast activation maximization for molecular sequence design. BMC
bioinformatics 22 , 1–20.
Liu, H., C. Li, Q. Wu, and Y . J. Lee (2024). Visual instruction tuning. Advances in neural information
processing systems 36 .
Matsushima, T., H. Furuta, Y . Matsuo, O. Nachum, and S. Gu (2020). Deployment-efficient rein-
forcement learning via model-based offline optimization. arXiv preprint arXiv:2006.03647 .
Murray, N., L. Marchesotti, and F. Perronnin (2012). Ava: A large-scale database for aesthetic visual
analysis. In 2012 IEEE conference on computer vision and pattern recognition , pp. 2408–2415.
IEEE.
Neu, G., A. Jonsson, and V . G ´omez (2017). A unified view of entropy-regularized markov decision
processes. arXiv preprint arXiv:1705.07798 .
Notin, P., J. M. Hern ´andez-Lobato, and Y . Gal (2021). Improving black-box optimization in vae
latent space using decoder uncertainty. Advances in Neural Information Processing Systems 34 ,
802–814.
Ouyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,
K. Slama, A. Ray, et al. (2022). Training language models to follow instructions with human
feedback. Advances in neural information processing systems 35 , 27730–27744.
Paine, T. L., C. Paduraru, A. Michi, C. Gulcehre, K. Zolna, A. Novikov, Z. Wang, and N. de Fre-
itas (2020). Hyperparameter selection for offline reinforcement learning. arXiv preprint
arXiv:2007.09055 .
Prabhudesai, M., A. Goyal, D. Pathak, and K. Fragkiadaki (2023). Aligning text-to-image diffusion
models with reward backpropagation. arXiv preprint arXiv:2310.03739 .
12Qiu, S., L. Wang, C. Bai, Z. Yang, and Z. Wang (2022). Contrastive ucb: Provably efficient contrastive
self-supervised learning in online reinforcement learning. In International Conference on Machine
Learning , pp. 18168–18210. PMLR.
Rigter, M., B. Lacerda, and N. Hawes (2022). Rambo-rl: Robust adversarial model-based offline
reinforcement learning. Advances in neural information processing systems 35 , 16082–16097.
Rombach, R., A. Blattmann, D. Lorenz, P. Esser, and B. Ommer (2022, June). High-resolution image
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp. 10684–10695.
Sample, P. J., B. Wang, D. W. Reid, V . Presnyak, I. J. McFadyen, D. R. Morris, and G. Seelig (2019).
Human 5 utr design and variant effect prediction from a massively parallel translation assay. Nature
biotechnology 37 (7), 803–809.
Sarkar, A., Z. Tang, C. Zhao, and P. Koo (2024). Designing dna with tunable regulatory activity using
discrete diffusion. bioRxiv , 2024–05.
Schuhmann, C. (2022, Aug). Laion aesthetics.
Sohl-Dickstein, J., E. Weiss, N. Maheswaranathan, and S. Ganguli (2015). Deep unsupervised
learning using nonequilibrium thermodynamics. In International conference on machine learning ,
pp. 2256–2265. PMLR.
Song, J., C. Meng, and S. Ermon (2020). Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 .
Srinivas, N., A. Krause, S. M. Kakade, and M. Seeger (2009). Gaussian process optimization in the
bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995 .
Stark, H., B. Jing, C. Wang, G. Corso, B. Berger, R. Barzilay, and T. Jaakkola (2024). Dirichlet flow
matching with applications to dna sequence design. arXiv preprint arXiv:2402.05841 .
Touvron, H., L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra,
P. Bhargava, S. Bhosale, et al. (2023). Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 .
Trabucco, B., A. Kumar, X. Geng, and S. Levine (2021). Conservative objective models for effective
offline model-based optimization. In International Conference on Machine Learning , pp. 10358–
10368. PMLR.
Uehara, M. and W. Sun (2021). Pessimistic model-based offline reinforcement learning under partial
coverage. arXiv preprint arXiv:2107.06226 .
Uehara, M., Y . Zhao, K. Black, E. Hajiramezanali, G. Scalia, N. L. Diamant, A. M. Tseng, T. Bian-
calani, and S. Levine (2024). Fine-tuning of continuous-time diffusion models as entropy-
regularized control. arXiv preprint arXiv:2402.15194 .
Valko, M., N. Korda, R. Munos, I. Flaounas, and N. Cristianini (2013). Finite-time analysis of
kernelised contextual bandits. arXiv preprint arXiv:1309.6869 .
Vargas, F., W. Grathwohl, and A. Doucet (2023). Denoising diffusion samplers. arXiv preprint
arXiv:2302.13834 .
Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint , V olume 48.
Cambridge university press.
Wallace, B., M. Dang, R. Rafailov, L. Zhou, A. Lou, S. Purushwalkam, S. Ermon, C. Xiong, S. Joty,
and N. Naik (2023). Diffusion model alignment using direct preference optimization. arXiv
preprint arXiv:2311.12908 .
Wu, K. E., K. K. Yang, R. van den Berg, S. Alamdari, J. Y . Zou, A. X. Lu, and A. P. Amini (2024).
Protein structure generation via folding diffusion. Nature Communications 15 (1), 1059.
13Wu, X., K. Sun, F. Zhu, R. Zhao, and H. Li (2023). Better aligning text-to-image models with human
preference. arXiv preprint arXiv:2303.14420 .
Wu, Y ., G. Tucker, and O. Nachum (2019). Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361 .
Xie, T., C.-A. Cheng, N. Jiang, P. Mineiro, and A. Agarwal (2021). Bellman-consistent pessimism
for offline reinforcement learning. Advances in neural information processing systems 34 .
Xiong, W., H. Dong, C. Ye, Z. Wang, H. Zhong, H. Ji, N. Jiang, and T. Zhang (2023). Iterative
preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint.
InICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models .
Xu, J., X. Liu, Y . Wu, Y . Tong, Q. Li, M. Ding, J. Tang, and Y . Dong (2023). Imagereward: Learning
and evaluating human preferences for text-to-image generation. arXiv preprint arXiv:2304.05977 .
Yang, K., J. Tao, J. Lyu, C. Ge, J. Chen, Q. Li, W. Shen, X. Zhu, and X. Li (2023). Using human feed-
back to fine-tune diffusion models without any reward model. arXiv preprint arXiv:2311.13231 .
Yu, T., G. Thomas, L. Yu, S. Ermon, J. Y . Zou, S. Levine, C. Finn, and T. Ma (2020). Mopo:
Model-based offline policy optimization. Advances in Neural Information Processing Systems 33 ,
14129–14142.
Yuan, H., K. Huang, C. Ni, M. Chen, and M. Wang (2023). Reward-directed conditional diffusion:
Provable distribution estimation and reward improvement. arXiv preprint arXiv:2307.07055 .
Zhan, W., M. Uehara, N. Kallus, J. D. Lee, and W. Sun (2023). Provable offline preference-based
reinforcement learning. In The Twelfth International Conference on Learning Representations .
Zhang, Q. and Y . Chen (2021). Path integral sampler: a stochastic control approach for sampling.
arXiv preprint arXiv:2111.15141 .
Zhang, T., T. Ren, M. Yang, J. Gonzalez, D. Schuurmans, and B. Dai (2022). Making linear mdps
practical via contrastive representation learning. In International Conference on Machine Learning ,
pp. 26447–26466. PMLR.
14NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In abstract and intro, we explained our algorithm in details.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We summarize it in Section 8.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We provide theoretical results in Section 5, and . Due to the page limit, we add
more detailed results in the Appendix as well.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We have added details as much as possible.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes] .
Justification: We have added codes.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We have added the details in the main text and sup material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We have added details as much as possible. In Figure 2, we show the distribu-
tion. In Table 1, we show the confidence interval. In Figure 3(b), we also plot the std of ˆr
for generated samples.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We have added details as much as possible.
159.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We discussed potential positive societal impact in the introduction. Regarding
negative impact, we have regonized unintended uses is possible.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [No]
Justification: Due to the page limit, we haven’t included it. However, we recognize diffusion
models could be misused to make fake images. We aim to address this problem in the future.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [No]
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA] .
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA] .
16A Additional Related Works
In this section, we summarize additional related works.
Conservative approaches in offline RL/offline contextual bandits. Conservative approaches
have been explored in offline RL and contextual bandits. Firstly, in both model-free and model-based
RL, one prevalent method involves incorporating an additional penalty on top of the reward functions
(Yu et al., 2020; Chang et al., 2021). Secondly, in model-based RL, a common strategy is to train
transition dynamics in a conservative manner (Kidambi et al., 2020; Rigter et al., 2022; Uehara
and Sun, 2021). Thirdly, in model-free RL, typical approaches include conservative learning of
q-functions (Kumar et al., 2020; Xie et al., 2021) or the inclusion of KL penalties against behavioral
policies (Wu et al., 2019; Fakoor et al., 2021).
However, these works are not designed to incorporate a diffusion model, unlike our approach. Hence,
it remains unclear how their works can generate designs that remain within intricate valid design
space, such as high-quality images using stable diffusion.
Design with generative models. Many works are focusing on design problems with generative
models. However, these works are typically limited to the usage of V AEs. (Notin et al., 2021; G ´omez-
Bombarelli et al., 2018) Our work is still significantly different because we focus on generative
models.
Fine-tuning in LLMs from human feedbacks. A closely related area of research involves fine-
tuning LLMs through the optimization of reward functions using human feedback (Touvron et al.,
2023; Ouyang et al., 2022). Especially, following works such as Zhan et al. (2023), from a theoretical
viewpoint, Xiong et al. (2023) explores the effectiveness of pessimism in offline scenarios and its
theoretical aspect. However, our theoretical findings are more specifically tailored to diffusion models.
Indeed, our main result in Theorem 1 is novel, and our algorithm differs significantly, as fine-tuning
methods in offline settings in the literature on LLMs typically rely on policy gradient or PPO, whereas
we use more direct backpropagation approaches. Furthermore, the meaning of step size is different as
well. Hence, in (Xiong et al., 2023), they do not use soft entropy regularized MDPs.
Typically, in the above works, human feedback is considered to be given in the form of preferences.
Similarly, in the context of diffusion models, Yang et al. (2023); Wallace et al. (2023) discusses
fine-tuning of diffusion models using preference-based feedback. However, these works focus on
online settings but not offline settings.
Sampling from unnormalized distributions. In our approach, we use an RL method to sample
from our target distribution that is proportional to exp((ˆ r−ˆg(x))ppre(x). While MCMC has
traditionally been prevalent in sampling from unnormalized Boltzmann distributions exp(r(x)),
recent discussions (Zhang and Chen, 2021; Vargas et al., 2023), have explored RL approaches similar
to our approach. However, their focus differs from ours, as they do not address the fine-tuning of
diffusion models (i.e., no ppre) or sample efficiency in offline settings.
Another relevant literature discusses sampling from unnormalized Boltzmann distributions when
pre-trained diffusion models are available (Kong et al., 2024). However, their algorithm closely
resembles classifier-based guidance (Dhariwal and Nichol, 2021), rather than a fine-tuning algorithm.
Additionally, they do not examine conservatism in an offline setting.
B Direct Back Propagation
Our planning algorithm has been summarized in Algorithm 2. Here, we parametrize each policy by
neural networks.
For the sake of explanation, we also add typical cases where the domain is Euclidean in Algorithm 3.
17Algorithm 2 Direct Back Propagation (General case)
1:Require : Set a diffusion-model p(·|xt−1;θ), pre-trained model {ppre(·|xt−1)}1
t=T+1, batch size
m, a parameter α∈R+.
2:Initialize :θ1=θpre
3:fors∈[1,···, S]do
4: Setθ=θs.
5: Collect msamples {x(i)
t(θ)}0
t=T+1from a current diffusion model (i.e., generating by sequen-
tially running polices {pt(·|xt;θ)}1
t=T+1from t=T+ 1tot= 1)
6: Update θstoθs+1by adding the gradient of the following loss L(θ)with respect to θatθs:
L(θ) =1
mmX
i=1"
ˆr(x(i)
0(θ))−ˆg(x(i)
0(θ))−α1X
t=T+1KL(pt(·|xt;θ)∥ppre(·|xt))#
.(10)
7:end for
8:Output : Policy {pt(·|·;θS)}1
t=T+1
Algorithm 3 Direct Back Propagation (in Euclidean space)
1:Require : Set a diffusion-model {N(ρ(t, xt;θ), σ2
t);θ∈Θ}1
t=T+1, pre-trained model
{N(ρ(t, xt;θpre), σ2
t)}1
t=T+1, batch size m, a parameter α∈R+.
2:Initialize :θ1=θpre
3:fors∈[1,···, S]do
4: Setθ=θs.
5: Collect msamples {x(i)
t(θ)}0
t=T+1from a current diffusion model (i.e., generating by sequen-
tially running polices {N(ρ(t, xt;θ), σ2
t)}1
t=T+1from t=T+ 1tot= 1)
6: Update θstoθs+1by adding the gradient of the following loss L(θ)with respect to θatθs:
L(θ) =1
mmX
i=1"
ˆr(x(i)
0(θ))−ˆg(x(i)
0(θ))−α1X
t=T+1∥ρ(x(i)
t(θ), t;θ)−ρ(x(i)
t(θ), t;θpre)∥2
2σ2(t)#
.
(11)
7:end for
8:Output : Policy {pt(·|·;θS)}1
t=T+1
C All Proofs
C.1 Proof of Theorem 1
Here, we actually prove a stronger statement.
Theorem 3 (Marginal and Posterior distributions) .Letˆpt(xt)andˆpb
t(xt|xt−1)be marginal distri-
butions at tor posterior distributions of xtgiven xt−1, respectively, induced by optimal policies
{ˆpt}1
T+1. Then,
ˆpt(xt) = exp( vt(xt)/α)ˆppre
t(xt)/C, ˆpb
t(xt|xt−1) = ˆppre
t(xt|xt−1).
Proof. To simplify the notation, we let f(x) = ˆr(x)−ˆg(x). As a first step, by using induction, we
aim to obtain an analytical form of the optimal policy {ˆpt(·|xt−1)}.
First, we define the soft-optimal value function as follows:
vt−1(xt−1) =E{ˆpt}"
f(x)−α1X
k=t−1KL(ˆpk(·|xk)∥ppre
k(·|xk))|xt−1#
.
Then, by induction, we have
ˆpt(xt−1|xt) = argmax
pt∈∆(X)E{ˆpt}[vt−1(xt−1)−αKL(pt(·|xt)∥ppre
t(·|xt))|xt].
18With simple algebra, we obtain
ˆpt(xt−1|xt)∝expvt−1(xt−1)
α
ppre
t(xt−1|xt). (12)
Here, noting
vt(xt) = max
pt∈∆(X)E{ˆpt}[vt−1(xt−1)−αKL(pt(·|xt)∥ppre
t(·|xt))|xt],
we get the soft Bellman equation:
expvt(xt)
α
=Z
expvt−1(xt−1)
α
ppre
t(xt−1|xt)dxt−1. (13)
Therefore, by plugging (13) into (12), we actually have
ˆpt(xt−1|xt) =exp
vt−1(xt−1)
α
ppre
t(xt−1|xt)
exp
vt(xt)
α . (14)
Finally, with the above preparation, we calculate the marginal distribution:
ˆpt(xt) :=Z(tY
s=T+1ˆps(xs−1|xs))
dxt+1:T+1.
Now, by using induction, we aim to prove
ˆpt(xt) = expvt(xt)
α
ppre
t(xt).
Indeed, when t=T+ 1, from (14), this hold as follows:
ˆpT+1(xT+1) =1
CexpvT(xT+1)
α
ppre
T+1(xT+1).
Now, suppose the above holds at t. Then, this also holds for t−1:
ˆpt−1(xt−1) =Z
ˆpt(xt−1|xt)ˆpt(xt)dxt
=Z
expvt−1(xt−1)
α
{ppre
t(xt−1|xt)}ppre
t(xt)dxt (Use Equation 14)
= expvt−1(xt−1)
α
ppre
t−1(xt−1).
By invoking the above when t= 0, the statement is concluded.
C.2 Proof of Theorem 2
In the following, we condition on the event where
∀x∈ Xpre;|r(x)−ˆr(x)| ≤g(x).
holds.
First, we define
ˆJα(π) :=Ex∼π[ˆr(x)−ˆg(x)]−αKL(π∥pun), J α(π) :=Ex∼π[r(x)]−αKL(π∥pun).
We note that, ˆπαmaximizes ˆJα(π). Therefore, we have
Jα(π)−Jα(ˆπα) =Jα(π)−ˆJα(π) +ˆJα(π)−ˆJα(ˆπα) +ˆJα(ˆπα)−Jα(ˆπα)
≤Jα(π)−ˆJα(π) +ˆJα(ˆπα)−Jα(ˆπα) (Definition of ˆπα)
(i)
≤Jα(π)−ˆJα(π). (Pessimism)
19Here, in the step (i), we use
∀x∈ Xpre;|r(x)−ˆr(x)| ≤g(x).
Then,
Jα(π)−Jα(ˆπα)≤Jα(π)−ˆJα(π)≤2Ex∼π[ˆg(x)]
≤2p
Ex∼π[{ˆg(x)}2] (Jensen’s inequality)
≤2sπ
ppre
∞Ex∼ppre[{ˆg(x)}2]. (Importance sampling)
Hence, the statement is concluded.
D Theoretical Guarantees with Gaussian Processes
We explain the theoretical guarantee when using Gaussian processes. In this section, we suppose the
model is well-specified.
Assumption 2. y=r(x) +ϵwhere ϵ∼ N(0, I)where rbelongs to an RKHS in Hk.
D.1 Preparation
We introduce the notation to state our guarantee. For details, see Srinivas et al. (2009, Appendix B),
Uehara and Sun (2021, Chapter 6.2), Chang et al. (2021, Chapter C.3).
For simplicity, we first suppose the following.
Assumption 3. The space Xis compact, and ∀x∈ X;k(x, x)≤1.
We introduce the following definition. Regarding details, refer to Wainwright (2019, Chapter 12).
Definition 1. LetHkbe the RKHS with the kernel k(·,·). We denote the associated norm and inner
product by ∥ · ∥k,⟨·,⟩k, respectively. We introduce analogous notations for
ˆk(x, x′) =k(x, x′)−k(x)⊤{K+λI}−1k(x′).
and denote the norm and inner product by ∥ · ∥ ˆk,⟨·,⟩ˆk.
Note as explained in Srinivas et al. (2009, Appendix B) and Chang et al. (2021, Chapter C.3), actually,
we have Hk=Hˆk.
In the following, We use the feature mapping associated with an RKHS Hk. To define this, from
Mercer’s theorem, note we can ensure the existence of orthonormal eigenfunctions and eigenvalues
{ψi, µi}such that
k(·,⋄) =∞X
i=1µiψi(·)ψi(⋄),R
ψi(x)ψi(x)psp(x)dx= 1R
ψi(x)ψj(x)psp(x)dx= 0(i̸=j).
Then, we define the feature mapping:
Definition 2 (Feature mapping) .
ϕ(x) := [√µ1ψ1(x),√µ1ψ1(x),···]⊤.
Assuming eigenvalues are in non-increasing order, we can also define the effective dimension
following Srinivas et al. (2009, Appendix B), Uehara and Sun (2021, Chapter 6.2), Chang et al. (2021,
Chapter C.3):
Definition 3 (Effective dimension) .
d′= min
j

j∈N:j≥n∞X
k=jµk

.
The effective dimension is commonly used and calculated in many kernels (Valko et al., 2013). In
finite-dimensional linear kernels {x7→a⊤ϕ(x) :a∈Rd}such that k(x, z) =ϕ⊤(x)ϕ(z), letting
d′:= rank( Ex∼psp[ϕ(x)ϕ(x)]), we have
d′≤˜d≤d
because there exists µ˜d+1= 0, µ˜d+2= 0,···.
20D.2 Calibrated oracle
Using a result in Srinivas et al. (2009). we show
ˆr(x)−r(x)≤C(δ)q
ˆk(x, x)
where
C(δ) =c1q
1 + log3(n/δ)In,In= log(det( I+K)).
Then, with probability 1−δ, we have
ˆr(x)−r(x) =⟨ˆr(·)−r(·),ˆk(·, x)⟩ˆk(Reproducing property)
≤ ∥ˆr(·)−r(·)∥ˆk× ∥ˆk(·, x)∥ˆk(CS inequality)
≤ ∥ˆr(·)−r(·)∥ˆkq
ˆk(x, x)
≤C(δ)q
ˆk(x, x). (Use Theorem 6 in Srinivas et al. (2009))
D.3 Regret Guarantee (Proof of Corollary 1)
Recall from the proof of Theorem 3,
Jα(π)−Jα(ˆπα)≤2Ex∼π[ˆg(x)] = 2 C(δ)Ex∼π[q
ˆk(x, x)].
Now, first, to upper-bound Ex∼π[q
ˆk(x, x)], we borrow Theorem 25 in Chang et al. (2021), which
shows
Ex∼π[q
ˆk(x, x)]≤c1s
˜Cπd′{d′+ log( c2/δ)}
n.
where
˜Cπ:= sup
κ:∥κ∥2=1κ⊤Ex∼π[ϕ(x)ϕ⊤(x)]κ
κ⊤Ex∼psp[ϕ(x)ϕ⊤(x)]κ
Next, in order to upper-bound C(δ), we borrow Theorem 24 in Chang et al. (2021), which shows
In≤c1{d′+ log( c2/δ)}d′log(1 + n).
The statement in Corollary 1 is immediately concluded.
E Additional Details of Experiments
E.1 DNA/RNA sequences
In this subsection, we add the details of experiments in Section 7.
E.1.1 Architecture of Neural Networks
Diffusion models. Regarding diffusion models for sequences, we adopt the architecture and
algorithm tailored to biological sequences over the simple space (Avdeyev et al., 2023). Its architecture
is described in Table 1.
Oracles. We use the architecture in Avsec et al. (2021), which is a state-of-the-art model in sequence
modeling. We just change the last layer so that it is tailored to a regression problem as in Lal et al.
(2024).
E.1.2 Hyperparameters
In all experiments, we use A100 GPUs. The important hyperparameters are summarized in the
following table (Table 2 on page 22).
21Table 1: Basic architecture of networks for diffusion models
Layer Input dimension Output dimension Explanation
1 200×4 256 Linear + ReLU
2 256 256 Conv1D + ReLU
··· ··· ··· ···
10 256 256 Conv1D + ReLU
11 256 256 ReLU
Table 2: Important hyperparameters for fine-tuning. For all methods, we use Adam as an optimizer.
Method Type Value
BRAIDBatch size 128
KL parameter α 0.001
LCB parameter (bonus) c0.1(UTRs), 0.1(Enhancers)
Number of bootstrap heads 3
Sampling to neural SDE Euler Maruyama
Step size (fine-tuning) 50
GuidanceGuidance level 10
Guidance target Top5%
Hyperparameter selection. The process of selecting hyperparameters in offline RL is known to be
a challenging task (Rigter et al., 2022; Paine et al., 2020) in general. A common practice in existing
literature is determining crucial hyperparameters with a limited number of online interactions. In
our case, the key hyperparameters include the strength of the LCB parameter (utilized online in
BRAID-Bonus ) and the termination criteria during training (applied to all fine-tuning algorithms
such as STRL ). To ensure a fair comparison, we operate within a framework where we can utilize
120×20online samples. This implies that, for instance, in STRL andBRAID-Bonus , we conduct
an online evaluation using 120samples for 20pre-defined epochs. However, in BRAID-Bonus , given
the additional hyperparameters to be tested (strengths of the bonus term 0.01,0.1,1.0), we use 40
samples for each 20pre-defined epoch.
E.1.3 Ablation Studies
We performed ablation studies by varying the strength of the bonus parameter Cin Figure 5. We
chose the one with the best performance in the main text (See the previous section to see the validity
of this procedure).
Offline Pretrained DDOM Guidance STRL BRAID-Boot BRAID-Bonus BRAID-Bonus2 BRAID-Bonus3
Method1.5
1.0
0.5
0.00.51.01.5MRL
Figure 4: UTRs
E.2 Images
In this section, we describe the additional experiment regarding image generation in Section 7.2.
22Offline Pretrained DDOM Guidance STRL BRAID-Boot BRAID-Bonus BRAID-Bonus2 BRAID-Bonus3
Method0246810HepG2
Figure 5: Enhancers
E.2.1 Description of Offline Data
We utilize images from the A V A dataset (Murray et al., 2012) as samples x, containing over 250,000
image aesthetic evaluations. Rather than using the raw scores directly from the dataset, we derive the
labels yby utilizing the pre-trained LAION Aesthetic Predictor V2 Schuhmann (2022) built on top
of CLIP embeddings. This choice is made because we employ the LAION Aesthetic Predictor as the
ground truth scorer to assess both our methods and baselines. In total, we have curated an offline
dataset comprising 255490 image-score pairs: {x(i), y(i)}.
E.2.2 Architecture of Neural Networks
We adopt the standard StableDiffusion v1.5 (Rombach et al., 2022) as the pre-trained model with the
DDIM scheduler (Song et al., 2020). Note this pre-trained model is a conditional diffusion model.
Using the offline dataset {(x(i), y(i))}, we train the reward oracle ˆrby an MLP on the top of CLIP
embeddings. The detailed MLP structure is listed in Table 3. Note that, compared to the true LAION
Aesthetic Score Predictor V2 (Schuhmann, 2022), our reward oracle proxy has a simpler structure
with fewer hidden dimensions and fewer layers. We aim to impose the hardness of fitting the true
reward model, which is typically infeasible in many applications. In such scenarios, a pessimistic
reward oracle is especially beneficial to mitigate overoptimization.
Table 3: Architecture of reward oracle for aesthetic scores
Layer Input dimension Output dimension Explanation
1 768 256 Linear + ReLU
2 256 64 Linear + ReLU
3 64 16 Linear + ReLU
4 16 1 Linear
E.2.3 LLM-aided evaluation
As stated in the main text, the original LAION Aesthetic Predictor V2 (Schuhmann, 2022) tends to
assign higher scores even to images that disregard the original prompts, which is undesirable. To
effectively identify such problematic scenarios, we employ a pre-trained multi-modal language model
to verify whether the original prompt is present in the image or not. For each generated image, we
provide the following prompt to LLaV A (Liu et al., 2024) along with the image:
<image>
USER: Does this image include {prompt}? Answer with Yes or No
ASSISTANT:
We evaluated its accuracy and precision with human evaluators by generating images using Stable
Diffusion with animal prompts (such as dog or cat). The achieved F1 score was 1.0.
23E.2.4 Hyperparameters
In all image experiments, we use four A100 GPUs for fine-tuning StableDiffusion v1.5 (Rombach
et al., 2022). The set of training hyperparameters is listed in Table 4.
Table 4: Important hyperparameters for fine-tuning Aesthetic Scores.
Method Parameters Values
BRAIDGuidance weight 7.5
DDIM Steps 50
Batch size 128
KL parameter α 1
LCB bonus parameter C 0.001
Number of bootstrap heads 4
STRLGuidance weight 7.5
DDIM Steps 50
Batch size 128
KL parameter α 1
Offline guidanceGuidance level 100
Guidance target 10
OptimizationOptimizer AdamW
Learning rate 0.001
(ϵ1, ϵ2) (0.9,0.999)
Weight decay 0.1
Clip grad norm 5
Truncated back-propagation step KK∼Uniform (0,50)
E.2.5 Effectiveness of LLaV A-aided evaluation
In our evaluation, we utilize a large multi-modal model like LLaV A. As previously mentioned, relying
solely on the raw score fails to detect scenarios where generated images ignore the given prompts.
Table 5 illustrates the outcomes of LLaV A-assisted evaluations for the pre-trained model and four
checkpoints of the STRL baseline. It is evident that LLaV A successfully identifies all samples
generated by the pre-trained model and the first two checkpoints. However, despite seemingly
high-reward samples, many samples from checkpoints 3 and 4 do not align correctly with their
prompts, resulting in a reduced mean reward. Figure 6 showcases five failure examples from each of
checkpoints 3 and 4. Thus, we can validate our quantitative evaluation of reward overoptimization.
Table 5: Statistics of LLaV A-adjusted scores.
method mean min max invalid/total samples
pre-trained model 5.789 4 .666 6 .990 0 /400
STRL-ckpt-1 6.228 4 .769 7 .193 0 /400
STRL-ckpt-2 6.870 5 .892 7 .602 0 /400
STRL-ckpt-3 6.484 0 .0 7 .944 50 /400
STRL-ckpt-4 0.200 0 .0 7 .620 389 /400
E.2.6 Ablation Studies
Ablation on BRAID-Bonus hyperparameter We provide the boxplots for different Bonus hyper-
parameters in Figure 7, indicating our method’s robustness to hyperparameter tuning.
Additional qualitative results More image visualizations for BRAID and baselines can be found
in Figure 8.
24/uni00000053/uni00000048/uni00000044/uni00000046/uni00000052/uni00000046/uni0000004e/uni00000003/uni0000005f/uni00000003/uni0000001a/uni00000011/uni00000015/uni00000015
 /uni00000055/uni00000044/uni00000045/uni00000045/uni0000004c/uni00000057/uni00000003/uni0000005f/uni00000003/uni0000001a/uni00000011/uni00000013/uni00000015
 /uni0000004b/uni00000052/uni00000055/uni00000056/uni00000048/uni00000003/uni0000005f/uni00000003/uni00000019/uni00000011/uni0000001a/uni00000015
 /uni00000050/uni00000052/uni00000051/uni0000004e/uni00000048/uni0000005c/uni00000003/uni0000005f/uni00000003/uni0000001a/uni00000011/uni0000001b/uni0000001c
 /uni00000045/uni00000058/uni00000057/uni00000057/uni00000048/uni00000055/uni00000049/uni0000004f/uni0000005c/uni00000003/uni0000005f/uni00000003/uni0000001a/uni00000011/uni00000017/uni0000001b
/uni00000055/uni00000044/uni00000045/uni00000045/uni0000004c/uni00000057/uni00000003/uni0000005f/uni00000003/uni0000001a/uni00000011/uni0000001b/uni00000013
 /uni00000050/uni00000052/uni00000051/uni0000004e/uni00000048/uni0000005c/uni00000003/uni0000005f/uni00000003/uni0000001b/uni00000011/uni00000013/uni00000017
 /uni00000053/uni00000044/uni00000051/uni00000047/uni00000044/uni00000003/uni0000005f/uni00000003/uni0000001a/uni00000011/uni0000001c/uni00000017
 /uni00000047/uni00000052/uni0000004a/uni00000003/uni0000005f/uni00000003/uni0000001a/uni00000011/uni0000001a/uni00000013
 /uni0000004b/uni00000052/uni00000055/uni00000056/uni00000048/uni00000003/uni0000005f/uni00000003/uni0000001b/uni00000011/uni00000013/uni00000019Figure 6: Image-prompt alignment failures detected by LLaV A.
Offline Pretrained Guidance STRL Bonus-5e-4 Bonus-1e-3 Bonus-5e-3 Bonus-1e-2 Bonus-5e-2
Method23456789Reward
Figure 7: Ablation study for BRAID-Bonus . By adjusting the pessimism strength C1while keeping
λ= 0.1, we show that BRAID-Bonus outperforms all baselines for a wide range of hyperparameter
selection.
/uni00000018/uni00000011/uni0000001b/uni00000016/uni00000032/uni00000049/uni00000049/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000003
/uni00000003/uni0000002a/uni00000058/uni0000004c/uni00000047/uni00000044/uni00000051/uni00000046/uni00000048/uni00000046/uni00000044/uni00000057
/uni00000019/uni00000011/uni00000014/uni00000017/uni00000047/uni00000052/uni0000004a
/uni00000018/uni00000011/uni00000017/uni00000018/uni0000004b/uni00000052/uni00000055/uni00000056/uni00000048
/uni00000019/uni00000011/uni00000017/uni00000017/uni00000050/uni00000052/uni00000051/uni0000004e/uni00000048/uni0000005c
/uni00000019/uni00000011/uni00000016/uni0000001b/uni00000055/uni00000044/uni00000045/uni00000045/uni0000004c/uni00000057
/uni00000019/uni00000011/uni00000013/uni00000014/uni00000045/uni00000058/uni00000057/uni00000057/uni00000048/uni00000055/uni00000049/uni0000004f/uni0000005c
/uni00000018/uni00000011/uni00000017/uni00000019/uni00000053/uni00000048/uni00000044/uni00000046/uni00000052/uni00000046/uni0000004e
/uni00000019/uni00000011/uni00000015/uni0000001b/uni00000053/uni00000044/uni00000051/uni00000047/uni00000044
/uni00000019/uni00000011/uni0000001b/uni00000014/uni00000036/uni00000037/uni00000035/uni0000002f
/uni0000001a/uni00000011/uni00000013/uni00000019
 /uni0000001a/uni00000011/uni00000015/uni00000013
 /uni00000019/uni00000011/uni0000001b/uni00000016
 /uni00000019/uni00000011/uni00000018/uni0000001c
 /uni00000019/uni00000011/uni00000017/uni00000016
 /uni00000019/uni00000011/uni00000019/uni0000001a
 /uni00000019/uni00000011/uni00000019/uni0000001b
/uni0000001a/uni00000011/uni0000001b/uni00000015/uni00000025/uni00000035/uni00000024/uni0000002c/uni00000027/uni00000010/uni00000025/uni00000052/uni00000052/uni00000057
/uni0000001a/uni00000011/uni00000017/uni00000018
 /uni0000001a/uni00000011/uni00000016/uni00000016
 /uni0000001a/uni00000011/uni00000016/uni0000001c
 /uni0000001a/uni00000011/uni00000016/uni00000015
 /uni0000001a/uni00000011/uni0000001a/uni0000001c
 /uni0000001a/uni00000011/uni0000001b/uni00000013
 /uni0000001a/uni00000011/uni0000001a/uni00000017
/uni0000001a/uni00000011/uni00000017/uni0000001b/uni00000025/uni00000035/uni00000024/uni0000002c/uni00000027/uni00000010/uni00000025/uni00000052/uni00000051/uni00000058/uni00000056
/uni0000001a/uni00000011/uni0000001b/uni00000017
 /uni0000001a/uni00000011/uni00000017/uni0000001c
 /uni0000001a/uni00000011/uni00000019/uni00000017
 /uni0000001a/uni00000011/uni00000018/uni00000017
 /uni0000001a/uni00000011/uni00000014/uni00000018
 /uni0000001a/uni00000011/uni00000017/uni00000018
 /uni0000001a/uni00000011/uni00000018/uni00000013
Figure 8: More images generated by BRAID and baselines. All algorithms choose the best checkpoint
according to our LLaV A-aided evaluation. The visualization demonstrates the benefits of introducing
pessimistic terms that can help to achieve high scores while mitigating reward overoptimization.
25