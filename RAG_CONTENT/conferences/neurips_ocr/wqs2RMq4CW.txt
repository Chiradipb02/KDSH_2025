Corruption-Robust Linear Bandits: Minimax
Optimality and Gap-Dependent Misspecification
Haolin Liu∗
University of Virginia
srs8rh@virginia.eduArtin Tajdini
University of Washington
artin@cs.washington.edu
Andrew Wagenmaker
University of California, Berkeley
ajwagen@berkeley.eduChen-Yu Wei
University of Virginia
chenyu.wei@virginia.edu
Abstract
In linear bandits, how can a learner effectively learn when facing corrupted re-
wards? While significant work has explored this question, a holistic understanding
across different adversarial models and corruption measures is lacking, as is a full
characterization of the minimax regret bounds. In this work, we compare two types
of corruptions commonly considered: strong corruption , where the corruption level
depends on the learner’s chosen action, and weak corruption , where the corrup-
tion level does not depend on the learner’s chosen action. We provide a unified
framework to analyze these corruptions. For stochastic linear bandits, we fully
characterize the gap between the minimax regret under strong and weak corruptions.
We also initiate the study of corrupted adversarial linear bandits, obtaining upper
and lower bounds with matching dependencies on the corruption level.
Next, we reveal a connection between corruption-robust learning and learning with
gap-dependent misspecification —a setting first studied by Liu et al. (2023a), where
the misspecification level of an action or policy is proportional to its suboptimality.
We present a general reduction that enables any corruption-robust algorithm to
handle gap-dependent misspecification. This allows us to recover the results of Liu
et al. (2023a) in a black-box manner and significantly generalize them to settings
like linear MDPs, yielding the first results for gap-dependent misspecification in
reinforcement learning. However, this general reduction does not attain the optimal
rate for gap-dependent misspecification. Motivated by this, we develop a special-
ized algorithm that achieves optimal bounds for gap-dependent misspecification in
linear bandits, thus answering an open question posed by Liu et al. (2023a).
1 Introduction
The real world is rarely truly stochastic—in practice, our observations are often corrupted—and
furthermore, rarely are the modeling assumption typically made in theory—that the true data-
generating process lives in our model class—met in reality. Therefore, robustly handling these
deviations from idealized assumptions is crucial. These challenges are particularly pronounced
in interactive decision-making settings, where deviations from idealized assumptions could lead
an algorithm to take unsafe or severely suboptimal actions. In this work, we seek to address
these challenges, and develop a unified understanding for robust learning in corruption-robust and
misspecified settings.
∗Authors are listed in alphabetical order by last name.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).We first consider the corruption-robust learning setting. Robust learning in the presence of corruptions
requires designing algorithms whose guarantee have a tight scaling in the corruption level. That is,
although some amount of suboptimality is inevitable if our observations are corrupted, we would
hope to obtain the minimum amount of suboptimality possible at a given corruption level. While
much work has been done on learning with corrupted observations, existing work has failed to yield a
tight characterization of this scaling in the corruption level, even in simple settings such as linear
bandits. We address this shortcoming, and develop an algorithm which achieves the optimal scaling
in the corruption level, and further extend this to a novel corrupted adversarial linear bandit setting,
where in addition to corrupted observations, the rewards themselves may be adversarially chosen
from round to round. We obtain the first provably efficient bounds in this setting.
Model misspecification, another extensively studied problem in the literature, can be thought of as a
form of corruption, where the corruption level is the amount of misspecification between the “closest”
model in the model class and the true environment. Standard discussions on misspecification usually
assume that the misspecification for every action has a uniform upper bound, and the final regret
guarantee scales linearly with the amount of misspecification. The work of Liu et al. (2023a) initiated
the study on the gap-dependent misspecification setting, where the misspecification level for a given
action scales with the suboptimality of that action. They demonstrated that the linear scaling in
regret is not necessary in this case. We revisit this problem, and show a general reduction from the
gap-dependent misspecified setting to the corruption setting. We utilize this reduction to show that
settings previously not known to be learnable—for example, linear MDPs with policy gap-dependent
misspecification—are in fact efficiently learnable with existing corruption robust algorithms.
Together, our results present a unified picture of optimally learning in the presence of observation
corruption, and (certain types of) model misspecification. We summarize our contributions as follows
(see Section 2 and Section 3 for formal definitions of the mentioned quantities):
1.In Section 4, we develop a stochastic linear bandit algorithm with eO(d√
T+ min {dC,√
dC∞})
regret, where dis the feature dimension, Tis the number of rounds, Cis the strong corruption
measure, and C∞is the weak corruption measure. These bounds are unimprovable.
2.In Section 5, we initiate the study of adversarial linear bandits with corruptions. We obtain
eO(d√
T+√
dC∞)andeO(√
d3T+dC)regret for weak and strong corruptions, respectively.
3.We prove a general reduction that efficiently handles gap-dependent misspecification with
corruption-robust algorithms. We apply our reduction to show that linear MDPs with gap-
dependent misspecification are efficiently learnable (Section 6).
4.Finally, while the reduction in item 3 is general, it is unable to obtain the tightest possible rate
for gap-dependent misspecification. We thus develop a specialized algorithm which, in the linear
bandit setting, obtains the optimal rate. This resolves the open problem of Liu et al. (2023a).
In Section 2 we present our problem setting, and in Section 3, compare the corruption notions in
previous and our work. More related works are discussed in Appendix A. In Section 4–Section 6, we
present our main results as outlined above.
2 Problem Setting and Preliminaries
We consider the corrupted linear bandit problem. The learner interacts with the environment for T
rounds. The learner is given an action set A ⊂Rd. At the beginning of round t, the environment
determines a reward vector θt∈Rdand a corruption function ϵt(·) :A → [−1,1], which are
both hidden from the learner. The learner then selects an action at∈ A . Then a reward value
rt=a⊤
tθt+ϵt(at) +ζtis revealed to the learner, for some zero-mean noise ζt∈[−1,1]2. We
assume that ∥a∥2≤1,∥θt∥2≤√
d, anda⊤θt∈[−1,1]for any a∈ A and any t= 1,2, . . . , T . We
define ϵt= max a∈A|ϵt(a)|.
In the stochastic setting, the environment is restricted to choose θt=θ⋆for all t, while in the
adversarial setting, θtcan arbitrarily depend on the history up to round t−1. The regret of the learner
is defined as
RegT= max u∈APT
t=1u⊤θt−PT
t=1a⊤
tθt.
2We assume both the corruption function and the noise are bounded for simplicity. All our results can be
generalized to the case where the corruption is unbounded and the noise is sub-Gaussian. See the “additional
note on corruption” in Page 5 of Wei et al. (2022) for reducing this case to the bounded case.
2Note that although the non-stationarity of θtin the adversarial setting captures a certain degree of
corruption, this form of corruption is limited to a linear form a⊤(θt−θ⋆), which is not as general as
ϵt(a)that could be an arbitrarily function. Therefore, the corrupted linear bandit problem cannot be
reduced to an adversarial linear bandit problem.
Notation. We denote [n] ={1,2, . . . , n }. Let ∆(A)be the set of distribution over A. For any
p∈∆(A), define the lifted covariance matrix dCov(p) =Ea∼p
aa⊤a
a⊤1
∈R(d+1)×(d+1). For
A, B∈Rd×d, define ⟨A, B⟩=Tr(AB⊤).Et[·]is the expectation conditioned on history up to t−1.
G-Optimal Design. A G-optimal design over Ais a distribution ρ∈∆(A)such that ∥a∥2
G−1≤d
for all a∈ A, where G=P
a∈Aρ(a)aa⊤. Note that such a distribution is guaranteed to exist, and
can be efficiently computed (Pukelsheim, 2006; Lattimore and Szepesvári, 2020).
3Two Equivalent Views: On Adversary Adaptivity and Corruption Measure
Previous works have studied corruption with various assumptions on the adaptivity of the adversary
and different measures for the corruption level. In this work, we consider both the strong andweak
guarantees, which can cover different notions of corruptions studied in previous works. We provide
two different viewpoints to understand them. In the first viewpoint, the weak and strong guarantee
differ by the adaptivity of the adversary, while in the second viewpoint, the two guarantees differ in
themeasure of corruption . Then we argue that the two viewpoints are equivalent.
Adversary Adaptivity (AA) Viewpoint. In this viewpoint, the corruption is specified only for the
chosen action. That is, in each round t, the adversary only decides a single corruption level ϵt∈R≥0
and ensures |E[rt]− ⟨at, θ⋆⟩| ≤ ϵt. We consider two kinds of adversary: strong adversary who
decides ϵtafter seeing the chosen action at, and weak adversary who decides ϵtbefore seeing at.
The robustness of the algorithm is measured by how the regret depends onPT
t=1ϵt.
Corruption Measure (CM) Viewpoint. In this viewpoint, the corruption is individually specified
forevery action. That is, at each round t, the adversary decides ϵt(a)for all action a∈ A and ensures
E[rt|at=a]− ⟨a, θ⋆⟩=ϵt(a)for all a. The adversary always decides ϵt(·)before seeing at. To
evaluate the performance, we consider two different measures of the total corruption: the strong
measurePT
t=1|ϵt(at)|and the weak measurePT
t=1max a∈A|ϵt(a)|.
We argue that the two viewpoints are equivalent in the sense that the performance guarantee of an
algorithm under strong/weak adversary in the AA viewpoint are the same as those under strong/weak
measure in the CM viewpoint, respectively. This is by the following observation. A strong adversary
in the AA viewpoint who decides the corruption level ϵtafter seeing atcan be viewed as deciding
the corruption ϵt(a)for all action abefore seeing at, and set ϵt=|ϵt(at)|after seeing at. In other
words, ϵt(a)is the corruption planned (before seeing at) by a strong adversary assuming at=a, and
the adversary simply carries out its plan after seeing at. It is clear that this is equivalent to the CM
viewpoint withPT
t=1|ϵt(at)|as the corruption measure. See Appendix B for more details. On the
other hand, a weak adversary in the AA viewpoint has to decide an upper bound of the corruption level
ϵtno matter which action atis chosen by the learner. This can be viewed as deciding the corruption
ϵt(a)for every action abefore seeing atwith the restriction |ϵt(a)| ≤ϵtfor all a. Therefore, this is
equivalent to usingPT
t=1max a|ϵt(a)|to measure total corruption in the CM viewpoint.
In this work, we adopt the CM viewpoint as described in Section 2. With the CM viewpoint, for both
strong and weak settings, the power of the adversary remains the same as the standard “adaptive
adversary” (i.e., deciding the corruption function ϵt(·)based on the history up to time t−1), and
we only need to derive regret bounds with different corruption measures. All our results can also be
interpreted in the AA viewpoint, as the above argument suggests.
With this unified viewpoint, we categorize in Table 1 previous works on linear (contextual) bandits
based on the corruption measure, all under the same type of adversary. According to the definitions in
Table 1, CandC∞correspond to the strong measure and weak measure mentioned above, respectively.
It is easy to see that C≤ {C∞, Csq} ≤Csq,∞≤Cms, where C∞andCsqare incomparable.
3Table 1: Classification of previous works based on the corruption measure. Foster et al. (2020),
Takemura et al. (2021), and He et al. (2022) studied the more general linear contextual bandit setting
where the action set can be chosen by an adaptive adversary in every round. Foster et al. (2020) and
Takemura et al. (2021) reported their bounds in Csq,∞andCms, respectively, though one can make
minor modifications to their analysis and show that their algorithms actually ensure the Csqbound.
Measure Definition Work
Cms Tmax t,a|ϵt(a)| Lattimore et al. (2020), Neu and Olkhovskaya (2020)
Csq,∞ 
TPT
t=1max aϵt(a)21/2Liu et al. (2024)
Csq 
TPT
t=1ϵt(at)21/2Foster et al. (2020), Takemura et al. (2021)
C∞PT
t=1max a|ϵt(a)| Li et al. (2019), Bogunovic et al. (2020)
CPT
t=1|ϵt(at)| Bogunovic et al. (2021, 2022), He et al. (2022)
Table 2: Regret bounds under corruption measure CandC∞. See Table 1 for their definitions.
He et al. (2022) studied the more general linear contextual bandits setting, though it also gives the
state-of-the-art Cbound for linear bandits.
Setting C∞bound Cbound
Upper boundStochastic LBd√
T+√
dC∞
(Algorithm 1)d√
T+dC
(He et al., 2022)
Adversarial LBd√
T+√
dC∞
(Algorithm 2)√
d3T+dC
(Algorithm 3)
Lower boundd√
T+√
dC∞
(Lattimore et al., 2020)d√
T+dC
(Bogunovic et al., 2021)
For stochastic linear bandits, considering the relations among different corruption measures, the
Pareto frontiers of the existing upper bounds are eO(d√
T+√
dCsq)by Foster et al. (2020) and
Takemura et al. (2021), and eO(d√
T+dC)by He et al. (2022). The lower bound frontiers are
Ω(d√
T+√
dCms)by Lattimore et al. (2020) and Ω(d√
T+dC)by Bogunovic et al. (2020). These
results imply an eO(d√
T+dC∞)upper bound and an Ω(d√
T+√
dC∞)lower bound, which still
have a gap. In this work, we close the gap by showing an eO(d√
T+√
dC∞)upper bound.
For adversarial linear bandits, we are only aware of upper bound eO(d√
T+√
dCsq,∞)by Liu
et al. (2024), and not aware of any upper bounds related to C∞orC. In this work, we show
eO(d√
T+√
dC∞)andeO(√
d3T+dC)upper bounds. The results are summarized in Table 2. As
in most previous work, we assume that C∞andC(or their upper bounds) are known by the learner
when developing the algorithms. The case of unknown C∞orCis discussed in Appendix C.
We emphasize that before our work, for both stochastic and adversarial linear bandits, it was unknown
how to achieve eO(d√
T+√
dC∞)regret. To see how C∞is different from other notions such
asCmsandCsq, we observe that for stochastic linear bandits, while eO(d√
T+√
dCsq)can be
achieved via deterministic algorithms, it is not the case for eO(d√
T+√
dC∞). The reason is that for
deterministic algorithms, the adversary can control C∞to be the same as C, for which Ω(d√
T+dC)
is unavoidable. We formalize this in Proposition 1, with the proof given in Appendix D. This
precludes the possibility of many previous algorithms to actually achieve the eO(d√
T+√
dC∞)
upper bound, e.g., Lattimore et al. (2020), Takemura et al. (2021), Bogunovic et al. (2020, 2021), He
et al. (2022).
Proposition 1. For stochastic linear bandits, there exists a deterministic algorithm achieving RegT=
eO(d√
T+√
dC sq), while any deterministic algorithm must suffer RegT= Ω(d√
T+dC∞).
4Algorithm 1: Randomized Phased Elimination (for stochastic C∞andCbounds)
1Input :Z=√
dC∞ordC, action space A ⊂Rd, confidence level δ.
2LetA1=AandL=dlog(|A|T/δ).
3fork= 1,2, . . .do
4 Compute a G-optimal design (defined in Section 2) pkoverAk, and let Gk=P
apk(a)aa⊤.
Define Ik= [(2k−1−1)L+ 1,(2k−1)L]andmk=|Ik|= 2k−1L.
5 fort∈ IkdoDraw at∼pkand receive rtwhere E[rt] =a⊤
tθ⋆+ϵt(at).
6 Define reward vector estimator bθk= (mkGk)−1P
t∈Ikatrtand active action set:
Ak+1=n
a∈ Ak: max
b∈Akb⊤bθk−a⊤bθk≤8q
dlog(|A|T/δ)
mk+2Z
mko
. (1)
4 Stochastic Linear Bandits
In this section, we introduce Algorithm 1, which achieves optimal regret for both CandC∞.
Algorithm 1 is an elimination-based algorithm. At each epoch k, it samples actions from a fixed
distribution pk∈∆(Ak), which is a G-optimal design over the active action set Ak(Line 4). At the
end of epoch k, only actions that are within the error threshold will be kept in the active action set
of the next epoch (Eq. (1)). While previous works by Lattimore et al. (2020) and Bogunovic et al.
(2021) have used a similar elimination framework to obtain eO(d√
T+√
dCms)andeO(d√
T+d3
2C)
bounds, respectively, we note that their algorithms only specify the number of times the learner
should sample for each action in each epoch. This is different from our algorithm that requires the
learner to exactly use the distribution pkto sample actions in every round in epoch k. As argued in
Proposition 1, if their algorithms are instantiated as a deterministic algorithm, then the regret will be
at least Ω(d√
T+dC∞). Thus, this subtle difference is important.
Note that to achieve the tight C∞(orC) bound, Z=√
dC∞(orZ=dC) has to be input to the
algorithm to decide the error threshold. The guarantee of Algorithm 1 is stated in Theorem 4.1.
Theorem 4.1. With input Z=√
dC∞orZ=dC, Algorithm 1 ensures with probability at least
1−δthatRegT≤ O(dp
Tlog(T/δ) +ZlogT).
Algorithm 1 can also be shown to ensure that RegT≤ O(p
dTlog(|A|T/δ)+ZlogT), which could
be smaller than the bound given in Theorem 4.1 when |A|is small.
5 Adversarial Linear Bandits
In this section, we consider corrupted adversarial linear bandits. Although adversarial linear bandits
have been widely studied, robustness under corruption is an under-explored topic: there is no prior
work obtaining regret bounds that linearly depends on either C∞orC.
5.1 C∞bound in Adversarial Linear Bandits
Our algorithm (Algorithm 2) is based on follow-the-regularized-leader (FTRL) with logdet regularizer.
Similar to previous works (Foster et al., 2020; Zimmert and Lattimore, 2022; Liu et al., 2024, 2023b)
that utilize logdet regularizer, the feasible set His inR(d+1)×(d+1), which is the space of the
covariance matrix for distributions over the lifted action space (Line 2–Line 3). At round t, the
algorithm obtains a covariance matrix HHHtby solving the FTRL objective (Eq. (2)). The action
distribution ptis such that the induced covariance matrix is equal to HHHt(Eq. (3)). After sampling
at∼ptand obtaining the reward rt, the algorithm constructs reward vector estimator bθt(Line 8) and
feeds it to FTRL. The reader may refer to Zimmert and Lattimore (2022) for more details.
In typical corruption-free adversarial linear bandits, the learner would construct an unbiased reward
vector estimator. However, in the presence of corruption, the learner can no longer construct an
unbiased estimator. To compensate the bias, we adopt the idea of “adding exploration bonus” inspired
5Algorithm 2: FTRL with log-determinant barrier regularizer (for adversarial C∞bound)
1Parameters :α= max
C∞√
dlog(T),√
T
,η=q
log(T)
Tandγ=d√
T.
2Letρ∈∆(A)be a G-optimal design over A, and let ∆γ(A) =
p:p= (1−γ)p′+γρ, p′∈∆(A)	
.
3Define feasible set H=n
dCov(p) :p∈∆γ(A)o
. (dCov(p)is defined in Section 2)
4Define G(HHH) =−log det( HHH)andB0= 0.
5fort= 1,2, . . .do
6 Solve the fixed-point problem Eq. (2)–Eq. (5).
HHHt= argmax
HHH∈H{η⟨HHH,ΛΛΛt−1⟩ −G(HHH)}where ΛΛΛt−1=
αBt1
2Pt−1
s=1bθs
1
2Pt−1
s=1bθ⊤
s 0
(2)
pt∈∆γ(A)be such that HHHt=dCov(pt), (3)
Σt=X
a∈Apt(a)aa⊤, (4)
Bt=BONUS (Bt−1,Σt). (Defined in Figure 1a ) (5)
7 Sample at∼pt. Observe reward rtwithE[rt] =a⊤
tθt+ϵt(at).
8 Construct reward estimator bθt= Σ−1
tatrt.
Function B′=BONUS (B,Σ):
ifB⪯Σ−1then return B′= Σ−1.
else
Perform eigen-decomposition:
B−1
2Σ−1B−1
2=Pd
i=1λiviv⊤
i,
where {vi}d
i=1are unit eigenvectors.
return B′=B1
2Pd
i=1max{λi,1}viv⊤
i
B1
2.
(a) The bonus function
𝐵𝑡−1
Σ𝑡−1
𝐵𝑡(b) Illustration for Bt=BONUS (Bt−1,Σt).
A psd matrix with eigenvalues (λi)d
i=1is rep-
resented as an ellipsoid with radius (√λi)d
i=1.
Figure 1: The bonus function and its illustration
by previous work on high-probability adversarial linear bandits (Lee et al., 2020; Zimmert and
Lattimore, 2022). In the regret analysis, the exploration bonus creates a negative term that cancels the
bias of the loss estimator. The bonus is represented by the Btin Eq. (5).
To decide the form of Bt, we first analyze the bias. With the standard construction of the reward
estimator, the bias on the benchmark action ucan be calculated as (with ϵt:= max a|ϵt(a)|)
u⊤ 
Et[Σ−1
tatrt]−θt=u⊤Et[Σ−1
tatϵt(at)]≤ϵtq
u⊤Σ−1
tEt[ata⊤
t]Σ−1
tu=ϵt∥u∥Σ−1
t,(6)
where Σtis the feature covariance matrix induced by pt(defined in Eq. (4)). Below, we compare
different bonus designs in previous and our work.
Bonus design in previous work. In Zimmert and Lattimore (2022), which is also based on logdet-
FTRL but where the goal is only to get a high-probability bound, the bonus introduces an additional
regret the form −α∥u∥2
Σ−1
t+αP
apt(a)∥a∥2
Σ−1
t. This can be used to cancel off the bias in Eq. (6):
PT
t=1ϵt∥u∥Σ−1
t−αPT
t=1∥u∥2
Σ−1
t+αPT
t=1P
a∈Apt(a)∥a∥2
Σ−1
t≤PT
t=1ϵ2
t
α+αdT, (7)
where we use AM-GM. Unfortunately, with the optimal α, this only leads to an additive regretp
dTP
tϵ2
t=√
dCsq,∞>√
dC∞, which does not meet our goal.
6Bonus design in our work. To obtain the tighter√
dC∞=√
dP
tϵtbound, our idea is to construct
a positive-definite matrix Btsuch that Bt⪰Σ−1
τfor all τ∈[t], and add bonus Bt−Bt−1at round t.
This way, the total negative regret on ubecomes −α∥u∥2
BTand the cancellation becomes
TX
t=1ϵt∥u∥Σ−1
t−α∥u∥2
BT+αTX
t=1X
a∈Apt(a)∥a∥2
Bt−Bt−1≤ PT
t=1ϵt2
α+αTX
t=1⟨Σt, Bt−Bt−1⟩,
(8)
where we use BT⪰Σ−1
tfor all tand AM-GM. With this, it suffices to find Btsatisfying our
condition Bt⪰Σ−1
τforτ≤t, and bound the overheadPT
t=1⟨Σt, Bt−Bt−1⟩byeO(d).
It turns out that there exists a way to inductively construct Btso that Bt⪰Σ−1
τfor all τ≤tandPT
t=1⟨Σt, Bt−Bt−1⟩≲log det( BT) =eO(d). This is by letting Btto be a minimal matrix such
thatBt⪰Bt−1andBt⪰Σ−1
t. By induction, this ensures Bt⪰Σ−1
τfor all τ≤t. The function
Bt=BONUS (Bt−1,Σt)is formally defined in Figure 1a. The geometric interpretation is finding the
minimal ellipsoid that contains both ellipsoids induced by Bt−1andΣ−1
t. An illustration figure is
given in Figure 1b.
We adopt the fixed-point formulation in Zimmert and Lattimore (2022) (see their FTRL-FB) that
includes the bonus for round t(i.e.,Bt) in the FTRL objective when calculating the policy at round t
(Eq. (2)). Notice that Bt, in turn, depends on the policy at round t(Eq. (5), where Σtdepends on
pt), and thus this forms a fixed-point problem. In the regret analysis, this avoids the “stability term”
of the bonus to appear in the regret bound. While the fixed-point solution always exists, it may not
be computationally efficient to find. For completeness, in Algorithm 4 (Appendix F), we present a
version that does not require solving fixed point but has a suboptimal d√logTC∞additive regret.
The guarantee of Algorithm 2 is stated in Theorem 5.1, with its proof deferred to Appendix F.
Theorem 5.1. Algorithm 2 ensures with probability of 1−δ,RegT=eO(d√
T+√
dC∞), where
eO(·)hides log(T/δ)factors.
5.2 Cbound in Adversarial Linear Bandits
To see how to obtain a Cbound, we perform the bias analysis again. Similar but slightly different
from Eq. (6), with the standard loss estimator, the bias on action u’s reward is bounded by
u⊤ 
Et[Σ−1
tatrt]−θt=u⊤Et[Σ−1
tatϵt(at)]≤ ∥u∥Σ−1
tEth
∥at∥Σ−1
t|ϵt(at)|i
. (9)
Unlike in Eq. (6), we do not relax |ϵt(at)|toϵt= max a|ϵt(a)|because we want the final bound
to depend on C=P
t|ϵt(at)|. The idea to ensure that the sum of Eq. (9) over tcan be related to
Cis to make ∥at∥Σ−1
tbounded by a constant poly( d), which allows us to further bound Eq. (9) by
poly( d)∥u∥Σ−1
t|ϵt(at)|. Such a property holds in standard linear bandit algorithms that operate in the
continuous action space where atis a point in the convex hull of A, and utilize a more concentrated
action sampling scheme. Algorithms that are of this type include SCRiBLe (Abernethy et al., 2008)
and continuous exponential weights (CEW) (Ito et al., 2020).
For SCRiBLe and CEW, the work by Lee et al. (2020) and Zimmert and Lattimore (2022) developed
techniques that incorporate bonus terms to get high probability regret bounds. The bonus terms
introduced by Zimmert and Lattimore (2022) is similar to that discussed in Eq. (7), which only
allows us to get a Csqbound. The bonus terms introduced by Lee et al. (2020) allows us to obtain
aCbound, but the overhead introduced by the bonus terms is much larger, resulting in a highly
sub-optimal regret bound. Indeed, as shown in Appendix J, adopting their bonus construction results
in an additional regret of d5
2C. With several attempts, we are only able to obtain the tight corruption
dependency dCusing the bonus in Section 5.1. To use that bonus, however, it is necessary to lift
the problem to (d+ 1)2-dimensional space. Unfortunately, existing SCRiBLe and CEW algorithms
only operate in the original d-dimensional space, and as discussed above, we need them to ensure
∥at∥Σ−1
t≤poly( d).
In order to combine these two useful ideas (i.e., our bonus design in Section 5.1, and the concentrated
sampling scheme by SCRiBLe or CEW), we end up with the algorithm that runs CEW over the lifted
action space (Algorithm 3). In order to simplify the exposition, we assume without loss of generality
7Algorithm 3: Continuous exponential weights (for adversarial Cbound)
1Parameters :γ= 1/T,α=√
dT+C,β= 4 log(10 dT),η=p
d/T.
2fort= 1,2, . . . , T do
3 Solve the fixed-point problem Eq. (10)-Eq. (13).
q′
t(h) =exp 
η⟨h, ϕ(ΛΛΛt−1)⟩
R
h′∈ϕ(H)exp 
η⟨h′, ϕ(ΛΛΛt−1)⟩
dh′where ΛΛΛt−1=
αBt1
2Pt−1
s=1bθs
1
2Pt−1
s=1bθ⊤
s 0
.
(10) 4
5 Letqt∈∆(H)andpt∈∆(A)be the distributions of HHH∈ H anda∈ A, respectively,
generated by the following ( ZZZis ad×(d+ 1) matrix):
h∼q′
t, HHH=ϕ−1(h), a =
1 0 ··· 0 0
0 1 ··· 0 0
...............
0 0 ··· 1 0
HHHed+1:=ZZZHHHed+1. (11)
˜pt(a) =pt(a) 1{∥a∥Σ−1
t≤√
dβ}
R
a′∈Apt(a′) 1{∥a′∥Σ−1
t≤√
dβ}da′,where Σt=Ea∼pt[aa⊤]. (12)
Bt=BONUS (Bt−1,eΣt),whereeΣt=γI+Ea∼˜pt[aa⊤]. (13)
6 Sample at∼˜pt, and observe reward rtwithE[rt] =a⊤
tθt+ϵt(at).
7 Construct reward estimator bθt=eΣ−1
tatrt.
thatA=conv(A). The lifted action space is H={dCov(p) :p∈∆(A)} ⊂R(d+1)×(d+1). The
price of the lifting is that the “regularization penalty term” in the regret analysis now grows from
eO(d/η)toeO(d2/η), which gives us the√
d3Tsub-optimal regret.
Note that CEW requires the assumption that the feasible set is a convex body with non-zero volume,
but the effective dimension of His strictly smaller than (d+1)2and thus have zero volume in R(d+1)2.
To correctly write the algorithm, we introduce an invertible linear transformation ϕ:R(d+1)2→Rm
that maps an (d+ 1)2-dimensional action set Hto an m-dimensional one, where mis the effective
dimension of H. In Appendix I, we formally define this ϕ. The algorithm uses ϕto map all lifted
actions and reward estimators from R(d+1)×(d+1)toRm.
The exponential weights runs over the space of ϕ(H)(see Eq. (10)). A point h∈ϕ(H)sampled
from the exponential weights can be linearly mapped to an action a∈ A according to Eq. (11). We
useq′
tto denote the exponential weight distribution in ϕ(H), and use ptto denote the corresponding
distribution in A. Instead of sampling atfrompt, we sample it through rejection sampling that rejects
samples with ∥at∥Σ−1
t>˜Θ(√
d)(Eq. (12)). This technique was developed by Ito et al. (2020), and
this guarantees ∥at∥Σ−1
t≤eO(√
d)—which is our goal as discussed in Eq. (9)—while keeping the
clipped distribution ˜ptclose enough to the original distribution pt. This last property heavily relies
on the log-concavity of the exponential weight distribution (Ito et al., 2020). The definition of the
bonus term is similar to that in Algorithm 2 (Eq. (13)). The construction of the reward estimator
(Line 7) and the way of lifting (Eq. (10)) are also similar to those in Algorithm 2. Again, we adopt
the fixed-point formulation where the calculation of the policy at time tinvolves the bonus at time t,
which, in turn, depends on the policy at time t. It is unlikely that this algorithm can be polynomial
time. As a remedy, we provide a polynomial time algorithm (Algorithm 6) in Appendix J with a
much worse regret bound of eO(d3√
T+d5/2C). The regret guarantee of Algorithm 3 is given in the
following theorem.
Theorem 5.2. Algorithm 3 ensures with probability at least 1−δ,RegT=eO √
d3T+dC
, where
eO(·)hides polylog (T/δ)factors.
86 Gap-Dependent Misspecification
Intimately related to corrupted settings are misspecified settings, settings where our model class is
unable to capture the true environment we are working with. For example, we might consider a
stochastic linear bandit problem where the underlying reward function f(·)is nearly linear, i.e., there
exists some θandϵmis(·)such that |f(a)−a⊤θ| ≤ϵmis(a)for each a. Indeed, in such settings,
playing on our true (nearly linear) environment is equivalent to playing on the environment with
reward mean a⊤θand with corruption ϵmis(a)at each step. Thus, if we can solve corruption settings,
it stands to reason that we can solve misspecified settings.
Here we are particularly interested in obtaining bounds on misspecified decision-making that scale
precisely with action-dependent misspecification, ϵmis(a). While it is relatively straightforward
to obtain bounds on learning in misspecified settings for a uniform level of misspecification ϵ≥
max a∈Aϵmis(a), obtaining bounds on learning with action-dependent misspecification have proved
more elusive. To formalize this, we consider, in particular, the following gap-dependent notion of
misspecification defined in Liu et al. (2023a).
Assumption 1 (Gap-Dependent Misspecification (Liu et al., 2023a)) .There exists some θ∈Rdsuch
that some ρ >0, denoting ∆(a) = max a′f(a′)−f(a), we have for any a∈ A,
|f(a)−a⊤θ| ≤ρ·∆(a).
We letM⋆denote the original environment with reward function f(a)(with RegM⋆
Tthe corresponding
regret), and M0the environment with linear reward, a⊤θ, (with RegM0
Tthe corresponding regret).
Assumption 1 allows the reward to be misspecified, but the misspecification level for an action scales
with how suboptimal that action is. This could correspond to real-world settings where, for example,
significant attention has been given to modeling near-optimal behavior, such that it is accurately
represented within our model class, but much less attention has been given to modeling suboptimal
behavior. We assume access to a generic corruption-robust algorithm.
Assumption 2. We have access to a regret minimization algorithm which takes as input some C′and
with probability at least 1−δhas regret bounded on M0as
RegM0
T≤ C1(δ, T)√
T+C2(δ, T)C′
ifC′≥C≜PT
t=1ϵmis(at), and by Totherwise, for Cas defined above and for (problem-dependent)
constants C1(δ, T),C2(δ, T)which may scale at most logarithmically with Tand1
δ.
Assumption 2 is essentially the guarantee of a corruption-robust algorithm in terms of strong corrup-
tion measure (defined in Section 3). Note, in particular, that Assumption 2 only needs to obtain a
sub-linear regret guarantee in the known-corruption setting, and can have linear regret in the setting
where the corruption level is unknown. We then have the following result.
Theorem 6.1. Assume our environment satisfies Assumption 1 and that we have access to a corruption-
robust algorithm satisfying Assumption 2. Then as long as ρ≤min{1
2,1
4C2(δ
T, T)−1}, with proba-
bility at least 1−2δwe can achieve regret bounded as:
RegM⋆
T≤6C1(δ
T, T)√
T+ 4p
2Tlog(1/δ) + 4.
Theorem 6.1 states that, assuming our environment exhibits gap-dependent misspecification with
tolerance ρ≤min{1
2,1
4C2(δ
T, T)−1}, then we can achieve regret on the true environment bounded as
the leading-order term of our corruption-robust oracle, C1(δ
T, T)√
T, with additional overhead of only
˜O(√
T). This reduction is almost entirely black-box: it requires knowledge of C1(δ, T)andC2(δ, T),
but does not require knowledge of ρor any other facts about the corruption-robust algorithm.
Remark 1 (Anytime Algorithm) .The oracle of Assumption 5 must be anytime , achieving the above
regret guarantee for any Tnot given as an input. Though many existing corruption-robust algorithms
takeTas input, the standard doubling trick can convert them into an anytime algorithm.
6.1 Optimal Misspecification Rate for Linear Bandits
We are particularly interested in how stringent a condition on the misspecification level—how
small a value of ρ—Theorem 6.1 requires. As we have shown, Theorem 4.1 obtains the optimal
misspecification level of dC. We then have the following corollary.
9Corollary 6.1.1. Assume our environment is a misspecified linear bandit satisfying Assumption 1
withρ≤ O(1
dlogT). Then instantiating Assumption 2 with the algorithm of Theorem 4.1, we can
achieve regret bounded with probability 1−δasRegM⋆
T≤ O(dp
Tlog(T/δ)).
While the regret bound of Corollary 6.1.1 achieves a scaling of eO(d√
T), which is tight for linear
bandits (Lattimore and Szepesvári, 2020), it is unclear its requirement on ρofρ≤eO(1
d)is optimal.
The result below shows that it is not optimal because ρ≤ O(1√
d)suffices for eO(d√
T)regret.
Theorem 6.2. Assume our environment is a misspecified linear bandit satisfying Assumption 1 with
ρ≤ O(1√
d). Then there exists an algorithm that achieves, w.p. 1−δ:RegM⋆
T≤ O(dp
Tlog(T/δ)).
Theorem 6.2 relies on a specialized algorithm for the gap-dependent misspecification setting, and
improves on the best-known bound for gap-dependent misspecification in linear bandits, which
requires ρ≤eO(1
d)(Liu et al., 2023a). Moreover, for ρ > c T1√
dfor some logarithmic term cT,
adapting the lower-bound instance from Lattimore et al. (2020), we show that achieving sub-linear
regret is not possible (Theorem K.2). These results jointly show that ρ≈1√
dis the best ρwe can
hope for. This disproves the conjecture of Liu et al. (2023a) that ρ= Θ(1) is possible.
Note that the reduction in Theorem 6.1 is notable to achieve a tight ρ—while reducing from gap-
dependent misspecification to corruption allows for black-box usage of existing algorithms, it requires
more stringent conditions on the misspecification level than specialized algorithms for this setting.
6.2 Gap-Dependent Misspecification in Reinforcement Learning
Theorem 6.1 is a corollary of a more general result, Theorem L.1, which applies to misspecified rein-
forcement learning, where we there assume a generalized notion of gap-dependent misspecification:
for each policy π,EM⋆,π[PH
h=1ϵmis
h(sh, ah)]≤ρ·(V⋆
0−Vπ
0), forVπ
0the expected reward of policy
π, and ϵmis
h(s, a)a measure of the misspecification at step h, state s, and action a. To illustrate this
general reduction, we consider the following setting, a generalization of linear MDPs (Jin et al., 2020).
Assumption 3 (Gap-Dependent Misspecified Linear MDPs) .Letϕ(s, a) :S × A → Rddenote
some feature map and µh· S → Rdsome measure which satisfy ∥ϕ(s, a)∥2≤1,∀s, a, and
∥R
s|dµh(s)|∥2≤√
d. Assume that the transitions Ph(· |s, a)on our true environment satisfy:
∥Ph(· |s, a)− ⟨ϕ(s, a),µh(·)⟩∥TV≤ϵmis
h(s, a)
for some ϵmis
h(s, a)≥0and∥P−Q∥TVthe total variation distance between PandQ. Furthermore,
assume that for any policy π, we have Eπ[PH
h=1ϵmis
h(sh, ah)]≤ρ·(V⋆
0−Vπ
0).
We then have the following result.
Corollary 6.2.1. Assume our environment satisfies Assumption 3 with ρ≤eO(1
dH). Then there exists
an algorithm that achieves regret bounded with probability 1−δasRegM⋆
T≤eO(√
d3H2T).
To the best of our knowledge, Corollary 6.2.1 is the first result showing that it is possible to efficiently
learn in linear MDPs with gap-dependent misspecification. Note that under Assumption 3, our MDP
could be far from a linear MDP—we simply assume that if we play a “good” policy, it appears as
approximately linear. This result is almost immediate by instantiating our reduction with a known
corruption-robust algorithm for linear MDPs (Ye et al., 2023).
7 Open Problems
It remains open how to achieve d√
T+dCregret in corrupted adversarial linear bandits. The tight
C∞bound for corrupted linear contextual bandits, where the action set can be chosen by an adaptive
adversary in every round, also remains open. The best known upper and lower bounds for this setting
areeO(d√
T+dC∞)by He et al. (2022) and Ω(d√
T+√
dC∞)by Lattimore and Szepesvári (2020).
With the AA viewpoint in Section 3, our work first shows the separation between the achievable
regret under weak adversary and strong adversary in corrupted linear bandits. An interesting future
direction is to investigate similar separation in general decision making (Foster et al., 2021).
10References
Abernethy, J. D., Hazan, E., and Rakhlin, A. (2008). Competing in the dark: An efficient algorithm
for bandit linear optimization. In COLT , pages 263–274. Citeseer.
Auer, P., Cesa-Bianchi, N., Freund, Y ., and Schapire, R. E. (2002). The nonstochastic multiarmed
bandit problem. SIAM journal on computing , 32(1):48–77.
Auer, P. and Chiang, C.-K. (2016). An algorithm with nearly optimal pseudo-regret for both stochastic
and adversarial bandits. In Conference on Learning Theory .
Bogunovic, I., Krause, A., and Scarlett, J. (2020). Corruption-tolerant gaussian process bandit
optimization. In International Conference on Artificial Intelligence and Statistics , pages 1071–
1081. PMLR.
Bogunovic, I., Li, Z., Krause, A., and Scarlett, J. (2022). A robust phased elimination algorithm for
corruption-tolerant gaussian process bandits. Advances in Neural Information Processing Systems ,
35:23951–23964.
Bogunovic, I., Losalka, A., Krause, A., and Scarlett, J. (2021). Stochastic linear bandits robust to
adversarial attacks. In International Conference on Artificial Intelligence and Statistics , pages
991–999. PMLR.
Bubeck, S., Cesa-Bianchi, N., and Kakade, S. M. (2012). Towards minimax policies for online
linear optimization with bandit feedback. In Conference on Learning Theory , pages 41–1. JMLR
Workshop and Conference Proceedings.
Bubeck, S. and Slivkins, A. (2012). The best of both worlds: Stochastic and adversarial bandits. In
Conference on Learning Theory .
Chewi, S. (2023). The entropic barrier is n-self-concordant. In Geometric Aspects of Functional
Analysis: Israel Seminar (GAFA) 2020-2022 , pages 209–222. Springer.
Dann, C., Lattimore, T., and Brunskill, E. (2017). Unifying pac and regret: Uniform pac bounds for
episodic reinforcement learning. Advances in Neural Information Processing Systems , 30.
Dann, C., Wei, C.-Y ., and Zimmert, J. (2023). A blackbox approach to best of both worlds in bandits
and beyond. In The Thirty Sixth Annual Conference on Learning Theory , pages 5503–5570. PMLR.
Du, S. S., Kakade, S. M., Wang, R., and Yang, L. F. (2019). Is a good representation sufficient for
sample efficient reinforcement learning? arXiv preprint arXiv:1910.03016 .
Foster, D. J., Gentile, C., Mohri, M., and Zimmert, J. (2020). Adapting to misspecification in
contextual bandits. Advances in Neural Information Processing Systems , 33:11478–11489.
Foster, D. J., Kakade, S. M., Qian, J., and Rakhlin, A. (2021). The statistical complexity of interactive
decision making. arXiv preprint arXiv:2112.13487 .
Hajiesmaili, M., Talebi, M. S., Lui, J., Wong, W. S., et al. (2020). Adversarial bandits with corruptions:
Regret lower bound and no-regret algorithm. Advances in Neural Information Processing Systems ,
33:19943–19952.
He, J., Zhou, D., Zhang, T., and Gu, Q. (2022). Nearly optimal algorithms for linear contextual bandits
with adversarial corruptions. Advances in Neural Information Processing Systems , 35:34614–
34625.
Ito, S. (2021). Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret
bounds. In Conference on Learning Theory .
Ito, S., Hirahara, S., Soma, T., and Yoshida, Y . (2020). Tight first-and second-order regret bounds for
adversarial linear bandits. Advances in Neural Information Processing Systems , 33:2028–2038.
Ito, S. and Takemura, K. (2023). Best-of-three-worlds linear bandit algorithm with variance-adaptive
regret bounds. In The Thirty Sixth Annual Conference on Learning Theory , pages 2653–2677.
PMLR.
11Ito, S. and Takemura, K. (2024). An exploration-by-optimization approach to best of both worlds in
linear bandits. Advances in Neural Information Processing Systems , 36.
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. (2017). Contextual
decision processes with low bellman rank are pac-learnable. In International Conference on
Machine Learning , pages 1704–1713. PMLR.
Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2020). Provably efficient reinforcement learning with
linear function approximation. In Conference on learning theory , pages 2137–2143. PMLR.
Jin, T., Liu, J., Rouyer, C., Chang, W., Wei, C.-Y ., and Luo, H. (2024). No-regret online reinforcement
learning with adversarial losses and transitions. Advances in Neural Information Processing
Systems , 36.
Kong, F., Zhao, C., and Li, S. (2023). Best-of-three-worlds analysis for linear bandits with follow-the-
regularized-leader algorithm. In The Thirty Sixth Annual Conference on Learning Theory , pages
657–673. PMLR.
Lattimore, T. and Szepesvári, C. (2020). Bandit algorithms . Cambridge University Press.
Lattimore, T., Szepesvari, C., and Weisz, G. (2020). Learning with good feature representations in
bandits and in rl with a generative model. In International conference on machine learning , pages
5662–5670. PMLR.
Lee, C.-W., Luo, H., Wei, C.-Y ., and Zhang, M. (2020). Bias no more: high-probability data-
dependent regret bounds for adversarial bandits and mdps. Advances in neural information
processing systems , 33:15522–15533.
Lee, C.-W., Luo, H., Wei, C.-Y ., Zhang, M., and Zhang, X. (2021). Achieving near instance-
optimality and minimax-optimality in stochastic and adversarial linear bandits simultaneously. In
International Conference on Machine Learning , pages 6142–6151. PMLR.
Li, Y ., Lou, E. Y ., and Shan, L. (2019). Stochastic linear optimization with adversarial corruption.
arXiv preprint arXiv:1909.02109 .
Li, Y . and Yang, L. (2024). On the model-misspecification in reinforcement learning. In International
Conference on Artificial Intelligence and Statistics , pages 2764–2772. PMLR.
Liu, C., Yin, M., and Wang, Y .-X. (2023a). No-regret linear bandits beyond realizability. arXiv
preprint arXiv:2302.13252 .
Liu, H., Wei, C.-Y ., and Zimmert, J. (2023b). Towards optimal regret in adversarial linear mdps with
bandit feedback. arXiv preprint arXiv:2310.11550 .
Liu, H., Wei, C.-Y ., and Zimmert, J. (2024). Bypassing the simulator: Near-optimal adversarial linear
contextual bandits. Advances in Neural Information Processing Systems , 36.
Nesterov, Y . and Nemirovskii, A. (1994). Interior-point polynomial algorithms in convex program-
ming . SIAM.
Neu, G. and Olkhovskaya, J. (2020). Efficient and robust algorithms for adversarial linear contextual
bandits. In Conference on Learning Theory , pages 3049–3068. PMLR.
Pukelsheim, F. (2006). Optimal design of experiments . SIAM.
Seldin, Y . and Lugosi, G. (2017). An improved parametrization and analysis of the exp3++ algorithm
for stochastic and adversarial bandits. In Conference on Learning Theory .
Seldin, Y . and Slivkins, A. (2014). One practical algorithm for both stochastic and adversarial bandits.
InInternational Conference on Machine Learning .
Takemura, K., Ito, S., Hatano, D., Sumita, H., Fukunaga, T., Kakimura, N., and Kawarabayashi, K.-i.
(2021). A parameter-free algorithm for misspecified linear contextual bandits. In International
Conference on Artificial Intelligence and Statistics , pages 3367–3375. PMLR.
12Wang, R., Salakhutdinov, R. R., and Yang, L. (2020). Reinforcement learning with general value
function approximation: Provably efficient approach via bounded eluder dimension. Advances in
Neural Information Processing Systems , 33:6123–6135.
Wei, C.-Y ., Dann, C., and Zimmert, J. (2022). A model selection approach for corruption robust
reinforcement learning. In International Conference on Algorithmic Learning Theory , pages
1043–1096. PMLR.
Wei, C.-Y . and Luo, H. (2018). More adaptive algorithms for adversarial bandits. In Conference On
Learning Theory .
Ye, C., Xiong, W., Gu, Q., and Zhang, T. (2023). Corruption-robust algorithms with uncertainty
weighting for nonlinear contextual bandits and markov decision processes. In International
Conference on Machine Learning , pages 39834–39863. PMLR.
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E. (2020). Learning near optimal policies
with low inherent bellman error. In International Conference on Machine Learning , pages 10978–
10989. PMLR.
Zhang, W., He, J., Fan, Z., and Gu, Q. (2023). On the interplay between misspecification and
sub-optimality gap in linear contextual bandits. In International Conference on Machine Learning ,
pages 41111–41132. PMLR.
Zimmert, J. and Lattimore, T. (2022). Return of the bias: Almost minimax optimal high probability
bounds for adversarial linear bandits. In Conference on Learning Theory , pages 3285–3312.
PMLR.
Zimmert, J., Luo, H., and Wei, C.-Y . (2019). Beating stochastic and adversarial semi-bandits optimally
and simultaneously. In International Conference on Machine Learning .
Zimmert, J. and Seldin, Y . (2019). An optimal algorithm for stochastic and adversarial bandits. In
The 22nd International Conference on Artificial Intelligence and Statistics , pages 467–475. PMLR.
13Appendices
A Related Work 15
B Equivalence Between AA and CM Viewpoints for Strong Corruption 16
C The Case of Unknown C∞orC 16
D Proof of Proposition 1 16
E Proof of Theorem 4.1 17
F Proof of Theorem 5.1 20
G Computationally Efficient Algorithm for Adversarial C∞Bound 25
H Proof of Theorem 5.2 26
I Dimension Reduction for Continuous Exponential Weights 29
J Computationally Efficient Algorithm for Adversarial CBound 30
J.1 Preliminaries for Entropic Barrier . . . . . . . . . . . . . . . . . . . . . . . . . . 31
J.2 Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
J.3 Regret Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
K Gap-dependent Misspecification 35
L General Reduction from Corruption-Robust Algorithms to Misspecification 38
M Auxiliary Lemmas 42
14A Related Work
Model Misspecification Theoretical works on bandits or RL often assume that the underlying
world is well-specified by a particular model. Algorithms that are purely built on this assumption
are vulnerable to potential misspecifications. Therefore, some works, besides proposing the main
results, also discuss the case where the model is misspecified, such as Jiang et al. (2017); Jin et al.
(2020); Zanette et al. (2020); Wang et al. (2020); Li and Yang (2024). These discussions, however,
usually assume that the amount of misspecification has a uniform upper bound for all actions / states /
policies, and the performance degradation is proportional to this uniform upper bound.
For settings like stochastic linear bandits and stochastic linear contextual bandits, it was also found
that some widely used algorithm such as LinUCB cannot achieve the tightest guarantee under
misspecification (Du et al., 2019). Therefore, a line of work developed better algorithms that have
optimal robustness against misspecification, such as Lattimore et al. (2020); Foster et al. (2020);
Takemura et al. (2021).
While most work focus on the stochastic setting, Neu and Olkhovskaya (2020) took a first step in
studying misspecification in linear contextual bandits with stochastic contexts and adversarial rewards.
They established near-optimal regret dependencies on the amount of misspecification.
Gap-dependent Misspecification Gap-dependent misspecification is a setting where the amount
of misspecification for an action is bounded by a constant times that action’s sub-optimality gap. To
our knowledge, this setting is first studied by Liu et al. (2023a) for linear bandits. Another related
work is Zhang et al. (2023), which assumes that the misspecification is bounded by a constant times
theminimal sub-optimality gap among all actions. Although this assumption is more restrictive, they
handle the more general linear contextual bandit setting, and derive instance-dependent logarithmic
regret bounds.
Corruption-robust Bandits The guarantees on model misspecification is rather pessimistic in the
sense that if the misspecification is time-varying, and large misspecification only appears in a few
rounds, then the existing guarantees for misspeicifcation still scale with the largest misspeicification.
To refine such guarantee, previous works have consider different notions of time-varying corruption,
and established more fine-grained regret guarantees. These include Csq,∞, Csq, C∞, andCdiscussed
in Section 3. Among them, Csq,∞andC∞are usually studied under the “weak adversary” framework
where the adversary decides the corruption before seeing the action chosen by the learner. On
the other hand, CsqandCare usually studied under the “strong adversary” framework where the
adversary decides the corruption after seeing the action chosen the learner. In Section 3, we provide a
unified view for them so that they can both be regarded as weak adversarial setting but with different
corruption measure.
The algorithms of Foster et al. (2020) and Takemura et al. (2021) achieved the optimal bound with
respect to Csqfor stochastic linear contextual bandits (i.e., d√
T+√
dCsq), and He et al. (2022)
showed the optimal bound with respect to C(i.e.,d√
T+dC). However, it is still unclear whether
the tight dependency on C∞is√
dC∞ordC∞. In this paper, we answer it for the context-free linear
bandit setting, showing that d√
T+√
dC∞is achievable. However, the question remains open for
linear contextual bandits.
For the adversarial setting, Liu et al. (2024) showed d2√
T+√
dCsq,∞bound for linear contextual
bandits with stochastic contexts and adversarial rewards, which can be improved to d√
T+√
dCsq,∞
when specialized to adversarial linear bandits. To our knowledge, no C∞orCbound has been shown
for adversarial linear bandits, and our work make the first attempts on them.
We remark that for A-armed adversarial bandits, it is easy to see that√
AT+C∞bound is achievable
simply by running standard adversarial multi-armed bandit algorithm that handles adaptive adversary
(e.g., EXP3.P by Auer et al. (2002)). The work of Hajiesmaili et al. (2020) is the only one that we
know to obtain Cbound for adversarial bandits. They showed a√
AT+ACbound for A-armed
bandits, which is tight.
Best-of-both-worlds Bounds The study of the best-of-both-world problem was initiated by Bubeck
and Slivkins (2012) and extended by Seldin and Slivkins (2014); Auer and Chiang (2016); Seldin
15and Lugosi (2017); Wei and Luo (2018); Zimmert and Seldin (2019); Zimmert et al. (2019); Ito
(2021); Ito and Takemura (2023, 2024); Dann et al. (2023); Kong et al. (2023). The goal of this line
of work is to have a single algorithm that achieves a eO(√
T)regret when the reward is adversarial
andO(logT)when the reward is stochastic, without knowing the type of reward in advance. These
results should be viewed as refinements of the standard adversarial setting but not the corruption
setting considered in our work, though they also used the term “corruption” in their work.
For example, Lee et al. (2021); Ito and Takemura (2024, 2023, 2024); Dann et al. (2023); Kong
et al. (2023) studied the best-of-both-world linear bandits problem. The underlying world could be
stochastic ( θt=θ⋆for all t) or adversarial ( θt’s are arbitrary). Their algorithm achieves a bound of
O(d2log(T)/∆)in the former case, where ∆is the reward gap between the best and the second-best
arm, and eO(d√
T)in the latter phase. They also define the corruption C′=P
tmax a|a⊤(θt−θ⋆)|
and show that their algorithm achieves a regret of O(d2log(T)/∆ +p
d2log(T)C′/∆). Compared
to our setting, their corruption is in a more limited form, but their target regret bound in the stochastic
setting is tighter than ours.
B Equivalence Between AA and CM Viewpoints for Strong Corruption
We show that strong corruption in both definitions is equivalent, that is, for any adversary having
strong corruption C=P
t|ϵt|from AA viewpoint, there exists an adversary using the equal amount
of strong corruptionP
t|ϵ′
t(at)|from CM viewpoint, where |ϵt|=|ϵ′
t(at)|for all t, and vice versa.
Assume that ϵ(Ht−1, at)is the function used by an AA strong adversary to decide the corruption at
timet, where Ht−1is the history up to time t−1andatis the chosen action at time t. Then we
define ϵ′
t(a)≜ϵ(Ht−1, a),∀afor the CM viewpoint, thus |ϵt|=|ϵ(Ht−1, at)|=|ϵ′
t(at)|. Note that
the function ϵ′
t(·)only depends on the history up to time t−1, so the definition of ϵ′
tis known to
adversary before observing at. The other direction of this equivalence is achieved by setting the
corruption in AA viewpoint as ϵt=ϵ′
t(at). Note that since atis known to a strong adversary in AA
viewpoint, ϵtis also known.
C The Case of Unknown C∞orC
In the corrupted stochastic setting, Wei et al. (2022) developed a black-box reduction that can turn
any algorithm achieving β1√
T+β2+β3C∞regret with the knowledge of C∞into an algorithm
achieving log(T)×(β1√
T+β2+β3C∞)regret without knowledge of C∞. This reduction can be
directly applied to our stochastic C∞bound result (Theorem 4.1), which allows us to achieve almost
the same regret bound without knowledge of C∞. The idea of Wei et al. (2022) has been extended to
the adversarial setting by Jin et al. (2024) (see their Section 4). Similarly, for the adversarial setting,
one can turn any algorithm achieving β1√
T+β2+β3C∞regret with known C∞into one achieving
log(T)×(β1√
T+β2+β3C∞)regret without knowing C∞. This can be directly applied to our
adversarial C∞result (Theorem 5.1).
The case of unknown Cis quite different. It has been proven by Bogunovic et al. (2021) that it
is impossible to achieve a bound that has linear scaling in C(e.g., β1√
T+β2+β3C) for all C
simultaneously if Cis not known by the learner. This is also mentioned in He et al. (2022) again.
Hence, almost all previous work studying Cbound assumes knowledge on C. IfCis unknown,
simply setting ¯C=√
Tas an upper bound of Cyields a bound of O(√
T+C2)—ifC≤√
Tindeed
holds, then ¯Cis a correct upper bound, so the regret can be bounded by O(√
T+¯C) =O(√
T); if
C >√
T, then simply bound the regret by T≤ O(C2).
D Proof of Proposition 1
First, we argue that there exists a deterministic algorithm achieving eO(d√
T+√
dCsq)upper bound.
The algorithm of Takemura et al. (2021) is such an algorithm, although they only showed an upper
bound of eO(d√
T+√
dCms). To argue the stronger eO(d√
T+√
dCsq)bound, we only need to
slightly modify their analysis: In their proof of Lemma 2 (in their Page 6), the original proof bound
16the per-step regret due to the misspecification as the following (the calculation below uses their
original notation):
X
τ∈Ψt,sϵτ(iτ)xτ(iτ)⊤V−1
t−1,sxt(i)≤ϵs
|Ψt,s|X
τ∈Ψt,s 
xτ(iτ)⊤V−1
t−1,sxt(i)2,
≤ϵq
|Ψt,s|xt(i)⊤V−1
t−1,sxt(i)
≤ϵq
|Ψt,s|c−2s
≤eO(ϵ√
d). (by their Lemma 1)
We can tighten their analysis by doing the following:
X
τ∈Ψt,sϵτ(iτ)xτ(iτ)⊤V−1
t−1,sxt(i)≤vuuut
X
τ∈Ψt,sϵτ(iτ)2

X
τ∈Ψt,s 
xτ(iτ)⊤V−1
t−1,sxt(i)2
,
≤vuuut
X
τ∈Ψt,sϵτ(iτ)2
xt(i)⊤V−1
t−1,sxt(i)
≤vuuut
X
τ∈Ψt,sϵτ(iτ)2
c−2s
≤eO
vuutd
|Ψt,s|X
τ∈Ψt,sϵτ(iτ)2
. (by their Lemma 1)
Since the regret for every step in Ψt,scan be bounded by this value, when summing the regret over
ΨT+1,s, one can get a regret of order
eO
s
d|ΨT+1,s|X
τ∈ΨT+1,sϵτ(iτ)2
.
Further summing this over s(there are logarithmically many different s) and using that [T] =
S
sΨT+1,sand using Cauchy-Schwarz, we get aq
dTPT
t=1ϵt(it)2=√
dCsqbound.
To argue that any deterministic algorithm must suffer at least Ω(d√
T+dC∞)regret, we only need
to use the lower bound instance of Ω(d√
T+dC). At the beginning of round t, the adversary
simply change the corruptions ϵt(a)to be zero for all a̸=at(the adversary knows what atsince the
algorithm is deterministic). This makes C=C∞, and thus the lower bound Ω(d√
T+dC∞)holds.
E Proof of Theorem 4.1
Lemma E.1. With probability at least 1−2δ, for all kand for all b∈ Ak,
|⟨b,bθk−θ⋆⟩| ≤4s
dlog(|A|T/δ)
mk+min{√
dC′, dC}
mk.
Proof. LetEt[·]be the expectation conditioned on the history up to round t−1. We fix kandband
consider
Xt=b⊤(mkGk)−1atrt
17fort∈ Ik. Notice thatP
t∈IkXt=b⊤bθkand
X
t∈IkEt[Xt] =X
t∈Ikb⊤(mkGk)−1Et
at(a⊤
tθ⋆+ϵt(at))
=b⊤θ⋆+b⊤(mkGk)−1X
t∈IkEt[atϵt(at)].
Also, by the definition of Gk, we have |Xt|=b⊤(mkGk)−1atrt≤1
mk∥b∥G−1
k∥at∥G−1
k≤d
mk.
Thus, by Freedman’s inequality, with probability at least 1−δ
|A|T, the following holds:
D
b,bθk−θ⋆E=X
t∈IkXt−X
t∈IkEt[Xt]+b⊤(mkGk)−1X
t∈IkEt[atϵt(at)]
≤vuutlog|A|T
δX
t∈IkEt[X2
t]
| {z }
term 1+d
mklog|A|T
δ
+b⊤(mkGk)−1X
t∈IkEt[atϵt(at)]
| {z }
term 2.
We bound term 1by
term 1=s
log(|A|T/δ)X
t∈IkEt
b⊤(mkGk)−1ata⊤
t(mkGk)−1b
=s
log(|A|T/δ)1
m2
kX
t∈Ikb⊤G−1
kb≤s
dlog(|A|T/δ)
mk,
andterm 2by
term 2=1
mkX
t∈IkX
a∈Akpk(a)ϵt(a)b⊤G−1
ka
≤1
mkX
t∈IksX
a∈Akpk(a)ϵt(a)2sX
a∈Akpk(a) 
b⊤G−1
ka2
≤1
mkX
t∈Ikmax
a′ϵt(a′)√
d≤√
dC′
mk.
or
term 2≤1
mkX
t∈IkEt
ϵt(at)b⊤G−1
kat
≤1
mkX
t∈Ikϵt(at)b⊤G−1
kat+1
mkvuutlog|A|T
δX
t∈IkEth
ϵt(at)2b⊤G−1
kat2i
+ϵt(at)b⊤G−1
kat
mklog|A|T
δ
(Freedman’s inequality)
≤d
mkX
t∈Ikϵt(at) +1
mkvuutlog|A|T
δX
t∈IkEt
b⊤G−1
kata⊤
tG−1
kb
+d
mklog|A|T
δ
≤dC
mk+1
mkvuutlog|A|T
δX
t∈Ikb⊤G−1
kb+d
mklog|A|T
δ
≤dC
mk+s
dlog(|A|T/δ)
mk+d
mklog|A|T
δ
.
18Thus, for any b∈ Ak, with probability at least 1−δ
|A|T,
D
b,bθk−θ⋆E≤2s
dlog(|A|T/δ)
mk+2d
mklog(|A|T/δ) +1
mkminn√
dC′, dCo
≤4s
dlog(|A|T/δ)
mk+1
mkminn√
dC′, dCo
. (mk≥dlog(|A|/δ))
Taking a union bound over kandb∈ Akfinishes the proof.
Lemma E.2. Leta⋆= argmaxa∈Aa⊤θ⋆. Then with probability at least 1−2δ,a⋆∈ Akfor all k.
Proof. Suppose that the high-probability event in Lemma E.1 holds. For any k, ifa⋆∈ Ak, then for
anyb∈ Ak,
b⊤bθk−a⋆⊤bθk≤b⊤θ⋆−a⋆⊤θ⋆+b⊤(bθk−θ⋆)+a⋆⊤(bθk−θ⋆)
≤0 + 2
4s
dlog(|A|T/δ)
mk+min{√
dC′, dC}
mk
.
By the definition of Ak+1in Eq. (1), we have a⋆∈ Ak+1. The lemma is then proven by an induction
argument.
Proof of Theorem 4.1. We first calculate the regret in epoch k > 1assuming that the event in
Lemma E.2 holds.
X
t∈Ik
max
a∈Aa⊤θ⋆−a⊤
tθ⋆
≤X
t∈Ik
max
a∈Aa⊤bθk−1−a⊤
tbθk−1
+ 2mkmax
a∈Aka⊤
bθk−1−θ⋆
≤mk· O s
dlog(|A|T/δ)
mk−1+min{√
dC′, dC}
mk−1!
=Op
dmklog(|A|T/δ) + min {√
dC′, dC}
.
Summing this over kand using that m1=dlog(|A|T/δ), we get
TX
t=1
max
a∈Aa⊤θ⋆−a⊤
tθ⋆
≤ Op
dTlog(|A|T/δ) +dlog(|A|T/δ) + min {√
dC′, dC}logT
.
Notice that without loss of generality we can assume dlog(|A|T/δ)≤T(otherwise the right-hand
side is vacuous). Using this fact gives the desired bound.
From Exercise 27.6 in Lattimore and Szepesvári (2020), the ϵ-covering number of Ais bounded by 6d
ϵd. LetC(A, ϵ)be the ϵ-net of A, we then haveC(A,6d
T)≤Td. Thus, when |A| ≥ Td, we can
useC(A,6d
T)asA1in Algorithm 1 to conduct phase elimination. In that case, following above proof,
we have
TX
t=1 
max
a∈C(A,6d
T)a⊤θ⋆−a⊤
tθ⋆!
≤ O s
dTlogC
A,6d
TT/δ
+ min {√
dC′, dC}logT!
≤ O
dp
Tlog(T/δ) + min {√
dC′, dC}logT
.
19From the definition of covering number, there exists a a⋆
c∈ C(A,6d
T)such that
max
a∈Aa⊤θ⋆−(a⋆
c)⊤θ⋆≤6d
T.
We have
TX
t=1 
max
a∈Aa⊤θ⋆−max
a∈C(A,6d
T)a⊤θ⋆!
≤TX
t=1
max
a∈Aa⊤θ⋆−(a⋆
c)⊤θ⋆
+TX
t=1 
(a⋆
c)⊤θ⋆−max
a∈C(A,6d
T)a⊤θ⋆!
≤6d.
Thus,
TX
t=1
max
a∈Aa⊤θ⋆−a⊤
tθ⋆
≤ O
dp
Tlog(T/δ) + min {√
dC′, dC}logT
.
F Proof of Theorem 5.1
In this section, we use the following notation:
bγbγbγt=
01
2bθt
1
2bθ⊤
t 0
, D DDt=
αBt−αBt−10
0 0
.
Algorithm 2 is equivalent to the FTRL update:
HHHt= argmax
HHH∈H(*
HHH,t−1X
s=1bγbγbγs+tX
s=1DDDs+
−G(HHH)
η)
. (14)
Algorithm 4 is equivalent to
HHHt= argmax
HHH∈H(t−1X
s=1⟨HHH,bγbγbγs+DDDs⟩ −G(HHH)
η)
. (15)
By the standard analysis for FTRL algorithms (e.g., Theorem 2 in Zimmert and Lattimore (2022)),
the regret bounds of Eq. (14) and Eq. (15) are given by the following lemmas, respectively.
Lemma F.1. The update rule Eq. (14) (Algorithm 2) ensures for any UUU∈ H,
TX
t=1⟨UUU−HHHt,bγbγbγt⟩
≤G(UUU)−minHHH∈HG(HHH)
η−TX
t=1⟨UUU−HHHt,DDDt⟩+TX
t=1max
HHH∈H
⟨HHH−HHHt,bγbγbγt⟩ −DG(HHH,HHHt)
η
.
Lemma F.2. The update rule Eq. (15) (Algorithm 4) ensures for any UUU∈ H,
TX
t=1⟨UUU−HHHt,bγbγbγt⟩
≤G(UUU)−minHHH∈HG(HHH)
η−TX
t=1⟨UUU−HHHt,DDDt⟩+TX
t=1max
HHH∈H
⟨HHH−HHHt,bγbγbγt+DDDt⟩ −DG(HHH,HHHt)
η
.
We consider an arbitrary comparator p⋆∈∆(A)withu⋆=Ea∼p⋆[a]. Define p= (1−γ)p⋆+γρ.
We have p∈∆γ(A), and define u=Ea∼p[a]andUUU=dCov(p). The regret with respect to p⋆can be
20decomposed as the following: With probability at least 1−δ,
RegT(p⋆) (16)
=TX
t=1⟨u⋆−at, θt⟩
=TX
t=1⟨u−at, θt⟩+ 2γT
≤TX
t=1⟨u−xt, θt⟩+Op
Tlog(1/δ)
+ 2γT (Azuma’s inequality)
=TX
t=1D
u−xt, θt−Et[bθt]E
| {z }
Bias+TX
t=1D
u−xt,Et[bθt]−bθtE
| {z }
Deviation+TX
t=1⟨UUU−HHHt,bγbγbγt⟩
| {z }
FTRL+Op
Tlog(1/δ) +γT
.
(17)
By Lemma F.1, the FTRL term can further be bounded by
FTRL ≤G(UUU)−minHHH∈HG(HHH)
η| {z }
Penalty−TX
t=1⟨UUU−HHHt,DDDt⟩
| {z }
Bonus+TX
t=1max
HHH∈H⟨HHH−HHHt,bγbγbγt⟩ −DG(HHH,HHHt)
η
| {z }
Stability.
(18)
In the following five lemmas, we bound the five terms Bias,Deviation ,Penalty ,Bonus , and Stability .
Lemma F.3.
Bias≤C∞max
t∥u∥Σ−1
t+√
dC∞.
Proof.
Et
u−xt,−Σ−1
tatϵt(at)
≤Etq
(u−xt)⊤Σ−1
tata⊤
tϵ2
t(at)Σ−1
t(u−xt)
≤q
(u−xt)⊤Σ−1
tEt
ata⊤
tϵ2
t(at)
Σ−1
t(u−xt)
≤ϵt∥u−xt∥Σ−1
t
≤ϵt∥xt∥Σ−1
t+ϵt∥u∥Σ−1
t
≤√
dϵt+ϵt∥u∥Σ−1
t. (Σt⪰xtx⊤
t)
Thus,
Bias=TX
t=1
u−xt, θt−Et
Σ−1
tata⊤
tθt
| {z }
=0+Et"TX
t=1
u−xt,−Σ−1
tatϵt(at)#
≤C∞max
t∥u∥Σ−1
t+√
dC∞.
Lemma F.4. With probability of at least 1−δ, we have
Deviation ≤max
t∥u∥Σ−1
t 
12p
Tlog(T/δ) +12√
dlog(T/δ)√γ!
+ 12p
dTlog(T/δ) +12dlog(T/δ)√γ
21Proof. Notice that
⟨u−xt,bθt⟩≤(u−xt)⊤Σ−1
tat
≤ ∥u−xt∥Σ−1
t∥at∥Σ−1
t
≤√
d√γ∥u−xt∥Σ−1
t.
By the strengthened Freedman’s inequality (Lemma M.3), with probability at least 1−δ,
Deviation =TX
t=1D
u−xt,Et[bθt]−bθtE
≤3vuutTX
t=1EtD
u−xt,bθtE2
log(d4T4/δ) + 2·√
d√γmax
t∥u−xt∥Σ−1
tlog(d4T4/δ)
≤max
t∥u−xt∥Σ−1
t 
12p
Tlog(T/δ) +12√
dlog(T/δ)√γ!
≤max
t∥u∥Σ−1
t 
12p
Tlog(T/δ) +12√
dlog(T/δ)√γ!
+ 12p
dTlog(T/δ) +12dlog(T/δ)√γ.
Lemma F.5.
Penalty ≤(d+ 1) log( T)
η.
Proof. Define HHH0=Ea∼ρ
aa⊤a
a⊤1
. By the definition of the feasible set H, for any HHH∈ H ,
HHH⪰γHHH0=d+1
THHH0andHHH⪯(d+ 1)HHH0. Thus, Penalty can be upper bounded by
G(UUU)−minHHH∈HG(HHH)
η≤G d+1
THHH0
−G((d+ 1)HHH0)
η=1
ηlog 
det (( d+ 1)HHH0)
det d+1
THHH0!
=(d+ 1) log( T)
η.
Lemma F.6.
Bonus ≤3αdlog(T)−αmax
t∥u∥2
Σ−1
t.
Proof. Given Ea∼p0[aa⊤]⪰Ea∼p0[a]Ea∼p0[a]⊤=uu⊤, we have
TX
t=1⟨UUU,DDDt⟩=
Ea∼p0[aa⊤], αB T
≥
uu⊤, αB T
=α∥u∥2
BT.
Recall that B1= Σ−1
1and for t≥2,
Σ−1
t=B1
2
t−1 dX
i=1λtivtiv⊤
ti!
B1
2
t−1, ({vti}d
i=1are unit eigenvectors)
Bt−1=B1
2
t−1 dX
i=1vtiv⊤
ti!
B1
2
t−1, (Pd
i=1vtiv⊤
ti=I)
Bt=B1
2
t−1 dX
i=1max{λti,1}vtiv⊤
ti!
B1
2
t−1, (19)
22which ensures Bt⪰Bt−1andBt⪰Σ−1
t. By induction, it leads to BT⪰Σ−1
tfor any t. This
implies
∥u∥2
BT≥max
t∥u∥2
Σ−1
t.
Thus,PT
t=1⟨UUU,DDDt⟩ ≥αmax t∥u∥2
Σ−1
t.
Next, we upper boundPT
t=1⟨HHHt,DDDt⟩. First, notice that ⟨HHH1,DDD1⟩=αTr(ΣtBt) =Tr(I) =αd.
Fort≥2,
⟨HHHt,DDDt⟩=αTr(Σt(Bt−Bt−1))
=αTr
B−1
2
t−1 dX
i=1λtivtiv⊤
ti!−1
B−1
2
t−1B1
2
t−1 dX
i=1max{λti−1,0}vtiv⊤
ti!
B1
2
t−1

(by Eq. (19))
=αTr
 dX
i=1λtivtiv⊤
ti!−1 dX
i=1max{λti−1,0}vtiv⊤
ti!

=αdX
i=1max
1−1
λti,0
≤αdX
i=1max{logλti,0}.
We also have
log det ( Bt)−log det ( Bt−1)
= log
det
B1
2
t−1
detPd
i=1max{λti,1}vtiv⊤
ti
det
B1
2
t−1
det
B1
2
t−1
detPd
i=1vtiv⊤
ti
det
B1
2
t−1

= log
detPd
i=1max{λti,1}vtiv⊤
ti
detPd
i=1vtiv⊤
ti

=dX
i=1max{logλti,0}.
Thus,
TX
t=1⟨HHHt,DDDt⟩ ≤αd+αlog det ( BT)−αlog det ( B1)≤αd+αlog det ( BT). (20)
Finally, we bound log det ( BT). Since Σt=P
apt(a)aa⊤⪰γP
aρ(a)aa⊤, by Theorem 3 of
Bubeck et al. (2012), we have Σt⪰γ
dIandΣ−1
t⪯d
γIfor all t. Thus, B1= Σ−1
1⪯d
γI. Below, we
use induction to show that Bt⪯td
γI. Assume Bt−1⪯(t−1)d
γI. Then,
Bt=B1
2
t−1 dX
i=1max{λti,1}vtiv⊤
ti!
B1
2
t−1
⪯B1
2
t−1 dX
i=1(λti+ 1)vtiv⊤
ti!
B1
2
t−1
= Σ−1
t+Bt−1⪯d
γI+(t−1)d
γI=td
γI.
23By induction, we get BT⪯Td
γIandlog det ( BT)≤2dlog(T)by setting γ=d√
T. Overall, by
Eq. (20), we have
TX
t=1⟨HHHt,DDDt⟩ ≤3αdlog(T).
Combining the upper bound forPT
t=1⟨HHHt,DDDt⟩and the lower bound forPT
t=1⟨UUU,DDDt⟩finishes the
proof.
Lemma F.7. With probability at least 1−δ
Stability ≤ O
dηT+ηdlog(1/δ)
γ
.
Proof. For any p, define µ(p) =Ea∼p[a]and
Cov(p) =Ea∼p[(a−µ(p))(a−µ(p))⊤],dCov(p) =Ea∼p
Cov(p) +µ(p)µ(p)⊤µ(p)
µ(p)⊤1
.
For any HHH=
H+hh⊤h
h⊤1
, given HHHt=
Cov(pt) +xtx⊤
txt
x⊤
t 1
, we have
⟨HHH−HHHt,bγbγbγt⟩ −DG(HHH,HHHt)
2η≤ ⟨HHH−HHHt,bγbγbγt⟩ −∥xt−h∥2
Cov(pt)−1
2η(Lemma M.1)
=D
h−xt,bθtE
−∥xt−h∥2
Cov(pt)−1
2η
≤η∥bθt∥2
Cov(pt) (AM-GM)
=ηr2
ta⊤
tΣ−1
tCov(pt)Σ−1
tat
≤η∥at∥2
Σ−1
t. (|rt| ≤1and Cov (pt)⪯Σt)
By Freedman’s inequality, since Eth
∥at∥2
Σ−1
ti
=d, and η∥at∥2
Σ−1
t≤ηd
γ, with probability at least
1−δ, we have
ηTX
t=1∥at∥2
Σ−1
t≤ O
dηT+ηdlog(1/δ)
γ
.
Proof of Theorem 5.1. Using Lemma F.3–Lemma F.7 in Eq. (17) and Eq. (18), we get
RegT
≤ Odlog(T)
η+ηdT+αdlog(T) +p
dTlog(T/δ) +dlog(T/δ)√γ+ηdlog(1/δ)
γ+√
dC∞+γT
+ max
t∥u∥Σ−1
t 
12p
Tlog(T/δ) +12√
dlog(T/δ)√γ+C∞!
−αmax
t∥u∥2
Σ−1
t
≤ Odlog(T)
η+ηdT+αdlog(T) +p
dTlog(T/δ) +dlog(T/δ)√γ+ηdlog(1/δ)
γ
+(C∞)2
α+Tlog(T/δ)
α+dlog2(T/δ)
γα+√
dC∞+γT
. (AM-GM)
Therefore, the choice γ=d√
T, α= max
C∞√
dlog(T),√
T
andη=q
log(T)
Tgives
RegT≤ O
d√
Tlog(T/δ) +C∞p
dlog(T)
.
24Algorithm 4: FTRL with log-determinant barrier regularizer
1Parameters :α= max
C∞√
dlog(T),√
T
,η= minn√
log(T)
16C∞,q
log(T)
To
, and γ=d√
T.
2Letρ∈∆(A)be John’s exploration over A, and let ∆γ(A) =n
p:p= (1−γ)p′+γρ, p′∈∆(A)o
.
3Define feasible set H=n
dCov(p) :p∈∆γ(A)o
.
4Define G(HHH) =−log det( HHH)andB0= 0.
5fort= 1,2, . . .do
6 Compute
HHHt= argmax
HHH∈H{η⟨HHH,ΘΘΘt−1⟩ −G(HHH)}where ΘΘΘt−1=
αBt−11
2Pt−1
s=1bθs
1
2Pt−1
s=1bθ⊤
s 0
,
pt∈∆γ(A)be such that HHHt=dCov(pt),
Σt=X
a∈Apt(a)aa⊤,
Bt=BONUS (Bt−1,Σt). (defined in Figure 1a)
7 Sample at∼pt. Observe reward rtwithE[rt] =a⊤
tθt+ϵt(at).
8 Construct reward estimator bθt= Σ−1
tatrt.
9end
G Computationally Efficient Algorithm for Adversarial C∞Bound
Most proof is the same as Appendix F. Namely, we follow Eq. (17) in Appendix F together with a
different decomposition
RegT(p⋆)≤TX
t=1D
u−xt, θt−Et[bθt]E
| {z }
Bias+TX
t=1D
u−xt,Et[bθt]−bθtE
| {z }
Deviation+TX
t=1⟨UUU−HHHt,bγbγbγt⟩
| {z }
FTRL+Op
Tlog(1/δ) +γT
.
(21)
By Lemma F.2, we can further bound FTRL by
FTRL ≤G(UUU)−minHHH∈HG(HHH)
η| {z }
Penalty+TX
t=1⟨UUU−HHHt,−DDDt⟩
| {z }
Bonus
+TX
t=1max
HHH∈H
⟨HHH−HHHt,bγbγbγt⟩ −DG(HHH,HHHt)
2η
| {z }
Stability-1+TX
t=1max
HHH∈H
⟨HHHt−HHH,−DDDt⟩ −DG(HHH,HHHt)
2η
| {z }
Stability-2.
(22)
Among the terms above, Bias,Penalty ,Deviation ,Bonus , and Stability-1 follow the same bounds as
Lemma F.3, Lemma F.5, Lemma F.6, and Lemma F.7, respectively. It remains to bound Stability-2 .
Lemma G.1. Ifη≤1
16√
dα, then
Stability-2 ≤8ηα2d.
Proof. From the analysis of bias term and HHHtandDDDtare both positive semi-definite, we have
p
Tr(HHHtDDDtHHHtDDDt) =αp
Tr(Σt(Bt−Bt−1) Σt(Bt−Bt−1))≤α√
d
25where the last inequality is due to Σ−1
t⪰Bt−Bt−1. Since η≤1
16√
dα, by Lemma M.2, with
probability of at least 1−δ, we have
Stability-2 ≤8ηTX
t=1Tr(HHHtDDDtHHHtDDDt)
≤8ηα2TX
t=1Tr(Σt(Bt−Bt−1) Σt(Bt−Bt−1))
≤8ηα2d
where the last step follows the similar analysis in Lemma F.6.
Proof of Theorem 5.1 (Option II). Using Lemma F.3–Lemma G.1 in Eq. (21) and Eq. (22), we get
RegT=Odlog(T)
η+ηdT+dαlog(T) +p
dTlog(T/δ) +dlog(T/δ)√γ+ηdlog(1/δ)
γ+ηα2d
+(C∞)2
α+Tlog(T/δ)
α+dlog2(T/δ)
γα+√
dC∞+γT
By choosing α= max
C∞√
dlog(T),√
T
andη= minn√
log(T)
16C∞,q
log(T)
To
, and γ=d√
T, we
could ensure η≤1
16√
dα. This gives the final regret O
d√
Tlog(T/δ) +dC∞√logT
. The
additional√
dfactor comes from the additional condition for the Stability-2 term.
H Proof of Theorem 5.2
Similar to before, we define
bγbγbγt=
01
2bθt
1
2bθ⊤
t 0
, D DDt=
αBt−αBt−10
0 0
,
andxt=Ea∼pt[a],˜xt=Ea∼˜pt[a]. We perform the regret decomposition as the following.
RegT=TX
t=1⟨u−at, θt⟩
=TX
t=1⟨u−xt, θt⟩+TX
t=1⟨xt−˜xt, θt⟩+TX
t=1⟨˜xt−at, θt⟩
=TX
t=1⟨u−xt, θt⟩+O 
γT+p
Tlog(1/δ)
=TX
t=1D
u−xt, θt−Et[bθt]E
| {z }
Bias+TX
t=1D
u−xt,Et[bθt]−bθtE
| {z }
Deviation+TX
t=1D
u−xt,bθtE
| {z }
FTRL+O 
γT+p
Tlog(1/δ)
.
(23)
26TheFTRL term can be further bounded as the following.
FTRL
=TX
t=1Ea∼pthD
u−a,bθtEi
=TX
t=1EHHH∼qt[⟨UUU−HHH,bγbγbγt⟩]
≤d2logT
η+1
ηTX
t=1EHHH∼qt[exp ( η⟨HHH,bγbγbγt⟩)−η⟨HHH,bγbγbγt⟩ −1] +TX
t=1EHHH∼qt[⟨HHH−UUU,DDDt⟩]
(by Theorem I.2)
=d2logT
η+1
ηTX
t=1Ea∼pth
exp
ηD
a,bθtE
−ηD
a,bθtE
−1i
| {z }
Stability+αTX
t=1Ea∼pth
∥a∥2
Bt−Bt−1i
−αTX
t=1∥u∥2
BT
| {z }
Bonus.
(24)
In the following four lemmas, we bound the four terms Bias,Deviation ,Bonus ,Stability .
Lemma H.1.
Bias≤
max
t∥xt−u∥eΣ−1
tp
dγT+ 2p
Tlog(1/δ) +√
dβC
.
Proof.
Bias=TX
t=1D
u−xt, θt−Et[bθt]E
=TX
t=1D
u−xt, θt−eΣ−1
tEt
ata⊤
t
θt+eΣ−1
tEt[atϵt(at)]E
=γTX
t=1D
u−xt,eΣ−1
tθtE
+TX
t=1D
u−xt,−eΣ−1
tEt[atϵt(at)]E
(eΣt=γI+Et[ata⊤
t])
≤γTX
t=1∥xt−u∥eΣ−1
t∥θt∥eΣ−1
t+TX
t=1∥xt−u∥eΣ−1
tEth
∥at∥eΣ−1
t|ϵt(at)|i
≤
max
t∥xt−u∥eΣ−1
t 
γs
d
γT+TX
t=1Eth
∥at∥eΣ−1
t|ϵt(at)|i!
(eΣt⪰γIand∥θt∥2≤√
d)
≤
max
t∥xt−u∥eΣ−1
t 
p
dγT+√
dβTX
t=1Et[|ϵt(at)|]!
≤
max
t∥xt−u∥eΣ−1
t 
p
dγT+ 2p
Tlog(1/δ) +√
dβTX
t=1|ϵt(at)|!
.
(Azuma’s inequality)
Lemma H.2.
Deviation ≤ O
max
t∥u−xt∥eΣ−1
tdβ√
Tlog(T/δ)
.
27Proof. Notice that
⟨u−xt,bθt⟩≤(u−xt)⊤eΣ−1
tat
≤ ∥u−xt∥eΣ−1
t∥at∥eΣ−1
t
≤√
dβ∥u−xt∥eΣ−1
t.
By the strengthened Freedman’s inequality (Lemma M.3), with probability at least 1−δ,
Deviation =TX
t=1D
u−xt,Et[bθt]−bθtE
≤ O
3vuutTX
t=1EtD
u−xt,bθtE2
log(Td/δ) + 2√
dβmax
t∥u−xt∥eΣ−1
tlog(Td/δ)

≤ O
max
t∥u−xt∥eΣ−1
tdβ√
Tlog(T/δ)
. (using the assumption d≤T)
Lemma H.3.
Bonus ≤3αdlog(T)−αmax
t∥u∥2
eΣ−1
t.
Proof. The proof the same as in the logdet case. See the proof of Lemma F.6.
Lemma H.4.
Stability ≤ O(ηdTlog2T).
Proof.
Stability =1
ηTX
t=1Ea∼pth
exp
ηD
a,bθtE
−ηD
a,bθtE
−1i
Since q′
tis a log-concave distribution, so are qtandpt, which further implies that ηD
a,bθtE
follows a
log-concave distribution. Furthermore,
Ea∼pt
η2D
a,bθtE2
≤Ea∼pth
η2a⊤
teΣ−1
taa⊤eΣ−1
tati
≤2η2∥at∥2
eΣ−1
t≤2η2dβ2≤1
100,
where we use Lemma J.2 in the second-last inequality . By Lemma 6 of Ito et al. (2020), we have
1
ηTX
t=1Ea∼pth
exp
ηD
a,bθtE
−ηD
a,bθtE
−1i
≤ηTX
t=1Ea∼ptD
a,bθtE2
≤2ηTX
t=1∥at∥2
eΣ−1
t≤2ηβ2dT.
Proof of Theorem 5.2. Combining Eq. (23), Eq. (24), and Lemma H.1, Lemma H.2, Lemma H.3,
Lemma H.4, we see that the regret is bounded by
eOd2
η+ηdT+ max
t∥u−xt∥eΣ−1
t(d√
T+√
dC) +dC+αd
−α∥u∥2
BT
≤eOd2
η+ηdT+αd
+d2T+dC2
α(AM-GM inequality)
Choosing optimal αandηleads to eO(√
d3T+dC).
28I Dimension Reduction for Continuous Exponential Weights
First, the intrinsic dimension of Xcan be defined as the following:
Definition 1. The intrinsic dimension of Xis defined as
dim(X) = dim (span ( X − X )),
where X − X ≜{x−x′:x, x′∈ X} .
A convex region X ⊂Rncan be translated and rotated so that it entirely lies in Rmwhere m=
dim(X)and has non-zero volume in Rm. We more precisely define this transformation below.
Definition 2. LetX ⊂Rnbe a convex region with dim(X) =m. We define ϕ:Rn→Rmas the
following linear transformation:
ϕ(x)≜ZMx,
where M∈Rn×nis a rotation matrix (i.e., orthogonal matrix) such that for any v∈ X − X ,Mv
has non-zero elements only in the first mcoordinates (this is always possible by the definition of
dim(X)in Definition 1), and
Z=
1 0 ··· 0 0 ··· 0
0 1 ··· 0 0 ··· 0
...............···...
0 0 ··· 1 0 ··· 0
∈Rm×n
extracts the first mcoordinates of a given n-dimensional vector.
Lemma I.1. For any x∈ X and any θ∈Rn,
⟨x, θ⟩=⟨ϕ(x), ϕ(θ)⟩+f(ϕ, θ),
where f(ϕ, θ)∈Ris some quantity that only depends on ϕandθbut not x.
Proof. Letx, x′∈ X. By the definition of ϕ, we have
⟨ϕ(x)−ϕ(x′), ϕ(θ)⟩=⟨ZM(x−x′), ZMθ ⟩.
By the choice of Min Definition 2, M(x−x′)only has non-zero elements in the first mcoordinates.
Furthermore, since Zextracts the first mcoordinates, we have
⟨ZM(x−x′), ZMθ ⟩=mX
i=1(M(x−x′))i(Mθ)i
=nX
i=1(M(x−x′))i(Mθ)i
=⟨M(x−x′), Mθ⟩
=⟨x−x′, θ⟩. (M⊤=M−1because Mis a rotation matrix)
Thus,
⟨x, θ⟩ − ⟨ϕ(x), ϕ(θ)⟩=⟨x′, θ⟩ − ⟨ϕ(x′), ϕ(θ)⟩,
meaning that the value of ⟨x, θ⟩ − ⟨ϕ(x), ϕ(θ)⟩is shared by all x∈ X. Defining this value as f(ϕ, θ)
finishes the proof.
We consider the continuous exponential weight algorithm (Algorithm 5) running on ϕ(X)⊂Rm:
29Algorithm 5: Exponential Weights
1LetX ⊂Rn, and let ϕ(X)≜{ϕ(x) :x∈ X} .
2fort= 1,2, . . .do
3 Define for y∈ϕ(X),
wt(y) = exp 
ηt−1X
s=1⟨y, ϕ(θs)⟩+ηtX
s=1⟨y, ϕ(bs)⟩!
andpt(y) =wt(y)R
y′∈ϕ(X)wt(y′)dy′
for some bonus term bt.
4 Sample yt∼pt, and play xt=ϕ−1(yt), where ϕ−1is the inverse mapping of ϕ.
5 Receive θt∈Rn.
In Algorithm 5, we require that the inverse mapping of ϕexists. This is true because for any x, x′∈ X,
we have ∥ϕ(x)−ϕ(x′)∥=∥ZM(x−x′)∥=∥M(x−x′)∥=∥x−x′∥, and thus ϕcannot map
x, x′∈ X withx̸=x′to the same point.
Theorem I.2. Letqt∈∆(X)be the distribution such that x∼qtis equivalent to first drawing
y∼ptand then taking x=ϕ−1(y). Algorithm 5 ensures for any x∈ X,
TX
t=1⟨x, θt+bt⟩ −TX
t=1Ex∼qt[⟨x, θt+bt⟩]≤mlogT
η+1
ηTX
t=1Ex∼qt[exp ( η⟨x, θt⟩)−η⟨x, θt⟩ −1].
Proof. Note that Algorithm 5 is a standard continuous exponential weight algorithm over reward
vectors ϕ(θt)and in the space of ϕ(X)⊂Rm. By the standard analysis (see, e.g., Ito et al. (2020);
Zimmert and Lattimore (2022)), we have for any sequence λ1, . . . , λ T∈Rand any y∈ϕ(X),
TX
t=1⟨y, ϕ(θt) +ϕ(bt)⟩ −TX
t=1Ey∼pt[⟨y, ϕ(θt) +ϕ(bt)⟩]
≤mlogT
η+1
ηTX
t=1Ey∼pt
exp 
η⟨y, ϕ(θt)⟩+λt
− 
η⟨y, ϕ(θt)⟩+λt
−1
.
By Lemma I.1, the above implies
TX
t=1
ϕ−1(y), θt+bt
−TX
t=1Ey∼pt
ϕ−1(y), θt+bt
≤mlogT
η+1
ηTX
t=1Ey∼pth
exp
η
ϕ−1(y), θt
−ηf(ϕ, θt) +λt
−
η
ϕ−1(y), θt
−ηf(ϕ, θt) +λt
−1i
,
which further implies that for any x∈ X,
TX
t=1⟨x, θt+bt⟩ −TX
t=1Ex∼qt[⟨x, θt+bt⟩]≤mlogT
η+1
ηTX
t=1Ex∼qt[exp ( η⟨x, θt⟩)−η⟨x, θt⟩ −1]
by the definition of qtand by letting λt=ηf(ϕ, θt).
J Computationally Efficient Algorithm for Adversarial CBound
In this section, we present Algorithm 6, a polynomial-time algorithm that ensures eO(d3√
T+d5
2C)
regret. The algorithm is based on the continuous exponential weight algorithm in the original feature
space (Ito et al., 2020; Zimmert and Lattimore, 2022), with the bonus construction similar to Lee
et al. (2020).
30J.1 Preliminaries for Entropic Barrier
Entropic barrier For any convex body A, the family of exponential distribution is
pw(x) =exp(w⊤x) 1{y∈ A}R
Aexp(w⊤y)dy.
For any x∈ A, there is a unique w(x)such that Ey∼pw(x)[y] =x. The entropic barrier F(x)is the
negative entropy of pw(x). Namely
F(x) =Z
pw(x)(y) log 
pw(x)(y)
dy
We have ∇F(x) =w(x)and∇2F(x) =Ey∼pw(x)
(y−x)(y−x)⊤
. We know that F(x)is a
d-self-concordant barrier on A.
The equivalence of mean-oriented FTRL and continuous exponential weights Consider FTRL
with entropic barrier as the regularizer that solves xtfor round t∈[T]following
xt+1= argmax
x∈A(*
x,tX
s=1θs+
−F(x)
ηt)
.
This is equivalent to
∇F(xt+1) =ηttX
s=1θs.
Given that Ey∼pw(xt+1)[y] =xt+1and∇F(xt+1) = w(xt+1), playing xt+1yields the same
expected reward as playing according to distribution pw(xt+1)where w(xt+1) =ηtPt
s=1θs. Thus,
we have pw(xt+1)(x)∝exp
ηtD
x,Pt
s=1θsE
forx∈ A.
J.2 Auxiliary Lemmas
Lemma J.1 (Lemma 1 of Ito et al. (2020)) .Ifxfollows a log-concave distribution poverRdand
Ex∼p[xx⊤]⪯I, we have
Pr
∥x∥2
2≥dβ2
≤dexp(1−β).
for arbitrary β >0.
Lemma J.2. With the choice of β≥4 log(10 dT), we have
|Ea∼pt[f(a)]−Ea∼˜pt[f(a)]| ≤10dexp(−β)≤1
2T2
for any f:A → [−1,1]and
3
4Ea∼pt[aa⊤]⪯Ea∼˜pt[aa⊤]⪯4
3Ea∼pt[aa⊤].
Proof. The proof follows that of Lemma 4 of Ito et al. (2020), with the observation that ptis a
log-concave distribution.
Lemma J.3 (Lemma 14 of Zimmert and Lattimore (2022)) .Letfbe aν-self-concordant barrier for
A ⊂Rd. Then for any u, x∈ A,
∥u−x∥∇2f(x)≤ −γ′⟨u−x,∇f(x)⟩+ 4γ′ν+ 2√ν
where γ′=8
3√
3+73
2
6√
3ν(γ′∈[1,4]forν≥1).
Minkowsky Functions. The Minkowsky function of a convex boday Awith the pole at w∈int(A)
is a function πw:A →Rdefined as
πw(u) = inf
t >0w+u−w
t∈ A
. (25)
31Algorithm 6: Continuous exponential weights (for adversarial Cbound)
1LetA ⊂Rdbe a convex body and Fbe its entropic barrier.
2Parameters :γ=log(T/δ)
T,α=˜Θ(√
dC+d√
T),η= minn
1
160√
d3T,1
32√
dαo
.
3fort= 1,2, . . . , T do
4 Define wt(a) = exp 
ηPt−1
s=1⟨a,bθs−bs⟩
and
pt(a) =wt(a) R
y∈Awt(y)dy
,˜pt(a) =pt(a) 1{∥a∥Σ−1
t≤√
dβ}
R
a′∈Apt(a′) 1{∥a′∥Σ−1
t≤√
dβ}da′,
where Σt=Ea∼pt[aa⊤].
5 Playat∼˜pt, and observe reward rtwithE[rt] =a⊤
tθt+ϵt(at).
6 Construct reward estimator bθt=eΣ−1
tatrt, where eΣt=γI+Ea∼˜pt[aa⊤].
7 Define BBBt=I+eΣ−1
t −eΣ−1
txt
−x⊤
teΣ−1
tx⊤
teΣ−1
txt
, where xt=Ea∼pt[a].
8 ifλmax(BBBt−P
τ∈IBBBs)>0then
9 I ← I ∪ { t}.
10 bt=−α∇F(xt)where xt=Ea∼pt[a]and∇F(xt) =ηPt−1
s=1(bθs−bs).
11 elsebt= 0.
Lemma J.4 (Proposition 2.3.2 in Nesterov and Nemirovskii (1994)) .Letfbe aν-self-concordant
barrier on A ⊆Rd, and u, w∈int(A). Then
f(u)−f(w)≤νlog1
1−πw(u)
.
J.3 Regret Analysis
We perform regret decomposition. For regret comparator u⋆∈ A, define x⋆= min x∈AF(x)and
u= (1−1
T)u⋆+1
Tx⋆. With probability at least 1−δ,
RegT=TX
t=1⟨u⋆−at, θt⟩
=TX
t=1⟨u−at, θt⟩+1
TTX
t=1⟨u⋆−x⋆, θt⟩
=TX
t=1⟨u−˜xt, θt⟩+Op
Tlog(1/δ)
+ 2
(define ˜xt=Ea∼˜pt[a]and by Azuma’s inequality)
=TX
t=1⟨u−xt, θt⟩+TX
t=1⟨xt−˜xt, θt⟩+Op
Tlog(1/δ)
=TX
t=1D
u−xt, θt−Et[bθt]E
| {z }
Bias+TX
t=1D
u−xt,Et[bθt]−bθtE
| {z }
Deviation+TX
t=1D
u−xt,bθt+btE
| {z }
FTRL
−TX
t=1⟨u−xt, bt⟩
| {z }
Bonus+γT+Op
Tlog(1/δ)
. (26)
32By standard FTRL analysis, we have
FTRL ≤F(u)−minx∈AF(x)
η| {z }
Penalty+E"TX
t=1max
x∈AD
x−xt,bθt+btE
−1
ηDF(x, xt)#
| {z }
Stability.(27)
The individual terms Bias,Deviation ,Bonus ,Penalty ,Stability terms are bounded in Lemma J.6,
Lemma J.7, Lemma J.9, Lemma J.10, Lemma J.12.
Lemma J.5. For any t∈[T], ifa∼pt, then with probability of at least 1−δ,
∥a∥Σ−1
t≤√
dlog3d
δ
.
Proof. Define y= Σ−1
2
ta. Then Ey
yy⊤
= Σ−1
2
tEa∼pt[aa⊤]Σ−1
2
t=I. Since ptis a log-
concave distribution, and log-concavity is preserved under liner transformation, yis also log-concave.
Applying Lemma J.1 on it leads to
Prh
∥a∥2
Σ−1
t≥dβ2i
=Pr
∥y∥2
2≥dβ2
≤dexp(1−β)≤3dexp(−β).
Setting δ= 3dexp(−β), we conclude that with probability at least 1−δ,∥a∥2
Σ−1
t≤dlog 3d
δ2.
Lemma J.6. With probability at least 1− O(δ),
Bias≤
max
t∥xt−u∥eΣ−1
tp
dγT+ 2p
Tlog(1/δ) +√
dβC
.
Proof. The proof is the same as that of Lemma H.1.
Lemma J.7.
Deviation ≤ O
max
t∥u−xt∥eΣ−1
tdβ√
Tlog(T/δ)
.
Proof. The proof is the same as that of Lemma H.2.
Lemma J.8.
|I| ≤ dlog24T
γ
.
Proof. Our proof is similar to Lemma B.12 in Lee et al. (2020). Let {t1,···, tn+1}be the rounds
such that bt̸= 0. Define AAAi=Pi
j=1BBBtj. For any i >1, since λmax(BBBti−AAAi−1)>0, there exists
a vector y∈Rd+1such that y⊤BBBtiy > y⊤AAAi−1y. Thus, y⊤AAAiy≥2y⊤AAAi−1y. Let z=AAA1
2
i−1y,
we have z⊤AAA−1
2
i−1AAAiAAA−1
2
i−1z≥2∥z∥2
2. This implies λmax
AAA−1
2
i−1AAAiAAA−1
2
i−1
≥2. Moreover, we have
λmin
AAA−1
2
i−1AAAiAAA−1
2
i−1
≥1because
AAA−1
2
i−1AAAiAAA−1
2
i−1=AAA−1
2
i−1(AAAi−1+BBBti)AAA−1
2
i−1⪰I.
Thus,det(AAAi)
det(AAAi−1)= det
AAA−1
2
i−1AAAiAAA−1
2
i−1
≥2. By induction, we have det (AAAn+1)≥2ndet (AAA1). We
now give a upper bound fordet(AAAn+1)
det(AAA1). Define aaa=
a
1
. By AM-GM inequality, we have
det 
AAAn+1AAA−1
1
= det
n+1X
j=1BBBtjBBB−1
t1
≤
1
dTr
n+1X
j=1BBBtjBBB−1
t1

d
.
33Notice that for any t,BBBt⪰IandTr(BBBt) =Tr(I) +Tr(eΣ−1
t) +∥xt∥2
eΣ−1
t≤2(d+1)
γ. Thus, we can
upper bound the last expression further by

1
dTr
n+1X
j=1BBBtj

d
≤2(d+ 1)( n+ 1)
dγd
≤4T
γd
.
Overall, we have 2n≤det(AAAn+1)
det(AAA1)≤(4T/γ)d, and thus n≤dlog2(4T/γ).
Lemma J.9.
Bonus ≤ −α
8max
t∥u−xt∥eΣ−1
t+O 
αd2logT
.
Proof. Letρ= max t∥u−xt∥eΣ−1
tandt∗= argmaxt∥u−xt∥eΣ−1
t, We discuss two conditions:
• Ift∗∈ I, then ρ2≤P
τ∈I∥u−xt∥2
eΣ−1
τ.
• Ift∗/∈ I, then BBBt⋆⪯P
τ∈IBBBτ. Letuuu≜
u
1
. This implies
ρ2=∥u−xt∗∥2
eΣ−1
t∗=∥uuu∥2
BBBt⋆≤X
τ∈I∥uuu∥2
BBBτ=X
τ∈I∥u−xτ∥2
eΣ−1
τ,
where we use the definitions of BBBtanduuuin the second and the last equality.
Thus, max t∥u−xt∥eΣ−1
t≤P
τ∈I∥u−xτ∥eΣ−1
τ.
TX
t=1⟨xt−u, bt⟩
=X
τ∈I⟨xτ−u, bτ⟩
=αX
τ∈I⟨xτ−u,−∇F(xτ)⟩
≤ −α
γ′X
τ∈I∥u−xτ∥∇2F(xτ)+ 4αd|I|+2α√
d|I|
γ′
(Lemma J.3 and Fisd-self-concordant barrier)
≤ −α
2γ′X
τ∈I∥u−xτ∥eΣ−1
τ+O(αd2logT) (∇2F(xτ) = Σ−1
τ⪰1
4eΣ−1
τand Lemma J.8)
≤ −α
2γ′max
t∥u−xt∥eΣ−1
t+O(αd2logT).
Lemma J.10.
Penalty ≤dlog(T)
η.
Proof. Since x⋆= min x∈AF(x)andπx⋆(u)≤1−1
Tfrom Eq. (25). We have from Lemma J.4
Penalty =F(u)−F(x⋆)
η≤dlog(T)
η.
34Lemma J.11 (Lemma 17 in Zimmert and Lattimore (2022)) .LetFbe the entropic barrier and
∥w∥∇2F(xt)−1≤1
16η, then
max
x∈A
⟨x−xt, w⟩ −1
ηDF(x, xt)
≤2η∥w∥2
∇2F(xt)−1.
Lemma J.12. With probability at least 1−δ,
Stability ≤ O 
ηβ2dT+ηα2d2logT
.
Proof. Since Fis ad-self-concordant barrier (Chewi, 2023), we have
∥bt∥∇2F(xt)−1=α∥∇F(xt)∥∇2F(xt)−1≤α√
d.
By Lemma J.2, we have eΣ−1
t⪯ 
Ea∼˜pt[aa⊤]−1⪯2Σ−1
t, and thus
∥bθt∥2
∇2F(xt)−1=∥eΣ−1
tatrt∥2
Σt≤2a⊤
teΣ−1
tat≤2dβ2.
Thus,∥bθt+bt∥∇2F(xt)−1≤β√
2d+α√
d. Ifη≤1
16(β√
2d+α√
d), by Lemma J.11, we have
Stability ≤2ηTX
t=1∥bθt+bt∥2
∇2F(xt)−1
≤4ηTX
t=1∥bθt∥2
∇2F(xt)−1+ 4ηX
τ∈I∥bτ∥2
∇2F(xτ)−1
≤ O 
ηβ2dT+ηα2d|I|
≤ O 
ηβ2dT+ηα2d2logT
Theorem J.13. Algorithm 6 ensures with probability at least 1−δ,RegT=eO 
d3√
T+d5
2C
,
whereeO(·)hides polylog (T/δ)factors.
Proof. Putting Lemma J.6, Lemma J.7, Lemma J.9, Lemma J.10, Lemma J.12 into Eq. (26) and
Eq. (27), with η≤1
16(β√
2d+α√
d)andγ=1
T, we have with probability at least 1− O(δ),
Reg≤max
t∥u−xt∥eΣ−1
t
eO(d√
T+√
dC)−α
8
+eO
αd2+d
η+ηα2d2+ηdT+√
T
.
By settingα
8=˜Θ(d√
T+√
dC), we have
Reg≤eO
d3√
T+d5
2C+d
η+ηd4T+ηd3C2
.
Setting η=1
160d√
T+32α√
d, we get
Reg≤eO
d3√
T+d5
2C
.
K Gap-dependent Misspecification
We consider the same setting as Liu et al. (2023a), but remove an assumption for it. Consider bandit
learning with general reward function f0where for any action xt∈ X ⊂ Rdat round t, the learner
get reward yt=f0(xt) +ηtwhere ηts are zero mean, σ-sub-Gaussian noise. We assume there exists
a linear function θ⊤xthat could approximate f0(x)in the following manner.
35Definition 3.
sup
x∈Xθ⊤x−f0(x)
f⋆
0−f0(x)≤ρ
where f⋆
0= max
x∈Xf0(x)and0≤ρ <1.
The algorithm in Liu et al. (2023a) only gets O√
T
regret when ρ≤1
d√logTand we improve it to
ρ≤1√
dby using elimination-based methods in Algorithm 7. For any design πon action set A, define
G(π) =X
a∈Aπ(a)aa⊤g(π) = max
a∈A∥a∥2
G(π)−1
Algorithm 7: Phased Elimination for Misspecification
1Input: Action set A1=A. Initialize m1=⌈64dlog log dlog
|A|
δ
⌉+ 16 .
2forℓ= 1,2,···, Ldo
3 Find the approximate G-optimal design πℓonAℓwithg(π)≤2dand
|Supp(π)| ≤4dlog log d+ 16
4 Compute uℓ(a) =⌈mℓπℓ(a)⌉anduℓ=P
a∈Aℓuℓ(a)
5 Take each action a∈ Aℓexactly u(a)times with reward y(a).
6 Calculate
bθℓ=G−1
ℓX
a∈Aℓu(a)ay(a)where Gℓ=X
a∈Aℓu(a)aa⊤
Update active action set
Aℓ+1=(
a∈Aℓ: max
b∈Aℓ⟨bθℓ, b⟩ − ⟨bθℓ, a⟩ ≤s
4d
mℓlog|A|
δ
+1
2ℓ)
mℓ+1←4mℓ
Define Gap(x) =f⋆
0−f0(x)as the suboptimal gap at point x. Definition 3 implies the true value
function f0(x) =θ⊤x+ ∆( x)where |∆(x)| ≤ρ(f⋆
0−f0(x)) =ρGap(x). We further assume that
|∆(x)| ≤ρGap(x)which captures both standard uniform misspecification and the gap-dependent
misspecification. With this assumption, our main result is summarized in Theorem K.1.
Theorem K.1. For action a, assume y(a) =f0(a) +ηawhere ηais zero-mean sub-gaussian noise
andf0(a) =θ⊤a+ ∆( a)with|∆(a)| ≤ρGap(a). Ifρ≤1
64√
d, with probability of at least 1−δ,
we have
RegM⋆
T≤ Op
dTlog|A|/δ
Proof. First, with probability of at least 1−δ, for any ℓandb∈ Aℓ, we have
⟨b,bθℓ−θ⟩=b⊤G−1
ℓX
a∈Aℓu(a)ay(a)−b⊤θ
=b⊤G−1
ℓX
a∈Aℓu(a)aa⊤ηa+b⊤G−1
ℓX
a∈Aℓu(a)a∆(a)
≤s
4d
mℓlog|A|
δ
+b⊤G−1
ℓX
a∈Aℓu(a)a∆(a)
where in the last step, we use standard concentration by Equation (20.2) of Lattimore and Szepesvári
(2020) and the apply union bound for all actions.
36For the last term, we have
b⊤G−1
ℓX
a∈Aℓu(a)a≤max
c∈Aℓ∆(c)·vuut X
a∈Aℓu(a)!
b⊤G−1
ℓX
a∈Aℓu(a)aa⊤Σ−1
ℓb
(Cauchy-Schwarz)
= max
c∈Aℓ∆(c)·q
u∥b∥2
G−1
ℓ≤max
c∈Aℓ∆(c)·r
2du
mℓ≤max
c∈Aℓ∆(c)·2√
d.
Thus, for any b∈ Aℓ,
⟨b,bθℓ−θ⟩≤s
4d
mℓlog|A|
δ
+ 2√
dmax
a∈Aℓ∆(a) =s
4d
mℓlog|A|
δ
+ 2√
dρmax
a∈AℓGap(a)
When ℓ= 1, since m1=⌈256dlog log dlog
|A|
δ
⌉+16, we haver
4d
m1log
|A|
δ
≤1
24. Moreover,
by trivial bound, max a∈A1Gap(a)≤2anda⋆∈ A 1.
We will jointly do two inductions. Assume for round ℓ, we have a⋆∈ Aℓandmax a∈AℓGap(a)≤
1
2ℓ−2. We first show a⋆∈ A ℓ+1. Thus, for any b∈ A ℓ, given ρ≤1
64√
d, since mℓ= 4ℓ−1m1, we
have
⟨b,bθℓ−θ⟩≤s
4d
mℓlog|A|
δ
+ 2√
dρmax
a∈AℓGap(a)≤1
2ℓ−11
24+1
2ℓ+3=1
2ℓ+2
From the induction hypothesis, let baℓ= arg max b∈Aℓ⟨bθℓ, b⟩we have
bθ⊤
ℓbaℓ−bθ⊤
ℓa⋆≤θ⊤baℓ−θ⊤a⋆+bθ⊤
ℓbaℓ−θ⊤baℓ|{z}
≤1
2ℓ+2+θ⊤a⋆−bθ⊤
ℓa⋆
|{z}
≤1
2ℓ+2
≤f0(baℓ)−f0(a⋆)|{z }
≤0+|∆(baℓ)|+1
2ℓ+1
≤ρGap(baℓ) +1
2ℓ+1≤1
2ℓ
Forℓ+ 1, the remaining actions a∈ Aℓ+1satisfy
max
b∈Aℓ⟨bθℓ, b⟩ − ⟨bθℓ, a⟩ ≤s
4d
mℓlog|A|
δ
+1
2ℓ≤1
2ℓ+3+1
2ℓ
This implies a⋆∈ Aℓ+1. Moreover, since a⋆∈ Aℓ, fora∈ Aℓ+1, we have
Gap(a) =f⋆
0−f0(a) =θ⊤a⋆−θ⊤a+|∆(a)|
≤bθ⊤
ℓa⋆−bθ⊤
ℓa+ρGap(a) + (θ−bθℓ)⊤a⋆+ (bθℓ−θ)⊤a
≤bθ⊤
ℓa⋆−bθ⊤
ℓa+ρGap(a) + (θ−bθℓ)⊤a⋆+ (bθℓ−θ)⊤a
≤bθ⊤
ℓa⋆−bθ⊤
ℓbaℓ|{z}
≤0givena⋆∈Aℓ+bθ⊤
ℓbaℓ−bθ⊤
ℓa|{z}
≤1
2ℓ+3+1
2ℓ+ρGap(a) +1
2ℓ+1
Given ρ≤1
64√
d≤1
64, this implies
Gap(a)≤1
1−ρ(1
2ℓ+1
2ℓ+1+1
2ℓ+3)≤63
64(1
2ℓ+1
2ℓ+1+1
2ℓ+3)≤1
2ℓ−1
The above arguments show that as ℓincreases, max a∈AℓGap(a)will shrink by1
2at every step. Since
fora∈ Aℓ, Gap (a)≤1
2ℓ−1=Or
4d
mℓlog
|A|
δ
37Finally, given L= log( T), we have
Reg=LX
ℓ=1X
a∈Aℓuℓ(a)Gap(a)≤LX
ℓ=1mℓs
4d
mℓlog|A|
δ
≤ Op
dTlog|A|/δ
When |A| ≥ Td, we can apply similar covering number arguments as in Appendix E, replacing A1
with a1
T-net of A. Combined with Theorem K.1, this yields the result in Theorem 6.2.
Using the hard instance for ϵ-misspecified linear bandits setting in Lattimore et al. (2020), we now
show that ρ=˜Ω(1√
d)for an algorithm to achieve sub-linear regret, proving the above algorithm is
optimal in terms of ρassumption.
Theorem K.2. Ifρ≥q
8 log(3 T)
d−1then there exists an instance that RT= Ω(ρT).
Proof. Using Theorem F.5 in Lattimore et al. (2020), there exist a discrete time-invariant action space
{ai∈Rd}3T
i=1that satisfies these two conditions:
1.∥ai∥= 1∀i
2.⟨ai, aj⟩ ≤q
8 log(3 T)
d−1∀i̸=j
and let θ∗=q
d−1
8 log(3 T)ϵai∗for some i∗, and let misspecification at each round for all non-optimal
arms be ϵto make the true expected reward zero. Defining τ:= max( t|is̸=i∗∀s≤t), we have
E[RT]≥q
d−1
8 log(3 T)ϵE[τ]. Since the observed rewards are independent of ai∗before time τ, and i∗
is chosen randomly, we have E[τ]≥min{T,3T−1
2}. So,
E[RT]≥ϵTs
d−1
8 log(3 T)
Finally, we have ρ≥ϵq
d−1
8 log(3 T)ϵ−0=q
8 log(3 T)
d−1, so choosing ϵ= min(q
8 log(3 T)
d−1,q
d−1
8 log(3 T))
completes the proof showing linear regret when ρis large enough.
L General Reduction from Corruption-Robust Algorithms to
Misspecification
In this section, we extend the results of Section 6 to the reinforcement learning setting. We consider
episodic MDPs, denoted by a tuple M= (S,A,{Ph}H
h=1,{rh}H
h=1, s1)forSthe set of states, A
the set of actions, Ph:S × A → △ Sthe transition kernel, rh:S × A → △ [0,1]the reward, and
s1the starting state. We assume each episode starts in state s1, where the agent takes action a1,
transitions to s2∼P1(· |s1, a1)and receives reward r1∼r1(s1, a1). This proceeds for Hsteps at
which point the episode terminates and the process resets. We assume thatPH
h=1rh∈[0,1]almost
surely (note that the linear bandit setting with rewards in [-1,1] can be incorporated into this with a
simple rescaling).
We let πdenote a policy, πh:S → △ S, a mapping from states to actions. We denote the value of
a policy πon MDP MasVM,π
0 :=EM,π[PH
h=1rh]. We assume access to some function class
F ⊆ {S × A → R}. In the MDP setting, we define regret on MDP Mas:
RegM
T:=T·sup
πVM,π
0−TX
t=1VM,πt
0 .
In the MDP setting, we consider the following notion of misspecification.
38Definition 4 (Misspecification) .For our environment of interest M⋆, there exists some environment
M0such that, for each fh+1∈ F,π, and (s, a, h ), we have:
EM⋆,π[rh+fh+1(sh+1, ah+1)|sh=s, ah=a]
−EM0,π[rh+fh+1(sh+1, ah+1)|sh=s, ah=a]≤ϵmis
h(s, a)
and
∃fh∈ F s.t.fh(s, a) =EM0[rh+ max
a′fh+1(sh+1, a′)|sh=s, ah=a]
for some ϵmis
h(s, a)>0.
We make the following assumption on gap-dependent misspecification.
Assumption 4 (Gap-Dependent Misspecification) .For any policy π, we have
EM⋆,π"HX
h=1ϵmis
h(sh, ah)#
≤ρ·∆(π)
for some ρ≥[0,1).
We are interested in relating the above misspecification setting to the corruption-robust setting. In the
MDP setting, we allow both the reward and transitions to be corrupted. For some MDP M, define
the corruption at episode tand step has:
ϵt,h(st
h, at
h) := sup
g∈{S×A→ [0,H]}|(Thg− Th
bg)(st
h, at
h)|
where
Thg(s, a) :=EM[rh+ max
a′g(sh+1, a′)|sh=s, ah=a]
denotes the Bellman operator, and Th
bdenotes the corrupted Bellman operator, i.e. Th
bdenotes the
expected reward and next state under the corrupted reward and transition distribution. We denote the
total corruption level as
C:=TX
t=1HX
h=1ϵt,h(st
h, at
h).
Note that this definition of corruption encompasses both bandits and RL with function approximation.
Now assume we have access to the following oracle.
Assumption 5. We have access to a regret minimization algorithm which takes as input Fand some
C′and with probability at least 1−δhas regret bounded on M0as
RegM0
T≤ C1(δ, T)√
T+C2(δ, T)C′(28)
ifC′≥C, and by HT otherwise, for Cas defined above and for (problem-dependent) constants
C1(δ, T),C2(δ, T)which may scale at most logarithmically with Tand1
δ.
Before stating our main reduction from corruption-robust to gap-dependent misspecification, we
require the following assumption.
Assumption 6. For any π, we have that there exists some f∈ F such that for all (s, a, h ),
QM0,π
h(s, a) =fh(s, a).
We then have the following result.
Theorem L.1. Assume our environment satisfies Assumption 4. Then under Assumption 5 and
Assumption 6, as long asρC2(δ
T,T)
1−ρ≤1/2, with probability at least 1−2δwe can achieve regret
bounded as:
RegM⋆
T≤3
1−ρ· C1(δ
T, T)√
T+2
1−ρ·
Hp
2Tlog(1/δ) +H
.
39Proof of Theorem L.1. First, note that by Assumption 4, we can bound
TX
t=1EM⋆,πt"HX
h=1ϵmis
h(sh, ah)#
≤TX
t=1ρ·∆(πt)≤ρ·RegT
where we abbreviate RegT:=RegM⋆
T. Furthermore, note that under Assumption 4 interacting with
M⋆is equivalent to interacting with M0but where the rewards and transitions are corrupted up to
levelϵmis
h(s, a)at(s, a, h ).
Relating Regret on M0toM⋆.Define the regret on M0as
RegM0
T:=T·sup
πEM0,π"HX
h=1rh#
−TX
t=1EM0,πt"HX
h=1rh#
.
Under Assumption 4, we have that EM⋆,π⋆hPH
h=1ϵmis
h(sh, ah)i
= 0. Lemma L.2 then implies that
EM⋆,π⋆"HX
h=1rh#
=EM0,π⋆"HX
h=1rh#
and so
EM⋆,π⋆"HX
h=1rh#
≤sup
πEM0,π"HX
h=1rh#
.
Furthermore, Lemma L.2 also implies
EM0,πt"HX
h=1rh#
−EM⋆,πt"HX
h=1rh#≤EM⋆,πt"HX
h=1ϵmis
h(sh, ah)#
.
Putting these together we can bound
RegT≤RegM0
T+TX
t=1EM⋆,πt"HX
h=1ϵmis
h(sh, ah)#
≤RegM0
T+ρ·RegT,
where the last inequality holds by Assumption 4. Rearranging this gives
RegT≤1
1−ρ·RegM0
T.
Bounding the Regret. Consider running the algorithm of Assumption 5 on M⋆and assume we
run with parameter C′←βwhich we will choose shortly. From the above observation, this is
equivalent to running on M0with corruption level ϵmis
h(s, a)at(s, a, h ). Then by Assumption 5,
with probability at least 1−δwe have regret on M0bounded as
RegM0
T≤ C1(δ, T)√
T+C2(δ, T)β
ifβ≥PT
t=1PH
h=1ϵmis
h(st
h, at
h), and by HT otherwise. Furthermore, by the above argument this
then immediately implies a regret bound on RegT.
LetE1,tdenote the event that {β≥Pt
t′=1PH
h=1ϵmis
h(st′
h, at′
h)}. LetE2denote the event that for all
t≤T, we have
Regt≤1
1−ρ·
C1(δ
T, T)√
t+C2(δ
T, T)β
+TH
1−ρ·I{Ec
1,t},
and note that by the above and under Assumption 5 we then have that E2occurs with probability
at least 1−δ. For simplicity, for the remainder of the proof we abbreviate C1:=C1(δ
T, T)and
C2:=C2(δ
T, T).
40Note that ϵh(st
h, at
h)∈[0, H]by construction. It follows that, with probability at least 1−δ, via
Azuma-Hoeffding,
TX
t=1HX
h=1ϵmis
h(st
h, at
h)≤TX
t=1Eπt"HX
h=1ϵmis
h(st
h, at
h)#
+Hp
2Tlog 1/δ≤ρ·RegT+Hp
2Tlog 1/δ.
Denote this event as E3.
Now consider choosing
β=
1−ρC2
1−ρ−1
·ρ
1−ρ· C1√
T+Hp
2Tlog 1/δ+H
so that
β=ρ
1−ρ·
C1√
T+C2β
+Hp
2Tlog 1/δ+H.
OnE2∩ E3, assume that
β < ρ ·RegT+Hp
2Tlog 1/δ. (29)
Lett⋆denote the minimum time such that
t⋆X
t=1ρ∆(πt) +Hp
2Tlog 1/δ > β andt⋆−1X
t=1ρ∆(πt) +Hp
2Tlog 1/δ≤β,
and note that such a time is guaranteed to exist under (29) and since β≥Hp
2Tlog 1/δ+H
by construction so ρ∆(π1) +Hp
2Tlog 1/δ≤H+Hp
2Tlog 1/δ≤β. Furthermore, since
∆(π)≤H, we have here thatPt⋆−1
t=1ρ∆(πt)> β−ρH−Hp
2Tlog 1/δ. We then have
Regt⋆−1=t⋆−1X
t=1∆(πt)
>β
ρ−H−H
ρp
2Tlog 1/δ
=1
1−ρ·
C1√
T+C2β
≥1
1−ρ· 
C1√
t⋆−1 +C2β
.
However, since by assumptionPt⋆−1
t=1ρ∆(πt) +Hp
2Tlog 1/δ≤β, onE3E1,t⋆−1holds so on
E2∩ E3we have that
Regt⋆−1≤1
1−ρ· 
C1√
t⋆−1 +C2β
.
This contradicts the above. Therefore, on E2∩ E3we must have that β≥ρ·RegT+Hp
2Tlog 1/δ,
soE1,Tholds on E3, and so on E2∩ E3,
RegT≤1
1−ρ·
C1√
T+C2β
.
From our setting of βwe can bound this as
≤1
1−ρ· C1√
T+1
1−ρ· C2·
1−ρC2
1−ρ−1
·ρ
1−ρ· C1√
T+Hp
2Tlog 1/δ+H
.
The result follows from some simplification.
Lemma L.2. For MDPs M⋆,M0satisfying Definition 4, under Assumption 6 we have
VM0,π
0−VM⋆,π
0≤EM⋆,π"HX
h=1ϵmis
h(sh, ah)#
.
41Proof. Letr′denote the reward function on M0, and note that under Assumption 6 we have that
there exists f∈ F such that VM0,π
h(s) =fh(s, πh(s))for all π, s, h . Then Lemma E.15 of Dann
et al. (2017) gives that
VM0,π
0−VM⋆,π
0 =EM⋆,πHX
h=1(r′
h−rh)
+HX
h=1EM0,π[VM0,π
h(sh+1)|sh]−EM⋆,π[VM0,π
h(sh+1)|sh]
.
By Definition 4, we can bound this as
≤EM⋆,π"HX
h=1ϵmis
h(sh, ah)#
.
Proof of Corollary 6.2.1. First, note that under Assumption 3, we have that Assumption 4 holds for
Fthe set of functions linear in ϕ,F={ϕ(s, a)⊤w:w∈Rds.t.ϕ(s, a)⊤w∈[0, H],∀s, a},
andϵmis
h(s, a)of Assumption 4 set to Hϵmis
h(s, a)forϵmis
h(s, a)of Assumption 3. To see this, let
M0be the MDP with transitions ⟨ϕ(s, a),µh(·)⟩, and note that this the immediately implies linear
realizability on M0(and furthermore that Assumption 6 holds). Furthermore, since the total reward
is at most H, it is easy to see that under Assumption 3, we can take ϵmis
h(s, a)←Hϵmis
h(s, a).
Next, note that Theorem 4.2 of Ye et al. (2023) gives an algorithm on M0satisfying Assumption 5
withC1=eO(√
H2d3)andC2=eO(Hd)(assuming thatPH
h=1rh∈[0,1]almost surely). We can
then apply Theorem L.1 to obtain the result.
M Auxiliary Lemmas
Lemma M.1 (Lemma 16 of Zimmert and Lattimore (2022)) .LetXXX=
X+xx⊤x
x⊤1
andYYY=

Y+yy⊤y
y⊤1
. Then
DG(XXX,YYY) =DG(X, Y) +∥x−y∥2
Y−1≥ ∥x−y∥2
Y−1.
Lemma M.2 (Lemma 34 of Liu et al. (2023b)) .LetGbe the log-determinant barrier. For any
matrix DDD, ifp
Tr(HHHtDDDHHHtDDD)≤1
16η, then
max
HHH∈H⟨HHH−HHHt,DDD⟩ −DG(HHH,HHHt)
η≤8ηTr(HHHtDDDHHHtDDD).
Lemma M.3 (Strengthened Freedman’s inequality (Theorem 9 of Zimmert and Lattimore (2022))) .
LetX1, X2, . . . , X Tbe a martingale difference sequence with a filtration F1⊆ F 2⊆ ··· such that
E[Xt|Ft] = 0 andE[|Xt| | F t]<∞almost surely. Then with probability at least 1−δ,
TX
t=1Xt≤3s
VTlog2 max{UT,√VT}
δ
+ 2UTlog2 max{UT,√VT}
δ
,
where VT=PT
t=1E[X2
t| Ft]andUT= max {1,max t∈[T]|Xt|}.
42NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
•[NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a
proper justification is given (e.g., "error bars are not reported because it would be too computationally
expensive" or "we were unable to find the license for the dataset we used"). In general, answering
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
•Delete this instruction block, but keep the section heading “NeurIPS paper checklist" ,
•Keep the checklist subsection headings, questions/answers and guidelines below.
•Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes] .
Justification: The main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope. The claims are validated by detailed proofs.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes] .
43Justification: The paper discuss the limitations of the work when introducing the algorithms.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes] .
Justification: The paper provides detailed assumptions and proofs.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA] .
Justification: This is a theoretical paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
44•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
45•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA] .
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA] .
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA] .
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
46•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes] .
Justification: The research conducted in the paper conforms, in every respect, with the
NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This is a theoretical work. There is no societal impact of the work performed.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA] .
Justification: The paper poses no such risks.
Guidelines:
47• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA] .
Justification: This paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA] .
Justification: This paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA] .
48Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA] .
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
49