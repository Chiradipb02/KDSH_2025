Least Squares Regression Can Exhibit
Under-Parameterized Double Descent
Xinyue Li
Applied Math, Yale University
xinyue.li.xl728@yale.eduRishi Sonthalia
Math, Boston College
rishi.sonthalia@bc.edu
Abstract
The relationship between the number of training data points, the number of parame-
ters, and the generalization capabilities of models has been widely studied. Previous
work has shown that double descent can occur in the over-parameterized regime
and that the standard bias-variance trade-off holds in the under-parameterized
regime. These works provide multiple reasons for the existence of the peak. We
postulate that the location of the peak depends on the technical properties of both
the spectrum as well as the eigenvectors of the sample covariance. We present two
simple examples that provably exhibit double descent in the under-parameterized
regime and do not seem to occur for reasons provided in prior work.
1 Introduction
This paper demonstrates interesting new phenomena that suggest that our understanding of the
relationship between the number of data points, the number of parameters, and the generalization
error is incomplete, even for simple linear models. The classical bias-variance theory postulates that
the generalization risk versus the number of parameters for a fixed number of training data points is
U-shaped (Figure 1a1). However, modern machine learning has shown that if we keep increasing
the number of parameters, the generalization error eventually starts decreasing again [2, 3] (Figure
1b2). This second descent has been termed as double descent and occurs in the over-parameterized
regime , which is when the number of parameters exceeds the number of data points. Understanding
the location and the cause of such peaks in the generalization error is of significant importance.
(a) Classical Bias Variance Trade-off.
 (b) Modern Double Descent.
Figure 1: Bias-variance trade-off and double descent.
Many different theories have been postulated for the appearance of the peak. The prevalent theory
is that when the model is under-parameterized, the learning is constrained. This constraint on the
1Image source [1]
2Image source [3]
38th Conference on Neural Information Processing Systems (NeurIPS 2024).learning results in increased variance until the interpolation point. After this point, there exists a high
dimensional space of solutions, and learning methods, such as gradient descent, pick solutions that
generalize well. This conjecture has been empirically validated for deep neural networks. Due to
the challenges of analyzing deep neural networks, theoretical understanding of this phenomenon has
focused on linear models - linear regression [4–13] and kernelized regression [14–22]. These works
show that there exists a peak at the boundary between the under and over-parameterized regimes .
Hence validating the above postulated theory for their setting. Careful theoretical analysis then shows
that the generalization error can be decomposed into various terms, one of which is the norm of the
estimator. Specifically, it has been shown that the curve for the norm of the estimator versus the
dimension of the data also exhibits double descent, with the peak occurring at the same point as the
peak in the generalization error curve. In most cases, this is the only term in the decomposition that
exhibits double descent. This leads to a second theory for the occurrence of the peak, that is, the peak
in the generalization error for linear models occurs due to the norm of the estimator blowing up.
Contributions. Since understanding the reasons that peaks occur is of critical importance, it is
crucial that we have a robust theory for their appearance. However, most work focuses on the over-
parameterized regime and ignores the under-parameterized regime. This is because it is commonly
believed that the variance is monotonic in the under-parameterized regime. We show that this is not
true and present two simple examples that exhibit double descent in the under-parameterized regime.
•Why does the Peak Occur . We argue that the location of the peak depends on two factors: the
alignment between the targets yand singular vectors Vof the training data matrix and the spectrum
of the data. We show that by modifying these quantities appropriately, we can move the peak into
the underparameterized regime.
•Modifying the Alignment . For the first example, we consider a spiked covariate model, where one
eigendirection dominates, and the regression target only depends on the dominant eigendirection.
For this model, we consider the ridge regularized problem with ridge parameter µ2and show that
the ridge parameter µ2controls the alignment between the targets yand the singular vectors V. We
show that for µ >0the peak occurs in the under-parameterized regime (Theorem 2). Specifically,
when the ratio of the dimension to the number of training data points cis equal to (1 + µ2)−1
(c:=d/n= 1/(1 +µ2)).
•Modifying the Spectrum For the second example, we consider training data that is a mixture
of isotropic Gaussian vectors and vectors from along a fixed direction z. By varying the mixture
proportions ( π1, π2), we can modify the spectrum of the covariance matrix. We show that the
expected risk displays under-parameterized double descent (Theorem 4), with the peak occurring
when c:=d/n=π1).
•Norm of the estimator . We further analyze the first example and show that if we fix the number
of training data points nand vary the dimension dof the problem, then for large values of µ, the
risk curve does not display a double descent. However, the curve for the norm of the estimator
does display descent. Thus, the peak in the norm of the estimator does not imply a peak in the
generaliation error.
Organization. The rest of the paper is organized as follows. Section 2 presents a quick overview
of prior work on double descent for linear models. Section 3 highlights two less-studied properties
that influence the location of the peak. Section 4 presents the first of the two examples of under-
parameterized double descent. This model also shows that a double descent in the norm of the
estimator does not translate to a double descent in the risk. Finally, Section 5 presents the second
example of under-parameterized double descent.
2 Prior Work on Double Descent
In this section, we present the current prevailing theories for the occurrence of local maximums in the
risk curve. Concretely, consider the following simple linear model that is a special case of the general
models studied in [5, 8, 11, 23] amongst many other works. Let xi∼ N(0, Id)and let β∈Rdbe a
linear model with ∥β∥= 1. Letyi=βTxi+ξiwhere ξ∼ N(0,1). Then, let βoptbe the minimum
norm solution to arg min ˜β∥βTXtrn−˜βTXtrn+ξtrn∥, where ξtrn∈Rn×1. One important quantity
is the aspect ratio of X. Specifically, for a d×nmatrix, the aspect ratio is c:=d/n. With this
terminology, we see that a model is under-parameterized if c <1and over-parameterized when c >1.
2Table 1: Table showing various assumptions on the data and the location of the double descent peak
for linear regression and denoising. We only present a subset of references for each problem setting.
For the low rank setting in this paper, see Appendix F.
Noise Ridge Reg. Dim. Peak Location Reference
Input Yes 1 Under-parameterized This paper.
Output No Full Under-parameterized This paper
Input No Low Interpolation point [24, 37]
Input Yes Full Interpolation point [38]
Output No Full Over-parameterized/interpolation point [5, 8, 11]
Output Yes Full Over-parameterized/interpolation point [11, 23]
Output No Low Over-parameterized/interpolation point [39]
Output Yes Low Over-parameterized/interpolation point [40]
Output No Low No peak [41]
Finally, the interpolation point, i.e., the point at which we can exactly fit the training data, is c= 1,
assuming we have full-rank data.
Then, the excess risk Rand the expected norm of βoptcan be expressed as follows:
R=(
c
1−cc <1
c−1
c+1
c−1c >1and βopt=(
1 +c
1−cc <1
1
c+1
c−1c >1.
In this model, there are a few important features that are ubiquitous in many prior double descent
studies for linear models:
1.The peak happens at c= 1, on the border between the under and over-parameterized
regimes.
2. Further, at c= 1the training error equals zero. Hence, this is the interpolation point.
3.The peak occurs due to the expected norm of the estimator βoptblowing up near the
interpolation point.
This is further validated by works that study ridge regularized regression [23–26]. Works such as
[23] have shown that optimally regularized regression no longer exhibits double descent. Further,
increasing the amount of regularization from zero to the optimal amount of regularization results in the
magnitude of the peak in the generalization getting smaller until a peak no longer exists. However, the
location of the peak does not change by changing the amount of regularization. Further, Chen, Min,
Belkin, and Karbasi [27] proved that double descent cannot take place in the under-parameterized
regime for the above model.
Subsequently, works such as [10, 23, 28–31] show that there can be multiple descents in the over-
parameterized regime. Specifically, d’Ascoli, Sagun, and Biroli [30] show that the first peak in triple
descent is due to the norm of the estimator peaking and that the second peak is due to the initialization
of the random features. Their results, Figure 3 in [30], show that the peaks only occur if the model is
over-parameterized. Further Chen, Min, Belkin, and Karbasi [27] show that by considering a variety
of product data distributions, any shaped risk curve can be observed in the over-parameterized regime.
Finally, Curth, Jeffares, and van der Schaar [32] says that the peak occurs at the point of effective
dimensionality of the model and not the true dimensionality. Here, we see that there are three other
reasons provided for the occurrence of peaks in the risk curve.
1.Regularization can reduce the effective dimensionality of the model and move the peak to
the right into the over-parameterized regime [32].
2.For random feature models, we see that the random initialization results in a second peak in
the over-parameterized regime [30].
3.Due to the data having a complex covariance structure, any shaped risk curve is possible in
the over-parameterized regime [27].
Other works [33–36] have considered the problem for other loss models and shown a variety of
different risk curves can exist. Table 1 summarizes some of the prior work.
3Double Descent with Input Noise. There has also been prior work that studies double descent for
models with input noise rather than output noise [24, 37, 38]. From these Sonthalia and Nadakuditi
[24] and Kausik, Srivastava, and Sonthalia [37] consider the unregularized problem and show that
the peak occurs at the boundary. Dhifallah and Lu [38] considers ridge regularization with isotropic
Gaussian data and again sees that the peak occurs at the boundary.
3 Spectral Properties of the Data Affect the Peak Location
In this section, we identify two important spectral properties that govern the location of the peak. In
later sections, we delve into two examples that modify these properties and move the peak into the
under-parameterized regime. We begin with definitions and notations. Throughout the paper, we
assume that training data X= [x1. . . x n]∈Rd×nandy= [y1. . . , y n]∈Rk×n. We are interested
in the standard ridge regularized least squares problem.
min
ˆβ∥y−ˆβTX∥2
F+µ2∥ˆβ∥2
In the unregularized case, the minimum norm solution is given by ˆβT=yX†, where X†is the
Moore-Penrose Pseudoinverse of X. Prior work on linear models has shown that a double descent in
the risk is due to a double descent in the norm of the estimator. Suppose X=UΣVTis the SVD,
ˆβ∈Rd×1, then using unitary invariance, we have that
∥ˆβ∥2=rank(X)X
i=1(yV)2
i
σ2
i
where σiis the ith singular value. Hence, this is controlled by
1. The alignment between yandV. Here Vare the eigenvectors of the data gram matrix.
2. The spectrum of the gram matrix XTX.
Many prior works deal with the alignment in one of two ways. If y=βTX+ξ, with ξhaving an
isotropic distribution, then prior works either assume that βhas an isotropic distribution [4, 5, 42] or
they assume that Xis isotropic Gaussian or that xi=ˇΣ1/2zi, where ziis from an isotropic Gaussian
[23, 43] and ˇΣis a deterministic matrix. For example, if βhas an isotropic distribution, taking the
expectation with respect to β, ξwe get that
Eβ,ξh
∥ˆβ∥2i
=E[β2
1]∥XX†∥2
F+E[ξ2
1]rank(X)X
i=11
σ2
i.
This quantity is then studied by looking at the distribution of the spectrum as d, n→ ∞ .
Definition 1. Given a random matrix A, ifλ1, . . . , λ kare its eigenvalues. Then the empirical
spectral distribution (ESD) is the following sum of Dirac delta measure
νk=1
kkX
i=1δλi
and the limiting spectral distribution νis a measure such that νk→νweakly almost surely.
In general, the limiting distribution νcdepends on the limiting aspect ratio (i.e., d/n →c).
Definition 2. Given a measure νcthat is supported on the interval J⊂R, the Stieljtes transform is
mνc(ζ) =Eλ∼νc1
λ−ζ
, ζ∈C\J.
One common assumption is that the limiting distribution of the empirical spectral distribution is the
Marchenko-Pastur distribution [44]. Other limiting distributions have been considered in [5, 45, 46].
For the Marchenko-Pastur distribution, it is known (see, for example, Lemma 5 in [24]) that
mνc(0) = Eλ∼νc1
λ
=(
c
1−cc <1
1
c−1c >1.
4Hence, the risk is governed by the value of the Stieljtes transform of the limiting spectrum at ζ= 0.
In particular, for the above example, the location of the peak of the risk as a function of cdepended
on the location of the peak of
c7→mνc(0) =: G(c).
Hence the risk depends on both the spectrum and the alignment between yandV. Thus, the peak
occurs at c= 1because of the following two conditions.
Alignment of yand right singular vectors of X
Assumption 1. IfX=UΣVTis the SVD, then yVis isotropic.
Stieljtes Transform Peak Assumption
Assumption 2. The function c7→mνc(0) =: G(c)has a local maximum at c= 1.
In this paper, we show that violating either one of the above two assumptions can move the peak from
the interpolation point into the under-parameterized regime.
4 Alignment Mismatch
In this section, we present the first example that exhibits double descent in the under-parameterized
regime. This model violates Assumption 1.
4.1 Model Assumptions
For any k≤d, letβ∈Rd×kbe fixed such that the operator norm ||βT||isΘ(1) . LetXtrn∈Rd×n
be the signal matrix and Atrn∈Rd×nbe the noise matrix. Then, the ridge regularized least square
estimator Woptis the minimum norm solution to
Wopt:= arg min
W∥βTXtrn−W(Xtrn+Atrn)∥2
F+µ2∥W∥2
F. (1)
Given test data Xtst+Atst, the mean squared generalization error is given by
R(Wopt) =EAtrn,Atst∥βTXtst−Wopt(Xtst+Atst)∥2
F
ntst
. (2)
Assumption 3. LetU ⊂Rdbe a one dimensional space with a unit basis vector u. Then let
Xtrn=σtrnuvT
trn∈Rd×nandXtst=σtstuvT
tst∈Rd×ntstbe the respective SVDs for the training
data and test data matrices. We further assume that σtrn=O(√n)andσtst=O(√ntst).
There are no assumptions on the distribution of vtrn, vtstbesides having unit norm. First, we see
that the data X+Ahas a spiked covariance, with the dominant eigendirection closely aligned with
u. Since the targets only depend on X, we consider Ato represent noise. Even with the rank 1
assumption, the model captures many different scenarios. If k= 1, then the problem is similar to
error-in-variables regression. If k=dandβ=I, then this is the supervised denoising problem. If
the columns of Xtrnare all ±uandβ=u, then this captures the binary classification problem (with
MSE loss) for two Gaussian clusters centered at uand−uwith labels ±1.
Assumptions about A.The analysis works for general assumptions in [24]. For simplicity, we
assume that the matrix Ahas I.I.D. entries drawn from a normalized Gaussian.
Assumption 4. The entries of the matrices A∈Rd×nare I.I.D. from N(0,1/d).
4.2 Expected Risk and Peak Location
We begin by providing a formula for the generalization error given by Equation 2 for the least squares
solution given by Equation 1. All proofs are in Appendix E.
5Theorem 1 (Generalization Error Formula) .Suppose the training data Xtrnand test data Xtst
satisfy Assumption 3 and the noise Atrn, Atstsatisfy Assumption 4. Let µbe the regularization
parameter. Then for the under-parameterized regime (i.e., c <1) for the solution Woptto Problem 1,
the generalization error or risk given by Equation 2 is given by
R(c, µ) =cσ2
trn(σ2
trn+ 1))
2dτ21 +c+µ2cp
(1−c+µ2c)2+ 4µ2c2−τ−2cσ2
trn(σ2
trn+ 1))
2d+τ−2σ2
tst
ntst+o1
d
,
where
1
τ=2∥βTu∥
2 +σ2
trn(1 +c+µ2c−p
(1−c+µ2c) + 4µ2c2).
Sketch. The proof proceeds in four key steps. First, we use the Sherman-Morrison formula for
pseudoinverses [47]. Next, we decompose the error into constituent dependent quadratic forms.
Through random matrix theory and concentration of measure arguments, we demonstrate that each
quadratic form concentrates around a deterministic value characterized by the Stieltjes transform
of the limiting empirical spectral distribution. Finally, we establish concentration bounds for the
products and sums of these dependent forms, yielding the desired error rate.
Since the focus is on the under-parameterized regime, Theorem 1 only presents the under-
parameterized case. The over-parameterized case can be found in Appendix E.3. Due to the
complexity of the expression, it is difficult to discern how the risk scales with respect to the training
data signal strength σ2
trn, the regularization strength µ, or the aspect ratio c. Since the focus of the
paper is the scaling with respect to c, we present the connection between the risk curve and cin the
main text. However, the shape of the risk curve with respect to the other parameters is also interesting
and can be found in Appendix D.
To understand the shape of the risk curve as cvaries, we first consider that data scaling regime . That
is, fix dand change n. The following theorem 2 shows that the risk curve is theoretically guaranteed
to have a peak at c=1
1+µ2.
Theorem 2 (Under-Parameterized Peak) .Letµ∈R>0,σ2
trn=n=d/candσ2
tst=ntst, and dis
sufficiently large, so that the error term o(1/d)is small, then the risk R(c)from Theorem 1, as a
function of c, has a local maximum in the under-parameterized regime at c=1
1+µ2.
Theorem 2 contrasts with prior works, in which double descent occurs in the over-parameterized
regime or on the boundary between the two regimes. We numerically verify the predictions from
Theorems 1 and 2. Figure 2 shows that the theoretically predicted risk matches the numerical risk,
thus verifying that double descent occurs in the under-parameterized regime.3
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.0010.0020.0030.0040.005Generalization ErrorUnder-Parameterized 
RegimeEmpirical
Theoretical
1
2+1
Peak
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.001100.001120.001140.001160.001180.00120Generalization Error<- Under-Parameterized 
     RegimeEmpirical
Theoretical
1
2+1
Peak
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.001050.001060.001070.001080.001090.001100.00111Generalization ErrorUnder 
Parameterized 
RegimeEmpirical
Theoretical
1
2+1
Peak
Figure 2: Figure showing the theoretical risk curve from Theorem 1 and empirical values in the
data scaling regime for different values of µ[(L)µ= 0.1, (C) µ= 1, (R) µ= 2]. Here σtrn=√n, σtst=√ntst, d= 1000 , ntst= 1000 . For each empirical point, we ran at least 100 trials. More
details can be found in Appendix G.
3All code for the experiments can be found at https://github.com/rsonthal/Under-Parameterized-Double-
Descent
64.3 The Peak Occurs Due to Alignment Mismatch
We now show that the peak occurs due to a mismatch between the target vector and the right singular
vectors of the input data. To begin, note that the ridge regularized problem can be written as follows
∥βT[Xtrn0d×d]|{z}
ˆXtrn−W([Xtrn0d×d] + [AtrnµI]|{z}
ˆAtrn])∥2
F.
In this expression, y=βTˆXtrn= (βTu)[vT
trn0p]. Hence the direction is given by ˆvT
trn= [vT
trn0p].
The right singular vectors of ˆXtrn+ˆAtrnare more difficult to compute,thus we use proxies. Since
ˆXtrnis rank 1, we use the right singular vectors of ˆAtrnas a proxy. Lemma 5 in the appendix, shows
that if A=UΣVT, then we can express ˆAtrnasˆUˆΣˆVT, where ˆU=U,ˆΣ2= Σ2+µ2I, and
ˆV=
V1:pΣˆΣ−1
µUˆΣ−1
∈Rn+d×d.Here V1:dare the first dcolumns of V. Then,
yˆV= (βTu)(ˆvT
trnV1:d)ΣˆΣ−1.
Since V1:dcame from a Gaussian random matrix, (ˆvT
trnV1:d)has isotropic entries. However, the
diagonal matrix ΣˆΣ−1results in the entries of yˆVnot being isotropic. Note when µ= 0,ΣˆΣ−1=I,
hence it is isotropic. Hence, µcontrols the deviation from isotropy.
We use ˆΣTˆΣIas a proxy for the spectrum of the sample covariance. By Lemma 5, we have that
ˆΣTˆΣ = ΣTΣ+µ2. We know that the limiting spectrum for ΣTΣis the Marchenko-Pastur distribution
for which the map G(c) =mνc(0)has a maximum for c= 1. Shifting the spectrum changes the
magnitude of the peak but does not change the location. This suggests that the peak occurs due to the
misalignment between the target vector and the right singular vectors of the input data.
Ablation experiment To verify that the location of the peak is due to the misalignment, we conduct
two experiments. First, we solve the unregularized problem. However, we change the spectrum of
the noise matrix Atrn. That is, instead of using Atrn=UΣVT, we use ˜A=U(Σ2+µ2I)1/2VT.
If the spectrum was the primary factor determining the location of the peak, the peak should occur at
c= 1/(1 +µ2). However, as seen in Figure 3a it still occurs at c= 1. Second, we replace ˆVwith a
uniformly random orthogonal matrix Q4. Clearly, yQis now isotropic. Figure 3b shows that, in this
case, the peak moves to the over-parameterized regime.
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
1
c101102Generalization Risk = 1
0.4 0.6 0.8 1.0 1.2
c101102103Generalization Risk = 1
Figure 3: Risk for the ablation experiment. Left: Empirical Expected Risk when using ˜Afor the
noise. Right: Empirical risk when we replace ˆVwith a random orthogonal matrix.
Connection to Prior Double Descent Theory Prior double descent theory postulates that the peak
for the ridge regularized model occurs at the interpolation point for the unregularized model or further
to the right into the over-parameterized regime. Hence, this model goes against prior expectations,
with the peak moving to the left into the under-parameterized regime. However, there might still be
some connection between the training error and the location of the peak. For example, the peak may
correspond to a local minimum of the training loss. We explore this in Appendix C and see only a
weak connection with the third derivative of the training error.
4We obtain such a matrix by computing the full SVD of a Gaussian random vector in Pytorch
74.4 Peak in the Norm of the Estimator Does Not Imply a Peak in the Risk
Prior double descent theories suggest that double descent occurs due to the norm of the estimator
increasing and then decreasing. This is true for the above model where we fixed dand varied n. How-
ever, the connection breaks if we fix nand vary dinstead ( parameter scaling regime ). This difference
between the two regimes is due to the normalization considered and has been observed before [30].
Figure 4 shows that for the parameter scaling regime, for small values of µ, we see under-
parameterized double descent. However, as we increase µ, the risk curve becomes monotonic.
Nevertheless, as shown in Figure 5, for larger values of µ,there is still a peak in the curve for the
norm of the estimator ∥Wopt∥2
F. Hence, the curve for the norm of the estimator exhibits under-
parameterized double descent even if the risk does not. This is further highlighted in Figure 6. Here,
we see that even though the variance is non-monotonic, the risk is dominated by the bias term. Thus,
we show that a peak in the generalization error for linear models does not imply a peak in the norm of
the estimator. The following theorem provides a local maximum in the E
∥Wopt∥2
F
curve for c <1.
Theorem 3 (∥Wopt∥FPeak) .Ifσtst=√ntst,σtrn=√nandµis such that p(µ)<0, then for
fixednthat is sufficiently large enough, we have that E[∥Wopt∥F]versus c=d/n curve has a local
maximum in the under-parameterized regime at c= (µ2+ 1)−1.
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.0020.0040.0060.0080.010Generalization Error     Under 
     Parameterized 
     RegimeEmpirical
Theoretical
1
2+1
Peak
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.0020.0040.0060.0080.010Generalization ErrorUnder 
Parameterized 
RegimeEmpirical
Theoretical
1
2+1
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.0020.0040.0060.0080.010Generalization ErrorUnder 
Parameterized 
RegimeEmpirical
Theoretical
1
2+1
Figure 4: Figure showing the theoretical risk curve from Theorem 1 and empirical values in the
parameter scaling regime for different values of µ[(L)µ= 0.1, (C)µ= 0.2, (R)µ= 0.5]. Here,
onlyµ= 0.1has a local peak. Here n=ntst= 1000 andσtrn=σtst=√
1000 . Each empirical
point is an average of 100 trials.
1 2 3 4 5
W2
F
0.0040.0050.0060.0070.0080.0090.0100.011Generalization Error
0.00.20.40.60.81.0
c
(a)µ= 0.1
1.075 1.100 1.125 1.150 1.175 1.200
W2
F
0.0020.0040.0060.0080.010Generalization Error
0.00.20.40.60.81.0
c (b)µ= 1
1.020 1.025 1.030 1.035 1.040
W2
F
0.0020.0040.0060.0080.010Generalization Error
0.00.20.40.60.81.0
c (c)µ= 2
Figure 5: Figure showing generalization error versus E
∥Wopt∥2
F
for the parameter scaling regime
for three different values of µ.
5 Shifting Local Maximum for Stieljtes Transform as a Function of c
In this section, we present the next example of under-parameterized double descent that violates
Assumption 2. In particular, the maximum of the map c7→mνc(0)does not occur at c= 1. We show
that the maximum can be chosen to be any value in (0,1). We consider the following mixture model.
Letπ1, π2be mixture weights such that π1+π2= 1. Then, with probability π1, the data is sampled
fromN(0,1
dI)and with probability π2, the data point is αzfor fixed z∈Rdandα∼ N(0,1). For
this model, the uncentered covariance matrix is given by
E[xxT] =π1Ex∼N(0,1
dI)[xxT] +π2E[α2zzT] =π1
dI+π2zzT.
80.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c0.000000.000020.000040.000060.000080.000100.00012(IW)Xtst2
F
Ntst - Bias
<- Under Parameterized 
     RegimeEmpirical
Theoretical
1
2+1
(a) Bias
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn1.061.081.101.121.141.161.181.20W2
F
<- Under Parameterized 
     RegimeEmprircal
Theory
1
2+1
 (b)∥Wopt∥2
F
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.0020.0040.0060.0080.010Generalization ErrorUnder Parameterized 
RegimeEmprircal
Theory
1
2+1
 (c) Generalization Error
Figure 6: Figure showing the E
∥Wopt∥2
F
, and the generalization error in the parameter scaling
regime for µ= 1,σtrn=√n, and σtst=√ntst. Here n= 1000 andntst= 1000 . For each
empirical data point, we ran at least 100 trials. More details can be found in Appendix G.
Then, the expected excess risk for a solution ˆβcompared to βis
R=E[∥βTx−ˆβTx∥2|X] =Ehπ1
d∥βT−ˆβT∥2+π2∥(β−ˆβ)Tz∥2|Xi
.
LetXtrn= [A zvT]∈Rd×n, where the A∈Rd×n−kwith each column a data point sampled I.I.D.
fromN(0,1
dI)andv∈Rkis the vector with random coefficients in front of z. Let β∈Rdbe the
target regressor function and let yT=βTXtrn+ξT
trn, where ξT
trnhas I.I.D. entries from a standard
normal. Let βT
opt=yTXT
trn(XtrnXT
trn)−1be the minimum norm. Then, Theorem 4 shows that the
peak occurs when d/n=c=π1<1. To experimentally verify Theorem 4, we consider two cases,
one where we enforce β⊥zand one where we do not. As seen in Figure 7, Theorem 4 is accurate
for both cases. This suggests that the β⊥zassumption seems to only be needed for simplifying the
proof.
0.4 0.6 0.8 1.0 1.2
d
Ntrn100101102103RiskEmprirical Risk
Theoretical Risk
0.4 0.6 0.8 1.0 1.2
d
Ntrn100101102103RiskEmprirical Risk
Theoretical Risk
Figure 7: Figure showing under-parameterized double descent. (Left) We have β=d·z. (Right) We
haveβ⊥z. The solid blue line represents the theoretical estimate from Theorem 4 and the scatter
points are from empirical experiments with d= 600 . For the empirical points, we average over 50
trials. The dashed vertical purple line is π1.
Theorem 4 (Under-parametrized Peak) .For the above model, if k/n→π2, and d/n→c, then the
expected risk is given by R=(π1c
π1−cc < π 1
π1
π1
c−π1+ 
1−π1
c
∥β∥2
d−(βTz)2
∥z∥2d
c > π 1.
Theorem 4 is quite surprising as it shows that the only peak in the risk curve occurs in the under-
parameterized regime. One might assume that is due to the low rankness of the data from the
second mixture. While this is true, prior work does not indicate that this is the reason. Specifically,
Huang, Hogg, and Villar [41] shows that projecting onto low-dimensional versions of the data
acts as a regularizer and removes the peak altogether. Xu and Hsu [39], also looks at a similar
problem, but they consider isotropic Gaussian data and project onto the first pcomponents. In
this case, the data is artificially high-dimensional (since only the first kcoordinates are non-zero).
9They again see a peak at the interpolation point ( n=p). Wu and Xu [40] also looks at a version
of Principal Component Regression in which the data dimension is reduced. That is, the data is
not embedded in high-dimensional space anymore. Wu and Xu [40] sees a peak at the boundary
between the under and over-parameterized regions. Finally, Sonthalia and Nadakuditi [24] and Kausik,
Srivastava, and Sonthalia [37] look at the denoising problem for low-dimensional data and have peaks
atc= 1. Therefore, prior work does not immediately imply that low dimensional data results in
under-parameterized double descent. If we had only low-dimensional data, then the peak “should”
move into the over-parameterized regime. This is because if the true dimensionality of the data is
r < d . Then, one might think that the peak occurs when the number of training data points nequals
rsince that is the interpolation point5. We are in the over-parameterized regime since d > r =n.
We can understand this phenomenon as follows. The data from the second mixture does not affect
the smallest eigenvalue of the covariance matrix. This is because the second mixture lives in a
one-dimensional space. Hence, it only affects the top eigenvalue. Since the Stieljtes transform at 0 is
dominated by the behavior of the smallest non-zero eigenvalue, data from the second mixture has
very little effect on the Stieljtes transform of the ESD at 0. We expect the above intuition to hold,
even when replacing rank 1with rank rfor any fixed small r.
Connection to Prior Double Descent Theory For this example, it is easy to show that there is a
strong connection to prior double descent theory. Specifically, even though we cannot interpolate the
data, the minimum training error will occur at c=π1. Further, we see that the blow-up in the excess
risk is due to the norm of the estimator blowing up.
Additionally, we see that comparing with the result from [11] (which is the case when π2= 0), we
see that the π2>0results in sifting the peak to π1and rescales the variance by π1. However, we
also see an additional correction term in the overparameterized regime:

1−π1
c
−(βTz)2
∥z∥2d
Here we see that the term depends on the alignment between the target βand the spike direction z.
6 Conclusion
This paper presents two simple models with double descent in the under-parameterized regime.
While such peaks seem limited to special cases, understanding the cause is important for a complete
understanding of the double descent phenomenon. Our analysis reveals that the location of peaks
depends critically on two properties: the alignment between targets and the eigenvectors of the
training data gram matrix and the behavior of the Stieltjes transform of the limiting empirical spectral
distribution.
We demonstrate that violating either of these properties can shift the peak into the under-parameterized
regime. The first model shows that ridge regularization can create a misalignment between targets
and singular vectors, leading to a peak at c= 1/(1 +µ2). The second model, using a mixture of
isotropic Gaussian vectors and directional vectors, demonstrates that modifying the spectrum can
result in a peak at c=π1. These findings challenge several prevailing theories about double descent.
They show that peaks need not occur at or beyond the interpolation threshold and that a peak in the
estimator’s norm does not necessarily imply a peak in the generalization error.
Investigating the interaction between spectral properties and generalization in deep neural networks
to provide a general theory of double descent is an important avenue for future work. Understanding
whether similar phenomena occur in other architectures and the implications for model selection and
regularization remain open questions.
References
[1] Scott Fortmann-Roe. Understanding the Bias-Variance Tradeoff, June 2012. URL:http:
//scott.fortmann-roe.com/docs/BiasVariance.html (cited on page 1).
5At least for 1-dimensional targets
10[2] Manfred Opper and Wolfgang Kinzel. Statistical Mechanics of Generalization. Models of
Neural Networks III: Association, Generalization, and Representation , 1996 (cited on page 1).
[3] Mikhail Belkin, Daniel J. Hsu, Siyuan Ma, and Soumik Mandal. Reconciling Modern Machine-
Learning Practice and the Classical Bias–Variance Trade-off. Proceedings of the National
Academy of Sciences , 2019 (cited on page 1).
[4] Madhu S. Advani, Andrew M. Saxe, and Haim Sompolinsky. High-dimensional Dynamics of
Generalization Error in Neural Networks. Neural Networks , 2020 (cited on pages 2, 4).
[5] Edgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: ridge regres-
sion and classification. The Annals of Statistics , 2018 (cited on pages 2–4).
[6] Gabriel Mel and Surya Ganguli. A Theory of High Dimensional Regression with Arbitrary
Correlations Between Input Features and Target Functions: Sample Complexity, Multiple
Descent Curves and a Hierarchy of Phase Transitions. In Proceedings of the 38th International
Conference on Machine Learning , 2021 (cited on page 2).
[7] Vidya Muthukumar, Kailas V odrahalli, and Anant Sahai. Harmless Interpolation of Noisy Data
in Regression. 2019 IEEE International Symposium on Information Theory (ISIT) , 2019 (cited
on page 2).
[8] Peter Bartlett, Philip M. Long, Gábor Lugosi, and Alexander Tsigler. Benign Overfitting in
Linear Regression. Proceedings of the National Academy of Sciences , 2020 (cited on pages 2,
3).
[9] Mikhail Belkin, Daniel J. Hsu, and Ji Xu. Two Models of Double Descent for Weak Features.
SIAM Journal on Mathematics of Data Science , 2020 (cited on page 2).
[10] Michal Derezinski, Feynman T Liang, and Michael W Mahoney. Exact Expressions for Double
Descent and Implicit Regularization Via Surrogate Random Design. In Advances in Neural
Information Processing Systems , 2020 (cited on pages 2, 3).
[11] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in High-
Dimensional Ridgeless Least Squares Interpolation. The Annals of Statistics , 2022 (cited on
pages 2, 3, 10).
[12] Bruno Loureiro, Gabriele Sicuro, Cedric Gerbelot, Alessandro Pacco, Florent Krzakala, and
Lenka Zdeborova. Learning Gaussian Mixtures with Generalized Linear Models: Precise
Asymptotics in High-dimensions. In Advances in Neural Information Processing Systems ,
2021 (cited on page 2).
[13] Chen Cheng and Andrea Montanari. Dimension Free Ridge Regression. arXiv preprint
arXiv:2210.08571 , 2022 (cited on page 2).
[14] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization Error of Random
Feature and Kernel Methods: Hypercontractivity and Kernel Matrix Concentration. Applied
and Computational Harmonic Analysis , 2022 (cited on page 2).
[15] Song Mei and Andrea Montanari. The Generalization Error of Random Features Regression:
Precise Asymptotics and the Double Descent Curve. Communications on Pure and Applied
Mathematics , 75, 2021 (cited on page 2).
[16] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A Mean Field View of the Landscape
of Two-layer Neural Networks. Proceedings of the National Academy of Sciences of the United
States of America , 2018 (cited on page 2).
[17] Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Covariate Shift in High-Dimensional
Random Feature Regression. ArXiv , 2021 (cited on page 2).
[18] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborova.
Generalisation Error in Learning with Random Features and the Hidden Manifold Model.
InProceedings of the 37th International Conference on Machine Learning , 2020 (cited on
page 2).
[19] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay
Golan, Daniel Soudry, and Nathan Srebro. Kernel and Rich Regimes in Overparametrized
Models. In Proceedings of Thirty Third Conference on Learning Theory , 2020 (cited on
page 2).
[20] Bruno Loureiro, C’edric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc M’ezard,
and Lenka Zdeborov’a. Learning Curves of Generic Features Maps for Realistic Datasets with
a Teacher-Student Model. In NeurIPS , 2021 (cited on page 2).
11[21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of
Lazy Training of Two-layers Neural Network. In Advances in Neural Information Processing
Systems , 2019 (cited on page 2).
[22] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When Do
Neural Networks Outperform Kernel Methods? In Advances in Neural Information Processing
Systems , 2020 (cited on page 2).
[23] Preetum Nakkiran, Prayaag Venkat, Sham M. Kakade, and Tengyu Ma. Optimal Regularization
can Mitigate Double Descent. In International Conference on Learning Representations , 2020
(cited on pages 2–4, 17).
[24] Rishi Sonthalia and Raj Rao Nadakuditi. Training data size induced double descent for
denoising feedforward neural networks and the role of training noise. Transactions on Machine
Learning Research , 2023 (cited on pages 3–5, 10, 17, 20, 21, 24, 26, 31).
[25] Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez. The Optimal Ridge Penalty for Real-
World High-Dimensional Data Can Be Zero or Negative Due to the Implicit Ridge Regulariza-
tion. Journal of Machine Learning Research , 2022 (cited on page 3).
[26] Fatih Furkan Yilmaz and Reinhard Heckel. Regularization-Wise Double Descent: Why it
Occurs and How to Eliminate it. In IEEE International Symposium on Information Theory ,
2022 (cited on page 3).
[27] Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi. Multiple Descent: Design Your Own
Generalization Curve. Advances in Neural Information Processing Systems , 2021 (cited on
page 3).
[28] Lechao Xiao and Jeffrey Pennington. Precise learning curves and higher-order scaling limits
for dot product kernel regression. arXiv preprint arXiv:2205.14846 , 2022 (cited on page 3).
[29] Ben Adlam and Jeffrey Pennington. The Neural Tangent Kernel in High Dimensions: Triple
Descent and a Multi-Scale Theory of Generalization. In International Conference on Machine
Learning , 2020 (cited on page 3).
[30] Stéphane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple Descent and the Two Kinds of
Overfitting: Where and Why Do They Appear? In Advances in Neural Information Processing
Systems , 2020 (cited on pages 3, 8).
[31] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized
Two-layers Neural Networks in High Dimension. The Annals of Statistics , 2021 (cited on
page 3).
[32] Alicia Curth, Alan Jeffares, and Mihaela van der Schaar. A u-turn on double descent: re-
thinking parameter counting in statistical learning. In Thirty-seventh Conference on Neural
Information Processing Systems , 2023. URL:https://openreview.net/forum?id=
O0Lz8XZT2b (cited on page 3).
[33] Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for
high-dimensional binary linear classification. Information and Inference: A Journal of the IMA ,
11(2):435–495, 2022 (cited on page 3).
[34] Ganesh Ramachandra Kini and Christos Thrampoulidis. Analytic study of double descent in
binary classification: the impact of loss. In 2020 IEEE International Symposium on Information
Theory (ISIT) , pages 2527–2532. IEEE, 2020 (cited on page 3).
[35] Pragya Sur and Emmanuel J Candès. A modern maximum-likelihood theory for high-
dimensional logistic regression. Proceedings of the National Academy of Sciences ,
116(29):14516–14525, 2019 (cited on page 3).
[36] Francesca Mignacco, Florent Krzakala, Yue Lu, Pierfrancesco Urbani, and Lenka Zdeborova.
The role of regularization in classification of high-dimensional noisy gaussian mixture. In
International conference on machine learning , pages 6874–6883. PMLR, 2020 (cited on
page 3).
[37] Chinmaya Kausik, Kashvi Srivastava, and Rishi Sonthalia. Generalization Error without
Independence: Denoising, Linear Regression, and Transfer Learning, 2023 (cited on pages 3,
4, 10, 42).
[38] Oussama Dhifallah and Yue Lu. On the inherent regularization effects of noise injection during
training. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research ,
pages 2665–2675. PMLR, 18–24 Jul 2021. URL:https://proceedings.mlr.press/
v139/dhifallah21a.html (cited on pages 3, 4, 14).
12[39] Ji Xu and Daniel J Hsu. On the number of variables to use in principal component regression.
Advances in neural information processing systems , 2019 (cited on pages 3, 9).
[40] Denny Wu and Ji Xu. On the Optimal Weighted \ell_2Regularization in Overparameterized
Linear Regression. Advances in Neural Information Processing Systems , 2020 (cited on pages 3,
10).
[41] Ningyuan Huang, David W. Hogg, and Soledad Villar. Dimensionality reduction, regulariza-
tion, and generalization in overparameterized regressions. SIAM Journal on Mathematics of
Data Science , 2022 (cited on pages 3, 9).
[42] Wei Hu, Zhiyuan Li, and Dingli Yu. Simple and Effective Regularization Methods for Training
on Noisily Labeled Data with Generalization Guarantee. In International Conference on
Learning Representations , 2020 (cited on page 4).
[43] Yutong Wang, Rishi Sonthalia, and Wei Hu. Near-interpolators: rapid norm growth and the
trade-off between interpolation and generalization. In Sanjoy Dasgupta, Stephan Mandt, and
Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelli-
gence and Statistics , volume 238 of Proceedings of Machine Learning Research , pages 4483–
4491. PMLR, February 2024. URL:https://proceedings.mlr.press/v238/
wang24k.html (cited on page 4).
[44] Vladimir Marcenko and Leonid Pastur. Distribution of Eigenvalues for Some Sets of Random
Matrices. Mathematics of The Ussr-sbornik , 1967 (cited on pages 4, 28).
[45] Lucas Benigni and Sandrine Péché. Eigenvalue distribution of some nonlinear models of
random matrices. Electronic Journal of Probability , 26:1–37, 2021 (cited on page 4).
[46] Olivier Ledoit and Sandrine Péché. Eigenvectors of some large sample covariance matrix
ensembles. Probability Theory and Related Fields , 151(1):233–264, 2011 (cited on page 4).
[47] Carl D. Meyer Jr. Generalized Inversion of Modified Matrices. SIAM Journal on Applied
Mathematics , 1973 (cited on pages 6, 22, 24).
[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: an imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32 ,
pages 8024–8035. Curran Associates, Inc., 2019. URL:http://papers.neurips.cc/
paper/9015-pytorch-an-imperative-style-high-performance-deep-
learning-library.pdf .
[49] Chris M. Bishop. Training with Noise is Equivalent to Tikhonov Regularization. Neural
Computation , 1995 (cited on page 17).
[50] Friedrich Götze and Alexander Tikhomirov. The Rate of Convergence for Spectra of GUE and
LUE Matrix Ensembles. Central European Journal of Mathematics , 2005 (cited on page 28).
[51] Friedrich Götze and Alexander Tikhomirov. Rate of Convergence to the Semi-Circular Law.
Probability Theory and Related Fields , 2003 (cited on page 28).
[52] Friedrich Götze and Alexander Tikhomirov. Rate of Convergence in Probability to the
Marchenko-Pastur Law. Bernoulli , 2004 (cited on page 28).
[53] Z. Bai, Baiqi. Miao, and Jian-Feng. Yao. Convergence Rates of Spectral Distributions of Large
Sample Covariance Matrices. SIAM Journal on Matrix Analysis and Applications , 2003 (cited
on page 28).
130.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.020.030.040.050.060.07Generalization Error Under Parameterized 
RegimeEmprircal
Theory
1
2+1
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.00710.00720.00730.00740.00750.00760.0077Generalization ErrorUnder Parameterized 
RegimeEmprircal
Theory
1
2+1
0.0 0.5 1.0 1.5 2.0 2.5
c=d/Ntrn100101102Generalization ErrorFigure 8: Generalization Error for low-dimension MNIST using a linear denoiser. For the left figure,
we use 10 dimensions and µ= 0.1. For the central figure, we use 5 dimensions and µ= 1. For the
right figure, we use 784 dimensions and µ= 1.
A Higher Rank for Denoising Model
One might be led to believe that the restriction that the data lie on a line embedded in high dimensional
space is crucial to the appearance of this phenomenon. However, this is not true. As long as the
rank of the data is relatively low, for the input noise setting, we can see this phenomenon. Hence,
we extend our results beyond the one-dimensional case to the low-dimensional case. Due to space
constraints, the conjectured formula for the risk is in Appendix F.6We verify the conjectured formula
as well as the role of low dimensionality using MNIST data. Specifically, we project the data
onto a r-dimensional subspace. We then add noise to the low-dimensional representation and then
solve the denoising problem. The left two figures in Figure 8 show that the phenomenon exists for
low-dimensional data. That is, we see that a peak occurs in the under-parameterized regime, and
the location of the peak seems to occur at1
µ2+1. However, running the same experiment without
projecting to a low-dimensional space (right figure) results in very different phenomena. We no
longer see double descent at all. Hence we see that if a peak occurs, then its location does not depend
on the dimension. However, the occurrence of the peak does depend on the dimension. Thus, we see
that this complements the results in [38].
6While these are currently presented as a conjecture. This is because we only computed the expectation
terms. Careful analysis of the variance would allow us to formalize the statement.
140.5 1.0 1.5 2.0 2.5 3.0
c=d/Ntrn6789Generalization Error1e8+1.271e 3
0.5 1.0 1.5 2.0 2.5 3.0
c=d/Ntrn1.22×103
1.23×103
1.24×103
1.25×103
1.26×103
1.27×103
Train ErrorFigure 9: Generalization Risk and Training error for denoising a low-dimensional version of MNIST
using a 1-hidden layer neural network. Here d= 2·784.
B Non-Linear Model
To show that under-parameterized peak occurs for non-linear models. We conducted an experiment
on MNIST with a 1-hidden layer neural network with a width of 784. The network has no bias, so has
2·7842parameters. Let Vbe a random 10-dimensional subspace. We project our data onto V, add
noise to the low dimensional representation, and train our network to remove this noise. We use full
batch gradient descent with weight decay of 0.001to train the model for 500 epochs with a learning
rate of 2×10−2. We test it on the complete MNIST test data. Figure 9 shows the training error and
generalization error as a function of the number of training data points. As seen in the figure, we
have multiple peaks in the under-parameterized regime, and the peaks in the risk correspond to local
minimums in the training error. Interestingly, the risk curve here exhibits 4 peaks!
There are, however, many crucial differences between the peaks for this neural network case and the
linear model case. First, the location of the peak does not seem to depend on the strength of the ridge
regularization. Second, the peak seems to directly correspond to the local minimums of the training
error. Hence, while we see peaks in the under-parameterized regime, the mechanisms that create
these peaks are likely to be different.
150.0 0.2 0.4 0.6 0.8 1.0
c=d/Ntrn0.002
0.0000.0020.0040.006Empirical
Training Error
Training Error 3rd Derivative
1
2+1
Minimum
0.0 0.2 0.4 0.6 0.8 1.0
c=d/Ntrn0.000.020.040.060.08
Empirical
Training Error
Training Error 3rd Derivative
1
2+1
Minimum
0 2 4 6 8 10
2
0.20.40.60.81.0cMinimum of 3rd Derivative
1
2+1
2=1
Figure 10: Figure showing the training error, the third derivative of the training error, and the location
of the peak of the generalization error for different values of µ[(L)µ= 1, (C)µ= 2] for the data
scaling regime. (R) shows the location of the local minimum of the third derivative and1
µ2+1.
C Training Error
As seen in the prior section, the peak occurs in the interior of the under-parameterized regime and not
on the border between the under-parameterized and over-parameterized regimes. We have also seen
that it does not necessarily occur whenever there is a peak in the norm of the estimator. The final
postulate for where the peak occurs is that it occurs when we first hit zero training error.
In this section, we explore the connection between the training error and the risk. Theorem 5 derives
a formula for the training error in the under-parameterized regime.
Theorem 5 (Training Error) .Letτbe as in Theorem 1. The training error for c <1is given by
EAtrn[∥Xtrn−Wopt(Xtrn+Atrn)∥2
F] =τ−2 
σ2
trn(1−c·T1) +σ4
trnT2
+o(1),
where T1=µ2
2 
1 +c+µ2cp
(1−c+µ2c)2+ 4µ2c2−1!
+1
2+1 +µ2c−p
(1−c+µ2c)2+ 4c2µ2
2c,
and
T2= (µ2c+c−1−p
(1−c+µ2c)2+ 4c2µ2)2 
µ2c+c+ 1
2p
(1−c+µ2c)2+ 4c2µ2+1
2!
.
Since we are studying the ridge regularized problem, it is impossible for the training error to be
exactly equal to 0. Hence, we may expect the peak to correspond to other features of the training error
curve. Given the analytical formula for the training error, we can compute the derivatives. We found
that the training error curve does not seem to signal the location of the peak in the generalization
error curve.
Since we are studying the ridge regularized problem, it is impossible for the training error to be
exactly equal to 0. Hence, we may expect the peak to correspond to other features of the training
error curve. For example, the peak could correspond to a local minimum of the training error, or it
could correspond to a point where the training error or its derivatives suddenly change. The first, a
local minimum, is easy to see from the plot of the training error, but the second can be more subtle as
we do not usually have access to the derivatives. However, since we have an analytical formula for
the training error, we can compute the derivatives.
Figure 10 plots the location of the peak and the training error. Here, the figure shows that the training
error curve does not seem to signal the location of the peak in the generalization error curve. However,
it shows that for the data scaling regime, the peak roughly corresponds to a local minimum of the
third derivative of the training error. While the minimum of the third derivative is difficult to interpret,
as we can see from the third plot in Figure 10, the minimum seems to closely track the location of the
peak.
16D Regularization Trade-off
It has been seen in prior work that the amount of noise added to the input data can be viewed as a
regularizer [24, 49]. In our setup, we have two different regularizers: the amount of noise added to
the data (since we are dealing with linear models, this is equivalent to the strength of the signal σtrn)
and the strength of the ridge regularizer µ. It is interesting to analyze the trade-off between the two
regularizers and the generalization error.
0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50
trn/Ntrn
0.0012050.0012100.0012150.0012200.0012250.0012300.0012350.001240Generalization ErrorEmpirical
Theoretical
Minimum
0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50
trn/Ntrn
0.001930.001940.001950.001960.001970.001980.00199Generalization ErrorEmpirical
Theoretical
Minimum
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.001100.001120.001140.001160.001180.00120Generalization Error<- Under 
     Parameterized 
     RegimeEmpirical
Theoretical
1
2+1
Peak
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.0020.0040.0060.0080.010Generalization ErrorUnder 
Parameterized 
RegimeEmpirical
Theoretical
1
2+1
Peak
Figure 11: The first two figures show the σtrnversus risk curve for c= 0.5, µ= 1andc= 2, µ= 0.1
withd= 1000 . The second two figures show the risk when training using the optimal σtrnfor the
data scaling and parameter scaling regimes.
Optimal σtrn First, we fix µand determine the optimal σtrn. Figure 11 displays the generalization
error versus σ2
trncurve. The figure shows that the error is initially large but then decreases until the
optimal generalization error. The generalization error when using the optimal σtrnis also shown in
Figure 11. Here, unlike [23], picking the optimal value of σtrndoes not mitigate double descent.
Proposition 1 (Optimal σtrn).The optimal value of σ2
trnforc <1is given by
σ2
trn=σ2
tstd[2c(µ2+ 1)2−2T(cµ2+c+ 1) + 2( cµ2−2c+ 1)] + Ntst(µ2c2+c2+ 1−T)
Ntst(c3(µ2+ 1)2−T(µ2c2+c2−1)−2c2−1).
Additionally, it is interesting to determine how the optimal value of σtrndepends on both µandc.
Figure 12 shows that for small values of µ∈(0.1,0.5), ascchanges, there exists an (inverted) double
descent curve for the optimal value of σtrn. However, for the data scaling regime, the minimum
of this double descent curve does not match the location for the peak of the generalization error .
Further, as the amount of ridge regularization increases, the optimal amount of noise regularization
decreases proportionally; optimal σ2
trn≈dµ2. Thus, for higher values of ridge regularization, it is
preferable to have higher-quality data.
Optimal Value of µWe now explore the effect of fixing σtrnand then changing µ. Figure 13,
shows a Ushaped curve for the generalization error versus µ, suggesting that there is an optimal
value of µ, which should be used to minimize the generalization error. Next, we compute the optimal
value of µusing grid search and plot it against c. Figure 14 shows double descent for the optimal
value of µfor small values of σtrn. Thus, for low SNR data, we see a double descent, but we do not
for high SNR data.
Finally, for a given value of µandc, we compute the optimal σtrn. We then compute the generalization
error (when using the optimal σtrn) and plot the generalization error versus µcurve. Figure 15
displays a very different trend from Figure 13. Instead of having a U-shaped curve, we have a
0 50 100 150 200 250 300 350 400
2
0100200300400500600700800Optimal 2
trn/Ntrn
c=0.10, slope = 0.10
c=0.29, slope = 0.30
c=0.48, slope = 0.50
c=0.67, slope = 0.71
c=0.86, slope = 0.91
c=1.05, slope = 1.11
c=1.24, slope = 1.30
c=1.43, slope = 1.49
c=1.62, slope = 1.68
c=1.81, slope = 1.87
c=2.00, slope = 2.06
0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn1.01.52.02.53.0optimal 2
trn/Ntrn
Risk peak
Minimum
0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
c=d/Ntrn0.500.751.001.251.501.752.00optimal 2
trn/Ntrn
Risk peak
Minimum
Figure 12: The first figure plots the optimal σ2
trn/nversus µcurve. The middle figure plots the
optimal σ2
trn/nversus cin the data scaling regime for µ= 0.5, and the last figure plots the optimal
σ2
trn/nversus cin the parameter scaling regime for µ= 0.1.
170 2 4 6 8 10
0.0020.0040.0060.0080.010Generalization ErrorTheoretical
Empirical
Minimum(a)c= 0.5
0 2 4 6 8 10
0.0020.0040.0060.008Generalization ErrorTheoretical
Empirical
Minimum (b)c= 2
Figure 13: Figure showing the generalization error versus µforσ2
trn=nandσ2
tst=Ntst.
0 1 2 3 4 5
c0.10.20.30.40.5Optimal 
(a)σtrn≈0.2
0 1 2 3 4 5
c0.20.40.60.81.0Optimal 
 (b)σtrn≈0.3
0 1 2 3 4 5
c12345Optimal 
 (c)σtrn≈1
Figure 14: Figure for the optimal value of µverses for different values of σtrn
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
103
1.2×103
1.4×103
1.6×103
1.8×103
2×103
Generalization Riskc = 0.5
(a)c= 0.5
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
103
1.2×103
1.4×103
1.6×103
1.8×103
Generalization Riskc = 2 (b)c= 2
Figure 15: Figure showing the generalization error versus µfor the optimal σ2
trnandσ2
tst=Ntst.
monotonically decreasing generalization error curve. This suggests that we can improve generalization
by using higher-quality training while compensating for this by increasing the amount of ridge
regularization.
Interaction Between the Regularizers The optimal values of µandσtrnare jointly computed
using grid search for µ∈(0,100] andσtrn/√n∈(0,10]. Figure 16 shows the results. Specifically,
σtrnis at the highest possible value (so best quality data), and then the model regularizes purely
using the ridge regularizer. This results in a monotonically decreasing generalization error curve.
Thus, in the data scaling model, there is an implicit bias that favors one regularizer over the other.
Specifically, the model’s implicit bias is to use higher quality data while using ridge regularization to
regularize the model appropriately . It is surprising that the two regularizers are not balanced.
180 2 4 6 8 10
1
c=Ntrn
d9.69.810.010.210.4Optimal trn/Ntrn
0 2 4 6 8 10
1
c=Ntrn
d51015202530Optimal 
0 2 4 6 8 10
1
c=Ntrn
d0.00099900.00099910.00099920.00099930.0009994Generalization ErrorFigure 16: Trade-off between the regularizers. The left column is the optimal σtrn, the central column
is the optimal µ, and the right column is the generalization error for these parameter restrictions.
0 1 2 3 4 5
c=d
Ntrn45678910Optimal trn
0 1 2 3 4 5
c=d
Ntrn678910Optimal 
0 1 2 3 4 5
c=d
Ntrn0.0000.0020.0040.0060.0080.010Generalization Error
(a)0.1≤µ≤10and0.1≤σtrn≤10
0 1 2 3 4 5
c=d
Ntrn9.709.759.809.859.909.9510.00Optimal trn
0 1 2 3 4 5
c=d
Ntrn51015202530Optimal 
0 1 2 3 4 5
c=d
Ntrn0.0000.0020.0040.0060.0080.010Generalization Error
(b)0.1≤µ≤100and0.1≤σtrn≤10
Figure 17: Trade-off between the regularizers. The left column is the optimal σtrn, the central column
is the optimal µ, and the right column is the generalization error for these parameter restrictions
Next we look at the trade-off between σtrnandµfor the parameter scaling regime. We again see,
Figure 17, that the model implicitly prefers regularizing via ridge regularization and not via input
data noise regularizer.
19E Proofs
E.1 Linear Regression
We begin by noting,
βT= (βT
optX+ξtrn)X†
trn.
Thus, we have,
∥β∥2= Tr( βTβ)
= Tr( βT
optXtrnX†
trn(X†
trn)TXtrnβopt) + Tr( ξtrnX†
trn(X†
trn)TξT
trn) + 2 Tr( βT
optXtrnX†
trnX†
trn)TξT
trn.
Taking the expectation, with respect to ξtrn, we see that the last term vanishes.
Letting Xtrn=UXΣXVT
X. We see that using the rotational invariance of X,UX, VXare independent
and uniformly random. Thus, s:=βT
optUXis a uniformly random unit vector.
Thus, we see,
EXtrn,ξtrnh
Tr(βT
optXtrnX†
trn(X†
trn)TXtrnβopt)i
=min(d,n)X
i=1E[s2
i] = min
1,1
c
Similarly, we see,
EXtrn,ξtrnh
ξtrnX†
trn(X†
trn)TξT
trni
=min(d,n)X
i=1E1
σi(Xtrn)2
Multiplying and dividing by d, normalizes the singular values squared of Xtrnso that the limiting
distribution is the Marchenko Pastur distribution with shape c. Thus, we can estimate using Lemma 5
from Sonthalia and Nadakuditi [24] to get,
(
c
1−c+o(1) c <1
1
c−1+o(1) c >1.
Finally, the cross-term has an expectation equal to zero. Thus,
EXtrn,ξtrn[∥βopt∥2] =(
1 +c
1−cc <1
1
c+1
c−1c >1
Then we have,
βTβopt=βT
optXtrnX†
trnβopt+ξtrnX†
trnβopt
The second term has an expectation equal to zero, and the first term is similar to before and has an
expectation equal to min
1,1
c
.
E.2 Proof for Output Noise Model
Theorem 4 (Under-parametrized Peak) .For the above model, if k/n→π2, and d/n→c, then the
expected risk is given by R=(π1c
π1−cc < π 1
π1
π1
c−π1+ 
1−π1
c
∥β∥2
d−(βTz)2
∥z∥2d
c > π 1.
Proof. We begin with c < π 1. Here, since AATis invertible, we can see that
ˆβT= 
βT
A zvT
+ξT
AT
vzT 
AAT+∥v∥2zzT−1
Let us focus on the term without the ξ. Using the Sherman Morrison formula and the orthogonality
ofβandz, we see that
 
βT
A zvT
AT
vzT 
AAT+∥v∥2zzT−1=βT
20Thus, we get that
ˆβT=βT+ξT
AT
vzT 
AAT+∥v∥2zzT−1
For this model, the uncentered covariance matrix is given by
E[xxT] =π1Ex∼N(0,1
dI)[xxT] +π2E[α2zzT] =π1
dI+π2zzT.
Then, the expected excess risk for a solution ˆβcompared to βis
R=E[∥βTx−ˆβTx∥2|X] =Ehπ1
d∥βT−ˆβT∥2+π2∥(β−ˆβ)Tz∥2|Xi
.
The first term is given by
E[∥β−ˆβT∥2|X] =E"ξT
AT
vzT 
AAT+∥v∥2zzT−12
|X#
Taking the expectation over ξ, we get that the expected excess risk is given by
Tr
AT
vzT 
AAT+∥v∥2zzT−2
A zvT
= Tr 
AAT+∥v∥2zzT−1
Again, using the Sherman-Morrison formula, we get that
Tr((AAT+∥v∥zzT)−1) = Tr(( AAT)−1−∥v||2
1 +∥v||2zT(AAT)−1zTr 
zT(AAT)−2z
Suppose νis the limiting distribution of the empirical spectral distribution for the non-zero eigenvalues.
Then using the concentration results from [24], we see that the error can be expressed
Tr((AAT)−1−∥v||2
1 +∥v||2zT(AAT)−1zTr 
zT(AAT)−2z
=d·Eλ∼ν1
λ
−∥v∥2
1 +∥v∥2Eλ∼ν1
λEλ∼ν1
λ2
The second term of the expected risk is
ξT
AT
vzT 
AAT+∥v∥zzT−1z2
Again, if we take the expectation over ξand write as a trace, we get
Tr(zT
(AAT)−1−(AAT)−1∥v∥2zzT(AAT)−1
1 +∥v∥2zT(AAT)−1z
z) =zT(AAT)−1z
1 +∥v∥2zT(AAT)−1z
Again, this can expressed as
Eλ∼ν1
λ
1 +∥v∥2Eλ∼ν1
λ
Putting it all together, the expected excess risk is
π1Eλ∼ν1
λ
+π2Eλ∼ν1
λ
1 +∥v∥2Eλ∼ν1
λ−π1
d∥v∥2
1 +∥v∥2Eλ∼ν1
λEλ∼ν1
λ2
If we then send n, k, d to infinity, while noting that ∥v∥ → ∞ , then we get then limiting risk
π1Eλ∼ν1
λ
=π1c
π1−c
For the d > n −kcase, AATis no longer invertible. Hence, we need to replace the inverse with
pseudoinverse. Hence, we have that
ˆβT= 
βT
A zvT
+ξT
AT
vzT 
AAT+∥v∥2zzT†
21We now expand the pseudoinverse using Theorem 1 from [47]. To write is succinctly, let M=
AAT, P= (I−MM†),γ=zTPz, and τ=zTM†zto get that
(AAT+∥v∥2zzT)†= (AAT)†−1
γ 
(AAT)†zzTP+PzzT(AAT)†
+(1 +∥v∥2)τ
∥v∥2γ2PzzTP
Note that Pis the projection onto the orthogonal complement of the range of A. Hence we see that
MP=M†P= 0. Thus, multiplying through, and setting terms to zero, we see that
(M+∥v∥2zzT)(M+∥v∥2zzT)†=MM†−1
γMM†zzTP+∥v∥2zzTM†
−1
γ∥v∥2zzTM†zzTP−1
γ∥v∥2zzTPzzTM†
+(1 +∥v∥2τ)
γ2zzTPzzTP
Using the fact that zTPz=γ,zTM†z=τand cancelling, we get
(M+∥v∥2zzT)(M+∥v∥2zzT)†=MM†−1
γMM†zzTP
−1
γ∥v∥2zzTM†zzTP+(1 +∥v∥2τ)
γzzTP
=MM†−1
γMM†zzTP
−1
γ∥v∥2zτzTP+(1 +∥v∥2τ)
γzzTP
=MM†−1
γMM†zzTP+1
γzzTP
=MM†+ 
I−MM†1
γzzTP
=MM†+1
γPzzTP
Thus, the first two terms in the error are
π1
d∥β−ˆβ∥2+π2∥βTz−ˆβTz∥2
Let us look at the second term first. Here we see that
ˆβTz=βT
MM†+1
γPzzTP
z=βT(MM†z+Pz)
Thus, we see that
βTz−ˆβTz=βTz−βT(MM†z+Pz)
=βT(I−MM†)z−βT(I−MM†)z
= 0
For the first term, we first note that
βT−ˆβT=βT−βT
MM†+1
γPzzTP
=βTP−1
γβTPzzTP
To compute the norm, we expand and get
βTP2+1
γ2Tr 
βTPzzTPPzzTPβ
−2
γTr 
βTPzzTPPβ
22Noting that Pis a projection matrix, so PP=Pand using zTPZ=γ, we get that this term
simplifies toβTP2−1
γTr 
βTPzzTPβ
Note that the subspace for Pcomes from a Gaussian random matrix. Hence is uniformly random.
Hence
EhβTP2i
=
1−n−k
d
∥β∥2=
1−π1
c
∥β∥2
Similarly,
E
−1
γTr 
βTPzzTPβ
=
1−π1
c(βTz)2
∥z∥2
For the next two terms, we need to first simplify

AT
vzT
(AAT+∥v∥2zzT)†
Substituting in our formulas and noticing that
ATP= 0andATM†=A†
we get that

AT
vzT
(AAT+∥v∥2zzT)†="
A†−1
γA†zzTP−0 + 0
vzTM†−1
γτvzTP−vzTM†+1+∥v∥2τ)
∥v∥2γvzTP#
="
A†−1
γA†zzTP
−1
γτvzTP+1
γτvzTP+1
∥v∥2γvzTP#
="
A†−1
γA†zzTP
1
∥v∥2γvzTP#
Then we have that
E"ξT
AT
vzT
(AAT+∥v∥2zzT)†2#
=
AT
vzT
(AAT+∥v∥2zzT)†2
= Tr
"
A†−1
γA†zzTP
1
∥v∥2γvzTP#T"
A†−1
γA†zzTP
1
∥v∥2γvzTP#

= Tr
M†−1
γm†zzTP−1
γPzztM†+1 +∥v∥2τ
∥v∥2γ2PzzTP
= Tr 
M†
−Tr1
γM†zzTP
−Tr1
γPzzTM†
+ Tr1 +∥v∥2τ
∥v∥2γ2PzzTP
Using the cycling invariance of the trace and the fact that M†P= 0, we get that
E"ξT
AT
vzT
(AAT+∥v∥2zzT)†2#
= Tr 
M†
+1 +∥v∥2τ
∥v∥2γ
The last equality is obtained using cyclic invariance, the fact that P2=P, and γ=zTPz. Using
Lemma 6 for p=n−kandq=d, we get that asymptotically,
π1
dTr(M†) =π2
1
c−π1.
Note that we similarly get that asymptotically,
π1
dτ=1
dπ2
1
c−π1→0.
23Thus, the contribution to the risk from this term isπ2
1
c−π1.
Finally, for the last term, we have that

AT
vzT
(AAT+∥v∥2zzT)†z="
A†z−1
γA†zzTPz
1
∥v∥2γvzTPz#
=0
v
∥v∥2
Thus,
E"ξT
AT
vzT
(AAT+∥v∥2zzT)†z2#
=1
∥v∥2
Note that ∥v∥2≈kand hence this term is aympotitcally zero. Putting it all together, we get the
needed result.
E.3 Proofs for Theorem 1
The proof structure closely follows that of [24].
E.3.1 Step 1: Decompose the error into bias and variance terms.
First, we decompose the error. Since we are not in the supervised learning setup, we do not have
standard definitions of bias/variance. However, we will call the following terms the bias/variance of
the model. First, we recall the following from [24].
Lemma 1 (Sonthalia and Nadakuditi [24]) .IfAtsthas mean 0 entries and Atstis independent of
XtstandW, then
EAtst[∥Xtst−WYtst∥2
F] =EAtst[∥Xtst−WX tst∥2
F]| {z }
Bias+EAtst[∥WA tst∥2
F]| {z }
V ariance.(3)
E.3.2 Step 2: Formula for Wopt
Here, we compute the explicit formula for Woptin Problem 1. Let ˆAtrn= [Atrn µI],ˆXtrn=
[Xtrn 0], and ˆYtrn=ˆXtrn+ˆAtrn. Then solving arg minW∥Xtrn−WYtrn∥2
F+µ2∥W∥2
Fis
equivalent to solving arg minW∥ˆXtrn−WˆYtrn∥2
F. Thus, Wopt= arg minW∥ˆXtrn−WˆYtrn∥2
F=
ˆXtrnˆY†
trn. Expanding this out, we get the following formula for ˆW. Let ˆube the left singular
vector and ˆvtrnbe the right singular vectors of ˆXtrn. Note that the left singular does not change
after ridge regularization, so ˆu=u. Let ˆh= ˆvT
trnˆA†
trn,ˆk=ˆA†
trnu,ˆs= (I−ˆAtrnˆA†
trn)u,
ˆt= ˆvtrn(I−ˆA†
trnˆAtrn),ˆγ= 1 + σtrnˆvT
trnˆA†
trnu,ˆτ=σ2
trn∥ˆt∥2∥ˆk∥2+ ˆγ2.
Proposition 2. Ifˆγ̸= 0andAtrnhas full rank then
Wopt=σtrnˆγ
ˆτuˆh+σ2
trn∥ˆt∥2
ˆτuˆkTˆA†
trn.
Proof. Here we know that uis arbitrary. We have that ˆAtrnhas full rank. Thus, the rank of ˆAtrnis
d, and the range of ˆAtrnis the whole space. Thus, ulives in the range of ˆAtrn. In this case, we want
Theorem 3 from [47]. We define
ˆp=−σ2
trn∥ˆk∥2
ˆγˆtT−σtrnˆkandˆqT=−σtrn∥ˆt∥2
ˆγˆkTˆA†
trn−ˆh.
Then we have,
(ˆAtrn+σtrnuˆvT
trn)†=ˆA†
trn+σtrn
ˆγˆtTˆkTˆA†
trn−ˆγ
ˆτˆpˆqT.
Note that, by our assumptions, we have ˆt= ˆvtrn(I−ˆA†
trnˆAtrn), and(I−ˆA†
trnˆAtrn)is a projection
matrix, thus
24ˆvT
trnˆtT= ˆvT
trn(I−ˆA†
trnˆAtrn)TˆvT
trn
= ˆvT
trn(I−ˆA†
trnˆAtrn)T(I−ˆA†
trnˆAtrn)TˆvT
trn.
To compute Wopt=ˆXtrn(ˆXtrn+ˆAtrn)†=σtrnuˆvT
trn(ˆAtrn+σtrnuˆvT
trn)†, using ˆγ−1 =
σtrnˆvT
trnˆA†
trnu=σtrnˆhu, we multiply this through.
σtrnuˆvT
trn(ˆAtrn+σtrnuˆvT
trn)†=σtrnuˆvT
trn(ˆA†
trn+σtrn
ˆγˆtTˆkTˆA†
trn−ˆγ
ˆτˆpˆqT)
=σtrnuˆh+σ2
trn∥ˆt∥2
ˆγuˆkTˆA†
trn
+σtrnˆγ
ˆτuˆvT
trn 
σ2
trn∥ˆk∥2
ˆγˆtT+σtrnˆk!
ˆqT
=σtrnuˆh+σ2
trn∥ˆt∥2
ˆγuˆkTˆA†
trn+σ3
trn∥ˆk∥2∥ˆt∥2
ˆτuˆqT
+σtrnˆγ(ˆγ−1)
ˆτuˆqT.
Then we have,
σ3
trn∥ˆk∥2∥ˆt∥2
ˆτuˆqT=σ3
trn∥ˆk∥2∥ˆt∥2
ˆτu
−σtrn∥ˆt∥2
ˆγˆkTˆA†
trn−ˆh
=−σ4
trn∥ˆk∥2∥ˆt∥4
ˆτˆγuˆkTˆA†
trn−σ3
trn∥ˆk∥2∥ˆt∥2
ˆτuˆh
and
σtrnˆγ(ˆγ−1)
ˆτuˆqT=σtrnˆγ(ˆγ−1)
ˆτu
−σtrn∥ˆt∥2
ˆγˆkTˆA†
trn−ˆh
=−σ2
trn∥ˆt∥2(ˆγ−1)
ˆτuˆkTˆA†
trn−σtrnˆγ(ˆγ−1)
ˆτuˆh.
Substituting back in and collecting like terms, we get,
σtrnuˆvT
trn(ˆAtrn+σtrnuˆvT
trn)†=σtrn 
1−σ2
trn∥ˆk∥2∥ˆt∥2
ˆτ−ˆγ(ˆγ−1)
ˆτ!
uˆh+
σ2
trn 
∥ˆt∥2
ˆγ−σ2
trn∥ˆk∥2∥ˆt∥4
ˆτˆγ−∥ˆt∥2(ˆγ−1)
ˆτ!
uˆkTˆA†
trn.
We can then simplify the constants as follows.
1−σ2
trn∥ˆk∥2∥ˆt∥2
ˆτ−ˆγ(ˆγ−1)
ˆτ=ˆτ−σ2
trn∥ˆk∥2∥ˆt∥2−γ2+γ
ˆτ=ˆγ
ˆτ
and
∥ˆt∥2
ˆγ−σ2
trn∥ˆk∥2∥ˆt∥4
ˆτˆγ−∥ˆt∥2(ˆγ−1)
ˆτ=∥ˆt∥2
ˆτ−σ2
trn∥ˆk∥2∥ˆt∥2−ˆγ2+ ˆγ
ˆτˆγ=∥ˆt∥2
ˆτ.
This gives us the result.
25E.3.3 Step 3: Decompose the terms into a sum of various trace terms.
For the bias and variance terms, we have the following two Lemmas.
Lemma 2. IfWoptis the solution to Equation 1, then
Xtst−WoptXtst=ˆγ
ˆτXtst.
Proof. To see this, note that we have n+M > M .
Xtst−WoptXtst=Xtst−σtrnˆγ
ˆτuˆhuvT
tst−σ2
trn∥ˆt∥2
ˆτuˆkTˆA†
trnuvT
tst
=Xtst−ˆσtrnˆγ
ˆτuˆvT
trnˆA†
trnuvT
tst−σ2
trn∥ˆt∥2
ˆτuˆkTˆA†
trnuvT
tst.
Note that ˆγ= 1 + σtrnˆvT
trnˆA†
trnu. Thus, we have that σtrnˆvT
trnˆA†
trnu= ˆγ−1. Substituting this
into the second term, we get,
Xtst−WoptXtst=Xtst−ˆγ(ˆγ−1)
ˆτuvT
tst−σ2
trn∥ˆt∥2
ˆτuˆkTˆA†
trnuvT
tst.
For the third term, since ˆk=ˆA†
trnu,ˆkTˆA†
trnu=ˆkTˆk=∥ˆk∥2. Substituting this into the expression,
we get that
Xtst−WoptXtst=Xtst−ˆγ(ˆγ−1)
ˆτuvT
tst−σ2
trn∥ˆt∥2∥ˆk∥2
ˆτuvT
tst.
Since Xtst=uvT
tst, we get,
Xtst−WoptXtst=Xtst 
1−ˆγ(ˆγ−1)
ˆτ−σ2
trn∥ˆt∥2∥ˆk∥2
ˆτ!
.
Simplify the constants using ˆτ=σ2
trn∥ˆt∥2∥ˆk∥2+ ˆγ2, we get,
ˆτ+ ˆγ−ˆγ2−σ2
trn∥ˆt∥2∥ˆk∥2
ˆτ=ˆγ
ˆτ.
Lemma 3 (Sonthalia and Nadakuditi [24]) .If the entries of Atstare independent with mean 0, and
variance 1/d, then we have that EAtst[∥WoptAtst∥2] =Ntst
d∥Wopt∥2.
Lemma 4. Ifˆγ̸= 0andAtrnhas full rank, then we have that
∥Wopt∥2
F=σ2
trnˆγ2
τ2Tr(ˆhTˆh) + 2σ3
trn∥ˆt∥2ˆγ
ˆτ2Tr(ˆhTˆkTˆA†
trn) +σ4
trn∥ˆt∥4
ˆτ2Tr((ˆA†
trn)TˆkˆkTˆA†
trn)| {z }
ρ.
Proof. We have
∥Wopt∥2
F= Tr( WT
optWopt)
= Tr σtrnˆγ
ˆτuˆh+σ2
trn∥ˆt∥2
ˆτuˆkTˆA†
trnTσtrnˆγ
ˆτuˆh+σ2
trn∥ˆt∥2
ˆτuˆkTˆA†
trn!
=σ2
trnˆγ2
ˆτ2Tr(ˆhTuTuˆh) + 2σ3
trn∥ˆt∥2ˆγ
ˆτ2Tr(ˆhTuTuˆkTˆA†
trn)
+σ4
trn∥ˆt∥4
ˆτ2Tr((ˆA†
trn)TˆkuTuˆkTˆA†
trn)
=σ2
trnˆγ2
ˆτ2Tr(ˆhTˆh) + 2σ3
trn∥ˆt∥2ˆγ
ˆτ2Tr(ˆhTˆkTˆA†
trn) +σ4
trn∥ˆt∥4
ˆτ2Tr((ˆA†
trn)TˆkˆkTˆA†
trn).
Where the last inequality is true due to the fact that ∥u∥2= 1.
26E.3.4 Step 4: Estimate With Random Matrix Theory
Lemma 5. LetAbe ap×qmatrix and let ˆA= [A µI ]∈Rp×q+p. Suppose A=UΣVTbe the
singular value decomposition of A. IfˆA=ˆUˆΣˆVTis the singular value decomposition of ˆA, then
ˆU=Uand if p < q
ˆΣ =
p
σ1(A)2+µ2 0 ··· 0
0p
σ2(A)2+µ2 0
.........
0 0 ···p
σp(A)2+µ2
∈Rp×p,
and
ˆV=
V1:pΣˆΣ−1
µUˆΣ−1
∈Rq+p×p.
Here V1:pare the first pcolumns of V.
Proof. Since p < q , we have that U∈Rp×p,Σ∈Rp×pare invertible. Here also consider the form
of the SVD in which VT∈Rp×q.
We start by nothing that ˆUˆΣ2ˆUT=ˆAˆAT=AAT+µ2I=U(Σ2+µ2Ip)UT. Thus, we immediately
see that σi(ˆA)2=σi(A)2+µ2and that ˆU=U.
Finally, we see,
ˆVT=ˆΣ−1UTˆA=ˆΣ−1ΣVT
1:pµˆΣ−1UT
Lemma 6. LetAbe ap×qmatrix and let ˆA= [A µI ]∈Rp×q+p. Suppose A=UΣVTbe the
singular value decomposition of A. IfˆA=ˆUˆΣˆVTis the singular value decomposition of ˆA, then
ˆU=Uand if p > q
ˆΣ =
p
σ1(A)2+µ2 0 ··· 0 ··· 0
0p
σ2(A)2+µ2 0
............
0 0 ···p
σq(A)2+µ2 0
µ
......0
0 0 ··· 0 ··· 0µ
∈Rp×p.
Here we will denote the upper left q×qblock by C. Further,
ˆV=
VΣT
1:q,1:qC−10
µU1:qC−1Uq+1:p
∈Rq+p×p.
Proof. Since p > q , we have that U∈Rp×pand we have that Σ∈Rp×q. Here VT∈Rq×qis
invertible.
We start with nothing,
ˆUˆΣ2ˆUT=ˆAˆAT=AAT+µ2I=U
Σ2
1:q,1:q 0
0 0 q−p
+µ2Iq
UT.
Thus, we immediately see that for i= 1, . . . , p σ i(ˆA)2=σi(A)2+µ2and for i=p+ 1, . . . , q , we
have that σi(ˆA)2=µ2and that ˆU=U.
Then, we see,
ˆVT=ˆΣ−1UTˆA=ˆΣ−1ΣVTµˆΣ−1UT
.
Note that Σhas0for the last p−qentries. Thus,
ˆΣ−1ΣV=
C−1Σ1:q,1:qV
0q−p,q
.
27Similarly, due to the structure of ˆΣ, we see,
µˆΣ−1UT= [µC−1UT
1:qµ1
µUT
q+1:p].
Lemma 7. Suppose Ais anpbyqmatrix such that p < q , the entries of Aare independent and have
mean 0, variance 1/p, and bounded fourth moment. Let c=p/q. Let ˆA= [A µI ]∈Rp×q+p. Let
Wp=ˆAˆATand let Wq=ˆATˆA. Suppose λpis a random non-zero eigenvalue from the largest p
eigenvalues of Wp, and λqis a random non-zero eigenvalue of Wq. Then
1.Eh
1
λpi
=Eh
1
λqi
=√
(1+µ2c−c)2+4µ2c2−1−µ2c+c
2µ2c+o(1).
2.Eh
1
λ2pi
=Eh
1
λ2qi
=µ2c2+c2+µ2c−2c+1
2µ4c√
4µ2c2+(1−c+µ2c)2+1
2µ4 
1−1
c
+o(1).
Proof. First, we note that the non-zero eigenvalues of WpandWqare the same. Hence we focus on
Wp.Wpis nearly a Wishart matrix but is not normalized by the correct value. However, cWpdoes
have the correct normalization.
Due to the assumptions on A, we have that the eigenvalues of cAATconverge to the Marchenko-
Pastur. Hence since the eigenvalues of cWpare
(cλp)i=cσi(A)2+cµ2,
we can estimate them by estimating cσi(A)2with the Marchenko-Pastur [44, 50–53]. In particular,
we want the expectation of the inverse. We need to use the Stieljes transform. We know that if mc(z)
is the Stieljes transform for the Marchenko-Pastur with shape parameter c, then if λis sampled from
the Marchenko-Pastur distribution, then
mc(z) =Eλ1
λ−z
.
Thus, we have that the expected inverse of the eigenvalue can be approximated m(−cµ2). We know
that the Steiljes transform:
mc(z) =−1−z−c−p
(1−z−c)2−4cz
−2zc.
Thus, we have,
E1
cλp
=m(−cµ2) =p
(1 +µ2c−c)2+ 4µ2c2−1−µ2c+c
2µ2c2.
Canceling 1/cfrom both sides, we get,
E1
λp
=p
(1 +µ2c−c)2+ 4µ2c2−1−µ2c+c
2µ2c.
Then for the estimate of E
1/λ2
p
, we need to compute the derivative of the mc(z)and evaluate it at
−cµ2. Hence, we see,
m′
c(z) =(c−z+p
−4cz+ (1−c−z)2−1)(c+z+p
−4cz+ (1−c−z)2−1)
4cz2p
−4cz+ (1−c−z)2.
Thus,
E1
c2λ2p
=m′
c(−cµ2)
=(c+µ2c+p
4µ2c2+ (1−c+µ2c)2−1)(c−µ2c+p
4µ2c2+ (1−c+µ2c)2−1)
4µ4c3p
4µ2c2+ (1−c+µ2c)2.
28Canceling the 1/c2from both sides, we get,
E1
λ2p
=(c+µ2c+p
4µ2c2+ (1−c+µ2c)2−1)(c−µ2c+p
4µ2c2+ (1−c+µ2c)2−1)
4µ4cp
4µ2c2+ (1−c+µ2c)2.
Multiplying out and simplifying
E1
λ2p
=µ2c2+c2+µ2c−2c+ 1
2µ4cp
4µ2c2+ (1−c+µ2c)2+1
2µ4
1−1
c
.
Lemma 8. Suppose Ais anpbyqmatrix such that p > q , the entries of Aare independent and have
mean 0, variance 1/p, and bounded fourth moment. Let c=p/q. Let ˆA= [A µI ]∈Rp×q+p. Let
Wp=ˆAˆATand let Wq=ˆATˆA. Suppose λpis a random non-zero eigenvalue of Wp, and λqis a
random eigenvalue from the largest qeigenvalues of Wq. Then
1.Eh
1
λqi
=Eh
1
λpi
=√
4µ2c+(−1+c+µ2c)2−c−µ2c+1
2µ2 +o(1).
2.Eh
1
λ2qi
=Eh
1
λ2pi
=1−2c+c2+µ2c+µ2c2
2µ4√
4µ2c+(−1+c+µ2c)2+ (1−c)1
2µ4+o(1).
Proof. First, we note that the non-zero eigenvalues of WpandWqare the same. Hence we focus on
Wp. Due to the assumptions on A, we have that the eigenvalues of ATAconverge to the Marchenko-
Pastur with shape c−1. Hence if λpis one of the first qeigenvalues of Wp, we see,
E1
λp
=mc−1(µ2) =p
(1 +µ2−1/c)2+ 4µ2/c−1−µ2+ 1/c
2µ2/c.
Then for the estimate of E
1/λ2
p
, we need to compute the derivative of the mc−1(z)and evaluate it
at−µ2. Hence, we see,
E1
λ2p
=(1/c+µ2+p
4µ2/c+ (1−1/c+µ2)2−1)(1/c−µ2+p
4µ2/c+ (1−1/c+µ2)2−1)
4µ4/cp
4µ2/c+ (1−1/c+µ2)2
=(1 +µ2c+cp
4µ2/c+ (1−1/c+µ2)2−c)(1−µ2c+cp
4µ2/c+ (1−1/c+µ2)2−c)
4µ4cp
4µ2/c+ (1−1/c+µ2)2
=(1 +µ2c+p
4µ2c+ (−1 +c+µ2c)2−c)(1−µ2c+p
4µ2c+ (−1 +c+µ2c)2−c)
4µ4p
4µ2c+ (−1 +c+µ2c)2
This can be further simplified to
1−2c+c2+µ2c+µ2c2
2µ4p
4µ2c+ (−1 +c+µ2c)2+ (1−c)1
2µ4+o(1)
We will also need to estimate some other terms.
Lemma 9. Suppose Ais anpbyqmatrix such that the entries of Aare independent and have mean
0, variance 1/p, and bounded fourth moment. Let ˆA= [A µI ]∈Rp×q+p. LetWp=ˆAˆATand let
Wq=ˆATˆA. Suppose λp, λqare random non-zero eigenvalues of Wp, Wqfrom the largest min(p, q)
eigenvalues of Wp, Wq. Then
1. Ifp > q ,Eh
λp−µ2
λpi
=c
1
2+1+µ2c−√
(−1+c+µ2c)2+4µ2c
2c
+o(1).
2. Ifp < q ,Eh
λq−µ2
λqi
=1
2+1+µ2c−√
(1−c+µ2c)2+4c2µ2
2c+o(1).
293. Ifp > q ,Eh
λp−µ2
λ2pi
=c
1+c+µ2c
2√
(−1+c+µ2c)2+4µ2c−1
2
+o(1).
4. Ifp < q ,Eh
λq−µ2
λ2qi
=1+c+µ2c
2√
(1−c+cµ2)2+4c2µ2−1
2+o(1).
Proof. Notice that if λis an eigenvalue of A(so unshifted).
λ
λ+µ2= 1−µ2
λ+µ2andλ
(λ+µ2)2=1
λ+µ2−µ2
(λ+µ2)2
Then use Lemmas 7, and 8 to finish the proof.
Bounding the Variance.
Lemma 10. Letηnbe a uniform measure on nnumbers a1, . . . , a nsuch that ηn→ηweakly in
probability. Then for any bounded continuous function f
1
nn−1X
i=1f(ai)→Ex∼η[f(x)].
Proof. Using weak convergence
1
nnX
i=1f(ai)→Ex∼η[f(x)].
Then using the boundedness of f, we get,
1
nn−1X
i=1f(ai)−1
nnX
i=1f(ai) =−1
nf(an)→0.
Lemma 11. Letηnbe a uniform measure on nnumbers a1, . . . , a nsuch that ηn→ηweakly in
probability. Let sbe a uniformly random unit vector in Rmindependent of ηn. Suppose n/m→ζ∈
(0,1]. Then for any bounded function f,
Es"nX
i=1s2
if(ai)#
→ζEx∼η[f(x)]
and
Es
 nX
i=1s2
if(ai)!2
−Es"nX
i=1s2
if(ai)#2
→0.
Proof. The first limit comes directly from weak convergence.
For the second, notice,
 nX
i=1s2
if(ai)!2
=nX
i=1s4
if(ai)2+X
i̸=js2
is2
jf(ai)f(aj) =nX
i=1s4
if(ai)2+nX
i=1s2
if(ai)X
j̸=is2
jf(aj).
Taking the expectation with respect to swe get,
Es
 nX
i=1s2
if(ai)!2
=1
m2+O(m)nX
i=1f(ai)2+1
m2+O(m)nX
i=1f(ai)X
j̸=if(aj)
Then using Lemma 10 for any fixed i, we have,
1
mX
j̸=if(aj)→ζEx∼η[f(x)].
30Thus, as n→ ∞ , we have,
Es
 nX
i=1s2
if(ai)!2
→ζ2Ex∼η[f(x)]2.
Then since
Es"nX
i=1s2
if(ai)#2
→ζ2Ex∼η[f(x)]2.
Thus, the variance goes to zero.
The interpretation of the above Lemma is that the variance of the sum decays to zero as m→ ∞ .
Lemma 12. Suppose Ais an pbyqmatrix such that the entries of Aare independent and have
mean 0, variance 1/p, and bounded fourth moment. Let ˆA= [A µI ]∈Rp×q+p. Letx∈Rpand
ˆy∈Rp+qbe unit norm vectors such that ˆyT= [yT0p]. Then
1. Ifp < q , thenE[Tr(xT(ˆAˆAT)†x] =√
(1−c+µ2c)2+4µ2c2−1−µ2c+c
2µ2c+o(1).
2. Ifp > q , thenE[Tr(xT(ˆAˆAT)†x] =√
(−1+c+µ2c)2+4µ2c−1−µ2c+c
2µ2c+o(1).
3. Ifp < q , thenE[Tr(ˆyT(ˆATˆA)†ˆy] =c
1+c+µ2c
2√
(1−c+µ2c)2+4c2µ2−1
2
+o(1).
4. Ifp > q , thenE[Tr(ˆyT(ˆATˆA)†ˆy] =c
1+c+µ2c
2√
(−1+c+µ2c)2+4µ2c−1
2
+o(1).
The variance of each above is o(1).
Proof. Let us start with p < q .
LetˆA=ˆUˆΣˆVT, where ˆΣisp×p. Then we see,
(ˆAˆAT)†=ˆUˆΣ−2ˆUT.
Where ˆUis uniformly random. Thus similar to [24], we can use Lemma 7 to get,
E[Tr(xT(ˆAˆAT)†x] =p
(1 +µ2c−c)2+ 4µ2c2−1−µ2c+c
2µ2c+o(1).
On the other hand, for p > q , we have that only the first qeigenvalues have the expectation in Lemma
8 The other p−qare equal to1
µ2. Thus, we see,
E[Tr(xT(ˆAˆAT)†x] =1
c p
4µ2c+ (−1 +c+µ2c)2−c−µ2c+ 1
2µ2+o(1)!
+
1−1
c1
µ2
=p
4µ2c+ (−1 +c+µ2c)2+c−µ2c−1
2cµ2.
Again let us first consider the case when p < q . Then we have,
(ˆATˆA)†=ˆVˆΣ−2ˆVT=
V1:pΣˆΣ−1
µUˆΣ−1
ˆΣ−2ˆΣ−1ΣVT
1:pµˆΣ−1UT
.
Since ˆyhas zeros in the last pcoordinates, we see,
ˆyT(ˆATˆA)†ˆy=yTV1:pΣˆΣ−4ΣVT
1:py.
Thus, we can use Lemma 9 to estimate this as,
c 
1 +c+µ2c
2p
(1−c+cµ2)2+ 4c2µ2−1
2!
+o(1).
31The extra factor of ccomes from the sum of pcoordinates of a uniformly unit vector in qdimensional
space. And for p > q , we have that the estimate is
c 
1 +c+µ2c
2p
(1 +µ2−1/c)2+ 4µ2/c−1
2!
+o(1).
For the variance term, use Lemma 11. For three of the cases, the limiting distribution is the Marchenko-
Pastur distribution. For the other case, the limiting measure is a mixture of the Marchenko-Pastur and
a dirac delta at 1/µ2.
The rest of the lemmas in this section are used to compute the mean and variance of the various terms
that appear in the formula of Wopt.
Lemma 13. We have that
EAtrnh
∥ˆh∥2i
=

c
1+c+µ2c
2√
(1−c+µ2c)2+4µ2c2−1
2
+o(1) c <1
c
1+c+µ2c
2√
(−1+c+µ2c)2+4µ2c−1
2
+o(1) c >1
and that V(∥ˆh∥2) =o(1).
Proof. Here we see that
∥ˆh∥2= Tr(ˆ vT
trn(ˆAT
trnˆAtrn)†ˆvT
trn).
Thus, using the Lemma 12 we get that if c <1
E[∥ˆh∥2] =c 
1 +c+µ2c
2p
(1−c+µ2c)2+ 4µ2c2−1
2!
+o(1)
and if c >1
E[∥ˆh∥2] =c 
1 +c+µ2c
2p
(−1 +c+µ2c)2+ 4µ2c−1
2!
+o(1).
Lemma 14. We have
EAtrnh
∥ˆk∥2i
=

√
(1−c+µ2c)2+4µ2c2−1−µ2c+c
2µ2c+o(1) c <1√
(−1+c+µ2c)2+4µ2c−1−µ2c+c
2µ2c+o(1) c >1
and that V(∥ˆk∥2) =o(1).
Proof. Since ˆk=ˆA†
trnu, we have that
∥ˆk∥2= Tr( uT(ˆAtrnˆAT
trn)†u).
According to the Lemma 12, if c <1
E[∥ˆk∥2] =p
(1−c+µ2c)2+ 4µ2c2−1−µ2c+c
2µ2c+o(1)
and if c >1
E[∥ˆk∥2] =p
(−1 +c+µ2c)2+ 4µ2c−1−µ2c+c
2µ2c+o(1).
Lemma 15. We have that
EAtrn
∥ˆt∥2
=

1
2
1−c−µ2c+p
(1−c+µ2c)2+ 4c2µ2
+o(1) c <1
1
2
1−c−µ2c+p
(−1 +c+µ2c)2+ 4µ2c
+o(1) c >1
and we have that V(∥ˆt∥2) =o(1)
32Proof. Here we see that ˆt= ˆvtrn(I−ˆA†
trnˆAtrn). Thus, we see that
∥ˆt∥2=∥vtrn∥2−ˆvT
trnˆA†
trnˆAtrnˆvtrn= 1−ˆvT
trnˆA†
trnˆAtrnˆvtrn.
IfˆV∈Rp+q×p+q, we have that
ˆA†
trnˆAtrn=ˆV
Ip0
0 0 q
ˆVT.
Then if p < q using Lemma 6 and the fact that the last pcoordinates of ˆvtrnare 0, we see that
ˆvT
trnˆA†
trnˆAtrnˆvtrn=vT
trnV1:pΣˆΣ−2ΣVT
1:pvtrn.
Then using Lemma 9 to estimate the middle diagonal matrix, we get that
E[∥ˆt∥2] = 1−c 
1
2+1 +µ2c−p
(1 +µ2c−c)2+ 4c2µ2
2c!
=1
2
1−c−µ2c+p
(1−c+µ2c)2+ 4c2µ2
+o(1).
Similarly for c >1, we have that
E[∥ˆt∥2] = 1− 
1
2+c+µ2c−cp
(1 +µ2−1/c)2+ 4µ2/c
2!
+o(1)
=1
2
1−c−µ2c+p
(−1 +c+µ2c)2+ 4µ2c
+o(1).
The variance of ˆA†
trnˆAtrnis also o(1)using Lemma 11.
Lemma 16. We have that EAtrn[ˆγ] = 1 andV(γ) =O(σ2
trn/d).
Proof. Noting that ˆA=UˆΣˆVT, we have that
ˆγ= 1 + σtrnˆvT
trnˆA†
trnu= 1 + σtrnmin(n,d)X
i=1σi(ˆA)−1ˆaibi.
Here ˆaT= ˆvT
trnˆVandb=UTu.Uis a uniformly random rotation matrix that is independent of ˆΣ
andˆV. Thus, taking the expectation with respect to Atrn, we get that the expectation is equal to zero.
For the variance, let us first consider the case when c <1. For this case, we have that
ˆV=
V1:dΣˆΣ−1
µUˆΣ−1
.
Thus, letting aT=vT
trnV1:d, we get that
ˆγ= 1 +dX
i=1σi(A)
σ2
i(A) +µ2aibi.
Squaring and taking the expectation, we see that
E[γ2] = 1 +σ2
trn
nEλ∼µcλ
(λ+µ2)2
+oσ2
trn
n
.
Similarly for c >1, we have that
E[γ2] = 1 +σ2
trn
dEλ∼µcλ
(λ+µ2)2
+oσ2
trn
d
.
33Lemma 17. We have that
Eh
Tr((ˆA†
trn)TˆkˆkTˆA†
trn)i
=E[ρ] =

µ2c2+c2+µ2c−2c+1
2µ4c√
4µ2c2+(1−c+µ2c)2+1
2µ4 
1−1
c
+o(1) c <1
1−2c+c2+µ2c+µ2c2
2µ4c√
4µ2c+(−1+c+µ2c)2+ 
1−1
c1
2µ4+o(1) c >1
and that V(ρ) =o(1).
Proof. Here we have that
ρ= Tr( ˆkT(ˆAT
trnˆAtrn)†ˆk) = Tr( uT(ˆAtrnˆAT
trn)†(ˆAtrnˆAT
trn)†u).
We first notice that
(ˆAtrnˆAT
trn)†(ˆAtrnˆAT
trn)†=ˆUTˆΣ2ˆU.
Thus using Lemmas 7 and 8, we see that if c <1
E[ρ] =µ2c2+c2+µ2c−2c+ 1
2µ4cp
4µ2c2+ (1−c+µ2c)2+1
2µ4
1−1
c
and if c >1
E[ρ] =1
c 
1−2c+c2+µ2c+µ2c2
2µ4p
4µ2c+ (−1 +c+µ2c)2+ (1−c)1
2µ4!
+
1−1
c1
µ4
=1−2c+c2+µ2c+µ2c2
2µ4cp
4µ2c+ (−1 +c+µ2c)2+
1−1
c1
2µ4.
The variance being o(1)comes from Lemma 11 again.
Lemma 18. We have that
EAtrnh
Tr(ˆhTˆkTˆA†
trn)i
= 0
and the variance is o(1).
Proof. Letting ˆA=UˆΣˆVT, we get that
Tr(ˆhTˆkTˆAT) =uTUˆΣ−3ˆVTˆvT
trn.
Then again since Uis uniformly random and independent of ˆΣandˆV, the expectation is equal to
zero. The variance is computed similarly to Lemma 16.
E.3.5 Step 5: Putting it together
Lemma 19. We have that
Eτ
σ2
trn
=

1
σ2
trn+1
2
1 +µ2c+c−p
(1−c+µ2c)2+ 4µ2c2
+o(1) c <1
1
σ2
trn+1
2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2c
+o(1) c >1
and that V(τ/σ2
trn) =o(1).
Proof. Using the fact that all of the quantities concentrate, we can use the previous estimates.
Specifically, we use that
|E[XY]−E[X]E[Y]| ≤p
V[X]V[Y].
Thus, since our variances decay, we can use the product of the expectations. Further,
|V[XY]|=|V[X]V[Y] +E[X]2V[Y] +E[Y]2V[X]−2E[X]E[Y]Cov(X, Y) +Cov(X2, Y2)−Cov(X, Y)2|
≤ |V[X]V[Y] +E[X]2V[Y] +E[Y]2V[X]|+ 2|E[X]E[Y]|p
V[X]V[Y] +|V[X]V[Y]|+|p
V[X2]V[Y2]|.
Thus, since the variances individually go to 0, we see that the variance of the product also goes to 0.
Then using Lemma 15 and 14, we have that if c <1
Eh
∥ˆt∥2∥ˆk∥2i
=1
2
1 +µ2c+c−p
(1−c+µ2c)2+ 4µ2c2
+o(1)
34andV(∥ˆt∥2∥ˆk∥2) =o(1). Then since
|V[X+Y]| ≤ |V[X] +V[Y]|+ 2p
V[X]V[Y]
we have that using Lemma 16, that if c <1
Eτ
σ2
trn
=1
σ2
trn+1
2
1 +µ2c+c−p
(1−c+µ2c)2+ 4µ2c2
+o(1)
and that that variance is o(1). Ifc >1
Eτ
σ2
trn
=1
σ2
trn+1
2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2c
+o(1).
Lemma 20. We have that
EAtrn1
σ2
trn∥ˆh∥2+∥ˆt∥4ρ
=

c(1+σ−2
trn)
2
µ2c+c+1√
(1−c+µ2c)2+4µ2c2−1
+o(1) c <1
c(1+σ−2
trn)
2
µ2c+c+1√
(−1+c+µ2c)2+4µ2c−1
+o(1) c >1
and that the variance is o(1).
Proof. Similar to Lemma 19, we can multiply the expectations since the variances are small. For
c <1, simplifying, we get that
EAtrn1
σ2
trn∥ˆh∥2+∥ˆt∥4ρ
=c(1 +σ−2
trn)
2 
µ2c+c+ 1p
(1−c+µ2c)2+ 4µ2c2−1!
+o(1)
and if c >1, we get that
EAtrn1
σ2
trn∥ˆh∥2+∥ˆt∥4ρ
=c(1 +σ−2
trn)
2 
µ2c+c+ 1p
(−1 +c+µ2c)2+ 4µ2c−1!
+o(1)
and the variance decays since the variances decay individually.
Lemma 21. We have that
EAtrn
∥Wopt∥2
F
=σ4
trn
τ2

c(1+σ−2
trn)
2
µ2c+c+1√
(1−c+µ2c)2+4µ2c2−1
+o(1) c <1
c(1+σ−2
trn)
2
µ2c+c+1√
(−1+c+µ2c)2+4µ2c−1
+o(1) c >1
and that V(∥Wopt∥2
F) =o(1).
Proof. Follows immediately from Lemmas 4, 17, 18, and 20.
Theorem 1 (Generalization Error Formula) .Suppose the training data Xtrnand test data Xtst
satisfy Assumption 3 and the noise Atrn, Atstsatisfy Assumption 4. Let µbe the regularization
parameter. Then for the under-parameterized regime (i.e., c <1) for the solution Woptto Problem 1,
the generalization error or risk given by Equation 2 is given by
R(c, µ) =cσ2
trn(σ2
trn+ 1))
2dτ21 +c+µ2cp
(1−c+µ2c)2+ 4µ2c2−τ−2cσ2
trn(σ2
trn+ 1))
2d+τ−2σ2
tst
ntst+o1
d
,
where
1
τ=2∥βTu∥
2 +σ2
trn(1 +c+µ2c−p
(1−c+µ2c) + 4µ2c2).
Proof. Rewritingˆγ2
τ2asˆγ2/σ4
trn
τ2/σ4
trn, we can the concentration from Lemmas 16 and 19. Then using
Lemma 21 we get the needed result.
35Theorem 6. For the over-parameterized case, we have that the generalization error is given by
R(c, µ) =τ−2 
σ2
tst
Ntst+cσ2
trn(σ2
trn+ 1))
2d 
1 +c+µ2cp
(−1 +c+µ2c)2+ 4µ2c−1!!
+o1
d
,
where τ−1=2
2 +σ2
trn(1 +c+µ2c−p
(−1 +c+µ2c) + 4µ2c).
Proof. Rewritingˆγ2
τ2asˆγ2/σ4
trn
τ2/σ4
trn, we can the concentration from Lemmas 16 and 19. Then using
Lemma 21 we get the needed result.
E.4 Proof of Theorem 2
Theorem 2 (Under-Parameterized Peak) .Letµ∈R>0,σ2
trn=n=d/candσ2
tst=ntst, and dis
sufficiently large, so that the error term o(1/d)is small, then the risk R(c)from Theorem 1, as a
function of c, has a local maximum in the under-parameterized regime at c=1
1+µ2.
Proof. First, we compute the derivative of the risk. We do so using SymPy and get the following
expression.
4c
4cµ2+ 
µ2−1 
cµ2−c+ 1
− 
µ2+ 1q
4c2µ2+ (cµ2−c+ 1)2
d
4c2µ2+ (cµ2−c+ 1)2
cµ2+c−q
4c2µ2+ (cµ2−c+ 1)2+ 12
+2c 
µ2+ 1
4c2µ2+ 
cµ2−c+ 12
− 
4cµ2+ 
µ2−1 
cµ2−c+ 1 
cµ2+c+ 1
d
4c2µ2+ (cµ2−c+ 1)23
2
cµ2+c−q
4c2µ2+ (cµ2−c+ 1)2+ 12
+2 
4c2µ2+ 
cµ2−c+ 123
cµ2+c−q
4c2µ2+ (cµ2−c+ 1)2+ 16!
M
4c2µ2+ (cµ2−c+ 1)27
2
cµ2+c−q
4c2µ2+ (cµ2−c+ 1)2+ 17
We can then compute the limit as c→0+. Again using SymPy we see that
lim
c→0+∂
∂cR(c, µ2;σ2
trn=d/c) =1
d>0.
Let
T(c, µ) =c2µ4+ 2c2µ2+c2+ 2cµ2−2c+ 1
To find the critical point, we shall write the derivative as one fraction of the form
∂cR(c, µ) =−2(cµ2+c−1)P(c, µ, d, T )
Q(c, µ, d, T ).
Here we see that
P(c, µ, d, T ) =c2µ4+ 2c2µ2+c2+ 2cµ2+ 1−(cµ+c+ 1)√
T
and
Q(c, µ, d, T ) =d(cµ2+c+ 1−√
T)2T3/2
We can see that the numerator is zero at c= (1 + µ2)−1. To evaluate the denominator at this point,
we first get that
T((1 + µ2)−1, µ2) =4µ2
µ2+ 1<4
36Thus, we see that the denominator is
d 
2−2µp
µ2+ 1!24µ2
µ2+ 13/2
Since T((1 + µ2)−1, µ2<4, we have that
2−2µp
µ2+ 1>0
Hence the denominator is non-zero. Thus, we see that c= (1 + µ2)−1is a critical point.
Next we want to show that this point is a local maximum. To do so, we compute the second derivative
and evaluate at c= (1 + µ2)−1. Using SymPy, we get that the value of the second derivative at this
point is
(µ2+ 1)4·(4µ3+ 3µ−4µ2p
µ2+ 1−p
µ2+ 1)
8d·µ3·(µ2−µp
µ2+ 1 + 1)3
To see that it is a maximum, we need to show that the above is negative. We begin by showing that the
denominator is positive. Since 8dµ3>0, we only need to look at the second term. Using arithmetic
mean and geometric mean inequality, we see that
µp
µ2+ 1 =p
µ2(µ2+ 1)≤µ2+µ2+ 1
2=µ2+1
2< µ2+ 1
Hence the denominator is positive. To show that the numerator is negative, we have the following
4µ3+ 3µ−4µ2p
µ2+ 1−p
µ2+ 1<0⇐⇒ 4µ2+ 3<(4µ2+ 1)p
µ2+ 1
µ
⇐⇒ 16µ4+ 9 + 24 µ2<(16µ4+ 1 + 8 µ2)·
1 +1
µ2
⇐⇒ 16µ2+ 8<1
µ2(16µ4+ 8µ2+ 1)
⇐⇒ 0<1
µ2
Where we are allowed to square both sides because both quantities are non-negative. Thus, we see
get the needed result.
E.5 Proof of Theorem 3
Theorem 3 (∥Wopt∥FPeak) .Ifσtst=√ntst,σtrn=√nandµis such that p(µ)<0, then for
fixednthat is sufficiently large enough, we have that E[∥Wopt∥F]versus c=d/n curve has a local
maximum in the under-parameterized regime at c= (µ2+ 1)−1.
Proof. Here we note that the expression for the norm of Woptis given by Lemma 21. We follow the
same proof structure as Theorem 2. Differentiating with respect to c, we see that the numerator is if
the form
(cµ2+c−1)P(c, µ, T )
When the denominator is
(cµ2+c+ 1−p
(T))7T7/2
Where is as is in the proof of Theorem 2. Again at when c= 1/(µ2+1). We see that the denominator
is positive because√
T <2. Hence again, we have a critical point c= (1 + µ2)−1.
E.6 Proof of Theorem 5
Theorem 5 (Training Error) .Letτbe as in Theorem 1. The training error for c <1is given by
EAtrn[∥Xtrn−Wopt(Xtrn+Atrn)∥2
F] =τ−2 
σ2
trn(1−c·T1) +σ4
trnT2
+o(1),
37where T1=µ2
2 
1 +c+µ2cp
(1−c+µ2c)2+ 4µ2c2−1!
+1
2+1 +µ2c−p
(1−c+µ2c)2+ 4c2µ2
2c,
and
T2= (µ2c+c−1−p
(1−c+µ2c)2+ 4c2µ2)2 
µ2c+c+ 1
2p
(1−c+µ2c)2+ 4c2µ2+1
2!
.
Proof. Note that we have:
EAtrn∥Xtrn−WoptYtrn∥2
F
n
=1
nEAtrn
∥Xtrn−Wopt(Xtrn+Atrn))∥2
F
=1
nE[∥Xtrn−WoptXtrn∥2] +1
nE[∥WoptAtrn∥2]
+2
nE
Tr((Xtrn−WoptXtrn)TWoptAtrn)
.
First, by Lemma 2, we have Xtrn−WoptXtrn=ˆγ
ˆτXtrn. Then, E[∥Xtrn−WoptXtrn∥2] =
ˆγ2
ˆτ2E[∥Xtrn∥2] =ˆγ2σ2
trn
ˆτ2. Then, let us look at the EAtrn[∥WoptAtrn∥2
F]term.
EAtrn[∥WoptAtrn∥2
F] =E[Tr(AT
trnWT
optWoptAtrn)]
=σ2
trnˆγ2
ˆτ2E[Tr(AT
trnˆhTuTuˆhAtrn)]
+σ3
trnˆγ∥ˆt∥2
ˆτ2E[Tr(AT
trnˆhTuTuˆkTˆA†
trnAtrn)]
+σ3
trnˆβ∥ˆt∥2
ˆτ2E[Tr(AT
trn(ˆA†
trn)TˆkuTuˆhAtrn)]
+σ4
trn∥ˆt∥4
ˆτ2E[Tr(AT
trn(ˆA†
trn)TˆkuTuˆkTˆA†
trnAtrn)]
=σ2
trnˆγ2
ˆτ2E[Tr(ˆhAtrnAT
trnˆhT)]
+σ3
trnˆγ∥ˆt∥2
ˆτ2E[Tr(ˆkTˆA†
trnAtrnAT
trnˆhT)]
+σ3
trnˆγ∥ˆt∥2
ˆτ2E[Tr(ˆhAtrnAT
trn(ˆA†
trn)Tˆk)]
+σ4
trn∥ˆt∥4
ˆτ2E[Tr(ˆkTˆA†
trnAtrnAT
trn(ˆA†
trn)Tˆk)]
=σ2
trnˆγ2
ˆτ2E[Tr(ˆvT
trnˆA†
trnAtrnAT
trn(ˆA†
trn)TˆvT
trn)]
+σ3
trnˆγ∥ˆt∥2
ˆτ2E[Tr(uT(ˆA†
trn)TˆA†
trnAtrnAT
trn(ˆA†
trn)TˆvT
trn)]
+σ3
trnˆγ∥ˆt∥2
ˆτ2E[Tr(ˆvT
trnˆA†
trnAtrnAT
trn(ˆA†
trn)TˆA†
trnu)]
+σ4
trn∥ˆt∥4
ˆτ2E[Tr(uT(ˆA†
trn)TˆA†
trnAtrnAT
trn(ˆA†
trn)TˆA†
trnu)]
=σ2
trnˆγ2
ˆτ2E[Tr(ˆvT
trnˆA†
trnAtrnAT
trn(ˆA†
trn)TˆvT
trn)]
+σ4
trn∥ˆt∥4
ˆτ2E[Tr(uT(ˆA†
trn)TˆA†
trnAtrnAT
trn(ˆA†
trn)TˆA†
trnu)].
38Then, we look at the Tr((Xtrn−WoptXtrn)TWoptAtrn)term. By Lemma 2, we have Xtrn−
WoptXtrn=ˆγ
ˆτXtrn. Then,
ˆγ
ˆτTr(XT
trnWoptAtrn) =ˆγ
ˆτTr
XT
trnσtrnˆγ
ˆτuˆh+σ2
trn∥ˆt∥2
ˆτuˆkTˆA†
trn
Atrn
=σtrnˆγ2
ˆτ2Tr
XT
trnuˆhAtrn
+σ2
trnˆγ∥ˆt∥2
ˆτ2Tr
XT
trnuˆkTˆA†
trnAtrn
=σtrnˆγ2
ˆτ2Tr
σtrnvtrnˆvT
trnˆA†
trnAtrn
+σ2
trnˆγ∥ˆt∥2
ˆτ2Tr
σtrnvtrnuT(ˆA†
trn)TˆA†
trnAtrn
=σ2
trnˆγ2
ˆτ2Tr
ˆvT
trnˆA†
trnAtrnvtrn
+σ3
trnˆγ∥ˆt∥2
ˆτ2Tr
uT(ˆA†
trn)TˆA†
trnAtrnvtrn
=σ2
trnˆγ2
ˆτ2Tr
ˆvT
trnˆA†
trnAtrnvtrn
.
In conclusion, we have the training error:
EAtrn∥Xtrn−WoptYtrn∥2
F
n
=ˆγ2σ2
trn
nˆτ2+σ2
trnˆγ2
nˆτ2E[Tr(ˆvT
trnˆA†
trnAtrnAT
trn(ˆA†
trn)TˆvT
trn)]
+σ4
trn∥ˆt∥4
nˆτ2E[Tr(uT(ˆA†
trn)TˆA†
trnAtrnAT
trn(ˆA†
trn)TˆA†
trnu)]
+ 2σ2
trnˆγ2
nˆτ2Eh
Tr
ˆvT
trnˆA†
trnAtrnvtrni
.
Now we estimate the above terms using random matrix theory. Here we focus on the c <1case. For
c <1, we note that
ˆA†
trnAtrnAT
trn(ˆA†
trn)T=ˆVˆΣ−1ΣΣTˆΣ−1ˆVT.
Thus, for c <1
ˆvT
trnˆA†
trnAtrnAT
trn(ˆA†
trn)Tˆvtrn=dX
i=1a2
iσi(A)4
(σi(A)2+µ2)2
where aT=vT
trnV1:d. Taking the expectation, and using Lemma 9 we get that
EAtrnh
ˆvT
trnˆA†
trnAtrnAT
trn(ˆA†
trn)Tˆvtrni
=
c 
1
2+1 +µ2c−p
(1−c+µ2c)2+ 4c2µ2
2c+µ2 
1 +c+µ2c
2p
(1−c+cµ2)2+ 4c2µ2−1
2!!
+o(1).
Using Lemma 11, we see that the variance is o(1). Similarly, we have that
(ˆA†
trn)TˆA†
trnAtrnAT
trn(ˆA†
trn)TˆA†
trn=UˆΣ−2ΣΣTˆΣ−2UT.
Thus, again, using a similar argument, we see that
EAtrnh
Tr(uT(ˆA†
trn)TˆA†
trnAtrnAT
trn(ˆA†
trn)TˆA†
trnu)i
=1 +c+µ2c
2p
(1−c+cµ2)2+ 4c2µ2−1
2+o(1)
and again using Lemma 11, the variance is o(1). Finally,
ˆA†
trnAtrn=ˆVˆΣ−1ΣV.
39Thus,
Tr(ˆvT
trnˆA†
trnAtrnvtrn=dX
i=1a2
iσi(A)2
σi(A)2+µ2.
Thus, using Lemma 9, we get that
EAtrnh
Tr(ˆvT
trnˆA†
trnAtrnvtrni
=1
2+1 +µ2c−p
(1−c+µ2c)2+ 4c2µ2
2c+o(1)
and using Lemma 11, the variance is o(1). Then, similar to the proof of Theorem 1, we can simplify
the above expression to get the final result.
E.7 Proof of Proposition 1
Proposition 1 (Optimal σtrn).The optimal value of σ2
trnforc <1is given by
σ2
trn=σ2
tstd[2c(µ2+ 1)2−2T(cµ2+c+ 1) + 2( cµ2−2c+ 1)] + Ntst(µ2c2+c2+ 1−T)
Ntst(c3(µ2+ 1)2−T(µ2c2+c2−1)−2c2−1).
Proof. Letσ:=σ2
trnand
F=τ−2σ2
tst
Ntst+1
d(σ∥ˆh∥2
2+σ2∥ˆt∥4
2ρ)
.
Notice that only τis a function of σ,∥ˆh∥2
2,∥ˆt∥2
2, and∥ˆk∥2
2are all functions of µ. Then
∂F
∂σ=τ−21
d(∥ˆh∥2
2+ 2σ∥ˆt∥4
2ρ)−2τ−3∂τ
∂σσ2
tst
Ntst+1
d
σ∥ˆh∥2
2+σ2∥ˆt∥4
2ρ
=τ−21
d(∥ˆh∥2
2+ 2σ∥ˆt∥4
2ρ)−2τ−3∥ˆt∥2
2∥ˆk∥2
2σ2
tst
Ntst+1
d(σ∥ˆh∥2
2+σ2∥ˆt∥4
2ρ)
=τ−21
d(∥ˆh∥2
2+ 2σ∥ˆt∥4
2ρ)−2τ−1∥ˆt∥2
2∥ˆk∥2
2σ2
tst
Ntst+1
d(σ∥ˆh∥2
2+σ2∥ˆt∥4
2ρ)
.
The optimal σ∗satisfies∂F
∂σ|σ=σ∗= 0. Thus, we can solve the equation
τ−2= 0 or1
d(∥ˆh∥2
2+ 2σ∥ˆt∥4
2ρ)−2τ−1∥ˆt∥2
2∥ˆk∥2
2σ2
tst
Ntst+1
d(σ∥ˆh∥2
2+σ2∥ˆt∥4
2ρ)
.
Letα:=∥ˆt∥2
2∥ˆk∥2
2,δ:=dσ2
tst
Ntst. Then
τ−2= 0 = ⇒σ=−1
∥t∥2
2∥k∥2
2.
Notice that σ <0implies σtrnis an imaginary number, something we don’t want. Thus, we look at
the other expression.
0 =1
d(∥ˆh∥2
2+ 2σ∥ˆt∥4
2ρ)−2τ−1∥ˆt∥2
2∥k∥2
2σ2
tst
Ntst+1
d(σ∥ˆh∥2
2+σ2∥ˆt∥4
2ρ)
=1
d(∥ˆh∥2
2+ 2σ∥ˆt∥4
2ρ)−2τ−1αδ
d+1
d(σ∥ˆh∥2
2+σ2∥ˆt∥4
2ρ)
. [α=∥ˆt∥2
2∥ˆk∥2
2]
40Then multiplying through by dandτ
0 = (1 + ασ)(∥ˆh∥2
2+ 2σ∥ˆt∥4
2ρ)−2α(δ+σ∥ˆh∥2
2+σ2∥ˆt∥4
2ρ) [ τ= 1 + ασ]
=∥ˆh∥2
2+ 2∥ˆt∥4
2ρσ+α∥ˆh∥2
2σ+ 2α∥ˆt∥4
2ρσ2−2αδ−2α∥ˆh∥2
2σ−2α∥ˆt∥4
2ρσ2
=∥ˆh∥2
2+ 2∥ˆt∥4
2ρσ+α∥ˆh∥2
2σ−2αδ−2α∥ˆh∥2
2σ.
Then solving for σ, we get that
σ=2αδ− ∥ˆh∥2
2∥t∥4ρ−α∥ˆh∥2=2d∥ˆt∥2
2∥ˆk∥2
2σ2
tst− ∥ˆh∥2Ntst
Ntst(2∥ˆt∥4
2ρ− ∥ˆt∥2
2∥ˆk∥2
2∥ˆh∥2
2).
Then we use the random matrix theory lemmas to estimate this quantity.
41F Proof of low-rank case
Similar to the proof in [37], we conducted the low rank case.
We begin by defining some notation. Let ˆXtrn=UΣtrnVT
trn. Here Uisd×rwithUTU=I,Σtrn
isr×r, and Vtrnisr×(d+N). All of the following matrices are full rank.
1.ˆXtrnandˆXtstisd×(d+N)with rank r.ˆXtrn= [Xtrn 0]
2.ˆXtrn=UΣV, by the singular value decomposition. Let ˆXtst=UL.
3.Uisd×rwithUTU=Ir×r.VTis isr×(d+N).
4.Σtrnisr×r, with rank r.
5.ˆAtrn= [Atrn µI].
6.ˆAtrnisd×(N+d)with rank d.
7.ˆA†
trnˆAtrnis(N+d)×(N+d).
8.Hisr×(N+d), with rank r.
9.Kis(N+d)×r, with rank r.
10.Zisr×r, with rank r.
11.H1isr×r, with rank r.
12.ˆAtrn=ηtrnˆUˆΣˆVT.
13.ˆUisd×dunitary.
14.ˆΣisd×d.
For rank rdata and r < N , with c=d
N, the following is true.
1.We denote the minimum norm linear denoiser Woptby just Win this subsection. It is given
by
Wopt=−UΣtrnH−1
1KTˆA†
trn+UΣtrnH−1
1ZT(QQT)−1H
2. The test error when Xtst=ULis given by
EˆAtrn1
Ntst∥UΣtrnH−1
1ZT(QQT)−1Σ−1
trnL∥2
F+σ2
tst
d∥Wopt∥2
F
,
where Q=VT(I−ˆA†
trnˆAtrn),H=VT
trnˆA†
trn,1K=−ˆA†
trnUΣtrn,Z=I+VT
trnˆA†
trnUΣtrn,
H1=KTK+ZT(QQT)−1Z.
Forc <1, we have that if d < N then
E[Σ−1
trnKTKΣ−1
trn] = p
(1 +µ2c−c)2+ 4µ2c2−1−µ2c+c
2µ2c+o(1)!
Ir
and if d > N then
E[Σ−1
trnKTKΣ−1
trn] = p
4µ2c+ (−1 +c+µ2c)2−1−µ2c+c
2µ2c+o(1)!
Ir.
When d < N then
E[Σ−1
trnKTˆA†
trn(ˆA†
trn)TKΣ−1
trn] =µ2c2+c2+µ2c−2c+ 1
2µ4cp
4µ2c2+ (1−c+µ2c)2Ir+1
2µ4
1−1
c
Ir+o(1),
ifd > N then
E[Σ−1
trnKTˆA†
trn(ˆA†
trn)TKΣ−1
trn] =1−2c+c2+µ2c+µ2c2
2cµ4p
4µ2c+ (−1 +c+µ2c)2Ir+ (1−1
c)1
2µ4Ir+o(1).
42We have that
E[QQT] =c 
1
2+1 +µ2c−p
(−1 +c+µ2c)2+ 4µ2c
2c!
+o(1).
and
E[(QQT)−1] =2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2c+o(1).
When d < N we have that
E[HHT] =c 
1 +c+µ2c
2p
(1−c+µ2c)2+ 4c2µ2−1
2!
Ir+o(1)
and when d > N , we have
E[HHT] =c 
1 +c+µ2c
2p
(−1 +c+µ2c)2+ 4µ2c−1
2!
Ir+o(1).
When d < N , we have
E[∥W∥2
F] = 
µ2c2+c2+µ2c−2c+ 1
2µ4cp
4µ2c2+ (1−c+µ2c)2+1
2µ4
1−1
c!
Tr
 p
(1 +µ2c−c)2+ 4µ2c2−1−µ2c+c
2µ2cIr+2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cΣ−2!−2

+ 
2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2c!2
c 
1 +c+µ2c
2p
(1−c+µ2c)2+ 4c2µ2−1
2!
Tr(Σ−2)
Tr
 p
(1 +µ2c−c)2+ 4µ2c2−1−µ2c+c
2µ2cIr+2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cΣ−2!−2

+o(1),
when d > N this is estimated by
E[∥W∥2
F] = 
1−2c+c2+µ2c+µ2c2
2cµ4p
4µ2c+ (−1 +c+µ2c)2+ (1−1
c)1
2µ4!
Tr
 p
4µ2c+ (−1 +c+µ2c)2−1−µ2c+c
2µ2cIr+2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cΣ−2!−2

+ 
2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2c!2
c 
1 +c+µ2c
2p
(−1 +c+µ2c)2+ 4µ2c−1
2!
Tr(Σ−2)
Tr
 p
4µ2c+ (−1 +c+µ2c)2−1−µ2c+c
2µ2cIr+2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cΣ−2!−2

+o(1).
43When d < N the test error R(W, X tst)forW=Woptis given by
R(W, X tst) =1
NtstTr((p
4µ2c+ (−1 +c+µ2c)2−1−µ2c+c
2µ2cIr
+2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cΣ−2)−2)
2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cTr(Σ−2)
+σ2
tst
d 
µ2c2+c2+µ2c−2c+ 1
2µ4cp
4µ2c2+ (1−c+µ2c)2+1
2µ4
1−1
c!
Tr
 p
(1 +µ2c−c)2+ 4µ2c2−1−µ2c+c
2µ2cIr+2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cΣ−2!−2

+σ2
tst
d 
2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2c!2
c 
1 +c+µ2c
2p
(1−c+µ2c)2+ 4c2µ2−1
2!
Tr(Σ−2)
Tr
 p
(1 +µ2c−c)2+ 4µ2c2−1−µ2c+c
2µ2cIr+2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cΣ−2!−2

+o(1),
when d > N this is estimated by
R(W, X tst) =1
NtstTr((p
4µ2c+ (−1 +c+µ2c)2−1−µ2c+c
2µ2cIr
+2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cΣ−2)−2)
2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cTr(Σ−2)
+σ2
tst
d 
1−2c+c2+µ2c+µ2c2
2cµ4p
4µ2c+ (−1 +c+µ2c)2+ (1−1
c)1
2µ4!
Tr
 p
4µ2c+ (−1 +c+µ2c)2−1−µ2c+c
2µ2cIr+2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cΣ−2!−2

+σ2
tst
d 
2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2c!2
c 
1 +c+µ2c
2p
(−1 +c+µ2c)2+ 4µ2c−1
2!
Tr(Σ−2)
Tr
 p
4µ2c+ (−1 +c+µ2c)2−1−µ2c+c
2µ2cIr+2
1 +µ2c+c−p
(−1 +c+µ2c)2+ 4µ2cΣ−2!−2

+o(1).
G Experiments
All experiments were conducted using Pytorch and run on Google Colab using an A100 GPU. For
each empirical data point, we did at least 100 trials. The maximum number of trials for any experiment
was 20000 trials.
For each configuration of the parameters, Ntrn, Ntst, d, σ trn, σtst, andµ. For each trial, we sampled
u, vtrn, vtstuniformly at random from the appropriate dimensional sphere. We also sampled new
training and test noise for each trial.
44For the data scaling regime, we kept d= 1000 and for the parameter scaling regime, we kept
Ntrn= 1000 . For all experiments, Ntst= 1000 .
NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Both the abstract and introduction reference the main two Theorems (Theorem
4 and Theorem 2). Additionally, both put them in the context of prior work.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We believe the main purpose of the paper is to show that a certain phenomenon
exists and are very careful with our assumptions.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
453.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All statements have corresponding detailed proofs.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We have very few experiments. However, for each one we have a section in
the appendix with the needed details and have provided code as part of the supplementary
material.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
46In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We include the code as part of the submission. All data used is synthetic or
already open source.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We have very few experiments. However, for each one we have a section in
the appendix with the needed details and have provided code as part of the supplementary
material.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: The paper is a theory paper about mean behavior under a variety of concentra-
tion results.
Guidelines:
47• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Google Collab with an A100 was used.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification:
Guidelines: The paper conforms with the code of ethics.
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This is a theoretical work that helps build an understanding of existing phe-
nomena.
48Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This is a theoretical work
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We credit pytorch.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
49•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This is a theoretical work
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This is a theoretical work
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This is a theoretical work
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
50•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
51