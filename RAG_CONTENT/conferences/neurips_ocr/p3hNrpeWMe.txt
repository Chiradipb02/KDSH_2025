A Walsh Hadamard Derived Linear Vector Symbolic
Architecture
Mohammad Mahmudul Alam1, Alexander Oberle1, Edward Raff1,2, Stella Biderman2,
Tim Oates1,James Holt3
1University of Maryland, Baltimore County,2Booz Allen Hamilton,
3Laboratory for Physical Sciences
m256@umbc.edu ,aoberle1@umbc.edu ,Raff_Edward@bah.com ,
biderman_stella@bah.com ,oates@cs.umbc.edu ,holt@lps.umd.edu
Abstract
Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-
symbolic AI, where two vectors in Rdare ‘bound’ together to produce a new vector
in the same space. VSAs support the commutativity and associativity of this binding
operation, along with an inverse operation, allowing one to construct symbolic-
style manipulations over real-valued vectors. Most VSAs were developed before
deep learning and automatic differentiation became popular and instead focused
on efficacy in hand-designed systems. In this work, we introduce the Hadamard-
derived linear Binding (HLB), which is designed to have favorable computational
efficiency, and efficacy in classic VSA tasks, and perform well in differentiable
systems. Code is available at https://github.com/FutureComputing4AI/
Hadamard-derived-Linear-Binding .
1 Introduction
Vector Symbolic Architectures (VSAs) are a unique approach to performing symbolic style AI.
Such methods use a binding operation B:RdˆRdÝÑRd, where Bpx, yq“zdenotes that two
concepts/vectors xandyare connected to each other. In VSA, any arbitrary concept is assigned to
vectors in Rd(usually randomly). For example, the sentence “the fat cat and happy dog” would be
represented as Bpfat,catq`Bphappy ,dogq“S. One can then ask, “what was happy” by unbinding
the vector for happy, which will return a noisy version of the vector bound to happy. The unbinding
operation is denoted B˚px, yq, and so applying B˚pS,happyq«dog.
Because VSAs are applied over vectors, they offer an attractive platform for neuro-symbolic methods
by having natural symbolic AI-style manipulations via differentiable operations. However, current
VSA methods have largely been derived for classical AI tasks or cognitive science-inspired work.
Many such VSAs have shown issues in numerical stability, computational complexity, or otherwise
lower-than-desired performance in the context of a differentiable system.
As noted in [ 39], most VSAs can be viewed as a linear operation where Bpa, bq “aJGband
B˚pa, bq “aJFb, where GandFaredˆdmatrices. Hypothetically, these matrices could be
learned via gradient descent, but would not necessarily maintain the neuro-symbolic properties of
VSAs without additional constraints. Still, the framework is useful as all popular VSAs we are
aware fit within this framework. By choosing GandFwith specified structure, we can change the
computational complexity from Opd2q, down to Opdqfor a diagonal matrix.
In this work, we derive a new VSA that has multiple desirable properties for both classical VSA
tasks, and in deep-learning applications. Our method will have only Opdqcomplexity for the binding
step, is numerically stable, and equals or improves upon previous VSAs on multiple recent deep
38th Conference on Neural Information Processing Systems (NeurIPS 2024).learning applications. Our new VSA is derived from the Walsh Hadamard transform, and so we term
our method the Hadamard-derived linear Binding (HLB) as it will avoid the Opdlogdqnormally
associated with the Hadamard transform, and has better performance than more expensive VSA
alternatives.
Related work to our own will be reviewed in section 2, including our baseline VSAs and their
definitions. Our new HLB will be derived in section 3, showing it theoretically desirable properties.
section 4 will empirically evaluate HLB in classical VSA benchmark tasks, and in two recent deep
learning tasks, showing improved performance in each scenario. We then conclude in section 5.
2 Related Work
Smolensky [ 38] started the VSA approach with the Tensor Product Representation (TPR), where d
dimensional vectors (each representing some concept) were bound by computing an outer product.
Showing distributivity ( Bpx,y`zq“Bpx,yq`Bpx,zq) and associativity, this allowed specifying
logical statements/structures [ 13]. However, for ρtotal items to be bound together, it was impractical
due to the Opdρqcomplexity. [ 36,25,24] have surveyed many of the VSAs available today, but
our work will focus on three specific alternatives, as outlined in Table 1. The Vector-Derived
Transformation Binding (VTB) will be a primary comparison because it is one of the most recently
developed VSAs, which has shown improvements in what we will call “classic” tasks, where
the VSA’s symbolic like properties are used to manually construct a series of binding/unbinding
operations that accomplish a desired task. Note, that the VTB is unique in it is non-symmetric
(Bpx,yq‰Bpy,xq). Ours, and most others, are symmetric.
Table 1: The binding and initialization mechanisms for our new HLB with baseline methods. HLB is
related to the HRR in being derived via a similar approach, but replacing the Fourier transform Fp¨q
with the Hadamard transform (which simplifies out). The MAP is most similar to our approach in
mechanics, but the difference in derived unbinding steps leads to dramatically different performance.
The VTB is the most recently developed VSA in modern use. The matrix Vyof VTB is a block-
diagonal matrix composed from the values of the yvector, which we refer the reader to [ 12] for
details. The TorchHD library [15] is used for implementations of prior methods.
METHOD BINDBpx, yq UNBIND B˚px, yq INITx
HRR F´1pFpxqdFpyqqF´1pFpxqcFpyqq xi„Np0,1{dq
VTB Vyx VJ
yx ˜xi„Np0,1qÑx“˜x{}˜x}2
MAP-C xdy x dy x i„Up´1,1q
MAP-B xdy x dy x i„t´ 1,1u
HLB xdy x cy x u„tNp´µ,1{dq,Npµ,1{dqu
Next is the Holographic Reduced Representation (HRR) [ 32], which can be defined via the Fourier
transform Fp¨q. One derives the inverse operation of the HRR by defining the one vector ⃗1as the
identity vector and then solving Fpa˚qiFpaqi“1. We will use a similar approach to deriving
HLB but replacing the Fourier Transform with the Hadamard transform, making the HRR a key
baseline. Last, the Multiply Add Permute (MAP) [ 10] is derived by taking only the diagonal of
the tensor product from [ 38]’s TPR. This results in a surprisingly simple representation of using
element-wise multiplication for both binding/unbinding, making it a key baseline. The MAP binding
is also notable for its continuous (MAP-C) and binary (MAP-B) forms, which will help elucidate the
importance of the difference in our unbinding step compared to the initialization avoiding values near
zero. HLB differs in devising for the unbinding step, and we will later show an additional corrective
term that HLB employs for ρdifferent items bound together, that dramatically improve performance.
Our motivation for using the Hadamard Transform comes from its parallels to the Fourier Transform
(FT) used to derive the HRR and the HRR’s relatively high performance. The Hadamard matrix has
a simple recursive structure, making analysis tractable, and its transpose is its own inverse, which
simplifies the design of the inverse function B˚. Like the FT, WHT can be computed in log-linear
time, though in our case, the derivation results in linear complexity as an added benefit. The WHT is
already associative and distributive, making less work to obtain the desired properties. Finally, the
WHT involves only t´1,1uvalues, avoiding numerical instability that can occur with the HRR/FT.
2This work shows that these motivations are well founded, as they result in a binding with comparable
or improved performance in our testing.
Our interest in VSAs comes from their utility in both classical symbolic tasks and as useful priors in
designing deep learning systems. In classic tasks VSAs are popular for designing power-efficient
systems from a finite set of operations [ 14,23,17,30]. HRRs, in particular, have shown biologically
plausible models of human cognition [ 21,5,40,6] and solving cognitive science tasks [ 8]. In deep
learning the TPR has inspired many prior works in natural language processing [ 34,16,35]. To wit,
The HRR operation has seen the most use in differentiable systems [ 43,41,42,27,29,33,1,2,28]. To
study our method, we select two recent works that make heavy use of the neuro-symbolic capabilities
of HRRs. First, an Extreme Multi-Label (XML) task that uses HRRs to represent an output space of
tens to hundreds of thousands of classes Cin a smaller dimension dăC[9], and an information
privacy task that uses the HRR binding as a kind of “encrypt/decrypt” mechanism for heuristic
security [3]. We will explain these methods in more detail in the experimental section.
3 Methodology
First, we will briefly review the definition of the Hadamard matrix Hand its important properties that
make it a strong candidate from which to derive a VSA. With these properties established, we will
begin by deriving a VSA we term HLB where binding and unbinding are the same operation in the
same manner as which the original HRR can be derived [ 32]. Any VSA must introduce noise when
vectors are bound together, and we will derive the form of the noise term as η˝. Unsatisfied with the
magnitude of this term, we then define a projection step for the Hadamard matrix in a similar spirit to
‘[9]’s complex-unit magnitude projection to support the HRR and derive an improved operation with
a new and smaller noise term ηπ. This will give us the HLB bind/unbind steps as noted in Table 1.
Hadamard Hdis a square matrix of size dˆdof orthogonal rows consisting of only `1and´1s given
in Equation 1 where d“2n@nPN:ně0. Bearing in mind that Hadamard or Walsh-Hadamard
Transformation (WHT) can be equivalent to discrete multi-dimensional Fourier Transform (FT) when
applied to a ddimensional vector [ 26], it has additional advantages over Discrete Fourier Transform
(DFT). Unlike DFT, which operates on complex Cnumbers and requires irrational multiplications,
WHT only performs calculations on real Rnumbers with addition and subtraction operators and does
not require any irrational multiplication.
H1“r1s H2“„
1 1
1´1ȷ
¨¨¨ H2n“„
H2n´1H2n´1
H2n´1´H2n´1ȷ
(1)
Vector symbolic architectures (VSA), for instance, Holographic Reduced Representations (HRR) em-
ploys circular convolution to represent compositional structure which is computed using Fast Fourier
Transform (FFT) [ 32]. However, it can be numerically unstable due to irrational multiplications of
complex numbers. Prior work [ 9] devised a projection step to mitigate the numerical instability of
the FFT and it’s inverse, but we instead ask if re-deriving the binding/unbinding operations may yield
favorable results if we use the favorable properties of the Hadamard transform as given in Lemma 3.1.
Lemma 3.1 (Hadamard Properties) .LetHbe the Hadamard matrix of size dˆdthat holds the
following properties for x, yPRd. First, HpHxq“dx, and second Hpx`yq“Hx`Hy.
The bound composition of two vectors into a single vector space is referred to as BINDING . The
knowledge retrieval from a bound representation is known as UNBINDING . We define the binding
function by replacing the Fourier transform in circular convolution with the Hadamard transform
given in Definition 3.1. We will denote the binding function four our specific method by Band the
unbinding function by B˚.
Definition 3.1 (Binding and Unbinding) .The binding of vectors x, yPRdin Hadamard domain is
defined in Equation 2 where dis the elementwise multiplication. The unbinding function is defined
in a similar fashion, i.e., B“B˚. In the context of binding, Bpx, yqcombines the vectors xandy,
whereas in the context of unbinding B˚px, yqrefers to the retrieval of the vector associated with y
from x.
Bpx, yq“1
d¨HpHxdHyq (2)
3Now, we will discuss the binding of ρdifferent representations, which will become important later in
our analysis but is discussed here for adjacency to the binding definition. Composite representation in
vector symbolic architectures is defined by the summation of the bound vectors. We define a parameter
ρPN:ρě1that denotes the number of vector pairs bundled in a composite representation. Given
vectors xi, yiPRdand@iPN: 1ďiďρ, we can define the composite representation χas
χ
ρ“1“Bpx1, y1q χ
ρ“2“Bpx1, y1q`Bpx2, y2q ¨¨¨ χρ“ρÿ
i“1Bpxi, yiq (3)
Next, we require the unbinding operation, which is defined via an inverse function via the following
theorem. This will give a symbolic form of our unbinding step that retrieves the original component
xbeing searched for, as well as a necessary noise component η˝, which must exist whenever ρě2
items are bound together without expanding the dimension d.
Theorem 3.1 (Inverse Theorem) .Given the identity function Hx¨Hx:“1where x:is the inverse
ofxin Hadamard domain, then B˚pBpx1, y1q`¨¨¨` Bpxρ, yρq, y:
iq“"xi ifρ“1
xi`η˝
i elseρą1
where xi, yiPRdandη˝
iis the noise component.
Proof of Theorem 3.1. We start from the identity function Hx¨Hx:“1and thus Hx:“1
Hx. Now
using Equation 2 we get,
B˚pBpx1, y1q`¨¨¨` Bpxρ, yρq, y:
iq“1
d¨HppHx1dHy1`¨¨¨` HxρdHyρqd1
Hyiq
“1
d¨HpHxi`1
Hyidρÿ
j“1
j‰ipHxjdHyjqq“xi`1
d¨Hp1
Hyidρÿ
j“1
j‰ipHxjdHyjqqLemma 3.1
“"xi ifρ“1
xi`η˝
i elseρą1
To reduce the noise component and improve the retrieval accuracy, [ 9,32] proposes a projection
step to the input vectors by normalizing them by the absolute value in the Fourier domain. While
such identical normalization is not useful in the Hadamard domain since it will only transform the
elements to`1and´1s, we will define a projection step with only the Hadamard transformation
without normalization given in Definition 3.2.
Definition 3.2 (Projection) .The projection function of xis defined by πpxq“1
d¨Hx.
If we apply the Definition 3.2 to the inputs in Theorem 3.1 then we get
B˚pBpπpx1q, πpy1qq`¨¨¨` Bpπpxρq, πpyρqq, πpyiq:q“B˚p1
d¨Hpx1dy1`¨¨¨ xρdyρq,1
yiq
“1
d¨Hp1
yidpx1dy1`¨¨¨ xρdyρqq
(4)
The retrieved value would be projected onto the Hadamard domain as well and to get back the original
data we apply the reverse projection. Since the inverse of the Hadamard matrix is the Hadamard
matrix itself, in the reverse projection step we just apply the Hadamard transformation again which
derives the output to
Hp1
d¨Hp1
yidpx1dy1`¨¨¨ xρdyρqqq“1
yidpx1dy1`¨¨¨` xρdyρq
“$
&
%xi ifρ“1
xi`ρř
j“1, j‰ixjyj
yielseρą1“"xi ifρ“1
xi`ηπ
i elseρą1(5)
where ηπ
iis the noise component due to the projection step. In expectation, ηπ
iăη˝
i(see Appendix A).
Thus, the projection step diminishes the accumulated noise. More interestingly, the retrieved output
4term does not contain any Hadamard matrix. Therefore, we can recast the initial binding definition
by multiplying the query vector yito the output of Equation 5 which makes the binding function
as the sum of the element-wise product of the vector pairs and the compositional structure a linear
timeOpnqrepresentation. Thus, the redefinition of the binding function is B1px, yq“xdyandρ
bundle of the vector pairs is χ1
ρ“řρ
i“1pxidyiq. Consequently, the unbinding would be a simple
element-wise division of the bound representation by the query vector, i.e, B˚1px, yq“xd1
ywhere
xandyare the bound and query vector, respectively.
3.1 Initialization of HLB
For the binding and unbinding operations to work, vectors need to have an expected value of zero.
However, since we would divide the bound vector with query during unbinding, values close to zero
would destabilize the noise component and create numerical instability. Thus, we define a Mixture
ofNormal Distribution (MiND) with an expected value of zero but an absolute mean greater than
zero given in Equation 6 where Uis the Uniform distribution. Considering half of the elements are
sampled for a normal distribution of mean ´µand the rest of the half with a mean of µ, the resulting
vector has a zero mean with an absolute mean of µ. The properties of the vectors sampled from a
MiND distribution are given in Properties 3.1.
Ωpµ,1{dq“"Np´µ,1{dq ifUp0,1qą0.5
Npµ,1{dq elseUp0,1qď0.5(6)
Properties 3.1 (Initialization Properties) .LetxPRdsampled from Ωpµ,1{dqholds the following
properties. Erxs“0,Er|x|s“µ, and}x}2“a
µ2d
3.2 Similarity Augmentation
In VSAs, it is common to measure the similarity with an extracted embedding ˆxwith some other
vector xusing the cosine similarity. For our HLB, we devise a correction term when it is known that
ρitems have been bound together to extract ˆx, i.e.,B˚pχρ,zq“ˆx. Then if ˆxis the noisy version
of the true bound term x, we want cossimpˆx,xq“1, and cossimpˆx,yq“0,@y‰x. We achieve
this by instead computing cossimpˆx,xq¨?ρ, and the derivation of this corrective term is given by
Theorem 3.2.
Theorem 3.2 (ϕ–ρRelationship) .Given xi, yi„Ωpµ,1{dq @iPN: 1ďiďρ, the cosine
similarity ϕbetween the original xiand retrieved vector ˆxiis approximately equal to the inverse
square root of the number of vector pairs in a composite representation ρgiven by ϕ«1?ρ.
Proof of Theorem 3.2. We start with the definition of cosine similarity and insert the value of ˆxi. The
step-by-step breakdown is shown in Equation 7.
ϕ“dřxi¨ˆxi
}xi}2¨}ˆxi}2“dřxi¨˜
xi`ρř
j“1, j‰ixjyj
yi¸
}xi}2¨}xi`ρř
j“1, j‰ixjyj
yi}2“dřxi¨xi`dřxi¨˜
ρř
j“1, j‰ixjyj
yi¸
}xi}2¨}xi`ρř
j“1, j‰ixjyj
yi}2(7)
Employing Properties 3.1 we can derive that }xi}2“ařxi¨xi“a
µ2dand}xjyj
xi}“a
µ2d.
Thus, the square of the }xi`ρř
j“1, j‰ixjyj
yi}2can be expressed as
“}xi}2
2`ρÿ
j“1, j‰i››››xjyj
yi››››2
2`2¨dÿ
xi˜ρÿ
j“1, j‰ixjyj
yi¸
looooooooooooomooooooooooooon
α`dÿρ´1ÿ
j“1
j‰iρ´1ÿ
l“1
l‰jxjyj
yi¨xlyl
yi
loooooooooooomoooooooooooon
β
“µ2d`pρ´1q¨µ2d`2α`2β“ρ¨µ2d`2α`2β(8)
Therefore, using Equation 7 and Equation 8 we can write that
Erϕs“µ2d`αa
µ2d¨a
ρ¨µ2d`2α`2β«1µ2da
µ2d¨a
ρ¨µ2d“µ2d?ρ¨µ2d“1?ρ
5The experimental result of the ϕ´ρrelationship closely follows the theoretical expectation provided
in Appendix C which also indicates that the approximation is valid. Since, we know from Theorem 3.2
that similarity score ϕdrops by the inverse square root of the number of vector pairs in a composite
representation ρ, in places where ρis known or can be estimated from››χρ››
2«µ2?ρ¨d(proof in
Appendix B), it can be used to update the cosine similarity multiplying the scores by?ρ. Equation 9
shows the updated similarity score where in a positive case p`q,ϕwould be close to 1{?ρand in a
negative casep´q,ϕwould be close to zero.
ϕ1“ϕˆ?ρ ϕ1
p`q“ϕÑ1?ρˆ?ρ«1 ϕ1
p´q“ϕÑ0ˆ?ρ«0 (9)
5 10 15 20 25 30 35 40 45 50
No. of bundled pairs ρ246810n= log2(d)φ/prime
(+)=φ→1/√ρ×√ρ≈1
5 10 15 20 25 30 35 40 45 50
No. of bundled pairs ρ246810n= log2(d)φ/prime
(−)=φ→0×√ρ≈0
0.00.20.40.60.81.0
Figure 1: Empirical comparison of the corrected cosine similarity scores between ϕ1
p`q(on top) and
ϕ1
p´q(on bottom) for varying nandρshown in heatmap. The dimension, i.e., d“2nis varied from
2to1024pnPt1,2,¨¨¨,10uqand the number of vector pairs bundled is varied from 1to50. This
shows that we can accurately identify when a vector xhas been bound to a VSA or not when we
keep track of how many pairs of terms ρare included.
Empirical results of ϕ1for varying nandρare visualized and verified by a heatmap. In a composite
representation χ1
ρ“řρ
i“1pxidyiq, when unbinding is applied using the query yi, i.e.,B˚1pχ1
ρ, yiq“
ˆxi, a positive case is a similarity between xiandˆxi. On the contrary, similarity between ˆxiand any
xjwhere jPt1,2,¨¨¨, ρuandj‰i, is a negative case. Mean cosine similarity scores of 100trials
for both positive and negative cases in presented in Figure 1 where the scores for the positive cases
are in the redp«1qshades and the scores for the negative cases are in the blue p«0qshades.
4 Empirical Results
4.1 Classical VSA Tasks
A common VSA task is, given a bundle (addition) of ρpairs of bound vectors s“řρ
i“1Bpxi,yiq,
given a query xqPs, c can the corresponding vector yqbe correctly retrieved from the bundle. To
test this, we perform an experiment similar to one in [ 37]. We first create a pool PofN“1000
random vectors, then sample (with replacement) ppairs of vectors for pPt1,2,¨¨¨,25u. The pairs
are bound together and added to create a composite representation s. Then, we iterate through all left
pairsxqin the composite representation and attempt to retrieve the corresponding yq,@qPr1, ps. A
retrieval is considered correct if B˚ps,xqqJyqąB˚ps,xqqJyj,@j‰q. The total accuracy score
for the bundle is recorded, and the experiment is repeated for 50trials. Experiments are performed to
1Here, αandβare the noise terms and in expectation Erαs«0andErβs«0.
6compare HRR [ 32], VTB [ 12], MAP [ 10], and our HLB VSAs. For each VSA, at each dimension
of the vector, the area under the curve (AUC) of the accuracy vs. the no. of bound terms plot is
computed, and the results are shown in Figure 2. In general, HLB has comparable performance to
HRR and VTB, and performs better than MAP.
122162202242282322362
Dimensiond0.50.60.70.80.91.0AUC (%)HRR
VTB
MAP
HLB
Figure 2: The area under the accuracy curve due to the change of no. of bundled pairs ρfor dimensions
d. All the dimensions are chosen to be perfect squares due to the constraint of VTB.
The scenario we just considered looked at bindings of only two items together, summed of many
pairs of bindings. [ 12] proposed addition evaluations over sequential bindings that we now consider.
In the random case we have an initial vector b0, and for prounds, we will modify it by a random
vector xtsuch that bt`1“Bpbt,xtq, after which we unbind each xtto see how well the previous bt
is recovered. In the auto binding case, we use a single random vector xfor all prounds.
In this task, we are concerned with the quality of the similarity score in random/auto-binding, as we
wantB˚pbt`1,xtqJbt“1. For VSAs with approximate unbinding procedures, such as HRR, VTB,
and MAP-C, the returned value will be 1 if p“1but will decay as pincreases. HLBuses an exact
unbinding procedure so that the returned value is expected to be 1 @p. We are also interested in the
magnitude of the vectors }B˚pbt`1,xtq}2, where an ideal VSA has a constant magnitude that does
not explode/vanish as pincreases.
5 10 15 20 250.00.20.40.60.81.0SimilarityφRandom
5 10 15 20 250.00.20.40.60.81.0Auto
5 10 15 20 25
No. of vectors p10−3100Magnitude
5 10 15 20 25
No. of vectors p103107
HRR VTB MAP HLB
Figure 3: When repeatedly binding different random (left) or a single vector (right), HLB consistently
returns the ideal similarity score of 1 for a present item (top row) and has a constant magnitude
(bottom row), avoiding exploding/vanishing values.
Figure 3 shows that HLB maintains a stable magnitude regardless of the number of bound vectors in
both cases. This property arises due to the properties of the distribution shown in Properties 3.1. As all
components have an expected absolute value of 1, the product of all components also has an expected
absolute value of 1. Thus, the norm of the binding is simply?
d. It also shows HLB maintains the
desired similarity score as pincreases. Combined with Figure 1 that shows the scores are near-zero
when an item is not present, HLB has significant advantages in consistency for designing VSA
solutions.
74.2 Deep Learning with Hadamard-derived Linear Binding
Two recent methods that integrate HRR with deep learning are tested to further validate our approach
and briefly summarized in the two sub-sections below. In each case, we run all four VSAs and see that
HLB either matches or exceeds the performance of other VSAs. In every experiment, the standard
method of sampling vectors from each VSA is followed as outlined in Table 1. All the experiments
are performed on a single NVIDIA TESLA PH402 GPU with 32GB memory.
4.2.1 Connectionist Symbolic Pseudo Secrets
When running on low-power computing environments, it is often desirable to offload the computation
to a third-party cloud environment to get the answer faster and use fewer local resources. However,
this may be problematic if one does not fully trust the available cloud environments. Homomorphic
encryption (HE) is the ideal means to alleviate this problem, providing cryptography for computations.
HE is currently more expensive to perform than running a neural network itself [ 11], defeating its
own utility in this scenario. Connectionist Symbolic Pseudo Secrets (CSPS) [ 3] provides a heuristic
means of obscuring the nature of the input (content), and output (number of classes/prediction), while
also reducing the total local compute required.
CSPS mimics a “one-time-pad” by taking a random VSA vector sas the secret and binding it to the
inputx. The value Bps,xqobscures the original x, and the third-party runs the bulk of the network
on their platform. A result ˜yis returned, and a small local network computes the final answer after
unbinding with the secret B˚p˜y,sq. Other than changing the VSA used, we follow the same training,
testing, architecture size, and validation procedure of [3].
CSPS experimented with 5 datasets MNIST, SVHN, CIFAR-10 (CR10), CIFAR-100 (CR100), and
Mini-ImageNet (MIN). First, we look at the accuracy of each method, which is lower due to the noise
of the random vector sadded at test time since no secret VSA is ever reused. The results are shown
in Table 2, where HLB outperforms all prior methods significantly. Notably, the MAP VSA is second
best despite being one of the older VSAs, indicating its similarity to HLB in using a simple binding
procedure, and thus simple gradient may be an important factor in this scenario.
Table 2: Accuracy comparison of the proposed HLB with HRR, VTB, MAP-C, and MAP-B in CSPS.
The dimensions of the inputs along with the no. of classes are listed in the Dims/Labels column. The
last row shows the geometric mean of the results.
DATASETDIMS/
LABELSCSPS + HRR CSPS + VTB CSPS + MAP-C CSPS + MAP-B CSPS + HLB
Top@1 Top@5 Top@1 Top@5 Top@1 Top@5 Top@1 Top@5 Top@1 Top@5
MNIST 282{10 98 .51 – 98.44 – 98.46 – 98.40 – 98.73 –
SVHN 322{10 88 .44 – 19.59 – 79.95 – 92.43 – 94.53 –
CR10 322{10 78 .21 – 74.22 – 76.69 – 82.83 – 83.81 –
CR100 322{100 48 .84 75 .82 35 .87 61 .79 56 .77 81 .52 57 .76 84 .63 58.82 87 .50
MIN 842{100 40 .99 66 .99 45 .81 73 .52 52 .22 78 .63 57 .91 82 .81 59.48 83 .35
GM 67.14 71 .26 47 .24 67 .40 70 .89 80 .06 75 .90 83 .72 77.17 85 .40
However, improved accuracy is not useful in this scenario if more information is leaked. The test in
this scenario, as proposed by [ 3], is to calculate the Adjusted Rand Index (ARI) after attempting
to cluster the inputs xand the outputs ˆy, which are available/visible to the snooping third-party. To
be successful, the ARI must be near zero (indicating random label assignment) for both inputs and
outputs.
We use K-means, Gaussian Mixture Model (GMM), Birch [ 45], and HDBSCAN [ 7] as the clustering
algorithms and specify the true number of classes to each method to maximize attacker success
(information they would not know). The results can be found in Table 3, where the top rows indicate
the clustering of the input Bpx,sq, and the bottom rows the clustering of the output ˆy. All the
numbers are percentages p%q, showing all methods do a good job at hiding information from the
adversary (except on the MNIST dataset, which is routinely degenerate).
The MNIST result is a good reminder that CSPS security is heuristic, not guaranteed. Nevertheless,
we see HLB has consistently close-to-zero scores for SVHN, CIFARs, and Mini-ImageNet, indicating
8Table 3: Clustering results of the main network inputs (top rows) and outputs (bottom rows) in terms
of Adjusted Rand Index (ARI). Because CSPS is trying to hide information, scores near zero are
better. Cell color corresponds to the cell absolute value, with blue indicating lower ARI and red
indicating higher ARI. All numbers in percentages, and show HLB is better at information hiding.
CLUSTERING
METHODSHRR VTB
MNIST SVHN CR10 CR100 MIN MNIST SVHN CR10 CR100 MIN
K-M EANS´0.02´0.01 0.18 0.54 0.42´0.00´0.01´0.01 0.02 0.00
0.00.20.40.60.81.0
GMM 0.01 0.00 0.09 0.61 0.44 4.67 1.37´0.01 0.02 0.01
BIRCH 0.20 0.00 0.14 0.45 0.35 0.02 0.03 0.04 0.08 0.03
HDBSCAN 0.00´0.24 1.23 0.01 0.02 0.00 0.00 0.00 0.00 0.00
K-M EANS 1.28 0.06 0.21 0.03 0.08 8.52 0.13 1.11 0.05 0.12
GMM 1.28 0.06 0.17 0.04 0.09 8.63 0.14 1.63 0.05 0.00
BIRCH 1.51 0.03 0.13 0.05 0.07 3.24 0.00 0.64 0.06 0.17
HDBSCAN 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.00 0.00 0.00
CLUSTERING
METHODSMAP HLB
MNIST SVHN CR10 CR100 MIN MNIST SVHN CR10 CR100 MIN
K-M EANS 0.17 0.01 0.01 0.00 0.00 0.09 0.00 0.00 0.00 0.00
GMM 3.39´0.01 0.01 0.00 0.00 2.53 0.00 0.00 0.00 0.00
BIRCH 0.84´0.00 0.00 0.01 0.00 0.83 0.00 0.00 0.01 0.00
HDBSCAN 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
K-M EANS 15.91 0.09 0.00 0.03 0.01 13.67´0.04 0.01 0.02´0.00
GMM 42.43 0.11 0.00 0.03 0.00 14.96´0.04 0.01 0.02 0.00
BIRCH 7.09´0.07´0.02 0.01´0.00 18.44´0.07 0.00 0.01 0.02
HDBSCAN 0.48 0.00 0.00 0.00 0.00 7.60 0.01 0.00 0.00 0.00
that its improved accuracy with simultaneously improved security. This also validates the use of the
VSA in deep learning architecture design and the efficacy of our approach.
4.2.2 Xtreme Multi-Label Classification
Extreme Multi-label (XML) is the scenario where, given a single input of size d,Cąądclasses are
used to predict. This is common in e-commerce applications where new products need to be tagged,
and an input on the order of d«5000 is relatively small compared to Cě100,000 or more classes.
This imposes unique computational constraints due to the output space being larger than the input
space and is generally only solvable because the output space is sparse — often less than 100 relevant
classes will be positive for any one input. VSAs have been applied to XML by exploiting the low
positive class occurrence rate to represent the problem symbolically [9].
While many prior works focus on innovative strategies to cluster/make hierarchies/compress the
penultimate layer[ 19,20,31,18,44,22], a neuro-symbolic approach was proposed by [ 9]. Given K
total possible classes, they assigned each class a vector ckto be each class’s representation, and the
set of all classes a“řK
k“1ck.
The VSA trick used by [ 9] was to define an additional “present” class pand a “missing” class m.
Then, the target output of the network fp¨qis itself a vector composed of two parts added together.
First,Bpp,ř
kckqrepresents all present classes, and so the sum is over a finite smaller set. Then
the absent classes compute the missing representing Bpm,a´ř
kckq, which again only needs to
compute over the finite set of present classes, yet represents the set of all non-present classes by
exploiting the symbolic properties of the VSA.
For XML classification, we have a set of Kclasses that will be present for a given input, where
K«10is the norm. Yet, there will be Ltotal possible classes where Lě100,000is quite common.
Forming a normal linear layer to produce Loutputs is the majority of computational work and
memory use in standard XML models, and thus the target for reduction. A VSA can be used to
side-step this cost, as shown by [ 9], by leveraging the symbolic manipulation of the outputs. First,
consider the target label as a vector sPRdsuch that d!L. By defining a VSA vector to represent
“present” and “missing” classes as pandm, where each class is given its own vector c1,...,L, we can
shift the computational complexity form OpLqtoOpKqby manipulating the “missing” classes as
the compliment of the present classes as shown in Equation 10.
9Similarly, the loss to calculate the gradient can be computed based on the network’s prediction
ˆsby taking the cosine similarity between each expected class and one cosine similarity for the
representation of all missing classes. The excepted response of 1 or 0 for an item being present/absent
from the VSA is used to determine if we want the similarity to be 0 (1- cos) or 1 (just cos), as shown
in Equation 11.
s“Labels Present OpdKqhkkkkkkkikkkkkkkjÿ
iPyi“1Bpp,ciq `Labels Absent OpdLqhkkkkkkkkkikkkkkkkkkjÿ
jPyj“´1Bpm,cjq“Labels Present Opd Kqhkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkj
B˜
p,˜
a—ÿ
iPyi“1ci¸¸
`Labels Absent OpdKqhkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkj
B˜
m,˜
a´ÿ
iPyi“1ci¸¸
(10)
loss“Present Classes Opd Kqhkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkjÿ
iPyi“1p1´cospB˚pp,ˆsq,ciqq`Absent classes Opd Kqhkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkj
cos˜
B˚pm,ˆsq,ÿ
iPyi“1ci¸
(11)
The details and network sizes of [ 9] are followed, except we replace the original VSA with our
four candidates. The network is trained on 8datasets listed in Table 4 from [ 4] and evaluated
using normalized discounted cumulative gain (nDCG) and propensity-scored (PS) based normalized
discounted cumulative gain (PSnDCG) as suggested by [19].
Table 4: XML classification results in dense label representation with HRR, VTB, MAP, and HLB in
terms of nDCG and PSnDCG. The proposed HLB has attained the best nDCG and PSnDCG scores
on all the datasets setting a new SOTA.
DATASET BIBTEX DELICIOUS MEDIAMILL EURL EX-4K
METRICS nDCG PSnDCG nDCG PSnDCG nDCG PSnDCG nDCG PSnDCG
HRR 60.296 45 .572 66 .454 30 .016 83 .885 63 .684 77 .225 30 .684
VTB 57.693 45 .219 63 .325 31 .449 87 .232 66 .948 76 .964 31 .180
MAP-C 59.280 46 .092 65 .376 31 .943 87 .255 66 .886 72 .439 26 .752
MAP-B 59.412 46 .340 65 .431 32 .122 86 .886 66 .562 71 .128 26 .340
HLB 61.741 48 .639 67 .821 32 .797 88 .064 67 .525 77 .868 31 .526
DATASET EURL EX-4.3K W IKI10-31K A MAZON -13K D ELICIOUS -200K
METRICS nDCG PSnDCG nDCG PSnDCG nDCG PSnDCG nDCG PSnDCG
HRR 84.497 38 .545 81 .068 9 .185 93 .258 49 .642 44 .933 6 .839
VTB 84.663 38 .540 78 .025 9 .645 92 .373 49 .463 44 .092 6 .664
MAP-C 85.472 39 .233 80 .203 10 .027 92 .013 48 .686 45 .373 6 .862
MAP-B 85.023 38 .820 80 .238 10 .035 92 .307 48 .812 45 .459 6 .870
HLB 88.204 43 .622 83 .589 11 .869 93 .672 50 .270 46 .331 6 .952
The classification result in terms of nDCG and PSnDCG in all the eight datasets is presented in
Table 4 where the top four datasets are comparatively easy with maximum no. of features of 5000
and no. of labels of 4000 . The bottom four datasets are comparatively hard with the no. of features
and labels on the scale of 100K. The proposed HLB has attained the best results in all the datasets
on both metrics. In contrast to the prior CSPS results, here we see that the performance differences
between HRR, VTB, and MAP are more varied, with no clear “second-place” performer.
5 Conclusion
In this paper, a novel linear vector symbolic architecture named HLB is presented derived from
Hadamard transform. Along with an initialization condition named MiND distribution is proposed for
which we proved the cosine similarity ϕis approximately equal to the inverse square root of the no.
of bundled vector pairs ρwhich matches with the experimental results. The proposed HLB showed
superior performance in classical VSA tasks and deep learning compared to other VSAs such as
HRR, VTB, and MAP. In learning tasks, HLB is applied to CSPS and XML classification tasks. In
both of the tasks, HLB has achieved the best results in terms of respective metrics in all the datasets
showing a diverse potential of HLB in Neuro-symbolic AI.
10References
[1]Mohammad Mahmudul Alam, Edward Raff, Stella Biderman, Tim Oates, and James Holt.
Recasting self-attention with holographic reduced representations. In Proceedings of the 40th
International Conference on Machine Learning , ICML’23. JMLR.org, 2023.
[2]Mohammad Mahmudul Alam, Edward Raff, and Tim Oates. Towards generalization in subitiz-
ing with neuro-symbolic loss using holographic reduced representations. Neuro-Symbolic
Learning and Reasoning in the era of Large Language Models , 2023.
[3]Mohammad Mahmudul Alam, Edward Raff, Tim Oates, and James Holt. Deploying convo-
lutional networks on untrusted platforms using 2D holographic reduced representations. In
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato,
editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of
Proceedings of Machine Learning Research , pages 367–393. PMLR, 17–23 Jul 2022.
[4]K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y . Prabhu, and M. Varma. The extreme
classification repository: Multi-label datasets and code, 2016.
[5]Peter Blouw and Chris Eliasmith. A neurally plausible encoding of word order information into a
semantic vector space. 35th Annual Conference of the Cognitive Science Society , 35:1905–1910,
2013.
[6]Peter Blouw, Eugene Solodkin, Paul Thagard, and Chris Eliasmith. Concepts as semantic
pointers: A framework and computational model. Cognitive Science , 40(5):1128–1162, 7 2016.
[7]Ricardo JGB Campello, Davoud Moulavi, and Jörg Sander. Density-based clustering based on
hierarchical density estimates. In Pacific-Asia conference on knowledge discovery and data
mining , pages 160–172. Springer, 2013.
[8]C. Eliasmith, T. C. Stewart, X. Choo, T. Bekolay, T. DeWolf, Y . Tang, and D. Rasmussen. A
large-scale model of the functioning brain. Science , 338(6111):1202–1205, 11 2012.
[9]Ashwinkumar Ganesan, Hang Gao, Sunil Gandhi, Edward Raff, Tim Oates, James Holt, and
Mark McLean. Learning with holographic reduced representations. Advances in neural
information processing systems , 34:25606–25620, 2021.
[10] Ross W. Gayler. Multiplicative binding, representation operators & analogy (workshop poster),
1998.
[11] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John
Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and
accuracy. In International conference on machine learning , pages 201–210. PMLR, 2016.
[12] Jan Gosmann and Chris Eliasmith. Vector-derived transformation binding: An improved binding
operation for deep symbol-like processing in neural networks. Neural Comput. , 31(5):849–869,
2019.
[13] Klaus Greff, Sjoerd van Steenkiste, and Jürgen Schmidhuber. On the binding problem in
artificial neural networks. arXiv , 2020.
[14] Robert Guirado, Abbas Rahimi, Geethan Karunaratne, Eduard Alarcón, Abu Sebastian, and
Sergi Abadal. Whype: A scale-out architecture with wireless over-the-air majority for scalable
in-memory hyperdimensional computing. IEEE Journal on Emerging and Selected Topics in
Circuits and Systems , 13(1):137–149, 2023.
[15] Mike Heddes, Igor Nunes, Pere VergÃ ©s, Denis Kleyko, Danny Abraham, Tony Givargis,
Alexandru Nicolau, and Alexander Veidenbaum. Torchhd: An open source python library to
support research on hyperdimensional computing and vector symbolic architectures. Journal of
Machine Learning Research , 24(255):1–10, 2023.
11[16] Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, and Dapeng Wu. Tensor product
generation networks for deep nlp modeling. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers) , pages 1263–1273. Association for Computational
Linguistics, jun 2018.
[17] Mohsen Imani, Deqian Kong, Abbas Rahimi, and Tajana Rosing. V oicehd: Hyperdimen-
sional computing for efficient speech recognition. In 2017 IEEE International Conference on
Rebooting Computing (ICRC) , pages 1–8. IEEE, nov 2017.
[18] Himanshu Jain, Venkatesh Balasubramanian, Bhanu Chunduri, and Manik Varma. Slice: Scal-
able linear extreme classifiers trained on 100 million labels for related searches. In Proceedings
of the twelfth ACM international conference on web search and data mining , pages 528–536,
2019.
[19] Himanshu Jain, Yashoteja Prabhu, and Manik Varma. Extreme multi-label loss functions for
recommendation, tagging, ranking & other missing label applications. In Proceedings of the
22nd ACM SIGKDD international conference on knowledge discovery and data mining , pages
935–944, 2016.
[20] Ankit Jalan and Purushottam Kar. Accelerating extreme classification via adaptive feature
agglomeration. arXiv preprint arXiv:1905.11769 , 2019.
[21] Michael N. Jones and Douglas J.K. Mewhort. Representing word meaning and order information
in a composite holographic lexicon. Psychological Review , 114(1):1–37, 2007.
[22] Armand Joulin, Moustapha Cissé, David Grangier, Hervé Jégou, et al. Efficient softmax
approximation for gpus. In International conference on machine learning , pages 1302–1310.
PMLR, 2017.
[23] Denis Kleyko, Mike Davies, Edward Paxon Frady, Pentti Kanerva, Spencer J. Kent, Bruno A. Ol-
shausen, Evgeny Osipov, Jan M. Rabaey, Dmitri A. Rachkovskij, Abbas Rahimi, and Friedrich T.
Sommer. Vector symbolic architectures as a computing framework for emerging hardware.
Proceedings of the IEEE , 110(10):1538–1571, 2022.
[24] Denis Kleyko, Dmitri Rachkovskij, Evgeny Osipov, and Abbas Rahimi. A survey on hyperdi-
mensional computing aka vector symbolic architectures, part ii: Applications, cognitive models,
and challenges. ACM Comput. Surv. , 55(9), jan 2023.
[25] Denis Kleyko, Dmitri A. Rachkovskij, Evgeny Osipov, and Abbas Rahimi. A survey on hyperdi-
mensional computing aka vector symbolic architectures, part i: Models and data transformations.
ACM Comput. Surv. , 55(6), dec 2022.
[26] Kunz. On the equivalence between one-dimensional discrete walsh-hadamard and multidi-
mensional discrete fourier transforms. IEEE Transactions on Computers , 100(3):267–268,
1979.
[27] Siyu Liao and Bo Yuan. Circconv: A structured convolution with low complexity. Proceedings
of the AAAI Conference on Artificial Intelligence , 33(01):4287–4294, Jul. 2019.
[28] Mohammad Mahmudul Alam, Edward Raff, Stella R Biderman, Tim Oates, and James Holt.
Holographic global convolutional networks for long-range prediction tasks in malware detection.
InProceedings of The 27th International Conference on Artificial Intelligence and Statistics ,
volume 238 of Proceedings of Machine Learning Research , pages 4042–4050. PMLR, 02–04
May 2024.
[29] Nicolas Menet, Michael Hersche, Geethan Karunaratne, Luca Benini, Abu Sebastian, and Abbas
Rahimi. Mimonets: Multiple-input-multiple-output neural networks exploiting computation
in superposition. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,
editors, Advances in Neural Information Processing Systems , volume 36, pages 39553–39565.
Curran Associates, Inc., 2023.
12[30] Peer Neubert, Stefan Schubert, and Peter Protzel. Learning vector symbolic architectures for
reactive robot behaviours. In Proc. of Intl. Conf. on Intelligent Robots and Systems (IROS)
Workshop on Machine Learning Methods for High-Level Cognitive Capabilities in Robotics ,
2016.
[31] Alexandru Niculescu-Mizil and Ehsan Abbasnejad. Label filters for large scale multilabel
classification. In Artificial intelligence and statistics , pages 1448–1457. PMLR, 2017.
[32] Tony A Plate. Holographic reduced representations. IEEE Transactions on Neural networks ,
6(3):623–641, 1995.
[33] Rebecca Saul, Mohammad Mahmudul Alam, John Hurwitz, Edward Raff, Tim Oates, and James
Holt. Lempel-ziv networks. In Proceedings on "I Can’t Believe It’s Not Better! - Understanding
Deep Learning Through Empirical Falsification" at NeurIPS 2022 Workshops , volume 187 of
Proceedings of Machine Learning Research , pages 1–11. PMLR, 03 Dec 2023.
[34] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast
weight programmers. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th Inter-
national Conference on Machine Learning , volume 139 of Proceedings of Machine Learning
Research , pages 9355–9366. PMLR, 2021.
[35] Imanol Schlag and Jürgen Schmidhuber. Learning to reason with third order tensor products. In
Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018.
[36] Kenny Schlegel, Peer Neubert, and Peter Protzel. A comparison of vector symbolic architectures.
arXiv , 2020.
[37] Kenny Schlegel, Peer Neubert, and Peter Protzel. A comparison of vector symbolic architectures.
Artificial Intelligence Review , 55(6):4523–4555, Aug 2022.
[38] Paul Smolensky. Tensor product variable binding and the representation of symbolic structures
in connectionist systems. Artificial Intelligence , 46(1):159–216, 1990.
[39] Julia Steinberg and Haim Sompolinsky. Associative memory of structured knowledge. Scientific
Reports , 12(1), December 2022.
[40] Terrence C. Stewart and Chris Eliasmith. Large-scale synthesis of functional spiking neural
circuits. Proceedings of the IEEE , 102(5):881–898, 2014.
[41] Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. Learning to attend via word-aspect associative
fusion for aspect-based sentiment analysis. Proceedings of the AAAI Conference on Artificial
Intelligence , 32(1), Apr. 2018.
[42] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, Nilesh Agrawal, and Partha Talukdar. In-
teracte: Improving convolution-based knowledge graph embeddings by increasing feature
interactions. In Proceedings of the 34th AAAI Conference on Artificial Intelligence , pages
3009–3016. AAAI Press, 2020.
[43] Ryosuke Yamaki, Tadahiro Taniguchi, and Daichi Mochihashi. Holographic CCG parsing.
In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pages 262–276, Toronto, Canada, July 2023. Association for Computational Linguistics.
[44] Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, and Shanfeng Zhu.
Attentionxml: Label tree-based attention-aware deep model for high-performance extreme
multi-label text classification. Advances in neural information processing systems , 32, 2019.
[45] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. Birch: an efficient data clustering method
for very large databases. ACM sigmod record , 25(2):103–114, 1996.
13A Noise Decomposition
When a single vector pair is combined, one of the vector pairs can be exactly retrieved with the
help of the other component and the inverse function, recalling the retrieved output does not contain
any noise component for a single pair of vectors, i.e., ρ“1. However, when more than one vector
pairs are bundled, noise starts to accumulate. In this section, we will uncover the noise components
accumulated with and without the projection to the inputs and analyze their impact on expectation.
We first start with the noise component without the projection step η˝
i.
η˝
i“1
d¨Hp1
Hyidρÿ
j“1
j‰ipHxjdHyjq (12)
Let, set the value of nto be 1thus, d“2n“2and the number of vector pairs ρ“2, i.e.,
χρ“2“Bpx1, y1q`Bpx2, y2q. We want to retrieve x1using the query y1, thereby, the expression of
η˝
iis uncovered step by step for ρ“2shown in Equation 13.
η˝
i
ρ“2“1
d¨Hp1
Hy1dpHx2dHy2qq
“1
d¨?
d¨H˜1
yp0q
1`yp1q
11
yp0q
1´yp1q
1dpxp0q
2`xp1q
2q¨pyp0q
2`yp1q
2q
pxp0q
2´xp1q
2q¨pyp0q
2´yp1q
2q¸
“1
d¨d¨¨
˚˝pxp0q
2`xp1q
2qpyp0q
2`yp1q
2qpyp0q
1´yp1q
1q`pxp0q
2´xp1q
2qpyp0q
2´yp1q
2qpyp0q
1`yp1q
1q
pyp0q
1`yp1q
1qpyp0q
1´yp1q
1q
pxp0q
2`xp1q
2qpyp0q
2`yp1q
2qpyp0q
1´yp1q
1q´pxp0q
2´xp1q
2qpyp0q
2´yp1q
2qpyp0q
1`yp1q
1q
pyp0q
1`yp1q
1qpyp0q
1´yp1q
1q˛
‹‚
“˜φ1śd
k“1pHy1qkφ2śd
k“1pHy1qk¸
“Ppx2, y2, y1q
śd
k“1pHy1qk(13)
Here, φk@kPN: 1ďkďdare the polynomials comprises of px2, y2q, and the query vector y1.
Pis the vector of polynomials consisting of φk. From the noise expression, we can observe that the
numerator is a polynomial and the denominator is the product of all the elements of the Hadamard
transformation of the query vector. This is true for any value of nandρ. Thus, in general, for any
query yiwe can express η˝
ias shown in Equation 14.
η˝
i“ρ
P
j“1, j‰ipxj, yj, yiq
śd
k“1pHyiqk(14)
The noise accumulated after applying the projection to the inputs is quite straightforward as given in
Equation 15.
ηπ
i“ρř
j“1, j‰ipxjdyjq
yi(15)
Although the vectors xi, yi@iPN: 1ďiďρare sampled from a MiND with an expected value of
0given in Equation 6, the sample mean of xioryiwould be ˆµ«0butˆµ‰0. Both the numerator
ofη˝
iandηπ
iare the polynomials thus the expected value would be very close to 0. However, the
expected value of the denominator of η˝
iwould be Erśd
k“1pHyiqks “śd
k“1ErpHyiqks “ˆµd
whereas the expected value of the denominator of ηπ
iisEryis “ ˆµ. Since, ˆµdăˆµ, hence, in
14expectation ηπ
iăη˝
i. This is also verified by an empirical study where n, i.e., the dimension d“2n
is varied along with the no. of bound vector pairs ρand the amount of absolute mean noise in retrieval
is estimated.
Figure 4 shows the heatmap visualization of the noise for both η˝
iandηπ
iin natural log scale. The
amount of noise accumulated without any projection to the inputs is much higher compared to
the noise accumulation with the projection. For varying nandρ, the maximum amount of noise
accumulated when projection is applied is 7.18and without any projection, the maximum amount
of noise is 19.38. Also, most of the heatmap of ηπ
iremains in the blue region whereas as nandρ
increase, the heatmap of η˝
imoves towards the red region. Therefore, it is evident that the projection
to the inputs diminishes the amount of accumulated noise with the retrieved output.
5 10 15 20 25 30 35 40 45 50
No. of bundled pairs ρ246810n= log2(d)Noise component without any projection η◦
i
5 10 15 20 25 30 35 40 45 50
No. of bundled pairs ρ246810n= log2(d)Noise component due to the projection ηπ
i
0.02.55.07.510.012.515.017.520.0
Figure 4: Heatmap of the empirical comparison of the noise components η˝
iandηπ
ifor varying
nandρshown in natural logarithm scale. The dimension, i.e., d“2nis varied from 2to1024
pnPt1,2,¨¨¨,10uqand the number of vector pairs bundled is varied from 2to50.
B Norm Relation
Theorem B.1 (χρ–ρRelationship) .Given xi, yi„Ωpµ,1{dqPRd@iPN: 1ďiďρ, the norm
of the composite representation χρis proportional to?ρand approximately equal to the µ2?ρ¨d.
Proof of Theorem B.1. Given χρis the composite representation of the bound vectors, i.e., the
summation of ρno. of individual bound terms. First, let’s compute the norm of the single bound term
as shown in Equation 16.
}Bpxi, yiq}2“}xi¨yi}2
“b
pxp1q
iyp1q
iq2`pxp2q
iyp2q
iq2`¨¨¨`p xpdq
iypdq
iq2
“a
p˘µ2q2`p˘ µ2q2`¨¨¨`p˘ µ2q2”
Erxp1qs¨Eryp1qs“˘ µ¨˘µ“˘µ2ı
“a
µ4d
(16)
15Now, let’s expand and compute the square norm of the composite representation given in Equation 17.
››χρ››2
2“››Bpx1, y1q`Bpx2, y2q`¨¨¨` Bpxρ, yρq››2
2
“}Bpx1, y1q}2
2`}Bpx2, y2q}2
2`¨¨¨`››Bpxρ, yρq››2
2`ξ
where ξis the rest of the terms of square expansion.
“µ4d`µ4d`¨¨¨` µ4d`ξ
“ρ¨µ4d`ξ
››χρ››
2“a
ρ¨µ4d`ξ
«a
ρ¨µ4drξis the noise term and discarded to make an approximation s
“µ2a
ρ¨d(17)
Thus, given the composite representation and the mean of the MiND distribution, we can estimate the
no. of bound terms bundled together by ρ«››χρ››2
2{µ4d.
Figure 5 shows the comparison between the theoretical relationship and actual experimental results
where the norm of the composite representation is computed for µ“0.5andρ“t1,2,¨¨¨,200u.
The figure indicates that the theoretical relationship aligns with the experimental results. However, as
the number of bundled pair increases, the variation in the norm increases. This is because of making
the approximation by discarding ξin Equation 17.
20 40 60 80 100 120 140 160 180 200
No. of bundled pairs ρ20406080100120Norm of composite representation ||χρ||2
 Theoretical||χρ||2=µ2√ρ·d
Experimental||χρ||2≈µ2√ρ·d
Figure 5: Comparison between the theoretical and experimental relationship of Theorem B.1. The
norm of the composite representation of the bound vectors is computed for no. of bundled vectors
from 1to200of dimension d“1024 . The figure shows how the experimental value of the norm
closely follows the theoretical relation between››χρ››
2andρ.
16C Cosine Relation
Theorem 3.2 shows how the cosine similarity ϕbetween the original xiand retrieved vector ˆxi
is approximately equal to the inverse square root of the number of vector pairs in a composite
representation ρ. In this section, we will perform an empirical analysis of the theorem and compare
it with the theoretical results. For ρ“ t1,2,¨¨¨,50u, similarity score ϕis calculated for vector
dimension d“512. Additionally, the theoretical cosine similarity score is also calculated using the
value of ϕfollowing the theorem. Figure 6 shows the comparison between the two results where the
experimental result closely follows the theoretical result. The figure also shows the standard deviation
for100trials, indicating a minute change from the actual value.
5 10 15 20 25 30 35 40 45 50
No. of bundled pairs ρ0.00.20.40.60.81.0Cosine Similarity φTheoreticalφ=1√ρExperimental φ≈1√ρµφ±σφ
Figure 6: Comparison between the theoretical and experimental ϕ´ρrelationship. Vectors of
dimension d“512are combined and retrieved with a varied number of vectors from 1to50. The
zoom portion shows how closely experimental results match with the theoretical conclusion.
17NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The paper claims to present a new Hadamard-derived linear vector symbolic
architecture in the abstract and introduction which accurately reflects the contribution and
scope of the paper.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Limitations are described in the paper.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
18Answer: [Yes]
Justification: All the theoretical results and proofs are provided in the Methodology section.
(See section 3)
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The experimental setup is based on previous papers. All the self-used parame-
ters are discussed in the paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
19Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: All the code is provided with the supplemental material. Data is publicly
available and instruction is provided on how to get the data.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Experimental details and their sources are cited in the Empirical results section.
(see section 4)
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: See Figure 2, Figure 3
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
20•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: See section 4.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We followed the NeurIPS code of conduct.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The work has no negative societal impacts.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
21•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: No high-risk data or models are used.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Original paper and code are cited in the paper.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
22•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: Documentation of the code is provided.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No Crowdsourcing and Research with Human Subjects are used.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No participants are used.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
23