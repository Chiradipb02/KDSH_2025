Controlling Multiple Errors Simultaneously with a
PAC-Bayes Bound
Reuben Adams
Department of Computer Science
University College London
reuben.adams.20@ucl.ac.ukJohn Shawe-Taylor
Department of Computer Science
University College London
j.shawe-taylor@ucl.ac.uk
Benjamin Guedj
Department of Computer Science, University College London and Inria
b.guedj@ucl.ac.uk
Abstract
Current PAC-Bayes generalisation bounds are restricted to scalar metrics of perfor-
mance, such as the loss or error rate. However, one ideally wants more information-
rich certificates that control the entire distribution of possible outcomes, such
as the distribution of the test loss in regression, or the probabilities of different
mis-classifications. We provide the first PAC-Bayes bound capable of providing
such rich information by bounding the Kullback-Leibler divergence between the
empirical and true probabilities of a set of Merror types, which can either be
discretized loss values for regression, or the elements of the confusion matrix (or a
partition thereof) for classification. We transform our bound into a differentiable
training objective. Our bound is especially useful in cases where the severity of
different mis-classifications may change over time; existing PAC-Bayes bounds
can only bound a particular pre-decided weighting of the error types. In contrast
our bound implicitly controls all uncountably many weightings simultaneously.
1 Introduction
Generalisation bounds are a core component of the theoretical understanding of machine learning
algorithms. For over two decades now, PAC-Bayesian theory has been at the core of studies on
generalisation abilities of machine learning algorithms. PAC-Bayes originated in the seminal work
of McAllester [1998, 1999] and was further developed by Catoni [2003, 2004, 2007], among other
authors—we refer to the surveys Guedj [2019] and Alquier [2021] for an introduction to the field. The
outstanding empirical success of deep neural networks in the past decade calls for better theoretical
understanding of deep learning, and PAC-Bayes has emerged as one of the few frameworks that
can be used to derive meaningful (and non-vacuous) generalisation bounds for neural networks:
the pioneering work of Dziugaite and Roy [2017] has been followed by a number of contributions,
including Neyshabur et al. [2018], Zhou et al. [2019], Letarte et al. [2019], Pérez-Ortiz et al. [2021],
Perez-Ortiz et al. [2021] and Biggs and Guedj [2022a,b], to name but a few.
Much of the PAC-Bayes literature focuses on the case of binary classification, or of multiclass
classification where one only distinguishes whether each classification is correct or incorrect. This is
in stark contrast to the complexity of contemporary real-world learning problems, such as medical
diagnosis where the severity of Type I and Type II errors may be crucial and context-dependent.
This work aims to bridge this gap by deriving a generalisation bound that provides information-rich
measures of performance at test time by controlling the probabilities of errors of any finite number of
user-specified types. More precisely, we bound the KL-divergence between the empirical and true
38th Conference on Neural Information Processing Systems (NeurIPS 2024).distributions over the different error types. From this single bound one can then derive bounds on
arbitrary linear combinations of these error probabilities, which will all hold simultaneously with the
same probability as the original bound. In addition, these bounds are guaranteed to be non-vacuous
(this follows since the KL-divergence blows up on the boundary of the simplex).
As a concrete example, if the severity of Type I and Type II errors of a medical test are context-
dependent, one would want to be able to bound arbitrary linear combinations of these error probabili-
ties. Existing bounds could only bound finitely many pre-specified weightings by employing a union
bound, which would also degrade the bound. In contrast, by constraining the KL-divergence between
the true and empirical error probabilities, our bound constrains all uncountably many weightings of
the error probabilities simultaneously.
The usual setting of PAC-Bayes bounds is that of binary classification, namely an input set X, output
setY={−1,1}, hypothesis space H ⊆ YXand a sample S∈(X × Y )mdrawn i.i.d. from a data-
generating distribution D. A number of PAC-Bayes bounds in this setting (e.g. Maurer [2004]) have
been unified by a single general bound found in Bégin et al. [2016]. Briefly, Bégin et al. [2016] prove
a bound on the discrepancy d(RS(Q), RD(Q))between the error probability RD(Q)of a stochastic
classifier Q(a distribution over Hwhich classifies by first drawing h∼Qand then classifying
according to h) and its empirical counterpart RS(Q)(the fraction of the sample Qmisclassifies).
The bound holds with high probability for all Qsimultaneously. The bound in Bégin et al. [2016] is
binary in the sense that Ycontains two elements, but a more subtle way to look at this is that only two
cases are distinguished—correct classification and incorrect classification. While it can be applied to
multiclassification provided one maintains the second binary characteristic by only distinguishing
correct and incorrect classifications. It is this heavy restriction that our result lifts, by considering the
new framework of error types .
A new framework of errors types We consider a user-specified partition of the space Y × Y
of prediction-truth label-pairs into a finite partition of error types E1, . . . , E M. Our bound then
simultaneously constrains the probability with which errors of each type occur. In multiclass
classification for example, one can choose the error types to be the set of all different possible
mis-classifications, in which case our bound will control the entire confusion matrix, bounding how
far the true confusion matrix (i.e. expected over the data-generating distribution) can diverge from the
empirical one (i.e. on the training set). From this one can then derive bounds on the probabilities
with which each mis-classification may be made, and arbitrary linear combinations of these error
probabilities, and all of these will hold simultaneously with the same probability as the original
bound. Our bound therefore paints a far richer picture of the performance of the final learned model
than can be provided by any existing PAC-Bayes bound.
Formally, we letSM
j=1Ejbe a user-specified disjoint partition of Y2into a finite number of M
error types , where we say that a hypothesis h∈ H makes an error of type jon datapoint (x, y)if
(h(x), y)∈Ej(by convention, every pair (ˆy, y)∈ Y2is interpreted as a predicted value ˆyfollowed
by a true value y, in that order). It should be stressed that not all of the Ejneed correspond to
mislabellings—indeed, some of the Ejmay distinguish different correct labellings.
Relation to previous results. Our framework of a finite number of user-specified “error types”
includes multiclass classification as a particular case, and it is in this field that one finds the work
most closely related to ours. Little is known of multiclass classification from a theoretical perspective
and, to the best of our knowledge, only a handful of relevant strategies or generalisation bounds can
be compared to the present paper.
The closest is the work of Morvant et al. [2012], which establishes a PAC-Bayes bound on the spectral
norm of the difference between the true and empirical confusion matrices. Our bound differs from
theirs in two respects. First, they consider the confusion matrix, whereas ours applies to the more
general setting of a finite number of error types, which can be the set of all mis-classifications, or some
partition thereof. Second, they deal with the spectral norm, whereas we employ the KL-divergence.
Since the KL-divergence follows a simple formula, this means we can much more easily infer bounds
on the individual error probabilities, which would be very challenging for the spectral norm. The
follow-up work Koço and Capponi [2013] shows how a proxy of the spectral norm bound can be
used as a training objective that may deal with imbalanced classes. In the present work, we show how
our bound can be used as a differentiable training objective directly (without the need of a proxy) and
2that it can more sensitively deal with imbalanced classes, or errors of different severity, by assigning
each error type a user-specified loss value.
Laviolette et al. [2017] extend the celebrated C-bound in PAC-Bayes to ensembles, obtaining a bound
on the risk of the majority vote classifier in the case of multiclass classification. In this context, our
bound is able to distinguish different mis-classifications and control them, whereas they bound the
scalar risk which lumps all mis-classifications together. The C-bound has alternately been generalised
by Lacasse et al. [2006] (see also Germain et al. [2015]) to simultaneously control three metrics,
namely the so-called expected disagreement, expected joint success and expected joint error of the
posterior. While they restricted themselves to the ternary case, some of their proof techniques share
similarities with ours. In cases where one has exactly three error types, for example the {−1,0,1}-
valued excess loss , the work of Wu and Seldin [2022] is applicable; they construct so-called ‘split-kl’
inequalities (both classical and PAC-Bayesian) which deftly handle this specific scenario.
Pires et al. [2013] present a comprehensive analysis of convex surrogate losses in cost-sensitive
multiclass classification, providing conditions for consistency, bounding the excess loss of a predictor,
and extending the analysis to the “Simplex Coding” scheme. We are considering the generalisation gap
rather than the excess loss. Lei et al. [2019] study data-dependent bounds for multiclass classification.
Their analysis is restricted to SVMs however, whereas ours applies to arbitrary hypothesis spaces.
Outline. We fix notation in Section 2. Theorem 1 in Section 3 is our main result—a PAC-Bayes
bound on the KL-divergence between the true and empirical error distributions. For multiclass
classification with a fully refined partition this becomes a bound on the KL-divergence between the
true and empirical confusion matrices. Proposition 1 then bounds the individual error probabilities.
Our second main result, Theorem 2 in Section 4, allows us to use bounds on linear combinations of
error probabilities as training objectives. We prove Theorem 1 in Section 5 via Proposition 4, which
bounds the distribution of errors via a general convex function dand may be of independent interest.
Section 6 outlines positive empirical results1from using our bound as a training objective for neural
networks and Section 7 gives perspectives for follow-up work..
2 Notation
For any set A, letM(A)be the set of probability measures on A. LetXandYbe arbitrary input
(e.g., feature) and output ( e.g., label) sets respectively, and D∈ M (X × Y )be a data-generating
distribution. For any sample S∼Dmdrawn i.i.d. from D, letˆD(S)∈ M (X × Y )denote the
empirical distribution ˆD(S) :=1
mP
(x,y)∈Sδ(x,y). We consider the setting where the user has
specified a partition {E1, . . . , E M}ofY2intoMerror types .
We are interested in simple hypotheses h:X → Y andsofthypotheses H:X → M (Y). For
example, a neural network outputting scores (logits) in RYis converted to a simple or soft hypothesis,
respectively, by passing the scores through the argmax or softmax function, respectively. For any
A⊆ Y,H(x)(A)can be interpreted as the probability according to Hthat the label of xis inA. We
will see in Section 4 that soft hypotheses permit more flexible training procedures and a more fine-
grained analysis. Note that while soft hypotheses output distributions, they do so deterministically,
always returning the same distribution for the same input x, and so are distinct from the stochastic
classifiers introduced shortly.
For a simple hypothesis h:X → Y andj∈[M], define the j-risk ofhto be Rj
D(h) :=
P(x,y)∼D((h(x), y)∈Ej), namely the probability that hmakes an error of type Ejfor a ran-
domly sampled (x, y)∼D. For a soft hypothesis H:X → M (Y)define the j-risk ofHto be
Rj
D(H) :=P(x,y)∼D,ˆy∼H(x)((ˆy, y)∈Ej), namely the probability that one would make an error of
typeEjon a randomly sampled (x, y)∼Dif one predicted by sampling ˆyfrom the distribution
H(x). From now until Section 4 it will not matter whether we are dealing with simple or soft
hypotheses. So, unless stated explicitly, we will refer to both simply as hypotheses, denote both by
lowercase h, and refer to the hypothesis class H, whether it is a subset of YXorM(Y)X.
Our goal is to control the risk vector RD(h) := ( R1
D(h), . . . , RM
D(h)), since controlling this
vector controls all linear combinations of j-risks. Since this is unobervable, we will control it by
1Code available here: https://github.com/reubenadams/PAC-Bayes-Control
3bounding how far it diverges from its empirical counterpart RS(h) :=RˆD(S)(h), which we term
theempirical risk vector . Note that ES∼DmRS(h) =RD(h), and that, for a simple hypothesis
h∈ YX,RS(h)is the vector of proportions of the sample on which hmakes an error of type
Ej2. Since the Ejpartition Y2,RD(h)andRS(h)are elements of the M-dimensional simplex
△M:={u∈[0,1]M:u1+···+uM= 1}. Thus we can choose our divergence measure to be
kl(RS(h)∥RD(Q)), where for q,p∈ △ Mwe define kl(q∥p) :=PM
j=1qjlnqj
pj.3When M= 2we
abbreviate kl((q,1−q)∥(p,1−p))tokl(q∥p), which is then the conventional definition of kl(·∥·)
found in the PAC-Bayes literature [as in Seeger, 2002, for example]. We define the riskandempirical
riskofQasRD(Q) :=Eh∼QRD(h)andRS(Q) :=Eh∼QRS(h), respectively, and seek a bound
onkl(RS(Q)∥RD(Q)). Note we still have ES[RS(Q)] =RD(Q), this time using Fubini. Moreover,
for a sample Sof size m, we have that RS(Q) =K/mwhere K∼Mult(m, M, RD(Q)). Recall
that for m, M ∈Nandr∈ △ M, the multinomial distribution Mult(m, M, r)has probability mass
function Mult(k;m, M, r) := m
k1k2···kMQM
j=1rkj
j,where m
k1k2···kM
:=m!QM
j=1kj!for
k∈Sm,M :=
(k1, . . . , k M)∈NM
0:k1+···+kM=m	
, and zero otherwise. As a final piece
of notation, we let △>0
M:=△M∩(0,1)MandS>0
m,M:=Sm,M∩NMdenote the vector elements of
△MandSm,M, respectively, that have no zero components.
3 Main result
We now state our main result, which bounds the KL-divergence between the true and empirical
risk vectors RD(Q)andRS(Q), interpreted as probability distributions. As is conventional in the
PAC-Bayes literature, we refer to sample independent and dependent distributions on M(H)(i.e.
stochastic hypotheses) as priors (denoted P) and posteriors (denoted Q) respectively, even if they are
not related by Bayes’ theorem.
Theorem 1. LetXandYbe arbitrary sets andSM
j=1Ejbe a disjoint partition of Y2intoMerror
types. Let D∈ M(X × Y )be a data-generating distribution and Hbe a simple ( H ⊆ YX) or soft
(H ⊆ M (Y)X) hypothesis class. For any prior P∈ M (H),δ∈(0,1]and sample size m≥M,
with probability at least 1−δover the random draw S∼Dm, we have that simultaneously for all
posteriors Q∈ M(H), the divergence kl 
RS(Q)∥RD(Q)
is upper bounded by
1
m
KL(Q∥P) + lnξ(m, M )
δ
,where (1)
ξ(m, M ) :=√πe1/(12m) m
2M−1
2PM−1
z=0 M
z 2
mz/2Γ M−z
2−1∈ O 
(mM)M
.
The fact that the logarithmic term is of order O(Mln(mM/δ ))means the bound is linear in Mup to
logarithmic terms, while this may seem excessive, one should note that the quantity that our theorem
bounds also depends on M. Further, the bound has been successfully used in by Biggs and Guedj
[2023] to improve on state of the art PAC-Bayes bounds.
To see how our bound compares to existing PAC-Bayes bounds for binary classification, take
Y={−1,1},M= 2,E1={(−y, y) :y∈ Y} andE2={(y, y) :y∈ Y} . The argument of the
logarithm then reduces to1
δe1/(12m) 
2 +pπm
2
≤1.25√mwhen mis large. The corresponding
term in Maurer [2004] is 2√m, which is only larger because he relaxes the term for aesthetics.
Therefore our bound gracefully reduces to Maurer’s in the case of binary classification.
Suppose after a use of Theorem 1 we have a bound of the form kl(RS(Q)∥RD(Q))≤B. We
can then derive bounds on the individual j-risks Rj
D(Q)or, more generally, on linear combinations
thereof. While one could obtain such bounds perhaps more directly with existing PAC-Bayes
bounds, the significance of our bound is that allsuch derived bounds hold with high probability
simultaneously . Existing PAC-Bayes bounds would require the use of a union bound in order to bound
multiple combinations simultaneously, whereas ours bounds all uncountably many combinations
simultaneously, as a package. As for the individual j-risks Rj
D(Q), the following proposition
2(RS(h))j=Rj
ˆD(h) =P(x,y)∼ˆD((h(x), y)∈Ej) =1
mP
(x,y)∈S1[(h(x), y)∈Ej].
3We follow the usual convention that 0 ln0
x= 0forx≥0andxlnx
0=∞forx >0.
4then yields the bounds Lj≤Rj
D(Q)≤Uj, where Lj:= inf {p∈[0,1] : kl(Rj
S(Q)∥p)≤
B}andUj:= sup {p∈[0,1] : kl(Rj
S(Q)∥p)≤B}. Moreover, since in the worst case we
have kl(RS(Q)∥RD(Q)) = B, the proposition shows that the lower and upper bounds Ljand
Ujare the tightest possible, since if Rj
D(Q)̸∈[Lj, Uj]then kl(Rj
S(Q)∥Rj
D(Q))> B implying
kl(RS(Q)∥RD(Q))> B. For a more precise version of this argument and a proof of Proposition 1,
see Appendix C.4.
Proposition 1. Letq,p∈ △ M. Then kl(qj∥pj)≤kl(q∥p)for all j∈[M], with equality when
pi=1−pj
1−qjqi.for all i̸=j.
More generally, suppose we can quantify how costly an error of each type is by means of a loss vector
ℓ∈[0,∞)M, where ℓjis the loss we attribute to an error of type Ej. We may then be interested in
bounding the total risk RT
D(Q) :=ℓ·RD(Q). Then, given a bound kl(RS(Q)∥RD(Q))≤Bfrom
Theorem 1, we can deduce
RT
D(Q)≤sup{ℓ·r:r∈ △ M,kl(RS(Q)∥r)≤B}=ℓ·kl−1
ℓ(RS(Q)|B), (2)
where we define kl−1
ℓ(u|c)∈ △ Mas follows. To see that it is indeed well-defined (at least when
u∈ △>0
M), see the discussion at the beginning of Appendix C.5.
Definition 1. Foru∈ △ M, c∈[0,∞)andℓ∈[0,∞)M, define kl−1
ℓ(u|c)to be an element
v∈ △ Msolving the constrained optimisation problem
Maximise: fℓ(v) :=ℓ·v, (3)
Subject to: kl(u∥v)≤c. (4)
This motivates the following training procedure: search for a posterior Qfor which the bound
ℓ·kl−1
ℓ(RS(Q)|B)on the total risk RT
D(Q)is minimised. While this requires a particular choice of
loss vector ℓ, we emphasise that at the end of training, Theorem 1 bounds kl(RS(Q)∥RD(Q))and
so can be used to bound anylinear combination of the j-risks, not just the one given the loss vector ℓ
chosen for training. It is this flexibility which is the main advantage of our bound; changes in the
severity of different error types over time do not require union bounds or retraining.
In the next section we provide a theorem for calculating kl−1
ℓ(u|c)and its derivatives so that the
training procedure can be executed.
4 Construction of a Differentiable Training Objective
We now state and prove Theorem 2, which provides a speedy method for approximating kl−1
ℓ(u|c)
and its derivatives to arbitrary precision, provided c >0and∀j uj>0. The only approximation
step required is that of approximating the unique root of a continuous and strictly increasing scalar
function. Thus, provided the ujthemselves are differentiable, Theorem 1 combined with Theorem
2 shows that the upper bound on the total risk can be used as a tractable and fully differentiable
training objective. See Appendix A for more details, including a pseudocode algorithm and an
implementation. Since the proof of Theorem 2 is rather long and technical, we defer it to Appendix
C.5. The requirement that the ℓjare not all equal only rules out trivial cases where RT
D(Q)is
independent of RD(Q).
Theorem 2. Fixℓ∈[0,∞)Msuch that not all ℓjare equal, and define fℓ:△M→[0,∞)by
fℓ(v) :=PM
j=1ℓjvj. For all ˜u= (u, c)∈ △>0
M×(0,∞), define v∗(˜u) := kl−1
ℓ(u|c)∈ △ Mand let
µ∗(˜u)∈(−∞,−max jℓj)be the unique solution to c=ϕℓ(µ), where ϕℓ: (−∞,−max jℓj)→R
is given by ϕℓ(µ) := ln( −PM
j=1uj
µ+ℓj) +PM
j=1ujln(−(µ+ℓj)), which is continuous and strictly
increasing. Then v∗(˜u) =kl−1
ℓ(u|c)is given by
v∗(˜u)j=λ∗(˜u)uj
µ∗(˜u) +ℓjforj∈[M], where λ∗(˜u) =
MX
j=1uj
µ∗(˜u) +ℓj
−1
. (5)
Further, defining f∗
ℓ:△>0
M×(0,∞)→[0,∞)byf∗
ℓ(˜u) :=fℓ(v∗(˜u)), we have that
∂f∗
ℓ
∂uj(˜u) =λ∗(˜u)
1 + lnuj
v∗(˜u)j
and∂f∗
ℓ
∂c(˜u) =−λ∗(˜u). (6)
5A final wrinkle in evaluating our bound is that while the empirical risk vector RS(Q) =Eh∼QRS(h)
does not depend on the data-generating distribution D, the expectation over Qmay still be in-
tractable. This would be the default case when Qis a Gaussian over the weights of a multi-
layer perceptron, for example. In such cases, we can estimate RS(Q)via a Monte Carlo sample
RS(ˆQ) :=1
NPN
n=1RS(hn)(where the hnare drawn i.i.d. from Q) and use the following two
results. Proposition 2 shows that the kl(Rj
S(ˆQ)∥Rj
D(Q))can be simultaneously bounded, whence
Proposition 3 can be used to obtain a bound on kl (RS(ˆQ)∥RD(Q)).
Proposition 2. LetX∼Multinomial (N, M, p). Then for any δ∈(0,1), with probability at least
1−δwe have that for all j∈[M]simultaneously kl 1
NXjpj
≤ln2M
δ
N.
Proof. Each bound holds separately with probability at least 1−δ/M by Theorem 2.5 in Langford
and Caruana [2001]. They then hold simultaneously by application of a union bound.
Proposition 3. Suppose q,p,ˆq∈ △ Mare such that kl(q∥p)≤B1andkl(ˆqj∥qj)≤B2for all
j∈[M]. For each j, define qj= inf{r∈[0,1] :kl(ˆqj∥r)≤B2}. Then
kl(ˆq∥p)≤MB 2−MX
j=1(1−ˆqj) ln1−ˆqj
1−qj+B1max
jˆqj
qj→B1asB2→0. (7)
Proof. Deferred to C.1.
The fact that the bound on kl(ˆq∥p)→B1asB2→0ensures that as we increase the size of our
Monte Carlo sample for estimating RS(Q)the bound on kl(RS(ˆQ)∥RD(Q))approaches that of
kl(RS(Q)∥RD(Q)), meaning in the limit we pay an arbitrarily small price in the bound for the
approximation.
5 Proof of the main bound
We split the proof of Theorem 1 into three parts. First, we prove Proposition 4, a bound on
d(RS(Q),RD(Q))for an arbitrary convex function d, which may be of independent interest. Second,
we prove Corollary 1 by specialising Proposition 4 to the case d(·,·) =kl(·∥·). Finally, we show that
the bound in Theorem 1 is a loosened version of the bound in Corollary 1.
Proposition 4. Letd:△2
M→Rbe jointly convex. In the setting of Theorem 1,
d 
RS(Q),RD(Q)
≤1
β
KL(Q∥P) + lnId(m, β)
δ
,where (8)
Id(m, β) := supr∈△MhP
k∈Sm,MMult(k;m, M, r) exp
βd k
m,ri
.
This is a generalisation of the unifying PAC-Bayes bound given in Bégin et al. [2016] where we replace
the scalar risk quantities RS(Q)andRD(Q)with their vector counterparts RS(Q)andRD(Q). To
see this, note that we can recover it by setting Y={−1,1},M= 2,E1={(−y, y) :y∈ Y}
andE2={(y, y) :y∈ Y} . Then, for any convex function d: [0,1]2→R, apply Theorem
4 with the convex function d′:△2
M→Rdefined by d′((u1, u2),(v1, v2)) := d(u1, v1)so that
Theorem 4 bounds d′ 
RS(Q),RD(Q)
=d 
R1
S(Q), R1
D(Q)
which equals d(RS(Q), RD(Q))
in the notation of Bégin et al. [2016]. Further,P
k∈Sm,2Mult(k;m,2,r) exp
βd′ k
m,r
=
Pm
k=0Bin(k;m, r 1) exp
βd k
m, r1
,so that the supremum over r1∈[0,1]of the right hand side
equals the supremum over r∈ △ 2of the left hand side, which, when substituted into (8), yields the
bound given in Bégin et al. [2016].
To prove Proposition 4 we require the following two lemmas. The first is the well-known change of
measure in equality (Csiszár, 1975, Donsker and Varadhan, 1975). The second is a generalisation
from Binomial to Multinomial distributions of a result found in Maurer [2004], the proof of which
we defer to Appendix C.2.
6Lemma 1. For any set H, anyP, Q∈ M(H)and any measurable function ϕ:H →R,E
h∼Qϕ(h)≤
KL(Q∥P) + ln E
h∼Pexp(ϕ(h)).
Lemma 2. LetX1, . . . ,Xmbe i.i.d △M-valued random vectors with mean µand suppose
thatf:△m
M→Ris convex. If X′
1, . . . ,X′
mare i.i.d. Mult(1, M,µ)random vectors, then
E[f(X1, . . . ,Xm)]≤E[f(X′
1, . . . ,X′
m)].
The consequence of Lemma 2 is that the worst case (in terms of bounding d(RS(Q),RD(Q))) occurs
whenR{(x,y)}(h)is a one-hot vector for all (x, y)∈Sandh∈ H, namely when H ⊆ M (Y)Xonly
contains hypotheses that, when labelling S, put all their mass on elements ˆy∈ Y that incur the same
error type4. In particular, this is the case for hypotheses that put all their mass on a single element of
Y, equivalent to the simpler case H ⊆ YXas discussed in Section 2. Thus, Lemma 2 shows that the
bound given in Proposition 4 cannot be made tighter only by restricting to such hypotheses.
Proof. (of Proposition 4) The case H ⊆ YXfollows directly from the more general case by taking
H′:={h′∈ M(Y)X:∃h∈ H such that ∀x∈ X h′(x) =δh(x)}, where δh(x)∈ M(Y)denotes a
point mass on h(x). For the general case H ⊆ M (Y)X, using Jensen’s inequality with the convex
function d(·,·)and Lemma 1 with ϕ(h) =βd(RS(h),RD(h)), we see that for all Q∈ M(H)
βd 
RS(Q),RD(Q)
=βd
E
h∼QRS(h),E
h∼QRD(h)
≤E
h∼Qβd 
RS(h),RD(h)
≤KL(Q∥P) + ln
E
h∼Pexp
βd 
RS(h),RD(h)
=KL(Q∥P) + ln( ZP(S)),
where ZP(S) :=Eh∼Pexp 
βd(RS(h),RD(h))
. Note that ZP(S)is a non-negative random
variable, so that by Markov’s inequality P
S∼Dm
ZP(S)≤ES′∼DmZP(S′)
δ
≥1−δ.Thus, since ln(·)
is strictly increasing, with probability at least 1−δoverS∼Dm, we have that simultaneously for
allQ∈ M(H)
βd 
RS(Q),RD(Q)
≤KL(Q∥P) + lnE
S′∼DmZP(S′)
δ. (9)
To bound ES′∼DmZP(S′), letXi:=R{(xi,yi)′}(h)∈ △ Mfori∈[m], where (xi, yi)′is the
i’th element of the dummy sample S′. Noting that each Xihas mean RD(h), define the random
vectors X′
i∼Mult(1, M,RD(h))andY:=Pm
i=1X′
i∼Mult(m, M, RD(h)). Finally let f:
△m
M→Rbe defined by f(x1, . . . , x m) := exp 
βd 1
mPm
i=1xi,RD(h)
,which is convex since
the average is linear, dis convex and the exponential is non-decreasing and convex. Then, by
swapping expectations (which is permitted by Fubini’s theorem since the argument is non-negative)
and applying Lemma 2, we have that ES′∼DmZP(S′)can be written as
ES′∼DmZP(S′) = E
S′∼DmE
h∼Pexp
βd 
RS′(h),RD(h)
=E
h∼PE
S′∼Dmexp
βd 
RS′(h),RD(h)
=E
h∼PE
X1,...,Xmexp 
βd 
1
mmX
i=1Xi,RD(h)!!
≤E
h∼PE
X′
1,...,X′mexp 
βd 
1
mmX
i=1X′
i,RD(h)!!
=E
h∼PE
Yexp
βd1
mY,RD(h)
4More precisely, when ∀h∈ H ∀ (x, y)∈S∃j∈[M]such that h(x)[{ˆy∈ Y: (ˆy, y)∈Ej)}] = 1 .
7=E
h∼PX
k∈Sm,MMult 
k;m, M, RD(h)
exp
βd k
m,RD(h)
≤sup
r∈△M
X
k∈Sm,MMult 
k;m, M, r
exp
βd k
m,r
,
which is the definition of Id(m, β). Inequality (8) then follows by substituting this bound on
ES′∼DmZP(S′)into (9) and dividing by β.
We now specialise Proposition 4 to the case d(·,·) =kl(·∥·)to obtain Corollary 1.
Corollary 1. In the setting of Theorem 1,
kl 
RS(Q)∥RD(Q)
≤1
m
KL(Q∥P) + lnη(m, M )
δ
,where (10)
η(m, M ) :=m!
mmX
k∈Sm,MMY
j=1kkj
j
kj!. (11)
Proof. Applying Proposition 4 with d(·,·) =kl(·∥·)andβ=mgives that with probability at least
1−δoverS∼Dm, simultaneously for all posteriors Q∈ M(H),
kl 
RS(Q)∥RD(Q)
≤1
m
KL(Q∥P) + lnIkl(m, m )
δ
,
where Ikl(m, m ) := supr∈△M[P
k∈Sm,MMult(k;m, M, r) exp 
mkl(k
m,r)
]. Thus it suffices to
show that Ikl(m, m )≤η(m, M ).
To prove this, for each fixed r= (r1, . . . , r M)∈ △ MletJr={j∈[M] :rj= 0}. Then
Mult(k;m, M, r) = 0 for any k∈Sm,M such that kj̸= 0 for some j∈Jr. For the other
k∈Sm,M, namely those such that kj= 0 for all j∈Jr, the probability term can be written as
Mult(k;m, M, r) =m!QM
j=1kj!QM
j=1rkj
j=m!Q
j̸∈Jrkj!Q
j̸∈Jrrkj
j,and (recalling the convention that
0 ln0
0= 0) the term exp(mkl(k
m,r))can be written as
exp
mMX
j=1kj
mlnkj
m
rj
= exp
X
j̸∈Jrkjlnkj
mrj
=Y
j̸∈Jrkj
mrjkj
=1
mmY
j̸∈Jrkj
rjkj
,
where the last equality is obtained by recalling that the kjsum to m. Substituting these two
expressions into the definition of Ikl(m, m )and only summing over those k∈Sm,M with non-zero
probability, we obtain
X
k∈Sm,MMult(k;m, M, r) exp 
mkl k
m,r
=X
k∈Sm,M:
∀j∈Jrkj=0Mult(k;m, M, r) exp 
mkl k
m,r
=X
k∈Sm,M:
∀j∈Jrkj=0m!Q
j̸∈Jrkj!Y
j̸∈Jrrkj
j1
mmY
j̸∈Jrkj
rjkj
=m!
mmX
k∈Sm,M:
∀j∈Jrkj=0Y
j̸∈Jrkkj
j
kj!
=m!
mmX
k∈Sm,M:
∀j∈Jrkj=0MY
j=1kkj
j
kj!(because00
0!= 1)
≤m!
mmX
k∈Sm,MMY
j=1kkj
j
kj!,
8which is η(m, M ). Since this is independent of r, it also holds after taking the supremum over
r∈ △ Mof the left hand side, showing that Ikl(m, m )≤η(m, M ).
The final step in obtaining Theorem 1 is to loosen the bound given in Corollary 1 (which is intractable
when mis large) to the tractable form given in Theorem 1. For this we require the following technical
lemma, the proof of which we defer to Appendix C.3.
Lemma 3. For integers M≥1andm≥M,P
k∈S>0
m,M1QM
j=1√
kj≤πM
2mM−2
2
Γ(M
2).
Proof. (Of Theorem 1) It suffices to show that for all m≥M≥1we have η(m, M )≤ξ(m, M ).
We achieve this by applying Stirling’s approximation√
2πn n
en< n!<√
2πn n
ene1
12n(valid
forn≥1) to the factorials in η(m, M )and then using Lemma 3.
Since Stirling’s approximation requires that all the kjare at least one, we partition the sum in
η(m, M )according to the number of coordinates of kat which kj= 0. Letzindex the number of
such coordinates. Defining f:S∞
M=2Sm,M→Rbyf(k) =Q|k|
j=1kkj
j/kj!and noting that fis
symmetric under permutations of its arguments, we then have
η(m, M ) =m!
mmX
k∈Sm,Mf(k) =m!
mmM−1X
z=0M
zX
k∈S>0
m,M−zf(k). (12)
Stirling’s approximation can now be applied to each k∈S>0
m,Mf(k)≤QM
j=1kkj
j√
2πkjkj
ekj=
QM
j=1ekj√
2πkj=em
(2π)M/2QM
j=11√
kj.An application of Lemma 3 now gives
X
k∈S>0
m,M−zf(k)≤X
k∈S>0
m,M−zem
(2π)M−z
2M−zY
j=11p
kj≤em
(2π)M−z
2πM−z
2mM−z−2
2
Γ M−z
2=emmM−z−2
2
2M−z
2Γ M−z
2.
Substituting this into equation (12) and bounding m!using Stirling’s approximation, we have
η(m, M )≤√
2πme1/(12m)
emPM−1
z=0 M
zemmM−z−2
2
2M−z
2Γ(M−z
2)=ξ(m, M ),which completes the proof of
the bound. As for the order of the bound, it is sufficient to bound lnξ(m, M )using the crude
approximations M
z
≤MM,(2/m)z/2≤1andΓ((M−z)/2)≥1.
6 Numerical experiments
We use binarised versions of MNIST, and HAM10000 Tschandl [2018]. In both cases we partition
Y2intoE0={(0,0),(1,1)},E1={(0,1)}andE2={(1,0)}, and take ℓ= (0,1,3). Each
dataset is split into prior and certification sets. We take Hto be two-layer MLPs. As is common in
the PAC-Bayes literature, we restrict PandQto be isotropic and diagonal Gaussian distributions
over the parameter space, respectively. The means of PandQare set to the parameters of an MLP
trained on the prior set. The mean and variances of Qand the variance of Pare tuned via Theorem
2 to minimize the bound on the total risk RT
D(Q). See Appendix A for pseudocode, Appendix B
for full experimental details and https://github.com/reubenadams/PAC-Bayes-Control for
code. The results for MNIST can be seen in Figure 1.
We estimate RS(Q)with a Monte Carlo and obtain a PAC-Bayes bound on RT
D(Q)by combining
Proposition 2 (with δ= 0.01andN= 100000 ) and Proposition 3. We obtain RT
D(Q)≤0.2640 for
MNIST and RT
D(Q)≤0.8379 for HAM10000, where both bounds hold with probability at least
1−0.05−0.01 = 0 .94. While these bounds are far from vacuous—the maximum possible value of
RT
D(Q)is3for our choice of ℓ—one might wonder whether one can do better by bounding each error
probability individually using Maurer’s inequality Maurer [2004], and then unioning these bounds.
As with our Theorem 1, this would also constrain the entire distribution of error types since for any ℓ,
one could then calculate the maximimum value of RT
D(Q)that satisfies all of these constraints. Both
9Dataset Volume Our Region Volume Maurer Region
MNIST 0.0025 (0.002498, 0.002504) 0.0028 (0.002793, 0.002800)
HAM10000 0.0012 (0.001207, 0.001211) 0.0011 (0.001142, 0.001146)
Table 1: Point estimates and 95% confidence intervals for the volumes of the confidence regions
forRD(Q)given by Theorem 1 and a union over Mindividual Maurer bounds, respectively. Our
method is superior for MNIST and inferior for HAM100000.
(a)
 (b)
 (c)
Figure 1: Experimental results for binarised MNIST. (a) The PAC-Bayes bound on the total risk
decreases when tuning the posterior via Theorem 2. (b) This is achieved by a shift in the empirical
error probabilities. (c) The bound on kl(RS(Q)∥RD(Q))is not substantially increased, meaning we
still retain good control of RD(Q)after optimizing Qfor this particular choice of ℓ.
methods constrain the region of the simplex in which RD(Q)can lie (with high probability), and a
reasonable metric by which to compare them is the volumes of these regions. This can be estimated
via a MC sample by uniformly sampling points rfrom△Mand counting how samples are legal
values of RD(Q)according to each method. The 95% confidence intervals for the volumes of the
two regions are given in Table 1. A more comprehensive table for synthetic values of RS(Q)can be
found in Appendix B.
7 Perspectives
We introduce the framework of error types, considering the vectors RS(Q)andRD(Q)of empirical
and true probabilities of errors of different types. We prove a PAC-Bayes bound (Theorem 1) on
kl(RS(Q)∥RD(Q))which controls the entire distribution of error probabilities, and hence can be
used to derive bounds on arbitrary linear combinations of the error probabilities, all of which hold
simultaneously with high probability; this cannot be achieved with any existing PAC-Bayes bound.
We construct a differential training objective based on our bound by introducing the the vectorised
kl inverse, providing a recipe for quickly computing its value and derivatives (Theorem 2). Our
framework is flexible enough to encompass multiclass classification or discretised regression, but
also structured output prediction, multi-task learning and learning-to-learn.
Another potential application of our work is to the excess risk, since under a misclassification loss
there are three different error types, corresponding to excess losses of {−1,0,1}. Biggs and Guedj
[2023] adapted Theorems 1 and 2 to this setting, leading to an empirically tighter PAC-Bayes bound
for certain classification tasks.
We require i.i.d. data, which in practice is frequently not the case or is hard to verify. Further, the
number of error types Mmust be finite. In continuous scenarios it would be preferable to be able to
control the entire distribution of loss values without having to discretise into finitely many error types.
We leave this direction to future work.
10Acknowledgments and Disclosure of Funding
We warmly thank reviewers and the Area Chair who provided insigthful comments and suggestions
which greatly helped us improve our manuscript. R.A. was supported by the UKRI grant number
EP/S021566/1 and gratefully thanks Felix Biggs for his insights. J.S-T gratefully acknowledges the
European Union’s Horizon 2020 Research and Innovation Program through the grant numbers 951847
(European learning and intelligent systems excellence, ELISE) and 952026 (Human-centred artificial
intelligence, HumanE-AI-Net). B.G. acknowledges partial support by the U.S. Army Research
Laboratory and the U.S. Army Research Office, and by the U.K. Ministry of Defence and the U.K.
Engineering and Physical Sciences Research Council (EPSRC) under grant number EP/R013616/1.
B.G. acknowledges partial support from the French National Agency for Research, through grants
ANR-18-CE40-0016-01 and ANR-18- CE23-0015-02, and through the programme “France 2030”
and PEPR IA on grant SHARP ANR-23-PEIA-0008.
References
Pierre Alquier. User-friendly introduction to PAC-Bayes bounds. arXiv preprint arXiv:2110.11216 ,
2021.
Amiran Ambroladze, Emilio Parrado-Hernández, and John Shawe-Taylor. Tighter PAC-Bayes
bounds. In Bernhard Schölkopf, John C. Platt, and Thomas Hofmann, editors, Advances in Neural
Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural
Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006 ,
pages 9–16. MIT Press, 2006. URL https://proceedings.neurips.cc/paper/2006/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html .
Felix Biggs and Benjamin Guedj. On margins and derandomisation in pac-bayes. In International
Conference on Artificial Intelligence and Statistics , pages 3709–3731. PMLR, 2022a.
Felix Biggs and Benjamin Guedj. Non-vacuous generalisation bounds for shallow neural networks.
InInternational Conference on Machine Learning , pages 1963–1981. PMLR, 2022b.
Felix Biggs and Benjamin Guedj. Tighter pac-bayes generalisation bounds by leveraging example
difficulty. In International Conference on Artificial Intelligence and Statistics , pages 8165–8182.
PMLR, 2023.
Luc Bégin, Pascal Germain, François Laviolette, and Jean-Francis Roy. PAC-Bayesian Bounds based
on the Rényi Divergence. In Arthur Gretton and Christian C. Robert, editors, Proceedings of the
19th International Conference on Artificial Intelligence and Statistics , volume 51 of Proceedings
of Machine Learning Research , pages 435–444, Cadiz, Spain, 09–11 May 2016. PMLR. URL
https://proceedings.mlr.press/v51/begin16.html .
Olivier Catoni. A PAC-Bayesian approach to adaptive classification. preprint , 840, 2003.
Olivier Catoni. Statistical Learning Theory and Stochastic Optimization: Ecole d’Eté de Probabilités
de Saint-Flour XXXI-2001 . Springer, 2004.
Olivier Catoni. PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning ,
volume 56 of Institute of Mathematical Statistics (IMS) Lecture Notes - Monograph Series . Institute
of Mathematical Statistics, 2007. ISBN 9780940600720. URL https://books.google.fr/
books?id=acnaAAAAMAAJ .
Eugenio Clerico, George Deligiannidis, and Arnaud Doucet. Conditionally gaussian pac-bayes. In
International Conference on Artificial Intelligence and Statistics , pages 2311–2329. PMLR, 2022.
Imre Csiszár. I-divergence geometry of probability distributions and minimization problems. The
Annals of Probability , pages 146–158, 1975.
MD Donsker and SRS Varadhan. Large deviations for Markov processes and the asymptotic evaluation
of certain markov process expectations for large times. In Probabilistic Methods in Differential
Equations , pages 82–88. Springer, 1975.
11Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Conference on
Uncertainty in Artificial Intelligence [UAI] , 2017.
Gintare Karolina Dziugaite and Daniel M. Roy. Entropy-SGD optimizes the prior of a PAC-Bayes
bound: Generalization properties of entropy-SGD and data-dependent priors. In Jennifer G.
Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 , volume 80
ofProceedings of Machine Learning Research , pages 1376–1385. PMLR, 2018. URL http:
//proceedings.mlr.press/v80/dziugaite18a.html .
Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and Daniel Roy. On
the role of data in pac-bayes bounds. In International Conference on Artificial Intelligence and
Statistics , pages 604–612. PMLR, 2021.
Pascal Germain, Alexandre Lacasse, Francois Laviolette, Mario Marchand, and Jean-Francis Roy.
Risk bounds for the majority vote: From a pac-bayesian analysis to a learning algorithm. arXiv
preprint arXiv:1503.08329 , 2015.
Benjamin Guedj. A Primer on PAC-Bayesian Learning. In Proceedings of the second congress of the
French Mathematical Society , 2019. URL https://arxiv.org/abs/1901.05353 .
Sokol Koço and Cécile Capponi. On multi-class classification through the minimization of the
confusion matrix norm. In Asian Conference on Machine Learning , pages 277–292. PMLR, 2013.
Alexandre Lacasse, François Laviolette, Mario Marchand, Pascal Germain, and Nicolas Usunier.
Pac-bayes bounds for the risk of the majority vote and the variance of the gibbs classifier. Advances
in Neural information processing systems , 19, 2006.
John Langford and Rich Caruana. (not) bounding the true error. Advances in Neural Information
Processing Systems , 14, 2001.
François Laviolette, Emilie Morvant, Liva Ralaivola, and Jean-Francis Roy. Risk upper bounds
for general ensemble methods with an application to multiclass classification. Neurocomputing ,
219:15–25, 2017. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2016.09.016. URL
https://www.sciencedirect.com/science/article/pii/S0925231216310177 .
Yunwen Lei, Ürün Dogan, Ding-Xuan Zhou, and Marius Kloft. Data-dependent generalization
bounds for multi-class classification. IEEE Transactions on Information Theory , 65(5):2995–3021,
2019. doi: 10.1109/TIT.2019.2893916.
Gaël Letarte, Pascal Germain, Benjamin Guedj, and Francois Laviolette. Dichotomize and generalize:
PAC-Bayesian binary activated deep neural networks. In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. dAlché Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems 32 , pages 6872–6882. Curran Associates, Inc., 2019.
Guy Lever, François Laviolette, and John Shawe-Taylor. Distribution-dependent PAC-Bayes priors.
InInternational Conference on Algorithmic Learning Theory , pages 119–133. Springer, 2010.
Guy Lever, François Laviolette, and John Shawe-Taylor. Tighter PAC-Bayes bounds through
distribution-dependent priors. Theoretical Computer Science , 473:4–28, February 2013. ISSN
0304-3975. doi: 10.1016/j.tcs.2012.10.013. URL https://linkinghub.elsevier.com/
retrieve/pii/S0304397512009346 .
Andreas Maurer. A note on the PAC-Bayesian theorem. arXiv preprint cs/0411099 , 2004.
David A McAllester. Some PAC-Bayesian theorems. In Proceedings of the eleventh annual conference
on Computational Learning Theory , pages 230–234. ACM, 1998.
David A McAllester. PAC-Bayesian model averaging. In Proceedings of the twelfth annual conference
on Computational Learning Theory , pages 164–170. ACM, 1999.
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient
estimation in machine learning. J. Mach. Learn. Res. , 21(132):1–62, 2020.
12Emilie Morvant, Sokol Koço, and Liva Ralaivola. PAC-Bayesian generalization bound on confusion
matrix for multi-class classification. In Proceedings of the 29th International Conference on
Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012 . icml.cc /
Omnipress, 2012. URL http://icml.cc/2012/papers/434.pdf .
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian approach to
spectrally-normalized margin bounds for neural networks. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings . OpenReview.net, 2018. URL https://openreview.net/forum?id=Skz_
WfbCZ .
Emilio Parrado-Hernández, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PAC-
Bayes bounds with data dependent priors. J. Mach. Learn. Res. , 13:3507–3531, 2012. URL
http://dl.acm.org/citation.cfm?id=2503353 .
María Pérez-Ortiz, Omar Rivasplata, Benjamin Guedj, Matthew Gleeson, Jingyu Zhang, John Shawe-
Taylor, Miroslaw Bober, and Josef Kittler. Learning pac-bayes priors for probabilistic neural
networks. arXiv preprint arXiv:2109.10304 , 2021.
Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesvari. Tighter risk cer-
tificates for neural networks. Journal of Machine Learning Research , 22(227):1–40, 2021. URL
http://jmlr.org/papers/v22/20-879.html .
Bernardo Avila Pires, Csaba Szepesvari, and Mohammad Ghavamzadeh. Cost-sensitive multiclass
classification risk bounds. In International Conference on Machine Learning , pages 1391–1399.
PMLR, 2013.
Omar Rivasplata, Csaba Szepesvári, John Shawe-Taylor, Emilio Parrado-Hernández, and Shiliang
Sun. PAC-Bayes bounds for stable algorithms with instance-dependent priors. In Samy Ben-
gio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman
Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal,
Canada , pages 9234–9244, 2018. URL https://proceedings.neurips.cc/paper/2018/
hash/386854131f58a556343e056f03626e00-Abstract.html .
Matthias Seeger. PAC-Bayesian generalisation error bounds for Gaussian process classification.
Journal of Machine Learning Research , 3(Oct):233–269, 2002.
Akira Takayama and Takayama Akira. Mathematical economics . Cambridge university press, 1985.
Philipp Tschandl. The HAM10000 dataset, a large collection of multi-source dermatoscopic images
of common pigmented skin lesions, 2018. URL https://doi.org/10.7910/DVN/DBW86T .
Yi-Shan Wu and Yevgeny Seldin. Split-kl and pac-bayes-split-kl inequalities for ternary random
variables. Advances in Neural Information Processing Systems , 35:11369–11381, 2022.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, and Peter Orbanz. Non-vacuous
generalization bounds at the ImageNet scale: a PAC-Bayesian compression approach. In 7th
International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019 . OpenReview.net, 2019. URL https://openreview.net/forum?id=BJgqqsAct7 .
13A Recipe for implementing Theorems 1 and 2
We here outline more explicitly how Theorem 1 and Theorem 2 may be used to formulate a fully
differentiable objective by which a model may be trained.
First, if one wishes to make hard labels, namely H ⊆ YX, it will first be necessary to use a surrogate
class of soft hypotheses H′⊆ M (Y)Xduring training, before reverting to hard labels for example
by taking the mean label or the one with highest probability. Using soft hypotheses during training
is necessary to ensure that the empirical j-risks Rj
S(Q)are differentiable with respect to the model
parameters. Since how one chooses to do this will depend on the specific use case, we restrict our
attention here to the case of soft hypotheses. Specifically, we consider a class of soft hypotheses
H={hθ:θ∈RN} ⊆ M (Y)Xparameterised by the weights θ∈RNof some neural network
of a given architecture with Nparameters in such a way that the Rj
S(hθ)are differentiable in θ. A
concrete example would be multiclass classification using a fully connected neural network with
output being softmax probabilities on the classes so that the Rj
S(hθ)are differentiable.
Second, it is necessary to restrict the prior and posterior P, Q∈ M (H)to a parameterised subset
ofM(H)in which KL(Q∥P)has a closed form which is differentiable in the parameterisation. A
simple choice for our case of a neural network with Nparameters is P, Q∈ {N (w,diag(s)) :w∈
RN,s∈RN
>0}. For prior a Pv,r=N(v,diag(r))and posterior Qw,s=N(w,diag(s))we have
the closed form
KL(Qw,s∥Pv,r) =1
2"NX
n=1sn
rn+(wn−vn)2
rn+ lnrn
sn
−N#
,
which is indeed differentiable in v,r,wands. While Qw,sandPv,rare technically distributions on
RDrather than H, the KL-divergence between the distributions they induce on Hwill be at most as
large as the expression above. Thus, substituting the expression above into the bounds we prove in
Section 3 can only increase the value of the bounds, meaning the enlarged bounds certainly still hold
with probability at least 1−δ.
Third, in all but the simplest cases Rj
S(Qw,s)will not have a closed form, much less one that is
differentiable in wands. A common solution to this is to use the so-called pathwise gradient
estimator. In our case, this corresponds to drawing ϵ∼ N(0,I), where Iis the N×Nidentity
matrix, and estimating
∇w,sRj
S(Qw,s) =∇w,sh
Eϵ′∼N(0,I)Rj
S(hw+ϵ′⊙√s)i
≈ ∇w,sRj
S(hw+ϵ⊙√s),
where hwdenotes the function expressed by the neural network with parameters w. For a proof that
this is an unbiased estimator, and for other methods for estimating the gradients of expectations, see
the survey Mohamed et al. [2020].
Fourth, one must choose the prior. Designing priors which are optimal in some sense ( i.e., minimising
the Kullback-Leibler term in the right-hand side of generalisation bounds) has been at the core of an
active line of work in the PAC-Bayesian literature. For the sake of simplicity, and since it is out of the
scope of our contributions, we assume here that the prior is given beforehand, although we stress that
practitioners should pay great attention to its tuning. For our purposes, it suffices to say that if one
is using a data-dependent prior then it is necessary to partition the sample into S=SPrior∪SBound ,
where SPrioris used to train the prior and SBound is used to evaluate the bound. Since our bound holds
uniformly over posteriors Q∈ M (H), the entire sample Sis free to be used to train the posterior
Q. For a more in-depth discussion on the choice of prior, we refer to the following body of work:
Ambroladze et al. [2006], Lever et al. [2010, 2013], Parrado-Hernández et al. [2012], Dziugaite and
Roy [2017, 2018], Rivasplata et al. [2018], Letarte et al. [2019], Pérez-Ortiz et al. [2021], Dziugaite
et al. [2021], Biggs and Guedj [2022a,b].
Finally, given a confidence level δ∈(0,1], one may use Algorithm 1 to obtain a posterior Qw,s
with minimal upper bound on the total risk. Note we take the pointwise logarithm of the variances
randsto obtain unbounded parameters on which to perform stochastic gradient descent or some
other minimisation algorithm. We use ⊕to denote vector concatenation. The algorithm can be
straightforwardly adapted to permit mini-batches by, for each epoch, sequentially repeating the steps
withSequal to each mini-batch.
14Input:
X,Y/* Arbitrary input and output spaces */SM
j=1Ej=Y2/* A finite partition into error types */
ℓ∈[0,∞)M/* A vector of losses, not all equal */
S=SPrior∪SBound∈(X × Y )m/* A partitioned i.i.d. sample */
N∈N/* The number of model parameters */
Pv,r,v(SPrior)∈RN,r(SPrior)∈RN
≥0/* A (data-dependent) prior */
Qw0,s0,w0∈RN,s0∈RN
≥0/* An initial posterior */
δ∈(0,1]/* A confidence level */
λ >0/* A learning rate */
T/* The number of epochs to train for */
Output:
Qw,s,w∈RN,s∈RN
≥0/* A trained posterior */
Procedure:
ζ0←logs0/* Transform to unbounded scale parameters */
p←w0⊕ζ0/* Collect mean and scale parameters */
fort←1toTdo
Drawϵ∼ N(0,I)
u←RS
hw+ϵ⊙√
exp(ζ)
B←
1
m
KL 
Qw,exp(ζ)Pv,r
+ ln
1
δ√πe1/12m m
2M−1
2PM−1
z=0 M
z1
(πm)z/2Γ(M−z
2)
˜u←(u1, . . . , u M, B)
G←02N×(M+1)/* Initialise gradient matrix */
F←0M+1/* Initialise gradient vector */
forj←1toM+ 1do
Fj←∂f∗
ℓ
∂˜uj(˜u)/* Gradients of total loss from Theorem 2 */
fori←1to2Ndo
Gi,j←∂˜uj
∂pi(p)/* Gradients of empirical risks and bound */
end
end
H←GF /* Gradients of total loss w.r.t. parameters */
p←p−λH/* Gradient step */
end
w= (p1, . . . , p N)
s= (exp( pN+1), . . . , exp(p2N))
return w,s
Algorithm 1: Calculating a posterior with minimal bound on the total risk.
B Additional Experimental Details
For MNIST we map labels {0,1,2,3,4}to0and{5,6,7,8,9}to1. For HAM10000 we map the
cancerous or pre-cancerous labels {Melanoma ,Basal Cell Carcinoma ,Actinic Keratosis }
to1and the other labels to 0. In both cases we partition Y2intoE0={(0,0),(1,1)},E1={(0,1)}
andE2={(1,0)}, and take ℓ= (0,1,3). For HAM10000, E1andE2then refer to Type I and Type
II errors, respectively, and ℓreflects the greater severity of false negatives.
Each dataset is split into prior and certification sets SPriorandSBound , respectively. For MNIST, we
use the conventional training set of size 60000 as the prior set, and the conventional test set of size
10000 as the certification set. For HAM10000 we pool the conventional train, validation and test sets
together and then split 50-50 to obtain prior and certification sets each of size 5860 . For HAM10000
15we resize the images to (28,28)and use just the first channel so that the data dimension is the same
for both datasets.
We take Hto be two-layer MLPs with 784, 100 and 2 units in the input, hidden and output layers,
respectively. As is common in the PAC-Bayes literature, we restrict Pto be an isotropic Gaussian
N(v, λI)andQto be a diagonal Gaussian N(w,diag(s)). Further, as in Dziugaite and Roy [2017],
we restrict λto be of the form λj=cexp(−j/b)for some j∈N, taking c= 0.1andb= 100 .
Since, at the end of training, we will then have one prior Pjfor each j∈N, we can choose the jthat
minimizes the PAC-Bayes bound provided we take a union over all of them, taking δj=6δ
π2j2so thatP
jδj= 1and all the bounds hold simultaneously with probability at least 1−δ. After applying
Algorithm 1 we round λto a discrete λj, either up or down depending on which gives the smaller
bound.
For both datasets we set the prior mean vto be the parameters of an MLP trained on the prior
set. In both cases we use SGD with learning rate 0.01to minimise the cross-entropy loss, using
a portion of the prior set as a validation set. For MNIST we train the MLP for 20 epochs to get
an error rate of 14%, for HAM10000 we train the MLP for 5 epochs to get an error rate of 22%.
We then apply Algorithm 1. By combining Proposition 2 (with δ= 0.01andN= 100000 ) and
Proposition 3. We obtain RS(ˆQ) = (0 .8879,0.0919,0.0203) andRT
D(Q)≤0.2640 for MNIST and
RS(ˆQ) = (0 .7860,0.0146,0.1995) andRT
D(Q)≤0.8379 for HAM10000, where both bounds hold
with probability at least 1−0.05−0.01 = 0 .94.
The full results are shown in Figure 2. Figures 2a, 2c and 2e are the same as Figures 1a, 1b and
1c, and are repeated here for easier comparison with the HAM10000 results. Figure 2b shows that
Algorithm 1 has failed to reduce the bound on the total risk beyond the initialisation of QtoP, with
the small variation being explained by different MC samples being drawn from Qduring training
rather than Qchanging substantially. Indeed, Figure 2h shows that Qdoes not appreciably move from
its initialisation at P—KL(Q∥P)remains below 0.1whereas in the MNNIST experiment, which
has the same number of parameters, exceeds 30. It is therefore unsurprising that Figures 2d and 2f
show negligible change in the empirical error probabilities and the bound on kl(RS(Q)∥RD(Q)),
respectively. The divergence in the results is likely due to the difference in sample size; the certification
set for the MNIST experiment contains 10000 samples, whereas for the HAM10000 dataset there are
only5000 , which, all else equal, makes an increase in KL (Q∥P)twice as expensive.
Recall from Section 6 that while RD(Q)can be effectively constrained to a sub-region of the simple
△Musing our Theorem 1, this can also be achieved by unioning MMaurer bounds, one for each
error probability. Table 1 gave the 95% confidence intervals for the volumes of the confidence regions
in which RD(Q)was likely to lie for experiments on MNIST and HAM10000, but neither region
was uniformly smaller, making it unclear which method should be preferred.
Table 2 provides additional data by taking synthetic values for RS(Q)andKL(Q∥P), for different
values of m(the size of the certification set) and M(the number of error types). ‘Individual’ denotes
unioning individual Maurer bounds, ‘Ours’ is our method, ‘Intersection’ is the intersection of the
confidence regions given by the previous two methods (but loosened so that they now both hold
simultaneously with probability at least 0.95), and ‘Morv.’ is the confidence region produced by
Morvant’s bound Morvant et al. [2012]. The 95% confidence intervals for the volumes of all the
regions have been produced by Monte Carlo samples. We see that our confidence region is tighter
than the individual one in 4/9 cases (green), worse in 3/9 cases (red) and ties in 2/9 cases (orange).
Interestingly, union bounding the naive CR and our CR and intersecting often beats both of these
(bold ). Morvant’s result is either not applicable or their confidence region is much larger than ours
and essentially takes up the entire simplex, hence the volume estimate of 1.000. The reason their
bound is sometimes inapplicable is because it requires every class to contain at least 8Linstances,
where Lis the number of labels—in the L= 5, M= 25, m= 100 case this would require each class
to contain at least 5×8 = 40 instances which is impossible with m= 100 samples.
16(a)
 (b)
(c)
 (d)
(e)
 (f)
(g)
 (h)
Figure 2: MNIST (first column) and HAM10000 (second column) experiments.
17M m V ol. Individual V ol. Ours V ol. Intersection V ol. Morv.
100 (0.1195, 0.1196) (0.1165, 0.1166) (0.1160, 0.1161) (1.0, 1.0)
22300 (0.02920, 0.02926) (0.03071, 0.03078) (0.02893, 0.02900) (1.0, 1.0)
1000 (5.635e-3, 5.664e-3) (6.475e-3, 6.507e-3) (5.706e-3, 5.735e-3) (1.0, 1.0)
100 (0.3190, 0.3192) (0.1757, 0.1758) (0.1582, 0.1584) N/A
52300 (1.306e-3, 1.320e-3) (3.672e-4, 3.748e-4) (2.515e-4, 2.578e-4) (1.0, 1.0)
1000 (1.090e-08, 1.024-07) (2.422e-09, 7.225e-08) (0.000, 3.689e-08) (1.0, 1.0)
100 (0.9990, 0.9990) (1.000, 1.000) (0.9995, 0.9995) N/A
102300 (0.3534, 0.3536) (0.1688, 0.1689) (0.1306, 0.1307) N/A
1000 (3.454e-8, 1.5763e-7) (0.000, 3.688e-8) (0.000, 3.688e-8) (1.0, 1.0)
Table 2: 95% confidence intervals for the volumes of the confidence regions for RD(Q). We set
KL(Q∥P) = 0 ,δ= 0.05,RS(Q) = (1 /M, ..., 1/M)and use 108Monte Carlo samples.
C Proofs
C.1 Proof of Proposition 3
Write kl (ˆq∥p)as
MX
j=1ˆqjlnˆqj
qj+MX
j=1ˆqjlnqj
pj. (13)
The result then follows by bounding the two sums by
MX
j=1ˆqjlnˆqj
qj=MX
j=1kl(ˆqj∥qj)−(1−ˆqj) ln1−ˆqj
1−qj≤MB 2−MX
j=1(1−ˆqj) ln1−ˆqj
1−qj(14)
and
MX
j=1ˆqjlnqj
pj=MX
j=1ˆqj
qjqjlnqj
pj≤max
jˆqj
qjMX
j=1qjlnqj
pj≤B1max
jˆqj
qj. (15)
Putting these together we obtain the bound on kl(ˆq∥p). The limit follows because each qj→ˆqjas
B2→0.
C.2 Proof of Lemma 2
LetEM:={e1, . . . ,eM}, namely the set of M-dimensional basis vectors. We will denote a typical
element of Em
Mbyη(m)= (η1, . . . ,ηm). For any x(m)= (x1, . . . ,xm)∈ △m
M, a straightforward
induction on myields
X
η(m)∈Em
M mY
i=1xi·ηi!
= 1. (16)
To see this, for m= 1we have E1
M={(e1,), . . . , (eM,)}, where we have been pedantic in using
1-tuples to maintain consistency with larger values of m. Thus, for any x(1)= (x1,)∈ △1
M, the left
hand side of equation (16) can be written as
MX
j=1x1·ej=MX
j=1(x1)j= 1.
18Now suppose that equation (16) holds for any x(m)∈ △m
Mand let x(m+1)= (x1, . . . ,xm+1)∈
△m+1
M. Then the left hand side of equation (16) can be written as
X
η(m+1)∈Em+1
M m+1Y
i=1xi·ηi!
=X
η(m)∈Em
MMX
j=1 mY
i=1xi·ηi!
(xm+1·ej)
=X
η(m)∈Em
M mY
i=1xi·ηi!MX
j=1(xm+1·ej) = 1 .
We now show that any x(m)= (x1, . . . ,xm)∈ △m
Mcan be written as a convex combination of the
elements of Em
Min the following way
x(m)=X
η(m)∈Em
M mY
i=1xi·ηi!
η(m). (17)
We have already shown that the weights sum to one, and they are clearly elements of [0,1], so the
right hand side of equation (17) is indeed a convex combination of the elements of Em
M. We now
show that equation (17) holds, again by induction.
Form= 1and any x(1)= (x1,)∈ △1
M, the right hand side of equation (17) can be written as
MX
j=1(x1·ej)(ej,) = (x1,) =x.
For the inductive hypothesis, suppose equation (17) holds for some arbitrary m≥1, and denote
elements of Em+1
M byη(m)⊕(e,)for some η(m)∈Em
Mande∈EM, where ⊕denotes vector
concatenation. Then for any x(m+1)=x(m)⊕(xm+1,) = (x1, . . . ,xm+1)∈ △m+1
M, the right
hand side of equation (17) can be written as
X
η(m+1)∈Em+1
M m+1Y
i=1xi·ηi!
η(m+1)=X
η(m)∈Em
MMX
j=1 mY
i=1xi·ηi!
(xm+1·ej)η(m)⊕(ej,)
=X
η(m)∈Em
MMX
j=1 mY
i=1xi·ηi!
(xm+1·ej)η(m)
⊕X
η(m)∈Em
MMX
j=1 mY
i=1xi·ηi!
(xm+1·ej)(ej,)
=MX
j=1(xm+1·ej)X
η(m)∈Em
M mY
i=1xi·ηi!
η(m)
⊕X
η(m)∈Em
M mY
i=1xi·ηi!MX
j=1(xm+1·ej)(ej,)
= 1·x(m)⊕1·(xm+1,) =x(m+1),
where in the penultimate equality we have used the inductive hypothesis and (twice) the result of the
previous induction.
We can now prove the statement of the Lemma. Applying Jensen’s inequality to equation (17) with
the convex function f, we have that
f(x1, . . . ,xm) =f
X
η(m)∈Em
M mY
i=1xi·ηi!
η(m)

≤X
η(m)∈Em
M mY
i=1xi·ηi!
f
η(m)
.
19Letµ=E[X1]denote the mean of the i.i.d. random vectors Xi. Then the above inequality implies
E[f(X1, . . . ,Xm)]≤X
η(m)∈Em
M mY
i=1µ·ηi!
f
η(m)
=X
η(m)∈Em
M mY
i=1P(X′
i=ηi)!
f
η(m)
=E[f(X′
1, . . . ,X′
m)].
C.3 Proof of Lemma 3
The proof of Lemma 3 itself requires two technical helping lemmas which we now state and prove.
Lemma 4. For any integers n≥2andp≥ −1,
n−1X
k=1(n−k)p/2
√
k≤np+1
2Z1
0(1−x)p/2
√xdx.
Proof. The case of p=−1, namely
n−1X
k=11p
k(n−k)≤Z1
01p
x(1−x)dx,
has already been demonstrated in Maurer [2004]. For p >−1, let
fp(x) :=(1−x)p/2
√x.
We will show that each fp(·)is monotonically decreasing on (0,1). Indeed,
d fp
dx(x) =−(1−x)p
2−1(px+ 1−x)
2x3/2≤ −(1−x)p/2
2x3/2<0,
where for the inequalities we have used the fact that p >−1andx∈(0,1). We therefore see that
n−1X
k=1(n−k)p/2
√
k=n−1X
k=1np/2(1−k
n)p/2
√nq
k
n
=np+1
2n−1X
k=11
n(1−k
n)p/2
q
k
n
=np+1
2n−1X
k=11
nfpk
n
≤np+1
2n−1X
k=1Zk
n
k−1
nfp(x)dx
=np+1
2Z1−1
n
0fp(x)dx
≤np+1
2Z1
0fp(x)dx.
Intuitively, the proof of the above lemma works by bounding the integral below by a Riemann sum.
In the following lemma we actually calculate this integral, yielding a more explicit bound on the sum
in Lemma 4. We found it is easier to calculate a slightly more general integral, where the 1in the
limit and the integrand is replaced by a positive constant a.
20Lemma 5. For any real number a >0and integer n≥ −1,
Za
0(a−x)n/2
√xdx=√πΓ(n+2
2)
Γ(n+3
2)an+1
2.
Proof. Define
In(a) :=Za
0(a−x)n/2
√xdx and fn(a) :=√πΓ(n+2
2)
Γ(n+3
2)an+1
2.
We proceed by induction, increasing nby2each time. This means we need two base cases. First, for
n=−1, we have
I−1(a) =Za
01p
x(a−x)dx=
2 arcsinrx
aa
0=π=f−1(a),
since Γ(1
2) =√πandΓ(1) = 1 . Second, for n= 0,
I0(a) =Za
01√xdx=
2√xa
0= 2√a=f0(a),
since Γ(3
2) =√π
2. Now, by the Leibniz integral rule, we have
d
daIn+2(a) =Za
0∂
∂a(a−x)n+2
2√xdx=n+ 2
2Za
0(a−x)n
2√xdx=n+ 2
2In(a).
Thus
In+2(a) =n+ 2
2Za
0In(t)dt+In(0)
=n+ 2
2Za
0In(t)dt,
since I n(0) = 0 .
Now, for the inductive step, suppose In(a) =fn(a)for some n≥ −1. Then, using the previous
calculation, we have
In+2(a) =n+ 2
2Za
0fn(t)dt
=n+ 2
2Za
0√πΓ(n+2
2)
Γ(n+3
2)tn+1
2dt
=√πn+2
2Γ(n+2
2)
n+3
2Γ(n+3
2)an+3
2
=√πΓ(n+2
2+ 1)
Γ(n+3
2+ 1)an+3
2
=√πΓ
(n+2)+2
2
Γ
(n+2)+3
2a(n+2)+1
2
=fn+2(a).
This completes the proof.
We are now ready to prove Lemma 3 which, for ease of reference, we restate here. For integers
M≥1andm≥M,
X
k∈S>0
m,M1QM
j=1p
kj≤πM
2mM−2
2
Γ(M
2).
21Proof. (of Lemma 3) We proceed by induction on M. For M= 1, the set Sm,M contains a single
element, namely the one-dimensional vector k= (k1,) = (m,). In this case, the left hand side is
1/√mwhile the right hand side is√π/(√mΓ(1/2)) = 1 /√m, since Γ(1/2) =√π.
Now, as the inductive hypothesis, assume the inequality of Lemma 3 holds for some fixed M≥1
and all m≥M. Then for all m≥M+ 1, we have
X
k∈S>0
m,M +11QM+1
j=1p
kj=m−MX
k1=11√k1X
k′∈S>0
m−k1,M1
QM
j=1q
k′
j
≤m−MX
k1=11√k1πM
2(m−k1)M−2
2
Γ(M
2)(by the inductive hypothesis)
=πM
2
Γ(M
2)m−MX
k1=1(m−k1)M−2
2√k1
≤πM
2
Γ(M
2)m−1X
k1=1(m−k1)M−2
2√k1(enlarging the sum domain)
≤πM
2
Γ(M
2)mM−1
2Z1
0(1−x)M−2
2√xdx (by Lemma 4)
=πM
2
Γ(M
2)mM−1
2√πΓ(M
2)
Γ(M+1
2)(by Lemma 5)
=πM+1
2mM−1
2
Γ(M+1
2),
as required.
C.4 Proof of Proposition 1
Proof. The case where qj= 1 orpj= 1 can be dealt with trivially by splitting into the three
following subcases
•qj=pj= 1 = ⇒kl(qj∥pj) =kl(q∥p) = 0
•qj= 1, pj̸= 1 = ⇒kl(qj∥pj) =kl(q∥p) =−logpj
•qj̸= 1, pj= 1 = ⇒kl(qj∥pj) =kl(q∥p) =∞.
Forqj̸= 1andpj̸= 1define the distributions ˜q,˜p∈ △ Mby˜qj= ˜pj= 0and
˜qi=qi
1−qjand ˜pi=pi
1−pj
fori̸=j. Then
X
i̸=jqilogqi
pi=X
i̸=j(1−qj)˜qilog(1−qj)˜qi
(1−pj)˜pi
= (1−qj)X
i̸=j˜qilog˜qi
˜pi+ ˜qilog1−qj
1−pj
= (1−qj)kl(˜q∥˜p) + (1 −qj) log1−qj
1−pj
≥(1−qj) log1−qj
1−pj.
22The final inequality holds since kl(˜q∥˜p)≥0. Further, note that we have equality if and only if ˜q=˜p,
which, by their definitions, translates to
pi=1−pj
1−qjqi
for all i̸=j. If we now add qjlogqj
pjto both sides, we obtain
kl(q∥p)≥(1−qj) log1−qj
1−pj+qjlogqj
pj=kl(qj∥pj),
with the same condition for equality.
The following proposition makes more precise the argument found at the beginning of Section 4
for how Proposition 1 can be used to derive the tightest possible lower and upper bounds on each
Rj
D(Q).
Proposition 5. Suppose that q,p∈ △ Mare such that kl(q∥p)≤B, where qis known and pis
unknown. Then, in the absence of any further information, the tightest bound that can be obtained on
eachpjis
pj≤kl−1(qj, B).
Proof. Suppose pj>kl−1(qj, B). Then, by definition of kl−1, we have that kl(qj∥pj)> B .
By Proposition 1, this would then imply kl(q∥p)> B , contradicting our assumption. Therefore
pj≤kl−1(qj, B). Now, with the information we have, we cannot rule out that
pi=1−pj
1−qjqi
for all i̸=jand thus, by Proposition 1, that kl(qj∥pj) =kl(q∥p). Further, we cannot rule out that
kl(q∥p) =B. Thus, it is possible that kl(qj∥pj) =B, in which case pj=kl−1(qj, B). We therefore
see that kl−1(qj, B)is the tightest possible upper bound on pj, for each j∈[M].
C.5 Proof of Theorem 2
Before proving the proposition, we first argue that kl−1
ℓ(u|c)given by Definition 1 is well-defined.
First, note that Au:={v∈ △ M:kl(u∥v)≤c}is compact (boundedness is clear and it is closed
because it is the preimage of the closed set [0, c]under the continuous map v7→kl(u∥v)) and so the
continuous function fℓachieves its supremum on Au. Further, note that Auis a convex subset of
△M(because the map v7→kl(u∥v)is convex) and fℓis linear, so the supremum of fℓoverAuis
achieved and is located on the boundary of Au. This means we can replace the inequality constraint
kl(u∥v)≤cin Definition 1 with the equality constraint kl(u∥v) =c. Finally, if u∈ △>0
MthenAu
is astrictly convex subset of △M(because the map v7→kl(u∥v)is then strictly convex) and so the
supremum of fℓoccurs at a unique point on the boundary of Au. In other words, if u∈ △>0
Mthen
kl−1
ℓ(u|c)is defined uniquely .
We now prove Theorem 2. While our proof technique is somewhat analogous to the technique used
in Clerico et al. [2022] to obtain derivatives of the one-dimensional kl-inverse, our theorem directly
yields derivatives on the total risk by (implicitly) employing the envelope theorem (see for example
Takayama and Akira, 1985).
Proof Outline: We first derive the expression given for v∗(˜u) =kl−1
ℓ(u|c)given on line (5) of the
theorem using the method of Lagrange multipliers. Since we are working on the simplex, we make
things easier for ourselves by first making the substitution tj= lnvjto make the vj>0constraints
unnecessary. The method of Lagrange multipliers yields both the maximum and the minimum (recall
thatkl−1
ℓ(u|c)is defined as the location of a maximum) for the two values of the Lagrange multiplier
µ. We show that exactly one of these values lies in the interval µ∈(−∞,−max jℓj)and that this
one corresponds to the maximum. This shows that the value µ∗Theorem 2 instructs us to find indeed
yields v∗(˜u) =kl−1
ℓ(u|c). Finally, we derive the partial derivatives of kl−1
ℓ(u|c)with respect the ˜uj
to obtain the second part of the theorem, namely line (6) by employing the envelope theorem.
23Proof. (of Theorem 2) We start by deriving the implicit expression for v∗(˜u) =kl−1
ℓ(u|c)given in
the proposition by solving a transformed version of the optimisation problem given by Definition
1 using the method of Lagrange multipliers. We obtain two solutions to the Lagrangian equations,
which must correspond to the maximum and minimum total risk over the set Au:={v∈ △ M:
kl(u∥v)≤c}because, as argued in the main text (see the discussion after Definition 1), Auis
compact and so the linear total risk fℓ(v)attains its maximum and minimum on Au.
By definition of v∗(˜u) = kl−1
ℓ(u|c), we know that kl(v∗(˜u)∥u)≤c. Since, by assumption,
uj>0for all j, we see that v∗(˜u)j>0for all j, otherwise we would have kl(v∗(˜u)∥u) =∞, a
contradiction. Thus v∗(˜u)∈ △>0
Mand we are permitted to instead optimise over the unbounded
variable t∈RM, where tj:= ln vj. With this transformation, the constraint v∈ △ Mcan be
replaced simply byP
jetj= 1and the optimisation problem becomes
Maximise: F(t) :=MX
j=1ℓjetj
Subject to: g(t;u, c) := kl(u∥et)−c= 0,
h(t) :=MX
j=1etj−1 = 0 ,
where et∈RMis defined by (et)j:=etj. Note that F(t) =fℓ(et). Following the terminology
of mathematical economics, we call the tjtheoptimisation variables , and the ˜uj(namely the uj
andc) the choice variables . The vector ℓis considered fixed—we neither want to optimise over
it nor differentiate with respect to it—which is why we occasionally suppress it from the notation
henceforth.
For each ˜u, letv∗(˜u)andt∗(˜u)be the solutions to the original and transformed optimisation
problems respectively. Since the map v=etis one-to-one, it is clear that since v∗(˜u)exists uniquely,
so does t∗(˜u), and that they are related by v∗(˜u) =et∗(˜u). We therefore have the identity
fℓ(v∗(˜u))≡F(t∗(˜u)).
Recalling that f∗
ℓ(˜u) :=fℓ(v∗(˜u)), we see that
∇˜uf∗
ℓ(˜u)≡ ∇ ˜uF(t∗(˜u)). (18)
the derivatives of fℓ(kl−1
ℓ(u|c))with respect to uandcare given by ∇˜uF(t∗(˜u)).
Using the method of Lagrange multipliers, there exist real numbers λ∗=λ∗(˜u)andµ∗=µ∗(˜u)
such that (t∗, λ∗, µ∗)is a stationary point (with respect to t, λandµ) of the Lagrangian function
L(t, λ, µ;˜u) :=F(t) +λg(t;˜u) +µh(t).
LetFt(·)andht(·)denote the gradient vectors of Fandhrespectively, and let gt(·;˜u)andg˜u(t;·)
denote the gradient vectors of gwith respect to tonly and ˜uonly, respectively. Simple calculation
yields
gt(t;˜u) =∂g
∂t1(t;˜u), . . . ,∂g
∂tM(t;˜u)
=−uand
g˜u(t;˜u) =∂g
∂˜u1(t;˜u), . . . ,∂g
∂˜uM+1(t;˜u)
=
1−t1+ log u1, . . . , 1−tM+ log uM,−1
.
(19)
Then, taking the partial derivatives of Lwith respect to λ, µ and the tj, we have that (t, λ, µ) =
(t∗(˜u), λ∗(˜u), µ∗(˜u))solves the simultaneous equations
Ft(t) +λgt(t;˜u) +µht(t) =0, (20)
g(t;˜u) = 0 ,and
h(t) = 0 ,
where the last two equations recover the constraints. Substituting the gradients Ft, gtandht, the first
equation reduces to
ℓ⊙et−λu+µet=0,
24which implies that for all j∈[M]
etj=λuj
µ+ℓj. (21)
Substituting this into the constraints g=h= 0yields the following simultaneous equations in λand
µ
c=kl(u∥et) =MX
j=1ujloguj
etj=MX
j=1ujlogµ+ℓj
λand λMX
j=1uj
µ+ℓj= 1.
Substituting the second into the first and rearranging the second, this is equivalent to solving
c=MX
j=1ujlog 
(µ+ℓj)MX
k=1uk
µ+ℓk!
and λ=
MX
j=1uj
µ+ℓj
−1
. (22)
It has already been established in the discussion after Definition 1 that fℓ(v)attains its maximum
on the set Au:={v∈ △ M:kl(u∥v)≤c}. Therefore F(t)also attains its maximum on RMand
one of the solutions to these simultaneous equations corresponds to this maximum. We first show
that there is a single solution to the first equation in the set (−∞,−max jℓj), referred to as µ∗(˜u)in
the proposition. Second, we show that any other solution corresponds to a smaller total risk, so that
µ∗(˜u)corresponds to the maximum total risk and yields v∗(˜u) =kl−1
ℓ(u|c)when µ∗(˜u)and the
associated λ∗(˜u)are substituted into Equation 21.
For the first step, note that since the etjare probabilities, we see from Equation 21 that either
µ+ℓj>0for all j(in the case that λ > 0), orµ+ℓj<0for all j(in the case that λ < 0).
Thus any solutions µto the first equation must be in (−∞,−max jℓj)or(−minjℓj,∞). If
µ∈(−∞,−max jℓj)then the first equation can be written as c=ϕℓ(µ), with ϕℓas defined in the
statement of the proposition. We now show that ϕℓis strictly increasing in µ, and that ϕℓ(µ)→0as
µ→ −∞ andϕℓ(µ)→ ∞ asµ→ − max jℓj, so that c=ϕℓ(µ)does indeed have a single solution
in the set (−∞,−max jℓj). Straightforward differentiation and algebra shows that
ϕ′
ℓ(µ) =MX
j=1uj
(µ+ℓj)PM
k=1uk
µ+ℓk MX
k′=1uk′
µ+ℓk′−(µ+ℓj)MX
k′=1uk′
(µ+ℓk′)2!
=PM
j=1uj
µ+ℓj2
−PM
j=1uj
(µ+ℓj)2
PM
k=1uk
µ+ℓk.
Jensen’s inequality demonstrates that the numerator is strictly negative, where strictness is due to
the assumption that the ℓjare not all equal. Further, since the denominator is strictly negative (since
we are dealing with the case where µ∈(−∞,−max jℓj)), we see that ϕℓis strictly increasing for
µ∈(−∞,−max jℓj).5Turning to the limits, we first show that ϕℓ(µ)→ ∞ asµ→ − max jℓj.
We now determine the left hand limit. Define J={j∈[M] :ℓj= max kℓk}, noting that
this is a strict subset of [M]since by assumption the ℓjare not all equal. We then have that for
5Incidentally, this argument also shows that there is at most one solution to the first equation in (22) in the
range (−minjℓj,∞). There indeed exists a unique solution, which corresponds to the minimum total risk, but
we do not prove this.
25µ∈(−∞,max jℓj)
eϕℓ(µ)=
−MX
j=1uj
µ+ℓj
 MY
k=1 
−(µ+ℓk)uk!
=
−X
j∈Juj
µ+ℓj−X
j′̸∈Juj′
µ+ℓj′
Y
k∈J 
−(µ+ℓk)ukY
k′̸∈J 
−(µ+ℓk′)uk′
≥
−X
j∈Juj
µ+ℓj
Y
k∈J 
−(µ+ℓk)ukY
k′̸∈J 
−(µ+ℓk′)uk′
=P
j∈JujQ
k′̸∈J 
−(µ+ℓk′)uk′
 
−(µ+ max jℓj)1−P
k∈Juk.
The first term in the numerator is a positive constant, independent of µ. The second term in the
numerator tends to a finite positive limit as µ↑ −max jℓj. Since [M]\Jis non-empty, the power
in the denominator is positive and the term in the outer brackets is positive and tends to zero as
µ↑ −max jℓj. Thus eϕℓ(µ)→ ∞ asµ↑ −max jℓjand, by the continuity of the logarithm, ϕℓ(µ)
asµ↑ −max jℓj.
We now determine limµ→−∞ ϕℓ(µ)by sandwiching ϕ(µ)between two functions that both tend to
zero as µ→ −∞ . First, since ℓj≥0for all j, forµ∈(−∞,−max jℓj)we have
log
−MX
j=1uj
µ+ℓj
≥log
−MX
j=1uj
µ
=−log(−µ) =−MX
j=1ujlog(−µ),
and so
ϕℓ(µ)≥ −MX
j=1ujlog(−µ)+MX
j=1ujlog 
−(µ+ℓj)
=MX
j=1ujlog
1 +ℓj
µ
→0asµ→ −∞ .
Similarly,
MX
j=1ujlog 
−(µ+ℓj)
≤MX
j=1ujlog(−µ) = log( −µ),
and so
ϕℓ(µ)≤log
µMX
j=1uj
µ+ℓj
= log
MX
j=1uj
1 +ℓj
µ
→0asµ→ −∞ .
This completes the first step, namely showing that there does indeed exist a unique solution µ∗(˜u)in
the set (−ℓ1,∞)to the first equation in line (22).
We now turn to the second step, namely showing that this solution corresponds to the maximum total
risk. Given a value of the Lagrange multiplier µ, substitution into Equation 21 gives
etj(µ) =uj
µ+ℓjPM
k=1uk
µ+ℓk
and therefore total risk
R(µ) =PM
j=1ujℓj
µ+ℓjPM
k=1uk
µ+ℓk.
To prove that the solution µ∗(˜u)∈(−∞,−max jℓj)is the solution to the first equation in line (22)
that maximises R, it suffices to show that R(µ)→PM
j=1ujℓjas|µ| → ∞ andR′(µ)≥0for all
µ∈(−∞,−max jℓj)∪(−minjℓj,∞), so that
inf
µ∈(−∞,−maxjℓj)R(µ)≥ sup
µ∈(−minjℓj,∞)R(µ).
26This suffices as we have already proved that µ∗(˜u)is the only solution in (−∞,−max jℓj)to the
first equation in line (22), and that no solutions exists in the set [−max jℓj,−minjℓj].
The limit can be easily evaluated by first rewriting R(µ)and then taking the limit as |µ| → ∞ as
follows
R(µ) =PM
j=1ujℓj
1+ℓj
µPM
k=1uk
1+ℓk
µ→PM
j=1ujℓjPM
k=1uk=MX
j=1ujℓj.
To show that R′(µ)≥0, letℓ(j)denote the j’th smallest component of ℓ(breaking ties arbitrarily),
so that ℓ(1)≤ ··· ≤ ℓ(M), and use the quotient rule to see that
R′(µ)≥0⇐⇒PM
k=1uk
µ+ℓkPM
j=1−ujℓj
(µ+ℓj)2
−PM
j=1ujℓj
µ+ℓjPM
k=1−uk
(µ+ℓk)2
PM
p=1up
µ+ℓp2≥0
⇐⇒MX
j=1MX
k=1ujukℓj
(µ+ℓj)(µ+ℓk)1
µ+ℓk−1
µ+ℓj
≥0
⇐⇒X
j,k∈[M]
k<jujukℓ(j)
(µ+ℓ(j))(µ+ℓ(k))1
µ+ℓ(k)−1
µ+ℓ(j)
+X
j,k∈[M]
k>jujukℓ(j)
(µ+ℓ(j))(µ+ℓ(k))1
µ+ℓ(k)−1
µ+ℓ(j)
≥0,
where in the final line we have dropped the summands where k=jsince they equal zero as the terms
in the bracket cancel. This final inequality holds since the first sum can be bounded below by the
negative of the second sum as follows
X
j,k∈[M]
k<jujukℓ(j)
(µ+ℓ(j))(µ+ℓ(k))1
µ+ℓ(k)−1
µ+ℓ(j)
≥X
j,k∈[M]
k<jujukℓ(k)
(µ+ℓ(j))(µ+ℓ(k))1
µ+ℓ(k)−1
µ+ℓ(j)
(since ℓ(k)≤ℓ(j)fork < j )
=X
j,k∈[M]
k>jukujℓ(j)
(µ+ℓ(k))(µ+ℓ(j))1
µ+ℓ(j)−1
µ+ℓ(k)
(swapping dummy variables j, k).
We now turn to finding the partial derivatives of F(t∗(˜u))with respect the ˜uj, which in turn will
allow us to find the partial derivatives of kl−1
ℓ(u|c). Let∇˜udenote the gradient operator with respect
to˜u. Then the quantity we are after is ∇˜uF(t∗(˜u))∈RM+1, thej’th component of which is
 
∇˜uF(t∗(˜u))
j=M+1X
k=1∂F
∂tk(t∗(˜u))∂t∗
k
∂˜uj(˜u) =Ft(t∗(˜u))·∂t∗
∂˜uj(˜u)∈R.
Thus the full gradient vector is
∇˜uF(t∗(˜u)) =Ft(t∗(˜u))∇˜ut∗(˜u), (23)
where ∇˜ut∗(˜u)is the M×(M+ 1) matrix given by
 
∇˜ut∗(˜u)
j,k=∂t∗
k
∂˜uj(˜u).
Finding an expression for this matrix is difficult. Fortunately we can avoid needing to by using a trick
from mathematical economics referred to as the envelope theorem, as we now show.
27First, note that since, for all ˜u, the constraints g=h= 0are satisfied by t∗(˜u), we have the identities
g(t∗(˜u),˜u)≡0and h(t∗(˜u))≡0.
Differentiating these identities with respect to ˜ujthen yields
gt(t∗(˜u),˜u)·∂t∗
∂˜uj(˜u) +g˜uj(t∗(˜u),˜u)≡0and ht(t∗(˜u))·∂t∗
∂˜uj(˜u)≡0.
As before, we can write these M+ 1pairs of equations as the following pair of matrix equations
gt(t∗(˜u),˜u)∇˜ut∗(˜u) +g˜u(t∗(˜u),˜u)≡0and ht(t∗(˜u))∇˜ut∗(˜u)≡0.
Multiplying these identities by λ∗(˜u)andµ∗(˜u)respectively, and combining with equation (23),
yields
∇˜uF(t∗(˜u)) =
Ft(t∗(˜u)) +λ∗(˜u)gt(t∗(˜u),˜u) +µ∗(˜u)ht(t∗(˜u))
∇˜ut∗(˜u)
+λ∗(˜u)g˜u(t∗(˜u),˜u)
=λ∗(˜u)g˜u(t∗(˜u),˜u),
where the final equality comes from noting that the terms in the large bracket vanish due to equation
(20). Recalling the expression for g˜u(t;˜u)given by Equation 19 and that v∗(˜u) = exp( t∗(˜u))we
obtain
∇˜uF(t∗(˜u)) =λ∗(˜u)
1−t∗(˜u)1+ log u1, . . . , 1−t∗(˜u)M+ log uM,−1
=λ∗(˜u)
1 + logu1
v∗(˜u)1, . . . , 1 + loguM
v∗(˜u)M,−1
Finally, recalling Equivalence (18), namely ∇˜uf∗
ℓ(˜u)≡ ∇ ˜uF(t∗(˜u)), we see that the above
expression gives the derivatives∂f∗
ℓ
∂uj(˜u)and∂f∗
ℓ
∂c(˜u)stated in the proposition, thus completing the
proof.
28NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our Theorem 1 is a PAC-Bayes bound of the form we claim to prove in the
abstract. Further, Proposition 2 is a recipe for the differentiable training objective we claim
to derive in the abstract.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss these in Section 7.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The proof of our main result Theorem 1 is found in Section 5. Proposition 1 is
proved in Appendix C.4. Our second main result, Theorem 2 is proved in Appendix C.5.
Proposition 2 is proved directly after its statement. Proposition 3 is proved in Appendix C.1.
Proposition 4 is proved in Section 5. Lemma 1 does not require proof as it is the classic
change of measure inequality (Csiszár, 1975, Donsker and Varadhan, 1975). Lemma 2 is
proved in Appendix C.2. Finally, Corollary 1 and Lemma 3 are proved directly after their
statement.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Appendix A is devoted to a detailed explanation of how to implement our
training regime, and Appendix B gives the details of the specific experiments we run.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: URL is provided.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: This is outlined in Appendix B.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The paper concerns bounds, meaning the results themselves are confidence
regions.
298.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: The compute resources required are not stated as they are negligible.
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: There are no data-related concerns or societal impact concerns.
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: The introduction describes a class of concrete real-world problems for which
our method may have positive impact.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper uses only the MNIST dataset.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: None required.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: None required.
30