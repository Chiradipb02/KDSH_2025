Provably Efficient Reinforcement Learning with
Multinomial Logit Function Approximation
Long-Fei Li1, 2,Yu-Jie Zhang3,Peng Zhao1, 2,Zhi-Hua Zhou1, 2
1National Key Laboratory for Novel Software Technology, Nanjing University, China
2School of Artificial Intelligence, Nanjing University, China
3The University of Tokyo, Chiba, Japan
{lilf, zhaop, zhouzh}@lamda.nju.edu.cn ,yujie.zhang@ms.k.u-tokyo.ac.jp
Abstract
We study a new class of MDPs that employs multinomial logit (MNL) function
approximation to ensure valid probability distributions over the state space. Despite
its benefits, introducing the non-linear function raises significant challenges in both
computational andstatistical efficiency. The best-known result of Hwang and Oh
[2023] has achieved an eO(κ−1dH2√
K)regret, where κis a problem-dependent
quantity, dis the feature dimension, His the episode length, and Kis the number
of episodes. While this result attains the same rate in Kas linear cases, the method
requires storing all historical data and suffers from an O(K)computation cost per
episode. Moreover, the quantity κcan be exponentially small in the worst case,
leading to a significant gap for the regret compared to linear function approximation.
In this work, we first address the computational and storage issue by proposing an
algorithm that achieves the same regret with only O(1)cost. Then, we design an
enhanced algorithm that leverages local information to enhance statistical efficiency.
It not only maintains an O(1)computation and storage cost per episode but also
achieves an improved regret of eO(dH2√
K+d2H2κ−1), nearly closing the gap
with linear function approximation. Finally, we establish the first lower bound for
MNL function approximation, justifying the optimality of our results in dandK.
1 Introduction
Reinforcement Learning (RL) with function approximation has achieved remarkable success in
various applications involving large state and action spaces, such as games [Silver et al., 2016],
algorithm discovery [Fawzi et al., 2022] and large language models [Ouyang et al., 2022]. Therefore,
establishing the theoretical foundation for RL with function approximation is of great importance.
Recently, there have been many efforts devoted to understanding the linear function approximation,
yielding numerous valuable results [Yang and Wang, 2019, Jin et al., 2020, Ayoub et al., 2020].
While these studies make important steps toward understanding RL with function approximation,
there are still some challenges to be solved. In linear function approximation, transitions are assumed
to be linear in specified feature mappings, such as P(s′|s, a) =ϕ(s′|s, a)⊤θ∗for linear mixture
MDPs and P(s′|s, a) =ϕ(s, a)⊤µ∗(s′)for linear MDPs. Here P(s′|s, a)is the probability from state
sto state s′taking action a,ϕ(s′|s, a)andϕ(s, a)are feature mappings, θ∗andµ∗(s′)are unknown
parameters. However, a transition function is a probability distribution over states, i.e., the magnitude
falls within the range of [0,1], and the aggregation equals 1. For certain feature mappings, linear
transitions may not yield a valid probability distribution with arbitrary θandµas show by [Hwang
and Oh, 2023]. While there are also some works exploring generalized linear [Wang et al., 2021] and
general function approximation [Russo and Roy, 2013, Foster et al., 2021, Chen et al., 2023], they
made assumptions over value functions rather than transitions, hence do not tackle this challenge.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Table 1: Comparison between previous works and ours in terms of the regret, computation cost
and storage cost. Here κandκ∗are exponentially small problem-dependent quantities defined in
Assumption 1, dis the feature dimension, His the episode length and Kis the number of episodes.
The computational cost and storage cost per episode indicate the dependence on episode count k.
Reference Regret Com. Sto. MDP model
Hwang and Oh [2023] eO(κ−1dH2√
K) O(k)O(k) homogeneous
UCRL-MNL-OL (Theorem 1) eO(κ−1dH2√
K) O(1)O(1) inhomogeneous
UCRL-MNL-LL (Theorem 2) eO(dH2√
K+d2H2κ−1)O(1)O(1) inhomogeneous
Lower Bound (Corollary 1) Ω(dH√
Kκ∗) / / infinite action space
Towards addressing the limitation of linear function approximation, a new class of MDPs that utilizes
multinomial logit function approximation has been proposed by Hwang and Oh [2023] recently.
Despite its benefits, the introduction of non-linear functions raises significant challenges in both
computational andstatistical efficiency. Specifically, the best-known approach of Hwang and Oh
[2023] has achieved an eO(κ−1dH2√
K)regret. While matching the optimal regret of eO(d√
H3K)
as linear function approximation in terms of K[Zhou et al., 2021, He et al., 2023], their method
requires storing all historical data and the computational cost per episode grows linearly with the
episode count, which is expensive. Moreover, the quantity κcan be exponentially small, leading to a
significant gap for the regret compared to the linear cases. Thus, a natural question arises:
Is MNL function approximation more difficult than linear function approximation for RL?
In this paper, we answer this question by making significant advancements in both computational
andstatistical efficiency for MDPs with MNL function approximation, nearly closing the gap with
linear function approximation. Table 1 presents a comparison between previous studies and our work.
Specifically, our contributions are summarized as follows:
•We first propose the UCRL-MNL-OL algorithm based on a variant of Online Newton Step, attaining
aneO(κ−1dH2√
K)regret with O(1)computational and storage costs per episode. This result
matches the best-known regret by Hwang and Oh [2023], yet achieves the same computational and
storage efficiency as linear function approximation [Jin et al., 2020, Zhou et al., 2021].
•As the quantity κcan be exponentially small, we propose an enhanced algorithm UCRL-MNL-LL,
which leverages the local information to improve the statistical efficiency. It not only maintain
O(1)computational and storage costs but achieve an improved regret of eO(dH2√
K+d2H2κ−1).
The higher-order term matches the optimal regret eO(d√
H3K)for linear mixture MDPs [Zhou
et al., 2021] and linear MDPs [He et al., 2023] in dandK, differing only by an O(H1/2)factor.
•We establish the firstlower bound for MDPs with MNL function approximation by introducing a
reduction to the logistic bandit problem. We prove a lower bound of Ω(dH√
Kκ∗)for MDPs with
infinite action space. Though this does not constitute a strict lower bound for the finite actions case
studied in this work, it suggests that our result may be optimal with respect to dandK.1
From a technical perspective, inspired by recent advances in logistic bandits [Zhang et al., 2016,
Faury et al., 2020, Zhang and Sugiyama, 2023], we observe that the negative log-likelihood function
is exponentially concave, which motivates us to apply online Newton step to estimate the unknown
transition parameter in an online manner. Moreover, we employ the Bernstein-like inequalities and
the self-concordant-like property [Bach, 2010] of the log-loss to achieve the better dependence on κ.
Organization. We introduce the related work in Section 2 and present the setup in Section 3. Then,
we design an algorithm with constant computational and storage costs in Section 4. Next, we present
an algorithm with improved regret guarantee in Section 5. Finally, we establish the lower bound in
Section 6. Section 7 concludes the paper. Due to space limits, we defer all proofs to the appendixes.
1After the submission of our work to arXiv [Li et al., 2024a], a follow up work by Park et al. [2024] improved
the lower bound to Ω(dH3/2√
K). This confirms that our result is indeed optimal with respect to dandK.
2Notations. Denote by [n]the set {1, . . . , n }and use [x][a,b]to denote min(max( x, a), b). For
X, Y∈Rd×d,X⪰Ymeans X−Yis positive semi-definite. For a vector x∈Rdand positive
semi-definite matrix A∈Rd×d, denote ∥x∥A=√
x⊤Ax. TheeO(·)hides polylogarithmic factors.
2 Related Work
In this section, we review related works from both setup and technical perspectives.
RL with Generalized Linear Function Approximation. There are recent efforts devoted to
investigating function approximation beyond the linear models. Wang et al. [2021] investigated
RL with generalized linear function approximation. Notably, unlike our approach which models
transitions using a generalized linear model, they apply this approximation directly to the value
function. Another line of works [Chowdhury et al., 2021, Li et al., 2022, Ouhamma et al., 2023]
has studied RL with exponential function approximation and also aimed to ensure that transitions
constitute valid probability distributions. The MDP model can be viewed as an extension of bilinear
MDPs in their work while our setting extends linear mixture MDPs. These studies are complementary
to ours and not directly comparable. Moreover, these works also enter the computational and statistical
challenges arising from non-linear function approximation that remain to be addressed. The most
relevant work to ours is the recent work by Hwang and Oh [2023], which firstly explored a similar
setting to ours, where the transition is characterized using a multinomial logit model. We significantly
improve upon their results by providing computationally and statistically more efficient algorithms.
RL with General Function Approximation. There have also been some works studies RL with
general function approximation. Russo and Roy [2013] and Osband and Roy [2014] initiated the
study on the minimal structural assumptions that render sample-efficient learning by proposing a
structural condition called Eluder dimension. Recently, several works have investigated different
conditions for sample-efficient interactive learning, such as Bellman Eluder (BE) dimension [Jin
et al., 2021], Bilinear classes [Du et al., 2021], Decision-Estimation Coefficient (DEC) [Foster et al.,
2021], and Admissible Bellman Characterization (ABC) [Chen et al., 2023]. A notable difference is
that they impose assumptions on the value functions while we study function approximation on the
transitions to ensure valid probability distributions. Moreover, the goal of these works is to study the
conditions for sample-efficient reinforcement learning, but not focus on the computational efficiency.
Multinomial Logit Bandits. There are two types of multinomial logit bandits studied in the literature:
the single-parameter model, where the parameter is a vector [Cheung and Simchi-Levi, 2017] and
multiple-parameter model, where the parameter is a matrix [Amani and Thrampoulidis, 2021]. We
focus on the single-parameter model, which are more relevant to our setting. The pioneering work by
Cheung and Simchi-Levi [2017] achieved a Bayesian regret of eO(κ−1d√
K), where Kdenotes the
number of rounds in bandits. This result was further enhanced by subsequent studies [Oh and Iyengar,
2019, 2021, Agrawal et al., 2023]. In particular, Périvier and Goyal [2022] significantly improved the
dependence on κ, obtaining a regret of eO(d√
κK+κ−1)in the uniform revenue setting. Most prior
methods required storing all historical data and faced computational challenge. To address this issue,
the most recent work by Lee and Oh [2024] proposed an algorithm with constant computational and
storage costs building on recent advances in multiple-parameter model [Zhang and Sugiyama, 2023].
Their algorithm achieves the optimal regret of eO(d√
κK+κ−1)andeO(d√
K+κ−1)under uniform
and non-uniform rewards respectively. However, although the underlying models of MNL bandits and
MDPs share similarities, the challenges they present differ substantially, and techniques developed for
MNL bandits cannot be directly applied to MNL MDPs. For example, in MNL bandits, the objective
is to select a series of assortments with varying sizes that maximize the expected revenue, whereas in
MNL MDPs, the goal is to choose oneaction at each stage to maximize the cumulative reward. Thus,
it is necessary to design new algorithms tailored for MDPs to address these unique challenges.
3 Problem Setup
In this section, we present the problem setup of RL with multinomial logit function approximation.
Inhomogeneous, Episodic MDPs. An inhomogeneous, episodic MDP instance can be denoted by a
tuple M= (S,A, H,{Ph}H
h=1,{rh}H
h=1), where Sis the state space, Ais the action space, His the
length of each episode, Ph:S ×A×S → [0,1]is the transition kernel with Ph(s′|s, a)is being the
3probability of transferring to state s′from state sand taking action aat stage h,rh:S × A → [0,1]
is the deterministic reward function. A policy π={πh}H
h=1is a collection of mapping πh, where
eachπh:S → ∆(A)is a function maps a state sto distributions over Aat stage h. For any policy π
and(s, a)∈ S × A , we define the action-value function Qπ
hand value function Vπ
has follows:
Qπ
h(s, a) =E"HX
h′=hrh′(sh′, ah′)sh=s, ah=a#
, Vπ
h(s) =Ea∼πh(·|s)[Qπ
h(s, a)],
where the expectation of Qπ
his taken over the randomness of the transition Pand policy π. The optimal
value function V∗
hand action-value function Q∗
hgiven by V∗
h(s) = supπVπ
h(s)andQ∗
h(s, a) =
supπQπ
h(s, a). For any function V:S →R, we define [PhV](s, a) =Es′∼Ph(·|s,a)V(s′). The
Bellman equation for policy πand Bellman optimality equation are given by
Qπ
h(s, a) =rh(s, a) +
PhVπ
k,h+1
(s, a), Q∗
h(s, a) =rh(s, a) +
PhV∗
k,h+1
(s, a).
Learning Protocol. In the online MDP setting, the learner interacts with the environment without the
knowledge of the transition {Ph}H
h=1. As learning rewards is no harder than transitions, we assume
the reward {rh}H
h=1is known. The interaction proceeds in Kepisodes. At the beginning of episode
k, the learner chooses a policy πk={πk,h}H
h=1. At each stage h∈[H], starting from the initial
statesk,1, the learner observes the state sk,h, chooses an action ak,hsampled from πk,h(· |sk,h),
obtains reward rh(sk,h, ak,h)and transits to the next state sk,h+1∼Ph(· |sk,h, ak,h)forh∈[H].
The episode ends when sH+1is reached. The goal of the learner is to minimize regret, defined as
Reg(K) =KX
k=1V∗
1(sk,1)−KX
k=1Vπk
1(sk,1),
which is the difference between the cumulative reward of the optimal policy and the learner’s policy.
Multinomial Logit (MNL) Mixture MDPs. Although significant advances have been achieved
for MDPs with linear function approximation, Hwang and Oh [2023] show that there exists a set
of features such that no linear transition model (including bilinear and low-rank MDPs) can induce
a valid probability distribution over the state space, which limits the expressiveness of function
approximation. To overcome this limitation, they propose a new class of MDPs with multinomial
logit function approximation. We introduce the definition of MNL mixture MDPs below.
Definition 1 (Reachable States) .For any (h, s, a )∈[H]× S × A , we define the “reachable states”
as the set of states that can be reached from state staking action aat stage hwithin a single transition,
i.e.,Sh,s,a≜{s′∈ S |Ph(s′|s, a)>0}. Furthermore, we define Sh,s,a≜|Sh,s,a|and denote by
U≜max (h,s,a )Sh,s,a the maximum number of reachable states.
Remark 1. There are many cases that even when the state space is very large, the maximum number
of reachable states Uis small. This phenomenon is common in situations where the next state is close
to the current state. An illustrative example is the RiverSwim problem [Strehl and Littman, 2004].
Definition 2 (MNL Mixture MDP) .An MDP instance M= (S,A, H,{Ph}H
h=1,{rh}H
h=1)is called
an inhomogeneous, episodic B-bounded MNL mixture MDP if there exist a known feature mapping
ϕ(s′|s, a) :S × A × S → Rdwith∥ϕ(s′|s, a)∥2≤1andunknown vectors {θ∗
h}H
h=1∈Θwith
Θ ={θ∈Rd,∥θ∥2≤B}, such that for all (s, a, h )∈ S × A × [H]ands′∈ Sh,s,a, it holds that
Ph(s′|s, a) =exp(ϕ(s′|s, a)⊤θ∗
h)P
es∈Sh,s,aexp(ϕ(es|s, a)⊤θ∗
h).
Remark 2. While the work of Hwang and Oh [2023] focuses on the homogeneous setting, where
the transitions remain the same across all stages (i.e., P1=...=PH), we address the more general
inhomogeneous setting, allowing transitions to vary across different stages.
For any θ∈Rd, we define the induced transition as ps′
s,a(θ) =exp(ϕ(s′|s,a)⊤θ)P
es∈Ss,aexp(ϕ(es|s,a)⊤θ). We introduce
the key problem-dependent quantities for this problem firstly introduced by Hwang and Oh [2023].
Assumption 1. There exists 0< κ≤κ∗<1such that for all (s, a, h )∈ S × A × [H]and
s′, s′′∈ Sh,s,a, it holds that infθ∈Θps′
s,a(θ)ps′′
s,a(θ)≥κandps′
s,a(θ∗)ps′′
s,a(θ∗)> κ∗.
Assumption 1 is similar to the standard assumption in generalized linear model literature [Filippi
et al., 2010, Wang et al., 2021] and logistic bandits literature [Faury et al., 2020, Abeille et al., 2021,
Zhang and Sugiyama, 2023] to guarantee the Hessian matrix is non-singular in Property 1.
44 Computationally Efficient Algorithm by Online Learning
In this part, we design an algorithm that achieves O(1)computational and storage costs per episode.
Since the transition parameter θ∗
his unknown, we need to estimate it using the historical data. At
episode k, we collect a trajectory {(sk,h, ak,h)}H
h=1, then define the variable: yk,h∈ {0,1}Nk,h
where ys′
k,h=1(sk,h+1=s′)fors′∈ S k,h≜Ssk,h,ak,handNk,h=|Sk,h|. We denote by
ps′
k,h(θ) =ps′
sk,h,ak,h(θ). Then yk,his a sample from the following multinomial distribution:
yk,h∼multinomial (1,[ps1
k,h(θ∗), . . . , psNk,h
k,h(θ∗)]),
where the parameter 1indicates that yk,his a single-trial sample. Furthermore, we define the noise
ϵs′
k,h=ys′
k,h−ps′
k,h(θ∗
h). It is clear that ϵk,h∈[−1,1]Nk,h,E[ϵk,h] =0andP
s′∈Sk,hϵs′
i,h= 0.
Efficiency Concern. Hwang and Oh [2023] made the first step by proposing an approach using the
maximum likelihood estimation (MLE). Specifically, their estimator θk,his defined as
θk,h= arg min
θ∈Rdk−1X
i=1X
s′∈Si,h−ys′
i,hlogps′
i,h(θ) +λ
2∥θ∥2
2. (1)
Despite favorable theoretical guarantee, the computational and storage cost of this method is expensive.
First, in episode k, the estimator θk,hin(1)is computed using all samples collected in previous
episodes, resulting an O(k)storage cost. Second, to solve this optimization problem, the projected
gradient descent [Boyd and Vandenberghe, 2004] are usually applied. However, as discussed in Faury
et al. [2022], the optimization of the MLE problem typically require O(klog(1 /ϵ))iterations to
achieve an ϵ-accurate solution, which is computationally expensive.
To address this issue, we introduce a novel algorithm named UCRL-MNL with Online Learning
(UCRL-MNL-OL), which achieves both the same regret with O(1)computational and storage cost.
At a high level, our algorithm can be divided into two phases: (i) efficient online estimation, and (ii)
efficient optimistic value function construction. We introduce the details in the following.
Efficient Online Estimation. Instead of using all historical data, our algorithm updates the estimator
in an online manner. Specifically, inspired by the works [Hazan et al., 2014, Zhang et al., 2016] on
logistic bandit, we find the negative log-likelihood function is exponentially concave, which motivates
us to apply a variant of online Newton step [Zhang et al., 2016, Oh and Iyengar, 2021].
First, we define per-episodic loss function fk,h(θ), gradient gk,h(θ)and Hessian matrix Hk,h(θ)as
fk,h(θ) =−X
s′∈Sk,hys′
k,hlogps′
k,h(θ), g k,h(θ) =X
s′∈Sk,h(ps′
k,h(θ)−ys′
k,h)ϕs′
k,h,
Hk,h(θ) =X
s′∈Sk,hps′
k,h(θ)ϕs′
k,h(ϕs′
k,h)⊤−X
s′∈Sk,hX
s′′∈Sk,hps′
k,h(θ)ps′′
k,h(θ)ϕs′
k,h(ϕs′′
k,h)⊤. (2)
We introduce the following key property of the loss function fk,h(θ)by a second-order Taylor
expansion and mean value theorem, whose proof can be found in Lemma 5 in Appendix A.
Property 1. (i). For any θ1, θ2∈Rd,∃¯θ=νθ1+ (1−ν)θ2such that
fk,h(θ2) =fk,h(θ1) +⟨gk,h(θ1), θ2−θ1⟩+1
2∥θ2−θ1∥2
Hk,h(¯θ).
(ii). For any θ∈Θ, it holds that
Hk,h(θ)⪰κX
s′∈Sk,hϕs′
k,h(ϕs′
k,h)⊤≜κWk,h. (3)
Property 1 implies that fk,h(θ)is exponentially concave, which enables us to apply the online Newton
step to construct an efficient estimator. Specifically, we construct bθk+1,hby solving the problem:
bθk+1,h= arg min
θ∈Θ⟨gk,h(bθk,h), θ−bθk,h⟩+1
2∥θ−bθk,h∥2
bΣk+1,h, (4)
wherebΣk+1,h=bΣk,h+κ
2P
s′∈Sk,hϕs′
k,h(ϕs′
k,h)⊤is the feature covariance matrix.
5Algorithm 1 UCRL-MNL-OL
Input: Regularization parameter λ, confidence width bβk, confidence parameter δ.
1:Initialization: bΣ1,h=λI,bθ1,h=0for all h∈[H].
2:fork= 1, . . . , K do
3: Compute bQk,h(·,·)in a backward way as in (7).
4: forh= 1, . . . , H do
5: Observe current state sk,h, select action ak,h= arg maxa∈AbQk,h(sk,h, a).
6: SetbΣk+1,h=bΣk,h+κ
2P
s′∈Sk,hϕs′
k,h(ϕs′
k,h)⊤.
7: Compute bθk+1,h= arg minθ∈Θ⟨gk,h(bθk,h), θ−bθk,h⟩+1
2∥θ−bθk,h∥2
bΣk+1,h.
8: end for
9:end for
We show the online estimator bθk,hin(4)enjoys computational and storage efficiency simultaneously.
As the optimization problem exhibits a standard online mirror descent formulation, it can be solved
with a projected gradient step with the following equivalent formulation by
θ′
k+1,h=bθk,h−bΣ−1
k+1,hgk,h(bθk,h),andbθk+1,h= arg min
θ∈Θ∥θ−θ′
k+1,h∥bΣk+1,h.
This update enjoys a computational cost of only O(d3U), independent of episode count k[Zhang
and Sugiyama, 2023, Lee and Oh, 2024]. As for storage costs, it avoids the need to store all historical
data by updating the feature covariance matrix bΣk,hincrementally, requiring only O(d2)storage
cost. We note that a dependence on Uis introduced in the computational cost. However, as we have
discussed in Remark 1, Ucan be much smaller than the state space size S, which is acceptable. This
dependency is typical for model-based methods that directly learn transitions, as it need to control the
estimation error of transitions, which typically involves a total of Uelements. Similar dependencies
have been observed in the literature [Hwang and Oh, 2023]. Additionally, even for the model-free
method [Yang and Wang, 2020, Zhou et al., 2021] which learn value functions, a common assumption
is the valueP
s′∈Sk,hϕs′
k,hV(s′)can be obtained by an Oracle, which depends on Uimplicitly.
Then, we show the estimator bθk,his close to the true parameter θ∗
hby the following lemma.
Lemma 1. For any δ∈(0,1), define the confidence set as
bCk,h=
θ∈Θ| ∥θ−bθk,h∥bΣk,h≤√
λB+s
8
κdlog
1 +kUH
dλδ
≜bβk
. (5)
Then, we have Pr[θ∗
h∈bCk,h]≥1−δ,∀k∈[K], h∈[H].
Remark 3. Notably, a similar confidence set is achieved by Hwang and Oh [2023] by using the MLE
defined in (1). In contrast, we obtain the same results by using an online estimator which only suffers
O(1)computation and storage cost per round. Besides, we identify a technical issue in their analysis.
Specifically, they bound the confidence set using the self-normalized concentration for vector-valued
martingales in Lemma 15. However, it is crucial to recognize that the noise is not independent and
satisfiesP
s′∈Si,hϵs′
i,h= 0since the learner visits each stage hexactly once per episode. Thus, the
noise ϵs′
i,hbecomes deterministic and non-zero given the remaining noise ϵs′′
i,hfors′′̸=s′. This
contravenes the zero-mean sub-Gaussian condition in Lemma 15. We observe similar oversights also
appear in the works on multinomial logit contextual bandits [Oh and Iyengar, 2019, 2021, Agrawal
et al., 2023]. To our knowledge, this issue has not been explicitly identified in previous studies. We
note that this issue can be resolved by a new self-normalized concentration with dependent noises in
Lemma 1 of Li et al. [2024b] with only slight modifications in constant factors.
Efficient Optimistic Value Function Construction. Given the confidence set bCk,h, the most direct
way is to adopt the principle of “optimism in the face of uncertainty” and construct the optimistic
value function as the maximum expected reward over the confidence set. Specifically, we define the
optimistic value function bQk,h(s, a)andbVk,h(s)as
bQk,h(s, a) =
rh(s, a) + max
θ∈bCk,hX
s′∈Sh,s,aps′
s,a(θ)bVk,h+1(s′)
[0,H],bVk,h(s) = max
a∈AbQk,h(s, a).(6)
6However, this construction is not efficient as it requires solving a maximization problem over the
confidence set. Actually, the algorithm only requires the estimated action-value function to be an
upper bound for the true value functions. Thus, we can use a closed-form confidence bound instead
of computing the maximal value over the confidence set. To this end, we present the following lemma
that enables us to construct the optimistic value function more efficiently.
Lemma 2. Suppose Lemma 1 holds. For any V:S → [0, H]and(h, s, a )∈[H]× S × A , it holdsX
s′∈Sh,s,aps′
s,a(bθk,h)V(s′)−X
s′∈Sh,s,aps′
s,a(θ∗
h)V(s′)≤Hbβkmax
s′∈Sh,s,a∥ϕs′
s,a∥bΣ−1
k,h.
Based on this lemma, we can replace the maximization problem in (6) with closed-form confidence
bound and construct the optimistic value function bQk,h(s, a)as
bQk,h(s, a) =
rh(s, a) +X
s′∈Sh,s,aps′
s,a(bθk,h)bVk,h+1(s′) +Hbβkmax
s′∈Sh,s,a∥ϕs′
s,a∥bΣ−1
k,h
[0,H].(7)
At state sk,h, our algorithm chooses ak,h= arg maxa∈AbQk,h(sk,h, a). Algorithm 1 presents the
detailed procedure. We show that our algorithm achieves the following regret guarantee.
Theorem 1. For any δ∈(0,1), setbβkas in Lemma 1 and λ= 1, with probability at least 1−δ,
Algorithm 1 ensures the following regret guarantee
Reg(K)≤eO 
κ−1dH2√
K
.
Remark 4. Our algorithm attains the same regret of Hwang and Oh [2023], yet for the more general
inhomogeneous MDPs. Importantly, our algorithm only requires constant computational and storage
costs, matching the computational efficiency of the linear cases [Jin et al., 2020, Zhou et al., 2021].
5 Statistically Improved Algorithm by Local Learning
In this section, we present an enhanced algorithm, UCRL-MNL-LL, that leverages local information
to improve the statistical efficiency for MDPs with MNL function approximation.
While the UCRL-MNL-OL algorithm in Section 4 offers favorable computational and storage
efficiency, it suffers from a dependence on κ−1in the regret. By Assumption 1, the quantity κmay
be exponentially small in the worst case, exhibiting an exponential dependence on the radius of the
parameter set and linear in the number of reachable states, i.e., κ−1=O((Uexp(B))2). This creates
a significant gap between MNL and linear function approximation.
Recently, the improved dependence on κhas been achieved in the logistic bandit literature [Faury
et al., 2020, Abeille et al., 2021, Périvier and Goyal, 2022] by the use of generalization of the
Bernstein-like tail inequality [Faury et al., 2020] and the self-concordant-like property of the log
loss [Bach, 2010]. Thus, a natural question then arises: Can we improve the statistical efficiency
of MNL function approximation for MDPs? We answer this question affirmatively by proposing
an enhanced algorithm that reduces the dependence on κsignificantly through the use of local
information. Though the achievement has been made in the logistic bandit, the extension to MDPs
with MNL function approximation is non-trivial and new techniques specific to MDPs are required.
We first analyze where the dependence on κcomes from. By the analysis of Theorem 1, we can see
that the regret of the algorithm can be upper-bounded as follows,
Reg(K)≤2HbβKKX
k=1HX
h=1max
s′∈Sk,h∥ϕs′
k,h∥bΣ−1
k,h+KX
k=1HX
h=1Mk,h. (8)
Here, the first term corresponds to the overestimation of the value function, and the second term
represents the martingale difference sequence arising from the stochastic transition dynamics. The
second term can be bounded using the Azuma-Hoeffding inequality, which is independent of κ. For
the first term, by the construction of the confidence set in Lemma 1, bβkrepresents the width of the
confidence set, contributing to a κ−1/2dependence. The feature covariance matrix bΣk,hdefined in (4)
reflects the degree of exploration across different states, which also results in a κ−1/2dependence.
To mitigate these dependencies, we design a local learning algorithm that: (i) constructs a confidence
set independent of κ; and (ii) builds a κ-independent feature covariance matrix that effectively
captures the exploration degree across different states. We introduce the details in the following.
7Algorithm 2 UCRL-MNL-LL
Input: Step size η, regularization parameter λ, confidence width eβk, confidence parameter δ.
1:Initialization: H1,h=λI,bθ1,h=0for all h∈[H].
2:fork= 1, . . . , K do
3: Compute eQk,h(·,·)in a backward way as in (11).
4: forh= 1, . . . , H do
5: Observe state sk,h, select action ak,h= arg maxa∈AeQk,h(sk,h, a).
6: Update eHk,h=Hk,h+ηHk,h(eθk,h).
7: Compute eθk+1,h= arg minθ∈Θ⟨gk,h(eθk,h), θ−eθk,h⟩+1
2η∥θ−eθk,h∥eHk,h.
8: Update Hk,h=Hk,h+Hk,h(eθk+1,h).
9: end for
10:end for
5.1 Improved Online Estimation
In Property 1, we show the Hessian matrix Hk,his lower bounded by a positive definite matrix
κP
s′∈Sk,hϕs′
k,h(ϕs′
k,h)⊤uniformly over the parameter domain, as in (3). This quantity measures the
exploration degree across different states and is used to update the parameter estimation and construct
the confidence set. However, the bound is not tight, as the Hessian matrix can be significantly larger
in certain regions, away from the global minimum. This observation motivates us to design a local
learning algorithm that can adaptively leverage local information for improved guarantee.
Inspired by recent advances in multinomial logistic bandit (MLogB) [Zhang and Sugiyama, 2023]
and multinomial logit contextual bandit (MNL) [Lee and Oh, 2024], we run an online mirror descent
algorithm to estimate the parameter θ∗
h. Differently from (4), we use the local Hessian matrix eHk,h
to update the estimation instead of the global lower bound. Specifically, we estimate eθk,has follows:
eθk+1,h= arg min
θ∈Θ⟨gk,h(eθk,h), θ−eθk,h⟩+1
2η∥θ−eθk,h∥eHk,h. (9)
where ηis the step size, Hk,h=Pk−1
i=1Hi,h(eθi+1,h) +λIdandeHk,h=Hk,h+ηHk,h(eθk,h). Note
that both Hk,handeHk,hcan be updated incrementally. Similar to the update in (4), the optimization
problem in (9) can be efficiently solved using the following two-step update:
θ′
k+1,h=θk,h−ηeH−1
k,hgk,h(θk,h),andeθk+1,h= arg min
θ∈Θ∥θ−θ′
k+1,h∥eHk,h.
This two-step update procedure incurs a computational cost of O(d3U)and a storage cost of O(d2)
per episode, both independent of the episode count k.
Based on this estimator, we can construct the κ-independent confidence set as follows.
Lemma 3. For any δ∈(0,1), define the confidence set as
eCk,h=
θ∈Θ| ∥θ−eθk,h∥Hk,h≤eβk	
,
whereeβk=O(√
dlogUlog(KH/δ )). Then, we have Pr[θ∗
h∈eCk,h]≥1−δ,∀k∈[K], h∈[H].
Remark 5. Zhang and Sugiyama [2023] studied the multiple-parameter MLogB model, where the
unknown parameter is a matrix. Consequently, the confidence set in Theorem 3 of their work exhibits
a polynomial dependence on the number of possible outcomes, which corresponds to the number
of reachable states Uin our setting. This dependence is acceptable in the bandit setting while is
not suitable for the MDP setting. In contrast, Lee and Oh [2024] focused on the single-parameter
MNL model and revisited the self-concordant-like property, demonstrating that the log-loss of the
single-parameter MNL model is 3√
2-self-concordant-like (Proposition B.1 in Lee and Oh [2024]).
This property is crucial for the improved confidence set in Lemma 3 that is independent of κandU.
85.2 Improved Optimistic Value Function Construction
Based on the confidence set in Lemma 3, a natural choice for the optimistic value function construction
is analogous to (7) and can be expressed as:
¯Qk,h(s, a) =
rh(s, a) +X
s′∈Sh,s,aps′
s,a(bθk,h)¯Vk,h+1(s′) +Heβkmax
s′∈Sh,s,a∥ϕs′
s,a∥H−1
k,h
[0,H].(10)
However, though eβKnow is independent of κandH−1
k,hpreserves the local information in (10), the
norm max s′∈Sk,h∥ϕs′
k,h∥H−1
k,his still in a global manner due to the maximum operation, leading to
aκ−1/2dependence. To address this issue, we propose a new construction of the optimistic value
function. Specifically, we employ a second-order Taylor expansion to more accurately bound the
value difference arising from transition estimation errors.
Although the idea of using second-order Taylor expansion has been explored in bandits [Périvier and
Goyal, 2022, Zhang and Sugiyama, 2023, Lee and Oh, 2024], fundamental differences arise in the
MDP setting. Specifically, Périvier and Goyal [2022] studied the uniform revenue setting, where the
reward is identical for all actions. Zhang and Sugiyama [2023] and Lee and Oh [2024] focused on
the non-uniform setting, but the rewards for different actions are known a priori. However, in the
MDP setting, the value function is state-dependent andunknown to the learner, leading to a more
challenging problem. Moreover, in MNL bandits, the objective is to select a series of assortments
with varying sizes that maximize the expected revenue, whereas in MNL MDPs, the goal is to choose
oneaction at each stage to maximize the cumulative reward. Due to these differences, the analyses
used in the bandit setting cannot be directly applied to the MDP setting.
For MDPs, we show the value difference arising from the transition estimation error as follows.
Lemma 4. Suppose Lemma 3 holds. For any V:S → [0, H]and(h, s, a )∈[H]× S × A , it holds
X
s′∈Sh,s,aps′
s,a(bθk,h)V(s′)−X
s′∈Sh,s,aps′
s,a(θ∗
h)V(s′)≤ϵfst
s,a+ϵsnd
s,a.
where
ϵfst
s,a=HeβkX
s′∈Sh,s,aps′
s,a(eθk,h)ϕs′
s,a−X
s′′∈Sh,s,aps′′
s,a(eθk,h)ϕs′′
s,a
H−1
k,h, ϵsnd
s,a=5
2Heβ2
kmax
s′∈Sh,s,a∥ϕs′
s,a∥2
H−1
k,h.
Based on Lemma 4, we construct the optimistic value function as follows:
eQk,h(s, a) =
rh(s, a) +X
s′∈Sh,s,aps′
s,a(bθk,h)eVk,h+1(s′) +ϵfst
s,a+ϵsnd
s,a
[0,H]. (11)
In contrast to the value function specified in (10), where the term max s′∈Sh,s,a∥ϕs′
s,a∥H−1
k,his utilized,
the refined value function introduced in (11) substitutes this term with ϵfst
s,a+ϵsnd
s,a. This adjustment
better preserves local information, offering a more precise and κ-independent estimation error bound.
At state sk,h, our algorithm chooses action ak,h= max eQk,h(sk,h,·). The detailed algorithm is
presented in Algorithm 2. We show the regret guarantee of UCRL-MNL-LL in the following theorem.
Theorem 2. For any δ∈(0,1), seteβk=O(√
dlogUlog(KH/δ )),η=1
2log(1 + U) + (B+ 1)
andλ= 84√
2η(B+d), with probability at least 1−δ, UCRL-MNL-LL algorithm ensures the
following regret guarantee
Reg(K)≤eO 
dH2√
K+κ−1d2H2
.
Remark 6. The high-order term in Theorem 2 is now independent κ, significantly improving the
statistical efficiency compared with Theorem 1. In comparison with the optimal regret of eO(d√
H3K)
for linear cases [Zhou et al., 2021, He et al., 2023], the higher-order term in Theorem 2 only differs by
a factor of H1/2, almost matching the same computational and statistical efficiency simultaneously.
96 Lower Bound
In this section, we establish a lower bound for MNL mixture MDPs by presenting a novel reduction,
which connects MNL mixture MDPs and the logistic bandit problem.
Consider the following logistic bandit problem [Faury et al., 2020, Abeille et al., 2021]: at each round
k∈[K], the learner selects an action xk∈ X and receives a reward rksampled from Bernoulli
distribution with mean µ(x⊤θ∗) = (1 + exp( −x⊤θ∗))−1, where θ∗∈ {θ∈Rd,∥θ∥2≤B}is the
unknown parameter. The learner aims to to minimize the regret:
RegLogB(K) = max
x∈XKX
k=1µ(x⊤θ∗)−KX
k=1µ(x⊤
kθ∗).
Theorem 3 (Lower Bound) .For any logistic bandit problem B, there exists an MNL mixture MDP
Msuch that learning Mis as hard as learning H/2independent instances of Bsimultaneously.
Corollary 1. For any problem instance {θ∗
h}H
h=1and for K≥d2κ∗, there exists an MNL mixture
MDP with infinite action space such that Reg(K)≥Ω(dH√
Kκ∗).
Remark 7. Corollary 1 can be proved by combining Theorem 3 and the Ω(d√
Kκ∗)lower bound
for logistic bandits with infinite arms by Abeille et al. [2021]. To the best of our knowledge, a lower
bound for logistic bandits with finite arms has not been established, which is beyond the scope of this
work. This absence leaves the lower bound for MNL mixture MDPs with a finite action space open
through this reduction. However, after the submission of our work to arXiv [Li et al., 2024a], a follow
up work by Park et al. [2024] proposed a new reduction that bridges MNL mixture MDPs with linear
mixture MDPs by approximating multinomial logit functions to linear functions, employing the mean
value theorem. Leveraging this new reduction, they established an Ω(dH3/2√
K)lower bound for
MNL mixture MDPs with the finite action space. This achievement confirms that our result is indeed
optimal with respect to the dependence on dandK, only differing by a O(H1/2)factor.
Dependence on H.By the discussion in Remark 7, we note that our result is optimal with respect to
dandK, but loosing by a O(H1/2)factor. We discuss the challenges in improving the dependence on
H. Notably, MNL mixture MDPs can be viewed as a generalization of linear mixture MDPs [Ayoub
et al., 2020, Zhou et al., 2021]. The pioneering work by Ayoub et al. [2020] achieved a regret bound
ofeO(dH2√
K)for linear mixture MDPs, which matches our results in Theorem 2, differing only
on the lower order term. Later, Zhou et al. [2021] enhanced the dependence on Hand attained an
optimal regret bound of eO(d√
H3K). This was made possible by recognizing that the value function
in linear mixture MDPs is linear, allowing for direct learning of the value function while incorporating
variance information . In contrast, the value function for MNL mixture MDPs does not conform to a
specific structure, posing a significant challenge in using the variance information of value functions.
Thus, it remains open whether similar improvements on Hare attainable for MNL mixture MDPs.
7 Conclusion and Future Work
In this work, we study MNL mixture MDPs that employ multinomial logit function approximation
to ensure valid probability distributions over the state space. We address both the computational
and statistical challenges for this problem. Specifically, we first propose an algorithm based on the
online Newton step that attains the eO(κ−1dH2√
K)regret with O(1)computational and storage
costs per episode. Next, we propose an enhanced algorithm that leverages local information to
improve the statistical efficiency. It not only maintains O(1)computational and storage costs but
also achieves an improved regret of eO(dH2√
K+d2H2κ−1), nearly matching the result of linear
function approximation from both computational and statistical perspectives. Finally, we establish
the first lower bound for MNL mixture MDPs, justifying the optimality of our results in dandK.
There are several interesting directions for future work. First, there still exists a gap between the upper
and lower bounds and how to close this gap is an open problem. Besides, while this work focuses
on stochastic rewards, extending this model to the non-stationary settings and studying the dynamic
regret [Wei and Luo, 2021, Zhao et al., 2022, Li et al., 2023] is an another important direction.
10Acknowledgments
This research was supported by National Science and Technology Major Project (2022ZD0114800)
and NSFC (U23A20382, 62206125). Peng Zhao was supported in part by the Xiaomi Foundation.
References
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems 24 (NIPS) , pages 2312–2320,
2011.
Marc Abeille, Louis Faury, and Clément Calauzènes. Instance-wise minimax-optimal algorithms for
logistic bandits. In Proceedings of the 24th International Conference on Artificial Intelligence and
Statistics (AISTATS) , pages 3691–3699, 2021.
Priyank Agrawal, Theja Tulabandhula, and Vashist Avadhanula. A tractable online learning algorithm
for the multinomial logit contextual bandit. European Journal of Operational Research , 310(2):
737–750, 2023.
Sanae Amani and Christos Thrampoulidis. Ucb-based algorithms for multinomial logistic regression
bandits. In Advances in Neural Information Processing Systems 34 (NeurIPS) , pages 2913–2924,
2021.
Alex Ayoub, Zeyu Jia, Csaba Szepesvári, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In Proceedings of the 37th International Conference on
Machine Learning (ICML) , pages 463–474, 2020.
Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics , 4:
384–414, 2010.
Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.
Zixiang Chen, Chris Junchi Li, Huizhuo Yuan, Quanquan Gu, and Michael I. Jordan. A general
framework for sample-efficient function approximation in reinforcement learning. In Proceedings
of the 11th International Conference on Learning Representations (ICLR) , 2023.
Wang Chi Cheung and David Simchi-Levi. Thompson sampling for online personalized assortment
optimization problems with multinomial logit choice models. Available at SSRN 3075658 , 2017.
Sayak Ray Chowdhury, Aditya Gopalan, and Odalric-Ambrym Maillard. Reinforcement learning in
parametric mdps with exponential families. In Proceedings of the 24th International Conference
on Artificial Intelligence and Statistics (AISTATS) , pages 1855–1863, 2021.
Simon S. Du, Sham M. Kakade, Jason D. Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and
Ruosong Wang. Bilinear classes: A structural framework for provable generalization in RL. In
Proceedings of the 38th International Conference on Machine Learning (ICML) , pages 2826–2836,
2021.
Louis Faury, Marc Abeille, Clément Calauzènes, and Olivier Fercoq. Improved optimistic algorithms
for logistic bandits. In Proceedings of the 37th International Conference on Machine Learning
(ICML) , pages 3052–3060, 2020.
Louis Faury, Marc Abeille, Kwang-Sung Jun, and Clément Calauzènes. Jointly efficient and optimal
algorithms for logistic bandits. In Proceedings of the 25th International Conference on Artificial
Intelligence and Statistics (AISTATS) , pages 546–580, 2022.
Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Moham-
madamin Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz
Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning.
Nature , 610(7930):47–53, 2022.
Sarah Filippi, Olivier Cappé, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The
generalized linear case. In Advances in Neural Information Processing Systems 23 (NIPS) , pages
586–594, 2010.
11Dylan J. Foster, Sham M. Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of
interactive decision making. ArXiv preprint , 2112.13487, 2021.
Elad Hazan, Tomer Koren, and Kfir Y . Levy. Logistic regression: Tight bounds for stochastic and
online optimization. In Proceedings of The 27th Conference on Learning Theory (COLT) , pages
197–209, 2014.
Jiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement
learning for linear Markov decision processes. In Proceedings of the 40th International Conference
on Machine Learning (ICML) , pages 12790–12822, 2023.
Taehyun Hwang and Min-hwan Oh. Model-based reinforcement learning with multinomial logistic
function approximation. In Proceedings of the 37th AAAI Conference on Artificial Intelligence
(AAAI) , pages 7971–7979, 2023.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcement
learning with linear function approximation. In Proceedings of the 33rd Conference on Learning
Theory (COLT) , pages 2137–2143, 2020.
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL
problems, and sample-efficient algorithms. In Advances in Neural Information Processing Systems
34 (NeurIPS) , pages 13406–13418, 2021.
Joongkyu Lee and Min-hwan Oh. Nearly minimax optimal regret for multinomial logistic bandit. In
Advances in Neural Information Processing Systems 36 (NeurIPS) , page to appear, 2024.
Gene Li, Junbo Li, Anmol Kabra, Nati Srebro, Zhaoran Wang, and Zhuoran Yang. Exponential
family model-based reinforcement learning via score matching. In Advances in Neural Information
Processing Systems 35 (NeurIPS) , 2022.
Long-Fei Li, Peng Zhao, and Zhi-Hua Zhou. Dynamic regret of adversarial linear mixture MDPs. In
Advances in Neural Information Processing Systems 36 (NeurIPS) , pages 60685–60711, 2023.
Long-Fei Li, Yu-Jie Zhang, Peng Zhao, and Zhi-Hua Zhou. Provably efficient reinforcement
learning with multinomial logit function approximation. ArXiv preprint , 2405.17061, 2024a. URL
https://arxiv.org/abs/2405.17061v1 .
Long-Fei Li, Peng Zhao, and Zhi-Hua Zhou. Improved algorithm for adversarial linear mixture
MDPs with bandit feedback and unknown transition. In Proceedings of the 27th International
Conference on Artificial Intelligence and Statistics (AISTATS) , pages 3061–3069, 2024b.
Min-hwan Oh and Garud Iyengar. Thompson sampling for multinomial logit contextual bandits. In
Advances in Neural Information Processing Systems 32 (NeurIPS) , pages 3145–3155, 2019.
Min-hwan Oh and Garud Iyengar. Multinomial logit contextual bandits: Provable optimality and
practicality. In Proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI) , pages
9205–9213, 2021.
Francesco Orabona. A modern introduction to online learning. ArXiv preprint , 1912.13213, 2019.
Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.
InAdvances in Neural Information Processing Systems 27 (NIPS) , pages 1466–1474, 2014.
Reda Ouhamma, Debabrota Basu, and Odalric Maillard. Bilinear exponential family of mdps:
Frequentist regret bound with tractable exploration & planning. In Proceedings of the 37th AAAI
Conference on Artificial Intelligence (AAAI) , pages 9336–9344, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in Neural Information Processing Systems 35
(NeurIPS) , pages 27730–27744, 2022.
Jaehyun Park, Junyeop Kwon, and Dabeen Lee. Infinite-horizon reinforcement learning with multi-
nomial logistic function approximation. ArXiv preprint , 2406.13633, 2024.
12Noémie Périvier and Vineet Goyal. Dynamic pricing and assortment under a contextual MNL demand.
InAdvances in Neural Information Processing Systems 35 (NeurIPS) , pages 3461–3474, 2022.
Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic
exploration. In Advances in Neural Information Processing Systems 26 (NIPS) , pages 2256–2264,
2013.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game
of Go with deep neural networks and tree search. Nature , pages 484–489, 2016.
Alexander L. Strehl and Michael L. Littman. An empirical evaluation of interval estimation for
markov decision processes. In Proceedings of the 16th IEEE International Conference on Tools
with Artificial Intelligence (ICTAI) , pages 128–135, 2004.
Yining Wang, Ruosong Wang, Simon Shaolei Du, and Akshay Krishnamurthy. Optimism in rein-
forcement learning with generalized linear function approximation. In Proceedings of the 9th
International Conference on Learning Representations (ICLR) , 2021.
Chen-Yu Wei and Haipeng Luo. Non-stationary reinforcement learning without prior knowledge: An
optimal black-box approach. In Proceedings of the 34th Conference on Learning Theory (COLT) ,
pages 4300–4354, 2021.
Lin Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features.
InProceedings of the 36th International Conference on Machine Learning (ICML) , pages 6995–
7004, 2019.
Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In Proceedings of the 37th International Conference on Machine Learning (ICML) ,
pages 10746–10756, 2020.
Lijun Zhang, Tianbao Yang, Rong Jin, Yichi Xiao, and Zhi-Hua Zhou. Online stochastic linear
optimization under one-bit feedback. In Proceedings of the 33nd International Conference on
Machine Learning (ICML) , pages 392–401, 2016.
Yu-Jie Zhang and Masashi Sugiyama. Online (multinomial) logistic bandit: Improved regret and
constant computation cost. In Advances in Neural Information Processing Systems 36 (NeurIPS) ,
pages 29741–29782, 2023.
Canzhe Zhao, Ruofeng Yang, Baoxiang Wang, and Shuai Li. Learning adversarial linear mixture
Markov decision processes with bandit feedback and unknown transition. In Proceedings of the
11th International Conference on Learning Representations (ICLR) , 2023.
Peng Zhao, Long-Fei Li, and Zhi-Hua Zhou. Dynamic regret of online Markov decision processes.
InProceedings of the 39th International Conference on Machine Learning (ICML) , pages 26865–
26894, 2022.
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvári. Nearly minimax optimal reinforcement learning
for linear mixture Markov decision processes. In Proceedings of the 34th Conference on Learning
Theory (COLT) , pages 4532–4576, 2021.
13A Properties of the Multinomial Logit Function
This section collects several key properties of the multinomial logit function used in the paper.
Without loss of generality, we assume ∀Sh,s,a,∃s′∈ Sh,s,a such that ϕ(s′|s, a) =0. Otherwise, we
can always define a new feature mapping ϕ′(s′′|s, a) =ϕ(s′|s, a)−ϕ(s′′|s, a)for any s′′∈ Sh,s,a
such that ϕ′(s′|s, a) =0and the transition induced by ϕ′is the same as that induced by ϕ. We
denote this state as ˙sh,s,a and˙Sh,s,a=Sh,s,a\{˙sh,s,a}.
Recall the following definitions in the paper:
fk,h(θ) =−X
s′∈Sk,hys′
k,hlogps′
k,h(θ),
gk,h(θ) =X
s′∈Sk,h(ps′
k,h(θ)−ys′
k,h)ϕs′
k,h
Hk,h(θ) =X
s′∈Sk,hps′
k,h(θ)ϕs′
k,h(ϕs′
k,h)⊤−X
s′∈Sk,hX
s′′∈Sk,hps′
k,h(θ)ps′′
k,h(θ)ϕs′
k,h(ϕs′′
k,h)⊤.
bΣk,h=κ
2k−1X
i=1X
s′∈Si,hϕs′
i,h(ϕs′
i,h)⊤+λId,
Hk,h=k−1X
i=1Hi,h(eθi+1,h) +λId,
Lemma 5. The following statements hold for any k∈[K], h∈[H]:
Hk,h(θ)⪰X
s′∈˙Sk,hps′
k,h(θ)p˙sk,h
k,h(θ)ϕs′
k,h(ϕs′
k,h)⊤⪰κX
s′∈˙Sk,hϕs′
k,h(ϕs′
k,h)⊤.
Proof. First, note that
∀x, y∈Rd,(x−y)(x−y)⊤=xx⊤+yy⊤−xy⊤−yx⊤⪰0 =⇒xx⊤+yy⊤⪰xy⊤+yx⊤.
Then, we have
Hk,h(θ)
=X
s′∈Sk,hps′
k,h(θ)ϕs′
k,h(ϕs′
k,h)⊤−X
s′∈Sk,hX
s′′∈Sk,hps′
k,h(θ)ps′′
k,h(θ)ϕs′
k,h(ϕs′′
k,h)⊤
=X
s′∈˙Sk,hps′
k,h(θ)ϕs′
k,h(ϕs′
k,h)⊤−X
s′∈˙Sk,hX
s′′∈˙Sk,hps′
k,h(θ)ps′′
k,h(θ)ϕs′
k,h(ϕs′′
k,h)⊤
=X
s′∈˙Sk,hps′
k,h(θ)ϕs′
k,h(ϕs′
k,h)⊤−1
2X
s′∈˙Sk,hX
s′′∈˙Sk,hps′
k,h(θ)ps′′
k,h(θ)
ϕs′
k,h(ϕs′′
k,h)⊤+ϕs′′
k,h(ϕs′
k,h)⊤
⪰X
s′∈˙Sk,hps′
k,h(θ)ϕs′
k,h(ϕs′
k,h)⊤−1
2X
s′∈˙Sk,hX
s′′∈˙Sk,hps′
k,h(θ)ps′′
k,h(θ)
ϕs′
k,h(ϕs′
k,h)⊤+ϕs′′
k,h(ϕs′′
k,h)⊤
=X
s′∈˙Sk,hps′
k,h(θ)ϕs′
k,h(ϕs′
k,h)⊤−X
s′∈˙Sk,hX
s′′∈˙Sk,hps′
k,h(θ)ps′′
k,h(θ)ϕs′
k,h(ϕs′
k,h)⊤
=X
s′∈˙Sk,hps′
k,h(θ)
1−X
s′′∈˙Sk,hps′′
k,h(θ)
ϕs′
k,h(ϕs′
k,h)⊤
=X
s′∈˙Sk,hps′
k,h(θ)p˙sk,h
k,h(θ)ϕs′
k,h(ϕs′
k,h)⊤
⪰κX
s′∈˙Sk,hϕs′
k,h(ϕs′
k,h)⊤,
where the last inequality holds by the definition of κin Assumption 1. This finishes the proof. ■
14Lemma 6. Suppose λ≥1, define eϕs′
k,h=ϕs′
k,h−P
s′′∈Sk,hps′′
k,h(eθk+1,h)ϕs′′
k,h, for any k∈[K], h∈
[H], the following statements hold:
(I)kX
i=1max
s′∈Si,h∥ϕs′
i,h∥2
bΣ−1
i,h≤4
κdlog
1 +kU
λd
(II)kX
i=1X
s′∈Si,hps′
i,h(eθi+1,h)p˙sk,h
i,h(eθi+1,h)∥ϕs′
i,h∥2
H−1
i,h≤2dlog
1 +k
λd
(III)kX
i=1max
s′∈Si,h∥ϕs′
i,h∥2
H−1
i,h≤2
κdlog
1 +k
λd
(IV)kX
i=1X
s′∈Si,hps′
i,h(eθi+1,h)∥eϕs′
i,h∥2
H−1
i,h≤2dlog
1 +k
λd
(V)kX
i=1max
s′∈Si,h∥eϕs′
i,h∥2
H−1
i,h≤2
κdlog
1 +k
λd
Proof. We prove the five statements individually.
Proof of statement (I). By the definition that bΣk+1,h=bΣk,h+κ
2P
s′∈Sk,hϕs′
k,h(ϕs′
k,h)⊤, we have
det(bΣk+1,h) = det( bΣk,h)
1 +κ
2X
s′∈Sk,h∥ϕs′
k,h∥2
bΣ−1
k,h
.
Then, we get
kX
i=1log
1 +κ
2X
s′∈Si,h∥ϕs′
i,h∥2
bΣ−1
i,h
≤log 
det(bΣk+1,h)
det(bΣ1,h)!
≤dlog
1 +kU
λd
, (12)
where the last inequality holds by determinant trace inequality in Lemma 16. Since λ≥1, we have
max s′∈Si,h∥ϕs′
i,h∥bΣ−1
i,h≤1, thus, we have
kX
i=1max
s′∈Si,h∥ϕs′
i,h∥bΣ−1
i,h≤2
κkX
i=1min
1,κ
2X
s′∈Si,h∥ϕs′
i,h∥bΣ−1
i,h
≤4
κkX
i=1log
1 +κ
2X
s′∈Si,h∥ϕs′
i,h∥2
bΣ−1
i,h
≤4
κdlog
1 +kU
λd
,
where the first inequality holds by the fact that z≤2 log(1 + z)for any z∈[0,1]and the last
inequality holds by (12).
Proof of statement (II). By Lemma 5, we have Hk,h(θ)⪰P
s′∈˙Sk,hps′
k,h(θ)p˙sk,h
k,h(θ)ϕs′
k,h(ϕs′
k,h)⊤.
Thus, we have
Hk+1,h⪰ H k,h+X
s′∈˙Sk,hps′
k,h(eθk+1,h)p˙sk,h
k,h(eθk+1,h)ϕs′
k,h(ϕs′
k,h)⊤
Then, we get
det(Hi+1,h)≥det(Hi,h)
1 +X
s′∈˙Si,hps′
i,h(eθi+1,h)p˙si,h
i,h(eθi+1,h)∥ϕs′
i,h∥2
H−1
i,h
.
15Since λ≥1, we haveP
s′∈˙Si,hps′
i,h(eθi+1,h)p˙si,h
i,h(eθi+1,h)∥ϕs′
i,h∥2
H−1
i,h≤1. Using the fact that
z≤2 log(1 + z)for any z∈[0,1], we get
kX
i=1X
s′∈˙Si,hps′
i,h(eθi+1,h)p˙si,h
i,h(eθi+1,h)∥ϕs′
i,h∥2
H−1
i,h
≤2kX
i=1log
1 +X
s′∈˙Si,hps′
i,h(eθi+1,h)p˙si,h
i,h(eθi+1,h)∥ϕs′
i,h∥2
H−1
i,h
≤2 logdet(Hk+1,h)
det(H1,h)
≤2dlog
1 +k
λd
.
Proof of statement (III). By Lemma 5, we have
Hk+1,h⪰ H k+1,h+κX
s′∈˙Sk,hϕs′
k,h(ϕs′
k,h)⊤
Since λ≥1, we have κmax s′∈Si,h∥ϕs′
i,h∥H−1
i,h≤κ. Using the fact that z≤2 log(1 + z)for any
z∈[0,1]. By a similar analysis as the statement (2), we have
kX
i=1max
s′∈Si,h∥ϕs′
i,h∥2
H−1
i,h≤2
κkX
i=1log
1 + max
s′∈Si,h∥ϕs′
i,h∥H−1
i,h
≤2
κlogdet(Hk+1,h)
det(H1,h)
≤2
κdlog
1 +k
λd
.
This finishes the proof.
Proof of statement (IV). By the definition of Hk,h(θ), we have
Hi,h(θ) =X
s′∈Si,hps′
i,h(θ)ϕs′
i,h(ϕs′
i,h)⊤−X
s′∈Si,hX
s′′∈Si,hps′
i,h(θ)ps′′
i,h(θ)ϕs′
i,h(ϕs′′
i,h)⊤
=Es′∈pi,h(θ)[ϕs′
i,h(ϕs′
i,h)⊤]−Es′∈pi,h(θ)[ϕs′
i,h] 
Es′∈pi,h(θ)[ϕs′
i,h]⊤
=Es′∈pi,h(θ)
(ϕs′
i,h−Es′′∈pi,h(θ)ϕs′′
i,h)(ϕs′
i,h−Es′′∈pi,h(θ)ϕs′′
i,h)⊤
Thus, we have
Hi,h(eθi+1,h)⪰X
s′∈Si,hpi,h(eθi+1,h)(eϕs′
i,h)(eϕs′
i,h)⊤. (13)
Then, we get
Hk+1,h⪰ H k,h+X
s′∈Sk,hpk,h(eθk+1,h)(eϕs′
k,h)(eϕs′
k,h)⊤
The remaining proof is similar to the proof of statement (2).
Proof of statement (V). By (13), we have
Hi,h(eθi+1,h)⪰X
s′∈Si,hpi,h(eθi+1,h)(eϕs′
i,h)(eϕs′
i,h)⊤⪰κX
s′∈Si,h(eϕs′
i,h)(eϕs′
i,h)⊤.
Then, the remaining proof is similar to the proof of statement (3). ■
16B Proof of Lemma 1
B.1 Main Proof
Proof. The main proof follows the proof of Theorem 1 in Zhang et al. [2016]. We define
¯fk,h(θ) =Eyk,h[fk,h(θ)| Fk,h],¯gk,h(θ) =Eyk,h[gk,h(θ)| Fk,h].
By Property 1, we have fk,h(θ)is exponential concave such that
fk,h(bθk,h)≤fk,h(θ∗
h) +⟨gk,h(θ∗
h),bθk,h−θ∗
h⟩ −κ
2∥(bθk,h−θ∗
h)∥Wk,h.
Taking expectations on both sides, we have
¯fk,h(bθk,h)≤¯fk,h(θ∗
h) +⟨¯gk,h(θ∗
h),bθk,h−θ∗
h⟩ −κ
2∥(bθk,h−θ∗
h)∥Wk,h. (14)
Based the property of KL diverge, we ensure the true parameter θ∗
his the minimizer of the expected
loss function by the following lemma.
Lemma 7. For any θ∈Θ, we have ¯fk,h(θ∗
h)≤¯fk,h(θ).
Combining Lemma 7 and Equation (14), we have
0≤¯fk,h(bθk,h)−¯fk,h(θ∗
h)
≤ ⟨¯gk,h(bθk,h),bθk,h−θ∗
h⟩ −κ
2∥θ∗
h−bθk,h∥2
Wk,h
≤ ⟨gk,h(bθk,h),bθk,h−θ∗
h⟩ −κ
2∥θ∗
h−bθk,h∥2
Wk,h+⟨¯gk,h(bθk,h)−gk,h(bθk,h),bθk,h−θ∗
h⟩.(15)
By standard analysis of OMD in Lemma 17, it holds
2gk,h(bθk,h)(bθk,h−θ∗
h)≤ ∥gk,h(bθk,h)∥bΣk+1,h+∥bθk,h−θ∗
h∥bΣk+1,h− ∥bθk+1,h−θ∗
h∥bΣk+1,h.(16)
Combining Equation (15) and Equation (16), we have
0≤1
2∥gk,h(bθk,h)∥2
bΣ−1
k+1,h+1
2∥bθk,h−θ∗
h∥2
bΣk+1,h−1
2∥bθk+1,h−θ∗
h∥2
bΣk+1,h
−κ
2∥θ∗
h−bθk,h∥2
Wk,h+⟨¯gk,h(bθk,h)−gk,h(bθk,h),bθk,h−θ∗
h⟩. (17)
First, we consider the first term . We show the gradient can be bounded by the following lemma.
Lemma 8. For any positive semi-definite matrix Z, it holds that
∥gk,h(θ)∥2
Z≤4 max
s′∈Sk,h∥ϕs′
k,h∥2
Z. (18)
Then, we consider the second term. Note that bΣk+1,h=bΣk,h+κ
2Wk,h, we have
∥bθk,h−θ∗∥2
bΣk+1,h=∥bθk,h−θ∗∥2
bΣk,h+κ
2∥bθk,h−θ∗∥2
Wk,h. (19)
Next, we bound ⟨¯gk,h(bθk,h)−gk,h(bθk,h),bθk,h−θ∗
h⟩, which is a martingale difference sequence.
Lemma 9. For any δ∈(0,1)andθ1, . . . , θ k, θ∗∈[0, B]d, with probability at least 1−δ, for any
k∈[K], h∈[H]it holds that
kX
i=1⟨¯gi,h(θi)−gi,h(θi), θi−θ∗
h⟩ ≤κ
4kX
i=1∥θi−θ∗∥2
Wi,h+ (4
κ+ 8B)γk. (20)
where γk= log2k2Hlog(kU)
δ.
17Combining (17), (18), and (19), we have
∥bθk+1,h−θ∗
h∥2
bΣk+1,h≤ ∥bθk,h−θ∗
h∥2
bΣk+1,h−κ∥θ∗
h−bθk,h∥2
Wk,h+∥gk,h(bθk,h)∥2
bΣ−1
k+1,h
+ 2⟨¯gk,h(bθk,h)−gk,h(bθk,h),bθk,h−θ∗
h⟩
≤ ∥bθk,h−θ∗
h∥2
bΣk,h+ 4 max
s′∈Sk,h∥ϕs′
k,h∥2
bΣ−1
k+1,h+ (8
κ+ 16B)γk (21)
where the first inequality holds by rearranging the terms in (17), the second holds by (18),(19)
and (20). Summing (21) from i= 1tok, we have
∥bθk+1,h−θ∗
h∥2
bΣk+1,h≤λB2+ 4kX
i=1max
s′∈Si,h∥ϕs′
i,h∥2
bΣ−1
i+1,h+ (8
κ+ 16B)γk
≤λB2+8
κdlog
1 +kU
λd
+ (8
κ+ 16B)γk,
where the second inequality holds by ∥ϕs′
i,h∥2≤1andbΣi,h≥I,∀i∈[K], and the last inequality is
by Lemma 6. This finishes the proof. ■
B.2 Proof of Auxiliary Lemmas
In this section, we provide the proofs of the lemmas used in Appendix B.1.
B.2.1 Proof of Lemma 7
Proof. By the definition of ¯fk,h(θ), we have
¯fk,h(θ)−¯fk,h(θ∗
h) =X
s′∈Sk,hps′
k,h(θ∗
h) logps′
k,h(θ∗
h)
ps′
k,h(θ)≥0,
where the last inequality is due toP
s′∈Sk,hps′
k,h(θ∗
h) logps′
k,h(θ∗
h)
ps′
k,h(θ)is the Kullback-Leibler divergence
between pk,h(θ∗
h)andpk,h(θ), which always is non-negative. ■
B.2.2 Proof of Lemma 8
Proof. For any positive semi-definite matrix Z,
(xi−xj)⊤Z(xi−xj) =x⊤
iZxi−x⊤
iZxj−x⊤
jZxi+x⊤
jZxj≥0,∀xi, xj∈Rd. (22)
which implies x⊤
iZxi+x⊤
jZxj≥x⊤
iZxj+x⊤
jZxi,∀xi, xj∈Rd.
Letxi= (ps′
k,h(θ)−ys′
k,h)ϕs′
k,h, we have
∥gk,h(θ)∥2
Z
=X
s′∈Sk,hX
s′′∈Sk,h(ps′
k,h(θ)−ys′
k,h)(ps′′
k,h(θ)−ys′′
k,h)ϕs′
k,hZϕs′′
k,h
=X
s′∈Sk,h(ps′
k,h(θ)−ys′
k,h)2ϕs′
k,hZϕs′
k,h
+1
2X
s′∈Sk,hX
s′′∈Sk,h(ps′
k,h(θ)−ys′
k,h)(ps′′
k,h(θ)−ys′′
k,h)(ϕs′
k,hZϕs′′
k,h+ϕs′′
k,hZϕs′
k,h)
≤X
s′∈Sk,h(ps′
k,h(θ)−ys′
k,h)2ϕs′
k,hZϕs′
k,h+X
s′∈Sk,h(ps′
k,h(θ)−ys′
k,h)2ϕs′
k,hZϕs′
k,h
= 2X
s′∈Sk,h(ps′
k,h(θ)−ys′
k,h)2ϕs′
k,hZϕs′
k,h
≤4 max
s′∈Sk,h∥ϕs′
k,h∥2
Z.
This finishes the proof. ■
18B.2.3 Proof of Lemma 9
Proof. First, notice that (¯gi,h(θi)−gi,h(θi))⊤(θi−θ∗)is a martingale difference sequence. Also,
we have
(¯gi,h(θi)−gi,h(θi))⊤(θi−θ∗)
≤(¯gi,h(θi))⊤(θi−θ∗)+(gi,h(θi))⊤(θi−θ∗)
≤ ∥¯gi,h(θi)∥2∥θi−θ∗∥2+∥gi,h(θi)∥2∥θi−θ∗∥2
≤4√
2B,
where the last inequality holds by ∥¯gi,h(θi)∥2=∥P
s′∈Si,h(ps′
i,h(θ)−ys′
i,h)ϕs′
i,h∥2≤√
2.
We define the martingale Mk,h=Pk
i=1(¯gi,h(θi)−gi,h(θi))⊤(θi−θ∗), and define the conditional
variance σ2
ias
σ2
k,h=E[M2
k,h| Fi−1]
=kX
i=1Eyk,hh
(¯gi,h(θi)−gi,h(θi))⊤(θi−θ∗)2i
≤kX
i=1Eyk,hh
(gi,h(θi))⊤(θi−θ∗)2i
≤kX
i=1X
s′∈Si,h(ϕs′
i,h)⊤(θi−θ∗)2
=kX
i=1∥θi−θ∗∥2
Wi,h
≜Ak,h.
where the first inequality is due to the fact that E[(ξ−E[ξ])2]≤E[ξ2]for any random variable ξ.
Note that Ak,his a random variable, so we cannot directly apply the Bernstein inequality to Mk,h.
Instead, we consider the following two cases:(i) Ak,h≤4B2
kUand (ii) Ak,h>4B2
kU.
Case (i): Ak,h=Pk
i=1∥θi−θ∗∥2
Wi,h≤4B2
kU. Then, we have
Mk,h=kX
i=1(¯gi,h(θi)−gi,h(θi))⊤(θi−θ∗)
=kX
i=1X
s′∈Si,h(ps′
i,h(θi)−ys′
i,h)(ϕs′
i,h)⊤(θi−θ∗)
≤kX
i=1X
s′∈Si,h|(ϕs′
i,h)⊤(θi−θ∗)|
≤vuutkUkX
i=1X
s′∈Si,h((ϕs′
i,h)⊤(θi−θ∗))2
≤2B.
where the second equality is due to the definition of ¯gi,h(θi)andgi,h(θi), the first inequality holds by
ps′
i,h(θi)−ys′
i,h∈[−1,1], the second inequality holds by the Cauchy-Schwarz inequality, and the last
inequality holds by the condition of case (i).
19Case (ii): Ak,h=Pk
i=1∥θi−θ∗∥2
Wj,h>4B2
kU. We have both a lower and upper bound for Ak,h, i.e.,
4B2
kU< A k,h≤4B2kU. Then, we can use the peeling process to bound Mk,has follows:
Pr
Mk,h≥2p
τk,hAk,h+8Bτk,h
3
= Pr
Mk,h≥2p
τk,hAk,h+8Bτk,h
3,4B2
kU< A k,h≤4kUB2
= Pr
Mk,h≥2p
τk,hAk,h+8Bτk,h
3,4B2
kU< A k,h≤4kUB2, σk,h≤Ak,h
≤mX
i=1Pr
Mk,h≥2p
τk,hAk,h+8Bτk,h
3,4B22i−1
kU< A k,h≤4B22i
kU, σk,h≤Ak,h
≤mX
i=1Pr"
Mk,h≥r
8B22i
3kUτk,h+8Bτk,h
3, σk,h≤4B22i
kU#
≤mexp(−τk,h).
where m= 2 log2(kU), and the last inequality follows the Bernstein inequality for martingales.
Combining above two cases, letting τ= logmk2
δ/Hand taking the union bound over kandh∈[H],
we have with probability at least 1−δ, for any k∈[K], h∈[H]it holds that
Mk,h=kX
i=1(¯gi,h(θi)−gi,h(θi))⊤(θi−θ∗)
≤2p
τk,hAk,h+8Bτk,h
3+ 4√
2B
≤2vuutkX
i=1∥θi−θ∗∥2
Wi,hlog2k2Hlog(kU)
δ+ 8B
1 + log2k2Hlog(kU)
δ
= 2vuutγkkX
i=1∥θi−θ∗∥2
Wi,h+ 8B(1 +γk), (23)
where γk= log2k2Hlog(kU)
δ.
Then, applying uv≤cu2+v2/(4c)for any c, u, v > 0withc= 2/κ, we have
vuutγkkX
i=1∥θi−θ∗∥2
Wi,h≤2γk
κ+κ
8kX
i=1∥θi−θ∗∥2
Wi,h. (24)
Combining (23) and (24), we have
kX
i=1(¯gi,h(θi)−gi,h(θi))⊤(θi−θ∗)≤κ
4kX
i=1∥θi−θ∗∥2
Wi,h+ (4
κ+ 8B)γk.
This finishes the proof. ■
C Proof of Lemma 2
Proof. The gradient of ps,a(θ)is given by
∇ps′
s,a(θ) =ps′
s,a(θ)ϕs′
s,a−ps′
s,a(θ)X
s′′∈Sh,s,aps′′
s,a(θ)ϕs′′
s,a.
20By the mean value theorem, there exists ¯θ=νθ∗
h+ (1−ν)bθk,hfor some ν∈[0,1], such thatX
s′∈Sh,s,aps′
s,a(bθk,h)V(s′)−X
s′∈Sh,s,aps′
s,a(θ∗
h)V(s′)
=X
s′∈Sh,s,a∇ps′
s,a(¯θ)(bθk,h−θ∗
h)V(s′)
=X
s′∈Sh,s,aps′
s,a(¯θ)ϕs′
s,a(bθk,h−θ∗
h)V(s′)−X
s′∈Sh,s,aps′
s,a(¯θ)X
s′′∈Sh,s,aps′′
s,a(¯θ)ϕs′′
s,a(bθk,h−θ∗
h)V(s′)
=X
s′∈Sh,s,aps′
s,a(¯θ)ϕs′
s,a(bθk,h−θ∗
h)
V(s′)−X
s′′∈Sh,s,aps′′
s,a(¯θ)V(s′′)
≤HX
s′∈Sh,s,aps′
s,a(¯θ)|ϕs′
s,a(bθk,h−θ∗
h)|
≤Hmax
s′∈Sh,s,aϕs′
s,a(bθk,h−θ∗
h) (25)
≤Hmax
s′∈Sh,s,a∥ϕs′
s,a∥bΣ−1
k,h∥bθk,h−θ∗
h∥bΣk,h
≤Hβkmax
s′∈Sh,s,a∥ϕs′
s,a∥bΣ−1
k,h,
where the first inequality is by V:S → [0, H], the second inequality is by the definition of ϕs′
k,h, the
third inequality is by the Holder’s inequality, the last inequality is by Lemma 1. ■
D Proof of Theorem 1
D.1 Main Proof
To prove the theorem, we first introduce the following lemma.
Lemma 10. Suppose for all (h, s, a )∈[H]× S × A , it holds thatX
s′∈Sh,s,aps′
s,a(bθk,h)V(s′)−X
s′∈Sh,s,aps′
s,a(θ∗
h)V(s′)≤Γh,s,a. (26)
Define
bQk,h(s, a) =
rh(s, a) +X
s′∈Sh,s,aps′
s,a(bθk,h)bVk,h+1(s′) + Γ h,s,a
[0,H]. (27)
Then, for any δ∈(0,1], with probability at least 1−δ, it holds that
Reg(K)≤2KX
k=1HX
h=1Γh,sk,h,ak,h+Hp
2KHlog(2 /δ).
Proof of Theorem 1. Substituting Γh,s,a=Hbβkmax s′∈Sh,s,a∥ϕs′
s,a∥bΣ−1
k,hinto Lemma 10, we have
Reg(K)≤2HbβkKX
k=1HX
h=1max
s′∈Sk,h∥ϕs′
k,h∥bΣ−1
k,h+Hp
2KHlog(2 /δ)
≤2HbβkHX
h=1vuutKKX
k=1max
s′∈Sk,h∥ϕs′
k,h∥2
bΣ−1
k,h+Hp
2KHlog(2 /δ)
≤2H2bβks
4dK
κlog
1 +KU
dλ
+Hp
2KHlog(2 /δ)≤eO(κ−1dH2√
K),
where the second inequality holds by the Cauchy-Schwarz inequality and the third inequality holds
by Lemma 6. This finishes the proof. ■
21D.2 Proof of Auxiliary Lemmas
In this section, we provide the proofs of the lemmas used in Appendix D.1.
First, we introduce the following lemma.
Lemma 11. Suppose (26) and(27) in Lemma 10 holds for all k∈[K], h∈[H]. Then, for any
(s, a, h )∈ S × A × [H], it holds that
Q∗
h(s, a)≤bQk,h(s, a)≤rh(s, a) +PhVk,h+1(s, a) + 2Γ h,s,a.
Proof. First, we prove the left-hand side of the lemma. We prove this by backward induction on h.
For the stage h=H, by definition, we have
bQk,H(s, a) =rH(s, a) =Q∗
H(s, a),bVk,H+1(s) = 0 = V∗
H+1(s).
Suppose the statement holds for h+ 1, we show it holds for h. By definition, if bQk,h(s, a) =H, this
holds trivially. Otherwise, we have
bQk,h(s, a) =rh(s, a) +ps,a(bθk,h)bVk,h+1+ Γh,s,a
≥rh(s, a) +ps,a(bθk,h)V∗
k+ Γh,s,a≥rh(s, a) +ps,a(θ∗
h)V∗
k=Q∗
h(s, a).
where the first inequality is by the induction hypothesis, and the second inequality is by (26).
Then, we prove the right-hand side of the lemma. By the definition of bQk,h(s, a), we have
bQk,h(s, a) =rh(s, a) +ps,a(bθk,h)bVk,h+1+ Γh,s,a≤rh(s, a) +ps,a(θ∗
h)bVk,h+1+ 2Γ h,s,a,
where the inequality is by (26). ■
D.2.1 Proof of Lemma 10
Proof. By the definition of Reg(K) =PK
k=1V∗
1(sk,1)−PK
k=1Vπk
1(sk,1), we have
KX
k=1V∗
1(sk,1)−KX
k=1Vπk
1(sk,1) =KX
k=1Q∗
1(sk,1, π∗(sk,1))−KX
k=1Vπk
1(sk,1)
≤KX
k=1bQ1(sk,1, π∗(sk,1))−KX
k=1Vπk
1(sk,1)
≤KX
k=1bQ1(sk,1, ak,1)−KX
k=1Vπk
1(sk,1),
where the first inequality is by Lemma 11 and the second is by ak,h= arg maxa∈AbQk,h(sk,h, a).
By the right-hand side of Lemma 11, we have
bQ1(sk,1, ak,1)−Vπk
1(sk,1)
=r(sk,1, ak,1) +P1bVk,2(sk,1, ak,1) + +2Γ h,sk,1,ak,1−r(sk,1, ak,1)−P1Vπk
2(sk,1, ak,1)
≤P1(bVk,2−Vπk
2)(sk,1, ak,1)−(bVk,2−Vπk
2)(sk,2) + (bVk,2−Vπk
2)(sk,2) + 2Γ h,sk,1,ak,1
≤P1(bVk,2−Vπk
2)(sk,1, ak,1)−(bVk,2−Vπk
2)(sk,2) +
bQ2(sk,2, ak,2)−Vπk
2(sk,2)
+ 2Γ h,sk,1,ak,1.
Define Mk,h=Ph(bVk,h+1−Vπk
h+1)(sk,h, ak,h)−(bVk,h+1−Vπk
h+1)(sk,h+1). Applying this recur-
sively, we have
bQ1(sk,1, ak,1)−Vπk
1(sk,1)≤2HX
h=1Γh,sk,h,ak,h+HX
h=1Mk,h
Summing over k, we have for any δ∈(0,1], with probability at least 1−δ, it holds that
Reg(K)≤2KX
k=1HX
h=1Γh,sk,h,ak,h+KX
k=1HX
h=1Mk,h≤2KX
k=1HX
h=1Γh,sk,h,ak,h+Hp
2KHlog(2 /δ)
where the inequality holds by the Azuma-Hoeffding inequality as Mk,his a martingale difference
sequence with Mk,h≤2H. This finishes the proof. ■
22E Proof of Lemma 3
Proof. The proof is similar to the proof of Theorem 3 in Zhang and Sugiyama [2023] and Lemma 1
of Lee and Oh [2024]. Define
efk,h(θ) =fk,h(θk,h) +⟨gk,h(eθk,h), θ−eθk,h⟩+1
2∥θ−eθk,h∥Hk,h(eθk,h),
which is a second order approximation of the original function fk,h(θ)ateθk,h. Then, the update rule
in (9) can be rewritten as
eθk+1,h= arg min
θ∈Θefk,h(θ) +1
2η∥θ−eθk,h∥Hk,h. (28)
Then, we present the following lemma that bounds the estimation error of the update rule in (28).
Lemma 12 (Lemma E.1 of Lee and Oh [2024]) .For the update rule defined in (28), it holds that
∥eθk+1,h−θ∗
h∥2
Hk+1,h≤2η kX
i=1fi,h(θ∗
h)−kX
i=1fi,h(eθi+1,h)!
+ 4λB
+ 12√
2BηkX
i=1∥eθi+1,h−eθi,h∥2
2−kX
i=1∥eθi+1,h−eθi,h∥2
Hi,h.
We bound the right-hand side of the above lemma separately in the following.
First, we bound the first term. Define the softmax function as follows.
[σk,h(z)]s=exp([ z])s
1 +P
s∈˙Sk,hexp([ z])s,∀s∈ Sk,h.
Then, the loss function in (2) can be rewritten as
fk,h(zk,h, yk,h) =X
s′∈Sk,h1[ys′
k,h= 1] log1
[σk,h(zk,h)]s′
.
Define a pseudo-inverse function of σk,h(·)as
[σ−1
k,h(p)]s′= log[p]s′
1− ∥p∥1
,∀p∈ {p∈[0,1]Nk,h| ∥p∥1<1}.
Then, we decompose the first term as follows.
kX
i=1fi,h(θ∗
h)−kX
i=1fi,h(eθi+1,h)
=kX
i=1fi,h(θ∗
h)−kX
i=1fi,h(zi,h, yi,h)
| {z }
(a)+kX
i=1fi,h(zi,h, yi,h)−kX
i=1fi,h(eθi+1,h)
| {z }
(b)
where zk,h=σ−1
k,h(Eθ∼Pk,h[σk,h((ϕs′
k,h)⊤θ)s′∈˙Sk,h]),Pk,h≜N(eθk,h,(1 +cH−1
k,h))is the Gaussian
distribution with mean eθk,hand covariance (1 +cH−1
k,h)where cis a constant to be specified later.
First, we bound the term (a)as follows.
Lemma 13 (Lemma E.2 of Lee and Oh [2024]) .Letδ∈(0,1]andλ≥1, for all k∈[K], h∈[H],
with probability at least 1−δ, we have
kX
i=1fi,h(θ∗
h)−kX
i=1fi,h(zi,h, yi,h)
≤(3 log(1 + ( U+ 1)k) + 3) 
17
16λ+ 2√
λlog2H√
1 + 2 k
δ
+ 16
log2H√
1 + 2 k
δ2!
+ 2.
23Then, we bound the term (b)as follows.
Lemma 14 (Lemma E.3 of Lee and Oh [2024]) .For any c≥0, letλ≥max{2,72cd}, then for all
k∈[K], h∈[H], we have
kX
i=1fi,h(zi,h, yi,h)−kX
i=1fi,h(eθi+1,h)≤1
2ckX
i=1eθi,h−θi+1,h2
Hi,h+√
6cdlog
1 +k+ 1
2λ
.
Now, we are ready to prove Lemma 3. Combining Lemma 12, Lemma 13, and Lemma 14, we have
∥eθk+1,h−θ∗
h∥2
Hk+1,h
≤2η"
(3 log(1 + ( U+ 1)k) + 3) 
17
16λ+ 2√
λlog2H√
1 + 2 k
δ
+ 16
log2H√
1 + 2 k
δ2!
+2 +√
6cdlog
1 +k+ 1
2λ#
+ 4λB+ 12√
2BηkX
i=1∥eθi+1,h−eθi,h∥2
2+ (η
c−1)kX
i=1∥eθi+1,h+eθi,h∥2
Hi,h
≤2η"
(3 log(1 + ( U+ 1)k) + 3) 
17
16λ+ 2√
λlog2H√
1 + 2 k
δ
+ 16
log2H√
1 + 2 k
δ2!
+ 2 +√
6cdlog
1 +k+ 1
2λ#
+ 4λB,
where the second inequality holds by setting c= 7η/6andλ≥max{84√
2ηB,84dη}, we have
12√
2BηkX
i=1∥eθi+1,h−eθi,h∥2
2+η
c−1kX
i=1∥eθi+1,h+eθi,h∥2
Hi,h
= 12√
2BηkX
i=1∥eθi+1,h−eθi,h∥2
2−1
7kX
i=1∥eθi+1,h+eθi,h∥2
Hi,h
≤
12√
2Bη−λ
7kX
i=1∥eθi+1,h−eθi,h∥2
2
≤0.
Thus, by setting η=1
2log(U+ 1) + ( B+ 1) ,λ= 84√
2η(B+d), we have
∥eθk+1,h−θ∗
h∥Hk+1,h≤ O √
dlogUlog(KH/δ )
≜eβk.
This finishes the proof. ■
F Proof of Lemma 4
Proof. By the second-order Taylor expansion, there exists ¯θ=νθ∗
h+(1−ν)bθk,hfor some ν∈[0,1],
such that
X
s′∈Sh,s,aps′
s,a(bθk,h)V(s′)−X
s′∈Sh,s,aps′
s,a(θ∗
h)V(s′)
=X
s′∈Sh,s,a∇ps′
s,a(eθk,h)⊤(θ∗
h−eθk,h)V(s′) +1
2X
s′∈Sh,s,a(bθk,h−θ∗
h)⊤∇2ps′
s,a(¯θ)(bθk,h−θ∗
h)V(s′)
The gradient of ps,a(θ)is given by
∇ps′
s,a(θ) =ps′
s,a(θ)ϕs′
s,a−ps′
s,a(θ)X
s′′∈Sh,s,aps′′
s,a(θ)ϕs′′
s,a.
24For the first-order term, we haveX
s′∈Sh,s,a∇ps′
s,a(eθk,h)⊤(θ∗
h−eθk,h)V(s′)
=X
s′∈Sh,s,aps′
s,a(eθk,h)ϕs′
s,a(θ∗
h−eθk,h)V(s′)−X
s′∈Sh,s,aps′
s,a(eθk,h)X
s′′∈Sh,s,aps′′
s,a(eθk,h)ϕs′′
s,a(θ∗
h−eθk,h)V(s′)
≤HX
s′∈S+
h,s,aps′
s,a(eθk,h)
ϕs′
s,a(θ∗
h−eθk,h)−X
s′′∈Sh,s,aps′′
s,a(eθk,h)ϕs′′
s,a(θ∗
h−eθk,h)

≤HX
s′∈S+
h,s,aps′
s,a(eθk,h)
ϕs′
s,a−X
s′′∈Sh,s,aps′′
s,a(eθk,h)ϕs′′
s,a
H−1
k,hθ∗
h−eθk,h
Hk,h

≤HeβkX
s′∈S+
h,s,aps′
s,a(eθk,h)
ϕs′
s,a−X
s′′∈Sh,s,aps′′
s,a(eθk,h)ϕs′′
s,a
H−1
k,h

≤HeβkX
s′∈Sh,s,aps′
s,a(eθk,h)
ϕs′
s,a−X
s′′∈Sh,s,aps′′
s,a(eθk,h)ϕs′′
s,a
H−1
k,h
 (29)
where in the first inequality, we denote S+
h,s,a as the subset of Sh,s,a such that ϕs′
s,a(θ∗
h−eθk,h)−P
s′′∈Sh,s,aps′′
s,a(eθk,h)ϕs′′
s,a(θ∗
h−eθk,h)is non-negative, the second inequality holds by the Holder’s
inequality, and the third inequality is by the confidence set in Lemma 3.
For the second-order term, we define us′
s,a(θ) =ϕs′
s,aθ, andps′
s,a(u) =exp(us′
s,a)
1+P
s′′exp(us′′
s,a), further define
F(u) =X
s′∈Sh,s,aexp(us′
s,a)
1 +P
s′′∈Sh,s,aexp(us′′
s,a),eF(u) =X
s′∈Sh,s,aexp(us′
s,a)V(s′)
1 +P
s′′∈Sh,s,aexp(us′′
s,a).
Then, we have
1
2X
s′∈Sh,s,a(bθk,h−θ∗
h)⊤∇2ps′
s,a(¯θ)(bθk,h−θ∗
h)V(s′)
=1
2 
u(bθk,h)−u(θ∗
h)⊤∇2eF(u(¯θ)) 
u(bθk,h)−u(θ∗
h)
=1
2X
s′∈Sh,s,aX
s′′∈Sh,s,a 
us′
s,a(bθk,h)−us′
s,a(θ∗
h)⊤∂2eF(u(¯θ))
∂s′∂s′′ 
us′′
s,a(bθk,h)−us′′
s,a(θ∗
h)
≤H
2X
s′∈Sh,s,aX
s′′∈Sh,s,aus′
s,a(bθk,h)−us′
s,a(θ∗
h)∂2F(u(¯θ))
∂s′∂s′′us′′
s,a(bθk,h)−us′′
s,a(θ∗
h)
where the inequality holds by V(s)∈[0, H],∀s.
According to Lemma 18, we have (omit the subscript Sh,s,a for simplicity):
H
2X
s′X
s′′us′
s,a(bθk,h)−us′
s,a(θ∗
h)∂2F(u(¯θ))
∂s′∂s′′us′′
s,a(bθk,h)−us′′
s,a(θ∗
h)
≤HX
s′X
s′′̸=s′us′
s,a(bθk,h)−us′
s,a(θ∗
h)ps′
s,a 
u(¯θ)
ps′′
s,a 
u(¯θ)us′′
s,a(bθk,h)−us′′
s,a(θ∗
h)
+3H
2X
s′ 
us′
s,a(bθk,h)−us′
s,a(θ∗
h)2ps′
s,a(u(¯θ)). (30)
To bound the first term, by applying the AM-GM inequality, we obtain
HX
s′X
s′′̸=s′us′
s,a(bθk,h)−us′
s,a(θ∗
h)ps′
s,a 
u(¯θ)
ps′′
s,a 
u(¯θ)us′′
s,a(bθk,h)−us′′
s,a(θ∗
h)
25≤HX
s′X
s′′us′
s,a(bθk,h)−us′
s,a(θ∗
h)ps′
s,a 
u(¯θ)
ps′′
s,a 
u(¯θ)us′′
s,a(bθk,h)−us′′
s,a(θ∗
h)
≤H
2X
s′X
s′′ 
us′
s,a(bθk,h)−us′
s,a(θ∗
h)2ps′
s,a 
u(¯θ)
ps′′
s,a 
u(¯θ)
+H
2X
s′X
s′′ 
us′′
s,a(bθk,h)−us′′
s,a(θ∗
h)2ps′
s,a 
u(¯θ)
ps′′
s,a 
u(¯θ)
≤HX
s′ 
us′
s,a(bθk,h)−us′
s,a(θ∗
h)2ps′
s,a 
u(¯θ)
(31)
Plugging (31) into (30), we have
H
2X
s′X
s′′us′
s,a(bθk,h)−us′
s,a(θ∗
h)∂2F(u(¯θ))
∂s′∂s′′us′′
s,a(bθk,h)−us′′
s,a(θ∗
h)
≤5H
2X
s′ 
us′
s,a(bθk,h)−us′
s,a(θ∗
h)2ps′
s,a(u(¯θ))
=5H
2X
s′ 
(ϕs′
s,a)⊤(bθk,h−θ∗
h)2ps′
s,a(u(¯θ))
≤5
2Heβ2
kmax
s′∥ϕs′
s,a∥2
H−1
k,h, (32)
where the last inequality holds by Lemma 3. Combining (29) and (32) finishes the proof. ■
G Proof of Theorem 2
Proof. Combining Lemma 4 and Lemma 10, we have
KX
k=1V∗
1(sk,1)−KX
k=1Vπk
1(sk,1)≤2KX
k=1HX
h=1(ϵfst
k,h+ϵsnd
k,h) +Hp
2KHlog(2 /δ)
where
ϵfst
k,h=HeβkX
s′∈Sk,hps′
k,h(eθk,h)ϕs′
k,h−X
s′′∈Sk,hps′′
k,h(eθk,h)ϕs′′
k,h
H−1
k,h,
ϵsnd
k,h=5
2Heβ2
kmax
s′∈Sk,h∥ϕs′
k,h∥2
H−1
k,h.
Next, we bound ϵfst
k,handϵsnd
k,hrespectively.
Bounding ϵfst
k,h.For simplicity, we denote
Eθ[ϕs′
k,h] =Es′∼ps
k,h(θ)[ϕs′
k,h],¯ϕs′
s,a=ϕs′
s,a−Eeθk,h[ϕs′
k,h],eϕs′
s,a=ϕs′
s,a−Eeθk+1,h[ϕs′
k,h]
Then, we have
X
s′∈Sk,hps′
k,h(eθk,h)ϕs′
k,h−X
s′′∈Sk,hps′′
k,h(eθk,h)ϕs′′
k,h
H−1
k,h=X
s′∈Sk,hps′
k,h(eθk,h)¯ϕs′
k,h
H−1
k,h
≤X
s′∈Sk,hps′
k,h(eθk,h)¯ϕs′
k,h−eϕs′
k,h
H−1
k,h+X
s′∈Sk,hps′
k,h(eθk,h)eϕs′
k,h
H−1
k,h
=X
s′∈Sk,hps′
k,h(eθk,h)¯ϕs′
k,h−eϕs′
k,h
H−1
k,h
| {z }
(c)+X
s′∈Sk,h(ps′
k,h(eθk,h)−ps′
k,h(eθk+1,h))eϕs′
k,h
H−1
k,h
| {z }
(d)
+X
s′∈Sk,hps′
k,h(eθk+1,h)eϕs′
k,h
H−1
k,h
| {z }
(e).
26We bound these terms separately in the following.
For the first term (c), we have
¯ϕs′
k,h−eϕs′
k,h
H−1
k,h
=X
s′′∈Sk,h
ps′′
k,h(eθk+1,h)−ps′′
k,h(eθk,h)
ϕs′′
k,h
H−1
k,h
=X
s′′∈Sk,h
∇ps′′
k,h(ξk,h)⊤(eθk+1,h−eθk,h)
ϕs′′
k,h
H−1
k,h
≤X
s′′∈Sk,h∇ps′′
k,h(ξk,h)⊤(eθk+1,h−eθk,h)ϕs′′
k,h
H−1
k,h
=X
s′′∈Sk,h
ps′′
k,h(ξk,h)ϕs′′
k,h−ps′′
k,h(ξk,h)X
s′′′∈Sk,hps′′′
k,h(ξk,h)ϕs′′′
k,h⊤
(eθk+1,h−eθk,h)ϕs′′
k,h
H−1
k,h
≤X
s′′∈Sk,hps′′
k,h(ξk,h) 
ϕs′′
k,h⊤(eθk+1,h−eθk,h)ϕs′′
k,h
H−1
k,h
+X
s′′∈Sk,hps′′
k,h(ξk,h)ϕs′′
k,h
H−1
k,hX
s′′′∈Sk,hps′′′
k,h(ξk,h)(ϕs′′′
k,h)⊤(eθk+1,h−eθk,h)
≤X
s′′∈Sk,hps′′
k,h(ξk,h)eθk+1,h−eθk,h
Hk,hϕs′′
k,h2
H−1
k,h
+X
s′′∈Sk,hps′′
k,h(ξk,h)ϕs′′
k,h
H−1
k,hX
s′′′∈Sk,hps′′′
k,h(ξk,h)ϕs′′′
k,h
H−1
k,heθk+1,h−eθk,h
Hk,h
≤2η√
λX
s′′∈Sk,hps′′
k,h(ξk,h)ϕs′′
k,h2
H−1
k,h+2η√
λX
s′′∈Sk,hps′′
k,h(ξk,h)ϕs′′
k,h
H−1
k,h2
≤4η√
λX
s′′∈Sk,hps′′
k,h(ξk,h)ϕs′′
k,h2
H−1
k,h
≤4η√
λmax
s′′∈Sk,hϕs′′
k,h2
H−1
k,h
where and the fifth inequality is by Cauchy-Schwarz inequality and the fourth inequality is because
by Lemma 17, since eHk,h⪰ H k,h⪰λId, we have
eθk+1,h−eθk,h
Hk,h≤eθk+1,h−eθk,heHk,h≤2η∥gk,h(eθk,h)∥eH−1
k,h≤2η√
λ∥gk,h(eθk,h)∥2,
and since gk,h(θ) =P
s′∈Sk,h(ps′
k,h(θ)−ys′
k,h)ϕs′
k,h, we have
∥gk,h(eθk,h)∥2≤X
s′∈Sk,hps′
k,h(eθk,h)ϕs′
k,h
2+X
s′∈Sk,hys′
k,hϕs′
k,h
2≤2 max
s′∈Sk,hϕs′
k,h
2≤2.
Therefore, we have
KX
k=1HX
h=1X
s′∈Sk,hps′
k,h(eθk,h)¯ϕs′
k,h−eϕs′
k,h
H−1
k,h≤4η√
λKX
k=1HX
h=1X
s′∈Sk,hps′
k,h(eθk,h) max
s′′∈Sk,hϕs′′
k,h2
H−1
k,h
≤4η√
λKX
k=1HX
h=1X
s′∈Sk,hmax
s′′∈Sk,hϕs′′
k,h2
H−1
k,h
≤8Hη
κ√
λdlog
1 +K
dλ
. (33)
where the last inequality holds by Lemma 6.
27For the term (d), by similar analysis, we have
(ps′
k,h(eθk,h)−ps′
k,h(eθk+1,h))eϕs′
k,h
H−1
k,h
=∇ps′
k,h(ξk,h)⊤(eθk,h−eθk+1,h)eϕs′
k,h
H−1
k,h
=
ps′
k,h(ξk,h)ϕs′
k,h−ps′
k,h(ξk,h)X
s′′∈Sk,hps′′
k,h(ξk,h)ϕs′′
k,h⊤
(eθk+1,h−eθk,h)eϕs′
k,h
H−1
k,h
≤2η√
λ
ps′
k,h(ξk,h)∥ϕs′
k,h∥H−1
k,heϕs′
k,h
H−1
k,h+ps′
k,h(ξk,h)eϕs′
k,h
H−1
k,hX
s′′∈Sk,hps′′
k,h(ξk,h)ϕs′′
k,h
H−1
k,h

≤2η√
λ
max
s′′∈Sk,h∥ϕs′′
k,h∥H−1
k,heϕs′′
k,h
H−1
k,h+ max
s′′∈Sk,heϕs′′
k,h
H−1
k,hmax
s′′′∈Sk,hϕs′′′
k,h
H−1
k,h
≤2η√
λmax
s′′∈Sk,hϕs′′
k,h2
H−1
k,h+eϕs′′
k,h2
H−1
k,h
2
+2η√
λ
max s′′∈Sk,heϕs′′
k,h
H−1
k,h2
+
max s′′′∈Sk,hϕs′′′
k,h
H−1
k,h2
2
≤4η√
λmax
max
s′′∈Sk,hϕs′′
k,h2
H−1
k,h,max
s′′∈Sk,heϕs′′
k,h2
H−1
k,h
,
where the third inequality holds by the AM-GM inequality. Thus, we have
KX
k=1HX
h=1X
s′∈Sk,h(ps′
k,h(eθk,h)−ps′
k,h(eθk+1,h))eϕs′
k,h
H−1
k,h
≤4η√
λKX
k=1HX
h=1max
max
s′′∈Sk,hϕs′′
k,h2
H−1
k,h,max
s′′∈Sk,heϕs′′
k,h2
H−1
k,h
≤8Hη
κ√
λdlog
1 +K
dλ
, (34)
where the last inequality holds by Lemma 6.
Finally, we bound the term (e)as follows.
KX
k=1X
s′∈Sk,hps′
k,h(eθk+1,h)eϕs′
k,h
H−1
k,h
≤vuutKX
k=1X
s′∈Sk,hps′
k,h(eθk+1,h)vuutKX
k=1X
s′∈Sk,hps′
k,h(eθk+1,h)eϕs′
k,h2
H−1
k,h
≤√
Ks
2dlog
1 +K
dλ
, (35)
where the first inequality holds by the Cauchy-Schwarz inequality and the last holds by Lemma 6.
Thus, combining (33), (34), and (35), we have
KX
k=1HX
h=1ϵfst
k,h=HeβKKX
k=1HX
h=1X
s′∈Sk,hps′
k,h(eθk,h)ϕs′
k,h−X
s′′∈Sk,hps′′
k,h(eθk,h)ϕs′′
k,h
H−1
k,h
≤H2eβK s
2dKlog
1 +K
dλ
+16η
κ√
λdlog(1 +K
dλ)!
(36)
28For the second-order term, by Lemma 6, we have
KX
k=1HX
h=1ϵsnd
k,h=5
2Heβ2
kKX
k=1HX
h=1max
s′∈Sk,h∥ϕs′
k,h∥2
H−1
k,h
≤5
κH2eβ2
Kdlog
1 +K
dλ
. (37)
where the last inequality holds by Lemma 6.
Combining (36) and (37), we have
Reg(K)≤2KX
k=1HX
h=1(ϵfst
k,h+ϵsnd
k,h) +Hp
2KHlog(2 /δ)
≤H2eβK s
2dKlog
1 +K
dλ
+16η
κ√
λdlog(1 +K
dλ)!
+5
κH2eβ2
Kdlog
1 +K
dλ
+Hr
2KHlog2
δ
≤eO 
dH2√
K+d2H2κ−1
This finishes the proof. ■
H Proof of Theorem 3
Proof. Our proof is similar to the proof of the lower bound for adversarial linear mixture MDPs with
the unknown transition in Zhao et al. [2023]. We prove this lemma by reducing the MNL mixture
MDP problem to a sequence of binary logistic bandit problems.
Specifically, we use each three layers to construct a block. Note that the third layer of block iis also
the first layer of block i+ 1and hence there are total H/2blocks. In each block, both the first and
third layers of this block only have one state, and the second layer has two states. Here we take block
ias an example. The first two layers of this block are associated with transition probability Pi,1and
Pi,2. Denote by si,1the only state in the first layer of this block. In the second layer of the block
i, we assume there exist two states s∗
i,2andsi,2. Let si,3be the only state in the third layer of this
block. Further, for any a∈ A, letϕ(si,1, a, s i,2) = 0 . The probability of transferring to state s∗
i,2
when executing action aat state si,1isρa, in particular,
Pi,1(s∗
i,2|si,1, a) =exp(ϕ(s∗
i,2|si,1, a)⊤θ∗
i,1)
1 + exp( ϕ(s∗
i,2|si,1, a))⊤θ∗
i,1)=ρa,
Pi,1(si,2|si,1, a) =1
1 + exp( ϕ(s∗
i,2|si,1, a)⊤θ∗
i,1)= 1−ρa.
For the second layer, the MDP instance satisfies that ∀s=s∗
i,2, si,2, anda∈ A,Pi,2(si,3|s, a) = 1 .
The reward function satisfies rk(si,1, a) = 0 for the first layer and rk(s∗
i,2, a) = 1 ,rk(si,2, a) = 0
for the second layer for all a∈ A.
Consider the logistic bandit problem where a learner selects action x∈Rdand receives a reward rk
sampled from Bernoulli distributed with mean
µ(x⊤θ∗) =1
1 + exp( x⊤θ∗).
By this configuration, we can see that learning in this block of MDP can be regarded as learning
ad-dimensional logistic bandit problem with Aarms, where the arm set is ϕ(s∗
i,2|si,1, a)and the
expected reward of each arm is ρa. It is also clear that the optimal policy at state si,1is to select
action a∗
i,1= arg maxa∈Aϕ(s∗
i,2|si,1, a). Thus, learning this MNL mixture MDP equals to learning
H/2logistic bandit problems. This finishes the proof. ■
29I Supporting Lemmas
In this section, we provide several supporting lemmas used in the proofs of the main results.
Lemma 15 (Abbasi-Yadkori et al. [2011, Theorem 1]) .Let{Ft}∞
t=0be a filtration. Let {ηt}∞
t=1be
a real-valued stochastic process such that ηtisFt-measurable and ηtis conditionally zero-mean
R-sub-Gaussian for some R≥0i.e.∀λ∈R,E
eληt|Ft−1
≤exp 
λ2R2/2
. Let{Xt}∞
t=1be an
Rd-valued stochastic process such that XtisFt−1-measurable. Assume that Vis ad×dpositive
definite matrix. For any t≥0, define
Vt=V+t−1X
s=1XsX⊤
sSt=t−1X
s=1ηsXs.
Then, for any δ >0, with probability at least 1−δ, for all t≥0,
∥St∥2
V−1
t≤2R2log 
det (Vt)1/2det(V)−1/2
δ!
.
Lemma 16 (Abbasi-Yadkori et al. [2011, Lemma 10]) .Suppose x1, . . . , x t∈Rdand for any
1≤s≤t,∥xs∥2≤L. LetVt=λId+Pt
s=1xsx⊤
sforλ≥0. Then, for any 1≤s≤t, we have
det(Vt)≤
λ+tL2
dd
.
Lemma 17 (Orabona [2019, Lemma 6.9]) .LetZbe a positive define matrix and Xbe a convex set,
ex= arg min
x∈X⟨g,x⟩+1
2η∥x−bx∥2
Z.
Then, for all x∗∈ X, we have
∥ex−bx∥Z≤2η∥g∥Z−1.
and
⟨g,bx−x∗⟩ ≤η
2∥g∥2
Z−1+1
2η∥bx−x∗∥2
Z−1
2η∥ex−x∗∥2
Z.
Lemma 18 (Lee and Oh [2024, Lemma D.3]) .Define Q:RK→R, such that for any u=
(u1, . . . , u K)∈RK, Q(u) =PK
i=1exp(ui)
v+PK
k=1exp(uk). Let pi(u) =exp(ui)
v+PK
k=1exp(uk). Then, for all
i∈[K], we have
∂2Q
∂i∂j⩽3pi(u) ifi=j,
2pi(u)pj(u)ifi̸=j.
30NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the limitations of this work in the conclusion section.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate “Limitations” section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
31Justification: We present the assumption in Assumption 1 and provide detailed proofs for all
theoretical results in the appendices.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: This is a theoretical paper and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
32Answer: [NA]
Justification: This is a theoretical paper and does not include experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: This is a theoretical paper and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: This is a theoretical paper and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
33•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: This is a theoretical paper and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: This paper adheres fully to the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This is a theoretical paper and there is no societal impact of the work performed.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
34generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This is a theoretical paper and does not release any data or models that have a
high risk for misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: This is a theoretical paper and does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
35Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This is a theoretical paper and does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
36