Neural Synaptic Balance
Anonymous Author(s)
Affiliation
Address
email
Abstract
For a given additive cost function R(regularizer), a neuron is said to be in balance 1
if the total cost of its input weights is equal to the total cost of its output weights. 2
The basic example is provided by feedforward layered networks of ReLU units 3
trained with L2regularizers, which exhibit balance after proper training. We 4
develop a general theory that extends this phenomenon in three broad directions 5
in terms of: (1) activation functions; (2) regularizers, including all Lp(p >0) 6
regularizers; and (3) architectures (non-layered, recurrent, convolutional, mixed 7
activations). Gradient descent on the error function alone does not converge in 8
general to a balanced state where every neuron is in balance, even when starting 9
from a balanced state. However, gradient descent on the regularized error function 10
must converge to a balanced state, and thus network balance can be used to assess 11
learning progress. The theory is based on two local neuronal operations: scaling 12
which is commutative, and balancing which is not commutative. Finally, and most 13
importantly, given any initial set of weights, when local balancing operations are 14
applied to each neuron in a stochastic manner, global order always emerges through 15
the convergence of the stochastic algorithm to the same unique set of balanced 16
weights. The reason for this convergence is the existence of an underlying strictly 17
convex optimization problem where the relevant variables are constrained to a 18
linear, only architecture-dependent, manifold. The theory is corroborated through 19
simulations carried out on benchmark data sets. Balancing operations are entirely 20
local and thus physically plausible in biological and neuromorphic networks. 21
1 Introduction 22
When large neural networks are trained on complex tasks, they produce large arrays of synaptic 23
weights that have no clear structure and are difficult to interpret. Thus finding any kind of structure in 24
the weights of large neural networks is of great interest. Here we study a particular kind of structure 25
we call neural synaptic balance and the conditions under which it emerges. Neural synaptic balance 26
is different from the biological notion of balance between excitation and inhibition [Froemke, 2015, 27
Field et al., 2020, Howes and Shatalina, 2022, Kim and Lee, 2022, Shirani and Choi, 2023]. We 28
use this term to refer to any systematic relationship between the input and output synaptic weights 29
of individual neurons or layers of neurons. Here we consider the case where the cost of the input 30
weights is equal to the cost of the output weights, where the cost is defined by some regularizer. One 31
of the most basic examples of such a relationship is when the sum of the squares of the input weights 32
of a neuron is equal to the sum of the squares of its output weights. 33
Basic Example: The basic example where this happens is with a neuron with a ReLU activation 34
function inside a network trained to minimize an error function with L2regularization. If we multiply 35
the incoming weights of the neuron by some λ >0(including the bias) and divide the outgoing 36
weights of the neuron by the same λ, it is easy to see that this scaling operation does not affect in any 37
way the contribution of the neuron to the rest of the network. Thus, the component of the overall 38
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.error function that depends only on the input-output function of the network is unchanged. However, 39
the value of the L2regularizer changes with λand we can ask what is the value of λthat minimizes 40
the corresponding contribution given by: 41
X
i∈IN(λwi)2+X
i∈OUT(wi/λ)2=λ2A+1
λ2B (1.1)
where INandOUT denote the set of incoming and outgoing weights respectively, A=P
i∈INw2
i, 42
andB=P
i∈OUTw2
i. The product of the two terms on the right-hand side of Equation 1.1 is equal to 43
ABand does not depend on λ. Thus, the minimum is achieved when these two terms are equal, which 44
yields: (λ∗)4=B/A for the optimal λ∗. The corresponding new set of weights, vi=λ∗wifor the 45
input weights and vi=wi/λ∗for the outgoing weights, must be balanced:P
i∈INv2
i=P
i∈OUTv2
i. 46
This is because its optimal scaling factor can only be λ∗= 1. Thus, we can define two operations 47
that can be applied to the incoming and outgoing weights of a neuron: scaling and balancing. It 48
is easy to check that scaling operations applied to any two neurons commute, whereas balancing 49
operations do not commute if the two neurons are directly connected (Appendix). If a network of 50
ReLU neurons is properly trained using a standard error function with an L2regularizer, at the end of 51
training one observes a remarkable phenomenon: for each ReLU neuron, the norm of the incoming 52
synaptic weights is approximately equal to the norm of the outgoing synaptic weights, i.e. every 53
neuron is balanced. 54
There have been isolated previous studies of this kind of synaptic balance [Du et al., 2018, Stock 55
et al., 2022] under special conditions. For instance, in Du et al. [2018], it is shown that if a deep 56
network is initialized in a balanced state with respect to the sum of squares metric, and if training 57
progresses with an infinitesimal learning rate, then balance is preserved throughout training. Here, 58
we take a different approach aimed at uncovering the generality of neuronal balance phenomena, 59
the learning conditions under which they occur, as well as new local balancing algorithms and their 60
convergence properties. We study neural synaptic balance in its generality in terms of activation 61
functions, regularizers, network architectures, and training stages. In particular, we systematically 62
answer questions such as: Why does balance occur? Does it occur only with ReLU neurons? Does it 63
occur only with L2regularizers? Does it occur only in fully connected feedforward architectures? 64
Does it occur only at the end of training? And what happens if we balance neurons at random in a 65
large network? 66
2 Generalization of the Activation Functions 67
What enables scaling ReLU neurons without changing their input-output function is the homogeneous 68
property of ReLU activation function. An activation function fis said to be homogeneous if for every 69
λ >0,f(λx) =λf(x). To fully characterize the class of homogeneous activation functions, we first 70
define a new class of activation functions, corresponding to bilinear units (BiLU), consisting of two 71
half-lines meeting at the origin. 72
Definition 2.1. (BiLU) A neuronal activation function f:R→Ris bilinear (BiLU) if and only if 73
f(x) =axwhen x <0, and f(x) =bxwhen x≥0, for some fixed parameters aandbinR. 74
BiLU units include linear units ( a=b), ReLU units ( a= 0, b= 1), leaky ReLU ( a=ϵ;b= 1) units, 75
and symmetric linear units ( a=−b), all of which can also be viewed as special cases of piece-wise 76
linear units [Tavakoli et al., 2021], with a single hinge. One advantage of ReLU and more generally 77
BiLU neurons, which is very important during backpropagation learning, is that their derivative is 78
very simple and can only take one of two values ( aorb). We have the following equivalence. 79
Proposition 2.2. A neuronal activation function f:R→Ris homogeneous if and only if it is a 80
BiLU activation function. 81
Proof. Every function in BiLU is clearly homogeneous. Conversely, any homogeneous function f 82
must satisfy: (1) f(0x) = 0 f(x) =f(0) = 0 ; (2)f(x) =f(1x) =f(1)xfor any positive x; and (3) 83
f(x) =f(−u) =f(−1)u=−f(−1)xfor any negative x. Thus fis in BiLU with a=−f(−1) 84
andb=f(1). 85
In the Appendix, we provide a simple proof that networks of BiLU neurons, even with a single hidden 86
layer, have universal approximation properties. 87
2While in the rest of this work we use BiLU neurons, it is possible to generalize the notions of scaling 88
and balancing even further. To see this, suppose that there is a neuron with an activation function 89
f:R→R, and functions g: (a, b)→Randh: (a, b)→R, such that: f(g(λ)x) =h(λ)f(x), 90
for any λ∈(a, b). Then if we multiply the incoming weights by g(λ)and divide the outgoing 91
weights by h(λ)̸= 0 (generalized scaling), we see again that the influence of the neuron on the 92
rest of the network is unchanged. And thus, again, we can try to find the value of λthat minimizes 93
the regularization cost (generalized balancing). Here we provide an example of such an activation 94
function, with g(λ) =λandh(λ) =λc. Additional details are given in the Appendix. 95
Proposition 2.3. The set of activation functions fsatisfying f(λx) =λcf(x)for any x∈Rand 96
anyλ >0consist of the functions of the form: 97
f(x) =Cxcifx≥0
Dxcifx <0.(2.1)
where c∈R,C=f(1)∈R, and D=f(−1)∈R. We call these bi-power units (BiPU). If, in 98
addition, we want fto be continuous at 0, we must have either c >0, orc= 0withC=D. 99
Note that in the general case where c >0,CandDdo not need to be equal. In particular, one of 100
them can be equal to zero, and the other one can be different from zero giving rise to rectified power 101
units. 102
3 Generalization of the Regularizers 103
As we have seen, given a BiLU neuron, scaling its input and output weights by λand1/λrespectively 104
does not alter its contribution to the rest of the network and thus we can adjust λto reduce or even 105
minimize the contribution of the corresponding weights to the regularizer. It is reasonable to assume 106
that the regularizer has the general additive form: R(W) =P
wgw(w)where Wdenotes all the 107
weights in the network. Without much loss of generality, we can assume that the gware continuous, 108
and lower-bounded by 0. To ensure the existence and uniqueness of a minimum during the balancing 109
of any neuron, We will assume that each function gwdepends only on the magnitude |w|of the 110
corresponding weight, and that gwmonotonically increases from 0 to +∞. Clearly, L2, L1and 111
more generally all Lpregularizers are special cases where, for p >0,Lpregularization is defined 112
by:R(W) =P
w|w|p. Differentiability conditions can be added to be able to derive closed form 113
solutions for the balance (optimal scaling). This is satisfied by all forms of Lpregularization, for 114
p >0. We have the following theorem. 115
Theorem 3.1. (Balance and Regularizer Minimization) Assume an additive regularizer with the 116
properties described above, where in addition we assume that the functions gware continuously 117
differentiable, except perhaps at the origin. Then, for any neuron, there exists one optimal value λ∗118
that minimizes R(W). This value must be a solution of the consistency equation: 119
λ2X
w∈IN(i)wg′
w(λw) =X
w∈OUT (i)wg′
w(w/λ) (3.1)
Once the weights are rebalanced accordingly, the new weights must satisfy the generalized balance 120
equation: 121
X
w∈IN(i)wg′(w) =X
w∈OUT (i)wg′(w) (3.2)
In particular, if gw(w) =|w|pfor all the incoming and outgoing weights of neuron i, then the optimal 122
value λ∗is unique and equal to: 123
λ∗=P
w∈OUT (i)|w|p
P
w∈IN(i)|w|p1/2p
=||OUT (i)||p
||IN(i)||p1/2
(3.3)
After balancing, the decrease ∆R≥0in the value of the Lpregularizer R=P
w|w|pis given by: 124
3∆R= X
w∈IN(i)|w|p1/2− X
w∈OUT (i)|w|p1/22
(3.4)
After balancing neuron i, its new weights satisfy the generalized Lpbalance equation: 125
X
w∈IN(i)|w|p=X
w∈OUT (i)|w|p(3.5)
Proof. The results are obtained by setting the derivative of the regularizer with respect to the scaling 126
factor λto 0. Note that the theorem applies to regularizers combining different Lp’s (e.g. of the form 127
$alphaL 2+βL1). The details are given in the Appendix. 128
4 Generalization of the Architectures 129
It is straightforward to check that the scaling and balancing operations can be extended in the 130
following cases (see Appendix for additional details): 131
1.Mixed networks containing both BiLU and non-BiLU units. One can just restrict those 132
operations to the BiLU neurons. 133
2. Recurrent networks containing BiLU neurons, not just feedforward networks. 134
3. Networks that are not layered, or not fully connected. 135
4.In addition, scaling and balancing operations can be applied layer-wise to an entire layer of 136
BiLU neurons in a tied manner, by using the same scaling factor λwith a single optimal 137
value λ∗for all the neurons in the layer. In particular, this allows the application of scaling 138
and balancing to convolutional layers of BiLU neurons. 139
5 Balancing Algorithms 140
Gradient Descent: When a network of BiLU neurons is trained by gradient descent to minimize 141
an error function E(W), such as the negative log-likelihood of the data, there is no reason for the 142
final weights to be balanced. However, when a network is properly trained to minimize a regularized 143
error function E=E(W) +R(W), the final weights ought to be balanced. The reason is that if a 144
neuron is not in a balanced state at the end of training, then we can further reduce its contribution to 145
Rsmoothly by balancing it. This implies that the gradient of E(W)is not equal to zero at the end of 146
training, and thus training has not properly converged. The converse is that the degree of balance can 147
be used as a proxy for assessing whether learning has converged or not. 148
Stochastic Balancing: More interestingly, we now investigate what happens if we fix the weights W 149
of a network and iteratively balance its BiLU neurons. 150
Theorem 5.1. (Convergence of Stochastic Balancing) Consider a network of BiLU neurons with 151
an error function E(W) =E(W) +R(W)where Ris any Lp(p >0) regularizer. Let Wdenote 152
the initial weights. When the neuronal stochastic balancing algorithm is applied throughout the 153
network so that every neuron is visited from time to time, then E(W)remains unchanged but R(W) 154
must converge to some finite value that is less or equal to the initial value, strictly less if the initial 155
weights are not balanced. In addition, for every neuron i,λ∗
i(t)→1and the weights themselves must 156
converge to a limit W∗which is globally balanced, with E(W) =E(W∗)andR(W)≥R(W∗), 157
and with equality if only if Wis already balanced. Finally, W∗is unique as it corresponds to the 158
solution of a strictly convex optimization problem with special linear constraints that depend only on 159
the network architecture (and not on W). Stochastic balancing projects to stochastic trajectories in 160
the linear manifold that run from the origin to the unique optimal configuration. 161
Proof. Each individual balancing operation leaves E(W)unchanged because the BiLU neurons are 162
homogeneous. Furthermore, each balancing operation reduces the regularization error R(W), or 163
leaves it unchanged. Since the regularizer is lower-bounded by zero, the value of the regularizer must 164
approach a limit as the stochastic updates are being applied. However, this alone does not imply 165
4Figure 1: Two hidden units (1 and 7) connected by two different directed paths 1-2-3-4-7 and 1-5-6-7 in a
BiLU network. Each unit ihas a scaling factor Λi, and each directed edge from unit jto unit ihas a scaling
factor Mij= Λi/Λj. The products of the Mij’s along each path is equal to:Λ2
Λ1Λ3
Λ2Λ4
Λ3Λ7
Λ4=Λ5
Λ1Λ6
Λ5Λ7
Λ6=Λ7
Λ1.
Therefore the variables Lij= log Mijmust satisfy the linear equation: L21+L32+L43+L74=L51+
L65+L76=log Λ 7−log Λ 1.
that the weights are converging and whether the limit is unique or not. To address these issues, for 166
simplicity, we use a continuous time notation. After a certain time teach neuron has been balanced a 167
certain number of times. While the balancing operations are not commutative as balancing operations, 168
they are commutative as scaling operations. Thus we can reorder the scaling operations and group 169
them neuron by neuron so that, for instance, neuron ihas been scaled by the sequence of scaling 170
operations of the form: 171
Sλ∗
1(i)Sλ∗
2(i). . . S λ∗nit(i) =SΛi(t)(i) (5.1)
where nitcorresponds to the count of the last update of neuron iprior to time t, and: 172
Λi(t) =Y
1≤n≤nitλ∗
n(i) (5.2)
For the input and output units, we can consider that their balancing coefficients λ∗are always equal 173
to 1 (at all times) and therefore Λi(t) = 1 for any visible unit i. At time tthe weight connecting unit 174
jto unit iis given by: wij(t) =wij(0)Λ i(t)/Λj(t), where wij(0)corresponds to the initial value. 175
In the Appendix, we show upfront that for all BiLU units i,Λi(t)converges to some limit Λi>0, 176
and thus the weights converge too. Here, we first suppose that the coefficients Λi(t)converge to 177
some limit Λi, and recover the convergence at the end from understanding the overall proof. As a 178
result, for any Lpregularizer, the coefficients Λicorresponding to a globally balanced state must be 179
solutions of the following optimization problem: 180
min
ΛR(Λ) =X
ij|Λi
Λjwij|p(5.3)
under the simple constraints: Λi>0for all the BiLU hidden units, and Λi= 1for all the visible (input 181
and output) units. In this form, the problem is not convex. Introducing new variables Mj= 1/Λj 182
is not sufficient to render the problem convex. Using variables Mij= Λ i/Λjis better, but still 183
problematic for 0< p≤1. However, let us instead introduce the new variables Lij= log(Λ i/Λj). 184
These are well defined since we know that Λi/Λj>0. The objective now becomes: 185
minR(L) =X
ij|eLijwij|p=X
ijepLij|wij|p(5.4)
This objective is strictly convex in the variables Lij, as a sum of strictly convex functions (exponen- 186
tials). However, to show that it is a convex optimization problem we need to study the constraints 187
on the variables Lij. In particular, from the set of Λi’s it is easy to construct a unique set of Lij. 188
However what about the converse? 189
Definition 5.2. A set of real numbers Lij, one per connection of a given neural architecture, is 190
self-consistent if and only if there is a unique corresponding set of numbers Λi>0(one per unit) 191
such that: Λi= 1for all visible units and Lij= log Λ i/Λjfor every directed connection from a unit 192
jto a unit i. 193
5CBAFigure 2: The problem of minimizing the strictly con-
vex regularizer R(Lij) =P
ijepLij|wij|p(p >0), over
the linear (hence convex) manifold of self-consistent con-
figurations defined by the linear constraints of the formP
πLij= 0, where πruns over input-output paths. The
regularizer function depends on the weights. The linear
manifold depends only on the architecture, i.e., the graph
of connections. This is a strictly convex optimization prob-
lem with a unique solution associated with the point A. At
Athe corresponding weights must be balanced, or else a
self-consistent configuration of lower cost could be found
by balancing any non-balanced neuron. Finally, any other
self-consistent configuration Bcannot correspond to a bal-
anced state of the network, since there must exist balancing
moves that further reduce the regularizer cost (see main
text). Stochastic balancing produces random paths from the
origin, where Lij=logMij= 0, to the unique optimum
point A.
Remark 5.3.This definition depends on the graph of connections, but not on the original values of 194
the synaptic weights. Every balanced state is associated with a self-consistent set of Lij, but not 195
every self-consistent set of Lijis associated with a balanced state. 196
Proposition 5.4. A set Lijassociated with a neural architecture is self-consistent if and only if 197P
πLij= 0where πis any directed path connecting an input unit to an output unit or any directed 198
cycle (for recurrent networks). 199
Proof. If we look at any directed path πfrom unit ito unit j, it is easy to see that we must have: 200
X
πLkl= log Λ i−log Λ j (5.5)
This is illustrated in Figure 1. Thus along any directed path that connects any input unit to any output 201
unit, we must haveP
πLij= 0. In addition, for recurrent neural networks, if πis a directed cycle 202
we must also have:P
πLij= 0. Thus in short we only need to add linear constraints of the form: 203P
πLij= 0. Any unit is situated on a path from an input unit to an output unit. Along that path, it is 204
easy to assign a value Λito each unit by simple propagation starting from the input unit which has a 205
multiplier equal to 1. When the propagation terminates in the output unit, it terminates consistently 206
because the output unit has a multiplier equal to 1 and, by assumption, the sum of the multipliers 207
along the path must be zero. So we can derive scaling values Λifrom the variables Lij. Finally, it is 208
easy to show that there are no clashes, i.e. that it is not possible for two different propagation paths to 209
assign different multiplier values to the same unit i(see Appendix). 210
Remark 5.5.Thus the constraints associated with being a self-consistent configuration of Lij’ s are 211
all linear. This linear manifold of constraints depends only on the architecture, i.e., the graph of 212
connections. The strictly convex function R(Lij)depends on the actual weights W. Different sets of 213
weights Wproduce different convex functions over the same linear manifold. 214
Remark 5.6.One could coalesce all the input units and all output units into a single unit, in which 215
case a path from an input unit to and output unit becomes also a directed cycle. In this representation, 216
the constraints are that the sum of the Lijmust be zero along any directed cycle. In general, it is not 217
necessary to write a constraint for every path from input units to output units. It is sufficient to select 218
a representative set of paths such that every unit appears in at least one path. 219
We can now complete the proof of Theorem 5.1. Given a neural network of BiLUs with a set 220
of weights W, we can consider the problem of minimizing the regularizer R(Lij)over the self- 221
admissible configuration Lij. For any p >0, theLpregularizer is strictly convex and the space of 222
self-admissible configurations is linear and hence convex. Thus this is a strictly convex optimization 223
6A
B
CD
FE
Figure 3: SGD applied to Ealone,
in general, does not converge to
a balanced state, but SGD ap-
plied to E+Rconverges to a bal-
anced state. (A-C) Simulations use
a deep fully connected autoencoder
trained on the MNIST dataset. (D-
F)Simulations use a deep locally
connected network trained on the
CFAR10 dataset. (A,D) Regulariza-
tion leads to neural balance. (B,E)
The training loss decreases and con-
verges during training (these panels
are not meant for assessing the qual-
ity of learning when using a regu-
larizer). (C,F) Using weight reg-
ularization decreases the norm of
weights. (A-F) Shaded areas corre-
spond to one s.t.d around the mean
(in some cases the s.t.d. is small and
the shaded area is not visible).
problem that has a unique solution (Figure 2). Note that the minimization is carried over self- 224
consistent configurations, which in general are not associated with balanced states. However, the 225
configuration of the weights associated with the optimum set of Lij(point Ain Figure 2) must be 226
balanced. To see this, imagine that one of the BiLU units–unit iin the network is not balanced. Then 227
we can balance it using a multiplier λ∗
iand replace ΛibyΛ′
i= Λiλ∗. It is easy to check that the new 228
configuration including Λ′
iis self-consistent. Thus, by balancing unit i, we are able to reach a new 229
self-consistent configuration with a lower value of Rwhich contradicts the fact that we are at the 230
global minimum of the strictly convex optimization problem. 231
We know that the stochastic balancing algorithm always converges to a balanced state. We need to 232
show that it cannot converge to any other balanced state, and in fact that the global optimum is the 233
only balanced state. By contradiction, suppose it converges to a different balanced state associated 234
with the coordinates (LB
ij)(point Bin Figure 2). Because of the self-consistency, this point is also 235
associated with a unique set of (ΛB
i)coordinates. The cost function is continuous and differentiable 236
in both the Lij’s and the Λi’s coordinates. If we look at the negative gradient of the regularizer, it 237
is non-zero and therefore it must have at least one non-zero component ∂R/∂ Λialong one of the 238
Λicoordinates. This implies that by scaling the corresponding unit iin the network, the regularizer 239
can be further reduced, and by balancing unit ithe balancing algorithm will reach a new point ( Cin 240
Figure 2) with lower regularizer cost. This contradicts the assumption that Bwas associated with a 241
balanced stated. Thus, given an initial set of weights W, the stochastic balancing algorithm must 242
always converge to the same and unique optimal balanced state W∗associated with the self-consistent 243
point A. A particular stochastic schedule corresponds to a random path within the linear manifold 244
from the origin (at time zero, all the multipliers are equal to 1, and therefore Mij= 1andLij= 0 245
for any iand any j) to the unique optimum point A. 246
247
Remark 5.7.From the proof, it is clear that the same result holds also for any deterministic balancing 248
schedule, as well as for tied and non-tied subset balancing, e.g., for layer-wise balancing and tied 249
layer-wise balancing. In the Appendix, we provide an analytical solution for the case of tied layer-wise 250
balancing in a layered feed-forward network. 251
Remark 5.8.From the proof, it is also clear that the same convergence to the unique global optimum 252
is observed if each neuron, when stochastically visited, is favorably scaled rather than balanced, i.e., 253
it is scaled with a factor that reduces Rbut not necessarily minimizes R. Stochastic balancing can 254
also be viewed as a form of EM algorithm where the E and M steps can be taken fully or partially. 255
7A
BD
CE
F
Figure 4: Even if the starting state
is balanced, SGD does not pre-
serve the balance unless the learn-
ing rate is infinitely small. (A-C)
Simulations use a deep fully con-
nected autoencoder trained on the
MNIST dataset. (D-F) Simulations
use a deep locally connected net-
work trained on the CFAR10 dataset.
(A-F) The initial weights are bal-
anced using the stochastic balanc-
ing algorithm. Then the network is
trained by SGD. (A,D) When the
learning rate (lr) is relatively large,
without regularization, the initial
balance of the network is rapidly dis-
rupted. (B,E) The training loss de-
creases and converges during train-
ing (these panels are not meant for
assessing the quality of learning
when using a regularizer). (C,F) Us-
ing weight regularization decreases
the norm of the weights. (A-F)
Shaded areas correspond to one s.t.d
around the mean (in some cases the
s.t.d. is small and the shaded area is
not visible).
6 Simulations 256
To further corroborate the results, we ran multiple experiments. Here we report the results from two 257
series of experiments. The first one is conducted using a six-layer, fully connected, autoencoder 258
trained on MNIST [Deng, 2012] for a reconstruction task with ReLU activation functions in all layers 259
and the sum of squares errors loss function. The number of neurons in consecutive layers, from 260
input to output, is 784, 200, 100, 50, 100, 200, 784. Stochastic gradient descent (SGD) learning by 261
backpropagation is used for learning with a batch size of 200. 262
The second one is conducted using three locally connected layers followed by three fully connected 263
layers trained on CFAR10 [Krizhevsky and Hinton, 2009] for a classification task with leaky ReLU 264
activation functions in the hidden layers, a softmax output layer, and the cross entropy loss function. 265
The number of neurons in consecutive layers, from input to output, is 3072, 5000, 2592, 1296, 300, 266
100, 10. Stochastic gradient descent (SGD) learning by backpropagation is used for learning with a 267
batch size of 5. 268
In all the simulation figures (Figures 3, 4, and 5) the left column presents results obtained from the 269
first experiment, while the right column presents results obtained from the second experiment. While 270
we used both L1andL2regularizers in the experiments, in the figures we report the results obtained 271
with the L2regularizer, which is the most widely used regularizer. In Figures 3 and 4, training is 272
done using batch gradient descent on the MNIST and CIFAR data. The balance deficit for a single 273
neuron iis defined as: P
w∈IN(i)w2−P
w∈OUT (i)w22, and the overall balance deficit is defined 274
as the sum of these single-neuron balance deficits across all the hidden neurons in the network. The 275
overall deficit is zero if and only if each neuron is in balance. In all the figures, ||W||Fdenotes the 276
Frobenius norm of the weights. 277
Figure 3 shows that learning by gradient descent with a L2regularizer results in a balanced state. 278
Figure 4 shows that even when the network is initialized in a balanced state, without the regularizer 279
the network can become unbalanced if the fixed learning rate is not very small. Figure 5 shows that 280
the local stochastic balancing algorithm, by which neurons are randomly balanced in an asynchronous 281
fashion, always converges to the same (unique) global balanced state. 282
8A C
B DFigure 5: Stochastic balancing converges to
a unique global balanced state (A-B) Sim-
ulations use a deep fully connected autoen-
coder trained on the MNIST dataset. (C-D)
Simulations use a deep locally connected net-
work trained on the CFAR10 dataset. (A,C)
The weights of the network are initialized
randomly and saved. The stochastic balanc-
ing algorithm is applied and the resulting
balanced weights are denoted by Wbalanced .
The stochastic balancing algorithm is ap-
plied 1,000 different times. In all repeti-
tions, the weights converge to the same value
Wbalanced .(B,D) Stochastic balancing de-
creases the norm of the weights. (A-D)
Shaded areas correspond to one standard de-
viation around the mean.
7 Conclusion 283
While the theory of neural synaptic balance is a mathematical theory that stands on its own, it is 284
worth considering some of its possible consequences and applications, at the theoretical, algorithmic, 285
biological, and neuromorphic hardware levels. At the theory level, for instance, it suggests extending 286
theorems obtained with ReLU neurons to BiLU neurons, using balance ideas to study learning in 287
linear regularized networks, and using the manifolds of equivalent weights to study issues of over- 288
parameterization (e.g. the data needs only to specify the balanced state, not the entire equivalence 289
class). At the algorithmic level, balancing algorithms could be used for instance to balance networks 290
at any stage of learning, including at the beginning, and as an alternative way to regularize networks. 291
Finally, because scaling and balancing are local operations, they are potentially of interest in physical, 292
as opposed to digitally-simulated, neural networks. In particular, it would be interesting to know if 293
some notion of balance applies to biological neurons. Unfortunately, current recording technologies 294
do not allow the measurement of all incoming and outgoing synapses of a neuron. Perhaps some 295
approximation could be obtained statistically and at the population level, or perhaps approximate 296
measurements could be carried in very simple networks (e.g. C. elegans )or using neurons in culture. 297
Finally, in neuromorphic hardware, the balance could be relevant for training spiking neural networks 298
with low energy consumption [Sorbaro et al., 2020, Rueckauer et al., 2017]). In particular, ReLU 299
scaling can influence the number of spikes generated in each layer and the average energy consumption 300
at each layer. Similarly, in memristor networks [Ivanov et al., 2022, Liang and Wong, 2000] ), L2 301
minimization is directly connected to power consumption. Moreover, the issue of the limited 302
conductivity range of memristors is mentioned in Ivanov et al. [2022] and in Ji et al. [2016] Therefore, 303
a local algorithm to reduce the norm of the weights could help mitigate this issue as well. 304
The theory of neural synaptic balance explains some basic findings regarding L2balance in feedfor- 305
ward networks of ReLU neurons and extends them in several directions. The first direction is the 306
extension to BiLU and other activation functions (BiPU). The second direction is the extension to 307
more general regularizers, including all Lp(p >0) regularizers. The third direction is the extension to 308
non-layered architectures, recurrent architectures, convolutional architectures, as well as architectures 309
with mixed activation functions. The theory is based on two local neuronal operations: scaling 310
which is commutative, and balancing which is not commutative. Finally, and most importantly, given 311
any initial set of weights, when local balancing operations are applied in a stochastic or determin- 312
istic manner, global order always emerges through the convergence of the balancing algorithm to 313
the same unique set of balanced weights. The reason for this convergence is the existence of an 314
underlying convex optimization problem where the relevant variables are constrained to a linear, 315
only architecture-dependent, manifold. Scaling and balancing operations are local and thus may 316
have applications in physical, non-digitally simulated, neural networks where the emergence of 317
global order from local operations may lead to better operating characteristics and lower energy 318
consumption. 319
9References 320
P. Baldi. Deep Learning in Science . Cambridge University Press, Cambridge, UK, 2021. 321
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal 322
Processing Magazine , 29(6):141–142, 2012. 323
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous 324
models: Layers are automatically balanced. Advances in Neural Information Processing Systems , 325
31, 2018. 326
Rachel E Field, James A D’amour, Robin Tremblay, Christoph Miehl, Bernardo Rudy, Julijana 327
Gjorgjieva, and Robert C Froemke. Heterosynaptic plasticity determines the set point for cortical 328
excitatory-inhibitory balance. Neuron , 106(5):842–854, 2020. 329
Robert C Froemke. Plasticity of cortical excitatory-inhibitory balance. Annual review of neuroscience , 330
38:195–219, 2015. 331
Oliver D Howes and Ekaterina Shatalina. Integrating the neurodevelopmental and dopamine hypothe- 332
ses of schizophrenia and the role of cortical excitation-inhibition balance. Biological psychiatry , 333
2022. 334
Dmitry Ivanov, Aleksandr Chezhegov, Mikhail Kiselev, Andrey Grunin, and Denis Larionov. Neuro- 335
morphic artificial intelligence systems. Frontiers in Neuroscience , 16:1513, 2022. 336
Yu Ji, YouHui Zhang, ShuangChen Li, Ping Chi, CiHang Jiang, Peng Qu, Yuan Xie, and WenGuang 337
Chen. Neutrams: Neural network transformation and co-design under neuromorphic hardware 338
constraints. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture 339
(MICRO) , pages 1–13. IEEE, 2016. 340
Dongshin Kim and Jang-Sik Lee. Neurotransmitter-induced excitatory and inhibitory functions in 341
artificial synapses. Advanced Functional Materials , 32(21):2200497, 2022. 342
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. 343
Faming Liang and Wing Hung Wong. Evolutionary monte carlo: Applications to cp model sampling 344
and change point problem. STATISTICA SINICA , 10:317–342, 2000. 345
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Data-dependent path 346
normalization in neural networks. arXiv preprint arXiv:1511.06747 , 2015. 347
Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conver- 348
sion of continuous-valued deep networks to efficient event-driven networks for image classification. 349
Frontiers in neuroscience , 11:294078, 2017. 350
Farshad Shirani and Hannah Choi. On the physiological and structural contributors to the dynamic 351
balance of excitation and inhibition in local cortical networks. bioRxiv , pages 2023–01, 2023. 352
Martino Sorbaro, Qian Liu, Massimo Bortone, and Sadique Sheik. Optimizing the energy consump- 353
tion of spiking neural networks for neuromorphic applications. Frontiers in neuroscience , 14:662, 354
2020. 355
Christopher H Stock, Sarah E Harvey, Samuel A Ocko, and Surya Ganguli. Synaptic balancing: A 356
biologically plausible local learning rule that provably increases neural network noise robustness 357
without sacrificing task performance. PLOS Computational Biology , 18(9):e1010418, 2022. 358
A. Tavakoli, F. Agostinelli, and P. Baldi. SPLASH: Learnable activation functions for improving 359
accuracy and adversarial robustness. Neural Networks , 140:1–12, 2021. Also: arXiv:2006.08947. 360
10Appendix 361
A Homogeneous and BiLU Activation Functions 362
In this section, we generalize the basic example of the introduction from the standpoint of the 363
activation functions. In particular, we consider homogeneous activation functions (defined below). 364
The importance of homogeneity has been previously identified in somewhat different contexts 365
Neyshabur et al. [2015]. Intuitively, homogeneity is a form of linearity with respect to weight scaling 366
and thus it is useful to motivate the concept of homogeneous activation functions by looking at other 367
notions of linearity for activation functions. This will also be useful for Section E where even more 368
general classes of activation functions are considered. 369
A.1 Additive Activation Functions 370
Definition A.1. A neuronal activation function f:R→Ris additively linear if and only if 371
f(x+y) =f(x) = (f(y)for any real numbers xandy. 372
Proposition A.2. The class of additively linear activation functions is exactly equal to the class of 373
linear activation functions, i.e., activation functions of the form f(x) =ax. 374
Proof. Obviously linear activation functions are additively linear. Conversely, if fis additively linear, 375
the following three properties are true: 376
(1) One must have: f(nx) =nf(x)andf(x/n) =f(x)/nfor any x∈Rand any n∈N. As a 377
result, f(n/m) =nf(1)/mfor any integers nandm(m̸= 0). 378
(2) Furthermore, f(0 + 0) = f(0) + f(0)which implies: f(0) = 0 . 379
(3) And thus f(x−x) =f(x) +f(−x) = 0 , which in turn implies that f(−x) =−f(x). 380
From these properties, it is easy to see that fmust be continuous, with f(x) =xf(1), and thus f 381
must be linear. 382
A.2 Multiplicative Activation Functions 383
Definition A.3. A neuronal activation function f:R→Ris multiplicative if and only if f(xy) = 384
f(x)(f(y)for any real numbers xandy. 385
Proposition A.4. The class of continuous multiplicative activation functions is exactly equal to the 386
class of functions comprising the functions: f(x) = 0 for every x,f(x) = 1 for every x, and all the 387
even and odd functions satisfying f(x) =xcforx≥0, where cis any constant in R. 388
Proof. It is easy to check the functions described in the proposition are multiplicative. Conversely, 389
assume fis multiplicative. For both x= 0andx= 1, we must have f(x) =f(xx) =f(x)f(x)and 390
thusf(0)is either 0 or 1, and similarly for f(1). Iff(1) = 0 , then for any xwe must have f(x) = 0 391
because: f(x) =f(1x) =f(1)f(x) = 0 . Likewise, if f(0) = 1 , then for any xwe must have 392
f(x) = 1 because: 1 =f(0) = f(0x) =f(0)f(x) =f(x). Thus, in the rest of the proof, we can 393
assume that f(0) = 0 andf(1) = 1 . By induction, it is easy to see that for any x≥0we must have: 394
f(xn) =f(x)nandf(x1/n) = (f(x))1/nfor any integer (positive or negative). As a result, for any 395
x∈Rand any integers nandmwe must have: f(xn/m) =f(x)n/m. By continuity this implies 396
that for any x≥0and any r∈R, we must have: f(xr) =f(x)r. Now there is some constant csuch 397
that:f(e) =ec. And thus, for any x >0,f(x) =f(elogx) = [f(e)]logx=eclogx=xc. To address 398
negative values of x, note that we must have f[(−1)(−1 =f(1) = 1 f(−1)2. Thus, f(−1)is either 399
equal to 1 or to -1. Since for any x >0we have f(−x) =f(−1)f(x), we see that if f(−1) = 1 400
the function must be even ( f(−x) =f(x) =xc), and if f(−1) =−1the function must be odd 401
(f(−x) =−f(x)). 402
We will return to multiplicative activation function in a later section. 403
11A.3 Linearly Scalable Activation Functions 404
Definition A.5. A neuronal activation function f:R→Ris linearly scalable if and only if 405
f(λx) =λf(x)for every λ∈R. 406
Proposition A.6. The class of linearly scalable activation functions is exactly equal to the class of 407
linear activation functions, i.e., activation functions of the form f(x) =ax. 408
Proof. Obviously, linear activation functions are linearly scalable. For the converse, if fis linearly 409
multiplicative we must have f(λx) =λf(x) =xf(λ)for any xand any λ. By taking λ= 1, we get 410
f(x) =f(1)xand thus fis linear. 411
Thus the concepts of linearly additive or linearly scalable activation function are of limited interest 412
since both of them are equivalent to the concept of linear activation function. A more interesting 413
class is obtained if we consider linearly scalable activation functions, where the scaling factor λis 414
constrained to be positive ( λ >0), also called homogeneous functions. 415
A.4 Homogeneous Activation Functions 416
Definition A.7. (Homogeneous) A neuronal activation function f:R→Ris homogeneous if and 417
only if: f(λx) =λf(x)for every λ∈Rwithλ >0. 418
Remark A.8.Note that if fis homogeneous, f(λ0) = λf(0) = f(0)for any λ > 0and thus 419
f(0) = 0 . Thus it makes no difference in the definition of homogeneous if we set λ≥0instead of 420
λ >0). 421
Remark A.9.Clearly, linear activation functions are homogeneous. However, there exists also 422
homogeneous functions that are non-linear, such as ReLU or leaky ReLU activation functions. 423
We now provide a full characterization of the class of homogeneous activation functions. 424
A.5 BiLU Activation Functions 425
We first define a new class of activation functions, corresponding to bilinear units (BiLU), consisting 426
of two half-lines meeting at the origin. This class contains all the linear functions, as well as the 427
ReLU and leaky ReLU functions, and many other functions. 428
Definition A.10. (BiLU) A neuronal activation function f:R→Ris bilinear (BiLU) if and only if 429
f(x) =axwhen x <0, and f(x) =bxwhen x≥0, for some fixed parameters aandbinR. 430
These include linear units ( a=b), ReLU units ( a= 0, b= 1), leaky ReLU ( a=ϵ;b= 1) units, 431
and symmetric linear units ( a=−b), all of which can also be viewed as special cases of piece-wise 432
linear units Tavakoli et al. [2021], with a single hinge. One advantage of ReLU and more generally 433
BiLU neurons, which is very important during backpropagation learning, is that their derivative is 434
very simple and can only take one of two values ( aorb). 435
Proposition A.11. A neuronal activation function f:R→Ris homogeneous if and only if it is a 436
BiLU activation function. 437
Proof. Every function in BiLU is clearly homogeneous. Conversely, any homogeneous function f 438
must satisfy: (1) f(0x) = 0 f(x) =f(0) = 0 ; (2)f(x) =f(1x) =f(1)xfor any positive x; and (3) 439
f(x) =f(−u) =f(−1)u=−f(−1)xfor any negative x. Thus fis in BiLU with a=−f(−1) 440
andb=f(1). 441
In Appendix A, we provide a simple proof that networks of BiLU neurons, even with a single 442
hidden layer, have universal approximation properties. In the next two sections, we introduce two 443
fundamental neuronal operations, scaling and balancing, that can be applied to the incoming and 444
outgoing synaptic weights of neurons with BiLU activation functions. 445
12B Scaling 446
Definition B.1. (Scaling) For any BiLU neuron iin network and any λ >0, we let Sλ(i)denote the 447
synaptic scaling operation by which the incoming connection weights of neuron iare multiplied by λ 448
and the outgoing connection weights of neuron iare divided by λ. 449
Note that because of the homogeneous property, the scaling operation does not change how neuron i 450
affects the rest of the network. In particular, the input-output function of the overall network remains 451
unchanged after scaling neuron ibt any λ >0. Note also that scaling always preserves the sign of 452
the synaptic weights to which it is applied, and the scaling operation can never convert a non-zero 453
synaptic weight into a zero synaptic weight, or vice versa. 454
As usual, the bias is treated here as an additional synaptic weight emanating from a unit clamped to 455
the value one. Thus scaling is applied to the bias. 456
Proposition B.2. (Commutativity of Scaling) Scaling operations applied to any pair of BiLU neurons 457
iandjin a neural network commute: Sλ(i)Sµ(j) =Sµ(j)Sλ(i), in the sense that the resulting 458
network weights are the same, regardless of the order in which the scaling operations are applied. 459
Furthermore, for any BiLU neuron i:Sλ(i)Sµ(i) =Sµ(i)Sλ(i) =Sλµ(i). 460
This is obvious. As a result, any set Iof BiLU neurons in a network can be scaled simultaneously or 461
in any sequential order while leading to the same final configuration of synaptic weights. If we denote 462
by1,2, . . . , n the neurons in I, we can for instance write:Q
i∈ISλi(i) =Q
σ(i)∈ISλσ(i)(σ(i))for 463
any permutation σof the neurons. Likewise, we can collapse operations applied to the same neuron. 464
For instance, we can write: S5(1)S2(2)S3(1)S4(2) = S15(1)S8(2) = S8(2)S15(1) 465
Definition B.3. (Coordinated Scaling) For any set Iof BiLU neurons in a network and any λ >0, 466
we let Sλ(I)denote the synaptic scaling operation by which all the neurons in Iare scaled by the 467
same λ. 468
C Balancing 469
Definition C.1. (Balancing) Given a BiLU neuron in a network, the balancing operation B(i)is 470
a particular scaling operation B(i) =Sλ∗(i), where the scaling factor λ∗is chosen to optimize a 471
particular cost function, or regularizer, associated with the incoming and outgoing weights of neuron 472
i. 473
For now, we can imagine that this cost function is the usual L2(least squares) regularizer, but in 474
the next section, we will consider more general classes of regularizers and study the corresponding 475
optimization process. For the L2regularizer, as shown in the next section, this optimization process 476
results in a unique value of λ∗such that sum of the squares of the incoming weights is equal to 477
the sum of the squares of the outgoing weights, hence the term “balance”. Note that obviously 478
B(B(i)) =B(i)and that, as a special case of scaling operation, the balancing operation does not 479
change how neuron icontributes to the rest of the network, and thus it leaves the overall input-output 480
function of the network unchanged. 481
Unlike scaling operations, balancing operations in general do not commute as balancing operations 482
(they still commute as scaling operations). Thus, in general, B(i)B(j)̸=B(j)B(i). This is because 483
if neuron iis connected to neuron j, balancing iwill change the connection between iandj, and, in 484
turn, this will change the value of the optimal scaling constant for neuron jand vice versa. However, 485
if there are no non-zero connections between neuron iand neuron jthen the balancing operations 486
commute since each balancing operation will modify a different, non-overlapping, set of weights. 487
Definition C.2. (Disjoint neurons) Two neurons iandjin a neural network are said to be disjoint if 488
there are no non-zero connections between iandj. 489
Thus in this case B(i)B(j) =Sλ∗(i)Sµ∗(j) =Sµ∗(j)Sλ∗(i) =B(j)B(i). This can be extended to 490
disjoint sets of neurons. 491
Definition C.3. (Disjoint Set of Neurons) A set Iof neurons is said to be disjoint if for any pair iand 492
jof neurons in Ithere are no non-zero connections between iandj. 493
For example, in a layered feedforward network, all the neurons in a layer form a disjoint set, as long 494
as there are no intra-layer connections or, more precisely, no non-zero intra-layer connections. All 495
13the neurons in a disjoint set can be balanced in any order resulting in the same final set of synaptic 496
weights. Thus we have: 497
Proposition C.4. If we index by 1,2, . . . , n the neurons in a disjoint set Iof BiLU neurons in a 498
network, we have:Q
i∈IB(i) =Q
i∈ISλ∗
i(i) =Q
σ(i)∈ISλ∗
σ(i)(σ(i)) =Q
σ(i)∈IB(σ(i))for any 499
permutation σof the neurons. 500
Finally, we can define the coordinated balancing of any set Iof BiLU neurons (disjoint or not 501
disjoint). 502
Definition C.5. (Coordinated Balancing) Given any set Iof BiLU neurons (disjoint or not disjoint) 503
in a network, the coordinated balancing of these neurons, written as Bλ∗(I), corresponds to the 504
coordinated scaling all the neurons in Iby the same factor λ∗, Where λ∗minimizes the cost functions 505
of all the weights, incoming and outgoing, associated with all the neurons in I. 506
Remark C.6.While balancing corresponds to a full optimization of the scaling operation, it is also 507
possible to carry a partial optimization of the scaling operation by choosing a scaling factor that 508
reduces the corresponding contribution to the regularizer without minimizing it. 509
D General Framework and Single Neuron Balance 510
In this section, we generalize the kinds of regularizer to which the notion of neuronal synaptic balance 511
can be applied, beyond the usual L2regularizer and derive the corresponding balance equations. 512
Thus we consider a network (feedforward or recurrent) where the hidden units are BiLU units. 513
The visible units can be partitioned into input units and output units. For any hidden unit i, if we 514
multiply all its incoming weights IN(i)by some λ >0and all its outgoing weights OUT (i)by 515
1/λthe overall function computed by the network remains unchanged due to the BiLU homogeneity 516
property. In particular, if there is an error function that depends uniquely on the input-output function 517
being computed, this error remains unchanged by the introduction of the multiplier λ. However, if 518
there is also a regularizer Rfor the weights, its value is affected by λand one can ask what is the 519
optimal value of λwith respect to the regularizer, and what are the properties of the resulting weights. 520
This approach can be applied to any regularizer. For most practical purposes, we can assume that 521
the regularizer is continuous in the weights (hence in λ) and lower-bounded. Without any loss of 522
generality, we can assume that it is lower-bounded by zero. If we want the minimum value to be 523
achieved by some λ >0, we need to add some mild condition that prevents the minimal value from 524
being approached as λ→0), or as λ→+∞. For instance, it is enough if there is an interval [a, b] 525
with0< a < b where Rachieves a minimal value RminandR≥Rminin the intervals (0, a]and 526
[b,+∞). Additional (mild) conditions must be imposed if one wants the optimal value of λto be 527
unique, or computable in closed form (see Theorems below). Finally, we want to be able to apply the 528
balancing approach 529
Thus, we consider overall regularized error functions, where the regularizer is very general, as long 530
as it has an additive form with respect to the individual weights: 531
E(W) =E(W) +R(W) with R(W) =X
wgw(w) (D.1)
where Wdenotes all the weights in the network and E(W)is typically the negative log-likelihood 532
(LMS error in regression tasks, or cross-entropy error in classification tasks). We assume that the gw 533
are continuous, and lower-bounded by 0. To ensure the existence and uniqueness of minimum during 534
the balancing of any neuron, We will assume that each function gwdepends only on the magnitude 535
|w|of the corresponding weight, and that gwis monotonically increasing from 0 to +∞(gw(0) = 0 536
andlimx→+∞gw(x) = +∞). Clearly, L2, L1and more generally all Lpregularizers are special 537
cases where, for p >0,Lpregularization is defined by: R(W) =P
w|w|p. 538
When indicated, we may require also that the functions gwbe continuously differentiable, except 539
perhaps at the origin in order to be able to differentiate the regularizer with respect to the λ’s and 540
derive closed form conditions for the corresponding optima. This is satisfied by all forms of Lp 541
regularization, for p >0. 542
Remark D.1.Often one introduces scalar multiplicative hyperparameters to balance the effect of E 543
andR, for instance in the form: E=E+βR. These cases are included in the framework above: 544
multipliers like βcan easily be absorbed into the functions gwabove. 545
14Theorem D.2. (General Balance Equation). Consider a neural network with BiLU activation 546
functions in all the hidden units and overall error function of the form: 547
E=E(W) +R(W) with R(W) =X
wgw(w) (D.2)
where each function gw(w)is continuous, depends on the magnitude |w|alone, and grows monotoni- 548
cally from gw(0) = 0 togw(+∞) = +∞. For any setting of the weights Wand any hidden unit iin 549
the network and any λ >0we can multiply the incoming weights of ibyλand the outgoing weights 550
ofiby1/λwithout changing the overall error E. Furthermore, there exists a unique value λ∗where 551
the corresponding weights v(v=λ∗wfor incoming weights, v=w/λ∗for the outgoing weights) 552
achieve the balance equation: 553
X
v∈IN(i)gw(v) =X
w∈OUT (i)gw(v) (D.3)
Proof. Under the assumptions of the theorem, Eis unchanged under the rescaling of the incoming and 554
outgoing weights of unit idue to the homogeneity property of BiLUs. Without any loss of generality, 555
let us assume that at the beginning:P
w∈IN(i)gw(w)<P
w∈OUT (i)gw(w). As we increase λfrom 556
1 to+∞, by the assumptions on the functions gw, the termP
w∈IN(i)gw(λw)increases continuously 557
from its initial value to +∞, whereas the termP
w∈OUT (i)gw)w/λ)decreases continuously from 558
its initial value to 0. Thus, there is a unique value λ∗where the balance is realized. If at the beginning 559P
w∈IN(i)gw(w)>P
w∈OUT (i)gw(w), then the same argument is applied by decreasing λfrom 1 560
to 0. 561
Remark D.3.For simplicity, here and in other sections, we state the results in terms of a network of 562
BiLU units. However, the same principles can be applied to networks where only a subset of neurons 563
are in the BiLU class, simply by applying scaling and balancing operations to only those neurons. 564
Furthermore, not all BiLU neurons need to have the same BiLU activation function. For instance, the 565
results still hold for a mixed network containing both ReLU and linear units. 566
Remark D.4.In the setting of Theorem D.2, the balance equations do not necessarily minimize the 567
corresponding regularization term. This is addressed in the next theorem. 568
Remark D.5.Finally, zero weights ( w= 0) can be ignored entirely as they play no role in scaling or 569
balancing. Furthermore, if all the incoming or outgoing weights of a hidden unit were to be zero, it 570
could be removed entirely from the network 571
Theorem D.6. (Balance and Regularizer Minimization) We now consider the same setting as in 572
Theorem D.2, but in addition, we assume that the functions gware continuously differentiable, except 573
perhaps at the origin. Then, for any neuron, there exists at least one optimal value λ∗that minimizes 574
R(W). This value must be a solution of the consistency equation: 575
λ2X
w∈IN(i)wg′
w(λw) =X
w∈OUT (i)wg′
w(w/λ) (D.4)
Once the weights are rebalanced accordingly, the new weights must satisfy the generalized balance 576
equation: 577
X
w∈IN(i)wg′(w) =X
w∈OUT (i)wg′(w) (D.5)
In particular, if gw(w) =|w|pfor all the incoming and outgoing weights of neuron i, then the optimal 578
value λ∗is unique and equal to: 579
λ∗=P
w∈OUT (i)|w|p
P
w∈IN(i)|w|p1/2p
=||OUT (i)||p
||IN(i)||p1/2
(D.6)
The decrease ∆R≥0in the value of the Lpregularizer R=P
w|w|pis given by: 580
15∆R= X
w∈IN(i)|w|p1/2− X
w∈OUT (i)|w|p1/22
(D.7)
After balancing neuron i, its new weights satisfy the generalized Lpbalance equation: 581
X
w∈IN(i)|w|p=X
w∈OUT (i)|w|p(D.8)
Proof. Due to the additivity of the regularizer, the only component of the regularizer that depends on 582
λhas the form: 583
R(λ) =X
w∈IN(i)gw(λw) +X
w∈OUT (i)gw(w/λ) (D.9)
Because of the properties of the functions gw,Rλis continuously differentiable and strictly bounded 584
below by 0. So it must have a minimum, as a function of λwhere its derivative is zero. Its derivative 585
with respect to λhas the form: 586
R′(λ) =X
w∈IN(i)wg′
w(λw) +X
w∈OUT (i)(−w/λ2)g′
w(w/λ) (D.10)
Setting the derivative to zero, gives: 587
λ2X
w∈IN(i)wg′
w(λw) =X
w∈OUT (i)wg′
w(w/λ) (D.11)
Assuming that the left-hand side is non-zero, which is generally the case, the optimal value for λ 588
must satisfy: 589
λ=P
w∈OUT (i)wg′
w(w/λ)
P
w∈IN(i)wg′w(λw)1/2
(D.12)
If the regularizing function is the same for all the incoming and outgoing weights ( gw=g), then the 590
optimal value λmust satisfy: 591
λ=P
w∈OUT (i)wg′(w/λ)
P
w∈IN(i)wg′(λw)1/2
(D.13)
In particular, if g(w) =|w|pthen g(w)is differentiable except possibly at 0 and g′(w) = 592
s(w)p|w|p−1, where s(w)denotes the sign of the weight w. Substituting in Equation D.13, the 593
optimal rescaling λmust satisfy: 594
λ∗=P
w∈OUT (i)ws(w)|w|p−1
P
w∈IN(i)w|ws(w)|p−11/2p
=P
w∈OUT (i)|w|p
P
w∈IN(i)|w|p1/2p
=||OUT (i)||p
||IN(i)||p1/2
(D.14)
At the optimum, no further balancing is possible, and thus λ∗= 1. Equation D.11 yields immediately 595
the generalized balance equation to be satisfied at the optimum: 596
X
w∈IN(i)wg′(w) =X
w∈OUT (i)wg′(w) (D.15)
In the case of LPregularization, it is easy to check by applying Equation D.15, or by direct calculation 597
that: 598
X
w∈IN(i)|λ∗w|p=X
w∈OUT (i)|w/λ∗|p(D.16)
16which is the generalized balance equation. Thus after balancing neuron, the weights of neuron i 599
satisfy the Lpbalance (Equation D.8). The change in the value of the regularizer is given by: 600
∆R=X
w∈IN(i)|w|p+X
w∈OUT (i)|w|p−X
w∈IN(i)|λ∗w|p−X
w∈OUT (i)|w/λ∗|p(D.17)
By substituting λ∗by its explicit value given by Equation D.14 and collecting terms gives Equation 601
D.7. 602
Remark D.7.The monotonicity of the functions gwis not needed to prove the first part of Theorem 603
D.6. It is only needed to prove the uniqueness of λ∗in the Lpcases. 604
Remark D.8.Note that the same approach applies to the case where there are multiple additive 605
regularizers. For instance with both L2andL1regularization, in this case the function fhas the form: 606
gw(w) =αw2+β|w|. Generalized balance still applies. It also applies to the case where different 607
regularizers are applied in different disconnected portions of the network. 608
Remark D.9.The balancing of a single BiLU neuron has little to do with the number of connections. 609
It applies equally to fully connected neurons, or to sparsely connected neurons. 610
E Scaling and Balancing Beyond BiLU Activation Functions 611
So far we have generalized ReLU activation functions to BiLU activation functions in the context of 612
scaling and balancing operations with positive scaling factors. While in the following sections we 613
will continue to work with BiLU activation functions, in this section we show that the scaling and 614
balancing operations can be extended even further to other activation functions. The section can be 615
skipped if one prefers to progress towards the main results on stochastic balancing. 616
Given a neuron with activation function f(x), during scaling instead of multiplying and dividing by 617
λ >0, we could multiply the incoming weights by a function g(λ)and divide the outgoing weights 618
by a function h(λ), as long as the activation function fsatisfies: 619
f(g(λ)x) =h(λ)f(x) (E.1)
for every x∈Rto ensure that the contribution of the neuron to the rest of the network remains 620
unchanged. Note that if the activation function fsatisfies Equation E.1, so does the activation 621
function −f. In Equation E.1, λdoes not have to be positive–we will simply assume that λbelongs 622
to some open (potentially infinite) interval (a, b). Furthermore, the functions gandhcannot be zero 623
forλ∈(a, b)since they are used for scaling. It is reasonable to assume that the functions gandhare 624
continuous, and thus they must have a constant sign as λvaries over (a, b). 625
Now, taking x= 0gives f(0) = h(λ)f(0)for every λ∈(a, b), and thus either f(0) = 0 orh(λ) = 1 626
for every λ∈(a, b). The latter is not interesting and thus we can assume that the activation function 627
fsatisfies f(0) = 0 . Taking x= 1gives f(g(λ)) =h(λ)f(1)for every λin(a, b). For simplicity, 628
let us assume that f(x) = 1 . Then, we have: f(g(λ)) =h(λ)for every λ. Substituting in Equation 629
E.1 yields: 630
f(g(λ)x) =f(g(λ))f(x) (E.2)
for every x∈Rand every λ∈(a, b). This relation is essentially the same as the relation that defines 631
multiplicative activation functions over the corresponding domain (see Proposition A.4), and thus 632
we can identify a key family of solutions using power functions. Note that we can define a new 633
parameter µ=g(λ), where µranges also over some positive or negative interval Iover which: 634
f(µx) =f(µ)f(x). 635
E.1 Bi-Power Units (BiPU) 636
Let us assume that λ >0,g(λ) =λandh(λ) =λcfor some c∈R. Then the activation function 637
must satisfy the equation: 638
f(λx) =λcf(x) (E.3)
17for any x∈Rand any λ >0. Note that if f(x) =xcwe get a multiplicative activation function. 639
More generally, these functions are characterized by the following proposition. 640
Proposition E.1. The set of activation functions fsatisfying f(λx) =λcf(x)for any x∈Rand 641
anyλ >0consist of the functions of the form: 642
f(x) =Cxcifx≥0
Dxcifx <0.(E.4)
where c∈R,C=f(1)∈R, and D=f(−1)∈R. We call these bi-power units (BiPU). If, in 643
addition, we want fto be continuous at 0, we must have either c >0, orc= 0withC=D. 644
Given the general shape, these activations functions can be called BiPU (Bi-Power-Units). Note that 645
in the general case where c >0,CandDdo not need to be equal. In particular, one of them can 646
be equal to zero, and the other one can be different from zero giving rise to “rectified power units” 647
(Figure 6). 648
Linear Leaky ReLU BiPU (D=0,C=1,c=2) BiPU (D=1,C=1,c=2) 
Figure 6
Proof. By taking x= 1, we get f(λ) =f(1)λcfor any λ > 0. Let f(1) = C. Then we see 649
that for any x > 0we must have: f(x) =Cxc. In addition, for every λ > 0we must have: 650
f(λ0) = f(0) = λcf(0). So if c= 0, then f(x) =C=f(1)forx≥0. Ifc̸= 0, then f(0) = 0 . In 651
this case, if we want the activation function to be continuous, then we see that we must have c≥0. So 652
in summary for x >0we must have f(x) =f(1)xc=Cxc. For the function to be right continuous 653
at 0, we must have either f(0) = f(1) = Cwithc= 0orf(0) = 0 withc >0. We can now look 654
at negative values of x. By the same reasoning, we have f(λ(−1)) = f(−λ) =λcf(−1)for any 655
λ >0. Thus for any x <0we must have: f(x) =f(−1)|x|c=D|x|cwhere D=f(−1). Thus, if 656
fis continuous, there are two possibilities. If c= 0, then we must have C=f(1) = D(f−1)−and 657
thusf(x) =Ceverywhere. If c̸= 0, then continuity requires that c >0. In this case f(x) =Cxc658
forx≥0withC=f(1), and f(x) =Dxcforx <0withf(−1) = D. In all cases, it is easy to 659
check directly that the resulting functions satisfy the functional equation given by Equation E.3. 660
E.2 Scaling BiPU Neurons 661
A BiPU neuron can be scaled by multiplying its incoming weight by λ >0and dividing its outgoing 662
weights by 1/λc. This will not change the role of the corresponding unit in the network, and thus it 663
will not change the input-output function of the network. 664
E.3 Balancing BiPU Neurons 665
As in the case of BiLU neurons, we balance a multiplicative neuron by asking what is the optimal 666
scaling factor λthat optimizes a particular regularizer. For simplicity, here we assume that the 667
regularizer is in the Lpclass. Then we are interested in the value of λ > 0that minimizes the 668
function: 669
λpX
w∈IN|w|p+1
λpcX
w∈OUT|w|p(E.5)
A simple calculation shows that the optimal value of λis given by: 670
18λ∗=cP
OUT|w|p
P
IN|w|p1/p(c+1)
(E.6)
Thus after balancing the weights, the neuron must satisfy the balance equation: 671
cX
OUT|w|p=X
IN|w|p(E.7)
in the new weights w. 672
So far, we have focused on balancing individual neurons. In the next two sections, we look at 673
balancing across all the units of a network. We first look at what happens to network balance when a 674
network is trained by gradient descent and then at what happens to network balance when individual 675
neurons are balanced iteratively in a regular or stochastic manner. 676
F Network Balance: Gradient Descent 677
A natural question is whether gradient descent (or stochastic gradient descent) applied to a network of 678
BiLU neurons, with or without a regularizer, converges to a balanced state of the network, where all 679
the BiLU neurons are balanced. So we first consider the case where there is no regularizer ( E=E). 680
The results in Du et al. [2018] may suggest that gradient descent may converge to a balanced state. In 681
particular, they write that for any neuron i: 682
d
dt X
w∈IN(i)w2−X
w∈OUT (i)w2
= 0 (F.1)
Thus the gradient flow exactly preserves the difference between the L2cost of the incoming and 683
outgoing weights or, in other words, the derivative of the L2balance deficit is zero. Thus if one were 684
to start from a balanced state and use an infinitesimally small learning rate one ought to stay in a 685
balanced state at all times. 686
However, it must be noted that this result was derived for the L2metric only, and thus would not 687
cover other Lpforms of balance. Furthermore, it requires an infinitesimally small learning rate. In 688
practice, when any standard learning rate is applied, we find that gradient descent does notconverge 689
to a balanced state (Figure 1). However, things are different when a regularizer term is included in 690
the error functions as described in the following theorem. 691
Theorem F.1. Gradient descent in a network of BiLU units with error function E=E+Rwhere R 692
has the properties described in Theorem D.6 (including all Lp) must converge to a balanced state, 693
where every BiLU neuron is balanced. 694
Proof. By contradiction, suppose that gradient descent converges to a state that is unbalanced and 695
where the gradient with respect to all the weights is zero. Then there is at least one unbalanced neuron 696
in the network. We can then multiply the incoming weights of such a neuron by λand the outgoing 697
weights by 1/λas in the previous section without changing the value of E. Since the neuron is not in 698
balance, we can move λinfinitesimally so as to reduce R, and hence E. But this contradicts the fact 699
that the gradient is zero. 700
Remark F.2.In practice, in the case of stochastic gradient descent applied to E+R, at the end of 701
learning the algorithm may hover around a balanced state. If the state reached by the stochastic 702
gradient descent procedure is not approximately balanced, then learning ought to continue. In other 703
words, the degree of balance could be used to monitor whether learning has converged or not. Balance 704
is a necessary, but not sufficient, condition for being at the optimum. 705
Remark F.3.If early stopping is being used to control overfitting, there is no reason for the stopping 706
state to be balanced. However, the balancing algorithms described in the next section could be used 707
to balance this state. 708
19G Network Balance: Stochastic or Deterministic Balancing Algorithms 709
In this section, we look at balancing algorithms where, starting from an initial weight configuration 710
W, the BiLU neurons of a network are balanced iteratively according to some deterministic or 711
stochastic schedule that periodically visits all the neurons. We can also include algorithms where 712
neurons are partitioned into groups (e.g. neuronal layers) and neurons in each group are balanced 713
together. 714
G.1 Basic Stochastic Balancing 715
The most interesting algorithm is when the BiLU neurons of a network are iteratively balanced 716
in a purely stochastic manner. This algorithm is particularly attractive from the standpoint of 717
physically implemented neural networks because the balancing algorithm is local and the updates 718
occur randomly without the need for any kind of central coordination. As we shall see in the following 719
section, the random local operations remarkably lead to a unique form of global order. The proof 720
for the stochastic case extends immediately to the deterministic case, where the BiLU neurons are 721
updated in a deterministic fashion, for instance by repeatedly cycling through them according to 722
some fixed order. 723
G.2 Subset Balancing (Independent or Tied) 724
It is also possible to partition the BiLU neurons into non-overlapping subsets of neurons, and then 725
balance each subset, especially when the neurons in each subset are disjoint of each other. In this 726
case, one can balance all the neurons in a given subset, and repeat this subset-balancing operation 727
subset-by-subset, again in a deterministic or stochastic manner. Because the BiLU neurons in each 728
subset are disjoint, it does not matter whether the neurons in a given subset are updated synchronously 729
or sequentially (and in which order). Since the neurons are balanced independently of each other, 730
this can be called independent subset balancing. For example, in a layered feedforward network with 731
no lateral connections, each layer corresponds to a subset of disjoint neurons. The incoming and 732
outgoing connections of each neuron are distinct from the incoming and outgoing connections of 733
any other neuron in the layer, and thus the balancing operation of any neuron in the layer does not 734
interfere with the balancing operation of any other neuron in the same layer. So this corresponds to 735
independent layer balancing, 736
As a side note, balancing a layer h, may disrupt the balance of layer h+ 1. However, balancing 737
layers handh+ 2(or any other layer further apart) can be done without interference of the balancing 738
processes. This suggests also an alternating balancing scheme, where one alternatively balances all 739
the odd-numbered layers, and all the evenly-numbered layers. 740
Yet another variation is when the neurons in a disjoint subset are tied to each other in the sense that 741
they must all share the same scaling factor λ. In this case, balancing the subset requires finding the 742
optimal λfor the entire subset, as opposed to finding the optimal λfor each neuron in the subset. 743
Since the neurons are balanced in a coordinated or tied fashion, this can be called coordinated or tied 744
subset balancing. For example, tied layer balancing must use the same λfor all the neurons in a given 745
layer. It is easy to see that this approach leads to layer synaptic balance which has the form (for an 746
Lpregularizer): 747
X
iX
w∈IN(i)|w|p=X
iX
w∈OUT (i)|w|p(G.1)
where iruns over all the neurons in the layer. This does notnecessarily imply that each neuron 748
in the layer is individually balanced. Thus neuronal balance for every neuron in a layer implies 749
layer balance, but the converse is not true. Independent layer balancing will lead to layer balance. 750
Coordinated layer balancing will lead to layer balance, but not necessarily to neuronal balance of 751
each neuron in the layer. Layer-wise balancing, independent or tied, can be applied to all the layers 752
and in a deterministic (e.g. sequential) or stochastic manner. Again the proof given in the next section 753
for the basic stochastic algorithm can easily be applied to these cases (see also Appendix B). 754
20G.3 Remarks about Weight Sharing and Convolutional Neural Networks 755
Suppose that two connections share the same weight so that we must have: wij=wklat all times. 756
In general, when the balancing algorithm is applied to neuron iorj, the weight wijwill change 757
and the same change must be applied to wkl. The latter may disrupt the balance of neuron korl. 758
Furthermore, this may not lead to a decrease in the overall value of the regularizer R. 759
The case of convolutional networks is somewhat special, since allthe incoming weights of the 760
neurons sharing the same convolutional kernel are shared. However, in general, the outgoing weights 761
are not shared. Furthermore, certain operations like max-pooling are not homogeneous. So if one 762
trains a CNN with Ealone, or even with E+R, one should not expect any kind of balance to emerge 763
in the convolution units. However, all the other BiLU units in the network should become balanced 764
by the same argument used for gradient descent above. The balancing algorithm applied to individual 765
neurons, or the independent layer balancing algorithm, will not balance individual neurons sharing 766
the same convolution kernel. The only balancing algorithm that could lead to some convolution layer 767
balance, but not to individual neuronal balance, is the coordinated layer balancing, where the same λ 768
is used for all the neurons in the same convolution layer, provided that their activation functions are 769
BiLU functions. 770
We can now study the convergence properties of balancing algorithms. 771
H Convergence of Balancing Algorithms 772
We now consider the basic stochastic balancing algorithm, where BiLU neurons are iteratively and 773
stochastically balanced. It is essential to note that balancing a neuron jmay break the balance of 774
another neuron ito which jis connected. Thus convergence of iterated balancing is not obvious. 775
There are three key questions to be addressed for the basic stochastic algorithm, as well as all the 776
other balancing variations. First, does the value of the regularizer converge to a finite value? Second, 777
do the weights themselves converge to fixed finite values representing a balanced state for the entire 778
network? And third, if the weights converge, do they always converge to the same values, irrespective 779
of the order in which the units are being balanced? In other words, given an initial state Wfor the 780
network, is there a unique corresponding balanced state, with the same input-output functionalities? 781
H.1 Notation and Key Questions 782
For simplicity, we use a continuous time notation. After a certain time teach neuron has been 783
balanced a certain number of times. While the balancing operations are not commutative as balancing 784
operations, they are commutative as scaling operations. Thus we can reorder the scaling operations 785
and group them neuron by neuron so that, for instance, neuron ihas been scaled by the sequence of 786
scaling operations: 787
Sλ∗
1(i)Sλ∗
2(i). . . S λ∗nit(i) =SΛi(t)(i) (H.1)
where nitcorresponds to the count of the last update of neuron iprior to time t, and: 788
Λi(t) =Y
1≤n≤nitλ∗
n(i) (H.2)
For the input and output units, we can consider that their balancing coefficients λ∗are always equal 789
to 1 (at all times) and therefore Λi(t) = 1 for any visible unit i. 790
Thus, we first want to know if Rconverges. Second, we want to know if the weights converge. This 791
question can be split into two sub-questions: (1) Do the balancing factors λ∗
n(i)converge to a limit as 792
time goes to infinity? Even if the λ∗
n(i)’s converge to a limit, this does not imply that the weights of 793
the network converge to a limit. After a time t, the weight wij(t)between neuron jand neuron ihas 794
the value wijΛi(t)/Λj(t), where wij=wij(0)is the value of the weight at the start of the stochastic 795
balancing algorithm. Thus: (2) Do the quantities Λi(t)converge to finite values, different from 0? 796
And third, if the weights converge to finite values different from 0, are these values unique or not, i.e. 797
do they depend on the details of the stochastic updates or not? These questions are answered by the 798
following main theorem.. 799
21H.2 Convergence of the Basic Stochastic Balancing Algorithm to a Unique Optimum 800
Theorem H.1. (Convergence of Stochastic Balancing) Consider a network of BiLU neurons with an 801
error function E(W) =E(W)+R(W)where Rsatisfies the conditions of Theorem D.2 including all 802
Lp(p >0). Let Wdenote the initial weights. When the neuronal stochastic balancing algorithm is 803
applied throughout the network so that every neuron is visited from time to time, then E(W)remains 804
unchanged but R(W)must converge to some finite value that is less or equal to the initial value, 805
strictly less if the initial weights are not balanced. In addition, for every neuron i,λ∗
i(t)→1and 806
Λi(t)→Λiast→ ∞ , where Λiis finite and Λi>0for every i. As a result, the weights themselves 807
must converge to a limit W′which is globally balanced, with E(W) =E(W′)andR(W)≥R(W′), 808
and with equality if only if Wis already balanced. Finally, W′is unique as it corresponds to the 809
solution of a strictly convex optimization problem in the variables Lij= log(Λ i/Λj)with linear 810
constraints of the formP
πLij= 0along any path πjoining an input unit to an output unit and along 811
any directed cycle (for recurrent networks). Stochastic balancing projects to stochastic trajectories in 812
the linear manifold that run from the origin to the unique optimal configuration. 813
Proof. Each individual balancing operation leaves E(W)unchanged because the BiLU neurons are 814
homogeneous. Furthermore, each balancing operation reduces the regularization error R(W), or 815
leaves it unchanged. Since the regularizer is lower-bounded by zero, the value of the regularizer must 816
approach a limit as the stochastic updates are being applied. 817
For the second question, when neuron iis balanced at some step, we know that the regularizer R 818
decreases by: 819
∆R= X
w∈IN(i)|w|p1/2− X
w∈OUT (i)|w|p1/22
(H.3)
If the convergence were to occur in a finite number of steps, then the coefficients λ∗
i(t)must become 820
equal and constant to 1 and the result is obvious. So we can focus on the case where the convergence 821
does not occur in a finite number of steps (indeed this is the main scenario, as we shall see at the end 822
of the proof). Since ∆R→0, we must have: 823
X
w∈IN(i)|w|p→X
w∈OUT (i)|w|p(H.4)
But from the expression for λ∗(Equation D.14), this implies that for every i,λ∗
n(i)→1as time 824
increases ( n→ ∞ ). This alone is not sufficient to prove that Λi(t)converges for every iast→ ∞ . 825
However, it is easy to see that Λi(t)cannot contain a sub-sequence that approaches 0 or ∞(Figure 7). 826
Furthermore, not only ∆Rconverges to 0, but the seriesP∆Ris convergent. This shows that, for 827
every i,∆i(t)must converge to a finite, non-zero value ∆i. Therefore all the weights must converge 828
to fixed values given by wij(0)Λ i/Λj. 829
Finally, we prove that given an initial set of weights W, the final balanced state is unique and 830
independent of the order of the balancing operations. The coefficients Λicorresponding to a globally 831
balanced state must be solutions of the following optimization problem: 832
min
ΛR(Λ) =X
ij|Λi
Λjwij|p(H.5)
under the simple constraints: Λi>0for all the BiLU hidden units, and Λi= 1for all the visible (input 833
and output) units. In this form, the problem is not convex. Introducing new variables Mj= 1/Λj 834
is not sufficient to render the problem convex. Using variables Mij= Λ i/Λjis better, but still 835
problematic for 0< p≤1. However, let us instead introduce the new variables Lij= log(Λ i/Λj). 836
These are well defined since we know that Λi/Λj>0. The objective now becomes: 837
minR(L) =X
ij|eLijwij|p=X
ijepLij|wij|p(H.6)
This objective is strictly convex in the variables Lij, as a sum of strictly convex functions (exponen- 838
tials). However, to show that it is a convex optimization problem we need to study the constraints on 839
22ȿ1(t)=1 ȿ2(t) ȿ3(t) ȿ4(t) ȿ5(t)=1 
ȿ2(t)/ȿ1(t) ȿ3(t)/ ȿ2(t) ȿ4(t)/ȿ3(t) ȿ5(t)/ȿ4(t) Input Unit Output Unit Figure 7: A path with three hidden BiLU units connecting one input unit to one output unit. During the
application of the stochastic balancing algorithm, at time teach unit ihas a cumulative scaling factor Λi(t),
and each directed edge from unit jto unit ihas a scaling factor Mij(t) = Λ i(t)/Λj(t). The λi(t)must
remain within a finite closed interval away from 0 and infinity. To see this, imagine for instance that there
is a subsequence of Λ3(t)that approaches 0. Then there must be a corresponding subsequence of Λ4(t)that
approaches 0, or else the contribution of the weight w43Λ4(t)/Λ3(t)to the regularizer would go to infinity. But
then, as we reach the output layer, the contribution of the last weight w54Λ5(t)/Λ4(t)to the regularizer goes to
infinity because Λ5(t)is fixed to 1 and cannot compensate for the small values of Λ4(t). And similarly, if there
is a subsequence of Λ3(t)going to infinity, we obtain a contradiction by propagating its effect towards the input
layer.
Λ1 Λ2Λ3 Λ4 Λ5 Λ2/Λ1Λ3/Λ2 Λ4/Λ3Λ5/Λ4Input Unit Output Unit 
Figure 8: A path with five units. After the stochastic balancing algorithm has converged, each unit ihas a scaling
factor Λi, and each directed edge from unit jto unit ihas a scaling factor Mij= Λi/Λj. The products of the
Mij’s along the path is given by:Λ2
Λ1Λ3
Λ2Λ4
Λ3Λ5
Λ4=Λ5
Λ1. Accordingly, if we sum the variables Lij= log Mij
along the directed path, we get L21+L32+L43+L54= log Λ 5−log Λ 1. In particular, if unit 1 is an input unit
and unit 5 is an output unit, we must have Λ1= Λ 5= 1and thus: L21+L32+L43+L54= 0. Likewise, in the
case of a directed cycle where unit 1 and unit 5 are the same, we must have: L21+L32+L43+L54+L15= 0.
the variables Lij. From the set of Λi’s it is easy to construct a unique set of Lij. However what about 840
the converse? 841
Definition H.2. A set of real numbers Lij, one per connection of a given neural architecture, is 842
self-consistent if and only if there is a unique corresponding set of numbers Λi>0(one per unit) 843
such that: Λi= 1for all visible units and Lij= log Λ i/Λjfor every directed connection from a unit 844
jto a unit i. 845
Remark H.3.This definition depends on the graph of connections, but not on the original values of 846
the synaptic weights. Every balanced state is associated with a self-consistent set of Lij, but not 847
every self-consistent set of Lijis associated with a balanced state. 848
Proposition H.4. A set Lijassociated with a neural architecture is self-consistent if and only if 849P
πLij= 0where πis any directed path connecting an input unit to an output unit or any directed 850
cycle (for recurrent networks). 851
Remark H.5.Thus the constraints associated with being a self-consistent configuration of Lij’ s 852
are all linear. This linear manifold of constraints depends only on the architecture, i.e., the graph of 853
connections. The strictly convex function R(Lij)depends on the actual weights W. Different sets of 854
weights Wproduce different convex functions over the same linear manifold. 855
23ɲ ɴɶ ɷȿ’i
ȿiFigure 9: Consider two paths α+βandγ+δfrom the input layer to the output layer going through the
same unit i. Let us assume that the first path assigns a multiplier Λito unit iand the second path assigns a
multiplier Λ′
ito the same unit. By assumption we must have:P
αLij+P
βLij= 0 for the first path, andP
γLij+P
δLij= 0. But α+δandγ+βare also paths from the input layer to the output layer and
therefore:P
αLij+P
δLij= 0andP
γLij+P
βLij= 0. As a result,P
αLij= log Λ i=P
γLij= Λ′
i.
Therefore the assignment of the multiplier Λimust be consistent across different paths going through unit i.
Remark H.6.Note that one could coalesce all the input units and all output units into a single unit, 856
in which case a path from an input unit to and output unit becomes also a directed cycle. In this 857
representation, the constraints are that the sum of the Lijmust be zero along any directed cycle. In 858
general, it is not necessary to write a constraint for every path from input units to output units. It is 859
sufficient to select a representative set of paths such that every unit appears in at least one path. 860
Proof. If we look at any directed path πfrom unit ito unit j, it is easy to see that we must have: 861
X
πLkl= log Λ i−log Λ j (H.7)
This is illustrated in Figures 8 and 1. Thus along any directed path that connects any input unit to any 862
output unit, we must haveP
πLij= 0. In addition, for recurrent neural networks, if πis a directed 863
cycle we must also have:P
πLij= 0. Thus in short we only need to add linear constraints of the 864
form:P
πLij= 0. Any unit is situated on a path from an input unit to an output unit. Along that 865
path, it is easy to assign a value Λito each unit by simple propagation starting from the input unit 866
which has a multiplier equal to 1. When the propagation terminates in the output unit, it terminates 867
consistently because the output unit has a multiplier equal to 1 and, by assumption, the sum of the 868
multipliers along the path must be zero. So we can derive scaling values Λifrom the variables 869
Lij. Finally, we need to show that there are no clashes, i.e. that it is not possible for two different 870
propagation paths to assign different multiplier values to the same unit i. The reason for this is 871
illustrated in Figure 9. 872
We can now complete the proof Theorem H.1. Given a neural network of BiLUs with a set of weights 873
W, we can consider the problem of minimizing the regularizer R(Lijover the self-admissible 874
configuration Lij. For any P > 0, the Lpregularizer is strictly convex and the space of self- 875
admissible configurations is linear and hence convex. Thus this is a strictly convex optimization 876
problem that has a unique solution (Figure 2). Note that the minimization is carried over self- 877
consistent configurations, which in general are not associated with balanced states. However, the 878
configuration of the weights associated with the optimum set of Lij(point Ain Figure 2) must be 879
balanced. To see this, imagine that one of the BiLU units–unit iin the network is not balanced. Then 880
we can balance it using a multiplier λ∗
iand replace ΛibyΛ′
i= Λiλ∗. It is easy to check that the new 881
configuration including Λ′
iis self-consistent. Thus, by balancing unit i, we are able to reach a new 882
self-consistent configuration with a lower value of Rwhich contradicts the fact that we are at the 883
global minimum of the strictly convex optimization problem. 884
24We know that the stochastic balancing algorithm always converges to a balanced state. We need to 885
show that it cannot converge to any other balanced state, and in fact that the global optimum is the 886
only balanced state. By contradiction, suppose it converges to a different balanced state associated 887
with the coordinates (LB
ij)(point Bin Figure 2). Because of the self-consistency, this point is also 888
associated with a unique set of (ΛB
i)coordinates. The cost function is continuous and differentiable 889
in both the Lij’s and the Λi’s coordinates. If we look at the negative gradient of the regularizer, it 890
is non-zero and therefore it must have at least one non-zero component ∂R/∂ Λialong one of the 891
Λicoordinates. This implies that by scaling the corresponding unit iin the network, the regularizer 892
can be further reduced, and by balancing unit ithe balancing algorithm will reach a new point ( Cin 893
Figure 2) with lower regularizer cost. This contradicts the assumption that Bwas associated with a 894
balanced stated. Thus, given an initial set of weights W, the stochastic balancing algorithm must 895
always converge to the same and unique optimal balanced state W∗associated with the self-consistent 896
point A. A particular stochastic schedule corresponds to a random path within the linear manifold 897
from the origin (at time zero all the multipliers are equal to 1, and therefore Mij= 1andLij= 0) 898
for any iand any jto the unique optimum point A. 899
Remark H.7.It should be clear from the proof that the same result holds also for any deterministic 900
balancing schedule, as well as for tied and non-tied subset balancing, e.g., for layer-wise balancing 901
and tied layer-wise balancing. In the Appendix, we provide an analytical solution for the case of tied 902
layer-wise balancing in a layered feed-forward network. 903
Remark H.8.It should be clear from the proof that the same convergence to the unique global 904
optimum is observed if each neuron, when stochastically visited, is favorably scaled rather than 905
balanced, i.e., it is scaled with a factor that reduces Rbut not necessarily minimizes R. Stochastic 906
balancing can also be viewed as a form of EM algorithm where the E and M steps can be taken fully 907
or partially. 908
I Universal Approximation Properties of BiLU Neurons 909
Here we show that any continuous real-valued function defined over a compact set of the Euclidean 910
space can be approximated to any degree of precision by a network of BiLU neurons with a single 911
hidden layer. As in the case of the similar proof given in Baldi [2021] using linear threshold gates in 912
the hidden layer, it is enough to prove the theorem for a continuous function f:0,1→R. 913
Theorem I.1. (Universal Approximation Properties of BiLU Neurons) Let fbe any continuous 914
function from [0,1]toRandϵ >0. Letgλbe the ReLU activation function with slope λ∈Rs. Then 915
there exists a feedforward network with a single hidden layer of neurons with ReLU activations of the 916
formgλand a single output linear neuron, i.e., with BiLU activation equal to the identity function, 917
capable of approximating feverywhere within ϵ(sup norm). 918
Proof. To be clear, gλ(x) = 0 forx <0andgλ(x) =λxfor0≤x. Since fis continuous over a 919
compact set, it is uniformly continuous. Thus there exists α >0such that for any x1andx2in the 920
[0,1]interval: 921
|x2−x1|< α =⇒ |f(x2)−f(x1)|< ϵ (I.1)
LetNbe an integer such that 1< Nα , and let us slice the interval [0,1]intoNconsecutive slices 922
of width h= 1/N, so that within each slice the function fcannot jump by more than ϵ. Let us 923
connect the input unit to all the hidden units with a weight equal to 1. Let us have Nhidden units 924
numbered 1, . . . , N with biases equal to 0,1/N,2/N, ...., N 1/Nrespectively and activation function 925
of the form gλk. It is essential that different units be allowed to have different slopes λk. The input 926
unit is connected to all the hidden units and all the weights on these connections are equal to 1. Thus 927
when xis in the k-th slice, (k−1)/N≤x < k/N , all the units from k+ 1toNhave an output 928
equal to 0, and all the units from 1 to khave an output determined by the corresponding slopes. All 929
the hidden units are connected to the output unit with weights β1, . . . , β N, and β0is the bias of the 930
output unit. We want the output unit to be linear. In order for the ϵapproximation to be satisfied, 931
it is sufficient if in the (k−1)/N≤x < k/N interval, the output is equal to the line joining the 932
point f((k−1)/N)to the point f(k/N). In other words, if x∈[(k−1)/N, k/N ), then we want 933
the output of the network to be: 934
25β0+kX
i=1βiλi(x−(i−1)h) =f(k−1
N) +f(k
N)−f(k−1
N)
h(x−(k−1)h) (I.2)
By equating the y-intercept and slope of the lines on the left-hand side and the righ- hand side of 935
Equation I.2, we can solve for the weights β’s and the slopes λ’s. 936
As in the case of the similar proof using linear threshold functions in the hidden layer (see Baldi 937
[2021],) this proof can easily be adapted to continuous functions defined over a compact set of Rn, 938
even with a finite number of finite discontinuities, and into Rm. 939
J Analytical Solution for the Unique Global Balanced State 940
Here we directly prove the convergence of stochastic balancing to a unique final balanced state, and 941
derive the equations for the balanced state, in the special case of tied layer balancing (as opposed to 942
single neuron balancing). The Proof and the resulting equations are also valid for stochastic balancing 943
(one neuron at a time) in a layered architecture comprising a single neuron per layer. Let us call tied 944
layer scaling the operation by which all the incoming weights to a given layer of BiLU neurons are 945
multiplied by λ >0and all the outgoing weights of the layer are multiplied by 1/λ, again leaving the 946
training error unchanged. Let us call layer balancing the particular scaling operation corresponding 947
to the value of λthat minimizes the contribution of the layer to the L2(or any other Lp) regularizer 948
value. This optimal value of λ∗results in layer-wise balance equations: the sum of the squares of all 949
the incoming weights of the layer must be equal to the sum of the squares of all the outgoing weights 950
of the layer in the L2case, and similarly in all LPcases. 951
Theorem J.1. Assume that tied layer balancing is applied iteratively and stochastically to the layers 952
of a layered feedforward network of BiLU neurons. As long as all the layers are visited periodically, 953
this procedure will always converge to the same unique set of weights, which will satisfy the layer- 954
balance equations at all layers, irrespective of the details of the schedule. Furthermore, the balance 955
state can be solved analytically. 956
Proof. Every time a layer balancing operation is applied, the training error remains the same, and the 957
L2(or any other Lp) regularization error decreases or stays the same. Since the regularization error 958
is always positive, it must converge to a certain value. Using the same arguments as in the proof of 959
Theorem H.1, the weights must also converge to a stable configuration, and since the configuration 960
is stable all its layers must satisfy the layer-wise balance equation. The key remaining question is 961
why is this configuration unique and can we solve it analytically? Let A1, A2, . . . A Ndenote the 962
matrices of connections between the layers of the network. Let Λ1,Λ2, . . . , ΛN−1beN−1strictly 963
positive multipliers, representing the limits of the products of the corresponding λ∗
iassociated with 964
each balancing step at layer i, as in the proof of Theorem H.1. In this notation, layer 0 is the input 965
layer and layer Nis the output layer (with Λ0= 1andΛN= 1). 966
After converging, each matrix Aibecomes the matrix Λi/Λi−1Ai=MiAifori= 1. . . N , with 967
Mi=λi/Λi−1. The multipliers Mimust minimize the regularizer while satisfying M1. . . M N= 1 968
to ensure that the training error remains unchanged. In other words, to find the values of the Mi’s we 969
must minimize the Lagrangian: 970
L(M1, . . . , M N) =NX
i=1||MiAi||2+µ(1−NY
i=1Mi) (J.1)
written for the L2case in terms of the Frobenius norm, but the analysis is similar in the general Lp 971
case. From this, we get the critical equations: 972
∂L
∂Mi= 2Mi||Ai||2−µM1. . . M i−1Mi+1. . . M N= 0 for i= 1, . . . , N andNY
i=1Mi= 1
(J.2)
As a resut, for every i: 973
262Mi||Ai||2−µ
Mi= 0 or µ= 2M2
i||Ai||2(J.3)
Thus each Mi>0can be expressed in a unique way as a function of the Lagrangian multiplier µas: 974
Mi= (µ/2||Ai||2)1/2. By writing again that the product of the Miis equal to 1, we finally get: 975
µN= 2NNY
i=1||Ai||2orµ= 2NY
i=1||Ai||2/N(J.4)
Thus we can solve for Mi: 976
Mi=µ
2||Ai||2=QN
i=1||Ai||2/N
||Ai||2fori= 1, . . . , N (J.5)
Thus, in short, we obtain a unique closed-form expression for each Mi. From there, we infer the 977
unique and final state of the weights, where A∗
i=MiAi= ΛiAl/Λl−1. Note that each Midepends 978
on all the other Mj’s, again showcasing how the local balancing algorithm leads to a unique global 979
solution. 980
K Computer Resources 981
The simulations we have described do not require major computing resources. They were all 982
performed using Google Colab and the NVIDIA TESLA T4 GPU that it provides. 983
L Code Availability 984
The code for reproducing the simulation results is available under the Apache 2.0 license at: 985
https://anonymous.4open.science/r/a-theory-of-neural-synaptic-balance-00C1 986
27References 987
P. Baldi. Deep Learning in Science . Cambridge University Press, Cambridge, UK, 2021. 988
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal 989
Processing Magazine , 29(6):141–142, 2012. 990
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous 991
models: Layers are automatically balanced. Advances in Neural Information Processing Systems , 992
31, 2018. 993
Rachel E Field, James A D’amour, Robin Tremblay, Christoph Miehl, Bernardo Rudy, Julijana 994
Gjorgjieva, and Robert C Froemke. Heterosynaptic plasticity determines the set point for cortical 995
excitatory-inhibitory balance. Neuron , 106(5):842–854, 2020. 996
Robert C Froemke. Plasticity of cortical excitatory-inhibitory balance. Annual review of neuroscience , 997
38:195–219, 2015. 998
Oliver D Howes and Ekaterina Shatalina. Integrating the neurodevelopmental and dopamine hypothe- 999
ses of schizophrenia and the role of cortical excitation-inhibition balance. Biological psychiatry , 1000
2022. 1001
Dmitry Ivanov, Aleksandr Chezhegov, Mikhail Kiselev, Andrey Grunin, and Denis Larionov. Neuro- 1002
morphic artificial intelligence systems. Frontiers in Neuroscience , 16:1513, 2022. 1003
Yu Ji, YouHui Zhang, ShuangChen Li, Ping Chi, CiHang Jiang, Peng Qu, Yuan Xie, and WenGuang 1004
Chen. Neutrams: Neural network transformation and co-design under neuromorphic hardware 1005
constraints. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture 1006
(MICRO) , pages 1–13. IEEE, 2016. 1007
Dongshin Kim and Jang-Sik Lee. Neurotransmitter-induced excitatory and inhibitory functions in 1008
artificial synapses. Advanced Functional Materials , 32(21):2200497, 2022. 1009
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. 1010
Faming Liang and Wing Hung Wong. Evolutionary monte carlo: Applications to cp model sampling 1011
and change point problem. STATISTICA SINICA , 10:317–342, 2000. 1012
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Data-dependent path 1013
normalization in neural networks. arXiv preprint arXiv:1511.06747 , 2015. 1014
Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conver- 1015
sion of continuous-valued deep networks to efficient event-driven networks for image classification. 1016
Frontiers in neuroscience , 11:294078, 2017. 1017
Farshad Shirani and Hannah Choi. On the physiological and structural contributors to the dynamic 1018
balance of excitation and inhibition in local cortical networks. bioRxiv , pages 2023–01, 2023. 1019
Martino Sorbaro, Qian Liu, Massimo Bortone, and Sadique Sheik. Optimizing the energy consump- 1020
tion of spiking neural networks for neuromorphic applications. Frontiers in neuroscience , 14:662, 1021
2020. 1022
Christopher H Stock, Sarah E Harvey, Samuel A Ocko, and Surya Ganguli. Synaptic balancing: A 1023
biologically plausible local learning rule that provably increases neural network noise robustness 1024
without sacrificing task performance. PLOS Computational Biology , 18(9):e1010418, 2022. 1025
A. Tavakoli, F. Agostinelli, and P. Baldi. SPLASH: Learnable activation functions for improving 1026
accuracy and adversarial robustness. Neural Networks , 140:1–12, 2021. Also: arXiv:2006.08947. 1027
28NeurIPS Paper Checklist 1028
1.Claims 1029
Question: Do the main claims made in the abstract and introduction accurately reflect the 1030
paper’s contributions and scope? 1031
Answer: [Yes] 1032
Justification: We have included all the main points of the paper in the abstract. 1033
Guidelines: 1034
•The answer NA means that the abstract and introduction do not include the claims 1035
made in the paper. 1036
•The abstract and/or introduction should clearly state the claims made, including the 1037
contributions made in the paper and important assumptions and limitations. A No or 1038
NA answer to this question will not be perceived well by the reviewers. 1039
•The claims made should match theoretical and experimental results, and reflect how 1040
much the results can be expected to generalize to other settings. 1041
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 1042
are not attained by the paper. 1043
2.Limitations 1044
Question: Does the paper discuss the limitations of the work performed by the authors? 1045
Answer: [Yes] 1046
Justification: The majority of our results are theorems backed up by mathematical proofs. 1047
We discuss at lenght that balancing improves the value of the regularizer only (it leaves the 1048
valuue of the data-dependent component of the error unchanged). We also mention that 1049
while it would be interesting to study any kind of balance in biological neural networks, 1050
current technnological limirations do not allow recording all the incoming and outgoing 1051
synaptic strengths of a neuron. 1052
Guidelines: 1053
•The answer NA means that the paper has no limitation while the answer No means that 1054
the paper has limitations, but those are not discussed in the paper. 1055
• The authors are encouraged to create a separate "Limitations" section in their paper. 1056
•The paper should point out any strong assumptions and how robust the results are to 1057
violations of these assumptions (e.g., independence assumptions, noiseless settings, 1058
model well-specification, asymptotic approximations only holding locally). The authors 1059
should reflect on how these assumptions might be violated in practice and what the 1060
implications would be. 1061
•The authors should reflect on the scope of the claims made, e.g., if the approach was 1062
only tested on a few datasets or with a few runs. In general, empirical results often 1063
depend on implicit assumptions, which should be articulated. 1064
•The authors should reflect on the factors that influence the performance of the approach. 1065
For example, a facial recognition algorithm may perform poorly when image resolution 1066
is low or images are taken in low lighting. Or a speech-to-text system might not be 1067
used reliably to provide closed captions for online lectures because it fails to handle 1068
technical jargon. 1069
•The authors should discuss the computational efficiency of the proposed algorithms 1070
and how they scale with dataset size. 1071
•If applicable, the authors should discuss possible limitations of their approach to 1072
address problems of privacy and fairness. 1073
•While the authors might fear that complete honesty about limitations might be used by 1074
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 1075
limitations that aren’t acknowledged in the paper. The authors should use their best 1076
judgment and recognize that individual actions in favor of transparency play an impor- 1077
tant role in developing norms that preserve the integrity of the community. Reviewers 1078
will be specifically instructed to not penalize honesty concerning limitations. 1079
3.Theory Assumptions and Proofs 1080
29Question: For each theoretical result, does the paper provide the full set of assumptions and 1081
a complete (and correct) proof? 1082
Answer: [Yes] 1083
Justification: All the theorems and propositions have clear assumptions and all the proofs 1084
are complete and have been checked carefully multiple times. Details of some of the proofs 1085
are provided in the Appendix. 1086
Guidelines: 1087
• The answer NA means that the paper does not include theoretical results. 1088
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 1089
referenced. 1090
•All assumptions should be clearly stated or referenced in the statement of any theorems. 1091
•The proofs can either appear in the main paper or the supplemental material, but if 1092
they appear in the supplemental material, the authors are encouraged to provide a short 1093
proof sketch to provide intuition. 1094
•Inversely, any informal proof provided in the core of the paper should be complemented 1095
by formal proofs provided in appendix or supplemental material. 1096
• Theorems and Lemmas that the proof relies upon should be properly referenced. 1097
4.Experimental Result Reproducibility 1098
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 1099
perimental results of the paper to the extent that it affects the main claims and/or conclusions 1100
of the paper (regardless of whether the code and data are provided or not)? 1101
Answer: [Yes] 1102
Justification: We have provided all the explanations necessary for reproducing the exper- 1103
imental results in the technical appendix and also provided the code for reproducing our 1104
experimental results. 1105
Guidelines: 1106
• The answer NA means that the paper does not include experiments. 1107
•If the paper includes experiments, a No answer to this question will not be perceived 1108
well by the reviewers: Making the paper reproducible is important, regardless of 1109
whether the code and data are provided or not. 1110
•If the contribution is a dataset and/or model, the authors should describe the steps taken 1111
to make their results reproducible or verifiable. 1112
•Depending on the contribution, reproducibility can be accomplished in various ways. 1113
For example, if the contribution is a novel architecture, describing the architecture fully 1114
might suffice, or if the contribution is a specific model and empirical evaluation, it may 1115
be necessary to either make it possible for others to replicate the model with the same 1116
dataset, or provide access to the model. In general. releasing code and data is often 1117
one good way to accomplish this, but reproducibility can also be provided via detailed 1118
instructions for how to replicate the results, access to a hosted model (e.g., in the case 1119
of a large language model), releasing of a model checkpoint, or other means that are 1120
appropriate to the research performed. 1121
•While NeurIPS does not require releasing code, the conference does require all submis- 1122
sions to provide some reasonable avenue for reproducibility, which may depend on the 1123
nature of the contribution. For example 1124
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 1125
to reproduce that algorithm. 1126
(b)If the contribution is primarily a new model architecture, the paper should describe 1127
the architecture clearly and fully. 1128
(c)If the contribution is a new model (e.g., a large language model), then there should 1129
either be a way to access this model for reproducing the results or a way to reproduce 1130
the model (e.g., with an open-source dataset or instructions for how to construct 1131
the dataset). 1132
30(d)We recognize that reproducibility may be tricky in some cases, in which case 1133
authors are welcome to describe the particular way they provide for reproducibility. 1134
In the case of closed-source models, it may be that access to the model is limited in 1135
some way (e.g., to registered users), but it should be possible for other researchers 1136
to have some path to reproducing or verifying the results. 1137
5.Open access to data and code 1138
Question: Does the paper provide open access to the data and code, with sufficient instruc- 1139
tions to faithfully reproduce the main experimental results, as described in supplemental 1140
material? 1141
Answer: [Yes] 1142
Justification: We have provided an anonymous link to our code which is available in the 1143
appendix and also uploaded our code as supplementary material. 1144
Guidelines: 1145
• The answer NA means that paper does not include experiments requiring code. 1146
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 1147
public/guides/CodeSubmissionPolicy ) for more details. 1148
•While we encourage the release of code and data, we understand that this might not be 1149
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 1150
including code, unless this is central to the contribution (e.g., for a new open-source 1151
benchmark). 1152
•The instructions should contain the exact command and environment needed to run to 1153
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 1154
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 1155
•The authors should provide instructions on data access and preparation, including how 1156
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 1157
•The authors should provide scripts to reproduce all experimental results for the new 1158
proposed method and baselines. If only a subset of experiments are reproducible, they 1159
should state which ones are omitted from the script and why. 1160
•At submission time, to preserve anonymity, the authors should release anonymized 1161
versions (if applicable). 1162
•Providing as much information as possible in supplemental material (appended to the 1163
paper) is recommended, but including URLs to data and code is permitted. 1164
6.Experimental Setting/Details 1165
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 1166
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 1167
results? 1168
Answer: [Yes] 1169
Justification: We have provided the required details in the appendix. 1170
Guidelines: 1171
• The answer NA means that the paper does not include experiments. 1172
•The experimental setting should be presented in the core of the paper to a level of detail 1173
that is necessary to appreciate the results and make sense of them. 1174
•The full details can be provided either with the code, in appendix, or as supplemental 1175
material. 1176
7.Experiment Statistical Significance 1177
Question: Does the paper report error bars suitably and correctly defined or other appropriate 1178
information about the statistical significance of the experiments? 1179
Answer: [Yes] 1180
Justification: Error bars are included in all images. 1181
Guidelines: 1182
• The answer NA means that the paper does not include experiments. 1183
31•The authors should answer "Yes" if the results are accompanied by error bars, confi- 1184
dence intervals, or statistical significance tests, at least for the experiments that support 1185
the main claims of the paper. 1186
•The factors of variability that the error bars are capturing should be clearly stated (for 1187
example, train/test split, initialization, random drawing of some parameter, or overall 1188
run with given experimental conditions). 1189
•The method for calculating the error bars should be explained (closed form formula, 1190
call to a library function, bootstrap, etc.) 1191
• The assumptions made should be given (e.g., Normally distributed errors). 1192
•It should be clear whether the error bar is the standard deviation or the standard error 1193
of the mean. 1194
•It is OK to report 1-sigma error bars, but one should state it. The authors should 1195
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 1196
of Normality of errors is not verified. 1197
•For asymmetric distributions, the authors should be careful not to show in tables or 1198
figures symmetric error bars that would yield results that are out of range (e.g. negative 1199
error rates). 1200
•If error bars are reported in tables or plots, The authors should explain in the text how 1201
they were calculated and reference the corresponding figures or tables in the text. 1202
8.Experiments Compute Resources 1203
Question: For each experiment, does the paper provide sufficient information on the com- 1204
puter resources (type of compute workers, memory, time of execution) needed to reproduce 1205
the experiments? 1206
Answer: [Yes] 1207
Justification: We have provided this information in the computer resources section in the 1208
appendix. 1209
Guidelines: 1210
• The answer NA means that the paper does not include experiments. 1211
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 1212
or cloud provider, including relevant memory and storage. 1213
•The paper should provide the amount of compute required for each of the individual 1214
experimental runs as well as estimate the total compute. 1215
•The paper should disclose whether the full research project required more compute 1216
than the experiments reported in the paper (e.g., preliminary or failed experiments that 1217
didn’t make it into the paper). 1218
9.Code Of Ethics 1219
Question: Does the research conducted in the paper conform, in every respect, with the 1220
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 1221
Answer: [Yes] 1222
Justification: The research conducted in our paper conforms, in every respect, with the 1223
NeurIPS Code of Ethics. 1224
Guidelines: 1225
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 1226
•If the authors answer No, they should explain the special circumstances that require a 1227
deviation from the Code of Ethics. 1228
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 1229
eration due to laws or regulations in their jurisdiction). 1230
10.Broader Impacts 1231
Question: Does the paper discuss both potential positive societal impacts and negative 1232
societal impacts of the work performed? 1233
Answer: [NA] 1234
Justification: Our paper has no conceivable direct societal impact. 1235
32Guidelines: 1236
• The answer NA means that there is no societal impact of the work performed. 1237
•If the authors answer NA or No, they should explain why their work has no societal 1238
impact or why the paper does not address societal impact. 1239
•Examples of negative societal impacts include potential malicious or unintended uses 1240
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 1241
(e.g., deployment of technologies that could make decisions that unfairly impact specific 1242
groups), privacy considerations, and security considerations. 1243
•The conference expects that many papers will be foundational research and not tied 1244
to particular applications, let alone deployments. However, if there is a direct path to 1245
any negative applications, the authors should point it out. For example, it is legitimate 1246
to point out that an improvement in the quality of generative models could be used to 1247
generate deepfakes for disinformation. On the other hand, it is not needed to point out 1248
that a generic algorithm for optimizing neural networks could enable people to train 1249
models that generate Deepfakes faster. 1250
•The authors should consider possible harms that could arise when the technology is 1251
being used as intended and functioning correctly, harms that could arise when the 1252
technology is being used as intended but gives incorrect results, and harms following 1253
from (intentional or unintentional) misuse of the technology. 1254
•If there are negative societal impacts, the authors could also discuss possible mitigation 1255
strategies (e.g., gated release of models, providing defenses in addition to attacks, 1256
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 1257
feedback over time, improving the efficiency and accessibility of ML). 1258
11.Safeguards 1259
Question: Does the paper describe safeguards that have been put in place for responsible 1260
release of data or models that have a high risk for misuse (e.g., pretrained language models, 1261
image generators, or scraped datasets)? 1262
Answer: [NA] 1263
Justification: Our paper poses no such risks. 1264
Guidelines: 1265
• The answer NA means that the paper poses no such risks. 1266
•Released models that have a high risk for misuse or dual-use should be released with 1267
necessary safeguards to allow for controlled use of the model, for example by requiring 1268
that users adhere to usage guidelines or restrictions to access the model or implementing 1269
safety filters. 1270
•Datasets that have been scraped from the Internet could pose safety risks. The authors 1271
should describe how they avoided releasing unsafe images. 1272
•We recognize that providing effective safeguards is challenging, and many papers do 1273
not require this, but we encourage authors to take this into account and make a best 1274
faith effort. 1275
12.Licenses for existing assets 1276
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 1277
the paper, properly credited and are the license and terms of use explicitly mentioned and 1278
properly respected? 1279
Answer: [Yes] 1280
Justification: The only assets that we have use are the MNIST and CIFAR-10 datasets and 1281
we have cited these datasets in the paper properly. 1282
Guidelines: 1283
• The answer NA means that the paper does not use existing assets. 1284
• The authors should cite the original paper that produced the code package or dataset. 1285
•The authors should state which version of the asset is used and, if possible, include a 1286
URL. 1287
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 1288
33•For scraped data from a particular source (e.g., website), the copyright and terms of 1289
service of that source should be provided. 1290
•If assets are released, the license, copyright information, and terms of use in the 1291
package should be provided. For popular datasets, paperswithcode.com/datasets 1292
has curated licenses for some datasets. Their licensing guide can help determine the 1293
license of a dataset. 1294
•For existing datasets that are re-packaged, both the original license and the license of 1295
the derived asset (if it has changed) should be provided. 1296
•If this information is not available online, the authors are encouraged to reach out to 1297
the asset’s creators. 1298
13.New Assets 1299
Question: Are new assets introduced in the paper well documented and is the documentation 1300
provided alongside the assets? 1301
Answer: [NA] 1302
Justification: Our paper does not introduce new assets. 1303
Guidelines: 1304
• The answer NA means that the paper does not release new assets. 1305
•Researchers should communicate the details of the dataset/code/model as part of their 1306
submissions via structured templates. This includes details about training, license, 1307
limitations, etc. 1308
•The paper should discuss whether and how consent was obtained from people whose 1309
asset is used. 1310
•At submission time, remember to anonymize your assets (if applicable). You can either 1311
create an anonymized URL or include an anonymized zip file. 1312
14.Crowdsourcing and Research with Human Subjects 1313
Question: For crowdsourcing experiments and research with human subjects, does the paper 1314
include the full text of instructions given to participants and screenshots, if applicable, as 1315
well as details about compensation (if any)? 1316
Answer: [NA] 1317
Justification: Our research does not involve human subjects or crowdsourcing. 1318
Guidelines: 1319
•The answer NA means that the paper does not involve crowdsourcing nor research with 1320
human subjects. 1321
•Including this information in the supplemental material is fine, but if the main contribu- 1322
tion of the paper involves human subjects, then as much detail as possible should be 1323
included in the main paper. 1324
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 1325
or other labor should be paid at least the minimum wage in the country of the data 1326
collector. 1327
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 1328
Subjects 1329
Question: Does the paper describe potential risks incurred by study participants, whether 1330
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 1331
approvals (or an equivalent approval/review based on the requirements of your country or 1332
institution) were obtained? 1333
Answer: [NA] 1334
Justification: Our research does not involve any human subjects. 1335
Guidelines: 1336
•The answer NA means that the paper does not involve crowdsourcing nor research with 1337
human subjects. 1338
34•Depending on the country in which research is conducted, IRB approval (or equivalent) 1339
may be required for any human subjects research. If you obtained IRB approval, you 1340
should clearly state this in the paper. 1341
•We recognize that the procedures for this may vary significantly between institutions 1342
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1343
guidelines for their institution. 1344
•For initial submissions, do not include any information that would break anonymity (if 1345
applicable), such as the institution conducting the review. 1346
35