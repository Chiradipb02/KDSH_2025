Equivariant Machine Learning on Graphs with
Nonlinear Spectral Filters
Ya-Wei Eileen Lin†Ronen Talmon†Ron Levie‡
†Viterbi Faculty of Electrical and Computer Engineering, Technion
‡Faculty of Mathematics, Technion
Abstract
Equivariant machine learning is an approach for designing deep learning mod-
els that respect the symmetries of the problem, with the aim of reducing model
complexity and improving generalization. In this paper, we focus on an extension
of shift equivariance, which is the basis of convolution networks on images, to
general graphs. Unlike images, graphs do not have a natural notion of domain
translation. Therefore, we consider the graph functional shifts as the symmetry
group: the unitary operators that commute with the graph shift operator. Notably,
such symmetries operate in the signal space rather than directly in the spatial space.
We remark that each linear filter layer of a standard spectral graph neural network
(GNN) commutes with graph functional shifts, but the activation function breaks
this symmetry. Instead, we propose nonlinear spectral filters (NLSFs) that are fully
equivariant to graph functional shifts and show that they have universal approxima-
tion properties. The proposed NLSFs are based on a new form of spectral domain
that is transferable between graphs. We demonstrate the superior performance of
NLSFs over existing spectral GNNs in node and graph classification benchmarks.
1 Introduction
In many fields, such as chemistry [ 37], biology [ 36,3], social science [ 9], and computer graphics
[97], data can be described by graphs. In recent years, there has been a tremendous interest in the
development of machine learning models for graph-structured data [ 98,13]. This young field, often
called graph machine learning (graph-ML) orgraph representation learning , has made significant
contributions to the applied sciences, e.g., in protein folding, molecular design, and drug discovery
[45,2,70,89], and has impacted the industry with applications in social media, recommendation
systems, traffic prediction, computer graphics, and natural language processing, among others.
Geometric Deep Learning (GDL) [ 13] is a design philosophy for machine learning models where
the model is constructed to inherently respect symmetries present in the data, aiming to reduce
model complexity and enhance generalization. By incorporating knowledge of these symmetries
into the model, it avoids the need to expend parameters and data to learn them. This inherent
respect for symmetries is automatically generalized to test data, thereby improving generalization
[6,76,25]. For instance, convolutional neural networks (CNNs) respect the translation symmetries
of 2D images, with weight sharing due to these symmetries contributing significantly to their success
[54]. Respecting node re-indexing in a scalable manner revolutionized machine learning on graphs
[13,12] and has placed GNNs as a main general-purpose tool for processing graph-structured data.
Moreover, within GNNs, respecting the 3D Euclidean symmetries of the laws of physics (rotations,
reflections, and translations) led to state-of-the-art performance in molecule processing [24].
Our Contribution. In this paper, we focus on GDL for graph-ML. We consider extensions of shift
symmetries from images to general graphs. Since graphs do not have a natural notion of domain
38th Conference on Neural Information Processing Systems (NeurIPS 2024).translation, as opposed to images, we propose considering functional translations instead. We model
the group of translations on graphs as the group of all unitary operators on signals that commute
with the graph shift operator. Such unitary operators are called graph functional shifts . Note that
each linear filter layer of a standard spectral GNN commutes with graph functional shifts, but the
activation function breaks this symmetry. Instead, we propose non-linear spectral filters (NLSFs) that
are fully equivariant to graph functional shifts and have universal approximation properties.
In Sec. 3, we introduce our NLSFs based on new notions of analysis andsynthesis that map signals
between their node-space representations and spectral representations. Our transforms are related to
standard graph Fourier and inverse Fourier transforms but differ from them in one important aspect.
One key property of our analysis transform is that it is independent of a specific choice of Laplacian
eigenvectors. Hence, our spectral representations are transferable between graphs. In comparison,
standard graph Fourier transforms are based on an arbitrary choice of eigenvectors, and therefore,
the standard frequency domain is not transferable. To achieve transferability, standard graph Fourier
methods resort to linear filter operations based on functional calculus. Since we do not have this
limitation, we can operate on the frequency coefficients with arbitrary nonlinear functions such as
multilayer perceptrons. In Sec. 4, we present theoretical results of our NLSFs, including the universal
approximation and expressivity properties. In Sec. 5, we demonstrate the efficacy of our NLSFs in
node and graph classification benchmarks, where our method outperforms existing spectral GNNs.
2 Background
Notation. ForN∈N, we denote [N] ={1, . . . , N }. We denote matrices by boldface uppercase letter
B, vectors (assumed to be columns) by lowercase boldface b, and the entries of matrices and vectors
are denoted with the same letter in lower case, e.g., B= (bi,j)i,j∈[N]. LetG= ([N],E,A,X)be an
undirected graph with a node set [N], an edge set E ⊂[N]×[N], an adjacency matrix A∈RN×N
representing the edge weights, and a node feature matrix (also called a signal) X∈RN×dcontaining
d-dimensional node attributes. Let Dbe the diagonal degree matrix of G, where the diagonal element
di,iis the degree of node i. Denote by ∆any normal graph shift operator (GSO). For example, ∆
could be the combinatorial graph Laplacian or the normalized graph Laplacian given by L=D−A
andN=D−1
2LD−1
2, respectively. Let ∆=VΛV⊤be the eigendecomposition of ∆, where Vis
the eigenvector matrix and Λ=diag(λ1, . . . , λ N)is the diagonal matrix with eigenvalues (λi)N
i=1
ordered by |λ1| ≤. . .≤ |λN|. An eigenspace is the span of all eigenvectors corresponding to the
same eigenvalue. Let Pi=P∆;idenote the projection upon the i-th eigenspace of ∆in increasing
order of |λ|. We denote the Euclidean norm by ∥X∥2. We define the channel-wise signal norm
∥X∥sigof a feature matrix X∈RN×das the vector ∥X∥sig= (∥X:,j∥2)d
j=1∈Rd, where X:,jis
thej-th column on Xand0≤a≤1. We abbreviate multilayer perceptrons by MLP.
2.1 Linear Graph Signal Processing
Spectral GNNs define convolution operators on graphs via the spectral domain. Given a self-adjoint
graph shift operator (GSO) ∆, e.g., a graph Laplacian, the Fourier modes of the graph are defined
to be the eigenvectors {vi}N
i=1of∆and the eigenvalues {λi}N
i=1are the frequencies. A spectral
filter is defined to directly satisfy the “convolution theorem” [ 11] for graphs. Namely, given a signal
X∈RN×dand a function Q:R→Rd′×d, the operator Q(∆) :RN×d→RN×d′defined by
Q(∆)X:=NX
i=1viv⊤
iXQ(λi)⊤, (1)
is called a filter. Here, d′is the number of output channels. Spectral GNNs, e.g., [ 21,49,61,5], are
graph convolutional networks where convolutions are via Eq. (1), with a trainable function Qat each
layer, and a nonlinear activation function.
2.2 Equivariant GNNs
Equivariance describes the ability of functions fto respect symmetries. It is expressed as f(Hκx) =
Hκf(x), where K ∋κ7→Hκis an action of a symmetry group Kon the domain of f. GNNs [ 83,14],
including spectral GNNs [ 22,49] and subgraph GNNs [ 31,4], are inherently permutation equivariant
2w.r.t. the ordering of nodes. This means that the network’s operations are unaffected by the specific
arrangement of nodes, a property stemming from passive symmetries [ 95] where transformations
are applied to both the graph signals and the graph domain. This permutation equivariance is often
compared to the translation equivariance in CNNs [ 56,57], which involves active symmetries [ 44].
The key difference between the two symmetries lies in the domain: while CNNs operate on a fixed
domain with signals transforming within it, graphs lack a natural notion of domain translation. To
address this, we consider graph functional shifts as the symmetry group, defined by unitary operators
that commute with the graph shift operator. This perspective allows for our NLSFs to be interpreted
as an extension of active symmetry within the graph context, bridging the gap between the passive
and active symmetries inherent to GNNs and CNNs, respectively.
3 Nonlinear Spectral Graph Filters
In this section, we present new concepts of analysis and synthesis under which the spectral domain is
transferrable between graphs. Following these concepts, we introduce new GNNs that are equivariant
tofunctional symmetries – symmetries of the Hilbert space of signals rather than symmetries in the
domain of definition of the signal [64].
3.1 Translation Equivariance of CNNs and GNNs
For motivation, we start with the grid graph Rwith node set [M]2and circular adjacency B, we
define the translation operator Tm,nby[m, n]as
Tm,n∈RM2×M2; (Tm,nx)i,j=xl,k where l= (i−m) mod Mandk= (j−n) mod M .
Note that any Tm,nis a unitary operator that commutes with the grid Laplacian ∆R, i.e.,Tm,n∆R=
∆RTm,n, and therefore it belongs to the group of all unitary operators URthat commute with the grid
Laplacian ∆R. In fact, the space of isotropic convolution operators (with 90orotation and reflection
symmetric filters) can be seen as the space of all normal operators1that commute with unitary
operators from UR[18]. Applying a non-linearity after the convolution retains this equivariance,
and hence, we can build multi-layer CNNs that commute with UR. By the universal approximation
theorem [ 19,34,58], this allows us to approximate any continuous function that commutes with UR.
Note that such translation equivariance cannot be extended to general graphs. Achieving equivariance
to graph functional shifts through linear spectral convolutional layers Q(∆)is straightforward, since
these layers commute with the space of all unitary operators U∆that commute with ∆. However,
introducing non-linearity ρ(Q(∆)X)breaks the symmetry. That is, there exists U∈ U∆such that
ρ(Q(∆)UX)̸=Uρ(Q(∆)X),
where ρis any non-linear activation function, e.g., ReLU, Sigmoid, etc.
This means that multi-layer spectral GNNs do not commute with UG, and are hence not appropriate
as approximators of general continuous functions that commute with UG(see App. A for an example
illustrating how non-linear activation functions break the functional symmetry). Instead, we propose
in this paper a multi-layer GNN that is fully equivariant to UG, which we show to be universal: it can
approximate any continuous graph-signal function (w.r.t. some metric) commuting with UG.
3.2 Graph Functional Symmetries and Their Relaxations
We define the symmetry group of graph functional shifts as follows.
Definition 1 (Graph Functional Shifts) .The space of graph functional shifts is the unitary subgroup
U∆, where a unitary matrix Uis inU∆iff it commutes with the GSO ∆, namely, U∆=∆U.
It is important to note that functional shifts, in general, are not induced from node permutations.
Instead, functional shifts are related to the notion of functional maps [ 73] used in shape correspon-
dence and are general unitary operators that are not permutation matrices in general. The value of the
functionally translated signal at a given node can be a mixture of the content of the original signal at
many different nodes. For example, the functional shift can be a combination of shifts of different
frequencies at different speeds. See App. B for illustrations and examples of functional translations.
1The operator Bis normal iff B∗B=BB∗. Equivalently, iff Bhas an orthogonal eigendecomposition.
3A fundamental challenge with the symmetry group in Def. 1 is its lack of transferability between
different graphs. Hence, we propose to relax this symmetry group. Let g1, . . . , g S:R→Rbe the
indicator functions of the intervals {[ls, ls+1]}S
s=1, which constitute a partition of the frequency band
[l1, lS]⊂R. The operators gj(∆), interpreted via functional calculus Eq. (1), are projections of
the signal space upon band-limited signals. Namely, gj(∆) =P
i:λi∈[lj,lj+1]viv⊤
i. In our work,
we consider filters gjthat are supported on the dyadic sub-bands
λNrS−j+1, λNrS−j
, where
0< r < 1is the decay rate. See Fig. 5 in App. F for an illustrated example. Note that for j= 1, the
sub-band falls in [0, λNrS−1]. The total band [0, lS]is[0, λN].
Definition 2 (Relaxed Functional Shifts) .The space of relaxed functional shifts with respect to the
filter bank {gj}K
j=1(of indicators) is the unitary subgroup Ug
∆, where a unitary matrix Uis inUg
∆iff
it commutes with gj(∆)for all j, namely, Ugj(∆) =gj(∆)U.
Similarly, we can relax functional shifts by restricting to the leading eigenspaces.
Definition 3 (Leading Functional Shifts) .The space of leading functional shifts is the unitary
subgroup UJ
∆, where a unitary Uis inUJ
∆iff it commutes with the eigenspace projections {Pj}J
j=1.
3.3 Analysis and Synthesis
We use the terminology of analysis and synthesis, as in signal processing [ 68], to describe transforma-
tions of signals between their graph and spectral representations. Here, we consider two settings: the
eigenspace projections case and the filter bank (of indicators) case. The definition of the frequency
domain depends on a given signal X∈RN×d, where its projections to the eigenspaces of ∆are
taken as the Fourier modes. Spectral coefficients are modeled as matrices Rthat mix the Fourier
modes, allowing to synthesize signals of general dimensions.
Spectral Index Case. We first define analysis and synthesis using the spectral index up to frequency
J. LetPJ+1=I−PJ
j=1Pjbe the orthogonal complement to the first Jeigenprojections. Let
X∈RN×dbe the graph signal and R∈R(J+1)d×(J+1)edthe spectral coefficients to be synthesized,
whereedrepresents number of output channels. The analysis and synthesis are defined respectively by
Aind(∆,X) =
∥PiX∥sigJ+1
i=1∈R(J+1)dandSind(R;∆,X) ="
PjX
∥PjX∥a
sig+e#J+1
j=1R,(2)
where 0≤a≤1and0< e≪1are parameters that promote stability, and the power
∥PjX∥a
sigas well as the division in Eq. (2)are element-wise operations on each entry. Here,

PjX/(∥PjX∥a
sig+e)J+1
j=1∈RN×(J+1)ddenotes concatenation. We remark that the orthogo-
nal complement in the (J+ 1) -th filter alleviates the loss of information due to projecting to the
low-frequency bands, and therefore, the full spectral range of the signal can be captured. This is
particularly important for heterophilic graphs, which rely on high-frequency components for accurate
label representation. The term index stems from the fact that eigenvalues are treated according to their
index when defining the projections Pj. Note that the synthesis here differs from classic signal pro-
cessing as it depends on a given signal on the graph. When treating ∆andXas fixed, this synthesis
operation is denoted by Sind
∆,X(R) :=Sind(R;∆,X). We similarly denote Aind
∆(X) :=Aind(∆,X).
Filter Bank Case. Similarly, we define the analysis and synthesis in the filter bank up to band gKas
follows. Let gK+1(∆) =I−PK
j=1gj(∆)denote the orthogonal complement to the first Kbands.
LetX∈RN×dbe the graph signal, and let R∈R(K+1)d×(K+1)edrepresent the spectral coefficients
to be synthesized, where edrefers to the general dimension. The analysis and synthesis in the filter
bank case are defined by
Aval(∆,X) =
∥gi(∆)X∥sigK+1
i=1∈R(K+1)dandSval(R;∆,X) ="
gj(∆)X
∥gj(∆)X∥a
sig+e#K+1
j=1R,
(3)
respectively, where a, eare as before. Here,
gj(∆)X/(∥gj(∆)X∥a
sig+e)K+1
j=1∈RN×(K+1)d.
The term value refers to how eigenvalues are used based on their magnitude when defining the
projections gj(∆). As before, we denote Sval
∆,XandAval
∆.
41812121400C
Graph DomainSpectral DomainAnalysis to spectral domain
…Nonlinear spectral filters
InputOutput
Synthesis to graph domainFigure 1: Illustration of nonlinear spectral filters for equivariant machine learning on graphs. Given
a graph G, the node features Xare projected onto eigenspaces (analysis A). The function Ψmap
a sequence of frequency coefficients to a sequence of frequency coefficients. The coefficients are
synthesized to the graph domain using the using S.
In App. C.1, we present a special case of diagonal synthesis where ed=d. In App. D.3, we show that
the diagonal synthesis is stably invertible.
3.4 Definitions of Nonlinear Spectral Filters
We introduce three novel types of non-linear spectral filters (NLSF) : Node-level NLSFs, Graph-level
NLSFs, and Pooling-NLSFs. Fig. 1 illustrates our NLSFs for equivariant machine learning on graphs.
Node-level NLSFs. To be able to transfer NLSFs between different graphs and signals, one key
property of NLSF is that they do not depend on the specific basis chosen in each eigenspace. This
independence is facilitated by the synthesis process, which relies on the input signal X. Following
the spectral index and filter bank cases in Sec. 3.3, we define the Index NLSFs and Value NLSFs by
Θind(∆,X) =S(ind)
∆,X(Ψind(A(ind)
∆(X))) ="
PjX
∥PjX∥a
sig+e#J+1
j=1h
Ψind
∥PiX∥sigiJ+1
i=1, (4)
Θval(∆,X) =S(val)
∆,X(Ψval(A(val)
∆(X))) ="
gj(∆)X
∥gj(∆)X∥a
sig+e#K+1
j=1h
Ψval
∥gi(∆)X∥sigiK+1
i=1,
(5)
where Ψind:R(J+1)d→R(J+1)d×(J+1)edandΨval:R(K+1)d→R(K+1)d×(K+1)edare called
nonlinear frequency responses , andedis the output dimension. To adjust the feature output dimension,
we apply an MLP with shared weights to all nodes after the NLSF. In the case when ed=dand the
filters operator diagonally (i.e., the product and division are element-wise in synthesis), we refer to it
as diag-NLSF. See App. C for more details.
Graph-level NLSFs. We first introduce the Graph-level NLSFs that are fully spectral, where the
NLSFs map a sequence of frequency coefficients to an output vector. Specifically, the Index-based
and Value-based Graph-level NLSFs are given by
Φind(∆,X)=bΨind
∥PiX∥sig
and Φval(∆,X)=bΨval
∥gi(∆)X∥sig
, (6)
wherebΨind:R(J+1)d→Rd′,bΨval:R(K+1)d→Rd′, and d′is the output dimension.
Pooling-NLSFs. We introduce another type of graph-level NLSFs by first representing each graph in
a Node-level NLSFs as in Eq. (4)and Eq. (5). The final graph representation is obtained by applying
a nonlinear activation function followed by a readout function to these node-level representations. We
consider four commonly used pooling methods, including mean, sum, max, and Lp-norm pooling,
as the readout function for each graph. We apply an MLP after readout function to obtain a d′-
dimensional graph-level representation. We term these graph-level NLSFs as Pooling-NLSFs .
5218
…
Index NLSFsValue NLSFs0C
P1P2P3P4P5P6P7P8P9PN…P10
1021214
g1g2g3g4g5
…
1Attention0C
181212140
Figure 2: Illustration of Laplacian attention NLSFs. An attention mechanism is applied to both Index
NLSFs and Value NLSFs, enabling the adaptive selection of the most appropriate parameterization.
3.5 Laplacian Attention NLSFs
To understand which Laplacian and parameterization (index v/s value) of the NLSF are preferable in
different settings, we follow the random geometric graph analysis outlined in [ 74]. Specifically, we
consider a setting where random geometric graphs are sampled from a metric-probability space S.
In such a case, the graph Laplacian approximates continuous Laplacians on the metric spaces under
some conditions. We aim for our NLSFs to produce approximately the same outcome for any two
graphs sampled from the same underlying metric space S, ensuring that the NLSF is transferable .
In App. D.5, we show that if the nodes of the graph are sampled uniformly from S, then using the
graph Laplacian Lin Index NLSFs yields a transferable method. Conversely, if the nodes of the
graph are sampled non-uniformly, and any two balls of the same radius in Shave the same volume,
then utilizing the normalized graph Laplacian Nin Value NLSFs is a transferable method. Given that
graphs may fall between these two boundary cases, we present an architecture that chooses between
the Index NLSFs with respect to Land Value NLSFs with respect to N, as illustrated in Fig. 2. While
the above theoretical setting may not be appropriate as a model for every real-life graph dataset,
it suggests that index NLSF may be more appropriate with L, value NLSFs with N, and different
graphs are more appropriately analyzed by different balances between these two cases.
In the Laplacian attention architecture, a soft attention mechanism is employed to dynamically choose
between the two parameterizations, given by
att(Θind(L,X),Θval(N,X)) =αΘind(L,x)∥(1−α)Θval(N,X),
where 0≤α≤1is obtained using a softmax function to normalize the scores into attention weights,
balancing each NLSFs’ contribution.
4 Theoretical Properties of Nonlinear Spectral Filters
We present the desired theoretical properties of our NLSFs at the node-level and graph-level.
4.1 Complexity of NLSFs
NLSFs are implemented by computing the eigenvectors of the GSO. Most existing spectral GNNs
avoid direct eigendecomposition due to its perceived inefficiency. Instead, they use filters implemented
by applying polynomials [ 49,22] or rational functions [ 61,5] to the GSO in the spatial domain.
However, power iteration-based eigendecomposition algorithms, e.g., variants of the Lanczos method,
can be highly efficient [ 80,55]. For matrices with Enon-zero entries, the computational complexity
of one iteration for finding Jeigenvectors corresponding to the smallest or largest eigenvalues (called
leading eigenvectors ) isO(JE). In practice, these eigendecomposition algorithms converge quickly
due to their super-exponential convergence rate, often requiring only a few iterations, which makes
them as efficient as message passing networks of signals with√
Jchannels.
This makes NLSFs applicable to node-level tasks on large sparse graphs, as demonstrated empirically
in App. F.5, since they rely solely on the leading eigenvectors. In Sec. 4.4, we show that using the
leading eigenvectors can approximate GSOs well in the context of learning on graphs. Note that we
can precompute the spectral projections of the signal before training. For node-level tasks, such as
semi-supervised node classification, the leading eigenvectors only need to be pre-computed once,
with a complexity of O(JE). This During the learning phase , each step of the architecture search
and hyperparameter optimization takes O(NJd)complexity for analysis and synthesis, and O(J2d2)
6for the MLP in the spectral domain, which is faster than the complexity O(Ed2)of message passing
or standard spectral methods if NJ < Ed . Empirical studies on runtime analysis are in App. F.
For dense matrices, the computational complexity of a full eigendecomposition is O(Nb)per iteration,
where Nbis the complexity of matrix multiplication. This is practical for graph-level tasks on
relatively small and dense graphs, which is typical for many graph classification datasets. In these
cases, the eigendecomposition of all graphs in the dataset can be performed as a pre-computation
step, significantly reducing the complexity during the learning phase.
4.2 Equivariance of Node-level NLSFs
We demonstrate the node-level equivariance of our NLSFs, ensuring that our method respects the
functional shift symmetries. The proof is given in App. D.1.
Proposition 1. Index NLSFs in Eq. (4)are equivariant to the graph functional shifts U∆, and Value
NLSFs in Eq. (5)are equivariant to the relaxed graph functional shifts Ug
∆.
4.3 Universal Approximation and Expressivity
In this subsection, we discuss the approximation power of NLSFs.
Node-Level Universal Approximation. We begin with a setting where a graph is given as a fixed
domain, and the data distribution consists of multiple signals defined on this graph. An example of
this setup is a spatiotemporal graph [ 17], e.g., traffic networks, where a fixed sensor system defines a
graph and the different signals represent the sensor readings collected at different times.
In App. D.2.1, we prove the following lemma, which shows that linear NLSFs exhaust the space of
linear operators that commute with graph functional shifts.
Lemma 1. A linear operator RN×d→RN×dcommutes with UJ
∆(resp.Ug
∆) iff it is a NLSF based
on a linear function Ψin Eq. (4)(resp. Eq. (5)).
Lemma 1 shows a close relationship between functions that commute with functional shifts and those
defined in the spectral domain. This motivates the following construction of a pseudo-metric on
RN×d. In the case of relaxed functional shifts, we define the standard Euclidean metric distEin
the spectral domain R(K+1)×d. We pull back the Euclidean metric to the spatial domain to define a
signal pseudo-metric. Namely, for two signals XandX′, their distance is defined by
dist∆ 
X,X′
:= dist E 
A(∆,X),A(∆′,X′)
.
This pseudo metric can be made into a metric by considering each equivalence class of signals with
zero distance as a single point in the space. As MLPs Ψcan approximate any continuous function
R(K+1)×d→R(K+1)×d(the universal approximation theorem [ 19,34,58]), node-level NLSFs
can approximate any continuous function that maps (equivalence classes of) signals to (equivalence
classes of) signals. For details, see App. D.2.2. A similar analysis applies to hard functional shifts.
Graph-Level Universal Approximation. The above analysis also motivates the construction of a
graph-signal metric for graph-level tasks. For graphs with d-channel signals, we consider again the
standard Euclidean metric distEin the spectral domain R(K+1)×d. We define the distance between
any two graphs with GSOs and signals (∆,X)and(∆′,X′)to be
dist 
(∆,X),(∆′,X′)
:= dist E 
A(∆,X),A(∆′,X′)
.
This definition can be extended into a metric by considering the space Gof equivalence classes
of graph-signals with distance 0. As before, this distance inherits the universal approximation
properties of standard MLPs. Namely, any continuous function G →Rd′with respect to distcan be
approximated by NLSFs based on MLPs. Additional details are in App. D.2.3.
Graph-Level Expressivity of Pooling-NLSFs. In App. D.2.4, we show that Pooling-NLSFs are
more expressive than graph-level NLSF when any Lpnorm is used in Eq. (4)and Eq. (5)withp̸= 2,
both in the definition of the NLSF and as the pooling method. Specifically, for every graph-level
NLSF, there is a Pooling-NLSF that coincides with it. Additionally, there are graph signals (∆,X)
and(∆′,X′)for which a Pooling-NLSF can attain different values, whereas any graph-level NLSF
must attain the same value. Hence, Pooling-NLSFs have improved discriminative power compared to
graph-level NLSFs. Indeed, as shown in Tab. 3, Pooling-NLSFs outperform Graph-level NLSFs in
7Table 1: Semi-supervised node classification accuracy.
Cora Citeseer Pubmed Chameleon Squirrel Actor
GCN 81.92 ±0.9 70.73 ±1.1 80.14 ±0.6 43.64 ±1.9 33.26 ±0.8 27.63 ±1.7
GAT 83.64 ±0.7 71.32 ±1.3 79.45 ±0.7 42.19 ±1.3 28.21 ±0.9 29.46 ±0.9
SAGE 74.01 ±2.1 66.40 ±1.2 79.91 ±0.9 41.92 ±0.7 27.64 ±2.1 30.85 ±1.8
ChebNet 79.72 ±1.1 70.48 ±1.0 76.47 ±1.5 44.95 ±1.2 33.82 ±0.8 27.42 ±2.3
ChebNetII 83.95 ±0.8 71.76 ±1.2 81.38 ±1.3 46.37 ±3.1 34.40 ±1.1 33.48 ±1.2
CayleyNet 81.76 ±1.9 68.32 ±2.3 77.48 ±2.1 38.29 ±3.2 26.53 ±3.3 30.62 ±2.8
APPNP 83.19 ±0.8 71.93 ±0.8 82.69 ±1.4 37.43 ±1.9 25.68 ±1.3 35.98 ±1.3
GPRGNN 82.82 ±1.3 70.28 ±1.4 81.31 ±2.6 39.27 ±2.3 26.09 ±1.3 31.47 ±1.6
ARMA 81.64 ±1.2 69.91 ±1.6 79.24 ±0.5 39.40 ±1.8 27.42 ±0.7 30.42 ±2.6
JacobiConv 84.12 ±0.7 72.59 ±1.4 82.05 ±1.9 49.66 ±1.9 33.65 ±0.8 34.61 ±0.7
BernNet 82.96 ±1.1 71.25 ±1.0 81.07 ±1.6 42.65 ±3.4 31.68 ±1.5 33.92 ±0.8
Specformer 82.27 ±0.7 73.45 ±1.4 81.62 ±1.0 49.79 ±1.2 38.24 ±0.9 34.12 ±0.6
OptBasisGNN 81.97 ±1.2 70.46 ±1.6 80.38 ±0.9 47.12 ±2.4 37.66 ±1.1 34.84 ±1.3
att-Node-level NLSFs 85.37 ±1.8 75.41 ±0.8 82.22 ±1.2 50.58 ±1.3 38.39 ±0.9 35.13 ±1.0
practice, which can be attributed to their increased expressivity. We refer to App. D.2.5 for additional
discussion on graph-level expressivity.
4.4 Uniform Approximation of GSOs by Their Leading Eigenvectors
Since NLSFs on large graphs are based on the leading eigenvectors of ∆, we justify its low-rank
approximation in the following. While approximating matrices with low-rank matrices might lead
to a high error in the spectral and Frobenius norms, we show that such an approximation entails
a uniformly small error in the cut norm. We define and interpret the cut norm in App. D.4.1, and
explain why it is a natural graph similarity measure for graph machine learning.
The following theorem is a corollary of the Constructive Weak Regularity Lemma presented in [ 29].
Its proof is presented in App. D.4.
Theorem 1. LetMbe a symmetric matrix with entries bounded by |mi,j| ≤α, and let J∈N.
Suppose mis sampled uniformly from [J], and let R≥1s.t.J/R∈N. Let ϕ1, . . . , ϕmbe the
leading eigenvectors of M, with eigenvalues µ1, . . . , µ mordered by their magnitudes |µ1| ≥. . .≥
|µm|. Define C=Pm
k=1µkϕkϕ⊤
k. Then, with probability 1−1
R(w.r.t. the choice of m),
∥M−C∥□<3α
2r
R
J.
Note that the bound in Thm. 1 is uniformly small, independently of Mand its dimension N. This
theorem justifies using the leading eigenvectors when working with the adjacency matrix as the GSO.
For a justification when working with other GSOs see App. D.4.
5 Experimental Results
We evaluate the NLSFs on node and graph classification tasks. Additional implementation details are
in App. E, and additional experiments, including runtime analysis and ablation studies, are in App. F.
5.1 Semi-Supervised Node Classification
We first demonstrate the main advantage of the proposed Node-level NLSFs over existing GNNs
with convolution design on semi-supervised node classification tasks. We test three citation networks
[84,100]: Cora, Citeseer, and Pubmed. In addition, we explore three heterophilic graphs: Chameleon,
Squirrel, and Actor [79, 90]. For more comprehensive descriptions of these datasets, see App. E.
We compare the Node-level NLSFs using Laplacian attention with existing spectral GNNs for node-
level predictions, including GCN [ 49], ChebNet [ 22], ChebNetII [ 41], CayleyNet [ 61], APPNP [ 50],
GPRGNN [ 16], ARMA [ 5], JacobiConv [ 96], BernNet [ 42], Specformer [ 7], and OptBasisGNN [ 38].
We also consider GAT [ 93] and SAGE [ 39]. For datasets splitting on citation graphs (Cora, Citeseer,
and Pubmed), we apply the standard splits following [ 100], using 20 nodes per class for training,
8500 nodes for validation, and 1000 nodes for testing. For heterophilic graphs (Chameleon, Squirrel,
and Actor), we use the sparse splitting as in [ 16,41], allocating 2.5% of samples for training, 2.5%
for validation, and 95% for testing. We measure the classification quality by computing the average
classification accuracy with a 95% confidence interval over 10 random splits. We utilize the source
code released by the authors for the baseline algorithms and optimize their hyperparameters using
Optuna [ 1]. Each model’s hyperparameters are fine-tuned to achieve the highest possible accuracy.
Detailed hyperparameter settings are provided in App. E.
Tab. 1 presents the node classification accuracy of our NLSFs using Laplacian attention and the
various competing baselines. We see that att-Node-level NLSFs outperform the competing models
on the Cora, Citeseer, and Chameleon datasets. Notably, it shows remarkable performance on the
densely connected Squirrel graph, outperforming the baselines by a large margin. This can be
explained by the sparse version in Eq. (21) of Thm. 1, which shows that the denser the graphs is, the
better its rank- Japproximation. For the Pubmed and Actor datasets, att-Node-level NLSFs yield
the second-best results, which are comparable to the best results obtained by APPNP.
5.2 Node Classification on Filtered Chameleon and Squirrel Datasets in Dense Split Setting
Table 2: Node classification performance on original and
filtered Chameleon and Squirrel datasets.
Chameleon Squirrel
Original Filtered Original Filtered
ResNet+SGC 49.93 ±2.3 41.01 ±4.5 34.36 ±1.2 38.36 ±2.0
ResNet+adj 71.07 ±2.2 38.67 ±3.9 65.46 ±1.6 38.37 ±2.0
GCN 50.18 ±3.3 40.89 ±4.1 39.06 ±1.5 39.47 ±1.5
GPRGNN 47.26 ±1.7 39.93 ±3.3 33.39 ±2.1 38.95 ±2.0
FSGNN 77.85 ±0.5 40.61 ±3.0 68.93 ±1.7 35.92 ±1.3
GloGNN 70.04 ±2.1 25.90 ±3.6 61.21 ±2.0 35.11 ±1.2
FAGCN 64.23 ±2.0 41.90 ±2.7 47.63 ±1.9 41.08 ±2.3
att-Node-level NLSFs 79.84 ±1.2 42.53 ±1.5 68.17 ±1.9 42.66 ±1.7Recently, the work in [ 77] identi-
fied the presence of many duplicate
nodes across the train, validation,
and test splits in the dense split set-
ting of the Chameleon and Squirrel
[75]. This results in train-test data
leakage, causing GNNs to inadver-
tently fit the test splits during train-
ing, thereby making performance re-
sults on Chameleon and Squirrel less
reliable. To further validate the per-
formance of our Node-level NLSFs
on these datasets in the dense split setting, we use both the original and filtered versions of Chameleon
and Squirrel, which do not contain duplicate nodes, as suggested in [ 77]. We use the same random
splits as in [ 77], dividing the datasets into 48% for training, 32% for validation, and 20% for testing.
Tab. 2 presents the classification performance comparison between the original and filtered Chameleon
and Squirrel. The baseline results are taken from [ 77], and we include the following competitive
models: ResNet+SGC [ 77], ResNet+adj [ 77], GCN [ 49], GPRGNN [ 16], FSGNN [ 69], GloGNN
[62], and FAGCN [ 8]. The detailed comparisons are in App. F. We see in the table that the att-
Node-level NLSFs consistently outperform the competing baselines on both the original and filtered
datasets. att-Node-level NLSFs demonstrate less sensitivity to node duplicates and exhibit stronger
generalization ability, further validating the reliability of the Chameleon and Squirrel datasets in the
dense split setting. We note that compared to the dense split setting, the sparse split setting in Tab. 1
is more challenging, resulting in lower classification performance. A similar trend of significant
performance difference between the two settings of Chameleon and Squirrel is also observed in [ 41].
5.3 Graph Classification
We further illustrate the power of NLSFs on eight graph classification benchmarks [ 47]. Specifically,
we consider five bioinformatics datasets [ 10,53,86]: MUTAG, PTC, NCI1, ENZYMES, and
PROTEINS, where MUTAG, PTC, and NCI1 are characterized by discrete node features, while
ENZYMES and PROTEINS have continuous node features. Additionally, we examine three social
network datasets: IMDB-B, IMDB-M, and COLLAB. The unattributed graphs are augmented by
adding node degree features following [26]. For more details of these datasets, see App. E.
In graph classification tasks, a readout function is used to summarize node representations for each
graph. The final graph-level representation is obtained by aggregating these node-level summaries
and is then fed into an MLP with a (log)softmax layer to perform the graph classification task. We
compare our NLSFs with two kernel-based approaches: GK [ 87] and WL [ 86], as well as nine GNNs:
GCN [ 49], GAT [ 93], SAGE [ 39], ChebNet [ 22], ChebNetII [ 41], CayleyNet [ 61], APPNP [ 50],
GPRGNN [ 16], and ARMA [ 5]. Additionally, we consider the hierarchical graph pooling model
9Table 3: Graph classification accuracy.
MUTAG PTC ENZYMES PROTEINS NCI1 IMDB-B IMDB-M COLLAB
GK 76.38 ±2.9 52.13 ±1.2 30.07 ±6.1 72.33 ±4.5 62.62 ±2.2 67.63 ±1.0 44.19 ±0.9 72.32 ±3.7
WL 78.04 ±1.8 52.44 ±1.3 51.29 ±5.9 76.20 ±4.1 76.45 ±2.3 71.42 ±3.2 46.63 ±1.3 76.23 ±2.2
GCN 81.63 ±3.1 60.22 ±1.9 43.66 ±3.4 75.17 ±3.7 76.29 ±1.8 72.96 ±1.3 50.28 ±3.2 79.98 ±1.9
GAT 83.17 ±4.4 62.31 ±1.4 39.83 ±3.7 74.72 ±4.1 74.01 ±4.3 70.62 ±2.5 45.67 ±2.7 74.27 ±2.5
SAGE 75.18 ±4.7 61.33 ±1.1 37.99 ±3.7 74.01 ±4.3 74.90 ±1.7 68.38 ±2.6 46.94 ±2.9 73.61 ±2.1
DiffPool 82.40 ±1.4 56.43 ±2.9 60.13 ±3.2 79.47 ±3.1 77.18 ±0.7 71.09 ±1.6 50.43 ±1.5 80.16 ±1.8
ChebNet 82.15 ±1.6 64.06 ±1.2 50.42 ±1.4 74.28 ±0.9 76.98 ±0.7 73.14 ±1.1 49.82 ±1.6 77.40 ±1.6
ChebNetII 84.17 ±3.1 70.03 ±2.8 64.29 ±2.9 78.31 ±4.1 81.14 ±3.6 77.09 ±3.9 52.69 ±2.7 80.06 ±3.3
CayleyNet 83.06 ±4.2 62.73 ±5.1 42.28 ±5.3 74.12 ±4.7 77.21 ±4.5 71.45 ±4.8 51.89 ±3.9 76.33 ±5.8
APPNP 84.45 ±4.4 65.26 ±6.9 49.68 ±3.9 77.26 ±4.8 73.24 ±7.3 72.94 ±2.3 44.36 ±1.9 73.85 ±3.3
GPRGNN 80.26 ±2.0 58.41 ±1.4 45.29 ±1.7 73.90 ±2.5 73.12 ±4.0 69.17 ±2.6 47.07 ±2.8 77.93 ±1.9
ARMA 83.21 ±2.7 69.23 ±2.2 61.21 ±3.4 76.62 ±3.6 79.51 ±2.9 73.27 ±2.7 53.60 ±1.9 78.34 ±1.1
att-Graph-level NLSFs 84.13 ±1.5 68.17 ±1.0 65.94 ±1.6 82.69 ±1.9 80.51 ±1.2 74.26 ±1.8 52.49 ±0.7 79.06 ±1.2
att-Pooling-NLSFs 86.89 ±1.2 71.02 ±1.3 69.94 ±1.0 84.89 ±0.9 80.95 ±1.4 76.78 ±1.9 55.28 ±1.7 82.19 ±1.3
DiffPool [ 101]. For dataset splitting, we apply the random split following [ 94,101,67,104], using
80% for training, 10% for validation, and 10% for testing. This random splitting process is repeated
10 times, and we report the average performance along with the standard deviation. For the baseline
algorithms, we use the source code released by the authors and optimize their hyperparameters using
Optuna [ 1]. We fine-tune the hyperparameters of each model to achieve the highest possible accuracy.
Detailed hyperparameter settings for both the baselines and our method are provided in App. E.
Tab. 3 presents the graph classification accuracy. Notably, the att-Graph-level NLSFs (i.e., NLSFs
without the synthesis and pooling process) perform the second best on the ENZYMES and PROTEINS.
Additionally, att-Pooling-NLSFs consistently outperform att-Graph-level NLSFs, indicating that
the node features learned in our Node-level NLSFs representation are more expressive, corroborating
our theoretical findings in Sec. 4.3. Furthermore, our att-Pooling-NLSFs consistently outperform
all baselines on the MUTAG, PTC, ENZYMES, PROTEINS, IMDB-M, and COLLAB datasets. For
NCI1 and IMDB-B, att-Pooling-NLSFs rank second and are comparable to ChebNetII.
6 Summary
We presented an approach for defining non-linear filters in the spectral domain of graphs in a
transferable and equivariant way. Transferability between different graphs is achieved by using the
input signal as a basis for the synthesis operator, making the NLSF depend only on the eigenspaces of
the GSO and not on an arbitrary choice of the eigenvectors. While different graph-signals may be of
different sizes, the spectral domain is a fixed Euclidean space independent of the size and topology of
the graph. Hence, our spectral approach represents graphs as vectors. We note that standard spectral
methods do not have this property since the coefficients of the signal in the frequency domain depend
on an arbitrary choice of eigenvectors, while our representation depends only on the eigenspaces.
We analyzed the universal approximation and expressivity power of NLSFs through metrics that
are pulled back from this Euclidean vector space. From a geometric point of view, our NLSFs are
motivated by respecting graph functional shift symmetries, making them related to Euclidean CNNs.
Limitation and Future Work. One limitation of NLSFs is that, when deployed on large graphs, they
only depend on the leading eigenvectors of the GSO and their orthogonal complement. However,
important information can also lie within any other band. In future work, we plan to explore NLSFs
that are sensitive to eigenvalues that lie within a set of bands of interest, which can be adaptive to the
graph.
Acknowledgments
The work of YEL and RT was supported by the European Union’s Horizon 2020 research and
innovation programme under grant agreement No. 802735-ERC-DIFFOP. The work of RL was
supported by the Israel Science Foundation grant No. 1937/23, and the United States - Israel
Binational Science Foundation (NSF-BSF) grant No. 2024660.
10References
[1]T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama. Optuna: A next-generation hyper-
parameter optimization framework. In Proceedings of the 25th ACM SIGKDD international
conference on knowledge discovery & data mining , pages 2623–2631, 2019.
[2]K. Atz, F. Grisoni, and G. Schneider. Geometric deep learning on molecular representations.
Nature Machine Intelligence , 3:1023–1032, 2021.
[3]A.-L. Barabasi and Z. N. Oltvai. Network biology: understanding the cell’s functional
organization. Nature reviews genetics , 5(2):101–113, 2004.
[4]B. Bevilacqua, F. Frasca, D. Lim, B. Srinivasan, C. Cai, G. Balamurugan, M. M. Bronstein,
and H. Maron. Equivariant subgraph aggregation networks. In International Conference on
Learning Representations , 2022.
[5]F. M. Bianchi, D. Grattarola, L. Livi, and C. Alippi. Graph neural networks with convolutional
ARMA filters. IEEE transactions on pattern analysis and machine intelligence , 44(7):3496–
3507, 2021.
[6]A. Bietti, L. Venturi, and J. Bruna. On the sample complexity of learning with geometric
stability. arXiv preprint arXiv:2106.07148 , 2021.
[7]D. Bo, C. Shi, L. Wang, and R. Liao. Specformer: Spectral graph neural networks meet
transformers. In The Eleventh International Conference on Learning Representations , 2023.
[8]D. Bo, X. Wang, C. Shi, and H. Shen. Beyond low-frequency information in graph convolu-
tional networks. In Proceedings of the AAAI conference on artificial intelligence , volume 35,
pages 3950–3957, 2021.
[9]S. P. Borgatti, A. Mehra, D. J. Brass, and G. Labianca. Network analysis in the social sciences.
science , 323(5916):892–895, 2009.
[10] K. M. Borgwardt, C. S. Ong, S. Schönauer, S. Vishwanathan, A. J. Smola, and H.-P. Kriegel.
Protein function prediction via graph kernels. Bioinformatics , 21(suppl_1):i47–i56, 2005.
[11] R. Bracewell and P. B. Kahn. The fourier transform and its applications. American Journal of
Physics , 34(8):712–712, 1966.
[12] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veli ˇckovi ´c. Geometric deep learning: Grids,
groups, graphs, geodesics, and gauges. preprint arXiv:2104.13478 , 2021.
[13] M. M. Bronstein, J. Bruna, Y . LeCun, A. Szlam, and P. Vandergheynst. Geometric deep
learning: Going beyond euclidean data. IEEE Signal Processing Magazine , 34(4):18–42,
2017.
[14] J. Bruna, W. Zaremba, A. Szlam, and Y . LeCun. Spectral networks and locally connected
networks on graphs. arXiv preprint arXiv:1312.6203 , 2013.
[15] D. Burago, S. O. Ivanov, and Y . Kurylev. Spectral stability of metric-measure laplacians. Israel
Journal of Mathematics , 232:125–158, 2019.
[16] E. Chien, J. Peng, P. Li, and O. Milenkovic. Adaptive universal generalized pagerank graph
neural network. In International Conference on Learning Representations , 2020.
[17] A. Cini and I. Marisca. Torch Spatiotemporal, 3 2022.
[18] T. Cohen and M. Welling. Group equivariant convolutional networks. In ICML , pages
2990–2999. PMLR, 2016.
[19] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of
control, signals and systems , 2(4):303–314, 1989.
11[20] A. Daigavane, S. Kim, M. Geiger, and T. Smidt. Symphony: Symmetry-equivariant point-
centered spherical harmonics for molecule generation. arXiv preprint arXiv:2311.16199 ,
2023.
[21] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs
with fast localized spectral filtering. In NeurIPS . Curran Associates Inc., 2016.
[22] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs
with fast localized spectral filtering. Advances in neural information processing systems , 29,
2016.
[23] L. Du, X. Shi, Q. Fu, X. Ma, H. Liu, S. Han, and D. Zhang. Gbk-gnn: Gated bi-kernel graph
neural networks for modeling both homophily and heterophily. In Proceedings of the ACM
Web Conference 2022 , pages 1550–1558, 2022.
[24] A. Duval, S. V . Mathis, C. K. Joshi, V . Schmidt, S. Miret, F. D. Malliaros, T. Cohen, P. Liò,
Y . Bengio, and M. Bronstein. A hitchhiker’s guide to geometric gnns for 3d atomic systems,
2024.
[25] B. Elesedy and S. Zaidi. Provably strict generalisation benefit for equivariant models. ICML ,
2021.
[26] F. Errica, M. Podda, D. Bacciu, and A. Micheli. A fair comparison of graph neural networks
for graph classification. In International Conference on Learning Representations , 2019.
[27] P. Esser, L. Chennuru Vankadara, and D. Ghoshdastidar. Learning theory can (sometimes)
explain generalisation in graph neural networks. Advances in Neural Information Processing
Systems , 34:27043–27056, 2021.
[28] M. Fey and J. E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds , 2019.
[29] B. Finkelshtein, ˙I.˙I. Ceylan, M. Bronstein, and R. Levie. Learning on large graphs using
intersecting communities. arXiv preprint arXiv:2405.20724 , 2024.
[30] M. Finzi, S. Stanton, P. Izmailov, and A. G. Wilson. Generalizing convolutional neural networks
for equivariance to lie groups on arbitrary continuous data. In International Conference on
Machine Learning , pages 3165–3176. PMLR, 2020.
[31] F. Frasca, B. Bevilacqua, M. Bronstein, and H. Maron. Understanding and extending subgraph
GNN by rethinking their symmetries. Advances in Neural Information Processing Systems ,
35:31376–31390, 2022.
[32] A. M. Frieze and R. Kannan. Quick approximation to matrices and applications. Combinator-
ica, 1999.
[33] F. Fuchs, D. Worrall, V . Fischer, and M. Welling. SE(3)-transformers: 3d roto-translation
equivariant attention networks. Advances in neural information processing systems , 33:1970–
1981, 2020.
[34] K.-I. Funahashi. On the approximate realization of continuous mappings by neural networks.
Neural networks , 2(3):183–192, 1989.
[35] V . Garg, S. Jegelka, and T. Jaakkola. Generalization and representational limits of graph neural
networks. In International Conference on Machine Learning , pages 3419–3430. PMLR, 2020.
[36] T. Gaudelet, B. Day, A. R. Jamasb, J. Soman, C. Regep, G. Liu, J. B. Hayter, R. Vickers,
C. Roberts, J. Tang, et al. Utilizing graph machine learning within drug discovery and
development. Briefings in bioinformatics , 22(6):bbab159, 2021.
[37] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing
for quantum chemistry. In International conference on machine learning , pages 1263–1272.
PMLR, 2017.
12[38] Y . Guo and Z. Wei. Graph neural networks with learnable and optimal polynomial bases. In
International Conference on Machine Learning , pages 12077–12097. PMLR, 2023.
[39] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs.
Advances in Neural Information Processing Systems , 30, 2017.
[40] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770–
778, 2016.
[41] M. He, Z. Wei, and J.-R. Wen. Convolutional neural networks on graphs with chebyshev
approximation, revisited. Advances in Neural Information Processing Systems , 35:7264–7276,
2022.
[42] M. He, Z. Wei, H. Xu, et al. Bernnet: Learning arbitrary graph spectral filters via bernstein
approximation. Advances in Neural Information Processing Systems , 34:14239–14251, 2021.
[43] W. Hoeffding. Probability inequalities for sums of bounded random variables. The collected
works of Wassily Hoeffding , pages 409–426, 1994.
[44] N. Huang, R. Levie, and S. Villar. Approximately equivariant graph networks. Advances in
Neural Information Processing Systems , 36, 2024.
[45] J. M. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvu-
nakool, R. Bates, A. Zídek, A. Potapenko, A. Bridgland, C. Meyer, S. A. A. Kohl, A. Ballard,
A. Cowie, B. Romera-Paredes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. A.
Reiman, E. Clancy, M. Zielinski, M. Steinegger, M. Pacholska, T. Berghammer, S. Bodenstein,
D. Silver, O. Vinyals, A. W. Senior, K. Kavukcuoglu, P. Kohli, and D. Hassabis. Highly
accurate protein structure prediction with alphafold. Nature , 596:583 – 589, 2021.
[46] N. Keriven and G. Peyré. Universal invariant and equivariant graph neural networks. Advances
in Neural Information Processing Systems , 32, 2019.
[47] K. Kersting, N. M. Kriege, C. Morris, P. Mutzel, and M. Neumann. Benchmark data sets for
graph kernels. 2016.
[48] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[49] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks.
InInternational Conference on Learning Representations , 2016.
[50] J. Klicpera, A. Bojchevski, and S. Günnemann. Predict then propagate: Graph neural networks
meet personalized pagerank. In International Conference on Learning Representations , 2019.
[51] M. Kofinas, B. Knyazev, Y . Zhang, Y . Chen, G. J. Burghouts, E. Gavves, C. G. M. Snoek,
and D. W. Zhang. Graph neural networks for learning equivariant representations of neural
networks. 2024.
[52] N. J. Korevaar and R. M. Schoen. Sobolev spaces and harmonic maps for metric space targets.
Communications in Analysis and Geometry , 1:561–659, 1993.
[53] N. Kriege and P. Mutzel. Subgraph matching kernels for attributed graphs. arXiv preprint
arXiv:1206.6483 , 2012.
[54] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional
neural networks. In Advances in Neural Information Processing Systems , volume 25. Curran
Associates, Inc., 2012.
[55] C. Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential
and integral operators. 1950.
[56] Y . LeCun, Y . Bengio, et al. Convolutional networks for images, speech, and time series. The
handbook of brain theory and neural networks , 3361(10):1995, 1995.
13[57] Y . LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision.
InProceedings of 2010 IEEE international symposium on circuits and systems , pages 253–256.
IEEE, 2010.
[58] M. Leshno, V . Y . Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a
nonpolynomial activation function can approximate any function. Neural networks , 6(6):861–
867, 1993.
[59] R. Levie. A graphon-signal analysis of graph neural networks. In NeurIPS , 2023.
[60] R. Levie, W. Huang, L. Bucci, M. Bronstein, and G. Kutyniok. Transferability of spectral
graph convolutional neural networks. Journal of Machine Learning Research , 22(272):1–59,
2021.
[61] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein. Cayleynets: Graph convolutional neural
networks with complex rational spectral filters. IEEE Transactions on Signal Processing ,
67(1):97–109, 2018.
[62] X. Li, R. Zhu, Y . Cheng, C. Shan, S. Luo, D. Li, and W. Qian. Finding global homophily in
graph neural networks when meeting heterophily. In International Conference on Machine
Learning , pages 13242–13256. PMLR, 2022.
[63] D. Lim, F. Hohne, X. Li, S. L. Huang, V . Gupta, O. Bhalerao, and S. N. Lim. Large scale
learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances
in Neural Information Processing Systems , 34:20887–20902, 2021.
[64] D. Lim, J. Robinson, S. Jegelka, and H. Maron. Expressive sign equivariant networks for
spectral geometric learning. Advances in Neural Information Processing Systems , 36, 2024.
[65] L. Lovász and B. Szegedy. Szemerédi’s lemma for the analyst. GAFA Geometric And
Functional Analysis , 2007.
[66] L. M. Lovász. Large networks and graph limits. In volume 60 of Colloquium Publications ,
2012.
[67] Y . Ma, S. Wang, C. C. Aggarwal, and J. Tang. Graph convolutional networks with eigenpooling.
InProceedings of the 25th ACM SIGKDD international conference on knowledge discovery &
data mining , pages 723–731, 2019.
[68] S. G. Mallat. A theory for multiresolution signal decomposition: the wavelet representation.
IEEE transactions on pattern analysis and machine intelligence , 11(7):674–693, 1989.
[69] S. K. Maurya, X. Liu, and T. Murata. Simplifying approach to node classification in graph
neural networks. Journal of Computational Science , 62:101695, 2022.
[70] O. Méndez-Lucio, M. Ahmad, E. A. del Rio-Chanona, and J. K. Wegner. A geometric deep
learning approach to predict binding conformations of bioactive molecules. Nature Machine
Intelligence , 3:1033–1039, 2021.
[71] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric
deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages 5115–5124, 2017.
[72] C. Morris, N. M. Kriege, F. Bause, K. Kersting, P. Mutzel, and M. Neumann. Tudataset: A
collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663 ,
2020.
[73] M. Ovsjanikov, M. Ben-Chen, J. Solomon, A. Butscher, and L. Guibas. Functional maps:
A flexible representation of maps between shapes. ACM Transactions on Graphics (ToG) ,
31(4):1–11, 2012.
[74] R. Paolino, A. Bojchevski, S. Günnemann, G. Kutyniok, and R. Levie. Unveiling the sampling
density in non-uniform geometric graphs. In The Eleventh International Conference on
Learning Representations , 2023.
14[75] H. Pei, B. Wei, K. C.-C. Chang, Y . Lei, and B. Yang. Geom-gcn: Geometric graph convolu-
tional networks. In International Conference on Learning Representations , 2020.
[76] M. Petrache and S. Trivedi. Approximation-generalization trade-offs under (approximate)
group equivariance. Advances in Neural Information Processing Systems , 36, 2024.
[77] O. Platonov, D. Kuznedelev, M. Diskin, A. Babenko, and L. Prokhorenkova. A critical look at
the evaluation of gnns under heterophily: Are we really making progress? In The Eleventh
International Conference on Learning Representations , 2023.
[78] O. Puny, D. Lim, B. Kiani, H. Maron, and Y . Lipman. Equivariant polynomials for graph
neural networks. In International Conference on Machine Learning , pages 28191–28222.
PMLR, 2023.
[79] B. Rozemberczki, C. Allen, and R. Sarkar. Multi-scale attributed node embedding. Journal of
Complex Networks , 9(2):cnab014, 2021.
[80] Y . Saad. Numerical methods for large eigenvalue problems: revised edition . SIAM, 2011.
[81] R. Sato, M. Yamada, and H. Kashima. Random features strengthen graph neural networks. In
Proceedings of the 2021 SIAM international conference on data mining (SDM) , pages 333–341.
SIAM, 2021.
[82] V . G. Satorras, E. Hoogeboom, and M. Welling. E (n) equivariant graph neural networks. In
International conference on machine learning , pages 9323–9332. PMLR, 2021.
[83] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural
network model. IEEE transactions on neural networks , 20(1):61–80, 2008.
[84] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classifi-
cation in network data. AI magazine , 29(3):93–93, 2008.
[85] J.-P. Serre et al. Linear representations of finite groups , volume 42. Springer, 1977.
[86] N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, and K. M. Borgwardt.
Weisfeiler-Lehman graph kernels. Journal of Machine Learning Research , 12(9), 2011.
[87] N. Shervashidze, S. Vishwanathan, T. Petri, K. Mehlhorn, and K. Borgwardt. Efficient graphlet
kernels for large graph comparison. In Artificial intelligence and statistics , pages 488–495.
PMLR, 2009.
[88] Y . Shi, Z. Huang, S. Feng, H. Zhong, W. Wang, and Y . Sun. Masked label prediction: Unified
message passing model for semi-supervised classification. arXiv preprint arXiv:2009.03509 ,
2020.
[89] J. M. Stokes, K. Yang, K. Swanson, W. Jin, A. Cubillos-Ruiz, N. M. Donghia, C. R. MacNair,
S. French, L. A. Carfrae, Z. Bloom-Ackermann, et al. A deep learning approach to antibiotic
discovery. Cell, 180(4):688–702, 2020.
[90] J. Tang, J. Sun, C. Wang, and Z. Yang. Social influence analysis in large-scale networks. In
Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and
data mining , pages 807–816, 2009.
[91] N. Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff, and P. Riley. Tensor field
networks: Rotation-and translation-equivariant neural networks for 3D point clouds. arXiv
preprint arXiv:1802.08219 , 2018.
[92] L. N. Trefethen and D. Bau. Numerical Linear Algebra, Twenty-fifth Anniversary Edition .
Society for Industrial and Applied Mathematics, 2022.
[93] P. Veli ˇckovi ´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention
networks. arXiv preprint arXiv:1710.10903 , 2017.
[94] P. Velickovic, W. Fedus, W. L. Hamilton, P. Liò, Y . Bengio, and R. D. Hjelm. Deep graph
infomax. ICLR , 2(3):4, 2019.
15[95] S. Villar, D. W. Hogg, W. Yao, G. A. Kevrekidis, and B. Schölkopf. Towards fully covariant
machine learning. arXiv preprint arXiv:2301.13724 , 2023.
[96] X. Wang and M. Zhang. How powerful are spectral graph neural networks. In International
conference on machine learning , pages 23341–23362. PMLR, 2022.
[97] Y . Wang, Y . Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon. Dynamic graph
cnn for learning on point clouds. ACM Transactions on Graphics (tog) , 38(5):1–12, 2019.
[98] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y . Philip. A comprehensive survey on graph
neural networks. IEEE transactions on neural networks and learning systems , 32(1):4–24,
2020.
[99] K. Xu, M. Zhang, J. Li, S. S. Du, K.-i. Kawarabayashi, and S. Jegelka. How neural networks
extrapolate: From feedforward to graph neural networks. arXiv preprint arXiv:2009.11848 ,
2020.
[100] Z. Yang, W. Cohen, and R. Salakhudinov. Revisiting semi-supervised learning with graph
embeddings. In International conference on machine learning , pages 40–48. PMLR, 2016.
[101] Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec. Hierarchical graph
representation learning with differentiable pooling. Advances in neural information processing
systems , 31, 2018.
[102] J. Zhu, R. A. Rossi, A. Rao, T. Mai, N. Lipka, N. K. Ahmed, and D. Koutra. Graph neural
networks with heterophily. In Proceedings of the AAAI conference on artificial intelligence ,
volume 35, pages 11168–11176, 2021.
[103] J. Zhu, Y . Yan, L. Zhao, M. Heimann, L. Akoglu, and D. Koutra. Beyond homophily in graph
neural networks: Current limitations and effective designs. Advances in neural information
processing systems , 33:7793–7804, 2020.
[104] Y . Zhu, Y . Xu, F. Yu, Q. Liu, S. Wu, and L. Wang. Graph contrastive learning with adaptive
augmentation. In Proceedings of the Web Conference 2021 , pages 2069–2080, 2021.
16A Non-linear Activation Functions Break the Functional Symmetry
We offer a construction for complex-valued signals and first order derivative as the graph Laplacian,
which is easy to extend to the real case and second order derivative. Consider a circular 1D grid with
100nodes and the first order central difference as the Laplacian. Here, the Laplacian eigenvectors
are the standard Fourier modes. In this case, one can see that a graph functional shift is any operator
that is diagonal in the frequency domain and multiplies each frequency by any complex number with
the unit norm. Consider the nonlinearity that takes the real part of the signal and then applies ReLU,
which we denote in short by ReLU. Consider a graph signal x= (eiπ10n/100+eiπ20n/100)99
n=0. We
consider a graph functional shift Sthat shifts frequencies 10 and 20 at a distance of 5, and every other
frequency is not shifted. Namely, frequency 10 is multiplied by e−iπ50/100=−i, frequency 20 by
e−iπ=−iπ100/100=−1, and every other frequency is multiplied by 1. Consider also the classical shift
Dthat translates the whole signal by 5uniformly. Since xconsists only of the frequencies 10 and 20,
it is easy to see that Sx=Dx. Hence, ReLU (Sx) =ReLU (Dx) =D(ReLU (x)). Conversely, if
we apply ReLU (x)and only then shift, note that ReLU (x)consists of many frequencies in addition
to 10 and 20. For example, by nonnegativity of ReLU ,ReLU (x)has a nonzero DC component
(zeroth frequency). Now, S(ReLU (x))only translates the 10 and 20 frequencies, so we have
S(ReLU (x))̸=D(ReLU (x)) = ReLU (S(x)).
B Illustrating Functional Translations
Here we present an additional discussion and illustrations on the new notions of symmetry.
B.1 Functional Translation of a Gaussian Signal with Different Speeds at Different
Frequencies
To illustrate the concepts of relaxed symmetry and functional translation, we present a toy example
involving the classical translation and a functional translation of a Gaussian signal on a 2D grid with
a standard deviation of 1.
The classical translation involves moving the entire signal uniformly across the grid. This uniform
movement can be represented as I′(x, y) =I(x−tx, y−ty), where txandtyare the translation
amounts in the xandydirections, respectively, and x−txandy−tyare circular translations,
namely, subtractions modulo the size of the circular grid. For simplicity, we consider tx=ty=t.
For instance, if t= 5, the Gaussian is shifted by 5 units in both the xandydirections. Fig. 3
top-row shows the classical translation for t= 0,t= 5,t= 10 , and t= 15 . We see that every
part of the signal shifts at the same rate and direction, preserving the overall shape of the Gaussian
signal. Note that classical translation is equivalent to modulation of the frequency domain, i.e.
bI′(u, v) =bI(u, v)e−i2π(utx+vty), where bI(u, v)is the Fourier transform of I(x, y). Therefore, we
can view the classical translation as a specific type of functional translation.
Next, we illustrate a functional translation that is based on a frequency-dependent movement.
Specifically, the Gaussian signal is decomposed into low and high-frequency components based
on two indicator band-pass filters. In our example, the translation parameter tis different for the
low and high-frequency components: low frequencies are shifted by tlowwhile high frequencies
are shifted by thigh. This functional translation is defined via modulations in the frequency do-
main given by bI′
low(u, v) =bIlow(u, v)e−i2π(utx,low +vty,low )for low-frequency components and
bI′
high(u, v) =bIhigh(u, v)e−i2π(utx,high +vty,high )for high-frequency components. The combined
functionally translated signal in the frequency domain is then bI′= (bI′
low,bI′
high). Fig. 3 bottom-row
demonstrates the functional translation for t= 0,t= 5,t= 10 , and t= 15 . We observe that the
low-frequency components (smooth parts) of the signal move at one speed, while high-frequency
components move at another. This demonstrates that functional symmetries are typically more rich
than domain symmetries.
17Classical Translation t=0
 Classical Translation t=5
 Classical Translation t=10
 Classical Translation t=15
Functional Translation t=0
 Functional Translation t=5
 Functional Translation t=10
 Functional Translation t=15Figure 3: Comparison of classical and functional translation of a Gaussian signal. Top Row (Classical
Translation): The Gaussian signal moves uniformly across the grid without changing shape. Bottom
Row (Functional Translation): The Gaussian signal translates as low-frequency components move at
different speeds than high-frequency components, demonstrating relaxed symmetry.
B.2 Robust Graph Functional Shifts in Image Translation and MNIST Classification on
Perturbed Grids
We present another toy example to illustrate that functional translations are more stable than hard
symmetries of the graph, namely, graph automorphisms (isomorphisms from the graph to itself).
Automorphisms are another analog to translations on general graphs, which competes with our notion
of functional shifts. Consider as an example the standard 2D circular grid (discretizing the torus).
The automorphisms of the grid include all translations. However, this notion of hard symmetry is very
sensitive to graph perturbations. If one adds or deletes even one edge in the graph, the automorphism
group becomes much smaller and does not contain the translations anymore.
In contrast, we claim that functional shifts are not so sensitive to graph perturbations. To demonstrate
this empirically, we conducted the following toy experiment. We add a Gaussian noise to the edge
weights of the 2D grid to create a perturbed graph. Given a standard domain shift, we optimize the
coefficients of a functional shift so it is as close as possible to the classical domain shift of the clean
grid in Frobenius error. Fig. 4 presents an example of classically and functionally shifted image of
the digit 5. The original digit (left), a classical domain translation of the clean grid (middle), and a
functional translation constructed from the perturbed graph (right) to match the classical translation.
We see that standard translations can be approximated by a graph functional shift on a perturbed grid.
We further demonstrate the robustness to graph perturbations of NLSFs trained on MNIST classi-
fication. The experimental setup follows previous work in [ 22,71,61]. We compare the NLSF on
the clean grid (i.e., without Gaussian noise) to the NLSF on the perturbed grid. Tab. 4 presents the
classification accuracy of the NLSF. We see that the classification performance is almost unaffected
by the perturbation, indicating that NLSFs on the perturbed grid can roughly express CNN-like
operations (translation equivariant functions).
OriginalTranslatedFunc. translated
Figure 4: Approximate a standard translation by
functional translation on a perturbed graph.MNIST Perturbed MNIST
Ours 99.19 99.16
Table 4: Classification accuracy on the MNIST
and perturbed MNIST using NLSFs.
18C Special Cases of NLSFs
We present two special cases of NLSFs as follows.
C.1 Diagonal NLSFs
In Sec. 3, we introduced the NLSFs with the output dimension ed, which is a tunable hyperparameter.
Here, we present a special case when ed=dsuch that the multiplication and division in synthesis are
operated diagonally. Specifically, the diagonal analysis and synthesis in the spectral index case and in
the filter bank case are defined respectively by
Aind, diag(∆,X) =
∥PjX∥sigJ+1
j=1∈R(J+1)×dand (7)
Aval, diag(∆,X) =
∥gj(∆)X∥sigK+1
j=1∈R(K+1)×d(8)
and
Sind, diag(R;∆,X) =J+1X
j=1rj⊙PjX
∥PjX∥a
sig+eand (9)
Sval, diag(R;∆,X) =K+1X
j=1rj⊙gj(∆)X
∥gj(∆)X∥a
sig+e, (10)
where the product and division are element-wise along the channel direction. That is, rj⊙
PjX
∥PjX∥a
sig+e=h
rj1PjX:,1
∥PjX:,1∥a
2+e, . . . , r jdPjX:,d
∥PjX:,d∥a
2+ei
in the spectral index case (resp. rj⊙
gj(∆)X
∥gj(∆)X∥a
sig+e=h
rj1gj(∆)X:,1
∥gj(∆)X:,1∥a
2+e, . . . , r jdgj(∆)X:,d
∥gj(∆)X:,d∥a
2+ei
in the filter bank case). Here,
R= (rj)J+1
j=1∈R(J+1)×din the spectral index case (resp. R= (rj)K+1
j=1∈R(K+1)×din the
filter bank case) are the spectral coefficients to be synthesized and a, eare as before.
For Node-level diag-NLSFs, we define the Index diag-NLSFs and Value diag-NLSFs as follows
Θind, diag (∆,X) =S(ind, diag )
∆,X (Ψ(A(ind)
∆(X))) =J+1X
j=1
Ψ
∥PiX∥sigJ+1
i=1
jPjX
∥PjX∥a
sig+e,(11)
Θval, diag (∆,X) =S(val, diag )
∆,X (Ψ(A(val)
∆(X))) =K+1X
j=1
Ψ
∥gi(∆)X∥sigK+1
i=1
jgj(∆)X
∥gj(∆)X∥a
sig+e,
(12)
where Ψ :R(J+1)d→R(J+1)dinΘind, diag andΨ :R(K+1)d→R(K+1)dinΘval, diag . To adjust the
feature output dimension at each node, we apply an MLP with shared weights to all nodes after the
NLSF. We present the empirical study of diag-NLSFs in App. F.7.
C.2 Leading NLSFs
In Sec. 3.3, we introduce the NLSFs with the orthogonal complement. Specifically, the (J+ 1) -th
filter in the Index NLSFs is given by PJ+1=I−PJ
j=1Pj, and the (K+ 1)-th filter in Vale NLSFs
is defined as gK+1(∆) =I−PK
j=1gj(∆). To explore the effects of the orthogonal complement, we
focus on the leading NLSFs in both the Index NLSFs and Vale NLSFs, considering only the first J
andKfilters without including their orthogonal complements. In this case, the analysis and synthesis
in the spectral index case and in the filter bank case are respectively given by
Aind, lead(R;∆,X) =
∥PiX∥sigJ
i=1∈RJdand
Aval, lead(R;∆,X) =
∥gi(∆)X∥sigK
i=1∈RKd,(13)
19and
Sind, lead(R;∆,X) ="
PjX
∥PjX∥a
sig+e#J
j=1Rand
Sval, lead(R;∆,X) ="
gj(∆)X
∥gj(∆)X∥a
sig+e#K
j=1R,(14)
where Pjandgj(∆)are defined as in Sec. 3.2. The lead-NLSFs are then defined by
Θind, lead (∆,X) =S(ind, lead )
∆,X (Ψind(A(ind)
∆(X)))
="
PjX
∥PjX∥a
sig+e#J
j=1h
Ψind
∥PiX∥sigiJ
i=1,(15)
Θval, ind(∆,X) =S(val, ind )
∆,X(Ψval(A(val)
∆(X)))
="
gj(∆)X
∥gj(∆)X∥a
sig+e#K
j=1h
Ψval
∥gi(∆)X∥sigiK
i=1,(16)
where Ψind:RJd→RJd×Jed,Ψval:RKd→RKd×Ked, andedis the output dimension. To adjust the
feature output dimension, we apply an MLP with shared weights to all nodes after the NLSF.
Similar to App. C.1, when considering ed=dsuch that the multiplication and division in synthesis
are operated diagonally, we have the lead-diag-NLSFs defined by
Θind, lead, diag (∆,X) =S(ind, lead, diag )
∆,X (Ψ(A(ind)
∆(X)))
=JX
j=1
Ψ
∥PiX∥sigJ
i=1
jPjX
∥PjX∥a
sig+e,(17)
Θval, lead, diag (∆,X) =S(val, lead, diag )
∆,X (Ψ(A(val)
∆(X)))
=KX
j=1
Ψ
∥gi(∆)X∥sigK
i=1
jgj(∆)X
∥gj(∆)X∥a
sig+e,(18)
where Ψ :RJd→RJdinΘind, lead, diag andΨ :RKd→RKdinΘval, lead, diag . We present the
empirical study of lead-NLSFs and lead-diag-NLSFs in App. F.7.
D Theoretical Analysis
We note that the numbering of the statements corresponds to the numbering used in the paper, and we
have also included several separately numbered propositions and lemmas that are used in supporting
the proofs presented. We restate the claim of each statement for convenience.
D.1 Equivariance of NLSFs
We start with two simple lemmas that characterize the graph functional shifts. The lemmas directly
follow the fact that two normal operators commute iff the projections upon their eigenspaces commute.
Projection to Eigenspaces Case. Note that
RN=
J+1Y
j=1PjRN
,
whereQdenotes direct products of linear spaces and the (J+1)-th filter is the orthogonal complement,
given by PJ+1=I−PJ
j=1Pj. Denote by ⊕the direct sum of operators.
20Lemma D.1. Uis inUJ
∆iff it has the form U=LJ+1
j=1Uj
, where Ujis a unitary operator in
PjRN.
Proof. LetU∈ UJ
∆. Since Ucommute with Pjforj= 1, . . . , J , and with I, it also commutes with
PJ+1=I−PJ
j=1Pj. Therefore, we can write
U=J+1X
j=1PjU=J+1X
j=1P2
jU=J+1X
j=1PjUPj.
Now, when restricting PjUPjto an operator in PjRN, it is unitary. Indeed, for every v=Pjv∈
PjRNandu=Pju∈PjRN, since Uis unitary and Pjis self-adjoint and satisfies P2
j=Pj, we
have
⟨PjUPjv,u⟩=
v,PjU−1Pju
,
andPjU−1Pjis the inverse of PjUPjinPjRN, since for every v∈PjRN
PjU−1PjPjUPjv=PjU−1UPjv=Pjv=v,
and similarly PjUPjPjU−1Pjv=v. Here, an invertible normal operator commutes with an
orthogonal projection if and only if its inverse does.
The other direction is trivial.
Projection to Bands Case. Note that
RN=
K+1Y
j=1gj(∆)RN
,
where the (K+ 1) -th filter is the orthogonal complement, given by gK+1(∆) =I−PK
j=1gj(∆).
Lemma D.2. Uis inUg
∆iff it has the form U=LK+1
j=1Uj
, where Ujis a unitary operator in
gj(∆)RN.
The proof is analogous to the proof of Lemma D.1.
D.1.1 Proof of Propositions 1
Proposition 1. Index NLSFs in Eq. (4)are equivariant to the graph functional shifts U∆, and Value
NLSFs in Eq. (5)are equivariant to the relaxed graph functional shifts Ug
∆.
Proof. We start with Index-NLSFs. Consider the Index NLSF defined as in Eq. (4)
Θind(∆,X) ="
PjX
∥PjX∥a
sig+e#J+1
j=1h
Ψind
∥PiX∥sigiJ+1
i=1.
We need to show that for any unitary operator U∈ U∆,
Θind(∆,UX) =UΘind(∆,X).
Consider U∈ U∆and apply it to the graph signal X. The Index NLSF with the transformed input is
given by
Θind(∆,UX) ="
PjUX
∥PjUX∥a
sig+e#J+1
j=1h
Ψind
∥PiUX∥sigiJ+1
i=1.
SinceU∈ U∆, it commutes with Pjfor all j, i.e.,UPj=PjU. Using this commutation property,
we can rewrite the norm and the projections as
∥PjUX∥sig=∥UPjX∥sig.
In addition, since Uis a unitary matrix, it preserves the norm. Therefore, we have
∥UPjX∥sig=∥PjX∥sig.
21Substituting these expressions back into the definition of the Index NLSFs gives
Θind(∆,UX) ="
UPjX
∥PjX∥a
sig+e#J+1
j=1h
Ψind
∥PiX∥sigiJ+1
i=1.
Notice that Uappears linearly in the numerator of the fraction. Hence, we can factor it out by
Θind(∆,UX) =U"
PjX
∥PjX∥a
sig+e#J+1
j=1h
Ψind
∥PiX∥sigiJ+1
i=1.
The expression inside the summation is exactly the original Index NLSFs applied to X, so
Θind(∆,UX) =UΘind(∆,X).
Therefore, we have shown that applying the unitary operator Uto the graph signal Xresults in the
Index NLSFs being transformed by the same unitary operator U, proving the equivariance property.
The proof for value-NLSF follows the same steps.
D.2 Expressivity and Universal Approximation
In this section, we focus on value parameterized NLSFs. The analysis for index-NLSF is equivalent.
D.2.1 Proof of Lemma 1 - Linear Node-level NLSF Exhaust the Linear Operators that
Commute with Functional Shifts
We next show that node-level linear NLSFs exhaust the space of linear operators that commute with
graph functional shifts (on the fixed graph).
Lemma 1. A linear operator RN×d→RN×dcommute with UJ
∆(resp.Ug
∆) iff it is a NLSF based
on a linear function Ψin Eq. (4)(resp. Eq. (5)).
Proof. For simplicity, we restrict the analysis to the case of 1D signals ( d= 1). The extension to a
general dimension dis natural.
By Lemma D.2, the unitary operators UinUg
∆are exhausted by the operators of the form
U=
K+1M
j=1Uj
,
where Ujis any unitary operator in gj(∆)RN. Hence, since the unitary representation T7→Tof
the group of unitary operators in Rm(for any m∈N) is irreducible, by Schur’s lemma [ 85] any
linear operator Bthat commutes with all operators of Ug
∆must be a scalar times the identity when
projected to gj(∆)RN. Namely, Bhas the form
B=
K+1M
j=1(bjIj)
,
where Ijis the identity operator in gj(∆)RN. This means that linear node-level NLSFs exhaust the
space of linear operators that commute with Ug
∆.
The case of hard graph functional shifts is treated similarly.
D.2.2 Node-Level Universal Approximation
The above analysis motivates the following construction of a metric on RN. Given the filter bank
{gj}K+1
j=1, on graph with d-dimensional signals, we define the standard Euclidean metric distEin
the spectral domain R(K+1)×d. We pull back the Euclidean metric to the spatial domain to define a
graph-signal metric. Namely, for two signals XandX′, their distance is defined to be
dist∆ 
X,X′
:= dist E 
A(∆,X),A(∆′,X′)
.
22This defines a pseudometric on the space of graph-signals. To obtain a metric, we consider each
equivalence class of signals with zero distance as a single point. Namely, we define the space
N∆:=RN/dist∆to be the space of signals with 1D features modulo dist. InN∆, the pseudometric
distbecomes a metric, and A∆/dist∆an isometry of metric spaces2.
Now, since MLPs Ψcan approximate any continuous function R(K+1)×d→R(K+1)×d′(by the
universal approximation theorem), and by the fact that A∆/dist∆is an isometry, node-level NLSFs
based on MLPs in the spectral domain can approximate any continuous function from signals with d
features to signal with d′features (N∆)d→(N∆)d′.
D.2.3 Graph-Level Universal approximation
The above analysis also motivates the construction of a graph-signal metric for graph-level tasks. For
graphs with d-channel signals, we define the standard Euclidean metric distEin the spectral domain
R(K+1)×d. We pull back the Euclidean metric to the spatial domain to define a graph-signal metric.
Namely, for two graphs with Laplacians and signals (∆,X)and(∆′,X′), their distance is defined
to be
dist 
(∆,X),(∆′,X′)
:= dist E 
A(∆,X),A(∆′,X′)
This defines a pseudometric on the space of graph-signals. To obtain a metric, we consider equivalence
classes of graph-signals with zero distance as a single point. Namely, we define the space Gto be the
space of graph-signal modulo dist. InG, the function distbecomes a metric, and Aan isometry.
By the isometry property, Ginherits any approximation property from R(K+1)×d. For example, since
MLPs can approximate any continuous function R(K+1)×d→Rd′, the space of NLSFs based on
MLPs Ψhas a universality property: any continuous function G →Rd′with respect to distcan be
approximated by a NLSF based on an MLP.
D.2.4 Graph-Level Expressivity of Pooling-NLSFs
We now show that pooling-NLSF are more expressive than graph-level NLSF if the norm in Eq. (4)
and Eq. (5) is Lpwithp̸= 2.
First, we show that there are graph-signals that graph-level-NLSFs cannot separate and Pooling-
NLSFs can. Consider an index NLSF with norm L1normalize by 1/N. Namely, for a∈RN,
∥a∥1=1
NNX
n=1|an|.
The general case is similar.
For the two graphs, take the graph Laplacian
1−1
−1 1
with eigenvalues 0,1, and corresponding eigenvectors (1,1)and(1,−1). Take the graph Laplacian
 1−1 0
−1 2 −1
0−1 1!
with eigenvalues 0,1,3. The first two eigenvectors are (1,1,1)and(1,0,−1)respectively.
Consider an Index-NLSF based on two eigenprojections P1,P2. As the signal of the first graph take
(1,1) + (1 ,−1), and for the second graph take (1,1,1) +3
2(1,0,−1). Both graph-signals have the
same spectral coefficients (1,1), so graph-level NLSF cannot separate them. Suppose that the NLSF
is
Θval(∆,X) = (∥P1X∥a
sig+e)P1X
∥P1X∥a
sig+e+ (∥P2X∥a
sig+e)P2X
∥P2X∥a
sig+e.
2A∆/dist∆operates on an equivalence class [X]of signals by applying A∆on an arbitrary element Yof
[X]. The output of A∆onYdoes not depend on the specific representative Y, but only on [X].
23The outcome of the corresponding Pooling NLSF on the two graphs is
1 =∥(1 + 1 ,1−1)∥1̸=∥(1 + 3 /2,1,1−3/2)∥1=4
3.
Hence, Pooling-NLSFs separate these inputs, while graph-level-NLSFs do not.
Next, we show that Pooling-NLSFs are at least as expressive as graph-level NLSFs. In particular, any
graph-level NLSF can be expressed as a pooling NLSF.
LetΨbe a graph-level NLSF. Define the node-level NLSF that chooses one spectral index j
with a nonzero value (e.g., the band with the largest coefficient) and projects upon it the value
Ψ(∆,X)(∥PjX∥a
sig+e)/∥PjX∥sig. Hence, before pooling, the NLSF gives
Θ(∆,X) = Ψ( ∆,X)(∥PjX∥a
sig+e)PjX
∥PjX∥a
sig+e/∥PjX∥sig,
where jdepends on the spectral coefficients (e.g., it is the index of the largest spectral coefficient).
Hence, after pooling, the Pooling NLSF returns Ψ(∆,X), which coincides with the output of the
graph-level NLSF.
Now, let us show that for p= 2, they have the same expressivity. For any NLSF,
Θ(∆,X) =JX
j=1
Ψ
∥PiX∥sigJ
i=1
jPjX
∥PjX∥a
sig+e2
sig
=JX
j=1
Ψ
∥PiX∥sigJ
i=12
j∥PjX∥2
sig
∥PjX∥a
sig+e.
This is a generic fully spectral NLSF.
D.2.5 Discussion on Graph-Level Expressivity of NLSFs
We offer additional discussion on the expressivity of graph-level NLSFs. Note that graph-level NLSFs
are bounded by the expressive power of MPNNs with random positional encodings. For example,
in [81], the authors showed that random features improve GNN expressivity, distinguishing certain
structures that deterministic GNNs cannot. The Lanczos algorithm for computing the leading J
eigenvectors can be viewed as a message-passing algorithm with a randomly initialized J-channel
signal.
The example in App. D.2.4 can be written as a standard linear graph spectral filter, followed by the
pooling (readout) function. This shows that graph-level NLSFs (without synthesis and pooling) are
not more expressive than standard spectral GNNs with pooling. In the other direction, there are
no graph-signals that can be separated by a graph-level NLSF but not by a spectral GNN. Given
two graph-signals that an NLSF can separate, we can build a specific standard spectral GNN that
gives the same output as the NLSF specifically for the two given graph-signals. However, several
considerations should be noted:
1.the feature dimension of this GNN would is as large as the combined number of eigenvalues
of the two graphs times the number of features,
2.this GNN is designed to fit these two specific graphs, and will not work for other graphs,
and,
3.if implemented via polynomial filters, the polynomial order would have to be the combined
number of eigenvalues of the two graphs, which makes it very unstable.
Let us explain the architecture next. Given the two graphs (G1, f1)withN1vertices and (G2, f2)
withN2vertices, with node features of dimension D, define band-pass filters that separate the
combined set of eigenvalues of the two given graphs. Concatenate these filters to build a single
multi-channel filter. Namely, this filter maps the signal to the sequence of band-pass filtered signal
– each band at a different channel, for a total of D(N1+N2)channels. If the band-pass filters are
based on polynomials, each polynomial would be chosen to be zero on all eigenvalues except for
one. Apply L2-norm pooling on each channel to obtain a single feature of dimension D(N1+N2).
This gives the sequence of norms of the signal at the bands, where, for the two given graphs, the two
24pooled signals of the two graphs are supported on disjoint sets of indices. Hence, the linear spectral
filter contains the frequency information of the NLSF. Then, one can apply the MLP of the NLSF to
the channels corresponding to the first graph, and to the channels corresponding to the second graph.
This means that the linear spectral GNN gives the exact same outputs on (G1, f1)and(G2, f2)as the
NLSF.
Regarding pooling-NLSF, we do not offer an answer to the question whether standard spectral
GNNs are more powerful (assuming an unlimited budget) than pooling-NLSFs, or vice-versa. More
practically meaningful questions would be:
1.Compare the expressivity of standard spectral GNNs to NLSFs for a given budget of
parameters.
2.How many graph-signal can a single spectral GNN separate, vs a single NLSF, of the same
budget.
We leave these questions as open problems for future research. We note that, in practice, NLSFs with
the same budget as standard spectral GNNs perform better.
D.3 Stable Invertibility of Synthesis
We present the analysis for diagonal synthesis defined from App. C.1. In the fixed graph setting,
given any 1D signal Xsuch that gj(∆)X̸= 0for every j∈[K+ 1], we show that the synthesis
operator Sval, diag
∆,Xis stably invertible.
Since we consider 1D signal X, the diagonal synthesis in filter bank case in Eq. (10) can be written as
Sval, diag
∆,X=Hr′,
where
RN×(K+1)∋H=g1(∆)X
∥g1(∆)X∥a
2+e, . . . ,gK+1(∆)X
∥gK+1(∆)X∥a
2+e
,
gj(∆)X
∥gj(∆)X∥a
2+e∈RN, andr′= [r1, r2, . . . , r K+1]∈RK+1.
Since (gj1(∆)X)⊤(gj2(∆)X) = 0 for all j1, j2∈[K+ 1], the matrix H⊤His a diagonal matrix
with entriesg1(∆)X
∥g1(∆)X∥a
2+e2
2. Therefore, the singular values of Hare
σj=∥g1(∆)X∥2
∥g1(∆)X∥a
2+e
the right singular vectors are the standard basis elements ejinRK+1, and the left singular vectors are
gj(∆)X
∥gj(∆)X∥2.
forj∈[K+ 1]. Hence, we haveSval, diag
∆,X
2= max
jσj
and
Sval, diag
∆,X−1
2=1
minjσj.
Suppose that a= 1ande= 0. In this case, Sval, diag
∆,Xis an isometry from the spectral domain to a
subspace of the signal space RN. Analysis is the adjoint of synthesis. This analysis can be extended
to the index parametrization case for diagonal synthesis, and to higher dimensional signals.
D.4 Uniform Approximation of Graphs with JEigenvectors
In this section, we develop the setting under which the low-rank approximation of GSOs with their
leading eigenvectors can be interpreted as a uniform approximation (Sec. 4.4).
25D.4.1 Cut Norm
The cut norm of M∈RN×Nis defined to be
∥M∥□:=1
N2sup
S,T⊂[N]X
i∈SX
j∈Tmi,j. (19)
The distance between two matrices in cut norm is defined to be ∥M−M′∥□.
The cut norm has the following interpretation, which has precise formulation in terms of the weak
regularity lemma [ 32,65]. Any pair of (deterministic) graphs are close to each other in cut norm if
and only if they can be described as pseudo-random graphs sampled from the same stochastic block
model. Hence, the cut norm is a meaningful notion of graph similarity for practical graph machine
learning, where graphs are noisy and can represent the same underlying phenomenon even if they
have different sizes and topologies. In addition, the distance between non-isomorphic graphons is
always positive in cut norm [ 66]. In this context, the work in [ 59] showed that GNNs with normalized
sum aggregation cannot separate graphs that have zero distance in the cut norm. This means that the
cut norm is sufficiently discriminative for practical machine learning on graphs.
D.4.2 The Constructive Weak Regularity Lemma in Hilbert Spaces
The following lemma, called the constructive weak regularity lemma in Hilbert spaces , was proven
in [29]. It is an extension of the classical respective result from [65].
We define the Frobenius norm normalized by 1/Nas
∥M∥F=vuut1
NNX
i,j=1|mi,j|2.
Lemma D.3. [[29]] Let{Kj}j∈Nbe a sequence of nonempty subsets of a real Hilbert space Hand
letδ≥0. LetJ >0, letR≥1such that J/R∈N, and let g∈ H. Letmbe randomly uniformly
sampled from [J]. Then, in probability 1−1
R(with respect to the choice of m), any vector of the form
g∗=mX
j=1γjfj such that γ= (γj)m
j=1∈Rmand f= (fj)m
j=1∈ K 1×. . .× Km
that gives a close-to-best Hilbert space approximation of gin the sense that
∥g−g∗∥ ≤(1 +δ) inf
γ,f∥g−mX
i=1γifi∥, (20)
where the infimum is over γ∈Rmandf∈ K 1×. . .× Km, also satisfies
∀w∈ Km+1,|⟨w, g−g∗⟩| ≤ ∥ w∥∥g∥r
R
J+δ.
D.4.3 Proof of Theorem 1
Theorem 1. LetMbe a symmetric matrix with entries bounded by |mi,j| ≤α, and let J∈N.
Suppose mis sampled uniformly from [J], and let R≥1s.t.J/R∈N. Consider ϕ1, . . . , ϕmas the
leading eigenvectors of M, with eigenvalues µ1, . . . , µ mordered by their magnitudes |µ1| ≥. . .≥
|µm|. Define C=Pm
k=1µkϕkϕ⊤
k. Then, with probability 1−1
R(w.r.t. the choice of m),
∥M−C∥□<3α
2r
R
J.
Proof. Let us use Lemma D.3, with H=RN×N, andKj=Kthe set of symmetric rank one
matrices of the form vv⊤where v∈RNis a column vector. Denote by Ymthe space of linear
combinations of melements of K, which is the space of symmetric matrices of rank bounded by m.
For the Hilbert space norm, we take the Frobenius norm. In the setting of the lemma, we take g=M,
andg∗∈ Ym, and δ= 0. By the lemma, with probability 1−1/R, any Frobenius minimizer C,
26namely, that satisfies ∥M−C∥F= min C′∈Ym∥M−C′∥F, also satisfies
⟨Y,M−C⟩ ≤ ∥Y∥F∥M∥Fr
R
J≤α∥Y∥Fr
R
J
for every Y∈ Ym. Hence, for every choice of subset S, T⊂[N], we haveX
i∈SX
j∈T(mi,j−ci,j)
=1
2X
i∈SX
j∈T(mi,j−ci,j) +X
i∈TX
j∈S(mi,j−ci,j)
=1
2X
i∈S∪TX
j∈S∪T(mi,j−ci,j)−X
i∈SX
j∈S(mi,j−ci,j)−X
i∈TX
j∈T(mi,j−ci,j)
≤1
2
1S∪T 1⊤
S∪T,M−C
F+1
2
1S 1⊤
S,M−C
F+1
2
1T 1⊤
T,M−C
F
≤3α
2r
R
J,
where for a set S⊂[N], denote by 1S∈RNthe column vector with 1at coordinates in Sand0
otherwise.
Hence, we also have
∥M−C∥□≤3α
2r
R
J.
Lastly, note that by the best rank- mapproximation theorem (Eckart–Young–Mirsky Theorem [ 92,
Thm. 5.9]), any Frobenius minimizer Cis the projection upon the mleading eigenvectors of M(or
some choice of these eigenvectors in case of multiplicity higher than 1).
Now, one can use the adjacency matrix AasMin Thm. 1. When working with sparse matrices of
E≪N2edges, to achieve a meaningful scaling of cut distance, we re-normalize the cut norm and
define
∥M∥(E)
□=N2
E∥M∥□.
With this norm, Thm. 1 gives
∥M−C∥(E)
□<3αN2
2Er
R
J. (21)
While this bound is not uniformly small in N, it is still independent of M. In contrast, the error
bounds for spectral and Frobenius norms do depend on the specific properties of M.
Now, if we want to apply Thm. 1 to other GSOs ∆, we need to make some assumptions. Note
that when the GSO is a Laplacian, we take as the leading eigenvectors the ones correspoding to
the smallest eigenvalues, not the largest ones. To make the theorem applicable, we need to reorder
the eigenvectors of ∆. This can be achieved by applying a decreasing function hto∆, such as
h(∆) =∆−1. The role of his to amplify the eigenspaces of ∆in which most of the energy of
signals interest (the ones that often appear in the dataset) is concentrated. Under the assumption that
h(∆)has entries bounded by some not-too-high α >0, one can now justify approximating GSOs by
low-rank approximations based on the smallest eigenvectors.
27D.5 NLSFs on Random Geometric Graphs
In this section we consider the L2[N]norm normalized by 1/N1/2, namely,
∥v∥2=vuut1
NNX
n=1|vn|2.
We follow a similar analysis to [ 74], mostly skipping the rigorous Monte-Carlo error rate proofs, as
these are standard.
LetSbe a compact metric space with metric dand with a Borel probability measure, that we formally
call the standard measure dx. Let r >0, and denote by Br(x)the ball of radius rabout x∈ S.
LetV(x)be the volume of Br(x)with respect to the standard measure (note that V(x)need not be
constant). We consider an underlying Laplacian on the space Sdefined on signals f:S →Rby
Lf(x) =CZ
Br(x)(f(x)−f(y))dy, (22)
where we assume C= 1 in the analysis below without loss of generality. In case the integral in
Eq.(22) is normalized by 1/r2V(x), this Laplacian was called ρ-Laplacian in [ 15], which we denote
byLr. Such a Laplacian is related to Korevaar-Schoen type energies [ 52], in which the limit case
of the radius rgoing to 0 is considered. It was shown in [ 15] that the ρ-Laplacian is self-adjoint
with spectrum supported inside some interval [0, Q], for some Q >0, where for some 0< R < Q ,
the part of the spectrum in [0, R)∪(R, Q]is discrete (consisting of isolated eigenvalues with finite
multiplicities). The intuition behind this result is that, for the ρ-Laplacian Lr, we have
Lrf=1
r2f−1
V(x)r2Z
Br(x)f(y)dy. (23)
The first term in the right-hand-side of Eq. (23) is a scaled version of the identity operator, and
the second term is a compact self-adjoint integral operator, and hence has a discrete spectrum with
accumulation point at 0, or no accumulation point. After showing that the sum of these two operators
is self-adjoint, we end up with only one accumulation point of the spectrum of Lr.
We show below that Lis self-adjoint under the assumption that V(x)is bounded from above. In this
case it must also be positive semi-definite (the proof is equivalent to the positivity of the combinatorial
graph Laplacian). Consider the decomposition
Lf(x) =CV(x)f(x)−CZ
Br(x)f(y)dy. (24)
The second term of Eq. (24) is a compact integral operator, and hence only has an accumulation
point at 0, or has no accumulation point. The first term is a multiplication operator by the real-valued
function V(x), so it is self-adjoint and its spectrum is the closure of {V(x)|x∈ S} . We suppose
thatV(x)is bounded from below by some R′>0and also bounded from above. This shows that Lis
bounded and self-adjoint (as a sum of two bounded self-adjoint operators). Under these assumptions,
it is reasonable to further assume that the spectrum of Lin the interval [0, R), for some R > 0, is
discrete, with accumulation point at R. This happens, for example, if V(x) =Vis constant.
We now assume that the signal fconsists only of frequencies in [0, R). This means that we can
project Lupon this part of the spectrum, giving a self-adjoint operator with discrete spectrum, and
accumulation point of the eigenvalues only at R.
Letw:S →Rbe a measurable weight function withR
Sw(x)dx= 1. Suppose that w(x)is
continuous and varies slowly over S. Namely, we assume that w(x)≈w(y)for every y∈Br(x). To
generate a random geomertic graph of Nnodes, we sample Npoints x={xn}N
n=1independently
from the weighted probability measure w(x)dx. We connect node xjtoxjby an edge if and only
ifd(xi, xj)≤rto obtain the adjacency matrix A. Let Lbe the combinatorial Laplacian with
respect to A, andNthe normalized symmetric Laplacian. The number of samples inside the ball of
radius raround xis approximately w(x)V(x)N. Hence, the degree of the node xnis approximately
w(xn)V(xn)N.
We consider the leading Jeigenvectors the ones corresponding to the smallest eigenvalues of Lin
the interval [0, R), where J≪N. We order the eigenvalues in increasing order. Let PW(J)be the
space spanned by the Jleading eigenvectors of L, called the Paley-Wiener space of L. LetpJbe the
28projection upon the Paley-Wiener space of L. LetRN:L2(S)→L2[N]be the sampling operator,
defined by (RNf)j=f(xj).
Letf:S →Rbe a bounded signal over the metric space and suppose that f∈ PW (J). Denote
f= (f(xn))N
n=1=RNf. By Monte Carlo theory, we have
(Lf)j≈w(xj)NZ
Br(xj)(f(x)−f(y))dy=w(xj)NLf(xj),
So, in case the sampling is uniform. i.e., w(x) = 1 , we have
L≈NL,
pointwise. To make this precise, let us recall Hoeffding’s Inequality [43].
Theorem D.1 (Hoeffding’s Inequality [ 43]).LetY1, . . . , Y Nbe independent random variables such
thata≤Yn≤balmost surely. Then, for every k >0,
P 1
NNX
n=1(Yn−E[Yn])≥k!
≤2 exp
−2k2N
(b−a)2
.
Using Hoeffding’s Inequality, one can now show that there is an event of probability more than 1−p
in which for every j∈[N], the error between (1
NLRNf)jandLfsatisfies
RNLf−1
NLRNf=O 
Jr
log(1/p) + log( N) + log(2)
N!
.
The fact that different graphs of different sizes Napproximate Lwith different scaling factors means
thatLis not value transferable. Let us show that Lis index transferable.
We now evoke the transferability theorem – a slight reformulation of Section 3 of Theorem 4 in [ 60].
Theorem D.2. LetLbe a compact operator on L2(S)andLan operator on CN. LetJ∈Nand
letRNbe the sampling operator defined above. Let PW(J)be the space spanned by the Jleading
eigenvectors of L, and let f∈ PW (J). Letg:R→Rbe Lipschitz with Lipschitz constant D. Then,RNg(L)f−1
Ng(L)RNf≤D√
JRNLf−1
NLRNf.
For every leading eigenvalue λjofLletγijbe the leading eigenvalue of Lclosest to λ, where, if
there are repeated eigenvalues, ijis chosen arbitrarily from the eigenvalues of Lthat best approximate
λi. Let
δ= min {α, β},
where
α= min
j∈[J]min
min
ij̸=i∈[N]|λj−γi|,min
j̸=m∈[J]|λj−λm|,min
j̸=m∈[J]γij−λm	
and
β= min
i∈[J],n∈[N]|γi−γn|
and suppose that δ >0. For each j∈[N]there exists a Lipschitz continuous function gjwith
Lipschitz constant δ−1such that gj(λj) =gj(γij) = 1 andγis zero on all other leading eigenvalues
ofLand all other eigenvalues of L. Hence,
gj(L)f=pL
jf, g j(L)RNf=pL
ijf,
where pL
jis the projection upon the space spanned by the j-th eigenvector of L, and pL
ijis the
projection upon the space spanned by the ij-th eigenvector of L.
Now, by the transferability theorem,
RNpL
jf−pL
ijf≤D√
JRNLf−1
NLf=O(p
J/N).
By induction over j, with base j= 1, we must have ij=jfor every j∈[J].
Now, note that by standard Monte Carlo theory (evoking Hoeffding’s inequality again and intersecting
events), we have RNpL
jf2
2−pL
jf2
2=O(J−1/2)
29Table 5: Node classification datasets statistics.
Dataset #Nodes #Classes #Edges #Features
Cora 2708 7 5278 1433
Citeseer 3327 6 4552 3703
Pubmed 19717 5 44324 500
Chameleon 2277 5 31371 2325
Squirrel 5201 5 198353 2089
Actor 7600 5 26659 932
in high probability. Hence, by the fact that
RNpL
jf2
2−pL
jf2
2=RNpL
jf
2−pL
jf
2RNpL
jf
2+pL
jf
2
andRNpL
jf
2+pL
jf
2is bounded from below by the constantpL
jf
2, we have
RNpL
jf
2−pL
jf
2=O(J−1/2).
This shows thatpL
jf
2≈pL
jf
2,
which shows index transferability. Namely, for two graphs of NandN′nodes sampled from S, with
corresponding Laplacians LandL′, by the triangle inequality,
pL
jf
2≈pL′
jf
2.
Next, we show value transferability for N. Here,
Nf(xj)≈1
V(xj)Lf(xj).
Hence, if V(x) =Vis constant, we have N≈ L up to a constant that does not depend on N. In
this case, by a similar analysis to the above, Nis value transferable. We note that Nis also index
transferable, but value transferability is guaranteed in a more general case, where we need not assume
a separable spectrum.
E Additional Details on Experimental Study
We describe the experimental setups and additional details of our experiments in Sec. 5. The
experiments are performed on NVIDIA DGX A100.
E.1 Semi-Supervised Node Classification
We provide a detailed overview of the experimental settings for semi-supervised node classification
tasks, along with the validated hyperparameters used in our benchmarks.
Datasets. We consider six datasets in node classification tasks, including Cora, Citeseer, Pubmed,
Chameleon, Squirrel, and Actor. The detailed statistics of the node classification benchmarks are
summarized in Tab. 5. For datasets splitting on citation graphs (Cora, Citeseer, and Pubmed), we
follow the standard splits from [ 100], using 20 nodes per class for training, 500 nodes for validation,
and 1000 nodes for testing. For heterophilic graphs (Chameleon, Squirrel, and Actor), we adopt the
sparse splitting method from [ 16,41], allocating 2.5% of samples for training, 2.5% for validation,
and 95% for testing. The classification quality is assessed by computing the average classification
accuracy across 10 random splits, along with a 95% confidence interval.
Baselines. For GCN, GAT, SAGE, ChebNet, ARMA, and APPNP, we use the implementations from
the PyTorch Geometric library [ 28]. For other baselines, we use the implementations released by the
respective authors.
30Table 6: Graph classification datasets statistics.
Dataset #Graphs ( Z)#Classes ( C)|Ni|min|Ni|max|Ni|avg#Features ( d)
MUTAG 188 2 10 28 17.93 7
PTC 344 2 2 64 14.29 18
ENZYMES 600 6 2 126 32.63 18
PROTEINS 1113 2 4 620 39.06 29
NCI1 4110 2 3 111 29.87 37
IMDB-B 1000 2 12 136 19.77 None
IMDB-M 1500 3 7 89 13.00 None
COLLAB 5000 3 32 492 74.50 None
Hyperparameters Settings. The hidden dimension is set to be either 64 or 128 for all mod-
els and datasets. We implement our proposed Node-level NLSFs using PyTorch and opti-
mize the model with the Adam optimizer [ 48]. To determine the optimal dropout probabil-
ity, we search within the range [0,0.9]in increments of 0.1. The learning rate is examined
within the set {1e−1,5e−2,1e−2,5e−3,1e−3}. We explore weight decay values within the set
{1e−2,5e−3,1e−3,5e−4,1e−4,5e−5,1e−5,5e−6,1e−6,0.0}. Furthermore, the number of layers is
varied from 1 to 10. The number of leading eigenvectors Jin Index NLSFs is set within [1,1e2]. The
decay rate in the Value NLSFs is determined using dyadic sampling within the set {1
4,1
3,1
2,2
3,3
4},
the sampling resolution Swithin [1,1e2], and the number of the bands in Value NLSFs K≤S. For
hyperparameter optimization, we conduct a grid search using Optuna [ 1] for each dataset. An early
stopping criterion is employed during training, stopping the process if the validation loss does not
decrease for 200 consecutive epochs.
E.2 Graph Classification
We provide a comprehensive description of the experimental settings for graph classification tasks
and the validated hyperparameters used in our benchmarks. The results are reported in the main
paper.
Problem Setup. Consider a set of Zgraphs GZ={G1, G2, . . . , G Z}, where in each graph Gi=
([Ni],Ei,Ai,Xi), we have Ninodes for each graph Gi,Eirepresents the edge set, Ai∈RNi×Ni
denotes the edge weights, and Xi∈RNi×drepresents the node feature matrix with d-dimensional
node attributes. Let YZ∈RZ×Cbe the label matrix with Cclasses such that yi,j= 1if the graph
Gibelongs to the class j, andyi,j= 0otherwise. Given a set of Z′graphs GZ′⊂ G, where Z′< Z,
with the label information YZ′∈RZ′×C, our goal is to classify the set of unseen graph labels of
GZ\ GZ′.
Datasets. We consider eight datasets [ 72] for graph classification tasks, including five bioinformatics:
MUTAG, PTC, NCI1, ENZYMES, and PROTEINS, and three social network datasets: IMDB-
B, IMDB-M, and COLLAB. The detailed statistics of the graph classification benchmarks are
summarized in Tab. 6. We use the random split from [ 94,101,67,104], using 80% for training, 10%
for validation, and 10% for testing. This process is repeated 10 times, and we report the average
performance and standard deviation.
Baselines. For GCN, GAT, SAGE, ChebNet, ARMA, APPNP, and DiffPool, we use the implemen-
tations from the PyTorch Geometric library [ 28]. For other baselines, we use the implementations
released by the respective authors.
Hyperparameters Settings. The dimension of node representations is set to 128 for all methods and
datasets. We implement the proposed Pooling-NLSFs and Graph-level NLSFs using PyTorch and
optimize the model with the Adam optimizer [ 48]. A readout function is applied to aggregate the node
representations for each graph, utilizing mean, add, max, or RMS poolings. The learning rate and
weight decay are searched within {1e−1,1e−2,1e−3,1e−4,1e−5}, the pooling ratio within [0.1,0.9]
with step 0.1, the number of layers within [1,10]with step 1, the number of leading eigenvectors J
in Index NLSFs within [1,1e2], the decay rate in the Value NLSFs using dyadic sampling within
31Table 7: Semi-supervised node classification accuracy with random split, following the experimental
protocol established by [41]. Results marked with∗are taken from [41].
Cora Citeseer Pubmed Chameleon Squirrel Actor
GCN 79.19 ±1.4∗69.71 ±1.3∗78.81 ±0.8∗38.15 ±3.8∗31.18 ±1.0∗22.74 ±2.3∗
GAT 80.03 ±0.8 68.16 ±0.9 77.26 ±0.5 34.16 ±1.2 27.40 ±1.4 24.35 ±1.7
SAGE 72.68 ±1.9 63.87 ±1.2 77.68 ±0.8 31.77 ±1.8 22.67 ±1.8 25.61 ±1.8
ChebNet 78.08 ±0.9∗67.87 ±1.5∗73.96 ±1.7∗37.15 ±1.5∗26.55 ±0.5∗26.58 ±1.9∗
ChebNetII 82.42 ±0.6∗69.89 ±1.2∗79.53 ±1.0∗43.42 ±3.5∗33.96 ±1.2∗30.18 ±0.8∗
CayleyNet 80.25 ±1.4 66.46 ±2.9 75.42 ±3.4 34.52 ±3.1 24.08 ±2.9 27.42 ±3.3
APPNP 82.39 ±0.7∗69.79 ±0.9∗79.97 ±1.6∗32.73 ±2.3∗24.50 ±0.9∗29.74 ±1.0∗
GPRGNN 82.37 ±0.9∗69.22 ±1.3∗79.28 ±1.3∗33.03 ±1.9∗24.36 ±1.5∗28.58 ±1.0∗
ARMA 79.14 ±1.1∗69.35 ±1.4∗78.31 ±1.3∗37.42 ±1.7∗24.15 ±0.9∗27.02 ±2.3∗
att-Node-level NLSFs 82.94 ±1.1 72.13 ±1.1 79.62 ±1.2 46.75 ±1.3 35.17 ±1.6 29.96 ±0.8
{1
4,1
3,1
2,2
3,3
4}, the sampling resolution Swithin [1,1e2], and the number of the bands in Value
NLSFs within K≤S. The graph-level representation is then fed into an MLP with a (log)softmax
classifier, using a cross-entropy loss function for predictions over the labels. Specifically, the MLP
consists of three fully connected layers with 256, 128, and 64 neurons, respectively, followed by
a (log)softmax classifier. We conduct a grid search on the hyperparameters for each dataset using
Optuna [ 1]. An early stopping criterion is employed during training, stopping the process if the
validation loss does not decrease for 100 consecutive epochs.
F Additional Experimental Results
Here, we present additional experiments on node and graph classification benchmarks, ablation
studies, runtime analysis, and uniform sub-bands.
F.1 Semi-Supervised Node Classification Following [41] Protocol
We present additional experimental results for semi-supervised node classification using random
splits, adhering to the protocol established by [ 41]. The results, summarized in Tab. 7, demonstrate
the classification accuracy across six benchmark datasets: Cora, Citeseer, Pubmed, Chameleon,
Squirrel, and Actor. Our att-Node-level NLSFs achieve the highest accuracy on four out of the six
datasets, outperforming other models significantly. On Pubmed, it records a close second-highest
accuracy, slightly behind APPNP. Our method achieves a competitive second place in the Actor
dataset. att-Node-level NLSFs demonstrate substantial improvements, particularly in challenging
datasets like Chameleon and Squirrel. The comparison models, including GCN, GAT, SAGE,
ChebNet, ChebNetII, CayleyNet, APPNP, GPRGNN, and ARMA, varied in their effectiveness, with
ChebNetII and APPNP also showing strong results on several datasets. These findings highlight the
efficacy of the att-Node-level NLSFs in semi-supervised node classification tasks.
F.2 Node Classification on Filtered Chameleon and Squirrel Datasets in Dense Split Setting
Following [ 77], we conduct additional experiments on the original and filtered Chameleon and
Squirrel datasets in the dense split setting. We use the same random splits as in [ 77], dividing the
datasets into 48% for training, 32% for validation, and 20% for testing. We compare the Node-
level NLSFs using Laplacian attention with GCN [ 49], SAGE [ 39], GAT [ 93], GT [ 88], H 2GCN
[103], CPGNN [ 102], GPRGNN [ 16], FSGNN [ 69], GloGNN [ 62], FAGCN [ 8], GBKGNN [ 23],
JacobiConv [ 96], and ResNet [ 40] with GNN models [ 77]. The study by [ 103] demonstrates the
benefits of separating ego- and neighbor-embeddings in the GNN aggregation step when dealing
with heterophily. Therefore, [ 77] also adopts this approach for the GNN aggregation step in GAT
and GT models, denoted as “sep.” The baseline results used for comparison are taken from [ 77].
Tab. 8 presents the full performance comparison on the original and filtered datasets. Note that
Tab. 8 is the same as Tab. 2 but with more baseline methods. att-Node-level NLSFs achieve the
highest accuracy on both the filtered Chameleon and Squirrel datasets. Additionally, att-Node-level
NLSFs demonstrate strong performance on the original Chameleon dataset, achieving the highest
32Table 8: Full Performance comparison of node classification on original and filtered Chameleon and
Squirrel datasets in dense split setting. The baseline results used for comparison are taken from [ 77].
Chameleon Squirrel
Original Filtered Original Filtered
ResNet 49.52 ±1.7 36.73 ±4.7 33.88 ±1.8 36.55 ±1.8
ResNet+SGC 49.93 ±2.3 41.01 ±4.5 34.36 ±1.2 38.36 ±2.0
ResNet+adj 71.07 ±2.2 38.67 ±3.9 65.46 ±1.6 38.37 ±2.0
GCN 50.18 ±3.3 40.89 ±4.1 39.06 ±1.5 39.47 ±1.5
SAGE 50.18 ±1.8 37.77 ±4.1 35.83 ±1.3 36.09 ±2.0
GAT 45.02 ±1.8 39.21 ±3.1 32.21 ±1.6 35.62 ±2.1
GAT-sep 50.24 ±2.2 39.26 ±2.5 35.72 ±2.0 35.46 ±3.1
GT 44.93 ±1.4 38.87 ±3.7 31.61 ±1.1 36.30 ±2.0
GT-sep 50.33 ±2.6 40.31 ±3.0 36.08 ±1.6 36.66 ±1.6
H2GCN 46.27 ±2.7 26.75 ±3.6 29.45 ±1.7 35.10 ±1.2
CPGNN 48.77 ±2.1 33.00 ±3.2 30.91 ±2.0 30.04 ±2.0
GPRGNN 47.26 ±1.7 39.93 ±3.3 33.39 ±2.1 38.95 ±2.0
FSGNN 77.85 ±0.5 40.61 ±3.0 68.93 ±1.7 35.92 ±1.3
GloGNN 70.04 ±2.1 25.90 ±3.6 61.21 ±2.0 35.11 ±1.2
FAGCN 64.23 ±2.0 41.90 ±2.7 47.63 ±1.9 41.08 ±2.3
GBKGNN 51.36 ±1.8 39.61 ±2.6 37.06 ±1.2 35.51 ±1.7
JacobiConv 68.33 ±1.4 39.00 ±4.2 46.17 ±4.3 29.71 ±1.7
att-Node-level NLSFs 79.42 ±1.6 42.06 ±1.3 67.81 ±1.4 42.18 ±1.2
Table 9: An ablation study investigated the effect of Node-level NLSFs on node classification,
comparing the use of Index NLSFs and Value NLSFs. The symbol (↑)denotes an improvement using
Laplacian attention.
Cora Citeseer Pubmed Chameleon Squirrel Actor
Θind(L,·) 82.46 ±1.2 71.31 ±1.0 80.97 ±0.9 44.37 ±1.8 34.12 ±1.4 29.88 ±1.1
Θind(N,·) 81.73 ±1.4 70.25 ±0.8 79.88 ±1.2 44.52 ±1.7 34.26 ±1.5 29.97 ±1.7
Θval(L,·) 80.25 ±1.5 70.43 ±1.7 80.06 ±1.6 45.52 ±1.2 35.23 ±0.8 32.69 ±1.9
Θval(N,·) 81.98 ±0.7 71.16 ±1.3 80.33 ±1.7 47.91 ±1.4 35.78 ±0.8 33.53 ±1.4
att(Θind(L,·),Θval(L,·)) 82.46 ±1.2 71.48 ±0.8(↑)80.97 ±0.9 46.71 ±2.2(↑)36.92 ±1.1(↑)32.69 ±1.9
att(Θind(N,·),Θval(N,·)) 81.98 ±0.7 72.45 ±1.4(↑)80.33 ±1.7 47.91 ±1.4 36.87 ±0.7(↑)33.53 ±1.4
att(Θind(N,·),Θval(L,·)) 82.65 ±1.2(↑)71.26 ±1.7(↑)80.56 ±1.7(↑)45.98 ±2.3(↑)37.03 ±1.9(↑)33.09 ±1.2(↑)
att(Θind(L,·),Θval(N,·)) 84.75 ±0.7(↑)73.62 ±1.1(↑)81.93 ±1.0(↑)49.68 ±1.6(↑)38.25 ±0.7(↑)34.72 ±0.9(↑)
accuracy and the second-highest accuracy on the original Squirrel dataset. att-Node-level NLSFs
show less sensitivity to node duplicates and exhibit stronger generalization ability, further validating
the reliability of the Chameleon and Squirrel datasets in the dense split setting.
F.3 Ablation Study on att-Node-level NLSFs, att-Graph-NLSF, and att-Pooling-NLSFs
In Sec. 3.5, we note that using the graph Laplacian Lin Index NLSFs and the normalized graph
Laplacian Nin Value NLSFs is transferable. Since real-world graphs often fall between these two
boundary cases, we present the Laplacian attention NLSFs that operate between them at both the
node-level and graph-level. Indeed, as demonstrated in Sec. 5, the proposed att-Node-level NLSFs,
att-Graph-level NLSFs, and att-Pooling-NLSFs outperform existing spectral GNNs.
We conduct an ablation study to evaluate the contribution and effectiveness of different components
within the att-Node-level NLSFs, att-Graph-level-NLSFs, and att-Pooling-NLSFs on node and
graph classification tasks. Specifically, we compare the Index NLSFs and Value NLSFs using both
the graph Laplacian Land the normalized graph Laplacian Nto understand their individual and
Laplacian attention impact on these tasks.
The ablation study of att-Node-level NLSPs for node classification is summarized in Tab. 9. We
investigate the performance on six node classification benchmarks as in Sec. 5, including Cora,
Citeseer, Pubmed, Chameleon, Squirrel, and Actor. Tab. 9 shows that using the graph Laplacian
Lin Index NLSFs (denoted as Θind(L,·)) and the normalized graph Laplacian Nin Value NLSFs
(denoted as Θval(N,·)) has superior node classification accuracy compared to using the normalized
graph Laplacian Nin Index NLSFs (denoted as Θind(N,·)) and the graph Laplacian Lin Value
33Table 10: An ablation study investigated the effect of Graph-NLSFs on graph classification, comparing
the use of Index NLSFs and Value NLSFs. The symbol (↑)denotes an improvement using Laplacian
attention.
MUTAG PTC ENZYMES PROTEINS NCI1 IMDB-B IMDB-M COLLAB
Φind(L,·) 82.14 ±1.2 65.73 ±2.4 62.31 ±1.4 78.22 ±1.6 80.51 ±1.2 63.27 ±1.1 50.29 ±1.4 70.06 ±1.3
Φind(N,·) 83.27 ±0.3 66.08 ±2.1 60.18 ±0.5 80.31 ±1.4 78.40 ±0.6 64.58 ±2.2 52.33 ±0.9 71.88 ±1.7
Φval(L,·) 83.40 ±1.4 65.67 ±1.4 66.42 ±1.1 83.36 ±1.4 76.17 ±1.2 72.71 ±1.4 52.13 ±0.9 74.19 ±1.0
Φval(N,·) 81.19 ±0.3 66.20 ±0.7 63.32 ±1.2 78.20 ±0.9 78.68 ±1.2 74.26 ±1.8 52.49 ±0.7 79.06 ±1.2
att(Φind(L,·),Φval(L,·)) 83.40 ±1.4 66.32 ±1.9(↑)66.42 ±1.1 83.36 ±1.4 80.51 ±1.2 72.71 ±1.4 52.13 ±0.9 75.14 ±1.8(↑)
att(Φind(N,·),Φval(N,·)) 83.71 ±1.7(↑)67.14 ±1.5(↑)65.97 ±1.1(↑)81.79 ±1.2(↑)79.83 ±0.6(↑)74.26 ±1.8 52.49 ±0.7 79.06 ±1.2
att(Φind(N,·),Φval(L,·)) 83.78 ±0.8(↑)66.20 ±0.7 66.42 ±1.1 83.36 ±1.4 78.40 ±0.6 72.71 ±1.4 52.33 ±0.9 76.22 ±0.8(↑)
att(Φind(L,·),Φval(N,·)) 84.13 ±1.5(↑)68.17 ±1.0(↑)65.94 ±1.6(↑)82.69 ±1.9(↑)80.51 ±1.2 74.26 ±1.8 52.49 ±0.7 79.06 ±1.2
Table 11: An ablation study investigated the effect of Pooling-NLSFs on graph classification, com-
paring the use of Index NLSFs and Value NLSFs. The symbol (↑)denotes an improvement using
Laplacian attention.
MUTAG PTC ENZYMES PROTEINS NCI1 IMDB-B IMDB-M COLLAB
ΘP
ind(L,·) 86.41 ±1.9 68.76 ±0.9 69.88 ±1.3 84.27 ±1.3 80.33 ±0.8 60.40 ±1.3 51.01 ±1.3 71.28 ±0.9
ΘP
ind(N,·) 84.52 ±0.8 67.19 ±1.2 66.37 ±2.1 81.12 ±1.7 77.05 ±2.2 62.08 ±1.6 52.48 ±1.0 72.03 ±1.2
ΘP
val(L,·) 83.64 ±0.9 67.74 ±1.3 65.87 ±1.4 84.18 ±1.4 79.43 ±1.4 71.98 ±2.0 51.85 ±1.7 78.80 ±1.1
ΘP
val(N,·) 86.58 ±1.0 69.13 ±1.1 68.89 ±0.4 84.80 ±1.0 80.82 ±0.8 76.48 ±1.8 54.12 ±1.0 81.31 ±0.7
attP(Θind(L,·),Θval(L,·)) 86.41 ±1.9 68.76 ±0.9 69.88 ±1.3 84.27 ±1.3 80.33 ±0.8 72.44 ±1.3(↑)51.85 ±1.7 78.80 ±1.1
attP(Θind(N,·),Θval(N,·)) 86.58 ±1.0 69.67 ±1.2(↑)69.06 ±1.1(↑)84.80 ±1.0 80.82 ±0.8 76.48 ±1.8 54.37 ±1.2(↑)81.70 ±0.9(↑)
attP(Θind(N,·),Θval(L,·)) 84.93 ±1.7(↑)68.14 ±1.6(↑)67.26 ±1.0(↑)84.18 ±1.4 79.43 ±1.4 72.64 ±1.2(↑)52.48 ±1.0 78.80 ±1.1
attP(Θind(L,·),Θval(N,·)) 86.89 ±1.2(↑)71.02 ±1.3(↑)69.94 ±1.0(↑)84.89 ±0.9(↑)80.95 ±1.4(↑)76.78 ±1.9(↑)55.28 ±1.7(↑)82.19 ±1.3(↑)
NLSFs (denoted as Θval(L,·)). This is in line with our theoretical findings in App. D.5. Moreover, the
att-Node-level NLSFs using the Laplacian attention att(Θind(L,·),Θval(N,·))yield the highest
accuracies across all datasets, corroborating the findings in Sec. 3.5. We also note that without
Laplacian attention, the Node-level NLSFs ( Θind(L,·)andΘval(N,·)) alone still achieve more
effective classification performance compared to existing baselines, as shown in Tab. 1.
Tab. 10 demonstrates the ablation study of Graph-level NLSFs for graph classification tasks. We
examine the eight graph datasets as in Sec. 5, including MUTAG, PTC, ENZYMES, PROTEINS,
NCI1, IMDB-B, IMDB-M, and COLLAB. Similar to the above, we investigate the Index and Value
settings using graph Laplacian Land normalized graph Laplacian N, including Φind(L,·),Φind(N,·),
Φval(L,·), and Φval(N,·), along with their variants using Laplacian attention. Here, we see that
att-Graph-level NLSFs do not show significant improvement over the standard Graph-level NLSFs.
Notably, Φval(N,·)outperforms other models in social network datasets (IMDB-B, IMDB-M, and
COLLAB), where the node features are augmented by the node degree. We plan to investigate the
limited graph attribution in future work.
The ablation study of att-Pooling-NLSFs for graph classification is reported in Tab. 11. Similar to
Tab. 10, we consider eight graph classification benchmarks: MUTAG, PTC, ENZYMES, PROTEINS,
NCI1, IMDB-B, IMDB-M, and COLLAB. Tabl. 11 demonstrates that using the graph Laplacian
Lin Index Pooling-NLSFs (denoted as ΘP
ind(L,·)) and the normalized graph Laplacian Nin Value
Pooling-NLSFs (denoted as ΘP
val(N,·)) achieves superior graph classification accuracy compared
to using the normalized graph Laplacian Nin Index Pooling-NLSFs (denoted as ΘP
ind(N,·)) and
the graph Laplacian Lin Value Pooling-NLSFs (denoted as ΘP
val(L,·)). This finding aligns with our
theoretical results in App. D.5. Moreover, the att-Pooling-NLSFs using the Laplacian attention
attP(Θind(L,·),Θval(N,·))yield the highest accuracies across all datasets, corroborating the findings
in Sec. 3.5. In addition, att-Pooling-NLSFs consistently outperform att-Graph-level NLSFs as
shown in Tab. 10, indicating that the node features learned in our Node-level NLSFs representation
are more expressive. This supports our theoretical findings in Sec. 4.3.
The ablation study demonstrates that the Laplacian attention between the Index and Value NLSFs
significantly enhances classification accuracy for both node and graph classification tasks across
various datasets, outperforming existing baselines.
34Table 12: Average running time per epoch(ms)/average total running time(s).
Cora Citeseer Pubmed Chameleon Squirrel Actor
GCN 8.97/2.1 9.1/2.3 12.15/3.9 11.68/2.7 25.52/6.2 14.51/3.8
GAT 13.13/3.3 13.87/3.6 19.42/6.2 14.83/3.4 46.13/15.9 20.13/4.4
SAGE 11.72/2.2 12.11/2.4 25.31/6.1 60.61/12.7 321.65/72.8 25.16/5.4
ChebNet 21.36/4.9 22.51/5.3 34.53/13.1 42.21/16.0 38.21/45.1 42.91/9.3
ChebNetII 20.53/5.9 20.61/5.7 33.57/12.9 39.03/17.3 37.29/38.04 40.05/9.1
CayleyNet 401.24/79.3 421.69/83.4 723.61/252.3 848.51/389.4 972.53/361.8 794.61/289.4
APPNP 18.31/4.2 19.17/4.8 19.63/5.9 18.56/3.8 24.18/4.9 15.93/4.6
GPRGNN 19.07/3.8 18.69/4.0 19.77/6.3 19.31/3.6 28.31/5.5 17.28/4.8
ARMA 20.91/5.2 19.33/4.9 34.27/14.5 41.63/19.7 39.42/42.7 46.22/5.7
att-Node-level NLSFs 18.22/4.5 18.51/4.4 20.23/6.1 28.51/17.1 25.56/5.1 17.09/4.6
Table 13: Experimental results on large heterophilic graphs. The results for BernNet, ChebNet,
ChebNetII, and GPRGNN are taken from [ 41], while the results for OptBasisGNN are taken from
[38]. All other competing results are taken from [63].
Penn94 Pokec Genius Twitch-Gamers Wiki
GCN 82.47 ±0.3 75.45 ±0.2 87.42 ±0.4 62.18 ±0.3 OOM
LINK 80.79 ±0.5 80.54 ±0.0 73.56 ±0.1 64.85 ±0.2 57.11 ±0.3
LINKX 84.71 ±0.5 82.04 ±0.1 90.77 ±0.3 66.06 ±0.2 59.80 ±0.4
GPRGNN 83.54 ±0.3 80.74 ±0.2 90.15 ±0.3 62.59 ±0.4 58.73 ±0.3
ChebNet 82.59 ±0.3 72.71 ±0.7 89.36 ±0.3 62.31 ±0.4 OOM
ChebNetII 84.86 ±0.3 82.33 ±0.3 90.85 ±0.3 65.03 ±0.3 60.95 ±0.4
BernNet 83.26 ±0.3 81.67 ±0.2 90.47 ±0.3 64.27 ±0.3 59.02 ±0.3
OptBasisGNN 84.85 ±0.4 82.83 ±0.0 90.83 ±0.1 65.17 ±0.2 61.85 ±0.0
att-Node-level NLSFs 85.19 ±0.3 82.96 ±0.1 91.24 ±0.1 65.97 ±0.2 62.44 ±0.3
F.4 Runtime Analysis
In our NLSFs, the eigendecomposition can be calculated once for each graph and reused in the
training process. This step is essential as the cost of the forward pass during model training often
surpasses the initial eigendecomposition preprocessing cost. Note that the computation time for
eigendecomposition is considerably less than the time needed for model training. For medium and
large graphs, the computation time is further reduced when partial eigendecomposition is utilized,
making it more efficient than the training times of competing baselines. To evaluate the computational
complexity of our NLSFs compared to baseline models, we report the empirical training time in
Tab. 12. Our Node-level NLSFs showcase competitive running times, with moderate values per epoch
and total running times that are comparable to the most efficient models like GCN, GPRGNN, and
APPNP. Notably, Node-level NLSFs are particularly efficient for the Cora, Citeseer, and Pubmed
datasets. For the dense Squirrel graph, our Node-level NLSFs exhibit efficient performance with a
moderate running time, outperforming several models that struggle with significantly higher times.
F.5 Scalability to Large-Scale Datasets
To demonstrate the scalability of our method, we conduct additional tests on five large heterophilic
graphs: Penn94, Pokec, Genius, Twitch-Gamers, and Wiki datasets from [ 63]. The experimental
setup is in line with previous work by [ 16,63,41]. We use the same hyperparameters for our NLSFs
as reported in App. E. Tab. 13 presents the classification accuracy. We see that NLSFs outperform
the competing methods on the Penn94, Pokec, Genius, and Wiki datasets. For the Twitch-Gamers
dataset, NLSFs yield the second-best results. Our additional experiments show that our method could
indeed scale to handle large-scale graphs effectively.
F.6 Index-by-Index Index-NLSFs and Band-by-Band Value-NLSFs
Our primary objective in this work is to introduce new GNNs that are equivariant to functional
symmetries, based on a novel spectral domain that is transferable between graphs. We emphasize
35Table 14: Graph classification performance using Diagonal NLSFs, including index-by-index Index-
NLSFs and band-by-band Value-NLSFs, along with their variants using Laplacian attention. The
symbol (↑)denotes an improvement using Laplacian attention.
MUTAG PTC ENZYMES PROTEINS NCI1 IMDB-B IMDB-M COLLAB
ΓP
ind(L,·) 82.33 ±1.7 66.43 ±1.8 69.69 ±2.4 83.47 ±2.2 77.43 ±1.4 60.17 ±0.8 50.04 ±1.2 70.89 ±0.9
ΓP
ind(N,·) 82.85 ±0.8 63.40 ±1.7 68.06 ±2.1 82.69 ±0.8 78.00 ±1.4 62.27 ±1.8 51.74 ±1.4 70.69 ±1.3
ΓP
val(L,·) 82.77 ±0.8 63.20 ±0.9 64.19 ±2.3 83.39 ±1.7 78.26 ±1.4 70.61 ±2.2 51.86 ±1.9 77.03 ±1.5
ΓP
val(N,·) 80.29 ±0.8 65.02 ±1.4 65.29 ±1.7 81.62 ±1.2 78.28 ±1.0 74.33 ±1.7 52.89 ±0.9 80.41 ±0.8
attP(Γind(L,·),Γval(L,·)) 83.19 ±1.4(↑)66.43 ±1.8 69.69 ±2.4 83.47 ±2.2 78.26 ±1.4 70.61 ±2.2 52.03 ±1.4(↑)77.56 ±1.3(↑)
attP(Γind(N,·),Γval(N,·)) 83.34 ±1.2(↑)66.58 ±1.2(↑)68.26 ±1.9(↑)82.99 ±1.8(↑)78.28 ±1.0 74.33 ±1.7 53.14 ±0.8(↑)80.41 ±0.8
attP(Γind(N,·),Γval(L,·)) 83.42 ±1.5(↑)64.08 ±1.7(↑)68.06 ±2.1 83.39 ±1.7 78.26 ±1.4 70.61 ±2.2 52.13 ±1.4(↑)78.12 ±1.9(↑)
attP(Γind(L,·),Γval(N,·)) 83.85 ±1.4(↑)67.12 ±1.6(↑)69.69 ±2.4 83.47 ±2.2 78.28 ±1.0 74.33 ±1.7 53.91 ±1.6(↑)81.03 ±1.2(↑)
Table 15: Semi-supervised node classification accuracy using NLSFs, diag-NLSFs, lead-NLSFs, and
lead-diag-NLSFs.
Cora Citeseer Pubmed Chameleon Squirrel Actor
att-Node-level diag-NLSFs 85.37 ±1.8 75.41 ±0.8 82.22 ±1.2 50.58 ±1.3 38.39 ±0.9 35.13 ±1.0
att-Node-level NLSFs 86.03 ±1.2 74.87 ±1.2 83.15 ±1.5 50.12 ±1.2 36.23 ±1.6 35.01 ±1.7
att-Node-level lead-NLSFs 85.26 ±0.4 74.16 ±1.4 82.24 ±0.9 49.86 ±1.9 34.21 ±1.1 33.68 ±1.1
att-Node-level lead-diag-NLSFs 84.75 ±0.7 73.62 ±1.1 81.93 ±1.0 49.68 ±1.6 38.25 ±0.7 34.72 ±0.9
the unique aspects of our method rather than providing an exhaustive list of operators in this group,
which, while important, is a key direction for future research.
Here, we present a new type of NLSFs: index-by-index Index-NLSFs and band-by-band Value-NLSFs.
We denote them as follows:
Γind(∆,X) =JX
j=1γj
∥PjX∥sigPjX
∥PjX∥a
sig+eand
Γval(∆,X) =KX
j=1γj
∥gj(∆)X∥siggj(L)X
∥gp(∆)X∥a
sig+e,
where a, eare as before in Sec. 3.3. Note that these operators are also equivariant to our group
actions.
We investigate index-by-index Index-NLSFs and band-by-band Value-NLSFs in graph classification
tasks as described in Sec. 5, including MUTAG, PTC, ENZYMES, PROTEINS, NCI1, IMDB-B,
IMDB-M, and COLLAB datasets. Unlike Graph-NLSFs, which are fully spectral and map a sequence
of frequency coefficients to an output vector, index-by-index Index-NLSFs and band-by-band Value-
NLSFs do not possess such a sequential spectral form. In index-by-index Index-NLSFs and band-
by-band Value-NLSFs, the index-by-index (or band-by-band) frequency response is projected back
to the graph domain. Consequently, for graph classification tasks, we apply the readout function as
defined for Pooling-NLSFs in Sec. 3.4.
Following App. F.3, we examine index-by-index Index-NLSFs and band-by-band Value-NLSFs
settings using graph Laplacian Land normalized graph Laplacian N, including ΓP
ind(L,·),ΓP
ind(N,·),
ΓP
val(L,·), and ΓP
val(N,·), along with their variants using Laplacian attention, where Pdenotes the
pooling function as in Tab. 11.
Tab. 14 presents the graph classification accuracy using index-by-index Index-NLSFs and band-by-
band Value-NLSFs. It shows that incorporating Laplacian attention consistently improves classifica-
tion performance, aligning with the findings in App. F.3. We note that index-by-index Index-NLSFs
and band-by-band Value-NLSFs perform comparably to existing baselines in graph classification
benchmarks. However, index-by-index Index-NLSFs and band-by-band Value-NLSFs are generally
less effective compared to Pooling-NLSFs, as shown in Tab. 11.
36102121418Figure 5: Illustration of dyadic sub-bands for r=1
2andS= 4.
Table 16: Graph classification performance using Graph-NLSFs with uniform sub-bands. The symbol
(↑)denotes an improvement using Laplacian attention.
MUTAG PTC ENZYMES PROTEINS NCI1 IMDB-B IMDB-M COLLAB
Φind(L,·) 82.14 ±1.2 65.73 ±2.4 62.31 ±1.4 78.22 ±1.6 80.51 ±1.2 63.27 ±1.1 50.29 ±1.4 70.06 ±1.3
Φind(N,·) 83.27 ±0.3 66.08 ±2.1 60.18 ±0.5 80.31 ±1.4 78.40 ±0.6 64.58 ±2.2 52.33 ±0.9 71.88 ±1.7
Φval(L,·) 81.73 ±1.4 64.81 ±1.4 64.46 ±1.2 78.31 ±0.9 78.22 ±1.4 70.84 ±2.3 50.42 ±1.8 73.49 ±1.4
Φval(N,·) 81.12 ±0.6 65.13 ±0.7 60.06 ±1.3 78.88 ±0.4 78.48 ±1.3 74.05 ±2.4 51.13 ±0.8 78.16 ±1.3
att(Φind(L,·),Φval(L,·)) 82.76 ±0.9(↑)65.73 ±2.4 64.58 ±1.7(↑)79.23 ±1.2(↑)80.51 ±1.2 70.84 ±2.3 51.04 ±2.2 (↑)74.21 ±1.2(↑)
att(Φind(N,·),Φval(N,·)) 83.27 ±0.3 66.08 ±2.1 61.25 ±1.8(↑)81.07 ±1.7(↑)79.26 ±1.9(↑)74.05 ±2.4 52.33 ±0.9 78.16 ±1.3
att(Φind(N,·),Φval(L,·)) 83.27 ±0.3 66.08 ±2.1 64.46 ±1.2 80.31 ±1.4 79.14 ±1.1(↑)70.84 ±2.3 52.33 ±0.9 74.54 ±1.9(↑)
att(Φind(L,·),Φval(N,·)) 82.53 ±1.9(↑)65.73 ±2.4 62.31 ±1.4 79.54 ±2.1(↑)80.51 ±1.2 74.05 ±2.4 51.37 ±0.5 (↑)78.16 ±1.3
F.7 Node Classification Using Diag-NLSFs and Lead-NLSFs
In App. C, we presented the diag-NLSFs, considering ed=d, and lead-NLSFs for leading filters that
do not include orthogonal complements. We explore the leading filters, orthogonal complement, and
diagonal operation in our NLSFs. The results of these investigations are summarized in Tab. 15 ,
which shows the node classification accuracy achieved using various configurations, including NLSFs,
diag-NLSFs, lead-NLSFs, and lead-diag-NLSFs. We see that incorporating both the orthogonal
complement and the multi-channel approach yields the highest classification accuracy.
F.8 Uniform Sub-Bands
Our primary objective in this work is to introduce new GNNs that are equivariant to functional
symmetries, based on a novel spectral domain transferable between graphs using analysis and
synthesis. Our NLSFs in Sec. 3.4 consider filters gjthat are supported on the dyadic sub-bands
λNrS−j+1, λNrS−j
. The closer to the low-frequency range, the denser the sub-bands become. We
illustrate an example of filters gjsupported on dyadic sub-bands with r=1
2andS= 4in Fig. 5,
showing that sub-bands become denser as they approach the low-frequency range. Our primary
goal is to highlight the unique aspects of our method rather than sub-band separation, which, while
crucial, is an important consideration across spectral GNNs. Therefore, we also present uniform
sub-bands, where filters gjare supported on the uniform sub-bands
(j−1)λN
S, jλN
S
. Note that the
modifications required for our NLSFs are minimal, and most steps can be seamlessly applied with
filters supported on the uniform sub-bands.
We evaluate our NLSFs with uniform sub-bands on graph classification tasks. We consider the graph
benchmarks as in Sec. 5, including five bioinformatics datasets: MUTAG, PTC, NCI1, ENZYMES,
and PROTEINS, and three social network datasets: IMDB-B, IMDB-M, and COLLAB. Note the
sub-bands only affect our Value-NLSFs, where Index-NLSFs remain the same as in Sec. 3.4.
We report the graph classification accuracy of Graph-NLSFs and Pooling-NLSFs using uniform
sub-bands in Tab. 16 and Tab. 17, respectively, where the Φind(L,·),Φind(N,·),Θind(L,·), and
Θind(N,·)are the index-based NLSFs and therefore they are the same as in Tab. 10 and Tab. 11.
Interestingly, the Laplacian attention does not yield significant improvements for either Graph-level
NLSFs or Pooling-NLSFs when considering uniform sub-bands. Moreover, we observe that the
graph classification performance is generally worse than when using the dyadic sub-bands reported
in Tab. 10 and Tab. 11. We emphasize the importance of considering the spectral support of filters
gj. Empirically, we found that using the dyadic grid is more effective, which is why we focus on it
and report those results in the main paper. However, exploring other sub-bands remains an important
task for future work. For instance, we plan to investigate sub-bands based on {arj−b}K
j=1for other
choices of 1< r < 2andb >0. In the limit when r→1,a→ ∞ andb→ ∞ this “converges” to
the uniform grid.
37Table 17: Graph classification performance using Pooling-NLSFs with uniform sub-bands. The
symbol (↑)denotes an improvement using Laplacian attention.
MUTAG PTC ENZYMES PROTEINS NCI1 IMDB-B IMDB-M COLLAB
ΘP
ind(L,·) 86.41 ±1.9 68.76 ±0.9 69.88 ±1.3 84.27 ±1.3 80.33 ±0.8 60.40 ±1.3 51.01 ±1.3 71.28 ±0.9
ΘP
ind(N,·) 84.52 ±0.8 67.19 ±1.2 66.37 ±2.1 81.12 ±1.7 77.05 ±2.2 62.08 ±1.6 52.48 ±1.0 72.03 ±1.2
ΘP
val(L,·) 83.49 ±1.1 67.12 ±1.0 65.38 ±1.7 82.89 ±2.2 80.57 ±2.4 72.85 ±1.4 50.91 ±1.4 75.02 ±1.2
ΘP
val(N,·) 83.34 ±1.3 64.22 ±0.9 62.24 ±0.7 80.03 ±0.4 80.90 ±1.6 72.49 ±1.1 52.08 ±0.9 80.89 ±2.2
attP(Θind(L,·),Θval(L,·)) 86.41 ±1.9 68.76 ±0.9 69.88 ±1.3 84.27 ±1.3 80.33 ±0.8 72.85 ±1.4 51.01 ±1.3 76.13 ±0.7(↑)
attP(Θind(N,·),Θval(N,·)) 84.52 ±0.8 67.19 ±1.2 67.13 ±2.2(↑)82.53 ±1.1(↑)80.90 ±1.6 72.49 ±1.1 53.16 ±0.8(↑)80.89 ±2.2
attP(Θind(N,·),Θval(L,·)) 84.52 ±0.8 68.01 ±1.4(↑)66.44 ±0.9(↑)83.52 ±2.4(↑)80.57 ±2.4 72.85 ±1.4 52.48 ±1.0 77.42 ±0.7(↑)
attP(Θind(L,·),Θval(N,·)) 86.41 ±1.9 68.76 ±0.9 69.88 ±1.3 84.27 ±1.3 80.90 ±1.6 72.49 ±1.1 53.48 ±1.2(↑)81.12 ±1.4(↑)
G Additional Related Equivariant GNNs
Equivariant GNNs are designed to handle graph data with symmetries. The output of an equivariant
GNN respects the same transformation applied to the input. Depending on the application, the
transformation could involve translations, reflections, or permutations, to name but a few [ 82,
46,78]. Due to respecting the symmetries, equivariant GNNs can reduce model complexity and
improve generalization [ 35,27], which can be applied to test data and produce more interpretable
representations [ 99,51]. When symmetry plays a critical role, such as in physical simulations,
molecular modeling, and protein folding, equivariant GNNs have been demonstrated to be effective
[20]. For example, [ 91,33] employ spherical harmonics to handle 3D molecular structures, and [ 82]
simplify computations for explicit rotation and translation matrices by focusing on relative distances
between nodes. In addition, [ 30] embeds symmetry transformations by parameterizing convolution
operations over Lie groups, such as rotations, translations, and scalings, through Lie algebra.
38NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We point out that our NLSFs are fully equivariant to graph functional shifts
and show that they have universal approximation properties, where the proposed NLSFs are
based on a new form of spectral domain that is transferable between graphs. We list out our
contributions in Sec. 1.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We present the limitation discussion of our method in Sec. 6 (lines 359-363).
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
39Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Right before and/or after each stated theoretical result, we indicate that the
proof and the theoretical analysis can be found in App. D.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Detailed descriptions of the methodology are provided in the manuscript and
App. E. The method can be re-implemented, and results are reproducible.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
405.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide detailed descriptions of our method’s implementation within the
paper and report the hyper-parameters in App. E.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide comprehensive information on data splits, hyperparameters, and
the criteria for their selection. Additionally, we describe the type of optimizer used and
other relevant training details. All this information is thoroughly detailed in App. E.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: In Sec. 5, we report the node classification quality by computing the average
classification accuracy along with a 95% confidence interval. Additionally, we present the
mean and standard deviation for the graph classification accuracy to provide a comprehensive
understanding of the experimental results.
Guidelines:
• The answer NA means that the paper does not include experiments.
41•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Detailed descriptions of the type of computer resources and time of execution
are provided in App. E and App. F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have respected the NeurIPS code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper presents work whose goal is to advance the field of graph machine
learning. The paper has no foreseeable societal impact.
42Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We provide credit to the original papers of all external data, code, and models
used in this work, ensuring that their contributions are acknowledged and their licensing
terms are followed.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
43• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We provide detailed descriptions of our method’s implementation within the
paper and report the hyper-parameters in App. E.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing nor research with human subjects was involved in this work.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No crowdsourcing nor research with human subjects was involved in this work.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
44•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
45