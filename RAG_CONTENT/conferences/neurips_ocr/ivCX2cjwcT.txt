Identifiable Shared Component Analysis of Unpaired
Multimodal Mixtures
Subash Timilsina
School of EECS
Oregon State University
Corvallis, OR 97331
timilsis@oregonstate.eduSagar Shrestha
School of EECS
Oregon State University
Corvallis, OR 97331
shressag@oregonstate.edu
Xiao Fu
School of EECS
Oregon State University
Corvallis, OR 97331
xiao.fu@oregonstate.edu
Abstract
A core task in multi-modal learning is to integrate information from multiple
feature spaces (e.g., text and audio), offering modality-invariant essential repre-
sentations of data. Recent research showed that, classical tools such as canonical
correlation analysis (CCA) provably identify the shared components up to minor
ambiguities, when samples in each modality are generated from a linear mixture of
shared and private components. Such identifiability results were obtained under
the condition that the cross-modality samples are aligned/paired according to their
shared information. This work takes a step further, investigating shared component
identifiability from multi-modal linear mixtures where cross-modality samples
are unaligned. A distribution divergence minimization-based loss is proposed,
under which a suite of sufficient conditions ensuring identifiability of the shared
components are derived. Our conditions are based on cross-modality distribution
discrepancy characterization and density-preserving transform removal, which
are much milder than existing studies relying on independent component analy-
sis. More relaxed conditions are also provided via adding reasonable structural
constraints, motivated by available side information in various applications. The
identifiability claims are thoroughly validated using synthetic and real-world data.
1 Introduction
The same data entities can often be represented in different feature spaces (e.g., audio, text and image),
due to the variety of sensing modalities or domains. Learning common latent components of data
from multiple modalities is well-motivated in representation learning. The shared components are
considered modality-invariant essential representations of data, which can often enhance performance
of downstream tasks by shedding modality-specific noise [1–4] and avoiding over-fitting [5–7].
A prominent theoretical aspect of shared component learning lies in identifiability of the components
of interest. The literature posed an intriguing theoretical question [1, 2, 8]: If every modality of
data is represented by a linear mixture of shared and private components with an unknown mixing
system, are the shared components identifiable (up to acceptable ambiguities)? Such component
identification problems are often nontrivial due to the ill-posed nature of any linear mixture model
(see, e.g., [9 –14]). Interestingly, the work [1] showed that using the classical canonical correlation
38th Conference on Neural Information Processing Systems (NeurIPS 2024).analysis (CCA) provably find the shared components up to rotation and scaling. In fact, shared
component identification from multimodal/multiview linear mixtures were considered in various
contexts (see, e.g., [15 –18]), although some of these works did not model private components. The
identifiability results in [1, 2] were generalized to nonlinear mixture models as well [4, 19]. The
shared component identification perspective was also related to the success of representation learning
in self-supervised learning (SSL) [5–7].
Nonetheless, the treatment in [1, 2] and the related works [15 –17] all assumed that the cross-modality
data are aligned (i.e., paired) according to their shared components. In many applications, such
as cross-language information retrieval [20 –22], domain adaptation [23 –25], and biological data
translation [26, 27], aligned cross-modality data are hard to acquire, if not outright unavailable.
A natural question is: When the multimodal linear mixtures are unaligned, can the shared latent
components still be provably identified under reasonably mild conditions?
Existing Studies. Theoretical characteristics of unaligned multimodal learning were studied under
various settings. The work [28] considered a case where one modality is a linear transform of another
modality, and showed that the linear transformation is potentially identifiable. The recent work [29]
extended this model to a nonlinear transform setting. However, these works did not consider latent
component models—yet the latter are more versatile in many ways, e.g., facilitating one-to-many
cross-domain translations [30, 31]. The work [32] considered unaligned mixtures of shared and
private components, but the assumptions (e.g., the availability of a large amount of modalities) to
ensure identifiability may not be easy to satisfy. The most related work is perhaps [8]. But their
approach also relied on somewhat stringent assumptions, e.g., that all the latent components are
element-wise statistically independent with at most one component being Gaussian. This is because
their procedure had to invoke the classical independent component analysis (ICA) [33].
Contributions. In this work, we provide a suite of sufficient conditions under which the shared
components can be provably identified from unaligned multimodal linear mixtures up to reasonable
ambiguities. The model and identification problem are referred to as unaligned shared component
analysis (unaligned SCA) in the sequel.
(i) An Identifiable Learning Loss for Unaligned SCA. We propose to tackle the unaligned SCA
problem by matching the probability distributions of linearly embedded multi-modal data. We show
that under reasonable conditions, the linear transformations identifies the shared components up to
the same ambiguities as those in the aligned case [1, 2]. The conditions are considerably milder
compared to the existing unaligned SCA work [8].
(ii) Enhanced Identifiability via Structural Constraints. We come up with two types of structural
constraints, motivated by available side information in applications, to further relax the identifiability
conditions. Specifically, we look into cases where the multi-modal data have similar linear mixing
systems and cases where a few cross-domain aligned samples available. We show that by adding
constraints accordingly, unaligned SCA are identifiable under much milder conditions.
Our contributions primarily lie in identifiability analysis. Nonetheless, we also show the usefulness
of our results in real-world applications, namely, cross-lingual word retrieval ,genetic information
alignment andimage data domain adaptation . Particularly, it shows that our succinct multimodal
linear mixture model can effectively post-process outputs of pre-trained encoders, e.g., those in
[34, 35], to improve data representations and enhance downstream task performance.
Notation. Notation definitions can be found in Appendix A.
2 Background
Generative Model of Interest. Following the classical settings in [1, 2, 15, 16, 18], we consider
modeling the multi-modal data as linear mixtures. More specifically, we adopt the model in [1, 2]that
splits the latent representation of data into shared components and private components:
x(q)=A(q)z(q),z(q)= [c⊤,(p(q))⊤]⊤, q= 1,2, (1)
where x(q)∈Rd(q)represents the data from the qth modality, z(q)∈RdC+d(q)
Prepresents the
corresponding latent code, c∈RdCandp(q)∈Rd(q)
Pstand for the shared components and the private
components, respectively. The data x(q)’s are assumed to be zero-mean, which can be enforced by
2centering. Note that the positions of candpqare not necessarily arranged as [c⊤,(p(q))⊤]⊤(more
generally, z(q)=Π(q)[c⊤,(p(q))⊤]⊤with an unknown permutation matrix Π(q)). However, the
representation in (1)is without loss of generality as one can define A(q):=A(q)(Π(q))⊤to reach the
representation in (1). For all the domains, we have
c∼Pc,p(q)∼Pp(q), (2)
where PcandPp(q)represent the distributions of the shared components and the domain-private
components, respectively. Under (1), the two different range spaces range( A(q))forq= 1,2
represent two feature spaces. Then latent p(q)further distinguishes the modalities and often has
interesting physical interpretation. For example, some vision literature use cto model “content” and
p(q)“style” of the images [31, 36]. In cross-lingual word embedding retrieval [2], crepresents the
semantic meaning of the words, while p(q)represents the language-specific components. The goal of
SCA boils down to finding linear operators to recover cto a reasonable extent.
Aligned SCA: Identifiability of CCA and Extensions. Learning cwithout knowing A(q)is a typical
component analysis problem. Learning latent components from linear mixture models (LMMs) like
x=Azlacks identifiability in general, due to the bilinear nature of the models. This is because one
can find an infinite number of invertible matrices Bsuch that x=ABB−1z. Then, both (A,z)and
(AB,B−1z)can fit to the data x, making the problem ill-posed in terms of solution uniqueness; see,
e.g., [9, 37] and more discussions in Sec. 5. Nonetheless, the works [1, 2] studied the identifiability
ofcunder the model (1), using the assumption that the cross-modality samples share the same care
aligned. In particular, [1] formulated the c-identification problem as a CCA problem:
minimize
{Q(q)}2
q=1EQ(1)x(1)−Q(2)x(2)2
2
(3a)
subject to Q(q)Eh
x(q)(x(q))⊤i
(Q(q))⊤=Iq= 1,2, (3b)
where Q(q)∈RdC×d(q). The expectation in (3a)is taken from the joint distribution of the aligned
pairsPx(1),x(2), where every pair (x(1),x(2))shares the same c. The formulation aims to find Q(q)
such that the transformed representations of the aligned pairs Q(1)x(1)andQ(2)x(2)are equal. In [1],
it was shown that
bQ(q)x(q)=Θc (4)
under mild conditions (see Appendix E.1 for details), where (bQ(1),bQ(2))is an optimal solution of
the CCA formulation and Θis a certain non-singular matrix. Eq. (4)means that bQ(q)finds the range
space where clives in, i.e., range( A(q)
1:dC)under our notation.
Unaligned SCA: Existing Result and Theoretical Gap. The work in [8] studied the identifiability
ofcunder (1)when x(1)andx(2)areunaligned . Their approach works under the condition
that the elements of z(q)= [c⊤,(p(q))⊤]⊤are mutually statistically independent . There, bz(q)=
Π(q)Σ(q)z(q)is assumed to have been estimated by ICA, where Π(q)andΣ(q)represent the scaling
and permutation ambiguities, respectively, which cannot be removed by ICA. The work [8] assumed
Σ(q)=Iby imposing a unit-variance assumption on all the z(q)
i’s. Then, a cross-domain matching
algorithm is used to match the shared elements in bz(1)andbz(2). The formulation can be summarized
as finding dCpairs of non-repetitive (i, j)such that e⊤
ibz(1)ande⊤
jbz(2)have identical distributions,
where eiis the ith unit vector. Denote bc(1)
m=e⊤
imbz(1)andbc(2)
m=e⊤
jmbz(2)form∈[dC]. It can be
shown that
bc(q)
m=kc(q)
π(m), m∈[dC], (5)
where k∈ {+1,−1}andπis a permutation of {1, . . . , d C}(see details in Appendix E.2 summarized
from [8]). This method effectively applies ICA to each modality, and thus the ICA identifiability
conditions [33] have to met by x(1)andx(2)individually. However, if one only aims to extract Θc
as in CCA, these assumptions appear to be overly stringent.
3−3−2−10 1 2 3 4−4−3−2−101234Θ(1)c
−3−2−10 1 2 3 4−4−3−2−101234Θ(2)cFigure 1: Scatter plots of matched distribution Θ(1)c
(left) and Θ(2)c(right) when cfollows the Gaussian
distribution. Colors in the scatter plot represent align-
ment; same color represent the data are aligned.
Figure 2: Illustration of A(1)in
Assumption 1 in a case where
dC= 2andd(1)
P= 1.
3 Proposed Approach
Unaligned SCA: Problem Formulation We assume that x(q)’s are zero-mean. We use the notation
from CCA in (3a). However, since no aligned samples are available, we replace the sample-level
matching objective with a distribution matching (DM) module, as DM can be carried out without
sample level alignment:
find Q(q)∈RdC×d(q), q= 1,2, (6a)
subject to Q(1)x(1)(d)= = =Q(2)x(2), (6b)
Q(q)Eh
x(q)(x(q))⊤i
(Q(q))⊤=Iq= 1,2. (6c)
where “ u(d)= = =v” means the distributions of uandvare the same.
The formulation in (6)can be realized using various distribution matching tools, e.g., maximum mean
discrepancy (MMD) [38] and Wasserstein distance [39]. We use the adversarial loss:
min
Q(1),Q(2)max
fEx(1)log
f(Q(1)x(1))
+Ex(2)log
1−f(Q(2)x(2))
+λ2X
q=1R
Q(q)
,(7)
The first and second terms comprise the adversarial loss from GAN [40]. It finds Q(q)to confuse the
best-possible discriminator f:RdC→R, where fis represented by a neural network in practice. It is
well known that the minimax optimal point of the first two terms is attained when (6b) is met [40]. We
useR(Q(q)) =∥Q(q)E[x(q)(x(q))⊤](Q(q))⊤−I∥2
Fto “lift” the constraints. This way, the learning
criterion in (7) can be readily handled by any off-the-shelf adverserial learning tools.
Identifiability of Unaligned SCA As we saw in Theorem 4, CCA identifies bQ(q)x(q)=Θcwhere
Θ∈RdC×dCunder the settings of aligned SCA. Establishing a similar result for unaligned SCA is
much more challenging. First, it is unclear if (6b) could disentangle cfromp(q). In general, Q(q)x(q)
could still be a mixture of candp(q)yet (6b) still holds (e.g., when both candp(q)are Gaussian.)
Second, even when the disentanglement is attained via enforcing (6b) and we have Q(q)x(q)=Θ(q)c,
in general it does not hold that Θ(1)=Θ(2). This is because Θ(1)c(d)= = =Θ(2)cwhere Θ(1)̸=Θ(2)
can still be perfectly met (e.g., when PΘ(q)cis symmetric Gaussian in Fig. 1 ). However, Θ(1)̸=Θ(2)
means that the extracted representations from the two modalities are not matched. This creates
challenges for applications like cross-domain information retrieval, language translation, or domain
adaptation.
Our intuition is as follows: If the two distributions Pc,p(1)andPc,p(2)are very different, then
Q(1)x(1)(d)= = =Q(2)x(2)cannot hold unless Q(q)A(q)= [Θ(q),0]. We use the following to charac-
terize such difference between the joint distributions:
4Assumption 1 (Modality Variability) .For any two linear subspaces P(q)⊂RdC+d(q)
P, q= 1,2, with
dim(P(q)) =d(q)
P,P(q)̸=0×Rd(q)
Pand linearly independent vectors {y(q)
i∈RdC+d(q)
P}dC
i=1, q=
1,2, the sets A(q)= conv {0,y(q)
1, . . . ,y(q)
dC}+P(q), q= 1,2,are such that if Pc,p(q)[A(q)]>0
forq= 1 orq= 2, then there exists a k∈Rsuch that the joint distributions Pc,p(1)[kA(1)]̸=
Pc,p(2)[kA(2)], where kA(q)={ka|a∈ A(q)}.
The condition in Assumption 1 is a geometric way to characterize the difference between Pc,p(1)
andPc,p(2)—if the joint distributions have different measures for all possible “stripes”, each being
a direct sum of a subspace and a convex hull (see Fig. 2), then Pc,p(1)andPc,p(2)must be very
different. Note that the difference is contributed by the modality-specific term p(q), and thus we call
this condition “modality variability”. Modality variability is similar to the “domain variablity” used
in [32, 41]—both characterize the discrepancy of the joint probabilities Pc,p(1)andPc,p(2). However,
there are key differences: The domain variability was defined in a unified latent domain over arbitrary
setsA, which could be stringent. Instead, we use the fact that (6)relies on linear operations to
construct A(q), which makes the condition defined over a much smaller class of sets—thereby largely
relaxing the requirements. Restricting A(q)to be stripes also makes the modality variability condition
much more relaxed compared to the domain variability condition.
We show the following:
Theorem 1. Under Assumption 1 and the generative model in (1), denote any solution of (6)asbQ(q)
q= 1,2. Then, if the mixing matrices A(q)are full column ranks and E[cc⊤])is full rank, we have
bQ(q)x(q)=Θ(q)c. In addition, assume that either of the following is satisfied:
(a)The individual elements of the content components are statistically independent and non-
Gaussian. In addition, ci(d)
̸=kcj,∀i̸=j,∀k∈Randci(d)
̸=−ci,∀i, i.e., the marginal
distributions of the content elements cannot be matched with each other by mere scaling.
(b)The support of Pc, denoted by C, is a hyper-rectangle, i.e., C= [−a1, a1]×···× [−adC, adC].
Further, suppose that ci(d)
̸=kcj,∀i̸=j,∀k∈Randci(d)
̸=−ci,∀i.
Then, we have bQ(q)x(q)=Θc,i.e.,Θ(q)=Θfor all q= 1,2, where Θ(q).
In Theorem 1, Assumption 1 is used to guarantee bQ(q)x(q)=Θ(q)cand either of conditions (a) or
(b) is used to make sure Θ(1)=Θ(2). Note that both (a) and (b) are milder than those in [8] (cf.
Theorem 5), where the element-wise statistical independence of z(q)was relied on to find shared
representation of x(1)andx(2). The proof is in Appendix B.
Numerical Validation. In Fig. 3, the top and bottom rows validate Theorem 1 under the assumptions
in (a) and (b), respectively. In the top row, we set c∈R2, where c1is sampled from Gaussian
mixtures with three components and c2is sampled from a Gamma distribution (and c1⊥ ⊥c2). We
setp(1)andp(2)as one-dimensional Laplacian and uniform distributions. In the bottom row, the
dimensions of candp(q)forq= 1,2are unchanged, but their distributions are replaced in order to
satisfy conditions in (b) (see details in Appendix F). One can see that clearly bc(q)=Θc; i.e., the
learned bc(q)forq= 1,2are identically rotated and scaled versions of c.
A remark is that our framework still allows to identify individual ci’s as in [8].
Corollary 1. Under the conditions in Theorem 1 (a), Assume that at most one cifori∈[dC]is
Gaussian. Then, the components of care identifiable up to permutation and scaling ambiguities by
applying ICA to bc(q)=bQ(q)x(q)for either q= 1orq= 2.
The corollary means that to identify individual ci, using our formulation still enjoys much milder
conditions relative to [8]. Specifically, our condition only specifies the independence among elements
ofc, but the condition in [8] needs that all the elements in z(q)= [c⊤,(p(q))⊤]⊤are independent.
5−2−10 1 2 3 4 50.02.55.07.510.012.515.017.520.0(a)c
−75−50−250255075100−60−40−200204060x(1)
−75−50−25025 50 75−75−50−250255075x(2)
−6−4−2 0−20246/hatwidec(1)
−8−6−4−2 0−20246/hatwidec(2)
−3−2−10 1 2 3−3−2−10123(b)c
−100−75−50−250255075100−60−40−20020406080100x(1)
−60−40−20020406080100−100−75−50−250255075100x(2)
−5−4−3−2−10 1−2−101234/hatwidec(1)
−5−4−3−2−10 1−2−101234/hatwidec(2)Figure 3: Validation of Theorem 1. Top row: results under assumption (a). Bottom row: results under
assumption (b).
4 Enhanced Identifiability via Structural Constraints
Theorem 1 was well-supported by the synthetic data experiments. However, our experiments found
that the learning criterion (6)often struggles to produce sensible results in some applications. Our
conjecture is that the Assumptions in Theorem 1 (a) and (b) might not have been satisfied by the real
data under our tests. Although they are not necessary conditions for identifiability, these conditions
do indicate that the requirements to guarantee identifiability of unaligned SCA using (6)are nontrivial
to meet. In this section, we explore a couple of structural constraints arising from side information in
applications to remove the need for the relatively stringent assumptions on c.
Homogeneous Domains. The first structural constraint that we consider is A(q)=Aforq=
1,2. This model is motivated by the fact that advanced representation learning tools, e.g., self-
supervised learning tools (e.g., SimCLR [42]) and foundation models (e.g., CLIP [35]), are already
capable of mapping the data clusters to a shared linearly separable space—which indicates that the
representations share a subspace, i.e., x(q)≈Az(q). Under such circumstances, the proposed model
and method can be used to further process the data by discarding the private components in the latent
representation.
Here, we consider the special case of generative process in (1) where,
x(q)=A[c⊤,(p(q))⊤]⊤. (8)
Under this model, we look for the shared components by solving (6)with a single Q=Q(1)=Q(2).
We use the following version of the modality variability condition:
Assumption 2. For any linear subspace P ⊂ RdC+dP, dP=d(1)
P=d(2)
P, with dim(P) =dP,
P ̸=0×RdPand linearly independent vectors {yi∈RdC+dP}dC
i=1, q= 1,2, the sets A=
conv{0,y1, . . . ,ydC}+P, q= 1,2.are such that if Pc,p(q)[A]>0forq= 1orq= 2, then the
joint distributions Pc,p(1)[kA]̸=Pc,p(2)[kA]for some k∈R.
Theorem 2. Consider the mixture model in (8). Assume that rank(A) = dC+dPand
rank(E[cc⊤]) =dC, and that Assumption 2 holds. Denote bQas any solution of (6)by constraining
Q=Q(1)=Q(2). Then, we have bQx(q)=Θc.
One can see that the conditions (a) and (b) in Theorem 1 are completely removed, if the structure
A(1)=A(2)is imposed. In fact, the result in Theorem 2 is expected and readily seen from the proof
of Theorem 1, as the cause for Θ(1)̸=Θ(2)is the use of two different Q(q)’s. Nonetheless, this
simple variation will prove useful in a series of real-data experiments.
The Weakly Supervised Case. Another way to add structural constraints is to use available auxiliary
information. For example, some datasets have weak annotations and selected pairs; see, e.g., [43, 44].
Assumption 3 (Weak Supervision) .There exist a set of available aligned samples (x(1)
ℓ,x(2)
ℓ)for
ℓ∈ L such that x(q)
ℓ=A(q)z(q)
ℓ,z(q)
ℓ= [c⊤
ℓ,(p(q)
ℓ)⊤]⊤;i.e.,(x(1)
ℓ,x(2)
ℓ)share the same cℓ.
6The condition can be added into our formulation in (6) as a constraint, i.e.,
Q(1)x(1)
ℓ=Q(2)x(2)
ℓ,∀ℓ∈ L. (9)
In the next theorem, we show that the incorporation of aligned samples helps relax conditions (a) and
(b) in Theorem 1:
Theorem 3. Assume that Assumption 1 is satisfied, that |L| ≥ dCpaired samples (x(1)
ℓ,x(2)
ℓ)are
available, that A(q)forq= 1,2have full column rank, and that Pcis absolutely continuous. Denote
(bQ(1),bQ(2))as any optimal solution of (6)under the constraint (9). Then, we have bQ(q)x(q)=Θc.
The proof and synthetic data validation can be found in Appendices D and F, respectively. Note that
to realize (9), one only needs to add a regularization term βP
ℓ∈L∥Q(1)x(1)
ℓ−Q(2)x(2)
ℓ∥2
2to the
loss in (7), where β≥0is a tunable parameter. The overall loss is still differentiable and thus can be
easily handled by gradient based approaches.
A remark is that our weakly supervised formulation can use as few as dCpairs of (x(1)
ℓ,x(2)
ℓ)to
establish identifiability of shared component. In contrast, CCA requires at least dC+d(1)
P+d(2)
P
pairs to attain the same identifiability (cf. Appendix. E.1).
Private Component Identifiability. Although our focus is shared component identification, we show
that private components are also identifiable with additional assumptions; see Appendix H.
5 Related Works
Identifiability of Component Analysis under Linear Mixture Models. Various component analysis
models were studied in the past several decades, e.g., principal component analysis [45], independent
component analysis [33], sparse component analysis [10, 12], bounded component analysis [13],
simplex component analysis [46, 47], and polytopic component analysis [14]—motivated by their
applications in dimensionality reduction, representation learning, and latent variable identification
(see, e.g., topic mining [48, 49], hyperspectral unmixing [46, 47], audio/speech separation [33] and
community detection [50]). The classical component analysis tools mostly study a single modality.
The identifiability results under these models are well developed and documented.
Identifiability of Shared Components from Aligned Modalities. Modeling multimodal data as two
or more linear/nonlinear mixtures of latent components was considered in CCA-related works
[1, 2, 15, 19], independent vector analysis (IV A) works [17, 18], multiview ICA works [16, 51],
and SSL works [5 –7, 52]. Partitioning the latent components into shared and private blocks was
considered in [1, 2, 4, 5, 7, 52]. Shared component identifiability was established at the block level
(see, e.g., [1, 2, 5]) and the individual component level (e.g., [51]) in these works. Nonetheless, they
all rely on completely paired/aligned cross-modality samples, which we do not use in this work.
Distribution Matching and Unaligned Multimodal Analysis. Using distribution matching in unaligned
multimodal data analytics for different purpose also has a long history; see applications in image-
to-image translation [53], domain adaptation [54], cross-platform image super-resolution [55], and
cross-domain information retrieval [21]. The recent works [56] and [57] pointed out the identifiability
challenge and the existence of density-preserving transforms. The works in [28, 29] started studying
the uniqueness issues in distribution matching. However, the latent mixture models were not studied
in this line of work.
Identifiability of Unaligned SCA. The works in [32, 41] investigated the shared component identifiabil-
ity when the multimodal data are nonlinear mixtures of content and style (which are shared and private
components, respectively) under the same mixing system. Hence, our identical linear mixing case in
Theorem 2 can be understood as a special case of theirs. But their analysis relies on the assumption
that all the latent components are statistically independent, which is much stronger than our conditions
in Theorem 2. Their results also require that there are a large amount of modalities available. But our
proof works for just two modalities. The most related work is [8], which uses the model in (1)in the
context of multi-view causal graph learning. As discussed before, their assumptions on the latent
components are much stronger than ours (see Corollary 1 and Appendix E.2).
7Table 1: Classification accuracy on the target domain of office-31 dataset (ResNet50 embedding).
source →target ResNet DANN MDD MCC SDAT ELS Proposed
A→W 85.2±0.2 86.3±0.3 86.4±0.4 88.3±0.3 88.6±0.4 87.2±0.3 90.4±0.4
D→W 97.5±0.1 97.4±0.3 97.7±0.1 96.9±0.1 97.6±0.1 97.7±0.1 97.8±0.2
W→D 99.5±0.3 98.7±0.2 99.7±0.1 97.4±0.2 99.1±0.2 99.3±0.2 99.5±0.3
A→D 89.4±0.2 84.3±0.4 89.9±0.2 87.4±0.5 86.3±0.4 87.1±0.2 90.1±0.3
D→A 71.4±0.3 71.7±0.4 70.6±0.3 74.9±0.4 72.3±0.4 71.6±0.3 71.9±0.1
W→A 73.1±0.2 73.5±0.2 72.3±0.4 73.0±0.4 73.6±0.3 73.7±0.3 74.6±0.1
Average 86.0±0.2 85.3±0.3 86.1±0.2 86.3±0.3 86.2±0.3 86.1±0.2 87.3±0.2
6 Numerical Validation
More Synthetic-Data Validation. We first validate our proposed method on synthetic data that
follows our model; see Appendix F for details.
Application (i) - Domain Adaptation. We first test the proposed methods over a number of domain
adaptation (DA) tasks. In DA, we have the source domain data {x(1)}and the target domain {x(2)},
respectively. Only the source domain data have labels and the two domains are unaligned. We hope
to use our method to find shared representations of source and target, and thus the classifier trained
using source data can also work well on the target data.
Dataset : We use two standard benchmarks of DA, i.e., Office-31 [58] and Office-Home [59]. The
Office-31 dataset has 4652 images and 31 categories from three domains, namely, Amazon images
(A), Webcam images ( W) and DSLR images ( D). The Office-Home dataset contains 15,500 images
with 65 object classes from four domains, i.e., Artistic images ( Ar), Clip art images ( Cl), Product
images ( Pr), and Real-world images ( Rw).
Setup : We first test the homogeneous domain model in Sec. 4. The images are pre-processed using a
ResNet50-based image encoder pre-trained over ImageNet1k [42]. As mentioned, it was observed
that self-supervised representation encoders find embeddings that are linearly separable [42], which
justifies the use of the model x(q)≈Az(q)in the embedding domain. After pre-processing, each
image is represented by d(q)= 2048 features for q= 1,2. We set dC= 256 forOffice-31 and
dC= 512 forOffice-Home . More detailed settings are in Appendix G.
Baselines and Training Setup : The baselines are representative DA methods, namely, DANN [25],
MDD [60], MCC [61], SDAT [62], and ELS [63]. All the baselines use the same encoder-produced
embeddings as inputs; see Appendix G.1 for their configurations. We also use ResNet encoder’s
outputs as an extra baseline as it learns informative and transferable features from the ImageNet-
1K dataset. We follow the training strategies adopted by the baselines [25, 60, 62] to learn a
classifier jointly with the shared latent components. This strategy arguably regularizes towards more
classification-friendly geometry of the shared features. Therefore we append a cross-entropy (CE)
based classifier training module to our loss in (7)that learns our feature extractor Q. More details are
in Appendix G.1.
Metric : The evaluation metric is the classification accuracy in the target domain {x(2)}. The classifier
is trained with the projected source domain bQx(1)and the associated labels.
Result : Table 1 and Table 2 show the classification accuracy (mean ±std) on Office-31 andOffice-
Home , respectively. The results are averaged over 5 runs. One can observe that the proposed method
offers the best and second best performance in most of the cases. In some tasks (e.g.,“ A→W”,
“Ar→Cl”, “Ar→Pr” and “ Rw→Cl”), the proposed method outperforms the best-performing base-
lines by at least 2% in accuracy.
More results on the DA task can be found in Appendix G.1.
Application (ii) - Single Cell Sequence Analysis. In biomedical research, it is desired to fuse
measurements from multiple sensorial modalities of the same cells, in order to have better characteri-
zations of the cells. However, obtaining multimodal data of the same cells simultaneously is almost
impossible, due to the sensing limitations. Therefore, many methods are proposed in the literature for
aligning unpaired multi-modal single cell data [27, 64, 65]. We focus on the following two modalities
of single-cell data [66]: (1) the RNA sequences {x(1)}and (2) the ATAC sequences {x(2)}.
8Table 2: Classification accuracy on the target domain of office-Home dataset (ResNet50 embedding).
source →target ResNet DANN MDD MCC SDAT ELS Proposed
Ar→Cl 42.0±0.2 46.7±0.2 47.4±0.3 44.4±0.3 47.3±0.4 48.5±0.2 51.0±0.3
Ar→Pr 69.2±0.1 70.2±0.4 72.8±0.4 72.4±0.2 71.1±0.3 71.0±0.3 75.8±0.1
Ar→Rw 80.2±0.3 81.2±0.4 81.2±0.1 80.3±0.3 80.5±0.1 80.8±0.4 82.5±0.2
Cl→Ar 60.7±0.4 60.8±0.3 62.4±0.1 59.2±0.4 57.6±0.2 59.8±0.1 62.7±0.4
Cl→Pr 71.0±0.1 69.8±0.3 70.0±0.4 71.1±0.4 66.5±0.1 68.5±0.2 72.5±0.3
Cl→Rw 74.8±0.2 73.3±0.1 74.1±0.1 76.2±0.2 70.7±0.1 71.7±0.1 75.8±0.1
Pr→Ar 60.6±0.2 62.2±0.1 64.3±0.1 59.2±0.1 62.5±0.4 60.9±0.2 64.4±0.3
Pr→Cl 44.8±0.1 48.8±0.1 48.0±0.3 46.2±0.2 49.0±0.3 49.6±0.3 50.4±0.1
Pr→Rw 79.6±0.1 80.3±0.4 79.6±0.3 80.3±0.2 80.0±0.1 79.2±0.1 81.7±0.2
Rw→Ar 70.1±0.2 71.5±0.1 71.4±0.3 67.8±0.2 71.6±0.4 71.3±0.4 72.6±0.1
Rw→Cl 45.8±0.2 50.9±0.2 50.3±0.1 50.0±0.2 51.4±0.1 50.7±0.1 53.2±0.1
Rw→Pr 80.7±0.1 80.6±0.4 81.1±0.1 81.2±0.1 80.7±0.1 79.8±0.3 82.9±0.3
Average 64.9±0.1 66.3±0.2 66.8±0.2 65.6±0.2 65.7±0.2 65.9±0.2 68.7±0.2
5 8 10 12 14 16 18 20
K of the K-NN evaluation metric0.000.100.200.300.400.500.600.700.80K-NN accuracy
Proposed: No paired
Proposed: 1 paired
Proposed: 50 pairedProposed: 256 paired
CM-AE: 256 paired + class labels
CM-AE: 770 paired + class labels
Figure 4: k-NN accuracy for single-cell
sequence alignment.Table 3: Average precision P@1 of cross-language in-
formation retrieval.
source →target Adv- NN proposed - NN Adv- CSLS proposed - CSLS
en→es 61.3 66.4 70.2 74.9
es→en 55.4 65.3 67.6 75.6
en→it 48.2 54.4 60.8 67.7
it→en 55.2 51.9 63.8 66.0
en→fr 63.6 60.2 72.6 73.7
fr→en 55.4 58.4 64.1 71.4
en→de 51.4 56.7 59.3 67.6
de→en 42.5 57.0 51.0 59.3
en→ru 32.7 34.9 38.6 41.4
ru→en 27.6 41.6 35.0 50.8
en→ar 12.6 22.7 16.7 29.1
ar→en 15.7 26.9 20.1 35.6
en→vi 2.1 10.4 7.7 22.8
vi→en 2.7 17.3 4.4 33.0
Average 37.6 44.5 45.1 54.9
Dataset : We use human lung adenocarcinoma A549 cells data from [66]. The dataset contains 1,874
samples of RNA sequences {x(1)}and ATAC sequences {x(2)}. Each data set is split into 1534
training samples and 340 testing samples as in [27]. The data have labeled associations between
the two domains—part of which will be used to test our weakly supervised formulation. For this
experiment, features of RNA sequence and the ATAC sequence have dimensions of d(1)= 815 and
d(2)= 2613 , respectively. We set dC= 256 . We use our weakly supervised formulation as shown in
(9). We uniformly sampled a set of indices from the training set to serve as L.
Baseline and Metric : We use weakly supervised algorithm, namely, cross-modal autoencoder (CM-
AE) work in [27], as a baseline, which also learns the shared representation between unaligned RNA
and ATAC sequences. We use the K-nearest neighbor ( k-NN) accuracy to evaluate the performance
as suggested in [27].
Result : The plot in Fig. 4 shows the k-NN accuracy of the methods on the test set. Results show
the mean and standard deviation over 10 runs, each having a different random initialization. For
the proposed method, we vary the number of available paired samples from 0(cf. Theorem 1) to
dC= 256 (cf. Theorem 3). Note that the baseline uses more (i.e., 256and770) paired samples. It
also needs additional class labels, i.e., y(q)
ifor the ith sample x(q)
i. Here, y(q)
irepresents the number
of hours ( 0,1or3) of cell treatment [27, 66]. The proposed method without any supervision (i.e., 0
paired samples) already exhibits around 3 times greater k-NN accuracy compared to the baseline for
allk. Moreover, including just one paired sample boosts the k-NN accuracy of the proposed method
to around 5 times higher than the baseline for all k. Finally, one can observe a steadily increasing
k-NN accuracy with respect to the number of available paired samples. This corroborates with our
Theorem 3.
9Application (iii) - Multi-lingual Information Retrieval. We also evaluated our method on a word
embedding association problem from the natural language processing literature [20, 21]. This task
aims to associate high-dimensional word embeddings across different languages according to their
semantic meaning. The word embeddings in two languages are represented using two sets of vectors,
i.e.,{x(1)
i}I
i=1and{x(2)
j}J
j=1. The postulate is that if x(1)
iandx(2)
jhave the same meaning (e.g.,
both representing “cat”) in two languages (e.g., English and German), they should share a latent
components c.
Dataset : We use the word embeddings from the MUSE dataset ( https://github.com/
facebookresearch/MUSE ) [21]. These monolingual word embedding are generated using fast-
Text [67] and has dimensions of d(q)= 300 forq= 1,2. The training dataset include 200,000 word
embeddings in each language. In our experiment we set dC= 256 . We follow the generative model
under (8) and run the formulation in (7) to learn the linear transformation Q.
Baseline : We use Adv [21] as the baseline which also uses distribution matching between two
language domains. Unlike our method, Advdoes not use linear mixture models.
Metric : We follow [21] to use the average precision score calculated based on nearest neighbor (NN)
andcross domain similarity local scaling (CSLS). Precision at k(“kprecision”) is computed by the
number of times that one of the correct translations of source word is retrieved at top- kresults ( k={1,
5, 10}). The final score is normalized to be in the range of 0 to 100, with 100 being the highest score
indicating the best performance. To evaluate the performance, we use the same test data as in [21].
For each source and target language pair, this dataset includes 1,500 source word embeddings. The
source embeddings are used to retrieve corresponding embeddings from a pool of 200,000 target
word embeddings.
Result : Table 3 reports the P@1 scores over the test data calculated for each source and target
language pair. The languages are denoted as as en- English, es- Spanish, it- Italian, fr- French,
de- Germany, ru- Russian, ar- Arabic and vi- Vietnamese. One can observe that the proposed
method exhibits a better precision performance than that of Advin most of the translation tasks. In
particular, the proposed method significantly outperforms the baseline on the tasks en→ar,ar→en,
en→viandvi→en, showing at least 10% precision gains. Similarly, our method shows at least 5%
improvements in both NN and CSLS based precision metrics in en→esandes→entasks.
More details and additional experiments can be found in Appendix G.3.
7 Conclusion
In this work, we considered the problem of identifying shared components from unaligned multi-
domain mixtures. We proposed a learning loss that matches the distributions of linearly transformed
data. Based on this loss, we came up with a suite of sufficient conditions to ensure the identifiability
of shared components. Furthermore, we proposed modified models and losses that enjoy more relaxed
conditions for shared component identifiability. This was achieved via introducing structural con-
straints, namely, the homogeneity of the mixing systems and the existence of weak supervision. Our
theoretical claims were validated with both synthetic and real-world data, demonstrating soundness
of the theorems and usefulness of the models/algorithms.
Limitations. First, our conditions for shared component identification are sufficient. The necessary
conditions are not underpinned, but necessary conditions assist understanding the limitations of
the models and algorithms. Second, our methods were developed under the linear mixture model,
which has limited expressiveness, and thus often requires pre-processing to approximately meet
the model specification. We expect that results with similar flavors to be derived for nonlinear
models in the future. Third, the results were derived under an unlimited data assumption. It would
be interesting have a finite sample analysis. Finally, optimizing GAN-based losses is sensitive
to hyperparameter settings. Back-propagation based minimax optimization occasionally fails to
converge. More optimization-friendly losses and more stable algorithms are desirable in the context
of distribution matching.
10Acknowledgment
This work is supported in part by the Army Research Office (ARO) under Project ARO W911NF-21-
1-0227, and in part by the National Science Foundation (NSF) CAREER Award ECCS-2144889.
References
[1]M. S. Ibrahim, A. S. Zamzam, A. Konar, and N. D. Sidiropoulos, “Cell-edge detection via
selective cooperation and generalized canonical correlation,” IEEE Transactions on Wireless
Communications , vol. 20, no. 11, pp. 7431–7444, 2021.
[2]M. Sørensen, C. I. Kanatsoulis, and N. D. Sidiropoulos, “Generalized canonical correlation
analysis: A subspace intersection approach,” IEEE Transactions on Signal Processing , vol. 69,
pp. 2452–2467, 2021.
[3]P. Rastogi, B. Van Durme, and R. Arora, “Multiview LSA: Representation learning via gen-
eralized CCA,” in Proc. Conference of the North American chapter of the Association for
Computational Linguistics: human language technologies , 2015, pp. 556–566.
[4]Q. Lyu and X. Fu, “Nonlinear multiview analysis: Identifiability and neural network-assisted
implementation,” IEEE Transactions on Signal Processing , vol. 68, pp. 2697–2712, 2020.
[5]J. V on Kügelgen, Y . Sharma, L. Gresele, W. Brendel, B. Schölkopf, M. Besserve, and F. Lo-
catello, “Self-supervised learning with data augmentations provably isolates content from
style,” in Advances in Neural Information Processing Systems (NeurIPS) , vol. 34, 2021, pp.
16 451–16 467.
[6]Q. Lyu, X. Fu, W. Wang, and S. Lu, “Understanding latent correlation-based multiview learning
and self-supervision: An identifiability perspective,” in Proc. International Conference on
Learning Representations (ICLR) , 2022.
[7]I. Daunhawer, A. Bizeul, E. Palumbo, A. Marx, and J. E. V ogt, “Identifiability results for multi-
modal contrastive learning,” in Proc. International Conference on Learning Representations
(ICLR) , 2022.
[8]N. Sturma, C. Squires, M. Drton, and C. Uhler, “Unpaired multi-domain causal representation
learning,” in Advances in Neural Information Processing Systems (NeurIPS) , vol. 34, 2021.
[9]P. Comon and C. Jutten, Handbook of Blind Source Separation: Independent component analysis
and applications . Academic press, 2010.
[10] J. Hu and K. Huang, “Global identifiability of ℓ1-based dictionary learning via matrix volume
optimization,” in Advances in Neural Information Processing Systems (NeurIPS) , vol. 36, 2024.
[11] N. Gillis, “The why and how of nonnegative matrix factorization,” Regularization, optimization,
kernels, and support vector machines , vol. 12, no. 257, pp. 257–291, 2014.
[12] J. Sun, Q. Qu, and J. Wright, “Complete dictionary recovery over the sphere i: Overview and
the geometric picture,” IEEE Transactions on Information Theory , vol. 63, no. 2, pp. 853–884,
2016.
[13] A. T. Erdogan, “A class of bounded component analysis algorithms for the separation of both
independent and dependent sources,” IEEE Transactions on Signal Processing , vol. 61, no. 22,
pp. 5730–5743, 2013.
[14] G. Tatli and A. T. Erdogan, “Polytopic matrix factorization: Determinant maximization based
criterion and identifiability,” IEEE Transactions on Signal Processing , vol. 69, pp. 5431–5447,
2021.
[15] F. R. Bach and M. I. Jordan, “A probabilistic interpretation of canonical correlation analysis,”
2005.
[16] H. Richard, P. Ablin, B. Thirion, A. Gramfort, and A. Hyvarinen, “Shared independent compo-
nent analysis for multi-subject neuroimaging,” in Advances in Neural Information Processing
Systems (NeurIPS) , vol. 34, 2021, pp. 29 962–29 971.
[17] Y .-O. Li, T. Adali, W. Wang, and V . D. Calhoun, “Joint blind source separation by multiset
canonical correlation analysis,” IEEE Transactions on Signal Processing , vol. 57, no. 10, pp.
3918–3929, 2009.
11[18] M. Anderson, G.-S. Fu, R. Phlypo, and T. Adalı, “Independent vector analysis: Identification
conditions and performance bounds,” IEEE Transactions on Signal Processing , vol. 62, no. 17,
pp. 4399–4410, 2014.
[19] P. A. Karakasis and N. D. Sidiropoulos, “Revisiting deep generalized canonical correlation
analysis,” IEEE Transactions on Signal Processing , vol. 71, pp. 4392–4406, 2023.
[20] G. Lample, A. Conneau, L. Denoyer, and M. Ranzato, “Unsupervised machine translation using
monolingual corpora only,” in Proc. International Conference on Learning Representations
(ICLR) , 2018.
[21] G. Lample, A. Conneau, M. Ranzato, L. Denoyer, and H. Jégou, “Word translation without
parallel data,” in Proc. International Conference on Learning Representations (ICLR) , 2018.
[22] E. Grave, A. Joulin, and Q. Berthet, “Unsupervised alignment of embeddings with wasserstein
procrustes,” in Proc. International Conference on Artificial Intelligence and Statistics (AISTATS) .
PMLR, 2019, pp. 1880–1890.
[23] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira, “Analysis of representations for domain
adaptation,” in Advances in Neural Information Processing Systems (NeurIPS) , vol. 19, 2006.
[24] M. Long, Y . Cao, J. Wang, and M. Jordan, “Learning transferable features with deep adaptation
networks,” in Proc. International Conference on Machine Learning (ICML) . PMLR, 2015, pp.
97–105.
[25] Y . Ganin and V . Lempitsky, “Unsupervised domain adaptation by backpropagation,” in Proc.
International Conference on Machine Learning (ICML) . PMLR, 2015, pp. 1180–1189.
[26] S. Waaijenborg, P. C. Verselewel de Witt Hamer, and A. H. Zwinderman, “Quantifying the
association between gene expressions and dna-markers by penalized canonical correlation
analysis,” Statistical applications in genetics and molecular biology , vol. 7, no. 1, 2008.
[27] K. D. Yang, A. Belyaeva, S. Venkatachalapathy, K. Damodaran, A. Katcoff, A. Radhakrishnan,
G. Shivashankar, and C. Uhler, “Multi-domain translation between single-cell imaging and
sequencing data using autoencoders,” Nature communications , vol. 12, no. 1, p. 31, 2021.
[28] I. Gulrajani and T. Hashimoto, “Identifiability conditions for domain adaptation,” in Proc.
International Conference on Machine Learning (ICML) , 2022, pp. 7982–7997.
[29] S. Shrestha and X. Fu, “Towards identifiable unsupervised domain translation: A diversified
distribution matching approach,” in Proc. International Conference on Learning Representations
(ICLR) , 2024.
[30] Y . Choi, Y . Uh, J. Yoo, and J.-W. Ha, “StarGAN v2: Diverse image synthesis for multiple
domains,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,
2020, pp. 8188–8197.
[31] X. Huang, M.-Y . Liu, S. Belongie, and J. Kautz, “Multimodal unsupervised image-to-image
translation,” in Proc. European Conference on Computer Vision (ECCV) , 2018, pp. 172–189.
[32] S. Xie, L. Kong, M. Gong, and K. Zhang, “Multi-domain image generation and translation
with identifiability guarantees,” in Proc. International Conference on Learning Representations
(ICLR) , 2023.
[33] P. Comon, “Independent component analysis, a new concept?” Signal processing , vol. 36, no. 3,
pp. 287–314, 1994.
[34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016, pp. 770–778.
[35] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P. Mishkin, J. Clark et al. , “Learning transferable visual models from natural language super-
vision,” in Proc. International Conference on Machine Learning (ICML) . PMLR, 2021, pp.
8748–8763.
[36] Y . Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, “StarGAN: Unified generative
adversarial networks for multi-domain image-to-image translation,” in Proc. IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , 2018, pp. 8789–8797.
[37] X. Fu, K. Huang, N. D. Sidiropoulos, and W.-K. Ma, “Nonnegative matrix factorization for
signal and data analytics: Identifiability, algorithms, and applications,” IEEE Signal Processing
Magazine , vol. 36, no. 2, pp. 59–80, 2019.
12[38] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. Smola, “A kernel two-sample
test,” The Journal of Machine Learning Research , vol. 13, no. 1, pp. 723–773, 2012.
[39] C. Villani et al. ,Optimal transport: old and new . Springer, 2009, vol. 338.
[40] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y . Bengio, “Generative adversarial networks,” in Advances in Neural Information Processing
Systems (NeurIPS) , 2014.
[41] L. Kong, S. Xie, W. Yao, Y . Zheng, G. Chen, P. Stojanov, V . Akinwande, and K. Zhang,
“Partial disentanglement for domain adaptation,” in Proc. International Conference on Machine
Learning (ICML) , 2022, pp. 11 455–11 472.
[42] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning
of visual representations,” in Proc. International Conference on Machine Learning (ICML) .
PMLR, 2020, pp. 1597–1607.
[43] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra et al. , “Matching networks for one shot
learning,” in Advances in Neural Information Processing Systems (NeurIPS) , vol. 29, 2016.
[44] E. Triantafillou, T. Zhu, V . Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin, C. Gelada,
K. Swersky, P.-A. Manzagol et al. , “Meta-dataset: A dataset of datasets for learning to learn
from few examples,” in Proc. International Conference on Learning Representations (ICLR) ,
2019.
[45] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,” Chemometrics and
intelligent laboratory systems , vol. 2, no. 1-3, pp. 37–52, 1987.
[46] J. Li and J. M. Bioucas-Dias, “Minimum volume simplex analysis: A fast algorithm to unmix
hyperspectral data,” in IEEE International Geoscience and Remote Sensing Symposium , vol. 3,
2008, pp. III–250.
[47] X. Fu, K. Huang, B. Yang, W.-K. Ma, and N. D. Sidiropoulos, “Robust volume minimization-
based matrix factorization for remote sensing and document clustering,” IEEE Transactions on
Signal Processing , vol. 64, no. 23, pp. 6254–6268, 2016.
[48] S. Arora, R. Ge, Y . Halpern, D. Mimno, A. Moitra, D. Sontag, Y . Wu, and M. Zhu, “A practical
algorithm for topic modeling with provable guarantees,” in Proc. International Conference on
Machine Learning (ICML) . PMLR, 2013, pp. 280–288.
[49] K. Huang, X. Fu, and N. D. Sidiropoulos, “Anchor-free correlated topic modeling: Identifiability
and algorithm,” Advances in Neural Information Processing Systems , vol. 29, 2016.
[50] X. Mao, P. Sarkar, and D. Chakrabarti, “Estimating mixed memberships with sharp eigenvector
deviations,” Journal of the American Statistical Association , vol. 116, no. 536, pp. 1928–1940,
2021.
[51] L. Gresele, P. K. Rubenstein, A. Mehrjou, F. Locatello, and B. Schölkopf, “The incomplete
Rosetta stone problem: Identifiability results for multi-view nonlinear ICA,” in Proc. Uncer-
tainty in Artificial Intelligence , 2020, pp. 217–227.
[52] C. Eastwood, J. von Kügelgen, L. Ericsson, D. Bouchacourt, P. Vincent, M. Ibrahim, and
B. Schölkopf, “Self-supervised disentanglement by leveraging structure in data augmentations,”
inCausal Representation Learning Workshop at NeurIPS 2023 , 2023.
[53] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-
consistent adversarial networks,” in Proc. the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2017, pp. 2223–2232.
[54] M. Long, H. Zhu, J. Wang, and M. I. Jordan, “Deep transfer learning with joint adaptation
networks,” in Proc. International Conference on Machine Learning (ICML) . PMLR, 2017, pp.
2208–2217.
[55] W. Wang, H. Zhang, Z. Yuan, and C. Wang, “Unsupervised real-world super-resolution: A
domain adaptation perspective,” in Proc. IEEE International Conference on Computer Vision
(ICCV) , 2021, pp. 4318–4327.
[56] T. Galanti, L. Wolf, and S. Benaim, “The role of minimal complexity functions in unsupervised
learning of semantic mappings,” in Proc. International Conference on Learning Representations
(ICLR) , 2018.
13[57] N. Moriakov, J. Adler, and J. Teuwen, “Kernel of CycleGAN as a principle homogeneous space,”
inProc. International Conference on Learning Representations (ICLR) , 2020.
[58] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, “Adapting visual category models to new domains,”
inProc. European Conference on Computer Vision (ECCV) . Springer, 2010, pp. 213–226.
[59] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan, “Deep hashing network for
unsupervised domain adaptation,” in Proc. IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2017, pp. 5018–5027.
[60] Y . Zhang, T. Liu, M. Long, and M. Jordan, “Bridging theory and algorithm for domain adapta-
tion,” in Proc. International Conference on Machine Learning (ICML) , 2019.
[61] Y . Jin, X. Wang, M. Long, and J. Wang, “Minimum class confusion for versatile domain
adaptation,” in Proc. IEEE International Conference on Computer Vision (ICCV) . Springer,
2020, pp. 464–480.
[62] H. Rangwani, S. K. Aithal, M. Mishra, A. Jain, and V . B. Radhakrishnan, “A closer look at
smoothness in domain adversarial training,” in Proc. International Conference on Machine
Learning (ICML) . PMLR, 2022, pp. 18 378–18 399.
[63] Y . Zhang, J. Liang, Z. Zhang, L. Wang, R. Jin, T. Tan et al. , “Free lunch for domain adversar-
ial training: Environment label smoothing,” in Proc. International Conference on Learning
Representations (ICLR) , 2023.
[64] L. Eyring, D. Klein, T. Uscidda, G. Palla, N. Kilbertus, Z. Akata, and F. Theis, “Unbalancedness
in neural monge maps improves unpaired domain translation,” arXiv preprint arXiv:2311.15100 ,
2023.
[65] M. Amodio and S. Krishnaswamy, “MAGAN: Aligning biological manifolds,” in Proc. Interna-
tional Conference on Machine Learning (ICML) . PMLR, 2018, pp. 215–223.
[66] J. Cao, D. A. Cusanovich, V . Ramani, D. Aghamirzaie, H. A. Pliner, A. J. Hill, R. M. Daza,
J. L. McFaline-Figueroa, J. S. Packer, L. Christiansen et al. , “Joint profiling of chromatin
accessibility and gene expression in thousands of single cells,” Science , vol. 361, no. 6409, pp.
1380–1385, 2018.
[67] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, and T. Mikolov, “Fasttext.zip: Com-
pressing text classification models,” arXiv preprint arXiv:1612.03651 , 2016.
[68] K. B. Athreya and S. N. Lahiri, Measure theory and probability theory . Springer, 2006, vol. 19.
[69] J.-F. Cardoso, “Super-symmetric decomposition of the fourth-order cumulant tensor. blind
identification of more sources than sensors.” in Proc. International Conference on Acoustics,
Speech, and Signal Processing (ICASSP) , vol. 91, 1991, pp. 3109–3112.
[70] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Proc. ICLR , 2015.
[71] A. L. Maas, A. Y . Hannun, and A. Y . Ng, “Rectifier nonlinearities improve neural network
acoustic models,” in Proc. International Conference on Machine Learning (ICML) , vol. 30,
no. 1, 2013, p. 3.
[72] K. Ghasedi Dizaji, A. Herandi, C. Deng, W. Cai, and H. Huang, “Deep clustering via joint
convolutional autoencoder embedding and relative entropy minimization,” in Proc. the IEEE
International Conference on Computer Vision (ICCV) , 2017, pp. 5736–5745.
[73] J. Xie, R. Girshick, and A. Farhadi, “Unsupervised deep embedding for clustering analysis,” in
Proc. International Conference on Machine Learning (ICML) . PMLR, 2016, pp. 478–487.
[74] W. Wang, X. Yan, H. Lee, and K. Livescu, “Deep variational canonical correlation analysis,”
arXiv preprint arXiv:1610.03454 , 2016.
[75] A. Gretton, K. Fukumizu, C. Teo, L. Song, B. Schölkopf, and A. Smola, “A kernel statistical test
of independence,” in Advances in Neural Information Processing Systems (NeurIPS) , vol. 20,
2007.
[76] T. M. Cover, Elements of information theory . John Wiley & Sons, 1999.
14Supplementary Material of “Identifiable Shared Component Analysis of
Unpaired Multimodal Mixtures”
A Notation
The notations used throughout the paper are summarized in the Table 4.:
Table 4: Definition of notations.
Notation Definition
x,x,X scalar, vector and matrix
x(q)variable from q-th domain
xi,xi both represents i-th element of vector x
Xij represents the element of i-th row and j-th column of matrix X
x⊤,X⊤transpose of xandX
|X| represents the cardinality of set X
Null(X) represents the null space of matrix X
conv(·) returns the convex hull of the given set
dim(X) denotes the dimension of subspace X
kA {ka|a∈ A, k∈R}
x+X {x+z|z∈ X}
X+Y {x+y|x∈ X,y∈ Y}
APreImg (X) preimage of X;{x|Ax∈ X}
[N] set of whole numbers up to N;{1. . . N}
I identity matrix
Px probability distribution of random variable x
Px,y joint probability distribution of random variable xandy
E[·] expectation
x(d)= = =y xandyrandom vectors have the same distribution
x(d)
̸=y xandyrandom vectors have different distributions
x⊥ ⊥y xandyrandom vectors are statistically independent
[a, b] represents continuous interval between aandb
N(µ, σ2) normal distribution with mean µand variance σ2
Uniform [a, b] uniform distribution with interval aandb
Gamma (α, θ) gamma distribution with the shape parameter αand scale parameter θ
Laplace (µ, b) Laplace distribution with location µand diversity or scale parameter b
VonMises (µ, κ)von Mises distribution with location µandκconcentration parameter.
Beta (α, β) beta distribution with the shape parameters αandβ
15B Proof of Theorem 1
We restate the theorem here:
Theorem 1 Under Assumption 1 and the generative model in (1), denote any solution of (6)as
bQ(q)q= 1,2. Then, if the mixing matrices A(q)are full column ranks and E[cc⊤])is full rank,
we have bQ(q)x(q)=Θ(q)c. In addition, assume that either of the following is satisfied:
(a)The individual elements of the content components are statistically independent and
non-Gaussian. In addition, ci(d)
̸=kcj,∀i̸=j,∀k∈Randci(d)
̸=−ci,∀i, i.e., the
marginal distributions of the content elements cannot be matched with each other by
mere scaling.
(b)The support Cis a hyper-rectangle, i.e., C= [−a1, a1]× ··· × [−adC, adC]. Further,
suppose that ci(d)
̸=kcj,∀i̸=j,∀k∈Randci(d)
̸=−ci,∀i.
Then, we have bQ(q)x(q)=Θc,i.e.,Θ(q)=Θfor all q= 1,2, where Θ(q).
We will prove the theorem in following two steps. For the first step we will prove bQ(q)x(q)=Θ(q)c
and for second step we will employ either assumption (a) or (b) to prove that Θ(q)=Θ,∀q= 1,2.
B.1 Linearly transformed content identification
Let us define
H(q)=Q(q)A(q)∈RdC×(dC+d(q)
P).
We want to show that
Null(H(q)) =0×Rd(q)
P, (10)
since this will imply that that H(q)does not depend upon the style component. Combined with
the fact that rank(H(q)) =dC, this will imply that H(q)is an invertible function of the content
component. To that end, consider the following line of arguments.
Since the objective in (6)matches the distribution for latent random variables bc(1)=Q(1)x(1)and
bc(2)=Q(2)x(2), the following holds for any Rc⊆RdC,∀k∈R,
Pbc(1)[kRc] =Pbc(2)[kRc],
(a)⇐⇒ Pz(1)[H(1)
PreImg (kRc)] =Pz(2)[H(2)
PreImg (kRc)] (11)
(b)⇐⇒ Pz(1)[kH(1)
PreImg (Rc)] =Pz(2)[kH(2)
PreImg (Rc)],
where, H(q)
PreImg (Rc) :={z(q)|H(q)z(q)∈ R c}is the pre-image of H(q).(a)follows because
Pbc(q)[kRc] =PH(q)z(q)[kRc] =Pz(q)[H(q)
PreImg (kRc)][68, Section 2.2]. (b)follows because H(q)
is a linear operator.
Although (11) holds for any Rc, we will see that it is sufficient to consider a special Rcto prove (10).
To that end, take Rc= conv {0,a1, . . . ,adC}, where ai∈RdCsuch that Pbc(q)[Rc]>0. Let us take
y(q)
i∈RdC+d(q)
P, such that H(q)y(q)
i=ai. For reasons that will be clear later, we hope to show that
H(q)
PreImg (Rc) = conv {0,y(q)
1, . . . ,y(q)
dC}+ Null( H(q)).
To that end, observe that for any r∈ Rc, we can represent ras,
r=1
dC+ 1dCX
i=1wiai,for some {wi}dC
i=1s.t.dCX
i=1wi≤1,∀i.
16For both view q= 1,2, we get,
r=1
dC+ 1dCX
i=1wiH(q)y(q)
i
=⇒r=H(q) 
1
dC+ 1dCX
i=1wiy(q)
i!
H(q)
PreImg 
1
dC+ 1dCX
i=1wiai!
=1
dC+ 1dCX
i=1wiy(q)
i+ Null( H(q)) (12)
We can write,
H(q)
PreImg (Rc) = conv {0,y(q)
1, . . . ,y(q)
dC}+ Null( H(q)) (13)
We have that Null(H(q))⊂RdC+d(q)
Pis a linear subspace with dim(Null( H(q))) =d(q)
P. LetA(q)=
H(q)
PreImg (Rc). Note that Pz(1)[kA(1)] =Pz(2)[kA(2)],∀k∈R(from (11), andPz(q)[A(q)]>0(by
the construction of Rc). Further, the set A(q)is of the form
conv{0,y(q)
1, . . . ,y(q)
dC}+P(q),
because Null(H(q))is a subspace of dimension d(q)
P, hence it satisfies the definition of P(q). Hence,
Assumption 1 implies that
Null(H(q)) =0×Rd(q)
P.
Denoting the Nth toMth columns of H(q)byH(q)(N:M), the above is equivalent to saying
H(q)(dC+ 1 : dC+d(q)
P) = 0 . (14)
Denote,
Θ(q)=H(q)(1 :dC)∀q= 1,2.
Then, we can write,
Q(q)x(q)=Θ(q)c,∀q= 1,2. (15)
Next, we use Assumption (a) or (b) to show that Θ(1)=Θ(2)=Θ. To that end, note that the
distribution matching constraint implies that
Θ(1)c(d)= = =Θ(2)c
=⇒c(d)= = = (Θ(1))−1Θ(2)c.
Hence M= (Θ(1))−1Θ(2)is an invertible matrix such that c(d)= = =Mc. However, in the following,
we will show that if either Assumption (a) or (b) is satisfied, then M=I, and thus Θ(1)=Θ(2).
B.2 Considering Assumption (a)
We want to show that when Assumption (a) is satisfied, if Mc(d)= = =cfor any invertible M, then
M=I.
Note that Mc= [m1. . .mdC]
c1
...
cdC
. By Assumption (a), we have that the components of content
are statistically independent ci⊥ ⊥cj, i̸=j, non-Gaussian, and has non-zero kurtosis. Then,
according to cumulant multilinearity and additivity properties, the fourth order cumulant tensor
Cum(Mc)ofMc has the following unique decomposition [69],
Cum(Mc) =dCX
i=1κimi◦mi◦mi◦mi (16)
17where◦is the outer product, κiis the kurtosis of component ci, andmi, i∈[dC]are the columns of
M.
SinceMc(d)= = =c, the following should hold
Cum(Mc) = Cum( c) = Cum( Ic) (17)
=⇒dCX
d=1κdmd◦md◦md◦md=dCX
d=1κded◦ed◦ed◦ed, (18)
eiis the ith column of identity matrix I.
Because of statistical independence of components of c, the CP-decomposition of Cum(Mc) =
Cum(Ic)is unique [69] upto permutation and scaling ambiguities, i.e., Mshould be a permutation
scaling matrix.
LetM=ΠΣ where, Π∈RdC×dCis a permutation matrix and Σ= Diag( r1, . . . r dC)∈RdC×dC
is a diagonal scaling matrix.
Finally, since ci̸(d)= = =kcj,∀i̸=j,∀k∈R,Mhas to to be identity matrix. To see the reason, for the
sake of contradiction, suppose that either (i) there exist i, j∈[dC]×[dC]andk∈R, with i̸=jsuch
that[Mc]i=kcj, or (ii) ∃i∈[dC]such that [Mc]i=kcifor some k∈R, k̸= 1.
For case (i), since Mc(d)= = =c,[Mc]i(d)= = =ci,∀i. Hence,
[Mc]i=kcj
=⇒[Mc]i(d)= = =kcj
=⇒ci(d)= = =kcj,
which is a contradiction to the assumption ci(d)
̸=kcj.
For case (ii), [Mc]i=kcjimplies that ci(d)= = =kcj. First, k̸=±1, cannot hold because it will mean
thatvar(ci) =k2var(ci)which cannot hold for k̸=±1since var(ci)>0. Hence, the only possible
option is k=−1, which is already ruled out by the assumption that ci(d)
̸=−ci. Hence Mis an
identity matrix. This concludes the proof.
B.3 Considering Assumption (b)
Let
ei= [0,0, . . . , 1
ith location,0,0]∈RdC
denote the standard basis vector in RdC. Let vertex of hyper-rectangle vi=aiei=Λei, where
Λ= Diag([ a1, . . . , a dC]T),
where Diag(·)represents the diagonal matrix formed by the given vector.
IfMc(d)= = =c, then the supports of Mc andcshould match, i.e.,
M(C) =C,
where M(C) ={Mc|c∈ C} .
Note that ∀c∈ C, the set of points visatisfy the following property
c=dCX
i=1αivi, for some −1≤αi≤1 (19)
=⇒Mc=dCX
i=1αi(Mv i).
18Since the support of Mc isC, this implies that ∀c∈ C,
c=dCX
i=1αi(Mv i), for some −1≤αi≤1
The last equation implies that the set of points Mv i,∀i∈[dC]also satisfy property (19). Hence,
for each i∈[dC],Mv i=±vjfor some unique j∈[dC]. Note that jshould be unique for each i
because Mis invertible, hence Mcannot map two orthogonal vectors viandvk,i̸=k, to the same
vector ±vjwith same or different signs.
LetV= [v1, . . . ,vdC]T. Then one can write
MV =VΣΠ,
where Σis some diagonal matrix with diagonal entries from {+1,−1}andΠis a permutation matrix.
Then the above implies
MΛI=ΛIΣΠ
=⇒M=ΛΣΠΛ−1.
Hence Mis a permutation and scaling matrix.
Finally, by the same argument presented in last paragraph of Sec. B.2 (i.e., proof with Assumption
(a)), we conclude that Mis an identity matrix.
C Proof of Theorem 2
We restate the theorem here:
Theorem 2 Consider the mixture model in (8). Assume that rank(A) = dC+dPand
rank(E[cc⊤]) =dC, and that Assumption 2 holds. Denote bQas any solution of (6)by constrain-
ingQ=Q(1)=Q(2). Then, we have bQx(q)=Θc.
One can follow the same argument as in the step 1 of proof in B.
Let us define
H=QA∈RdC×(dC+dP).
We want to show that
Null(H) =0×RdP, (20)
since this will imply that that Hdoes not depend upon the style component. Combined with the fact
thatrank(H) =dC, this will imply that His an invertible function of the content component. To
that end, consider the following line of arguments.
Since the objective in (6)matches the distribution for latent random variables bc(1)=Qx(1)and
bc(2)=Qx(2), the following holds for any Rc⊆RdC,,∃k∈R
Pbc(1)[kRc] =Pbc(2)[kRc],
(a)⇐⇒ Pz(1)[HPreImg (kRc)] =Pz(2)[HPreImg (kRc)] (21)
(b)⇐⇒ Pz(1)[kHPreImg (Rc)] =Pz(2)[kHPreImg (Rc)], (22)
where, HPreImg (Rc) :={z|Hz∈ Rc}is the pre-image of H.(a)follows because Pbc(q)[kRc] =
PHz(q)[kRc] =Pz(q)[HPreImg (kRc)][68, Section 2.2]. (b)follows because His a linear operation.
Although (21) holds for any Rc, we will see that it is sufficient to consider a special Rcto prove (20).
To that end, take Rc= conv {0,a1, . . . ,adC}, where ai∈RdCsuch that Pbc(q)[Rc]>0. Let us take
yi∈RdC+dP, such that Hyi=ai. For reasons that will be clear later, we hope to show that
HPreImg (Rc) = conv {0,y1, . . . ,ydC}+ Null( H).
19To that end, observe that for any r∈ Rc, we can represent ras,
r=1
dC+ 1dCX
i=1wiai,for some {wi}dC
i=1s.t.dCX
i=1wi≤1,∀i.
For both view q= 1,2, we get,
r=1
dC+ 1dCX
i=1wiHyi
=⇒r=H 
1
dC+ 1dCX
i=1wiyi!
HPreImg 
1
dC+ 1dCX
i=1wiai!
=1
dC+ 1dCX
i=1wiyi+ Null( H) (23)
We can write,
HPreImg (Rc) = conv {0,y1, . . . ,ydC}+ Null( H) (24)
We have that Null(H)⊂RdC+dPis a linear subspace with dim(Null( H)) = dP. Let A=
HPreImg (Rc). Note that Pz(1)[kA] =Pz(2)[kA],∀k∈R(from (21), and Pz(q)[A]>0(by the
construction of Rc). Further, the set Ais of the form
conv{0,y1, . . . ,ydC}+P,
because Null(H)is a subspace of dimension dP, hence it satisfies the definition of P. Hence,
Assumption 2 implies that
Null(H) =0×RdP.
Denoting the Nth toMth columns of HbyH(N:M), the above is equivalent to saying
H(dC+ 1 : dC+dP) = 0 . (25)
Denote,
Θ=H(1 :dC)∀v= 1,2.
Then, we can write,
Qx(q)=Θc,∀v= 1,2. (26)
This concludes the proof.
D Proof of Theorem 3
We restate the theorem here:
Theorem 3 Assume that Assumption 1 is satisfied, that |L| ≥ dCpaired samples (x(1)
ℓ,x(2)
ℓ)
are available, that A(q), q= 1,2have full column rank, and that Pcis absolutely continuous.
Denote (bQ(1),bQ(2))as any optimal solution of (6)under the constraint (9). Then, we have
bQ(q)x(q)=Θc.
From our objective in (6), we obtain
Q(1)x(1)(d)= = =Q(2)x(2). (27)
Using Assumption 1 and following the proof of step 1 in Theorem B, we can obtain:
Q(q)x(q)=Θ(q)c,∀q= 1,2,
20for some invertible matrices Θ(q),∀q. Hence,
Θ(1)c(d)= = =Θ(2)c (28)
=⇒c(d)= = = (Θ(1))−1Θ(2)c. (29)
Hence we can have linear transformation M:= (Θ(1))−1Θ(2)which has same probability density
asPc. However, the sample matching constraint (9), for ℓ−th sample implies that
Q(1)x(1)
ℓ=Q(2)x(2)
ℓ
=⇒Θ(1)cℓ=Θ(2)cℓ
=⇒cℓ= (Θ(1))−1Θ(2)cℓ
=⇒cℓ=Mcℓ.
LetC= [c1. . .cNp]. Then the above implies:
C=MC
=⇒(M−I)C=0.
Now we show that Cis a full row rank matrix, which implies that M−I=0=⇒M=I. To
that end, note that random variables x(1)
iandx(2)
ibeing i.i.d implies that c(i)are i.i.d from Pc. This
implies that for any 1≤i≤ |L| ,
Pr[ci∈span({cn1, . . . ,cndC−1})] = 0 . (30)
This is because span({cn1, . . . ,cndC−1})fornj∈[|L|], is a lower dimensional subspace in RdC,
which has zero probability under absolutely continuous distribution Pc. Hence any dCout of |L|
column vectors in Care linearly independent with probability 1.
This concludes the proof.
E Detailed Identifiability Conditions of Existing Results
E.1 Identifiability of CCA
Theorem 4 (Identifiability of Aligned SCA via CCA [1]) .Under (1), assume that every aligned
pair(x(1),x(2))share the same c, and that A(q)has full column rank. Also assume that there
exists an N-sample set {ℓ1, . . . , ℓ N}such that [C⊤,(P(1))⊤,(P(2))⊤]∈RN×(dC+d(1)
P+d(2)
P)has
full column rank, where C= [cℓ1, . . .cℓN]∈RdC×NandP(q)= [p(q)
ℓ1. . .p(q)
ℓN]∈Rd(q)
P×Nfor
q= 1,2. Denote (bQ(1),bQ(2))as an optimal solution of the CCA formulation. Then, we we
have
bQ(q)x(q)=Θc,
where Θis nonsingular.
In the above theorem, one can see that N≥(dC+d(1)
P+d(2)
P)is anecessary condition for the
identifiability of Θc. Hence, CCA needs at least dC+d(1)
P+d(2)
Ppaired samples for identifiability.
E.2 Identifiability of Unaligned SCA in [8]
We summarize the result in [8] in the following
Theorem 5 (Identifiability of Unaligned SCA via ICA [8]) .Under (1), assume that the following
are met: (i) The conditions for ICA identifiability [33] is met by each modality, including that the
components of z(q)= [c⊤,(p(q))⊤]⊤are mutually statistically independent and contain at most
one Gaussian variable. In addition, each z(q)
ihas unit variance; (ii) Pz(q)
i̸=Pz(q)
j,Pz(q)
i̸=
P−z(q)
j∀i, j∈[dC+d(q)
P], i̸=j. Then, assume that (im, jm)are obtained by ICA followed
by cross domain matching (see the part on Unaligned SCA in Section 2 ) for m= 1, . . . , d C.
21Denotebc(1)
m=e⊤
imbz(1)andbc(2)
m=e⊤
jmbz(2). We have the following:
bc(q)
m=kc(q)
π(m), m∈[dC], (31)
where k∈ {+1,−1}andπis a permutation of {1, . . . , d C}.
F Additional Synthetic Data Experiments
Hyperparameter Settings: We use Adam optimizer [70] to solve (7)and learn matrices Q(q), q=
1,2and the discriminator f. We set the initial learning rate of matrix and discriminator to be 0.009
and0.00008 respectively. We set the λ= 0.1in(7)to enforce (6c). For weak supervision experiment
in F, we set β= 0.01in(9). We generate total of 100,000 samples in each domain. For our
experiment we set the batch size to be 1,000 and run (7)for 50 epochs. Our discriminator is a 6-layer
multilayer perceptron (MLP) with hidden units { 1024, 521, 512, 256, 128, 64 } in each layer. All the
layers use leaky ReLU activation functions [71] with a slope of 0.2 except for the last layer which has
sigmoid activations. We include a label smoothing coefficient of 0.2 in the discriminator predictions
as suggested in [40].
Additional Details for Validation of Theorem 1 in Sec. 3: Here we explain the data gen-
eration details of the result shown in Fig. 3. For the result in top row, we sample c1from
a Gaussian mixture with three Gaussian components. Each component follows a normal dis-
tribution N(µ,2)where µ∼ N (0,10). The second component, i.e., c2, is independently
sampled from the gamma distribution Gamma (1,3). The private components are sampled from
p(1)∼Laplace (1.0,6.5)andp(2)∼Uniform [−10,10], both only having one dimension. In
the bottom row, we sample c∈R2∼VonMises (2.5,2.0)distribution. The private components
satisfy p(1)∼Laplace (1.0,6.5)andp(2)∼Gamma (0.5,3.0). Each element of mixing matrices
are sampled from A(q)
ij∼ N(0,1), q= 1,2. The readers are referred to Table 4 for the definition of
notations used for distributions.
Validation of Theorem 1 under different sample sizes and imbalanced data: Here we observe
the shared component identification performance of the proposed method numerically. We conducted
two experiments in different settings. First, we vary the sample sizes in both modalities, but the
two modalities have the same sample size. Second, we only vary the sample size of modality 2
while keeping the sample size of modality 1 fixed. This way, we create the data imbalance between
modalities. Note that the shared components are identified if the following two conditions are met:
1.Q(q)A(q)= [Θ,0], i.e.,bΘ(1)=bΘ(2)=Θand
2.∥Q(q)A(q)(dC:dC+d(q)
P)∥F=0.
Therefore, we use the above as our performance evaluation metrics. For the following experiments
(Table 5 and 6), we generate the data for the two modalities by sampling a two-dimensional content
c∼VonMises (2.5,2.0)and private components from p(1)∼Laplace (1.0,6.5)andp(2)∼
Gamma (0.5,3.0). The elements of the mixing matrices are sampled as A(q)
ij∼ N(0,1), q= 1,2. We
report the mean and standard deviation of ∥bΘ(1)(1 :dC)−bΘ(2)(1 :dC)∥Fand1/2P2
q=1∥bΘ(q)(dC:
dC+d(q)
P)∥Fobtained from 5 different runs.
Table 5 shows the performance of SCA and CCA under different sample sizes (i.e., N). One can
see that the proposed method (SCA) clearly identifies the shared components even when only 100
samples are available. The performance starts to deteriorate when N≤50, probably because the
min-max optimization problem is difficult to solve with very few samples. CCA does not really work
under this setting as it needs aligned cross-domain samples.
Table 6 shows the performance of SCA in the cases where two modalities have unbalanced data sizes.
The number of samples in the first modality is fixed to 100,000 while the second modality’s data size
varies from 10,000 to 10 samples. The data generation process remains the same as in the previous
experiment. One can see that even under obvious cross-domain data size imbalance (e.g., 100,000 to
1,000), the proposed method performs reasonably well in terms of shared component identification.
22Table 5: Shared component identification performance over different N.
N∥bΘ(1)(1 :dC)−bΘ(2)(1 :dC)∥F1/2P2
q=1∥bΘ(q)(dC:dC+d(q)
P)∥F
SCA CCA SCA CCA
100,000 0.015±0.020 1.623 ±0.273 0.031±0.009 0.232 ±0.033
10,000 0.021±0.006 1.667 ±0.240 0.030±0.002 0.267 ±0.031
1,000 0.018±0.011 1.572 ±0.474 0.042±0.059 0.280 ±0.070
100 0.053±0.014 2.224 ±0.525 0.083±0.096 0.364 ±0.153
50 0.132±0.118 1.469 ±0.299 0.142±0.132 1.470 ±1.520
20 1.373±0.626 2.084 ±0.661 0.490±0.321 0.546 ±0.269
Table 6: Shared component identification performance under imbalanced multi-modal data sizes.
# samples in modality 2 ∥bΘ(1)(1 :dC)−bΘ(2)(1 :dC)∥F1
2P2
q=1∥bΘ(q)(dC:dC+d(q)
P)∥F
10,000 0.020±0.018 0.018±0.005
1,000 0.065±0.029 0.026±0.015
100 0.145±0.051 0.081±0.049
10 1.290±0.239 0.293±0.064
Validation of Theorem 3. Fig. 5 presents numerical validation for Theorem 3.
Data Generation: We set dC= 3 andd(q)
P= 1 forq= 1,2. We sample each component of
shared component ci∼Laplace (0.0,6.5)i= 1,2,3,p(1)∈R1∼Uniform [−10,10]and
p(2)∼Gamma (0.5,3.0). Although csatisfies component-wise independence assumption, it does not
satisfy the condition that ci(d)
̸=kcj,∀i̸=jbecause ci(d)= = =cj,∀i, j∈[3]. Therefore, Theorem 1
does not cover this case. Nonetheless, this case falls under the jurisdiction of Theorem 3.
Result: Fig. 5 corroborates with our Theorem 3. That is, one needs at least |L| ≥ dC= 3pairs of
“anchors” (i.e., aligned cross domain pairs) to ensure identifiability of bc(q)=Θcforq= 1,2.
−100−50 0 50 100
c−100−50050100Ground truth
−100−50 0 50 100−50050x(1)
−50 0 50−50050x(2)
−100−50 0 50 100
/hatwidec(1)−100−50050100No anchors
−100−50 0 50 100
/hatwidec(2)−100−50050100
−100−50 0 50 100
/hatwidec(1)−100−500501001 anchors
−100−50 0 50 100
/hatwidec(2)−100−50050100
−100−50 0 50 100
/hatwidec(1)−100−500501003 anchors
−100−50 0 50 100
/hatwidec(2)−100−50050100
Figure 5: Validation of Theorem 3 dC= 3andd(1)
P= 1.
G Real Data Experiment Settings and Additional Results
G.1 Domain Adaptation
Hyperparameter Settings: The domain adaptation task follows the hyperparameter settings de-
scribed in Table. 7.
23Table 7: Hyperparameter settings for domain adaptation.
Parameter Value
Optimizer Adam
Learning rate of Q 0.0002
Learning rate of f 0.00002
Learning rate of classifier 0.02
Learning rate decay of classifier 0.75
λ(see Eq. (7)) 1.0
γ(see Eq. (32)) 0.1
Batch size 64
Number of epochs 20
Discriminator; farchitecture 6 layers, hidden units {1024, 521, 512, 256, 128, 64}
Activation functions of f Leaky ReLU (slope 0.2), Sigmoid (final layer)
Label smoothing coefficient in f 0.2
Table 8: Classification accuracy on the target domain of office-31 dataset using CLIP embeddings.
source →target CLIP DANN MDD MCC SDAT SDAT+MCC ELS ELS+MCC Proposed Proposed+MCC
A→W 93.4 93.7 94.1 95.9 95.0 98.1 96.8 98.7 95.3 98.3
D→W 99.1 100.0 99.3 100.0 100.0 100.0 100.0 100.0 99.7 100.0
W→D 100.0 99.5 99.5 98.4 99.5 99.5 99.5 100.0 100.0 99.9
A→D 91.9 92.1 94.2 97.7 95.7 97.7 95.0 97.7 93.7 99.1
D→A 81.4 81.9 79.2 85.7 81.2 84.6 81.3 83.0 83.9 85.9
W→A 81.7 83.0 82.2 84.7 84.7 86.7 82.6 86.3 85.8 87.1
Average 91.2 91.6 91.4 93.7 92.6 94.4 92.5 94.2 93.0 95.0
Baselines and Training Setup : The baselines are representative DA methods, namely, DANN [25],
MDD[60], MCC[61], SDAT [62], and ELS[63]. We use the implementations of DANN ,MDD, and MCC
from the https://github.com/thuml/Transfer-Learning-Library , while SDAT andELSare
taken from https://github.com/yfzhang114/Environment-Label-Smoothing . In all the
baselines, the classifier is jointly optimized with the feature extractor Qwhich arguably regularizes
towards more classification-friendly geometry of the shared features; see [72, 73]. Following their
training strategy, we also append a cross-entropy (CE) based classifier training module to our loss in
(7)(which learns our feature extractor Q). The CE part uses Qx(1)and the labels of the sources as
inputs to learn the classifier, i.e.,
LCE=−γNX
ℓ=1KX
k=1I[yℓ=k] logrθ([Qx(1)
ℓ]k), (32)
where rθ(·) :RdC→RKis the classifier that aims to map the learned feature vector Qx(1)
ℓto a
K-dimensional probability mass function (i.e., the distribution of the ground-truth label over K
classes), yℓ∈[K]represents the label of the ℓth sample in source domain, and the indicator function
I[yℓ=k] = 1 only when the event yℓ=khappens (other wise I[yℓ=k] = 0 . The γ≥0is the
tunable parameter. The joint loss is still differentiable, and thus we still use the Adam optimizer to
jointly optimize Qandθ.
Additional domain adaptation experiment using CLIP features: In this experiment, we use CLIP
as an image encoder as it learns informative and transferable features from very large datasets [35].
Table 8 and Table 9 show the results on Office-31 andOffice-Home datasets, respectively, using CLIP
embeddings. Compared to the results on ResNet50 embeddings in Table 1 and Table 2, one can
observe that all the methods, including proposed method, gains an advantage. This is likely because
CLIP was trained on a large and diverse dataset [35], which may have include similar content to the
Office-31 andOffice-Home datasets.
The results show that, as a foundation model, CLIP can already unify the embeddings of the source
and target domains to a reasonable extent. In addition, our model and algorithm when combined
with regularization techniques like MCC, can still further enhance performance, even with simple
post-processing of CLIP embeddings.
24Table 9: Classification accuracy on the target domain of office-Home dataset using CLIP embeddings.
source →target CLIP DANN MDD MCC SDAT SDAT+MCC ELS ELS+MCC Proposed Proposed+MCC
Ar→Cl 78.0 80.4 80.2 80.9 79.6 80.7 80.0 81.3 82.0 83.2
Ar→Pr 88.7 91.7 88.9 93.3 89.4 94.3 91.2 93.9 91.4 95.2
Ar→Rw 90.6 90.2 91.0 92.8 90.1 92.1 89.4 92.1 91.9 93.8
Cl→Ar 85.2 83.2 85.1 87.4 83.1 86.1 84.4 87.2 85.4 87.7
Cl→Pr 89.0 89.7 90.1 93.4 90.2 93.5 89.7 93.5 91.1 94.9
Cl→Rw 89.8 88.1 89.4 89.3 87.9 90.5 88.3 90.6 90.4 92.0
Pr→Ar 78.2 80.4 81.8 83.7 81.0 85.0 81.8 86.1 83.0 86.6
Pr→Cl 72.7 75.8 75.8 78.4 75.4 78.5 75.7 78.3 77.7 81.3
Pr→Rw 89.0 90.4 90.3 92.6 90.8 92.1 90.0 92.3 91.0 93.6
Rw→Ar 86.6 84.9 85.9 85.3 85.3 86.3 85.4 87.0 87.7 88.2
Rw→Cl 78.1 79.4 79.8 79.0 78.6 79.8 79.1 79.8 81.3 81.8
Rw→Pr 94.3 94.6 93.9 95.9 94.8 95.4 94.0 95.2 94.7 96.0
Average 85.0 85.7 86.0 87.6 85.5 87.8 85.7 88.1 87.3 89.5
Visualization Result: Fig. 6 shows the 2-dimensional visualization of the CLIP-learned features
(d= 256 ) from two domains, namely, DSLR and Amazon images ( Office-31 ), using t-SNE. One can
see that CLIP could roughly group the same classes from the two domains together. But the proposed
method can further pull the circles and the triangle markers together—meaning that the Qreally
learns shared representations of the same data in the DSLR and Amazon domains.
40
 30
 20
 10
 0 10 20 30 4050
40
30
20
10
0102030Features from CLIP
(a) Clip features d(q)= 768 .
40
 20
 0 20 4040
30
20
10
010203040Low dimensional features from proposed method (b) Features from proposed method d(q)
C= 256 .
Figure 6: Office-31 dataset: DSLR images features represented as circle markers, Amazon images
features represented as triangle markers. Different color represent different classes.
G.2 Single-cell Sequence Analysis
Hyperparameter Settings: The hyperparameter settings for single-cell sequence analysis is pre-
sented in Table. 10.
Baseline: For more details on baseline refer to the implementation in https://github.com/
uhlerlab/cross-modal-autoencoders .
G.3 Multi-lingual Information Retrieval
Hyperparameter Settings: The hyperparameter settings for multi-lingual information retrieval is
described in the Table. 11.
25Table 10: Hyperparameter settings for single-cell sequence analysis.
Parameter Value
Optimizer Adam
Learning rate of Q(q)0.001
Learning rate of f 0.0001
λ(see Eq. (7)) 1.0
β(see Eq. (9)) 10.0
Batch size 32
Number of epochs 75
Discriminator; farchitecture 6 layers, hidden units {1024, 521, 512, 256, 128, 64}
Activation functions of f Leaky ReLU (slope 0.2), Sigmoid (final layer)
Label smoothing coefficient in f 0.2
Table 11: Hyperparameter settings for multi-lingual information retrieval.
Parameter Value
Optimizer Adam
Learning rate of Q 0.0001
Learning rate of f 0.00001
λ(see Eq. (7)) 1.0
Batch size 32
Number of epochs 5
Discriminator; f(similar as in [21]) 2 layers, 2048 hidden units each
Activation functions of f Leaky ReLU (slope 0.2), Sigmoid (final layer)
Dropout rate (Input) in f 0.1
Label smoothing coefficient in f 0.2
Additional Results: Table 12 reports the P@5 and P@10 scores over the test data, calculated for
different source and target language pairs. It can be observed that the proposed method achieves
higher precision than Advin most of the translation tasks (e.g., by at least 1%in the en→esand
es→entasks) when considering both P@5 and P@10 scores.
G.4 Computation resources
All the experiments were run on Nvidia H100 GPU. The approximate runtime for a single run of the
algorithm is 20 minutes for multi-lingual information retrieval, 15 minutes for domain adaptation,
and 3 minutes for single-cell sequence analysis.
Complexity Analysis:
Since the proposed objective is tackled using stochastic gradient (SG)-based first-order iterative
method, the computational complexity of the proposed algorithm depends upon the per-iteration
complexity.
For each sample, the per-iteration complexity is composed of a forward pass and a backward pass.
Note that the problem size depends upon d(q)(the data dimension), dC, and the batch size B. We
assume that the network architecture of f(the number of layers and hidden units in each layer) is
fixed, represented by f=σ◦FL◦ ··· ◦ σ◦F1, where Fℓandσare the linear layer (matrix) and
activation function corresponding to the ℓth layer. Only the input dimension dCof first matrix F1,
varies with the problem size.
The forward pass involves computing bc(q)=Q(q)x(q)andf(q)(bc(q)), both of which scale linearly
withdC, d(q)and the batch size B. Hence, the forward pass time complexity is O(BdC(d(1)+d(2))).
Similarly, the backward pass requires computing of∂L
∂bc(q)
i,∀i∈[dC]and∂bc(q)
i
∂Q(q)jk,∀i∈[dC], j, k∈
[dC×d(q)], where Lis the loss function. The first gradient computation is linear in BdC, while the
second gradient computation has a complexity of O(BdC(d(1)+d(2))). Hence the computational
complexity of our method is O(BdC(d(1)+d(2))).
26Table 12: Average precision P@k of cross-language information retrieval
P@k source →target Adv- NN proposed - NN Adv- CSLS proposed - CSLS
P@5en→es 77.9 78.8 83.6 85.2
es→en 73.2 79.0 83.2 86.0
en→it 68.0 70.8 76.8 82.4
it→en 79.0 77.6 71.2 66.9
en→fr 79.2 75.2 85.7 85.2
fr→en 69.8 73.5 77.2 83.5
P@10en→es 82.4 82.6 87.0 87.8
es→en 79.0 82.3 87.2 88.8
en→it 74.1 75.6 81.7 85.6
it→en 71.5 75.8 82.0 82.9
en→fr 83.4 79.1 88.0 88.4
fr→en 73.8 77.4 80.6 86.6
The memory complexity involves storing the network parameters and the aforementioned gradients.
Hence, only the size of Q(q),F1, andc(q)changes with the problem dimension. The size of
Q(q),F1, andc(q)aredCd(q),O(dC)anddC, respectively. Therefore, the space complexity is
O(BdC(d(1)+d(2))).
In summary, both the memory and computational complexities of the proposed method scales linearly
withdC.
H Extension: Private Component Identification
Theorems 1-3 are concerned with learning the shared component c. The goal, there, was to ensure
thatQ(q)
Cx(q)Θc,∀q. In some cases, the private components p(q)is also of interest [6, 31, 74]. To
learnp(q), we propose to solve the following learning criterion:
find Q(q)
C∈RdC×d(q),Q(q)
P∈Rd(q)
P×d(q)q= 1,2, (33a)
subject to Q(1)
Cx(1)(d)= = =Q(2)
Cx(2), (33b)
Q(q)
Cx(q)⊥ ⊥Q(q)
Px(q)q= 1,2, (33c)
Q(q)
CEh
x(q)(x(q))⊤i
(Q(q)
C)⊤=Iq= 1,2, (33d)
Q(q)
PEh
x(q)(x(q))⊤i
(Q(q)
P)⊤=Iq= 1,2, (33e)
where u⊥ ⊥vmeans that the random vectors uandvare independent with each other.
For implementation we use following criterion,
min
QC(1),QC(2)QP(1),QP(1)max
fEx(1)log
f(QC(1)x(1))
+Ex(2)log
1−f(QC(2)x(2))
+λ2X
q=1R
QC((q))
+ω2X
q=1R
QP((q))
+ρ2X
q=1HSIC( Q(q)
Cx(q),Q(q)
Px(q)), (34)
where, first two term are adversarial loss for distribution matching. The constraint on
(33d) and (33e) are enforced as R(QC(q))andR(QC(q))respectively, where R(Q(q)) =
∥Q(q)E[x(q)(x(q))⊤](Q(q))⊤−I∥2
F. The constraint on (33c) is realized with Hilbert-Schmidt
Independence Criterion (HSIC) [75]. HSIC measures the independence between two distribution.
So, we minimize HSIC between estimated shared component and estimated private component to
promote independence between shared and private components.
We show that under some reasonable conditions the block p(q)can also be learned up to a matrix
multiplication:
27Theorem 6. Assume that the blocks c,p(1)andp(2)are statistically independent, i.e.,
p(c,p(1),p(2)) =p(c)p(p(1))p(p(2)). Then, if one of the following holds:
(i)Assumption 1 and assumptions in Theorem 1 are satisfied, and (33) is solved yielding
solutions bQ(q)
CandbQ(q)
P
(ii)Assumption 2 is satisfied and has same mixing matrix A(q)=Aand (33) with
Q(q)
P=QPandQ(q)
C=QCis solved yielding bQ(q)
CandbQ(q)
Pas the solutions.
(iii) Assumption 1 is satisfied and dCpaired samples (x(1)
ℓ,x(2)
ℓ)are available (weak
supervision), and denote bQ(q)
CandbQ(q)
Pas the solutions after solving (33).
Then, we have bQ(q)
Cx(q)=ΘcandbQ(q)
Px(q)=Ξ(q)p(q),for some invertible Ξ(q)for all
q= 1,2.
Proof. For each case in Theorem. 6 (i) - (iii), we can prove
bc(q)=bQ(q)
Cx(q)=Θc, q= 1,2 (35)
using Theorems 1-3. The proofs are referred to Appendix B-D.
Let us denote
bp(q)=bQ(q)
Px(q)=bQ(q)
PA(q)c
p(q)
=H(q)c
p(q)
,
(36)
where H(q)=bQ(q)
PA(q)∈Rd(q)
P×(dC+d(q)
P). Note that the constraint (33c) implies that the mutual
information between bp(q)andbc(q)is zero, i.e.,
I(bp(q);bc(q)) = 0 .
Note that bp(q)→bc(q)→Θ−1bc(q)=cis a Markov chain. This is because when conditioned on bc(q),
Θ−1bc(q)becomes constant, making it independent of bp(q). This allows us to use the data processing
inequality [76, Theorem 2.8.1], which results in the following:
I(bp(q);bc(q))≥I(bp(q);Θ−1bc(q)) =I(bp(q);c)).
Since mutual information is always non-negative, the above implies that I(bp(q);c) = 0 . This implies
thatbp(q)=H(q)c
p(q)
is independent of c. Hence, H(q)[1 :dC] = 0,∀q.
Therefore bp(q)=H(q)[dC+ 1 : dC+d(q)
P]p(q)=Ξ(q)p(q),∀q, where Ξ(q)=H(q)[dC+ 1 :
dC+d(q)
P]. Note that H(q)is full row-rank because of constraint (33e) . This implies that Ξ(q), q= 1,2
are invertible matrices.
This concludes the proof.
H.1 Validation of Theorem 6
Fig. 7 presents numerical validation for Theorem 6.
Hyperparameter Setting The hyperparameter setting is the same as mentioned in Appendix. F.
We solve (34) to obtain bQ(q)
CandbQ(q)
Pto recover the shared and private components, respectively.
For learning Q(q)
P, we use Adam optimizer and set initial learning rates to be 0.001. Also we set the
regularization parameter ω= 10.0andρ= 50.0.
28Data Generation: We set dC= 2 andd(q)
P= 1 as in the previous synthetic experiments. We
sample c∼VonMises (2.5,2.0)The private components are sampled from p(1)∼Beta (1.0,3.0)
andp(2)∼Gamma (0.5,3.0)distributions. Each element of mixing matrices are sampled from
A(q)
ij∼ N(0,1), q= 1,2.
Result: Fig. 7 shows the result for proposed method for private component identification. The first
column shows the data domain, the second column shows the true and extracted shared component,
and the third and fourth columns shows the true and extracted private components. Especially, the
last row of the third and fourth columns shows the plot of ground truth p(q)onx−axis and bp(q)
on the y-axis. The plot is approximately a straight line which indicates that the estimated private
components bp(q)are scaled version (i.e., invertible linear transformations) of ground truth private
components. This verifies our Theorem 6.
−3−2−10 1 2 3−3−2−10123c
0.0 0.2 0.4 0.6 0.8050100150200250300p(1)
01020304050607001000200030004000p(2)
−40−20020 40 60 80−75−50−250255075x(1)
−4−3−2−10 1 2−1012345/hatwidec(1)
−0.50.00.51.01.52.0050100150200250/hatwidep(1)
0 10 20 30 4001000200030004000/hatwidep(2)
−100−50 0 50 100−80−60−40−20020406080x(2)
−4−3−2−10 1 2−1012345/hatwidec(2)
0.0 0.2 0.4 0.6 0.8
−p(1)→−0.50.00.51.01.52.0−/hatwidep(1)→
010203040506070
−p(2)→010203040−/hatwidep(2)→
Figure 7: Validation of Theorem 6 dC= 2andd(1)
P= 1.
29NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: See Section 3, 4 and 6.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See Section 7.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
30Justification: See Appendix B, C, D and H.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: See Appendix F and G.
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
31Justification: Yes the code is provided in the supplemental material.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: See Appendix F and G.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We have provided the error bars for the experiment that is computationally
less demanding.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
32•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: See Appendix G.4.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have followed the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The contribution of this paper is on theoretical aspects of machine learning.
We don’t foresee any immediate societal impacts.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
33•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Theoretical paper. So not applicable in our case.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: See Appendix G.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
34•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We donot release any new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Not applicable.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Not Applicable.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
35