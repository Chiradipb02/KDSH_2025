Stochastic Optimal Control and Estimation with
Multiplicative and Internal Noise
Francesco Damiani
Center for Brain and Cognition,
Department of Engineering
Pompeu Fabra University
Barcelona, ES
francesco.damiani@upf.eduAkiyuki Anzai
Department of Brain and Cognitive Sciences
University of Rochester
Rochester, USA
aanzai@ur.rochester.edu
Jan Drugowitsch
Department of Neurobiology
Harvard Medical School
Boston, USA
jan_drugowitsch@hms.harvard.eduGregory C. DeAngelis
Department of Brain and Cognitive Sciences
University of Rochester
Rochester, USA
gdeangelis@ur.rochester.edu
Rubén Moreno-Bote
Center for Brain and Cognition, Department of Engineering,
Serra Húnter Fellow Programme
Pompeu Fabra University
Barcelona, ES
ruben.moreno@upf.edu
Abstract
A pivotal brain computation relies on the ability to sustain perception-action
loops. Stochastic optimal control theory offers a mathematical framework to
explain these processes at the algorithmic level through optimality principles.
However, incorporating a realistic noise model of the sensorimotor system —
accounting for multiplicative noise in feedback and motor output, as well as internal
noise in estimation — makes the problem challenging. Currently, the algorithm
that is commonly used is the one proposed in the seminal study in [ 1]. After
discovering some pitfalls in the original derivation, i.e., unbiased estimation does
not hold, we improve the algorithm by proposing an efficient gradient descent-based
optimization that minimizes the cost-to-go while only imposing linearity of the
control law. The optimal solution is obtained by iteratively propagating in closed
form the sufficient statistics to compute the expected cost and then minimizing this
cost with respect to the filter and control gains. We demonstrate that this approach
results in a significantly lower overall cost than current state-of-the-art solutions,
particularly in the presence of internal noise, though the improvement is present
in other circumstances as well, with theoretical explanations for this enhanced
performance. Providing the optimal control law is key for inverse control inference,
especially in explaining behavioral data under rationality assumptions.
1 Introduction
The sensorimotor system possesses a remarkable ability to reliably execute actions aligned with
external and internal goals in spite of the noise sources affecting it [ 2,3] and the numerous solutions
38th Conference on Neural Information Processing Systems (NeurIPS 2024).through which the same goals can be achieved [ 4]. Our nervous system is able to combine the
complexity of the mechanical properties of the body with a regulatory control system [ 5]. How such
control is implemented at a computational and algorithmic level is still an open question in systems
neuroscience.
Optimal feedback control provides a valuable framework for understanding how the motor system
creates coordinated and adaptable behavior [ 6,5]. However, using optimality principles to infer the
underlying computation [ 7] is a powerful yet potentially risky approach. Indeed, multiple independent
factors can lead to discrepancies with experimental data. For instance, predictions might be based
on unsuitable approximations or conditions. Consequently, although optimal feedback control, and
more broadly, stochastic optimal control theory, can be regarded as ideal candidates to understand the
principles of motor control [ 5,6,1,7], their effectiveness depends on the mathematical correctness
of the derived predictions and the accuracy of their assumptions.
Solving an optimal control problem implies deriving an optimal state-to-action policy, or control-law,
to minimize a certain cost function, usually embedding control effort and task related goals [ 6].
The classical framework in which analytical solutions for optimal feedback control of stochastic,
partially observable, continuous, non stationary, and high-dimensional systems can be derived, is the
Linear-Quadratic-Additive-Gaussian (LQAG) problem (see Section 2), that assumes linear dynamics,
a quadratic cost function and additive Gaussian noise [ 8,9]. Despite having been used in the past
to model motor control [ 10,11,12], these assumptions are too limiting to explain a wide range of
observed, relevant behaviors, like smooth velocity profiles [ 1,13,14], speed-accuracy trade-offs
[1,15,16,17] and movement corrections [ 18,3,19]. Including a realistic noise model for the
sensorimotor system is crucial to fill this gap, even at the cost of decreasing the mathematical
tractability of the problem [ 1]. Indeed, accounting for control [ 1,15,6,17,20] and signal-dependent
[1,21,22,23,14] noise at the motor output and sensory feedback level, and for internal noise
[1,2,24] in the estimation process, permits explaining a broad range of experimentally observed
phenomena [18, 3, 1, 25, 26], as discussed in Section 2.
The seminal study in [ 1], widely regarded as state-of-the-art for solving optimal control problems
under this extended noise model, offers an iterative algorithm whereby a stochastic optimal control
problem, incorporating multiplicative motor and sensory noise and additive internal noise, can be
efficiently solved. Such an algorithm is currently used to explain behavioral data in the context
of inverse optimal control [ 27]. Unfortunately, the derivation used in [ 1] erroneously assumes
unbiased estimators. We propose an alternative algorithm that addresses this issue by assuming
only linear control. The algorithm leverages the fact that the cost function can be computed from
closed-form moment expressions, which can then be minimized numerically. To handle potential
high computational costs, we derive an analytical counterpart for the optimization, based on the
efficient propagation of cost function derivatives over time. For simplicity, the algorithm is derived
for a simpler, yet relevant, case as outlined in Section 3.3, with extensions to the more general
case also discussed. Our algorithm outperforms the solutions in [ 1] under internal noise, providing
both theoretical and heuristic explanations for the performance differences. In a sensorimotor hand-
reaching task, it reduces the cost by up to 90% when internal noise constitutes 10% of the total. This
reveals qualitatively different behaviors, underscoring the importance of using the actual optimal
controller, particularly when explaining behavior in a principled way [27].
In Section 2, to fix notation and ideas, we begin by formalizing the optimal control problem using
the classic LQAG framework [ 8], addressing partial observability and assuming fully additive noise
(Section 2.1). We then introduce the Linear-Quadratic-Multiplicative-Gaussian (LQMG) framework,
which extends the noise model to include multiplicative noise in both control and observations, as
well as additive internal noise, following the approach of [ 1] (Section 2.2). We demonstrate that the
well-established solution from [ 1] produces suboptimal solutions in the presence of internal noise
and prior to full algorithmic convergence (Section 2.3). In Section 3, we introduce a novel numerical
algorithm that achieves optimal solutions and outperforms the approach in [ 1], as demonstrated
empirically in Section 3.2. Finally, in Section 3.3, we present the analytical counterpart to the
numerical algorithm.
22 Control and Estimation with Multiplicative and Internal Noise
2.1 The Classic Linear-Quadratic-Additive-Gaussian (LQAG) Problem
Stochastic optimal control theory formalizes the idea of controlling a dynamical system under partial
observability to accomplish a goal [ 8]. In the LQAG problem (typically referred to as LQG), a linear
system with latent state xt∈Rm
xt+1=Axt+But+ξt (1)
is controlled by a control signal ut∈Rp, with time-independent matrices A∈Rm×mandB∈Rm×p,
and initial condition x1– considering time-dependent matrices is straightforward. The term ξt∈Rm
stands for a Gaussian random variable with zero mean and covariance Ωξ(we always consider i.i.d.
random variables, but note that temporally correlated random variables can be generated by filtering
the noise with the linear dynamics in Eq. 1). In the most relevant case, the controller does not have
full access to the latent state xt: the observation yt∈Rkis a noisy version of xt,
yt=Hxt+ωt, (2)
with observation matrix H∈Rk×mandωt∈Rkbeing a Gaussian random variable with zero mean
and covariance Ωω. Note that all noise sources are additive , that is, state independent, hence we refer
to the classic LQG problem as LQAG. The controller ut≡ut(y˜t)is constrained to be a function
of the past observations only, y˜t= (y1, ..., y t−1), and it must be optimized to minimize the total
quadratic cost
E[J] =TX
t=1E[jt] =TX
t=1E[x⊺
tQtxt+u⊺
tRtut], (3)
where Tis the duration of the task, and jtis the cost per step in a trial, which includes a control
cost (reflecting the internal goal of minimizing control effort) determined by the symmetric positive
definite matrix Rt∈Rp×p, with Rt>0, and a state cost (modeling potential external goals, such as
minimizing the distance to a chosen target), determined by Qt∈Rm×m. Again, Qtis symmetric
and positive definite, Qt>0, and modulates the cost of the state being far from a chosen target.
Jis the total cost, over a whole trial, while E[J]is the total expected cost. Here, the expectation
E[f(·)]denotes an average over all noise random variables with the same initial condition, that is,
E[f(·)] =R
dx2,...,Tdy1,...,Tf(·)p(x2,...,T, y1,...,T).
The optimal controller can be derived analytically [ 8] (see Appendix A.1). In summary, it is a linear
function of the state estimate ˆxt,ut=Ltˆxt, where Lt∈Rp×mis the control gain, and ˆxtis the
estimator of the unobserved variable xt, recursively computed with a linear Kalman filter
ˆxt+1=Aˆxt+But+Kt(yt−Hˆxt) (4)
with filter gains Kt∈Rm×kand initial condition ˆx1=E[x1](x1is a random Gaussian variable with
covariance Σx1) – to start with an unbiased estimate of the latent variable. Intuitively, the estimate at
timet+ 1consists of a next-state prediction term (the first two terms in the r.h.s.) from the current
estimate ˆxtplus a correction (third term) that depends on the prediction error, the difference between
the new observation ytand the previous state prediction, weighted by its reliability. For example, if
the noise magnitude is very large, Kt= 0, indicating that an open-loop strategy would be optimal
[8].
For the classic LQAG problem, it is well known that the optimal Kalman filter satisfies the
orthogonality principle [8], stating that the estimation error is orthogonal to the optimal esti-
mator ˆxt, i.e. E[(xt−ˆxt)ˆx⊺
t] = 0 , where we define from here onwards the expectation as
E[f(·)] =R
dx2,...,T dˆx2,...,Tf(·)p(x2,...,T,ˆx2,...,T), where pis the joint density of latent and
estimation variables with initial condition ˆx1=E[x1]– e.g., E[ˆxt] =R
dˆxtˆxtp(ˆxt).
Also, as it is clear from the analytical expression (Appendix A.1), the computation for the optimal
controller and filter gains are mathematically independent of each other, the so-called separation
principle [28, 29, 30], which is closely related to the concept of certainty equivalence [31].
2.2 An Extended Noise Model: Optimal Control Beyond the LQAG Framework
Purely additive noise sources alone are insufficient to model the sensorimotor action-perception loop,
as multiplicative noise affects both motor control [ 1,15,17,16] and sensory feedback, including
3visual and proprioceptive signals [ 6,1,23,21,22,14]. For instance, stronger muscle forces produce
greater noise [ 15,17], and visual sensory noise increases in the periphery relative to the fovea
[1,23,21,22]. Accounting for these characteristics is crucial for explaining and reproducing
various behavioral features in reaching movements, such as stereotyped bell-shaped velocity profiles
[1, 13, 14] and the speed-accuracy trade-off [1, 15, 16, 17].
These considerations result in the Linear-Quadratic-Multiplicative-Gaussian (LQMG) model, with
the following dynamics for state and sensory feedback [1]
xt+1=Axt+But+ξt+cX
i=1εi
tCiut (5)
yt=Hxt+ωt+dX
i=1ρi
tDixt. (6)
In comparison to Eqs. 1-2 for the classic LQAG model, the LQMG model adds the final terms to
account for multiplicative motor noise (Eq. 5) and sensory noise (Eq. 6). Specifically, performing a
control action utintroduces noise proportional to the control signal itself (Eq. 5), while perceiving
the state variable xtinduces noise proportional to the observed state (Eq. 6). Here, Ci∈Rm×p
andDi∈Rk×mare constant scaling matrices, and εt∈Rcandρt∈Rdare zero-mean Gaussian
noise terms with covariances Ωε=IandΩρ=I, respectively [ 1]. For simplicity, in the expressions
derived below, we set c=d= 1to improve readability, without loss of generality.
As in the LQAG problem, the objective is to find the optimal control signal ut∈Rpthat depends
solely on past observations y1,···,t−1to minimize the cost function defined in Eq. 3. In this case, the
optimal state estimate and corresponding filters are state-dependent and in general intractable, but we
can simplify the problem by assuming, as in [ 1], that the filter is non-adaptive (i.e. independent of the
state estimate, [ 1]), similar to the classic LQAG problem. This leads to the assumption that the state
estimate follows the equation
ˆxt+1=Aˆxt+But+Kt(yt−Hˆxt) +ηt, (7)
with the same terminology as in Eq. 4. The initial state x1and its estimate ˆx1, assumed to be
independent, are Gaussian variables with the same mean E[x1] =E[ˆx1], and covariances Σx1and
Σˆx1, respectively. This dynamics of the state estimate only differs from Eq. 7 due to the presence
of an additional zero-mean Gaussian noise term ηt∈Rmwith covariance Ωη, which models the
possibility of inefficient filtering of the past observations. This noise term may represent internal
fluctuations in neural activity [ 2,24,32,3] or inaccuracies in the filtering process, and is important
for explaining behavioral data [1].
Under the new LQMG model (Eqs. 5-7), the task is to find the optimal control signal ut=ut(ˆxt),
fort= 1, ..., T −1, and the filter gains K1,···,T−2, that minimize the quadratic cost in Eq. 3.
2.3 State-of-the-Art Solutions for the LQMG Model: Causes of Suboptimality
It is important to first recognize that solving the LQMG problem in Eqs. 5-7 is significantly more
complex than in the classic LQAG problem: while in the latter the separation principle applies,
allowing for a direct analytical solution (Appendix A.1), in the former the principle does not hold,
resulting in tightly intertwined controller and filter gains. Notably, the presence of internal noise
alone introduces control-estimation interdependencies— a factor previously overlooked in earlier
approaches.
The algorithm currently used to solve the optimal control problem under the LQMG model (Eqs.
5-7) is the one introduced in the seminal work of [1], whose solutions are detailed in Appendix A.2.
The impact of this research extends beyond theoretical considerations [ 27,33,34,35,36,37,38].
We now describe some pitfalls in the original derivation and explain why certain assumptions fail,
leading to suboptimality. In Section 3, we propose an alternative algorithm, and in Section 3.2, we
demonstrate that our solutions outperform the previous ones.
The algorithm in [ 1] assumes, throughout the derivation of the optimal control-estimation loop,
that the estimator is unbiased, meaning E[xt|ˆxt] = ˆxt. However, this condition is never truly
satisfied. To illustrate this conceptually, we can consider a one-dimensional toy problem involving
a partially observable stochastic process xt(Fig. 1a). Assume that at time t−1, the condition
4E[xt−1|ˆxt−1] = ˆxt−1holds. Now, suppose that at the same time, a large positive fluctuation, possibly
caused by sensory or internal noise, affects the agent’s internal estimate. As a result, while the actual
statextchanges only slightly compared to xt−1, the state estimate ˆxtchanges significantly, so that
ˆxt≫ˆxt−1. At this point, it becomes clear that the expected value of xtconditioned on ˆxtcannot
equal ˆxt, thus violating the unbiasedness condition (see Fig. 1a). This effect is more pronounced
when the state estimate undergoes large fluctuations, but a similar bias, although smaller, would still
be present with minor fluctuations.
We demonstrate this issue numerically by considering a one-dimensional problem ( m=p=k= 1)
with multiplicative, additive, and internal noise (for the details see Appendix A.3). In the absence
of internal noise, the violation of unbiasedness is still present, though it becomes pronounced only
for large values of ˆxt(Fig. 1b,c). However, when internal noise is introduced, the bias increases
substantially since the internal fluctuations are not directly attenuated by the gains Kt(see also
Appendix A.3).
Figure 1: The invalidity of the unbiasedness condition .(a)A toy example illustrating estimation bias.
The black line represents the dynamics of a partially observable process, xt, while the red line shows
the state estimate, ˆxt, biased by random internal fluctuations from the noise term ηt(orange arrow).
(b-c)E[xt|ˆxt], fort= 8, as a function of ˆxtforση= 0.0,0.6, respectively, using the solutions from
[1]. The conditional expectation E[xt|ˆxt]is computed through Monte Carlo simulations (dots ±error
bars: mean ±1std). The gray straight line represents the identity line, where E[xt|ˆxt] = ˆxt.
As said above, in the classic LQAG problem the optimal Kalman filter satisfies the orthogonality
principle [ 8]. In contrast, when internal noise is non-zero we show that the orthogonality principle
does not hold anymore for the optimal filter (see Appendix A.3.1). We further show that the condition
of unbiasedness implies the orthogonality principle, but not the other way around. Therefore, applying
unbiasedness to solve the optimal control problem introduced in Section 2 leads to suboptimal
solutions when internal noise is present, irrespective of control or signal-dependent noise. A similar
issue arises when the algorithm has not yet converged, even for zero internal noise, because the
orthogonality principle only holds for optimized filter gains (and in the absence of internal noise).
As a result, the algorithm in [ 1] also fails to produce the optimal control when derived with a fixed
suboptimal estimator, and to produce the optimal filter estimate with a fixed suboptimal controller.
3 A Novel Algorithm for Optimal Control Problems
To address the issues outlined in the previous section, we introduce an alternative method for solving
the LQMG problem presented in Section 2.2. We compute the expected total accumulated cost,
E[J], by averaging over all stochastic terms present in Eqs. 5-7, and as a function of LtandKt.
For fixed LtandKt,E[J]serves as the objective function for a standard gradient descent algorithm
aimed at minimizing it. In Section 3.1, we detail the computation of this objective function through
moment propagation and discuss the minimization process with respect to L1,···,T−1andK1,···,T−2.
In Section 3.2, we demonstrate that this approach outperforms state-of-the-art algorithms. In Section
3.3, we derive the analytical counterpart to our numerical algorithm.
3.1 Minimization of Theoretical Expected Cost Through Numerical Gradient Descent (GD)
Our method assumes linear control, where the control signal utis linear in the internal estimate
ut=Ltˆxt. (8)
This assumption is not very limiting, as it is correct for the classic LQAG problem and has been
used before for the LQMG problem [ 1]. Crucially, we do not assume unbiasedness to solve the
5optimal control problem. The expected total accumulated cost is computed by propagating the first
two moments of xandˆxin closed form. Given that both control and estimation are linear in xandˆx,
and the cost function is quadratic in xandu, the first and second moments act as sufficient statistics.
As a result, no additional approximations (e.g., assuming Gaussianity of xandˆx) are required to find
the optimal solutions. By using Eq. 8 and the formula for the expected cost of a quadratic form, Eq.
3 can be rewritten as
E[J] =TX
t=1E[jt] =TX
t=1(E[xt]⊺QtE[xt] +E[ˆxt]⊺L⊺
tRtLtE[ˆxt]+
+Tr[QtΣxt] +Tr[L⊺
tRtLtΣˆxt]),(9)
where Tr[·]stands for the trace operation, Σxtis the covariance matrix of the latent state xtandΣˆxt
is the covariance of the state estimate at time t. Note that E[xt],E[ˆxt],ΣxtandΣˆxtwill implicitly
depend on L1,...,t−1andK1,...,t−1. From Eqs. 5-7 we can derive the update equations to propagate
the first and second-order moments E[xt]E[ˆxt],ΣxtandΣˆxtin a closed-form manner, in order
to compute the total expected cost E[J]at fixed L1,···,T−1andK1,···,T−2(for the derivation see
Appendix A.4.1). Here and in the following we set c=d= 1 for simplicity (the case c, d > 1
follows simply by replacing terms with DorCmatrices by CiandDi, respectively, and sum over i).
To rewrite our results in a more compact form, we define (similarly to [27, 39])
µt=
µxt
µˆxt
=
E[xt]
E[ˆxt]
, (10)
Σt=
ΣxtΣxt,ˆxt
Σˆxt,xtΣˆxt
, (11)
Mt=
A BL t
KtH A +BLt−KtH
(12)
and
Gt=CLt(Σˆxt+µˆxtµ⊺
ˆxt)L⊺
tC⊺+ Ωξ 0
0 KtD(Σxt+µxtµ⊺
xt)D⊺K⊺
t+KtΩωK⊺
t+ Ωη
,
(13)
where Σxt,ˆxt=E[xtˆx⊺
t]−E[xt]E[ˆxt]⊺andΣˆxt,xt= Σ⊺
xt,ˆxt. We have defined µtas a column vector
whose block elements are m−dimensional vectors. Similarly, Σt,MtandGtare block matrices,
whose block elements are m×mmatrices.
With these definitions, we see that the first and second moments propagate in a closed manner as
µt+1=Mtµt, (14)
Σt+1=MtΣtM⊺
t+Gt. (15)
In other words, if the first and second moments are known at time t, their values can be recursively
computed at time t+ 1, and no other moments are involved in the calculations.
As a result, given the initial conditions for µ1andΣ1, we can compute the expected accumulated cost
E[J]at fixed L1,···,T−1andK1,···,T−2, by using Eqs. 14-15 together with Eq. 9. The pseudo-code
for the algorithm to compute the expected cost E[J]is provided in Appendix A.4.2, Algorithm 1.
To find the optimal control and filter gains we would then use E[J]as the objective function of a
numerical gradient descent procedure. The analytical gradient descent counterpart is discussed in
Section 3.3.
3.2 Experiments: Enhanced Performance with the GD Algorithm
We apply our algorithm and compare it with the state-of-the-art solutions in two scenarios governed
by a linear dynamical system (Eqs. 5-8). Hereafter, GD refers to our numerical algorithm (Section
3.1), and TOD refers to the algorithm from [ 1]. First, in a simplified one-dimensional reaching task
(m=p=k= 1) with all noise sources present, we show that for non-zero internal noise, Ωη>0,
GD outperforms TOD, resulting in a lower accumulated cost. Second, in a reaching task with a
four-dimensional state and one-dimensional control and sensory feedback ( m= 4,p=k= 1), GD
predicts qualitatively different behavior and shows a 90% performance improvement when internal
noise contributes 10% of the total.
6One-Dimensional Case: Understanding the Qualitative Differences We examine the case where
m=p=k= 1, incorporating multiplicative, additive, and internal noise. The system parameters
are provided in Table 2 in Appendix A.5.1 (note that we define the strength of the internal noise as
ση=p
Ωη). With non-zero internal noise, our algorithm achieves lower-cost solutions compared to
the method proposed in [ 1] (Fig. 2a). This improved performance arises from different modulations
ofLtandKtasσηvaries (Figs. 2b, c). Crucially, our solution results in control gains that decrease
as internal noise increases, while the TOD solution shows little sensitivity to internal noise magnitude
(Fig. 2b): internal noise increasingly intertwines the optimal solutions for KtandLt. In Appendix
A.5.2, we provide a geometric interpretation of why this modulation is optimal, demonstrating that
this optimality enhances adaptability, and showing how internal noise disrupts the orthogonality
principle.
As outlined in Section 3, the incorrect unbiasedness condition implies the orthogonality principle.
Thus, even if the estimator is biased, the algorithm in [ 1] finds the optimal solution with zero internal
noise, as this principle holds for the optimal Kalman filter. However, for non-zero internal noise,
ση>0, the TOD algorithm underperforms due to the breakdown of the orthogonality principle. In
Appendix A.5.3, we also discuss that this suboptimality is observed before the algorithm converges,
when the optimal control law is derived from fixed suboptimal filters, and vice versa, regardless of
the presence of internal noise.
Figure 2: Enhanced performance and different solutions with internal noise .(a)Expected accumu-
lated cost E[J], computed by averaging the quantity from Eq. 3 over 50ktrials, as a function of the
internal noise strength ση, for TOD [ 1] and GD (Section 3.1) algorithms (mean ±1SEM from Monte
Carlo simulations, error bars not visible as too small). The expected value aligns with our theoretical
estimate of E[J], as derived in Section 3.1. (b-c) Optimal control and filter gains, LtandKt, for
TOD and GD and algorithms. We also present the solutions derived from the analytical counterpart
of the numerical GD algorithm to demonstrate that they match the optimal solutions (‘Fixed Point
Optimization with Moments Propagation’ – FPOMP – algorithm, see Section 3.3).
Multi-Dimensional Case: A Motor Control Application To demonstrate the scalability of our
algorithm to more realistic motor control scenarios, we examine a problem with a four-dimensional
state vector ( m= 4) and one-dimensional control and sensory feedback ( p=k= 1). The task
is identical to that in [ 1], except that we include internal noise, which was absent in the original
formulation. We model a single-joint reaching movement aimed at minimizing the distance between
the hand position ptand a target, while minimizing control effort. The state variable of the problem is
xt= [pt,˙pt≡dpt/dt, f t, gt], where ftis the force acting on the hand and gtis an auxiliary variable
used to filter the control signal ut(see [ 1] and [ 40] for a more detailed discussion). We include
control and state-dependent noise, as well as internal noise, perturbing the estimate of pt. Note that
now we denote ση=q
Ω1,1
η. All parameters are listed in Appendix A.5.4.
Our results confirm the findings from the previous scenario in this more complex sensorimotor task.
The GD algorithm achieves a lower expected accumulated cost, with the performance gap widening
as internal noise σηincreases (Fig. 3a). This is achieved by reducing control gains with increasing
ση(Fig. 3b), resulting in a smoother control signal on individual trials (Fig. 3c) and overall reduced
control effort (Fig. 3d). These adjustments lead to two key behavioral outcomes: slower movements
compared to TOD solutions and significantly reduced trial-to-trial variability (Fig. 3e). As mentioned
7earlier, GD outperforms the algorithm proposed by [ 1], reducing the cost by up to 90% when internal
noise contributes approximately 10% of the total noise ( ση= 0.05). To further quantify the impact
of internal noise in this scenario and enhance clarity, we calculate the ratio between the average
fluctuation amplitude of the state estimate ( FA, the standard deviation of the state estimate) and
the average range of variation of the state ( RV, the range of variation in position pt, defined as
maxt(pt)−mint(pt)). The resulting ratio is FA/RV≈0.5forση= 0.05(see also Appendix A.5.5).
We emphasize that the GD algorithm naturally handles arbitrarily high-dimensional problems without
requiring any further adjustments. Algorithm 1 in Appendix A.4.2, which serves as the objective
function for the numerical optimization via gradient descent, is designed to accommodate arbitrary
dimensions for state, control, and sensory feedback, as well as trial duration. However, the time
horizon must remain finite by assumption, similar to [ 1]. We empirically demonstrate the scalability
of our algorithm by applying it to a significantly higher-dimensional problem, where the linear
dynamical system is governed by random matrices with Gaussian entries. Specifically, we consider
m= 10 ,p= 4, and k= 10 for the dimensions of state, control, and observation, respectively
(Appendix A.5.6).
Figure 3: Single-joint reaching task .(a)Expected accumulated cost E[J]as a function of the internal
noise (mean ±1SEM, error bars not visible as too small), for TOD and GD algorithms. (b)Magnitude
of the control gain vector as a function of time for TOD and GD solutions. (c)control signal utin
a sample trial for the two algorithms for ση= 0.05.(d)Amount of control as σηincreases, that is,
mean integral of the absolute control signal for the two algorithms. (e)Mean position over time for
the two solutions. All averages are over 50ktrials; shadowed areas are SEM.
3.3 An Analytical Approach: FPOMP Algorithm
Although, as discussed in the previous section, our GD algorithm performs well for arbitrarily
high-dimensional problems (see also Appendix A.5.6), its application to complex, real-world tasks
can become computationally expensive. With L1,···,T−1∈Rp×mandK1,···,T−2∈Rm×k, the total
number of parameters to optimize via numerical Gradient Descent is mp(T−1) + mk(T−2),
which can become quite large for high-dimensional state spaces. For example, in the 1D problem,
the TOD and GD algorithms have comparable and short computation times. However, for the
multi-dimensional sensorimotor task presented above, the GD algorithm takes significantly longer:
while the TOD algorithm completes in just a few minutes on a standard laptop, the GD optimization
requires several hours (approximately 4 hours).
To address this, we propose an analytically derived iterative algorithm, where we alternate between
finding the optimal (i.e., cost-minimizing) L1,···,T−1andK1,···,T−2, denoted as L∗
tandK∗
t, for
fixed state and state estimate moments, µtandΣt, and re-computing these moments in light of the
updated Lt’s and Kt’s. We refer to this method as the ‘Fixed Point Optimization with Moments
8Propagation’ (FPOMP) algorithm. Note that, when optimizing for LtandKt, we condition also on
all the future control and filter gains, therefore not only on µtandΣt, but also on Lt+1,···,T−1and
Kt+1,···,T−2. To simplify the notation, we will omit this implicit dependence and explicitly state
only the dependence on µtandΣt. Therefore, at each iteration, we identify the critical points of the
total conditional expected cost with respect to LtandKt. We can compute the conditional expected
cost per step at time t+i,i= 0, ..., T −tas
E[jt+i|µt,Σt] =E[xt+i|µt,Σt]⊺Qt+iE[xt+i|µt,Σt]+
+E[ˆxt+i|µt,Σt]⊺L⊺
t+iRt+iLt+iE[ˆxt+i|µt,Σt]+
+Tr[Qt+iΣxt+i|µt,Σt] +Tr[L⊺
t+iRt+iLt+iΣˆxt+i|µt,Σt],(16)
where E[xt+i|µt,Σt],E[xt+i|µt,Σt],Σxt+i|µt,ΣtandΣˆxt+i|µt,Σtare computed by propagating the
moments µtandΣt(Eqs. 14-15) until ˜t=t+i. Indeed, as discussed in Section 3.1, µtandΣtserve
as sufficient statistics for computing the expected cost. To derive L∗
tandK∗
t, we set the derivatives
of the expected cost in Eq. 9 to zero. Excluding the constant terms, we obtain
∂
∂LtT−tX
i=0E[jt+i|µt,Σt] = 0 (17)
∂
∂KtT−tX
i=1E[jt+i|µt,Σt] = 0 . (18)
As shown in Appendix A.6.1 and A.6.2, solving Eqs. 17-18 leads to a backward algorithm to compute
L∗
tandK∗
t,
L∗
t=f(µt,Σt, L∗
t+1,···,T−1, K∗
t+1,···,T−2) (19)
K∗
t=g(µt,Σt, L∗
t+1,···,T−1, K∗
t+1,···,T−2), (20)
witht= 1, ..., T −1forL∗
tandt= 1, ..., T −2forK∗
t. From this we can build a recursive
relationship that, starting from an initial guess for L∗
1,···,T−1andK∗
1,···,T−2, iteratively computes
all the moments µ1,···,TandΣ1,···,T(Eqs. 14-15) at fixed L∗
1,···,T−1andK∗
1,···,T−2. Given those
moments, L∗
1,···,T−1andK∗
1,···,T−2are updated by using Eqs. 19-20, and so on, until convergence is
attained. The pseudo-code for the FPOMP algorithm, with its implementation details, can be found
in Appendix A.6.3, Algorithm 2. In such a way, we eliminate the numerical optimization procedure,
making the algorithm suitable for extremely large optimal control problems. The FPOMP algorithm
is flexible and works for arbitrary dimensions of state, control, sensory feedback, and trial duration,
with computational costs and runtime comparable to those of the approach proposed in [1].
In Appendix A.6.1, we explicitly solve Eqs. 17-18 for the one-dimensional case, while in Appendix
A.6.2 we extend the approach to a multi-dimensional scenario, considering, for the sake of simplicity,
the LQAG problem (but, crucially, including internal noise), to prove the generalizability of Algorithm
2. In Appendix A.6.2, we provide the solution for Eq. 17, with the same procedure applying to Eq.
18. We also discuss the potential extension to the full noise model. Lastly, in Appendix A.7, we
examine the assumption of linear dynamics and extend our approach to a switching linear dynamical
system to make it less restrictive.
Experiments To empirically validate our iterative algorithm, we apply it to the same one-
dimensional problem discussed in Section 3.2. The FPOMP algorithm aligns with the optimal
solutions found by the GD algorithm, resulting in identical solution and performance (Figs. 2b,c).
In Fig. 2a, the FPOMP algorithm follows the same cost trend as the GD algorithm, with the curves
overlapping (the FPOMP curve is omitted for clarity, see also Fig. 10 in Appendix A.8.1).
In Appendix A.8.2, we analyze the same multi-dimensional task as in Section 3.2, excluding multi-
plicative noise but including internal noise. The results show that the FPOMP algorithm matches the
GD optimal solutions for the controller and outperforms TOD when ση>0. Even in its current form,
FPOMP surpasses the method from [1] when internal noise is considered.
4 Conclusion
In this paper, we provide a novel approach for solving stochastic optimal control problems adapted
to the noise characteristics of the human sensorimotor system. Our work builds on the seminal
9study in [ 1], where the classical LQG framework (called here LQAG) is extended to the LQMG
framework to include both control and signal-dependent noise, as well as internal noise in the
estimation process. This extension provides a more realistic description of the sensorimotor system,
enabling the reproduction of a larger sample of behavioral data in motor control, albeit at the cost of
reduced mathematical tractability.
However, the solution derived in [ 1], which is widely used [ 27,33,41,34,35,42,36,37,38],
suffers from an ill-conditioned derivation. Specifically, that solution assumes unbiased estimators
–a condition that, as we prove numerically and conceptually in this work, does not hold, leading to
suboptimal performance when internal noise is considered or before algorithmic convergence. This
suboptimality arises from the close relationship between unbiasedness and the orthogonality principle
of an optimal estimator, where the former, mathematically, implies the latter. Yet, the orthogonality
principle is satisfied by the optimal filter only in the absence of internal noise and under algorithmic
convergence.
Assuming only that control is linear in the current state estimate, we derive an alternative algorithm
that optimizes control and estimation without requiring unbiasedness. The optimal solution is obtained
by propagating sufficient statistics to compute the expected cost, which is minimized via numerical
gradient descent on filter and control gains. For a more constrained, but still relevant, version of
the problem, we derive the analytical counterpart, which alternates between forward propagation of
moments and backward optimization of control and filter gains until convergence. This makes our
approach suitable for high-dimensional problems, significantly reducing computational cost.
We demonstrate superior performance in the presence of internal noise and before convergence is
reached (that is, when filter or control gains are fixed at suboptimal values, regardless of the level
of internal noise), and provide both mathematical and heuristic explanations. Joint modulation of
control and filter gains helps filter internal fluctuations, enhancing adaptability and generalization
across internal noise levels, as discussed in Appendix A.5.2. By applying our algorithm to a
sensorimotor task, we make novel behavioral predictions that distinguish our solution from previous
ones. Specifically, we find that control gains decrease with increasing internal noise, leading to
smoother control signals in individual trials. This results in slower movements with reduced trial-to-
trial variability.
In summary, our algorithm extends optimal feedback control to a broader range of problems in
systems neuroscience.
Limitations and Future Work One limitation of our work is the assumption of state-independent
filter gains for the optimal estimator: in the presence of multiplicative noise, non-adaptive estimation
proves sub-optimal. Additionally, incorporating more realistic cost functions could extend our
framework beyond the traditional quadratic dependence. Further investigation into the connections
between our optimal control law and biologically plausible learning rules [ 43] may also be necessary.
Moreover, we have not formally demonstrated the convergence properties of our algorithm to a global
minimum, although our algorithm is guaranteed to converge at least to a local minima, and we did
not find any numerical evidence for multiple local minima. The next immediate step is to derive the
FPOMP algorithm for the general case with multiplicative noise, as discussed in Appendix A.6.2.
10Acknowledgments and Disclosure of Funding
This work was supported by: NEI-NIH R01 EY016178; Grant PRE2021-097778, funded by MI-
CIU/AEI/10.13039/501100011033 and by “ESF+”; "Project PID2023-146524NB-I00 financed by
MCIN/AEI/10.13039/501100011033/ ERDF, EU, the Spanish State Research Agency and the Euro-
pean Union, and ICREA Academia.
References
[1]Emanuel Todorov. Stochastic optimal control and estimation methods adapted to the noise
characteristics of the sensorimotor system. Neural computation , 17(5):1084–1108, 2005.
[2]A Aldo Faisal, Luc PJ Selen, and Daniel M Wolpert. Noise in the nervous system. Nature
reviews neuroscience , 9(4):292–303, 2008.
[3]David W Franklin and Daniel M Wolpert. Computational mechanisms of sensorimotor control.
Neuron , 72(3):425–442, 2011.
[4]Emmanuel Guigon, Pierre Baraduc, and Michel Desmurget. Computational motor control:
redundancy and invariance. Journal of neurophysiology , 97(1):331–347, 2007.
[5]Stephen H Scott. Optimal feedback control and the neural basis of volitional motor control.
Nature Reviews Neuroscience , 5(7):532–545, 2004.
[6]Emanuel Todorov and Michael I Jordan. Optimal feedback control as a theory of motor
coordination. Nature neuroscience , 5(11):1226–1235, 2002.
[7]Emanuel Todorov. Optimality principles in sensorimotor control. Nature neuroscience , 7(9):907–
915, 2004.
[8] Mark Davis. Stochastic modelling and control . Springer Science & Business Media, 2013.
[9] Robert F Stengel. Optimal control and estimation . Courier Corporation, 1994.
[10] Gerald E Loeb, WS Levine, and Jiping He. Understanding sensorimotor feedback through
optimal control. In Cold Spring Harbor symposia on quantitative biology , volume 55, pages
791–803. Cold Spring Harbor Laboratory Press, 1990.
[11] Arthur D Kuo. An optimal control model for analyzing human postural balance. IEEE
transactions on biomedical engineering , 42(1):87–101, 1995.
[12] Bruce Richard Hoff. A computational description of the organization of human reaching and
prehension . University of Southern California, 1992.
[13] Tamar Flash and Neville Hogan. The coordination of arm movements: an experimentally
confirmed mathematical model. Journal of neuroscience , 5(7):1688–1703, 1985.
[14] Christopher M Harris and Daniel M Wolpert. Signal-dependent noise determines motor planning.
Nature , 394(6695):780–784, 1998.
[15] GG Sutton and K Sykes. The variation of hand tremor with force in healthy subjects. The
Journal of physiology , 191(3):699–711, 1967.
[16] Emanuel Todorov. Cosine tuning minimizes motor errors. Neural computation , 14(6):1233–
1260, 2002.
[17] Richard A Schmidt, Howard Zelaznik, Brian Hawkins, James S Frank, and John T Quinn Jr.
Motor-output variability: a theory for the accuracy of rapid motor acts. Psychological review ,
86(5):415, 1979.
[18] Reza Shadmehr and John W Krakauer. A computational neuroanatomy for motor control.
Experimental brain research , 185:359–381, 2008.
11[19] Joseph Y Nashed, Frédéric Crevecoeur, and Stephen H Scott. Influence of the behavioral
goal and environmental obstacles on rapid feedback responses. Journal of neurophysiology ,
108(4):999–1009, 2012.
[20] Kelvin E Jones, Antonia F de C Hamilton, and Daniel M Wolpert. Sources of signal-dependent
noise during isometric force production. Journal of neurophysiology , 88(3):1533–1544, 2002.
[21] Chistina A Burbeck and Yen Lee Yap. Two mechanisms for localization? evidence for
separation-dependent and separation-independent processing of position information. Vision
research , 30(5):739–750, 1990.
[22] David Whitaker and Keziah Latham. Disentangling the role of spatial scale, separation and
eccentricity in weber’s law for position. Vision research , 37(5):515–524, 1997.
[23] Emanuel Vassilev Todrov. Studies of goal directed movements . PhD thesis, Massachusetts
Institute of Technology, 1998.
[24] Rubén Moreno-Bote, Jeffrey Beck, Ingmar Kanitscheider, Xaq Pitkow, Peter Latham, and
Alexandre Pouget. Information-limiting correlations. Nature neuroscience , 17(10):1410–1417,
2014.
[25] Philippe Vindras and Paolo Viviani. Frames of reference and control parameters in visuomanual
pointing. Journal of Experimental Psychology: Human Perception and Performance , 24(2):569,
1998.
[26] Andrew M Gordon, Goran Westling, Kelly J Cole, and Roland S Johansson. Memory represen-
tations underlying motor commands used during manipulation of common and novel objects.
Journal of neurophysiology , 69(6):1789–1796, 1993.
[27] Matthias Schultheis, Dominik Straub, and Constantin A Rothkopf. Inverse optimal control
adapted to the noise characteristics of the human sensorimotor system. Advances in Neural
Information Processing Systems , 34:9429–9442, 2021.
[28] Sanjoy K Mitter. Filtering and stochastic control: A historical perspective. IEEE Control
Systems Magazine , 16(3):67–76, 1996.
[29] Tryphon T Georgiou and Anders Lindquist. The separation principle in stochastic control,
redux. IEEE Transactions on Automatic Control , 58(10):2481–2494, 2013.
[30] D Peter Joseph and T Julius Tou. On linear control theory. Transactions of the American
Institute of Electrical Engineers, Part II: Applications and Industry , 80(4):193–196, 1961.
[31] Henk Van de Water and Jan Willems. The certainty equivalence property in stochastic control
theory. IEEE Transactions on Automatic Control , 26(5):1080–1087, 1981.
[32] Mark M Churchland, Afsheen Afshar, and Krishna V Shenoy. A central source of movement
variability. Neuron , 52(6):1085–1096, 2006.
[33] Dominik Straub and Constantin A Rothkopf. Putting perception into action with inverse optimal
control for continuous psychophysics. Elife , 11:e76635, 2022.
[34] Jonathon W Sensinger and Strahinja Dosen. A review of sensory feedback in upper-limb
prostheses from the perspective of human motor control. Frontiers in neuroscience , 14:345,
2020.
[35] Dan Liu and Emanuel Todorov. Evidence for the flexible sensorimotor strategies predicted by
optimal feedback control. Journal of Neuroscience , 27(35):9354–9368, 2007.
[36] Jun Izawa, Tushar Rane, Opher Donchin, and Reza Shadmehr. Motor adaptation as a process of
reoptimization. Journal of Neuroscience , 28(11):2883–2891, 2008.
[37] Tomohiko Takei, Stephen G Lomber, Douglas J Cook, and Stephen H Scott. Transient deac-
tivation of dorsal premotor cortex or parietal area 5 impairs feedback control of the limb in
macaques. Current Biology , 31(7):1476–1487, 2021.
12[38] Maryam M Shanechi, Ziv M Williams, Gregory W Wornell, Rollin C Hu, Marissa Powers, and
Emery N Brown. A real-time brain-machine interface combining motor target and trajectory
intent using an optimal feedback control design. PloS one , 8(4):e59049, 2013.
[39] Jur Van Den Berg, Pieter Abbeel, and Ken Goldberg. Lqg-mp: Optimized path planning for
robots with motion uncertainty and imperfect state information. 2011.
[40] David A Winter. Biomechanics and motor control of human movement . John wiley & sons,
2009.
[41] Philipp Karg, Simon Stoll, Simon Rothfuß, and Sören Hohmann. Inverse stochastic optimal
control for linear-quadratic gaussian and linear-quadratic sensorimotor control models. In 2022
IEEE 61st Conference on Decision and Control (CDC) , pages 2801–2808. IEEE, 2022.
[42] Joseph Y Nashed, Frédéric Crevecoeur, and Stephen H Scott. Rapid online selection between
multiple motor plans. Journal of Neuroscience , 34(5):1769–1780, 2014.
[43] Johannes Friedrich, Siavash Golkar, Shiva Farashahi, Alexander Genkin, Anirvan Sengupta,
and Dmitri Chklovskii. Neural optimal feedback control with local learning rules. Advances in
Neural Information Processing Systems , 34:16358–16370, 2021.
[44] Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical
University of Denmark , 7(15):510, 2008.
[45] Philip Becker-Ehmck, Jan Peters, and Patrick Van Der Smagt. Switching linear dynamics for
variational bayes filtering. In International conference on machine learning , pages 553–562.
PMLR, 2019.
[46] Steven L Brunton, Bingni W Brunton, Joshua L Proctor, and J Nathan Kutz. Koopman invariant
subspaces and finite linear representations of nonlinear dynamical systems for control. PloS
one, 11(2):e0150171, 2016.
13A Appendix
A.1 Background: Classic LQAG Solutions
The optimal control and filter gains, L1,···,T−1andK1,···,T−2, for the classic LQAG problem —
defined by Eqs. 1-2 - can be derived analytically and are given by [8, 1]
Lt=−(Rt+B⊺St+1B)−1B⊺St+1A (21)
St=Qt+A⊺St+1(A−BLt) (22)
Kt=AΣe
tH⊺(HΣe
tH⊺+ Ωω)−1(23)
Σe
t+1= Ω ξ+ (A−KtH)Σe
tA⊺. (24)
A.2 Background: Solutions from [1]
The algorithm proposed in [ 1] alternates between optimizing control and estimation. As for the classic
LQAG problem, the control is optimized iteratively in a backward-in-time fashion, while keeping
the filters Ktfixed. The solution is derived by using the method of dynamic programming, writing
down the Bellman equation for the optimal cost-to-go assuming the unbiasedness of the estimator [ 1].
From there, the optimal filters Ktare found at fixed Lt, again by minimizing the cost-to-go. Taken
together, these two optimization steps lead to an iterative algorithm that is supposed to provide the
optimal solution to the control problem [ 1]. For completeness, we present here the optimal solutions
forLtandKtfrom [1].
The optimal control gains are given by the following backward algorithm
Lt= (Rt+B⊺Sx
t+1B+X
iC⊺
i(Sx
t+1+Se
t+1)Ci)−1B⊺Sx
t+1A (25)
Sx
t=Qt+A⊺Sx
t+1(A−BLt) +X
iD⊺
iK⊺
tSe
t+1KtDi (26)
Se
t=A⊺Sx
t+1BLt+ (A−KtH)⊺Se
t+1(A−KtH) (27)
withST=QTandSe
T= 0. Note that in [ 1] the optimal control law is defined as ut=−Ltˆxt,
whereas we use Eq. 8: to compare the solutions from [ 1] with ours, we need to invert the sign of the
control gains Lt.
The optimal filter gains follow instead a forward optimization
Kt=AΣe
tH⊺(HΣe
tH⊺+ Ωω+X
iDi(Σe
t+ Σˆx
t+ Σˆxe
t+ Σeˆx
t)D⊺
i)−1(28)
Σe
t+1= Ω ξ+ Ωη+ (A−KtH)Σe
tA⊺+X
iCiLtΣˆx
tL⊺
tC⊺
i (29)
Σˆx
t+1= Ω η+KtHΣe
tA⊺+ (A−BLt)Σˆx
t(A−BLt)⊺+
+ (A−BLt)Σˆxe
tH⊺K⊺
t+KtHΣeˆx
t(A−BLt)⊺(30)
Σˆxe
t+1= (A−BLt)Σˆxe
t(A−KtH)⊺−Ωη (31)
Σeˆx
t= (Σˆxe
t)⊺(32)
withΣe
1= Σ x1,Σˆx
1= ˆx1ˆx⊺
1andΣˆxe
1= 0. Note that in [ 1]Σe
t:=E[ete⊺
t], where et:=xt−ˆxt,
Σˆx
t:=E[ˆxtˆx⊺
t]andΣˆxe
t:=E[ˆxte⊺
t].
A.3 Unbiasedness and Orthogonality Principle: How Internal Noise Affects Optimality
In Section 2.3, we discussed the invalidity of the unbiasedness condition. Here, we provide the details
of the one-dimensional problem used to numerically validate this assertion, presenting the plots in
Figs. A.3b, c also for the case where ση= 0.3. The system parameters are listed in Table 2 in
14Appendix A.5.1, where ση=p
Ωη. The trial duration is set to T= 10 time steps, and we vary ση
across the values 0.0,0.3,0.6. Att= 8, we compute E[xt|ˆxt]as a function of ˆxt. To do this, we
collect the values of x8andˆx8over5·107trials, bin the data for ˆxtwith a bin size of δˆx= 0.1, and
compute the mean of xtwithin each bin. The standard deviation is shown as error bars. Note that the
choice of tis arbitrary.
Figure 4: The invalidity of the unbiasedness condition . Here we plot E[xt|ˆxt], for a given value
oft, as a function of ˆxtfor different ση, using the solutions from [ 1]. The conditional expectation
E[xt|ˆxt]is computed through Monte Carlo (MC) simulations. (a)E[xt|ˆxt]as a function of ˆxtfor
ση= 0(dots with error bars given by the std of our MC estimate). The gray dotted line stands for
the bisector, where E[xt|ˆxt] = ˆxt.(b)Same as (a), but for ση= 0.3.(c)Same as (a)and(b), but
forση= 0.6.(d-f) Absolute value of the distance between E[xt|ˆxt]andˆxtas a function of |ˆxt|for
ση= 0.0,0.3,0.6. The gray dotted lines represent E[xt|ˆxt] = ˆxt.
A.3.1 Orthogonality Principle and Suboptimality
In Section 2.3, we outlined the relationship between the unbiasedness condition and the orthogonality
principle, highlighting how this connection results in the suboptimality of the approach in [ 1] when
internal noise is present. For simplicity, and without loss of generality, we consider a 1D scenario,
m=p=k= 1, where the orthogonality principle implies [8]
Ωt≡E[ˆxt(xt−ˆxt)] =E[ˆx2
t]−E[xtˆxt] = 0 . (33)
This results in the estimation error being orthogonal to the estimate itself ˆxt. We also set c=d= 1.
We demonstrate here that the condition E[xt|ˆxt] = ˆxt, used in [ 1] to derive the optimal control law,
implies the orthogonality principle and that this principle cannot be satisfied by an optimal estimator
when internal noise is present.
Assuming
E[xt|ˆxt] = ˆxt (34)
and multiplying by ˆxton both sides and then taking the expectation over ˆxtwe obtain
E[ˆx2
t] =E[xtˆxt] (35)
corresponding to Eq. 33. In the absence of internal noise, the optimal filter gains Ktcan be found by
imposing the orthogonality principle, without the need to minimize the cost function. It holds
Ωt+1= (K2
tH2+K2
tD2−AKtH)E[x2
t]+
+ (A2+K2
tH2+ABL t−2AKtH−BLtKtH)Ωt+
+ (AKtH−K2
tH2)E[xtˆxt] +K2
tΩω+ Ωη(36)
If we use Ω1= 0, as in [ 1] due to the initial conditions, we can solve the equation Ωt= 0,
∀t= 1, .., T , obtaining an equation for Kt,
15Kt=AHΓt±p
A2H2Γ2
t−4(H2Γt+D2E[x2
t] + Ω ω)Ωη
2(H2Γt+D2E[x2
t] + Ω ω)(37)
with
Γt=E[x2
t]−E[xtˆxt]. (38)
ForΩη= 0, Eq. 37 simplifies to
Kt=AHΓt±AHΓt
2(H2Γt+D2E[x2
t] + Ω ω). (39)
Observing that the solution Kt= 0would correspond to an open-loop strategy, sub-optimal (sensory
information would not be integrated) for a stochastic partially observable system as the one we are
considering, we get for the optimal filter gains
K∗
t=AHΓt
H2Γt+D2E[x2
t] + Ω ω. (40)
It can be shown that the solution in [ 1] forΩη= 0aligns with Eq. 40. We observe that Eq. 40 can
replace Eq. 20 in Algorithm 2 to optimize the filter gains. For Ωη= 0, this leads to the optimal
solution. Thus, in the absence of internal noise, the optimization of control and estimation can be
performed using two separate objective functions: one enforcing the orthogonality principle for the
optimal estimator, and the other minimizing the cost function for the optimal controller, regardless
of the multiplicative nature of the noise. This could also be relevant for more biologically plausible
scenarios [5].
However, when Ωη>0, the existence of a real solution for Eq. 37 depends on the initial conditions
and is no longer guaranteed. Moreover, as we demonstrate in Appendix A.5.2, the optimal solutions
do not satisfy Ωt= 0forΩη>0. Therefore, the orthogonality principle holds for an optimal Kalman
filter only when Ωη= 0. Consequently, the algorithm in [ 1] assumes unbiased estimation, which
should imply the orthogonality principle, even in cases where it no longer applies to the optimal
estimator.
Separation Principle, Orthogonality Principle, Unbiasedness: A Brief Digression Unbiased-
ness, orthogonality, and the separation principle are related but distinct concepts. Here, we briefly
clarify their differences and commonalities.
The separation principle stems from the formulation of the classic LQAG problem, where the optimal
solutions for control and estimation are mathematically independent, allowing for their separate
optimization. However, with multiplicative noise, this independence is lost [ 1]. We have shown that
this breakdown occurs even with additive internal noise and zero multiplicative noises.
The orthogonality principle (in 1D) states that E[xtˆxt] =E[ˆx2
t], meaning that estimation error and
estimate are orthogonal. This condition holds for an optimal Kalman filter only in the absence of
internal noise (see Fig. 5a). Internal fluctuations, however, disrupt the mathematical independence
between control and estimation, invalidating the orthogonality principle as well. These two concepts
are distinct: for instance, with no internal noise but non-zero multiplicative noise, the orthogonality
principle would still hold, yet the mathematical independence between control and estimation would
be broken.
The unbiasedness condition (which, as previously discussed, never holds) states that E[xt|ˆxt] = ˆxt,
implying the orthogonality principle. This explains the optimality of the solutions in [ 1] in the
absence of internal noise—not due to the validity of the unbiasedness condition, but because the
orthogonality condition holds.
A.4 A Novel Algorithm for Optimal Control Problems
A.4.1 Derivation of Closed-Form Equations for Moments Propagation
We explicitly derive Eqs. 14-15 here. Notably, no approximations are required to propagate the
first two moments of the joint variable (x,ˆx)in closed form, as both control and estimation are
linear in the state and state estimate (see Eqs. 5-8). Consequently, Eqs. 14-15 hold regardless of the
16distribution of (x,ˆx). By taking the expected value of Eqs. 5-7 over the joint distribution of state,
state estimate and sensory feedback, we obtain
E[xt+1] =AE[xt] +BLtE[ˆxt] (41)
E[ˆxt+1] =KtHE[xt] + (A+BLt−KtH)E[ˆxt], (42)
which correspond to Eq. 14. Similarly, we compute the second non-central moments of the joint
variable (x,ˆx), resulting in
E[xt+1x⊺
t+1] =AE[xtx⊺
t]A⊺+BLtE[ˆxtˆx⊺
t]L⊺
tB⊺+
+AE[xtˆx⊺
t]L⊺
tB⊺+BLtE[ˆxtx⊺
t]A⊺+CLtE[ˆxtˆx⊺
t]L⊺
tC⊺+ Ωξ(43)
E[ˆxt+1ˆx⊺
t+1] =KtHE[xtx⊺
t]H⊺K⊺
t+ (A+BLt−KtH)E[ˆxtˆx⊺
t](A+BLt−KtH)⊺+
+KtHE[xtˆx⊺
t](A+BLt−KtH)⊺+ (A+BLt−KtH)E[ˆxtx⊺
t]H⊺K⊺
t+
+KtDE[xtx⊺
t]D⊺K⊺
t+KtΩωK⊺
t+ Ωη(44)
E[ˆxt+1x⊺
t+1] =KtHE[xtx⊺
t]A⊺+ (A+BLt−KtH)E[ˆxtˆx⊺
t]L⊺
tB⊺+
+KtHE[xtˆx⊺
t]L⊺
tB⊺+ (A+BLt−KtH)E[ˆxtx⊺
t]A⊺(45)
E[xt+1ˆx⊺
t+1] =E[ˆxt+1x⊺
t+1]⊺. (46)
From this, we can derive Eq. 15.
Since the cost function is quadratic in the state and state estimate, the variables µtandΣt, defined in
Eqs. 10-11, serve as sufficient statistics to compute E[J](Eq. 9), which is all that is needed to derive
the optimal control and filter gains.
A.4.2 Pseudo-Code
For the GD algorithm (Section 3.1) we minimize the expected accumulated cost E[J], computed
through Algorithm 1, with respect to the filter and control gains L1,···,T−1, and K1,···,T−2, using
the function "GradientDescent()" in the "Optim.jl" Julia package. The hyper-parameters of the used
algorithms are listed in Table 1 in Appendix A.5.
Algorithm 1 Propagation of the expected cost - GD algorithm
1:Input: µ1,Σ1(initial conditions of the system), L1,···,T−1,K1,···,T−2, and the system parame-
ters (A,B,H,Ci=1,...,c,Di=1,...,d,Ωξ,Ωω,Ωη).
2:Output: E[J]
3:Algorithm steps:
4:E[J] = 0
5:µold=µ1
6:Σold= Σ 1
7:foreach iteration t= 1,2, . . . , T do
8: E[J]←E[J] +E[jt], (Eq. 9)
9: Update MtandGt(Eqs. 12-13)
10: Σnew=MtΣoldM⊺
t+Gt
11: µnew=Mtµold
12: Σold←Σnew
13: µold←µnew
14:end for
A.5 Experiments: GD Algorithm
The hyper-parameters of all the used algorithms are provided in Table 1. For the GD algorithm
(Section 3.1) we minimize the expected accumulated cost E[J], computed through Algorithm 1, using
the function "GradientDescent()" in the "Optim.jl" Julia package.
17Table 1: Hyper-parameters of the used algorithms
Algorithm Description value
GD Number of iterations of the "GradientDescent()" function 100000
FPOMP Number of iterations of the control-estimation optimization 1000
TOD Number of iterations of the control-estimation optimization 1000
A.5.1 One-Dimensional Case: Parameters
We set c=d= 1.
Table 2: Parameters of the one-dimensional problem
Name Description value
A Linear map for the system dynamics 1.0
B Scaling of the control signal 1.0
C Scaling matrix for control-dependent noise 0.5
D Scaling for signal-dependent noise in the sensory feedback 0.5
H Observation matrix 1
Rt Control-dependent cost at each t < T 1
Qt Task-related cost at each time t < T 1
QT Task-related cost at time t=T 20
T time steps 100
E[ˆx1] =E[x1]Initial condition for the mean state and state estimate 1.0
Σx1 Initial covariance of the state 0.0
Σˆx1 Initial covariance of the state estimate 0.0
Ωξ Covariance matrix of the additive Gaussian noise ξt 0.52
Ωω Covariance matrix of the additive Gaussian noise ωt 0.52
ση Standard deviation of the additive internal Gaussian noise ηt{0.0 : 0.1 : 2.0}
A.5.2 One-Dimensional Case: Understanding the Qualitative Differences
When the Orthogonality Principle Is No Longer Optimal As discussed in Section 2.3 and
Appendix A.3.1, the presence of internal noise causes the optimal estimator to no longer satisfy the
orthogonality principle. Here, we demonstrate this result numerically, using the same one-dimensional
problem presented in Section 3.2.
Figure 5: Filtering out the internal fluctuations .(a)Ωt, averaged over time (we indicate the time
average with ⟨·⟩), as a function of σηfor TOD and GD algorithms. (b)⟨Γ⟩as a function of ση.(c)
⟨e⟩as a function of ση. The error bars (mean ±1SEM from Monte Carlo simulations) are not visible
as too small.
The optimal solutions do not minimize Ωt=E[ˆx2
t]−E[xtˆxt](Fig. 5a). Instead, the optimal strategy
appears to favor lower values of Γt=E[x2
t]−E[xtˆxt](Fig. 5b). This allows the system to filter
out internal fluctuations affecting the estimation process, reducing their correlation with the latent
state dynamics, x. As a result, the absolute estimation error, |et|=p
E[(xt−ˆxt)2] =√Ωt+ Γt,
is slightly (but significantly) larger for the GD solutions (Fig. 5c), which seems to help decorrelate
internal noise from the state evolution.
18This ‘decorrelation mechanism’ is achieved through an intertwined modulation of control and filter
gains. In the next paragraph, we provide a geometric interpretation of this behavior through an
eigenvector decomposition of the dynamical system under investigation.
Eigenvector Decomposition and Adaptability of the Solutions In one dimension we can write
the update equations for
Γt=E[x2
t]−E[xtˆxt] (47)
Ωt=E[ˆx2
t]−E[xtˆxt] (48)
as 
Γt+1
Ωt+1
=Mt
Γt
Ωt
+
Ωξ+C2L2
tE[ˆx2
t]
Ωη+K2
tΩω+K2
tD2E[x2
t]
(49)
where
Mt= (A−KtH)
A −BLt
−KtH A +BLt−KtH
. (50)
The eigenvectors of Mtare given by
⃗ w1=
−1
1
(51)
⃗ w2=
BLt/KtH
1
. (52)
Note that the angles θtbetween these two eigenvectors are the same as the angles between the
eigenvectors of the matrix Mt. Indeed, the eigenvectors of Mtare given by
⃗ v1=
1
1
(53)
⃗ v2=
−BLt/KtH
1
. (54)
A parity operation (along the x-axis) maps ones into the others, preserving the angles.
The optimal solution arises from the adjustment of the angle θbetween the two eigenvectors (in this
one-dimensional case). As σηincreases, θalso increases, due to the joint modulation of LtandKt
withση(Fig. 6a). This increase in θallows the system to filter internal fluctuations more effectively
and better generalize to other levels of ση(Fig. 6b).
Figure 6: Intertwined modulation of control and filter gains to deal with the internal noise .(a)Angles
θtbetween the two eigenvectors of the matrix Mt(Eq. 12 and see next Paragraph), at different levels
of internal noise σoptim
η , for TOD and GD algorithms. (b)"Adaptability" of the two solutions; the
solution found by the GD algorithm (right panel) generalizes better than the one by TOD (left panel)
when optimized for a certain level of internal noise, σoptim
η , and tested on another one, σtest
η: for
larger σoptim
η , the generalization property improves thanks to due modulation of θt.
By examining the modulation of ⃗ w2asσηchanges in the (Γt−Ωt)plane, we can offer a heuristic
interpretation of the different solutions found by the TOD and GD algorithms. As σηincreases,
19the angle between ⃗ w1and⃗ w2grows for both algorithms. However, this modulation is much more
pronounced in the GD solution (Fig. 6a).
Furthermore, if only additive noise were considered, there would be no modulation of θtwithση
in the TOD solution (for confirmation, see Appendix A.8.2: without multiplicative noise, TOD’s
derivation does not modulate the control gains with ση).
The joint modulation of LtandKtcauses ⃗ w2to move closer to the y-axis in the GD solution (Fig. 7,
green line). This configuration results in more effective filtering of internal fluctuations, decoupling
them from the latent state dynamics, since these fluctuations occur on Ωt(see Eq. 49). This result
aligns with the observed decrease of Ltasσηincreases (see Figs. 2 and 3), where lowering the
control gain moves ⃗ w2closer to the y-axis. Thus, this eigenvector analysis qualitatively explains the
trends observed in Fig. 5 for ⟨Γ⟩as a function of σηin both the TOD and GD solutions.
Figure 7: Eigenvector decomposition of the dynamics . We show here a qualitative representation of
the eigenvectors of the matrix Mtin the plane (Γt,Ωt). The black arrow represents the "shared"
eigenvector ⃗ w1, while the blue (green) arrow represents ⃗ w2for TOD (GD) solution. Note that the
optimal Ltare negative, while the optimal Ktare positive (Fig. 2).
A.5.3 One-Dimensional Case: Improving Performance Without Internal Noise
We briefly show that, even in the absence of internal noise, if the algorithm has not yet converged,
the solution proposed by [ 1] does not yield the optimal control law. We demonstrate this in a one-
dimensional example, using the same parameters shown in Appendix A.5.1 (but the result is valid in
general), while only varying the scaling matrix for the multiplicative sensory noise Dand keeping
ση= 0. We fix the filter gains at the suboptimal constant value K1,···,T−2=A= 1.0, and optimize
the vector L1,···,T−1using TOD and GD algorithms.
Figure 8: Enhanced performance when optimizing control at fixed filter gains and zero internal noise .
We plot the expected accumulated cost E[J], computed by averaging the quantity from Eq. 3 over
50ktrials, as a function of the scaling matrix D, with error bars (mean ±1SEM from Monte Carlo
simulations, error bars not visible as too small), for the two algorithms TOD and GD.
We find that TOD control law leads to a higher expected accumulated cost E[J](Fig. 8). The
improved performance of the GD (and FPOMP - in Section 3.2 we show that the solutions of the two
match) algorithm arises from not assuming unbiasedness when optimizing control. As a result, the
20algorithm adjusts the control gains to account for the bias introduced by the suboptimal estimator.
Similar to the case with internal noise, the control gains found by the GD algorithms in this scenario
are typically smaller than those found by the algorithm of [ 1]. A similar behavior occurs when
optimizing the filter gains while keeping the control gains fixed and suboptimal. The difference in
performance is due to the fact that the derivation of the optimal estimator in [ 1] is only valid when
the control gains are optimal. Thus, the algorithm in [ 1] does not apply when the suboptimality of the
controller needs to be ‘balanced’ by the estimator. When the algorithm in [ 1] is run in its entirety,
optimizing both control and filter gains iteratively, these issues are resolved. Upon convergence, the
controller and estimator are optimally adjusted to each other, in the absence of internal noise.
A.5.4 Multi-Dimensional Case: Parameters
For the sensorimotor task described in Section 3.2, the discrete-time dynamics is the same as in [1],
p(t+ ∆t) =p(t) + ˙p(t)∆t (55)
˙p(t+ ∆t) = ˙p(t) +f(t)∆t/m (56)
f(t+ ∆t) =f(t)(1−∆t/τ2) +g(t)∆t/τ2 (57)
g(t+ ∆t) =g(t)(1−∆t/τ1) +u(t)(1 + σεεt)∆t/τ1 (58)
We have therefore the following system parameters (with c=d= 1)
A=
1 ∆ t 0 0
0 1 ∆ t/m 0
0 0 1 −∆t/τ2 ∆t/τ2
0 0 0 1 −∆t/τ1
 (59)
B= (0 0 0 ∆ t/τ1)⊺(60)
C= (0 0 0 σε∆t/τ1)⊺(61)
H=
1 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
 (62)
D=
σρ0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
 (63)
Q1,···,T−1=
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
 (64)
QT=⃗ p⃗ p⊺+⃗ v⃗ v⊺+⃗f⃗f⊺(65)
R1,···,T−1=r
T−1(66)
RT= 0 (67)
⃗ p= (1 0 0 0 ) (68)
⃗ v= (0wv0 0) (69)
⃗f= (0 0 wv0) (70)
Ωξ=
0 0 0 0
0σ2
ξ0 0
0 0 0 0
0 0 0 0
 (71)
Ωω=σ2
ω (72)
Ωη=
σ2
η0 0 0
0σ2
ηv0 0
0 0 σ2
ηf0
0 0 0 σ2
ηc
(73)
21with the initial conditions given by
E[x1] = (z0 0 0 )⊺(74)
E[ˆx1] =E[x1] (75)
Σx1=
σ2
z0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
 (76)
Σˆx1=
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
. (77)
The parameters of the problem are listed in Table 3 (std = standard deviation).
Table 3: Parameters of the sensorimotor task
Name Description value
∆t time-step ( s) 0.010
m mass of the hand ( Kg), modelled as a point mass 1
τ1 time constant of the second order low pass filter 0.04
τ2 time constant of the second order low pass filter 0.04
r Control-dependent cost at each t < T 1e−5
wv Task-related cost at time t=Tfor the velocity 0.2
wf Task-related cost at time t=Tfor the force 0.01
T time steps 100
z Target position 0.15
σz Target position standard deviation 0.0
σξ std of the additive Gaussian noise ξt 0.0
σω std of the additive Gaussian noise ωt 0.0
σε std of the control-dependent noise εt 0.5
σρ std of the signal-dependent noise ρ 0.5
ση std of the additive internal noise ηtfor the position estimate {0.0,0.005,0.05,0.5}
σηv std of the additive internal noise ηtacting on the velocity estimate 0
σηf std of the additive internal noise ηtfor the force estimate 0
σηg std of the additive internal noise ηtfor the estimate of g 0
Note that the initial condition for the state x1is the actual target position: in such a way the control
signal utaims at minimizing the distance from xt= 0.
A.5.5 Multi-Dimensional Case: Impact of Internal Noise
We provide here an extended analysis of the impact of internal noise in the sensorimotor task discussed
in Section 3.2, offering additional insights. We compute the posterior variance of xt(state belief
variance) to assess how internal noise affects it. For ση= 0,0.005,0.05,0.5att= 80 (an arbitrarily
chosen time), we find σ2[xt] = 2·10−5,5·10−5,8·10−5,2·10−4, respectively. In the case of
ση= 0.05, where internal noise accounts for about 10%, there is a significant increase in state
uncertainty compared to the scenario without internal noise. Indeed, at t= 80 , since both the state xt
and the state estimate ˆxtare near zero (with the target position as the reference point in our coordinate
system), internal noise becomes the dominant source of fluctuations, as we are only accounting for
multiplicative noise in this task.
A.5.6 Scaling to Higher-Dimensional Problems: An Application
We demonstrate how our algorithm scales to high-dimensional problems, building on the discussion in
the final paragraph of Section 3.2. We implement a high-dimensional task to show the generalizability
of the GD algorithm. The same results would apply to its analytical counterpart, the FPOMP
algorithm, as discussed in Section 3.3, and Appendices A.8.1, A.8.2. In this scenario, we set the
22dimensions of the state, control, and observation to m= 10 ,p= 4, and k= 10 , respectively. Note
that this significantly increases the dimensionality compared to the problem in Section 3.2 (for the
multi-dimensional case).
Figure 9: High-dimensional task .(a)Expected accumulated cost as a function of σηfor TOD (blue
dots) and GD (green dots) algorithms. We see that even in this high-dimensional task, GD solutions
outperform the ones from [ 1]. To compute the expected cost, we used Algorithm 1 (but the results
are confirmed by Monte Carlo simulations). (b)Pseudo-determinant of the control gains L(averaged
over time), denoted as |L|pas a function of σηfor TOD (blue dots) and GD (green dots) algorithms.
The system matrices A,B, and Dare random matrices with elements drawn from a standard normal
distribution (mean zero, standard deviation one), while Cis defined as C=σεB. The matrix His
the identity matrix, and the time horizon is set to T= 10 . All elements of the state and state estimate
vectors are initialized to one. We used σξ=σω=σρ=σε= 0.5and varied σηacross values of 0.0,
0.5, and 1.0. The matrices defining the quadratic cost functions, QandR, are identity matrices at
each time step. All the findings from Section 3.2 are confirmed in this high-dimensional setting (Fig.
9). The GD algorithm continues to outperform the solutions in [ 1], with performance improving as
internal noise increases, and the control gain magnitude decreases as internal fluctuations grow. In
fact, as internal noise increases, the optimal strategy involves reducing control over the system. To
quantify control magnitude, we compute the pseudo-determinant of L1,···,T−1and average it over
time. The pseudo-determinant, a generalization of the determinant for non-square matrices, provides
a measure of the volume scaling induced by the control gains.
A.6 FPOMP Algorithm: An Analytical Counterpart to the Numerical GD Algorithm
A.6.1 One-Dimensional Case
In the one-dimensional case we have m=p=k= 1. Additionally, to simplify the notation, we set
c=d= 1. We start by defining
⃗Ft= Ft,1
Ft,2
Ft,3!
=
A2
(B2+C2)L2
t
2ABL t
 (78)
⃗Gt= Gt,1
Gt,2
Gt,3!
=
K2
t(H2+D2)
(A+BLt)2+K2
tH2−2AKtH−2BLtKtH
2BLtKtH+ 2AKtH−2K2
tH2
 (79)
⃗Ht= Ht,1
Ht,2
Ht,3!
=
AKtH
ABL t+B2L2
t−BLtKtH
A2+ABL t−AKtH+BLtKtH
. (80)
In one dimension, we can then propagate the non-central moments as
E[x2
t+1] =Ft,1E[x2
t] +Ft,2E[ˆx2
t] +Ft,3E[xtˆxt] + Ω ξ (81)
E[ˆx2
t+1] =Gt,1E[x2
t] +Gt,2E[ˆx2
t] +Gt,3E[xtˆxt] +K2
tΩω+ Ωη (82)
E[xt+1ˆxt+1] =Ht,1E[x2
t] +Ht,2E[ˆx2
t] +Ht,3E[xtˆxt]. (83)
23The derivatives of the non-central moments with respect to LtandKtobey the following equations,
fori= 1, ..., T ,
∂E[x2
t+i]
∂Lt=at+i−1,1Lt+bt+i−1,1 (84)
∂E[ˆx2
t+i]
∂Lt=at+i−1,2Lt+bt+i−1,2 (85)
∂E[xt+iˆxt+i]
∂Lt=at+i−1,3Lt+bt+i−1,3 (86)
and
∂E[x2
t+i]
∂Kt=αt+i−1,1Kt+βt+i−1,1 (87)
∂E[ˆx2
t+i]
∂Kt=αt+i−1,2Kt+βt+i−1,2 (88)
∂E[xt+iˆxt+i]
∂Kt=αt+i−1,3Kt+βt+i−1,3, (89)
with⃗ a,⃗b,⃗ αand⃗βgiven by the following recursive equations
⃗ at+1= at+1,1
at+1,2
at+1,3!
=
⃗Ft+1·⃗ at
⃗Gt+1·⃗ at
⃗Ht+1·⃗ at
 (90)
⃗bt+1= bt+1,1
bt+1,2
bt+1,3!
=
⃗Ft+1·⃗bt
⃗Gt+1·⃗bt
⃗Ht+1·⃗bt
 (91)
⃗ αt+1= αt+1,1
αt+1,2
αt+1,3!
=
⃗Ft+1·⃗ αt
⃗Gt+1·⃗ αt
⃗Ht+1·⃗ αt
 (92)
⃗βt+1= βt+1,1
βt+1,2
βt+1,3!
=
⃗Ft+1·⃗βt
⃗Gt+1·⃗βt
⃗Ht+1·⃗βt
. (93)
The initial conditions for Eqs. 90-93 are
⃗ at=
2(B2+C2)E[ˆx2
t]
2B2E[ˆx2
t]
2B2E[ˆx2
t]
 (94)
⃗bt=
2ABE[xtˆxt]
2ABE[ˆx2
t]−2BKtH(E[ˆx2
t]−E[xtˆxt])
AB(E[ˆx2
t] +E[xtˆxt])−BKtH(E[ˆx2
t]−E[xtˆxt])
 (95)
⃗ αt=
0
2H2(E[x2
t] +E[ˆx2
t]−2E[xtˆxt]) + 2Ω ω+ 2D2E[x2
t]
0
 (96)
⃗βt=
0
−2H(A+BLt)(E[ˆx2
t]−E[xtˆxt])
AH(E[x2
t]−E[xtˆxt])−BLtH(E[ˆx2
t]−E[xtˆxt])
. (97)
By observing that the expected accumulated cost, Eq. 9 (adapted to the one-dimensional case), will
be a function of E[x2
t]andE[ˆx2
t], fort= 1, ..., T −t, and by using Eqs. 84-89, we can rewrite Eqs.
2417-18 as
∂
∂LtT−tX
i=0E[jt+i|µt,Σt] = 2RtE[ˆx2
t]Lt+
+T−tX
i=1[(Qt+iat+i−1,1+Rt+iL2
t+iat+i−1,2)Lt+
+ (Qt+ibt+i−1,1+Rt+iL2
t+ibt+i−1,2)] = 0(98)
and
∂
∂KtT−tX
i=0E[jt+i|µt,Σt] =T−tX
i=1[(Qt+iαt+i−1,1+Rt+iL2
t+iαt+i−1,2)Kt+
+ (Qt+iβt+i−1,1+Rt+iL2
t+iβt+i−1,2)] = 0 .(99)
Therefore, from Eqs. 98-99, we have the following instantiations of Eqs. 19-20 for the optimal
control and filter gains at time t,L∗
tandK∗
t,
L∗
t=−Lnum
t
Lden
t(100)
K∗
t=−Knum
t
Kden
t, (101)
with
Lnum
t=T−tX
i=1 
Qt+ibt+i−1,1+Rt+iL2
t+ibt+i−1,2
, (102)
Lden
t= 2RtE[ˆx2
t]+
+T−tX
i=1(Qt+iat+i−1,1+Rt+iL2
t+iat+i−1,2)(103)
and
Knum
t=T−tX
i=1 
Qt+iβt+i−1,1+Rt+iL2
t+iβt+i−1,2
, (104)
Kden
t=T−tX
i=1 
Qt+iαt+i−1,1+Rt+iL2
t+iαt+i−1,2
. (105)
We can then use Eqs. 100-101, to implement Algorithm 2 and extract L∗
1,···,T−1, and K∗
1,···,T−2, for
the one-dimensional problem.
A.6.2 Multi-Dimensional Case
For the multi-dimensional case, we derive Eqs. 19-20 for the classic LQAG problem ( Ci= 0for
i= 1, .., c andDi= 0fori= 1, .., d ) in the presence of internal noise ( Ωη≥0).
As a title of example, we derive here Eq. 19 for the optimal L∗
t(to be used in Algorithm 2), but
the approach would be the same for the optimal filter gains K∗
t. The extension to the more general
scenario including the multiplicative sources of noise would follow a similar method. As outlined in
Section 3.1, Eq. 9, the expected cost per step is given by
E[jt+i] =E[xt+i]⊺Qt+iE[xt+i] +E[ˆxt+i]⊺L⊺
tRtLt+iE[ˆxt+i]+
+Tr[Qt+iΣxt+i] +Tr[L⊺
t+iRt+iLt+iΣˆxt+i],(106)
fori= 0, ..., T −t.
When computing E[jt+i|µt,Σt]to write down Eq. 17 (with Ci= 0,i= 1, ..., c andDi= 0,
i= 1, ..., d ), and derive Eq. 19, the coefficients multiplying E[ˆxt]E[ˆxt]⊺coming from the term
25E[xt+i]⊺Qt+iE[xt+i]in Eq. 106 will be the same as the ones multiplying Σˆxtand coming from the
termTr[Qt+iΣxt+i]. The same holds for the coefficients multiplying respectively E[xt]E[ˆxt]⊺and
Σxt,ˆxt.
Similarly, we can group together the coefficients coming from the other two factors
E[ˆxt+i]⊺L⊺
tRtLt+iE[ˆxt+i]andTr[L⊺
t+iRt+iL⊺
t+iΣˆxt+i]in Eq. 106.
We now note that the terms dependent on Ltappearing in E[jt+i|µt,Σt]will show a dependence
on the afore-mentioned moments E[ˆxt]E[ˆxt]⊺,Σˆxt,E[xt]E[ˆxt]⊺andΣxt,ˆxt. More specifically, the
quadratic factors in Ltwill only depend on E[ˆxt]E[ˆxt]⊺andΣˆxt. Taken together, these observations
lead to the following form for Eq. 17,
JtL∗
tE[ˆxtˆx⊺
t] +StE[xtˆx⊺
t] +PtE[ˆxtˆx⊺
t] = 0, (107)
where we have used Σˆxt+E[ˆxt]E[ˆxt]⊺=E[ˆxtˆx⊺
t]andΣxt,ˆxt+E[xt]E[ˆxt]⊺=E[xtˆx⊺
t].
Therefore, to find the optimal control gains L∗
tfrom Eq. 107, we only need to compute the coefficients
Jt,StandPt, similar to what we have done for the one-dimensional case in Appendix A.6.1. As
before, we can compute the coefficients Jt,StandPtby only looking at the first two terms appearing
in Eq. 106, that is E[xt+i]⊺Qt+iE[xt+i]andE[ˆxt+i]⊺L⊺
tRtLt+iE[ˆxt+i]. By using ([44])
∂⃗ v⊺X ⃗ w
∂X=⃗ v ⃗ w⊺, (108)
∂⃗ v⊺X⊺⃗ w
∂X=⃗ w⃗ v⊺, (109)
∂
∂X(⃗ v⊺X⊺NX⃗ v ) = 2 NX⃗ v⃗ v⊺, (110)
where ⃗ vand⃗ ware vectors and Nis a symmetric matrix, we obtain
Jt= 2Rt+ 2T−tX
i=1
V⊺
t+i−1(Qt+i+L⊺
t+iRt+iLt+i)Vt+i−1
(111)
St= 2T−tX
i=1n
V⊺
t+i−1h
Qt+i
µt+i
Lt=0,(I,0)
1+L⊺
t+iRt+iLt+i
µt+i
Lt=0,(I,0)
2io
(112)
Pt= 2T−tX
i=1n
V⊺
t+i−1h
Qt+i
µt+i
Lt=0,(0,I)
1+L⊺
t+iRt+iLt+i
µt+i
Lt=0,(0,I)
2io
(113)
withVt+igiven by
Vt+i=iY
j=1(A+BLt+j)B (114)
fori= 1, ..., T −t, and
Vt=B . (115)
In Eqs. 112-113,
µt+i
Lt=0,(·,·)
, is a vector whose elements are m×mmatrices:
µt+i
Lt=0,(·,·)=

µt+i
Lt=0,(·,·)
1 
µt+i
Lt=0,(·,·)
2
 (116)
The subscript (·,·)indicates the initial condition ( i= 0) for the evolution of µt+i
Lt=0,(·,·), with I
denoting the m×midentity matrix and 0being an m×mmatrix whose elements are all zeros, e.g.,
µt
Lt=0,(I,0)=
I
0
. (117)
µt+i
Lt=0,(·,·)is updated through the following equations
µt+i
Lt=0,(·,·)=(˜Mtµt
Lt=0,(·,·), fori= 1
Mt+i−1µt+i−1
Lt=0,(·,·),fori= 2, ..., T −t(118)
26withMtgiven by Eq. 12 and ˜Mthaving the same form as the block matrix Mt, but with Lt= 0,
˜Mt=
A 0
KtH A −KtH
. (119)
From Eq. 107 we can then write for Eq. 19
L∗
t=−J−1
t(StE[xtˆx⊺
t] +PtE[ˆxtˆx⊺
t])E[ˆxtˆx⊺
t]†(120)
where ·†denotes the pseudoinverse operation. Note that Jtis a symmetric p×pmatrix with
det[Jt]>0and therefore invertible. Due to the initial conditions for Σˆx1andE[ˆx1], the symmetric
matrix E[ˆxtˆx⊺
t]could have a null determinant: for this reason we use the pseudoinverse operation.
This consideration is relevant only for an initial transient: after a certain time ˜t >0(depending on
the dynamics parameters) we would have det[E[ˆxtˆx⊺
t]]>0andE[ˆxtˆx⊺
t]†=E[ˆxtˆx⊺
t]−1, due to the
properties of the pseudoinverse. With Eq. 120 we can implement Algorithm 2 to find the optimal
control gains L∗
1,···,T−1. Notably, from the form of Eq. 120, we can see why, mathematically, the
control gains decrease when the internal noise level is increased: the factor E[ˆxtˆx⊺
t]will get bigger
and bigger as Ωηgets larger.
The derivation of Eq. 20 for the optimal filter gains K∗
twould follow the same procedure. To
extend the presented approach to the case with multiplicative noise, we need to propagate the terms
depending on Ci,i= 1, ..., c andDi,i= 1, ..., d coming from the factors Tr[Qt+iΣxt+i]and
Tr[L⊺
t+iRt+iLt+iΣˆxt+i]in Eq. 106, similarly to what we have done with the other terms in Eq. 118,
but using ([44])
∂
∂XTr[˜AX˜B] =˜A⊺˜B⊺, (121)
∂
∂XTr[˜AX⊺˜B] =˜B˜A, (122)
∂
∂XTr[˜AX˜BX⊺˜C] =˜A⊺˜C⊺X˜B⊺+˜C˜AX˜B (123)
where ˜A,˜Band˜Care matrices. We observe that, even when considering multiplicative noise, Eq.
120 will still be valid: only the matrices Jt,StandPtwill change, including now also the terms
depending on Ci,i= 1, ..., c , and Di,i= 1, ..., d .
A.6.3 Pseudo-Code
Algorithm 2 (Section 3.3) extracts the optimal solutions analytically by identifying the critical points
of the total expected cost, conditioned on the first two moments of the joint variable distribution
(xt,ˆxt). In the pseudo-code, L(k)
tandK(k)
tstand for, respectively, the control and filter gains at time
tand at optimization step k. The hyper-parameters of the used algorithms are listed in Table 1 in
Appendix A.5.
Algorithm 2 FPOMP algorithm
Input: µ1,Σ1(initial conditions of the system), L(1)
1,···,T−1,K(1)
1,···,T−2(initial guesses for L∗
andK∗), and the system parameters ( A,B,H,Ci=1,...,c,Di=1,...,d,Ωξ,Ωω,Ωη).
2:Output: L∗
1,···,T−1,K∗
1,···,T−2(optimal control and filter gains)
Algorithm steps:
4:foreach iteration k= 2, . . . , optimization steps do
µ1,···,T−1,Σ1,···,T−1←Eqs. 14-15 using L(k−1)
1,···,T−1andK(k−1)
1,···,T−2
6: foreach iteration i= 1, . . . , T −1do
t←T−i
8: L(k)
t←f(µt,Σt, L(k)
t+1,···,T−1, K(k−1)
t+1,···,T−2)
K(k)
t←g(µt,Σt, L(k−1)
t+1,···,T−1, K(k)
t+1,···,T−2)
10: end for
end for
12:L∗
1,···,T−1←L(k)
1,···,T−1
K∗
1,···,T−2←K(k)
1,···,T−2
27A.7 Extension to Switching Linear Dynamics
We discuss here how to extend our approach to switching linear dynamics. One of the underlying
assumptions in this work and in [ 1] is that the agent has complete knowledge of the updating rules
of the latent dynamical system. By using the same set of matrices to update the state and the state
estimate, we implicitly assume that all uncertainty in the estimation process arises solely from
noise sources: the problem of inferring the matrices AandBgoes beyond the objectives of this
approach. For this reason, to extend our work to the more general and realistic case of Switching
Linear Dynamics (SLD), we can consider a matrix Adepending on the time step t,At. A complete
formulation of SLD might require adding another variable, a discrete switch variable stregulating
the way the matrices Atvary with time and context [ 45]. Given that in our case the agent has access
to the updating rules of the dynamical system, we can omit st(the agent does not have to infer st
andAt) and directly consider the case in which we have a predetermined set of matrices A1,···,T−1.
The same applies to the matrix B, that can be replaced by B1,···,T−1. Note that to preserve linearity
we assume AtandBtto be independent on xandˆx. We consider here the multidimensional case to
be as general as possible. To extend the GD algorithm we only need to modify the block matrix Mt
that we use to update the moments Σt, µtand eventually propagate the expected cost E[J]through
Eq. 3. Indeed, once we can compute the expected cost at fixed control and filter gains, L1,···,T−1,
andK1,···,T−2, we can use Algorithm 1 to define the objective function to be minimized through
gradient descent with respect to LtandKt. To update the block matrix Mtwe have to substitute A
andBrespectively with AtandBtin Eq. 12. To handle the potentially high computational costs
of performing a numerical gradient descent, we introduced the analytical counterpart of the GD
algorithm, the FPOMP algorithm. For the one-dimensional case, it supports all the noise sources
mentioned in Section 2 (additive, multiplicative and internal). We extended this algorithm to the
multi-dimensional case for additive and internal noise for the sake of simplicity, leaving the more
general version for future work (Appendix A.6.2 outlines how this can be done). Here, we extend
the afore-mentioned FPOMP algorithm (for both one-dimensional and multi-dimensional cases) to
switching linear dynamics, following a similar procedure to that of the numerical algorithm. For the
one-dimensional case, we replace AandBrespectively with AtandBtin Eqs. 78-80, and 94-97.
For the multi-dimensional case we have to substitute AwithAt+jandBwithBt+jin Eq. 114 and
BwithBtin Eq. 115. Finally, as previously done, we replace AwithAtin Eq. 119 for ˜Mt. With
these changes, we can implement Algorithm 2 for the case with switching linear dynamics.
We observe that the extension to switching linear dynamics aims to make the assumption of linearity
less restrictive. Additionally, given the flexibility of our approach in handling high-dimensional
systems (Section 3.2), it is reasonable to think that this assumption does not limit the effective
description of lower-dimensional nonlinear dynamics, potentially by employing the Koopman operator
[46].
A.8 Experiments: FPOMP Algorithm
A.8.1 One-Dimensional Case
We show that the FPOMP and GD algorithms yield the same performance in the one-dimensional
problem introduced in Section 3.2, confirming their equivalence as discussed in Section 3.2.
Figure 10: Accumulated cost difference . Difference of E[J]for GD and FPOMP solutions (computed
by averaging the quantity from Eq. 3 over 50ktrials), as a function of ση, with error bars (SEM).
28A.8.2 Multi-Dimensional Case
We consider the same problem described in Section 3.2 (see also Appendix A.5.4) but without
multiplicative noise sources, to validate the FPOMP algorithm derived in Appendix A.6.2. The
parameters of the problem are listed in Table 4.
Table 4: Parameters of the problem - sensorimotor task without multiplicative noise
Name Description value
∆t time-step ( s) 0.010
m mass of the hand ( Kg), modelled as a point mass 1
τ1 time constant of the second order low pass filter 0.04
τ2 time constant of the second order low pass filter 0.04
r Control-dependent cost at each t < T 1e−5
wv Task-related cost at time t=Tfor the velocity 0.2
wf Task-related cost at time t=Tfor the force 0.01
T time steps 50
z Target position 0.15
σz Target position standard deviation 0.0
σξ std of the additive Gaussian noise ξt 0.5
σω std of the additive Gaussian noise ωt 0.5
σε std of the control-dependent noise εt 0.0
σρ std of the signal-dependent noise ρt 0.0
ση std of the additive internal noise ηtfor the position estimate {0.0,1.0,2.0}
σηv std of the additive internal noise ηtacting on the velocity estimate 0
σηf std of the additive internal noise ηtfor the force estimate 0
σηg std of the additive internal noise ηtfor the estimate of g 0
Figure 11: Application of the FPOMP algorithm .(a)E[J], computed by averaging the quantity
from Eq. 3 over 50ktrials, as a function of ση, with error bars (mean ±1SEM from Monte Carlo
simulations, error bars not visible as too small) and for TOD, GD and FPOMP (at fixed filters given
by TOD solution). (b)Magnitude of the control gain vector as a function of time for TOD and
FPOMP (at fixed filters given by TOD). (c)Same as (a), but comparing GD and FPOMP (now at
fixed filters given by GD). (d)First component of the vector Ltfor the solution given by GD and
FPOMP (now at fixed filters given by GD solution).
29We recall that, for readability, we derived solutions only for the optimal controller; the procedure
for the optimal estimator follows similarly but was not explicitly derived. Extensions to the optimal
estimator and the general case with multiplicative noise are left for future work, as discussed in
Appendix A.6.2.
Optimizing the control gains Ltusing the FPOMP algorithm (with fixed filter gains Ktfrom the TOD
solution) leads to improved performance (Fig. 11a, orange dashed line) as internal noise increases.
However, this solution is not fully optimal, as the estimator is still optimized using the TOD algorithm.
When LtandKtare both optimized with the GD method, a lower accumulated cost is achieved
(green dashed line). An interesting feature of our algorithm is that, being fully analytical, it can
enhance numerical solutions. Due to a potentially shallow parameter landscape (vanishing gradient)
or limited computation time, the GD optimization may stop near the global optimum without fully
reaching it. We find that re-optimizing the control gains Ltusing the FPOMP algorithm after the GD
solution for filter gains yields a small but significant performance boost (Fig. 11c), with minimal
changes in the final Ltvector (Fig. 11d). This also confirms that our algorithm finds the optimal
solutions. Extensions to estimator optimization and the multiplicative noise case are discussed in
Appendix A.6.2.
The qualitative trends observed in the sensorimotor task (Section 3.2, Fig. 3) are confirmed: control
magnitude decreases as internal noise increases (Fig. 11b). Additionally, while the TOD solution
does not modulate control with respect to internal noise when only additive noise is present, the
FPOMP algorithm introduces such modulation, leading to a lower accumulated cost (Figs. 11a,b),
consistent with our discussion in Appendix A.5.2.
30NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer:[Yes]
Justification: The claims made in the abstract and introduction are accurately confirmed by
the results provided in the results and commented in the conclusion.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations of the work are discussed in the conclusion.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
31Justification: The assumptions and mathematical proofs are explained in the paper and
presented in more details in the Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: All the information is provided in Section 3 and the Appendix, including
pseudo-codes and detailed equations for the implementation of the proposed algorithms.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
32Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The codes to generate the data and the figures are provided in the supplemental
material.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The details about the implementation of the algorithms and the parameters of
the experiments are discussed in the Appendix, in Section A.5.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We present error bars representing the standard error of the mean calculated
across multiple trials.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
33•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: The derived algorithms can be efficiently performed on a standard commercial
CPU.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We present a theoretical study aimed at improving current algorithms for
solving optimal control problems. Our research is entirely computational and does not
involve human or animal subjects, thus raising no ethical concerns.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Since our study is theoretical, it does not have any clear societal impacts.
Guidelines:
34• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The theory presented in this paper carries no potential for misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper relies solely on original codes and data, without utilizing any
existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
35•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not introduce new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not make any use of human data.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The research presented in this paper does not include any studies involving
human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
36•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
37