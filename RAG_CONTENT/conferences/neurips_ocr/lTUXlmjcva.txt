From Alexnet to Transformers:
Measuring the Non-linearity of Deep Neural Networks
with Affine Optimal Transport
Anonymous Author(s)
Affiliation
Address
email
Abstract
In the last decade, we have witnessed the introduction of several novel deep 1
neural network (DNN) architectures exhibiting ever-increasing performance across 2
diverse tasks. Explaining the upward trend of their performance, however, remains 3
difficult as different DNN architectures of comparable depth and width – common 4
factors associated with their expressive power – may exhibit a drastically different 5
performance even when trained on the same dataset. In this paper, we introduce 6
the concept of the non-linearity signature of DNN, the first theoretically sound 7
solution for approximately measuring the non-linearity of deep neural networks. 8
Built upon a score derived from closed-form optimal transport mappings, this 9
signature provides a better understanding of the inner workings of a wide range 10
of DNN architectures and learning paradigms, with a particular emphasis on the 11
computer vision task. We provide extensive experimental results that highlight the 12
practical usefulness of the proposed non-linearity signature and its potential for 13
long-reaching implications. 14
1 Introduction 15
Deep neural networks (DNNs) are undoubtedly the most powerful AI models currently available 16
[1,2,3,4,5]. Their performance on many tasks, including natural language processing (NLP) [ 6] 17
and computer vision [ 7], is already on par or exceeds that of a human being. One of the reasons 18
explaining such progress is of course the increasing computational resources [ 8,9]. Another one is 19
the endeavour for finding ever more efficient neural architectures pursued by researchers over the 20
last decade. As of today, the transformer architecture [ 10] has firmly imposed itself as a number 21
one choice for most, if not all, of the recent breakthroughs [ 11,12,13] in the machine learning and 22
artificial intelligence fields. 23
24
Limitations But why transformers are more capable than other architectures? Answering this 25
question requires finding a meaningful measure to compare the different famous models over 26
time gauging the trend of their intrinsic capacity. For such a comparison to be informative, it is 27
particularly appropriate to consider the computer vision field that produced many of the landmark 28
neural architectures improving upon each other over the years. Indeed, the decade-long revival of 29
deep learning started with Alexnet’s [ 14] architecture, the winner of the ImageNet Large Scale Visual 30
Recognition Challenge [ 15] in 2012. By achieving a significant improvement over the traditional 31
approaches, Alexnet was the first truly deep neural network to be trained on a dataset of such 32
scale, suggesting that deeper models were likely to bring even more gains. In the following years, 33
researchers proposed novel ways to train deeper models with hundreds of layers [ 16,17,18,19] 34
pushing the performance frontier even further. The AI research landscape then reached a turning 35
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.point with the proposal of transformers [ 10], starting their unprecedented dominance first in NLP and 36
then in computer vision [ 20]. Surprisingly, transformers are not particularly deep, and the size of 37
their landmark vision architecture is comparable to that of Alexnet, and this despite a significant 38
performance gap between the two. Ultimately, this gap should be explained by the differences in the 39
expressive power [ 21] of the two models: a term used to denote the ability of a DNN to approximate 40
functions of a certain complexity. Unfortunately, the existing theoretical results related to this either 41
associate higher expressive power with depth [ 22,23,24] or width [ 25,26,27,28] falling short in 42
comparing different families of architectures. This, in turn, limits our ability to understand what 43
underpins the achieved progress and what challenges and limitations still exist in the field, guiding 44
future research efforts. 45
46
Contributions We argue that quantifying the non-linearity of a DNN may be what we were missing 47
so far to understand the evolution of the deep learning models at a more fine-grained level. To verify 48
this hypothesis in practice, we put forward the following contributions: 49
1.We propose a first theoretically sound measure, called the affinity score, that estimates the 50
non-linearity of a given (activation) function using optimal transport (OT) theory. We use 51
the proposed affinity score to introduce the concept of the non-linearity signature of DNNs 52
defined as a set of affinity scores of all its activation functions. 53
2.We compare non-linearity signatures of a wide range of popular DNNs used in computer 54
vision: from Alexnet to vision transformers (ViT) and their more recent variations. Through 55
this, we clearly illustrate the disruptive patterns in the evolution of the deep learning field. 56
3.We demonstrate that non-linearity signature can be predictive of DNNs performance and 57
used to meaningfully identify the family of approaches to which a given DNN belongs. We 58
further show that the non-linearity signature is unique as it doesn’t correlate strongly with 59
other potential candidates used for this task. 60
The rest of the paper is organized as follows. We start by presenting the relevant background 61
knowledge on OT in Section 2. Then, we introduce the affinity score together with its different 62
theoretical properties in Section 3. Section 4 presents experimental evaluations on a wide range of 63
popular convolutional neural networks. Finally, we conclude in Section 5. 64
2 Background 65
Optimal Transport Let(X, d)be a metric space equipped with a lower semi-continuous cost 66
function c:X×X→R≥0, e.g the Euclidean distance c(x, y) =∥x−y∥. Then, the Kantorovich 67
formulation of the OT problem between two probability measures µ, ν∈ P(X)is given by 68
OTc(µ, ν) = min
γ∈ADM( µ,ν)Eγ[c], (1)
where ADM( µ, ν)is the set of joint probabilities with marginals µandν, andEν[f]denotes the 69
expected value of funder ν. The optimal γminimizing equation 1 is called the OT plan . Denote by 70
L(X)the law of a random variable X. Then, the OT problem extends to random variables X, Y and 71
we write OTc(X, Y)meaning OTc(L(X),L(Y)). 72
Assuming that either of the considered measures is absolutely continuous , then the Kantorovich 73
problem is equivalent to the Monge problem 74
OTc(µ, ν) = min
T:T#µ=νEX∼µ[c(X, T(X))], (2)
where the unique minimizing Tis called the OT map , and T#µdenotes the push-forward measure , 75
which is equivalent to the lawofT(X), where X∼µ. 76
Wasserstein distance LetXbe a random variable over Rdsatisfying E[∥X−x0∥2]<∞for some 77
x0∈Rd, and thus for any x∈Rd. We denote this class of random variables by P2(Rd). Then, the 78
2-Wasserstein distance W2between X, Y∈ P2(Rd)is defined as 79
W2(X, Y) = OT ||x−y||2(X, Y)1
2. (3)
We now proceed to the presentation of our main contribution. 80
23 Non-linearity signature of deep neural networks 81
Among all non-linear operations introduced into DNNs in the last several decades, activation functions 82
remain the only structural piece that they all inevitably share. Without non-linear activation functions, 83
most of DNNs, no matter how deep, reduce to a linear function unable to learn complex patterns. 84
Activation functions were also early identified [ 29,30,31,32] as a key to making even a shallow 85
network capable of approximating any function, however complex it may be, to arbitrary precision. 86
We thus build our study on the following intuition: if activation functions play in important role 87
in making DNNs non-linear, then measuring their degree of non-linearity can provide us with an 88
approximation of the DNN’s non-linearity itself. To implement this intuition in practice, however, we 89
first need to find a way to measure the non-linearity of an activation function. Surprisingly, there is 90
no widely accepted measure for this, neither in the field of mathematics nor in the field of computer 91
science. To fill this gap, we will use the OT theory to develop a so-called affinity score below. 92
3.1 Affinity score 93
Identifiability We consider the pre-activation signal Xof an activation function within a neural 94
network, and the post-activation signal σ(X)denoted by Yas input and output random variables. 95
Our first step to build the affinity score then is to ensure that we can identify when σis linear with 96
respect to (wrt) X(for instance, when an otherwise non-linear activation is locally linear at the 97
support of X). To show that such an identifiability condition can be satisfied with OT, we first recall 98
the following classic result from the literature characterizing the OT maps. 99
Theorem 3.1 ([33]).LetX∈ P2(Rd),T(x) =∇ϕ(x)for a convex function ϕwithT(X)∈ P2(Rd). 100
Then, Tis the unique optimal OT map between µandT#µ. 101
Using this theorem about the uniqueness of OT maps expressed as gradients of convex functions, we 102
can prove the following result (all proofs can be found in the Appendix C): 103
Corollary 3.2. Without loss of generality, let X, Y∈ P2(Rd)be centered, and let Y=σ(X) =TX, 104
where Tis a positive definite linear transformation. Then, Tis the OT map from XtoY. 105
Whenever the activation function σis linear, the solution to the OT problem Texactly reproduces it. 106
Characterization We now seek to understand whether Tcan be characterized more explicitly. For 107
this, we prove the following theorem stating that Tcan be computed in closed-form using the normal 108
approximations of XandY. 109
Theorem 3.3. LetX, Y∈ P2(Rd)be centered and Y=TXfor a positive definite matrix T. Let 110
NX∼ N(µ(X),Σ(X))andNY∼ N(µ(Y),Σ(Y))be their normal approximations where µand 111
Σdenote mean and covariance, respectively. Then, W2(NX, NY) =W2(X, Y)andT=Taff, where 112
Taffis the OT map between NXandNYand can be calculated in closed-form 113
Taff(x) =Ax+b, A = Σ(Y)1
2
Σ(Y)1
2Σ(X)Σ(Y)1
2−1
2Σ(Y)1
2,
b=µ(Y)−Aµ(X).(4)
Upper bound When the activation σis non-linear wrt X, the affine OT mapping Taff(X)will 114
deviate from the true activation outputs Y. One important step toward quantifying this deviation is 115
given by the famous Gelbrich bound, formalized by means of the following theorem: 116
Theorem 3.4 (Gelbrich bound [ 34]).LetX, Y∈ P2(Rd)and let NX, NYbe their normal approxi- 117
mations. Then, W2(NX, NY)≤W2(X, Y). 118
This upper bound provides a first intuition of why OT can be a great tool for measuring non-linearity: 119
the cost of the affine map solving the OT problem on the left-hand side increases when the map 120
becomes non-linear. We now upper bound the difference between W2(NX, NY)andW2(X, Y), two 121
quantities that coincide only when σis linear. 122
Proposition 3.5. LetX, Y∈ P2(Rd)andNX, NYbe their normal approximations. Then, 123
1.|W2(NX, NY)−W2(X, Y)| ≤2 Trh
(Σ(X)Σ(Y))1
2i
√
Tr[Σ( X)]+Tr[Σ( Y)]. 124
3Higher mismatchNon-linearity signature = [ (ReLU 1), (ReLU 2), (ReLU 3), ... , (ReLU n)]Convolution 
+ 
ReLU 3Convolution 
+ 
ReLU 2Convolution 
+ 
ReLU 1KernelOutput
ReLU n
Figure 1: Illustration of how the non-linearity of a given neural network is measured. ( Top) The
non-linearity signature of a DNN is a collection of affinity scores calculated for each activation
function spread across its hidden layers. ( Bottom ) The affinity score is calculated based on 3 main
steps. First, given an input (grey) and an output (red) of an activation function ( left), we estimate
the best affine OT fit Taff(X)(green) transporting the input to the output ( middle-left ). Second, we
measure the mismatch between the two by summing the transportation costs ( middle-right ) to obtain
the Wasserstein distance W2(TaffX, Y). Finally, this distance is normalized with the magnitudes of
variance (arrows in the rightmost plot) of the output data based on its covariance matrix.
2. For Taffas in (4), W2(TaffX, Y)≤p
2 Tr [Σ( Y)]. 125
To have a more informative non-linearity measure, we now need to normalize the non-negative Wasser- 126
stein distance W2(TaffX, Y)to an interpretable interval of [0,1]. The bound given in Proposition 3.5 127
lets us define the following affinity score 128
ρaff(X, σ(X)) = 1 −W2(TaffX, σ(X))p
2 Tr[Σ( σ(X))]. (5)
The proposed affinity score quantifies how far a given activation σis from an affine transformation. 129
It is equal to 1 for any input for which the activation function is linear, and 0 when it is maximally 130
non-linear, i.e., when TaffXandσ(X)are independent random variables. 131
Remark 3.6. One may wonder whether a simpler alternative to the affinity score can be to use, 132
instead of Taff, a mapping TW(x) =Wx defined as a solution of a linear regression problem 133
minW||Y−WX||2
F. Then, one can use the coefficient of determination ( R2score) to measure how 134
wellTWfits the observed data. This approach, however, has two drawbacks. First, following the 135
famous Gauss-Markov theorem, TWis an optimal linear (linear in Y) estimator. On the contrary, Taff 136
is a globally optimal non-linear mapping aligning XandY. Second, R2compares the fit of TWwith 137
that of a mapping outputting µ(Y)for any value of X. This is contrary to ρaffthat compares how 138
wellTafffits the data wrt to the worst possible cost incurred by Taffas quantified in Proposition 3.5. 139
This gives us a bounded score, i.e. ρaff∈[0,1], whereas R2is not lower bounded, i.e. R2∈[−∞,1]. 140
We confirm experimentally in Section 4 that the two coefficients do not correlate consistently across 141
the studied DNNs suggesting that R2is a poor proxy to ρaff. 142
4(A)
 (B)
(C)
Figure 2: (A)Non-linearity of ReLU depends on the range of input values ( red);(B)ReLU, Tanh,
and Sigmoid exhibit different degrees of non-linearity for the same input; (C)Affinity score captures
the increasing non-linearity of polynomials of different degrees.
3.2 Non-linearity signature 143
We now turn our attention to the definition of a non-linearity signature of deep neural networks. We 144
define a neural network Nas a composition of layers Fiwhere each layer Fiis a function taking 145
as input a tensor Xi∈Rhi×wi×ci(for instance, an image of size 224×224×3fori= 1) and 146
outputting a tensor Yi∈Rhi+1×wi+1×ci+1used as an input of the following layer Fi+1. This defines 147
N=FL⊙...⊙Fi...⊙F1=J
k=1,...,LFkwhere ⊙stands for a composition. 148
We now present the definition of a non-linearity signature of a network N. Below, we abuse the 149
compositional structure of Fiand see it as an ordered sequence of functions. 150
Definition 3.1. LetN=J
k=1,...,LFkbe a neural network. Define by Aa finite set of common
activation functions such that A:={σ|σ:Rh×w×c→Rh×w×c}. Letrbe a pooling operation such
thatr:Rh×w×c→Rc. Then, the non-linearity signature of Ngiven an input Xis defined as follows:
ρaff(N; X) = {ρaff(r(Xi), σ(r(Xi))),∀σ∈Fi∩ A, i={1, . . . , L }}.
Non-linearity signature, illustrated in Figure 1, associates to each network Na vector of affinity 151
scores calculated over the inputs and outputs of all activation functions encountered across its layers. 152
153
What makes an activation function non-linear? We now want to understand the mechanism 154
behind achieving a lower or higher non-linearity with a given (activation) function. This will 155
explain what the different values of the affinity scores stand for when defining the non-linearity 156
signature of a DNN. In Figure 2(A), we show how the ReLU function [ 35], defined element-wise as 157
ReLU (x) =max(0, x), achieves its varying degree of non-linearity. Interestingly, this degree depends 158
only on the range of the input values. Second, in Figure 2(B) we also show how the shape of activation 159
functions impacts their non-linearity for a fixed input: surprisingly, piece-wise linear ReLU function 160
is more non-linear than Sigmoid (x) = 1 /(e−x+ 1) [36] orTanh(x) = ( e−x−ex)/(e−x+ex). 161
Similar observations also apply to compare polynomials of varying degrees (Figure 2(C)). We refer 162
the reader to Appendix D for more visualizations of the affinity score of popular activation functions. 163
3.3 Related work 164
Layer-wise similarity analysis of DNNs A line of work that can be distantly related to our main 165
proposal is that of quantifying the similarity of the hidden layers of the DNNs as proposed [37] and 166
[38] (see [ 39] for a complete survey of the subsequent works). [ 37] extracts activation patterns of 167
the hidden layers in the DNNs and use CCA on the singular vectors extracted from them to measure 168
how similar the two layers are. Their analysis brings many interesting insights regarding the learning 169
dynamics of the different convnets, although they do not discuss the non-linearity propagation in the 170
5convnets, nor do they propose a way to measure it. [ 38] proposed to use a normalized Frobenius 171
inner product between kernel matrices calculated on the extracted activations of the hidden layers 172
and argued that such a similarity measure is more meaningful than that proposed by [37]. 173
Impact of activation functions [40] provides the most comprehensive survey on the activation 174
functions used in DNNs. Their work briefly discusses the non-linearity of the different activation 175
functions suggesting that piecewise linear activation functions with more linear components are more 176
non-linear (e.g., ReLU vs. ReLU6). [ 41] show theoretically that smooth versions of ReLU allow 177
for more efficient information propagation in DNNs with a positive impact on their performance. 178
Our work provides a first extensive comparison of all popular activation functions; we also show that 179
smooth version of ReLU exhibit wider regions of high non-linearity (see Appendix D). 180
Non-linearity measure The only work similar to ours in spirit is the paper by [ 42] proposing the 181
non-linearity coefficient in order to predict the train and test error of DNNs. Their coefficient is 182
defined as a square root of the Jacobian of the neural network calculated wrt its input, multiplied by 183
the covariance matrix of the Jacobian, and normalized by the covariance matrix of the input. The 184
presence of the Jacobian in it calls for the differentiability assumption making its application to 185
most of the neural networks with ReLU non-linearity impossible as is. The authors didn’t provide 186
any implementation of their coefficient and we were not able to find any other study reporting the 187
reproduced results from this work. 188
4 Experimental evaluations 189
We consider computer vision models trained and evaluated on the same Imagenet dataset with 1,000 190
output categories (Imagenet-1K) publicly available at [ 43]. The non-linearity signatures of different 191
studied models presented in the paper is calculated by passing batches of size 512 through the 192
pre-trained models for the entirety of the Imagenet-1K validation set (see Appendix H for more 193
datasets) with a total of 50,000 images. We include the following landmark architectures in our study: 194
Alexnet [ 14], four VGG models [ 16], Googlenet [ 44], Inception v3 [ 17], five Resnet models [ 18], 195
four Densenet models [ 19], four MNASNet models [ 45], four EfficientNet models [ 46], five ViT 196
models, three Swin transformer [ 47] and four Convnext models [ 48]. We include MNASNet and 197
EfficientNet models as prominent representatives of the neural architecture search approach [ 49]. 198
Such models are expected to explicitly maximize the accuracy for a given computational budget. 199
Swin transformer and Convnext models are introduced as ViTs with traditional computer vision 200
priors. Their presence will be useful to better grasp how such priors impact ViTs. We refer the reader 201
to Appendix E for more practical details. 202
History of deep vision models at a glance We give a general outlook of the developments in 203
computer vision over the last decade when seen through the lens of their non-linearity. In Figure 3 204
we present the minimum, median, and maximum values of the affinity scores calculated for the 205
considered neural networks (see Appendix F for raw non-linearity signatures). We immediately 206
see that until the arrival of transformers, the trend of the landmark models was to decrease their 207
non-linearity, rather than to increase it. On a more fine-grained level, we note that pure convolution 208
architectures such as Alexnet (2012) and VGGs (2014) exhibit a very low spread of the affinity 209
score values. This trend changes with the arrival of the inception module first used in Googlenet 210
(2014): the latter includes activation functions that extend the range of the non-linearity on both 211
ends of the spectrum. Importantly, we can see that the trend toward increasing the maximum and 212
average non-linearity of the neural networks has continued for almost the whole decade. Even more 213
surprisingly, EfficientNet models (2019), trained through neural architecture search, have strong 214
negative skewness toward higher linearity, although they were state-of-the-art in their time. The 215
second surprising finding comes with the arrival of ViTs (2020): they break the trend and leverage 216
the non-linearity of their hidden activation functions becoming more or more non-linear with the 217
varying size of the patches (see Appendix F for a more detailed comparison with raw signatures). 218
This trend remains valid also for Swin transformers (2021), although introducing the computer vision 219
priors into them makes their non-linearity signature look more similar to pure convolutional networks 220
from the early 2010s, such as Alexnet and VGGs. Finally, we observe that the non-linearity signature 221
of a modern Convnext architecture (2022), designed as a convnet for 2020s using the best practices 222
of Swin transformers, further confirms this observation. 223
6Figure 3: Median, minimum, and maximum values of non-linearity signatures of the different
architectures spanning a decade (2012-2022) of computer vision research. We observe a clear trend
toward the increase of the spread and the maximum values of the linearity in neural networks lasting
until the arrival of transformers in 2020. ViTs have a distinct pattern of maximizing the non-linearity
of their activation functions. Swin transformers and Convnext models retain this property from them
while remaining close to the pure convolutional networks.
60 80
Accuracy1.11.2Max/MedianR2: 0.512
60 80
Accuracy102030DepthR2: 0.686Pre-residual
70 80 90
Accuracy1.21.4Max/MedianR2: 0.814
70 80 90
Accuracy510OpsR2: 0.666Residual
70 80 90
Accuracy0.0750.1000.125StdR2: 0.767
70 80 90
Accuracy12Nb paramsR2: 0.955NAS-based
70 80 90
Accuracy0.10.20.3MinR2: 0.974
70 80 90
Accuracy05001000OpsR2: 0.842ViT s
80 90
Accuracy0.550.60MedianR2: 0.818
80 90
Accuracy1015DepthR2: 0.788Post-ViT
Figure 4: Best found dependency between the different statistics extracted from the non-linearity
signatures of the DNN families and their respective Imagenet-1K accuracy. The results are compared
in terms of the R2score against the most precise of the other common DNN characteristics such as
depth, size, and the GFLOPS.
7Densenets Resnets NAS + Inception Swins Convnext Pure convnets ViTsA.
D.
B.
Convolution 
+ 
ReLU 1ReLU 2
Convolution
Residual connectionC.Figure 5: Comparing the different families of the neural architectures based on their non-linearity
signatures. (A)Hierarchical clustering of all DNNs considered in our study revealing meaningful
clusters with close architectural characteristics; (B)9 representative architectures from all studied
families and the similarities between them. Note how the similarities between early convnets and other
models is decreasing with time until computer vision priors are introduced into Swin transformers in
2021; (C)Distributions of affinity scores in each network. Most models expand the non-linearity
ranges of their activation functions compared to early convnets. ViTs are dominated by highly
non-linear activation functions, Resnets have a bimodal distribution, Densenets, and EfficientNets
have a diametrically skewed distribution compared to ViTs. (D)Comparing the same convnet with 20
layers when trained with (Residual Resnet20) and without (Plain Resnet20) residual connections (top
row). Residual connections introduce a clear trend toward a bimodal distribution of affinity scores;
the same effect is observed for Resnet18 and Resnet34 (bottom row).
Closer look at accuracy/non-linearity trade-off Different families of vision models leverage differ- 224
ent characteristics of their internal non-linearity to achieve better performance. To better understand 225
this phenomenon, we now turn our attention to a more detailed analysis of the accuracy/non-linearity 226
trade-off by looking for a statistic extracted from their non-linearity signatures that is the most predic- 227
tive of their accuracy as measured by the R2score. Additionally, we also want to understand whether 228
the non-linearity of DNNs can explain their performance better than the traditional characteristics 229
such as the number of parameters, the number of giga floating point operations per second (GFLOPS), 230
and the depth. From the results presented in Figure 4, we observe the following. First, the information 231
extracted from the non-linearity signatures often correlates more with the final accuracy, than the 232
usual DNN characteristics. This is the case for Residual networks (ResNets and DenseNets), ViTs, 233
and vision models influenced by transformers (Post-ViT). Unsurprisingly, for models based on neural 234
architecture search (NAS-based, i.e. EfficientNets and MNASNets) the number of parameters is 235
the most informative metric as they are specifically designed to reach the highest accuracy with the 236
increasing model size and compute. For Pre-residual pure convolutional models (Alexnet, VGGs, 237
Googlenet, and Inception), the spread of the non-linearity explains the accuracy increase similarly to 238
depth. Second, we observe that all models preceding ViTs were implicitly optimizing the spread of 239
their affinity score values to achieve better performance. After the arrival of the transformers, the 240
observed trend is to increase either the median or the minimum values of the non-linearity. This 241
suggests a fundamental shift in the implicit bias that the transformers carry. 242
8Table 1: Pearson correlations between the non-linearity signature and other metrics, for all the
architectures evaluated in this study. The highest absolute value in each group is reported in bold .
Models CKA NORM SPARSITY ENTROPY R2
VGGs 0.0 ±0.05 -0.67 ±0.06 -0.18 ±0.03 -0.90±0.04 -0.21±0.06
ResNets 0.53 ±0.04 -0.41 ±0.19 -0.68±0.02 -0.38±0.12 -0.48 ±0.24
DenseNets 0.88 ±0.02 -0.76 ±0.02 -0.89±0.02 -0.66±0.03 0.85 ±0.04
MNASNets 0.67±0.11 -0.54±0.14 -0.63 ±0.07 -0.55 ±0.16 0.45 ±0.17
EfficientNets 0.42±0.10 -0.16±0.22 -0.17 ±0.23 -0.16 ±0.14 0.21 ±0.12
ViTs -0.22 ±0.40 -0.67±0.20 -0.09±0.56 0.17 ±0.25 -0.10 ±0.34
Swins -0.15 ±0.13 -0.53±0.10 -0.26±0.17 0.06 ±0.35 -0.13 ±0.13
Convnexts 0.69 ±0.08 0.21 ±0.15 0.23 ±0.16 0.02 ±0.09 0.79±0.05
Average 0.33 ±0.45 -0.44±0.34 -0.32±0.42 -0.31 ±0.39 0.14 ±0.49
Distinct signature for every architecture Non-linearity signature correctly identifies the different 243
families of neural architectures. To show this, we perform hierarchical clustering using pairwise 244
dynamic time warping (DTW) distances [ 50] between the non-linearity signatures of the models from 245
Figure 3. The results in Figure 5 (A), as well as the pairwise distance matrix between a representative 246
of each studied family in Figure 5 (B) (see Appendix G for the full matrix), show that we correctly 247
cluster all similar models together, both within their respective families (such as the different 248
variations of the same architecture) and across them (such as the cluster of Swin and pure convolution 249
models). Additionally, we highlight the individual affinity scores’ distributions of representative 250
models in Figure 5 (C). Finally, we highlight the exact effect of residual connections proposed in 251
2016 and used ever since by every benchmark model in Figure 5 (D). It reveals vividly that residual 252
connections make the distribution of the affinity scores bimodal with one such mode centered around 253
highly linear activation functions. This confirms in a principled way that residual connections indeed 254
tend to enable the learning of the identity function just as suggested in the seminal work that proposed 255
them [ 18]. Non-linearity signatures can also be applied to meaningfully identify training methods, 256
such as popular nowadays self-supervised approaches, for a fixed architecture (see Appendix I). 257
258
Uniqueness of the affinity score No other metric extracted from the activation functions of the 259
considered networks exhibits a strong consistent correlation with the non-linearity signature. To 260
validate this claim, we compare in Table 1 the Pearson correlation between the non-linearity signature 261
and several other metrics comparing the inputs and the outputs of the activation functions. We can see 262
that for different models the non-linearity correlates with different metrics suggesting that it captures 263
the information that other metrics fail to capture consistently across all architectures. This becomes 264
even more apparent when analyzing the individual correlation values (in Appendix G). Overall, the 265
proposed affinity score and the non-linearity signatures derived from it offer a unique perspective on 266
the developments in the ML field. 267
5 Discussions 268
We proposed the first sound approach to measure non-linearity of activation functions in neural 269
networks and defined their non-linearity signature based on it. We further used non-linearity signatures 270
to provide a meaningful overview of the evolution of neural architectures proposed over the last 271
decade with clear interpretable patterns. We showed that until the arrival of transformers, the trend in 272
DNNs was to decrease their non-linearity, rather than to increase it. Vision transformers changed 273
this pattern drastically. We also showcased that our measure is unique, as no other metric correlates 274
strongly with it across all architectures. 275
In the future, our work can be applied to study the non-linearity of the LLM models to better under- 276
stand the effect of different architectural choices in them. On a higher level, our approach can also be 277
used to identify new disruptive neural architectures by identifying those of them that leverage different 278
internal non-linearity characteristics to obtain better performance. This capacity of identifying novel 279
technologies is even more crucial in the age of very large models where experimenting with the 280
building blocks of the optimized backbone comes at a very high cost. 281
9References 282
[1]Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature , 521(7553):436–444, 283
2015. 284
[2]Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks , 285
61:85–117, 2015. 286
[3]Michael I Jordan and Tom M Mitchell. Machine learning: Trends, perspectives, and prospects. 287
Science , 349(6245):255–260, 2015. 288
[4]I. Goodfellow, Y . Bengio, and A. Courville. Deep Learning . Adaptive computation and machine 289
learning. MIT Press, 2016. 290
[5]Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud A.A. Setio, Francesco Ciompi, 291
Mohsen Ghafoorian, Jeroen A.W.M. van der Laak, Bram van Ginneken, and Clara I. Sánchez. 292
A survey on deep learning in medical image analysis. Medical image analysis , 42:60–88, 2017. 293
[6]Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Wei Chen. Deberta: Decoding-enhanced 294
bert with disentangled attention. In Proceedings of the International Conference on Learning 295
Representations , 2021. 296
[7]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: 297
Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE 298
International Conference on Computer Vision , page 1026–1034, 2015. 299
[8] OpenAI. Ai and compute. 2018. Accessed: March 13, 2024. 300
[9]Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for 301
deep learning in nlp. arXiv preprint arXiv:1906.02243 , 2019. 302
[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, 303
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- 304
tion Processing Systems , pages 5998–6008, 2017. 305
[11] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, 306
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are 307
few-shot learners. arXiv preprint arXiv:2005.14165 , 2020. 308
[12] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- 309
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, 310
Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation 311
language models, 2023. 312
[13] OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023. 313
[14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep 314
convolutional neural networks. Advances in neural information processing systems , 25:1097– 315
1105, 2012. 316
[15] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng 317
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 318
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision , 319
115(3):211–252, 2015. 320
[16] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale 321
image recognition. In International Conference on Learning Representations , 2015. 322
[17] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Arman Alemi. Rethinking the 323
inception architecture for computer vision. arXiv preprint arXiv:1512.00567 , 2016. 324
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image 325
recognition. arXiv preprint arXiv:1512.03385 , 2016. 326
10[19] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected 327
convolutional networks. arXiv preprint arXiv:1608.06993 , 2017. 328
[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, 329
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, 330
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image 331
recognition at scale. In ICLR , 2021. 332
[21] Ingo Gühring, Mones Raslan, and Gitta Kutyniok. Expressivity of deep neural networks. 333
arXiv:2007.04759 , 2020. 334
[22] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In 29th 335
Annual Conference on Learning Theory , pages 907–940, 2016. 336
[23] Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with 337
neural networks. In Proceedings of the 34th International Conference on Machine Learning , 338
pages 2979–2987, 2017. 339
[24] Peter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc- 340
dimension and pseudodimension bounds for piecewise linear neural networks. Journal of 341
Machine Learning Research , 20(63):1–17, 2019. 342
[25] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the 343
expressive power of deep neural networks. In Proceedings of the International Conference on 344
Machine Learning , pages 2847–2854, 2017. 345
[26] Guido Montúfar, Razvan Pascanu, KyungHyun Cho, and Yoshua Bengio. On the number of 346
linear regions of deep neural networks. In NeurIPS , pages 2924–2932, 2014. 347
[27] Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power 348
of neural networks: a view from the width. In Advances in Neural Information Processing 349
Systems , page 6232–6240, 2017. 350
[28] Gal Vardi, Gilad Yehudai, and Ohad Shamir. On the optimal memorization power of relu neural 351
networks. In The Tenth International Conference on Learning Representations, ICLR , 2022. 352
[29] Kurt Hornik. Multilayer feedforward networks are universal approximators. Neural Networks , 353
2(5):359–366, 1989. 354
[30] Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Mach. 355
Learn. , 14(1):115–133, 1994. 356
[31] Kurt and Hornik. Approximation capabilities of multilayer feedforward networks. Neural 357
Networks , 4(2):251–257, 1991. 358
[32] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, 359
Signals, and Systems (MCSS) , 2(4):303–314, 1989. 360
[33] Cyril S Smith and Martin Knott. Note on the optimal transportation of distributions. Journal of 361
Optimization Theory and Applications , 52(2):323–329, 1987. 362
[34] Matthias Gelbrich. On a formula for the l2 wasserstein metric between measures on euclidean 363
and hilbert spaces. Mathematische Nachrichten , 147(1):185–203, 1990. 364
[35] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann 365
machines. In Proceedings of the International Conference on Machine Learning , pages 807– 366
814, 2010. 367
[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating 368
errors. Nature , 323(6088):533–536, 1986. 369
[37] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular 370
vector canonical correlation analysis for deep learning dynamics and interpretability. In NIPS’17 , 371
page 6078–6087, 2017. 372
11[38] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural 373
network representations revisited. In ICML , volume 97, pages 3519–3529. PMLR, 09–15 Jun 374
2019. 375
[39] MohammadReza Davari, Stefan Horoi, Amine Natik, Guillaume Lajoie, Guy Wolf, and Eugene 376
Belilovsky. Reliability of CKA as a similarity measure in deep learning. In ICLR , 2023. 377
[40] Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Activation functions in 378
deep learning: A comprehensive survey and benchmark. Neurocomput. , 503(C):92–108, 2022. 379
[41] Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function 380
on deep neural networks training. In Proceedings of the 36th International Conference on 381
Machine Learning , pages 2672–2680, 2019. 382
[42] George Philipp. The nonlinearity coefficient - A practical guide to neural architecture design. 383
CoRR , abs/2105.12210, 2021. 384
[43] TorchVision maintainers and contributors. Torchvision: Pytorch’s computer vision library. 385
GitHub repository , 2016. 386
[44] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, 387
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. 388
arXiv preprint arXiv:1409.4842 , 2014. 389
[45] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and 390
Quoc V . Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of 391
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019. 392
[46] Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural 393
networks. In Proceedings of the International Conference on Machine Learning , pages 6105– 394
6114, 2019. 395
[47] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining 396
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings 397
of the IEEE/CVF International Conference on Computer Vision , 2021. 398
[48] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining 399
Xie. A convnet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision 400
and Pattern Recognition , 2022. 401
[49] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. 402
Journal of Machine Learning Research , 20(55):1–21, 2019. 403
[50] Hiroaki Sakoe and Seibi Chiba. Dynamic programming algorithm optimization for spoken word 404
recognition. IEEE transactions on acoustics, speech, and signal processing , 26(1):43–49, 1978. 405
[51] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In 406
Geoffrey Gordon, David Dunson, and Miroslav Dudík, editors, Proceedings of the Fourteenth 407
International Conference on Artificial Intelligence and Statistics , volume 15 of Proceedings 408
of Machine Learning Research , pages 315–323, Fort Lauderdale, FL, USA, 11–13 Apr 2011. 409
PMLR. 410
[52] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint 411
arXiv:1606.08415 , 2016. 412
[53] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Wei Wang, Wenhan Weng, 413
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for 414
mobile vision applications. In Proceedings of the 2017 IEEE Conference on Computer Vision 415
and Pattern Recognition , pages 4200–4210. IEEE, 2017. 416
[54] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural 417
network acoustic models. In Proceedings of the ICML Workshop on Deep Learning for Audio, 418
Speech and Language Processing , 2013. 419
12[55] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network 420
function approximation in reinforcement learning. Neural networks , 107:3–11, 2018. 421
[56] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, 422
Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. 423
InProceedings of the IEEE/CVF international conference on computer vision , pages 1314–1324, 424
2019. 425
[57] Olivier Ledoit and Michael Wolf. Honey, i shrunk the sample covariance matrix. Journal of 426
Portfolio Management , 30(4):110–119, 2004. 427
[58] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. 428
Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural 429
information processing systems , 33:9912–9924, 2020. 430
[59] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, 431
and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings 432
of the IEEE/CVF international conference on computer vision , pages 9650–9660, 2021. 433
[60] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for 434
unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on 435
computer vision and pattern recognition , pages 9729–9738, 2020. 436
[61] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Procedings of the British 437
Machine Vision Conference 2016 . British Machine Vision Association, 2016. 438
[62] Mert Bülent Sarıyıldız, Yannis Kalantidis, Karteek Alahari, and Diane Larlus. No reason for 439
no supervision: Improved generalization in supervised models. In The Eleventh International 440
Conference on Learning Representations , 2023. 441
[63] Julien Denize, Jaonary Rabarisoa, Astrid Orcesi, Romain Hérault, and Stéphane Canu. Similarity 442
contrastive estimation for self-supervised soft contrastive learning. In Proceedings of the 443
IEEE/CVF Winter Conference on Applications of Computer Vision , pages 2706–2716, 2023. 444
[64] Guangrun Wang, Keze Wang, Guangcong Wang, Philip HS Torr, and Liang Lin. Solving 445
inefficiency of self-supervised representation learning. In Proceedings of the IEEE/CVF 446
International Conference on Computer Vision , pages 9505–9515, 2021. 447
[65] Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, and 448
Chang Xu. Ressl: Relational self-supervised learning with weak augmentation. Advances in 449
Neural Information Processing Systems , 34:2543–2555, 2021. 450
13A Broader Impacts 451
This paper presents work whose goal is to advance the field of Machine Learning and better understand 452
the underlying behavior of Deep Neural Networks architectures. There are many potential societal 453
consequences of our work, none which we feel must be specifically highlighted here. 454
B Limitations 455
An important assumption of Theorem 3.3, is that the activation function that we want to analyze 456
through ρaffneeds to be a positive definite transformation of the inputs. Fortunately, this is the case for 457
activation functions, that we consider in this paper. Finally, we note that despite the strong correlation 458
between the statistics extracted from the non-linearity signatures for certain DNNs’ architectures, 459
we are yet to show that explicitly optimizing affinity scores through backpropagation can have an 460
actionable impact on DNNs performance or its other properties, such as robustness or transferability. 461
C Proofs of main theoretical results 462
In this section, we provide proofs of the main theoretical results from the paper. 463
Corollary 3.2. Without loss of generality, let X, Y∈ P2(Rd)be centered, and such that Y=TX, 464
where Tis a positive semi-definite linear transformation. Then, Tis the OT map from XtoY. 465
Proof. We first proof that we can consider centered distributions without loss of generality. To this 466
end, we note that 467
W2
2(X, Y) =W2
2(X−E[X], Y−E[Y]) +∥E[X]−E[Y]∥2, (6)
implying that splitting the 2-Wasserstein distance into two independent terms concerning the L2468
distance between the means and the 2-Wasserstein distance between the centered measures. 469
Furthermore, if we have an OT map T′between X−E[X]andY−E[Y], then 470
T(x) =T′(x−E[X]) +E[Y], (7)
is the OT map between XandY. 471
To prove the statement of the Corollary, we now need to apply Theorem 3.1 to the convex ϕ(x) = 472
xTTx, where Tis positive semi-definite. 473
Theorem 3.3. LetX, Y∈ P2(Rd)be centered and Y=TX for a positive definite matrix T. Let 474
NX∼ N(µ(X),Σ(X))andNY∼ N(µ(Y),Σ(Y))be their normal approximations where µandΣ 475
denote mean and covariance, respectively. Then, W2(NX, NY) =W2(X, Y)andT=Taff, where 476
Taffis the OT map between NXandNYand can be calculated in closed-form 477
Taff(x) =Ax+b, A = Σ(Y)1
2
Σ(Y)1
2Σ(X)Σ(Y)1
2−1
2Σ(Y)1
2,
b=µ(Y)−Aµ(X).(8)
Proof. Corollary 3.2 states that Tis an OT map, and 478
Σ(TNX) =TΣ(X)T= Σ(Y).
Therefore, TNX=NY, and by Theorem 3.1, Tis the OT map between NXandNY. Finally, we 479
compute 480
W2
2(NX, NY) = Tr[Σ( X)] + Tr[ TΣ(X)T]−2 Tr[T1
2Σ(X)T1
2]
= arg min
T:T(X)=YEX[∥X−T(X)∥2]
=W2
2(X, Y).
481
14Proposition 3.5. LetX, Y∈ P2(Rd)andNX, NYbe their normal approximations. Then, 482
1.|W2(NX, NY)−W2(X, Y)| ≤2 Trh
(Σ(X)Σ(Y))1
2i
√
Tr[Σ( X)]+Tr[Σ( Y)]. 483
2. For Taffas in (4), W2(TaffX, Y)≤√
2 Tr [Σ( Y)]1
2. 484
Proof. By Theorem 3.4, we have W2(NX, NY)≤W2(X, Y). On the other hand, 485
W2
2(X, Y) = min
γ∈ADM( X,Y)Z
Rd×Rd∥x−y∥2dγ(x, y)
≤Z
Rd×Rd 
∥x∥2+∥y∥2
dγ(x, y)
= Tr[Σ( X)] + Tr[Σ( Y)].
Combining the above inequalities, we get 486
|W2(NX, NY)−W2(X, Y)| ≤p
Tr[Σ( X)] + Tr[Σ( Y)]−W2(NX, NY).
Leta= Tr[Σ( X)] + Tr[Σ( Y)], and so W2
2(NX, NY) =a−b, where b= 2 Trh
(Σ(X)Σ(Y))1
2i
. 487
Then the RHS of can be written as 488
√a−√
a−b=|a−(a−b)|√a+√
a−b≤b√a,
where the inequality follows from positivity of W2(NX, NY) =√
a−b. Letting X=TaffXin the 489
obtained bound gives 2). 490
1520
 10
 0 10 200.00.20.40.60.81.0activation
0.000.250.500.751.001.25
aff
sigmoid aff
 baseline
20
 10
 0 10 201.0
0.5
0.00.51.0activation
0.000.250.500.751.001.25
aff
tanh aff
 baseline
20
 10
 0 10 201.0
0.5
0.00.51.0activation
0.000.250.500.751.001.25
aff
hardtanh aff
 baseline
20
 10
 0 10 2005101520activation
0.000.250.500.751.001.25
aff
relu aff
 baseline
20
 10
 0 10 2005101520activation
0.000.250.500.751.001.25
aff
leaky_relu aff
 baseline
20
 10
 0 10 200246activation
0.000.250.500.751.001.25
aff
relu6 aff
 baseline
20
 10
 0 10 2005101520activation
0.000.250.500.751.001.25
aff
gelu aff
 baseline
20
 10
 0 10 2005101520activation
0.000.250.500.751.001.25
aff
hardswish aff
 baseline
20
 10
 0 10 2005101520activation
0.000.250.500.751.001.25
aff
silu aff
 baselineFigure 6: Median affinity scores of Sigmoid, ReLU, GELU, ReLU6, LeakyReLU with a default
value of slope, Tanh, HardTanh, SiLU, and HardSwish obtained across random draws from Gaussian
distribution with a sliding mean and varying stds used as their input. Whiskers of boxplots show the
whole range of values obtained for each mean across all stds. The baseline value is the affinity score
obtained for a sample covering the whole interval. The ranges and extreme values of each activation
function over its subdomain are indicative of its non-linearity limits.
D Affinity scores of other popular activation functions 491
Many works aimed to improve the way how the non-linearity – represented by activation functions – 492
can be defined in DNNs. As an example, a recent survey on the commonly used activation functions in 493
deep neural networks [ 40] identifies over 40 activation functions with first references to sigmoid dating 494
back to the seminal paper [ 36] published in late 80s. The fashion for activation functions used in deep 495
neural networks evolved over the years in a substantial way, just as the neural architectures themselves. 496
Saturating activations, such as sigmoid and hyperbolic tan, inspired by computational neuroscience 497
were a number one choice up until the arrival of rectifier linear unit (ReLU) in 2010. After being the 498
workhorse of many famous models over the years, the arrival of transformers popularized Gaussian 499
Error Linear Unit (GELU) which is now commonly used in many large language models including 500
GPTs. 501
We illustrate in Figure 6 the affinity scores obtained after a single pass of the data through the 502
following activation functions: Sigmoid, ReLU [ 51], GELU [ 52], ReLU6 [ 53], LeakyReLU [ 54] 503
with a default value of the slope, Tanh, HardTanh, SiLU [ 55], and HardSwish [ 56]. As the non- 504
linearity of activation functions depends on the domain of their input, we fix 20 points in their 505
domain equally spread in [−20,20]interval. We use these points as means {mi}20
i=1of Gaussian 506
distributions from which we sample 1000 points in R300with standard deviation (std) σtaking values 507
in[2,1,0.5,0.25,0.1,0.01]. Each sample denoted by Xσjmiis then passed through the activation 508
function act∈ {sigmoid ,ReLU ,GELU }to obtain ρmi,σj
aff :=ρaff(Xσjmi,act(Xσjmi)). Larger std 509
values make it more likely to draw samples that are closer to the region where the studied activation 510
functions become non-linear. We present the obtained results in Figure S2 where each of 20 boxplots 511
showcases median (ρmi,σ·
aff)values with 50% confidence intervals and whiskers covering the whole 512
range of obtained values across all σj. 513
16This plot allows us to derive several important conclusions. We observe that each activation function 514
can be characterized by 1) the lowest values of its non-linearity obtained for some subdomain of the 515
considered interval and 2) the width of the interval in which it maintains its non-linearity. We note 516
that in terms of 1) both GELU and ReLU may attain affinity scores that are close to 0, which is not 517
the case for Sigmoid. For 2), we observe that the non-linearity of Sigmoid and GELU is maintained 518
in a wide range, while for ReLU it is rather narrow. We can also see a distinct pattern of more 519
modern activation functions, such as SiLU and HardSwish having a stronger non-linearity pattern in 520
large subdomains. We also note that despite having a shape similar to Sigmoid, Tanh may allow for 521
much lower affinity scores. Finally, the variations of ReLU seem to have a very similar shape with 522
LeakyReLU being on average more linear than ReLU and ReLU6. 523
17ave. flat. sum
 0.550.600.650.700.750.800.850.900.95aff
gelu: 0.61±.003hardswish: 0.7±.002hardtanh: 0.78±.002
leakyrelu:0.63±.003
relu, relu6: 0.62±(.004,.002)sigmoid: 0.91±.001
silu: 0.68±.002tanh: 0.81±.001
5001500 2500 3500 4500
d0.650.700.750.800.850.900.95aff
 gelu: 0.78hardswish: 0.83hardtanh: 0.88
leakyrelu:0.79
relu, relu6: 0.78sigmoid: 0.95
silu: 0.82tanh: 0.9
5001500 2500 3500 4500
d0.600.650.700.750.800.850.90aff
gelu: 0.61hardswish: 0.7hardtanh: 0.78
leakyrelu:0.63
relu, relu6: 0.62sigmoid: 0.91
silu: 0.67tanh: 0.81Figure 7: (Top left) Affinity score is robust to the dimensionality reduction both when using averaging
and summation over the spatial dimensions; (Top right) When d > n , sample covariance matrix
estimation leads to a lack of robustness in the estimation of the affinity score; (Bottom) Shrinkage of
the covariance matrix leads to constant values of the affinity scores with increasing d.
E Implementation details 524
Dimensionality reduction Manipulating 4-order tensors is computationally prohibitive and thus 525
we need to find an appropriate lossless function rto facilitate this task. One possible choice for r 526
may be a vectorization operator that flattens each tensor into a vector. In practice, however, such 527
flattening still leads to very high-dimensional data representations. In our work, we propose to use 528
averaging over the spatial dimensions to get a suitable representation of the manipulated tensors. In 529
Figure 7 (left), we show that the affinity score is robust wrt such an averaging scheme and maintains 530
the same values as its flattened counterpart. 531
Computational considerations The non-linearity signature requires calculating the affinity score 532
over “wide” matrices. Indeed, after the reduction step is applied to a batch of ntensors of size 533
h×w×c, we end up with matrices of size n×cwhere nmay be much smaller than c. This is also 534
the case when input tensors are 2D when the batch size is smaller than the dimensionality of the 535
embedding space. To obtain a well-defined estimate of the covariance matrix in this case, we use a 536
known tool from the statistics literature called Ledoit-Wolfe shrinkage [ 57]. In Figure 7 (right), we 537
show that shrinkage allows us to obtain a stable estimate of the affinity scores that remain constant in 538
all regimes. 539
Robustness to batch size and different seeds In this section, we highlight the robustness of the 540
non-linearity signature with respect to the batch size and the random seed used for training. To this 541
end, we concentrate on VGG16 architecture and CIFAR10 dataset to avoid costly Imagenet retraining. 542
In Figure 8, we present the obtained result where the batch size was varied between 128 and 1024 543
with an increment of 128 (left plot) and when VGG16 model was retrained with seeds varying from 544
1 to 9 (right plot). The obtained results show that the affinity score is robust to these parameters 545
suggesting that the obtained results are not subject to a strong stochasticity. 546
181 4 8 11 15
Depth00.20.40.60.81aff
Batch size
1024
128
256
512
1 4 8 11 15
Depth00.20.40.60.81aff
Seed
1
2
3
4
5
6
8
9
7
Figure 8: Non-linearity signature of VGG16 on CIFAR10 with a varying batch size (left) and when
retrained from 9 different random seeds (right).
1 4 8 11 15
Depth00.20.40.60.81aff
Init
False
True
Figure 9: Non-linearity signatures of VGG16 on CIFAR10 in the beginning and end of training on
Imagenet.
Impact of training Finally, we also show how a non-linearity signature of a VGG16 model looks 547
like at the beginning and in the end of training on Imagenet. We extract its non-linearity signature 548
at initialization when making a feedforward pass over the whole CIFAR10 dataset and compare it 549
to the non-linearity signature obtained in the end. In Figure 9, we can see that at initialization the 550
network’s non-linearity signature is increasing, reaching almost a perfectly linear pattern in the last 551
layers. Training the network enhances the non-linearity in a non-monotone way. Importantly, it also 552
highlights that the non-linearity signature is capturing information from the training process. 553
191 2 4 5 7
Depth00.20.40.60.81aff
Alexnet (ReLU, std=0.005)
1 4 8 11 15
Depth00.20.40.60.81aff
Vgg16 (ReLU, std=0.008)
0 10 20 30 40 50 60
Depth00.20.40.60.81aff
Inception v3 (ReLU, std=0.004)
0 25 50 75 100 125 150
Depth00.20.40.60.81aff
Resnet152 (ReLU, std=0.005)
0 25 50 75 100 125 150
Depth00.20.40.60.81aff
Densenet161 (ReLU, std=0.020)
0 50 100 150
Depth0.00.20.40.60.81.01.2aff
Efficientnet b6 (std=0.008)
SiLU
Squeeze (SiLU)
Excite (Sigmoid)
0 5 10 15 20 25 30
Depth00.20.40.60.81aff
Vit Huge 14x14 (GELU, std=0.013)
0 2 4 6 8 10
Depth00.20.40.60.81aff
Swin T (GELU, std=0.022)
0 5 10 15
Depth00.20.40.60.81aff
Convnext (GELU, std=0.019)Figure 10: Raw non-linearity signatures of popular DNN architectures, plotted as affinity scores over
the depth throughout the network.
0 5 10 15 20
Depth00.20.40.60.81aff
Vit Large 16x16 (GELU, std=0.024)
0 5 10 15 20
Depth00.20.40.60.81aff
Vit Large 32x32 (GELU, std=0.014)
0 5 10 15 20 25 30
Depth00.20.40.60.81aff
Vit Huge 14x14 (GELU, std=0.013)
Figure 11: ViTs: Large ViT with 16x16 and 32x32 patch sizes and Huge ViT.
F Raw signatures 554
In Figure 10, we portray the raw non-linearity signatures of several representative networks studied 555
in the main paper. We use different color codes for distinct activation functions appearing repeatedly 556
in the considered architecture (for instance, every first ReLU in a residual block of a Resnet). We 557
also indicate the mean standard deviation of the affinity scores over batches in the title. 558
We see that the non-linearities across ReLU activations in all of Alexnet’s 8 layers remain stable. Its 559
successor, VGG network, reveals tiny, yet observable, variations in the non-linearity propagation with 560
increasing depth and, slightly lower overall non-linearity values. We attribute this to the decreased 561
size of the convolutional filters (3x3 vs. 7x7). The Googlenet architecture was the first model 562
to consider learning features at different scales in parallel within the so-called inception modules. 563
This add more variability as affinity scores of activation in Googlenet vary between 0.6 and 0.9. 564
Despite being almost 20 times smaller than VGG16, the accuracy of Googlenet on Imagenet remains 565
comparable, suggesting that increasing and varying the linearity is a way to have high accuracy with 566
a limited computational complexity compared to predecessors. This finding is further confirmed with 567
Inception v3 that pushed the spread of the affinity score toward being more linear in some hidden 568
layers. When comparing this behavior with Alexnet, we note just how far we are from it. Resnets 569
achieve the same spread of values of the non-linearity but in a different, and arguably, simpler way. 570
Indeed, the activation after the skip connection exhibits affinity scores close to 1, while the activations 571
in the hidden layers remain much lower. Densenet, that connect each layer to all previous layers and 572
201 3 5 7 10
Depth00.20.40.60.81aff
Vgg11 (ReLU, std=0.007)
1 3 6 9 12
Depth00.20.40.60.81aff
Vgg13 (ReLU, std=0.007)
1 4 8 11 15
Depth00.20.40.60.81aff
Vgg16 (ReLU, std=0.008)
1 5 9 13 18
Depth00.20.40.60.81aff
Vgg19 (ReLU, std=0.008)Figure 12: Impact of depth on the non-linearity signature of VGGs.
5 10 15
Depth00.20.40.60.81aff
Resnet18 (ReLU, std=0.012)
0 10 20 30
Depth00.20.40.60.81aff
Resnet34 (ReLU, std=0.010)
0 10 20 30 40 50
Depth00.20.40.60.81aff
Resnet50 (ReLU, std=0.011)
0 20 40 60 80 100
Depth00.20.40.60.81aff
Resnet101 (ReLU, std=0.018)
0 25 50 75 100 125 150
Depth00.20.40.60.81aff
Resnet152 (ReLU, std=0.005)
Figure 13: Impact of depth on the non-linearity signature of Resnets.
not just to the one that precedes it, is slightly more non-linear than Resnet152, although the two bear 573
a striking similarity: they both have an activation function that maintains the non-linearity low with 574
increasing depth. Additionally, transition layers in Densenet act as linearizers and allow it to reset the 575
non-linearity propagation in the network by reducing the feature map size. ViTs (Large with 16x16 576
and 32x32 patch sizes, and Huge with 14x14 patches) are all highly non-linear models to the degree 577
yet unseen. Interestingly, as seen in Figure 11 the patch size affects the non-linearity propagation 578
in a non-trivial way: for 16x16 size a model is more non-linear in the early layers, while gradually 579
becoming more and more linear later, while 32x32 patch size leads to a plateau in the hidden layers 580
of MLP blocks, with a steep change toward linearity only in the final layer. We hypothesize that 581
attention modules in ViT act as a focusing lens and output the embeddings in the domain where the 582
activation function is the most non-linear. 583
Finally, we explore the role of increasing depth for VGG and Resnet architectures. We consider 584
VGG11, VGG13, VGG16 and VGG19 models in the first case, and Resnet18, Resnet34, Resnet50, 585
Resnet101 and Resnet152. The results are presented in Figure 12 and Figure 13 for VGGs and 586
Resnets, respectively. Interestingly, VGGs do not change their non-linearity signature with increasing 587
depth. In the case of Resnets, we can see that the separation between more linear post-residual 588
activations becomes more distinct and approaches 1 for deeper networks. 589
21Table 2: Pearson correlations between the affinity score and other metrics, for all the architectures
evaluated in this study. We see that no other metric can reliably provide the same information as the
proposed non-linearity signature across different neural architectures.
Model CKA Norm Sparsity Entropy R2
alexnet -0.75 -0.86 0.14 -0.80 -0.41
vgg11 -0.07 -0.76 -0.15 -0.95 -0.27
vgg13 0.08 -0.66 -0.23 -0.93 -0.26
vgg16 0.01 -0.63 -0.19 -0.88 -0.17
vgg19 -0.01 -0.62 -0.15 -0.86 -0.14
googlenet 0.74 -0.60 -0.83 -0.49 0.73
inception v3 0.69 -0.66 -0.75 -0.45 0.35
resnet18 0.59 -0.17 -0.67 -0.30 -0.44
resnet34 0.48 -0.18 -0.65 -0.19 -0.08
resnet50 0.56 -0.60 -0.71 -0.50 -0.78
resnet101 0.51 -0.57 -0.70 -0.51 -0.64
resnet152 0.52 -0.51 -0.68 -0.42 -0.48
densenet121 0.84 -0.75 -0.87 -0.62 0.82
densenet161 0.87 -0.74 -0.87 -0.67 0.81
densenet169 0.87 -0.74 -0.87 -0.67 0.81
densenet201 0.89 -0.75 -0.91 -0.67 0.90
efficientnet b1 0.35 -0.41 -0.39 0.01 0.03
efficientnet b2 0.49 -0.02 -0.44 -0.06 0.34
efficientnet b3 0.32 -0.12 -0.18 -0.13 0.18
efficientnet b4 0.30 -0.51 -0.29 -0.44 0.11
vit b 32 0.47 -0.31 -0.29 0.39 0.51
vit l 32 -0.14 -0.61 -0.47 -0.02 -0.06
vit b 16 -0.27 -0.71 0.04 0.39 -0.22
vit l 16 -0.39 -0.89 -0.66 -0.23 -0.24
vit h 14 -0.77 -0.83 0.92 0.31 -0.49
swin t -0.12 -0.39 -0.02 -0.42 -0.06
swin s -0.003 -0.61 -0.31 0.18 -0.03
swin b -0.32 -0.59 -0.43 0.42 -0.32
convnext tiny 0.77 -0.01 -0.04 0.09 0.80
convnext small 0.57 0.22 0.25 0.13 0.72
convnext base 0.67 0.41 0.35 -0.03 0.82
convnext large 0.75 0.23 0.35 -0.10 0.84
Average 0.31 ±0.45 -0.44±0.35 -0.31±0.43 -0.29 ±0.39 0.13 ±0.50
G Detailed comparisons between architectures 590
We consider the following metrics as 1) the linear CKA [38] commonly used to assess the similarity 591
of neural representations, the average change in 2) SPARSITY and 3) ENTROPY before and after the 592
application of the activation function as well as the 4) Frobenius NORM between the input and output 593
of the activation functions, and the 5) R2score between the linear model fitted on the input and the 594
output of the activation function. We present in Table 2, the detailed values of Pearson correlations 595
obtained for each architecture and all the metrics considered in this study. In Figure 14, we show the 596
full matrix of pairwise DTW distances [ 50] obtained between architectures, then used to obtain the 597
clustering presented in the main text. 598
22alexnet
convnext baseconvnext largeconvnext smallconvnext tinydensenet121densenet161densenet169densenet201efficientnet b1efficientnet b2efficientnet b3efficientnet b4googlenet
inception v3mnasnet0 5mnasnet0 75mnasnet1 0mnasnet1 3resnet101resnet152resnet18resnet34resnet50swin bswin sswin tvgg11vgg13vgg16vgg19vit b 16vit b 32vit h 14vit l 16vit l 32alexnet
convnext base
convnext large
convnext small
convnext tiny
densenet121
densenet161
densenet169
densenet201
efficientnet b1
efficientnet b2
efficientnet b3
efficientnet b4
googlenet
inception v3
mnasnet0 5
mnasnet0 75
mnasnet1 0
mnasnet1 3
resnet101
resnet152
resnet18
resnet34
resnet50
swin b
swin s
swin t
vgg11
vgg13
vgg16
vgg19
vit b 16
vit b 32
vit h 14
vit l 16
vit l 32
0.00.51.01.52.02.53.0Figure 14: Full matrix of DTW distances between non-linearity signatures.
alexnet
convnext base
convnext large
convnext small
convnext tiny
densenet121
densenet161
densenet169
densenet201
maxvit t
resnet101
resnet152
resnet18
resnet34
resnet50
shufflenet v2 x0 5
shufflenet v2 x1 0
shufflenet v2 x1 5
shufflenet v2 x2 0
swin b
swin s
swin t
vgg11
vgg13
vgg16
vgg19
vit b 16
vit b 32
vit h 14
vit l 16
vit l 32
wide resnet101 2
wide resnet50 20.00.51.01.5CIFAR10: 0.28±0.11 CIFAR100: 0.26±0.11 Random: 0.77±0.32
Figure 15: Deviation in terms of the Euclidean distance of the non-linearity signature obtained on
CIFAR10, CIFAR100, and Random datasets from the non-linearity signature of the Imagenet dataset.
H Results on more datasets 599
Below, we compare the results obtained on CIFAR10, CIFAR100 datasets as well as when the random 600
data tensors are passed through the network. As the number of plots for all chosen 33 models on 601
these datasets will not allow for a meaningful visual analysis, we rather plot the differences – in terms 602
of the DTW distance – between the non-linearity signature of the model on Imagenet dataset with 603
respect to three other datasets. We present the obtained results in Figure 15. 604
We can see that the overall deviation for CIFAR10 and CIFAR100 remains lower than for Random 605
dataset suggesting that these datasets are semantically closer to Imagenet. 606
23resnet50 swav 800
resnet50 dino
resnet50 swav 400
resnet50 moco 800
resnet50 moco 200
resnet50 swav 200
resnet50 trexstar
resnet50 trex
wide resnet50 2
resnet50
resnet50 sce 300
resnet50 sce 200
resnet50 sce 1000
resnet50 sce 100
resnet50 triplet 200
resnet50 triplet 1k
resnet50 ressl 2000.000.050.10DistanceFigure 16: Hierarchical clustering of supervised and self-supervised pre-trained Resnet50 using the
DTW distances between their non-linearity signatures.
Table 3: Robustness of the different criteria when considering the same architectures pre-trained for
different tasks. Affinity score achieves the lowest standard deviation suggesting that it is capable of
correctly identifying the architecture even when it was trained differently.
Criterion Mean±std
ρaff 0.76±0.04
Linear CKA 0.90±0.07
Norm 448.56 ±404.61
Sparsity 0.56±0.16
Entropy 0.39±0.46
I Results for self-supervised methods 607
In this section, we show that the non-linearity signature of a network remains almost unchanged 608
when considering other pertaining methodologies such as for instance, self-supervised ones. To this 609
end, we use 17 Resnet50 architecture pre-trained on Imagenet within the next 3 families of learning 610
approaches: 611
1.SwA V [ 58], DINO [ 59], and MoCo [ 60] that belong to the family of contrastive learning 612
methods with prototypes; 613
2.Resnet50 [ 18], Wide Resnet50 [ 61], TRex, and TRex* [ 62] that are supervised learning 614
approaches; 615
3.SCE [ 63], Truncated Triplet [ 64], and ReSSL [ 65] that perform contrastive learning using 616
relational information. 617
From the dendrogram presented in Figure 16, we can observe that the DTW distances between the 618
non-linearity signatures of all the learning methodologies described above allow us to correctly cluster 619
them into meaningful groups. This is rather striking as the DTW distances between the different 620
instances of the Resnet50 model are rather small in magnitude suggesting that the affinity scores still 621
retain the fact that it is the same model being trained in many different ways. 622
While providing a fine-grained clustering of different pre-trained models for a given fixed architecture, 623
the average affinity scores over batches remain surprisingly concentrated as shown in Table 3. This 624
hints at the fact that the non-linearity signature is characteristic of architecture but can also be subtly 625
multi-faceted when it comes to its different variations. 626
24resnet50
resnet50 dino
resnet50 moco 200resnet50 moco 800resnet50 ressl 200resnet50 sce 100resnet50 sce 1000resnet50 sce 200resnet50 sce 300resnet50 swav 200resnet50 swav 400resnet50 swav 800resnet50 trex
resnet50 trexstarresnet50 triplet 1kresnet50 triplet 200wide resnet50 2resnet50
resnet50 dino
resnet50 moco 200
resnet50 moco 800
resnet50 ressl 200
resnet50 sce 100
resnet50 sce 1000
resnet50 sce 200
resnet50 sce 300
resnet50 swav 200
resnet50 swav 400
resnet50 swav 800
resnet50 trex
resnet50 trexstar
resnet50 triplet 1k
resnet50 triplet 200
wide resnet50 2
0.000.050.100.150.200.25Figure 17: DTW distances associated with the clustering presented in Figure 16. We can see distinct
clusters as revealed by the dendrogram.
25NeurIPS Paper Checklist 627
1.Claims 628
Question: Do the main claims made in the abstract and introduction accurately reflect the 629
paper’s contributions and scope? 630
Answer: [Yes] 631
Justification: Proposition of affinity score and non-linearity signature in Section 3. Experi- 632
ments showing non-linearity signatures of DNNs, prediction of performance, clustering and 633
uniqueness in Section 4. 634
Guidelines: 635
•The answer NA means that the abstract and introduction do not include the claims 636
made in the paper. 637
•The abstract and/or introduction should clearly state the claims made, including the 638
contributions made in the paper and important assumptions and limitations. A No or 639
NA answer to this question will not be perceived well by the reviewers. 640
•The claims made should match theoretical and experimental results, and reflect how 641
much the results can be expected to generalize to other settings. 642
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 643
are not attained by the paper. 644
2.Limitations 645
Question: Does the paper discuss the limitations of the work performed by the authors? 646
Answer: [Yes] 647
Justification: We discuss limitations in Appendix B. 648
Guidelines: 649
•The answer NA means that the paper has no limitation while the answer No means that 650
the paper has limitations, but those are not discussed in the paper. 651
• The authors are encouraged to create a separate "Limitations" section in their paper. 652
•The paper should point out any strong assumptions and how robust the results are to 653
violations of these assumptions (e.g., independence assumptions, noiseless settings, 654
model well-specification, asymptotic approximations only holding locally). The authors 655
should reflect on how these assumptions might be violated in practice and what the 656
implications would be. 657
•The authors should reflect on the scope of the claims made, e.g., if the approach was 658
only tested on a few datasets or with a few runs. In general, empirical results often 659
depend on implicit assumptions, which should be articulated. 660
•The authors should reflect on the factors that influence the performance of the approach. 661
For example, a facial recognition algorithm may perform poorly when image resolution 662
is low or images are taken in low lighting. Or a speech-to-text system might not be 663
used reliably to provide closed captions for online lectures because it fails to handle 664
technical jargon. 665
•The authors should discuss the computational efficiency of the proposed algorithms 666
and how they scale with dataset size. 667
•If applicable, the authors should discuss possible limitations of their approach to 668
address problems of privacy and fairness. 669
•While the authors might fear that complete honesty about limitations might be used by 670
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 671
limitations that aren’t acknowledged in the paper. The authors should use their best 672
judgment and recognize that individual actions in favor of transparency play an impor- 673
tant role in developing norms that preserve the integrity of the community. Reviewers 674
will be specifically instructed to not penalize honesty concerning limitations. 675
3.Theory Assumptions and Proofs 676
Question: For each theoretical result, does the paper provide the full set of assumptions and 677
a complete (and correct) proof? 678
26Answer: [Yes] 679
Justification: Full proofs in Appendix C. 680
Guidelines: 681
• The answer NA means that the paper does not include theoretical results. 682
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 683
referenced. 684
•All assumptions should be clearly stated or referenced in the statement of any theorems. 685
•The proofs can either appear in the main paper or the supplemental material, but if 686
they appear in the supplemental material, the authors are encouraged to provide a short 687
proof sketch to provide intuition. 688
•Inversely, any informal proof provided in the core of the paper should be complemented 689
by formal proofs provided in appendix or supplemental material. 690
• Theorems and Lemmas that the proof relies upon should be properly referenced. 691
4.Experimental Result Reproducibility 692
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 693
perimental results of the paper to the extent that it affects the main claims and/or conclusions 694
of the paper (regardless of whether the code and data are provided or not)? 695
Answer: [Yes] 696
Justification: All models are pretrained checkpoints from torchvision. Experiments are 697
conducted on Imagenet, publicly available. 698
Guidelines: 699
• The answer NA means that the paper does not include experiments. 700
•If the paper includes experiments, a No answer to this question will not be perceived 701
well by the reviewers: Making the paper reproducible is important, regardless of 702
whether the code and data are provided or not. 703
•If the contribution is a dataset and/or model, the authors should describe the steps taken 704
to make their results reproducible or verifiable. 705
•Depending on the contribution, reproducibility can be accomplished in various ways. 706
For example, if the contribution is a novel architecture, describing the architecture fully 707
might suffice, or if the contribution is a specific model and empirical evaluation, it may 708
be necessary to either make it possible for others to replicate the model with the same 709
dataset, or provide access to the model. In general. releasing code and data is often 710
one good way to accomplish this, but reproducibility can also be provided via detailed 711
instructions for how to replicate the results, access to a hosted model (e.g., in the case 712
of a large language model), releasing of a model checkpoint, or other means that are 713
appropriate to the research performed. 714
•While NeurIPS does not require releasing code, the conference does require all submis- 715
sions to provide some reasonable avenue for reproducibility, which may depend on the 716
nature of the contribution. For example 717
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 718
to reproduce that algorithm. 719
(b)If the contribution is primarily a new model architecture, the paper should describe 720
the architecture clearly and fully. 721
(c)If the contribution is a new model (e.g., a large language model), then there should 722
either be a way to access this model for reproducing the results or a way to reproduce 723
the model (e.g., with an open-source dataset or instructions for how to construct 724
the dataset). 725
(d)We recognize that reproducibility may be tricky in some cases, in which case 726
authors are welcome to describe the particular way they provide for reproducibility. 727
In the case of closed-source models, it may be that access to the model is limited in 728
some way (e.g., to registered users), but it should be possible for other researchers 729
to have some path to reproducing or verifying the results. 730
5.Open access to data and code 731
27Question: Does the paper provide open access to the data and code, with sufficient instruc- 732
tions to faithfully reproduce the main experimental results, as described in supplemental 733
material? 734
Answer: [Yes] 735
Justification: Anonymized code to reproduce experiments is available as a zip file, with a 736
README file to explain how to run it. 737
Guidelines: 738
• The answer NA means that paper does not include experiments requiring code. 739
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 740
public/guides/CodeSubmissionPolicy ) for more details. 741
•While we encourage the release of code and data, we understand that this might not be 742
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 743
including code, unless this is central to the contribution (e.g., for a new open-source 744
benchmark). 745
•The instructions should contain the exact command and environment needed to run to 746
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 747
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 748
•The authors should provide instructions on data access and preparation, including how 749
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 750
•The authors should provide scripts to reproduce all experimental results for the new 751
proposed method and baselines. If only a subset of experiments are reproducible, they 752
should state which ones are omitted from the script and why. 753
•At submission time, to preserve anonymity, the authors should release anonymized 754
versions (if applicable). 755
•Providing as much information as possible in supplemental material (appended to the 756
paper) is recommended, but including URLs to data and code is permitted. 757
6.Experimental Setting/Details 758
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 759
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 760
results? 761
Answer: [Yes] 762
Justification: Experimental details are described in Section 4 and Appendix E. 763
Guidelines: 764
• The answer NA means that the paper does not include experiments. 765
•The experimental setting should be presented in the core of the paper to a level of detail 766
that is necessary to appreciate the results and make sense of them. 767
•The full details can be provided either with the code, in appendix, or as supplemental 768
material. 769
7.Experiment Statistical Significance 770
Question: Does the paper report error bars suitably and correctly defined or other appropriate 771
information about the statistical significance of the experiments? 772
Answer: [Yes] 773
Justification: Standard deviations across multiple batch of data are reported. 774
Guidelines: 775
• The answer NA means that the paper does not include experiments. 776
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 777
dence intervals, or statistical significance tests, at least for the experiments that support 778
the main claims of the paper. 779
•The factors of variability that the error bars are capturing should be clearly stated (for 780
example, train/test split, initialization, random drawing of some parameter, or overall 781
run with given experimental conditions). 782
28•The method for calculating the error bars should be explained (closed form formula, 783
call to a library function, bootstrap, etc.) 784
• The assumptions made should be given (e.g., Normally distributed errors). 785
•It should be clear whether the error bar is the standard deviation or the standard error 786
of the mean. 787
•It is OK to report 1-sigma error bars, but one should state it. The authors should 788
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 789
of Normality of errors is not verified. 790
•For asymmetric distributions, the authors should be careful not to show in tables or 791
figures symmetric error bars that would yield results that are out of range (e.g. negative 792
error rates). 793
•If error bars are reported in tables or plots, The authors should explain in the text how 794
they were calculated and reference the corresponding figures or tables in the text. 795
8.Experiments Compute Resources 796
Question: For each experiment, does the paper provide sufficient information on the com- 797
puter resources (type of compute workers, memory, time of execution) needed to reproduce 798
the experiments? 799
Answer: [Yes] 800
Justification: All experiments are carried out on a single A100 GPU. 801
Guidelines: 802
• The answer NA means that the paper does not include experiments. 803
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 804
or cloud provider, including relevant memory and storage. 805
•The paper should provide the amount of compute required for each of the individual 806
experimental runs as well as estimate the total compute. 807
•The paper should disclose whether the full research project required more compute 808
than the experiments reported in the paper (e.g., preliminary or failed experiments that 809
didn’t make it into the paper). 810
9.Code Of Ethics 811
Question: Does the research conducted in the paper conform, in every respect, with the 812
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 813
Answer: [Yes] 814
Justification: Standard and public datasets used, no experiments on human subjects. 815
Guidelines: 816
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 817
•If the authors answer No, they should explain the special circumstances that require a 818
deviation from the Code of Ethics. 819
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 820
eration due to laws or regulations in their jurisdiction). 821
10.Broader Impacts 822
Question: Does the paper discuss both potential positive societal impacts and negative 823
societal impacts of the work performed? 824
Answer: [Yes] 825
Justification: We discuss broader impacts in Appendix A. 826
Guidelines: 827
• The answer NA means that there is no societal impact of the work performed. 828
•If the authors answer NA or No, they should explain why their work has no societal 829
impact or why the paper does not address societal impact. 830
•Examples of negative societal impacts include potential malicious or unintended uses 831
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 832
(e.g., deployment of technologies that could make decisions that unfairly impact specific 833
groups), privacy considerations, and security considerations. 834
29•The conference expects that many papers will be foundational research and not tied 835
to particular applications, let alone deployments. However, if there is a direct path to 836
any negative applications, the authors should point it out. For example, it is legitimate 837
to point out that an improvement in the quality of generative models could be used to 838
generate deepfakes for disinformation. On the other hand, it is not needed to point out 839
that a generic algorithm for optimizing neural networks could enable people to train 840
models that generate Deepfakes faster. 841
•The authors should consider possible harms that could arise when the technology is 842
being used as intended and functioning correctly, harms that could arise when the 843
technology is being used as intended but gives incorrect results, and harms following 844
from (intentional or unintentional) misuse of the technology. 845
•If there are negative societal impacts, the authors could also discuss possible mitigation 846
strategies (e.g., gated release of models, providing defenses in addition to attacks, 847
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 848
feedback over time, improving the efficiency and accessibility of ML). 849
11.Safeguards 850
Question: Does the paper describe safeguards that have been put in place for responsible 851
release of data or models that have a high risk for misuse (e.g., pretrained language models, 852
image generators, or scraped datasets)? 853
Answer: [NA] 854
Justification: No such risks, no checkpoints released. 855
Guidelines: 856
• The answer NA means that the paper poses no such risks. 857
•Released models that have a high risk for misuse or dual-use should be released with 858
necessary safeguards to allow for controlled use of the model, for example by requiring 859
that users adhere to usage guidelines or restrictions to access the model or implementing 860
safety filters. 861
•Datasets that have been scraped from the Internet could pose safety risks. The authors 862
should describe how they avoided releasing unsafe images. 863
•We recognize that providing effective safeguards is challenging, and many papers do 864
not require this, but we encourage authors to take this into account and make a best 865
faith effort. 866
12.Licenses for existing assets 867
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 868
the paper, properly credited and are the license and terms of use explicitly mentioned and 869
properly respected? 870
Answer: [Yes] 871
Justification: Torchvision contributors credited for checkpoints, and datasets as well, in 872
Section 4. 873
Guidelines: 874
• The answer NA means that the paper does not use existing assets. 875
• The authors should cite the original paper that produced the code package or dataset. 876
•The authors should state which version of the asset is used and, if possible, include a 877
URL. 878
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 879
•For scraped data from a particular source (e.g., website), the copyright and terms of 880
service of that source should be provided. 881
•If assets are released, the license, copyright information, and terms of use in the 882
package should be provided. For popular datasets, paperswithcode.com/datasets 883
has curated licenses for some datasets. Their licensing guide can help determine the 884
license of a dataset. 885
•For existing datasets that are re-packaged, both the original license and the license of 886
the derived asset (if it has changed) should be provided. 887
30•If this information is not available online, the authors are encouraged to reach out to 888
the asset’s creators. 889
13.New Assets 890
Question: Are new assets introduced in the paper well documented and is the documentation 891
provided alongside the assets? 892
Answer: [Yes] 893
Justification: Anonymized code to reproduce experiments is available as a zip file, with a 894
README file to explain how to run it. 895
Guidelines: 896
• The answer NA means that the paper does not release new assets. 897
•Researchers should communicate the details of the dataset/code/model as part of their 898
submissions via structured templates. This includes details about training, license, 899
limitations, etc. 900
•The paper should discuss whether and how consent was obtained from people whose 901
asset is used. 902
•At submission time, remember to anonymize your assets (if applicable). You can either 903
create an anonymized URL or include an anonymized zip file. 904
14.Crowdsourcing and Research with Human Subjects 905
Question: For crowdsourcing experiments and research with human subjects, does the paper 906
include the full text of instructions given to participants and screenshots, if applicable, as 907
well as details about compensation (if any)? 908
Answer: [NA] 909
Justification: No experiments on human subjects. 910
Guidelines: 911
•The answer NA means that the paper does not involve crowdsourcing nor research with 912
human subjects. 913
•Including this information in the supplemental material is fine, but if the main contribu- 914
tion of the paper involves human subjects, then as much detail as possible should be 915
included in the main paper. 916
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 917
or other labor should be paid at least the minimum wage in the country of the data 918
collector. 919
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 920
Subjects 921
Question: Does the paper describe potential risks incurred by study participants, whether 922
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 923
approvals (or an equivalent approval/review based on the requirements of your country or 924
institution) were obtained? 925
Answer: [NA] 926
Justification: No experiments on or with human subjects. 927
Guidelines: 928
•The answer NA means that the paper does not involve crowdsourcing nor research with 929
human subjects. 930
•Depending on the country in which research is conducted, IRB approval (or equivalent) 931
may be required for any human subjects research. If you obtained IRB approval, you 932
should clearly state this in the paper. 933
•We recognize that the procedures for this may vary significantly between institutions 934
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 935
guidelines for their institution. 936
•For initial submissions, do not include any information that would break anonymity (if 937
applicable), such as the institution conducting the review. 938
31