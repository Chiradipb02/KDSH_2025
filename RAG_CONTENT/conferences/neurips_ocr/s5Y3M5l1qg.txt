Carefully Blending Adversarial Training and
Purification Improves Adversarial Robustness
Anonymous Author(s)
Affiliation
Address
email
Abstract
In this work, we propose a novel adversarial defence mechanism for image classi- 1
fication – CARSO – blending the paradigms of adversarial training andadversarial 2
purification in a synergistic robustness-enhancing way. The method builds upon 3
an adversarially-trained classifier, and learns to map its internal representation 4
associated with a potentially perturbed input onto a distribution of tentative clean 5
reconstructions. Multiple samples from such distribution are classified by the same 6
adversarially-trained model, and an aggregation of its outputs finally constitutes the 7
robust prediction of interest. Experimental evaluation by a well-established bench- 8
mark of strong adaptive attacks, across different image datasets, shows that CARSO 9
is able to defend itself against adaptive end-to-end white-box attacks devised for 10
stochastic defences. Paying a modest clean accuracy toll, our method improves 11
by a significant margin the state-of-the-art forCIFAR-10 ,CIFAR-100 , and 12
TINYIMAGE NET-200 ℓ∞robust classification accuracy against A UTOATTACK . 13
1 Introduction 14
Vulnerability to adversarial attacks [8, 57] – i.e.the presence of inputs, usually crafted on purpose, 15
capable of catastrophically altering the behaviour of high-dimensional models [9] – constitutes a major 16
hurdle towards ensuring the compliance of deep learning systems with the behaviour expected by 17
modellers and users, and their adoption in safety-critical scenarios or tightly-regulated environments. 18
This is particularly true for adversarially- perturbed inputs, where a norm-constrained perturbation – 19
often hardly detectable by human inspection [48, 5] – is added to an otherwise legitimate input, with 20
the intention of eliciting an anomalous response [34]. 21
Given the widespread nature of the issue [30], and the serious concerns raised about the safety and 22
reliability of data-learnt models in the lack of an appropriate mitigation [7], adversarial attacks have 23
been extensively studied. Yet, obtaining generally robust machine learning ( ML) systems remains a 24
longstanding issue, and a major open challenge. 25
Research in the field has been driven by two opposing, yet complementary, efforts. On the one 26
hand, the study of failure modes in existing models and defences, with the goal of understanding 27
their origin and developing stronger attacks with varying degrees of knowledge and control over the 28
target system [57, 21, 44, 60]. On the other hand, the construction of increasingly capable defence 29
mechanisms. Although alternatives have been explored [15, 59, 11, 68], most of the latter is based on 30
adequately leveraging adversarial training [21, 42, 58, 49, 23, 31, 54, 62, 18, 47], i.e.training a ML 31
model on a dataset composed of (or enriched with) adversarially-perturbed inputs associated with 32
their correct, pre-perturbation labels. In fact, adversarial training has been the only technique capable 33
of consistently providing an acceptable level of defence [24], while still incrementally improving up 34
to the current state-of-the-art [18, 47]. 35
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Another defensive approach is that of adversarial purification [53, 66], where a generative model is 36
used – similarly to denoising – to recover a perturbation-free version of the input before classification 37
is performed. Nonetheless, such attempts have generally fallen short of expectations due to inherent 38
limitations of the generative models used in early attempts [45], or due to decreases in robust 39
accuracy1when attacked end-to-end [25] – resulting in subpar robustness if the defensive structure is 40
known to the adversary [60]. More recently, the rise of diffusion-based generative models [28] and 41
their use for purification have enabled more successful results of this kind [45, 13] – although at the 42
cost of much longer training and inference times, and a much brittler robustness evaluation [13, 38]. 43
In this work, we design a novel adversarial defence for supervised image classification, dubbed 44
CARSO (i.e., Counter-Adversarial Recall of Synthetic Observations). The approach relies on an 45
adversarially-trained classifier (called hereinafter simply the classifier ), endowed with a stochastic 46
generative model (called hereinafter the purifier ). Upon classification of a potentially-perturbed input, 47
the latter learns to generate – from the tensor2of (pre)activations registered at neuron level in the 48
former – samples from a distribution of plausible, perturbation-free reconstructions. At inference 49
time, some of these samples are classified by the very same classifier , and the original input is robustly 50
labelled by aggregating its many outputs. This method – to the best of our knowledge the first attempt 51
to organically merge the adversarial training andpurification paradigms – avoids the vulnerability 52
pitfalls typical of the mere stacking of a purifier and a classifier [25], while still being able to take 53
advantage of independent incremental improvements to adversarial training or generative modelling. 54
An empirical assessment3of the defence in the ℓ∞white-box setting is provided, using a conditional 55
[56, 64] variational autoencoder [32, 50] as the purifier and existing state-of-the-art adversarially 56
pre-trained models as classifiers. Such choices are meant to give existing approaches – and the 57
adversary attacking our architecture end-to-end as part of the assessment – the strongest advantage 58
possible. Yet, in all scenarios considered, CARSO improves significantly the robustness of the 59
pre-trained classifier – even against attacks specifically devised to fool stochastic defences like ours. 60
Remarkably, with a modest clean accuracy toll, our method improves by a significant margin the 61
current state-of-the-art forCIFAR -10[33], CIFAR -100 [33], and TINYIMAGE NET-200 [14]ℓ∞ 62
robust classification accuracy against A UTOATTACK [17]. 63
In summary, the paper makes the following contributions: 64
•The description of CARSO , a novel adversarial defence method synergistically blending 65
adversarial training andadversarial purification ; 66
•A collection of relevant technical details fundamental to its successful training and use, 67
originally developed for the purifier being a conditional variational autoencoder – but 68
applicable to more general scenarios as well; 69
•Experimental assessment of the method, against standardised benchmark adversarial attacks 70
– showing higher robust accuracy w.r.t. to existing state-of-the-art adversarial training and 71
purification approaches. 72
The rest of the manuscript is structured as follows. In section 2 we provide an overview of selected 73
contributions in the fields of adversarial training andpurification-based defences – with focus on 74
image classification. In section 3, a deeper analysis is given of two integral parts of our experimental 75
assessment: PGDadversarial training and conditional variational autoencoders. Section 4 is devoted 76
to the intuition behind CARSO , its architectural description, and the relevant technical details that 77
allow it to work. Section 5 contains details about the experimental setup, results, comments, and 78
limitations. Section 6 concludes the paper and outlines directions of future development. 79
2 Related work 80
Adversarial training as a defence The idea of training a model on adversarially-generated examples 81
as a way to make it more robust can be traced back to the very beginning of research in the area. 82
1Thetest set accuracy of the frozen-weights trained classifier – computed on a dataset entirely composed of
adversarially-perturbed examples generated against that specific model.
2Which we call internal representation .
3Implementation of the method and code for the experiments (based on PyTorch [46], AdverTorch [19], and
ebtorch [4]) can be found in the supplementary materials of the paper.
2The seminal [57] proposes to perform training on a mixed collection of clean and adversarial data, 83
generated beforehand. 84
The introduction of FGSM [21] enables the efficient generation of adversarial examples along the 85
training, with a single normalised gradient step. Its iterative counterpart PGD[42] – discussed 86
in section 3 and Appendix A – significantly improves the effectiveness of adversarial examples 87
produced, making it still the de facto standard for the synthesis of adversarial training inputs [24]. 88
Further incremental improvements have also been developed, some focused specifically on robustness 89
assessment ( e.g.adaptive-stepsize variants, as in [17]). 90
The most recent adversarial training protocols further rely on synthetic data to increase the numerosity 91
of training datapoints [23, 49, 62, 18, 47], and adopt adjusted loss functions to balance robustness and 92
accuracy [67] or generally foster the learning process [18]. The entire model architecture may also be 93
tuned specifically for the sake of robustness enhancement [47]. At least some of such ingredients are 94
often required to reach the current state-of-the-art in robust accuracy via adversarial training. 95
Purification as a defence Amongst the first attempts of purification-based adversarial defence, 96
[25] investigates the use of denoising autoencoders [61] to recover examples free from adversarial 97
perturbations. Despite its effectiveness in the denoising task, the method may indeed increase 98
the vulnerability of the system when attacks are generated against it end-to-end . The contextually 99
proposed improvement adds a smoothness penalty to the reconstruction loss, partially mitigating such 100
downside [25]. Similar in spirit, [39] tackles the issue by computing the reconstruction loss between 101
the last-layers representations of the frozen-weights attacked classifier, respectively receiving, as 102
input, the clean and the tentatively denoised example. 103
In [52], Generative Adversarial Networks (GANs) [22] learnt on clean data are used at inference time 104
to find a plausible synthetic example – close to the perturbed input – belonging to the unperturbed 105
data manifold. Despite encouraging results, the delicate training process of GANs and the existence 106
of known failure modes [70] limit the applicability of the method. More recently, a similar approach 107
[27] employing energy-based models [37] suffered from poor sample quality [45]. 108
Purification approaches based on (conditional) variational autoencoders include [29] and [53]. Very 109
recently, a technique combining variational manifold learning with a test-time iterative purification 110
procedure has also been proposed [65]. 111
Finally, already-mentioned techniques relying on score- [66] and diffusion- based [45, 13] models 112
have also been developed, with generally favourable results – often balanced in practice by longer 113
training and inference times, and a much more fragile robustness assessment [13, 38]. 114
3 Preliminaries 115
PGDadversarial training The task of finding model parameters robust to adversarial perturbations 116
is framed by [42] as a min-max optimisation problem seeking to minimise adversarial risk . The 117
inner optimisation ( i.e., the generation of worst-case adversarial examples) is solved by an iterative 118
algorithm – Projected Gradient Descent – interleaving gradient ascent steps in input space with 119
the eventual projection on the shell of an ϵ-ball centred around an input datapoint, thus imposing a 120
perturbation strength constraint. 121
In this manuscript, we will use the shorthand notation ϵpto denote ℓpnorm-bound perturbations of 122
maximum magnitude ϵ. 123
The formal details of such method are provided in Appendix A. 124
(Conditional) variational autoencoders Variational autoencoders ( VAEs) [32, 50] allow the learn- 125
ing from data of approximate generative latent-variable models of the form p(x,z) =p(x|z)p(z), 126
whose likelihood and posterior are approximately parameterised by deep artificial neural networks 127
(ANN s). The problem is cast as the maximisation of a variational lower bound. 128
In practice, optimisation is performed iteratively – on a loss function given by the linear mixture of 129
data-reconstruction loss and empirical KLdivergence w.r.t. a chosen prior, computed on mini-batches 130
of data. 131
3Conditional Variational Autoencoders [56, 64] extend VAEs by attaching a conditioning tensor c– 132
expressing specific characteristics of each example – to both xandzduring training. This allows the 133
learning of a decoder model capable of conditional data generation. 134
Further details on the functioning of such models are given in Appendix B. 135
4 Structure of C ARSO 136
The core ideas informing the design of our method are driven more by first principles rather than 137
arising from specific contingent requirements. This section discusses such ideas, the architectural 138
details of CARSO , and a group of technical aspects fundamental to its training and inference processes. 139
4.1 Architectural overview and principle of operation 140
From an architectural point of view, CARSO is essentially composed of two ANN models – a classifier 141
and a purifier – operating in close synergy. The former is trained on a given classification task, 142
whose inputs might be adversarially corrupted at inference time. The latter learns to generate samples 143
from a distribution of potential input reconstructions, tentatively free from adversarial perturbations. 144
Crucially, the purifier has only access to the internal representation of the classifier – and not even 145
directly to the perturbed input – to perform its task. 146
During inference, for each input, the internal representation of the classifier is used by the purifier to 147
synthesise a collection of tentatively unperturbed input reconstructions. Those are classified by the 148
same classifier , and the resulting outputs are aggregated into a final robust prediction . 149
There are no specific requirements for the classifier, whose training is completely independent of 150
the use of the model as part of CARSO . However, training it adversarially improves significantly 151
theclean accuracy of the overall system, allowing it to benefit from established adversarial training 152
techniques. 153
The purifier is also independent of specific architectural choices, provided it is capable of stochastic 154
conditional data generation at inference time, with the internal representation of the classifier used as 155
the conditioning set. 156
In the rest of the paper, we employ a state-of-the-art adversarially pre-trained WIDERESNETmodel 157
as the classifier, and a purpose-built conditional variational autoencoder as the purifier, the latter 158
operating decoder-only during inference. Such choice was driven by the deliberate intention to assess 159
the adversarial robustness of our method in its worst-case scenario against a white-box attacker, and 160
with the least advantage compared to existing approaches based solely on adversarial training. 161
In fact, the decoder of a conditional V AE allows for exact algorithmic differentiability [6] w.r.t. its 162
conditioning set, thus averting the need for backward-pass approximation [2] in generating end-to-end 163
adversarial attacks against the entire system, and preventing (un)intentional robustness by gradient 164
obfuscation [2]. The same cannot be said [13] for more capable and modern purification models, 165
such as those based e.g.on diffusive processes, whose robustness assessment is still in the process of 166
being understood [38]. 167
A downside of such choice is represented by the reduced effectiveness of the decoder in the synthesis 168
of complex data, due to well-known model limitations. In fact, we experimentally observe a modest 169
increase in reconstruction cost for non-perturbed inputs, which in turn may limit the clean accuracy of 170
the entire system. Nevertheless, we defend the need for a fair and transparent robustness evaluation, 171
such as the one provided by the use of a V AE-based purifier, in the evaluation of any novel architecture- 172
agnostic adversarial defence technique. 173
A diagram of the whole architecture is shown in Figure 1, and its detailed principles of operation are 174
recapped below. 175
Training At training time, adversarially-perturbed examples are generated against the classifier , 176
and fed to it. The tensors containing the classifier (pre)activations across the network are then 177
extracted. Finally, the conditional VAE serving as the purifier is trained on perturbation-free input 178
reconstruction, conditional on the corresponding previously extracted internal representations, and 179
using pre-perturbation examples as targets. 180
4Figure 1: Schematic representation of the CARSO architecture used in the experimental phase of this
work. The subnetwork bordered by the red dashed line is used only during the training of the purifier .
The subnetwork bordered by the blue dashed line is re-evaluated on different random samples ziand
the resulting individual ˆyiare aggregated into ˆyrob. The classifier f(·;θ)is always kept frozen; the
remaining network is trained on LV AE(x,ˆx). More precise details on the functioning of the networks
are provided in subsection 4.1.
Upon completion of the training process, the encoder network may be discarded as it will not be used 181
for inference. 182
Inference The example requiring classification is fed to the classifier . Its corresponding internal 183
representation is extracted and used to condition the generative process described by the decoder 184
of the VAE. Stochastic latent variables are repeatedly sampled from the original priors, which are 185
given by an i.i.d. multivariate Standard Normal distribution. Each element in the resulting set of 186
reconstructed inputs is classified by the same classifier , and the individually predicted class logits are 187
aggregated. The result of such aggregation constitutes the robust prediction of the input class. 188
Remarkably, the only link between the initial potentially-perturbed input and the resulting purified 189
reconstructions (and thus the predicted class) is through the internal representation of the classifier, 190
which serves as a featurisation of the original input. The whole process is exactly differentiable end- 191
to-end , and the only potential hurdle to the generation of adversarial attacks against the entire system 192
is the stochastic nature of the decoding – which is easily tackled by Expectation over Transformation 193
[3]. 194
4.2 A first-principles justification 195
If we consider a trained ANN classifier, subject to a successful adversarial attack by means of a 196
slightly perturbed example, we observe that – both in terms of ℓpmagnitude and human perception 197
– a small variation on the input side of the network is amplified to a significant amount on the 198
output side, thanks to the layerwise processing by the model. Given the deterministic nature of such 199
processing at inference time, we speculate that the trace obtained by sequentially collecting the 200
(pre)activation values within the network, along the forward pass, constitutes a richer characterisation 201
of such an amplification process compared to the knowledge of the input alone. Indeed, as we do, it 202
is possible to learn a direct mapping from such featurisation of the input, to a distribution of possible 203
perturbation-free input reconstructions – taking advantage of such characterisation. 204
4.3 Hierarchical input and internal representation encoding 205
Training a conditional V AE requires [56] that the conditioning set cis concatenated to the input x 206
before encoding occurs, and to the sample of latent variables zright before decoding. The same is 207
5also true, with the suitable adjustments, for any conditional generative approach where the target and 208
the conditioning set must be processed jointly. 209
In order to ensure the usability and scalability of CARSO across the widest range of input data and 210
classifier models, we propose to perform such processing in a hierarchical and partially disjoint 211
fashion between the input and the conditioning set. In principle, the encoding of xandccan be 212
performed by two different and independent subnetworks, until some form of joint processing must 213
occur. This allows to retain the overall architectural structure of the purifier, while having finer-grained 214
control over the inductive biases [43] deemed the most suitable for the respective variables. 215
In the experimental phase of our work, we encode the two variables independently. The input 216
is compressed by a multilayer convolutional neural network (CNN). The internal representation – 217
which in our case is composed of differently sized multi-channel images – is processed layer by 218
layer by independent multilayer CNNs (responsible for encoding local information), whose flattened 219
outputs are finally concatenated and compressed by a fully-connected layer (modelling inter-layer 220
correlations in the representation). The resulting compressed input and conditioning set are then 221
further concatenated and jointly encoded by a fully-connected network (FCN). 222
In order to use the V AE decoder at inference time, the entire compression machinery for the condi- 223
tioning set must be preserved after training, and used to encode the internal representations extracted. 224
The equivalent input encoder may be discarded instead. 225
4.4 Adversarially-balanced batches 226
Training the purifier in representation-conditional input reconstruction requires having access to 227
adversarially-perturbed examples generated against the classifier, and to the corresponding clean data. 228
Specifically, we use as input a mixture of clean and adversarially perturbed examples, and the clean 229
input as the target. 230
Within each epoch, the training set of interest is shuffled [51, 10], and only a fixed fraction of each 231
resulting batch is adversarially perturbed. Calling ϵthe maximum ℓpperturbation norm bound for 232
the threat model against which the classifier was adversarially pre-trained, the portion of perturbed 233
examples is generated by an even split of F GSM ϵ/2, PGDϵ/2, FGSM ϵ, and P GDϵattacks. 234
Any smaller subset of attack types and strengths, or a detailedly unbalanced batch composition, 235
always experimentally results in a worse performing purification model. More details justifying such 236
choice are provided in Appendix C. 237
4.5 Robust aggregation strategy 238
At inference time, many different input reconstructions are classified by the classifier , and the 239
respective outputs concur to the settlement of a robust prediction . 240
Calling lα
ithe output logit associated with class i∈ {1, . . . , C }in the prediction by the classifier on 241
sample α∈ {1, . . . , N }, we adopt the following aggregation strategy: 242
Pi:=1
ZNY
α=1eelα
i
withPibeing the aggregated probability of membership in class i,Za normalisation constant such 243
thatPC
i=1Pi= 1, and eEuler’s number. 244
Such choice produces a robust prediction much harder to take over in the event that an adversary 245
selectively targets a specific input reconstruction. A heuristic justification for this property is given in 246
Appendix D. 247
5 Experimental assessment 248
Experimental evaluation of our method is carried out in terms of robust andclean image classification 249
accuracy within three different scenarios ( a,bandc), determined by the specific classification task. 250
6The white-box threat model with a fixed ℓ∞norm bound is assumed throughout, as it generally 251
constitutes the most demanding setup for adversarial defences. 252
5.1 Setup 253
Data TheCIFAR -10[33] dataset is used in scenario (a) , the CIFAR -100 [33] dataset is used in 254
scenario (b) , whereas the T INYIMAGE NET-200 [14] dataset is used in scenario (c) . 255
Architectures AWIDERESNET-28-10 model is used as the classifier , adversarially pre-trained on 256
the respective dataset – the only difference between scenarios being the number of output logits: 10 257
inscenario (a) ,100inscenario (b) , and 200inscenario (c) . 258
The purifier is composed of a conditional V AE, processing inputs and internal representations in a 259
partially disjoint fashion, as explained in subsection 4.3. The input is compressed by a two-layer 260
CNN; the internal representation is instead processed layerwise by independent CNNs (three-layered 261
inscenarios (a) and(b), four-layered in scenario (c) ) whose outputs are then concatenated and 262
compressed by a fully-connected layer. A final two-layer FCN jointly encodes the compressed input 263
and conditioning set, after the concatenation of the two. A six-layer deconvolutional network is used 264
as the decoder. 265
More precise details on all architectures are given in Appendix E. 266
Outer minimisation Inscenarios (a) and(b), the classifier is trained according to [18]; in scenario 267
(c), according to [62]. Classifiers were always acquired as pre-trained models, using publicly available 268
weights provided by the respective authors. 269
Thepurifier is trained on the VAE loss, using summed pixel-wise channel-wise binary cross-entropy 270
as the reconstruction cost. Optimisation is performed by RA DAM +LOOKAHEAD [41, 69] with a 271
learning rate schedule that presents a linear warm-up, a plateau phase, and a linear annealing [55]. 272
To promote the learning of meaningful reconstructions during the initial phases of training, the KL 273
divergence term in the V AE loss is suppressed for an initial number of epochs. Afterwards, it is 274
linearly modulated up to its actual value, during a fixed number of epochs ( βincrease ) [26]. The 275
initial and final epochs of such modulation are reported in Table 14. 276
Additional scenario-specific details are provided in Appendix E. 277
Inner minimisation ϵ∞=8/255is set as the perturbation norm bound. 278
Adversarial examples against the purifier are obtained, as explained in subsection 4.4, by FGSM ϵ/2, 279
PGDϵ/2,FGSM ϵ, and PGDϵ, in a class-untargeted fashion on the cross-entropy loss. In the case of 280
PGD, gradient ascent with a step size of α= 0.01is used. 281
The complete details and hyperparameters of the attacks are described in Appendix E. 282
Evaluation In each scenario, we report the clean androbust test-set accuracy – the latter by means 283
of A UTOATTACK [17] – of the classifier and the corresponding C ARSO architecture. 284
For the classifier alone, the standard version of AUTOATTACK (AA) is used: i.e., the worst-case 285
accuracy on a mixture of AUTOPGDon the cross-entropy loss [17] with 100steps, AUTOPGDon 286
thedifference of logits ratio loss [17] with 100steps, FAB[16] with 100steps, and the black-box 287
SQUARE attack [1] with 5000 queries. 288
In the evaluation of the CARSO architecture, the number of reconstructed samples per input is set to 8, 289
the logits are aggregated as explained in subsection 4.5, and the output class is finally selected as the 290
arg max of the aggregation. Due to the stochastic nature of the purifier , robust accuracy is assessed 291
by a version of AUTOATTACK suitable for stochastic defences ( randAA ) – composed of AUTOPGD 292
on the cross-entropy and difference of logits ratio losses, across 20Expectation over Transformation 293
(EOT) [3] iterations with 100gradient ascent steps each. 294
Computational infrastructure All experiments were performed on an NVIDIA DGX A100 system. 295
Training in scenarios (a) and(c)was run on 8 NVIDIA A100 GPUs with 40 GB of dedicated memory 296
each; in scenario (b) 4 of such devices were used. Elapsed real training time for the purifier in all 297
scenarios is reported in Table 1. 298
7Table 1: Elapsed real running time for training the purifier in the different scenarios considered.
Scenario (a) (b) (c)
Elapsed real training time 159 min 138 min 213 min
5.2 Results and discussion 299
An analysis of the experimental results is provided in the subsection that follows, whereas their 300
systematic exposition is given in Table 2. 301
Table 2: Clean (results in italic ) and adversarial (results in upright) accuracy for the different models and
datasets used in the respective scenarios. The following abbreviations are used: Scen : scenario considered;
AT/Cl : clean accuracy for the adversarially-pretrained model used as the classifier , when considered alone;
C/Cl : clean accuracy for the CARSO architecture; AT/AA : robust accuracy (by the means of AUTOATTACK ) for
the adversarially-pretrained model used as the classifier , when considered alone; C/randAA : robust accuracy for
theCARSO architecture, when attacked end-to-end byAUTOATTACK for randomised defences; Best AT/AA :
best robust accuracy result for the respective dataset (by the means of AUTOATTACK ), obtained by adversarial
training alone (any model); Best P/AA : best robust accuracy result for the respective dataset (by the means
ofAUTOATTACK ), obtained by adversarial purification (any model). Robust accuracies in round brackets are
obtained using the PGD+EOT[38] pipeline, developed for diffusion-based purifiers. The best clean and robust
accuracies per dataset are shown in bold . The clean accuracies for the models referred to in the Best columns
are shown in Table 15 (in Appendix F).
Scen. Dataset AT/Cl C/Cl AT/AAC/rand-AA
(PGD+EOT)Best AT/AABest P/AA
(PGD+EOT)
(a) C IFAR -10 0.9216 0.8686 0.67730.7613
(0.7689)0.71070.7812
(0.6641)
(b) C IFAR -100 0.7385 0.6806 0.3918 0.6665 0.4267 0.4609
(c) T INYIMAGE NET-200 0.6519 0.5632 0.3130 0.5356 0.3130
Scenario (a) Comparing the robust accuracy of the classifier model used in scenario (a) [18] 302
with that resulting from the inclusion of the same model in the CARSO architecture, we observe 303
a+8.4%increase. This is counterbalanced by a −5.6%clean accuracy toll. The same version of 304
CARSO further provides a +5.03robustness increase w.r.t. the current best AT-trained model [47] 305
that employs a ∼3×larger R AWIDERESNET-70-16 model. 306
In addition, our method provides a remarkable +9.72% increase in robust accuracy w.r.t. to the best 307
adversarial purification approach [40], a diffusion-based purifier. However, the comparison is not as 308
straightforward. In fact, the paper [40] reports a robust accuracy of 78.12% using AUTOATTACK on 309
the gradients obtained via the adjoint method [45]. As noted in [38], such evaluation (which uses the 310
version of AUTOATTACK that is unsuitable for stochastic defences) leads to a large overestimation of 311
the robustness of diffusive purifiers. As suggested in [38], the authors of [40] re-evaluate the robust 312
accuracy according to a more suitable pipeline ( PGD+EOT, whose hyperparameters are shown in 313
Table 12), obtaining a much lower robust accuracy of 66.41%. Consequently, we repeat the same 314
evaluation for CARSO and compare the worst-case robustness amongst the two. In line with typical 315
AT methods, and unlike diffusive purification, the robustness of CARSO assessed by means of randAA 316
is still lower w.r.t. than achieved by P GD+EOT. 317
Scenario (b) Moving to scenario (b) ,CARSO achieves a robust accuracy increase of +27.47% w.r.t. 318
theclassifier alone [18], balanced by a 5.79% decrease in clean accuracy. Our approach also improves 319
upon the robust accuracy of the best AT-trained model [62] ( WIDERESNET-70-16 ) by23.98%. In 320
the absence of a reliable robustness evaluation by means of PGD+EOTfor the best purification-based 321
method [40], we still obtain a +20.25% increase in robust accuracy upon its (largely overestimated) 322
AA result. 323
Scenario (c) Inscenario (c) ,CARSO improves upon the classifier alone [62] (which is also the 324
best AT-based approach for TINYIMAGE NET-200 ) by+22.26%. A significant clean accuracy toll is 325
8imposed by the relative complexity of the dataset, i.e.−8.87%. In this setting, we lack any additional 326
purification-based methods. 327
Assessing the impact of gradient obfuscation Although the architecture of CARSO is algorith- 328
mically differentiable end-to-end – and the integrated diagnostics of the randAA routines raised no 329
warnings during the assessment – we additionally guard against the eventual gradient obfuscation [2] 330
induced by our method by repeating the evaluation at ϵ∞= 0.95, verifying that the resulting robust 331
accuracy stays below random chance [12]. Results are shown in Table 3. 332
Table 3: Robust classification accuracy against AUTOATTACK , forϵ∞= 0.95, as a way to assess the (lack of)
impact of gradient obfuscation on robust accuracy evaluation.
Scenario (a) (b) (c)
ϵ∞= 0.95acc. <0.047 <0.010 ≈0.0
5.3 Limitations and open problems 333
In line with recent research aiming at the development of robust defences against multiple perturb- 334
ations [20, 35], our method determines a decrease in clean accuracy w.r.t. the original model on 335
which it is built upon – especially in scenario (c) as the complexity of the dataset increases. This 336
phenomenon is partly dependent on the choice of a V AE as the generative purification model, a 337
requirement for the fairest evaluation possible in terms of robustness. 338
Yet, the issue remains open: is it possible to devise a CARSO -like architecture capable of the same 339
– if not better – robust behaviour, which is also competitively accurate on clean inputs? Potential 340
avenues for future research may involve the development of CARSO -like architectures in which 341
representation-conditional data generation is obtained by means of diffusion or score-based models. 342
Alternatively, incremental developments aimed at improving the cross-talk between the purifier and 343
the final classifier may be pursued. 344
Lastly, the scalability of CARSO could be strongly improved by determining whether the internal 345
representation used in conditional data generation may be restricted to a smaller subset of layers, 346
while still maintaining the general robustness of the method. 347
6 Conclusion 348
In this work, we presented a novel adversarial defence mechanism tightly integrating input purification, 349
and classification by an adversarially-trained model – in the form of representation-conditional data 350
purification. Our method is able to improve upon the current state-of-the-art inCIFAR -10,CIFAR - 351
100, and TINYIMAGE NETℓ∞robust classification, w.r.t. both adversarial training andpurification 352
approaches alone. 353
Such results suggest a new synergistic strategy to achieve adversarial robustness in visual tasks and 354
motivate future research on the application of the same design principles to different models and 355
types of data. 356
9References 357
[1] Maksym Andriushchenko et al. ‘Square Attack: a query-efficient black-box adversarial attack via random 358
search’. In: 16th European Conference on Computer Vision . 2020. 359
[2] Anish Athalye, Nicholas Carlini and David Wagner. ‘Obfuscated Gradients Give a False Sense of Security: 360
Circumventing Defenses to Adversarial Examples’. In: Proceedings of the International Conference on 361
Machine Learning . 2018. 362
[3] Anish Athalye et al. ‘Synthesizing Robust Adversarial Examples’. In: Proceedings of the International 363
Conference on Machine Learning . 2018. 364
[4] Emanuele Ballarin. ebtorch : Collection of PyTorch additions, extensions, utilities, uses and abuses . 365
2024. URL:https://github.com/emaballarin/ebtorch . 366
[5] Vincent Ballet et al. ‘Imperceptible Adversarial Attacks on Tabular Data’. In: Thirty-third Conference on 367
Neural Information Processing Systems, Workshop on Robust AI in Financial Services: Data, Fairness, 368
Explainability, Trustworthiness, and Privacy (Robust AI in FS) . 2019. 369
[6] Atilim Gunes Baydin et al. ‘Automatic differentiation in machine learning: a survey’. In: The Journal of 370
Machine Learning Research 18.153 (2018), pp. 1–43. 371
[7] Battista Biggio and Fabio Roli. ‘Wild patterns: Ten years after the rise of adversarial machine learning’. 372
In:Pattern Recognition 84 (2018), pp. 317–331. 373
[8] Battista Biggio et al. ‘Evasion Attacks against Machine Learning at Test Time’. In: Proceedings of the 374
2013th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume 375
Part III . 2013. 376
[9] Luca Bortolussi and Guido Sanguinetti. Intrinsic Geometric Vulnerability of High-Dimensional Artificial 377
Intelligence . 2018. arXiv: 1811.03571 . 378
[10] Léon Bottou. ‘On-Line Algorithms and Stochastic Approximations’. In: On-Line Learning in Neural 379
Networks . Cambridge University Press, 1999. Chap. 2. 380
[11] Ginevra Carbone et al. ‘Robustness of Bayesian Neural Networks to Gradient-Based Attacks’. In: 381
Advances in Neural Information Processing Systems . 2020. 382
[12] Nicholas Carlini et al. On Evaluating Adversarial Robustness . 2019. arXiv: 1902.06705 . 383
[13] Huanran Chen et al. Robust Classification via a Single Diffusion Model . 2023. arXiv: 2305.15241 . 384
[14] Patryk Chrabaszcz, Ilya Loshchilov and Frank Hutter. A Downsampled Variant of ImageNet as an 385
Alternative to the CIFAR datasets . 2017. arXiv: 1707.08819 . 386
[15] Moustapha Cisse et al. ‘Parseval Networks: Improving Robustness to Adversarial Examples’. In: Pro- 387
ceedings of the International Conference on Machine Learning . 2017. 388
[16] Francesco Croce and Matthias Hein. Minimally distorted Adversarial Examples with a Fast Adaptive 389
Boundary Attack . 2020. arXiv: 1907.02044 . 390
[17] Francesco Croce and Matthias Hein. ‘Reliable Evaluation of Adversarial Robustness with an Ensemble of 391
Diverse Parameter-free Attacks’. In: Proceedings of the International Conference on Machine Learning . 392
2020. 393
[18] Jiequan Cui et al. Decoupled Kullback-Leibler Divergence Loss . 2023. arXiv: 2305.13948 . 394
[19] Gavin Weiguang Ding, Luyu Wang and Xiaomeng Jin. AdverTorch v0.1: An Adversarial Robustness 395
Toolbox based on PyTorch . 2019. arXiv: 1902.07623 . 396
[20] Hadi M. Dolatabadi, Sarah Erfani and Christopher Leckie. ‘ ℓ∞-Robustness and Beyond: Unleashing 397
Efficient Adversarial Training’. In: 18th European Conference on Computer Vision . 2022. 398
[21] Ian Goodfellow, Jonathon Shlens and Christian Szegedy. ‘Explaining and Harnessing Adversarial Ex- 399
amples’. In: International Conference on Learning Representations . 2015. 400
[22] Ian Goodfellow et al. ‘Generative Adversarial Nets’. In: Advances in Neural Information Processing 401
Systems . 2014. 402
[23] Sven Gowal et al. ‘Improving Robustness using Generated Data’. In: Advances in Neural Information 403
Processing Systems . 2021. 404
[24] Sven Gowal et al. Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial 405
Examples . 2020. arXiv: 2010.03593 . 406
[25] Shixiang Gu and Luca Rigazio. ‘Towards Deep Neural Network Architectures Robust to Adversarial 407
Examples’. In: Workshop Track of the International Conference on Learning Representations . 2015. 408
[26] Irina Higgins et al. ‘beta-V AE: Learning Basic Visual Concepts with a Constrained Variational Frame- 409
work’. In: International Conference on Learning Representations . 2017. 410
[27] Mitch Hill, Jonathan Mitchell and Song-Chun Zhu. ‘Stochastic Security: Adversarial Defense Using Long- 411
Run Dynamics of Energy-Based Models’. In: International Conference on Learning Representations . 412
2021. 413
[28] Chin-Wei Huang, Jae Hyun Lim and Aaron C Courville. ‘A Variational Perspective on Diffusion-Based 414
Generative Models and Score Matching’. In: Advances in Neural Information Processing Systems . 2021. 415
10[29] Uiwon Hwang et al. ‘PuV AE: A Variational Autoencoder to Purify Adversarial Examples’. In: IEEE 416
Access . 2019. 417
[30] Andrew Ilyas et al. ‘Adversarial Examples Are Not Bugs, They Are Features’. In: Advances in Neural 418
Information Processing Systems . 2019. 419
[31] Xiaojun Jia et al. ‘LAS-AT: Adversarial Training With Learnable Attack Strategy’. In: Proceedings of the 420
IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2022. 421
[32] Diederik P. Kingma and Max Welling. ‘Auto-Encoding Variational Bayes’. In: International Conference 422
on Learning Representations . 2014. 423
[33] Alex Krizhevsky. ‘Learning Multiple Layers of Features from Tiny Images’. In: 2009. 424
[34] Alexey Kurakin, Ian J. Goodfellow and Samy Bengio. ‘Adversarial Examples in the Physical World’. In: 425
Artificial Intelligence Safety and Security (2018). 426
[35] Cassidy Laidlaw, Sahil Singla and Soheil Feizi. ‘Perceptual Adversarial Robustness: Defense Against 427
Unseen Threat Models’. In: International Conference on Learning Representations . 2021. 428
[36] Yann LeCun and Corinna Cortes. The MNIST handwritten digit database . 2010. 429
[37] Yann LeCun et al. ‘A tutorial on energy-based learning’. In: Predicting structured data . MIT Press, 2006. 430
Chap. 1. 431
[38] Minjong Lee and Dongwoo Kim. ‘Robust Evaluation of Diffusion-Based Adversarial Purification’. In: 432
International Conference on Computer Vision . 2024. 433
[39] Fangzhou Liao et al. ‘Defense Against Adversarial Attacks Using High-Level Representation Guided 434
Denoiser’. In: IEEE Conference on Computer Vision and Pattern Recognition . 2018. 435
[40] Guang Lin et al. Robust Diffusion Models for Adversarial Purification . 2024. arXiv: 2403.16067 . 436
[41] Liyuan Liu et al. ‘On the Variance of the Adaptive Learning Rate and Beyond’. In: International 437
Conference on Learning Representations . 2020. 438
[42] Aleksander Madry et al. ‘Towards Deep Learning Models Resistant to Adversarial Attacks’. In: Interna- 439
tional Conference on Learning Representations . 2018. 440
[43] Tom M. Mitchell. The Need for Biases in Learning Generalizations . Tech. rep. New Brunswick, NJ: 441
Rutgers University, 1980. 442
[44] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi and Pascal Frossard. ‘DeepFool: A Simple and 443
Accurate Method to Fool Deep Neural Networks’. In: IEEE Conference on Computer Vision and Pattern 444
Recognition . 2016. 445
[45] Weili Nie et al. ‘Diffusion Models for Adversarial Purification’. In: Proceedings of the International 446
Conference on Machine Learning . 2022. 447
[46] Adam Paszke et al. ‘PyTorch: An Imperative Style, High-Performance Deep Learning Library’. In: 448
Advances in Neural Information Processing Systems . 2019. 449
[47] ShengYun Peng et al. Robust Principles: Architectural Design Principles for Adversarially Robust CNNs . 450
2023. 451
[48] Yao Qin et al. ‘Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recog- 452
nition’. In: Proceedings of the International Conference on Machine Learning . 2019. 453
[49] Sylvestre-Alvise Rebuffi et al. ‘Data Augmentation Can Improve Robustness’. In: Advances in Neural 454
Information Processing Systems . 2021. 455
[50] Danilo Jimenez Rezende, Shakir Mohamed and Daan Wierstra. ‘Stochastic Backpropagation and Ap- 456
proximate Inference in Deep Generative Models’. In: Proceedings of the International Conference on 457
Machine Learning . 2014. 458
[51] Herbert Robbins and Sutton Monro. ‘A Stochastic Approximation Method’. In: The Annals of Mathemat- 459
ical Statistics 22.3 (1951), pp. 400–407. 460
[52] Pouya Samangouei, Maya Kabkab and Rama Chellappa. ‘Defense-GAN: Protecting Classifiers Against 461
Adversarial Attacks Using Generative Models’. In: International Conference on Learning Representations . 462
2018. 463
[53] Changhao Shi, Chester Holtz and Gal Mishne. ‘Online Adversarial Purification based on Self-supervised 464
Learning’. In: International Conference on Learning Representations . 2021. 465
[54] Naman D Singh, Francesco Croce and Matthias Hein. Revisiting Adversarial Training for ImageNet: 466
Architectures, Training and Generalization across Threat Models . 2023. arXiv: 2303.01870 . 467
[55] Leslie N. Smith. ‘Cyclical Learning Rates for Training Neural Networks’. In: IEEE Winter Conference 468
on Applications of Computer Vision . 2017. 469
[56] Kihyuk Sohn, Honglak Lee and Xinchen Yan. ‘Learning Structured Output Representation using Deep 470
Conditional Generative Models’. In: Advances in Neural Information Processing Systems . 2015. 471
[57] Christian Szegedy et al. ‘Intriguing properties of neural networks’. In: International Conference on 472
Learning Representations . 2014. 473
[58] Florian Tramèr and Dan Boneh. ‘Adversarial Training and Robustness for Multiple Perturbations’. In: 474
Advances in Neural Information Processing Systems . 2019. 475
11[59] Florian Tramèr et al. ‘Ensemble Adversarial Training: Attacks and Defenses’. In: International Conference 476
on Learning Representations . 2018. 477
[60] Florian Tramèr et al. ‘On Adaptive Attacks to Adversarial Example Defenses’. In: Advances in Neural 478
Information Processing Systems . 2020. 479
[61] Pascal Vincent et al. ‘Extracting and composing robust features with denoising autoencoders’. In: Inter- 480
national Conference on Machine Learning . 2008. 481
[62] Zekai Wang et al. Better Diffusion Models Further Improve Adversarial Training . 2023. arXiv: 2303. 482
10130 . 483
[63] Han Xiao, Kashif Rasul and Roland V ollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking 484
Machine Learning Algorithms . 2017. arXiv: 1708.07747 . 485
[64] Xinchen Yan et al. ‘Attribute2Image: Conditional Image Generation from Visual Attributes’. In: Proceed- 486
ings of the European Conference on Computer Vision . 2016. 487
[65] Zhaoyuan Yang et al. ‘Adversarial Purification with the Manifold Hypothesis’. In: AAAI Conference on 488
Artificial Intelligence . 2024. 489
[66] Jongmin Yoon, Sung Ju Hwang and Juho Lee. ‘Adversarial Purification with Score-based Generative 490
Models’. In: Proceedings of the International Conference on Machine Learning . 2021. 491
[67] Hongyang Zhang et al. ‘Theoretically Principled Trade-off between Robustness and Accuracy’. In: 492
Proceedings of the International Conference on Machine Learning . 2019. 493
[68] M. Zhang, S. Levine and C. Finn. ‘MEMO: Test Time Robustness via Adaptation and Augmentation’. In: 494
Advances in Neural Information Processing Systems . 2022. 495
[69] Michael Zhang et al. ‘Lookahead Optimizer: k steps forward, 1 step back’. In: Advances in Neural 496
Information Processing Systems . 2019. 497
[70] Zhaoyu Zhang, Mengyan Li and Jun Yu. ‘On the Convergence and Mode Collapse of GAN’. In: SIG- 498
GRAPH Asia 2018 Technical Briefs . 2018. 499
12A On Projected Gradient Descent adversarial training 500
The task of determining model parameters θ⋆that are robust to adversarial perturbations is cast in 501
[42] as a min-max optimisation problem seeking to minimise adversarial risk ,i.e.: 502
θ⋆≈ˆθ⋆:= arg min
θE(x,y)∼D
max
δ∈SL(f(x+δ;θ), y)
where Dis the distribution on the examples xand the corresponding labels y,f(·;θ)is a model 503
with learnable parameters θ,Lis a suitable loss function, and Sis the set of allowed constrained 504
perturbations. In the case of ℓpnorm-bound perturbations of maximum magnitude ϵ, we can further 505
specify S:={δ|∥δ∥p≤ϵ}. 506
The inner optimisation problem is solved, in [42], by Projected Gradient Descent (PGD), an iterative 507
algorithm whose goal is the synthesis of an adversarial perturbation ˆδ=δ(K)afterKgradient ascent 508
and projection steps defined as: 509
δ(k+1)←PS
δ(k)+αsign
∇δ(k)Lce(f(x+δ(k);θ), y)
where δ(0)is randomly sampled within S,αis a hyperparameter ( step size ),Lceis the cross-entropy 510
function, and PAis the Euclidean projection operator onto set A,i.e.: 511
PA(a) := arg min
a′∈A||a−a′||2.
The outer optimisation is carried out by simply training f(·;θ)on the examples found by PGDagainst 512
the current model parameters – and their original pre-perturbation labels. The overall procedure just 513
described constitutes P GDadversarial training . 514
B On the functioning of (conditional) Variational Autoencoders 515
Variational autoencoders ( VAEs) [32, 50] learn from data a generative distribution of the form 516
p(x,z) =p(x|z)p(z), where the probability density p(z)represents a prior over latent variable z, 517
andp(x|z)is the likelihood function, which can be used to sample data of interest x, given z. 518
Training is carried out by maximising a variational lower bound, −L V AE(x), on the log-likelihood 519
logp(x)– which is a proxy for the Evidence Lower Bound (ELBO ) –i.e.: 520
−L V AE(x) :=Eq(z|x)[logp(x|z)]−KL(q(z|x)∥p(z))
where q(z|x)≈p(z|x)is an approximate posterior and KL(·∥·)is the Kullback-Leibler divergence. 521
By parameterising the likelihood with a decoder ANN pθD(x|z;θD)≈p(x|z), and a possible 522
variational posterior with an encoder ANN qθE(z|x;θE)≈q(z|x), the parameters θ⋆
Dof the 523
generative model that best reproduces the data can be learnt – jointly with θ⋆
E– as: 524
θ⋆
E,θ⋆
D:=
arg min
(θE,θD)LV AE(x) =
arg min
(θE,θD)Ex∼Dh
−Ez∼qθE(z|x;θE)[logpθD(x|z;θD)] + KL( qθE(z|x;θE)∥p(z))i
where Dis the distribution over the (training) examples x. 525
From a practical point of view, optimisation is based on the empirical evaluation of LV AE(x;θ)on 526
mini-batches of data, with the term −Ez∼qθE(z|x;θE)[logpθD(x|z;θD)]replaced by a reconstruction 527
cost 528
13LReco(x,x′)≥0|LReco(x,x′) = 0 ⇐⇒ x=x′.
The generation of new data according to the fitted model is achieved by sampling from 529
pθ⋆
D(x|z;θ⋆
D)
z∼p(z)
i.e.decoding samples from p(z). 530
The setting is analogous in the case of conditional Variational Autoencoders [56, 64] (see section 3), 531
where conditional sampling is achieved by 532
xcj∼pθ⋆
D(x|z,c;θ⋆
D)
z∼p(z);c=cj.
C Justification of Adversarially-balanced batches 533
During the incipient phases of experimentation, preliminary tests were performed with the MNIST 534
[36] and Fashion-MNIST [63] datasets – using a conditional VAE as the purifier , and small FCNs or 535
convolutional ANN s as the classifiers . Adversarial examples were generated against the adversarially 536
pre-trained classifier , and tentatively denoised by the purifier with one sample only. The resulting 537
recovered inputs were classified by the classifier and the overall accuracy was recorded. 538
Importantly, such tests were not meant to assess the end-to-end adversarial robustness of the whole 539
architecture, but only to tune the training protocol of the purifier . 540
Generating adversarial training examples by means of PGDis considered the gold standard [24] 541
and was first attempted as a natural choice to train the purifier. However, in this case, the following 542
phenomena were observed: 543
•Unsatisfactory clean accuracy was reached upon convergence, speculatively a consequence 544
of the VAE having never been trained on clean -to-clean example reconstruction; 545
• Persistent vulnerability to same norm-bound F GSM perturbations was noticed; 546
• Persistent vulnerability to smaller norm-bound F GSM and P GDperturbations was noticed. 547
In an attempt to mitigate such issues, the composition of adversarial examples was adjusted to 548
specifically counteract each of the issues uncovered. The adoption of any smaller subset of attack 549
types or strength, compared to that described in subsection 4.4, resulted in unsatisfactory mitigation. 550
At that point, another problem emerged: if such an adversarial training protocol was carried out in 551
homogeneous batches, each containing the same type and strength of attack (or none at all), the 552
resulting robust accuracy was still partially compromised due to the homogeneous ordering of attack 553
types and strengths across batches. 554
Such observations lead to the final formulation of the training protocol, detailed in subsection 4.4, 555
which mitigates to the best the issues described so far. 556
D Heuristic justification of the robust aggregation strategy 557
The rationale leading to the choice of the specific robust aggregation strategy described in subsec- 558
tion 4.5 was an attempt to answer the following question: ‘How is it possible to aggregate the results 559
of an ensemble of classifiers in a way such that it is hard to tilt the balance of the ensemble by 560
attacking only a few of its members?’. The same reasoning can be extended to the reciprocal problem 561
we are trying to solve here, where different input reconstructions obtained from the same potentially 562
perturbed input are classified by the same model (the classifier ). 563
Far from providing a satisfactory answer, we can analyse the behaviour of our aggregation strategy 564
as the logit associated with a given model and class varies across its domain, under the effect of 565
14adversarial intervention. Comparison with existing (and more popular) probability averaging and 566
logit averaging aggregation strategies should provide a heuristic justification of our choice. 567
We recall our aggregation strategy: 568
Pi:=1
ZNY
α=1eelα
i.
Additionally, we recall logit averaging aggregation 569
Pi:=1
Ze1
NPN
α=1lα
i=1
ZNY
α=1e1
Nlα
i=1
Z NY
α=1elα
i!1
N
andprobability averaging aggregation 570
Pi:=1
ZNX
α=1elα
i
PC
j=1elα
j=NX
α=1elα
i1
Qα
where Qα=PC
j=1elα
j. 571
Finally, since lα
i∈R,∀lα
i,limx→−∞ ex= 0 ande0= 1, we can observe that elα
i>0and 572
eelα
i>1,∀lα
i. 573
Now, we consider a given class i⋆and the classifier prediction on a given input reconstruction α⋆, and 574
study the potential effect of an adversary acting on lα⋆
i⋆. This adversarial intervention can be framed 575
in two complementary scenarios: either the class i⋆is correct and the adversary aims to decrease its 576
membership probability, or the class i⋆is incorrect and the adversary aims to increase its membership 577
probability. In any case, the adversary should comply with the ϵ∞-boundedness of its perturbation on 578
the input. 579
Logit averaging In the former scenario, the product of elα
iterms can be arbitrarily deflated (up to 580
zero) by lowering the lα⋆
i⋆logit only. In the latter scenario, the logit can be arbitrarily inflated, and 581
such effect is only partially suppressed by normalisation by Z(a sum of1/N-exponentiated terms). 582
Probability averaging In the former scenario, although the effect of the deflation of a single logit 583
is bounded by elα⋆
i⋆>0, two attack strategies are possible: either decreasing the value of lα⋆
i⋆or 584
increasing the value of Qα⋆, giving rise to complex combined effects. In the latter scenario, the 585
reciprocal is possible, i.e.either inflating lα⋆
i⋆or deflating Qα⋆. Normalisation has no effect in both 586
cases. 587
Ours In the former scenario, the effect of logit deflation on a single product term is bounded by 588
eelα⋆
i⋆>1, thus exerting only a minimal collateral effect on the product, through a decrease of Z. 589
This effectively prevents aggregation takeover by logit deflation. Similarly to logit averaging , in the 590
latter scenario, the logit can be arbitrarily inflated. However, in this case, the effect of normalisation 591
byZis much stronger, given its increased magnitude. 592
From such a comparison, our aggregation strategy is the only one that strongly prevents adversarial 593
takeover bylogit deflation , while still defending well against perturbations targeting logit inflation . 594
E Architectural details and hyperparameters 595
In the following section, we provide more precise details about the architectures (subsection E.1) and 596
hyperparameters (subsection E.2) used in the experimental phase of our work. 597
15E.1 Architectures 598
In the following subsection, we describe the specific structure of the individual parts composing the 599
purifier – in the three scenarios considered. As far as the classifier architectures are concerned, we 600
redirect the reader to the original articles introducing those models ( i.e.: [18] for scenarios (a) and 601
(b), [62] for scenario (c) ). 602
During training, before being processed by the purifier encoder, input examples are standardised 603
according to the statistics of the respective training dataset. 604
Afterwards, they are fed to the disjoint input encoder (see subsection 4.3), whose architecture is 605
shown in Table 4. The same architecture is used in all scenarios considered. 606
Table 4: Architecture for the disjoint input encoder of the purifier . The same architecture is used in all scenarios
considered. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The
following abbreviations are used: Conv2D : 2-dimensional convolutional layer; ch_in : number of input channels;
ch_out : number of output channels; ks: kernel size; s: stride; p: padding; b: presence of a learnable bias
term; BatchNorm2D : 2-dimensional batch normalisation layer; affine : presence of learnable affine transform
coefficients; slope : slope for the activation function in the negative semi-domain.
Disjoint Input Encoder (all scenarios)
Conv2D(ch_in=3, ch_out=6, ks=3, s=2, p=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=6, ch_out=12, ks=3, s=2, p=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
The original input is also fed to the classifier . The corresponding internal representation is extracted, 607
preserving its layered structure. In order to improve the scalability of the method, only a subset of 608
classifier layers is used instead of the whole internal representation. Specifically, for each block of 609
theWIDERESNETarchitecture, only the first layers have been considered; two shortcut layers have 610
also been added for good measure. The exact list of those layers is reported in Table 5. 611
Each extracted layerwise (pre)activation tensor has the shape of a multi-channel image, which is 612
processed – independently for each layer – by a different CNN whose individual architecture is shown 613
in Table 6 ( scenarios (a) and(b)) and Table 7 ( scenario (c) ). 614
The resulting tensors (still having the shape of multi-channel images) are then jointly processed by a 615
fully-connected subnetwork whose architecture is shown in Table 8. The value of fcrepr for the 616
different scenarios considered is shown in Table 13. 617
Thecompressed input andcompressed internal representation so obtained are finally jointly encoded 618
by an additional fully-connected subnetwork whose architecture is shown in Table 9. The output is a 619
tuple of means and standard deviations to be used to sample the stochastic latent code z. 620
The sampler used for the generation of such latent variables z, during the training of the purifier , 621
is a reparameterised [32] Normal sampler z∼ N (µ, σ). During inference, zis sampled by re- 622
parameterisation from the i.i.d Standard Normal distribution z∼ N (0,1)(i.e.from its original 623
prior). 624
The architectures for the decoder of the purifier are shown in Table 10 ( scenarios (a) and(b)) and 625
Table 11 ( scenario (c) ). 626
E.2 Hyperparameters 627
In the following section, we provide the hyperparameters used for adversarial example generation and 628
optimisation during the training of the purifier , and those related to the purifier model architectures. 629
We also provide the hyperparameters for the PGD+EOTattack, which is used as a complementary 630
tool for the evaluation of adversarial robustness. 631
16Table 5: Classifier model ( WIDERESNET-28-10 ) layer names used as (a subset of) the internal represent-
ation fed to the layerwise convolutional encoder of the purifier . The names reflect those used in the model
implementation.
All scenarios
layer.0.block.0.conv_0
layer.0.block.0.conv_1
layer.0.block.1.conv_0
layer.0.block.1.conv_1
layer.0.block.2.conv_0
layer.0.block.2.conv_1
layer.0.block.3.conv_0
layer.0.block.3.conv_1
layer.1.block.0.conv_0
layer.1.block.0.conv_1
layer.1.block.0.shortcut
layer.1.block.1.conv_0
layer.1.block.1.conv_1
layer.1.block.2.conv_0
layer.1.block.2.conv_1
layer.1.block.3.conv_0
layer.1.block.3.conv_1
layer.2.block.0.conv_0
layer.2.block.0.conv_1
layer.2.block.0.shortcut
layer.2.block.1.conv_0
layer.2.block.1.conv_1
layer.2.block.2.conv_0
layer.2.block.2.conv_1
layer.2.block.3.conv_0
layer.2.block.3.conv_1
Table 6: Architecture for the layerwise internal representation encoder of the purifier . The architecture shown
in this table is used in scenarios (a) and(b). The architecture is represented layer by layer, from input to output,
in a PyTorch-like syntax. The following abbreviations are used: Conv2D : 2-dimensional convolutional layer;
ch_in : number of input channels; ch_out : number of output channels; ks: kernel size; s: stride; p: padding; b:
presence of a learnable bias term; BatchNorm2D : 2-dimensional batch normalisation layer; affine : presence
of learnable affine transform coefficients; slope : slope for the activation function in the negative semi-domain.
The abbreviation [ci] indicates the number of input channels for the (pre)activation tensor of each extracted
layer. The abbreviation ceil indicates the ceiling integer rounding function.
Layerwise Internal Representation Encoder (scenarios (a) and (b))
Conv2D(ch_in=[ci], ch_out=ceil([ci]/2), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=ceil([ci]/2), ch_out=ceil([ci]/4), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=ceil([ci]/4), ch_out=ceil([ci]/8), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
17Table 7: Architecture for the layerwise internal representation encoder of the purifier . The architecture shown
in this table is used in scenario (c) . The architecture is represented layer by layer, from input to output, in
a PyTorch-like syntax. The following abbreviations are used: Conv2D : 2-dimensional convolutional layer;
ch_in : number of input channels; ch_out : number of output channels; ks: kernel size; s: stride; p: padding; b:
presence of a learnable bias term; BatchNorm2D : 2-dimensional batch normalisation layer; affine : presence
of learnable affine transform coefficients; slope : slope for the activation function in the negative semi-domain.
The abbreviation [ci] indicates the number of input channels for the (pre)activation tensor of each extracted
layer. The abbreviation ceil indicates the ceiling integer rounding function.
Layerwise Internal Representation Encoder (scenario (c))
Conv2D(ch_in=[ci], ch_out=ceil([ci]/2), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=ceil([ci]/2), ch_out=ceil([ci]/4), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=ceil([ci]/4), ch_out=ceil([ci]/8), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=ceil([ci]/8), ch_out=ceil([ci]/16), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Table 8: Architecture for the fully-connected representation encoder of the purifier . The architecture shown
in this table is used in all scenarios considered. The architecture is represented layer by layer, from input to
output, in a PyTorch-like syntax. The following abbreviations are used: Concatenate : layer concatenating
its input features; flatten_features : whether the input features are to be flattened before concatenation;
feats_in ,feats_out : number of input and output features of a linear layer; b: presence of a learnable bias
term; BatchNorm1D : 1-dimensional batch normalisation layer; affine : presence of learnable affine transform
coefficients; slope : slope for the activation function in the negative semi-domain. The abbreviation [computed]
indicates that the number of features is computed according to the shape of the concatenated input tensors. The
value of fcrepr for the different scenarios considered is shown in Table 13.
Fully-Connected Representation Encoder (all scenarios)
Concatenate(flatten_features=True)
Linear(feats_in=[computed], feats_out=fcrepr, b=False)
BatchNorm1D(affine=True)
LeakyReLU(slope=0.2)
Table 9: Architecture for the fully-connected joint encoder of the purifier . The architecture shown in this
table is used in all scenarios considered. The architecture is represented layer by layer, from input to output,
in a PyTorch-like syntax. The following abbreviations are used: Concatenate : layer concatenating its input
features; flatten_features : whether the input features are to be flattened before concatenation; feats_in ,
feats_out : number of input and output features of a linear layer; b: presence of a learnable bias term;
BatchNorm1D : 1-dimensional batch normalisation layer; affine : presence of learnable affine transform
coefficients; slope : slope for the activation function in the negative semi-domain. The abbreviation [computed]
indicates that the number of features is computed according to the shape of the concatenated input tensors. The
value of fjoint for the different scenarios considered is shown in Table 13. The last layer of the network
returns a tuple of 2 tensors, each independently processed – from the output of the previous layer – by the two
comma-separated sub-layers .
Fully-Connected Joint Encoder (all scenarios)
Concatenate(flatten_features=True)
Linear(feats_in=[computed], feats_out=fjoint, b=False)
BatchNorm1D(affine=True)
LeakyReLU(slope=0.2)
( Linear(feats_in=fjoint, feats_out=fjoint, b=True),
Linear(feats_in=fjoint, feats_out=fjoint, b=True) )
18Table 10: Architecture for the decoder of the purifier . The architecture shown in this table is used in scenarios
(a)and(b). The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The
following abbreviations are used: Concatenate : layer concatenating its input features; flatten_features :
whether the input features are to be flattened before concatenation; feats_in ,feats_out : number of input
and output features of a linear layer; b: presence of a learnable bias term; ConvTranspose2D : 2-dimensional
transposed convolutional layer; ch_in : number of input channels; ch_out : number of output channels; ks:
kernel size; s: stride; p: padding; op: PyTorch parameter ‘ output padding ’, used to disambiguate the number
of spatial dimensions of the resulting output; b: presence of a learnable bias term; BatchNorm2D : 2-dimensional
batch normalisation layer; affine : presence of learnable affine transform coefficients; slope : slope for the
activation function in the negative semi-domain. The values of fjoint andfcrepr for the different scenarios
considered are shown in Table 13.
Decoder (scenarios (a) and (b))
Concatenate(flatten_features=True)
Linear(feats_in=[fjoint+fcrepr], feats_out=2304, b=True)
LeakyReLU(slope=0.2)
Unflatten(256, 3, 3)
ConvTranspose2D(ch_in=256, ch_out=256, ks=3, s=2, p=1, op=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=256, ch_out=128, ks=3, s=2, p=1, op=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=128, ch_out=64, ks=3, s=2, p=1, op=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=64, ch_out=32, ks=3, s=2, p=1, op=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=32, ch_out=3, ks=2, s=1, p=1, op=0, b=True)
Sigmoid()
Table 11: Architecture for the decoder of the purifier . The architecture shown in this table is used in scenario
(c). The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following
abbreviations are used: Concatenate : layer concatenating its input features; flatten_features : whether
the input features are to be flattened before concatenation; feats_in ,feats_out : number of input and output
features of a linear layer; b: presence of a learnable bias term; ConvTranspose2D : 2-dimensional transposed
convolutional layer; ch_in : number of input channels; ch_out : number of output channels; ks: kernel size;
s: stride; p: padding; op: PyTorch parameter ‘ output padding ’, used to disambiguate the number of spatial
dimensions of the resulting output; b: presence of a learnable bias term; BatchNorm2D : 2-dimensional batch
normalisation layer; affine : presence of learnable affine transform coefficients; slope : slope for the activation
function in the negative semi-domain. The values of fjoint andfcrepr for the different scenarios considered
are shown in Table 13.
Decoder (scenario (c))
Concatenate(flatten_features=True)
Linear(feats_in=[fjoint+fcrepr], feats_out=4096, b=True)
LeakyReLU(slope=0.2)
Unflatten(256, 4, 4)
ConvTranspose2D(ch_in=256, ch_out=256, ks=3, s=2, p=1, op=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=256, ch_out=128, ks=3, s=2, p=1, op=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=128, ch_out=64, ks=3, s=2, p=1, op=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=64, ch_out=32, ks=3, s=2, p=1, op=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=32, ch_out=3, ks=3, s=1, p=1, op=0, b=True)
Sigmoid()
19Attacks The hyperparameters used for the adversarial attacks described in subsection 4.4 are shown 632
in Table 12. The value of ϵ∞is fixed to ϵ∞=8/255. With the only exception of ϵ∞,AUTOATTACK 633
is to be considered a hyperparameter-free adversarial example generator. 634
Table 12: Hyperparameters for the attacks used for training and testing the purifier TheFGSM andPDGattacks
refer to the training phase (see subsection 4.4), whereas the PGD+EOTattack [38] refers to the robustness
assessment pipeline. The entry CCEdenotes the Categorical CrossEntropy loss function. The ℓ∞threat model is
assumed, and all inputs are linearly rescaled within [0.0,1.0]before the attack.
FGSM PGD PGD+EOT
Input clipping [0.0,1.0] [0 .0,1.0] [0 .0,1.0]
# of steps 1 40 200
Step size ϵ∞ 0.01 0 .007
Loss function CCE CCE CCE
# of EoT iterations 1 1 20
Optimiser SGD SGD
Architectures Table 13 contains the hyperparameters that define the model architectures used as 635
part of the purifier , in the different scenarios considered. 636
Table 13: Scenario-specific architectural hyperparameters for the purifier , as referred to in Table 8, Table 9,
Table 10, and Table 11.
Scenario (a) Scenario (b) Scenario (c)
fcrepr 512 512 768
fjoint 128 128 192
Training Table 14 collects the hyperparameters governing the training of the purifier in the different 637
scenarios considered. 638
Table 14: Hyperparameters used for training the purifier , grouped by scenario. The entry CCEdenotes the
Categorical CrossEntropy loss function. The LRscheduler is stepped after each epoch.
Allscenarios Sc. (a) Sc. (b) Sc. (c)
Optimiser RA DAM+LOOKAHEAD
RADAMβ1 0.9
RADAMβ2 0.999
RADAMϵ 10−8
RADAM Weight Decay None
LOOKAHEAD averaging decay 0.8
LOOKAHEAD steps 6
Initial LR 5×10−9
Loss function CCE
Sampled reconstructions per input 8
Epochs 200 200 250
LRwarm-up epochs 25 25 31
LRplateau epochs 25 25 31
LRannealing epochs 150 250 188
Plateau LR 0.064 0 .064 0 .0128
Final LR 4.346×10−44.346×10−41.378×10−4
βincrease initial epoch 25 25 32
βincrease final epoch 34 34 43
Batch size 5120 2560 1024
Adversarial batch fraction 0.5 0 .15 0 .01
F Additional tables 639
The following section contains additional tabular data that may be of interest to the reader. 640
20Table 15 reports the respective clean accuracies for the best models available in terms of AUTOAT- 641
TACK robust accuracy, in scenarios (a) and(b). Models are further divided in AT-based and 642
purification-based, so as to match the corresponding columns for robust accuracy shown in Table 2. 643
The best AT-based model for CIFAR -10is taken from [18], whereas that for CIFAR -100 from [62]. 644
Both best purification-based models are taken from [40]. 645
The clean and robust accuracies for the best AT-based model on TINYIMAGE NET-200 (scenario (c) ) 646
are already part of Table 2 and we redirect the reader there for such information. We are not aware of 647
any published state-of-the-art adversarial purification-based model for T INYIMAGE NET-200. 648
Table 15: Clean accuracy for the best models (by robust accuracy) on the datasets considered in scenarios
(a)and(b), mentioned in Table 2. The following abbreviations are used: Scen : scenario considered; Best
AT/Cl : clean accuracy for the most robust model (by the means of AUTOATTACK ) on the respective dataset,
obtained by adversarial training alone; Best P/Cl : clean accuracy for the most robust model (by the means of
AUTOATTACK ) on the respective dataset, obtained by adversarial purification alone.
Scen. Dataset Best AT/Cl Best P/Cl
(a) C IFAR -10 0.9323 0.9082
(b) C IFAR -100 0.7522 0.6973
21NeurIPS Paper Checklist 649
1.Claims 650
Question: Do the main claims made in the abstract and introduction accurately reflect the 651
paper’s contributions and scope? 652
Answer: [Yes] 653
Justification: Claims made in the abstract and introduction of the paper accurately reflect 654
its contributions, and those are directly corroborated by experimental analysis. Results and 655
their discussion is available in subsection 5.2 and subsection 5.3. 656
2.Limitations 657
Question: Does the paper discuss the limitations of the work performed by the authors? 658
Answer: [Yes] 659
Justification: subsection 5.2 and subsection 5.3 also contain the discussion of potential 660
limitations of the method, and open problems it introduces. 661
3.Theory Assumptions and Proofs 662
Question: For each theoretical result, does the paper provide the full set of assumptions and 663
a complete (and correct) proof? 664
Answer: [NA] 665
Justification: The paper does not contribute novel theoretical results. Assumptions anyway 666
related to the contribution are clearly stated throughout the paper. 667
4.Experimental Result Reproducibility 668
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 669
perimental results of the paper to the extent that it affects the main claims and/or conclusions 670
of the paper (regardless of whether the code and data are provided or not)? 671
Answer: [Yes] 672
Justification: Training and evaluation details required to reproduce the experimental results 673
of the paper are reported in section 5 and Appendix E. Code and data for the reproduction 674
of all experiments are additionally released as part of Supplementary Material. 675
5.Open access to data and code 676
Question: Does the paper provide open access to the data and code, with sufficient instruc- 677
tions to faithfully reproduce the main experimental results, as described in supplemental 678
material? 679
Answer: [Yes] 680
Justification: Code and data for the reproduction of all experiments are released to reviewers 681
as part of Supplementary Material. The public version of the paper will include instructions 682
to obtain the same material from a dedicated publicly accessible source. 683
6.Experimental Setting/Details 684
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 685
parameters, how they were chosen, type of optimiser, etc.) necessary to understand the 686
results? 687
Answer: [Yes] 688
Justification: Training and evaluation details, including hyperparameters, required to re- 689
produce the experimental results of the paper are reported in section 5 and Appendix E. 690
Code and data for the reproduction of all experiments are additionally released as part of 691
Supplementary Material. 692
7.Experiment Statistical Significance 693
Question: Does the paper report error bars suitably and correctly defined or other appropriate 694
information about the statistical significance of the experiments? 695
Answer: [No] 696
22Justification: As doing adversarial training with 40-steps PGD is roughly 40 times more 697
computationally demanding than nominal training, unfortunately, we are unable to show 698
error bars or otherwise quantify statistical errors. In any case, the improvement induced 699
by our method w.r.t. their state-of-the-art counterparts is well clear of the threshold for 700
statistical significance. 701
8.Experiments Compute Resources 702
Question: For each experiment, does the paper provide sufficient information on the com- 703
puter resources (type of compute workers, memory, time of execution) needed to reproduce 704
the experiments? 705
Answer: [Yes] 706
Justification: Information about computational resources and required time is contained in 707
subsection 5.1 as well as in the supplementary materials. 708
9.Code Of Ethics 709
Question: Does the research conducted in the paper conform, in every respect, with the 710
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 711
Answer: [Yes] 712
Justification: The research conducted in the paper does conform, in every respect, with the 713
NeurIPS Code of Ethics. 714
10.Broader Impacts 715
Question: Does the paper discuss both potential positive societal impacts and negative 716
societal impacts of the work performed? 717
Answer: [Yes] 718
Justification: The paper proposes a novel technique to mitigate the problem of adversarial 719
vulnerability of high-dimensional classifiers. Such vulnerability may pose potential societal 720
impacts, as discussed in section 1. 721
11.Safeguards 722
Question: Does the paper describe safeguards that have been put in place for responsible 723
release of data or models that have a high risk for misuse (e.g., pretrained language models, 724
image generators, or scraped datasets)? 725
Answer: [NA] 726
Justification: We do not plan to release models or data with a high risk for misuse. Models 727
to be released do not reasonably carry risk for misuse. 728
12.Licenses for existing assets 729
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 730
the paper, properly credited and are the license and terms of use explicitly mentioned and 731
properly respected? 732
Answer: [Yes] 733
Justification: The creators and/or owners of assets used in the paper are either credited by 734
reference to their original research work, or directly with a link to the preferred landing page 735
for such assets. The use of licensed material is compliant with the respective licenses. 736
13.New Assets 737
Question: Are new assets introduced in the paper well documented and is the documentation 738
provided alongside the assets? 739
Answer: [Yes] 740
Justification: Trained model weights are provided alongside the paper, and their details are 741
provided as part of supplementary materials. In case of release to the public, such details 742
will be provided contextually to the models. 743
14.Crowdsourcing and Research with Human Subjects 744
Question: For crowdsourcing experiments and research with human subjects, does the paper 745
include the full text of instructions given to participants and screenshots, if applicable, as 746
well as details about compensation (if any)? 747
23Answer: [NA] 748
Justification: No crowdsourcing or research with human subjects has been performed as part 749
of the work of, or leading to, this paper. 750
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 751
Subjects 752
Question: Does the paper describe potential risks incurred by study participants, whether 753
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 754
approvals (or an equivalent approval/review based on the requirements of your country or 755
institution) were obtained? 756
Answer: [NA] 757
Justification: No crowdsourcing or research with human subjects has been performed as part 758
of the work of or leading to this paper - including that potentially requiring IRB approval or 759
equivalent authorisation. 760
24