An Accelerated Algorithm for Stochastic Bilevel
Optimization under Unbounded Smoothness
Xiaochuan Gong Jie Hao Mingrui Liu∗
Department of Computer Science
George Mason University
{xgong2, jhao6, mingruil}@gmu.edu
Abstract
This paper investigates a class of stochastic bilevel optimization problems where
the upper-level function is nonconvex with potentially unbounded smoothness
and the lower-level problem is strongly convex. These problems have significant
applications in sequential data learning, such as text classification using recurrent
neural networks. The unbounded smoothness is characterized by the smoothness
constant of the upper-level function scaling linearly with the gradient norm, lack-
ing a uniform upper bound. Existing state-of-the-art algorithms require eO(ϵ−4)
oracle calls of stochastic gradient or Hessian/Jacobian-vector product to find an
ϵ-stationary point. However, it remains unclear if we can further improve the
convergence rate when the assumptions for the function in the population level
also hold for each random realization almost surely (e.g., Lipschitzness of each
realization of the stochastic gradient). To address this issue, we propose a new
Accelerated Bilevel Optimization algorithm named AccBO. The algorithm updates
the upper-level variable by normalized stochastic gradient descent with recursive
momentum and the lower-level variable by the stochastic Nesterov accelerated
gradient descent algorithm with averaging. We prove that our algorithm achieves
an oracle complexity of eO(ϵ−3)to find an ϵ-stationary point. Our proof relies
on a novel lemma characterizing the dynamics of stochastic Nesterov accelerated
gradient descent algorithm under distribution drift with high probability for the
lower-level variable, which is of independent interest and also plays a crucial role
in analyzing the hypergradient estimation error over time. Experimental results on
various tasks confirm that our proposed algorithm achieves the predicted theoretical
acceleration and significantly outperforms baselines in bilevel optimization. The
code is available here.
1 Introduction
Bilevel optimization receives tremendous attention recently in the machine learning community,
due to its applications in meta-learning [ 25,56], hyperparameter optimization [ 25,23], data hyper-
cleaning [ 41], continual learning [ 6,35], and reinforcement learning [ 44]. The bilevel optimization
problem has the following formulation:
min
x∈RdxΦ(x) :=f(x, y∗(x)),s.t., y∗(x)∈arg min
y∈Rdyg(x, y),(1)
where fandgare upper-level and lower-level functions respectively. For example, in meta-
learning [ 24,25],xdenotes the layers of neural networks for shared representation learning, y
denotes the task-specific head encoded in the last layer, and the formulation (1)aims to learn the a
∗Corresponding Author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).common representation learning encoder xsuch that it can be quickly adapted to downstream tasks
by only updating the task-specific head y. In machine learning, people typically consider stochas-
tic optimization setting such that f(x, y) =Eξ∼Df[F(x, y;ξ)]andg(x, y) =Eζ∼Dg[G(x, y;ζ)],
where DfandDgare the underlying unknown data distributions for fandgrespectively, and one
can access noisy observations of fandgbased on sampling from DfandDg.
There emerges a wave of studies for algorithmic design and analysis for solving the bilevel opti-
mization problem (1)under different assumptions of fandg. Most theoretical work assumes the
upper-level function is smooth (i.e., gradient is Lipschitz) and nonconvex, and the lower-level function
is strongly convex [ 28,41,39,31,45]. However, as pointed out by [ 72,14], certain neural networks
such as recurrent neural networks [ 20], long-short term memory networks [ 38] and transformers [ 62]
have smoothness constants that scale with gradient norm, potentially leading to unbounded smooth-
ness constants (i.e., gradient Lipschitz constant can be infinity). Motivated by this, Hao et al. [ 36]
designed the first bilevel optimization algorithm to handle the cases where fis nonconvex with
potentially unbounded smoothness and gis strongly convex. The algorithm in [ 36] achieves eO(ϵ−4)
oracle complexity for finding an ϵ-stationary point (i.e., a point xsuch that ∥∇Φ(x)∥ ≤ϵ). Gong et
al. [30] proposed an single-loop algorithm under the same setting as in [ 36] and also achieved eO(ϵ−4)
oracle complexity. This complexity result is worse than the eO(ϵ−3)oracle complexity under the
relatively easier setting where fhas a Lipschitz gradient, and each realization of the stochastic oracle
calls is Lipschitz with respect to its argument (e.g., almost-sure Lipschitz oracle) [ 68,43,17,32,40].
This naturally motivates us to study the following question:
Is it possible to improve the eO(ϵ−4)oracle complexity for bilevel optimization problems where
the upper-level function is nonconvex with unbounded smoothness and the lower-level function
is strongly convex, by assuming that the properties of the function at the population level also
hold almost surely for each random realization?
In this paper, we give a positive answer to this question by designing a new algorithm named AccBO
with an improved oracle complexity of eO(ϵ−3). Our algorithm is inspired by momentum-based
variance reduction techniques used in nonconvex smooth optimization [ 17] under the almost-sure
Lipschitz stochastic gradient oracle framework. The innovation of AccBO lies in its update rules: it
employs normalized stochastic gradient descent with recursive momentum for the upper-level variable
and stochastic Nesterov accelerated gradient descent with averaging for the lower-level variable. Our
approach differs from existing accelerated bilevel optimization algorithms, such as those proposed
by [68,43] in two key ways: (i) while these algorithms use recursive momentum for the upper-
level variable update, AccBO utilizes normalized recursive momentum to address the unbounded
smoothness of the upper-level function; (ii) for the lower-level variable update, we use stochastic
Nesterov accelerated gradient descent with averaging, in contrast to the recursive momentum method
used by the other algorithms. The primary challenge in analyzing the convergence rate of AccBO
arises from the need to simultaneously control errors from both upper-level and lower-level variables,
given the unbounded smoothness, large learning rate, and recursive momentum in the upper-level
problem. Our main contributions are summarized as follows.
•We design a new algorithm named AccBO for solving bilevel optimization problems where
the upper-level function is nonconvex with unbounded smoothness and the lower-level
function is strongly convex. AccBO leverages normalized recursive momentum for the upper-
level variable and Nesterov momentum for the lower-level variable under the stochastic
setting to achieve acceleration. To the best of our knowledge, the simultaneous usage of
these two techniques in stochastic bilevel optimization is novel and has not been previously
explored in the bilevel optimization literature.
•We prove that the AccBO algorithm requires eO(ϵ−3)oracle calls for finding an ϵ-stationary
point. This complexity strictly improves the state-of-the-art oracle complexity for un-
bounded smooth nonconvex upper-level problem and strongly-convex lower-level problem
as described in [ 36,30]. To achieve this result, we introduce novel proof techniques for ana-
lyzing the dynamics of stochastic Nesterov accelerated gradient descent under distribution
drift with high probability for the lower-level variable, which are crucial for analyzing the
hypergradient error and also of independent interest.
•We empirically verify the effectiveness of our proposed algorithm on various tasks, in-
cluding deep AUC maximization and data hypercleaning. Our algorithm indeed achieves
2the predicted theoretical acceleration and significantly outperforms baselines in bilevel
optimization.
2 Related Work
Relaxed Smoothness. The concept of relaxed smoothness was initially introduced by [ 72], inspired
by the loss landscapes observed in recurrent neural networks and long-short term memory networks.
They show that techniques such as gradient clipping and normalization could improve the performance
compared with gradient descent in these scenarios. It inspired further investigations that concentrate
on various aspects, including improved analysis on gradient clipping and normalization [ 71,42],
adaptive algorithms [ 14,48,65,22], federated algorithms [ 51,15,16], generalized assumptions [ 14,
13], and recursive momentum based methods with faster rates [ 57,53]. The work of [ 36,30]
considered a relaxed smoothness condition for the upper-level problem in the bilevel optimization
setting.
Bilevel Optimization. Bilevel optimization refers to a special kind of optimization where one
problem is embedded within another. It was first introduced by [ 8]. Early works developed specific
bilevel optimization algorithms with asymptotic convergence analysis [ 64,1,66]. Ghadimi and
Wang [ 28] initiated the study of non-asymptotic convergence for gradient-based methods in bilevel
optimization where the upper-level problem is smooth and the lower-level problem is strongly convex.
This field saw further advancements with improved complexity results [ 39,41,10,19,11] and fully
first-order algorithms [ 45,49]. There is a line of work which leverages almost-sure Lipschitz oracle
(e.g., stochastic gradient) to obtain improved convergence rates of bilevel optimization algorithms [ 68,
43]. When the lower-level function is not strongly convex, several algorithmic framework and
approximation schemes were proposed [ 58,60,46,59,52,9]. The setting considered in this paper is
very close to [ 36,30], where the upper-level function is nonconvex and unbounded smooth, and the
lower-level function is strongly convex. However, the work of [ 36,30] do not have an accelerated
rateeO(ϵ−3)for finding an ϵ-stationary point as established in this paper.
Nesterov Accelerated Gradient and Variants. Nesterov Accelerated Gradient (NAG) method was
introduced by [ 55] for deterministic convex optimization problems. The stochastic version of NAG
(SNAG) was extensively studied in the literature [ 3,4,63,12]. To the best of our knowledge, none
of them provide a high probability analysis for SNAG. In the online learning setting, there is a line
of work focusing on the perspective of sequential stochastic/online optimization with distributional
drift [ 5,67,54,18]. While these studies provide valuable insights into adaptive techniques and
performance bounds under distributional drift, they do not explore the potential integration of such
methods with bilevel optimization problems, nor do they consider the application of SNAG within
this framework.
3 Problem Setup and Preliminaries
Define ⟨·,·⟩and∥ · ∥ as the inner product and the Euclidean norm. Throughout the paper, we use
asymptotic notation eO(·),eΘ(·),eΩ(·)to hide polylog factors in ϵ−1and1/δ. Denote f:Rdx×Rdy→
Ras the upper-level function, and g:Rdx×Rdy→Ras the lower-level function. The hypergradient
∇Φ(x)has the following form [28]:
∇Φ(x) =∇xf(x, y∗(x))− ∇2
xyg(x, y∗(x))
∇2
yyg(x, y∗(x))−1∇yf(x, y∗(x)). (2)
To avoid the Hessian inverse computation, we typically use the following Neumann series to approxi-
mate the hypergradient [28, 43, 39]. In particular, for the stochastic setting, define
¯∇f(x, y;¯ξ) =∇xF(x, y;ξ)−Q
lg,1∇2
xyG(x, y;ζ(0))q(Q)Y
i=1 
I−∇2
yyG(x, y;ζ(i))
lg,1!
∇yF(x, y;ξ),
where q(Q)∼Uniform {0, . . . , Q −1},¯ξ:={ξ, ζ(0), . . . , ζ(q(Q))}and we use the convention thatQj
i=1Ai=Iifj= 0. Then E¯ξ[¯∇f(x, y;¯ξ)]is a good approximation of ∇Φ(x)ifyandy∗(x)are
close [28].
Throughout the paper, we make the following assumptions.
3Assumption 3.1 ((Lx,0, Lx,1, Ly,0, Ly,1)-smoothness [ 36]).Letz= (x, y)andz′= (x′, y′), there
exists Lx,0, Lx,1, Ly,0, Ly,1>0such that for all z, z′, if∥z−z′∥ ≤ 1/q
2(L2
x,1+L2
y,1), then
∥∇xf(z)− ∇ xf(z′)∥ ≤(Lx,0+Lx,1∥∇xf(z)∥)∥z−z′∥and∥∇yf(z)− ∇ yf(z′)∥ ≤(Ly,0+
Ly,1∥∇yf(z)∥)∥z−z′∥.
Remark : Assumption 3.1 is introduced by [ 36] for describing the bilevel optimization problems with
recurrent neural networks. This assumption can be regarded as a block-wise relaxed smoothness
assumptions for two blocks xandy, which is a variant of the relaxed smoothness assumption [ 72]
and the coordinate-wise relaxed smooth assumption [14].
Assumption 3.2. Suppose the followings hold for objective functions fandg: (i)fis continuously
differentiable and (Lx,0, Lx,1, Ly,0, Ly,1)-smooth in (x, y); (ii) For every x,∥∇yf(x, y)∥ ≤lf,0for
ally; (iii) For every x,g(x, y)isµ-strongly-convex in yforµ >0; (iv) gislg,1-smooth jointly in
(x, y); (v)gis twice continuously differentiable, and ∇2
xyg,∇2
yygarelg,2-Lipschitz jointly in (x, y).
Remark : Assumption 3.2 is standard in the bilevel optimization literature [ 45,36,28]. Assump-
tion 3.2 (i) characterizes the unbounded smoothness of the upper-level function and is empirically
observed in recurrent neural networks [36].
Assumption 3.3. The following stochastic estimators are unbiased and have the following properties:
Eξ∼Df[∥∇xF(x, y;ξ)− ∇ xf(x, y)∥2]≤σ2
f,1,Eξ∼Df[∥∇yF(x, y;ξ)− ∇ yf(x, y)∥2]≤σ2
f,1,
Pr(∥∇yG(x, y;ξ)− ∇ yg(x, y)∥ ≥λ)≤2 exp(−2λ2/σ2
g,1)∀λ >0,
Eζ∼Dg[∥∇2
xyG(x, y;ζ)− ∇2
xyg(x, y)∥2]≤σ2
g,2,Eζ∼Dg[∥∇2
yyG(x, y;ζ)− ∇2
yyg(x, y)∥2]≤σ2
g,2.
Remark: Assumption 3.3 assumes the stochastic oracle for the upper-level problem has bounded
variance, which is standard in nonconvex stochastic optimization [ 26–28]. It also assumes the
stochastic oracle for the lower-level problem is light-tailed, which is common for the high probability
analysis for the lower-level problem [ 47,37]. Note that the same assumption is also made in [ 36,30]
for the bilevel problems with a unbounded smooth upper-level function.
Assumption 3.4. F(x, y;ξ)andG(x, y;ζ)satisfy Assumption 3.2 for every ξandζalmost surely.
Remark: Assumption 3.4 assumes that each random realization of the upper- and lower-level
functions satisfies the same property as in the population level. Note that this condition is the key to
achieve an improved eO(ϵ−3)oracle complexity under various settings, including both single-level
nonconvex smooth problems [ 21,17,61] and bilevel problems with nonconvex smooth upper-level
objectives [ 68,43]. Furthermore, this assumption is shown to be necessary for achieving improved
oracle complexity in single-level problems [2].
4 Algorithm and Analysis
4.1 Main Challenges and Algorithm Design
Main Challenges. We begin by explaining why existing bilevel optimization algorithms and their
corresponding analysis techniques are insufficient in our setting. First, most algorithms developed
for bilevel optimization require the upper-level function is smooth (i.e., the gradient of the upper-
level function is Lipschitz) [ 28,41,39,68,43,19,45]. They characterize the estimation error
of the optimal solution for the lower-level problem, utilize an approximate hypergradient descent
approach and the descent lemma for L-smooth functions to prove the convergence. In particular,
they demonstrate that a potential function, incorporating both the function value and the bilevel
error from the lower-level problem, progressively decreases in expectation. However, when the
upper-level function is (Lx,0, Lx,1, Ly,0, Ly,1)-smooth as illustrated in Assumption 3.1, the previous
algorithms and analyses relying on L-smoothness do not work. The reason is that the hypergradient
bias depends on the approximation error of the lower-level variable as well as the hypergradient
itself: these elements are statistically dependent and the standard potential function argument with an
expectation-based analysis would not work. Second, the recent work of Hao et al. [ 36] and Gong et
al. [30] considered that the upper-level function is unbounded smooth and addressed this issue by
performing normalized stochastic gradient with momentum for the upper-level variable and periodic
updates or stochastic gradient descent for the lower-level variable, but their oracle complexity is not
4Algorithm 1 STOCHASTIC NESTEROV ACCELERATED GRADIENT METHOD (SNAG)
1:Input: x,˜y−1,˜y0,˜α, T0 #SNAG(x,˜y0,˜α, T0)
2:fort= 0,1, . . . , T 0−1do
3: Sample ˜πtfrom distribution Dg
4: ˜zt= ˜yt+γ(˜yt−˜yt−1)
5: ˜yt+1= ˜zt−˜α∇yG(x,˜zt; ˜πt)
6:end for
Algorithm 2 ACCELERATED BILEVEL OPTIMIZATION ALGORITHM (ACCBO)
1:Input: αinit, α, β, γ, η, τ, I, S, T 0, T, setx0, yinit
0= 0
2:y0=SNAG(x0, yinit
0, αinit, T0), and set y−1= ˆy0=y0 # Warm-start
3:fort= 0,1, . . . , T −1do
4: Sample q(Q)∼Uniform {0, . . . , Q −1}and{ζ(0)
t,s, . . . , ζ(q(Q))
t,s}S
s=1∼ D g
5: Sample {ξt,s}S
s=1∼ D f, denote ¯ξt:=∪S
s=1{q(Q), ξt,s, ζ(0)
t,s, . . . , ζ(q(Q))
t,s}
6: # Lower-Level: Stochastic Nesterov Accelerated Gradient Descent with Averaging
7: # Option I: from Line 8∼9(for quadratic lower-level function)
8: zt=yt+γ(yt−yt−1)
9: yt+1=zt−α∇yG(xt, zt;πt), where πt∼ D g
10: # Option II: from Line 11∼20(for general strongly convex lower-level function)
11: ift >0andtis a multiple of Ithen
12: Sety0
t=y−1
t=yt
13: forj= 0,1, . . . , N −1do
14: zj
t=yj
t+γ(yj
t−yj−1
t)
15: yj+1
t=zj
t−α∇yG(xt, zj
t;πj
t), where πj
t∼ D g
16: end for
17: yt+1=yN+1
t
18: else
19: yt+1=yt
20: end if
21: ˆyt+1= (1−τ)ˆyt+τyt+1 # Averaging
22: # Upper-Level: Normalized Stochastic Gradient Descent with Recursive Momentum
23: mt=βmt−1+ (1−β)¯∇f(xt,ˆyt;¯ξt) +β(¯∇f(xt,ˆyt;¯ξt)−¯∇f(xt−1,ˆyt−1;¯ξt))ift≥1else
m0=¯∇f(x0,ˆy0;¯ξ0)
24: xt+1=xt−ηmt
∥mt∥
25:end for
better than eO(ϵ−4). These facts indicate that we need new algorithm design and analysis techniques
to get potential acceleration.
Algorithm Design . To obtain potential acceleration, our key idea is to update the upper-level variable
by normalized stochastic gradient descent with recursive momentum and update the lower-level
variable by the stochastic Nesterov accelerated gradient (SNAG) method. Different from [ 36,30],
the key innovation of our algorithm design is that we achieve acceleration for both upper-level
and lower-level problems simultaneously but without affecting each other. The upper-level update
rule can be regarded as a generalization of the acceleration technique (e.g., the momentum-based
variance reduction technique) [ 17,53] from single-level to bilevel problems. The main challenge is
that we need to deal with the accumulated error of the recursive momentum over time due to the
hypergradient bias, which is caused by the inaccurate estimation of the optimal lower-level variable.
Therefore we require a very small tracking error between the iterate of the lower-level variable and the
optimal lower-level solution defined by the upper-level variable (i.e., y∗(x)) at every iteration. This
requirement is satisfied by executing SNAG method under the distribution drift for the lower-level
problem, where the drift is caused by the change of the upper-level variable over time.
The detailed description of our algorithm is illustrated in Algorithm 2. At the very beginning, we run
a certain number of iterations of SNAG for the fixed upper-level variable x0(line2) as the warm-start
stage, and then update the lower-level variable by SNAG (line 8∼20) with averaging (line 21) and
update the upper-level variable by normalized stochastic gradient descent with recursive momentum
5(line23∼24). Note that we have two options for implementing SNAG. In Option I (line 8∼9),
the algorithm simply runs SNAG under distribution drift caused by the sequence {xt}. Option I is
specifically designed for a particular subset of bilevel optimization problems where the lower-level
function is a quadratic function. Option II (line 11∼20) is designed for a broader range of bilvel
optimization problems, accommodating general strongly-convex lower-level functions. In Option II,
we run SNAG with periodic updates: the lower-level update is performed for Niterations only when
the iteration number tis a multiple of I.
4.2 Main Results
We first introduce some useful notations. Let σ(·)be the σ-algebra generated by the random
variables in the argument. We define the following filtrations for t≥1:Finit=σ(˜π0, . . . , ˜πT0−1),
Ft=σ(¯ξ0, . . . , ¯ξt−1),eF1
t=σ(π0, . . . , π t−1), and we also define eF2
t=σ(π0
t, . . . , πN−1
t)when tis
a multiple of I. We use Et,EFtandEto denote the conditional expectation E[· | F t], the expectation
overFtand the total expectation over FTrespectively.
Theorem 4.1. Suppose Assumptions 3.1 to 3.4 hold. Let {xt}be the iterates produced by Algo-
rithm 2. For any given δ∈(0,1)and small enough ϵ(see exact choice in (52)), we set parameters
αinit, α, β, γ, η, τ, I, N, S, Q, T 0(see exact choices in (53),(54),(55),(56), and (57)) as
αinit=eΘ(ϵ4), α =eΘ(ϵ2),1−β=eΘ(ϵ2), η =eΘ(ϵ2), τ =eΘ(ϵ), γ =O(1),
T0=eO(ϵ−2), I =eO(ϵ−1), N =eO(ϵ−1), Q =eO(1), S =eO(1).
Then with probability at least 1−2δover the randomness in σ(Finit∪eF1
T)(for Option I) or
σ(Finit∪(∪t≤TeF2
t))(for Option II), Algorithm 2 guarantees1
TPT
t=1E∥∇Φ(xt)∥ ≤20ϵwithin
T=4d0
ηϵ=eO(ϵ−3)iterations, where d0:= Φ( x0)−infxΦ(x)and the expectation is taken over
the randomness over FT. For Option I, it requires T0+SQT =eO(ϵ−3)oracle calls of stochastic
gradient or Hessian/Jacobian vector product. For Option II, it requires T0+NT
I+SQT =eO(ϵ−3)
oracle calls of stochastic gradient or Hessian/Jacobian vector product.
Remark: Theorem 4.1 established an improved eO(ϵ−3)oracle complexity for finding an ϵ-stationary
point. This complexity result strictly improves the eO(ϵ−4)obtained by [ 36,30] when the upper-level
function is nonconvex and unbounded smooth. This complexity result also matches that in the
single-level unbounded smooth setting [ 53] and is nearly optimal in terms of the dependency on ϵ[2].
The full statement of Theorem 4.1 is included in Theorem E.2.
4.3 Proof Sketch
In this section, we provide a roadmap of proving Theorem 4.1 and the main steps. The detailed proofs
can be found in Appendix D and E. The key idea is to prove two things: (1) the lower-level iterate is
very close to the optimal lower-level variable at every iteration; (2) two consecutive iterates of the
lower-level iterates are close to each other. In particular, define y∗
t=y∗(xt), and we aim to prove
that∥ˆyt−y∗
t∥ ≤O(ϵ)and∥ˆyt+1−ˆyt∥ ≤O(ϵ2)for every t. These two requirements are essential to
control the hypergradient estimation error (i.e., ∥mt−∇Φ(xt)∥) caused by inaccurate estimate of the
lower-level problem. Lemma 4.7 provides the guarantee for the lower-level problem, and Lemma 4.8
characterizes the hypergradient estimation error. Equipped with these two lemmas, we can adapt the
momentum-based variance reduction techniques [ 17,53] to the upper-level problem and prove the
main theorem.
The main technical contribution of this paper is to provide a general framework for proving the
convergence of SNAG under distributional drift in Section 4.3.1, which can be leveraged as a tool
to control the lower-level error in bilevel optimization and derive the Lemma 4.7, as illustrated in
Section 4.3.2. In particular, we can regard the change of the upper-level variable xat each iteration as
the distributional drift for the lower-level problem: the drift is small due to the normalization operator
of the upper-level update rule and also the Lipschitzness of y∗(x). Once we have the general lemma
for tracking the minimizer for any fixed distributional drift over time, this lemma can be applied to
our algorithm analysis and establish guarantees for the bilevel problem.
64.3.1 Stochastic Nesterov Accelerated Gradient Descent under Distributional Drift
In this section, we study the sequences of stochastic optimization problems minw∈Rdϕt(w)indexed
by time t∈N. We denote the minimizer and the minimal value of ϕtasw∗
tandϕ∗
t, and we define
theminimizer drift at time tto be ∆t:=∥w∗
t−w∗
t+1∥. With a slight abuse of notation2, we consider
the SNAG algorithm applied to the sequence {ϕt}T
t=1, where Tis the total number of iterations:
zt=wt+γ(wt−wt−1)
wt+1=wt+γ(wt−wt−1)−αgt,(3)
where gt=∇ϕt(zt;ξt)is the stochastic gradient evaluated at ztwith random sample ξt. Define
εt=gt− ∇ϕt(zt)as the stochastic gradient noise at t-th iteration. Define Ht=σ(ξ1, . . . , ξ t−1)as
the filtration, which is the σ-algebra generated by all random variables until t-th iteration. We make
the following assumption, which is the same as Assumption 3 in [18] for high probability analysis.
Assumption 4.2. Function ϕt:Rd→Risµ-strongly convex and L-smooth for constants µ, L > 0.
Also, there exists constants ∆, σ > 0such that the drift ∆2
tis sub-exponential conditioned on Ht
with parameter ∆2and the noise εtis norm sub-Gaussian conditioned on Htwith parameter σ/2.
Lemma 4.3. Suppose Assumption 4.2 holds and let {wt}be the iterates produced by the update
rule(3)with constant learning rate α≤min{1/2L,1/103µ, µ/103L2}, and set γ=1−√µα
1+√µα. Define
θt= [(wt−w∗
t)⊤,(wt−1−w∗
t)]⊤∈R2d, and the potential function Vtas
Vt=θ⊤
tPθt+ϕt(wt)−ϕt(w∗
t),where P=1
2α1√µα−1√µα−1 (1−√µα)2
⊗Id.
Then for any given δ∈(0,1)and all t≥0, the following holds with probability at least 1−δover
the randomness in Ht(here edenotes the base of natural logarithms):
(i) (With drift) Let w∈Rdandϕt(w):=µ
2∥w−w∗
t∥2, then
Vt≤
1−√µα
4t
V0+
2ασ2+80∆2
α
lneT
δ.
(ii)(Without drift) Let w∈Rdandϕt(w)≡ϕ(w)be any general strongly convex functions with
∆ = 0 , then
Vt≤
1−√µα
4t
V0+ 2ασ2lneT
δ.
Remark : When {ϕt}T
t=1is a sequence of quadratic functions with moving minimizers, Lemma 4.3
provides a high probability tracking guarantee for SNAG with distributional drift, which is useful
to provide guarantees for Option I in Algorithm 2. Note that this guarantee strictly improves the
guarantee of stochastic gradient descent with distributional drift (e.g., Theorem 6 in [ 18]) and
therefore is of independent interest. In particular, for small α, the decaying factor in the first term
is improved from 1−µα
2to1−√µα
4, and the drift term is improved from∆2
α2to∆2
α. To the best
of our knowledge, this is the first high probability guarantee with improved rate for SNAG under
distributional drift. When there is no drift, Lemma 4.3 also provides a high probability guarantee for
SNAG. It holds for any smooth and strongly convex function ϕ, and it is useful to provide guarantees
for Option II of Algorithm 2.
4.3.2 Application of Stochastic Nesterov Accelerated Gradient to Bilevel Optimization
Inspired by [ 36,30], we can regard ϕt(·)asϕt(·) :=g(xt,·)in the bilevel setting, and then we have
∆t=ηlg,1/µfor every tdue to the upper-level update rule and the Lipschitzness of y∗(x). Therefore
we can focus on the high probability analysis on the lower-level variable without worrying about the
randomness from the upper-level. Throughout, we assume Assumption 3.1, 3.2, 3.3 and 3.4 hold. In
addition, the failure probability δ∈(0,1)andϵ >0are chosen in the same way as in Theorem 4.1.
2The notation in Section 4.3.1 is independent of that in other sections, although there may be incidental
overlaps in terminology.
7Lemma 4.4 (Warm-start) .Let{yinit
t}be the iterates produced by line 2 of Algorithm 2. Set
αinit=eΘ(ϵ4)andϕt(y)≡g(x0, y). Then ∥yinit
T0−y∗
0∥ ≤pµα
32ϵ
L0holds with probability at least
1−δover the randomness in eFinit(we denote this event as Einit) inT0=eO(ϵ−2)iterations.
Remark: Lemma 4.4 shows that for fixed initialization x0, running SNAG for at most T0=eO(ϵ−2)
iterations can guarantee that the Euclidean distance between the lower-level variable yinit
T0and the
optimal solution y∗(x0)is at most O(ϵ), with high probability.
Lemma 4.5 (Option I) .Under event Einit, let{yt}be the iterates produced by Option I. Set α=eΘ(ϵ2)
andϕt(y) =g(xt, y) =µ
2∥y−y∗
t∥2. Then for any t∈[T], Algorithm 2 guarantees with probability
at least 1−δover the randomness in eF1
T(we denote this event as E1
y) that∥yt−y∗
t∥ ≤ϵ/2L0.
Lemma 4.6 (Option II) .Under event Einit, let{yt}be the iterates produced by Option II. Set
α=eΘ(ϵ2),N=eO(ϵ−1),I=eO(ϵ−1)andϕt(y) =g(xt, y)when tis a multiple of I(i.e.,xt
is fixed for each update round of Option II so gcan be general functions). Then for any t∈[T],
Algorithm 2 guarantees with probability at least 1−δover the randomness in σ(∪t≤TeF2
t)(we denote
this event as E2
y) that∥yt−y∗
t∥ ≤ϵ/L0.
Remark : Lemma 4.5 and Lemma 4.6 show that, under event Einitand both option I and option II,
the algorithm guarantees that each iterate ytisO(ϵ)-close to the the optimal lower-level variable y∗
t
at every iteration twith high probability.
Lemma 4.7 (Averaging) .Under event Einit∩ E1
y(Option I) or Einit∩ E2
y(Option II), set τ=√µα
in the averaging step (line 21 of Algorithm 2). Then for any t≥0we have ∥ˆyt−y∗
t∥ ≤2ϵ
L0and
∥ˆyt+1−ˆyt∥ ≤µϵ2
24L2
0σg,1=:ϑ.
Remark: Lemma 4.7 shows that after performing averaging operations over the sequence {yt}T
t=1,
the averaged sequence enjoys stronger guarantees. First, each averaged iterate ˆytis still O(ϵ)-close to
the optimal lower-level variable y∗
t; Second, two consecutive averaged iterates (i.e., ˆytandˆyt+1) is
O(ϵ2)-close to each other. The stronger guarantees are crucial to control the hypergradient estimation
error as described in Lemma 4.8.
Lemma 4.8. Under event Einit∩ E1
y(Option I) or Einit∩ E2
y(Option II), define ϵt=mt−
Et[¯∇f(xt,ˆyt;¯ξt)], then we have the following averaged cumulative error bound:
1
TT−1X
t=0E∥ϵt∥ ≤¯σ
T(1−β)+p
1−β¯σ+¯L0√1−βr
2(η2+ϑ2)
S+¯L1s
2(η2+ϑ2)
S(1−β)1
TT−1X
t=0E∥∇Φ(xt)∥,
where Sdenotes the batch size, and ¯σ,¯L0,¯L1are defined in Lemmas B.4 and B.6.
Remark: Lemma 4.8 characterizes the upper-level hypergradient estimation error under the good
event that the lower-level error can be controlled. One can choose hyperparameters appropriately
such that the cumulative error (i.e., LHS) grows only sublinearly in terms of T, which is important
for establishing the fast convergence of our algorithm.
5 Experiments
Deep AUC Maximization with Recurrent Neural Networks . AUC (Area Under the ROC
Curve) [ 34] is a critical metric in evaluating the performance of binary classification models.
It measures the ability of the model to distinguish between positive and negative classes, and
it is defined as the probability that the prediction score of a positive example is higher than
that is a negative example [ 33]. Deep AUC maximization [ 50,69] can be formulated as a min-
max optimization problem [ 50]:minw∈Rd,(a,b)∈R2max α∈Rf(w, a, b, α ):=Ez[F(w, a, b, α ;z)],
where F(w, a, b, α ;z) = (1 −r)(h(w;x)−a)2I[c=1]+r(h(w;x)−b)2I[c=−1]+ 2(1 +
α)(rh(w;x)I[c=−1]−(1−r)h(w;x)I[c=1])−r(1−r)α2,wdenotes the model parameter, z= (x, c)
is the random data sample ( xdenote the feature vector and c∈ {+1,−1}denotes the label), h(w,x)
is the score function defined by a neural network, and r=Pr(c= 1) denotes the ratio of positive
samples in the population. This min-max formulation is an special case of the bilevel problem with
80 5 10 15 20 25
Epoch0.50.60.70.80.9Train AUCTrain AUC vs. Epoch
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO(a) Training AUC
0 5 10 15 20 25
Epoch0.550.600.650.700.75Test AUCTest AUC vs. Epoch
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO (b) Test AUC
0 100 200 300 400 500 600
running time /s0.500.550.600.650.700.750.800.85Train AUCTrain AUC vs. running time (s)
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO (c)Training AUC vs. running time
0 100 200 300 400 500 600
running time /s0.550.600.650.700.75Test AUCTest AUC vs. running time (s)
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO (d) Test AUC vs. running time
Figure 1: Results of bilevel optimization on deep AUC maximization. Figure (a), (b) are the results
over epochs, and Figure (c), (d) are the results over running time.
g=−fin (1), which can be reformulated as the following:
min
w∈Rd,(a,b)∈R2Ez[F(w, a, b, α∗(w, a, b);z)]s.t., α∗(w, a, b)∈arg min
α∈R−Ez[F(w, a, b, α ;z)]
(4)
where (w, a, b)denotes the upper-level variable, and αdenotes the lower-level variable. In this case,
the lower-level is a quadratic function in terms of αand is strongly convex, and the upper-level
function is non-convex function with potential unbounded smoothness when using a recurrent neural
network as the predictive model.
We aim to perform imbalanced text classification task and maximize the AUC metric. The Deep
AUC maximization experiment is performed on imbalanced Sentiment140 [ 29] dataset (under the
license of CC BY 4.0), which is a binary text classification task. Specifically, we follow [ 70] to make
training set imbalanced with a pre-defined imbalanced ratio ( r), and leave the test set unchanged.
Given r, we randomly discard the positive samples (with label 1) in original training set until the
portion of positive samples equals to r. The imbalance ratio ris set to 0.2 in our experiment, which
means only 20% data is positive in the training set. We use a two-layer recurrent neural network with
input dimension=300, hidden dimension=4096, and output dimension=2 for the model prediction.
We compare with some bilevel optimization baselines, including StocBio [ 41], TTSA [ 39], SABA
[19], MA-SOBA [ 11], SUSTAIN [ 43], VRBO [ 68] and BO-REP [ 36]. We show the training and
test AUC result with 25 epochs in (a) (b) of Figure 1 and running time in (c), (d) of Figure 1. Our
algorithm AccBO achieves highest AUC score among all the baselines over epochs and running time.
The running time figure shows AccBO converges to a good result faster than other baselines. The
detailed parameter tuning and selection are included in Appendix G.
Data Hypercleaning. The Data hypercleaning task tries to learn a set of weights λfor the corrupted
training data Dtr, such that the model trained on the weighted corrupted training set can achieve
good performance on the clean validation set Dval, where the corrupted training set Dtr:={xi,¯yi}
and the label ¯yiis randomly flipped to one of other labels with probability 0< p < 1. The data
hyper-cleaning can be formulated as a bilevel optimization problem,
min
λ1
|Dval|X
ξ∈D valL(w∗(λ);ξ),s.t.w∗(λ)∈arg min
w1
|Dtr|X
ζi∈D trσ(λi)L(w;ζi) +c∥w∥2,(5)
where wis the model parameter of a neural network, and σ(x) =1
1+e−xis the sigmoid function.
We perform bilevel optimization algorithms on the noisy text classification dataset Stanford Natural
Language Inference (SNLI) [ 7] (under the license of CC BY 4.0) with a three-layer recurrent
neural network with input dimension=300, hidden dimension=4096, and output dimension=3 for the
label prediction. Each of sentence-pairs manually labeled as entailment, contradiction, and neutral.
Specifically, the label of each training data is randomly flipped to one of the other two labels with
probability p. We set p= 0.1andp= 0.2in the experiments, respectively. We compare all the
baselines used in the deep AUC maximization experiment. Different from the formulation (4)for the
deep AUC maximization, the lower-level function in (5)is not quadratic function of the lower-level
variable. Therefore we choose Option II in Algorithm 2, i.e., periodic updates for the lower-level
variable. The results are presented in Figure 2 ( p= 0.1andp= 0.2). Our algorithm AccBO exhibits
the highest classification accuracy on training and test set among all the bilevel baselines, and also
shows a high runtime efficiency. More detailed parameter tuning and selection can be found in
90.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
Epoch0.40.50.60.70.8Train AccTrain Acc vs. Epoch
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO(a) Training ACC
0 5 10 15
Epoch0.350.400.450.500.550.600.650.70Test AccTest Acc vs. Epoch
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO (b) Test ACC
0 500 1000 1500 2000 2500
running time /s0.40.50.60.7Train AccTrain Acc vs. running time (s)
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO (c)Training ACC vs. running time
0 500 1000 1500 2000 2500
running time /s0.350.400.450.500.550.600.650.70Test AccTest Acc vs. running time (s)
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO (d) Test ACC vs. running time
0 5 10 15
Epoch0.350.400.450.500.550.600.65Train AccTrain Acc vs. Epoch
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO
(e) Training ACC
0 5 10 15
Epoch0.350.400.450.500.550.600.65Test AccTest Acc vs. Epoch
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO (f)Test ACC
0 500 1000 1500 2000 2500
running time /s0.350.400.450.500.550.60Train AccTrain Acc vs. running time (s)
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO (g) Training ACC vs. running time
0 500 1000 1500 2000 2500
running time /s0.350.400.450.500.550.600.65Test AccTest Acc vs. running time (s)
StocBio
TTSA
SABA
MA-SOBA
SUSTAIN
VRBO
BO-REP
AccBO (h) Test ACC vs. running time
Figure 2: Results of bilevel optimization on data hyper-cleaning with p= 0.1. Figure (a), (b), (c), (d)
are the results with noise rate p= 0.1where (a), (b) are the results over epochs, and Figure (c), (d)
are the results over running time. Figure (e), (f), (g), (h) are the results with noise rate p= 0.2.
Appendix G. All the experiments are run on the device of NVIDIA A6000 (48GB memory) GPU and
AMD EPYC 7513 32-Core CPU.
6 Conclusion
In this paper, we propose a new algorithm named AccBO for solving bilevel optimization problems
where the upper-level is nonconvex and unbounded smooth and the lower-level problem is strongly
convex. The algorithm achieved eO(ϵ−3)oracle complexity for finding an ϵ-stationary point, which
matches the rate of the state-of-the-art single-level relaxed smooth optimization [ 53] and is nearly
optimal in terms of dependency on ϵ[2].
Limitations. One limitation of our work is that the convergence analysis for the Option I of our
algorithm relies on the lower-level problem being a quadratic function: only under this case the
algorithm becomes a single-loop procedure. However, it remains unclear how to design single-loop
algorithms for more general lower-level strongly convex functions.
Acknowledgments and Disclosure of Funding
We would like to thank the anonymous reviewers for their helpful comments. We would like to thank
Tianbao Yang for improving our proof for the quadratic function with distributional drift in the earlier
version of our paper. This work has been supported by the Presidential Scholarship, the ORIEI seed
funding, and the IDIA P3 fellowship from George Mason University, the Cisco Faculty Research
Award, and NSF award #2436217, #2425687. The Computations were run on ARGO, a research
computing cluster provided by the Office of Research Computing at George Mason University (URL:
https://orc.gmu.edu).
References
[1]G Anandalingam and DJ White. A solution method for the linear static stackelberg problem
using penalty functions. IEEE Transactions on automatic control , 35(10):1170–1173, 1990.
[2]Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Wood-
worth. Lower bounds for non-convex stochastic optimization. Mathematical Programming , 199
(1-2):165–214, 2023.
10[3]Mahmoud Assran and Michael Rabbat. On the convergence of nesterov’s accelerated gradient
method in stochastic settings. arXiv preprint arXiv:2002.12414 , 2020.
[4]Necdet Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar. Robust accel-
erated gradient methods for smooth strongly convex functions. SIAM Journal on Optimization ,
30(1):717–751, 2020.
[5]Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Opera-
tions research , 63(5):1227–1244, 2015.
[6]Zalán Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for
continual learning and streaming. Advances in neural information processing systems , 33:
14879–14890, 2020.
[7]Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large
annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326 ,
2015.
[8]Jerome Bracken and James T McGill. Mathematical programs with optimization problems in
the constraints. Operations research , 21(1):37–44, 1973.
[9]Lesi Chen, Jing Xu, and Jingzhao Zhang. On bilevel optimization without lower-level strong
convexity. arXiv preprint arXiv:2301.00712 , 2023.
[10] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating
stochastic gradient methods for bilevel problems. Advances in Neural Information Processing
Systems , 34:25294–25307, 2021.
[11] Xuxing Chen, Tesi Xiao, and Krishnakumar Balasubramanian. Optimal algorithms for stochastic
bilevel optimization under relaxed smoothness conditions. arXiv preprint arXiv:2306.12067 ,
2023.
[12] You-Lin Chen, Sen Na, and Mladen Kolar. Convergence analysis of accelerated stochastic
gradient descent under the growth condition. Mathematics of Operations Research , 2023.
[13] Ziyi Chen, Yi Zhou, Yingbin Liang, and Zhaosong Lu. Generalized-smooth nonconvex opti-
mization is as efficient as smooth nonconvex optimization. arXiv preprint arXiv:2303.02854 ,
2023.
[14] Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang.
Robustness to unbounded smoothness of generalized signsgd. Advances in neural information
processing systems , 2022.
[15] Michael Crawshaw, Yajie Bao, and Mingrui Liu. Episode: Episodic gradient clipping with
periodic resampled corrections for federated learning with heterogeneous data. In The Eleventh
International Conference on Learning Representations , 2023.
[16] Michael Crawshaw, Yajie Bao, and Mingrui Liu. Federated learning with client subsampling,
data heterogeneity, and unbounded smoothness: A new algorithm and lower bounds. In
Thirty-seventh Conference on Neural Information Processing Systems , 2023.
[17] Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex
sgd. Advances in neural information processing systems , 32, 2019.
[18] Joshua Cutler, Dmitriy Drusvyatskiy, and Zaid Harchaoui. Stochastic optimization under
distributional drift. Journal of Machine Learning Research , 24(147):1–56, 2023.
[19] Mathieu Dagréou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel
optimization that enables stochastic and global variance reduction algorithms. Advances in
Neural Information Processing Systems , 35:26698–26710, 2022.
[20] Jeffrey L Elman. Finding structure in time. Cognitive science , 14(2):179–211, 1990.
11[21] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-
convex optimization via stochastic path-integrated differential estimator. Advances in neural
information processing systems , 31, 2018.
[22] Matthew Faw, Litu Rout, Constantine Caramanis, and Sanjay Shakkottai. Beyond uniform
smoothness: A stopped analysis of adaptive sgd. arXiv preprint arXiv:2302.06570 , 2023.
[23] Matthias Feurer and Frank Hutter. Hyperparameter optimization. In Automated Machine
Learning , pages 3–33. Springer, Cham, 2019.
[24] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In International conference on machine learning , pages 1126–1135.
PMLR, 2017.
[25] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil.
Bilevel programming for hyperparameter optimization and meta-learning. In International
conference on machine learning , pages 1568–1577. PMLR, 2018.
[26] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex
stochastic programming. SIAM Journal on Optimization , 23(4):2341–2368, 2013.
[27] Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Mathematical Programming , 156(1-2):59–99, 2016.
[28] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv
preprint arXiv:1802.02246 , 2018.
[29] Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervi-
sion. CS224N project report, Stanford , 1(12):2009, 2009.
[30] Xiaochuan Gong, Jie Hao, and Mingrui Liu. A nearly optimal single loop algorithm for
stochastic bilevel optimization under unbounded smoothness. In Forty-first International
Conference on Machine Learning , 2024.
[31] Riccardo Grazzi, Massimiliano Pontil, and Saverio Salzo. Bilevel optimization with a lower-level
contraction: Optimal sample complexity without warm-start. arXiv preprint arXiv:2202.03397 ,
2022.
[32] Zhishuai Guo, Quanqi Hu, Lijun Zhang, and Tianbao Yang. Randomized stochastic
variance-reduced methods for multi-task stochastic bilevel optimization. arXiv preprint
arXiv:2105.02266 , 2021.
[33] James A Hanley and Barbara J McNeil. The meaning and use of the area under a receiver
operating characteristic (roc) curve. Radiology , 143(1):29–36, 1982.
[34] James A Hanley and Barbara J McNeil. A method of comparing the areas under receiver
operating characteristic curves derived from the same cases. Radiology , 148(3):839–843, 1983.
[35] Jie Hao, Kaiyi Ji, and Mingrui Liu. Bilevel coreset selection in continual learning: A new
formulation and algorithm. Advances in Neural Information Processing Systems , 36, 2023.
[36] Jie Hao, Xiaochuan Gong, and Mingrui Liu. Bilevel optimization under unbounded smoothness:
A new algorithm and convergence analysis. In The Twelfth International Conference on Learning
Representations , 2024.
[37] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms
for stochastic strongly-convex optimization. Journal of Machine Learning Research , 15(1):
2489–2512, 2014.
[38] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):
1735–1780, 1997.
[39] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic
algorithm framework for bilevel optimization: Complexity analysis and application to actor-
critic. SIAM Journal on Optimization , 33(1):147–180, 2023.
12[40] Quanqi Hu, Zi-Hao Qiu, Zhishuai Guo, Lijun Zhang, and Tianbao Yang. Blockwise stochastic
variance-reduced methods with parallel speedup for multi-block bilevel optimization. In
International Conference on Machine Learning , pages 13550–13583. PMLR, 2023.
[41] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and
enhanced design. In International conference on machine learning , pages 4882–4892. PMLR,
2021.
[42] Jikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang. Non-convex distributionally robust
optimization: Non-asymptotic analysis. Advances in Neural Information Processing Systems ,
34:2771–2782, 2021.
[43] Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang.
A near-optimal algorithm for stochastic bilevel optimization via double-momentum. Advances
in Neural Information Processing Systems (NeurIPS) , 34:30271–30283, 2021.
[44] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information
processing systems , 12, 1999.
[45] Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert D Nowak. A fully first-order
method for stochastic bilevel optimization. In International Conference on Machine Learning ,
pages 18083–18113. PMLR, 2023.
[46] Jeongyeol Kwon, Dohyun Kwon, Steve Wright, and Robert Nowak. On penalty methods
for nonconvex bilevel optimization and first-order stochastic approximation. arXiv preprint
arXiv:2309.01753 , 2023.
[47] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical
Programming , 133(1-2):365–397, 2012.
[48] Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. Convergence of adam under relaxed
assumptions. arXiv preprint arXiv:2304.13972 , 2023.
[49] Bo Liu, Mao Ye, Stephen Wright, Peter Stone, and Qiang Liu. Bome! bilevel optimization
made easy: A simple first-order approach. Advances in Neural Information Processing Systems ,
35:17248–17262, 2022.
[50] Mingrui Liu, Zhuoning Yuan, Yiming Ying, and Tianbao Yang. Stochastic auc maximization
with deep neural networks. ICLR , 2020.
[51] Mingrui Liu, Zhenxun Zhuang, Yunwen Lei, and Chunyang Liao. A communication-efficient
distributed gradient clipping algorithm for training deep neural networks. Advances in Neural
Information Processing Systems , 35:26204–26217, 2022.
[52] Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A generic first-order
algorithmic framework for bi-level programming beyond lower-level singleton. In International
Conference on Machine Learning , pages 6305–6315. PMLR, 2020.
[53] Zijian Liu, Srikanth Jagabathula, and Zhengyuan Zhou. Near-optimal non-convex stochastic
optimization under generalized smoothness. arXiv preprint arXiv:2302.06032 , 2023.
[54] Liam Madden, Stephen Becker, and Emiliano Dall’Anese. Bounds for the tracking error of
first-order online optimization methods. Journal of Optimization Theory and Applications , 189:
437–457, 2021.
[55] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o
(1/k2). In Soviet Mathematics Doklady , volume 27, pages 372–376, 1983.
[56] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with
implicit gradients. In Advances in Neural Information Processing Systems (NeurIPS) , pages
113–124, 2019.
[57] Amirhossein Reisizadeh, Haochuan Li, Subhro Das, and Ali Jadbabaie. Variance-reduced
clipping for non-convex optimization. arXiv preprint arXiv:2303.00883 , 2023.
13[58] Shoham Sabach and Shimrit Shtern. A first order method for solving convex bilevel optimization
problems. SIAM Journal on Optimization , 27(2):640–660, 2017.
[59] Han Shen and Tianyi Chen. On penalty-based bilevel gradient descent method. arXiv preprint
arXiv:2302.05185 , 2023.
[60] Daouda Sow, Kaiyi Ji, Ziwei Guan, and Yingbin Liang. A constrained optimization approach to
bilevel optimization with multiple inner minima. arXiv preprint arXiv:2203.01123 , 2022.
[61] Quoc Tran-Dinh, Nhan H Pham, Dzung T Phan, and Lam M Nguyen. Hybrid stochastic gradient
descent algorithms for stochastic nonconvex optimization. arXiv preprint arXiv:1905.05920 ,
2019.
[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
[63] Sharan Vaswani, Benjamin Dubois-Taine, and Reza Babanezhad. Towards noise-adaptive,
problem-adaptive (accelerated) stochastic gradient descent. In International conference on
machine learning , pages 22015–22059. PMLR, 2022.
[64] Luis Vicente, Gilles Savard, and Joaquim Júdice. Descent approaches for quadratic bilevel
programming. Journal of optimization theory and applications , 81(2):379–399, 1994.
[65] Bohan Wang, Huishuai Zhang, Zhiming Ma, and Wei Chen. Convergence of adagrad for
non-convex objectives: Simple proofs and relaxed assumptions. In The Thirty Sixth Annual
Conference on Learning Theory , pages 161–190. PMLR, 2023.
[66] Douglas J White and G Anandalingam. A penalty function approach for solving bi-level linear
programs. Journal of Global Optimization , 3:397–419, 1993.
[67] Craig Wilson, Venugopal V Veeravalli, and Angelia Nedi ´c. Adaptive sequential stochastic
optimization. IEEE Transactions on Automatic Control , 64(2):496–509, 2018.
[68] Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization.
Advances in Neural Information Processing Systems , 34:13670–13682, 2021.
[69] Yiming Ying, Longyin Wen, and Siwei Lyu. Stochastic online auc maximization. In Advances
in Neural Information Processing Systems , pages 451–459, 2016.
[70] Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang. Large-scale robust deep auc
maximization: A new surrogate loss and empirical studies on medical image classification. In
Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 3040–3049,
2021.
[71] Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. Improved analysis of clipping algorithms
for non-convex optimization. Advances in Neural Information Processing Systems , 33:15511–
15521, 2020.
[72] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. International Conference on Learning
Representations , 2020.
14A Technical Lemmas
In this section, we will introduce a few useful lemmas. The following technical lemma on recursive
control is crucial for providing high probability guarantee of the lower-level variables ytandˆyt
in Algorithm 2 at anytime . We follow a similar argument as in [ 18, Proposition 29] with a slight
generalization.
Lemma A.1 (Recursive control on MGF) .Consider scalar stochastic processes (Vt),(V′
t,1),(V′
t,2),
(Dt,1),(Dt,2)and(Xt)on a probability space with filtration (Ht), which are linked by the inequality
Vt+1≤αtVt+Dt,1q
V′
t,1+Dt,2q
V′
t,2+Xt+κt
for some deterministic constants αt∈(−∞,1]andκt∈R. Suppose the following properties hold.
•Vt, V′
t,1andV′
t,2are non-negative and Ht-measurable.
•Dt,iis mean-zero sub-Gaussian conditioned on Htwith deterministic parameter σi, and
V′
t,i≤Vtfori= 1,2:
E[exp( λDt,i)| Ht]≤exp(λ2σ2
i/2) for all λ∈R.
•Xtis non-negative and sub-exponential conditioned on Htwith deterministic parameter νt:
E[exp( λXt)| Ht]≤exp(λνt)for all 0≤λ≤1/νt.
Then the estimate
E[exp( λVt+1)]≤exp(λ(νt+κt))E[exp( λ(1 +αt)Vt/2)]
holds for any λsatisfying 0≤λ≤minn
1−αt
2(σ2
1+σ2
2),1
2νto
.
Proof of Lemma A.1. For any index t≥0and any scalar λ≥0, the law of total expectation implies
E[exp( λVt+1)]≤Eh
exp(λ(αtVt+Dt,1q
V′
t,1+Dt,2q
V′
t,2+Xt+κt))i
= exp( λκt)Eh
exp(λαtVt)Eh
exp
λ
Dt,1q
V′
t,1+Dt,2q
V′
t,2
exp(λXt)| Htii
.
Hölder’s inequality in turn yields
Eh
exp(λαtVt)Eh
exp
λ
Dt,1q
V′
t,1+Dt,2q
V′
t,2
exp(λXt)| Htii
≤r
Eh
exp
2λ
Dt,1q
V′
t,1+Dt,2q
V′
t,2
| Hti
·E[exp(2 λXt)| Ht]
≤q
exp 
2λ2(σ2
iV′
t,1+σ2
iV′
t,2)
exp(2 λνt)
≤exp 
λ2(σ2
1+σ2
2)Vt
exp(λνt)
provided 0≤λ≤1/2νt, where we use V′
t,i≤Vtfori= 1,2in the last inequality. Therefore, under
the condition that
0≤λ≤min1−αt
2(σ2
1+σ2
2),1
2νt
,
the following estimate hold for all t≥0:
E[exp( λVt+1)]≤exp(λκt)E
exp(λαtVt) exp 
λ2(σ2
1+σ2
2)Vt
exp(λνt)
= exp( λ(νt+κt))E
exp 
λ(αt+λ(σ2
1+σ2
2))Vt
≤exp(λ(νt+κt))E[exp( λ(1 +αt)Vt/2)],
where the last inequality follows by the given range of λ. Thus the proof is completed.
Next, we introduce the following Young’s inequality beyond Euclidean norm cases. This lemma
serves as an important role when dealing with distributional drift for high probability SNAG analysis.
15Lemma A.2 (Young’s inequality) .For any vectors v1, v2∈Rd, positive semidefinite (PSD) matrix
Q∈Rd×d, and scalar c >0, it holds that3
∥v1+v2∥2
Q≤(1 +c)∥v1∥2
Q+
1 +1
c
∥v2∥2
Q.
Proof of Lemma A.2. By definition of ∥ · ∥Q, we have
∥v1+v2∥2
Q= (v1+v2)⊤Q(v1+v2)
=∥v1∥2
Q+∥v2∥2
Q+ 2v⊤
1Qv2.(6)
SinceQ∈Rd×dis PSD, let Q=UU⊤be the Cholesky decomposition, then
2v⊤
1Qv2= 2v⊤
1UU⊤v2= 2(U⊤v1)⊤(U⊤v2)
≤c∥U⊤v1∥2+1
c∥U⊤v2∥2
=c∥v1∥2
Q+1
c∥v2∥2
Q,(7)
where we use Young’s inequality and definition of ∥ · ∥Qfor the second and third lines, respectively.
Combing (6) and (7) gives the result as claimed.
B Auxiliary Lemmas for Bilevel Optimization
In this section, we provide important properties of the objective function Φin bilevel optimization
problems, as well as characterizations (such as variance and bias) for stochastic hypergradient
estimator ¯∇f(x, y;¯ξ)based on Neumann series. For readers’ convenience, we only list the results
here and defer the detailed proofs to Appendix F.
Lemma B.1 (Lipschitz property, [ 36, Lemma 8]) .Under Assumptions 3.1 and 3.2, y∗(x)is(lg,1/µ)-
Lipschitz continuous.
Lemma B.2 ((L0, L1)-smoothness, [ 36, Lemma 9]) .Under Assumptions 3.1 and 3.2, for any x, x′
we have
∥∇Φ(x)−∇Φ(x′)∥ ≤(L0+L1∥∇Φ(x′)∥)∥x−x′∥if∥x−x′∥ ≤1q
2(1 + l2
g,1/µ2)(L2
x,1+L2
y,1),
where (L0, L1)-smoothness constant L0andL1are defined as
L0=s
1 +l2
g,1
µ2
Lx,0+Lx,1lg,1lf,0
µ+lg,1
µ(Ly,0+Ly,1lf,0) +lf,0lg,1lg,2+lg,2µ
µ2
and L1=s
1 +l2
g,1
µ2Lx,1.
Lemma B.3 (Descent inequality, [ 36, Lemma 10]) .Suppose Assumptions 3.1 and 3.2 and 3.2 hold.
Then for any x, x′we have
Φ(x)≤Φ(x′)+⟨∇Φ(x′), x−x′⟩+L0+L1∥∇Φ(x′)∥
2∥x−x′∥2if∥x−x′∥ ≤1q
2(1 + l2
g,1/µ2)(L2
x,1+L2
y,1).
Lemma B.4 ([43, Lemma B.1]) .Under Assumptions 3.1 to 3.4, the bias of the stochastic hypergradi-
ent estimate of the upper-level objective satisfies
∥¯∇f(x, y)−E¯ξ[¯∇f(x, y;¯ξ)]∥ ≤lg,1lf,0
µ
1−µ
lg,1Q
,
where Qis the number of samples chosen to approximate the Hessian inverse. Moreover, we have
E¯ξ[∥¯∇f(x, y;¯ξ)−E¯ξ[¯∇f(x, y;¯ξ)]∥2]≤σ2
f,1+3
µ2
(σ2
f,1+l2
f,0)(σ2
g,2+ 2l2
g,1) +σ2
f,1l2
g,1:= ¯σ2.
3Here we define ∥v∥Q:=p
v⊤Qvfor any vector v∈Rdand PSD matrix Q∈Rd×d.
16Lemma B.5. Under Assumptions 3.1 to 3.4, we have
∥¯∇f(x, y)− ∇Φ(x)∥ ≤(¯L+Lx,1∥∇Φ(x)∥)∥y−y∗(x)∥,
where constant ¯Lis defined as
¯L:=Lx,0+Lx,1lg,1lf,0
µ+lg,1
µ(Ly,0+Ly,1lf,0) +lf,0µlg,2+lg,1lg,2
µ2≤L0.
Lemma B.6. Under Assumptions 3.1 to 3.4, we have
(i) For any fixed y∈Rdyand any x, x′∈Rdx,
E¯ξ∥¯∇f(x, y;¯ξ)−¯∇f(x′, y;¯ξ)∥2≤(¯L2
0+¯L2
1∥∇Φ(x)∥2)∥x−x′∥2.
(ii) For any fixed x∈Rdxand any y, y′∈Rdy,
E¯ξ∥¯∇f(x, y;¯ξ)−¯∇f(x, y′;¯ξ)∥2≤(¯L2
0+¯L2
1∥∇Φ(x)∥2)∥y−y′∥2.
In the above expressions, we define ¯L0and¯L1as
¯L0=(
4
Lx,0+Lx,1lg,1lf,0
µ+
Lx,0+Lx,1lg,1lf,0
µ
∥y−y∗(x)∥2
+6Q
2µlg,1−µ2 
l2
g,1(Ly,0+Ly,1lf,0)2+l2
f,0l2
g,2+l2
f,0l2
g,1l2
g,2Q2
(lg,1−µ)2!)1/2
,
¯L1= 2Lx,1(1 +Lx,1∥y−y∗(x)∥).
Note that in Lemma B.6, constant ¯L0depends on the value of ∥y−y∗(x)∥. When we consider this
term in Algorithm 2, it turns into ∥yt−y∗
t∥or∥ˆyt−y∗
t∥, which are both as small as O(ϵ)(and thus
bounded) with high probability by Lemmas 4.5 to 4.7. In other words, we can treat this term as
another constant for our algorithm and analysis.
C Proofs of Results in Section 4.3.1
For convenience, we will restate a few concepts included in Section 4.3.1 here. We consider the
sequences of stochastic optimization problems
min
w∈Rdϕt(w) (8)
indexed by time t∈N. We denote the minimizer and the minimal value of ϕtasw∗
tandϕ∗
t, and
we define the minimizer drift at time tto be ∆t:=∥w∗
t−w∗
t+1∥. With a slight abuse of notation,
we consider the SNAG algorithm applied to the sequence {ϕt}T
t=1, where Tis the total number of
iterations:
zt=wt+γ(wt−wt−1)
wt+1=wt+γ(wt−wt−1)−αgt,(9)
where gt=∇ϕt(zt;ξt)is the stochastic gradient evaluated at ztwith random sample ξt. Define
εt=gt− ∇ϕt(zt)as the stochastic gradient noise at t-th iteration. Define Ht=σ(ξ1, . . . , ξ t−1)as
the filtration, which is the σ-algebra generated by all random variables until t-th iteration. We will
make the following standard assumption, as illustrated below4.
Assumption C.1. The sequences of time-varying functions satisfy that, each function ϕt:Rd→R
isµ-strongly convex and L-smooth for some constants µ, L > 0.
Assumption C.2 (Sub-Gaussian drift and noise) .There exists constants ∆, σ > 0such that the
following holds for all t≥0:
(i) (Drift ) The drift ∆2
tis sub-exponential conditioned on Htwith parameter ∆2:
E
exp(λ∆2
t)| Ht
≤exp(λ∆2)for all 0≤λ≤∆−2.
4Note that Assumptions C.1 and C.2 are more concrete than that in Section 4.3.1.
17(ii) (Noise ) The noise εtis norm sub-Gaussian conditioned on Htwith parameter σ/2:
Pr{∥εt∥ ≥ϱ| Ht} ≤2 exp(−2ϱ2/σ2)for all ϱ >0.
The following lemma characterize the one-step improvement for stochastic Nesterov accelerated
gradient method. Although part of our analysis is similar to [ 12,4], our final goal is quite different: we
aim to derive a careful formulation (see (13)) such that we can apply Lemma A.1 to recursively control
the moment generating function of Vtwith distributional drift, thus leading to a high probability
bound for Vtatanytime (see Lemma C.5), while [ 12,4] only show the convergence in expectation
without distributional drift.
Lemma C.3 (Distance recursion, with drift) .Suppose that Assumptions C.1 and C.2 hold. Let {wt}
be the iterates produced by update rule (9)with constant learning rate α≤1/2Land set constants
γ, ρ > 0, and matrix P∈R2d×2das
γ=1−√µα
1 +√µα, ρ2= 1−√µα,P=1
2α1√µα−1√µα−1 (1−√µα)2
⊗Id. (10)
Define θt= [(wt−w∗
t)⊤,(wt−1−w∗
t)⊤]⊤∈R2d, also define the potential function and ut,1, ut,2
as
Vt=θ⊤
tPθt+ϕt(wt)−ϕt(w∗
t), u t,1=wt−w∗
t
∥wt−w∗
t∥, u t,2=zt−wt
∥zt−wt∥. (11)
Then for all t≥0, it holds that

wt+1−w∗
t
wt−w∗
t⊤
P
wt+1−w∗
t
wt−w∗
t
+ϕt(wt+1)−ϕt(w∗
t)
≤ρ2Vt−α(1−Lα)⟨∇ϕt(zt), εt⟩+Lα2
2∥εt∥2.(12)
Specifically, if ϕt(w):=µ
2∥w−w∗
t∥2, then we have
Vt+1≤
1−√µα
2
Vt+
1 +√µα
4
−p
2µα(1−Lα)⟨ut,1, εt⟩rµ
2∥wt−w∗
t∥
−2µ√
2α
1 +√µαα(1−Lα)⟨ut,2, εt⟩1 +√µα
2√
2α∥zt−wt∥+Lα2
2∥εt∥2#
+20µ∆2
t√µα.(13)
Proof of Lemma C.3. We first apply Lemma A.2 with
v1+v2=θt+1=
wt+1−w∗
t+1
wt−w∗
t+1
, v 1=
wt+1−w∗
t
wt−w∗
t
, v 2=
w∗
t−w∗
t+1
w∗
t−w∗
t+1
,Q=P
to obtain
Vt+1=θ⊤
t+1Pθt+1+ϕt+1(wt+1)−ϕt+1(w∗
t+1)
≤
1 +√µα
4
wt+1−w∗
t
wt−w∗
t⊤
P
wt+1−w∗
t
wt−w∗
t
+
1 +4√µα
w∗
t−w∗
t+1
w∗
t−w∗
t+1⊤
P
w∗
t−w∗
t+1
w∗
t−w∗
t+1
+ϕt+1(wt+1)−ϕt+1(w∗
t+1)
=
1 +√µα
4(
wt+1−w∗
t
wt−w∗
t⊤
P
wt+1−w∗
t
wt−w∗
t
+ϕt(wt+1)−ϕt(w∗
t))
| {z }
(A)
+ϕt+1(wt+1)−ϕt+1(w∗
t+1)−
1 +√µα
4
(ϕt(wt+1)−ϕt(w∗
t))
| {z }
(B)+
1 +4√µα
w∗
t−w∗
t+1
w∗
t−w∗
t+1⊤
P
w∗
t−w∗
t+1
w∗
t−w∗
t+1
| {z }
(C).
Now we bound terms (A),(B)and(C)individually.
18Bounding (A).Let us define vector ωt∈R2dand matrices A,B∈R2d×2das
ωt=
∇ϕt(zt)
εt
,A=
1 +γ−γ
1 0
⊗Id,B=
−α−α
0 0
⊗Id.
By (9) we have [(wt+1−w∗
t)⊤,(wt−w∗
t)⊤]⊤=Aθt+Bωt. Since ϕtisµ-strongly convex, then
ϕt(wt)−ϕt(zt)≥ ⟨∇ ϕt(zt), wt−zt⟩+µ
2∥wt−zt∥2. (14)
ByL-smoothness of ϕtand the fact that wt+1=zt−αgt, we have
ϕt(zt)−ϕt(wt+1)≥ ⟨∇ ϕt(zt), zt−wt+1⟩ −L
2∥wt+1−zt∥2
=α⟨∇ϕt(zt), gt⟩ −Lα2
2∥gt∥2
=α∥∇ϕt(zt)∥2+α⟨∇ϕt(zt), εt⟩ −Lα2
2(∥∇ϕt(zt)∥2+ 2⟨∇ϕt(zt), εt⟩+∥εt∥2)
=α
2(2−Lα)∥∇ϕt(zt)∥2−Lα2
2∥εt∥2+α(1−Lα)⟨∇ϕt(zt), εt⟩,(15)
where we use gt=∇ϕt(zt) +εtin the second equality. Noting that by (9) we have
wt−zt=−γ(wt−w∗
t) +γ(wt−1−w∗
t),
and combining (14) and (15) we obtain
ϕt(wt)−ϕt(wt+1)≥ ⟨∇ ϕt(zt), wt−zt⟩+µ
2∥wt−zt∥2
+α
2(2−Lα)∥∇ϕt(zt)∥2−Lα2
2∥εt∥2+α(1−Lα)⟨∇ϕt(zt), εt⟩
=
θt
ωt⊤
X1
θt
ωt
−Lα2
2∥εt∥2+α(1−Lα)⟨∇ϕt(zt), εt⟩,(16)
where matrix X1∈R4d×4dis defined as
X1=1
2
µγ2−µγ2−γ 0
−µγ2µγ2γ 0
−γ γ α (2−Lα) 0
0 0 0 0
⊗Id.
Then applying the strong convexity of ϕtagain gives
ϕt(w∗
t)−ϕt(zt)≥ ⟨∇ ϕt(zt), w∗
t−zt⟩+µ
2∥w∗
t−zt∥2. (17)
Noting that
w∗
t−zt= (w∗
t−wt)−γ(wt−w∗
t) +γ(wt−1−w∗
t)
=−(1 +γ)(wt−w∗
t) +γ(wt−1−w∗
t),
and combining (15) and (17) we obtain
ϕt(w∗
t)−ϕt(wt+1)≥ ⟨∇ ϕt(zt), w∗
t−zt⟩+µ
2∥w∗
t−zt∥2
+α
2(2−Lα)∥∇ϕt(zt)∥2−Lα2
2∥εt∥2+α(1−Lα)⟨∇ϕt(zt), εt⟩
=
θt
ωt⊤
X2
θt
ωt
−Lα2
2∥εt∥2+α(1−Lα)⟨∇ϕt(zt), εt⟩,(18)
where matrix X2∈R4d×4dis defined as
X2=1
2
µ(1 +γ)2−µγ(1 +γ)−(1 +γ) 0
−µγ(1 +γ) µγ2γ 0
−(1 +γ) γ α (2−Lα) 0
0 0 0 0
⊗Id.
19Next, we multiply (16) by ρ2and (18) by 1−ρ2, then sum them up to get
ρ2(ϕt(wt)−ϕt(w∗
t))−(ϕt(wt+1)−ϕt(w∗
t))
≥
θt
ωt⊤
(ρ2X1+ (1−ρ2)X2)
θt
ωt
−Lα2
2∥εt∥2+α(1−Lα)⟨∇ϕt(zt), εt⟩.(19)
With definition of γ, ρ2andPgiven in the statement of the theorem, by direct calculation we have

A⊤PA−ρ2P A⊤PB
B⊤PA B⊤PB
−(ρ2X1+ (1−ρ2)X2)
=
−µ(1−√µα)3
2√µα(1+√µα)µ(1−√µα)3
2√µα(1+√µα)0 −1+µα
2(1+√µα)
µ(1−√µα)3
2√µα(1+√µα)−µ(1−√µα)3
2√µα(1+√µα)01−√µα
2(1+√µα)
0 0 −α(1−Lα)
2α
2
−1+µα
2(1+√µα)1−√µα
2(1+√µα)α
2α
2
⊗Id:=C,
where matrix Cis negative semidefinite [12], i.e., C⪯0. Then we use this fact to leverage

wt+1−w∗
t
wt−w∗
t⊤
P
wt+1−w∗
t
wt−w∗
t
−ρ2θ⊤
tPθt=
θt
ωt⊤
(ρ2X1+ (1−ρ2)X2+C)
θt
ωt
≤
θt
ωt⊤
(ρ2X1+ (1−ρ2)X2)
θt
ωt
≤ −(ϕt(wt+1)−ϕt(w∗
t)) +ρ2(ϕt(wt)−ϕt(w∗
t))−α(1−Lα)⟨∇ϕt(zt), εt⟩+Lα2
2∥εt∥2,
where the last inequality follows by (19). Rearrange the above inequality and by definition of the
potential function Vt, we obtain

wt+1−w∗
t
wt−w∗
t⊤
P
wt+1−w∗
t
wt−w∗
t
+ϕt(wt+1)−ϕt(w∗
t)
≤ρ2Vt−α(1−Lα)⟨∇ϕt(zt), εt⟩+Lα2
2∥εt∥2.(20)
Now recall that in (8)our objective function has the form of ϕt(w) =µ
2∥w−w∗
t∥2, hence ∇ϕt(zt) =
µ(zt−w∗
t). Plugging this into the above inequality gives

wt+1−w∗
t
wt−w∗
t⊤
P
wt+1−w∗
t
wt−w∗
t
+ϕt(wt+1)−ϕt(w∗
t)
≤ρ2Vt−µα(1−Lα)⟨zt−w∗
t, εt⟩+Lα2
2∥εt∥2
=ρ2Vt−µα(1−Lα)⟨wt−w∗
t, εt⟩ −µα(1−Lα)⟨zt−wt, εt⟩+Lα2
2∥εt∥2
=ρ2Vt−p
2µα(1−Lα)⟨ut,1, εt⟩rµ
2∥wt−w∗
t∥
−2µ√
2α
1 +√µαα(1−Lα)⟨ut,2, εt⟩1 +√µα
2√
2α∥zt−wt∥+Lα2
2∥εt∥2,(21)
where ut,1andut,2are defined as
ut,1=wt−w∗
t
∥wt−w∗
t∥, u t,2=zt−wt
∥zt−wt∥.
20Therefore, we conclude that
(A)≤
1 +√µα
4
ρ2Vt−p
2µα(1−Lα)⟨ut,1, εt⟩rµ
2∥wt−w∗
t∥
−2µ√
2α
1 +√µαα(1−Lα)⟨ut,2, εt⟩1 +√µα
2√
2α∥zt−wt∥+Lα2
2∥εt∥2#
≤
1−3√µα
4
Vt+
−p
2µα(1−Lα)⟨ut,1, εt⟩rµ
2∥wt−w∗
t∥
−2µ√
2α
1 +√µαα(1−Lα)⟨ut,2, εt⟩1 +√µα
2√
2α∥zt−wt∥+Lα2
2∥εt∥2#
,(22)
where the last inequality follows from the definition of ρand simple calculation

1 +√µα
4
ρ2=
1 +√µα
4
(1−√µα)≤1−3√µα
4.
Bounding (B).Recall that under distributional drift, the objective function in (8)has the form of
ϕt(w) =µ
2∥w−w∗
t∥2, then we have
(B) =ϕt+1(wt+1)−ϕt+1(w∗
t+1)−
1 +√µα
4
(ϕt(wt+1)−ϕt(w∗
t))
≤
1 +√µα
4
(ϕt+1(wt+1)−ϕt+1(w∗
t+1)−ϕt(wt+1) +ϕt(w∗
t))
=µ
2
1 +√µα
4
(∥wt+1−w∗
t+1∥2− ∥wt+1−w∗
t∥2)
≤µ
2
1 +√µα
4
∥w∗
t−w∗
t+1∥∥wt+1−w∗
t+wt+1−w∗
t+1∥
≤µ
2
1 +√µα
4
∆t(2∥wt+1−w∗
t+1∥+∥w∗
t+1−w∗
t∥)
Since ϕt+1isµ-strongly convex and matrix Pis PSD, then
Vt+1=θ⊤
t+1Pθt+1+ϕt+1(wt+1)−ϕt(w∗
t+1)≥ϕt+1(wt+1)−ϕt(w∗
t+1)
≥µ
2∥wt+1−w∗
t+1∥2=⇒ ∥ wt+1−w∗
t+1∥ ≤r2
µp
Vt+1.
Plugging the above fact back into the upper bound for (B)gives
(B)≤µ
2
1 +√µα
4
∆t
2r2
µp
Vt+1+ ∆ t
=p
2µ∆t
1 +√µα
4p
Vt+1+µ
2
1 +√µα
4
∆2
t.(23)
Bounding (C).For this part, we handle the distributional drift. By definition of Pin(10), we have
(C) =
1 +4√µα
w∗
t−w∗
t+1
w∗
t−w∗
t+1⊤1
2αId (√µα−1)Id
(√µα−1)Id(1−√µα)2Id
w∗
t−w∗
t+1
w∗
t−w∗
t+1
=µ
2
1 +4√µα
∆2
t,(24)
where in the last equality we use the basic algebra of block matrix multiplication and the definition of
∆t=∥w∗
t−w∗
t+1∥.
21Final Bound for Vt+1.Now we are ready to derive the upper bound for Vt+1. Combining (22),
(23) and (24) together yields
Vt+1≤(A) + (B) + (C)
≤
1−3√µα
4
Vt+
−p
2µα(1−Lα)⟨ut,1, εt⟩rµ
2∥wt−w∗
t∥
−2µ√
2α
1 +√µαα(1−Lα)⟨ut,2, εt⟩1 +√µα
2√
2α∥zt−wt∥+Lα2
2∥εt∥2#
+p
2µ∆t
1 +√µα
4p
Vt+1+µ
1 +√µα
8+2√µα
∆2
t.(25)
For simplicity, we define Das the following
D=
1−3√µα
4
Vt+
−p
2µα(1−Lα)⟨ut,1, εt⟩rµ
2∥wt−w∗
t∥
−2µ√
2α
1 +√µαα(1−Lα)⟨ut,2, εt⟩1 +√µα
2√
2α∥zt−wt∥+Lα2
2∥εt∥2#
+µ
1 +√µα
8+2√µα
∆2
t.
Hence (25) turns into
Vt+1−p
2µ∆t
1 +√µα
4p
Vt+1−D≤0.
Solving the above inequality we get
p
Vt+1≤1
2
p
2µ∆t
1 +√µα
4
+sp
2µ∆t
1 +√µα
42
+ 4D

≤p
2µ∆t
1 +√µα
4
+√
D
Then an application of Young’s inequality reveals
Vt+1≤
1 +√µα
4
D+
1 +4√µαp
2µ∆t
1 +√µα
42
=
1 +√µα
4
1−3√µα
4
Vt+
−p
2µα(1−Lα)⟨ut,1, εt⟩rµ
2∥wt−w∗
t∥
−2µ√
2α
1 +√µαα(1−Lα)⟨ut,2, εt⟩1 +√µα
2√
2α∥zt−wt∥+Lα2
2∥εt∥2#
+µ
1 +√µα
8+2√µα
∆2
t)
+
1 +4√µα
2µ∆2
t
1 +√µα
42
≤
1−√µα
2
Vt+
1 +√µα
4
−p
2µα(1−Lα)⟨ut,1, εt⟩rµ
2∥wt−w∗
t∥
−2µ√
2α
1 +√µαα(1−Lα)⟨ut,2, εt⟩1 +√µα
2√
2α∥zt−wt∥+Lα2
2∥εt∥2#
+µ
1 +√µα
4
1 +√µα
8+2√µα
∆2
t+
1 +4√µα
2µ∆2
t
1 +√µα
42
,
where we plug in the definition of Dfor the first equality. Since the learning rate α≤1/Land thus
µα≤1, then we have

1 +√µα
4
1 +√µα
8+2√µα
≤125
32√µα, 2
1 +4√µα
1 +√µα
42
≤125
8√µα.
22Therefore, we finally conclude that
Vt+1≤
1−√µα
2
Vt+
1 +√µα
4
−p
2µα(1−Lα)⟨ut,1, εt⟩rµ
2∥wt−w∗
t∥
−2µ√
2α
1 +√µαα(1−Lα)⟨ut,2, εt⟩1 +√µα
2√
2α∥zt−wt∥+Lα2
2∥εt∥2#
+20µ∆2
t√µα,
which is as claimed in (13).
When there is no drift, the following lemma holds for any general strongly convex functions ϕinRd.
Lemma C.4 (Distance recursion, without drift) .Under the same settings as in Lemma C.3 with
ϕt(w)≡ϕ(w),w∈Rd, and w∗
t≡w∗, where ϕ(w)can be any general strongly functions in Rd. We
redefine ut,1, ut,2as
ut,1=∇ϕ(wt)− ∇ϕ(w∗)
∥∇ϕ(wt)− ∇ϕ(w∗)∥, u t,2=∇ϕ(zt)− ∇ϕ(wt)
∥∇ϕ(zt)− ∇ϕ(wt)∥. (26)
Then for all t≥0, it holds that
Vt+1≤(1−√µα)Vt−r2
µLα(1−Lα)⟨ut,1, εt⟩1
Lrµ
2∥∇ϕt(wt)− ∇ϕt(w∗)∥
−2L√
2α
1 +√µαα(1−Lα)⟨ut,2, εt⟩1 +√µα
2L√
2α∥∇ϕt(zt)− ∇ϕt(wt)∥+Lα2
2∥εt∥2.(27)
Proof of Lemma C.4. By (12) in Lemma C.3 with ϕt(w)≡ϕ(w)andw∗
t≡w∗, we have
Vt+1≤ρ2Vt−α(1−Lα)⟨∇ϕt(zt), εt⟩+Lα2
2∥εt∥2
=ρ2Vt−α(1−Lα)⟨∇ϕt(wt)− ∇ϕt(w∗), εt⟩ −α(1−Lα)⟨∇ϕt(zt)− ∇ϕt(wt), εt⟩+Lα2
2∥εt∥2
= (1−√µα)Vt−r2
µLα(1−Lα)⟨ut,1, εt⟩1
Lrµ
2∥∇ϕt(wt)− ∇ϕt(w∗)∥
−2L√
2α
1 +√µαα(1−Lα)⟨ut,2, εt⟩1 +√µα
2L√
2α∥∇ϕt(zt)− ∇ϕt(wt)∥+Lα2
2∥εt∥2.
Hence the proof is completed.
The following result shows the first part of Lemma 4.3. To the best of our knowledge, this is the first
high probability guarantee with improved rate for SNAG under distributional drift.
Lemma C.5 (High-probability distance tracking, with drift) .Under the same setting as in Lemma C.3
withα≤min{1/2L, µ/ 2L2,1/103µ}, for any given δ∈(0,1)and all t∈[T], the following holds
with probability at least 1−δover the randomness in Ht:
Vt≤
1−√µα
4t
V0+
2ασ2+80∆2
α
lneT
δ. (28)
Proof of Lemma C.5. We will invoke Lemma A.1 to show the results. To apply Lemma A.1, we first
need to show the following two facts:
Fact (I) :rµ
2∥wt−w∗
t∥ ≤p
Vt & Fact (II) :1 +√µα
2√
2α∥zt−wt∥ ≤p
Vt.(29)
Fact (I) verification. Since ϕtisµ-strongly convex and matrix Pis PSD, then
Vt=θ⊤
tPθt+ϕt(wt)−ϕt(w∗
t)≥ϕt(wt)−ϕt(w∗
t)
≥µ
2∥wt−w∗
t∥2=⇒rµ
2∥wt−w∗
t∥ ≤p
Vt.
23Fact (II) verification. By definition of matrix Pand simple calculation we have
p
Vt≥q
θ⊤
tPθt=r
1
2α∥(wt−w∗
t) + (√µα−1)(wt−1−w∗
t)∥
=r
1
2α∥(1−√µα)(wt−wt−1) +√µα(wt−w∗
t)∥
≥r
1
2α(1−√µα)∥wt−wt−1∥ −rµ
2∥wt−w∗
t∥
≥r
1
2α(1−√µα)∥wt−wt−1∥ −p
Vt.
Rearrange the above inequality, and recall the update rule of stochastic Nesterov accelerated gradient
method, we have
∥zt−wt∥=γ∥wt−wt−1∥ ≤2γ√
2α
1−√µαp
Vt=2√
2α
1 +√µαp
Vt,
where for the last equality we use the definition of γas in (10). Rearrange it gives (29).
By Lemma C.3 we have
Vt+1≤
1−√µα
2
Vt+
1 +√µα
4p
2µα(1−Lα)⟨ut,1,−εt⟩rµ
2∥wt−w∗
t∥
+2µ√
2α
1 +√µαα(1−Lα)⟨ut,2,−εt⟩1 +√µα
2√
2α∥zt−wt∥+Lα2
2∥εt∥2#
+20µ∆2
t√µα.(30)
Note that under Assumption C.2, there exists an absolute constant c≥1such that for all t≥0,
∥εt∥2is sub-exponential conditioned on Htwith parameter cσ2, and εtis mean-zero sub-Gaussian
conditioned on Htwith parameter cσ[18, Theorem 30]. For convenience we simply let c= 1
here. Thus ⟨ut,1,−εt⟩is mean-zero sub-Gaussian conditioned on Htwith parameter σ, and ∆2
tis
sub-exponential conditioned on Htwith parameter ∆2by assumption. Hence, in light of (30), we
apply Lemma A.1 with
Ht=Ht, V t=Vt, V′
t,1=µ
2∥wt−w∗
t∥2, V′
t,2=(1 +√µα)2
8α∥zt−wt∥2,
Dt,1=
1 +√µα
4p
2µα(1−Lα)⟨ut,1,−εt⟩, D t,2=
1 +√µα
42µ√
2α
1 +√µαα(1−Lα)⟨ut,2,−εt⟩,
Xt=
1 +√µα
4Lα2
2∥εt∥2+20µ∆2
t√µα, α t= 1−√µα
2, κ t= 0,
σ1=
1 +√µα
4p
2µα(1−Lα)σ, σ 2=
1 +√µα
42µ√
2α
1 +√µαα(1−Lα)σ,
νt=
1 +√µα
4Lα2
2σ2+20µ∆2
√µα,
yielding the following recursion
E[exp( λVt+1)]≤exp
1 +√µα
4Lα2
2σ2+20µ∆2
√µα
E
exp
λ
1−√µα
4
Vt
(31)
for all λsatisfying
0≤λ≤min2
125α√µασ2,1
5ασ2/4 + 40 µ∆2/√µα
.
24We then apply (31) recursively to deduce
E[exp( λVt+1)]≤exp"
λ
1−√µα
4t
V0+λ
1 +√µα
4Lα2
2σ2+20µ∆2
√µαt−1X
i=0
1−√µα
4i#
≤exp(
λ"
1−√µα
4t
V0+ 2
1 +√µα
4Lα2
√µασ2+80∆2
α#)
for all λsatisfying
0≤λ≤min2
125α√µασ2,1
5ασ2/4 + 40 µ∆2/√µα
.
Moreover, setting
ν:= 2ασ2+80∆2
α
and taking into account α≤min{1/2L, µ/ 2L2,1/103µ}, then we have
2
1 +√µα
4Lα2
√µασ2+80∆2
α≤ν
and
1
ν=1
2ασ2+ 80∆2/α≤min2
125α√µασ2,1
5ασ2/4 + 40 µ∆2/√µα
.
Thus we obtain
E"
exp 
λ 
Vt−
1−√µα
4t
V0!!#
≤exp(λν)for all 0≤λ≤1/ν.
Taking λ= 1/νand applying Markov’s inequality and union bound completes the proof.
The following result shows the second part of Lemma 4.3.
Lemma C.6 (High-probability distance tracking, without drift) .Under the same setting as in
Lemma C.4 with α≤min{1/2L, µ/ 103L2}, for any given δ∈(0,1)and all t∈[T], the following
holds with probability at least 1−δover the randomness in Ht:
Vt≤
1−√µα
4t
V0+ 2ασ2lneT
δ. (32)
Proof of Lemma C.6. First it is easy to verify that
1
Lrµ
2∥∇ϕt(wt)− ∇ϕt(w∗)∥ ≤p
Vt and1 +√µα
2L√
2α∥∇ϕt(zt)− ∇ϕt(wt)∥ ≤p
Vt.
Then we apply Lemma A.1 to obtain the final result. We omit the detailed proof here since it follows
the same procedure as in proof of Lemma C.5.
D Proofs of Results in Section 4.3.2
We first present the following algebraic fact under suitable choice of parameters.
Lemma D.1 (Parameter choice, informal) .For any given δ∈(0,1)and any small ϵsatisfying
ϵ≤ 
164·32ed0L2
0σ2
g,1
δµ2maxlg,1
σg,1,¯σ
d0!1/3
, (33)
if we set parameters α, β, η, T as
1−β=µ2ϵ2
164·64L2
0σ2
g,1ln(P), η = minσg,1
lg,1,d0
¯σ
(1−β), α =1
µ(1−β), T =4d0
ηϵ,
(34)
25where ¯σis defined in Lemma B.4, and d0andPare defined as
d0= Φ(x0)−inf
x∈RdxΦ(x), P = 
164·64ed0L2
0σ2
g,1
δµ2ϵ3maxlg,1
σg,1,¯σ
d0!2
. (35)
Then the following holds for all t∈[T]: 
4ασ2
g,1
µ+160η2l2
g,1
µ3α!
lneT
δ≤ϵ2
64L2
0.
Proof of Lemma D.1. By Lemma B.1, we have ∆t=∥y∗
t−y∗
t+1∥ ≤lg,1
µ∥xt−xt+1∥=ηlg,1/µ.
Thus in our bilevel setting, we choose ∆ =ηlg,1/µ, where ∆is defined in Section 4.3.1. By choice
ofα, η, T as in (34), we have 
4ασ2
g,1
µ+160η2l2
g,1
µ3α!
lneT
δ= 
4(1−β)σ2
g,1
µ2+160η2l2
g,1
µ2(1−β)!
ln4ed0
δηϵ
≤164(1−β)σ2
g,1
µ2ln4ed0
δϵ(1−β)maxlg,1
σg,1,¯σ
4d0
.
Now we choose βto be
1−β=µ2ϵ2
164·64L2
0σ2
g,1ln(P),where P= 
164·64ed0L2
0σ2
g,1
δµ2ϵ3maxlg,1
σg,1,¯σ
d0!2
.
Then we have
164(1−β)σ2
g,1
µ2ln4ed0lg,1
δϵσg,1(1−β)
=ϵ2
64L2
0ln(P)ln√
Pln(P)
≤ϵ2
64L2
0,
where we use the fact that ln(√
Pln(P))≤ln(P)≤ln2(P)for any P≥4by choice of ϵas in
(33).
In the rest of this section, we assume Assumptions 3.1 to 3.4 hold. In addition, the failure probability
δ∈(0,1)andϵ >0are chosen in the same way as in Theorem 4.1.
D.1 Proof of Lemma 4.4
Lemma D.2 (Warm-start, Restatement of Lemma 4.4) .Let{yinit
t}be the iterates produced by line
2 of Algorithm 2. Set αinit=µα2=eΘ(ϵ4)withαdefined in (34) andϕt(y)≡g(x0, y). Then
∥yinit
T0−y∗
0∥ ≤pµα
32ϵ
L0holds with probability at least 1−δover the randomness in eFinit(we denote
this event as Einit) inT0=eO(ϵ−2)iterations, where
T0= lnµ3α3ϵ2
256L2
0∥yinit
0−y∗
0∥2.
ln
1−µα
4
=eO(ϵ−2). (36)
Proof of Lemma D.2. By Lemmas C.6 and D.1 and µ-strong convexity of giny, we have with
probability at least 1−δover the randomness in Finitthat
∥yinit
T0−y∗
0∥2≤2
µ 
1−p
µ2α2
4!T0
Uinit
0+4µα2σ2
g,1
µlneT0
δ
≤2
µ
1−µα
4T0
Uinit
0+µαϵ2
64L2
0,
where the first inequality uses the choice of αinit=µα2. Bylg,1-smoothness of gwe have
Uinit
0≤2−p
µ2α2+µ2α2
2µα2∥yinit
0−y∗
0∥2+g(x0, yinit
0)−g(x0, y∗
0)
≤3
2µα2∥yinit
0−y∗
0∥2+lg,1
2∥yinit
0−y∗
0∥2≤2
µα2∥yinit
0−y∗
0∥2,
26where the last inequality uses α≤1/lg,1. Now we set
2
µ
1−µα
4T02
µα2∥yinit
0−y∗
0∥2+µαϵ2
64L2
0≤µαϵ2
32L2
0,
which gives
T0≥lnµ3α3ϵ2
256L2
0∥yinit
0−y∗
0∥2.
ln
1−µα
4
.
By choice of αas in (34) and simple calculation we obtain T0=eO(ϵ−2)when ϵis small.
D.2 Proof of Lemma 4.5
Lemma D.3 (Option I, Restatement of Lemma 4.5) .Under event Einit, let{yt}be the iterates
produced by Option I. Set α=eΘ(ϵ2)as in (34) andϕt(y) =g(xt, y) =µ
2∥y−y∗
t∥2. Then for
anyt∈[T], Algorithm 2 guarantees with probability at least 1−δover the randomness in eF1
T(we
denote this event as E1
y) that∥yt−y∗
t∥ ≤ϵ/2L0.
Proof of Lemma D.3. By Lemmas C.5 and D.2 we have
∥yt−y∗
t∥2≤2
µ
1−√µα
4t
U0+ 
4ασ2
g,1
µ+160η2l2
g,1
µ3α!
lneT
δ
≤2
µ
1−√µα
4t2
α∥yinit
T0−y∗
0∥2+ 
4ασ2
g,1
µ+160η2l2
g,1
µ3α!
lneT
δ
≤4
µα∥yinit
T0−y∗
0∥2+ϵ2
64L2
0≤ϵ2
4L2
0.
Thus we conclude that for all t∈[T], we have ∥yt−y∗
t∥ ≤ϵ/2L0.
D.3 Proof of Lemma 4.6
Lemma D.4 (Option II, Restatement of Lemma 4.6) .Under event Einit, let{yt}be the iterates
produced by Option II. Set α=eΘ(ϵ2)as in (34) and run SNAG in each update round for
N= lnµα
128.
ln
1−√µα
4
=eO(ϵ−1)
steps in every I=µϵ
2(1−β)L0σg,1=eO(ϵ−1)iterations, set ϕt(y) =g(xt, y)when tis a multiple of
I(i.e.,xtis fixed for each update round of Option II so gcan be general functions). Then for any
t∈[T], Algorithm 2 guarantees with probability at least 1−δover the randomness in σ(∪t≤TeF2
t)
(we denote this event as E2
y) that∥yt−y∗
t∥ ≤ϵ/L0.
Proof of Lemma D.4. At the beginning of the first round, by Lemmas D.2 and D.3 we have ∥y0−
y∗
0∥ ≤ϵ/2L0, then we do not update the lower-level variable until t=I-th iteration, then for t=I,
we have
∥yI−y∗
I∥=∥y0−y∗
I∥ ≤ ∥ y0−y∗
0∥+IX
i=1∥y∗
i−y∗
i−1∥
≤ϵ
2L0+ηlg,1
µI=ϵ
L0,
where in the last equality we plug in the definition of ηandI. Bylg,1-smoothness of gwe have
UI≤2−2√µα+µα
2α∥yI−y∗
0∥2+g(xI, yI)−g(xI, y∗
I)
≤3
2α∥yI−y∗
I∥2+lg,1
2∥yI−y∗
I∥2≤2ϵ2
αL2
0
27Then for Nsteps update in the inner loops of t=I-th iteration, we set
2
µ
1−√µα
4N2ϵ2
αL2
0+ϵ2
64L2
0≤ϵ2
16L2
0,
which gives
N≥lnµα
128.
ln
1−√µα
4
By choice of αas in (34) and simple calculation we obtain N=eO(ϵ−1)when ϵis small. Now we
have
∥yI+1−y∗
I∥2=∥yN
I−y∗
I∥2≤2
µ
1−√µα
4N2ϵ2
αL2
0+ϵ2
64L2
0≤ϵ2
16L2
0,
which yields
∥yI+1−y∗
I+1∥ ≤ ∥ yI+1−y∗
I∥+∥y∗
I−y∗
I+1∥ ≤ϵ
4L0+ηlg,1
µ≤ϵ
2L0,
where we choose 1−βto be small (see (53) for details) such that ηis small enough to make above
inequality holds. Repeating the same process yields the result.
D.4 Proof of Lemma 4.7
Lemma D.5 (Averaging, Restatement of Lemma 4.7) .Under Assumptions 3.1 to 3.4 and event
Einit∩ E1
y(Option I) or Einit∩ E2
y(Option II), we further set τ=√µαin the averaging step (line 21
of Algorithm 2). Then for any t≥0we have
∥ˆyt−y∗
t∥ ≤2ϵ
L0and∥ˆyt+1−ˆyt∥ ≤µϵ2
24L2
0σg,1=:ϑ.
Proof of Lemma D.5. We will first show the following result by induction, i.e., for any t≥0, the
averaged sequence {ˆyt}satisfies
∥ˆyt−y∗
t∥ ≤(1−τ)ηlg,1
τµ+ϵ
L0. (37)
Fort= 0, by Lemma D.2 we have
∥ˆy0−y∗
0∥=∥yinit
T0−y∗
0∥ ≤rµα
32ϵ
L0=r
1−β
32ϵ
L0≤ϵ
L0,
thus the base case holds. Now suppose (37) holds for some t≥0, then for time step t+ 1we have
∥ˆyt+1−y∗
t+1∥=∥(1−τ)(ˆyt−y∗
t+1) +τ(yt+1−y∗
t+1)∥
=∥(1−τ)(ˆyt−y∗
t) + (1 −τ)(y∗
t−y∗
t+1) +τ(yt+1−y∗
t+1)∥
≤(1−τ)∥ˆyt−y∗
t∥+ (1−τ)∥y∗
t−y∗
t+1∥+τ∥yt+1−y∗
t+1∥
≤(1−τ)(1−τ)ηlg,1
τµ+ϵ
L0
+(1−τ)ηlg,1
µ+τϵ
L0
≤(1−τ)ηlg,1
τµ+ϵ
L0,
where we use induction hypothesis in the second inequality. Therefore, we have that (37) holds for
anyt≥0. Also, as a consequence, for any t≥0we have
∥ˆyt+1−ˆyt∥=∥τ(yt+1−ˆyt)∥
≤τ∥yt+1−y∗
t+1∥+τ∥y∗
t+1−y∗
t∥+τ∥y∗
t−ˆyt∥
≤τϵ
L0+ηlg,1
µ+(1−τ)ηlg,1
τµ+ϵ
L0
=τηlg,1
τµ+2ϵ
L0
.
28Now we plug in the definition of α, β, τ, η as in (34) to obtain
∥ˆyt−y∗
t∥ ≤(1−τ)ηlg,1
τµ+ϵ
L0=σg,1
µp
1−β+ϵ
L0≤2ϵ
L0
and
∥ˆyt+1−ˆyt∥ ≤τηlg,1
τµ+2ϵ
L0
≤µϵ2
24L2
0σg,1.
D.5 Proof of Lemma 4.8
Lemma D.6 (Restatement of Lemma 4.8) .Under Assumptions 3.1 to 3.4 and event Einit∩E1
y(Option
I) orEinit∩E2
y(Option II), define ϵt=mt−Et[¯∇f(xt,ˆyt;¯ξt)], then we have the following averaged
cumulative error bound:
1
TT−1X
t=0E∥ϵt∥ ≤¯σ
T(1−β)+p
1−β¯σ+¯L0√1−βr
2(η2+ϑ2)
S+¯L1s
2(η2+ϑ2)
S(1−β)1
TT−1X
t=0E∥∇Φ(xt)∥.
Proof of Lemma D.6. Define ϵt=mt−Et[¯∇f(xt,ˆyt;¯ξt)], also define ˜ϵtandˆϵtas
˜ϵt=¯∇f(xt,ˆyt;¯ξt)−Et[¯∇f(xt,ˆyt;¯ξt)],
ˆϵt=¯∇f(xt,ˆyt;¯ξt)−¯∇f(xt−1,ˆyt−1;¯ξt)−Et[¯∇f(xt,ˆyt;¯ξt)] +Et[¯∇f(xt−1,ˆyt−1;¯ξt)].
By definition of ϵt,˜ϵtandˆϵt, we have the following recursion for any t≥0:
ϵt+1=βϵt+ (1−β)ˆϵt+1+β˜ϵt+1. (38)
Then we apply (38) recursively to obtain
ϵt=βtϵ0+βtX
i=1βt−i˜ϵi+ (1−β)tX
i=1βt−iˆϵi,
which by triangle inequality and total expectation gives
E∥ϵt∥=βtE∥ϵ0∥|{z}
Err 1+(1−β)EtX
i=1βt−iˆϵi
|{z }
Err 2+βEtX
i=1βt−i˜ϵi
|{z }
Err 3. (39)
Bounding Err 1.By definition of ϵ0and Lemma B.4, along with Jensen’s inequality, we have
E∥ϵ0∥ ≤p
E∥ϵ0∥2≤¯σ. (40)
Bounding Err 2.We apply Lemma B.4 and follow the similar procedure as in [ 36, Lemma D.9] to
obtain
EtX
i=1βt−i˜ϵi≤vuutEtX
i=1βt−i˜ϵi2
≤vuuttX
i=1β2(t−i)E∥˜ϵi∥2≤¯σ√1−β, (41)
where we use Jensen’s inequality for the first step.
Bounding Err 3.We will first use induction to show that for 0≤i≤t+1, the following inequality
holds:
E"tX
j=1βt−jˆϵj#
≤r
2(η2+ϑ2)
S¯L1tX
j=t+1−iβt−jE∥∇Φ(xj)∥+E
vuut2(η2+ϑ2)¯L2
0
SiX
j=1β2j−2+t−iX
j=1βt−jˆϵj2
.
(42)
29Then it’s easy to check that by setting i=t+ 1we can obtain the bound. When i= 0,(42) holds
obviously since
E"tX
j=1βt−jˆϵj#
≤E
vuuttX
j=1βt−jˆϵj2
=E"tX
j=1βt−jˆϵj#
.
Hence the base case stands. Now suppose (42) holds for some i≥0, and we aim to show that (42)
holds for i+ 1. In fact, we have
E
vuut2(η2+ϑ2)¯L2
0
SiX
j=1β2j−2+t−iX
j=1βt−jˆϵj2
 (43)
=EFt−i−1
Et−i−1
vuut2(η2+ϑ2)¯L2
0
SiX
j=1β2j−2+t−iX
j=1βt−jˆϵj2

 (44)
≤EFt−i−1
Et−i−1vuuut
2(η2+ϑ2)¯L2
0
SiX
j=1β2j−2+t−iX
j=1βt−jˆϵj2

 (45)
=EFt−i−1
Et−i−1vuuut
2(η2+ϑ2)¯L2
0
SiX
j=1β2j−2+β2i∥ˆϵt−i∥2+t−i−1X
j=1βt−jˆϵj2

 (46)
≤EFt−i−1
Et−i−1vuuut
2(η2+ϑ2)¯L2
0
SiX
j=1β2j−2+β2i
S2(¯L2
0+¯L2
1∥∇Φ(xt−i)∥2)(η2+ϑ2) +t−i−1X
j=1βt−jˆϵj2


(47)
=EFt−i−1
vuut2β2i
S¯L2
1(η2+ϑ2)∥∇Φ(xt−i)∥2+2(η2+ϑ2)¯L2
0
Si+1X
j=1β2j−2+t−i−1X
j=1βt−jˆϵj2
(48)
≤EFt−i−1
r
2(η2+ϑ2)
Sβi¯L1∥∇Φ(xt−i)∥+vuut2(η2+ϑ2)¯L2
0
Si+1X
j=1β2j−2+t−i−1X
j=1βt−jˆϵj2
,(49)
where (44) follows by law of total expectation, (45) follows by Jensen’s inequality, (46) uses the
fact that ˆϵjforj < t−iareFt−i−1-measurable, and are uncorrelated with ˆϵt−i; for (47) we use
Lemmas B.6 and D.5 to derive
Et−i−1[∥ˆϵt−i∥2] =Et−i−1
∥¯∇f(xt−i,ˆyt−i;¯ξt−i)−¯∇f(xt−i−1,ˆyt−i−1;¯ξt−i)
−Et−i[¯∇f(xt−i,ˆyt−i;¯ξt−i)] +Et−i[¯∇f(xt−i−1,ˆyt−i−1;¯ξt−i)]∥2
≤Et−i−1
∥¯∇f(xt−i,ˆyt−i;¯ξt−i)−¯∇f(xt−i−1,ˆyt−i−1;¯ξt−i)∥2
≤2Et−i−1
∥¯∇f(xt−i,ˆyt−i;¯ξt−i)−¯∇f(xt−i,ˆyt−i−1;¯ξt−i)∥2
+ 2Et−i−1
∥¯∇f(xt−i,ˆyt−i−1;¯ξt−i)−¯∇f(xt−i−1,ˆyt−i−1;¯ξt−i)∥2
≤2
S(¯L2
0+¯L2
1∥∇Φ(xt−i)∥2)(∥ˆyt−i−ˆyt−i−1∥2+∥xt−i−xt−i−1∥2)
=2
S(¯L2
0+¯L2
1∥∇Φ(xt−i)∥2)(η2+ϑ2).
And (48) follows from the fact that xt−iisFt−i−1-measurable, (49) uses√
a+b≤√a+√
bfor
a, b≥0. Hence the induction proof is completed. We set i=t+ 1to obtain
E"tX
i=1βt−iˆϵi#
≤r
2(η2+ϑ2)
S¯L1tX
i=0βt−iE∥∇Φ(xi)∥+vuut2(η2+ϑ2)¯L2
0
StX
i=0β2i
≤r
2(η2+ϑ2)
S¯L1tX
i=0βt−iE∥∇Φ(xi)∥+¯L0√1−βr
2(η2+ϑ2)
S.(50)
30Final Bound. Combining (40), (41) and (50) yields
E∥ϵt∥ ≤βt¯σ+p
1−β¯σ+¯L0√1−βr
2(η2+ϑ2)
S+r
2(η2+ϑ2)
S¯L1tX
i=0βt−iE∥∇Φ(xi)∥.
Taking summation and dividing 1/Ton both sides gives the final result
1
TT−1X
t=0E∥ϵt∥ ≤¯σ
T(1−β)+p
1−β¯σ+¯L0√1−βr
2(η2+ϑ2)
S+¯L1s
2(η2+ϑ2)
S(1−β)1
TT−1X
t=0E∥∇Φ(xt)∥.
E Proof of Theorem 4.1
Before starting the proof of main results, i.e., Theorem 4.1, we first need the following lemma.
Lemma E.1. Suppose that Assumptions 3.1 to 3.4 hold. For any ηsatisfying
η≤1q
2(1 + l2
g,1/µ2)(L2
x,1+L2
y,1),
it holds that

1−1
2ηL1−2L1∥ˆyt−y∗
t∥1
TT−1X
t=0E∥∇Φ(xt)∥
≤Φ(x0)−Φ(xT)
Tη+2
TT−1X
t=0E∥ϵt∥+2lg,1lf,0
µ
1−µ
lg,1Q
+2L0
TT−1X
t=0∥ˆyt−y∗
t∥+1
2ηL0.
Proof of Lemma E.1. Define ht=mt− ∇Φ(xt). Then we apply Lemma B.3 to obtain
Φ(xt+1)≤Φ(xt) +⟨∇Φ(xt), xt+1−xt⟩+L0+L1∥∇Φ(xt)∥
2∥xt+1−xt∥2
= Φ(xt)−η⟨∇Φ(xt),mt
∥mt∥⟩+1
2η2(L0+L1∥∇Φ(xt)∥)
= Φ(xt)−η⟨mt−ht,mt
∥mt∥⟩+1
2η2(L0+L1∥∇Φ(xt)∥)
= Φ(xt)−η∥mt∥+η⟨ht,mt
∥mt∥⟩+1
2η2(L0+L1∥∇Φ(xt)∥)
≤Φ(xt)−η∥mt∥+η∥ht∥+1
2η2(L0+L1∥∇Φ(xt)∥)
≤Φ(xt)−η∥∇Φ(xt)∥+ 2η∥ht∥+1
2η2(L0+L1∥∇Φ(xt)∥),(51)
where for the last two lines we use Cauchy-Schwarz inequality and ∥ht∥=∥∇Φ(xt) +ht∥ ≥
∥∇Φ(xt)∥ − ∥ ht∥. Now expanding htby triangle inequality, we have
∥ht∥=∥mt− ∇Φ(xt)∥
≤ ∥mt−Et[¯∇f(xt,ˆyt;¯ξt)]∥+∥Et[¯∇f(xt,ˆyt;¯ξt)]−¯∇f(xt,ˆyt)∥+∥¯∇f(xt,ˆyt)− ∇Φ(xt)∥
≤ ∥ϵt∥+lg,1lf,0
µ
1−µ
lg,1Q
+ (L0+L1∥∇Φ(xt)∥)∥ˆyt−y∗
t∥,
31where we use definition of ϵt, Lemmas B.4 and B.5 in the last inequality. Plugging the above
inequality back into (51) we obtain
Φ(xt+1)≤Φ(xt)−η∥∇Φ(xt)∥+ 2η∥ϵt∥+ 2ηlg,1lf,0
µ
1−µ
lg,1Q
+ 2η(L0+L1∥∇Φ(xt)∥)∥ˆyt−y∗
t∥+1
2η2(L0+L1∥∇Φ(xt)∥)
= Φ(xt)−
η−1
2η2L1−2ηL1∥ˆyt−y∗
t∥
∥∇Φ(xt)∥+ 2η∥ϵt∥
+ 2ηlg,1lf,0
µ
1−µ
lg,1Q
+ 2ηL0∥ˆyt−y∗
t∥+1
2η2L0.
Dividing 1/Tη on both sides, then taking telescope sum and total expectation, and rearranging it
finally yields

1−1
2ηL1−2L1∥ˆyt−y∗
t∥1
TT−1X
t=0E∥∇Φ(xt)∥
≤Φ(x0)−Φ(xT)
Tη+2
TT−1X
t=0E∥ϵt∥+2lg,1lf,0
µ
1−µ
lg,1Q
+2L0
TT−1X
t=0∥ˆyt−y∗
t∥+1
2ηL0.
Theorem E.2 (Restatement of Theorem 4.1) .Suppose Assumptions 3.1 to 3.4 hold. Let {xt}be the
iterates produced by Algorithm 2. For any given δ∈(0,1)and any small ϵ >0satisfying
ϵ≤min

L0
32L1,lg,1L0
µ¯L1,L0
8¯L1,L0lg,1σg,1
µ2,L0
µr
lg,1σg,1
L1, 
164·32ed0L2
0σ2
g,1
δµ2maxlg,1
σg,1,¯σ
d0!1/3

,
(52)
if we set parameters α, α′, αinit, β, γ, η, τ, I, N, S, Q, T 0as
1−β= min(
µ2ϵ2
164·16L2
0σ2
g,1ln(P),lg,1
4σg,1L1,ϵ2
4¯σ2)
, η = minσg,1
lg,1,d0
¯σ
(1−β),(53)
αinit=1−β
µ+lg,1α=1−β
µ, γ =1−√µα
1 +√µα, τ = 1−√µα, (54)
T0= lnµ3α3ϵ2
256L2
0∥yinit
0−y∗
0∥2.
ln
1−µα
4
, (55)
I=µϵ
2(1−β)L0σg,1, N = lnµα
128.
ln
1−√µα
4
, (56)
S= max(
128 ln( P),128¯L2
0
L2
0ln(P),µ2¯L2
0
l2
g,1L2
0)
, Q = ln
1−µ
lg,1.
lnµϵ
lg,1lf,0
,(57)
where d0andPare defined in (35). Then with probability at least 1−2δover the randomness
inσ(Finit∪eF1
T)(for Option I) or σ(Finit∪(∪t≤TeF2
t))(for Option II), Algorithm 2 guarantees
1
TPT
t=1E∥∇Φ(xt)∥ ≤20ϵwithin T=4d0
ηϵ=eO(1/ϵ3)iterations, where the expectation is taken
over the randomness in FT. For Option I, it requires T0+SQT =eO(1/ϵ3)oracle calls of stochastic
gradient or Hessian/Jacobian vector product. For Option II, it requires T0+NT
I+SQT =eO(1/ϵ3)
oracle calls of stochastic gradient or Hessian/Jacobian vector product.
32Proof of Theorem E.2. By Lemmas D.6 and E.1, we have

1−1
2ηL1−2L1∥ˆyt−y∗
t∥1
TT−1X
t=0E∥∇Φ(xt)∥
≤Φ(x0)−Φ(xT)
Tη+2
TT−1X
t=0E∥ϵt∥+2lg,1lf,0
µ
1−µ
lg,1Q
+2L0
TT−1X
t=0∥ˆyt−y∗
t∥+1
2ηL0
≤d0
Tη+2lg,1lf,0
µ
1−µ
lg,1Q
+2L0
TT−1X
t=0∥ˆyt−y∗
t∥+1
2ηL0
+2¯σ
T(1−β)+ 2p
1−β¯σ+2¯L0√1−βr
2(η2+ϑ2)
S+ 2¯L1s
2(η2+ϑ2)
S(1−β)1
TT−1X
t=0E∥∇Φ(xt)∥.
Rearranging the above inequality gives
 
1−1
2ηL1−2L1∥ˆyt−y∗
t∥ −2¯L1s
2(η2+ϑ2)
S(1−β)!
| {z }
(LHS)1
TT−1X
t=0E∥∇Φ(xt)∥
≤d0
Tη+2lg,1lf,0
µ
1−µ
lg,1Q
+2L0
TT−1X
t=0∥ˆyt−y∗
t∥+1
2ηL0+2¯σ
T(1−β)+ 2p
1−β¯σ+2¯L0√1−βr
2(η2+ϑ2)
S
| {z }
(RHS).
Bounding (LHS). By Lemma D.5, we have
(LHS) ≥1−σg,1L1
2lg,1(1−β)−2L12ϵ
L0−2¯L1s
2(η2+ϑ2)
S(1−β)
≥1−1
8−1
8−1
4=1
2(58)
Bounding (RHS). By choice of parameters, we have
(RHS) ≤1
4ϵ+ 2ϵ+ 4ϵ+ϵ+1
2ϵ+ϵ+ϵ≤10ϵ. (59)
Combining (58) and (59) finally yields
1
TT−1X
t=0E∥∇Φ(xt)∥ ≤20ϵ.
F Omitted Proofs in Appendix B
F.1 Proof of Lemma B.5
Lemma F.1 (Restatement of Lemma B.5) .Under Assumptions 3.1 to 3.4, we have
∥¯∇f(x, y)− ∇Φ(x)∥ ≤(¯L+Lx,1∥∇Φ(x)∥)∥y−y∗(x)∥,
where constant ¯Lis defined as
¯L:=Lx,0+Lx,1lg,1lf,0
µ+lg,1
µ(Ly,0+Ly,1lf,0) +lf,0µlg,2+lg,1lg,2
µ2≤L0.
Proof of Lemma B.5. Recall that the exact expressions of ¯∇f(x, y)and∇Φ(x)are
¯∇f(x, y) =∇xf(x, y)− ∇2
xyg(x, y)[∇2
yyg(x, y)]−1∇yf(x, y)
33and
∇Φ(x) =∇xf(x, y∗(x))− ∇2
xyg(x, y∗(x))[∇2
yyg(x, y∗(x))]−1∇yf(x, y∗(x)).
Then by Assumption 3.2 we have
∥¯∇f(x, y)− ∇Φ(x)∥ ≤ ∥∇ xf(x, y)− ∇ xf(x, y∗(x))∥
+∥∇2
xyg(x, y)[∇2
yyg(x, y)]−1∇yf(x, y)− ∇2
xyg(x, y∗(x))[∇2
yyg(x, y∗(x))]−1∇yf(x, y∗(x))∥
≤(Lx,0+Lx,1∥∇xf(x, y∗(x))∥)∥y−y∗(x)∥
+∥∇2
xyg(x, y)[∇2
yyg(x, y)]−1∇yf(x, y)− ∇2
xyg(x, y∗(x))[∇2
yyg(x, y)]−1∇yf(x, y)∥
+∥∇2
xyg(x, y∗(x))[∇2
yyg(x, y)]−1∇yf(x, y)− ∇2
xyg(x, y∗(x))[∇2
yyg(x, y∗(x))]−1∇yf(x, y)∥
+∥∇2
xyg(x, y∗(x))[∇2
yyg(x, y∗(x))]−1∇yf(x, y)− ∇2
xyg(x, y∗(x))[∇2
yyg(x, y∗(x))]−1∇yf(x, y∗(x))∥
≤
Lx,0+Lx,1lg,1lf,0
µ+∥∇Φ(x)∥
∥y−y∗(x)∥
+lf,0
µlg,2∥y−y∗(x)∥+lf,0lg,1
µ2lg,2∥y−y∗(x)∥+lg,1
µ(Ly,0+Ly,1∥∇yf(x, y∗(x))∥)∥y−y∗(x)∥
=
Lx,0+Lx,1lg,1lf,0
µ+lg,1
µ(Ly,0+Ly,1lf,0) +lf,0µlg,2+lg,1lg,2
µ2+Lx,1∥∇Φ(x)∥
∥y−y∗(x)∥.
By definition of ¯Lwe conclude the proof.
F.2 Proof of Lemma B.6
Lemma F.2 (Restatement of Lemma B.6) .Under Assumptions 3.1 to 3.4, we have
(i) For any fixed y∈Rdyand any x1, x2∈Rdx,
E¯ξ∥¯∇f(x1, y;¯ξ)−¯∇f(x2, y;¯ξ)∥2≤(¯L2
0+¯L2
1∥∇Φ(x1)∥2)∥x1−x2∥2.
(ii) For any fixed x∈Rdxand any y1, y2∈Rdy,
E¯ξ∥¯∇f(x, y1;¯ξ)−¯∇f(x, y2;¯ξ)∥2≤(¯L2
0+¯L2
1∥∇Φ(x1)∥2)∥x1−x2∥2.
In the above expressions, we define ¯L0and¯L1as
¯L0=(
4
Lx,0+Lx,1lg,1lf,0
µ+
Lx,0+Lx,1lg,1lf,0
µ
∥y1−y∗
1∥2
+6Q
2µlg,1−µ2 
l2
g,1(Ly,0+Ly,1lf,0)2+l2
f,0l2
g,2+l2
f,0l2
g,1l2
g,2K2
(lg,1−µ)2!)1/2
,
¯L1= 2Lx,1(1 +Lx,1∥y1−y∗
1∥).
Proof of Lemma B.6. We show statement (i)of the lemma, and (ii)follows by similar arguments.
For any fixed y∈Rdyand any x1, x2∈Rdx, by definition of ¯∇f(x, y;¯ξ)we have
∥¯∇f(x1, y;¯ξ)−¯∇f(x2, y;¯ξ)∥2
≤2∥∇xF(x1, y;ξ)− ∇ xF(x2, y;ξ)∥2+ 2∇2
xyG(x1, y;ζ(0))"
Q
lg,1qY
i=1
I−1
lg,1∇2
yyG(x1, y;ζ(i))#
∇yF(x1, y;ξ)
−∇2
xyG(x2, y;ζ(0))"
Q
lg,1qY
i=1
I−1
lg,1∇2
yyG(x2, y;ζ(i))#
∇yF(x2, y;ξ)2
≤2(Lx,0+Lx,1∥∇xf(x1, y)∥)2∥x1−x2∥2+ 2∇2
xyG(x1, y;ζ(0))"
Q
lg,1qY
i=1
I−1
lg,1∇2
yyG(x1, y;ζ(i))#
∇yF(x1, y;ξ)
−∇2
xyG(x2, y;ζ(0))"
Q
lg,1qY
i=1
I−1
lg,1∇2
yyG(x2, y;ζ(i))#
∇yF(x2, y;ξ)2
.
34For the second term above, we have∇2
xyG(x1, y;ζ(0))"
Q
lg,1qY
i=1
I−1
lg,1∇2
yyG(x1, y;ζ(i))#
∇yF(x1, y;ξ)
−∇2
xyG(x2, y;ζ(0))"
Q
lg,1qY
i=1
I−1
lg,1∇2
yyG(x2, y;ζ(i))#
∇yF(x2, y;ξ)2
≤3l2
g,1Q2
l2
g,1
1−µ
lg,12q
∥∇yF(x1, y;ξ)− ∇ yF(x2, y;ξ)∥2
+ 3l2
f,0Q2
l2
g,1
1−µ
lg,12q
∥∇2
xyG(x1, y;ζ(0))− ∇2
xyG(x2, y;ζ(0))∥2
+ 3l2
g,1l2
f,0Q
lg,1qY
i=1
I−1
lg,1∇2
yyG(x1, y;ζ(i))
−Q
lg,1qY
i=1
I−1
lg,1∇2
yyG(x2, y;ζ(i))2
≤3Q2
1−µ
lg,12q
(Ly,0+Ly,1∥∇yf(x1, y)∥)2∥x1−x2∥2+ 3l2
f,0Q2
l2
g,1
1−µ
lg,12q
l2
g,2∥x1−x2∥2
+ 3l2
f,0Q2qY
i=1
I−1
lg,1∇2
yyG(x1, y;ζ(i))
−qY
i=1
I−1
lg,1∇2
yyG(x2, y;ζ(i))2
.
Then we take expectation with respect to qand obtain
Eq∇2
xyG(x1, y;ζ(0))"
Q
lg,1qY
i=1
I−1
lg,1∇2
yyG(x1, y;ζ(i))#
∇yF(x1, y;ξ)
−∇2
xyG(x2, y;ζ(0))"
Q
lg,1qY
i=1
I−1
lg,1∇2
yyG(x2, y;ζ(i))#
∇yF(x2, y;ξ)2
≤ 
3Q2(Ly,0+Ly,1∥∇yf(x1, y)∥)2∥x1−x2∥2+ 3l2
f,0Q2
l2
g,1l2
g,2∥x1−x2∥2!
Eq"
1−µ
lg,12q#
+ 3l2
f,0Q2EqqY
i=1
I−1
lg,1∇2
yyG(x1, y;ζ(i))
−qY
i=1
I−1
lg,1∇2
yyG(x2, y;ζ(i))2
≤ 
3Q2(Ly,0+Ly,1∥∇yf(x1, y)∥)2∥x1−x2∥2+ 3l2
f,0Q2
l2
g,1l2
g,2∥x1−x2∥2!
·l2
g,1
Q(2µlg,1−µ2)
+ 3l2
f,0Q2·l2
g,1l2
g,2Q
(lg,1−µ)2(2µlg,1−µ2)∥x1−x2∥2
≤3Q
2µlg,1−µ2 
l2
g,1(Ly,0+Ly,1lf,0)2∥x1−x2∥2+l2
f,0l2
g,2∥x1−x2∥2
+3l2
f,0l2
g,1l2
g,2Q3
(lg,1−µ)2(2µlg,1−µ2)∥x1−x2∥2
=3Q
2µlg,1−µ2 
l2
g,1(Ly,0+Ly,1lf,0)2+l2
f,0l2
g,2+l2
f,0l2
g,1l2
g,2Q2
(lg,1−µ)2!
∥x1−x2∥2
Finally, taking expectation on both sides yields
E¯ξ∥¯∇f(x1, y;¯ξ)−¯∇f(x2, y;¯ξ)∥2≤2(Lx,0+Lx,1∥∇xf(x1, y)∥)2∥x1−x2∥2
+6Q
2µlg,1−µ2 
l2
g,1(Ly,0+Ly,1lf,0)2+l2
f,0l2
g,2+l2
f,0l2
g,1l2
g,2Q2
(lg,1−µ)2!
∥x1−x2∥2.
Since for any y∈Rdy, we have
∥∇xf(x1, y)− ∇ xf(x1, y∗
1)∥ ≤(Lx,0+Lx,1∥∇xf(x1, y∗
1)∥)∥y1−y∗
1∥
≤
Lx,0+Lx,1lg,1lf,0
µ+∥∇Φ(x1)∥
∥y1−y∗
1∥
=
Lx,0+Lx,1lg,1lf,0
µ+Lx,1∥∇Φ(x1)∥
∥y1−y∗
1∥,
35which yields
∥∇xf(x1, y)∥ ≤ ∥∇ xf(x1, y∗
1)∥+
Lx,0+Lx,1lg,1lf,0
µ+Lx,1∥∇Φ(x1)∥
∥y1−y∗
1∥
≤lg,1lf,0
µ+∥∇Φ(x1)∥+
Lx,0+Lx,1lg,1lf,0
µ+Lx,1∥∇Φ(x1)∥
∥y1−y∗
1∥
=lg,1lf,0
µ+
Lx,0+Lx,1lg,1lf,0
µ
∥y1−y∗
1∥
+ (1 + Lx,1∥y1−y∗
1∥)∥∇Φ(x1)∥.
Therefore, we conclude that
E¯ξ∥¯∇f(x1, y;¯ξ)−¯∇f(x2, y;¯ξ)∥2≤2(Lx,0+Lx,1∥∇xf(x1, y)∥)2∥x1−x2∥2
+6Q
2µlg,1−µ2 
l2
g,1(Ly,0+Ly,1lf,0)2+l2
f,0l2
g,2+l2
f,0l2
g,1l2
g,2Q2
(lg,1−µ)2!
∥x1−x2∥2
≤2
Lx,0+Lx,1lg,1lf,0
µ+
Lx,0+Lx,1lg,1lf,0
µ
∥y1−y∗
1∥
+Lx,1(1 +Lx,1∥y1−y∗
1∥)∥∇Φ(x1)∥2
∥x1−x2∥2
+6Q
2µlg,1−µ2 
l2
g,1(Ly,0+Ly,1lf,0)2+l2
f,0l2
g,2+l2
f,0l2
g,1l2
g,2Q2
(lg,1−µ)2!
∥x1−x2∥2
≤4
Lx,0+Lx,1lg,1lf,0
µ+
Lx,0+Lx,1lg,1lf,0
µ
∥y1−y∗
1∥2
∥x1−x2∥2
+ 4L2
x,1(1 +Lx,1∥y1−y∗
1∥)2∥∇Φ(x1)∥2∥x1−x2∥2
+6Q
2µlg,1−µ2 
l2
g,1(Ly,0+Ly,1lf,0)2+l2
f,0l2
g,2+l2
f,0l2
g,1l2
g,2Q2
(lg,1−µ)2!
∥x1−x2∥2
= (¯L2
0+¯L2
1∥∇Φ(x1)∥2)∥x1−x2∥2,
where we use the definition of ¯L0and¯L1in the last equality.
G Additional Experimental Details
Hyerparameter setting. We tune the best hyperparameters for each algorithm, including upper-
/lower-level step size, the number of inner loops, momentum parameters, etc. The upper-level
learning rate ηupand lower-level learing rate ηloware tuned in the range of [0.001,0.1]for all the
baselines on experiments of AUC maximization and data hyper-cleaning, the best (ηup, ηlow)on
AUC maximization are summarized as follows: StocBio: ( 0.01,0.001), TTSA: (0.005,0.01),
SABA: (0.01,0.005), MA-SOBA: (0.01,0.005), SUSTAIN: (0.03,0.01), VRBO: (0.05,0.01),
BO-REP: (0.001,0.001), AccBO: (0.005,0.005). The best learning rate on the experiment of
data hyper-cleaning are summarized as follows: Stocbio: (0.01,0.002), TTSA: (0.001,0.01),
SABA: (0.05,0.02), MA-SOBA: (0.01,0.01), SUSTAIN: (0.05,0.05), VRBO: (0.1,0.05), BO-
REP: (0.02,0.01), AccBO: (0.1,0.1). Note that SUSTAIN decays its upper-/lower-level step size
with epoch ( t) byηup=ηup/(t+2)1/3, ηlow=ηup/(t+2)1/3, while other algorithms use a constant
learning rate. The number for neumann series estimation in StocBiO and VRBO is fixed to 3, while it
is uniformly sampled from {1,2,3}in TTSA, SUSTAIN, and AccBO. In AUC maximization, AccBO
uses Option I (Option II in data hyper-cleaning) to update the lower-level variable, and sets the Ne-
strov momentum parameter γ= 0.5, the averaging parameter τ= 0.5(γ= 0.1andτ= 0.5in data
hyper-cleaning). In AUC maximization, the batch size is set to be 32for all algorithms except VRBO,
which uses larger batch size of 64(tuned in the range of {32,64,128,256,512}) at the checkpoint
step and 32otherwise. In data hyper-cleaning, the batch size is set to be 128for all algorithms except
VRBO, which uses larger batch size of 256(tuned in the range of {63,128,256,512,1024}) at the
checkpoint step and 128otherwise. AccBO uses Option II in data hyper-cleaning, and the periodical
update for low-level variable sets the iterations N= 3and update interval I= 2. Other hyperparame-
ters setting keep the same in AUC maximization and data hyper-cleaning: The momentum parameter
βis fixed to 0.9in AccBO, MA-SOBA, BO-REP. The warm start steps for lower-level variable in
AccBO is set to 3. The number of inner loops for StocBio is set to 3. BO-REP uses the periodical
update for low-level variable, and set the iterations N= 3and the update interval I= 2.
36NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Every claim made in the abstract is specified a section of the paper, including
algorithm design and analysis in Section 4 and experiments in Section 5.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discussed the limitations of our work in Section 6.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
37Justification: We provide both assumptions and proofs in Appendices C to E.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The experimental details are fully specified in Section 5 and Appendix G.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
38Answer: [Yes]
Justification: The code and data are attached as a supplement with instructions for repro-
ducibility.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The experimental details are included in Section 5 and Appendix G.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: We only run once due to limited computational budget.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
39•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The hardware specification is described in Section 5.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have read and conformed to the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper presents work whose goal is to advance the field of Machine
Learning from algorithmic and theoretical aspects. We do not see any direct paths to
negative societal impacts.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
40•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our paper does not involve the release of any data or models.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Our paper uses existing text classification datasets and are cited in Section 5
and their licenses are mentioned.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
41•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Our paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing or research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing or research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
42