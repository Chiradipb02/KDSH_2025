Conditional Outcome Equivalence: A Quantile
Alternative to CATE
Josh Givens
University of Bristol
josh.givens@bristol.ac.ukHenry W J Reeve
University of Bristol
henry.reeve@bristol.ac.uk
Song Liu
University of Bristol
song.liu@bristol.ac.ukKatarzyna Reluga
University of Bristol
katarzyna.reluga@bristol.ac.uk
Abstract
The conditional quantile treatment effect (CQTE) can provide insight into the ef-
fect of a treatment beyond the conditional average treatment effect (CATE). This
ability to provide information over multiple quantiles of the response makes the
CQTE especially valuable in cases where the effect of a treatment is not well-
modelled by a location shift, even conditionally on the covariates. Nevertheless,
the estimation of the CQTE is challenging and often depends upon the smoothness
of the individual quantiles as a function of the covariates rather than smoothness
of the CQTE itself. This is in stark contrast to the CATE where it is possible to
obtain high-quality estimates which have less dependency upon the smoothness
of the nuisance parameters when the CATE itself is smooth. Moreover, relative
smoothness of the CQTE lacks the interpretability of smoothness of the CATE
making it less clear whether it is a reasonable assumption to make. We combine
the desirable properties of the CATE and CQTE by considering a new estimand,
the conditional quantile comparator (CQC). The CQC not only retains informa-
tion about the whole treatment distribution, similar to the CQTE, but also having
more natural examples of smoothness and is able to leverage simplicity in an aux-
iliary estimand. We provide finite sample bounds on the error of our estimator,
demonstrating its ability to exploit simplicity. We validate our theory in numer-
ical simulations which show that our method produces more accurate estimates
than baselines. Finally, we apply our methodology to a study on the effect of
employment incentives on earnings across different age groups. We see that our
method is able to reveal heterogeneity of the effect across different quantiles.
1 Introduction
In many real world scenarios such as personalised treatment allocation and individual level policy
decisions, understanding the effect of a treatment/intervention at an individual level is invaluable
in providing bespoke care. The field which aims to understand a treatment’s effect given certain
covariates is referred to as heterogeneous treatment effect (HTE) estimation and has seen popularity
across many applications [11, 10, 24, 20]. Within HTE, the conditional average treatment effect
(CATE) has proved itself to be a popular target of study in this area due to its simplicity and inter-
pretability [2, 13, 28]. A key limitation of the CATE however, is that it fails to paint a full picture of
the differences between distributions of the two responses. In addition, it can be sensitive to outliers,
with extreme values leading to a biased outcome. As such, the conditional quantile treatment effect
(CQTE), an estimand which compares the conditional quantiles of the distributions in the treated
and untreated populations, has established itself as a popular alternative [1, 5, 26].
38th Conference on Neural Information Processing Systems (NeurIPS 2024).While the CQTE offers more information than the CATE and is more robust to outliers, it lacks some
of the CATE’s desirable estimation properties. Specifically, CQTE estimation involves estimating
the quantile functions for the two marginal outcomes. This harms the estimation procedure in cases
where estimation of marginal quantile functions is more challenging than estimation of the CQTE
itself. An example of this is when the marginal quantile functions are less smooth as a function
of the covariates than the CQTE. This aligns with a recurring idea within HTE estimation that the
effect of a treatment may be simpler than the marginal outcomes. In contrast to the CQTE, there are
many CATE estimators which aim to learn the CATE directly allowing them to exploit its relative
simplicity. These include the X-learner [17], R-learner [23], and Doubly Robust (DR) learner [15].
Before estimating the CATE, these procedures require estimation of intermediary estimands (nui-
sance parameters) which condition on the covariates such as the average marginal outcomes and the
propensity score (the probability of being assigned to treatment group). These nuisance parameters
are then used to aid the estimation of the CATE. With the DR learner specifically, it has been shown
that it can still achieve optimal convergence rates even when estimation of the nuisance parameters
is worse than estimation of the CATE itself. This notion is referred to as double robustness , as our
estimation is robust to sub-optimal estimation of both of the nuisance parameters.
Some attempts have been made to improve CQTE estimation [34, 33] with a key work being that of
Kallus and Oprescu [14]. In this they provide an extension of the double robustness property to the
CQTE, creating an estimation procedure that can achieve strong convergence even when nuisance
parameters are more difficult to estimate. Unfortunately, one of the nuisance parameters which must
be estimated is the reciprocal of the conditional densities over the response. These are highly difficult
to estimate and risk the errors blowing up in low density regions which could potentially nullify the
desirable estimation rates they achieve even with the dependence on the estimation accuracy of
these nuisance parameters being less strong. Furthermore it is still unclear how one can interpret
relative smoothness in the CQTE compared to the individual quantiles with their being relatively
little discussion of this within the literature. In general there is a distinct lack of illustrative examples;
which are present for the CATE. To our knowledge no other works specifically aim to tackle this
double robustness phenomenon for the CQTE or other quantile based treatment effect estimands.
We introduce a novel estimand called the “conditional quantile comparator” (CQC). The CQC gives
the outcome for a treated individual in the same quantile as a given outcome for an untreated indi-
vidual, conditional on covariates. This relates to the conditional Quantile-Quantile (QQ) plot for the
treated and untreated outcomes as demonstrated in Figure 1. Similarly to the CQTE, our new esti-
0.0
0.2
0.4
0.6
0.8
1.0x 0.00.20.40.60.81.0
y0.00.20.40.60.81.0
g*(y|x)
0.0 0.2 0.4 0.6 0.8 1.0
Y|X=x,A=0
Untreated Response0.00.20.40.60.81.01.2Treated Response
Y|X=x,A=1g*(y|x=0)
g*(y|x=0.5)
g*(y|x=1)x=0
x=0.5
x=1
Figure 1: The left plot gives the CQC surface which takes in covariates ( x) and an untreated response
(y) and returns the treated response of the equivalent quantile ( g∗(y|x)). The right plot is a QQ-plot
of the responses ( Y) in the untreated ( A= 0) vs treated ( A= 1) population conditional on various
covariates ( X= 0,0.5,1). These conditional QQ-plots correspond to “slices” of the CQC surface,
as shown by the coloured lines in the left plot. The plot is best viewed in colour.
mand, the CQC, allows us to compare equivalent quantiles while working exclusively in the response
landscape, making it a more interpretable tool. This allows us to construct canonical examples of
the CQC being smoother than various nuisance parameters, the CQTE, and the CATE; adding to this
interpretability. In addition, using the pseudo-outcome framework presented in Kennedy [15], we
can leverage CATE estimation procedures to estimate the CQC in a doubly robust way, as mentioned
2above. Crucially, the CQC can keep the valuable quantile-level information previously offered by the
CQTE while building on much of the CATE literature to acquire its desirable robustness properties
and interpretability. Our contributions are as follows:
• Introduce a new estimand for HTE analysis: the conditional quantile comparator (CQC).
• Propose an estimation procedure which we prove to be doubly robust.
• Demonstrate better estimation accuracy especially when the CQC is smooth but individual
conditional cumulative distribution functions are not.
• Provide insights into real-world datasets on employment intervention and medical treat-
ment.
2 Set-up
We now introduce the standard HTE set-up in our notation. Let Zdenote the random triple (Y, X, A )
withYa random variable (RV) on Y ⊆R,Xa RV on X ⊆Rd, and Aa RV on {0,1}. We treat Z
as representing an individual and interpret the components as
Y:Outcome/Response X:Observed covariates A:Treatment assignment
Remark 1. We could view our setting as coming from a potential outcome framework [27]. Under
this framework we assume there exists RVs Y0, Y1onYrepresenting the outcome with and without
treatment and that Y≡YA.Y1−Awould then be unobserved/unknown for each individual.
We define the propensity score π:X → (0,1)by
π(x):=P(A= 1|X=x),
in other words, πdenotes the conditional probability of being assigned to treatment given the co-
variates. We shall assume that πis continuous and bounded away from 0and1. From a potential
outcomes perspective, this means that each individual could potentially be assigned to either treat-
ment. We also define
Fa(y|x):=P(Y≤y|X=x, A=a), (1)
F−1
a(α|x):= inf{y∈R|Fa(y|x)≥α}, (2)
and refer to them as the conditional cumulative distribution function (CCDF) and the quantile func-
tionrespectively. We also refer to F−1
ain (2) as the generalised inverse ofFa.
We can now define the CATE and CQTE to be given by τ:X →Randτq: [0,1]× X → Rwith
τ(x):=E[Y|X=x, A= 1]−E[Y|X=x, A= 0],
τq(α|x):=F−1
1(α|x)−F−1
0(α|x).
We let D:={Zi}2n
i=1≡ {(Yi, Xi, Ai)}2n
i=1forn∈Nbe IID copies of Zrepresenting our data
sample with iindexing the individual. We assume an even number of samples for notational conve-
nience. For a∈ {0,1}, we take Ia:={i|Ai= 1}, the indices of individuals on treatment a. We can
then define Da:={Zi}{i∈Ia}andna:=|Ia|as the dataset and sample size of those on treatment a.
Forn∈N, let[n]:={1, . . . , n }. For a vector w∈Rpletwjto represent the jthcomponent of w
and let ∥w∥be the Euclidean norm unless otherwise specified. We also take ∥w∥1as the 1-norm
and∥w∥∞:= max j∈[p]|wj|. We keep a summary table of all notation used in Appendix A.1.
2.1 Introducing the quantile comparator
Our aim is to find “equivalent quantiles" between the treated and non-treated distributions condi-
tional on the covariates. Specifically, for each y0∈ Y,x∈ X we aim to find y1such that
F1(y1|x) =F0(y0|x).
This now allows us to define our primary estimand of interest, the conditional quantile comparator .
Definition 1 (Conditional quantile comparator (CQC)) .For our triple (Y, X, A ), the conditional
quantile comparator is the measurable function g∗:Y × X → Y such that, for all y∈ Y,x∈ X,
F1(g∗(y|x)|x) =F0(y|x).
3We then simply define y1asg∗(y0|x). The name conditional quantile comparator derives from the
fact that it returns the value of y1in the equivalent quantile of Y|X=x, A= 1 as the quantile of
Y|X=x, A= 0thaty0is in.
Remark 2. For simplicity and to ensure such a function is well defined, we will assume that Y|X=
x, A=ais a continuous RV for any given x∈ X, a∈ {0,1}with strictly positive density on its
support. We will however allow the support of Y|X=x, A=ato vary in both xanda.
We now introduce another estimand which will serve as a useful stepping stone in our estimation.
Definition 2 (CCDF contrasting function) .TheCCDF contrasting function is defined to be
h∗:Y × Y × X → [−1,1]given by
h∗(y0, y1|x):=F1(y1|x)−F0(y0|x). (3)
This estimand allows following alternative definitions for g∗which help its interpretation and esti-
mation (detailed later). We take h∗−1representing the inverse of h∗with respect to the 2ndargument.
g∗(y0|x) =h∗−1(y0,0|x) =F−1
1(F0(y0|x)|x). (4)
The second equality still holds if we replace the inverses in the above with generalised inverses.
The equality (4) falls straight from the definition of each object and shows how we can use h∗to
estimate g∗. Moreover, the equality (4) allows us to generalise g∗to discontinuous Y(or pdfs with
non-trivial 0density regions inside the support).
2.2 Exploring the CQC
We specifically focus on the CQC, g∗, as we feel it gives insightful information allowing comparison
of the two distributions ( Y|X, A = 1) and ( Y|X, A = 0).
The CQC allows us to compare the two distributions beyond simply a single point estimate such
as that given by the CATE. This is especially valuable in cases where the two distributions differ
beyond just a shift. For example, the effect of some treatments varies greatly between individuals
with the same or similar covariates. An example of this is antidepressants, where some patients
respond positively while others may have adverse reactions leading to a worse outcome than no
treatment whatsoever. Another example is the use of opioids as painkillers where some patients
have an increased tolerance making them less effective [12, 22].
As well as being of interest on its own, the quantile comparator relates closely to other estimands of
interest. For example, if we take ∆∗(y0|x):=g∗(y0|x)−y0then∆∗tells us whether the equivalent
quantile in the treated distribution is higher or lower. This estimand then serves as a heuristic for
whether the treatment is beneficial at that untreated response value.
Furthermore, CQTE can be written as
τq(α|x) = ∆∗
F−1
Y|X,A=0(α|x)x
=g∗
F−1
Y|X,A=0(α|x)x
−F−1
Y|X,A=0(α|x), (5)
linking the quantile comparator back to the CQTE. This equivalence highlights the perspective that
the CQC can be seen as rephrasing the input of the CQTE in terms of the outcome space.
A key idea within CATE literature is the notion that the CATE itself may be a simpler estimand to
study than the marginal treatment outcomes ( E[Y|X=x, A=a]) may be individually. One can
exploit this feature to improve the CATE’s estimation. A similar concept exists with the CQC as we
will see in the example below.
Example 1 (Illustrative Example) .Suppose that
Y|X=x, A= 0∼N(sin(10 x),12), Y |X=x, A= 1∼N(2 sin(10 x),22).
Then we have g∗(y|x) = 2 ywhich does not depend on xand does not include the sine term present
in the individual CDFs. Interestingly, E[Y|X, A = 1]−E[Y|X, A = 0] = sin( x)hence the
CATE is still non-constant in this case (the same also holds for the CQTE). Additionally, we have
∆∗(y|x) =g∗(y|x)−y=ysuggesting the intervention is beneficial for positive yand detrimental
for negative y. We now show 3D plots of a CCDF , the CQC, and the CQTE in Figure 2.
4−1.0−0.50.00.51.0 x−3−2−10123
y0.20.40.60.8
FY|X=x,0(a)FY|X,0(y|x)
−1.0−0.50.00.51.0 x−3−2−10123
y−3−2−10123
g*(y∣x) (b)g∗(y|x)
−1.0−0.50.00.51.0 x0.00.20.40.60.81.0
α−3−2−10123
τq(α|x) (c)τ∗
q(α|x)
Figure 2: Surface plots for CCDF (panel (a)), CQC (panel (b)) and CQTE (panel (c)). We can see
that CCDF, and CQTE have high-frequency change in xwhile the CQC does not depend on x.
Example 2 (General Smoothness Case) .Suppose we are in the potential outcomes framework so
thatY0, Y1exist with Y≡YA. Now also suppose that Y1=ϕ(Y0, X)for some transformation
ϕincreasing in Y0for each X. Then ϕgives the CQC (i.e. ϕ=g∗) meaning that smoothness
of the CQC can be seen as smoothness of ϕ. This gives a generalisation of the CATE case where
smoothness is present when Y1=Y0+ψ(X)withψsmooth
A specific example could be a treatment which halves all individuals blood pressure. In this case
ϕ∗(y,x) =g∗(y|x) =1
2yand so the CQC is smooth but the CQTE and CATE would not be if the
individual responses are non-smooth.
3 Estimation procedure
We now describe our estimation procedure for the CQC which is motivated by equation (4). At a
high level our approach for estimating g∗(y0|x)will be the following:
• Estimate h∗(y0, .|x)using a pseudo-outcome – a specified proxy response computed from
(Y, X, A )which we will regress against.
• Find the value ˆy1which makes our estimate of h∗(y0, .|x)closest to 0.
Section 3.1 and Algorithm 1 give our h∗estimation procedure while Section 3.3 and Algorithm 2
give our g∗estimation procedure.
3.1 Estimating the CCDF contrasting function h∗
We focus on estimating h∗primarily for its two nice properties:
1. Similar to g∗,h∗can exhibit smoothness even when the individual CCDFs are not smooth.
2. The estimation of h∗can be re-framed as a CATE problem.
The first property is important as the smoothness of h∗determines the best estimation rate that can
be achieved when using non-parametric regression, setting a target for our approach. In particular,
smoother functions have better estimation rates. The second property is important as it gives us
a method for attaining this target rate. By re-framing the estimation as a CATE problem, we can
leverage existing results to build a robust estimator which achieves the target estimation accuracy
rate even when the rate of estimating nuisance parameters is sub-optimal. We demonstrate this
robustness later using finite sample bounds on the estimation accuracy (Proposition 1 & Theorem
2).
First, we show how the estimation of h∗can be solved using a CATE estimator. Note that
h∗(y0, y1|x) =E[1{Y≤y1}|X=x, A= 1]−E[1{Y≤y0}|X=x, A= 0].
Hence, for a given y0, y1, if we define the RV Wy0,y1:=1{Y≤yA}, then estimating h∗(y0, y1|.)
is equivalent to estimating the CATE with Wy0,y1replacing Yas the response. To perform this
estimation, we turn to a recent method developed by Kennedy [15]. They propose to write the
CATE as a conditional expectation of a function of Zcalled a pseudo-outcome. A robust estimator
5is then obtained by regressing this pseudo-outcome against X. In our setting, the pseudo-outcome
with the new response Wy0,y1, for a sample (y,x, a)is given by
φy0,y1(y,x, a) : =a−π(x)
π(x)(1−π(x)){1{y≤ya} −Fa(ya|x)}+F1(y1|x)−F0(y0|x) (6)
=a−π(x)
π(x)(1−π(x)){1{y≤ya} −Fa(ya|x)}+h∗(y0, y1|x).
Since h∗(y0, y1|x) =E[φy0,y1(Z)|X=x](Proposition 5, Appendix C.1), regressing φy0,y1(Z)on
Xprovides an estimate for h∗.
As we do not know the CDFs nor the propensity score, we need to replace them in (6) with estimates.
We define ˆπ,ˆF0,ˆF1to be estimates of π, F0, F1respectively. We then construct ˆφy0,y1in the same
way as φy0,y1, but using estimated quantities ˆπ,ˆFainstead. ˆφy0,y1can now serve as the pseudo-
outcome in our regression. We also use sample splitting to de-correlate the propensity score and
CDF estimates from the h∗estimate. This helps make our estimator doubly robust, as we will see in
the following theory.
We are now ready to define our Doubly Robust (DR)-learner to estimate h∗in Algorithm 1.
Algorithm 1 DR estimation procedure for the CCDF contrasting function h∗
Require: y0, y1∈ Y, Data D, a regressor (e.g. linear smoother)
1:Define I:= [n],J:={n+ 1, . . . , 2n}and split DintoDI:={Zi}i∈I, DJ:={Zj}j∈J.
2:Using DIto estimate ˆπ,ˆF0(y0|.),ˆF1(y1|.)by regressing 1{A= 1},1{Y≤y0},1{Y≤y1}
respectively against X.
3:Use these estimates to obtain ˆφ(Zj)forj∈DJusing equation (6)
4:Using DJto regress ˆφagainst Xto obtain estimate ˆh(y0, y1|.)ofh∗(y0, y1|.).
Remark 3. Cross-fitting can be implemented by repeating the procedure with the roles of IandJ
switched and then averaging the two estimates of h∗. We could also perform this procedure multiple
times with different random splits of the data to improve our estimator’s potential stability.
Note that our algorithm is not specific on which form of regression to use allowing for any parametric
or non-parametric procedure. Further to this, it can also be easily adapted to use other pseudo-
outcome procedures such as the R-learner of Nie and Wager [23] or a standard inverse propensity
weighting approach which we describe in Appendix A.3.
3.2 Finite sample bound of h∗estimator
In this section, we prove the estimation accuracy of ˆh, which will play important roles in the follow-
ing notation and assumptions we need for estimating g∗. These accuracy statements will be made
for an arbitrarily fixed x∈ X. For our theoretical and experimental results we use linear smoothers
for the final regression. This means our estimate ˆhand oracle estimate h′are of the form
ˆh(y0, y1|x) =X
j∈Jwjˆφy0,y1(Zj) h′(y0, y1|x) =X
j∈Jwjφy0,y1(Zj)
where the weights wj≡wj(x, XJ)are constructed using XJ:={Xj}j∈Jwithwj≥0and
∥w∥1. Linear smoothers encompass a broad class of estimation techniques used in both low and
high-dimensional settings. Examples include k-NN regression [8, 9], kernel ridge regression [29],
generalised forests [3], and Mondrian forests [18]. Additionally linear smoothers have been shown
to adapt to intrinsic low dimensionality in regression problems in higher dimensions [16], making
them an apt estimator for our purposes.
Forϕ:Z →Rtreated as deterministic and p >1, we also define the norms
∥ϕ∥ws:=s
1
∥ws∥1X
i∈Jws
jE[ϕ(Z)2|X=Xi]
and take ∥ϕ∥w:=∥ϕ∥w1. Note that this norm is random as the weights depend upon XJ.
6We now aim to show that we are able to exploit smoothness in h∗even when the CCDFs and
propensity score are less smooth. We introduce the notion of smoothness through Hölder functions.
Definition 3 (Hölder functions) .We say that a function f:X → Ris(γ, C)-Hölder for γ∈
(0,1], C≥1if for any x′,x′′∈ X,
|f(x′)−f(x′′)| ≤C∥x′−x′′∥γ.
Here, larger γrepresents a smoother function which can be estimated at faster rates.
Assumption 1. For any y0, y1∈ Y, δ > 0, a∈ {0,1}:
(a) There exists ξ∈(0,1/2]such that π(x),ˆπ(x′)∈[ξ,1−ξ]for all x′∈ X.
(b) With probability at least 1−δ,∥ˆφy0,y1−φy0,y1∥w2≤εˆφ(n, δ).
(c) With probability at least 1−δ,∥ˆπ−π∥w≤εα(n, δ)and∥Fa(ya|.)−ˆFa(ya|.)∥w≤εβ(n, δ).
(d) For γ∈(0,1], C≥1,h∗(y0, y1|.)is(γ, C)-Hölder.
Assumption 1(a) exists to ensure for any covariate value, neither treatment assignment has too low
a probability. Assumption 1(b) controls the convergence of the estimated pseudo-outcome to the
true pseudo-outcome. Assumption 1(c) sets up the smoothness of the propensity score and CCDFs
alongside the convergence rates of their estimators as εα, εβrespectively. Assumption 1(d) sets up
the smoothness of h∗which will control the convergence rate of the oracle estimation procedure.
Assumptions 1 (c) and (d) control the accuracy of our estimator in the following result.
Proposition 1. Suppose that Assumption 1 holds and let ˆhbe a linear smoother estimated as in
Algorithm 1. Then for any y1, y0∈ Y, δ∈(0,2/e]and our x∈ X, with probability at least 1−δ,
ˆh(y0, y1|x)−h∗(y0, y1|x)≤εh(n, δ).
Here, for each δ∈(0,2/e]we have
εh(n, δ):=p
2 log(8 /δ)/n ε ˆφ(n, δ/4) +εα(n, δ/4)εβ(n, δ/4) +εγ(n, δ/4),
εγ(n, δ):=|E[h′(y0, y1|x)−h∗(y0, y1|x)|XJ, DI]|
+p
2 log(2 /δ)/n∥w∥∥φ−h(y0, y1|.)∥w2+2∥w∥∞log(2/δ)/(3ξ).
We have that εγgives an upper bound on the accuracy of the oracle estimation and so acts as a target
for our estimation procedure. If εˆφ(n, δ)→0then the first term in εhis guaranteed to be o(εγ(n, δ))
for fixed δ. Hence the first and last terms converge at oracle rates with respect to n. As the εαandεβ
terms are multiplied together we can obtain better rates than either of them individually have. This
is because both εαandεβcan converge to 0slower than εγwhile their product εα·εβconverges
quicker. This provides the desired double robustness as our estimation can converge at oracle rates
even when convergence for the nuisance parameters is slower. Now that we have this we can convert
our estimate of h∗, into an estimate of g∗.
3.3 Estimating the conditional quantile comparator g∗
In order to obtain an estimate of g∗(y0|x)at a fixed y0,x, we need to obtain estimates of
h∗(y0, y1|x)at various values of y1. As h∗is monotonic, we would then like to search for a
monotonic function which aligns with these estimates. This monotonicity is especially important
because it allows us to bound the estimation accuracy of ˆhuniformly over all y1at a similar rate
to our pointwise accuracy. With this, we can easily translate the estimation accuracy in ˆh(ob-
tained in proposition 1) into the accuracy in ˆg. Additionally, it simplifies the process of inverting
ˆh. In general our method in Algorithm 1 will not produce monotonic ˆh(this is in contrast to some
other approaches such as an IPW pseudo-outcome, see Appendix A.3), or separately estimating the
CCDFs). We can however obtain a monotonic estimate of h∗using isotonic projection.
Definition 4 (Isotonic Projection) .We define the isotonic projection ofα′∈Rpas follows:
P(α′):= argmin
α∈Iso(p)∥α−α′∥
where Iso(p):={α∈Rp|αj≤αl+1∀l∈[p−1]}, the set of all isotonic vectors in Rp.
7Remark 4. We can use the Pool Adjacent Violators Algorithm (PAVA) [6] which performs isotonic
projection and is implemented in the IsotonicRegression class of sci-kit learn in Python [25].
Hence, for a fixed value of (y0,x)and a set of predictions ˆαl=ˆh(y0, y(l)
1|x)withy(l)≤y(l+1),
we can take ˜α:=PIso(p)(ˆα)and use these to obtain a new monotonic estimate of h∗. Furthermore,
by a result in Yang and Barber [32], ˜αwill be at least as accurate as ˆαin the worst case. We now
describe our approach for estimating the CQC using this projection approach in Algorithm 2.
Algorithm 2 DR estimation procedure for the CQC g∗
Require: DataD; test point (y0,x); sorted evaluation points {y(l)}p
l=1
1:Apply Algorithm 1 to obtain estimate of h∗given by ˆh.
2:Define ˆαl:=ˆh(y0, y(l)|x)forl∈[p].
3:Isotonically project ˆαusing PA V A to obtain ˜αwith˜αi≤˜αi+1.
4:Take ˆg(y0|x):=y(l∗)withl∗:= argminl∈[p]|˜αl|.
Remark 5. For the case where ˆhis a step function, these steps can serve as our evaluation points
while for continuous ˆhone could take these candidate y1points at small evenly spaced intervals.
Empirically we also find that ˆhis already close to isotonic and so step 3. of the algorithm is mostly
for the theoretical justification of our approach.
While it may seem inefficient to be estimating the CQC via the CCDF contrasting function and then
inverting, due to the monotonicity of h∗, we actually pay a very small cost in estimation accuracy
for having to estimate the CCDF contrasting function over all y1. We make this notion more explicit
in the following section.
3.4 Finite sample bound of the CQC estimator
We now provide the accuracy of our estimate ˆgobtained by Algorithm 2 when used in conjunction
with linear smoothers. We assume that ˆFaare also fit using linear smoothers of the form
ˆFa(y|x′, a) =X
i∈IwFa;i(x′;XI, AI)1{Yi≤y}
withwj(x′)≡wj(x′, XI, AI)>0,∥w(x′)∥1= 1. We will also require the following assumptions.
Assumption 2. For our RV X, any y∈ Y,x′∈ X,δ < e−1:
(a) There exists some s, η > 0such that F1(y′|x)≥ηfor all y′∈Bs(g∗(y|x)).
(b) W.p. at least 1−δ,max j∈Jwj≤εw(n, δ)andmax i∈IwFa;i(X)≤εw(n, δ).
Assumption 2(a) is a mild assumption which allows us to convert the ˆhaccuracy into ˆgaccuracy
while Assumption 2(b) bounds the rates of decay of the weights in our linear smoothers.
Theorem 2. Letˆgbe estimated as using Algorithm 2 with {Yi}i∈I1, sorted and then used as our
evaluation points and linear smoothers used for regressions in Algorithm 1. Then provided Assump-
tions 1 & 2 hold we have that for δ∈(0, e−1)and sufficiently large n, w.p. at least 1−δ,
|ˆg(y|x)−g∗(y|x)|≤2 
η−1εh(n, δ/(2n)) +ξ−1εw(n, δ/(2n))
.
From this result we see that if our weights decay at rate faster than εh(n, δ)then this error will be
dominated by the εhterm. We believe this to hold in most cases and show that it does comfortably
when using Nadaraya-Watson (NW) estimation [21, 31] with a box kernel in Appendix C.4. Further-
more, if the dependence on δin both terms is of the form logc(1/δ)for some c >0then we obtain the
same rate of estimation as for h∗up to polylog factors. This means we translate our desirable double
robustness ˆhover to ˆg. We also obtain finite sample bounds on E[|ˆg(Y|x)−g∗(Y|x)| |A= 0,ˆg]
with high probability and present this in Appendix C.3.
4 Numerical experiments
We now apply our approach to a series of simulated and real data scenarios in order to demonstrate
the utility of our estimand and the effectiveness of our estimation procedure. For these, we use NW
estimation as our regression procedure throughout. See appendix A.2 for details on NW estimation.
84.1 Simulated experiment
In this section, we test our method’s performance in terms of our estimator’s mean absolute error
under simulated scenarios.1In each scenario we test against a separate estimator which estimates
the two CCDFs separately and simply takes their difference, an IPW pseudo-outcome estimator
detailed in Appendix A.3, the CQTE estimator of Kallus and Oprescu [14], and the oracle DR
estimator where ˆφis replaced with φ(i.e. exact π, Faare used). In this experiment, we return back
to the set-up of example 1. We now change the frequency of the sine term by taking
Y|X=x, A= 0∼N(sin( γπx),12), Y |X=x, A= 1∼N(2 sin( γπx),22),
π(x) = 0 .4 sin( γπx) + 0.5.
forγ∈[0,10]so that increasing γimitates decreasing smoothness of our nuisance parameters.
In our experiments half the samples are used to estimate the propensity score and CCDFs and the
other half are used to regress against the pseudo-outcome. Our estimate ˆgis then compared against
g∗using a hold-out testing set. This process is repeated 500 times with new training data on each
run. From this, a Monte-Carlo estimate of Eˆg[EX[EY|X,A=0[|ˆg(Y|X)−g∗(Y|X)|]]]is produced
alongside 95% confidence intervals (CIs). In our first experiment, we let 2n= 1000 and vary γin
[0,10]. In our second experiment, we let γ= 6 and2nvary in [200,5000] . The results of this are
shown in Figure 3.
0 2 4 6 8 10
γ0.20.40.60.81.01.21.4̂g error
Separate Estimator
IPW Estimator
CQTE Estimator (Kallus et al.)
DR Estimator (Ours)
Exact DR Estimator (Oracle)
0 2000 4000 6000 8000 10000
2n0.00.51.01.52.02.53.0̂g error
Separate Estimator
IPW Estimator
CQTE Estimator (Kallus et al.)
DR Estimator (Ours)
Exact DR Estimator (Oracle)
Figure 3: Mean absolute error with 95% CIs for various estimators. The left plot has fixed sample
size (2n= 1000 ) and increasing γ. The right plot has γ= 6and increasing sample size.
We can see that the average error decreases as the sample size 2ngrows and mildly increases as γ
grows. This is expected as increasing γmakes estimating the nuisance parameters more challenging.
The result shows that the proposed DR method achieves the best performance compared with the
Separate and IPW estimators and is only marginally outperformed by the oracle estimator. Addi-
tionally we see that the method of Kallus and Oprescu [14] is much more affected by the increase
inγ. This is because unlike the CQC, the CQTE has a complexity that depends on the frequency
term ( γ). We also observe much better performance as the sample size increases. We hypothe-
sise that the plateau in the CQTE approach is due to the difficulty of estimating the reciprocal of
the PDF, which causes the estimation to be unstable irrespective of sample size. Further simulated
experiments including 10-dimensional xand linear CQC are given in Appendix B.1.
4.2 Real world employment example
To show the performance in the real-world scenarios, we use a dataset on an employment programme
which has been studied in various prior works [4, 5, 26]. Within the programme, some participants
were given job placements or temporary help jobs while others received no intervention. Partici-
pants’ earnings were then monitored over the next 8 quarters following their enrolments. We take
their net earnings as our response ( Y) and the employment intervention as the treatment ( A= 1).
We use each participant’s age at their entry to the study as our covariate ( X). We fit our quantile
comparator function on 2,000 participants. Figure 4 shows our estimate of ∆∗(y|x) =g∗(y|x)−y
for various values of (y,x).
We see that participants around age 23 and between ages 32-37 benefit most from this scheme, as
indicated by the darker colour on the heat map. Additionally, the lower quantile of the income distri-
bution (wage ≤$7500) shows the least change, indicating that wage improvements primarily occur
1Code implementation can be found at: github.com/joshgivens/ConditionalOutcomeEquivalence
925303540Age (x)05000100001500020000
Earnings (y)250050007500100001250015000
Δ*(y|x)
22.5 25.0 27.5 30.0 32.5 35.0 37.5 40.0
Age (x)02500500075001000012500150001750020000Earnings (y)
−15000−10000−5000050001000015000
Δ*(y|x)Figure 4: Surface and heat plot of ∆∗(y|x)for our employment data with X=Age,Y=Income.
for the higher income group. For participants aged 40, there appears to be little change in outcomes
overall. To demonstrate our approach in a medical setting, we apply our method to evaluate the
effectiveness of a colon cancer treatment, as detailed in Appendix B.3. For comparative purposes,
we also provide an estimate of the CQTE for this data in Appendix B.2.
5 Limitations
The theory provided here gives a strong foundation and motivation for framing our problem in this
particular manner. There is however a great deal more to be explored in this area from a theoretical
perspective. For example, one immediate improvement would be to give a more general case where
our weight decay (in Assumption 2 (b) for Theorem 2) is sufficiently fast. In addition more work
needs to be done exploring the relationship between the smoothness of g∗and the smoothness h∗.
Smoothness in h∗appears to be a stronger condition so ideally we would like to make theoretical
statements directly on the smoothness of g∗. Interestingly our experiments on synthetic data do seem
to suggest that it is the complexity of g∗which drives the estimation rate as in these experiments
h∗increases in complexity while g∗remains constant. Additionally, changing our experiment in
Section 4.1 to have uniform response so that both h∗, g∗are constant rather than just g∗, seems to
give no material improvement to the performance of our estimator (see Appendix B.1.3).
Another limitation of our current estimation procedure is that it requires learning an estimand and
then inverting it to obtain our final estimator. While this process is relatively simple, it could be
streamlined and made more computationally efficient if we could produce a more direct estimator
similar to the DR-Learner for CATE [15] or the CQTE estimator in Kallus and Oprescu [14].
Finally, while we have been able to provide more concrete examples of smoothness for the CQC
these are still limited to the case of a deterministic treatment effect which we would like to ex-
pand upon. This is closely related to a more general limitation with quantile-based estimands, the
CQC included, in that they lack meaningful interpretability for the individual. While the CATE
can be viewed as the expected difference in an individual’s outcome on and off the treatment, no
such individual-level interpretation exists for the CQC or indeed any other estimand trying to learn
higher level distributional information than the mean. As a result the CATE is still a more naturally
interpretable estimand. To facilitate this interpretation however, one still needs to make assumptions
about a lack of confounding between the treatment assignment and the potential outcomes, which
are only verifiable in certain restrictive scenarios [30].
6 Conclusion
In this paper we have introduced a new treatment effect estimand, the conditional quantile com-
parator and demonstrated its efficacy both in terms of its doubly robust estimation, and its ability to
provide valuable data insights. This is a promising direction as it allows quantile-based treatment
effect exploration to “keep up" with the CATE in terms of estimation quality offering more flexibil-
ity as to which estimand can be used to best describe the data. For these reasons, we see the CQC
as an exciting and worthwhile new direction within the HTE framework.
10Acknowledgments and Disclosure of Funding
Josh Givens was supported by a PhD studentship from the EPSRC Centre for Doctoral Training in
Computational Statistics and Data Science (COMPASS).
References
[1] Abadie, A., Angrist, J., and Imbens, G. (2002). Instrumental variables estimates of the effect of
subsidized training on the quantiles of trainee earnings. Econometrica , 70(1):91–117.
[2] Abadie, A. and Imbens, G. W. (2002). Simple and bias-corrected matching estimators for av-
erage treatment effects. Working Paper 283, National Bureau of Economic Research. Series:
Technical working paper series.
[3] Athey, S., Tibshirani, J., and Wager, S. (2019). Generalized random forests. The Annals of
Statistics , 47(2):1148 – 1178. Publisher: Institute of Mathematical Statistics.
[4] Autor, D. H. and Houseman, S. N. (2010). Do temporary-help jobs improve labor market out-
comes for low-skilled workers? Evidence from "Work First". American Economic Journal:
Applied Economics , 2(3):96–128.
[5] Autor, D. H., Houseman, S. N., and Kerr, S. P. (2017). The effect of work first job placements
on the distribution of earnings: An instrumental variable quantile regression approach. Journal
of Labor Economics , 35(1):149–190.
[6] Barlow, R. E., Bartholomew, D. J., Bremner, J. M., and Brunk, H. D. (1972). Statistical Infer-
ence under Order Restrictions (The Theory and Application of Isotonic Regression) . Wiley Series
in Probability and Statistics. John Wiley & Sons Ltd.
[7] Boucheron, S., Lugosi, G., and Massart, P. (2013). Concentration inequalities: a nonasymptotic
theory of independence . Oxford University Press.
[8] Chen, G. (2019). Nearest neighbor and kernel survival analysis: Nonasymptotic error bounds
and strong consistency rates. In Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings of
the 36th international conference on machine learning , volume 97 of Proceedings of machine
learning research , pages 1001–1010. PMLR.
[9] Chen, P., Dong, W., Lu, X., Kaymak, U., He, K., and Huang, Z. (2019). Deep representation
learning for individualized treatment effect estimation using electronic health records. Journal of
Biomedical Informatics , 100:103303.
[10] Collins, F. S. and Varmus, H. (2015). A new initiative on precision medicine. The New England
journal of medicine , 372(9):793–795. Place: United States.
[11] Hirano, K. and Porter, J. R. (2009). Asymptotics for statistical treatment rules. Econometrica ,
77(5):1683–1701. Publisher: [Wiley, The Econometric Society].
[12] Huynh, P., Villaluz, J., Bhandal, H., Alem, N., and Dayal, R. (2021). Long-Term Opioid
Therapy: The Burden of Adverse Effects. Pain Medicine , 22(9):2128–2130.
[13] Imbens, G. W. (2004). Nonparametric estimation of average treatment effects under exogene-
ity: a review. The Review of Economics and Statistics , 86(1):4–29.
[14] Kallus, N. and Oprescu, M. (2023). Robust and agnostic learning of conditional distributional
treatment effects. In Ruiz, F., Dy, J., and van de Meent, J.-W., editors, Proceedings of the 26th
international conference on artificial intelligence and statistics , volume 206 of Proceedings of
machine learning research , pages 6037–6060. PMLR.
[15] Kennedy, E. H. (2023). Towards optimal doubly robust estimation of heterogeneous causal
effects. Electronic Journal of Statistics , 17(2):3008 – 3049. Institute of Mathematical Statistics
and Bernoulli Society.
[16] Kpotufe, S. (2011). k-NN regression adapts to local intrinsic dimension. In Shawe-Taylor, J.,
Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K., editors, Advances in neural information
processing systems , volume 24. Curran Associates, Inc.
11[17] Künzel, S. R., Sekhon, J. S., Bickel, P. J., and Bin Yu (2019). Metalearners for estimating
heterogeneous treatment effects using machine learning. Proceedings of the National Academy
of Sciences , 116(10):4156–4165.
[18] Lakshminarayanan, B., Roy, D. M., and Teh, Y . W. (2014). Mondrian forests: Efficient online
random forests. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and Weinberger, K.,
editors, Advances in neural information processing systems , volume 27. Curran Associates, Inc.
[19] Laurie, J. A., Moertel, C. G., Fleming, T. R., Wieand, H. S., Leigh, J. E., Rubin, J., McCor-
mack, G. W., Gerstner, J. B., Krook, J. E., and Malliard, J. (1989). Surgical adjuvant therapy of
large-bowel carcinoma: an evaluation of levamisole and the combination of levamisole and flu-
orouracil. The North Central Cancer Treatment Group and the Mayo Clinic. Journal of clinical
oncology , 7(10):1447–1456. Place: United States.
[20] Lei, L. and Candès, E. J. (2021). Conformal inference of counterfactuals and individual treat-
ment effects. Journal of the Royal Statistical Society, Series B , 83(5):911–938.
[21] Nadaraya, E. (1965). On non-parametric estimates of density functions and regression curves.
Theory of Probability & Its Applications , 10(1):186–190.
[22] Nadeau, S. E., Wu, J. K., and Lawhern, R. A. (2021). Opioids and chronic pain: An analytic
review of the clinical evidence. Frontiers in Pain Research , 2.
[23] Nie, X. and Wager, S. (2020). Quasi-oracle estimation of heterogeneous treatment effects.
Biometrika , 108(2):299–319.
[24] Obermeyer, Z. and Emanuel, E. J. (2016). Predicting the Future - Big Data, Machine Learning,
and Clinical Medicine. The New England journal of medicine , 375(13):1216–1219. Place: United
States.
[25] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel, O., Blondel, M.,
Prettenhofer, P., Weiss, R., Dubourg, V ., Vanderplas, J., Passos, A., Cournapeau, D., Brucher,
M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research , 12:2825–2830.
[26] Powell, D. (2020). Quantile Treatment Effects in the Presence of Covariates. The Review of
Economics and Statistics , 102(5):994–1005.
[27] Rubin, D. B. (2005). Causal inference using potential outcomes. Journal of the American
Statistical Association , 100(469):322–331.
[28] Semenova, V . and Chernozhukov, V . (2021). Debiased machine learning of conditional average
treatment effects and other causal functions. The Econometrics Journal , 24(2):264–289.
[29] Singh, R., Xu, L., and Gretton, A. (2023). Kernel methods for causal functions: dose,
heterogeneous and incremental response curves. Biometrika , 111(2):497–516. tex.eprint:
https://academic.oup.com/biomet/article-pdf/111/2/497/57467664/asad042.pdf.
[30] VanderWeele, T. J. (2008). The sign of the bias of unmeasured confounding. Biometrics.
Journal of the International Biometric Society , 64(3):702–706. Publisher: [Wiley, International
Biometric Society].
[31] Watson, G. S. (1964). Smooth regression analysis. Sankhy ¯a: The Indian Journal of Statistics,
Series A , pages 359–372.
[32] Yang, F. and Barber, R. F. (2019). Contraction and uniform convergence of isotonic regression.
Electronic Journal of Statistics , 13(1):646 – 677. Publisher: Institute of Mathematical Statistics
and Bernoulli Society.
[33] Ying Zhang, Lei Wang, M. Y . and Shao, J. (2020). Quantile treatment effect estimation with
dimension reduction. Statistical Theory and Related Fields , 4(2):202–213. Publisher: Taylor &
Francis tex.eprint: https://doi.org/10.1080/24754269.2019.1696645.
[34] Zhou, T., Carson, W. E. t., and Carlson, D. (2022). Estimating Potential Outcome Distributions
with Collaborating Causal Networks. Transactions on machine learning research , 2022. Place:
United States.
12A Additional details
A.1 Notation table
Table 1: Table of notation.
Notation Definition Description
Y RV on Y ⊆R Response
X RV on X ⊆RdCovariates
A RV on {0,1} Treatment
Z (Y, X, A )
π(x) P(A= 1|X=x) Propensity Score
Fa(y|x) P(Y≤y|X=x, A=a) Conditional CDF (CCDF)
F−1
a(α|x) inf {y∈ Y|Fa(y|x)≥α} Conditional quantile func-
tion
τ(x) E[Y|X=x, A= 1]−E[Y|X=x, A= 0] Conditional average treat-
ment effect (CATE)
τq(α|x) F−1
1(α|x)−F−1
0(α|x) Conditional quantile treat-
ment effect (CQTE)
D {Zi}2n
i=1 IID data sample
Ia {i∈[2n]|Ai= 1} Treatment aindex
Da {Zi}i∈Ia
na |Ia|
[n] {1, . . . , n }
∥x∥q
1
dPd
i=1x2
i Euclidean Norm
∥x∥11
dPd
j=1|xj| 1-norm
∥x∥∞ max j∈[d]|wj| ∞ -norm
g∗(y|x) F−1
1(F0(y|x)|x) Conditional quantile com-
parator CQC
h∗(y0, y1|x) F1(y1|x)−FY|X,0(y0|x) CCDF contrasting function
∆∗(y|x) g∗(y|x)−y Quantile difference function
ˆf Estimate of f/f∗
φy0,y1(y,x, a)a−π(x)
π(x)(1−π(x)){1{y≤ya} −Fa(ya|x)}
+F1(y1|x)−F0(y0|x)The Pseudo-Outcome
ˆφy0,y1(y,x, a)φwithπ, Fareplaced by ˆπ,ˆFa
I,J { 1, . . . , n },{n+ 1, . . . , 2n} Sample splitting indices
DI, DJ {Zi}i∈I,{Zj}j∈J Data split
w∈Rnwj≡wj(x) ˆhestimation weights
ˆh(y0, y1|x)P
j∈Jwj(x) ˆφy0,y1(Zj) h∗estimate
h′(y0, y1|x)P
j∈Jwj(x)φy0,y1(Zj) Oracle h∗estimate
∥ϕ∥wsq
∥ws∥−1
1P
i∈Jws
jE[ϕ(Z)2|X=Xi]
ξ π (x′)∈[ξ,1−ξ]for all x′∈ X
εˆφ(n, δ) ∥ˆφ−φ∥w2≤εˆφ(n, δ)w.p.1−δ ˆφerror bound w.h.p.
εα(n, δ) ∥ˆπ−π∥w2≤εα(n, δ)w.p.1−δ ˆπerror bound w.h.p.
εβ(n, δ) ∥ˆFa(y|.)−Fa(y|.)∥w2≤εβ(n, δ)w.p.1−δ ˆFaerror bound w.h.p.
γ Smoothness of h∗(y0, y1|.)
εγ(n, δ)E[h′(y0, y1|x)−h∗(y0, y1|x)|XJ, DI]|
+p
2 log(1 /δ)/n∥w∥∥φ−h(y0, y1|.)∥w2
+ 2ξ−1∥w∥∞εδ(n)/3h′error bound w.h.p.
εTn(n, δ)p
2 log(1 /δ)/nεw(n, δ)εˆφ(n, δ) Tn(x)error bound w.h.p.
εh(n, δ)εTn(n, δ/4) + ( εα·εβ)(n, δ/4)+
εγ(n, δ/4)ˆherror bound w.h.p.
Iso(p) {α∈Rp|αl≤αl+1∀l∈[p−1]} Set of isotonic vectors
PIso(p))(α′) argminα∈Iso(p)∥α−α′∥ Isotonic projection
wFa∈RnwFa;i(x)≡wFa;i(x′, XI) ˆFaestimation weights
13Notation Definition Description
ˆFa(y|x′)P
i∈JwFa(x)1{Yj≤y} CCDF estimate
η F 1(y′|x)> η∀, y′∈Bs(g∗(y|x)) Density lower bound
εw(n, δ) max {∥w(x)∥∞,∥wFa(X)∥∞}w.p.1−δ Variance lower bound w.h.p.
Appendix Notation
k(x,x′) NW estimation kernel
mf(x) E[f(Z)|X=x]
ˆmf(x)P
j∈Jwj(x)f(Zj)
ˆb(x) mˆφ−φ(x)
εδ(n) max {log(1/δ)/n,1/n}
[n]0 {0,1, . . . , n }
f(∆y) lim ε↓0f(y)−f(y−ε) Step size of faty
A.2 Nadaraya-Watson estimation
Throughout, we use NW estimation as our standard non-parametric regression technique. For a
kernel k:X × X → [0,∞), IID data sample D:={(Yi, Xi)}n
i=1, andx∈ X the NW estimate of
E[Y|X=x]is given by
nX
i=1k(x, Xi)Pn
j=1k(x, Xj)Yi.
Applying this to our pseudo-outcome regression in Algorithm 1, we get that
ˆh(y0, y1|x):=X
i∈Jk(x, Xi)P
j∈Jk(x, Xj)ˆφy0,y1(Yi, Xi, Ai) (7)
=X
i∈Jk(x, Xi)P
j∈Jk(x, Xj)Ai−ˆπ(Xi)
ˆπ(Xi)(1−ˆπ(Xi))(8)
·n
1{Yi≤yAi} −ˆFY|X,Ai(yAi|Xi)o
+ˆFa(y1|Xi)−ˆF0(y0|X=Xi)
where for any x′∈ X,a∈ {0,1}
ˆπ(x′):=X
i∈Ik(x′, Xi)P
j∈Ik(x′, Xj)1{Ai= 1} (9)
ˆFa(ya|x′):=X
i∈Ik(x′, Xi)P
j∈Ik(x′, Xj)1{Aj=a}1{Yi≤ya}1{Ai=a}. (10)
Note that for our theoretical results we do not specify how our nuisance parameters are estimated
and the results only depend on the accuracy of our estimation of the nuisance parameters.
Remark 6. We can re-write (8)as:
ˆh(y0, y1|x):=X
i∈I1k(x, Xi)Pn
i=1k(x, Xi)1
ˆπ(Xi)n
1{Yi≤y1} −ˆF1(y1|Xi)o
+ˆF1(y1|Xi)−ˆF0(y0|Xi)
−X
i∈I0k(x, Xi)Pn
i=1k(x, Xi)1
1−ˆπ(Xi)n
1{Yi≤y0} −ˆF0(y0|Xi)o
+ˆF0(y0|Xi)−ˆF1(y1|Xi)
Which can be helpful in terms of the practical implementation.
14A.3 IPW pseudo-outcome estimator
Another pseudo-outcome one can use for estimating the treatment effect is the based on inverse
propensity weighting. Specifically we can take our pseudo-outcome to be
ψy0,y1(Y′, X′, A′):=A′−π(X′)
π(X′)(1−π(X′))1{Y′≤yA′}. (11)
If we regress against it using NW estimation, and also use NW estimation for our nuisance parameter
estimation as in (9) & (10), our estimate ˆhwill be increasing in y1and decreasing in y0meaning we
do not need to perform any isotonic projection.
A.4 Additional experimental details
As the method of Kallus and Oprescu [14] is one for estimating the CQTE, we transform it into
an estimator of the CQC (which we denote by ˆg) using the following formula where ˆτq(.|.)is our
CQTE estimator
ˆg(y|x) = ˆτq 
FY|X,0(y|x)|x
+y.
using the true CCDF, FY|X,0.
This is the inverse of equation (5) which defines the CQTE in terms of the CQC. Conversely we also
tested transforming all our CQC estimators into CQTE estimators (using exactly equation (5) with
ˆgreplacing g∗) and testing the accuracy on this space and found similar results.
Each experiment took no longer than 1 hour to run on a single 4 core CPU with 8GB of RAM.
The bandwidths of the kernels for the NW estimation of the nuisance parameters were chosen by a
limited grid search on additional simulated data by validating against the true value of the nuisance
parameters. While this is unrealistic in practice it was done to make estimation of the nuisance
parameters as strong as possible. This was to err on the side of caution as strong nuisance parameter
estimation would naturally favour the baseline approaches.
Code to implement our approach alongside Jupyter notebooks running our numerical experiments
can be found in the supplementary materials. The code to implement the kernels is adapted from
https://github.com/wittawatj/kernel-gof/ which is free to use under the MIT Licence.
B Additional results
B.1 Simulation results
We run additional simulated experiments in order to further test and explore our approach. Through-
out, our overall experimental set-up is the same as in Section 4.1
B.1.1 10-dimensional example
To test our problem in higher dimensions we ran experiments where Xwas 10-dimensional with X
uniform on [−1,1]10. That is each component Xjwas independent with Xj∼U(−1,1). We then
took
Y|X=x, A=a∼N(sin(γπ(β⊤x),1)
π(x) = 0 .4 sin( γπ(β⊤x)) + 0 .5
where βwas randomly sampled from N(0,(0.2)2)andγ∈[0,3]. This gave the maximum gradient
multiplied by 0.5 diam( X) =√
dclose to 1making the rate of change of the CDFs and propensities
similar to our 1-dimensional examples.
In our first experiment we take 2n= 1000 andγ∈ {0,0.5,1,1.5,2,2.5,3}. The results of this are
shown in Figure 5a In our second experiment we take γ= 1and2n∈ {200,500,1000,2000,5000}.
The results of this are shown in Figure 5b.
As we can see the DR approach performs the best with it performing close to oracle for low sample
size and low frequency. As the frequency increases the DR estimator deviates from the oracle.
150.0 0.5 1.0 1.5 2.0 2.5 3.0
γ0.20.40.60.81.01.2̂g error
Separate Estimator
IPW Estimator
CQTE Estimator (Kallus et al.)
DR Estimator (Ours)
Exact DR Estimator (Oracle)(a) Average error as γincreases with 95% CIs for
various approaches.
0 2000 4000 6000 8000 10000
2n0.00.20.40.60.81.0̂g error
Separate Estimator
IPW Estimator
CQTE Estimator (Kallus et al.)
DR Estimator (Ours)
Exact DR Estimator (Oracle)(b) Average error as sample size nincreases with
95% CIs for various approaches.
Figure 5: Estimation accuracy of 10-dimensional CQC estimation procedures for highly varying
CCDFs and constant CQC with respect to x.
B.1.2 Varying CQC
In this experiment our set-up is
Y|X, A = 0∼Nsin(γπx)
0.5x+ 1.5,1
Y|X, A = 1∼N 
sin(γπx) + 0.25x+ 0.75,(0.5x+ 1.5)2
π(x) = 0 .4 sin( γπx) + 0.5
forγ∈[0,10].This gives g∗= (y+ 0.5)(0.5x+ 1.5)which is still simpler than the individual
CCDFs but now does depend upon x.
In our first experiment we take 2n= 1000 andγ∈ {0,2,4,6,8,10}. The results of this are shown
in figure 5a In our second experiment we take γ= 6 and2n∈ {200,500,1000,2000,5000}. The
results of this are shown in Figure 5b.
0 2 4 6 8 10
γ0.250.500.751.001.251.501.752.00̂g error
Separate Estimator
IPW Estimator
CQTE Estimator (Kallus et al.)
DR Estimator (Ours)
Exact DR Estimator (Oracle)
(a) Average error as nincreases with 95% CIs for
various approaches.
0 2000 4000 6000 8000 10000
2n0.51.01.52.0̂g error
Separate Estimator
IPW Estimator
CQTE Estimator (Kallus et al.)
DR Estimator (Ours)
Exact DR Estimator (Oracle)(b) Average error as nincreases with 95% CIs for
various approaches.
Figure 6: Estimation accuracy of CQC estimation procedures for highly varying CCDFs and linear
CQC with respect to x.
B.1.3 Constant h∗
In our previous examples, while g∗has been simple and or constant, h∗has actually included the
high frequency sine term. In this experiment we adjust our original illustrative example to give a
constant h∗as well. Specifically we take
Y|X=x, A= 0∼Unif(sin(γπx),sin(γπx) + 1) ,
Y|X=x, A= 1∼Unif(2 sin( γπx),2 sin( γπx) + 2) .
forγ∈[0,10]. We also take the propensity to be π(x) = 0 .4 sin( γπx) + 0 .5. In this case
h∗(y0, y1|x) =1
2y1−y0which does not depend on xfor any y0, y1∈ Y.
16In our first experiment we take 2n= 1000 andγ∈ {0,2,4,6,8,10}. The results of this are shown
in figure 5a In our second experiment we take γ= 6 and2n∈ {200,500,1000,2000,5000}. The
results of this are shown in Figure 5b. As we can see we obtain very similar results to original
0 2 4 6 8 10
γ0.20.40.60.81.01.2̂g error
Separate Estimator
IPW Estimator
CQTE Estimator (Kallus et al.)
DR Estimator (Ours)
Exact DR Estimator (Oracle)
(a) Average error as nincreases with 95% CIs for
various approaches.
0 2000 4000 6000 8000 10000
2n0.00.20.40.60.81.01.21.4̂g error
Separate Estimator
IPW Estimator
DR Estimator (Ours)
Exact DR Estimator
CQTE Estimator (Kallus et al.)(b) Average error as nincreases with 95% CIs for
various approaches.
Figure 7: Constant h∗
Figure 8: Estimation accuracy of CQC estimation procedures for highly varying CCDFs and con-
stanth∗and CQC.
illustrative example suggesting that our method is effectively exploiting simplicity in g∗rather than
inh∗.
B.2 Employment example: Comparing to the CQTE
To further explore the interpretability of our estimator we provide an estimate of the CQTE for
comparison. This can be seen in Figure 9.
25303540Age (x)0.20.40.60.8
Earnings Quantile (α)0250050007500100001250015000
τ*q(α|x)
22.5 25.0 27.5 30.0 32.5 35.0 37.5 40.0
Age (x)0.20.30.40.50.60.70.8Earnings Quantile (α)
−15000−10000−5000050001000015000
τ*
q(α|x)
Figure 9: Surface and heat plot of the CQTE, τ∗
q(α|x), for our employment data with X=Age,
α=Income Quantile.
In these plots we can see that the interpretation of the CQTE is less immediate than for the case
of the CQC. This is because, for each covariate value, the quantile value corresponds to a different
untreated response making comparisons between various values of the covariates less direct. From
this plot we do still see that higher income quantiles are associated with a greater increase in wages.
Interestingly the value of the CQTE plummets 90% at the highest quantile for individuals of age 40.
This is likely an estimation error due to the limited data at higher quantiles as and higher ages.
B.3 Colon cancer treatment
To demonstrate the effectiveness of our approach in medical settings, we apply it to a trial on the
effect of colon cancer treatment on survival time/time to remission. This dataset was originally
introduced in Laurie et al. [19] and can be found in the “survival" package in R and loaded with the
linedata(colon, package="survival") . In this dataset 929 are randomised to either receive
Placebo or Levamisole. They are then followed up for a period of up to 3329 days and the time till
17either death or recurrence of their cancer. For our analysis we take our response ( Y) as the time to
first of death or recurrence. For simplicity, when an individual makes it to the end of a trial without
an event and is censored we take that to be the time of their event. Looking at the data censoring
times are mostly at around 2,000-3,000 days while over 90% of death or recurrence events that do
occur, occur within 1,500 days. Therefore censored individuals will still have better responses than
those who had events, as we would want. As our covariate, we took the patients’ ages when joining
the trial. We show a 3D plot and heat plot of the estimate of ∆(y|x) =g∗(y|x)−yin Figure 10.
45505560657075 Age (x)02004006008001000
Time to event (y)025050075010001250
Δ*(y|x)
45 50 55 60 65 70
Age (x)02004006008001000Time to event (y)
−1500−1000−500050010001500
Δ*(y|x)
Figure 10: Surface plot and heat plot of ∆(y|x)overy,xfor colon cancer trial data with X=Age,
Y=Time to Event.
Interestingly from Figure 10 we see that the treatment appears to have little effect on patients aged
53-57. For the reaming patients, we can see that there is relatively little effect on the time to event
for events of 400 days or less while at around 500 days the difference in time to event jumps to
1000 days. This jump takes the time to event (TTE) to the time when censoring begins. This gives
evidence that rather than the treatment delaying the time to event, it increases the proportion of the
patients who have no event during the trial essentially fully treating those patients for the duration
of the trial. This shows the value of the CQC it was able to reveal meaningful information about
the treatment beyond simply its positive effect. On top of this, we are able to identify the censoring
within our results without explicitly controlling for it in any way.
C Additional theory
C.1 Proof of ˆhaccuracy
We first define some additional notation. For an output fwhich depends upon z= (y, x, a )define
mf(x) =E[f(Z)|X=x]and
ˆmf(x) =X
j∈Jwjf(Zj),
withwjdefined as before so that ˆmˆφ(x)is our estimator and ˆmφ(x)is the oracle estimator.
Additionally define
ˆb(x):=mˆφ−φ(x),
in other words the bias in our estimate of the pseudo-outcome for a given x. We will also work with
ˆmb(x)where we view bas being a function of z.
Finally for n∈N,δ∈(0,1), define
εδ(n):=log(2/δ)/n ifδ∈(0,2/e]
1/n otherwise..
Theorem 3 (Stability Result) .Forδ∈(0,1)we have that with probability at least 1−δ
|ˆmφ(x)−mφ(x)|≤E[ ˆmφ(x)−mφ(x)|XJ, DI]| (12)
+p
2εδ(n)∥w∥∥φ−h(y0, y1|.)∥w2+2∥w∥∞εδ(n)
3ξ=:B(φ).
18Moreover, with probability at least 1−δwe have
|ˆmˆφ(x)−mφ(x)| ≤B(φ) +|ˆmˆb(x)|+p
2εδ(n)∥w∥2∥ˆφ−φ∥w2=:B+( ˆφ). (13)
Proof. To prove (12) we first observe that since wisDI, XJ-measurable we have
E[ ˆmφ(x)2|DI, XJ]−E[ ˆmφ(x)|DI, XJ]2=X
j∈Jw2
jE[(φ(Zj)−E[φ(Z)|X=Xj])2|DI, XJ]
Note also that we have max j∈J|wj{φ(Zj)−E[φ(Zj)|DI, XJ]}|≤ ∥ w∥∞2ξ−1almost surely.
Note also that {φ(Zj)}j∈ J are conditionally independent given DI, XJ. Hence, the bound (12)
follows from two applications Bernstein’s inequality Boucheron et al. [7, Theorem 2.10], applied
conditionally on DI, XJ.
Next we note that by the triangle inequality we have
|E{ˆmˆφ(x)−mφ(x),|DI, XJ}|
≤ |E{ˆmˆφ(x)−ˆmφ(x)|DI, XJ}|+|E{ˆmφ(x)−mφ(x)|DI, XJ}|
≤X
j∈JwjE[ ˆφ(Zj)−φ(Zj)|DI, XJ]+|E{ˆmφ(x)−mφ(x)|DI, XJ}|
=∥ˆmˆb∥w,1+|E{ˆmφ(x)−mφ(x)|DI, XJ}|
Moreover, since E[ ˆφ(Zj)|DI, XJ]is the projection of ˆφ(Zj)onto the subspace of DI, XJ-
measureable functions we have
∥w∥2
2∥ˆmˆφ−mˆφ(x)∥2
w2
=X
j∈Jw2
jE{( ˆφ(Z)−E{ˆφ(Z)|X=Xj})2|DI, XJ}
≤X
i∈[n]w2
jE{( ˆφ(Zj)−E{φ(Zj)|DI, XJ})2|DI, XJ}
=X
i∈[n]w2
jE({( ˆφ(Zj)−φ(Zj)) + ( φ(Zj)−E{φ(Zj)|DI, XJ})}2|DI, XJ)
=X
i∈[n]w2
j(E{( ˆφ(Zj)−φ(Zj))2|DI, XJ}+E{(φ(Zj)−E{φ(Zj)|DI, XJ})2|DI, XJ})
=∥w∥2
2{(∥ˆmˆφ−ˆmφ∥2
w2+∥ˆmφ−mφ∥2
w2}
≤ ∥w∥2
2{(∥ˆmˆφ−ˆmφ∥w2+∥ˆmφ−mφ∥w2}2.
Hence, to deduce (13) we apply the first bound with ˆφin place of φto obtain the following bound
with probability at least 1−δ,
|ˆmφ(x)−mφ(x)| ≤|E{ˆmˆφ(x)−mφ(x)|DI, XJ}|+p
2εδ(n)∥w∥2∥ˆmˆφ−mˆφ∥w2
+ 2ξ−1∥w∥∞εδ(n)/3
≤∥ˆmˆb∥w,1+|E{ˆmφ(x)−mφ(x)|DI, XJ}|
+p
2εn(δ)∥w∥2{(∥ˆmˆφ−ˆmφ∥w2+∥ˆmφ−mφ∥w2}
+ 2ξ−1∥w∥∞εδ(n)/3
=B(φ) +∥ˆmˆb∥w,1+p
2εδ(n)∥w∥2∥ˆmˆφ−ˆmφ∥w2
=B+( ˆφ),
as required.
We apply the following result from Kennedy [15].
19Proposition 4 (Proposition 2 from Kennedy [15]) .Suppose that ˆb(x) =ˆb1(x)ˆb2(x)then
|ˆmˆb(x)|=∥w∥1∥ˆb1∥w∥ˆb1∥w.
Proof. Follows from the Cauchy-Schwartz inequality.
Proposition 5. Our pseudo-outcome is conditionally unbiased,
E[φy0,y1(Z)|X=x] =h∗(y0, y1|x).
Proof. We have
E[φy0,y1(Z)|X=x, A= 1] =1
π(x)
E[1{Y≤y1}|X=x, A= 1]−F1(y1|x)
+h∗(y0, y1|x)
=h∗(y0, y1|x),
and similarly E[φy0,y1(Z)|X=x, A= 1] = h∗(y0, y1|x). The result now follows by the law of
total expectation.
C.1.1 Proof of Proposition 1
Proof of Proposition 1. Fixy0, y1,xLetˆmˆφ(x)be the regression of Xagainst the estimated
pseudo-outcome ˆφy0,y1(Z)evaluated at x(i.e.ˆh(y0, y1|x)), let ˆmφbe the same but with the esti-
mated pseudo-outcome replaced with the exact pseudo-outcome φy0,y1. Finally take
mφ(x):=E[φy0,y1(Z)|X=x] =P(Y≤y1|X=x, A= 1)−P(Y≤y1|X=x, A= 0).
Theorem 3 gives us that with probability. at least 1−δ
|ˆmˆφ(x)−mφ(x)| ≤εγ(n, δ) +|ˆmˆb(x)|+p
2εδ(n)∥w∥2∥ˆφ−φ∥w2
≤εγ(n, δ) +|ˆmˆb(x)|+p
2εδ(n)∥ˆφ−φ∥w2
withˆb((y,x, a)):=E[ ˆφ(Z)−φ(Z)|X=x].
To bound ˆbwe have that from Proposition 5,
E[φy0,y1(Z)|X=x] =P(Y≤y1|X=x, A= 1)−P(Y≤y0|X=x, A= 0).
Additionally, by splitting over the events {A= 1},{A= 0}and noting that P(A= 1|X=x) =
π(x)we have
E[ ˆφy0,y1(Z)|X=x] =π(x)
ˆπ(x)
P(Y≤y1|X=x, A= 1)−ˆP(Y≤y1|X=x, A= 1)
−1−π(x)
1−ˆπ(x)
P(Y≤y0|X=x, A= 0)−ˆP(Y≤y0|X=x, A= 0)
+ˆP(Y≤y1|X=x, A= 1)−ˆP(Y≤y0|X=x, A= 0).
Hence
ˆb(x) =π(x)
ˆπ(x)−1
P(Y≤y1|X=x, A= 1)−ˆP(Y≤y1|X=x, A= 1)
−1−π(x)
1−ˆπ(x)−1
P(Y≤y0|X=x, A= 0)−ˆP(Y≤y0|X=x, A= 0)
.
20We have that using Proposition 4
|ˆmˆb(x)| ≤ ∥w∥1π
ˆπ−1
wP(Y≤y1|X, A = 1)−ˆP(Y≤y1|X, A = 1)
w
+1−π
1−ˆπ−1
wP(Y≤y0|X, A = 0)−ˆP(Y≤y0|X, A = 0)
w
≤1
ξ1X
a=0∥π−ˆπ∥wP(Y≤ya|X, A =a)−ˆP(Y≤ya|X, A =a)
w.
Now it is simply a matter of bounding all the relevant terms which we do through the following
events
Eoracle :=n
|ˆmˆφ−mφ|≤ |ˆmφ(x)−mφ(x)|+|ˆmˆb(x)|+p
2εδ(n)∥w∥2∥ˆφ−φ∥w2o
Eˆφ:={∥ˆφ−φ∥w2≤εˆφ(n, δ/4)}
Eγ:={ˆmφ−mφ≤εγ(n, δ/4)}
Eα:=n
∥π−ˆπ∥w,2≤εα(n, δ/4)o
Eβ,0:=n
∥ˆP(Y≤y0|X, A = 0)−P(Y≤y0|X, A = 0)∥w,2≤gβ(n, δ/4)o
Eβ,1:=n
∥ˆP(Y≤y1|X, A = 1)−P(Y≤y1|X, A = 1)∥w,2≤εβ(n, δ/4)o
According to our assumptions and previous results, Eoracle, Eˆφ, Eγ, Eα∩Eβ,0∩Eβ,1separately
occur w.p. at least 1−δ/4. Then by the union bound, the intersection of all these events holds w.p.
at least 1−δand under these events
|ˆmˆφ−ˆmφ+ ˆmφ−mφ| ≤εTn(n, δ/4) +2
ξεα(n, δ/4)εβ(n, δ/4) +εγ(n, δ/4)εw(n, δ/4)
Where the first term comes from Proposition 3, the second from Proposition 4 and the final term
from smoothness of g∗.
C.2 Proof of ˆgaccuracy
Proposition 6 (Maximum step-size bound) .Suppose that both the outer regression and estimation
of CCDFs is fit using linear smoothers so that
ˆh(y0, y1|x) =X
j∈Jwj(x;XJ) ˆφ(Zj) ˆFa(y|x′) =X
i∈IwFa;i(x′;XJ)1{Yi≤y}
withwj(x), wi(x′)Additionally suppose with probability at least 1−δ
max
max
j∈Jwj(x),max
i∈IwFa;i(X)
≤εw(n, δ).
Then with probability at least 1−δ
max
j∈[n1+1] 0|ˆh(y0, Y(j)
1|x)−ˆh(y0, Y(j+1)
1|x)|≥ξ−1εw(n, δ/n )
where [n1+ 1] 0:= [n1+ 1]∪ {0},ˆh(y0, Y(0)
1|x) =−1,ˆh(y0, Y(n1+1)
1 |x), and{Y(i)}i∈I1is the
sorted version of {Yi}i∈I1.
Proof. We immediately have via union bounds that with probability at least 1−δ
max
max
j∈Jwj(x),max
j∈JwFa;i(Xj)
≤εw(n, δ/n ).
Now we can try to bound the jump size of our function. To do so for a step function f:Y →Rand
a step points y∈ Y we define
f(∆y):= lim
ε↓0f(y)−f(y−ε),
21i.e. the size of the step at y. From the definition of our pseudo-outcome, the jumps occur in different
forms at {Yj}{j∈J,Aj=1},{Y(i)}{i∈I|A(i)=1}. Note that we are assuming Ycontinuous so these
points are all distinct a.s. .
ForYjwithj∈ J andAj= 1. We have
ˆh(y0,∆Yj|x) =wφ,j1
ˆπ(Xj)1{Yj≤∆Yj}
≤wφ,j
ξ.
Fori∈ IwithA(i)= 1, we have that
|ˆh(y0,∆Yi|x)|=X
j∈Jwj
−1{Aj= 1}1
ˆπ(Xj)ˆF1(∆Yi|Xj) +ˆF1(∆Yi|Xj)
≤1
ξmax
j∈JˆF1(∆Yi|Xj)
≤1
ξmax
j∈JwFa;i(Xj)1{Yi≤∆Yi}
=1
ξmax
j∈JwFa;i(Xj).
Hence the maximum jump size is less than
1
ξmax
max
j∈Jwφ,j(x),max
i∈I,j∈JwFa;i(Xj)
.
Therefore combining this with our previous bounds gives that with probability at least 1−δ
max
i∈[n1+1] 0|ˆh(y0, Y(i)|x)−ˆh(y0, Y(i+1)|x)|≤ξ−1εw(n, δ/n )
F1We also have a result ensuring the step size of the projected function is even smaller.
Proposition 7. Letα∈Rpand˜α:=PIso(p)(α). Then |αl−αl+1|≥ |˜αl−˜αl+1|for all l∈[p].
Proof. We will prove this by first giving an algorithm to compute the projection. Define
ψl(α):=(
α ifαl≤αl+1 
α1, . . . ,αl+αl+1
2,αl+αl+1
2, αm
ifαl> αl+ 1.
Now define α(0,0)=α,α(n,i)=ψi(α(n,i−1))fori∈[p−1]andα(n,0)=α(n−1,p−1)forn∈N.
Finally define α(t)=α(⌊t/p⌋,mod(t,p). Then by Yang and Barber [32] we know that limt→∞α(t)=
PIso(p)(α).
Now if |α(n,l+1)
l+1−α(n,l+1)
l|>|α(n,l)
l+1−α(n,l)
l|. Then α(n,l+1)
l+1has be moved by ψl+1. Hence
α(n,l+1)
l+1< α(n,l)
l+1. For this to increase the distance then we must have
α(n,l+1)
l+1< α(n,l+1)
l.
Therefore as α(n+1,l−1)
l≥αn,l+1
l, we have that
α(n+1,l)
l+1=α(n+1,l)
l.
By a similar (simpler) argument |α(n,l−1)
l−α(n,l−1)
l+1|>|α(n,l−2)
l−α(n,l−2)
l+1|thenα(n,l)
l=α(n,l)
l+1.
Hence if any iteration moves adjacent points further apart, a later iteration will make them equal
meaning that on convergence they will be equal. Hence we have proved our claim.
22We now have the result which justifies our choice to take the isotonic projection of the data which is
take from Yang and Barber [32].
Proposition 8 (Theorem 1 from Yang and Barber [32]) .Letz∗,ˆz∈Rpwithz∗∈Iso(p). Then for
˜z:=PIso(p)(ˆz),
max
l|z∗
l−˜zl|≤max
l|z∗
l−ˆzl|.
We now use this isotonic projection to obtain supremum bounds on the accuracy of our estimator.
Proposition 9 (Supremum bound on ˆhaccuracy) .Fixx∈ X, y∈y0and let ˆhbe our original
estimate of h∗. Form∈N, take{Y(l)}m
l=1to be a potentially random set of points in Yin increasing
order.
Now define α∈Rmbyαl:=ˆh(y0, Y(l)|x)and˜α:=PIso(m)(α). Finally, define ˜hto be the
piecewise constant right continuous function with ˜h(y0, Y(l)|x):= ˜αl.
Suppose for any y1∈ Y,n∈N, and δ, δ′>0that
P˜h(y0, y1|x)−h∗(y0, y1|x)≥εh(n, δ)
≤δ
P
max
l∈[m]0ˆh(y0, Y(l)|x)−ˆh(y0, Y(l+1)|x)≥εstep(n, m, δ )
≤δ
where we take [m]0:= [m]∪ {0},ˆh(y0, Y(0)|x):=−1, and ˆh(y0, Y(m+1)|x):= 1.
Then for any δ, δ′>0, n∈N,
P
sup
y1∈Yˆh(y0, y1|x)−h∗(y0, y1|x)≤εh(n, δ/m ) +εstep(n, m, δ′)
≥1−δ−δ′.
Proof. For each l∈[m]define the event
Eh,l:=nˆh(y0, Y(l)
1|x)−h∗(y0, Y(l)
1|x)≤εh(n, δ/m )o
and take Eh:=Tn
l=1Eh,l. Additionally define the event
Estep:=
max
l∈[m]0ˆh(y0, Y(l)|x)−ˆh(y0, Y(l+1)|x)< εstep(n, m, δ )
Then EhandEstephold w.p.s at least 1−δand1−δ′respectively. Also under Eh, Estep, by
Propositions 7 & 8, we have that
max
l∈[m]˜h(y0, Y(l)
1|x)−h∗(y0, Y(l)
1|x)≤εh(n, δ/m )
max
l∈[m]0˜h(y0, Y(l)|x)−˜h(y0, Y(l+1)|x)< εstep(n, m, δ )
We then have that for any y1∈ Y there exists i∈[m]such that y(l−1)
1≤y1≤y(l)
1.
Hence by monotonicity, we have the following 2 inequalities
ˆh(y0, y1|x)−h∗(y0, y1|x)≥˜h(y0, Y(l−1)
1|x)−h∗(y0, Y(l)
1|x)| {z }
Λ1
ˆh(y0, y1|x)−h∗(y0, y1|x)≤˜h(y0, Y(l)
1|x)−h∗(y0, Y(l−1)
1|x)| {z }
Λ2.
Now for the first inequality we have
Λ1=˜h(y0, Y(l−1)
1|x)−˜h∗(y0, Y(l)
1|x) +˜h(y0, Y(l)
1|x)−h∗(y0, Y(l)
1|x)
≥ −εh(n, δ)−εstep(n, m, δ′).
23With the final inequality coming from Eh,l−1and our definition of h. By a similar argument we get
from Eh,lthatΛ2≤εh(n, δ) +εstep(n, m, δ′). Again we can use the same approach to get that
under Eh,l−1, Eh,l, Estep
−εh(n, δ)−εstep(n, m, δ′)≤h∗(y0, y1|x)−˜h(y0, y1|x)≤εh(n, δ) +εstep(n, m, δ′).
Hence for our specific y1under Eh,l, Eh,l−1, Estep
h∗(y0, y1|x)−˜h(y0, y1|x)≤εh(n, δ) +εstep(n, m, δ′)
Now as this inequality holds for arbitrary y1under Eh∩Estep(an event which not depend on our
choice of y1) and the intersection of these two events holds w.p. at least 1−δ−δ′by the union
bound we have our result.
Our final key result before we can piece them all together will be to obtain accuracy in gfrom our
accuracy in h∗
Theorem 10 (Single point accuracy bound) .Fixx, y0, assume that h,˜hare strictly monotonic in
y1and suppose that
sup
y1|h∗(y0, y1|x)−˜h(y0, y1|x)|< ε.
Additionally let α:=F0(y0|x)and assume that fY|X,A=1(y1|x)> η for all y1∈F−1
1(Bε(α)|x)
where here we are taking F−1
1(A|x)to be the pre-image in Yof the set Afor fixed x.
Then
|g∗(y0|x)−ˆg(y0|x)|≤2ε
η,
where g∗(y0|x)is the unique y1such that h∗(y0, y1|x) = 0 .
Proof. Lety∗
1:=g∗(y0|x)andˆy1:= ˆg(y0|x)so that F1(y∗
1|x). From our accuracy assumption on
ˆhin the set-up of the Theorem we have
|F1(y∗
1|x)−F1(ˆy1|x)|=|h∗(y0, y∗
1|x)−h∗(y0,ˆy1|x)|
=|h∗(y0,ˆy1|x)|
≤ |h∗(y0,ˆy1|x)−ˆh(y0,ˆy1|x)|+|ˆh(y0,ˆy1|x)|
≤ |h∗(y0,ˆy1|x)−ˆh(y0,ˆy1|x)|+|ˆh(y0, y∗
1|x)|
≤ |h∗(y0,ˆy1|x)−ˆh(y0,ˆy1|x)|+|ˆh(y0, y∗
1|x)−h∗(y0, y∗
1|x)|
≤2ε,
where the second and fifth line come from the definition of y∗
1and the fourth from the definition of
ˆy1.
If we define ∂kfto be the derivative with respect to the kthargument of fthen we have
|ˆg(y0|x)−g∗(y0|x)|=|F−1
1(F1(y∗
1|x)|x)−F−1
1(F1(ˆy1|x)|x)|
=ZF1(ˆy1|x)
F1(y∗
1|x)∂1F−1
1(β|x)dβ
=ZF1(ˆy1|x)
F1(y∗
1|x)1
f((F−1
1(β|x)|x)dβ
≤2εmax
y1∈(y∗
1,ˆy1)1
f(y1|x)
≤2ε
η.
24C.2.1 Proof of Theorem 2
We can now finally combine all these results to prove Theorem 2.
Proof of Theorem 2. From proposition 1 we have that
Pˆh(y0, y1|x)−h∗(y0, y1|x)≤εh(n, δ)
≥1−δ
with
εh(n, δ):=εTn(n, δ/4) +εα(n, δ/4)εβ(n, δ/4) +εγ(n, δ/4).
Now define {Y(i)}n
i=1to be the sorted version of {Yi}n
i=1. Then by Proposition 6 we have
P
max
j∈[n]0|ˆh(y0, Y(i)
1|x)−h∗(y0, Y(i)
1|x)|≥εh−step(n, δ)
≤δ.
Plugging this into Proposition 9 with εstep(n, m, δ )replaced by ξ−1εw(n, δ/n ))andεh(n, δ)re-
placed with εh(n, δ/2)gives
P
sup
y1∈Y˜h(y0, y1|x)−h∗(y0, y1|x)≤εh(n, δ/(2n)) +ξ−1εw(n, δ/n ))
≥1−δ.
Now let y∗
1:=g∗(y0|x). Then for any y1∈ Y if|y′
1−y∗
1|> s this implies that |F1(y′
1|x)−
F1(y∗
1|x)|> sη by our lower bound on the density in Bs(y∗
1). Hence by the contrapositive, if
|F1(y′
1|x)−F1(y∗
1|x)|≤sηthen|y′
1−y∗
1|< s.
Now as
|F1(y′
1|x)−F1(y∗
1|x)|≤sη⇔y′
1∈F−1
1(Bsη(F1(y∗
1)))
⇔y′
1∈BsηF−1
1(Bsη(F0(y0)))
Hence if nis sufficiently large so that ε:=εh(n, δ/(2n)) +ξ−1εw(n, δ/n ))≤ηsthen we satisfy
the bounded density condition of Theorem 10. Therefore we can plug our bound into theorem 10
gives
P 
|ˆg(y0|x)−g∗(y0|x)| ≤2 
η−1εh(n, δ/(2n)) +ξ−1εw(n, δ/n ))
≥1−δ.
C.3 Extension to expectation
Proposition 11. LetY0, Dbe RVs on Y,Znrespectively (with Drepresenting data used to fit a
model and Y0representing the point where the model is fit). Now take l(Y0, D)to be a non-negative
bounded loss so that l(Y0, D)< lmaxa.s. . Suppose that for any δ >0, for all y∈ Y
P(l(y, D)> ε(n, δ))< δ
Then for any t, δ0∈[0,1]andp, q∈[1,∞]such that 1/p+ 1/q= 1
P
E[l(Y0, D)|D]≤t1/qE[l(Y0, D)p|D]1/p+ε(n, δ0)
≥1−δ0
t.
In particular if l(y, D)< lmaxa.s. then taking q= 1, p=∞yields
P(E[l(Y0, D)|D]≤tlmax+ε(n, δ0))≥1−δ0
t.
Proof. First fix t, δ0∈[0,1]We will first bound the probability that the number of ywhich don’t
satisfy our bound isn’t too large. We do this by defining the event
A:={l(Y0, D)> ε(n, δ0)},
and now aim to bound the probability that P(A|D)> t(this probability is just w.r.t Y0treating Das
fixed.)
25By Markov’s inequality,
P(P(A|D)> t)≤1
tE[P(A|D)]
=1
tE[P(A|Y0)]by Fubini’s Theorem
<1
tE[δ0] =δ0
t.
Now define B:={P(A|D)≤t}so that P(B) = 1−δ0
t. Then under B
E[l(Y0, D)|D] =E[1Al(Y0, D)|D] +E[l(Y0, D)|Ac, D]P(Ac|D)
≤E[1Al(Y0, D)|D] +ε(n, δ0)by definitions of A
≤E[1A|D]1/qE[l(Y0, D)p|D]1/p+ε(n, δ0)by Holder’s inequality
≤t1/qE[l(Y0, D)p|D]1/pt+ε(n, δ0)As we are assuming B.
Corollary 12. Assume that F1(y|x)> ηfor all y1∈ Y. Then, for our fixed x∈ X andδ∈(0, e−1),
w.p. at least,
1−δdiam(Y)
2 (η−1εh(n, δ/2n) +ξ−1εw(n, δ/n ))),
Eh
|ˆg(Y|x)−g∗(Y|x)|A= 0,ˆg
≤4 
η−1εh(n, δ/n ) + 2ξ−1εw(n, δ/n ))
where εh(δ, n):=εTn(n, δ/4) +εα(n, δ/4)εβ(n, δ/4) +εγ(n, δ/4).
In particular for 2 
η−1εh(n, δ/n ) + 2ξ−1εw(n, δ/n ))
≤log(e1(n)/δ)a/e2(n). For δ <1/ewe
have that w.p. at least 1−δ
Eh
|ˆg(Y|x)−g∗(Y|x)|A= 0,ˆg
≤log(2 diam( Y)e2(n)e1(n)/δ)a/e2(n)
Proof. Plugging the result of theorem 2 into proposition 11, noting that |ˆg(y0|x)−g∗(y|x)|≤
diam(Y)and taking
t=2
diam(Y) 
η−1εh(n, δ/(2n)) +ξ−1εw(n, δ/n ))
yields that w.p. at least
1−δdiam(Y)
2 (η−1εh(n, δ/n ) +ξ−1εw(n, δ/n ))),
Eh
|ˆg(Y|x)−g∗(Y|x)|A= 0,ˆg
≤4 
η−1εh(n, δ/n ) + 2ξ−1εw(n, δ/n ))
.
Following from this if 2 
η−1εh(n, δ/n ) + 2ξ−1εw(n, δ/n ))
≤log(e1(n)/δ)α/e2(n). Then we
have
P
Eh
|ˆg(Y|x)−g∗(Y|x)|A= 0,ˆg
≥2 log( e1(n)/δ)a/e2(n)
≤δdiam(Y)e2(n)
log(e1(n)/δ)a
forδ <1/ewe have
δ <1/e⇒δ <exp
−(1
2)1/a
⇔log(1/δ)a>1
2
⇒log(e1(n)/δ)a>1
2
⇒2δlog(e1(n)/δ)a> δ
⇔2δ >δ
log(e1(n)/δ)a.
26Hence
P
Eh
|ˆg(Y|x)−g∗(Y|x)|A= 0,ˆg
≥2 log( e1(n)/δ)a/e2(n)
≤2δdiam(Y)e2(n).
Finally δ′= 2δdiam(Y)e2(n)gives
P
Eh
|ˆg(Y|x)−g∗(Y|x)|A= 0,ˆg
≥2 log(2 diam( Y)e1(n)e2(n)/δ′)a/e2(n)
≤δ′.
C.4 Application and justification of NW estimation with box kernel
We aim to show that the box kernel satisfies some of our conditions. Specifically conditions 2 & 3
from Proposition 1. We first start by bounding the step size.
Proposition 13 (Effective sample size) .Suppose that for our x∈ X there exists C0, r0>0such
that for any r∈(0, r0)
P(X∈Br(x))≥C0rd.
Now for r∈(0, r0)take our kernel to be kr(x,x′):=1{∥x−x′∥≤r}. Then w.p. at least 1−δ
X
j∈J1{Xj∈Br(x)} ≥
nC0rd−q
2nC0rdlog(1/δ)−log(1/δ)/3)−1
.
Proof. We have 1{Xj∈Br(x)} ≤1andE[1{Xj∈Br(x)}2] =E[1{Xj∈Br(x)}2] =P(Xj∈
Br(x)) =C0rd. Therefore by one sided Bernstein’s inequality with ε= log(1 /δ)we get
P
X
j∈J1{Xj∈Br(x)} ≤nC0rd−q
2nC0rdlog(1/δ)−1
3log(1/δ)
≤δ.
This gives our desired result.
Note thatP
j∈J1{Xj∈Br(x)}is also the effective sample size of our estimation as it is the
number of samples used in the average.
Now that we have the effective sample size result in terms of our kernel radius r, we need to obtain
the optimal rate of decay of rfor our estimation.
Proposition 14 (NW estimation with box kernel) .letˆmf(x)be the NW estimation of mf(x):=
E[f(Z)|X=x]using IID copies (Zi)n
i=1ofZ. and assume |f(Z)| ≤M. For a fixed r∈(0, r),
use kernel kras defined above and suppose the same assumptions hold. Suppose that mf(x)isα
smooth for α≤1(i.e.α-Holder continuous.) Then for sufficiently large ndepending on C0, αand
δ≤2e−1w.p. at least 1−δ,
|ˆmf(x)−mf(x)|≤2M+ 1
C0log(2/δ)n−1
2+d/α.
Proof. With our kernel define Ir:={i∈[n]|kr(x, Xi) = 1}andnr=|Ir|. Now define the event
En:={nr≥C0nrd−q
2C0rdnlog(2/δ)−log(2/δ)/3}.
27Then by Proposition 13 this event occur w.p. at least 1−δ/2. Now if we define εi:=f(Zi)−mf(Xi)
then we have E[εi|Xi] = 0 . Also, we have
|ˆmf(x)−mf(x)|=1
nrX
i∈Irf(Zi)−mf(x)
=1
nrX
i∈Irmf(Xi) +εi−mf(x)
≤1
nrX
i∈Ir|mf(Xi)−mf(x)|+1
nrX
i∈Irεi
≤rα+1
nrX
i∈Irεi.
With the final equality coming by our smoothness condition and definition of Ir. Now by Hoeffding
bounds we have that
P
X
i∈Irεi≥s
2 log(4 /δ)M2
nr
≤δ
2.
Hence by combining this event and Enthen we have that with probability at least 1−δ
|ˆmf(x)−mf(x)| ≤rα+s
2 log(2 /δ)M2
nr
≤rα+s
2 log(2 /δ)M2
C0nrd−p
2C0nrdlog(2/δ)−log(2/δ)/3.
Now for sufficiently large n
C0nrd−q
2C0nrdlog(2/δ)−log(2/δ)/3≥C0nrd
2 log(2 /δ).
This in turn gives
|ˆmf(x)−mf(x)| ≤rα+2 log(2 /δ)Mp
C0nrd.
Then the optimal choice of ris such that
rα=1√
nrd
⇔rα+d
2=n−1
2
⇔r=n−1
2α+d
⇔rα=n−1
2+d/α=1√
nrd.
Hence plugging this in we get that for sufficiently large ndepending on C0, αwe have that for
δ≤2e−1w.p. at least 1−δ
|ˆmf(x)−mf(x)|≤2M+ 1
C0log(2/δ)n−1
2+d/α.
Proposition 15 (Final weight decay) .Suppose that for our x∈ X there exists C0, r0>0such that
for any r∈(0, r)
P(X∈Br(x))≥C0rd.
28Suppose that we are performing NW estimation of an αsmooth function at points x∈ X using
kernel krn(x,x′):=1{∥x−x′∥≤r}withrndecaying optimally. Now define
wj:=krn(x, Xj)P
j′∈Jkrn(x, Xj′)
the weight of the jthcomponent. Then with probability at least 1−δ
max
j∈Jwj≤=log(2/δ)
C0n−2
2+d/α.
Proof. We have
max
j∈Jwj≤1P
j∈Jk(x, Xj)
=1P
j∈J1{Xj∈Br(x)}.
Hence by proposition 13 we have w.p. at least 1−δ
max
j∈Jwj≤
C0nrd−q
2C0nrdlog(1/δ)−log(1/δ)/3)−1
.
We know from Proposition 14 that the optimal radius decay gives1
nrd=n−2
2+d/γ. Plugging this
into our result gives that
max
j∈Jwj≤
C0n2
2+d/γ−p
2C0log(1/δ)n1
2+d/γ−log(1/δ)/3)−1
≤log(1/δ)
C0n2
2+d/γFor sufficiently large ndepending on γ, C 0.
Note that for a γsmooth function the MSE is C−1n−1
2+d/γ. Hence the box kernel comfortably
satisfies our weight decay condition 2 in Assumptions 2.
Additionally now if we have an αsmooth function and assume that for any x′∈ X there exists
C0(x′)such that for all r∈(0, r0)P(X∈Br(x′)≥C0rd. Then we have that w.p. at least 1−δ,
wFa;i(X, XI)≤log(2/δ)
C0(X)n−2
2+d/α.
Thus if we assume that there exists C′′>0such that w.p. at least 1−δ,1
C0(X)≤C′′p
log(1/δ)
then we get that w.p. at least 1−δ
wFa;i(X, XI)≤C′′log3/2(3/δ)n−2
2+d/α.
This then gives our condition 2 in Assumptions 2.
We now try to boundP
j∈Jw2
jσ(Xj)
E[P
j∈Jw2
jσ(Xj)].
Corollary 16 (Accuracy under NW estimation with box kernel) .Suppose that our linear smoother
is NW estimation with the box kernel and optimally decaying radius additionally assume that:
•εˆφ(n, δ) =eˆφ(n)p
log(1/δ)withe1=o(1).
•εα(n, δ) =Cαp
log(1/δ)n−1
2+d/α.
•εβ(n, δ) =Cβp
log(1/δ)n−1
2+d/β.
29•β >d
2(1+d/γ). Then for any δ, for sufficiently large nthe following events each separately
hold w.p. at least 1−δ,
ˆh(y0, y1|x)−h∗(y0, y1|x)≤Chlog3/2(1/δ)
eh(n)
|ˆg(y|x)−g∗(y|x)| ≤Cg
ηξlog3/2(n/δ)
eh(n)
Eh
|ˆg(Y|x)−g∗(Y|x)|A= 0,ˆg
≤Cg,2diam(Y)
ξηlog3/2(e2(n)n/δ)
e2(n)
withCh, Cg, Cg,2depending on Cα, Cβ, C′, C′′(where C′, C′′are the constants define in
Proposition 15.
Proof. For any NW estimator we have that ∥w∥1,∥w∥2≤1a.s. meaning we can take εw(n, δ)≡1.
We also have that εγ(n, δ) =Cγlog(1/δ)n−1
1+d/γ. Hence εTn=p
2εδ(n)eˆφ(n)p
log(1/δ). This
then gives
ˆh(y0, y1|x)−h∗(y0, y1|x)≤Cγlog1/2(7/δ)n−1
2+d/γ+CαCβlog(7/δ)n−(1
2+d/α+1
2+d/β)
+q
2εδ/4(n)eˆφ(n)p
log(7/δ)
≤Chlog(1/δ)n−1
2+d/γ+ log(1 /δ)n−(1
2+d/α+1
2+d/β)
≤Chlog(1/δ)
eh(n)
forδ < e−1andCh= 7(Cγ+CαCβ+√
2)giving our first result.
Using our weight decay results for NW estimation and plugging into Proposition 6 we get that
εh−step=C′′ξ−1log3/2
eh(n)(n/δ).
Hence plugging into Theorem 2 gives
|ˆg(y|x)−g∗(y|x)| ≤Ch√
2η−1log(n/δ)
eh(n)+ 4C′′ξ−1log3/2(n/δ)n−1
2+d/γ
≤Cg
ξηlog3/2(n/δ)
eh(n)
withCg=Ch√
2 + 4C′′giving our second result. Finally by Corollary 12 for δ < e−1w.p. at least
1−δ,
Eh
|ˆg(Y|x)−g∗(Y|x)|A= 0,ˆgi
≤Cgdiam(Y)
ξηlog3/2(neh(n)/δ)
eh(n).
Note that both εTnandεh−step are actually both o(εγ(n, c)), thus if we were allowed to take n
sufficiently large depending upon δthen we would have all log3/2terms replaced with logterms and
the dependence on C′, C′′removed.
30NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We introduce a a new estimand, the CQC. We claim this to maintain the
distributional information of the CQTE which we show through the definition and also
on real world scenarios in Section 4. We claim that our method is able to obtain double
robustness which we show in proposition 1 and Theorem 2. We also demonstrate its strong
performance empirically on simulated data scenarios in Section 4 and Appendix B.1
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We have a dedicated limitations section where we discuss limitations in our
theoretical results regarding both assumptions of sufficiently quickly decaying weights in
our linear smoother and requiring smoothness in h∗rather than in g∗. We also discuss the
limitations in the interpretability of quantile based approaches in general when exploring
treatment effect as well as limitations in the interpretability of our assumptions within our
theory. Finally we briefly address the methodological limitations of our approach acknowl-
edging this as a potential area for improvement.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The au-
thors should reflect on how these assumptions might be violated in practice and what
the implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
31• If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The two main results of the paper, Proposition 1 and Theorem 2 are proved
in sections C.1 and C.2. The assumptions for these results are given in Assumptions 1 & 2
with the implications of these assumptions discussed.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The paper gives detailed information about the experimental set-up and ex-
plicit algorithms for methods introduced. The specific use of algorithm 1 with NW esti-
mation is also described Appendix A.2. In addition code is provided in the supplementary
materials with ‘SimulatedExperiments.ipynb‘ reproducing all of the experiments. We also
provide a link to a public github repository containig this in a footnote on page 9.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might suffice, or if the contribution is a specific model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
32the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Code is provided in the supplementary materials with each function docu-
mented as well as in a linked public github repository. Jupyter Notebooks are provided to
reproduce all of the experiments and real world data settings used within the paper.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
• While we encourage the release of code and data, we understand that this might not
be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
336.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Most of the experimental details are provided alongside the experimental
results in Section 4 with additional details provided in Appendix A.4. In addition code to
replicate all of the experiments is provided in the supplmentary materials.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [Yes]
Justification: For our experimental results we explain that 500 replications of the entire
fitting procedure (including newly generated data making each run entirely independent)
was run and that 95% confidence intervals are provided for the mean absolute error of each
estimator.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
34Answer: [Yes]
Justification: Information on runtime and compute resources is provided in Appendix A.4.
Overall computational resources were very low with all experiments run on a personal
laptop and each one taking less than an hour even with a large number of replications.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: No outsourcing was done for this project/paper and all datasets used are pub-
licly available and completely anonymised. Additionally we feel there is essentially no
scope for our work to cause negative societal impact.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We discuss the potential positive impact of our work in terms of further em-
powering HTE analysis. We do not for see any potential negative impacts of our work as it
is simply furthering the field of HTE and quantile based treatment effect analysis. To our
knowledge, there is no suggestion that these fields could be used maliciously or unfairly
beyond the ways that any regression technique or analysis could.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
cific groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
35generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: There are no new datasets released in our work. Our models use standard non-
parametric regression techniques and adapt these to best estimate our estimand, the CQC
meaning there is no real risk of misuse beyond the misuse of any regression technique.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: All datasets are publicly available and properly referenced. Proprietary code
used is also open source and used in accordance with its licence. The licence for this code
can also be found in code itself. The model we use is our own and so no licensing is
required.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
36• If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [Yes]
Justification: The only asset is the code which is implemented in our method which is
provided in the supplementary materials alongside a licence.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing or research with human subjects is used.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
37• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
38