Clustering with Non-adaptive Subset Queries
Hadley Black
UC San DiegoEuiwoong Lee
University of MichiganArya Mazumdar
UC San DiegoBarna Saha
UC San Diego
Abstract
Recovering the underlying clustering of a set Uofnpoints by asking pair-wise
same-cluster queries has garnered significant interest in the last decade. Given a
query S⊂U,|S|= 2, the oracle returns yesif the points are in the same cluster
andnootherwise. We study a natural generalization of this problem to subset
queries for|S|>2, where the oracle returns the number of clusters intersecting
S. Our aim is to determine the minimum number of queries needed for exactly
recovering an arbitrary k-clustering. We focus on non-adaptive schemes, where all
the queries are asked in one round, thus allowing for the querying process to be
parallelized, which is a highly desirable property.
Foradaptive algorithms with pair-wise queries, the complexity is known to be
Θ(nk), where kis the number of clusters. In contrast, non-adaptive pair-wise query
algorithms are extremely limited: even for k= 3, such algorithms require Ω(n2)
queries, which matches the trivial O(n2)upper bound attained by querying every
pair of points. Allowing for subset queries of unbounded size, O(n)queries is
possible with an adaptive scheme. However, the realm of non-adaptive algorithms
remains completely unknown. Is it possible to attain algorithms that are non-
adaptive while still making a near-linear number of queries?
In this paper, we give the first non-adaptive algorithms for clustering with subset
queries. We provide, (i) a non-adaptive algorithm making O(nlog2nlogk)queries
which improves to O(nlogk)when the cluster sizes are within any constant factor
of each other, (ii) for constant k, a non-adaptive algorithm making O(nlog log n)
queries. In addition to non-adaptivity, we take into account other practical con-
siderations, such as enforcing a bound on query size. For constant k, we give an
algorithm making eO(n2/s2)queries on subsets of size at most s≤√n, which is
optimal among all non-adaptive algorithms within a logn-factor. For arbitrary k,
the dependence varies as ˜O(n2/s).
1 Introduction
Clustering is one of the most fundamental problems in unsupervised machine learning, and permeates
beyond the boundaries of statistics and computer science to social sciences, economics and so on. The
goal of clustering is to partition items so that similar items are in the same group. The applications
of clustering are manifold. However, finding the underlying clusters is sometimes hard for an
automated process due to data being noisy, incomplete, but easily discernible by humans. Motivated
by this scenario, in order to improve the quality of clustering, early works have studied the so-called
clustering under “limited supervision” (e.g.,[ 1,2]). Balcan and Blum initiated the study of clustering
under active feedback [ 3] where given the current clustering solution, the users can provide feedback
whether a cluster needs to be merged or split. Perhaps a simpler query model would be where users
only need to answer the number of clusters, and that too only on a subset of points without requiring
to analyze the entire clustering. This scenario is common in unsupervised learning problems, where a
centralized algorithm aims to compute a clustering by crowdsourcing. The crowd-workers play the
role of an oracle here, and are able to answer simple queries that involve a small subset of the universe.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Mazumdar and Saha [ 4,5,6], and in independent works Mitzenmacher and Tsourakis [ 7], as well as
Asthani, Kushagra and Ben-David [ 8] initiated a theoretical study of clustering with pair-wise aka
same-cluster queries. Given any pair of points u, v, the oracle returns whether uandvbelong to the
same cluster or not. Such queries are easy to answer and lend itself to simple implementations [ 9].
This has been subsequently extremely well-studied in the literature, e.g. [ 10,11,4,12,13]. In fact,
triangle-queries have also been studied, e.g. [ 14]. Moreover, clustering with pair-wise queries is
intimately related to several well-studied problems such as correlation clustering [ 15,16,17,10,18],
edge-sign prediction problem [19, 7], stochastic block model [20, 21] etc.
Depending on whether there is an interaction between the learner/algorithm and the oracle, the
querying algorithms can be classified as adaptive and non-adaptive [ 5]. In adaptive querying, the
learner can decide the next query based on the answers to the previous queries. An algorithm is
called non-adaptive if all of its queries can be specified in one-round. Non-adaptive algorithms can
parallelize the querying process as they decide the entire set of queries apriori. This may greatly
speed up the algorithm in practice, significantly reducing the time to acquire answers [ 22]. Thus,
in a crowdsourcing setting being non-adaptive is a highly desirable property. On the flip side, this
makes non-adaptive algorithms significantly harder to design. In fact, when adaptivity is allowed,
nkpair-wise queries are both necessary and sufficient to recover the entire clustering, where nis
the number of points in the ground set to be clustered and k(unknown) is the number of clusters.
However as shown in [ 5] and our Theorem C.1, even for k= 3, even randomized non-adaptive
algorithms can do no better than the trivial O(n2)upper bound attained by querying all pairs .
We study a generalization of pair-wise queries to subset queries, where given any subset of points,
the oracle returns the number of clusters in it. We consider the problem of recovering an unknown
k-clustering (a partition) on a universe Uofnpoints via black-box access to a subset query oracle .
More precisely, we assume that there exists a groundtruth partitioning of U=Fk
i=1Ci, and upon
querying with a subset S⊆U, the oracle returns q(S) =|{i:Ci∩S̸=∅}|, the number of clusters
intersecting S. Considering the limitations of pair-wise queries for non-adaptive schemes, we ask the
question if it is possible to use subset queries to design significantly better non-adaptive algorithms.
In addition to being a natural model for interactive clustering, this problem also falls into the growing
body of work known as combinatorial search [23,24] where the goal is to reconstruct a hidden object
by viewing it through the lens of some indirect query model (such as group testing [ 25,26,24,27,28]).
The problem is also intimately connected to coin weighing where given a hidden vector x∈ {0,1}n,
the goal is to reconstruct xusing queries of the form q(S) :=P
i∈SxiforS⊆[n]. It is known that
Θ(n/logn)is the optimal number of queries [ 29,30,31], which can be obtained by a non-adaptive
algorithm. There are improvements for the case when ∥x∥1=dford≪n[32,33,34]. Moreover,
there has been significant work on graph reconstruction where the task is to reconstruct a hidden
graph G= (V, E)from queries of the form q(S, T) :=|{(u, v)∈E:u∈S, v∈T}|for subsets
S, T⊆V. [35,36,37,38]. There are also algorithms that perform certain tasks more efficiently than
learning the whole graph (sometimes using different types of queries) [ 39,40,41,42,43,44,45,46],
and quantum algorithms that use fewer queries than classical algorithms [47].
It is not too difficult to show that an algorithm making O(nlogk)queries (Appendix H) is possible
fork-clustering, while Ω(n)queries is an obvious information theoretic lower bound since each query
returns logkbits of information and the number of possible k-clusterings is kn= 2nlogk. In fact, it
is possible to have an algorithm with O(n)query complexity (personal communication, Chakrabarty
and Liao). However, both of these algorithms are adaptive, ruling them out for the non-adaptive
setting. So far, the non-adaptive setting of this problem remained unexplored.
1.1 Results
Our main results showcase the significant strength of using subset queries in the non-adaptive setting.
We give randomized algorithms that recover the exact clustering with probability 1−δ, for any
arbitrary constant δ >0using only near-linear number of subset queries.
Theorem 1.1. (Theorem 2.5, simplified) There is a randomized, non-adaptive k-clustering algorithm
making O(nlog2nlogk)subset queries.
For constant k, this dependency can be further improved.
2Theorem 1.2. (Theorem 2.2, simplified) There is a randomized, non-adaptive k-clustering algorithm
making O(nlog log n)subset queries when kis any constant.
Note that the algorithm of Theorem 1.2 works for any value of k, but its dependence on this parameter
is inferior to that of Theorem 1.1 (see the formal version Theorem 2.2 for the exact dependence on k).
Thus, we state the theorem above for constant kto emphasize the much improved dependence on n.
Our algorithms also run in polynomial time, and generalizes to work with queries of bounded size.
Bounding query size: Another practical consideration is query size, s. Depending on the scenarios,
and capabilities of the oracle, it may be easier to handle queries on small subsets. An extreme case is
pair-wise queries (s= 2), where O(nk)pair queries are enough with adaptivity but any non-adaptive
algorithm has to use Ω(n2)queries even for k= 3. Since a subset query on Scan be simulated by  |S|
2
pair queries, we immediately get the following theorem.
Theorem 1.3. (Corollary C.2, restated) Any non-adaptive k-clustering algorithm that is only allowed
to query subsets of size at most smust make at least Ω(min(n2
s2, n))queries.
Theorems 1.1 and 1.2 above show that this can be bypassed by allowing larger subset queries.
However, some of these queries are of size Ω(n), and this raises the question, is there a near-linear
non-adaptive algorithm which only queries subsets of size at most O(√n)?We answer this in the
affirmative, implying that our lower bound is tight in terms of s.
Theorem 1.4 (Theorem A.1, informal) .There is a non-adaptive k-clustering algorithm making
O(nlognlog log n)subset queries of size at most O(√n)when kis any constant. For all sufficiently
small s=o(√n), the algorithm makes O(n2
s2logn)subset queries of size at most s.
The result also extends to arbitrary kwith slightly worse dependency on s(Theorem 2.5). Our
algorithm for bounded queries from Theorem 1.4 has the additional desirable property of being
sample-based meaning that each of its queries is a set formed by independent, uniform samples. I.e.
the algorithm specifies a query size t≤s, and then receives (S, q(S))where Sis formed by ti.i.d.
uniform samples from U. Being sample-based enables the algorithm to leave the task of curating
each query up to the individual answering the query. The algorithm needs only to specify the query
sizes, and then recover the clustering once the queries have been curated and answered.
The "roughly balanced" case: Next, we consider the natural special case of recovering a k-clustering
when the cluster sizes are within a constant factor of one another. Informally, let us call such a
clustering "roughly balanced".
Theorem 1.5 (Theorems B.1 and E.1, informal) .There are non-adaptive algorithms for recovering a
roughly balanced k-clustering which make (a) O(nlogk)subset queries when k≤O(n
log3n), and
(b)O(nlog2k)subset queries for any k≤n.
Allowing two rounds of adaptivity Finally, we show if we allow an extra round of adaptivity, then
that helps to improve the dependency on the logarithmic factors further. Specifically, we prove the
following theorems.
Theorem 1.6 (Theorems F.1 and F.3, informal) .There is a 2-round deterministic k-clustering
algorithm making O(nlogk)subset queries. There is a randomized 2-round algorithm for recovering
a roughly-balanced k-clustering making O(nlog log k)subset queries.
Organization: The remainder of the paper is organized as follows. In Section 2, we give our main
results developing non-adaptive algorithms with near-linear query complexity Theorems 1.1 and 1.2.
Our results for sample-based, bounded query algorithms are given in Appendix A. Finally, we prove
our results for the balanced setting in Appendix B, our lower bounds in Appendix C, and our results
for two-round algorithms in Appendix F.
2 Algorithms with Nearly Linear Query Complexity
In this section we describe the algorithms behind our main results, Theorems 1.1 and 1.2, and give
formal proofs of their correctness. In Section 2.1 we describe an algorithm making O(nlog log n)
subset queries when the number of clusters kis assumed to be a constant. In general, the dependence
on the number of clusters is O(klogk). In Section 2.2, we give an alternative algorithm with eO(n)
query complexity for any k≤n.
32.1 An O(nlog log n)Algorithm for Constant k
Warm Up. When there are only 2clusters, there is a trivial non-adaptive algorithm making O(n)
pair queries : Choose an arbitrary x∈Uand query {x, y}for every y∈U. The set of points y
where q({x, y}) = 1 form one cluster, and the second cluster is the complement. If we allow one
more round of adaptivity, then for 3-clustering we could repeat this one more time and again get an
O(n)query algorithm. However, for non-adaptive 3-clustering it is impossible to do better than the
trivial O(n2)algorithm (see Theorem C.1). Essentially, this is because in order to distinguish the
clusterings ({x},{y}, U\ {x, y})and({x, y},∅, U\ {x, y})the algorithm must query {x, y}and
their are n
2
ways to hide this pair. Overcoming this barrier using subset queries require significant
new ideas.
Our main ideas are best communicated by focusing on the case of 3-clustering. It suffices to correctly
reconstruct the two largest clusters, since the third cluster is just the complement of their union. Let
A, B denote the largest, and second largest clusters, respectively. Since |A| ≥n/3, it is easy to find:
sample a random x∈Uand query {x, y}for every y∈U. The cluster containing xis precisely
{y∈U:q({x, y}) = 1}. With probability at least 1/3, we have x∈Aand so repeating this a
constant number of times will always recover A. On the other hand, Bmay be arbitrarily small
and in this case the procedure clearly fails to recover it. The first observation is that once we know
A, we can exploit larger subset queries to explore U\Asince q(S\A) =q(S)−1(S∩A̸=∅).
Importantly, the algorithm is non-adaptive and so the choice of Scannot depend on A, but we are
still able to exploit this trick with the following two strategies. Let δn=|B|denote the size of Band
note that this implies |A| ≥(1−2δ)nsince the third cluster is of size at most B.
Strategy 1: Suppose a query Scontains exactly one point outside of A, i.e.S\A={x}. Then, for
y /∈A,q(S∪ {y}) =q(S)iffx, ybelong to the same cluster. Thus, we can query S∪ {y}for every
y∈Uto learn the cluster containing x. IfSis a random set of size t≈1/δ, then the probability
that|S\A|= 1is at least t·δ·(1−2δ)t−1= Ω(1) . Of course, we do not know δ, but we can try
t= 2pfor every p≤lognand one of these choices will be within a factor of 2from 1/δ. This gives
us an O(nlogn)query algorithm since we make nqueries per iteration.
Strategy 2: Suppose Sintersects Aand contains exactly two points outside of A, i.e.S\A={x, y}.
Then, q({x, y}) =q(S)−1which tells us whether or not x, ybelong to the same cluster. If x, y
belong to same cluster, add it to a set E, and let G(U\A, E)denote a graph on the remaining points
with this set of edges. By transitivity, a connected component in this graph corresponds to a subset of
one of the remaining two clusters. In particular, if the induced subgraph, G[B], is connected, then we
recover B. Moreover, if Sis a random set of size t≈1/δ, then the probability that two points land
inBand the rest land in Ais at least t
2
·δ2·(1−2δ)t−2= Ω(1) . A basic fact from random graph
theory says that after ≈ |B|ln|B| ≤δnlnnoccurrences of this, G[B]becomes connected with high
probability and so querying Ω(δnlnn)random Sof size ≈1/δwill suffice. Again, we try t= 2p
for every p≤logn, resulting in a total of ≈nlnnP
p2−p=O(nlogn)queries.
Finally, we can combine strategies (1) and (2) as follows to obtain our O(nlog log n)query algorithm.
The main observation is that the query complexity of strategy (2) improves greatly if we assume that
|B|is small enough. If we know that δ≤1
logn, then we only need to try t= 2p≥lognand so the
query complexity becomes ≈nlnnP
p≥log log n2−p=O(n). On the other hand, if we assume that
δ >1
logn, then in strategy (1) we only need to try p≤log log nyielding a total of O(nlog log n)
queries. Combining these yields the final algorithm.
Remark 2.1 (On approximate clustering) .We point out that these ideas can be used to obtain more
efficient algorithms for the easier task of correctly clustering a (1−α)-fraction of points. In this
setting we can ignore the case of δ < α/ 2(recall the definition of δabove) as this will only result in
an incorrect classification of an α-fraction of points. Thus, for example, one can employ "strategy 1"
above, but only iterate over p≤log(2 /α), leading to an O(nlog1
α)query algorithm. However, in
this paper we focus on the more challenging task of recovering the clustering exactly, and leave the
possibility of more efficient approximate algorithms as a possible direction of future work.
Algorithm. A full description of the algorithm is given in pseudocode Alg. 1, which is split into
two phases: a "query selection phase", which describes how queries are chosen by the algorithm, and
a "reconstruction phase" which describes how the algorithm uses the query responses to determine
the clustering. Both phases contain a for-loop iterating over all p∈ {0,1, . . . , logn}where the goal
4of the algorithm during the p’th iteration is to learn all remaining clusters of size at leastn
2k·2p. This
is accomplished by two different strategies depending on whether pis small or large.
When p≤log log n, the algorithm samples O(klogk)random sets Tformed by 2psamples from U
and makes a query on TandT∪ {x}for every x∈U(see lines 5-9 of Alg. 1). Let Rpbe the union
of all clusters reconstructed before phase p(i.e., clusters of size at leastn
2k·2p−1). If such a Tcontains
exactly one point z∈T\ Rpbelonging to an unrecovered cluster, then we can use these queries to
learn the cluster containing z(see lines 24-28 of Alg. 1), since for x∈U\ Rp,q(T) =q(T∪ {x})
if and only if x, zbelong to the same cluster. Moreover, we show that this occurs with probability
Ω(1) and repeat this O(klogk)times to ensure that every cluster Cwhere |C| ∈[n
2k·2p,n
2k·2p−1)is
learned with high probability. The total number of queries made during iterations p≤log log nis
O(nlog log n·klogk).
When p >log log n, the algorithm queries O(nk·logn
2p)random sets Tagain formed by 2psamples
from U(see lines 11-14 of Alg. 1). Note thatP
p>log log n2−p=O(1
logn)and so the total number
of queries made during these iterations is O(nk).
We now describe the reconstruction phase (see lines 32-37 of Alg. 1). If Tcontains exactly two points
x, y∈T\ Rpbelonging to unrecovered clusters, then we can use the fact that we already know the
clustering on Rpto tell whether or not x, ybelong to the same cluster or not, i.e. we can compute
q({x, y})∈ {1,2}from q(T). We then consider the set of all such pairs where q({x, y}) = 1 (this
isQ′′
pdefined in line 34) and consider the graph Gwith this edge set, and vertex set U\ Rp, the
set of points whose cluster hasn’t yet been determined. If two points belong to the same connected
component in this graph, then they belong to the same cluster. Thus, the analysis for this iteration
boils down to showing that with high probability, the induced subgraph G[C]will be connected for
every Cwhere |C| ∈[n
2k·2p,n
2k·2p−1). This is accomplished by applying a basic fact from the theory
of random graphs, namely Fact 2.4.
Analysis We restate the main theorem for this section.
Theorem 2.2. There is a non-adaptive algorithm for k-clustering that uses O(nlog log n·klogk)
subset queries and succeeds with probability at least 1−δfor any constant δ >01.
The following Lemma 2.3 establishes that after the first piterations of the algorithm’s query selection
and reconstruction phases, all clusters of size at leastn
2k·2phave been learned with high probability.
This is the main technical component of the proof. After stating the lemma we show it easily implies
that Alg. 1 succeeds with probability at least 99/100by an appropriate union bound. The choice of
99/100is arbitrary, and can be made 1−δfor any constant δ.
Lemma 2.3. For each p= 0,1, . . . , logn, letEpdenote the event that all clusters of size at least
n
2k·2phave been successfully recovered immediately following iteration pof Alg. 1. Then,
Pr[¬E0]≤1
100kand Pr[¬Ep| Ep−1]≤1
100kfor all p∈ {1,2. . . ,logn}.
Proof of Theorem 2.2: Before proving Lemma 2.3, we first observe that it immediately implies
the correctness of Alg. 1 and thus proves Theorem 2.2. Let I0= (n
2k, n]and for 1≤p≤logn, let
Ip= [n
2k·2p,n
2k·2p−1). If there are no clusters Cfor which |C| ∈Ip, then trivially Pr[¬Ep| Ep−1] = 0 ,
and otherwise Pr[¬Ep| Ep−1]≤1
100kby the lemma. Since there are kclusters, clearly there are at
most kvalues of pfor which there exists a cluster with size in the interval Ip. Using this observation
and a union bound, we have
Pr[¬Elogn]≤Pr[¬E0] +lognX
p=1Pr[¬Ep| Ep−1]≤1
100
which completes the proof of correctness since the algorithm succeeds iff Elognoccurs.
Query complexity: During iterations p <log log nthe algorithm makes at most O(nlog log n·
klogk)queries. During iterations p >log log n, it makes at most O(nklogn)P
p>log log n2−p=
O(nk)queries since k≤n.
1For simplicity of exposition, we use a constant δin our proofs. The success probability can be boosted to
any1−1
poly( n)by paying a lognfactor in the query complexity in all algorithms.
5Time complexity: We assume that obtaining a uniform random sample from a set of size ncan
be done in O(1)time. Thus, since the algorithm makes O(nlog log n·klogk)queries and each is
on a set of size at most n, the total runtime of the query selection phase (lines 3-15) is bounded by
O(n2log log n·klogk). We now account for the runtime in the reconstruction phase. Lines (25-28)
clearly can be performed in O(n)time and so the time spent in lines (24-28) is O(|Qp| ·n). Now,
forT∈Qp, checking if |T\ Rp|= 2 can clearly be done in O(n)time and so lines (33-34) run
in time O(|Qp| ·n). Line (36) amounts to finding every connected component in Gpwhich can be
done in time O(|Q′′
p|+n) =O(|Qp|+n)by iteratively running a BFS (costing time linear in the
number of edges plus the number of vertices). Thus, the runtime of the p’th iteration of the for-loop
is always dominated by O(|Qp| ·n). Since the total number of queries is O(nlog log n·klogk), the
total runtime of the reconstruction phase is O(n2log log n·klogk).
We now prove the main Lemma 2.3.
Proof. of Lemma 2.3. LetCpdenote the set of clusters recovered before phase pand let Rp=S
C∈CpC. When p= 0, both of these sets are empty. We will consider three cases depending on the
value of p.
Case 1: p= 0. Let Cdenote some cluster of size |C| ≥n
2k. Note that in this iteration the sets T
sampled by the algorithm in line (7) are singletons. We need to argue that one of these singletons will
land in C, and thus Cis recovered in line (28), with probability at least 1−1
100k2. Since there are at
most kclusters, applying a union bound completes the proof in this case.
A uniform random element lands in Cwith probability at least1
2kand so this fails to occur for all
|Q0| ≥4kln 10ksamples with probability at most (1−1
2k)4kln 10k≤exp(−2 ln 10 k) =1
100k2, as
claimed.
Case 2: 1≤p≤log log n. Let Cdenote some cluster with size |C| ∈[n
2k·2p,n
2k·2p−1). Note that
we are conditioning on the event that every cluster of size ≥n
2k·2p−1has already been successfully
recovered after iteration p−1. Thus, the number of elements belonging to unrecovered clusters is
|U\ Rp| ≤k·n
2k·2p−1=n
2p. We need to argue that the set Qpwill contain some Tsampled in line
(7) such that T\ Rp={z}where z∈C, and thus Cis successfully recovered in line (28), with
probability at least 1−1
100k2. Once this is established, the lemma again follows by a union bound.
We have
Pr
T:|T|=2p[|T\Rp|= 1andT\Rp⊆C] =|T|·|C|
n·|Rp|
n|T|−1
≥2p
k·2p+1
1−1
2p2p
≥1
2ek
and so the probability that this occurs for some T∈Qpis at least 1−(1−1
2ek)4ekln 10k≥1−1
100k2,
as claimed.
Case 3: p >log log n. Let Cdenote some cluster with size |C| ∈[n
2k·2p,n
2k·2p−1). Note that
|U\ Rp| ≤k·n
2k·2p−1=n
2p. Recall from lines (34-35) the definition of Q′′
pand recall that Gpis
the graph with vertex set U\ Rpand edge set Q′′
p. We need to argue that the induced subgraph
Gp[C]is connected, and thus Cis successfully recovered in lines (36-37), with probability at least
1−1
100k2. Once this is established, the lemma again follows by a union bound. We rely on the
following standard fact from the theory of random graphs. For completeness, we give a proof in
Appendix D.2.
Fact 2.4. LetG(N, p)denote an Erdös-Rényi random graph. That is, the graph contains Nvertices
and there is an edge between each pair of vertices with probability p. Ifp≥1−(δ/3N)2/N, then
G(N, p)is connected with probability at least 1−δ.
Consider any x, y∈Cand observe that
Pr
T:|T|=2p[T\ Rp={x, y}] =2p
2
·1
n2·|Rp|
n2p−2
≥22p
3n2
1−1
2p2p
≥22p
10n2.
6Algorithm 1: Non-adaptive Algorithm for Constant k
1Input: Subset query access to a hidden partition C1⊔ ··· ⊔ Ck=Uof|U|=npoints;
2(Query Selection Phase)
3forp= 0,1, . . . , logndo
4 Initialize Qp← ∅;
5 ifp≤log log nthen
6 Repeat 4ekln(10 k)times;
7 −→Sample T⊆Uformed by 2pindependent uniform samples from U;
8 −→Query TandT∪ {x}for all x∈U;
9 −→AddTtoQp;
10 end
11 ifp >log log nthen
12 Repeat40nkln(300 nk2)
2p times;
13 −→Sample T⊆Uformed by 2pindependent uniform samples from U;
14 −→Query Tand add it to Qp;
15 end
16end
17(Reconstruction Phase)
18Initialize learned cluster set C0← ∅;
19forp= 0,1, . . . , logndo
20 LetCpdenote the collection of clusters reconstructed before iteration p;
21 LetRp=S
C∈CpCdenote the points belonging to these clusters;
22 Initialize Cp+1← C p;
23 ifp≤log log nthen
24 forT∈Qpdo
25 if|T\ Rp|= 1then
26 Letzdenote the unique point in T\ Rp;
27 Ifx∈U\ Rp, then q(T) =q(T∪ {x})iffx, zare in the same cluster;
28 Thus, we add {x∈U\ Rp:q(T) =q(T∪ {x})}toCp+1;
29 end
30 end
31 end
32 ifp >log log nthen
33 LetQ′
p={T\ Rp:T∈Qpand|T\ Rp|= 2}. Since each T∈Qpis a uniform
random set, the elements of Q′
pare uniform random pairs in U\ Rp;
34 LetQ′′
p={{x, y} ∈Q′
p:q({x, y}= 1)}denote the set of pairs in Q′
pwhere both points
lie in the same cluster. This set can be computed since q(T\ Rp) =q(T)−q(T∩ Rp)
andq(T∩ Rp)is known since at this point we have reconstructed the clustering on Rp;
35 LetGpdenote the graph with vertex set U\ Rpand edge set Q′′
p;
36 LetC1, . . . , C ℓdenote the connected components of Gpwith size at leastn
2k·2p;
37 AddC1, . . . , C ℓtoCp+1;
38 end
39end
40Output clustering Clogn+1
7Recall that the algorithm queries |Qp|=40·nkln(300 nk2)
2p random sets Tof size 2p. Thus,
Pr
Qp[(x, y)∈E(Gp[C])] = Pr
Qp
{x, y} ∈Q′′
p
= Pr
Qp[∃T∈Qp:T\ Rp={x, y}]
≥1−
1−22p
10n240n
2p·kln(300 nk2)
≥1−exp
−2p
n·4kln(300 nk2)
and using |C| ≥n
2k·2pand|C| ≤n, we obtain
Pr
Qp[(x, y)∈E(Gp[C])]≥1−exp
−2 ln(300 nk2)
|C|
≥1−exp
−2 ln(300 k2|C|)
|C|
= 1−1
300k2|C|2
|C|
.
Thus, (x, y)is an edge in Gp[C]with probability at least 1−
1
300k2|C|2
|C|and so by Fact 2.4 Gp[C]
is connected with probability at least 1−1
100k2, as claimed.
Bounded Query Size We can restrict the query size to s≤√n, and still achieve a near-linear
query complexity. We sketch the main ideas here for the case of k= 3 similar to the "warm-up"
in Section 2.1. Details are provided in Appendix A. Our Theorem 1.4 gives an O(nlognlog log n)
query non-adaptive sample-based algorithm using subset queries of size at most O(√n). The
main idea is to employ "Strategy 2" described in the warm-up section of Section 2.1 with a slight
alteration. Let A, B denote the largest, and second largest clusters, respectively, where |B|=δn
and so |A| ≥(1−2δ)n. Observe that if we take a random set Sof size t≈p
1/δ, then the
probability that two points land in Band the rest land in Ais at least t
2
·δ2·(1−2δ)t−2= Ω( δ).
Recalling the definition of the graph Gand the discussion in Section 2.1, after querying Ω(nlnn)
suchS, the induced subgraph G[B]becomes connected with high probability, thus recovering the
clustering. Similar ideas let us generaize to any s, and achieve an optimal dependency on sas stated
in Corollary C.2 for constant k.
2.2 An O(nlog2nlogk)Algorithm for General k
We now consider the situation with general k, for which our algorithm and analysis follow a
completely different approach by using techniques from combinatorial group testing.
Warm up. The main subroutine in our algorithm is a procedure for recovering the support of a
Boolean vector via ORqueries. Given a vector v∈ {0,1}n, anORquery on a set S⊆[n]returns
ORS(v) =W
i∈Svi, i.e. it returns 1iffvhas a 1-valued coordinate in S. The problem of recovering
the support of v,supp(v) ={i:vi= 1}viaORqueries is a basic problem from the group testing
and coin-weighing literature. The relevance of this problem for k-clustering with subset queries is
as follows. Consider a hidden clustering C1⊔ ··· ⊔ Ck=U. Given x∈U, letC(x)denote the
cluster containing U={x1, . . . , x n}(an arbitrary ordering of U), and let v(x)∈ {0,1}ndenote the
Boolean vector where v(x)
i=1(xi∈C(x)). An ORquery on set Stov(x)can be simulated by a
subset query to the clustering on sets SandS∪ {x}since
ORS(v(x)) =_
i∈Sv(x)
i=1(C(x)∩S̸=∅) =1(q(S∪ {x}) =q(S)).
Thus, the problem or reconstructing C(x)via subset queries is equivalent to the problem of recovering
v(x)viaORqueries, up to a factor of 2in the query complexity.
Then, to learn a cluster Cwith sizen
2p≤ |C| ≤n
2p−1it suffices to sample O(2p)random x(one
of which lands in Cwith high probability) and then recover C(x)using O(n
2plogn
δ)ORqueries.
Iterating over every p≤lognand boosting the number of samples to guarantee a high probability of
success for all kclusters yields our algorithm.
8This algorithm can also be restricted to only make subset queries of size at most s, and the query
complexity scales with1
s.
Theorem 2.5. For every s∈[2, n], there is a non-adaptive k-clustering algorithm making
O(nlognlogk·(n
s+ log s))subset queries of size at most s. In particular, for unbounded query
size the algorithm makes O(nlog2nlogk)queries.
Proof of Theorem 2.5 We will use the following lemma for recovering supp(v) ={i:vi= 1}via
ORqueries. We prove and discuss this lemma in Appendix D.1 (see Lemma D.5).
Lemma 2.6. Letv∈ {0,1}nands, t≥1be positive integers where s≤n
t. There is a non-adaptive
algorithm that makes O(n
slogn
δ)ORqueries on subsets of size s, and if |supp(v)| ≤t, returns
supp(v)with probability 1−δ, and otherwise certifies that |supp(v)|> t. The algorithm runs in
timeO(nlogn
δ).
Recall that ORS(v(x)) =1(q(S∪ {x}) =q(S)), i.e. an ORquery on Sis simulated by subset
queries on sets SandS∪ {x}. Thus, we immediately get the following corollary.
Corollary 2.7. Letx∈Uandr≥2, t≥1be positive integers where r≤n
t. There is a non-
adaptive algorithm that makes O(n
rlogn
δ)subset -queries on sets of size at most r, and if |C(x)| ≤t,
returns C(x)with probability 1−δ, and otherwise certifies that |C(x)|> t. The algorithm runs in
timeO(nlogn
δ).
Algorithm The pseudocode for the algorithm is given in Alg. 2. The idea is to draw random points
x∈U(line 5) and then use the procedure from Corollary 2.7 as a subroutine to try to learn C(x)
(line 6). By the corollary, this will succeed with high probability in recovering C(x)as long as tis set
to something larger than |C(x)|. Note that the query complexity of this subroutine depends2ont. If a
cluster Cis small, then Pr[x∈C]is small, but we can call the subroutine with small t, while if C(x)
is large, then Pr[x∈C]is reasonably large, though we will need to call the subroutine with larger t.
Concretely, the algorithm iterates over every p∈ {1, . . . , logn}(line 3), and in iteration pthe goal is
to learn every cluster Cwith|C| ∈[n
2p,n
2p−1]. To accomplish this, we sample Θ(2plogk)random
points x∈U(line 4-5) and for each one, call the subroutine with t=n
2p−1(line 6), which is an upper
bound on the sizes of the clusters we are trying to learn.Note that we always invoke the corollary
with query size r= min( s,2p−1)≤s, enforcing the query size bounded stated in Theorem 2.5.
Algorithm 2: Non-adaptive Algorithm for General k
1Input: Subset query access to a hidden partition C1⊔ ··· ⊔ Ck=Uof|U|=npoints;
2Initialize hypothesis clustering C ← ∅ ;
3forp= 1, . . . , logndo
4 Repeat 2pln(200 k)times:
5−→Sample x∈Uuniformly at random;
6−→Run the procedure from Corollary 2.7 on xwitht=n
2p−1, query-size r= min( s,2p−1),
and error probability δ=1
200k. This outputs C(x), the cluster containing x, with
probability at least 1−δif|C(x)| ≤t;
7−→If the procedure returns a set C, then set C ← C ∪ { C}. Otherwise, continue;
8end
9Output the clustering C.
Query complexity: Note that the number of queries made in line (6) during the p’th iteration is
O(n
slogn)when 2p−1≥s, andO(n
2plogn)when 2p−1< s. Therefore, the total number of queries
made is at most
O(logk)
X
p: 1≤2p−1<sO(2p·n
2plogn) +X
p:s≤2p−1≤nO(2p·n
slogn)
.
The first sum is bounded by O(nlognlogs)and the second sum is bounded by O(n2
slogn). The
time-complexity is clearly identical by Corollary 2.7.
2For intuition, if the subroutine is called with r=n
t, then Corollary 2.7 makes O(tlogn
δ)queries.
9Time complexity: We assume that attaining a uniform sample from a set of size ncan be performed
inO(1)time. The procedure in line (6) has runtime at most O(nlogn)since we set δ= Θ(1
k). Thus,
the total runtime of the algorithm is O(nlognlogk)·P
p≤logn2p=O(n2lognlogk).
Correctness: Consider any cluster Cand let p∈ {1, . . . , logn}be such thatn
2p≤ |C| ≤n
2p−1.
LetECdenote the event that some element x∈Cis sampled in line (5) during iteration p. Let
RCdenote the event that C∈ C when the algorithm terminates. Observe that by Corollary 2.7,
Pr[RC| EC]≥1−δ= 1−1
200k. Moreover, using our lower bound on Cwe have
Pr[¬EC]≤
1−|C|
n2pln 200 k
≤
1−1
2p2pln 200 k
≤1
200k.
Thus, Pr[¬RC]≤Pr[¬EC] + Pr[ ¬RC| EC]≤1
100kand taking another union bound over all k
clusters completes the proof.
Acknowledgements. Hadley Black, Arya Mazumdar, and Barna Saha were supported by NSF
TRIPODS Institute grant 2217058 (EnCORE) and NSF 2133484. Euiwoong Lee was also supported
in part by NSF grant 2236669 and Google. The collaboration is the result of an EnCORE Institute
Workshop.
10References
[1]David Cohn, Rich Caruana, and Andrew McCallum. Semi-supervised clustering with user
feedback. Constrained clustering: advances in algorithms, theory, and applications , 4(1):17–32,
2003. 1
[2]Eric Bair. Semi-supervised clustering methods. Wiley Interdisciplinary Reviews: Computational
Statistics , 5(5):349–361, 2013. 1
[3]Maria-Florina Balcan and Avrim Blum. Clustering with interactive feedback. In International
Conference on Algorithmic Learning Theory , pages 316–328. Springer, 2008. 1
[4]Arya Mazumdar and Barna Saha. A theoretical analysis of first heuristics of crowdsourced
entity resolution. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 31,
2017. 2
[5]Arya Mazumdar and Barna Saha. Clustering with noisy queries. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman
Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,
pages 5788–5799, 2017. 2
[6]Arya Mazumdar and Barna Saha. Query complexity of clustering with side information.
InAdvances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , 2017. 2
[7]Michael Mitzenmacher and Charalampos E Tsourakakis. Predicting signed edges with
o(n1+o(1)logn)queries. arXiv preprint arXiv:1609.00750 , 2016. 2
[8]Hassan Ashtiani, Shrinu Kushagra, and Shai Ben-David. Clustering with same-cluster queries.
Advances in neural information processing systems , 29, 2016. 2
[9]Arya Mazumdar and Soumyabrata Pal. Semisupervised clustering, and-queries and locally
encodable source coding. Advances in Neural Information Processing Systems , 30, 2017. 2
[10] Barna Saha and Sanjay Subramanian. Correlation clustering with same-cluster queries bounded
by optimal cost. In 27th Annual European Symposium on Algorithms (ESA 2019) . Schloss-
Dagstuhl-Leibniz Zentrum für Informatik, 2019. 2
[11] Alberto Del Pia, Mingchen Ma, and Christos Tzamos. Clustering with queries under semi-
random noise. In Conference on Learning Theory , pages 5278–5313. PMLR, 2022. 2
[12] Marco Bressan, Nicolò Cesa-Bianchi, Silvio Lattanzi, and Andrea Paudice. Exact recovery
of mangled clusters with same-cluster queries. Advances in Neural Information Processing
Systems , 33:9324–9334, 2020. 2
[13] Wasim Huleihel, Arya Mazumdar, Muriel Médard, and Soumyabrata Pal. Same-cluster querying
for overlapping clusters. Advances in Neural Information Processing Systems , 32, 2019. 2
[14] Ramya Korlakai Vinayak and Babak Hassibi. Crowdsourced clustering: Querying edges vs
triangles. Advances in Neural Information Processing Systems , 29, 2016. 2
[15] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine learning ,
56(1):89–113, 2004. 2
[16] Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information:
Ranking and clustering. Journal of the ACM , 55(5):1–27, 2008. 2
[17] Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, and Grigory Yaroslavtsev. Near
optimal LP rounding algorithm for correlation clustering on complete and complete k-partite
graphs. In Proceedings of the 47th Annual ACM Symposium on Theory of Computing (STOC) ,
pages 219–228, 2015. 2
11[18] Nairen Cao, Vincent Cohen-Addad, Euiwoong Lee, Shi Li, Alantha Newman, and Lukas V ogl.
Understanding the cluster lp for correlation clustering. In Proceedings of the 56th Annual ACM
Symposium on Theory of Computing (STOC) , 2024. 2
[19] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Predicting positive and negative links
in online social networks. In Proceedings of the 19th international conference on World wide
web, pages 641–650, 2010. 2
[20] Emmanuel Abbe. Community detection and stochastic block models: recent developments.
Journal of Machine Learning Research , 18(177):1–86, 2018. 2
[21] Chandra Sekhar Mukherjee, Pan Peng, and Jiapeng Zhang. Recovering unbalanced communities
in the stochastic block model with application to clustering with a faulty oracle. Advances in
Neural Information Processing Systems , 36, 2024. 2
[22] Quanquan Gu and Jiawei Han. Towards active learning on graphs: An error bound minimization
approach. In 2012 IEEE 12th International Conference on Data Mining , pages 882–887. IEEE,
2012. 2
[23] Martin Aigner. Combinatorial search. John Wiley & Sons, Inc. , 1988. 2
[24] Dingzhu Du and Frank K Hwang. Combinatorial group testing and its applications. World
Scientific , 12, 2000. 2
[25] Dingzhu Du, Frank K Hwang, and Frank Hwang. Combinatorial group testing and its applica-
tions , volume 12. World Scientific, 2000. 2
[26] F. Hwang and V . Sós. Non-adaptive hypergeometric group testing. Studia Sci. Math. Hungar ,
1987. 2, 19
[27] Arya Mazumdar. Nonadaptive group testing with random set of defectives. IEEE Transactions
on Information Theory , 62(12):7522–7531, 2016. 2
[28] Ely Porat and Amir Rothschild. Explicit non-adaptive combinatorial group testing schemes. In
Automata, Languages and Programming, 35th International Colloquium, ICALP 2008 , Lecture
Notes in Computer Science, 2008. 2, 19
[29] Bernt Lindström. On a combinatory detection problem i. A Magyar Tudományos Akadémia
Matematikai Kutató Intézetének Közleményei , 9(1-2):195–207, 1964. 2
[30] Bernt Lindström. On a combinatory detection problem. ii. Studia Sci. Math. Hungar , 1:353–361,
1966. 2
[31] David G Cantor and WH Mills. Determination of a subset from certain combinatorial properties.
Canadian Journal of Mathematics , 18:42–48, 1966. 2
[32] Nader H Bshouty. Optimal algorithms for the coin weighing problem with a spring scale. In
COLT , volume 2009, page 82, 2009. 2
[33] Nader H. Bshouty and Hanna Mazzawi. On parity check (0, 1)-matrix over zp. InProceedings,
ACM-SIAM Symposium on Discrete Algorithms (SODA) , 2011. 2
[34] Nader H. Bshouty and Hanna Mazzawi. Algorithms for the coin weighing problems with the
presence of noise. Electron. Colloquium Comput. Complex. , TR11-124, 2011. 2
[35] Dana Angluin and Jiang Chen. Learning a hidden graph using o(log n) queries per edge. 2004.
2
[36] Sung-Soon Choi and Jeong Han Kim. Optimal query complexity bounds for finding graphs. In
Proceedings of the fortieth annual ACM symposium on Theory of computing , pages 749–758,
2008. 2
[37] Hanna Mazzawi. Optimally reconstructing weighted graphs using queries. In Proceedings
of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms , pages 608–615.
SIAM, 2010. 2
12[38] Sung-Soon Choi. Polynomial time optimal query algorithms for finding graphs with arbitrary
real weights. In Conference on Learning Theory , pages 797–818. PMLR, 2013. 2
[39] Aviad Rubinstein, Tselil Schramm, and S Matthew Weinberg. Computing exact minimum cuts
without knowing the graph. arXiv preprint arXiv:1711.03165 , 2017. 2
[40] Sagnik Mukhopadhyay and Danupon Nanongkai. Weighted min-cut: sequential, cut-query, and
streaming algorithms. In Proceedings, ACM Symposium on Theory of Computing (STOC) , 2020.
2
[41] Sepehr Assadi, Deeparnab Chakrabarty, and Sanjeev Khanna. Graph connectivity and single
element recovery via linear and or queries. In 29th Annual European Symposium on Algorithms
(ESA 2021) . Schloss-Dagstuhl-Leibniz Zentrum für Informatik, 2021. 2
[42] Simon Apers, Yuval Efron, Pawel Gawrychowski, Troy Lee, Sagnik Mukhopadhyay, and
Danupon Nanongkai. Cut query algorithms with star contraction. In Proceedings, IEEE
Symposium on Foundations of Computer Science (FOCS) , 2022. 2
[43] Hang Liao and Deeparnab Chakrabarty. Learning spanning forests optimally in weighted
undirected graphs with cut queries. In International Conference on Algorithmic Learning
Theory , pages 785–807. PMLR, 2024. 2
[44] Paul Beame, Sariel Har-Peled, Sivaramakrishnan Natarajan Ramamoorthy, Cyrus Rashtchian,
and Makrand Sinha. Edge estimation with independent set oracles. ACM Trans. Algorithms ,
16(4):52:1–52:27, 2020. 2
[45] Xi Chen, Amit Levi, and Erik Waingarten. Nearly optimal edge estimation with independent set
queries. In Shuchi Chawla, editor, Proceedings, ACM-SIAM Symposium on Discrete Algorithms
(SODA) , 2020. 2
[46] Raghavendra Addanki, Andrew McGregor, and Cameron Musco. Non-adaptive edge counting
and sampling via bipartite independent set queries. In Shiri Chechik, Gonzalo Navarro, Eva
Rotenberg, and Grzegorz Herman, editors, 30th Annual European Symposium on Algorithms,
ESA 2022, September 5-9, 2022, Berlin/Potsdam, Germany , volume 244 of LIPIcs , pages
2:1–2:16. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2022. 2
[47] Troy Lee, Miklos Santha, and Shengyu Zhang. Quantum algorithms for graph problems with
cut queries. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA) ,
pages 939–958. SIAM, 2021. 2
[48] Sepehr Assadi, Deeparnab Chakrabarty, and Sanjeev Khanna. Graph connectivity and single
element recovery via linear and OR queries. In Petra Mutzel, Rasmus Pagh, and Grzegorz
Herman, editors, 29th Annual European Symposium on Algorithms, ESA 2021, September 6-8,
2021, Lisbon, Portugal (Virtual Conference) , volume 204 of LIPIcs , pages 7:1–7:19. Schloss
Dagstuhl - Leibniz-Zentrum für Informatik, 2021. 19
[49] Vladimir Grebinski and Gregory Kucherov. Optimal reconstruction of graphs under the additive
model. Algorithmica , 28(1):104–124, 2000. 24
13A Bounded Query Size and Sample-Based Algorithms
In this section we present an algorithm using subset queries with size bounded by s, which matches
the lower bound of Theorem C.1, up to a logn-factor. Our algorithm has the additional desirable
property of being sample-based , meaning that the subsets it queries are formed by taking uniform
independent samples. In addition to Theorem A.1, we also obtain a non-adaptive sample-based
algorithm using O(nklogn)unbounded queries in Theorem G.1, using a similar approach. We also
show a lower bound of Ω(nlogn)for any k≥2in Appendix C.2 for sample-based algorithms,
showing that the dependence on nis optimal for this special class of algorithms.
Theorem A.1. There are non-adaptive, sample-based k-clustering algorithms making (a)
O(nklognlog log n)subset queries of size at most O(√n), and (b) O(n2
s2klogn)subset queries of
size at most s=n1/2−δfor any constant δ∈(0,1/2). Each algorithm is correct with probability at
least99/100.
For convenience, we will parameterize the query-size bound by s=n1/rwhere ris any positive real
number in the range 2≤r≤logn. Before proving the theorem formally, we informally describe the
algorithm and its analysis. A full description of the algorithm is given in pseudocode in Alg. 3, which
is split into two phases: a "query selection phase", describing how queries are chosen by the algorithm,
and a "reconstruction phase", describing how the algorithm uses the query responses to determine
the clustering. Both phases contain a for-loop iterating over all p∈ {0,1, . . . , logrlogn−1}
where the goal of the algorithm during the p’th iteration is to learn all remaining clusters of size at
leastn
k·2−rp+1. We prove that this occurs with high probability in Lemma 2.3, which gives the
main analysis. If each iteration is successful in doing so than the entire clustering has been learned
successfully after iteration p= logrlogn−1(since 2−rlogrlogn= 2−logn=1
n), and we justify this
formally just after the statement of Lemma A.2.
We describe the algorithm and it’s analysis informally for the case of r= 2, i.e. when the query sizes
are bounded by s=√n. We also refer the reader to Section 2 for discussion of the ideas for the
simple case of k= 3. Consider some iteration p∈ {0,1, . . . , log log n−1}and suppose that prior
to this iteration, all clusters of size at leastn
k·2−2phave been successfully recovered. Let Cpdenote
the collection of all such clusters and let Rp=F
C∈CpCbe the set of points they contain. The goal
in iteration pis to learn every cluster Cwith|C| ∈[n
k·2−2p+1,n
k·2−2p). The algorithm queries
O(nklogn)random sets Tformed by 22psamples3from U(see lines 5-7 of Alg. 3). Similar to the
proof of Theorem 2.2, if Tcontains exactly two points x, y∈T\ Rpbelonging to unrecovered
clusters, then we can use the fact that we already know the clustering on Rpto tell whether or not
x, ybelong to the same cluster or not, i.e. we can compute q({x, y})∈ {1,2}from q(T). We then
consider the set of all such pairs where q({x, y}) = 1 (this is Q′′
pdefined in line 16) and consider the
graph Gwith this edge set, and vertex set U\ Rp, the set of points whose cluster hasn’t yet been
determined. If two points belong to the same connected component in this graph, then they belong to
the same cluster. Thus, the analysis boils down to showing that with high probability, the induced
subgraph G[C]will be connected for every Cwhere|C| ∈[n
k·2−2p+1,n
k·2p). This is accomplished
by applying a basic fact from the theory of random graphs, namely Fact 2.4.
Proof of Theorem A.1: The following Lemma A.2 establishes that after the first piterations of the
algorithm’s query selection and reconstruction phases, all clusters of size at leastn
k·2−rp+1have
been learned with high probability. This is the main effort of the proof. After stating the lemma we
show it easily implies that Alg. 3 succeeds with probability at least 99/100by an appropriate union
bound.
Lemma A.2. For each p= 0,1, . . . , logrlogn−1, letEpdenote the event that all clusters of size at
leastn
k·2−rp+1have been successfully recovered immediately following iteration pof Alg. 3. Then,
Pr[¬E0]≤1
100kand Pr[¬Ep| Ep−1]≤1
100kfor all p∈ {1,2, . . . , logrlogn−1}.
Before proving Lemma A.2, we observe that it immediately implies Theorem A.1 as follows. Let
I0= [n
k·2−r, n]and for 1≤p <logrlogn, letIp= [n
k·2−rp+1,n
k·2−rp). If there are no clusters
3Note that p≤log log n−1and so 22p≤21
2logn=√n.
14Algorithm 3: Sample-Based Algorithm Using Bounded Queries
1Input: Subset query access to a hidden partition C1⊔ ··· ⊔ Ck=Uof|U|=npoints;
2(Query Selection Phase)
3forp= 0,1, . . . , logrlogn−1do
4 Initialize query set Qp← ∅;
5 Repeat 20·nkln(300 nk2)·2rp+1(1−2
r)times;
6−→Sample T⊆Uformed by 2rpindependent uniform samples from U;
7−→Query Tand add it to Qp;
8end
9(Reconstruction Phase)
10Initialize learned cluster set C0← ∅;
11forp= 0,1, . . . , logrlogn−1do
12 LetCpdenote the collection of clusters reconstructed before iteration p;
13 LetRp=S
C∈CpCdenote the points belonging to these clusters;
14 Initialize Cp+1← C p;
15 LetQ′
p={T\ Rp:T∈Qpand|T\ Rp|= 2}. Since each T∈Qpis a uniform random
set, the elements of Q′
pare uniform random pairs in U\ Rp;
16 LetQ′′
p={{x, y} ∈Q′
p:q({x, y}= 1)}denote the set of pairs in Q′
pwhere both points lie
in the same cluster. This set can be computed since q(T\ Rp) =q(T)−q(T∩ Rp)and
q(T∩ Rp)is known since at this point we have reconstructed the clustering on Rp;
17 LetGpdenote the graph with vertex set U\ Rpand edge set Q′′
p;
18 LetC1, . . . , C ℓdenote the connected components of Gpwith size at leastn
k·2−rp+1;
19 AddC1, . . . , C ℓtoCp+1;
20end
21Output clustering Clogrlogn;
Cfor which |C| ∈Ip, then trivially Pr[¬Ep| Ep−1] = 0 , and otherwise Pr[¬Ep| Ep−1]≤1
100kby
the lemma. Since there are kclusters, clearly there are at most kvalues of pfor which there exists a
cluster with size in the interval Ip. Using this observation and a union bound, we have
Pr[¬Elogrlogn−1]≤Pr[¬E0] +logrlognX
p=1Pr[¬Ep| Ep−1]≤1
100
which completes the proof of correctness since the algorithm succeeds iff Elogrlogn−1occurs.
Query complexity: Note that the total number of queries made is O(nklogn)·Plogrlogn
p=12rp(1−2
r).
When r= 2, the summation evaluates to log log nwhich establishes the query complexity in item (a)
of Theorem A.1.
Otherwise, let r= 2 + Cfor some constant C >0. We will argue that 2rp(1−2
r)≤1
22rp+1(1−2
r)for
anyp≤logrlogn−1greater than some constant and thus the summation is bounded as
logrlognX
p=12rp(1−2
r)=O(2rlogrlogn(1−2
r)) =O(n1−2
r) =O(n/s2)
establishing the query complexity in item (b) of Theorem A.1. Observe that 2rp(1−2
r)≤1
22rp+1(1−2
r)
is equivalent to rp(1−2
r)≤rp+1(1−2
r)−1, or equivalently
rp−1≥1
(r−1)(r−2)=1
C(1 +C)
which clearly holds as long as p−1>log1
Csince r >2.
Time complexity: We assume that sampling a uniform random element from a set of size ncan be
done in O(1)time. Thus any set that is sampled during the course of the algorithm can be constructed
15inO(s)time. No matter the value of s, the number of queries made by the algorithm is dominated
byO(n2
s2klognlog log n). Thus, the runtime of the query selection phase (lines 3-7) is bounded by
O(n2
sklognlog log n). Now for the reconstruction phase. In line (15), |T\ Rp|can be computed in
O(n)time and so lines (15-16) take time O(|Qp| ·n). Line (18) amounts to finding every connected
component in Gpwhich can be done in time O(|Q′′
p|+n) =O(|Qp|+n)by iteratively running
a BFS (costing time linear in the number of edges plus the number of vertices). Thus, the runtime
of the p’th iteration of the for-loop is always dominated by O(|Qp| ·n). Since the total number of
queries is dominated by O(n2
s2klognlog log n), the total runtime of the reconstruction phase (lines
11-19) is O(n3
s2klognlog log n), which dominates the runtime of the query selection phase.
We now prove the main Lemma A.2.
Proof. of Lemma A.2. LetRpdenote the set of points belonging to a cluster which has been recovered
before iteration p.
Case 1: p= 0. In this iteration, the algorithm queries |Q0| ≥8·nkln(300 nk2)·2r−2random pairs
and we need to show that it successfully recovers all clusters with size at leastn
k·2rwith probability at
least1−1
100k. LetCdenote any such cluster and recall from lines (16-17) the definition of the graph
G0with vertex set Uand edge set Q′′
0. We will show that the induced subgraph G0[C]is connected,
and thus Cis correctly recovered in lines (18-19), with probability at least 1−1
100k2. Since there are
at most kclusters, the lemma holds by a union bound.
Consider any two vertices x, y∈Cand note that |Q0| ≥2n2ln(300 nk2)
|C|since|C| ≥n
k·2r. We lower
bound the probability that (x, y)is an edge in G0[C]as follows. Note that this occurs iff {x, y} ∈Q0.
Thus,
Pr
Q0[(x, y)∈E(G0[C])] = Pr
Q0[{x, y} ∈Q0] = 1−
1−1
n2|Q0|
≥1−exp
−2 ln(300 nk2)
|C|
≥1−exp
−2 ln(300 k2|C|)
|C|
= 1−1
300k2|C|2
|C|
(1)
and so by Fact 2.4, G0[C]is connected with probability at least 1−1
100k2as claimed.
Case 2: 1≤p <logrlogn. Recall from lines (12-13) that Cpdenotes the set of clusters recovered
prior to iteration pandRp=S
C∈CpCis the set of points belonging to these clusters. Note that we
are conditioning on the event that every cluster of size at leastn
k·2−rphas been recovered prior to
iteration p. LetCdenote some cluster with size
|C| ∈hn
k·2−rp+1,n
k·2−rp
and note that |U\ Rp| ≤k·n
k·2−rp=n·2−rp.
Recall from lines (16-17) the definition of Q′′
pand that Gpis the graph with vertex set U\ Rpand
edge set Q′′
p. We need to argue that the induced subgraph Gp[C]is connected, and thus Cis correctly
recovered in lines (18-19), with probability at least 1−1
100k2. Since there are at most kclusters, a
union bound completes the proof of the lemma.
Consider any two vertices x, y∈C. We lower bound the probability that (x, y)is an edge in Gp[C],
which occurs iff there is some T∈Qpwhere T\ Rp={x, y}. We have
Pr
T:|T|=2rp[T\ Rp={x, y}] =2rp
2
·1
n2·|Rp|
nt−2
≥22rp
3n2
1−2−rpt
≥22rp
10n2
16and since |Qp|= 20nkln(300 nk2)·2rp+1(1−2
r), we have
Pr
Qp[(x, y)∈E(Gp[C])] = Pr
Qp
{x, y} ∈Q′′
p
= Pr
Qp[∃T∈Qp:T\ Rp={x, y}]
≥1−
1−22rp
10n220·nk·2rp+1−2rpln(300 nk2)
≥1−exp 
−2·2rp+1kln(300 nk2)
n!
and plugging in |C| ≥n
k·2−rp+1and|C| ≤ninto the RHS yields
Pr
Qp[(x, y)∈E(Gp[C])]≥1−exp
−2 ln(300 nk2)
|C|
≥1−exp
−2 ln(300 k2|C|)
|C|
= 1−1
300k2|C|2
|C|
.
Therefore, (x, y)is an edge in Gp[C]with probability at least 1−
1
300k2|C|2
|C|, which by Fact 2.4
implies that Gp[C]is connected with probability at least 1−1
100k2as claimed.
B The Special Case of Balanced Clusters
Given B≥1, we say that a k-partition C1, . . . , C kisB-balanced ifn
Bk≤ |Ci| ≤Bn
kfor all i∈[k].
In this section we prove the following theorem, which gives a non-adaptive algorithm for recovering
a roughly balanced k-clustering making O(nlogk)subset queries when k=O(n
log3n). We give an
alternative algorithm making O(nlog2k)queries for arbitrary kin Appendix E. We also described a
two-round algorithm for this setting making O(nlog log k)queries in Appendix F.2.
Theorem B.1. There is a non-adaptive algorithm that recovers a B-balanced k-clustering using
O(B2nlogk)+O(Bklog4k)subset queries of size O(klogk)and succeeds with probability 49/50.
Pseudocode for the algorithm is given in Alg. 4. In line (3) we draw s= Θ( B2logk)setsT1, . . . , T s
each formed by k/B samples from Uand in line (5) learn the clustering over their union using
Theorem 2.5. I.e., for T=T1∪ ··· ∪ Ts, we find Rj=T∩Cj. Then, we query TiandTi∪ {x}for
every x∈Uand every i∈[s]in line (5). Now, consider some point x∈Uand let j∗be it’s cluster’s
index. Note that q(Ti∪ {x}) =q(Ti)iffTiintersects Cj∗. Thus, if Tidoes not intersect Cj∗, then
every cluster jthatTiintersects can be ruled out as a candidate for being the cluster containing x.
The set Jxcomputed in line (8) is the set of all jwhich can be ruled out in this way. If for every
j̸=j∗, there is some Ticontaining j, but not j∗, then Jx={j∗}and we determine j∗in line (9).
This occurs for every x∈Uif the following holds: for every pair (j, j′)∈ U
2
, there exists Ti
intersecting Cj, but not Cj′. We show in Claim B.2 that this happens with high probability.
Proof of Theorem B.1 There are O(B2nlogk)queries made in line (5) and O(B·klog4k)queries
in line (4), since |S
i∈[s]Ti|=O(Bklogk).
Time complexity: We assume the attaining a uniform sample from any set can be done O(1)time.
Thus, constructing sets T1, . . . , T sin line (3) costs O(Bklnk)time and by Theorem 2.5 line (4) costs
O(k2B2ln4k). Line (5) costs O(n·s) =O(B2nlnk)time. Constructing Jxin line (8) amounts to
checking if q(Ti∪ {x})̸=q(Ti)and if Ti∩Rj̸=∅for each i∈[s]andj∈[k]. This can be done
in time O(|Ti| · |Rj|) =O(k2lnk)simply using |Rj| ≤ |R|=O(Bklnk)and|Ti|=k/B. Thus,
the total runtime of lines (7-14) is dominated by O(nk2lnk).
Correctness: We now prove correctness, which is due to the following claim.
Claim B.2. Fori∈[s], j∈[k], letEi,jdenote the event that Ti∩Cj̸=∅. Then,
Pr
T1,...,T s
∀(j, j′)∈[k]
2
,∃i∈[s]:Ei,j∧ ¬E i,j′
≥99
100. (2)
17Algorithm 4: Algorithm for the B-Balanced Case
1Input: Subset query access to a B-balanced partition C1⊔ ··· ⊔ Ck=Uof|U|=npoints;
2(Query Selection Phase)
3Choose s= 2eB2ln(100 k2)setsT1, . . . , T seach formed byk
Buniform samples from U;
4Run the algorithm from Theorem 2.5 to learn the clustering restricted on R=Ss
i=1Ti. Let
R1, . . . , R kbe the output of the algorithm. I.e., if the algorithm is successful, then
Rj=R∩Cjfor all j∈[k];
5Query TiandTi∪ {x}for all i∈[s]and all x∈U;
6(Reconstruction Phase)
7forx∈Udo
8 LetJx=S
i∈[s]:q(Ti∪{x})̸=q(Ti){j∈[k]:Ti∩Rj̸=∅}. Note that Ti∩Rj̸=∅iff
Ti∩Cj̸=∅. Note that q(Ti∪ {x})̸=q(Ti)iffxdoes not belong to any cluster that is hit
byTi. Thus, Jxis the collection of all jsuch that some set Tihas revealed that x /∈Cj;
9 if|Jx|=k−1then
10 AddxtoRj∗where j∗is the unique element of [k]\Jx;
11 else
12 Output fail;
13 end
14end
15Output clustering (R1, . . . , R k);
Proof. Firstly, for fixed i∈[s]andj̸=j′, since each cluster’s size is bounded in the interval
[n
Bk,Bn
k], we have
Pr
Ti[Ei,j∧ ¬E i,j′] = Pr[ Ei,j]·Pr[¬Ei,j′| Ei,j]
= 
1−
1−|Cj|
n|Ti|!
·
1−|Cj′|
n|Ti|−1
≥ 
1−
1−1
Bkk/B!
·
1−B
kk/B
≥ 
1−exp 
B−2
·1
e≥1
2eB2
and so for a fixed (j, j′)∈ [k]
2
, we have
Pr
T1,...,T s[∀i∈[s]:¬(Ei,j∧ ¬E i,j′)]≤
1−1
2eB22eB2ln(100 k2)
≤1
100k2
and the claim follows by a union bound over all (j, j′)∈ [k]
2
.
By Claim B.2, with probability at least 99/100, for every j̸=j′∈[k]we have some Tisuch that
Ti∩Cj̸=∅andTi∩Cj′=∅. In particular, for x∈U, letCj∗be the cluster containing x. For
every j̸=j∗we have some Tisuch that Ti∩Cj̸=∅andTi∩Cj∗=∅which means that in line (9)
of the algorithm, we have Jx= [k]\ {j∗}and so we successfully identify the cluster containing x.
Moreover, this occurs for all x. Finally, line (4) succeeds with probability 99/100and thus the entire
algorithm succeeds with probability at least 49/50by a union bound.
C Lower Bounds
C.1 An Ω(n2
s2)Lower Bound for Non-adaptive 3-Partition Recovery
Theorem C.1. Non-adaptive 3-clustering requires Ω(n2)pair queries.
Proof. For every (x, y)∈ U
2
consider the following pair of partitions:
P1
x,y= ({x, y},∅, U\ {x, y})andP2
x,y= ({x},{y}, U\ {x, y}).
18Observe that the oracle returns the same value for P1
x,yandP2
x,yon every possible query except on
the set {x, y}. Thus, if query set Q⊆U×Udistinguishes these two clusterings, then Q∋ {x, y}.
Therefore, the number of pairs {x, y}such that Qdistinguishes P1
x,yandP2
x,yis at most |Q|. Now, let
Abe any non-adaptive pair-query algorithm which successfully recovers an arbitrary 3-clustering with
probability ≥2/3. The algorithm Aqueries a random set Q⊆U×Uaccording to some distribution,
DA. In particular, for every {x, y} ∈ U
2
,Qdistinguishes P1
x,yandP2
x,ywith probability ≥2/3.
Thus,
2
3n
2
≤X
{x,y}∈(U
2)Pr
Q∼DA[Qdistinguishes P1
x,yandP2
x,y]
=EQ∼DA
{x, y} ∈U
2
:Qdistinguishes P1
x,yandP2
x,y
≤ |Q|
using linearity of expectation, and this completes the proof.
Corollary C.2. Non-adaptive 3-clustering requires Ω(n2/s2)subset queries of size at most s.
Proof. This follows from Theorem C.1 since one s-sized query can be simulated by s
2
pair-
queries.
Thus, in order to achieve a near-linear non-adaptive upper bound for 3-clustering, we require an
algorithm which makes queries of size eΩ(√n).
C.2 An Ω(nlogn)Lower Bound for Sample-Based 2-Partition Recovery
Theorem C.3. Sample-based 2-clustering requires Ω(nlogn)subset queries.
Proof. Let|U|=nbe even and let A, B⊆Ube two disjoint sets of size |A|=|B|=n/2. Let
P= (A, B)and for any x∈UletPxdenote the partition obtained by switching the set that x
belongs to. We show that it requires Ω(nlogn)sample-based subset queries to distinguish Pfrom
Pxfor all x. For x∈UandT⊆U, letEx,Tdenote the event that querying Tdistinguishes Pfrom
Px. Note that Exoccurs iff x∈TandT\x⊆AorT\x⊆B. Thus, for a random set Tof size
s≥2, we have
Pr
T:|T|=sh
Ex,Ti
=s·1
n·2·n/2
ns−1
=s
n·1
2s−2
≤2
n(3)
since the second-to-last quantity is clearly maximized when s= 2. Now, let Qbe a collection of sets,
each of which consists of some of number of independent uniform samples. Note that the cardinality
of these sets can differ from one another. Note that Qdistinguishes Pfrom every PxiffEx,Toccurs
for every xand some T. By eq. (3) and a standard coupon-collector argument, if |Q|=o(nlogn),
then with high probability there will be some xfor which ¬W
T∈QEx,Toccurs.
D Useful Lemmas
D.1 Vector Support Recovery from ORQueries
Given x∈ {0,1}n, letsupp(x) ={i:xi= 1}denote the support of x. An OR-query on set S⊆[n]
returns
ORS(x) =_
i∈Sxi=1(supp(x)∩S̸=∅).
This section discusses the problem of recovering the support of a vector via ORqueries. In particular,
we are interested in non-adaptive algorithms for this problem. The results in this section are standard
in the combinatorial group testing and coin-weighing literature. See e.g. [ 26,28] and also [ 48], who
applied these results to obtain query algorithms for graph connectivity.
Lemma D.1. Letx∈ {0,1}nsuch that |supp(x)|= 1. There is a deterministic, non-adaptive
algorithm that makes ⌈logn⌉ORqueries and returns supp(x). The runtime is also O(logn).
19Proof. Since|supp(x)|= 1, anORquery on set Sis equivalent to taking ⟨x, v⟩where vi= 1
iffi∈S. Let Mbe the ⌈logn⌉ ×nmatrix whose i’th column is simply bi∈ {0,1}⌈logn⌉, the
binary representation of i. The rows of Mcorrespond to ORqueries. Then, Mx=Pn
i=1xibi=P
i:xi=1bi=bjwhere jis the unique coordinate where xj= 1.
Lemma D.2. Letx∈ {0,1}n. There is a deterministic, non-adaptive algorithm SER1bit that makes
2⌈logn⌉ORqueries and certifies whether |supp(x)|= 0,|supp(x)|= 1, or|supp(x)|>1. If
|supp(x)|= 1, then it outputs supp(x). The runtime is also O(logn).
Proof. LetMbe the ⌈logn⌉ ×nmatrix described in the proof of Lemma D.1. Let 1= 1⌈logn⌉×n
denote the all 1’s matrix with the same dimensions. We query M·xand(1−M)·xwhere here (·)
denotes the " ORproduct". I.e. the i’th coordinate of M·xis1((Mx)i>0). Note that 1−Mis
obtained by flipping every bit in M. Note that if |supp(x)|= 1, then M·xis guaranteed to return
the unique coordinate where xhas a one, as in the proof of Lemma D.1. Thus, it suffices to show that
we can use these queries to determine whether |supp(x)|is0,1, or strictly greater than 1.
First,|supp(x)|= 0iff(M·x)1= 0and((1−M)·x)1= 0since the sets of 1-coordinates in the
first row of Mand1−Mpartition [n].
Next, we claim that |supp(x)|>1iff there exists some i∈[⌈logn⌉]such that (M·x)i= 1 and
((1−M)·x)i= 1. Note that for every row i, the1-coordinates in the i’th row of Mand1−M
partition [n]. Thus, clearly if (M·x)i= 1and((1−M)·x)i= 1, then there are at least 2coordinates
where xhas a one. Now we prove the converse. Suppose there exists i̸=j∈[n]where xi=xj= 1.
Letbi, bj∈ {0,1}⌈logn⌉denote the binary representations of i, jrespectively. Since i̸=j, there
exists some bit kwhere bi
k̸=bj
k. Without loss of generality let bi
k= 1andbj
k= 0. Then,
(M·x)k=1  nX
ℓ=1xℓbℓ!
k>0!
=1 nX
ℓ:xℓ=1bℓ
k>0!
= 1,
((1−M)·x)k=1  nX
ℓ=1xℓ(⃗1−bℓ)!
k>0!
=1 nX
ℓ:xℓ=1(1−bℓ
k)>0!
= 1
and this completes the proof.
Next, we describe a randomized non-adaptive algorithm for recovering the entire support of x.
Lemma D.3. Letx∈ {0,1}n. There is a non-adaptive algorithm that makes O(tlogn
δ)ORqueries
on subsets of size ⌈n
t⌉, and if |supp(x)| ≤t,returns supp(x)with probability 1−δ, and otherwise
certifies that |supp(x)|> t. The algorithm’s runtime is O(nlogn
δ).
Proof. For brevity, we assume that tdivides n. Let m=e·tlnn
δ. We make ORqueries on sets
S1, . . . , S m, each formed by taking n/ti.i.d. uniform samples from [n]and define
X= [n]\[
ℓ∈[m]:ORSℓ(x)=0Sℓ. (4)
If|X|> t, we certify |supp(x)|> tand if|X| ≤t, then we output X.
Assuming a uniform sample from [n]can be obtained in O(1)time, the runtime of the algorithm is
O(m·n
t) =O(nlnn
δ).
Suppose that |supp(x)|> t. Observe that supp(x)⊆Xand so |X|> twith probability 1. Thus, the
algorithm is always correct in this case.
Now suppose |supp(x)| ≤t. We argue that X=supp(x)with probability at least 1−δ. Consider
some i /∈supp(x). Note that i /∈Xiff there is some query Sℓ∋ifor which Sℓ∩supp(x) =∅. Let
Ei,ℓdenote the event that i∈SℓandSℓ∩supp(x) =∅. Then, since |supp(x)| ≤t, we have
Pr[Ei,ℓ] =n
t·1
n·
1−|supp(x)|
nn
t−1
≥1
t
1−t
nn
t
≥1
et
20and so
Pr[i∈X] = Pr
¬Ei,ℓfor all ℓ∈[m]
≤
1−1
etm
≤δ
N
since m=e·tlnN
δ. Thus, by a union bound, we have Pr[X̸=supp(x)]≤δ.
Finally, we make the following simple observation regarding algorithms that are restricted to making
ORqueries on subsets of bounded size.
Observation D.4. A single ORquery on a set Scan be simulated by|S|
squeries of size at most s.
Combining this observation with Lemma D.3 gives the following lemma.
Lemma D.5. Letx∈ {0,1}nands, t≥1be positive integers where s≤n
t. There is a non-adaptive
algorithm that makes O(n
slogn
δ)ORqueries on subsets of size s, and if |supp(x)| ≤t,returns
supp(x)with probability 1−δ, and otherwise certifies that |supp(x)|> t. The algorithm runs in
timeO(nlogn
δ).
D.2 Connectivity of Erdös-Rényi Random Graphs
Our proofs in Section 2.1, Appendix A, and Appendix G make use of the following bound on the
probability of a random graph being connected. For intuition, note that for sufficiently large n,
1−(δ/3n)2/n≈1−exp(−2 ln(3 n/δ)
n)≈ln(3n/δ)
n.
Thus, Fact D.6 asserts that for sufficiently large na random graph containing ≫nlnnedges is
connected with high probability, which may be a more familiar statement to the reader. However,
we need such a bound to be true even for very small nand so we give the following more broadly
applicable version.
Fact D.6. LetG(n, p)denote an Erdös-Rényi random graph. If p≥1−(δ/3n)2/n, then G(n, p)is
connected with probability at least 1−δ.
Proof. A graph G= (V, E)is connected if and only if for every cut S⊂V, there exists an edge
(u, v)∈E∩(S×S). When Gis drawn from G(n, p), this does not occur for a cut Sof size |S|=t
with probability exactly (1−p)t(n−t). There are exactly n
t
such cuts. Thus, taking a union bound
over all cuts and using our lower bound on p, we have
Pr
G∼G(n,p)[Gnot connected ]≤n−1X
t=1n
tδ
3n2
n·t(n−t)
≤2⌊n/2⌋X
t=1n
tδ
3n2
n·t(n−t)
≤2⌊n/2⌋X
t=1n
tδ
3n2
n·tn
2
≤2⌊n/2⌋X
t=1ntδ
3nt
= 2⌊n/2⌋X
t=1(δ/3)t≤δ
and this completes the proof.
E An O(nlog2k)Algorithm for the Balanced Case
In Appendix B, we gave an algorithm for k-clustering making O(nlogk+klog4k)subset queries
when the cluster sizes are balanced within any constant factor. This query complexity simplifies to
O(nlogk)as long as k=O(n
log3n). In this section we give an alternative algorithm which is more
efficient when k≫n
log3n.
Theorem E.1. There is a non-adaptive algorithm for recovering a B-balanced k-clustering using
O(B2nlog2k)subset queries of size O(k)which succeeds with probability 99/100.
21Proof. Recall that for a vector v∈ {0,1}n, anORquery on a set S⊆[n]returns ORS(v) =W
i∈Svi.
We will use the following lemma for recovering supp(v) ={i:vi= 1}viaORqueries. We prove
and discuss this lemma in Appendix D.1 (see Lemma D.2).
Lemma E.2. There is a deterministic, non-adaptive algorithm that takes an arbitrary v∈ {0,1}n,
makes 2⌈logn⌉ORqueries, and certifies whether |supp(v)|= 0,|supp(v)|= 1, or|supp(v)|>1.
If|supp(v)|= 1, then it outputs supp(v). The runtime is O(logn).
Given x∈U={x1, . . . , x n}, letC(x)denote the cluster containing it. Let v(x)∈ {0,1}n
denote the Boolean vector with v(x)
i=1(xi∈C(x)). As in Section 2.2, we have ORS(x) =
1(q(S∪ {x}) =q(S)). I.e. ORqueries to v(x)are simluted by two subset queries to the clustering.
This implies the following corollary.
Corollary E.3. Given a k-clustering on Uof size nand an element x∈U, letC(x)denote the
cluster containing x. There is a deterministic non-adaptive algorithm which takes as input xand a
setR⊆U, makes O(log|R|)subset queries, and if |R∩C(x)|= 1, then the algorithm returns the
unique z∈R∩C(x), and otherwise certifies that |R∩C(x)| ̸= 1. The runtime is O(log|R|).
The pseudocode for the algorithm is given in Alg. 5. In words, Corollary E.3 says that if we have
a setRcontaining exactly one representative from C(x), then with O(log|R|)subset queries we
can identify that representative. Thus, suppose we have a collection of sets R1, . . . , R ssuch that for
every cluster j∈[k], there is some Ricontaining a unique representative from Cj. Consider the
bipartite graph where on the left we have Uand on the right we have R1∪ ··· ∪ Rs. Then, for every
x∈Uand every Riwe can run the procedure from Corollary E.3, and if it returns a representative
y∈Ri∩C(x), then we add the edge (x, y)to this graph. By the property of R1, . . . , R s, two
vertices x, y∈Ubelong to the same cluster iff they are connected by a path of length 2in this graph.
We show that setting s= Θ( B2logk)and letting each Ribe a random sample of k/B elements
from Uresults in a collection of sets with this good property with high probability. This leads to a
query complexity of n·s·O(logk) =O(nlog2k).
Algorithm 5: Second Algorithm for the B-Balanced Case
1Input: Subset query access to a B-balanced partition C1⊔ ··· ⊔ Ck=Uof|U|=npoints;
2Choose s=eB2ln(100 k)setsR1, . . . , R seach formed byk
Buniform samples from U;
3Construct a bipartite graph G(U,Ss
j=1Rj, E)as follows;
4forx∈Uandi∈[s]do
5 Run the algorithm from Corollary E.3 on input xandRi;
6 ifthe algorithm certifies there is a unique y∈Risuch that x, yare in the same cluster then
7 Add the edge (x, y)toE(G);
8 end
9end
10LetC1, . . . , C ℓdenote the connected components of G;
11Output the clustering (C1, . . . , C ℓ);
Query complexity and time complexity: The algorithm makes n·s·O(logk
B) =O(B2nlog2k)
queries. We assume that a uniform random sample can be obtained in O(1)time. Thus, line (2) runs
inO(Bklnk)time. By Corollary E.3, line (5) runs in time O(|Ri|) =O(logk). Thus, the entire
for-loop (lines 4-9) runs in time O(nslogk) =O(B2nlog2k). The bipartite graph Ghas at most
O(n+Bklogk)vertices and at most O(ns) =O(B2nlogk)edges. Thus, line (10) can be executed
in time O(B2nlogk)time. The total runtime is thus dominated by O(B2nlog2k).
The correctness of the algorithm now follows immediately from the following claim.
Claim E.4. With probability at least 99/100, for every j∈[k], there exists i∈[s]such that
|Ri∩Cj|= 1.
22Proof. Fixj∈[k]andi∈[s]. We have
Pr[|Ri∩Cj|= 1] = |Ri| ·|Cj|
n·
1−|Cj|
n|Ri|−1
≥k
B·1
Bk·
1−B
kk/B
≥1
eB2
and so for a fixed j∈[k],
Pr[∀i∈[s]:|Ri∩Cj| ̸= 1]≤
1−1
eB2eB2ln(100 k)
≤1
100k
and so by a union bound
Pr[∃j∈[k],∀i∈[s]:|Ri∩Cj| ̸= 1]≤1
100
and this completes the proof.
F Two-Round Algorithms
In this section we describe two algorithms that use two rounds of adaptivity. That is, these algorithms
are allowed to specify a round of queries, receive the responses, perform some computation, then
specify a second round of queries and receive the responses, before finally recovering the clustering.
We give a simple deterministic algorithm making O(nlogk)queries in Appendix F.1 and a random-
ized algorithm for recovering a balanced clustering with O(nlog log k)queries in Appendix F.2.
Both algorithms exploit the additional round of queries to first compute a set containing exactly one
representative from every cluster.
F.1 A Two Round O(nlogk)Deterministic Algorithm using Single Element Recovery
Theorem F.1. There is a two-round, non-adaptive, deterministic algorithm for k-clustering using
O(nlogk)subset queries.
Algorithm 6: Deterministic 2-Round Algorithm
1Input: Subset query access to a hidden partition C1, . . . , C kofU={x1, . . . , x n};
2Round 1:
3Query Pt={xi:i≤t}for every t∈[n];
4Define R={xt:q(Pt)−q(Pt−1) = 1}containing exactly one point from every cluster;
5For each y∈R, define cluster Ry={y};
6Round 2:
7forx∈Udo
8 Use the O(logk)deterministic non-adaptive algorithm of Corollary F.2 to find the unique
y∈Rfor which x, ylie in the same cluster;
9 Place xintoRy;
10end
11Output clustering (Ry:y∈R);
Proof. Pseudocode for the algorithm is given in Alg. 6. The runtime is clearly dominated by the
for-loop (lines 7-9) which run in time O(nlogk)by Corollary E.3. Fix an arbitrary ordering U=
{x1, . . . , x n}. The first round of queries (lines 3-5) is used to compute a set R⊆Ucontaining exactly
one representative from every cluster. This is done by querying every prefix Pt={x1, . . . , x t}
and observing that q(Pt)−q(Pt−1) = 1 iffxtis the only representative for its cluster in Pt. Thus,
the set Rcomputed in line (4) contains, for each cluster C, the first member of Cin the ordering
x1, . . . , x n. In particular, it contains exactly one representative from every cluster. The second round
of queries is used to determine, for every x∈U, the unique representative of C(x)inR(see line 8).
To accomplish this we recall Corollary E.3 from Appendix E which we restate below. This completes
the proof.
Corollary F.2. Given a k-clustering on Uof size nand an element x∈U, letC(x)denote the
cluster containing x. There is a deterministic non-adaptive algorithm which takes as input xand a
setR⊆U, makes O(log|R|)subset queries, and if |R∩C(x)|= 1, then the algorithm returns the
unique z∈R∩C(x), and otherwise certifies that |R∩C(x)| ̸= 1.
23F.2 A Two Round O(nlog log k)Algorithm for Balanced Clusters
Recall that a clustering C1⊔ ··· ⊔ Ck=UisB-balanced ifn
Bk≤ |Cj| ≤Bn
k.
Theorem F.3. There is a two round, non-adaptive algorithm which recovers a B-balanced k-
clustering using O(√
B·nlog log k)subset queries.
Proof. We will use the following result of [ 49] on query-based reconstruction of bipartite graphs
as a black-box. Given a bipartite graph G(V, W, E ), an edge-count query on (S, T)where S⊆V,
T⊆Wreturns |E∩S×T|, the number of edges between SandT.
Lemma F.4 ([49], see Section 4.3) .There is a non-adaptive algorithm which reconstructs any
bipartite graph G(V, W, E )where (a) |V|=n, (b)|W|=m, and (c) every vertex in Vhas degree
at most 1, using O(n·logn
logm)edge-count queries.
We will say a set A⊆Uis an independent set if each element of Abelongs to a distinct cluster.
Given two independent sets A, B letM(A, B)be the matching where there is an edge from x∈A
toy∈Bifx, ybelong to the same cluster. We observe that edge-count queries in M(A, B)can be
simulated by subset queries, leading to the following corollary.
Corollary F.5. Suppose that A, B⊆Uare independent sets. There is a deterministic, non-adaptive
algorithm which reconstructs M(A, B)using O(|A| ·log|A|
log|B|)subset queries.
Proof. We need to show that an edge-count query (S, T)where S⊆A,B⊆Tcan be simulated by
a constant number of subset queries. Let m(S, T)denote the number of edges in M(A, B)between
SandT. Since A, B are independent sets, S, T are also independent sets, and so we have
m(S, T) =q(S) +q(T)−q(S∪T)
since m(S, T)is the number of clusters intersected by both SandT. Thus, one edge-count query to
M(A, B)can be simulated by three subset queries and this completes the proof.
Pseudocode for the algorithm is given in Alg. 7. The algorithm is parameterized in terms of a value
τ >1which we will choose later in the proof so as to minimize the query complexity. The first
round is used to accomplish the following. In lines (4-5) we construct a set Rcontaining exactly
one representative from every cluster and use this to define an initial clustering. In line (6) we
sample random sets I1, . . . , I sand in line (8) make a query to each to check whether or not it is an
independent set. Line (10) defines Vwhich is the union of all the Ii’s which are independent sets.
We now describe the second round. In line (14) we run the procedure of Corollary F.5 to construct
the matching M(Ii, R)whenever Iiis an independent set. Finally, we determine for every x∈U,
the unique y∈Rfor which x, ybelong to the same cluster. If x∈Vthis is done in lines (18-20) by
taking x’s neighbor in M(Ii, R)for some independent set Ii. Ifx /∈V, this is done in lines (23-24)
by running the procedure of Corollary F.2.
The algorithm always either outputs fail in line (11), or correctly reconstructs the clustering by
Corollary F.5 and Corollary F.2. Thus we only need to argue that |U\V| ≤n
τoccurs with probability
at least 99/100allowing it to pass the check in line (11), and that conditioned on this, the algorithm
makes O(nln lnk)queries when we set τappropriately. Let us first count the number of queries
conditioned on this event. Line (8) performs squeries. Since each Iiis of size√
kand|R|=k,
by Corollary F.5, lines (13-14) perform a total of O(s·√
klnτ) =O(√
B·nlnτ)queries. Lines
(22-23) use |U\V|O(logk) =O(n
τlogk)queries. Setting τ= Θ(ln k)yields a query complexity
ofO(√
Bnlog log k). We now prove in Claim F.6 that the required bound on |U\V|holds with
high probability, and this completes the proof.
Claim F.6. With probability at least 99/100, we have |U\V| ≤n
τ.
Proof. We prove an appropriate bound on E[|U\V|]and then apply Markov’s inequality. Fix x∈U.
Fori∈[s], letEx,idenote the event that x∈IiandIiis an independent set. Observe that x∈U\V
iffEx,idoes not occur for every i∈[s]. We first lower bound the probability of Ex,i. Observe that
Pr
Ii[Ex,i] = Pr[ x∈Ii] Pr[Iian independent set |x∈Ii] (5)
24Algorithm 7: Two Round Algorithm for Balanced Clustering
1Input: Subset query access to a hidden partition C1⊔ ··· ⊔ Ck=Uof|U|=npoints;
2Round 1:
3Query Pt={xi:i≤t}for every t∈[n];
4Define R={xt:q(Pt)−q(Pt−1) = 1}containing exactly one point from every cluster;
5For each y∈R, define cluster Ry={y};
6Sample s= 10q
B
k·nln(100 τ)setsI1, . . . , I s⊂Ueach formed byq
k
10Bsamples from U;
7fori∈[s]do
8 Query Ii. (This is to check if q(Ii) =|Ii|, i.e. whether Iiis an independent set.);
9end
10LetV=S
i∈[s]:q(Ii)=|Ii|Iibe the points in Ulying in an independent set among I1, . . . , I s;
11If|V|< n(1−1
τ), then output fail . Otherwise, continue;
12Round 2:
13fori∈s:q(Ii) =|Ii|do
14 Run the algorithm from Corollary F.5 on sets Ii, Rand let Mi⊂Ii×Rbe the output;
15end
16forx∈Udo
17 ifx∈Vthen
18 Choose Iisuch that x∈IiandIiis an independent set;
19 Lety∈Rdenote the neighbor of xin the matching Mi⊂Ii×R;
20 Place xintoRy;
21 end
22 ifx∈U\Vthen
23 Use the O(logk)deterministic non-adaptive algorithm of Corollary F.2 to find the unique
y∈Rfor which x, ylie in the same cluster;
24 Place xintoRy;
25 end
26end
27Output clustering (Ry:y∈R);
and
Pr
Ii[x∈Ii] = 1−
1−1
n|Ii|
≥1−exp
−|Ii|
n
≥|Ii|
2n≥r
k
B·1
8n
where we have used the inequality exp(−z)≤1−z
2forz∈[0,1]. Next, by a simple union bound
over all pairs in Iiand the fact that every cluster is bounded as |Cj| ≤Bn
k, we have
Pr[Iinot an independent set |x∈Ii]≤ |Ii|2B
k≤1
10.
Plugging these bounds back into Equation (5) yields PrIi[Ex,i]≥q
k
B·1
10nand noting that these
events are independent due to the Ii’s being independent yields
Pr[x /∈V] = Pr[ ¬Ex,i,∀i∈[s]]≤ 
1−r
k
B·1
10n!s
= exp( −ln(100 τ)) =1
100τ(6)
where we have used the definition of s= 10p
B/k·nln(100 τ). Finally, this implies E[|U\V|]≤
n
100τand so by Markov’s inequality Pr[|U\V|>n
τ]<1
100. This completes the proof.
G Sample-Based Algorithm using Unbounded Queries
Theorem G.1. There is a non-adaptive, sample-based k-clustering algorithm making O(nklogn)
subset queries which is correct with probability at least 99/100.
25Proof. The algorithm is defined in Alg. 8. The proof techniques are quite similar to that of Theo-
rems 2.2 and A.1 detailed in Section 2.1 and appendix A. We also refer the reader to Section 2 for a
discussion on the main ideas.
Algorithm 8: Sample-Based Algorithm Using Unbounded Queries
1Input: Subset query access to a hidden partition C1⊔ ··· ⊔ Ck=Uof|U|=npoints;
2(Query Selection Phase)
3forp= 0,1, . . . , logndo
4 Initialize query set Qp← ∅;
5 Repeat40nkln(300 nk2)
2p times;
6−→Sample T⊆Uformed by 2pindependent uniform sample from U;
7−→Query Tand add it Qp;
8end
9(Reconstruction Phase)
10Initialize hypothesis clustering C0← ∅;
11forp= 0,1, . . . , logndo
12 LetCpdenote the collection of clusters reconstructed before phase p;
13 LetRp=S
C∈CpCdenote the points belonging to these clusters;
14 Initialize Cp+1← C p;
15 LetQ′
p={T\ Rp:T∈Qpand|T\ Rp|= 2}. Since each T∈Qpis a uniform random
set, the elements of Q′
pare uniform random pairs in U\ Rp;
16 LetQ′′
p={{x, y} ∈Q′
p:q({x, y}= 1)}denote the set of pairs in Q′
pwhere both points lie
in the same cluster. This set can be computed since q(T\ Rp) =q(T)−q(T∩ Rp)and
q(T∩ Rp)is known since at this point we have reconstructed the clustering on Rp;
17 LetGpdenote the graph with vertex set U\ Rpand edge set Q′′
p;
18 LetC1, . . . , C ℓdenote the connected components of Gpwith size at leastn
2k·2p;
19 AddC1, . . . , C ℓtoCp+1;
20end
21Output clustering Clogn+1
SincePlogn
p=01
2p=O(1), the number of queries made by the algorithm is O(nklogn). To prove
correctness it suffices to prove the following lemma.
Lemma G.2. For each p= 0,1, . . . , logn, letEpdenote the event that all clusters of size at least
n
2k·2phave been successfully recovered immediately following iteration pof Alg. 8. Then,
Pr[¬E0]≤1
100kand Pr[¬Ep| Ep−1]≤1
100kfor all p∈ {1,2. . . ,logn}.
The proof that Lemma G.2 implies Theorem G.1 is identical to the proof that Lemma 2.3 implies
Theorem 2.2 given just after the statement of Lemma 2.3. Thus, we move on to proving Lemma G.2.
Proof. of Lemma G.2. First consider the case of p= 0. In this iteration, the algorithm queries
|Q0| ≥40·nkln(300 nk2)random pairs and we need to show that it successfully recovers all clusters
with size at leastn
2kwith probability at least 1−1
100k. LetCdenote any such cluster and recall from
lines (16-17) the definition of the graph G0with vertex set Uand edge set Q′′
0. We will show that
the induced subgraph G0[C]is connected, and thus Cis correctly recovered in lines (18-19), with
probability at least 1−1
100k2. Since there are at most kclusters, the lemma holds by a union bound.
Consider any two vertices x, y∈Cand note that |Q0| ≥20n2ln(300 nk2)
|C|since|C| ≥n
2k. We lower
bound the probability that (x, y)is an edge in G0[C]as follows. Note that this occurs iff {x, y} ∈Q0.
Using an identical calculation to that of eq. (1), this probability is at least 1−(1
300k2|C|)2/|C|, implying
thatG0[C]is connected with probability at least 1−1
100k2by Fact 2.4.
The argument for the case of p >0is identical to the argument given in "Case 3" of in the proof of
Lemma 2.3 in Section 2.
26H An O(nlogk)Adaptive algorithm
Here we sketch a simple adaptive algorithm using O(nlogk)queries. Suppose, we have identified
one element from iclusters (initially i= 0, and we have i≤kalways). Suppose they are
X={x1, x2, ..., x i}. We now want to find the cluster to which a new point ybelongs to. We first
query{X, y}. If the answer is i+ 1, then yis part of a new cluster and igrows to i+ 1. Otherwise,
yis part of the iclusters, and we detect the cluster to which ybelongs to using a binary search. We
consider the two sets X1={x1, x2, .., x⌈i/2⌉}, andX2={x⌈i/2⌉+1, .., x i}. We then query {X1, y}.
If the answer is ⌈i/2⌉+ 1, then we search recursively in X2, else if the query answer is ⌈i/2⌉, then
we search recursively in X1. Clearly, the query complexity is O(logk)per item, and it requires
O(logk)rounds of adaptivity even to place one element.
27NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: All of our claims made in the abstract and theorems stated in the introduction
are proved formally in the paper.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: All of our results are theoretical and are expressed in the form of theorem
statements, in which any assumptions are explicitly stated.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
28Answer: [Yes]
Justification: All of our results are theoretical and the paper contains a complete formal
proof of every result.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: Our paper does not include any experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
29Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: Our paper does not include any experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: Our paper does not include any experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: Our paper does not include any experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
30• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: Our paper does not include any experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Our results are all purely theoretical and did not require the use of any data-sets
or human subjects and don’t pose any potential violation of the code of ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Since all of our results are theoretical and pertain to a specific model for the
very broadly applicable problem of clustering, it is difficult to meaningfully discuss the
specific societal impact of our work.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
31•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
32•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
33