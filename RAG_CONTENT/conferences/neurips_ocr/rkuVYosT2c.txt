Distributed Least Squares in Small Space via
Sketching and Bias Reduction
Sachin Garg
Computer Science & Engineering
University of Michigan
sachg@umich.eduKevin Tan
Department of Statistics
University of Pennsylvania
kevtan@umich.edu
Michał Derezi ´nski
Computer Science & Engineering
University of Michigan
derezin@umich.edu
Abstract
Matrix sketching is a powerful tool for reducing the size of large data matrices. Yet
there are fundamental limitations to this size reduction when we want to recover an
accurate estimator for a task such as least square regression. We show that these
limitations can be circumvented in the distributed setting by designing sketching
methods that minimize the bias of the estimator, rather than its error. In particular,
we give a sparse sketching method running in optimal space and current matrix
multiplication time, which recovers a nearly-unbiased least squares estimator using
two passes over the data. This leads to new communication-efficient distributed
averaging algorithms for least squares and related tasks, which directly improve on
several prior approaches. Our key novelty is a new bias analysis for sketched least
squares, giving a sharp characterization of its dependence on the sketch sparsity.
The techniques include new higher-moment restricted Bai-Silverstein inequalities,
which are of independent interest to the non-asymptotic analysis of deterministic
equivalents for random matrices that arise from sketching.
1 Introduction
Matrix sketching is a powerful collection of randomized techniques for compressing large data
matrices, developed over a long line of works as part of Randomized Numerical Linear Algebra
[RandNLA, e.g., 45,26,37,39,21]. Sketching can be used to reduce the large dimension nof a data
matrix A∈Rn×dby applying a random sketching matrix (operator) S∈Rm×nto obtain the sketch
˜A=SA∈Rm×dwhere m≪n. For example, sketching can be used to approximate the solution
to the least squares problem, x∗= argminxL(x)where L(x) =∥Ax−b∥2, by using a sketched
estimator ˜x= argminx∥˜Ax−˜b∥2, where ˜A=SAand˜b=Sb.
Perhaps the simplest form of sketching is subsampling, where the sketching operator Sselects a
random sample of the rows of matrix A. However, the real advantage of sketching as a framework
emerges as we consider more complex operators S, such as sub-Gaussian matrices [ 1], randomized
Hadamard transforms [ 3], and sparse random matrices [ 12]. These approaches ensure higher quality
and more robust compression of the data matrix, e.g., leading to provable ϵ-approximation guarantees
for the estimate ˜xin the least squares task, i.e., L(˜x)≤(1 + ϵ)L(x∗). Nevertheless, there are
fundamental limitations to how far we can compress a data matrix using sketching while ensuring
anϵ-approximation. These limitations pose a challenge particularly in space-limited computing
38th Conference on Neural Information Processing Systems (NeurIPS 2024).A Subsample ˜O(d/ϵ)×d
˜O(1/ϵ)rows
˜O(1/ϵ)rows
...
˜O(1/ϵ)rowsSketch O(d)×d
...
Figure 1: Illustration of the leverage score sparsification algorithm used in Theorem 1. Each row of
the sketch mixes ˜O(1/ϵ)leverage score samples from A. Remarkably, the ϵ-error guarantee of the
subsampled estimator is retained as ϵ-bias of the sketched estimator.
environments, such as for streaming algorithms where we observe the matrix A, say, one row at a
time, and we have limited space for storing the sketch [11].
One strategy for overcoming the fundamental limitations of sketching as a compression tool is to look
beyond the single approximation guarantee provided by a sketching-based estimator ˜x, and consider
how its broader statistical properties can be leveraged in a given computing environment. To that
end, many recent works have demonstrated both theoretically and empirically that sketching-based
estimators often exhibit not only approximation robustness but also statistical robustness, for instance
enjoying sharp confidence intervals, effectiveness of statistical inference tools such as bootstrap
and cross-validation, as well as accuracy boosting techniques such as distributed averaging [e.g.,
35,25,32,33]. Yet, these results have had limited impact on the traditional computational complexity
analysis in RandNLA and sketching literature, as many of them either impose additional assumptions,
focus on sharpening the constant factors, or require using more expensive sketching techniques. In
this work, we demonstrate that the statistical properties of sketching-based estimators can in fact have
a substantial impact on the computational trade-offs that arise in RandNLA.
Our key motivating example is the above mentioned least squares regression task. It is well understood
that for an n×dleast squares task, to recover an ϵ-approximate solution out of an m×dsketch, we
need sketch size at least m= Ω(d/ϵ). This has been formalized in the streaming setting with a lower
bound of Ω(ϵ−1d2log(nd))bits of space required, when all of the input numbers use O(log(nd))bits
of precision [ 11]. One setting where this can be circumvented is in the distributed computing model
where the bits can be spread out across many machines, so that the per-machine space can be smaller.
Here, one could for instance hope that we can maintain small O(d)×dsketches in q=O(1/ϵ)
machines and then combine their estimates to recover an ϵ-approximate solution. A simple and
attractive approach is to average the estimates ˜xiproduced by the individual machines, returning
ˆx=1
qPq
i=1˜xi, as this only requires each machine to communicate O(dlog(nd))bits of information
about its sketch. This approach requires the sketching-based estimates ˜xito have sufficiently small
bias for the averaging scheme to be effective. While this has been demonstrated empirically in
many cases, existing theoretical results still require relatively expensive sketching methods to recover
low-bias estimators, leading to an unfortunate trade-off in the distributed averaging scheme between
the time and space complexity required.
In this work, we address the time-space trade-off in distributed averaging of sketching-based estima-
tors, by giving a sharp characterization of how their bias depends on the sparsity of the sketching
matrix. Remarkably, we show that in the distributed streaming environment one can compress the
data down to the minimum size of O(d2log(nd))bits at no extra computational cost, while still being
able to recover an ϵ-approximate solution for least squares and related problems. Importantly, our
results require the sketching matrix to be slightly denser than is necessary for obtaining approximation
guarantees on a single estimate, and thus, cannot be recovered by standard RandNLA sampling
methods such as approximate leverage score sampling [27].
Before we state the full result in the distributed setting, we give our main technical contribution.
which is the following efficient construction of a low-bias least squares estimator in a single pass
using only O(d2log(nd))bits of space, assuming all numbers use O(log(nd))bits of precision.
Below, γ >0denotes an arbitrarily small constant.
2Reference Method Total runtime Parallel passes
Folklore Gaussian sketch O(ndω−1) 1
[43] Leverage Score Sampling nnz(A) +˜O(dω/√ϵ) 1
[5] Determinantal Point Process nnz(A) +˜O(dω) log3(n/ϵ)
[9] Weighted Mb-SGD (sequential) nnz(A) +˜O(d2/ϵ) 1/ϵ
This work (Thm. 1) Leverage Score Sparsification nnz(A) +˜O(d2/ϵ) 1
Table 1: Comparison of time complexities and parallel passes over the data required for differ-
ent methods to obtain a (1 + ϵ)-approximation in O(d2log(nd))bits of space for an n×dleast
squares problem (A,b), given a preconditioner Psuch that κ(AP) =O(1)(see Section 3 for our
computational model). We include the fully sequential Weighted Mb-SGD as a reference.
Theorem 1. Given streaming access to A∈Rn×dandb∈Rn, and direct access to a pre-
conditioner matrix P∈Rd×dsuch that κ(AP)≤α, within a single pass over (A,b), in
O(γ−1nnz(A) +ϵ−1αd2+γpolylog( d))time and O(d2log(nd))bits of space, we can construct a
randomized estimator ˜xfor the least squares solution x∗= argminx∥Ax−b∥2such that:
(Bias)AE[˜x]−b2≤(1 +ϵ)∥Ax∗−b∥2,
(Variance) E
∥A˜x−b∥2
≤2∥Ax∗−b∥2.
Remark 1. The above construction assumes access to a preconditioner matrix Pwithκ(AP)≤α
(where κdenotes the condition number). Such matrix can be obtained efficiently with α=O(1)in a
separate single pass, leading to a two-pass algorithm described later in Theorem 2.
Our estimator ˜xis constructed at the end of the data pass from a sketch (˜A,˜b)where ˜A=SAand
˜b=Sb, by minimizing ∥˜Ax−˜b∥2using preconditioned conjugate gradient. Here, Sis a carefully
constructed sparse sketching matrix which is inspired by the so-called leverage score sparsified
(LESS) embeddings [ 18]. Leverage scores represent the relative importances of the rows of Awhich
are commonly used for subsampling in least squares (see Definition 1), and their estimates can be
easily obtained in a single pass by using the preconditioner matrix P.
Our time complexity bound of ˜O(nnz(A) +d2/ϵ)matches the time it would take (for a single
machine) to subsample ˜O(d/ϵ)rows of Aaccording to the approximate leverage scores and produce
an estimator ˜xthat achieves the ϵ-error bound ∥A˜x−b∥2≤(1 +ϵ)∥Ax∗−b∥2. However, this
strategy requires either maintaining ˜O(d2/ϵ)bits of space for the sketch, or computing ˜xdirectly
along the way, blowing up the runtime to ˜O(dω/ϵ). Since approximate leverage score sampling leads
to significant least squares bias, averaging can only improve this to ˜O(dω/√ϵ)(see Table 1). An
alternate strategy would be to combine leverage score sampling with a preconditioned mini-batch
stochastic gradient descent (Weighted Mb-SGD), with mini-batches chosen so that they fit in ˜O(d2)
space. This achieves the same time and space complexity as our method, but due to the streaming
access to Aand the sequential nature of SGD, it requires O(1/ϵ)data passes.
Instead, our algorithm essentially mixes an ˜O(d/ϵ)size leverage score sample into an O(d)size
sketch, merging ˜O(1/ϵ)rows of Ainto a single row of the sketch (see Figure 1). This results
in better data compression compared to direct leverage score sampling, with only ˜O(d2)bits of
space, while retaining the same ˜O(nnz(A) +d2/ϵ)runtime complexity as the above approaches
and requiring only a single data pass. The resulting estimator ˜xcan no longer recover the ϵ-error
bound, but remarkably, its expectation E[˜x]still does. To turn this into an improved estimator in a
distributed model, we can simply average q= 1/ϵsuch estimators, i.e., ˆx=1
qPq
i=1˜xi, obtaining
E∥Aˆx−b∥2≤(1 + 2 ϵ)∥Ax∗−b∥2. As shown in Table 1, ours is the first result in this model to
achieve ˜O(d2)space in a single pass and faster than current matrix multiplication time O(dω).
Finally, by incorporating a preconditioning scheme, we illustrate how our construction can be used
to design the first algorithm that solves least squares in current matrix multiplication time, constant
parallel passes and O(d2log(nd))bits of space. We note that the O(dω)cost comes only from
the worst-case complexity of constructing the preconditioner P, which can often be accelerated in
practice. The computational model used in Theorem 2 is described in detail in Section 3.
3Theorem 2. Given A∈Rn×dandb∈Rnin the parallel computing model, using two parallel
passes with qmachines, we can compute ˜xsuch that with probability 0.9
∥A˜x−b∥ ≤
1 +ϵ+O(1/q)
∥Ax∗−b∥
inO(γ−1nnz(A) +dω+ϵ−1d2+γpolylog( d))time,O(d2log(nd))bits of space and O(dlog(nd))
bits of communication. In particular, choosing q= 1/ϵwe recover an O(ϵ)-approximation.
Remark 2. While the parallel computing model in Theorem 2 assumes that all machines have
streaming access to the entire data matrix, this result can be easily extended to the setting where
Ahas been randomly down-sampled or partitioned into separate size nchunks A1, ...,Aq, and
each machine constructs an estimate ˜xibased on a sketch of its own chunk. Then, with the same
computational guarantees as in Theorem 2, the averaged estimator ˜x=1
qPq
i=1˜xiwith probability
0.9enjoys a guarantee of:
∥A˜x−b∥ ≤
1 +O(ϵ+ 1/q+Bchunk)
∥Ax∗−b∥,
where Bchunk is the bias that would be incurred if we solved each chunk exactly and averaged those
solutions. Using existing guarantees for uniform down-sampling of least squares [Theorem 20, 43],
one can bound this bias as Bchunk =˜O µ
qn+ (µ
n)2
, where µis the coherence of the data matrix
andnis the chunk size. For sufficiently large chunks, this bias is negligible compared to the error ϵ.
We prove the above high probability guarantee in Theorem 6 in Appendix E.
Further applications. Our least squares analysis can be extended to other settings where prior
works [e.g., 17,18,16,22] have analyzed randomized estimators based on sparse sketching via
techniques from asymptotic random matrix theory. The primary and most direct application involves
correcting inversion bias in the so-called sketched inverse covariance estimate (˜A⊤˜A)−1, which was
the motivating task of [ 18], with applications including distributed second-order optimization and
statistical uncertainty quantification, where quantities like (˜A⊤˜A)−1xare approximated.
Theorem 3 (informal Theorem 5) .Given A∈Rn×dand its LESS embedding Swith sketch size
m≥Cdandsnon-zeros per row, the inverse covariance sketch (m
m−dA⊤S⊤SA)−1is an (ϵ, δ)-
unbiased estimator of (A⊤A)−1(see Definition 3) for ϵ=˜O 
(1 +p
d/s)√
d
m
andδ= 1/poly( d).
This result should be compared with ϵ=˜O 
(1 +d/s)√
d
m
obtained by [ 18]. Thus, we get a direct
improvement for very sparse sketches, i.e., s=o(d). This can be immediately translated into an
improved local convergence guarantee for Distributed Newton Sketch which is a second-order convex
minimization algorithm used in settings where the Hessian matrix can be expressed as A⊤Afor a
tall matrix A, e.g., in generalized linear models like logistic regression. In this method, following
Corollary 16 of [ 18] as well as related results [ 16,44,19], we use sketching and averaging to estimate
a Newton step:
xt+1=xt−1
qqX
i=1˜H−1
igt,
where gtis the gradient at xtand˜H1, ...,˜Hqare Hessian sketches constructed by independent
machines. The following corollary, which is a direct improvement over Corollary 16 of [18], shows
that Distributed Newton Sketch on a generalized linear model task can achieve a fast local convergence
rate of the formf(xt+1)−f(x∗)
f(xt)−f(x∗)=o(1)withO(dω)time per iteration (see Appendix B.1 for details).
Corollary 1 (Distributed Newton Sketch) .Consider f(x) =1
nPn
i=1ℓi(x⊤ϕi) +λ
2∥x∥2, where ℓi
are convex twice continuously differentiable functions, such that fhas a Lipschitz Hessian, λ >0,
andϕ⊤
iis the ith row of an n×ddata matrix Φ. Given ϵ >0, there is a neighborhood Uϵaround
the minimizer x∗= argminxf(x)such that, for any xt∈Uϵ, using two parallel passes with ˜O(1/ϵ)
machines, we can compute a Distributed Newton Sketch update xt+1such that
f(xt+1)−f(x∗)≤ϵ·
f(xt)−f(x∗)
,
inO(γ−1nnz(Φ) +dω+ϵ−1d2+γpolylog( d))time,O(d2log(nd))bits of space and O(dlog(nd))
bits of communication.
4Our Techniques. At the core of our analysis are techniques inspired by asymptotic random matrix
theory (RMT) in the proportional limit [e.g., see 6]. Here, in order to establish the limiting spectral
distribution (such as the Marchenko-Pastur law) of a random matrix ˜A⊤˜Awhose dimensions diverge
to infinity, one aims to show the convergence of the Stieltjes transform of its resolvent matrix
(˜A⊤˜A−zI)−1. Recently, [ 18] showed that these techniques can be adapted to sparse sketching
matrices (via leverage score sparsification) in order to characterize the bias of the sketched inverse
covariance (˜A⊤˜A)−1, where ˜A=SA.
Our main contribution is two-fold. First, we show that a similar argument can also be applied to
analyze the bias of the least squares estimator, ˜x= (˜A⊤˜A)−1˜A⊤˜b. Unlike the inverse covariance,
this estimator no longer takes the form of a resolvent matrix, but its bias is also associated with
the inverse, which means that we can use a leave-one-out argument to characterize the effect of
removing a single row of the sketch on the estimation bias. Our second main contribution is to
improve the sharpness of the bounds relative to the sparsity of the sketching matrix by combining
a careful application of Hölder’s inequality with a higher moments analysis of the restricted Bai-
Silverstein inequality for quadratic forms. Those improvements are not only applicable to the least
squares analysis, but also to all existing RMT-style results for LESS embeddings, including the
aforementioned inverse covariance estimation, as well as applications in stochastic optimization,
resulting in the sketching cost of LESS embeddings dropping below matrix multiplication time.
2 Related Work
Randomized numerical linear algebra. RandNLA sketching techniques have been developed
over a long line of works, starting from fast least squares approximations of [ 42]; for an overview,
see [ 45,26,37,39,21] among others. Since then, these methods have been used in designing fast
algorithms not only for least squares but also many other fundamental problems in numerical linear
algebra and optimization including low-rank approximation [ 13,34],lpregression [ 14], solving
linear systems [ 28,24] and more. Using sparse random matrices for matrix sketching also has a long
history, including data-oblivious sketching methods such as CountSketch [ 12], OSNAP [ 40], and
more [ 38]. Leverage score sparsification (LESS) was introduced by [ 18] as a data-dependent sparse
sketching method to enable RMT-style analysis for sketching (see below).
Unbiased estimators for least squares. To put our results in a proper context, let us consider
other approaches for producing near-unbiased estimators for least squares, see also Table 1. First, a
well known folklore result states that the least squares estimator computed from a dense Gaussian
sketching matrix is unbiased. The bias of other sketching methods, including leverage score sampling
and OSNAP, has been studied by [ 43], showing that these methods need a√ϵ-error guarantee to
achieve an ϵ-bias which leads to little improvement unless ϵis extremely small and the sketch size
is sufficiently large. Another approach of constructing unbiased estimators for least squares, first
proposed by [ 23], is based on subsampling with a non-i.i.d. importance sampling distribution based
on Determinantal Point Processes [DPPs, 31,20]. However, despite significant efforts [ 30,7,5],
sampling from DPPs remains quite expensive: the fastest known algorithm requires running a Markov
chain for polylog( n/ϵ)many steps, each of which requires a separate data pass and takes O(dω)time.
Other approaches have also been considered which provide partial bias reduction for i.i.d. RandNLA
subsampling schemes in various regimes that are are either much more expensive or not directly
comparable to ours [2, 44].
Statistical and RMT analysis of sketching. Recently, there has been significant interest in statisti-
cal and random matrix theory (RMT) analysis of matrix sketching; see [ 21] for an overview. These
approaches include both asymptotic analysis via limiting spectral distributions and deterministic
equivalents [ 35,25,32,33], and non-asymptotic analysis under statistical assumptions [ 36,41,4].
A number of works have shown that the RMT-style techniques based on deterministic equivalents
can be made rigorously non-asymptotic for certain sketching methods such as dense sub-Gaussian
[17], LESS matrices [ 16,18], and other sparse matrices [ 9], which has been applied to low-rank
approximation, fast subspace embeddings and stochastic optimization. Our new analysis can be
viewed as a general strategy for directly improving the sparsity required by LESS embeddings (and
thereby, the sketching time complexity) in many of these applications, specifically those that rely on
analysis inspired by the calculus of deterministic equivalents via generalized Stieltjes transforms.
53 Preliminaries
Notations. In all our results, we use lowercase letters to denote scalars, lowercase boldface for
vectors, and uppercase boldface for matrices. The norm ∥ · ∥ denotes the spectral norm for matrices
and the Euclidean norm for vectors, whereas ∥ · ∥Fdenotes the Frobenius norm for matrices. We use
⪯to denote the p.s.d. ordering of matrices.
Computational model. We next clarify the computational model that is used in Theorem 2. We
consider a central data server storing (A,b), and qmachines. The jth machine has a handle
Stream( j), which can be used to open a stream and to read the next row/label pair (ai, bi)in the
stream. After a full pass, the machine can re-open the handle and begin another pass over the data.
The machines can operate their streams entirely asynchronously, and each has its own limited local
storage space, e.g., in Theorem 2 we use O(d2log(nd))bits of space per machine. At the end, they
can communicate some information back to the server, e.g., in Theorem 2, they communicate their
final estimate vectors ˜xj, using O(dlog(nd))bits of communication. Then, the server computes the
final estimate, in our case via averaging, ˜x=1
qPq
i=1˜xi, which can be done either directly or via a
map-reduce type architecture.
We define the parallel passes required by such an algorithm as the maximum number of times the
stream is opened by any single machine. We analogously define time/space/communication costs by
taking a maximum over the costs required by any single machine (for communication, this refers
only to the number of bits sent from the machine back to the server).
Definitions and useful lemmas. In our framework, we construct a sparse sketching matrix S
where sparsification is achieved using a probability distribution over rows of data matrix A, that is
proportional to the leverage scores of A. The next definition [following, e.g., 9] provides the explicit
definition of exact and approximate leverage scores for our setting.
Definition 1 ((β1, β2)-approximate leverage scores) .Fix a matrix A∈Rn×dand consider matrix
U∈Rn×dwith orthonormal columns spanning the column space of A. Then, the leverage scores
li,1≤i≤nare defined as the row norms squared of U, i.e.,li=∥ui∥2, where u⊤
iis the ith row of
U. Furthermore, consider fixed β1, β2>1. Then ˜liare called (β1, β2)-approximate leverage scores
forAif the following holds for all i
li
β1≤˜liandnX
i=1˜li≤β2·d.
The approximate leverage scores can be computed by first constructing a preconditioner matrix
P∈Rd×dsuch that κ(AP) =O(1), which takes O(nnz(A)+dω)in a single pass, and then relying
on the following norm approximation scheme.
Lemma 1 (Based on Lemma 7.2 from [ 10]).Given A∈Rn×dandP∈Rd×d, using a single pass
overAin time O(γ−1(nnz(A) +d2))for small constant γ >0, we can compute estimates ˜l1, ...,˜ln
such that with probability ≥0.95:
n−γ∥e⊤
iAP∥2≤˜li≤O(log(n))∥e⊤
iAP∥2∀i andX
i˜li≤O(1)· ∥AP∥2
F.
In the next definition, we give the sparse sketching strategy used in our analysis. This approach is
similar to the original leverage score sparsification proposed by [ 18], except: 1) we adapted it so that
it can be implemented effectively in a single pass, and 2) we use it in a much sparser regime (fewer
non-zeros per row).
Definition 2 ((s, β1, β2)-LESS embedding) .Fix a matrix A∈Rn×dand some s≥0. Let the tuple
(˜l1,···,˜ln)denote (β1, β2)-approximate leverage scores for A. Letpi= min {1,sβ1˜li
d}. We define
a(s, β1, β2)-approximate leverage score sparsifier ξas follows.
ξ=b1√p1,···,bn√pn
where bi∼Bernoulli( pi).
Moreover, we define the (s, β1, β2)-leverage score sparsified (LESS) embedding of size mas matrix
S∈Rm×dwith i.i.d. rows1√mxisuch that xi= diag( ξi)yiwhere ξidenotes a randomly generated
(β1, β2)-approximate leverage score sparsifier and yi∈Rnconsist of random ±1entries.
6A key property of a sketching matrix is the subspace embedding property, defined below. It was
recently shown by [ 9] that LESS embeddings require only polylogarithmically many non-zeros per
row of Sto prove that Sis a subspace embedding for the data matrix Awith the optimal m=O(d)
sketching dimension. For our main results, it is sufficient to use η=O(1)below.
Lemma 2 (Subspace embedding for LESS, Theorem 1.3, [ 9]).Fixη, δ > 0. Consider β1, β2>1and
a full rank matrix A∈Rn×d. Then for a (β1, β2)-leverage score sparsified embedding S∈Rm×n
withs≥O(log4(d/δ)/η4)andm=O((d+ log 1 /δ)/η2), with probability 1−δwe have
1
1 +η·A⊤A⪯A⊤S⊤SA⪯(1 +η)·A⊤A. (1)
4 Least Squares Bias Analysis
In this section we provide an outline of the bias analysis for the sketched least squares estimator
constructed using a LESS embedding, leading to the proofs of our main results, Theorems 1 and 2. In
particular, we prove the following main technical result (detailed proof in Appendix C).
Theorem 4 (Bias of LESS-sketched least squares) .FixA∈Rn×dand let Sbe an (s, β1, β2)-LESS
embedding of size mforA. LetSsatisfy (1)withη=1
2and probability 1−δwhere δ <1
m4. Then
there exists an event Ewith probability at least 1−δsuch that
L(EE[˜x])−L(x∗) =Od
m2
1 +d
s
log9(n/δ)
·L(x∗).
Remark 3. Thus, the bias L(EE[˜x])−L(x∗)of the LESS estimator using O(β1β2s) =˜O(s)non-
zeros per row of Sis of the order ˜O(d2
sm2+d
m2)·L(x∗). By comparison, the standard expected
loss bound which holds for sketched least squares (including this estimator) is E[L(˜x)]−L(x∗)≤
˜O(d
m)L(x∗), and the best known bound on the bias of most standard sketched estimators (e.g.,
leverage score sampling) is ˜O(d2
m2)L(x∗), given by [ 43]. So, our result recovers the standard bias
bound for s= 1and improves on it for s≫1by a factor of min{s, d}. At the end of the section, we
discuss how to deal with the lower order term ˜O(d
m2)to reduce the bias further.
Proof sketch. Using a standard argument, we can replace the matrix Awith the matrix U∈Rn×d
consisting of orthonormal columns spanning the column space of A, and assume that n= poly( d).
LetSbe an (s, β1, β2)-LESS embedding for U. Also, let b∈Rnbe a vector of responses/labels
corresponding to nrows in U. Let ˜x= argminx∥SUx−Sb∥2. Furthermore for any x∈Rd
we can find the loss at xasL(x) =∥Ux−b∥2. Additionally, we use rto denote the residual
b−Ux∗. We also define Q= (γU⊤S⊤SU)−1as the sketched inverse covariance matrix with
scaling γ=m
m−drepresenting the standard correction accounting for inversion bias. We condition on
the high probability event Eguaranteed in Lemma 2 and consider L(EE(˜x))−L(x∗). By Pythagorean
theorem, we have L(EE[˜x])−L(x∗) =∥U(EE[˜x])−Ux∗∥2. Note that by the normal equations
we have ˜x= (U⊤S⊤SU)−1U⊤S⊤Sb=γQU⊤S⊤Sb, and also S⊤S=1
mPm
i=1xix⊤
i. These two
facts lead to writing the bias as follows:
L(EE[˜x])−L(x∗) =∥γ·EE[QU⊤xix⊤
ir]∥2.
Using a leave-one-out technique , we replace QwithQ−i= (γU⊤S⊤
−iS−iU)−1, where S−idenotes
matrix Swithout the ith row, by noting that Q= (γU⊤S⊤
−iS−iU+γ
mU⊤xix⊤
iU)−1and applying
the Sherman-Morrison formula. This leads to the following relation:
L(EE[˜x])−L(x∗)≤2∥EE[Q−iU⊤xix⊤
ir]∥2
| {z }
∥Z0r∥2+2EEγ
γi−1
Q−iU⊤xix⊤
ir2
| {z }
∥Z2r∥2
where γi= 1 +γ
mx⊤
iUQ−iU⊤xi. Due to the subspace embedding assumption and assuming
mlarge enough, we have ∥Q∥=O(1)and also ∥Q−i∥=O(1). The first term ∥Z0r∥2is quite
straightforward to bound since, if not for the conditioning on the high probability event E, we would
haveE[Q−iU⊤xix⊤
ir] =E[Q−iU⊤r] =0, which follows from U⊤(b−Ux∗) =0. We get an
upper bound on ∥Z0r∥2asO
d2log(d/δ)
sm2+d
m2
· ∥r∥2, which is sufficient for us.
7The central novelty of our analysis lies in bounding ∥Z2r∥2for(s, β1, β2)-LESS embeddings, which
is the dominant term. Our key observation is that, when examining a random variable of the form x⊤
iv
for some vector v, the dependence on the sparsity of row xionly arises when considering moments
higher than 2 +1
O(log(n)), because otherwise we can simply rely on the fact that E[xix⊤
i] =I. Thus,
when decomposing ∥Z2r∥2, we must carefully separate the contribution of near-second moments vs
the contribution of higher moments to the overall bound.
To obtain this separation, we start by applying Hölder’s inequality on ∥Z2r∥withp=O(log(n))
andq= 1 +1
O(log(n))to get
∥Z2r∥ ≤
EE[γ
γi−1p]1/p
· 
sup
∥v∥=1EE[v⊤Q−iU⊤xix⊤
ir]q!1/q
.
Furthermore applying Cauchy-Schwarz inequality on the second term leads to
∥Z2r∥ ≤
EE[γ
γi−1p]1/p
·
EE∥Q−iU⊤xi∥2q1/2q
·
EE∥x⊤
ir∥2q1/2q
.
Unlike [ 18], we exploit the fact that ∥xi∥1/O(log(n))=O(1)and get a constant upper bound on
(EE∥Q−iU⊤xi∥2q). However, this results in a much more careful argument, requiring now an upper
bound on (EE[γ
γi−1p])1/pforp=O(log(n)). First, we observe that

EE[γ
γi−1p]1/p
≤ |γ−¯γ|+ (EE[(γi−¯γ)p])1/p(2)
where ¯γ= 1 +γ
mEE(x⊤
iUQ−iU⊤xi). In particular, for the second term, we have
(EE[(γi−¯γ)p])1/p≤γ
m
·h 
EE
(tr(Q−i)−x⊤
iUQ−iU⊤xi)p1/p+ (EE[tr(Q−i)−EEtr(Q−i)]p)1/pi
.
(3)
To bound the first of these two terms, we prove a new version of the Restricted Bai-Silverstein
inequality (Lemma 3) for (s, β1, β2)-LESS embeddings. Unlike [ 18], we provide a proof with any p
and any (β1, β2)values. Furthermore, utilizing the subspace embedding guarantee from Lemma 2,
we prove a much more general result where the number of non-zeros in the approximate leverage
score sparsifier ξcan be much smaller than d(proof in Appendix D).
Lemma 3 (Restricted Bai-Silverstein for (s, β1, β2)-LESS embeddings) .Letp∈Nbe fixed and
U∈Rn×dbe such that U⊤U=I. Let xi= diag( ξ)yiwhere yi∈Rnhas independent ±1
entries and ξis an(s, β1, β2)-approximate leverage score sparsifier for U. Then for any matrix with
0⪯C⪯ O(1)·Iand any δ >0we have for an absolute constant c >0.
 
E[tr(C)−x⊤
iUCU⊤xi]p1/p< c·√
dp3· 
1 +r
dplog(d/δ)
s!
.
Using Lemma 3, we upper bound the first term squared in (3) as ˜O d
m2 
1 +d
s
. Moreover, also
using Lemma 3, we get a matching upper bound on |γ−¯γ|. We then design a martingale concentration
argument to prove a high probability upper bound on the last remaining term, |tr(Q−i)−EEtr(Q−i)|,
which implies the desired moment bound (proof in Appendix B), concluding the proof of Theorem 4.
Lemma 4. For given δ >0and matrix Q−iwe have with probability 1−δ:
|tr(Q−i)−Etr(Q−i)| ≤c′γ·d√mlog4.5(m/δ).
Completing the proof of Theorem 1. First, suppose that ϵ≥O(polylog( d)/d)so that the
bias bound can be achieved from Theorem 4. Our implementation is mainly based on the online
construction of approximate leverage scores, given the preconditioner P, using Lemma 1. Briefly,
this construction proceeds by first sketching Pusing a d×O(1/γ)Gaussian matrix Gto produce
the matrix ˜P=PG, and then, for each observed row aiofA, we compute ˜li=∥a⊤
i˜P∥2. Assuming
without loss of generality that d= poly( n)and adjusting γ, the estimates satisfy β1β2=O(αdγ).
8Next, we sample the non-zero entries of Scorresponding to the observed row ai, i.e., the i-th column
ofS. Note that for this we only need to know the single leverage score estimate ˜li. Crucially for our
analysis, the entries of this column need to be sampled i.i.d., which can be done in time proportional
to the number of non-zeros in that column by first sampling a corresponding Binomial distribution to
determine how many non-zeros we need, then picking a random subset of that size, and then sampling
the random ±1values. Altogether, the cost of constructing the sketch is O(γ−1nnz(A)+β1β2sd2) =
O(γ−1nnz(A)+αϵ−1d2+γpolylog( d))by setting s=O(polylog( d)/ϵ). Finally, once we construct
the sketch, at the end of the pass we can run conjugate gradient preconditioned with Pon the sketched
problem, which takes ˜O(αd2).
We note that in the (somewhat artificial) regime where we require extremely small bias, i.e., ϵ=
o(polylog( d)/d), the bound claimed in Theorem 1 can still be obtained, since in this case for small
enough γwe have d2+γ/ϵ=O(dω/√ϵ)withω < 2.5, so we can rely on direct leverage score
sampling (which corresponds to s= 1), and instead of maintaining the sketch, we compute the
estimator ˜x= (˜A⊤˜A)−1˜A⊤bdirectly along the way. This involves performing a separate d×d
matrix multiplication after collecting each dleverage score samples, to gradually compute ˜A⊤˜A,
and then inverting the matrix at the end. From Theorem 4, we see that it suffices to set sketch size
m=˜O(d/√ϵ), which leads to the desired runtime.
Completing the proof of Theorem 2. Here, we use a slightly modified variant of Lemma 2, given
as Theorem 1.4 in [ 9], which shows that using a single pass we can compute a sketch ˜Ain time
O(nnz(A) +dω), which satisfies the subspace embedding property (1)withη=1
2. Then, we can
perform the QR decomposition ˜A=QR and set P=R−1in additional time O(dω)to obtain
the desired preconditioner. Next, we use Theorem 1 to construct qi.i.d. estimators ˜xiin a second
parallel pass, and finally, the estimators are aggregated to compute ˆx=1
qPq
i=1˜xi, which satisfies
E∥Aˆx−b∥2≤ 
1 +ϵ+O(1/q)
∥Ax∗−b∥2. Applying Markov’s inequality concludes the proof.
5 Experiments
In this section, we illustrate empirically how our results point to a practical free lunch phenomenon
in distributed averaging of sketching-based estimators. As mentioned in Section 1, our construction
from Theorem 1 essentially works by taking a subsample of the data and then mixing groups of
those rows together to produce an even smaller sketch (see Figure 1). According to our theory, while
the small sketch does not recover the same ϵ-small error as the larger subsample, it does recover an
ϵ-small bias. Moreover, this happens without incurring any additional computational cost, as the cost
of the sketching is proportional to the cost of simply reading the subsampled rows. This suggests that
we can use sparse sketching to compress a data subsample down to a small size while retaining the
least squares performance in a distributed averaging environment.
To verify this, we evaluate the effectiveness of distributed averaging of sketched least squares
estimators on several benchmark datasets. Specifically, we visualize the relative error of the averaged
sketch-and-solve estimatorL(ˆx)−L(x∗)
L(x∗), against the number of machines qused to generate the estimate
ˆx=1
qPq
i=1˜xi. Each estimate ˜xiis constructed with the same sparsification strategy used by LESS,
except that instead of sparsifying the sketch with leverage scores, we instead sparsify them with
uniform probabilities (which is often sufficient in practice). Following [ 16], we call the resulting
method LESSUniform. Within each dataset, we perform four simulations, designed so that the total
sketching cost stays the same for all four test cases, by simultaneously changing sketch size and
sparsity. Concretely, we vary these so that the product (sketch size ×nnz per row) stays the same in
each case, so as to ensure that the total cost of sketching is fixed in each plot.
In Figure 2, on the X-axis we plot the number qof estimators being averaged, so that the bias of
a single estimator appears on the right-hand side of the plot (large q), whereas the variance (error)
appears on the left-hand side ( q= 1). In each plot, the line with nnz per row = 1(and large sketch
size) corresponds to uniform subsampling, whereas the remaining ones are sketches produced by
compressing that subsample. The plot shows that decreasing the sketch size (i.e., compressing the
sample) does increase the error of a single estimator (as expected), however it also shows that the
bias of these estimators remains essentially unchanged regardless of the sketch size (since all lines
9Figure 2: Distributed averaging experiment on YearPredictionMSD andAbalone datasets [ 8],
showing that sparse sketching can be used to compress the data while preserving near-unbiasedness
without increasing the estimation cost (see Appendix F for results on the Boston dataset).
meet as q→ ∞ ), confirming that suitable sparse sketches that “compress” rows of data can preserve
near-unbiasedness without increasing the cost.
This phenomenon may not occur for all sketching methods. Figure 4 within Appendix F showcases
a few more interesting results. We first further demonstrate that suitable sketches that “compress”
rows of data Ainto a single row of ˜Acan preserve near-unbiasedness without increasing the cost.
In particular, this desirable phenomenon that LESSUniform enjoys also extends to LESS proper,
as well as the Gaussian and Subgaussian (Rademacher) sketches. In fact, we observe that LESS
enjoys similar desirable performance as the Gaussian and Subgaussian sketches and virtually no
least squares bias, while retaining the computational speedups of sparse sketching, suggesting that it
attains the best of both worlds.
However, when we decrease the number of subsamples within leverage score subsampling, the bias
introduced by subsampling increases as expected. This happens as the number of subsamples is
reduced without increasing the amount of “compression” as one would with LESS or LESSUniform.1
We also show that the subsampled randomized Hadamard transform (SRHT) can exhibit some amount
of least squares bias as the sketch size decreases. The numerical results shown for the bias introduced
by leverage score subsampling and the SRHT complement the lower bounds established in [18].
6 Conclusions
We gave a new sparse sketching method that, using two passes over the data, produces a nearly-
unbiased least squares estimator, which can be used to improve upon the space-time trade-offs of
solving least squares in parallel or distributed environments via simple averaging. In particular, our
algorithm is the first to require only O(d2log(nd))bits of space and current matrix multiplication
timeO(dω)while obtaining an ϵ=o(1)approximation in few passes. Our techniques are of broader
interest to sketching-based optimization algorithms, including Distributed Newton Sketch.
Acknowledgments
This work was partially supported by NSF CAREER CCF-2338655.
1One can think of LESS and LESSUniform as generalizations of leverage score subsampling and uniform
subsampling respectively, where we mix ˜O(1/ϵ)subsamples into a single row of the sketch. This suggests that
sparse sketching, as an extension of subsampling, yields desirable bias reduction without significantly increasing
computational costs.
10References
[1]Dimitris Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary
coins. Journal of computer and System Sciences , 66(4):671–687, 2003.
[2]Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for
machine learning in linear time. The Journal of Machine Learning Research , 18(1):4148–4187,
2017.
[3]Nir Ailon and Bernard Chazelle. The fast Johnson–Lindenstrauss transform and approximate
nearest neighbors. SIAM Journal on computing , 39(1):302–322, 2009.
[4]Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel ridge regression with sta-
tistical guarantees. In Proceedings of the 28th International Conference on Neural Information
Processing Systems , pages 775–783, 2015.
[5]Nima Anari, Yang P Liu, and Thuy-Duong Vuong. Optimal sublinear sampling of spanning
trees and determinantal point processes via average-case entropic independence. In 2022 IEEE
63rd Annual Symposium on Foundations of Computer Science (FOCS) , pages 123–134. IEEE,
2022.
[6]Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices ,
volume 20. Springer, 2010.
[7]Daniele Calandriello, Michal Derezinski, and Michal Valko. Sampling from a k-dpp without
looking at all items. Advances in Neural Information Processing Systems , 33:6889–6899, 2020.
[8]Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology , 2:27:1–27:27, 2011.
[9]Shabarish Chenakkod, Michał Derezi ´nski, Xiaoyu Dong, and Mark Rudelson. Optimal embed-
ding dimension for sparse subspace embeddings. In 56th Annual ACM Symposium on Theory of
Computing , 2024.
[10] Nadiia Chepurko, Kenneth L Clarkson, Praneeth Kacham, and David P Woodruff. Near-optimal
algorithms for linear algebra in the current matrix multiplication time. In Proceedings of the
2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) , pages 3043–3068. SIAM,
2022.
[11] Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In
Proceedings of the forty-first annual ACM symposium on Theory of computing , pages 205–214,
2009.
[12] Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in input
sparsity time. In Proceedings of the forty-fifth annual ACM symposium on Theory of Computing ,
pages 81–90, 2013.
[13] Michael B Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank
approximation via ridge leverage score sampling. In Proceedings of the Twenty-Eighth Annual
ACM-SIAM Symposium on Discrete Algorithms , pages 1758–1777. SIAM, 2017.
[14] Michael B Cohen and Richard Peng. Lp row sampling by lewis weights. In Proceedings of the
symposium on Theory of computing , pages 183–192, 2015.
[15] Michał Derezi ´nski. Stochastic variance-reduced newton: Accelerating finite-sum minimization
with large batches. arXiv preprint arXiv:2206.02702 , 2022.
[16] Michał Derezi ´nski, Jonathan Lacotte, Mert Pilanci, and Michael W Mahoney. Newton-LESS:
Sparsification without trade-offs for the sketched newton update. Advances in Neural Informa-
tion Processing Systems , 34:2835–2847, 2021.
[17] Michał Derezi ´nski, Feynman T Liang, Zhenyu Liao, and Michael W Mahoney. Precise expres-
sions for random projections: Low-rank approximation and randomized newton. Advances in
Neural Information Processing Systems , 33, 2020.
11[18] Michał Derezi ´nski, Zhenyu Liao, Edgar Dobriban, and Michael Mahoney. Sparse sketches with
small inversion bias. In Conference on Learning Theory , pages 1467–1510. PMLR, 2021.
[19] Michał Derezi ´nski and Michael W Mahoney. Distributed estimation of the inverse Hessian
by determinantal averaging. In Advances in Neural Information Processing Systems 32 , pages
11401–11411. 2019.
[20] Michał Derezi ´nski and Michael W Mahoney. Determinantal point processes in randomized
numerical linear algebra. Notices of the American Mathematical Society , 68(1):34–45, 2021.
[21] Michał Derezi ´nski and Michael W Mahoney. Recent and upcoming developments in randomized
numerical linear algebra for machine learning. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining , pages 6470–6479, 2024.
[22] Michał Derezi ´nski and Elizaveta Rebrova. Sharp analysis of sketch-and-project methods via
a connection to randomized singular value decomposition. SIAM Journal on Mathematics of
Data Science , 6(1):127–153, 2024.
[23] Michał Derezi ´nski and Manfred K. Warmuth. Unbiased estimates for linear regression via
volume sampling. In Advances in Neural Information Processing Systems 30 , pages 3087–3096,
2017.
[24] Michał Derezi ´nski and Jiaming Yang. Solving dense linear systems faster than via precondition-
ing. In 56th Annual ACM Symposium on Theory of Computing , 2024.
[25] Edgar Dobriban and Sifan Liu. Asymptotics for sketching in least squares regression. Advances
in Neural Information Processing Systems , 32, 2019.
[26] Petros Drineas and Michael W Mahoney. Randnla: randomized numerical linear algebra.
Communications of the ACM , 59(6):80–90, 2016.
[27] Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Sampling algorithms for ℓ2
regression and applications. In Proceedings of the seventeenth annual ACM-SIAM symposium
on Discrete algorithm , pages 1127–1136, 2006.
[28] Robert M Gower and Peter Richtárik. Randomized iterative methods for linear systems. SIAM
Journal on Matrix Analysis and Applications , 36(4):1660–1690, 2015.
[29] William B Johnson, Gideon Schechtman, and Joel Zinn. Best constants in moment inequalities
for linear combinations of independent and exchangeable random variables. The Annals of
Probability , pages 234–253, 1985.
[30] Alex Kulesza and Ben Taskar. k-DPPs: Fixed-Size Determinantal Point Processes. In Pro-
ceedings of the 28th International Conference on Machine Learning , pages 1193–1200, June
2011.
[31] Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning . Now
Publishers Inc., Hanover, MA, USA, 2012.
[32] Jonathan Lacotte, Sifan Liu, Edgar Dobriban, and Mert Pilanci. Optimal iterative sketching
methods with the subsampled randomized Hadamard transform. Advances in Neural Information
Processing Systems , 33:9725–9735, 2020.
[33] Daniel LeJeune, Pratik Patil, Hamid Javadi, Richard G Baraniuk, and Ryan J Tibshirani.
Asymptotics of the sketched pseudoinverse. arXiv preprint arXiv:2211.03751 , 2022.
[34] Yi Li and David Woodruff. Input-sparsity low rank approximation in schatten norm. In
International Conference on Machine Learning , pages 6001–6009. PMLR, 2020.
[35] Miles E Lopes, Shusen Wang, and Michael W Mahoney. A bootstrap method for error estimation
in randomized matrix multiplication. The Journal of Machine Learning Research , 20(1):1434–
1473, 2019.
12[36] Ping Ma, Yongkai Chen, Xinlian Zhang, Xin Xing, Jingyi Ma, and Michael W Mahoney.
Asymptotic analysis of sampling estimators for randomized numerical linear algebra algorithms.
The Journal of Machine Learning Research , 23(1):7970–8014, 2022.
[37] Per-Gunnar Martinsson and Joel A Tropp. Randomized numerical linear algebra: Foundations
and algorithms. Acta Numerica , 29:403–572, 2020.
[38] Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-
sparsity time and applications to robust linear regression. In Proceedings of the Symposium on
Theory of Computing , STOC ’13, pages 91–100, 2013.
[39] R. Murray, J. Demmel, M. W. Mahoney, N. B. Erichson, M. Melnichenko, O. A. Malik,
L. Grigori, M. Derezi ´nski, M. E. Lopes, T. Liang, and H. Luo. Randomized Numerical Linear
Algebra – a perspective on the field with an eye to software. Technical Report arXiv preprint
arXiv:2302.11474, 2023.
[40] Jelani Nelson and Huy L Nguyên. Osnap: Faster numerical linear algebra algorithms via sparser
subspace embeddings. In 2013 ieee 54th annual symposium on foundations of computer science ,
pages 117–126. IEEE, 2013.
[41] G. Raskutti and M. W. Mahoney. A statistical perspective on randomized sketching for ordinary
least-squares. Journal of Machine Learning Research , 17(214):1–31, 2016.
[42] Tamas Sarlos. Improved approximation algorithms for large matrices via random projections.
In2006 47th annual IEEE symposium on foundations of computer science (FOCS’06) , pages
143–152. IEEE, 2006.
[43] S. Wang, A. Gittens, and M. W. Mahoney. Sketched ridge regression: Optimization perspective,
statistical perspective, and model averaging. Journal of Machine Learning Research , 18(218):1–
50, 2018.
[44] Shusen Wang, Fred Roosta, Peng Xu, and Michael W Mahoney. GIANT: globally improved
approximate newton method for distributed optimization. Advances in Neural Information
Processing Systems , 31:2332–2342, 2018.
[45] David P Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends ®
in Theoretical Computer Science , 10(1–2):1–157, 2014.
13A Detailed preliminaries
We start by providing several classical results, used in our analysis. The following formula provides a
way to compute the inverse of matrix Aafter a rank- 1update, given the inverse before the update.
Lemma 5 (Sherman-Morrison formula) .For an invertible matrix A∈Rd×dand vector u,v∈Rd,
A+uv⊤is invertible if and only if 1 +v⊤A−1u̸= 0. If this holds then,
(A+uv⊤)−1=A−1−A−1uv⊤A−1
1 +v⊤A−1u.
In particular,
(A+uv⊤)−1u=A−1u
1 +v⊤A−1u.
The following inequality provides a crucial tool for writing expectation of the product of two random
variables as the product of higher individual moments.
Lemma 6 (Hölder’s inequality) .For real-valued random variables XandY,
E[|XY|]≤(E[|X|p])1/p·(E[|Y|q])1/q
where p, q > 0are Hölder’s conjugates, i.e.1
p+1
q= 1.
The following technical lemmas provide concentration results for the sum of random quantities. We
collect these results here and then refer to them while using in our analysis.
Lemma 7 (Matrix Chernoff Inequality) .Fori= 1,2,···, nconsider a sequence Ziofd×dpositive
semi-definite random matrices such that E[1
nP
iZi] =Idand∥Zi∥ ≤R. Then for any ϵ >0, we
have
Pr 
λmax 
1
nnX
i=1Zi!
≥(1 +ϵ)!
≤d·exp
−nϵ2
(2 +ϵ)R
.
where λmax denotes taking the maximum eigenvalue.
Lemma 8 (Azuma’s inequality) .If{Y0, Y1, Y2,···} is a martingale with |Yj−Yj−1| ≤cjthen for
anym, λ > 0we have
Pr (|Ym−Y0| ≥λ)≤2·exp 
−λ2
2Pm
j=1c2
j!
.
Lemma 9 (Rosenthal’s inequality ([ 29], Theorem 2.5 and Corollary 2.6)) .Let1≤p <∞and
X1, X2,···, Xnare nonnegative, independent random variables with finite pthmoments then,
 
E"X
iXi#p!1/p
≤2p
log(p)·max

X
iE[Xi], X
iE[Xp
i]!1/p

.
Furthermore, for mean-zero independent and symmetric random variables we have
 
E"X
iXi#p!1/p
≤2pp
log(p)·max

 X
iE[X2
i]!1/2
, X
iE[Xp
i]!1/p

.
Lemma 10 (Bai-Silverstein’s Inequality Lemma B.26 from [ 6]).LetBbe ad×dbe a fixed matrix
andxbe a random vector of independent entries. Let E[xi] = 0 andE[x2
i] = 1 ,andE|xj|l≤νl.
Then for any p≥1,
E|x⊤Bx−tr(B)|p≤(2p)p·
(ν4tr(BB⊤))p/2+ν2ptr(BB⊤)p/2
.
14B Inversion bias analysis
We use the notion of an (ϵ, δ)unbiased estimator of Aas defined in [18].
Definition 3 ((ϵ, δ)-unbiased estimator) .Forϵ, δ > 0, a random positive definite matrix B∈Rd×d
is called an (ϵ, δ)unbiased estimator of Aif there exists an event EwithPr(E)≥1−δsuch that,
1
1 +ϵA⪯EE[B]⪯(1 +ϵ)Aand,B⪯O(1)·A,
when conditioned on the event E.
In this section, we give a formal statement and proof for Theorem 3, which is then used in the proof
of Theorem 4. We replace AwithUsuch that Uconsists of dorthonormal columns spanning the
column space of A. Here Sm∈Rm×ndenotes a LESS sketching matrix with independent rows
1√mx⊤,x⊤=y⊤·diag( ξ)where yconsists of ±1Rademacher entries and ξis an (s, β1, β2)-
approximate leverage score sparsifier. Note that E[xx⊤] =In. We assume that the sketching matrix
Smconsists of m≥10di.i.d rows and 3divides m. Also, we assume that the Smsatisfies the
subspace embedding condition for U(Theorem 2) with η=1
2. LetQ= (γU⊤S⊤
mSmU)−1where
γ=m
m−d.
Theorem 5 (Small inversion bias for (s, β1, β2)-LESS embeddings) .Letδ >0satisfy δ <1
m4and
m≥O(d). LetSm∈Rm×nbe an (s, β1, β2)-LESS embedding for data matrix U∈Rn×dsuch
thatU⊤U=I. Then there exists an event EwithPr(E)≥1−δsuch that,
1
1 +ϵ·I⪯EE[Q]⪯(1 +ϵ)·Iand1
2I⪯Q⪯2Iwhen conditioned on E
where ϵ=O√
dlog4.5(n/δ)
m
1 +q
d
s
.
Proof. LetS−idenote Smwithout the ithrow, and S−ijdenote Smwith the ithandjthrows
removed. Let Q−i= (γU⊤S⊤
−iS−iU)−1andQ−ij= (γU⊤S⊤
−ijS−ijU)−1. We proceed with the
same proof strategy as adopted in [18]. We define the events Ejas follows:
Ej=3
mU⊤
tjX
i=t(j−1)+1xix⊤
i
U⪰1
2I, j= 1,2,3,E=∧3
j=1Ej.
Note that event Ejmeans that the sketching matrix with just (1/3)rdrows (scaled to maintain
unbiasedness of the sketch) from Smsatisfies a lower spectral approximation of U⊤U=I. Also
we notice that events E1,E2,E3are independent, and for any pair (i, j)there exists at least one event
Ek, k∈ {1,2,3}such that Ekis independent of both xiandxj. Furthermore conditioned on Ekwe
have
Q−i⪯6·IdandQ−ij⪯6·Id.
Note that as guaranteed in Theorem 2, we have Pr(Ek)≥1−δ′for all kand therefore Pr(E)≥1−δ
withδ′=δ/3. LetEEdenote the expectation conditioned on the event E.
I−EE[Q] =−EE[Q] +γEE[QU⊤S⊤
mSmU]
=−EE[Q] +γEE[QU⊤xix⊤
iU]
=−EE[Q] +γEEQ−iU⊤xix⊤
iU
1 +γ
mx⊤
iUQ−iU⊤xi
=EE[Q−iU⊤(xix⊤
i−I)U]| {z }
Z0+EE[Q−i−Q]|{z}
Z1+EE γ
γi−1
Q−iU⊤xix⊤
iU
| {z }
Z2
where γi= 1 +γ
mx⊤
iUQ−iU⊤xi. The second equality follows by noting that S⊤
mSm=
1
mPm
i=1xix⊤
iand using linearity of expectation. The third equality holds due to the application of
Sherman Morrison’s (Lemma 5) formula on Q= (γU⊤S⊤
−iS−iU+γ
mU⊤xix⊤
iU)−1. We start by
upper bounding Z0and use the following from [18].
15Lemma 11 (Upper bound on ∥Z0∥).For any k >0we have,
∥EE[Q−iU⊤(xix⊤
i−I)U]∥ ≤12
kδ′+Z∞
kPr(x⊤
iUU⊤xi≥x)dx
.
Using Chebyshev’s inequality we have, Pr(x⊤
iUU⊤xi≥x)≤Var(x⊤
iUU⊤xi)
x2 . Using Restricted
Bai-Silverstein inequality for (s, β1, β2)-approximate LESS embeddings i.e., Lemma 3 (proved in
Lemma 16) with p= 2, we have Var(x⊤
iUU⊤xi)≤cd·
1 +dlog(d/δ)
s
for some absolute constant
c. Letk=m2andδ′<1
m4we get,
∥Z0∥=O1
m2+d
m2+d2log(d/δ)
sm2
=Od
m2+d2log(d/δ)
sm2
. (4)
We use the bound on the term ∥Z1∥directly from [18], provided below as a Lemma.
Lemma 12 (Upper bound on ∥Z1∥, [18]) .
∥EE′[Q−i−Q]∥=O(1/m). (5)
It remains to upper bound ∥Z2∥.
∥Z2∥=∥EE γ
γi−1
Q−iU⊤xix⊤
iU
∥ ≤ sup
∥v∥=1,∥z∥=1EE[|γ
γi−1| · |v⊤Q−iU⊤xix⊤
iUz|].
Applying Hölder’s inequality with p=O(log(n))andq=p
p−1= 1 + ∆ for∆ =1
O(log(n)), we get,
∥Z2∥ ≤ sup
∥v∥=1,∥z∥=1
EE
|γ
γi−1|p1/p
· 
EE
|v⊤Q−iU⊤xix⊤
iUz|q1/q
≤
EE
|γ
γi−1|p1/p
·sup
∥v∥=1 
EE
|v⊤Q−iU⊤xi|2q1/2q
| {z }
O(1)·sup
∥z∥=1 
EE
|x⊤
iUz|2q1/2q
| {z }
O(1)
(6)
where we used Cauchy-Schwarz inequality on EE
|v⊤Q−iU⊤xix⊤
iUz|q
. Now note that xi=
diag( ξ)·yi, we have ∥xi∥= poly( n)and therefore ∥xi∥∆=O(1). We now show the terms
involving exponents depending on qareO(1)as highlighted in the inequality (6).
 
sup
∥v∥=1EE
|v⊤Q−iU⊤xi|2q!1/2q
≤ 
sup
∥v∥=1EE
|v⊤Q−iU⊤xi|2·v⊤Q−iU⊤xi|2∆]!1/2q
≤ 
sup
∥v∥=1EE
|v⊤Q−iU⊤xi|2·v⊤Q−iU⊤∥2∆· ∥xi∥2∆]!1/2q
≤ O(1)· 
EE
∥Q−iU⊤xi∥2· ∥Q−iU⊤∥2∆1/2q
≤ O(1)· 
2·EE′
∥Q−iU⊤xi∥2· ∥Q−iU⊤∥2∆1/2q.
HereE′is an event independent of xi. Without loss of generality, we can assume that E′=E1∧ E2.
We first condition on Q−iand take expectation over xi. Also note that Q−iandxiare independent
and event E′is independent of xi, and furthermore EE′[x⊤
ixi] =E[x⊤
ixi] = 1 . We get,
 
sup
∥v∥=1EE
|v⊤Q−iU⊤xi|2q!1/2q
≤ O(1)· 
EE′[∥Q−iU⊤∥2q]1/2q.
Now we use that conditioned on E′,∥Q−i∥ ≤6and∥U⊤∥= 1, we get,
 
sup
∥v∥=1EE
|v⊤Q−iU⊤xi|2q!1/2q
=O(1).
16Similarly, using ∥xi∥∆=O(1)andEE′[x⊤
ixi] =E[x⊤
ixi] = 1 ,
sup
∥z∥=1 
EE
|x⊤
iUz|2q1/2q=O(1).
Now we prove an upper bound on
EEh
γ
γi−1pi1/p
. Without loss of generality, we assume that
pis even. We have,
EEγ
γi−1p
≤2·EE′γ
γi−1p
where E′is event independent of xi. The above can be upper bounded as,

EE′γ
γi−1p1/p
≤(EE′[(γ−γi)p])1/p= (EE[(γ−¯γ+ ¯γ−γi)p])1/p
≤ |γ−¯γ|+ (EE′[(γi−¯γ)p])1/p(7)
where ¯γ= 1 +γ
mEE′(x⊤
iUQ−iU⊤xi). AsE′is independent of xiandQ−iis independent of xi,
we get EE′(x⊤
iUQ−iU⊤xi) =EE′tr(Q−i). Therefore, ¯γ= 1 +γ
mEE′tr(Q−i). We now aim to
upper bound (EE′[(γi−¯γ)p])1/pas,
(EE′[(γi−¯γ)p])1/p≤γ
m
·h 
EE′
(tr(Q−i)−x⊤
iUQ−iU⊤xi)p1/p+ (EE′[tr(Q−i)−EE′tr(Q−i)]p)1/pi
.
Using our new Restricted Bai-Silverstein inequality from Lemma 3 (restated as Lemma 16 and proven
in Appendix D), we have
 
EE′
(tr(Q−i)−x⊤
iUQ−iU⊤xi)p1/p< c·p3√
d· 
1 +r
dplog(d/δ)
s!
.
We now consider (EE′[tr(Q−i)−EE′tr(Q−i)]p)1/p. In Lemma 4 (restated below as Lemma 13), we
show that |tr(Q−i)−EE′tr(Q−i)| ≤c′γ√m·dlog4.5(m/δ)with probability at least 1−δ. Conditioned
on this high-probability event we have,
(EE′[tr(Q−i)−EE′tr(Q−i)]p)1/p≤c′γ√m·dlog4.5(m/δ)
for an absolute constant c′>0. Therefore we get with probability at least 1−δ,
(EE′[(γi−¯γ)p])1/p≤γ
m"
c·p3√
d· 
1 +r
dplog(d/δ)
s!
+c′γ√m·dlog4.5(m/δ)#
.
Asm > d , we get (EE′[(γi−¯γ)p])1/p=O√
dlog4.5(n/δ)
m·
1 +q
d
s
. Also using the analysis
in [18] for upper bounding |γ−¯γ|, we get a matching upper bound on |γ−¯γ|as follows:
|γ−¯γ|=O √
dlog4.5(n/δ)
m· 
1 +r
d
s!!
.
Substituting these bounds in (7) and then in (6) we get,
∥Z2∥=O √
dlog4.5(n/δ)
m· 
1 +r
d
s!!
. (8)
Combining the upper bounds for Z0,Z1andZ2using relations (4,5,8) we conclude our proof.
We now provide the proof of Lemma 4, which we restate in the following Lemma.
17Lemma 13. For given δ >0and matrix Q−iwe have with probability 1−δ:
|tr(Q−i)−EE′tr(Q−i)| ≤c′γ√m·dlog4.5(m/δ)
for an absolute constant c′>0.
Proof. Writing tr(Q−i)−EE′tr(Q−i)as a finite sum, we have,
tr(Q−i)−EE′tr(Q−i) =mX
j=1EE′,jtr(Q−i)−EE′,j−1tr(Q−i).
Denoting Xj=EE′,jtr(Q−i)withX0=EE′tr(Q−i), we have the following formulation
tr(Q−i)−EE′tr(Q−i) =mX
j=1Xj−Xj−1
withEE′,j−1[Xj] = Xj−1. The random sequence Xjforms a martingale and tr(Q−i)−
EE′tr(Q−i) =Xm−X0. We find an upper bound on |Xj−Xj−1|. To achieve that we note,
Xj−Xj−1=EE′,jtr(Q−i)−EE′,j−1tr(Q−i) =−(EE′,j−EE′,j−1)(tr(Q−ij−Q−i)−tr(Q−ij)).
Therefore with ψj= (EE′,j−EE′,j−1)tr(Q−ij−Q−i)andχj=−(EE′,j−EE′,j−1)tr(Q−ij),
we have,
|Xj−Xj−1| ≤ |ψj+χj| ≤ |ψj|+|χj|.
From [18], we have |χj| ≤1
m. We now prove an upper bound on ψj.
0≤tr(Q−ij)−tr(Q−i) = tr γ
mQ−ijU⊤xjx⊤
jUQ−ijU⊤U
1 +γ
mx⊤
jUQ−ijU⊤xj!
=γ
mx⊤
j(UQ−ijU⊤)2xj
1 +γ
mx⊤
jUQ−ijU⊤xj≤γ
mx⊤
jUQ2
−ijU⊤xj.
Now look at the term x⊤
jUQ2
−ijU⊤xj. For any a >0and any k >0, by Markov’s inequality we
have,
Pr 
x⊤
jUQ2
−ijU⊤xj≥a
≤E[|x⊤
jUQ2
−ijU⊤xj|k]
ak
≤2k−1·E[|x⊤
jUQ2
−ijU⊤xj−tr(Q2
−ij)|k
ak+2k−1·E[(tr(Q2
−ij))k]
ak.
LetE1be an event independent of both xiandxjand have probability at least 1−δ′. Therefore we
haveE[·]≤2·EE1[·]. We get,
Pr 
x⊤
jUQ2
−ijU⊤xj≥a
≤2k·EE1[|x⊤
jUQ2
−ijU⊤xj−tr(Q2
−ij)|k
ak+2k·EE1[(tr(Q2
−ij))k]
ak.
(9)
We now upper bound both terms on the right-hand side separately. Considering the term
EE1[|x⊤
jUQ2
−ijU⊤xj−tr(Q2
−ij)|kand using Lemma 3 we get,
EE1[|x⊤
jUQ2
−ijU⊤xj−tr(Q2
−ij)|k]≤ck·k3k·d2klog(d/δ)
s+dk/2
. (10)
Now considering the second term in (9), i.e., EE1[(tr(Q2
−ij))k]. We use that conditioned on E1, we
haveQ−ij⪯6Id. Therefore,
EE1[(tr(Q2
−ij))k]≤6k·dk. (11)
18Substituting (10) and (11) in (9),
Pr 
x⊤
jUQ2
−ijU⊤xj≥a
≤2k·ck·k3k·
d2klog(d/δ))
s+dk/2
ak+2k·6k·dk
ak
Pr 
x⊤
jUQ2
−ijU⊤xj≥a
≤2k·ck·k3k·dk·(klog(d/δ))k/2
ak
for some potentially different constant c. Consider k=l
log(m/δ)
log(2)m
anda= 4·c·k3·d·p
klog(d/δ)
and we have with probability at least 1−δ/m,
x⊤
jUQ2
−ijU⊤xj≤4·c·k3·d·p
log(kd/δ).
This implies that for an absolute constant c′we have,
|tr(Q−ij)−tr(Q−i)| ≤c′·γ
m·d·log3.5(m/δ).
Therefore we now have an upper bound for |ψj|
|ψj|=|EE′,j−EE′,j−1(tr(Q−ij−Q−i))| ≤2c′·γ
m·d·log3.5(m/δ).
This means for all j, we have,
|Xj−Xj−1| ≤4c′·γ
m·d·log3.5(m/δ)
with probability at least 1−δ. Consider cj= 4c′·γ
m·d·log3.5(m/δ). ThenPm
j=1c2
j=γ2
m·16c′2·
d2log7(m/δ). Applying Azuma’s inequality (Lemma 8) with λ=γ√m·4c′·d·log3.5(m/δ). We
get with probability at least 1−δand for potentially different absolute constant c′>0:
|Xm−X0| ≤c′γ√m·dlog4.5(m/δ).
This concludes our proof.
B.1 Proof of Corollary 1
We consider the following Distributed Newton Sketch method:
xt+1=xt−pt, pt=1
qqX
i=1˜H−1
igt,
where gt=∇f(xt)and˜Hi=m
m−dA⊤
tS⊤
iSiAtis a sketched estimate of the Hessian Ht=
∇2f(xt), when expressed as the matrix product Ht=A⊤
tAtfor the appropriately chosen n×d
matrix At(as can be done for any generalized linear model). This update can be viewed as an
approximate Newton step with the Hessian inverse estimate ˆH−1
t=1
qPq
i=1˜H−1
i. As long as each
˜H−1
tis an(√ϵ, δ/2q)-unbiased estimator of H−1
t, then using Lemma 34 from [ 18] we get that the
averaged Hessian inverse with q=˜O(1/ϵ)satisfies:
1
1 + ˜ϵH−1
t⪯ˆH−1
t⪯(1 + ˜ϵ)H−1
t,˜ϵ=√ϵ+˜O(1/√q) =O(√ϵ).
Using standard approximate Newton analysis (e.g., see Lemmas 1 and 3, and the related discussion in
[15]), this implies that when xtis in a sufficiently small neighborhood around x∗(determined solely
by the strong convexity and Lipschitz constants of the Hessian of f), we have:
f(xt+1)−f(x∗)
f(xt)−f(x∗)=O(˜ϵ2) =O(ϵ).
Adjusting the constants appropriately, relying on Theorem 3 for constructing ˜Hi, and following the
complexity analysis from Theorem 2, we recover the claim.
19C Least squares bias analysis: Proof of Theorem 4
In this section, we aim to prove Theorem 4. Let x∗= argminx∥Ux−b∥2where U∈Rn×dis the
data matrix containing ndata points and b∈Rnis a vector containing labels corresponding to ndata
points. We adopt the same notations as used in the proof of Theorem 5. Let ˜x= argminx∥SmUx−
Smb∥2. Furthermore for any x∈Rdwe can find the loss at xasL(x) =∥Ux−b∥2. Additionally,
we use rto denote the residual b−Ux∗. We aim to provide an upper bound on the bias introduced
due to this sketch and solve paradigm, i.e. L(E(˜x))−L(x∗). Similar to Theorem 4 we condition
on the high probability event Eand consider L(EE(˜x))−L(x∗). By Pythagorean theorem, we have
L(EE[˜x])−L(x∗) =∥U(EE[˜x])−Ux∗∥2. Also,
L(EE[˜x])−L(x∗) =∥U(EE[˜x])−Ux∗∥2=∥EE[˜x]−x∗∥2
=EE[(U⊤S⊤
mSmU)−1U⊤S⊤
mSmb]−U⊤b2
=EE[(U⊤S⊤
mSmU)−1U⊤S⊤
mSm](b−UU⊤b)2.
Note that b−UU⊤b=b−Ux∗=r. We get,
L(EE[˜x])−L(x∗) =EE[(U⊤S⊤
mSmU)−1U⊤S⊤
mSm]r2.
Consider Q= (γU⊤S⊤
mSmU)−1,Q−i= (γU⊤S⊤
−iS−iU)−1andγ=m
m−d,
EE[(U⊤S⊤
mSmU)−1U⊤S⊤
mSmr] =EE[γ(γU⊤S⊤
mSmU)−1U⊤S⊤
mSmr]
=γEE[QU⊤S⊤
mSmr]
=γEE[QU⊤xix⊤
ir]
where we used linearity of expectation in the last line combined with S⊤
mSm=1
mPm
i=1xix⊤
i.
Using Sherman-Morrison formula (Lemma 5) we have QU⊤xi=Q−iU⊤xi
1+γ
mx⊤
iUQ−iU⊤xi. Denote
γi= 1 +γ
mx⊤
iUQ−iU⊤xiand substitute we get,
EE[(U⊤S⊤
mSmU)−1U⊤S⊤
mSmr] =EEγ
γi
Q−iU⊤xix⊤
ir
EE[(U⊤S⊤
mSmU)−1U⊤S⊤
mSmr] =EE[Q−iU⊤xix⊤
ir] +EEγ
γi−1
Q−iU⊤xix⊤
ir
.
So we get the following decomposition:
L(EE[˜x])−L(x∗) =EE[(U⊤S⊤
mSmU)−1U⊤S⊤
mSm]r2
=EE[Q−iU⊤xix⊤
ir] +EEγ
γi−1
Q−iU⊤xix⊤
ir2
≤2∥EE[Q−iU⊤xix⊤
ir]∥2
| {z }
∥Z0r∥2+2EEγ
γi−1
Q−iU⊤xix⊤
ir2
| {z }
∥Z2r∥2.(12)
Note that a similar decomposition was considered in the proof of Theorem 5 (see Appendix B) with
slightly different Z0andZ2. We first bound ∥Z0r∥2in the following argument. Without loss of
generality, we assume that events E1andE2are independent of xiandE′=E1∧ E2.
Z0r=EE[Q−iU⊤xix⊤
ir] =E[Q−iU⊤xix⊤
ir·1E]
Pr(E)
=E[Q−iU⊤xix⊤
ir·1E1·1E2·(1−1¬E3)]
Pr(E)
=E[Q−iU⊤xix⊤
ir·1E1·1E2]
Pr(E)−E[Q−iU⊤xix⊤
ir·1E1·1E2·1¬E3)]
Pr(E)
=1
1−δ′(EE′[Q−iU⊤xix⊤
ir]−EE′[Q−iU⊤xix⊤
ir·1¬E3)]).
20Now note that in the first term Q−iis independent of xiandxiis also independent of the event E′.
Using this with the fact that E[xix⊤
i] =Iwe get EE′[Q−iU⊤xix⊤
ir] =EE′[Q−iU⊤r] = 0 , since
U⊤r= 0. Therefore,
∥Z0r∥2≤2∥EE′[Q−iU⊤xix⊤
ir·1¬E3]∥2
≤72∥EE′[U⊤xix⊤
ir·1¬E3]∥2.
The last inequality holds because conditioned on E′, we know that ∥Q−i∥ ≤6. Using Cauchy-
Schwarz inequality we have,
∥Z0r∥2≤72q
EE′[x⊤
iUU⊤xi·1¬E3]·q
EE′[r⊤xix⊤
ir]2
.
Note that EE′[r⊤xix⊤
ir] =∥r∥2. Also, we can bound EE′[x⊤
iUU⊤xi·1¬E3]as following:
EE′[x⊤
iUU⊤xi·1¬E3] =Z∞
0Pr(x⊤
iUU⊤xi·1¬E3> x)dx
=Zy
0Pr(x⊤
iUU⊤xi·1¬E3> x)dx+Z∞
yPr(x⊤
iUU⊤xi·1¬E3> x)dx
≤yδ′+Z∞
yPr(x⊤
iUU⊤xi> x)dx.
Using Chebyshev’s inequality, Pr(x⊤
iUU⊤xi≥x)≤Var(x⊤
iUU⊤xi)
x2 . By Restricted Bai-Silverstein,
Lemma 3 with p= 2, we have Var(x⊤
iUU⊤xi)≤c·
d+d2log(d/δ)
s
for some absolute constant c.
Lety=m2andδ′<1
m4we get,
E[x⊤
iUU⊤xi·1¬E3] =O1
m2+d
m2+d2log(d/δ)
sm2
=Od
m2+d2log(d/δ)
sm2
.
This finishes upper bounding ∥Z0r∥2as:
∥Z0r∥2=Od
m2+d2log(d/δ)
sm2
· ∥r∥2. (13)
Now we proceed with ∥Z2r∥2,
∥Z2r∥=EEγ
γi−1
Q−iU⊤xix⊤
ir.
Applying Hölder’s inequality with p=O(log(n))andq= 1 + ∆ where ∆ =1
O(log(n)), we get,
∥Z2r∥ ≤
EEγ
γi−1p1/p
· 
sup
∥v∥=1EE[v⊤Q−iU⊤xix⊤
ir]q!1/q
≤
EEγ
γi−1p1/p
·
EE∥Q−iU⊤xi∥2q1/2q
·
EE∥x⊤
ir∥2q1/2q
≤
EEγ
γi−1p1/p
·
2·EE′[∥Q−iU⊤xi∥2· ∥Q−iU⊤xi∥2∆]1/2q
·
2·EE′[∥x⊤
ir∥2· ∥x⊤
ir∥2∆]1/2q
.
Since ∥xi∥= poly( n), we have ∥xi∥2∆=O(1)and therefore we have ∥Q−iU⊤xi∥2∆=
O(1)∥Q−iU⊤∥2∆. Also EE′∥Q−iU⊤xi∥2=EE′∥Q−i∥2. Using that conditioned on E′we
have∥Q−i∥ ≤6, we get
EE∥Q−iU⊤xi∥2q1/2q
=O(1). Similarly using ∥xi∥2∆=O(1)we
get
EE∥x⊤
ir∥2q1/2q
=O(1)∥r∥. This gives us:
∥Z2r∥ ≤ O (1)·
EEγ
γi−1p1/p
· ∥r∥.
21Now using (8) from the proof of Theorem 5, we get,

EEγ
γi−1p1/p
=O √
dlog4.5(n/δ)
m· 
1 +r
d
s!!
.
Finally the bound for ∥Z2r∥2follows as:
∥Z2r∥2=Odlog9(n/δ)
m2·
1 +d
s
· ∥r∥2. (14)
Combining (13) and (14) we conclude our proof.
D Higher-Moment Restricted Bai-Silvestein (Lemma 3)
In this section, we prove Lemma 3. We need the following two auxiliary lemmas to derive the main
theorem of this section. The first lemma uses Matrix-Chernoff concentration inequality to upper
bound the spectral norm of U⊤
ξUξwhere ξis a(s, β1, β2)-approximate leverage score sparsifier.
Lemma 14 (Spectral norm bound with leverage score sparsifier) .LetU∈Rn×dhas orthonormal
columns. Let ξbe a(s, β1, β2)-approximate leverage score sparsifier for U, and denote Uξ=
diag( ξ)U. Then for any δ >0we have,
PrU⊤
ξUξ≥
1 +3dlog(d/δ)
s
≤δifs < d,
Pr U⊤
ξUξ≥(1 + 3 log( d/δ))
≤δifs≥d.
Proof. Writing U⊤
ξUξas a sum of matrices we have,
U⊤
ξUξ=nX
i=1ξ2
iuiu⊤
i=nX
i=1bi
piuiu⊤
i=nX
i=1Zi
where pi= min {1,sβ1˜li
d}andZi=bi
piuiu⊤
i. Note that Z′
isare independent random variables
andE[Zi] =uiu⊤
i. AlsoPn
i=1EZi=U⊤U=Id. Ifpi= 1 thenZi=uiu⊤
iand therefore
∥Zi∥=∥ui∥2≤1. Ifpi<1, we have ∥Zi∥ ≤1
pi∥ui∥2=d
sβ1˜li·li. As li≤β1˜li, we get
∥Zi∥ ≤d
s. Therefore ∥Zi∥ ≤max{1,d
s}for all i. Denote R= max {1,d
s}. We use Matrix Chernoff
(Lemma 7) to upper bound the largest eigenvalue of U⊤
ξUξ. For any ϵ >0, we have,
Pr 
λmax nX
i=1Zi!
≥(1 +ϵ)!
≤d·exp
−ϵ2
(2 +ϵ)R
.
With R= max {1,d
s}and depending on the case whether s≤dors > d we get
Pr 
λmax nX
i=1Zi!
≥
1 +3dlog(d/δ)
s!
≤δifs≤d,
Pr 
λmax nX
i=1Zi!
≥(1 + 3 log( d/δ))!
≤δifs > d.
LetAξdenote the eventU⊤
ξUξ≤1 +3dlog(d/δ)
s, holding with probability at least 1−δ, for small
δ >0. In the next result we upper bound the higher moments of the trace of UξCU⊤
ξfor any matrix
C⪯ O(1)·I. We first prove the upper bound in the case when the high probability event Aξdoes
not occur.
22Lemma 15 (Trace moment bound over small probability event) .Letk∈Nbe fixed. Let U∈Rn×d
have orthonormal columns. Let ξbe a(s, β1, β2)-approximate leverage score sparsifier for U. Let
Uξ= diag( ξ)U. Also let Pr(Aξ)≥1−1
(12d)4kand event E′be independent of the sparsifier ξ.
Then we have,
EE′
(tr(UξCU⊤
ξUξCU⊤
ξ))k|¬A ξ
≤(4k)4k
for any fixed matrix Csuch that 0⪯C⪯6I.
Proof.
EE′
(tr(UξCU⊤
ξUξCU⊤
ξ))k|¬A ξ
=Z∞
0Pr((tr( UξCU⊤
ξUξCU⊤
ξ))k·1¬Aξ≥x|E′)dx
=Z∞
0Pr((tr( UξCU⊤
ξUξCU⊤
ξ))k·1¬Aξ≥x)dx.
The last equality holds because E′is independent of ξ. Consider some fixed y >0.
EE′
(tr(UξCU⊤
ξUξCU⊤
ξ))k|¬A ξ
≤Zy
0Pr((tr( UξCU⊤
ξ))2k·1¬Aξ≥x)dx
+Z∞
yPr((tr( UξCU⊤
ξ))2k·1¬Aξ≥x)dx
≤y·δ+E[(tr(UξCU⊤
ξ))4k]·Z∞
y1
x2dx. (15)
The last inequality holds because by Chebyshev’s inequality Pr((tr( UξCU⊤
ξ))2k≥x)≤
E[(tr(UξCU⊤
ξ))4k]
x2 . Also note that,
(tr(UξCU⊤
ξ))4k= nX
i=1ξ2
iu⊤
iCui!4k
= nX
i=1bi
piu⊤
iCui!4k
.
For1≤i≤n, letRibe random variables denotingbi
piu⊤
iCui. Then note that Riare independent
random variables with E[Ri] =u⊤
iCui. Also Riare non-negative random variables with finite
(4k)thmoment. Using Rosenthal’s inequality (Lemma 9) we get,
E
 nX
i=1bi
piu⊤
iCui!4k
≤24k·(4k)4k·
nX
i=1E[R4k
i] + nX
i=1E[Ri]!4k
.
NowPn
i=1E[Ri] = tr( UCU⊤)andE[R4k
i]can be found as follows: if pi= 1thenRi=u⊤
iCui
and therefore R4k
i= (u⊤
iCui)4k≤ ∥ui∥2(4k−1)u⊤
iC4kui≤u⊤
iC4kui, ifpi<1, we have,
E[R4k
i] =pi·1
p4k
i(u⊤
iCui)4k=d4k−1
s4k−1·1
(β1˜li)4k−1(u⊤
iCui)4k
≤d4k−1
s4k−1·1
(β1˜li)4k−1· ∥ui∥2(4k−1)u⊤
iC4kui
=d4k−1
s4k−1·1
(β1˜li)4k−1·l4k−1
iu⊤
iC4kui.
Now using li≤β1˜liwe get,
E[R4k
i]≤d4k−1
s4k−1·u⊤
iC4kui.
Therefore,
E
 nX
i=1bi
piu⊤
iCui!4k
≤24k·(4k)4k·
max
1,d4k−1
s4k−1
·tr(C4k) + (tr( C))4k
.
23Using the above inequality along with using C⪯6Iwe get,
E
 sX
t=11
spitu⊤
itCuit!4k
≤24k·(4k)4k·64k·d4k= (12)4k·(4k)4k·d4k.
Substituting the above bound in (15), it follows that:
EE′
(tr(UξCU⊤
ξUξCU⊤
ξ))k|¬A ξ
≤y·δ+ (12)4k·(4k)4k·d4k·1
y.
Fory >(12d)4kandδ <1
y, we get the desired result.
We are now ready to prove the main result of this section. The following result, which is central to
our analysis, upper bounds the high moments of a deviation of a quadratic form from its mean.
Lemma 16 (Restricted Bai-Silverstein for (s, β1, β2)-LESS embedding) .Letp∈Nbe fixed and
U∈Rn×dhave orthonormal columns. Let xi= diag( ξ)yiwhere yi∈Rnhas independent ±1
entries and ξis a(s, β1, β2)-approximate leverage score sparsifier for U. LetUξ= diag( ξ)U.
Then for any matrix 0⪯C⪯6Iand any δ >0we have,
 
E[tr(C)−x⊤
iUCU⊤xi]p1/p< c·p3√
d· 
1 +r
dplog(d/δ)
s!
for an absolute constant c >0.
Proof. Letxi= diag( ξ)yiwhere yiis vector of Rademacher ±1entries. Denote Uξ= diag( ξ)U.
E
(tr(C)−x⊤
iUCU⊤xi)p
=E
(tr(C)−y⊤
iUξCU⊤
ξyi)p
=E
(tr(C)−tr(UξCU⊤
ξ) + tr( UξCU⊤
ξ)−y⊤
iUξCU⊤
ξyi)p
≤2p−1
E
tr(C)−tr(UξCU⊤
ξ)p
| {z }
T1+E
tr(UξCU⊤
ξ)−y⊤
iUξCU⊤
ξyip
| {z }
T2
.
(16)
First, consider T1, substitute ξ2
i=bi
pi, and assume exponent pto be even.
E
tr(C)−tr(UξCU⊤
ξ)p=E
tr(UCU⊤)−tr(UξCU⊤
ξ)p
=E" nX
i=1(ξ2
i−1)u⊤
iCui!p#
=E" nX
i=1(bi
pi−1)u⊤
iCui!p#
.
For1≤i≤nconsider random variables Riwhere Ri=bi
piu⊤
iCui. Furthermore let Yi=
Ri−u⊤
iCui. ThenPn
i=1Yi=Pn
i=1(bi
pi−1)u⊤
iCuiandE[Yi] = 0 .Yiare independent mean
zero random variables with finite pthmoments and therefore we can use Rosenthal’s inequality (for
symmetric random variables, Lemma 9) to get,
E"nX
i=1Yi#p
< A(p)
nX
i=1E[Yi]p
|{z}
T1
1+ nX
i=1E[Yi]2!p/2
| {z }
T2
1
(17)
where A(p)is a constant depending on p. We bound T1
1andT2
1separately, starting with T1
1as,
T1
1=nX
i=1E[Yi]p.
24Recall that Yi=Ri−u⊤
iCui.Riis always non-negative because Cis a positive semi-definite
matrix. Therefore we have,
E(Yi)p≤E(Ri)p.
We find the pthmoment of Ri,E(Ri)p= (u⊤
iCui)pifpi= 1. Ifpi<1then,
E(Ri)p=pp−1
i(u⊤
iCui)p=dp−1
sp−1·1
(β1˜li)p−1(u⊤
iCui)p≤dp−1
sp−1·1
(β1˜li)p−1· ∥ui∥2(p−1)u⊤
iCpui
≤dp−1
sp−1u⊤
iCpui
where in the last inequality we use ∥ui∥2(p−1)=lp−1
i≤(β1˜li)p−1. Summing over ifrom 1tonwe
get an upper bound for T1
1as the following:
nX
i=1E(Ri)p≤nX
i=1E(Ri)p≤max
1,dp−1
sp−1
·tr(UCpU⊤). (18)
Now we upper bound T2
1. Note that E(Yi)2=E[Ri]2= (u⊤
iCui)2ifpi= 1and if pi<1we have
E[Ri]2=1
pi·(u⊤
iCui)2≤d
s·1
β1˜li∥ui∥2u⊤
iC2ui≤d
su⊤
iC2ui. Summing over ifrom 1tonwe
get an upper bound for T2
1as,
 nX
i=1E(Yi)2!p/2
≤
max
1,d
s
tr(UC2U⊤)p/2
. (19)
Substituting (18) and (19) in (17) we have,
E
tr(C)−tr(UξCU⊤
ξ)p≤A(p)· 
max
1,dp−1
sp−1
·tr(UCpU⊤) +
max
1,d
s
tr(UC2U⊤)p/2!
.
(20)
Now we aim to upper bound the term T2in (16) i.e., Eh
tr(UξCU⊤
ξ)−y⊤
iUξCU⊤
ξyiip
. First, we
condition over ξand take expectation over yi. This requires using standard Bai-Silverstein inequality
(Lemma 10), we get,
E
tr(UξCU⊤
ξ)−y⊤
iUξCU⊤
ξyip≤B(p)·Eh 
ν4tr(UξCU⊤
ξUξCU⊤
ξ)p/2+ν2ptr(UξCU⊤
ξUξCU⊤
ξ)p/2i
where B(p)is a constant depending on p. Since yiconsists of ±1entries we have ν4, ν2p≤1. Also
using tr(AB)≤tr(A)tr(B)and considering the high probability event AξcapturingU⊤
ξUξ≤
1 + max
3dlog(d/δ)
s,3 log( d/δ)
. We get the following,
E
tr(UξCU⊤
ξ)−y⊤
iUξCU⊤
ξyip≤2B(p)·E 
tr(UξCU⊤
ξUξCU⊤
ξ)p/2
=2B(p)·h
E 
tr(UξCU⊤
ξUξCU⊤
ξ)·1Aξp/2i
+ 2B(p)·h
E 
tr(UξCU⊤
ξUξCU⊤
ξ)·1¬Aξp/2i
≤2B(p)·
1 + max3dlog(d/δ)
s,3 log( d/δ)p/2
E 
tr(UξC2U⊤
ξ)p/2+ 2B(p)·(2p)2p.
(21)
The first term in the last inequality follows from the Matrix-Chernoff (Lemma 7) and the second term
follows from Lemma 15 by considering k=p/2(assuming that δis small enough so that Lemma 15
is satisfied). We now upper-bound E
tr(UξC2U⊤
ξ)p/2
using Rosenthal’s inequality for uncentered
(non-symmetric) random variables,
tr(UξC2U⊤
ξ) =nX
i=1ξ2
iu⊤
iC2ui=nX
i=1bi
piu⊤
iC2ui.
25For1≤i≤nconsider independent random variables R′
i=bi
piu⊤
iC2ui. We have,
nX
i=1R′
i= tr(UξC2U⊤
ξ).
HereR′
iare positive random variables with finite (p/2)thmoment. Using Rosenthal’s inequality we
get,
E nX
i=1R′
i!p/2
≤C(p/2)·
nX
i=1E(R′
i)p/2
|{z}
T1
2+ nX
i=1ER′
i!p/2
|{z }
T2
2
. (22)
It is straightforward to upper bound E(R′
i)p/2as,
E(R′
i)p/2= (u⊤
iC2ui)p/2ifpi= 1,
≤dp
2−1
sp
2−1u⊤
iCpuiifpi<1.
Summing over ifrom 1tonwe get upper bound for T1
2as,
nX
i=1E(R′
n)p/2≤max
1,dp
2−1
sp
2−1
tr(UCpU⊤). (23)
Now we consider T2
2. It is simply given as,
 nX
i=1ER′
j!p/2
= 
tr(UC2U⊤)p/2. (24)
Combining (23) and (24) and substituting in (22) we get,
E 
tr(UξC2U⊤
ξ)p/2≤C(p/2)·
max
1,dp
2−1
sp
2−1
tr(UCpU⊤) + 
tr(UC2U⊤)p/2
.(25)
Substituting (25) in (21) and let w= max(1 , d/s),
E
tr(UξCU⊤
ξ)−y⊤
iUξCU⊤
ξyip≤2B(p)·(2p)2p
+2B(p)C(p/2)·(1 + 3 wlog(d/δ))p/2·h
wp
2−1tr(UCpU⊤) + 
tr(UC2U⊤)p/2i
. (26)
Combining the bounds for T1(20) and T2(26) substituting in (16), and noting that tr(UCkU⊤) =
tr(Ck)for any k, we get,
E
(tr(C)−x⊤
iUCU⊤xi)p
≤2p−1A(p)·
wp−1·tr(Cp) + 
w·tr(C2)p/2
+2pB(p)C(p/2)·(1 + 3 wlog(d/δ))p/2·h
wp
2−1tr(Cp) + 
tr(C2)p/2i
+2pB(p)(2p)2p.
Now we specify the various constants depending on p. We have A(p)≤(2p)p, B(p)≤
(2p)p, C(p/2)≤pp/2. Also we use tr(Ck)≤6k·dsince C⪯6I. This implies for an abso-
lute constant cwe have,
 
E[tr(C)−x⊤
iUCU⊤xi]p1/p≤c·p3√
d· 
1 +r
dplog(d/δ)
s!
where δ >0is now arbitrary.
26E Application to distributed settings with partitioned data
In this section, we extend our main result to settings where the dataset is partitioned across multiple
machines. We illustrate this in the distributed setting considered in [ 43]. LetA∈RN×dandb∈RN
withrank(A) =d. In practice, we randomly shuffle the rows of Aandb, and partition the data
uniformly across qmachines, with every machine getting chunks of size ndenoted by (Ai,bi). In
this distributed setup, Aican be considered a uniformly subsampled sketch of Awith sketch size
n. However, similar to the analysis in [ 43], in the following analysis we assume that each machine
constructs its sketch (Ai,bi)by uniformly sampling rows from Aandbwith replacement and
independently from other machines. This setting is not ideally a partition but we believe similar
results can also be shown for the setting where we partition the dataset after reshuffling the rows.
Letµ=Nmaxn
i=1ℓi(A)denote the matrix coherence of A. LetS∈Rn×Nbe a uniform sampling
matrix with size n, where we will assume that Salso scales the samples byp
N/n , which implies
thatE[S⊤S] =I. Then the following holds:
Lemma 17 (Subspace embedding using uniform subsampling; based on Theorem 12 in [ 43]).
LetS∈Rn×Nbe a uniform sampling matrix of size nwhere n≥O(µlog(d/δ)). Then with
probability 1−δwe have,
1
1 +η·A⊤A⪯A⊤S⊤SA⪯(1 +η)·A⊤A
forη=O(p
µlog(d/δ)/n).
Letx∗=A†bdenote the minimizer of least squares loss ∥Ax−b∥2andx∗
i=A†
ibidenote the
solution to the sketched least squares problem at ithmachine. Here Ai=SiAandbi=Sib, where
Sidenotes the uniform subsampling matrix for ithmachine. We can use the model averaging result
from [43] to prove the following result.
Lemma 18 (Model averaging result; adapted from Theorem 20 in [ 43]).Let there be qmachines
and each machine constructs its sketch (Ai,bi)independently by uniformly sampling nrows from A
andb, where n=O(µlog(dq/δ)). Then with probability 0.99,
A¯x∗−b2
≤
1 +cµlog(d/δ)
qn+µ2log2(d/δ)
n2
· ∥Ax∗−b∥2. (27)
where ¯x∗=1
qPq
i=1x∗
iandc >0is an absolute constant2.
Since computing x∗
iat any machine could be potentially expensive due to large n, we instead find ˜xi
atithmachine using the sketching procedure from Theorem 2. Note that this requires applying our
debiasing techniques to the least squares problem min∥Aix−bi∥2at every machine. To that end,
let¯x=1
qPq
i=1˜xiand consider EP∥A¯x−b∥2, where EPmeans conditional expectation given the
sketches (Ai,bi)at every machine. The following theorem states that if the bias from partitioning is
much smaller than the sketching error in each machine, then our approach can be successfully used
to reduce the bias of the sketching estimate so that averaging will work as desired.
Theorem 6. Let there be qmachines and each machine constructs its sketch (Ai,bi)independently
by uniformly sampling nrows from Aandb, where n≥O(µlog(dq/δ)). Also, let ithmachine
construct ˜xivia locally sketching Aiandbisuch that ˜xisatisfies Theorem 1 (with Aiandbi) atith
machine. Then, the estimator ¯x=1
qPq
i=1˜xiaveraged across the machines satisfies
EP∥A¯x−b∥2≤
1 +c′
ϵ+1
q+µlog(d/δ)
qn+µ2log2(d/δ)
n2
∥Ax∗−b∥2.
with probability at least 0.9and an absolute constant c′>0.
Proof. By the Pythagorean theorem we have:
EP∥A¯x−b∥2=∥Ax∗−b∥2+EP∥A(¯x−x∗)∥2. (28)
2The result Theorem 20 [ 43] also requires that ∥S⊤
iSi∥2≤N
nfor all i, which holds trivially if Siis a
sampling matrix without replacement. However, in our setting, we are implicitly assuming that with high
probability ∥S⊤
iSi∥2=O(N
n)and absorb the extra resulting constant in c.
27We proceed to upper bound EP∥A(¯x−x∗)∥2as,
EP∥A(¯x−x∗)∥2≤2 
EP∥A(¯x−¯x∗)∥2+∥A(¯x∗−x∗)∥2
.
We use Lemma 18 to upper bound the second term and get,
EP∥A(¯x−x∗)∥2≤2
EP∥A(¯x−¯x∗)∥2+cµlog(d/δ1)
qn+µ2log2(d/δ1)
n2
∥Ax∗−b∥2
.
(29)
On the other hand,
EP∥A(¯x−¯x∗)∥2=EP∥A(¯x−EP[¯x] +EP[¯x]−¯x∗∥2
=EP∥A(¯x−EP[¯x])∥2+∥A(EP[¯x]−¯x∗)∥2. (30)
We upper bound both terms in (30) separately. Proceeding with the first term we have,
EP∥A(¯x−EP[¯x])∥2=EPA1
qqX
i=1(˜xi−EP[˜xi])2
=1
q2·EPAqX
i=1(˜xi−EP[˜xi])2
=1
q2·qX
i=1EP∥A(˜xi−EP[˜xi])∥2(31)
≤1 +η
q2·qX
i=1EP∥Ai(˜xi−EP[˜xi])∥2(32)
≤2(1 + η)
q2·qX
i=1EP
∥Ai(˜xi−x∗
i)∥2+∥Ai(EP[˜xi]−x∗
i)∥2
≤2(1 + η)
q2·qX
i=1∥Aix∗
i−bi∥2(33)
≤2(1 + η)
q2·qX
i=1∥Aix∗−bi∥2. (34)
The equality (31) holds as all ˜x′
isare independent. The inequality (32) holds due to the subspace
embedding property (Lemma 17). The inequality (33) holds due to the variance bound in Theorem 1.
The last inequality holds because x∗
iminimizes the least squares loss ∥Aix−bi∥2atithmachine.
We now upper bound the second term in (30) as
∥A(EP[¯x]−¯x∗)∥=A1
qqX
i=1(EP[˜xi]−x∗
i)
≤1
q·qX
i=1∥A(EP[˜xi]−x∗
i)∥
≤√1 +η
q·qX
i=1∥Ai(EP[˜xi]−x∗
i)∥ (35)
≤p
ϵ(1 +η)
q·qX
i=1∥Aix∗
i−bi∥ (36)
≤p
ϵ(1 +η)
q·qX
i=1∥Aix∗−bi∥
The inequality (35) holds due to the subspace embedding property (Lemma 17) and inequality (36)
holds due to the bias bound in Theorem 1. Therefore we get,
∥A(EP[¯x]−¯x∗)∥2≤ϵ(1 +η)
q2qX
i=1∥Aix∗−bi∥2
. (37)
28Combining (28, 29, 34, 37) and assuming η <1we get,
EP∥A¯x−b∥2≤
1 + 2 cµlog(d/δ1)
qn+µ2log2(d/δ1)
n2
· ∥Ax∗−b∥2+8
q2·qX
i=1∥Aix∗−bi∥2
+4ϵ
q2qX
i=1∥Aix∗−bi∥2
. (38)
AsE
1
qPq
i=1∥Aix∗−bi∥2
=∥Ax∗−b∥2, we just use Markov’s inequality to upperbound the
second term in (38) by160
q∥Ax∗−b∥2with probability 0.95. For the last term, we have
qX
i=1∥Aix∗−bi∥2
=q·1
qqX
i=1∥Aix∗−bi∥2+ (q2−q)·1
q2−qX
i,j,i̸=j∥Aix∗−bi∥ · ∥Ajx∗−bj∥
Again1
qPq
i=1∥Aix∗−bi∥2≤20∥Ax∗−b∥2with probability 0.95. The last term is an average
ofq2−qrandom variables and we can provide uniform bound for the expectation of all of them
using the independence among different machines. We have E[∥Aix∗−bi∥ · ∥Ajx∗−bj∥]≤p
E[∥Aix∗−bi∥2]·p
E[∥Ajx∗−bj∥2] =∥Ax∗−b∥2. Again using Markov’s inequality we
can upper bound the average of these q2−qrandom variables with 20times their expectation with
probability 0.95, finishing the proof of the theorem.
F Further experimental details
Here, we provide a small set of numerical experiments to empirically examine the relative error of
distributed averaging estimates from individual machines to return an estimator ˆx=1
qPq
i=1˜xi.
First, we show that when the sketching-based estimates ˜xido have a non-negligible bias, so that
the distributed averaging estimator ˆxremains inconsistent in the number of machines – even with
an unlimited number of machines, as long as the space on each machine is limited, the averaged
estimator’s performance will be limited by the bias of the individual estimates. Second, we show
that one can use sketching to compress a data subsample at no extra computational cost, without
increasing its bias, which we refer to as the free lunch in distributed averaging via sketching.
We examine three benchmark regression datasets, Abalone (4177 rows, 8 features), Boston (506
rows, 13 features), and YearPredictionMSD (truncated to the first 2500 rows, with 90 features),
from the libsvm repository from [ 8]. All the experiments were run on an i9-13900k processor with
128GB of RAM and an NVIDIA RTX 3090 GPU. We repeat the experiment 100 times, with the
shaded region representing the standard error. We visualize the relative error of the averaged sketch-
and-solve estimatorL(ˆx)−L(x∗)
L(x∗), against the number of machines qused to generate the estimate
ˆx=1
qPq
i=1˜xi.
Each estimate ˜xiwas constructed with the same sparsification strategy used by LESS, except
that instead of sparsifying the sketch with leverage scores, we instead sparsify them with uniform
probabilities. Following [ 16], we call the resulting method LESSUniform. Within each dataset, we
perform four simulations, each with different sketch sizes and different numbers of nonzero entries
per row. We vary these so that the product (sketch size ×nnz per row) stays the same, so as to ensure
that the total cost of sketching is fixed in each plot.
As expected, decreasing the sketch size while increasing the number of nonzeros per row (effectively
increasing the amount of “compression” occurring here by sparse sketching) increases the error in
all three datasets. However, remarkably, increasing the amount of “compression” does not seem
to increase the amount of bias. We can therefore conclude that sparse sketches preserve near-
unbiasedness, while enabling us to reduce the sketch size from subsampling without incurring any
additional computational cost. The increase in error can be mitigated in a distributed setting by
increasing the number of estimates/machines.
F.1 Comparison with other sketching estimators
To further illustrate the phenomenon that suitable sketched least squares estimators enjoy small bias,
we provide a further set of numerical experiments below in Figure 4 for the Boston and Abalone
29Figure 3: Comparison of the relative error of the distributed averaging estimator of sketch-and-
solve least squares estimates where the sketches are constructed with sparse sketching matrices
with uniform probabilities (LESSUniform) on libsvm dataset Boston (see Figure 2 for results on
YearPredictionMSD andAbalone ). For each dataset, the computational cost of sketching is the
same in all four parameter settings. Remarkably, sketching to a smaller size appears to preserve
near-unbiasedness without incurring any additional computational cost.
datasets. Recall that in Figures 2 and 3, we show that increasing the amount of “compression” of the
sample, by reducing the sketch size when using sparse sketching matrices with uniform probabilities,
does not increase the bias of the sketched estimator. Figure 4 further demonstrates that Gaussian and
Subgaussian sketches, which are computationally much more expensive than our proposed sparse
sketches, exhibit virtually no bias in sketched least squares (as expected). A similar conclusion can
be made for the proper LESS method, which uses leverage score estimates, and is therefore more
expensive than the LESSUniform method we use in other experiments, but still potentially much
cheaper than Subgaussian sketches.
However, the unbiasedness does not hold for all sparse sketches, as we demonstrate below in Figure 4.
At a high level, we show that sketches constructed using leverage score subsampling and the SRHT
can exhibit a non-negligible level of least squares bias.
Gaussian and Subgaussian sketches, as we see in the figure, enjoy near-unbiasedness while not
introducing additional bias as the sketch size decreases. This does not hold for leverage score
subsampling. When we decrease the number of subsamples within leverage score subsampling, the
bias introduced by subsampling increases as suggested by the lower bound in Theorem 10 of [ 18].
Intuitively, this happens as the number of subsamples is reduced without increasing the amount of
“compression” as one would with LESS or LESSUniform.
We also show that the subsampled randomized Hadamard transform (SRHT) also introduces increased
bias as the sketch size decreases. This complements the lower bound established for the failure
of SRHT and other data-oblivious sparse embeddings like CountSketch to satisfy the restricted
Bai-Silverstein property that is a structural condition for near-unbiasedness sketches [18].
For completeness, we also demonstrate the desirable performance of LESS proper. Figure 4 demon-
strates that the desirable phenomenon that LESSUniform enjoys also extends to LESS proper. In fact,
we observe that LESS enjoys similar desirable performance as the Gaussian and Subgaussian sketches
and does not increase (in fact, it has minimal) least squares bias, while enjoying the computational
speedups that sparse sketches also enjoy, suggesting a best-of-both-worlds property.
30Figure 4: Distributed averaging experiment for the Boston and Abalone datasets using five different
sketching techniques: Leverage Score Subsampling, Subsampled Randomized Hadamard Transform
(SRHT), Gaussian sketches, Subgaussian sketches, and LESS. Both Gaussian and Subgaussian
sketches exhibit no observable least squares bias, whereas Leverage Score Subsampling and SRHT
exhibit a small but measurable level of bias. LESS enjoys the similar high performance as the
Gaussian and Subgaussian sketches and does not increase (in fact, it has minimal) least squares bias,
but as a sparse sketch also allows for the significant improvements in runtime that sparse sketches
enjoy, demonstrating that it enjoys the best-of-both-worlds.
31NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The paper provides concise mathematical proofs for the claims made in the
abstract.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [NA]
Justification: The results proved in the paper hold in a very general setting, without any
limitations or requiring strong assumptions.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
32Answer: [Yes]
Justification: All theoretical results have been proved. Most of the formal proofs are part of
the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The experiments demonstrate the results for solving a few least-squares
problems using the Matrix sketching technique described in the paper. All the experimental
results are easily reproducible.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
335.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: A supplementary zipfile containing the code is provided to reproduce
the experimental results. One of the experiments requires the file ‘YearPrediction-
MSD.txt’ file, which can be obtained at ( https://archive.ics.uci.edu/dataset/
203/yearpredictionmsd ).
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The paper describes the parameters e.g. sketch size, sparsity of sketch (nnz per
row), and number of estimates(number of machines) along with dataset details to understand
the results.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The shaded areas in our plots denote the standard error obtained after averaging
the result over 100trials.
Guidelines:
• The answer NA means that the paper does not include experiments.
34•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: All experiments were run on an i9-13900k with 128GB of RAM and a NVIDIA
RTX 3090 GPU.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: This work aligns with the NeurIPS code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
35Justification: Our theoretical work provides a Matrix sketching algorithm, recovering a nearly
unbiased least squares estimator. Solving linear systems approximately is an important
component of a large number of algorithms from varied domains. Our work can thus provide
a way to recover approximation solutions with a smaller bias. This does not lead to any
negative societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This work does not release any data or model, and carries no risk of misuse.
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We test our theoretical results on a few publicly available datasets that have
been cited in the paper as well.
Guidelines:
• The answer NA means that the paper does not use existing assets.
36• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: No new assets are released in this work.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This work does not involve crowdsourcing or research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This work does not involve crowdsourcing or research with human subjects.
37Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
38