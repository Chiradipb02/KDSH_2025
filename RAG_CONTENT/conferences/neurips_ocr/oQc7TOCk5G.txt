On Theoretical Limits of Learning with Label
Differential Privacy
Anonymous Author(s)
Affiliation
Address
email
Abstract
Label differential privacy (DP) is designed for learning problems with private labels 1
and public features. Although various methods have been proposed for learning 2
under label DP, the theoretical limits remain unknown. The main challenge is to 3
take infimum over all possible learners with arbitrary model complexity. In this 4
paper, we investigate the fundamental limits of learning with label DP under both 5
central and local models. To overcome the challenge above, we derive new lower 6
bounds on testing errors that are adaptive to the model complexity. Our analyses 7
indicate that ϵ-local label DP only enlarges the sample complexity with respect to 8
ϵ, without affecting the convergence rate over the sample size N, except the case 9
with heavy-tailed label. Under the central model, the performance loss due to the 10
privacy mechanism is further weakened, such that the additional sample complexity 11
becomes negligible. Overall, our analysis validates the promise of learning under 12
the label DP from a theoretical perspective and shows that the learning performance 13
can be significantly improved by weakening the DP definition to only labels. 14
1 Introduction 15
Many modern machine learning tasks require sensitive training samples that need to be protected 16
from leakage [1]. As a standard approach for privacy protection, differential privacy (DP) [2] has 17
been extensively studied [3 –9]. However, the learning performances under original DP definition 18
are usually far from satisfactory [10 –13]. Therefore, researchers attempt to design weakened DP 19
requirements, under which the performances can be significantly improved, while still securing 20
sensitive information. Under such background, label DP has emerged in recent years [14], which 21
regards features as public, while only labels are sensitive and need to be protected. Such setting is 22
realistic in many applications, such as computational advertising [15], recommendation systems [16] 23
and medical diagnosis [17]. These tasks usually use some basic demographic information as features, 24
which can be far less sensitive. 25
Despite various approaches for learning with label DP [14, 18 –21], the fundamental limits are 26
still unknown. An interesting question is: By weakening the DP definitions to only labels, how 27
much accuracy improvement is possible? From an information-theoretic perspective [22], the 28
underlying limits of statistical problems are characterized by the minimax lower bound, which takes 29
the supremum over all possible distributions from a general class, and infimum over all learners. 30
Deriving minimax lower bounds for learning under the label DP is challenging in two aspects. Firstly, 31
under label DP, each sample has both public (i.e. the feature) and private (i.e. the label) components. 32
Directly applying the methods for original DP [23 –27] treats all components as private, and thus does 33
not yield tight results. Secondly, the classical packing method [47] is only suitable for fixed model 34
structures with fixed dimensionality. However, to establish lower bounds, one needs to take infimum 35
over all possible learners with arbitrary model complexity. 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.ClassificationRegression Regression
Bounded label noise Unbounded label noise
Local ˜O((N(ϵ2∧1))−β(γ+1)
2β+d)˜O
(N(ϵ2∧1)−2β
d+2β)
O
(Nϵ2)−2β(p−1)
2pβ+d(p−1)∨N−2β
2β+d
Central ˜O
N−β(γ+1)
2β+d+ (ϵN)−β(γ+1)
β+d
O
N−2β
2β+d+ (ϵN)−2β
d+β
O
N−2β
2β+d+ (ϵN)−2β(p−1)
pβ+d(p−1)
Local full O((N(ϵ2∧1))−β(γ+1)
2β+2d) O((N(ϵ2∧1))−β
β+d) O((N(ϵ2∧1))−β(p−1)
pβ+d(p−1))
Non-priv. O(N−β(γ+1)
2β+d) O(N−2β
2β+d) O(N−2β
2β+d)
Table 1: Minimax rate of convergence under label differential privacy. dis the dimension of features.
In this paper, we investigate the theoretical limits of classification and regression problems under label 37
DP. Our analysis involves both central and local models. For each problem, we derive the information- 38
theoretic minimax lower bound of the risk function over a wide class of distributions satisfying the 39
β-Hölder smoothness and the γ-Tsybakov margin assumption [28] (see Assumption 1 for details). 40
The general idea is to convert the problem to multiple hypothesis testing. To overcome the challenges 41
above, we provide a bound of Kullback-Leibler divergence over joint distributions of private and 42
public random variables, which is tighter than the bound between fully private variables. Moreover, 43
under the central model, instead of using the packing method, we develop a new lower bound on the 44
minimum testing error for each pair of hypotheses based on the group privacy property [4], which 45
is suitable for arbitrary model complexity. After deriving minimax lower bounds, we also propose 46
algorithms with matching upper bounds to validate the tightness of our results. 47
The results are shown in Table 1, in which the third row refers to the bounds under the original local 48
DP definition, while the fourth row lists the non-private baselines. To the best of our knowledge, 49
minimax rates under central DP have not been established, and are thus not listed here. The main 50
findings are summarized as follows. 51
•Under ϵ-local label DP, for classification and regression with bounded label noise, the 52
sample complexity is larger by a factor of O(1/ϵ2). However, the convergence rate remains 53
unaffected, which is in clear contrast with the original DP, under which the convergence rate 54
is slower. 55
•Under ϵ-local label DP constraint, for regression with heavy-tailed label noise, the conver- 56
gence rate of risk over Nbecomes slower, indicating that heavy-tailed labels increase the 57
difficulty of privacy protection. 58
•Under ϵ-central label DP constraint, the performance loss caused by the privacy mechanism 59
becomes further weakened. The risk only increases by a term that decays faster than the 60
non-private rate, indicating that the additional sample complexity caused by the privacy 61
mechanism becomes negligible with large N. 62
In general, our analysis provides a theoretical perspective of understanding label DP. The result 63
shows that by weakening the DP definition to protecting labels only, the learning performances can 64
be significantly improved. 65
2 Related Work 66
Label DP. Under the local model, labels are randomized before training. The simplest method is 67
randomized response [30]. An important improvement is proposed in [14], called RRWithPrior, 68
which incorporates prior distribution. [19] proposes ALIBI, which further improves randomized 69
response by generating soft labels through Bayesian inference. There are also several methods for 70
regression under label DP [18, 31]. Under central label DP, [20] proposes a clustering approach. [19] 71
proposes private aggregation of teacher ensembles (PATE), which is then further improved in [21]. 72
Minimax analysis for public data. Minimax theory provides a rigorous framework for the best 73
possible performance of an algorithm given some assumptions. Classical methods include Le 74
Cam [32], Fano [33] and Assouad [34]. Using these methods, minimax lower bounds have been 75
widely established for both classification and regression problems [28, 29, 35 –41]. If the feature 76
vector has bounded support, then the minimax rate of classification and regression are O(N−β(γ+1)
2β+d) 77
andO(N−2β
2β+d), respectively. 78
2Minimax analysis for private data. Under the local model, [42] finds the relation between label DP 79
and stochastic query. [23] and [24] develop the variants of Le Cam, Fano, and Assouad’s method 80
under local DP. Lower bounds are then established for various statistical problems, such as mean 81
estimation [43 –46], classification [26] and regression [27]. Under central model, for pure DP, the 82
standard approach is the packing method [47], which is then used in hypothesis testing [48], mean 83
estimation [49,50], and learning of distributions [51 –53]. There are also several works on approximate 84
DP, such as [54, 55]. 85
This work studies the theoretical limits of label DP, under which each sample is a mixture of public 86
feature and private labels, thus existing methods can not be directly applied here. Under the central 87
model, the minimax analysis becomes more challenging, since the packing method is only suitable 88
for fixed model structures (i.e. the dimensionality of model output is fixed), while we need to find the 89
minimum possible error over all possible learners with arbitrary output dimensions. As a result, the 90
lower bounds of general classification and regression problems have not been established even under 91
the original DP definition. To overcome such challenge, we develop a new approach to bound the 92
error of hypothesis testing (see Lemma 1 in Appendix D). 93
3 Preliminaries 94
In this section, we show some necessary definitions, background information, and notations. 95
3.1 Label DP 96
To begin with, we review the definition of DP. Suppose the dataset consists of Nsamples (xi, yi), 97
i= 1, . . . , N , in which xi∈ X is the feature vector, while yi∈ Y ⊂ Rdis the label. 98
Definition 1. (Differential Privacy (DP) [2]) Let ϵ≥0. A randomized function A: (X,Y)N→Θ 99
isϵ-DP if for any two adjacent datasets D, D′∈(X,Y)Nand any S⊆Θ, 100
P(A(D)∈S)≤eϵP(A(D′)∈S), (1)
in which DandD′are adjacent if they differ only on a single sample, including both the feature 101
vector and the label. 102
In machine learning tasks, the output of Ais the model parameters, while the input is the training 103
dataset. Definition 1 requires that both features and labels are privatized. Consider that in some 104
applications, the features may be much less sensitive, the notion of label DP is defined as follows. 105
Definition 2. (Central label DP) A randomized function Aisϵ-label DP if for any two datasets D 106
andD′that differ on the label of only one training sample and any S⊆Θ,(1)holds. 107
Compared with Definition 1, Definition 2 only requires the output to be insensitive to the replacement 108
of a label. Therefore label DP is a weaker requirement. Correspondingly, the local label DP is defined 109
as follows. 110
Definition 3. (Local label DP) A randomized function M: (X,Y)→ Z isϵ-local label DP if 111
sup
y,y′∈Ysup
S⊆ZlnP(M(x, y)∈S)
P(M(x, y′)∈S)≤ϵ. (2)
Definition 3 requires that each label is privatized locally before running any machine learning 112
algorithms. It is straightforward to show that local label DP ensures central label DP. To be more 113
precise, we have the following proposition. 114
Proposition 1. Letzi=M(xi, yi)fori= 1, . . . , N . IfAis a function of (xi,zi),i= 1, . . . , N , 115
thenAisϵ-label DP . 116
3.2 Risk of Classification and Regression 117
In supervised learning problems, given Nsamples (Xi, Yi),i= 1, . . . , N drawn from a common 118
distribution, the task is to learn a function g:X → Y . For a loss function l:Y × Y → R, the goal 119
is to minimize the risk function , which is defined as the expectation of loss function between the 120
predicted value and the ground truth: 121
R=E[l(ˆY , Y)]. (3)
3The minimum risk among all function gis called Bayes risk, i.e. R∗= min gE[l(g(X, Y))]. In 122
practice, the sample distribution is unknown, and we need to learn gfrom samples. Therefore, the 123
risk of any practical classifiers is larger than Bayes risk. The gap R−R∗is called excess risk, and we 124
hope that R−R∗to be as small as possible. Now we discuss classification and regression problems 125
separately. 126
1) Classification. For classification problems, the size of Yis finite. For convenience, we denote 127
Y= [K], in which [K] :={1, . . . , K }. In this paper, we use 0−1loss, i.e. l(ˆY , Y) =1(ˆY̸=Y), 128
thenR=P(ˆY̸=Y). Define Kfunctions η1, . . . , η Kas the conditional class probabilities: 129
ηk(x) =P(Y=k|X=x), k= 1, . . . , K. (4)
Under this setting, the Bayes optimal classifier and the corresponding Bayes risk is 130
c∗(x) = arg max
j∈[K]ηj(x), (5)
R∗
cls=P(c∗(X)̸=Y). (6)
2) Regression. Now we consider the case with Yhaving infinite size. We use ℓ2loss in this paper, i.e. 131
l(ˆY , Y) = ( ˆY−Y)2. Then the Bayes risk is 132
R∗
reg=E[(Y−η(X))2]. (7)
Then the following proposition gives a bound of the excess risk for classification and regression 133
problems. 134
Proposition 2. For any classifier c:X → [K], the excess risk of classification is bounded by 135
Rcls−R∗
cls=Z
(η∗(x)−E[ηc(x)(x)])f(x)dx. (8)
For any regression estimate ˆη:X → Y , the excess risk of regression is bounded by 136
Rreg−R∗
reg=E[(ˆη(X)−η(X))2]. (9)
The proof of Proposition 2 is shown in Appendix A. Finally, we state some basic assumptions that 137
will be used throughout this paper. 138
Assumption 1. There exists some constants L,β,CT,γ,c,Dandθ∈(0,1]such that 139
(a) For all j∈[K]and any x,x′,|ηj(x)−ηj(x′)| ≤L∥x−x′∥β; 140
(b) For any t >0,P(0< η∗(X)−ηs(X)< t)≤CTtγ,in which ηs(x)is the second largest one 141
among {η1(x), . . . , η K(x)}; 142
(c) The feature vector Xhas a probability density function (pdf) fwhich is bounded from below, i.e. 143
f(x)≥c; 144
(d) For all r < D ,Vr(x)≥θvdrd,in which Vr(x)is the volume (Lebesgue measure) of B(x, r)∩X, 145
vdis the volume of a unit ball. 146
Assumption 1 (a) requires that all ηjare Hölder continuous. This condition is common in literatures 147
about nonparametric statistics [28]. (b) is generalized from the Tsybakov noise assumption for binary 148
classification, which is commonly used in many existing works in the field of both nonparametric 149
classification [29, 37, 40, 41] and differential privacy [26, 27]. If K= 2, then η∗andηsrefer to the 150
larger and smaller class conditional probability, respectively. An intuitive understanding of (b) is that 151
in the majority of the support, the maximum value among {η1(x), . . . , η K(x)}should have some 152
gap to the second largest one. With sufficiently large sample size and model complexity, assumption 153
(b) ensures that for test samples within the majority of the support X, the algorithm is highly likely to 154
correctly identify the class with the maximum conditional probability. Therefore, in (b), we only care 155
about η∗(x)andηs(x), while other classes with small conditional probabilities can be ignored. (c) 156
is usually called "strong density assumption" in existing works [39, 40], which is quite strong. It is 157
possible to relax this assumption so that the theoretical analysis becomes suitable for general cases. 158
However, we do not focus on such generalization in this paper. Assumption (d) prevents the corner of 159
the support Xfrom being too sharp. In the remainder of this section, denote Fclsas the set of all 160
pairs (f, η)satisfying Assumption 1. 161
44 Classification 162
In this section, we derive the upper and lower bounds of learning under central and local label DP, 163
respectively. 164
4.1 Local Label DP 165
1) Lower bound. The following theorem shows the minimax lower bound, which characterizes the 166
theoretical limit. 167
Theorem 1. Denote Mϵas the set of all privacy mechanisms satisfying ϵ-local label DP (Definition 168
3). Then 169
inf
ˆYinf
M∈Mϵsup
(f,η)∈Fcls(Rcls−R∗
cls)≳
N 
ϵ2∧1−β(γ+1)
2β+d. (10)
Proof. (Outline) It suffices to derive (10) withK= 2. We convert the problem into multiple binary 170
hypothesis testing problems. In particular, we divide the support into Gbins. For some of them, we 171
construct two opposite hypotheses such that they are statistically not distinguishable. Our proof uses 172
some techniques in local DP [24] and some classical minimax theory [28]. The detailed proof is 173
shown in Appendix B. 174
In Theorem 1, (10) takes supremum over all joint distributions of (X, Y), and infimum over all 175
classifiers and privacy mechanisms satisfying ϵ-local label DP. 176
2) Upper bound. We then show that the bound (10) is achievable. Let the privacy mechanism M(x, y) 177
outputs a Kdimensional vector, with each component being either 0or1, such that 178
P(M(x, y)(j) = 1) =(
eϵ
2
eϵ
2+1ify=j
1
eϵ
2+1ify̸=j,(11)
andP(M(x, y)(j) = 0) = 1 −P(M(x, y)(j) = 1) , in which M(x, y)(j)is the j-th component of 179
M(x, y). ForNrandom training samples (Xi, Yi), letZi=M(Xi, Yi), and correspondingly, Zi(j) 180
is the j-th component of Zi. 181
Divide the support XintoGbins, named B1, . . . , B G, such that the length of each bin is h. 182
B1, . . . , B Gare disjoint, and these bins form a covering of X, i.e.X ⊂ ∪G
l=1Bl. Then calcu- 183
late 184
Slj=X
i:Xi∈BlZi(j), l= 1, . . . , G, j = 1, . . . , K, (12)
The classification within the l-th bin is 185
cl= arg max
jSlj, (13)
such that the the prediction given xisc(x) =clfor all x∈Bl. The next theorem shows the privacy 186
guarantee, as well as the bound of the excess risk. 187
Theorem 2. The privacy mechanism Misϵ-local label DP . Moreover, under Assumption 1, with 188
h∼ 
N(ϵ2∧1)/lnK−1
2β+d, the excess risk of the classifier described above can be upper bounded 189
as follows: 190
Rcls−R∗
cls≲N(ϵ2∧1)
lnK−β(γ+1)
2β+d
. (14)
Proof. (Outline) For privacy guarantee, we need to show that (11) is ϵ-local label DP: 191
P(M(x, y) =z)
P(M(x, y′) =z)= ΠK
j=1P(M(x, y)(j) =z(j))
P(M(x, y′)(j) =z(j))
=P(M(x, y)(y) =z(y))
P(M(x, y′)(y) =z(y))P(M(x, y)(y′) =z(y′))
P(M(x, y′)(y′) =z(y′))
≤eϵ
2eϵ
2=eϵ. (15)
5According to Definition 3, Misϵ-local label DP. For the performance guarantee (14), according to 192
Proposition 2, we need to bound η∗(x)−E[ηc(x)(x)]for each x. Ifη∗(x)−ηs(x)is large, then with 193
high probability, c(x) =c∗(x), and then η∗(x) =ηc(x)(x). Thus we mainly consider the case with 194
small η∗(x)−ηs(x). The details of proof are shown in Appendix C. 195
The lower bound (10) and the upper bound (14) match up to a logarithm factor, indicating that the 196
results are tight. Now we comment on the results. 197
Remark 1. 1)Comparison with non-private bound. The classical minimax lower bound for non- 198
private classification problem is N−β(γ+1)
2β+d. Therefore, the lower bound (10) reaches the non-private 199
bound with ϵ≳1. With small ϵ,Ntraining samples with privatized labels roughly equals Nϵ2200
non-privatized samples in terms of performance. 201
2)Comparison with local DP that protects both features and labels. In this case, the optimal 202
excess risk is (Nϵ2)−β(γ+1)/(2β+2d)∨N−β(γ+1)/(2β+d), which is worse than the right hand side of 203
(10). Such result indicates that compared with classical DP , label DP incurs significantly weaker 204
performance loss. 205
3)Comparison with other baseline methods. If we use the randomized response method instead 206
of the privacy mechanism (11), then the performance decreases sharply with the number of classes 207
K. Several methods have been proposed to improve the randomized response method, such as 208
RRWithPrior [14] and ALIBI [19]. However, these methods are not guaranteed in theory. 209
4.2 Central Label DP 210
1) Lower bound. The following theorem shows the minimax lower bound under the central label DP. 211
Theorem 3. Denote Aϵas the set of all learning algorithms satisfying ϵ-label DP (Definition 2). 212
Then 213
inf
A∈A ϵsup
(f,η)∈Fcls(Rcls−R∗
cls)≳N−β(γ+1)
2β+d+ (ϵN)−β(γ+1)
β+d. (16)
Proof. (Outline) Lower bounds under central DP are usually constructed by packing method [47], 214
which works for fixed output dimensions. However, to achieve a desirable bias and variance tradeoff, 215
the model complexity needs to increase with N. In our proof, we still divide the support into Gbins 216
and construct two hypotheses for each bin, but we develop a new tool (see Lemma 1) to give a lower 217
bound of the minimum error of hypothesis testing. We then use the group privacy property [4] to get 218
the overall lower bound. The details can be found in Appendix D. 219
2) Upper bound. Now we show that (16) is achievable. Similar to the local label DP problem, now 220
divide the support into Gbins, such that the length of each bin is h. Now the classification within the 221
l-th bin follows a exponential mechanism [56]: 222
P(cl=j|X1:N, Y1:N) =eϵnlj/2
PK
k=1eϵnlk/2, (17)
in which nlj=PN
i=11(Xi∈Bl, Yi=j). Then let c(x) =clforx∈Bl. The excess risk is 223
bounded in the next theorem. 224
Theorem 4. The privacy mechanism (17) isϵ-label DP . Moreover, under Assumption 1, if hscales as 225
h∼(lnK/ϵN )1
β+d+ (ln K/N )1
2β+d, then the excess risk can be bounded as follows: 226
R−R∗≲N
lnK−β(γ+1)
2β+d
+ϵN
lnK−β(γ+1)
β+d
. (18)
Proof. (Outline) The privacy guarantee of the exponential mechanism has been analyzed in [4]. 227
Following these existing analyses, it can be shown that (17) isϵ-label DP. It remains to show (18). 228
Note that if η∗(x)−ηs(x)is large, then the difference between the largest and the second largest 229
one from {nlj|j= 1, . . . , K }will also be large. From (17), the following inequality holds with high 230
probability: cl= arg maxjnlj= arg maxjηj(x) =c∗(x), which means that the classifier makes 231
6optimal prediction. Hence we mainly consider the case with small η∗(x)−ηs(x). The details of the 232
proof can be found in Appendix E. 233
The upper and lower bounds match up to logarithmic factors. In (18), the first term is just the 234
non-private convergence rate, while the second term (ϵN)−β(γ+1)
β+dcan be regarded as the additional 235
risk caused by the privacy mechanism. It decays faster with Ncompared with the first term, thus the 236
additional performance loss caused by the privacy mechanism becomes negligible as Nincreases. 237
This result is crucially different from the local model, under which the privacy mechanism always 238
induces higher sample complexity by a factor of O(1/(ϵ2∧1)). 239
5 Regression with Bounded Noise 240
Now we analyze the theoretical limits of regression problems under local and central label DP. 241
Throughout this section, we assume that the label is restricted within a bounded interval. 242
Assumption 2. Given any x∈ X, P(|Y|< T|X=x) = 1 . 243
Assumption 1 remains the same here. In the remainder of this section, denote Freg1as the set of 244
(f, η)that satisfies Assumption 1 and 2. 245
5.1 Local Label DP 246
1) Lower bound. Theorem 5 shows the minimax lower bound. 247
Theorem 5. Denote Mϵas the set of all privacy mechanisms satisfying ϵ-label DP . Then 248
inf
ˆηinf
M∈Mϵsup
(f,η)∈Freg1(Rreg−R∗
reg)≳(N(ϵ2∧1))−2β
d+2β. (19)
The proof of Theorem 5 is similar to that of Theorem 1, except for some details in hypotheses 249
construction and the final bound of excess risk. The details are shown in Appendix F. 250
2) Upper bound. The privacy mechanism is Z=Y+W, in which W∼Lap(2 T/ϵ). Then the 251
privacy mechanism satisfies ϵ-label DP. In this case, the real regression function η(x)can be estimated 252
using the nearest neighbor approach. Let 253
ˆη(x) =1
kX
i∈Nk(x)Zi, (20)
in which Nk(x)is the set of knearest neighbors of xamong X1, . . . ,XN. 254
Theorem 6. The method described above is ϵ-local label DP . Moreover, with k∼N2β
d+2β(ϵ∧1)−2d
d+2β, 255
then under Assumption 1 and 2, 256
Rreg−R∗
reg≲(N(ϵ2∧1))−2β
d+2β. (21)
Proof. (Outline) Since |Y|< T,W∼Lap(2 T/ϵ), it is obvious that Z=Y+Wisϵ-local label 257
DP. For the performance (21), the bias can be bounded by the knearest neighbor distances based on 258
Assumption 1(a). The variance of ˆη(x)scales inversely with k. An appropriate kcan be selected to 259
achieve a good tradeoff between bias and variance. The details are shown in Appendix G. 260
From standard minimax analysis on regression problems, the non-private convergence rate is 261
N−2β/(d+2β). From Theorem 5 and 6, the privatization process makes sample complexity larger by 262
aO(1/ϵ2)factor. 263
5.2 Central Label DP 264
1) Lower bound. The following theorem shows the minimax lower bound. 265
Theorem 7. LetAϵbe the set of all algorithms satisfying ϵ-central DP . Then 266
inf
A∈A ϵsup
(f,η)∈Freg1(Rreg−R∗
reg)≳N−2β
2β+d+ (ϵN)−2β
d+β. (22)
72) Upper bound. For each bin Bl, letnl=PN
i=11(Xi∈Bl)be the number of samples in Bl. If 267
nl>0, then 268
ˆηl=1
nlNX
i=11(Xi∈Bl)Yi+Wl, (23)
in which Wl∼Lap(2 /(nlϵ)). Ifnl= 0, i.e. no sample falls in Bl, then just let ˆηl= 0. For all 269
x∈Bl, letˆη(x) = ˆηl. The excess risk can be bounded with the following theorem. 270
Theorem 8. (23) isϵ-label DP . Moreover, under Assumption 1 and 2, if hscales as h∼N−1
2β+d+ 271
(ϵN)−1
d+β, then the excess risk is bounded by 272
R−R∗≲N−2β
2β+d+ (ϵN)−2β
d+β. (24)
The upper and lower bounds match, indicating that the results are tight. Again, the second term in 273
(24) converges faster than the first one with respect to N, the performance loss caused by privacy 274
constraints becomes negligible as Nincreases. 275
6 Regression with Heavy-tailed Noise 276
In this section, we consider the case such that the noise has tails. We make the following assumption. 277
Assumption 3. For all x∈ X,E[|Y|p|X=x]≤Mpfor some p≥2. 278
Instead of requiring |Y|< T for some T, now we only assume that the p-th order moment is bounded. 279
For non-private cases, given fixed noise variance, the tail does not affect the mean squared error of 280
regression. As a result, as long as p≥2, the convergence rate of regression risk is the same as the 281
case with bounded noise. However, the label DP requires the output to be insensitive to the worst 282
case replacement of labels, which can be harder if the noise has tails. To achieve ϵ-DP, the clipping 283
radius decreases with ϵ, thus the noise strength needs to grow faster than O(1/ϵ). As a result, the 284
convergence rate becomes slower than the non-private case. In the remainder of this section, denote 285
Freg2as the set of (f, η)that satisfies Assumption 1 and 3. 286
6.1 Local Label DP 287
1) Lower bound. In earlier sections about classification and regression with bounded noise, the impact 288
of privacy mechanisms is only a polynomial factor on ϵ, while the convergence rate of excess risk 289
with respect to Nis not changed. However, this rule no longer holds when the noise has heavy tails. 290
Theorem 9. Denote Mϵas the set of all privacy mechanisms satisfying ϵ-label DP . Then for small ϵ, 291
inf
ˆηinf
M∈Mϵsup
(f,η)∈F(Rreg−R∗
reg)≳(N(eϵ−1)2)−2β(p−1)
2pβ+d(p−1)+N−2β
2β+d. (25)
2) Upper bound. Since now the noise has unbounded distribution, without preprocessing, the 292
sensitivity is unbounded, thus simply adding noise to Ycan no longer protect the privacy. Therefore, 293
a solution is to clip Yinto[−T, T], and add noise proportional to T/ϵto achieve ϵ-local label DP. 294
Such truncation will inevitably introduce some bias. To achieve a tradeoff between clipping bias and 295
sensitivity, the value of Tneeds to be tuned carefully. Based on such intuition, the method is precisely 296
stated as follows. Let Zi=YTi+Wi, in which YTiis the truncation of Yi, i.e.YTi= (Yi∧T)∨(−T), 297
andW∼Lap(2 T/ϵ). The result is shown in the next theorem. 298
Theorem 10. The method above is ϵ-local label DP . Moreover, with k∼(Nϵ2)2pβ
2pβ+d(p−1)∨N2β
2β+d, 299
andT∼(kϵ2)1
2p, the risk is bounded by 300
Rreg−R∗
reg≲(Nϵ2)−2β(p−1)
2pβ+d(p−1)+N−2β
2β+d. (26)
Proof. (Outline) It can be shown that the clipping bias scales as T2(1−p). To meet the ϵ-label DP, an 301
additional error that scales as T/ϵis needed. By averaging over knearest neighbors, the variance 302
caused by noise Wscales with T2/(kϵ2). From standard analysis on nearest neighbor methods [29], 303
the non-private mean squared error scales as 1/k+ (k/N)2β/d. Put all these terms together, Theorem 304
10 can be proved. Details can be found in Appendix K. 305
8With the limit of p→ ∞ , the problem reduces to the case with bounded noise, and the growth rate of 306
kand the convergence rate of risk are the same as those in Theorem 6. For finite p,2β(p−1)/(2pβ+ 307
d(p−1))<2β/(2β+d), thus the convergence rate becomes slower due to the privacy mechanism. 308
6.2 Central Label DP 309
1) Lower bound. The minimax lower bound is shown in Theorem 11. 310
Theorem 11. The minimax lower bound is 311
inf
A∈A ϵsup
(f,η)∈Freg2(Rreg−R∗
reg)≳N−2β
2β+d+ (ϵN)−2β(p−1)
pβ+d(p−1) (27)
2) Upper bound. Now we derive the upper bound. To restrict the sensitivity, instead of estimating 312
with (23) directly, now we calculate an average of clipped label values: 313
ˆηl=1
nlNX
i=11(Xi∈Bl) Clip( Yi, T) +Wl, (28)
in which Wl∼Lap(2 T/(nlϵ)). Then for all x∈Bl, letˆη(x) = ˆηl. The following theorem bounds 314
the excess risk. 315
Theorem 12. (28) isϵ-label DP . Moreover, under Assumption 1 and 3, if handTscales as h∼ 316
N−1
2β+d+ (ϵN)−1
pβ+d(p−1), and T∼(ϵNhd)1/p, then the excess risk can be bounded by 317
Rreg−R∗
reg≲N−2β
2β+d+ (ϵN)−2β(p−1)
pβ+d(p−1). (29)
The proof of Theorem 11 and 12 follow that of Theorem 7 and 8. The details are shown in Appendix 318
L and M respectively. With p= 2, the right hand side of (29) becomes (ϵ∧1)−2β
2β+d, indicating that 319
the privacy constraint blows up the sample complexity by a constant factor. With larger p, the second 320
term in (29) becomes negligible compared with the first one. 321
The theoretical analyses in this section are summarized as follows. In general, with fixed noise 322
variance, if the label noise is heavy-tailed, while the non-private convergence rates remain unaffected, 323
the additional risk caused by privacy mechanisms becomes significantly higher, indicating the 324
difficulty of privacy protection for heavy-tailed distributions. 325
7 Conclusion 326
In this paper, we have derived the minimax lower bounds of learning under label DP for both central 327
and local models. Furthermore, we propose methods whose upper bounds match these lower bounds. 328
The results indicate the theoretical limits of learning under the label DP. From these results, it is 329
discovered that under local label DP constraints, the sample complexity blows up by a factor of at least 330
O(1/ϵ2). Under central label DP requirements, the additional error caused by privacy mechanisms 331
is significantly smaller. Finally, it is shown that for regression problem with heavy-tailed label 332
distribution, the additional risk induced by privacy requirement becomes inevitably higher. 333
Limitations: The limitations of our work include the following aspects. Some assumptions can 334
be weakened. For example, current analysis assumes that feature distributions have bounded sup- 335
ports, which may be extended to the unbounded case. One can let the bin splitting and nearest 336
neighbor method be adaptive in the tails of features, such as [41]. Moreover, the bounds derived in 337
this paper require that samples increase exponentially with dimensionality. However, in practice, 338
the performance of learning under the label DP can be quite well even in high dimensions. The 339
discrepancy can be explained by the fact that the minimax lower bound considers the worst-case 340
distribution over a wide range of distributions. However, in most realistic cases, the distributions 341
satisfy significantly better properties. A better modeling is to assume that these samples lie on a low 342
dimensional manifold [57, 58]. In this case, it is possible to achieve a much better convergence rate. 343
Finally, it is not sure whether approximate DP (i.e. (ϵ, δ)-DP) can improve the convergence rates. 344
9References 345
[1]Rao, B., J. Zhang, D. Wu, et al. Privacy inference attack and defense in centralized and federated 346
learning: A comprehensive survey. IEEE Transactions on Artificial Intelligence , 2024. 347
[2]Dwork, C., F. McSherry, K. Nissim, et al. Calibrating noise to sensitivity in private data analysis. 348
InTheory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, 349
NY, USA, March 4-7, 2006. Proceedings 3 , pages 265–284. Springer, 2006. 350
[3]Abadi, M., A. Chu, I. Goodfellow, et al. Deep learning with differential privacy. In Proceedings 351
of the 2016 ACM SIGSAC Conference on Computer and Communications Security , pages 352
308–318. 2016. 353
[4]Dwork, C., A. Roth, et al. The algorithmic foundations of differential privacy. Foundations and 354
Trends® in Theoretical Computer Science , 9(3–4):211–407, 2014. 355
[5]Bassily, R., A. Smith, A. Thakurta. Private empirical risk minimization: Efficient algorithms 356
and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of Computer 357
Science , pages 464–473. IEEE, 2014. 358
[6]Bassily, R., V . Feldman, K. Talwar, et al. Private stochastic convex optimization with optimal 359
rates. Advances in Neural Information Processing Systems , 32, 2019. 360
[7]Wang, D., H. Xiao, S. Devadas, et al. On differentially private stochastic convex optimization 361
with heavy-tailed data. In International Conference on Machine Learning , pages 10081–10091. 362
PMLR, 2020. 363
[8]Asi, H., V . Feldman, T. Koren, et al. Private stochastic convex optimization: Optimal rates in l1 364
geometry. In International Conference on Machine Learning , pages 393–403. PMLR, 2021. 365
[9]Das, R., S. Kale, Z. Xu, et al. Beyond uniform lipschitz condition in differentially private 366
optimization. In International Conference on Machine Learning , pages 7066–7101. PMLR, 367
2023. 368
[10] Tramer, F., D. Boneh. Differentially private learning needs better features (or much more data). 369
InInternational Conference on Learning Representations . 2021. 370
[11] Bu, Z., J. Mao, S. Xu. Scalable and efficient training of large convolutional neural networks 371
with differential privacy. Advances in Neural Information Processing Systems , 35:38305–38318, 372
2022. 373
[12] De, S., L. Berrada, J. Hayes, et al. Unlocking high-accuracy differentially private image 374
classification through scale. arXiv preprint arXiv:2204.13650 , 2022. 375
[13] Wei, J., E. Bao, X. Xiao, et al. Dpis: An enhanced mechanism for differentially private sgd with 376
importance sampling. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and 377
Communications Security , pages 2885–2899. 2022. 378
[14] Ghazi, B., N. Golowich, R. Kumar, et al. Deep learning with label differential privacy. Advances 379
in Neural Information Processing Systems , 34:27131–27145, 2021. 380
[15] McMahan, H. B., G. Holt, D. Sculley, et al. Ad click prediction: a view from the trenches. In 381
Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and 382
data mining , pages 1222–1230. 2013. 383
[16] McSherry, F., I. Mironov. Differentially private recommender systems: Building privacy into 384
the netflix prize contenders. In Proceedings of the 15th ACM SIGKDD international conference 385
on Knowledge discovery and data mining , pages 627–636. 2009. 386
[17] Bussone, A., B. Kasadha, S. Stumpf, et al. Trust, identity, privacy, and security considerations 387
for designing a peer data sharing platform between people living with hiv. Proceedings of the 388
ACM on Human-Computer Interaction , 4(CSCW2):1–27, 2020. 389
[18] Ghazi, B., P. Kamath, R. Kumar, et al. Regression with label differential privacy. In The 390
Eleventh International Conference on Learning Representations . 2022. 391
[19] Malek Esmaeili, M., I. Mironov, K. Prasad, et al. Antipodes of label differential privacy: Pate 392
and alibi. Advances in Neural Information Processing Systems , 34:6934–6945, 2021. 393
[20] Esfandiari, H., V . Mirrokni, U. Syed, et al. Label differential privacy via clustering. In 394
International Conference on Artificial Intelligence and Statistics , pages 7055–7075. PMLR, 395
2022. 396
10[21] Tang, X., M. Nasr, S. Mahloujifar, et al. Machine learning with differentially private labels: 397
Mechanisms and frameworks. Proceedings on Privacy Enhancing Technologies , 2022. 398
[22] Cover, T. M. Elements of information theory . John Wiley & Sons, 1999. 399
[23] Duchi, J. C., M. I. Jordan, M. J. Wainwright. Local privacy and statistical minimax rates. In 400
2013 IEEE 54th Annual Symposium on Foundations of Computer Science , pages 429–438. 401
IEEE, 2013. 402
[24] —. Minimax optimal procedures for locally private estimation. Journal of the American 403
Statistical Association , 113(521):182–201, 2018. 404
[25] Gopi, S., G. Kamath, J. Kulkarni, et al. Locally private hypothesis selection. In Conference on 405
Learning Theory , pages 1785–1816. PMLR, 2020. 406
[26] Berrett, T., C. Butucea. Classification under local differential privacy. arXiv preprint 407
arXiv:1912.04629 , 2019. 408
[27] Berrett, T. B., L. Györfi, H. Walk. Strongly universally consistent nonparametric regression and 409
classification with privatised data. Electronic Journal of Statistics , 15:2430–2453, 2021. 410
[28] Tsybakov, A. B. Introduction to Nonparametric Estimation . 2009. 411
[29] Audibert, J.-Y ., A. B. Tsybakov. Fast learning rates for plug-in classifiers. Annals of Statistics , 412
2007. 413
[30] Warner, S. L. Randomized response: A survey technique for eliminating evasive answer bias. 414
Journal of the American Statistical Association , 60(309):63–69, 1965. 415
[31] Badanidiyuru Varadaraja, A., B. Ghazi, P. Kamath, et al. Optimal unbiased randomizers for 416
regression with label differential privacy. Advances in Neural Information Processing Systems , 417
36, 2023. 418
[32] LeCam, L. Convergence of estimates under dimensionality restrictions. The Annals of Statistics , 419
pages 38–53, 1973. 420
[33] Verdú, S., et al. Generalizing the fano inequality. IEEE Transactions on Information Theory , 421
40(4):1247–1251, 1994. 422
[34] Assouad, P. Deux remarques sur l’estimation. Comptes rendus des séances de l’Académie des 423
sciences. Série 1, Mathématique , 296(23):1021–1024, 1983. 424
[35] Yang, Y . Minimax nonparametric classification. i. rates of convergence. IEEE Transactions on 425
Information Theory , 45(7):2271–2284, 1999. 426
[36] —. Minimax nonparametric classification. ii. model selection for adaptation. IEEE Transactions 427
on Information Theory , 45(7):2285–2292, 1999. 428
[37] Chaudhuri, K., S. Dasgupta. Rates of convergence for nearest neighbor classification. Advances 429
in Neural Information Processing Systems , 27, 2014. 430
[38] Yang, Y ., S. T. Tokdar. Minimax-optimal nonparametric regression in high dimensions. The 431
Annals of Statistics , pages 652–674, 2015. 432
[39] Döring, M., L. Györfi, H. Walk. Rate of convergence of k-nearest-neighbor classification rule. 433
Journal of Machine Learning Research , 18(227):1–16, 2018. 434
[40] Gadat, S., T. Klein, C. Marteau. Classification in general finite dimensional spaces with the 435
k-nearest neighbor rule. Annals of Statistics , 2016. 436
[41] Zhao, P., L. Lai. Minimax rate optimal adaptive nearest neighbor classification and regression. 437
IEEE Transactions on Information Theory , 67(5):3155–3182, 2021. 438
[42] Kasiviswanathan, S. P., H. K. Lee, K. Nissim, et al. What can we learn privately? SIAM Journal 439
on Computing , 40(3):793–826, 2011. 440
[43] Li, M., T. B. Berrett, Y . Yu. On robustness and local differential privacy. The Annals of Statistics , 441
51(2):717–737, 2023. 442
[44] Feldman, V ., T. Koren, K. Talwar. Private stochastic convex optimization: optimal rates in linear 443
time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing , 444
pages 439–449. 2020. 445
[45] Duchi, J., R. Rogers. Lower bounds for locally private estimation via communication complexity. 446
InConference on Learning Theory , pages 1161–1191. PMLR, 2019. 447
11[46] Huang, Z., Y . Liang, K. Yi. Instance-optimal mean estimation under differential privacy. 448
Advances in Neural Information Processing Systems , 34:25993–26004, 2021. 449
[47] Hardt, M., K. Talwar. On the geometry of differential privacy. In Proceedings of the forty-second 450
ACM symposium on Theory of computing , pages 705–714. 2010. 451
[48] Bun, M., G. Kamath, T. Steinke, et al. Private hypothesis selection. Advances in Neural 452
Information Processing Systems , 32, 2019. 453
[49] Narayanan, S. Better and simpler lower bounds for differentially private statistical estimation. 454
arXiv preprint arXiv:2310.06289 , 2023. 455
[50] Kamath, G., V . Singhal, J. Ullman. Private mean estimation of heavy-tailed distributions. In 456
Conference on Learning Theory , pages 2204–2235. PMLR, 2020. 457
[51] Kamath, G., J. Li, V . Singhal, et al. Privately learning high-dimensional distributions. In 458
Conference on Learning Theory , pages 1853–1902. PMLR, 2019. 459
[52] Alabi, D., P. K. Kothari, P. Tankala, et al. Privately estimating a gaussian: Efficient, robust, and 460
optimal. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing , pages 461
483–496. 2023. 462
[53] Arbas, J., H. Ashtiani, C. Liaw. Polynomial time and private learning of unbounded gaussian 463
mixture models. In International Conference on Machine Learning , pages 1018–1040. 2023. 464
[54] Bun, M., J. Ullman, S. Vadhan. Fingerprinting codes and the price of approximate differential 465
privacy. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing , 466
pages 1–10. 2014. 467
[55] Kamath, G., A. Mouzakis, V . Singhal. New lower bounds for private estimation and a generalized 468
fingerprinting lemma. Advances in neural information processing systems , 35:24405–24418, 469
2022. 470
[56] McSherry, F., K. Talwar. Mechanism design via differential privacy. In 48th Annual IEEE 471
Symposium on Foundations of Computer Science (FOCS’07) , pages 94–103. IEEE, 2007. 472
[57] Kpotufe, S. k-nn regression adapts to local intrinsic dimension. Advances in neural information 473
processing systems , 24, 2011. 474
[58] Carter, K. M., R. Raich, A. O. Hero III. On local intrinsic dimension estimation and its 475
applications. IEEE Transactions on Signal Processing , 58(2):650–663, 2009. 476
12A Proof of Proposition 2 477
From (5) and (6), the Bayes risk is 478
R∗
cls=P(Y̸=c∗(X)) =Z
P(Y̸=c∗(x)|X=x)f(x)dx=Z
(1−η∗(x))f(x)dx. (30)
The risk of classifier cis 479
Rcls=P(Y̸=c(X)) =EZ 
1−ηc(x)(x)
f(x)dx
. (31)
From (31) and (6), 480
Rcls−R∗
cls=Z
(η∗(x)−E[ηc(x)(x)])f(x)dx. (32)
The proof is complete. 481
B Proof of Theorem 1 482
In this section, we prove the minimax lower bound of multi-class classification. The problem with K 483
classes with K > 2is inherently harder than that with K= 2. Therefore, we just need to prove the 484
lower bound for binary classification, in which Y={1,2}. Let 485
η(x) =η2(x)−η1(x). (33)
Since η1(x) +η2(x) = 1 always holds, we have 486
η1(x) =1−η(x)
2, η2(x) =1 +η(x)
2. (34)
Therefore, η(x)captures the conditional distribution of Ygiven x. 487
FindGdisjoint cubes B1, . . . , B G⊂ X , such that the length of each cube is h. Denote c1, . . . ,cG 488
as the centers of these cubes. Let ϕ(u)be some function supported at [−1/2,1/2]d, such that 489
0≤ϕ(u)≤1. (35)
Letf(x) =coverx∈ X. Forv∈ V:={−1,1}m, let 490
ηv(x) =mX
k=1vkϕx−ck
h
hβ. (36)
It can be proved that if for some constant CM, 491
m≤CMhγβ−d, (37)
then for any η=ηv,η1andη2satisfies Assumption 1(b). Denote 492
ˆvk= arg max
s∈{−1,1}Z
Bkϕx−ck
h
1(sign(ˆ η(x)) =s)f(x)dx. (38)
Then the excess risk is bounded by 493
R−R∗=Z
|ηv(x)|P(sign(ˆ η(x))̸= sign( ηv(x)))f(x)dx
≥mX
k=1Z
Bk|ηv(x)|P(sign(ˆ η(x))̸= sign( ηv(x)))f(x)dx
=mX
k=1hβZ
Bkϕx−ck
h
P(sign(ˆ η(x)))f(x)dx. (39)
Ifˆvk̸=vk, then from (38), 494
Z
Bkϕx−ck
h
1(sign(ˆ η(x)))f(x)dx≥Z
Bkϕx−ck
h
1(sign(ˆ η(x)) =vk)f(x)dx.(40)
13Therefore 495Z
Bkϕx−ck
h
1(sign(ˆ η(x))̸=vk)f(x)dx≥1
2Z
Bkϕx−ck
h
f(x)dx≥1
2chd∥ϕ∥1.(41)
Hence 496
R−R∗≥1
2chβ+d∥ϕ∥1mX
k=1P(ˆvk̸=vk)
=1
2chβ+d∥ϕ∥1E[ρH(ˆv,v)], (42)
in which ρHdenotes the Hamming distance. Then 497
inf
ˆYinf
M∈Mϵsup
(f,η)∈P(R−R∗)≥1
2hβ+d∥ϕ∥1inf
ˆvinf
M∈Mϵmax
v∈VE[ρH(ˆv,v)]. (43)
Define 498
δ= sup
M∈Mϵmax
v,v′:ρH(v,v′)=1DKL(P(X,Z)1:N|v||P(X,Z)1:N|v′), (44)
in which P(X,Z)1:N|vdenotes the distribution of (X1, Z1), . . . , (XN, ZN)with η=ηv.DKL 499
denotes the Kullback-Leibler divergence. Then from [28], Theorem 2.12(iv), 500
inf
ˆvinf
Mmax
v∈VE[ρH(ˆv,v)]≥m
2 
1
2e−δ,1−r
δ
2!
. (45)
It remains to bound δ. Without loss of generality, suppose v1̸=v′
1, and vi=v′
ifori̸= 1. Then 501
DKL(P(X,Z)1:N|v||P(X,Z)1:N|v′)(a)=NDKL(PX,Z|v||PX,Z|v′)
(b)=NZ
B1f(x)DKL(PZ|X=x,v||PZ|X=x,v′)dx
(c)
≤NZ
B1f(x)(eϵ−1)2TV2(PZ|X=x,v, PZ|X=x,v′)dx
=NZ
B1f(x)(eϵ−1)2η2
v(x)dx
=N(eϵ−1)2Z
B1f(x)ϕ2x−c1
h
h2βdx
(d)=N(eϵ−1)2h2β+d∥ϕ∥2
2. (46)
In (a), PX,Z|vdenotes the distribution of a single sample with privatized label (X, Z), with η=ηv. 502
In (b), PZ|X=x,vdenotes the conditional distribution of Zgiven X=x, with η=ηv. (c) uses [24], 503
Theorem 1. In (d), ∥ϕ∥2
2=R
ϕ2(u)du, which is a constant. Moreover, 504
DKL(PX,Z|v||PX,Z|v′)(a)
≤DKL(PX,Y|v||PX,Y|v′)
=Z
B1f(x)
P(Y= 1|v) lnP(Y= 1|v)
P(Y= 1|v′)+P(Y=−1|v) lnP(Y=−1|v)
P(Y=−1|v′)
dx
=Z
B1f(x)1 +ηv(x)
2ln1 +ηv(x)
1−ηv(x)+1−ηv(x)
2ln1−ηv(x)
1 +ηv(x)
dx
(b)
≤3Z
B1f(x)η2
v(x)dx
≤3h2β+d∥ϕ∥2
2. (47)
For (a), note that Zis generated from Y. From data processing inequality, (a) holds. For (b), without 505
loss of generality, suppose that v1= 1, thus ηv(x)≥0inB1. Then ln(1 + ηv(x))≤ηv(x). From 506
(35) and (36), |ηv(x)| ≤1/2. Therefore, −ln(1−ηv(x))≤2ηv(x). Therefore (b) holds. 507
14From (46) and (47), 508
δ≤N
(eϵ−1)2∧3
h2β+d∥ϕ∥2
2. (48)
Let 509
h∼ 
N 
ϵ2∧1−1
2β+d. (49)
Then δ≲1. From (45), with m∼hγβ−d, 510
inf
ˆvinf
M∈Mϵmax
v∈VE[ρH(ˆv,v)]≳hγβ−d. (50)
Hence 511
inf
ˆYinf
M∈Mϵsup
(f,η)∈P(R−R∗)≳hβ+dhγβ−d∼hβ(γ+1)∼
N 
ϵ2∧1−β(γ+1)
2β+d. (51)
The proof is complete. 512
C Proof of Theorem 2 513
Denote 514
nl=NX
i=11(Xi∈Bl), (52)
and for Z=M(X, Y), let 515
˜ηj(x) := E[Z(j)|X=x]
=eϵ
2
eϵ
2+ 1ηj(x) +1
eϵ
2+ 1(1−ηj(x)) (53)
as the number of training samples whose feature vectors fall in Bl, and 516
vlj:=1
nlX
i:Xi∈Bl˜ηj(Xi). (54)
Recall (12) that defines Slj. From Hoeffding’s inequality, 517
P(|Slj−nlvlj|> t|X1:N)≤2 exp
−2t2
nl
, (55)
in which X1:Ndenotes X1, . . . ,XN. 518
Define 519
v∗
l:= max
jvlj, (56)
and 520
c∗
l:= arg max
jvlj. (57)
Now we bound P(v∗
l−vlcl> t), in which clis defined in (13).clcan be viewed as the prediction at 521
thel-th bin. We would like to show that the even if the prediction is wrong, the value (i.e. conditional 522
probability) of the predicted class is close to the ground truth. v∗
l−vlcl> tonly if ∃j,v∗
l−vlj> t, 523
andSlj> Slc∗
l. Therefore either Slj−nlvlj> t/2orSlc∗
l−nlv∗
l> t/2holds. Hence 524
P(v∗
l−vlcl≥t)≤P
∃j,|Slj−nlvlj| ≥1
2nlt
≤2Kexp
−1
2nlt2
. (58)
Define 525
t0=s
2 ln(2 K)
nl. (59)
15Then 526
v∗
l−E[vlcl|X1:N] =Z1
0P(v∗
l−vlcl> t)dt
≤t0+Z∞
t02Kexp
−1
2nlt2
dt
(a)
≤t0+ 2r
2π
nlKexp
−1
2nlt2
0
=s
2 ln(2 K)
nl+r
2π
nl
≤3s
ln(2K)
nl. (60)
In (a), we use the inequality 527Z∞
te−u2
2σ2du≤√
2πσe−t2
2σ2. (61)
Now we bound the excess risk. 528
R−R∗=Z 
η∗(x)−E[ηc(x)(x)]
f(x)dx
=GX
l=1Z
Bl 
η∗(x)−E[ηc(x)(x)]
f(x)dx. (62)
We need to boundR
Bl 
η∗(x)−E[ηc(x)(x)]
f(x)dxfor each l. From Assumption 1(a), for any 529
x,x′∈Bl, the distance is bounded by ∥x−x′∥ ≤√
dL. Thus 530
|ηj(x)−ηj(x′)| ≤Ldhβ, (63)
in which Ldis defined as Ld:=L√
d. From (63) and (53), 531
|˜ηj(x)−˜ηj(x′)| ≤eϵ
2−1
eϵ
2+ 1Ldhβ. (64)
Define 532
˜η∗(x) = max
j˜ηj(x), (65)
then 533
η∗(x)−E[ηcl(x)|X1:N]≤eϵ
2+ 1
eϵ
2−1(˜η∗(x)−E[˜ηcl(x)|X1:N])
≤eϵ
2+ 1
eϵ
2−1(v∗
l−E[vlcl|X1:N]) + 2 Ldhβ
≤3eϵ
2+ 1
eϵ
2−1s
2 ln(2 K)
nl+ 2Ldhβ. (66)
Take integration over cube Bl, we get 534Z
Bl(η∗(x)−E[ηcl(x)])f(x)dx
≤P
nl<1
2Np(Bl)Z
Bl
η∗(x)−E[ηcl(x)|nl<1
Np(Bl)]
f(x)dx
+Z
Bl
η∗(x)−E[ηcl(x)|nl≥1
Np(Bl)]
f(x)dx
≤p(Bl)e−1
2(1−ln 2)Np(Bl)+"
3eϵ
2+ 1
eϵ
2−1s
2 ln(2 K)
Np(Bl)+ 2Ldhβ#
p(Bl), (67)
16in which p(Bl) =P(X∈Bl)is the probability mass of Bl. Moreover, define 535
∆l= inf
x∈Bl(η∗(x)−ηs(x)), (68)
and 536
˜∆l= inf
x∈Bl(˜η∗(x)−˜ηs(x)) =eϵ
2−1
eϵ
2+ 1∆l, (69)
in which the ˜ηsis the second largest value of ˜ηjamong j= 1, . . . , K , which follows the definition 537
ofηs. 538
If∆l>0, then c∗(x)is the same over Bl. Then either v∗
l−vlcl= 0orv∗
l−vlcl≥∆lholds. Hence 539
˜η∗(x)−E[˜ηcl(x)|X1:N]
=Z1
0P(˜η∗(x)−˜ηcl(x)> t|X1:N)dt
≤Z1
0P
v∗
l−vlcl> t−2Ldhβeϵ
2+ 1
eϵ
2−1|X1:N
dt
≤Z˜∆l+2Ldhβ
0P(v∗
l−vlcl≥∆l)dt+Z∞
˜∆l+2Ldhβ2Kexp
−1
2nl(t−2Ldhβ)2
dt
≤2Kexp
−1
2nl˜∆2
l
(˜∆l+ 2Ldhβeϵ
2+ 1
eϵ
2−1) + 2Kr
2π
nlexp
−1
2nl˜∆2
l
=
2K
˜∆l+ 2Ldhβeϵ
2+ 1
eϵ
2−1
+ 2Kr
2π
nl
exp
−1
2nl˜∆2
l
. (70)
Take expectation over X1:N, we get 540
Z
Bl(η∗(x)−E[ηcl(x)])f(x)dx≤p(Bl)e−1
2(1−ln 2)Np(Bl)
+2Kp(Bl) 
∆l+ 2Ldhβ+eϵ
2+ 1
eϵ
2−1s
2π
Np(Bl)!
exp"
−1
2Np(Bl)∆2
leϵ
2−1
eϵ
2+ 12#
.(71)
Define 541
al="
3eϵ
2+ 1
eϵ
2−1r
2 ln(2 K)
cNhd+ 2Ldhβ#
p(Bl), (72)
and 542
bl= 2Kp(Bl) 
∆l+ 2Ldhβ+eϵ
2+ 1
eϵ
2−1r
2π
cNhd!
exp"
−1
2cNhd∆2
leϵ
2−1
eϵ
2+ 12#
. (73)
From Assumption 1(c), p(Bl)≥cNhd. Therefore, from (67) and (71) 543
R−R∗≤GX
l=1h
p(Bl)e−1
2(1−ln 2)Np(Bl)+ min {al, bl}i
≤e−1
2(1−ln 2)cNhd+GX
l=1min{al, bl}. (74)
It remains to boundPG
l=1min{al, bl}. Note that for all x∈Bl,η∗(x)−ηs(x)≤∆l+ 2Ldhβ. 544
Thus 545
X
l:∆l≤up(Bl)≤P 
η∗(X)−ηs(X)≤u+ 2Ldhβ
≤M(u+ 2Ldhβ)γ. (75)
17Let 546
∆0=eϵ
2+ 1
eϵ
2−1r
2 ln(2 K)
cNhd, (76)
and 547
I0={l|∆l≤∆0}, (77)
Ik={l|2k−1∆0<∆l≤2k∆0}, k= 1,2, . . . (78)
Then 548
min
l∈I0{al, bl} ≤X
l∈I0al
≤
X
l:∆l≤∆0p(Bl)
"
3eϵ
2+ 1
eϵ
2−1r
2 ln(2 K)
cNhd+ 2Ldhβ#
≤M(∆0+ 2Ldhβ)γ"
3eϵ
2+ 1
eϵ
2−1r
2 ln(2 K)
cNhd+ 2Ldhβ#
≲1
ϵ2∧1lnK
Nhdγ+1
2
+hβ(γ+1). (79)
ForIkwithk≥1, 549
min
l∈Ik{al, bl} ≤X
l∈Ikbl
≤
X
l:∆l≤2k∆0p(Bl)
·2K 
2k∆0+ 2Ldhβ+ ∆ 0
exp"
−1
2eϵ
2−1
eϵ
2+ 12
cNhd22k−2∆2
0#
≤M(2k∆0+ 2Ldhβ)γ 
(2k+ 1)∆ 0+ 2Ldhβ
(2K)−22k−2+1
≤M(∆0+ 2Ldhβ)γ+12kγ+k−22k−2+2. (80)
It is obvious that there exists a finite constant C′<∞that depends on γ, such that 550
∞X
k=12kγ+k−22k−2+2≤C′. (81)
Therefore 551
∞X
k=1X
l∈Ikmin{al, bl}≲1
ϵ2∧1lnK
Nhdγ+1
2
+hβ(γ+1). (82)
Combine (74), (79) and (82), 552
R−R∗≲1
ϵ2∧1lnK
Nhdγ+1
2
+hβ(γ+1). (83)
To minimize the overall excess risk, let 553
h∼N(ϵ2∧1)
lnK−1
2β+d
, (84)
then 554
R−R∗≲N(ϵ2∧1)
lnK−β(γ+1)
2β+d
. (85)
Compare to the simple random response method, the bin splitting avoids the polynomial decrease 555
overK. 556
18D Proof of Theorem 3 557
We still divide the support as the local label DP setting, except that the value of his different, which 558
will be specified later in this section. Note that (42) still holds here. Let Vtakes values from 559
{−1,1}mrandomly with equal probability, and Vkis the k-th element. Then ηV(x)is a random 560
function. The corresponding random output of hypothesis testing is denoted as ˆVk, which is calculated 561
by (38). Then 562
inf
A∈A ϵsup
(f,η)∈Fcls(R−R∗)≥1
2chβ+d∥ϕ∥1inf
A∈A ϵmax
v∈VmX
k=1P(ˆvk̸=vk)
≥1
2hβ+d∥ϕ∥1inf
A∈A ϵmX
k=1P(ˆVk̸=Vk)
=1
2hβ+d∥ϕ∥1mX
k=1inf
A∈A ϵP(ˆVk̸=Vk), (86)
in which the last step holds since ˆVkfor different kare calculated independently. 563
It remains to give a lower bound of P(ˆVk̸=Vk). Denote nkas the number of samples falling in Bk, 564
¯Ykas the average label values in Bk: 565
nk:=NX
i=11(Xi∈Bk), (87)
¯Yk:=1
nkNX
i=1Yi1(Xi∈Bk). (88)
Moreover, define 566
ak:=1
nkNX
i=1|η(Xi)|1(Xi∈Bk)
=hβ
nkNX
i=1ϕXi−ck
h
1(Xi∈Bk), (89)
in which the last step comes from (36). Then 567
E[¯Yk|X1:N, Vk] =Vkak, (90)
in which X1:Nmeans X1, . . . ,XN. We then show the following lemma. 568
Lemma 1. If0≤t≤ln 2/(ϵnk), and nktis an integer, then 569
P(ˆVk= 1|X1:N,¯Yk=−t) +P(ˆVk=−1|X1:N,¯Yk=t)≥2
3. (91)
Proof. Construct D′by changing the label values of l=nktitems from these nksamples falling in 570
Bk, from −1to1. Then the average label values in Bkis denoted as ¯Y′
kafter such replacement. ˆVk 571
also becomes ˆV′
k. Then from the ϵ-label DP requirement, 572
P(ˆVk= 1|X1:N,¯Yk=−t)(a)
≥e−lϵP
ˆV′
k= 1|X1:N,¯Y′
k=−t+2l
nk
(b)
≥e−lϵP
ˆVk= 1|X1:N,¯Yk=−t+2l
nk
≥e−nktϵ
1−P
ˆVk=−1|X1:N,¯Yk=−t+2l
nk
≥1
2h
1−P
ˆVk=−1|X1:N,¯Yk=ti
. (92)
19in which (a) uses the group privacy property. The Hamming distance between DandD′isl, thus the 573
ratio of probability between DandD′is within [e−lϵ, elϵ]. (b) holds because the algorithm does not 574
change after changing DtoD′. Similarly, 575
P(ˆVk=−1|X1:N,¯Yk=t)≥1
2h
1−P
ˆVk= 1|X1:N,¯Yk=−ti
. (93)
Then (91) can be shown by adding up (92) and (93). 576
Now we use Lemma 1 to bound the excess risk. With sufficiently large nk,ˆYkwill be close to 577
Gaussian distribution with mean ak. To be more rigorous, by Berry-Esseen theorem [ ?], for some 578
absolute constant CE, 579
P ¯Yk≤ak|X1:N, Vk= 1
≥1
2−CE√nk. (94)
Similarly, 580
P ¯Yk≥ −ak|X1:N, Vk=−1
≥1
2−CE√nk. (95)
We first analyze cubes with 581
nk>16C2
E, ak<ln 2
ϵnk. (96)
Under condition (96), the right hand side of (94) and (95) are at least 1/4. Therefore 582
P(ˆVk̸=Vk|X1:N) =1
2P(ˆVk= 1|X1:N, Vk=−1) +1
2P(ˆVk=−1|X1:N, Vk= 1)
≥1
8P
ˆVk= 1|X1:N,¯Yk≥ −ln 2
ϵnk
+1
8P
ˆVk=−1|X1:N,¯Yk≤ln 2
ϵnk
≥1
12. (97)
From (86), 583
inf
A∈A ϵsup
(f,η)∈Fcls(R−R∗)≥1
2hβ+d∥ϕ∥1mX
k=11
12P
ak<ln 2
ϵnk, nk>16C2
E
(98)
From (35), (89) and (87), ak≤hβ. Therefore 584
inf
A∈A ϵsup
(f,η)∈Fcls(R−R∗)≥1
24hβ+d∥ϕ∥1mX
k=1P
16C2
E< nk<ln 2
ϵhβ
. (99)
Recall that each cube has probability mass chd. Select hsuch that 585
2Nchd=ln 2
ϵhβ. (100)
From Chernoff inequality, 16C2
E< nk<ln 2/(ϵhβ)holds with high probability. (100) yields 586
h∼(ϵN)−1
d+β. (101)
Recall the bound of min (37). Let m∼hγβ−d, then (99) becomes 587
inf
A∈A ϵsup
(f,η)∈Fcls(R−R∗)≳hβ(γ+1)
≳(ϵN)−β(γ+1)
d+β. (102)
Moreover, the standard lower bound for classification [28] is 588
inf
A∈A ϵsup
(f,η)∈Fcls(R−R∗)≳N−β(γ+1)
2β+d. (103)
Therefore 589
inf
A∈A ϵsup
(f,η)∈Fcls(R−R∗)≳N−β(γ+1)
2β+d+ (ϵN)−β(γ+1)
d+β. (104)
20E Proof of Theorem 4 590
Denote 591
n∗
l= max
jnlj, (105)
592
nl:=KX
j=1nlj=NX
i=11(Xi∈Bl). (106)
For all jsuch that n∗
l−nlj> t, 593
P(cl=j|X1:N, Y1:N) =eϵnlj/2
PK
k=1eϵnlk/2
≤eϵn∗
l/2
PK
k=1eϵnlk/2e−1
2ϵt
≤e−1
2ϵt. (107)
Therefore 594
P(n∗
l−nlcl> t) =X
j:n∗
l−nlj>tP(cl=j|X1:N, Y1:N)≤Ke−1
2ϵt. (108)
Hence 595
E[n∗
l−nlcl] =Z∞
0P(n∗
l−nlj> t)dt
≤Z2 lnK/ϵ
01dt+Z∞
2 lnK/ϵKe−1
2ϵtdt
=2
ϵ(lnK+ 1). (109)
Define 596
vlj=1
nlNX
i=11(Xi∈Bl)ηj(Xi), (110)
then 597
E[nlj|X1:N] =nlvlj. (111)
From Hoeffding’s inequality, 598
P(|nlj−nlvlj|> t)≤2e−1
2nlt2
. (112)
Thus 599
E
max
j|nlj−nlvlj|
=Z∞
0P 
∪K
j=1{|nlj−nlvlj|> t}
dt
≤Z∞
0min
1,2Ke−1
2nlt2
dt
=p
2nlln(2K) +Z∞
√
2nlln(2K)2Ke−1
2nlt2
dt
<2p
2nlln(2K), (113)
in which the last step uses the inequalitR∞
te−u2/(2σ2)du≤√
2πσe−t2/(2σ2). Then 600
E[v∗
l−vlcl|X1:N] =1
nlE[nlv∗
l−nlvlcl]
=1
nlE[n∗
l−nlcl+nlv∗
l−n∗
l+nlcl−nlvlcl]
≤1
nlE[n∗
l−nlcl] +2
nlE
max
j|nlj−nlvlj|
≤2
ϵnl(lnK+ 1) + 4s
2 ln(2 K)
nl. (114)
21By Hölder continuity assumption (Assumption 1(a)), for x∈Bl, 601
|vlj−ηj(x)| ≤1
nlNX
i=11(Xi∈Bl)|ηj(Xi)−ηj(x)| ≤Ldhβ, (115)
in which Ld=L√
d,Lis the constant in Assumption 1(a). Thus 602
E[η∗(x)−ηcl(x)|X1:N]≤2
ϵnl(lnK+ 1) + 4s
2 ln(2 K)
nl+ 2Ldhβ. (116)
Now take integration over Bl. 603
Z
Bl(η∗(x)−E[ηcl(x)])f(x)dx
≤P
nl<1
2Np(Bl)Z
Bl
η∗(x)−E
ηcl(x)|nl<1
2Np(Bl)
f(x)dx
+Z
Bl
η∗(x)−E
ηcl(x)|nl≥1
2Np(Bl)
f(x)dx
≤p(Bl) exp
−1
2(1−ln 2)Np(Bl)
+"
2(lnK+ 1)
ϵNp(Bl)+ 4s
2 ln(2 K)
Np(Bl)+ 2Ldhβ#
p(Bl),
(117)
in which p(Bl) =P(X∈Bl) =R
Blf(x)dx.(117) is the central label DP counterpart of (67). The 604
remainder of the proof follows arguments of the local label DP. We omit detailed steps. The result is 605
R−R∗≲ 
lnK
ϵNhd+r
lnK
Nhd+hβ!γ+1
. (118)
Let 606
h∼lnK
ϵN 1
β+d
+lnK
N 1
2β+d
, (119)
then 607
R−R∗≲lnK
ϵNβ(γ+1)
β+d
+lnK
Nβ(γ+1)
2β+d
. (120)
The proof is complete. 608
F Proof of Theorem 5 609
FindGcubes in the support and the length of each cube is h. Letϕ(u)be the same as the classification 610
case shown in appendix B. For v∈ V:={−1,1}G, let 611
ηv(x) =KX
k=1vkϕx−ck
h
hβ. (121)
Let P(Y= 1|x) = (1 + ηv(x))/2, P(Y=−1|x) = (1 −ηv(x)), then η(x) =E[Y|x] =ηv(x). 612
The overall volume of the support is bounded. Thus, we have 613
G≤CGh−d(122)
for some constant CG. 614
Denote 615
ˆvk= signZ
Bkˆη(x)ϕx−ck
h
f(x)dx
, (123)
22then the excess risk is bounded by 616
R=Eh
(ˆη(X)−ηv(X))2i
=KX
k=1Z
BkE
(ˆη(x)−ηv(x))2
f(x)dx. (124)
Ifˆvk̸=vk, from (123), 617
Z
Bk
ˆη(x)−vkϕx−ck
h
hβ2
f(x)dx≥Z
Bk
ˆη(x) +vkϕx−ck
h
hβ2
f(x)dx.(125)
Therefore, if ˆvk̸=vk, then 618
Z
Bk(ˆη(x)−ηv(x))2dx≥1
2Z
Bkϕ2x−ck
h
h2βf(x)dx=1
2ch2β+d∥ϕ∥2
2. (126)
Therefore 619
R−R∗≥E1
2ch2β+d∥ϕ∥2
21(ˆvk̸=vk)
=1
2ch2β+d∥ϕ∥2
2E[ρH(ˆv,v)]. (127)
Similar to the classification problem analyzed in Appendix B, let 620
h∼ 
N(ϵ∧1)2−1
2β+d, (128)
thenδ≲1, and 621
inf
ˆvsup
M∈Mϵmax
v∈VE[ρH(ˆv,v)]≳G∼h−d. (129)
Thus 622
inf
ˆηinf
M∈Mϵsup
PX,Y∈Freg1R≳h2η+dh−d∼h2β∼(N(ϵ∧1)2)−2β
2β+d. (130)
G Proof of Theorem 6 623
According to Assumption 2, |Y|< T with probability 1, thus Var[Y|x]≤T2for any x. A Laplacian 624
distribution with parameter λhas variance 2λ2, thus 625
Var[W] = 2λ2= 22T
ϵ2
=8T2
ϵ2. (131)
Hence 626
Var[Z] = Var[ Y] + Var[ W]≤T2
1 +8
ϵ2
. (132)
Now we analyze the bias first. 627
E[ˆη(x)] =E
1
kX
i∈Nk(x)Zi
=E
1
kX
i∈Nk(x)η(Xi)
. (133)
23Thus 628
|E[ˆη(x)]−η(x)| ≤ E
1
kX
i∈Nk(x)|η(Xi)−η(x)|

≤E
1
kX
i∈Nk(x)minn
L∥Xi−x∥β,2To

≤E
1
kX
i∈Nk(x)min
Lρβ(x),2T	

≤2TP(ρ(x)> r0) +Lrβ
0
≤2Te−(1−ln 2)k+L2k
Ncv dθβ
d
≤C1k
Nβ
d
, (134)
for some constant C1. 629
It remains to bound the variance. 630
Var[ˆη(x)] =E[Var [ˆ η(x)|X1, . . . ,XN]] + Var[ E[ˆη(x)]|X1, . . . ,XN]. (135)
For the first term in (135), 631
Var[ˆη(x)|X1, . . . ,XN] = Var
1
kX
i∈Nk(x)Zi|X1, . . . ,XN

=1
k2X
i∈Nk(x)Var[Zi|X1, . . . ,XN]
≤1
kT2
1 +8
ϵ2
. (136)
For the second term in (135), 632
Var[E[ˆη(x)|X1, . . . ,XN]] = Var
1
kX
i∈Nk(x)η(Xi)

≤E

1
kX
i∈Nk(x)η(Xi)−η(x)
2

=1
kX
i∈Nk(x)E
(η(Xi)−η(x))2
≤1
kX
i∈Nk(x)Eh
minn
L2∥Xi−x∥2β,4T2oi
≤4T2e−(1−ln 2)k+L2r2β
0
≤C2
1k
N2β
d
. (137)
Therefore (135) becomes 633
Var[ˆη(x)]≤1
kT2
1 +8
ϵ2
+C2
1k
N2β
d
. (138)
24Combine the analysis of bias and variance, 634
E[(ˆη(x)−η(x))2]≤1
kT2
1 +8
ϵ2
+ 2C2
1k
N2β
d
. (139)
Therefore the overall risk is bounded by 635
R=E[(ˆη(X)−η(X))2]≲1
kT2
1 +8
ϵ2
+ 2C2
1k
N2β
d
. (140)
The optimal growth rate of koverNis 636
k∼N2β
d+2β(ϵ∧1)−2d
d+2β. (141)
Then the convergence rate of the overall risk becomes 637
R≲(N(ϵ∧1)2)−2β
d+2β. (142)
H Proof of Theorem 7 638
From (127), 639
R−R∗≥1
2ch2β+d∥ϕ∥2
2E[ρH(ˆV,V)]
=1
2ch2β+d∥ϕ∥2
2GX
k=1P(ˆVk̸=Vk). (143)
Follow the analysis of lower bounds of classification in Appendix D, let hscales as (101) , then 640
P(ˆVk̸=Vk)≳1. Moreover, G∼h−d. Hence 641
inf
A∈A ϵsup
(f,η)∈Freg1(R−R∗)≳h2β∼(ϵN)−2β
d+β. (144)
Moreover, note that the non-private lower bound of regression is 642
inf
A∈A ϵsup
(f,η)∈Freg1(R−R∗)≳N−2β
2β+d. (145)
Combine (144) and (145), 643
inf
A∈A ϵsup
(f,η)∈Freg1(R−R∗)≳N−2β
2β+d+ (ϵN)−2β
d+β. (146)
I Proof of Theorem 8 644
1) Analysis of bias. Note that 645
E[ˆηl|X1:N] =E[Y|X∈Bl] =1
p(Bl)Z
η(u)f(u)du. (147)
Therefore, for all x∈Bl, 646
|E[ˆηl|X1:N]−η(x)| ≤1
p(Bl)Z
|η(u)−η(x)|f(u)du
≤Ldhβ. (148)
Therefore for all x∈Bl, 647
|E[ˆηl]−η(x)| ≤Ldhβ. (149)
2) Analysis of variance. Ifnl>0, 648
Var"
1
nlNX
i=11(Xi∈Bl)Yi|X1:N#
=1
nlVar[Y|X∈Bl]≤1
nl. (150)
25Therefore 649
Var"
1
nlNX
i=11(Xi∈Bl)Yi#
≤P
nl<1
2Np(Bl)
+P
nl≥1
2Np(Bl)2
Np(Bl)
≤exp
−1
2(1−ln 2)Np(Bl)
+2
Nchd. (151)
Similarly, 650
Var[Wl]≤P
nl<1
2Np(Bl)1
ϵ2+P
nl≥1
2Np(Bl)8
 1
2Np(Bl)2ϵ2
≲1
N2h2dϵ2. (152)
The mean squared error can then be bounded by the bounds of bias and variance. 651
E
(ˆη(x)−η(x))2
≲h2β+1
Nhd+1
N2h2dϵ2. (153)
Let 652
h∼N−1
2β+d+ (ϵN)−1
d+β. (154)
Then 653
R−R∗≲N−2β
2β+d+ (ϵN)−2β
d+β. (155)
J Proof of Theorem 9 654
Now we prove the minimax lower bound of nonparametric regression under label DP constraint. We 655
focus on the case in which ϵis small. 656
Similar to the steps of the proof of Theorem 5 in Appendix F, we find Bcubes in the support. The 657
definition of ηv,ˆvkare also the same as (121) and(123) . Compared with the case with bounded 658
noise, now Ycan take values in R. 659
For given x, let 660
Y=

T with probability1
2
Mp
Tp+ηv(x)
T
0 with probability 1−Mp
Tp
−Twith probability1
2
Mp
Tp−ηv(x)
T
.(156)
In Appendix F about the case with bounded noise, Tis a fixed constant. However, here Tis not fixed 661
and will change over N. It is straightforward to show that the distribution of Yin(156) satisfies 662
Assumption 3: 663
E[|Y|p|x] =Mp. (157)
Moreover, by taking expectation over Y, it can be shown that ηvis still the regression function: 664
E[Y|x] =ηv(x). (158)
Let 665
T=1
2Mph−β 1
p−1
. (159)
Here we still define 666
δ= sup
M∈Mϵmax
v,v′:ρH(v,v′)=1D(P(X,Z)1:N|v||P(X,Z)1:N|v′). (160)
26Without loss of generality, suppose that v1=v′
1fori̸= 1. Then 667
D(P(X,Z)1:N|v||P(X,Z)1:N|v′) = ND(PX,Z|v||PX,Z|v′)
=NZ
B1f(x)D(PZ|X,v||PZ|X,v′)dx
≤NZ
B1f(x)(eϵ−1)2TV2 
PZ|X,v, PZ|X,v′
dx
=NZ
B1f(x)(eϵ−1)2η2
v(x)1
T2dx
=N(eϵ−1)2h2β
T2Z
B1f(x)ϕ2x−c1
h
dx
=N(eϵ−1)2h2β+d∥ϕ∥2
2T−2
=N(eϵ−1)2∥ϕ∥2
21
2Mp−2
p−1
h2β+d+2β
p−1. (161)
Let 668
h∼(N(eϵ−1)2)−p−1
2pβ+d(p−1), (162)
thenδ≲1. Hence 669
inf
ˆηinf
M∈Mϵsup
(f,η)∈FR≳h2β∼(N(eϵ−1)2)−2β(p−1)
2pβ+d(p−1). (163)
K Proof of Theorem 10 670
Define 671
ηT(x) :=E[YT|x]. (164)
Then 672
ˆη(x)−η(x) =ηT(x)−η(x) +E[ˆη(x)]−ηT(x) + ˆη(x)−E[ˆη(x)]. (165)
Therefore 673
Eh
(ˆη(x)−η(x))2i
≤3(ηT(x)−η(x))2+ 3(E[ˆη(x)]−ηT(x))2+ 3 Var[ˆ η(x)]
:= 3( I1+I2+I3). (166)
Now we bound I1,I2andI3separately. 674
Bound of I1.We show the following lemma (which will also be used later). 675
Lemma 2.
|ηT(x)−η(x)| ≤Mp
p−1T1−p. (167)
Proof. Firstly, we decompose ηT(x)andη(x): 676
ηT(x) =E[YT|x] =E[Y1(−T≤Y≤T)|x] +TP(Y > T |x)−TP(Y < T |x), (168)
677
η(x) =E[Y|x] =E[Y1(−T≤Y≤T)|x] +E[Y1(Y > T )|x]−E[Y1(Y < T )|x].(169)
The first term is the same between (168) and(169) . Therefore we only need to compare the second 678
and the third term. 679
E[Y1(Y > T )|x] =ZT
0P(Y > T |x)dt+Z∞
TP(Y > T |x)dt
≤TP(Y > T |x) +Z∞
TMpt−pdt
=TP(Y > T |x) +Mp
p−1T1−p. (170)
27Therefore 680
E[Y1(Y > T )|x]−TP(Y > T |x)≤Mp
p−1T1−p. (171)
Similarly, 681
TP(Y < T |x)−E[Y1(Y < T )|x]≤Mp
p−1T1−p. (172)
A Combination of these two inequalities yields the (167). 682
With Lemma 2, 683
I1≤M2
p
(p−1)2T2(1−p). (173)
Bound of I2.Follow the steps in (134), 684
I2≤C2
1k
N2β
d
. (174)
Bound of I3.We decompose Var[ˆη(x)]as following: 685
Var[ˆη(x)] =E[Var[ˆη(x)|X1, . . . ,XN]] + Var[ E[ˆη(x)|X1, . . . ,XN]]. (175)
For the first term in (175) , from Assumption 3, E[|Y|p|x]≤Mp. Since p≥2, we have E[Y2|x] = 686
M2
pp. Therefore 687
Var[Zi|X1, . . . ,XN] = Var[ YT] + Var[ W]≤M2
pp+8T2
ϵ2. (176)
Recall (20), we have 688
Var[ˆη(x)|X1, . . . ,XN] =1
k2X
i∈Nk(x)Var[Zi|X1, . . . ,XN]
≤1
k
M2
pp+8T2
ϵ2
. (177)
For the second term in (175), (137) still holds, thus 689
Var[E[ˆη(x)|X1, . . . ,XN]]≤C2
1k
N2β
d
, (178)
and 690
I3≤1
k
M2
pp+8T2
ϵ2
+C2
1k
N2β
d
. (179)
Plug (173), (174) and (179) into (166), and take expectations, we get 691
R=E[(ˆη(X)−η(X))2]
≲T2(1−p)+1
k+T2
kϵ2+k
N2β
d
. (180)
Let 692
T∼(kϵ2)1
2p, k∼(Nϵ2)2pβ
d(p−1)+2pβ∨N2β
2β+d, (181)
then 693
R≲(Nϵ2)−2β(p−1)
d(p−1)+2pβ∨N−2β
2β+d. (182)
28L Proof of Theorem 11 694
LetYbe distributed as (156) . Recall Lemma 1 for the problem of classification and regression with 695
bounded noise. 696
Now we show the corresponding lemma for regression with unbounded noise. 697
Lemma 3. If0≤t≤Tln 2/(ϵnk), and nkt/Tis an integer, then 698
P(ˆVk= 1|X1:N,¯Yk=−t) +P(ˆVk=−1|X1:N,¯Yk=t)≥2
3. (183)
Here we briefly explain the condition nktis an integer. Recall the definition of ¯Ykin(88). Now since 699
Ytake values in {−T,0, T},nk¯Yk/Tmust be an integer. Therefore, in Lemma 3, we only need to 700
consider the case such that nkt/Tis an integer. 701
Proof. The proof follows the proof of Lemma 1 closely. We provide the proof here for completeness. 702
Construct D′by changing the label values of l=nkt/Titems from these nksamples falling in Bk, 703
from−TtoT. Then the average label values in Bkis denoted as ¯Y′
kafter such replacement. ˆVkalso 704
becomes ˆV′
k. Then from the ϵ-label DP requirement, 705
P(ˆVk= 1|X1:N,¯Yk=−t)(a)
≥e−lϵP
ˆV′
k= 1|X1:N,¯Y′
k=−t+2l
nk
(b)
≥e−lϵP
ˆVk= 1|X1:N,¯Yk=−t+2l
nk
≥e−nktϵ
1−P
ˆVk=−1|X1:N,¯Yk=−t+2l
nk
≥1
2h
1−P
ˆVk=−1|X1:N,¯Yk=ti
. (184)
in which (a) uses the group privacy property. The Hamming distance between DandD′isl, thus the 706
ratio of probability between DandD′is within [e−lϵ, elϵ]. (b) holds because the algorithm does not 707
change after changing DtoD′. Similarly, 708
P(ˆVk=−1|X1:N,¯Yk=t)≥1
2h
1−P
ˆVk= 1|X1:N,¯Yk=−ti
. (185)
Then (183) can be shown by adding up (184) and (185). 709
We then follow the proof of Theorem 3 in Appendix D. (101) becomes 710
h∼ϵN
T−1
d+β
. (186)
In(156) , note that P(Y=T)≥0andP(Y=−T)≥0. Therefore Mp/Tp≥ηv(x)/T. This 711
requires hβTp−1≤Mp. LetT∼h−β
p−1, then 712
h∼(ϵN)−1
d+βhβ
(d+β)(p−1), (187)
i.e. 713
h∼(ϵN)−p−1
pβ+d(p−1). (188)
Combine with standard minimax rate, the lower bound of regression with unbounded noise is 714
inf
A∈A ϵsup
(f,η)∈Freg2(R−R∗)≳N−2β
2β+d+ (ϵN)−2β(p−1)
pβ+d(p−1). (189)
29M Proof of Theorem 12 715
1) Analysis of bias. Note that Lemma 2 still holds here. Moreover, recall (149). Therefore 716
|E[ˆηl]−η(x)| ≤ |E[ˆηl−ηT(x)]|+|ηT(x)−η(x)| ≤Ldhβ+Mp
p−1T1−p. (190)
2) Analysis of variance. Similar to (151), it can be shown that 717
Var"
1
nlNX
i=11(Xi∈Bl)Yi#
≲1
Nhd. (191)
Moreover, the noise variance can be bounded by 718
Var[Wl]≲T2
N2h2dϵ2. (192)
The mean squared error is then bounded by 719
Eh
(ˆη(x)−η(x))2i
≲h2β+T2(1−p)+T2
N2h2dϵ2+1
Nhd. (193)
LetT∼(ϵNhd)1/p, then 720
R−R∗=E
(ˆη(X)−η(X))2
≲h2β+1
Nhd+ (ϵNhd)−2(1−1/p). (194)
To minimize (194), let 721
h∼N−1
2β+d+ (ϵN)−p−1
pβ+d(p−1), (195)
then 722
R−R∗≲N−2β
2β+d+ (ϵN)−2β(p−1)
pβ+d(p−1). (196)
30NeurIPS Paper Checklist 723
1.Claims 724
Question: Do the main claims made in the abstract and introduction accurately reflect the 725
paper’s contributions and scope? 726
Answer: [Yes] 727
Justification: The main contribution (i.e. proposing a new Huber loss minimization approach 728
which is more suitable to realistic cases, and providing theoretical analysis) has been made 729
clear in the abstract and introduction. 730
Guidelines: 731
•The answer NA means that the abstract and introduction do not include the claims 732
made in the paper. 733
•The abstract and/or introduction should clearly state the claims made, including the 734
contributions made in the paper and important assumptions and limitations. A No or 735
NA answer to this question will not be perceived well by the reviewers. 736
•The claims made should match theoretical and experimental results, and reflect how 737
much the results can be expected to generalize to other settings. 738
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 739
are not attained by the paper. 740
2.Limitations 741
Question: Does the paper discuss the limitations of the work performed by the authors? 742
Answer: [Yes] 743
Justification: It is explained at the end of conclusion section. 744
Guidelines: 745
•The answer NA means that the paper has no limitation while the answer No means that 746
the paper has limitations, but those are not discussed in the paper. 747
• The authors are encouraged to create a separate "Limitations" section in their paper. 748
•The paper should point out any strong assumptions and how robust the results are to 749
violations of these assumptions (e.g., independence assumptions, noiseless settings, 750
model well-specification, asymptotic approximations only holding locally). The authors 751
should reflect on how these assumptions might be violated in practice and what the 752
implications would be. 753
•The authors should reflect on the scope of the claims made, e.g., if the approach was 754
only tested on a few datasets or with a few runs. In general, empirical results often 755
depend on implicit assumptions, which should be articulated. 756
•The authors should reflect on the factors that influence the performance of the approach. 757
For example, a facial recognition algorithm may perform poorly when image resolution 758
is low or images are taken in low lighting. Or a speech-to-text system might not be 759
used reliably to provide closed captions for online lectures because it fails to handle 760
technical jargon. 761
•The authors should discuss the computational efficiency of the proposed algorithms 762
and how they scale with dataset size. 763
•If applicable, the authors should discuss possible limitations of their approach to 764
address problems of privacy and fairness. 765
•While the authors might fear that complete honesty about limitations might be used by 766
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 767
limitations that aren’t acknowledged in the paper. The authors should use their best 768
judgment and recognize that individual actions in favor of transparency play an impor- 769
tant role in developing norms that preserve the integrity of the community. Reviewers 770
will be specifically instructed to not penalize honesty concerning limitations. 771
3.Theory Assumptions and Proofs 772
Question: For each theoretical result, does the paper provide the full set of assumptions and 773
a complete (and correct) proof? 774
31Answer: [Yes] 775
Justification: Proofs are shown in the appendix, and intuition is provided in the paper. 776
Guidelines: 777
• The answer NA means that the paper does not include theoretical results. 778
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 779
referenced. 780
•All assumptions should be clearly stated or referenced in the statement of any theorems. 781
•The proofs can either appear in the main paper or the supplemental material, but if 782
they appear in the supplemental material, the authors are encouraged to provide a short 783
proof sketch to provide intuition. 784
•Inversely, any informal proof provided in the core of the paper should be complemented 785
by formal proofs provided in appendix or supplemental material. 786
• Theorems and Lemmas that the proof relies upon should be properly referenced. 787
4.Experimental Result Reproducibility 788
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 789
perimental results of the paper to the extent that it affects the main claims and/or conclusions 790
of the paper (regardless of whether the code and data are provided or not)? 791
Answer: [NA] 792
Justification: This is a theoretical paper without experiments. 793
Guidelines: 794
• The answer NA means that the paper does not include experiments. 795
•If the paper includes experiments, a No answer to this question will not be perceived 796
well by the reviewers: Making the paper reproducible is important, regardless of 797
whether the code and data are provided or not. 798
•If the contribution is a dataset and/or model, the authors should describe the steps taken 799
to make their results reproducible or verifiable. 800
•Depending on the contribution, reproducibility can be accomplished in various ways. 801
For example, if the contribution is a novel architecture, describing the architecture fully 802
might suffice, or if the contribution is a specific model and empirical evaluation, it may 803
be necessary to either make it possible for others to replicate the model with the same 804
dataset, or provide access to the model. In general. releasing code and data is often 805
one good way to accomplish this, but reproducibility can also be provided via detailed 806
instructions for how to replicate the results, access to a hosted model (e.g., in the case 807
of a large language model), releasing of a model checkpoint, or other means that are 808
appropriate to the research performed. 809
•While NeurIPS does not require releasing code, the conference does require all submis- 810
sions to provide some reasonable avenue for reproducibility, which may depend on the 811
nature of the contribution. For example 812
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 813
to reproduce that algorithm. 814
(b)If the contribution is primarily a new model architecture, the paper should describe 815
the architecture clearly and fully. 816
(c)If the contribution is a new model (e.g., a large language model), then there should 817
either be a way to access this model for reproducing the results or a way to reproduce 818
the model (e.g., with an open-source dataset or instructions for how to construct 819
the dataset). 820
(d)We recognize that reproducibility may be tricky in some cases, in which case 821
authors are welcome to describe the particular way they provide for reproducibility. 822
In the case of closed-source models, it may be that access to the model is limited in 823
some way (e.g., to registered users), but it should be possible for other researchers 824
to have some path to reproducing or verifying the results. 825
5.Open access to data and code 826
32Question: Does the paper provide open access to the data and code, with sufficient instruc- 827
tions to faithfully reproduce the main experimental results, as described in supplemental 828
material? 829
Answer: [NA] 830
Justification: This is a theoretical paper without experiments. 831
Guidelines: 832
• The answer NA means that paper does not include experiments requiring code. 833
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 834
public/guides/CodeSubmissionPolicy ) for more details. 835
•While we encourage the release of code and data, we understand that this might not be 836
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 837
including code, unless this is central to the contribution (e.g., for a new open-source 838
benchmark). 839
•The instructions should contain the exact command and environment needed to run to 840
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 841
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 842
•The authors should provide instructions on data access and preparation, including how 843
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 844
•The authors should provide scripts to reproduce all experimental results for the new 845
proposed method and baselines. If only a subset of experiments are reproducible, they 846
should state which ones are omitted from the script and why. 847
•At submission time, to preserve anonymity, the authors should release anonymized 848
versions (if applicable). 849
•Providing as much information as possible in supplemental material (appended to the 850
paper) is recommended, but including URLs to data and code is permitted. 851
6.Experimental Setting/Details 852
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 853
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 854
results? 855
Answer: [NA] 856
Justification: This is a theoretical paper without experiments. 857
Guidelines: 858
• The answer NA means that the paper does not include experiments. 859
•The experimental setting should be presented in the core of the paper to a level of detail 860
that is necessary to appreciate the results and make sense of them. 861
•The full details can be provided either with the code, in appendix, or as supplemental 862
material. 863
7.Experiment Statistical Significance 864
Question: Does the paper report error bars suitably and correctly defined or other appropriate 865
information about the statistical significance of the experiments? 866
Answer: [NA] 867
Justification: No experiments. 868
Guidelines: 869
• The answer NA means that the paper does not include experiments. 870
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 871
dence intervals, or statistical significance tests, at least for the experiments that support 872
the main claims of the paper. 873
•The factors of variability that the error bars are capturing should be clearly stated (for 874
example, train/test split, initialization, random drawing of some parameter, or overall 875
run with given experimental conditions). 876
•The method for calculating the error bars should be explained (closed form formula, 877
call to a library function, bootstrap, etc.) 878
33• The assumptions made should be given (e.g., Normally distributed errors). 879
•It should be clear whether the error bar is the standard deviation or the standard error 880
of the mean. 881
•It is OK to report 1-sigma error bars, but one should state it. The authors should 882
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 883
of Normality of errors is not verified. 884
•For asymmetric distributions, the authors should be careful not to show in tables or 885
figures symmetric error bars that would yield results that are out of range (e.g. negative 886
error rates). 887
•If error bars are reported in tables or plots, The authors should explain in the text how 888
they were calculated and reference the corresponding figures or tables in the text. 889
8.Experiments Compute Resources 890
Question: For each experiment, does the paper provide sufficient information on the com- 891
puter resources (type of compute workers, memory, time of execution) needed to reproduce 892
the experiments? 893
Answer: [NA] 894
Justification: No experiments. 895
Guidelines: 896
• The answer NA means that the paper does not include experiments. 897
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 898
or cloud provider, including relevant memory and storage. 899
•The paper should provide the amount of compute required for each of the individual 900
experimental runs as well as estimate the total compute. 901
•The paper should disclose whether the full research project required more compute 902
than the experiments reported in the paper (e.g., preliminary or failed experiments that 903
didn’t make it into the paper). 904
9.Code Of Ethics 905
Question: Does the research conducted in the paper conform, in every respect, with the 906
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 907
Answer: [Yes] 908
Justification: Our paper does not violate code of ethics. 909
Guidelines: 910
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 911
•If the authors answer No, they should explain the special circumstances that require a 912
deviation from the Code of Ethics. 913
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 914
eration due to laws or regulations in their jurisdiction). 915
10.Broader Impacts 916
Question: Does the paper discuss both potential positive societal impacts and negative 917
societal impacts of the work performed? 918
Answer: [NA] 919
Justification: This paper is foundational and theoretical research and not tied to particular 920
applications. 921
Guidelines: 922
• The answer NA means that there is no societal impact of the work performed. 923
•If the authors answer NA or No, they should explain why their work has no societal 924
impact or why the paper does not address societal impact. 925
•Examples of negative societal impacts include potential malicious or unintended uses 926
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 927
(e.g., deployment of technologies that could make decisions that unfairly impact specific 928
groups), privacy considerations, and security considerations. 929
34•The conference expects that many papers will be foundational research and not tied 930
to particular applications, let alone deployments. However, if there is a direct path to 931
any negative applications, the authors should point it out. For example, it is legitimate 932
to point out that an improvement in the quality of generative models could be used to 933
generate deepfakes for disinformation. On the other hand, it is not needed to point out 934
that a generic algorithm for optimizing neural networks could enable people to train 935
models that generate Deepfakes faster. 936
•The authors should consider possible harms that could arise when the technology is 937
being used as intended and functioning correctly, harms that could arise when the 938
technology is being used as intended but gives incorrect results, and harms following 939
from (intentional or unintentional) misuse of the technology. 940
•If there are negative societal impacts, the authors could also discuss possible mitigation 941
strategies (e.g., gated release of models, providing defenses in addition to attacks, 942
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 943
feedback over time, improving the efficiency and accessibility of ML). 944
11.Safeguards 945
Question: Does the paper describe safeguards that have been put in place for responsible 946
release of data or models that have a high risk for misuse (e.g., pretrained language models, 947
image generators, or scraped datasets)? 948
Answer: [NA] 949
Justification: This paper has no such risks. 950
Guidelines: 951
• The answer NA means that the paper poses no such risks. 952
•Released models that have a high risk for misuse or dual-use should be released with 953
necessary safeguards to allow for controlled use of the model, for example by requiring 954
that users adhere to usage guidelines or restrictions to access the model or implementing 955
safety filters. 956
•Datasets that have been scraped from the Internet could pose safety risks. The authors 957
should describe how they avoided releasing unsafe images. 958
•We recognize that providing effective safeguards is challenging, and many papers do 959
not require this, but we encourage authors to take this into account and make a best 960
faith effort. 961
12.Licenses for existing assets 962
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 963
the paper, properly credited and are the license and terms of use explicitly mentioned and 964
properly respected? 965
Answer: [NA] 966
Justification: This paper does not use existing assets. 967
Guidelines: 968
• The answer NA means that the paper does not use existing assets. 969
• The authors should cite the original paper that produced the code package or dataset. 970
•The authors should state which version of the asset is used and, if possible, include a 971
URL. 972
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 973
•For scraped data from a particular source (e.g., website), the copyright and terms of 974
service of that source should be provided. 975
•If assets are released, the license, copyright information, and terms of use in the 976
package should be provided. For popular datasets, paperswithcode.com/datasets 977
has curated licenses for some datasets. Their licensing guide can help determine the 978
license of a dataset. 979
•For existing datasets that are re-packaged, both the original license and the license of 980
the derived asset (if it has changed) should be provided. 981
35•If this information is not available online, the authors are encouraged to reach out to 982
the asset’s creators. 983
13.New Assets 984
Question: Are new assets introduced in the paper well documented and is the documentation 985
provided alongside the assets? 986
Answer: [NA] 987
Justification: This paper does not release new assets 988
Guidelines: 989
• The answer NA means that the paper does not release new assets. 990
•Researchers should communicate the details of the dataset/code/model as part of their 991
submissions via structured templates. This includes details about training, license, 992
limitations, etc. 993
•The paper should discuss whether and how consent was obtained from people whose 994
asset is used. 995
•At submission time, remember to anonymize your assets (if applicable). You can either 996
create an anonymized URL or include an anonymized zip file. 997
14.Crowdsourcing and Research with Human Subjects 998
Question: For crowdsourcing experiments and research with human subjects, does the paper 999
include the full text of instructions given to participants and screenshots, if applicable, as 1000
well as details about compensation (if any)? 1001
Answer: [NA] 1002
Justification: This paper does not involve crowdsourcing. 1003
Guidelines: 1004
•The answer NA means that the paper does not involve crowdsourcing nor research with 1005
human subjects. 1006
•Including this information in the supplemental material is fine, but if the main contribu- 1007
tion of the paper involves human subjects, then as much detail as possible should be 1008
included in the main paper. 1009
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 1010
or other labor should be paid at least the minimum wage in the country of the data 1011
collector. 1012
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 1013
Subjects 1014
Question: Does the paper describe potential risks incurred by study participants, whether 1015
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 1016
approvals (or an equivalent approval/review based on the requirements of your country or 1017
institution) were obtained? 1018
Answer: [NA] 1019
Justification: This paper does not involve crowdsourcing. 1020
Guidelines: 1021
•The answer NA means that the paper does not involve crowdsourcing nor research with 1022
human subjects. 1023
•Depending on the country in which research is conducted, IRB approval (or equivalent) 1024
may be required for any human subjects research. If you obtained IRB approval, you 1025
should clearly state this in the paper. 1026
•We recognize that the procedures for this may vary significantly between institutions 1027
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1028
guidelines for their institution. 1029
•For initial submissions, do not include any information that would break anonymity (if 1030
applicable), such as the institution conducting the review. 1031
36