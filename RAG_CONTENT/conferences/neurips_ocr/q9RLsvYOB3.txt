FlexPlanner: Flexible 3D Floorplanning via Deep
Reinforcement Learning in Hybrid Action Space with
Multi-Modality Representation
Ruizhe Zhong1, Xingbo Du1, Shixiong Kai2, Zhentao Tang2, Siyuan Xu2,
Jianye Hao2,3, Mingxuan Yuan2, Junchi Yan1∗
1Dept. of CSE & School of AI & MoE Key Lab of AI, Shanghai Jiao Tong University
2Noah’s Ark Lab, Huawei
3College of Intelligence and Computing, Tianjin University
{zerzerzerz271828, duxingbo, yanjunchi}@sjtu.edu.cn
{kaishixiong, tangzhentao1, xusiyuan520, yuan.mingxuan}@huawei.com
jianye.hao@tju.edu.cn
Abstract
In the Integrated Circuit (IC) design flow, floorplanning (FP) determines the po-
sition and shape of each block. Serving as a prototype for downstream tasks, it
is critical and establishes the upper bound of the final PPA (Power, Performance,
Area). However, with the emergence of 3D IC with stacked layers, existing methods
are not flexible enough to handle the versatile constraints. Besides, they typically
face difficulties in aligning the cross-die modules in 3D ICs due to their heuristic
representations, which could potentially result in severe data transfer failures. To
address these issues, we propose FlexPlanner, a flexible learning-based method in
hybrid action space with multi-modality representation to simultaneously handle
position, aspect ratio, and alignment of blocks. To our best knowledge, FlexPlanner
is the first learning-based approach to discard heuristic-based search in the 3D FP
task. Thus, the solution space is not limited by the heuristic floorplanning repre-
sentation, allowing for significant improvements in both wirelength and alignment
scores. Specifically, FlexPlanner models 3D FP based on multi-modalities, includ-
ing vision, graph, and sequence. To address the non-trivial heuristic-dependent
issue, we design a sophisticated policy network with hybrid action space and
asynchronous layer decision mechanism that allow for determining the versatile
properties of each block. Experiments on public benchmarks MCNC and GSRC
show the effectiveness. We significantly improve the alignment score from 0.474
to 0.940 and achieve an average reduction of 16% in wirelength. Moreover, our
method also demonstrates zero-shot transferability on unseen circuits. Code is
publicly available at: https://github.com/Thinklab-SJTU/EDA-AI .
1 Introduction
In the very beginning stage of physical design in Electronic Design Automation (EDA), floorplanning
(FP) plays a critical role. As a subsequent stage of hardware design [ 1] and logic synthesis [ 2],
floorplanning provides a prototype for downstream tasks [ 3], ranging from power delivery network
(PDN) design [ 4] to placement [ 5,6] & routing [ 7,8] (P&R), hence determining the upper bound
of final PPA (Power, Performance, Area). Recognized as an NP-hard problem [ 9], FP establishes
∗Corresponding Author. This work was partly supported by NSFC (62222607, 92370201) and Shanghai
Municipal Science and Technology Major Project under Grant 2021SHZDZX0102.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).the chip’s physical layout by optimizing the position and shape of the major blocks to minimize
interconnect lengths and ensure efficient silicon area utilization. As floorplanning technologies
evolve, 3D FP with stacked layers emerges with more challenges. In particular, the cross-die module
alignment becomes another pivotal factor in 3D FP [ 10,11,12]. For instance, vertical buses [ 11]
for cross-die communication connect the aligned blocks spread among multiple dies [ 11]. Another
example is the Memory-on-Logic technology [ 12,13,14], partitioning the processor [ 15] into two
tiers: memory tier and logic tier. The memory tier consists of memory blocks and customized
intellectual property cores (IP cores) [ 16], while the logic tier contains other components, such
as logic blocks. Blocks on different dies should be aligned together, enabling the communication
established by bonding bumps or pads [14].
Existing works can be categorized into heuristic-based methods, analytical approaches, and learning-
based methods. Heuristics-based methods [ 17,18,19,20,21] model the FP with a certain heuristic
representation. To refine the current FP, they modify the heuristic representation and convert it
to the corresponding FP through a decoding scheme. However, this implementation limits the
flexibility to directly adjust the position of blocks. After a single modification, the entire FP result
needs to be regenerated, incapable of making fine-grained adjustments. Moreover, the alignment
constraints cannot be satisfied by simply incorporating alignment metrics into its heuristics. Analytical
approaches [ 9,22] compute the gradient of objectives w.r.t. block position and utilize gradient descent
technique to optimize FP. However, the calculation of alignment is non-differentiable, making them
inapplicable in 3D FP. Recently, with the emergence of Reinforcement Learning (RL), learning-based
methods are promising in FP [ 23,24,25,26,27,28,29]. Among these approaches, [ 23,24,25]
still retain traditional FP representation, thereby leading to the same issues encountered by heuristic
methods. Conversely, [ 26,27,28,29] focus on determining block positions in 2D scenarios. However,
when directly implemented in 3D scenarios, these methods could result in 1) overlooking alignment
requirements and 2) multi-die property. Specifically, in 2D FP, blocks are arranged exclusively on
a single die and organized in a queue that represents the placing order of all blocks. However, in
3D scenarios, multiple queues exist due to the multi-die property, necessitating a layer decision
mechanism to merge these queues and determine the comprehensive placing order. This is a critical
issue, yet it remains insufficiently explored in current research. Additionally, current learning-based
methods are 3) incapable of addressing the variable aspect ratio of modules. Characteristics of typical
approaches are summarized in Table 1.Table 1: Characteristics of typical methods.
Method Type AR Aln 3D Mod
PeF [9] Analytical ✗ ✗ ✗ N/A
3D-B*-SA [17, 21] Heuristics ✗ ✗ ✓ H
Wiremask-BBO [30] Heuristics ✗ ✗ ✗ V
RL-CBL [24] Heuristics, RL ✗ ✗ ✗ H, G
GraphPlace [26] RL ✗ ✗ ✗ G
DeepPlace [27] RL ✗ ✗ ✗ V , G
MaskPlace [28] RL ✗ ✗ ✗ V
Ours RL ✓ ✓ ✓ V , G, S
AR: aspect ratio of blocks. Aln: cross-die block alignment. Mod: Modality.
*H = Heuristics, V = Vision, G = Graph, S = Sequence.To address the aforementioned challenges, we
propose FlexPlanner , a flexible deep-learning-
based approach in hybrid action space with
multi-modality representation for 3D FP. Flex-
Planner directly outputs the final FP result, with-
out the reliance on any heuristic representation.
Under the Actor-Critic framework, the policy
network consists of three sub-modules, respon-
sible for determining the position, layer, and
aspect ratio of blocks, spanning a hybrid action space. Empirical results demonstrate the effectiveness
and significance of FlexPlanner. The main contributions are highlighted as follows:
•First learning-based method to discard heuristic-based search in the 3D FP task. We propose a
novel learning-based method with flexible hybrid action space for 3D FP, simultaneously handling
the position, aspect ratio, and cross-die alignment of blocks. Without relying on the heuristic-based
search, FlexPlanner allows the position and aspect ratio of each block to be explored across a
comprehensive spectrum, rather than be limited by the constraints of heuristic FP representation,
thereby breaking through the upper bound of performance. And we propose an innovative strategy
for more effectively addressing the alignment issue.
•Tackle the non-trivial issue of dependency on heuristics by incorporating hybrid action space
and multi-modality representation. It is non-trivial to avoid the dependency on heuristics-based
search in 3D FP due to the difficulty of modeling the complex solution space. Heuristics can
only represent a subset of the entire solution space, resulting in limitations on the upper bound
performance. To address this issue, we initially introduce three modalities, including vision, graph,
and sequence, to comprehensively represent the state space. Additionally, we design a sophisticated
policy network with hybrid action space and asynchronous layer decision mechanisms, enabling
learning versatile properties such as position, aspect ratio, layer for each block in a 3D FP setting.
2•Zero-shot transferability. Leveraging the advantage of the learning scheme and multi-modalities,
FlexPlanner demonstrates the ability to exhibit zero-shot transferability on previously unseen
circuits. This capability shows strengths in its efficiency, as it conserves substantial training
resources when confronted with new IC cases.
•SOTA experimental results with significant alignment improvement. Within the learning
framework, FlexPlanner achieves state-of-the-art performance on wirelength and alignment in
3D FP. Specifically, the average reduction in wirelength arrives at 16%, compared to previous
works. Moreover, by effectively incorporating the alignment constraint, we achieve 0.940 on the
alignment score, significantly surpassing the previous SOTA score of 0.474.
2 Preliminary and Formulation
Floorplan. The 2D floorplan task aims to determine the position and shape of each block given
the block list, I/O port list, and netlist. Based on this, the 3D floorplan task is further required to
place all blocks across multiple dies/layers. Each die/layer d∈ D is a rectangular region with
width Wand height H, and all dies are of the same shape. Specifically, the block list is denoted
asB={b1, b2, . . . , b n}withnblocks, where each block biis a rectangle with width wi, height
hi, and area ai=wi·hi. The bottom-left coordinate of block biis denoted as (xi, yi), and the
layer where the biis located is denoted as zi. There are two types of blocks: hard block andsoft
block . For a hard block , its aspect ratio ARi=wi
hiis fixed. For a soft block ,ARican vary between
[AR min,ARmax], while the area must always satisfy ai=wi·hi. Additionally, we denote the
I/O port list as T={t1, t2, . . . , t m}withmports2. Each porttjis viewed as a point with a pre-
determined position (xj, yj, zj), where (xj, yj)is the coordinate, and zjis the layer index. Netlist
is defined as a set consisting of all nets, where each netis a set of blocks and ports, representing
the interconnections. Since the layer ziof block biis pre-assigned, we only need to determine the
coordinate (xi, yi)and the aspect ratio ARifor all blocks to optimize the following objectives:
1) Alignment. Given two blocks bi, bjon different layers, alignment evaluates the overlap/intersection
area between them on the common projected 2D plane. We define the alignment score aln(i, j):
alnx(i, j) = max (0 ,min(xi+wi, xj+wj)−max( xi, xj)),
alny(i, j) = max (0 ,min(yi+hi, yj+hj)−max( yi, yj)),
aln(i, j) = min
1,alnx(i, j)·alny(i, j)
alnm(i, j)
,(1)
where alnm(i, j)is the required minimum alignment area between block biandbj. The total
alignment score should be maximized to satisfy the alignment requirement. (bi, bj)forms an
alignment pair if alnm(i, j)>0, and bi, bjare mutual alignment partners .
Cross-die block alignment is common in 3D FP [ 11,13,14]. Taking two blocks in different dies as
an example, vertical buses [ 11] or bonding bumps/pads [ 14] are employed for communication, which
requires capabilities for cross-die block alignment. That is, considering their projection onto a 2D
plane, the related blocks must exhibit some minimum intersecting region, denoted as alnm(i, j).
2) HPWL and 3) Overlap. Half Perimeter Wire Length (HPWL) is an approximate metric of
wirelength. It can be computed much more efficiently, as accurate wirelength can be accessed only
after the time-consuming routing stage. The summation of HPWL should be minimized :
X
net∈netlist
max
mi∈netxc
i−min
mi∈netxc
i+ max
mi∈netyc
i−min
mi∈netyc
i
, (2)
where miis either a block or port in net and xc
iis the center x-coordinate. For a block, xc
i=xi+wi
2,
and for a port, xc
i=xi. Given two blocks bi, bjon the same die, the overlap area between them
should be minimized (detailed calculation is given in Alg. 4 in Appendix G.1). Besides, all blocks
should be placed within the fixed outline.
2It is also commonly referred to as terminal.
3Critic
Position
Decider
Layer
Decider
Aspect Ratio
Decider
Vision
GraphSequence
layer  prob
AR pr ob
aln maskpos mask
pos pr ob masked
pos pr obMulti-Modal State Policy Network
Hybrid
Action Space
constraints
Shar ed EncoderFigure 1: Pipeline of FlexPlanner. Under the Actor-Critic framework, taking the multi-modality
representation as input, the policy network consists of three sub-modules, responsible for determining
the position, layer, and aspect ratio of blocks. Alignment mask and position mask are incorporated to
filter out invalid positions where constraints (alignment, non-overlap, etc.) are not satisfied.
Canvas
 Alignment X
 Alignment Y
 Alignment
 Valid Position
01
01020
0102030
0250500750
01
(a) (b) (c) (d) (e)Axis Y
Figure 2: Demonstration of alignment. In (a), the light blue region will be occupied by the block to
place, and the dark blue region is occupied by its alignment partner block which has been placed.
By sliding block to place across the plane, we obtain the alignment values at each position along
the X and Y dimensions, as shown in (b) and (c). Final alignment can then be calculated through
element-wise matrix multiplication, as illustrated in (d). Only (x, y)satisfying alnx·alny≥alnmare
valid positions shown in (e), and this binary mask can be incorporated to filter out invalid positions.
3 Methodology
Overview. The 3D floorplanning task can be formulated as an episodic Markov Decision Process. As
shown in Fig. 1, pipeline of our approach mainly includes state, hybrid action space, policy network
and critic network. State stconsists of three modalities, including vision, graph, and sequence. Action
atis represented as (x, y, z, AR), where position (x, y, z )are discrete variables and aspect ratio
ARis a continuous variable. These properties form a hybrid action space. Under the Actor-Critic
framework, critic Vϕ(st)evaluates the current state. Policy network πθ(at|st)determines the 2D
coordinate (xt, yt)of current block bt, the layer zt+1to access next block bt+1, and the aspect
ratio ARt+1ofbt+1. To explicitly impose constraints on the action space, masks are applied to
the probability matrix of block positions. This process filters out invalid coordinates that violate
constraints, such as non-alignment, overlap, or out-of-bounds locations. We respectively introduce
the multi-modalities, layer decision, and reward function design in Sec. 3.1, Sec. 3.2 and Sec. 3.3.
3.1 Multi-Modality Representation of 3D Floorplanning
The state space contains three modalities: FP vision, netlist graph, and block placing sequence.
3.1.1 Vision Modality
It represents current floorplanning through images. We utilize four vision masks to depict chip layout.
Alignment Mask. fa∈NW×H(Nis the field of natural number) is a matrix to evaluate the alignment
area between blocks biand its alignment partner bj. Ifbjhas already been placed, fa[x, y]is the
intersection area on a 2D projected plane if biis placed at (x, y). Ifbjhas not been placed yet, the
4alignment mask of biwill be set to a matrix full-filled with alnm(i, j). The native approach has
the complexity O(WH). However, when WorHis large, it has a low efficiency. We design an
efficient alignment mask generation algorithm with meshgrid operation, which only iterates in regions
causing projection overlap between these blocks and harnessing the power of parallel computing,
thus reducing the complexity to O(wh)namely O(1). Demonstration of alignment and details of the
algorithm are shown in Fig. 2 and Alg. 1. Alignment mask is also utilized to filter out the positions
that do not satisfy the alignment constraint.
Algorithm 1: Alignment mask generation.
Input: Current block bi, alignment partner bj, chip
width and height W, H
Output: Alignment mask f(i)
afor block bi
xs= max(0 , xj−wi),xe= min( xj+wj, W)
ys= max(0 , yj−hi),ye= min( yj+hj, H)
xi=arange (xs, xe),yi=arange (ys, ye)
Xi,Yi=meshgrid (xi,yi)
Ui=Xi,Vi=Xi+wi
Uj=xj,Vj=xj+wj// broadcast to a matrix
U=where (Ui>Uj,Ui,Uj)
V=where (Vi<Vj,Vi,Vj)
f(i)
ax= max (0 ,V−U)
// we omit y-dimension due to page limitation
f(i)
a=zeros (W, H ), f(i)
a[Xi,Yi] =f(i)
ax⊙f(i)
ayCanvas Mask. Canvas mask fc∈N|D|×W×H
is a global observation of current chip layout.
The canvas mask is initialized to all zeros. After
placing a block bat each step, we modify the
canvas mask with fc[z, x:x+w, y:y+h] +=
1. Based on the canvas mask, we can implement
a fast calculation of the total overlap of chip
layout, shown in Alg. 4 in Appendix G.1.
Wire Mask & Position Mask [ 28].Wire mask
fw∈NW×His a matrix for how HPWL will
increase if a block is placed at the position. It
records the increase of HPWL by placing the
current block to all candidate positions. Position
mask fp∈ {0,1}W×Hindicates available posi-
tions for current block to place without overlap
or out-of-boundary. 1implies this position is feasible to place the block.
Overall, in step t, given current block bt, we concatenate f(t)
a, f(t)
c, f(t)
w, f(t)
p. We also incorporate
masks f(t+1)
w, f(t+1)
p of header block in FIFO queue q(introduced in sequence modality in Sec. 3.1.2)
in each die, which represent all possible choices for next block in step t+ 1, providing policy with
future horizon. All of these input masks constitute the floorplanning vision modality.
3.1.2 Graph and Sequence Modality
Graph Modality. Given a netlist, we convert it to a graph G(V, E), where Vis the set of vertices
andEis the set of edges. For two blocks bi, bjwithin a net, we add edges eij, ejibetween vertex pair
(vi, vj). For the vertex feature, We select (bid, x, y, z, w, h, a, p ), where bidis the block index and p
indicates whether this block has already been placed or not. Considering each block has a placing
order showing the property as a sequence, it is natural to utilize positional encoding [ 31] to model
this feature. Furthermore, we employ a Graph Attention Network [ 32] to produce embeddings of
graph and nodes, representing the logical connection among blocks.
Sequence Modality. Given a die di, blocks in diare sorted by their area in descending order, forming
a FIFO (first in, first out) queue qi. Thus, the block placing order for each die is pre-determined, and
each queue can be viewed as a sequence. Combined with block features, we enhance and re-organize
the multi-die sequence S∈R|D|×L×C, where Lis the maximum number of blocks in a single die,
Cis the number of features. We select (bid, x, y, z, w, h, a, p )as sequence features. The sequence
modality provides the model with global observation of entire block placing order, and is employed
in the asynchronous layer decision discussed in Sec. 3.2.
3.2 Asynchronous Layer Decision
In 2D FP, blocks are arranged on a single die and are organized in a FIFO queue representing
the placing order. However, in 3D scenarios, multiple queues exist due to the multi-die property,
necessitating a layer decision mechanism to merge these queues and determine the comprehensive
placing order. A native approach is synchronous block placing, in which we initially place all blocks
on the first die, followed by blocks on the second die, and continue in this manner until the last die.
In [26,27,28,30] with synchronous placing, the entire placing order keeps fixed. However, this
synchronous die-by-die placing order neglects the cross-die connections and alignment requirements,
leading to a relatively poor FP layout. To address this issue, we propose an asynchronous layer
decision mechanism, which determines the layer for accessing next block. If the policy selects die di
for next step, the header block of the FIFO queue qiwill be popped as the next block to place.
5During the training process under asynchronous block placing, on one hand, the entire block placing
order could vary hugely and become unstable due to the sparsity of the reward, making it non-trivial
for position decider to determine the positions of blocks. On the other hand, the layer decision module
could converge too fast to mode collapse (a fixed placing order leading to poor quality), empirically
degenerating to synchronous die-by-die block placing. In our analysis, it is the short forward horizon
leading to this problem. Since policy is only able to sense current and next states, it lacks the global
receptive field of entire placing order.
To address these issues, we enhance the representation with the sequence modality, and employ
Transformer [ 31] to extract global placing order feature. Given the multi-die sequence feature
S∈R|D|×L×C, self-attention is computed. Sis viewed as the source sequence input with length L.
The memory M∈R|D|×L×L, output of self-attention of Sis given by [31]:
M= LN(MSA( S) +S),M= LN(FFN( M) +M), (3)
where MSA is the multi-head self-attention, LNis layer normalization, and FFN is the feed-forward
network. Next, cross-attention is applied on Mand current block feature bk. The single block feature
bk∈R1×Cis treated as the target sequence with length 1, serving as the query vector. The output of
cross-attention O∈R|D|× 1×Cis [31]:
O= LN(MHA( bk,M,M) +bk),O= LN(FFN( O) +O), (4)
where MHA( Q,K,V)denotes multi-head attention. Finally, the next layer decision probability
vector can be further computed through linear projection and softmax operation.
3.3 Reward Function with Local Advantage and Global Baseline
Algorithm 2: Reward function with local ad-
vantage and global baseline.
Input: (Partial) Alignment score aln, (partial)
overlap o, (partial) HPWL , and
corresponding weight wa, wo, wl
Output: Reward rfor each step
fortfrom len(episode )to1do
iftis the end of an episode then
rt=wa·alnt−wo·ot−wl·HPWL t
b=rt
else
rt=wa·(alnt−alnt−1)−wo·(ot−
ot−1)−wl·(HPWL t−HPWL t−1) +bIn 3D floorplanning task, final wirelength, over-
lap and alignment can only be accessed at the
end of each episode, leading to a sparse re-
ward. GraphPlace [ 26] uses this sparse reward
design, where rewards at intermediate steps are
all zeros. DeepPlace [ 27] adopts it with ad-
ditional intrinsic reward via Random Network
Distillation [ 33]. MaskPlace [ 28] introduces a
dense reward scheme based on partial HPWL,
which is calculated only on the currently placed
blocks at each step. However, in two former
methods [ 26,27], they fail to accurately sense
the intermediate quality through reward, and in
MaskPlace [ 28], only difference between local
adjacent steps is involved, lacking the global view of the whole episode. As a result, all these reward
designs demonstrate relatively poor performances, especially with the complicated hybrid action
space consisting of position, layer and aspect ratio.
To alleviate this problem, we design a novel reward function with local advantage and global baseline.
We define local advantage as the difference of metric between two adjacent steps, and global baseline
as the overall metric at the end of an episode. With local advantage, our model has the ability to
acquire current state is whether better or worse than the previous. Global baseline depicts the overall
quality of the final floorplanning result. It is also essential for asynchronous layer decision module to
avoid early convergence and degeneration to poor die-by-die synchronous block placing order, shown
in Sec. 4.4 and Fig. 6b. Details of reward design is shown in Alg. 2.
3.4 Flexible RL with Hybrid Action Space for 3D Floorplanning
We employ RL with a hybrid action space to address the 3D FP task. The state space consists of
three modalities, including floorplanning vision, netlist graph and sequence of block placing order.
The hybrid action space is formulated as X × Y × Z × R , with X,Y,Zas discrete sets and R
as a continuous set. In each step t, the policy outputs three distributions: 1)a discrete probability
distribution for 2D position (xt, yt)of current block bt,2)a discrete probability distribution to
determine the layer zt+1for accessing next block bt+1, and finally 3)the mean value and standard
deviation of a contiguous Gaussian distribution to determine the aspect ratio ARt+1forbt+1. The
6Table 2: Alignment score comparison among baselines and our method. The higher the alignment
score, the better, and the optimal results are shown in bold . C/M means Circuit/Method.
C/M 3D-B*-SA [21] RL-CBL [24] Wiremask-BBO [30] GraphPlace [26] DeepPlace [27] MaskPlace [28] Ours
ami33 0.550±0.058 0.132 ±0.038 0.179 ±0.091 0.207 ±0.067 0.286 ±0.051 0.300 ±0.017 0.905±0.017
ami49 0.438±0.099 0.107 ±0.043 0.222 ±0.082 0.265 ±0.063 0.180 ±0.056 0.218 ±0.052 0.955±0.010
n10 0.383±0.167 0.241 ±0.076 0.211 ±0.004 0.197 ±0.049 0.235 ±0.080 0.354 ±0.066 0.917±0.012
n30 0.537±0.159 0.108 ±0.040 0.288 ±0.051 0.233 ±0.039 0.287 ±0.074 0.511 ±0.067 0.920±0.024
n50 0.626±0.158 0.048 ±0.016 0.290 ±0.053 0.378 ±0.120 0.343 ±0.064 0.764 ±0.002 0.970±0.004
n100 0.131±0.051 0.016 ±0.008 0.195 ±0.034 0.279 ±0.050 0.332 ±0.073 0.575 ±0.046 0.961±0.017
n200 0.033±0.025 0.013 ±0.009 0.182 ±0.031 0.360 ±0.060 0.387 ±0.039 0.534 ±0.041 0.923±0.020
n300 0.009±0.009 0.005 ±0.005 0.205 ±0.038 0.383 ±0.023 0.399 ±0.031 0.533 ±0.023 0.965±0.010
Avg. 0.338 0.084 0.222 0.288 0.306 0.474 0.940
Table 3: HPWL (the lower the better) comparison. The optimal results are shown in bold .
C/M 3D-B*-SA [21] RL-CBL [24] Wiremask-BBO [30] GraphPlace [26] DeepPlace [27] MaskPlace [28] Ours
ami33 85,162 ±5,563 85,303 ±4,147 66,387 ±3,531 82,685 ±6,271 79,457 ±6,885 62,125 ±829 58,339 ±1,894
ami49 1,400,787 ±68,043 1,338,219 ±98,556 1,100,891 ±95,467 1,455,872 ±80,468 1,356,203 ±56,434 1,128,110 ±90,645 762,712 ±12,878
n10 35,230 ±115 34,805 ±1,132 33,046 ±36 36,520 ±869 34,371 ±817 33,648 ±1,096 29,781 ±75
n30 101,672 ±2,665 105,796 ±1,844 87,198 ±1,862 98,437 ±1,860 97,293 ±3,192 86,291 ±764 83,962 ±1,030
n50 132,421 ±3,123 156,113 ±4,079 111,878 ±3,371 136,980 ±2,219 126,910 ±2,836 113,145 ±407 105,839 ±916
n100 223,381 ±9,123 275,982 ±10,348 181,572 ±1,966 209,940 ±4,161 223,359 ±5,330 189,100 ±2,133 176,375 ±960
n200 422,060 ±9,101 572,649 ±37,831 325,453 ±3,488 402,650 ±6,117 418,348 ±5,134 375,250 ±2,939 316,199 ±1,080
n300 633,344 ±5,513 990,465 ±37,719 467,906 ±5,362 596,615 ±4,353 635,165 ±9,636 532,087 ±4,214 459,221 ±5,935
Avg. 379,257 444,917 296,791 377,462 371,388 314,969 249,053
reason for action atcontaining the layer and aspect ratio for next step t+ 1instead of tis that: after
the execution of at, next state st+1can be generated only after block bt+1to place at step t+ 1and
its shape have already been determined, since bt+1and its shape (wt+1, ht+1)are involved in the
calculation of alignment mask and wire mask. To guarantee adherence to the specified non-overlap
and alignment constraints, position mask fpand alignment mask faare incorporated in 2D position
decision. Only (x, y)satisfying fp[x, y] = 1 andfa[x, y]≥alnmare considered as valid positions,
where alnmis the required minimum alignment area between current block and its alignment partner.
Finally, without the reliance on conventional heuristic FP representation, our approach exhibits more
flexibility to directly solve position, aspect ratio and cross-die alignment for blocks.
We select the Actor-Critic [ 34] framework and Hybrid Proximal Policy Optimization [ 35,36]
algorithm. The objective function of our hybrid policy πθ(at|st)can be formulated as:
L(θ) =3X
k=1λk·ˆEth
min
r(k)
t(θ)ˆAt,clip
r(k)
t(θ),1−ε,1 +ε
ˆAti
, (5)
where k= 1,2,3represents position, layer and aspect ratio decision. λkis the weight for each clip
loss.r(k)
t(θ)is the probability ratioπθ(a(k)
t|st)
πθold(a(k)
t|st).ˆAtdenotes the generalized advantage estimation
(GAE) [ 37], and Gt=ˆAt+Vtis the cumulative discounted reward [ 37,38].Vtis the estimated
state value from critic network Vϕ(st), and critic network is updated with minimizing Mean Squared
Error (MSE) L(ϕ) =λϕ·ˆEt
(Gt−Vϕ(st))2
. Entropy of each action distribution is also added as
a regularization term for exploration encouragement. Training algorithm is shown in Appendix G.2,
and details of model architecture are shown in Appendix D.
4 Experiment and Analysis
4.1 Evaluation Protocol and Benchmark
We evaluate the performance of FlexPlanner and other typical methods on public benchmark MCNC3
andGSRC4shown in Table 7 in Appendix B. The floorplanning region is set to square region. I/O
ports are projected to the fixed outline, and their positions are kept unchanged. Aspect ratio of each
block can vary in range1
2,2
. Each experiment is run for five times with different seeds. More
implementation details and hyper-parameter settings can be found in Appendix C.
3http://vlsicad.eecs.umich.edu/BK/MCNCbench/
4http://vlsicad.eecs.umich.edu/BK/GSRCbench/
7Die 01
23
456
78
9
1011 12
1314
15
Die 11
23
456
78
9
1011 12
1314
15w/o aln partner(a) Floorplan of circuit n50
Die 0
12
34
5
6789 10 11
1213
141516
1718192021
222324
2526
272829
30
Die 1
12
34
5
6789 10 11
1213
141516
1718192021
222324
2526
272829
30w/o aln partner (b) Floorplan of circuit n100
Figure 3: Our 3D floorplan result. Two blocks with the same index and the same color on different
dies form an alignment pair, which roughly locate on the same positions and share a 2D common
projected area. Gray blocks mean they do not have alignment partners.
4.2 Comparison with Baselines
Methods including heuristic-based and learning-based approaches are selected as baselines, and
the implementation details are shown in Appendix E. Average alignment score and total HPWL
are employed as evaluation metrics. Results are shown in Table 2 and 3. For alignment score, our
approach achieves 0.940, significantly surpassing the second-best method which scores 0.474. For
HPWL, our approach achieves an average reduction of 16%. Our method reduces HPWL to 249,053,
and the second-best method (Wiremask-BBO) reduces it to 296,791. However, it only achieves
0.222 for alignment score, failing to effectively tackle the alignment constraint, empirically showing
that our approach is capable of address alignment and HPWL simultaneously. Demonstration of
3D floorplanning results by our method are also shown in Fig. 3 and Fig. 4. We also compare
performances on overlap, out-of-bound and runtime, shown in Appendix F. Capability and flexibility
of our approach to address pre-placed modules (PPMs) are also demonstrated in Appendix F.5.
Die 01
23
4
5
67
8
910
Die 11
23
4
5 67
8
910Aligned Not Aligned without aln partner
(a) GraphPlace, alignment score = 0.40
Die 01
234
5
6
78
9
10
Die 11
234 5
678
9
10Not Aligned Aligned without aln partner (b) DeepPlace, alignment score = 0.31
Die 01
23456
78
9
10
Die 11
2345
67
8
910Aligned Not Aligned without aln partner
(c) Wiremask-BBO, alignment score = 0.35
Die 0
1234
56
78
9
10
Die 1
1 234
56
78
9
10Aligned without aln partner (d) Ours, alignment score = 0.98
Figure 4: Visualization of cross-die block alignment on circuit n50. Two blocks with the same index
forms an alignment pair. For a pair with block i, j, we calculate individual alignment score aln(i, j)
according to Eq. 1. Green means these two blocks are aligned ( aln(i, j)≥0.5) while red means not
aligned ( aln(i, j)<0.5). Total alignment score is calculated according to Alg. 3 in Appendix G.1. It
demonstrates that our method achieves much better alignment score than other baselines.
80 500 1000
Epoch0.50.60.70.80.9Alignment
Finetune
From scratch
0 500 1000
Epoch3.23.43.63.8HPWL1e5
Finetune
From scratch(a) Training curve on circuit n200.
0 200 400 600
Epoch0.50.60.70.80.9Alignment
Finetune
From scratch
0 200 400 600
Epoch4.755.005.255.505.75HPWL1e5
Finetune
From scratch (b) Training curve on circuit n300.
Figure 5: Training curve between fine-tune (based on circuit n100) and training from scratch.
Table 4: Zero-shot transferability evaluation (training on circuit n100).
Metric/Circuit ami33 ami49 n10 n30 n50 n200 n300
Alignment ( ↑)value 0.859 0.894 0.875 0.947 0.970 0.877 0.908
ratio 0.949 0.936 0.954 1.029 1.000 0.950 0.941
HPWL ( ↓)value 59,923 835,170 30,720 87,784 111,039 322,242 462,780
ratio 1.027 1.095 1.032 1.046 1.049 1.019 1.008
Table 5: Ablation study on n100. sync: synchronous die-by-die placing. w/o aln: remove alignment
mask in vision modality. w/o seq: remove sequence modality. w/o graph: remove graph modality.
sparse rew: the same reward as GraphPlace [26]. diff rew: the same reward as MaskPlace [28].
Metric/Method sync w/o aln w/o graph w/o seq sparse rew diff rew Ours
Alignment 0.850±0.042 0.349 ±0.026 0.874 ±0.027 0.860 ±0.021 0.840 ±0.018 0.721 ±0.054 0.961±0.017
HPWL 185,624 ±1,822 187,617 ±3,096 185,079 ±1,681 186,005 ±846 189,219 ±1,164 190,384 ±2,192 176,639 ±1,001
4.3 Transferable 3D Floorplanning
The zero-shot transferability of our method is also evaluated. We firstly train the model on circuit
n100, and directly test the performance without any fine-tuning. In Table 4, ratio is calculated
between running inference and training on corresponding circuit. It demonstrates that our model
exhibits a good zero-shot transferability, on either smaller or larger cases. The transferability is also
demonstrated through the training curves between fine-tuning (based on pre-trained weights of circuit
n100) and training from scratch. In Fig. 5, through fine-tune technique, better or similar performance
can be achieved, conserving substantial training resources.
4.4 Ablation Studies
We ablate effectiveness of each component in our approach, including asynchronous layer decision,
multi-modality representation and reward function. Experiments on circuit n100 are shown in Table 5.
Removing the alignment mask leads to a huge drop of alignment score, from 0.961 to 0.349. We also
evaluate the effectiveness of incorporating the alignment mask as an input feature and a constraint. In
Fig. 6a, ‘input’ refers to the scenario where we feed the alignment mask as input to the model, while
‘constr.’ indicates that the alignment mask is utilized to filter out invalid positions that do not satisfy
the alignment constraint. As the input, alignment mask plays a critical role to effectively capture the
alignment information. As the constraint, it reduces the action space and accelerates training process.
Shown in the column ‘sync’ in Table 5, fixed synchronous die-by-die block placing order performs
worse than asynchronous layer decision, since in the latter, more flexibility is provided to plan the
entire placing order. Our reward design, incorporating the local advantage and global baseline, is
crucial for fully leveraging the asynchronous layer decision mechanism. In Fig. 6b, with the guidance
of our reward, the layer decision module keeps active to learn an optimal placing order. However,
with other reward design schemes, it is faced with either rapid convergence to the degeneration of
synchronous die-by-die decision-making, or unstable oscillation. Under reward function ‘diff’ in
MaskPlace [ 28], only the difference between two consecutive steps is focused, while the global
reward is disregarded, leading to rapid convergence to the degeneration of synchronous die-by-die
layer decision. Under reward function ‘sparse’ in GraphPlace [ 26], rewards for intermediate steps are
all zeros, and only the reward of the final step is non-zero. The policy lacks the modeling of local
information, resulting in its inability to accurately assess the influence of current action on the entire
decision-making process.
90 200 400 600 800 1000
Epoch0.00.20.40.60.81.0Alignmentinput + constr.
input onlyconstr. only
both N/A(a) Effectiveness of the alignment mask.
0250 500 750 1000 1250 1500 1750 2000
Epoch1020304050Sum of first half seq (Z)diff sparse ours uniform (b) Effectiveness of rewards on layer decision.
Figure 6: (a) Effectiveness of the alignment mask. As the input feature, it plays a critical role in
capturing the alignment information. As the constraint (constr.), it reduces the action space and
accelerates the training process. (b) Effectiveness of rewards on layer decision, shown in circuit n100
with episode length L= 100 and|D|= 2dies.ztis the determined layer index at step t, and we note
Z=PL/2
t=1zt.Z→0orZ→L/2means degeneration to die-by-die synchronous layer decision
(almost all die 0 or 1 in the first half episode).
5 Related Works
Classical methods in FP can be roughly categorized into heuristics and analytical approaches. The
former typically models FP with a certain representation, such as B*-tree [ 17], Corner Block List [ 18]
and Sequence Pair [ 39], based on which heuristic algorithm (especially Simulated Annealing [ 40]) is
utilized to search for an optimal solution. Finally, the representation is converted to corresponding FP
result via a decoding scheme. Apart from the heuristics, analytical approaches [ 9,22,41] regard the
FP problem as an electrostatic system [ 42,43]. They compute the gradient of objective functions
w.r.t. block coordinates, and utilize gradient descent-based algorithm to optimize the solution.
RL-based floorplanning approaches. Recently, floorplanning also attracted attention from the rein-
forcement learning communities. For instance, [ 23,24,25] incorporate traditional FP representation
and RL. Among these methods, at each step, the policy network either 1) decides whether to accept
the randomly perturbed state or not [ 23,25] or 2) determines how to perturb the current state [ 24].
[26,27,28] primarily focus on making decisions about block position in 2D scenarios with RL.
GraphPlace [ 26] and DeepPlace [ 27] incorporate graph and vision as representation. MaskPlace [ 28]
employs visual representation, and designs a dense reward function based on partial HPWL.
RL with hybrid action space. Common RL can be categorized into discrete and continuous action
spaces. However, in certain scenarios, besides the discrete action, we need to make decisions
regarding its parameters, typically residing in continuous spaces [ 36]. Consequently, it gives rise
to a hybrid action space. [ 44] proposes to discretize the continuous portion, which creates a large
discrete set and sacrifices fine-grained control. Alternatively, [ 45,46,47,48] convert the discrete
action selection into a continuous space with an actor network and employ a DQN-based [ 49,50]
algorithm for training, but found to be unstable and inefficient. [ 36] suggests to use multiple policy
heads consisting of one for discrete actions and the others for corresponding continuous parameters
separately. [51] proposes to encode the hybrid action space to a continuous space with GAE [37].
6 Conclusion
In this paper, we propose FlexPlanner, a flexible learning-based method in hybrid action space
with multi-modality representation for 3D floorplanning task. It involves no reliance on heuristic-
based search, thus achieves better flexibility to tackle position, aspect ratio and cross-die alignment
for blocks under complex constraints. FlexPlanner outperforms baselines in alignment score and
wirelength, and it also demonstrates zero-shot transferability on unseen circuits. This paper also has
some limitations for future work: further optimization in 3D FP could be involved such as thermal
optimization. Our method has no potential harm to the public society at the moment.
10References
[1]Yao Lai, Sungyoung Lee, Guojin Chen, Souradip Poddar, Mengkang Hu, David Z Pan, and Ping Luo.
Analogcoder: Analog circuit design via training-free code generation. arXiv preprint arXiv:2405.14918 ,
2024.
[2]Yao Lai, Jinxin Liu, David Z Pan, and Ping Luo. Scalable and effective arithmetic tree generation for adder
and multiplier designs. arXiv preprint arXiv:2405.06758 , 2024.
[3]Lei Chen, Yiqi Chen, Zhufei Chu, Wenji Fang, Tsung-Yi Ho, Ru Huang, Yu Huang, Sadaf Khan, Min
Li, Xingquan Li, et al. Large circuit models: opportunities and challenges. Science China Information
Sciences , 2024.
[4]Chen Wang, Jingkun Mao, Giuseppe Selli, Shaofeng Luan, Lin Zhang, Jun Fan, David J Pommerenke,
Richard E DuBroff, and James L Drewniak. An efficient approach for power delivery network design
with closed-form expressions for parasitic interconnect inductances. IEEE Transactions on Advanced
Packaging , 2006.
[5]Chung-Kuan Cheng, Andrew B Kahng, Ilgweon Kang, and Lutong Wang. Replace: Advancing solution
quality and routability validation in global placement. IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems , 2018.
[6]Ruizhe Zhong, Junjie Ye, Zhentao Tang, Shixiong Kai, Mingxuan Yuan, Jianye Hao, and Junchi Yan.
Preroutgnn for timing prediction with order preserving partition: Global circuit pre-training, local delay
learning and attentional cell modeling. In Proceedings of the AAAI Conference on Artificial Intelligence ,
2024.
[7]Jinwei Liu, Chak-Wa Pui, Fangzhou Wang, and Evangeline FY Young. Cugr: Detailed-routability-driven 3d
global routing with probabilistic resource model. In 2020 57th ACM/IEEE Design Automation Conference
(DAC) , 2020.
[8]Xingbo Du, Chonghua Wang, Ruizhe Zhong, and Junchi Yan. Hubrouter: Learning global routing via hub
generation and pin-hub connection. Advances in Neural Information Processing Systems , 36, 2023.
[9]Ximeng Li, Keyu Peng, Fuxing Huang, and Wenxing Zhu. Pef: Poisson’s equation based large-scale
fixed-outline floorplanning. IEEE Transactions on Computer-Aided Design of Integrated Circuits and
Systems , 2022.
[10] Jill HY Law, Evangeline FY Young, and Royce LS Ching. Block alignment in 3d floorplan using layered
tcg. In Proceedings of the 16th ACM Great Lakes symposium on VLSI , 2006.
[11] Johann Knechtel, Evangeline FY Young, and Jens Lienig. Planning massive interconnects in 3-d chips.
TCAD , 2015.
[12] Anthony Agnesina, Moritz Brunion, Jinwoo Kim, Alberto Garcia-Ortiz, Dragomir Milojevic, Francky
Catthoor, Gioele Mirabelli, Manu Komalan, and Sung Kyu Lim. Power, performance, area, and cost
analysis of face-to-face bonded 3d ics. IEEE Transactions on Components, Packaging and Manufacturing
Technology , 2023.
[13] Lennart Bamberg, Alberto García-Ortiz, Lingjun Zhu, Sai Pentapati, Sung Kyu Lim, et al. Macro-3d: A
physical design methodology for face-to-face-stacked heterogeneous 3d ics. In 2020 Design, Automation
& Test in Europe Conference & Exhibition (DATE) , 2020.
[14] Sai Pentapati, Anthony Agnesina, Moritz Brunion, Yen-Hsiang Huang, and Sung Kyu Lim. On legalization
of die bonding bumps and pads for 3d ics. In Proceedings of the 2023 International Symposium on Physical
Design , 2023.
[15] Sankatali Venkateswarlu, Subrat Mishra, Herman Oprins, Bjorn Vermeersch, Moritz Brunion, Jun-Han
Han, Mircea R Stan, Dwaipayan Biswas, Pieter Weckx, and Francky Catthoor. Impact of 3-d integration on
thermal performance of risc-v mempool multicore soc. IEEE Transactions on Very Large Scale Integration
(VLSI) Systems , 2023.
[16] David Koblah, Rabin Acharya, Daniel Capecci, Olivia Dizon-Paradis, Shahin Tajik, Fatemeh Ganji,
Damon Woodard, and Domenic Forte. A survey and perspective on artificial intelligence for security-aware
electronic design automation. ACM Transactions on Design Automation of Electronic Systems , 2023.
[17] Yun-Chih Chang, Yao-Wen Chang, Guang-Ming Wu, and Shu-Wei Wu. B*-trees: A new representation
for non-slicing floorplans. In Design Automation Conference (DAC) , 2000.
11[18] Jai-Ming Lin, Yao-Wen Chang, and Shih-Ping Lin. Corner sequence-a p-admissible floorplan representation
with a worst case linear-time packing scheme. IEEE Transactions on Very Large Scale Integration (VLSI)
Systems , 2003.
[19] Xianlong Hong, Sheqin Dong, Gang Huang, Yici Cai, Chung-Kuan Cheng, and Jun Gu. Corner block list
representation and its application to floorplan optimization. IEEE Transactions on Circuits and Systems II:
Express Briefs , 2004.
[20] Hai Zhou and Jia Wang. Acg-adjacent constraint graph for general floorplans. In IEEE International
Conference on Computer Design: VLSI in Computers and Processors , 2004.
[21] Paul Falkenstern, Yuan Xie, Yao-Wen Chang, and Yu Wang. Three-dimensional integrated circuits (3d
ic) floorplan and power/ground network co-synthesis. In Asia and South Pacific Design Automation
Conference (ASP-DAC) , 2010.
[22] Fuxing Huang, Duanxiang Liu, Xingquan Li, Bei Yu, and Wenxing Zhu. Handling orientation and aspect
ratio of modules in electrostatics-based large scale fixed-outline floorplanning. In IEEE/ACM International
Conference on Computer Aided Design (ICCAD) , 2023.
[23] Qi Xu, Hao Geng, Song Chen, Bo Yuan, Cheng Zhuo, Yi Kang, and Xiaoqing Wen. Goodfloorplan:
Graph convolutional network and reinforcement learning-based floorplanning. IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems , 2021.
[24] Mohammad Amini, Zhanguang Zhang, Surya Penmetsa, Yingxue Zhang, Jianye Hao, and Wulong Liu.
Generalizable floorplanner through corner block list representation and hypergraph embedding. In SIGKDD ,
2022.
[25] Wenbo Guan, Xiaoyan Tang, Hongliang Lu, Yuming Zhang, and Yimen Zhang. Thermal-aware fixed-
outline 3-d ic floorplanning: An end-to-end learning-based approach. IEEE Transactions on Very Large
Scale Integration (VLSI) Systems , 2023.
[26] Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang,
Young-Joon Lee, Eric Johnson, Omkar Pathak, Azade Nazi, et al. A graph placement methodology for fast
chip design. Nature , 2021.
[27] Ruoyu Cheng and Junchi Yan. On joint learning for solving placement and routing in chip design. NeurIPS ,
2021.
[28] Yao Lai, Yao Mu, and Ping Luo. Maskplace: Fast chip placement via reinforced visual representation
learning. NeurIPS , 2022.
[29] Yao Lai, Jinxin Liu, Zhentao Tang, Bin Wang, Jianye Hao, and Ping Luo. Chipformer: Transferable chip
placement via offline decision transformer. In ICML , 2023.
[30] Yunqi Shi, Ke Xue, Song Lei, and Chao Qian. Macro placement by wire-mask-guided black-box optimiza-
tion. NeurIPS , 2023.
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS , 2017.
[32] Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.
Graph attention networks. In ICLR , 2018.
[33] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. arXiv preprint arXiv:1810.12894 , 2018.
[34] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. NeurIPS , 1999.
[35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
[36] Zhou Fan, Rui Su, Weinan Zhang, and Yong Yu. Hybrid actor-critic reinforcement learning in parameterized
action space. In IJCAI , 2019.
[37] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438 , 2015.
[38] Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su,
and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. Journal of Machine
Learning Research , 2022.
12[39] Hiroshi Murata, Kunihiro Fujiyoshi, Shigetoshi Nakatake, and Yoji Kajitani. Vlsi module placement based
on rectangle-packing by the sequence-pair. IEEE Transactions on Computer-Aided Design of Integrated
Circuits and Systems , 1996.
[40] Dimitris Bertsimas and John Tsitsiklis. Simulated annealing. Statistical science , 1993.
[41] Xingbo Du, Ruizhe Zhong, Shixiong Kai, Zhentao Tang, Siyuan Xu, Jianye Hao, Mingxuan Yuan, and
Junchi Yan. Jigsawplanner: Jigsaw-like floorplanner for eliminating whitespace and overlap among
complex rectilinear modules. In 2024 IEEE/ACM International Conference on Computer Aided Design
(ICCAD) , pages 1–9. ACM, 2024.
[42] Jingwei Lu, Pengwen Chen, Chin-Chih Chang, Lu Sha, Dennis Jen-Hsin Huang, Chin-Chi Teng, and
Chung-Kuan Cheng. eplace: Electrostatics-based placement using fast fourier transform and nesterov’s
method. ACM Transactions on Design Automation of Electronic Systems (TODAES) , 2015.
[43] Yibo Lin, Shounak Dhar, Wuxi Li, Haoxing Ren, Brucek Khailany, and David Z Pan. Dreamplace: Deep
learning toolkit-enabled gpu acceleration for modern vlsi placement. In Proceedings of the 56th Annual
Design Automation Conference 2019 , 2019.
[44] Alexander A Sherstov and Peter Stone. Function approximation via tile coding: Automating parameter
choice. In International symposium on abstraction, reformulation, and approximation , 2005.
[45] Matthew Hausknecht and Peter Stone. Deep reinforcement learning in parameterized action space. arXiv
preprint arXiv:1511.04143 , 2015.
[46] Warwick Masson, Pravesh Ranchod, and George Konidaris. Reinforcement learning with parameterized
actions. In Proceedings of the AAAI conference on artificial intelligence , 2016.
[47] Jiechao Xiong, Qing Wang, Zhuoran Yang, Peng Sun, Lei Han, Yang Zheng, Haobo Fu, Tong Zhang, Ji Liu,
and Han Liu. Parametrized deep q-networks learning: Reinforcement learning with discrete-continuous
hybrid action space. arXiv preprint arXiv:1810.06394 , 2018.
[48] Craig J Bester, Steven D James, and George D Konidaris. Multi-pass q-networks for deep reinforcement
learning with parameterised action spaces. arXiv preprint arXiv:1905.04388 , 2019.
[49] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 ,
2013.
[50] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR , 2016.
[51] Boyan Li, Hongyao Tang, YAN ZHENG, HAO Jianye, Pengyi Li, Zhen Wang, Zhaopeng Meng, and
LI Wang. Hyar: Addressing discrete-continuous action reinforcement learning via hybrid action represen-
tation. In ICLR , 2021.
[52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. NeurIPS , 2019.
[53] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[54] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. NeurIPS ,
2016.
[55] Bing Xu. Empirical evaluation of rectified activations in convolutional network. arXiv preprint
arXiv:1505.00853 , 2015.
13A Notation
All notations in this paper are shown in Table 6.
Table 6: Notation.
Notation Meaning
b block
t I/O port (terminal)
d die/layer
D the set of all dies/layers
w, h width, height of a block
a area of a block
AR aspect ratio of a block
x, y 2D coordinate of a block/port
z the layer/die index of a block/port
W, H width, height of die/layer
oij overlap area between block bi, bj
aln(i, j) alignment score between block bi, bj
alnm(i, j)required minimum alignment area between block bi, bj
qi the block placing order FIFO queue of die di
fa alignment mask
fc canvas mask
fw wire mask
fp position mask
B Statistics of Benchmark
We evaluate the performance of FlexPlanner and other typical methods on public benchmark MCNC5and
GSRC6shown in Table 7. The ‘alignment’ means the number of blocks with alignment partner.
Table 7: MCNC and GSRC benchmark
circuit block I/O port net alignment
ami33 33 40 121 20
ami49 49 22 396 20
n10 10 69 118 10
n30 30 212 349 20
n50 50 209 485 30
n100 100 334 885 60
n200 200 564 1585 60
n300 300 569 1893 60
C Implementation Details
C.1 Computational Resources
We use PyTorch [ 52] deep learning framework and tianshou [ 38] Reinforcement Learning framework. We
select Adam [ 53] optimizer with learning rate 0.0001 . We train and test our model on a Linux server with one
NVIDIA GeForce RTX 3090 GPU with 24 GB CUDA memory, two AMD Ryzen Threadripper 3970X 32-Core
Processors at 3.70 GHz and 128 GB RAM.
5http://vlsicad.eecs.umich.edu/BK/MCNCbench/
6http://vlsicad.eecs.umich.edu/BK/GSRCbench/
14C.2 Minimum Alignment Requirement Configuration
For the alignment setting, given an alignment pair (bi, bj), the minimum alignment requirement alnm(i, j)is
set to alnm(i, j) =αij·min{ai, aj}, where ai, ajare the area of block bi, bj. The coefficient αijcontrols
the minimum alignment requirement between block bi, bj, and can be adjusted according to the specific
circumstances for each alignment pair. In our experiments, we set allαij= 1.0, which is the most challenging
scenario to evaluate the effectiveness of our approach.
C.3 Hyper-Parameters
Other hyper-parameters are shown in Table 8.
Table 8: Hyper-parameters.
Argument Value
learning rate 0.0001
parallel environments ne 8
buffer size ne×len(episode )
reward weight for alignment wa 0.5
reward weight for HPWL wl 1.0
reward weight for overlap wo 0.5
area utilization 85%
clip loss weight for position decision λ11.0
clip loss weight for layer decision λ2 1.0
clip loss weight for ratio decision λ3 0.5
range of block aspect ratio1
2,2
batch size 128
die width W 128
die height H 128
number of dies |D| 2
value loss weight λϕ 0.5
number of epochs 1,000
number of update epochs 10
clipε 0.2
reward discount factor γ 0.99
GAE λ 0.95
D Model Architecture
We introduce the networks in our pipeline shown in Fig. 1 as follows:
Shared Encoder. The shared encoder Eϕmainly consists of two parts: 1) a CNN-based backbone for
floorplanning vision modality input, and 2) a GNN (Graph Neural Network)-based backbone Graph Attention
Network [32] with two layers for graph modality.
In step t, given current block bt, we concatenate the alignment mask f(t)
a, canvas mask f(t)
c, wire mask f(t)
w, and
position mask f(t)
ptogether. We also incorporate f(t+1)
w, f(t+1)
p of header block in FIFO queue qof each die.
These masks of queue header blocks represent all possible choices in step t+ 1, providing policy with future
horizon. All of these input masks constitute the floorplanning vision modality.
Taking the netlist graph as input, the GNN backbone outputs nodes embeddings, representing the local receptive
field information. Besides, a global average pooling layer is applied on these node embeddings to calculate the
graph embedding, which captures the global view of the whole netlist graph.
Critic Network. The critic network Vϕmainly consists of three parts: 1) the shared encoder Eϕ, 2) a
sequence Transformer [ 31] with two encoder layers and two decoder layers for block placing order sequence, and
3) a die/layer embedding model to represent each die/layer with a learnable feature vector. The shared encoder
Eϕis responsible for processing the floorplanning vision input, and the Transformer is for the input sequence
modality. Serving as the source sequence in Transformer, the multi-die sequence feature S∈R|D|×L×Cis
employed in the calculation of memory, which is the output of self-attention. And the single block feature
15bk∈R1×Cis treated as the target sequence with length 1, serving as the query in cross-attention. To Stabilize
training, the shared encoder Eϕis only updated during the updating process of critic network. As a result, the
critic network is responsible to take input as the state stand output corresponding state value vt.
Policy Network.
•Position Decider. The position decider network π(1)
θmainly consists of a generator network. It takes input as
the output of the shared encoder Eϕ, and outputs a 2D feature map as the probability matrix to determine
the 2D position for current block. We borrow the generator architecture from InfoGAN [ 54], which utilizes
up-sampling layer instead of transposed convolutional layer to realize scaling-up. It contains three blocks and
each block is with convolutional layer, batch normalization, leaky ReLU [ 55] activation and up-sampling
layer. Finally, it outputs a probability matrix M(t)
p∈RW×Hto determine the position (xt, yt)for current
block bt.
•Layer Decider. The layer decider network π(2)
θmainly consists of three parts: 1) a CNN-based model for
FP vision modality, 2) a die embedding model to represent current input die, and 3) a Transformer network
to process the input block placing order sequence for each die. The CNN model consists of three blocks,
and each block has one convolutional layer, one max pooling layer, with ReLU as activation function. The
Transformer model consists of two encoder layers and two decoder layers, and is designed to process the input
multi-die block placing order sequence. The layer decider network takes input as the state stand outputs a
probability vector p(t)
z∈R|D|to determine the layer zt+1for accessing next block bt+1.
•Aspect Ratio Decider. The aspect ratio decider network π(3)
θmainly has a CNN model, consisting of three 2D
convolutional layers with ReLU activation function and max pooling layer. It takes input as the floorplanning
vision modality in state stand the feature of next block bt+1to determine the aspect ratio ARt+1forbt+1.
Since the aspect ratio is an action in contiguous space, we select Gaussian distribution to depict it, and the
network outputs corresponding mean value and standard deviation.
E Baselines
The baselines referred in Sec. 4 are introduced as follows:
3D-B*-SA [21] is a heuristic-based approach, which represents a 3D floorplanning by B*-tree [17]. Simulated
Annealing (SA) [ 40] is selected to search an optimal result. At each iteration, current B*-tree is randomly
perturbed to a new state, which will be accepted based on a certain probability. Finally, the B*-tree is converted to
corresponding floorplannning result via a decoding scheme. In previous scenarios, only HPWL and out-of-bound
penalty are incorporated into heuristics (cost/energy function). To optimize the alignment in 3D FP, we also
combine the alignment score into its heuristics to guide the solution searching.
RL-CBL [24] combines heuristic-based search Corner Block List (CBL) [ 18,19] and Reinforcement Learning
together, utilizing policy network to determine how to perturb current FP at each step. It utilizes intermediate
out-of-bound penalty and final HPWL as reward function. In 3D FP with multiple stacked layers, we extend CBL
to 3D scenario. In order to realize the optimization of cross-die block alignment, we also incorporate alignment
score into reward function. However, it is incapable of addressing the variable aspect ratio of soft blocks.
Wiremask-BBO [30] is a black-box optimization (BBO) framework, by using a wire-mask-guided [ 28] greedy
procedure for objective evaluation. At each step, it utilize the position mask to filter out invalid positions causing
overlap or out-of-bound. Within the valid positions, the location with the minimum increment of HPWL is
selected to place current block. Considering the alignment requirement in 3D FP, we incorporate the alignment
mask to further filter out invalid positions which do not satisfy the alignment constraint. However, it is incapable
of addressing the variable aspect ratio of soft blocks.
GraphPlace [26],DeepPlace [27],MaskPlace [28] are three learning-based approaches, utilizing Reinforce-
ment Learning to decide the position for each block on 2D chip layout. Variable aspect ratios of soft blocks
are not taken into consideration. In this task, we extend them to 3D scenarios, and incorporate the alignment
mask to filter out invalid positions where alignment constraint is not satisfied. Besides, alignment score is also
involved into the calculation of reward function, with the purpose of assisting them to realize the optimization of
cross-die alignment.
F Addtional Experiments
F.1 Runtime Comparison
Comparison of runtime among our approach and other baselines are shown in Table 9. The runtime of our method
is either lower or comparable to other learn-based methods [ 26,27,28], and is better than the heuristic-based
method [ 21], where tens of thousands of iterations are required. Pipeline of Wiremask-BBO [ 30] is the same as
16MaskPlace [ 28], while neural network is not involved in it, contributing to the shortest inference time. For the
approach RL-CBL [ 24], it is faster than other learning-based methods since its environment is relatively simple,
only responsible for decoding the heuristic representation Corner Block List (CBL) [ 18,19] into corresponding
FP result. However, the performance is also limited by CBL, and the final FP result is further worse than other
methods without the reliance of heuristic representation.
Table 9: Runtime (second) comparison among baselines and our method. The lower the runtime, the
better. The unit of runtime is in seconds. C/M means Circuit/Method.
C/M 3D-B*-SA [21] RL-CBL [24] Wiremask-BBO [30] GraphPlace [26] DeepPlace [27] MaskPlace [28] Ours
ami33 18.374 1.131 0.228 3.155 1.819 4.679 2.419
ami49 31.917 1.308 0.426 2.782 2.736 3.593 2.219
n10 6.679 0.965 0.094 1.813 1.330 2.682 2.026
n30 21.944 1.122 0.262 1.867 2.456 2.256 2.709
n50 35.385 1.323 0.427 4.778 3.599 2.342 2.533
n100 65.971 2.575 1.027 4.408 4.259 4.593 3.975
n200 127.140 2.583 2.477 5.915 6.187 5.475 7.991
n300 176.222 3.599 4.492 10.225 9.521 10.128 10.261
Avg. 60.454 1.826 1.179 4.368 3.989 4.469 4.266
F.2 Out-of-Bound
For heuristics-based methods, such as 3D-B*-SA [ 21] and RL-CBL [ 24], the heuristic representation will
be converted to corresponding floorplan result through a decoding scheme. Although non-overlap can be
guaranteed, blocks in the final FP could be out of the fixed outline, leading to the low area utilization of chip die
and out-of-bound. In our method FlexPlanner, each block can be naturally placed within the boundary due to
the alignment mask and position mask. Consequently, the occurrence of out-of-bound floorplan is effectively
prevented. We compare the out-of-bound in Table 10, and the outbound is calculated as follows:
xm= max
bi∈B{xi+wi}
ym= max
bi∈B{yi+hi}
outbound =max{0, xm−W}
2W+max{0, ym−H}
2H.(6)
Table 10: Comparison of out-of-bound, the lower the better.
Circuit/Outbound 3D-B*-SA [21] RL-CBL [24] Ours
ami33 0.000±0.000 0.087 ±0.043 0.000 ±0.000
ami49 0.000±0.000 0.213 ±0.059 0.000 ±0.000
n10 0.006±0.011 0.026 ±0.031 0.000 ±0.000
n30 0.000±0.000 0.116 ±0.025 0.000 ±0.000
n50 0.000±0.000 0.156 ±0.052 0.000 ±0.000
n100 0.000±0.000 0.223 ±0.081 0.000 ±0.000
n200 0.008±0.014 0.359 ±0.117 0.000 ±0.000
n300 0.014±0.009 0.549 ±0.097 0.000 ±0.000
Average 0.004 0.216 0.000
F.3 Overlap
We also compare performance on overlap among our method and baselines, shown in Table 11. Our method
reduces the overlap to 0.001, which comparable with 3D-B*-SA [ 21], RL-CBL [ 24], and better than other
methods. In 3D-B*-SA [ 21] and RL-CBL [ 24], although non-overlap can be guaranteed, blocks in corresponding
FP could be out of the fixed outline, leading to the low area utilization of chip die and out-of-bound as discussed
in Appendix F.2.
F.4 Influence of Grid Size
Due to the different size of circuits, we discretize the sizes into uniform region. Given a chip die with original
shape Wo×Ho, the region is projected into W×H. For each block bwith shape w×h, its shape is also
projected into maxn
1,round( w·W
Wo)o
×maxn
1,round( h·H
Ho)o
. We also empirically investigate the
17Table 11: Overlap comparison among baselines and our method. The lower the overlap, the better.
C/M 3D-B*-SA [21] RL-CBL [24] Wiremask-BBO [30] GraphPlace [26] DeepPlace [27] MaskPlace [28] Ours
ami33 0.000±0.000 0.000 ±0.000 0.050 ±0.027 0.037 ±0.025 0.024 ±0.016 0.012 ±0.009 0.000 ±0.000
ami49 0.000±0.000 0.000 ±0.000 0.000 ±0.001 0.002 ±0.002 0.001 ±0.001 0.009 ±0.008 0.000 ±0.000
n10 0.000±0.000 0.000 ±0.000 0.208 ±0.005 0.229 ±0.047 0.112 ±0.028 0.085 ±0.065 0.009 ±0.008
n30 0.000±0.000 0.000 ±0.000 0.007 ±0.007 0.042 ±0.028 0.037 ±0.022 0.016 ±0.005 0.000 ±0.000
n50 0.000±0.000 0.000 ±0.000 0.000 ±0.000 0.009 ±0.014 0.029 ±0.020 0.004 ±0.007 0.000 ±0.000
n100 0.000±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.002 ±0.001 0.000 ±0.000 0.000 ±0.000
n200 0.000±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000
n300 0.000±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000
Avg. 0.000 0.000 0.033 0.040 0.026 0.016 0.001
influence of different grid size W, H , shown in Table 12. Metric ‘Error’ evaluates the area error between the
projected region and the original region as follows:
Error =1
|B|X
b∈B|wWo
W×hHo
H−wo×ho|
wo×ho, (7)
whereBis the set of all blocks. Similar performances in terms of alignment, HPWL, and overlap are achieved
across different grid number settings. Lower grid numbers offer faster execution speeds but may result in
relatively higher area errors that may not meet precision requirements. Higher grid numbers produce more
fine-grained floorplanning results but lead to a larger action space and require additional computational resources.
As a trade-off between runtime and precision, a grid size of 128 is selected.
Table 12: The influence of different grid size on circuit n100.
Grid/Metric Alignment HPWL Overlap Runtime (s) Error
32 0.903 179,371 0.000 3.981 0.106
64 0.936 172,159 0.000 4.122 0.052
128 0.941 176,320 0.000 4.417 0.024
256 0.961 176,375 0.000 4.572 0.013
512 0.937 178,395 0.000 5.948 0.007
F.5 Floorplanning with Pre-Placed Modules
Our approach is also capable of addressing the circuits with pre-placed modules (PPMs). PPMs are blocks whose
position and aspect ratio are pre-determined and fixed during the FP process. Heuristics-based methods [ 21,24]
are incapable of address the existence of PPMs. In these heuristics approaches, after the perturbation is applied
on current FP representation, the entire FP layout result may undergo changes. During this process, it cannot
be guaranteed that the positions and shapes of pre-placed modules will remain unchanged. Consequently, they
lack the flexibility to address circuits with PPMs. Our method FlexPlanner is capable to solve this issue. At the
beginning of FP process (the beginning of an episode), all PPMs are placed on their pre-determined positions,
and their aspect ratios keep unchanged. After that, we modify the canvas mask and position mask, indicating that
these positions have been occupied by PPMs. In each step tto place a movable block bt, the overlap among bt
and other blocks (PPMs and other placed blocks) can be avoided. The floorplanning result with PPMs is shown
in Fig. 7.
G Algorithms
G.1 Algorithm for Calculation of Alignment Rate and Overlap
The calculation of alignment score alntand overlap otat step tare shown in Alg. 3 and Alg. 4. These values are
involved in the calculation of reward function with local advantage and global baseline shown in Alg. 2.
G.2 Training Algorithm
The overall training pipeline is shown in Alg. 5, based on PPO [ 35] algorithm with RL framework tianshou [ 38].
During a training epoch, the policy network collects training data into replay buffer by interacting with the envi-
ronment. To accelerate the training process, the policy network simultaneously interacts with neenvironments
18Die 01
23 4
5 6
7
89
10
11 121314
15
Die 11
23 4
5 6
7
89
10
11 121314
15Pre-Placed Module (PPM)
w/o aln partner(a) Circuit n50 with 6 PPMs.
Die 0
12
34
5
6 789
10
11
1213
1415
16
1718
1920
21
2223
2425262728
29
30
Die 1
12
34
5
6 789
10
11
1213
1415
16
1718
1920
21
2223
2425262728
29
30Pre-Placed Module (PPM)
w/o aln partner
(b) Circuit n100 with 12 PPMs.
Figure 7: Our 3D floorplan result. Blue blocks are PPMs whose position and aspect ratio are fixed
during FP. Two blocks with the same index on different dies form an alignment pair, which roughly
locate on the same positions and share a 2D common projected area.
Algorithm 3: Alignment score calculation.
Input: Current block bt, alignment score alnt−1at step t−1, number of alignment pairs na
Output: Alignment score alntat step t
alnt= aln t−1
ifbihas alignment parnter bjandbothbi, bjhave already been placed then
alnt+=aln(i,j)
na// according to Eq. 1
end
Algorithm 4: Overlap calculation.
Input: Current block bt, canvas mask fc, chip die width Wand height H
Output: Overlap otat step t
fc[zi, xi:xi+wi, yi:yi+hi] += 1
calculate overlap otas follows:
ot=P
z,x,ymax{0, fc[z, x, y ]−1}
WH
19in parallel. We set the buffer size to satisfy
Lbuf≡0 mod ( ne×len (episode )), (8)
ensuring that each episode in the replay buffer is completed and terminated with its final step, where Lbufis the
size of replay buffer. After data collection, we further compute the reward, cumulative discounted reward and
advantage. Since each episode is completed in the replay buffer, each step has the corresponding terminated step
(end step) within the same episode to calculate the reward with local advantage and global baseline. Finally,
both critic network and policy network are updated.
20Algorithm 5: Training Algorithm.
forepoch from 1ton_epochs do
// data collection
while replay buffer is not full do
// action probability calculation
calculate action probability for position of current block bt:πθ
a(1)
t|st
sample (xt, yt)∼πθ
a(1)
t|st
calculate action probability for layer to next block bt+1:πθ
a(2)
t|st
sample zt+1∼πθ
a(2)
t|st
calculate action probability for aspect ratio of next block bt+1:πθ
a(3)
t|st
sample ARt+1∼πθ
a(3)
t|st
// action execution and next state observation
execute the action at:
1. place block btat position (xt, yt)
2. access next block bt+1from the FIFO queue qzt+1of layer zt+1
3. set the aspect ratio of bt+1toARt+1
observe next state st+1from environment
get alignment score alnt, wirelength HPWL t, overlap otfrom environment
add sample
s, s′,(x, y, z, AR),(aln,HPWL , o),(π(1)
θold, π(2)
θold, π(3)
θold)
into buffer
end
// data processing
foreach sample inbuffer do
re-compute reward rwith local advantage and global baseline according to Alg. 2
calculate state value for current state s:v=Vϕ(s)
calculate state value for next state s′:v′=Vϕ(s′)
modify current sample with adding (r, v, v′)
end
foreach sample inbuffer do
compute cumulative discounted reward Gand advantage ˆAfor current sample according
to generalized advantage estimation (GAE) [37, 35]
ˆAt=PT−t−1
i=0(γλ)iδt+i, where δt=rt+γvt+1−vt
Gt=ˆAt+vt
modify current sample with adding (ˆA, G)
end
// network update
forupdate_epoch from 1ton_update_epochs do
forminibatch inbuffer do
// policy/actor network update
compute action probability π(1)
θ, π(2)
θ, π(3)
θ
compute action distribution entropy h(1)
θ, h(2)
θ, h(3)
θupdate policy/actor network according to Eq. 5
// critic network update
compute state value v=Vϕ(s)
update critic network according to Sec. 3.4
end
end
end
21NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope?
Answer: [Yes]
Justification: We make clear claims about contributions and scope of our paper.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims made in the
paper.
•The abstract and/or introduction should clearly state the claims made, including the contributions
made in the paper and important assumptions and limitations. A No or NA answer to this
question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how much the
results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals are not
attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the liminations in Sec. 6.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that the paper
has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to violations of
these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,
asymptotic approximations only holding locally). The authors should reflect on how these
assumptions might be violated in practice and what the implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was only tested
on a few datasets or with a few runs. In general, empirical results often depend on implicit
assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach. For
example, a facial recognition algorithm may perform poorly when image resolution is low or
images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide
closed captions for online lectures because it fails to handle technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms and how
they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to address problems
of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by reviewers
as grounds for rejection, a worse outcome might be that reviewers discover limitations that
aren’t acknowledged in the paper. The authors should use their best judgment and recognize
that individual actions in favor of transparency play an important role in developing norms that
preserve the integrity of the community. Reviewers will be specifically instructed to not penalize
honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and a complete
(and correct) proof?
Answer: [NA]
Justification: The paper does not include theoretical results.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
22•The proofs can either appear in the main paper or the supplemental material, but if they appear in
the supplemental material, the authors are encouraged to provide a short proof sketch to provide
intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented by
formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main experimental
results of the paper to the extent that it affects the main claims and/or conclusions of the paper
(regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We have provided detailed hyper-parameters settings in Appendix C and the full training
pipeline in Alg. 5 in Appendix G.1.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived well by the
reviewers: Making the paper reproducible is important, regardless of whether the code and data
are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken to make
their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways. For
example, if the contribution is a novel architecture, describing the architecture fully might suffice,
or if the contribution is a specific model and empirical evaluation, it may be necessary to either
make it possible for others to replicate the model with the same dataset, or provide access to
the model. In general. releasing code and data is often one good way to accomplish this, but
reproducibility can also be provided via detailed instructions for how to replicate the results,
access to a hosted model (e.g., in the case of a large language model), releasing of a model
checkpoint, or other means that are appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submissions
to provide some reasonable avenue for reproducibility, which may depend on the nature of the
contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how to
reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe the
architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should either be
a way to access this model for reproducing the results or a way to reproduce the model (e.g.,
with an open-source dataset or instructions for how to construct the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case authors are
welcome to describe the particular way they provide for reproducibility. In the case of
closed-source models, it may be that access to the model is limited in some way (e.g.,
to registered users), but it should be possible for other researchers to have some path to
reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instructions to
faithfully reproduce the main experimental results, as described in supplemental material?
Answer: [Yes]
We have provided detailed hyper-parameters settings in Appendix C and the full training pipeline in
Alg. 5 in Appendix G.1. Code and data will be public available once our paper is accepted.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be possible,
so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless
this is central to the contribution (e.g., for a new open-source benchmark).
•The instructions should contain the exact command and environment needed to run to reproduce
the results. See the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
23•The authors should provide instructions on data access and preparation, including how to access
the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new proposed
method and baselines. If only a subset of experiments are reproducible, they should state which
ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized versions (if
applicable).
•Providing as much information as possible in supplemental material (appended to the paper) is
recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [Yes]
Justification: We have provided detailed hyper-parameters settings in Appendix C and the full training
pipeline in Alg. 5 in Appendix G.1.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail that is
necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate informa-
tion about the statistical significance of the experiments?
Answer: [Yes]
Justification: Experiments in Sec. 4 are run five times with different seeds. Both mean value and
standard deviation are provided.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the main claims
of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for example,
train/test split, initialization, random drawing of some parameter, or overall run with given
experimental conditions).
•The method for calculating the error bars should be explained (closed form formula, call to a
library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error of the
mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report
a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is
not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or figures
symmetric error bars that would yield results that are out of range (e.g. negative error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how they were
calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
Answer: [Yes]
Justification: We provide the information about compute resources in Appendix C.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
provider, including relevant memory and storage.
24•The paper should provide the amount of compute required for each of the individual experimental
runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute than the
experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into
the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code
of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conform, in every respect, is with the NeurIPS Code
of Ethics.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a deviation
from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consideration due
to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative societal impacts
of the work performed?
Answer: [Yes]
Justification: Broader impacts including both positive and negative societal impacts are discussed in
Sec. 6.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal impact or
why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses (e.g.,
disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-
ment of technologies that could make decisions that unfairly impact specific groups), privacy
considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied to particular
applications, let alone deployments. However, if there is a direct path to any negative applications,
the authors should point it out. For example, it is legitimate to point out that an improvement in
the quality of generative models could be used to generate deepfakes for disinformation. On the
other hand, it is not needed to point out that a generic algorithm for optimizing neural networks
could enable people to train models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is being used
as intended and functioning correctly, harms that could arise when the technology is being used
as intended but gives incorrect results, and harms following from (intentional or unintentional)
misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation strategies
(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitor-
ing misuse, mechanisms to monitor how a system learns from feedback over time, improving the
efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible release of
data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or
scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with necessary
safeguards to allow for controlled use of the model, for example by requiring that users adhere to
usage guidelines or restrictions to access the model or implementing safety filters.
25•Datasets that have been scraped from the Internet could pose safety risks. The authors should
describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do not require
this, but we encourage authors to take this into account and make a best faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,
properly credited and are the license and terms of use explicitly mentioned and properly respected?
Answer: [Yes]
Justification: The license for both datasets GSRC and MCNC is MIT which can be found in http:
//vlsicad.eecs.umich.edu/BK/copyright.html
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of service of
that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package should
be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for
some datasets. Their licensing guide can help determine the license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of the derived
asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to the asset’s
creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation provided
alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their sub-
missions via structured templates. This includes details about training, license, limitations,
etc.
•The paper should discuss whether and how consent was obtained from people whose asset is
used.
•At submission time, remember to anonymize your assets (if applicable). You can either create an
anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper include
the full text of instructions given to participants and screenshots, if applicable, as well as details about
compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
•Including this information in the supplemental material is fine, but if the main contribution of the
paper involves human subjects, then as much detail as possible should be included in the main
paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
labor should be paid at least the minimum wage in the country of the data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects
26Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an
equivalent approval/review based on the requirements of your country or institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent) may be
required for any human subjects research. If you obtained IRB approval, you should clearly state
this in the paper.
•We recognize that the procedures for this may vary significantly between institutions and
locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for
their institution.
• For initial submissions, do not include any information that would break anonymity (if applica-
ble), such as the institution conducting the review.
27