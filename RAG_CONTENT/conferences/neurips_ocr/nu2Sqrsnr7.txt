Compute-Optimal Solutions for Acoustic Wave
Equation Using Hard-Constraint PINNs
Anonymous Author(s)
Affiliation
Address
email
Abstract
This paper explores the optimal imposition of hard constraints, strategic sampling 1
of PDEs, and computational domain scaling for solving the acoustic wave equation 2
within a specified computational budget. First, we derive a formula to systemat- 3
ically enforce hard boundary and initial conditions in Physics-Informed Neural 4
Networks (PINNs), employing continuous functions within the PINN ansatz to 5
ensure that these conditions are satisfied. We demonstrate that optimally selecting 6
these functions significantly enhances the convergence of the solution. Secondly, 7
we introduce a Dynamic Amplitude-Focused Sampling (DAFS) method that opti- 8
mizes the efficiency of hard-constraint PINNs under a fixed number of sampling 9
points. Leveraging these strategies, we develop an algorithm to determine the opti- 10
mal computational domain size, given a computational budget. Our approach offers 11
a practical framework for domain decomposition in large-scale implementation of 12
acoustic wave equation systems. 13
1 Introduction 14
The concept of using artificial neural networks to solve differential equations was first explored in the 15
1990s by Lagaris et al. [1998]. In the work of Lagaris et al. [1998], they developed an ansatz solution 16
that inherently satisfies the boundary conditions (BC) and the initial conditions (IC) of differential 17
equations. More recently, the advent of physics-informed neural networks (PINNs) was marked by 18
the influential study of Raissi et al. [2019]. This work leverages modern deep neural networks to solve 19
forward and inverse problems involving nonlinear partial differential equations (PDEs), incorporating 20
BCs and ICs through soft constraints in loss functions. 21
Subsequent research has introduced various modifications to PINNs to enhance their accuracy, 22
efficiency, and scalability [Lu et al., 2021a]. There are a couple of drawbacks for many PINNs with 23
soft constraints for BCs and ICs. The selection of weights and samples for BCs and ICs cannot 24
certainly be determined and requires many trial-and-error tests. Even when the loss function is 25
minimized, the BCs and ICs are not strictly satisfied. To target the scaling problems of general PDEs 26
and take advantage of parallel computing, XPINNs and FBPINNs have been developed based on 27
domain decomposition methods [Jagtap and Karniadakis, 2020, Shukla et al., 2021, Moseley et al., 28
2023]. 29
There are a few key points that these previous reseearches missed. First, how to formulate ansatz 30
solutions satisfying BCs and ICs, specifically the function multiplier of NN. Second, if BC and IC 31
are inheriently satisfied by constructing the ansatz solution, how to optimally sample the PDEs in the 32
training process. Furthermore, for the existing PINNs handling scaling problems, how to decompose 33
the domain to save the overall compute budget. 34
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.In this paper, we set up a 1D wave equation problem and investigate the optimal sampling and 35
constraint imposing method given a compute budget. 36
The contributions of this paper are as follows. 37
•We systematically derived the implementation of hard BC and IC constraints in PINNs to 38
solve acoustic wave equations. We give a strategy to select basic functions in the PINN 39
ansatz solution that guarantee the satisfaction of BCs and ICs. We find that optimal selection 40
of the basic function in the PINN ansatz can improve the convergence of PINNs. 41
•We developed a Dynamic Amplitude-Focused Sampling (DAFS) algorithm to improve the 42
convergence of hard-constraint PINNs for wave equations given a fixed number of sampling 43
points. 44
•With the hard constraint and importance sampling strategies, we propose an algorithm 45
to find the optimal size of the computational given a compute budget. This domain size 46
optimization algorithm can help the domain decomposition-based PINNs for large-scale 47
problems save computational cost. 48
2 Related Work 49
Hard constraint Hard constraint PINNs can guarantee the satisfaction of BCs, ICs, symmetries, 50
and/or conservation laws. There are comprehensive studies of embedding BCs in PINNs. Lu 51
et al. [2021b] demontrated various ansatz equations to strictly meet Dirichlet and periodic BCs, 52
and proposed the penalty method and the augamented Lagrangian method to impose inequality 53
constraints as hard constraints. Liu et al. [2022] developed a unified ansatz formula to enforce the 54
Dirichlet, Neumann, and Robin boundary conditions for high-dimensional and geometrically complex 55
domains. Moseley et al. [2023] implemented the hard Dirichlet in the subdomain using a tanh2(ωx) 56
function as the multiplier function of the neural networks in their FBPINN ansatz solution. However, 57
studies on how to impose both hard BC and IC constraints in PINNs for acoustic wave equations 58
that have a second-order time dirivative term are still limited. Alkhadhr and Almekkawy [2023] 59
compared the accuracy and performance of PINNs with a combination of hard-BC/soft-BC and 60
hard-IC/soft-IC for solving a 1D wave equation with a time-dependent point source function. This 61
implementation of the hard-IC only considers the satisfaction of the wavefield values at the initial 62
timeu(x, t= 0) , but neglects the hard constraint of the first-order time derivative of the wavefield 63
u(x, t), i.e., ∂tu(x, t= 0) . Brecht et al. [2023] proposed improved physics-informed DeepONets 64
with hard constraints, and presented a numerical example of a 1D standing wave equation with 65
Dirichlet BCs. The DeepONet framework used in the paper has an inherent satisfaction of the initial 66
wavefield, but ∂tu(x, t= 0) is also neglected. This neglection does not affect the numerical results 67
for the 1D standing wave equation in their paper, since they simply assume ∂tu(x, t= 0) = 0 . 68
Strategic Sampling Many sampling algorithms have been developed to improve the training effi- 69
ciency, mitigating failure modes of PINNs. [Wu et al., 2023] provided a comprehensive comparison of 70
ten sampling methods, including non-adaptive and residual-based adaptive methods. Daw et al. [2023] 71
proposed a Retain-Resample-Release (R3) Sampling algorithm to mitigate the failure propagation 72
during the training processes of PINNs. [Gao et al., 2023a,b] developed failure informed adamptive 73
sampling for PINNs, with the extentions of combining re-sampling and subset simulation. Yang et al. 74
[2023] introduced a Dynamic Mesh-Based Importance Sampling (DMIS) method to enhance the 75
training of PINNs. Additionally, [Zhang et al., 2024] proposed an annealed adaptive importance 76
sampling method for solving high-dimensional partial differential equations using PINNs. 77
Domain Scaling Computational domain scaling is a key issue to apply PINNs to real-world large 78
spatial-temporal scale applications. [Jagtap and Karniadakis, 2020] proposed a generalized space- 79
time domain decomposition framework for PINNs, named extended PINNs (XPINNs), which can 80
handle nonlinear PDEs on complex-geometry domains. XPINNs provide large representation and 81
parallelization capacity by deploying multiple neural networks in smaller subdomains, offering both 82
space and time parallelization to reduce training costs effectively. Shukla et al. [2021] developed 83
a distributed framework for PINNs based on two extensions: conservative PINNs (cPINNs) and 84
XPINNs. These methods employ domain decomposition in space and time-space, respectively, 85
enhancing the parallelization capacity, representation capacity, and efficient hyperparameter tuning of 86
2PINNs. The framework allows for optimizing all hyperparameters of each neural network separately 87
in each subdomain, providing significant advantages for multi-scale and multi-physics problems. They 88
demonstrated the efficiency of cPINNs and XPINNs through various forward problems, highlighting 89
that cPINNs are more communication-efficient while XPINNs offer greater flexibility for handling 90
complex subdomains. Moseley et al. [2023] addressed the limitations of PINNs in solving large 91
domains and multi-scale solutions by proposing Finite Basis PINNs (FBPINNs). FBPINNs use neural 92
networks to learn basis functions defined over small, overlapping subdomains, inspired by classical 93
finite element methods. This approach mitigates the spectral bias of neural networks and reduces the 94
complexity of the optimization problem by using smaller neural networks in a parallel, divide-and- 95
conquer approach. Their experiments showed that FBPINNs outperform standard PINNs in accuracy 96
and computational efficiency for both small and large, multi-scale problems. Chalapathi et al. [2024] 97
introduced a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE) 98
in neural network architectures. This method imposes constraints over smaller decomposed domains, 99
with each domain solved by an expert through differentiable optimization. The independence of each 100
expert allows for parallelization across multiple GPUs, improving accuracy, training stability, and 101
computational efficiency for predicting the dynamics of complex nonlinear systems. The optimal 102
decomposition of subdomains is critical to the effectiveness of these scaling methods, given a fixed 103
compute budget. Our work focuses on finding the maximum subdomain size that even a 64x2 small 104
PINN can handle within a compute budget. 105
3 Methodology 106
In this section, we outline our approach to effectively implement hard constraints, strategically 107
sampling partial differential equations (PDEs), and optimizing the scaling of computational domains. 108
These methods are utilized to solve the acoustic wave equation within a specified computational 109
budget. 110
We focus on an acoustic wave equation defined by: 111
D[u(x, t);c(x)] =f(x, t), x∈Ω, t∈[t0, T],
Bi[u(x, t)] =Ui(x, t),x∈∂Ωi, t∈[t0, T],
Ij[u(x, t0)] =Vj(x), x∈Ω,(1)
where: 112
•Drepresents the differential operator. For a simplified one-dimensional acoustic wave 113
equation, D=∂tt−c2(x)∇2, indicating the second temporal derivative minus the spatial 114
derivative scaled by the square of the local speed of sound, c(x). 115
•Bidenotes the boundary condition operator applied at x∈∂Ωi. 116
•Ijsignifies the initial condition operator, defining the state of the system at t=t0across 117
the domain Ω. 118
3.1 Hard constraint imposing 119
A prevalent ansatz employed in prior studies on hard-constraint PINNs for 1D wave equations is 120
expressed as: 121
u(x, t) =τ(t)˜u(x, t) + (1 −τ(t))u(x,0), (2)
where ˜u(x, t)represents the neural network output with inputs xandt, and τ(t)is a function that 122
satisfies τ(0) = 0 . This design ensures that the initial condition u(x,0)is met precisely when t= 0. 123
To accommodate boundary conditions (BCs) at x= 0andx=L, the ansatz is often modified to: 124
u(x, t) =x(L−x)˜u(x, t) +Ui(x, t), (3)
ensuring that u(xi, t) =Ui(xi, t)forx∈∂Ωi. 125
A more comprehensive form, 126
u(x, t) =x(L−x)τ(t)˜u(x, t) + (1 −τ(t))u(x,0)
+L−x
L(u(0, t)−(1−τ(t))u(0,0))
+x
L(u(L, t)−(1−τ(t))u(L,0)),(4)
3can ensure both Dirichlet BCs and the initial condition u(x, t)|t=0=u(x,0). However, this ansatz 127
does not account for ∂tu(x, t)|t=0, unless it is assumed to be zero. 128
We propose a more general hard constraint imposition formula: 129
u(x, t) =x(L−x)τ(t)˜u(x, t) + ((1 −τ(t)) +t∂t)u(x,0)
+L−x
L(u(0, t)−((1−τ(t)) +t∂t)u(0,0))
+x
L(u(L, t)−((1−τ(t)) +t∂t)u(L,0)),(5)
which guarantees satisfaction of the conditions: 130
u(x, t) =Ui(x, t), x ∈∂Ωi,
u(x, t)|t=0=Vj(x), x ∈Ω,
∂tu(x, t)|t=0=Wj(x), x ∈Ω,(6)
where Ui(x, t),Vj(x),Wj(x)are the specified functions in BCs and ICs, and τ(t)is an arbitrary 131
function satisfying τ(0) = dtτ(0) = 0 . 132
It is straightforward to demonstrate that the proposed ansatz correctly imposes all BCs and ICs as 133
required: 134

u(x, t)|x=0 =u(0, t),
u(x, t)|x=L =u(L, t),
u(x, t)|t=0 =u(x,0),
∂tu(x, t)|t=0=∂tu(x,0).(7)
In Section 4.2, we will explore numerical tests to optimize the selection of τ(t)by evaluating 135
convergence rates and mean absolute errors (MAE). 136
The primary advantage of employing hard constraints in our model is the elimination of the need to 137
fine-tune the weights of PDE, BC, and IC loss terms typically required in soft-constraint PINNs. 138
3.2 Sampling strategy 139
Sampling is crucial for efficient training of PINNs, ensuring rapid convergence and mitigating 140
potential failure modes. To enhance the computational efficiency of our hard-constraint PINNs, 141
we introduce the Dynamic Amplitude-Focused Sampling (DAFS) method. This strategy optimally 142
selects the number of points, Npde, used in the training. 143
Initially, we segmented the computational domain to identify regions with high-amplitude acoustic 144
wave fields, based on low-resolution finite difference (FD) simulations. These high-amplitude regions 145
are defined by a threshold δ, which determines the intensity level above which areas are considered 146
to be of high amplitude. Within these identified regions, we uniformly sampled αNpdepoints. This 147
was supplemented by uniformly sampling (1−α)Npdepoints in the remaining areas of the domain. 148
Both and αare parameters crucial to the sampling process and are optimally chosen to balance the 149
computational budget and the accuracy of the simulations. By adjusting these parameters, we can 150
tailor the distribution of sample points to areas that are most influential in the wave dynamics, thereby 151
improving the efficiency of our PINN training. 152
The pseudocode for the DAFS algorithm is provided in Algorithm 1. 153
This sampling strategy, characterized by its focus on dynamically identified regions of interest based 154
on wave amplitude, significantly optimizes the efficiency of the computation during the PINN training 155
phase. The numerical tests for DAFS are in Section 4.3. 156
4 Experiments 157
4.1 Problem setup 158
We applied our method to three numerical examples for three different types of 1D acoustic wave 159
equations — standing waves, string waves, and traveling waves. The ground truth wavefields are 160
shown in Figure 1. 161
4Algorithm 1 Dynamic Amplitude-Focused Sampling (DAFS)
Require: Npde, α,domain ,FD results (low-resolution Finite Difference results indicating amplitude)
Ensure: Sampled points for training
1:Initialize points ←[]
2:Identify high-amplitude regions from FD results
3:Nhigh←αN pde ▷Number of points in high-amplitude regions
4:Nlow←(1−α)Npde ▷Number of points in low-amplitude regions
5:Uniformly sample Nhighpoints in high-amplitude regions and add to points
6:Uniformly sample Nlowpoints in the remaining areas of the domain and add to points
return points
(a) standing waves
(b) string waves
(c) Gaussian traveling waves
Figure 1: Ground truth wavefields for (a) standing waves, (b) string waves, and (c) traveling waves
withk= 1,2,3.
Standing waves for Dirichlet BCs Our first numerical example is a standing wave solution for the 162
following 1D wave equation with Dirichlet BCs: 163
∂2u(x, t)
∂t2−c2∂2u
∂x2= 0, x∈(0, L)
B.C.: u(0, t) =u(L, t) = 0 ,
I.C.: u(x,0) = U(x),∂u
∂t(x,0) = V(x).(8)
The analytical solution u(x, t)for Equation 8 is 164
u(x, t) =∞X
n=1Ansinnπx
L
cosnπct
L
+Bnsinnπx
L
sinnπct
L
. (9)
A standing wave solution 165
u(x, t) = sinkπx
L
coskπct
L
, k∈Z+(10)
can be achieved if we assume U(x) = sin kπx
L
andV(x) = 0 . We show the solutions for k= 1,2,3 166
in Figure 1(a). 167
5String waves for time-dependent BCs Our third example is a string wave solution for time- 168
dependent BCs shown in Equation 11. The ground truth solutions in Figuer 1(b) are achieved by 169
finite different simulation. 170
∂2u(x, t)
∂t2−c2∂2u
∂x2= 0, x∈(0, L)
B.C.: u(0, t) =u(L, t) = sin(2 πt),
I.C.: u(x,0) = 0 ,∂u
∂t(x,0) = 2 πcos2kπx
L(11)
Traveling waves for Gaussian source time functions Our third example is a traveling wave 171
solution for initial conditions of Gaussian source time functions shown in Equation 12. The ground 172
truth solutions in Figuer 1(c) are computed by finite different simulation. 173
∂2u(x, t)
∂t2−c2∂2u
∂x2= 0, x∈(0, L)
B.C.: u(0, t) =u(L, t) = 0 ,
I.C.: u(x,0) =1
σ√
2πexp
−(x−µ)2
2σ2
,∂u
∂t(x,0) = 0(12)
4.2 Optimal τ(t)selection for hard constraints 174
We selected six candidate functions for τ(t)to construct PINNs with a network configuration of only 175
64x2 neurons. Figures 2 through 4 illustrate the L2loss and L1error as functions of training epochs. 176
Our findings suggest that τ(t)significantly influences both the convergence rate and the emergence of 177
failure modes. In general, t2,2t2
1+t2performs better in general, especially for higher modes k= 2,3. 178
We show a few training dynmaics in Appendix C. 179
Our analysis indicates that the frequency characteristics of τ(t)and the corresponding wavefields may 180
be critical for selecting an appropriate τ(t). Matching these characteristics can potentially enhance 181
the model’s efficiency by aligning τ(t)’s influence on the neural network’s learning dynamics with 182
the physical properties of the wave phenomena being modeled. 183
(a)L2loss
(b)L1error
Figure 2: L2loss and L1error for standing waves with PINNs constructed using six canditate τ(t)
functions.
6(a)L2loss
(b)L1error
Figure 3: L2loss and L1error for string waves with PINNs constructed using six canditate τ(t)
functions.
(a)L2loss
(b)L1error
Figure 4: L2loss and L1error for travelling Gaussian waves with PINNs constructed using six
canditate τ(t)functions.
4.3 Dynamic Amplitude-Focused Sampling 184
We demonstrate the efficacy of our proposed Dynamic Amplitude-Focused Sampling (DAFS) in 185
enhancing both the convergence and accuracy of Physics-Informed Neural Networks (PINNs). 186
Experiments varying αfrom 0 to 0.5 to 1 indicate that optimal results are typically achieved when α 187
is around 0.5. 188
This suggests a balanced sampling strategy, where a significant portion of the samples is concentrated 189
in regions of higher amplitude. However, exclusively focusing on these high-amplitude areas can 190
hinder information transfer from boundary conditions to the interior of the domain, potentially leading 191
to failure modes. Figures 5 and 6 illustrate these dynamics, showing the L2loss and L1error across 192
different values of α, and the impact on the predicted wavefield and its accuracy. 193
7Figure 5: L2loss and L1error with varied αfrom 0 to 1.
(a)α= 0.00
(b)α= 0.50
(c)α= 1.00
Figure 6: Visualizations for α= 0.00,0.50, and 1.00(top to bottom): Left - Predicted wavefield,
Middle - Difference between the prediction and ground truth, Right - Sampling distribution.
4.4 Optimal subdomain 194
We then propose an optimal subdomain selection method shown in a flow chart in Figure 7. This 195
method will automatically determine the optimal kour 64x2 small PINNs can handle, given a 196
compute budget. 197
5 Limitations and Training Dynamics 198
While our proposed methods significantly enhance the functionality and efficiency of PINNs, the 199
determination of the optimal function τ(t)presents certain limitations. The choice of τ(t)is crucial 200
as it directly affects the model’s ability to satisfy boundary and initial conditions rigidly. However, 201
finding an ideal τ(t)that adapts across different problems and boundary conditions without extensive 202
trial and error remains challenging. The training dynamics are also sensitive to the form of τ(t), where 203
inappropriate selections can lead to slower convergence or even divergence in some cases. These 204
issues underscore the need for a more automated, perhaps adaptive, approach to selecting τ(t)that 205
can dynamically adjust based on the evolving training characteristics and the specific requirements of 206
the PDE being solved. 207
8Figure 7: The flow chart of optimal subdomain determination.
6 Conclusion 208
This work presented a comprehensive approach to improving the effectiveness and efficiency of 209
Physics-Informed Neural Networks (PINNs) for solving acoustic wave equations. By integrating 210
a well-formulated hard constraint imposition strategy and the novel Dynamic Amplitude-Focused 211
Sampling (DAFS) method, we have significantly enhanced both the accuracy and convergence of 212
PINNs. 213
Our methodological innovations include: 214
•A systematic derivation of hard boundary and initial conditions in PINNs that ensures these 215
constraints are inherently satisfied, leading to better convergence and stability of the solution. 216
•The introduction of DAFS, which optimally allocates computational resources by focus- 217
ing sampling in regions of high amplitude while ensuring adequate coverage across the 218
computational domain to prevent information isolation. 219
•Development of a domain size optimization algorithm that assists in domain decompo- 220
sition, enabling efficient scaling of PINNs for large-scale applications while managing 221
computational costs. 222
These contributions mark a significant step forward in the practical deployment of PINNs, especially 223
in fields requiring the simulation of complex physical phenomena over large scales. Future work will 224
focus on extending these strategies to other types of partial differential equations and exploring the 225
integration of our methods with other deep learning frameworks to further enhance the adaptability 226
and efficiency of PINNs in diverse applications, for example, we will explore the integration of our 227
methods with existing PINNs frameworks that employ domain decomposition techniques, such as 228
XPINNs and FBPINNs, to further enhance their scalability and adaptability. We aim to make PINNs 229
more adaptable and efficient for a broader range of applications, particularly in complex systems 230
where traditional numerical methods struggle. By advancing these strategies, we can significantly 231
contribute to the deployment of PINNs in real-world scenarios, tackling large-scale and multi-scale 232
challenges effectively. 233
9References 234
Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and 235
partial differential equations. IEEE transactions on neural networks , 9(5):987–1000, 1998. 236
Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning 237
framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal 238
of Computational physics , 378:686–707, 2019. 239
Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving 240
differential equations. SIAM review , 63(1):208–228, 2021a. 241
Ameya D Jagtap and George Em Karniadakis. Extended physics-informed neural networks (xpinns): A 242
generalized space-time domain decomposition based deep learning framework for nonlinear partial differential 243
equations. Communications in Computational Physics , 28(5):2002–2041, 2020. 244
Khemraj Shukla, Ameya D Jagtap, and George Em Karniadakis. Parallel physics-informed neural networks via 245
domain decomposition. Journal of Computational Physics , 447:110683, 2021. 246
Ben Moseley, Andrew Markham, and Tarje Nissen-Meyer. Finite basis physics-informed neural networks 247
(fbpinns): a scalable domain decomposition approach for solving differential equations. Advances in 248
Computational Mathematics , 49(4):62, 2023. 249
Lu Lu, Raphaël Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G. Johnson. Physics- 250
Informed Neural Networks with Hard Constraints for Inverse Design. SIAM Journal on Scientific Computing , 251
43(6):B1105–B1132, January 2021b. ISSN 1064-8275, 1095-7197. doi: 10.1137/21M1397908. URL 252
https://epubs.siam.org/doi/10.1137/21M1397908 . 253
Songming Liu, Zhongkai Hao, Chengyang Ying, Hang Su, Jun Zhu, and Ze Cheng. A Unified Hard-Constraint 254
Framework for Solving Geometrically Complex PDEs. Advances in Neural Information Processing Systems , 255
35:20287–20299, 2022. 256
Shaikhah Alkhadhr and Mohamed Almekkawy. Wave Equation Modeling via Physics-Informed Neural Networks: 257
Models of Soft and Hard Constraints for Initial and Boundary Conditions. Sensors , 23(5):2792, March 2023. 258
ISSN 1424-8220. doi: 10.3390/s23052792. URL https://www.mdpi.com/1424-8220/23/5/2792 . 259
Rüdiger Brecht, Dmytro R. Popovych, Alex Bihlo, and Roman O. Popovych. Improving physics- 260
informed DeepONets with hard constraints, September 2023. URL http://arxiv.org/abs/2309.07899 . 261
arXiv:2309.07899 [physics]. 262
Chenxi Wu, Min Zhu, Qinyang Tan, Yadhu Kartha, and Lu Lu. A comprehensive study of non-adaptive and 263
residual-based adaptive sampling for physics-informed neural networks. Computer Methods in Applied 264
Mechanics and Engineering , 403:115671, 2023. 265
Arka Daw, Jie Bu, Sifan Wang, Paris Perdikaris, and Anuj Karpatne. Mitigating propagation failures in physics- 266
informed neural networks using retain-resample-release (r3) sampling. In International Conference on 267
Machine Learning , pages 7264–7302. PMLR, 2023. 268
Zhiwei Gao, Liang Yan, and Tao Zhou. Failure-informed adaptive sampling for pinns. SIAM Journal on Scientific 269
Computing , 45(4):A1971–A1994, 2023a. 270
Zhiwei Gao, Tao Tang, Liang Yan, and Tao Zhou. Failure-informed adaptive sampling for pinns, part ii: combin- 271
ing with re-sampling and subset simulation. Communications on Applied Mathematics and Computation , 272
pages 1–22, 2023b. 273
Zijiang Yang, Zhongwei Qiu, and Dongmei Fu. Dmis: Dynamic mesh-based importance sampling for train- 274
ing physics-informed neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence , 275
volume 37, pages 5375–5383, 2023. 276
Zhengqi Zhang, Jing Li, and Bin Liu. Annealed adaptive importance sampling method in pinns for solving high 277
dimensional partial differential equations. arXiv preprint arXiv:2405.03433 , 2024. 278
Nithin Chalapathi, Yiheng Du, and Aditi Krishnapriyan. Scaling physics-informed hard constraints with 279
mixture-of-experts. arXiv preprint arXiv:2402.13412 , 2024. 280
10A Phase diagrams of loss weights 281
Figure 8: Phase diagrams
B Seed 282
C Training dynmaics 283
mono: 284
string: increase Npde to 104, we have converged solution(each 104steps): 285
11(a) standing waves
(b) string waves
(c) Gaussian traveling waves
Figure 9: t2,t2
t2+1,2t2
t2+1,tanh2(t),
tanh( t)
tanh(1)2
12Figure 10: 0, 1000, 2000, and the last(converged)
NeurIPS Paper Checklist 286
The checklist is designed to encourage best practices for responsible machine learning research, addressing 287
issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The 288
papers not including the checklist will be desk rejected. The checklist should follow the references and follow 289
the (optional) supplemental material. The checklist does NOT count towards the page limit. 290
Please read the checklist guidelines carefully for information on how to answer these questions. For each 291
question in the checklist: 292
• You should answer [Yes] , [No] , or [NA] . 293
•[NA] means either that the question is Not Applicable for that particular paper or the relevant 294
information is Not Available. 295
• Please provide a short (1–2 sentence) justification right after your answer (even for NA). 296
The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area 297
chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) 298
with the final version of your paper, and its final version will be published with the paper. 299
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While 300
"[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper 301
justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or 302
"we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not 303
grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is 304
often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting 305
evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer 306
[Yes] to a question, in the justification please point to the section(s) where related material for the question can 307
be found. 308
13Figure 11: 0, 10000, 20000, and the last(converged)
IMPORTANT, please: 309
•Delete this instruction block, but keep the section heading “NeurIPS paper checklist" , 310
•Keep the checklist subsection headings, questions/answers and guidelines below. 311
•Do not modify the questions and only use the provided macros for your answers . 312
1.Claims 313
Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s 314
contributions and scope? 315
Answer: [Yes] 316
Justification: NA 317
2.Limitations 318
Question: Does the paper discuss the limitations of the work performed by the authors? 319
Answer: [Yes] 320
Justification: NA 321
3.Theory Assumptions and Proofs 322
Question: For each theoretical result, does the paper provide the full set of assumptions and a complete 323
(and correct) proof? 324
Answer: [TODO] 325
Justification: [TODO] 326
4.Experimental Result Reproducibility 327
14Question: Does the paper fully disclose all the information needed to reproduce the main experimental 328
results of the paper to the extent that it affects the main claims and/or conclusions of the paper 329
(regardless of whether the code and data are provided or not)? 330
Answer: [TODO] 331
Justification: [TODO] 332
5.Open access to data and code 333
Question: Does the paper provide open access to the data and code, with sufficient instructions to 334
faithfully reproduce the main experimental results, as described in supplemental material? 335
Answer: [TODO] 336
Justification: [TODO] 337
6.Experimental Setting/Details 338
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, 339
how they were chosen, type of optimizer, etc.) necessary to understand the results? 340
Answer: [TODO] 341
Justification: [TODO] 342
7.Experiment Statistical Significance 343
Question: Does the paper report error bars suitably and correctly defined or other appropriate informa- 344
tion about the statistical significance of the experiments? 345
Answer: [TODO] 346
Justification: [TODO] 347
8.Experiments Compute Resources 348
Question: For each experiment, does the paper provide sufficient information on the computer 349
resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? 350
Answer: [TODO] 351
Justification: [TODO] 352
9.Code Of Ethics 353
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code 354
of Ethics https://neurips.cc/public/EthicsGuidelines ? 355
Answer: [TODO] 356
Justification: [TODO] 357
10.Broader Impacts 358
Question: Does the paper discuss both potential positive societal impacts and negative societal impacts 359
of the work performed? 360
Answer: [TODO] 361
Justification: [TODO] 362
11.Safeguards 363
Question: Does the paper describe safeguards that have been put in place for responsible release of 364
data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or 365
scraped datasets)? 366
Answer: [TODO] 367
Justification: [TODO] 368
12.Licenses for existing assets 369
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, 370
properly credited and are the license and terms of use explicitly mentioned and properly respected? 371
Answer: [Yes] 372
Justification: NA 373
13.New Assets 374
Question: Are new assets introduced in the paper well documented and is the documentation provided 375
alongside the assets? 376
Answer: [No] 377
15Justification: NA 378
14.Crowdsourcing and Research with Human Subjects 379
Question: For crowdsourcing experiments and research with human subjects, does the paper include 380
the full text of instructions given to participants and screenshots, if applicable, as well as details about 381
compensation (if any)? 382
Answer: [No] 383
Justification: NA. 384
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 385
Question: Does the paper describe potential risks incurred by study participants, whether such 386
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an 387
equivalent approval/review based on the requirements of your country or institution) were obtained? 388
Answer: [No] 389
Justification: NA 390
16