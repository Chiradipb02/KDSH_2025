Learning to Understand:
Identifying Interactions via the M ¨obius Transform
Justin Singh Kang
UC Berkeley
justin kang@berkeley.eduYigit Efe Erginbas
UC Berkeley
erginbas@berkeley.eduLandon Butler
UC Berkeley
landonb@berkeley.edu
Ramtin Pedarsani
UC Santa Barbara
ramtin@ece.ucsb.eduKannan Ramchandran
UC Berkeley
kannanr@berkeley.edu
Abstract
One of the key challenges in machine learning is to find interpretable representa-
tions of learned functions. The M ¨obius transform is essential for this purpose, as
its coefficients correspond to unique importance scores forsets of input variables .
This transform is closely related to widely used game-theoretic notions of impor-
tance like the Shapley andBhanzaf value , but it also captures crucial higher-order
interactions. Although computing the M ¨obius Transform of a function with nin-
puts involves 2ncoefficients, it becomes tractable when the function is sparse and
oflow degree as we show is the case for many real-world functions. Under these
conditions, the complexity of the transform computation is significantly reduced.
When there are Knon-zero coefficients, our algorithm recovers the M ¨obius trans-
form in O(Kn)samples and O(Kn2)time asymptotically under certain assump-
tions, the first non-adaptive algorithm to do so. We also uncover a surprising con-
nection between group testing and the M ¨obius transform. For functions where all
interactions involve at most tinputs, we use group testing results to compute the
M¨obius transform with O(Ktlogn)sample complexity and O(Kpoly( n))time.
A robust version of this algorithm withstands noise and maintains this complexity.
This marks the first nsub-linear query complexity, noise-tolerant algorithm for the
M¨obius transform. In several examples, we observe that representations generated
via sparse M ¨obius transform are up to twice as faithful to the original function, as
compared to Shapley and Banzhaf values, while using the same number of terms.
1 Introduction
As machine learning models become increasingly complex, our ability to interpret them has not
kept pace. A natural question to ask is: What is the most fundamental interpretable representation
of the functions we learn? The Shapley value [ 1], a concept from cooperative game theory, has
become a popular way to interpret model predictions [ 2] by assigning importance scores to individual
inputs such as features, data samples or tokens. This value represents the weighted average marginal
contribution of an input, quantifying the change in the function’s output when that input is included.
Recent research has expanded the scope of interpretability to encompass sets of inputs [ 3,4], capturing
the collective influence of input combinations and their synergies on model predictions. Central to
this advancement is the M ¨obius Transform [ 5], a mathematical transformation that projects functions
onto a fundamental interpretable basis known in game theory as the unanimity function basis .
The M ¨obius transform has a more powerful and nuanced explanation capability than the Shapley
value. Consider a sentiment analysis model (BERT [ 6] fine-tuned on the IMDB dataset [ 7]) explained
38th Conference on Neural Information Processing Systems (NeurIPS 2024).f
Her
acting
never
fails
to
impress
=F
Her
acting
never
fails
to
impress
+F
Her
acting
never
fails
to
impress
+F
Her
acting
never
fails
to
impress
+F
Her
acting
never
fails
to
impress
+· · ·+F
Her
acting
never
fails
to
impress
+F
Her
acting
never
fails
to
impress
+F
Her
acting
never
fails
to
impress
+· · ·+F
Her
acting
never
fails
to
impress
+. . .
+0 .98 0.00 −0.80 −0.96 +0 .59 +0 .87 −0.75 −0.58 +2 .59
0th Order 1st Order 2nd Order 3rd Order and Higher
f
Her
acting
never
fails
to
impress
=F
Her
acting
never
fails
to
impress
+F
Her
acting
never
fails
to
impress
+F
Her
acting
never
fails
to
impress
+. . . +F
Her
acting
never
fails
to
impress
+. . .
−0.96 0.00 −0.96 +0 .59 −0.58
SV/bracketleftig
Her/bracketrightig
SV/bracketleftig
acting/bracketrightig
SV/bracketleftig
never/bracketrightig
SV/bracketleftig
fails/bracketrightig
SV/bracketleftig
to/bracketrightig
SV/bracketleftig
impress/bracketrightig
+0 .05 +0 .01 +0 .32 +0 .19 +0 .10 +0 .31Figure 1: The movie review “Her acting never fails to impress” is passed into a BERT language model
fine-tuned to do sentiment analysis [ 8]. Presented are 1st,2ndand3rdorder M ¨obius coefficients,
with positive interactions in green and negative in red computed via (1). The coefficients explain how
groups of words influence BERT’s perception of sentiment. For instance, while never andfails have
strong negative sentiments individually, when combined, they impose a profound positive sentiment.
In the second row, the word never is deleted, resulting in a large change in sentiment. In contrast, the
Shapley values of each word SV(·), presented at the bottom of the figure, are less informative.
using both Shapley values and the M ¨obius transform as depicted in Fig. 1. The model’s objective
is to classify the sentiment of the review as positive or negative. The M ¨obius transform assigns a
score to all word subsets within a sentence. For instance, in the sentence “Her acting never fails to
impress” each subset of words is evaluated—positive interactions receive positive scores, and negative
interactions, negative scores. Summing these scores yields the overall sentiment +0.98. This granular
analysis reveals the model’s understanding of linguistic constructs like double negatives, as seen in the
interaction between never andfails, and the inherent positivity of words like impress . When the word
never is masked, interactions involving never are excluded, shifting the sentiment negatively to −0.96.
This level of detail is not readily available with the Shapley value, which assigns scores to individual
words without considering their interplay. The value of the M ¨obius transform is apparent, but given
its complex structure, is it possible to compute efficiently?
In general, to compute a M ¨obius transform over nfeatures requires 2ninferences (masking over all 2n
subsets of features), as well as n2ntime using a divide-and-conquer approach similar to that of the Fast
Fourier Transform (FFT) algorithm. GPT-4 currently supports in the range of 8000 words-per-prompt,
and context length will continue to grow with new architectures [ 9]. Running inference 28000times is
not even close to possible, and even if you could, 28000coefficients are hardly interpretable! In Fig. 1
we see that many coefficients are insignificant . This is typical. The solution to the computational
problem is to just focus on computing the largest M ¨obius interactions and ignore the small ones. Is
this possible in a systematic way? Yes—assuming that only KM¨obius coefficients (which Kvalues
are significant is unknown) are non-zero, our algorithm enables us to intelligently query the model to
significantly reduce the number of samples of that are required to O(Kn)withO(Kn2)time. We
also explore the regime where the non-zero interactions occur between at most tinputs, with t≪n,
showing that only O(Ktlog(n))samples are required in O(Kpoly( n))time. We also have a robust
algorithm that allows for some noise in the sampling process, effectively relaxing the constraint that
the insignificant coefficients are exactly zero while maintaining the same complexities.
Defining the M ¨obius Transform We define a value function for a model with ninputs across
subsets S⊆[n]denoted as f(S). The construction of this function varies based on the model: in
Fig. 1, words not in Smight be masked or omitted. In other cases, we might take a conditional
expectation over words not in S. To facilitate later discussion on group testing, we express the function
asf:Zn
2→R, where f(S) =f(m)withS={i:mi= 1}. The relationship between f:Zn
2→R
2and its M ¨obius transform F:Zn
2→Ris characterized by the forward and inverse transforms:
Inverse: f(m) =X
k≤mF(k), Forward: F(k) =X
m≤k(−1)1T(k−m)f(m), (1)
where k≤mmeans that ki≤mi∀i. This transform acts as a bridge, connecting various importance
metrics, which can be expressed as projections onto a subset of the M ¨obius basis. The Shapley value
SV(i)and Banzhaf value BZ(i)for feature iis elegantly represented within this framework:
SV(i) =X
k:ki=11
|k|F(k), BZ(i) =X
k:ki=11
2|k|−1F(k). (2)
These relationships are foundational to the definition of the Shapley and Bhanzaf values themselves.
The left equality appears as Eq. 10 in Lloyd Shapley’s original report from 1952 [ 1] where the concept
of Shapley value was first introduced and is central to his derivation of a closed-form expression.
1.1 Related Works and Applications
This work is inspired by the literature on sparse Fourier transforms, which began with [ 10,11,12].
The sparse Boolean Fourier (Hadamard) transform [13, 14] is most relevant.
Group Testing This manuscript establishes a strong connection between the interaction identification
problem and group testing [ 15]. Group testing was first described by Dorfman [ 16], who noted that
when testing soldiers for syphilis, pooling blood samples from many soldiers, and testing the pooled
blood samples reduced the total number of tests needed. [ 17] is the first work to exploit group testing
in a feature selection/importance problem, using a group testing matrix in their algorithm. [ 18] also
mentions group testing in relation to Shapley values.
M¨obius Transform M¨obius transforms [ 5] have been studied in the pseudo-Boolean (set) function
literature, and dates back to at least [ 19]. [20] develops a framework for computing sparse transforms
of pseudo-Boolean functions. They do not directly consider the M ¨obius transform as we define it,
but one can apply their algorithm to compute a Ksparse transform in O(nK)adaptive samples and
O(K2n)time. In the sparse and noiseless setting, our algorithm improves on this by being fully non-
adaptive and having lower time complexity in most non-trivial settings. [ 20] does not consider the
important low degree setting and does not consider robustness to noise (approximate sparsity), which
are critical aspects of this work. In [ 21], the authors show that a classifier satisfying certain properties
can be well represented by a sparse and low degree M ¨obius transform.
Explainability [2] proposes model explanation via pseudo-Boolean functions approximated by
Shapley values, effectively utilizing only first-order M ¨obius coefficients. Constructing these functions,
[22,23,24,25] especially for generative models with complex outputs [ 26,27,28], is an ongoing
research area. [ 3] presents the Taylor-Shapley interaction index (STII), scoring interactions up to size
t. For sets smaller than t, STII are exactly M ¨obius coefficients. [ 4] introduces the Faithful Shapley
Interaction index (FSI), which computes scores via projection onto up to tthorder M ¨obius coefficients.
[29] develops methods for computing FSI, STII, and other interaction indices. The relationship
between the M ¨obius transform, FSI, STII, Shapley value, and Banzhaf value is detailed in Appendix A.
Data Valuation In data valuation [ 18] the goal is to assign an importance score to data, either to
determine a fair price [ 30] or to curate a more efficient dataset [ 31]. A feature of this problem is the
high cost of getting a sample since we need to determine the accuracy of our model when trained on
different subsets of data, making sample complexity of critical importance. [ 32,33] try to approximate
this by looking at the accuracy of partially trained models, though this introduces sampling noise.
1.2 Main Contributions
Our algorithm and proofs are deeply interdisciplinary , and the contributions of this paper are
theoretical. We use modern ideas spanning across signal processing, algebra, coding and information
theory, and group testing to address the important problem of interpretability at the forefront of
machine learning. The main contributions of this manuscript are:
•For a function with Knon-zero M ¨obius coefficients chosen uniformly at random, the Sparse
M¨obius Transform (SMT) algorithm exactly recovers the transform FinO(Kn)samples and
O(Kn2)time in the limit as n→ ∞ withKgrowing at most as 2nδwithδ≤1
3.
3100101102103104105106
Sparsity0.000.250.500.751.00R2(Faithfulness)
XGBoost for Breast Cancer Diagnosis
100101102103104105
Sparsity0.000.250.500.751.00R2(Faithfulness)
BERT for Sentiment Analysis
100101102103104
Sparsity0.000.250.500.751.00R2(Faithfulness)
BERT for Multiple Choice
0 3 6 9 12 15 18 21
Degree0.000.250.500.751.00R2(Faithfulness)
0 3 6 9 12 15 18
Degree0.000.250.500.751.00R2(Faithfulness)
0 3 6 9 12 15
Degree0.000.250.500.751.00R2(Faithfulness)
Figure 2: These plots are strong indicators that sparsity and low degree assumptions are worthy of
consideration. We consider three different learning tasks. The left-most plot shows results from an
XGBoost [ 34] model used for breast cancer diagnosis. The middle plot shows results from word-
level sentiment analysis task using a BERT model [ 8] like in Fig. 1. The right-most plot shows results
from a multiple choice question and answer task also using a BERT model [ 35]. Error bars represent
standard deviation over 10 different instances. Details for each setting are in Appendix B. In all cases,
the number of features n≈20, for which it is possible to perform the full M ¨obius transform. On the
top row, we plot achievable faithfulness R2as a function of sparsity. We observe that in allcases,
faithfulness approaching 1requires only a few thousand M ¨obius coefficients, motivating our sparsity
assumption. The bottom row of plots considers achievable faithfulness vs. degree, i.e., what R2can
be achieved using only M ¨obius coefficients ˆFup to a given degree. Here we observe that in nearly all
cases, low degree coefficients suffice to get quite small R2, motivating our low degree assumption.
•We develop a formal connection with group testing and present a variant of SMT that works when
all non-zero interactions are low order. If the maximum order of interaction is t= Θ( nα)where
α <0.409then we can compute the M ¨obius transform in O(Ktlog(n))samples in O(Kpoly( n))
time with error going to zero as n→ ∞ with growing K.
•Using robust group testing, we develop an algorithm that, under certain assumptions, computes
the M ¨obius transform in O(Ktlog(n))samples, with vanishing error as n→ ∞ with growing K.
In addition to our asymptotic analysis, we provide synthetic and real-world experiments that verify
that our algorithm performs well even in the finite nregime. Furthermore, our results are non-adaptive
meaning that all samples can be computed in parallel. Code has been made publicly available1.
Notation Lowercase boldface xand uppercase boldface Xdenote vectors and matrices respectively.
x≥ymeans that xi≥yi∀i. Multiplication is always standard real field multiplication, but addition
between two elements in Z2should be interpreted as a logical OR ∨. We define subtraction, of
x−yforx≥yby standard real field subtraction. ¯xcorresponds to bit-wise negation for Boolean x,
andx⊙yrepresents an element-wise multiplication.
2 Understanding Assumptions: Sparsity and Low Degree
Computing the forward transform (1)typically requires sampling all 2ninput combinations, an
infeasible task, even for modest n. For an arbitrary f, one cannot do any better. In fact, the same is
true of the Shapley value, yet, computational tools like SHAP [ 2] exist because practical functions of
interest are not arbitrary . To help understand this, we define faithfulness for an explanation model ˆf:
R2= 1− ∥ˆf−f∥2/∥f∥2,where∥f∥2=X
m∈Zn
2f(m)2. (3)
Note that this corresponds to the standard definition of R2in statistics when fis zero-mean, and
we generally define fsuch that this is the case. A good explanation model should have a high R2,
1https://github.com/basics-lab/sparseMobiusTransform
4a succinct representation, and most importantly, be easily computed. For the M ¨obius transform,
we aim to learn coefficients ˆF(k)efficiently and construct ˆfusing the inverse transform (1). With
no restrictions on ˆF(k)we can achieve R2= 1, but this fails to meet our simplicity criterion.
Fortunately, many real-world functions are sparse —only a few ˆF(k)coefficients need to be non-
zero to yield R2≈1. Fig. 2 considers three machine learning models for breast cancer diagnosis,
sentiment analysis, and question answering respectively. In all three cases, we find that we only need
a small number of M ¨obius coefficients to achieve R2≈1. Furthermore, real-world functions are low
degree , such that those small number of non-zero coefficients satisfy |k| ≤tfor some small t. This
results in a much more compact representation and as we shall see, also enables efficient computation.
Fig. 2 validates our assumption for the deep-learning models mentioned above. Prior research [ 36,21],
have presented empirical and theoretical evidence that sparsity and low degree properties are common
in well-trained models. Further investigation of the spectral properties of explanation functions could
be a promising avenue for future research. Our formal statements of assumptions are given below:
Assumption 2.1. (KUniform Interactions) f:Zn
27→Rhas a M ¨obius transform of the following
form: k1, . . . ,kKare sampled uniformly at random from Zn
2, and have F(ki)̸= 0,∀i∈[K], but
F(k) = 0 for all other k∈Zn
2.
Assumption 2.2. (K t-Degree Interactions) f:Zn
27→Rhas a M ¨obius transform of the following
form: k1, . . . ,kKare sampled uniformly from {k:|k| ≤t,k∈Zn
2}, and have F(ki)̸= 0,∀i∈
[K], butF(k) = 0 otherk∈Zn
2.
Assumption Limitations By assuming that the non-zero coefficients are uncorrelated and uniformly
distributed, we aim to understand the fundamental difficulty in learning a sparse M ¨obius transform.
Correlation between non-zero coefficients means identifying one coefficient would tell us information
about the locations of the others, which can be further exploited. The existence of a scheme that works
well under the uniform setting suggests that it is possible to solve the problem where correlations
between interactions exist. We also consider exact sparsity in our assumptions. In practice, these
“zero” coefficients may instead have some small magnitude. We investigate this in Section 4.
3 Algorithm Overview
3.1 Subsampling and AliasingAlgorithm 1 Sparse M ¨obius Transform (SMT)
1:Input: Hc∈Zb×n
2forc= 1, . . . , C
2: Dc∈ZP×n
2 forc= 1, . . . , C
3:ˆF(k)←0∀k;K ← ∅ ;
4:forc= 1toCdo
5: forp= 1toPdo
6: uc,p(ℓ)←f
HTcℓ+dc,p
,∀ℓ∈Zb
2
7: Uc,p←FastMobius ( uc,p)
8: end for
9:end for
10:S={(c,j,k, v) : Detect ( Uc(j)) =HS(k, v)}
11:while|S|>0do
12: for(c,j,k, v)∈ S withk∈ K do
13: ˆF(k)←v;K ← K ∪ { k}
14: forc′= 1toCdo
15: res←Uc′(Hc′k)−ˆF(k)(1−Dc′k)
16: Uc′(Hc′k)←res
17: end for
18: end for
19: Update S: Re-run Detect ( ·)
20:end while
21:Output: ˆFFirst we perform functional subsampling : For
some b < n we construct uaccording to
u(ℓ) =f(mℓ),ℓ∈Zb
2,mℓ∈Zn
2,(4)
where we have the freedom to choose mℓ.
Critically, the M ¨obius transform of u, de-
noted U, is related to Fvia the well-known
signal processing phenomenon of aliasing :
U(j) =X
k∈A(j)F(k), (5)
whereA(j)corresponds to an aliasing set de-
termined by mℓ. Fig. 3 shows this subsam-
pling procedure on a “sparsified” version of
our sentiment analysis example using differ-
entmℓ. Our goal is to choose mℓsuch that
the non-zero values of F(k)are uniformly
spread across the aliasing sets, since that
makes them easier to recover. If only a single
kwith non-zero F(k)ends up in an aliasing
setA(j), we call it a singleton . In Fig. 3, our
first subsampling generated two singletons, while our second one generated only one. Maximizing
the number of singletons is one of our goals since we can ultimately use those singletons to construct
the M ¨obius transform. In this work, we have determined two different subsampling procedures that
perform well under our two assumptions:
5u1(00) = f(110011)
u1(01) = f(110111)
u1(10) = f(111011)
u1(11) = f(111111)
u2(00) = f(111100)
u2(01) = f(111101)
u2(10) = f(111110)
u2(11) = f(111111)U1(00) = 0
U1(01) = F(k3)
U1(10) = F(k1)
U1(11) = F(k2) +F(k4)
U2(00) = F(k1) +F(k2) +F(k3)
U2(01) = F(k4)
U2(10) = 0
U2(11) = 0
Her
acting
never
fails
to
impress
=
0
0
1
0
0
0
=k1

Her
acting
never
fails
to
impress
=
0
0
0
1
0
0
=k3
Her
acting
never
fails
to
impress
=
0
0
1
1
0
0
=k2

Her
acting
never
fails
to
impress
=
0
0
1
1
0
1
=k4Aliasing 1
Aliasing 2Non-zero Interactions Transform
Zeroton
Singleton
MultitonFigure 3: This figure considers a “sparsified” version of the M ¨obius coefficients depicted in Fig 1,
keeping only the largest 4 depicted. Two different sampling choices are shown, as well as the resulting
aliasing sets. In the first aliasing set, there is one zeroton, two singletons, and one multiton. In the
second aliasing set, there are two zerotons, one singleton, and one multiton.
Lemma 3.1. Choose mℓ=HTℓ, which results in A(j) ={k:Hk=j}.His chosen as follows:
1. Under Assumption 2.1, we choose H= [Ib×b0b,n−b], or any column permutation of this matrix.
2.Under Assumption 2.2 with t= Θ( nα)for some α≤0.409, there exists a matrix Hchosen from
brows of a near constant column weight group testing matrix.
With the given H, non-zero indices are mapped to the 2bsampling sets A(j)independently and
uniformly at random asymptotically, thus maximizing the number of singletons when b= Θ(log( K)).
In both cases the matrix Hcan be viewed as a part of a group testing matrix . This is explicit under
Assumption 2.2, but under Assumption 2.1, Hcan be viewed as part of an individual testing matrix,
where elements are tested one-by-one. These matrices are optimal in an information-theoretic sense
because they achieve an optimal rate asymptotically in their respective setting. We can think of rate
as the amount of information about kwhat we get from the matrix product Hk(see [ 15] for a formal
definition). For example, if the matrix Hkwas always the same for all k, then we would get little
information about kfromHk, and we would say Hhas a low rate . Conversely, if Hkis different
for different kthen the product Hkprovides more information about k. Information theoretically,
maximizing the rate of Hcan be thought of as maximizing the entropy [37] ofHk. It is this
connection between rate and the randomness of the product Hkthat enables us to prove Lemma 3.1.
A detailed discussion of Lemma 3.1 is in Appendix C.2, with an enhanced version for independence
across multiple H, as is required for our overall result. The proof of this lemma touches many areas
of mathematics, including the theory of monoids, information theory, and optimal group testing.
3.2 Singleton Detection and Identification
Singletons are useful, but we cannot immediately use them to recover F(k). We first need to know that
a given U(j)is a singleton . Secondly, we need to identify the value of kthat singleton corresponds
to. Section 4 discusses both tasks. For now, we discuss the rest of the algorithm assuming that we
can accomplish both tasks.
3.3 Message Passing to Resolve Collisions
Recovered Singleton
SubtractSampling Group c = 1 Sampling Group c = 2
Figure 4: Depiction of our peeling message passing
algorithm for the samples in Fig. 3. The singleton
inU2(01) is subtracted (peeled) so we can resolve
F(k2)from U1(11).Since we don’t know the non-zero indices be-
forehand, collisions between multiple non-zero
indices in the same aliasing set are inevitable.
These are called multitons . One approach to deal
with these multitons is to repeat the procedure
over again:
uc(ℓ) =f(mc,ℓ)⇐⇒ Uc(j) =X
k∈Ac(j)F(k),
6c= 1, . . . , C . Each time, we get different aliasing sets Ac(j)resulting in different singletons, and
thus find different kwith non-zero F(k). While this approach works, a better approach is to combine
this idea with message passing to use known non-zero indices and values (k, F(k))to resolve these
multitons and turn them into singletons. The type of message passing algorithm we use is called
graph peeling . The aliasing structure can be represented as a bipartite graph. Each Uc(j)is acheck
node , and each non-zero coefficient F(k)is avariable node . The variable node F(k)is connected to
the check node Uc(j)ifHck=j. Fig. 4 constructs this bipartite graph for the aliasing in Fig. 3. Note
thatU1(11) = F(k2) +F(k4)is a multiton; however, in the other sub-samping group U2(01) =
F(k4)is a singleton. Once we resolve U2(01), we can simply subtract F(k4)fromU1(11), allowing
us to create a new singleton, and extract F(k2). The remaining values of Fboth appear as singletons
in the first sampling group, so we can resolve all 4non-zero interactions Fwith only 8(7unique)
samples. Peeling algorithms were popularized in information and coding theory for decoding fountain
codes [ 38] and have since been applied to variety of applications in communications [ 39] and signal
processing [ 13,40]. They can be analyzed using density evolution theory [ 41], which we also use as
part of our proof.
4 Singleton Detection and Identification
We have discussed how to subsample efficiently to maximize singletons and how to use message
passing to recover as many coefficients as possible. Now we discuss (1) how to identify singletons
and (2) how to determine the k∗corresponding to the singleton. The following result is key:
Lemma 4.1. Consider H∈Zb×n
2, andf:Zn
27→R, and some d∈Zn
2. IfUis the M ¨obius transform
ofu, and Fis the M ¨obius transform of fwe have:
u(ℓ) =f
HTℓ+d
⇐⇒ U(j) =X
k≤ds.t.Hk=jF(k). (6)
The proof can be found in Appendix C.4. The form of (6)allows us to shrink the aliasing set in a
controlled way. Define dc,0:=0n, andDc∈ZP×n
2 for some P >0. The ithrow of Dcis denoted
dc,p,p= 1. . . , P . Using these vectors, we construct C(P+ 1) different subsampled functions uc,p:
uc,p(ℓ) =f
HTcℓ+dc,p
,∀ℓ∈Zb
2. (7)
We compute the M ¨obius transform of each uc,pdenoted by Uc,pand construct a vector-valued function
Uc(j):= [Uc,0(j), . . . , U c,P(j)]T. The goal of singleton detection is to identify when Uc(j)reduces
to a single term, and for what value kthat term corresponds to. To do so, we define the Type ( ·):
1.Type ( Uc(j)) =HZdenotes a zeroton , for which there does not exist F(k)̸= 0s.t.Hk=j.
2.Type ( Uc(j)) =HS(k, F(k))denotes a singleton with only one kwithF(k)̸= 0s.t.Hk=j.
3.Type ( Uc(j)) =HMdenotes a multiton with more than one kwithF(k)̸= 0s.t.Hk=j.
To describe our type estimation rule, we define the following ratios between elements of Uc(j):
yc,p:= 1−Uc,p(j)
Uc,0(j), p= 1, . . . , P, (8)
and construct the vector yc:= [yc,1, . . . , y c,P]T. Then, our estimate for the type is given by
Detect ( Uc(j)):=

HZ, Uc(j) =0
HM, yc/∈ {0,1}P
HS(k, F(k)),yc∈ {0,1}P.(9)
By considering the definition of Ucit is possible to show that if Type ( Uc(j)) =HS(k∗, F(k∗)),
thenyc=Dck∗. When we have a singleton taking Dc=I, and thus P=nsuffices to recover k∗.
So long as the non-zero coefficients, F(k)are not chosen in an adversarial way, this choice of Dc
also ensures that Detect ( Uc(j)) = Type ( Uc(j)). For the purposes of our formal proof, we will
assume that non-zero F(k)are drawn from an absolutely continuous joint distribution. We can’t
do better if we don’t have any extra information about k∗, but we can if we know |k∗| ≤tas we
show below. Going back to our example in Fig. 3, with Dc=Iwe use a total of 8×7 = 56 samples
as opposed to 26= 64 . While this improvement is modest at this scale, for larger problems the
improvement is dramatic.
7Singleton Identification in the Low Degree Setting Let’s say we want to determine the singleton
from U1(10) in Fig. 3, and we know |k∗| ≤1. By exploiting group testing, it is possible to design
Dcwith fewer rows, and thus fewer overall measurements:
Decode
y k1k1=
Dc=y Her acting never fails to impress
0 0 0 1 1 1 0
0 1 1 0 0 1 1
1 0 1 0 1 0 1
Figure 5: We use group testing [ 16] to identify the singleton k1, which corresponds to the first-order
term “never”. By designing the masking patterns with the help of group testing, we can efficiently
recover interactions with O(tlog(n))extra measurements.
The matrix product Dck∗has a different output for each |k| ≤1. In this case, the result ycorresponds
to the binary index of the location of the 1. It requires P= 3, rather than the P= 6forDc=I. If
all non-zero F(k)had satisfied |k| ≤1, we could use this matrix for our example in Fig. 3. However,
we only have |k| ≤3for non-zero F(k)in this example, so Dcas in (5)does not suffice. In the case
of general |k| ≤t, [42] says that for any scaling of twithn, there exists a group testing design Dc
withP=O(tlog(n))that can recover k∗in the limit as n→ ∞ with vanishing error in poly( n)
time. If we also assume that F(k)areDetect ( Uc(j))has vanishing error (see Appendix C.7.2).
Extension to Noisy Setting We now relax the assumption that most of the coefficients are exactly
zero. To do this, we assume each subsampled M ¨obius coefficient is corrupted by noise:
Uc,p(j) =X
k≤dps.t.Hck=jF(k) +Zc,p(j), (10)
where Zc,p(j)i.i.d.∼ N (0, σ2). There are two main changes that must be made compared to the
noiseless case. First, we must place an assumption on the magnitude of non-zero coefficients |F(ki)|,
such that the signal-to-noise ratio (SNR) remains fixed. Secondly, the matrix Dcmust be modified.
It now consists of two parts: Dc= [D1
c;D2
c]. We design D2
c∈ZP2×n
2 as a standard noise robust
Bernoulli group testing matrix with P2=O(tlog(n))tests, which suffices for singleton identification
under any fixed SNR [ 43]. However, unlike the noiseless case, the samples from the rows of D2
c
are not enough to ensure a vanishing error for singleton detection in the Detect ( ·)procedure. To
solve this, we design D1
c∈ZP1×n
2 as a Bernoulli group testing matrix with a different parameter. In
Appendix C.7.4, we show this modified version of Detect ( ·)has vanishing error if P1=O(tlog(n)).
5 Results
Now that we have discussed all components of the algorithm, we present our theoretical guarantees:
Theorem 5.1. (Recovery with KUniform Interactions) Let fsatisfy Assumption 2.1 for some K=
O(2nδ)withδ≤1
3and let the non-zero coefficients of Fbe drawn from an absolutely continuous
distribution. For {Hc}C
c=1chosen as in Lemma C.3 with b=O(log(K)),C= 3 andDc=I,
Algorithm 1 exactly computes the transform FinO(Kn)samples and O(Kn2)time complexity with
probability at least 1−O(1/K).
Theorem 5.2. (Noise-Robust Recovery with K t-Degree Interactions) Let fsatisfy Assumption 2.2
forK=O(poly( n))andt= Θ( nα)withα≤0.409. Assume either:
1. The non-zero coefficients of Fare drawn from an arbitrary continuous distribution, or
2.Uc,pis corrupted by noise as in (10) and let non-zero coefficients satisfy |F(k)|=ρ.
Then, for {Hc}C
c=1chosen as in Lemma C.4 with b=O(log(K)),C= 3, and Dcchosen as a
suitable group testing matrix, Algorithm 1 exactly computes the transform FinO(Ktlog(n))samples
andO(Kpoly( n))time complexity with probability at least 1−O(1/K)in both the noiseless case
(1) and noisy case (2).
821002200230024002500260027002800290021000100 200 300 400 500 600 700 800 9001000 n
N104105106Sample Complexity
0%20%40%60%80%100%Perfect Reconstruction %(a)
0 5 10 15 20 25
SNR (dB)0.00.20.40.60.81.0R2(Faithfulness)
t= 6
t= 8
t= 10
t= 12 (b)
282102122142162182202222242262282308 10 12 14 16 18 20 22 24 26 28 30 n
N012345678910Runtime Complexity (sec)
SHAP IQ
LASSO
SMT (c)
Figure 6: (a) Perfect reconstruction against nand sample complexity under Assumption 2.1. Holding
C= 3, we scale bto increase the sample complexity. We observe that the number of samples required
to achieve perfect reconstruction is scaling linearly in nas predicted. Results are plotted across 5
runs for each choice of bandn. (b) Plot of the noise-robust version of our algorithm. For various
values of t, we set n= 500 andK= 500 , using a group testing matrix with P= 1000 . We plot
the performance of our algorithm against SNR, measured in terms of the R2. Error bands represent
the standard deviation over 10runs. (c) Runtime comparison of SMT, SHAP-IQ [ 29], and t= 5
order FSI via LASSO [ 4]. All are computing the M ¨obius transform in the setting where all non-zero
interactions are order t,K= 10 . SMT easily outperforms both, while the other methods quickly
become intractable. Error bands represent standard deviation over 10runs.
0 612 18 24 30 36 42 48 54
Sparsity0.00.20.40.60.81.0R2(Faithfulness)XGBoost for Breast Cancer Diagnosis
(n= 30)
0 5 10 15 20 25 30 35 40
Sparsity0.00.20.40.60.81.0R2(Faithfulness)BERT for Sentiment Analysis
(n≈25)
0 3 6 9 12 15 18 21
Sparsity0.00.20.40.60.81.0R2(Faithfulness)BERT for Multiple Choice
(n= 15)
Shapley Values Banzhaf Values Faith-Banzhaf Indices SMT (Algorithm 1)
Figure 7: Since our ultimate goal is compact, meaningful and computable representations, we
compare representations generated from SMT (Algorithm 1) with other popular explanation models.
We plot R2(faithfulness) vs. the number of terms used in the representation (sparsity). For Shapley
and Banzhaf values, to generate an r-sparse representation, we use the top rmagnitude values. For
SMT and Faith-Banzhaf, we do a slightly more sophisticated refinement procedure. Faith-Banzhaf is
included because it is the first-order metric that maximizes R2. As observed in the breast cancer and
sentiment analysis tasks, SMT can achieve better R2than other approaches by utilizing higher-order
interactions. In the sentence-level multiple choice dataset, we observe less of a difference, since in
those cases the entire answer to a question is usually contained in a single sentence, thus higher-order
interactions provide little advantage. Error bands represent the standard deviation over 10instances.
The proof of Theorem 5.1 and 5.2 is provided in Appendix C.5. It combines results from all of
the parts of the algorithm we have discussed: aliasing, singleton detection and graph peeling. The
requirement |F(k)|=ρis only due to limitations of group testing theory. In practice, we observe
that a lower bound on the magnitude suffices. In addition to our theoretical results, we also conduct
numerical experiments on synthetic and real word models, which are discussed below.
5.1 Synthetic Simulations
We tested SMT’s efficacy on functions satisfying Assumption 2.1 and 2.2, setting non-zero F(k)
uniformly in [−1,1]. SMT is implemented as in Algorithm 1, with group testing decoding via linear
programming (see Appendix F.2). Fig. 6a is a phase transition plot that shows the percent of cases
where SMT achieves R2= 1with fixed K= 100 at different sample complexities and values of n.
Wevastly outperform the naive approach: when n= 1000 , we get perfect reconstruction with only
10−294percent of total samples! Furthermore, the number of samples necessary to achieve perfect
reconstruction scales linearly in nas predicted. Fig. 6b assesses SMT under noise for various values
oft, plotting R2against SNR with K= 500 ,n= 500 , and P= 1000 . Fig. 6c plots the runtime
9for SMT and competing methods. Test functions fhaveK= 10 non-zero M ¨obius coefficients at
locations that satisfy |k|= 5(restricted to equality due to limitations in the SHAP-IQ code at the
time of running). We compare against SHAP-IQ [ 29] configured to compute 5thorder FSI, as well
as the method of [ 4] which computes 5thorder FSI via LASSO. As shown in Appendix A, the tth
order FSI are exactly the tthorder M ¨obius coefficients for our chosen f. This figure exemplifies that
SMT is the sole feasible method for identifying interactions on the scale of n≥100. Additional
simulations and discussion can be found in Appendix E.
5.2 Real-World Models
Our objective is a computable, faithful, and compact representation of real-world machine-learned
functions. Fig. 7 addresses this goal head-on, by plotting R2against the number of terms used in
the representation (sparsity) for SMT and other popular model explanation approaches. We consider
three different tasks: The first is an XGboost model for breast cancer diagnosis, and the other two are
transformer-based BERT models for the tasks of sentiment analysis and multiple choice question
answering respectively. Appendix B discusses the setup in great detail. For Shapley and Banzhaf
values, to generate an r-sparse representation, we use the top rmagnitude values. For SMT and Faith-
Banzhaf, we do a slightly more sophisticated refinement procedure using LASSO [ 44], described in
the Appendix. We observe that for the breast cancer and sentiment analysis tasks, SMT can generate
representations that, with the same number of terms, achieve a much higher R2. This is done by
identifying interactions between inputs that are important to the model output. Interestingly, in the
case of the multiple choice model, there is less of a difference between the Faith-Banzhaf Indices
and the SMT representations. This is likely because in the corresponding dataset, answers to the
questions are usually contained in single sentences, making interactions less important.
6 Conclusion
Identifying interactions between inputs is an important open research question in machine learning,
with applications to explainability, data valuation, and many other problems. We approached this
problem by studying the M ¨obius transform, which is a representation over the fundamental interaction
basis. We introduced several new tools to the problem of identifying interactions. The use of ideas
from sparse signal processing and group testing has allowed SMT to operate in regimes where all
other methods fail due to computational burden. Our theoretical results guarantee asymptotic exact
reconstruction and are complemented by numerical simulations that show SMT performs well with
finite parameters and also under a noisy model.
Limitations Our assumption of independently sampled interactions was made for information-
theoretic hardness and may not hold in some settings where correlated interactions exist. For instance,
in the sentiment problem in Fig. 1, words with strong 2ndorder interactions are likely to appear
together in important 3rdorder interactions. In such settings, correlation is exploitable, so a more
specific algorithm can likely exploit this correlation and eliminate this assumption. Another limitation
is that we have focused on taking a sparse M ¨obius transform in this work. In practice, we may be
more interested in taking a sparse projection onto a subset of low-order terms. For unitary transforms,
projection can be achieved by truncation, however, with non-orthogonal transforms like the M ¨obius
Transform, projection is not straightforward. This is an important distinction, because there can be
functions which are well-approximated by a sparse low degree M ¨obius projection, but do not have a
sparse transform.
Future Work Applying SMT to real-world tasks like understanding protein language models [ 45],
LLM chatbots [ 46] or diffusion models [ 47] would be insightful. Working with large and complicated
models will likely require further improvements to robustness—both in terms of dealing with noise
from small but non-zero interactions and dealing with potential correlations between interactions.
Some interesting ideas in this direction could be using more standard statistical ideas like in [ 29] or
considering concepts from adaptive group testing. Finally, due to the connection with the Shapley or
Banzhaf values (2), methods for computing the M ¨obius transform can also be used for computing
these values by first computing the M ¨obius transform and then using (2).
10References
[1] L. S. Shapley, A Value for N-Person Games . Santa Monica, CA: RAND Corporation, 1952.
[2]S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model predictions,” in
Proceedings of the 31st International Conference on Neural Information Processing Systems ,
ser. NIPS’17. Red Hook, NY , USA: Curran Associates Inc., 2017, p. 4768–4777.
[3]M. Sundararajan, K. Dhamdhere, and A. Agarwal, “The Shapley Taylor interaction index,” in
International Conference on Machine Learning , Jul 2020, pp. 9259–9268. [Online]. Available:
https://proceedings.mlr.press/v119/sundararajan20a.html
[4]C.-P. Tsai, C.-K. Yeh, and P. Ravikumar, “Faith-shap: The faithful Shapley interaction index,”
Journal of Machine Learning Research , vol. 24, no. 94, pp. 1–42, 2023.
[5]M. Grabisch, J.-L. Marichal, and M. Roubens, “Equivalent Representations of Set Functions,”
Mathematics of Operations Research , vol. 25, no. 2, pp. 157–178, May 2000. [Online].
Available: https://pubsonline.informs.org/doi/10.1287/moor.25.2.157.12225
[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional
transformers for language understanding,” in North American Chapter of the Association for
Computational Linguistics , 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID:
52967399
[7]J. Lee, “IMDB Finetuned BERT-base-uncased,” 2023, accessed Jan 2024. [Online]. Available:
https://huggingface.co/JiaqiLee/imdb-finetuned-bert-base-uncased
[8]J. M. P ´erez, J. C. Giudici, and F. Luque, “pysentimiento: A python toolkit for sentiment analysis
and SocialNLP tasks,” 2021.
[9]T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R ´e, “Flashattention: Fast and memory-efficient exact
attention with IO-awareness,” Advances in Neural Information Processing Systems , vol. 35, pp.
16 344–16 359, 2022.
[10] H. Hassanieh, P. Indyk, D. Katabi, and E. Price, “Simple and practical algorithm for sparse
Fourier transform,” in SIAM Symposium on Discrete Algorithms (SODA) , 2012, pp. 1183–1194.
[Online]. Available: https://epubs.siam.org/doi/abs/10.1137/1.9781611973099.93
[11] P. Stobbe and A. Krause, “Learning Fourier sparse set functions,” in International Conference
on Artificial Intelligence and Statistics (AISTATS) , ser. Proceedings of Machine Learning
Research, La Palma, Canary Islands, Apr 2012, pp. 1125–1133. [Online]. Available:
https://proceedings.mlr.press/v22/stobbe12.html
[12] S. Pawar and K. Ramchandran, “Computing a k-sparse n-length Discrete Fourier Transform
using at most 4ksamples and O(klogk )complexity,” in IEEE International Symposium on
Information Theory (ISIT) , 2013, pp. 464–468.
[13] X. Li, J. K. Bradley, S. Pawar, and K. Ramchandran, “The SPRIGHT algorithm for robust
sparse Hadamard Transforms,” in IEEE International Symposium on Information Theory (ISIT) ,
2014, pp. 1857–1861.
[14] A. Amrollahi, A. Zandieh, M. Kapralov, and A. Krause, “Efficiently learning fourier sparse set
functions,” Advances in Neural Information Processing Systems , vol. 32, 2019.
[15] M. Aldridge, O. Johnson, and J. Scarlett, “Group testing: An information theory perspective,”
Foundations and Trends ®in Communications and Information Theory , vol. 15, no. 3–4, p.
196–392, 2019. [Online]. Available: http://dx.doi.org/10.1561/0100000099
[16] R. Dorfman, “The detection of defective members of large populations,” The Annals of mathe-
matical statistics , vol. 14, no. 4, pp. 436–440, 1943.
[17] Y . Zhou, U. Porwal, C. Zhang, H. Q. Ngo, X. Nguyen, C. R ´e, and V . Govindaraju, “Parallel
feature selection inspired by group testing,” in Advances in Neural Information Processing
Systems , vol. 27, 2014.
[18] R. Jia, D. Dao, B. Wang, F. A. Hubis, N. Hynes, N. M. G ¨urel, B. Li, C. Zhang, D. Song, and
C. J. Spanos, “Towards efficient data valuation based on the shapley value,” in International
Conference on Artificial Intelligence and Statistics (AISTATS) , vol. 89, 16–18 Apr 2019, pp.
1167–1176. [Online]. Available: https://proceedings.mlr.press/v89/jia19a.html
[19] J. C. Harsanyi, “A bargaining model for the cooperative n-person game,” Ph.D. dissertation,
Department of Economics, Stanford University, Stanford, CA, USA, 1958.
11[20] C. Wendler, A. Amrollahi, B. Seifert, A. Krause, and M. P ¨uschel, “Learning set functions that
are sparse in non-orthogonal Fourier bases,” in AAAI Conference on Artificial Intelligence ,
vol. 35, 2021, pp. 10 283–10 292.
[21] Q. Ren, J. Gao, W. Shen, and Q. Zhang, “Where we have arrived in proving the
emergence of sparse interaction primitives in DNNs,” in International Conference on Learning
Representations , 2024. [Online]. Available: https://openreview.net/forum?id=3pWSL8My6B
[22] K. Aas, M. Jullum, and A. Løland, “Explaining individual predictions when features are
dependent: More accurate approximations to Shapley values,” Artificial Intelligence , vol. 298,
p. 103502, 2021.
[23] H. Chen, J. D. Janizek, S. Lundberg, and S.-I. Lee, “True to the model or true to the data?”
2020. [Online]. Available: https://arxiv.org/pdf/2006.16234
[24] D. Janzing, L. Minorics, and P. Bl ¨obaum, “Feature relevance quantification in explainable AI:
A causal problem,” in International Conference on Artificial Intelligence and Statistics , 2020,
pp. 2907–2916.
[25] J. Ren, Z. Zhou, Q. Chen, and Q. Zhang, “Can we faithfully represent absence states to compute
shapley values on a DNN?” in International Conference on Learning Representations , 2023.
[Online]. Available: https://openreview.net/forum?id=YV8tP7bW6Kt
[26] J. Enouen, H. Nakhost, S. Ebrahimi, S. O. Arik, Y . Liu, and T. Pfister, “TextGenSHAP: Scalable
post-hoc explanations in text generation with long documents,” 2023. [Online]. Available:
https://arxiv.org/pdf/2312.01279
[27] V . Miglani, A. Yang, A. Markosyan, D. Garcia-Olano, and N. Kokhlikyan, “Using Captum to
explain generative language models,” in Workshop for Natural Language Processing Open
Source Software (NLP-OSS 2023) , Singapore, Dec. 2023, pp. 165–173. [Online]. Available:
https://aclanthology.org/2023.nlposs-1.19
[28] L. M. Paes, D. Wei, H. J. Do, H. Strobelt, R. Luss, A. Dhurandhar, M. Nagireddy, K. N.
Ramamurthy, P. Sattigeri, W. Geyer et al. , “Multi-level explanations for generative language
models,” 2024. [Online]. Available: https://arxiv.org/pdf/2403.14459
[29] F. Fumagalli, M. Muschalik, P. Kolpaczki, E. H ¨ullermeier, and B. E. Hammer, “SHAP-IQ:
Unified approximation of any-order shapley interactions,” in Conference on Neural Information
Processing Systems , 2023. [Online]. Available: https://openreview.net/forum?id=IEMLNF4gK4
[30] J. S. Kang, R. Pedarsani, and K. Ramchandran, “The fair value of data under heterogeneous
privacy constraints in federated learning,” Transactions on Machine Learning Research , 2024.
[Online]. Available: https://openreview.net/forum?id=ynG5Ak7n7Q
[31] J. T. Wang and R. Jia, “Data Banzhaf: A robust data valuation framework for machine learning,”
inInternational Conference on Artificial Intelligence and Statistics , vol. 206, 25–27 Apr 2023,
pp. 6388–6421. [Online]. Available: https://proceedings.mlr.press/v206/wang23e.html
[32] A. Ghorbani and J. Zou, “Data Shapley: Equitable valuation of data for machine learning,”
inInternational Conference on Machine Learning , vol. 97, 09–15 Jun 2019, pp. 2242–2251.
[Online]. Available: https://proceedings.mlr.press/v97/ghorbani19c.html
[33] A. Ghorbani, M. Kim, and J. Zou, “A distributional framework for data valuation,” in
International Conference on Machine Learning , vol. 119, 13–18 Jul 2020, pp. 3535–3544.
[Online]. Available: https://proceedings.mlr.press/v119/ghorbani20a.html
[34] T. Chen and C. Guestrin, “XGBoost: A scalable tree boosting system,” in ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining , San Francisco, 2016, pp.
785–794. [Online]. Available: http://doi.acm.org/10.1145/2939672.2939785
[35] A. Barbosa, “Roberta large fine tuned on race,” 2021. [Online]. Available: https://huggingface.
co/LIAMF-USP/roberta-large-finetuned-race/tree/main
[36] Q. Ren, Y . Xu, J. Zhang, Y . Xin, D. Liu, and Q. Zhang, “Towards the dynamics of a DNN
learning symbolic interactions,” 2024. [Online]. Available: https://arxiv.org/pdf/2407.19198
[37] T. M. Cover, Elements of Information Theory . John Wiley & Sons, 1999.
[38] M. Luby, “LT codes,” in IEEE Symposium on Foundations of Computer Science , 2002, pp. 271–
280.
12[39] K. R. Narayanan and H. D. Pfister, “Iterative collision resolution for slotted aloha: An optimal
uncoordinated transmission policy,” in International Symposium on Turbo Codes and Iterative
Information Processing (ISTC) , 2012, pp. 136–139.
[40] Y . E. Erginbas, J. Kang, A. Aghazadeh, and K. Ramchandran, “Efficiently computing sparse
fourier transforms of q-ary functions,” in IEEE International Symposium on Information Theory
(ISIT) , 2023, pp. 513–518.
[41] S.-Y . Chung, T. Richardson, and R. Urbanke, “Analysis of sum-product decoding of low-density
parity-check codes using a gaussian approximation,” IEEE Transactions on Information Theory ,
vol. 47, no. 2, pp. 657–670, 2001.
[42] W. H. Bay, J. Scarlett, and E. Price, “Optimal non-adaptive probabilistic group testing in
general sparsity regimes,” Information and Inference: A Journal of the IMA , vol. 11, no. 3, pp.
1037–1053, 02 2022. [Online]. Available: https://doi.org/10.1093/imaiai/iaab020
[43] J. Scarlett and O. Johnson, “Noisy non-adaptive group testing: A (near-)definite defectives
approach,” IEEE Transactions on Information Theory , vol. 66, no. 6, pp. 3775–3797, 2020.
[44] R. Tibshirani, “Regression shrinkage and selection via the lasso,” Journal of the Royal Statistical
Society Series B: Statistical Methodology , vol. 58, no. 1, pp. 267–288, 1996.
[45] Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y . Shmueli,
A. dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido, and A. Rives, “Evolutionary-scale
prediction of atomic-level protein structure with a language model,” Science , vol. 379, no. 6637,
pp. 1123–1130, 2023. [Online]. Available: https://www.science.org/doi/abs/10.1126/science.
ade2574
[46] OpenAI, “GPT-4 technical report,” 2023.
[47] D. Kingma, T. Salimans, B. Poole, and J. Ho, “Variational diffusion models,” in Advances in
Neural Information Processing Systems , vol. 34, 2021, pp. 21 696–21 707.
[48] P. L. Hammer and R. Holzman, “Approximations of pseudo-boolean functions; applications to
game theory,” Zeitschrift f ¨ur Operations Research , vol. 36, no. 1, pp. 3–21, 1992. [Online].
Available: https://doi.org/10.1007/BF01541028
[49] W. Wolberg, O. Mangasarian, N. Street, and W. Street, “Breast Cancer Wisconsin (Diagnostic),”
UCI Machine Learning Repository, 1995, DOI: https://doi.org/10.24432/C5DW2B.
[50] D. Q. Nguyen, T. Vu, and A. T. Nguyen, “BERTweet: A pre-trained language model for
English Tweets,” in Conference on Empirical Methods in Natural Language Processing: System
Demonstrations , 2020, pp. 9–14.
[51] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts, “Learning word vectors
for sentiment analysis,” in Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies . Portland, Oregon, USA: Association for Computational
Linguistics, June 2011, pp. 142–150. [Online]. Available: http://www.aclweb.org/anthology/
P11-1015
[52] G. Lai, Q. Xie, H. Liu, Y . Yang, and E. Hovy, “RACE: Large-scale ReAding comprehension
dataset from examinations,” in Conference on Empirical Methods in Natural Language
Processing , Copenhagen, Denmark, Sep. 2017, pp. 785–794. [Online]. Available: https:
//aclanthology.org/D17-1082
[53] A. Coja-Oghlan, O. Gebhard, M. Hahn-Klimroth, and P. Loick, “Information-theoretic and
algorithmic thresholds for group testing,” IEEE Transactions on Information Theory , vol. 66,
no. 12, pp. 7911–7928, 2020.
[54] C. L. Chan, S. Jaggi, V . Saligrama, and S. Agnihotri, “Non-adaptive group testing: Explicit
bounds and novel algorithms,” IEEE Transactions on Information Theory , vol. 60, no. 5, pp.
3019–3035, 2014.
13A Relationship between M ¨obius Transform and Other Importance Metrics
We begin with some notation. We define the M ¨obius basis function (which are all possible products
of inputs) as:
bk(m):=Y
i:ki=1mi. (11)
Now we define the following sub-spaces of pseudo-Boolean function in terms of the linear span of
M¨obius basis functions:
Mt:= span {bk(m) :|k| ≤t}. (12)
Now we define the projection operator Projµ(f,D), as the projection of the function fonto the
function space Dwith respect to the measure µ. Ifg(m) = Projµ(f,D), we write its decomposition
asg(m) =P
k∈Zn
2c(f,D, µ,k)bk(m). Note that linear independence implies the uniqueness of this
representation.
Shapley Value The Shapley values SV(i)[1] of the inputs mi, i= 1, . . . , n with respect to the
function fare [48]:
SV(i) =c(f,M1, σ,ei) =X
k:ki=11
|k|F(k), (13)
where σis the Shapley kernel. SV(i) =F(ei)when fis a linear function.
Banzhaf Index The Banzhaf index BZ(i)of the inputs mi, i= 1, . . . , n with respect to the function
fare [48]:
BZ(i) =c(f,M1, µ,ei) =X
k:ki=11
2|k|−1F(k), (14)
where µis the uniform measure. BZ(i) =F(ei)when fis a linear function.
Faith Shapley Interaction Index Thetthorder Faith Shapley interaction index SVt(k)for|k| ≤t
[4] is
SVt(k) =c(f,Mt, σ,k) =F(k) + (−1)t−|k||k|
t+|k|t
|k|X
p>k
|p|>tF(p), (15)
where σis the Shapley kernel. SVt(k) =F(k)when fis atthorder function, i.e., F(k) = 0 when
|k|> t.
Faith Banzhaf Interaction Index Thetthorder Faith Shapley interaction index BZt(k)for|k| ≤t
[4] is
BZt(k) =c(f,Mt, µ,k) =F(k) + (−1)t−|k|X
p>k
|p|>t1
2|p|−|k||p| − |k| −1
t− |k|
F(p), (16)
where µis the uniform measure. BZt(i) =F(k)when fis atthorder function, i.e., F(k) = 0 when
|k|> t.
Shapley-Taylor Interaction Index Thetthorder Shapley-Taylor Interaction Index [ 3]STII t(k)is:
STII t(k) =F(k) |k|< t
c(f−ft−1,Mt− M t−1, σ,k)|k|=t,, ft−1(m) =X
k≤m
|k|<tF(k), (17)
where σis the Shapley kernel. Explicitly, it can be shown that:
c(f−ft−1,Mt− M t−1, σ,k) =X
k≤k′|k′|
t−1
F(k′)for|k|=t. (18)
As a consequence of the above, we have STII t(k) = SV t(k) =F(k)when fis atthorder function,
i.e.,F(k) = 0 for|k|> t.
14B Experiment Details
Letfbe the real-world function we wish to explain. In subsections B.1, B.2, and B.3, we describe
how we formed these functions for the tasks of breast cancer diagnosis, sentiment analysis, and
multiple choice answering respectively. For our experiments, we plot the R2(faithfulness) for a
variety of explanation models ˆf, measured through:
R2= 1−ˆf−f2
2f−f2
2.
where we use the notation ∥f∥2
2=P
m∈Zn
2f(m)2.
In Figure 2, we consider settings where n≈20, such that we can run optimization procedures to find
faithful approximations that are sparse andlow degree .
Achievable Low Degree: To find the best approximation ˆfof up to degree t, we solve the following
quadratic programming problem:
min
ˆf,αˆf−f2
2(19)
s.t. ˆf(m) =X
k≤m,|k|≤tαk,∀m. (20)
Achievable Sparsity: On the other hand, we cannot efficiently find the optimal faithful K-sparse ap-
proximation due to the problem’s combinatorial nature. Instead, informed by the strong faithfulness of
low degree approximations, we employ the following heuristic to obtain some sparse approximation.
LetSK⊆Zn
2be a set containing the first Kcoordinates with the lowest degree, where ties are
randomly broken. With this set, we solve the following quadratic programming problem:
min
ˆf,αˆf−f2
2(21)
s.t. ˆf(m) =X
k≤m,k∈SKαk,∀m. (22)
In Figure 7, we consider the four explanation models described below.
Shapley Values: We approximate Shapley values by iterating through permutations of the inputs [ 2].
For an efficient implementation of the algorithm, we use the SHAP Python package [ 2]. To measure
the faithfulness captured by Shapley values at some sparsity level r, we consider approximations that
only include the top- rShapley values by magnitude.
Banzhaf Values: We approximate Banzhaf values using the Maximum Sample Reuse Monte Carlo
procedure described in [ 31]. To measure the faithfulness captured by Banzhaf values at some sparsity
levelr, we consider approximations that only include the top- rBanzhaf values by magnitude.
Faith-Banzhaf Indices: We calculate Faith-Banzhaf indices using the regression formulation de-
scribed in [ 4]. To measure the faithfulness captured by sparse approximations of Faith-Banzhaf in-
dices, we modify the regression problem by adding an ℓ1penalty on the values of the Faith-Banzhaf
indices. We vary the penalty coefficient to obtain different levels of sparsity.
SMT: We run SMT (Algorithm 1) to obtain a sparse M ¨obius representation ˆFwith support supp( ˆF).
Then, we fine-tune the values of the coefficients by solving the following regression problem over a
uniformly sampled set of points D ⊆Zn
2:
min
ˆf,αX
m∈D(ˆf(m)−f(m))2
s.t. ˆf(m) =X
k≤m,k∈supp( ˆF)αk,∀m.
15To measure the faithfulness captured by sparse approximations, we modify the regression problem by
adding an ℓ1penalty on the values of the M ¨obius coefficients. Then, we vary the penalty coefficient
λto obtain different levels of sparsity:
min
ˆf,αX
m∈D(ˆf(m)−f(m))2+λX
k∈supp( ˆF)|αk|
s.t. ˆf(m) =X
k≤m,k∈supp( ˆF)αk,∀m.
B.1 XGBoost for Breast Cancer Diagnosis
We train an XGBoost model for classification using the Wisconsin Breast Cancer dataset [ 49]. This
dataset contains the mean, standard deviation, and largest value of ten measurements, resulting in
thirty features. For Figure 2, we use only the mean and standard deviation, resulting in twenty
features. For Figure 2, we use the first ten data points in the training set and for Figure 7, we present
the aggregated results over the first twenty.
To explain the XGBoost model h(the probability associated with a positive classification) on each
data point x∈ X , we use an interventional expected value formulation: we freeze some of the
features and take an expectation over all data points by infilling the remaining features. Formally,
f(m) =E[h(X)|do(Xm=xm)]
where we use the notation xm={xi:mi= 1}.
B.2 BERT for Sentiment Analysis
We employ the sentiment analysis model from [ 8], which is built upon BERTweet [ 50], a RoBERTa
model trained on English tweets. We take movie reviews from the IMDb Movie Reviews dataset
[51]. For a particular review, we define its function as a mapping from maskings of words (using the
[UNK] token) to the model’s logit value associated to the correct sentiment classification.
For Figure 2, we use the first ten sentences in the dataset with 17, 18, or 19 words, where words
separated through spaces in the review. Below, we include the reviews and their low degree and
sparse approximations calculated with equations 20 and 22 respectively.
n(words) R EVIEW SENTIMENT
(a) 18 A rating of “1” does not begin to express how dull, depressing and relentlessly bad this movie is. Negative
(b) 18 Hated it with all my being. Worst movie ever. Mentally- scarred. Help me. It was that bad.TRUST ME!!! Negative
(c) 19 This is a good film. This is very funny. Yet after this film there were no good Ernest films! Positive
(d) 19 The characters are unlikeable and the script is awful. It’s a waste of the talents of Deneuve and Auteuil. Negative
(e) 18 I don’t know why I like this movie so well, but I never get tired of watching it. Positive
(f) 19 If you like Pauly Shore, you’ll love Son in Law. If you hate Pauly Shore, then, well...I liked it! Positive
(g) 17 This is the definitive movie version of Hamlet. Branagh cuts nothing, but there are no wasted moments. Positive
(h) 19 Without a doubt, one of Tobe Hoppor’s best! Epic storytellng, great special effects, and The Spacegirl (vamp me baby!). Positive
(i) 17 Add this little gem to your list of holiday regulars. It is sweet, funny, and endearing Positive
(j) 17 no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT! Negative
0 3 6 9 12 15 18
Degree0.00.20.40.60.81.0R2(Faithfulness)
(a)
0 3 6 9 12 15 18
Degree0.00.20.40.60.81.0R2(Faithfulness)
 (b)
0 3 6 9 12 15 18
Degree0.00.20.40.60.81.0R2(Faithfulness)
 (c)
0 3 6 9 12 15 18
Degree0.00.20.40.60.81.0R2(Faithfulness)
 (d)
In Figure 7, we take a random sampling of reviews, with number of words spanning from 17 to 38.
The reviews we used, alongside their word counts and sentiment, are included below:
B.3 BERT for Multiple Choice
For multiple choice answering, we use a RoBERTa model [ 35] fine-tuned on RACE [ 52]: a large-
scale reading comprehension dataset. This dataset contains over 28,000 passages, each containing
160 3 6 9 12 15 18
Degree0.00.20.40.60.81.0R2(Faithfulness)
(e)
0 3 6 9 12 15 18
Degree0.00.20.40.60.81.0R2(Faithfulness)
 (f)
0 3 6 9 12 15 18
Degree0.00.20.40.60.81.0R2(Faithfulness)
 (g)
0 3 6 9 12 15 18
Degree0.00.20.40.60.81.0R2(Faithfulness)
 (h)
 
0 3 6 9 12 15 18
Degree0.00.20.40.60.81.0R2(Faithfulness)
(i)
0 3 6 9 12 15 18
Degree0.00.20.40.60.81.0R2(Faithfulness)
 (j)
 
100101102103104105
Sparsity0.00.20.40.60.81.0R2(Faithfulness)
(a)
100101102103104105
Sparsity0.00.20.40.60.81.0R2(Faithfulness)
 (b)
100101102103104105
Sparsity0.00.20.40.60.81.0R2(Faithfulness)
 (c)
100101102103104105
Sparsity0.00.20.40.60.81.0R2(Faithfulness)
 (d)
100101102103104105
Sparsity0.00.20.40.60.81.0R2(Faithfulness)
(e)
100101102103104105
Sparsity0.00.20.40.60.81.0R2(Faithfulness)
 (f)
100101102103104105
Sparsity0.00.20.40.60.81.0R2(Faithfulness)
 (g)
100101102103104105
Sparsity0.00.20.40.60.81.0R2(Faithfulness)
 (h)
 
100101102103104105
Sparsity0.00.20.40.60.81.0R2(Faithfulness)
(i)
100101102103104105
Sparsity0.00.20.40.60.81.0R2(Faithfulness)
 (j)
 
corresponding multiple-choice questions. For our experiments, we found the first ten passages with
15 sentences, and took their first multiple-choice question.
To construct the function, we consider sentence-level maskings of the passages using the [PAD] token.
We pass the masked passage, alongside the multiple choice question into the RoBERTa model, and
measure the logit value of the question’s correct answer.
17n(words) R EVIEW SENTIMENT
17 This is the definitive movie version of Hamlet. Branagh cuts nothing, but there are no wasted moments. Positive
18 I don’t know why I like this movie so well, but I never get tired of watching it. Positive
23Brilliant movie. The drawings were just amazing. Too bad it ended before it begun. I ´ve waited 21
years for a sequel, but nooooo!!!Positive
26Malcolm McDowell has not had too many good movies lately and this is no different. Especially
designed for people who like Yellow filters on their movies.Negative
24Excellent episode movie ala Pulp Fiction. 7 days - 7 suicides. It doesnt get more depressing than this.
Movie rating: 8/10 Music rating: 10/10Positive
26You’ve got to be kidding. This movie sucked for the sci-fi fans. I would only recommend watching
this only if you think Armageddon was good.Negative
27Despite its interesting premise, Sniper is quite tedious. With a tighter script and sharper directing it
could have been electrifying; instead it plods along with little tension.Negative
29You may like Tim Burton’s fantasies, but not in a commercial-like show off lasting 8 minutes. It
demonstrates good technical points without real creativity or some established narrative pace.Negative
27Brilliant execution in displaying once and for all, this time in the venue of politics, of how ”good
intentions do actually pave the road to hell”. Excellent!Positive
28I can’t believe they got the actors and actresses of that caliber to do this movie. That’s all I’ve got to
say - the movie speaks for itself!!Positive
33Something does not work in this movie. There are absolutely no energies between the actors. In fact,
their very acting seems frozen, sometimes amateur. Also, the script is not convincing and not reliable.Negative
24Great story, great music. A heartwarming love story that’s beautiful to watch and delightful to listen
to. Too bad there is no soundtrack CD.Positive
38A very carelessly written film. Poor character and idea development. The silly plot and weak acting
by just about the ensemble cast didn’t help. Seriously, watching this movie will NOT make you smile.
It may make you retch.Negative
19 This is a good film. This is very funny. Yet after this film there were no good Ernest films! Positive
18 The characters are unlikeable and the script is awful. It’s a waste of the talents of Deneuve and Auteuil. Negative
C Missing Proofs
C.1 Boolean Arithmetic
Table 1 the addition and multiplication table for arithmetic between x, y∈Z2. We also note that
Z2is typically used to refer to the integer ring modulo 2. The arithmetic we are describing here is
actually that of a monoid . Since the audience for this paper is people interested in machine learning,
we continue to use Z2since it is commonly used to simply refer to the set {0,1}.
Addition Table
+ x= 1 x= 0
y= 1 1 1
y= 0 1 0Multiplication Table
× x= 1 x= 0
y= 1 1 0
y= 0 0 0Subtraction Table
− x= 1 x= 0
y= 1 0 1
y= 0 N/A 0
Table 1: Addition, Multiplication and Subtraction table for Boolean arithmetic in this paper. Subtrac-
tion is for y−x.
C.2 Discussion of Aliasing of the M ¨obius Transform
When a function has many small or zero Mobius coefficients (interactions), our goal is to subsample
(4)in such a way that the aliasing causes the non-zero coefficients to end up in different aliasing
sets(5)(as opposed to all of them being aliased together, making them more difficult to reconstruct).
Lemma C.1 is a key tool that we will use in this work to design subsampling patterns that result in
good aliasing patterns.
Lemma C.1. Consider H∈Zb×n
2,b < n andf:Zn
27→R. Let
u(ℓ) =f
HTℓ
,∀ℓ∈Zb
2. (23)
IfUis the Mobius transform of u, and Fis the Mobius transform of fwe have:
U(j) =X
Hk=jF(k). (24)
This lemma is a powerful tool, allowing us to control the aliasing sets through the matrix H. The
proof can be found in Appendix C.3, and is straightforward, given the relationship between uand
18f. Understanding why we choose this relationship, however, is more complicated. Underlying this
choice is the algebraic theory of monoids and abstract algebra.
As we have mentioned, our ultimate goal is to design Hto sufficiently “spread out” the non-zero
indices among the aliasing sets. Below, we define a simple and useful construction for H.
Definition C.2. Consider {i1, . . . , i b}=I⊂[n], with|I|=b, andH∈Zb×n
2. Lethicorrespond to
theithrow of H, given by hi=eij, the length nunit vector in coordinate ij. Then if we subsample
according to (23) we have:
U(j) =X
k:ki=ji∀i∈IF(k). (25)
which happens to result in aliasing sets A(j) ={k:ki=ji∀i∈I}all of equal size 2b. The above
choice Hactually induces a rather simple sampling procedure when we follow (23). For instance if
I= [b], we have:
u(ℓ) =f 
[ℓ;1n−b]
, (26)
In other words, in this case, we construct samples by freezing n−bof the inputs to 1and then varying
the remaining binputs across all the 2bpossible options. In the case where the non-zero Mobius
interactions are chosen uniformly at random, this construction does a good job at spacing them out
across the various aliasing sets. The following result formalizes this.
Lemma C.3. (Uniform interactions) Let k1, . . . ,kKbe sampled uniformly at random from Zn
2,
where F(ki)̸= 0,∀i∈[K], butF(k) = 0 for all other k∈Zn
2. Construct disjoint sets Ic⊂[n]for
c= 1, . . . , C , and the corresponding matrix Hcaccording to Definition C.2. Let Ac(j)correspond
to the aliasing sets after sampling with respect to matrix Hc. Now define:
jsuch that ki∈ Ac(j):=jc
i. (27)
Then if b=O(log(K)),K=O(2n/C), in the limit as n→ ∞ withC=O(1),jc
iare mutually
independent and uniformly distributed over Zb
2.
The proof is given in Appendix C.6.1, and follows directly from the form of the aliasing sets Ac(j).
Corollary C.3 means that using Has constructed in Definition C.2 ensures that we all kwithF(k)̸=
0are uniformly distributed over the aliasing sets, which maximizes the number of singletons. This
result, however, hinges on the fact that the non-zero coefficients are uniformly distributed. We are
also interested in the case where the non-zero coefficients are all low degree. In order to induce a
uniform distribution in this case, we need to exploit a group testing matrix.
Lemma C.4. (low degree interactions) Let k1, . . . ,kKbe sampled uniformly at random from {k:
|k| ≤t,k∈Zn
2}, where F(ki)̸= 0,∀i∈[K], butF(k) = 0 for all other k∈Zn
2. By constructing
Cmatrices Hc, c= 1, . . . , C from rows of a near constant column weight group testing matrix, and
sampling as in (23), ift= Θ( nα)forα <0.409, and b=O(log(K)),K=O(nt), in the limit as
n→ ∞ ,jc
ias defined in (27) are mutually independent and uniformly distributed over Zb
2.
The proof is given in Appendix C.6.2. It relies on an information theoretic argument, exploiting a
result from optimal group testing [53].
C.3 Proof of Lemma C.1
Proof. Taking the Mobius transform of ugives us:
19U(k) =X
ℓ≤k(−1)1T(k−ℓ)u(ℓ)
=X
ℓ≤k(−1)1T(k−ℓ)f K
i:ℓi=0hi!
=X
ℓ≤k(−1)1T(k−ℓ)X
r≤J
i:ℓi=0hiF(r)
=X
ℓ∈Zb
2(−1)1T(k−ℓ)1{ℓ≤k}X
r∈Zn
2F(r)1(
r≤K
i:ℓi=0hi)
=X
r∈Zn
2F(r)
X
ℓ∈Zb
2(−1)1T(k−ℓ)1{ℓ≤k}1(
r≤K
i:ℓi=0hi)

=X
r∈Zn
2F(r)I(r)
Now let’s just focus on the term in the parenthesis for now, which we have called I(r).
Case 1: Hr=k
I(r) =X
ℓ≤k(−1)1T(k−ℓ)1(
r≤K
i:ℓi=0hi)
(28)
First note that under this condition, ℓ=k=⇒r≤J
i:ℓi=0hi. To see this, note that kj= 0 = ⇒
r≤hj. Since this holds for all jsuch that kj= 0, we have the previously mentioned implication.
Conversely, if ℓj< kj(this means ℓj= 0AND kj= 1) for some j, thenrandhjmust overlap. Thus,
1
r≤hj	
= 0 = ⇒1(
r≤K
i:ℓi=0hi)
= 0
We can split I(r)into two parts, the part where ℓ=kand the part where ℓ<k:
I(r) = 1(
r≤K
i:ki=0hi)
+X
ℓ<k(−1)1T(k−ℓ)1(
r≤K
i:ℓi=0hi)
(Hr=k) (29)
= 1 +X
ℓ<k0 (30)
= 1 (31)
Case 2: Hr̸=kLetHr=k′̸=k. This case itself will be broken into two parts. First let’s
say there is some jsuch that kj= 0 andk′
j= 1. Since k′
j= 1 we know that 1
r≤hj	
= 0.
Furthermore, since ∀ℓ∈ {ℓ:ℓ≤k}we have ℓj= 0. Then by a similar argument to our previous
one, we have 1
r≤J
i:ℓi=0hi	
= 0∀ℓ≤k. It follows immediately that I(r) = 0 in this case.
Finally, we have the case where k′<k. First, if there is a coordinate jsuch that 0 =ℓj< k′
j= 1,
we know that 1
r≤hj	
= 0so we have 1
r≤J
i:ℓi=0hi	
= 0∀ℓs.t.∃j, ℓj< k′
j. The only
ℓthat remain are those such that k′≤ℓ≤k. It is easy to see that this is a sufficient condition for
1
r≤J
i:ℓi=0hi	
= 1.
I(r) =X
ℓ≤k(−1)1T(k−ℓ)1(
r≤K
i:ℓi=0hi)
(32)
=X
k′≤ℓ≤k(−1)1T(k−ℓ)(33)
= 0 (34)
20Where the final sum is zero because exactly half of the ℓhave even and odd parity respectively.
Thus, the subsampling pattern becomes:
U(k) =X
Hr=kF(r).
C.4 Proof of Section 4
U(k) =X
ℓ≤k(−1)1T(k−ℓ)u(ℓ)
=X
ℓ≤k(−1)1T(k−ℓ)f  K
i:ℓi=0hi!
⊙d!
=X
ℓ≤k(−1)1T(k−ℓ)X
r≤J
i:ℓi=0hiF(r)1
r≤d	
=X
ℓ∈Zb
2(−1)1T(k−ℓ)1{ℓ≤k}X
r∈Zn
2F(r)1(
r≤K
i:ℓi=0hi)
1
r≤d	
=X
r∈Zn
2F(r)1
r≤d	
X
ℓ∈Zb
2(−1)1T(k−ℓ)1{ℓ≤k}1(
r≤K
i:ℓi=0hi)

=X
r∈Zn
2F(r)1
r≤d	
I(r)
=X
Hr=k
r≤dF(r)
C.5 Proof of Main Theorems
Theorem 5.1. (Recovery with KUniform Interactions) Let fsatisfy Assumption 2.1 for some K=
O(2nδ)withδ≤1
3and let the non-zero coefficients of Fbe drawn from an absolutely continuous
distribution. For {Hc}C
c=1chosen as in Lemma C.3 with b=O(log(K)),C= 3 andDc=I,
Algorithm 1 exactly computes the transform FinO(Kn)samples and O(Kn2)time complexity with
probability at least 1−O(1/K).
Theorem 5.2. (Noise-Robust Recovery with K t-Degree Interactions) Let fsatisfy Assumption 2.2
forK=O(poly( n))andt= Θ( nα)withα≤0.409. Assume either:
1. The non-zero coefficients of Fare drawn from an arbitrary continuous distribution, or
2.Uc,pis corrupted by noise as in (10) and let non-zero coefficients satisfy |F(k)|=ρ.
Then, for {Hc}C
c=1chosen as in Lemma C.4 with b=O(log(K)),C= 3, and Dcchosen as a
suitable group testing matrix, Algorithm 1 exactly computes the transform FinO(Ktlog(n))samples
andO(Kpoly( n))time complexity with probability at least 1−O(1/K)in both the noiseless case
(1) and noisy case (2).
Proof. The first step for proving both Theorem 5.1 and Theorem 5.2 is to show that Algorithm 1 can
successfully recover all Mobius coefficients with probability 1−O(1/K)under the assumption that
we have access to a Detect ( Uc(j))function that can output the type Type ( Uc(j))for any aliasing
setUc(j). Under this assumption, we use density evolution proof techniques to obtain Theorem C.5
and conclude both theorems.
Then, to remove this assumption, we need to show that we can process each aliasing set Uc(j)correctly,
meaning that each bin is correctly identified as a zeroton, singleton, or multiton. Define Eas the error
21event where the detector makes a mistake in O(K)peeling iterations. If the error probability satisfies
Pr(E)≤O(1/K), the probability of failure of the algorithm satisfies
PF= Pr
bF̸=F|Ec
Pr(Ec) + Pr
bF̸=F|E
Pr(E)
≤Pr
bF̸=F|Ec
+ Pr(E)
=O(1/K).
In the following, we describe how we achieve Pr(E)≤O(1/K)under different scenarios.
In the case of uniformly distributed interactions without noise, singleton identification and detection
can be performed without error as described in Section C.7.1. In the case of interactions with low
degree and without noise, singleton identification and detection can be performed with vanishing error
as described in Section C.7.2. Lastly, we can perform noisy singleton identification and detection
with vanishing error for low degree interactions as described in Section C.7.2.
C.6 Density Evolution Proofs
The density evolution proof is generally separated into two parts.
• We show that with high probability, nearly all of the variable nodes will be resolved.
•We show that with high probability, the graph is a good expander , which ensures that if only
a small number are unresolved, the remaining variable nodes will be resolved.
Whether the decoding succeeds or fails depends entirely on the graph (or rather distribution over
graphs) that is induced by the algorithm. The graph ensemble is parameterized as G
D,{Mc}c∈[C]
.
Dis the support distribution. The set of non-zero Mobius coefficients {r:M[f](r)̸= 0} ∼ D is
drawn from this distribution. In [ 13], using the arguments above it is shown that if the following
conditions hold, the peeling message passing successfully resolves all variable nodes:
1.In the limit as n→ ∞ asymptotic check-node degree distribution from an edge perspective
converges to that of independent an identically distributed Poisson distribution (shifted by 1).
2.The variable nodes have a constant degree C≥3(This is needed for the expander property).
3. The number of check nodes bin each of the Csampling group is such that 2b=O(K).
Theorem C.5 ([13]).If the above three conditions hold, the peeling decoder recovers all Mobius
coefficients with probability 1−O(1/K).
In the following section, we show that for suitable choice of sampling matrix, these conditions are
satisfied, both in the case of uniformly distributed and low degree Mobius coefficients.
C.6.1 Uniform Distribution
In order to satisfy the conditions for the case of a uniform distribution of we use the matrix in
Corollary C.3. We select C= 3different I1, I2, I3such that Ii∩Ij=∅ ∀i̸=j∈ {1,2,3}. Note
that this satisfies condition (2) above. Furthermore, we let kscale as O(2nδ). In order to satisfy
condition (3), we must have δ <1
3, since each Iican consist of at most1
3of all the coordinates.
We now introduce some notation. Let gj(·)represent the hash function , that maps a frequency rto a
check node index kin each subsampling group j= 1, . . . , C , i.e.,gj(r) =Hjr. Per our assumption,
we have Knon-zero variable notes r(1), . . . ,r(K)chosen uniformly at random. Technically, we are
sampling without replacement, however, sinceK
2n→0, the probability of selecting a previously
selected r(i)vanishes. Going forward in this subsection, we will assume that each riis sampled
with replacement for a more brief solution. A more careful analysis that deals with sampling with
replacement before taking limits yields an identical result.
22First, let’s consider the marginal distribution of gj(r(i))for some arbitrary j∈[C]andi∈[K].
Assuming sampling with replacement, we have:
Pr
gj(r(i)) =k
= Pr
r(i)
Ij=k
=Y
m∈IjPr
r(i)
m=km
=1
2b. (35)
Thus, we have established that the our approach induces a uniform marginal distribution over the
2bcheck nodes. Next, we consider the independence of our bins. By assuming sampling with
replacement, we can immediately factor our probability mass function.
Pr
\
i,jgj(r(i)) =k(i,j)
=Y
iPr
\
jgj(r(i)) =k(i,j)
 (36)
Furthermore, since we carefully chose the Iisuch that they are pairwise disjoint, we have
Pr
\
jgj(r(i)) =k(i,j)
= Pr
\
jr(i)
Ij=k(i,j)
=Y
jPr
r(i)
Ij=k(i,j)
=
Y
jPr
gj(r(i)) =k(i,j)
,(37)
establishing independence. Let’s define an inverse load factor η=2b
K. From a edge perspective,
sampling with replacement with independent uniformly distributed gives us:
ρj=jηK
j1
2bj
1−1
2bk−j
, (38)
For fixed η, asymptotically as K→ ∞ this converges to:
ρj→(1/η)j−1e−1/η
(j−1)!. (39)
C.6.2 Low Degree Distribution
For this proof, we take an entirely different approach to the uniform case. We instead exploit the
results of Theorem F.1, which is about asymptotically exactly optimal group testing, and then make
an information-theoretic argument. Let Xnbe a group testing matrix (constructed either by an i.i.d.
Bernoulli design or a constant column weight design using the parameters required for the given n).
We don’t explicitly write the dependence of Xnont, since by invoking Theorem F.1, we assume
some implicit relationship where t= Θ( nθ)forθsatisfying the theorem conditions. Now consider
somernchosen uniformly at random from the n
t
weight tbinary vectors. Note that in this work we
actually use what is known as the “i.i.d prior” as opposed to the “combinatorial prior” that we have
just defined. As noted in [ 15], these are actually equivalent, so we can arbitrarily choose to work with
one, and the result holds for the other as well. We define:
Yn=Xnrn. (40)
Furthermore, we define the decoding function Decn(·), which represents the deterministic procedure
that successfully recovers rwith vanishing error probability. We have the following bounds on the
entropy of Yn:
H(Yn) = H(Yn
1) +H(Yn
2|Yn
1) +···+H(Yn
T|Yn
1, . . . , Yn
T−1) (41)
≤T, (42)
where we have used the fact that binary random variables have a maximum entropy of 1. Furthermore,
by the properties of entropy we also have H(Yn)≥H(Dec( Yn,Xn)|Xn). Dividing through by
T, we have:
H(Dec( Yn,Xn)|Xn))
T≤H(Yn)
T≤1. (43)
23LetDecn(Yn,Xn)) =rn+ err n(Yn,Xn). It is known (see [ 15]) that Pr(err n(Yn,Xn)̸= 0) =
O(poly( T)e−T). Thus, we can bound the left-hand side as:
H(Dec( Yn,Xn)|Xn)
T=H(rn+ err n(Yn,Xn)|Xn)
T(44)
≥H(rn)−H(errn(Yn,Xn)|Xn)
T(45)
≥H(rn)−H(errn(Yn,Xn))
T, (46)
Where in (45) we have used the bound H(A+B)≥H(A)−H(B)and the fact that Xnandrn
are independent, and in (46) we have used the fact that conditioning only decreases entropy. By the
continuity of entropy and Theorem F.1, we have that:
lim
n→∞H(rn)−H(errn(Yn,Xn))
T= lim
n→∞log n
t
T−lim
n→∞H(errn(Yn,Xn))
T= 1−0 = 1 .(47)
This establishes that:
lim
n→∞1
T(n)T(n)X
i=1H
Yn
i|Yn
1:(i−1)
= 1. (48)
Unfortunately, this does not immediately imply that allof the summands have a limit of 1, however,
it does mean that the fraction of total summands that are less than one goes to zero (it grows as
o(T(n))). Let G⊂Ncorrespond to the set containing all the indicies iof the summands that are
equal to 1. By using the fact that conditioning only reduces entropy, we have
lim
n→∞H 
Yn
i|Yn
Si
= 1, Si={j < i, j ∈G}, (49)
Now we define the countable sequence of random variables:
¯Yi= lim
n→∞Yn
i, i∈N. (50)
By continuity of entropy, and the above limit and definition, we have:
H ¯Yi|¯YSi
= 1, (51)
Noting that conditioning only decreases entropy, we immediately have that ¯Yi∼Bern(1
2). Now
consider some arbitrary finite set S∗⊂G. We will now prove that {¯Yi, i∈S∗}is mutually
independent.
Proof. Leti1< i2< . . . < i |S∗|be an ordered indexing of the elements of S∗. Furthermore, let
Qj={iq|1≤q≤j}. Assume the set {¯Yi, i∈Qj}is mutually independent, and use the notation
YQjto represent a vector containing all of these entries. We have:
H(Yij+1,YQj) =H(YQj) +H(Yij+1|YQj). (52)
However, by using the fact that conditioning only decreases entropy we have:
1 =H(Yij+1|YSj+1)≤H(Yij+1|YQj)≤H(Yij+1)≤1, (53)
thus,
H(Yij+1|YQj) =H(Yij+1) = 1 . (54)
This leads to the following chain of implications:
H(Yij+1,YQj) =H(YQj) +H(Yij+1)⇐⇒ Yij+1⊥ ⊥YQj. (55)
From this, and the initial inductive assumption, we can conclude that {¯Yi, i∈Qj+1}is mutually
independent. The base case of j= 1follows from the fact that a set containing just one single random
variable is mutually independent. Since Q|S∗|=S∗the proof is complete.
Now let L(n) =|G∩[n]|we know L= Θ( T(n)), which follows from the stronger result that
limn→∞L(n)
T(n)= 1. Take b≤L(n)
CBy leveraging the above results, we can select our subsampling
matrices {Hi}C
i=1from suitable rows of Xn. LetS(n)
1, . . . , S(n)
C⊂G∩[n],S(n)
i=bandS(n)
i∩
S(n)
j=∅. Then take
Hi(n) =Xn
Si,:. (56)
Due to the independence result proved above, the asymptotic degree distribution is:
ρj→(1/η)j−1e−1/η
(j−1)!. (57)
24C.7 Singleton Detection and Identification
In this section, we prove the main results about singleton detection and identification. For the
noiseless case, we introduce another assumption:
Assumption C.6. (No Cancellation) Suppose the non-zero values F(k1), ... ,F(kK)are sampled
from a joint distribution Pthat satisfies the following condition:
X
i∈SF(ki)̸= 0,;∀S⊆[K],;S̸=∅. (58)
This assumption is quite mild, and there are many classes of Pthat satisfy this assumption.
Lemma C.7. Any absolutely continuous Psatisfies Assumption C.6 a.s..
Proof. For simplicity let Frepresent a Kdimensional random vector containing F(ki)at index i.
Let the set R(S, α) ={F:P
i∈SF(ki) =α}. Since Pis absolutely continuous, a density pexists
such that:
P(F∈ R(S, α)) =Z
R(S,α)dp. (59)
However, dim(R(S, α)) =K− |S|. Thus for S̸=∅,R(S, α)has Lebesgue measure zero, thus
Pr X
i∈SF(ki) = 0!
=P(F∈ R(S,0)) =Z
R(S,0)dp= 0. (60)
C.7.1 Uniform Interactions Singleton Identification and Detection without Noise
For singleton identification, we observe that in the case of a singleton, yc=k∗, thus, in the case of a
singleton k∗can be recovered. We still need to show that the Detect function correctly identifies the
type. We separate the proof into three parts:
(1) Prove Detect( Uc(j)) =HZ=⇒Type( Uc(j)) =HZ.
(2) Prove Detect( Uc(j)) =HS=⇒Type( Uc(j)) =HS.
(3) Prove Detect( Uc(j)) =HM=⇒Type( Uc(j)) =HM.
Consider the subsampling group Uc(j)for some fixed c,j. We first consider the case where |k|is not
restricted, and denote the set of non-zero indices k1, . . . ,kKasN.
Proof of (1) LetDetect( Uc(j)) =HZ, and for contradiction’s sake assume Type( Uc(j))̸=HZ.
Detect( Uc(j)) =HZ=⇒X
k≤¯dps.t.Hck=jF(k) = 0∀p, (61)
Since Type( Uc(j))̸=HZ, we have that N ∩ { k:Hck=j} ̸=∅. Thus, if we consider the above
implication for the case of p= 0and noting that d0=0, we have:
Detect( Uc(j)) =HZ=⇒X
N∩{k:Hck=j}F(k) = 0 (62)
But considering the no cancellation condition, this is impossible, thus proving (1).
Proof of (2) Note that the converse of (1) is true (the proof is immediate). Thus, proving
Detect( Uc(j)) =HS=⇒ ¬ (Type( Uc(j)) =HM) is the same as proving (2). We will again use
the method of contradiction, and assume Detect( Uc(j)) =HSandType( Uc(j)) =HM.
Detect( Uc(j)) =HS=⇒Uc,p(j)∈ {0, Uc,0(j)} ∀p >1. (63)
Note that by our assumption, Uc,0(j)̸= 0. By our assumption that Type( Uc(j)) =HM, we must
have|N ∩ { k:Hck=j} ̸=∅| ≥2. Choose k1,k2∈ N ∩ { k:Hck=j}. Given our choice of dp
25∃p∗>1s.t. only *one* of k1ork2≤¯dp∗. Without loss of generality we will assume k2≤¯dp∗
andk1≰¯dp∗. Now, define the following sets:
J0=N ∩ { k:Hck=j} (64)
Jp∗=N ∩ { k:Hck=j k≤¯dp∗} (65)
We know k2∈ Jp∗andk1∈ J0\ Jp∗, thus|Jp∗| ≥1and|J0\ Jp∗| ≥1. With this, we can show
that the implication above cannot be satisfied.
Case 1: Uc,p∗(j) = 0 .
Uc,p∗(j) = 0 = ⇒X
k∈J∗pF(k) = 0 . (66)
SinceJ∗
pis not empty, from our distributional assumption, the above sum cannot be 0.
Case 2: Uc,p∗=Uc,0(j).
Uc,0(j)−Uc,p∗(j) = 0 = ⇒X
k∈J0\J∗pF(k) = 0 . (67)
SinceJ0\ J∗
pis not empty, from our distributional assumption, the above sum cannot be 0. This
implies that Type( Uc(j)) =HMmust be false, thus proving (2).
Proof of (3): Since we have a converse for (1), a converse for (2) suffices to prove (3). The converse
follows below:
Type( Uc(j)) =HS=⇒ ∃k∗s.t.Uc,p∈ {0, F(k∗)},∀p. (68)
Since F(k∗)̸= 0, we have Uc,0(j)̸= 0 and all entries of Uc(j)are either F(k∗)or0. Thus,
Detect( Uc(j)) =HS.
C.7.2 Low Degree Singleton Identification and Detection without Noise
Here we include a sketch of the argument for the noiseless part of Theorem 2, which focuses on the
case|k| ≤t(the rest of Theorem 2 is unchanged).
(1) Prove Detect( Uc(j)) =HZ=⇒Type( Uc(j)) =HZ.
(2+3) Prove Pr (Detect( Uc(j)) = Type( Uc(j)))→1.
Proof of (1) The proof is identical to above.
Proof of (2+3) The proof is the same, with one notable exception. In the low degree case, p∗may
not always exist. In this case, we can simply rely on the result of [ 54]. Since Pr(ˆk̸=k∗)≤n−β,
we correctly recover each k∗in the limit, meaning we must have Pr(Dck1̸=Dck2)→1(this
is equivalent to the existence of p∗). Thus, by the same argument this implies that Detect ( Uc(j))
has vanishing error in the limit. The complete argument requires us to union bound over all of the
singleton identifications success, but since Tis linear in β, so long as K= poly( n), constant β
suffices for vanishing error.
C.7.3 Singleton Identification in i.i.d. Spectral Noise
In this section, we discuss how to ensure that we can detect the true non-zero index r∗from the
delayed samples, under the i.i.d. noise assumption. We first discuss the delay matrix itself, D∈
ZP1×n
2 . As in the noiseless case, we want to choose this matrix to be a group testing matrix. For the
purposes of theory, we will choose Dsuch that each element is drawn i.i.d. as a Bern ν
t
for some
ν= Θ(1) . We denote the ithrow of Dasdi. Each group test is derived from one of the delayed
samples. Under the i.i.d. spectral noise model, this means each sample has the form:
Ui(k) =X
Hr=k
r≤diF(r) +Zi(k) (69)
=F(r∗)1
r∗≤¯di	
+Zi(k), (70)
26where Zi(k)∼ N 
0, σ2
. Essentially, we can view this as a hypothesis testing problem, where we
have one sample X, and hypothesis and the alternative are:
H0:X=Z H 1:X=F(r∗) +Z, Z ∼ N(0, σ2)
Furthermore, lets say the magnitude of |F[k]|=ρis known. We construct a threshold test:
φ(X) =1{|X|> γ} (71)
With such a test, we can compute the cross-over probabilities:
p01= Pr
H0(|X|> γ) = 2 Q(γ/σ), (72)
p10= Pr
H1(|X|< γ) = Φ(( γ−ρ)/σ)−Φ((−γ−ρ)/σ). (73)
For the sake of simplicity, we will make the choice to choose γsuch that p10=p01. In that case, we
can numerically solve for the cross-over probability which is fixed for a given signal-to-noise ratio.
10−1100101
ρ/σ0.00.10.20.30.40.5Crossover Probability
Figure 14: Symmetric cross-over probability induced by hypothesis testing problem for noisy
singleton identification/detection.
Now we can use [ 54], to prove our desired result. let qbe the resulting cross-over probability for a
given ρ/σ. The probability that all of our Singleton identifications succeed can computed by a union
bound on Pe= Pr
ˆk̸=k
≤n−β. IfK= poly( n), a constant βsuffices us to drive the union
bound to zero.
Lemma C.8. For any fixed SNR, taking Dcsuch that each element is Bern ν
t
, and t= Θ( nα)for
α∈(0,1), taking P1=O(tlog(n))suffices to ensure that we can achieve error of bin identification
failure with probability of error O(1/K3).
C.7.4 Singleton Detection in i.i.d. Spectral Noise
We note that the general flow of this proof follows [ 40], but there are several fundamental differences
that make this proof overall quite different. We define Ebas the error event where a bin kis decoded
wrongly, and then using a union bound over different bins and different iterations, the probability of
the algorithm making a mistake in bin identification satisfies
Pr(E)≤(# of iterations) ×(# of bins) ×Pr(Eb)
The number of bins is at most ηKfor some constant ηand the number of iterations is at most CK
(at least one edge is peeled off at each iteration in the worst case). Hence, Pr(E)≤ηCK2Pr(Eb). In
order to satisfy Pr(E)≤O(1/K), we need to show that Pr(Eb)≤O(1/K3).
We already showed in Lemma C.8 that we can achieve singleton identification under noise with
vanishing error O(1/K3)with a delay matrix D∈ZP1×n
2 .
To achieve type detection, we construct another pair of delay matrices D1∈ZP2×n
2 andD2∈ZP2×n
2 .
We will choose D1andD2such that each element is drawn i.i.d. as a Bern 
(1/2)1/t
. We denote
27theithrow of D1asd1
iand denote the ithrow of D2asd2
i. Then, with these delay matrices, we can
obtain observations of the form
U1
i(k) =X
Hr=k
r≤d1
iF(r) +Zi(k)
U2
i(k) =X
Hr=k
r≤d2
iF(r) +Zi(k).
Note that we can represent these observations as
U1=S1α+W1
U2=S2α+W2
withW1,W2∼ N (0, σ2I), aαvector with entries F(r)for coefficients in the set and binary
signature matrices S1,S2with entries indicating the subsets of coefficients included in each sum.
Then, we subtract these observations to obtain a single observation U=U1−U2which can be
written as
U=Sα+W
withW∼ N(0,2σ2I)andS=S1−S2. This construction allows us to show that the columns of S
are sufficiently incoherent and hence we can correctly perform identification.
Lemma C.9. For any fixed SNR, taking D1
candD2
csuch that each element is Bern 
(1/2)1/t
, and
t= Θ( nα)forα∈(0,1/2)and taking P2=O(tlog(n))suffices to ensure that the probability
Pr(Eb)for an arbitrary bin can be upper bounded as Pr(Eb)≤O(1/K3).
Proof. In the following, we prove that Pr(Eb)≤O(1/K3)holds using the observation model. We
consider separate cases where the bin in consideration is fixed as a zeroton, singleton, or multiton.
The error probability Pr(Eb)for an arbitrary bin can be upper bounded as
Pr(Eb)≤X
F∈{H Z,HM}Pr(F ← H S(r, F(r)))
+X
F∈{H Z,HM}Pr(HS(br,bF(r))← F)
+Pr(HS(br,bF(r))← H S(r, F(r)))
above, each of these events should be read as:
1.{F ← H S(r, F(r))}: missed verification in which the singleton verification fails when the
ground truth is in fact a singleton.
2.{HS(br,bF(r))← F} : false verification in which the singleton verification is passed when
the ground truth is not a singleton.
3.{HS(br,bF(r))← H S(r, F(r))}: crossed verification in which a singleton with a wrong
index-value pair passes the singleton verification when the ground truth is another singleton
pair.
We can upper-bound each of these error terms using Propositions C.10, C.11, and C.12. Note that all
upper-bound terms decay exponentially with P2except for the term Pr(br̸=r)≤O(1/K3).
We use Theorem F.3 to show that we can achieve Pr(br̸=r)≤O(1/K3)if we choose P1=
O(tlogn). Since all other error probabilities decay exponentially with P2, it is clear that if P2is
chosen as P2=O(tlogn), the error probability can be bounded as Pr(Eb)≤O(1/K3).
28Proposition C.10 (False Verification Rate) .For0< γ <η
4SNR , the false verification rate for each
bin hypothesis satisfies:
Pr(HS(br,bF(br))← H Z)≤e−P2
2(√1+2γ−1)2,
Pr(HS(br,bF(br))← H M)≤e−P2γ2
4(1+4 γ)+K2e−ϵ
1−4γν2
ρ22
P2,
where P2is the number of the random offsets.
Proof. The probability of detecting a zeroton as a singleton can be upper-bounded by the probability
of a zeroton failing the zeroton verification. This means
Pr(HS(br,bF(br))← H Z)≤Pr1
P2∥W∥2≥(1 +γ)ν2
≤e−P2
4(√1+2γ−1)2,
by noting that W∼ N(0, ν2I)and applying Lemma C.13.
On the other hand, given some multiton observation U=Sα+W, the probability of detecting it as
a singleton with index-value pair (br,bF(br))can be written as
Pr(HS(br,bF(br))← H M) = Pr1
P2U−bF(br)sbr2
≤(1 +γ)ν2
=
Pr1
P2∥g+v∥2≤(1 +γ)ν2
,
where g:=S(α−bF(br)ebr)andv:=W. Then, we can upper bound this probability as
Pr1
P2∥g+v∥2≤(1 +γ)ν2∥g∥2
P2≥2γν2
+ Pr∥g∥2
P2≤2γν2
.
To upper bound the first term, we use Lemma C.13. Note that the first term is conditioned on the
event∥g∥2/P2≥2γν2, thus the normalized non-centrality parameter satisfies θ0≥2γ. As a
result, we can use Lemma C.13 by letting τ2= (1 + γ)ν2. Then, the first term is upper bounded
byexp{−(P2γ2)/(4(1 + 4 γ))}. To analyze the second term, we let β=α−bF(br)ebrand write
g=Sβ. Denoting its support as L:= supp( β), we can further write Sβ=SLβLwhere SLis the
sub-matrix of Sconsisting of the columns in LandβLis the sub-vector consisting of the elements in
L. Then, we consider two scenarios:
• The multiton size is a constant, i.e., |L|=L=O(1). In this case, we have
λmin(S⊤
LSL)∥βL∥2≤ ∥SLβL∥2
Using∥βL∥2≥Lρ2, the probability can be bounded as
Pr∥g∥2
P2≤2γν2
≤Pr
λmin1
P2S⊤
LSL
≤2γν2
Lρ2
On the other hand, using Lemma C.14 with the selection β= 1/2andη=1
1+2L(1
2−2γν2
Lρ2),
we have
Pr∥g∥2
P2≤2γν2
≤2L2e−P2
2(1+2 L)2
1
2−2γν2
Lρ22
.
which holds as long as γ < Lρ2/(4ν2) =Lη
4SNR .
•The multiton size grows asymptotically with respect to K, i.e.,|L|=L=ω(1). As a result,
the vector of random variables g=SLβLbecomes asymptotically Gaussian due to the
central limit theorem with zero mean and a covariance
E[ggH] =1
2Lρ2I
29Therefore, by Lemma C.13, we have
Pr∥g∥2
P2≤2γν2
≤e−P2
2
1−γν2
Lρ2
which holds as long as γ < Lρ2/ν2=LηSNR .
By combining the results from both cases, there exists some absolute constant ϵ >0such that
Pr∥g∥2
P2≤2γν2
≤K2e−ϵ
1−4γν2
ρ22
P2
as long as γ < ρ2/(4ν2) =η
4SNR .
Proposition C.11 (Missed Verification Rate) .For0< γ <η
2SNR , the missed verification rate for
each bin hypothesis satisfies
Pr(HZ← H S(r, F[r]))≤e−P2
4(ρ2/ν2−γ)2
1+2ρ2/ν2
Pr(HM← H S(r, F[r]))≤e−P2
4(√1+2γ−1)2+ 2e−ρ2
2ν2P2+ 2Pr(br̸=r)
where P2is the number of the random offsets.
Proof. The probability of detecting a singleton as a zeroton can be upper bounded by the probability
of a singleton passing the zeroton verification. Hence, by noting that W∼ N(0, ν2I)and applying
Lemma C.13,
Pr(HZ← H S(r, F[r]))
≤Pr1
P2∥F[r]sr+W∥2≤(1 +γ)ν2
≤e−P2
4(ρ2/ν2−γ)2
1+2ρ2/ν2.
which holds as long as γ < ρ2/ν2=ηSNR .
On the other hand, the probability of detecting a singleton as a multiton can be written as the
probability of failing the singleton verification step for some index-value pair (br,bF[br]). Hence, we
can write
Pr(HM← H S(r, F[r])) = Pr1
P2U−bF[br]sbk2
≥(1 +γ)ν2
≤Pr1
P2U−bF[br]sbk2
≥(1 +γ)ν2bF[br] =F[r]∧br=r
+ Pr(bF[br]̸=F[r]∨br̸=r).
Then, using Lemma C.13, the first term is upper-bounded as
Pr1
P2U−bF[br]sbk2
≥(1 +γ)ν2bF[br] =F[r]∧br=r
≤Pr1
P2∥W∥2≥(1 +γ)ν2
≤e−P2
4(√1+2γ−1)2.
On the other hand, the second term can be bounded as
Pr(bF[br]̸=F[r]∨br̸=r)≤Pr(bF[br]̸=F[r]) + Pr( br̸=r)
= Pr(bF[br]̸=F[r]|br̸=r)Pr(br̸=r)
+ Pr(bF[br]̸=F[r]|br=r)Pr(br=r)
+ Pr(br̸=r)
≤Pr(bF[br]̸=F[r]|br=r) + 2Pr( br̸=r)
30The first term is the error probability of a BPSK signal with amplitude ρ, and it can be bounded as
Pr(bF[br]̸=F[r]|br=r)≤2e−ρ2
2ν2P2
Proposition C.12 (Crossed Verification Rate) .For0< γ <η
2SNR , the crossed verification rate for
each bin hypothesis satisfies
Pr(HS(br,bF[br])← H S(r, F[r]))≤e−P2γ2
4(1+4 γ)+Ke−ϵ
1−4γν2
ρ22
P2+K2e−ϵ
1−4γν2
ρ22
P2
2/t.
where P2is the number of the random offsets.
Proof. This error event can only occur if a singleton with index-value pair (r, F[r])passes the
singleton verification step for some index-value pair (br,bF[br])such that r̸=br. Hence,
Pr(HS(br,bF[br])← H S(r, F[r]))
≤Pr1
P2∥F[r]sr−bF[br]sbr+W∥2≤(1 +γ)ν2
= Pr1
P2∥Sβ+W∥2≤(1 +γ)ν2
= Pr1
P2∥Sβ+W∥2≤(1 +γ)ν2∥Sβ∥2≥2γν2
+ Pr 
∥Sβ∥2≤2γν2
where βis a2-sparse vector with non-zero entries from {ρ,−ρ}. Using Lemma C.13, the first term
is upper-bounded as
Pr1
P2∥Sβ+W∥2≤(1 +γ)ν2∥Sβ∥2≥2γν2
≤e−P2γ2
4(1+4 γ).
By Lemma C.14, the second term is upper bounded as
Pr 
∥Sβ∥2≤2γν2
≤8e−P2
50
1
2−γν2
Lρ22
which holds as long as γ < ρ2/(2ν2) =η
2SNR .
Lemma C.13 (Non-central Tail Bounds (Lemma 11 in [ 13])).Given any g∈RPand a Gaussian
vector v∼ N(0, ν2I), the following tail bounds hold:
Pr1
P∥g+v∥2≥τ1
≤e−P
4(√
2τ1/ν2−1−√1+2θ0)2
Pr1
P∥g+v∥2≤τ2
≤e−P
4(1+θ0−τ2/ν2)2
1+2θ0
for any τ1andτ2that satisfy τ1≥ν2(1 +θ0)≥τ2where
θ0:=∥g∥2
Pν2
is the normalized non-centrality parameter.
Lemma C.14. Suppose β= Θ(1) ,η= Ω(1) , and t= Θ( nα)for some α∈(0,1/2). Then, there
exists some n0such that for all n≥n0, we have
Pr
λmin1
P2S⊤
LSL
≤2β(1−β)−(2L+ 1)η
≤2L2exp
−η2
2P2
.
31Proof. For any rsampled uniformly from vectors up to degree t, the probability that it will have
degree 0≤k≤tcan be written as
Pr (|r|=k) = n
k
Pt
k=1 n
k
We know that the entries of srare given as (s1
r)i=1
r≤¯d1
i	
and(s2
r)i=1
r≤¯d2
i	
. Therefore,
Pr 
(s1
r)i= 1
= Pr 
d1
ij= 0,∀j∈supp( r)
=tX
k=1Pr 
d1
ij= 0,∀j∈supp( r)||r|=k
Pr (|r|=k)
=Pt
k=1 n
k
βk/t
Pt
k=1 n
k.
=:g(t, n)
With β= Θ(1) andt= Θ( nα)forα∈(0,1/2), we can show that limn→∞g(t, n) =β. Therefore,
there exists some n0such that |Pr 
(s1
r)i= 1
−β| ≤ηfor all n≥n0. For the rest of the proof, let
g= Pr 
(s1
r)i= 1
and assume |g−β| ≤η.
Then, recalling (sr)i= (s1
r)i−(s2
r)i, the distribution for each entry of srcan be written as
Pr ((sr)i= 1) = Pr (( sr)i=−1) = g(1−g).
Hence, using Hoeffding’s inequality, we obtain
Pr1
P2s⊤
rsr≤2β(1−β)−η
≤Pr1
P2s⊤
rsr≤2g(1−g)−η
≤exp
−η2
2P2
.
Furthermore, the conditional probability of another vector m̸=rbeing included in test iis given by
Pr 
(s1
m)i= 1|(s1
r)i= 1,|r|=k
= Pr ( dij= 0,∀j∈supp( m)\supp( r)||r|=k)
=tX
ℓ=0
β1/tℓ
1−k
nℓk
nt−ℓ
=k
n+
1−k
n
β1/tt
=:f(t, n, k ).
Withβ= Θ(1) andt= Θ( nα)forα∈(0,1), for any k≤t, we can show that limn→∞f(t, n, k ) =
β. Therefore, there exists some n0such that |Pr 
(s1
m)i= 1|(s1
r)i= 1
−β| ≤ηfor all n≥n0.
For the rest of the proof, let f= Pr 
(s1
m)i= 1|(s1
r)i= 1
and assume |f−β| ≤η.
On the other hand,
Pr ((sm)i(sr)i= 1) = 2 fg[1−g−(1−f)g]
Pr ((sm)i(sr)i=−1) = 2 [(1 −f)g]2
As a result, we have
E[(sm)i(sr)i] = 2g(f−g).
Since limn→∞E[(sm)i(sr)i] = 0 , there exists some n0such that −η≤E[(sm)i(sr)i]≤ηfor all
n≥n0. For the rest of the proof assume −η≤E[(sm)i(sr)i]≤η. As a result, we can write
Pr1
P2|s⊤
rsm| ≥2η
≤Pr 
|s⊤
rsm−P2E[(sm)i(sr)i]| ≥P2η
≤exp
−η2
2P2
.
32By Gershgorin Circle Theorem, the minimum eigenvalue of1
P2S⊤
LSLis lower bounded as
λmin1
P2S⊤
LSL
≥1
P2min
r∈L
|s⊤
rsr| −X
m∈L
m̸=r|s⊤
rsm|
.
Lastly, we apply a union bound over all (r,m)pairs to obtain
Pr
λmin1
P2S⊤
LSL
≤2β(1−β)−(2L+ 1)η
≤2L2exp
−η2
2P2
.
D Worst-Case Time Complexity
In this section, we discuss the computational complexity of Algorithm 1, which is broken down into
the following parts:
Computing Samples Computing samples for one sapling matrix requires computing the row-span
ofHc, which can be computed in n2boperations. Then for each sample, we must take the bit-wise
and with each row of the delay matrix, so the total complexity is: Cn2bP.
Taking Small Mobius Transform Computing the Mobius transform for each of the CPsubsam-
pled functions is CPb2b.
Singleton Detection To detect each singleton requires computing y. This requires Pdivisions for
each of the C2bbins, for a total of CP2boperations.
Singleton Identification To identify each singleton requires different complexity for our different
assumptions.
1.In the case of uniformly distributed interactions, singleton detection is O(1), since y=k∗
immediately, so doing this for each singleton makes the total complexity CK.
2.In the noiseless low degree case decoding k∗fromyispoly( n), so for each singleton the
complexity is CKpoly( n)
Message Passing In the worst case, we peel exactly one singleton per iteration, resulting in CK
subtractions (the above singleton identification bounds already take into account the need to re-do
singleton identification).
Thus in the case of uniformly distributed and low degree interactions respectively, the complexity is:
Uniform distributed noiseless time complexity =O(CPn 2b+CPb2b+CK)
=O(CPnK )
=O(n2K).
Low degree (noisy) time complexity =O(CPn 2b+CPb2b+CKpoly( n))
=O(CPpoly( n)K)
=O(poly( n)K).
E Additional Simulations
In this section, we present some additional simulations that did not fit in the body of the manuscript.
Fig 15 and 16. Plot the runtime of SMT vs. nunder both of our assumptions. In both cases we
observe excellent scaling with n. We note that our low degree setting has a higher fixed cost since we
33are using linear programming to solve our group testing problem and the solver appears to have some
non-trivial fixed time cost.
Fig. 17 plots the perfect reconstruction percentage against nand sample complexity. We also observe
a phase transition, however, the phase threshold appears very insensitive to n, as expected, since our
sample complexity requirement is growing like log(n), and we are already plotting on a log scale.
21002200230024002500260027002800290021000100 200 300 400 500 600 700 800 900 1000 n
N10−310−210−1100101Runtime Complexity (sec)
SMT
Figure 15: Time complexity of SMT under As-
sumption 2.1. The parameter Kis fixed and we
plot the runtime v.s. n. our algorithm remains
possible to run for n= 1000 where other com-
petitors fail.
21002200230024002500260027002800290021000100 200 300 400 500 600 700 800 900 1000 n
N10−310−210−1100101Runtime Complexity (sec)
SMTFigure 16: Time complexity of SMT under as-
sumption 2.2. The parameters Kandtare fixed
and we plot the runtime v.s. n. Our theory says
we have a poly( n)complexity. In practice, for
reasonable nour algorithm is running quickly.
21002200230024002500260027002800290021000100 200 300 400 500 600 700 800 9001000 n
N104105Sample Complexity
0%20%40%60%80%100%Perfect Reconstruction %
Figure 17: Perfect reconstruction percentage plotted against sample complexity and nunder Assump-
tion 2.2. Holding C= 3, we scale bto increase the sample complexity. We observe that the number
of samples required to achieve perfect reconstruction is scaling linearly is very insensitive to nas
predicted. We also include N= 2non the bottom axis, which is the total number of interactions. In
this regime we do not appear to consistently maintain zero error. This could be due to the fact that the
asymptotic behaviour of group testing might not yet be fuly realized in the regime with n≤1000 .
F Group Testing
F.1 Group Testing Achievability Results From Literature
Theorem F.1 (Part of Theorem 4.1 and 4.2 in [ 15]).Asymptotic Rate 1 Noiseless Group Testing:
Consider a noiseless group testing problem with t= Θ( nθ)defects out of nelements. We define the
rate of a group testing procedure as:
R:=log n
t
T(74)
where Tis the number of tests performed by the group testing procedure. For an i.i.d. Bernoulli
design matrix, for θ∈[0,1/3], in the limit as n→ ∞ , a rate R∗
BERN= 1is achievable with vanishing
error. Furthermore, for the constant column-weight design matrix, for θ∈[0,0.409] a rate R∗
CCW= 1
is achievable with vanishing error.
Theorem F.2 ([42,54]).Noiseless Group Testing : Consider the noiseless non-adaptive group
testing setup with t=|k|defects out of nitems, with tscaling arbitrarily in n. Let ˆkbe the output
34of a group testing decoder and let T∗= Θ (min {tlog(n), n}). Then there exists a strategy using
T≤(1 +ϵ)T∗such that in the limit as n→ ∞ we have:
Pr
ˆk̸=k
→0. (75)
Furthermore, there is a poly( n)algorithm for computing ˆk. From [ 54], for t=o(n)we can achieve:
Pr
ˆk̸=k
≤n−δ. (76)
with number of tests T=O((1 + δ)tlog(n)).
Note that the above error rate is not a state-of-art result, but suffices in this case for our proof, and is
very convenient in its form.
Theorem F.3 ([43]).Noisy Group Testing Under General Binary Noise : Consider the general
binary noisy group testing setup with crossover probabilities p10andp01. We use i.i.d Bernoulli
testing with parameter ν >0. There are a total of |k|=t= Θ( nθ)defects, where θ∈(0,1). Let
T∗= maxn
T(D)
1, T(ND)
1 , T(D)
2, T(ND)
2o
, where we have
T(D)
1 =1
νp10D(α/p10)tlog(t), (77)
T(ND)
1 =1
νwD (α/w)tlog(n), (78)
T(D)
2 =1
νe−ν(1−p10)D(β/p10)tlog(t), (79)
T(ND)
2 =1
νp01D(β/p01)tlog(n). (80)
where D(x) =xlog(x)−x+1, andw= (1−p01)e−ν+p10(1−e−ν). For any α∈(p10,1−p01),
β∈(p01,1−p10), there exist some number of tests T <(1 +ϵ)T∗where the Noisy DD algorithm
produces ˆksuch that in the limit as n→ ∞ we have:
Pr
ˆk̸=k
→0. (81)
The above result is state-of-art for noisy group testing and could be of interest generally for proving
the type of results we have here, however, but for simplicity, we state a similar more compact result
that suffices for our proofs in this paper.
Theorem F.4 ([54]).Let|k|=t=o(n), and consider an i.i.d. Bernoulli design group testing matrix.
Further consider the binary symmetric noise model with crossover probability q. If we construct ˆk
via the noisy column matching algorithm, we achieve:
Pr
ˆk̸=k
≤n−β, (82)
with number of tests
T=16(1 +√γ)2(1 +β) ln(2)
1−e−2(1−2q)2tlog(n). (83)
where γis a constant that depends on β.
F.2 Group Testing Implementation
We implement group testing via linear programming. As noted in [ 15], linear programming generally
outperforms most other group testing algorithms in both the noisy and noiseless case. We use the
35following linear program, to implement group testing.
min
k,ξnX
i=1ki+λPX
p=1ξj
s.t.ki≥0
ξp≥0
ξp≤1ps.t.yp= 1
dT
pk=ξpps.t.yp= 0
dT
pk+ξp≥1ps.t.yp= 1(84)
G Impact Statement
Rigorous tools for understanding models can potentially profoundly increase trust in deep learning
systems. If we can understand and reason for ourselves why a model is making a decision, we can put
greater trust into those decisions. Furthermore, if we understand why a model is doing something that
we believe is incorrect, we can better steer it towards doing what we believe is correct. This “steering”
of model behavior is sometimes described as alignment , and is a critical task for addressing things
like incorrect or misleading information generated by a model, or for address any undesirable biases.
In terms of concerns, it is important to not misinterpret or over-interpret the interaction indices that
come out of SMT. It could be the case that looking over some selection of interactions doesn’t reveal
the full picture, and leads one down an incorrect line of reasoning.
36NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and precede the (optional) supplemental material. The checklist does NOT
count towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
•[NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While ”[Yes] ” is generally preferable to ”[No] ”, it is perfectly acceptable to answer ”[No] ”
provided a proper justification is given (e.g., ”error bars are not reported because it would be too
computationally expensive” or ”we were unable to find the license for the dataset we used”). In
general, answering ”[No] ” or ”[NA] ” is not grounds for rejection. While the questions are phrased
in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your
best judgment and write a justification to elaborate. All supporting evidence can appear either in the
main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in
the justification please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
•Delete this instruction block, but keep the section heading “NeurIPS paper checklist” ,
•Keep the checklist subsection headings, questions/answers and guidelines below.
•Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction explicitly state the main theoretical contributions
of this work. The main contributions section highlight this, and specifically call out the
theoretical nature of this work. The introduction contains information about the practical
applications and motivations. A detailed descriptions of assumptions is left to Section 2, to
avoid an overly technical introduction, but it is made clear that there are limiting assumptions
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
37Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: There is a limitations section, and limitations are mentioned periodically when
relevant.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ”Limitations” section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The proofs are included in the Appendix, and are complete and correct. Section
3 and 4 are effectively a proof sketch explaining how and why the algorithm works.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
38Justification: The algorithm is exactly stated in the paper, and implementation details
(specifically related to group testing) are included in the appendix. A reader would be able
to exactly reproduce our code with the given information. We plan to release the code with
the camera ready version.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Code is included.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
39•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyperpa-
rameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [Yes]
Justification: The algorithms are relatively simple and complicated implementation details
are in the appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Error bars are included in the main paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer ”Yes” if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Experiments are not run at very large scale and runtime numbers are included
only for relative comparison.
40Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Ethical concerns are satisfied.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Though this work is primarily theoretical, there is a clear application in mind,
and thus is discussed in appendix section G.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
41Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The details of the code are included in the main body of the paper.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
42Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
43