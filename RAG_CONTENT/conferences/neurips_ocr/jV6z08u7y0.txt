The Implicit Bias of Gradient Descent toward
Collaboration between Layers: A Dynamic Analysis of
Multilayer Perceptions
Zheng Wang Geyong Min
Department of Computer Science
University of Exeter
{zw360;G.Min}@exeter.ac.ukWenjie Ruan∗
School of Computer Science
USTC
rwjie@ustc.edu.cn
Abstract
Theimplicit bias ofgradient descent has long been considered the primary mecha-
nism explaining the superior generalization of over-parameterized neural networks
without overfitting, even when the training error is zero. However, the implicit bias
toward adversarial robustness has rarely been considered in the research commu-
nity, although it is crucial for the trustworthiness of machine learning models. To
fill this gap, in this paper, we explore whether layers in neural networks collabo-
rate to strengthen adversarial robustness during gradient descent. By quantifying
this collaboration between layers using our proposed concept, co-correlation , we
demonstrate a monotonically increasing trend in co-correlation, which implies a
decreasing trend in adversarial robustness during gradient descent. Additionally,
we observe different behaviours between narrow and wide neural networks during
gradient descent. We conducted extensive experiments that verified our proposed
theorems.
1 Introduction
As Artificial Intelligence (AI) has been widely applied in many industrial sectors, understanding the
theoretical properties behind modern machine learning models is important, especially for neural
networks due to their black-box nature. One such property is implicit bias , stemming from the
phenomenon where over-parameterized neural networks, trained in a gradient descent manner, often
exhibit great generalization without over-fitting. This implicit bias of gradient descent is often
explained as steering neural networks towards solutions characterized by max-margin [3, 23, 12].
Another intriguing phenomenon is the existence of adversarial examples — imperceptible perturba-
tions of inputs that alter classification results. Apart from existing work on attacks — algorithms for
generating adversarial examples [ 5,39,37],defences against attacks, e.g., adversarial training [ 25,36]
and distillation [ 28], and verification [40,34] to identify safe regions guaranteeing the absence of ad-
versarial examples, recently some works are aiming at a theoretical understanding behind adversarial
robustness. However, a principled way to comprehend the core contributors to the vulnerability of
neural networks, especially a theoretical understanding of their relation to generalization capabilities,
remains fragmented. This fragmentation is largely due to the intricate nature of neural networks,
where robustness is interconnected with many factors spanning across input data distribution [ 33,14],
sampling complexity [ 1,27], optimization techniques [ 25], weight initialization strategies [ 41], and
model capacity and architectures [ 32,2,18,35]. Not to mention that only very few works can address
both generalization and adversarial robustness in a uniform framework. One such research by Frei
et al. [13] investigates the implicit bias concerning both generalization andadversarial robustness ,
∗Corresponding Author
38th Conference on Neural Information Processing Systems (NeurIPS 2024).asserting that while implicit bias leads to solutions with improved generalization, it results in weaker
adversarial robustness. However, this work ignores the architectural factor of neural networks and is
hard to generalize to neural networks with more than two layers.
Inspired by this work and to investigate whether neural network layers collaborate against adversarial
examples during training, this paper first adopts the novel concept of Dirichlet energy—originating
from Partial Differential Equations (PDEs) to assess the variability of a function [ 11]—to evaluate
the adversarial robustness of neural networks. We then theoretically demonstrate that Dirichlet
energy serves as an effective measure of adversarial risk. By decomposing the Dirichlet energy
across the entire neural network into its constituent layers, we can quantify the interactions between
adjacent layers concerning adversarial robustness. We term this interaction collaboration correlation
(co-correlation) and find that this metric reflects ’alignments’ in feature selection between neigh-
bouring layers. Furthermore, we conduct a dynamic analysis of co-correlation in two-layer MLPs,
demonstrating with high probability a monotonic increase under gradient descent, which indicates
diminishing adversarial robustness. Additionally, our experiments show that two-layer MLPs with
small widths tend to enhance their performance through strengthened co-correlation, a pattern not
observed in wide two-layer MLPs. Our key contributions in this paper can be summarized as follows:
1.To the best of our knowledge, this work is the first to study the implicit bias of interaction
between layers. We have quantified the interactions between adjacent layers and theoretically
demonstrated that co-correlation between layers strengthens during gradient descent in neural
networks under mild assumptions, suggesting that it not only fails to collaborate against
adversarial perturbations but may even hinder resistance to them during gradient descent.
2.We demonstrate how neural networks with a large width differ in behaviour from the neural
network of a small width, showing that MLPs with larger widths exhibit more resistance to
increased co-correlation and, therefore, are more adversarial robust, which is complementary
for the work by Dohmatob and Bietti [8].
3.Extensive experiments have been conducted to validate our proposed framework. By
controlling the weight initialization, a perspective also suggested by Zhu et al. [41], we
challenge the argument, as proposed by Huang et al. [18], that a wide neural network
does not necessarily lead to better adversarial robustness, through the lens of cross-layer
collaboration.
2 Related Works
2.1 Implicit Bias of Gradient Descent
The mystery of over-parameterized neural network trained with gradient descent manner hardly
over-fitting has long been studied. Chizat and Bach [3]study the two-layer neural network with
infinite width and homogeneous activations, showing that gradient flow can be characterized as a
max-margin classifier on exponentially tailed losses. Lyu et al. [24], Sarussi et al. [29] study the
two-layer Leaky ReLU neural network on linearly separable data and claim that networks converge
to a max-margin linear predictor by gradient descent manner. Frei et al. [12] confirms those claims
on high-dimensional nearly orthogonal data.
Lyu and Li [23], Ji and Telgarsky [19] claims that the homogeneous neural networks with
exponentially-tailed classification losses converge to a KKT point of a maximum-margin problem.
Kunin et al. [21] extend these results to a more boarder family of quasi-homogeneous neural networks.
A more recent research [ 13] considers both generalization and robustness for two-layer ReLU neural
networks, arguing that gradient descent is biased towards solutions that generalise well but are more
vulnerable against adversarial examples, even the neural network is highly over-parameterized.
2.2 Theoretical Investigation of Adversarial Robustness
Since the phenomenon of adversarial examples has been discovered [ 15], various works have been
proposed to understand the theoretical fundamentals behind it, especially for neural networks. Some
researchers argue that the source of adversarial vulnerability comes from the input data [ 33,8,31,
26,14,7,30,1,27]. The more recent researches investigate the fragility of neural networks from an
architectural perspective. Simon-Gabriel et al. [32] study the vulnerability of feed-forward neural
2networks measured by Lpnorm of the loss function w.r.t. input data, suggesting that the vulnerability
increases with input dimension independent of model structures. Daniely and Shacham [6]examined
the ReLU neural network characterized by decreasing dimensions at each layer. They asserted that
the manifestation of adversarial robustness is intrinsically tied to the network’s architecture, which
contrasts with the propositions put forth by Simon-Gabriel et al. [32]. Bubeck et al. [2]expanded
the findings of Daniely and Shacham’s work on two-layer neural networks from an "under-complete
case" scenario to an "over-complete" one where the number of neurons surpasses the input dimension.
They further broadened the conclusions drawn by Daniely and Shacham [6]and Bubeck et al. [2]
to encompass Deep ReLU networks, hinting at a crucial role played by bottleneck layers in these
networks. Zhu et al. [41], instead of merely considering random weights as the standard configuration,
conducted a comprehensive analysis of the effects of weight initialization on adversarial robustness.
Unlike previous studies that focus solely on the overall assessment of neural networks while over-
looking layer interactions, our research examines the synergistic involvement between layers within
neural networks, taking into account both weight initialization and optimization.
3 Preliminary
3.1 General Setting
We follow the binary classification setting where the input data is X ⊆Rd, with the label Y ⊆ { 0,1}.
Given data set D={(xi, yi)}n
i=1drawn from an unknown probability measure PonX × Y and the
neural network f:X ×Θ→R, where Θdenotes the set of parameters, our objective is to optimize
fby updating the weights with gradient descent method such that it can predict the label accurately.
The prediction result is shown in Equation (1).
ypred=1, sig(fW(x))>0.5
0, sig(fW(x))≤0.5,(1)
where sigis the sigmoid function, i.e., sig(x) = 1 /(1 +e−x). We use Binary Cross-Entropy (BCE)
lossin Equation (2)as our loss function. For simplicity, we denote ui=f(xi, W), i∈[n]as the
output of the neural network for input xi, where [n] ={k∈N+|k≤n}.
L(f, y) =1
nnX
i=1L(sig(ui), yi) =−1
nnX
i=1
yilog(sig(ui)) + (1 −yi) log(1 −sig(ui))
.(2)
3.2 Neural Networks and Adversarial Risk
Our exploration starts from a basic linear model, then to Multilayer Perceptrons (MLP) . We provide
the proof for both linear and 2-layer MLPs, which can be extended to MLPs with more layers. They
are defined as
flinear (x, W) =aT(Wx) (3a)
fmlp(x, W) =aT(σ(Wx)), (3b)
where x∈ X is the input data, W∈Rm×ddenotes the linear transformation, mis the width of the
networks. σdenotes the element-wise activation functions .
We follow the initialization setting in [ 9], where ais randomly initialized and fixed from a binary
selection of {−1√m,1√m}m. Additionally, we introduce a slightly different setting for the weights W,
which are randomly initialized following the normal distribution N(0,1
m1+2q)withq >0, instead of
N(0,1
m). However, in our experiment, different settings of qare considered, including the scenario
thatq≤0.
Generalization ability is one of the most important concepts for machine learning models. Classifiers
with better generalization power indicate lower natural risk for unseen data. Given data points
(x, y)∼Pand classifier f, the natural risk is defined as
R(f) = E
(x,y)∼P[L(f(x), y))]. (4)
When it comes to 0-1loss, the natural risk becomes the probability of misclassification for unseen
data points.
3Similar to natural risk, the adversarial risk is defined as the probability of misclassification under
adversarial perturbations as is shown in Definition 3.1.
Definition 3.1 (Adversarial Risk) .Given data points (x, y)∼P, and a perturbation εwithin a
norm-ball, i.e.,
Br={∥ε∥2≤r}, (5)
where r >0indicates the L2-norm budget for perturbations. The adversarial risk for neural network
fon loss function Lis defined as
Rrob(f, r) = E
(x,y)∼P
sup
ε∈BrL(f(x+ε), y))
(6)
Since adversarial perturbations are almost invisible to human eyes, we expect rto be quite small.
3.3 Dirichlet Energy
The concept of Dirichlet energy , originating from Partial Differential Equations (PDEs) , serves as a
tool to assess the variability of a function [ 11]. However, as argued by Dohmatob and Bietti [8], it
serves as a more effective measure of adversarial robustness than the Lipschitz constant . We extend
this concept to mappings to make it more suitable for multi-dimensional problems, which is formally
defined in Definition 3.2.
Definition 3.2 (Dirichlet Energy for Mappings) .Let convex set Z1⊆Rm1andZ2⊆Rm2. Given a
differentiable mapping ϕ:Z1→ Z 2, the Dirichlet Energy w.r.t. x∼Pxis defined as
S(ϕ)≜q
∥Jϕ(x)∥2
L2(Px)=q
Ex∼Px[∥Jϕ(x)∥2
2], (7)
where∥ · ∥ 2refers to the operator norm and Jϕ(x)denotes the Jacobian matrix of ϕw.r.t. its input x.
To be clear, we use ∥·∥ 2to denote the L2-norm for vectors and operator norm for matrices throughout
our analysis. This concept can be extended to layers with ReLU activation by extending the derivative
toReLU′(x) =1{x≥0}, where 1{x≥0}is the indicator function. With the concepts defined, we now
establish the relationship between adversarial risk and Dirichlet energy.
4 Measure Adversarial Risk by Dirichlet Energy
In this section, we establish the relationship between the adversarial risks andDirichlet energy ,
showing that Dirichlet Energy is a proper measurement for adversarial risk. Dohmatob and Bietti [8]
only compares the Dirichlet energy with the Lipschitz constant. We, instead, illustrate that Dirichlet
energy is a proper representation of the gap between natural risk and adversarial risk.
Theorem 4.1. Given data points (x, y)∼Pandx∼Px, the relationship between adversarial risk
and Dirichlet energy for classifier fwith differentiable loss function Lis shown as
Rrob(f, r)≲R(f) +rS(L(f)), (8)
where r > 0is the largest perturbation budget and S(L(f)) =r
Ex∼Pxh
∥∇fLT·Jf(x)∥2
2i
indicating the Dirichlet energy of the classifier on loss L.
As is shown, the Dirichlet energy is a proper representation of the gap between generalization
and adversarial risk which indicates the adversarial robustness. The proof relies on the linear
approximation of L(f). The detailed proof is shown in Appendix A.
We also empirically show that the Dirichlet energy for classifier f, i.e.,S(f)instead of S(L(f)),
is the part that influences the adversarial robustness, as is shown in Figure 1 where we compare the
level of Dirichlet energy for fwith the robust accuracy of the classifier attacked by Auto-attack [5].
Since the Dirichlet energy defined in Equation (7)can be used to measure the variability of mappings,
it follows that this metric could be employed to evaluate the adversarial robustness of individual
layers or modules within neural networks. Consequently, this allows for an assessment of whether
there is collaboration between these components in terms of adversarial robustness.
44.1 Measure Correlation between Layers
To evaluate the collective impact of neighbouring layers on the adversarial robustness, we regard the
neural network as compounded separate mappings, i.e., ϕ◦φ. By calculating the Dirichlet energy
for each mapping ( S(ϕ)andS(φ)) and comparing it with the overall Dirichlet energy for the whole
classifier ( S(ϕ◦φ)), we can quantify the collaboration between these two mappings, and we term
this quantification co-correlation which is defined in 4.2.
Definition 4.2 (Co-correlation) .Letφ:Z1→ Z 2andϕ:Z2→ Z 3be two successive mappings,
where both Z1andZ2are convex. Given input x∼Px, the collaboration correlation (co-correlation)
is defined as
ϱϕ,φ≜ 
Ex∼Px∥Jϕ◦φ(x)∥2
21
2
 
Ex∼Px
∥Jϕ(φ)∥2
2· ∥Jφ(x)∥2
21
2. (9)
To avoid any confusion, we use φinterchangeably to denote both the function φ:Z1→ Z 2and
the output of the function φ=φ(x)for a given input x. Consequently, φinJϕ(φ)represents the
outputs, while in Jφ(x), it represents the mapping. Since for each x∈Px, we have
∥Jϕ◦φ(x)∥2=∥Jϕ(φ)·Jφ(x)∥2≤ ∥Jϕ(φ)∥2· ∥Jφ(x)∥2.
We expect 0≤ϱϕ,φ≤1. The concept of co-correlation can be explained by the feature alignment of
layers in neural networks.
Interpretation of co-correlation. Intuitively, co-correlation can be viewed as feature ’alignment’
between layers. We illustrate this intuition with a 2-layer neural network with linear activation
functions, i.e.,
f(x) =ϕ(φ(x)) =W2W1x, (10)
where W1andW2are weight matrices, and φ(x) =W1xrepresents the feature selection in layer-1,
whileϕ(φ) =W2φrepresents it in layer-2. By Definition 4.2, the co-correlation between ϕandφis
ϱϕ,φ=∥W2W1∥2
∥W2∥2∥W1∥2. (11)
where ∥ · ∥ 2denotes the operator norm. Let Sφ= arg max ∥x∥2=1∥W1x∥2, representing the
set of input features that maximize the L2-norm of their corresponding outputs. Similarly, let
Sϕ= arg max ∥z∥2=1∥W2z∥2. Assume ϱϕ,φ= 1, there exists x∈ Sφsuch thatφ(x)
∥φ(x)∥2∈ Sϕ.
This implies that the maximal output in terms of the L2-norm at the first layer can lead the output
at the second layer to reach its maximum. In other words, in terms of maximizing outputs, layer 1
aligns with layer 2. We can thus regard co-correlation as the degree of alignment in feature selection
between adjacent layers within the context of output maximization. In the non-linear case, the weight
matrices are replaced by the Jacobians of the respective layers, which can be equivalently viewed as
weight matrices that vary based on their inputs.
Definition 4.3 (Other Related Statistics) .Given the same assumptions in Definition 4.2, we define
the linear correlation between ∥Jϕ(φ)∥2and∥Jφ(x)∥2as
ρϕ,φ≜Ex∼Px
∥Jϕ(φ)∥2· ∥Jφ(x)∥2

Eφ∼φ(x)∥Jϕ(φ)∥2
21
2
Ex∼Px∥Jφ(x)∥2
21
2, (12)
where φ∼φ(x)shows that φfollows the distribution of φ(x),x∼Px. It is obvious that
0≤ρϕ,φ≤1and when ρϕ,φ= 1,∥Jϕ(φ)∥2and∥Jφ(x)∥2are linear correlated.
The mean and variance for ∥Jϕ(φ)∥2· ∥Jφ(x)∥2are defined as
µϕ,φ≜Ex∼Px
∥Jϕ(φ)∥2· ∥Jφ(x)∥2
, (13)
and
varϕ,φ≜V arx∼Px
∥Jϕ(φ)∥2· ∥Jφ(x)∥2
. (14)
5(a) Robust Acc.
 (b)S(f)
0 10 20 30 40 50 
Epoch0.40.60.81.0
ϱ
a
,W
width-16 
width-32 
width-64 
width-128 
width-256 
width-512 
width-1024 
width-2048 
width-4096 
width-8192 (c) Linear Model
0 10 20 30 40 50 
Epoch0.40.60.81.0
ϱ
a
,
σ
○
W
width-16 
width-32 
width-64 
width-128 
width-256 
width-512 
width-1024 
width-2048 
width-4096 
width-8192 (d) MLP
Figure 1: For all MLPs considered, lower value of Dirichlet Energy (Figure 1b) corresponds to
larger robust accuracy on test-set attacked by L2-norm Auto-attack with ϵ= 0.5. The dynamics
of the co-correlation ϱfor linear and MLPs is shown in Figure 1c and 1d. The architectures of
neural networks are defined in Equation (3b). The parameters to control the weight initialization in
Assumption 5.1 is set to q= 0.25
Remark 4.4 (Linear Correlation for L2-norm of the Jacobian) .We have ρϕ,φ= 1 iff there exist
t∈Rsuch that
P(t∥Jϕ(φ)∥2=∥Jφ(x)∥2) = 1 ,
implying that ρϕ,φcertainly can be used to assess the linear correlations. It reduces to Pearson
correlation coefficient [4] when the mean of both random variables equals zero.
Now we give the theorem that binds them together.
Theorem 4.5 (Robustness Decomposition) .Given the same assumption in Definition 4.2, the mea-
surement for overall adversarial robustness can be decomposed as
S(ϕ◦φ) =
Ex∼P
∥Jϕ◦φ(x)∥2
21
2
=ϱϕ,φ
1 +varϕ,φ
µ2
ϕ,φ1
2ρϕ,φS(ϕ)S(φ) (15)
The proof is straightforward from the definition. Based on this theorem, we have conducted exper-
iments with various linear models and 2-layer MLPs. These experiments demonstrate that, apart
from the co-correlation, all other statistics are negligible, as shown in Figure 5 in the Appendix D.
Consequently, our analysis primarily focuses on the co-correlation ϱ.
5 On Dynamics of Co-Correlation
5.1 Dynamics for Linear Model
Before delving into our analysis, we state our assumptions explicitly.
Assumption 5.1. We assume that each element wi,jin the weight matrix W(0)∈Rm×dat ini-
tialization follows the Gaussian distribution N(0,1
m1+2q), with q >0. Additionally, each element
ar, r∈[m]inais randomly selected from the set {−1√m,1√m}, and fixed during training.
Assumption 5.2. We assume that for each (xi, yi)∈D, i∈[n],xiisL2norm bounded such that
∥xi∥2= 1for all i∈[n].
Since we only assume bounded inputs and a specific weight initialization method, compared to
existing works [ 23,19,21,12], our approach can be easily extended to MLPs with more than two
layers.
Now let us focus on the co-correlation defined in Equation (9)and show the dynamics of ϱϕ,φfor
each step of gradient descent. We start from the linear model described in Equation (3a). Given the
binary classification problem and the linear model described. Our first theorem demonstrates that
co-correlation ϱϕ,φgradually accumulates throughout gradient descent optimization. Despite the
simplicity of the linear model, it effectively exhibits most of the core properties under consideration.
6Since the weights are updated by the gradient descent, the update of the weights at step t∈Nis
∆wr(t) =ηar1
nnX
i=1 
yi−sig(ui(t))
xi,
where ηdenotes the learning rate. Therefore, the dynamics of the weights can be expressed as
˙W(t) =a⊗exT(t), (16)
where ⊗denotes the Kronecker product and ex(t)is the error weighted input such that
ex(t) =1
nnX
i=1h
yi−sig 
ui(t)i
xi. (17)
Given the weight updates, we demonstrate that the dynamics of the co-correlation for the linear
model, denoted as ˙ϱa,W(t), exhibit an increasing trend, particularly during the initial steps of training
when most predictions are still essentially random.
Theorem 5.3 (Dynamics of the Co-correlation for Linear Model) .Given the linear model defined in
Equation (3a)and training dataset D={(xi, yi)}n
i=1. Assume that assumptions 5.1 and 5.2 hold for
Wanda. The gradient descent applied to the weights results in the dynamics of the co-correlation
being expressed as:
˙ϱa,W(t) =ηC(t)ϱa,W, (18)
and with high probability,
C(t)≥Pt
τ=1ex(τ)Tex(t)
∥W(t)∥2
2·
1− 
v(t)Ta2
+O1
mq
(19)
where the v(t)is the dominate eigenvector for W(t)W(t)T.
When mis sufficiently large, and during the initial steps of the optimization process, ex(τ), τ∈[t]
are quite similar to each other in terms of cosine similarity, implying an acute angle to each other,
which leads toPt
τ=1ex(τ)Tex(t)≥0. As a result, we can conclude that C(t)≥0.
The detailed proof can be found in the Appendix B. This assertion is also corroborated by the results
of our experiments as shown in Figure 1c. Even though Theorem 5.3 is based on a linear model, the
essential properties are universally applicable and can be summarized as follows:
Property 1. The co-correlation ϱa,Wdevelops during the initial stages of training and becomes
saturated as training progresses to its later stages.
Property 2. The speed of the accumulation of co-correlation ϱa,Wis inversely related to the operator
norm of weights ∥W(t)∥2.
Under the same weight initialization conditions specified in Assumption 5.1, an increase in network
width leads to a decrease in the L2-norm of the weight, consequently causing a substantial rise in
co-correlation.
5.2 Dynamics for MLP Model
For the non-linear case, we make certain assumptions regarding activation functions.
Assumption 5.4. The derivative of the activation function σ′(x)in non-linear neural networks is
bounded by M. In other words, we have |σ′(x)| ≤M.
With Assumption 5.4, Theorem 5.3 can be extended to MLPs, and the two properties still hold.
Different from the linear model, the update of weights for non-linear MLP defined in (3b) is
∆wr=ηar1
nnX
i=1 
yi−sig(ui)
σ′(wT
rxi)xi, (20)
where σ′(wT
rxi)is the derivative of activation function w.r.t. its input. Hence,
∆W=η
a1exT
1...
amexT
m
,
7(a) ResNets
0 10 20 30 40 50 
Epoch0.230.240.25
ϱ
ResNet50 
WRN50 (b) A1-A1
0 10 20 30 40 50 
Epoch0.400.450.500.550.600.650.70
ϱ
ResNet50 
WRN50 (c) B1-B2
Figure 2: The dynamics of co-correlation for ResNet50 and WRN50 under different way of partition.
The way of partition is illustrated in Figure 2a. A1-A2 and B1-B2 represent the separations that
distinguish the head and tail separately.
where for each r∈[m],
exr=1
nnX
i=1σ′(wT
rxi) 
yi−sig(ui)
xi.
As is shown in Equation (20),ais not the dominant eigenvector for ∆W∆WTdue to the difference
ofexr, r∈[m]fromex. Thus to average out the difference, we define the weighted sum of inputs for
the non-linear model as
ex∗(t)≜1
nnX
i=1αi(t,x) 
yi−sig(ui(t))
xi, (21)
where (xi, yi)∈ D are realized r.v., and x∼Px. Given wi,ji.i.d.∼N(0,1
m1+2q)for each element in
W(0), we have w(0)∼N(0,1
m1+2qId). Furthermore, given that W(t)is calculated from W(0), it
is evident that each row in W(t), denoted as w(t), constitutes a random variable. As a result, both
σ′(w(t)Tx)andσ′(w(t)Txi)are bounded random variables that are contingent upon W(0). Hence,
we define αi(t,x)as
αi(t,x)≜EW(0)h
σ′(w(t)Tx)σ′(w(t)Txi)i
.
Since, x∼Px,αi(t,x)is still r.v. contingent to xand so does the ex∗(t). Now we show the dynamics
of co-correlation for two-layer MLP defined in Equation (3b).
Theorem 5.5. (Dynamics of the Co-correlation for MLP) Given the MLP defined in Equation (3)
with training dataset D={(xi, yi)}n
i=1,x∈ X such that x∼Px. Assume that Assumption 5.1 and
5.2 hold for Wanda, and Assumption 5.4 holds for the activation function. we have
˙ϱa,σ◦W(t) =ηC(t)ϱa,σ◦W(t).
With high probability,
C(t)≥Pt
τ=1 
1−aTv(τ)aTv(t)
Ex∼PxexT
∗(τ)ex∗(t)
Ex∼Px∥D(t)W(t)∥2
2+ max
O1√m
,O1
mq
,
where
D(t) =diag(σ′(w1(t)Tx),···, σ′(wm(t)Tx)),
andv(t)denotes the dominant eigenvector for W(t)W(t)T, withexT
∗is defined in Equation (21).
Similar to the Theorem 5.3, when mis sufficiently large, and during the initial steps of the optimization
where the error-weighted inputs exT
∗(τ), τ∈[t]do not significantly fluctuate, we have that C(t)≥0.
The detailed proof is in Appendix C. In Theorem 5.5, ex∗serves a similar purpose as exfor the linear
model. In addition, it considers the influence of the activation function. Property 1 still holds for the
MLPs, and Property 2 extends to Ex∼Px∥D(t)W(t)∥2
2=∥Jσ◦W(x)∥2
L(Pz).
80 10 20 30 40 50 
Epoch0.30.40.50.60.70.80.9
ϱ
a
,
σ
○
W
q=-0.15 
q=-0.2 
q=-0.05 
q=0
q=0.05 
q=0.15 
q=0.25(a) width-32
0 10 20 30 40 50 
Epoch0.30.40.50.60.70.80.9
ϱ
a
,
σ
○
W
q=-0.15 
q=-0.1 
q=-0.05 
q=0
q=0.05 
q=0.15 
q=0.25 (b) width-512
0 10 20 30 40 50 
Epoch0.30.40.50.60.70.80.9
ϱ
a
,
σ
○
W
q=-0.15 
q=-0.1 
q=-0.05 
q=0
q=0.05 
q=0.15 
q=0.25 (c) width-2048
0 10 20 30 40 50 
Epoch0.30.40.50.60.70.80.9
ϱ
a
,
σ
○
W
q=-0.15 
q=-0.1 
q=-0.05 
q=0
q=0.05 
q=0.15 
q=0.25 (d) width-8192
Figure 3: The dynamic of co-correlation under different set-up of weight initialization. MLP network
defined in Equation (3)with ReLU activation function for width 32,512,2048 and8192 are included.
6 Experiments
To estimate the co-correlation more efficiently and in parallel, we employ the Power Iteration
algorithm [ 10] in conjunction with Functorch [17]. The corresponding pseudo-code is presented in
the Appendix E.
We verify our proposed theorem on linear and MLP models on the MNIST dataset [ 22]. The width of
hidden layers varied from 24to213, with weights initialized via a Gaussian distribution N(0,1
m1+2q)
where qwas set to values ranging from 0.25to−0.15. We train both the linear and MLP for 50
epochs with a batch size of 512 using the SGD optimizer with a learning rate of 0.003. In addition,
we also conduct the experiment on more complex ResNet50 [ 16] and WRN50 [ 38]. For both models,
we opted for default random weight initialization and used the Adam optimizer with a learning rate
of0.0005 on CIFAR10 [ 20]. To calculate the co-correlation and other statistics, we cover the entire
testset. We consider using L2Auto-attack [ 5] with ϵ= 0.5for all MLPs we trained. The experiments
were executed on a Nvidia RTX3090 GPU, using Python 3.9.7 and PyTorch 1.9.1. The code for the
experiment is available at https://github.com/squarewang2077/co-correlation .
6.1 Empirical Evidence for Proposed Theorem
Figure 1 presents a comparison between the robust accuracy and the Dirichlet energy S(f)across all
trained MLPs. As observed in Figure 1a and 1b, models with lower levels of Dirichlet energy S(f)
tend to exhibit higher robust accuracy, suggesting that Dirichlet energy is an effective representation
of adversarial robustness. Another noteworthy finding is that wider neural networks, with the same
level of weight initialization, demonstrate improved adversarial robustness.
Figure 1 also depicts the dynamic behaviour of shallow neural networks with a weight initialization
parameter of q= 0.25. As is shown in Figure 1c and 1d, the co-correlation ϱincreases throughout
training. Except for narrow widths like 24and25, the majority of networks demonstrate an upward
trend. This trend, however, flattens for non-linear models, suggesting potentially stronger adversarial
robustness due to the non-linearity of the activation function introduced in MLPs.
Figure 2 shows the dynamics of the co-correlation on ResNet50 and Wide-ResNet50. Both networks
are trained on CIFAR10 using the Adam optimizer. We divide them by the pattern of A1-A2 and B1-
B2, as shown in Figure 2a. The co-correlation outcomes for these divisions are displayed in Figure 2b
and Figure 2c, it shows that even with the Adam optimizer, without specific weight initialization
considerations, there is a noticeable rise in co-correlation.
6.2 The Impact of Width and Weight Initialization
Figure 3 illustrates the co-correlation dynamics under varying qfor both linear and MLPs with widths
of 32, 512, 2,048, and 8,192. The figure highlights that our proposed theorems’ assumption of q >0
is quite tight, as all trajectories with q <0remain flat throughout training. We can also observe that
the speed of accumulation significantly increases with larger network widths.
Figure 4 displays the accuracy on testset and co-correlation for both linear and MLPs as heat-maps.
Each cell in the heat-map represents a trained network. From Figure 4c and Figure 4d, we observe that
the best performance and robustness are shown by the MLPs with the largest widths (width = 8192 )
and the smallest weight initializations ( q=−0.15). And when we alter the weight initialization to
9(a) Linear Model/Acc.
 (b) Linear Model/ ϱ
 (c) MLP ReLU/Acc.
 (d) MLP ReLU/ ϱ
Figure 4: Accuracy and Co-correlation under different initialization and width. Figure 4a and 4b
show the heat map for linear model, and Figure 4c and 4d is for MLP ReLU.
control the co-correlation, making it increase from 0.25to0.83, the accuracy declines accordingly
from 0.93to0.68. On the contrary, for another extreme case of models with a width of 16, enhanced
performance is accompanied by increased co-correlation. Consequently, an interesting conclusion
can be drawn about the diverse behaviour of neural networks with small and larger width. Gradient
descent tends to enhance the training of neural networks with smaller width by fostering co-correlation
among layers, which is intrinsically brittle. However, wide networks are trained with less reliance on
interlayer correlation, resulting in inherently more robust models.
7 Conclusion and Limitation
Our work investigates the implicit bias of gradient descent toward adversarial robustness from
the perspective of collaboration between layers. By adapting Dirichlet energy to estimate the
adversarial robustness of neural networks’ individual components, we characterized the collaboration
behaviour between consecutive layers and identified two fundamental properties for dynamics of
the co-correlation. The first property shows that the co-correlation for MLPs will build up during
gradient descent under mild assumptions for weight initialization. The second property shows that
the speed of accumulation for co-correlation is inversely related to the operator norm of Jacobian for
the corresponding sub-modules. In addition, we observed that networks with small widths tend to
foster co-correlation among layers to improve performance, whereas wide networks’ performance
improvement does not heavily rely on establishing such co-correlation. Future research can expand
upon this by examining the effects of increased network depth and more sophisticated structures on
the observed phenomena.
Limitation Our work can be easily extended to multi-layer neural networks since we only assume
that the inputs are bounded by the L2-norm. However, like many theoretical studies, extending our
approach to more complex models is challenging. It remains unknown whether complex models
exhibit the same behaviors.
References
[1]R. Bhattacharjee, S. Jha, and K. Chaudhuri. Sample complexity of robust linear classification
on separated data. In International Conference on Machine Learning , pages 884–893. PMLR,
2021.
[2]S. Bubeck, Y . Cherapanamjeri, G. Gidel, and R. Tachet des Combes. A single gradient step finds
adversarial examples on random two-layers neural networks. Advances in Neural Information
Processing Systems , 34:10081–10091, 2021.
[3]L. Chizat and F. Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In Conference on Learning Theory , pages 1305–1338. PMLR,
2020.
[4]I. Cohen, Y . Huang, J. Chen, J. Benesty, J. Benesty, J. Chen, Y . Huang, and I. Cohen. Pearson
correlation coefficient. Noise reduction in speech processing , pages 1–4, 2009.
10[5]F. Croce and M. Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks. In International conference on machine learning , pages 2206–2216.
PMLR, 2020.
[6]A. Daniely and H. Shacham. Most relu networks suffer from l2adversarial perturbations.
Advances in Neural Information Processing Systems , 33:6629–6636, 2020.
[7]E. Dobriban, H. Hassani, D. Hong, and A. Robey. Provable tradeoffs in adversarially robust
classification. arXiv preprint arXiv:2006.05161 , 2020.
[8]E. Dohmatob and A. Bietti. On the (non-) robustness of two-layer neural networks in different
learning regimes. arXiv preprint arXiv:2203.11864 , 2022.
[9]S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-
parameterized neural networks. arXiv preprint arXiv:1810.02054 , 2018.
[10] J. F. Epperson. An introduction to numerical methods and analysis . John Wiley & Sons, 2021.
[11] L. C. Evans. Partial differential equations , volume 19. American Mathematical Society, 2022.
[12] S. Frei, G. Vardi, P. L. Bartlett, N. Srebro, and W. Hu. Implicit bias in leaky relu networks
trained on high-dimensional data. arXiv preprint arXiv:2210.07082 , 2022.
[13] S. Frei, G. Vardi, P. L. Bartlett, and N. Srebro. The double-edged sword of implicit bias:
Generalization vs. robustness in relu networks. arXiv preprint arXiv:2303.01456 , 2023.
[14] J. Gilmer, L. Metz, F. Faghri, S. S. Schoenholz, M. Raghu, M. Wattenberg, and I. Goodfellow.
Adversarial spheres. arXiv preprint arXiv:1801.02774 , 2018.
[15] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572 , 2014.
[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770–
778, 2016.
[17] R. Z. Horace He. functorch: Jax-like composable function transforms for pytorch. https:
//github.com/pytorch/functorch , 2021.
[18] H. Huang, Y . Wang, S. Erfani, Q. Gu, J. Bailey, and X. Ma. Exploring architectural ingredients
of adversarially robust deep neural networks. Advances in Neural Information Processing
Systems , 34:5545–5559, 2021.
[19] Z. Ji and M. Telgarsky. Directional convergence and alignment in deep learning. Advances in
Neural Information Processing Systems , 33:17176–17186, 2020.
[20] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University
of Tront , 2009.
[21] D. Kunin, A. Yamamura, C. Ma, and S. Ganguli. The asymmetric maximum margin bias of
quasi-homogeneous neural networks. arXiv preprint arXiv:2210.03820 , 2022.
[22] Y . LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist , 2, 2010.
[23] K. Lyu and J. Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890 , 2019.
[24] K. Lyu, Z. Li, R. Wang, and S. Arora. Gradient descent on two-layer nets: Margin maximization
and simplicity bias. Advances in Neural Information Processing Systems , 34:12978–12991,
2021.
[25] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017.
11[26] S. Mahloujifar, D. I. Diochnos, and M. Mahmoody. The curse of concentration in robust
learning: Evasion and poisoning attacks from concentration of measure. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 33, pages 4536–4543. Association for the
Advancement of Artificial Intelligence (AAAI), 2019.
[27] Y . Min, L. Chen, and A. Karbasi. The curious case of adversarially robust models: More data
can help, double descend, or hurt generalization. In Uncertainty in Artificial Intelligence , pages
129–139. PMLR, 2021.
[28] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. In 2016 IEEE symposium on security and privacy
(SP), pages 582–597. IEEE, 2016.
[29] R. Sarussi, A. Brutzkus, and A. Globerson. Towards understanding learning in neural networks
with linear teachers. In International Conference on Machine Learning , pages 9313–9322.
PMLR, 2021.
[30] L. Schmidt, S. Santurkar, D. Tsipras, K. Talwar, and A. Madry. Adversarially robust generaliza-
tion requires more data. Advances in neural information processing systems , 31, 2018.
[31] A. Shafahi, W. R. Huang, C. Studer, S. Feizi, and T. Goldstein. Are adversarial examples
inevitable? arXiv preprint arXiv:1809.02104 , 2018.
[32] C.-J. Simon-Gabriel, Y . Ollivier, L. Bottou, B. Schölkopf, and D. Lopez-Paz. First-order
adversarial vulnerability of neural networks and input dimension. In International conference
on machine learning , pages 5809–5817. PMLR, 2019.
[33] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry. Robustness may be at odds
with accuracy. arXiv preprint arXiv:1805.12152 , 2018.
[34] F. Wang, P. Xu, W. Ruan, and X. Huang. Towards verifying the geometric robustness of
large-scale neural networks. arXiv preprint arXiv:2301.12456 , 2023.
[35] Z. Wang and W. Ruan. Understanding adversarial robustness of vision transformers via cauchy
problem. In Machine Learning and Knowledge Discovery in Databases: European Conference,
ECML PKDD 2022, Grenoble, France, September 19–23, 2022, Proceedings, Part III , pages
562–577. Springer, 2023.
[36] X. Yin and W. Ruan. Boosting adversarial training via fisher-rao norm-based regularization. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
24544–24553, 2024.
[37] X. Yin, W. Ruan, and J. Fieldsend. Dimba: discretely masked black-box attack in single object
tracking. Machine Learning , 113(4):1705–1723, 2024.
[38] S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,
2016.
[39] H. Zhang, Y . Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan. Theoretically principled
trade-off between robustness and accuracy. In International conference on machine learning ,
pages 7472–7482. PMLR, 2019.
[40] T. Zhang, W. Ruan, and J. E. Fieldsend. Proa: A probabilistic robustness assessment against
functional perturbations. In Joint European Conference on Machine Learning and Knowledge
Discovery in Databases , pages 154–170. Springer, 2022.
[41] Z. Zhu, F. Liu, G. Chrysos, and V . Cevher. Robustness in deep learning: The good (width), the
bad (depth), and the ugly (initialization). Advances in Neural Information Processing Systems ,
35:36094–36107, 2022.
12A The Proof for Theorem 4.1
Suppose that the loss function Lis differentiable w.r.t. f. Given data points (x, y)∼Pandx∼Px,
we have
Rrob(f, r) = E
(x,y)∼Psup
ε∈BrL(f(x+ε), y)) (22)
=E
(x,y)∼P
sup
ε∈BrL(f(x), y)) + sup
ε∈BrL(f(x+ε), y))−sup
ε∈BrL(f(x, y))
(23)
≤E
(x,y)∼P[L(f(x), y))] +s
E
(x,y)∼P
sup
ε∈Br(L(f(x+ε), y))−L(f(x, y)))2
(24)
≈E
(x,y)∼P[L(f(x), y))] +s
E
(x,y)∼P
sup
ε∈Br(∇fLT·Jf(x)ε)2
(25)
=E
(x,y)∼P[L(f(x), y))] + rr
E
(x,y)∼P∥∇fLT·Jf(x)∥2
2, (26)
where∇fLis the gradients of Lw.r.t. f,∇fLTindicates that it is a row vector. Jf(x)is the Jacobian
matrix for classifier f.
B The Proof of Theorem 5.3
Before formally proving the theorem, we provide some lemmas which are useful for our proof.
Lemma B.1 (Dynamic of Weights for Linear Model) .Given gradient descent to optimize the weights,
the dynamic of the weights at step tis
˙W(t) =a⊗exT(t), (27)
Proof. Given the step size η, the update for rthrow of the weight matrix Wis
∆wr=−η
nnX
i=1∂l
∂ui∂f(W,xi)
∂wr(28)
=ηar1
nnX
i=1 
yi−sig(ui)
xi, (29)
where sig(·)denotes the sigmoid function. Hence, for weight matrix W, we have
∆W=
∆wT
1...
∆wT
m
 (30)
=η
a11
nPn
i=1 
yi−sig(ui)
xT
i...
am1
nPn
i=1 
yi−sig(ui)
xT
i
. (31)
After replacing with
ex=1
nnX
i=1 
yi−sig(ui)
xi, (32)
13we have
∆W=η
a1exT
...
amexT
 (33)
=ηa⊗exT. (34)
Since∥xi∥2= 1, we also have ∥ex∥2≤1. Therefore, we can say that the dynamics of weights, i.e.
˙W(t), is
˙W(t) =a⊗exT(t). (35)
Lemma B.2 (The Concentration for L2Norm of Gaussian r.v.) .LetImbe identical matrix of size
m×m. Given nGaussian random vectors z1,···,znsuch that zii.i.d.∼N(0,1
m1+2qIm), q >
0,∀i∈[n]. With probability at least 1−δ, we have several conclusions:
1. Average of the norm
1
nnX
i=1∥zi∥2=1
mq+Or
8 log(2 n/δ)
m1+2q
. (36)
2. Average of the square norm
1
nnX
i=1∥zi∥2
2=1
m2q+O
max8 log(2 /δ)
m1+2qn,r
8 log(2 /δ)
m1+4qn
. (37)
3. Square root of the average of the square norm
vuut1
nnX
i=1∥zi∥2
2=1
mq+Or
8 log(2 /δ)
m1+2qn
. (38)
Proof. We first prove the concentration property for the average of the norm. Since√
m1+2qzi,ri.i.d.∼
N(0,1), i∈[n], r∈[m], we have that m1+2qz2
i,r∼χ2(1). Denote the sub-Exponential distribution
asSE(ν2, α), we have m1+2qz2
i,r−1∈SE(4,4). Hence ∀i∈[n],
P1
mmX
r=1 √
m1+2qzi,r2−1≥ϵ
≤

2 exp
−mϵ2
8
ϵ∈(0,1)
2 exp
−mϵ
8
ϵ≥1. (39)
Therefore, we have ∀ϵ >0
P1√m∥√
m1+2qzi∥2−1≥ϵ
≤P1
m∥√
m1+2qzi∥2
2−1≥max{ϵ, ϵ2}
(40)
≤2 exp
−mϵ2
8
. (41)
Equation (40) is because of the fact that given c >0fixed,∀x >0we have
|x−1| ≥c⇒ |x2−1| ≥max{c, c2} (42)
14Hence,
P1
nnX
i=1∥mqzi∥2−1≥ϵ
=P1
nnX
i=11√m∥√
m1+2qzi∥2−1≥ϵ
(43)
≤PnX
i=11√m∥√
m1+2qzi∥2−1≥nϵ
(44)
≤nX
i=1P1√m∥√
m1+2qzi∥2−1≥ϵ
(45)
≤2nexp
−mϵ2
8
(46)
Equivalently, with probability at least 1−δ,
1
nnX
i=1mq∥zi∥2−1≤r
8
mlog2n
δ(47)
⇒1
nnX
i=1∥zi∥2−1
mq≤1
mqr
8
mlog2n
δ(48)
To prove the second convergence, we starts from the fact that
1
nnX
i=1∥zi∥2
2=1
nnX
i=1mX
r=1z2
i,r=1
m2q1
nmnX
i=1mX
r=1 √
m1+2qzi,r2. (49)
Since √
m1+2qzi,r2i.i.d.∼χ2(1),∀i∈[n], r∈[m], similar to Equation (39), we have
P1
nmnX
i=1mX
r=1 √
m1+2qzi,r2−1≥ϵ
≤

2 exp
−nm
8ϵ2
ϵ∈(0,1)
2 exp
−nm
8ϵ
ϵ≥1(50)
Equivalently, with probability at least 1−δ,
1
nnX
i=1m2q∥zi∥2
2−1≤(q
8
mnlog2
δϵ∈(0,1)
8
mnlog2
δϵ≥1(51)
⇒1
nnX
i=1∥zi∥2
2−1
m2q≤1
m2qmax8
mnlog2
δ,r
8
mnlog2
δ
(52)
Now we prove the third concentration, with similar trick of the first concentration, we have
Pvuut1
nmnX
i=1mX
r=1 √
m1+2qzi,r2−1≥ϵ
≤P1
nmnX
i=1mX
r=1 √
m1+2qzi,r2−1≥max{ϵ, ϵ2}
(53)
≤2 exp
−nm
8ϵ2
(54)
Therefore,
Pvuut1
nnX
i=1m2q∥zi∥2
2−1≥ϵ
≤2 exp
−nm
8ϵ2
, (55)
15with probability at least 1−δ, we have
mqvuut1
nnX
i=1∥zi∥2
2−1≤r
8
mnlog2
δ(56)
⇒vuut1
nnX
i=1∥zi∥2
2−1
mq≤1
mqr
8
mnlog2
δ(57)
Now, we prove the theorem 5.3. Given the linear model and training dataset D={(xi, yi)}n
i=1.
Assume that each wi,ji.i.d.∼N(0,1
m1+2q), q > 0andais randomly initialized subject to the constraint
∥a∥2= 1and fixed during training. Hence, with high probability, we have
˙ϱa,W≥ηϱa,W·Pt
τ=1ex(τ)Tex(t)
∥W(t)∥2
2·
1− 
v(t)Ta2
+O1
mq
(58)
Proof. We first show that the derivative of co-correlation is
˙ϱa,W(t) =d
dtϱa,W(t) (59)
=d
dtEx[∥aTW(t)∥2
2]1
2
Ex[∥W(t)∥2
2]1
2(60)
=d
dt∥aTW(t)∥2
∥W(t)∥2(61)
=ϱa,W(t)
2d∥aTW(t)∥2
2/dt
∥aTW(t)∥2
2−d∥W(t)∥2
2/dt
∥W(t)∥2
2
(62)
≥ϱa,W(t)
2∥W(t)∥2
2d∥aTW(t)∥2
2
dt−d∥W(t)∥2
2
dt
, (63)
by the law of derivatives for inner product of matrices and the eigenvalue of matrix W(t)W(t)T, we
have
d∥aTW(t)∥2
2
dt=aT(˙W(t)W(t)T+W(t)˙W(t)T)a (64)
d∥W(t)∥2
2
dt=v(t)T(˙W(t)W(t)T+W(t)˙W(t)T)v(t), (65)
where v(t)is the dominant eigenvector for W(t)W(t)T. Equation (60) is because JW(x) =Wand
∥a∥2= 1by assumption. Since Equation (60) do not depend on x, we can safely drop the Expectation
Exas is shown in Equation (61). Equation (63) is because ∥aTW∥2
2≤ ∥a∥2
2∥W∥2
2=∥W∥2
2. By
Lemma B.1, the weight matrix W(t)at training step tcan be approximated as
W(t) =W(0) +tX
τ=1∆W(τ). (66)
16then
˙W(t)W(t)T=a⊗exT(t)
W(0)T+tX
τ=1∆W(τ)T
(67)
=a⊗exT(t)
W(0)T+ηtX
τ=1aT⊗ex(τ)
(68)
=a⊗exT(t)
W(0)T+ηaT⊗tX
τ=1ex(τ)
(69)
=a⊗exT(t)W(0)T+η
a⊗exT(t)
aT⊗tX
τ=1ex(τ)
(70)
=aexT(t)W(0)T+ηtX
τ=1ex(τ)Tex(t)
aaT, (71)
similarity,
W(t)˙W(t)T=
W(0) +tX
τ=1∆W(τ)
aT⊗ex(t) (72)
=
W(0) + ηa⊗tX
τ=1exT(τ)
aT⊗ex(t) (73)
=W(0)ex(t)aT+ηtX
τ=1ex(τ)Tex(t)
aaT. (74)
Hence
1
2d∥aTW(t)∥2
2
dt−d∥W(t)∥2
2
dt
(75)
=η
1− 
aTv(t)2 tX
τ=1ex(τ)Tex(t)
| {z }
1+aTW(0)ex(t)aTa−v(t)TW(0)ex(t)aTv(t)| {z }
2(76)
1is the main part of our theorem. And for 2, it can be bounded as
|2|= 
a−aTv(t)v(t)TW(0)ex(t) (77)
≤1
nnX
i=1 
yi−sig(ui) 
a−aTv(t)v(t)TW(0)xi (78)
≤1
nnX
i=1|yi−sig(ui)|q
1−(aTv(t))2
| {z }
0≤...≤1∥W(0)xi∥2 (79)
≤1
nnX
i=1∥W(0)xi∥2 (80)
Now since wi,j∼N(0,1
m1+2q), q > 0and each wi,jis independent with each other, for given
xi, i= 1, ..., n , we have
W(0)xi=
w1(0)Txi
...
wm(0)Txi
∼N
0,∥xi∥2
2
m1+2qIm
, (81)
17where ∥xi∥2
2= 1by assumption. By the lemma B.2, with high probability,
1
nnX
i=1∥W(0)xi∥2=1
mq+Or
8 log(2 n/δ)
m1+2q
. (82)
Therefore,
|2|=O1
mq
(83)
with high probability, and it comes to our conclusion.
C The Proof of Theorem 5.5
To prove the Theorem 5.5, the following lemma is crucial.
Lemma C.1 (Dynamic of Weights for A Nonlinear Model for Initial Steps) .Given gradient descent
to optimize the weights, the dynamic of the weights at the initial step tfor the non-linear model is
˙W(t) =
a1exT
1(t)
...
amexT
m(t)
, (84)
where
exr(t) =ar1
nnX
i=1 
yi−sig(ui(t))
σ′(wT
r(t)xi)xi, r∈[m]. (85)
Proof. Similar to the linear case, the update for the rthrow for the weight matrix is
∆wr=−η
nnX
i=1∂l
∂ui∂f(W,xi)
∂wr(86)
=ηar1
nnX
i=1 
yi−sig(ui)
σ′(wT
rxi)xi. (87)
Hence that the update of the weight matrix W(t)is
∆W=
∆wT
1...
∆wT
m
 (88)
=η
a11
nPn
i=1 
yi−sig(ui)
σ′(wT
1xi)xT
i...
am1
nPn
i=1 
yi−sig(ui)
σ′(wT
mxi)xT
i
 (89)
=η
a1exT
1...
amexT
m
. (90)
Hence our conclusion.
Lemma C.2 (Concentration for Weighted Sum and Square Root Scalar for Bounded Variables) .
Suppose Xr−µ∈[l, u],∀i∈[m]is independent identically distributed r.v. with mean 0. Given a
vector v∈Rm,∥v∥2= 1, andasuch that ar∈ {− 1,1}, r∈[m]we have concentration inequality
P1√mmX
r=1arvrXr−1√maTvµ≥ϵ
≤2 exp
−mϵ2
2(u−l)2
. (91)
18Proof. We first show the moment generation function of the r.v. we would like to estimate.
E
exp
t1√mmX
r=1arvrXr−1√maTvµ
≤expt2
2m(u−l)2
. (92)
Now, we prove this inequality.
E
exp
t1√mmX
r=1arvrXr−1√maTvµ
=E
expt√mmX
r=1arvrXr−mX
r=1arvrµ
(93)
=mY
r=1E
exptarvr√m
Xr−µ
(94)
≤mY
r=1expv2
rt2
2m(u−l)2
(95)
= expt2
2m(u−l)2mX
r=1v2
r
(96)
= expt2
2m(u−l)2
, (97)
where t≥0. Hence by Chernoff’s bound, we have that
P1√mmX
r=1arvrXr−1√maTvµ≥ϵ
≤2 inf
t≥0expt2
2m(u−l)2	
exp{tϵ}(98)
= 2 exp
−mϵ2
2(u−l)2
(99)
Here, we prove the theorem 5.5. Given a shallow neural network, with high probability we have
˙ϱa,σ◦W(t) =ηϱa,σ◦W(t)
Ex∥D(t)W(t)∥2
2tX
τ=1(1−aTv(τ)aTv(t))Exex∗(τ)Tex∗(t)
(100)
+ max
O1√m
,O1
mq
(101)
Proof. Because of activation function depends on the input x, we cannot drop the expectation Ex.
Hence, we have
˙ϱa,W(t) =d
dtϱa,W(t) (102)
=d
dtEx[∥aTD(t)W(t)∥2
2]1
2
Ex[∥D(t)W(t)∥2
2]1
2(103)
=ϱa,W(t)
2Exd∥aTD(t)W(t)∥2
2/dt
Ex∥aTD(t)W(t)∥2
2−Exd∥D(t)W(t)∥2
2/dt
Ex∥D(t)W(t)∥2
2
(104)
≥ϱa,W(t)
2Ex∥D(t)W(t)∥2
2Exd∥aTD(t)W(t)∥2
2
dt−d∥D(t)W(t)∥2
2
dt
, (105)
where D(t) = diag(σ′(wT
1x), ..., σ′(wT
mx)). And for ReLu activation function, σ′(wT
mx) =
1{wTmx≥0}. We assume that the activation does not change at an infinitely small change of t, implying
˙D(t) =0. Hence, we have
19d∥aTD(t)W(t)∥2
2
dt=aT
˙D(t)W(t)W(t)TD(t) +D(t)W(t)W(t)T˙D(t)
+D(t)˙W(t)W(t)TD(t) +D(t)W(t)˙W(t)TD(t)
a
=aT
D(t)˙W(t)W(t)TD(t) +D(t)W(t)˙W(t)TD(t)
a
= 2aTD(t)ηtX
τ=1
a1exT
1(τ)
...
amexT
m(τ)
(a1ex1(t)··· amexm(t))D(t)a
| {z }
1
+ 2aTD(t)
a1exT
1(t)
...
amexT
m(t)
W(0)TD(t)a
| {z }
2.
And similarly,
d∥D(t)W(t)∥2
2
dt=vT(t)
D(t)˙W(t)W(t)TD(t) +D(t)W(t)˙W(t)TD(t)
v(t)
= 2vT(t)D(t)ηtX
τ=1
a1exT
1(τ)
...
amexT
m(τ)
(a1ex1(t)··· amexm(t))D(t)v(t)
| {z }
1′
+ 2vT(t)D(t)
a1exT
1(t)
...
amexT
m(t)
W(0)TD(t)v(t)
| {z }
2′.
1can be
1=ηtX
τ=1mX
r=1a2
rσ′(wr(τ)Tx)exT
r(τ) mX
r=1a2
rσ′(wr(t)Tx)exr(t)
(106)
=ηtX
τ=11
nnX
i=1mX
r=1h
a2
rσ′(wr(τ)Tx)σ′(wr(τ)Txi)i
(y−sig(ui))xT
i
(107)
·1
nnX
i=1mX
r=1h
a2
rσ′(wr(t)Tx)σ′(wr(t)Txi)i
(y−sig(ui))xi
. (108)
Since ar∈ {−1√m,1√m}and the derivative activation function σ′is bounded by Mby assumption,
we have that
PW(0)mX
r=1a2
rσ′(wr(t)Tx)σ′(wr(t)Txi)−αi(t;x)≥ϵ
(109)
=PW(0)1
mmX
r=1σ′(wr(t)Tx)σ′(wr(t)Txi)−αi(t;x)≥ϵ
≤2e−mϵ2
2M4, (110)
20where
αi(t,x) =EW(0)h
σ′(w(t)Tx)σ′(w(t)Txi)i
. (111)
Equivalently, with probability at least 1−δ, we have
mX
r=1a2
rσ′(wr(t)Tx)σ′(wr(t)Txi)−αi(t;x)≤r
2M4
mlog2
δ(112)
Let
ex∗(t)△=1
nnX
i=1αi(t,x) 
yi−sig(ui(t))
xi (113)
ex(t)△=1
nnX
i=1 
yi−sig(ui(t))
xi (114)
Hence, with probability at least 1−δ,
1=ηtX
τ=11
nnX
i=1αi(τ,x) 
yi−sig(ui(τ))
xT
i+Or
1
mlog2
δ
ex(τ)T
(115)
·1
nnX
i=1αi(t,x) 
yi−sig(ui(t))
xi+Or
1
mlog2
δ
ex(t)
(116)
=ηtX
τ=1
ex∗(τ)T+Or
1
mlog2
δ
ex(τ)T
·
ex∗(t) +Or
1
mlog2
δ
ex(t)
(117)
=ηtX
τ=1ex∗(τ)Tex∗(t) +Or
1
mlog2
δ
ex(τ)Tex∗(t) (118)
+Or
1
mlog2
δ
ex∗(τ)Tex(t) +O1
mlog2
δ
ex(t)Tex(t). (119)
Since
∥ex∗(t)∥2≤1
n
α1(τ,x) 
y1−sig(u1(t))
...
αn(τ,x) 
yn−sig(un(t))

2
xT
1...
xT
n

2≤M2(120)
∥ex(t)∥2≤1
n
y1−sig(u1(t))
...
yn−sig(un(t))

2
xT
1...
xT
n

2≤1, (121)
we have that for at least 1−δ,
1=ηtX
τ=1ex∗(τ)Tex∗(t) +Or
1
mlog2
δ
(122)
Similarly,
1′=ηtX
τ=11
nnX
i=1mX
r=1h
vr(t)arσ′(wr(τ)Tx)σ′(wr(τ)Txi)i
(yi−sig(ui))xT
i
(123)
·1
nnX
i=1mX
r=1h
vr(t)arσ′(wr(t)Tx)σ′(wr(t)Txi)i
(yi−sig(ui))xi
, (124)
21Leta∗=√ma∈ {− 1,1}mand by the conclusion of lemma C.2,
PW(0)mX
r=1vr(t)arσ′(wr(t)Tx)σ′(wr(t)Txi)−mX
r=1vr(t)arαi(t,x)≥ϵ
(125)
=PW(0)1√mmX
r=1a∗
rvr(t)σ′(wr(t)Tx)σ′(wr(t)Txi)−aTv(t)αi(t;x)≥ϵ
(126)
≤2 exp
−mϵ2
2M4
. (127)
Equivalently, with probability at least 1−δ,
1√mmX
r=1a∗
rvr(t)σ′(wr(t)Tx)σ′(wr(t)Txi)−aTv(t)αi(t;x)≤r
2M4
mlog2
δ. (128)
Hence,
1′=ηtX
τ=11
nnX
i=1aTv(τ)αi(τ,x) 
yi−sig(ui(τ))
xT
i+Or
1
mlog2
δ
ex(τ)T
(129)
·1
nnX
i=1aTv(t)αi(t,x) 
yi−sig(ui(t))
xi+Or
1
mlog2
δ
ex(t)
(130)
=ηtX
τ=1
aTv(τ)ex∗(τ)T+Or
1
mlog2
δ
ex(τ)T
·
aTv(t)ex∗(t) +Or
1
mlog2
δ
ex(t)
(131)
=ηtX
τ=1aTv(τ)aTv(t)ex∗(τ)Tex∗(t) (132)
+Or
1
mlog2
δ
ex(τ)Tex∗(t) +Or
1
mlog2
δ
ex∗(τ)Tex(t) +O1
mlog2
δ
ex(t)Tex(t).
(133)
We have that for at least 1−δ,
1′=ηtX
τ=1aTv(τ)aTv(t)ex∗(τ)Tex∗(t) +Or
1
mlog2
δ
(134)
For 2, we have
|2|=aTD(t)
a1exT
1(t)
...
amexT
m(t)
W(0)TD(t)a (135)
≤ ∥aTD(t)∥2
2(a1W(0)ex1(t)··· amW(0)exm(t))
2(136)
≤M2(a1W(0)ex1(t)··· amW(0)exm(t))
F(137)
≤M2vuutmX
r=1a2r∥W(0)exr∥2
2, (138)
22where
∥W(0)exr∥2
2=1
nnX
i=1σ′(wr(0)Txi)(yi−sig(ui))W(0)xi2
2(139)
≤1
n2
σ′(wr(0)Tx1)(y1−sig(u1))
...
σ′(wr(0)Txn)(yn−sig(un))
2
2(W(0)x1··· W(0)xn)2
2(140)
≤1
n2nX
i=1σ′(wr(0)Txi)2(yi−sig(ui))2
| {z }
0≤...≤M2(W(0)x1··· W(0)xn)2
F(141)
≤M2
nnX
i=1∥W(0)xi∥2
2 (142)
Similar to the proof of the linear case, and accords to Lemma B.2, we have that with probability at
least1−δ,
vuut1
nnX
i=1∥W(0)xi∥2
2=1
mq+Or
8 log(2 /δ)
m1+2qn
. (143)
Therefore, with high probability,
|2| ≤M4vuutmX
r=1a2r1
nnX
i=1∥W(0)xi∥2
2 (144)
=M4vuut1
nnX
i=1∥W(0)xi∥2
2 (145)
=O1
mq
(146)
Follow the exact same procedure. With high probability, we have
|2′|=vT(t)D(t)
a1exT
1(t)
...
amexT
m(t)
W(0)TD(t)v(t) (147)
≤ ∥v(t)TD(t)∥2
2(a1W(0)ex1(t)··· amW(0)exm(t))
2(148)
≤M2vuutmX
r=1a2r∥W(0)exr∥2
2 (149)
=O1
mq
(150)
We put all the information together, with high probability, we have
˙ϱa,σ◦W(t)≥ϱa,σ◦W(t)
Ex∥D(t)W(t)∥2
2Ex
1−1′+2−2′
(151)
≥ϱa,σ◦W(t)
Ex∥D(t)W(t)∥2
2
ηtX
τ=1(1−aTv(τ)aTv(t))Exex∗(τ)Tex∗(t)
(152)
+ max
O1
m1
2
,O1
mq
(153)
230 10 20 30 40 50 
Epoch0.0050.0100.0150.0200.0250.030
std/µ
width-16 
width-32 
width-64 
width-128 
width-256 
width-512 
width-1024 
width-2048 
width-4096 
width-8192(a) Relative Std
0 10 20 30 40 50 
Epoch0.999650.999720.999790.999860.999931.00000
⍴ (b) Linear Correlation
Figure 5: Linear Correlation &Relative Std. The linear correlation and the standard deviation over
the mean are given for all MLPs with the initialization parameter q= 0.25.
4 5 6 7 8 9 10 11 12 13 
width(2
n
)0.00.30.6
tot 
Δϱ
a
,W
q=-0.15 
q=-0.1 
q=-0.05 
q=0 
q=0.05 
q=0.15 
q=0.254 5 6 7 8 9 10 11 12 13 
width0.00.30.6
tot 
Δϱ
a
,
σ
○
W
Figure 6: The accumulation of co-correlation under different width. X-axis denotes the width of
the neural network, and the line in different color shows the result under different setting of weight
initialization. The lower plot is the result for linear model and the result for shallow ReLU is at the
top.
D Extra Experiment Results
Figure 5 summarizes the statistics of linear correlation and relative variability, illustrating that
co-correlation is the dominant factor in the decomposition outlined in Theorem 4.5.
Figure 6 shows the accumulated change in co-correlation during training across varying network
widths. Each line, represented by different colours, corresponds to a unique weight initialization
setting of q. A clear inference from the figure is the positive relationship between the accumulated
co-correlation change and network width under the same q—the wider the network, the greater the
accumulated co-correlation change. Moreover, as we shift the weight initialization setting from
q=−0.15toq= 0.25, the accumulation of co-correlation also increases. These observations align
with our Theorem 5.3 and 5.5, indicating that the increase in co-correlation is inversely proportional
to∥W(t)∥2.
24E Algorithm
We use the Power Iteration algorithm [10] along with Functorch [17] to compute co-correlation and
other relevant statistics. This combination assists in approximating the L2-norm of the Jacobian
for the layers under consideration. The algorithm to estimate the related statistics including co-
correlation are outlined in Algorithm 1. The power iteration used to obtain the L2-norm of the
Jacobian is summarized in Algorithm 2.
Algorithm 1 Estimation of Related Statistics
Input : Input dataset and modules ϕ, φ for the given blocks in sequential {xi, i= 1, .., N}
Output : Approximation of co-correlations bϱϕ,φ, linear correlation bρϕ,φ, mean bµϕ,bµφand variance
σϕ, σφof the Jacobian of the module w.r.t. their inputs
1:Computation of Jacobin of ∥Jφ(xi)∥2,∥Jϕ(φ(xi))∥2for{xi, i∈[n]}w.r.t. their inputs
2:bϱϕ,φ←1
NPN
i=1∥Jϕ(φ(xi))·Jφ(xi)∥2
1
NPN
i=1∥Jϕ(φ(xi))∥2∥Jφ(xi)∥2
3:bρϕ,φ←1
NPN
i=1∥Jϕ(φ(xi))∥2∥Jφ(xi)∥2 
1
NPN
i=1∥Jϕ(φ(xi))∥2
21
2 
1
NPN
i=1∥Jφ(xi)∥2
21
2
4:bµϕ←1
NPN
i=1∥Jϕ(φ(xi))∥2∥Jφ(xi)∥2
5:bσϕ←1
N−1PN
i=1(∥Jϕ(φ(xi))∥2∥Jφ(xi)∥2−ˆµϕ)21
2
Algorithm 2 Power Iteration Based Computation
Input : Input dataset and module for the given sequential blocks {xi, i= 1, .., N}, ϕ, φ
Output : Approximation of co-correlations ϱϕ,φ, linear correlation ρϕ,φ, mean µϕ, µφand variance
σϕ, σφof the Jacobian of the module w.r.t. their inputs
1:COMPUTATION OF ∥Jφ(xi)∥2FOR{xi, i∈[n]}
2:while i≤Nbdo
3: sample randomly u∼U([0,1])
4:u←u/∥u∥2
5: generate jacobian-vector product function jvp(·;xi)
6: generating vector-jacobian product function vjp(·;xi)
7:∥Jφ(xi)∥old
2←None
8: while err≥10−6do
9: v←jvp(u;xi)
10: u←vjp(v;xi)
11: u←u/∥u∥2
12: v←v/∥v∥2
13: ∥Jφ(xi)∥new
2←u/∥u∥2
v/∥v∥2
14: if∥Jφ(xi)∥old
2̸=None then
15: err=∥Jφ(xi)∥new
2−∥Jφ(xi)∥old
2
∥Jφ(xi)∥old
2
16: end if
17: ∥Jφ(xi)∥old
2=∥Jφ(xi)∥new
2
18: end while
19:end while
25NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction have clearly stated the claims made, including
the contributions made in the paper.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: A separate section "Discussion and Limitations" has been designed and can be
seen in the end of this paper.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
26Answer: [Yes]
Justification: The complete proofs are provided in the appendix. Theorems and Lemmas are
properly referenced in this paper.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The algorithm and detailed experiment settings are provided.
Guidelines: The code is provided in the supplemental material.
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
27Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code is uploaded in github, including detailed README file.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All details can be checked in Experiment section.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The detailed methods can be found in Experiment section.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
28• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The paper introduces the configuration of the computer in Experiment section.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have read the NeurIPS Code of Ethics and confirm that the research
conforms in every respect.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: There is no societal impact of the work performed. This is a theoretical paper
to understand existing ML methods.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
29•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We have cited the original paper that produced the code package or dataset
properly.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
30•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
31