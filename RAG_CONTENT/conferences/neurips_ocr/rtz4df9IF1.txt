Optimal Parallelization of Boosting
Arthur da Cunha
Department of Computer Science
Aarhus University
dac@cs.au.dkMikael Møller Høgsgaard
Department of Computer Science
Aarhus University
hogsgaard@cs.au.dk
Kasper Green Larsen
Department of Computer Science
Aarhus University
larsen@cs.au.dk
Abstract
Recent works on the parallel complexity of Boosting have established strong lower
bounds on the tradeoff between the number of training rounds pand the total par-
allel work per round t. These works have also presented highly non-trivial parallel
algorithms that shed light on different regions of this tradeoff. Despite these ad-
vancements, a significant gap persists between the theoretical lower bounds and
the performance of these algorithms across much of the tradeoff space. In this
work, we essentially close this gap by providing both improved lower bounds on
the parallel complexity of weak-to-strong learners, and a parallel Boosting algo-
rithm whose performance matches these bounds across the entire pvs.tcompro-
mise spectrum, up to logarithmic factors. Ultimately, this work settles the parallel
complexity of Boosting algorithms that are nearly sample-optimal.
1 Introduction
Boosting is an extremely powerful and elegant idea that allows one to combine multiple inaccu-
rate classifiers into a highly accurate voting classifier . Algorithms such as AdaBoost [Freund and
Schapire, 1997] work by iteratively running a base learning algorithm on reweighted versions of the
training data to produce a sequence of classifiers h1, . . . , h p. After obtaining hi, the weighting of the
training data is updated to put larger weights on samples misclassified by hi, and smaller weights on
samples classified correctly. This effectively forces the next training iteration to focus on points with
which the previous classifiers struggle. After sufficiently many rounds, the classifiers h1, . . . , h pare
finally combined by taking a (weighted) majority vote among their predictions. Many Boosting al-
gorithms have been developed over the years, for example Grove and Schuurmans [1998], Rätsch
et al. [2005], Servedio [2003], Friedman [2001], with modern Gradient Boosting [Friedman, 2001]
algorithms like XGBoost [Chen and Guestrin, 2016] and LightGBM [Ke et al., 2017] often achiev-
ing state-of-the-art performance on learning tasks while requiring little to no data cleaning. See e.g.
the excellent survey by Natekin and Knoll [2013] for more background on Boosting.
While Boosting enjoys many advantages, it does have one severe drawback, also highlighted in
Natekin and Knoll [2013]: Boosting is completely sequential as each of the consecutive training
steps requires the output of previous steps to determine the reweighted learning problem. This
property is shared by all Boosting algorithms and prohibits the use of computationally heavy training
by the base learning algorithm in each iteration. For instance, Gradient Boosting algorithms often
require hundreds to thousands of iterations to achieve the best accuracy. The crucial point is that
even if you have access to thousands of machines for training, there is no way to parallelize the steps
of Boosting and distribute the work among the machines (at least beyond the parallelization possible
38th Conference on Neural Information Processing Systems (NeurIPS 2024).for the base learner). In effect, the training time of the base learning algorithm is directly multiplied
by the number of steps of Boosting.
Multiple recent works [Long and Servedio, 2013, Karbasi and Larsen, 2024, Lyu et al., 2024] have
studied parallelization of Boosting from a theoretical point of view, aiming for an understanding of
the inherent tradeoffs between the number of training rounds pand the total parallel work per round
t. These works include both strong lower bounds on the cost of parallelization and highly non-trivial
parallel Boosting algorithms with provable guarantees on accuracy. Previous studies however leave
a significant gap between the performance of the parallel algorithms and the proven lower bounds.
The main contribution of this work is to close this gap by both developing a parallel algorithm with
a better tradeoff between pandt, as well as proving a stronger lower bound on this tradeoff. To
formally state our improved results and compare them to previous works, we first introduce the
theoretical framework under which parallel Boosting is studied.
Weak-to-Strong Learning. Following the previous works Karbasi and Larsen [2024], Lyu et al.
[2024], we study parallel Boosting in the theoretical setup of weak-to-strong learning. Weak-to-
strong learning was introduced by Kearns [1988], Kearns and Valiant [1994] and has inspired the
development of the first Boosting algorithms [Schapire, 1990]. In this framework, we consider
binary classification over an input domain Xwith an unknown target concept c:X → {− 1,1}
assigning labels to samples. A γ-weak learner for cis then a learning algorithm Wthat for any
distribution DoverX, when given at least some constant m0i.i.d. samples from D, produces with
constant probability a hypothesis hwithLD(h)≤1/2−γ. Here LD(h) = Pr x∼D[h(x)̸=c(x)].
The goal in weak-to-strong learning is then to boost the accuracy of Wby invoking it multiple times.
Concretely, the aim is to produce a strong learner: A learning algorithm that for any distribution D
overXand any 0< δ, ε < 1, when given m(ε, δ)i.i.d. samples from D, produces with probability
at least 1−δa hypothesis h:X → {− 1,1}such that LD(h)≤ε. We refer to m(ε, δ)as the sample
complexity of the weak-to-strong learner.
Weak-to-strong learning has been extensively studied over the years, with many proposed algo-
rithms, among which AdaBoost [Freund and Schapire, 1997] is perhaps the most famous. If H
denotes a hypothesis set such that Walways produces hypotheses from H, and if ddenotes the
VC-dimension of H, then in terms of sample complexity, AdaBoost is known to produce a strong
learner with sample complexity mAda(ε, δ)satisfying
mAda(ε, δ) = O 
dln(d
εγ) ln(1
εγ)
γ2ε+ln(1/δ)
ε!
. (1)
This can be proved by observing that after t= O( γ−2lnm)iterations, AdaBoost produces a voting
classifier f(x) = sign(Pt
i=1αihi(x))with all margins on the training data being Ω(γ). The sample
complexity bound then follows by invoking the best known generalization bounds for large margin
voting classifiers [Breiman, 1999, Gao and Zhou, 2013]. Here the margin of the voting classifier f
on a training sample (x, c(x))is defined as c(x)Pt
i=1αihi(x)/Pt
i=1|αi|. This sample complexity
comes within logarithmic factors of the optimal sample complexity mOPT(ε, δ) = Θ( d/(γ2ε) +
ln(1/δ)/ε)obtained e.g. in Larsen and Ritzert [2022].
Parallel Weak-to-Strong Learning. The recent work by Karbasi and Larsen [2024] formalized
parallel Boosting in the above weak-to-strong learning setup. Observing that all training happens
in the weak learner, they proposed the following definition of parallel Boosting: A weak-to-strong
learning algorithm has parallel complexity (p, t)if for pconsecutive rounds it queries the weak
learner with tdistributions. In each round i, ifDi
1, . . . , Di
tdenotes the distributions queried, the
weak learner returns thypotheses hi
1, . . . , hi
t∈ H such that LDi
j(hi
j)≤1/2−γfor all j. At the
end of the prounds, the weak-to-strong learner outputs a hypothesis f:X → {− 1,1}. The queries
made in each round and the final hypothesis fmust be computable from the training data as well as
all hypotheses hi
jseen in previous rounds. The motivation for the above definition is that we could
let one machine/thread handle each of the tparallel query distributions in a round.
Since parallel weak-to-strong learning is trivial if we make no requirements on LD(f)for the output
f:X → {− 1,1}(simply output f(x) = 1 for all x∈ X), we from hereon focus on parallel weak-
to-strong learners that are near-optimal in terms of the sample complexity and accuracy tradeoff.
2More formally, from the upper bound side, our goal is to obtain a sample complexity matching at
least that of AdaBoost, stated in Eq. (1). That is, rewriting the loss εas a function of the number of
samples m, we aim for output classifiers fsatisfying
LD(f) = Odln(m) ln(m/d) + ln(1 /δ)
γ2m
.
When stating lower bounds in the following, we have simplified the expressions by requiring that
theexpected loss satisfies LD(f) = O( m−0.01). Note that this is far larger than the upper bounds,
except for values of mvery close to γ−2d. This only makes the lower bounds stronger. We remark
that all the lower bounds are more general than this, but focusing on m−0.01in this introduction
yields the cleanest bounds.
With these definitions, classic AdaBoost and other weak-to-strong learners producing voting clas-
sifiers with margins Ω(γ)all have a parallel complexity of (Θ(γ−2lnm),1): They all need
γ−2lnmrounds to obtain Ω(γ)margins. Karbasi and Larsen [2024] presented the first alter-
native tradeoff by giving an algorithm with parallel complexity (1,exp(O( dln(m)/γ2))). Sub-
sequent work by Lyu et al. [2024] gave a general tradeoff between pandt. When requiring
near-optimal accuracy, their tradeoff gives, for any 1≤R≤1/(2γ), a parallel complexity of
(O(γ−2ln(m)/R),exp(O( dR2)) ln(1 /γ)). The accuracy of both of these algorithms was proved
by arguing that they produce a voting classifier with all margins Ω(γ).
On the lower bound side, Karbasi and Larsen [2024] showed that one of three things must hold:
Either p≥min{Ω(γ−1lnm),exp(Ω( d))}, ort≥min{exp(Ω( dγ−2)),exp(exp(Ω( d)))}or
pln(tp) = Ω( dln(m)γ−2).
Lyu et al. [2024] also presented a lower bound that for some parameters is stronger than that of
Karbasi and Larsen [2024], and for some is weaker. Concretely, they show that one of the following
two must hold: Either p≥min{Ω(γ−2d),Ω(γ−2lnm),exp(Ω( d))}, ort≥exp(Ω( d)). Observe
that the constraint on tis only single-exponential in d, whereas the previous lower bound is double-
exponential. On the other hand, the lower bound on pis essentially stronger by a γ−1factor. Finally,
they also give an alternative lower bound for p= O( γ−2), essentially yielding plnt= Ω(γ−2d).
Even in light of the previous works, it is still unclear what the true complexity of parallel boosting
is. In fact, the upper and lower bounds only match in the single case where p= Ω( γ−2lnm)and
t= 1, i.e. when standard AdaBoost is optimal.
Our Contributions. In this work, we essentially close the gap between the upper and lower
bounds for parallel boosting. From the upper bound side, we show the following general result.
Theorem 1.1. Letc:X → {− 1,1}be an unknown concept, Wbe aγ-weak learner for cusing a
hypothesis set of VC-dimension d,Dbe an arbitrary distribution, and S∼ Dmbe a training set of
sizem. For all R∈N, Algorithm 1 yields a weak-to-strong learner ARwith parallel complexity
(p, t)for
p= Olnm
γ2R
and t=eO(dR)·lnlnm
δγ2,
such that, with probability at least 1−δoverSand the randomness of AR, it holds that
LD(AR(S)) = Odln(m) ln(m/d) + ln(1 /δ)
γ2m
.
Observe that this is a factor Rbetter than the bound by Lyu et al. [2024] in the exponent of t.
Furthermore, if we ignore the ln(ln( m)/(δγ2))factor, it gives the clean tradeoff
plnt= Odlnm
γ2
,
for any pfrom 1toO(γ−2lnm).
We complement our new upper bound by an essentially matching lower bound. Here we show that
3Theorem 1.2. There is a universal constant C≥1for which the following holds. For any 0<
γ < 1/C, any d≥C, any sample size m≥C, and any weak-to-strong learner Awith parallel
complexity (p, t), there exists an input domain X, a distribution D, a concept c:X → {− 1,1}, and
aγ-weak learner Wforcusing a hypothesis set Hof VC-dimension dsuch that if the expected loss
ofAover the sample is no more than m−0.01, then either p≥min{exp(Ω( d)),Ω(γ−2lnm)}, or
t≥exp(exp(Ω( d))), orplnt= Ω(γ−2dlnm).
Comparing Theorem 1.2 to known upper bounds, we first observe that p= Ω( γ−2lnm)corre-
sponds to standard AdaBoost and is thus tight. The term p= exp(Ω( d))is also near-tight. In
particular, given msamples, by Sauer-Shelah, there are only O((m/d)d) = exp(O( dln(m/d)))
distinct labellings by Hon the training set. If we run AdaBoost, and in every iteration, we check
whether a previously obtained hypothesis has advantage γunder the current weighing, then we make
no more than exp(O( dln(m/d)))queries to the weak learner (since every returned hypothesis must
be distinct). The plnt= Ω( γ−2dlnm)matches our new upper bound in Theorem 1.1. Thus, only
thet≥exp(exp(Ω( d)))term does not match any known upper bound.
Other Related Work. Finally, we mention the work by Long and Servedio [2013], which initiated
the study of the parallel complexity of Boosting. In their work, they proved that the parallel com-
plexity (p, t)must satisfy p= Ω(γ−2lnm), regardless of t(they state it as p= Ω(γ−2), but it is not
hard to improve by a lnmfactor for loss m−0.01). This seems to contradict the upper bounds above.
The reason is that their lower bound has restrictions on which query distributions the weak-to-strong
learner makes to the weak learner. The upper bounds above thus all circumvent these restrictions.
As a second restriction, their lower bound instance has a VC-dimension that grows with m.
2 Upper Bound
In this section, we discuss our proposed method, Algorithm 1. Here, Cnrefers a universal constant
shared among results.
We provide a theoretical analysis of the algorithm, showing that it realizes the claims in Theorem 1.1.
Our proof goes via the following intermediate theorem:
Theorem 2.1. There exists universal constant Cn≥1such that for all 0< γ < 1/2,R∈N,
concept c:X → {− 1,1}, and hypothesis set H ⊆ {− 1,1}Xof VC-dimension d, Algorithm 1 given
an input training set S∈ Xm, aγ-weak learner W,
p≥4 lnm
γ2R, and t≥e16CndR·RlnpR
δ,
produces a linear classifier gat Line 21 such that with probability at least 1−δover the randomness
of Algorithm 1, g(x)c(x)≥γ/8for all x∈S.
In Theorem 2.1 and throughout the paper, we define a linear classifier gas linear combination of
hypotheses g(x) =Pk
i=1αihi(x)withP
i|αi|= 1. A linear classifier thus corresponds to a voting
classifier with coefficients normalized and no sign operation. Observe that the voting classifier
f(x) = sign( g(x))is correct if and only if c(x)g(x)>0, where c(x)is the correct label of x.
Furthermore, c(x)g(x)is the margin of the voting classifier fon input x.
Theorem 1.1 follows from Theorem 2.1 via generalization bounds for linear classifiers with large
margins. Namely, we apply Breiman’s min-margin bound:
Theorem 2.2 (Breiman [1999]) .Letc:X → {− 1,1}be an unknown concept, H ⊆ {− 1,1}X
a hypothesis set of VC-dimension dandDan arbitrary distribution over X. There is a universal
constant C > 0such that with probability at least 1−δover a set of msamples S∼ Dm, it holds
for every linear classifier gsatisfying c(x)g(x)≥γfor all (x, c(x))∈Sthat
LD(sign( g))≤C·dln(m) ln(m/d) + ln(1 /δ)
γ2m.
Thus far, our general strategy mirrors that of previous works: We seek to show that given suitable
parameters Algorithm 1 produces a linear classifier with margins of order γwith good probability.
4Algorithm 1: Proposed parallel boosting algorithm
Input : Training set S={(x1, c(x1)), . . . , (xm, c(xm))},γ-weak learner W, number of
calls to weak learner per round t, number of rounds p
Output: V oting classifier f
1α←1
2ln1/2+γ/2
1/2−γ/2
2n←Cnd/γ2
3D1←(1
m,1
m, . . . ,1
m)
4fork←0top−1do
5 parallel for r←1toRdo
6 parallel for j←1tot/R do
7 Sample TkR+r,j∼Dn
kR+1
8 hkR+r,j← W (TkR+r,j,Uniform( TkR+r,j))
9 HkR+r← {hkR+r,1, . . . ,hkR+r,t/R} ∪ {− hkR+r,1, . . . ,−hkR+r,t/R}
10 forr←1toRdo
11 ifthere exists h∗∈HkR+rs.t.LDkR+r(h∗)≤1/2−γ/2then
12 hkR+r←h∗
13 αkR+r←α
14 else
15 hkR+r←arbitrary hypothesis from HkR+r
16 αkR+r←0
17 fori←1tomdo
18 DkR+r+1(i)←DkR+r(i) exp(−αkR+rc(xi)hkR+r(xi))
19 ZkR+r←Pm
i=1DkR+r(i) exp(−αkR+rc(xi)hkR+r(xi))
20 DkR+r+1←DkR+r+1/ZkR+r
21g←x7→1PpR
j=1αjPpR
j=1αjhj(x)
22return f:x7→sign(g(x))
Therefore, this section focuses on the lemmas that describe how, with suitable parameters, Algo-
rithm 1 produces a classifier with large margins. With these results in hand, the proof of Theorem 2.1
becomes quite straightforward, so we defer it to Appendix B.3.
Algorithm 1 is a variant of Lyu et al. [2024, Algorithm 2]. The core idea is to use bagging to produce
(in parallel) a set of hypotheses and use it to simulate a weak learner. To be more precise, we reason
in terms of the following definition.
Definition 1 (ε-approximation) .Given a concept c:X → {− 1,1}, a hypothesis set H ⊆ {− 1,1}X,
and a distribution DoverX, a multiset Tis anε-approximation forD,c, andHif for all h∈ H, it
holds that
|LD(h)− LT(h)| ≤ε,
where LT(h):=LUniform( T)(h)is the empirical loss of honT. Moreover, we omit the reference
tocandHwhen no confusion seems possible.
Consider a reference distribution D0over a training dataset S. The bagging part of the method
leverages the fact that if a subsample T∼Dn
0is aγ/2-approximation for D0, then inputting T
(with the uniform distribution over it) to a γ-weak learner produces a hypothesis hthat, besides
having advantage γonT, also has advantage γ/2on the entire dataset S(relative to D0). Indeed, in
this setting, we have that LD0(h)≤ LT(h)+γ/2≤1/2−γ+γ/2 = 1 /2−γ/2.We can then take h
as if produced by a γ/2-weak learner queried with (S, D 0),and compute a new distribution D1via
a standard Boosting step1. That is, we can simulate a γ/2-weak learner as long as we can provide a
γ/2-approximation for the target distribution. The strategy is to have a parallel bagging step in which
we sample T1,T2, . . . ,Ttiid∼Dn
0and query the γ-weak learner on each Tjto obtain hypotheses
h1, . . . ,ht. Then, we search within these hypotheses to sequentially perform RBoosting steps,
obtaining distributions D1, D2, . . . , D R. As argued, this approach will succeed whenever we can
1Notice that we employ a fixed learning rate that assumes a worst-case advantage of γ/2.
5find at least one γ/2-approximation for each Dramong h1,h2, . . . ,ht. A single parallel round of
querying the weak learner is thus sufficient for performing Rsteps of Boosting, effectively reducing
pby a factor R. Crucially, testing the performance of the returned hypotheses h1, . . . ,htuses only
inference/predictions and no calls to the weak learner.
The challenge is that the distributions Drdiverge (exponentially fast) from D0as we progress in the
Boosting steps. For the first Boosting step, the following classic result ensures a good probability of
obtaining an approximation for D0when sampling from D0itself.
Theorem 2.3 (Li et al. [2001], Talagrand [1994], Vapnik and Chervonenkis [1971]) .There is a
universal constant C > 0such that for any 0< ε, δ < 1,H ⊆ {− 1,1}Xof VC-dimension d,
and distribution DoverX, it holds with probability at least 1−δover a set T∼ DnthatTis an
ε-approximation for D,c, andHprovided that n≥C((d+ ln(1 /δ))/ε2).
However, we are interested in approximations for Drwhen we only have access to samples from
D0. Lyu et al. [2024] approaches this problem by tracking the “distance” between the distributions
in terms of their max-divergence
D∞(Dr, D0):= ln 
sup
x∈XDr(x)/D0(x)
. (2)
By bounding both D∞(Dr, D0)andD∞(D0, Dr), the authors can leverage the advanced composi-
tion theorem [Dwork et al., 2010]2from the differential privacy literature to bound the probability of
obtaining an approximation for Drwhen sampling from D0. In turn, this allows them to relate the
number of samples tand the (sufficiently small) number of Boosting steps Rin a way that ensures
a good probability of success at each step.
Besides setting up the application of advanced composition, the use of the max-divergence also sim-
plifies the analysis since its “locality” allows one to bound the divergence between the two distribu-
tions via a worst-case study of a single entry. However, this approach sacrifices global information,
limiting how much we can leverage our understanding of the distributions generated by Boosting
algorithms. With that in mind, we instead track the distance between DrandD0in terms of the
Kullback-Leibler divergence (KL divergence) [Kullback and Leibler, 1951] between them:
KL(Dr∥D0):=X
x∈XDr(x) lnDr(x)
D0(x).
Comparing this expression to Eq. (2) reveals that the max-divergence is indeed a worst-case estima-
tion of the KL divergence.
The KL divergence —also known as relative entropy — between two distributions PandQis always
non-negative and equal to zero if and only if P=Q. Moreover, in our setting, it is always finite due
to the following remark.3
Remark 1. In the execution Algorithm 1, every distribution Dℓ, forℓ∈[pR], has the same support.
This must be the case since Line 20 always preserves the support of D1.
On the other hand, the KL divergence is not a proper metric as it is not symmetric and it does not
satisfy the triangle inequality, unlike the max-divergence. This introduces a number of difficulties
in bounding the divergence between D0andDr. Overcoming these challenges requires a deeper
and highly novel analysis. Our results reveal that the KL divergence captures particularly well the
behavior of our Boosting algorithm. We remark that we are not the first to relate KL divergence and
Boosting, see e.g. Schapire and Freund [2012, Chapter 8 and the references therein], yet we make
several new contributions to this connection.
To study the probability of obtaining a γ/2-approximation for Drwhen sampling from D0, rather
than using advanced composition, we employ the duality formula for variational inference [Donsker
and Varadhan, 1975] —also known as Gibbs variational principle , orDonsker-Varadhan formula —
to estimate such a probability in terms of KL(Dr∥D0).
2Lemma 4.6 of Lyu et al. [2024].
3We only need Pto be absolutely continuous with respect to Q; i.e., that for any event A, we have P(A) = 0
whenever Q(A) = 0 . We express our results in terms of identical supports for the sake of simplicity as they
can be readily generalized to only require absolute continuity.
6Lemma 2.4 (Duality formula4).Given finite probability spaces (Ω,F, P)and(Ω,F, Q), ifPand
Qhave the same support, then for any real-valued random variable Xon(Ω,F, P)we have that
lnEP
eX
≥EQ[X]−KL(Q∥P). (3)
Lemma 2.4 allows us to prove that if KL(Dr∥D0)is sufficiently small, then the probability of
obtaining a γ/2-approximation for Drwhen sampling from D0is sufficiently large. Namely, we
prove the following.
Lemma 2.5. There exists universal constant Cn≥1for which the following holds. Given 0< γ <
1/2,R, m∈N, concept c:X → {− 1,1}, and hypothesis set H ⊆ {− 1,1}Xof VC-dimension d,
let˜DandDbe distributions over [m]andG ∈[m]∗be the family of γ/2-approximations for D,c,
andH. If˜DandDhave the same support and
KL(D∥˜D)≤4γ2R,
then for all n≥Cn·d/γ2it holds that
Pr
T∼˜Dn[T∈ G]≥exp(−16CndR).
Proof sketch. Our argument resembles a proof of the Chernoff bound: After taking exponentials
on both sides of Eq. (3), we exploit the generality of Lemma 2.4 by defining the random variable
X:T7→λ1{T∈G}and later carefully choosing λ. We then note that Theorem 2.3 ensures that X
has high expectation for T∼Dn. Setting λto leverage this fact, we obtain a lower bound on the
expectation of Xrelative to T∼˜Dn, yielding the thesis.
We defer the detailed proof to Appendix B.1.
With Lemma 2.5 in hand, recall that our general goal is to show that, with high probability, the linear
classifier gproduced by Algorithm 1 satisfies that c(x)g(x) = Ω( γ)for all x∈S. Standard tech-
niques allow us to further reduce this goal to that of showing that the product of the normalization
factors,QpR
ℓ=1Zℓ, is sufficiently small. Accordingly, in our next lemma, we bound the number of
samples needed in the bagging step to obtain a small product of the normalization factors produced
by the Boosting steps.
Here, the analysis in terms of the KL divergence delivers a clear insight into the problem, revealing
an interesting trichotomy: if KL(Dr∥D0)is small, Lemma 2.5 yields the result; on the other hand,
ifDrhas diverged too far from D0, then either the algorithm has already made enough progress for
us to skip a step, or the negation of some hypothesis used in a previous step has sufficient advantage
relative to the distribution at hand. Formally, we prove the following.
Lemma 2.6. There exists universal constant Cn≥1such that for all R∈N,0< δ < 1,0<
γ <1/2, and γ-weak learner Wusing a hypothesis set H ⊆ {− 1,1}Xwith VC-dimension d, ift≥
R·exp(16 CndR)·ln(R/δ),then with probability at least 1−δthe hypotheses hkR+1, . . . ,hkR+R
obtained by Algorithm 1 induce normalization factors ZkR+1, . . . ,ZkR+Rsuch that
RY
r=1ZkR+r<exp(−γ2R/2).
Proof sketch. We assume for simplicity that k= 0 and argue by induction on R′∈[R]. After
handling the somewhat intricate stochastic relationships of the problem, we leverage the simple
4Corollary of, e.g., Dembo and Zeitouni [1998, Lemma 6.2.13] or Lee [2022, Theorem 2.1]. Presented here
in a weaker form for the sake of simplicity.
7remark that KL(DR′∥DR′) = 0 to reveal the following telescopic decomposition:
KL(DR′∥D1) = KL( DR′∥D1)−KL(DR′∥DR′)
= KL( DR′∥D1)−KL(DR′∥D2)
+ KL( DR′∥D2)−KL(DR′∥D3)
+···
+ KL( DR′∥DR′−1)−KL(DR′∥DR′)
=R′−1X
r=1KL(DR′∥Dr)−KL(DR′∥Dr+1).
Moreover, given r∈ {1, . . . , R′−1},
KL(DR′∥Dr)−KL(DR′∥Dr+1) =mX
i=1DR′(i) lnDR′(i)
Dr(i)−mX
i=1DR′(i) lnDR′(i)
Dr+1(i)
=mX
i=1DR′(i) lnDr+1(i)
Dr(i)
=−lnZr−mX
i=1DR′(i)αrc(xi)hr(xi).
Altogether, we obtain that
KL(DR′∥D1) =−lnR′−1Y
r=1Zr+R′−1X
r=1αrmX
i=1DR′(i)c(xi)(−hr(xi)).
Now, if KL(DR′∥D1)is small (at most 4γ2R), Lemma 2.5 ensures that with sufficient probability
there exists a γ/2-approximation for DR′within TR′,1, . . . ,TR′,t/R, yielding the induction step
(by Claim 1). Otherwise, if KL(DR′∥D1)is large, then either (i)the term −lnQR′−1
r=1Zris
large enough for us to conclude thatQR′−1
r=1Zris already less than exp(−γ2R′/2)and we can skip
the step; or (ii)the termPR′−1
r=1αrPm
i=1DR′(i)c(xi)(−hr(xi))is sufficiently large to imply the
existence of h∗∈ {−h1, . . . ,−hR′−1}satisfying that
mX
i=1DR′(i)c(xi)h∗(xi)> γ,
which implies that such h∗has margin at least γwith respect to DR′and we can conclude the
induction step as before.
We defer the detailed proof to Appendix B.2.
3 Overview of the Lower Bound
In this section, we overview of the main ideas behind our improved lower bound. The details are
available in Appendix C. Our lower bound proof is inspired by, and builds upon, that of Lyu et al.
[2024]. Let us first give the high level idea in their proof. Similarly to Karbasi and Larsen [2024],
they consider an input domain X= [2m], where mdenotes the number if training samples available
for a weak-to-strong learner Awith parallel complexity (p, t). In their construction, they consider
a uniform random concept c:X → {− 1,1}and give a randomized construction of a weak learner.
Proving a lower bound on the expected error of Aunder this random choice of concept and weak
learner implies, by averaging, the existence of a deterministic choice of concept and weak learner
for which Ahas at least the same error.
The weak learner is constructed by drawing a random hypothesis set H, using inspiration from the
so-called coin problem . In the coin problem, we observe pindependent outcomes of a biased coin
and the goal is to determine the direction of the bias. If a coin has a bias of β, then upon seeing n
8outcomes of the coin, any algorithm for guessing the bias of the coin is wrong with probability at
leastexp(−O(β2n)). Now to connect this to parallel Boosting, Lyu et al. construct Hby adding
cas well as prandom hypotheses h1, . . . ,hptoH. Each hypothesis hihas each hi(x)chosen
independently with hi(x) =c(x)with probability 1/2 + 2 γ. The weak learner Wnow processes
a query distribution Dby returning the first hypothesis hiwith advantage γunder D. If no such
hypothesis exists, it instead returns c. The key observation is that if Wis never forced to return c,
then the only information Ahas about c(x)for each xnot in the training data (which is at least half
of all x, since |X|= 2m), is the outcomes of up to pcoin tosses that are 2γbiased towards c(x).
Thus, the expected error becomes exp(−O(γ2p)). For this to be smaller than m−0.01then requires
p= Ω(γ−2lnm)as claimed in their lower bound.
The last step of their proof, is then to argue that Wrarely has to return cupon a query. The idea here
is to show that in the ith parallel round, Wcan use hito answer all queries, provided that tis small
enough. This is done by observing that for any query distribution Dthat is independent of hi, the
expected loss satisfies Ehi[LD(hi)] = 1 /2−2γdue to the bias. Using inspiration from [Karbasi
and Larsen, 2024], they then show that for sufficiently "well-spread" queries D, the loss of hiunder
Dis extremely well concentrated around its expectation (over the random choice of hi) and thus hi
may simultaneously answer all (up to) twell-spread queries in round i. To handle "concentrated"
queries, i.e. query distribution with most of the weight on a few x, they also use ideas from [Karbasi
and Larsen, 2024] to argue that if we add 2O(d)uniform random hypotheses to H, then these may
be used to answer all concentrated queries.
Note that the proof crucially uses that hiis independent of the queries in the ith round. Here the key
idea is that if Wcan answer all the queries in round iusing hi, then hi+1, . . . ,hpare independent
of any queries the weak-to-strong learner makes in round i+ 1.
In our improved lower bound, we observe that the expected error of exp(−O(γ2p))is much larger
thanm−0.01for small p. That is, the previous proof is in some sense showing something much too
strong when trying to understand the tradeoff between pandt. What this gives us, is that we can
afford to make the coins/hypotheses himuch more biased towards cwhen pis small. Concretely,
we can let the bias be as large as β= Θ(p
ln(m)/p), which may be much larger than 2γ. This in
turns gives us that it is significantly more likely that himay answer an independently chosen query
distribution D. In this way, the same himay answer a much larger number of queries t, resulting
in a tight tradeoff between the parameters. As a second contribution, we also find a better way of
analyzing this lower bound instance, improving one term in the lower bound on tfrom exp(Ω( d))
toexp(exp( d)). We refer the reader to the full proof for details.
4 Conclusion
In this paper, we have addressed the parallelization of Boosting algorithms. By establishing both
improved lower bounds and an essentially optimal algorithm, we have effectively closed the gap
between theoretical lower bounds and performance guarantees across the entire tradeoff spectrum
between the number of training rounds and the parallel work per round.
Given that, we believe future work may focus on better understanding the applicability of the theo-
retical tools developed here to other settings since some lemmas obtained seem quite general. They
may aid, for example, in investigating to which extent the post-processing of hypotheses obtained in
the bagging step can improve the complexity of parallel Boosting algorithms, which remains as an
interesting research direction.
Acknowledgments and Disclosure of Funding
This research is co-funded by the European Union (ERC, TUCLA, 101125203) and Independent
Research Fund Denmark (DFF) Sapere Aude Research Leader Grant No. 9064-00068B. Views and
opinions expressed are however those of the author(s) only and do not necessarily reflect those of the
European Union or the European Research Council. Neither the European Union nor the granting
authority can be held responsible for them.
9References
Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of computer and system sciences , 55(1):119–139, 1997.
Adam J Grove and Dale Schuurmans. Boosting in the limit: Maximizing the margin of learned
ensembles. In AAAI/IAAI , pages 692–699, 1998.
Gunnar Rätsch, Manfred K Warmuth, and John Shawe-Taylor. Efficient margin maximizing with
boosting. Journal of Machine Learning Research , 6(12), 2005.
Rocco A. Servedio. Smooth boosting and learning with malicious noise. J. Mach. Learn. Res. , 4
(null):633–648, dec 2003. ISSN 1532-4435. doi: 10.1162/153244304773936072. URL https:
//doi.org/10.1162/153244304773936072 .
Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. The Annals of
Statistics , 29(5):1189 – 1232, 2001.
Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In KDD , pages 785–
794. ACM, 2016. ISBN 978-1-4503-4232-2.
Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-
Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In NIPS , 2017.
Alexey Natekin and Alois Knoll. Gradient boosting machines, a tutorial. Frontiers in Neurorobotics ,
7, 2013. ISSN 1662-5218.
Philip M. Long and Rocco A. Servedio. Algorithms and hardness results for parallel large margin
learning. J. Mach. Learn. Res. , 14(1):3105–3128, jan 2013. ISSN 1532-4435.
Amin Karbasi and Kasper Green Larsen. The impossibility of parallelizing boosting. In Claire
Vernade and Daniel Hsu, editors, Proceedings of The 35th International Conference on Algorith-
mic Learning Theory , volume 237 of Proceedings of Machine Learning Research , pages 635–653.
PMLR, 25–28 Feb 2024. URL https://proceedings.mlr.press/v237/karbasi24a.html .
Xin Lyu, Hongxun Wu, and Junzhao Yang. The cost of parallelizing boosting. In David P. Woodruff,
editor, Proceedings of the 2024 ACM-SIAM Symposium on Discrete Algorithms, SODA 2024,
Alexandria, VA, USA, January 7-10, 2024 , pages 3140–3155. SIAM, 2024. doi: 10.1137/1.
9781611977912.112. URL https://doi.org/10.1137/1.9781611977912.112 .
Michael Kearns. Learning boolean formulae or finite automata is as hard as factoring. Technical
Report TR-14-88 Harvard University Aikem Computation Laboratory , 1988.
Michael Kearns and Leslie Valiant. Cryptographic limitations on learning boolean formulae and
finite automata. Journal of the ACM (JACM) , 41(1):67–95, 1994.
Robert E Schapire. The strength of weak learnability. Machine learning , 5(2):197–227, 1990.
Leo Breiman. Prediction games and arcing algorithms. Neural Computation , 11(7):1493–
1517, 1999. doi: 10.1162/089976699300016106. URL https://doi.org/10.1162/
089976699300016106 .
Wei Gao and Zhi-Hua Zhou. On the doubt about margin explanation of boosting. Artif. Intell. , 203:
1–18, 2013.
Kasper Green Larsen and Martin Ritzert. Optimal weak to strong learning. In Sanmi
Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Ad-
vances in Neural Information Processing Systems 35: Annual Conference on Neural Informa-
tion Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - De-
cember 9, 2022 , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
d38653cdaa8e992549e1e9e1621610d7-Abstract-Conference.html .
Yi Li, Philip M. Long, and Aravind Srinivasan. Improved bounds on the sample complexity of
learning. Journal of Computer and System Sciences , 62(3):516–527, 2001. doi: 10.1006/JCSS.
2000.1741. URL https://doi.org/10.1006/jcss.2000.1741 .
10Michel Talagrand. Sharper bounds for gaussian and empirical processes. The Annals of Probability ,
pages 28–76, 1994.
Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. Theory of Probability and its Applications , 16(2):264–280, 1971.
Cynthia Dwork, Guy N. Rothblum, and Salil P. Vadhan. Boosting and differential privacy. In
51th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October 23-26,
2010, Las Vegas, Nevada, USA , pages 51–60. IEEE Computer Society, 2010. doi: 10.1109/FOCS.
2010.12. URL https://doi.org/10.1109/FOCS.2010.12 .
Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathe-
matical statistics , 22(1):79–86, 1951.
Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms . The MIT Press, 05
2012. ISBN 9780262301183. doi: 10.7551/mitpress/8291.001.0001. URL https://doi.org/
10.7551/mitpress/8291.001.0001 .
Monroe D Donsker and SRS386024 Varadhan. Asymptotic evaluation of certain markov process
expectations for large time, i and ii. Communications on Pure and Applied Mathematics , 28(1-2):
279–301, 1975.
Amir Dembo and Ofer Zeitouni. Large Deviations Techniques and Applications . Springer, 1998.
ISBN 978-1-4612-5320-4. doi: 10.1007/978-1-4612-5320-4. URL https://doi.org/10.
1007/978-1-4612-5320-4 .
Se Yoon Lee. Gibbs sampler and coordinate ascent variational inference: A set-theoretical review.
Communications in Statistics-Theory and Methods , 51(6):1549–1568, 2022.
11A Auxiliary Results
In this section we state and proof claims utilized in our argument. The arguments behind those are
fairly standard, so they are not explicitly stated in the main text.
Claim 1. Letℓ∈Nand0< γ < 1/2. If a hypothesis hℓhas advantage γℓsatisfying LDℓ(hℓ) =
1/2−γℓ≤1/2−γ/2andαℓ=α, then
Zℓ≤p
1−γ2≤e−γ2/2.
Proof. It holds that
Zℓ=mX
i=1Dℓ(i) exp(−αℓc(xi)hℓ(xi))
=X
i:hℓ(xi)=c(xi)Dℓ(i)e−α+X
i:hℓ(xi)̸=c(xi)Dℓ(i)eα
=1
2+γℓr1−γ
1 +γ+1
2−γℓ
·r1 +γ
1−γ
=1/2 +γℓ
1 +γ+1/2−γℓ
1−γp
(1 +γ)(1−γ)
=1−2γ·γℓ
1−γ2p
1−γ2.
Finally, since γℓ≥γ/2andγ∈(0,1/2), and, thus, 1−γ2>0, we have that
1−2γ·γℓ
1−γ2≤1−γ2
1−γ2= 1.
Claim 2. Algorithm 1 produces a linear classifier gwhose exponential loss satisfies
mX
i=1exp
−c(xi)pRX
j=1αjhj(xi)
=mpRY
j=1Zj.
Proof. It suffices to consider the last distribution DpR+1produced by the algorithm. It holds that
1 =mX
i=1DpR+1(i) (asDpR+1is a distribution)
=mX
i=1DpR(i)·exp(−αpRc(xi)hpR(xi))
ZpR(by Line 20)
=mX
i=1D1(i)·pRY
j=1exp(−αjc(xi)hj(xi))
Zj(by further unrolling the Djs)
=1
m·mX
i=1exp(−c(xi)PpR
j=1αjhj(xi))
QpR
j=1Zj. (asD1is uniform)
B Detailed Proofs
In this section, provide full proofs for the results from Section 2. For convenience, we provide copies
of the statements before each proof.
12B.1 Proof of Lemma 2.5
Lemma 2.5. There exists universal constant Cn≥1for which the following holds. Given 0< γ <
1/2,R, m∈N, concept c:X → {− 1,1}, and hypothesis set H ⊆ {− 1,1}Xof VC-dimension d,
let˜DandDbe distributions over [m]andG ∈[m]∗be the family of γ/2-approximations for D,c,
andH. If˜DandDhave the same support and
KL(D∥˜D)≤4γ2R,
then for all n≥Cn·d/γ2it holds that
Pr
T∼˜Dn[T∈ G]≥exp(−16CndR).
Proof. Letλ∈R>0(to be chosen later) and X: [m]n→ {0, λ}be the random variable given by
X(T) =λ1{T∈G}.
Since ˜DandDhave the same support, so do ˜DnandDn. Thus, taking the exponential of both sides
of Eq. (3), Lemma 2.4 yields that
exp(−KL(Dn∥˜Dn) +EDn[X])≤E˜Dn
eX
. (4)
We have that
EDn[X] =λ·Pr
T∼Dn[T∈ G]. (5)
Moreover,
E˜Dn
eX
=ET∼˜Dn
eλ·1{T∈G}+1{T̸∈G}
=ET∼˜Dn
eλ·1{T∈G}+ 1−1{T∈G}
= 1 + ( eλ−1)ET∼˜Dn
1{T∈G}
= 1 + ( eλ−1) Pr
T∼˜Dn[T∈ G]. (6)
Applying Eqs. (5) and (6) to Eq. (4), we obtain that
exp
−KL(Dn∥˜Dn) +λPr
T∼Dn[T∈ G]
≤1 + (eλ−1) Pr
T∼˜Dn[T∈ G]
and, thus,
Pr
T∼˜Dn[T∈ G]≥exph
−KL(Dn∥˜Dn) +λPrT∼Dn[T∈ G]i
−1
eλ−1
for any λ >0. Choosing
λ=KL(Dn∥˜Dn) + ln 2
PrT∼Dn[T∈ G],
we obtain that
Pr
T∼˜Dn[T∈ G]≥1
eλ−1
≥e−λ. (7)
Now, by Theorem 2.3 (using δ= 1/2), there exists a constant Cn≥1such that having
n≥Cn·d
γ2
ensures that
Pr
T∼Dn[T∈ G]≥1
2.
13Also, since, by hypothesis, KL(D∥˜D)≤4γ2R,we have that
KL(Dn∥˜Dn) =nKL(D∥˜D)
≤4CndR.
Applying it to Eq. (7), we conclude that
Pr
T∼˜Dn[T∈ G]≥exp
−4CndR+ ln 2
1/2
≥exp(−16CndR).
B.2 Proof of Lemma 2.6
Lemma 2.6. There exists universal constant Cn≥1such that for all R∈N,0< δ < 1,0<
γ <1/2, and γ-weak learner Wusing a hypothesis set H ⊆ {− 1,1}Xwith VC-dimension d, ift≥
R·exp(16 CndR)·ln(R/δ),then with probability at least 1−δthe hypotheses hkR+1, . . . ,hkR+R
obtained by Algorithm 1 induce normalization factors ZkR+1, . . . ,ZkR+Rsuch that
RY
r=1ZkR+r<exp(−γ2R/2).
Proof. Assume, for simplicity, that k= 0.
Letting
ER′=R′Y
r=1Zr<exp(−γ2R′/2)
,
we will show that for all R′∈[R]it holds that
Pr[ER′| E1, . . . ,ER′−1]≥1−δ/R. (8)
The thesis then follows by noting that
Pr[E1∩ ··· ∩ E R] =RY
r=1Pr[Er| E1, . . . ,Er−1] (by the chain rule)
≥
1−δ
RR
(by Eq. (8))
≥1−R·δ
R(by Bernoulli’s inequality)
= 1−δ.
LetGDR′⊆[m]nbe the family of γ/2-approximations for DR′and recall that if T∈GDR′,
then any h=W(T,Uniform( T))satisfies LDR′(h)≤1/2−γ/2.Therefore, the existence of
TR′,j∗∈GDR′, for some j∗∈[t/R], implies that hR′,j∗∈HR′has margin at least γ/2relative
toDR′. Hence, Algorithm 1 can select hR′,j∗at Line 11, setting αR′=αso that, by Claim 1, we
have that ZR′≤exp(−γ2/2).
Now notice that, by the law of total probability,
Prh
ER′∩R′−1
r=1Eri
= Prh
KL(DR′∥D1)≤4γ2R∩R′−1
r=1Eri
·Prh
ER′∩R′−1
r=1Er,KL(DR′∥D1)≤4γ2Ri
+ Prh
KL(DR′∥D1)>4γ2R∩R′−1
r=1Eri
·Prh
ER′∩R′−1
r=1Er,KL(DR′∥D1)>4γ2Ri
.
(9)
14We will show that, conditioned on ∩R′−1
r=1Er, ifKL(DR′∥D1)≤4γ2R, we can leverage Lemma 2.5
to argue that with probability at least 1−δ/R there exists a γ/2-approximation for DR′within
TR′,1, . . . ,TR′,t/R, and that ER′follows. On the other hand, if KL(DR′∥D1)>4γ2R, we shall
prove that ER′necessarily holds. Under those two claims, Eq. (9) yields that
Prh
ER′∩R′−1
r=1Eri
≥Prh
KL(DR′∥D1)≤4γ2R∩R′−1
r=1Eri
·
1−d
R
+ Prh
KL(DR′∥D1)>4γ2R∩R′−1
r=1Eri
·1
≥Prh
KL(DR′∥D1)≤4γ2R∩R′−1
r=1Eri
·
1−d
R
+ Prh
KL(DR′∥D1)>4γ2R∩R′−1
r=1Eri
·
1−d
R
= 1−δ
R,
which, as argued, concludes the proof.
To proceed, we ought to consider the relationships between the random variables involved. To do
so, for r∈[R]letTr={Tr,1, . . . ,Tr,t/R}. Notice that Dn
R′is itself random and determined by
D1, andT1, . . . ,TR′−1.
For the first part, let D1andT1, . . . ,TR′−1be realizations of D1andT1, . . . ,TR′−1such that
∩R′−1
r=1Erholds and KL(DR′∥D1)≤4γ2R. Notice that if there exists a γ/2-approximation for
DR′withinTR, then we can choose some hR′∈HR′with advantage at least γ/2so that
R′Y
r=1Zr=ZR′·R′−1Y
r=1Zr
<ZR′·exp(−γ2(R′−1)/2) (as we condition on ∩R′−1
r=1Er)
≤exp(−γ2R′/2) (by Claim 1)
and, thus, ER′follows. That is,
Prh
ER′∩R′−1
r=1Er,KL(DR′∥D1)≤4γ2Ri
≥ Pr
TR′,1,...,TR′,t/Riid∼Dn
1
∃j∈[t/R],TR′,j∈ GDR′
.
(10)
Finally, since by Remark 1 the distributions DR′andD1must have the same support, and we assume
thatKL(DR′∥D1)≤4γ2R, Lemma 2.5 ensures that
Pr
T∼Dn
1[T∈ GDR′]≥exp(−16CndR).
Therefore,
Pr
TR′,1,...,TR′,t/Riid∼Dn
1
∀j∈[t/R],TR′,j/∈ GDR′
=
Pr
T∼Dn
1
T/∈ GDR′t/R
(by IIDness)
≤(1−exp(−16CndR))t/R
≤exp
−t
R·exp(−16CndR)
≤δ
R,
where the second inequality follows since 1 +x≤exfor all x∈Rand the last from the hypothesis
thatt≥R·exp(16 CndR)·ln(R/δ). Considering the complementary event and applying Eq. (10),
we obtain that ER′holds with probability at least 1−δ/R.
For the second part, consider instead D1andT1, . . . ,TR′−1realizations of D1andT1, . . . ,TR′−1
such that ∩R′−1
r=1Erholds and
4γ2R <KL(DR′∥D1), (11)
15and argue that ER′necessarily follows.
Observe that
KL(DR′∥D1) = KL( DR′∥D1)−KL(DR′∥DR′)
= KL( DR′∥D1)−KL(DR′∥D2)
+ KL( DR′∥D2)−KL(DR′∥D3)
+···
+ KL( DR′∥DR′−1)−KL(DR′∥DR′)
=R′−1X
r=1KL(DR′∥Dr)−KL(DR′∥Dr+1). (12)
Moreover, given r∈ {1, . . . , R′−1},
KL(DR′∥Dr)−KL(DR′∥Dr+1) =mX
i=1DR′(i) lnDR′(i)
Dr(i)−mX
i=1DR′(i) lnDR′(i)
Dr+1(i)
=mX
i=1DR′(i) lnDr+1(i)
Dr(i)
=mX
i=1DR′(i) lnexp(−αrc(xi)hr(xi))
Zr
=−lnZr−mX
i=1DR′(i)αrc(xi)hr(xi).
Applying it to Eqs. (11) and (12) yields that
4γ2R <KL(DR′∥D1) =−lnR′−1Y
r=1Zr−R′−1X
r=1αrmX
i=1DR′(i)c(xi)hr(xi).
Thus, either
−lnR′−1Y
r=1Zr>4γ2R
2, (13)
or
−R′−1X
r=1αrmX
i=1DR′(i)c(xi)hr(xi)>4γ2R
2. (14)
We proceed to analyze each case.
If Eq. (13) holds, then
R′−1Y
r=1Zr<exp(−2γ2R)
≤exp(−γ2R′/2)
andER′follows by noting that ZR′= 1 regardless of the outcome of Line 11 soQR′
r=1Zr≤QR′−1
r=1Zr.
On the other hand, if Eq. (14) holds, then, letting R={r∈[R′−1]|αr̸= 0},
2γ2R <−R′−1X
r=1αrmX
i=1DR′(i)c(xi)hr(xi)
=−X
r∈RαmX
i=1DR′(i)c(xi)hr(xi).
16Since|R| ≤ R, we obtain that
X
r∈R1
|R|mX
i=1DR′(i)c(xi)(−hr(xi))>2γ2
α
so that there exists h∗∈ {− hr|r∈ R} such that
mX
i=1DR′(i)c(xi)h∗(xi)>2γ2
α. (15)
Moreover, from the definition of α,
α=1
2ln1/2 +γ/2
1/2−γ/2
=1
2ln
1 +2γ
1−γ
≤γ
1−γ
<2γ, (16)
where the last inequality holds for any γ∈(0,1/2). Applying it to Eq. (15) yields that
mX
i=1DR′(i)c(xi)h∗(xi)>2γ2
2γ
≥γ,
thusLDR′(h∗)<1/2−γ/2and, as before, ER′follows by Claim 1 and the conditioning on
∩R′−1
r=1Er.
B.3 Proof of Theorem 2.1
Theorem 2.1. There exists universal constant Cn≥1such that for all 0< γ < 1/2,R∈N,
concept c:X → {− 1,1}, and hypothesis set H ⊆ {− 1,1}Xof VC-dimension d, Algorithm 1 given
an input training set S∈ Xm, aγ-weak learner W,
p≥4 lnm
γ2R, and t≥e16CndR·RlnpR
δ,
produces a linear classifier gat Line 21 such that with probability at least 1−δover the randomness
of Algorithm 1, g(x)c(x)≥γ/8for all x∈S.
Proof. Letk∈ {0,1, . . . , p −1}. Applying Lemma 2.6 with failure probability δ/p, we obtain that
with probability at least 1−δ/p,
RY
r=1ZkR+r<exp(−γ2R/2).
Thus, by the union bound, the probability that this holds for all k∈ {0,1, . . . , p −1}is at least 1−δ.
Under this event, we have that
mX
i=1exp
−c(xi)pRX
j=1αjhj(xi)
=mpRY
j=1Zj (by Claim 2)
=mp−1Y
k=0RY
r=1ZkR+r
≤mp−1Y
k=0exp(−γ2R/2)
=mexp(−γ2pR/2). (17)
17Now, let θ≥0. Ifc(x)g(x)< θ, then, by the definition of gat Line 21, it must hold that
c(x)PpR
j=1αjhj(x)<PpR
j=1αjθ,thus the differencePpR
j=1αjθ−c(x)PpR
j=1αjhj(x)is strictly
positive. Taking the exponential, we obtain that, for all x∈S,
1{c(x)g(x)<θ}≤1
<exp
pRX
j=1αjθ−c(x)pRX
j=1αjhj(x)

≤exp(pRαθ ) exp
−c(x)pRX
j=1αjhj(x)
. (asαj≤α)
Therefore,
mX
i=11{c(xi)g(xi)<θ}<exp(pRαθ )mX
i=1exp
−c(xi)pRX
j=1αjhj(xi)
.
Applying Eq. (17), we obtain that
mX
i=11{c(xi)g(xi)<θ}< mexp(pRαθ ) exp(−γ2pR/2)
=mexp 
pR(αθ−γ2/2)
.
Finally, since 0≤α≤2γ(see Eq. (16)), we have that, for 0≤θ≤γ/8,
αθ−γ2/2≤2γ·γ/8−γ2/2
≤ −γ2/4
and thus
mX
i=11{c(xi)g(xi)<γ/8}< mexp 
−pRγ2/4
≤m·m−1(asp≥4R−1γ−2lnm)
= 1,
and we can conclude that all points have a margin greater than γ/8.
C Lower Bound
In this section, we prove Theorem 1.2. Theorem 1.2 is a consequence of the following Theorem C.2.
Before we state Theorem C.2 we will: state the assumptions that we make in the lower bound for
a learning algorithm Awith parallel complexity (p, t), the definition of a γ-weak learner in this
section and describe the hard instance. For this let c:X → {− 1,1}denote a labelling function.
Furthermore, throughout Appendix C let Csize:=Cs≥1,Cbias:=Cb≥1andCloss:=Cl≥1
denote the same universal constants.
Assumption C.1. LetQiwith|Qi| ≤tbe the queries made by a learning algorithm Awith parallel
complexity (p, t)during the ith round. We assume that a query Qi
j∈Qifori= 1, . . . , p and
j= 1, . . . , t is on the form (Si
j, c(Si
j),Di
j), where the elements in Si
jare contained in S, and that
the distribution Di
jhas support supp (Di
j)⊂ {(Si
j)1, . . . , (Si
j)m}. Furthermore, we assume Q1only
depends on the given sample S∈ Xmand the sample labels c(S)where c(S)i=c(Si), and that Qi
fori= 2, . . . , p only depends on the label sample S,c(S)and the previous i−1queries and the
responses to these queries.
We now clarify what we mean by a weak learner in this section.
Definition 2. Aγ-weak learner Wacting on a hypothesis set H, takes as input (S, c(S), D),
where S∈ X∗=∪∞
i=1Xi,c(S)i=c(Si)and supp (D)⊆ {S1, S2. . .}. The output of
h=W(H)(S, c(S), D)is such thatP
iD(i)1{h(i)̸=c(i)} ≤1/2−γ.
18We now define the hard instance which is the same construction as used in Lyu et al. [2024](which
found inspiration in Karbasi and Larsen [2024]). For d∈N, samples size m, and 0< γ <1
4Cbwe
consider the following hard instance
1. The universe Xwe take to be [2m].
2. The distribution Dwe will use on [2m]will be the uniform distribution Uover[2m].
3. The random concept cthat we are going to use is the uniform random concept {−1,1}2m, i.e.
all the labels of care i.i.d. and Prc[c(i) = 1] = 1 /2fori= 1, . . . , m .
4. The random hypothesis set will depend on the number of parallel rounds p, a scalar R∈N, and
the random concept c, thus we will denote it Hp,c,R. We will see Hp,c,Ras a matrix where the
rows are the hypothesis so vectors of length 2m, where the ith entry specifies the prediction the
hypothesis makes on element i∈[2m]. To define Hp,c,Rwe first define two random matrices
HuandHc.Huis a random matrix consisting of R⌈exp (Csd)⌉rows, where the rows in Huare
i.i.d. with distribution r∼ {− 1,1}2m(rhas i.i.d. entries Prr∼{− 1,1}2m[r(1) = 1] = 1 /2).Hcis
a random matrix with Rrows, where the rows in Hcare i.i.d. ˙with distribution b∼ {− 1,1}2m
Cb,
meaning the entries of bare independent and has distribution Prb∼{− 1,1}2m
Cb[b(i)̸=c(i)] =
1/2−Cbγ(soCbγbiased towards the sign of c). We now let H1
u,H1
c, . . . ,Hp
u,Hp
cdenote i.i.d.
copies of respectively HuandHc, and set Hp,c,Rto be these i.i.d. copies stack on top of each
other and Hp,c,R∪cto be the random matrix which first rows are Hp,c,Rand its last row is c,
Hp,c,R=
H1
u
H1
c...
Hp
u
Hp
c
Hp,c,R∪c=
Hp,c,R
c
.
5. The algorithm Wwhich given matrix/hypothesis set M∈Rℓ×R2m(where Mi,·denotes the ith
row of M) is the following algorithm W(M).
Algorithm 2: W(M)
Input : Triple (S, c(S), D)where S∈[2m]∗,c(S)i=c(Si)and probability distribution
Dwith supp (D)⊂ {S1, S2, . . . ,}.
Output: Hypothesis h=Mi,·for some i= 1, . . . , ℓ such that:P
iD(i)1{h(i)̸=c(i)} ≤1/2−γ.
1fori∈[ℓ]do
2 ifP
jD(j)1{Mi,j̸=c(j)} ≤1/2−γ// Notice that Wdoesn’t know cbut
can calculate this quantity using the information in (S, c(S), D)
which is given as input.
3 then
4 return Mi,·.
5return M1,·.
We notice that with this construction, we have that |Hp,c,R| ≤R⌈exp (Csd)⌉+RpandW(Hp,c,R∪
c)a weak learner since it either finds a row in Hp,c,Rwith error less than 1/2−γfor a query or
outputs cwhich has 0error for any query - this follows by the Assumption C.1 that the learning
algorithm given (S,c(S))make queries which is consistent with c.
With these definitions and notation in place, we now state Theorem C.2, which Theorem 1.2 is a
consequence of.
Theorem C.2. Ford∈N,m∈N, margin 0< γ <1
4Cb,R, p, t ∈N, universe [2m],Uthe
uniform distribution on [2m], and cthe uniform concept on [2m]any learning algorithm Awith
parallel complexity (p, t), given labelled training set (S,c(S)), where S∼ Um, and query access
toW(Hp,c,R∪c)we have that
ES,c,H[Lc
U(A(S,c(S),W(Hp,c,R∪c)))]
≥exp(−ClC2
bγ2Rp)
4Cl
1−exp
−mexp(−ClC2
bγ2Rp)
8Cl
−ptexp (−Rd)
,
19We now restate and give the proof of Theorem 1.2.
Theorem 1.2. There is a universal constant C≥1for which the following holds. For any 0<
γ < 1/C, any d≥C, any sample size m≥C, and any weak-to-strong learner Awith parallel
complexity (p, t), there exists an input domain X, a distribution D, a concept c:X → {− 1,1}, and
aγ-weak learner Wforcusing a hypothesis set Hof VC-dimension dsuch that if the expected loss
ofAover the sample is no more than m−0.01, then either p≥min{exp(Ω( d)),Ω(γ−2lnm)}, or
t≥exp(exp(Ω( d))), orplnt= Ω(γ−2dlnm).
Proof of Theorem 1.2. Fixd≥1, sample size m≥(e80Cl)100, margin 0< γ≤1
4Cb,psuch
thatp≤min
exp(d/8),ln(m0.01/(80Cl))
2ClC2
bγ2
,t≤exp(exp( d)/8)andpln(t)≤dln(m0.01/(80Cl))
8ClC2
bγ2 .
We now want to invoke Theorem C.2 with different values of Rdepending on the value of p. We
consider 2 cases. Firstly, the case
ln 
m0.01/(80Cl)
2ClC2
bγ2⌊exp(d)⌋≤p≤ln 
m0.01/(80Cl)
2ClC2
bγ2.
In this case one can choose R∈Nsuch that 1< R≤ ⌊exp(d)⌋and
ln(m0.01/(80Cl))
2ClC2
bγ2R≤p≤ln(m0.01/(80Cl))
2ClC2
bγ2(R−1).
Let now Rbe such. We now invoke Theorem C.2 with the above parameters and get
ES,c,H[Lc
U(A(S,c(S),W(Hp,c,R∪c)))] (18)
≥exp(−ClC2
bγ2Rp)
4Cl
1−exp
−mexp(−ClC2
bγ2Rp)
8Cl
−ptexp (−Rd)
,
We now bound the individual terms on the right-hand side of Eq. (18). First by p≤
ln(m0.01/(80Cl))
2ClC2
bγ2(R−1)≤ln(m0.01/(80Cl))
ClC2
bγ2Rwe get thatexp(−ClC2
bγ2Rp)
4Cl≥20m−0.01which further im-
plies exp
−mexp(−ClC2
bγ2Rp)
8Cl
≤exp(−10m0.99)≤e−10.We further notice that for Ras above
we have that pln(exp( Rd/4))≥dln(m0.01/(80Cl))
8ClC2
bγ2 . This implies that t≤exp(Rd/4), since else we
would have t >exp(Rd/4)andpln(t)> pexp(Rd/4)≥dln(m0.01/(80Cl))
8ClC2
bγ2 which is a contradic-
tion with our assumption that pln(t)≤dln(m0.01/(80Cl))
8ClC2
bγ2 . Since we also assumed that p≤exp(d/8)
we have that pt≤exp(d/8 +Rd/4·). Combining this with R > 1andd≥1we have that
ptexp (Rd)≤exp (Rd/2)≥e−1. Combining the above observations we get that the right-hand
side of Eq. (18) is at least
ES,c,H[Lc
U(A(S,c(S),W(Hp,c,R∪c)))]≥20m−0.01 
1−e−10−e−1
≥m−0.01.
Now in the case that
p <ln 
m0.01/(80Cl)
2ClC2
bγ2⌊exp(d)⌋, (19)
we choose R=⌊exp(d)⌋. Invoking Theorem C.2 again give use the expression in Eq. (18)
(with the parameter R=⌊exp(d)⌋now) and we again proceed to lower bound the right-hand
side of Eq. (18). First we observe that by the upper bound on pin Eq. (19), R=⌊exp(d)⌋and
exp(−x/2)≥exp (−x)forx≥1we get thatexp(−ClC2
bγ2Rp)
4Cl≥exp(−ln(m0.01/(80Cl))/2)
4Cl≥
20m−0.01, which further implies that exp
−mexp(−ClC2
bγ2Rp)
8Cl
≤e−10.Now since ⌊x⌋ ≥x/2
forx≥1,R=⌊exp(d)⌊and we assumed that t≤exp(exp( d)/8)andp≤exp(d/8)we get that
ptexp(−Rd)≤exp (exp( d)/8 +d/8−dexp(d)/2)≤exp (−dexp(d)/4)≤e−e/4. Combining
the above observations we get that the right-hand side of Eq. (18) is at least
ES,c,H[Lc
U(A(S,c(S),W(Hp,c,R∪c)))]≥20m−0.01
1−e−10−e−e/4
≥m−0.01.
20Thus, for any of the above parameters d, m, γ, p, t in the specified parameter ranges, we have that
the expected loss of AoverS,c,Hp,c,Ris at least m−0.01, so there exists concept cand hypothesis
Hsuch that the expected loss of AoverSis at least m−0.01. Furthermore, if Awere a random
algorithm Yao’s minimax principle would give the same lower bound for the expected loss over A
andSas the above bound holds for any deterministic A.
Now as remarked on before the proof the size of the hypothesis set Hp,c,Ris at most |Hp,c,R| ≤
R⌈exp (Csd)⌉+Rp, see Item 4. Combining this with us in the above arguments having p≤
exp(d/8),R≤exp(d)we conclude that |H ∪ c| ≤ exp(˜Cd/2)for˜Clarge enough. Thus,
we get at bound of log2(|H ∪ c|)≤log2(exp( ˜Cd/2))≤˜Cdwhich is also an upper bound of
the VC-dimension of H ∪c. Now redoing the above arguments with dscaled by 1/˜Cwe get
that the VC-dimension of H ∪cis upper bounded by dand the same expected loss of m−0.01.
The constraints given in the start of the proof with this rescaling of dis now d≥˜C,m≥
(e80Cl)100,0< γ≤1
4Cb,p≤minn
exp(d/(8˜C)),ln (m0.01/(80Cl))
2ClC2
bγ2o
,t≤exp (exp ( d/˜C)/8)and
pln(t)≤dln(m0.01/(80Cl))
8˜CClC2
bγ2 . Thus, with the universal constant C= maxn
(e80Cl)100,4Cb,˜Co
andm, d≥Candγ≤1/Cwe have that the expected loss is at least m−0.01when p≤
min
exp(O(d)), O(ln (m)/γ2)	
,t≤exp (exp ( O(d)))andpln(t)≤O(dln (m)/γ2)which con-
cludes the proof.
We now move on to prove Theorem C.2. For this, we now introduce what we will call the extension
ofAwhich still terminates if it receives a hypothesis with loss more than 1/2−γ. We further show
two results about this extension one which says that with high probability we can replace Awith
its extension and another saying that with high probability the loss of the extension is large, which
combined will give us Theorem C.2.
6. The output of the extension BAofAon input (S, c(S),W)is given through the outcome of
recursive query sets Q1, . . ., where each of the sets contains tqueries. The recursion is given in
the following way: Make Q1toWasAwould have done on input (S, c(S),·)(this is possible
by Assumption C.1 which say Q1is a function of only (S, c(S))). For i= 1, . . . , p such that
for all j= 1, . . . , t it is the case that W(Qi−1
j)has loss less than 1/2−γunder Di−1
jletQi
be the query set QthatAwould have made after having made query sets Q1, . . . , Qi−1and
received hypothesis {W(Ql
j)}(l,j)∈[i−1]×[t]. If this loop ends output the hypothesis that Awould
have made with responses {W(Ql
j)}(l,j)∈[i]×[t]to its queries. If there is an l, jsuch that W(Ql
j)
return a hypothesis with loss larger than 1/2−γreturn the all 1hypothesis.
We now go to the two results we need in the proof of Theorem C.2. The first result Corollary 1 says
that there exists an event Ewhich happens with high probability over Hp,c,Rsuch that Arun with
W(Hp,c,R,c)is the same as BArun with W(Hp,c,R). This corollary can be proved by following
the proofs of Theorem 5 and 8 in Lyu et al. [2024] and is thus not included here.
Corollary 1. Ford∈N,m∈N, margin 0< γ <1
4Cb, labelling function c: [2m]→ {− 1,1},
R, p, t ∈N, random matrix Hp,c,R , learning algorithm A,BA, training sample S∈[2m]m, we
have that there exist and event Eover outcomes of Hp,c,R such that
A(S, c(S),W(Hp,c,R∪c))1E=BA(S, c(S),W(Hp,c,R))1E
and
Pr
Hp,c,R[E]≥1−ptexp (−Rd).
The second result that we are going to need is Lemma C.3 which relates parameters R, β, p to the
success of any function of (S,c(S),Hp,c,R)which tries to guess the signs of c— which is the
number of failures in our hard instance. For a training sample S∈[2m]∗we will use |S|to denote
the number of distinct elements in Sfrom [2m], so for S∈[2m]mwe have |S| ≤m.
Lemma C.3. There exists universal constant Cs, Cl≥1such that: For m∈N,p∈N,Hp,β,c,R,
function Bthat takes as input S∈[2m]mwith labels c(S), and hypothesis set Hp,β,c,R, we have
21that
Pr
c,Hp,c,R"2mX
i=11{B(S,c(S),Hp,c,R)(i)̸=c(i)} ≥(2m− |S|) exp(−ClC2
bγ2Rp)
2Cl#
≥1−exp
−(2m− |S|) exp(−ClC2
bγ2Rp)
8Cl
.
We postpone the proof of Lemma C.3 and now give the proof of Theorem C.2.
Proof of Theorem C.2. We want to lower bound ES,c,H[Lc
U(A(S,W(H∪c)))]. To this end since
Sandcare independent and Hp,c,Rdepended on cthe expected loss can be written as
ES,c,H[Lc
U(A(S,c(S),W(Hp,c,R∪c)))]
=ES
Ec
EHp,c,R[Lc
U(A(S,c(S),W(Hp,c,R∪c)))]
.
Now let S∈[2m]m, cbe any outcome of Sandc. Then for these S, cwe have by Lemma C.3 that
there exists some event EoverHp,c,R such that
Lc
U(A(S, c(S),W(Hp,c,R, c)))1E=Lc
U(BA(S, c(S),W(Hp,c,R)))1E,
and
Pr
Hp,c,R[E]≥1−ptexp (−Rd),
furthermore, define E′be the event that
E′=(2mX
i=11{BA(S, c(S),W(Hp,c,R))(i)̸=c(i)} ≥(2m− |S|) exp(−ClC2
bγ2Rp)
2Cl)
.
Using the above and Ubeing the uniform measure on [2m]so assigns 1/(2m)mass to every point
and that |S| ≤mwe now get that,
EHp,c,R[Lc
U(A(S, c(S),W(Hp,c,R, c)))]≥EHp,c,R[Lc
U(A(S, c(S),W(Hp,c,R, c)))1E1E′]
=EHp,c,R[Lc
U(BA(S, c(S),W(Hp,c,R)))1E1E′]≥(2m− |S|) exp(−ClC2
bγ2Rp)
4ClmEHp,c,R[1E1E′]
≥exp(−ClC2
bγ2Rp)
4Cl
1−Pr
Hp,c,R[E′]−ptexp (−Rd)
.
We can do this for any pair candS∈[2m]m, so we have that
Ec
EHp,c,R[Lc
U(A(S,c(S),W(Hp,c,R∪c)))]
≥exp(−ClC2
bγ2Rp)
4Cl
1−Pr
c,Hp,c,R[E′]−ptexp (−Rd)
.
Now by Lemma C.3 and |S| ≤mwe have that Prc,Hp,c,R
E′
is at most
Pr
c,Hp,c,R
E′
≤exp
−(2m− |S|) exp(−ClC2
bγ2Rp)
8Cl
≤exp
−mexp(−ClC2
bγ2Rp)
8Cl
.
I.e. we have shown that
Ec
EHp,c,R[Lc
U(A(S,c(S),W(Hp,c,R∪c)))]
≥exp(−ClC2
bγ2Rp)
4Cl
1−exp
−mexp(−ClC2
bγ2Rp)
8Cl
−ptexp (−Rd)
,
for any S∈[2m]m. Now by taking expectation over S∼ Umwe get that
ES,c,H[Lc
U(A(S,c(S),W(Hp,c,R∪c)))]
≥exp(−ClC2
bγ2Rp)
4Cl
1−exp
−mexp(−ClC2
bγ2Rp)
8Cl
−ptexp (−Rd)
,
which concludes the proof.
22We now prove Lemma C.3 which is a consequence of maximum-likelihood, and the following
Fact 1, where Fact 1 gives a lower bound on how well one from ntrials of a biased {−1,1}random
variable, where the direction of the bias itself is random, can guess this random direction of the bias.
Fact 1. For function f:{−1,1}n→ {− 1,1}and0< γ≤1
4Cb
Ec∼{− 1,1}[Eb∼{− 1,1}n
Cb[1{f(b)̸=c}]]≥exp(−ClC2
bγ2n)/Cl.
Proof. This is the classic coin problem. The lower bound follows by first observing, by maximum-
likelihood, that the function f⋆minimizing the above error is the majority function. The result then
follows by tightness of the Chernoff bound up to constant factors in the exponent.
With Fact 1 in place we are now ready to proof Lemma C.3, which we restate before the proof
Proof of Lemma C.3. LetHCbbe the matrix consisting of the i.i.d Cbγbiased matrices in Hp,c,R ,
Hc1, . . . ,Hcpstack on top of each other,
HCb=
H1
c...
Hp
c
.
Furthermore, for i= 1, . . . , 2mletHCb,idenote the ith column of HCbwhich is a vector of length
pR. Now for iinside S,Bhas the sign ofc(i), so the best function that Bcan be is to be equal
toc(i). For ioutside S,Bdoes not know c(i)from the input but has information about it through
HCb,i, we notice that the sign’s of the hypotheses in H1
u, . . . ,Hp
uandHCb,jj̸=iandc(S)is
independent of c(i)and does not hold information about c(i), thus the best possible answer any B
can make is to choose the sign which is the majority of the sign’s inHCb,i- the maximum likelihood
estimator. We now assume that Bis this above-described "best" function - as this function will be
a lower bound for the probability of failures for any other B, so it suffices to show the lower bound
for this B. Now with the above described B, we have that
X:=2mX
i=11{B(S,c(S),Hp,c,R)(i)̸=c(i)}=X
i̸∈S1{sign
pRX
j=1HCbi,j
̸=c(i)}.
Thus, we have that Xis a sum of 2m− |S|(where |S|is the number of distinct elements in S)
independent {0,1}-random variables and by Fact 1 we have that the expectation of each these ran-
dom variables is at least Ec,Hp,c,R[X]≥(2m− |S|) exp(−ClC2
bγ2Rp)/Cl. Thus, we now get by
Chernoff that
Pr
c,Hp,c,R
X≥(2m− |S|) exp(−ClC2
bγ2Rp)
2Cl
≥Pr
c,Hp,c,R[X≥E[X]/2]
≥1−exp(−E[X]/8)≥1−exp
−(2m− |S|) exp(−ClC2
bγ2Rp)
8Cl
,
as claimed.
23NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The theoretical claims in the introduction and abstract exactly match what we
prove in the paper and claim as the scope and contributions.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [NA]
Justification: The paper is theoretical, so given the assumptions of the claims, there are no
limitations.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The au-
thors should reflect on how these assumptions might be violated in practice and what
the implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
24Answer: [Yes]
Justification: The main paper includes a proof sketch for both the upper and lower bound.
Parts of the formal proofs of the upper bound and lower bound are in the main text and the
remaining are in the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: The paper is theoretical, and we have no experiments, data or code in the
paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might suffice, or if the contribution is a specific model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
255.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: The paper is theoretical, and we have no experiments, data or code in the
paper.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
• While we encourage the release of code and data, we understand that this might not
be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: The paper is theoretical, and we have no experiments, data or code in the
paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [NA]
Justification: The paper is theoretical, and we have no experiments, data or code in the
paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
26• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: The paper is theoretical, and we have no experiments, data or code in the
paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We see no violations of the NeurIPS Code of Ethics.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The result of the paper is theoretical/foundational research, so we have no
experiments, data or code in the paper.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
27• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
cific groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper is theoretical, and we have no experiments, data or code in the
paper.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper is theoretical, and we have no experiments, data or code in the
paper.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
28• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [NA]
Justification: The paper is theoretical, and we have no experiments, data or code in the
paper.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justification: The paper is theoretical, and we have no experiments, data or code in the
paper.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper is theoretical, and we have no experiments, data or code in the
paper.
Guidelines:
29• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
30