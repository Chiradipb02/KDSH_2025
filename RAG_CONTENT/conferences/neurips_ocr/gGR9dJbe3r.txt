Exponential Quantum Communication Advantage in
Distributed Inference and Learning
Dar Gilboa∗†
Google Quantum AI
Venice, CA, United StatesHagay Michaeli†
Technion
Haifa, IsraelDaniel Soudry
Technion
Haifa, IsraelJarrod R. McClean
Google Quantum AI
Venice, CA, United States
Abstract
Training and inference with large machine learning models that far exceed the
memory capacity of individual devices necessitates the design of distributed ar-
chitectures, forcing one to contend with communication constraints. We present
a framework for distributed computation over a quantum network in which data
is encoded into specialized quantum states. We prove that for models within this
framework, inference and training using gradient descent can be performed with
exponentially less communication compared to their classical analogs, and with
relatively modest overhead relative to standard gradient-based methods. We show
that certain graph neural networks are particularly amenable to implementation
within this framework, and moreover present empirical evidence that they perform
well on standard benchmarks. To our knowledge, this is the first example of expo-
nential quantum advantage for a generic class of machine learning problems that
hold regardless of the data encoding cost. Moreover, we show that models in this
class can encode highly nonlinear features of their inputs, and their expressivity
increases exponentially with model depth. We also delineate the space of models
for which exponential communication advantages hold by showing that they cannot
hold for linear classification. Communication of quantum states that potentially
limit the amount of information that can be extracted from them about the data and
model parameters may also lead to improved privacy guarantees for distributed
computation. Taken as a whole, these findings form a promising foundation for
distributed machine learning over quantum networks.
1 Introduction
As the scale of the datasets and parameterized models used to perform computation over data
continues to grow [ 62,51], distributing workloads across multiple devices becomes essential for
enabling progress. The choice of architecture for large-scale training and inference must not only
make the best use of computational and memory resources, but also contend with the fact that
communication may become a bottleneck [ 97]. This is particularly pertinent as models grow so large
that they cannot rely on high-bandwidth interconnects within datacenters [ 17], but are instead trained
across multiple datacenters [ 108]. When using modern optical interconnects, classical computers
exchange bits represented by light. This however does not fully utilize the potential of the physical
substrate; given suitable computational capabilities and algorithms, the quantum nature of light
can be harnessed as a powerful communication resource. Here we show that for a broad class
of parameterized models, if quantum bits ( qubits ) are communicated instead of classical bits, an
exponential reduction in the communication required to perform inference and gradient-based training
can be achieved. This protocol additionally guarantees improved privacy of both the user data and
∗darg@google.com.
†Equal contribution.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).model parameters through natural features of quantum mechanics, without the need for additional
cryptographic or privacy protocols. To our knowledge, this is the first example of generic, exponential
quantum advantage on problems that occur naturally in the training and deployment of large machine
learning models. These types of communication advantages help scope the future roles and interplay
between quantum and classical communication for distributed machine learning.
Quantum computers promise dramatic speedups across a number of computational tasks, with perhaps
the most prominent example being the ability revolutionize our understanding of nature by enabling
the simulation of quantum systems, owing to the inherently quantum nature of many many physical
phenomena [ 39,72]. However, much of the data that one would like to compute with in practice
seems to come from an emergent classical world rather than directly exhibiting quantum properties.
While there are some well-known examples of exponential quantum speedups for classical problems,
most famously factoring [ 106] and related hidden subgroup problems [ 31], these tend to be isolated
and at times difficult to relate to practical applications that involve learning from data. In addition,
even though significant speedups are known for certain ubiquitous problems in machine learning
such as matrix inversion [ 48] and principal component analysis [ 73], the advantage is often lost when
including the cost of loading classical data into the quantum computer or of reading out the result into
classical memory. This is because the complexity of loading dense classical data into the amplitudes of
a quantum state (which is typically the encoding needed to obtain an exponential runtime advantage)
and of reading out the amplitudes from a quantum state into classical memory, are both polynomial in
the number of amplitudes [ 1]. In applications where an efficient data access model avoids the above
pitfalls, the complexity of quantum algorithms tends to depend on condition numbers of matrices
which scale with system size in a way that reduces or even eliminates any quantum advantage [ 82].
It is worth noting that much of the discussion about the impact of quantum technology on machine
learning has focused on computational advantage. However quantum resources are not only useful
in reducing computational complexity — they can also provide an advantage in communication
complexity, enabling exponential reductions in communication for some problems [ 101,15]. Inspired
by these results, we study a setting where quantum advantage in communication is possible across
a wide class of machine learning models. This advantage holds without requiring any sparsity
assumptions or elaborate data access models such as QRAM [42].
We focus on compositional distributed learning, known as pipelining [56,16]. While there are a num-
ber of strategies for distributing machine learning workloads that are influenced by the requirements
of different applications and hardware constraints [ 115,61], splitting up a computational graph in
a compositional fashion (Figure 1) is a common approach. We describe distributed, parameterized
quantum circuits that can be used to perform inference over data when distributed in this way, and
can be trained using gradient methods. The ideas we present can also be used to optimize models that
use certain forms of data parallelism (Appendix C). In principle, such circuits could be implemented
on quantum computers that are able to communicate quantum states.
We show the following:
•Even for simple distributed quantum circuits, there is an exponential quantum advantage
in communication for the problem of estimating the loss and the gradients of the loss
with respect to the parameters (Section 3). This additionally implies a privacy advantage
from Holevo’s bound (Appendix H). We also show that this is advantage is not a trivial
consequence of the data encoding used, since it does not hold for certain problems like
linear classification (Appendix E).
•We study a class of models that can efficiently approximate certain graph neural networks
(Section 4), and show that they both maintain the exponential communication advantage
and achieve performance comparable to standard classical models on common node and
graph classification benchmarks (Section 5).
•For certain distributed circuits, there is an exponential advantage in communication for the
entire training process, and not just for a single round of gradient estimation. This includes
circuits for fine-tuning using pre-trained features. The proof is based on convergence rates
for stochastic gradient descent under convexity assumptions (Appendix D).
•The ability to interleave multiple unitaries encoding nonlinear features of data enables
expressivity to grow exponentially with depth, and universal function approximation in
some settings. This implies that these models are highly expressive in contrast to popular
belief about linear restrictions in quantum neural networks (Appendix F).
22 Preliminaries
2.1 Large-scale learning problems and distributed computation
Figure 1: Left: Distributed, compositional computation. Dashed lines separate devices with com-
putational and storage resources. The circular nodes represent parameterized functions that are
allocated distinct hardware resources and are spatially separated, while the square nodes represent
data (yellow) and outputs corresponding to different tasks (green). The vertical axis represents time.
This framework of hardware allocation enables flexible modification of the model structure in a
task-dependent fashion. Right: Computation of gradient estimators gℓat different layers of a model
distributed across multiple devices by pipelining. Computing forward features µℓand backwards
features νℓ(also known as computing a forward or backward pass) requires a large amount of classical
communication (grey) but an exponentially smaller amount of quantum communication (yellow). L
is the classical loss function, and P0an operator whose expectation value with respect to a quantum
model gives the analogous loss function in the quantum case.
Pipelining is a commonly used method of distributing a machine learning workload, in which different
layers of a deep model are allocated distinct hardware resources [ 56,87]. Training and inference
then require communication of features between nodes. Pipelining enables flexible changes to the
model architecture in a task-dependent manner, since subsets of a large model can be combined
in an adaptive fashion to solve many downstream tasks. Additionally, pipelining allows sparse
activation of a subset of a model required to solve a task, and facilitates better use of heterogeneous
compute resources since it does not require storing identical copies of a large model. The potential
for large models to be easily fine-tuned to solve multiple tasks is well-known [ 25,20], and pipelined
architectures which facilitate this are the norm in the latest generation of large language models
[99,16]. Data parallelism, in contrast, involves storing multiple copies of the model on different
nodes, training each on a subsets of the data and exchanging information to synchronize parameter
updates. In practice, different parallelization strategies are combined in order to exploit trade-offs
between latency and throughput in a task-dependent fashion [ 115,61,97]. Distributed quantum
models were considered recently in [ 94], but the potential for quantum advantage in communication
in these settings was not discussed.
2.2 Communication complexity
Communication complexity [ 117,65,98] is the study of distributed computational problems using
a cost model that focuses on the communication required between players rather than the time or
computational complexity. The key object of study in this area is the tree induced by a communication
protocol whose nodes enumerate all possible communication histories and whose leaves correspond
to the outputs of the protocol. The product structure induced on the leaves of this tree as a function of
the inputs allows one to bound the depth of the tree from below, which gives an unconditional lower
bound on the communication complexity. The power of replacing classical bits of communication
with qubits has been the subject of extensive study [ 30,23,27]. For certain problems such as Hidden
Matching [ 15] and a variant of classification with deep linear models [ 101] an exponential quantum
communication advantage holds, while for other canonical problems such as Disjointness only a
polynomial advantage is possible [ 102]. Exponential advantage was also recently shown for the
problem of sampling from a distribution defined by the solution to a linear regression problem
3[83]. While there are many models of both quantum and classical communication, our results apply
torandomized classical communication complexity, wherein the players are allowed to exchange
random bits independent of their problem inputs, and are allowed to output an incorrect answer
with some probability (bounded away from 1/2for a problem with binary output). It is also worth
noting that communication advantages of the type we demonstrate can be naturally related to space
advantages in streaming algorithms that may be of interest even in settings that do not involve
distributed training [103].
At a glance, the development of networked quantum computers may seem much more challenging
than the already herculean task of building a fault tolerant quantum computer. However, for some
quantum network architectures, the existence of a long-lasting fault tolerant quantum memory as a
quantum repeater, may be the enabling component that lifts low rate shared entanglement to a fully
functional quantum network [ 86], and hence the timelines for small fault tolerant quantum computers
and quantum networks may be more coincident than it might seem at first. As such, it is well
motivated to consider potential communication advantages alongside computational advantages when
talking about the applications of fault tolerant quantum computers. In Appendix G we briefly survey
approaches to implementing quantum communication in practice, and the associated challenges.
In addition, while we largely restrict ourselves here to discussions of communication advantages, and
most other studies focus on purely computational advantages, there may be interesting advantages at
their intersection. For example, it is known that no quantum state built from a simple (or polynomial
complexity) circuit can confer an exponential communication advantage, however states made from
simple circuits can be made computationally difficult to distinguish [ 59]. Hence the use of quantum
pre-computation [ 57] and communication may confer advantages even when traditional computational
and communication cost models do not admit such advantages due to their restriction in scope.
3 Distributed learning with quantum resources
In this work we focus on parameterized models that are representative of the most common models
used and studied today in quantum machine learning, sometimes referred to as quantum neural net-
works [ 79,38,28,104]. We will use the standard Dirac notation of quantum mechanics throughout. A
summary of relevant notation and the fundamentals of quantum mechanics is provided in Appendix A.
We define a class models with parameters Θ, taking an input xwhich is a tensor of size N. The
models take the following general form:
Definition 3.1. {Aℓ(θA
ℓ, x)},{Bℓ(θB
ℓ, x)}forℓ∈ {1, . . . , L }are each a set of unitary matrices of
sizeN′×N′for some N′such that logN′=O(logN)3. The θA
ℓ, θB
ℓare vectors of Pparameters
each. For every ℓ, i, we assume that∂Aℓ
∂θA
ℓiis anti-hermitian and has two eigenvalues, and similarly for
Bℓ4.
The model we consider is defined by
|φ(Θ, x)⟩ ≡ 1Y
ℓ=LAℓ(θA
ℓ, x)Bℓ(θB
ℓ, x)!
|ψ(x)⟩, (3.1)
where ψ(x)is a fixed state of logN′qubits.
The loss function is given by
L(Θ, x)≡ ⟨φ(Θ, x)|P0|φ(Θ, x)⟩, (3.2)
where P0is a Pauli matrix that acts on the first qubit.
In standard linear algebra notation, the output of the model is a unit norm N′-dimensional complex
vector φL, defined recursively by
φ0=ψ(x), φ ℓ=Aℓ(θA
ℓ, x)Bℓ(θB
ℓ, x)φℓ−1, (3.3)
3We will consider some cases where N′=N, but will find it helpful at times to encode nonlinear features of
xin these unitaries, in which case we may have N′> N .
4The condition on the derivatives is in fact satisfied by many of the most common quantum neural network
architectures [ 28,34,104]. It is satisfied for example if Aℓ=QP
j=1eiαA
ℓjθA
ℓjPA
ℓjand the PA
ℓjare Pauli matrices,
while αA
ℓjare scalars. Such models are naturally amenable to implementation on quantum devices, and for
P=˜O(N2)any unitary over logN′qubits can be written in this form [90].
4where the entries of φLare represented by the amplitudes of a quantum state. The loss takes the
formL(Θ, x) = (φ∗
L)TP0φLwhere∗indicates the entrywise complex conjugate, and this definition
includes the standard L2loss as a special case.
Subsequently we omit the dependence on xandΘ(or subsets of it) to lighten notation, and consider
special cases where only subsets of the unitaries depend on x, or where the unitaries take a particular
form and may not be parameterized. Denote by ∇A(B)Lthe entries of the gradient vector that
correspond to the parameters of {Aℓ}({Bℓ}).
In the special case where xin a unit norm N-dimensional vector, a simple choice of |ψ(x)⟩is the
amplitude encoding of x, given by
|ψ(x)⟩=|x⟩=N−1X
i=0xi|i⟩. (3.4)
However, despite its exponential compactness in representing the data, a naive implementation of the
simplest choice is restricted to representing quadratic features of the data that can offer no substantial
quantum advantage in a learning task [ 55], so the choice of data encoding is critical to the power of a
model. The interesting parameter regime for classical data and models is one where N, P are large,
while Lis relatively modest. For general unitaries P=O(N2), which matches the scaling of the
number of parameters in fully-connected networks. When the input tensor xis a batch of datapoints,
Nis equivalent to the product of batch size and input dimension.
The model in Definition 3.1 can be used to define distributed inference and learning problems by
dividing the input xand the parameterized unitaries between two players, Alice and Bob. We define
their respective inputs as follows:
Alice : |ψ(x)⟩,{Aℓ},
Bob : {Bℓ}.(3.5)
The problems of interest require that Alice and Bob compute certain joint functions of their inputs.
As a trivial base case, it is clear that in a communication cost model, all problems can be solved with
communication cost at most the size of the inputs times the number of parties, by a protocol in which
each party sends its inputs to all others. We will be interested in cases where one can do much better
by taking advantage of quantum communication.
Alice :
|ψ(x)⟩
.
Bob : B1(θB
1)A1(θA
1)
B2(θB
2)A2(θA
2)
Figure 2: Distributed quantum circuit implementing LforL= 2. Both Land its gradients with
respect to the parameters of the unitaries can be estimated with total communication that is polyloga-
rithmic in the size of the input data Nand the number of trainable parameters per unitary P.
Given the inputs eq. (3.5), we will be interested chiefly in the two problems specified below.
Problem 1 (Distributed Inference) .Alice and Bob each compute an estimate of ⟨φ|P0|φ⟩up to
additive error ε.
The straightforward algorithm for this problem, illustrated in fig. 2, requires Lrounds of communica-
tion. The other problem we consider is the following:
Problem 2 (Distributed Gradient Estimation) .Alice computes an estimate of ∇A⟨φ|P0|φ⟩, while
Bob computes an estimate of ∇B⟨φ|Z0|φ⟩, up to additive error εinL∞.
3.1 Communication complexity of inference and gradient estimation
We show that inference and gradient estimation are achievable with a logarithmic amount of quantum
communication, which will represent an exponential improvement over the classical cost for some
cases:
5Lemma 1. Problem 1 can be solved by communicating O(logN)qubits over O(L/ε2)rounds.
Proof: Appendix B.
Lemma 2. Problem 2 can be solved with probability greater than 1−δby communicating
˜O(logN(logP)2log(L/δ)/ε4)qubits over O(L2)rounds. The time and space complexity of the
algorithm is√
P Lpoly( N,logP, ε−1,log(1/δ)).
Proof: Appendix B.
This upper bound is obtained by simply noting that the problem of gradient estimation at every layer
can be reduced to a shadow tomography problem [7]:
Theorem 1 (Shadow Tomography [ 3] solved with Threshold Search [ 13]).For an unknown state
|ψ⟩oflogNqubits, given Kknown two-outcome measurements Ei, there is an explicit algorithm
that takes |ψ⟩⊗kas input, where k=˜O(log2KlogNlog(1/δ)/ε4), and produces estimates of
⟨ψ|Ei|ψ⟩for all iup to additive error εwith probability greater than 1−δ.˜Ohides subdominant
polylog factors.
Using reductions from known problems in communication complexity, we can show that the amount
of classical communication required to solve this problem is polynomial in the size of the input, and
additionally give a lower bound on the number of rounds of communication required by any quantum
or classical algorithm:
Lemma 3. i)The classical randomized communication complexity of Problem 1 and Problem 2
withε <1/2isΩ(max(√
N, L)).5
ii)Any algorithm (quantum or classical) for Problem 1 or Problem 2 requires either Ω(L)rounds
of communication or Ω(N/L4)qubits (or bits) of communication.
Proof: Appendix B
The implication of the second result in Lemma 3 is that Ω(L)rounds of communication are necessary
in order to obtain an exponential communication advantage for small L, since otherwise the number
of qubits of communication required can scale linearly with N.
The combination of Lemma 1, Lemma 2 and Lemma 3 immediately implies exponential savings in
communication for gradient estimation and inference:
Theorem 2. IfL=O(polylog( N)), P=O(poly( N))and sufficiently large N, solving Problem 1
or Problem 2 with nontrivial success probability requires Ω(√
N)bits of classical communication,
while O(polylog( N,1/δ)poly(1 /ε))qubits of communication suffice to solve these problems with
probability at least 1−δ.
The regime where L=O(polylog( N))is relevant for many classes of machine learning models. The
required overhead in terms of time and space is only polynomial when compared to the straightforward
classical algorithms for these problems.
The distribution of the model as in eq. (3.5) is an example of pipelining. Data parallelism is another
common approach to distributed machine learning in which subsets of the data are distributed to
identical copies of the model. In Appendix C we show that it can also be implemented using quantum
circuits, which can then trained using gradient descent requiring quantum communication that is
logarithmic in the number of parameters and input size.
Quantum advantage is possible in these problems because there is a bound on the complexity of
the final output, whether it be correlated elements of the gradient up to some finite error or the
low-dimensional output of a model. This might lead one to believe that whenever the output takes
such a form, encoding the data in the amplitudes of a quantum state will trivially give an exponential
advantage in communication complexity. We show however that the situation is slightly more
nuanced, by considering the problem of inference with a linear model:
5The inputs to Problem 1 and Problem 2 are defined in terms of real numbers, which is seemingly incompatible
with the setting of communication complexity which typically deals with finite inputs. However, similar (but
slightly worse) lower bounds hold for discretized analogs of these problem that use O(logN)bits to represent
the real numbers [101].
6Lemma 4. For the problem of distributed linear classification, there can be no exponential advantage
in using quantum communication in place of classical communication.
The precise statement and proof of this result are presented in Appendix E. This result also highlights
that the worst case lower bounds such as Lemma 3 may not hold for circuits with certain low-
dimensional or other simplifying structure.
4 Graph neural networks
The communication advantages in the previous section apply to relatively unstructured data and
quantum circuits (essentially the only structure in the problem is related to the promise of the
vector-in-subspace problem [ 101]), and it is a priori unclear how relevant they are to circuits that
approximate useful neural networks, or act on structured data. Here we consider a class of shallow
graph neural networks that achieve good performance on node classification tasks on large graphs
[40]. We prove that an exponential quantum communication advantage still holds for this class of
models.
Consider a graph with Nnodes. Define a local message passing operator on the graph Awhich may
be the normalized Laplacian or some other operator. Given some N×D0matrix of graph features
X, we consider models of the following form:
Φ(X) =P(σ(AXW 1))W2 (4.1)
where W1∈RD0×D1, W2∈RD1×D2are parameter matrices, σis a non-linearity and Pis a sum
pooling operator acting on the first index of its input, which can be represented as multiplication by an
N/t×Nmatrix ˜P. Since we would like a scalar output and a nonlinearity that can be implemented
on a quantum computer, we instead compute
φ(X) = tr [ P(σ(AXW 1))W2], (4.2)
with
σ(x) =ax2+bx+c (4.3)
andW2∈RD1×N/twhere tis the size of the pooling window.
This allows us to define the following problem:
Problem 3 (Quadratic graph network inference ( QGNIN,t)).Alice is given X, Bob is given
A, W 1, W2. Only Alice is allowed to send messages. Their goal is to estimate φ(X/||X||F)to
additive error ε.
This models a scenario where only Bob has access to the connectivity of the graph, while Alice has
access to the graph features. The normalization ensures that the choice of Xdoes not introduce a
dependence of the final output on N.
In the following, we denote by R→
εandQ→
εthe classical (public key randomized) communication
complexity and quantum communication complexity respectively. We show:
Lemma 5. R→
ε(QGNIN,t) = Ω(p
N/t)for any ε≤1
4(t+1
2)(t+3
2).
Proof: Appendix B
Lemma 6. Q→
ε(QGNIN,t) =O((|a|α2+|b|α)W2˜P
∞log(ND 0)/ε)where α=∥W1∥∥A∥.
Proof: Appendix B
If this upper bound was a polynomial function of N, it would imply that an exponential communica-
tion advantage is impossible. For the parameter choices that realize classical communication lower
bound, this is not the case, implying the following:
Lemma 7. An exponential quantum advantage in communication holds for solving the inference
problem QGNIN,tup to error ε≤1
4(t+1
2)(t+3
2), for any tsuch that t= polylog( N).
7Table 1: Test Accuracy for Node Classification and Decision Problem. Replacing PReLU with a
polynomial of degree 2 causes a slight reduction in accuracy (less than 1%) for both node classification
and decision problem across all datasets.
Node Classification Decision Problem
Model OGBN-Products Reddit Cora OGBN-Products Reddit Cora
SIGN (PReLU) 79.48 ±0.07 96.55 ±0.02 78.84 ±0.37 84.39 ±1.73 90.33 ±0.33 88.10 ±5.61
SIGN (Poly) 78.51 ±0.05 96.31 ±0.03 78.69 ±0.26 83.70 ±1.48 89.37 ±0.60 87.14 ±3.92
Proof: Appendix B. Note that this exponential advantage does not hold only for a single setting of the
model weights, but rather for the entire family of models that can be used to solve f−BHP N,tfor
functions fthat satisfy eq. (B.24).
Note that generically, one would not expect the numerator in the upper bound of Lemma 6 to scale
polynomially with N. IfAis for example a normalized graph Laplacian then ||A|| ≤2. If we use
a standard initialization scheme for the weights (say Gaussians with variance 1/(nin+nout)), the
upper bound scales like O((|a|+|b|) log( ND 0)poly( t, D0, D1)/ε)in expectation. Note that if the
model output decays polynomially with N, the upper bound will not be useful since one would need
to choose εto be inverse polynomial in N. This could happen for example in a classification task
considered in Section 5.2.1 when the classes are exactly balanced, or when the network is untrained
and not sensitive to the structure in the data. While it is difficult to argue analytically about the scaling
out the network output or the norms of the weight matrices after training due to the nonlinearity of
the dynamics, we empirically compute these and find that they remain controlled for the datasets we
study (see Appendix J.3).
In Section 5, we show that models of the form studied here achieve good performance on standard
benchmarks, commensurate with state of the art models. Of particular relevance are the graph
classification problems considered in Section 5.2.1, where the output takes the form eq. (4.2).
5 Experimental results
5.1 Model
We evaluate our model6(as defined in Equation (4.1)) on several graph tasks using common bench-
marks and the DGL library [ 113]. We use the SIGN model proposed by [ 40] as a baseline. The SIGN
model can be seen as an instance of our model where the message passing operator Arepresents a
column stack of Rhops, the original features of Xare duplicated Rtimes and W1is a block diagonal
matrix. In Section 5.2 we simply replace the PReLU activation function with a second-degree
polynomial with trainable coefficients [ 80] and compare the models on three node classification tasks.
In Section 5.3, we implement a more general form of SIGN by relaxing W1to be a dense matrix and
evaluate our model over several graph-classification datasets.
5.2 Node classification
We evaluate our model on three public node classification datasets: OGBN-Products [ 54], Reddit
[47], and Cora [ 78]. For both the baseline and polynomial model, we use SIGN with 5 hops of the
neighbor averaging operator. We train on each dataset for 1000 epochs using Adam optimizer and
report the test accuracy averaged on 10 runs (full details in Appendix J). Our results in Table 3 show
that replacing the PReLU activation function with a second-degree polynomial causes a reduction of
less than 1% on all of the tested datasets.
5.2.1 Decision problems
We reduce the node classification task into a binary graph classification task by proposing the
following decision problem: for a pair of classes (c1, c2), return 1 if c1has more nodes; otherwise,
return 0. We solve this task for each pair of classes by summing the node classification model output
across all nodes and choosing the class with the higher score. We use the node classification training,
6Our code will be available at github.com/hmichaeli/quantum_gnns/.
8Table 2: Graph Classification Test Accuracy. Our model achieves comparable results to GIN and
other known models on most datasets (see full table in Table 5).
Dataset
Model MUTAG PTC NCI1 PROTEINS COLLAB IMDB-M REDDIT-M
GIN [114] 89.40 ±5.60 64.60 ±7.0 82.17 ±1.7 76.2 ±2.8 80.2 ±1.90 52.3 ±2.8 57.5 ±1.5
DropGIN[92] 90.4 ±7.0 66.3 ±8.6 - 76.3 ±6.1 - 51.4 ±2.8 -
DGCNN[118] 85.8 ±1.7 58.6 ±2.5 - 75.5 ±0.9 - 47.8 ±0.9 -
U2GNN [89] 89.97 ±3.65 69.63 ±3.60 - 78.53 ±4.07 77.84 ±1.48 53.60 ±3.53 -
HGP-SL[119] - - 78.45 ±0.77 84.91 ±1.62 - - -
WKPI[120] 88.30 ±2.6 68.10 ±2.4 87.5 ±0.5 78.5 ±0.4 - 49.5 ±0.4 59.5 ±0.6
SIGN (ours) 92.02 ±6.45 68.0 ±8.17 77.25 ±1.42 76.55 ±5.10 81.82 ±1.42 53.13 ±3.01 54.09 ±1.76
choose the model with the highest validation accuracy on the graph classification task, and report its
accuracy on the test sets. The model output in this form is given by eq. (4.1).
5.3 Graph classification
We evaluate our model on several graph classification benchmarks: bioinformatics datasets (MUTAG,
PTC, NCI1, PROTEINS) [ 105,50,35,21] and social networks (COLLAB, IMDB-BINARY , IMDB-
MULTI, REDDIT-BINARY , REDDIT-MULTI) [ 116]. For the bioinformatics datasets, we use the
standard categorical node features. As proposed in [ 114], we use one-hot encoding of the node degree
as node features for the COLLAB and IMDB datasets, and for REDDIT datasets all nodes are set
with an identical scalar feature of 1. We convert the polynomial SIGN model in Section 5.2 into a
graph classification model by inserting a SumPool operator as described in Equation (4.1). We use
the sign diffusion operator [ 113] and stack Riinstances of each of its four message passing operators,
where {Ri}4
i=1are selected during a hyperparameter tuning, as well as the hidden dimension size
and optimization setting (see Appendix J for more details). We follow the validation regime proposed
by [114]; we perform 10-fold cross-validation, train each fold for 350 epochs using Adam optimizer,
and report in Table 2 the maximal value and standard-deviation of the averaged validation accuracy
curve. For all datasets, except for REDDIT, our model achieves comparable to or better than other
commonly used models, despite most of them using multiple layers. While the results show that on
most datasets our shallow architecture suffices given sufficient width in the message passing and
hidden layer, we hypothesize that datasets without any node features (such as REDDIT) require at
least two layers of message passing.
6 Discussion
This work constitutes a preliminary investigation into a generic class of quantum circuits that has
the potential for enabling an exponential communication advantage in problems of classical data
processing including training and inference with large parameterized models over large datasets,
with inherent privacy advantages. Communication constraints may become even more relevant
if such models are trained on data that is obtained by inherently distributed interaction with the
physical world [ 37]. The ability to compute using data with privacy guarantees can be potentially
applied to proprietary data. This could become highly desirable even in the near future as the rate of
publicly-available data production appears to be outstripped by the growth rate of training sets of
large language models [110].
A limitation of the current results is that it’s unclear to what extent powerful neural networks can
be approximated using quantum circuits, even though we provide positive evidence in the form of
the results on graph networks in Section 4. Additionally, the advantages we study require deep
(poly( N)), fault-tolerant quantum circuits. While this is a common feature of problems for which
quantum communication advantages hold, the overhead of quantum error-correction in such circuits
may be considerable. Detailed resource estimates would be necessary to understand better the
practicality of this approach for achieving useful quantum advantage. Our results naturally raise
further questions regarding the expressive power and trainability of these types of circuits, which
may be of independent interest. We collect some of these in Appendix I.
9Acknowledgements
The authors would like to thank Amira Abbas, Ryan Babbush, Dave Bacon and Robbie King for
helpful discussions and comments on the manuscript. The research of DS was Funded by the European
Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the
author only and do not necessarily reflect those of the European Union or the European Research
Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be
held responsible for them. DS also acknowledges the support of the Schmidt Career Advancement
Chair in AI.
10References
[1] Scott Aaronson. Read the fine print. Nat. Phys. , 11(4):291–293, 2015.
[2]Scott Aaronson. Introduction to quantum information science. https://www.
scottaaronson.com/qclec.pdf , 2017.
[3]Scott Aaronson. Shadow tomography of quantum states. In Proceedings of the 50th Annual
ACM SIGACT Symposium on Theory of Computing , pages 325–338, New York, NY , USA,
2018. ACM.
[4]Scott Aaronson, Andris Ambainis, J ¯anis Iraids, Martins Kokainis, and Juris Smotrovs. Poly-
nomials, quantum query complexity, and grothendieck’s inequality. In Proceedings of the
31st Conference on Computational Complexity , number Article 25 in CCC ’16, pages 1–19,
Dagstuhl, DEU, 2016. Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik.
[5]Scott Aaronson, Xinyi Chen, Elad Hazan, Satyen Kale, and Ashwin Nayak. Online learning
of quantum states. J. Stat. Mech. , 2019(12):124019, 2019.
[6]Scott Aaronson and Guy N Rothblum. Gentle measurement of quantum states and differential
privacy. In STOC’19—Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of
Computing , pages 322–333. ACM, New York, 2019.
[7]Amira Abbas, Robbie King, Hsin-Yuan Huang, W Huggins, R Movassagh, D Gilboa, and
J McClean. On quantum backpropagation, information reuse, and cheating measurement
collapse. Neural Inf Process Syst , abs/2305.13362:44792–44819, 2023.
[8]Dimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary
coins. J. Comput. System Sci. , 66(4):671–687, 2003.
[9]Alekh Agarwal, Peter L Bartlett, Pradeep Ravikumar, and Martin J Wainwright. Information-
theoretic lower bounds on the oracle complexity of stochastic convex optimization. arXiv
[stat.ML] , 2010.
[10] Srinivasan Arunachalam, Uma Girish, and Noam Lifshitz. One clean qubit suffices for quantum
communication advantage. arXiv [quant-ph] , 2023.
[11] Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C Bardin, Rami Barends, Rupak
Biswas, Sergio Boixo, Fernando G S L Brandao, David A Buell, Brian Burkett, Yu Chen,
Zijun Chen, Ben Chiaro, Roberto Collins, William Courtney, Andrew Dunsworth, Edward
Farhi, Brooks Foxen, Austin Fowler, Craig Gidney, Marissa Giustina, Rob Graff, Keith Guerin,
Steve Habegger, Matthew P Harrigan, Michael J Hartmann, Alan Ho, Markus Hoffmann,
Trent Huang, Travis S Humble, Sergei V Isakov, Evan Jeffrey, Zhang Jiang, Dvir Kafri,
Kostyantyn Kechedzhi, Julian Kelly, Paul V Klimov, Sergey Knysh, Alexander Korotkov,
Fedor Kostritsa, David Landhuis, Mike Lindmark, Erik Lucero, Dmitry Lyakh, Salvatore
Mandrà, Jarrod R McClean, Matthew McEwen, Anthony Megrant, Xiao Mi, Kristel Michielsen,
Masoud Mohseni, Josh Mutus, Ofer Naaman, Matthew Neeley, Charles Neill, Murphy Yuezhen
Niu, Eric Ostby, Andre Petukhov, John C Platt, Chris Quintana, Eleanor G Rieffel, Pedram
Roushan, Nicholas C Rubin, Daniel Sank, Kevin J Satzinger, Vadim Smelyanskiy, Kevin J
Sung, Matthew D Trevithick, Amit Vainsencher, Benjamin Villalonga, Theodore White,
Z Jamie Yao, Ping Yeh, Adam Zalcman, Hartmut Neven, and John M Martinis. Quantum
supremacy using a programmable superconducting processor. Nature , 574(7779):505–510,
2019.
[12] Koji Azuma, Sophia E Economou, David Elkouss, Paul Hilaire, Liang Jiang, Hoi-Kwong Lo,
and Ilan Tzitrin. Quantum repeaters: From quantum networks to the quantum internet. arXiv
[quant-ph] , 2022.
[13] Costin B ˘adescu and Ryan O’Donnell. Improved quantum data analysis. In Proceedings of the
53rd Annual ACM SIGACT Symposium on Theory of Computing , pages 1398–1411, 2021.
[14] Krishna C Balram and Kartik Srinivasan. Piezoelectric optomechanical approaches for ef-
ficient quantum microwave-to-optical signal transduction: the need for co-design. arXiv
[physics.optics] , 2021.
11[15] Ziv Bar-Yossef, T S Jayram, and Iordanis Kerenidis. Exponential separation of quantum and
classical one-way communication complexity. SIAM J. Comput. , 38(1):366–384, 2008.
[16] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Dan Hurt,
Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan Saeta, Parker Schuh,
Ryan Sepassi, Laurent El Shafey, Chandramohan A Thekkath, and Yonghui Wu. Pathways:
Asynchronous distributed dataflow for ML. arXiv [cs.DC] , 2022.
[17] Luiz André Barroso, Jimmy Clidaras, and Urs Hölzle. The datacenter as a computer: An
introduction to the design of warehouse-scale machines, second edition. Synth. Lect. Comput.
Archit. , 8(3):1–154, 2013.
[18] C H Bennett, G Brassard, C Crépeau, R Jozsa, A Peres, and W K Wootters. Teleporting an
unknown quantum state via dual classical and einstein-podolsky-rosen channels. Phys. Rev.
Lett., 70(13):1895–1899, 1993.
[19] Charles H Bennett, Gilles Brassard, Sandu Popescu, Benjamin Schumacher, John A Smolin,
and William K Wootters. Purification of noisy entanglement and faithful teleportation via
noisy channels. arXiv [quant-ph] , 1995.
[20] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik
Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen,
Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya,
Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn,
Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha,
Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E Ho, Jenny Hong, Kyle Hsu, Jing
Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti,
Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna,
Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec,
Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D Manning,
Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak
Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko,
Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance,
Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani,
Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam,
Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W Thomas, Florian
Tramèr, Rose E Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie,
Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang,
Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of
foundation models. arXiv [cs.LG] , 2021.
[21] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vishwanathan, Alex J
Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics ,
21(suppl_1):i47–i56, 2005.
[22] Fernando G S Brandão, Amir Kalev, Tongyang Li, Cedric Yen-Yu Lin, Krysta M Svore, and
Xiaodi Wu. Quantum SDP solvers: Large speed-ups, optimality, and applications to quantum
learning. arXiv [quant-ph] , 2017.
[23] Gilles Brassard. Quantum communication complexity (a survey). arXiv [quant-ph] , 2001.
[24] Adam R Brown and Leonard Susskind. The second law of quantum complexity. arXiv [hep-th] ,
2017.
[25] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel
Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Advances in
Neural Information Processing Systems , 33:1877–1901, 2020.
12[26] Sébastien Bubeck. Convex optimization: Algorithms and complexity. arXiv [math.OC] , 2014.
[27] Harry Buhrman, Richard Cleve, Serge Massar, and Ronald de Wolf. Non-locality and commu-
nication complexity. arXiv [quant-ph] , 2009.
[28] M Cerezo, Andrew Arrasmith, Ryan Babbush, Simon C Benjamin, Suguru Endo, Keisuke
Fujii, Jarrod R McClean, Kosuke Mitarai, Xiao Yuan, Lukasz Cincio, and Patrick J Coles.
Variational quantum algorithms. arXiv [quant-ph] , 2020.
[29] Shantanav Chakraborty, András Gilyén, and Stacey Jeffery. The power of block-encoded
matrix powers: improved regression techniques via faster hamiltonian simulation. arXiv
[quant-ph] , 2018.
[30] A Chi-Chih Yao. Quantum circuit complexity. In Proceedings of 1993 IEEE 34th Annual
Foundations of Computer Science , pages 352–361, 1993.
[31] Andrew M Childs and Wim van Dam. Quantum algorithms for algebraic problems. arXiv
[quant-ph] , 2008.
[32] Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A
tensor analysis. arXiv [cs.NE] , 2015.
[33] Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through
pooling geometry. arXiv [cs.NE] , 2016.
[34] Gavin E Crooks. Gradients of parameterized quantum gates using the parameter-shift rule and
gate decomposition. arXiv [quant-ph] , 2019.
[35] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and
Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic
nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of
medicinal chemistry , 34(2):786–797, 1991.
[36] João F Doriguello and Ashley Montanaro. Exponential quantum communication reductions
from generalizations of the boolean hidden matching problem. arXiv [quant-ph] , 2020.
[37] Danny Driess, Fei Xia, Mehdi S M Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian
Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen
Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol
Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. PaLM-
E: An embodied multimodal language model. arXiv [cs.LG] , 2023.
[38] Edward Farhi and Hartmut Neven. Classification with quantum neural networks on near term
processors. arXiv [quant-ph] , 2018.
[39] Richard P Feynman. Simulating physics with computers. Int. J. Theor. Phys. , 21(6):467–488,
1982.
[40] Fabrizio Frasca, Emanuele Rossi, Davide Eynard, Ben Chamberlain, Michael Bronstein, and
Federico Monti. SIGN: Scalable inception graph neural networks. arXiv [cs.LG] , 2020.
[41] András Gilyén, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singular value
transformation and beyond: exponential improvements for quantum matrix arithmetics. arXiv
[quant-ph] , 2018.
[42] Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum random access memory.
Phys. Rev. Lett. , 100(16):160501, 2008.
[43] Lukas Gonon and Antoine Jacquier. Universal approximation theorem and error bounds for
quantum neural networks and quantum reservoirs. arXiv [quant-ph] , 2023.
[44] Google Quantum AI. Suppressing quantum errors by scaling a surface code logical qubit.
Nature , 614(7949):676–681, 2023.
13[45] Goren Gordon and Gustavo Rigolin. Generalized teleportation protocol. arXiv [quant-ph] ,
2005.
[46] Robert Gower, Donald Goldfarb, and Peter Richtarik. Stochastic block BFGS: Squeezing more
curvature out of data. In Maria Florina Balcan and Kilian Q Weinberger, editors, Proceedings
of The 33rd International Conference on Machine Learning , volume 48 of Proceedings of
Machine Learning Research , pages 1869–1878, New York, New York, USA, 2016. PMLR.
[47] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs, 2018.
[48] Aram W Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems
of equations. Phys. Rev. Lett. , 103(15):150502, 2009.
[49] Aram W Harrow and John C Napp. Low-depth gradient measurements can improve conver-
gence in variational hybrid quantum-classical algorithms. Phys. Rev. Lett. , 126(14):140502,
2021.
[50] Christoph Helma, Ross D. King, Stefan Kramer, and Ashwin Srinivasan. The predictive
toxicology challenge 2000–2001. Bioinformatics , 17(1):107–108, 2001.
[51] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia
Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W Rae, Oriol Vinyals, and Laurent
Sifre. Training compute-optimal large language models. arXiv [cs.CL] , 2022.
[52] Alexander Semenovich Holevo. Bounds for the quantity of information transmitted by a
quantum communication channel. Problemy Peredachi Informatsii , 1973.
[53] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classifica-
tion. arXiv [cs.CL] , 2018.
[54] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele
Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs,
2021.
[55] Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo,
Hartmut Neven, and Jarrod R McClean. Power of data in quantum machine learning. Nat.
Commun. , 12(1):2631, 2021.
[56] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen,
Hyoukjoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and Zhifeng Chen. GPipe: Efficient
training of giant neural networks using pipeline parallelism. arXiv [cs.CV] , 2018.
[57] William J Huggins and Jarrod R McClean. Accelerating quantum algorithms with precompu-
tation. arXiv [quant-ph] , 2023.
[58] Rahul Jain, Jaikumar Radhakrishnan, and Pranab Sen. The quantum communication com-
plexity of the pointer chasing problem: The bit version. In FST TCS 2002: Foundations of
Software Technology and Theoretical Computer Science , Lecture notes in computer science,
pages 218–229. Springer Berlin Heidelberg, Berlin, Heidelberg, 2002.
[59] Zhengfeng Ji, Yi-Kai Liu, and Fang Song. Pseudorandom quantum states. In Lecture Notes in
Computer Science , Lecture notes in computer science, pages 126–152. Springer International
Publishing, Cham, 2018.
[60] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive
variance reduction. In NeurIPS , 2013.
[61] Norman P Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant
Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei
Zhou, and David Patterson. TPU v4: An optically reconfigurable supercomputer for machine
learning with hardware support for embeddings. arXiv [cs.AR] , 2023.
14[62] J Kaplan, S McCandlish, T Henighan, T B Brown, and others. Scaling laws for neural language
models. arXiv preprint arXiv , 2020.
[63] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers
are RNNs: Fast autoregressive transformers with linear attention. arXiv [cs.LG] , 2020.
[64] V Krutyanskiy, M Galli, V Krcmarsky, S Baier, D A Fioretto, Y Pu, A Mazloom, P Sekatski,
M Canteri, M Teller, J Schupp, J Bate, M Meraner, N Sangouard, B P Lanyon, and T E
Northup. Entanglement of trapped-ion qubits separated by 230 meters. arXiv [quant-ph] ,
2022.
[65] Eyal Kushilevitz and Noam Nisan. Communication Complexity . Cambridge University Press,
Cambridge, England, 2011.
[66] Nikolai Lauk, Neil Sinclair, Shabir Barzanjeh, Jacob P Covey, Mark Saffman, Maria Spirop-
ulu, and Christoph Simon. Perspectives on quantum transduction. Quantum Sci. Technol. ,
5(2):020501, 2020.
[67] Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an
exponential convergence rate for finite training sets. arXiv [math.OC] , 2012.
[68] James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. FNet: Mixing tokens
with fourier transforms. arXiv [cs.CL] , 2021.
[69] Yoav Levine, Or Sharir, Alon Ziv, and Amnon Shashua. On the long-term memory of deep
recurrent networks. arXiv [cs.LG] , 2017.
[70] Yoav Levine, Noam Wies, Or Sharir, Hofit Bata, and Amnon Shashua. The depth-to-width
interplay in self-attention. arXiv [cs.LG] , 2020.
[71] Bo Li, Yuan Cao, Yu-Huai Li, Wen-Qi Cai, Wei-Yue Liu, Ji-Gang Ren, Sheng-Kai Liao,
Hui-Nan Wu, Shuang-Lin Li, Li Li, Nai-Le Liu, Chao-Yang Lu, Juan Yin, Yu-Ao Chen,
Cheng-Zhi Peng, and Jian-Wei Pan. Quantum state transfer over 1200 km assisted by prior
distributed entanglement. Phys. Rev. Lett. , 128(17):170501, 2022.
[72] S Lloyd. Universal quantum simulators. Science , 273(5278):1073–1078, 1996.
[73] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal component analysis.
Nat. Phys. , 10(9):631–633, 2014.
[74] Guang Hao Low and Isaac L Chuang. Optimal hamiltonian simulation by quantum signal
processing. Phys. Rev. Lett. , 118(1):010501, 2017.
[75] P Magnard, S Storz, P Kurpiers, J Schär, F Marxer, J Lütolf, T Walter, J-C Besse, M Gabureac,
K Reuer, A Akin, B Royer, A Blais, and A Wallraff. Microwave quantum link between
superconducting circuits housed in spatially separated cryogenic systems. Phys. Rev. Lett. ,
125(26):260502, 2020.
[76] Vitaly Maiorov and Allan Pinkus. Lower bounds for approximation by MLP neural networks.
Neurocomputing , 25(1):81–91, 1999.
[77] John M Martyn, Zane M Rossi, Andrew K Tan, and Isaac L Chuang. A grand unification of
quantum algorithms. arXiv [quant-ph] , 2021.
[78] Andrew McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the
construction of internet portals with machine learning. Inf. Retr. , 3(2):127–163, 2000.
[79] Jarrod R McClean, Jonathan Romero, Ryan Babbush, and Alán Aspuru-Guzik. The theory of
variational hybrid quantum-classical algorithms. arXiv [quant-ph] , 2015.
[80] Hagay Michaeli, Tomer Michaeli, and Daniel Soudry. Alias-free convnets: Fractional shift
invariance via polynomial activations. arXiv [cs.CV] , 2023.
[81] Boris Mityagin. The zero set of a real analytic function. arXiv [math.CA] , 2015.
15[82] Ashley Montanaro and Sam Pallister. Quantum algorithms and the finite element method.
arXiv [quant-ph] , 2015.
[83] Ashley Montanaro and Changpeng Shao. Quantum communication complexity of linear
regression. arXiv preprint arXiv:2210.01601 , 2022.
[84] Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic
L-BFGS algorithm. In Arthur Gretton and Christian C Robert, editors, Proceedings of the 19th
International Conference on Artificial Intelligence and Statistics , volume 51 of Proceedings of
Machine Learning Research , pages 249–258, Cadiz, Spain, 2016. PMLR.
[85] Danial Motlagh and Nathan Wiebe. Generalized quantum signal processing. arXiv [quant-ph] ,
2023.
[86] William J Munro, Koji Azuma, Kiyoshi Tamaki, and Kae Nemoto. Inside quantum repeaters.
IEEE J. Sel. Top. Quantum Electron. , 21(3):78–90, 2015.
[87] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur,
Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. PipeDream: generalized pipeline
parallelism for DNN training. In Proceedings of the 27th ACM Symposium on Operating
Systems Principles , SOSP ’19, pages 1–15, New York, NY , USA, 2019. Association for
Computing Machinery.
[88] Ashwin Nayak and Felix Wu. The quantum query complexity of approximating the median
and related statistics. arXiv [quant-ph] , 1998.
[89] Dai Quoc Nguyen, Tu Dinh Nguyen, and Dinh Phung. Universal graph transformer self-
attention networks, 2022.
[90] Michael A Nielsen and Isaac L Chuang. Quantum Computation and Quantum Information:
10th Anniversary Edition . Cambridge University Press, 2010.
[91] Brad G Osgood. Lectures on the Fourier Transform and Its Applications (Pure and Applied
Undergraduate Texts) (Pure and Applied Undergraduate Texts, 33) . American Mathematical
Society, 2019.
[92] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Random
dropouts increase the expressiveness of graph neural networks, 2021.
[93] Adrián Pérez-Salinas, Alba Cervera-Lierta, Elies Gil-Fuster, and José I Latorre. Data re-
uploading for a universal quantum classifier. arXiv [quant-ph] , 2019.
[94] Lirandë Pira and Chris Ferrie. An invitation to distributed quantum neural networks. Quantum
Machine Intelligence , 5(2):1–24, 2023.
[95] M Pompili, S L N Hermans, S Baier, H K C Beukers, P C Humphreys, R N Schouten, R F L
Vermeulen, M J Tiggelman, L Dos Santos Martins, B Dirkse, S Wehner, and R Hanson. Realiza-
tion of a multinode quantum network of remote solid-state qubits. Science , 372(6539):259–264,
2021.
[96] Stephen J Ponzio, Jaikumar Radhakrishnan, and S Venkatesh. The communication complexity
of pointer chasing. J. Comput. System Sci. , 62(2):323–355, 2001.
[97] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm
Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling
transformer inference. arXiv [cs.LG] , 2022.
[98] Anup Rao and Amir Yehudayoff. Communication Complexity and Applications . Cambridge
University Press, 2020.
[99] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. DeepSpeed: System
optimizations enable training deep learning models with over 100 billion parameters. In
Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
& Data Mining , KDD ’20, pages 3505–3506, New York, NY , USA, 2020. Association for
Computing Machinery.
16[100] Arthur G Rattew and Patrick Rebentrost. Non-linear transformations of quantum amplitudes:
Exponential improvement, generalization, and applications. arXiv [quant-ph] , 2023.
[101] Ran Raz. Exponential separation of quantum and classical communication complexity. In
Proceedings of the thirty-first annual ACM symposium on Theory of Computing , STOC ’99,
pages 358–367, New York, NY , USA, 1999. Association for Computing Machinery.
[102] Alexander Razborov. Quantum communication complexity of symmetric predicates. arXiv
[quant-ph] , 2002.
[103] Tim Roughgarden. Communication complexity (for algorithm designers). arXiv [cs.CC] ,
2015.
[104] Maria Schuld, Ryan Sweke, and Johannes Jakob Meyer. The effect of data encoding on the
expressive power of variational quantum machine learning models. arXiv [quant-ph] , 2020.
[105] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M.
Borgwardt. Weisfeiler-lehman graph kernels. J. Mach. Learn. Res. , 12:2539–2561, 2011.
[106] P W Shor. Algorithms for quantum computation: discrete logarithms and factoring. In
Proceedings 35th Annual Symposium on Foundations of Computer Science . IEEE Comput.
Soc. Press, 1994.
[107] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang,
and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv
[cs.CL] , 2023.
[108] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui
Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, and Others. Gemini: a
family of highly capable multimodal models. arXiv preprint arXiv:2312. 11805 , 2023.
[109] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
L Ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I Guyon, U V Luxburg,
S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett, editors, Advances in Neural
Information Processing Systems , volume 30, pages 5998–6008. Curran Associates, Inc., 2017.
[110] Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and
Anson Ho. Will we run out of data? an analysis of the limits of scaling datasets in machine
learning. arXiv [cs.LG] , 2022.
[111] A J Walker. New fast method for generating discrete random numbers with arbitrary frequency
distributions. Electron. Lett. , 10(8):127–128, 1974.
[112] Changqing Wang, Ivan Gonin, Anna Grassellino, Sergey Kazakov, Alexander Romanenko,
Vyacheslav P Yakovlev, and Silvia Zorzetti. High-efficiency microwave-optical quantum
transduction based on a cavity electro-optic superconducting system with long coherence time.
npj Quantum Information , 8(1):1–10, 2022.
[113] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma,
Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang.
Deep graph library: A graph-centric, highly-performant package for graph neural networks.
arXiv preprint arXiv:1909.01315 , 2019.
[114] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks?, 2019.
[115] Yuanzhong Xu, Hyoukjoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul
Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam
Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen. GSPMD: General and
scalable parallelization for ML computation graphs. arXiv [cs.DC] , 2021.
[116] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM
SIGKDD international conference on knowledge discovery and data mining , pages 1365–1374,
2015.
17[117] Andrew Chi-Chih Yao. Some complexity questions related to distributive comput-
ing(preliminary report). In Proceedings of the eleventh annual ACM symposium on Theory of
computing , STOC ’79, pages 209–213, New York, NY , USA, 1979. Association for Computing
Machinery.
[118] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
architecture for graph classification. In Proceedings of the AAAI conference on artificial
intelligence , volume 32, 2018.
[119] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang.
Hierarchical graph pooling with structure learning, 2019.
[120] Qi Zhao and Yusu Wang. Learning metrics for persistence-based summaries and applications
for graph classification, 2019.
18A Notation and a very brief review of quantum mechanics
We denote by {ai}a set of elements indexed by i, with 1-based indexing unless otherwise specified,
with the maximal value of iexplicitly specified when it is not clear from context. [N]denotes the set
{0, . . . , N −1}. The complex conjugate of a number cis denoted by c∗, and the conjugate transpose
of a complex-valued matrix AbyA†.
We denote by |ψ⟩a vector of complex numbers {ψi}representing the state of a quantum system
when properly normalized, and by ⟨ψ|its dual (assuming it exists). The inner product between two
such vectors of length Nis denoted by
⟨ψ|φ⟩=N−1X
i=0ψ∗
iφi. (A.1)
Denoting by |i⟩fori∈[N]a basis vector in an orthonormal basis with respect to the above inner
product, we can also write
|ψ⟩=N−1X
i=0ψi|i⟩. (A.2)
Matrices will be denoted by capital letters, and when acting on quantum states will always be unitary.
These can be specified in terms of their matrix elements using the Dirac notation defined above, as in
A=X
ijAij|i⟩⟨j|. (A.3)
Matrix-vector product are specified naturally in this notation by
Quantum mechanics is, in the simplest possible terms, a theory of probability based on conservation
of the L2norm rather than the standard probability theory based on the L1norm [ 2,90]. The state of
a pure quantum system is described fully by a complex vector of Nnumbers known as amplitudes
which we denote by {ψi}where i∈ {0, . . . , N −1}, and is written using Dirac notation as |ψ⟩. The
state is normalized so that
⟨ψ|ψ⟩=N−1X
i=0ψ∗
iψi=N−1X
i=0|ψi|2= 1, (A.4)
which is the L2equivalent of the standard normalization condition of classical probability theory. It
is a curious fact that the choice of L2requires the use of complex rather than real amplitudes, and
that no consistent theory can be written in this way for any other Lpnorm [ 2]. The most general state
of a quantum system is a probabilistic mixture of pure states, in the sense of the standard L1-based
rules of probability. We will not be concerned with these types of states, and so omit their description
here, and subsequently whenever quantum states are discussed, the assumption is that they are pure.
Since any closed quantum system conserves probability, the L2norm of a quantum state is conserved
during the evolution of a quantum state. Consequently, when representing and manipulating quantum
states on a quantum computer, the fundamental operation is the application of a unitary matrix to a
quantum state.
Given a quantum system with some discrete degrees of freedom, the number of amplitudes corre-
sponds to the number of possible states of the system, and is thus exponential in the number of
degrees of freedom. The simplest such degree of freedom is a binary one, called a qubit, which is
analogous to a bit. Thus a state of logNqubits is described by Ncomplex amplitudes.
A fundamental property of quantum mechanics is that the amplitudes of a quantum state are not
directly measurable. Given a Hermitian operator
O=N−1X
i=0λi|vi⟩⟨vi| (A.5)
with real eigenvalues {λi}, a measurement of Owith respect to a state |ψ⟩gives the result λiwith
probability |⟨vi|ψ⟩|2. The real-valued quantity
⟨ψ|O |ψ⟩=N−1X
i=0λi|⟨ψ|vi⟩|2(A.6)
19is the expectation value of Owith respect to |ψ⟩, and its value can be estimated by measurements.
After a measurement with outcome λi, the original state is destroyed, collapsing to the state |vi⟩.
A consequence of the fundamentally destructive nature of quantum measurement is that simply
encoding information in the amplitudes of a quantum state dues not necessarily render it useful
for downstream computation. It also implies that operations using amplitude-encoded data such as
evaluating a simple loss function incur measurement error, unlike their classical counterparts that are
typically limited only by machine precision. The design of quantum algorithms essentially amounts
to a careful and intricate design of amplitude manipulations and measurements in order to extract
useful information from the amplitudes of a quantum state. For a more complete treatment of these
topics see [90].
B Proofs
Proof of Lemma 1. ⟨φ|P0|φ⟩can be estimated by preparing |φ⟩and measuring it O(1/ε2)times.
Preparing each copy of |φ⟩requires O(L)rounds of communication, with each round involving the
communication of a logN′-qubit quantum state. Alice first prepares |ψ(x)⟩, and this state is passed
back and forth with each player applying AℓorBℓrespectively for ℓ∈ {1, . . . , L }.
Proof of Lemma 2. We consider the parameters of the unitaries that Alice possesses first, and an
identical argument follows for the parameters of Bob’s unitaries.
We have
∂
∂θA
ℓi⟨φ|P0|φ⟩=2Re⟨φ|P0ℓ+1Y
k=LAkBk∂Aℓ
∂θA
ℓiBℓ1Y
k=ℓ−1AkBk|ψ(x)⟩
≡2Re
νA
ℓi|µA
ℓ(B.1)
where
µA
ℓ
=Bℓ1Y
k=ℓ−1AkBk|ψ(x)⟩,νA
ℓi
=∂Aℓ
∂θA
ℓi†ℓ+1Y
k=LB†
kA†
kP0|φ⟩, (B.2)
correspond to forward and backward features for the i-the parameter of Aℓrespectively. This is
illustrated graphically in Figure 1. We also write
νA
ℓ0
=ℓ+1Y
k=LB†
kA†
kP0|φ⟩. (B.3)
Attaching an ancilla qubit denoted by ato the feature states defined above, we define
ψA
ℓi
≡1√
2 
|0⟩µA
ℓ
+|1⟩νA
ℓi
, (B.4)
and a Hermitian measurement operator
EA
ℓi≡
|0⟩⟨0| ⊗I+|1⟩⟨1| ⊗∂Aℓ
∂θA
ℓi
Xa 
|0⟩⟨0| ⊗I+|1⟩⟨1| ⊗∂Aℓ
∂θA
ℓi†!
=|1⟩⟨0| ⊗∂Aℓ
∂θA
ℓi
+|0⟩⟨1| ⊗∂Aℓ
∂θA
ℓi†
,(B.5)
we then have 
ψA
ℓ0EA
ℓiψA
ℓ0
=
ψA
ℓiXaψA
ℓi
=∂
∂θA
ℓi⟨φ|P0|φ⟩,(B.6)
where Xaacts on the ancilla.
20Note thatψA
ℓ0⊗kcan be prepared by Alice first preparing (|+⟩|ψ(x)⟩)⊗kand sending this state
back and forth at most 2Ltimes, with each player applying the appropriate unitaries conditioned on
the value of the ancilla. Additionally, for any choice of ℓand any i, Alice has full knowledge of the
EA
ℓi. They can thus be applied to quantum states and classical hypothesis states without requiring any
communication.
The gradient can then be estimated using shadow tomography (Theorem 1). Specifically, for each
ℓ, Alice prepares ˜O(log2PlogNlog(L/δ)/ε4)copies ofψAℓ
0E
, which requires O(L)rounds of
communication, each of ˜O(log2Plog2Nlog(L/δ)/ε4)qubits. She then runs shadow tomography
to estimate ∇Aℓ⟨φ|Z0|φ⟩up to error εwith no additional communication. Bob does the same to
estimate ∇Bℓ⟨φ|Z0|φ⟩. In total O(L2)rounds are needed to estimate the full gradient. The success
probability of all Lapplications of shadow tomography is at least 1−δby a union bound.
Based on the results of [ 22], the space and time complexity of each application of shadow tomography
is√
Ppoly( N,logP, ε−1,log(1/δ)). This is the query complexity of the algorithm to oracles that
implement the measurement operatorsn
EQ
ℓio
. Instantiating these oracles will incur a cost of at most
O(N2). In cases where these operators have low rank the query complexity complexity will depend
polynomially only on the rank instead of on N.
Proof of Lemma 3. We first prove an Ω(√
N)lower bound on the amount of classical communication.
Consider the following problem:
Problem 4 ([101]).Alice is given a vector x∈SN−1and two orthogonal linear subspaces of RN
each of dimension N/2, denoted M1, M2. Bob is given an orthogonal matrix O. Under the promise
that either ∥M1Ox∥2≥√
1−θ2or∥M2Ox∥2≥√
1−θ2for0< θ < 1/√
2, Alice and Bob must
determine which of the two cases holds.
Ref. [ 101] showed that the randomized7classical communication complexity of the problem is
Ω(√
N).
The reduction from Problem 4 to Problem 1 is obtained by choosing θ= 1/2and simply setting
L= 1, B1=O,|ψ(x)⟩=|x⟩,P0=Z0, and
A1=N/2−1X
j=0|0⟩|j⟩
v1
j+N/2−1X
j=0|1⟩|j⟩
v2
j, (B.7)
where the first register contains a single qubit and
vk
j	
form an orthonormal basis of Mk, and
picking any ε <1/2. Note that this choice of |ψ(x)⟩implies N′=N. Estimating Lto this accuracy
now solves the desired problem since L=⟨x|OT(Π1−Π2)O|x⟩where Πkis a projector onto
Mk, and hence estimating this quantity up to error 1/2allows Alice and Bob to determine which
subspace has large overlap with Ox.
The reduction from Problem 4 to Problem 2 is obtained by setting L= 2, picking |ψ(x)⟩, A1, B1as
before, and additionally B2=I, A 2=e−iθA
2,1X0/2initialized at θA
2,1=−π/2. By the parameter shift
rule [ 34], we have that if U=e−iθP/2for some Pauli matrix P, and Uis part of the parameterized
circuit that defines |φ⟩, then
∂L
∂θ=1
2(L(θ+π
2)− L(θ−π
2)). (B.8)
7In this setting Alice and Bob can share an arbitrary number of random bits that are independent of their
inputs.
21It follows that
∂L
∂θA
2,1
θA
2,1=−π/2=1
2(L(0)− L(−π))
=1
2
L(0)− ⟨x|B†
1A†
1e−iπ
2X0Z0eiπ
2X0A1B1|x⟩
=1
2
L(0)− ⟨x|B†
1A†
1X0Z0X0A1B1|x⟩
=1
2
L(0) +⟨x|B†
1A†
1Z0A1B1|x⟩
=L(0).(B.9)
Estimating ∇A⟨φ|Z0|φ⟩to accuracy ε <2allows one to determine the sign of L(0), which as
before gives the solution to Problem 4.
Next, we show that Ω(L)rounds are necessary in both the quantum and classical setting by a reduction
from the bit version of pointer-chasing, as studied in [58, 96].
Problem 5 (Pointer-chasing, bit version) .Alice receives a function fA: [N]→[N]and Bob receives
a function fB: [N]→[N]. Alice is also given a starting point x∈[N], and both receive an integer
L0. Their goal is to compute the least significant bit of f(L0)(x), where f(1)(x) =fB(x), f(2)(x) =
fA(fB(x)), . . ..
Ref. [58] show that the quantum communication complexity of L0-round bit pointer-chasing when
Bob speaks first is Ω(N/L4
0)(which holds for classical communication as well). This also bounds
the(L0−1)-round complexity when Alice speaks first (since such a protocol is strictly less powerful
given that there are fewer rounds of communication). On the other hand, there is a trivial L0-round
protocol when Alice speaks first that requires logNbits of communication per round, in which Alice
sends Bob x, he sends back f(1)(x), she replies with f(2)(f(1)(x)), and so forth. This, combined
with the lower bound, implies as exponential separation in communication complexity as a function
of the number of rounds.
To reduce this problem to Problem 1, we assume fA, fBare invertible. This should not make the
problem any easier since it implies that fA, fBhave the largest possible image. In this setting, fA, fB
can be described by unitary permutation matrices:
UA=X
i|fA(i)⟩⟨i|, UB=X
i|fB(i)⟩⟨i|. (B.10)
The corresponding circuit eq. (3.5) is then given by
|φ⟩= SWAP 0↔logN−1UB. . . U AUB|x⟩ (B.11)
in the case where Bob applies the function last, with an analogous circuit in the converse situation (if
Bob performed the swap, Alice applies an additional identity map). Estimating Z0to accuracy ε <1
using this state will then reveal the least significant bit of f(L0)(x). This gives a circuit with Llayers,
where L0≤2L−1. Thus any protocol with less than L0rounds (meaning less than 2L−1rounds)
would require communicating Ω(N/L4
0) = Ω( N/L4)qubits, since the converse will contradict the
results of [ 58]. The reduction to Problem 2 is along similar lines to the one described by eq. (B.9),
with the state in that circuit replaced by eq. (B.11). This requires at most two additional rounds of
communication.
Since quantum communication is at least as powerful than classical communication, these bounds
also hold for classical communication. Since each round involves communicating at least a single bit,
this gives an Ω(L)bound on the classical communication complexity.
Proof of Lemma 5. The proof is based on a reduction from the f-Boolean Hidden Partition problem
(f−BHP N,t) studied in [36]. This is defined as follows:
Problem 6 (Boolean Hidden Partition [ 36] (f−BHP N,t)).Assume tdivides N. Alice is given
x∈ {− 1,1}N. Bob is given a permutation Πover[N], a boolean function f:{−1,1}t→ {− 1,1},
and a vector v∈ {− 1,1}N/t. We are guaranteed that for any k∈ {1, . . . , N/t },
f([Πx][(k−1)t+1:kt])∗vk=s (B.12)
22for some s∈ {− 1,1}. Their goal is to determine the value of s.
A polynomial pf:{−1,1}t→Ris said to sign-represent a boolean function fifsign(pf(y)) =f(y)
for all y∈ {0,1}t. The sign-degree off(sdeg( f)) is the minimal degree of a polynomial that sign-
represents it. In the special case sdeg( f) = 2 ,f−BHP N,tcan be solved with exponential quantum
communication advantage [ 36]. For a vector y∈ {0,1}t, define ˜y= (1, y1, . . . , y t). It is also known
that if sdeg( f) = 2 , then there exists a sign-representing polynomial pfthat can be written as
pf(y) = ˜yTR˜y (B.13)
for some matrix real R[4]. Moreover, for any fthere exists such a pfwithmax x∈{−1,1}t|pf(x)| ≤3.
We denote by β= min x∈{−1,1}t|pf(x)|thebias ofpf.
We now describe a reduction from f−BHP N,twithsdeg( f) = 2 toQGNICN,t for some constant
1≤C≤3/2. As is typical in communication complexity, the parties are allowed to exchange
bits that are independent of the problem input, and these are not counted when measuring the
communication complexity of a protocol that depends on the inputs. Before receiving their inputs,
Alice thus sends two orthogonal vectors u0, u1of length D0to Bob, with each entry described by K
bits8.
Assume Alice and Bob are given an instance of BHP N,t. They use it to construct an instance of
QGNI(t+1)N/t,t withD1= 1. Alice constructs X∈R(t+1)N/tby picking the rows Xiaccording to
Xi=

1√
(t+1)N/t 1−xi
2uT
0+1+xi
2uT
1
i≤N
1√
(t+1)N/tu1 i > N. (B.14)
Note that with this definition ||X||F= 1. Bob defines a permutation π′over[(t+ 1)N/t]by
π′(i) =⌊i/t⌋(t+ 1) + i%t+ 1 i≤N
(i−N−1)(t+ 1) + 1 i > N, (B.15)
denoting the corresponding permutation matrix Π′. Define by xthe concatenation of Alice’s input x
with1(t+1)N/t. The purpose of this permutation is that Π′Πx≡˜xwill be a concatenation of N/t
vectors of length t+ 1, with the i-th vector equal to (1,[Πx]t(i−1)+1, . . . , [Πx]ti)≡˜x(i).
Note that we can assume wlog that Rin eq. (B.13) is symmetric since pfis independent of its anti-
symmetric part. It can thus be diagonalized by an orthogonal matrix U, and denoting the diagonal
matrix of its real eigenvalues by D, we define a (complex-valued) matrix S=U√
Dthat satisfies
R=SST. Bob therefore defines his model by
A= (IN/t⊗ST)Π′Π, W 1=u1−u0, W 2=vT. (B.16)
Additionally, he picks the pooling operator P:R(t+1)N/t→RN/tto be sum pooling with window
sizet+ 1(i.e.P(x)j=Pj(t+1)
k=(j−1)(t+1)+1xk). Bob also uses a simple quadratic nonlinearity by
choosing a= 1, b=c= 0in eq. (4.3).
8Since D0is arbitrary and in particular independent of N, even if we count this communication it will not
affect the scaling with Nwhich the main property we are interested in. This independence is also natural since it
implies that the number of local graph features is independent of the size of the graph.
23To see that solving QGNI(t+1)N/t,t to error ε <1/2indeed provides a solution to BHP N,t, note that
P(σ(AXW 1))i=P 
σ(1p
(t+ 1)N/tAx)!
i(B.17)
=P 
σ(1p
(t+ 1)N/t(IN/t⊗ST)Π′Πx)!
i(B.18)
=P 
σ(1p
(t+ 1)N/t(IN/t⊗ST)˜x)!
i(B.19)
=1
(t+ 1)N/tt+1X
j=1([ST˜x(i)]j)2(B.20)
=1
(t+ 1)N/t˜xT
(i)SST˜x(i) (B.21)
=1
(t+ 1)N/tpf([Πx][(i−1)t+1:it]). (B.22)
Given the choice of W2, one obtains
φ(X/||X||F) =1
(t+ 1)N/tN/tX
i=1pf([Πx][(i−1)t+1:it])vi. (B.23)
It follows that sign(φ(X)) = sand|φ(X)| ≥β. It is thus possible to decide the value of sif
φ(X/||X||F)is estimated to some error smaller than β.
From Theorem 4 of [ 36], we have R→(f−BHP N,t) = Ω(p
N/t)for any fthat has sign-degree 2
and satisfies some additional conditions. The reduction then implies
R→
β(QGNIN,t) = Ω(p
(t/(t+ 1)) N/t). (B.24)
This can be simplified by noting that since t≥2,t/(t+ 1)≥2/3. The lower bound in [ 36] is
based on choosing fwhich belongs to a specific class of symmetric boolean functions (meaning
f(y) =˜f(|y|)where |y|=|{i:yi=−1}|). Specifically, ˜fis defined by the choice of tand two
additional integer parameters θ1, θ2such that 0≤θ1< θ2< tand
˜f(|y|) =1 0 ≤ |y| ≤θ1orθ2<|y|,
−1θ1<|y| ≤θ2,(B.25)
(and an additional technical condition that will not be of relevance to our analysis).
We next construct a sign-representing polynomial pffor any fthat takes the above form, and compute
its bias β. Since fis symmetric of sign degree 2, it suffices to construct a polynomial ˜pf:R→R
such that pf(y) = ˜pf(|y|)for this purpose. If we can produce some β′that bounds βfrom below for
any choice of t, θ1, θ2, then the lower bound from Theorem 4 of [ 36] holds for any error smaller than
β′.
We choose ˜pf(z) = ˜az2+˜bz+ 1, with the constraints ˜pf(θ1+ 1/2) = 0 ,˜pf(θ2+ 1/2) = 0 . These
lead to the solution
˜pf(z) =1
θ+
1θ+
2z2−1
θ+
1θ+
2θ+2
2−θ+2
1
θ+
2−θ+
1z+ 1. (B.26)
Since this is a quadratic function with known roots that is only evaluated at integer inputs, if we want
to bound the bias of pfit suffices to check the values of ˜pfat the integers closest to the roots, namely
24{θ1, θ1+ 1, θ2, θ2+ 1}. Plugging in these values gives
˜pf(θ1) =1 −θ2−1
(1 +1
2θ1)(θ2+1
2)
≥1−1
1 +1
2θ1
≥1
4θ1
≥1
4t,(B.27)
where in the third line we used1
1+x≤1−x/2which holds for 0≤x≤1. Using θ2≤θ1+ 1, we
also have
˜pf(θ1+ 1) =1 −θ1+ 1
 
θ1+1
2
1 +1
2θ2
≤1−θ1+ 1
 
θ1+1
2
1 +1
2θ1+2
=−1
4 
θ1+1
2 
θ1+3
2
≤ −1
4 
t+1
2 
t+3
2.(B.28)
˜pf(θ2)takes the same value as ˜pf(θ1+ 1) . Similarly,
˜pf(θ2+ 1) =1 −θ2+ 1
(θ2+1
2)(1 +1
2θ1)
≥1−θ2+ 1
(θ2+1
2)(1 +1
2θ2−2)
=2θ2
2−5
4
θ2
2−1
4.(B.29)
Forθ2≤1this is a monotonically increasing function of θ2, and is thus lower bounded by picking
θ2= 1, giving ˜pf(2)≥1. It follows that for any choice of t, θ1, θ2, the bias is bounded from below
by
β′=1
4 
t+1
2 
t+3
2. (B.30)
Note that our bound on the bias allows us to use the reduction from f−BHP N,ttoQGNI(t+1)N/t,t
for any valid choice of f(satisfying eq. (B.24)).
Proof of Lemma 6. Alice encodes her input in the quantum state
˜|X⟩0≡1√
2∥X∥F|0⟩N−1X
i=0D0−1X
j=0Xij|i, j⟩+1√
2|1⟩0⊗N,0⊗D0
(B.31)
overlog(ND 0) + 1 qubits. She sends this state to Bob. Define D= max {D0, D1}. Bob augments
this state by attaching zero qubits and, reordering the first two qubits, obtains the state
˜|X⟩ ≡1√
2∥X∥F|0⟩|0⟩N−1X
i=0D0−1X
j=0Xiji, j,0⊗(D−D0)E
+1√
2|1⟩|0⟩0⊗N,0⊗D
≡1√
2|0⟩|0⟩X
+1√
2|1⟩|0⟩0⊗N,0⊗D(B.32)
25overlog(ND) + 2 qubits.
Define by W1theD×Dmatrix obtained by appending zero rows or columns to the rectangular
matrix W1to obtain a square matrix, and denote α=A⊗W1. Bob prepares an (α,1,0)-block-
encoding of A⊗W1, denoted UA⊗W1, which acts on log(ND) + 1 qubits. Bob then applies this
unitary conditioned on the value of the first qubit, giving
 
|0⟩⟨0|UA⊗W1+|1⟩⟨1|˜|X⟩=1√
2|0⟩UA⊗W1|0⟩X
+1√
2|1⟩|0⟩0⊗N,0⊗D
=1√
2|0⟩1
α|0⟩A⊗W1X
+|1⟩|g⟩
+1√
2|1⟩|0⟩0⊗N,0⊗D
≡1√
2|0⟩1
α|0⟩AXW 1
+|1⟩|g⟩
+1√
2|1⟩|0⟩0⊗N,0⊗D
≡|ψ⟩
(B.33)
where |g⟩is an unnormalized garbage state. Above, AXW 1is anN×Dmatrix obtained by adding
zero columns to W1as needed.
The sum pooling operator Pcan be implemented by multiplication by an N/t×Nmatrix which we
denote by ˜P. Define by W2theD×N/t matrix obtained by appending zero rows to W2if needed.
Given a matrix Mof size N1×N2, denote by V[M]the vectorization of M. Bob then constructs
the Hermitian matrix
O=
2aα2|0⟩⟨0| ⊗diag( V[W2˜P]) bα|0⟩⟨0| ⊗V[W2˜P]
0⊗N,0⊗D
bα|0⟩⟨0| ⊗0⊗N,0⊗D
V[W2˜P]†0
. (B.34)
It follows that
⟨ψ|O|ψ⟩+ctr
W2P1N×D1
=a
∥X∥2
Ftrh
W2˜P(AXW 1)2i
+b
∥X∥Ftr [W2PAXW 1] +ctr
W2P1N×D1
=tr
W2˜Pσ(AX
∥X∥FW1)
=φ(X/∥X∥F),
(B.35)
where 1N×D1is an all ones matrix. The last term on the RHS is independent of Xand can be
computed by Bob without requiring Alice’s message. Estimating ⟨ψ|O|ψ⟩to accuracy εrequires
O(∥O∥/ε)measurements. Since
∥O∥ ≤2aα2|0⟩⟨0| ⊗diag( V[W2˜P])+ 2bα|0⟩⟨0| ⊗V[W2˜P]
0⊗N,0⊗D
≤2(|a|α2+|b|α)W2˜P
∞,(B.36)
Bob requires O((|a|α2+|b|α)W2˜P
∞/ε)copies of Alice’s state in order to do this.
Proof of lemma 7. For the parameter choices used to obtain the classical lower bound (eq. (B.14)
and eq. (B.16)), we have ||W1||= 1,||W2P||∞≤t. Additionally, for the polynomials constructed
in eq. (B.26), we have |pf(y)| ≤Ct2from which it follows that the matrix Rused in the matrix
representation of pfhas constant operator norm C, and thus ∥A∥=∥S∥=√
C. We also have
a= 1, b=c= 0 for the nonlinearity used (eq. (4.3)), and it thus follows from Lemma 6 that
Q→
ε(QGNIN,t) =O(t3log(ND 0))forε≤1
4(t+1
2)(t+3
2). With this choice of ε, the classical lower
bound in Lemma 5 holds, and thus an exponential advantage in communication is obtained by using
quantum communication.
Proof of Lemma 11. Consider first a single variable z, with data-dependent unitaries given by
eq. (F.4a). If {λℓi}are chosen i.i.d. from a uniform distribution over say [0,1], then with probability
1they are all unique and so are all sums of the form Λj=LP
ℓ=1λℓjℓas well as differences Λj−Λkfor
26k <jwhere the inequality holds element-wise. Set Bℓto be the Hadamard transform over logN′
qubits for all ℓ, and pick the measurement operator P0=X0. We then have
L1=⟨φ|X0|φ⟩
=X
j,k∈[N′]Le2πi(Λj−Λk)z
B†
1
1j1
B†
2
j1j2. . .
B†
L
jL−1jL(X0)jLkL(BL)kLkL−1. . .(B1)k11
=X
j,k∈[N′]L,j̸=ke2πi(Λj−Λk)z
B†
1
1j1
B†
2
j1j2. . .
B†
L
jL−1jL(X0)jLkL(BL)kLkL−1. . .(B1)k11
=X
j∈[N′]LX
k<j2 cos
2π(Λj−Λk)z
B†
1
1j1. . .
B†
L
jL−1jL(X0)jLkL. . .(B1)k11
=X
j[:−1]∈[N′]L−1X
k[:−1]<j[:−1]N′X
jL=12 cos
2π(Λj[:−1]−Λk[:−1]+λLjL−λL˜jL)z
∗
B†
1
1j1. . .
B†
L
jL−1jL(BL)˜jL,kL−1. . .(B1)k11
(B.37)
where ˜jL=jL+ (−1)⌊jL/(N′/2+1)⌋N′/2. In the third line, we dropped the diagonal terms in the
double sum since they vanish due to the X0matrix having 0on its diagonal. In the fourth line,
we collected terms and used the symmetry of
B†
1
1j1. . .
B†
L
jL−1jL(X0)jLkL. . .(B1)k11to the
permutation of jandk. In the last line we performed the sum over kLusing the structure of X0. By
our assumption about the {λℓi}, each term in the final sum has a unique frequency so no cancellations
are possible. The coefficient of each cosine is nonzero (and is equal to 2N′−Lor−2N′−L). There
are a total of
N′(N′−1)
2L−1
N′such summands. This completes the first part of the proof for this
choice of {Bℓ}.
Considering instead the case of two variables, with unitaries given by eq. (F.4b), an equivalent
calculation gives
L2=X
j[:−1]∈[N′]L−1X
k[:−1]<j[:−1]N′X
jL=12 cos
ω1
jky+ω2
jkz
B†
1
1j1. . .
B†
L
jL−1jL(BL)˜jL,kL−1. . .(B1)k11,
(B.38)
where
ω1
jk= 2π
Λj[:N′/2+1]−Λk[:N′/2+1]
, ω2
jk= 2π
Λj[N′/2+1:−1]−Λk[N′/2+1:−1]+λLjL−λL˜jL
.
(B.39)
As before, there are
N′(N′−1)
2L−1
N′summands in total. Since
cos
ω1
jky+ω2
jkz
= cos
ω1
jky
cos
ω2
jkz
−sin
ω1
jky
sin
ω2
jkz
, (B.40)
we can rewrite eq. (B.38) as a sum over 2
N′(N′−1)
2L−1
N′terms that are pairwise orthogonal
w.r.t. the L2inner product over R2. It follows from the definition of the separation rank that
sep (L2;y, z) = 2N′(N′−1)
2L−1
N′. (B.41)
We next use the assumption that the real and imaginary parts of each element of Bℓare real analytic
function of parameters Θ. This implies that the same property holds for product of entries of the form

B†
1
1j1. . .
B†
L
jL−1jL(BL)˜jL,kL−1. . .(B1)k11 (B.42)
for any choice of j,k. This coefficient is equal to 0iff both the real and imaginary parts are equal to
0. Since the zero set of a real analytic function has measure 0[81], the set of values of Θfor which
any of the coefficients in eq. (B.38) vanishes also has measure 0, for all choices of j,k. The result
follows.
27Proof of Lemma 12. Consider a periodic function fwith period 1. Denote by SM[f]the truncated
Fourier series of fwritten in terms of trigonometric functions:
SM[f](y) =M−1X
m=01/2Z
x=−1/2f(x) cos (2 πmx) dxcos (2 πmy) +M−1X
m=01/2Z
x=−1/2f(x) sin (2 πmx) dxsin (2 πmy)
≡M−1X
m=0ˆf+
mcos (2 πmy) +M−1X
m=0ˆf−
msin (2 πmy).
(B.43)
Iffisp-times continuously differentiable, it is known that the Fourier series converges uniformly,
with rate
∥SM[f]−f∥∞<C
Mp−1/2. (B.44)
for some absolute constant C[91]. For analytic functions the rate is exponential in M.
We now define the following circuit:
A1(x) = diag((1 , . . . , 1|{z}
N′/2,1, e2πix, e2πi2x. . . , e2πi(N′/4−1)x
| {z }
N′/4,1, e2πix, e2πi2x. . . , e2πi(N′/4−1)x
| {z }
N′/4)),
(B.45)
B1=ˆf
⟨0|+|0⟩
ˆf, (B.46)
where
ˆf
=1rP
mˆfmN′/4−1X
m=0 q
ˆf+m|0⟩+ sign( ˆf+
m)|1⟩√
2|0⟩+q
ˆf−m|0⟩ −isign( ˆf−
m)|1⟩√
2|1⟩!
|m⟩.
(B.47)
Choosing |ψ(x)⟩=|0⟩as the initial state, this gives
|φ⟩=A1B1|0⟩
=A1ˆf
=1rP
mˆfmN′/4−1X
m=0 q
ˆf+m|0⟩+ sign( ˆf+
m)e2πimx|1⟩√
2|0⟩+q
ˆf−m|0⟩ −isign( ˆf−
m)e2πimx|1⟩√
2|1⟩!
|m⟩
(B.48)
It follows that
⟨φ|X0|φ⟩=1
P
mˆfmN′/4−1X
m=0ˆf+
m⟨0|+ sign( ˆf+
m)e−2πimx⟨1|√
2X0|0⟩+ sign( ˆf+
m)e2πimx|1⟩√
2
+ˆf−
m⟨0|+isign( ˆf−
m)e−2πimx⟨1|√
2X0|0⟩ −isign( ˆf−
m)e2πimx|1⟩√
2
=1
P
mˆfmN′/4−1X
m=0ˆf+
mcos(2 πmx) +ˆf−
msin(2πmx)
=1
P
mˆfmSN′/4[f](x)
(B.49)
This approximation thus converges uniformly according to eq. (B.44), with error decaying exponen-
tially with number of qubits logN′as long as fis continuously differentiable at least once.
Proof of Lemma 10. The algorithm in Theorem 5 of [ 100] takes as input a state-preparation unitary
Uacting on n= log Nqubits such that U|0⟩⊗n=|z⟩. Using O(log 1 /ε)queries to UandU†and
28n+ 4ancillas, it creates a state |φ⟩such that measuring 0on the first n+ 4qubits of |φ⟩results in a
state|ˆφ⟩that obeys|ˆφ⟩ −1
∥σ(z)∥2|σ(z)⟩
2< ε. (B.50)
Additionally, the probability of measuring 0on the first n+ 4qubits is O(1).
We will be interested in applying this algorithm to the state |U1x⟩. The state preparation unitary
can be instantiated with a single round of communication by Alice starting with the state |0⟩⊗2n+4,
applying a unitary that encodes xin the last nqubits of this state, and then sending it to Bob who
applies U1to the same nqubits. The conjugate of the state-preparation unitary can be applied in a
similar fashion by reversing this procedure. This can include any conditioning required on the values
of the other qubits.
Based on the query complexity of the algorithm in [ 100] to the state preparation unitary, O(log(1 /ε))
rounds will suffice to obtain a state
|˜φσ⟩=α|0⟩⊗n+4|˜y⟩+|ϕ⟩, (B.51)
such that |˜y⟩ −1
∥σ(U1x)∥2σ(U1x)
2< ε. (B.52)
Bob then applies U2to the state |˜φσ⟩conditioned on the first n+ 4qubits being in the state |0⟩⊗n+4.
The state |ϕ⟩is unaffected. Unitary of U2combined with the above bound guarantees|ˆy⟩ −U21
∥σ(U1x)∥2σ(U1x)
2< ε. (B.53)
Additionally, from Theorem 3 of [100] we are guaranteed that α=O(1).
C Data parallelism
Data parallelism involves storing multiple copies of a model on different devices and training each
copy on a subset of the full data. We consider a model of the form
|φ(Θ, x)⟩ ≡ 1Y
ℓ=LUℓ(θℓ, x)!
|x⟩, (C.1)
where xis anN1×N2matrix which we write as x= [xA, xB]for two N1/2×N2matrices xA, xB.
Assume also that ∥x∥F= 1. This model can be used to define a distributed problem with dara
parallelism by considering the following inputs to both players:
Alice : xA,{Uℓ},
Bob : xB,{Uℓ}.(C.2)
The state |x⟩can be prepared in a single round of communication involving log(N1N2)qubits. Alice
simply prepares the state
|xA⟩+q
1− ∥xA∥2
F|N1/2,0⟩= (xA)ijN1/2−1X
i=0N2−1X
j=0|i, j⟩+q
1− ∥xA∥2
F|N1/2,0⟩
= (xA)ijN1/2−1X
i=0N2−1X
j=0|i, j⟩+∥xB∥F|N1/2,0⟩,(C.3)
using zero-based indexing of the elements of xA. After sending this to Bob, he applies the unitary
1
∥xB∥F(xB)i,jN1/2−1X
i=0N2−1X
j=0|ij⟩⟨N1/2,0|+h.c.. (C.4)
The resulting state is |x⟩. As before, the gradients with respect to the parameters of the unitaries
{Uℓ}can be estimated by preparing copies of this state and using shadow tomography. The number
of copies will again be logarithmic in N1, N2and the number of trainable parameters.
29D Exponential advantages in end-to-end training
So far we have discussed the problems of inference and estimating a single gradient vector. It is
natural to also consider when these or other gradient estimators can be used to efficiently solve
an optimization problem (i.e. when the entire training processes is considered rather than a single
iteration). Applying the gradient estimation algorithm detailed in Lemma 2 iteratively gives a
distributed stochastic gradient descent algorithm which we detail in Algorithm 2, yet one may be
concerned that a choice of ε=O(logN)which is needed to obtain an advantage in communication
complexity will preclude efficient convergence. Here we present a simpler algorithm that requires a
single quantum measurement per iteration, and can provably solve certain convex problems efficiently,
as well as an application of shadow tomography to fine-tuning where convergence can be guaranteed,
again with only logarithmic communication cost. In both cases, there is an exponential advantage in
communication even when considering the entire training process.
D.1 “Smooth” circuits
Consider the case where Aℓare product of rotations for all ℓ, namely
Aℓ=PY
j=1e−1
2iβA
ℓjθA
ℓjPA
ℓj, (D.1)
wherePA
ℓjare Pauli matrices acting on all qubits, and similarly for Bℓ. These can also be interspersed
with other non-trainable unitaries. This constitutes a slight generalization of the setting considered in
[49], and the algorithm we present is essentially a distributed distributed version of theirs. Denote by
βan2PL-dimensional vector with elements βQ
ℓjwhere Q∈ {A, B}9. The quantity ∥β∥1is the total
evolution time if we interpret the state |φ⟩as a sequence of Hamiltonians applied to the initial state
|x⟩.
In Appendix D.3 we describe an algorithm that converges to the neighborhood of a minimum, or
achieves EL(Θ)− L(Θ⋆)≤ε0, for a convex Lafter
2Θ(0)−Θ⋆2
2∥β∥2
1/ε2
0 (D.2)
iterations, where Θ⋆are the parameter values at the minimum of L. The expectation is with respect
to the randomness of quantum measurement and additional internal randomness of the algorithm.
The algorithm is based on classically sampling a single coordinate to update at every iteration, and
computing an unbiased estimator of the gradient with a single measurement. It can thus be seen as a
form of probabilistic coordinate descent.
This implies an exponential advantage in communication for the entire training process as long asΘ(0)−Θ⋆2
2∥β∥2
1= polylog( N). Such circuits either have a small number of trainable parameters
(P=O(polylog( N))), depend weakly on each parameter (e.g. βQ
ℓj=O(1/P)for arbitrary P), or
have structure that allows initial parameter guesses whose quality diminishes quite slowly with system
size. Nevertheless, over a convex region the loss can rapidly change by an O(1)amount. One may
also be concerned that in the settingΘ(0)−Θ⋆2
2∥β∥2
1= polylog( N)only a logarithmic number
of parameters is updated during the entire training process and so the total effect of the training
process may be negligible. It is important to note however that each such sparse update depends on
the structure of the entire gradient vector as seen in the sampling step. In this sense the algorithm is a
form of probabilistic coordinate descent, since the probability of updating a coordinate |βQ
ℓj|/∥β∥1is
proportional to the the magnitude of the corresponding element in the gradient (actually serving as an
upper bound for it).
Remarkably, the time complexity of a single iteration of this algorithm is proportional to a forward
pass, and so matches the scaling of classical backpropagation. This is in contrast to the polynomial
overhead of shadow tomography (Theorem 1). Additionally, it requires a single measurement per
iteration, without any of the additional factors in the sample complexity of shadow tomography.
9[49] actually consider a related quantity for which has smaller norm in cases where multiple gradient
measurements commute, leading to even better rates.
30D.2 Fine-tuning the last layer of a model
Consider a model given by eq. (3.1) where only the parameters of ALare trained, and the rest
are frozen, and denote this model by |φf⟩. The circuit up to that unitary could include multiple
data-dependent unitaries that represent complex features in the data. Training only the final layer in
this manner is a common method of fine-tuning a pre-trained model [53]. If we now define
˜EA
Li=|1⟩⟨0| ⊗A†
LP0∂AL
∂θA
Li+|0⟩⟨1| ⊗∂AL
∂θA
Li†
P0AL, (D.3)
the expectation value of ˜EA
Liusing the state |+⟩µA
L
gives∂L
∂θA
ℓi. HereµA
L
=
BL(x)1Q
k=L−1Ak(x)Bk(x)|ψ(x)⟩is the forward feature computed by Alice at layer Lwith the
parameters of all the other unitaries frozen (hence the dependence on them is dropped). Since the
observables in the shadow tomography problem can be chosen in an online fashion [ 5,6,13], and
adaptively based on previous measurements, we can simply define a stream of measurement operators
by measuring Pobservables to estimate the gradients w.r.t. an initial set of parameters, updating
these parameters using gradient descent with step size η, and defining a new set of observables using
the updated parameters. Repeating this for Titerations gives a total of PTobservables (a complete
description of the algorithm is given in Algorithm 3).
By the scaling in Lemma 2, the total communication needed is ˜O(logN(logTP)2log(1/δ)/ε4)
overO(L)rounds (since only O(L)rounds are needed to create copies ofµAL
. This implies
an exponential advantage in communication for the entire training process (under the reasonable
assumption T=O(poly( N, P))), despite the additional stochasticity introduced by the need to
perform quantum measurements. For example, assume one has a bound ∥∇L∥2
2≤K. If the circuit
is comprised of unitaries with Hermitian derivatives, this holds with K=PL. In that case, denoting
bygthe gradient estimator obtained by shadow tomography, we have
∥g∥2
2≤ ∥∇L∥2
2+∥∇L − g∥2
2≤K+ε2PL. (D.4)
It then follows directly from Lemma 8 that for an appropriately chosen step size, if Lis convex one
can find parameter values Θsuch that L(Θ)− L(Θ⋆)≤ε0using
T= 2Θ(0)−Θ⋆2
2(K+ε2PL)2/ε2
0 (D.5)
iterations of gradient descent. Similarly if Lisλ-strongly convex then T= 2(K+ε2PL)2/λε0+
1iterations are sufficient. In both cases therefore an exponential advantage is achieved for the
optimization process as a whole, since in both cases one can implement the circuit that is used to
obtain the lower bounds in Lemma 3.
In the following, we make use of well-known convergence rates for stochastic gradient descent:
Lemma 8 ([26]).Given an objective function L(Θ)with a minimum at Θ⋆and a stochastic gradient
oracle that returns a noisy estimate of the gradient g(Θ)such that Eg(Θ) = ∇L(Θ),E∥g∥2
2≤G2,
and denoting by Θ(0)a point in parameter space and R=Θ(0)−Θ⋆
2, we have:
i)IfLis convex in a Euclidean ball of radius Raround Θ⋆, then gradient descent with step size
η=R
Gq
2
Tachieves
EL(1
TTX
t=1Θ(t))− L(Θ⋆)≤RGr
2
T. (D.6)
ii)IfLisλ-strongly convex in a Euclidean ball of radius Raround Θ⋆, then gradient descent with
step size ηt=2
λ(t+1)achieves
EL(1
T(T+ 1)TX
t=12tΘ(t))− L(Θ⋆)≤2G2
λ(T+ 1). (D.7)
31Algorithm 1 Distributed Probabilistic Coordinate Descent
Input: Alice: x,{Aℓ},ΘA,{ηt}, T. Bob: {Bℓ},ΘB,{ηt}, T.
Output: Alice: Updated parameters Θ(T)
A. Bob: Updated parameters Θ(T)
B.
1:Alice and Bob each pre-process their coefficient vectors βA, βBto enable efficient sampling.
2:Alice sends ∥βA∥1to Bob. { O(logP)bits of classical communication.}
3:fort∈ {1, . . . , T }do
4: Bob samples b∼Bernoulli( ∥βA∥1/∥β∥1)and sends bto Alice { 1bit of classical communi-
cation.}
5: ifb== 0 then
6: Bob samples (ℓ, i)from the discrete distribution defined by abs(βB)
7: Bob create the stateψB
ℓ0
{O(L)rounds of quantum communication}
8: Bob measures ˆEB
ℓi, as defined in eq. (D.8), obtaining a result m∈ {− 1,1}
9: Bob sets θB
ℓi←θB
ℓi−ηtsign(βB
ℓi)||β||1m
10: else
11: Alice runs steps 6-9, (replacing BwithA)
12: end if
13:end for
D.3 Distributed Probabilistic Coordinate Descent
Given distributed states of the form eq. (D.1), optimization over Θcan be performed using Algo-
rithm 1. We verify the correctness of this algorithm and provide convergence rates following [ 49].
Define the Hermitian measurement operator
ˆEQ
ℓi=
|0⟩⟨0| ⊗I−i|1⟩⟨1| ⊗ PQ
ℓi†
Xa
|0⟩⟨0| ⊗I−i|1⟩⟨1| ⊗ PQ
ℓi
, (D.8)
with eigenvalues in {−1,1}. Note that βQ
ℓiD
ψQ
ℓ0ˆEQ
ℓiψQ
ℓ0E
=∂L
∂θQ
ℓi, and this is essentially a compact
way of representing a Hadamard test for the relevant expectation value. Now consider a gradient
estimator that first samples (Q, ℓ, i )with probability |βQ
ℓi|/||β||1, then returns a one-sparse vector
withgQ
ℓi= sign( βQ
ℓi)∥β∥1m, where mis the result of a single measurement of ˆEQ
ℓiusing the stateψQ
ℓ0E
. For this estimator we have
EgQ
ℓi= sign( βQ
ℓi)∥β∥1βQ
ℓi
∥β∥1
ψA
ℓ0ˆEQ
ℓiψA
ℓ0
=∂L
∂θQ
ℓi, (D.9)
where the expectation is taken over both the index sampling process and the quantum measurement.
The procedure generates a valid gradient estimator.
In order to show convergence, one simply notes that by construction, ∥g∥2=∥β∥1. It then follows
immediately from Lemma 8 that, with an appropriately chosen step size, Algorithm 1 achieves
EL(Θ)− L(Θ⋆)≤ε0for a convex Lusing
2Θ(0)−Θ⋆2
2∥β∥2
1
ε2
0(D.10)
queries. For a λ-strongly convex L, only
2∥β∥2
1
λε0+ 1 (D.11)
queries are required. The pre-processing in step 1 of Algorithm 1 requires time O(PlogP)and
subsequently enables sampling in time O(1)using e.g. [111]10.
32Algorithm 2 Shadow Tomographic Distributed Gradient Descent
Input: Alice: x,{Aℓ},Θ(1)
A, η, T . Bob: {Bℓ},Θ(1)
B, η, T.
Output: Alice: Updated parameters Θ(T)
A. Bob: Updated parameters Θ(T)
B.
1:fort∈ {1, . . . , T }do
2: forℓ∈ {1, . . . , L }do
3: Alice prepares ˜O(log2PlogN′log(L/δ)/ε4)copies ofψA
ℓ0(Θ(t))
{O(L)rounds of com-
munication}
4: Alice runs Shadow Tomography to estimate {EEA
ℓi(Θ(t))}P
i=1up to error ε, denoting these
{gA
ℓi(Θ(t))}P
i=1.
5: Bob prepares ˜O(log2PlogN′log(L/δ)/ε4)copies ofψB
ℓ0(Θ(t))
{O(L)rounds of com-
munication}
6: Bob runs Shadow Tomography to estimate {EEB
ℓi(Θ(t))}P
i=1up to error ε, denoting these
{gB
ℓi(Θ(t))}P
i=1.
7: Alice sets θA(t+1)
ℓ←θA(t)
ℓ−ηgA
ℓ(Θ(t)).
8: Bob sets θB(t+1)
ℓ←θB(t)
ℓ−ηgB
ℓ(Θ(t)).
9: end for
10:end for
Algorithm 3 Shadow Tomographic Distributed Fine-Tuning
Input: Alice: x,{Aℓ}, θA(1)
L, η, T . Bob: {Bℓ}
Output: Alice: Updated parameters Θ(T)
A.
1:Alice prepares ˜O(log2(PT) logN′log(1/δ)/ε4)copies ofµA
L
{O(L)rounds of communica-
tion}
2:fort∈ {1, . . . , T }do
3: Alice runs online Shadow Tomography to estimate {E˜EA
Li(θA(t)
L)}up to error ε, denoting
these{gA
Li(θA(t)
L)}.
4: Alice sets θA(t+1)
L ←θA(t)
L−ηgA
L(θA(t)
L).
5:end for
D.4 Algorithms based on Shadow Tomography
E Communication Complexity of Linear Classification
While the separation in communication complexity for expressive networks can be quite large,
interestingly we will show that for some of the simplest models this advantage can vanish due to
the presence of structure. In particular, when a linear classifier is well-suited to a task such that the
margin is large, the communication advantage will start to wane, while a lack of structure in linear
classification will make the problem difficult for quantum algorithms as well. More specifically, we
consider the following classification problem:
Problem 7 (Distributed Linear Classification) .Alice and Bob are given x, y∈SN, with the promise
that|x·y| ≥γfor some 0≤γ≤1. Their goal is to determine the sign of x·y.
This is one of the simplest distributed inference problem in high dimensions that one can formulate.
xcan be thought of as the input to the model, while ydefines a separating hyperplane with some
margin. Since with finite margin we are only required to resolve the inner product between the
vectors to some finite precision, it might seem that an exponential quantum advantage should be
possible for this problem by encoding the inputs in the amplitudes of a quantum state. However, we
show that classical algorithms can leverage this structure as well, and consequently that the quantum
advantage in communication that can be achieved for this problem is at most polynomial in N. We
10An even simpler algorithm that sorts the lists as a pre-processing step and uses inverse CDF sampling will
enable sampling with cost O(logP)
33prove this with respect to the the randomized classical communication model, in which Alice and
Bob are allowed to share random bits that are independent of their inputs11.
Lemma 9. The quantum communication complexity of Problem 7 is Ωp
N/max(1 ,⌈γN⌉)
. The
randomized classical communication complexity of Problem 7 is O(min( N,1/γ2)).
Proof. We first describe a protocol that allows Alice and Bob to solve the linear classification problem
with margin γusing O(1/γ2)bits of classical communication and shared randomness, assuming
γ >0. Note that this bound accords with the notion that the margin rather than the ambient dimension
sets the complexity of these types of problems, which is also manifest in the sample complexity of
learning with linearly separable data.
Alice and Bob share kNbits sampled i.i.d. from a uniform distribution over {0,1}, and that these
bits are arranged in a k×Nmatrix R. Alice and Bob then receive xandyrespectively, which are
valid inputs to the linear classification problem with margin γ. For any N-dimensional vector z,
define the random projection
f:Rn→Rk, f(z) =1
k(2R−1)z, (E.1)
where addition is element-wise. Applying the Johnson-Lindenstrauss lemma for projections with
binary variables [ 8], we obtain that if k=C/ε2, for some absolute constant C, then with probability
larger than 2/3we have for any z, z′∈ {x, y,0}(all of these being vectors in RN),fis an
approximate isometry in the sense
(1−ε)∥z−z′∥2
2≤ ∥f(z)−f(z′)∥2
2≤(1 +ε)∥z−z′∥2
2. (E.2)
The key feature of this result is that kis completely independent of N. Applying it repeatedly gives
∥f(x)−f(y)∥2
2− ∥f(x)∥2
2− ∥f(y)∥2
2≤(1 +ε)∥x−y∥2
2−2(1−ε)
f(x)·f(y) ≥(1 +ε)x·y−2ε.(E.3)
Obtaining an upper bound in a similar fashion using the converse inequalities, we have
(1 +ε)x·y−2ε≤f(x)·f(y)≤(1−ε)x·y+ 2ε. (E.4)
Assume now that x, y are valid inputs to the linear classification problem with margin γ, and
specifically that x·y≥γ. The lower bound above gives
(1 +ε)γ−2ε≤f(x)·f(y), (E.5)
and if we choose ε=γ/8we obtain
γ
2≤(1 +γ
8)γ−γ
4≤f(x)·f(y), (E.6)
where we used γ≤1. Similarly, if instead x·y≤ −γwe obtain
f(x)·f(y)≤ −(1−γ
8)γ+γ
4≤ −γ
2. (E.7)
It follows that if Alice computes f(x)and sends the resulting O(k) =O(1/γ2)bits that describe
this vector to Bob (assuming some finite precision that is large enough so as not to affect the margin,
which will contribute ), Bob can simply compute f(x)·f(y)which will reveal the result of the
classification problem, which he can then communicate to Alice using a single bit.
Ifγ= 0there is a trivial O(N)classical algorithm where Alice sends Bob x.
We next describe the quantum lower bound for Problem 7. Denote by dHthe Hamming distance
between binary vectors. We will use lower bounds for the following problem:
Problem 8 (Gap Hamming with general gap) .Alice and Bob are given ˆx,ˆy∈ {0,1}Nrespectively.
Given a promise that either dH(ˆx,ˆy)≥N/2 +g/2ordH(ˆx,ˆy)≤N/2−g/2, Alice and Bob must
determine which one is the case.
11This resource can have a dramatic effect on the communication complexity of a problem. The canonical
example is equality of Nbit strings, which can be solved with constant success probability using 1bit of
communication and shared randomness, while requiring Nbits of communication otherwise.
34There is a simple reduction from Problem 8 to Problem 7 for certain values of γ, which we will then
use to obtain a result for all γ. Assuming Alice is given ˆxand Bob is given ˆy, they construct unit
norm real vectors by x= (2ˆx−1)/√
N, y = (2ˆy−1)/√
Nwith addition performed element-wise.
IfdH(x, y)≥N/2 +g/2then
x·y=X
i,xi=yi1
N+X
i,xi̸=yi(−1
N)
≥N+g
21
N+N−g
2(−1
N)
=g
N.(E.8)
Similarly, dH(ˆx,ˆy)≤N/2−g/2⇒x·y≤ −g/N. It follows that x, yare valid inputs for a
linear classification problem over the unit sphere with margin 2g/N. From the results of [ 88], any
quantum algorithm for the Gap Hamming problem with gap g∈ {1, . . . , N }requires Ω(p
N/g)
qubits of communication. It follows that the linear classification problem requires Ω(p
1/γ)qubits
of communication. This bound holds for integer γN. To get a result for general 0< γ≤1we simply
note that the communication complexity must be a non-decreasing function of 1/γ, since any inputs
which constitute a valid instance with some γare also a valid instance for any γ′< γ. Given some
realγ, the resulting communication problem is at least as hard as the one with margin ⌈γN⌉/N≥γ.
It follows that a Ω(p
N/⌈γN⌉)bound holds for all 0< γ≤1.
Ifγ= 0, by a similar argument we can apply the lower bound for γ= 1/N, implying that Ω(√
N)
qubits of communication are necessary. Once again there is only a polynomial advantage at best.
F Expressivity of quantum circuits
F.1 Expressivity of compositional models
It is natural to ask how expressive models of the form of eq. (3.1) can be, given the unitarity constraint
of quantum mechanics on the matrices {Aℓ, Bℓ}. This is a nuanced question that can depend on the
encoding of the data that is chosen and the method of readout. On the one hand, if we pick |ψ(x)⟩
as in eq. (3.4) and use {Aℓ, Bℓ}that are independent of x, the resulting state |φ⟩will be a linear
function of xand the observables measured will be at most quadratic functions of those entries. On
the other hand, one could map bits to qubits 1-to-1 and encode any reversible classical function of
data within the unitary matrices {Aℓ(x)}with the use of extra space qubits. However, this negates the
possibility of any space or communication advantages (and does not provide any real computational
advantage without additional processing). As above, one prefers to work on more generic functions
in the amplitude and phase space, allowing for an exponential compression of the data into a quantum
state, but one that must be carefully worked with.
We investigate the consequences of picking {Aℓ(x)}that are nonlinear functions of x, and{Bℓ}
that are data-independent. This is inspired by a common use case in which Alice holds some data
or features of the data, while Bob holds a model that can process these features. Given a scalar
variable x, define Aℓ(x) = diag(( e−2πiλℓ1x, . . . , e−2πiλℓN′x))forℓ∈ {1, . . . , L }. We also consider
parameterized unitaries {Bℓ}that are independent of the {λℓi}and inputs x, y, and the state obtained
by interleaving the two in the manner of eq. (3.1) by |φ(x)⟩.
We next set λℓ1= 0 for all ℓ∈ {1, . . . , L }andλL2= 0. If we are interested in expressing the
frequency
Λj=L−1X
ℓ=1λℓjℓ, (F.1)
where jℓ∈ {2, . . . , N′}, we simply initialize with |ψ(x)⟩=|+⟩0|0⟩and use
Bℓ=|jℓ−1⟩⟨jℓ−1−1|+|jℓ−1−1⟩⟨jℓ−1|, (F.2)
withj1=jL= 2. It is easy to check that the resulting state is |φ(x)⟩= 
|0⟩+e−2πiΛjx|1⟩
/√
2.
Since the basis state |0⟩does not accumulate any phase, while the Bℓs swap the |1⟩state with
35the appropriate basis state at every layer in order to accumulate a phase corresponding to a single
summand in eq. (F.1). Choosing to measure the operator P0=X0, it follows that ⟨φ(x)|X0|φ(x)⟩=
cos(2 πΛjx).
It is possible to express O((N′)L−1)different frequencies in this way, assuming the Λjare distinct,
which will be the case for example with probability 1if the{λℓi}are drawn i.i.d. from some
distribution with continuous support. This further motivates the small Lregime where exponential
advantage in communication is possible. These types of circuits with interleaved data-dependent
unitaries and parameterized unitaries was considered for example in [ 104], and is also related to
the setting of quantum signal processing and related algorithms [ 74,77]. We also show that such
circuits can express dense function in Fourier space, and for small Nwe additionally find that these
circuits are universal function approximators (Appendix F.2), though in this setting the possible
communication advantage is less clear.
The problem of applying nonlinearities to data encoded efficiently in quantum states is non-trivial and
is of interest due to the importance of nonlinearities in enabling efficient function approximation [ 76].
One approach to resolving the constraints of unitarity with the potential irreversibility of nonlinear
functions is the introduction of slack variables via additional ancilla qubits, as typified by the
techniques of block-encoding [ 29,41]. Indeed, these techniques can be used to apply nonlinearities
to amplitude encoded data efficiently, as was recently shown in [ 100]. This approach can be applied
to the distributed setting as well. Consider the communication problem where Alice is given xas
input and Bob is given unitaries {U1, U2}overlogNqubits. Denote by σ:R→Ra nonlinear
function such as the sigmoid, exponential or standard trigonometric functions, and n= 2N. We show
the following:
Lemma 10. There exists a model |φσ⟩of the form definition 3.1 with L=O(log 1 /ε), N′= 2n′
where n′= 2n+ 4such that |φσ⟩=α|0⟩⊗n+4|ˆy⟩+|ϕ⟩for some α=O(1), where |ˆy⟩is a state
that obeys|ˆy⟩ −U21
∥σ(U1x)∥2σ(U1x)
2< ε. (F.3)
|ϕ⟩is a state whose first n+ 4registers are orthogonal to |0⟩⊗n+4.
Proof: Appendix B.
This result implies that with constant probability, after measurement of the first n+ 4qubits of |φσ⟩,
one obtains a state whose amplitudes encode the output of a single hidden layer neural network. It
may also be possible to generalize this algorithm and apply it recursively to obtain a state representing
a deep feed-forward network with unitary weight matrices.
It is also worth noting that the general form of the circuits we consider resembles self-attention
based models with their nonlinearities removed (motivated for example by [ 107]), as we explain in
Appendix F.3. Finally, in Appendix F.4 we discuss other strategies for increasing the expressivity of
these quantum circuits by combining them with classical networks.
F.2 Additional results on oscillatory features
Extending the unitaries considered in Appendix F.1 to more than one variable, for two scalar variables
x, ydefine
Aℓ(x) =diag(( e−2πiλℓ1x, . . . , e−2πiλℓN′x)), (F.4a)
Aℓ(x, y) =diag(( e−2πiλℓ1x, . . . , e−2πiλℓN′/2x, e−2πiλℓ,N′/2+1y, . . . , e−2πiλℓN′y)) (F.4b)
forℓ∈ {1, . . . , L }. Once again {Bℓ}are data-independent unitaries, and we denote by
|φ(x)⟩,|φ(x, y)⟩the states defined by interleaving these unitaries in the manner of eq. (3.1), and by
L1,L2the corresponding loss functions when measuring X0.
While the circuits in Appendix F.1 enable one to represent a small number of frequencies from a
set that is exponential in L, one can easily construct circuits that are supported on an exponentially
large number of frequencies, as detailed in Lemma 11. We also use measures of expressivity of
classical neural networks known as separation rank to show that circuits within the class eq. (3.1)
can represent complex correlations between their inputs. For a function fof two variables y, z, its
36separation rank is defined by
sep(f)≡min(
R:f(x) =RX
i=1gi(y)hi(z))
. (F.5)
If for example fcannot represent any correlations between yandz, then sep(f) = 1 . When
computed for certain classes of neural networks, y, zare taken to be subsets of a high-dimensional
input. The separation rank can be used for example to quantify the inductive bias of convolutional
networks towards learning local correlations [ 33], the effect of depth in recurrent networks [ 69], and
the ability of transformers to capture correlations across sequences as a function of their depth and
width [70].
We find that the output of estimating an observable using circuits of the form eq. (F.4) can be supported
on an exponential number of frequencies, and consequently has a large separation rank:
Lemma 11. For{λℓi}drawn i.i.d. from any continuous distribution and parameterized unitaries
{Bℓ}such that the real and imaginary parts of each entry in these matrices is a real analytic function
of parameters Θdrawn from a subset of RPL, aside from a set of measure 0over the choice of
{λℓi},{Bℓ},
i) The number of nonzero Fourier components in L1is
N′(N′−1)
2L−1
N′.
ii)
sep(L2) = 2N′(N′−1)
2L−1
N′. (F.6)
Proof: Appendix B
This almost saturates the upper bound on the number of frequencies that can be expressed by a circuit
of this form that is given in [ 104]. The separation rank implies that complex correlations between
different parts of the sequence can in principle be represented by such circuits. The constraint on
{Bℓ}is quite mild, and applies to standard choices of parameterize unitaries.
The main shortcoming of a result such as Lemma 11 is that it is not robust to measurement error
as it is based on constructing states that are equal weight superpositions of an exponential number
of terms. It is straightforward to show that circuits of this form can serve as universal function
approximators, at least for a small number of variables. For high-dimensional functions it is unclear
when a communication advantage is possible, as we describe below.
Lemma 12. Letfbe ap-times continuously differentiable function with period 1, and denote by ˆf:M
the vector of the first MFourier components of f. Ifˆf:M
1= 1then there exists a circuit of the
form eq. (3.1) overO(logM)qubits such that
∥L − f∥∞≤C
Mp−1/2(F.7)
for some absolute constant C.
Proof: Appendix B
This result improves upon the result in [ 93,104] about universal approximation with similarly
structured circuits both because it is non-asymptotic and because it shows uniform convergence rather
than convergence in L2. Non-asymptotic results universal approximation results were also obtained
recently by [ 43], however their approximation error scales polynomially with the number of qubits,
as opposed to exponentially as in Lemma 12.
The result of Lemma 12 applies to an L= 1circuit. The special hierarchical structure of the Fourier
transform implies that the same result can be obtained using even simpler circuits with larger L.
Consider instead single-qubit data-dependent unitaries over L+ 1qubits that take the form
Aℓ=|0⟩0⟨0|0+|1⟩0⟨1|0
|0⟩ℓ+1⟨0|ℓ+1+e2πi2ℓ−1x|1⟩ℓ+1⟨1|ℓ+1
, (F.8)
37forℓ∈ {1, . . . , L }. This is simply a single term in a hierarchical decomposition of the same feature
matrix we had in the shallow case, since
LY
ℓ=1Aℓ=|0⟩0⟨0|0⊗I1:L+|1⟩0⟨1|0⊗I1⊗
2L−1X
m=0e2πimx|m⟩⟨m|
, (F.9)
which is identical to eq. (B.45). As before, set
B1=ˆf
⟨0|+|0⟩
ˆf, (F.10)
withN′/4 = 2LandBℓ=Iforℓ >1. This again gives an approximation of fup to normalization.
The data-dependent unitaries are particularly simple when decomposed in this way. The fact that they
act on a single qubit and thus have "small width" is reminiscent of classical depth-separation result
such as [ 32], where it is shown that (roughly speaking) within certain classes of neural network, in
order to represent the function implemented by a network of depth L, a shallow network must have
width exponential in L. In this setting as well the expressive power as measured by the convergence
rate of the approximation error grows exponentially with Lby eq. (B.44).
The circuits above can be generalized in a straightforward way to multivariate functions of the form
f: [−1/2,1/2]D→Rand combined with multivariate generalization of eq. (B.44). In this case the
scalar mis replaced by a D-dimensional vector taking MDpossible values, and we can define
A1(x) =|0⟩0⟨0|0⊗I1:DlogM−1+|1⟩0⟨1|0⊗I1⊗
X
m∈[M]De2πim·x|m⟩⟨m|
. (F.11)
Note that using this feature map, the number of neurons is linear in the spatial dimension D. Because
of this, such circuits are not strictly of the form eq. (3.1) for general Nsince it is notthe case that
logN′=O(logN)where N′is the Hilbert space on which the unitaries in the circuit act and Nis
the size of x. An alternative setting where the features themselves are also learned from data could
enable much more efficient approximation of functions that are sparse in Fourier space.
F.3 Unitary Transformers
Transformers based on self-attention [ 109] form the backbone of large language models [ 25,16] and
foundation models more generally [ 20]. A self-attention layer, which is the central component of
transformers, is a map between sequences in RS×N′(where Sis the sequence length) defined in
terms of weight matrices WQ, WK, WV∈RN′×N′, given by
X′(X) = softmaxXW QWT
KXT
√
N
XW V≡A(X)XW V, (F.12)
where softmax( x)i=exi/P
iexifor a vector x, and acts row-wise on matrices. There is an extensive
literature on replacing the softmax-based attention matrix A(X)with matrices that can be computed
more efficiently, which can markedly improve the time complexity of inference and training without
a significant effect on performance [ 63,70]. In some cases A(X)is replaced by a unitary matrix [ 68].
Remarkably, recent work shows that models without softmax layers can in fact outperform standard
transformers on benchmark tasks while enabling faster inference and a reduced memory footprint
[107].
Considering a simplified model that does not contain the softmax operation as in [ 70] and dropping
normalization factors, the linear attention map is given by
X′
lin(X) =XW QWT
KXTXW V. (F.13)
Iterating this map twice gives
X′
lin(X′
lin(X)) =XW(1)
QW(1)T
KXTXW(1)
VW(2)
QW(2)T
KW(1)T
VXTX∗
W(1)
KW(1)T
QXTXW(1)
QW(1)T
KXTXW(1)
VW(2)
V.(F.14)
Iterating this map Ktimes (with different weight matrices at each layer) gives a function of the form:
X(K)
lin(X) =XR0(3K−1)/2Y
ℓ=1 
XTXRℓ
, (F.15)
38where the {Rℓ}matrices depend only on the trainable parameters. If we now constrain these to be
parameterized unitary matrices, and additionally replace XTXwith a unitary matrix UXencoding
features of the input sequence itself, then the i-th row of the output of this model is encoded in the
amplitudes of a state of the form eq. (3.5) with L= (3K−1)/2 + 1 ,|ψ(x)⟩=|Xi⟩, Aℓ(x) =
UX, Bℓ=Rℓ.
F.4 Ensembling and point-wise nonlinearities
An additional method for increasing expressivity while maintaining an advantage in communication
is through ensembling. Given Kmodels of the form Definition 3.1 with Pparameters each, one can
combine their loss functions L1, . . . ,LKinto any differentiable nonlinear function
˜L(L1(Θ1, x), . . . ,LK(ΘK, x),˜Θ, x) (F.16)
that depends on additional parameters ˜Θ. As long as Kand|˜Θ|scale subpolynomially with Nand
P, the gradients for this more expressive model can be computed while maintaining the exponential
communication advantage in terms of N, P .
G Realizing quantum communication
Given the formidable engineering challenges in building a large, fault tolerant quantum processor
[11,44], the problem of exchanging coherent quantum states between such processors might seem
even more ambitious. We briefly outline the main problems that need to be solved in order to realize
quantum communication and the state of progress in this area, suggesting that this may not be the
case.
We first note that sending a quantum state between two processors can be achieved by the well-known
protocol of quantum state teleportation [ 18,45]. Given an nqubit state |ψ⟩, Alice can send |ψ⟩to
Bob by first sharing nBell pairs of the form
|b⟩=1√
2(|0⟩|0⟩+|1⟩|1⟩), (G.1)
(sharing such a state involves sending a one of the two qubits to Bob) and subsequently performing
local processing on the Bell pairs and exchanging nbits of classical communication. Thus quantum
communication can be reduced to communicating Bell pairs up to a logarithmic overhead, and does
not require say transmitting an arbitrary quantum state in a fault tolerant manner, which appears to be
a daunting challenge given the difficulty of realizing quantum memory on a single processor. Bell
pairs can be distributed by a third party using one-way communication.
In order to perform quantum teleportation, the Bell pairs must have high fidelity. As long as the
fidelity of the communicated Bell pairs is above .5, purification can be used produce high fidelity
Bell pairs [ 19], with the fidelity of the purified Bell pair increasing exponentially with the number of
pairs used. Thus communicating arbitrary quantum states can be reduced to communicating noisy
Bell pairs.
Bell pair distribution has been demonstrated across multiple hardware platforms including supercon-
ducting waveguides [75], optical fibers [64], free space optics at distances of over 1,200kilometers
[71]. At least in theory, even greater distances can be covered by using quantum repeaters, which
span the distance between two network nodes. Distributing a Bell pair between the nodes can then be
reduced to sharing Bell pairs only between adjacent repeaters and local processing [12].
A major challenge in implementing a quantum network is converting entangled states realized in
terms of photons used for communication to states used for computation and vice versa, known as
transduction [ 66]. Transduction is a difficult problem due to the several orders of magnitude in energy
that can separate optical photons from the energy scale of the platform used for computation. Proof of
principle experiments have been performed across a number of platforms including trapped ions [ 64],
solid-state systems [95], and superconducting qubits operating at microwave frequencies [14, 112].
H Privacy of Quantum Communication
In addition to an advantage in communication complexity, the quantum algorithms may potentially
lead to advantages in terms of privacy. It is well known that the number of bits of information that
39can be extracted from an unknown quantum state is proportional to the number of qubits. It follows
immediately that since the above algorithm requires exchanging a logarithmic number of copies of
states over O(logN)qubits, even if all the communication between the two players is intercepted,
an attacker cannot extract more than a logarithmic number of bits of classical information about the
input data or model parameters. Specifically, we have:
Corollary 1. If Alice and Bob are implementing the quantum algorithm for gradient estimation
described in Lemma 2, and all the communication between Alice and Bob is intercepted by an
attacker, the attacker cannot extract more than ˜O(L2(logN)2(logP)2log(L/δ)/ε4)bits of classical
information about the inputs to the players.
This follows directly from Holevo’s theorem [ 52], since the multiple copies exchanged in each round
of the protocol can be thought of as a quantum state over ˜O((log N)2(logP)2log(L/δ)/ε4)qubits.
As noted in [ 3], this does not contradict the fact that the protocol allows one to estimate all Pelements
of the gradient, since if one were to place some distribution over the inputs, the induced distribution
over the gradient elements will generally exhibit strong correlations. An analogous result holds for
the inference problem described in Lemma 1.
It is also interesting to ask how much information either Bob or Alice can extract about the inputs of
the other player by running the protocol. If this amount is logarithmic as well, it provides additional
privacy to both the model owner and the data owner. It allows two actors who do not necessarily
trust each other, or the channel through which they communicate, to cooperate in jointly training
a distributed model or using one for inference while only exposing a vanishing fraction of the
information they hold.
It is also worth mentioning that data privacy is also guaranteed in a scenario where the user holding
the data also specifies the processing done on the data. In this setting, Alice holds both data xand
a full description of the unitaries she wishes to apply to her state. She can send Bob a classical
description of these unitaries, and as long as the data and features are communicated in the form of
quantum states, only a logarithmic amount of information can be extracted about them. In this setting
there is of course no advantage in communication complexity, since the classical description of the
unitary will scale like poly( N, P).
I Some Open Questions
I.1 Expressivity
Circuits that interleave parameterized unitaries with unitaries that encode features of input data
are also used in Quantum Signal Processing [ 74,77], where the data-dependent unitaries are time
evolution operators with respect to some Hamiltonian of interest. The original QSP algorithm involved
a single parameterized rotation at each layer, and it is also known that extending the parameter space
from U(1)toSU(2)by including an additional rotation improves the complexity of the algorithm
and improves its expressivity [ 85]. In both cases however the expressive power (in terms of the
degree of the polynomial of the singular values that can be expressed) grows only linearly with the
number of interleaved unitaries. Given the natural connection to the distributed learning problems
considered here, it is interesting to understand the expressive power of such circuits with more
powerful multi-qubit parameterized unitaries.
We present a method of applying a single nonlinearity to a distributed circuit using the results of
[100]. Since this algorithm requires a state-preparation unitary as input and produces a state with
a nonlinearity applied to the amplitudes, it is natural to ask whether it can be applied recursively
to produce a state with the output of a deep network with nonlinearties encoded in its amplitudes.
This will require extending the results of [ 100] to handle noisy state-preparation unitaries, yet the
effect of errors on compositions of block encodings [ 29,41], upon which these results are based, is
relatively well understood. It is also worth noting that these approaches rely on the approximation of
nonlinear functions by polynomials, and so it may also be useful to take inspiration directly from
classical neural network polynomial activations, which in some settings are known to outperform
other types of nonlinearities [80].
40I.2 Optimization
The results of Appendix D rely on sublinear convergence rates for general stochastic optimization
of convex functions (Lemma 8). It is known however that using additional structure, stochastic
gradients can be used to obtain linear convergence (meaning that the error decays exponentially
with the number of iterations). This is achievable when subsampling is the source of stochasticity
[67], or with occasional access to noiseless gradients in order to implement a variance reduction
scheme [ 60,84,46], neither of which seem applicable to the setting at hand. It is an interesting open
question to ascertain whether there is a way to exploit the structure of quantum circuits to obtain
linear convergence rates using novel algorithms. Aside from advantages in time complexity, this
could imply an exponential advantage in communication for a more general class of circuits.
Conversely, it is also known that given only black-box access to a noisy gradient oracle, an information-
theoretic lower bound of Ω(1/T)on the error holds given Toracle queries, precluding linear
convergence without additional structure, even for strongly convex objectives [ 9]. [49] provide a
similar lower bound for their algorithm, at least for a restricted class of circuits. Perhaps these results
be used to show optimality of algorithms that rely on the standard variational circuit optimization
paradigm that involves making quantum measurements at every iteration and using these to update
the parameters. This might imply that linear convergence is only possible if the entire optimization
process is performed coherently.
In this context, we note that the treatment of gradient estimation at every layer and every iteration
as an independent shadow tomography problem is likely highly suboptimal, since no use is made
of the correlations across iterations between the states and the observables of interest. While in
Appendix D.2 this is not the case, that algorithm applies only to fine-tuning of a single layer. Is
there a way to re-use information between iterations to reduce the resource requirements of gradient
descent using shadow tomography? One approach could be warm-starting the classical resource
states by reusing them between iterations. Improvements along these lines might find applications for
other problems as well.
I.3 Exponential advantage under weaker assumptions
The lower bound in Lemma 3 applies to circuits that contain general unitaries, and thus have depth
poly( N)when compiled using a reasonable gate set. One can ask whether the lower bound can
be strengthened to apply to more restricted classes of unitaries as well, and in particular log-depth
unitaries. While it is known that exponential communication advantages require the circuits to have
poly( N)gate complexity overall [ 7], this does not rule out the possibility of computational separations
resulting from the clever encoding and transmission of states nor does it rule out communication
advantages resulting from very short time preparations from log-depth protocols. The rapid growth
of complexity of random circuits composed from local gates with depth suggests that this might be
possible [ 24]. This is particularly interesting since Algorithm 1 requires only a single measurement
per iteration and may thus be suitable for implementation on near-term devices whose coherence
times restrict them to implementing shallow circuits. It has also been recently shown that an
exponential quantum advantage in communication holds for a problem which is essentially equivalent
to estimating the loss of a circuit of the form Definition 3.1 with L= 2, under a weaker model of
quantum communication than the standard one we consider [ 10]. This is the one-clean-qubit model,
in which the initial state |ψ(x)⟩consists of a single qubit in a pure state, while all other qubits are in
a maximally mixed state.
J Experiments additional details
J.1 Node classification training
We use the same training regime for all datasets using the recommended hyperparameters in DGL
[113] examples, reported in Table 3.
We trained each model 10 times for all three datasets using a single NVIDIA RTX A6000, taking
approximately 15 minutes per execution.
41Table 3: Hyper parameters of node classification training
Hyperparameter Value
Hidden dimension 512
SIGN hops 5
Learning rate 0.001
Input dropout 0.3
Hidden dropout 0.4
Weight decay 0.0
Table 4: Hyper parameters of node classification training
Hyperparameter Values
Hidden dimension 8,12,16,32,64,96,128,148,256
SIGN hops (per operation) [0-10]
Learning rate 0.001, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1
Input dropout 0.0
Hidden dropout 0.0, 0.1, 0.2, 0.3, 0.4, 0.5
Weight decay 0.0, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4
Batch size 32, 64, 128, 256, 512, 1024
Normalization layer BatchNorm, LayerNorm, none
J.2 Graph classification training
As, to the best of our knowledge, we are the first to use a SIGN variant on graph classification tasks,
we conducted a comprehensive hyperparameter tuning for the model structure (including the number
of message passing operators, the hidden dimension, and normalization after the hidden layer) and
optimization settings. The tuning was performed using Bayesian hyperparameter optimization to
identify the optimal values for each dataset. This process involved varying the hidden dimension, the
number of SIGN hops per operation, the learning rate, and dropout rates. The values considered for
each hyperparameter are detailed in Table 4. The full results of these experiments are in Table 5.
We scan each task for approximately 150 runs, using a single NVIDIA RTX A6000.
Table 5: Graph Classification Test Accuracy. Our model achieves comparable results to GIN and
other known models on most datasets.
Dataset
Model MUTAG PTC NCI1 PROTEINS COLLAB IMDB-B IMDB-M REDDIT-B REDDIT-M
GIN [114] 89.40 ±5.60 64.60 ±7.0 82.17 ±1.7 76.2 ±2.8 80.2 ±1.90 75.1 ±5.1 52.3 ±2.8 92.4 ±2.5 57.5 ±1.5
DropGIN[92] 90.4 ±7.0 66.3 ±8.6 - 76.3 ±6.1 - 75.7 ±4.2 51.4 ±2.8 - -
DGCNN[118] 85.8 ±1.7 58.6 ±2.5 - 75.5 ±0.9 - 70.0 ±0.9 47.8 ±0.9 - -
U2GNN [89] 89.97 ±3.65 69.63 ±3.60 - 78.53 ±4.07 77.84 ±1.48 77.04 ±3.45 53.60 ±3.53 - -
HGP-SL[119] - - 78.45 ±0.77 84.91 ±1.62 - - - - -
WKPI[120] 88.30 ±2.6 68.10 ±2.4 87.5 ±0.5 78.5 ±0.4 - 75.1 ±1.1 49.5 ±0.4 - 59.5 ±0.6
SIGN (ours) 92.02 ±6.45 68.0 ±8.17 77.25 ±1.42 76.55 ±5.10 81.82 ±1.42 76 ±2.49 53.13 ±3.01 78.95 ±2.72 54.09 ±1.76
J.3 Empirical bounds
We measure ∥W1∥and∥W2∥∞of the trained graph classification models in Section 5.2.1, corre-
sponding to Equation (4.2) and report the average results over 10 runs in Table 6 (note that we use
P=Iso that no pooling matrix is present, and in any case the pooling window will typically be a
small constant). W1is constructed as a block diagonal matrix of the weights of the SIGN hidden
layer.∥W2∥∞is the infinity norm of the weight matrix of the output layer of SIGN, multiplied by 2
(since we compute differnces between numbers of nodes in two classes).
We measure the score difference of the graph classification task in Section 5.2.1 and compare them
to the differences of the class sizes in Figure 3. Most of the differences are significant (larger than
1/poly( N)where Nis the number of nodes; see fig. 3(c)). Some class pairs have low differences
making them indistinguishable, however, fig. 3(a),(b) indicate those are typically classes with similar
42Table 6: Weight norms of the graph classification models. We measure the norms of the final decision
problem models, averaging the values over 8 runs.
Dataset
Value OGBN-Products Reddit Cora
∥W1∥ 3.46±0.27 1.32 ±0.12 8.5 ±8.0
∥W2∥∞0.13±0.02 0.04 ±0.00 0.1 ±0.07
#nodes 2,449,029 232,965 2708
0 10 20 30 40
Class0
10
20
30
40ClassClass size difference ogbn-products test
0100000200000300000400000500000
(a)
0 10 20 30 40
Class0
10
20
30
40ClassClass score difference ogbn-products test
02468 (b)
0.0 0.5 1.0 1.5 2.0
Difference0510152025Class pairs countClass score difference ogbn-products test (c)
Figure 3: (a): Difference between class sizes in ogbn-products test set. (b) Difference between
the graph classification model class scores. The score differences are correlated to the class size
differences. (c) Histogram of the class pairs differences. Most of the differences are significantly
larger than 1/N.
number of nodes. This provides evidence that when there is a considerable class imbalance (i.e. one
that scales with system size), the magnitude of the model output when computing this difference will
not decay with the size of the graph.
While evidence of asymptotic scaling will require experiments on graphs of different sizes, our results
suggest that the upper bound in Lemma 6 is not large for models trained on standard benchmarks,
implying that they can be efficiently simulated on a quantum computer.
43NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: See end of introduction for list of contributions and pointers to additional
details and proofs.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See the Discussion section for the main limitations we have identified.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
44Justification: Full proofs are given in the appendices (mostly Appendix B). Assumptions are
stated in Lemma and Theorem statements.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Experiments are explained in Section 5 and Appendix J.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
45Answer: [Yes]
Justification: All datasets are publicly available in DGL library.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Hyperparameters and hyper parameters tuning ranges are mentioned in Ap-
pendix J. Data splits are either fixed or identical to the DGL examples default.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: All experimental results include standard deviation.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
46•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The used computation resources are mentioned in Appendix J.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer:[Yes]
Justification: The code of ethics has been conformed to.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This work applies to models of computation that have not yet been realized. It
serves to motivate further study of these and will have no societal impact within the next
several years.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
47•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: No risk of misuse (see broader impacts answer).
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: All datasets are from DGL library.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
48•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Not relevant.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Not relevant.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
49