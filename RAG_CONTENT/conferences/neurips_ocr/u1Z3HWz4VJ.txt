RAMP: Boosting Adversarial Robustness Against
Multiple lpPerturbations for Universal Robustness
Enyi Jiang
Department of Computer Science
University of Illinois Urbana-Champaign
Urbana, IL 61801
enyij2@illinois.eduGagandeep Singh
Department of Computer Science
University of Illinois Urbana-Champaign
Urbana, IL 61801
ggnds@illinois.edu
Abstract
Most existing works focus on improving robustness against adversarial attacks
bounded by a single lpnorm using adversarial training (AT). However, these
AT models’ multiple-norm robustness (union accuracy) is still low, which is cru-
cial since in the real-world an adversary is not necessarily bounded by a single
norm. The tradeoffs among robustness against multiple lpperturbations and accu-
racy/robustness make obtaining good union and clean accuracy challenging. We
design a logit pairing loss to improve the union accuracy by analyzing the tradeoffs
from the lens of distribution shifts. We connect natural training (NT) with AT via
gradient projection, to incorporate useful information from NT into AT, where we
empirically and theoretically show it moderates the accuracy/robustness tradeoff.
We propose a novel training framework RAMP , to boost the robustness against
multiple lpperturbations. RAMP can be easily adapted for robust fine-tuning and
full AT. For robust fine-tuning, RAMP obtains a union accuracy up to 53.3%on
CIFAR-10, and 29.1%on ImageNet. For training from scratch, RAMP achieves a
union accuracy of 44.6%and good clean accuracy of 81.2%on ResNet-18 against
AutoAttack on CIFAR-10. Beyond multi-norm robustness RAMP -trained models
achieve superior universal robustness , effectively generalizing against a range of
unseen adversaries and natural corruptions.
1 Introduction
Though deep neural networks (DNNs) demonstrate superior performance in various vision appli-
cations, they are vulnerable against adversarial examples [Goodfellow et al., 2014, Kurakin et al.,
2018]. Adversarial training (AT) [Tramèr et al., 2017, Madry et al., 2017] which works by injecting
adversarial examples into training for enhanced robustness, is currently the most popular defense.
However, most AT methods address only a single type of perturbation [Wang et al., 2020, Wu
et al., 2020, Carmon et al., 2019, Gowal et al., 2020, Raghunathan et al., 2020, Zhang et al., 2021,
Debenedetti and Troncoso—EPFL, 2022, Peng et al., 2023, Wang et al., 2023]. An l∞robust model
may not be robust against lp(p̸=∞)attacks. Also, enhancing robustness against one perturbation
type can sometimes increase vulnerability to others [Engstrom et al., 2017, Schott et al., 2018]. On
the contrary, training a model to be robust against multiple lpperturbations is crucial as it reflects
real-world scenarios [Sharif et al., 2016, Eykholt et al., 2018, Song et al., 2018, Athalye et al., 2018]
where adversaries can use multiple lpperturbations. We show that multi-norm robustness is the key
to improving generalization against other threat models [Croce and Hein, 2022]. For instance, we
show it enables robustness against perturbations not easily defined mathematically, such as image
corruptions and unseen adversaries [Wong and Kolter, 2020].
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Two main challenges exist for training models robust against multiple perturbations: (i) tradeoff
among robustness against different perturbation models [Tramer and Boneh, 2019] and (ii) tradeoff
between accuracy and robustness [Zhang et al., 2019, Raghunathan et al., 2020]. Adversarial examples
induce a shift from the original distribution, causing a drop in clean accuracy with AT [Xie et al.,
2020, Benz et al., 2021]. The distinct distributions created by l1, l2, l∞adversarial examples make
the problem even more challenging. Through a finer analysis of the distribution shifts caused by these
adversaries, we propose the RAMP framework to efficiently boost the Robustness Against Multiple
Perturbations. RAMP can be used for both fine-tuning and training from scratch. It utilizes a novel
logit pairing loss on a certain pair and connects NT with AT via gradient projection [Jiang et al.,
2023] to improve union accuracy while maintaining good clean accuracy and training efficiency.
Logit pairing loss. We visualize the changing of l1, l2, l∞robustness when fine-tuning a l∞-AT
pre-trained model in Figure 1 using the CIFAR-10 training dataset. The DNN loses substantial
robustness against l∞attack after only 1epoch of fine-tuning: l1fine-tuning and E-AT [Croce and
Hein, 2022] (red and yellow histograms under Linf category) both lose significant l∞robustness
(compared with blue histogram under Linf category). Inspired by this observation, we devise a new
logit pairing loss for a lq−lrtradeoff pair to attain better union accuracy, which enforces the logit
distributions of lqandlradversarial examples to be close, specifically on the correctly classified lq
subsets. In comparison, our method (green histogram under Linf and union categories) preserves
more l∞and union robustness than others after 1epoch. We show this technique works on larger
models and datasets (Section 5.1).
Accuracy
0.20.40.60.8
Linf L1 L2 UnionEpoch 0 Epoch 1 - Fintune L1 Epoch 1 - EAT Epoch 1 - RAMP
Figure 1: Multiple-norm tradeoff with robust fine-
tuning : We observe that fine-tuning on l∞-AT model
using l1examples drastically reduces l∞robustness.
RAMP preserves more l∞and union robustness.Connect natural training (NT) with AT. We
explore the connections between NT and AT
to obtain a better accuracy/robustness trade-
off. We find that NT can help with adversarial
robustness: useful information in natural dis-
tribution can be extracted and leveraged to
achieve better robustness. To this end, we
compare the similarities of model updates of
NT and AT layer-wise for each epoch, where
we find and incorporate useful NT compo-
nents into AT via gradient projection (GP),
as outlined in Algorithm 2. In Figure 2 and
Section 5.1, we empirically and theoretically
show this technique strikes a better balance
between accuracy and robustness, for both sin-
gle and multiple lpperturbations. We provide
a theoretical analysis of why GP works for adversarial robustness in Theorem A.2 & 4.5.
Main contributions :
•We design a new logit pairing loss to mitigate the lq−lrtradeoff for better union accuracy, by
enforcing the logit distributions of lqandlradversarial examples to be close.
•We empirically and theoretically show that connecting NT with AT via gradient projection better
balances the accuracy/robustness tradeoff for lpperturbations, compared with standard AT.
•RAMP achieves good union accuracy, accuracy-robustness tradeoff, and generalizes better to
diverse perturbations and corruptions (Section 5.1) achieving superior universal robustness (75.5%
for common corruption and 26.1%union accuracy against unseen adversaries). RAMP fine-tuned
DNNs achieve union accuracy up to 53.3%on CIFAR-10, and 29.1%on ImageNet. RAMP
achieves a 44.6%union accuracy and good clean accuracy on ResNet-18 against AutoAttack on
CIFAR-10. Our code is available at https://github.com/uiuc-focal-lab/RAMP .
2 Related Work
Adversarial training (AT). Adversarial Training (AT) usually employs gradient descent to discover
adversarial examples, incorporating them into training for enhanced adversarial robustness [Tramèr
et al., 2017, Madry et al., 2017]. Numerous works focus on improving robustness by exploring
the trade-off between robustness and accuracy [Zhang et al., 2019, Wang et al., 2020], instance
reweighting [Zhang et al., 2021], loss landscapes [Wu et al., 2020], wider/larger architectures [Gowal
2et al., 2020, Debenedetti and Troncoso—EPFL, 2022], data augmentation [Carmon et al., 2019,
Raghunathan et al., 2020], and using synthetic data [Peng et al., 2023, Wang et al., 2023]. However,
these methods often yield DNNs robust against a single perturbation type while remaining vulnerable
to other types.
Robustness against multiple perturbations. Tramer and Boneh [2019], Kang et al. [2019] observe
that robustness against lpattacks does not necessarily transfer to other lqattacks ( q̸=p). Previous
studies [Tramer and Boneh, 2019, Maini et al., 2020, Madaan et al., 2021, Croce and Hein, 2022]
modified Adversarial Training (AT) to enhance robustness against multiple lpattacks, employing
average-case [Tramer and Boneh, 2019], worst-case [Tramer and Boneh, 2019, Maini et al., 2020], and
random-sampled [Madaan et al., 2021, Croce and Hein, 2022] defenses. There are also works [Nandy
et al., 2020, Liu et al., 2020, Xu et al., 2021, Xiao et al., 2022, Maini et al., 2022] using preprocessing,
ensemble methods, mixture of experts, and stability analysis to solve this problem. Ensemble
models and preprocessing methods are weakened since their performance heavily relies on correctly
classifying or detecting various types of adversarial examples. In certified training, Banerjee et al.
[2024], Banerjee and Singh [2024] propose verification/certifiable training methods under different
threat models for lpuniversal adversarial perturbation. However, prior works are hard to scale to
larger models and datasets, e.g. ImageNet, due to the efficiency issue. Furthermore, Croce and Hein
[2022] devise Extreme norm Adversarial Training (E-AT) and fine-tune a lprobust model on another
lqperturbation to quickly make a DNN robust against multiple lpattacks. However, E-AT does not
adapt to varying epsilon values. Our work demonstrates that the suboptimal tradeoff observed in prior
studies can be improved with our proposed framework.
Logit pairing in adversarial training. Adversarial logit pairing methods encourage logits for pairs
of examples to be similar [Kannan et al., 2018, Engstrom et al., 2018]. People apply this technique to
both clean images and their adversarial counterparts, to devise a stronger form of adversarial training.
In our work, we devise a novel logit pairing loss to train a DNN originally robust against lpattack to
become robust against another lq(q̸=p)attack on the correctly predicted lpsubsets, which helps
gain better union accuracy.
Adversarial versus distributional robustness. Sinha et al. [2018] theoretically studies the AT
problem through distributional robust optimization. Mehrabi et al. [2021] establishes a pareto-optimal
tradeoff between standard and adversarial risks by perturbing the test distribution. Other works explore
the connection between natural and adversarial distribution shifts [Moayeri et al., 2022, Alhamoud
et al., 2023], assessing transferability and generalizability of adversarial robustness across datasets.
However, little research delves into distribution shifts induced by l1, l2, l∞adversarial examples and
their interplay with the robustness-accuracy tradeoff [Zhang et al., 2019, Yang et al., 2020, Rade and
Moosavi-Dezfooli, 2021]. Our work, inspired by recent domain adaptation techniques [Jiang, 2023,
Jiang et al., 2023], designs a logit pairing loss and utilizes model updates from NT via GP to enhance
adversarial robustness. We show that GP adapts to both single and multi-norm scenarios.
3 AT against Multiple Perturbations
We consider a standard classification task with samples {(xi, yi)}N
i=0from an empirical data dis-
tribution bDn; we have input images x∈Rdand corresponding labels y∈Rk. Standard training
aims to obtain a classifier fparameterized by θto minimize a loss function L:Rk×Rk→Ron
bDn. Adversarial training (AT) [Madry et al., 2017, Tramèr et al., 2017] aims to find a DNN robust
against adversarial examples. It is framed as a min-max problem where a DNN is optimized using
the worst-case examples within an adversarial region around each xi. Different types of adversarial
regions Bp(x, ϵp) ={x′∈Rd:∥x′−x∥p≤ϵp}can be defined around a given image xusing
various lp-based perturbations. Formally, we can write the optimization problem of AT against a
certain lpattack as follows:
min
θE(x,y)∼bDn
max
x′∈Bp(x,ϵp)L(f(x′), y)
The above optimization is only for certain pvalues and is usually vulnerable to other perturbation
types. To this end, prior works have proposed several approaches to train the network robust
against multiple perturbations ( l1, l2, l∞) at the same time. We focus on the union threat model
3∆ =B1(x, ϵ 1)∪B2(x, ϵ 2)∪B∞(x, ϵ∞)which requires the DNN to be robust within the l1, l2, l∞
adversarial regions simultaneously [Croce and Hein, 2022]. Union accuracy is then defined as the
robustness against ∆(i)for each xisampled from D. In this paper, similar to the prior works, we use
union accuracy as the main metric to evaluate the multiple-norm robustness. Apart from that, we
define universal robustness as the generalization ability against a range of unseen adversaries and
common corruptions. Specifically, we have average accuracy across five severity levels for common
corruption and union accuracy against a range of unseen adversaries used in Laidlaw et al. [2020].
Worst-case defense follows the following min-max optimization problem to train DNNs using the
worst-case example from the l1, l2, l∞adversarial regions:
min
θE(x,y)∼bDn
max
p∈{1,2,∞}max
x′∈Bp(x,ϵp)L(f(x′), y)
MAX [Tramer and Boneh, 2019] and MSD [Maini et al., 2020] fall into this category. Finding
worst-case examples yields a good union accuracy but results in a loss of clean accuracy as the
distribution of generated examples is different from the clean data distribution.
Average-case defense train DNNs using the average of the l1, l2, l∞worst-case examples:
min
θE(x,y)∼bDn
Ep∈{1,2,∞} max
x′∈Bp(x,ϵp)L(f(x′), y)
A VG [Tramer and Boneh, 2019] is of this type. This method generally leads to good clean accuracy
but suboptimal union accuracy as it does not penalize worst-case behavior within the l1, l2, l∞
regions.
Random-sampled defense. The defenses mentioned above lead to a high training cost as they
compute multiple attacks for each sample. SAT [Madaan et al., 2021] and E-AT [Croce and Hein,
2022] randomly sample one attack out of each type at a time, contributing to a similar computational
cost as standard AT on a single perturbation model. They achieve a slightly better union accuracy
compared with A VG and relatively good clean accuracy. However, they are not better than worst-case
defenses for multiple-norm robustness, since they do not consider the strongest attack within the
union region all the time.
4 RAMP
There are two main tradeoffs in achieving better union accuracy while maintaining good accuracy: 1.
Among perturbations: there is a tradeoff among different attacks, e.g., a l∞pre-trained AT DNN is
not robust against l1, l2perturbations, which makes the union accuracy harder to attain. Also, we
observe there exists a main tradeoff pair of two attacks among the union over l1,l2,l∞attacks. 2.
Accuracy and robustness: all defenses lead to degraded clean accuracy. To address these tradeoffs,
we study the problem from the lens of distribution shifts.
Interpreting tradeoffs from the lens of distribution shifts. The adversarial examples with respect
to an empirical data distribution bDn, adversarial region Bp(x, ϵp), and DNN fθgenerate a new
adversarial distribution bDawith samples {(x′
i, yi)}N
i=0, that are correlated by adding certain perturba-
tions but different from the original bDn. Because of the shifts between bDnandbDa, DNN decreases
performance on bDnwhen we move away from it and towards bDa. Also, the distinct distributions
created by multiple perturbations, bDl1a,bDl2a,bDl∞a, contribute to the tradeoff among l1, l2, l∞attacks.
To address the tradeoff among perturbations while maintaining good efficiency, we focus on the
distributional interconnections between bDnandbDl1a,bDl2a,bDl∞a. From the insights we get from above,
we propose our framework RAMP , which includes (i) logit pairing to improve tradeoffs among
multiple perturbations, and (ii) identifying and combining the useful DNN components using the
model updates from NT and AT, to obtain a better robustness/accuracy tradeoff.
Identify the Key Tradeoff Pair. We study the common case with lpnorms ϵ1= 12, ϵ2= 0.5, ϵ∞=
8
255on CIFAR-10 [Tramer and Boneh, 2019]. The distributions generated by the two strongest attacks
show the largest shifts from bDn; also, they have the largest distribution shifts between each other
4because of larger and most distinct search areas. Thus, by comparing the single norm robustness of lp
adversarially trained models, we select the two lp-AT models with the lowest lprobustness against
themselves as the key tradeoff pair. They refer to the strongest attack since their lprobustness is
low. The attack with the highest lprobustness is mostly included by the convex hull of the other two
stronger attacks [Croce and Hein, 2022]. Here we identify l∞−l1as the key tradeoff pair.
4.1 Logit Pairing for Multiple Perturbations
Figure 1: Finetuning a lq-AT model on lrexamples reduces lqrobustness. To get a finer analysis
of the l∞−l1tradeoff mentioned above, we visualize the changing of l1, l2, l∞robustness of the
training dataset when we fine-tune a l∞pre-trained model with l1examples for 1epochs, as shown
in Figure 1: x-axis represents the robustness against different attacks and y-axis is the accuracy.
After 1 epoch of finetuning on l1examples or performing E-AT, we lose much l∞robustness since
blue/yellow histograms are much lower than the red histogram under the Linf category. RAMP
preserves both l∞and union robustness more effectively: the green histogram is higher than the
red/yellow histogram under Linf and Union categories. Specifically, RAMP maintains 14%, 28%
more union robustness than E-AT and l1fine-tuning. The above observations indicate the necessity
of preserving more lqrobustness as we adversarially fine-tune with lradversarial examples on a
lqpre-trained AT model, with lq−lras the key tradeoff pair, which inspires us to design our loss
design with logit pairing. We want to enforce the union predictions between lqandlr(q̸=r)attacks:
bringing the predictions of lqandlr(q̸=r)close to each other, specifically on the correctly predicted
lqsubsets. Based on our observations, we design a new logit pairing loss to enforce a DNN robust
against one lqattack to be robust against another lr(q̸=r)attack.
Enforcing the Union Prediction via Logit Pairing. Thelq−lr(q̸=r)tradeoff leads us to the
following principle to improve union accuracy: for a given set of images, when we have a DNN
robust against some lqexamples, we want it to be robust against lrexamples as well. This serves as
the main insight for our loss design: we want to enforce the logits predicted by lqandlradversarial
examples to be close, specifically on the correctly predicted lqsubsets. To accomplish this, we design
a KL-divergence (KL) loss between the predictions from lqandlrperturbations. For each batch
of data (x, y)∼ D , we generate lqandlradversarial examples x′
q, x′
rand their predictions pq, pr
using APGD [Croce and Hein, 2020]. Then, we select indices γ, which part elements of pqcorrectly
predicts the ground truth y. We denote the size of the indices as nc, and the batch size as N. We
compute a KL-divergence loss over this set of samples using KL(pq[γ]∥pr[γ])(Eq. 1). For the subset
indexed by γ, we want to push its lrlogit distribution towards its lqlogit distribution, such that we
prevent losing more lqrobustness when training with lradversarial examples.
LKL=1
nc·ncX
i=1kX
j=0pq[γ[i]][j]·logpq[γ[i]][j]
pr[γ[i]][j]
(1)
To further boost the union accuracy, apart from the KL loss, we add another loss term using a
MAX-style approach in Eq. 2: we find the worst-case example between lqandlradversarial regions
by selecting the example with the higher loss. Lmax is a cross-entropy loss over the approximated
worst-case adversarial examples. Here, we use Lceto represent the cross-entropy loss. Our final loss
Lcombines LKLandLmax, via a hyper-parameter λin Eq. 3.
Lmax=1
NNX
i=0
max
p∈{q,r}max
x′
i∈Bp(x,ϵp)Lce(f(x′
i), yi)
(2) L=Lmax+λ· LKL (3)
Algorithm 1 shows the pseudocode of robust fine-tuning with RAMP that leverages logit pairing.
4.2 Connecting Natural Training with AT
To improve the robustness and accuracy tradeoff against multiple perturbations, we explore the
connections between AT and NT. Since extracting valuable information in NT aids in improving
robustness (Section 4.2), we use gradient projection [Jiang et al., 2023] to compare and integrate
natural and adversarial model updates, which yields an improved tradeoff between robustness and
accuracy.
5NT can help adversarial robustness. Let us consider two models f1andf2, where f1is randomly
initialized and f2undergoes NT on bDnforkepochs: f2results in a better decision boundary and
higher clean accuracy. Performing AT on f1andf2subsequently, intuitively, f2becomes more robust
thanf1due to its improved decision boundary, leading to fewer misclassifications of adversarial
examples. This effect is empirically shown in Figure 2. For AT(blue), standard AT against l∞
attack [Madry et al., 2017] is performed, while for AT-pre (red), 50epochs of pre-training precede
the standard AT procedure. AT-pre shows superior clean and robust accuracy on CIFAR-10 against
l∞PGD-20 attack with ϵ∞= 0.031. Despite bDnandbDaare different, Figure 2 suggests valuable
information in bDnthat potentially enhances performance on bDa.
AT with Gradient Projection. To connect NT with AT more effectively, we analyze the training
procedures on bDnandbDa. We consider model updates over all samples from bDnandbDa, with
the initial model f(r)at epoch r, and models f(r)
nandf(r)
aafter1epoch of natural and adversarial
training from the same starting point f(r), respectively. Here, we compare the natural updates
bgn=f(r)
n−f(r)and adversarial updates bga=f(r)
a−f(r). Due to distribution shift, an angle exists
between them. Our goal is to identify useful components from gnand incorporate them into gafor
increased robustness in bDawhile maintaining accuracy in bDn. Inspired by Jiang et al. [2023], we
layer-wisely compute the cosine similarity between bgnandbga. For a specific layer lofbgl
nandbgl
a, we
preserve a portion of bgl
nbased on their cosine similarity score (Eq.4). Negative scores indicate that
bgl
nis not beneficial for robustness in bDa. Therefore, we filter components with similarity score ≤0.
We define the GP(Gradient Projection) operation in Eq.5 by projecting bgl
atowards bgl
n.
cos(bgl
n,bgl
a) =bgl
n·bgl
a
∥bgln∥∥bgla∥(4)GP(bgl
n,bgl
a) =(
cos(bgl
n,bgl
a)·bgl
n,cos(bgl
n,bgl
a)>0
0, cos(bgl
n,bgl
a)≤0(5)
Therefore, the total projected (useful) model updates gpcoming from bgncould be computed as Eq. 6.
We use Mto denote all layers of the current model update. Note thatS
l∈Mconcatenates all layers’
useful natural model update components. A hyper-parameter βis used to balance the contributions
ofgGPandbga, as shown in Eq. 7. By finding a proper β(0.5 as in Figure 4c), we can obtain better
robustness on bDa, as shown in Figure 2 and Figure 3. In Figure 2, with β= 0.5,AT-GP refers to AT
with GP; for AT-GP-pre , we perform 50epochs of NT before doing AT-GP . We see AT-GP obtains
a better accuracy/robustness tradeoff than AT. We observe a similar trend for AT-GP-pre vs.AT-pre .
Further, in Figure 3, RN-18 l∞-GP achieves good clean accuracy and better robustness than RN-18
l∞against AutoAttack [Croce and Hein, 2020].
gp=[
l∈MGP(bgl
n,bgl
a)(6)f(r+1)=f(r)+β·gp+ (1−β)·bga (7)
Algorithm 1 Fine-tuning via Logit Pairing
1:Input : model f, input samples (x, y)
from distribution bDn, fine-tuning rounds
R, hyper-parameter λ, adversarial regions
Bq, Brwith size ϵqandϵr,APGD attack.
2:forr= 1,2, ..., R do
3: for(x, y)∼training set Ddo
4: x′
q, pq←APGD (Bq(x, ϵq), y)
5: x′
r, pr←APGD (Br(x, ϵr), y)
6: γ←where (argmax p q=y)
7: nc←γ.size ()
8: calculate Lusing Eq. 3 and update f
9: end for
10:end for
11:Output : model f.Algorithm 2 Connect AT with NT via GP
1:Input : model f, input images with distri-
butionbDn, training rounds R, adversarial
region Bpand its size ϵp,β, natural train-
ingNTand adversarial training AT.
2:forr= 1,2, ..., R do
3: fn←NT(f(r),D)
4: fa←AT(f(r), Bp, ϵp,D)
5: compute bgn←fn−f(r),bga←fa−
f(r)
6: compute gpusing Eq. 6
7: update f(r+1)using Eq. 7 with βand
bga
8:end for
9:Output : model f.
64.3 Theoretical Analysis of GP for Adversarial Robustness
We define Dn={(xi, yi)}∞
i=0as the ideal data distribution with an infinite cardinality. Here, we con-
sider a classifier fθat epoch t. We define Daas the distribution created by {(xi+ϵ(fθ, xi, yi), yi)}∞
i=0
where (xi, yi)∼ D n.xi+ϵ(fθ, xi, yi)denotes the perturbed image, which could be both single and
multiple perturbations based on fθitself.
Assumption 4.1. We assume bDnconsists of Ni.i.d. samples from the ideal distribution Dnand
bDa={(xi+ϵ(fθ, xi, yi), yi)}N
i=0where (xi, yi)∼bDnconsists of Ni.i.d. samples from Da.
We define the population loss as LD(θ) :=E(x,y)∼DL(f(x), y), and let gD(θ) :=∇LD(θ).For
simplification, we use ga:=∇LDa(θ),bga:=∇LbDa(θ), andbgn:=∇LbDn(θ).gGP=β·gp+ (1−
β)·bga(Definition A.3) is the aggregation using GP. We define the following optimization problem.
Definition 4.2 (Aggregation for NT and AT) .fθis trained by iteratively updating the parameter
θ←θ−µ·Aggr(bga,bgn),
where µis the step size. We seek an aggregation rule Aggr(·) =bgAggrsuch that after training, fθ
minimizes the population loss function LDa(θ).
We need bgAggrto be close to gafor each iteration, since gais the optimal update on Da. Thus, we
define Lπ-Norm and delta error to indicate the performance of different aggregation rules.
Definition 4.3 (Lπ-Norm [Enyi Jiang, 2024]) .Given a distribution πon the parameter space θ, we
define an inner product ⟨gD, gD′⟩π=Eθ∼π[⟨gD(θ), gD′(θ)⟩]. The inner product induces the Lπ-
norm on gDas∥gD∥π:=p
Eθ∼π∥gD(θ)∥2. We use Lπ-norm to measure the gradient differences
under certain D.
Definition 4.4 (Delta Error of an aggregation rule Aggr(·)).We define the following squared error
term to measure the closeness between bgAggrandgaunderbDt
a(distribution at time step t), i.e.,
∆2
Aggr:=EbDta∥ga−bgAggr∥2
π.
Delta errors ∆2
ATand∆2
GPmeasure the closesness of gGP,bgafrom gainbDaat each iteration.
Theorem 4.5 (Error Analysis of GP) .When the model dimension m→ ∞ , for an epoch t, we have
an approximation of the error difference ∆2
AT−∆2
GPas follows
∆2
AT−∆2
GP≈β(2−β)EbDta∥ga−bga∥2
π−β2¯τ2∥ga−bgn∥2
π
¯τ2=Eπ[τ2]∈[0,1], where τ(θ)is the sin(·)value of the angle between bgnandga−bgn.
Theorem 4.5 shows ∆2
GPis generally smaller than ∆2
ATfor a large model dimension during each
iteration, as is the case for the models in our evaluation, with β= 0.5, since β(1−β)> β2(0.75>
0.25)and the small value of ¯τin practice (see Interpretation of Theorem A.2 in Appendix A, where we
show the order of difference is between 1e−8and1e−12). Thus, GP achieves better robust accuracy
than AT by achieving a smaller delta error; GP also obtains good clean accuracy by combining parts
of the model updates from the clean distribution bDn. Further, we provide an error analysis of a single
gradient step in Theorem A.1 and convergence analysis in Theorem A.2, showing that a smaller Delta
error results in better convergence. The full proof of all theorems is in Appendix A.
We outline the AT-GP method in Algorithm 2 and it can be extended to the multiple-norm scenario.
The overhead of this algorithm comes from natural training and GP operation. Their costs are small,
and we discuss this more in Section 5.2. Combining logit pairing and gradient projection methods,
we provide the RAMP framework which is similar to Algorithm 2, except that we replace line 4 of
Algorithm 2 as Algorithm 1 line 3-9.
5 Experiment
Datasets, baselines, and models. CIFAR-10 [Krizhevsky et al., 2009] includes 60K images with
50K and 10K images for training and testing respectively. ImageNet has ≈14.2M images and 1K
classes, containing ≈1.3M training, 50K validation, and 100K test images [Russakovsky et al.,
70 20 40 60 80 100 120
Epochs0.7000.7250.7500.7750.8000.8250.8500.8750.900Natural AccAT
AT-GP
AT-GP-pre
AT-pre(a) Clean Accuracy
0 20 40 60 80 100 120
Epochs0.200.250.300.350.400.450.50Robust AccAT
AT-GP
AT-GP-pre
AT-pre (b) Robust Accuracy: PGD-20
Figure 2: l∞AT-GP with PGD [Madry et al., 2017] with ϵ=
0.031on CIFAR-10 improves accuracy and robustness. Pre-
training on bDnfor50epochs further boosts the performance.Clean l∞
RN-18 l∞ 84.2 47 .4
RN-18 l∞-GP 84.548.3
RN-18 l∞-GP-pre 84.9 48 .3
Figure 3: l∞ AT-GP with
APGD [Croce and Hein, 2020]
improves robustness against l∞
AutoAttack [Croce and Hein, 2020]
with ϵ=8
255. RN-18 l∞-GP uses
AT-GP ; RN-18 l∞-GP-pre pre-trains
40epochs on bDnbefore AT-GP is
applied.
2015]. We compare RAMP with following baselines: 1. SAT [Madaan et al., 2021]: randomly
sample one of the l1,l2,l∞attacks. 2. A VG [Tramer and Boneh, 2019]: take the average of l1, l2, l∞
examples. 3. MAX [Tramer and Boneh, 2019]: take the worst of l1, l2, l∞attacks. 4. MSD [Maini
et al., 2020]: find the worst-case examples over l1, l2, l∞steepest descent directions during each step
of inner maximization. 5. E-AT [Croce and Hein, 2022]: randomly sample between l1,l∞attacks.
For models, we use PreAct-ResNet-18, ResNet-50, WideResNet-34-20, and WideResNet-70-16 for
CIFAR-10, as well as ResNet-50 and XCiT-S transformer for ImageNet.
Implementations and Evaluation. For AT from scratch for CIFAR-10, we train PreAct ResNet-
18 [He et al., 2016] with a lr= 0.05for70epochs and 0.005for10more epochs. We set λ= 2,
β= 0.5for training from scratch, and λ= 0.5for robust fine-tuning. For all methods, we use 10
steps for the inner maximization in AT. For ImageNet, we perform 1epoch of fine-tuning and use
a learning rate lr= 0.005,λ= 0.5for ResNet-50 and lr= 1e−4,λ= 0.5for XCiT-S models.
We reduce the rate by a factor of 10every1
3of the training epoch and set the weight decay to
1e−4. We use APGD with 5steps for l∞andl2,15steps for l1. Settings are similar to [Croce
and Hein, 2022]. We use the standard values of ϵ1= 12, ϵ2= 0.5, ϵ∞=8
255for CIFAR-10 and
ϵ1= 255 , ϵ2= 2, ϵ∞=4
255for ImageNet. We focus on l∞-AT models for fine-tuning, as Croce and
Hein [2022] shows their higher union accuracy for the ϵvalues in our evaluation. We report the clean
accuracy, robust accuracy against {l1, l2, l∞}attacks, union accuracy, universal robustness against
common corruptions and unseen adversaries, as well as runtime for RAMP . The robust accuracy is
evaluated using Autoattack [Croce and Hein, 2020]. More implementation details are in Appendix B.
5.1 Main Results
Table 1: Different epsilon values :RAMP consistently outperforms E-AT and MAX for both training
from scratch and robust fine-tuning when the key tradeoff pair changes.
(12,0.5,2
255) (12 ,1.5,8
255)
Clean l∞ l2 l1 Union Clean l∞ l2 l1 Union
Training from ScratchE-AT 87.2 73.3 64.1 55.4 55.4 83.5 41.0 25.5 52.9 25.5
MAX 85.6 72.1 63.6 56.4 56.4 74.6 42.9 35.7 50.3 35.6
RAMP 86.3 73.3 64.9 59.1 59.1 74.4 43.4 37.2 51.1 37.1
Robust Fine-tuningE-AT 86.5 74.8 66.7 57.9 57.9 80.2 42.8 31.5 52.4 31.5
MAX 85.7 74.0 66.2 60.0 60.0 74.8 43.8 36.7 50.2 36.6
RAMP 85.8 74.0 66.2 60.1 60.1 74.9 43.7 37.0 50.2 36.9
Robust fine-tuning. In Table 2, we apply RAMP to larger models and datasets (ImageNet). However,
the implementation of other baselines is not publicly available and Croce and Hein [2022] do not
report other baseline results except E-AT on larger models and datasets, so we only compare against
E-AT in Table 2, which shows RAMP consistently obtains better union accuracy and accuracy-
robustness tradeoff than E-AT. We observe that RAMP improves the performance more as the
model becomes larger. We obtain the SOTA union accuracy of 53.3%on CIFAR-10 and 29.1%on
ImageNet.
RAMP with varying ϵ1, ϵ2, ϵ∞values. We provide results with 1. (ϵ1= 12, ϵ2= 0.5, ϵ∞=2
255)
where ϵ∞size is small and 2. (ϵ1= 12 , ϵ2= 1.5, ϵ∞=8
255)where ϵ2size is large, using
PreAct ResNet-18 model for CIFAR-10 dataset: these cases have different tradeoff pair compared to
8Table 2: Robust fine-tuning on larger models and datasets (* uses extra data for pre-training). We
evaluate all CIFAR-10 and Imagenet test points. RAMP consistently achieves better union accuracy
with significant margins and good accuracy-robustness tradeoff.
Models Methods Clean l∞ l2 l1 Union
WRN-70-16- l∞(*) [Gowal et al., 2020] E-AT 89.6 54.4 76.7 58.0 51.6
RAMP 90.6 54.7 74.6 57.9 53.3
WRN-34-20- l∞[Gowal et al., 2020] E-AT 87.8 49.0 71.6 49.8 45.1
RAMP 87.1 49.7 70.8 50.4 46.9
WRN-28-10- l∞(*) [Carmon et al., 2019] E-AT 89.3 51.8 74.6 53.3 47.9
CIFAR-10 RAMP 89.2 55.9 74.7 55.7 52.7
WRN-28-10- l∞(*) [Gowal et al., 2020] E-AT 89.8 54.4 76.1 56.0 50.5
RAMP 89.4 55.9 74.7 56.0 52.9
RN-50- l∞[Engstrom et al., 2019] E-AT 85.3 46.5 68.3 45.3 41.6
RAMP 84.3 47.0 67.7 46.5 43.3
XCiT-S- l∞[Debenedetti and Troncoso—EPFL, 2022] E-AT 68.4 38.1 51.8 23.8 23.4
ImageNet RAMP 66.0 35.7 50.2 30.0 29.1
RN-50- l∞[Engstrom et al., 2019] E-AT 58.2 26.9 39.5 18.8 17.8
RAMP 55.6 25.1 38.3 22.4 20.9
Figure 1. The pair identified using our heuristic are l1-l2andl2-l∞. In Table 1, we observe that
RAMP consistently outperforms E-AT and MAX with significant margins in union accuracy, when
training from scratch and performing robust fine-tuning. In Table 1, when l2is the bottleneck, E-AT
obtains a lower union accuracy as it does not leverage l2examples. Similar observations are made
across various epsilon values, with RAMP consistently outperforming other baselines, as detailed in
Appendix B.4. Appendix B includes more training details/results, and ablation studies. Results for
applying the trades loss to RAMP outperforming E-AT are detailed in Appendix B.6. Appendix B.7
presents robust fine-tuning using ResNet-18, where RAMP achieves the highest union accuracy.
Table 3: RN-18 model trained from random initialization
on CIFAR-10 over 5 trials: RAMP achieves the best union
robustness and good clean accuracy compared with other
baselines. Baseline results are from Croce and Hein [2022].
Methods Clean l∞ l2 l1 Union
SAT 83.9 ±0.8 40.7 ±0.7 68.0 ±0.4 54.0 ±1.2 40.4 ±0.7
A VG 84.6 ±0.3 40.8 ±0.7 68.4 ±0.7 52.1 ±0.4 40.1 ±0.8
MAX 80.4 ±0.5 45.7 ±0.9 66.0 ±0.4 48.6 ±0.8 44.0 ±0.7
MSD 81.1 ±1.1 44.9 ±0.6 65.9 ±0.6 49.5 ±1.2 43.9 ±0.8
E-AT 82.2 ±1.8 42.7 ±0.7 67.5 ±0.5 53.6 ±0.1 42.4 ±0.6
RAMP (λ=5) 81.2 ±0.3 46.0 ±0.5 65.8 ±0.2 48.3 ±0.6 44.6±0.6
RAMP (λ=2) 82.1 ±0.3 45.5 ±0.3 66.6 ±0.3 48.4 ±0.2 44.0 ±0.2Adversarial training from random
initialization. Table 3 presents the re-
sults of AT from random initialization
on CIFAR-10 with PreAct ResNet-18.
RAMP has the highest union accu-
racy with good clean accuracy, which
indicates that RAMP can mitigate the
tradeoffs among perturbations and ro-
bustness/accuracy in this setting. The
results for all baselines are from Croce
and Hein [2022].
Table 4: Individual, average, and union accuracy against common corruptions (averaged across five
levels) and unseen adversaries using WideResNet-28-10 on CIFAR-10 dataset.
Models Common Corruptions l0 fog snow gabor elastic jpeginf Avg Union
l1-AT 78.2 79.0 41.4 22.9 40.5 48.9 48.4 46.9 12.8
l2-AT 77.2 67.5 48.7 26.1 44.1 53.2 45.4 47.5 16.2
l∞-AT 73.4 55.5 44.7 32.9 53.8 56.6 33.4 46.2 19.1
Winninghand [Diffenderfer et al., 2021] 91.1 74.1 74.5 18.3 76.5 12.6 0.0 42.7 0.0
E-AT 71.5 58.5 35.9 35.3 50.7 55.7 60.3 49.4 21.9
MAX 71.0 56.2 42.9 35.4 49.8 57.8 55.7 49.6 24.4
RAMP 75.5 55.5 40.5 40.2 52.9 60.3 56.1 50.9 26.1
Universal Robustness. In Table 4, we report average accuracy against common corruptions and
union accuracy against unseen adversaries from Laidlaw et al. [2020] (implementation details are in
Appendix B.3). We compare against lppretrained models, E-AT, MAX, winninghand [Diffenderfer
et al., 2021] (a SOTA method for natural corruptions) using WideResNet-28-10 architecture on
the CIFAR-10 dataset. Compared to E-AT and MAX, RAMP achieves 4%higher accuracy for
common corruptions with five severity levels and 2-4%better union accuracy against multiple unseen
adversaries. Winninghand has high corruption robustness but no adversarial robustness. The results
show that RAMP obtains a better robustness and accuracy tradeoff with stronger universal robustness.
In Appendix B.3, we evaluate on ResNet-18 to support this fact further.
5.2 Ablation Study and Discussion
Sensitivities of λ.We perform experiments with different λvalues in [0.1,0.5,1.0,1.5,2,3,4,5]
for robust fine-tuning and [1.5,2,3,4,5,6]for AT from scratch using PreAct-ResNet-18 model for
CIFAR-10 dataset. In Figure 4, we observe a decreased clean accuracy when λbecomes larger. We
9λAcc
354555657585
0 1 2 3 4 5Clean Linf L2 L1 Union(a)λ: Robust fine-tuning.
λAcc
40455055606570758085
2 3 4 5 6Clean Linf L2 L1 Union (b)λ: Train from scratch.
βAcc
40455055606570758085
0.2 0.4 0.6 0.8Clean Linf L2 L1 Union (c)β: Train from scratch.
Figure 4: Alabtion studies on λandβhyper-parameters.
pickλ= 2.0for training from scratch (Figure 4a) and λ= 0.5for robust fine-tuning (Figure 4b) in
our main experiments, as these values of λyield both good clean and union accuracy.
Choices of β.Figure 4c shows the performance of RAMP with varying βvalues on CIFAR-10
ResNet-18 experiments. We pick β= 0.5for combining natural training and AT via GP, which
achieves comparatively good robustness and clean accuracy. This choice is also based on Theorem 4.5
when β(2−β)has the largest difference from β2(0.75 vs 0.25).
Clean l∞ l2 l1 Union
RN-18 l∞-AT 81.5 45.5 66.4 47.0 42.9
RN-18 l1-AT 81.0 42.6 66.0 48.1 41.5
RN-18 l2-AT 84.1 41.6 69.1 45.4 39.4
Table 5: RAMP with l∞, l1, l2-RN-18-AT
models on CIFAR-10 with standard epsilons.Fine-tune lpAT models with RAMP. Table 5 shows
the robust fine-tuning results using RAMP withl∞-
AT (q=∞, r= 1),l1-AT ( q= 1, r=∞),l2-AT
(q=∞, r= 1) RN-18 models for CIFAR-10 dataset.
Forl∞−l1tradeoffs, RAMP on l∞-AT pre-trained
model achieves the best union accuracy.
Computational analysis and Limitations. The extra
training costs of AT-GP are small, e.g. for each epoch
on ResNet-18, the extra NT takes 6seconds and the
standard AT takes 78seconds using a single NVIDIA A100 GPU, and the GPoperation only takes
0.04seconds on average. RAMP is more expensive than E-AT and less expensive than MAX. We
have a complete runtime analysis in Appendix B.2. We notice occasional drops in clean accuracy
during fine-tuning with RAMP . In some cases, union accuracy improves slightly but clean accuracy
and single lprobustness reduce. Further, we find no negative societal impact from this work.
6 Conclusion
We introduce RAMP , a framework enhancing multiple-norm robustness and achieving superior
universal robustness against corruptions and perturbations by addressing tradeoffs among lpperturba-
tions and accuracy/robustness. We apply a new logit pairing loss and use gradient projection to obtain
SOTA union accuracy with favorable accuracy/robustness tradeoffs against common corruptions
and other unseen adversaries. Results demonstrate that RAMP surpasses SOTA methods in union
accuracy across model architectures on CIFAR-10 and ImageNet.
Acknowledgments
This work was supported in part by NSF Grants No. CCF-2238079, CCF-2316233, CNS-2148583.
We would like to thank Jacky Yibo Zhang for the helpful discussions and advice on the proof. Also,
we thank anonymous reviewers for their valuable feedback on the paper.
References
Kumail Alhamoud, Hasan Abed Al Kader Hammoud, Motasem Alfarra, and Bernard Ghanem.
Generalizability of adversarial robustness under distribution shifts. Transactions on Machine
Learning Research , 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=
XNFo3dQiCJ . Featured Certification.
10Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In International conference on machine learning , pages 284–293. PMLR, 2018.
Debangshu Banerjee and Gagandeep Singh. Relational dnn verification with cross executional bound
refinement. arXiv preprint arXiv:2405.10143 , 2024.
Debangshu Banerjee, Changming Xu, and Gagandeep Singh. Input-relational verification of deep
neural networks. Proceedings of the ACM on Programming Languages , 8(PLDI):1–27, 2024.
Philipp Benz, Chaoning Zhang, and In So Kweon. Batch normalization increases adversarial
vulnerability and decreases adversarial transferability: A non-robust feature perspective. In
Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 7818–7827,
2021.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. Advances in neural information processing systems , 32,
2019.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of
diverse parameter-free attacks. In International conference on machine learning , pages 2206–2216.
PMLR, 2020.
Francesco Croce and Matthias Hein. Adversarial robustness against multiple and single l_p-threat
models via quick fine-tuning of robust classifiers. In International Conference on Machine
Learning , pages 4436–4454. PMLR, 2022.
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flam-
marion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial
robustness benchmark. arXiv preprint arXiv:2010.09670 , 2020.
Francesco Croce, Maksym Andriushchenko, Naman D Singh, Nicolas Flammarion, and Matthias
Hein. Sparse-rs: a versatile framework for query-efficient sparse black-box adversarial attacks. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pages 6437–6445, 2022.
Edoardo Debenedetti and Carmela Troncoso—EPFL. Adversarially robust vision transformers, 2022.
James Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya Kailkhura. A
winning hand: Compressing deep networks can improve out-of-distribution robustness. Advances
in neural information processing systems , 34:664–676, 2021.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A
rotation and a translation suffice: Fooling cnns with simple transformations. 2017.
Logan Engstrom, Andrew Ilyas, and Anish Athalye. Evaluating and understanding the robustness of
adversarial logit pairing. arXiv preprint arXiv:1807.10272 , 2018.
Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness
(python library), 2019. URL https://github.com/MadryLab/robustness .
Sanmi Koyejo Enyi Jiang, Yibo Jacky Zhang. Principled federated domain adaptation: Gradient pro-
jection and auto-weighting. In The Twelfth International Conference on Learning Representations ,
2024. URL https://openreview.net/forum?id=6J3ehSUrMU .
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual
classification. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 1625–1634, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572 , 2014.
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering
the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint
arXiv:2010.03593 , 2020.
11Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. arXiv preprint arXiv:1903.12261 , 2019.
Enyi Jiang. Federated domain adaptation for healthcare, 2023.
Enyi Jiang, Yibo Jacky Zhang, and Oluwasanmi Koyejo. Federated domain adaptation via gradient
projection. arXiv preprint arXiv:2302.05049 , 2023.
Daniel Kang, Yi Sun, Tom Brown, Dan Hendrycks, and Jacob Steinhardt. Transfer of adversarial
robustness between perturbation types. arXiv preprint arXiv:1905.01034 , 2019.
Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373 , 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
InArtificial intelligence safety and security , pages 99–112. Chapman and Hall/CRC, 2018.
Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against
unseen threat models. arXiv preprint arXiv:2006.12655 , 2020.
Aishan Liu, Shiyu Tang, Xianglong Liu, Xinyun Chen, Lei Huang, Zhuozhuo Tu, Dawn Song, and
Dacheng Tao. Towards defending multiple adversarial perturbations via gated batch normalization.
arXiv preprint arXiv:2012.01654 , 2020.
Divyam Madaan, Jinwoo Shin, and Sung Ju Hwang. Learning to generate noise for multi-attack
robustness. In International Conference on Machine Learning , pages 7279–7289. PMLR, 2021.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 ,
2017.
Pratyush Maini, Eric Wong, and Zico Kolter. Adversarial robustness against the union of multiple
perturbation models. In International Conference on Machine Learning , pages 6640–6650. PMLR,
2020.
Pratyush Maini, Xinyun Chen, Bo Li, and Dawn Song. Perturbation type categorization for multiple
adversarial perturbation robustness. In Uncertainty in Artificial Intelligence , pages 1317–1327.
PMLR, 2022.
Mohammad Mehrabi, Adel Javanmard, Ryan A Rossi, Anup Rao, and Tung Mai. Fundamental
tradeoffs in distributionally adversarial training. In International Conference on Machine Learning ,
pages 7544–7554. PMLR, 2021.
Mazda Moayeri, Kiarash Banihashem, and Soheil Feizi. Explicit tradeoffs between adversarial
and natural distributional robustness. Advances in Neural Information Processing Systems , 35:
38761–38774, 2022.
Jay Nandy, Wynne Hsu, and Mong Li Lee. Approximate manifold defense against multiple adversarial
perturbations. In 2020 International Joint Conference on Neural Networks (IJCNN) , pages 1–8.
IEEE, 2020.
ShengYun Peng, Weilin Xu, Cory Cornelius, Matthew Hull, Kevin Li, Rahul Duggal, Mansi Phute,
Jason Martin, and Duen Horng Chau. Robust principles: Architectural design principles for
adversarially robust cnns. arXiv preprint arXiv:2308.16258 , 2023.
Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Reducing excessive margin to achieve a better
accuracy vs. robustness trade-off. In International Conference on Learning Representations , 2021.
12Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding
and mitigating the tradeoff between robustness and accuracy. arXiv preprint arXiv:2002.10716 ,
2020.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International journal of computer vision , 115:211–252, 2015.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially
robust neural network model on mnist. arXiv preprint arXiv:1805.09190 , 2018.
Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real
and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 acm sigsac
conference on computer and communications security , pages 1528–1540, 2016.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with
principled adversarial training. In International Conference on Learning Representations , 2018.
URL https://openreview.net/forum?id=Hk6kPgZA- .
Dawn Song, Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tramer,
Atul Prakash, and Tadayoshi Kohno. Physical adversarial examples for object detectors. In 12th
USENIX workshop on offensive technologies (WOOT 18) , 2018.
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations.
Advances in neural information processing systems , 32, 2019.
Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204 ,
2017.
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassified examples. In ICLR , 2020.
Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion
models further improve adversarial training. In Andreas Krause, Emma Brunskill, Kyunghyun
Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th
International Conference on Machine Learning , volume 202 of Proceedings of Machine Learning
Research , pages 36246–36263. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.
press/v202/wang23ad.html .
Eric Wong and J Zico Kolter. Learning perturbation sets for robust machine learning. arXiv preprint
arXiv:2007.08450 , 2020.
Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust general-
ization. Advances in Neural Information Processing Systems , 33:2958–2969, 2020.
Jiancong Xiao, Zeyu Qin, Yanbo Fan, Baoyuan Wu, Jue Wang, and Zhi-Quan Luo. Adaptive
smoothness-weighted adversarial training for multiple perturbations with its stability analysis.
arXiv preprint arXiv:2210.00557 , 2022.
Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. Adversarial
examples improve image recognition. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 819–828, 2020.
Kaidi Xu, Chenan Wang, Hao Cheng, Bhavya Kailkhura, Xue Lin, and Ryan Goldhahn. Mixture of
robust experts (more): A robust denoising method towards multiple perturbations. arXiv preprint
arXiv:2104.10586 , 2021.
Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaud-
huri. A closer look at accuracy vs. robustness. Advances in neural information processing systems ,
33:8588–8601, 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Procedings of the British
Machine Vision Conference 2016 . British Machine Vision Association, 2016.
13Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International conference on
machine learning , pages 7472–7482. PMLR, 2019.
Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli.
Geometry-aware instance-reweighted adversarial training. In International Conference on Learning
Representations , 2021. URL https://openreview.net/forum?id=iAX0l6Cz8ub .
14A Proof of Theorems
A.1 Proof of Theorem A.2
We first show what happens during one step of optimization, where we highlight the importance of
analyzing delta error.
Theorem A.1. Consider model parameter θ∼πand an aggregation rule Aggr(·)with step size
µ >0. Define the updated parameter as
θ+:=θ−µbgAggr(θ).
Assuming the gradient ∇L(θ)isγ-Lipschitz in θfor any input, and let the step size µ≤1
γ,we have
EbDa,θ[LDa(θ+)− LDa(θ)]≤ −µ
2(∥ga∥2
π−∆2
Aggr).
Proof. The proof is the same as Theorem A.1 in [Enyi Jiang, 2024].
Theorem A.2 (Convergence of Aggr(·)).For any probability measure πover the parameter space,
and an aggregation rule Aggr(·)with step size µ > 0. We update the parameter for Tsteps
byθt+1:=θt−µbgAggr(θt).Assume the gradient ∇L(θ)andbgAggr(θ)areγ
2-Lipschitz in θsuch
thatθt→bθAggr.∆Aggr_max is the Delta error at time t′when∥bgAggr(bθAggr)− ∇LDt′
a(bθAggr)∥2is
maximized. Then, given step size µ≤1
γand a small enough ϵ >0, with probability at least 1−δwe
have
∥∇LDTa(θT)∥2≤1
δ2q
Cϵ·∆2
Aggr_max +O(ϵ)2
+O1
T
+O(ϵ),
where Cϵ=EbDt′
a[1/π(Bϵ(bθAggr))]2andBϵ(bθAggr)⊂Rmis the ball with radius ϵcentered at bθAggr.
TheCϵmeasures how well πcovers where the optimization goes.
Proof. Denote random function bf:Rm→R+as
bf(θ) =∥bgAggr(θ)− ∇L Da(θ)∥, (8)
where the randomness comes from bDa. Note that bfisγ-Lipschitz by assumption. Now we consider
Bϵ(bθAggr)⊂Rm, i.e., the ball with radius ϵcentered at bθAggr. Then, by γ-Lipschitzness we have
Eθ∼πbf(θ) =Z
bf(θ) dπ(θ)
≥Z
Bϵ(bθAggr)(bf(bθAggr)−γϵ) dπ(θ)
= (bf(bθAggr)−γϵ)π(Bϵ(bθAggr))
Therefore,
bf(bθAggr)≤1
π(Bϵ(bθAggr))·Eθ∼πbf(θ) +O(ϵ).
Taking expectation w.r.t. bDaon both sides, we have
EbDabf(bθAggr)≤EbDa"
1
π(Bϵ(bθAggr))·Eθ∼πbf(θ)#
+O(ϵ)
≤vuutEbDa"
1
π(Bϵ(bθAggr))#2
·EbDah
Eθ∼πbf(θ)i2
+O(ϵ) (Cauchy-Schwarz)
=r
Cϵ·EbDah
Eθ∼πbf(θ)i2
+O(ϵ) (by definition of Cϵ)
≤r
Cϵ·EbDaEθ∼πh
bf(θ)i2
+O(ϵ) (Jensen’s inequality)
=q
Cϵ·∆2
Aggr+O(ϵ)
15By Markov’s inequality, with probability at least 1−δwe have a sampled dataset bDasuch that
bf(bθAggr)≤1
δEbDabf(bθAggr)≤1
δq
Cϵ·∆2
Aggr+O(ϵ/δ) (9)
Conditioned on such event, we proceed on to the optimization part.
Note that Theorem A.1 characterizes how the optimization works for one gradient update. We denote
Dt
aas the data distribution Daat time step t. Therefore, for any time step t= 0, . . . , T −1, we can
apply Theorem A.1 which only requires the Lipschitz assumption:
LDta(θt+1)− LDta(θt)≤ −µ
2 
∥∇LDta(θt)∥2− ∥bgAggr(θt)− ∇L Dta(θt)∥2
.
We notice that Dt
achanges based on θs of different time steps. On both sides, to sum over t=
0, . . . , T −1, we first consider two terms:
(LDta(θt+1)− LDta(θt)) + (LDt−1
a(θt)− LDt−1
a(θt−1))
To compare LDta(θt)andLDt−1
a(θt), since Dt
aoptimizes one more step than Dt−1
a, we assume
LDta(θt)≤ LDt−1
a(θt)for∀t. Therefore, we have:
(LDta(θt+1)− LDta(θt)) + (LDt−1
a(θt)− LDt−1
a(θt−1))≥(LDta(θt+1)− LDt−1
a(θt)) + (LDt−1
a(θt)− LDt−1
a(θt−1))
Summing up all time steps,
LDta(θt+1)− LD0a(θ0)≤ LDta(θt+1)− LDt−1
a(θt) +LDt−1
a(θt)− LDt−2
a(θt−1) +...− LD0a(θ0)
≤ −µ
2 T−1X
t=0∥∇LDt−1
a(θt)∥2−T−1X
t=0∥bgAggr(θt)− ∇L Dta(θt)∥2!
.
Dividing both sides by T, and with regular algebraic manipulation we derive
1
TT−1X
t=0∥∇LDt−1
a(θt)∥2≤2
µT(LD0a(θ0)− LDT−1
a(θT)) +1
TT−1X
t=0∥bgAggr(θt)− ∇L Dta(θt)∥2.
Note that we assume the loss function LD(θ) :=E(x,y)∼DL(f(x), y), is non-negative. Thus, we
have
1
TT−1X
t=0∥∇LDt−1
a(θt)∥2≤2LD0a(θ0)
µT+1
TT−1X
t=0∥bgAggr(θt)− ∇L Dta(θt)∥2. (10)
Note that we assume given bDawe have θt→bθAggr. Therefore, for any ϵ >0there exist Tϵsuch that
∀t > T ϵ:∥θt−bθAggr∥< ϵ. (11)
This implies that ∀t > T ϵ:
µ∥bgAggr(θt)∥=∥θt+1−bθAggr+bθAggr−θt∥ ≤ ∥ θt+1−bθAggr∥+∥bθAggr−θt∥<2ϵ. (12)
Moreover, (11) also implies ∀t1, t2> Tϵ:
∥∇LDt1a(θt1)− ∇LDt2a(θt2)∥ ≤γ∥θt1−θt2∥ (γ-Lipschitzness)
<2ϵ. (13)
16Now, let’s get back to (10). For ∀T > T ϵwe have
1
TT−1X
t=0∥∇LDt−1
a(θt)∥2≤2LD0a(θ0)
µT+1
TTϵ−1X
t=0∥bgAggr(θt)− ∇L Dta(θt)∥2+1
TT−1X
t=Tϵ∥bgAggr(θt)− ∇L Dta(θt)∥2
=O1
T
+1
TT−1X
t=Tϵ∥bgAggr(θt)− ∇L Dta(θt)∥2
=O1
T
+1
TT−1X
t=Tϵ∥bgAggr(θt)−bgAggr(bθAggr) +bgAggr(bθAggr)− ∇L Dta(θt)∥2
≤ O1
T
+1
TT−1X
t=Tϵ
∥bgAggr(θt)−bgAggr(bθAggr)∥+∥bgAggr(bθAggr)− ∇L Dta(θt)∥2
(triangle inequality)
=O1
T
+1
TT−1X
t=Tϵ
O(ϵ) +∥bgAggr(bθAggr)− ∇L Dta(θt)∥2
(by (12))
=O1
T
+O(ϵ) +1
TT−1X
t=Tϵ
∥bgAggr(bθAggr)− ∇L Dta(bθAggr) +∇LDta(bθAggr)− ∇L Dta(θt)∥2
≤ O1
T
+O(ϵ) +1
TT−1X
t=Tϵ
∥bgAggr(bθAggr)− ∇L Dta(bθAggr)∥+O(ϵ)2
(by (13))
≤ O1
T
+O(ϵ) +∥bgAggr(bθAggr)− ∇LDt′
a(bθAggr)∥2(14)
Equation 14 bounds the left hand side with the maximum ∥bgAggr(bθAggr)− ∇L Dta(bθAggr)∥2one can
get during the optimization steps. Here, we assume at time t′, the largest value is attained. We denote
∆2
Aggr_max as the delta error at time step t′.
Then, we can continue with what we have done at the beginning of the proof of this theorem:
(14)=O1
T
+O(ϵ) +f(bθAggr)2(by (8))
≤ O1
T
+O(ϵ) +1
δq
Cϵ·∆2
Aggr_max +O(ϵ/δ)2
(by (9))
Therefore, combining the above we finally have: for ∀T > T ϵwith probability at least 1−δ,
1
TT−1X
t=0∥∇LDt−1
a(θt)∥2≤ O1
T
+O(ϵ) +1
δ2q
Cϵ·∆2
Aggr_max +O(ϵ)2
(15)
17To complete the proof, let us investigate the left-hand side.
1
TT−1X
t=0∥∇LDt−1
a(θt)∥2=1
TTϵ−1X
t=0∥∇LDta(θt)∥2+1
TT−1X
t=Tϵ∥∇LDta(θt)∥2
=O1
T
+1
TT−1X
t=Tϵ∥∇LDta(θt)∥2
≥ O1
T
+1
TT−1X
t=Tϵ 
∥∇LDta(θt)− ∇L DTa(θT)∥ − ∥∇L DTa(θT)∥2
(triangle inequality)
=O1
T
+1
TT−1X
t=Tϵ 
O(ϵ) +∥∇LDTa(θT)∥2
(by (13))
=O1
T
+O(ϵ) +∥∇LDTa(θT)∥2. (16)
Combining (15) and (16), we finally have
∥∇LDTa(θT)∥2≤ O1
T
+O(ϵ) +1
δ2q
Cϵ·∆2
Aggr_max +O(ϵ)2
,
which completes the proof.
A.2 Proof of Theorem 4.5
To prove Theorem 4.5, we first use the following definitions and lemmas from [Enyi Jiang, 2024], to
get the delta errors of Gradient Projection (GP) and standard adversarial training (AT):
Definition A.3 (GP Aggregation) .Letβ∈[0,1]be the weight that balances between bgaandbgn. The
GP aggregation operation is
GP(bga,bgn) = 
(1−β)bga+βProj+(bga|bgn)
.
where Proj+(bga|bgn) = max {⟨bga,bgn⟩,0}bgn/∥bgn∥2is the operation that projects bgato the positive
direction of bgn.
Definition A.4 (AT Aggregation) .The AT aggregation operation is
AT(bga) =bga.
standard AT only leverages the gradient update on bDa.
Lemma A.5 (Delta Error of GP) .Given distributions bDa,DaandbDn, as well as the model updates
bga, ga,bgnon these distributions per epoch, we have ∆2
GPas follows
∆2
GP≈
(1−β)2+2β−β2
m
EbDa∥ga−bga∥2
π+β2¯τ2∥ga−bgn∥2
π,
In the above equation, mis the model dimension and ¯τ2=Eπ[τ2]∈[0,1]where τ(θ)is the sin(·)
value of the angle between bgnandga−bgn.∥ · ∥πis the π-norm over the model parameter space.
Proof. The proof is the same as Theorem 4.4 in Enyi Jiang [2024].
Lemma A.6 (Delta Error of AT) .Given distributions bDa,DaandbDn, as well as the model updates
bga, ga,bgnon these distributions per epoch, we have ∆2
ATas follows
∆2
AT=EbDa∥ga−bga∥2
π,
where ∥ · ∥πis the π-norm over the model parameter space.
18Then, we prove Theorem 4.5.
Theorem A.7 (Error Analysis of GP) .When the model dimension is large ( m→ ∞ ) at time step t,
we have
∆2
AT−∆2
GP≈β(2−β)EbDta∥ga−bga∥2
π−β2¯τ2∥ga−bgn∥2
π.
¯τ2=Eπ[τ2]∈[0,1]where τis the sin(·)value of the angle between bgnandga−bgn,∥ · ∥πis the
π-norm over the model parameter space.
Proof. ∆2
AT−∆2
GP
≈EbDta∥ga−bga∥2
π−
(1−β)2+2β−β2
m
EbDa∥ga−bga∥2
π−β2¯τ2∥ga−bgn∥2
π
=
1−((1−β)2+2β−β2
m)
EbDta∥ga−bga∥2
π−β2¯τ2∥ga−bgn∥2
π
= (1 +1
m)β(2−β)EbDta∥ga−bga∥2
π−β2¯τ2∥ga−bgn∥2
π
When m→ ∞ , we have a simplified version of the error difference as follows
∆2
AT−∆2
GP≈β(2−β)EbDt
a∥ga−bga∥2
π−β2¯τ2∥ga−bgn∥2
π
Interpretation. When β= 0.5, we can usually show ∆2
AT>∆2
GP, because β(2−β)>
β2¯τ2(0.75>0.25)for the coefficients of two terms. We estimate the actual values of terms
EbDat∥ga−bga∥2
π(variance), ∥ga−bgn∥2
π(bias), and ¯τusing the estimation methods in Enyi Jiang
[2024]. Table 6 displays the values of those terms as well as the error differences on ResNet18 exper-
iments at epoch 5,10,15,20,60. We plot the changing of these terms on the ResNet18 experiment in
Figure 5. The order of difference is always positive and usually smaller than 1e−08and approaches
the order of 1e−12in the end.
Table 6: Estimations the actual values of terms EdDat∥ga−bga∥2
π(variance), ∥ga−bgn∥2
π(bias), ¯τ,
and∆2
AT−∆2
GP(error differences) across different epochs.
Terms / epochs 5 10 15 20 60
EbDat∥ga−bga∥2
π4.6017e-08 2.0448e-09 6.9623e-10 6.4329e-10 2.3849e-11
∥ga−bgn∥2
π 0.0007 9.9098e-05 4.4932e-05 3.7930e-05 2.8391e-06
¯τ 0.0071 0.0052 0.0036 0.0038 0.0030
∆2
AT−∆2
GP 2.5335e-08 8.5709e-10 3.7609e-10 3.4487e-10 1.1574e-11
B Additional Experiment Information
In this section, we provide more training details, additional experiment results on the universal
robustness of RAMP to common corruptions and unseen adversaries, runtime analysis of RAMP,
additional ablation studies on different logit pairing losses, and AT from random initialization results
on CIFAR-10 using WideResNet-28-10.
B.1 More Training Details
We set the batch size to 128for the experiments on ResNet-18 and WideResNet-28-10 architectures.
We use an SGD optimizer with 0.9momentum and 5e−4weight decay. For other experiments on
ImageNet, we use a batch size of 64to fit into the GPU memory for larger models. For all training
procedures, we select the last checkpoint for the comparison. When the pre-trained model was
originally trained with extra data beyond the CIFAR-10 dataset, similar to Croce and Hein [2022],
we use the extra 500k images introduced by Carmon et al. [2019] for fine-tuning, and each batch
contains the same amount of standard and extra images. An epoch is completed when the whole
standard training set has been used.
190 20 40 60 80 100 120
Epoches0.00.20.40.60.81.01e8
 Variances
t
a|ga ga|2
0 20 40 60 80 100 120
Epoches0.000000.000020.000040.000060.000080.00010Bias
|ga gn|2
0 20 40 60 80 100 120
Epoches0.00250.00500.00750.01000.01250.01500.01750.0200
0 20 40 60 80 100 120
Epoches0.00.20.40.60.81.01e10
 Error Differences
2
AT2
GP
Figure 5: Plot of values of terms EbDat∥ga−bga∥2
π(variance), ∥ga−bgn∥2
π(bias), ¯τ, and∆2
AT−∆2
GP
(error differences).
B.2 Runtime Analysis of RAMP
We present runtime analysis results demonstrating the fact that RAMP is more expensive than E-AT
and less expensive than MAX in Table 7. These results, recorded in seconds per epoch, were obtained
using a single A100 40GB GPU. RAMP consistently supports that fact in all experiments.
Table 7: Analysis of time per epoch for RAMP and related baselines. RAMP is more expensive than
E-AT and less expensive than MAX.
Models \Methods E-AT [Croce and Hein, 2022] MAX RAMP
CIFAR-10 RN-18 scratch 78 219 157
CIFAR-10 WRN-28-10 scratch 334 1048 660
CIFAR-10 RN-50 188 510 388
CIFAR-10 WRN-34-20 1094 2986 2264
CIFAR-10 WRN-28-10 carmon 546 1420 1110
CIFAR-10 WRN-28-10 gowal 698 1895 1456
CIFAR-10 WRN-70-16 3486 10330 7258
ImageNet ResNet50 15656 41689 35038
ImageNet Transformer 38003 101646 81279
B.3 Additional Results on RAMP Generalizing to Common Corruptions and Unseen
Adversaries for Universal Robustness
In this section, we show RAMP can generalize better to other corruptions and unseen adversaries on
union accuracy for stronger universal robustness.
Implementations. For the l0attack, we use Croce et al. [2022] with an epsilon of 9pixels and 5k
query points. For common corruptions, we directly use the implementation of RobustBench [Croce
et al., 2020] for evaluation across 5 severity levels on all corruption types used in Hendrycks and
Dietterich [2019]. For other unseen adversaries, we follow the implementation of Laidlaw et al.
[2020], where we set eps= 12 for the fog attack, eps= 0.5for the snow attack, eps= 60 for the
gabor attack, eps= 0.125for the elastic attack, and eps= 0.125for the jpeglinf attack with 100
iterations. For ResNet-18 experiments, we do not compare with Winninghand [Diffenderfer et al.,
202021] since it uses a Wide-ResNet architecture. Also, we select the strongest baselines (E-AT and
MAX) from the Wide-ResNet experiment results to compare for ResNet-18 experiments on universal
robustness.
Results. For the ResNet-18 training from scratch experiment on CIFAR-10, in Table 8 and 9, we
also show RAMP generally outperforms by 0.5%on common corruptions and 7%on union accuracy
against unseen adversaries compared with E-AT.
Table 8: Accuracy against common corruptions using ResNet-18 on CIFAR-10 dataset.
Models common corruptions
E-AT 73.8
MAX 75.1
RAMP 74.3
Table 9: Individual, average, and union accuracy against unseen adversaries using ResNet-18 on
CIFAR-10 dataset.
Models l0 fog snow gabor elastic jpeglinf Avg Union
E-AT 58.5 41.8 30.8 45.9 55.0 59.1 48.5 18.8
MAX 70.8 40.0 34.4 45.1 54.8 56.8 50.3 20.6
RAMP 56.8 40.5 40.5 50.0 59.2 56.2 50.5 25.9
B.4 Additional Experiments with Different Epsilon Values
In this section, we provide additional results with different ϵ1, ϵ2, ϵ∞values. We select ϵ∞=
[2
255,4
255,12
255,16
255],ϵ1= [6,9,12,15], andϵ2= [0.25,0.75,1.0,1.5]. We provide additional RAMP
results compared with related baselines with training from scratch and performing robust fine-tuning
in Section B.4.1 and Section B.4.2, respectively. We observe that RAMP can surpass E-AT with
significant margins as well as a better accuracy-robustness tradeoff for both training from scratch and
robust fine-tuning with λ= 2.0for training from scratch and λ= 0.5for robust fine-tuning in most
cases.
B.4.1 Additional Results with Training from Scratch
Changing l∞perturbations with ϵ∞= [2
255,4
255,12
255,16
255].Table 10 and Table 11 show that
RAMP consistently outperforms E-AT [Croce and Hein, 2022] on union accuracy when training
from scratch.
Table 10: (ϵ∞=2
255, ϵ1= 12 , ϵ2= 0.5)and(ϵ∞=4
255, ϵ1= 12 , ϵ2= 0.5)with random
initializations.
Clean l∞ l2 l1Union
E-AT 87.2 73.3 64.1 55.4 55.4
RAMP 86.3 73.3 64.9 59.1 59.1Clean l∞ l2 l1Union
E-AT 86.8 58.9 66.4 54.6 53.7
RAMP 86.1 60.0 67.4 58.5 57.4
Table 11: (ϵ∞=12
255, ϵ1= 12 , ϵ2= 0.5)and(ϵ∞=16
255, ϵ1= 12 , ϵ2= 0.5)with random
initializations.
Clean l∞ l2 l1Union
E-AT 77.5 28.8 64.0 50.1 28.7
RAMP 73.7 34.6 59.1 38.9 33.3Clean l∞ l2 l1Union
E-AT 69.4 18.8 58.7 47.7 18.7
RAMP 65.0 25.7 49.8 32.6 25.0
Changing l1perturbations with ϵ1= [6,9,12,15].Table 12 and Table 13 show that RAMP
consistently outperforms E-AT [Croce and Hein, 2022] on union accuracy when training from scratch.
Changing l2perturbations with ϵ2= [0.25,0.75,1.0,1.5].Table 14 and Table 15 show that RAMP
consistently outperforms E-AT [Croce and Hein, 2022] on union accuracy when training from scratch.
21Table 12: (ϵ∞=8
255, ϵ1=6, ϵ2= 0.5)and(ϵ∞=8
255, ϵ1=9, ϵ2= 0.5)with random initializa-
tions.
Clean l∞ l2 l1Union
E-AT 85.5 43.1 67.9 63.9 42.8
RAMP 83.8 48.1 63.0 51.2 46.0Clean l∞ l2 l1Union
E-AT 84.6 41.8 67.7 57.6 41.4
RAMP 82.6 47.5 65.7 50.8 45.9
Table 13: (ϵ∞=8
255, ϵ1=15, ϵ2= 0.5)and(ϵ∞=8
255, ϵ1=18, ϵ2= 0.5)with random
initializations.
Clean l∞ l2 l1Union
E-AT 81.9 40.2 66.9 48.7 39.2
RAMP 80.9 45.0 66.4 46.7 43.3Clean l∞ l2 l1Union
E-AT 81.0 39.8 65.8 44.3 38.0
RAMP 79.9 43.5 65.7 45.0 41.9
Table 14: (ϵ∞=8
255, ϵ1= 12 , ϵ2=0.25)and(ϵ∞=8
255, ϵ1= 12 , ϵ2=0.75)with random
initializations.
Clean l∞ l2 l1Union
E-AT 82.8 41.3 75.6 52.9 40.5
RAMP 81.8 46.0 74.7 48.8 44.5Clean l∞ l2 l1Union
E-AT 83.0 41.2 57.6 53.0 40.5
RAMP 81.9 46.1 56.9 48.7 44.5
Table 15: (ϵ∞=8
255, ϵ1= 12 , ϵ2=1.0)and(ϵ∞=8
255, ϵ1= 12 , ϵ2=1.5)with random
initializations.
Clean l∞ l2 l1Union
E-AT 83.4 41.0 47.3 52.8 40.3
RAMP
(λ=5) 81.5 46.0 46.5 48.1 44.1Clean l∞ l2 l1Union
E-AT 83.5 41.0 25.5 52.9 25.5
RAMP 74.4 43.4 37.2 51.1 37.1
B.4.2 Additional Results with Robust Fine-tuning
Changing l∞perturbations with ϵ∞= [2
255,4
255,12
255,16
255].Table 16 and Table 17 show that
RAMP consistently outperforms E-AT [Croce and Hein, 2022] on union accuracy when performing
robust fine-tuning.
Table 16: (ϵ∞=2
255, ϵ1= 12 , ϵ2= 0.5)and(ϵ∞=4
255, ϵ1= 12 , ϵ2= 0.5)with robust
fine-tuning.
Clean l∞ l2 l1Union
E-AT 86.5 74.8 66.7 57.9 57.9
RAMP 85.8 74.0 66.2 60.1 60.1Clean l∞ l2 l1Union
E-AT 85.9 61.4 67.9 57.6 56.8
RAMP 85.7 60.9 67.6 59.3 58.1
Table 17: (ϵ∞=12
255, ϵ1= 12 , ϵ2= 0.5)and(ϵ∞=16
255, ϵ1= 12 , ϵ2= 0.5)with robust
fine-tuning.
Clean l∞ l2 l1Union
E-AT 75.5 30.8 62.4 44.6 30.0
RAMP 74.0 33.6 59.7 38.5 31.9Clean l∞ l2 l1Union
E-AT 68.7 20.7 56.1 42.1 20.5
RAMP 65.6 25.0 51.5 31.2 23.8
Changing l1perturbations with ϵ1= [6,9,12,15].Table 12 and Table 13 show that RAMP
consistently outperforms E-AT [Croce and Hein, 2022] on union accuracy when performing robust
fine-tuning.
Changing l2perturbations with ϵ2= [0.25,0.75,1.0,1.5].Table 14 and Table 15 show that RAMP
consistently outperforms E-AT [Croce and Hein, 2022] on union accuracy when performing robust
fine-tuning.
22Table 18: (ϵ∞=8
255, ϵ1=6, ϵ2= 0.5)and(ϵ∞=8
255, ϵ1=9, ϵ2= 0.5)with robust fine-tuning.
Clean l∞ l2 l1Union
E-AT 84.2 45.8 66.8 59.0 45.0
RAMP
(λ=1.5) 83.0 48.7 63.5 51.7 46.4Clean l∞ l2 l1Union
E-AT 83.1 44.9 67.2 52.6 43.2
RAMP 82.5 47.1 66.0 49.9 44.8
Table 19: (ϵ∞=8
255, ϵ1=15, ϵ2= 0.5)and(ϵ∞=8
255, ϵ1=18, ϵ2= 0.5)with robust fine-
tuning.
Clean l∞ l2 l1Union
E-AT 81.3 43.5 66.6 42.8 39.0
RAMP 80.4 44.2 66.1 44.4 41.2Clean l∞ l2 l1Union
E-AT 81.3 38.9 66.6 45.0 37.5
RAMP 80.7 40.6 66.3 43.5 38.8
Table 20: (ϵ∞=8
255, ϵ1= 12 , ϵ2=0.25)and(ϵ∞=8
255, ϵ1= 12 , ϵ2=0.75)with robust
fine-tuning.
Clean l∞ l2 l1Union
E-AT 82.3 44.2 75.3 47.2 41.4
RAMP 81.5 45.6 74.4 47.1 43.1Clean l∞ l2 l1Union
E-AT 83.0 43.5 58.1 46.5 40.4
RAMP 81.4 45.6 57.4 47.2 42.9
Table 21: (ϵ∞=8
255, ϵ1= 12 , ϵ2=1.0)and(ϵ∞=8
255, ϵ1= 12 , ϵ2=1.5)with robust fine-
tuning.
Clean l∞ l2 l1Union
E-AT 82.3 41.0 49.0 51.6 40.2
RAMP 81.4 45.6 47.8 47.1 42.9Clean l∞ l2 l1Union
E-AT 80.2 42.8 31.5 52.4 31.5
RAMP 74.9 43.7 37.0 50.2 36.9
B.5 Different Logit Pairing Methods
In this section, we test RAMP with robust fine-tuning using two more different logit pairing losses:
(1) Mean Squared Error Loss ( Lmse) (Eq. 17), (2) Cosine-Similarity Loss ( Lcos) (Eq. 18). We replace
the KL loss we used in the paper using the following losses. We use the same lambda value λ= 1.5
for both cases.
Lmse=1
nc·ncX
i=01
2(pq[γ[i]]−pr[γ[i]])2(17)
Lcos=1
nc·ncX
i=0(1−cos(pq[γ[i]], pr[γ[i]])) (18)
Table 22 displays RAMP robust fine-tuning results of different logit pairing losses using PreAct-
ResNet-18 on CIFAR-10 with λ= 1.5. We see those losses generally improve union accuracy
compared with baselines in Table 24. Lcoshas a better clean accuracy yet slightly worsened union
accuracy. Lmsehas the best union accuracy and the worst clean accuracy. LKLis in the middle of
the two others. However, we acknowledge the possibility that each logit pairing loss may have its
own best-tuned λvalue.
Table 22: RAMP fine-tuning results of different logit pairing losses using PreAct-ResNet-18 on
CIFAR-10.
Losses Clean l∞ l2 l1Union
KL 80.9 45.5 66.2 47.3 43.1
MSE 80.4 45.6 65.8 47.6 43.5
Cosine 81.6 45.4 66.7 47.0 42.9
23B.6 AT from Scratch Using WideResNet-28-10
Implementations. We use a cyclic learning rate with a maximum rate of 0.1for30epochs and
adopt the outer minimization trades loss from Zhang et al. [2019] with the default hyperparameters,
same as Croce and Hein [2022]; also, we set λ= 2.0andβ= 0.5for training RAMP . Additionally,
we use the WideResNet-28-10 architecture same as Zagoruyko and Komodakis [2016] for our
reimplementations on CIFAR-10.
Results. Since the implementation of experiments on WideResNet-28-10 in Croce and Hein [2022]
paper is not public at present, we report our implementation results on E-AT, where our results show
thatRAMP outperforms E-AT in union accuracy with a significant margin, as shown in Table 23.
Also, we experiment with using the trade loss ( RAMP w trades ) for the outer minimization, we
observe that RAMP w trades achieves a better union accuracy at the loss of some clean accuracy.
Table 23: WideResNet-28-10 trained from random initialization on CIFAR-10. RAMP outper-
forms E-AT on union accuracy with our implementation.
Methods Clean l∞ l2 l1 Union
E-AT w trades (reported in Croce and Hein [2022]) 79.9 46.6 66.2 56.0 46.4
E-AT w trades (ours) 79.2 44.2 64.9 54.9 44.0
RAMP w/o trades (ours) 81.1 46.6 65.9 48.1 44.6
RAMP w trades (ours) 79.9 47.1 65.1 49.0 45.8
B.7 Robust Fine-tuning Using PreAct-ResNet-18
Implementations. For robust fine-tuning with ResNet-18, we perform 3epochs on CIFAR-10. We
set the learning rate as 0.05for PreAct-ResNet-18 and 0.01for other models. We set λ= 0.5in this
case. Also, we reduce the learning rate by a factor of 10after completing each epoch.
Result. Table 24 shows the robust fine-tuning results using PreAct ResNet-18 model on the CIFAR-10
dataset with different methods. The results for all baselines are directly from the E-AT paper [Croce
and Hein, 2022] where the authors reimplemented other baselines (e.g., MSD, MAX) to achieve
better union accuracy than presented in the original works. RAMP surpasses all other methods on
union accuracy.
Table 24: RN-18 l∞-AT model fine-tuned for 3 epochs (repeated for 5 seeds). RAMP has the
highest union accuracy. Baseline results are from Croce and Hein [2022].
Methods Clean l∞ l2 l1 Union
RN-18- l∞-AT 83.7 48 .1 59 .8 7 .7 38 .5
+ SAT 83.5±0.2 43 .5±0.2 68 .0±0.4 47 .4±0.5 41 .0±0.3
+ A VG 84.2±0.4 43 .3±0.4 68 .4±0.6 46 .9±0.6 40 .6±0.4
+ MAX 82.2±0.3 45 .2±0.4 67 .0±0.7 46 .1±0.4 42 .2±0.6
+ MSD 82.2±0.4 44 .9±0.3 67 .1±0.6 47 .2±0.6 42 .6±0.2
+ E-AT 82.7±0.4 44 .3±0.6 68 .1±0.5 48 .7±0.5 42 .2±0.8
+RAMP (λ=1.5) 81.1±0.2 45 .4±0.3 66 .1±0.2 47 .2±0.143.1±0.2
+RAMP (λ= 0.5) 81.5±0.1 45 .5±0.2 66 .4±0.2 47 .0±0.142.9±0.2
B.8 Robust Fine-tuning with More Epochs
In Table 25, we apply robust fine-tuning on the PreAct ResNet-18 model for the CIFAR-10 dataset
with5,7,10,15epochs, and compare it with E-AT. RAMP consistently outperforms the baseline on
union accuracy, with a larger improvement when we increase the number of epochs.
C Additional Visualization Results
In this section, we provide additional t-SNE visualizations of the multiple-norm tradeoff and robust
fine-tuning procedures using different methods.
24Table 25: Fine-tuning with more epochs :RAMP consistently outperforms E-AT on union accuracy.
E-AT results are from Croce and Hein [2022].
5 epochs 7 epochs 10 epochs 15 epochs
Clean Union Clean Union Clean Union Clean Union
E-AT 83.0 43.1 83.1 42.6 84.0 42.8 84.6 43.2
RAMP 81.7 43.6 82.1 43.8 82.5 44.6 83.0 44.9
C.1 Pre-trained l1, l2, l∞AT models
Figure 6 shows the robust accuracy of l1, l2, l∞AT models against their respect l1, l2, l∞perturba-
tions, on CIFAR-10 using PreAct-ResNet-18 architecture. Similar to Figure ??,l∞-AT model has a
lowl1robustness and vice versa. In this common choice of epsilons, we further confirm that l∞−l1
is the key trade-off pair.
C.2 Robust Fine-tuning for all Epochs
We provide the complete visualizations of robust fine-tuning for 3 epochs on CIFAR-10 using l1
examples, E-AT, and RAMP . Rows in l1fine-tuning (Figure 7), E-AT fine-tuning (Figure 8), and
RAMP fine-tuning (Figure 9) show the robust accuracy against l∞, l1, l2attacks individually, of
epoch 0,1,2,3, respectively. We observe that throughout the procedure, RAMP manages to maintain
more l∞robustness during the fine-tuning with more points colored in cyan, in comparison with two
other methods. This visualization confirms that after we identify a lp−lr(p̸=r)key tradeoff pair,
RAMP successfully preserves more lprobustness when training with some lrexamples via enforcing
union predictions with the logit pairing loss.
Figure 6: l1, l2, l∞pre-trained RN18 l∞-AT models with correct/incorrect predictions against
l1, l2, l∞attacks. Correct predictions are colored with cyan and incorrect with magenta. Each row
represents l∞, l1, l2AT models, respectively. Each column shows the accuracy concerning a certain
lpattack.
25Figure 7: Finetune RN18 l∞-AT model on l1examples for 3 epochs . Each row represents the
prediction results of epoch 0,1,2,3respectively.
Figure 8: Finetune RN18 l∞-AT model with E-AT for 3 epochs . Each row represents the prediction
results of epoch 0,1,2,3respectively.
26Figure 9: Finetune RN18 l∞-AT model with RAMP for 3 epochs . Each row represents the
prediction results of epoch 0,1,2,3respectively.
27NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction present the claims made in the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the limitations of our work in the discussion section of the paper.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We state the assumptions along with theoretical results in the theory part of
the paper. The proofs are in the supplemental material.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide the implementations of our experiments in both main paper and
supplementary materials.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide scripts to reproduce all experimental results for the new proposed
method.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The full details are provided with the code and in appendix.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The results are accompanied by confidence intervals.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We provide the information on the computer resources in the discussion section
of the paper.
289.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We conduct the research with the NeurIPS Code of Ethics.
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We mention the societal impacts of the work in the discussion section of the
paper.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We cite the original paper that produced the code package or dataset.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We provide README and scripts to run for the new asset we introduce. They
are well documented.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not include crowdsourcing or research involving human
subjects.
29