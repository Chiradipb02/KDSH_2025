Mean-Field Analysis for Learning Subspace-Sparse
Polynomials with Gaussian Input
Ziang Chen
Department of Mathematics
Massachusetts Institute of Technology
Cambridge, MA 02139
ziang@mit.edu
Rong Ge
Department of Computer Science and Department of Mathematics
Duke University
Durham, NC 27708
rongge@cs.duke.edu
Abstract
In this work, we study the mean-field flow for learning subspace-sparse polynomials
using stochastic gradient descent and two-layer neural networks, where the input
distribution is standard Gaussian and the output only depends on the projection of
the input onto a low-dimensional subspace. We establish a necessary condition for
SGD-learnability, involving both the characteristics of the target function and the
expressiveness of the activation function. In addition, we prove that the condition is
almost sufficient, in the sense that a condition slightly stronger than the necessary
condition can guarantee the exponential decay of the loss functional to zero.
1 Introduction
Neural Networks (NNs) are powerful in practice to approximate mappings on certain data structures,
such as Convectional Neural Networks (CNNs) for image data, Graph Neural Networks (GNNs)
for graph data, and Recurrent Neural Networks (RNNs) for sequential data, stimulating numerous
breakthroughs in application of machine learning in many branches of science, engineering, etc.
The surprising performance of neural networks is often explained by arguing that neural networks
automatically learns useful representations of the data. However, how simple training procedures
such as stochastic gradient descent (SGD) extract features remains a major open problem.
Optimization of neural networks has received lots of attention. For simpler networks such as linear
neural networks, local minima are also globally optimal [ 14,15,17]. However, this is not true for
nonlinear networks even of depth 2 [ 23]. Neural Tangent Kernel (NTK, [ 4,12,13]) is a line of
work that establishes strong convergence results for wide neural networks. However, in the NTK
regime, neural network is equivalent to a kernel, which cannot learn useful features based on the
target function. Such limitation prevents neural networks in NTK regime from efficiently learning
even simple single index models [28].
As an alternative, the behavior of SGD can also be understood via mean-field analysis, for both
two-layer neural networks [ 9,19,20,22,24,25] and multi-layer neural networks [ 5,21,22]. Neural
networks in the mean-field regime have the potential to do feature learning. Recently, [ 1] showed
an interesting setup where a two-layer neural network can learn representations if the target func-
tion satisfies a merged-staircase property. More precisely, [ 1] considers a sparse polynomial as a
38th Conference on Neural Information Processing Systems (NeurIPS 2024).polynomial f∗:Rd→Rdefined on the hypercube {−1,1}d, i.e., f∗(x) =h∗(z) =h∗(xI)where
z=xI= (xi)i∈I,Iis an unknown subset of {1,2, . . . , d }with|I|=p, and h∗:{−1,1}p→Ris
a function on the subset of coordinates in I. They prove that a condition called the merged-staircase
property is necessary and in some sense sufficient for learning such f∗using SGD and two-layer
neural networks. The merged-staircase property proposed in [ 1] states that all monomials of h∗can
be ordered such that each monomial contains at most one zithat does not appear in any previous
monomial. For example, h∗(z) =z1+z1z2+z1z2z3satisfies the merged-staircase property while
h∗(z) =z1+z1z2z3does not. Results on similar structures can also be found in [ 3]. The work [ 2]
proposes the concept of leap complexity and generalizes the results in [ 1] to a larger family of sparse
polynomials.
In this work, we consider “subspace-sparse" polynomial that is more general. Concretely, let
f∗(x) = h∗(z) = h∗(xV), where Vis a subspace of Rdwith dim(V) = p≪d,xVis the
orthogonal projection of xonto the subspace V, and h∗:V→Ris an underlying polynomial map.
In other words, the sparsity is in the sense that f∗(x)only depends on the projection of the input
x∈Rdin a low-dimensional subspace. Throughout this paper, the input data distribution is the
standard d-dimensional normal distribution, i.e., x∼ N(0, Id), which is rotation-invariant in the
sense that Ox∼ N(0, Id)for any orthogonal matrix O∈Rd×d. Similar rotation-invariant/basis-free
settings are also considered in some recent studies, including [2, 8, 10, 11].
Our contribution and related works Our first contribution is a basis-free necessary condition for
SGD-learnability. More specially, we propose the reflective property of the underlying polynomial
h∗:V→Rwith respect to some subspace S⊂V, which also involves the expressiveness of
the activation function. We prove that as long as the reflective property is satisfied with respect to
nontrivial S, the training dynamics cannot learn any information about the behavior of h∗onS(see
Theorem 3.4). Therefore the loss functional will be bounded away from 0 during the whole training
procedure.
One key point is that our reflective property precisely characterizes the necessary expressiveness
of the activation function. If the activation function is expressive enough, the reflective property
equivalently recovers a necessary condition characterized by isoLeap [ 2] that is the maximal leap
complexity over all orthonormal basis and can be viewed as a basis-free generalization of the merged-
staircase property. This also indicates that our necessary condition is a bit weaker. Other related
rotation-invariant conditions in the previous literature include leap exponent/index [ 8,10], subspace
conditioning [ 10] and even-symmetric directions [ 11]. The analysis in [ 10,11] is for training the first
layer for finitely many iterations with fixed second-layer, and [ 8] studies the joint learning dynamics
where they assume that for any fixed first layer, the optimal parameters in second layer can be found
efficiently and reformulate the loss as a function of the first layer. Differently and more generally, our
analysis for the necessary condition does not require specific learning strategies and works for any
learning rates satisfying some mild conditions.
Our second contribution is a sufficient condition for SGD-learnability that is also basis-free and is
slightly stronger than the necessary condition. In particular, we show that if the training dynamics
cannot be trapped in any proper subspace of V, then one can choose the initial parameter distribution
and the learning rate such that the loss functional decays to zero exponentially fast with dimension-
free rates (see Theorem 4.3). Our training strategy is inspired by [ 1] with the difference that we take
the average of pindependent training trajectories, which can lift some linear independence property
required for polynomials on hypercube to algebraic independence in the general polynomial setting.
Technical challenges It may seem simple to leave the standard basis and generalize the results
of [1,2] to learn subspaces, because SGD itself is independent of the basis, and we can consider a
symmetric Gaussian input distribution. However, there are some significant barriers that motivated
our training process. The condition and the analysis in [ 1,2] rely on an orthonormal basis of the input
spaceRd. This is natural for polynomials on the hypercube {−1,1}d, but not for general polynomials
onRd. Particularly, their theory does not work for Gaussian input data x∼ N(0, Id), which is
probably the most common distribution in data science, unless an orthonormal basis of Rdis specified
andVis known to be spanned by pelements in the basis. In this work, we consider a more general
setting in which specifying a basis is not required and the space Vcan be any p-dimensional subspace
ofRd. This setting is consistent with the rotation-invariant property of N(0, Id)and introduces more
difficulties since less knowledge of Vis available prior to training.
2Organization The rest of this paper will be organized as follows. We introduce some preliminaries
on mean-field dynamics in Section 2. The basis-free necessary and sufficient conditions for SGD-
learnability are discussed in Section 3 and Section 4, respectively. We conclude in Section 5.
2 Preliminaries on Mean-Field Dynamics
The mean-field dynamics describes the limiting behavior of the training procedure when the step-
size/learning rate converges to zero, i.e., the evolution of a neuron converges to the solution of a
differential equation with continuous time, and when the number of neurons converges to infinity,
i.e., the empirical distribution of all neurons converges to some limiting probability distribution.
For two-layer neural networks, some quantitative results are established in [ 20] that characterize
the distance between the SGD trajectory and the mean-field evolution flow, and these results are
further improved as dimension-free in [ 19]. Such results suggest that analyzing the mean-field flow is
sufficient for understanding the SGD trajectory in some settings. In this section, we briefly review the
setup of two-layer neural networks, SGD, and their mean-field versions, following [19, 20].
Two-layer neural network and SGD The two-layer neural network is of the following form:
fNN(x; Θ) :=1
NNX
i=1τ(x;θi) =1
NNX
i=1aiσ(w⊤
ix), (2.1)
where Nis the number of neurons, Θ = ( θ1, θ2, . . . , θ N)withθi= (ai, wi)∈Rd+1is the set of
parameters, and σ:R→Ris the activation functions with τ(x;θ) :=aσ(w⊤x)forθ= (a, w).
Then the task is to find some parameter Θsuch that the ℓ2-distance between f∗andfNNis minimized:
min
ΘEN(Θ) :=1
2Ex∼N(0,Id)
|f∗(x)−fNN(x; Θ)|2
. (2.2)
In practice, a widely used algorithm for solving (2.2) is the stochastic gradient descent (SGD) that
iterates as
θ(k+1)
i =θ(k)
i+γ(k)
f∗(xk)−fNN(xk; Θ(k))
∇θτ(xk;θ(k)
i), (2.3)
where xk, k= 1,2, . . . are the i.i.d. samples drawn from N(0, Id)andγ(k)=diag(γ(k)
a, γ(k)
wId)⪰
0is the stepsize or the learning rate. In this paper, we only consider the one-pass model with each
data point being used exactly once, following [19].
Mean-field dynamics One can generalize (2.1) to an infinite-width two-layer neural network:
fNN(x;ρ) :=Z
τ(x;θ)ρ(dθ) =Z
aσ(w⊤x)ρ(da, dw ),
where ρ∈ P(Rd+1)is a probability measure on the parameter space Rd+1, and generalize the
loss/energy functional (2.2) to
E(ρ) :=1
2Ex∼N(0,Id)h
|f∗(x)−fNN(x;ρ)|2i
.
We will use P(X)to denote the collection of probability measures on a space Xthroughout this
paper. The limiting behavior of the SGD trajectory (2.3) when γ(k)→0andN→ ∞ can be
described by the following mean-field dynamics:
∂tρt=∇θ·(ρtξ(t)∇θΦ(θ;ρt)),
ρt
t=0=ρ0,(2.4)
where ξ(t) =diag(ξa(t), ξw(t)Id)∈R(d+1)×(d+1)withξa(t)≥0andξw(t)≥0being the learning
rates and
Φ(θ;ρ) =aEx∼N(0,Id)
(fNN(x;ρ)−f∗(x))σ(w⊤x)
.
One can also write Φ(θ;ρ)as
Φ(θ;ρ) =V(θ) +Z
U(θ, θ′)ρ(dθ′),
3where
V(θ) =−aEx
f∗(x)σ(w⊤x)
and U(θ, θ′) =aa′Ex
σ(w⊤x)σ((w′)⊤x)
. (2.5)
The PDE (2.4) is understood in the weak sense, i.e., ρtis a solution to (2.4) if and only if ρt
t=0=ρ0
and
ZZ
(−∂tη+∇θη·(ξ(t)∇θΦ(θ;ρt)))ρt(dθ)dt= 0,∀η∈ C∞
c(Rd+1×(0,+∞)),
where C∞
c(Rd+1×(0,+∞))is the collection of all smooth and compactly supported functions on
Rd+1×(0,+∞). It can also be computed that the energy functional is non-increasing along ρt:
d
dtE(ρt) =−Z
∇θΦ(θ;ρt)⊤ξ(t)∇θΦ(θ;ρt)ρt(dθ)≤0. (2.6)
There have been standard results in the existing literature that provide dimension-free bounds for the
distance between the empirical distribution of the parameters generalized by (2.3) and the solution to
(2.4) . For the simplicity of reading, we will not present those results and the proof; interested readers
are referred to [ 19]. In the rest of this paper, we will focus on the analysis of (2.4) and briefly discuss
the sample complexity results implied by our mean-field analysis.
3 Necessary Condition for SGD-Learnability
This section introduces a condition that can prevent SGD from recovering all information about f∗,
or in other words, prevent the loss functional E(ρt)decaying to a value sufficiently close to 0.
3.1 Reflective Property
Before rigorously presenting our main theorem, we state the assumptions used in this section.
Assumption 3.1. Assume that the followings hold:
(i)The activation function σ:R→Ris twice continuously differentiable with ∥σ∥L∞(R)≤
Kσ,∥σ′∥L∞(R)≤Kσ, and∥σ′′∥L∞(R)≤Kσfor some constant Kσ>0.
(ii)The learning rates ξa, ξw:R≥0→Rsatisfy that ∥ξa∥L∞(R≥0)≤Kξand∥ξw∥L∞(R≥0)≤
Kξfor some constant Kξ>0. Furthermore, ξaandξware Lipschitz continuous withR+∞
0ξa(t)dt= +∞andR+∞
0ξw(t)dt= +∞.
(iii) The initialization is ρ0=ρa×ρwsuch that ρais symmetric and is supported in [−Kρ, Kρ]
for some constant Kρ>0.
In Assumption 3.1, the Condition (i) is satisfied by some commonly used activation functions, such
asσ(x) =1
1+e−xandσ(x) = cos( x), and is required for establishing the existence and uniqueness
of the solution to (2.4). The Condition (ii) and (iii) are also standard and easy to satisfy in practice.
Remark 3.2. The symmetry of ρaimplies that fNN(x;ρ0) = 0 . Therefore, the initial loss E(ρ0) =
1
2Ex[|f∗(x)|2] =1
2ExV[|h∗(xV)|2] =1
2Ez[|h∗(z)|2], where xV=z∼ N(0, IV), can be viewed as
a constant depending only on h∗andp, independent of d. Noticing also the decay property (2.6) , the
loss at any time tcan be bounded as E(ρt)≤ E(ρ0) =1
2Ez[|h∗(z)|2].
The main goal of this section is to generalize the merged-staircase property in a basis-free setting
for general polynomials. Without a standard basis, it is hard to talk about having a “staircase”
of monomials. Even with a fixed basis, it is still nontrial to define the merged-staircase property
for general polynomials since the analysis in [ 1] highly depends on z2
i= 1 that is only true for
polynomials on the hypercube. Instead, we use the observation that when a function does not satisfy
the merged staircase property, it implies that two of the variables will behave the same in the training
dynamics. Such a symmetry can be generalized to the basis-free setting for general polynomials and
we summarize this as the following reflective property:
4Definition 3.3 (Reflective property) .LetS⊂V⊂Rdbe a subspace of V. We say that the underlying
polynomial h∗:V→Rsatisfies the reflective property with respect to the subspace Sand the
activation function σif
Ez∼N(0,IV)
h∗(z)σ′ 
u+v⊤z⊥
S
zS
= 0,∀u∈R, v∈V, (3.1)
where zS=PV
S(z)andz⊥
S=z− PV
S(z), withPV
S:V→Sbeing the orthogonal projection from
VontoS.
The reflective property defined above is closely related to the merged-staircase property in [ 1]. Let us
illustrate the intuition using a simple example. Consider V=R3andh∗(z) =z1+z1z2z3. Then
h∗does not satisfy the merged-staircase property since z1z2z3involves two new coordinates that
do not appear in the first monomial z1. In our setting, this h∗satisfies the reflective with respect to
S=span{e2, e3}, where eiis the vector in R3with the i-th entry being 1and other entries being 0.
More specifically, for z= (z1, z2, z3), one has that zS= (0, z2, z3)andz⊥
S= (z1,0,0). Thus, one
has for any u∈Randv∈Vthatσ′ 
u+v⊤z⊥
S
is independent of z2, z3and that
Ez2,z3
h∗(z)σ′ 
u+v⊤z⊥
S
zS
=σ′ 
u+v⊤z⊥
S
Ez2,z3 
0, z1z2+z1z2
2z3, z1z3+z1z2z2
3
= (0,0,0),
which leads to (3.1) . One can see from this example that satisfying the reflective property with respect
to a nontrivial subspace S⊂Vis in the same spirit as not satisfying the merged-staircase property.
Furthermore, the reflective property is rotation-invariant, meaning that using a different orthonormal
basis does not change the property. In this sense, our proposed condition is more general than that
in [1]. We also remark that there have been other rotation-invariant conditions generalizing [ 1], see
e.g., [2, 8, 10, 11].
Another comment is that the reflective property (3.1) depends on the activation function σ, while
conditions in previous works [ 1,2,8,10,11] are all defined for the target function f∗orh∗itself.
There does exist a variant of our reflective property that is independent of σ′, namely,
EzS∼N(0,IS)[h∗(z)zS] = 0,∀z⊥
S, (3.2)
which actually implies (3.1) . But these two conditions are different: h∗(z) =z1+z1z2+z1z2z3does
not satisfy (3.2) but still satisfies (3.1) ifσ(ζ) =ζ. We use (3.1) withσ′because we want to emphasize
that the SGD learnability depends on the activation function σ. Ifσis less expressive, then SGD
may not learn the target function even if h∗itself satisfies the merged-staircase property. Typically
people use activation functions that are expressive enough, for which (3.1) and(3.2) are similar. In
addition, it can be verified that (3.2) with some nontrivial Sis equivalent to isoLeap (h∗)≥2that
means h∗:V→Rdoes not satisfy the merged-staircase property for some orthonormal basis of
V[2], and the idea of leaps is used in [ 8,10]. We include the proof of equivalence in Appendix B.1.
Our main result in this section is that the reflective property with nontrivial Swould lead to a positive
lower bound of E(ρt)along the training dynamics, which provides a necessary condition for the
SGD-learnability and is formally stated as follows.
Theorem 3.4. Suppose that Assumption 3.1 holds with ρw∼ N(0,1
dId), and that h∗:V→R
satisfies the reflective property with respect to some subspace S⊂Vand activation function σ. Then
for any T >0, there exists a constant C >0depending only on p,h∗,Kσ,Kξ,Kρ, and T, such that
inf
0≤t≤TE(ρt)≥1
2Ez∼N(0,IV)
|h∗(z)−h∗
S⊥(z⊥
S)|2
−C
d1/2, (3.3)
where h∗
S⊥(z⊥
S) =EzS[h∗(z)]. In particular, if h∗(z)is not independent of zS, then for any T >0,
there exists d(T)>0depending only on p,h∗,Kσ,Kξ,Kρ, and T, such that for any d > d (T), we
have
inf
0≤t≤TE(ρt)≥1
4Ez∼N(0,IV)
|h∗(z)−h∗
S⊥(z⊥
S)|2
>0. (3.4)
It is worth remarking that in Theorem 3.4, the training time Tis a constant independent of the
dimension d. If a longer d-dependent training beyond a constant time is allowed, then E(ρt)might be
reasonably small even if the necessary condition is not satisfied, see e.g. [2, 18, 26].
In Appendix B.2, we include a brief discussion of the sample complexity result of SGD implied
by Theorem 3.4. In particular, SGD with O(d)samples cannot recover f∗reliably if the refelctive
5property holds, which is consistent with observations in previous works such as [ 1,2]. We also
remark that our result in Theorem 3.4 is established for the mean-field dynamics corresponding to the
one-pass SGD (2.3) , any may not apply for other variants of SGD. In particular, some recent works
[6,11,16] prove that multi-pass SGD with batch-reuse mechanism can learn some target functions
with fewer samples than one-pass SGD.
3.2 Proof Sketch for Theorem 3.4
To prove Theorem 3.4, the main intuition is that under some mild assumptions, if (3.1) is satisfied
and the initial distribution ρ0is supported in {(a, w)∈Rd+1:wS= 0}, where wSis the orthogonal
projection of w∈RdontoS, then ρtis supported in {(a, w)∈Rd+1:wS= 0}for all t≥0. This
means that the trained neural network fNN(x;ρt)learns no information about xS, the orthogonal
projection of x∈RdontoS, and hence cannot approximate f∗(x) =h∗(xV)with arbitrarily small
error if h∗(z)is dependent on zS. We formulate this observation in the following theorem.
Theorem 3.5. Suppose that Assumption 3.1 hold and let ρtbe the solution to (2.4) . LetS⊂Rdbe a
subspace with the projection map PS:Rd+1→Sthat maps (a, w)towS. If(PS)#ρ0=δS, where
δSis the delta measure on Sand
Ex
f∗(x)σ′ 
w⊤x⊥
S
xS
= 0,∀w∈Rd, (3.5)
where x⊥
S=x−xS, then it holds for any t≥0that
(PS)#ρt=δS. (3.6)
Here, the delta measure δSonSis a probability measure on Ssuch that for any continuous and
compactly supported function φ:S→R, it holds thatR
Sφ(x)δS(dx) =φ(0). In Theorem 3.5, the
condition (3.5) is stated in terms of f∗. We will show later that it is closely related to and is actually
implied by (3.1) , via a decomposition w⊤x⊥
S=w⊤(x−xV) +w⊤(xV−xS), with w⊤(x−xV)
andw⊤(xV−xS)corresponding to uandv⊤z⊤
Sin(3.1) , respectively. The main idea in the proof
of Theorem 3.5 is to construct a flow ˆρtin the space P(R×S⊥), where S⊥is the orthogonal
complement of SinRd, and then show that ρt= ˆρt×δSis the solution to (2.4) . More specifically,
the flow ˆρtis constructed as the solution to the following evolution equation in P(R×S⊥):(
∂tˆρt=∇ˆθ·
ˆρtˆξ(t)∇ˆθˆΦ(ˆθ; ˆρt)
,
ˆρt
t=0= ˆρ0,(3.7)
where ˆρ0∈ P(R×S⊥)satisfies ρ0= ˆρ0×δS,ˆθ= (a, w⊥
S),ˆξ(t) =diag(ξa(t), ξw(t)IS⊥), and
ˆΦ(ˆθ; ˆρ) =aExh
ˆfNN(x⊥
S; ˆρ)−f∗(x)
σ 
(w⊥
S)⊤x⊥
Si
=ˆV(ˆθ) +Z
ˆU(ˆθ,ˆθ′)ˆρ(dˆθ′),
with
ˆfNN(x⊥
S; ˆρ) =Z
aσ 
(w⊥
S)⊤x⊥
S
ˆρ(da, dw⊥
S),
and
ˆV(ˆθ) =−aEx
f∗(x)σ 
(w⊥
S)⊤x⊥
S
,ˆU(θ, θ′) =aa′Ex
σ 
(w⊥
S)⊤x⊥
S
σ 
((w⊥
S)′)⊤x⊥
S
.
The detailed proof will be presented in Appendix A.1.
In practice, both VandSare unknown and it is nontrivial to choose an initialization ρ0supported
in{(a, w)∈Rd+1:wS= 0}. However, one can set ρ0=ρa×ρwwith ρw∼ N (0,1
dId)
and this can make the marginal distribution of ρ0onSvery close to the the delta measure δSif
d >> p =dim(V)≥dim(S), which fits the setting of subspace-sparse polynomials. Rigorously, we
have the following theorem stating dimension-free stability with respect to initial distribution, with
the proof deferred to Appendix A.2.
Theorem 3.6. Suppose that Assumption 3.1 holds for both ρ0and˜ρ0. Let ρtsolve ∂tρt=∇θ·
(ρtξ(t)∇θΦ(θ;ρt))and let ˜ρtsolve ∂t˜ρt=∇θ·(˜ρtξ(t)∇θΦ(θ; ˜ρt)). Then for any T∈(0,+∞),
there exists a constant Cs>0depending only on p,h∗,Kσ,Kξ,Kρ, and T, such that
sup
0≤t≤TEx
|fNN(x;ρt)−fNN(x; ˜ρt)|2
≤CsW2
2(ρ0,˜ρ0), (3.8)
where W2(·,·)is the 2-Wasserstein metric.
Based on Theorem 3.5 and Theorem 3.6, Theorem 3.4 can be proved by some straightforward
computation, for which the details can be found in Appendix A.3.
6Algorithm 1 Training strategy
1:Set the initial distribution as ρ0=ρa×δRd, where ρa=U([−1,1])andδRdis the delta measure
onRd.
2:Setξa(t) = 0 andξw(t) = 1 for0≤t≤T, and train the neural network with activation function
σ. Denote by (a, w(a, t)),0≤t≤Tthe trajectory of a single particle that starts at (a,0).
3:Repeat Step 2 for ptimes independently and obtain pcopies of parameters at T, say(ai, w(ai, T))
withai∼ U([−1,1]),i= 1,2, . . . , p .
4:Reset ρTas the distribution of (0, u(a1, . . . , a p, T)) =
0,1
pPp
i=1w(ai, T)
. Train the neural
network with ξa(t) = 1 ,ξw(t) = 0 , and a new activation function ˆσ(ζ) = (1 + ζ)n, where
n=deg(f∗), fort≥Tstarting at ρT.
4 Sufficient Condition for SGD-Learnability
In this section, we propose a sufficient condition and a training strategy that can guarantee the
exponential decay of E(ρt)with constants independent of the dimension d.
4.1 Training Procedure and Convergence Guarantee
We prove in Section 3 that if the trained parameters always stay in a proper subspace {(a, w)∈
Rd+1:wS= 0}, then fNN(x;ρt)cannot learn all information about f∗orh∗. Ideally, one would
expect the negation to be a sufficient condition for the SGD-learnability, i.e., the existence of a choice
of learning rates and initial distribution that guarantees limt→∞E(ρt)with dimension-free rate. This
is almost true but we need a slightly stronger condition due to technical issues. More specifically, we
need that the Taylor’s expansion of some dynamics (not the dynamics itself) is not trapped in any
proper subspace.
Assumption 4.1. Consider the following flow ˆwV(t)inV:
d
dtˆwV(t) =Ez
zh∗(z)σ′( ˆwV(t)⊤z)
,
ˆwV(0) = 0 .(4.1)
We assume that for some s∈N+, the Taylor’s expansion up to s-th order of ˆwV(t)att= 0is not
contained in any proper subspace of V.
Assumption 4.1 aims to state the same observation as the merged-staircase property in [ 1]. As a
simple example, if V=Rpandh∗(z) =z1+z1z2+z1z2z3+···+z1z2···zpwhich satisfies the
merged-staircase property, then it can be computed that the leading order terms of the coordinates of
ˆwV(t)are given by (c1t, c2t2, c3t22, . . . , c pt2p−1)with nonzero constants c1, c2, . . . , c pifσ∈ Cs(R)
withs= 2p−1andσ(1)(0), σ(2)(0), . . . , σ(p)(0)are all nonzero (see Proposition 33 in [ 1]). This
is to say that Assumption 4.1 with s= 2p−1is satisfied for this example. We provide further
characterization of Assumption 4.1 by verifying it in a more general setting in Appendix D.1.
We also remark that the Taylor’s expansion of the flow ˆwV(t)that solves (4.1) depends only on the h∗
andσ(1)(0), σ(2)(0), . . . , σ(s)(0). We require some additional regularity assumption on higher-order
derivatives of σ.
Assumption 4.2. Assume that σsatisfies σ∈ CL+1(R)andσ, σ′, σ′′, σ(L+1)∈L∞(R), where
L= 2sn n+p
p
withn=deg(f∗) =deg(h∗)andsbeing the positive integer in Assumption 4.1.
Our proposed training strategy is stated in Algorithm 1. The training strategy is inspired by the
two-stage strategy proposed in [ 1] that trains the parameters wwith fixed afort∈[0, T]and
then trains the parameter awith fixed wand a perturbed activation function for t≥T. Several
important modifications are made since we consider general polynomials, rather than polynomials on
hypercubes as in [1]. In particular,
•We need to repeat Step 2 (training w) forptimes and use their average as the initialization
of training a, while this step only needs to be done once in [ 1]. The reason is that the
space of polynomials on the hypercube {±1}pis essentially a linear space with dimension
2p. However, the space of general polynomials on Vis anR-algebra that is also a linear
7space but is of infinite dimension. Therefore, to make the kernel matrix in training a
non-degenerate, we require some algebraic independence which can be guaranteed by
u(a1, . . . , a p, t)) =1
pPp
i=1w(ai, t),0< t≤T, though linear independence suffices
for [1]. Let us also emphasize that each run of Step 2 involves training an interacting particle
system instead of training a single particle.
•In Step 4, we use a new activation function ˆσ(ζ) = (1 + ζ)nthat is a polynomial of
the same degree as f∗andh∗. The reason is still that we work with the space general
polynomials whose dimension as a linear space is infinite. Thus, we need the specific form
ˆσ(ζ) = (1 + ζ)nto guarantee the trained neural network fNN(x;ρt)is a polynomial with
degree at most n=deg(f∗) =deg(h∗). As a comparison, in the setting of [ 1], all functions
on{±1}pcan be understood as a polynomial, and no specific format of the new activation
function is needed.
Our main theorem in this section is as follows, stating that the loss functional E(ρt)can decay to 0
exponentially fast, with rates independent of the dimension d.
Theorem 4.3. Suppose that Assumption 4.1 and 4.2 hold and let ρtbe the flow generated by
Algorithm 1. There exist constants C1, C2>0depending on h∗, σ, n, p, s , such that
E(ρt)≤C1exp(−C2t),∀t≥0.
Let us also remark that it is possible to use the original dynamics ˆwVdefined in (4.1) when we state
Assumption 4.1, which can actually imply its Taylor’s expansion up to some order is not trapped
in any proper subspace of Vif we further assume ˆwV(t)is analytic. We choose to directly use
Taylor’s expansion in Assumption 4.1 since we want to avoid the additional analytic assumption
and to emphasize that the constants C1, C2in Theorem 4.3 depend on the order sof the Tayler’s
expansion satisfying Assumption 4.1.
Discussion about the sample complexity implied by Theorem 4.3 is included in Appendix D.2,
suggesting that O(d)samples suffices for SGD to learn f∗reliably if conditions in Theorem 4.3 are
true. This is also consistent with previous works such as [1, 2].
4.2 Proof Sketch for Theorem 4.3
To prove Theorem 4.3 we follow the same general strategy as [ 1], though some technical analysis is
significantly different due to the roatation-invariant setting. The main goal here is to show before Step
4, the algorithm already learned a diverse set of features. After that, note that Step 4 in Algorithm 1 is
essentially a convex/quadratic optimization problem (since we only train aand set ξw(t) = 0 ). In
addition, thanks to the new activation function ˆσ(ζ) = (1 + ζ)n, one only needs to consider PV,nthat
is the space of of all polynomials on Vwith degree at most n=deg(h∗) =deg(f∗). The dimension
ofPV,nas a linear space is n+p
p
. Letp1, p2, . . . , p(n+p
p)be the orthonormal basis of PV,nwith input
z∼ N(0, IV)and define the kernel matrix
Ki1,i2(t) =Ea1,...,a p
Ez,z′
pi1(z)ˆσ(u(a1, . . . , a p, t)⊤z)ˆσ(u(a1, . . . , a p, t)⊤z′)pi2(z′)
,(4.2)
where ˆσ(ξ) = (1 + ξ)n,(a1, . . . , a p)∼ U([−1,1]p),1≤i1, i2≤ n+p
p
, and 0≤t≤T. As
long as this kernel matrix is non-degenerate, we know that the loss functional is strongly convex
with respect to the parameters in the second layer when fixing the first layer, and thus, it can be
computed straightforwardly that the loss decays to 0exponentially fast for t≥T, leading to the
desired convergence rate in Theorem 4.3; see Appendix C.3 for details. Thus, the main part in the
proof of Theorem 4.3 is to establish the non-degeneracy of the kernel matrix.
Proposition 4.4. Suppose that Assumption 4.1 and 4.2 hold. There exist constants C, T > 0
depending on h∗, σ, n, p, s , such that
λmin(K(t))≥Ct2sn(n+p
p),∀0≤t≤T, (4.3)
where λmin(K(t))is the smallest eigenvalue of K(t).
In the rest of this subsection, we sketch the main ideas in the proof of Proposition 4.4, with the details
of the proof being deferred to Appendix C. We first show that w(ai, t)andu(a1, . . . , a p, t)can be
8approximated well by ˆw(ai, t)andˆu(a1, . . . , a p, t) =1
pPp
i=1ˆw(ai, t)that are polynomials in ai
anda1, . . . , a prespectively. This approximation step follows [ 1] closely and is analyzed detailedly in
Appendix C.1. Therefore, to give a positive lower bound of λmin(K(t)), one only needs to show the
non-degeneracy of the matrix ˆM(a, t)∈R(n+p
p)×(n+p
p)with
ˆMi1,i2(a, t) =Ez
pi1(z)ˆσ(ˆu(ai2, t)⊤z)
,
where a=
a1,a2, . . . , a(n+p
p)
andai∈Rpfori= 1,2, . . . , n+p
p
. Intuitively, this non-
degeneracy can be implied by
span
ˆσ(ˆu(a1, . . . , a p, t)⊤z) :a1, a2, . . . , a p∈[−1,1]	
=PV,n,
which is true if ˆui(a1, . . . , a p, t),1≤i≤pareR-algebraically independent polynomials in
a1, a2, . . . , a p, where ˆuiis the i-th coefficient of ˆuunder some basis of V, and algebraic independence
can be obtained from linear independence by taking the average of independent copies. We illustrate
this intuition with a bit more detail.
Algebraic independence of ˆui.With Assumption 4.1, ˆw1(a, t),ˆw2(a, t), . . . , ˆw1(a, t)can be proved
asR-linear independent polynomials in a∈R. Then one can apply the following theorem to boost
the linear independence of ˆwi, whose constant term is zero since initialization in training is set as
ρ0=ρa×δRd, to the algebraic independence of ˆui.
Theorem 4.5. Letv1, v2, . . . , v p∈R[a]beR-linearly independent polynomials with the constant
terms being zero. Then1
p(v1(a1)+···+v1(ap)), . . . ,1
p(vp(a1)+···+vp(ap))∈R[a1, a2, . . . , a p]
areR-algebraically independent.
The proof of Theorem 4.5 is deferred to Appendix C.2 and is based on the celebrated Jacobian
criterion stated as follows.
Theorem 4.6 (Jacobian criterion [ 7]).v1, v2, . . . , v p∈R[a1, a2, . . . , a p]areR-algebraically inde-
pendent if and only if
det
∂v1
∂a1∂v1
∂a2. . .∂v1
∂ap
∂v2
∂a1∂v2
∂a2. . .∂v2
∂ap............
∂vp
∂a1∂vp
∂a2. . .∂vp
∂ap

is a nonzero polynomial in R[a1, a2, . . . , a p].
Non-degeneracy of ˆM(a, t).With the observation that span
ˆσ(q⊤z) :q∈V	
=PV,n(see
Lemma C.13), we define another matrix X(q)∈R(n+p
p)×(n+p
p)via
Xi1,i2(q) =Ez
pi1(z)σ(q⊤
i2z)
,
where q=
q1,q2, . . . , q(n+p
p)
withqi∈V, and prove that det(X(q))is a non-zero polynomial
inqof the form
det(X(q)) =(n+p
p)X
i=1X
0≤∥ji∥1≤nXjqj=(n+p
p)X
i=1X
0≤∥ji∥1≤nXj(n+p
p)Y
l=1qjl
l,
where qiis understood as a (coefficient) vector in Rpassociated with a fixed orthonormal basis of V
andj=
j1,j2, . . . , j(n+p
p)
withji∈Np. Then setting q= ˆu(ai2, t)leads to
det(ˆM(a, t)) =(n+p
p)X
i=1X
0≤∥ji∥1≤nXj(n+p
p)Y
l=1ˆu(al, t)jl.
To prove that det(ˆM(a, t))is a non-zero polynomial in a, i.e., ˆM(a, t)is non-degenerate, we use the
following lemma linking algebraic independence back to linear independence.
9Lemma 4.7. Suppose that v1, v2, . . . , v p∈R[a1, a2, . . . , a p]areR-algebraically independent. For
anym≥1, the following polynomials in a= (a1,a2, . . . , am)∈(Rp)mareR-linearly independent
mY
l=1v(al)jl,1≤ ∥ji∥1≤n,1≤i≤m,
where v= (v1, v2, . . . , v p).
The proof of Lemma 4.7 and some other related analysis are deferred to Appendix C.3.
5 Conclusion and Discussions
In this work, we generalize the merged-staircase property in [ 1] to a basis-free version and establish a
necessary condition for learning a subspace-sparse polynomial on Gaussian input with arbitrarily
small error. Moreover, we prove the exponential decay property of the loss functional under a
sufficient condition that is slightly stronger than the necessary one. The bounds and rates are all
dimension-free. Our work provides some understanding of the mean-field dynamics, though its
general behavior is extremely difficult to characterize due to the non-convexity of the loss functional.
Let us also make some comments on limitations and future directions. Firstly, there is still a gap
between the necessary condition and the sufficient condition, which is basically from the fact that the
sufficient condition is built on the Taylor’s expansion of the flow (4.1) . One future research question
is whether we can fill the gap by considering the original flow (4.1) rather than its Taylor’s expansion.
Secondly, Algorithm 1 repeats training wforptimes and takes the average of parameters, which
is different from the usual strategy for training neural networks. This step is used to guarantee the
algebraic independence. We conjecture that this step can be removed since the general algebraic
independence is too strong when we have some preknowledge on the degree of f∗orh∗, which
deserves future research.
Acknowledgments and Disclosure of Funding
The work of R. Ge is supported by NSF Award DMS-2031849 and CCF-1845171 (CAREER). We
thank Joan Bruna for helpful comments and discussion.
References
[1]Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property:
a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer
neural networks. In Conference on Learning Theory , pages 4782–4887. PMLR, 2022.
[2]Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. SGD learning on neural net-
works: leap complexity and saddle-to-saddle dynamics. In The Thirty Sixth Annual Conference
on Learning Theory , pages 2552–2623. PMLR, 2023.
[3]Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj.
The staircase property: How hierarchical structure can guide deep learning. Advances in Neural
Information Processing Systems , 34:26989–27002, 2021.
[4]Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. In International conference on machine learning , pages 242–252. PMLR,
2019.
[5]Dyego Araújo, Roberto I Oliveira, and Daniel Yukimura. A mean-field limit for certain deep
neural networks. arXiv preprint arXiv:1906.00193 , 2019.
[6]Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan. Repetita
iuvant: Data repetition allows SGD to learn high-dimensional multi-index functions. arXiv
preprint arXiv:2405.15459 , 2024.
[7]Malte Beecken, Johannes Mittmann, and Nitin Saxena. Algebraic independence and blackbox
identity testing. Information and Computation , 222:2–19, 2013.
10[8]Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning Gaussian multi-index
models with gradient flow. arXiv preprint arXiv:2310.19793 , 2023.
[9]Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. Advances in neural information processing
systems , 31, 2018.
[10] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. How
two-layer neural networks learn, one (giant) step at a time. In NeurIPS 2023 Workshop on
Mathematics of Modern Machine Learning , 2023.
[11] Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborova, and Florent
Krzakala. The benefits of reusing batches for gradient descent in two-layer networks: Breaking
the curse of information and leap exponents. In Forty-first International Conference on Machine
Learning , 2024.
[12] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations ,
2018.
[13] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems , 31,
2018.
[14] Kenji Kawaguchi. Deep learning without poor local minima. Advances in neural information
processing systems , 29, 2016.
[15] Kenji Kawaguchi and Yoshua Bengio. Depth with nonlinearity creates no bad local minima in
resnets. Neural Networks , 118:167–174, 2019.
[16] Jason D Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns low-
dimensional polynomials with SGD near the information-theoretic limit. arXiv preprint
arXiv:2406.01581 , 2024.
[17] Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint
arXiv:1702.08580 , 2017.
[18] Arvind Mahankali, Haochen Zhang, Kefan Dong, Margalit Glasgow, and Tengyu Ma. Beyond
NTK with vanilla gradient descent: A mean-field analysis of neural networks with polynomial
width, samples, and time. Advances in Neural Information Processing Systems , 36, 2024.
[19] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers
neural networks: dimension-free bounds and kernel limit. In Conference on Learning Theory ,
pages 2388–2464. PMLR, 2019.
[20] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of
two-layer neural networks. Proceedings of the National Academy of Sciences , 115(33):E7665–
E7671, 2018.
[21] Phan-Minh Nguyen. Mean field limit of the learning dynamics of multilayer neural networks.
arXiv preprint arXiv:1902.02880 , 2019.
[22] Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural networks:
An interacting particle system approach. Communications on Pure and Applied Mathematics ,
75(9):1889–1935, 2022.
[23] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural
networks. In International conference on machine learning , pages 4433–4441. PMLR, 2018.
[24] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A
central limit theorem. Stochastic Processes and their Applications , 130(3):1820–1852, 2020.
[25] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law
of large numbers. SIAM Journal on Applied Mathematics , 80(2):725–752, 2020.
11[26] Taiji Suzuki, Denny Wu, Kazusato Oko, and Atsushi Nitanda. Feature learning via mean-field
langevin dynamics: classifying sparse parities and beyond. Advances in Neural Information
Processing Systems , 36, 2024.
[27] Alain-Sol Sznitman. Topics in propagation of chaos. Lecture notes in mathematics , pages
165–251, 1991.
[28] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under-
standing neural networks. Advances in Neural Information Processing Systems , 32, 2019.
12A Proofs for Section 3
This section collects the proofs of Theorem 3.5, Theorem 3.6, and Theorem 3.4.
A.1 Proof of Theorem 3.5
Existence and uniqueness of solutions to (2.4) Before proving Theorem 3.5, let us remark on the
existence and uniqueness of solution to the mean-field dynamics (2.4) . According to Remark 7.1 in
[20] and Theorem 1.1 in [ 27], the PDE (2.4) admits a unique solution if Assumption 3.1 (ii) holds
and both ∇V(θ)and∇θU(θ, θ′)are bounded and Lipschitz continous. Here we recall that VandU
are defined in (2.5) . With Assumption 3.1 (i) and (iii), it is not hard to verify the boundedness and
Lipschitz continuity of ∇V(θ)and∇θU(θ, θ′)by noticing that any finite-order moment of N(0, Id)
is finite.
Proof of Theorem 3.5. Since (PS)#ρ0=δS, the initial distribution ρ0can be decomposed as ρ0=
ˆρ0×δS, where ˆρ0∈ P(R×S⊥). Consider the following evolution equation (3.7) inP(R×S⊥).
By the discussion at the beginning of Section A.1, we know that ρtis the unique solution to (2.4) .
Similar arguments also leads to the existence and uniquess of the solution to (3.7).
We will show that the solution to (2.4) must be of the form
ρt= ˆρt×δS, (A.1)
where ˆρtsolves (3.7) , and this decomposition can immediatel imply (3.6) . By the uniquess of the
solution, it suffices to verify that ρt= ˆρt×δSis a solution to (2.4) . It follows directly from (A.1)
that
fNN(x;ρt) =fNN(x; ˆρt×δS) =ˆfNN(x⊥
S; ˆρt),
and hence that
∂aΦ(θ;ρt) =∂aˆΦ(ˆθ; ˆρt),ifwS= 0. (A.2)
We also have that
∇w⊥
SΦ(θ;ρt) =aEx
(fNN(x;ρt)−f∗(x))σ′(w⊤x)x⊥
S
=aExh
ˆfNN(x⊥
S; ˆρt)−f∗(x)
σ′ 
(w⊥
S)⊤x⊥
S
x⊥
Si
=∇w⊥
SˆΦ(ˆθ; ˆρt),(A.3)
ifwS= 0. In addition, it holds also for wS= 0that
∇wSΦ(θ;ρt) =aEx
(fNN(x;ρt)−f∗(x))σ′(w⊤x)xS
=aExh
ˆfNN(x⊥
S; ˆρt)σ′ 
(w⊥
S)⊤x⊥
S
xSi
−aEx
f∗(x)σ′ 
(w⊥
S)⊤x⊥
S
xS
= 0,(A.4)
where we used ExS[xS] = 0 and(3.5) . Combining (A.2) ,(A.3) , and (A.4) , we have for any
η∈ C∞
c(Rd+1×(0,+∞)) =C∞
c(R×S⊥×S×(0,+∞))thatZZ
(−∂tη+∇θη·(ξ(t)∇θΦ(θ;ρt)))ρt(dθ)dt
=ZZZ
−∂tη(θ, t) +ξa(t)∂aη(θ, t)·∂aΦ(θ;ρt) +ξw(t)∇w⊥
Sη(θ, t)· ∇w⊥
SΦ(θ;ρt)
+ξw(t)∇wSη(θ, t)· ∇wSΦ(θ;ρt)
ˆρt(dˆθ)δS(dwS)dt
=ZZ
−∂tη(ˆθ,0, t) +ξa(t)∂aη(ˆθ,0, t)·∂aˆΦ(ˆθ; ˆρt)
+ξw(t)∇w⊥
Sη(ˆθ,0, t)· ∇w⊥
SˆΦ(ˆθ; ˆρt)
ˆρt(dˆθ)dt
=ZZ
−∂tη(ˆθ,0, t) +∇ˆθη(ˆθ,0, t)·(ˆξ(t)∇ˆθˆΦ(ˆθ; ˆρt))
ˆρt(dˆθ)dt
=0,
where the last equality holds by applying the test function η(·,0,·)∈ C∞
c(R×S⊥×(0,+∞))to
(3.7). The proof is hence completed.
13A.2 Proof of Theorem 3.6
The proof of Theorem 3.6 uses some ideas from the proof of Theorem 16 in [ 1]. Similar ideas also
exist in earlier works (see e.g., [19, 20]).
Lemma A.1. Suppose that Assumption 3.1 holds and let ρtsolve (2.4) . Then for any t >0,ρtis
supported in
θ= (a, w)∈Rd+1:|a| ≤Kρ+KξKσEz
|h∗(z)|21/2t	
.
Proof. The particle dynamics for atassociated with (2.4) is given by
d
dtat=ξa(t)Ex
(fNN(x;ρt)−f∗(x))σ(w⊤
tx)
,
which implies that
d
dtat≤KξKσ|Ex[fNN(x;ρt)−f∗(x)]| ≤KξKσ(2E(ρ0))1/2=KξKσEz
|h∗(z)|21/2.
Therefore, one has |at| ≤ |a0|+KξKσEz
|h∗(z)|21/2t, which completes the proof.
Lemma A.2. Suppose that Assumption 3.1 holds for both ρ0and˜ρ0. Let ρtsolve ∂tρt=∇θ·
(ρtξ(t)∇θΦ(θ;ρt))and let ˜ρtsolve ∂t˜ρt=∇θ·(˜ρtξ(t)∇θΦ(θ; ˜ρt)). For any coupling γ0∈
Γ(ρ0,˜ρ0), letγt∈Γ(ρt,˜ρt)be the associated coupling during the evolution and define
∆(t) =ZZ 
|a−˜a|2+∥w−˜w∥2
γt(dθ, d˜θ).
Then it holds for any 0≤t≤Tthat
Ex
|fNN(x;ρt)−fNN(x; ˜ρt)|2
≤Cf∆(t), (A.5)
and
d
dt∆(t)≤C∆∆(t), (A.6)
where CfandC∆are constants depending only on p,h∗,Kσ,Kξ,Kρ, and T.
Proof of Theorem 3.6. LetCfandC∆be the constants in Lemma A.2. For any ϵ >0, there exists a
coupling γ0∈Γ(ρ0,˜ρ0)such that
ZZ
(|a−˜a|2+∥w−˜w∥2)γ0(dθ, d˜θ)≤W2
2(ρ0,˜ρ0) +ϵ.
Define γt∈Γ(ρt,˜ρt)and∆(t)as in Lemma A.2. According (A.6) and the Grönwall’s inequality, it
holds that
∆(t)≤∆(0)eC∆t≤ 
W2
2(ρ0,˜ρ0) +ϵ
eC∆t,∀0≤t≤T,
which combined with (A.5) yields that
sup
0≤t≤TEx
|fNN(x;ρt)−fNN(x; ˜ρt)|2
≤ 
W2
2(ρ0,˜ρ0) +ϵ
CfeC∆T.
Then we can conclude (3.8) be setting ϵ→0andCs=CfeC∆T.
Corollary A.3. Under the same setting as in Theorem 3.6, one has
sup
0≤t≤T|E(ρt)− E(˜ρt)| ≤ 
CsEz
|h∗(z)|21/2W2(ρ0,˜ρ0) +1
2CsW2
2(ρ0,˜ρ0). (A.7)
Proof. It can be computed that
E(ρt) =1
2Ex
|(fNN(x; ˜ρt)−f∗(x)) + ( fNN(x;ρt)−fNN(x; ˜ρt))|2
=E(˜ρt) +Ex[(fNN(x; ˜ρt)−f∗(x))(fNN(x;ρt)−fNN(x; ˜ρt))]
+1
2Ex
|fNN(x;ρt)−fNN(x; ˜ρt)|2
,
14which implies that
sup
0≤t≤T|E(ρt)− E(˜ρt)| ≤Ex
|fNN(x; ˜ρt)−f∗(x)|21/2Ex
|fNN(x;ρt)−fNN(x; ˜ρt)|21/2
+1
2Ex
|fNN(x;ρt)−fNN(x; ˜ρt)|2
≤ 
CsExV
|h∗(xV)|21/2W2(ρ0,˜ρ0) +1
2CsW2
2(ρ0,˜ρ0),
where we used Theorem 3.6 and Remark 3.2.
Proof of Lemma A.2. We first prove (A.5). It can be computed that
|fNN(x;ρt)−fNN(x; ˜ρt)|=Z
aσ(w⊤x)ρt(dθ)−Z
˜aσ( ˜w⊤x)˜ρt(d˜θ)
≤ZZ
(a−˜a)σ(w⊤x)γt(dθ, d˜θ)+ZZ
˜a 
σ(w⊤x)−σ( ˜w⊤x)
γt(dθ, d˜θ),
and hence that
Exh
|fNN(x;ρt)−fNN(x; ˜ρt)|2i
≤2Ex"ZZ
(a−˜a)σ(w⊤x)γt(dθ, d˜θ)2#
+ 2Ex"ZZ
˜a 
σ(w⊤x)−σ( ˜w⊤x)
γt(dθ, d˜θ)2#
.
We then bound the two terms above as follows:
Ex"ZZ
(a−˜a)σ(w⊤x)γt(dθ, d˜θ)2#
≤K2
σZZ
|a−˜a|2γt(dθ, d˜θ)≤K2
σ∆(t),
and
Ex"ZZ
˜a 
σ(w⊤x)−σ( ˜w⊤x)
γt(dθ, d˜θ)2#
≤K2
σ
Kρ+√
2KξKσE(ρ0)1/2T2
Ex"ZZ
|(w−˜w)⊤x|γt(dθ, d˜θ)2#
≤K2
σ
Kρ+KξKσEz
|h∗(z)|21/2T2ZZ
Ex
|(w−˜w)⊤x|2
γt(dθ, d˜θ)
=K2
σ
Kρ+KξKσEz
|h∗(z)|21/2T2ZZ
∥w−˜w∥2γt(dθ, d˜θ)
≤K2
σ
Kρ+KξKσEz
|h∗(z)|21/2T2
∆(t),
where we used Lemma A.1 and (w−˜w)⊤x∼ N (0,∥w−˜w∥2)ifx∼ N (0, Id). Then we
can conclude (A.5) withCf=K2
σ+K2
σ
Kρ+KξKσEz
|h∗(z)|21/2T2
by combining all
estimations above.
We then head into the proof of (A.6), for which we need the particle dynamics
d
dtat=ξa(t)Ex
(fNN(x;ρt)−f∗(x))σ(w⊤
tx)
,
d
dt˜at=ξa(t)Ex
(fNN(x; ˜ρt)−f∗(x))σ( ˜w⊤
tx)
,
and d
dtwt=ξwatEx
(fNN(x;ρt)−f∗(x))σ′(w⊤
tx)x
,
d
dt˜wt=ξw˜atEx
(fNN(x; ˜ρt)−f∗(x))σ′( ˜w⊤
tx)x
.
The distance between the particle dynamics can be decomposed asd
dtat−d
dt˜at≤KξEx
(fNN(x;ρt)−fNN(x; ˜ρt))σ(w⊤
tx)
+KξEx
(fNN(x; ˜ρt)−f∗(x))(σ(w⊤
tx)−σ( ˜w⊤
tx)),(A.8)
15and
wt−˜wt,d
dtwt−d
dt˜wt
≤Kξ(at−˜at)Ex
(fNN(x;ρt)−f∗(x))σ′(w⊤
tx)(wt−˜wt)⊤x
+Kξ˜atEx
(fNN(x;ρt)−fNN(x; ˜ρt))σ′(w⊤
tx)(wt−˜wt)⊤x
+Kξ˜atEx
(fNN(x; ˜ρt)−f∗(x)) 
σ′(w⊤
tx)−σ′( ˜w⊤
tx)
(wt−˜wt)⊤x.(A.9)
Therefore, it can be computed that
d
dtZZ
|a−˜a|2γt(dθ, d˜θ)
=d
dtZZ
|at−˜at|2γ0(dθ0, d˜θ0)
= 2ZZ
(at−˜at)d
dtat−d
dt˜at
γ0(dθ0, d˜θ0)
≤ZZ
|at−˜at|2γ0(dθ0, d˜θ0) +ZZd
dtat−d
dt˜at2
γ0(dθ0, d˜θ0)
(A.8)
≤∆(t) + 2K2
ξK2
σZZ
Ex[|fNN(x;ρt)−fNN(x; ˜ρt)|]2γ0(dθ0, d˜θ0)
+ 2K2
ξK2
σZZ
Ex
|fNN(x; ˜ρt)−f∗(x)| ·(wt−˜wt)⊤x2γ0(dθ0, d˜θ0)
(A.5)
≤∆(t) + 2K2
ξK2
σCf∆(t)
+ 2K2
ξK2
σZZ
Ex
|fNN(x; ˜ρt)−f∗(x)| ·(wt−˜wt)⊤x2γ0(dθ0, d˜θ0)
≤∆(t) + 2K2
ξK2
σCf∆(t)
+ 2K2
ξK2
σZZ
Exh
|fNN(x; ˜ρt)−f∗(x)|2i
Exh(wt−˜wt)⊤x2i
γ0(dθ0, d˜θ0)
≤∆(t) + 2K2
ξK2
σCf∆(t) + 2K2
ξK2
σEz[|h∗(z)|2]ZZ
∥wt−˜wt∥2γ0(dθ0, d˜θ0)
≤ 
1 + 2 K2
ξK2
σCf+ 2K2
ξK2
σEz[|h∗(z)|2]
∆(t),
where we used Remark 3.2, and that
d
dtZZ
∥w−˜w∥2γt(dθ, d˜θ)
=d
dtZZ
∥wt−˜wt∥2γ0(dθ0, d˜θ0)
= 2ZZ
wt−˜wt,d
dtwt−d
dt˜wt
γ0(dθ0, d˜θ0)
(A.9)
≤2KξKσZZ
|at−˜at| ·Ex
|fNN(x;ρt)−f∗(x)| ·(wt−˜wt)⊤x
γ0(dθ0, d˜θ0)
+ 2KξKσZ Z
|˜at| ·Ex
|fNN(x;ρt)−fNN(x; ˜ρt))| ·(wt−˜wt)⊤x
γ0(dθ0, d˜θ0)
+ 2KξKσZZ
|˜at| ·Exh
|fNN(x; ˜ρt)−f∗(x)| ·(wt−˜wt)⊤x2i
γ0(dθ0, d˜θ0)
≤2KξKσZZ
|at−˜at| ·Ez
|h∗(z)|21/2Exh(wt−˜wt)⊤x2i1/2
γ0(dθ0, d˜θ0)
+ 2KξKσZ Z
|˜at| ·(Cf∆(t))1/2Exh(wt−˜wt)⊤x2i1/2
γ0(dθ0, d˜θ0)
+ 2KξKσZZ
|˜at| ·Ez
|h∗(z)|21/2Exh(wt−˜wt)⊤x4i1/2
γ0(dθ0, d˜θ0)
16≤2KξKσEz
|h∗(z)|21/2ZZ
|at−˜at| · ∥wt−˜wt∥γ0(dθ0, d˜θ0)
+ 2KξKσ
Kρ+KξKσEz
|h∗(z)|21/2T
(Cf∆(t))1/2Z Z
∥wt−˜wt∥γ0(dθ0, d˜θ0)
+ 2KξKσ
Kρ+KξKσEz
|h∗(z)|21/2T
Ez
|h∗(z)|21/2
·ZZ√
3∥wt−˜wt∥2γ0(dθ0, d˜θ0)
≤KξKσEz
|h∗(z)|21/2∆(t) + 2KξKσ
Kρ+KξKσEz
|h∗(z)|21/2T
C1/2
f∆(t)
+ 2√
3KξKσ
Kρ+KξKσEz
|h∗(z)|21/2T
Ez
|h∗(z)|21/2∆(t),
where we used Remark 3.2, (A.5) , Lemma A.1, and (w−˜w)⊤x∼ N(0,∥w−˜w∥2)ifx∼ N(0, Id).
Therefore, we can conclude that
d
dt∆(t) =d
dtZZ
|a−˜a|2γt(dθ, d˜θ) +d
dtZZ
∥w−˜w∥2γt(dθ, d˜θ)≤C∆∆(t),
where
C∆=1 + 2 K2
ξK2
σCf+ 2K2
ξK2
σExV[|h∗(xV)|2]
+KξKσEz
|h∗(z)|21/2+ 2KξKσ
Kρ+KξKσEz
|h∗(z)|21/2T
C1/2
f
+ 2√
3KξKσ
Kρ+KξKσEz
|h∗(z)|21/2T
Ez
|h∗(z)|21/2.
This proves (A.6).
A.3 Proof of Theorem 3.4
The proof of Theorem 3.4 is based on Theorem 3.5 and Theorem 3.6.
Proof of Theorem 3.4. LetPS:Rd+1→Sbe the projection in Theorem 3.5 and let P⊥
S=Id+1−
PS. Let ˜ρtsolve (2.4) with ˜ρ0=P⊥
Sρw×δS. It is clear that (PS)#˜ρ0=δS. In addition, with the
decomposition x=x1+x2+x3where x1=x⊥
V=x−xV,x2=xV−xS, and x3=xSare
independent Gaussian random variables, we have for any w∈Rdthat
Ex
f∗(x)σ′ 
w⊤x⊥
S
xS
=Ex1Ex2,x3
h∗(x2+x3)σ′ 
w⊤x1+w⊤x2
x3
= 0,
where we used (3.1) . Then according to Theorem 3.5, for any t≥0that(PS)#˜ρt=δS, which
implies that fNN(x;ρt)is a constant function in xSfor any fixed x⊥
S, giving a lowerbound on its loss:
E(˜ρt) =1
2Ex
∥f∗(x)−fNN(x; ˜ρt)∥2
=1
2Ex⊥
S
ExS
∥f∗(x)−fNN(x; ˜ρt)∥2
≥1
2Ex⊥
S
ExS
∥f∗(x)−ExS[f∗(x)]∥2
=1
2Ez⊥
S
EzS
∥h∗(z)−h∗
S⊥(z⊥
S)∥2
=1
2Ez
∥h∗(z)−h∗
S⊥(z⊥
S)∥2
,
where zS=PV
Sz,z⊥
S=z−zS, and h∗
S⊥(z⊥
S) =EzS[h∗(z)].
Now we show the actual flow is not very different. For ρ0=ρa×ρwwithρw∼ N(0, Id)and
˜ρ0=P⊥
Sρw×δS, it can be estimated that
W2
2(ρ0,˜ρ0)≤dimS
d≤p
d.
17Then applying Corollary A.3, we can conclude for any T >0that
inf
0≤t≤TE(˜ρt)≥inf
0≤t≤TE(˜ρt)− 
CsEz
|h∗(z)|21/2W2(ρ0,˜ρ0)−1
2CsW2
2(ρ0,˜ρ0)
≥1
2Ez
∥h∗(z)−h∗
S⊥(z⊥
S)∥2
− 
pCsEz
|h∗(z)|21/2
d1/2−pCs
2d,
where Cs>0is the constant in Theorem 3.6 depending only on p,h∗,Kσ,Kξ,Kρ, andT. Therefore,
we can obtain (3.3) and (3.4) immediately.
B Further Discussion and Characterization of the Reflective Property and
Theorem 3.4
B.1 Equivalence between (3.2) and isoLeap (h∗)≥2
We prove the following equivalence where isoLeap (h∗)is the isotropic leap complexity defined in [ 2,
Appendix B.2].
Proposition B.1. For any polynomial h∗:V→R, then it satisfies (3.2) with some nontrivial
subspace S⊂Vif and only if isoLeap (h∗)≥2.
Proof. Without loss of generality, we assume that V=Rp. Suppose that isoLeap (h∗)≥2, which
means that the leap complexity of h∗(as defined in [ 2, Definition 1]) is greater than one for some
orthonormal basis of V. We can assume that the basis is {e1, e2, . . . , e p}, where ejis the vector in
Rpwith the j-th entry being 1and other entries being 0. Denote the Hermite decomposition of h∗as
h∗(z) =mX
i=1cipY
j=1Heαi(j)(zj), (B.1)
where c1, c2, . . . , c mare nonzero coefficients, α1, α2, . . . , α mare pairwise distinct elements in
Npwithαi(j)being the j-th entry of αi, and Hekis the k-th order Hermite polynomial. Since
the leap complexity of h∗is at least two, the following is true after applying some permutation
on{1,2, . . . , m }: There exists some m1∈ {1,2, . . . , m −1}, such that it holds for any i∈
{m1+ 1, . . . , m }thatX
j∈Jm1αi(j)≥2, (B.2)
where
Jm1={j∈ {1,2, . . . , p }:αi(j) = 0 ,∀i∈ {1,2, . . . , m 1}}.
Thus, by the orthogonality of Hermite polynomials, we have for i∈ {m1+ 1, . . . , m }that
Ezj∼N(0,1)
zjpY
j=1Heαi(j)(zj)
= 0,∀j∈Jm1.
The above also holds for i∈ {1,2, . . . , m 1}by the definition of Jm1. Therefore, we can conclude
that
EzS∼N(0,IS)[h∗(z)zS] = 0,∀z⊥
S,
i.e.,(3.2) holds, for S=span{ej:j∈Jm1}. Moreover, Sis nontrivial since (B.2) implies Jm1is
not empty.
On the other hand, suppose that (3.2) is satisfied with some nontrivial subspace S⊂Vthat can be
assumed as S={(z1, . . . , z p1,0, . . . , 0) :z1, . . . , z p1∈R}with1≤p1≤p. We still consider the
Hermite decomposition as in (B.1) and rewrite it as
h∗(z) =m2X
i=1c′
ip1Y
j=1Heα′
i(j)(zj)hi(zp1+1, . . . , z p),
18where c′
1, c′
2, . . . , c′
m2are nonzero coefficients, α′
1, α′
2, . . . , α′
m2are pairwise distinct elements in
Np1, andh1, h2, . . . , h m2are nonzero polynomials defined on Rp−p′. Then it follows from (3.2) that
m2X
i=1c′
ihi(zp1+1, . . . , z p)Ezj∼N(0,1)
zjHeα′
i(j)(zj)p1Y
j′=1,j′̸=jEzj′∼N(0,1)Heα′
i(j′)(zj′) = 0 ,
for any j∈ {1,2, . . . , p 1}andzp1+1, . . . , z p∈R, which implies that
p1X
j=1αi(j)′̸= 1,∀i∈ {1,2, . . . , m 2}.
Therefore, the leap complexity of h∗is at least 2with respect to this basis, which leads to
isoLeap (h∗)≥2.
B.2 Discretization Results Implied by Theorem 3.4
We discuss the sample complexity result of SGD implied by Theorem 3.4 in this subsection. Recall
that there have been standard dimension-free results for bounding the distance between SGD and the
mean-field dynamics; see e.g., [ 19]. So the result in this subsection is somehow a direct corollary.
However, one needs to make minor modifications to guarantee that all boundedness assumptions in
[19] are satisfied.
Given a constant Cb
f>0, define
˜f∗(x) =sign(f∗(x)) min{|f∗(x), Cb
f|}, (B.3)
which is bounded with |˜f∗(x)| ≤Cb
f. One observation is that the subspace-sparse structure of f∗
implies that for any δ >0, there exists a dimension-free constant Cb
fdepending on h∗andδsuch
thatEx∼N(0,Id)[|f∗(x)−˜f∗(x)|2]< δ. The associated mean-field dynamics is
(
∂t˜ρt=∇θ·
˜ρtξ(t)∇θ˜Φ(θ; ˜ρt)
,
˜ρt
t=0=ρ0,(B.4)
where the learning rate ξ(t) =diag(ξa(t), ξw(t)Id)and the initialization ρ0are shared with (2.4) ,
and
˜Φ(θ;ρ) =aEx∼N(0,Id)h
fNN(x;ρ)−˜f∗(x)
σ(w⊤x)i
.
The corresponding SGD is given by
θ(k+1)
i =θ(k)
i+γ(k)
˜f∗(xk)−fNN(xk; Θ(k))
∇θτ(xk;θ(k)
i), i= 1,2, . . . , N, (B.5)
where Nis the number of neurons and γ(k)=diag(γ(k)
a, γ(k)
wId)⪰0is the learning rate with
γ(k)
a=ϵξa(kϵ)andγ(k)
w=ϵξw(kϵ).
Suppose that assumptions made in Theorem 3.4 hold and fix T > 0. Using similar analysis as in
Appendix A.2, one can conclude that for any δ >0, there exists a dimension-free constant Cb
fsuch
that
sup
0≤t≤T|E(ρt)− E(˜ρt)|< δ.
Applying Theorem 3.4 and [ 19, Theorem 1], we can conclude that for any µ∈(0,1), there exists
dimension-free constants N0, d0, Cϵ, such that for any N≥N0andd≥d0, the following holds with
probability at least µfor any ϵ≤Cϵ
d+log N:
inf
k∈[0,T/ϵ]∩NEN(Θ(k))≥1
8Ez∼N(0,IV)
|h∗(z)−h∗
S⊥(z⊥
S)|2
>0.
If we further assume that N=O(ed), this indicates that SGD as in (B.5) cannot learn the subspace-
sparse polynomial f∗within finite time horizon and with O(d)samples/data points.
19C Proofs for Section 4
C.1 Approximation of w(a, t)by Polynomials
This subsection follows [ 1] closely to approximate and analyze the behavior of w(a, t)for0≤t≤T
withξa(t) = 0 andξw(t) = 1 . The dynamics of a single particle starting at θ= (a,0)∈Rd+1can
be described by the following ODE:
∂
∂tw(a, t) =aEx
g(x, t)σ′(w(a, t)⊤x)x
,
w(a,0) = 0 ,(C.1)
where
g(x, t) =f∗(x)−fNN(x;ρt)
is the residual. The first observation is that
Ex
f∗(x)σ(w⊤xV)x⊥
V
=ExVh
h∗(xV)σ(w⊤xV)Ex⊥
V
x⊥
Vi
= 0,∀w∈Rd.
By Theorem 3.5, we have that ρtis supported in
(a, w) :w⊥
V= 0	
, and hence that
w⊥
V(a, t) = 0 ,∀0≤t≤T.
We then analyze the behaviour of wV(a, t). Let{e1, e2, . . . , e p}be an orthonormal basis of Vand
we denote wi=w⊤eiandxi=x⊤eifor any w, x∈Rdandi∈ {1,2, . . . , p }.
By Assumption 4.2, it holds that
σ′ 
w(a, t)⊤x
=m1+L−1X
l=1ml+1
l!(w(a, t)⊤x)l+O 
(w(a, t)⊤x)L
(C.2)
withml=σ(l)(0)and then an approximated solution to (C.1) (by polynomial expansion with
high-order terms omitted) can be written as
˜wi(a, t) =X
1≤j≤LQi,j(t)aj,1≤i≤p,
where Q(t)is given by Q(0) = 0 and the following dynamics:


d
dtQi,1(t) =Ex[xig(x, t)m1],
d
dtQi,j(t) =Ex
xig(x, t)L−1X
l=1ml+1
l!X
1≤i1,...,il≤pX
j1+···+jl=j−1lY
s=1Qis,js(t)xis
,2≤j≤L.
(C.3)
Let us remark that even if every single ˜wi(a, t)depends on the basis {e1, e2, . . . , e p}, the linear
combination
˜w(a, t) =pX
i=1ei˜wi(a, t) (C.4)
is basis-independent. To see this, let Qj(t)∈Rdbe the j-th column of Q(t)and it holds that


d
dtQ1(t) =Ex[xg(x, t)m1],
d
dtQj(t) =Ex
xg(x, t)L−1X
l=1ml+1
l!X
j1+···+jl=j−1lY
s=1x⊤Qjs(t)
,
The distance between wi(a, t)and˜wi(a, t)fori∈Ican be bounded as follows.
Proposition C.1. Suppose that Assumption 4.2 holds. Then
|wi(a, t)−˜wi(a, t)|=O((|a|t)L+1),∀i∈ {1,2, . . . , p }. (C.5)
We need the following two lemmas to prove Proposition C.1.
20Lemma C.2. For any i∈ {1,2. . . , p}andj∈ {1,2, . . . , L }, it holds that
Qi,j(t) =O(tj). (C.6)
Proof. The non-increasing property of the energy functional implies that
2E(ρt) =Ex[|g(x, t)|2]≤Ex[|f∗(x)−fNN(x, ρ0)|2] =Ez[|h∗(z)|2]<+∞.
Then (C.6) can be proved by induction. For j= 1, it follows from the boundedness of
d
dtQi,1(t)=|Ex[xig(x, t)m1]| ≤ |m1| 
Ex[x2
i]·Ex[|g(x, t)|2]1/2
thatQi,el(t) =O(t). Consider any 2≤j≤Land assume that Qi,j′=O(tj′)holds for any
1≤i≤pand1≤j′< j. Then one has that
d
dtQi,j(t)
≤
Ex[|g(x, t)|2]·Ex
xiL−1X
l=1ml+1
l!X
1≤i1,...,il≤pX
j1+···+jl=j−1lY
s=1Qis,js(t)xis2

1/2
=O(tj−1),
which implies that Qi,j(t) =O(tj).
Lemma C.3. Suppose that Assumption 4.2 holds. We have for all i∈ {1,2, . . . , p }that
wi(a, t) =O(|a|t). (C.7)
Proof. There exists a constant C > 0and a open subset A⊂ {w∈Rd:w⊥
V= 0}containing 0,
such that Ex
xig(x, t)σ′(w⊤x)≤C,∀w∈A, t≥0, i∈ {1,2, . . . , p }.
Thus, we have ∂
∂twi(a, t)≤ |a|,
as long as w(a, t)does not leave A. This implies (C.7).
Now we can proceed to prove Proposition C.1.
Proof of Proposition C.1. Set˜w⊥
V(a, t) = 0 . It can be estimated for any i∈ {1,2, . . . , p }that
∂
∂t˜wi(a, t)−aEx"
xig(x, t) 
m1+L−1X
l=1ml+1
l!( ˜w(a, t)⊤x)l!#
≤X
1≤j≤Ld
dtQi,j(t)aj−aEx
xig(x, t)
m1+L−1X
l=1ml+1
l!
X
1≤i′≤pX
1≤j≤LQi′,j(t)ajxi′
l


≤X
L+1≤j≤LL−1+1aj·Ex
xig(x, t)L−1X
l=1ml+1
l!X
1≤i1,...,il≤pX
j1+···+jl=j−1lY
s=1Qis,js(t)xis

≤X
L+1≤j≤LL−1+1|a|j
·
E[|g(x, t)|2]·Ex
xiL−1X
l=1ml+1
l!X
1≤i1,...,il≤pX
j1+···+jl=j−1lY
s=1Qis,js(t)xis2

1/2
21=O(|a|L+1tL),
which combined with (C.2) yields that
∂
∂tpX
i=1|wi(a, t)−˜wi(a, t)|
≤pX
i=1∂
∂twi(a, t)−∂
∂t˜wi(a, t)
≤pX
i=1aEx"
xig(x, t) 
m1+L−1X
l=1ml+1
l!(w(a, t)⊤x)l!#
−aEx"
xig(x, t) 
m1+L−1X
l=1ml+1
l!( ˜w(a, t)⊤x)l!#
+|aEx[xig(x, t)]| · O 
(w(a, t)⊤x)L
+O 
|a|L+1tL
≤pX
i=1Ex"
xia⊤g(x, t)L−1X
l=1ml+1
l! 
(w(a, t)⊤x)l−( ˜w(a, t)⊤x)l#+O 
|a|L+1tL
=O pX
i=1|wi(a, t)−˜wi(a, t)|!
+O 
|a|L+1tL
.
Then one can conclude (C.5) from Gronwall’s inequality.
Even if ˜w(a, t)approximates w(a, t)using polynomial expansion, the coefficients Q(t)are still very
difficult to analyze. Thus, we follow [ 1] to consider the following dynamics that is obtained by
replacing g(x, t) =f∗(x)−fNN(x;ρt)byf∗(x)in (C.3):
ˆwi(a, t) =X
1≤j≤LˆQi,j(t)aj, i∈ {1,2, . . . , p }, (C.8)
where ˆQ(t)is given by ˆQ(0) = 0 and


d
dtˆQi,1(t) =Ex[xif∗(x)m1],
d
dtˆQi,j(t) =Ex
xif∗(x)L−1X
l=1ml+1
l!X
1≤i1,...,il≤pX
j1+···+jl=j−1lY
s=1ˆQis,js(t)xis
,2≤j≤L.
(C.9)
Similar to ˜w(a, t), the linear combination ˆw(a, t)defined as
ˆw(a, t) =pX
i=1eiˆwi(a, t)
is also independent of the orthogonal basis {e1, e2, . . . , e p}.ˆQ(t)can be understood clearly as
follows.
Proposition C.4. For any i∈ {1,2, . . . , p }andj∈ {1,2, . . . , L }, there exists a constant ˆqi,j
depending only on h∗andσ, such that
ˆQi,j(t) = ˆqi,jtj. (C.10)
Proof. The proof is straightforward by induction on j.
The next proposition quantifies the distance between ˆQ(t)andQ(t).
Proposition C.5. It holds for any i∈ {1,2, . . . , p }andj∈ {1,2, . . . , L }that
|Qi,j(t)−ˆQi,j(t)|=O(tj+1). (C.11)
22We need the following lemma for the proof of Proposition C.5.
Lemma C.6. It holds that 
Ex
|fNN(x;ρt)|21/2=O(t). (C.12)
Proof. Noticing that fNN(x;ρ0) = 0 by the symmetry of ρa, one has that
fNN(x;ρt) =fNN(x;ρt)−fNN(x;ρ0)
=Z
a 
σ(w(a, t)⊤x)−σ(0)
ρa(da)
=Z
a LX
l=1ml
l!(w(a, t)⊤x)l+O 
(w(a, t)⊤x)L+1!
ρa(da),
and hence by Lemma C.3 and w⊥
V(a, t) = 0 that
Ex
|fNN(x, ρt)|2
=O(t2),
which implies (C.12).
Proof of Proposition C.5. We prove (C.11) by introduction on j. For j= 1, one has
d
dtQi,1(t)−d
dtˆQi,1(t)=|Ex[xifNN(x, ρt)m1]|=|m1| 
Ex[x2
i]·Ex[|fNN(x, ρt)|2]1/2=O(t),
which leads to |Qi,el(t)−ˆQi,el(t)|=O(t2). Then we consider 2≤j≤Land assume that
|Qi,j′(t)−ˆQi,j′(t)|=O(tj′+1)holds for 1≤j′< j. It can be estimated that
d
dtQi,j(t)−d
dtˆQi,j(t)
=Ex
xig(x, t)L−1X
l=1ml+1
l!X
1≤i1,...,il≤pX
j1+···+jl=j−1lY
s=1Qis,js(t)xis

−Ex
xif∗(x)L−1X
l=1ml+1
l!X
1≤i1,...,il≤pX
j1+···+jl=j−1lY
s=1ˆQis,js(t)xis

=Ex
xifNN(x;ρt)L−1X
l=1ml+1
l!X
1≤i1,...,il≤pX
j1+···+jl=j−1lY
s=1Qis,js(t)xis

−Ex
xif∗(x)L−1X
l=1ml+1
l!X
1≤i1,...,il≤pX
j1+···+jl=j−1 lY
s=1Qis,js(t)−lY
s=1ˆQis,js(t)!
xis

=Ex
xifNN(x, ρt)L−1X
l=1ml+1
l!X
1≤i1,...,il≤pX
j1+···+jl=j−1lY
s=1O(tjs)xis

−Ex
xif∗(x)L−1X
l=1ml+1
l!X
1≤i1,...,il≤pX
j1+···+jl=j−1
 lY
s=1 
ˆqis,jstjs+O(tjs+1)
−lY
s=1ˆqis,jstjs!
xis#
=O(tj),
where we used Lemma C.6. Then one can conclude (C.11).
23C.2 From Linear Independence to Algebraic Independence
We prove Theorem 4.5 in this subsection.
Definition C.7 (Algebraic independence) .Letv1, v2, . . . , v m∈R[a1, a2, . . . , a p]be polynomials
ina1, a2, . . . , a p. We say that v1, v2, . . . , v mare algebraically independent if for any nonzero
polynomial F:Rm→R,
F(v1(a1, a2, . . . , a p), . . . , v m(a1, a2, . . . , a p))̸= 0∈R[a1, a2, . . . , a p].
Lemma C.8. Ifv1, v2, . . . , v p∈R[a]areR-linearly independent, then
det
v1(a1)v1(a2)··· v1(ap)
v2(a1)v2(a2)··· v2(ap)
............
vp(a1)vp(a2)··· vp(ap)
(C.13)
is a non-zero polynomial in R[a1, a2, . . . , a p].
Proof. Letnibe the smallest degree of nonzero monomials of viand let cibe the accociated
coefficient for i= 1,2, . . . , p . Without loss of generality, we assume that n0< n 1<···< np
(otherwise one can perform some row reductions or row permutations). The polynomial defined in
(C.13) consists of monomials of degree at least n1+n2+···+np. So it suffices to prove that the
sum of monomials with degree being n1+n2+···+npis nonzero, which is true since
det
c1an1
1c1an1
2··· c1an1p
c2an2
1c2an2
2··· c2an2p
............
cpanp
1cpanp
2··· cpanpp
=c1c2. . . c p·det
an1
1an1
2··· an1p
an2
1an2
2··· an2p
............
anp
1anp
2··· anpp

is nonzero as a generalized Vandermonde matrix.
Proof of Theorem 4.5. Since v1, v2, . . . , v p∈R[a]beR-linearly independent with the constant terms
being zero, we can see that v′
1, v′
2, . . . , v′
p∈R[a]are also R-linearly independent. Noticing that
∂
∂aj1
p(vi(a1) +vi(a2) +···+vi(ap))
=1
pv′
i(aj),
one can conclude that1
p(v1(a1) +···+v1(ap)), . . . ,1
p(vp(a1) +···+vp(ap))∈R[a1, . . . , a p]are
R-algebraically independent by using Theorem 4.6 and Lemma C.8.
C.3 Proofs of Proposition 4.4 and Theorem 4.3
Some ideas in this subsection are from [ 1], but the proofs are significantly different since we need to
show the algebraic independence to obtain a non-degenerate kernel, as discussed in Section 4.
Lemma C.9. Suppose that Assumption 4.1 holds with s∈N+. There exists some orthonormal basis
{e1, e2, . . . , e p}ofVsuch that the coefficients ˆqi,j,1≤i≤p,1≤j≤Lin(C.10) satisfies
s1< s2<···< sp≤s, (C.14)
where
si= min {j: ˆqi,j̸= 0}, i= 1,2, . . . , p.
Proof. LetˆwV(t)be the dynamics defined in (4.1) . It can be seen that the Taylor’s expansion of
ˆwV(t)att= 0up to s-th order is given by
pX
i=1sX
j=1eiˆQi,j(t) =pX
i=1sX
j=1eiˆqi,jtj,
where ˆQi,jandˆqi,jare as in (C.9) and (C.5) . According to Assumption 4.1, the matrix
(ˆqi,j)1≤i≤p,1≤j≤sis of full-row-rank. One can thus perform the QR decomposition, or equivalently
choose some orthogonal basis, to obtain (C.14).
24In the rest of this subsection, we will always denote s= (s1, s2, . . . , s P)and
u(a1, . . . , a p, t) =1
p(w(a1, t) +w(a2, t) +···+w(ap, t)),
˜u(a1, . . . , a p, t) =1
p( ˜w(a1, t) + ˜w(a2, t) +···+ ˜w(ap, t)),
ˆu(a1, . . . , a p, t) =1
p( ˆw(a1, t) + ˆw(a2, t) +···+ ˆw(ap, t)),
fort∈[0, T], where w(a, t),˜w(a, t), and ˆw(a, t)are defined in (C.1) ,(C.4) , and (C.8) , respectively.
Recall that p1, p2, . . . , p(n+p
p)are the orthonormal basis of PV,nwith input z∼ N(0, IV), where
PV,nis the collection of all polynomials on Vwith degree at most n=deg(h∗) = deg(f∗).
Proposition 4.4 aims to bound from below the smallest eigenvalue of the kernel matrix (4.2) whose
definition is restated as follows
Ki1,i2(t) =Ea1,...,a p
Ez,z′
pi1(z)ˆσ(u(a1, . . . , a p, t)⊤z)ˆσ(u(a1, . . . , a p, t)⊤z′)pi2(z′)
,
where ˆσ(ξ) = (1 + ξ)n,(a1, . . . , a p)∼ U([−1,1]p), and 1≤i1, i2≤ n+p
p
. To do this, we define
three n+p
p
× n+p
p
matrices
˜Ki1,i2(t) =Ea1,...,a p
Ez,z′
pi1(z)ˆσ(˜u(a1, . . . , a p, t)⊤z)ˆσ(˜u(a1, . . . , a p, t)⊤z′)pi2(z′)
,
and
˜Mi1,i2(a, t) =Ez
pi1(z)ˆσ(˜u(ai2, t)⊤z)
,
ˆMi1,i2(a, t) =Ez
pi1(z)ˆσ(ˆu(ai2, t)⊤z)
,
where a=
a1,a2, . . . , a(n+p
p)
andai∈Rpfori= 1,2, . . . , n+p
p
.
Lemma C.10. It holds that
det(ˆM(a, t)) =(n+p
p)X
i=1X
0≤∥ji∥1≤Lnˆhjt∥j∥1aj, (C.15)
where j=
j1,j2, . . . , j(n+p
p)
withji∈Npfori= 1,2, . . . , n+p
p
,ˆhjis a constant depending on
h∗andσ, and ajrepresents the product of entrywise powers.
Proof. The result follows directly from Proposition C.4.
Lemma C.11. It holds that
det(˜M(a, t)) =(n+p
p)X
i=1X
0≤∥ji∥1≤Ln˜hj(t)aj,
with
˜hj(t) =ˆhjt∥j∥1+O(t∥j∥1+1).
Proof. The result follows directly from Proposition C.4 and Proposition C.5.
Lemma C.12. For any m∈N, one has that
span
(q⊤z)m:q∈V	
=Ph
V,m, (C.16)
where Ph
V,mis the collection of all homogeneous polynomials in z∈Vwith degree m.
Proof. Without loss of generality, we assume that V=Rpand prove the result by induction on p.
(C.16) is clearly true for p= 1. Then we consider p≥2and assume that (C.16) holds for p−1and
anym.
25For any q, z∈Rp, we denote that ¯q= (q2, . . . , q p)and¯z= (z2, . . . , z p). Lett0, t1, . . . , t m∈Rbe
distinct. Then it follows from the invertibility of (tj
i)0≤i,j≤mand

(t0z1+ ¯q⊤¯z)m
(t1z1+ ¯q⊤¯z)m
...
(tnz1+ ¯q⊤¯z)m
=
Pm
i=0 m
i
ti
0zi
1(¯q⊤¯z)m−i
Pm
i=0 m
i
ti
1zi
1(¯q⊤¯z)m−i
...Pm
i=0 m
i
ti
mzi
1(¯q⊤¯z)m−i

=
1t0··· tm
0
1t1··· tm
1............
1tm··· tm
m

 m
0
(¯q⊤¯z)m m
1
z1(¯q⊤¯z)m−1
... m
m
zm
1

that
zi
1(¯q⊤¯z)m−i∈span
(r⊤z)m:r∈Rp	
,∀¯q∈Rp−1, i∈ {0,1, . . . , m }.
Then using the induction hypothesis that (C.16) is true for p−1andm−i,i= 0,1, . . . , m , one can
conclude that (C.16) is also true for pandm.
Lemma C.13. Forˆσ(ξ) = (1 + ξ)n, one has that
span
ˆσ(q⊤z) :q∈V	
=PV,n.
Proof. One can still assume that V=Rpwithout loss of generality. Consider any q∈Rpand any
distinct t0, t1, . . . , t n∈R. Then

ˆσ((t0q)⊤z)
ˆσ((t1q)⊤z)
...
ˆσ((tnq)⊤z)
=
Pn
i=0 n
i
ti
0(q⊤z)i
Pn
i=0 n
i
ti
1(q⊤z)i
...Pn
i=0 n
i
ti
0(q⊤z)i
=
1t0··· tn
0
1t1··· tn
1............
1tn··· tn
n

 n
0
 n
1
q⊤z
... n
n
(q⊤z)n
.
Note that the matrix (tj
i)0≤i,j≤nis invertible when t0, t1, . . . , t Nare distinct. Therefore, one can
conclude that
(q⊤z)i∈span
σ(r⊤z) :r∈Rp	
,∀q∈Rp,0≤i≤n,
which combined with Lemma C.12 implies that
PV,n⊃span
ˆσ(q⊤z) :q∈Rp	
⊃Ph
V,0⊕Ph
V,1⊕ ··· ⊕ Ph
V,n=PV,n,
which completes the proof.
Proof of Lemma 4.7. We prove the result by induction. When m= 1, the result follows directly from
theR-algebraic independence of v1, v2, . . . , v p. Now we assume that the result is true for m−1and
consider the case of m. Suppose that
0 =X
j1,...,jmXjmY
l=1v(al)jl=X
jm
X
j1...,jm−1Xjm−1Y
l=1v(al)jl
v(am)jm.
By the R-algebraic independence of v1, v2, . . . , v p, one must have
X
j1...,jm−1Xjm−1Y
l=1v(al)jl= 0,∀jm,
which then leads to Xj= 0,∀jby the induction hypothesis.
Lemma C.14. Suppose that Assumption 4.1 and 4.2 hold and let ˆhjbe the coefficient of det(ˆM(a, t))
in(C.15) . Then there exists some ˆj=
ˆj1,ˆj2, . . . ,ˆj(n+p
p)
with∥ˆj∥1≤sn n+p
p
, such that ˆhˆj̸= 0.
26Proof. LetX(q)∈R(n+p
p)×(n+p
p)be defined as
Xi1,i2(q) =Ez
pi1(z)σ(q⊤
i2z)
,
where q=
q1,q2, . . . , q(n+p
p)
withqi∈V. Then det(X(q))is a polynomial in qof the form
det(X(q)) =(n+p
p)X
i=1X
0≤∥ji∥1≤nXjqj, (C.17)
where we understand qias a (coefficient) vector in Rpassociated with a fixed orthonormal basis
{e1, e2, . . . , e p}ofVthat satisfies Lemma C.9. By Lemma C.13, there exists some qsuch that
σ(q⊤
1z), σ(q⊤
2z), . . . , σ (q⊤
(n+p
p)z)form a basis of PV,n, which implies that det(X(q))̸= 0for this
q. Thus, (C.17) is a non-zero polynomial in q. Let s= (s1, s2, . . . , s p)collect all indices sifrom
Lemma C.9. Denote
S= min

(n+p
p)X
l=1s⊤jl:Xj̸= 0,0≤ ∥ji∥ ≤n,1≤i≤n+p
p

,
and
JS=

j:(n+p
p)X
l=1s⊤jl=S, X j̸= 0,0≤ ∥ji∥ ≤n,1≤i≤n+p
p

.
Then we have that
det(ˆM(a, t)) = det
X
ˆu(a1, t),ˆu(a2, t), . . . , ˆu(a(n+p
p), t)
=(n+p
p)X
i=1X
0≤∥ji∥1≤nXj(n+p
p)Y
l=1ˆu(al, t)jl
=X
j∈JSXj(n+p
p)Y
l=1ˆus(al, t)jl+O(tS+1),
where ˆus(al, t) = (ˆu2,s1(al, t), . . . , ˆup,sp(al, t))andˆui,si(a1, . . . , a p, t) =1
pPp
k=1ˆqi,sitsiasi
kcol-
lects the leading order terms of ˆui(a1, . . . , a p, t). According to Assumption C.9, Theorem 4.5, and
Lemma 4.7, we have that
X
j∈JSXj(n+p
p)Y
l=1ˆus(al, t)jl̸= 0,
which provides at least one non-zero term in det(ˆM(a, t))whose degree in tis
S≤snn+p
p
.
This completes the proof.
Proof of Proposition 4.4. According to Lemma C.14, there exist some ˆjwith∥ˆj∥1≤sn n+p
p
such
thatˆhˆj̸= 0. According to Lemma C.11 and Lemma 103 in [1], it holds that
Ea
det(˜M(a, t))2
≥C1˜hˆj(t)2
=C1ˆhˆjt∥ˆj∥1+O(t∥ˆj∥1+1)2
≥C1
2ˆhˆjt2∥ˆj∥1,(C.18)
where a∼ U
([−1,1]p)(n+p
p)
for some constant C1depending only on n, p, s , and for sufficiently
small t. It can be seen that
˜K(t) =1 n+p
pEah
˜M(a, t)˜M(a, t)⊤i
.
27By Jensen’s inequality, one has that
λmin(˜K(t))≥1 n+p
pEah
λmin
˜M(a, t)˜M(a, t)⊤i
. (C.19)
Since entries of ˜M(a, t)are all O(1)by Lemma C.2, which implies the boundedness of the eigenval-
uesλi
˜M(a, t)˜M(a, t)⊤
,i= 1,2, . . . , n+p
p
, one has that

det(˜M(a, t))2
= det
˜M(a, t)˜M(a, t)⊤
≤C2λmin
˜M(a, t)˜M(a, t)⊤
. (C.20)
Combining (C.18), (C.19), and (C.20), one can conclude that
λmin(˜K(t))≥C1
2ˆhˆj(α, m)t2∥ˆj∥1≥C1
2ˆhˆj(α, m)t2sn(n+p
p),
where we used ∥ˆj∥ ≤N n+p
p
max i∈Isiand only considered t≤1. By Proposition C.1, we have
that λmin(K(t))−λmin(˜K(t))=O(tL+1).
Then we can obtain (4.3) as we set L= 2sn n+p
p
.
Proof of Theorem 4.3. LetC, T be the constants in Proposition 4.4 and consider t > T . It follows
from Theorem 3.5 that w⊥
V(a, T) = 0 , which leads to that u⊥
V(a1, . . . , a p, T) = 0 and hence that
fNN(x;ρt)andg(x, t)only depend on xVfor all t≥T. Define g(t)∈R(n+p
p)viagi(t) =
Ex[g(x, t)pi(xV)]. Then according to (2.6) and (4.3), one has for t > T that
d
dtE(ρt) =−PX
i=1EaEx,x′
gi(x, t)σ(w(a, T)⊤x)σ(w(a, T)⊤x′)gi(x′, t)
=−g(t)⊤K(T)g(t)≤ −λmin(K(T))∥g(t)∥2
=−2λmin(K(T))E(ρt)≤ −2CT2sn(n+p
p)E(ρt),
which implies that
E(ρt)≤ E(ρT) exp
−2CT2sn(n+p
p)(t−T)
,
by Gronwall’s inequality. Then we obtain the desired exponential decay property by noticing that
E(ρT) =1
2Ez[∥h∗(z)∥2].
D Further Discussion and Characterization of Assumption 4.1 and
Theorem 4.3
D.1 Verification of Assumption 4.1
We provide some verification or characterization of Assumption 4.1. Without loss of generality, we
fix an orthonormal basis and Vand view that V=Rp. The results in this subsection are independent
of the choice of the orthonormal basis.
It has been discussed in Section 4.1 that if σ∈ Cs(R)withs= 2p−1andσ(1)(0), σ(2)(0), . . . , σ(p)(0)
are all nonzero, then Assumption 4.1 can be verified with sforh∗(z) =z1+z1z2+···+z1z2···zp,
using the calculation in [ 1, Proposition 33]. Similarly, the same result is also true for h∗(z) =
c1z1+c2z1z2+···+cpz1z2···zpifc1, c2, . . . , c pare nonzero. More generally, we have the
following.
Proposition D.1. Let{α1, α2, . . . , α m}be a set of pairwise distinct elements in Npthat
contains (1,0, . . . , 0),(1,1,0, . . . , 0), . . . , (1,1, . . . , 1). If σ∈ Cs(R)with s= 2p−1and
σ(1)(0), σ(2)(0), . . . , σ(p)(0)are all nonzero, then
h∗(z) =mX
i=1cipY
j=1Heαi(j)(zj),
satisfies Assumption 4.1 with sunless (c1, c2, . . . , c m)is in some measure-zero subset of Rmwith
respect to the Lebesgue measure.
28Proof. We assume that α1= (1,0, . . . , 0), α2= (1,1,0, . . . , 0), . . . , α m= (1,1, . . . , 1). As in the
proof of Lemma C.9, Assumption 4.1 is true with sif and only if the matrix ˆq:= (ˆqi,j)1≤i≤p,1≤j≤s
is of full-row-rank, i.e., det(ˆqˆq⊤)̸= 0. By (C.9) , each entry in ˆqis a polynomial in (c1, c2, . . . , c m),
which implies that det(ˆqˆq⊤)is also a polynomial in (c1, c2, . . . , c m). This polynomial is nonzero
since it takes nonzero value at (1,1, . . . , 1,0, . . . , 0)with the pentries being 1and all other entries
being 0, which is because that Assumption 4.1 is true for h∗(z) =z1+z1z2+···+z1z2···zp.
Finally, the conclusion of Proposition D.1 is true since the set of roots of a nonzero polynomial is of
measure zero with respect to the Lebesgue measure.
D.2 Discretization Results Implied by Theorem 4.3
The discussion in this subsection is similar to those in Appendix B.2. We slightly modified the flow
ρtgenerated by Algorithm 1 to guarantee some boundedness conditions, and then use the standard
dimension-free estimate [19] to derive a sample complexity result implied by Theorem 4.3.
We use the same bounded modification ˜f∗as in (B.3) for a given constant Cb
f>0. Simi-
larly, for any δ > 0andCw>0, there exists dimension-free constant Cb
σ>0depending on
h∗, δ, C w, such that one can modify the activation function ˆσ(ζ) = (1 + ζ)nto˜σsatisfying that
∥˜σ∥L∞(R)≤Cb
σ,∥˜σ′∥L∞(R)≤Cb
σ,∥˜σ′′∥L∞(R)≤Cb
σ, andEx∼N(0,Id)
|ˆσ(w⊤x)−˜σ(w⊤x)|2
<
δ,Ex∼N(0,Id)
|ˆσ(w⊤x)−˜σ(w⊤x)|2
< δ, for all w∈Rdwith∥w∥ ≤Cw.
The associated mean-field dynamics ˜ρt, that can be viewed as a slight modification of ρtgenerated
by Algorithm 1, is given by (B.4) for 0≤t≤Tand follows(
∂t˜ρt=∇θ·
˜ρtξ(t)∇θ˜Φ′(θ; ˜ρt)
,
˜ρt
t=T= ˜ρT,(D.1)
with
˜Φ′(θ;ρ) =aEx∼N(0,Id)h
˜fNN(x;ρ)−˜f∗(x)
˜σ(w⊤x)i
,
˜fNN(x;ρ) =Z
a˜σ(w⊤x)ρ(da, dw ),
fort≥T. Here, the learning rate ξ(t) =diag(ξa(t), ξw(t)Id)is shared with Algorithm 1, namely
ξa(t) = 0 , ξw(t) = 1 for0≤t≤Tandξa(t) = 1 , ξw(t) = 0 fort≥T.
The SGD associated to the modified mean-field dynamics is given by
w(k+1)
i =w(k)
i+ϵ
˜f∗(xk)−fNN(xk; Θ(k))
a(k)
iσ′ 
w(k)
i⊤xk
xk,
a(k+1)
i =a(k)
i,(D.2)
fori= 1,2, . . . , N andk= 0,1, . . . , T/ϵ −1, where Nis the number of neurons and T/ϵis assumed
to be an integer, and
w(k+1)
i =w(k)
i,
a(k+1)
i =a(k)
i+ϵ
˜f∗(xk)−˜fNN(xk; Θ(k))
˜σ 
w(k)
i⊤xk
,(D.3)
fori= 1,2, . . . , N andk=T/ϵ, T/ϵ + 1, . . .
Suppose that assumptions made in Theorem 4.3 hold and fix T′> T > 0. Using similar analysis as
in Appendix A.2, one can conclude that for any δ >0, there exist dimension-free constants Cb
fand
Cb
σsuch that
sup
0≤t≤T′|E(ρt)− E(˜ρt)|< δ.
Applying Theorem 4.3 and [ 19, Theorem 1], we can conclude that for any µ∈(0,1)and any δ >0,
there exists dimension-free constants N0, d0, Cϵ, such that for any N≥N0andd≥d0, the following
holds with probability at least µfor any ϵ≤Cϵ
d+log NwithT/ϵ∈N:
inf
k∈[0,T′/ϵ]∩NEN(Θ(k))< δ.
If we further assume that N=O(ed), this indicates that SGD with (D.2) and(D.3) can learn the
subspace-sparse polynomial f∗within finite time horizon and with O(d)samples/data points.
29NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The theoretical results claimed in the abstract and the introduction are estab-
lished in Section 3 and Section 4.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations are discussed in the last paragraph of Section 5.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
30Justification: The assumptions and the proof sketches are included in Section 3 and Section 4.
The complete proofs are in the appendices.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: This paper is purely theoretical and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
31Answer: [NA]
Justification: This paper is purely theoretical and does not include experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: This paper is purely theoretical and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: This paper is purely theoretical and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
32•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: This paper is purely theoretical and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The authors have reviewed the NeurIPS Code of Ethics and confirm that this
paper conforms with it in every respect.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper conducts fundamental research and is purely theoretical. Thus, it
has no societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
33•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper is purely theoretical and poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: This paper is purely theoretical and does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
34•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper is purely theoretical and does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper is purely theoretical/mathematical and does not involve crowdsourc-
ing or research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper is purely theoretical/mathematical and does not involve crowdsourc-
ing or research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
35