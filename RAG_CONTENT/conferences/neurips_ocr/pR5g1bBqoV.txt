µP2: Effective Sharpness Aware Minimization
Requires Layerwise Perturbation Scaling
Moritz Haas1Jin Xu2Volkan Cevher3,4Leena Chennuru Vankadara4
1University of Tübingen, Tübingen AI Center∗ 2University of Oxford∗
3LIONS, EPFL∗ 4AGI Foundations, Amazon
Abstract
Sharpness Aware Minimization (SAM) enhances performance across various
neural architectures and datasets. As models are continually scaled up to improve
performance, a rigorous understanding of SAM’s scaling behaviour is paramount.
To this end, we study the infinite-width limit of neural networks trained with SAM,
using the Tensor Programs framework. Our findings reveal that the dynamics
of standard SAM effectively reduce to applying SAM solely in the last layer
in wide neural networks, even with optimal hyperparameters. In contrast, we
identify a stable parameterization with layerwise perturbation scaling, which we
callMaximal Update and Perturbation Parameterization (µP2), that ensures all
layers are both feature learning and effectively perturbed in the limit. Through
experiments with MLPs, ResNets and Vision Transformers, we empirically
demonstrate that µP2achieves hyperparameter transfer of the joint optimum of
learning rate and perturbation radius across model scales. Moreover, we provide
an intuitive condition to derive µP2for other perturbation rules like Adaptive SAM
and SAM-ON, also ensuring balanced perturbation effects across all layers.
1 Introduction
Sharpness Aware Minimization (SAM) (Foret et al., 2021) and its variants (Kwon et al., 2021; Müller
et al., 2024) improves generalization performance across a wide range of neural architectures and
datasets (Chen et al., 2021; Kaddour et al., 2022). In the SAM formulation, we minimize a given
lossLbetween our prediction and the data yas a function of the architecture’s weights W, where an
adversary simultaneously maximizes the same loss by perturbing the weights within a budget ρ.
A standard SAM update for an L-hidden layer multi layer perceptron (MLP) is given by
Wl
t+1=Wl
t−ηl∇WlL(f(ξt;Wt+εt), yt),with εl
t=ρ·∇WlL(f(ξt;Wt), yt)
∥∇WL(f(ξt;Wt), yt)∥F,(SAM)
where W= [W1, . . . , WL+1],tis the iteration count and εl
tdenotes the perturbation in the l-th
MLP layer with width n∈N, and where we define an L-hidden layer MLP iteratively via
h1(ξ) :=W1ξ, xl(ξ) :=ϕ(hl(ξ)), hl+1(ξ) :=Wl+1xl(ξ), f (ξ) :=WL+1xL(ξ),
for inputs ξ∈Rdinwith trainable weight matrices W1∈Rn×din,Wl∈Rn×nforl∈[2, L], and
WL+1∈Rdout×n. We call hlpreactivations, xlactivations, and f(ξ)output function. Despite the
inherent difficulty of non-convex, non-concave optimization, SAM is quite successful in practice.
∗This work was conducted during Moritz’, Jin’s and V olkan’s time at Amazon. Correspondence to:
mo.haas@uni-tuebingen.de
38th Conference on Neural Information Processing Systems (NeurIPS 2024).10−210−1100
Perturbation radius ρ10−1100Learning rate ηµP-global
10−1100
Perturbation radius ρ10−1100µP2
width
4096
1024
256
10−1100
Perturbation radius ρ55.055.556.056.557.057.558.058.559.059.5Test accuracy
µP2
µP-globalFigure 1: Left and center (µP2transfers both ηandρ): Test accuracy as a function of learning
rateηand perturbation radius ρof a 3-layer MLP in µP trained with SAM on CIFAR10 for various
widths with global perturbation scaling ρ·n−1/2(left) and our layerwise perturbations scaling µP2
(right ), averaged over 3 independent runs.‘ ×’ denotes the optimum. Blue contours (the darker,
the wider) denote the region within 1%of the optimal test accuracy smoothened with a Gaussian
filter. Grey regions (the lighter, the wider) denote the unstable regime below 30% test accuracy.
Right (µP2improves generalization) : Same as left but sliced at the optimal learning rate of both
parameterizations for width 4096 with the base optimizer SGD in µP (dashed line) as a baseline.
Average and 2σ-CI from 16independent runs. Global perturbation scaling ρ·n−1/2achieves a
width-independent critical perturbation radius at which training becomes unstable, but does not
consistently improve over SGD in µP and does not transfer the optimal (η, ρ).µP2achieves joint
transfer in (η, ρ)and improves generalization performance.
On the other hand, the steadily growing scale of foundation models has sparked considerable
interest in scaling laws of model size and dataset size (Kaplan et al., 2020; Zhai et al., 2022). To
rigorously understand learning dynamics under width scaling, Yang and Hu (2021) have recently
provided general infinite-width theory for SGD, which has since been shown to be a good model
for understanding the properties of large models (Vyas et al., 2024). Yang and Hu (2021) show that
standard parameterizations (SP), including He or LeCun initialization (He et al., 2015; LeCun et al.,
2002) with a global learning rate, do not learn features in the infinite-width limit.
Instead, a different scaling of layerwise initialization variances and learning rates, termed Maximal
Update Parameterization (µP), is necessary to achieve feature learning in all layers in wide networks.
A crucial practical benefit of µP is the transferability of the optimal learning rate across model
scales (Yang et al., 2022). This can drastically reduce computational costs as it allows to tune
hyperparameters on smaller representative models and then to train the large model only once.
Contributions. In this paper, we adopt a scaling perspective to understand SAM’s learning dynamics.
Using the Tensor Programs framework (Yang, 2019; Yang and Hu, 2021; Yang and Littwin, 2023),
this work provides the first infinite-width theory for SAM with important practical consequences:
1.We show that training an MLP with the standard (SAM) update rule is equivalent to applying
perturbations only in the last layer in the infinite-width limit, even if the perturbation radius
is properly tuned. This holds for any width-dependent scaling of layerwise initialization
variances and learning rates, including SP and µP.
2.We demonstrate that the optimal perturbation radius can shift significantly in µP (Figure 1).
3.We postulate that jointly transferring the optimal learning rate ηand perturbation radius
ρrequires width-independent feature learning and effective perturbations in every layer
in the infinite-width limit. We define the perturbation of a trainable weight tensor to be
effective iff its effect on the output function scales width-independently. We show that this
can be achieved with layerwise scalings of the perturbation radius, and provide a complete
characterization of perturbation scaling parameterizations into four regimes: unstable,
vanishing, nontrivial and effective perturbations.
4.We derive the Maximal Update and Perturbation Parameterization (µP2) that achieves both
feature learning and effective perturbations in all layers in the infinite-width limit. We
empirically demonstrate that µP2achieves hyperparameter transfer in both learning rate η
and perturbation radius ρ(Figure 1).
5.We provide a versatile (spectral) scaling condition (∗)applicable to architectures such as
ResNets and Vision Transformers (ViTs), and to various SAM variants like SAM-ON and
Adaptive SAM (ASAM), and any SAM updates modeled in a Tensor Program.
22 Background and related work
We here provide a short summary of related work. A more detailed account is provided in Appendix B.
Sharpness Aware Minimization. SAM was motivated as an inductive bias towards flatter minima and
it provably reduces properties of the Hessian that are related to sharpness in simpler settings (Bartlett
et al., 2023; Wen et al., 2023; Monzio Compagnoni et al., 2023). However a full understanding of
why SAM works so well remains elusive (Andriushchenko et al., 2023b; Wen et al., 2024). For
example, applying SAM on only the normalization layers (SAM-ON) often improves generalization
further despite increasing sharpness (Müller et al., 2024). A plethora of SAM variants have recently
been proposed with the purpose of even stronger performance or reducing SAM’s computational and
memory complexity. We focus on two variants of Adaptive SAM (ASAM) (Kwon et al., 2021) which
achieve the overall strongest results in Müller et al. (2024) (see Appendix F.4 for more details).
Tensor Programs. We build on the Tensor Programs framework (Yang, 2019; Yang and Hu, 2021;
Yang and Littwin, 2023; Yang et al., 2022, 2023b), which covers many modern deep learning archi-
tectures, optimization algorithms and arbitrary abc-parameterizations. Each abc-parameterization is
essentially defined by a layerwise scaling of initialization variance and learning rate as a function
of network width. Beyond pure infinite-width limits, the simple1√
L-scaling allows depth-scaling
in ResNets and unlocks hyperparameter transfer across depths (Hayou et al., 2021; Li et al., 2021;
Bordelon et al., 2023; Yang et al., 2023b). Noci et al. (2022, 2024) provide infinite width and depth
analyses for Transformers with the goal of preventing rank collapse.
Look-LayerSAM (Liu et al., 2022) already considers layerwise perturbation scaling with the goal
of preserving good performance under large batch training. However, achieving µP2with Look-
LayerSAM requires nontrivial layerwise learning rate and perturbation rescaling (see Appendix B).
3 SAM induces vanishing perturbations in wide neural networks
This section shows that under the standard (SAM) update rule, weight perturbations induced by SAM
vanish in the infinite-width limit in every layer except the output layer. We later demonstrate that
other SAM variants also selectively perturb other subsets of layers. For enhanced readability of some
formulae, we use colors to distinguish four regimes of perturbation behaviour: Unstable, vanishing,
nontrivial and effective perturbations.
While our theory covers any stable parameterization including He and LeCun initializations, for
concreteness and for the clarity of exposition, we first present our results for MLPs under µP:
initialize W1∼ N(0,1/din), Wl∈Rn×n∼ N(0,1/n)forl∈[2, L], WL+1∼ N(0,1/n2)
with layerwise SGD learning rates η1=ηn, η l=η,forl∈[2, L], ηL+1=ηn−1.
By analyzing the infinite-width behaviour of the SAM update rule, we show that the training dynamics
under standard (SAM) become unstable as the network width increases. This result is first stated
informally below in Proposition 1 and then more formally in the next section.
Proposition 1 (Instability of standard SAM parameterization in wide neural networks ).Under
µP with the standard (SAM) update rule and default perturbation given in (SAM) , the output
function becomes unbounded after the first update step in the infinite-width limit for any fixed, positive
learning rate η >0and perturbation radius ρ >0.
Hence, to achieve stable optimization, it is necessary to introduce some width-dependent perturbation
scaling ρn−dfor some suitable d >0. To understand the layerwise scaling behaviour of SAM under
this scaling, we define the notion of vanishing perturbations .
Vanishing perturbations. The weight perturbation εlperturbs the l-th layer’s activations as
xl+˜δxl=ϕ((Wl+εl)(xl−1+˜δxl−1)),
where ˜δxldenotes the perturbation of the l-th layer’s activations accumulated from the weight
perturbations {εl′}l′∈[l]in all previous layers. We say a layer lhasvanishing perturbations if
˜δxl→0as the width approaches infinity. This occurs if the weight perturbations in all previous
layers are too small when measured in spectral norm, that is if ∥εl′∥∗/∥Wl′∥∗→0for all l′∈[l].
364 256 1024 4096 163840.00.20.40.60.81.01.21.4/bardbl˜W−W/bardblF//bardblW/bardbl∗×10−5 Input layer
SAM-SGD
SAM-SGD-LL
SGD
64 256 1024 4096 163840.00000.00020.00040.00060.00080.00100.00120.0014Hidden layer
64 256 1024 4096 163840.0000.0050.0100.0150.0200.025Output layer
64 256 1024 4096 16384
Width0.120.140.160.180.20/bardblxt−x0/bardbl2, normalized
64 256 1024 4096 16384
Width0.150.200.250.30
64 256 1024 4096 16384
Width345678
SAM-SGD
SAM-SGD-LL
SGDFigure 2: ((SAM) effectively only perturbs the last layer) Layerwise weight perturbations (top)
and normalized activation updates ∥∆xl∥2(bottom) for SAM, last-layer SAM and SGD as a baseline
across widths after training a 3-layer MLP in µP with global perturbation scaling ρ·n−1/2for 20
epochs on CIFAR10. Average and CI are computed from 4independent runs. Perturbations are
normalized by the weight spectral norm to measure their effect on the layer’s output. Activation
updates are normalized byp
dim(∆xl)to measure coordinatewise updates. We provide more neural
network statistics in Appendix H.1.
Informally, Proposition 2 below shows that for every choice of a decay parameter d >0, either
the training dynamics of SAM are unstable or all the hidden layers of the network have vanishing
perturbations in the limit. The formal results are stated in the next section.
Proposition 2 (Global perturbation scaling is unstable or induces vanishing perturbations ).Fix
ρ >0andt∈N. Let ˚ftdenote the infinite-width limit of the output function after training an MLP
of width nwith the SAM update rule (SAM) with perturbation radius ρn−dfortsteps. If d <1/2,
then output perturbations blow up, and ˚ftis unstable. If d >1/2, then the perturbations in all layers
vanish and ˚ftcorresponds to the limit after tsteps of SGD. If d= 1/2, then only the last layer is
effectively perturbed, all other layers have vanishing perturbations.
Figure 2 shows statistics of an MLP trained with (SAM) with global width-dependent scaling ρn−1/2
versus the same MLP trained with SAM where only the last-layer weights are perturbed andεl= 0
for all l∈[L]. As predicted by Proposition 2, both training algorithms produce equivalent training
dynamics, already at moderate width, and last-layer perturbations are scaled correctly.
Remark 3 (Practical implications ).According to Proposition 2, for sufficiently wide models, any
performance gains from standard SAM are primarily due to applying the SAM update rule to the last
layer even with a properly tuned perturbation radius. This implies that, when applying the standard
SAM update rule (SAM), one can remove the inner backward pass beyond the last layer and nearly
recover the computational cost of SGD. However, it may be undesirable for optimal generalization if
only the last layer is perturbed (Figure 1). ◀
Layerwise perturbation scaling. In the next section, we show that correcting the (SAM) update rule
to achieve effective perturbations in every single layer requires introducing additional hyperparam-
eters — layerwise width-dependent scaling of the perturbation radius. This is similar in spirit to µP
which corrects standard parameterization by introducing layerwise scaling of the learning rates. We
postulate that achieving width-independent scaling of both updates and perturbations is a necessary
condition for hyperparameter transfer under SAM. We also lay all theoretical foundations and derive
the stable parameterization, we call maximal update and perturbation parameterization (µP2) that
achieves both feature learning and effective perturbations in all layers in the infinite-width limit .
Figure 1 shows that µP2indeed achieves hyperparameter transfer in the optimal joint choice of (η, ρ),
while also achieving the best generalization performance (Table 2).
General perturbation scaling condition. For intuitive understanding and a generalization to other
perturbation rules, a simple condition for achieving effective perturbations in any layer follows from
our results: in every layer, perturbations should scale like updates in µP.
4The reason is that both updates and perturbations are gradient-based ∇WlL=∇hlL ·(xl−1)⊤, and
thus low-rank and correlated with the incoming activations xl−1. Therefore updates and perturbations
introduce the same LLN-like scaling factors, and require the same layerwise scaling corrections. Like
Yang et al. (2023a), we can rephrase this condition in terms of weight spectral norms to: For a weight
matrix Wl
t∈Rfan_out ×fan_in, its update δWl
tand its perturbation εl
t, it should hold at all times tthat
∥εl
t∥∗= Θ 
∥δWl
t∥∗
= Θ 
∥Wl
t∥∗
= Θp
fan_out /fan_in
. (∗)
with big-O notation that only tracks dependence on network width (Definition C.1). We discuss the
spectral perspective in more detail in Appendix F.7.
4 Sharpness Aware Minimization in the infinite-width limit
4.1 Characterization of layerwise perturbation scaling: Unstable, vanishing, nontrivial and
effective perturbations
To systematically and rigorously understand the width-scaling behaviour of neural networks trained
under the SAM update rule, we propose a new class of parameterizations, which we refer to as bcd-
parameterizations . Motivated by the analysis in Section 3, the class of bcd-parameterizations naturally
extends abc-parameterizations (Yang and Hu, 2021) by including layerwise scaling of the perturbation
radius. By setting all weight multiplier exponents al= 0, we do not need to modify the MLP architec-
ture and recover representatives of each abc-parameterization that capture their essence and condense
all equations: Ignoring numerical considerations (Blake et al., 2024), each abc-parameterization is
essentially a layerwise initialization and learning rate scaling. The effects of weight multipliers on
SAM are more nuanced than for SGD or Adam (see Remark 12 and Appendix F.6).
To study the infinite-width behaviour of networks trained with SAM in any bcd-parameterization,
we utilize the theoretical framework of NE⊗OR⊤programs (Yang et al., 2023b). We write the two
forward and backward passes for each SAM update (ascent/perturbation step then descent/update
step) using the NE⊗OR⊤computation rules and rigorously track all relevant scalings as provided
by the NE⊗OR⊤master theorem. All proofs are provided in Appendix E. The full formal result
statements can be found in Appendix D. Further theoretical considerations and generalizations around
perturbation scaling are provided in Appendix F.
Assumptions. For clarity of exposition, we present our main results for MLPs. Their extension to
other architectures is discussed in Appendix F.5. For all of the results in this section, we assume that
the used activation function is either tanh orσ-gelu forσ >0sufficiently small. For small enough
σ > 0,σ-gelu (Definition C.9) approximates ReLU arbitrarily well. We also assume constant
training time as width n→ ∞ . We assume batch size 1for clarity, but our results can be extended
without further complications to arbitrary fixed batch size as well as differing fixed batch sizes for
the ascent/perturbation and the descent/update step, as sometimes used for SAM (Foret et al., 2021).
Considering small perturbation batch size is practical, as it has been observed to enhance SAM’s
generalization properties (Andriushchenko and Flammarion, 2022).
Definition 4 (bcd-parametrization ).Abcd-parametrization {bl}l∈[L+1]∪ {cl}l∈[L+1]∪
{dl}l∈[L+1]∪ {d}defines the training of an MLP with SAM in the following way:
(a) Initialize weights iid as Wl
ij∼ N(0, n−2bl).
(b) Train the weights using the SAM update rule with layerwise learning rates,
Wl
t+1=Wl
t−ηn−cl∇WlL(f(ξt;Wt+εt), yt),
with the scaled perturbation εtvia layerwise perturbation radii,
εt:=ρn−dvt
∥vt∥,with vt= (v1
t, . . . , vL+1
t), vl
t:=n−dl· ∇WlL(f(ξt;Wt), yt),(LP)
W.l.o.g. we set ∥vt∥= Θ(1) , which prevents nontrivial width-dependence from the denominator.
This imposes the constraints: d1≥1/2−min(bL+1, cL+1), dl≥1−min(bL+1, cL+1)forl∈
[2, L],anddL+1≥1/2,with at least one equality required to hold (see Appendix E.1.3). The
normalization vt/∥vt∥removes one degree of freedom from {dl}l∈[L+1]via the equivalence
{d′
l}l∈[L+1]∼={dl}l∈[L+1]iff there exists a C∈Rsuch that d′
l=dl+Cfor all l∈[L+ 1].◀
5Stability. To ensure that the training dynamics of SAM are well-behaved with scale, we require
bcd-parameterizations to satisfy conditions of stability. Perturbed weights ˜Wl=Wl+εlinduce
perturbed activations xl+˜δxland a perturbed output function ˜ft(ξ) := f˜Wt(ξ). We call a bcd-
parameterization stable (Definition C.3) if the hidden activations have width-independent scaling
Θ(1) at initialization and during training, and neither the updates nor the perturbations ˜δxlof the
activations or output logits ˜ft−ftblow up at any point in training.
For stating the conditions that characterize the class of stable bcd-parameterizations, we define the
maximal feature perturbation scaling ˜rof abcd-parameterization as
˜r:= min( bL+1, cL+1) +d+L
min
l=1(dl−I(l̸= 1)) .
Similar to the maximal feature update scaling rfrom Yang and Hu (2021), ˜rdescribes how much the
last hidden-layer activations are perturbed as a function of width, xL+˜δxL= Θ( n−˜r). Hidden-layer
activation perturbations do not explode with width if and only if ˜r≥0. The output perturbations not
to blow up if and only if d+dL+1≥1andbL+1+ ˜r≥1. In particular, this implies that any stable
bc-parameterization together with naive perturbation scaling dl=d= 0for all l∈[L+1] isunstable
due to blowup in the last layer . We formally state the stability characterization in Theorem D.2.
Ideally, we will later require width-independent perturbation scaling which is attained iff ˜r= 0.
Effective SGD dynamics. Within the class of stable parameterizations, there are parameterizations
in which perturbations in the output vanish in the infinite-width limit at any point during training. In
other words, SAM training dynamics collapses to SGD dynamics with scale. We are mostly interested
in the opposing class of parameterizations with non-vanishing perturbations. We characterize this
class in Theorem 6 and refer to them as perturbation nontrivial (Definition 5).
Definition 5 (Perturbation nontriviality ).We say that a stable bcd-parametrization is per-
turbation nontrivial if there exists a training routine, t∈N0andξ∈Rdinsuch that
˜δft(ξ) :=f˜Wt(ξ)−fWt(ξ) = Ω(1) . Otherwise, the bcd-parametrization is perturbation trivial .◀
Theorem 6 (Perturbation nontriviality characterization ).A stable bcd-parametrization is pertur-
bation nontrivial if and only if d+dL+1= 1ormin(bL+1, cL+1) + ˜r= 1.
For the class of stable and perturbation nontrivial bcd-parameterizations, SAM learning is both stable
and deviates from SGD dynamics. A natural question to ask here is: what should be the ideal
SAM behaviour in the infinite-width limit? To address this question, we make the following crucial
distinction between non-vanishing andeffective perturbations .
Non-vanishing versus effective perturbations. Recall that the weight perturbation εlperturbs the
l-th layer’s activations as
xl+˜δxl=ϕ((Wl+εl)(xl−1+˜δxl−1)),
where ˜δxldenotes the perturbation of the l-th layer’s activations accumulated from the weight
perturbations {εl′}l′∈[l]in all previous layers. Therefore, perturbations ˜δxlcan stem both from
weight perturbations εl′in a previous layer l′< land/or from weight perturbations εlin the current
layer l. Intuitively, if we perturb a layer, we want this to affect the next layer’s activations and
thereby have a nontrivial effect on the output function. Otherwise one can simply set the layer’s
perturbations to 0by design and not change the learning algorithm in the infinite-width limit. This
motivates the definition of effective perturbations , which demands the weight perturbations of the
current layer to contribute non-vanishingly. From the weight perspective (∗), effective l-th layer
perturbations are achieved if and only if weight perturbations scale like the weights in spectral norm,
∥εl∥∗/∥Wl∥∗= Θ(1) . Without an effective perturbation εlof the l-th layer, this layer does not
inherit SAM’s inductive bias towards low spectral norm of the Hessian or enhanced sparsity and
does not improve generalization performance. We provide empirical evidence for these claims in
Appendix H.2. Therefore a distinction between non-vanishing andeffective perturbations is crucial.
Definition 7 (Non-vanishing perturbations ).Forl∈[L], we say that a stable parameterization has
non-vanishing perturbations in the l-th layer if there exists a t∈Nsuch that ˜δxl
t= Ω(1) . ◀
Definition 8 (Effective perturbations ).Forl∈[L+ 1], we say that a stable parameterization
effectively perturbs the l-th layer if there exists a t∈Nsuch that εl
t(xl−1
t+˜δxl−1
t) = Θ(1) , where
x0
t+˜δx0
t=ξt. ◀
6Figure 3: (Perturbation phase characterization of bcd-parameterizations) Given a choice of
layerwise initialization and learning rate scalings {bl, cl}l∈[L+1], the maximal feature perturbation
scaling ˜rand the last-layer perturbation scaling d+dL+1completely determine whether a bcd-
parameterization is unstable, has effective SGD dynamics, effective perturbations in some but not all
layers or effective perturbations in all layers. In SP or NTP (left), there does not exist a choice of
perturbation scalings that achieves effective perturbations in all layers, whereas in µP (right), there is
a unique choice as provided in Theorem 11.
Theorem 9 provides a characterization of stable bcd-parameterizations with vanishing perturbations
in any given layer.
Theorem 9 (Vanishing perturbation characterization ).For any l0∈[L], the following statements
are equivalent:
(a) A stable bcd-parametrization has vanishing perturbations in layer l0.
(b) A stable bcd-parametrization has vanishing perturbations in layer lfor all 1≤l≤l0.
(c)˜rl0:= min( bL+1, cL+1) +d+ minl0
m=1(dm−I(m̸= 1)) >0.
It follows from Theorem 9 that any stable bcd-parameterization that performs updates in the original
gradient direction (i.e., dl=Cfor all l∈[L+ 1] for some C∈R) has vanishing perturbations in all
input and hidden layers l∈[L], and the last layer l=L+ 1is effectively perturbed if and only if
d= 1/2. This covers the case of both standard and maximal update parameterizations with global
scaling of the perturbation radius discussed in Section 3. Negating the conditions of Theorem 9
implies that a stable bcd-parameterization has non-vanishing perturbations in layer l0if and only if
˜rl0= 0. Achieving effective perturbations is a stronger requirement for which Theorem 10 provides
the necessary and sufficient conditions.
Theorem 10 (Effective perturbation characterization ).Forl∈[L], a stable bcd-parametrization
effectively perturbs the l-th layer if and only if min(bL+1, cL+1) +d+dl−I(l̸= 1) = 0 .
A stable bcd-parametrization effectively perturbs the last layer if and only if d+dL+1= 1.
4.2 Maximal Update and Perturbation Parameterization ( µP2)
We postulate that just as the optimal learning rate transfers across widths under µP for SGD and
Adam due to non-vanishing width-independent feature evolution in all layers, the optimal learning
rate and perturbation radius may be jointly transferable across widths if additionally the weight
perturbations induce width-independent perturbations of the activations in all layers. Here, we show
that, for every stable initialization and learning rate scaling with bL+1≥1, there exists a unique
stable layerwise perturbation scaling which effectively perturbs every single layer. We term this
layerwise perturbation scaling {dl}l∈[L+1]∪ {d}the Maximal Perturbation Parameterization (MPP).
This concludes the phase characterization of perturbation scaling behaviours (Figure 3).
Theorem 11 (Maximal Perturbation Parameterization (MPP) ).Consider any stable bcd-
parametrization {bl}l∈[L+1]∪ {cl}l∈[L+1]∪ {dl}l∈[L+1]∪ {d}. IfbL+1<1, then there does
not exist a stable choice of {dl}l∈[L+1]∪ {d}that achieves effective perturbations before the last
layer. If bL+1≥1, then up to the equivalence d′
l=dl+C,C∈R,∀l∈[L+ 1], the unique stable
choice {dl}l∈[L+1]∪ {d}that effectively perturbs all layers l∈[L+ 1] is given by
d=−1/2, d l=

1/2−min(bL+1, cL+1)l= 1,
3/2−min(bL+1, cL+1)l∈[2, L],
3/2 l=L+ 1.(1)
7Maximal Update and Perturbation Parameterization µP2.To achieve feature learning in ev-
ery layer and hyperparameter transfer in the learning rate, µP is the unique2choice of layerwise
initialization variance and learning rate scalings {bl, cl}l∈[L+1](Yang and Hu, 2021). Together
with Theorem 11, this shows that there exists a unique2bcd-parameterization that achieves both
feature learning and effective perturbations in all layers, we call maximal update and perturbation
parametrization ,µP2for short. Now that we have found a parameterization that achieves width-
independent scaling of both activation updates and activation perturbations, µP2fulfills essential
necessary conditions for hyperparameter transfer to occur in both ηandρ.
Remark 12 (Achieving µP2with weight multipliers ).Appendix F.6 covers the extension of
our results to nontrivial weight multipliers. We show that, for each choice of weight multipliers
{al}l∈[L+1], there is a unique2choice of bcd-hyperparameters that achieves effective perturbations in
all layers. But unlike for SGD or Adam, these parameterizations lead to slightly different training
algorithms, because differing subsets of layers contribute non-vanishingly to the joint gradient
normalization term ∥∇WL∥Fin(SAM) . The term ∥∇WL∥Fcouples all layers so that there do not
exist layerwise but only layer-coupled equivalence classes for (SAM) . Most importantly, instead of
adapting (SAM) , we can adapt the architecture with the weight multipliers n−al·Wlwith
al=−1/2·I(l= 1) + 1 /2·I(l=L+ 1) (a-µP2)
to achieve effective perturbations in all layers with naive perturbation and learning rate scaling
such that all layers contribute non-vanishingly to the joint gradient norm (Appendix F.6). One
downside of (a-µP2), that also applies to naive weight multipliers al= 0, is its incompatibility with
unit scaling considerations for low precision training (Blake et al., 2024). ◀
Alternative perturbation scaling definitions. Scaling equivalent to (a-µP2)can be achieved
without multipliers by scaling the numerator and denominator terms in (LP) independently, and
choosing to scale all denominator terms to be width-independent (see perturbation rule (DP) and
Appendix F.7 for more details). The ablations in Appendix H.4 suggest that this has a negligible
effect on the optimal generalization performance of µP2, but can be more stable given suboptimal
hyperparameters. Gradient normalization in each layer separately is uncommon and performs slightly
worse (Appendix H.5). Appendix F.2 discusses further considerations that led to Definition 4.
Trivial, lazy, and feature learning regimes. A small last-layer initialization variance bL+1≥1is
required for stable feature learning. Theorem 11 shows that bL+1≥1is also required for effective
hidden-layer perturbations. Beyond this condition, the choice of {bl}and{cl}is decoupled from
that of perturbation scalings {dl} ∪ {d}for stable bcd-parameterizations, because the scale of the
activations of a layer lis entirely determined by the scale of initialization bland learning rates cl,
given stability. Consequently, whether a parameterization is trivial , in the lazy regime, or in the
feature learning regime is independent of the choice of dl’s provided that all stability constraints are
met. A complete characterization of these regimes for the class of bc-parameterizations has been
provided in Yang and Hu (2021) and remains unchanged for the class of stable bcd-parameterizations.
For completeness, formal definitions and the corresponding results are stated in Appendices C and D.
4.3 Generalizations to other architectures and SAM variants
Generalization to other architectures. Our results can be extended to other common layer types,
that are representable as a NE⊗OR⊤program, including all ResNet and Transformer components
(Appendix F.5). All considered layer types behave like input, hidden or output layers. Most
importantly, normalization layer weights and biases scale like input layer weights to the input 1.
Generalization to other SAM variants. We would like to find the correct layerwise perturbation
scaling without writing out the NE⊗OR⊤program for every perturbation rule individually. Formally
justified by our proof in Appendix E, we rephrase our equivalent spectral scaling condition (∗)from
Section 3 to: maximal stable perturbations are achieved in µP if and only if εl= Θ( δWl). This
condition holds as soon as weight updates δWland perturbations εlare both correlated with the
incoming activations xl−1, for example if both are gradient-based. Table 1 summarizes the application
of this condition to two ASAM variants that perform well empirically but cannot be written as a
NE⊗OR⊤program. Additional details are provided in Appendix F.4. We demonstrate that these
scalings perform well and transfer hyperparameters in the next section. Note that for hidden layers in
2Strictly speaking, unique up to smaller last-layer initialization bL+1≥1.
8Perturbed under global scaling? For effective perturbations with µP2:
Input,
biases,
norm.Other
hidden
layersOutput
layerGlobal
ρInput-
likeHidden-
likeOutput-
like
SAM ✗ ✗ ✓ n1/2n1/2n−1/2n−3/2
Layer. ASAM ✗ ✓ ✗ 1 1 n−11
Elem. ASAM ✓ ✓ ✓ n1/21 1 1
SAM-ON ✓ - - n1/21 - -
Table 1: (Layerwise perturbation scaling for effective perturbations in µP)Without layerwise
perturbation scaling ( left), each SAM variant perturbs a different subset of layers at large width
n→ ∞ , but we provide the unique layerwise perturbation rescaling µP2(right ) that achieves effective
perturbations in all layers. This parameterization transfers both the optimal ηandρacross widths.
µP2, it holds that εl= Θ( n−1)butWl= Θ( n−1/2)entrywise, due to large initialization, showing
that it is crucial to compare perturbations to updates or to measure weight scalings in spectral norm.
5 The maximal update and perturbation parameterization µP2achieves
hyperparameter transfer and improved generalization
In this section, we provide experimental results showing that µP2achieves hyperparameter transfer
in both ηandρacross architectures, and that µP2also improves generalization over SP and µP with
global perturbations – even after multi-epoch training to convergence. We train MLPs and ResNets
(He et al., 2016) on CIFAR10 (Krizhevsky et al., 2009) and Vision Transformers (ViTs) (Dosovitskiy
et al., 2021) on Imagenet1K (Deng et al., 2009). While we directly implement bcd-parameterizations
for MLPs and ResNets in PyTorch (Paszke et al., 2019), we use the mup-package (Yang et al., 2022)
as a basis for ViT experiments. Pseudocode and a spectral derivation of our µP2-implementation for
ViTs, which is equivalent to (a-µP2), are provided in Appendix F.7. All experimental details are
stated in Appendix G and all supplemental experiments can be found in Appendix H.
Comparing candidate parameterizations in MLPs. Figure 1 shows test accuracy as a function
of learning rate and perturbation radius for MLPs of varying width. While previous µP-literature
mostly focuses on the more immediate transfer in training error, for SAM it is crucial to consider
optimality in test error as the perturbation radius acts as a regularizer, so that optimality in test error
typically coincides with suboptimal training error. In µP without perturbation scaling, the regime of
stable perturbation radii shrinks (Figure H.6). In µP with global perturbation scaling ρ·n−1/2, the
regime of stable ρremains invariant under width scaling, but there is no significant improvement of
SAM beyond SGD, so that the optimal perturbation radius fluctuates within its stable regime due to
noise. Only µP2consistently achieves hyperparameter transfer across widths, and achieves significant
improvement over its base optimizer SGD in µP at scale. The full hyperparameter landscapes are
provided in Appendix H.3.
10−210−1100101
Perturbation radius ρ0.480.500.520.540.560.580.600.62Top 1 accuracywidth
768
384
192
Figure 4: (ρ-transfer in ViTs) Training
a ViT with SAM in µP2on ImageNet1K
from scratch for 100 epochs yields ρ-
transfer and large improvements over
AdamW in µP (dashed lines).ρ-transfer in ViTs. Figure 4 shows that the optimal pertur-
bation radius transfers for ViT-S/16 on Imagenet1K trained
with SAM in µP2. While Andriushchenko and Flammar-
ion (2022, Appendix E.3) observe diminishing benefits of
SAM at large widths in SP, here the improvements beyond
the base optimizer AdamW in µP are particularly large.
ρ-transfer for SAM variants in µP2.Figure 6 shows
that training a ResNet-18 in µP2achieves hyperparam-
eter transfer in ρfor all considered SAM variants with
varying width. µP with global perturbation scaling ( µP-
global) has a width-invariant stability threshold in ρand
the optimal ρclearly shifts toward that threshold. It
would be interesting to see whether this shift contin-
ues with larger width and leads to suboptimal perfor-
mance of µP-global in wider ResNets. Table 2 shows
that all SAM variants perform similarly well in µP2, some slightly outperforming the best-
910−210−1100
Perturbation radius ρ94959697Test accuracySAMµP-global
width
4.0
2.0
1.0
0.5
10−210−1100
Perturbation radius ρ94959697SAMµP2
10−1100101
Perturbation radius ρ94959697SAM-ONµP2
10−1100101
Perturbation radius ρ94959697Elem. ASAM µP2Figure 6: (ρ-transfer of ASAM variants in µP2)Test error as a function of perturbation radius
ρafter 200epochs of training a ResNet-18 in µP2on CIFAR10 with various SAM variants (see
subplot title). CI over 2 independent runs. Darker lines correspond to larger width multipliers.
Other hyperparameters are tuned at base width multiplier 0.5.µP2achieves transfer in ρand large
improvements over the base optimizer (dashed lines) SGD in µP with momentum and weight decay.
SAM global SAM µP2SAM-ON µP2Elem. ASAM µP2
SP 97.00±0.03(+0.96) 97 .00±0.03(+0.96) 97.29±0.06(+1.26) 97 .15±0.01(+1.11)
µP97.19±0.05(+0.93)97.23±0.08(+0.97)97.34±0.08(+1.08)97.32±0.05(+1.06)
Table 2: (Performance of µP2)Average test accuracy ±standard deviation across 4 runs (+improvement of
SAM over SGD) for ResNet-18 with width multiplier 4on CIFAR10 using SGD as a base optimizer.
In bold, all parameterizations within a 2σ-CI from the best-performing variant SAM-ON in µP2.
performing variant SAM-ON in SP. This suggests that for ResNets, even with a proper lay-
erwise balance, normalization layer perturbations may suffice, and performance differences in
SP are primarily caused by varying degrees to which the normalization layers are perturbed.
0 25 50 75 100 125 150 175 200
Epochs of Training101102100% - test accuracySAMµP2
SGDµP
SAM SP
SGD SP
Figure 5: (Stable training dynamics)
SAM in µP2stabilizes training dynamics
for a ResNet-18 with width multiplier 2.Without providing an explanation, Müller et al. (2024,
Section 5.3) observe that only SAM-ON and elementwise
ASAM sufficiently perturb normalization layers in SP. Ta-
ble 1 ( left) explains these observations by showing that
only these two SAM variants effectively perturb normal-
ization layers under global perturbation scaling. Table 1
(right ) also provides full control over which layers to per-
turb. For transferring the optimal ρwith SAM-ON in µP,
our theory predicts the global scaling ρ= Θ( n1/2)which
is confirmed by our empirical observations (Figure 6).
However, properly understanding the role of normaliza-
tion layer perturbations remains an important question for
future work. Note that we report results after fine-tuning
all hyperparameters. The performance gain of µP2over
SP and µP-global is likely much higher in larger models, for which fine-tuning is infeasible and the
lack of feature learning and effective perturbations is more pronounced. Even under optimal HPs,
µP2appears to stabilize SAM’s training dynamics compared to SP (Figure 5).
6 Future work
This study may serve as an inspiration of how scaling theory can be used to understand and improve
training procedures in minimax optimization and beyond. To reach a fully practical theory of deep
learning, it will be necessary to take data distributions and training dynamics into account in more
detail than it is possible with current Tensor Program theory (Everett et al., 2024). Existing Tensor
Program theory assumes constant batch size and training time, and does not make statements about
generalization. For example, we observe that MLPs and ResNets in SP can sometimes display HP
transfer in ηandρafter multi-epoch training to convergence (Appendix H.3.2). This goes beyond the
observations by Everett et al. (2024) as we observe transfer even without tuning layerwise learning
rates or weight multipliers. This transfer strongly contradicts the infinite-width theory from Yang
and Hu (2021) which predicts output blowup under large learning rates, and it shows that the exact
conditions which enable hyperparameter transfer in practice are not fully understood. It also remains
unclear how to optimally adapt (SAM) when increasing network depth. We plan to address some of
these questions in upcoming work.
10References
Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware mini-
mization. In International Conference on Machine Learning (ICML) , 2022. Cited on page 5, 9, 17,
37, 38, 48.
Maksym Andriushchenko, Dara Bahri, Hossein Mobahi, and Nicolas Flammarion. Sharpness-aware
minimization leads to low-rank features. arXiv:2305.16292 , 2023a. Cited on page 38, 53.
Maksym Andriushchenko, Francesco Croce, Maximilian Müller, Matthias Hein, and Nicolas Flam-
marion. A modern look at the relationship between sharpness and generalization. In Andreas
Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning (ICML) ,
2023b. Cited on page 3, 17.
Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on the edge of
stability in deep learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,
Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine
Learning (ICML) , 2022. Cited on page 17.
Peter L. Bartlett, Philip M. Long, and Olivier Bousquet. The dynamics of sharpness-aware minimiza-
tion: Bouncing across ravines and drifting towards wide minima. Journal of Machine Learning
Research (JMLR) , 24(316):1–36, 2023. Cited on page 3, 17.
Jeremy Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu Liu. On the distance between two
neural networks and the stability of learning. Advances in Neural Information Processing Systems
(NeurIPS) , 33, 2020. Cited on page 17.
Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You. Chinchilla scaling: A replication
attempt. arXiv:2404.10102 , 2024. Cited on page 17.
Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y Prince, Björn Deiseroth,
Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, and Douglas Orr. u- µP: The
unit-scaled maximal update parametrization. arXiv:2407.17465 , 2024. Cited on page 5, 8, 45, 49.
Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. Depthwise
hyperparameter transfer in residual networks: Dynamics and scaling limit. arXiv:2309.16620 ,
2023. Cited on page 3, 16.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in Neural Information Processing Systems (NeurIPS) , 33:1877–1901,
2020. Cited on page 60.
Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets
without pre-training or strong data augmentations. arXiv:2106.01548 , 2021. Cited on page 1, 17.
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on
neural networks typically occurs at the edge of stability. In International Conference on Learning
Representations (ICLR) , 2020. Cited on page 17.
Yan Dai, Kwangjun Ahn, and Suvrit Sra. The crucial role of normalization in sharpness-aware
minimization. Advances in Neural Information Processing Systems (NeurIPS) , 36, 2024. Cited on
page 17, 39.
Yann N Dauphin, Atish Agarwala, and Hossein Mobahi. Neglected hessian component explains
mysteries in sharpness regularization. arXiv:2401.10809 , 2024. Cited on page 17.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) , 2009. Cited on page 9, 51, 71.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Proceedings of the 34th International Conference on Machine Learning (ICML) ,
2017. Cited on page 17.
11Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.
InInternational Conference on Learning Representations (ICLR) , 2021. Cited on page 9.
Katie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak, Peter J Liu,
Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and Jeffrey Pennington.
Scaling exponents across parameterizations and optimizers. In Proceedings of the 41st International
Conference on Machine Learning (ICML) , 2024. Cited on page 10, 49, 61.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization. In International Conference on Learning Representations
(ICLR) , 2021. Cited on page 1, 5, 17, 37, 48.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.
Advances in Neural Information Processing Systems (NeurIPS) , 31, 2018. Cited on page 16.
Soufiane Hayou and Greg Yang. Width and depth limits commute in residual networks. In Interna-
tional Conference on Machine Learning (ICML) , 2023. Cited on page 17.
Soufiane Hayou, Eugenio Clerico, Bobby He, George Deligiannidis, Arnaud Doucet, and Judith
Rousseau. Stable resnet. In International Conference on Artificial Intelligence and Statistics , 2021.
Cited on page 3, 16.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In IEEE international conference on computer
vision (ICCV) , 2015. Cited on page 2, 51.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) , 2016. Cited on page 9.
Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Comput. , 9(1):1–42, 1997. Cited on
page 17.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Training compute-optimal large language models. arXiv:2203.15556 , 2022. Cited on page 17.
Satoki Ishikawa and Ryo Karakida. On the parameterization of second-order optimization effective
towards the infinite width. arXiv preprint arXiv:2312.12226 , 2023. Cited on page 17.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural Tangent Kernel: Convergence and gener-
alization in neural networks. In Advances in Neural Information Processing Systems (NeurIPS) ,
2018. Cited on page 16.
Yiding Jiang*, Behnam Neyshabur*, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantas-
tic generalization measures and where to find them. In International Conference on Learning
Representations (ICLR) , 2020. Cited on page 17.
Jean Kaddour, Linqing Liu, Ricardo Silva, and Matt J Kusner. When do flat minima optimizers work?
Advances in Neural Information Processing Systems (NeurIPS) , 35, 2022. Cited on page 1, 17.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv:2001.08361 , 2020. Cited on page 2, 17.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Cited on page 9, 51, 71.
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware
minimization for scale-invariant learning of deep neural networks. In International Conference on
Machine Learning (ICML) , 2021. Cited on page 1, 3, 17, 37, 39, 48.
12Yann LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. Efficient backprop. In Neural
networks: Tricks of the trade . Springer, 2002. Cited on page 2.
Mufan Li, Mihai Nica, and Dan Roy. The future is log-gaussian: Resnets and their infinite-depth-and-
width limit at initialization. In Advances in Neural Information Processing Systems (NeurIPS) ,
2021. Cited on page 3, 16.
Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and scalable
sharpness-aware minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , 2022. Cited on page 3, 17.
Philip M. Long and Peter L. Bartlett. Sharpness-aware minimization and the edge of stability.
arXiv:2309.12488 , 2023. Cited on page 17.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences , 115(33):E7665–E7671,
2018. Cited on page 16.
Enea Monzio Compagnoni, Luca Biggio, Antonio Orvieto, Frank Norbert Proske, Hans Kersting, and
Aurelien Lucchi. An SDE for modeling SAM: Theory and insights. In Andreas Krause, Emma
Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors,
Proceedings of the 40th International Conference on Machine Learning (ICML) , 2023. Cited on
page 3, 17, 39.
Maximilian Müller, Tiffany Vlaar, David Rolnick, and Matthias Hein. Normalization layers are all
that sharpness-aware minimization needs. Advances in Neural Information Processing Systems
(NeurIPS) , 36, 2024. Cited on page 1, 3, 10, 17, 35, 37, 39, 41, 48, 51, 52.
Radford M. Neal. Priors for Infinite Networks . Springer New York, 1996. Cited on page 16.
Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien
Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.
Advances in Neural Information Processing Systems (NeurIPS) , 35, 2022. Cited on page 3, 16.
Lorenzo Noci, Chuning Li, Mufan Li, Bobby He, Thomas Hofmann, Chris J Maddison, and Dan
Roy. The shaped transformer: Attention models in the infinite depth-and-width limit. Advances in
Neural Information Processing Systems (NeurIPS) , 36, 2024. Cited on page 3, 16.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems (NeurIPS) , 2019. Cited
on page 9, 71.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential
expressivity in deep neural networks through transient chaos. Advances in Neural Information
Processing Systems (NeurIPS) , 29, 2016. Cited on page 16.
David Samuel. (Adaptive) SAM Optimizer (PyTorch). https://github.com/davda54/sam , 2022.
Cited on page 37, 48, 49, 71.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. arXiv:1611.01232 , 2016. Cited on page 16.
Sungbin Shin, Dongyeop Lee, Maksym Andriushchenko, and Namhoon Lee. The effects of
overparameterization on sharpness-aware minimization: An empirical and theoretical analysis.
arXiv:2311.17539 , 2023. Cited on page 17.
Leena Chennuru Vankadara, Jin Xu, Moritz Haas, and V olkan Cevher. On feature learning in
structured state space models. In The Thirty-eighth Annual Conference on Neural Information
Processing Systems (NeurIPS) , 2024. Cited on page 17.
13Nikhil Vyas, Alexander Atanasov, Blake Bordelon, Depen Morwani, Sabarish Sainathan, and Cengiz
Pehlevan. Feature-learning networks are consistent across widths at realistic scales. Advances in
Neural Information Processing Systems (NeurIPS) , 36, 2024. Cited on page 2, 16, 60.
Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. How sharpness-aware minimization minimizes sharpness?
InThe Eleventh International Conference on Learning Representations (ICLR) , 2023. Cited on
page 3, 17.
Kaiyue Wen, Zhiyuan Li, and Tengyu Ma. Sharpness minimization algorithms do not only minimize
sharpness to achieve better generalization. Advances in Neural Information Processing Systems
(NeurIPS) , 36, 2024. Cited on page 3, 17.
Jonathan Wenger, Felix Dangel, and Agustinus Kristiadi. On the disconnect between theory and
practice of overparametrized neural networks. arXiv:2310.00137 , 2023. Cited on page 16.
Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi
Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based
image representation and processing for computer vision. arXiv:2006.03677 , 2020. Cited on
page 51.
Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and generaliza-
tion in deep neural networks. In International Conference on Machine Learning (ICML) , 2020.
Cited on page 16.
Greg Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian processes.
Advances in Neural Information Processing Systems (NeurIPS) , 32, 2019. Cited on page 2, 3, 16,
42.
Greg Yang. Tensor programs iii: Neural matrix laws. arXiv:2009.10685 , 2021. Cited on page 24.
Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks.
InInternational Conference on Machine Learning (ICML) , 2021. Cited on page 2, 3, 5, 6, 8, 10,
16, 18, 20, 21, 29, 31, 32, 33, 41.
Greg Yang and Etai Littwin. Tensor programs ivb: Adaptive optimization in the infinite-width limit.
arXiv:2308.01814 , 2023. Cited on page 2, 3, 16, 17, 19, 41.
Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder,
Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks
via zero-shot hyperparameter transfer. arXiv:2203.03466 , 2022. Cited on page 2, 3, 9, 16, 19, 43,
51, 57, 60, 61, 71.
Greg Yang, James B. Simon, and Jeremy Bernstein. A spectral condition for feature learning.
arXiv:2310.17813 , 2023a. Cited on page 5, 17, 39, 47.
Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs vi: Feature learning in
infinite-depth neural networks. arXiv:2310.02244 , 2023b. Cited on page 3, 5, 16, 17, 43.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. In International Conference on Learning Representations
(ICLR) , 2020. Cited on page 17.
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ,
2022. Cited on page 2, 17.
14Appendices
Appendix Contents.
A Notation 16
B Detailed related work 16
C Definitions 18
D Extensive main results 19
E Proof of main results 23
E.1 Tensor program formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
E.2 The infinite-width limit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E.3 Concluding the proof of all main results . . . . . . . . . . . . . . . . . . . . . . . 31
E.4 Analytic expression of the features after first SAM update . . . . . . . . . . . . . . 33
F Generalizations and further perturbation scaling considerations 35
F.1 Overview over choices of dlandd. . . . . . . . . . . . . . . . . . . . . . . . . . 35
F.2 Other ways to introduce layerwise perturbation scaling . . . . . . . . . . . . . . . 37
F.3 Extension to SAM without gradient normalization . . . . . . . . . . . . . . . . . . 38
F.4 Extension to Adaptive SAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
F.5 Representing general architectures and adaptive optimizers as Tensor Programs . . 41
F.6 Influence of width-dependent weight multipliers on bcd-parameterizations . . . . . 43
F.7 The spectral perspective on µP2. . . . . . . . . . . . . . . . . . . . . . . . . . . 47
G Experimental details 51
H Supplemental experiments 53
H.1 SAM is approximately LL-SAM in µP with global perturbation scaling . . . . . . 53
H.2 Propagating perturbations from the first layer does not inherit SAM’s benefits . . . 54
H.3 Hyperparameter transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
H.4 Gradient norm contributions have negligible effects on generalization performance 64
H.5 SAM with layerwise gradient normalization . . . . . . . . . . . . . . . . . . . . . 65
H.6 Test error over the course of training . . . . . . . . . . . . . . . . . . . . . . . . . 66
15A Notation
Symbol Meaning
n, η, ρ width, learning rate, perturbation radius
ϕ,L,(ξt, yt) activation function, loss function, input and label at time t
∥v∥:=∥v∥2,∥W∥:=∥W∥F2-norm as standard for vectors, Frobenius norm as standard for matrices
∥W∥∗ spectral norm for matrices (also called operator norm)
Wl
t trainable weights at time tin layer l
δWl
t weight updates at time tin layer l
εl
t weight perturbations at time tin layer l
∥vt∥ norm of the rescaled gradient in the perturbation denominator
hl
t,xl
t preactivations and activations at time tin layer l
δxl
t activation updates at time tin layer l
˜δxl
t activation perturbations at time tin layer l
δft,˜δft update/perturbation of the output function at time t
χt=L′(ft(ξt), yt) derivative of loss w.r.t. output function at time t
˜δWl
t=εl
t weight perturbations at time tin layer l(with ˜δfor consistency)
⊙ elementwise multiplication
dzt=θ−1
∇∇zf derivative of output function w.r.t. z∈ {hl, xl}at time t, normalized to Θ(1)
dzSAM,t derivative of perturbed output function w.r.t. perturbed z∈ {˜hl,˜xl}
at time t, normalized to Θ(1)
θ∇ scaling of the activation gradients
θl,˜θl update and perturbation scaling of hl
tandxl
t
θWl,˜θWl update and perturbation scaling of Wl
t˚θ limit scaling; under stability, all considered scalings ˚θ∈ {0,1}
Zzrandom variable distributed according to the limiting distribution
for the entries of the TP vector zspecified by the TP Master Theorem
Table A.1: (Notation) Overview over notation used in the main paper (top) and in the appendix
(bottom).
B Detailed related work
Signal propagation. Our work can be seen as scaling theory with the goal of preventing both
vanishing and exploding signals in forward and backward passes, where the analysis of SAM requires
considering stability of perturbations in each layer as well. In this sense, we build on a rich literature,
often restricted to an analysis at initialization (Schoenholz et al., 2016; Poole et al., 2016; Hanin and
Rolnick, 2018; Xiao et al., 2020). For scaling neural networks to infinite depth, residual connections
have been found to be beneficial for stabilizing signal propagation while retaining expressivity. The
simple1√
L-scaling allows depth-scaling in ResNets and unlocks hyperparameter transfer (Hayou
et al., 2021; Li et al., 2021; Bordelon et al., 2023; Yang et al., 2023b). Noci et al. (2022, 2024)
provide infinite width and depth analyses for Transformers with the goal of preventing rank collapse
and attaining a limit that has behaviour consistent with that of moderately large networks.
Tensor Programs. After kernel-based approaches to understand infinite-width limits of neural
networks (Neal, 1996; Jacot et al., 2018) and applications of mean-field theory (Mei et al., 2018),
the Tensor Program series (Yang, 2019; Yang and Hu, 2021; Yang and Littwin, 2023; Yang et al.,
2022, 2023b) marks the first important break through in the theory of large neural networks. The
framework covers many modern deep learning architectures, optimization algorithms and arbitrary
abc-parameterizations, where each abc-parameterization is essentially defined by a layerwise scaling
of initialization variance and learning rate as a function of network width. Yang and Hu (2021) propose
themaximal update parameterization (µP) and show that it is the unique stable parameterization that
achieves feature learning in all layers in the limit of infinite width. In this framework, training neural
networks with a global learning rate η >0for all layers and with He or LeCun initialization falls
under the category of so called standard parameterization (SP). The neural tangent parameterization
(NTP), studied in the neural tangent kernel literature, differs but does not achieve feature learning
in any layer, and is therefore less useful to describe the behaviour of finite width networks than µP
(Wenger et al., 2023; Vyas et al., 2024). Yang and Littwin (2023) characterize stable learning with
adaptive optimizers at infinite width into a feature learning versus a (nonlinear) operator regime. SAM
16is not covered by the update rule definition in Yang and Littwin (2023) since the nested application
of the gradient w.r.t. the weights is not a coordinatewise optimizer anymore. Yang et al. (2023a)
show that µP is equivalent to the spectral scaling conditions on the weights ∥∆Wl∥= Θ(p
nl/nl−1)
and∥∆Wl∥= Θ(p
nl/nl−1). Hence Bernstein et al. (2020) would have achieved their goal of
an optimizer with automatic update scaling, if they had normalized by the spectral instead of the
Frobenius norm and multiplied byp
fan_out /fan_in in each layer. While recent works have
considered joint limits of infinite width and depth (Yang et al., 2023b; Hayou and Yang, 2023), the
data distribution has not been taken into account in Tensor Program literature. The study of scaling
laws of jointly scaling model size, data set size and training time has predominantly been empirical
(Kaplan et al., 2020; Zhai et al., 2022; Hoffmann et al., 2022; Besiroglu et al., 2024). Developing
theory to inform Pareto optimal trade offs in a principled manner constitutes an important direction
for future work.
As an example of scaling theory for second order optimization, Ishikawa and Karakida (2023) derive
µP for KFAC and Shampoo. This scaling rule differs from µP for SGD. Similarly, Vankadara et al.
(2024) show that maximal updates are achieved by another different scaling rule for non-standard
architectures like structured state space models.
Sharpness Aware Minimization. Sharpness aware minimization (SAM) (Foret et al., 2021) has
shown to be extremely effective and robust in improving generalization performance across a wide
range of architectures and settings (Chen et al., 2021; Kaddour et al., 2022). SAM was motivated
as an inductive bias towards flatter minima and it has been understood to have an gradient-norm
adaptive edge of stability at which it drifts towards minima with smaller spectral norm of the Hessian
(Long and Bartlett, 2023; Bartlett et al., 2023). However a full understanding of why SAM works so
well remains elusive. While correlations between flatness and generalization have been observed in
some settings (Hochreiter and Schmidhuber, 1997; Jiang* et al., 2020), other studies have questioned
the usefulness of sharpness as a measure for generalization, especially for modern architectures
(Dinh et al., 2017; Andriushchenko et al., 2023b; Wen et al., 2024). Applying SAM on only the
normalization layers often even improves generalization in vision tasks depsite increasing sharpness
(Müller et al., 2024). Adaptive SAM (ASAM) (Kwon et al., 2021) is a variant of SAM derived from
a sharpness definition that is invariant to weight rescalings with respect to a chosen normalization
operator that leave the output function invariant. The results in Müller et al. (2024) suggest that two
of the most promising normalization operators are elementwise normalization Tl
w(x) =|Wl| ⊙x
and layerwise normalization Tl
w(x) =∥Wl∥F·x. We state the resulting update rules and a scaling
analysis in Appendix F.4. A variant of SAM that is often studied theoretically because of its
simplicity does not normalize the gradient of the perturbation. Our theory covers this variant too
(Appendix F.3), but Dai et al. (2024) argue that normalizing the gradients for the perturbation is crucial.
Monzio Compagnoni et al. (2023) find that unnormalized SAM gets stuck around saddles while SAM
slowly escapes through additional Hessian-induced noise. This suggests that the additional effort of
analysing the original SAM update rule with gradient normalization is necessary for practically useful
theory. Dauphin et al. (2024) draw connections between SAM and other second order optimizers
like gradient penalties and weight noise. They show that SAM is able to effectively use second
order information implicitly using ReLU, whereas the other two methods close the gap to SAM
when using GeLU since they require the localized second order information that GeLU provides in
contrast to ReLU. Wen et al. (2023) show that worst-case, ascent and average case sharpness are
biased towards minimizing the maximal eigenvalue, minimal non-zero eigenvalue and trace of the
Hessian, respectively. With an architecture-agnostic analysis, they show that 1-SAM minimizes the
trace of Hessian like average-case sharpness, for small enough ηandρ. Similarly, the theoretical
results by Andriushchenko and Flammarion (2022) rely on the assumption that learning rate ηand
perturbation radius ρare chosen sufficiently close to 0. Arguably, the empirically optimal choice of η
andρlies outside of this gradient flow-like regime and has qualitatively different properties (see e.g.
edge of stability literature (Cohen et al., 2020; Arora et al., 2022)).
Scaling theory for SAM. Shin et al. (2023) suggest that the generalisation improvement by SAM
continues to increase with growing overparametrization. This corroborates empirical observations
that performance monotonically improves with scale, and understanding the infinite-width limit is
not only of theoretical interest but entails immediate practical benefits.
Liu et al. (2022) introduce Look-LayerSAM with layerwise perturbation scaling for preserving good
performance under large batch training for enhanced training parallelization. They use LAMB (You
et al., 2020) for layerwise learning rate scaling for large batch training. The update scaling strategy in
17these kinds of algorithms follows
Wl
t+1=Wl
t−ηtϕ(∥Wl
t∥F)∇WlL
∥∇WlL∥F,
with some ϕ:R+→R+and where ∇WlLmay be replaced by Adam’smt√vt+ε. In practice, often
simple functions like ϕ(x) = max( c,min(x, C))orϕ(x) =xare used. The idea is to ensure that
the update has the same order of magnitude as the weights. Look-LayerSAM follows an analogous
approach for layerwise perturbation scaling. A derivation of µP for LAMB could also yield feature
learning in all layers in the infinite-width limit as well as hyperparameter transfer. It certainly requires
layerwise learning rate scaling. In the case ϕ(x) =x, following a heuristic scaling derivation as
in Appendix F.4 leads to layerwise learning rate scalings η1=ηL+1= Θ(1) andηl= Θ( n−1/2)
for hidden layers l∈[2, L]. With a bounded function like ϕ(x) = max( c,min(x, C)), the scalings
become η1= Θ( n1/2),ηL+1= Θ( n−1/2)andηl= Θ(1) for hidden layers l∈[2, L]. We leave a
closer investigation of feature learning and hyperparameter transfer with LAMB and Look-LayerSAM
in SP and µP to future work.
C Definitions
In this section, we collect all definitions that do not appear in the main text. With minor modifications,
we adopt all definitions from Yang and Hu (2021). If not stated otherwise, limits are taken with
respect to width n→ ∞ .
Definition C.1 (Big-O Notation ).Given a sequence of scalar random variables c={cn∈R}∞
n=1,
we write c= Θ ( n−a)if there exist constants A, B such that for almost every instantiation of
c={cn∈R}∞
n=1, fornlarge enough, An−a≤ |cn| ≤Bn−a. Given a sequence of random vectors
x={xn∈Rn}∞
n=1, we say xhas coordinates of size Θ (n−a)and write x= Θ ( n−a)to mean the
scalar random variable sequenceq
∥xn∥2/n
nisΘ (n−a). For the definition of c=O(n−a)
andc= Ω( n−a), adapt the above definition of c= Θ( n−a)by replacing An−a≤ |cn| ≤Bn−a
with|cn| ≤Bn−aandAn−a≤ |cn|, respectively. We write xn=o(n−a)ifna·q
∥xn∥2/n→0
almost surely. ◀
Definition C.2 (Training routine ).Atraining routine is a combination of base learning rate η≥0,
perturbation radius ρ≥0, training sequence {(ξt, yt)}t∈Nand a continuously differentiable loss
function L(f(ξ), y)using the SAM update rule with layerwise perturbation scaling (LP). ◀
In addition to the stability conditions from the corresponding SGD result, we demand that the
activation perturbations do not blow up. Otherwise the perturbations would strictly dominate both the
initialization and the updates which makes the perturbation too strong and is avoided in practice.
Definition C.3 (Stability ).We say a bcd-parametrization of an L-hidden layer MLP is stable if
1. For every nonzero input ξ∈Rdin\{0},
hl
0, xl
0= Θ ξ(1),∀l∈[L],andEf0(ξ)2=Oξ(1),
where the expectation is taken over the random initialization.
2. For any training routine, any time t∈N,l∈[L],ξ∈Rdin, we have
hl
t(ξ)−hl
0(ξ), xl
t(ξ)−xl
0(ξ) =O∗(1),and ft(ξ) =O∗(1),
where the hidden constant in O∗can depend on the training routine, t,ξ,land the initial
function f0.
3.For any training routine, any time t∈N0,l∈[L],ξ∈Rdin, for the perturbed
(pre-)activation ˜hl
t:=hl(˜Wt),˜xl
t:=xl(˜Wt)and output function ˜ft(˜Wt)we have
˜hl
t(ξ)−hl
t(ξ),˜xl
t(ξ)−xl
t(ξ) =O∗(1),and ˜ft(ξ) =O∗(1),
where the hidden constant in O∗can depend on the training routine, t,ξ,land the initial
function f0.
◀
18Definition C.4 (Nontriviality ).We say a bcd-parametrization is trivial if for every training routine,
ft(ξ)−f0(ξ)→0almost surely for n→ ∞ , for every time t >0and input ξ∈Rdin. Otherwise the
bcd-parametrization is nontrivial . ◀
Definition C.5 (Feature learning ).We say a bcd-parametrization admits feature learning in the l-th
layer if there exists a training routine, a time t >0and input ξsuch that xl
t(ξ)−xl
0(ξ) = Ω ∗(1),
where the constant may depend on the training routine, the time t, the input ξand the initial function
f0but not on the width n. ◀
Definition C.6 (Vanishing perturbations ).Letl∈[L]. We say that a stable bcd-parametrization has
vanishing perturbations in the l-th layer if for any training routine, t∈N0andξ∈Rdin, it holds that
˜xl
t−xl
t=o(1), and it has vanishing perturbations in the output if for any training routine, t∈N0
andξ∈Rdinit holds that ˜δft(ξ) :=f˜Wt(ξ)−fWt(ξ) =o(1). ◀
Definition C.7 (Perturbation nontriviality ).Letl∈[L]. We say that a stable bcd-parametrization
isperturbation nontrivial with respect to the l-th layer if and only if it does not have vanishing
perturbations in the l-th layer. A stable bcd-parametrization is perturbation nontrivial with respect to
the output if it does not have vanishing perturbations in the output. ◀
Definition C.8 (Effective perturbations ).Letl∈[L+ 1]. We say that a stable bcd-parametrization
effectively perturbs the l-th layer if there exists a training routine, t∈Nandξ∈Rdinsuch that
˜δWl
t˜xl−1
t(ξ) = Θ(1) where ˜δWl
tis defined in (LP) and ˜x0
t=x0
t=ξt. ◀
Definition C.9 (σ-gelu ).Define σ-gelu to be the function x7→x
2 
1 + erf 
σ−1x
+σe−σ−2x2
2√π.◀
In order to apply the Tensor Program Master Theorem, all Nonlin and Moment operations in the
NE⊗OR⊤program, which do not only contain parameters as inputs, are required to be pseudo-
Lipschitz in all of their arguments. For training with SGD, this is fulfilled as soon as ϕ′is pseudo-
Lipschitz. Both tanh as well as σ-gelu fulfill this assumption.
Definition C.10 (Pseudo-Lipschitz ).A function f:Rk→Ris called pseudo-Lipschitz of degree d
if there exists a C >0such that |f(x)−f(y)| ≤C∥x−y∥(1 +Pk
i=1|xi|d+|yi|d). We say fis
pseudo-Lipschitz if it is so for any degree d. ◀
D Extensive main results
Using the formal definitions from Appendix C, here we provide the full formal statements of all of
our main theoretical results together with further details and implications. The proof of all statements
is provided in Appendix E. Since SAM evaluates the gradients on perturbed weights, it is not covered
by the update rule definition in Yang and Littwin (2023) and an infinite-width analysis requires
explicitly deriving the corresponding N E⊗OR⊤program, scalings and infinite-width limits.
Recall that our definition of bcd-parameterizations extends abc-parameterizations by setting the
maximal perturbation scaling to n−dand allowing relative downweighting n−dlof the global scaling
in each layer l. The perturbation scaling does not affect the choice of layerwise initialization variance
scalings bland the layerwise learning rate scalings cl. Common bc-parametrizations for SGD are
summarized in Table D.1. SAM with SGD as a base optimizer requires the same scalings. Similarly,
SAM with Adam as a base optimizer requires the same scalings as Adam (Yang et al., 2022, Table 3).
Recall that, for convenience, we require width-independent denominator scaling ∥vt∥= Θ(1) of the
scaled gradient for the perturbation (LP), which imposes the constraints
d1≥1/2−min(bL+1, cL+1), d l≥1−min(bL+1, cL+1)forl∈[2, L], d L+1≥1/2.(D.1)
All (pre-)activation and function outputs can be thought of as outputs given a fixed input ξ∈Rdin\{0}
withdin∈Nfixed, e.g. ft:=fWt:=fWt(ξ). For the perturbed weights we write ˜Wt:=Wt+˜δWt,
with˜δWtdefined in (LP) asεl
t. Here we write weight perturbations as ˜δWl
tinstead of εl
tto show the
resemblance to weight updates δWl
t. Perturbed activations and function outputs at time tare written
as˜xl
t(ξ) =xl
˜Wt(ξ)and˜ft(ξ) =f˜Wl
t(ξ). Recall that for all of the results in this section we make the
following smoothness assumption on the activation function.
Assumption 1 (Smooth activation function ).The used activation function is either tanh orσ-gelu
forσ >0sufficiently small. ◀
19We define the maximal feature update scale of a bcd-parameterization
r:= min( bL+1, cL+1, d+dL+1) +L
min
l=1(cl−I(l̸= 1)) . (D.2)
as well as the maximal feature perturbation scale of a bcd-parameterization
˜r:= min( bL+1, cL+1) +d+L
min
l=1(dl−I(l̸= 1)) . (D.3)
Stability requires the constraints (a-c) from SGD and additional perturbation stability constraints
(d-e) that include the layerwise perturbation scales {dl}l=1,...,L +1.
Theorem D.2 (Stability characterization ).Abcd-parametrization is stable if and only if all of the
following are true:
(a) (Stability at initialization, hl
0, xl
0= Θ(1) for all l,f0=O(1))
b1= 0,bl= 1/2forl∈[2, L]andbL+1≥1/2.
(b) (Features do not blow up during training, i.e. ∆xl
t=O(1)for all l)
r≥0.
(c) (Output function does not blow up during training, i.e. ∆WL+1
txL
t, WL+1
0∆xL
t=O(1))
cL+1≥1andbL+1+r≥1.
(d) (Feature perturbations do not blow up, i.e. ˜δxl
t=O(1)for all l)
˜r≥0.
(e)(Output function perturbations do not blow up during training, i.e. ˜δWL+1
t˜xL
t, WL+1
t˜δxL
t=
O(1))
d+dL+1≥1andbL+1+ ˜r≥1.
The nontriviality and feature learning characterizations from SGD remain unaltered. This is because in
the definition of r, it holds that d+dL+1≥1(from perturbation stability), and min(bL+1, cL+1)≤1
already had to hold for nontriviality in SGD, so that stable perturbation scaling does not affect r.
Theorem D.3 (Nontriviality characterization ).A stable bcd-parametrization is nontrivial if and
only if cL+1= 1ormin(bL+1, cL+1) +r= 1.
As for nontriviality, the conditions under which a stable, nontrivial parameterization is feature learning
in the infinite-width limit are decoupled from the choice of perturbation scalings {dl}l∈[L+1]∪ {d}.
Hence the conditions are the same as for SGD. Below we provide a slightly refined result in terms
of the maximal feature update scale rl0of abcd-parameterization up to layer l0(as provided in the
Appendix of Yang and Hu (2021)).
Theorem D.4 (Feature learning characterization ).For any l0∈[L], the following statements are
equivalent:
(a) A stable, nontrivial bcd-parametrization admits feature learning in layer l0.
(b) A stable, nontrivial bcd-parametrization admits feature learning in layer lfor all l≥l0.
(c)rl0:= min( bL+1, cL+1, d+dL+1) + minl0
m=1(cm−I(m̸= 1)) = 0 .
Consequently, a stable, nontrivial bcd-parametrization admits feature learning (at least in the last
layer activations) if and only if r= 0.
Remark D.5 (Effective feature learning ).As for perturbations, feature learning in later layers can
be caused by weight updates in earlier layers that propagate through the network. One could demand
effective feature learning in the l-th layer as δWl
txl−1
t= Θ(1) and it would occur if and only if
min(bL+1, cL+1, d+dL+1) +cl−I(l̸= 1) = 0 . ◀
As for nontriviality, perturbation nontriviality in the output is attained if the constraints for ˜δWL+1
t˜xL
t
orWl
t˜δxL
tare exactly satisfied.
Theorem D.6 (Perturbation nontriviality characterization ).Letl∈[L]. A stable bcd-
parametrization is perturbation nontrivial with respect to the l-th layer if and only if
˜rl:= min( bL+1, cL+1) +d+l
min
m=1(dm−I(m̸= 1)) = 0 .
A stable bcd-parametrization is perturbation nontrivial with respect to the output if and only if
d+dL+1= 1ormin(bL+1, cL+1) + ˜r= 1.
20The converse formulation of the perturbation-nontriviality results characterizes the regime of vanish-
ing perturbations.
Corollary D.7 (Vanishing perturbation characterization ).For any l0∈[L], the following state-
ments are equivalent:
(a) A stable bcd-parametrization has vanishing perturbations in layer l0.
(b) A stable bcd-parametrization has vanishing perturbations in layer lfor all 1≤l≤l0.
(c)˜rl0:= min( bL+1, cL+1) +d+ minl0
m=1(dm−I(m̸= 1)) >0.
A stable bcd-parametrization has vanishing perturbations with respect to all layers and the output
function if and only if dL+1>1/2and˜r >max(0 ,1−bL+1). This case reduces to the results in
Yang and Hu (2021).
For perturbation nontriviality it suffices that the perturbation in any of the previous layers is scaled
correctly. For effective perturbations, we need the correct scaling in exactly that layer.
Theorem D.8 (Effective perturbation characterization ).Forl∈[L], a stable bcd-parametrization
effectively performs SAM in the l-th layer if and only if min(bL+1, cL+1) +d+dl−I(l̸= 1) = 0 .
A stable bcd-parametrization effectively performs SAM in the last layer if and only if d+dL+1= 1.
The above understanding of all update and perturbation scalings allows us to extract the most important
consequences of different choices of perturbation scaling on the learning dynamics. Beyond vanishing
hidden layer perturbations, the following theorem shows that the joint gradient norm ∥vt∥can be
approximated efficiently without an additional backward pass under global perturbation scaling.
Theorem D.9 (Global Perturbation Scaling ).Given any stable bcd-parametrization {bl}l∈[L+1]∪
{cl}l∈[L+1]∪ {dl}l∈[L+1]∪ {d}. The parametrization performs updates in the original gradient
direction if and only if dl=Cfor all l∈[L+ 1] for some C∈R. In this case, the parametrization
has vanishing perturbations in all hidden layers l∈[L], and the last layer l=L+ 1is effectively
perturbed if and only if d= 1/2. IfbL+1>1/2(as in µP), the gradient norm is dominated by the
last layer and simplifies to,
∥vt∥= Θ( n1/2−C),∥vt∥ − L′(ft(ξt), yt)∥xL
t∥=o(n1/2−C).
One might suspect that it is desirable to let all layers contribute non-vanishingly to the gradient norm
in the denominator of (LP) . The following proposition shows that this should be avoided with our
definition of bcd-parameterizations. Of course, if we add even more hyperparameters by decoupling
numerator and denominator scalings, we can set all contributions to Θ(1) , which is what we do in
Appendix F.7.
Proposition D.10 (Balancing gradient norm contributions ).Given any stable bcd-parametrization
{bl}l∈[L+1]∪ {cl}l∈[L+1]∪ {dl}l∈[L+1]∪ {d}. If all layers contribute to the gradient norm non-
vanishingly in the limit, i.e. ∥vl
t∥= Θ(∥vt∥)for all l∈[L+ 1], t∈N0, then the parametrization
has vanishing perturbations in all hidden layers l∈[L]. Such a parametrization effectively performs
SAM in the last layer l=L+ 1if and only if d= 1/2.
The following theorem provides the unique correct perturbation scaling for any stable bc-
parameterization with bL+1≥1.
Theorem D.11 (Perturbation Scaling Choice for Effective Perturbations ).Given any stable
bcd-parametrization {bl}l∈[L+1]∪ {cl}l∈[L+1]∪ {dl}l∈[L+1]∪ {d}. IfbL+1<1, then there does
not exist a stable choice of {dl}l∈[L+1]∪ {d}that achieves effective perturbations before the last
layer. If bL+1≥1, then up to the equivalence d′
l=dl+C,C∈R,∀l∈[L+ 1], the unique stable
choice {dl}l∈[L+1]∪ {d}with effective perturbations in all layers l∈[L+ 1] is given by
d=−1/2, d l=

1/2−min(bL+1, cL+1)l= 1,
3/2−min(bL+1, cL+1)l∈[2, L],
3/2 l=L+ 1.(D.4)
In this parameterization, the first layer dominates the gradient norm as
∥vt∥= Θ(1) ,∥v1
t∥ − ∥ vt∥= Θ( n−1/2).
21Definition SP SP (stable) NTP (stable) µP
blN(0, n−2bl)0 l= 1,
1/2l≥2.0 l= 1,
1/2l≥2.0 l= 1,
1/2l≥2.

0 l= 1,
1/2l∈[2, L],
1, l =L+ 1.
cl LRηn−cl 0 10l= 1,
1l≥2.

−1l= 1,
0 l∈[2, L],
1 l=L+ 1.
r Equation (D.2) -1 1/2 1/2 0
Stable? ✓ ✓ ✓
Nontrivial? ✓ ✓ ✓
Feature learning? ✓
Table D.1: (bc-parametrizations) Overview over common implicitly used bc-parametrizations
for training MLPs without biases in standard parametrization (SP), standard parametrization with
maximal stable nonadaptive LR c= 1 (SP (stable)), neural tangent parametrization (NTP) and
maximal update parametrization ( µP).
Definition Naive Global (stable) Effective
d ρn−d0 1/2 −1/2
dl n−dl∇WlLt 1/2 1 /2

1/2−c∇l= 1,
3/2−c∇l∈[2, L],
3/2 l=L+ 1.
˜r Equation (D.3) c∇−1/2 c∇ 0
Stable? ✗ ✓ ✓
Last layer effectively perturbed? ✗ ✓ ✓
All layers effectively perturbed? ✗ ✗ ✓
Table D.2: (Perturbation scalings) Overview over important choices of the global perturbation
scaling ρn−dand the layerwise perturbation scalings n−dlfor training MLPs without biases with
SAM: Naive scaling without width dependence (Naive), maximal stable global scaling along the
original gradient direction (Global) and the unique scaling that achieves effective perturbations in
all layers (Effective). An extensive overview that characterizes all possible choices of perturbation
scaling is provided in Appendix F.1. Recall the gradient scaling c∇:= min( bL+1, cL+1).
Table D.2 summarizes the consequences of Theorem D.11. Together with Theorem D.11, the
following proposition suggests that bL+1= 1is a good choice. However bL+1>1can also induce
effective perturbations, as long as danddL+1are chosen correctly.
Proposition D.12 (Effects of last-layer initialization bL+1on all perturbations ).If a stable bcd-
parametrization with min(bL+1, cL+1)≤1is perturbation nontrivial with respect to any hidden
layer l∈[L], it is also perturbation nontrivial with respect to the output.
Lastly, the following proposition shows that effective perturbations from the first layer propagate
through the entire network.
Proposition D.13 (Perturbations propagate through the forward pass ).All stable bcd-
parametrizations with d1=−min(bL+1, cL+1)−deffectively perturb the first layer and are
perturbation nontrivial in all layers.
Remark D.14 (Efficiency gains ).The above results may be used for efficiency gains. Given
any stable bcd-parametrization, we can compute the maximal layer l0such that ˜rl0>0, and
in wide networks do not have to compute SAM perturbations before layer l0+ 1; as soon as
bL+1>1/2(as for µP), the gradient norm for the SAM update rule is approximately given by
∥∇Lt∥ ≈ L′(ft(ξt), yt)∥xL
t∥, which can directly be computed without an additional backward pass.
The practical recommendation from our experiments however is to either use µP2or to completely
abstain from perturbations. ◀
Remark D.15 (SAM without gradient normalization ).For the SAM update rule without gradient
normalization simply set d= 0 and remove the gradient norm constraints (D.1) to arrive at the
22adapted N E⊗OR⊤program and bcd-constraints. Note that standard parametrization gets even more
unstable without dividing by ∥∇L∥= Θ( n1/2), now requiring dL+1≥1for stability. Similar to
the previous results, this shows that unawareness of bcd-parametrizations requires strongly scaling
down ρfor stability, while vasting computation on vanishing perturbations before the last layer. More
details can be found in Appendix F.3. ◀
E Proof of main results
In this section we derive the NE⊗OR⊤program that corresponds to training a MLP without biases
with SAM. For simplicity and clarity of the proof, we prove the one-dimensional case din= 1,
dout= 1, but an extension to arbitrary but fixed din,doutis straightforward. Recall Assumption 1
that allows us to apply the Tensor Program Master Theorem and explicitly state the infinite-width
limit of training MLPs with SAM in Appendix E.2.
E.1 Tensor program formulation
E.1.1 Tensor Program initialization
We initialize the matrices W2
0, . . . , WL
0as(Wl
0)αβ∼ N(0,1/n), which absorbs bl= 1/2.
We initialize the input layer matrix W1
0∈Rn×1and normalized output layer matrix ˆWL+1
0 =
WL+1
0nbL+1∈R1×nas(W1
0)α,(ˆWL+1
0)α∼ N(0,1), as initial vectors should have a distribution
that is Θ(1) .
In the NE⊗OR⊤formulation, we write all quantities as θzz, where θzdenotes their scaling nCfor
some C∈Randztherefore has a Θ(1) distribution. The stability, nontriviality and feature learning
conditions then stem from requiring either θz→0orθz= 1depending on zand its desired scale.
E.1.2 First forward pass
We denote a definition of a Tensor Program (TP) or NE⊗OR⊤computation as :=. Compared to MLPs
trained with SGD nothing changes in the first forward pass,
h1
0(ξ) :=W1
0ξ(NL), xl
0:=ϕ(hl
0)(NL), hl+1
0:=Wl+1
0xl
0.(MatMul)
In the case of MuP, f0(ξ) =WL+1
0xL
0(ξ)→0defines a scalar in the TP.
Observe the scalings x1
0= Θ( h1
0) = Θ( n−b1), xl
0= Θ( hl
0) = Θ( n1/2−bl)forl∈[2, L]due to CLT,
independence at initialization and xl
0= Θ( hl
0) = Θ(1) by stability. Hence stability at initialization
inductively requires b1= 0,bl= 1/2forl∈[2, L]andbL+1≥1/2.
E.1.3 First backward pass
The chain rule of the derivative remains the same, we just evaluate on different weights compared to
standard SGD. We denote the adversarially perturbed weights by ˜Wl
tand the normalized perturbations
by˜δWl
t. Before computing the updates we have to compute a full backward pass to determine these
perturbed weights for each layer, and then compute a forward pass with these perturbed weights to
compute the perturbed preactivations ˜hl
tthat we will need for computing the SAM update. Therefore
theNE⊗OR⊤program for SAM maintains a perturbed copy of all preactivations, activations, last-
layer weights and logits just for computing the updates of the actual parameters.
Under MuP, the loss derivative with respect to the function remains χ0:=L′(f0(ξ0), y0)→◦χ0:=
L′(0, y0). For the weight perturbation, we need to perform a SGD backward pass,
dxL
0:=ˆWL+1
0, dhl
0:=dxl
0⊙ϕ′(hl
0), dxl−1
0:= (Wl
0)Tdhl
0,
where dz:=θ−1
∇∇zf. For SGD (and for SAM, as we will see later) all gradients have scaling
θ∇:=n−bL+1in the first step, whereas we overload the notation θ∇:=n−min(bL+1,cL+1)for all
later steps. For clarity of presentation assume bL+1≥cL+1here, the other case follows analogously.
For the first step this can be understood from
∇xLf0=WL+1
0= Θ( n−bL+1),∇hLf0=∇xLf0⊙ϕ′(hL
0) = Θ( n−bL+1),
23since hL
0= Θ(1) by the stability assumption, and this scale Θ(n−bL+1)propagates through all layers
via the chain rule and remains stable in later backward passes. For hidden layer gradients, observe
that
∇xL−1ft= ( WL
t)T∇hLft= (WL
0+ ∆WL
t)T∇hLft
= Θ 
(WL
0)T∇hLft−n−cLt−1X
s=0((∇hLfs)T∇hLft)xL−1
s!
= Θ( n1−2bLθ∇−n−cLθ2
∇n) = Θ( θ∇),
where first term’s scale stems from the products (WL
0)TWL
0v= Θ( n1−2bLv)due to Yang (2021),
bL= 1/2for stability at initialization and bL+1+cL≥1for update stability during training ( r≥0).
If we allowed the second term to strictly dominate, the gradient scale would explode iteratively in the
backward pass.
The gradient norm. Before computing the weight perturbations, we need to compute the gradient
norm for the SAM update. The gradient norm at time tin each layer l∈[2, L]is given by the scalar,
θ−2
∇∂Lt
∂Wl2
=nX
i,j=1 
χt(dhl
t)i(xl−1
t)j2=χ2
t∥dhl
t(xl−1
t)T∥2
F=χ2
t 
(dhl
t)Tdhl
t 
(xl−1
t)Txl−1
t
,
where χt=L′(ft(ξt), yt)and we used ∂hl/∂Wl
ij= (xl−1
jδik)k=1,...,n.
Hence the gradient norm of all weights jointly is given by the unnormalized scalar
∥∇wLt∥2=χ2
t 
nθ2
∇(dh1
t)Tdh1
t
n(ξT
tξt) +LX
l=2n2θ2
∇(dhl
t)Tdhl
t
n(xl−1
t)Txl−1
t
n+n(xL
t)TxL
t
n!
,
(E.1)
with scaling θ2
∥∇∥= Θ( n2θ2
∇+n) = Θ( n), because stability at initialization requires bL+1≥1/2
so that n2θ2
∇≤n. Note that the first layer contributes vanishingly to the gradient norm, the hidden
layer gradients only if bL+1= 1/2(equivalently f0= Θ(1) ) and the last-layer activations always
in dominating order. So in µP, in the limit, ∥∇wLt∥=L′(ft(ξt), yt)∥xL
t∥. This means that the
unscaled gradient always aligns with the last-layer activation. For learning in µP, this dominance is
corrected by the layerwise learning rates.
The squared norm of the rescaled gradient is given by
∥vt∥2=χ2
t
nθ2
∇n−2d1(dh1
t)Tdh1
t
n(ξT
tξt) (E.2)
+LX
l=2n2θ2
∇n−2dl(dhl
t)Tdhl
t
n(xl−1
t)Txl−1
t
n+nn−2dL+1(xL
t)TxL
t
n
,
with scaling θ2
v= Θ( n1−2d1θ2
∇+PL
l=2n2−2dlθ2
∇+n1−2dL+1). For simplicity, set θv= 1. This
raises the constraints n1−2d1θ2
∇≤1,n2−2dlθ2
∇≤1forl∈[2, L]andn1−2dL+1≤1, which can be
rewritten as
d1≥1/2−min(bL+1, cL+1), d l≥1−min(bL+1, cL+1)forl∈[2, L], d L+1≥1/2,
where at least one equality is demanded to hold in order to attain θv= 1. If one of the equalities
holds, the respective layer contributes to the norm non-vanishingly in the limit.
Thus, applying the square root and dividing by θv= 1the square root of (E.2) defines a normalized
TP scalar.
Perturbations. Stability implies that also the perturbed (pre-)activations and output function remain
Θ(1) andO(1)respectively. Otherwise a SAM training step would induce blowup in the updates.
We call this weaker property of just the perturbations perturbation stability .
Definition E.1 (Perturbation stability) .We call a bcd-parametrization perturbation stable if and only
if˜hl
t,˜xl
t= Θ(1) for all l∈[L]andt∈Nand˜δft=O(1)for all t∈N. ◀
24Mathematically we get the normalized weight perturbations for l∈ {2, . . . , L },
˜δWL+1
0:=ρ χ0xL
0
∥v0∥,˜δWl
0=ρ χ0dhl
0(xl−1
0)T
∥v0∥,˜δW1
0=ρ χ0dh1
0ξT
0
∥v0∥,
which scale as ˜θL+1:=˜θWL+1:=n−(d+dL+1),Θ(n(d+dl)−bL+1)andΘ(n−(d+d1)−bL+1)respec-
tively. But the NE⊗OR⊤program computation rules do not allow to compute matrices ˜δWl
0, l∈[L],
therefore we use the weight updates to directly compute the preactivation and activation changes
analogous to the t-th forward pass. For all t≥0, we write
˜hl
t=hl
t+˜θl˜δhl
t, ˜xl
t=xl
t+˜θl˜δxl
t,
with the perturbations for l∈[2, L],
˜δh1
0(ξ) := +ρχ0(ξT
0ξ)dh1
0
∥v0∥,
˜δxl
t:= ˜θ−1
l(ϕ(hl
t+˜θl˜δhl
t)−ϕ(hl
t)),
˜θl˜δhl
0:= ˜θl−1Wl
0˜δxl−1
0+ (˜Wl
0−Wl
0)˜xl−1
0
= ˜θl−1Wl
0˜δxl−1
0+ρ˜θWlχ0
∥v0∥(xl−1
0)T˜xl−1
0
ndhl
0,
which defines a NonLin operation with the vectors Wl
0˜δxl−1
0anddhl
0and everything else treated
as scalars, and with first backward pass scalings ˜θW1:=n−(d+d1)θ∇,˜θWl:=n1−(d+dl)θ∇and
˜θl:= max( ˜θl−1,˜θWl) = maxl
m=1˜θWm, where we used that ˜xl−1
0= Θ(1) due to perturbation
stability. Note that these scalings may implicitly increase when t >0since θ∇=n−bL+1gets
replaced by θ∇=n−min(bL+1,cL+1).
The activation perturbations can then simply be defined via the NonLin operation,
˜δxl
0:=˜θ−1
l(ϕ(hl
0+˜θl˜δhl
0)−ϕ(hl
0)),
with the same scaling as ˜δhl
0.
The perturbation of the scalar output function can simply be defined via the NonLin operation,
˜δf0:=˜WL+1
0˜xL
0−WL+1
0xL
0=˜θ′
L+1˜δWL+1
0˜xL
0
n+˜θ′
L∇ˆWL+1
0˜δxL
0
n,
with˜θ′
L+1:=n˜θWL+1and˜θ′
L∇:=nθ∇˜θL.
SAM Update. Finally, we can compute the SAM updates as follows. In the case min(bL+1, cL+1)≤
d+dL+1the weight perturbation scale is dominated by the weight scale, so that
dxL
SAM, 0:=ˆWL+1
0+˜θ(L+1)/∇˜δWL+1
0,
with˜θ(L+1)/∇:=˜θL+1/θ∇≤1, whereas if min(bL+1, cL+1)> d+dL+1we write
dxL
SAM, 0:=˜θ∇/(L+1)ˆWL+1
0+˜δWL+1
0,
withθ∇/(L+1):=θ∇/˜θL+1≤1. In any case, the scaling of dxL
SAM, 0and all other SAM gradients is
θSAM := max( θ∇, n−(d+dL+1)) =n−min(bL+1,cL+1,d+dL+1). The other SAM gradients are given
by
dhl
SAM, 0:= dxl
SAM, 0⊙ϕ′(˜hl
0)
dxl−1
SAM, 0:= ( ˜Wl
0)Tdhl
SAM, 0= (Wl
0+˜θWl˜δWl
0)Tdhl
SAM, 0
= ( Wl
0)Tdhl
SAM, 0+ρθSAM˜θWlχ0
∥v0∥(dhl
0)Tdhl
SAM, 0
nxl−1
0.
where the last line define a NonLin operation in the vectors (Wl
0)Tdhl
SAM,t andxl−1
0and everything
else treated as scalars. Consequently, ∇hl
0f|˜W0is of the same scale as ∇xl
0f|˜W0and∇xl−1
0f|˜W0is
of the scale max( θSAM,˜θWlθSAM) =θSAM since ˜θWl≤1is required for perturbation stability.
25Note that for SAM’s weight updates the loss derivative is also evaluated on the perturbed weights,
˜χ0:=L′(˜WL+1
0˜xL
0, y0).
Constraints on the output function. Assuming ˜xL
0= Θ(1) (perturbation stability), we get ˜χ0=
O(1)if and only if ˜δWL+1
0=O(n−1)if and only if d+dL+1≥1.
We have ˜χ0= Θ(1) if and only if ˜f0=˜WL+1
0˜xL
0= Θ(1) . This can either be caused by changes
in the last-layer weights, by non-vanishing initial function WL+1
0xL
0(if and only if bL+1= 1/2) or
byWL+1
0˜δxL
0= Θ(1) , which holds if and only if bL+1+ ˜rL= 1(analogously, WL+1
0˜δxL
0=O(1)
if and only if bL+1+ ˜rL≥1). The first case requires ˜δWL+1
0= Θ( n−1), since ˜δWL+1
0 and˜xL
0
are highly correlated. ˜δWL+1
0= Θ( n−1)is fulfilled if and only if d+dL+1= 1(the analogue to
cL+1≥1for stability and cL+1= 1for nontriviality).
Hence perturbation stability of the output function holds only if d+dL+1≥1andbL+1+ ˜rL≥1.
Then, perturbation nontriviality holds if and only if d+dL+1= 1orbL+1+ ˜rL= 1.
In the t-th backward pass, bL+1+ ˜rL≥1will be replaced by the slightly stronger constraint
bL+1+ ˜r≥1.
E.1.4 t-th forward pass
Formally, we sum the updates in each step,
ˆWL+1
t:=ˆWL+1
0+θL+1/∇(δWL+1
1+···+δWL+1
t),
where δWL+1
t+1:=−η˜χt(˜xL
t)Tdenotes the normalized change in the weights WL+1(as a row
vector) of scaling θL+1=θWL+1=n−cL+1under perturbation stability and nontriviality so that
ˆWL+1
t scales as θ∇=n−min(bL+1,cL+1).δWL+1
t+1should not be confused with ˜δWL+1
t+1which
denotes the perturbation of the weights at time t+ 1. For every nontrivial stable parametrization
we have ˜χt= Θ(1) and˜xL
t= Θ(1) which requires ˜θL≤1. In the case cL+1< bL+1, we
write ˆWL+1
t :=n−bL+1+cL+1ˆWL+1
0+ (δWL+1
1+···+δWL+1
t)with the same scaling θ∇=
n−min(bL+1,cL+1).
For preactivations and activations we also sum the changes from each step,
hl
t:=hl
0+θl(δhl
1+···+δhl
t), xl
t:=xl
0+θl(δxl
1+···+δxl
t).
Using the fact that
W1
t−W1
t−1=−η˜χt−1θW1dh1
SAM,t −1ξT
t−1,
yields the normalized preactivation updates
δh1
t(ξ) :=−η˜χt−1dh1
SAM,t −1ξT
t−1ξ(NL),
with scaling θ1=θW1=n−c1θSAM =n−c1−min(bL+1,cL+1,d+dL+1)as for SGD under perturbation
stability and nontriviality where ˜χt−1= Θ(1) .
Forl∈[2, L], it holds that
Wl
t−Wl
t−1=−η˜χt−1θWl1
ndhl
SAM,t −1(˜xl−1
t−1)T,
with the right scaling θWl=n1−cl−min(bL+1,cL+1,d+dL+1)as for SGD under perturbation stability
˜xl−1
t−1= Θ(1) , so that we get δhl
tusing a telescope sum,
θlδhl
t= Wl
txl−1
t−Wl
t−1xl−1
t−1=Wl
t−1(xl−1
t−xl−1
t−1) + (Wl
t−Wl
t−1)xl−1
t
= θl−1 
Wl
0δxl−1
t+t−1X
s=1(Wl
s−Wl
s−1)δxl−1
t!
+ (Wl
t−Wl
t−1)xl−1
t
= θl−1 
Wl
0δxl−1
t−ηθWlt−1X
s=1˜χs−1(˜xl−1
s−1)Tδxl−1
t
ndhl
SAM,s −1!
26−ηθWl˜χt−1(˜xl−1
t−1)Txl−1
t
ndhl
SAM,t −1,
which defines a NonLin operation with the vectors Wl
0δxl−1
t, dhl
SAM, 0, dhl
SAM,t −1and everything
else treated as scalars. The scaling is given by
θl= max( θl−1, θWlθl−1, θWl) =lmax
m=1θWm=n−rl,
with
rl:= min( bL+1, cL+1, d+dL+1) +l
min
m=1(cm−I(m̸= 1)) ,
where θWl≤1for all l∈[L]for stability. Note that for l1≤l2, it holds that θl1≤θl2, which
explains the sufficiency of θL=n−rL=n−rfor the stability of the activation updates.
Activations with the same scaling θlcan then simply be defined via the NonLin operation
δxl
t:=θ−1
l(ϕ(hl
t−1+θlδhl
t)−ϕ(hl
t−1)).
The updates of the output function are scalars defined as
δft:=θ′
L+1δWL+1
txL
t
n+θ′
L∇ˆWL+1
t−1δxL
t
n,
where θ′
L+1=nθL+1=n1−cL+1andθ′
L∇=nθ∇θL=n1−min(bL+1,cL+1)−rL, where we will see
whyWL+1
t−1= Θ( n−min(bL+1,cL+1))in the next paragraph. This leads to the constraints cL+1≥1
andbL+1+r≥1for the stability of the output function, where equality in either constraint leads to
nontriviality.
E.1.5 t-th backward pass
Perturbations. Due to linearity and stability, the last layer remains
dxL
t:=ˆWL+1
t,
with scaling θ∇=n−min(bL+1,cL+1).
As in the first backward pass, we use the weight updates to directly compute the preactivation and
activation perturbations similar to the t-th forward pass but performing SGD instead of SAM in the
last step. The SGD backward pass for the perturbation is given by
dhl
t:= dxl
t⊙ϕ′(hl
t),
dxl−1
t:= ( Wl
t)Tdhl
t
= 
Wl
0−ηθWltX
s=1˜χs−11
ndhl
SAM,s −1(˜xl−1
s−1)T!T
dhl
t
= Wl
0dhl
t−η(n1−clθSAMθ∇)tX
s=1˜χs−1(dhl
SAM,s −1)Tdhl
t
n˜xl−1
s−1,
with scaling max( θ∇, n1−clθSAMθ∇) =θ∇, since n1−clθSAM≤1is implied by r≥0required
for the stability of (pre-)activation updates.
We write χt=L′(ft(ξt), yt)for the derivative of the loss with respect to the unperturbed function
(which is Θ(1) under stability and nontriviality), and get
˜δh1
t(ξ) := +ρχt(ξT
tξ)dh1
t
∥vt∥,
˜θl˜δhl
t:= ˜θl−1Wl
t˜δxl−1
t+ (˜Wl
t−Wl
t)˜xl−1
t
= ˜θl−1 
Wl
0˜δxl−1
t+tX
s=1(Wl
s−Wl
s−1)˜δxl−1
t!
+ (˜Wl
t−Wl
t)˜xl−1
t
27= ˜θl−1 
Wl
0˜δxl−1
t−η(n1−clθSAM)tX
s=1˜χs−1(˜xl−1
s−1)T˜δxl−1
t
ndhl
SAM,s −1!
+ρ˜θWlχt
∥vt∥(xl−1
t)T˜xl−1
t
ndhl
t,
which defines a NonLin operation with the vectors Wl
0˜δxl−1
t, dhl
SAM, 0, . . . , dhl
SAM,t −1, dhl
t,
and where we can now define the definitive scalings ˜θ1:=˜θW1:=n−(d+d1)θ∇=
n−(min( bL+1,cL+1)+d+d1),˜θWl:=n1−(d+dl)θ∇=n−(min( bL+1,cL+1)+d+(dl−1))and˜θl=
max( ˜θl−1, n1−clθSAM˜θl−1,˜θWl) = maxl
m=1˜θWm=n−˜rlwith
˜rl:= min( bL+1, cL+1) +d+l
min
m=1(dm−I(m̸= 1)) ,
where we used that n1−clθSAM≤1due to r≥0for stability and ˜xl−1
t= Θ(1) due to perturbation
stability. Perturbation stability of the hidden layer (pre-)activations ˜δhl,˜δxl=O(1)for all l∈[L]
holds if and only if ˜r:= ˜rL≥0since ˜rl≥˜rLfor all l≤L.
The activation perturbations ˜δxl
tand the perturbation of the output function ˜δftcan be defined exactly
as in the first backward pass,
˜δxl
t:= ˜θ−1
l(ϕ(hl
t+˜θl˜δhl
t)−ϕ(hl
t)),
˜δft:= ˜WL+1
t˜xL
t−WL+1
txL
t=˜θ′
L+1˜δWL+1
t˜xL
t
n+˜θ′
L∇ˆWL+1
t˜δxL
t
n,
with ˜δWL+1
t :=ρ χtxL
t
∥vt∥and the same scalings ˜θl,˜θ′
L+1=n1−(d+dL+1)and˜θ′
L∇=nθ∇˜θL=
n1−min(bL+1,cL+1)−˜rsince WL+1
t=WL+1
0+ ∆WL+1
t= max( n−bL+1, n−cL+1), which yields the
slightly stronger constraint (than in the first backward pass) min(bL+1, cL+1)+˜r≥1for perturbation
stability and either ˜θ′
L+1= 1ormin(bL+1, cL+1) + ˜r= 1for perturbation nontriviality.
SAM Update. For each l∈ {1, . . . , L }, as in the first backward pass, we get
dxL
SAM,t :=ˆWL+1
t+˜θ(L+1)/∇˜δWL+1
t,
with scaling θSAM =n−min(bL+1,cL+1,dL+1+1/2)as well as
dhl
SAM,t :=dxl
SAM,t ⊙ϕ′(˜hl
t).
Fordxl
SAM,t we again use a telescope sum over the weight changes,
dxl−1
SAM,t := ( ˜Wl
t)Tdhl
SAM,t = (Wl
0+θWltX
s=1δWl
s+˜θWl˜δWl
t)Tdhl
SAM,t
= ( Wl
0)Tdhl
SAM,t −η(n1−clθSAM)tX
s=1˜χs−1(dhl
SAM,s −1)Tdhl
SAM,t
n˜xl−1
s−1
+ρ(n1/2−dlθ∇)χt
∥vt∥(dhl
t)Tdhl
SAM,t
nxl−1
t,
which defines a NonLin operation in the vectors (Wl
0)Tdhl
SAM,t ,˜xl−1
0, . . . , ˜xl−1
t−1, xl−1
tand ev-
erything else treated as scalars. Note that the scalings remain θSAM , since ∇xl−1
tf|˜Wt=
Θ(max( θSAM, n1−clθ2
SAM, n1/2−dlθ∇θSAM)) = Θ( θSAM)under stability, nontriviality, perturba-
tion stability and perturbation nontriviality.
Finally define the loss derivative on the perturbed output function
˜χt:=L′(˜WL+1
t˜xL
t, yt),
and compute the normalized change in WL+1,
δWL+1
t+1:=−η˜χt˜xL
t.
28E.2 The infinite-width limit
In this section, we apply the Master Theorem’s computation rules to derive the marginal distributions
Zcorresponding to the vectors of the program constructed above. According to the Master Theorem,
each such vector zwill have roughly iid coordinates distributed like Zzin the large nlimit.
We assume stability holds, so that θ→˚θ∈ {0,1}for all scalars θin the program.
For the first forward pass, we have
Zh1
0(ξ)=ξZW1
0, Zxl
0(ξ)=ϕ(Zhl
0(ξ)), Zhl+1
0(ξ)=ZWl+1
0xl
0(ξ).
IfbL+1>1/2then˚f0= 0, otherwise if bL+1= 1/2then˚f0converges to a nontrivial Gaussian. For
the details we refer to Appendix H.4.1 in Yang and Hu (2021), as at initialization their results still
hold here.
For the first SGD backward pass, we have
ZdxL
0(ξ)=ZˆWL+1
0, Zdhl
0(ξ)=Zdxl
0(ξ)ϕ′(Zhl
0(ξ)), Zdxl−1
0(ξ)=Z(Wl
0)Tdhl
0(ξ),
where ˙Zdxl
0(ξ)= 0andZdxl
0(ξ)=ˆZdxl
0(ξ)for all ξ∈ X.
For general t >0, we have
ZdxL
t(ξ)= ZˆWL+1
t,
Zdhl
t(ξ)= Zdxl
t(ξ)ϕ′(Zhl
t(ξ)),
Zdxl−1
t(ξ)= Z(Wl
0)Tdhl
t(ξ)−η˚θWltX
s=1˚˜χs−1E[Zdhl
SAM,s −1Zdhl
t]Z˜xl−1
s−1,
where ˚˜χs=L′(˚˜fs(ξs), ys)fors < t , and Z(Wl
0)Tdhl
t(ξ)is aΘ(1) random variable distributed as
Z(Wl
0)Tdhl
t(ξ)=ˆZ(Wl
0)Tdhl
t(ξ)+X
v∈V:Wl
0v∈VZvE∂Zdhl
t(ξ)
∂ˆZWl
0v.
For all t≥0, the limit of the gradient norm is given by
∥˚v∥= ˚χt 
˚θ2
∥v1∥E[Z(dh1
t)2](ξT
tξt) +LX
l=2˚θ2
∥vl∥E[Z(dhl
t)2]E[Z(xl−1
t)2] +˚θ2
∥vL+1∥(xL
t)TxL
t
n!1/2
,
(E.3)
where ˚χt=L′(˚ft(ξt), yt),θ2
∥v1∥:=n1−2d1θ2
∇,θ2
∥vl∥:=n2−2dlθ2
∇forl∈[2, L]andθ2
∥vL+1∥:=
n1−2dL+1, and where ˚θ2
∥vL+1∥= 1 if and only if dL+1= 1/2and˚θ2
∥vL+1∥= 0 if and only if
dL+1>1/2, while ˚θ2
∥vl∥= 1if and only if 2dl= 1 + I(l >1)−2 min( bL+1, cL+1)and˚θ2
∥vl∥= 0
if and only if 2dl>1 +I(l >1)−2 min( bL+1, cL+1).
For the last-layer weight perturbations (for θ∇≥˜θL+1, else Zˆ˜WL+1
t=Z˜δWL+1
t) we have
Zˆ˜WL+1
t=ZˆWL+1
t+˚˜θ(L+1)/∇Z˜δWL+1
t, Z˜δWL+1
t=ρ˚χt
∥˚v∥ZxL
t.
Note that ˚χtcancels itself out and we purely get a perturbation in distribution ZxL
tscaled to have
standard deviation ρ.
For all t≥0andl∈[1, L], we have
Z˜hl
t=Zhl
t+˚˜θlZ˜δhl
t, Z˜xl
t=Zxl
t+˚˜θlZ˜δxl
t,
where for l= 1,
Z˜δh1
t(ξ)= +ρ˚χt(ξT
tξ)
∥˚v∥Zdh1
t.
29If˚˜θl= 0, then
Z˜δxl
t=ϕ′(Zhl
t)Z˜δhl
t,
otherwise˚˜θl= 1and
Z˜δxl
t=ϕ(Z˜hl
t)−ϕ(Zhl
t).
Forl≥2, we have
Z˜δhl
t=˚˜θ(l−1)/lZWl
0˜δxl−1
t−η˚θWl(˜l−1)/˜ltX
s=1˚˜χs−1E[Z˜xl−1
s−1Z˜δxl−1
t]Zdhl
SAM,s −1
+ρ˚˜θWl/l˚χt
∥˚v∥E[Zxl−1
tZ˜xl−1
t]Zdhl
t,
where ˜θ(l−1)/l=˜θl−1
˜θl,θWl(˜l−1)/˜l=θWl˜θl−1
˜θland˜θWl/l=˜θWl
˜θl, and ZWl
0˜δxl−1
thas the decomposi-
tion
ZWl
0˜δxl−1
t=ˆZWl
0˜δxl−1
t+X
v∈V: (Wl
0)Tv∈VZvE∂Z˜δxl−1
t
∂ˆZ(Wl
0)Tv.
The perturbed output function has the limit˚˜ft:=˚ft+˚˜δftwith
˚˜δft:=˚˜θ′
L+1E[Z˜δWL+1
tZ˜xL
t] +˚˜θ′
L∇E[ZˆWL+1
tZ˜δxL
t],
so that we can define ˚˜χt=L′(˚˜ft(ξt), yt)or equivalently ˚˜χt=L′(˚˜θL+1˚˜θLE[Zˆ˜WL+1
tZ˜xL
t], yt).
For the SAM gradients we have
ZdxL
SAM,t = ZˆWL+1
t+˚˜θ(L+1)/∇Z˜δWL+1
t,
Zdhl
SAM,t = Zdxl
SAM,t·ϕ′(Z˜hl
t)
Zdxl−1
SAM,t = Z(Wl
0)Tdhl
SAM,t−η˚θWltX
s=1˚˜χs−1E[Zdhl
SAM,s −1Zdhl
SAM,t ]Z˜xl−1
s−1
+ρ˚˜θWl˚χt
∥˚v∥E[Zdhl
tZdhl
SAM,t ]Zxl−1
t,
where Z(Wl
0)Tdhl
SAM,t is given by
Z(Wl
0)Tdhl
SAM,t =ˆZ(Wl
0)Tdhl
SAM,t +X
v∈V:Wl
0v∈VZvE∂Zdhl
SAM,t
∂ˆZWl
0v.
Now SAM’s (pre-)activation updates are given by
Zhl
t=Zhl
0+˚θl(Zδhl
1+···+Zδhl
t), Zxl
t=Zxl
0+˚θl(Zδxl
1+···+Zδxl
t),
with, for l∈[2, L],
Zδh1
t(ξ)= −η˚˜χt−1(ξT
t−1ξ)Zdh1
SAM,t −1,
Zδhl
t= ˚θ(l−1)/l 
ZWl
0δxl−1
t−η˚θWlt−1X
s=1˚˜χs−1E[Z˜xl−1
s−1Zδxl−1
t]Zdhl
SAM,s −1!
−η˚θWl/l˚˜χt−1E[Z˜xl−1
t−1Zxl−1
t]Zdhl
SAM,t −1,
where θ(l−1)/l:=θl−1/θl,θWl/l:=θWl/θlandZWl
0δxl−1
thas the decomposition
ZWl
0δxl−1
t=ˆZWl
0δxl−1
t+X
v∈V: (Wl
0)Tv∈VZvE∂Zδxl−1
t
∂ˆZ(Wl
0)Tv.
30If˚θl= 0, then
Zδxl
t=ϕ′(Zhl
t−1)Zδhl
t,
otherwise ˚θl= 1and
Zδxl
t=ϕ(Zhl
t)−ϕ(Zhl
t−1).
The last-layer SAM weight update is given by
ZˆWL+1
t=ZˆWL+1
0+˚θL+1/∇(ZδWL+1
1+···+ZδWL+1
t),
withZδWL+1
t=−η˚˜χt−1Z˜xL
t−1.
Fort >0, the SAM function update is given by
˚ft=˚f0+˚δf1+···+˚δft,
with˚δft=˚θ′
L+1E[ZδWL+1
tZxL
t] +˚θ′
L∇E[ZˆWL+1
t−1ZδxL
t].
E.3 Concluding the proof of all main results
After writing out the NE⊗OR⊤program and its limit, as well as tracking all scalings, the main
results stated in Appendix D all follow from the Tensor Program Master Theorem and from the
characterization results in Yang and Hu (2021) in the following way.
Formally Yang and Hu (2021) show feature learning for SGD with small enough learning rate η >0
by proving ∂2
ηE(ZxL
1(ξ0))2̸= 0atη= 0, and they show that learning does not occur in the kernel
regime by showing ∂3
η˚f1̸= 0, hence ˚f1−˚f0is not linear in η.
BothE(ZxL
1(ξ0))2and˚f1are defined via NE⊗OR⊤computations and can be written as a composition
of additions, multiplications, the expectation operator, applications of ϕandϕ′, overall applications
of infinitely differentiable, pseudo-Lipschitz functions to (Gaussian) random variables, ηandρ.
Consequently E(ZxL
1(ξ0))2and˚f1are infinitely often differentiable as a function of both ηandρ,
where differentiating the expectation operator is covered in Yang and Hu (2021, Lemma H.39). Since
Yang and Hu (2021) cover the case ρ= 0, their proofs immediately show the correctness of the
derived scalings for SAM as long as η >0andρ >0are chosen small enough. Both the gradient
evaluation for the perturbation as well as the gradient evaluation for the updates stay arbitrarily
close to those of SGD if ρ >0is chosen small enough. The conditions for stability, nontriviality,
feature learning, perturbation nontriviality and effective perturbations now follow from considering
the respective scaling.
E.3.1 Proof of Theorem D.2
Abcd-parameterization is stable if and only if all scalings in the Tensor Program have the limit
˚θ∈ {0,1}, where ˚θ= 1 is required for activations at initialization (for which nothing changes
compared to SGD). Potential cancellations are taken care of for sufficiently small η >0andρ >0
by the argument above. Now collecting all constraints that are already stated in the Tensor Program
formulation at the respective step concludes the proof.
E.3.2 Proof of Theorem D.3
A stable bcd-parameterization is nontrivial if and only if ˚ft= Θ(1) if and only if ˚θ′
L+1= 1 or
˚θ′
L∇= 1.
E.3.3 Proof of Theorem D.4
A stable bcd-parametrization is feature learning in layer lif and only if the feature update scaling
˚θl= 1where
θl=n−rl, r l:= min( bL+1, cL+1, dL+1+ 1/2) +l
min
m=1(cm−I(m̸= 1)) .
Hence a stable bcd-parametrization is feature learning in layer lif and only if rl= 0.
31Since for all l1≤l2, it holds that rl1≥rl2≥0, we get the equivalence for any l0∈[L]: A stable
bcd-parametrization is feature learning in layer l0if and only if it is feature learning in layer lfor all
l≥l0if and only if rl0= 0.
E.3.4 Proof of Theorem D.6
Given a stable bcd-parametrization, perturbation triviality is fulfilled if and only if˚˜θ′
L+1= 0 and
˚˜θ′
L∇= 0, where ˜θ′
L+1=n1/2−dL+1and˜θ′
L∇=nθ∇˜θL=n1−min(bL+1,cL+1)−˜r, hence if and only
ifdL+1>1/2andmin(bL+1, cL+1) + ˜r >1.
In that case,˚˜ft=˚ft, but ˚ftmay still be affected by non-vanishing SAM perturbations in δWL+1
t
andδxL
t. Only when all SAM perturbations vanish are we effectively only using SGD. By definition,
the perturbation scale in the l-th layer vanishes if and only if˚˜θl= 0, where ˜θl=n−˜rlwith
˜rl= min( bL+1, cL+1) + 1 /2 + minl
m=1(dm−I(m̸= 1)) , hence if and only if ˜rl>0. Since
˜rl≥˜rL= ˜rfor all l≤L, we get˚˜θl= 0 for all l∈[L]if and only if ˜r >0. Similarly, for any
reference layer l0∈[L], we get˚˜θl= 0for all l≤l0if and only if ˜rl0>0. In words, for any l0∈[L],
we have vanishing perturbations in layer l0if and only if we have vanishing perturbations until layer
l0if and only if ˜rl0>0.
Altogether, a stable bcd-parametrization has vanishing perturbations if and only if ˜r >0,dL+1>1/2
andmin(bL+1, cL+1) + ˜r > 1. This case reduces to the results in Yang and Hu (2021) in the
limit. Since stability requires cL+1≥1and˜r≥0, we can rewrite the equivalence conditions as
dL+1≥1/2and˜r >max(0 ,1−bL+1).
E.3.5 Proof of Theorem D.8
Recall ˜θW1:=n−(d+d1)θ∇,˜θWl:=n1−(d+dl)θ∇and, for the last layer ˜θWL+1:=n−(d+dL+1).
As opposed to perturbation nontriviality, we are not only interested in ˜θl= max( ˜θl−1,˜θWl) =
maxl
m=1˜θWm→1, but in a non-vanishing contribution of the perturbations in layer l, i.e.˚˜θWl= 1
or, for the last layer, ˚˜θL+1= 1.
E.3.6 Proof of Theorem D.9
The limit of the gradient norm is defined as a NE⊗OR⊤program scalar (E.3) . Note that for
bL+1>1/2, the last-layer scaling strictly dominates all other scalings leading to the simplified
gradient norm formula.
Now consider an arbitrary stable choice of layerwise initialization variances {bl}l∈[L+1]and learning
rates{cl}l∈[L+1]. To fulfill the gradient norm constraints (D.1) , we have to choose dl=C= 1/2
for all l∈[L+ 1], because stability requires min(bL+1, cL+1)≥1/2. Now stability of the output
function perturbations requires d≥1/2, where d >1/2yields vanishing perturbations and d= 1/2
yields effective last-layer SAM through the term ˜δWL+1
t˜xL
t. After choosing d≥1/2, we get
˜r≥min(bL+1, cL+1)≥1/2>0which implies vanishing perturbations in all hidden layers.
E.3.7 Proof of Proposition D.10
To achieve non-vanishing gradient norm contribution of the last layer in (D.1) , we need to choose
dL+1= 1/2, which requires d≥1/2for stability of the output function perturbations. Achieving
non-vanishing gradient norm contributions of all layers requires d1= 1/2−min(bL+1, cL+1)and
dl= 1−min(bL+1, cL+1)forl∈[2, L], which results in ˜r=d≥1/2>0which implies vanishing
perturbations in all hidden layers.
E.3.8 Proof of Theorem D.11
Given a stable bcd-parametrization, we know d+dL+1≥1, so that the feature learning constraint
ris not affected by any stable choice of d∪ {dl}l∈[L+1]. The maximal stable choice of layerwise
initialization variances {bl}l∈[L+1]and learning rates {cl}l∈[L+1]that constitute µP is therefore
unaffected by the perturbation scalings d∪ {dl}l∈[L+1].
32Stability of the output function perturbations requires bL+1+ ˜r≥1. Hence if bL+1<1, then
˜r≥1−bL+1>0, which implies vanishing perturbations in all hidden layers.
From now on consider bL+1≥1. Recall c∇:= min( bL+1, cL+1). InµP,c∇= 1, but effective
perturbations in all layers can be achieved more generally for c∇≥1. Choosing d1= 1/2−c∇
saturates the gradient norm constraint (D.1) . To reach effective perturbations already in the first
layer ˜r1=c∇+d+d1= 0, we need d=−1/2. For perturbation stability and last-layer
effective perturbations, we need d+dL+1= 1which requires dL+1= 3/2. Achieving perturbation
stability and effective perturbations in all hidden layers requires ˜θWl= 1 which is equivalent to
c∇+d+dl−I(l̸= 1) = 0 . For l∈[2, L], we therefore need dl= 3/2−c∇. This choice of
{dl}l∈[L+1]achieves effective perturbations in all layers.
To show uniqueness we iterate through all possibilities of saturating the norm bound constraint (D.1) .
We have considered the cases dL+1= 1/2in (b) leading to vanishing perturbations in all hidden
layers and d1= 1/2−c∇in (c) with only one choice for effective perturbations in all layers. Lastly
consider dl= 1−c∇forl∈[2, L]for non-vanishing gradient contribution of the hidden layers.
Note that all hidden layers play the same role in all relevant constraints. Effective perturbations in
any hidden layer l∈[2, L]requires ˜θWl= 1for which we need d= 0. But then, as d1≥1/2−c∇,
it holds that ˜r1≥1/2implying vanishing perturbations in the first layer. This shows the uniqueness
of (1).
For the gradient norm statements, note that the gradient norm ∥vt∥can be written as a
NE⊗OR⊤computation rule (E.2) where the layer scalings in this parameterization are Θ(1) for
the input layer, Θ(n−1/2)for hidden layers and Θ(n−1)for the output layer. Now the Tensor
Program master theorem immediately implies the result.
E.3.9 Proof of Proposition D.12
Perturbation nontriviality with respect to any hidden layer is equivalent to ˜r= 0 . Since
min(bL+1, cL+1)≤1, we get min(bL+1, cL+1)+ ˜r≤1. Since stability requires min(bL+1, cL+1)+
˜r≥1, we get min(bL+1, cL+1) + ˜r= 1, which implies perturbation nontriviality with respect to the
output.
E.3.10 Proof of Proposition D.13
The constraint is the same constraint as in Theorem D.8, which implies effective perturbations in
the first layer. Now ˜rl≤˜r1= 0 implies perturbation nontriviality in all hidden layers due to
Theorem D.6.
E.4 Analytic expression of the features after first SAM update
Below we state the analytic expression of the first SAM update, but leave a closer analysis of its
fine-grained dynamics in comparison to SGD to future work. Before looking into the effective
perturbation regime, we restate Lemma H.37 in Yang and Hu (2021) with a more detailed proof.
First, we define ℓ∈[L]as the unique index that satisfies θL=···=θℓ= 1> θℓ−1≥ ··· ≥ θ1. In
words, ℓis the first layer in which feature learning occurs. Analogously, we define ˜ℓ∈[L]as the
unique index that satisfies 1 =˜θL
˜θL=···=˜θ˜ℓ
˜θL>˜θ˜ℓ−1
˜θL≥ ··· ≥˜θ1
˜θL.
Lemma E.2 (Features after first SGD step ).Defining Zl
t:=Zhl
t,γl(η) =Eϕ(Zl
0)ϕ(Zl
1)forl≥1,
γ0=ξT
0ξandγl
11(η) =Eϕ′(Zl
0)ϕ′(Zl
1), we have
Zℓ−1
1=Zℓ−1
0, . . . , Z1
1=Z1
0,
and, for all l≥ℓ,
Zl
1=Zl
0+Il>ℓˆZWl
0δxl−1
1+ηβlZdxl
0ϕ′(Zl
0),
where βlis defined recursively by
βl=βl(η) =−˚χ0γl−1(η) +βl−1(η)γl−1
11(η),
withβℓ−1= 0. Note that βl(0)<0for all l≥ℓ.
33Proof. By the defining infinite-width equations, assuming ˚θWl/l= 1(so minimal stable choice of
cl),
Zl
1=Zl
0+˚θ(ℓ−1)/ℓZWl
0δxl−1
1−η˚χ0γl−1Zdxl
0ϕ′(Zl
0).
Atl=ℓ, we get ˚θ(ℓ−1)/ℓ= 0, whereas for l > ℓ we get ˚θ(l−1)/l= 1, which results in ˚θ(ℓ−1)/ℓ=Il>ℓ.
Now, for l > ℓ , the second term decomposes into ˆZWl
0δxl−1
1and
˙ZWl
0δxl−1
1=Zdhl
0E∂Zδxl−1
1
∂ˆZ(Wl
0)Tdhl
0.
Since by induction hypothesis,
Zδxl−1
1=ϕ(Zl−1
1)−ϕ(Zl−1
0) =ϕ
Zl−1
0+Il>ℓˆZWl
0δxl−1
1+ηβl−1Zdxl−1
0ϕ′(Zl−1
0)
−ϕ(Zl−1
0),
where Zdxl−1
0=Z(Wl
0)Tdhl
0is the only dependence on ˆZ(Wl
0)Tdhl
0, we get
∂Zδxl−1
1
∂ˆZ(Wl
0)Tdhl
0=ϕ′(Zl−1
1)ηβl−1ϕ′(Zl−1
0).
Plugging the derivative back into the defining equation and noticing that Zdhl
0=Zdxl
0ϕ′(Zl
0)
concludes the proof.
An analogous analysis for the perturbation at initialization shows.
Lemma E.3 (Feature perturbation at initialization ).The perturbation trivial layers fulfill
Z˜h˜ℓ−1
0=Zh˜ℓ−1
0, . . . , Z˜h1
0=Zh1
0,
and, for all l≥˜ℓ,
Z˜hl
0=Zhl
0+Il>˜ℓˆZWl
0˜δxl−1
0+ρ˜βlZdxl
0ϕ′(Zhl
0),
where ˜βlindependent of ηis defined recursively by
˜βl=˜βl(ρ) =˚χ0
˚¯∥∇L0∥E[ϕ(Zhl−1
0)ϕ(Z˜hl−1
0)] +˜βl−1E[ϕ′(Zhl−1
0)ϕ′(Z˜hl−1
0)]
with˜β˜ℓ−1= 0. Note that ˜βl(0)>0for all l≥˜ℓ.
Remark E.4. If˜ℓ= 1, in the definition of ˜βlreplace E[ϕ(Zhl−1
0)ϕ(Z˜hl−1
0)]byξT
0ξ. ◀
Now we are ready to state the closed form expression for the first SAM update.
Lemma E.5 (Features after first SAM update ).Defining Zl
t:=Zhl
tand˜Zl
t:=Z˜hl
t, we have
Zℓ−1
1=Zℓ−1
0, . . . , Z1
1=Z1
0,
and, for all l≥ℓ,
Zl
1=Zl
0+Il>ℓˆZWl
0δxl−1
1+ηβlZdxl
SAM, 0ϕ′(˜Zl
0) +ηγlZdhl
0,
where βlis defined recursively by
βl=βl(η) =−˚˜χ0E[ϕ(˜Zl−1
0)ϕ(Zl−1
1)] +βl−1(η)E[ϕ′(Zl−1
1)ϕ′(˜Zl−1
0)],
withβℓ−1= 0, and γl=γl(η)is recursively defined by
γl:=βl−1ρ˜βl−1E[ϕ′(Zl−1
1)ϕ′(Zl−1
0)ϕ′′(˜Zl−1
0)Zdxl−1
SAM, 0] +γl−1E[ϕ′(Zl−1
0)ϕ′(Zl−1
1)],
withγℓ−1=γℓ= 0.
Remark E.6. Ifℓ= 1, in the definition of βlreplace E[ϕ(˜Zl−1
0)ϕ(Zl−1
1)]by(ξT
t−1ξ). ◀
34Proof. By the defining infinite-width equations, for l≥ℓ, assuming ˚θWl/l= 1(so minimal stable
choice of cl),
Zl
1=Zl
0+˚θ(l−1)/lZWl
0δxl−1
1−η˚χ0E[ϕ(˜Zl−1
0)ϕ(Zl−1
1)]Zdxl
SAM, 0ϕ′(˜Zl
0). (E.4)
Atl=ℓ, we get ˚θ(ℓ−1)/ℓ= 0and˚θWℓ/ℓ= 1, whereas for l > ℓ we get ˚θ(l−1)/l= 1and˚θWl/l= 1
(under minimal stable choice of cl), which results in ˚θ(l−1)/l=Il>ℓ. Now, for l > ℓ , the second term
decomposes into ˆZWl
0δxl−1
1and˙ZWl
0δxl−1
1. For the rest of the proof it remains to analyse ˙ZWl
0δxl−1
1.
Since by induction hypothesis,
Zδxl−1
1= ϕ(Zl−1
1)−ϕ(Zl−1
0)
=ϕ
Zl−1
0+Il>ℓˆZWl
0δxl−1
1+ηβl−1Zdxl−1
SAM, 0ϕ′(˜Zl−1
0) +ηγl−1Zdhl−1
0
−ϕ(Zl−1
0),
where Zdxl−1
SAM, 0=Z(Wl
0)Tdhl
SAM, 0+ρ˚˜θWl˚χ0
˚¯∥∇L0∥E[Zdhl
0Zdhl
SAM, 0]Zxl−1
0with the second term
independent of (Wl
0)Tand by Lemma E.3 we know ˜Zl−1
0=Zl−1
0+Il−1>˜ℓˆZWl−1
0˜δxl−2
0+
ρ˜βl−1Zdxl−1
0ϕ′(Zl−1
0), where only the last term with Zdxl−1
0=Z(Wl
0)Tdhl
0influences ˙ZWl
0δxl−1
1, we
get
˙ZWl
0δxl−1
1=Zdhl
0E∂Zδxl−1
1
∂ˆZ(Wl
0)Tdhl
0+Zdhl
SAM, 0E∂Zδxl−1
1
∂ˆZ(Wl
0)Tdhl
SAM, 0, (E.5)
with
∂Zδxl−1
1
∂ˆZ(Wl
0)Tdhl
SAM, 0=ϕ′(Zl−1
1)ηβl−1ϕ′(˜Zl−1
0),
and, using Zdhl−1
0=Zdxl−1
0ϕ′(Zl−1
0) =Z(Wl
0)Tdhl
0ϕ′(Zl−1
0), yields
∂Zδxl−1
1
∂ˆZ(Wl
0)Tdhl
0= ϕ′(Zl−1
1) 
ηβl−1Zdxl−1
SAM, 0ϕ′′(˜Zl−1
0)∂˜Zl−1
0
∂ˆZ(Wl
0)Tdhl
0+ηγl−1ϕ′(Zl−1
0)!
=ϕ′(Zl−1
1)
ηβl−1Zdxl−1
SAM, 0ϕ′′(˜Zl−1
0)ρ˜βl−1ϕ′(Zl−1
0) +ηγl−1ϕ′(Zl−1
0)
.
Plugging Eq. (E.5) back into the defining equation (E.4) and noticing that Zdhl
SAM, 0=
Zdxl
SAM, 0ϕ′(˜Zl
0)as well as Zdhl
0=Zdxl
0ϕ′(Zl
0)concludes the proof.
F Generalizations and further perturbation scaling considerations
F.1 Overview over choices of dlandd
Since for some combinations of architectures and datasets it turns out that performing SAM on a
subset of layers performs better than effective perturbations in all layers (Müller et al., 2024), we
would like to know how to choose danddlto adjust which layers should be effectively perturbed
and which should have vanishing weight perturbations. In practice, simply set all perturbations that
should vanish to 0by design, and use the global scaling dand relative scalings dlfrom µP2for the
perturbed layers. This section is instead interested in a complete characterization of all possible
choices of {dl}l∈[L+1]andd. The heuristic derivation only requires the gradient norm constraints
(D.1) and the perturbation stability constraints that require ˜δW1=O(1)and˜δWl=O(n−1)for
l >1given by
dl≥

−c∇−d iflis input-like,
1−c∇−diflis hidden-like,
1−d iflis output-like,(F.1)
where a layer is effectively perturbed if and only if equality holds in the respective perturbation
stability inequality. This heuristic claim yields the characterization of all phases of the choices of
35Effective perturbations possible Gradient norm may be dominated by
input-like hidden-like output-like input-like hidden-like output-like
d=−1/2 ✓ ✓ ✓ ✓ ✗ ✗
d∈(−1/2,0) ✗ ✓ ✓ ✓ ✗ ✗
d= 0 ✗ ✓ ✓ ✓ ✓ ✗
d∈(0,1/2) ✗ ✗ ✓ ✓ ✓ ✗
d= 1/2 ✗ ✗ ✓ ✓ ✓ ✓
d >1/2 ✗ ✗ ✗ ✓ ✓ ✓
Table F.1: (Characterization of perturbation scalings) Overview over the regimes of all possible
choices of danddl. A layer is effectively perturbed if and only dlsatisfies (F.1) . At least one layer
must satisfy equality in its gradient norm constraint (D.1) . This table summarizes which layers can
exhibit effective perturbations, and which may dominate the gradient norm, given a choice of d. The
choice d <−1/2results in perturbation blowup ˜r <0. At the critical d=−1/2(respectively,
d= 0;d= 1/2) a input-like (respectively hidden-like; output-like) layer is effectively perturbed if
and only if it dominates the gradient norm. Consequently d=−1/2implies effective perturbations
in at least one input-like layer.
perturbation scalings danddlin Table F.1 and allows us to formulate a simple rule of how to choose
danddlgiven the information which layers should be effectively perturbed, and which should have
vanishing weight perturbations.
Choice of perturbation scaling from list of layers to effectively perturb. We denote the set of
all layers by L, whereas the subset of layers, which we want to effectively perturb, is denoted by
LSAM⊆ L.
1.If there exists an input-like layer l∈ LSAM , setd=−1/2. Input-like layers are effectively
perturbed if and only if dl= 1/2−c∇. Hidden-like (respectively, output-like) layers are
effectively perturbed if and only if dl= 3/2−c∇(respectively, dl= 3/2). For all layers that
have vanishing weight perturbations, do not perturb these weights or choose dl>1/2−c∇
for input-like, dl>3/2−c∇for hidden-like and dl>3/2for output-like layers.
2.If all input-like layers should have vanishing weight perturbations but there exists a hidden-
like layer l∈ LSAM , setd= 0. Hidden-like layers are effectively perturbed if and only if
dl= 1−c∇. Output-like layers are effectively perturbed if and only if dL+1= 1. For all
layers that have vanishing weight perturbations, do not perturb these weights, or set dl> c∇
for input-like, dl>1−c∇for hidden-like and dl>1for output-like layers (as required by
the perturbation stability and gradient norm constraints).
3.If both all input-like and all hidden-like layers have vanishing weight perturbations, but
there exists some output-like layer l∈ LSAM , then set d= 1/2. Output-like layers are
effectively perturbed if and only if dl= 1/2. For all layers that have vanishing weight
perturbations, do not perturb these weights or set dl≥1/2−c∇for input-like, dl≥1−c∇
for hidden-like and dl>1/2for output-like layers (as required by the perturbation stability
and gradient norm constraints).
4. IfLSAM =∅, then set d >1/2or simply perform SGD.
Example F.1 (First-layer-only effective perturbations ).Instead of simply using the rule set above,
we derive the necessary choice of perturbation scaling from the scaling equalities and the norm
constraints (D.1) . To achieve first-layer effective perturbations, but trivial weight perturbations in all
other layers, we need ˜θW1= 1and˚˜θWl= 0, for which we will choose ˜θWl=n−1. This requires
setting
d1=−(c∇+d), d l= 2−c∇−d, d L+1= 2−d,
where one of the constraints (D.1) has to be fulfilled. Plugging the above dl-choices into (D.1)
results in the constraints d≤ −1/2,d≤1,d≤3/2, hence choose d=−1/2so that only the first
layer contributes non-vanishingly to the gradient norm. Note that ˜r= 0 and output perturbation
nontriviality holds if and only if min(bL+1, cL+1) = 1 (as in µP). We apply this perturbation scaling
in Appendix H.2 to show that propagating perturbations from early layers are not enough to inherit
SAM’s inductive bias that leads to improved generalization performance. ◀
36F.2 Other ways to introduce layerwise perturbation scaling
Before presenting alternative ways how layerwise perturbation scaling could be accomplished, let us
collect desirable properties that a definition should fulfill:
• Layerwise perturbation scaling should enable stable, effective perturbations in every layer.
•The perturbation step should require at most one additional forward and backward pass in
each update step.
•The adapted optimization algorithm should recover the original (SAM) algorithm when not
using layerwise perturbation scaling.
We start with the simplest case where the perturbations are normalized in each layer separately.
Remark F.2 (SAM with layerwise gradient normalization ).For(SAM) with layerwise gradient
normalization of the perturbations
εl=ρl· ∇WlL/∥∇WlL∥, (LN)
where ∥ · ∥ may denote the Frobenius or the spectral norm (equivalent under limited perturbation
batch size), the spectral scaling condition (∗)immediately yields the correct layerwise perturbation
scaling ρl!= Θ(p
fan_out /fan_in ). ◀
However, in practice, perturbations are usually globally normalized across layers, according to
the GitHub repositories provided by Foret et al. (2021); Samuel (2022); Kwon et al. (2021); An-
driushchenko and Flammarion (2022); Müller et al. (2024). Preliminary ablations in Appendix H.5
suggest that layer-coupled SAM with global normalization slightly outperforms SAM with layerwise
gradient normalization. As our goal in this paper is to study (SAM) as applied in practice, we
consider SAM with joint gradient normalization.
A first alternative to Definition 4 could scale perturbations after the joint gradient normalization.
Opposed to Definition 4, for this variant the perturbation norm, i.e. the radius of the adversarial
ascent ball, is not guaranteed to be ρn−d, butρ(P
l∈[L+1]ρ2
l)1/2. The correct perturbation scaling
for this version more immediately follows from the condition that perturbations scale like updates.
Remark F.3 (Layerwise perturbation scaling after joint gradient normalization ).For(SAM)
with joint gradient normalization of the perturbations
εl=ρl·∇WlL
∥∇WL∥,
the correct perturbation scaling in µP is given by ρl!= Θ( n1/2·fan_out /fan_in ).
To understand this scaling rule, note that for bL+1>1/2(such as in µP), the last layer always
dominates the gradient norm (see Eq. (E.2) for the TP argument), resulting in the scaling
∥∇WL∥F≈ L′(ft(ξt), yt)∥xL∥= Θ( n1/2).
Thus, compared to SAM without gradient normalization (Appendix F.3), ∥∇WL∥Falways con-
tributes the scaling n1/2. Noting that perturbations should scale like updates, and updates receive the
layerwise learning rates ηl!= Θ( fan_out /fan_in )concludes the derivation. ◀
In Definition 4, we accept the additional layer-coupling complications that the layerwise gradient
scaling before the joint gradient normalization entails in order to analytically control the perturbation
radius to ρn−d. To simplify the analysis as much as possible, we will first ensure width-independence
of the normalization, so that the layerwise perturbation scaling is not affected by the normalization
term. Then, layerwise perturbations should be scaled like updates.
Another alternative to layerwise perturbation scaling as in Definition 4 is motivated by the observation,
that in µP2with Definition 4, only the first layer dominates the joint gradient norm (Theorem D.11).
To let all layers contribute width-independently to the joint gradient norm, we can introduce even
more hyperparameters (with limited benefit) by decoupling the numerator and denominator scalings
in the perturbation. Opposed to Definition 4, the perturbation norm is again not analytically set with
the choice of ρ, but may be smaller. Empirically, we do not observe performance differences due to
denominator contribution scaling (Appendix H.4). This is the perturbation scaling we implement for
ViTs (see Algorithm 1 for details).
37Remark F.4 (SAM with decoupled perturbation numerator and denominator scaling ).For
(SAM) with perturbations
εl=ρn−dl∇WlL
∥v∥with ∥v∥2=L+1X
l=1n−2˜dl∥∇WlL∥2, (DP)
with layerwise perturbation radii ρ·n−dland separate gradient norm scaling n−˜dl. ◀
In all alternatives, nontrivial layerwise perturbation scaling is necessary for effective perturbations
in every layer, which necessarily changes the direction away from the original gradient direction.
Such a layerwise gradient rescaling can also be achieved by adapting the architecture with width-
dependent weight multipliers. The multipliers (a-µP2)achieve effective perturbations without
layerwise perturbation scaling such that all layers contribute non-vanishingly to the joint gradient
norm. They rescale the gradients equivalently to (DP) when scaling all denominator terms to be
width-independent. See Appendix F.6 for all details about weight multipliers.
Adapting the TP-based analysis. Our TP-based analysis covers all of the above perturbation scaling
alternatives with minor adjustments. We just have to replace the normalized TP scalar (E.2) . If we
want to express ∥∇WL∥F, we just drop all perturbation scaling terms n−dl. For the examples of
(LN) and (DP), we replace (E.2) in each layer separately by the normalized TP scalars,
∥∇W1Lt∥:=χt(dh1
t)Tdh1
t
n(ξT
tξt)1/2
,
with scaling θ∥∇1∥=n1/2θ∇for the first layer, where θ∇is overloaded to denote θ∇=n−bL+1in
the first step and θ∇=n−min(bL+1,cL+1)in all later steps (in µP,θ∇=n−1always),
∥∇WlLt∥:=χt 
(dhl
t)Tdhl
t
n(xl−1
t)Txl−1
t
n!1/2
,
with scaling θ∥∇L+1∥=nθ∇for all hidden layers l∈[2, L], and
∥∇WL+1Lt∥:=χt(xL
t)TxL
t
n1/2
.
with scaling√nfor the output layer, with respective normalized limits
˚χt(E[Z(dh1
t)2](ξT
tξt))1/2,˚χt(E[Z(dhl
t)2]E[Z(xl−1
t)2])1/2,˚χt(E[Z(xL
t)2])1/2,
where ˚χt=L′(˚ft(ξt), yt).
The adapted scalings can then be tracked as before to derive the maximal stable layerwise perturbation
scaling. Consider for example input layers in (LN) . InµP, we know ∥∇W1Lt∥= Θ( n−1/2)and
∇W1Lt= Θ( n−1)entrywise. Effective perturbations are achieved with ε1= Θ(1) , so choose
ρl=n1/2as expected from ( ∗). Proceed similarly for all layers and perturbation scaling variants.
F.3 Extension to SAM without gradient normalization
Andriushchenko and Flammarion (2022) and Andriushchenko et al. (2023a) consider the SAM update
without normalizing the gradient in the adversarial ascent. The corresponding update rule is given by
Wt=Wt−η∇wL(f(ξt;Wt+ρvt, yt)), yt), v t=∇wL(f(ξt;Wt).
TheNE⊗OR⊤program for this update rule with arbitrary vl
t=n−cl∇wL(f(ξt;Wt)is also easily
adapted from the above derivation. Just note that the gradient norm appears in an equation if and
only if the perturbation radius ρn−dappears. Without dividing by ∥vt∥, the parameter dbecomes
superfluous. Simply set d= 0 and remove the gradient norm constraints (D.1) to arrive at the
NE⊗OR⊤program and bcd-constraints for the update rule without gradient normalization.
Perturbation scaling dlplays a similar role as learning rate scaling clas both scale similar gradients.
We get effective perturbations in the l-th layer from the equation dl+ min( bL+1, cL+1) =cl+
38min(bL+1, cL+1, dL+1)inµP, which yields dl=clfor all l∈[L](since dL+1= 1 for stability).
In particular, in µP, the correct layerwise perturbation scaling of unnormalized gradients is
given by the rulefan out
fan inor the squared weight (update) spectral norm ∥Wl∥2
∗(Yang et al., 2023a),
which could be efficiently approximately tracked with a running power iteration.
Note that Dai et al. (2024) argue that the normalizing the gradients for the perturbation is crucial (in
standard parametrization) due to a stabilizing effect and an enhanced drift along manifolds of minima.
Monzio Compagnoni et al. (2023) find that unnormalized SAM gets stuck around saddles while SAM
slowly escapes through additional Hessian-induced noise. This suggests that the additional effort
of analysing the original SAM update rule with gradient normalization is necessary for practically
useful theory. From this paper’s point of view, the gradient normalization may be adding stability via
then−1/2contribution which allows to scale down ρless aggressively in practice.
F.4 Extension to Adaptive SAM
Adaptive SAM (ASAM) (Kwon et al., 2021) is motivated by a sharpness definition that is invariant
to parameter rescaling operators that leave the output function invariant, and can provide a further
improvement over SAM of 0.5%to1%, depending on the considered vision dataset and model
(Müller et al., 2024). Here we consider the two examples of elementwise rescaling operators (with
p= 2) and layerwise rescaling operators (with p= 2), which are the best performing SAM variant in
most settings in Müller et al. (2024).
Proposition F.5. Neither elementwise ASAM, which performs (SAM) but using the perturbation
rule(F.4) , nor layerwise ASAM, which performs (SAM) but using the perturbation rule (F.6) , can be
written as a NE⊗OR⊤program.
Proof sketch. Elementwise ASAM requires an elementwise multiplication of matrices, and layerwise
ASAM requires calculating the Frobenius norm of a matrix. A NonLin operation only takes vectors as
arguments, so NE⊗OR⊤calculations with a matrix require its multiplication with a vector. But then a
single coordinate of the resulting vector contains a mixture of an entire row of that matrix. Since we
are only allowed to define random vectors and matrices, and the NE⊗OR⊤master theorem states that
coordinates of NE⊗OR⊤vectors behave iid-like, this mixture cannot be disentangled by choosing
a structured vector. Hence, already at initialization, the square of individual entries/the Frobenius
norm of a random matrix cannot be exactly recovered by a function of matrix-vector products with
NE⊗OR⊤vectors.
Although ASAM is not formally covered by our theory, we still expect that the ASAM perturbations
are correlated with the gradient and therefore with the incoming activations, so that heuristically
we can still expect LLN-like behaviour and apply our scaling condition. If the perturbation rules
still behave LLN-like, then Table 1 summarizes which layers are effectively perturbed under global
scaling and provides the unique maximal perturbation scalings for all considered SAM variants. The
correct perturbation scaling in µP for other perturbation rules that behave LLN-like can always be
derived following the same steps:
1. In µP, it always holds that
Wl=

Θ(1) l= 1,
Θ(n−1/2)l∈[2, L],
Θ(n−1) l=L+ 1,and ∇WlL=Θ(θ∇) = Θ( n−1)l≤L,
Θ(1) l=L+ 1.
(F.2)
2.Assuming the normalization term in the denominator is scaled to Θ(1) , track the layerwise
scalings of the numerator. Maximal stable perturbations are always achieved with
˜δWl
t=Θ(1) l= 1,
Θ(n−1)l >1.(F.3)
This yields constraints for achieving maximal stable perturbations in each layer.
3.Now replace the norm constraints (D.1) by tracking the scalings of each layer’s contribution
to the update rule’s total normalization term.
4. To ensure normalization term scaling Θ(1) , iterate through the layers l:
39(a) choose dlto satisfy its norm constraint,
(b) choose dto induce maximal stable perturbations in that layer,
(c)choose all other dl′,l′̸=l, minimal to both satisfy its norm constraint as well as
perturbation stability ˜δWl
t=O(1) l= 1,
O(n−1)l >1.
5.From the above configurations, choose the unique one that yields maximal stable perturba-
tions in all layers.3
F.4.1 Elementwise ASAM
If we want to be invariant to elementwise rescaling operators Tl
w(x) =|Wl| ⊙xwhere x, Wl∈
Rm×nand⊙denotes elementwise multiplication, the resulting ASAM perturbation rule (where we
introduce (layer-wise) perturbation scalings {d} ∪ {dl}l∈[L+1]) replaces (LP) and is given by
˜δWl
t:=ρn−dn−dl|Wl| ⊙ |Wl| ⊙ ∇ WlL(f(ξt;Wt), yt)
∥∇elem
ASAM∥, (F.4)
with normalization
∥∇elem
ASAM∥:=L+1X
l=1n−dl|Wl| ⊙ ∇ WlL(f(ξt;Wt), yt)
F,
where the absolute values |Wl|are computed and multiplied elementwise. To find the correct
perturbation scalings, we track the typical elementwise scaling of each quantity as before.
Elementwise ASAM in µP.InµP, the layerwise weights and gradients scale as (F.2). For
∥∇elem
ASAM∥=O(1), we therefore replace the constraints (D.1) by the constraints
dl≥1/2−c∇,forl∈[L], d L+1≥ −1/2, (F.5)
where we can choose {dl}l∈[L+1]to achieve equality in at least one constraint to achieve
∥∇elem
ASAM∥= Θ(1) .
The layerwise perturbations scale as ˜δWl
t=n−d

Θ(n−d1θ∇) l= 1,
Θ(n−1−dlθ∇)l∈[2, L],
Θ(n−dL+1n−2)l=L+ 1.
Stable and nontrivial perturbations in each layer are achieved under condition (F.3) , which induces
the constraints for optimal layerwise perturbation scaling
d+dl=−c∇,forl∈[L], d +dL+1=−1.
Irrespective which of the above norm constraints (F.5) we satisfy, we need d=−1/2to achieve
optimal layerwise perturbation scaling. Hence d=dL+1=−1/2anddl= 1/2−c∇forl∈[L]
is the unique choice of {d} ∪ { dl}l∈[L+1]modulo norm scaling equivalence that achieves Θ(1)
perturbation scaling in all layers. With this choice all layers contribute non-vanishingly to the gradient
norm. In µPc∇= 1, so that dl=−1/2for all l∈[L+ 1], so that ASAM does not require
layerwise rescaling of the gradients, but upscaling of the perturbation by n1/2to achieve nontrivial
perturbations in any layer. This may explain why ASAM often outperforms SAM in large models:
By only requiring global scaling, ASAM achieves maximal stable perturbations in all layers if the
perturbation radius is tuned globally at every width.
If instead of a global gradient norm ∥∇elem
ASAM∥, one would want to normalize in each layer separately
with∥∇elem,l
ASAM∥:=n−dl∥|Wl| ⊙ ∇ WlL(f(ξt;Wt), yt)∥F,the layerwise perturbation scalings
become ˜δWl
t=n−dΘ(n−1/2)l= 1,
Θ(n−3/2)l >1.Again, to achieve maximal stable perturbations in all
layers we need d=−1/2and no layerwise adaptation of the gradient norm.
3There must exist such a choice of {dl}l∈[L+1]andd, because {dl}l∈[L+1]allow to set any relative scalings
between layers and ddetermines the global scaling, which overall allows to set all possible layerwise scalings.
Any deviation from the choice that achieves effective perturbations in all layers either results in blowup or a
vanishing effect of the weight perturbation in some layer. This shows uniqueness.
40F.4.2 Layerwise ASAM
ASAM with layerwise rescaling as in Müller et al. (2024) employs the layerwise transformations
Tl
w(x) =∥Wl∥F·x. This ASAM perturbation rule replaces (LP) and is given by
˜δWl
t:=ρn−dn−dl∥Wl∥2
F∇WlL(f(ξt;Wt), yt)
∥∇layer
ASAM∥, (F.6)
with normalization
∥∇layer
ASAM∥:=L+1X
l=1n−dl∥Wl∥F∥∇WlL(f(ξt;Wt), yt)∥F.
Layerwise ASAM in µP.InµP, we have ∥Wl∥F=

Θ(n1/2)l= 1,
Θ(n1/2)l∈[2, L],
Θ(n−1/2)l=L+ 1.
Hence, the norm constraints (D.1) are now replaced by
d1≥1−c∇, d l≥3/2−c∇forl∈[2, L], d L+1≥0.
The scale of the perturbation numerator now scales as ˜δWl
t=n−d

Θ(n−d1nθ∇) l= 1,
Θ(n−dlnθ∇) l∈[2, L],
Θ(n−dL+1n−1)l=L+ 1.
InµP, achieving maximal stable perturbations (F.3) is therefore equivalent to satisfying the constraints
d+d1= 0, d +dl= 1 forl∈[2, L], d +dL+1= 0.
Now we can simultaneously satisfy the first- and last-layer norm constraints with d1= 0 and
dL+1= 0, while achieving effective perturbations in all layers with d= 0anddl= 1. Satisfying the
norm constraint in the hidden layers with dl= 1/2would imply vanishing perturbations in the first
and last layer (by requiring d≥1/2).
F.5 Representing general architectures and adaptive optimizers as Tensor Programs
Here, we lay out explicitly how to write some of the building blocks in ResNets and ViTs in a
Tensor Program and provide further scaling considerations. According to Yang and Hu (2021), it is
straightforward to generalize scaling conditions that induce feature learning in MLPs to these other
common neural network building blocks. Since perturbations should always scale like updates, the
conditions for stable feature learning and those for stable effective perturbations are analogous.
One potential complication in the case of SAM would be a contribution to the joint gradient normal-
ization ∥vt∥that differs from the classical input, hidden or output layer contribution. But we will see
that these contributions do not differ for any of the considered layer types.
Layernorm. The Layernorm operation is defined as
hl+1
t=γl
txl
t−νl
t
σl
t+ε+βl
t,
where ε>0is a small positive constant, γl
t, βl
tare learnable parameters and νl
t=1
nPn
i=1(xl
t)i
is an Avg operation as in Yang and Littwin (2023, Def. 2.6.1) and σl
t=q
1
nPn
i=1(xl
t−νl
t)2is
a composition of Nonlin, Avg and Nonlin. The parameters γl
t, βl
tcan be seen as input weights to
the input 1. They should be initialized as γl
0= 1andβl
0= 0. In the forward pass, the layernorm
preserves stability hl+1
t= Θ(1) when γl
t+βl
t= Θ(1) except for the Lebesgue nullset of learning
rates for which they exactly cancel each other out. Recall the notation dz=θ−1
z∂f/∂z , where
θz=nCfor some C∈Rdenotes the width-dependent scaling. The derivatives are
dβl
t=dhl+1
t, dγl
t=dhl+1
txl
t−νl
t
σl
t+ε.
41These gradients coincide both in shape and scaling with the scaling we expect for an input layer,
resulting in the same gradient spectral/Frobenius norm scaling. Continuing the backward pass, using
∂σl
t
∂xl
t=xl
t−νl
t
nσl
t, we get
dxl
t= dhl+1
tγl
t1
σl
t+ε(I−1
n)−xl
t−νl
t
(σl
t+ε)2∂σl
t
∂xl
t
= dhl+1
tγl
t1
σl
t+ε(I−1
n11T)−xl
t−νl
t
(σl
t+ε)2(xl
t−νl
t)T
nσl
t
,
which preserves the order as long as γl
t= Θ(1) , since xl
t= Θ(1) , we know νl
t, σl
t= Θ(1) .
Note that Layernorm removes the necessity to avoid blowup in the activations xl
tin the forward
pass (ignoring potential numerical issues), and always rescales to Θ(max( γl
t, βl
t)). However, in the
backward pass, a scaling xl
t= Θ( nc), with c >0, results in dxl
t= Θ( n−cdhl+1
tγl
t), hence vanishing
gradients. The gradients would only stabilize if ϕ′(hl
t) = Θ( hl
t), but no popular activation function
has a scale equivariant derivative. Yang (2019) shows how to write Batchnorm and Average Pooling
as a Tensor Program.
Convolutions. Convolutional layers can be seen as a collection of dense weight matrices where
width corresponds to the number of channels (Yang, 2019). With kernel positions ker, input
channels [nl]and output channels [nl+1], the weights of a stride-1 convolution are given by
{Wl
iαβ}i∈ker,α∈[nl+1],β∈[nl], so that for each i∈ker,Wl
i∈Rnl+1×nlis a dense matrix. With
{xl
iα}i∈posl,α∈[nl], the convolution operation is given by
(Wl∗x)iα=X
β,j:j+i∈poslWl
jαβxl
i+j,β,
which performs MatMul and Avg and where ker, poslare assumed to be of fixed size. For kerof
fixed size, convolutional weights scale like hidden layer weight matrices, also in Frobenius norm
contributing to ∥vt∥.
Residual connections. A residual connection propagates the current activation forward, skipping
an arbitrarily complex nonlinear block fl
t:Rnl→Rnl+1in between, where fl
tcan depend on
time-dependent parameters like a weight matrix. The forward pass can be written as
xl
t=xl−1
t+fl
t(xl−1
t).
Ifxl
t= Θ(1) for all layers lholds in the model without residual connections, it also holds in the
model with residual connections. At fixed depth, fl
t=o(1)should be avoided, as it would hold
thatxl+1
t=xl
tin the infinite-width limit and the layer would be superfluous. The derivative of the
activations becomes
dxl−1
t=dxl
t+dxl
t∂fl
t
∂xl−1
t,
where the second term stays the same as without the residual connection. For the example of fl
tbeing
a fully connected layer we get dxl−1
t=dxl
t+ (Wl
t)T 
dxl
t⊙ϕ′(Wl
txl−1
t)
. In this example, the
derivative with respect to the weights becomes
∂ft
∂Wl
t=dxl
t∂xl
t
∂Wl
t=dxl
t∂fl
t
∂Wl
t= (dxl
t⊙ϕ′(Wl
txl−1
t))(xl−1
t)T,
where the residual connection does not alter the functional dependence on dxl
tandxl
tcompared to a
MLP, but implicitly influences the weight gradient since dxl
tandxl
tare altered. As for the forward
pass, the gradient scaling dxl
tgets stabilized in the backward pass so that∂fl
t
∂xl−1
tis now allowed
to be vanishing with width. Again, we are not aware of an architecture in which that would be
desirable. Since a residual connection does not introduce learnable parameters, it interferes in ∥vt∥
only implicitly through the stabilized gradients in earlier layers, which can contribute non-vanishingly
to∥vt∥even if later layers are wrongly scaled and their scaling is not adapted.
Adam as a base optimizer. When using Adam or similar adaptive optimizers as a base optimizer, the
learning rate should scale as Θ(1) for input-like layers and biases, and Θ(n−1)for hidden and output
42layers (Yang et al., 2022). Yang et al. (2023b) provide proofs for arbitrary optimizers that perform
generalized, nonlinear outer products. In the example of Adam, the update rule can be written as
ϕ(u1
α, . . . , uk
α, v1
β, . . . , vk
β) =X
iγiui
αvk
β/ X
iωi(ui
αvi
β)2!1/2
,
where γi, ωiare the weights that stem from the moving averages. By using a learning rate of n−1
and using the fact that both uandvhave approximately iid coordinates of order Θ(1) , the law of
large numbers yields Θ(1) updates of the form
1
nnX
β=1ϕ(u1
α, . . . , uk
α, v1
β, . . . , vk
β)xβ=Eϕ(u1
α, . . . , uk
α, Zv1, . . . , Zvk)Zx.
Any other learning rate scaling would either result in blowup or vanishing updates.
Adaptive optimizers have not been used for the ascent/perturbation step. In the descent/update step,
nothing changes compared to unperturbed optimization as long as we ensure stable perturbations.
F.6 Influence of width-dependent weight multipliers on bcd-parameterizations
Our definition of bcd-parameterizations is convenient because it purely adapts the learning algorithm
but not the architecture. We can also adapt the architecture by using layerwise width-dependent
weight multipliers to effectively perturb all layers without any perturbation scaling. The reason
is that layerwise weight multipliers scale the layerwise gradients. Here, we study how the introduction
of weight multipliers affects bcd-parameterizations.
In this section, we consider L-hidden layer MLPs with weight multipliers {al}l∈[L+1], width n∈N,
inputs ξ∈Rdin, and with outputs f(ξ) := n−aL+1WL+1xL(ξ)where the activations xL(ξ)are
defined via the iteration
h1(ξ) :=n−a1W1ξ, xl(ξ) :=ϕ(hl(ξ)), hl+1(ξ) :=n−al+1Wl+1xl(ξ).
We define abcd -parameterizations in the same way as bcd-parameterizations, but instead of MLPs we
use MLPs with weight multipliers {al}l∈[L+1].
Definition F.6 (abcd -parametrization ).Anabcd -parametrization {al}l∈[L+1]∪ {bl}l∈[L+1]∪
{cl}l∈[L+1]∪ {dl}l∈[L+1]∪ {d}defines the training of an MLP with weight multipliers {al}l∈[L+1]
with SAM in the following way:
(a) Initialize weights iid as Wl
ij∼ N(0, n−2bl).
(b) Train the weights using the SAM update rule with layerwise learning rates,
Wl
t+1=Wl
t−ηn−cl∇WlL(f(ξt;Wt+εt), yt),
with the scaled perturbation εtvia layerwise perturbation radii,
εt:=ρn−dvt
∥vt∥,with vt= (v1
t, . . . , vL+1
t), vl
t:=n−dl· ∇WlL(f(ξt;Wt), yt),(LP)
W.l.o.g. we set ∥vt∥= Θ(1) , which prevents nontrivial width-dependence from the denominator.
This imposes the constraints:
d1+a1≥1/2−c∇, d l+al≥1−c∇, d L+1+aL+1≥1/2,
with at least one equality required to hold, where l∈[2, L], and where ∇xLf=n−aL+1WL+1=
Θ(n−c∇)withc∇= min( bL+1+aL+1, cL+1+ 2aL+1). The normalization vt/∥vt∥removes one
degree of freedom from {dl}l∈[L+1]via the equivalence {d′
l}l∈[L+1]∼={dl}l∈[L+1]iff there exists a
C∈Rsuch that d′
l=dl+Cfor all l∈[L+ 1]. ◀
F.6.1 abcd -equivalence classes
Update scalings behave as in SGD. The weight multiplier n−alscales the gradient ∇Wlfbyn−al.
In the following forward pass, another multiplication of the weight updates with n−alleads to the
activation update scaling n−2al. This can be counteracted by adapting the learning rate scaling. For
43abc-parameterizations and SGD training, this induces the layerwise equivalence between parame-
terizations with (al, bl, cl)or with (al+θl, bl−θl, cl−2θl). The extension of all of our results to
Adam as a base optimizer is straightforward, since learning rate scalings and perturbation scalings
are decoupled. For Adam, clshould be adapted to cl−θl.
Again, perturbations with joint gradient normalization complicate matters compared to SGD and
Adam. Keeping the gradient norm scalings invariant under al7→al+θwould require dl7→dl−θ,
but keeping the activation perturbation scaling invariant would require dl7→dl−2θas for updates.
Consequently, an exact equivalence between abcd -parameterizations at finite width requires θto be
the same for all layers and the conflicting gradient norm in the denominator and perturbation scaling
in the numerator to be accounted for by adapting the global perturbation scaling d7→d−θ(together
withdl7→dl−θ). In other words, (SAM) with layer-coupling gradient normalization (LP) does
not have layerwise analytical equivalence classes at finite width. Below, we provide two alternative
perturbation rules that resolve these complications and recover layerwise equivalence classes. The
following lemma formally states the layer-coupled equivalence relation for the perturbation rule (LP) .
All proofs are provided at the end of this section.
Lemma F.7 (abcd -equivalence classes ).Letft(ξ)denote the output of a MLP in a stable abcd -
parameterization with weight multipliers {al}l∈[L+1]aftertsteps of training with the SAM update
rule with layerwise perturbation scaling (LP) using a fixed sequence of batches and evaluated on
input ξ. Then for any θ∈Rand any C∈R,ft(ξ)stays fixed for all tandξif, for all l∈[L+ 1],
(al, bl, cl, dl, d)is reparameterized to (al+θ, bl−θ, cl−2θ, dl−θ+C, d−θ).
Remark F.8 (Infinite-width equivalences ).In the infinite-width limit, abcd -parameterizations
remain equivalent under (al+θl, bl−θl, cl−2θl, dl−2θl, d)layerwise as long as the set of layers
that contribute to the gradient norm non-vanishingly remains invariant. The gradient norm constraints
for∥vl∥=O(1)become
d1+a1≥1/2−c∇, d l+al≥1−c∇, d L+1+aL+1≥1/2,
where ∇xLf=n−aL+1WL+1= Θ( n−c∇)withc∇= min( bL+1+aL+1, cL+1+ 2aL+1)remains
invariant under equivalence transformations. ◀
Remark F.9 (SAM with layerwise gradient normalization ).As the layer coupling is induced by the
joint gradient normalization in the perturbations, layerwise gradient normalization simplifies the anal-
ysis. For (SAM) with layerwise gradient normalization (LN) of the perturbations global perturbation
scaling dis superfluous, and there exist layerwise equivalence classes: For any {θl}l∈[L+1]⊂R,
(al, bl, cl, dl)is equivalent to (al+θl, bl−θl, cl−2θl, dl−θl).
To understand this equivalence, observe that any layerwise gradient scaling is cancelled out by the
normalization ∇WlL/∥∇WlL∥. Only the n−alfactor from subsequent forward passes has to be
counteracted. ◀
Remark F.10 (SAM with decoupled perturbation numerator and denominator scaling ).A
perturbation rule with joint gradient normalization and layerwise equivalence classes can be achieved
by introducing even more hyperparameters and decoupling numerator and denominator scalings
of each layer. For (SAM) with perturbations (DP) with layerwise perturbation radii ρ·n−dland
separate gradient norm scaling n−˜dl, global perturbation scaling dis superfluous, and there exist
layerwise equivalence classes: For any {θl}l∈[L+1]⊂R,
(al, bl, cl, dl,˜dl)is equivalent to (al+θl, bl−θl, cl−2θl, dl−2θl,˜dl−θl).
This perturbation rule also allows us to recover an analytical equivalence between trivial weight
multipliers al= 0for all l, and any other weight multipliers. ◀
F.6.2 µP2under non-trivial weight multipliers.
Our goal here is to find the weight multipliers that simplify the necessary perturbation scaling for
effective perturbations in all layers as much as possible. The non-existence of layerwise equivalence
classes in abcd -parameterizations from (LP) is not an issue if we are interested in effective perturba-
tion properties and recovering µP2for arbitrary weight multipliers {al}l∈[L+1], as the equivalence
breaks due to varying gradient norm contributions, which are inconsequential for achieving effective
perturbations.
44As we aim to reproduce µP2, we restrict ourselves to the µP equivalence class of abc-
parameterizations. We do not allow layerwise perturbation scaling and are interested in the maximal
stable choice of global perturbation scaling ρn−dto at least achieve non-vanishing perturbations in
some layers. The following lemma shows even more: The choice
al=−1/2·I(l= 1) + 1 /2·I(l=L+ 1)
achieves effective perturbations in all layers with the naive (SAM) update rule with naive
perturbation scaling ρ·n0, and all layers contribute non-vanishingly to the joint gradient norm.
Hence this seems to be a natural choice of weight multipliers for SAM. However, it is in conflict
with unit scaling considerations (Blake et al., 2024). Effectively, naive learning rate and perturbation
scaling with these multipliers is equivalent to (DP) where all denominator terms are scaled to be
width independent, as implemented by Algorithm 1, which resembles our implementation for ViTs.
Our ablations in Appendix H.4 suggest that gradient norm contributions have a negligible effect on
generalization performance.
Lemma F.11 (Naive perturbation scaling can effectively perturb all layers ).Consider an abcd -
parameterization where {(al, bl, cl)}l∈[L+1]are chosen from the µP equivalence class, and where
there is some C∈Rsuch that dl=Cfor all l∈[L+ 1]. This reduces to training a MLP with
weight multipliers with (SAM) with global perturbation scaling ρn−dfor some d∈R. Effective
perturbations in all layers are achieved and all layers contribute non-vanishingly to the gradient
norm if and only if
a1=−d−1/2, a l=−dforl∈[2, L], a L+1=−d+ 1/2.
Achieving µP2with the current implementation of the mup-package requires both an adaptation of
the architecture and of the learning algorithm, as the following lemma shows. Hence the package is
not particularly suited for SAM learning in µP2when the goal is simple perturbation scaling.
Lemma F.12 (Effective perturbations with the mup-package ).Consider an abcd -parameterization
where{(al, bl, cl)}l∈[L+1]are chosen from the µP equivalence class, and with the weight multipliers
aL+1=I(l=L+ 1) as in the mup-package.
(a)(mup-package global scaling effectively perturbs hidden layers) Under global scaling
dl=C,C∈R, for all l∈[L+ 1], maximal stable perturbations are achieved with d= 0.
In this parameterization, hidden layers are effectively perturbed, but input and output layers
are not effectively perturbed.
(b)(µP2with the mup-package) Effective perturbations in all layers are achieved with the
choice d=d1=dL+1=−1/2anddl= 1/2forl∈[2, L].
The following lemma covers the general case how to achieve µP2given arbitrary weight multipliers.
Lemma F.13 (µP2with arbitrary weight multipliers ).Consider an abcd -parameterization where
{(al, bl, cl)}l∈[L+1]are chosen from the µP equivalence class. Then effective perturbations in all
layers are achieved with the choice d= min l∈[L+1](−al−1/2I(l= 1) + 1 /2I(l=L+ 1)) , and
d1=−1−d−2a1, d l=−d−2al,forl∈[2, L], d L+1= 1−d−2aL+1.
The following lemma shows that weight multipliers that achieve µP2with naive perturbation scaling
under perturbations with layerwise normalization (LN) are exactly the same as the ones for (LP).
Lemma F.14 ((LN) with naive perturbation scaling can effectively perturb all layers ).Consider
(SAM) with layerwise normalization (LN) . Assume {(al, bl, cl)}l∈[L+1]are chosen from the µP
equivalence class, and assume there is some C∈Rsuch that dl=Cfor all l∈[L+ 1]. Then all
layers are effectively perturbed if the multipliers are chosen as
a1=−1/2−C, a l=−C, a L+1= 1/2−C.
Proof of Lemma F .14. As derived in Appendix F.7, under al= 0for all l∈[L+ 1], all layers are
effectively perturbed if and only if dl=−1/2·I(l= 1) + 1 /2·I(l=L+ 1) . Now we can exploit
the layerwise equivalence relation to enforce dl=Cin each layer by adapting all al.
Proof of Lemma F .11. In general, in the abc-equivalence class of µP, the l-th layer’s gradient norm is
scaled by n−al. This induces the generalized gradient norm constraints for ∥∇WL∥= Θ(1) ,
d1+a1≥ −1/2, d l+al≥0, d L+1+aL+1≥1/2.
45Effective perturbations are achieved when ρn−d−dl−al∇WlL= Θ( n−I(l>1)), which induces the
perturbation stability constraints
d+d1+ 2a1≥ −1, d +dl+ 2al≥0, d +dL+1+ 2aL+1≥1,
with effective perturbations whenever the equality of the respective layer holds.
Under global scaling, the gradient norm constraints become, for some C∈R,
C+a1≥ −1/2, C +al≥0, C +aL+1≥1/2,
and the conditions for effective perturbations become
d+C+ 2a1≥ −1, d +C+ 2al≥0, d +C+ 2aL+1≥1.
Asd+Cis a common term in all layers, we get the relations al=a1+ 1/2,aL+1=a1+ 1, so that
all gradient norm constraints are simultaneously satisfied with C=−aland effective perturbations
are achieved in all layers with d=−al.
Proof of Lemma F .12. Under the choice al=I(l=L+ 1) , the gradient norm constraints become
d1≥ −1/2, d l≥0, d L+1≥ −1/2,
and the conditions for effective perturbations become
d+d1≥ −1, d +dl≥0, d +dL+1≥ −1.
Proof of (a):
Satisfying the gradient norm constraints with global scaling requires dl= 0for all l∈[L+ 1], then
the minimal stable choice of disd= 0which only effectively perturbs hidden layers.
Proof of (b):
The choice d=−1/2andd1=−1/2saturates the gradient norm constraint and achieves effective
perturbations in the input layer. Then the choice dl= 1/2anddL+1=−1/2satisfies the gradient
norm constraints and achieves effective perturbations in all layers.
Proof of Lemma F .7. To understand the influence of weight multipliers on updates and perturbations,
first note that under an equivalence transformation of all abcd -parameters w.l.o.g from al= 0for
alll∈[L+ 1], the scalings of hl, xland of n−alWlremain invariant. This implies that the scalings
of∇xLf=n−aL+1WL+1,∇hlf=∇xlf⊙ϕ′(hl)and∇xlffor all l∈[L]also remain invariant.
Hence the weight gradients, ∇WL+1f=n−aL+1xLand∇Wlf=n−al∇hlf·(xl−1)⊤are scaled
byn−alin each layer.
In the following forward pass, we get
hl=n−al(Wl+ ∆Wl)xl−1=n−al(Wl−ηn−cl∇WlL)xl−1,
so that activation/output updates and perturbations of layer lare scaled by n−2al.
Again, a complication compared to SGD or Adam arises through the gradient normalization of SAM’s
weight perturbation. If the gradients are simply normalized layerwise εl=ρ·n−dl·∇WlL/∥∇WlL∥,
then−al-term from the backward pass cancels out, and only in the forward pass we get a scaling n−al.
Hence an exact layerwise equivalence still exists for SAM with layerwise gradient normalization:
(al, bl, cl, dl)is equivalent to (al+θl, bl−θl, cl−2θl, dl−θl).
Under joint gradient normalization (SAM) , as we consider in our definition of bcd-parameterizations,
keeping the gradient norm scalings invariant under al7→al+θwould require dl7→dl−θ, but
keeping the perturbation scaling invariant would require dl7→dl−2θas for updates. Consequently,
due to the layer coupling of joint gradient normalization ∥∇WL∥, an exact equivalence between
abcd -parameterizations at finite width requires θto be the same for all layers and the conflicting
gradient norm in the denominator and perturbation scaling in the numerator to be accounted for by
dl7→dl−θandd7→d−θ.
46In the infinite-width limit, abcd -parameterizations remain equivalent under (al+θl, bl−θl, cl−
2θl, dl−2θl, d)layerwise as long as the set of layers that contributes to the gradient norm non-
vanishingly remains invariant. The gradient norm constraints for ∥vl∥=O(1)become
d1+a1≥1/2−c∇, d l+al≥1−c∇, d L+1+aL+1≥1/2,
where ∇xLf=n−aL+1WL+1= Θ( n−c∇)withc∇= min( bL+1+aL+1, cL+1+ 2aL+1)remains
invariant under equivalence transformations.
Proof of Lemma F .13. First, the choices of dlensure that the constraints for effective perturbations
from the proof of Lemma F.11 are saturated in each layer. It is left to show, that these choices
satisfy the ∥∇WL∥= Θ(1) -constraints. For input layers, since −d≥a1+ 1/2, it holds that
d1+a1≥ −1/2. For hidden layers, since −d≥al, it holds that dl+al≥0. For output layers, since
−d≥aL+1−1/2, it holds that dL+1+aL+1≥1/2. Observe that the minimizer in the definition of
dsaturates its gradient norm constraint.
F.7 The spectral perspective on µP2
While Tensor Programs allow to track the transformations of vectors like activations, Yang et al.
(2023a) provide an equivalent formulation in terms of weight matrix spectral norms. They find that the
spectral norm measures the effect of a weight update on the activations, under certain non-cancellation
assumptions and limited batch size. For all MLP layers, they show that µP is equivalent to achieving
the condition
∥Wl
t∥∗= Θ r
fan_out
fan_in!
and∥∆Wl
t∥∗= Θ r
fan_out
fan_in!
,
at all times t, where Wl
t:Rfan_in→Rfan_out. This condition is achieved with initialization σl, SGD
learning rate ηland Adam learning rate ηAdam
l chosen as,
σl= Θ 
1√fan_inmin(
1,r
fan_out
fan_in)!
, η l= Θfan_out
fan_in
, ηAdam
l = Θ1
fan_in
.
This generalizes µP to varying widths inside the network. For varying widths, we adopt the notation
Wl:Rnl−1→Rnlwithn0=dinandnL+1=dout, whereas fan_in andfan_out always adapt
to the weight matrix under consideration.
To understand why the spectral norm is desirable, note that ∆Wl=ηl∇hlL(xl−1)⊤is low rank
and aligned with the incoming activations. For batch size 1, we even have rank-1 updates with
∥∆Wl∥∗=ηl∥∇hlL∥2∥xl−1∥2, aligned with the incoming activations xl−1, hence ∥∆Wlxl−1∥2=
∥∆Wl∥∗∥xl−1∥2. This allows to achieve ∥∆xl∥2= Θ(√nl)irrespective of the layer type with
∥∆Wl
t∥∗= Θ(p
nl/nl−1).
Our simple condition that perturbations should scale like updates, which is rigorously justified by our
Tensor Program based proof in Appendix E, now allows to derive the correct perturbation scalings
using the spectral weight perspective.
Layerwise perturbations. As a simple starting point, consider a variant of (SAM) that does not
globally normalize the gradient of all layers jointly, but uses layerwise normalization (LN) , resulting
in the layerwise perturbation rule,
εl=ρl· ∇WlL/∥∇WlL∥,
where∥·∥may denote either the spectral or the Frobenius norm (equivalent under limited perturbation
batch size). Without the global normalization, the scalings of all layers are not coupled, and the
spectral condition ∥εl∥∗= Θ(p
fan_out /fan_in )immediately requires choosing
ρl=ρ·p
fan_out /fan_in
for effective perturbations in layer lwith width-independent hyperparameter ρ≥0.
Perturbations with global gradient normalization. Perturbations that are globally normalized
across layers have usually been implemented practice according to the GitHub repositories provided
47by Foret et al. (2021); Samuel (2022); Kwon et al. (2021); Andriushchenko and Flammarion (2022);
Müller et al. (2024). Since we are interested in analysing (SAM) as it is applied in practice, we study
variants with joint gradient normalization in more detail. Preliminary ablations in Appendix H.5
suggest that layer-coupled SAM with global normalization slightly outperforms SAM with layerwise
gradient normalization. To simplify the analysis as much as possible, we will first ensure width-
independence of the normalization, so that the layerwise perturbation scaling is not affected by the
normalization term. Then, layerwise perturbations should again be scaled like updates.
Separate denominator scalings. If we allow to scale each denominator term separately from the
corresponding numerator term (DP) , the perturbation radius in each layer for the numerator can be
scaled like updates, ρl= Θ fan_out
fan_in
.
Now, to ensure Θ(1) in the denominator, each input and hidden-like gradient norm ∥∇WlL∥F,
l∈[L], achieves width-independence if it scaled byp
fan_out /fan_in . The same rule applies to
biases when understanding them as weights R→Rnlto the input 1. These scalings are derived in
the next paragraph. The last-layer gradient norm ∥∇WL+1L∥Fshould be scaled as (nLnL+1)−1/2,
and∥∇bL+1L∥Fasn−1/2
L+1.
If we care about the correct width-independent constants, observe that the learning rate scaling
ηL+1= Θ( fan_out /fan_in )induces ∥WL+1∥=∥∆WL+1∥= Θ(q
n3
L+1/nL). If we wanted
to achieve ∆WL+1= Θ(p
nL+1/nL)we would need ηL+1= Θ(1 /fan_in ). AsnL+1=dout
is width-independent,p
fan_out /fan_in would result in the same width-dependent scaling for
the last layer, but ignoring large constants can introduce a significant width-independent spectral
distortion. For example in ImageNet1K, nL+1is large. By tuning input, hidden and output multipliers
such constant distortions may be corrected. The multiplier used in the mup-package does not correct
this distortion. Using a base width at which SP is recovered may also cement such spectral distortions,
if no multipliers are tuned.
Derivation of gradient norm ∥∇WlL∥Fscalings. In this paragraph, ∥ · ∥ may denote the Frobenius
or spectral norm. As all matrices are of limited rank, both norms scale equivalently. As a first step,
∥∇hlL∥= Θ(1√nl)can be reconstructed from
Θ(p
nl/nl−1) =∥∆Wl∥∗=ηl∥∇hlL∥∥xl−1∥2=nl/nl−1·√nl−1∥∇hlL∥.
Now, for input and hidden layers, ∥∇WlL∥=∥∇hlL∥∥xl−1∥2= Θ(p
nl−1/nl). Multiplying by
the inverse yields width-independent scaling. The output layer gradient ∇WL+1L ∈RnL+1×nLis
given by (∇WL+1L)ij=xL
j= Θ(1) , so that ∥∇WL+1L∥= Θ(√nLnL+1). Biases before the last
layer follow the scheme ∥∇blL∥=∥∇hlL∥= Θ(p
1/nl) = Θ(p
fan_in /fan_out ). The last
layer bias ∥∇bL+1L∥=√nL+1scales width-independently as it should, but needs to be scaled by a
different constant 1/√fan_out than earlier layers.
Extensions to ASAM. As ASAM cannot be written as a NE⊗OR⊤program, its scaling can only be
derived heuristically. As provided in Table 1 and derived in Appendix F.4, elementwise ASAM scales
all layer types correctly in relation to each other, and it suffices to rescale the global perturbation
radius by√nL, assuming all width dimensions scale proportionally. For SAM-ON, we only perturb
input-like layers such as normalization layers. As the conditions for correct scaling remain the same,
the above scalings for input layers in SAM also apply to SAM-ON.
For layerwise ASAM, first note that ∥Wl
t∥F= Θ(∥Wl
0∥F) = Θ(√nl)for input and hidden lay-
ersl∈[L]. As the numerator contains ∥Wl
t∥2
F, it requires the layerwise perturbation scaling
1
fan_in. In the denominator, width independence is achieved with the multiplierq
1
fan_in, since
∥Wl∥F∥∇WlL∥∗=√nlq
nl−1
nl=√nl−1. Again, the output layer requires a special treatment.
Due to its small initialization, it holds that ∥WL+1∥2
F=∥∆WL+1∥2
F= Θ(n3
L+1
nL). For perturbations
that fulfill the spectral condition ρL+1∥WL+1∥2
F∥∇Wl+1L∥∗= Θ(q
nL+1
nL), we need to choose
ρL+1=ρ·1
n3
L+1(width-independent, but very small). The last-layer denominator term scales as
∥WL+1∥F∥∇Wl+1L∥∗= Θ(q
n3
L+1
nL·√nLnL+1) = Θ( n2
L+1), which is width independent, but
48can be a large constant, as for ImageNet1K. The output bias numerator exactly conforms with the
correct scaling ∥∇bL+1L∥2
F=nL+1=fan_out /fan_in = Θ(1) .
Note that weight decay may break statements like ∥Wl
t∥F= Θ(∥Wl
0∥F)over long training. Everett
et al. (2024) have recently observed more generally that scalings may evolve differently over long
training than predicted by pure infinite-width TP theory, because alignments evolve dynamically
between CLT- and LLN-like behaviour.
Using the mup-package. Themup-package introduces the output layer weight multiplier n−1
Lso that
input and output layer learning rates may be scaled by the same width-dependent factor. Hence, only
the last-layer scalings change. The scalings of n−1
LWL+1andn−1
L∆WL+1remain the unique ones
that achieve µP, but∇WL+1Lis scaled by n−1
L. This requires adapting the last-layer learning rate
ηL+1to scale like input layers. For SAM, the last-layer perturbation radius can now be scaled like
input layers. That is, assuming proportionally growing width n, in the numerator ρL+1=ρ1=ρ·n
andρl=ρforl∈[2, L], and the gradient norm contributions should be scaled by√nfor input and
output layers, and by 1for hidden layers. The Tensor Program perspective on weight multipliers
can be found in Appendix F.6. The correct width-independent constants are achieved with the
last-layer numerator scaling ρL+1=ρ·nLand the last-layer denominator scalingp
nL/nL+1, since
∇WL+1L= Θ(p
nL+1/nL)and for the numerator we get an additional n−1
Lin the forward pass.
For SAM-ON nothing changes, as only input-like layers are perturbed. For elementwise ASAM,
ignoring width-independent constants, nothing changes as the weight multiplier n−1
Lincreases
the weight scaling WL+1and decreases the gradient scaling ∇WL+1Lby the same amount. The
additional n−1
L-factor in the numerator is cancelled out by the additional WL+1-factor. For the
correct width-independent constants with decoupled numerator and denominator scaling, we would
scale the denominator byq
nL/n3
L+1with or without weight multiplier, and scale the numerator by
ρL+1=ρ·nL/n2
L+1with or without weight multiplier. For the example of layerwise ASAM, we
still get for the denominator ∥WL+1∥F∥∇Wl+1L∥∗= Θ( n2
L+1), again because the weights WL+1
are scaled up by nLand the gradient is scaled down by the same amount. In the numerator, the
upscaling of the weights also cancels out the downscaling of the gradient and additional n−1
Lin the
subsequent forward pass, leading to an unchanged ρL+1=ρ·n−3
L+1, which is width-independent but
potentially leads to numerical issues.
Code for µP2with separate denominator scalings. Algorithm 1 provides a PyTorch code example
that implements the above µP2scalings for SAM, scaling the gradient norm contributions of all
layers to Θ(1) (equivalent to (a-µP2)together with naive perturbation and learning rate scaling). We
adapt the popular SAM implementation Samuel (2022) using the mup-package. This code resembles
our implementation for the ViT experiments. In the mup-package, ‘vector-like ´parameters scale as
n×constant orconstant ×nand include input and output weights. The last-layer multiplier n−1
Lis chosen so that input and output layers can be scaled by the same width-dependent factor. On the
other hand, ‘matrix-like ´parameters scale as n×nand include hidden weights. The implementation
uses a base width at which µP2and SP are equivalent; all width-dependent scalings then scale with
width-multipliers width /base_width . This allows to immediately transfer well-performing settings
from SP to µP2.
Let us recapitulate how the µP2scaling in the following code arises. The crucial variables to track are
factor ,group["rho"] andgroup["gradnorm_scaling"] . For limited batch size, the spectral
and Frobenius norm of gradients scale equivalently, and we get, for all l∈[L],
∥∇WlL∥F= Θ(∥∇WlL∥∗) = Θ r
fan_in
fan_out!
.
We want to scale each weight’s contribution in the denominator to be width-independent, hence need
the factor√
factor with factor =fan_out /fan_in . For the numerator, the spectral condition (∗)
demands ∥ρl· ∇WlL∥∗!= Θ(q
fan_out
fan_in), so that we need to scale the weight’s perturbation radius
toρl=ρ·factor . Since the mup-package sets the last-layer weight multiplier such that input and
output layers can be scaled in the same way, the implementation is short. For optimal numerical
properties however, this choice of multipliers is sub-optimal (Blake et al., 2024).
491import math , torch
2from mup import MuAdamW
3
4# specify parameterization
5parameterization = ’mupp ’ # ’sp - naive ’, ’mup - naive ’
6# for ’mup - global ’ use ’mup - naive ’ and scale rho accordingly
7
8# specify model and hyperparameters
9model , lr , rho , weight_decay , last_layer_weight_name = ...
10
11
12
13# adapt SAM to allow gradient norm scaling of each weight tensor
14class SAM ( torch . optim . Optimizer ):
15 ...
16
17 def grad_norm ( self ):
18 grads = []
19 for i, group in enumerate ( self . param_groups ):
20 for p in group [" params "]:
21 grads . append (( group["gradnorm_scaling"] * p. grad ). norm (p=2) )
22 norm = torch . stack ( grads ). norm (p=2)
23 return norm
24
25 @torch . no_grad ()
26 def first_step ( self ): # perturbation step before the weight update
27 grad_norm = self . grad_norm ()
28 for group in self . param_groups :
29 scale = group ["rho"] / ( grad_norm + 1e -12)
30 for p in group [" params "]:
31 if p. grad is None : continue
32 self . state [p][" old_p "] = p. data . clone ()
33 e_w = p. grad * scale .to(p)
34
35 p. add_ (e_w) # climb to the local maximum "w + e(w)"
36
37
38
39# set width - dependent rho and gradient norm scaling for each weight
40param_groups = []
41for name , p in model . named_parameters ():
42 if p. infshape . ninf () == 0 or ’naive ’ in parameterization :
43 factor = 1
44 elif p. infshape . ninf () == 1:
45 # vector - like
46 for d in p. infshape :
47 if d. base_dim is not None :
48 factor = d.dim / d. base_dim # width
49 break
50 elif p. infshape . ninf () == 2:
51 # matrix - like
52 factor = (p.infshape[0].dim/p.infshape[1].dim) * (p. infshape [1]. base_dim /
p. infshape [0]. base_dim ) # fan_out / fan_in
53 else :
54 raise NotImplementedError
55
56 group = {
57 " params ": [p],
58 "lr": lr ,
59 "rho": rho * factor,
60 " gradnorm_scaling ": math.sqrt(factor),
61 }
62 param_groups . append ( group )
63
64
5065optimizer = SAM( param_groups ,
66 base_optimizer = MuAdamW if parameterization == ’mup ’ else torch . optim .
AdamW , weight_decay = weight_decay )
Algorithm 1: Pytorch implementation of µP2for SAM using the mup-package. Key changes from
the original implementation that correct the layerwise perturbation scaling are highlighted with
gray boxes. This code decouples the scalings of numerator and denominator terms following (DP) ,
and scales the gradient norm contributions of all layers by group["gradnorm_scaling"] in the
denominator to be width-independent. The numerator terms group["rho"] of all weight tensors are
scaled to achieve effective perturbations. This scaling is equivalent to (a-µP2)together with naive
perturbation and learning rate scaling.
G Experimental details
If not mentioned otherwise, experiments use the settings specified in this section.
Implementation details. For MLPs, we exactly implement our Definition 4 of bcd-parameterizations
to precisely validate our theoretical results. For ResNets and ViTs, the width varies inside the network,
so that we implement the spectral scaling rules derived in Appendix F.7. Like the mup-package, we
introduce a base width at which SP and µP are equivalent, allowing to immediately transfer setups
that perform well in SP. We use the mup-package only for ViTs, and our implementation of µP2
resembles the pseudocode provided in Algorithm 1. For ResNets, we use no width-dependent last-
layer multiplier. At initialization, µP differs from SP only through a smaller last layer initialization.
For MLPs we exactly implement the bcd-parameterization with bL+1= 1, but use the large width-
independent input layer initialization variance 2instead of the width-independent 2/dininµP, which
can be seen as a tuned initialization variance multiplier. For ResNets and Vits, we initialize the last
layer to 0inµP, which corresponds to bL+1→ ∞ and which recovers the limit behaviour f0→0
already at finite width. We are working on making Python code to reproduce all of our experiments
publicly available.
MLPs. We train 3-layer MLPs without biases with ReLU activation function for 20epochs with
constant learning rate, using SGD as base optimizer as specified in Definition 4, but allow for SGD
batchsize larger than 1, defaulting to batch size 64. We evaluate the test accuracy after every epoch
and use the snapshot across training with the best accuracy. This is necessary as the test accuracy is
not monotonically increasing across training, while the training accuracy is. For ResNets we do not
observe such harmful overfitting. For the standard parametrization, we use He initialization (He et al.,
2015) and don’t tune multipliers to mimic standard training procedures. For µP, we resort to the
optimal multipliers from Yang et al. (2022). We then find the optimal learning rate and perturbation
radius for each bcd-parametrization and SAM variant separately.
ResNets. For ResNet18 experiments, we augment the CIFAR10 data with random crops and random
horizontal flips, set labelsmoothing to 0.1and use a cosine learning rate schedule. ResNets in
µP have base width 0.5, gradient norm scaling according to Definition 4 and their last layer is
initialized to 0. For SP, we again adopt the standard hyperparameters from Müller et al. (2024)
by using a momentum of 0.9, weight decay 0.0005 , an output multiplier of 1.0, and individually
tuned learning rate and perturbation radius for each SAM variant. For µP, at base width multiplier
0.5compared to the original width, for each SAM variant, we perform a random grid search over
the hyperparameters learning rate, perturbation radius, output multiplier [2−8,2−7, . . . , 28], weight
decay [0,10−5,10−4,5·10−4,10−3,10−2]and momentum [0,0.1,0.4,0.7,0.9]. Learning rate and
perturbation radius grids were either set to [2−10,2−9, . . . , 21]or centered around recommendations
from the literature. The optimal hyperparameter configurations found from at least 150runs for each
SAM variant are summarized in Table G.1. Learning rates and perturbation radii were further tuned
with the experiments from Appendix H.3.3.
ViTs. We train ViT-S/16 with 6 layers and 12 attention heads on ImageNet1K (Deng et al., 2009)
and a ViT-S/4 with 12 layers and 12 attention heads on CIFAR100 (Krizhevsky et al., 2009) (see
Appendix H.6), again adopting the hyperparameter settings from Müller et al. (2024). This means we
use AdamW as a base optimizer with warmup and a cosine learning rate decay. For CIFAR100, we
use random crops, random horizontal flips and AutoAugment as data augmentations. For Imagenet
we use the original preprocessing from Huggingface vit-base-patch16-224 (Wu et al., 2020).
ForµP, we tune multipliers at a basewidth 384, initialize the last layer and query weights to 0. By
51using the µP package, the relative perturbation scalings change as explained in Appendix F.7 and
Appendix F.6. Global and naive perturbation scaling in µP now coincide. Here, instead of the
original perturbation scaling Definition 4, we scale the gradient norm contributions of all layers in
the denominator to Θ(1) . The hyperparameter choices for ViTs on CIFAR100 and ImageNet are
summarized in Table G.2. For µP, the learning rate, perturbation radius, input multiplier, output
multiplier and weight decay were tuned using 3independent runs of Nevergrad NGOpt with budget
56on ImageNet. The same multipliers are used on CIFAR100.
Figures. Whenever multiple runs with independent random seeds are used for training, confidence
bands cover the interval from the empirical 2.5%- to the empirical 97.5%-quantile. The line then
denotes the average of all runs. When confidence bands are given, but the number of independent
runs is not specified, the number of runs defaults to 4.
Computational resources. We ran all of our experiments on Amazon EC2 G5 instances each
containing up to 8 NVIDIA A10G GPUs. On a single GPU, our µP2-SAM training script for MLPs
of width 4096 on CIFAR10 takes 502 seconds to run in total (25 seconds per epoch), where data
handling takes most of the time. The training times for ResNets and ViTs are presented in Table G.3.
Hyperparam. SAM SAM-ON ResNet18 Elem. ASAM Layer ASAM
SP µP2SP µP2SGD SP µP2SP µP2
Training epochs 200
Batch size 64
LRη 0.05 2−40.05 2−40.05 2−40.1 2−4
LR decay Cosine
Weight decay 0.0005
Momentum 0.9
Labelsmoothing 0.1
Pert. radius ρ 0.1 2−40.5 5·2−42 10·2−40.02 2−6
Output multiplier 1 0.125 1 0.125 1 0.125 1 0.125
Table G.1: (ResNet-18 hyperparameters for CIFAR10) Hyperparameters for SP are taken from
Müller et al. (2024). Learning rate and perturbation radius are tuned using the experiments in
Appendix H.3.3. ResNets in µP have base width 0.5, gradient norm scaling according to Definition 4
and their last layer is initialized to 0.
Hyperparam. SAM on ImageNet1K SAM on CIFAR100
SP µP2shared SP µP2
Training epochs 100 300
Batch size 128
LRη 0.001 0.00226 0.0005
LR warmup epochs 10 30
LR decay Cosine
Weight decay 0.1 0.0872 0.05
Labelsmoothing 0.1
Pert. radius ρ 1 1.1939 0.25 0.25
Input multiplier 1 1.7309 1 1.7309
Output multiplier 1 4.0946 1 4.0946
Layers 6 12
Attention heads 12
Patch size 16 4
Table G.2: (Vision Transformer hyperparameters) Hyperparameters for SP are taken from Müller
et al. (2024) using AdamW as a base optimizer. ViTs in µP have base width 384, last layer and query
weights are initialized to 0and gradient norm contributions of all layers are scaled to Θ(1) .
52H Supplemental experiments
This section provides more extensive empirical evaluations to validate the claims of the main paper.
By naive perturbation scaling (naive) we denote parameterizations that do not adapt any perturbation
scalings ( d=dl= 0 for all l). Global perturbation scaling (global) denotes the maximal stable
scaling n−dof the global perturbation radius that achieves effective perturbations in some layers
without layerwise perturbation scaling ( dl= 0for all l).
H.1 SAM is approximately LL-SAM in µP with global perturbation scaling
Figure H.1 compares SAM in µP under global perturbation scaling ( µP-global) with SAM under
global perturbation scaling where only the last-layer weights are perturbed (LL-SAM) by showing
more neural network statistics that are related to SAM’s inductive bias and to learning in general. From
top-left to bottom right, the statistics are: Frobenius norm of the layerwise weight perturbation (which
is closely related to spectral norm as perturbations are low rank); Frobenius norm of the layerwise
weight perturbation normalized by the weight spectral norm to upper bound the influence of the
perturbations on the output; spectral norm of the weight updates across training scaled by the spectral
condition n1/2,1andn−1/2for input, hidden and output layers respectively; norm of the activation
updates for each layer normalized by the square root of the layer’s output dimension to measure
coordinatewise update scaling; layerwise effective feature ranks measured as in Andriushchenko et al.
(2023a) by the minimal amount of singular values to make up 99% of the variance of the activations
in a given layer; gradient norm, Hessian spectral norm and Hessian trace of loss with respect to
weights; training accuracy, test accuracy after optimally stopping.
Observe that, especially for large widths, global perturbation scaling effectively only perturbs the last
layer, as predicted by Theorem 11. Last-layer SAM is more similar to µP-global SAM than SGD on
all of the tracked statistics, in particular at large widths. Only perturbing the last layer still affects the
gradients in earlier layers so that weight updates and activations change in all layers. We find that
SAM in µP with global scaling does not consistently improve generalization performance over SGD,
whereas µP2does improve over SGD for all widths (Figure H.3). Last-layer perturbation norms
coincide by design with the global perturbation radius n−dρand their effect on the activations stays
Θ(1) with increasing width as measured in relation to weight spectral norm. Formally the last-layer
perturbation norm converges due to
∥˜WL+1−WL+1∥F=n−dρ∥χtxL
t
∥vt∥∥F→n−dρ∥xL
t
∥xL
t∥∥F=n−dρ→0,
where the loss derivative χtalways cancels out due to the normalization and the global gradient
norm∥vt∥is dominated by the last-layer gradient norm due to the global scaling (Theorem 11).
Normalizing the weight perturbations by the weight spectral norm measures the influence of the
perturbations on the activations. Note that this influence is also vanishing. Feature ranks stay close to
initialization, since random initialization has high rank and training does low effective rank updates.
Here we do not observe that SAM reduces the feature rank compared to SGD. The Hessian spectral
norm and trace are quite noisy. The last-layer Hessian spectral norm explodes with width in µP,
because last-layer learning rate is scaled as n−1, hence the edge of stability explodes. ResNets in µP
are more stable, their Hessian spectral norm even shrinks with width (not shown).
Contrast the results for µP-global with the results for µP2in Figure H.2 for a comparison with SGD
inµP. The Hessian spectral norm is reduced by SAM as you would expect. Additionally µP2shows
low variability in performance and all other statistics. SAM in µP2does not reduce the feature rank
compared to SGD in µP. This suggests that the conclusions drawn by Andriushchenko et al. (2023a)
do not apply to MLPs in µP.
ResNet-18 on CIFAR10 ViT on CIFAR100 ViT on ImageNet1K
Width multiplier 0.5 1 2 4 0.5 1 2 0.5 1 2
Seconds per epoch 109 161 327 803 209 327 777 2550 4151 9802
Table G.3: (Training time per epoch) Training time (in seconds) per epoch of the entire data loading
and training pipeline of SAM in µP2on a single NVIDIA A10G GPU.
53H.2 Propagating perturbations from the first layer does not inherit SAM’s benefits
Here we apply a parametrization that only effectively perturbs the first layer weights (derived in
Example F.1). Figure H.2 shows that effective first-layer SAM loses both µP2SAM’s improvement
in test accuracy as well as SAM’s inductive bias towards smaller gradient norm and Hessian norm,
i.e. lower sharpness in MLPs. This performance deterioration occurs although the perturbation of
first-layer SAM has an effect of the same order of magnitude as µP2on weight and activation updates
in all layers. This shows that mere propagation of weight perturbations from earlier layers cannot
replace effective weight perturbations in each layer in order to benefit from SAM. It is crucial to
correctly adjust the layerwise perturbation scaling, and to distinguish between effective perturbations
and perturbation nontriviality in each layer.
SAM in µP2, on the other hand, achieves the correct perturbation and update scaling, has lower final
gradient and Hessian spectral norm, improves test accuracy over SGD and has overall lower variance
between training runs.
5464 256 1024 4096 163840.000000.000250.000500.000750.001000.001250.001500.00175/bardbl˜W−W/bardblFInput layer
64 256 1024 4096 163840.0000.0020.0040.0060.008Hidden layer
64 256 1024 4096 163840.0000.0050.0100.0150.0200.025Output layer
SAM-SGD
SAM-SGD-LL
64 256 1024 4096 16384
Width0.00.20.40.60.81.01.21.4/bardbl˜W−W/bardblF//bardblW/bardbl∗×10−5
64 256 1024 4096 16384
Width0.00000.00020.00040.00060.00080.00100.00120.0014
64 256 1024 4096 16384
Width0.0000.0050.0100.0150.0200.025
SAM-SGD
SAM-SGD-LL
64 256 1024 4096 16384
Width9.259.509.7510.0010.2510.5010.75||∆Wl||∗/clInput layer
64 256 1024 4096 16384
Width678910Hidden layer
64 256 1024 4096 16384
Width68101214Output layer
SAM-SGD
SAM-SGD-LL
SGD
64 256 1024 4096 16384
Width0.120.140.160.180.20Coord.wise/bardblxt−x0/bardblInput layer
64 256 1024 4096 16384
Width0.150.200.250.30Hidden layer
64 256 1024 4096 16384
Width345678Output layer
SAM-SGD
SAM-SGD-LL
SGD
64 256 1024 4096 16384
Width100200300400500Feature rankInput layer
64 256 1024 4096 16384
Width100200300400500Hidden layer
64 256 1024 4096 16384
Width8.08.28.48.68.89.0Output layer
SAM-SGD
SAM-SGD-LL
SGD
64 256 1024 4096 16384
Width0246810121416Gradient normSAM-SGD
SAM-SGD-LL
SGD
64 256 1024 4096 16384
Width0500100015002000250030003500Hessian spectral normSAM-SGD
SAM-SGD-LL
SGD
64 256 1024 4096 16384
Width010000200003000040000Hessian traceSAM-SGD
SAM-SGD-LL
SGD
64 256 1024 4096 16384
Width6065707580859095100Training accuracySAM-SGD
SAM-SGD-LL
SGD
64 256 1024 4096 16384
Width4648505254565860Test accuracySAM-SGD
SAM-SGD-LL
SGDFigure H.1: Several neural network statistics for SAM (blue), LL-SAM (green) and SGD as a
baseline (orange) across width after training a 3-layer MLP in µP-global for 20 epochs with the
optimal learning rate 0.3432 and perturbation radius 0.2154 . The statistics are explained in the text
of Appendix H.1.
5564 256 1024 409602468101214/bardbl˜W−W/bardblFInput layer
64 256 1024 40960.000.050.100.150.200.25Hidden layer
64 256 1024 40960.00000.00250.00500.00750.01000.01250.01500.0175Output layer
perturbation
mpp
ﬁrst layer
64 256 1024 4096
Width0.00000.00250.00500.00750.01000.01250.01500.0175/bardbl˜W−W/bardblF//bardblW/bardbl∗
64 256 1024 4096
Width0.0000.0050.0100.0150.020
64 256 1024 4096
Width0.0000.0050.0100.0150.0200.025perturbation
mpp
ﬁrst layer
64 256 1024 4096
Width11.512.012.513.0||∆Wl||∗/clInput layer
64 256 1024 4096
Width681012Hidden layer
64 256 1024 4096
Width4681012Output layer
mpp
ﬁrst layer
SGD
64 256 1024 4096
Width0.140.160.180.200.220.240.26Coord.wise/bardblxt−x0/bardblInput layer
64 256 1024 4096
Width0.150.200.250.300.35Hidden layer
64 256 1024 4096
Width23456789Output layer
mpp
ﬁrst layer
SGD
64 256 1024 4096
Width100200300400500Feature rankInput layer
64 256 1024 4096
Width100200300400500Hidden layer
64 256 1024 4096
Width8.08.28.48.68.89.0Output layer
mpp
ﬁrst layer
SGD
64 256 1024 4096
Width1020304050Gradient normmpp
ﬁrst layer
SGD
64 256 1024 4096
Width020040060080010001200Hessian spectral normmpp
ﬁrst layer
SGD
64 256 1024 4096
Width02505007501000125015001750Hessian eigenvalue gapmpp
ﬁrst layer
SGD
64 256 1024 4096
Width60708090100Training accuracympp
ﬁrst layer
SGD
64 256 1024 4096
Width4446485052545658Validation accuracympp
ﬁrst layer
SGDFigure H.2: Same neural network statistics as in Figure H.1 but SAM-SGD in µP2(blue) versus MUP
with perturbations scaled to only effectively perturb the first layer weights (green) with SGD in µP
as a baseline. The first-layer perturbation parameterization performs worse than µP2and results in
gradient norm and Hessian norm similar to that of SGD, larger than those of SAM. While the spectral
norm of the weights converges to a similar quantity as for µP2, the effect of the weight changes on
the hidden activation updates behaves more like SGD. Feature ranks all look similar.
56H.3 Hyperparameter transfer
In this section, we provide supplemental evidence that, µP2is the unique perturbation scaling that
robustly achieves hyperparameter transfer in µP both for the optimal learning rate and the optimal
perturbation radius across neural architectures and datasets. But we also show that both MLPs and
ResNets in SP can sometimes achieve hyperparameter transfer after long training.
H.3.1 MLPs in µP
Figure H.3 shows that in µP2the optimal hyperparameters in terms of test accuracy transfer in both
learning rate and perturbation radius at sufficient width, and test accuracy monotonically improves
with model scale. In addition, SAM in µP2outperforms SAM in µP with global perturbation scaling
at all widths.
While other works focus on hyperparameter transfer in training loss, we are ultimately interested
in transfer with respect to test accuracy. Especially under harmful overfitting, the test accuracy
is affected by nontrivial interactions between the learning rate and the perturbation radius. While
the joint optimum is slightly shifting towards larger learning rate and perturbation radius for small
widths, it remains remarkably stable for sufficient width ≥1024 . Note that slight shifts in the optimal
learning rate due to finite width biases have also been observed in earlier works (Yang et al., 2022).
Figure H.3: Training accuracy (top) and test accuracy (bottom) after optimally stopping 20 epoch
SAM training as a function of learning rate (left) with perturbation radius ρ= 0.2154 , and as a
function of perturbation radius (right) with learning rate η= 0.4529 inµP2. The optimal learning
rate transfers. The smaller the perturbation radius the better the training accuracy. For sufficiently
wide MLPs, the validation-optimal perturbation radius transfers as well and SAM reduces harmful
overfitting.
57Figure H.4: Training accuracy (top) and test accuracy (bottom) after optimally stopping 20 epoch
SAM training as a function of learning rate (left) and perturbation radius (right) in µP-global with the
same base learning rate and perturbation radius as in Figure H.9. For global perturbation scaling, we
do not observe a benefit of SAM over SGD.
Figure H.5: Same as Figure H.4 but with input multiplier 0.0305 and small output multiplier 0.0098 .
Note that networks with width at most 256perform better in terms of test accuracy than with the
other multiplier choice in Figure H.4, but the multipliers here have worse width scaling properties. To
the best of our knowledge, the issue that optimally tuned hyperparameters on small models may scale
worse than slightly suboptimal hyperparameters has not been stated before. This raises the question
when and how can we use small models to predict the optimal hyperparameters of large models.
58Figure H.4 shows that global perturbation scaling does transfer the same perturbation instability
threshold, whereas in µP-naive every fixed perturbation radius becomes unstable at sufficient width
(Figure H.7). But in µP-global we do not observe a benefit of SAM over SGD. While the optimal
learning rate with respect to the training accuracy transfers, the optimal learning rate with respect
to the validation error is smaller for MLPs of moderate widths due to harmful overfitting. How to
control for non-monotonic dependence of the test error on the training error is an important question
for future work. Figure H.5 also shows µP-global but with a different choice of input and output
multipliers. With these multipliers, networks with width at most 256perform better in terms of test
accuracy than with the other multiplier choice in Figure H.4, but these multipliers have worse width
scaling properties. To the best of our knowledge, the issue that optimally tuned hyperparameters on
small models may scale worse than slightly suboptimal hyperparameters has not been stated before.
This raises the question when and how can we use small models to predict the optimal choice of all
hyperparameters jointly in large models.
10−310−210−1
Perturbation radius ρ10−1100Learning rate ηµP-naive
Figure H.6: Same as Figure 1 but for µP with naive width-independent perturbation scaling ρ. The
regime of stable perturbation radii shrinks with increasing width as predicted by Proposition 1.
0.0
0.001
0.0018
0.0032
0.0056
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
Perturbation radius0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0Learning rate434343434343434342413810
464646464646464646454210
505050505050505049494710
545454545454545453535110
585858585858585757565510
606060606160616059595710
616262616162626060585610
596060605960595858552410
545453565652545451492110width=64
0.0
0.001
0.0018
0.0032
0.0056
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
Perturbation radius0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0Learning rate494949494949494847441010
545454555554545352501010
616161616161616059571010
686868686969696867651010
747575767677767674701010
808181828281818077731010
808382818182828076681010
797980818179767873471010
726972727171696863241010width=256
0.0
0.001
0.0018
0.0032
0.0056
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
Perturbation radius0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0Learning rate535353535252514944111111
606060606059585753111111
696969696969686664111111
777878787978777573111010
838685858585858379101010
908891919291888681101010
959296949189929080101010
878687868983868510101010
848582818378737510101010width=1024
0.0
0.001
0.0018
0.0032
0.0056
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
Perturbation radius0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0Learning rate545454535351501511121212
626262616159581212121211
727373727271701212111111
818282828281791211111110
908888878686841111101010
969595929291891110101010
999894939497951010101010
999492949294921010101010
918585858787801010101010width=4096
5254565860
Training accuracy
7476788082
Training accuracy
8688909294
Training accuracy
9092949698
Training accuracy
0.0
0.001
0.0018
0.0032
0.0056
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
Perturbation radius0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0Learning rate424242424242424241403715
454545454545444444434113
474747474747474746454411
494949494949484848484610
494950505049494949494710
505050504949494949494810
505049504949494949484710
494849484849484848472210
474747474647474746442010width=64
0.0
0.001
0.0018
0.0032
0.0056
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
Perturbation radius0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0Learning rate474747474747474646431913
495050505050494948471711
515151515151515151501311
525252525252525252511110
525253535353535352521010
535353535353535353521010
525252525252525252511010
515252515151515150351010
505050494949494947211010width=256
0.0
0.001
0.0018
0.0032
0.0056
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
Perturbation radius0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0Learning rate494950494949484744231411
525252525252525149201211
535353545454545353151111
545454545555545453121111
545454545555555554111111
545455555555555554111111
555555555555555553111111
545454545454545311111111
525251525152514911111111width=1024
0.0
0.001
0.0018
0.0032
0.0056
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
Perturbation radius0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0Learning rate505050505049483428171211
535353535353523424131110
545454555555553417121010
545555555555553213111010
545555555555552311101010
565656565656561410101010
585757575757561110101010
575756565756551010101010
545454545453511010101010width=4096
4546474849
Validation accuracy
4950515253
Validation accuracy
5152535455
Validation accuracy
5354555657
Validation accuracy
Figure H.7: Mean (over 3runs) of training accuracy (top) and of test accuracy (bottom) after optimally
stopping 20 epoch SAM training of a MLP in µP-naive as a function of learning rate and perturbation
radius. The optimal hyperparameters do not transfer. Every fixed perturbation radius becomes
unstable in sufficiently wide networks.
590.0
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0
1.7783
3.1623
Perturbation radius0.005
0.0097
0.0188
0.0364
0.0706
0.1369
0.2657
0.5154
1.0Learning rate393939393939393837351110
434343434343424241391010
464647474746464646441010
515151515151515150491010
555555555556555555541010
595959595959595959571010
616261616160616060581010
605960606059595858541010
545454555554545351481010width=64
0.0
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0
1.7783
3.1623
Perturbation radius0.005
0.0097
0.0188
0.0364
0.0706
0.1369
0.2657
0.5154
1.0Learning rate444444444444444443413310
494949494949494848464110
555555555555555454535010
626262626263636262615910
717171717272727271706710
797979797980807978766910
808281848082838281774910
838179817879787977721010
727171727271707067611010width=256
0.0
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0
1.7783
3.1623
Perturbation radius0.005
0.0097
0.0188
0.0364
0.0706
0.1369
0.2657
0.5154
1.0Learning rate474747474746464645433611
525252525252525150494411
616161616161606059575411
717272727272727171696711
818182828283828282807711
888889898988908888858110
969394959192929191918310
899291938990889086873110
838584847979808381751010width=1024
0.0
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0
1.7783
3.1623
Perturbation radius0.005
0.0097
0.0188
0.0364
0.0706
0.1369
0.2657
0.5154
1.0Learning rate474747474747474645433713
545454545353535251504511
636363636363626261595612
757575757575757574737012
858585858585858584828011
929192939291928889878510
979798979796949590918910
989898949596929596901010
818085928682888180781010width=4096
5254565860
Training accuracy
7476788082
Training accuracy
8688909294
Training accuracy
9092949698
Training accuracy
0.0
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0
1.7783
3.1623
Perturbation radius0.005
0.0097
0.0188
0.0364
0.0706
0.1369
0.2657
0.5154
1.0Learning rate393939393938383837351814
424242424242424140391813
454545454545454444431811
474747474747474746461610
494949494949494848471210
495049494949494949481010
505050504950494949481010
494949494949494847471010
474747474747474645431010width=64
0.0
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0
1.7783
3.1623
Perturbation radius0.005
0.0097
0.0188
0.0364
0.0706
0.1369
0.2657
0.5154
1.0Learning rate444444444444444342413318
474747474747474646454118
505050505050505049494715
515151515252525252515012
525252525252535252525110
535353535353535353525110
525353525353525352523710
525252515151515152511010
505050505050495048471010width=256
0.0
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0
1.7783
3.1623
Perturbation radius0.005
0.0097
0.0188
0.0364
0.0706
0.1369
0.2657
0.5154
1.0Learning rate464646464646464544433524
494949494949494948474424
525252525252525252515020
535353535353545454545315
545454545454545455545412
545454545455555555555411
555555555555555555555411
555454545454545554542411
515252525151525151501111width=1024
0.0
0.01
0.0178
0.0316
0.0562
0.1
0.1778
0.3162
0.5623
1.0
1.7783
3.1623
Perturbation radius0.005
0.0097
0.0188
0.0364
0.0706
0.1369
0.2657
0.5154
1.0Learning rate474747474747464645433629
505050505050505049484529
535353535353535353525126
545454545454545455555418
545454545555555555555513
555555555555555656555511
575757565757565757575611
585857575756565656561111
545454545454535454521111width=4096
4647484950
Validation accuracy
 4950515253
Validation accuracy
5152535455
Validation accuracy
 5455565758
Validation accuracy
Figure H.8: Mean (over 3runs) of training accuracy (top) and of test accuracy (bottom) after optimally
stopping 20 epoch SAM training of a MLP in µP-global as a function of learning rate and perturbation
radius. The global scaling of the perturbation radius by n−1/2compared to µP-naive (Figure H.7)
makes the stable regime invariant to width. But the suboptimal layerwise perturbation scaling that
only perturbs the last layer does not consistently improve over SGD ( ρ= 0).
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate49 49 49 49 49 47 42 13
52 53 52 52 52 51 45 14
56 56 56 56 56 54 48 13
59 59 59 59 59 57 51 15
61 61 61 61 61 59 53 21
61 61 61 59 60 59 55 25
58 58 58 57 58 59 55 27
55 54 55 54 52 53 51 29
45 44 45 46 44 45 41 19width=64
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate59 59 59 59 58 55 47 18
66 66 66 65 64 60 51 17
73 73 73 72 71 67 55 17
77 78 78 77 77 74 60 23
81 83 80 79 80 79 65 29
82 79 82 82 82 82 70 33
79 76 80 79 78 80 73 39
69 70 70 72 72 72 70 32
55 52 55 56 56 56 55 23width=256
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate67 67 67 66 64 59 49 22
76 75 75 74 73 66 53 20
83 83 83 83 81 75 58 18
88 88 88 88 88 85 64 26
91 92 95 90 94 91 73 33
94 94 93 90 91 92 83 38
87 89 90 90 89 93 90 44
76 84 81 78 81 84 89 38
64 65 66 65 66 71 70 27width=1024
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate71 70 70 69 67 60 49 22
79 79 79 78 76 69 54 19
87 87 86 86 84 78 59 18
93 93 92 91 88 88 66 29
95 94 96 94 91 93 76 34
98 99 96 97 95 97 88 40
91 96 93 93 95 98 96 45
87 82 84 80 89 91 97 42
66 60 72 70 77 79 84 30width=4096
5254565860
Training accuracy
7476788082
Training accuracy
8688909294
Training accuracy
9092949698
Training accuracy
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate46 46 46 46 46 46 42 15
48 48 48 48 48 48 44 15
49 49 49 49 49 49 46 15
49 49 50 49 50 50 48 15
50 50 50 50 50 50 49 22
49 49 49 50 50 50 49 25
49 48 48 48 49 49 49 29
46 46 47 46 46 47 46 30
41 43 42 43 42 41 39 20width=64
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate51 51 51 51 51 51 47 21
52 52 52 52 53 53 49 20
52 52 53 53 53 54 51 19
52 52 52 53 53 54 53 23
52 52 53 53 53 54 54 29
52 52 52 53 53 54 54 33
51 51 51 51 52 53 54 38
50 49 49 50 50 51 51 33
45 44 45 44 44 46 46 24width=256
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate53 53 53 53 54 53 48 28
53 53 53 53 54 55 51 27
54 54 54 54 55 56 53 25
54 54 54 54 55 56 55 26
54 54 55 55 55 57 57 33
55 55 55 55 55 56 57 38
54 53 54 54 54 55 57 43
51 52 52 51 52 53 55 37
46 46 46 47 47 48 49 27width=1024
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate54 54 54 54 54 54 48 31
54 54 54 54 55 56 51 32
54 54 54 55 55 57 53 31
55 55 55 55 55 57 56 29
56 56 56 56 56 58 58 34
58 58 58 57 57 58 59 40
56 56 57 57 56 58 59 44
53 54 54 54 53 56 58 40
47 48 47 47 48 50 53 31width=4096
4647484950
Test accuracy
5051525354
Test accuracy
5354555657
Test accuracy
5455565758
Test accuracy
Figure H.9: Mean (over 3runs) of training accuracy (top) and of test accuracy (bottom) after optimally
stopping 20 epoch SAM training of a MLP in µP2as a function of learning rate and perturbation
radius. At sufficient width, the optimal hyperparameters are stable in terms of test accuracy, even
under severe overfitting.
H.3.2 Some variants of SP can transfer optimal hyperparameters on CIFAR-10
Surprisingly, after training MLPs to convergence on CIFAR-10, some variants of SP with naive
perturbation scaling can transfer learning rate and perturbation radius, against the prediction by
infinite-width theory. For SGD, this has originally been observed in GitHub issue 52 of the mup-
package4. We train MLPs with SAM in variants of SP in Figure H.10 and also observe that some
variants achieve transfer while for others the optimal learning rate shrinks as in Yang et al. (2022).
To achieve shrinking stable and optimal learning rates in SP, Yang et al. (2022) use weight multipliers
tuned at base width 256and normalize the initialization variance to be invariant to these weight
multipliers, according to the Jupyter notebook5provided for reproducing their experiments. In
addition, they initialize the last layer to 0which contradicts SP scaling but results in more striking
shrinkage of the optimal learning rate. We observe that both with and without weight multipliers,
MLPs trained with SAM in SP-naive have surprisingly good transfer properties on CIFAR-10. With
tuned multipliers but initialization that is invariant to these multipliers, the optimal learning rate
shrinks. Because we are training to convergence, pure infinite-width theory does not adequately
describe the training dynamics anymore (Vyas et al., 2024). Infinite-width theory implies that scaling
the width further would eventually break the learning rate transfer. It remains a matter of ongoing
work to understand whether this stability of SP is a finite-width or a long training time effect, and
whether this empirical stability is particular to multi-epoch training on vision datasets. As shrinkage
of the optimal learning rate in SP has generally been observed in language settings (see e.g. Brown
et al., 2020, Table 2.1), we expect the same shrinkage for SAM in SP in such settings.
4Without tuned weight multipliers, MLPs trained with SGD in SP on CIFAR-10 can transfer the optimal
learning rate: https://github.com/microsoft/mup/issues/52
5https://github.com/microsoft/mup/blob/main/examples/MLP/demo.ipynb
60Note that the learning rate transfer in SP here is a much stronger observation than in Everett et al.
(2024) who choose the correct layerwise learning rates for SP. Hence their SP merely deviates from
µP through a larger output layer initialization and key-query normalization by√
din SP versus by
dinµP. Here however we even observe transfer in our stricter understanding of SP without any
layerwise learning rates or weight multipliers. This is not a peculiarity of SAM; we observe the same
learning rate transfer in plain SGD without any momentum or weight decay for MLPs on CIFAR-10
(not shown).
10−210−1100
Perturbation radius ρ10−210−1Learning rate ηSP-naive, no mults
width
4096
1024
256
10−210−1100
Perturbation radius ρ10−1100SP-naive (Y ang mults)
10−310−210−1
Perturbation radius ρ10−410−310−210−1SP-naive, initnorm
10−310−210−1
Perturbation radius ρ10−410−310−210−1SP-naive, initnorm, ll=0
Figure H.10: Optimal learning rate and perturbation radius (cross) and regions within 1%of the
optimal test accuracy (mean over 4runs) after optimally stopping 20 epoch SAM training of a MLP
in different variants of SP for varying widths (the darker, the wider). Observe transfer properties
in SP almost as stable as in µP2(Figure 1) and against infinite-width predictions slightly growing
with width, both with and without the tuned weight multipliers by Yang et al. (2022). Only when
normalizing the initialization variance to be independent of the width-independent weight multipliers
(initnorm), does the regime of stable learning rates shrink at the widths considered. Additionally
initializing the last layer to zero (ll = 0) (as in the Jupyter notebook provided to reproduce Figure 3 in
Yang et al. (2022)) shows even more pronounced learning rate shrinkage, but does not correspond to
SP scaling anymore.
ResNets in SP show hyperparameter transfer across most SAM variants too, as soon as we tune
momentum, weight decay and labelsmoothing (Figure H.11). This is in line with previous empirical
observations (Yang et al., 2022, Figure 16 for SGD) but contradicts infinite-width theory as for MLPs.
(a) No momentum, weight decay or labelsmoothing
 (b) Tuned momentum and weight decay
Figure H.11: Training accuracy (top) and test accuracy (bottom) after optimally stopping 100
epoch SAM training as a function of learning rate and perturbation radius in SP-naive without
regularization (left) and with tuned regularization (right) using momentum 0.9, weightdecay 0.0005
and labelsmoothing 0.1. CI denote the minimal and maximal value from 4 independent runs. Without
regularization, the optimal learning rate shrinks with width. Given the learning rate, the optimal
perturbation radius seems quite stable, but since the optimal learning rate shifts, the performance
scales worse than for µP2with the fixed learning rate that is tuned on the small model. With optimal
regularization, both optimal learning rate and perturbation radius remain remarkably stable. We plan
to investigate this mechanism in an upcoming work.
H.3.3 ResNets
In this section, we plot averages and σ-CI from 2 independent runs.
61ResNets in µP2transfer both the optimal learning rate and perturbation radius for SAM (Figure H.12),
SAM-ON (Figure H.14) and elementwise ASAM (Figure H.15), as well as different alternatives
of scaling the gradient norm contributions to SAM’s denominator (Figure H.18). This suggests
correctness of the derived scalings. At width multipliers 2and4,µP2achieves the same or slightly
better test accuracy than SP in all SAM variants.
Figure H.13 shows ResNets trained with SAM in different parameterizations. In ResNets of practical
scale, ρremains quite stable in µP2but surprisingly also in SP-NAIVE. In µP, for naive perturbation
scaling the regime of stable perturbation radii shrinks, for global perturbation scaling, the optimal
perturbation radius shifts, approaching its maximal stable value, which stays invariant to width
scaling. Here, it would be interesting to see whether even larger width would lead to suboptimal
performance of µP-global. µP2is most robust to the choice of ρand achieves the best test accuracy.
(a) SP, no ρscaling
10−210−1100
Learning rate9596979899100Training accuracy
10−210−1100
Perturbation radius9596979899100Training accuracy
10−210−1100
Learning rate9192939495969798Test accuracy
10−210−1100
Perturbation radius9192939495969798Test accuracywidth
0.5
1.0
2.0
4.0 (b)µP2
Figure H.12: Training accuracy (top) and test accuracy (bottom) after optimally stopping 200 epoch
SAM training as a function of learning rate and of perturbation radius in SP (left) and in µP2(right)
with optimized momentum 0.9, weight decay 5·10−4and labelsmoothing 0.1for both µP2and SP.
InµP2, the base learning rate is η= 2−4and the base perturbation radius is ρ= 2−4, in SP η= 0.05
andρ= 0.1, respectively. Observe monotonic improvement with width in both training and test error.
Optimal hyperparameters transfer across widths, surprisingly in both µP2and SP.
10−310−210−1
Perturbation radius ρ9394959697Test accuracySP-naive
10−310−210−1100
Perturbation radius ρ9394959697µP-naive
width multiplier
4.0
2.0
1.0
0.5
10−210−1100
Perturbation radius ρ9394959697µP-global
10−210−1100
Perturbation radius ρ9394959697µP2
Figure H.13: Test accuracy after optimally stopping 200 epoch SAM training as a function of
perturbation radius in various parameterizations. Dashed lines denote the base optimizer SGD with
tuned momentum and weight decay in the respective parameterization.
H.3.4 ASAM variants
As we are not aware of any use of ASAM with MLPs in the literature and since the amount of
necessary experiments for ViTs exceeds our computational budget, we only show that ResNets
trained with the all of the discussed SAM variants in µP2transfer the optimal (η, ρ).
For the examples of elementwise ASAM and SAM-ON the global perturbation scaling n1/2suffices
to reach µP2. The stability of the optimal perturbation radius in the applied scaling n1/2shows that
inµP with naive perturbation scaling the optimal perturbation radius would grow as n1/2.
See the previous section, for a discussion of the remarkable stability of ResNets in SP. For the
example of elementwise ASAM in SP, the optimal perturbation radius seems to grow.
For layerwise ASAM (Figure H.16), the optimal perturbation radius seems to grow in both SP
andµP2, suggesting that our scaling condition does not perfectly apply to this variant, although
µP2(97.09±0.03(+0.83)) still outperforms SP ( 96.86±0.05(+0.83)) in terms of the optimal test
62accuracy. As Frobenius norms of weights are the only component that is not representable as a
NE⊗OR⊤program, these Frobenius norms appear to scale differently than heuristically predicted
over the course of training.
(a) SP, no ρscaling
10−210−1100
Learning rate9596979899100Training accuracy
10−1100101
Perturbation radius9596979899100Training accuracy
10−210−1100
Learning rate9192939495969798Test accuracy
10−1100101
Perturbation radius9192939495969798Test accuracywidth
0.5
1.0
2.0
4.0 (b)µP2
Figure H.14: Same as Figure H.12 but for SAM-ON in SP without perturbation scaling (left) and in
µP2(right). Both optimal learning rate and perturbation radius are remarkably stable in both µP2and
SP. Since µP2for SAM-ON is just µP with global perturbation scaling n1/2, transfer here implies
thatµP with width-independent scaling would not transfer.
(a) SP, no ρscaling
10−210−1100
Learning rate9596979899100Training accuracy
10−1100101
Perturbation radius9596979899100Training accuracy
10−210−1100
Learning rate9192939495969798Test accuracy
10−1100101
Perturbation radius9192939495969798Test accuracywidth
0.5
1.0
2.0
4.0 (b)µP2
Figure H.15: Same as Figure H.12 but for elementwise ASAM in SP without perturbation scaling
(left) and in µP2(right). Observe a consistent HP landscape in µP2but growing optimal perturbation
radius in SP without perturbation scaling.
(a) SP, no ρscaling
10−210−1100
Learning rate9596979899100Training accuracy
10−310−210−1
Perturbation radius9596979899100Training accuracy
10−210−1100
Learning rate9192939495969798Test accuracy
10−310−210−1
Perturbation radius9192939495969798Test accuracywidth
0.5
1.0
2.0
4.0 (b)µP2
Figure H.16: Same as Figure H.12 but for layerwise ASAM in SP without perturbation scaling (left)
and in µP2(right). For layerwise ASAM, both µP2and SP seem to transfer the optimal learning rate
as well as perturbation radius.
63H.4 Gradient norm contributions have negligible effects on generalization performance
In this section we provide ablations concerning the question which layers should contribute non-
vanishingly to the gradient norm in the denominator of the layerwise SAM perturbation rule (LP).
For MLPs, in Figure H.17 we scale all contributions to Θ(1) , and then set the contribution of
individual layers to zero, one by one. We observe no significant effect on the optimal test loss or
hyperparameter transfer for MLPs. Any layer’s contribution to the gradient normalization in the
denominator of the SAM update rule can be set to 0without a significant effect on the test loss. This
raises the question which effect the gradient normalization has in µP. Does it contribute a scaling
correction in SP, but may be dropped entirely in µP?
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate47 47 47 47 47 46 44 18
48 48 48 48 48 48 46 17
49 49 49 49 49 50 48 16
50 50 50 50 50 50 49 16
50 50 50 50 50 50 50 23
50 49 50 50 49 50 50 28
48 48 48 49 49 48 49 39
46 46 46 46 47 46 46 37
42 42 42 43 41 41 40 22width=64
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate51 51 51 51 51 51 48 26
52 52 52 52 52 53 50 25
52 53 53 53 53 54 52 24
53 53 53 53 54 54 54 23
53 53 53 53 53 54 55 34
52 52 52 53 52 53 55 42
51 51 51 51 52 52 54 45
50 49 49 50 50 51 51 43
44 44 45 45 45 44 46 35width=256
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate53 53 53 53 54 53 50 30
53 54 54 54 54 55 52 30
54 54 54 54 55 56 54 29
54 54 54 54 55 56 56 29
54 55 54 54 55 57 57 39
55 55 55 55 55 56 57 46
54 54 54 54 54 55 57 49
52 52 52 52 52 52 55 48
46 46 46 47 46 48 49 40width=1024
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate54 54 54 54 54 54 50 33
54 54 55 55 55 56 53 33
54 54 54 55 55 57 55 33
55 55 55 55 55 57 57 32
57 56 56 56 56 58 58 37
58 57 58 57 57 57 59 46
56 57 56 57 56 57 59 50
53 54 54 54 54 55 57 50
47 47 48 47 48 50 52 45width=4096
4647484950
Test accuracy
5051525354
Test accuracy
5354555657
Test accuracy
5455565758
Test accuracy
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate46 46 46 46 46 45 43 20
48 48 48 48 49 48 45 18
49 49 49 49 50 49 47 15
50 50 50 50 50 50 48 17
50 49 50 49 50 50 49 21
49 49 49 49 49 49 49 31
48 49 48 48 48 48 48 39
47 47 46 46 46 47 47 31
42 42 41 43 43 41 40 18width=64
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate51 51 51 51 52 51 47 23
52 52 52 52 53 53 49 21
52 53 53 53 53 54 51 19
53 53 53 53 54 55 53 18
52 53 52 53 53 55 55 28
52 51 52 53 53 53 54 41
51 51 51 51 52 52 54 47
50 50 49 50 50 50 51 43
45 46 44 45 45 45 46 25width=256
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate53 53 53 53 53 53 48 30
53 54 54 54 55 55 51 30
54 54 54 55 56 56 53 28
54 55 55 55 56 57 55 27
54 54 55 55 56 57 57 31
56 55 55 55 55 56 58 43
54 53 53 54 53 55 57 49
52 51 52 51 52 52 55 48
46 45 46 46 48 48 49 28width=1024
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate53 54 54 54 54 53 49 32
54 54 54 55 55 56 51 32
54 54 54 55 56 57 54 32
55 55 55 55 56 58 56 30
56 56 56 56 57 58 58 32
58 58 58 57 57 58 59 45
56 56 57 56 56 58 59 49
54 54 54 54 54 55 57 50
47 47 45 47 48 50 52 37width=4096
4647484950
Test accuracy
5051525354
Test accuracy
5354555657
Test accuracy
5455565758
Test accuracy
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate46 46 46 46 46 46 43 20
48 48 48 48 48 48 45 18
49 49 49 49 50 49 47 16
50 50 50 50 50 50 49 18
50 50 50 50 50 50 49 24
49 49 49 49 49 50 50 33
48 49 48 47 48 49 49 39
47 46 47 46 46 47 47 41
42 42 42 41 42 42 41 29width=64
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate51 51 51 51 52 51 48 23
52 52 52 52 52 53 50 22
52 53 53 53 53 54 52 19
53 53 53 53 53 54 54 25
52 52 53 52 53 54 55 33
52 52 52 52 52 54 55 43
51 52 51 51 52 52 54 47
50 50 50 50 49 50 52 46
45 44 46 44 44 45 46 31width=256
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate53 53 53 53 53 53 49 31
53 54 54 54 54 55 52 31
54 54 54 55 55 56 54 29
54 55 55 55 55 57 56 27
54 55 54 55 56 57 58 33
56 55 55 54 55 56 57 46
54 54 53 55 53 55 57 49
52 51 51 51 51 53 55 45
46 46 46 46 47 48 49 37width=1024
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate53 54 54 54 54 54 49 32
54 54 55 54 55 56 52 33
54 54 54 55 56 56 55 32
55 55 55 55 56 57 57 30
56 56 56 56 56 58 59 39
58 58 57 57 57 58 59 48
56 56 57 57 57 58 59 49
54 53 54 55 54 56 58 51
47 46 47 47 48 51 53 42width=4096
4647484950
Test accuracy
5051525354
Test accuracy
5354555657
Test accuracy
5455565758
Test accuracy
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate46 46 46 46 46 46 44 21
48 48 48 48 48 48 45 19
49 49 49 49 49 50 48 17
50 50 50 50 50 50 49 16
50 50 49 50 50 50 50 16
49 49 49 48 48 49 50 23
48 48 48 48 48 48 48 42
47 47 47 46 47 47 47 42
42 41 42 41 41 41 42 33width=64
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate51 51 51 51 52 51 48 24
52 52 52 52 52 53 50 23
52 52 53 53 53 54 52 21
53 52 53 53 54 54 54 22
52 52 52 53 53 54 55 37
52 52 52 53 53 54 55 46
51 51 51 51 52 52 54 49
50 51 49 50 49 50 51 48
45 46 44 46 45 45 46 38width=256
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate53 53 53 53 53 53 50 31
53 54 54 54 54 55 52 31
54 54 54 54 55 56 54 30
54 54 55 55 56 56 56 29
54 55 55 55 55 57 58 34
56 55 56 55 55 56 58 47
54 53 54 54 54 54 57 51
52 51 51 52 52 52 54 52
46 47 47 46 47 47 49 45width=1024
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate53 54 54 54 54 54 50 33
54 54 54 54 55 55 52 33
54 54 55 54 55 56 55 33
55 55 55 55 56 57 57 32
56 56 57 56 57 57 59 34
58 59 58 57 57 58 59 47
56 56 56 57 56 57 59 52
54 54 54 54 53 55 57 53
47 48 47 48 47 49 52 49width=4096
4647484950
Test accuracy
5152535455
Test accuracy
5354555657
Test accuracy
5556575859
Test accuracy
Figure H.17: Scaling the gradient norm contributions of all layers to Θ(1) (first row) and then setting
the first layer gradient norm to 0(2nd row), respectively the hidden layer (3rd row), last-layer (4th
row). Each individual layer seems to have vanishing contribution to the optimal test error.
For ResNets, Figure H.18(a) shows accuracies when rescaling all layers’ gradient norms to Θ(1) ,
and Figure H.18(b) shows the results when using the original global gradient norm rescaled to
Θ(1) . Again, both methods achieve similar optimal test accuracy. The first variant shows cleaner
hyperparameter transfer and monotonous improvement with width. When comparing to our original
definition (LP) in Figure H.12, optimal performance is similar but rescaling all layers’ gradient norm
contributions to Θ(1) may even produce a slightly more stable hyperparameter-loss landscape for
ResNets.
6410−210−1100
Learning rate9596979899100Training accuracy
10−210−1100
Perturbation radius9596979899100Training accuracy
10−210−1100
Learning rate9192939495969798Test accuracy
10−210−1100
Perturbation radius9192939495969798Test accuracywidth
0.5
1.0
2.0
4.0(a) Rescaling all of SAM’s denominator terms to Θ(1)
10−210−1100
Learning rate9596979899100Training accuracy
10−210−1100
Perturbation radius9596979899100Training accuracy
10−210−1100
Learning rate9192939495969798Test accuracy
10−210−1100
Perturbation radius9192939495969798Test accuracywidth
0.5
1.0
2.0
4.0 (b) Global ∥∇L∥scaling
Figure H.18: Same as Figure H.12 but with scaling of the gradient norms in the SAM perturbation
(LP) denominator that scales all terms to Θ(1) (left) and only global denominator scaling∥∇L∥
nL(right). All denominator scalings achieve similar optimal accuracy, show HP transfer in learning rate
and monotonic test accuracy improvement with width. In global denominator scaling, the optimal ρ
shifts with width.
H.5 SAM with layerwise gradient normalization
Here we consider SAM without the gradient normalization over all layers jointly. Instead we apply
the layerwise perturbation rule presented in Appendix F.7,
εl
t=ρl· ∇WlL(f(ξt;Wt), yt)/∥∇WlL(f(ξt;Wt), yt)∥F.
In SP, we consider a global constant ρl=ρ, whereas for µP2the spectral condition (∗)requires
ρl=ρ·p
fan_out /fan_in .
Overall, SAM without layer coupling performs decently, but is outperformed by the original SAM in
particular in ResNets, in µP2and at large width. But note that for ResNets we adopt the hyperparam-
eters tuned for the original SAM with layer coupling, so that these ablations only serve as preliminary
experiments.
MLPs. SAM without layer coupling achieves similar optimal generalization in µP2at each width
compared to Figure H.9. The regime of stable (η, ρ)stays width-independent, but does not transfer
the optimum consistently. This suggests complex or noisy dependence of the training dynamics on ρ.
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate46 46 46 46 46 45 42 12
48 48 48 48 48 47 44 11
49 49 49 49 49 48 45 11
49 50 50 50 49 49 46 11
50 50 50 50 49 49 46 11
49 50 49 49 49 49 11 11
49 49 48 48 48 47 11 11
46 47 47 48 46 46 11 11
42 41 41 41 42 40 11 11width=64
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate51 51 51 51 51 51 48 13
51 52 52 52 52 52 51 12
52 52 52 53 53 53 51 11
53 52 53 53 53 53 52 11
53 53 53 53 53 54 52 11
52 52 51 52 51 53 52 11
51 52 51 51 51 51 50 13
49 49 48 49 51 49 47 12
45 45 44 45 44 44 11 12width=256
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate53 53 53 53 54 54 51 18
53 53 53 54 54 54 53 15
53 54 54 54 54 55 54 13
54 54 54 55 55 55 55 12
55 54 55 54 55 55 55 13
55 55 56 56 56 54 55 11
53 54 54 54 54 54 54 12
51 51 51 51 52 53 50 12
46 47 48 46 47 47 11 11width=1024
0.0 0.001 0.0032 0.01 0.0316 0.1 0.3162 1.0
Perturbation radius0.0283
0.0476
0.0801
0.1346
0.2264
0.3808
0.6404
1.0771
1.8114Learning rate53 54 54 54 54 54 52 23
54 54 54 54 55 55 54 20
54 54 54 54 55 56 55 17
54 55 55 55 55 56 56 15
56 56 56 56 56 56 56 14
58 59 57 58 57 57 57 13
56 56 56 56 56 57 57 13
53 53 53 53 53 55 53 13
47 48 48 47 48 48 14 14width=4096
4647484950
Test accuracy
4950515253
Test accuracy
5152535455
Test accuracy
5455565758
Test accuracy
(a)µP2
Figure H.19: (SAM with layerwise normalization in MLPs) Test accuracy as a function of learning
rateηand perturbation radius ρfor an optimally-stopped MLP trained with SAM with layerwise
normalization.
ResNets. Figure H.20 shows that decoupled SAM has decent performance, but is worse than original
SAM with global normalization (Figure H.13) in both SP and µP2, in particular at large width. As
expected, ρtransfers in µP2.
6510−310−210−1
Perturbation radius939495969798Test accuracywidth
4.0
2.0
1.0
0.5(a) SP
10−310−210−1
Perturbation radius939495969798Test accuracywidth
4.0
2.0
1.0
0.5 (b)µP2
Figure H.20: (SAM with layerwise normalization in ResNets) Test accuracy as a function of
perturbation radius ρfor ResNets trained with SAM with layerwise normalization.
H.6 Test error over the course of training
Figure H.21 shows the test error of ResNets and ViTs over the course of training. µP2always achieves
the best final test accuracy. In ResNets it also achieves a decent test accuracy the fastest and removes
training instabilities of SAM in SP. While SGD in µP alone cannot compete with SAM in SP, SAM
inµP2uniformly dominates over the entire course of training. Our theory suggests that in µP2the
gradients are scaled correctly from the beginning, whereas in SP they have to self-stabilize first,
which slows down convergence. We plan a closer analysis in an upcoming work.
In ViTs, µP generally achieves decent accuracy faster than SP, since gradient norms are already scaled
correctly at initialization. SAM converges slower than the base optimizer AdamW in favor of drifting
towards a better generalizing local minimum or saddle point. For ViTs at this moderate scale, SAM
in SP catches up to SAM in µP2at the end of training.
0 25 50 75 100 125 150 175 200
Epochs of Training101102100% - test accuracySAMµP2
SGDµP
SAM SP
SGD SP
(a) ResNet on CIFAR10
0 50 100 150 200 250 300
Epochs of training3×1014×1016×101100% - test accuracySAMµP2
AdamWµP
SAM SP
AdamW SP (b) ViT on CIFAR100
Figure H.21: Training a ResNet-18 with width multiplier 2on CIFAR10 (left) and a ViT with width
multiplier 2on CIFAR100 (right). SGD and AdamW are the respective base optimizers.
66NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In the abstract and introduction we state our main contributions while ac-
knowledging related work. All main claims are theoretically proven and/or empirically
verified.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Limitations are discussed in the future work section as well as in the section in
the appendix that is related to the respective limitation.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We state all assumptions in the main paper and Appendix C, and provide all
formal proofs in Appendix E.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
67•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: All experimental details are disclosed in Appendix G. Our perturbation scaling
rules are clearly stated in the main paper. Their implementation with flexible fan_in and
fan_out is explained in Appendix F.7, together with pseudocode for implementing our
proposed scaling rule.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: We only propose width-dependent scaling of hyperparameters of an existing
optimization algorithm. This can be easily implemented by following the scaling rules that
we clearly specify in the main paper. In Appendix F.7 we even provide a code example that
68contains the essential modifications.We are working on making Python code to reproduce
all of our experiments publicly available.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All experimental details are disclosed in the main paper or Appendix G.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: As stated in Appendix G, we repeat all main experiments with multiple
independent runs and report confidence bands within the empirical 2.5%- to97.5%-quantiles.
When we repeat experiments on Vision Transformers that we have also conducted on MLPs
or ResNets, we do not use multiple runs due to limitations in computational resources.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
69•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We provide the type of GPU used and number of GPU seconds required for
each experiment in Appendix G.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We provide a theoretical analysis of a widely used optimization algorithm,
point out the algorithm’s limitations in large models and propose a correction. We do not
foresee any ethical concerns.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper provides fundamental research toward understanding and improving
existing optimization algorithms for neural networks. We do not release any model or data
and do not consider generative models.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
70technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We only use standard vision architectures and vision datasets in our experi-
ments and do not release any data or models.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We cite the standard CIFAR10, CIFAR100 (Krizhevsky et al., 2009) and
ImageNet1K (Deng et al., 2009) datasets following the standard practice. We also cite
the Python assets PyTorch (Paszke et al., 2019), mup(Yang et al., 2022) and the GitHub
repository implementing SAM (Samuel, 2022) that we use as a basis for our experiments.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
71•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
72