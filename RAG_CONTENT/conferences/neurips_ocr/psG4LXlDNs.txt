Achieving ˜O(1/ε)Sample Complexity for Constrained
Markov Decision Process
Jiashuo Jiang
Department of Industrial Engineering & Decision Analytics
Hong Kong University of Science and Technology
Hong Kong, China
jsjiang@ust.hk
Yinyu Ye
Department of Management Science & Engineering
Institue of Computational Mathematics and Engineering
Stanford University
California, US
yyye@stanford.edu
Abstract
We consider the reinforcement learning problem for the constrained Markov deci-
sion process (CMDP), which plays a central role in satisfying safety or resource
constraints in sequential learning and decision-making. In this problem, we are
given finite resources and a MDP with unknown transition probabilities. At each
stage, we take an action, collecting a reward and consuming some resources, all
assumed to be unknown and need to be learned over time. In this work, we
take the first step towards deriving optimal problem-dependent guarantees for the
CMDP problems. We derive a logarithmic regret bound, which translates into a
O(1
∆·ε·log2(1/ε))sample complexity bound, with ∆being a problem-dependent
parameter, yet independent of ε. Our sample complexity bound improves upon
the state-of-art O(1/ε2)sample complexity for CMDP problems established in
the previous literature, in terms of the dependency on ε. To achieve this advance,
we develop a new framework for analyzing CMDP problems. To be specific, our
algorithm operates in the primal space and we resolve the primal LP for the CMDP
problem at each period in an online manner, with adaptive remaining resource
capacities. The key elements of our algorithm are: i) a characterization of the
instance hardness via LP basis, ii) an eliminating procedure that identifies one
optimal basis of the primal LP, and; iii) a resolving procedure that is adaptive to
the remaining resources and sticks to the characterized optimal basis.
1 Introduction
Reinforcement learning (RL) is pivotal in the realm of dynamic decision-making under uncertainty,
where the objective is to maximize total reward through ongoing interaction with and learning from
an enigmatic environment. Markov Decision Processes (MDPs) are a prevalent framework for
encapsulating environmental dynamics. MDPs have been instrumental in various domains, such as
video gaming [ 52], robotics [ 41], recommender systems [ 57], inventory control [ 11], and beyond.
Yet, they fall short in accommodating additional constraints that may influence the formulation of
the optimal policy and the decision-maker’s engagement with the uncertain environment. Often, in
MDP applications, there are stringent constraints on utilities or costs, emanating from areas like safe
38th Conference on Neural Information Processing Systems (NeurIPS 2024).autonomous driving [ 22], robotics [ 56], revenue management [ 33] and financial management [ 62].
These constraints might also symbolize limitations on resources within resource allocation applica-
tions. Constrained MDPs (CMDPs), as introduced in [ 4], enhance MDPs to factor in constraints
affecting long-term policy results. In CMDPs, the decision-maker aims to optimize cumulative
rewards while adhering to these constraints. Our paper focuses on CMDPs, and we aim to develop
efficient algorithmic solutions.
The significance of RL in CMDP contexts has garnered substantial attention in recent years. A
variety of methods for tackling CMDPs have been developed, including the primal-dual technique
[21,49,12,69,18,45,25,26], which leverages the Lagrangian dual of CMDPs and implements
an online learning strategy for the iterative update of dual variables. Other methods encompass
constrained optimization [ 1,61], the Lyapunov technique [ 14], among others. Previous research
has established minimax bounds for CMDPs, representing the optimal regret that can be achieved
for the most challenging problem within a specific class of problems. Nonetheless, these minimax
regret bounds and worst-case scenarios can be overly conservative, leading to a disconnect between
theoretical guarantees and practical performance for specific problem instances. A more tailored
approach is warranted—one that ensures great performance on every single problem instance and
furnishes problem-dependent guarantees. Our research takes the first step towards deriving optimal
problem-dependent guarantees for CMDP problems.
1.1 Preliminaries
We consider a CMDP problem with a finite set of states S={1,2, . . . ,|S|} and a finite set of
actions A={1,2, . . . ,|A|}. We denote by γ∈(0,1)a discount factor. We also denote by
P:S ×A → D (S)the probability transition kernel of the CMDP, where D(S)denotes a probability
measure over the state space S. Then, P(s′|s, a)denotes the probability of transiting from state sto
states′when the action ais executed. The initial distribution over the states of the CMDP is denoted
byµ1.
There is a stochaastic reward function r:S × A → D [0,1]andKstochastic cost functions
ck:S × A → D [0,1]for each k∈[K]. We also denote by ˆr(s, a) =E[r(s, a)]for each (s, a)and
ˆc(s, a) =E[ck(s, a)]for each (s, a). For any Markovian policy π, where the action of πdepends
only on the current state and the action of πis allowed to be randomized, we denote by Vr(π, µ1)the
infinite horizon discounted reward of the policy π, with the formulation of Vr(π, µ1)given below:
Vr(π, µ0) =E"∞X
t=0γt·r(st, at)|µ1#
, (1)
where (st, at)is generated according to the policy πand the transition kernel Pwith the initial state
distribution µ1. For each k∈[K], the infinite horizon discounted cost of the policy πis denoted by
Vk(π, µ1), and the following constraint needs to be satisfied by the policy π,
Vk(π, µ1) =E"∞X
t=0γt·ck(st, at)|µ1#
≤αk,∀k∈[K]. (2)
To solve the CMDP problem, we aim to find an optimal Markovian policy, denoted by π∗, that
maximizes the reward in (1)while satisfying the cost constraint (2)for each k∈[K], with αk∈h
0,1
1−γi
being a pre-specified value for each k∈[K]. Importantly, we assume that the reward
function r, the cost functions {ck}K
k=1, and the transition kernel P, are all unknown to the decision
maker. Our goal is to obtain a policy πthat approximates the optimal policy π∗with as few samples
as possible. We now describe the sampling procedure and present the performance measure of our
policy. We assume the existence of a stylized generative model M, as studied in [ 39,38,44]. The
model Msatisfies the following condition.
Assumption 1.1 For each state and action pair (s, a), we can query the model Mto obtain an
observation of r(s, a),ck(s, a)for each k∈[K], and the new state s′∈ S, where the transition from
stos′follows the probability kernel P(s′|s, a)independently.
Note that in reinforcement learning for CMDP problems, querying the generative model Mcan be
costly. Therefore, it is desirable to query the model Mas less as possible, while guaranteeing the
2near optimality of the approximate policy. To this end, we use sample complexity as the measure of
the performance of our policy. For any ε, we aim to find an ε-accurate policy πsuch that
Vr(π∗, µ1)−Vr(π, µ1)≤εandVk(π, µ1)−αk≤ε,∀k∈[K], (3)
with as few samples as possible.
1.2 Our Main Results and Contributions
The main result of our research is the introduction of a novel algorithm that promises a O(1
∆·ε·
log2(1/ε))sample complexity bound, where ∆is a positive constant that characterizes the gap
between the optimal policy and the sub-optimal ones. Note that the state-of-the-art sample complexity
bound under the worst-case scenarios is O(1/ε2), which has been established in a series of work
[68,37,21]. Though the O(1/ε)or better iteration complexity has been achieved in [ 45,50,74,26],
it comes with a sample complexity bound no better than O(1/ε2). Our algorithm enjoys a sample
complexity bound that has a better dependency in terms of ε. To achieve this improved result, we
develop several new elements listed below.
Contribution 1 : we develop new characterizations of the problem instance hardness for CMDP
problems. Note that a key component for achieving instance-dependent bounds is to characterize the
“hardness” of the underlying problem instance. That is, we need to identify a positive gap to separate
the optimal policy from the sub-optimal policies for a particular problem instance. The importance
of identifying such a gap has been demonstrated in instance-optimal learning for multi-arm-bandit
problems (e.g. [ 42]) and reinforcement learning problems (e.g. [ 6]), among others. For CMDP,
identifying such a gap is non-trivial because the optimal policies for CMDP are randomized policies
[4]. Then, the policies can be represented by distributions over the action set and the sub-optimal
policies can be arbitrarily close to the optimal policy as long as the corresponding sub-optimal
distributions converge to the optimal one. To tackle this problem, we show that the feasible region
for the policies can be represented as a polytope and we only need to focus on the corner points of
this polytope to find an optimal policy. Therefore, the hardness can simply be characterized as the
distance between the optimal corner point and the sub-optimal corner point, as illustrated in detail in
Section 2.1. This is the first characterization of problem instance hardness for CMDP problems.
Contribution 2 : we devise a new algorithmic framework to analyze CMDP problems, inspired by
the online packing/linear programming (LP) literature [ 3,40,47,46]. Specifically, we utilize a linear
programming reformulation of the CMDP problem, where policies are delineated via occupancy
measures [4]. The optimal policy emerges from the LP’s solution; however, the indeterminate model
parameters mean the LP cannot be solved directly but must be approached online as we obtain more
samples from the generative model. Each generative model query leads to solving an empirical LP
with accrued samples, and our final policy is derived from averaging these solutions—an approach
akin to the methodology in online LP. A critical feature of our algorithm is the adaptiveness of the LP
constraints’ right-hand side to the input samples, a technique demonstrated to achieve logarithmic
regret in online LP literature, which we now apply to CMDP problems.
Contribution 3 : we extend our contributions to the online LP literature. Note that after adopting
the LP reformulation, the corner points of the feasible region for policies can be represented by the
basis of the LP. Separating the optimal policy from the others is equivalent to identifying one optimal
basis of the LP. We utilize an approach that lexicographically restricts one variable to zero and tests
whether the LP value has changed. We show that this approach systematically pinpoints a particular
optimal LP basis with a high probability. Then, we develop a resolving procedure that capitalizes on
the structure of the identified optimal basis, which involves only the non-zero basic variables and the
active constraints. This is a new approach of deriving problem-dependent bound for online LP.
Other related literature. Problem-dependent guarantees have been considered extensively in the
RL literature, where a series of work [ 72,59,54,23,17,66,65,71,20] establishes the log(N)
regret bound or the O(κ·ε−1)sample complexity, with κbeing a problem-dependent constant.
Our approach can also be directly applied to the RL problems. Moreover, our approach can handle
long-term constraints and can deal with multi-objective (safe) RL problems.
our work presents a new algorithm for safe RL problems. We adopt an occupancy measure represen-
tation of the optimal and obtain an LP to work with, which is similar to the previous work. However,
our algorithm resolves an LP and operates in the primal space, which is fundamentally different from
the previous work that adopts a primal-dual update (e.g. [ 60], [19], [73], [8], [53]). There is also
3work developing primal-based algorithms, for example ([ 51], [13], [15], [16], [70]). Our algorithm
is completely different from the previous work and we obtain new results. The result is that we
are able to obtain an instance-dependent ˜O(1/ϵ)sample complexity the first time in the literature,
which improves upon the O(1/ϵ2)worst-case sample complexity established in the previous work.
Though the constrained optimization approach and the Lyapunov approach have also been developed
for CMDP problems, they do not enjoy a theoretical guarantee. In comparison to the literature, we
develop a new primal-based algorithm and achieve the first instance-dependent sample complexity
for CMDP problems.
Our new primal-based algorithm is motivated from the literature of online linear programming
[3,40,47] and bandits with knapsack problems (e.g. [ 46], [48]). In these problems, the optimal
policy can be written as an LP and we need to develop an online policy to solve this LP sequentially.
Note that a prevalent strategy is to resolve the LP adaptive to the remaining resources, which has
been developed in a long line of research on various applications, for example [ 31], [32], [5], [36],
[64], [35] and [ 10]. We make the innovation of resolving the LP while sticking to the identified
optimal basis, which distinguishes our algorithm from the previous ones in online LP. Note that the
online LP techniques has also been extended to handle non-stationarity, for example in [ 9], [34]. It
is an interesting future topic to explore whether our algorithm can be extended to non-stationary
environment.
2 LP Reformulation
The infinite horizon discounted setting described in Section 1.1 admits a linear programming refor-
mulation. To be specific, due to the existence of the constraints, the optimal policy of a CMDP can be
randomized policies (e.g. [ 4]), where it is optimal to take a stochastic action given the current state.
Therefore, it is convenient to represent a policy through the occupation measure , which gives us the
desired linear programming reformulations of the CMDP problems.
For the infinite horizon discounted problem, the occupancy measure is defined as qπ(s, a)for any
states, action a, and policy π. Note that qπ(s, a)represents the total expected discounted time spent
on the state-action pair (s, a), under policy π, multiplied by 1−γ. Then, following [ 4], the optimal
policy (and the optimal occupancy measure) can be obtained from the following linear programming.
VInfi= maxX
s∈SX
a∈Aˆr(s, a)·q(s, a) (4a)
s.t.X
s∈SX
a∈Aˆck(s, a)·q(s, a)≤αk ∀k∈[K](4b)
X
s′∈SX
a∈Aq(s′, a)·(δs,s′−γ·P(s|s′, a)) = (1 −γ)·µ1(s) ∀s∈ S (4c)
q(s, a)≥0 ∀s∈ S, a∈ A,(4d)
where δs,s′= 1s=s′, and µ1(s)denotes the probability for the first state to be realized as s∈ S
following the initial distribution µ1. Note that for an optimal solution {q∗(s, a)}∀s∈S,∀a∈Ato(4),
the corresponding optimal policy π∗will be
P(π∗(s) =a) =

q∗(s, a)P
a′∈Aq∗(s, a′),ifX
a′∈Aq∗(s, a′)>0
1/|A|, ifX
a′∈Aq∗(s, a′) = 0 ,(5)
where π∗(s)denotes the probability for the policy π∗to take the action a∈ A given the state s∈ S.
In fact, whenP
a′∈Aq∗(s, a′) = 0 , we can take an arbitrary action. In what follows, we rely on the
linear programming formulation (4) to derive our results.
4Figure 1: A graph illustration of the hardness characterization via LP basis, where the shaded area
denotes the feasible region for the policies.
2.1 Characterization of Instance Hardness
We now rewrite the LP (4) into the following standard formulation to proceed with our illustration.
V= max ˆr⊤q
s.t. Cq≤α
Bq=µ
q≥0.(6)
Note that one crucial step for achieving problem-dependent bounds is to characterize the hardness
of the underlying problem instance and define a gap that separates the optimal policies from the
others. For multi-arm-bandit problem, the characterization of the hardness can be the gap between
the optimal arm and the best sub-optimal arm (e.g. [ 42]). For reinforcement learning problem,
the characterization of the hardness can also be the gap between the optimal policy and the best
sub-optimal policy (e.g. [ 6]). As long as this separation gap is a positive constant, denoted by ∆,
separating the optimal policy from the others with a probability at least 1−ϵwould require samples
at most1
∆·log(1/ϵ), which finally implies the instance-optimal sample complexity bound.
For CMDP problem, characterizing the hardness of the problem instance can be hard. Based on LP
(6), we know that a feasible policy corresponds to a feasible solution and the sub-optimal solution
can be arbitrarily close to the optimal solution since the feasible set is “continuous”. Therefore, there
is no direct way to identify a positive gap between the optimal policies and the sub-optimal ones.
However, from standard LP theory, we know that one corner point of the feasible region must be
one optimal solution. Therefore, we can simply focus on the corner points when solving LP (6)and
we define the gap as the distance between the optimal corner point and the sub-optimal corner point ,
as illustrated in Figure 1. As we will show later, the LP reformulation (6)and such a characterization
of hardness via corner points will inspire our entire approach.
Since the problem hardness is characterized via corner points, it is essential to provide further
characterization of the corner points. Note that in LP theory, the corner point is called basic solution
and can be represented by LP basis , which involves the set of basic variables that are allowed to be
non-zero, and the set of active constraints that are binding under the corresponding basic solution.
Our next lemma follows from standard LP theory, where the proof is provided in the appendix for
completeness.
Lemma 2.1 Denote by bthe number of rows in the matrix B. Then, there exists a subset J∗⊂[K]
withK′=|J∗|and an optimal solution q∗to LP (6)such that there are b+K′variables in q∗are
non-zero. Moreover, we denote by I∗the index set of the non-zero element in q∗. Then, the optimal
solution q∗can be uniquely determined as the solution to the linear system
C(J∗,I∗)qI∗=αJ∗, (7a)
B(:,I∗)qI∗=µ, (7b)
qI∗c= 0. (7c)
withI∗cbeing the complementary set of the index set I∗.
Remark . Note that the minimax lower bound Ω(1/ϵ2)has been established in previous work [ 7,63].
However, this does not contradict with our ˜O(1/ϵ)sample complexity after we introduce the instance
5hardness measure ∆. To be specific, for a problem instance I, we can denote by S(I, ϵ)the number
of samples needed to construct an ϵ-optimal policy. Then the worst-case lower bound implies that
max IS(I, ϵ) = Θ(1 /ϵ2). However, if we do not consider the worst-case guarantee, i.e., if we do
not maximize over the problem instance I, then we can characterize an instance-dependent constant
∆(I)(independent of ϵ) such that S(I, ϵ) = ∆( I)/ϵ·polylog (1/ϵ). When the problem instance
is favorable such that the constant ∆(I)is smaller than 1/ϵ, our bound strictly improves upon the
worst-case bound.
Overview of our approach : in the first part, we aim to identify the optimal basis IandJ. In
this way, we identify the optimal corner point to look at. The detailed procedure is described in
Section 3. In the second part, we learn the optimal solution given the optimal basis we have identified,
which finally gives us a near-optimal policy with the desired sample complexity bound. The detailed
procedure is described in Section 4.
3 Construct Estimates and Identify Optimal Basis
We describe how to construct estimates for LP (6). To this end, for a round N0, we denote by FN0
the filtration of all the information collected up to round N0. Then, we denote by ¯CN0(resp. ¯BN0) an
estimate of the matrix C(resp. B), constructed using the information in the set FN0. We also denote
by¯rN0and estimate of ˆrconstructed from the information in the set FN0. To be specific, we define
¯rN0(s, a) =PN0
n=1rn(s, a)
N0,¯ck,N0(s, a) =PN0
n=1cn
k(s, a)
N0,and¯PN0(s′|s, a) =PN0
n=1 1sn(s,a)=s′
N0,
(8)
where rn(s, a)denotes the n-th observation of the reward, and cn
k(s, a)denotes the n-th observation
of the k-th cost, and sn(s, a)denotes the n-th observation of the state transition for the state-action
pair(s, a), forn∈[N0]. Then, similar to [ 21], we can use the following LP to obtain an estimate of
V(6).
¯VN0= max ( ¯rN0)⊤q
s.t. ¯CN0q≤α+λN0
¯BN0q≤µ+λN0
¯BN0q≥µ−λN0
q≥0,(9)
withλN0being a parameter that we specify later. To bound the estimation gap between ¯VN0andV, it
is useful to bound the optimal dual solution to (6). To this end, we adopt the approach in [ 29,55] that
utilizes Slater’s condition, which is imposed as an assumption below.
Assumption 3.1 There exists a policy ¯πsuch that all the resource constraints are satisfied strictly.
In other words, there exists an occupancy measure ¯qsuch that B¯q=µandC¯q<α. In fact, for
each state s∈ S, there exists a null action that consumes no resource.
The Slater point ¯qcan be set as the policy that takes the null action given each state. The estimation
error will be related to the gap between the Slater point ¯qand the optimal point q∗. We then define
the lower gap as
Gap1(N0, ε)≥V−¯VN0 (10)
and the upper gap as
Gap2(N0, ε)≥¯VN0−V (11)
with both inequalities (10) and (11) hold with probability at least 1−ε.
3.1 Bound the Estimation Gap
We denote by Rad(N0, ε) =q
log(2/ε)
2N0. Following the standard Hoeffding’s inequality, we know that
|¯rN0(s, a)−ˆr(s, a)|,|¯ck,N0(s, a)−ˆck(s, a)|, and|¯PN0(s′|s, a)−P(s′|s, a)|are all upper bounded
byRad(N0, ε)with probability at least 1−ε. We can simply set
Gap1(N0, ε) = Rad( N0, ε) (12)
6and
Gap2(N0, ε) =2Rad( N0, ε)
mink∈[K]{αk}·
1 +|S|
1−γ
+Rad2(N0, ε)
mink∈[K]{αk}·
|S|+|S|2
1−γ
. (13)
We have the following result, where the proof is relegated to appendix.
Lemma 3.2 As long as λN0= Rad( N0, ε), the following inequality
V≤¯VN0+ Gap1(N0, ε)≤V+ Gap1(N0, ε) + Gap2(N0, ε). (14)
holds with probability at least 1−(K|S||A| − |S|2|A|)·ε, where Gap1(N0, ε)is defined in (12)
andGap2(N0, ε)is defined in (13).
3.2 Characterize One Optimal Basis
We now describe how to identify one optimal basis of the LP (6)as required in Lemma 2.1, by
sequentially discarding the sub-optimal actions and the redundant constraints. The formal algorithm
to identify such non-zero elements and the constraints is given in Algorithm 1.
Algorithm 1 Algorithm for identifying one optimal basis
1:Input: the historical sample set FN0that contains N0samples for each (s, a)∈ S × A .
2:Compute the value of ¯VN0as in (9).
3:Initialize Ito be the whole index set that contains every column index of matrix Bin(6)and
J= [K].
4:fori∈ Ido
5: LetI′=I\{i}.
6: Compute the value of ¯VI′,N0as in (15).
7: If|¯VI′,N0−¯VN0| ≤2Gap1(N0, ε) + 2Gap2(N0, ε), then we set I=I′.
8:end for
9:fork= 1, . . . , K do
10: LetJ′=J \{q}.
11: Compute the value of Dual J′,I,N0as in (18).
12: If|¯VN0−DualJ′,I,N0| ≤2Gap1(N0, ε) + 2Gap2(N0, ε), then we set J=J′.
13:end for
14:Output : the set of indexes IandJ.
We now explain the intuition why Algorithm 1 works. Denote by q∗an optimal solution to LP (6).
We describe how to identify the non-zero elements in q∗and how to identify the constraints such that
the values of the non-zero elements of q∗can be uniquely determined by the corresponding linear
equation. For each i-th element of q∗, we compare the value of V(6)against Vwith an additional
constraint that qi= 0. If the two values are different, we identify a non-zero element. To this end, for
an index set I, we define an LP, as well as its estimate, as follows.
VI= max ˆr⊤q ¯VI,N0= max ( ¯rN0)⊤q
s.t. Cq≤α s.t. ¯CN0q≤α+λN0
Bq=µ |¯BN0q−µ| ≤λN0
qIc= 0 qIc= 0
q≥0, q≥0.(15)
where Icdenotes the complementary set of I. Note that if V−VI>0, we know that Iccontains a
non-zero basic variable. The steps 4-8 in Algorithm 1 reflect this point. Starting from Idenoting
the whole index set, we sequentially delete one element i(denoting (s, a)in the infinite horizon
discounted problem) from the set I. Once we detected that V−VI\{i}>0, we know that iis a
non-zero basic variable and we add iback into the set I. In this way, we can classify all the basic
variables into the set I. However, since we do not know the exact value of VandVI, we use the
estimates and compare the value of ¯VN0and¯VI,N0. For this comparison to be valid, the estimation
error has to be smaller than the intrinsic gap between VandVI. We define a constant
∆1= min
I{V−VI:V−VI>0} (16)
7and we need N0to be large enough such that the estimation gap is smaller than ∆1/2.
To find the corresponding active constraints, we consider the dual program of VI, where Iis
determined in steps 4-8 in Algorithm 1, and similarly, we test which dual variable can be set to 0
without influencing the dual objective value. For a dual variable index subset J ⊂ [K], we consider
the dual program as follows.
DualJ,I= min α⊤y+µ⊤z (17a)
s.t.(C(:,I))⊤y+ (B(:,I))⊤z≥ˆrI (17b)
yJc= 0 (17c)
y≥0,z≥ −∞ , (17d)
where Jc= [K]\J. The estimate of Dual J,I, can be obtained from the estimate of its dual, which
is given below.
DualJ,I,N0=¯VJ,I,N0= max ( ¯rN0)⊤q (18a)
s.t.¯CN0(J,:)q≤α+λN0 (18b)
|¯BN0q−µ| ≤λN0 (18c)
qI= 0 (18d)
q≥0, (18e)
Similarly, we compare the value of DualI, where DualI=DualJ′,IwithJ′= [K], and DualJ,I.
However, we can only compare the value of their estimates DualI,N0andDualJ,I,N0. To this end,
we define a constant
∆2= min
J⊂[K]{DualJ,I−DualI:DualJ,I−DualI>0}. (19)
For the comparison to be valid, we need N0to be large enough such that the estimation error is smaller
than∆2/2. In this way, we identify the linearly independent binding constraints corresponding to q∗,
as described in steps 9-13 of Algorithm 1.
4 Our Final Algorithm
We now describe our formal algorithm. From the output of Algorithm 1, we characterize one
optimal solution. If the sample size nis used in Algorithm 1, we denote by InandJnthe output of
Algorithm 1. To be specific, we have q∗
Icn= 0and the non-zero elements q∗
Incan be given as the
solution to C(Jn,In)
B(:,In)
·q∗
In=αJn
(1−γ)·µ
. (20)
However, in practice, both the matrices C(Jn,In)andB(:,In)are unknown. We aim to use the
samples to learn the matrices C(Jn,In)andB(:,In)such that the q∗
Incan also be determined.
Our formal algorithm is given in Algorithm 2. The steps 3-8 in Algorithm 2 is to use Algorithm 1
as a subroutine to identify the set IandJthat satisfy the conditions in Theorem 2.1. We can
show that as long as n≥N′
0, where N′
0is a threshold that depends on the problem parameters,
Algorithm 1 correctly obtains the set IandJsatisfying the conditions in Theorem 2.1. Therefore,
we exponentially increase the value of nas input to Algorithm 1 to reach N′
0.
A crucial element in Algorithm 2 (step 10) is that we adaptively update the value of αn
Jn−1andµn(s′)
as in (22) and(23). We then use the updated αn
Jn−1andµn(s′)to obtain the value of ˜qn
Inas in (21).
Such an algorithmic design follows the resolving idea from online LP to achieve a problem-dependent
bound. In step 11, we further project ˜qn+1to the set {q≥0 :∥q∥1≤2}to obtain qn+1. This
prevents ˜qnfrom behaving ill when nis not large and the estimates of C(Jn,In)andB(:,In)are
not accurate enough. We can show that when nis large enough, ˜qn+1automatically belongs to the
set{q≥0 :∥q∥1≤2}.
5 Theoretical Analysis
In this section, we conduct our theoretical analysis. The analysis can be divided into two parts. In
the first part, we show that Algorithm 1 can successfully help us identify one optimal basis to work
8Algorithm 2 The Adaptive-resolving Algorithm
1:Input: the number of samples Nfor each (s, a)∈ S × A .
2:Initialize F1=∅,α1=N·αandµ1=N·µ.
3:forn= 1, . . . , N do
4: ifn= 2mfor an integer mthen
5: Obtain the output InandJnfrom Algorithm 1 with the input Fn.
6: else
7: setIn=In−1andJn=Jn−1.
8: end if
9: Construct estimates ¯Cn(Jn,In)and¯Bn(:,In)using the sample set Fn.
10: Construct a solution ˜qnsuch that ˜qn
Ic
n= 0and˜q∗
Inis the solution to
"¯Cn(Jn,In)
¯Bn(:,In)#
·˜qn
In=
αn
Jn
N−n+ 1
µn
N−n+ 1
. (21)
11: Project ˜qnto the set {q:∥q∥1≤2}to obtain qn.
12: For each (s, a)∈ In⊂ S × A , we query the model Mto obtain a sample of the reward
rn(s, a)and the costs cn
k(s, a)for each k∈ Jn⊂[K], as well as the state transition sn(s, a).
13: Update Fn+1=Fn∪ {rn(s, a), cn
k(s, a), sn(s, a),∀(s, a)∈ S × A ,∀k∈[K]}.
14: Denote by cn(s, a) = (cn
k(s, a))∀k∈Jnand do the update:
αn+1
Jn=αn
Jn−X
(s,a)∈Incn(s, a)·qn(s, a). (22)
15: Do the update:
µn+1(s′) =µn(s′)−X
(s,a)∈Iqn(s, a)·(δs′,s−γ 1{s′=sn(s,a)}),∀s′∈ S. (23)
where 1{s′=sn(s,a)}is an indicator function of whether the state transition sn(s, a)equals s′.
16:end for
17:We define ¯qNsuch that ¯qN
Ic
N= 0and¯qN
IN=1
N·PN
n=1qn
IN. We then define a policy ¯πN
P(¯πN(s) =a) =

¯qN(s, a)P
a′∈A¯qN(s, a′),ifX
a′∈A¯qN(s, a′)>0
1/|A|, ifX
a′∈A¯qN(s, a′) = 0 .(24)
18:Output: policy ¯πN.
with. In the second part, we show how to learn the optimal distribution over the optimal basis we
have identified.
We now present the theorem showing that Algorithm 1 indeed helps us identify one optimal basis
with a high probability. In practice, the value of εwill be set to 1/Nin the following theorem.
Theorem 5.1 For any ε >0, as long as N0satisfies the condition
2Gap1(N0, ε) + 2Gap2(N0, ε)≤min{∆1,∆2} (25)
the outputs IN0andJN0of Algorithm 1 satisfy the conditions described in Lemma 2.1 with probability
at least 1−(K|S||A| − |S|2|A|)·ε. Moreover, the sets InandJnwill be common for any n≥N0
satisfying (25), which we denote by I∗andJ∗.
An important problem parameter related to I∗andJ∗can be described as follows. Define
A∗=C(J∗,I∗)
B(:,I∗)
. (26)
9We then denote by {σ1(A∗), . . . , σ |S|+K′(A∗)}the eigenvalues of the matrix A∗. We define σas
σ= min
|σ1(A∗)|, . . . ,|σ|S|+K′(A∗)|	
. (27)
From the non-singularity of the matrix A∗, we know that σ >0. We then have the following bound.
Theorem 5.2 With a sample complexity bound of
O(|S|+K)3· |S| · |A|
α2·ξ·σ(1−γ)·min{σ2,(1−γ)2·∆}·log2(1/ε)
ε
,
where ∆ = min {∆2
1,∆2
2},ξ= min (s,a)∈I∗{q∗(s, a)}andq∗denotes the optimal soluton to LP (6)
corresponding to the optimal basis I∗andJ∗, we obtain a policy ¯πNfrom Algorithm 2 (defined in
(24)) such that
Vr(π∗, µ1)−Vr(¯πN, µ1)≤εandVk(¯πN, µ1)−αk≤ε,∀k∈[K].
Our Algorithm 2 can be directly applied to solving MDP problems without resource constraints and
our bounds in Theorem E.2 and Theorem 5.2 still hold. Note that in the MDP problems, the parameter
σcan be lower bounded by 1−γ, which follows from the fact that the matrix A∗can simply be
represented by the probability transition matrix. Then, we have the following sample complexity
bound for our Algorithm 2.
Proposition 5.3 For the MDP problems without resource constraints, i.e., K= 0, with a sample
complexity bound of
O|S|4· |A|
(1−γ)4·ξ·∆·log2(1/ε)
ε
, (28)
we obtain a policy ¯πNfrom Algorithm 2 (defined in (24)) such that
Vr(π∗, µ1)−Vr(¯πN, µ1)≤εandVk(¯πN, µ1)−αk≤ε,∀k∈[K].
In terms of the dependency of our sample complexity bound on other problem parameters such as
|S|,|A|, and 1−γ, we compare to the series of work [ 58,67,68,2,27], that subsequently achieves
a sample complexity bound of O
|S|·|A|
(1−γ)3·ε2
, where the dependency over |S|,|A|, and 1−γis
optimal [ 24,43]. Our sample complexity bound in (28) has a worse dependency in terms of |S|and
1−γ. This is because we construct an empirical LP to estimate the value of LP (6)and resolve the
linear equation as in (21), where the size of the LP (which is |S|) and the eigenvalues of the matrix
A∗(which is bounded by 1−γ) will play a part. However, our bound (28) enjoys a better dependency
in terms of ε.
6 Conclusions
In this paper, we develop the first instance-dependent ˜O(1/ϵ)sample complexity for constrained
MDP problems. We characterize the instance hardness via corner points of the LP formulation and
we develop a resolving algorithm to learn the optimal solution while sticking to the identified optimal
basis. The work presented by this paper advances the field of Machine Learning and the algorithmic
ideas developed in this paper have a broader impact to inspire new algorithms. Our results are
developed for the tabular settings, which pose some limitations to the real-world applications of our
methods. We leave the extensions to more involved settings for future work.
Acknoledgement
Jiashuo Jiang is generously supported by the early career scheme 26210223 and the general research
fund 16204024 from Research Grants Council, Hong Kong.
10References
[1]Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization.
InInternational conference on machine learning , pages 22–31. PMLR, 2017.
[2]Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a
generative model is minimax optimal. In Conference on Learning Theory , pages 67–83. PMLR,
2020.
[3]Shipra Agrawal, Zizhuo Wang, and Yinyu Ye. A dynamic near-optimal algorithm for online
linear programming. Operations Research , 62(4):876–890, 2014.
[4] Eitan Altman. Constrained Markov decision processes . vol7. CRCPress, 1999.
[5]Alessandro Arlotto and Itai Gurvich. Uniformly bounded regret in the multisecretary problem.
Stochastic Systems , 9(3):231–260, 2019.
[6]Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement
learning. Advances in neural information processing systems , 21, 2008.
[7]Mohammad Gheshlaghi Azar, Rémi Munos, Mohammad Ghavamzadeh, and Hilbert Kappen.
Reinforcement learning with a near optimal rate of convergence. 2011.
[8]Qinbo Bai, Amrit Singh Bedi, and Vaneet Aggarwal. Achieving zero constraint violation
for constrained reinforcement learning via conservative natural policy gradient primal-dual
algorithm. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, pages
6737–6744, 2023.
[9]Santiago Balseiro, Haihao Lu, and Vahab Mirrokni. Dual mirror descent for online allocation
problems. In International Conference on Machine Learning , pages 613–628. PMLR, 2020.
[10] Pornpawee Bumpensanti and He Wang. A re-solving heuristic with uniformly bounded loss for
network revenue management. Management Science , 66(7):2993–3009, 2020.
[11] Boxiao Chen, Jiashuo Jiang, Jiawei Zhang, and Zhengyuan Zhou. Learning to order for
inventory systems with lost sales and uncertain supplies. Management Science , 2024.
[12] Yi Chen, Jing Dong, and Zhaoran Wang. A primal-dual approach to constrained markov
decision processes. arXiv preprint arXiv:2101.10895 , 2021.
[13] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained
reinforcement learning with percentile risk criteria. Journal of Machine Learning Research ,
18(167):1–51, 2018.
[14] Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A
lyapunov-based approach to safe reinforcement learning. Advances in neural information
processing systems , 31, 2018.
[15] Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control. arXiv preprint
arXiv:1901.10031 , 2019.
[16] Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757 , 2018.
[17] Christoph Dann, Teodor Vanislavov Marinov, Mehryar Mohri, and Julian Zimmert. Beyond
value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement
learning. Advances in Neural Information Processing Systems , 34:1–12, 2021.
[18] Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo Jovanovic. Provably
efficient safe exploration via primal-dual policy optimization. In International Conference on
Artificial Intelligence and Statistics , pages 3304–3312. PMLR, 2021.
[19] Dongsheng Ding, Kaiqing Zhang, Jiali Duan, Tamer Ba¸ sar, and Mihailo R Jovanovi ´c. Conver-
gence and sample complexity of natural policy gradient primal-dual methods for constrained
mdps. arXiv preprint arXiv:2206.02346 , 2022.
11[20] Yaqi Duan and Martin J Wainwright. Taming" data-hungry" reinforcement learning? stability
in continuous state-action spaces. arXiv preprint arXiv:2401.05233 , 2024.
[21] Yonathan Efroni, Shie Mannor, and Matteo Pirotta. Exploration-exploitation in constrained
mdps. arXiv preprint arXiv:2003.02189 , 2020.
[22] Jaime F Fisac, Anayo K Akametalu, Melanie N Zeilinger, Shahab Kaynama, Jeremy Gillula,
and Claire J Tomlin. A general safety framework for learning-based control in uncertain robotic
systems. IEEE Transactions on Automatic Control , 64(7):2737–2752, 2018.
[23] Dylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent
complexity of contextual bandits and reinforcement learning: A disagreement-based perspective.
arXiv preprint arXiv:2010.03104 , 2020.
[24] Mohammad Gheshlaghi Azar, Rémi Munos, and Hilbert J Kappen. Minimax pac bounds on
the sample complexity of reinforcement learning with a generative model. Machine learning ,
91:325–349, 2013.
[25] Arnob Ghosh, Xingyu Zhou, and Ness Shroff. Achieving sub-linear regret in infinite horizon av-
erage reward constrained mdp with linear function approximation. In The Eleventh International
Conference on Learning Representations , 2022.
[26] Egor Gladin, Maksim Lavrik-Karmazin, Karina Zainullina, Varvara Rudenko, Alexander Gas-
nikov, and Martin Takac. Algorithm for constrained markov decision process with linear
convergence. In International Conference on Artificial Intelligence and Statistics , pages 11506–
11533. PMLR, 2023.
[27] Jiafan He, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement learning
for discounted mdps. Advances in Neural Information Processing Systems , 34:22288–22300,
2021.
[28] Nicholas J Higham. Accuracy and stability of numerical algorithms . SIAM, 2002.
[29] Jean-Baptiste Hiriart-Urruty and Claude Lemaréchal. Convex analysis and minimization algo-
rithms I: Fundamentals , volume 305. Springer science & business media, 1996.
[30] Roger A Horn and Charles R Johnson. Matrix analysis . Cambridge university press, 2012.
[31] Stefanus Jasin and Sunil Kumar. A re-solving heuristic with bounded revenue loss for network
revenue management with customer choice. Mathematics of Operations Research , 37(2):313–
345, 2012.
[32] Stefanus Jasin and Sunil Kumar. Analysis of deterministic lp-based booking limit and bid price
controls for revenue management. Operations Research , 61(6):1312–1320, 2013.
[33] Jiashuo Jiang. Constant approximation for network revenue management with markovian-
correlated customer arrivals. arXiv preprint arXiv:2305.05829 , 2023.
[34] Jiashuo Jiang, Xiaocheng Li, and Jiawei Zhang. Online stochastic optimization with wasserstein
based non-stationarity. arXiv preprint arXiv:2012.06961 , 2020.
[35] Jiashuo Jiang, Will Ma, and Jiawei Zhang. Degeneracy is ok: Logarithmic regret for network
revenue management with indiscrete distributions. arXiv preprint arXiv:2210.07996 , 2022.
[36] Jiashuo Jiang and Jiawei Zhang. Online resource allocation with stochastic resource consump-
tion. arXiv preprint arXiv:2012.07933 , 2020.
[37] Yujia Jin and Aaron Sidford. Efficiently solving mdps with stochastic mirror descent. In
International Conference on Machine Learning , pages 4890–4900. PMLR, 2020.
[38] Sham Machandranath Kakade. On the sample complexity of reinforcement learning . University
of London, University College London (United Kingdom), 2003.
[39] Michael Kearns, Yishay Mansour, and Andrew Y Ng. A sparse sampling algorithm for near-
optimal planning in large markov decision processes. Machine learning , 49:193–208, 2002.
12[40] Thomas Kesselheim, Andreas Tönnis, Klaus Radke, and Berthold Vöcking. Primal beats dual
on online packing lps in the random-order model. In Proceedings of the forty-sixth annual ACM
symposium on Theory of computing , pages 303–312, 2014.
[41] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.
The International Journal of Robotics Research , 32(11):1238–1274, 2013.
[42] Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Ad-
vances in applied mathematics , 6(1):4–22, 1985.
[43] Tor Lattimore and Marcus Hutter. Pac bounds for discounted mdps. In Algorithmic Learn-
ing Theory: 23rd International Conference, ALT 2012, Lyon, France, October 29-31, 2012.
Proceedings 23 , pages 320–334. Springer, 2012.
[44] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Breaking the sample size
barrier in model-based reinforcement learning with a generative model. Operations Research ,
72(1):203–221, 2024.
[45] Tianjiao Li, Ziwei Guan, Shaofeng Zou, Tengyu Xu, Yingbin Liang, and Guanghui Lan.
Faster algorithm and sharper analysis for constrained markov decision process. arXiv preprint
arXiv:2110.10351 , 2021.
[46] Xiaocheng Li, Chunlin Sun, and Yinyu Ye. The symmetry between arms and knapsacks: A
primal-dual approach for bandits with knapsacks. In International Conference on Machine
Learning , pages 6483–6492. PMLR, 2021.
[47] Xiaocheng Li and Yinyu Ye. Online linear programming: Dual convergence, new algorithms,
and regret bounds. Operations Research , 70(5):2948–2966, 2022.
[48] Shang Liu, Jiashuo Jiang, and Xiaocheng Li. Non-stationary bandits with knapsacks. Advances
in Neural Information Processing Systems , 35:16522–16532, 2022.
[49] Tao Liu, Ruida Zhou, Dileep Kalathil, Panganamala Kumar, and Chao Tian. Learning policies
with zero or bounded constraint violation for constrained mdps. Advances in Neural Information
Processing Systems , 34:17183–17193, 2021.
[50] Tao Liu, Ruida Zhou, Dileep Kalathil, PR Kumar, and Chao Tian. Policy optimization for
constrained mdps with provable fast global convergence. arXiv preprint arXiv:2111.00552 ,
2021.
[51] Yongshuai Liu, Jiaxin Ding, and Xin Liu. Ipo: Interior-point policy optimization under
constraints. In Proceedings of the AAAI conference on artificial intelligence , volume 34, pages
4940–4947, 2020.
[52] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602 , 2013.
[53] Ted Moskovitz, Brendan O’Donoghue, Vivek Veeriah, Sebastian Flennerhag, Satinder Singh,
and Tom Zahavy. Reload: Reinforcement learning with optimistic ascent-descent for last-iterate
convergence in constrained mdps. In International Conference on Machine Learning , pages
25303–25336. PMLR, 2023.
[54] Wenlong Mou, Zheng Wen, and Xi Chen. On the sample complexity of reinforcement learning
with policy space generalization. arXiv preprint arXiv:2008.07353 , 2020.
[55] Angelia Nedi ´c and Asuman Ozdaglar. Subgradient methods for saddle-point problems. Journal
of optimization theory and applications , 142:205–228, 2009.
[56] Masahiro Ono, Marco Pavone, Yoshiaki Kuwata, and J Balaram. Chance-constrained dynamic
programming with application to risk-aware robotic space exploration. Autonomous Robots ,
39:555–571, 2015.
[57] Guy Shani, David Heckerman, Ronen I Brafman, and Craig Boutilier. An mdp-based recom-
mender system. Journal of Machine Learning Research , 6(9), 2005.
13[58] Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye. Near-optimal time and
sample complexities for solving markov decision processes with a generative model. Advances
in Neural Information Processing Systems , 31, 2018.
[59] Max Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for
tabular mdps. Advances in Neural Information Processing Systems , 32, 2019.
[60] Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning
by pid lagrangian methods. In International Conference on Machine Learning , pages 9133–9143.
PMLR, 2020.
[61] Zhongchang Sun, Sihong He, Fei Miao, and Shaofeng Zou. Constrained reinforcement learning
under model mismatch. arXiv preprint arXiv:2405.01327 , 2024.
[62] Aviv Tamar, Dotan Di Castro, and Shie Mannor. Policy gradients with variance related risk
criteria. In Proceedings of the twenty-ninth international conference on machine learning , pages
387–396, 2012.
[63] Sharan Vaswani, Lin Yang, and Csaba Szepesvári. Near-optimal sample complexity bounds for
constrained mdps. Advances in Neural Information Processing Systems , 35:3110–3122, 2022.
[64] Alberto Vera and Siddhartha Banerjee. The bayesian prophet: A low-regret framework for
online decision making. Management Science , 67(3):1368–1391, 2021.
[65] Andrew J Wagenmaker and Dylan J Foster. Instance-optimality in interactive decision making:
Toward a non-asymptotic theory. In The Thirty Sixth Annual Conference on Learning Theory ,
pages 1322–1472. PMLR, 2023.
[66] Andrew J Wagenmaker, Max Simchowitz, and Kevin Jamieson. Beyond no regret: Instance-
dependent pac reinforcement learning. In Conference on Learning Theory , pages 358–418.
PMLR, 2022.
[67] Martin J Wainwright. Variance-reduced q-learning is minimax optimal. arXiv preprint
arXiv:1906.04697 , 2019.
[68] Mengdi Wang. Randomized linear programming solves the markov decision problem in nearly
linear (sometimes sublinear) time. Mathematics of Operations Research , 45(2):517–546, 2020.
[69] Honghao Wei, Xin Liu, and Lei Ying. A provably-efficient model-free algorithm for constrained
markov decision processes. arXiv preprint arXiv:2106.01577 , 2021.
[70] Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo: A new approach for safe reinforcement
learning with convergence guarantee. In International Conference on Machine Learning , pages
11480–11491. PMLR, 2021.
[71] Yunbei Xu and Assaf Zeevi. Towards optimal problem dependent generalization error bounds
in statistical learning theory. Mathematics of Operations Research , 2024.
[72] Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference
on Machine Learning , pages 7304–7312. PMLR, 2019.
[73] Sihan Zeng, Thinh T Doan, and Justin Romberg. Finite-time complexity of online primal-dual
natural actor-critic algorithm for constrained markov decision processes. In 2022 IEEE 61st
Conference on Decision and Control (CDC) , pages 4028–4033. IEEE, 2022.
[74] Ruida Zhou, Tao Liu, Dileep Kalathil, PR Kumar, and Chao Tian. Anchor-changing regular-
ized natural policy gradient for multi-objective reinforcement learning. Advances in Neural
Information Processing Systems , 35:13584–13596, 2022.
14A Numerical Experiments
We implement our Algorithm 2 to study the numerical performance. We consider a CMDP problem
with the state space |S|= 10 and the action space |A|= 10 . We set the discount factor γ= 0.7.
We then randomly generate the probability transition kernel P. To be specific, for each state s∈ S,
action a∈ A , and the future state s′∈ S, we uniformly generate a randomly variable ps,a,s′.
Then, the transition probability is defined as P(s′|s, a) =ps,a,s′P
s′′∈Sps,a,s′′. For each state-action pair
(s, a)∈ S × A , the expected reward ˆr(s, a)is uniformly generated from the interval [1,2](with the
reward for the first action set to be 0). The actual reward r(s, a) = ˆr(s, a) +η, where ηis uniformly
distributed among [−0.5,0.5]. There are K= 5constraints and for each constraint k∈[K]and each
state-action pair (s, a)∈ S × A , we define the expected cost ˆck(s, a)to be uniformly generated from
[1,2]. The actual cost ck(s, a) = ˆck(s, a) +η′, where η′is uniformly distributed among [−0.5,0.5].
For each total iterations N, We apply Algorithm 2 and obtain the output q1, . . . ,qN. We compare
¯qNwith the optimal occupancy measure. Since our algorithm is a randomized algorithm, we
study the performance of our algorithm in expectation. To be specific, given the problem instance
and a fixed N, we implement our algorithm repeatedly for M= 500 rounds. Denote by ¯qN
mthe
output of our Algorithm 2 at round m, for m∈[M]. We define the error term as Err(N) =
1
M·PM
m=1∥¯qN
m−q∗∥1/∥q∗∥1. We study how the error term Err(N)scales with N. The results are
displayed in Figure 2. As we can see, the error term Err(N)decreases quickly with respect to N.
Moreover, since our Algorithm 2 requires only solving a set of linear equations in each iteration, the
computation cost of our Algorithm 2 is also moderate.
Figure 2: The computational performance of Algorithm 2. The x-label denotes the size of N, while
the y-label denotes the error term Err (N).
B Proof of Lemma 2.1
Letq∗be a basic optimal solution to LP (6). We also denote by Ithe index set of q∗such that
q∗
I>0. We then denote by J′∈[K]the set of active constraints at q∗. Then, the following linear
equations must be satisfied by q∗.
C(J′,I)q∗
I=αJ′andB(:,I)q∗
I=µ. (29)
From the property of the basic optimal solution, we must have |J′|+b≥ |I| .
If|J′|+b=|I|, then we know that we already find the desired index set IandJ=J′.
Otherwise, if |J′|+b >|I|, then we know that the LP (6)is degenerate. However, now the linear
system (29) is over-determined, i.e., there must be |J′|+b− |I| number of equations can be implied
by the others. It only remains to show that those redundant equations are all in the set J′. This
step can be done can showing that the matrix B(:,I)has a full row rank. We prove this by showing
contradiction.
15For the infinite horizon discounted problem, suppose that B(:,I)does not have full row rank, there
must exists β∈R|S|such that
β⊤B(:,I) =0. (30)
On the other hand, denote by π∗the optimal policy corresponding to q∗. We then define a matrix
Γ∈R|I|×Ssuch that the s-th column of Γis a vector that takes 0for all the (s′, a)∈ I-th element if
s′̸=s, and take a value of π∗(a|s)for all asuch that (s, a)∈ I. We know that
B(:,I)Γ = I−γ·Pπ∗
withI∈R|S|×|S|being the identify matrix and Pπ∗being the transition probability matrix under the
policy π∗, with the element at the s-th row and s′-th column denoting the probability of transiting
from state sto state s′under the policy π∗. It is well known that the matrix I−γ·Pπ∗is non-singular
(see for example the Gersgorin’s Theorem in [ 30]). Therefore, for any vector β∈R|S|, we know that
β⊤B(:,I)Γ =β⊤(I−γ·Pπ∗)̸= 0
which contradicts with (60). Our proof is thus completed.
C Proof of Lemma 3.2
We now condition on the event that |¯rN0(s, a)−ˆr(s, a)|,|¯ck,N0(s, a)−ˆck(s, a)|, and|¯PN0(s′|s, a)−
P(s′|s, a)|for each k∈[K],(s, a)∈ S × A ands′∈ Sare all bounded by Rad(N0, ε). From the
union bound, we know that this event happens with probability at least 1−(K|S||A| − |S|2|A|)·ε.
Note that for LP (4), by summing up the constraint (4c)for all s∈ S, we obtain that any feasible
solution qfor LP (4) would satisfy
∥q∥1= 1. (31)
We first upper bound the gap V−¯VN0. Denote by q∗one optimal solution to V. Then, from the
feasibility of q∗, we know that
¯CN0q∗=Cq∗+ (¯CN0−C)q∗≤α+ Rad( N0, ε)≤α+λN0 (32)
where the first inequality follows from ∥q∗∥1= 1 (31) and all elements of ¯CN0−Care upper
bounded by Rad(N0, ε). Also, we know that
¯BN0q∗=Bq∗+ (¯BN0−B)q∗≤(1−γ)·µ+γ·Rad(N0, ε)≤(1−γ)·µ+λN0 (33)
where the first inequality follows from ∥q∗∥1= 1 (31) and all elements of ¯BN0−Bare upper
bounded by γ·Rad(N0, ε). Similarly, we have that
¯BN0q∗=Bq∗+ (¯BN0−B)q∗≥(1−γ)·µ−γ·Rad(N0, ε)≥(1−γ)·µ−λN0.(34)
Therefore, as long as
λN0≥Rad(N0, ε), (35)
we know that q∗is a feasible solution to ¯VN0. We have that
¯r⊤
N0q∗≥ˆr⊤q∗−Rad(N0, ε) (36)
by noting ∥q∗∥1= 1(31) and all elements of ¯rN0−ˆrare upper bounded by Rad(N0, ε). Therefore,
we can obtain the bound
V−Rad(N0, ε)≤¯r⊤
N0q∗≤¯VN0. (37)
We then lower bound the gap V−¯VN0. We first define
¯VN0(λ′
N0) = max ( ¯rN0−λ′
N0)⊤q
s.t. ¯CN0q≤α+λN0
¯BN0q≤µ+λN0
¯BN0q≥µ−λN0
q≥0,(38)
16for any constant λ′
N0. Clearly, any optimal solution ¯q∗to¯VN0will be a feasible solution to ¯VN0(λ′
N0).
Moreover, by summing up the constraints ¯BN0q≤µ+λN0for all s∈ S, we know that
∥¯q∗∥1≤1 +|S|
1−γ·λN0. (39)
Then, it holds that
¯VN0≤(¯rN0−λ′
N0)⊤¯q∗+λ′
N0· ∥¯q∗∥1≤¯VN0(λ′
N0) +λ′
N0+|S|
1−γ·λN0λ′
N0. (40)
We then compare the value between ¯VN0(λ′
N0)andV. The dual of ¯VN0(λ′
N0)is given below.
Dual′
N0(λ′
N0) = min α⊤y+µ⊤(z1−z2) +λN0·(∥y∥1+∥z1∥1+∥z2∥1)
s.t. ¯C⊤
N0y+¯B⊤
N0(z1−z2)≥¯rN0−λ′
N0
y≥0,z1≥0,z2≥0.(41)
Denote by y∗andz∗one optimal solution to the dual of LP (6), given below.
Dual = min α⊤y+µ⊤z
s.t. C⊤y+B⊤z≥ˆr
y≥0,z≥ −∞ .(42)
We now show that y∗andz∗is also a feasible solution to Dual′
N0, with λ′
N0=1
mink∈[K]{αk}·
1 +|S|
1−γ
·Rad(N0, ε). We have the following claim regarding the upper bound on ∥y∗∥∞and
∥z∗∥∞.
Claim C.1 There exists an optimal solution y∗andz∗to the Dual (42) such that
∥y∗∥1≤1
mink∈[K]{αk}and∥z∗∥∞≤1
1−γ·1
mink∈[K]{αk}.
Then, we define ¯y∗=y∗,z∗
1= max {0,z∗}andz∗
2= max {0,−z∗}. We have
¯C⊤
N0¯y∗+¯B⊤
N0(z∗
1−z∗
2)≥C⊤y∗+B⊤z∗−Rad(N0, ε)·(∥y∗∥1+∥z∗∥1)
≥C⊤y∗+B⊤z∗−Rad(N0, ε)·1
mink∈[K]{αk}·
1 +|S|
1−γ
≥ˆr−1
mink∈[K]{αk}·
1 +|S|
1−γ
·Rad(N0, ε).
Thus, we know that ¯y∗andz∗
1,z∗
2is also a feasible solution to Dual N0(λ′
N0), and we have
Dual′
N0(λ′
N0)≤Dual +λN0·(∥y∗∥1+∥z∗∥1)≤Dual +Rad(N0, ε)
mink∈[K]{αk}·
1 +|S|
1−γ
.(43)
Combing (43) with (40) and also noting that ¯VN0(λ′
N0) =Dual′
N0(λ′
N0), we have
¯VN0≤V+2Rad( N0, ε)
mink∈[K]{αk}·
1 +|S|
1−γ
+Rad2(N0, ε)
mink∈[K]{αk}·
|S|+|S|2
1−γ
which completes our proof.
Proof of Claim C.1. We first bound ∥y∗∥∞. We utilize the approach in [ 29,55]. We define a
Lagrangian function, with only yas the Lagrangian dual variable.
L(y,q) :=α⊤y+ˆr⊤q−y⊤Cq (44)
where the feasible set for qis{q≥0 :Bq=µ}and the feasible set for yis{y≥0}. Following
Lemma 3 in [ 55], it is without loss of generality to restrict the feasible set of yto the set {y≥0 :
∥y∥1≤ρ}, where the constant ρis defined as
ρ=ˆr⊤q∗−ˆr⊤¯q
mink∈[K]{αk−C(k,:)¯q}(45)
17where ¯qis the occupancy measure that satisfies Slater’s condition as stated in Theorem 3.1. Note that
we have
ρ≤1
mink∈[K]{αk}.
Therefore, we obtain the following bound on y∗:
∥y∗∥1≤ρ. (46)
We now proceed to bound z∗. Denote by q∗the optimal solution corresponding to the optimal dual
solution (y∗,z∗), withy∗bounded as in (46). We also denote by π∗the optimal policy corresponding
toq∗. Then, from the complementary slackness condition, as long as q∗(s, a)>0for a(s, a), we
must have
C(:,(s, a))⊤y∗+B(:,(s, a))⊤z∗= ˆr(s, a). (47)
We now multiply both sides of (47) byq∗(s, a)/P
a′∈Aq∗(s, a′), and sum up over a, for each state
s. Then we get
(Cπ∗)⊤y∗+Bπ∗z∗=ˆrπ∗. (48)
Here, Cπ∗∈RK×|S|and the element at k-th row and s-th column isP
a∈Ack(s, a)·q∗(s,a)P
a′∈Aq∗(s,a′).
Bπ∗=I−γ·Pπ∗∈R|S|×S, where Pπ∗(s, s′) =P
a∈Aπ∗(a|s)·P(s′|s, a)denotes the tran-
sition probability matrix under the policy π∗. Also, ˆrπ∗∈R|S|with ˆrπ∗(s) =P
a∈Aˆr(s, a)·
q∗(s,a)P
a′∈Aq∗(s,a′). Then, we have
z∗= (Bπ∗)−1·
ˆrπ∗−(Cπ∗)⊤y∗
(49)
From [37], we know that
∥(Bπ∗)−1∥∞≤1
1−γ. (50)
Also, from the bound on y∗in (46), we know that
ˆrπ∗−(Cπ∗)⊤y∗
∞≤ρ. (51)
Therefore, we have that
∥z∗∥∞≤ρ
1−γ, (52)
which completes our proof. □
D Proof of Theorem 5.1
We now condition on the event that |¯rN0(s, a)−ˆr(s, a)|,|¯ck,N0(s, a)−ˆck(s, a)|, and|¯PN0(s′|s, a)−
P(s′|s, a)|for each k∈[K],(s, a)∈ S × A ands′∈ Sare all bounded by Rad(N0, ε). We know
that this event happens with probability at least 1−(K|S||A| − |S|2|A|)·ε.
Following the procedure in Algorithm 1, we identify an index set I, where we set qi= 0for each
i∈ Ic. Note that we cannot further delete one more ifrom the set I. Otherwise, when deleting this
particular iinIby setting I′=I\{i}, we will have VI′=Vand as a result of Lemma 3.2, we have
|¯VN0−¯VI′,N0| ≤ |V−VI′|+2Gap1(N0, ε)+2Gap2(N0, ε) = 2Gap1(N0, ε)+2Gap2(N0, ε)(53)
Therefore, if it is feasible (objective value does not change) to delete one more ifrom the index set I,
our algorithm will already do so.
We denote by q∗the optimal solution to LP (6), corresponding to the optimal basis Iidentified by
Algorithm 1. We know that q∗
I>0andq∗
Ic= 0, which implies that
V=VI= max ( ˆrI)⊤q
s.t. C(:,I)q≤α
B(:,I)q=µ
q≥0,(54)
18Note that in the formulation of (54), we simply discard the columns of the constraint matrix in the
index set I. Thus, one optimal solution to (54) will just be q∗
I. The dual of (54) is
DualI= min α⊤y+µ⊤z
s.t.(C(:,I))⊤y+ (B(:,I))⊤z≥ˆr(I)
y≥0,z≥ −∞ ,(55)
From the complementary slackness condition, we know that for the optimal dual variable corre-
sponding to q∗
I, all the constraints in (55) must hold with equality. Therefore, we have the following
result.
Claim D.1 It holds that
V=VI=DualI= min α⊤y+µ⊤z
s.t.(C(:,I))⊤y+ (B(:,I))⊤z=ˆr(I)
y≥0,z≥ −∞ .(56)
The proof of the claim is relegated to the end of this proof. We now show that Algorithm 1 identify the
linearly independent binding constraints for q∗such that the conditions in Lemma 2.1 are satisfied.
Denote by C(J,I)a sub-matrix of Csuch that the rows in the index set J ⊂ [K]and the columns
in the index set Iof the matrix Cremain. Now if the matrix
A= [C(J,I);B(:,I)] (57)
is singular, there must be
(i). one row in C(J,I)can be expressed as the linear combination of the other rows in A(we
know that the matrix B(:,I)has full row rank from Lemma 2.1);
(ii). or one column in Acan be expressed as the linear combination of other columns.
We consider the situation (i). Denote this row as k′and we know that restricting yk′= 0will not
change the objective value of DualI, which implies that k′will be added to the index set J. This is
because yk′cannot be a basic variable, otherwise, the optimal basis of the Dual (56) will be linearly
dependent. Therefore, we obtain a contradiction and we know that the matrix Amust have full row
rank. Thus, situation (i) will not happen.
We then consider the situation (ii). Note that we have
V=DualJ,I= min α⊤
Jy+µ⊤z
s.t.(C(J,I))⊤y+ (B(:,I))⊤z≥ˆrI
y≥0,z≥ −∞ ,(58)
Comparing the formulation (58) to the formulation (55), we only remain the rows of the matrix
C(:,I)in the index set J, where it is feasible (objective value does not change) to set yJc= 0. The
dual of (58) is
V=VJ,I= max ( ˆrI)⊤q
s.t. C(J,I)q≤αJ
B(:,I)q=µ
q≥0.(59)
If situation (ii) happens and one column of Acan be expressed as the linear combination of the
other columns in A, then we denote the index of this column by i′and we know that we can simply
restrict qi′= 0without changing the objective value of (59). This is because qi′cannot be a basic
variable otherwise the corresponding optimal basis will be linearly dependent. However, the index
i′∈ I. This means that we cannot further delete i′from the index set Iby restricting qi′= 0without
changing the objective value of LP (4). The above argument leads to a contradiction. Therefore, we
know that situation (ii) cannot happen.
Given the arguments above, we know that the matrix Ais a non-singular matrix and thus the
conditions in Lemma 2.1 are satisfied with the variable index set Iand the constraint index set J.
19It only remains to show that we can tell whether the objective value of VInfiorDualIhas changed
by restricting one variable to be 0. Following the same steps as in Lemma 3.2, we can show that for
index sets I′andJ′, it holds that
Gap1(N0, ε)≥VJ′,I′−¯VJ′,I′,N0 (60)
and the upper gap as
Gap2(N0, ε)≥¯VJ′,I′,N0−VJ′,I′ (61)
with the formulation of Gap1(N0, ε)andGap2(N0, ε)given in (12) and(13). Therefore, as long as
2Gap1(N0, ε) + 2Gap2(N0, ε)≥min{∆1,∆2}, (62)
where δ1is defined in (16) andδ2is defined in (19), we can tell whether VInfiis different from VInfi
I′
and whether Dual I′is different from Dual J′,I′. Our proof is thus completed.
Proof of Claim D.1. We first show that
V=VI= max ( ˆrI)⊤q
s.t. C(:,I)q≤α
B(:,I)q=µ
q≥0.(63)
Denote by q∗the optimal solution to Vcorresponding to the optimal basis I. It is clear to see that q∗
I
is a feasible solution to VIwith the same objective value. Then, we have
V≤VI. (64)
On the other hand, we denote by ˆqone optimal solution to VI, and we construct
˜qI=ˆqand ˜qIc= 0.
It is clear to see that ˜qis a feasible solution to Vwith the same objective value, which implies that
VI≤V. (65)
Therefore, (63) is proved from combining (64) and (65). The dual of VIis given by
DualI= min α⊤y+µ⊤z
s.t.(C(:,I))⊤y+ (B(:,I))⊤z≥ˆr(I)
y≥0,z≥ −∞ .(66)
We denote by y∗,z∗the optimal dual variable to Vcorresponding to q∗. It is easy to see that y∗,z∗
is also feasible to Dual Iand
DualI=VI=V=α⊤y∗+µ⊤z∗
where the first equality follows from the strong duality between VIandDualI, the second equality
follows from (63), and the third equality follows from the strong duality between Vand its dual.
Therefore, we know that y∗andz∗is also an optimal solution to DualI. Moreover, note that from
the complementary slackness condition, since q∗
I>0, we must have
(C(:,I))⊤y∗+ (B(:,I))⊤z∗=ˆr(I). (67)
We know that y∗,z∗is a feasible solution to
Dual′
I= min α⊤y+ (1−γ)·µ⊤z
s.t.(C(:,I))⊤y+ (B(:,I))⊤z=ˆr(I)
y≥0,z≥ −∞ ,(68)
which implies that
Dual′
I≤DualI.
On the other hand, any feasible solution to Dual′
Imust be a feasible solution to DualI, and we have
Dual′
I≥DualI.
Therefore, we must have Dual′
I=DualIand our proof is completed. □
20E Proof of Theorem 5.2
We first prove the following lemma.
Lemma E.1 For the optimal basis identified in Algorithm 1 and the corresponding optimal solution
q∗, we denote by I∗andJ∗the output sets as long as N0satisfies the condition (25). We also denote
by(y∗,c∗)the corresponding optimal dual solution. Then, it holds that
N·Vr(π∗, µ1)−NX
n=1ˆr⊤E[qn]≤X
j∈J∗y∗
j·E[αN
j] +X
s∈Sz∗
s·E[µN
s]. (69)
Therefore, it suffices to analyze how the “remaining resources” (αn
J∗,µn)behave. We now define
˜αk(n) =αn
k
N−n,∀k∈ J∗and˜µs(n) =µn
s
N−n,∀s∈ S,∀n∈[N]. (70)
The key is to show that the stochastic process ˜αk(n)and˜µspossess some concentration properties
such that they will stay within a small neighborhood of their initial value α1
kandµ1
sfor a sufficiently
long time. We denote by τthe time that one of ˜αk(n)for each k∈ J∗and˜µsfor each s∈ S escape
this neighborhood. Then, both Regretr(π, N)andRegretk(π, N)for each k∈[K]can be upper
bounded by E[N−τ]. From the update rule (22) and (23), we know that
˜αk(n+ 1) = ˜ αk(n)−P
(s,a)∈I∗cn(s, a)·qn(s, a)−˜αk(n)
N−n−1,∀k∈ J∗(71)
and
˜µs(n+ 1) = ˜ µs(n)−P
(s,a)∈I∗qn(s, a)·(δs′,s−γ 1{s′=sn(s,a)})−˜µs(n)
N−n−1. (72)
Ideally, both ˜αk(n+ 1) and˜µs(n+ 1) will have the same expectation as ˜αk(n)and˜µs(n)such that
they become a martingale. However, this is not true since we have estimation error over C(J∗,I∗)
andB(:,I∗), and we only use their estimates to compute qn. Nevertheless, we can show that ˜αk(n)
for each k∈ J∗and˜µsfor each s∈ S behave as a sub-martingale. Then, from the concentration
property of the sub-martingale, we upper bound E[αN
k]for each k∈ J∗andE[µN
s]for each s∈ S.
The term |E[αN
k]|for each k∈[K]\J∗can be upper bounded as well. The results are presented in
the following lemma.
Lemma E.2 Denote by ¯πNthe output policy of Algorithm 2 and denote by Nthe number of rounds.
Then, it holds that
N·Vr(π∗, µ1)−NX
n=1ˆr⊤E[qn]≤O(|S|+K)3
α·σ·min{σ2,(1−γ)2·∆}·log(N)
N
where the parameters α= min k∈[K]{αk},∆ = min {∆2
1,∆2
2}with∆1given in (16) and∆2given
in(19),σgiven in (27). Also, for any k∈[K], we have
N·αk−NX
n=1ˆc⊤
kE[qn]≤O(|S|+K)3
α·σ·min{σ2,(1−γ)2·∆}·log(N)
N
. (73)
E.1 Proof of Lemma E.1
Note that the distribution of qnis independent of the distribution of rn. We know that
E"NX
n=1(rn)⊤qn#
=NX
n=1(ˆr)⊤E[qn] =NX
n=1(ˆrI∗)⊤E[qn
I∗]
Denote by q∗andy∗,z∗the optimal primal-dual variable corresponding to the optimal basis I∗and
J∗. From the complementary slackness condition and noting that q∗
I∗>0, we know that
(C(J∗,I∗))⊤y∗
J∗+ (B(:,I∗))⊤z∗=ˆrI∗. (74)
21Also, we can define a matrix Cn(J∗,I∗)such that the element of Cn(J∗,I∗)at the k∈ J∗row
and(s, a)∈ I∗column is cn
k(s, a). We can also define a matrix Bn(:,I∗)such that the element of
Bn(:,I∗)at the s′∈ S row and (s, a)∈ I∗column is δs′,s−γ· 1{s′=sn(s,a)}. It is easy to see that
E[Cn(J∗,I∗)] =C(J∗,I∗)andE[Bn(:,I∗)] =B(:,I∗).
Then, it holds that
NX
n=1(ˆrI∗)⊤E[qn
I∗] =NX
n=1 
(C(J∗,I∗))⊤y∗
J∗+ (B(:,I∗))⊤z∗⊤E[qn
I∗]
=E"NX
n=1 
(Cn(J∗,I∗))⊤y∗
J∗+ (Bn(:,I∗))⊤z∗⊤qn
I∗#
=E"NX
n=1 
(y∗)⊤(Cn(J∗,I∗))qn
I∗+ (z∗)⊤(Bn(:,I∗))qn
I∗#(75)
Note that we have
NX
n=1Cn(J∗,I∗))qn
I∗=α1
J∗−αN(76)
and
NX
n=1Bn(:,I∗))qn
I∗=µ1−µN. (77)
Plugging (76) and (77) back into (75), we get that
NX
n=1(ˆrI∗)⊤E[qn
I∗] = (y∗
J∗)⊤α1
J∗+ (z∗)⊤µ1−(y∗
J∗)⊤E
αN
J∗
−(z∗)⊤E
µN
.
Note that
Vr(π∗, µ1) = (y∗
J∗)⊤α1
J∗+ (z∗)⊤µ1
that holds from the strong duality of VInfi(4). Then, we have that
N·Vr(π∗, µ1)−NX
n=1(ˆr)⊤E[qn]≤(y∗
J∗)⊤E
αN
J∗
+ (z∗)⊤E
µN
. (78)
Our proof is thus completed.
E.2 Proof of Lemma E.2
We now condition on the event that |¯rn(s, a)−ˆr(s, a)|,|¯ck,n(s, a)−ˆck(s, a)|, and|¯Pn(s′|s, a)−
P(s′|s, a)|for each k∈[K],(s, a)∈ S × A ands′∈ S are all bounded by Rad(n, ε), for any
n∈[N]. We know that this event happens with probability at least 1−N·(K|S||A| − |S|2|A|)·ε.
From now on, we set ε=1
N2to guarantee that the event happens with probability at least 1−
K|S||A| +|S|2|A|
N.
We consider the stochastic process ˜αk(n)and˜µs(n)defined in (70). For a fixed ν >0which we
specify later, we define a set
X={α′∈R|J∗|:α′
k∈[αk−ν, αk+ν],∀k∈ J∗}, (79)
and
Y={µ′∈R|S|:µ′
s∈[µs−ν, µs+ν],∀s∈ S}. (80)
It is easy to see that initially, ˜α(1)∈ X and˜µ(1)∈ Y. We show that ˜α(n)and˜µ(n)behave well as
long as they stay in the region XandYfor a sufficiently long time. To this end, we define a stopping
time
τ= min
n∈[N]{˜α(n)/∈ X or˜µ(n)/∈ Y} . (81)
Note that in Algorithm 2, to stop qnfrom behaving ill when nis small, we project it to a set that
guarantees ∥qn∥1≤2. We now show in the following claim that when nis large enough but smaller
than the stopping time τ, there is no need to do projection.
22Claim E.3 There exist two constants N′
0andν0. When max{N0, N′
0} ≤n≤τ, and ν≤ν0, it
holds that ∥˜qn
I∗∥1≤2, where ˜qn
I∗denotes the solution to the linear equations (21). Specifically, N0
is given in (25) andN′
0is given as follows
N′
0= 16·(|S|+K)2
σ2·log(1/ε) (82)
Also, ν0is given as follows
ν0:= 16·σ·(1−γ)
(|S|+K)2. (83)
We set νsatisfy the condition ν≤ν0withν0satisfies the condition in Claim E.3. We bound E[N−τ]
in the following claim.
Claim E.4 Let the stopping time τbe defined in (81). It holds that
E[N−τ]≤max{N0, N′
0}+ 2(K+|S|)·exp(−ν2/8)
where N0is given in (25) andN′
0is given in (82), as long as
N≥max{N0, N′
0}andN≥8
ν2≥8
ν2
0=(|S|+K)4
32σ2·(1−γ)2. (84)
Also, for any N′such that max{N0, N′
0} ≤N′≤N, it holds that
P(τ≤N′)≤(K+|S|)·ν2
4·exp
−ν2·(N−N′+ 1)
8
. (85)
From the definition of the stopping time τin (81), we know that for each k∈ J∗, it holds
ατ−1
k∈[(N−τ+ 1)·(αk−ν),(N−τ+ 1)·(αk+ν)]
Thus, we have that
|αN
k| ≤ |ατ−1
k|+NX
t=τX
(s,a)∈I∗cn
k(s, a)·qn(s, a) (86)
and thus E[αN
k]≤4E[N−τ]≤4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8). (87)
Following the same procedure, we can show that for each s∈ S, it holds that
E[µN(s)]≤4E[N−τ]≤4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8). (88)
We finally consider the other constraints k∈ J∗c. Note that following the definition of αnandµn,
we have that
A∗· NX
n=1E[qn
I∗]!
= [α1
J∗−E
αN
J∗
;µ1−E
µN
]. (89)
Also, from the bindingness of q∗regarding the optimal basis I∗andJ∗, we have
N·A∗·q∗
I∗= [α1
J∗;µ1]. (90)
Therefore, it holds that
NX
n=1E[qn
I∗] =N·q∗
I∗−(A∗)−1·
E
αN
J∗
;E
µN
, (91)
and NX
n=1E[qn
I∗c]
1≤2N0. (92)
Finally, for any k∈ J∗c, we have
(ˆck)⊤ NX
n=1E[qn]!
=N·(ˆck)⊤q∗
I∗−(ˆck)⊤·(A∗)−1·
E
αN
J∗
;E
µN
+ 2N0
=N·(ˆck)⊤q∗−(ˆck)⊤·(A∗)−1·
E
αN
J∗
;E
µN
+ 2N0(93)
23From the feasibility of q∗, we know that
N·αk≥N·(ˆck)⊤q∗.
Therefore, for any k∈ J∗c, it holds that
N·αk−NX
n=1ˆc⊤
kE[qn]≤(ˆck)⊤·(A∗)−1·
E[αN
J∗];E[µN]
+ 2N0
≤K+|S|
σ· 
4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8)
+ 2N0.
(94)
Moreover, the definition of σin(27) implies that following upper bound on the norm of the dual
variable y∗andz∗.
∥y∗∥1+∥z∗∥1=∥(A∗⊤)−1·ˆrJ∗∥1≤|S|+K
σ. (95)
Therefore, we know that the regret over the reward and the regret over the constraint violation can all
be bounded by using (87), (88), and (94). We present the bounds as follows.
N·Vr(π∗, µ1)−NX
n=1ˆr⊤E[qn]≤K+|S|
σ· 
4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8)
.(96)
Meanwhile, the constraint violations are bounded by (87),(88), and (94). Our proof is thus completed.
Proof of Claim E.3. Denote by q∗the optimal solution corresponding to the optimal basis I∗and
J∗. Then, it holds thatC(J∗,I∗)
B(:,I∗)
·q∗
I∗=αJ∗
µ
. (97)
We compare ˜qn
I∗withq∗
I∗when nlarge enough. Note that when n≥N0,˜qnis the solution to the
following linear equations
"¯Cn(J∗,I∗)
¯Bn(:,I∗)#
·˜qn
I∗=
αn
J∗
N−n+ 1
µn
N−n+ 1
. (98)
When n≤τ, we know thatαJ∗−αn
J∗
N−n+ 1≤ν (99)
and µ−µn
N−n+ 1≤ν. (100)
Moreover, we know that the absolute value of each element of ¯Cn(J∗,I∗)−C(J∗,I∗), and
¯Bn(:,I∗)−B(:,I∗)is upper bounded by Rad(n, ε). We now bound the distance between the
solutions to the linear equations (97) and (98). The perturbation of the matrix is denoted as
∆A∗="
C(J∗,I∗)−¯Cn(J∗,I∗)
B(:,I∗)−¯B(:,I∗)#
.
Clearly, it holds that
∥∆A∗∥1≤Rad(n, ε)·(K+|S|). (101)
Therefore, as long as
∥∆A∗∥1≤Rad(n, ε)·(K+|S|)≤1
2∥(A∗)−1∥1≤1
2σ, (102)
24following standard perturbation analysis of linear equations [28], we have that
∥˜qn
I∗−q∗
I∗∥1
∥q∗
I∗∥1≤κ(A∗)
1−κ(A∗)·∥∆A∗∥1
∥A∗∥1·∥∆A∗∥1
∥A∗∥1+(|S+K| ·ν)
∥[αJ∗;µ]∥1
≤2·κ(A∗)·∥∆A∗∥1
∥A∗∥1+(|S+K| ·ν)
∥[αJ∗;µ]∥1
≤2·κ(A∗)·∥∆A∗∥1
∥A∗∥1+(|S|+K))·ν
1−γ
,(103)
where κ(A∗) =∥A∗∥1· ∥(A∗)−1∥1denotes the conditional number of A∗. The last inequality
follows from ∥[αJ∗;µ]∥1≥1−γ. Further, note that ∥q∗
I∗∥1= 1. Therefore, in order to satisfy the
condition ∥˜qn
I∗∥1≤2, we only need the right hand side of (103) to be upper bounded by 1. Clearly,
as long as nsatisfies the condition (102) and the following condition
2·κ(A∗)·∥∆A∗∥1
∥A∥1≤2·Rad(n, ε)·(K+|S|)
σ≤1
2, (104)
we only need to select a νsuch that
2·κ(A∗)·(|S|+K)·ν
1−γ≤1
2. (105)
Combining (102) and (104), we know that nneeds to satisfy the following conditions: n≥N0and
n≥N′
0:= 16·(|S|+K)2
σ2·log(1/ε). (106)
Also, νis selected to satisfy the following condition
ν≤ν0:= 16·σ·(1−γ)
(|S|+K)2. (107)
Our proof is thus completed. □
Proof of Claim E.4. Now we fix a k∈ J∗. We specify a ¯N0= max {N0, N′
0}. For any
¯N0≤N′≤N, it holds that
˜αk(N′)−˜αk(¯N0) =N′−1X
n=¯N0(˜αk(n+ 1)−˜αk(n)).
We define ξk(n) = ˜αk(n+ 1)−˜αk(n). Then, we have
˜αk(N′)−˜αk(¯N0) =N′−1X
n=¯N0(ξk(n)−E[ξk(n)|Fn]) +N′−1X
n=¯N0E[ξk(n)|Fn].
where Fndenotes the filtration of information up to step n. Note that due to the update in (71), we
have
ξk(n) =˜αk(n)−P
(s,a)∈I∗cn
k(s, a)·qn(s, a)
N−n−1.
Then, it holds that
|ξk(n)−E[ξk(n)|Fn]| ≤2
N−n+ 1(108)
where the inequality follows from the fact that the value of ˜αk(n)is deterministic given the filtration
Fnand∥qn∥1≤2for any n. Note that
{ξk(n)−E[ξk(n)|Fn]}∀n=¯N0,...,N′
forms a martingale difference sequence. Following Hoeffding’s inequality, for any N′′≤N′and any
b >0, it holds that
P
N′′X
n=¯N0(ξk(n)−E[ξk(n)|Fn])≥b
≤2 exp 
−b2
2·PN′′
n=¯N01/(N−n+ 1)2!
≤2 exp
−b2·(N−N′′+ 1)
2
.
25Therefore, we have that
P
N′′X
n=¯N0(ξk(n)−E[ξk(n)|Fn])≥bfor some ¯N0≤N′′≤N′

≤N′X
N′′=¯N02 exp
−b2·(N−N′′+ 1)
2
≤b2·exp
−b2·(N−N′+ 1)
2(109)
holds for any b >0.
We now bound the probability that τ > N′for one particular N′such that ¯N0≤N′≤N. Suppose
thatN′≤τ, then, from Claim E.3, for each n≤N′, we know that ∥˜qn∥1≤2and therefore
qn=˜qnas the solution to (21). We have
˜αk(n) =X
(s,a)∈I∗¯ck,n(s, a)·qn(s, a).
It holds that
|E[ξk(n)|Fn]| ≤1
N−n+ 1·X
(s,a)∈I∗qn(s, a)· |E[¯ck,n(s, a)]−ˆcn
k(s, a)| ≤2Rad( n, ε)
N−n+ 1.(110)
Then, we know that
N′−1X
n=¯N0|E[ξk(n)|Fn]| ≤r
log(2/ε)
2·N′−1X
n=¯N01√n·(N−n)≤r
log(2/ε)
2·√
N′−1·N′−1X
n=¯N01
n·(N−n)
=r
log(2/ε)
2·√
N′−1
N·N′−1X
n=¯N01
n+1
N−n
≤p
2 log(2 /ε)·√
N′−1
N·log(N)≤p
2 log(2 /ε)√
N·log(N)≤ν
2
(111)
for aNlarge enough such that
N≥8
ν2≥8
ν2
0=(|S|+K)4
32σ2·(1−γ)2(112)
Combining (111) and(109) withb=ν/2, and apply a union bound over all k∈ J∗ands∈ S, we
know that
P(τ≤N′)≤(K+|S|)·ν2
4·exp
−ν2·(N−N′+ 1)
8
. (113)
Therefore, we know that
E[N−τ] =NX
N′=1P(τ≤N′)≤¯N0+NX
N′=¯N0P(τ≤N′)≤¯N0+ 2(K+|S|)·exp(−ν2/8)
which completes our proof. □
E.3 Final Proof of Theorem 5.2
Note that in the proof of Lemma E.2, we have shown the following bounds.
E[αN
k]≤4E[N−τ]≤4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8). (114)
holds for the each k∈ J∗. For each s∈ S, it holds that
E[µN(s)]≤4E[N−τ]≤4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8). (115)
The caveat of directly transferring the bound of (114) and(115) into the sample complexity bounds of
the policy ¯πNis that, the vector ¯qNdoes not directly characterize an occupancy measure. This point
26can be seen by noting that there is a gap between B¯qNandµ, though bounded by O(log(N)/N)
by setting ε= 1/N2. However, we can show that the gap between ¯qNandq∗is upper bounded by
O(log(N)/N), which implies a bound over the gap between the policy ¯πNand the optimal policy
π∗that corresponds to the occupancy measure q∗. This bound over the gap between the policy
distributions can be then transferred into the bound over the gap between the state-value functions
under the policy ¯πNandπ∗. The regret bounds can be obtained then.
We first bound the gap between ¯qNandq∗. Note that as long as n≥N0, we have In=I∗following
Theorem 5.1. Then, by noting IN=I∗, we know that
¯qN
I∗c=q∗
I∗c. (116)
Also, note that following the definition of αnandµn, we have that
A∗· NX
n=1E[qn
I∗]!
= [α1
J∗−E
αN
J∗
;µ1−E
µN
]. (117)
Also, from the bindingness of q∗regarding the optimal basis I∗andJ∗, we have
N·A∗·q∗
I∗= [α1
J∗;µ1]. (118)
Then, from (117) and (118), we know that
E¯qN
I∗
−q∗
I∗
∞=q∗
I∗−1
N·NX
n=1E[qn
I∗]
∞=(A∗)−1·
E
αN
J∗
;E
µN
∞
N
≤
E
αN
J∗
;E
µN
∞
σ·N
≤1
σ·N· 
4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8)
.(119)
From Markov’s inequality, for each i∈ I∗and any a >0, we know that
P 
|¯qN
i−q∗
i|> g
≤1
g·σ·N· 
4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8)
. (120)
We denote by
ξ= min
(s,a)∈I∗{q∗(s, a)}. (121)
The policy ¯πNis essentially random by noting that qNis a random variable, where the randomness
comes from the randomness of the filtration FN. For each s∈ S anda∈ A, we denote by ¯π(a|s)
the (ex-ante) probability that the random policy ¯πtakes the action agiven the state s. Then, for any
0< g≤ξ/2, we note that
|qN
i−q∗
i| ≤gfor each i= (s, a)∈ I∗implies thatqN(s, a)P
a′∈AqN(s, a′)−q∗(s, a)P
a′∈Aq∗(s, a′)≤2g
ξ,
(122)
for each i= (s, a)∈ I∗. For any 0< g≤ξ/2, note that
P 
|qN
i−q∗
i| ≤gfor each i= (s, a)∈ I∗
≥1−|S|+K
g·σ·N· 
4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8)
,
(123)
where the inequality follows from the bound (120) and the union bound over i∈ I∗. Therefore, for
any0< g≤ξ/2and any (s, a), we know that
P¯πN(a|s)−π∗(a|s)≤2g
ξ
≥1−|S|+K
g·σ·N· 
4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8)
.
(124)
27From the above inequality, for any (s, a), we have that
E
¯πN(a|s)
−π∗(a|s)
≤E¯πN(a|s)−π∗(a|s)
≤2
Nξ+2
ξ·Zξ/2
g=1
NP¯πN(a|s)−π∗(a|s)≥2g
ξ
dg
≤2
Nξ+2(|S|+K)
ξ·σ·N· 
4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8)
·Zξ/2
g=1
Ndg
g
=2
Nξ+2(|S|+K)
ξ·σ·N· 
4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8)
·(log(N) + log( ξ/2)).
(125)
We finally transfer the bound (125) into the sample complexity bounds of policy ¯πN. We use the
state-value functions Vr(π, s), defined for any initial state sand any policy πas follows
Vr(π, s) =E"∞X
t=0γt·r(st, at)|s#
, (126)
where (st, at)is generated according to the policy πand the transition kernel Pwith the initial state
s. Note that the value of Vr(π, s)for any s∈ S can be obtained from solving Bellman’s equation
under policy π
Vr(π, s) =Ea∼π(·|s)
ˆr(s, a) +γ·Es′∼P(·|s,a)[Vr(π, s′)]
. (127)
We define a matrix Bπ∈R|S|×|S|such that the s-th row s′-th column element is
Bπ(s, s′) =δs,s′−γ·X
a∈Aπ(a|s)·P(s′|s, a). (128)
Then, the matrix Bπrepresents the state transition probability matrix under the policy π. Denote by
Vr(π) = (Vr(π, s))∀s∈S
and
ˆr(π) = (X
a∈Aπ(a|s)·ˆr(s, a))∀s∈S.
We have that the state values Vr(π)is the solution to the linear equation
BπVr(π) =ˆr(π) (129)
To bound the regret, we bound the solution to the linear equation (129) withπbeing ¯πNandπ∗
separately. The perturbation of the right hand of the equation (129) is
∆ˆr=ˆr(¯πN)−ˆr(π∗).
Clearly, we have that
∥∆ˆr∥∞≤2
Nξ+2(|S|+K)
ξ·σ·N· 
4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8)
·(log(N) + log( ξ/2)).
(130)
The perturbation of the matrix is denoted as
∆B=B¯πN−Bπ∗.
Clearly, it holds that
∥∆B∥∞≤2γ
Nξ+2γ(|S|+K)
ξ·σ·N· 
4 max{N0, N′
0}+ 8(K+|S|)·exp(−ν2/8)
·(log(N) + log( ξ/2)).
(131)
We plug the formulation of N0in (25) and N′
0in (82) into the bound (130) and (131). We obtain
∥∆ˆr∥∞≤C1·(|S|+K)3
α2·ξ·σ·min{σ2,(1−γ)2·∆}·log2(N)
N(132)
28where C1is a constant, α= min k∈[K]{αk}, and ∆ = min {∆2
1,∆2
2}with∆1given in (16) and∆2
given in (19). We also obtain
∥∆B∥∞≤C1·γ·(|S|+K)3
α2·ξ·σ·min{σ2,(1−γ)2·∆}·log2(N)
N(133)
Therefore, as long as
C1·(|S|+K)3
α2·ξ·σ·min{σ2,(1−γ)2·∆}·log2(N)
N≤1/∥(Bπ∗)−1∥∞= 1/σ′, (134)
following standard perturbation analysis of linear equations [28], we have that
∥Vr(¯πN)−Vr(π∗)∥∞
∥Vr(π∗)∥∞≤C2·κ(Bπ∗)·∥∆B∥∞
∥Bπ∗∥∞+∥∆ˆr∥∞
∥ˆr(π∗)∥∞
, (135)
where κ(Bπ∗) =∥Bπ∗∥∞· ∥(Bπ∗)−1∥∞denotes the conditional number of Bπ∗, and C2is a
constant. Note that we have the regret
Regretr(¯πN, N) =µ⊤(Vr(¯πN)−Vr(π∗))≤(1−γ)∥Vr(¯πN)−Vr(π∗)∥∞
≤C2(1−γ)·κ(Bπ∗)· ∥Vr(π∗)∥∞·∥∆B∥∞
∥Bπ∗∥∞+∥∆ˆr∥∞
∥ˆr(π∗)∥∞
.(136)
It is clear to see that
∥Vr(π∗)∥∞≤1
1−γ(137)
and
∥Vr(π∗)∥∞≤∥ˆr(π∗)∥∞
1−γ. (138)
Following [37], we have the following bound.
σ′=∥(Bπ∗)−1∥∞≤1
1−γ. (139)
Also, from the definition, we have that
∥Bπ∗∥∞= 1−γ. (140)
Plugging the bound (132), (133), (137), and (138), into the inequality (136), we have that
Regretr(¯πN, N)≤C3·(|S|+K)3
α2·ξ·σ(1−γ)·min{σ2,(1−γ)2·∆}·log2(N)
N(141)
where C3is a constant. In a same way, for any k∈[K], we obtain that
Regretk(¯πN, N)≤C3·(|S|+K)3
α2·ξ·σ(1−γ)·min{σ2,(1−γ)2·∆}·log2(N)
N. (142)
To show the sample complexity bound, we denote by εa constant such that
ε=C3·(|S|+K)3
α2·ξ·σ(1−γ)·min{σ2,(1−γ)2·∆}·log2(N)
N.
Therefore, we have
N=O(|S|+K)3
α2·ξ·σ(1−γ)·min{σ2,(1−γ)2·∆}·log2(1/ε)
ε
.
Note that in each of the Nrounds, we obtain a sample for each (s, a)∈ S × A . Therefore, the bound
onNabove should be multiplied by |S| · |A| to obtain the final sample complexity bound. Our proof
is thus completed.
29NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
•[NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a
proper justification is given (e.g., "error bars are not reported because it would be too computationally
expensive" or "we were unable to find the license for the dataset we used"). In general, answering
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
•Delete this instruction block, but keep the section heading “NeurIPS paper checklist" ,
•Keep the checklist subsection headings, questions/answers and guidelines below.
•Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The contributions and scope is accurately reflected.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations are discussed in the conclusion.
30Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The assumptions and proofs are provided in full.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Experimental details given in the appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
31•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: This is a theoretical paper and all the data is simulated with details given in the
appendix.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
32•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All the experimental details are given in the appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The experiment has been repeated enough times to guarantee that the expected
regret has been appropriately approximated.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: The simulated experiments are easy to implement and no significant computing
resource is required.
Guidelines:
• The answer NA means that the paper does not include experiments.
33•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: It conforms.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Presented in the conclusion.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Not applied here.
34Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: Not applied here.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Not applied here.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
35Answer: [NA]
Justification: Not applied here.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: None is included.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
36