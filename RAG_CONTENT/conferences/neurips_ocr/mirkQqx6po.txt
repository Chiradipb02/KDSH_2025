Learning-Augmented Approximation Algorithms for
Maximum Cut and Related Problems
Vincent Cohen-Addad
Google Research
France
coheaddad@google.comTommaso d’Orsi∗
Bocconi University
Italy
tommaso.dorsi@unibocconi.itAnupam Gupta†
New York University
New York NY 10012
anupam.g@nyu.edu
Euiwoong Lee
University of Michigan
Ann Arbor MI 48105
euiwoong@umich.eduDebmalya Panigrahi
Duke University
Durham NC 27708
debmalya@cs.duke.edu
Abstract
In recent years, there has been a surge of interest in the use of machine-learned
predictions to bypass worst-case lower bounds for classical problems in combina-
torial optimization. So far, the focus has mostly been on online algorithms, where
information-theoretic barriers are overcome using predictions about the unknown
future. In this paper, we consider the complementary question of using learned
information to overcome computational barriers in the form of approximation
hardness of polynomial-time algorithms for NP-hard (offline) problems. We show
that noisy predictions about the optimal solution can be used to break classical
hardness results for maximization problems such as the max-cut problem and more
generally, maximization versions of constraint satisfaction problems (CSPs).
1 Introduction
The design and analysis of algorithms beyond the classical worst-case paradigm has been an active
area of research (see, e.g., the collection of surveys by Roughgarden [2020]). In recent years, this
has been accelerated by the success and widespread adoption of machine learning models across
application domains, leading researchers to ask: can we use machine-learned information to solve
typical instances of a problem better than what we can hope for in the worst case? This meta-question
has been particularly influential in the realm of online optimization, where the goal is to design
algorithms for inputs that are revealed sequentially over time. Indeed, assuming that the future unfolds
in a typical rather than worst-case manner, we can compensate for the information deficit of the online
algorithm with predictions learned based on past data, thereby helping it bypass information-theoretic
lower bounds. This principle has been successfully applied to a broad range of online problems, such
as caching [Lykouris and Vassilvitskii, 2021], rent-or-buy [Purohit et al., 2018], covering Bamas
et al. [2020b], network design Azar et al. [2022], scheduling Azar et al. [2021], matching Dinitz et al.
[2021], and many others (see related work for more references).
In this paper, we study the role of machine-learned predictions in offline NP-hard problems. For
offline problems, an algorithm has no information disadvantage compared to an optimal solution:
the disadvantage is computational. The NP-hardness of problems makes exact algorithms that run
in polynomial time unlikely. This has led to the field of approximation algorithms, where the goal
is to obtain polynomial-time algorithms that output solutions that are approximately optimal. In
∗Part of this work was done when the author was at ETH Zurich.
†Part of this work was done when the author was at Carnegie Mellon University.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).particular, an approximation algorithm for an optimization problem has an approximation factor of
αif the solution it produces on every instance is within a factor of αof that of an optimal solution.
The best approximation factor for an NP-hard offline problem is also subject to computational
barriers. For instance, for the classical MAXCUTproblem, it is known that the approximation factor
ofαGW≈0.878obtained in the celebrated work of Goemans and Williamson [1995] is the best
possible under the Unique Games Conjecture (UGC). This raises a natural question: can we use
machine-learned predictions to overcome computational barriers to approximation algorithms for
NP-hard problems?
1.1 Our Contributions
Suppose we are given a noisy prediction that is mildly correlated with an optimal solution for a given
problem instance. Can we use such a prediction to recover a better approximation to the optimal
solution than is possible without any prediction? For (strongly) NP-hard problems, we typically know
barriers for the best approximation factor αthat can be achieved by polynomial-time algorithms.
Using a machine-learned prediction of an optimal solution, we seek to go beyond this barrier: to
obtain a polynomial-time algorithm with an approximation factor strictly better than α. The quantum
of improvement naturally depends on the quality of the prediction: if the prediction is ε-correlated
with the target solution, can we get an α+f(ε)approximation?
To make these questions concrete, we first consider the MAXCUTproblem in the learning-augmented
setting. Given an undirected, weighted graph, the MAXCUTproblem asks for a bi-partition of the
vertices such that the total weight of edges in the cut is maximized. Assuming the widely-accepted
unique games conjecture, the Goemans-Williamson approximation bound of αGW≈0.878is the best
possible for a polynomial-time algorithm. But, suppose we are given a prediction for the optimal
max-cut that is independently correct for every vertex with probability 1/2+ε, for any ε >0. (Note
that random guessing achieves correctness of 1/2; so, we assume that the prediction is only ε-better
than random guesses.) Can this prediction be used to obtain an approximation factor better than αGW
in polynomial time? This question was posed by Svensson in his SODA 2023 plenary lecture.
Our first result is to unconditionally improve on worst-case performance for the MAXCUTproblem
using an ε-correlated prediction. This answers the question posed by Svensson in the affirmative.
Specifically, we give an algorithm that obtains an (αGW+˜Ω(ε4))-approximate MAXCUTsolution.
This also quantifies the dependence between the improvement in the approximation factor and
the correlation of the prediction with the actual optimal solution. Interestingly, we also relax the
independence requirement to just pairwise independence of the predictions on the vertices. This
is significant because in practice, the predictions for the different vertices are likely to be obtained
from a machine learning model or a human expert, either of which sources are unlikely to output
completely independent predictions for different vertices.
We further complement this result by considering another natural prediction model where instead of
a noisy prediction for every vertex, we get a correct prediction but only for an ε-fraction of randomly
chosen vertices. (To distinguish between these models, we call the former noisy predictions and
the latter partial predictions .) In this case, we obtain an (αRT+ Ω(ε))-approximate solution to
MAXCUT, where αRT≃0.858is the approximation factor obtained by Raghavendra and Tan for the
MAXBISECTION problem [Raghavendra and Tan, 2012]. Note that αRTis slightly smaller than αGW,
but we get a better advantage of Ω(ε)instead of Ω(ε4).
Next, we show that our algorithmic framework is applicable beyond the MAXCUTproblem. Con-
straint Satisfaction Problems (CSPs) are a broad class of optimization problems that includes
fundamental optimization tasks such as MAX-k-SAT,MAX-k-LIN,MAX-k-AND, etc. In particular,
2-CSPs are a subclass of CSPs where each constraint contains only two variables. This includes
problems such as MAXCUT,MAXDICUT, and MAX-2-SAT . A classical result of Arora et al. [1999]
showed that it is possible to obtain an approximation factor arbitrarily close to 1 for “dense” instances
of all 2-CSPs including MAXCUT. We show that ε-correlated predictions of the optimal solution is a
useful tool for dense instances of all 2-CSPs. In particular, we use the prediction to lower the “density
threshold” for obtaining an arbitrarily small approximation factor for dense instances of 2-CSPs. In
other words, the assumption on the density of the instance (which is a function of the prediction bias
and the approximation error) for our result in the learning-augmented setting is weaker than that of
Arora et al. [1999], i.e., our result applies to a broader set of instances.
21.2 Related Work
In recent years, the abundance of data and the tremendous success of machine learning has led to a
variety of attempts at going beyond traditional worst-case analysis for combinatorial optimization by
taking advantage of learned information. In clustering, Ashtiani et al. [2016] introduced a setting
where an algorithm can query an external oracle (e.g., a machine learning model) to learn if a pair
of points are in the same cluster in the optimal clustering (same-cluster queries). The goal then is
to recover an optimal solution (or a sufficiently good approximation) while minimizing the number
of oracle queries. Tight bounds have been obtained for various clustering objectives in this setting,
from k-means [Ailon et al., 2018] to correlation clustering [Mazumdar and Saha, 2017]. Moreover,
robust settings that incorporate noise in the oracle answers have also been studied Larsen et al. [2020],
Del Pia et al. [2022]. Another line of work Ergun et al. [2022], Gamlath et al. [2022] considers
k-means and related clustering problems where the algorithm is provided noisy node labels. For
instance, Gamlath et al. [2022] showed that even when the labels provided by the oracle are correct
with a tiny probability (say 1%), it is possible to obtain a (1 +o(1))-approximation to the k-means
objective as long as the clusters are not too small.
A different line of work has aimed to incorporate machine-learned predictions in the design of online
algorithms (see, e.g., the surveys Mitzenmacher and Vassilvitskii [2020, 2022]). This model was
introduced by Lykouris and Vassilvitskii for the caching problem Lykouris and Vassilvitskii [2021]
and has since been studied in many problem domains such as rent or buy Purohit et al. [2018],
Gollapudi and Panigrahi [2019], Anand et al. [2020], covering Bamas et al. [2020b], Anand et al.
[2022], Gupta et al. [2022], scheduling Purohit et al. [2018], Wei and Zhang [2020], Bamas et al.
[2020a], Lattanzi et al. [2020], Mitzenmacher [2020], Azar et al. [2021], Cohen and Panigrahi [2023],
Lindermayr et al. [2023], Lassota et al. [2023], caching Lykouris and Vassilvitskii [2021], Wei [2020],
Jiang et al. [2022b], Bansal et al. [2022], matching Dinitz et al. [2021], Lavastida et al. [2021],
secretary problems Antoniadis et al. [2020b], Dütting et al. [2021], graph problems Antoniadis et al.
[2020a], Jiang et al. [2022a], Almanza et al. [2021], Antoniadis et al. [2023], Bernardini et al. [2022],
Anand et al. [2022], Fotakis et al. [2021], Azar et al. [2022], and many others. (The reader is referred
to ALP [2023] for a compendium of papers in this area.) The main goal in this line of work is to
overcome information-theoretic lower bounds for online problems using machine-learned predictions
about the unknown future.
Two recent works (concurrent and independent of ours) have considered the computational complexity
ofMAXCUTand CSPs in the context of noisy predictions. The first of these is Bampis et al. [2024],
who tackle the problem of speeding up approximation schemes for dense CSP instances using noisy
predictions. Namely, they provide an algorithm that achieves a (1−ε)approximation whose running
time depends on the density of the instance and the error in the prediction. The algorithm runs in
polynomial time if the number of edges in the instance is Ω(n2/logn), assuming the prediction label
of each vertex is correct with constant probability. The second work is by Ghoshal et al. [2024], who
consider both the partial and the noisy predictions models under full independence of the predictions.
They provide a (1−O((ε√
∆)−1))-approximation for both models, where ∆is the average degree of
the graph. Alternatively, they show that if the edges are weighted, the value of the solution computed
is at least opt−q
nP
ijw2
ijε−1.
2 Preliminaries
The MAXCUTProblem. We start by describing the MAXCUTproblem. In this problem, we are
given a weighted graph G= (V, E)represented by a (symmetric) n×nadjacency matrix A, where
Aij=wij, the weight of edge {i, j}if it exists, and 0otherwise. (We assume the graph has no
self-loops, and hence Ahas zeroes on the diagonal.) We let Wi=P
jwijdenote the weighted degree
of vertex i. We use D:=diag(W1, . . . , W n)to denote the diagonal matrix with these weighted
degrees, and L=D−Ato denote the (unnormalized) Laplacian matrix of the graph. Note that
x∈ {− 1,1}ndenotes a cut in the graph, and the quadratic form ⟨x, Lx⟩=P
{i,j}∈Ewij(xi−xj)2
counts (four times) the weight of edges crossing the cut between the vertices labeled 1, and those
labeled −1.
3The MAXCUTproblem asks for the cut with maximum edge weight. Hence, MAXCUTcan be
rephrased as follows:
MAXCUT(G) := max
x∈{−1,1}n1/4· ⟨x, Lx⟩.
We defer the formal definition of CSPs to Section 5.
The Noisy/Partial Predictions Framework. We describe the prediction models for the MAXCUT
problem. We extend the noisy predictions model to general Max- 2-CSPs in Section 5.
1.In the noisy predictions model forMAXCUT, we assume there is some fixed and un-
known optimal solution x∗∈ {− 1,1}n. The algorithm has access to a prediction
vector Y∈ {− 1,1}n, such that for each vertex i,Yiis the correct label x∗
iwith
some (unknown) probability 1/2+ε, and is the other (incorrect) label with probabil-
ity1/2−ε. Here we only assume pairwise independence; for any two vertices iandj,
Pr[i, jboth give their correct labels ] = ( 1/2+ε)2.
2.In the partial predictions model forMAXCUT, the algorithm has access to a prediction
vector Y∈ {− 1,0,1}nsuch that for each vertex i,Yiis the correct label x∗
iwith some
(unknown) probability εand is 0otherwise. Again, we only assume pairwise independence;
for any two vertices iandj,Pr[i, jboth give their correct labels ] =ε2.
3 M AXCUTin the Noisy Prediction Model
Recall that in the noisy prediction model, the predicted label of each vertex is its correct label in
a fixed max-cut with probability 1/2+ε. Moreover, the labels are pairwise independent. One can
show that in this model, if one were to simply output the prediction itself, then its value would be
at least O(m/2 +ε2(opt−m/2))for an unweighted graphs with medges. But, this is generally
worse than the αGW·opt≈0.878·optbound obtained by the Goemans-Williamson MAXCUT
algorithm. Our main result is to give an algorithm that uses a noisy prediction to outperform αGWin
the approximation bound by an additive poly( ε)factor:
Theorem 3.1 (Noisy Predictions) .Given noisy predictions with a bias of ε, there is a polynomial-time
randomized algorithm that obtains an approximation factor of αGW+˜Ω(ε4)in expectation for the
MAXCUTproblem.
The rest of this section is devoted to proving the above theorem. A basic distinction that we will
use throughout this section is that of ∆-wide and ∆-narrow graphs; these should be thought of as
weighted analogs of high-degree and low-degree graphs. We first define these and related concepts
below, then we present an algorithm for the MAXCUTproblem on ∆-wide graphs in §3.1, followed
by the result for ∆-narrow graphs in §3.2. We finally wrap up with the proof of Theorem 3.1.
We partition the edges incident to vertex iinto two sets: the ∆-prefix foricomprises the ∆heaviest
edges incident to i(breaking ties arbitrarily), while the remaining edges make up the ∆-suffix for
i. We fix a parameter η∈(0,1/2). We will eventually set ∆ = Θ(1 /ε2)andηto be an absolute
constant. Recall that Wi=P
j∈[n]Aijis the weighted degree of i.
Definition 3.2 (∆-Narrow/Wide Vertex) .A vertex iis∆-wide if the total weight of edges in its
∆-prefix is at most ηWi, and so the weight of edges in its ∆-suffix is at least (1−η)Wi. Otherwise,
the vertex iis∆-narrow .
Intuitively, a ∆-wide vertex is one where most of its weighted degree is preserved even if we ignore
the∆heaviest edges incident to the vertex.
We partition the vertices V= [n]into the ∆-wide and∆-narrow sets; these are respectively denoted
V>∆andV<∆. We define W>∆:=P
i∈V>∆WiandW<∆:=P
i∈V<∆Wi, and hence the sum of
weighted degrees of all vertices is W:=Pn
i=1Wi=W>∆+W<∆.
Definition 3.3 (∆-Narrow/Wide Graph) .A graph is ∆-wide if the sum of weighted degrees of ∆-wide
vertices accounts for at least 1−ηfraction of that of all vertices; i.e., if W>∆≥(1−η)W. Otherwise,
it is∆-narrow.
43.1 Solving M AXCUTfor∆-wide graphs
For∆-wide graphs, we show:
Theorem 3.4. Fixε′∈(0,1). Given noisy predictions with bias ε, there is a polynomial-time
randomized algorithm that, given any ∆-wide graph, outputs a cut of value at least the maximum cut
minus (5η+ 2ε′)W, where ∆ := O(1/(ε·ε′)2), with probability 0.98.
Since the graph is ∆-wide, most vertices have their weight spread over a large number of their
neighbors. In this case, the prediction vector allows us to obtain a good estimate ˆrof the optimal
neighborhood imbalance r∗(the difference between the number of neighbors a vertex has on its
side versus the other side of the optimal cut). We can then write an LP to assign fractional labels to
vertices that maximize the cut value while remaining faithful to these estimates ˆr; finally rounding
the LP gives the solution.
3.1.1 The ∆-wide Algorithm
Define an n×nmatrix ˜Afrom the adjacency matrix Aas follows: for each row corresponding to the
edges incident to a vertex i, we set the entry ˜Aij= 0if the edge (i, j)is in the ∆-prefix of vertex i;
otherwise, ˜Aij=Aij. Now, define an n-dimensional vector ˆras follows:
ˆri=1
2ε(˜AY)iifiis∆-wide
0 ifiis∆-narrow
where Yis the prediction vector. Solve the linear program:
min
x∈[−1,1]n⟨ˆr, x⟩s.t.∥ˆr−Ax∥1≤(ε′+ 2η)W. (1)
Letˆx∈[−1,1]nbe the optimal LP solution.
Finally, do the following O(1/η)times independently, and output the best cut X∗among them:
randomly round the fractional solution ˆxindependently for each vertex to get a cut X∈ {− 1,1}n;
namely, Pr[Xi= 1] = (1+ˆxi)/2andPr[Xi=−1] = (1−ˆxi)/2.
3.1.2 The Analysis
For a labeling x∈ {− 1,1}n, the neighborhood imbalance for vertex iis defined asP
jAijxj=
(Ax)i. This denotes the (signed) difference between the total weight of edges incident to ithat
appear and do not appear in the cut defined by the labeling x. The maximality of the optimal cut
x∗∈ {− 1,1}nensures that x∗
i·sign(( Ax∗)i)≤0for all i; else, switching xifrom 1to−1or
vice-versa increases the objective. Define r∗:=Ax∗as the vector of imbalances for the optimal cut.
Lemma 3.5. The vector ˆrsatisfies
E[∥ˆr−r∗∥1] :=E"nX
i=1|ˆri−r∗
i|#
≤OW
ε√
∆
+ 2ηW.
Proof. Observe that
E[Yi] =x∗
i·Pr[Yi=x∗
i]−x∗
i·Pr[Yi=−x∗
i] =x∗
i(1/2+ε)−x∗
i(1/2−ε) = 2 εx∗
i.
Define Z:=1
2εY. Then, E[Z] =x∗, and so E[AZ] =r∗.
First, we consider a ∆-narrow vertex i. Since ˆri= 0, we have |ˆri−r∗
i|=|r∗
i| ≤Wi. So summing
over all ∆-narrow vertices gives
X
i∈V<∆|ˆri−r∗
i| ≤X
i∈V<∆Wi≤ηW, (2)
since the graph is ∆-wide.
Now, we consider a ∆-wide vertex i. We have
|ˆri−r∗
i|=|(˜AZ)i−r∗
i| ≤ |E[(˜AZ)i]−r∗
i|+|(˜AZ)i−E[(˜AZ)i]|. (3)
5To bound the first term in the RHS of (3), recall that r∗
i=E[(AZ)i]. Thus,
|E[(˜AZ)i]−r∗
i|=|E[(˜AZ)i]−E[(AZ)i]|=⟨(˜A−A)i,E[Z]⟩.
SinceE[Z] =x∗∈ {− 1,1}n, we get
|E[(˜AZ)i]−r∗
i|= (˜A−A)i·x∗≤ ∥(˜A−A)i∥1∥x∗∥∞≤ηWi,
where in the last step, we used the fact that iis a∆-wide vertex.
Now, we bound the second term in the RHS of (3). Using Chebyshev’s inequality on the sum
(˜AZ)i=P
j˜AijZj, we get
Pr[|(˜AZ)i−E[(˜AZ)i]| ≥λi]≤var((˜AZ)i)
λ2
i.
Since the variables Zjare pairwise independent, the variance var((˜AZ)i) =P
j˜A2
ijvar(Zj). The
variance of each ZjisO(1/ε2). ForP
j˜A2
ij, we know
X
j∈[n]˜A2
ij=∥˜Ai∥2
2≤ ∥˜Ai∥1· ∥˜Ai∥∞.
Note that the weight of any edge in the ∆-suffix of iis at most Wi/∆. Therefore, by our definition of
˜A, we have ∥˜Ai∥∞≤Wi/∆. Since ˜Aij≤Aijfor all j∈[n], we also have ∥˜Ai∥1≤ ∥Ai∥1=Wi.
Applying these bounds, we get:P
j∈[n]˜A2
ij≤W2
i/∆.Therefore,
E[|(˜AZ)i−E[(˜AZ)i]|]≤q
var((˜AZ)i)≤O(Wi/(ε√
∆)).
Summing over all ∆-wide vertices, we get
EX
i∈V>∆|ˆri−r∗
i|
≤OW>∆
ε√
∆
+ηW>∆≤OW
ε√
∆
+ηW.
Combining with (2) for ∆-narrow vertices, we get
E[∥ˆri−r∗
i∥1]≤OW
ε√
∆
+ 2ηW.
Now using Markov’s inequality with Lemma 3.5, we get that setting ∆ = Ω(1 /(εε′)2)for any fixed
constant ε′>0ensures that we get a vector of empirical imbalances ˆrsatisfying
∥ˆr−r∗∥1≤(ε′+ 2η)W. (4)
with probability at least 0.99. (Since the 2ηW losses are deterministically bounded, we can use
Markov’s inequality only on the random variableP
i∈V>∆|(˜AZ)i−E[(˜AZ)i]|.) Hence, when the
event in (4) happens, the vector x∗is a feasible solution to LP (1).
Next, we need to analyze the quality of the cut produced by randomly rounding the solution of LP (1).
Recall that for the (unnormalized) Laplacian Land some x∈ {− 1,1}n, the cut value is
f(x) := 1/4· ⟨x, Lx⟩=1/4·(⟨x, Dx⟩ − ⟨x, Ax⟩) = 1/4·(W− ⟨x, Ax⟩). (5)
Lemma 3.6. For any ∆-wide graph, the algorithm from §3.1.1 outputs X∗∈ {− 1,1}nthat satisfies
f(X∗)≥f(x∗)−(2ε′+ 5η)W
with probability at least 0.98.
Proof. Recall that the cut X∗is the best among T:=O(1/η)independent roundings of cut ˆx.
Consider one of the roundings X, and write:
⟨X, AX ⟩=⟨ˆx,ˆr⟩+ (⟨ˆx, Aˆx⟩ − ⟨ˆx,ˆr⟩) + (⟨X, AX ⟩ − ⟨ˆx, Aˆx⟩). (6)
Let us first bound the expectation of each of the terms in the RHS of (6) separately.
6To bound the first term ⟨ˆx,ˆr⟩, note that given (4)(which happens with probability 0.99), the solution
x∗is feasible for the LP in (1). This means the optimal solution ˆxhas objective function value
⟨ˆr,ˆx⟩ ≤ ⟨ ˆr, x∗⟩=⟨r∗, x∗⟩+⟨ˆr−r∗, x∗⟩ ≤ ⟨x∗, Ax∗⟩+∥ˆr−r∗∥1∥x∗∥∞
≤ ⟨x∗, Ax∗⟩+ (ε′+ 2η)W. (7)
Next, we bound the second term (⟨ˆx, Aˆx⟩ − ⟨ˆx,ˆr⟩)by
∥ˆx∥∞∥Aˆx−ˆr∥1≤(ε′+ 2η)W, (8)
by feasibility of ˆxfor the LP in (1). Finally, we bound the third term (⟨X, AX ⟩ − ⟨ˆx, Aˆx⟩), this time
in expectation:
E[⟨X, AX ⟩]− ⟨ˆx, Aˆx⟩= 0. (9)
Chaining eqs. (7) to (9) for the various parts of (6), we get
E[⟨X, AX ⟩]≤ ⟨x∗, Ax∗⟩+ (2ε′+ 4η)W.
Moreover, using that ⟨X, AX ⟩ ∈[−W, W ], we get
Pr
⟨X, AX ⟩ ≥E[⟨X, AX ⟩] +ηW
= Pr
⟨X, AX ⟩+W≥E[⟨X, AX ⟩] + (1 + η)W
≤Pr
⟨X, AX ⟩+W≥(1 +η/2) 
E[⟨X, AX ⟩] +W
≤1/(1+η/2).
IfX∗is the cut with the smallest value of ⟨X, AX ⟩among all the independent roundings:
Pr
⟨X∗, AX∗⟩ ≤ ⟨x∗, Ax∗⟩+ (2ε′+ 5η)W
≥1−(1/(1+η/2))T≥0.99.
Substituting into the definition of f(·)completes the proof.
This proves Lemma 3.6, and hence also Theorem 3.4.
Deterministic Rounding. We observe that we can replace the repetition by a simple pipeage rounding
algorithm to round the fractional solution ˆxto an integer solution X∗without suffering any additional
loss. Indeed, viewing ⟨x, Ax⟩as a function of some xikeeping the remaining {x1, . . . , x n} \ {xi}
fixed gives us a linear function of xi(since the diagonals of Aare zero). Hence we can increase or
decrease the value of xito decrease ⟨x, Ax⟩untilxi∈ {− 1,1}. Iterating over the variables gives the
result. However, this does not change the result qualitatively.
3.2 Solving M AXCUTfor∆-narrow graphs
Next, we consider ∆-narrow graphs. We show:
Theorem 3.7. For any ∆∈N, there is a randomized algorithm for the MAXCUTproblem with an
(expected) approximation factor of αGW+˜Ω(η5/∆2)on any ∆-narrow graph.
For the case of ∆-narrow graphs, we do not use predictions; rather, we adapt an existing algorithm
for the MAXCUTproblem for low-degree graphs by Feige et al. [2002] and its refinement due to
Hsieh and Kothari [2022]. Note that any graph with maximum degree ∆is clearly ∆-narrow (even
when η= 1).
3.2.1 The ∆-narrow Algorithm
We show that Theorem 3.7 holds for the Feige, Karpinski, and Langberg (FKL) MAXCUTalgo-
rithm [Feige et al., 2002]. We briefly recall this algorithm first. Consider the MAXCUTSDP with
triangle inequalities:
max
vi∈Sn∀i∈[n]X
i<j∈[n]Ai,j·1− ⟨vi, vj⟩
2
s.t.∥avi−bvj∥2
2+∥bvj−cvk∥2
2≥ ∥avi−cvk∥2
2∀i, j, k∈[n], a, b, c ∈ {− 1,1}.
where Snis the unit sphere of ndimensions. Let ˆvbe an optimal solution to this SDP.
7Letgbe a random vector where each coordinate is sampled independently from a standard normal
distribution. We use random hyperplane rounding from the MAXCUTalgorithm of Goemans and
Williamson [1995] to round ˆvtoˆx∈ {− 1,1}nas follows: if ⟨ˆvi, g⟩>0, then ˆxi= 1; else, ˆxi=−1.
Now, define F={i∈[n] :⟨ˆvi, g⟩ ∈[−δ, δ]}for some δ= Θ(1 /((∆/η)p
log(∆ /η))). We
partition Ni:= [n]\{i}as follows: Bi:={j∈Ni\F: ˆxj= ˆxi}, andCi:={j∈Ni\F: ˆxj̸= ˆxi}
andDi:=Ni∩F. We define F′⊆Fas follows: i∈F′ifi∈Fandw(Bi)> w(Ci) +w(Di)
where w(S) :=P
j∈SAij. In the final output X∈ {− 1,1}n, we flip the vertices in F′, namely
Xi=−ˆxiifi∈F′, else Xi= ˆxi.
We now give an analysis for the FKL algorithm establishing Theorem 3.7.
The “local gain” for a vertex i∈Fis defined as ∆i:= (|Bi| −(|Di|+|Ci|))+, where z+=
max( z,0). We now state the following key lemmas:
Lemma 3.8. For any vertex i∈[n],Pr[i∈F] = Ω( δ).
Proof. This lemma immediately follows from [Hsieh and Kothari, 2022, Fact 3].
Letibe a∆-narrow vertex, and w∈Rnbe its weight vector ( wi=Aijfor all j∈[n]) so that
Wi=∥wi∥1. Let w′∈Rnbe the projection of wonto its top ∆coordinates. The narrowness of i
implies that ∥w′∥1≥η∥w∥1, which implies that
∥w∥2
2≥ ∥w′∥2
2≥∥w′∥2
1
∆≥η2∥w∥2
1
∆.
It turns out that the analysis of Hsieh and Kothari [2022] still holds under the above bound between
ℓ1andℓ2norms of weight vectors. So we have the following slight generalization of their Lemma 8.
Lemma 3.9 (extends Lemma 8 of Hsieh and Kothari [2022]) .There is a large enough constant
Csuch that for any d≥3andδ=1
Cd√logd, for any vertex iwhose weight vector wsatisfies
∥w∥2
1≤d∥w∥2
2, it holds that the expected local gain of a vertex isatisfies:
E[∆i|i∈F] = ΩWi
d√logd
.
Proof. In Hsieh and Kothari [2022], the only place where the degree bound dis used is ∥w∥2
1≤
d∥w∥2
2at the end of the proof of Lemma 7.
Proof of Theorem 3.7. Note that the value of the cut Xexceeds that of ˆxbyP
i∈F′∆i, i.e.,
E[⟨X, LX ⟩] =E[⟨ˆx, Lˆx⟩] +X
i∈[n]E[∆i|i∈F]·Pr[i∈F]
≥E[⟨ˆx, Lˆx⟩] +X
i:∆-narrowE[∆i|i∈F]·Pr[i∈F].
Let the approximation factor of the cut ˆxoutput by the Goemans-Williamson algorithm be denoted
αGWand let optbe the size of the maximum cut. Then,
E[⟨ˆx, Lˆx⟩]≥αGW·opt.
From Lemmas 3.8 and 3.9 with d= ∆/η2, we get
E[⟨X, LX ⟩]≥αGW·opt + Ω 
1
(∆/η2)p
log(∆ /η2)·X
i:∆-narrowWi
(∆/η2)p
log(∆ /η2)!
.
SinceP
i:∆-narrowWi≥ηW≥2η·opt, we get
E[⟨X, LX ⟩]≥(αGW+˜Ω(η5/∆2))·opt.
3.3 Wrapping up: Proof of Theorem 3.1
For∆-wide graphs, Theorem 3.4 returns a cut with value at least opt−(2η+ε′)Wwith probability
0.98. Since we can always find a cut of value αGW·opt, and opt≥W/2, this means the expected
cut value is at least
0.98·(1−6η−2ε′) + 0.02·αGW
opt. And for ∆-narrow graphs, Theorem 3.7
finds a cut with expected value
αGW+˜Ω(η5/∆2)
·opt.Moreover, recall that ∆ = O(1/(εε′)2).
Setting η, ε′to be suitably small universal constants gives us that both the above approximation
factors are at least αGW+˜Ω(ε4), which proves Theorem 3.1.
84 MaxCut in the Partial Prediction Model
We now consider the partial prediction model, where each vertex pairwise-independently reveals their
correct label with probability ε. Intuitively, this prediction model provides more information than the
noisy prediction model since all predictions are guaranteed to be correct. Indeed, this is reflected
in out first bound: we show that since an ε2fraction of the edges are induced by the vertices with
the given labels, it is easy to get an approximation ratio of almost αGW+ Ω(ε2). (We give details in
Appendix A.)
Theorem 4.1. Given noisy predictions with a rate of ε, there is a polynomial-time randomized
algorithm that obtains an (expected) approximation factor of αGW+ε2for the MAXCUTproblem
Although the Ω(ε2)advantage in this theorem is already better than ˜Ω(ε4)in Theorem 3.1, we ask if
can we do even better given the more informative predictions. Ideally, we could get an Ω(ε)-advantage
if the hyperplane rounding performs better than αGWfor the edges with only one endpoint’s label
revealed. One naive way to achieve this is to hope that the rounding preserves the marginals ; i.e.,
E[xi] =⟨v0, vi⟩for all i∈[n]. In that case, if we consider (i, j)where if vi=±v0, the probability
that(i, j)is cut is exactly their contribution to the SDP (1− ⟨vi, vj⟩).
Since the hyperplane rounding does not satisfy this property, we use the rounding scheme developed
by Raghavendra and Tan [2012] for max-bisection that has an approximation ratio αRT≈0.858
while preserving the marginals. The proof of this theorem is deferred to Appendix A.
Theorem 4.2 (Partial Predictions) .Given partial predictions with a rate of ε, there is a polynomial-
time randomized algorithm that obtains an (expected) approximation factor of αRT+ (1−αRT−
o(1))(2 ε−ε2)for the MAXCUTproblem.
5 2-CSPs in the Noisy Prediction Model
In this section, we go beyond the MAXCUTproblem and consider general 2-CSPs. In particular, we
extend Theorem 3.4 to this broader class of problems. Let us first define Max- 2-CSPs (Constraint
Satisfaction Problems). Each constraint is a predicate on two variables: e.g., AND, OR, Not-Equals,
or Xor. We are given a collection of such constraints (each on two variables), and the goal is to
find an binary assignment to the variables that satisfy the maximum number of constraints. (E.g.,
if the predicate is Not-Equals, and constraints form the edges of some graph, we get the MAXCUT
problem.)
Formally, for a multi-index α∈[n]2we denote by α(i)itsi-th index and by xαthe pair (xα(1), xα(2)).
For variables x1, . . . , x n,we then write χα(x)for the monomialQ
i∈αxi.Given a predicate P:
{−1,+1}2→ {0,1},an instance Iof the CSP(P) problem over variables x1, . . . , x nis a multi-set
of triplets (w, c, α )representing constraints of the form P(c1xα(1), c2xα(2)) = 1 where α∈[n]2is
the scope, c∈ {± 1}2is the negation pattern and w≥0is the weight of the constraint. For brevity we
often write P(c◦xα)in place of P(c1xα(1), c2xα(2)). We let W=P
(w,c,α )∈Iw .We can represent
the predicate Pas the multilinear polynomial of degree 2in indeterminates xα(1), xα(2),
P(c◦xα) =X
α′⊆αcα′·ˆp(α′)·χα′(x),
where ˆp(α′)is the coefficient in Pof the monomial χα′(x).Notice that this formulation does not
rule out predicates with the same multi-index but different negation patterns or multi-indices in which
an index appears multiple times. Given a predicate P, an instance Iof CSP(P) with mconstraints
andx∈ {± 1}n, we define
valI(x) :=1
WX
(w,c,α )∈Iw·P(c◦xα) and optI:= max
x∈{±1}nvalI(x).
For instance, MAXCUTon a graph G= ([n], E)can be captured in this framework where P(x, y) =
(1−xy)/2, each edge (i, j)∈[n]2yields a triplet (w, c, α )where w= 1,c= (1,1)andα= (i, j).
In the noisy prediction model for CSPs, for an instance Iof CSP(P), we assume there is some
fixed assigment x∗with value valI(x∗) = optI.The algorithm has access to a prediction vector
9Y∈ {± 1}nsuch that predictions yi’s are 2-wise independently correct with probability1+ε
2for
unknown bias ε .We let Z=Y
2ε.With a slight abuse of notation we also write P(c◦Zα)even
though Zis a rescaled boolean vector.
For a literal i∈[n]and an instance Iof CSP(P) we let Si:={(w, c, α )∈ I | α(1) = i}.As
in Section 3.1.1, we can define ∆-wide literals and instances. For an instance I,we partition the
constraints in Siinto two sets: the ∆-prefix foricomprises the ∆heaviest constraints in Si(breaking
ties arbitrarily), while the remaining constraints make up the ∆-suffix fori, which we denote by ˜Si.
We fix a parameter η∈(0,1/2). We let Wi=P
(w,c,α )∈Siwi.
Definition 5.1 (∆-Narrow/Wide) .A literal iis∆-wide if the total weight of its in its ∆-prefix is at
mostηWi, and so the weight of edges in its ∆-suffix is at least (1−η)Wi. Otherwise, the literal iis
∆-narrow . An instance Iof CSP(P) is ∆-wide ifP
i∈[n]
∆-wideWi≥(1−η)W.
We are now ready to state the main theorem of the section.
Theorem 5.2. LetP:{±1}2→ {0,1}be a predicate. Let ε′∈(0,1), η∈(0,1/2)and
∆≥O(1/(ε′·ε)2). There exists a polynomial-time randomized algorithm that, given a ∆-wideIin
CSP(P) and noisy predictions with bias ε, returns a vector ˆx∈ {± 1}nsatisfying
valI(x)≥optI−5η−O(ε′),
with probability at least 0.98.
The proof of Theorem 5.2 follows closely that of Theorem 3.4, and is deferred to Appendix B.2.
6 Closing Remarks
Our work suggests many directions for future research. One immediate question is to quantitatively
improve the exponent of εfor noisy predictions, and the constants. Here are some broader questions:
1.We assume that our noisy predictions are correct with probability equal to 1/2+ε; we
can easily extend to the case where each node has a prediction that is correct with some
probability 1/2+εi, and each εi∈Θ(ε). Can we extend to the case when we are only
guaranteed εi≥εfor every i?
2.For which other problems can we improve the performance of the state-of-the-art algorithms
using noisy predictions? As we showed, the ideas used for the ∆-wide case extend to more
general maximization problems on 2-CSPs with “high degree”, but can we extend the results
for the “low-degree” case where each variable does not have a high-enough degree to infer a
clear signal? Can we extend these to minimization versions of 2-CSPs?
3.What general lower bounds can we give for our prediction models? We feel that αGW+O(ε)
is a natural barrier. One “evidence” we have is the following integrality gap for the SDP used
in the partial information model; starting from a gap instance and an SDP solution exhibiting
opt≤αGW·sdpfor the standard SDP (without incorporating revealed information), given
labels for an εnvertices, our new SDP simply fixes the positions of the corresponding εn
vectors, but doing that from the given SDP solution decreases the SDP value by at most O(ε)
in expectation, which still yields opt≤(αGW+O(ε))sdp . (Note that you can replace the
SDP gap with any hypothetical gap instance for stronger relaxations.)
Given that the partial predictions model is easier than the noisy predictions model and our
entire algorithm for the partial model is based on this SDP, this might be considered as a
convincing lower bound, but it would be nicer to have more general lower bounds against
all polynomial-time algorithms.
4.Our models only assume pairwise independence between vertices: can we extend our results
to other ways of modeling correlations between the predictions? In addition to stochastic
predictions, can we incorporate geometric predictions (e.g., in random graph models where
the probability of edges depend on the proximity of the nodes)?
10Acknowledgments
We thank Ola Svensson for enjoyable discussions. AG was supported in part by NSF awards
CCF-2006953 and CCF-2224718, and by Google, Inc. EL was supported in part by NSF award
CCF-2236669 and by Google, Inc. DP was supported in part by NSF awards CCF-1955703 and
CCF-2329230, and by Google, Inc.
References
Online index for algorithms with predictions, 2023. https://algorithms-with-predictions.
github.io/ .
Nir Ailon, Anup Bhattacharya, Ragesh Jaiswal, and Amit Kumar. Approximate clustering with
same-cluster queries. In Anna R. Karlin, editor, 9th Innovations in Theoretical Computer Science
Conference, ITCS 2018, January 11-14, 2018, Cambridge, MA, USA , volume 94 of LIPIcs , pages
40:1–40:21. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2018.
Matteo Almanza, Flavio Chierichetti, Silvio Lattanzi, Alessandro Panconesi, and Giuseppe Re. Online
facility location with multiple advice. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.
Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information
Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021,
NeurIPS 2021, December 6-14, 2021, virtual , pages 4661–4673, 2021.
Keerti Anand, Rong Ge, and Debmalya Panigrahi. Customizing ML predictions for online algorithms.
InProceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July
2020, Virtual Event , volume 119 of Proceedings of Machine Learning Research , pages 303–313.
PMLR, 2020.
Keerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. Online algorithms with multiple
predictions. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and
Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July
2022, Baltimore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research ,
pages 582–598. PMLR, 2022.
Antonios Antoniadis, Christian Coester, Marek Eliás, Adam Polak, and Bertrand Simon. Online
metric algorithms with untrusted predictions. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of
Machine Learning Research , pages 345–355. PMLR, 2020a.
Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online matching
problems with machine learned advice. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual , 2020b.
Antonios Antoniadis, Christian Coester, Marek Eliás, Adam Polak, and Bertrand Simon. Mixing
predictions for online metric algorithms. In Andreas Krause, Emma Brunskill, Kyunghyun
Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference
on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of
Proceedings of Machine Learning Research , pages 969–983. PMLR, 2023.
Sanjeev Arora, David R. Karger, and Marek Karpinski. Polynomial time approximation schemes for
dense instances of np-hard problems. J. Comput. Syst. Sci. , 58(1):193–210, 1999.
Hassan Ashtiani, Shrinu Kushagra, and Shai Ben-David. Clustering with same-cluster queries.
Advances in neural information processing systems , 29, 2016.
Yossi Azar, Stefano Leonardi, and Noam Touitou. Flow time scheduling with uncertain processing
time. In Samir Khuller and Virginia Vassilevska Williams, editors, STOC ’21: 53rd Annual
ACM SIGACT Symposium on Theory of Computing, Virtual Event, Italy, June 21-25, 2021 , pages
1070–1080. ACM, 2021.
11Yossi Azar, Debmalya Panigrahi, and Noam Touitou. Online graph algorithms with predictions. In
Joseph (Seffi) Naor and Niv Buchbinder, editors, Proceedings of the 2022 ACM-SIAM Symposium
on Discrete Algorithms, SODA 2022, Virtual Conference / Alexandria, VA, USA, January 9 - 12,
2022 , pages 35–66. SIAM, 2022.
Étienne Bamas, Andreas Maggiori, Lars Rohwedder, and Ola Svensson. Learning augmented energy
minimization via speed scaling. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual , 2020a.
Étienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning aug-
mented algorithms. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual , 2020b.
Evripidis Bampis, Bruno Escoffier, and Michalis Xefteris. Parsimonious learning-augmented approx-
imations for dense instances of NP-hard problems, 2024.
Nikhil Bansal, Christian Coester, Ravi Kumar, Manish Purohit, and Erik Vee. Learning-augmented
weighted paging. In Joseph (Seffi) Naor and Niv Buchbinder, editors, Proceedings of the 2022
ACM-SIAM Symposium on Discrete Algorithms, SODA 2022, Virtual Conference / Alexandria, VA,
USA, January 9 - 12, 2022 , pages 67–89. SIAM, 2022.
Giulia Bernardini, Alexander Lindermayr, Alberto Marchetti-Spaccamela, Nicole Megow, Leen
Stougie, and Michelle Sweering. A universal error measure for input predictions applied to online
graph problems. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and
A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on
Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November
28 - December 9, 2022 , 2022.
Ilan Reuven Cohen and Debmalya Panigrahi. A general framework for learning-augmented online
allocation. In Kousha Etessami, Uriel Feige, and Gabriele Puppis, editors, 50th International
Colloquium on Automata, Languages, and Programming, ICALP 2023, July 10-14, 2023, Pader-
born, Germany , volume 261 of LIPIcs , pages 43:1–43:21. Schloss Dagstuhl - Leibniz-Zentrum für
Informatik, 2023.
Alberto Del Pia, Mingchen Ma, and Christos Tzamos. Clustering with queries under semi-random
noise. In Conference on Learning Theory , pages 5278–5313. PMLR, 2022.
Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster
matchings via learned duals. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin,
Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing
Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021,
December 6-14, 2021, virtual , pages 10393–10406, 2021.
Paul Dütting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. Secretaries with advice. In
Péter Biró, Shuchi Chawla, and Federico Echenique, editors, EC ’21: The 22nd ACM Conference
on Economics and Computation, Budapest, Hungary, July 18-23, 2021 , pages 409–429. ACM,
2021.
Jon C. Ergun, Zhili Feng, Sandeep Silwal, David P. Woodruff, and Samson Zhou. Learning-augmented
$k$-means clustering. In The Tenth International Conference on Learning Representations, ICLR
2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022.
Uriel Feige, Marek Karpinski, and Michael Langberg. Improved approximation of max-cut on graphs
of bounded degree. J. Algorithms , 43(2):201–219, 2002.
Dimitris Fotakis, Evangelia Gergatsouli, Themis Gouleakis, and Nikolas Patris. Learning augmented
online facility location. CoRR , abs/2107.08277, 2021. URL https://arxiv.org/abs/2107.
08277 .
12Buddhima Gamlath, Silvio Lattanzi, Ashkan Norouzi-Fard, and Ola Svensson. Approximate cluster
recovery from noisy labels. In Po-Ling Loh and Maxim Raginsky, editors, Conference on Learning
Theory, 2-5 July 2022, London, UK , volume 178 of Proceedings of Machine Learning Research ,
pages 1463–1509. PMLR, 2022.
Suprovat Ghoshal, Konstantin Makarychev, and Yury Makarychev. Constraint satisfaction problems
with advice. CoRR , abs/2403.02212, 2024. doi: 10.48550/ARXIV .2403.02212. URL https:
//doi.org/10.48550/arXiv.2403.02212 .
Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut
and satisfiability problems using semidefinite programming. J. ACM , 42(6):1115–1145, 1995.
Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice.
In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA ,
volume 97 of Proceedings of Machine Learning Research , pages 2319–2327. PMLR, 2019.
Anupam Gupta, Debmalya Panigrahi, Bernardo Subercaseaux, and Kevin Sun. Augmenting online
algorithms with ε-accurate predictions. In NeurIPS , Dec 2022.
Jun-Ting Hsieh and Pravesh K. Kothari. Approximating max-cut on bounded degree graphs: Tighter
analysis of the FKL algorithm. CoRR , abs/2206.09204, 2022.
Shaofeng H.-C. Jiang, Erzhi Liu, You Lyu, Zhihao Gavin Tang, and Yubo Zhang. Online facility
location with predictions. In The Tenth International Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022a.
Zhihao Jiang, Debmalya Panigrahi, and Kevin Sun. Online algorithms for weighted paging with
predictions. ACM Trans. Algorithms , 18(4):39:1–39:27, 2022b.
Kasper Green Larsen, Michael Mitzenmacher, and Charalampos E. Tsourakakis. Clustering with a
faulty oracle. In Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten van Steen, editors, WWW
’20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 , pages 2831–2834. ACM /
IW3C2, 2020.
Alexandra Anna Lassota, Alexander Lindermayr, Nicole Megow, and Jens Schlöter. Minimalistic
predictions to schedule jobs with online precedence constraints. In Andreas Krause, Emma
Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors,
International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,
USA, volume 202 of Proceedings of Machine Learning Research , pages 18563–18583. PMLR,
2023.
Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online scheduling
via learned weights. In Shuchi Chawla, editor, Proceedings of the 2020 ACM-SIAM Symposium on
Discrete Algorithms, SODA 2020, Salt Lake City, UT, USA, January 5-8, 2020 , pages 1859–1877.
SIAM, 2020.
Thomas Lavastida, Benjamin Moseley, R. Ravi, and Chenyang Xu. Learnable and instance-robust
predictions for online matching, flows and load balancing. In Petra Mutzel, Rasmus Pagh, and
Grzegorz Herman, editors, 29th Annual European Symposium on Algorithms, ESA 2021, September
6-8, 2021, Lisbon, Portugal (Virtual Conference) , volume 204 of LIPIcs , pages 59:1–59:17. Schloss
Dagstuhl - Leibniz-Zentrum für Informatik, 2021.
Alexander Lindermayr, Nicole Megow, and Martin Rapp. Speed-oblivious online scheduling: Know-
ing (precise) speeds is not necessary. In Andreas Krause, Emma Brunskill, Kyunghyun Cho,
Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Ma-
chine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of Proceedings
of Machine Learning Research , pages 21312–21334. PMLR, 2023.
Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. J.
ACM , 68(4):24:1–24:25, 2021.
Arya Mazumdar and Barna Saha. Clustering with noisy queries. Advances in Neural Information
Processing Systems , 30, 2017.
13Michael Mitzenmacher. Scheduling with predictions and the price of misprediction. In Thomas
Vidick, editor, 11th Innovations in Theoretical Computer Science Conference, ITCS 2020, January
12-14, 2020, Seattle, Washington, USA , volume 151 of LIPIcs , pages 14:1–14:18. Schloss Dagstuhl
- Leibniz-Zentrum für Informatik, 2020.
Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. In Tim Roughgarden,
editor, Beyond the Worst-Case Analysis of Algorithms , pages 646–662. Cambridge University
Press, 2020.
Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. Commun. ACM , 65(7):
33–35, 2022.
Ryan O’Donnell. Analysis of boolean functions . Cambridge University Press, 2014.
Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ML predictions.
In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi,
and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montréal, Canada , pages 9684–9693, 2018.
Prasad Raghavendra and Ning Tan. Approximating CSPs with global cardinality constraints using
sdp hierarchies. In Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete
Algorithms , pages 373–387. SIAM, 2012.
Tim Roughgarden, editor. Beyond the Worst-Case Analysis of Algorithms . Cambridge University
Press, 2020.
Alexander Wei. Better and simpler learning-augmented online caching. In Jaroslaw Byrka and Raghu
Meka, editors, Approximation, Randomization, and Combinatorial Optimization. Algorithms and
Techniques, APPROX/RANDOM 2020, August 17-19, 2020, Virtual Conference , volume 176 of
LIPIcs , pages 60:1–60:17. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2020.
Alexander Wei and Fred Zhang. Optimal robustness-consistency trade-offs for learning-augmented
online algorithms. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual , 2020.
14Appendix
A Missing proofs for M AXCUTin the partial prediction model
Proof of Theorem 4.1. Given a graph G= (V, E)with the optimal cut (A∗, B∗)that cuts E∗=
E∩E(A∗, B∗), letSbe the set of vertices whose label is given, and let A=A∗∪S,B=B∗∪S.
Consider the following M AXCUTSDP that fixes the vertices with the revealed labels.
max
vi∈Sn∀i∈[n]X
i,j∈[n]Ai,j(1− ⟨vi, vj)⟩
2s.t. v i=v0∀i∈Aandvi=−v0∀i∈B.
Note that this is still a valid relaxation so the optimal SDP value sdpis at least opt. For each edge
e∈E∗,e∈E(A, B)with probability ε2; in other words, both of its endpoints will reveal their
labels. Let τdenotes the total weight of such edges, so that E[τ] =ε2opt. Note that sdp≥optfor
every partial prediction.
Perform the standard hyperplane rounding. For each e∈E∗∩E(A, B), the rounding will always
cute. For all other edges, we have an approximation ratio of αGW. Therefore, the expected weight of
the cut edges is at least
E[τW+αGW(sdp−τW)]≥ε2opt + αGW(1−ε2)opt = ( αGW+ (1−αGW)ε2)·opt.
Proof of Theorem 4.2. Given a graph G= (V, E)with the optimal cut (A∗, B∗)that cuts E∗=
E∩E(A∗, B∗), letSbe the set of vertices whose label is given, and let A=A∗∪S,B=B∗∪S.
LetE′be the set of the edges that are incident on AorB. Each edge cut in the optimal solution
will be in E′with probability 2ε−ε2. Let τbe the total weight of the edges in E∗∩E′so that
E[τ] = (2 ε−ε2)opt. Guess the value of τ(up to a o(1)multiplicative error that we will ignore in
the proof), and consider the following MAXCUTSDP that fixes the vertices with the revealed labels
and requires a large SDP contribution from E′.
max
vi∈Sn∀i∈[n]X
i,j∈[n]Ai,j(1− ⟨vi, vj)⟩
2
s.t. v i=v0 ∀i∈A
vi=−v0 ∀i∈B
X
(i,j)∈E′Ai,j(1− ⟨vi, vj)⟩
2≥τ.
Given the correctly guessed value of τ, the optimal solution is still feasible for the above SDP, so
sdp≥opt. We use Raghavendra and Tan [2012]’s rounding algorithm, which is briefly recalled
below.
•For each i∈[n], define µi∈[−1,+1]andwi∈Rnsuch that vi=µiv0+wiandwi⊥v0.
Letwi=wi/∥wi∥. (wi= 0if and only if vi=±v0. Then define wi= 0.)
•Pick a random Gaussian vector gorthogonal to v0. Letξi:=⟨g,wi⟩. Note that each ξiis a
standard Gaussian.
• Let the threshold ti:= Φ−1(µi/2 + 1 /2)where Φis the CDF of a standard Gaussian.
• Ifξi≤ti, setxi= 1and otherwise set xi=−1.
Raghavendra and Tan showed that this rounding achieves an (αRT≈0.858) -approximation for every
edge.
Consider an edge (i, j)∈E′and without loss of generality, assume i∈B, which implies that
vi=−v0. The contribution of this edge to the SDP objective is µj/2 + 1 /2. Note that Pr[xj= 1] is
exactly µj/2 + 1/2andE[xj] = (µj/2 + 1/2)−(1/2−µj/2) = µj. So, we get a 1-approximation
from this edge. Since other edges still have an αRT-approximation, the total expected weight of the
edges cut is at least
E[τ+αRT(sdp−τ)]≥(2ε−ε2)opt+ αRT(1−(2ε−ε2))opt = αRT·opt+(1 −αRT)(2ε−ε2)opt.
Hence the proof of Theorem 4.2.
15B Missing details for 2-CSPs
The goal of this section is to establish Theorem 5.2 for dense instances of 2-CSPs.
B.1 The 2-CSP Algorithm for Dense Instances
First observe that we may assume without loss of generality that each (w, c, α )appears exactly twice
inI.This is convenient so that for all x∈ {± 1}n,valI(x) =P
i∈[n]P
(w,c,α )∈Siw·P(c◦xα).
With a slight abuse of notation, for all (w, c, α )∈Si,we let
P(c◦(xi·Zα\i)) :=X
α′⊆αs.t.
α′(1)=iˆpα′cα′xi·χα′\α′(1)(Z) +X
α′⊆αs.t.
α′(1)̸=iˆpα′cα′χα′(Z),
and
P(c◦xα\i) :=X
α′⊆αs.t.
α′(1)=iˆpα′cα′χα′\α′(1)(x) +X
α′⊆αs.t.
α′(1)̸=iˆpα′cα′χα′(x),
We further define ˜Si⊆Sito be subset of constraints in Sithat are not part of the ∆-prefix of i .
We can now state the algorithm behind Theorem 5.2, which amounts to the following two steps.
1. Solve the linear program
max
x∈[−1,+1]nX
i∈[n]
∆-wideX
(w,c,α )∈˜SiwP(c◦(xi·Zα\i))
subject to
X
i∈[n]
∆-narrowX
(w,c,α )∈SiwP(c◦xα\i)+X
i∈[n]
∆-wideX
(w,c,α )∈Si\˜SiwP(c◦xα\i)
+X
i∈[n]
∆-wideX
(w,c,α )∈˜Siw
P(c◦xα\i)−P(c◦Zα\i)≤C(ε′+ 2η)W (10)
for some large enough absolute constant C >0.Letˆx∈[−1,+1]nbe the found optimal
solution.
2.Repeat O(1/η)times independently and output the best assignment X∗:independently for
eachi∈[n]setXi= 1with probability (1 + ˆxi)/2andXi=−1otherwise.
The LP above generalize the one in eq. (1), which comes as a special case where ˆpα′= 0 for all
α′⊂α∈[n]2.Indeed, since predicates contain only two literals, the program is linear.
B.2 The Analysis of the 2-CSP Algorithm
We obtain here the proof of Theorem 5.2.
Feasibility of the best assignment As in Lemma 3.5, we first prove that, in expectation over the
prediction Y,x∗is a feasible solution to the program.
Lemma B.1. Consider the settings of Theorem 5.2. Then
EX
i∈[n]
∆-narrowX
(w,c,α )∈SiwP(c◦x∗α\i)+X
i∈[n]
∆-wideX
(w,c,α )∈Si\˜SiwP(c◦x∗α\i)
+X
i∈[n]
∆-wideX
(w,c,α )∈˜Siw
P(c◦x∗α\i)−P(c◦Zα\i)≤W(2η+O(1/ε√
∆)).
16Proof. First, by definition of ∆-wide instance,
X
i∈[n]
∆-narrowX
(w,c,α )∈SiwP(c◦x∗α\i)≤ηW .
Second, by definition for any ∆-wide vertex i,
X
(w,c,α )∈Si\˜SiwP(c◦x∗α\i)≤ηWi.
Hence it remains to show
EX
i∈[n]
∆-wideX
(w,c,α )∈˜Siw
P(c◦x∗α\i)−P(c◦Zα\i)≤O(W/(ε√
∆).
Now, recall that E[Yi] = 2 εx∗
iand thus E[Z] =x∗. So for any (c, α)∈ I,E[P(c◦Zα)] =
E[P(c◦x∗α)]by pair-wise independence of the predictions. Thus it suffices to study, for each ∆-wide
i,varP
(w,c,α )∈SiwP(c◦Zα\i)
.To this end, notice that for any α, α′∈Siwithα∩α′={i}
it holds
Eh
P(c◦Yα\i)P(c◦Yα′\i)i
=Eh
P(c◦Yα\i)i
Eh
P(c◦Yα′\i)i
.
Moreover, since |α|= 2,there are at most 4distinct negation patterns. Therefore, by the AM-GM
inequality
var
X
(w,c,α )∈˜SiwP(c◦Zα\i)
≤X
(w,c,α )∈˜SiO(w2)var
P(c◦Zα\i)
≤X
(w,c,α )∈˜SiOw2
ε2
where we used the fact that entries of Zare bounded by 1/εand the coefficients of a boolean predicate
are bounded by 1(by Parseval’s Theorem, see O’Donnell [2014]). By construction of ˜Si,each
(w, c, α )∈˜Simust satisfy w≤Wi/∆.Using Holder’s inequality
var
X
(w,c,α )∈˜SiwP(c◦Zα\i)
≤OW2
i
∆·ε2
.
We can use this bound on the variance in combination with Chebishev’s inequality to obtain, for
λ >0,
P
X
(w,c,α )∈Siw
P(c◦x∗α\i)−P(c◦Zα\i)≥λ
≤OW2
i
ε2·∆·λ2
.
Letλ:=O(Wi/(ε√
∆)).A peeling argument now completes the proof:
E
X
(w,c,α )∈Siw
P(c◦x∗α\i)−P(c◦Zα\i)

≤λ+X
t≥02t+1λ·P
X
(w,c,α )∈Siw
P(c◦x∗α\i)−P(c◦Zα\i)≥2tλ
≤O(λ).
17Analysis of the algorithm We can use Lemma B.1 to obtain our main theorem for CSPs.
Proof of Theorem 5.2. We follow closely the proof of Lemma 3.6. Consider one of the assignments
X∈ {± 1}nfound in the second step of the algorithm. Recall ˆx∈[−1,+1]ndenotes the optimal
fractional solution found by the algorithm. We may rewrite for each ∆-wide iand(w, c, α )∈˜Si
X
i∈[n]
∆-wideX
(w,c,α )∈˜SiwP(c◦Xα) =X
i∈[n]
∆-wideX
(w,c,α )∈˜Siwh
P(c◦(ˆxi·Zα\i))
+P(c◦Xα)−P(c◦ˆxα)
+P(c◦ˆxα)−P(c◦(ˆxi·Zα\i))i
. (11)
We bound each term in Equation (11) separately. First, notice that by Markov’s inequality and
Lemma B.1, with probability 0.99, x∗is a feasible solution to the LP. Conditioning on this event E
X
i∈[n]
∆-wideX
(w,c,α )∈˜SiwP(c◦(ˆxi·Zα\i))≥X
i∈[n]
∆-wideX
(w,c,α )∈˜SiwP(c◦(x∗
i·Zα\i))
=X
i∈[n]
∆-wideX
(w,c,α )∈˜SiwP(c◦x∗α)
+X
i∈[n]
∆-wideX
(w,c,α )∈˜Siw
P(c◦(x∗
i·Zα\i))−P(c◦x∗α)
By Holder’s inequality and the fact that x∗is feasible, for ∆-wide i,
X
i∈[n]
∆-wideX
(w,c,α )∈˜Siw
P(c◦(x∗
i·Zα\i))−P(c◦x∗α)
≤X
i∈[n]
∆-wideX
(w,c,α )∈˜Siw
P(c◦Zα\i)−P(c◦x∗α\i)≤(O(ε′) + 2η)W .
Since by construction ˆxis feasible, another application of Holder’s inequality also yields the following
bound on the third term,
X
i∈[n]
∆-wideX
(w,c,α )∈˜Siw
P(c◦(ˆxi·Zα\i))−P(c◦ˆxα)
≤(O(ε′) + 2η)W .
For the second term in Equation (11), by construction of Xwe have E[P(c◦Xα)|E] =P(c◦ˆxα).
Combining the three bounds, we get that
optI≥E
1
WX
i∈[n]
∆-wideX
(w,c,α )∈˜SiwP(c◦Xα)E
≥optI−(O(ε′) + 4η).
Applying Markov’s inequality on the random variable optI−1
WP
i∈[n]
∆-wideP
(w,c,α )∈˜SiwP(c◦Xα),
we get
P
1
WX
i∈[n]
∆-wideX
(w,c,α )∈˜SiwP(c◦Xα)≤optI−(O(ε′) + 5η)E
≤1
1 +η
The theorem follows since we sample O(1/η)independent assignments Xand pick the best.
18NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Theorems 3.1, 4.1 and 5.2, which are proved in the paper, formalize the claims
in the abstract.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The models’ assumptions are accurately described (see Section 2).
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All assumptions are clearly stated. All theorems are formally proved in the
main body or in the appendix.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: The paper does not include experiments.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: The paper does not include experiments requiring code.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: The paper does not include experiments.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
9.Code Of Ethics
19Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This is a theoretical work; we do not foresee any societal impact of this work.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
20