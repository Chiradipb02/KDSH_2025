Dynamic Conditional Optimal Transport through
Simulation-Free Flows
Gavin Kerrigan
Department of Computer Science
University of California, Irvine
gavin.k@uci.eduGiosue Migliorini
Department of Statistics
University of California, Irvine
gmiglior@uci.edu
Padhraic Smyth
Department of Computer Science
University of California, Irvine
smyth@ics.uci.edu
Abstract
We study the geometry of conditional optimal transport (COT) and prove a dynamic
formulation which generalizes the Benamou-Brenier Theorem. Equipped with
these tools, we propose a simulation-free flow-based method for conditional gener-
ative modeling. Our method couples an arbitrary source distribution to a specified
target distribution through a triangular COT plan, and a conditional generative
model is obtained by approximating the geodesic path of measures induced by
this COT plan. Our theory and methods are applicable in infinite-dimensional
settings, making them well suited for a wide class of Bayesian inverse problems.
Empirically, we demonstrate that our method is competitive on several challenging
conditional generation tasks, including an infinite-dimensional inverse problem.
1 Introduction
Many fundamental tasks in machine learning and statistics may be posed as modeling a conditional
distribution ν(u|y)where obtaining an analytical representation of ν(u|y)is impractical. While
sampling-based approaches such as Markov Chain Monte Carlo (MCMC) methods are useful, they
suffer from several limitations. First, MCMC requires numerous likelihood evaluations, rendering it
prohibitively expensive in scientific and engineering applications where the likelihood is determined
by an expensive numerical simulator. Second, MCMC must be run anew for every observation y,
which is impractical in applications such as Bayesian inverse problems [Dashti and Stuart, 2013]
and generative modeling [Mirza and Osindero, 2014]. These limitations motivate the need for a
likelihood-free [Cranmer et al., 2020] and amortized [Amos et al., 2023] approach. While methods
like ABC [Beaumont, 2010] and variational inference [Blei et al., 2017] partially address these
challenges, they are difficult to scale to high dimensions or have limited flexibility.
Recently, generative models such as normalizing flows [Papamakarios et al., 2019, 2021], GANs
[Ramesh et al., 2022], and diffusion models [Sharrock et al., 2022] have shown promise in amortized
and likelihood-free inference. These models may be viewed in the framework of measure transport
[Baptista et al., 2020], where samples u∼η(u)from a tractable source distribution are transformed
by a mapping T(y, u)such that the transformed samples are approximately distributed as ν(u|y).
One way to achieve this is through triangular mappings [Baptista et al., 2020, Spantini et al., 2022],
where a joint source distribution η(y, u)is transformed by a mapping of the form T: (y, u)7→
(TY(y), TU(y, u)). Under suitable assumptions, if Ttransforms the source η(y, u)into the target
ν(y, u), then TU(y,−)couples the conditionals η(u|y)andν(u|y).
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Typically, such a map Tis not unique [Wang et al., 2023], and a natural idea is thus to regularize
the transport and search for an admissible mapping that is in some sense optimal. In other words,
learning a conditional sampler may be phrased as finding a conditional optimal transport (COT)
map. While there exists some work on learning COT maps, these approaches often rely on a difficult
adversarial optimization problem [Baptista et al., 2020, Hosseini et al., 2023, Bunne et al., 2022, Ray
et al., 2022] or simulating from the model during training [Baptista et al., 2023, Wang et al., 2023].
In this work, we propose a conditional generative model for likelihood-free inference based on a
dynamic formulation of conditional optimal transport. Specifically, our contributions are as follows:
1.We develop a general theoretical framework for dynamic conditional optimal transport
in separable Hilbert spaces. Our framework is applicable in infinite-dimensional spaces,
enabling applications in function space Bayesian inference. In Section 4, we study the
conditional Wasserstein space Pµ
p(Y×U)and show that this space admits constant speed
geodesics between any two measures. In Section 5, we characterize the absolutely continuous
curves of measures in Pµ
p(Y×U)via the continuity equation and triangular vector fields. As
a consequence, we obtain conditional generalizations of the McCann interpolants [McCann,
1997] and the Benamou-Brenier Theorem [Benamou and Brenier, 2000].
2.In Section 6, we propose COT flow matching (COT-FM), a simulation-free flow-based
model for conditional generation. This model directly leverages our theoretical framework,
where we learn to model a path of measures interpolating between an arbitrary source and
target distribution via a geodesic in the conditional Wasserstein space.
3.In Section 7, we demonstrate our method on several challenging conditional generation
tasks. We apply our method to two Bayesian inverse problems – one arising from the
Lotka-V olterra dynamical system, and an infinite-dimensional problem arising from the
Darcy Flow PDE. Our method shows competitive performance against recent COT methods.
2 Related Work
Conditional Optimal Transport. Conditional Optimal Transport (COT) remains relatively under-
explored in both machine learning and related fields. Recent approaches learn static COT maps via
input convex networks [Bunne et al., 2022, Wang et al., 2023] or normalizing flows [Wang et al.,
2023]. In addition, there have been a number of heuristic approaches to conditional simulation
through W-GANs [Sajjadi et al., 2017, Adler and Öktem, 2018, Kim et al., 2022, 2023], for which
Chemseddine et al. [2023] provide a rigorous basis. Closely related to our work are those which
employ triangular plans [Carlier et al., 2016, Trigila and Tabak, 2016], which have been modeled
through GANs in Euclidean spaces [Baptista et al., 2020] and function spaces [Hosseini et al., 2023].
In contrast, we use a novel dynamic formulation of COT, which we model through a generalization of
flow matching [Lipman et al., 2022, Albergo et al., 2023b, Liu et al., 2022]. This allows us to use
flexible architectures while avoiding the difficulties of training GANs [Arora et al., 2018].
Simulation-Free Continuous Normalizing Flows. Flow matching [Lipman et al., 2022] (and
the closely related stochastic interpolants [Albergo et al., 2023a] and rectified flows [Liu et al.,
2022]) are a class of methods for building continuous-time normalizing flows in a simulation-free
manner. Notably, these works do not approximate an optimal transport between the source and target
measures. Pooladian et al. [2023] and Tong et al. [2023] propose instead to couple the source and
target distributions via optimal transport, leading to marginally optimal paths. In this work, we study
an extension of these techniques for conditional generation.
While some works [Davtyan et al., 2023, Gebhard et al., 2023, Isobe et al., 2024, Wildberger et al.,
2024] have applied flow matching for conditional generation, these approaches do not employ COT.
Notably, these approaches are limited to the finite-dimensional setting, whereas our method adds to
the growing literature on function-space generative models [Hosseini et al., 2023, Kerrigan et al.,
2023, 2024, Lim et al., 2023, Franzese et al., 2024]. Concurrent work by Chemseddine et al. [2024]
develops the foundation of dynamic COT with applications to flow matching, and concurrent work by
Barboni et al. [2024] develop the theory of dynamic COT for the purposes of studying infinitely deep
ResNets. However, these works focus on the Euclidean setting, whereas our methods are applicable
in general separable Hilbert spaces.
23 Background and Notation
LetX, X′represent arbitrary separable Hilbert spaces, equipped with the Borel σ-algebra. We use
P(X)to represent the space of Borel probability measures on X, andPp(X)⊆P(X)to represent
the subspace of measures having finite pth moment. If η∈P(X)is a probability measure on Xand
T:X→X′is measurable, then the pushforward measure T#η(−) =η(T−1(−))is a probability
measure on X′. Maps of the form e.g. πX:X×X′→Xrepresent the canonical projection.
We assume that we have two separable Hilbert spaces of interest. The first, Y, is a space of
observations, and the second, U, is a space of unknowns. These spaces may be of infinite dimensions,
but a case of practical interest is when YandUare finite dimensional Euclidean spaces. We will
consider the product space Y×U, equipped with the canonical inner product obtained via the sum
of the inner products on YandU, under which the space Y×Uis also a separable Hilbert space.
Letη∈P(Y×U)be a joint probability measure. The measures πY
#η∈P(Y)andπU
#η∈P(U)
obtained via projection are the marginals ofη. We use ηy∈P(U)to represent the measure obtained
by conditioning ηon the value y∈Y. By the disintegration theorem [Bogachev and Ruas, 2007,
Chapter 10], such conditional measures exist and are essentially unique, in the sense that there exists
a Borel set E⊆YwithπY
#η(E) = 0 , and the ηyare unique for y /∈E.
3.1 Static Conditional Optimal Transport
In conditional optimal transport, we are given a target measure ν∈P(Y×U)and some source
measure η∈P(U). We seek a transport map T:Y×U→Usuch that, for any given y∈Y, the
mapping T(y,−) :U→Utransforms the source distribution ηinto the conditional distribution νy,
i.e.,T(y,−)#η=νy. In a sense, Tcan be thought of as a collection of transport maps indexed by
y∈Y. If such a Twere available, by drawing samples u0∼ηand transforming them, one would
obtain samples T(y, u0)∼νy. Solving this transport problem for each fixed yis expensive at best,
or impossible when only has a single (or no) samples (y, u)∼νfor any given y. Thus, one must
leverage information across different observations y. To that end, recent work has focused on the
notion of triangular mappings T:Y×U→Y×U[Hosseini et al., 2023, Baptista et al., 2020] of the
form T(y, u) = (TY(y), TU(TY(y), u))for some TY:Y→YandTU:Y×U→U. Triangular
mappings are of interest as they allow us to obtain conditional couplings from joint couplings.
Proposition 1 (Theorem 2.4 [Baptista et al., 2020], Prop. 2.3 [Hosseini et al., 2023])
Suppose η, ν∈P(Y×U)andT:Y×U→Y×Uis triangular. If T#η=ν, then
TU(TY(y),−)#ηy=νTY(y)forπY
#η-almost every y.
In many scenarios of practical interest, the source measure ηand the target measure νhave the same
Y-marginals. We will henceforth make this assumption, and use µ=πY
#η=πY
#νto represent
this marginal. In this case, we may take TYto be the identity mapping, so that the conclusion of
Proposition 1 simplifies to TU(y,−)#ηy=νyforµ-almost every y. We note that in situations where
such an assumption does not hold, one may simply preprocess the source measure ηvia an invertible
mapping TYsatisfying [TY]#[πY
#η] =πY
#ν[Hosseini et al., 2023, Prop 3.2].
Given a source and target measures η, ν∈Pµ(Y×U)and a cost function c: (Y×U)2→R, the
conditional Monge problem seeks to find a triangular mapping solving
inf
TZ
Y×Uc(y, u, T (y, u)) dη(y, u)|T#η=ν, T: (y, u)7→(y, TU(y, u))
. (1)
The conditional Monge problem also admits a relaxation under which one only considers couplings
whose Y-components are almost surely equal. To that end, for η, ν∈Pµ
p(Y×U)we define the set of
triangular couplings ΠY(η, ν)to be the couplings of ηandνthat almost surely fix the Y-components,
ΠY(η, ν) =n
γ∈P
(Y×U)2
|π1,2
#γ=η, π3,4
#γ=ν, π1,3
#= (I, I)#µo
. (2)
In other words, a triangular coupling γ∈ΠY(η, ν)has samples (y0, u0, y1, u1)∼γsuch that
y0=y1almost surely. The conditional Kantorovich problem seeks a triangular coupling solving
inf
γ(Z
(Y×U)2c(y0, u0, y1, u1) dγ(y0, u0, y1, u1)|γ∈ΠY(η, ν))
. (3)
3Hosseini et al. [2023] prove the existence of minimizers to the conditional Kantorovich and Monge
problems under very general assumptions. Moreover, optimal couplings to the conditional Kan-
torovich problem induce optimal couplings for µ-almost every conditional measure. We refer to
Appendix B and Hosseini et al. [2023] for further details.
4 Conditional Wasserstein Space
Motivated by our discussion on triangular transport maps, we introduce the conditional Wasserstein
spaces, consisting of joint measures with finite pth moments and having fixed Y-marginals µ.
Interestingly, Gigli [2008, Chapter 4] studies the same space for the purposes of constructing
geometric tangent spaces in the usual Wasserstein space.
Definition 1 (Conditional Wasserstein Space)
Suppose µ∈P(Y)is given and 1≤p <∞. The conditional p-Wasserstein space is
Pµ
p(Y×U) =
γ∈Pp(Y×U)|πY
#γ=µ	
. (4)
We now equip Pµ
p(Y×U)with a metric Wµ
p, the conditional Wasserstein distance. Intuitively,
the conditional Wasserstein distance measures the usual Wasserstein distance between all of the
conditional distributions in expectation under the fixed Y-marginal µ.
Definition 2 (Conditional p-Wasserstein Distance)
Suppose η, ν∈Pµ
p(Y×U)and1≤p <∞. The function Wµ
p:Pµ
p(Y×U)×Pµ
p(Y×U)→R,
Wµ
p(η, ν) = 
Ey∼µ
Wp
p(ηy, νy)1/p=Z
YWp
p(ηy, νy) dµ(y)1/p
(5)
is the conditional p-Wasserstein distance. Wpis the usual Wasserstein distance for measures on U.
By Jensen’s inequality we have Wµ
p(η, ν)≥Ey∼µ[Wp(ηy, νy)]. In the following, we show that the
conditional Wasserstein distance is a well-defined metric as well as a few other metric properties.
Proposition 2 (Some Properties of Wµ
p)
Let1≤p <∞.
(a)Wµ
pis well-defined, finite, and equals the minimal conditional Kantorovich cost.
(b)Wµ
pis a metric on the space Pµ
p(Y×U).
(c) There does not exist C >0such that Wµ
p(η, ν)≤CWp(η, ν)for all η, ν∈Pµ
p(Y×U).
(d) For all η, ν∈Pµ
p(Y×U),Wp
πU
#η, πU
#ν
≤Wµ
p(η, ν)andWp(η, ν)≤Wµ
p(η, ν).
Proposition 2(c, d) together shows that one should expect the topology generated by Wµ
pto be
stronger than the unconditional distance Wp. Here, we note that Gigli [2008] and Chemseddine et al.
[2023] previously showed that Wµ
pis a metric through an equivalence with restricted couplings. Our
approach builds on the results of Hosseini et al. [2023] and is somewhat more direct, and hence our
proofs may be of independent interest.
For the sake of concreteness, we include an example where the conditional 2-Wasserstein distance
may be explicitly computed. Here, the necessary calculations follows from the fact that the conditional
distributions of a multivariate are again Gaussian, and Gaussian distributions admit a closed-form
expression for the usual unconditional 2-Wasserstein distance.
Example: Gaussian Measures. Suppose Y=U=Rand that η, ν∈Pµ
p(Y×U)are Gaussians
η=N(0, I) ν=N
0,
1ρ
ρ1
|ρ|<1. (6)
It follows that µ=πY
#η=πY
#ν=N(0,1). As ηy, νyare Gaussians, their W2distance
admits a closed form and we can directly compute the expectation in Equation (5)to obtain
Wµ,2
2(η, ν) = 2(1 −p
1−ρ2). This is zero if and only if ρ= 0 , i.e. η=ν. However,
πU
#η=πU
#ν=N(0,1)andW2(πU
#η, πU
#ν) = 0 regardless of ρ. Moreover, the uncondi-
tional distance is W2
2(η, ν) = 2 
2−√1−ρ−√1 +ρ
, from which it is easy to verify that
W2(η, ν)≤Wµ
2(η, ν). See Appendix C for a similar derivation which applies to arbitrary Gaussians.
4Conditional Wasserstein Space as a Geodesic Space. We now turn our attention to the geodesics
inPµ
p(Y×U). In particular, we show that there exists a constant speed geodesic between any two
measures in Pµ
p(Y×U), generalizing a similar result in the unconditional setting [Santambrogio,
2015, Theorem 5.27]. Moreover, we show that under suitable regularity assumptions, solutions to
the conditional Monge problem (1)induce constant speed geodesics. Our motivation for studying
geodesics in Pµ
p(Y×U)is practical – in Section 6, we show how one can model geodesics in
Pµ
p(Y×U)in order to obtain a conditional flow-based model whose paths are easy to integrate.
Acurve is a continuous function γ•:I→Pµ
p(Y×U)where I= (a, b)⊆Ris any open interval of
finite length. A curve is absolutely continuous if there exists m∈L1((a, b))such that
Wµ
p(γs, γt)≤Zt
sm(τ) dτ∀a < s≤t < b. (7)
If(γt)is an absolutely continuous curve, then its metric derivative
|γ′|(t) = lim
s→tWµ
p(γs, γt)
|s−t|(8)
exists for almost every t∈(a, b), and, moreover, we almost surely have |γ′|(t)≤m(t)pointwise
for any msatisfying Equation (7)[Ambrosio et al., 2005, Theorem 1.1.2]. A curve (γt)is called a
constant speed geodesic if for all a < s ≤t < b , we have Wµ
p(γs, γt) =|t−s|Wµ
p(γa, γb). It is
straightforward to show that every constant speed geodesic is absolutely continuous.
Theorem 1 (Pµ
p(Y×U) is a Geodesic Space)
For any η, ν∈Pµ
p(Y×U), there exists a constant speed geodesic between ηandν.
When an optimal triangular coupling γ⋆∈ΠY(η, ν)is induced by an injective triangular map T⋆, we
may recover a constant speed geodesic in Pµ
p(Y×U), generalizing the McCann interpolant [McCann,
1997] to the conditional setting. We refer to Proposition 4 for sufficient conditions on η, νunder
which such a T⋆exists. Informally, samples from (y0, u0)∼ηflow in a straight path at a constant
speed to their destination T⋆(y0, u0).
Theorem 2 (Conditional McCann Interpolants)
Fixη, ν∈Pµ
p(Y×U). Suppose T⋆(y, u) = ( y, T⋆
U(y, u))is an injective triangular map solving
the conditional Monge problem (1). Define the maps Tt:Y×U→Y×Ufor0≤t≤1via
Tt= (1−t)I+tT⋆, and define the curve of measures γt= [Tt]#η∈Pγ
p(Y×U). Then,
(a)(γt)is absolutely continuous and a constant speed geodesic between η, ν
(b)The vector field vt(T⋆
t(y, u)) = (0 , T⋆
U(y, u)−u)generates the path γt, in the sense that
(γt, vt)solve the continuity equation (9).
5 Conditional Benamou-Brenier Theorem
In this section, we prove a characterization of the absolutely continuous curves in Pµ
p(Y×U). As
a corollary, we obtain a conditional generalization of the Benamou-Brenier Theorem [Benamou
and Brenier, 2000], giving us a dynamic characterization of the conditional Wasserstein distance.
Roughly speaking, all such curves are generated by a vector field on Y×Uwhich has zero velocity
in the Ycomponent. This is natural, as all measures in Pµ
p(Y×U)have a fixed Y-marginal µ.
Such a vector field can be informally seen as tangent to a curve of measures, and is the dynamic
analogue of the triangular maps discussed in Section 3. More formally, given an open interval I⊆R,
a time-dependent Borel vector field v:I×Y×U→Y×Uis said to be triangular if there exists a
Borel vector field vU:I×Y×U→Usuch that vt(y, u) = 
0, vU
t(y, u)
.
Continuity Equation. We introduce some necessary background which allows us to link vector
fields to curves of measures. The continuity equation ∂tγt+div(vtγt) = 0 describes the evolution
of a measure γtwhich flows along a given vector field vt[Ambrosio et al., 2005, Chapter 8]. This
equation must be understood in the sense of distributions, i.e. for every φin a space of test functions,
Z
IZ
Y×U(∂tφ(y, u, t ) +⟨vt(y, u),∇y,uφ(y, u, t )⟩) dγt(y, u) dt= 0. (9)
5We consider cylindrical test functions φ∈Cyl(Y×U×I), i.e. of the form φ(y, u, t ) =ψ(πd(y, u), t)
where πd:Y×U→Rdmaps (y, u)7→(⟨(y, u), e1⟩, . . . ,⟨(y, u), ed⟩)and{e1, e2, . . . , e d}is any
orthonormal family in Y×U. In the finite dimensional setting, one may take φ∈C∞
c(Y×U)to be
smooth and compactly supported [Ambrosio et al., 2005, Remark 8.1.1].
In Appendix E, we prove Lemma 1, which is key in proving Theorem 4 below. Informally, Lemma 1
states that if the weak continuity equation (9)is satisfied for a joint distribution and triangular vector
field, then the continuity equation is also satisfied for the corresponding conditional distributions and
Ucomponents of the vector field.
Lemma 1 (Triangular Vector Fields Preserve Conditionals)
Suppose vt(y, u) = (0 , vU
t(y, u))is triangular and that (γt)⊂Pµ
p(Y×U)is a path of measures
such that (vt, γt)satisfy the continuity equation in the sense of distributions. Then, it follows that for
µ-almost every y∈Y, we have ∂tγy
t+∇ ·(vU
t(y,−)γy
t) = 0 .
We note that having vtbe triangular is sufficient, but certainly not necessary, for the conditional
continuity equation to almost surely hold. For instance, the vector field in Rdthat rotates N(0, I)
about the origin is not triangular yet preserves all conditional distributions.
Absolutely Continuous Curves. In this section, we state our characterization of absolutely contin-
uous curves in Pµ
p(Y×U). Informally, given such a curve, Theorem 3 provides us with a triangular
vector field which generates the curve, in the sense that the pair solve the continuity equation.
Theorem 3 (Absolutely Continuous Curves in Pµ
p(Y×U))
LetI⊂Rbe an open interval, and suppose γt:I→Pµ
p(Y×U)is an absolutely continuous in the
Wµ
pmetric with |γ′|(t)∈L1(I). Then, there exists a Borel vector field vt(y, u)such that
(a)vtis triangular
(b)vt∈Lp(γt, Y×U)and∥vt∥Lp(γt,Y×U)≤ |γ′|(t)for a.e. t
(c)(vt, γt)solve the continuity equation in the sense of distributions.
Conversely, we show in Theorem 4 that if the pair (γt, vt)solve the continuity equation and vtis
triangular, then the curve (γt)is absolutely continuous and |γ′|(t)≤ ∥vt∥Lp(γt,Y×U). The main
technique of this result is to study the collection of conditional continuity equations (which is feasible
by Lemma 1) and to apply the converse of Ambrosio et al. [2005, Theorem 8.3.1]. In this setting, the
infinite-dimensional result is obtained via a finite-dimensional approximation argument.
Theorem 4 (Continuous Curves Generated by Triangular Vector Fields)
Suppose that γt:I→Pµ
p(Y×U)is narrowly continuous and (vt)is a triangular vector field such
that(γt, vt)solve the continuity equation with ∥vt∥Lp(γt,Y×U)∈L1(I). Then, γt:I→Pµ
p(Y×U)
is absolutely continuous in the Wµ
pmetric and |γ′|(t)≤ ∥vt∥Lp(µ,Y×U)for almost every t.
As a corollary of Theorem 3 and Theorem 4, we obtain a conditional version of the Benamou-Brenier
theorem [Benamou and Brenier, 2000]. Once we have our characterization of absolutely continuous
curves provided by these theorems, the proof of Theorem 5 largely follows the unconditional case
(see e.g. Ambrosio et al. [2005, Chapter 8]), but we include it for the sake of completeness.
Theorem 5 (Conditional Benamou-Brenier)
Let1< p < ∞. For any η, ν∈Pµ
p(Y×U), we have
Wp,µ
p(η, ν) = min
(γt,vt)Z1
0∥vt∥p
Lp(µt)dt|(vt, γt)solve (9),γ0=η, γ1=ν, and vtis triangular
.
6 COT Flow Matching
We have thus far seen that the COT problem (3)admits a dynamic formulation by Theorem 5, where
one may take the underlying vector fields to be triangular. We use these results to design a principled
model for conditional generation based on flow matching [Lipman et al., 2022, Albergo et al., 2023b,
Liu et al., 2022, Tong et al., 2023, Pooladian et al., 2023]. We hereafter use the squared-distance cost
(i.e.p= 2).
6YCheckerboardTrue
 COT-FM (Ours)
 PCP-Map
 COT-Flow Conditional
UYCircles
U
 U
 UTrue
COT-FM
PCP-Map
COT-FlowFigure 1: Samples from the ground-truth joint target distribution and the various models. Samples
from COT-FM more closely match the ground-truth distribution than the baselines. In the final
column, we plot conditional KDEs for samples drawn conditioned on the yvalue indicated by the
dashed horizontal line. See Appendix F for a larger figure and additional results.
Flow Matching. We assume that we have access to samples z0= (y0, u0)∼η(y0, u0)∈
Pµ
p(Y×U)from a source measure, and samples z1= (y1, u1)∼ν(y1, u1)∈Pµ
p(Y×U)from a
target measure. Let z= (z0, z1)∼ρ(z0, z1)∈Π(η, ν)be any coupling of the source and target
measure. We specify a collection of measures and vector fields on Y×Uvia
γt(y, u|z) =N(y, u|tz1+ (1−t)z0, C) vt(y, u|z) =z1−z0 (10)
where Cis any trace-class covariance operator [Da Prato and Zabczyk, 2014]. As is standard in flow
matching [Lipman et al., 2022, Kerrigan et al., 2024], we obtain from Equations (10) a marginal
measure γt(y, u)and vector field vt(y, u)satisfying the continuity equation via
γt(y, u) =Z
(Y×U)2γt(y, u|z) dρ(z) vt(y, u) =Z
(Y×U)2vt(y, u|z)dγt(y, u|z)
dγt(y, u)dρ(z).
(11)
This marginal path (γt)1
t=0interpolates between the source measure ( t= 0) and a smoothed version
of the target measure ( t= 1). To transform source samples from ηinto target samples from ν, we
seek to learn the intractable vector field vt(y, u)with a model vθ(t, y, u )by minimizing the loss1
L(θ) =Et,ρ(z),γt(y,u|z)vθ(t, y, u )−vt(y, u|z)2(12)
which has the same θ-gradient as the MSE loss to the true vector field ut(y, u)[Tong et al., 2023].
COT Flow Matching. In the preceding section, ρ(z)may be an arbitrary coupling between ηand
ν. Motivated by Proposition 1, we will choose ρto be a COT coupling. Under sufficient regularity
conditions (see Appendix B), this COT plan will be induced by a triangular map. In turn, Theorem 2
gives us that this triangular map is generated by a triangular vector field of the form (10). Thus, we
parametrize our model uθto also be triangular. Moreover, we recover the optimal dynamic transport
given in Theorem 5 as Tr(C)→0by a pointwise application of [Tong et al., 2023, Proposition 3.4].
Given a collection of samples {zi
0, zi
1}n
i=1drawn from ηandν, we approximate a conditional
optimal coupling ρusing standard numerical techniques with the cost function cϵ(y0, u0, y1, u1) =
|y1−y0|2+ϵ|u1−u0|2for some 0< ϵ≪1. Intuitively, such a cost penalizes mass transfer along
theYdimension, which is precisely the constraint sought in the COT problem (3). Asϵ↓0, we
recover the true optimal triangular map [Carlier et al., 2010, Hosseini et al., 2023]. The COT coupling
1Previous work has referred to this as the conditional flow matching loss [Tong et al., 2023], which is not to
be confused with the notion of conditioning that we focus on in this work.
7Table 1: Distances between the ground-truth and generated joint distributions for the 2D datasets.
Our method (COT-FM) obtains lower distances than the considered baselines. Average results ±one
standard deviation are reported across five test sets, with the lowest average distance in bold.
Checkerboard Moons Circles Swissroll
W2(1e-2) MMD (1e-3) W2(1e-2) MMD (1e-3) W2(1e-2) MMD (1e-3) W2(1e-2) MMD (1e-3)
PCP-Map 6.27±0.810.21±0.13 8.44±1.090.22±0.10 6.19±0.430.20±0.17 5.35±0.930.16±0.13
COT-Flow 8.20±0.490.26±0.16 18.49±2.221.32±0.79 10.04±1.690.24±0.22 6.47±0.690.19±0.19
FM 8.81±0.580.24±0.20 15.55±0.771.85±0.22 7.03±0.170.45±0.11 8.18±0.340.58±0.09
COT-FM (Ours) 4.69±1.000.17±0.13 6.50±1.410.13±0.10 5.56±0.430.20±0.04 4.64±1.260.15±0.19
can either be precomputed for small datasets or computed on each minibatch drawn during training.
While the use of minibatches is a computational necessity, we find that surprisingly small batch
sizes still yields accurate approximations of the true COT mapping using our COT-FM method. See
Appendix G.
After training, we obtain a learned triangular vector field vθ(t, y, u ). Given an arbitrary fixed y∈Y,
we may approximately sample from the target ν(u|y)by sampling u0∼η(u0|y)and numerically
solving the corresponding flow equation ∂t(y, ut) =vθ(t, y, u t)with initial condition (y, u0).
Source Measure. Our framework is agnostic to the choice of source measure η, allowing for
flexibility in the modeling process. The main requirement is that the Y-marginals of the source ηand
target ηmust match. In some scenarios, this is trivially satisfied. If one is interested in using a source
distribution which is simply random noise, one may take η(y0, u0) =πY
#ν(y0)⊗ηU(u0)to be the
product of two independent distributions where ηUis arbitrary, e.g. Gaussian noise.
7 Experiments
Table 2: Statistical distances between
MCMC and posterior samples u∼
ν(u|y)for each method on the LV
dataset. Average results ±one standard
deviation reported across five test sets.
W2(1e-2) MMD (1e-3)
PCP-Map 5.04±0.05 2.67±2.1
COT-Flow 4.86±1.1 0.83±0.50
FM 11.41±0.262.65±0.14
COT-FM (Ours) 4.02±0.06 0.95±0.03We now illustrate our methodology (COT-FM) on a variety
of conditional simulation tasks. We compare our method
against several competitive baselines, namely PCP-Map
[Wang et al., 2023], COT-Flow [Wang et al., 2023], and
WaMGAN [Hosseini et al., 2023]. These baselines are
chosen as they reflect current state-of-the-art approaches
to learning COT maps. We additionally compare against
flow matching [Lipman et al., 2022, Wildberger et al.,
2024] without COT, i.e. where the coupling between the
source and target measures is the independent coupling
ρ(z0, z1) =η⊗ν. This baseline serves as an ablation for
the COT component of our model.
Overall, our method (COT-FM) typically outperforms these baselines across the diverse and challeng-
ing set of tasks we consider. We find that PCP-Map [Wang et al., 2023] is a strong baseline, but we
emphasize that this model relies on the use of an input-convex neural network [Amos et al., 2017]
and it is hence unclear how to adapt this method to e.g. images. Appendix F contains further details
and results for all of our experiments.2
2D Synthetic Data. We first consider synthetic distributions where Y=U=R. Our source
measure is taken to be the independent product η(y, u) =πY
#ν⊗ N(0,1). We plot ground-truth
joint distributions and samples for two datasets in Figure 1. See Appendix F for additional results.
Samples from our method (COT-FM) closely match those from the ground-truth distribution, whereas
samples from PCP-Map and COT-Flow [Wang et al., 2023] can produce samples in regions of zero
support under the ground-truth distribution. In Table 1, we provide a quantitative analysis, where
we measure the W2and MMD distances between the generated and ground-truth joint distributions.
This is motivated by Proposition 1, as triangular maps which couple the joint distributions necessarily
couple the conditional distributions. Our method outperforms the baselines across all metrics.
2Code for all of our experiments is available at https://github.com/GavinKerrigan/cot_fm
8Lotka-Volterra (LV) Dynamical System. Here we estimate parameters of the Lotka-V olterra (LV)
model given only noisy observations of its solution. The LV model has parameters u= (α, β, γ, δ )∈
R4
≥0and a pair of coupled nonlinear ODEs of the form
dp1(t)
dt=αp1−βp1p2dp2(t)
dt=−γp2+δp1p2 (13)
0.50 0.83 1.25α
0 0.041 0.1β
0.8 1.08 1.2γ
0.02 0.04 0.06δ
MCMC COT-FM
Figure 2: Sample KDEs on the
Lotka-V olterra inverse problem.
The red lines denote the true pa-
rameter values.whose solution p(t) = (p1(t), p2(t))∈R2
≥0represents the num-
ber of prey and predator species at time t∈[0, T]. Follow-
ing Alfonso et al. [2023], we assume p(0) = (30 ,1)and that
log(u)∼ N (m,0.5I)with m= (−0.125,−3,−0.125,−3).
Given parameters u∈R4
≥0, we simulate Equation (13) for
t∈ {0,2, . . . , 20}to obtain a solution z(u)∈R22
≥0. An ob-
servation y∈R22
≥0is obtained by the addition of log-normal
noise, i.e. log(y)∼ N(log(z(u),0.1I). We thus may simulate
many (y, u)pairs from the target measure for training.
As a benchmark, we follow the settings of Alfonso et al. [2023]
and choose parameters u= (0.83,0.041,1.08,0.04)to generate
a single observation yas described above. In Figure 2, we plot
a histogram of 10,000samples from the posterior ν(u|y)of
COT-FM.
Since the ground-truth posterior is intractable, we compare
against differential evolution Metropolis MCMC [Braak, 2006].
Samples from our method qualitatively resemble those from
MCMC, and the posterior mode is typically close to the true
unknown u(shown in red). Our method is quantitatively closest
to the MCMC samples in the W2metric, and competitive in the
MMD metric (Table 2).
Darcy Flow Inverse Problem. Here we consider an infinite-
dimensional Bayesian inverse problem from the 2D Darcy flow
PDE. The setting is adapted from Hosseini et al. [2023]. We opt
to compare against WaMGAN [Hosseini et al., 2023], as this is
currently the only other extant amortized function-space COT
method, and FFM [Kerrigan et al., 2023] as a function-space flow
matching ablation.
Table 3: Predictive performance of the gener-
ated samples on the Darcy flow inverse prob-
lem. Average result ±one standard deviation
obtained on 5 test sets of 5,000 samples each.
MSE (1e-2) CRPS (1e-2)
WaMGAN 6.55±0.07 18.75±0.10
FFM 7.30±0.07 15.47±0.06
COT-FFM (Ours) 5.40±0.08 15.56±0.08The Darcy flow PDE is an elliptic equation on a
smooth domain Ω⊆Rdwhich relates a permeability
fieldexp(u), a pressure field ρ, and a source term f
via−divexp(u)∇ρ=fonΩsubject to ρ= 0 on
∂Ω. Our goal is to recover the permeability ufrom
noisy measurements yof the pressure ρ. Both the
unknown uand observations yare functions and thus
infinite-dimensional. To define our target measure,
we specify a prior ν(u) =N(0, C)with a Matérn
kernel Cof lengthscale ℓ= 1/2andν= 3/2. Given
u∼η(u), the Darcy flow PDE is solved numerically
[Alnæs et al., 2015] to obtain a solution ρ(u)observed at some finite but arbitrary number of points
{x1, . . . , x n} ⊂R2. An observation y(u)is obtained by adding Gaussian noise to each observation,
i.e.y(u)∼ N(ρ(u), σ2I)where σ= 2.5×10−2. We implement all models via a Fourier Neural
Operator [Li et al., 2020], allowing us to work with arbitrary discretizations, as required by the
functional nature of this problem.
We provide an illustration in Figure 3. As the true posterior is intractable, we compare against
preconditioned Crank-Nicolson (pCN) [Cotter et al., 2013], a function-space MCMC method. In
Figure 3, we plot the mean posteriors obtained from the various methods. Qualitatively, both COT-FM
and FFM are good approximations to pCN, while WaMGAN has visual artifacts. However, the MSE
9True (u)
 Pressure (ρ)
 Observed (y)
 pCN
MSE: 21.27COT-FM (Ours)
MSE: 22.83FFM
MSE: 25.52WaMGAN
MSE: 2.21
 MSE: 3.49
 MSE: 3.63
MSE: 4.40
 MSE: 4.66
 MSE: 6.63Figure 3: Darcy flow illustration. Several true permeability fields uare shown, as well as the pressure
fieldρand its observed, noisy version y. We compare an ensemble average of posterior samples from
the various methods against MCMC (pCN) [Cotter et al., 2013]. COT-FM achieves the lowest MSE
to pCN. We note here that WaMGAN has clear visual artifacts despite achieving reasonable MSE and
CRPS scores.
between our method and the pCN mean is lower than that of FFM. Table 3 provides a quantitative
comparison between the methods on a test set of 5,000 samples, where we measure MSE and CRPS
[Hersbach, 2000]. We compare the ensemble mean of 10 samples against the true uvalue as running
pCN for each observation is prohibitively expensive. COT-FM outperforms FFM and WaMGAN in
terms of MSE and is on-par with FFM in terms of CRPS. See Appendix F for further details.
8 Conclusion
We analyze conditional optimal transport from a geometric and dynamical point of view. Our analysis
culminates in the characterization of absolutely continuous curves of measures in a conditional
Wasserstein space, resulting in a conditional analog of the Benamou-Brenier Theorem.
We use these result to build on the framework of triangular transport and flow matching to develop
simulation-free methods for conditional generative models. Our methods are applicable across a wide
class of problems, and we demonstrate our methodology on several challenging inverse problems.
Limitations and Broader Impacts. A limitation COT-FM is that computing the full COT plan
can be expensive for large datasets, necessitating the use of minibatch approximations potentially
resulting in sub-optimal plans. While this approximation does not limit the practical applicability of
our method, an interesting challenge is to characterize the precise relationship between this minibatch
approximation and the full COT plan. Moreover, computing the COT plan incurs a small additional
computational cost compared to standard flow matching. As with all generative models, a potential
negative impact is the potential for disinformation through generated samples being purported as real.
Acknowledgments and Disclosure of Funding
This research was supported by the Hasso Plattner Institute (HPI) Research Center in Machine
Learning and Data Science at the University of California, Irvine, by the National Science Foundation
under award 1900644, and by the National Institutes of Health under award R01-LM013344.
10References
Oriol Abril-Pla, Virgile Andreani, Colin Carroll, Larry Dong, Christopher J Fonnesbeck, Maxim Kochurov, Ravin
Kumar, Junpeng Lao, Christian C Luhmann, Osvaldo A Martin, et al. PyMC: A modern, and comprehensive
probabilistic programming framework in python. PeerJ Computer Science , 9:e1516, 2023.
Jonas Adler and Ozan Öktem. Deep Bayesian inversion. arXiv preprint arXiv:1811.05910 , 2018.
Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework
for flows and diffusions. arXiv preprint arXiv:2303.08797 , 2023a.
Michael S Albergo, Mark Goldstein, Nicholas M Boffi, Rajesh Ranganath, and Eric Vanden-Eijnden. Stochastic
interpolants with data-dependent couplings. arXiv preprint arXiv:2310.03725 , 2023b.
Jason Alfonso, Ricardo Baptista, Anupam Bhakta, Noam Gal, Alfin Hou, Isa Lyubimova, Daniel Pocklington,
Josef Sajonz, Giulio Trigila, and Ryan Tsai. A generative flow for conditional sampling via optimal transport.
arXiv preprint arXiv:2307.04102 , 2023.
Martin Alnæs, Jan Blechta, Johan Hake, August Johansson, Benjamin Kehlet, Anders Logg, Chris Richardson,
Johannes Ring, Marie E Rognes, and Garth N Wells. The FEniCS project version 1.5. Archive of numerical
software , 3(100), 2015.
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré. Gradient Flows: In Metric Spaces and in the Space of
Probability Measures . Springer Science & Business Media, 2005.
Luigi Ambrosio, Alberto Bressan, Dirk Helbing, Axel Klar, Enrique Zuazua, Luigi Ambrosio, and Nicola Gigli.
A user’s guide to optimal transport. Modelling and Optimisation of Flows on Networks: Cetraro, Italy 2009,
Editors: Benedetto Piccoli, Michel Rascle , pages 1–155, 2013.
Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Conference on
Machine Learning , pages 146–155. PMLR, 2017.
Brandon Amos et al. Tutorial on amortized optimization. Foundations and Trends in Machine Learning , 16(5):
592–732, 2023.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? Some theory and empirics. In
International Conference on Learning Representations , 2018.
Ricardo Baptista, Bamdad Hosseini, Nikola B Kovachki, and Youssef Marzouk. Conditional sampling with
monotone GANs: from generative models to likelihood-free inference. arXiv preprint arXiv:2006.06755 ,
2020.
Ricardo Baptista, Youssef Marzouk, and Olivier Zahm. On the representation and learning of monotone
triangular transport maps. Foundations of Computational Mathematics , pages 1–46, 2023.
Raphaël Barboni, Gabriel Peyré, and François-Xavier Vialard. Understanding the training of infinitely deep and
wide ResNets with conditional optimal transport. arXiv preprint arXiv:2403.12887 , 2024.
Mark A Beaumont. Approximate Bayesian computation in evolution and ecology. Annual review of ecology,
evolution, and systematics , 41:379–406, 2010.
Jean-David Benamou and Yann Brenier. A computational fluid mechanics solution to the Monge-Kantorovich
mass transfer problem. Numerische Mathematik , 84(3):375–393, 2000.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal
of the American statistical Association , 112(518):859–877, 2017.
Vladimir Igorevich Bogachev and Maria Aparecida Soares Ruas. Measure Theory , volume 2. Springer, 2007.
Cajo JF Ter Braak. A Markov chain Monte Carlo version of the genetic algorithm differential evolution: easy
bayesian computing for real parameter spaces. Statistics and Computing , 16:239–249, 2006.
Charlotte Bunne, Andreas Krause, and Marco Cuturi. Supervised training of conditional monge maps. Advances
in Neural Information Processing Systems , 35:6859–6872, 2022.
Guillaume Carlier, Alfred Galichon, and Filippo Santambrogio. From knothe’s transport to Brenier’s map and a
continuation method for optimal transport. SIAM Journal on Mathematical Analysis , 41(6):2554–2576, 2010.
Guillaume Carlier, Victor Chernozhukov, and Alfred Galichon. Vector quantile regression: An optimal transport
approach. The Annals of Statistics , 44(3):1165 – 1192, 2016. doi: 10.1214/15-AOS1401. URL https:
//doi.org/10.1214/15-AOS1401 .
11Jannis Chemseddine, Paul Hagemann, and Christian Wald. Y-Diagonal couplings: Approximating posteriors
with conditional Wasserstein distances. arXiv preprint arXiv:2310.13433 , 2023.
Jannis Chemseddine, Paul Hagemann, Christian Wald, and Gabriele Steidl. Conditional Wasserstein distances
with applications in Bayesian OT flow matching. arXiv preprint arXiv:2403.18705 , 2024.
S. L. Cotter, G. O. Roberts, A. M. Stuart, and D. White. MCMC methods for functions: Modifying old algorithms
to make them faster. Statistical Science , 28(3):424 – 446, 2013.
Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of
the National Academy of Sciences , 117(48):30055–30062, 2020.
Giuseppe Da Prato and Jerzy Zabczyk. Stochastic Equations in Infinite Dimensions . Cambridge University
Press, 2014.
Masoumeh Dashti and Andrew M Stuart. The Bayesian approach to inverse problems. arXiv preprint
arXiv:1302.6989 , 2013.
Aram Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient video prediction via sparsely conditioned flow
matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 23263–
23274, 2023.
Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, Stanislas Chambon,
Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Léo Gautheron, Nathalie T.H. Gayraud,
Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J.
Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. POT: Python optimal transport. Journal
of Machine Learning Research , 22(78):1–8, 2021. URL http://jmlr.org/papers/v22/20-451.html .
Giulio Franzese, Giulio Corallo, Simone Rossi, Markus Heinonen, Maurizio Filippone, and Pietro Michiardi.
Continuous-time functional diffusion processes. Advances in Neural Information Processing Systems , 36,
2024.
Timothy D Gebhard, Jonas Wildberger, Maximilian Dax, Daniel Angerhausen, Sascha P Quanz, and Bernhard
Schölkopf. Inferring atmospheric properties of exoplanets with flow matching and neural importance sampling.
arXiv preprint arXiv:2312.08295 , 2023.
Nicola Gigli. On the geometry of the space of probability measures in Rn endowed with the quadratic optimal
transport distance . PhD thesis, Scuola Normale Superiore, 2008.
Hans Hersbach. Decomposition of the continuous ranked probability score for ensemble prediction systems.
Weather and Forecasting , 15(5):559–570, 2000.
Bamdad Hosseini, Alexander W Hsu, and Amirhossein Taghvaei. Conditional optimal transport on function
spaces. arXiv preprint arXiv:2311.05672 , 2023.
Noboru Isobe, Masanori Koyama, Kohei Hayashi, and Kenji Fukumizu. Extended flow matching: a method of
conditional generation with generalized continuity equation. arXiv preprint arXiv:2402.18839 , 2024.
Gavin Kerrigan, Justin Ley, and Padhraic Smyth. Diffusion generative models in infinite dimensions. In
Proceedings of The 26th International Conference on Artificial Intelligence and Statistics , volume 206 of
Proceedings of Machine Learning Research , pages 9538–9563. PMLR, 2023.
Gavin Kerrigan, Giosue Migliorini, and Padhraic Smyth. Functional flow matching. In Proceedings of The 27th
International Conference on Artificial Intelligence and Statistics , volume 238 of Proceedings of Machine
Learning Research , pages 3934–3942, 2024.
Young-geun Kim, Kyungbok Lee, and Myunghee Cho Paik. Conditional wasserstein generator. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence , 2022.
Young-geun Kim, Kyungbok Lee, Youngwon Choi, Joong-Ho Won, and Myunghee Cho Paik. Wasserstein
geodesic generator for conditional distributions. arXiv preprint arXiv:2308.10145 , 2023.
Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks.
Advances in neural information processing systems , 30, 2017.
Nikola B. Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew M.
Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. CoRR ,
abs/2108.08481, 2021.
12Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,
and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint
arXiv:2010.08895 , 2020.
Jae Hyun Lim, Nikola B Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli, Jean
Kossaifi, Vikram V oleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al. Score-based diffusion models in
function space. arXiv preprint arXiv:2302.07400 , 2023.
Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for
generative modeling. In The Eleventh International Conference on Learning Representations , 2022.
Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data
with rectified flow. arXiv preprint arXiv:2209.03003 , 2022.
Robert J McCann. A convexity principle for interacting gases. Advances in Mathematics , 128(1):153–179, 1997.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784 ,
2014.
George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast likelihood-free
inference with autoregressive flows. In The 22nd international conference on artificial intelligence and
statistics , pages 837–848. PMLR, 2019.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan.
Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research , 22(57):
1–64, 2021.
F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011.
Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and
Ricky Chen. Multisample flow matching: Straightening flows with minibatch couplings. arXiv preprint
arXiv:2304.14772 , 2023.
Poornima Ramesh, Jan-Matthis Lueckmann, Jan Boelts, Álvaro Tejero-Cantero, David S Greenberg, Pedro J
Gonçalves, and Jakob H Macke. Gatsbi: Generative adversarial training for simulation-based inference. arXiv
preprint arXiv:2203.06481 , 2022.
Deep Ray, Harisankar Ramaswamy, Dhruv V Patel, and Assad A Oberai. The efficacy and generalizability of
conditional gans for posterior inference in physics-based inverse problems. arXiv preprint arXiv:2202.07773 ,
2022.
Mehdi SM Sajjadi, Bernhard Scholkopf, and Michael Hirsch. Enhancenet: Single image super-resolution
through automated texture synthesis. In Proceedings of the IEEE international conference on computer vision ,
pages 4491–4500, 2017.
Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY , 55(58-63):94, 2015.
Louis Sharrock, Jack Simons, Song Liu, and Mark Beaumont. Sequential neural score estimation: Likelihood-
free inference with conditional score based diffusion models. arXiv preprint arXiv:2210.04872 , 2022.
Alessio Spantini, Ricardo Baptista, and Youssef Marzouk. Coupling techniques for nonlinear ensemble filtering.
SIAM Review , 64(4):921–953, 2022.
Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy
Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal
transport. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems , 2023.
Giulio Trigila and Esteban G Tabak. Data-driven optimal transport. Communications on Pure and Applied
Mathematics , 69(4):613–648, 2016.
Cédric Villani et al. Optimal Transport: Old and New , volume 338. Springer, 2009.
Zheyu Oliver Wang, Ricardo Baptista, Youssef Marzouk, Lars Ruthotto, and Deepanshu Verma. Efficient neural
network approaches for conditional optimal transport with applications in Bayesian inference. arXiv preprint
arXiv:2310.16975 , 2023.
Jonas Wildberger, Maximilian Dax, Simon Buchholz, Stephen Green, Jakob H Macke, and Bernhard Schölkopf.
Flow matching for scalable simulation-based inference. Advances in Neural Information Processing Systems ,
36, 2024.
13Appendix: Table of Contents
A Optimal Transport 14
B Conditional Optimal Transport 15
C Closed-Form Conditional Wasserstein Distance for Gaussian Measures 16
D Proofs: Section 4 17
D.1 Metric Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
D.2 Geodesics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
E Proofs: Section 5 21
E.1 Continuity Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
E.2 Absolutely Continuous Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
F Experiment Details 25
F.1 2D Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.2 Lotka-V olterra Dynamical System . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.3 Inverse Darcy Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
G Minibatch COT 34
A Optimal Transport
We provide here a brief and informal overview of optimal transport in the standard unconditional
setting. For more details, we refer to the standard references of Villani et al. [2009], Santambrogio
[2015], Ambrosio et al. [2005]. Let Xbe a separable metric space and fix a cost function c:
X×X→R∪ {+∞}. Suppose we have two Borel measures η, ν∈P(X). The Monge problem
seeks to find a measurable transport map T:X→Xminimizing the expected cost of transport, i.e.
corresponding to the optimization problem
inf
TZ
Xc(x, T(x)) dη(x)|T#η=ν
. (14)
This optimization problem is challenging, though, as it involves a nonlinear constraint and the set
of feasible maps may be empty. In contrast, the Kantorovich problem is a relaxation which seeks to
find an optimal coupling γ∈Π(η, ν), i.e. a probability distribution over X×Xwith marginals η, ν,
which solves
inf
γZ
X×Xc(x0, x1) dγ(x0, x1)|γ∈Π(η, ν)
. (15)
Under fairly weak conditions (e.g., the cost is lower semicontinuous and bounded from below
[Ambrosio et al., 2013, Theorem 2.5]), minimizers to the Kantorovich problem are guaranteed to
exist. If the cost function is c(x0, x1) =|x0−x1|pfor some 1< p < ∞, under sufficient regularity
conditions on ηa solution T⋆to the Monge problem is guaranteed to exist and, moreover, the coupling
γ⋆= (I, T⋆)#ηis optimal for the Kantorovich problem. See Ambrosio et al. [2013, Chapter 2] and
Ambrosio et al. [2005, Theorem 6.2.10].
14Wasserstein Space. In the special case that c(x0, x1) =|x0−x1|pfor1≤p <∞, and η, ν∈
Pp(X), the Kantorovich problem admits a finite-cost solution. The cost of such an optimal coupling
is the p-Wasserstein distance
Wp
p(η, ν) = min
γZ
X×X|x0−x1|pdγ(x0, x1)|γ∈Π(η, ν)
(16)
which, as the name suggests, is a metric on the space Pp(X)[Ambrosio et al., 2005, Section 7.1]
[Santambrogio, 2015, Section 5.1]. The Wasserstein distance admits a dynamical formulation via the
Benamou-Brenier theorem [Benamou and Brenier, 2000]. Namely, the p-Wasserstein distance can
be obtained by finding a time-dependent vector field transforming ηtoνacross time t∈[0,1]with
minimal energy:
Wp(η, ν) = min
(γt,vt)Z1
0Z
X|vt(x)|pdγt(x) dt|γ0=η, γ1=ν, ∂tγt+div(vtγt) = 0
.(17)
Here, we constrain our minimization problem over the set of measures and vector fields (γt, vt)
interpolating between ηandν, satisfying a continuity equation (see Section 5). In Section 4, we
study a generalization of the Wasserstein distances for conditional optimal transport problems. In
particular, Theorem 5 provides a generalization of the Benamou-Brenier theorem to the conditional
setting which recovers a conditional Wasserstein distance.
B Conditional Optimal Transport
This section contains additional discussion regarding the static COT problem, supplementing Section
3.1. We refer to Hosseini et al. [2023], Baptista et al. [2020], and Chemseddine et al. [2023] for
further results and details.
Given a source and target measures η, ν∈Pµ
p(Y×U), the conditional Monge problem seeks to find
a triangular mapping solving
inf
TZ
Y×Uc(y, u, T (y, u)) dη(y, u)|T#η=ν, T: (y, u)7→(y, TU(y, u))
. (18)
The conditional Monge problem also admits a relaxation under which one only considers couplings
whose Y-components are almost surely equal. To that end, we consider the subset C⊂(Y×U)2
whose Ycomponents are identical, i.e.,
C:=
(y0, u0, y1, u1)∈(Y×U)2|y0=y1	
(19)
and we define the set of (Y)-restricted probability measures RY⊂P 
(Y×U)2
such that every
γ∈ R Yis concentrated on C. In other words, if γ∈ R Y, then samples (y0, u0, y1, u1)∼γhave
y0=y1almost surely. In addition, for any η, ν∈P(Y×U), we define the set of triangular couplings
ΠY(η, ν)to be the probability measures in RYwhose marginals are ηandν, i.e.
ΠY(η, ν) =n
γ∈ R Y|π1,2
#γ=η, π3,4
#γ=νo
. (20)
Theconditional Kantorovich problem seeks a triangular coupling γ⋆solving
inf
γ(Z
(Y×U)2c(y0, u0, y1, u1) dγ(y0, u0, y1, u1)|γ∈ΠY(η, ν))
. (21)
Hosseini et al. [2023] prove the existence of minimizers to the conditional Kantorovich and Monge
problems under very general assumptions. Moreover, optimal couplings to the conditional Kan-
torovich problem induce optimal couplings for µ-almost every conditional measure. Assuming
sufficient regularity assumptions on the conditional measures, unique solutions to the conditional
Monge problem exist. We restate these results here for the sake of completeness.
15Proposition 3 (Prop 3.3 [Hosseini et al., 2023])
Fixη, ν∈Pµ(Y×U). Suppose the cost function cis continuous, infc >−∞, and there exists a
finite cost coupling γ∈ΠY(η, ν). Then, the conditional Kantorovich problem admits a minimizer
γ⋆. Moreover, γ⋆,y0(y1, u0, u1) = ˆγ⋆,y0(u0, u1)δ(y1−y0)where for µ-almost every ythe measure
γ⋆,yis an optimal coupling for ηy, νyunder the cost cy(u0, u1) =c(y, u0, y, u 1)
Proposition 4 (Prop 3.8 [Hosseini et al., 2023])
Fix1< p < ∞andη, ν∈Pµ
p(Y×U). Suppose c(y0, u0, y1, u1) =|u0−u1|p. Ifηyassign
zero measure to Gaussian null sets for µ-almost every y, then there is a unique solution T⋆to
the conditional Monge problem, and γ⋆= (I, T⋆)#ηis the unique solution to the conditional
Kantorovich problem. If νyalso assign zero measure to Gaussian null sets for µ-almost every y, then
T⋆is injective η-almost everywhere.
C Closed-Form Conditional Wasserstein Distance for Gaussian Measures
In this section, we provide additional details and results regarding the closed-form conditional
Wasserstein distance for Gaussian distributions. See Section 4 in the main paper.
Suppose Y=RdandU=Rd′are Euclidean spaces (of possibly different dimensions), and that
η, ν∈Pµ
p(Y×U)are Gaussians of the form
η=N
m
mη
u
,Σ Λη
ΛηTΣη
u
ν=N
m
mν
u
,Σ Λν
ΛνTΣν
u
(22)
where m∈Rd,mη
u, mν
u∈Rd′, and the (block) covariance matrices are Σ∈Rd×d,Λη,Λν∈Rd×d′,
andΣη
u,Σν
u∈Rd′×d′.
This form is chosen to ensure that ηandνhave equal Y-marginals. It follows that µ=πY
#η=
πY
#ν=N(m,Σ). Let
Qη= Ση
u−ΛηTΣ−1ΛηQν= Σν
u−ΛνTΣ−1ΛνR= (Λη−Λν)TΣ−1. (23)
We have that the conditionals ηy, νyare available in closed-form:
ηy=N
mη
u+ ΛηTΣ−1(y−m), Qη
νy=N
mν
u+ ΛνTΣ−1(y−m), Qν
. (24)
Thus, for any fixed y, we use the known closed-form unconditional Wasserstein distance to obtain
W2
2(ηy, νy) =mη
u−mν
u+R(y−m)2+Tr
Qη+Qν−2
(Qη)1/2Qν(Qη)1/21/2
.(25)
We now take an expectation over y∼µ=N(m,Σ)to compute Wµ,2
2. Observe that R(y−m)∼
N(0, RΣRT)and that Ey∼µ[|R(y−m)|2] =Tr(RΣRT). Thus,
Wµ,2
2(η, ν) =Ey∼µ
W2
2(ηy, νy)
(26)
=Ey∼µ
|mη
u−mν
u|2+ 2⟨mη
u−mν
u, R(y−m)⟩+|R(y−m)|2
(27)
+Tr
Qη+Qν−2
(Qη)1/2Qν(Qη)1/21/2
=|mη
u−mν
u|2+Tr
Qη+Qν−2
(Qη)1/2Qν(Qη)1/21/2
+RΣRT
.(28)
This form, perhaps unsurprisingly, closely resembles the unconditional Wasserstein distance between
two Gaussians, except for the presence of an additional Tr(RΣRT)term. Note that when η, νhave
uncorrelated Y, U components, we precisely recover W2
2(πU
#η, πU
#ν)as one may expect. As a special
case of interest, if Y=U=Rand
η=N(0, I) ν=N
0,
1ρ
ρ1
|ρ|<1 (29)
16then we obtain as a special case of Equation (26) thatWµ,2
2(η, ν) = 2(1 −p
1−ρ2). This is zero if
and only if ρ= 0, i.e.η=ν.
D Proofs: Section 4
In this section, we provide detailed proofs of our claims in Section 4, regarding the metric properties
of the conditional Wasserstein space.
D.1 Metric Properties
We first note that Wµ
p(η, ν)may be viewed as the minimal value of the constrained Kantorovich
problem in Equation (3)when one takes the cost to be the metric on the space Y×U. Similar results,
relating the conditional Wasserstein distance to triangular couplings, have appeared previously, but
our proof is independent of these prior works [Chemseddine et al., 2023, Gigli, 2008].
Proposition 5 (Equivalent Formulation of the Conditional Wasserstein Distance)
Fixη, ν∈Pµ
p(Y×U)and1≤p <∞. Then, Wµ
p(η, ν)is well-defined, finite, and
Wµ,p
p(η, ν) = min
γ(Z
(Y×U)2dp(y0, u0, y1, u1) dγ|γ∈ΠY(η, ν))
(30)
where Wµ,p
p(η, ν)represents the p-th power of the conditional p-Wasserstein distance.
Proof. The cost function dpis clearly continuous and non-negative, and hence by Proposition 3 it
suffices to exhibit a finite-cost coupling γ∈ΠY(η, ν)between ηandν. Indeed, take the conditionally
independent coupling
γ(y0, u0, y1, u1) =η(u0|y1)ν(u1|y1)δ(y1−y0)µ(y1) (31)
which is clearly in ΠY(η, ν). We then have that
Z
(Y×U)2dp(y0, u0, y1, u1) dγ(y0, u0, y1, u1) =Z
(Y×U)2∥(y0, u0)−(y1, u1)∥p
Y×Udγ(y0, u0, y1, u1)
≤2pZ
(Y×U)2 
∥(y0, u0)∥p
Y×U+∥(y1, u1)∥p
Y×U
dγ(y0, u0, y1, u1)
= 2pZ
Y×U∥(y0, u0)∥p
Y×Udη(y0, u0) +Z
Y×U∥(y1, u1)∥p
Y×Udν(y1, u1)
<+∞.
Hence, Equation (30) admits a minimizer γ⋆∈ΠY(η, ν). By Proposition 3, this minimizer may
be taken to have the form γ⋆=γ⋆,y1(u0, u1)δ(y1−y0)µ(y1)where γ⋆,y1(u0, u1)isµ(y1)-almost
surely an optimal coupling between ηy1, νy1for the cost |u1−u0|p. Thus,
Z
(Y×U)2dpdγ⋆=Z
YZ
U2|u1−u0|pdγ⋆,y(u0, u1) dµ(y) (32)
=Z
YWp
p(ηy, νy) dµ(y) =Wp,µ
p(η, ν). (33)
Here, we emphasize that the µ-almost sure uniqueness of the disintegrations of η, νalong Yresult in
a well-defined expression.
Moreover, if η∈Pµ
p(Y×U)it follows that ηy∈Pp(U)forµ-a.e.y, because
Z
YZ
U|u|pdηy(u) dµ(y)≤Z
YZ
U|(y, u)|pdηy(u) dµ(y) (34)
=Z
Y×U|(y, u)|pdη(y, u)<+∞. (35)
Thus all considered p-Wasserstein distances on Uare finite.
17We now proceed to prove several metric properties of our distance.
Proposition 2 (Some Properties of Wµ
p)
Let1≤p <∞.
(a)Wµ
pis well-defined, finite, and equals the minimal conditional Kantorovich cost.
(b)Wµ
pis a metric on the space Pµ
p(Y×U).
(c) There does not exist C >0such that Wµ
p(η, ν)≤CWp(η, ν)for all η, ν∈Pµ
p(Y×U).
(d) For all η, ν∈Pµ
p(Y×U),Wp
πU
#η, πU
#ν
≤Wµ
p(η, ν)andWp(η, ν)≤Wµ
p(η, ν).
Proof. Part (a). This is simply a restatement of Proposition 5.
Part (b). Fixη, ν, ρ ∈Pµ
p(Y×U). Since Wpis a metric on Pp(U), we immediately obtain the
symmetry of Wµ
p. Moreover, we have that Wµ
p(η, ν) = 0 if and only if ηy=νyforµ-almost every
y. Thus, if Wµ
p(η, ν) = 0 andE⊆Y×Uis Borel measurable,
η(E) =Z
Yηy(Ey) dµ(y) =Z
Yνy(Ey) dµ(y) =ν(E). (36)
which shows that η=ν. Here, Ey={u|(y, u)∈E}is the y-slice of E. Conversely, if η=ν,
thenηy=νyup to a µ-null set by the essential uniqueness of disintegrations. Thus, Wµ
p(η, ν) = 0
if and only if η=ν.
By Minkowski’s inequality and the triangle inequality for WponPp(U), we see
Wµ
p(η, ν)≤(Ey∼µ[(Wp(ηy, ρy) +Wp(ρy, νy))p])1/p(37)
≤Ey∼µ[Wp
p(ηy, ρy)]1/p+Ey∼µ[Wp
p(ρy, νy)]1/p(38)
=Wµ
p(η, ρ) +Wµ
p(ρ, ν). (39)
Part (c). We provide a counterexample. Fix any u0̸= 0∈Uandy0, y1∈Ysuch that y0̸=y1.
Define µ=1
2(δy0+δy1). Setuk= (k+ 1)u0fork= 1,2, . . . and for each k, define two measures
onY×Uby
ηk=1
2(δy0u0+δy1uk) νk=1
2(δy1u0+δuky0). (40)
It is clear that
Wµ,p
p(ηk, νk) =kp|u0|pWp
p(ηk, νk) = min {kp|u0|p,|y1−y0|p}. (41)
Moreover, as k→ ∞ we have Wµ
p(µk, νk)→ ∞ butWp
p(νk, ηk)remains bounded. See Figure 4.
Part (d). First, the unconditional distance Wp(η, ν)may be obtained via an unrestricted coupling in
Π(η, ν), i.e. the set of all joint measures on Y×Uhaving marginals η, ν. Since Π(η, ν)⊇ΠY(η, ν),
by part (a) we see that Wp(η, ν)≤Wµ
p(η, ν).
Letγ⋆(y0, u0, y1, u1) =γ⋆,y1(u0, u1)δ(y1−y0)µ(y1)be an optimal γ⋆∈ΠY(η, ν). We claim
thatγ(u0, u1) :=R
Yγ⋆,y(u0, u1) dµ(y)couples πU
#ηandπU
#ν. Let π0: (u0, u1)7→u0be the
projection onto the first coordinate of U×U. Observe that for µ-almost every y, we have that
γ⋆,y∈Π(ηy, νy)is optimal, and, in particular, π0
#γ⋆,y=ηy.Fix an arbitrary φ∈Cb(U). We then
haveZ
Uφ(u0) dπ0
#γ(u0) =Z
U2(φ◦π0) dγ(u0, u1) (42)
=Z
YZ
U2(φ◦π0) dγ⋆,y(u0, u1) dµ(y) =Z
YZ
Uφ(u0) dπ0
#γ⋆,y(u0) dµ(y) (43)
=Z
YZ
Uφ(u0) dηy(u0) dµ(y)=Z
Y×Uφ(u0) dη(u0, y) (44)
=Z
Y×U(φ◦πU) dη(u0, y) =Z
UφdπU
#η(u0). (45)
18Y
Uy0y1
u0 uk
Figure 4: The counterexample in Proposition 2. The measure ηkis shown in black and the measure
νkis shown in white.
Thus πU
#γ=πU
#η. A similar argument shows that for the map π1: (u0, u1)7→u1we have
π1
#γ=πU
#ν, so that γ∈Π(πU
#η, πU
#ν).
Now, as γ⋆,y1(u0, u1)∈Π(ηy1, νy1)isµ-almost surely optimal in the usual Wasserstein sense,
Wp,µ
p(η, ν) =Z
YZ
U2|u0−u1|pdγ⋆,y(u0, u1) dµ(y) (46)
=Z
U2|u0−u1|pdγ(u0, u1) (47)
≥Wp
p(πU
#η, πU
#ν) (48)
since γ∈Π(πU
#η, πU
#ν)is a coupling but potentially sub-optimal.
D.2 Geodesics
We now study the geodesics in the space Pµ
p(Y×U).
Theorem 1 (Pµ
p(Y×U) is a Geodesic Space)
For any η, ν∈Pµ
p(Y×U), there exists a constant speed geodesic between ηandν.
Proof. Write λt: (Y×U)2→Y×Ufor the linear interpolant
λt(y0, u0, y1, u1) = (ty0+ (1−t)y1, tu0+ (1−t)u1) 0 ≤t≤1. (49)
Letγ⋆∈ΠY(η, ν)be an optimal restricted coupling, and consider the path of measures in Pp(Y×U)
given by
γt= [λt]#γ⋆0≤t≤1. (50)
Step one: We check that for each 0≤t≤1, we have γt∈Pµ
p(Y×U). That is, we need to check
that for all Borel A⊆Y, we have γt(A×U) =µ(A). Indeed, recall that restricted measures are
concentrated on the set C(see Equation (19)). Thus,
γt(A×U) =γ⋆
λ−1
t(A×U)	
=γ⋆{(y, u0, y, u 1)|y∈A}
=π1
#γ⋆(A) = (π1◦π1,2)#γ⋆(A)
=π1
#η(A) =µ(A)
i.e.γt(A×Y) =µ(A)as claimed.
Step two: We show that Wµ
p(γt, γs) =|t−s|Wµ
p(η, ν). Setγs
t:= (λt, λs)#γ⋆for0≤s < t≤1.
We claim γs
t∈ΠY(γt, γs). Indeed, we have π1,2
#γs
t=γtbecause for all Borel A⊆Y×U,
(λt, λs)#γ⋆(A×Y×U) =γ⋆ 
λ−1
t(A)
= (λt)#γ∗(A). (51)
19An analogous calculation shows that π3,4
#γs
t=γs, so that γs
t∈Π(γt, γs). We now check that
γs
t∈ R Y(Y×U). Indeed, suppose E⊆Y×Uis a Borel set such that E∩C=∅. In other words,
for every (y0, u0, y1, u1)∈Ewe have y0̸=y1. SetD:= (λt, λs)−1(E). We claim D∩C=∅, so
that
γs
t(E) = (λt, λs)#γ⋆(E) =γ⋆((λt, λs)−1(E)) (52)
=γ⋆(D∩C) = 0 . (53)
Indeed, if c= (y, u0, y, u 1)∈C, then
(λt, λs)(c) = (y, tu 0+ (1−t)u1, y, su 0+ (1−s)u1)/∈E (54)
=⇒c /∈(πt, πs)−1(E). (55)
Thus γs
t∈ΠY(η, ν)as claimed. Now, we have
Wµ,p
p(γt, γs)≤Z
(Y×U)2dp(y0, u0, y1, u1) dλs
t(y0, u0, y1, u1)
=Z
(Y×U)2dp(λt(y0, u0, y1, u1), λs(y0, u0, y1, u1)) dγ⋆(y0, u0, y1, u1)
=Z
(Y×U)2 
|(t−s)(y0−y1)|2+|(t−s)(u0−u1)|2p/2dγ⋆(y0, u0, y1, u1)
=|t−s|pZ
(Y×U)2dp(y0, u0, y1, u1) dγ⋆(y0, u0, y1, u1)
=|t−s|pWµ,p
p(η, ν).
Conversely, an application of the previous inequality and the triangle inequality show that for
0≤s≤t≤1,
Wµ
p(η, ν)≤Wµ
p(η, γs) +Wµ
p(γs, γt) +Wµ
p(γt, ν) (56)
≤sWµ
p(η, ν) +Wµ
p(γs, γt) + (1 −t)Wµ
p(η, ν). (57)
Rearranging the previous inequality implies |t−s|Wµ
p(η, ν)≤Wµ
p(γs, γt)for all s, t∈[0,1], and
hence Wµ
p(γt, γs) =|t−s|pWµ
p(η, ν).
Theorem 2 (Conditional McCann Interpolants)
Fixη, ν∈Pµ
p(Y×U). Suppose T⋆(y, u) = ( y, T⋆
U(y, u))is an injective triangular map solving
the conditional Monge problem (1). Define the maps Tt:Y×U→Y×Ufor0≤t≤1via
Tt= (1−t)I+tT⋆, and define the curve of measures γt= [Tt]#η∈Pγ
p(Y×U). Then,
(a)(γt)is absolutely continuous and a constant speed geodesic between η, ν
(b)The vector field vt(T⋆
t(y, u)) = (0 , T⋆
U(y, u)−u)generates the path γt, in the sense that
(γt, vt)solve the continuity equation (9).
Proof. Consider the function wt:Y×U→Ugiven by
wt(y, u) = (0 , T⋆
U(y, u)−u) = (0 , wt,U(y, u)) (58)
and note this is precisely wt(y, u) =∂tT⋆
t(y, u). Define the vector field
vt(y, u) =
wt◦T⋆,−1
t
(y, u) =
0,(wt,U◦T⋆,−1
t,U)(y, u)
. (59)
For any φ∈Cyl(Y×U), we have
d
dtZ
Y×Uφ(y, u) dγt(y, u) =d
dtZ
Y×Uφ(y, u) d[Tt]#η(y, u) (60)
=d
dtZ
Y×Uφ(y, T⋆
t,U(y, u)) dη(y, u) (61)
=Z
Y×U⟨∇φ(y, T⋆
t,U(y, u), wt(y, u))⟩dη(y, u) (62)
=Z
Y×U⟨∇φ(y, u), vt(y, u)⟩dγt(y, u) (63)
20which shows that (γt, vt)solve the continuity equation.
Now, note that for 0≤a≤b≤1, we have
Zb
a∥vt∥Lp(γt,Y×U)dt=Zb
aZ
Y×Uwt◦T⋆,−1
tp
(y, u) dγt(y, u)1/p
dt (64)
=Zb
aZ
Y×U|wt|p(y, u) dη(y, u)1/p
dt (65)
=Zb
aZ
Y×U|u−T⋆
U(y, u)|p(y, u) dη(y, u)1/p
dt (66)
= (b−a)Wµ
p(η, ν). (67)
In particular,R1
0∥vt∥Lp(γt,Y×U)dt <∞and so by Theorem 4 (γt)is absolutely continuous.
A similar calculation shows that (b−a)Wµ
p(η, ν) =Wµ
p(γb, γa) =Rb
a|γ′(t)|, where the last line
follows from the absolute continuity of γt. Thus, ∥vt∥Lp(γt,Y×U)=|γ′|(t)for almost every t∈[0,1]
by Lebesgue differentiation.
E Proofs: Section 5
In this section, we provide proofs of all claims made in Section 5.
E.1 Continuity Equation
We begin with a lemma that is used in the proof of Theorem 4. Informally, a solution to the continuity
equation with a triangular vector field will result in the conditional measures almost surely satisfying
the continuity equation as well.
Lemma 1 (Triangular Vector Fields Preserve Conditionals)
Suppose vt(y, u) = (0 , vU
t(y, u))is triangular and that (γt)⊂Pµ
p(Y×U)is a path of measures
such that (vt, γt)satisfy the continuity equation in the sense of distributions. Then, it follows that for
µ-almost every y∈Y, we have ∂tγy
t+∇ ·(vU
t(y,−)γy
t) = 0 .
Proof. Fix any φ∈Cyl(U×I). Suppose ψ∈Cyl(Y)is given, and note that ψ(y)φ(u, t)∈
Cyl(Y×U×I). As(vt, γt)solve the continuity equation, it follows from the triangular structure of
vtthat upon testing against ψφwe haveZ
IZ
Yψ(y)Z
U 
∂tφ(u, t) +⟨vU
t(y, u),∇uφ(u, t)
dγy
t(u) dµ(y) dt= 0. (68)
Because ψ(y)∈Cyl(Y), it is of the form ρ(π(y))where π:Y→Rkfor some k≥1and
ρ∈C∞
c(Rk). Taking ρto be a sequence of smooth approximations to the indicator function of an
arbitrary rectangle E=E1×E2× ··· × Ek⊆Rk, we see
Z
π−1(E)Z
IZ
U 
∂tφ(u, t) +⟨vU
t(y, u),∇uφ(u, t)
dγy
t(u) dtdµ(y) = 0 . (69)
AsYis separable, the Borel σ-algebra on Yis generated by the cylinder sets, i.e. those which are
precisely of the form π−1(E)for some finite-dimensional rectangle E. We have thus shown that for
an arbitrary Borel measurable set E⊆Y,Z
EZ
IZ
U 
∂tφ(u, t) +⟨vU
t(y, u),∇uφ(u, t)
dγy
t(u) dtdµ(y) = 0 . (70)
From this, it follows thatZ
IZ
U 
∂tφ(u, t) +⟨vU
t(y, u),∇uφ(u, t)
dγy
t(u) dµ(y) dt= 0 µ-almost every y. (71)
21E.2 Absolutely Continuous Curves
We now proceed to prove the main results of this section. First, we introduce some preliminary
notions. We define the map jq:Lq(γ, Y×U)→Lp(γ, Y×U)for1/p+ 1/q= 1via
jq(w) =|w|q−2w w ̸= 0
0 w= 0(72)
which is the Fréchet differential of the convex functional1
q∥w∥q
Lq(γ,Y×U). A straightforward
calculation shows that this map satisfies
∥jq(w)∥p
Lp(γ,Y×U)=∥w∥q
Lq(γ,Y×U)=Z
Y×U⟨jq(w), w⟩dγ(y, u). (73)
See also Ambrosio et al. [2005, Chapter 8].
Theorem 3 (Absolutely Continuous Curves in Pµ
p(Y×U))
LetI⊂Rbe an open interval, and suppose γt:I→Pµ
p(Y×U)is an absolutely continuous in the
Wµ
pmetric with |γ′|(t)∈L1(I). Then, there exists a Borel vector field vt(y, u)such that
(a)vtis triangular
(b)vt∈Lp(γt, Y×U)and∥vt∥Lp(γt,Y×U)≤ |γ′|(t)for a.e. t
(c)(vt, γt)solve the continuity equation in the sense of distributions.
Proof. Assume without loss of generality that |γ′|(t)∈L∞(I)and that I= (0,1)[Ambrosio et al.,
2005, Lemma 1.1.4, Lemma 8.1.3]. Fix any φ∈Cyl(Y×U). For s, t∈Ithere exists an optimal
triangular coupling γst∈ΠY(γs, γt). By Hölder’s inequality,
|γt(φ)−γs(φ)| ≤Lip(φ)Wµ
p(γs, γt). (74)
It follows that t7→γt(φ)is absolutely continuous. We can introduce the upper semicontinuous and
bounded map
H(y0, u0, y1, u1) =(
|∇φ(y0, u0)| (y0, u0) = (y1, u1)
|φ(y0,u0)−φ(y1,u1)|
|(y0,u0)−(y1,u1)|(y0, u0)̸= (y1, u1). (75)
For|h|sufficiently small, choose any optimal coupling γ(s+h)h∈ΠY(γs+h, γs)and note that
|γs+h(φ)−γs(φ)|
|h|≤1
|h|Z
(Y×U)2|(y0, u0)−(y1, u1)|H(y0, u0, y1, u1) dγ(s+h)s (76)
≤Wµ
p(γs+h, γs)
|h| Z
(Y×U)2Hq(y0, u0, y1, u1) dγ(s+h)h,s!1/q
. (77)
Iftis a point of metric differentiability for t7→γt, note that γ(t+h)t→(I, I)#γtnarrowly, where I
is the identity map on Y×U. Moreover, since γt∈Pµ
p(Y×U), it follows that on the diagonal we
have that almost surely H(y0, u0, y0, u1) =ι(|∇uφ(y0, u0)|. Thus,
lim sup
h→0|γt+h(φ)−γt(φ)|
|h|≤ |γ′|(t)Z
Y×U|H|q(y0, u0, y0, u0) dγt(y0, u0)1/q
(78)
=|γ′|(t)∥ι(∇uφ)∥Lq(γt,Y×U)=|γ′|(t)∥∇uφ∥Lq(γt,U). (79)
Taking Q=Y×U×Iandγ=R
γtdt, fix any φ∈Cyl(Q). We have that
Z
Q∂sφ(y, u, s ) dγ(y, u, s )
= lim
h↓0Z
I1
h Z
Y×Uφ(y, u, s ) dγs(y, u)−Z
(Y×U)φ(y, u, s ) dγs+h(y, u)!
ds.(80)
22An application of Fatou’s Lemma, Equation (78), and Hölder’s inequality gives us
Z
Q∂sφ(y, u, s ) dγ(y, u, s )≤Z
J|γ′|(s) ds1/pZ
Q|∇uφ(y, u, s )|qdµ(y, u, s )1/q
(81)
for any interval J⊂Iwith supp φ⊂J×Y×U.
Fix the subspace
V={ι(∇uφ(y, u, s )) :φ∈Cyl(Q)} ⊆Y×U (82)
and denote by VitsLq(γ, Y×U×I)closure. Define the linear functional L:V→Rvia
L(∇uφ) =−Z
Q∂sφ(y, u, s ) dγ(y, u, s ) (83)
and note that Equation (81) implies that Lis a bounded linear functional on V. Thus (by Hahn-
Banach and the fact that V⊆Vis dense) we may uniquely extend LtoV. We thus have a convex
minimization problem
min
w∈V1
qZ
Q|w(y, u, s )|qdγ(y, u, s )−L(w) (84)
which admits the unique solution wsuch that jq(w)−L= 0. In particular, the estimate (81) shows
that the above functional is coercive and hence admits a minimizer which we may obtain via its
differential as a consequence of convexity. Thus, we obtain a triangular vector field v=jq(w)such
that for all φ∈Cyl(Q),
⟨v,∇φ⟩=Z
Q⟨v(y, u, s ),∇φ(y, u, s )⟩dγ(y, u, s ) =⟨L,∇φ⟩=−Z
Q∂sφ(y, u, s ) dγ(y, u, s ).
(85)
This precisely shows that (vt, γt)is a triangular distributional solution to the continuity equation.
Now, choose any interval J⊂Iand choose a sequence ηk∈C∞
c(J), with 0≤ηk≤1andηk→ 1J
ask→ ∞ . Moreover choose a sequence (∇uφn)⊂Vconverging to w=jp(v)inLq(γ, Q). Our
previous calculations give
Z
Qηk(s)|v(y, u, s )|pdγ(y, u, s ) =Z
Qηk(s)⟨v, w⟩dγ= lim
n→∞Z
Qηk⟨v,∇uφn⟩dγ (86)
= lim
n→∞⟨L,∇u(ηkφn)⟩ ≤Z
J|γ′|p(s) ds1/pZ
J×Y×U|v|pdγ1/p
. (87)
Taking k→ ∞ we see that
Z
JZ
Y×U|vt(y, u)|pdγt(y, u) dt≤Z
J|γ′|p(s) ds (88)
and since J⊂Iwas arbitrary, we conclude
∥vt∥Lp(γt,Y×U)≤ |γ′|(t) a.e.-t. (89)
We now prove, in some sense, a converse of the previous theorem.
Theorem 4 (Continuous Curves Generated by Triangular Vector Fields)
Suppose that γt:I→Pµ
p(Y×U)is narrowly continuous and (vt)is a triangular vector field such
that(γt, vt)solve the continuity equation with ∥vt∥Lp(γt,Y×U)∈L1(I). Then, γt:I→Pµ
p(Y×U)
is absolutely continuous in the Wµ
pmetric and |γ′|(t)≤ ∥vt∥Lp(µ,Y×U)for almost every t.
23Proof. We first assume that Uis finite dimensional. Our strategy is to check the hypotheses necessary
for Ambrosio et al. [2005, Theorem 8.3.1] to hold for µ-almost every y, followed by an application
of this theorem. By Lemma 1, for µ-almost every ywe have that (γy
t, vU
t(y,−))solve the continuity
equation distributionally on I×U.
By Jensen’s inequality (and the assumption p≥1) we see
Z
I∥vt∥Lp(γt,Y×U)dt=Z
IEy∼µhvU
t(y,−)p
Lp(γy
t,U)i1/p
dt (90)
≥Z
IEy∼µhvU
t(y,−)
Lp(γy
t,U)i
dt (91)
=Ey∼µZ
IvU
t(y,−)
Lp(γy
t,U)dt
. (92)
Since the first term is finite, it follows thatvU
t(y,−)
Lp(γy
t,U)∈L1(I) µ-almost every y. (93)
Now Ambrosio et al. [2005, Lemma 8.1.2] shows that for µ-almost every ywe have that (γy
t)admits
a narrowly continuous representative (˜γy
t)with˜γy
t=γy
tfor almost every t. It follows from Ambrosio
et al. [2005, Theorem 8.3.1] that for any t1≤t2inI, we have
Wp
p(˜γy
t1,˜γy
t2)≤(t2−t1)p−1Zt2
t1|vU
t(y, u)|pd˜γy
t(u) dt (94)
= (t2−t1)p−1Zt2
t1|vU
t(y, u)|pdγy
t(u) dt (95)
where the second line follows as ˜γy
t=γy
tfor almost every t.
Let˜γt=R
Y˜γy
tdµ(y)be the measure obtained via marginalizing over the Y-variables. Taking an
expectation over y∼µ, the previous inequality shows us that
Wµ,p
p(˜γt1,˜γt2)
(t2−t1)p≤1
t2−t1Zt2
t1∥vt∥p
Lp(γt,Y×U)dt. (96)
Now, note that t1is almost surely a Lebesgue point of the right-hand side and ˜γt1=γt1. Taking
t2→t1along a sequence where ˜γt2=γt2shows us that
|γ′|(t)≤ ∥vt∥Lp(γt),Y×U (97)
for almost every t∈I.
In the case that Uis infinite dimensional, fix any y∈Ysuch that Lemma 1 holds (which is of
full measure) and fix a countable orthonormal basis (ek)forU. Set πd:U→Rdto be the
projection operator for this basis, i.e. u7→(⟨u, e1⟩, . . . ,⟨u, ed⟩). We consider the collection of finite
dimensional conditional measures γd,y
t=πd
#γy
t. By the same argument in Ambrosio et al. [2005,
Theorem 8.3.1], there exists a vector field vd,y
tonRdsuch that (γd,y
t, vd,y
t)solve the continuity
equation andvd,y
t
Lp(γd,y
t,Rd)≤vU
t(y,−)
Lp(γy
t,U). (98)
It follows from the finite-dimensional case above that for almost every t1≤t2, we have
Wp
p(γd,y
t1, γd,y
t2)≤(t2−t1)p−1Zt2
t1vU
t(y,−)p
Lp(γy
t,U)dt. (99)
Letˆγy,d
t= (πd)⋆
#γy,d
twhere (πd)⋆:Rd→Umaps z7→Pd
k=1zkek. As d→ ∞ we have
ˆγd,y
t→γy
tnarrowly for all t∈I. Since (πd)⋆is an isometry, Ambrosio et al. [2005, Lemma 7.1.4]
shows that
Wp
p(γy
t1, γy
t2)≤lim inf
d→∞Wp
p(γd,y
t1, γd,y
t2)≤(t2−t1)p−1Zt2
t1vU
t(y,−)p
Lp(γy
t,U)dt. (100)
24Now, integration with respect to dµ(y)yields
Wp,µ
p(γt1, γt2)≤(t2−t1)p−1Zt2
t1∥vt∥p
Lp(γt,Y×U)dt. (101)
Taking t2→t1shows that for almost every twe have
|γ′|(t)≤ ∥vt∥Lp(γt,Y×U). (102)
Together, Theorem 3 and Theorem 4 give us a dynamical interpretation of the conditional Wasserstein
distance. The following result is a conditional analogue of the well-known Benamou-Brenier Theorem
[Benamou and Brenier, 2000]. Here, we note that the following proof follows the standard proof
closely – the main legwork in obtaining this conditional generalization is through the previous two
theorems.
Theorem 5 (Conditional Benamou-Brenier)
Let1< p < ∞. For any η, ν∈Pµ
p(Y×U), we have
Wp,µ
p(η, ν) = min
(γt,vt)Z1
0∥vt∥p
Lp(µt)dt|(vt, γt)solve (9),γ0=η, γ1=ν, and vtis triangular
.
Proof. Write Mfor the infimum on the right-hand side.
First, suppose that (vt, µt)are admissible andR1
0∥vt∥Lp(µt)<∞. It follows from Theorem 4 that
(γt)is an absolutely continuous curve in Pµ
p(Y×U)and∥vt∥Lp(µt,Y×U)≥ |γ′|(t). Thus,
Wµ,p
p(η, ν)≤Z1
0|γ′|(t) dtp
≤Z1
0∥vt∥p
Lp(µt,Y×U)dt≤M. (103)
Conversely, by Theorem 1 there exists a constant speed geodesic (γt)⊂Pµ
p(Y×U)connecting
ηandν. Recall that constant speed geodesics are absolutely continuous. By Theorem 3, there
exists a Borel triangular vector field vtsuch that (vt, γt)solve the continuity equation, and moreover
∥vt∥Lp(µt,Y×U)≤ |γ′|(t). In fact, because (vt, γt)solve the continuity equation, Theorem 4 yields
that∥vt∥Lp(µt,Y×U)=|γ′|(t).
Since γtis a constant speed geodesic in Pµ
p(Y×U), it follows that |µ′|(t) =Wµ
p(η, ν)for almost
every t∈(0,1). Hence,
Wµ,p
p(η, ν) =Z1
0|γ′|(t)pdt=Z1
0∥vt∥p
Lp(γt,Y×U)≥M. (104)
Thus, Wµ,p
p(η, ν) =Mas desired.
F Experiment Details
In this section, we provide additional details regarding all of our experiments, as well as additional
results not contained within the main paper. All models can be trained on a single GPU with less than
24 GB of memory, and our experiments were parallelized over 8 such GPUs on a local server. We
first describe our setting for the 2D and Lotka-V olterra experiments, as these share a similar setup.
Details for the Darcy flow inverse problem are described in the corresponding section.
Models. For FM and COT-FM, our model architecture is an MLP with SeLU activations [Klambauer
et al., 2017]. Time conditioning is achieved by concatenating the time variable as an input to the
network. The covariance operator Cchosen in the path of measures in Equation (10) is taken to be
C=σ2Iwhere σis a hyperparameter.
Our implementation of FM is adapted from the torchcfm package Tong et al. [2023], available under
the MIT License. For PCP-Map and COT-Flow, we adapt the open-source implementations from
Wang et al. [2023], available under the MIT License.
25Table 4: Hyperparameter grid used for random search of the FM and COT-FM models on the 2D and
Lotka-V olterra datasets.
Hyperparameter Description Values
ϵ COT coupling strength [1e-6, 1e-4, 1e-2, 1e-1]
σ Variance for C=σ2Iin (10) [1e-3, 1e-2, 1e-1, 5e-1]
Batch Size Training batch size [256, 512, 1024]
Width Layer width in MLP [256, 512, 1024, 2048]
LR Learning rate [1e-4, 3e-4, 7e-4, 1e-3]
Layers Number of MLP layers [4, 6, 8]
Training and Model Selection. Hyperparameter tuning of the PCP-Map and COT-Flow models
was performed directly using the code of Wang et al. [2023], essentially implementing grid-search
with an early stopping procedure. We refer to the paper and codebase of Wang et al. [2023] for further
details. For COT-FM and FM, we perform a random grid search over 100 hyperparameter settings
using the grid described in Table 4. For all model types, we select the best model used to generate the
results in the paper as the training checkpoint that resulted in the lowest W2error to the joint target
distribution on a held-out validation set. For training, we use the Adam optimizer where we only tune
the learning rate, leaving all other settings as their defaults in pytorch .
F.1 2D Synthetic Data
Data Generation. This experiment consists of four 2D synthetic datasets, where Y=U=R.
The datasets moons, circles, swissroll are available through scikit-learn [Pedregosa et al., 2011].
The moons dataset is generated with noise= 0.05followed by standard scaling with a mean of
m= (0.5,0.25)and standard deviation of σ= (0.75,0.25). The circles dataset is generated with
factor=0.5 andnoise=0.05 . The swissroll dataset is generated with noise=0.75 , followed by
projection to the first two coordinates and re-scaling by a factor of 12. All other unstated parameters
are left as their default values. We use the code available from Hosseini et al. [2023] to generate
the checkerboard dataset. For all datasets, we generate a training set (i.e., samples from the target
distribution) of 20,000samples and 1,000held-out validation samples for model selection. Means
and standard deviations in Table 1 are reported across five independent testing sets of 5,000 samples
for the best representative of each model type.
In COT-FM, to generate samples from the source distribution, we sample an additional 20,000points
from the target distribution and keep only the Ycoordinates. This ensures that the source and target
have equal Ymarginals. During training, standard Gaussian noise N(0,1)is sampled for the U
coordinate of these source points at each minibatch.
We use minibatch COT couplings [Tong et al., 2023] in this experiment as computing the full COT
plan was prohibitively expensive in terms of memory usage. However, we note that we use large
batch sizes, meaning that the COT plan we find in this way should not be too far from optimal. All
couplings are computed using the POTPython package [Flamary et al., 2021].
F.2 Lotka-Volterra Dynamical System
Data Generation. We adopt the settings of Alfonso et al. [2023] for this experiment. As de-
scribed in the main paper, we assume p(0) = (30 ,1)and that log(u)∼ N (m,0.5I)with
m= (−0.125,−3,−0.125,−3). Given parameters u∈R4
≥0, we simulate Equation (13) for
t∈ {0,2, . . . , 20}to obtain a solution z(u)∈R22
≥0. An observation y∈R22
≥0is obtained by the
addition of log-normal noise, i.e. log(y)∼ N(log(z(u),0.1I). We thus may simulate many (y, u)
pairs from the target measure for training.
We generate a training set of 10,000 (y, u)pairs using the procedure described above and a held-out
validation set of 10,000 (y, u)pairs for model selection. Means and standard deviations in Table
2 are reported across five independent testing sets of 5,000 samples for the best representative of
each model type. Figure 2 and Figures 9, 8, 7, 10 show 10,000samples from each model, as well
as10,000samples from the differential evolution Metropolis MCMC sampler [Braak, 2006] after a
26YCheckerboardTrue
 COT-FM (Ours)
 PCP-Map
 COT-Flow
 FM
YMoons
YCircles
UYSwissroll
U
 U
 U
 UFigure 5: Samples from the ground-truth joint target distribution and the various models for the 2D
datasets. Samples from COT-FM more closely match the ground-truth distribution than the baselines.
A common failure mode for the baselines is to generate samples from regions with zero support under
the true data distributions. Table 1 contains a quantitative evaluation.
burn-in of 50,000samples. This is implemented through the PyMC Python package [Abril-Pla et al.,
2023].
For COT-FM we use the full COT couplings, i.e. without minibatches. This is available to use due to
the smaller size of the training set used in this experiment. The COT couplings are computed in the
same way as the previous section, and as described in Section 7.
27CheckerboardCOT-FM (Ours) PCP-Map COT-Flow FMMoons Circles SwissrollFigure 6: Conditional KDEs shown for each of the methods on the 2D datasets. The conditioning
variable yis fixed at the horizontal dashed line shown in Figure 5. In all plots, the orange solid line
indicates the CKDE of the ground-truth joint samples. In each column, the dashed blue line indicates
the CKDE of samples generated from the respective method.
280.50 0.75 1.00 1.25αα
0.0 0.10.501.25β
0.00 0.05 0.10β
0.7 1.30.501.25γ
0.7 1.30.00.1
0.75 1.00 1.25γ
0.02 0.060.501.25δ
0.02 0.060.00.1
0.02 0.060.71.3
0.02 0.04 0.06δLotka-Volterra Samples: COT-FM (Ours)Figure 7: KDE plots of the samples on the Lotka-V olterra system, using the settings described in
Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs.
In all plots, samples from MCMC are drawn in orange, and samples from our method (COT-FM) are
indicated in blue. The true unknown parameters are indicated by the red vertical line in the diagonal
plots, or the black x in the off-diagonal plots.
290.50 0.75 1.00 1.25αα
0.0 0.10.501.25β
0.00 0.05 0.10β
0.7 1.30.501.25γ
0.7 1.30.00.1
0.75 1.00 1.25γ
0.02 0.060.501.25δ
0.02 0.060.00.1
0.02 0.060.71.3
0.02 0.04 0.06δLotka-Volterra Samples: PCP-MapFigure 8: KDE plots of the samples on the Lotka-V olterra system, using the settings described in
Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs.
In all plots, samples from MCMC are drawn in orange, and samples from PCP-Map are indicated in
blue. The true unknown parameters are indicated by the red vertical line in the diagonal plots, or the
black x in the off-diagonal plots.
300.50 0.75 1.00 1.25αα
0.0 0.10.501.25β
0.00 0.05 0.10β
0.7 1.30.501.25γ
0.7 1.30.00.1
0.75 1.00 1.25γ
0.02 0.060.501.25δ
0.02 0.060.00.1
0.02 0.060.71.3
0.02 0.04 0.06δLotka-Volterra Samples: COT-FlowFigure 9: KDE plots of the samples on the Lotka-V olterra system, using the settings described in
Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs.
In all plots, samples from MCMC are drawn in orange, and samples from COT-Flow are indicated in
blue. The true unknown parameters are indicated by the red vertical line in the diagonal plots, or the
black x in the off-diagonal plots.
310.50 0.75 1.00 1.25αα
0.0 0.10.501.25β
0.00 0.05 0.10β
0.7 1.30.501.25γ
0.7 1.30.00.1
0.75 1.00 1.25γ
0.02 0.060.501.25δ
0.02 0.060.00.1
0.02 0.060.71.3
0.02 0.04 0.06δLotka-Volterra Samples: FMFigure 10: KDE plots of the samples on the Lotka-V olterra system, using the settings described in
Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs.
In all plots, samples from MCMC are drawn in orange, and samples from flow matching (FM) are
indicated in blue. The true unknown parameters are indicated by the red vertical line in the diagonal
plots, or the black x in the off-diagonal plots.
32F.3 Inverse Darcy Flow
Dataset. The training and test datasets are generated following the same procedure as Hosseini
et al. [2023]: pressure fields uare sampled from a Gaussian process with Matérn kernel having
ν= 3/2and lengthscale ℓ= 1/2, on a regular 40×40grid. The parameters are then exponentiated
and used to simulate the permeability fields pfrom the forward model Fsolving the Darcy flow
PDE, using FEniCS [Alnæs et al., 2015]. Stochasticity arises from adding Gaussian noise to the
permeability fields, obtaining y=F(u) +ϵ, ϵ∼ N(0, σ2I). For our experiments we observe yon a
100×100grid, and we use σ= 2.5×10−2. We note that this level of noise is quite considerable, as
it accounts for roughly 60% of the variability in the y. Figure 11 showcases a data point for reference.
Our source and target training sets contain 1×104samples each, and our test set comprises 5×103
samples. We remark that although yanduare observed on a grid their resolution does not need to be
fixed, allowing for training at different resolutions.
Figure 11: Example of one random data point from the Darcy flow dataset.
Models. In order to make learning feasible in infinite-dimensional Hilbert spaces, we adapt the
architecture of a Fourier Neural Operator (FNO) [Li et al., 2020] from the neuraloperator package
[Kovachki et al., 2021] to accommodate for conditioning information observed at an arbitrary
resolution. We do so by introducing a projection layer mapping the conditioning information to
match the hidden channels of the input lifting block, and a pooling operation to project to the
input dimensions. The two are then concatenated and passed through an FNOBlock mapping from
(2×hidden_channels )×input_dim tohidden_channels ×input_dim , before following the
original architecture. For all of the models in consideration, we fix the architecture to be have
hidden_channels = 64 ,projection_channels = 256 , and 32Fourier modes. We train each
model for 1500 epochs, and hyperparameters for each architecture are selected as follows:
•WaMGAN [Hosseini et al., 2023]: using an adaptation to the FNO architecture of the original
code3, we perform a grid search as detailed in Table 5. We found the training procedure to
be rather unstable, and for this reason we checkpoint the model every 100 epochs and report
the results for the best performing model at its best checkpoint. We found this to be a model
with learning rate 1×10−4, 2 full critic iterations, and monotone penalty of 1×10−3. The
gradient penalty parameter did not seem to significantly affect performance on the test set,
and was set to 5.
•FFM [Kerrigan et al., 2024]: the learning rate is fixed to 5×10−4, and the covariance
operator Cis set to match that of the prior, but rescaled by a factor of σ= 1×10−3. We
use the code from the original repository4.
•COT-FFM: we set ϵ= 1×10−5in the cost function used to build the COT plan. The
learning rate and Care chosen to be the same as FFM. In order to build COT couplings, we
take the source measure to be the product measure πY
#η× N(0, C). Approximate couplings
are obtained on minibatches of size 256.
It should be noted that in any scenario where the source and the target U−marginals are identical,
using the OT coupling would yield the identity mapping as the optimal vector field minimizing (12).
Hence, the OT-CFM model [Tong et al., 2023] is inapplicable here.
3https://github.com/TADSGroup/ConditionalOT2023
4https://github.com/GavinKerrigan/functional_flow_matching
33Table 5: Hyperparameter search space for WaMGAN
Parameter Search Space
Learning rate {1×10−3,5×10−4,1×10−4}
Full critic iter. {2, 5, 10}
Monotone penalty {1×10−3,5×10−2,1×10−1}
Gradient penalty {1, 5, 10}
Sampling. The resulting amortized sampler, denoted for simplification by the mapping (y, u0)7→
u1=˜TU(y, u0), will parameterize an approximate posterior measure. Notice that, in contrast
to classical variational inference techniques, no distributional assumptions are made about the
approximate posterior. In turn, integrals are obtained numerically by Monte Carlo sampling K
samples from the prior, resulting in the approximation
νy(f)≈Z
fdδ˜TU(y,u0)dN(0, C)≈1
KKX
k=1f(˜TU(yk, u0,k)),{u0,k}K
k=1i.i.d.∼ N (0, C).(105)
G Minibatch COT
In this section, we perform an additional experiment demonstrating that our COT-FM is able to obtain
good approximations of the true COT map even when trained on relatively small minibatches.
We work on Y=U=Rand use a standard Gaussian source and a Gaussian target distribution
with covariance ρ= 0.75. These are chosen so that we may evaluate, in closed-form, the true
conditional 2-Wasserstein distance, as detailed in Section C. We then train our COT-FM method
using various batch sizes, and measure the resulting model’s conditional 2-Wasserstein distance by
sampling 10,000points from the source distribution, flowing each source point along the model’s
learned vector field, and computing the resulting squared distance to the corresponding terminal
point.
In Figure 12, we plot the resulting deviation from the true value of Wµ,2
2as a function of batch size.
We see that even at a relatively small batch size of 16, the resulting error in the (squared) distances is
less than 10% of the true value ( ≈0.678). While this experiment is only feasible on synthetic data
(as we must compute the true distance), it nonetheless demonstrates that even with small batches we
may recover the true distance.
1024 512 256 128 64 32 16 8 4
Batch Size0.05
0.000.050.100.150.20True CWD2 - Model CWD2Transport Distance Error
Figure 12: Error in the squared transportation distance as a function of batch size. Values closer
to zero indicate a better approximation of the COT cost. For fairly small batch sizes ( >16) the
magnitude of the error is stable and relatively small. Shading indicates one standard deviation
computed over ten random evaluation sets.
34NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The claims and contributions of our work are clearly stated in the introduction
and abstract. In addition, the introduction clearly points to the sections in the paper where
the claims are discussed in more detail.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations of the work are discussed in Section 8.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
35Answer: [Yes]
Justification: All formal claims in the paper have their assumptions clearly stated. Proofs of
all theoretical claims are available in the Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Our experimental settings are clearly described in Section 7 and further details,
including hyperparameter choices, are provided in Appendix F. We will release our code
upon publication of the paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
365.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We will release our code as open-source upon acceptance of the paper for
publication.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: These details are provided in Appendix F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Error bars are provided for all quantitative results. Explicit descriptions of how
these are calculated are included as well.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
37•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: These details are provided in Appendix F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have reviewed the ethics guidelines and conform to all requirements.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: The potential societal impacts are discussed in Section 8.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
38•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We believe that our paper does not pose any such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Any assets used are properly cited throughout the paper. We provide addiitonal
details in Appendix F.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
39•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Our paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
40•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
41