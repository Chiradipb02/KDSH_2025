Learning to Control the Smoothness of GCN Features
Anonymous Author(s)
Affiliation
Address
email
Abstract
The pioneering work of Oono & Suzuki [ICLR, 2020] and Cai & Wang 1
[arXiv:2006.13318] analyze the smoothness of graph convolutional network (GCN) 2
features. Their results reveal an intricate empirical correlation between node clas- 3
sification accuracy and the ratio of smooth to non-smooth feature components. 4
However, the optimal ratio that favors node classification is unknown, and the 5
non-smooth features of deep GCN with ReLU or leaky ReLU activation function 6
diminish. In this paper, we propose a new strategy to let GCN learn node features 7
with a desired smoothness to enhance node classification. Our approach has three 8
key steps: (1) We establish a geometric relationship between the input and output 9
of ReLU or leaky ReLU. (2) Building on our geometric insights, we augment the 10
message-passing process of graph convolutional layers (GCLs) with a learnable 11
term to modulate the smoothness of node features with computational efficiency. 12
(3) We investigate the achievable ratio between smooth and non-smooth feature 13
components for GCNs with the augmented message passing scheme. Our extensive 14
numerical results show that the augmented message passing remarkably improves 15
node classification for GCN and some related models. 16
1 Introduction 17
LetG= (V, E)be an undirected graph with V={vi}n
i=1andEbe the set of nodes and edges, resp. 18
LetA∈Rn×nbe the adjacency matrix of the graph with Aij=1(i,j)∈E, where 1is the indicator 19
function. Furthermore, let Gbe the following (augmented) normalized adjacency matrix 20
G:= (D+I)−1
2(I+A)(D+I)−1
2=˜D−1
2˜A˜D−1
2, (1)
where Iis the identity matrix, Dis the degree matrix with Dii=Pn
j=1Aij, and ˜A:=A+Iand 21
˜D:=D+I. Starting from the initial node features H0:= [(h0
1)⊤, . . . , (h0
n)⊤]⊤∈Rd×nwith 22
h0
i∈Rdbeing the ithnode feature vector, the graph convolutional network (GCN) [ 20] learns node 23
representations using the following graph convolutional layer (GCL) transformation 24
Hl=σ(WlHl−1G), (2)
where σis the activation function, e.g. ReLU [ 25], and Wl∈Rd×dis learnable. GCL smooths 25
feature vectors of the neighboring nodes. The smoothness of features helps node classification; see 26
e.g. [ 22,31,5], resonating with the idea of classical semi-supervised learning approaches [ 41,38]. 27
Accurate node classification requires a balance between smooth and non-smooth components of GCN 28
features [ 27]. Besides graph convolutional networks (GCNs) stacking GCLs, many other graph neural 29
networks (GNNs) have been developed using different mechanisms, including spectral methods [ 3,9], 30
spatial methods [ 12,30], sampling methods [ 13,36], and the attention mechanism [ 30]. Many other 31
GNN models can be found in recent surveys or monographs; see, e.g. [15, 1, 33, 39, 14]. 32
Deep neural networks usually outperform shallow architectures, and a remarkable example is convo- 33
lutional neural networks [ 21,16]. However, this does not carry to GCNs; deep GCNs tend to perform 34
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.significantly worse than shallow models [ 5]. In particular, the node feature vectors learned by deep 35
GCNs tend to be identical over each connected component of the graph; this phenomenon is referred 36
to as over-smoothing [22,26,27,4,5,32], which not only occurs for GCN but also for many other 37
GNNs, e.g., GraphSage [ 13] and MPNN [ 12]. Intuitively, each GCL smooths neighboring node 38
features, benefiting node classification [ 22,31,5]. However, stacking these smoothing layers will in- 39
evitably homogenize node features. Algorithms have been developed to alleviate the over-smoothing 40
issue of GNNs, including decoupling prediction and message passing [ 11], skip connection and batch 41
normalization [ 18,7,6], graph sparsification [ 29], jumping knowledge [ 34], scattering transform 42
[24], PairNorm [37], and controlling the Dirichlet energy of node features [40]. 43
From a theoretical perspective, it is proved that deep GCNs using ReLU or leaky ReLU activation 44
function learn homogeneous node features [ 27,4]. In particular, [ 27] shows that the distance of 45
node features to the eigenspace M– corresponding to the largest eigenvalue 1 of matrix Gin (1) 46
– goes to zero when the depth of GCN with ReLU goes to infinity. Meanwhile, [ 27] empirically 47
studies the intricate correlation between node classification accuracy and the ratio between smooth 48
and non-smooth components of GCN node features, i.e., projections of node features onto eigenspace 49
Mand its orthogonal complement M⊥, resp. The empirical results of [ 27] indicate that both smooth 50
and non-smooth components of node features are crucial for accurate node classification , while 51
the ratio between smooth and non-smooth components to achieve optimal accuracy is unknown and 52
task-dependent. Furthermore, [ 4] proves that the Dirichlet energy – another smoothness measure for 53
node features – goes to zero when the depth of GCN with ReLU or leaky ReLU goes to infinity. 54
A crucial step in the proofs of [ 27,4] is that ReLU and leaky ReLU reduce the distance of feature 55
vectors to Mand their Dirichlet energy. However, [ 4] points out that over-smoothing – characterized 56
by the distance of features to eigenspace Mor the Dirichlet energy – is a misnomer ; the real 57
smoothness should be characterized by a normalized smoothness , e.g., normalizing the Dirichlet 58
energy by the magnitude of the features. The ratio between smooth and non-smooth components 59
of node features – studied in [ 27] – is closely related to the normalized smoothness . Nevertheless, 60
analyzing the normalized smoothness of node features learned by GCN with ReLU or leaky ReLU 61
remains an open problem [ 4]. Moreover, it is interesting to ask if analyzing the normalized smoothness 62
can result in any new understanding of GCN features and algorithms to improve GCN’s performance. 63
1.1 Our contribution 64
We aim to (1) establish a new geometric understanding of how GCL smooths GCN features and 65
(2) develop an efficient algorithm to let GCN and related models learn node features with a desired 66
normalized smoothness to improve node classification. We summarize our main contributions towards 67
achieving our goal as follows: 68
•We prove that there is a high-dimensional sphere underlying the input and output vectors of ReLU 69
or leaky ReLU. This geometric characterization not only implies theories in [ 27,4] but also informs 70
that adjusting the projection of input onto eigenspace Mcan alter the smoothness of the output 71
vectors. See Section 3 for details. 72
•We show that both ReLU and leaky ReLU reduce the distance of node features to eigenspace M, 73
i.e., ReLU and leaky ReLU smooth their input vectors without considering their magnitude. In 74
contrast, when taking the magnitude into account, ReLU and leaky ReLU can increase, decrease, or 75
preserve the normalized smoothness of each dimension of the input vectors; see Sections 3 and 4. 76
•Inspired by our established geometric relationship between the input and output of ReLU or leaky 77
ReLU, we study how adjusting the projection of input onto eigenspace Maffects both normalized 78
and unnormalized smoothness of the output vectors. We show that the distance of the output to 79
eigenspace Mis no greater than that of the original input – no matter how we adjust the input by 80
changing its projection onto M. In contrast, adjusting the projection of input vectors onto Mcan 81
change the normalized smoothness of output to any desired value; see details in Section 4. 82
•Based on our theory, we propose a computationally efficient smoothness control term (SCT) 83
to let GCN and related models learn node features with a desired (normalized) smoothness to 84
improve node classification. We comprehensively validate the benefits of our proposed SCT in 85
improving node classification – for both homophilic and heterophilic graphs – using a few of the 86
most representative GCN-style models. See Sections 5 and 6 for details. 87
As far as we know, our work is the first thorough study of how ReLU and leaky ReLU affect the 88
smoothness of node features both with and without considering their magnitude. 89
21.2 Additional related works 90
Controlling the smoothness of node features to improve the performance of GCNs is another line of 91
related work. For instance, [ 37] designs a normalization layer to prevent node features from becoming 92
too similar to each other, and [ 40] constrains the Dirichlet energy to control the smoothness of node 93
features without considering the effects of nonlinear activation functions. While there has been effort 94
in understanding and alleviating the over-smoothing of GCNs and controlling the smoothness of 95
node features, there is a shortage of theoretical examination of how activation functions affect the 96
smoothness of node features, specifically accounting for the magnitude of features. 97
1.3 Notation and Organization 98
Notation. We denote the ℓ2-norm of a vector uas∥u∥. For vectors uandv, we use ⟨u,v⟩,u⊙v, 99
andu⊗vto denote their inner, Hadamard, and Kronecker product, resp. For a matrix A, we 100
denote its (i, j)thentry, transpose, and inverse as Aij,A⊤, andA−1, resp. We denote the trace of 101
A∈Rn×nasTrace( A) =Pn
i=1Aii. For two matrices AandB, we denote the Frobenius inner 102
product as ⟨A,B⟩F:= Trace( AB⊤)and the Frobenius norm of Aas∥A∥F:=p
⟨A,A⟩. 103
Organization. We provide preliminaries in Section 2. In Section 3, we establish a geometric 104
characterization of how ReLU and leaky ReLU affect the smoothness of their input vectors. We study 105
the smoothness of each dimension of node features and take their magnitude into account in Section 4. 106
Our proposed SCT is presented in Section 5. We comprehensively verify the efficacy of the proposed 107
SCT in Section 6. Technical proofs and more experimental results are provided in the appendix. 108
2 Preliminaries and Existing Results 109
From the spectral graph theory [ 8], we can sort eigenvalues of matrix Gin (1) as 1 =λ1=. . .= 110
λm> λm+1≥. . .≥λn>−1, where mis the number of connected components of the graph. We 111
decompose V={vk}n
k=1intomconnected components V1, . . . , V m. Letui= (1{vk∈Vi})1≤k≤nbe 112
the indicator vector of Vi, i.e., the kthcoordinate of uiis one if the kthnodevklies in the connected 113
component Vi; zero otherwise. Moreover, let eibe the eigenvector associated with λi, then{ei}n
i=1114
forms an orthonormal basis of Rn. Notice that {ei}m
i=1spans the eigenspace M– corresponding to 115
eigenvalue 1 of matrix G, and{ei}n
i=m+1spans the orthogonal complement of M, denoted by M⊥. 116
The paper [27] connects the indicator vectors uis with the space M. In particular, we have 117
Proposition 2.1 ([27]).All eigenvalues of matrix Glie in the interval (−1,1]. Furthermore, the 118
nonnegative vectors {˜D1
2ui/∥˜D1
2ui∥}1≤i≤mform an orthonormal basis of M. 119
For any matrix H:= [h1, . . . ,hn]∈Rd×n, we have the decomposition H=HM+HM⊥ 120
withHM=Pm
i=1Heie⊤
iandHM⊥=Pn
i=m+1Heie⊤
isuch that ⟨HM,HM⊥⟩F= 121
Trace Pm
i=1Heie⊤
i(Pn
j=m+1Heje⊤
j)⊤
= 0,implying that ∥H∥2
F=∥HM∥2
F+∥HM⊥∥2
F. 122
2.1 Existing smoothness notions of node features 123
Distance to the eigenspace M.Oono et al. [ 27] study the smoothness of features H:= [h1, . . . ,hn] 124
using their distance to the eigenspace Mas an unnormalized smoothness notion. 125
Definition 2.2 ([27]).LetRd⊗ M be the subspace of Rd×nconsisting of the sumPm
i=1wi⊗ei,
where wi∈Rdand{ei}m
i=1is an orthonormal basis of the eigenspace M. Then we define ∥H∥M⊥
– the distance of node features Hto the eigenspace M– as follows:
∥H∥M⊥:= inf
Y∈Rd⊗M∥H−Y∥F=H−mX
i=1Heie⊤
i
F.
With the decomposition H=HM+HM⊥,∥ · ∥M⊥can be related to ∥ · ∥Fas follows: 126
∥H∥M⊥=∥H−HM∥F=∥HM⊥∥F. (3)
Dirichlet energy. The paper [ 4] studies the unnormalized smoothness of node features using Dirichlet 127
energy, which is defined as follows: 128
Definition 2.3 ([4]).Let˜∆ =I−Gbe the (augmented) normalized Laplacian, then the Dirichlet 129
energy ∥H∥Eof node features His defined by ∥H∥2
E:= Trace( H˜∆H⊤). 130
3Normalized Dirichlet energy. [4] points out that the real smoothness of node features Hshould be 131
measured by the normalized Dirichlet energy Trace( H˜∆H⊤)/∥H∥2
F.This normalized measurement 132
is essential because data often originates from various sources with diverse measurement units or 133
scales. By normalization, we can mitigate biases resulting from these different scales. 134
2.2 Two existing theories of over-smoothing 135
Letλ= max {|λi| |λi<1}be the second largest magnitude of G’s eigenvalues, and slbe the largest 136
singular value of weight matrix Wl. [27] shows that ∥Hl∥M⊥≤slλ∥Hl−1∥M⊥under GCL when 137
σis ReLU. Therefore, ∥Hl∥M⊥→0asl→ ∞ ifslλ <1, indicating node features converge to M 138
and results in over-smoothing. A crucial step in the analysis in [ 27] is that ∥σ(Z)∥M⊥≤ ∥Z∥M⊥,for 139
any matrix Zwhen σis ReLU, i.e., ReLU reduces the distance to M. [27] points out that it is hard 140
to extend the above result to other activation functions even leaky ReLU. 141
Instead of considering ∥H∥M⊥, [4] shows that ∥Hl∥E≤slλ∥Hl−1∥Eunder GCL when σis 142
ReLU or leaky ReLU. Hence, ∥Hl∥E→0asl→ ∞ , implying over-smoothing of GCNs. Note that 143
∥H∥M⊥= 0or∥Hl∥E= 0indicates homogeneous node features. The proof in [ 4] applies to GCN 144
with both ReLU and leaky ReLU by establishing the inequality ∥σ(Z)∥E≤ ∥Z∥Efor any matrix Z. 145
3 Effects of Activation Functions: A Geometric Characterization 146
In this section, we present a geometric relationship between the input and output vectors of ReLU or 147
leaky ReLU. We use ∥H∥M⊥as the unnormalized smoothness notion for all subsequent analyses 148
since we observe that ∥H∥M⊥and∥H∥Eare equivalent as seminorms. In particular, we have 149
Proposition 3.1. ∥H∥M⊥and∥H∥Eare two equivalent seminorms, i.e., there exist two constants 150
α, β > 0s.t.α∥H∥M⊥≤ ∥H∥E≤β∥H∥M⊥,for any H∈Rd×n. 151
3.1 ReLU 152
Letσ(x) = max {x,0}be ReLU. The first main result of this paper is that there is a high-dimensional 153
sphere underlying the input and output of ReLU; more precisely, we have 154
Proposition 3.2 (ReLU) .For any Z=ZM+ZM⊥∈Rd×n, letH=σ(Z) =HM+HM⊥.
ThenHM⊥lies on the high-dimensional sphere centered at ZM⊥/2with radius
r:= 
∥ZM⊥/2∥2
F− ⟨HM,HM−ZM⟩F1/2.
In particular, HM⊥lies inside the ball centered at ZM⊥/2with radius ∥ZM⊥/2∥Fand hence we 155
have∥H∥M⊥≤ ∥Z∥M⊥. 156
3.2 Leaky ReLU 157
Now we consider leaky ReLU σa(x) = max {x, ax}, where 0< a < 1is a positive scalar. Similar 158
to ReLU, we have the following result for leaky ReLU 159
Proposition 3.3 (Leaky ReLU) .For any Z=ZM+ZM⊥∈Rd×n, letH=σa(Z) =HM+
HM⊥. Then HM⊥lies on the high-dimensional sphere centered at (1 +a)ZM⊥/2with radius
ra:= 
∥(1−a)ZM⊥/2∥2
F− ⟨HM−ZM,HM−aZM⟩F1/2.
In particular, HM⊥lies inside the ball centered at (1 +a)ZM⊥/2with radius ∥(1−a)ZM⊥/2∥F 160
and hence we see that a∥Z∥M⊥≤ ∥H∥M⊥≤ ∥Z∥M⊥. 161
3.3 Implications of the above geometric characterizations 162
Propositions 3.2 and 3.3 imply that the precise location of HM⊥(or∥HM⊥∥F=∥H∥M⊥) depends 163
on the center and the radius rorra. Given a fixed ZM⊥, the center of the spheres remains unchanged, 164
andrandraare only affected by changes in ZM. This observation motivates us to investigate how 165
changes in ZMimpact ∥H∥M⊥, i.e., the unnormalized smoothness of node features . 166
Propositions 3.2 and 3.3 imply both ReLU and leaky ReLU reduce the distance of node features to 167
eigenspace M, i.e.∥H∥M⊥≤ ∥Z∥M⊥. Moreover, this inequality is independent of ZM; consider 168
Z,Z′∈Rd×ns.t.ZM⊥=Z′
M⊥butZM̸=Z′
M. LetHandH′be the output of ZandZ′via 169
ReLU or leaky ReLU, resp. Then we have ∥H∥M⊥≤ ∥Z∥M⊥and∥H′∥M⊥≤ ∥Z′∥M⊥. Since 170
ZM⊥=Z′
M⊥, we deduce that ∥H′∥M⊥≤ ∥Z∥M⊥. In other words, when ZM⊥=Z′
M⊥is fixed, 171
changing ZMtoZ′
Mcan change the unnormalized smoothness of the output features but cannot 172
change the fact that ReLU and leaky ReLU smooth node features ; we demonstrate this result in 173
4Fig. 1a) in Section 4.1. Notice that without considering the nonlinear activation function, changing 174
ZMdoes not affect the unnormalized smoothness of node features measured by ∥H∥M⊥. 175
In contrast to the unnormalized smoothness, if one considers the normalized smoothness, we find 176
that adjusting ZMcan result in a less smooth output ; we will discuss this in Section 4.1. 177
4 How Adjusting ZMAffects the Smoothness of the Output 178
Throughout this section, we let ZandHbe the input and output of ReLU or leaky ReLU. The 179
smoothness notions based on the distance of feature to Mor their Dirichlet energy do not account 180
for the magnitude of each dimension of the features; [ 4] points out that analyzing the normalized 181
smoothness of features Z, given by ∥Z∥E/∥Z∥F, is an open problem. However, these two smooth- 182
ness notions aggregate the smoothness of node features across all dimensions; when the magnitude 183
of some dimensions is much larger than others, the smoothness will be dominated by them. 184
Motivated by the discussion in Section 3.3, we study the disparate effects of adjusting ZMon the 185
normalized and unnormalized smoothness in this section. For the sake of simplicity, we assume 186
the graph is connected ( m= 1); all the following results can be extended to graphs with multiple 187
connected components easily. Due to the equivalence between seminorms ∥ · ∥Mand∥ · ∥ E, we 188
introduce the following definition of the dimension-wise normalized smoothness of node features. 189
Definition 4.1. LetZ∈Rd×nbe the features over nnodes with z(i)∈Rnbeing its ithrow, i.e., the
ithdimension of the features over all nodes. We define the normalized smoothness of z(i)as follows:
s(z(i)):=∥z(i)
M∥/∥z(i)∥,
where we set s(z(i)) = 1 whenz(i)=0. 190
Remark 4.2.Notice that the normalized smoothness s(z(i)) =∥z(i)
M∥/∥z(i)∥is closely related to the 191
ratio between the smooth and non-smooth components of node features ∥z(i)
M∥/∥z(i)
M⊥∥. 192
The graph is connected implies that z(i)
M=⟨z(i),e1⟩e1and∥z(i)
M∥=|⟨z(i),e1⟩|. Without ambiguity, 193
we write zforz(i)andefore1– the eigenvector of Gassociated with the eigenvalue 1. Moreover, 194
we have 195
s(z) =∥zM∥
∥z∥=|⟨z,e⟩|
∥z∥=|⟨z,e⟩|
∥z∥ · ∥e∥⇒0≤s(z)≤1, (4)
It is evident that the larger s(z)is, the smoother the node feature zis1. In fact, we have
s(z)2+∥z∥M⊥
∥z∥2
=∥zM∥2
∥z∥2+∥zM⊥∥2
∥z∥2= 1,
where ∥z∥M⊥/∥z∥decreases as s(z)increases. 196
1.0
 0.5
 0.0 0.5 1.0
Parameter ()
0510Smoothness (s)
||z||
||(z)||
||a(z)||
1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
Parameter ()
0.00.51.0Smoothness (s)
s(z)
s((z))
s(a(z))
a) Smoothness b) Normalized smoothness
Figure 1: Contrasting the effects of varying parame-
terαon the smoothness and normalized smoothness
of output features σ(zα)andσa(zα). The disconti-
nuity of s(σ(zα))in b) comes from the definition of
normalized smoothness. Note that s(z) = 1 ifz=0,
andσ(zα)can become 0when αis large enough.To discuss how the smoothness s(h) =s(σ(z))
ors(σa(z))can be adjusted by changing zM, we
consider the function
z(α) =z−αe.
It is clear that
z(α)M⊥=zM⊥andz(α)M=zM−αe,
where we see that αonly alters zMwhile pre- 197
serves zM⊥. Moreover, it is evident that 198
s(z(α)) =s
1−∥z(α)M⊥∥2
∥z(α)∥2=s
1−∥zM⊥∥2
∥z(α)∥2.
It follows that s(z(α)) = 1 if and only if zM⊥=0(include the case z=0), showing that when 199
zM⊥=0, the vector zis the smoothest one. 200
4.1 The disparate effects of αon∥ · ∥M⊥ands(·): Empirical results 201
Let us empirically study possible values that the unnormalized smoothness ∥σ(z(α))∥M⊥, 202
∥σa(z(α))∥M⊥and the normalized smoothness s(σ(z(α))),s(σa(z(α)))can take when αvaries. 203
1Here,z∈Rnis a vector whose ithentry is the 1D feature associated with node i.
5We denote zα:=z(α) =z−αe. We consider a connected synthetic graph with 100nodes, and each 204
node is assigned a random degree between 2to10. Then we assign an initial node feature z∈R100, 205
sampled uniformly on the interval [−1.5,1.5], to the graph with each node feature being a scalar. 206
Also, we compute eby the formula e=˜D1
2u/∥˜D1
2u∥from Proposition 2.1, where u∈R100is 207
the vector whose entries are all ones and ˜Dis the (augmented) degree matrix. We examine two 208
different smoothness notions for the input zand the output σ(zα)andσa(zα), where the smoothness 209
is measured for various values of the smoothness control parameter α∈[−1.5,1.5]. In Fig. 1a), we 210
study the unnormalized smoothness measured by ∥·∥M⊥; we see that ∥σ(zα)∥M⊥and∥σa(zα)∥M⊥ 211
are always no greater than ∥z∥M⊥. This coincides with the discussion in Section 3.3; adjusting 212
the projection of zonto the eigenspace Mcan not change the fact that ∥σ(zα)∥M⊥≤ ∥z∥M⊥ 213
and∥σa(zα)∥M⊥≤ ∥z∥M⊥. Nevertheless, an interesting result is that altering the eigenspace 214
projection can adjust the unnormalized smoothness of the output : notice that altering the eigenspace 215
projection does not change its distance to M, i.e., the smoothness of the input is unchanged, but the 216
smoothness of the output after activation function can be changed. 217
In contrast, when studying the normalized smoothness s(·)in Fig. 1b), we find that s(σ(z(α))) 218
ands(σa(z(α)))can be adjusted by αto values smaller than s(z). More precisely, we see that by 219
adjusting α,s(σ(z(α)))ands(σa(z(α)))can achieve most of the values in [0,1]. In other words, 220
both smoother and less smooth features can be obtained by adjusting α. 221
4.2 Theoretical results on the smooth effects of ReLU and leaky ReLU 222
In this subsection, we build theoretical understandings of the above empirical findings on the 223
achievable smoothness shown in Fig. 1. Notice that if zM⊥=0, the inequalities presented in 224
Propositions 3.2 and 3.3 indicate that ∥σ(z(α))∥M⊥and∥σa(z(α))∥M⊥vanish. So we have 225
s(σ(z(α))) = 1 for any αwhenzM⊥=0. Then we may assume zM⊥̸=0for the following study. 226
Proposition 4.3 (ReLU) .Suppose zM⊥̸=0. Leth(α) =σ(z(α))withσbeing ReLU, then 227
min
αs(h(α)) =sP
xi=max xdiPn
j=1djandmax
αs(h(α)) = 1 ,
where x:=˜D−1
2z,maxx= max 1≤i≤nxi, and ˜Dis the augmented degree matrix with diagonals 228
d1, d2, . . . , d n. In particular, the normalized smoothness s(h(α))is monotone increasing as α 229
decreases whenever α <∥˜D1
2un∥maxxand it has range [min αs(h(α)),1]. 230
Proposition 4.4 (Leaky ReLU) .Suppose zM⊥̸=0. Leth(α) =σa(z(α))withσabeing leaky 231
ReLU, then (1) minαs(h(α)) = 0 , and (2) supαs(h(α)) = 1 ands(h(α))has range [0,1). 232
Proposition 4.4 also holds for other variants of ReLU, e.g., ELU2and SELU3.; see Appendix C. We 233
summarize Propositions 3.2, 3.3, 4.3, and 4.4 in the following corollary, which qualitatively explains 234
the empirical results in Fig. 1. 235
Corollary 4.5. Suppose zM⊥̸=0. Leth(α) =σ(z(α))orσa(z(α))withσbeing ReLU and σa 236
being leaky ReLU. Then we have ∥z∥M⊥≥ ∥h(α)∥M⊥for any α∈R; however, s(h(α))can be 237
smaller than, larger than, or equal to s(z)for different values of α. 238
Propositions 4.3 and 4.4, and Corollary 4.5, provide a theoretical basis for the empirical results in 239
Fig. 1. Moreover, our results indicate that for any given vector z, altering zMcan change both the 240
unnormalized and the normalized smoothness of the output vector h=σ(z)orσa(z). In particular, 241
the normalized smoothness of h=σ(z)orσa(z)can be adjusted to any value in the range shown 242
in Propositions 4.3 and 4.4. This provides us with insights to control the smoothness of features to 243
improve the performance of GCN and we will discuss this in the next section. 244
5 Controlling Smoothness of Node Features 245
We do not know how smooth features are ideal for a given node classification task. Nevertheless, our 246
theory indicates that both normalized and unnormalized smoothness of the output of each GCL can 247
be adjusted by altering the input’s projection onto M. As such, we propose the following learnable 248
smoothness control term to modulate the smoothness of each dimension of the learned node features 249
Bl
α=mX
i=1αl
ie⊤
i, (5)
2The ELU function is defined by f(x) = max( x,0) + min(0 , a·(ex−1))where a >0.
3The SELU function is defined by f(x) =c(max( x,0) + min(0 , a·(ex−1))) where a, c > 0.
6where lis the layer index, {ei}m
i=1is the orthonormal basis of the eigenspace M, andαl:={αl
i}m
i=1250
is a collection of learnable vectors with αl
i∈Rdbeing approximated by a multi-layer perceptron 251
(MLP). The detailed configuration of αl
iwill be specified in each experiment later. One can see that 252
Bl
αalways lies in Rd⊗ M . We integrate SCT into GCL, resulting in 253
Hl=σ(WlHl−1G+Bl
α). (6)
We call the corresponding model GCN-SCT. Again, the idea is that we alter the component in 254
eigenspace to control the smoothness of features . Each dimension of Hlcan be smoother, less 255
smooth, or the same as Hl−1in normalized smoothness, though Hlgets closer to MthanHl−1. 256
To design SCT, we introduce a learnable matrix Al∈Rd×mfor layer l, whose columns are αl
i, where 257
mis the dimension of the eigenspace Manddis the dimension of the features. We observe in our 258
experiments that the SCT performs best when informed by degree pooling over the subcomponents of 259
the graph. The matrix of the orthogonal basis vectors, denoted by Q:= [e1, . . . ,em]∈Rn×m, is used 260
to perform pooling HlQfor input Hl. In particular, we let Al=W⊙(HlQ), where W∈Rd×m261
is learnable and performs pooling over Hlusing the eigenvectors Q. The second architecture uses 262
a residual connection with hyperparameter βl= log( θ/l+ 1) and learnable matrices W0,W1∈ 263
Rd×dand the softmax function ϕ. Resulting in Al=ϕ(HlQ)⊙(βlW0H0Q+ (1−βl)W1HlQ). In 264
Section 6, we use the first architecture for GCN-SCT as GCN uses only Hlinformation at each 265
layer. We use the second architecture for GCNII-SCT and EGNN-SCT which use both H0andHl266
information at each layer. There are two particular advantages of the above design of SCT: (1) it can 267
effectively change the normalized smoothness of the learned features, and (2) it is computationally 268
efficient since we only use the eigenvectors corresponding to the eigenvalue 1 of matrix G, which is 269
determined based on the connectivity of the graph. 270
5.1 Integrating SCT into other GCN-style models 271
In this subsection, we present other usages of the proposed SCT. Due to the page limit, we carefully 272
select two other most representative models. The first example is GCNII [ 6], GCNII extends GCN 273
to express an arbitrary polynomial filter rather than the Laplacian polynomial filter and achieves 274
state-of-the-art (SOTA) performance among GCN-style models on various tasks [ 6,23], and we 275
aim to show that SCT can even improve the accuracy of the GCN-style model that achieves SOTA 276
performance on many node classification tasks. The second example is energetic GNN (EGNN) [ 40], 277
which controls the smoothness of node features by constraining the lower and upper bounds of the 278
Dirichlet energy of features and assuming the activation function is linear. In this case, we aim to 279
show that our new theoretical understanding of the role of activation functions and the proposed SCT 280
can boost the performance of EGNN with considering nonlinear activation functions. 281
GCNII. Each GCNII layer uses a skip connection to the initial layer H0and given as follows: 282
Hl=σ 
((1−αl)Hl−1G+αlH0)((1−βl)I+βlWl)
,
where αl, βl∈(0,1)are learnable scalars. We integrate SCT Bl
αinto GCNII, resulting in the 283
following GCNII-SCT layers 284
Hl=σ 
((1−αl)Hl−1G+αlH0)((1−βl)I+βlWl) +Bl
α
,
where the residual connection and identity mapping are consistent with GCNII. 285
EGNN. Each EGNN layer can be written as follows: 286
Hl=σ 
Wl(c1H0+c2Hl−1+ (1−cmin)Hl−1G)
, (7)
where c1, c2are learnable weights that satisfy c1+c2=cminwithcminbeing a hyperparameter. To
constrain Dirichlet energy, EGNN initializes trainable weights Wlas a diagonal matrix with explicit
singular values and regularizes them to keep the orthogonality during the model training. Ignoring
the activation function σ,Hl– node features at layer lof EGNN satisfies
cmin∥H0∥E≤ ∥Hl∥E≤cmax∥H0∥E,
where cmaxis the square of the maximal singular value of the initialization of W1. Similarly, we 287
modify EGNN to result in the following EGNN-SCT layer 288
Hl=σ 
Wl((1−cmin)Hl−1G+c1H0+c2Hl−1) +Bl
α
,
where everything remains the same as the EGNN layer except that we add our proposed SCT Bl
α. 289
76 Experiments 290
In this section, we comprehensively demonstrate the effects of SCT – in the three most representative 291
GCN-style models discussed in Section 5 – using various node classification benchmarks. The 292
purpose of all experiments in this section is to verify the efficacy of the proposed SCT – motivated 293
by our theoretical results – for GCN-style models. We consider the citation datasets (Cora, Citeseer, 294
PubMed, Coauthor-Physics, Ogbn-arxiv), web knowledge-base datasets (Cornell, Texas, Wisconsin), 295
and Wikipedia network datasets (Chameleon, Squirrel). We provide additional dataset details in 296
Appendix D.1. We implement baseline GCN [ 20] and GCNII [ 6] (without weight sharing) using PyG 297
(Pytorch Geometric) [10]. Baseline EGNN [40] is implemented using the public code4. 298
6.1 Node feature trajectory 299
a)α=−0.25 b)α= 0.0 c)α= 1.0
Figure 2: Node feature trajectories, with colorized
magnitude, for varying smoothness control param-
eterα. For classical GCN b), the node features
converge to the eigenspace M(red dashed line).We visualize the trajectory of the node features, fol- 300
lowing [ 27], for a graph with two nodes connected 301
by an edge and 1D node feature. In this case, (6) 302
becomes h1=σ(wh0G+bα), where w= 1.2in 303
our experiment, h0,h1,bα∈R2, andG∈R2×2. 304
We use a matrix G= [0.592,0.194; 0 .194,0.908] 305
whose largest eigenvalue is 1. Twenty initial node 306
feature vectors h0are sampled evenly in the domain 307
[−1,1]×[−1,1]. Fig. 2 shows the trajectories in 308
relation to the eigenspace M(red dashed line). In Fig 2a), one can see that some trajectories do not 309
directly converge to M. In Fig. 2b) when α= 0.0, GCL is recovered and all trajectories converge to 310
M. In Fig. 2c), large values of αenable the features to significantly deviate from Minitially. We 311
observe that the parameter αcan effectively change the trajectory of features. 312
Layers 2 4 16 32
Cora
GCN/GCN-SCT 81.1/82.9 80.4/82.8 64.9/71.4 60.3/67.2
GCNII/GCNII-SCT 82.2/83.8 82.6/84.3 84.6/84.8 85.4/85.5
EGNN/EGNN-SCT 83.2/84.1 84.2/84.5 85 .4/83.3 85.3/82.0
Citeseer
GCN/GCN-SCT 70.3/69.9 67 .6/67.7 18.3/55.4 25.0/51.0
GCNII/GCNII-SCT 68.2/72.8 68.9/72.8 72.9/73.8 73 .4/73.4
EGNN/EGNN-SCT 72.0/73.1 71.9/72.0 72.4/72.6 72.3/72.9
PubMed
GCN/GCN-SCT 79.0/79.8 76.5/78.4 40.9/76.1 22.4/77.0
GCNII/GCNII-SCT 78.2/79.7 78.8/80.1 80.2/80.7 79.8/80.7
EGNN/EGNN-SCT 79.2/79.8 79.5/80.4 80.1/80.3 80.0/80.4
Coauthor-Physics
GCN/GCN-SCT 92.4/92.6±1.6 92.1/92.5±5.9 13.5/50.9±15.0 13.1/43.6±16.0
GCNII/GCNII-SCT 92.5/94.4±0.4 92.9/94.2±0.3 92.9/93.7±0.7 92.9/94.1±0.3
EGNN/EGNN-SCT 92.6/93.9±0.7 92.9/94.1±0.4 93.1/94.0±0.7 93.3/93.8±1.3
Ogbn-arxiv
GCN/GCN-SCT 70.4/72.1±0.3 71.7/72.7±0.3 70.6/72.3±0.2 68.5/72.3±0.3
GCNII/GCNII-SCT 70.1/72.0±0.3 71.4/72.2±0.2 71.5/72.4±0.3 70.5/72.1±0.3
EGNN/EGNN-SCT 68.4/68.5±0.6 71.1/71.3±0.5 72.7/72.8±0.5 72 .7/72.3±0.5
Table 1: Accuracy for models of varying depth. We note vanishing gradients occur but not over-smoothing for
the accuracy drop using GCN-SCT with 16 or 32 layers. For Cora, Citeseer, and PubMed, we use a fixed split
with a single forward pass following [ 6]; only test accuracy is available in these experiments. For Coauthor-
Physics and Ogbn-arxiv, we use the splits from [ 40]; both test accuracy and standard deviation are reported. The
baseline results are copied from [6, 40] where the standard deviation was not reported. (Unit:%)
6.2 Baseline comparisons for node classification 313
Citation networks. We compare the three representative models discussed in Section 5, of different 314
depths, with and without SCT in Table 1. This task uses the citation datasets with fixed splits from 315
[35] for Cora, Citeseer, and Pubmed and splits from [ 40] for Coauthor-Physics and Ogbn-arxiv; a 316
detailed description of these datasets and splits are provided in Appendix D. Following [ 6], we use a 317
single training pass to minimize the negative log-likelihood loss using the Adam optimizer [ 19], with 318
1500 maximum epochs, and 100epochs of patience. A grid search for possible hyperparameters is 319
listed in Table 5 in Appendix D. We accelerate the hyperparameter search by applying a Bayesian 320
meta-learning algorithm [ 2] which minimizes the validation loss, and we run the search for 200 321
iterations per model. In particular, Table 1 presents the best test accuracy between ReLU and leaky 322
ReLU for GCN, GCNII, and all three models with SCT5. For the baseline EGNN, we follow [ 40] 323
using SReLU, a particular activation used for EGNN in [ 40]. These results show that SCT can boost 324
4https://github.com/Kaixiong-Zhou/EGNN
5A comparison of the results using ReLU and leaky ReLU is presented in Appendix D.
8the classification accuracy of baseline models; in particular, the improvement can be remarkable for 325
GCN and GCNII. However, EGNN-SCT (using ReLU or leaky ReLU) performs occasionally worse 326
than EGNN (using SReLU), and this is because of the choice of activation functions. In Appendix D.3, 327
we report the results of EGNN-SCT using SReLU, showing that EGNN-SCT outperforms EGNN in 328
all tasks. In fact, SReLU is a shifted version of ReLU, and our theory for ReLU applies to SReLU as 329
well. The model size and computational time are reported in Table 4 in the appendix. 330
Table 1 also shows that even with SCT, the accuracy of GCN drops when the depth is 16 or 32. This 331
motivates us to investigate the smoothness of the node features learned by GCN and GCN-SCT. Fig. 3 332
plots the heatmap of the normalized smoothness of each dimension of the learned node features 333
learned by GCN and GCN-SCT with 32 layers for Citeseer node classification. In these plots, the 334
horizontal and vertical dimensions denote the feature dimension and the layer of the model, resp. 335
We notice that the normalized smoothness of each dimension of the features – from layers 14 to 32 336
learned by GCN – closes to 1, confirming that deep GCN learns homogeneous features. In contrast, 337
the features learned by GCN-SCT are inhomogeneous, as shown in Fig. 3b). Therefore, we believe the 338
performance degradation of deep GCN-SCT is due to other factors. Compared to GCNII/GCNII-SCT 339
and EGNN/EGNN-SCT, GCN-SCT does not use skip connections, which is known to help avoid 340
vanishing gradients in training deep neural networks [ 16,17]. In Appendix D.3, we show that training 341
GCN and GCN-SCT do suffer from the vanishing gradient issue; however, the other models do not. 342
Besides Citeseer, we notice similar behavior occurs for training GCN and GCN-SCT for Cora and 343
Coauthor-Physics node classification tasks. 344
1816
dim1
16
32 Layer
0.00.20.40.60.81.0
1816
dim1
16
32 Layer
0.00.20.40.60.81.0
a) GCN b) GCN-SCT
Figure 3: The normalized smoothness – of
each dimension of the feature vectors at a
given layer – for a) GCN and b) GCN-SCT
on the Citeseer dataset with 32 layers and
16 hidden dimensions. GCN features be-
come entirely smooth since layer 14, while
GCN-SCT controls the smoothness for each
feature at any depth. Horizontal and verti-
cal axes represent the index of the feature
dimension and the intermediate layer, resp.Other datasets. We further compare different models 345
trained on different datasets using 10-fold cross-validation 346
and fixed 48/32/20% splits following [ 28]. Table 2 com- 347
pares GCN and GCNII with and without SCT, using leaky 348
ReLU, for classifying five heterophilic node classification 349
datasets. We exclude EGNN as these heterophilic datasets 350
are not considered in [ 40]. We report the average accu- 351
racy of GCN and GCNII from [ 6]. We tune all other 352
models using a Bayesian meta-learning algorithm to max- 353
imize the mean validation accuracy. We report the best 354
test accuracy for each model of depth searched over the set 355
{2,4,8,16,32}. SCT can significantly improve the clas- 356
sification accuracy of the baseline models. Table 2 also 357
contrasts the computational time (on Tesla T4 GPUs from 358
Google Colab) per epoch of models that achieve the best 359
test accuracy; the models using SCT can even save compu- 360
tational time to achieve the best accuracy which is because 361
the best accuracy is achieved at a moderate depth (Table 8 in Appendix D.4 lists the mean and 362
standard deviation for the test accuracies on all five datasets. Table 9 in Appendix D.4 lists the 363
computational time per epoch for each model of depth 8, showing that using SCT only takes a small 364
amount of computational overhead.
Cornell Texas Wisconsin Chameleon Squirrel
52.70/55.95(0.7/1.8)52.16/62.16(0.7/0.8)45.88/54.71(0.7/0.8)28.18/38.44(0.6/0.7)23.96/35.31(1.6/4.0)
74.86/75.41(2.0/2.0) 69.46/83.34 (3.1/2.0) 74.12/86.08(2.0/1.5)60.61/64.52(1.5/1.3)38.47/47.51(5.5/3.7)
Table 2: Mean test accuracy and average computational time per epoch (in the parenthesis) for the We-
bKB and WikipediaNetwork datasets with fixed 48/32/20% splits. First row: GCN/GCN-SCT. Second row:
GCNII/GCNII-SCT. (Unit:% for accuracy and ×10−2second for computational time.)365
7 Concluding Remarks 366
In this paper, we establish a geometric characterization of how ReLU and leaky ReLU affect the 367
smoothness of the GCN features. We further study the dimension-wise normalized smoothness of the 368
learned node features, showing that activation functions not only smooth node features but also can 369
reduce or preserve the normalized smoothness of the features. Our theoretical findings inform the 370
design of a simple yet effective SCT for GCN. The proposed SCT can change the smoothness, in 371
terms of both normalized and unnormalized smoothness, of the learned node features by GCN. 372
Limitations: Our proposed SCT provides provable guarantees for controlling the smoothness of 373
features learned by GCN and related models. A key aspect to establish our theoretical results is 374
demonstrating that, without SCT, the features of the vanilla model tend to be overly smooth; without 375
this condition, SCT cannot ensure performance guarantees. 376
98 Broader Impacts 377
Our paper focuses on developing new theoretical understandings of the smoothness of node features 378
learned by graph convolutional networks. The paper is mainly theoretical. We do not see any potential 379
ethical issues in our research; all experiments are carried out using existing benchmark settings and 380
datasets. 381
Our paper brings new insights into building new graph neural networks with improved performance 382
over existing models, which is crucial for many applications. In particular, for applications where 383
graph neural network is the method of choice. We expect our approach to play a role in material 384
science and biophysics applications. 385
References 386
[1]Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius 387
Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan 388
Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint 389
arXiv:1806.01261 , 2018. 390
[2]Lukas Biewald. Experiment tracking with weights and biases, 2020. Software available from 391
wandb.com. 392
[3]Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and 393
deep locally connected networks on graphs. In 2nd International Conference on Learning 394
Representations, ICLR 2014 , 2014. 395
[4]Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. arXiv preprint 396
arXiv:2006.13318 , 2020. 397
[5]Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the 398
over-smoothing problem for graph neural networks from the topological view. In Proceedings 399
of the AAAI Conference on Artificial Intelligence , volume 34, pages 3438–3445, 2020. 400
[6]Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep 401
graph convolutional networks. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 402
37th International Conference on Machine Learning , volume 119 of Proceedings of Machine 403
Learning Research , pages 1725–1735. PMLR, 13–18 Jul 2020. 404
[7]Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph 405
neural networks. In International Conference on Learning Representations , 2019. 406
[8] Fan RK Chung. Spectral graph theory , volume 92. American Mathematical Soc., 1997. 407
[9]Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks 408
on graphs with fast localized spectral filtering. Advances in neural information processing 409
systems , 29, 2016. 410
[10] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. 411
InICLR Workshop on Representation Learning on Graphs and Manifolds , 2019. 412
[11] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Günnemann. Combining neural 413
networks with personalized pagerank for classification on graphs. In International Conference 414
on Learning Representations , 2019. 415
[12] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. 416
Neural message passing for quantum chemistry. In Proceedings of the 34th International 417
Conference on Machine Learning - Volume 70 , ICML’17, page 1263–1272. JMLR.org, 2017. 418
[13] William Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large 419
graphs. Advances in neural information processing systems , 30, 2017. 420
[14] William L Hamilton. Graph representation learning . Morgan & Claypool Publishers, 2020. 421
10[15] William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods 422
and applications. arXiv preprint arXiv:1709.05584 , 2017. 423
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- 424
age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern 425
Recognition (CVPR) , June 2016. 426
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual 427
networks. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The 428
Netherlands, October 11–14, 2016, Proceedings, Part IV 14 , pages 630–645. Springer, 2016. 429
[18] Tatsuro Kawamoto, Masashi Tsubaki, and Tomoyuki Obuchi. Mean-field theory of graph neural 430
networks in graph partitioning. Advances in Neural Information Processing Systems , 31, 2018. 431
[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint 432
arXiv:1412.6980 , 2014. 433
[20] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional 434
networks. In International Conference on Learning Representations , 2017. 435
[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep 436
convolutional neural networks. Communications of the ACM , 60(6):84–90, 2017. 437
[22] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks 438
for semi-supervised learning. In Thirty-Second AAAI conference on artificial intelligence , 2018. 439
[23] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen 440
Chang, and Doina Precup. Revisiting heterophily for graph neural networks. In Alice H. 441
Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural 442
Information Processing Systems , 2022. 443
[24] Yimeng Min, Frederik Wenkel, and Guy Wolf. Scattering gcn: Overcoming oversmoothness in 444
graph convolutional networks. Advances in Neural Information Processing Systems , 33:14498– 445
14508, 2020. 446
[25] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. 447
InProceedings of the 27th international conference on machine learning (ICML-10) , pages 448
807–814, 2010. 449
[26] Hoang Nt and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass 450
filters. arXiv preprint arXiv:1905.09550 , 2019. 451
[27] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for 452
node classification. In International Conference on Learning Representations , 2020. 453
[28] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- 454
metric graph convolutional networks. In International Conference on Learning Representations , 455
2020. 456
[29] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep 457
graph convolutional networks on node classification. In International Conference on Learning 458
Representations , 2020. 459
[30] Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua 460
Bengio. Graph attention networks. In International Conference on Learning Representations , 461
2018. 462
[31] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. 463
Simplifying graph convolutional networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, 464
editors, Proceedings of the 36th International Conference on Machine Learning , volume 97 of 465
Proceedings of Machine Learning Research , pages 6861–6871. PMLR, 09–15 Jun 2019. 466
[32] Xinyi Wu, Zhengdao Chen, William Wei Wang, and Ali Jadbabaie. A non-asymptotic analysis 467
of oversmoothing in graph neural networks. In The Eleventh International Conference on 468
Learning Representations , 2023. 469
11[33] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A 470
comprehensive survey on graph neural networks. IEEE transactions on neural networks and 471
learning systems , 32(1):4–24, 2020. 472
[34] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and 473
Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In 474
International conference on machine learning , pages 5453–5462. PMLR, 2018. 475
[35] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning 476
with graph embeddings. In International conference on machine learning , pages 40–48. PMLR, 477
2016. 478
[36] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure 479
Leskovec. Graph convolutional neural networks for web-scale recommender systems. In 480
Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & 481
data mining , pages 974–983, 2018. 482
[37] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International 483
Conference on Learning Representations , 2020. 484
[38] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Schölkopf. 485
Learning with local and global consistency. Advances in neural information processing systems , 486
16, 2003. 487
[39] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng 488
Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and 489
applications. AI Open , 1:57–81, 2020. 490
[40] Kaixiong Zhou, Xiao Huang, Daochen Zha, Rui Chen, Li Li, Soo-Hyun Choi, and Xia Hu. 491
Dirichlet energy constrained learning for deep graph neural networks. Advances in Neural 492
Information Processing Systems , 34:21834–21846, 2021. 493
[41] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian 494
fields and harmonic functions. In Proceedings of the 20th International conference on Machine 495
learning (ICML-03) , pages 912–919, 2003. 496
12Appendix for “Learning to Control the Smoothness of GCN 497
Features" 498
A Details of Notations 499
For two vectors u= (u1, u2, . . . , u d)andv= (v1, v2, . . . , v d), their inner product is defined as
⟨u,v⟩=dX
i=1uivi,
their Hadamard product is defined as
u⊙v= (u1v1, u2v2, . . . , u dvd),
and their Kronecker product is defined as
u⊗v=uv⊤=
u1v1u1v2. . . u 1vd
u2v1u2v2. . . u 2vd
............
udv1udv2. . . u dvd
.
The Kronecker product can be defined for two vectors of different lengths in a similar manner as 500
above. 501
B Proofs in Section 3 502
First, we prove that the two smoothness notions used in [ 27,4] are two equivalent seminorms, i.e., 503
we prove Proposition 3.1 below. 504
Proof of Proposition 3.1. The matrix Hcan be decomposed as H=Pn
i=1Heie⊤
i, where each ei
is the eigenvector of Gassociated with eigenvalue λi. This indicates that
H˜∆ =H(I−G)
=nX
i=1Heie⊤
i(I−G)
=nX
i=1(Heie⊤
i−Heie⊤
iG)
=nX
i=1(Heie⊤
i−Hei(λiei)⊤)
=nX
i=1(1−λi)Heie⊤
i
=nX
i=m+1(1−λi)Heie⊤
i.
13Then using the fact that 1−λi≥0for each i, we obtain
∥H∥2
E= Trace( H˜∆H⊤)
= Trace nX
i=m+1(1−λi)Heie⊤
i(nX
j=1Heje⊤
j)⊤
= Trace nX
i=m+1nX
j=1(1−λi)Heie⊤
ieje⊤
jH⊤
= Trace nX
i=m+1(1−λi)Heie⊤
ieie⊤
iH⊤
= Trace nX
i=m+1p
1−λiHeie⊤
ieie⊤
iH⊤p
1−λi
= Trace nX
i=m+1p
1−λiHeie⊤
i(nX
j=m+1p
1−λjHeje⊤
j)⊤
=nX
i=m+1p
1−λiHeie⊤
i2
F.
That is,
∥H∥E=nX
i=m+1p
1−λiHeie⊤
i
F.
On the other hand, (3) implies
∥H∥M⊥=∥HM⊥∥F=nX
i=m+1Heie⊤
i
F.
We first show that both ∥H∥M⊥and∥H∥Eare seminorms. Since ∥cH∥F=|c| · ∥H∥Ffor any
c∈R, we have ∥cH∥M⊥=|c| · ∥H∥M⊥and∥cH∥E=|c| · ∥H∥E. Moreover, for any two
matrices H1andH2s.t.H=H1+H2, we have
nX
i=m+1H1eie⊤
i+nX
i=m+1H2eie⊤
i=nX
i=m+1Heie⊤
i,
nX
i=m+1p
1−λiH1eie⊤
i+nX
i=m+1p
1−λiH2eie⊤
i=nX
i=m+1p
1−λiHeie⊤
i.
Then the triangle inequality of ∥ · ∥Fimplies that of ∥H∥M⊥and∥H∥E, respectively. 505
Now since 0<1−λm+1≤1−λi≤2for any i=m+ 1, . . . , n , we may take α=p
1−λm+1
andβ=√
2. Then
α∥H∥M⊥=αnX
i=m+1Heie⊤
i
F≤nX
i=m+1p
1−λiHeie⊤
i
F
≤βnX
i=m+1Heie⊤
i
F
=β∥H∥M⊥.
The result thus follows from ∥H∥E=Pn
i=m+1√1−λiHeie⊤
i
F. 506
B.1 ReLU 507
We present a crucial tool to characterize how ReLU affects its input. 508
14Lemma B.1. LetZ∈Rd×n, and let Z+= max( Z,0)andZ−= max( −Z,0)be the positive and 509
negative parts of Z. Then (1) Z+,Z−are (component-wise) nonnegative and Z=Z+−Z−and 510
(2)⟨Z+,Z−⟩F= 0. 511
Proof of Lemma B.1. Notice that for any a∈R, we have
max( a,0) =aifa≥0
0otherwiseandmax(−a,0) =0 ifa≥0
−aotherwise.
This implies that a= max( a,0)−max(−a,0)andmax( a,0)·max(−a,0) = 0 . 512
LetZijbe the (i, j)thentry of Z. Then Z=Z+−Z−follows from Zij= max( Zij,0)−
max(−Zij,0). Also, one can deduce that
⟨Z+,Z−⟩F= Trace(( Z+)⊤Z−) =dX
i=1jX
j=1max( Zij,0) max( −Zij,0) = 0 .
513
Before proving Proposition 3.2, we notice the following relation between ZandH. 514
Lemma B.2. Given Z∈Rd×n, letH=σ(Z)withσbeing ReLU, then Hlies on the high- 515
dimensional sphere, in ∥ · ∥Fnorm, that is centered at Z/2and with radius ∥Z/2∥F. That is, H 516
andZsatisfy the following equation 517
H−Z
22
F=Z
22
F. (8)
Proof of Lemma B.2. We observe that H=σ(Z) = max( Z,0) =Z+is the positive part of Z.
Then
⟨H,Z⟩F=⟨H,Z+−Z−⟩F=⟨H,Z+⟩F− ⟨H,Z−⟩F=⟨H,H⟩F,
where we have used Z=Z+−Z−and⟨H,Z−⟩F=⟨Z+,Z−⟩F= 0from Lemma B.1. 518
Therefore, one can deduce the desired result as follows
⟨H,H⟩F− ⟨H,Z⟩F= 0⇒∥H∥2
F−2D
H,Z
2E
F+Z
22
F=Z
22
F
⇒H−Z
22
F=Z
22
F.
519
Applying ∥H∥2
F=∥HM+HM⊥∥2
F=∥HM∥2
F+∥HM⊥∥2
F, to bothZ
2andH−Z
2, we obtain
Z
22
F=ZM⊥
22
F+ZM
22
F,
andH−Z
22
F=HM⊥−ZM⊥
22
F+HM−ZM
22
F.
Then (8) becomes 520
ZM⊥
22
F−HM⊥−ZM⊥
22
F=HM−ZM
22
F−ZM
22
F(9)
By direct calculation, we have 521
HM−ZM
22
F−ZM
22
F=⟨HM,HM⟩F−2D
HM,ZM
2E
F
=⟨HM,HM−ZM⟩F.(10)
Combining (9) and (10), we obtain the following result 522
15Lemma B.3. For any Z=ZM+ZM⊥, letH=σ(Z) =HM+HM⊥, then 523
ZM⊥
22
F−HM⊥−ZM⊥
22
F=⟨Z+
M,Z−
M⟩F.
where Z+
M=Pm
i=1Z+eie⊤
i,Z−
M=Pm
i=1Z−eie⊤
i. 524
Proof of Lemma B.3. Recall that H=σ(Z) = max( Z,0) =Z+. Also, Z=Z+−Z−implies
ZM=Z+
M−Z−
M=H+
M−Z−
M. Therefore, we see that
⟨HM,HM−ZM⟩F=⟨Z+
M,Z−
M⟩F.
525
By using the fact that ⟨Z+
M,Z−
M⟩F≥0in Lemma B.3, we reveal a geometric relation between Z 526
andHmentioned in Proposition 3.2. 527
Proof of Proposition 3.2. Since Z+,Z−≥0are nonnegative and all the eigenvectors eiare also
nonnegative, we see that Z+
M=Pm
i=1Z+eie⊤
iandZ−
M=Pm
i=1Z−eie⊤
iare nonnegative. This
indicates that
⟨Z+
M,Z−
M⟩F= Trace
Z+
M(Z−
M)⊤
≥0.
Then according to Lemma B.3, we obtain
ZM⊥
22
F−HM⊥−ZM⊥
22
F=⟨Z+
M,Z−
M⟩F≥0.
So we haveHM⊥−ZM⊥
2
F=rZM⊥
22
F− ⟨Z+
M,Z−
M⟩F
=rZM⊥
22
F− ⟨HM,HM−ZM⟩F,
which shows that HM⊥lies on the high-dimensional sphere that we have claimed. Furthermore, we 528
conclude that 529
0≤HM⊥−ZM⊥
2
F≤ZM⊥
2
F. (11)
This demonstrates that HM⊥lies on the high-dimensional sphere we have stated. 530
Since the sphereHM⊥−ZM⊥
22
F=ZM⊥
22
Fpasses through the origin, the distance of any
HM⊥to the origin must be no greater than the diameter of this sphere, i.e., ∥HM⊥∥F≤ ∥ZM⊥∥F.
Also, this can be derived from
∥HM⊥∥F−ZM⊥
2
F≤HM⊥−ZM⊥
2
F≤ZM⊥
2
F.
One can see that the maximal smoothness ∥HM⊥∥F=∥ZM⊥∥Fis attained when HM⊥=ZM⊥, 531
the intersection of the surface and the line passing through the center and the origin. 532
After all, we complete the proof by using the fact that ∥ZM⊥∥F=∥Z∥M⊥for any matrix Z, which 533
implies ∥H∥M⊥=∥HM⊥∥F≤ ∥ZM⊥∥F=∥Z∥M⊥. 534
535
B.2 Leaky ReLU 536
For the leaky ReLU activation function, we have 537
Lemma B.4. IfH=σa(Z)withσabeing leaky ReLU, then Hlies on the high-dimensional sphere 538
centered at (1 +a)Z/2with radius ∥(1−a)Z/2∥F. 539
16Proof of Lemma B.4. Notice that
H=σa(Z) =Z+−aZ−.
ThenH−Z= (1−a)Z−andH−aZ= (1−a)Z+. Using ⟨Z−,Z+⟩F= 0, we have 540
⟨H−Z,H−aZ⟩F= 0⇒∥H∥2
F−2D
H,(1 +a)Z
2E
F+a∥Z∥2
F= 0
⇒∥H∥2
F−2D
H,(1 +a)Z
2E
F=−a∥Z∥2
F
⇒H−(1 +a)
2Z2
F=(1 +a)
2Z2
F−a∥Z∥2
F=(1−a)
2Z2
F.
541
Moreover, we notice that 542
Lemma B.5. For any Z=ZM+ZM⊥, letH=σa(Z) =HM+HM⊥, then 543
(1−a)
2ZM⊥2
F−HM⊥−(1 +a)
2ZM⊥2
F= (1−a)2⟨Z+
M,Z−
M⟩F
Proof of Lemma B.5. Similar to the proof of Lemma B.3, the orthogonal decomposition implies that
(1−a)
2ZM⊥2
F−HM⊥−(1 +a)
2ZM⊥2
F=HM−(1 +a)
2ZM2
F−(1−a)
2ZM2
F
=⟨HM−ZM,HM−aZM⟩F
=⟨(1−a)Z−
M,(1−a)Z+
M⟩F
=(1−a)2⟨Z−
M,Z+
M⟩F.
544
Proof of Proposition 3.3. Similar to the proof of Proposition 3.2, we apply ⟨Z−
M,Z+
M⟩F≥0to
Lemma B.5 and hence obtain the geometric condition as follows
HM⊥−(1 +a)
2ZM⊥
F=r(1−a)
2ZM⊥2
F− ⟨HM−ZM,HM−aZM⟩F.
Then we have the following inequality
0≤HM⊥−(1 +a)
2ZM⊥
F≤(1−a)
2ZM⊥
F.
Moreover, we deduce that
∥HM⊥∥F−(1 +a)
2ZM⊥
F≤HM⊥−(1 +a)
2ZM⊥
F≤(1−a)
2ZM⊥
F.
and hence
−(1−a)
2ZM⊥
F≤ ∥HM⊥∥F−(1 +a)
2ZM⊥
F≤(1−a)
2ZM⊥
F.
Therefore, we obtain a∥ZM⊥∥F≤ ∥HM⊥∥F≤ ∥ZM⊥∥F. (Remark that HM⊥achieves its 545
maximal norm when it is equal to ZM⊥, the intersection of the surface and the line passing through 546
the center and the origin. ) 547
By using the fact that ∥ZM⊥∥F=∥Z∥M⊥for any matrix Z, we conclude that a∥Z∥M⊥≤ 548
∥H∥M⊥≤ ∥Z∥M⊥. 549
17C Proofs in Section 4 550
Throughout this section, we assume that zM⊥̸=0. 551
Proof of Proposition 4.3. Recall that e=˜D1
2un/chas only positive entries where ˜Dis the aug- 552
mented degree matrix and un= [1, . . . , 1]⊤∈Rnandc=∥˜D1
2un∥. Let dibe the ithdiagonal 553
entry of ˜D. Then we have e= [√d1/c,√d2/c, . . . ,√dn/c]⊤andc=pPn
i=1di. 554
Note that z(α) =z−αe=z−α
c˜D1
2un=˜D1
2(˜D−1
2z−α
cun) =˜D1
2(x−α
cun), where we
assume x:=˜D−1
2z. Then we observe that when σis the ReLU activation function,
h(α) =σ(z(α)) =σ
˜D1
2(x−α
cun)
=˜D1
2σ
x−α
cun
,
and hence
⟨h(α),e⟩=D
˜D1
2σ
x−α
cun
,eE
=D
σ
x−α
cun
,˜D1
2eE
=D
σ
x−α
cun
,˜DunE
.
We may now assume x= [x1, . . . , x n]⊤is well-ordered s.t. x1≥x2≥. . .≥xn. Indeed, there is a
collection of indices {k1, ..., k l}s.t.
x1=. . . , x k1andxk1> xk1+1,
xkj−1+1=. . .=xkjandxkj> xkj+1for any j= 2, . . . , l −1,
xkl−1+1=. . .=xklandkl=n.
That is, x1=x2=. . .=xk1> xk1+1=. . .=xk2> xk2+1=. . .=xk3> xk3+1. . . 555
We first restrict the domain of αs.t.h(α)̸= 0. Note that we have
h(α) = 0⇔σ
x−α
cun
= 0
⇔xi−α
c≤0fori= 1, . . . , n
⇔x1−α
c≤0
⇔α≥cx1.
So we will study the smoothness s(h(α))when α < cx 1. 556
Letϵ >0and consider α=c(x1−ϵ). When ϵ≤x1−xk1+1=x1−xk2, we see that
x−α
cun= [ϵ, . . . , ϵ, ϵ −(x1−xk1+1), . . . , ϵ −(x1−xn)]⊤,
where only the first k1entries are positive since x1−xi≥ϵfor any i≥k1+ 1. Therefore,
h(α) =˜D1
2σ
x−α
cun
=˜D1
2[ϵ, . . . , ϵ, 0, . . . , 0]⊤
= [ϵp
d1, . . . , ϵp
dk1,0, . . . , 0]⊤.
and hence we can compute that ∥h(α)∥=ϵvuutk1X
i=1di.Also, we have
∥h(α)∥M=|⟨h(α),e⟩|= [ϵp
d1, . . . , ϵp
dk1,0, . . . , 0]⊤[p
d1/c,p
d2/c, . . . ,p
dn/c]
=ϵ
ck1X
i=1di.
Then we obtain the smoothness s(h(α))as follows
s(h(α)) =∥h(α)∥M
∥h(α)∥=ϵ
cPk1
i=1di
ϵqPk1
i=1di=qPk1
i=1di
c=K1
c<1,
18where K1:=qPk1
i=1di. Similarly, we may denoteqPkj
i=kj−1+1dibyKjforj= 2, . . . , l . 557
Now we are going to show that the smoothness s(h(α))is increasing as αgets smaller whenever α < 558
cx1, implyingK1
cis the minimum of the smoothness s(h(α)). Remember that we are considering 559
α=c(x1−ϵ)and we have studied the case when 0< ϵ≤x1−xk1+1=x1−xk2. 560
Letδj:=x1−xkjfor1≤j≤l. Clearly, we have δ1= 0andδj< δj+1for1≤j≤l−1. Fix a
j′∈ {2, . . . , l −1}, we see that when δj′< ϵ≤x1−xkj′+1,
x−α
cun
=h
ϵ−δ1, . . . , ϵ −δ1, ϵ−δ2, . . . , ϵ −δ2, ϵ−δ3, . . . , ϵ −δj′, ϵ−(x1−xkj′+1), . . . , ϵ −(x1−xn)i⊤
,
where we have ϵ−δj>0for2≤j≤j′andϵ−(x1−xi)≤0for any i≥kj′+ 1. Consequently,
h(α) =˜D1
2σ(x−α
cun) = [( ϵ−δ1)p
d1, . . . , (ϵ−δ1)p
dk1,(ϵ−δ2)p
dk1+1, . . . , (ϵ−δ2)p
dk2,
(ϵ−δ3)p
dk2+1, . . . , (ϵ−δj′)q
dkj′,0, . . . , 0]⊤.
Then we can compute
∥h(α)∥=vuuutj′X
j=1kjX
i=kj−1+1di(ϵ−δj)2=vuutj′X
j=1K2
j(ϵ−δj)2,
where we set k0:= 0for simplicity and Kj=qPkj
i=kj−1+1diforj= 1, . . . , j′. Also, we have
∥h(α)∥M=|⟨h(α),e⟩|=j′X
j=1kjX
i=kj−1+1di(ϵ−δj)
c=1
cj′X
j=1K2
j(ϵ−δj).
A careful calculation shows that∂
∂ϵs(h(α))>0whenever δj′< ϵ≤x1−xkj′+1which implies that
s(h(α))is increasing as ϵincreases. Indeed, we have
∂
∂ϵs(h(α))
=∂
∂ϵ Pj′
j=1K2
j(ϵ−δj)
cqPj′
j=1K2
j(ϵ−δj)2!
=
∂
∂ϵPj′
j=1K2
j(ϵ−δj)qPj′
j=1K2
j(ϵ−δj)2−Pj′
j=1K2
j(ϵ−δj)
∂
∂ϵqPj′
j=1K2
j(ϵ−δj)2
cPj′
j=1K2
j(ϵ−δj)2
=Pj′
j=1K2
jqPj′
j=1K2
j(ϵ−δj)2−Pj′
j=1K2
j(ϵ−δj)∂
∂ϵPj′
j=1K2
j(ϵ−δj)2
2qPj′
j=1K2
j(ϵ−δj)2
cPj′
j=1K2
j(ϵ−δj)2
=Pj′
j=1K2
jPj′
j=1K2
j(ϵ−δj)2−Pj′
j=1K2
j(ϵ−δj)Pj′
j=1K2
j(ϵ−δj)
cPj′
j=1K2
j(ϵ−δj)2qPj′
j=1K2
j(ϵ−δj)2.
Then to show that∂
∂ϵs(h(α))>0, it suffices to show that the numerator is positive, i.e.
j′X
j=1K2
jj′X
j=1K2
j(ϵ−δj)2−j′X
j=1K2
j(ϵ−δj)2
>0,
19since the denominator cPj′
j=1K2
j(ϵ−δj)2qPj′
j=1K2
j(ϵ−δj)2>0is always positive. In fact, this
follows from the Cauchy inequality ∥v∥∥u∥ ≥ ⟨v,u⟩, where we set
v:= [K1, K2, . . . , K J′]⊤,u:= [K1(ϵ−δ1), K2(ϵ−δ2), . . . , K j′(ϵ−δj′)]⊤.
Moreover, equality happens only when vis parallel to u. This is, however, impossible since 561
ϵ−δj> ϵ−δj+1for any j= 1, . . . , j′−1and each Kjis positive. 562
So we see that s(h(α))is increasing as ϵincreases whenever 0< ϵ, and hence the smoothness 563
s(h(α))is increasing as αdecreases whenever cxn≤α < cx 1. 564
For the case j′=lwhere δl=x1−xn< ϵ, we have xn−α/c=xn−(x1−ϵ) =ϵ−(x1−xn)>0,
implying α < cx nandh(α) =z(α). We have shown that the smoothness is increasing as αis going
far from ⟨z,e⟩; in particular, when α <⟨z,e⟩andαis decreasing. One can check that
cxn=Pn
i=1dixn
c=
xnun,˜Dun
c
≤
x,˜Dun
c
=
˜D1
2x,˜D1
2un
c
=⟨z,e⟩,
which means the smoothness is increasing as αdecreases whenever α < cx n. 565
We conclude that the smoothness increases as αdecreases provided α < cx 1. Also, we have 566
supα<cx 1s(h(α)) = 1 as the case in the proof of Proposition C.1. One can check that s(h(α))is a 567
continuous function for α < cx 1and thus it has range [K1/c,1)by the mean value theorem. 568
Finally, we can establish the result: K1/c=rP
xi=max xdiPn
j=1djis the minimum of s(h(α))and1is the 569
maximum of s(h(α))occurring whenever α≥cx1=qPn
j=1djmax ixi. Moreover, s(h(α))has 570
a monotone property when α <qPn
j=1djmax ixiand has rangehrP
xi=max xdiPn
j=1dj,1i
. 571
It is clear that the assumption on the ordering of the entries of xwill not affect this result. 572
To prove Proposition 4.4, we first prove an analogous result for the identity function, that is, h= 573
σ(z) =z. 574
Proposition C.1. Suppose zM⊥̸=0, then s(z(α))achieves its minimum 0ifα=⟨z,e⟩. Moreover, 575
supαs(z(α)) = 1 where s(z(α))is close to 1when αis far away from ⟨z,e⟩. 576
Notice that Proposition C.1 does not consider the activation function. 577
Proof of Proposition C.1. We know that 0≤s(z(α))≤1and
s(z(α)) =s
1−∥zM⊥∥2
∥z(α)∥2=s
1−∥zM⊥∥2
∥zM⊥∥2+∥z(α)M∥2
=s
1−∥zM⊥∥2
∥zM⊥∥2+∥zM−αe∥2.
Suppose s(z(α)) = 1 . Then we have∥zM⊥∥2
∥zM⊥∥2+∥zM−αe∥2= 0which forces ∥zM⊥∥= 0. However, 578
this contradicts the hypothesis zM⊥̸= 0. Sos(z(α))cannot attain its maximum. 579
But for any 0≤t <1, one can see that s(z(α)) =tif and only if
s
1−∥zM⊥∥2
∥zM⊥∥2+∥zM−αe∥2=t⇔∥zM⊥∥2
∥zM⊥∥2+∥zM−αe∥2= 1−t2
⇔ ∥zM⊥∥2= (1−t2) 
∥zM⊥∥2+∥zM−αe∥2
⇔t2∥zM⊥∥2= (1−t2)∥zM−αe∥2
⇔ ∥zM−αe∥=r
t2
1−t2· ∥zM⊥∥
20This implies that supαs(z(α)) = 1 ands(z(α))achieves its minimum 0if and only if α=⟨z,e⟩. 580
It is clear that s(z(α))get closer to 1when αis going far away from ⟨z,e⟩. i.e.,|α− ⟨z,e⟩|= 581
∥zM−αe∥is increasing. 582
Proof of Proposition 4.4. First, we notice that leaky ReLU has the following two properties 583
1.σa(x)>0forx≫0andσa(x)<0forx≪0. 584
2.σais a non-trivial linear map for x≫0. 585
We will use Property 1to show that minαs(h(α)) = 0 and Property 2to show that supαs(h(α)) = 1 . 586
Notice that σa(x)<0forx≪0implies that there exists a sufficient small α2<0s.t. all of the 587
entries of h(α2)are negative and hence |⟨h(α2),e⟩|<0. Similarly, σa(x)>0forx≫0implies 588
that there exists a sufficient large α1>0s.t. all of the entries of h(α1)are positive and hence 589
|⟨h(α1),e⟩|>0. Since |⟨h(α),e⟩|is a continuous function of αon[α1, α2], the Intermediate 590
Value Theorem follows that there exists an α∈(α1, α2)s.t.|⟨h(α),e⟩|= 0. Thus by definition 591
s(h(α)) =|⟨h(α),e⟩|/∥h(α)∥, we see that minαs(h(α)) = 0 . 592
On the other hand, since σais a non-trivial linear map for x≫0, we may assume σa(x) =cxfor
x > x 0where c̸= 0 is some non-zero constant and x0>0is some positive constant. Then we
can choose an α0>⟨z,e⟩s.t. for any α≥α0, all of the entries of z(α)are greater than x0. Then
whenever α≥α0, we have h(α) =σa(z(α)) =cz(α). This implies
s(h(α)) =|⟨h(α),e⟩|
∥h(α)∥=|⟨cz(α),e⟩|
∥cz(α)∥=|⟨z(α),e⟩|
∥z(α)∥=s(z(α)).
Thus supαs(h(α)) = 1 follows from the Proof of Proposition C.1 where we see that supαs(z(α)) = 593
1since s(z(α))gets closer to 1asαincreases. 594
595
Remark C.2.Indeed, it holds for any continuous function f:R→Rsatisfying the following 596
1.f(x)>0forx≫0,f(x)<0forx≪0orf(x)<0forx≫0,f(x)>0forx≪0, 597
2.fis a non-trivial linear map for x≫0orx≪0. 598
One can check the proof above only depends on these two properties. It is worth mentioning that 599
most activation functions, e.g. leaky LU, SiLU, tanh , satisfy condition 1. 600
Proof of Corollary 4.5. For any α, we notice that ∥z∥M⊥=∥zM⊥∥F=∥z(α)∥M⊥since α 601
only changes the component of zin the eigenspace M. Also, Propositions 3.2 and 3.3 show 602
that∥z(α)∥M⊥≥ ∥h(α)∥M⊥whenever h(α) =σ(z(α))orσa(z(α)). Therefore, we see that 603
∥z∥M⊥≥ ∥h(α)∥M⊥holds for any α. Since zM⊥̸= 0,s(z)must lie in [0,1). 604
605
D Experimental Details 606
This part includes the missing details about experimental configurations and additional experimental 607
results for Section 6. All tasks we run using Nvidia RTX 3090, GV100, and Tesla T4 GPUs. All 608
computational performance metrics, including timing procedures, are run using Tesla T4 GPUs from 609
Google Colab. 610
D.1 Dataset details 611
In this section, we briefly describe the benchmark datasets used. Table 3 provides additional details 612
about the underlying graph representation. 613
Citation Datasets: The five citation datasets considered are Cora, Citeseer PubMed, Coauthor- 614
Physics, and Ogbn-arxiv. Each dataset is represented by a graph with nodes representing academic 615
publications, features encoding a bag-of-words description, labels classifying the publication type, 616
and edges representing citations. 617
21Web Knowledge-Base Datasets: The three web knowledge-base datasets are Cornell, Texas, and 618
Wisconsin. Each dataset is represented by a graph with nodes representing CS department webpages, 619
features encoding a bag-of-words description, edges representing hyper-link connections, and labels 620
classifying the webpage type. 621
Wikipedia Network Datasets: The two Wikipedia network datasets are Chameleon and Squirrel. 622
Each dataset is represented by a graph with nodes representing CS department webpages, features en- 623
coding a bag-of-words description, edges representing hyper-link connections, and labels classifying 624
the webpage type. 625
# Nodes # Edges # Features # Classes Splits (Train/Val/Test)
Cornell 183 295 1 ,703 5 48/32/20%
Texas 181 309 1 ,703 5 48/32/20%
Wisconsin 251 499 1 ,703 5 48/32/20%
Chameleon 2,277 36 ,101 2 ,325 5 48/32/20%
Squirrel 5,201 217 ,073 2 ,089 5 48/32/20%
Citeseer 3,727 4 ,732 3 ,703 6 120/500/1000
Cora 2,708 5 ,429 1 ,433 7 140/500/1000
PubMed 19,717 44 ,338 500 3 60/500/1000
Coauthor-Physics 34,493 247,962 8415 5 100/150/34,243
Ogbn-arxiv 169,343 1,166,243 128 40 90,941/29,799/48,603
Table 3: Graph statistics.
D.2 Model size and computational time for citation datasets 626
Table 4 compares the model size and computational time for experiments on citation datasets in 627
Section 6.2. 628
# Parameters Training Time (s) Inference Time (ms)
Cora
GCN 100,423 8.4 1.6
GCNII 110,535 10.0 2.1
GCNII 708,743 57.6 12.3
GCNII-SCT 1,237,127 110.3 29.6
EGNN 712,839 65.6 14.4
EGNN-SCT 316,551 24.8 4.5
Citeseer
GCN 245,638 8.3 1.5
GCN-SCT 301,830 15.5 4.0
GCNII 999,174 57.6 12.3
GCNII-SCT 1,001,222 65.9 15.7
EGNN 739,078 39.6 7.2
EGNN-SCT 540,934 24.0 5.8
PubMed
GCN 40,451 9.0 1.8
GCN-SCT 40,707 11.1 2.2
GCNII 326,659 98.2 12.8
GCNII-SCT 590,851 71.7 17.4
EGNN 592,899 93.7 2.5
EGNN-SCT 130,563 16.0 3.1
Coauthor-Physics
GCN 547,141 35.2 8.0
GCN-SCT 547,397 33.9 8.3
GCNII 555,333 49.1 10.3
GCNII-SCT 555,461 67.0 9.5
EGNN 672,069 176.4 47.9
EGNN-SCT 572,229 51.7 14.8
Ogbn-arxiv
GCN 27,240 50.4 21.1
GCN-SCT 28,392 62.6 24.4
GCNII 76,392 205.4 94.8
GCNII-SCT 80,616 253.0 108.9
EGNN 77,416 206.8 98.0
EGNN-SCT 81,640 254.0 112.3
Table 4: Number of model parameters for varying numbers of layers using the optimal model hyperparameters.
The SCT is added at each layer and the size of the additional parameters scales with the number of eigenvectors
with an eigenvalue of one for matrix Gin (2).
22D.3 Additional Section 6.2 details for citation datasets 629
Table 5 lists the hyperparameters used in the grid search in generating the results in Table 1. Also, 630
Table 7 reports the classification accuracy of different models with different depths using either ReLU 631
or leaky ReLU.
Parameter Values
Learning Rate {1e-4,1e-3,1e-2}
Weight Decay (FC) {0,1e-4,5e-4,1e-3,5e-3,1e-2}
Weight Decay (Conv) {0,1e-4,5e-4,1e-3,5e-3,1e-2}
Dropout {0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9}
Hidden Channels {16,32,64,128}
GCNII- α {0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9}
GCNII- θ {0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9}
EGNN- cmax {0.5,1.0,1.5,2.0}
EGNN- α {0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9}
EGNN- θ {0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9}
Table 5: Hyperparameter grid search for Table 1.
632
Layers 2 4 16 32
Cora
EGNN/EGNN-SCT 83.2/83.4 84.2/84.3 85.4/85.5 85.3/85.5
Citeseer
EGNN/EGNN-SCT 72.0/72.1 71.9/72.3 72.4/72.6 72.3/72.8
PubMed
EGNN/EGNN-SCT 79.2/79.4 79.5/79.8 80 .1/80.1 80.0/80.2
Coauthor-Physics
EGNN/EGNN-SCT 92.6/92.8 92.9/93.0 93.1/93.3 93 .3/93.3
Ogbn-arxiv
EGNN/EGNN-SCT 68.4/68.5 71.1/71.3 72.7/73.0 72.7/72.9
Table 6: Test accuracy for EGNN and EGNN-SCT using SReLU activation function of varying depth on citation
networks with the split discussed in Section 6.2. (Unit:%)
D.3.1 Vanishing gradients 633
Figure 4 shows the vanishing gradient problem for training deep GCN – with or without SCT – in 634
comparison to models like GCNII and EGNN. This figure plots ||∂Hout/∂Hl||for layers l∈[0,32] 635
as the training epochs run from 0to100. Figures 4 (a) and (b) illustrate the vanishing gradient issue 636
for GCN and that it persists for GCN-SCT. Figures 4 (c) and (e) illustrate that GCNII and EGNN 637
do not suffer from vanishing gradients, and furthermore, because these models connect H0to every 638
layer, the gradient with respect to the weights in the first layer is nonzero. What is interesting about 639
the addition of SCT to both EGNN and GCNII is that the intermediate gradients become large as the 640
training epochs progress shown in Figure 4 (d) and (f). 641
D.4 Additional Section 6.2 details for other datasets 642
Table 8 reports the mean test accuracy and standard deviation over ten folds of the WebKB and 643
WikipediaNetwork datasets using SCT-based models. 644
Table 9 lists the average computational time for each epoch for different models of the same depth 645
– 8 layers. These results show that integrating SCT into GNNs only results in a small amount of 646
computational overhead. 647
230 20 40 60 80
Epoch0
20Layer
0.000.050.10
0 20 40 60 80
Epoch0
20Layer
0.000.050.10
0 20 40 60 80
Epoch0
20Layer
0.000.050.10
(a) GCN (c) GCNII (e) EGNN
0 20 40 60 80
Epoch0
20Layer
0.000.050.10
0 20 40 60 80
Epoch0
20Layer
0.000.050.10
0 20 40 60 80
Epoch0
20Layer
0.000.050.10
(b) GCN-SCT (d) GCNII-SCT (f) EGNN-SCT
Figure 4: Training gradients for ||∂Hout/∂Hl||forl∈[0,32]layers and 100 training epochs on the Citeseer
dataset. Here, all models have 32 layers and 16 hidden dimensions for each layer. We observe that (a) GCN
suffers from vanishing gradients. By contrast (c) GCNII and (e) EGNN do not suffer from vanishing gradients,
and we can observe their skip connection to H0. Because these models (GCNII/GCNII-SCT and EGNN/EGNN-
SCT) connect H0to every layer, the gradient at the first layer is nonzero. We notice that while SCT does
not overcome vanishing gradients for (b) GCN-SCT, it is able to increase the norm of the gradients for the
intermediate layers in (d) GCNII-SCT and (f) EGNN-SCT.
Cora
ReLU leaky ReLU
Layers 2 4 16 32 2 4 16 32
GCN-SCT 81.2 80 .3 71 .4 67 .2 82.9 82 .8 68 .0 65 .5
GCNII-SCT 83.5 83 .8 82 .7 83 .3 83.8 84 .8 84 .8 85 .5
EGNN-SCT 84.1 83 .8 82 .3 80 .8 83.7 84 .5 83 .3 82 .0
Citeseer
ReLU leaky ReLU
Layers 2 4 16 32 2 4 16 32
GCN-SCT 69.0 67.3 51.5 50.3 69.9 67.7 55.4 51.0
GCNII-SCT 72.8 72 .8 72 .8 73 .3 72.8 72 .9 73 .8 72 .7
EGNN-SCT 72.5 72 .0 70 .2 71 .8 73.1 71 .7 72 .6 72 .9
PubMed
ReLU leaky ReLU
Layers 2 4 16 32 2 4 16 32
GCN-SCT 79.4 78.2 75.9 77.0 79.8 78.4 76.1 76.9
GCNII-SCT 79.7 80 .1 80 .7 80 .7 79.6 80 .0 80 .3 80 .7
EGNN-SCT 79.7 80 .1 80 .0 80 .4 79.8 80 .4 80 .3 80 .2
Coauthor-Physics
ReLU leaky ReLU
Layers 2 4 16 32 2 4 16 32
GCN-SCT 91.8±1.6 91 .6±3.0 44 .5±13.0 42 .6±17.0 92.6±1.6 92 .5±5.9 50 .9±15.0 43 .6±16.0
GCNII-SCT 94.4±0.4 93 .5±1.2 93 .7±0.7 93 .8±0.6 94.0±0.4 94 .2±0.3 93 .3±0.7 94 .1±0.3
EGNN-SCT 93.6±0.7 94 .1±0.4 93 .4±0.8 93 .8±1.3 93.9±0.7 94 .0±0.7 94 .0±0.7 93 .3±0.9
Ogbn-arxiv
ReLU leaky ReLU
Layers 2 4 16 32 2 4 16 32
GCN-SCT 71.7±0.3 72 .6±0.3 71 .4±0.2 71 .9±0.3 72.1±0.3 72 .7±0.3 72 .3±0.2 72 .3±0.3
GCNII-SCT 71.4±0.3 72 .1±0.3 72 .2±0.2 71 .8±0.2 72.0±0.3 72 .2±0.2 72 .4±0.3 72 .1±0.3
EGNN-SCT 68.5±0.6 71 .0±0.5 72 .8±0.5 72 .1±0.6 67.7±0.5 71 .3±0.5 72 .3±0.5 72 .3±0.5
Table 7: Test accuracy results for models of varying depth with ReLU or leaky ReLU activation function on the
citation network datasets using the split discussed in Section 6.2.
Cornell Texas Wisconsin Chameleon Squirrel
GCN-SCT 55.95±8.5 62 .16±5.7 54 .71±4.4 38 .44±4.3 35 .31±1.9
GCNII-SCT 75.41±2.2 83 .34±4.5 86 .08±3.8 64 .52±2.2 47 .51±1.4
Table 8: Test mean ±standard deviation accuracy from 10fold cross validation on five heterophilic datasets
with fixed 48/32/20% splits. The depth of each model is 8 layers with 16 hidden channels. (Unit: second)
Cornell Texas Wisconsin Chameleon Squirrel
GCN [20] 0.011 0 .013 0 .012 0 .011 0 .022
GCNII [6] 0.017 0 .018 0 .017 0 .013 0 .022
GCN-SCT 0.015 0 .017 0 .015 0 .011 0 .023
GCNII-SCT 0.017 0 .018 0 .017 0 .020 0 .025
Table 9: Average computational time per epoch for five heterophilic datasets with fixed 48/32/20% splits. The
depth of each model is 8 layers with 16 hidden channels. (Unit: second)
24NeurIPS Paper Checklist 648
The checklist is designed to encourage best practices for responsible machine learning research, 649
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove 650
the checklist: The papers not including the checklist will be desk rejected. The checklist should 651
follow the references and precede the (optional) supplemental material. The checklist does NOT 652
count towards the page limit. 653
Please read the checklist guidelines carefully for information on how to answer these questions. For 654
each question in the checklist: 655
• You should answer [Yes] , [No] , or [NA] . 656
•[NA] means either that the question is Not Applicable for that particular paper or the 657
relevant information is Not Available. 658
• Please provide a short (1–2 sentence) justification right after your answer (even for NA). 659
The checklist answers are an integral part of your paper submission. They are visible to the 660
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it 661
(after eventual revisions) with the final version of your paper, and its final version will be published 662
with the paper. 663
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. 664
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a 665
proper justification is given (e.g., "error bars are not reported because it would be too computationally 666
expensive" or "we were unable to find the license for the dataset we used"). In general, answering 667
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we 668
acknowledge that the true answer is often more nuanced, so please just use your best judgment and 669
write a justification to elaborate. All supporting evidence can appear either in the main paper or the 670
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification 671
please point to the section(s) where related material for the question can be found. 672
IMPORTANT, please: 673
•Delete this instruction block, but keep the section heading “NeurIPS paper checklist" , 674
•Keep the checklist subsection headings, questions/answers and guidelines below. 675
•Do not modify the questions and only use the provided macros for your answers . 676
1.Claims 677
Question: Do the main claims made in the abstract and introduction accurately reflect the 678
paper’s contributions and scope? 679
Answer: [Yes] 680
Justification: See details in Sections 3, 4, 5, and 6. 681
Guidelines: 682
•The answer NA means that the abstract and introduction do not include the claims 683
made in the paper. 684
•The abstract and/or introduction should clearly state the claims made, including the 685
contributions made in the paper and important assumptions and limitations. A No or 686
NA answer to this question will not be perceived well by the reviewers. 687
•The claims made should match theoretical and experimental results, and reflect how 688
much the results can be expected to generalize to other settings. 689
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 690
are not attained by the paper. 691
2.Limitations 692
Question: Does the paper discuss the limitations of the work performed by the authors? 693
Answer: [Yes] 694
Justification: See Section 7. 695
25Guidelines: 696
•The answer NA means that the paper has no limitation while the answer No means that 697
the paper has limitations, but those are not discussed in the paper. 698
• The authors are encouraged to create a separate "Limitations" section in their paper. 699
•The paper should point out any strong assumptions and how robust the results are to 700
violations of these assumptions (e.g., independence assumptions, noiseless settings, 701
model well-specification, asymptotic approximations only holding locally). The authors 702
should reflect on how these assumptions might be violated in practice and what the 703
implications would be. 704
•The authors should reflect on the scope of the claims made, e.g., if the approach was 705
only tested on a few datasets or with a few runs. In general, empirical results often 706
depend on implicit assumptions, which should be articulated. 707
•The authors should reflect on the factors that influence the performance of the approach. 708
For example, a facial recognition algorithm may perform poorly when image resolution 709
is low or images are taken in low lighting. Or a speech-to-text system might not be 710
used reliably to provide closed captions for online lectures because it fails to handle 711
technical jargon. 712
•The authors should discuss the computational efficiency of the proposed algorithms 713
and how they scale with dataset size. 714
•If applicable, the authors should discuss possible limitations of their approach to 715
address problems of privacy and fairness. 716
•While the authors might fear that complete honesty about limitations might be used by 717
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 718
limitations that aren’t acknowledged in the paper. The authors should use their best 719
judgment and recognize that individual actions in favor of transparency play an impor- 720
tant role in developing norms that preserve the integrity of the community. Reviewers 721
will be specifically instructed to not penalize honesty concerning limitations. 722
3.Theory Assumptions and Proofs 723
Question: For each theoretical result, does the paper provide the full set of assumptions and 724
a complete (and correct) proof? 725
Answer: [Yes] 726
Justification: See Sections 3 and 4 for details. 727
Guidelines: 728
• The answer NA means that the paper does not include theoretical results. 729
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 730
referenced. 731
•All assumptions should be clearly stated or referenced in the statement of any theorems. 732
•The proofs can either appear in the main paper or the supplemental material, but if 733
they appear in the supplemental material, the authors are encouraged to provide a short 734
proof sketch to provide intuition. 735
•Inversely, any informal proof provided in the core of the paper should be complemented 736
by formal proofs provided in appendix or supplemental material. 737
• Theorems and Lemmas that the proof relies upon should be properly referenced. 738
4.Experimental Result Reproducibility 739
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 740
perimental results of the paper to the extent that it affects the main claims and/or conclusions 741
of the paper (regardless of whether the code and data are provided or not)? 742
Answer: [Yes] 743
Justification: See Section 6 and supplementary materials for details. 744
Guidelines: 745
• The answer NA means that the paper does not include experiments. 746
26•If the paper includes experiments, a No answer to this question will not be perceived 747
well by the reviewers: Making the paper reproducible is important, regardless of 748
whether the code and data are provided or not. 749
•If the contribution is a dataset and/or model, the authors should describe the steps taken 750
to make their results reproducible or verifiable. 751
•Depending on the contribution, reproducibility can be accomplished in various ways. 752
For example, if the contribution is a novel architecture, describing the architecture fully 753
might suffice, or if the contribution is a specific model and empirical evaluation, it may 754
be necessary to either make it possible for others to replicate the model with the same 755
dataset, or provide access to the model. In general. releasing code and data is often 756
one good way to accomplish this, but reproducibility can also be provided via detailed 757
instructions for how to replicate the results, access to a hosted model (e.g., in the case 758
of a large language model), releasing of a model checkpoint, or other means that are 759
appropriate to the research performed. 760
•While NeurIPS does not require releasing code, the conference does require all submis- 761
sions to provide some reasonable avenue for reproducibility, which may depend on the 762
nature of the contribution. For example 763
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 764
to reproduce that algorithm. 765
(b)If the contribution is primarily a new model architecture, the paper should describe 766
the architecture clearly and fully. 767
(c)If the contribution is a new model (e.g., a large language model), then there should 768
either be a way to access this model for reproducing the results or a way to reproduce 769
the model (e.g., with an open-source dataset or instructions for how to construct 770
the dataset). 771
(d)We recognize that reproducibility may be tricky in some cases, in which case 772
authors are welcome to describe the particular way they provide for reproducibility. 773
In the case of closed-source models, it may be that access to the model is limited in 774
some way (e.g., to registered users), but it should be possible for other researchers 775
to have some path to reproducing or verifying the results. 776
5.Open access to data and code 777
Question: Does the paper provide open access to the data and code, with sufficient instruc- 778
tions to faithfully reproduce the main experimental results, as described in supplemental 779
material? 780
Answer: [Yes] 781
Justification: See supplementary materials for details. 782
Guidelines: 783
• The answer NA means that paper does not include experiments requiring code. 784
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 785
public/guides/CodeSubmissionPolicy ) for more details. 786
•While we encourage the release of code and data, we understand that this might not be 787
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 788
including code, unless this is central to the contribution (e.g., for a new open-source 789
benchmark). 790
•The instructions should contain the exact command and environment needed to run to 791
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 792
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 793
•The authors should provide instructions on data access and preparation, including how 794
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 795
•The authors should provide scripts to reproduce all experimental results for the new 796
proposed method and baselines. If only a subset of experiments are reproducible, they 797
should state which ones are omitted from the script and why. 798
•At submission time, to preserve anonymity, the authors should release anonymized 799
versions (if applicable). 800
27•Providing as much information as possible in supplemental material (appended to the 801
paper) is recommended, but including URLs to data and code is permitted. 802
6.Experimental Setting/Details 803
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 804
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 805
results? 806
Answer: [Yes] 807
Justification: See Section 6 for details. 808
Guidelines: 809
• The answer NA means that the paper does not include experiments. 810
•The experimental setting should be presented in the core of the paper to a level of detail 811
that is necessary to appreciate the results and make sense of them. 812
•The full details can be provided either with the code, in appendix, or as supplemental 813
material. 814
7.Experiment Statistical Significance 815
Question: Does the paper report error bars suitably and correctly defined or other appropriate 816
information about the statistical significance of the experiments? 817
Answer: [Yes] 818
Justification: See Section 6 for details. 819
Guidelines: 820
• The answer NA means that the paper does not include experiments. 821
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 822
dence intervals, or statistical significance tests, at least for the experiments that support 823
the main claims of the paper. 824
• The factors of variability that the error bars are capturing should be clearly stated (for 825
example, train/test split, initialization, random drawing of some parameter, or overall 826
run with given experimental conditions). 827
•The method for calculating the error bars should be explained (closed form formula, 828
call to a library function, bootstrap, etc.) 829
• The assumptions made should be given (e.g., Normally distributed errors). 830
•It should be clear whether the error bar is the standard deviation or the standard error 831
of the mean. 832
•It is OK to report 1-sigma error bars, but one should state it. The authors should 833
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 834
of Normality of errors is not verified. 835
•For asymmetric distributions, the authors should be careful not to show in tables or 836
figures symmetric error bars that would yield results that are out of range (e.g. negative 837
error rates). 838
•If error bars are reported in tables or plots, The authors should explain in the text how 839
they were calculated and reference the corresponding figures or tables in the text. 840
8.Experiments Compute Resources 841
Question: For each experiment, does the paper provide sufficient information on the com- 842
puter resources (type of compute workers, memory, time of execution) needed to reproduce 843
the experiments? 844
Answer: [Yes] 845
Justification: See Section 6 for details. 846
Guidelines: 847
• The answer NA means that the paper does not include experiments. 848
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 849
or cloud provider, including relevant memory and storage. 850
28•The paper should provide the amount of compute required for each of the individual 851
experimental runs as well as estimate the total compute. 852
•The paper should disclose whether the full research project required more compute 853
than the experiments reported in the paper (e.g., preliminary or failed experiments that 854
didn’t make it into the paper). 855
9.Code Of Ethics 856
Question: Does the research conducted in the paper conform, in every respect, with the 857
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 858
Answer: [Yes] 859
Justification: We have fully complied with the NeurIPS Code of Ethics. 860
Guidelines: 861
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 862
•If the authors answer No, they should explain the special circumstances that require a 863
deviation from the Code of Ethics. 864
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 865
eration due to laws or regulations in their jurisdiction). 866
10.Broader Impacts 867
Question: Does the paper discuss both potential positive societal impacts and negative 868
societal impacts of the work performed? 869
Answer: [Yes] 870
Justification: See Section 8 for details. 871
Guidelines: 872
• The answer NA means that there is no societal impact of the work performed. 873
•If the authors answer NA or No, they should explain why their work has no societal 874
impact or why the paper does not address societal impact. 875
•Examples of negative societal impacts include potential malicious or unintended uses 876
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 877
(e.g., deployment of technologies that could make decisions that unfairly impact specific 878
groups), privacy considerations, and security considerations. 879
•The conference expects that many papers will be foundational research and not tied 880
to particular applications, let alone deployments. However, if there is a direct path to 881
any negative applications, the authors should point it out. For example, it is legitimate 882
to point out that an improvement in the quality of generative models could be used to 883
generate deepfakes for disinformation. On the other hand, it is not needed to point out 884
that a generic algorithm for optimizing neural networks could enable people to train 885
models that generate Deepfakes faster. 886
•The authors should consider possible harms that could arise when the technology is 887
being used as intended and functioning correctly, harms that could arise when the 888
technology is being used as intended but gives incorrect results, and harms following 889
from (intentional or unintentional) misuse of the technology. 890
•If there are negative societal impacts, the authors could also discuss possible mitigation 891
strategies (e.g., gated release of models, providing defenses in addition to attacks, 892
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 893
feedback over time, improving the efficiency and accessibility of ML). 894
11.Safeguards 895
Question: Does the paper describe safeguards that have been put in place for responsible 896
release of data or models that have a high risk for misuse (e.g., pretrained language models, 897
image generators, or scraped datasets)? 898
Answer: [Yes] 899
Justification: The data used in this paper are all benchmark tasks established by the commu- 900
nity. 901
Guidelines: 902
29• The answer NA means that the paper poses no such risks. 903
•Released models that have a high risk for misuse or dual-use should be released with 904
necessary safeguards to allow for controlled use of the model, for example by requiring 905
that users adhere to usage guidelines or restrictions to access the model or implementing 906
safety filters. 907
•Datasets that have been scraped from the Internet could pose safety risks. The authors 908
should describe how they avoided releasing unsafe images. 909
•We recognize that providing effective safeguards is challenging, and many papers do 910
not require this, but we encourage authors to take this into account and make a best 911
faith effort. 912
12.Licenses for existing assets 913
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 914
the paper, properly credited and are the license and terms of use explicitly mentioned and 915
properly respected? 916
Answer: [Yes] 917
Justification: We have fully acknowledged baseline models, codes, and data in our paper. 918
Guidelines: 919
• The answer NA means that the paper does not use existing assets. 920
• The authors should cite the original paper that produced the code package or dataset. 921
•The authors should state which version of the asset is used and, if possible, include a 922
URL. 923
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 924
•For scraped data from a particular source (e.g., website), the copyright and terms of 925
service of that source should be provided. 926
•If assets are released, the license, copyright information, and terms of use in the 927
package should be provided. For popular datasets, paperswithcode.com/datasets 928
has curated licenses for some datasets. Their licensing guide can help determine the 929
license of a dataset. 930
•For existing datasets that are re-packaged, both the original license and the license of 931
the derived asset (if it has changed) should be provided. 932
•If this information is not available online, the authors are encouraged to reach out to 933
the asset’s creators. 934
13.New Assets 935
Question: Are new assets introduced in the paper well documented and is the documentation 936
provided alongside the assets? 937
Answer: [Yes] 938
Justification: We have provided details documents for the codes. 939
Guidelines: 940
• The answer NA means that the paper does not release new assets. 941
•Researchers should communicate the details of the dataset/code/model as part of their 942
submissions via structured templates. This includes details about training, license, 943
limitations, etc. 944
•The paper should discuss whether and how consent was obtained from people whose 945
asset is used. 946
•At submission time, remember to anonymize your assets (if applicable). You can either 947
create an anonymized URL or include an anonymized zip file. 948
14.Crowdsourcing and Research with Human Subjects 949
Question: For crowdsourcing experiments and research with human subjects, does the paper 950
include the full text of instructions given to participants and screenshots, if applicable, as 951
well as details about compensation (if any)? 952
Answer: [NA] 953
30Justification: The paper does not involve crowdsourcing nor research with human subjects. 954
Guidelines: 955
•The answer NA means that the paper does not involve crowdsourcing nor research with 956
human subjects. 957
•Including this information in the supplemental material is fine, but if the main contribu- 958
tion of the paper involves human subjects, then as much detail as possible should be 959
included in the main paper. 960
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 961
or other labor should be paid at least the minimum wage in the country of the data 962
collector. 963
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 964
Subjects 965
Question: Does the paper describe potential risks incurred by study participants, whether 966
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 967
approvals (or an equivalent approval/review based on the requirements of your country or 968
institution) were obtained? 969
Answer: [NA] 970
Justification: The paper does not involve crowdsourcing nor research with human subjects. 971
Guidelines: 972
•The answer NA means that the paper does not involve crowdsourcing nor research with 973
human subjects. 974
•Depending on the country in which research is conducted, IRB approval (or equivalent) 975
may be required for any human subjects research. If you obtained IRB approval, you 976
should clearly state this in the paper. 977
•We recognize that the procedures for this may vary significantly between institutions 978
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 979
guidelines for their institution. 980
•For initial submissions, do not include any information that would break anonymity (if 981
applicable), such as the institution conducting the review. 982
31