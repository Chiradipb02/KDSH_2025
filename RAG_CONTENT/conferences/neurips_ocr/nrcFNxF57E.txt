Partial Gromov Wasserstein Metric
Anonymous Author(s)
Affiliation
Address
email
Abstract
The Gromov-Wasserstein (GW) distance has gained increasing interest in the 1
machine learning community in recent years, as it allows for the comparison 2
of measures in different metric spaces. To overcome the limitations imposed 3
by the equal mass requirements of the classical GW problem, researchers have 4
begun exploring its application in unbalanced settings. However, Unbalanced GW 5
(UGW) can only be regarded as a discrepancy rather than a rigorous metric/distance 6
between two metric measure spaces (mm-spaces). In this paper, we propose a 7
particular case of the UGW problem, termed Partial Gromov-Wasserstein (PGW). 8
We establish that PGW is a well-defined metric between mm-spaces and discuss its 9
theoretical properties, including the existence of a minimizer for the PGW problem 10
and the relationship between PGW and GW, among others. We then propose two 11
variants of the Frank-Wolfe algorithm for solving the PGW problem and show 12
that they are mathematically and computationally equivalent. Moreover, based 13
on our PGW metric, we introduce the analogous concept of barycenters for mm- 14
spaces. Finally, we validate the effectiveness of our PGW metric and related solvers 15
in applications such as shape matching, shape retrieval, and shape interpolation, 16
comparing them against existing baselines. 17
1 Introduction 18
The classical optimal transport (OT) problem [ 1] seeks to match two probability measures while 19
minimizing the expected transportation cost. At the heart of classical OT theory lies the principle of 20
mass conservation, which aims to optimize the transfer between two probability measures, assuming 21
they have the same total mass and strictly preserving it. Statistical distances that arise from OT, 22
such as Wasserstein distances, have been widely applied across various machine learning domains, 23
ranging from generative modeling [ 2,3] to domain adaptation [ 4] and representation learning [ 5]. 24
Recent advancements have extended the OT problem to address certain limitations within machine 25
learning applications. These advancements include: 1) facilitating the comparison of non-negative 26
measures that possess different total masses via unbalanced [ 6] and partial OT [ 7], and 2) enabling 27
the comparison of probability measures across distinct metric spaces through Gromov-Wasserstein 28
distances [ 8], with applications spanning from quantum chemistry [ 9] to natural language processing 29
[10]. 30
Regarding the first aspect, many applications in machine learning involve comparing non-negative 31
measures (often empirical measures) with varying total amounts of mass, e.g., domain adaptation 32
[11]. Moreover, OT distances (or dissimilarity measures) are often not robust against outliers and 33
noise, resulting in potentially high transportation costs for outliers. Many recent publications have 34
focused on variants of the OT problem that allow for comparing non-negative measures with unequal 35
mass. For instance, the optimal partial transport problem [ 7,12,13,14], Kantorovich–Rubinstein 36
norm [ 15,16,17], and the Hellinger–Kantorovich distance [ 18,19]. These methods fall under the 37
broad category of “unbalanced optimal transport”. In this regard, we also highlight [ 20,21,22], 38
which enhance OT’s robustness in the presence of outliers. 39
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Regarding the second aspect, comparing probability measures across different metric spaces is 40
essential in many machine learning applications, ranging from computer graphics, where shapes and 41
surfaces are compared [ 23,24], to graph partitioning and matching problems [ 25]. Source and target 42
distributions often arise from varied conditions, such as different times, contexts, or measurement 43
techniques, creating substantial differences in intrinsic distances among data points. The conventional 44
OT framework necessitates a meaningful distance across diverse domains, a requirement that is not 45
always achievable. To circumvent this issue, the Gromov-Wasserstein (GW) distances were proposed 46
in [8,24] as an adaptation of the Gromov-Hausdorff distance, which measures the discrepancy 47
between two metric spaces [ 26,27,28,29]. The GW distance [ 8,30] extends OT-based distances to 48
metric measure spaces (mm-spaces) up to isometries. Its invariance across isomorphic mm-spaces 49
makes the GW distance particularly valuable for applications like shape comparison and matching, 50
where invariance to rigid motion transformations is crucial. 51
The main computational challenge of the GW metric is the non-convexity of its formulation [ 8]. The 52
conventional computational approach relies on the Frank-Wolfe (FW) algorithm [ 31,32]. Optimal 53
transport (OT) computational methods [ 15,33,34,35,36,37,38,39,40], such as the Sinkhorn 54
algorithm, can be incorporated into FW iterations, which yields the classical GW solvers [ 41,42,43]. 55
Given that the GW distance is limited to the comparison of probability mm-spaces, recent works 56
have introduced unbalanced and partial variations [ 44,45,46]. These variations have been applied in 57
diverse contexts, including partial graph matching for social network analysis [ 47] and the alignment 58
of brain images [ 48]. Although solving these unbalanced variants of the GW problem yields notions 59
ofdiscrepancies between mm-spaces, their metric properties remain unclear in the literature. 60
Motivated by the emerging applications of the GW problem in unbalanced settings, this paper focuses 61
on developing a metric between general (not necessarily probability) mm-spaces and providing 62
efficient solvers for its computation. Our proposed metric arises from formulating a variant of the GW 63
problem for unbalanced contexts, rooted in the framework provided by [ 44], which we named the 64
Partial Gromov-Wasserstein (PGW) problem. In contrast to [ 44], which introduces a KL-divergence 65
penalty and a Sinkhorn solver, we employ a total variation penalty, demonstrate the resulting metric 66
properties, and provide novel, efficient solvers for this problem. To the best of our knowledge, this 67
paper presents the first metric for non-probability mm-spaces based on the GW distance. 68
Contributions. Our specific contributions in this paper are: 69
•GW metric in unbalanced settings. We propose the Partial Gromov-Wasserstein (PGW) 70
problem and prove that it gives rise to a metric between arbitrary mm-spaces. 71
•PGW solver. Analogous to the technique presented in [ 12], we show that the PGW problem 72
can be turned into a variant of the GW problem. Based on this relation, we propose two 73
mathematically equivalent, but distinct in numerical implementation, Frank-Wolfe solvers 74
for the discrete PGW problem. Inspired by the results of [ 32], we prove that similar to the 75
Frank-Wolfe solver presented in [ 45], our proposed solvers for the PGW problem converge 76
linearly to a stationary point. 77
•Numerical experiments. We demonstrate the performance of our proposed algorithms in 78
terms of computation time and efficacy on a series of tasks: shape-matching with outliers 79
between 2D and 3D objects, shape retrieval between 2D shapes, and shape interpolation 80
using the concept of PGW barycenters. We compare the performance of our proposed 81
algorithms against existing baselines for each task. 82
2 Background 83
In this section, we review the basics of OT theory, one of its variants in unbalanced contexts called 84
Partial OT (POT), and their connection as established in [12]. We then introduce the GW distance. 85
2.1 Optimal Transport and Partial Optimal Transport 86
LetΩ⊆Rdbe, for simplicity, a compact subset of Rd, andP(Ω)be the space of probability measures 87
defined on the Borel σ-algebra of Ω. 88
The Optimal Transport (OT) problem forµ, ν∈ P(Ω), with transportation cost c(x, y) : Ω×Ω→ 89
R+being a lower-semi continuous function, is defined as: 90
OT(µ, ν) := min
γ∈Γ(µ,ν)γ(c), where γ(c) :=Z
Ω2c(x, y)dγ(x, y) (1)
2and where Γ(µ, ν)denotes the set of all joint probability measures on Ω2:= Ω×Ωwith marginals 91
µ, ν, i.e., γ1:=π1#γ=µ, γ2:=π2#γ=ν, where π1, π2: Ω2→Ωare the canonical projections 92
π1(x, y) :=x, π2(x, y) :=y. A minimizer for (1)always exists [ 1,49] and when c(x, y) =∥x−y∥p, 93
forp≥1, it defines a metric on P(Ω), which is referred to as the “ p-Wasserstein distance”: 94
Wp
p(µ, ν) := min
γ∈Γ(µ,ν)Z
Ω2∥x−y∥pdγ(x, y). (2)
The Partial Optimal Transport (POT) problem [6,13,50] extends the OT problem to the set of 95
Radon measures M+(Ω), i.e., non-negative and finite measures. For λ >0andµ, ν∈ M +(Ω), the 96
POT problem is defined as: 97
POT (µ, ν;λ) := inf
γ∈M +(Ω2)γ(c) +λ(|µ−γ1|+|ν−γ2|), (3)
where, in general, |σ|denotes the total variation norm of a measure σ, i.e.,|σ|:=σ(Ω). The
constraint γ∈ M +(Ω2)in (3) can be further restricted to γ∈Γ≤(µ, ν):
Γ≤(µ, ν) :={γ∈ M +(Ω2) :γ1≤µ, γ2≤ν},
denoting γ1≤µif for any Borel set B⊆Ω,γ1(B)≤µ(B)(respectively, for γ2≤ν) [7]. Roughly 98
speaking, the linear penalization indicates that if the classical transportation cost exceeds 2λ, it is 99
better to create/destroy’ mass (see [40] for further details). 100
The relationship between POT and OT. By using the techniques in [ 12], the POT problem can be 101
transferred into an OT problem, and thus, OT solvers (e.g., network simplex) can be employed to 102
solve the POT problem. 103
Proposition 2.1. [12,40] Given µ, ν∈ M +(Ω), construct the following measures on ˆΩ := Ω ∪{ˆ∞}, 104
for an auxiliary point ˆ∞: 105
ˆµ=µ+|ν|δˆ∞ and ˆν=ν+|µ|δˆ∞. (4)
Consider the following OT problem 106
OT(ˆµ,ˆν) = min
ˆγ∈Γ(ˆµ,ˆν)ˆγ(ˆc), where ˆc(x, y) :=c(x, y)−2λifx, y∈Ω,
0 elsewhere .(5)
Then, there exists a bijection F: Γ≤(µ, ν)→Γ(ˆµ,ˆν)given by 107
F(γ) :=γ+ (µ−γ1)⊗δˆ∞+δˆ∞⊗(ν−γ2) +|γ|δˆ∞,ˆ∞. (6)
such that γis optimal for the POT problem (3)if and only if F(γ)is optimal for the OT problem (5). 108
It is worth noting that instead of considering the same underlying space Ωfor both measures µandν, 109
the OT and POT problems can be formulated in the scenario where µandνare defined on different 110
metric spaces XandY, respectively. In this setting, one needs a cost function c:X×Y→R+to 111
formulate the OT and POT problems. However, in practice it is usually difficult to define reasonable 112
‘distance’ or ground cost c(·,·)between the two spaces XandY. In particular, the p-Wasserstein 113
distance cannot be adopted if µ, νare defined on different spaces. To relax this requirement, in the 114
next section, we will review the fundamentals of the Gromov-Wasserstein problem [8]. 115
2.2 The Gromov-Wasserstein (GW) Problem 116
A metric measure space (mm-space) consists of a set Xendowed with a metric structure, that is, a 117
notion of distance dXbetween its elements, and equipped with a Borel measure µ. As in [ 8, Ch. 118
5], we will assume that Xis compact and that supp( µ) =X. Given two probability mm-spaces 119
X= (X, d X, µ),Y= (Y, dY, ν), with µ∈ P(X)andν∈ P(Y), and a non-negative lower 120
semi-continuous cost function L:R2→R+(e.g., the Euclidean distance or the KL-loss), the 121
Gromov-Wasserstein (GW) matching problem is defined as: 122
GWL(X,Y) := inf
γ∈Γ(µ,ν)γ⊗2(L(dX(·,·), dY(·,·))), (7)
where, for brevity, we employ the notation γ⊗2for the product measure dγ⊗2((x, y),(x′, y′)) = 123
dγ(x, y)dγ(x′, y′). IfL(a, b) =|a−b|p, for1≤p <∞, we denote GWL(·,·)simply by GWp(·,·). 124
In this case, the expression (7)defines an equivalence relation ∼among probability mm-spaces, i.e., 125
3X∼Yif and only if GWp(X,Y) = 01. A minimizer of the GW problem (7)always exists, and thus, 126
we can replace infbymin. Moreover, similar to OT, the above GW problem defines a distance for 127
probability mm-spaces after taking the quotient under ∼. For details, we refer to [8, Ch. 5 and 10]. 128
3 The Partial Gromov-Wasserstein (PGW) Problem 129
The Unbalanced Gromov-Wasserstein (UGW) problem for general (compact) mm-spaces X= 130
(X, d X, µ),Y= (Y, dY, ν), with µ∈ M +(X), ν∈ M +(Y), studied in [44] is defined as: 131
UGWL
λ(X,Y) := inf
γ∈M +(X×Y)γ⊗2(L(dX, dY)) +λ(Dϕ(γ⊗2
1∥µ⊗2) +Dϕ(γ⊗2
2∥ν⊗2)),(8)
where λ >0is a fixed linear penalization parameter, and Dϕis a Csiszár or ϕ-divergence. The above 132
formulation extends the classical GW problem (7)into the unbalanced setting ( µandνare no longer 133
necessarily probability measures but general Radon measures). 134
We underline two points: First, as discussed in [ 44], while the above quantity allows us to ‘compare’ 135
the mm-spaces XandY, itsmetric property is unclear. Secondly, when Dϕis the KL divergence, a 136
Sinkhorn solver has been proposed in [44]. However, a solver for general ϕ-divergences has not yet 137
been proposed. 138
In this paper, we will analyze the case when Dϕis the total variation norm. Specifically, for q≥1, 139
we consider the following problem, which we refer to as the Partial Gromov-Wasserstein (PGW) 140
problem: 141
PGWL
λ,q(X,Y) := inf
γ∈M +(X×Y)γ⊗2(L(dq
X, dq
Y)) +λ(|µ⊗2−γ⊗2
1|+|ν⊗2−γ⊗2
2|). (9)
Remark 3.1. Given γ∈Γ≤(µ, ν), the above cost functional can be rewritten as 142
γ⊗2(L(dq
X, dq
Y)) +λ(|µ⊗2−γ⊗2
1|+|ν⊗2−γ⊗2
2|) =γ⊗2(L(dq
X, dq
Y)−2λ) +λ 
|µ|2+|ν|2
|{z }
does not depend on γ.
Proposition 3.2. Given mm-spaces X= (X, d X, µ),Y= (Y, dY, ν), the minimization problem (9) 143
can be restricted to the set Γ≤(µ, ν) ={γ∈ M +(X×Y) :γ1≤µ, γ2≤ν}. That is, 144
PGWL
λ,q(X,Y) = inf
γ∈Γ≤(µ,ν)γ⊗2(L(dq
X, dq
Y)−2λ) +λ(|µ|2+|ν|2). (10)
For the proof, inspired by [50], we direct the reader to Appendix B. 145
We notice that a similar Partial Gromov-Wasserstein problem (and its solver) has been studied [ 45]. 146
Indeed, in [ 45], the λ-penalization in the optimization problem (10) is avoided, but the constraint set 147
is replaced by the subset of all γ∈Γ≤(µ, ν)such that |γ|=ρfor a fixed ρ∈[0,min{|µ|,|ν|}]. We 148
will call this formulation the Mass-Constrained Partial Gromov-Wasserstein (MPGW) problem. In 149
Appendix L, we explore the relations between PGW and MPGW, and in Section 5 and Appendices N, 150
O, P, we analyze the performance of the different solvers through different experiments. 151
Proposition 3.3. IfL(r1, r2) =|r1−r2|p, forp∈[1,∞), we use PGWp
λ,qto denote PGWL
λ,q. In 152
this case, (9)and(10) admit a minimizer. 153
The proof is given in Appendix C: Its idea extends results from [ 8] from probability mm-spaces to 154
arbitrary mm-spaces. 155
Next, we state one of our main results: The PGW problem gives rise to a metric between mm-spaces. 156
The rigorous statement as well as its proof is given in Appendix D. 157
Proposition 3.4. Letλ >0,1≤q, p < ∞andL(r1, r2) =|r1−r2|p. Then (PGWp
λ,q(·,·))1/p158
defines a metric between mm-spaces. 159
Finally, for consistency, we provide the following result when the penalization tends to infinity. Its 160
proof is given in Appendix E. 161
Proposition 3.5. Consider probability mm-spaces X= (X, d X, µ),Y= (Y, dY, ν), that is, |µ|= 162
|ν|= 1. Assume that Lis a continuous funtion. Then limλ→∞PGWL
λ,1(X,Y) =GWL(X,Y). 163
1Moreover, given two probability mm-spaces XandY,GW(X,Y) = 0 if and only if there exists a bijective
isometry ϕ:X→Ysuch that ϕ#µ=ν. In particular, the GW distance is invariant under rigid transformations
(translations and rotations) of a given probability mm-space.
44 Computation of the Partial GW Distance 164
In the discrete setting, consider mm-spaces X= (X, d X,Pn
i=1pX
iδxi),Y= (Y, dY,Pm
j=1qY
jδyj), 165
where X={x1, . . . , x n},Y={y1, . . . , y m}, the weights pX
i,qY
jare non-negative numbers, and 166
the distances dX,dYare determined by the matrices CX∈Rn×n,CY∈Rm×mdefined by 167
CX
i,i′:=dq
X(xi, xi′)∀i, i′∈[1 :n]and CY
j,j′:=dq
Y(yj, yj′)∀j, j′∈[1 :m]. (11)
Letp := [ qX
1, . . . , qX
n]⊤andq := [ qY
1, . . . , qY
m]⊤denote the weight vectors corresponding to the 168
given discrete measures. We view the sets of transportation plans Γ(p,q)andΓ≤(p,q)for the GW 169
and PGW problems, respectively, as the subsets of n×mmatrices 170
Γ(p,q) :={γ∈Rn×m
+ :γ1m= p, γ⊤1n= q},if|p|=nX
i=1pX
i= 1 =mX
j=1qY
j=|q|;(12)
171
Γ≤(p,q) :={γ∈Rn×m
+ :γ1m≤p, γ⊤1n≤q}, (13)
for any pair of non-negative vectors p∈Rn
+,q∈Rm
+, where 1nis the vector with all ones in Rn172
(resp. 1m), and γ1m≤pmeans that component-wise the ≤relation holds. 173
Given by a non-negative function L:Rn×n×Rm×m→R+, he transportation cost Mand the 174
‘partial’ transportation con ˜Mare represented by the n×m×n×mtensors: 175
Mi,j,i′,j′=L(CX
i,i′, CY
j,j′) and ˜M:=M−2λ:=M−2λ1n,m,n,m , (14)
where 1n,m,n,m is the tensor with ones in all its entries. For each n×m×n×mtensor Mand each
n×mmatrix γ, we define tensor-matrix multiplication M◦γ∈Rn×mby
(M◦γ)ij=X
i′,j′(Mi,j,i′,j′)γi′,j′.
Then, the Partial GW problem in (10) can be written as 176
PGWL
λ(X,Y) = min
γ∈Γ≤(p,q)L˜M(γ) +λ(|p|2+|q|2),where (15)
177
L˜M(γ) := ˜Mγ⊗2:=X
i,j,i′,j′˜Mi,j,i′,j′γi,jγi′,j′=X
ij(˜M◦γ)ijγij=:⟨˜M◦γ, γ⟩F, (16)
and⟨·,·⟩Fstands for the Frobenius dot product. The constant term λ(|p|2+|q|2)will be ignored in 178
the rest of this paper since it does not depend on γ. 179
4.1 Frank-Wolfe for the PGW Problem – Solver 1 180
In this section, we discuss the Frank-Wolfe (FW) algorithm for the PGW problem (15). A second 181
variant of the FW solver is provided in the Appendix G. 182
As a summary, in our proposed method, we address the discrete PGW problem (15), highlighting 183
that the direction-finding subproblem in the Frank-Wolfe (FW) algorithm is a POT problem for (15). 184
Specifically, (15) is treated as a discrete POT problem in our Solver 1, where we apply Proposition 185
2.1 to solve a discrete OT problem. 186
For each iteration k, the procedure is summarized in three steps detailed below. 187
The convergence analysis, detailed in Appendix K, applies the results from [ 32] to our context, 188
showing that the FW algorithm achieves a stationary point at a rate of O(1/√
k)for non-convex 189
objectives with a Lipschitz continuous gradient in a convex and compact domain. 190
Step 1. Computation of gradient and optimal direction. 191
It is straightforward to verify that the gradient of the objective function (16) in (15) is given by 192
∇L ˜M(γ) = 2 ˜M◦γ. (17)
The classical method to compute M◦γis the following: First, convert Minto an (n×m)×(n×m) 193
matrix, denoted as v(M), and convert γinto an (n×m)×1vector v(γ). Then, the computation 194
ofM◦γis equivalent to the matrix multiplication v(M)v(γ). The computational cost and the 195
5Algorithm 1: Frank-Wolfe Algorithm for PGW, ver 1
Input: µ=Pn
i=1pX
iδxi, ν=Pm
j=1qY
jδyj, γ(1)
Output: γ(final )
Compute CX, CY
fork= 1,2, . . .do
G(k)←2˜M◦γ(k)// Compute gradient
γ(k)′←arg min γ∈Γ≤(p,q)⟨G(k), γ⟩F// Solve the POT problem.
Compute α(k)∈[0,1]via (18) // Line search
γ(k+1)←(1−α(k))γ(k)+α(k)γ(k)′// Update γ
if convergence, break
end for
γ(final )←γ(k)
required storage space are O(n2m2). In certain conditions, the above computation can be reduced to 196
O(n2+m2). We refer to Appendices F and H for details. 197
Next, we aim to solve the following problem: 198
γ(k)′←arg min
γ∈Γ≤(p,q)⟨∇L ˜M(γ(k)), γ⟩F,
which is a discrete POT problem since it is equivalent to
min
γ∈Γ≤(p,q)⟨2M◦γ(k), γ⟩F+λ|γ(k)|(|p|+|q| −2|γ|).
The solver can be obtained by firstly converting the POT problem into an OT problem via Proposition 199
2.1 and then solving the proposed OT problem. 200
Step 2: Line search method. 201
In this step, at the k-th iteration, we need to determine the optimal step size:
α(k)= arg min
α∈[0,1]{L˜M((1−α)γ(k)+αγ(k)′)}.
The optimal α(k)takes the following values (see Appendix I for details): 202
Letα(k)=

0 ifa≤0, a+b >0,
1 ifa≤0, a+b≤0,
clip(−b
2a,[0,1]) ifa >0,where

δγ(k)=γ(k)′−γ(k),
a=⟨˜M◦δγ(k), δγ(k)⟩F
b= 2⟨˜M◦γ(k), δγ(k)⟩F.,(18)
andclip(−b
2a,[0,1]) = min {max{−b
2a,0},1}. 203
Step 3: Update γ(k+1)←(1−α(k))γ(k)+α(k)γ(k)′. 204
4.2 Numerical Implementation Details 205
The initial guess, γ(1).In the GW problem, the initial guess is simply set to γ(1)= pq⊤if there 206
is no prior knowledge. In PGW, however, as µ, ν may not necessarily be probability measures 207
(i.e.,P
ipX
i,P
jqY
j̸= 1in general), we set γ(1)=pq⊤
max(|p|,|q|).It is straightforward to verify that 208
γ(1)∈Γ≤(p,q)as 209
γ(1)1m=|q|p
max(|p|,|q|)≤p, γ(1)⊤1n=|p|q
max(|p|,|q|)≤q.
Column/Row-Reduction. According to the interpretation of the penalty weight parameter in the 210
Partial OT problem (e.g. see Lemma 3.2 in [ 40]), during the POT solving step, for each i∈[1 :n] 211
(orj∈[1 :m]), if the ithrow (jthcolumn) of ˜M◦γ(k)contains a non-negative entry, all the mass 212
ofpX
i(qY
j) will be destroyed (created). Thus, we can remove the corresponding row (column) to 213
improve the computational efficiency. 214
65 Experiments 215
In addition to the three experiments detailed here, we also perform a wall-clock time comparison 216
of our proposed PGW solvers in Appendix O and a positive-unlabeled (PU) learning experiment in 217
Appendix P. 218
5.1 Toy Example: Shape Matching with Outliers 219
We use the moon dataset and synthetic 2D/3D spherical data in this experiment. Let {xi}n
i=1,{yj}n
j=1220
denote the source and target point clouds. In addition, we add ηn(where η= 20% ) outliers to the 221
target point cloud. See Figure 1 for visualization. 222
We visualize the transportation plans given by the GW [ 8], MPGW [ 45], UGW [ 44], and our proposed 223
PGW problems. For MPGW, UGW, and PGW, we set the mass to be 1 for each point in the source 224
and target point clouds. For GW, we normalize the mass of these points so that the source and target 225
have the same total mass. From Figure 1, we observe that PGW and MPGW induce a one-by-one 226
relation in both cases and no outlier points are matched to the source point cloud. Meanwhile, GW 227
matches all of the outliers. For UGW, as it applies the Sinkhorn algorithm, we observe mass-splitting 228
transportation plans in both cases. Moreover, we observe that some mass from the outliers has been 229
matched, which is not desired. 230
Figure 1: The set of red points comprises the source point cloud. The union of the dark blue (outliers)
and light blue points comprises the target point cloud. For UGW, MPGW, and PGW, we set the mass
for each point to be the same. For GW, we normalize the mass for the balanced mass constraint
setting.
5.2 Shape Retrieval 231
Experiment setup. We now employ the PGW distance to distinguish between 2D shapes, as done 232
in [51], and use GW, MPGW, and UGW as baselines for comparison. Given a series of 2D shapes, 233
we represent the shapes as mm-spaces Xi= (R2,∥ · ∥ 2, µi), where µi=Pni
k=1αiδxi
k. For the GW 234
method, we normalize the mass for the balanced mass constraint setting (i.e. αi=1
ni), and for the 235
remaining methods we let αi=αfor all the shapes, where α >0is a fixed constant. In this manner, 236
we compute the pairwise distances between the shapes. 237
We then use the computed distances for nearest neighbor classification. We do this by choosing a 238
representative at random from each class in the dataset and then classifying each shape according to 239
its nearest representative. This is repeated over 10,000 iterations, and we generate a confusion matrix 240
for each distance used. Finally, using the approach given by [ 51,52], we combine each distance with 241
a support vector machine (SVM), applying stratified 10-fold cross validation. In each iteration of 242
cross validation, we train an SVM using exp(−σD)as the kernel, where Dis the matrix of pairwise 243
distances (w.r.t. one of the considered distances) restricted to 9 folds, and compute the accuracy of 244
the model on the remaining fold. We report the accuracy averaged over all 10 folds for each model. 245
Dataset setup. We test two datasets in this experiment, which we refer to as Dataset I and Dataset II. 246
We construct Dataset I by adapting the 2D shape dataset given in [ 51], consisting of 20 shapes in 247
7bone
 goblet
 star
 horseshoe
rectangle
 trapezoid
 disk
 annulus
bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusbone
rectangle
goblet
trapezoid
star
disk
horseshoe
annulusGW
bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusMPGW
bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusUGW
bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusPGW (ours)
0.00.20.40.60.81.0
rectangle
 arrow
 semicircle
house
 double arrow
 circle
rectanglehouse arrow
double arrowsemicirclecirclerectangle
house
arrow
double arrow
semicircle
circleGW
rectanglehouse arrow
double arrowsemicirclecircleMPGW
rectanglehouse arrow
double arrowsemicirclecircleUGW
rectanglehouse arrow
double arrowsemicirclecirclePGW (ours)
0.00.20.40.60.81.0Figure 2: In each row, the first figure visualizes an example shape from each class, and the second
figure visualizes the resulting pairwise distance matrices. The first row corresponds to Dataset I and
the second corresponds to Dataset II.
each of the classes bone, goblet, star, and horseshoe. For each class, we augment the dataset with an 248
additional class by selecting either a subset of points from each shape of that class (rectangle/bone, 249
trapezoid/goblet, disk/star) or adding additional points to each shape of that class (annulus/horseshoe). 250
Hence, the final dataset consists of 160 shapes across 8 total classes. This dataset is visualized in 251
Figure 6a. 252
For Dataset II, we generate 20 shapes for each of the classes rectangle, house, arrow, double arrow, 253
semicircle, and circle. These shapes were generated in pairs, such that each shape of class rectangle 254
is a subset of the corresponding shape of class house, and similarly for arrow/double arrow and 255
semicircle/circle. This dataset is visualized in Figure 6b. 256
Performance analysis . We refer to Appendix N for full numerical details, parameter settings, and 257
the visualization of the resulting confusion matrices. We visualize the two considered datasets and 258
the resulting pairwise distance matrices in Figure 2. For the SVM experiments, GW achieves the 259
highest accuracy on Dataset I, 98.13%, while the second best method is PGW, 96.25%. For Dataset 260
II, PGW achieves the highest accuracy, correctly classifying 100% of the samples. The complete set 261
of accuracies for all considered distances on each dataset is reported in Table 1a. 262
In addition, we report the wall-clock time required to compute all pairwise distances for each distance 263
in Table 1b. We observe that GW, MPGW, and PGW have similar wall-clock times across both 264
experiments (30-50 seconds for Dataset I, 80-140 seconds for Dataset II), with PGW admitting 265
a slightly faster runtime in both cases. Meanwhile, UGW requires almost 1500 seconds on the 266
experiment with Dataset I and over 500 seconds on the experiment with Dataset II. 267
5.3 Partial Gromov-Wasserstein Barycenter and Shape Interpolation 268
By [41], Gromov-Wasserstein can be applied to interpolate two shapes via the concept of Gromov- 269
Wasserstein Barycenters . In this paper, we introduce Partial Gromov-Wasserstein Barycenters by 270
extending the GW Barycenter to the setting of PGW as follows. 271
Distance Dataset I Dataset II
GW 0.9813 0.8083
MPGW 0.0813 0.0000
UGW 0.8938 0.7833
PGW (ours) 0.9625 1.0000
(a) Mean accuracy of SVM using each dis-
tance in kernel.Distance Dataset I Dataset II
GW 49.02s 137.12s
MPGW 49.10s 93.90s
UGW 1484.49s 519.91s
PGW (ours) 35.92s 79.27s
(b) Wall-clock time comparison.
8GW,5%
 PGW,5%
 GW,10%
dataPGW,10%
t=0/7
 t=1/7
 t=2/7
 t=3/7
 t=4/7
 t=5/7
 t=6/7
 t=7/7Figure 3: In the first column, the first and second figures are the source and target point clouds in the
first experiment ( η= 5% ); the third and fourth figures are the source and target point clouds in the
second experiment ( η= 10% ).
Consider the discrete mm-spaces X1, . . . ,XK, where Xk= (Xk,∥ · ∥Rdk,Pnk
i=1pk
iδxk
i), with Xk= 272
{xk
i}nk
i=1⊂Rdk. We denote Ck= [∥xk
i−xk
i′∥2]i,i′andpk= [pk
1, . . . , pk
nk]. Given positive constants 273
λ1, . . . , λ K>0, the PGW Barycenter is defined by: 274
min
C,γkX
kξk⟨M(C, Ck)◦γk, γk⟩ −2λk|γk|2(19)
where each γk∈Γ≤(p,pk). We refer to Appendix M for the solver of (19) and details. 275
Experiment setup. We apply the PGW barycenter to the following problem: Given two shapes 276
X={xi}n
i=1⊂Rd1andY={yi}m
i=1⊂Rd2, modeled as mm-spaces X= (X,∥ · ∥Rd1,Pn
i=1δxi) 277
andY= (Y,∥ · ∥Rd2,Pm
i=1δyi), we wish to find interpolations between them. In addition, we 278
assume Yis corrupted by noise, i.e., Yis redefined as Y= (˜Y ,∥ · ∥Rd2,Pm
i=1δyi+Pmη
i=1δ˜yi) 279
with ˜Y=Y∪ {˜yi}m
i=1, where η∈[0,1]is the noise level and each ˜yiis randomly selected from a 280
particular region R ⊂Rd2. 281
Dataset setup. We adapt the dataset given in [ 41]. See Appendix M.1 for further details on the 282
dataset. In this experiment, we test η= 5% ,10%. We visualize the barycenter interpolation from 283
t= 0/7tot= 7/7, where (1−t), tare the weight of the source Xand the target Y, respectively, 284
in the barycenter (19). The visualization given in Figure 3 is obtained by applying SMACOF MDS 285
(multidimensional scaling) of the minimizer C. 286
Performance analysis . From Figure 3, we observe that in this two scenarios, the interpolation 287
derived from GW is clearly disturbed by the noise data points. For example, in rows 1,3, columns 288
t= 1/7,2/7,3/7, we see that the point clouds reconstructed by MDS have significantly different 289
width-height ratios from those of the source and target point clouds. In contrast, PGW is significantly 290
less disturbed, and the interpolation is more natural. The width-height ratio of the point clouds 291
generated by the PGW barycenter is consistent with that of the source/target point clouds. 292
6 Summary 293
In this paper, we propose the Partial Gromov-Wasserstein (PGW) problem and introduce two Frank- 294
Wolfe solvers for it. As a byproduct, we provide pertinent theoretical results, including the relation 295
between PGW and GW, the metric property of PGW, and the PGW barycenter. Furthermore, we 296
demonstrate the efficacy of the PGW solver in solving shape-matching, shape retrieval, and shape 297
interpolation tasks. For the shape retrieval experiment, we observe that due to the metric property, 298
PGW and GW have similar accuracy and outperform the other methods evaluated. In the shape 299
matching and point cloud interpolation experiments, we demonstrate PGW admits a more robust 300
result when the data are corrupted by outliers/noisy data. 301
9References 302
[1] Cedric Villani. Optimal transport: old and new . Springer, 2009. 303
[2]Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial 304
networks. In International conference on machine learning , pages 214–223. PMLR, 2017. 305
[3]Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. 306
Improved training of wasserstein gans. Advances in neural information processing systems , 30, 307
2017. 308
[4]Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution 309
optimal transportation for domain adaptation. Advances in neural information processing 310
systems , 30, 2017. 311
[5]Soheil Kolouri, Navid Naderializadeh, Gustavo K Rohde, and Heiko Hoffmann. Wasserstein 312
embedding for graph learning. In International Conference on Learning Representations , 2020. 313
[6]Lenaic Chizat, Gabriel Peyré, Bernhard Schmitzer, and François-Xavier Vialard. Unbalanced 314
optimal transport: Dynamic and Kantorovich formulations. Journal of Functional Analysis , 315
274(11):3090–3123, 2018. 316
[7]Alessio Figalli. The optimal partial transport problem. Archive for rational mechanics and 317
analysis , 195(2):533–560, 2010. 318
[8]Facundo Mémoli. Gromov–wasserstein distances and the metric approach to object matching. 319
Foundations of computational mathematics , 11:417–487, 2011. 320
[9]Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural 321
message passing for quantum chemistry. In International conference on machine learning , 322
pages 1263–1272. PMLR, 2017. 323
[10] David Alvarez-Melis and Tommi Jaakkola. Gromov-wasserstein alignment of word embedding 324
spaces. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language 325
Processing , pages 1881–1890, 2018. 326
[11] Kilian Fatras, Thibault Séjourné, Rémi Flamary, and Nicolas Courty. Unbalanced minibatch 327
optimal transport; applications to domain adaptation. In International Conference on Machine 328
Learning , pages 3186–3197. PMLR, 2021. 329
[12] Luis A Caffarelli and Robert J McCann. Free boundaries in optimal transport and monge-ampere 330
obstacle problems. Annals of mathematics , pages 673–730, 2010. 331
[13] Alessio Figalli and Nicola Gigli. A new transportation distance between non-negative mea- 332
sures, with applications to gradients flows with dirichlet boundary conditions. Journal de 333
mathématiques pures et appliquées , 94(2):107–130, 2010. 334
[14] Anh Duc Nguyen, Tuan Dung Nguyen, Quang Nguyen, Hoang Nguyen, Lam M. Nguyen, and 335
Kim-Chuan Toh. On partial optimal transport: Revised sinkhorn and efficient gradient methods. 336
InProceedings of the AAAI Conference on Artificial Intelligence , volume 38, 2024. 337
[15] Kevin Guittet. Extended Kantorovich norms: a tool for optimization . PhD thesis, INRIA, 2002. 338
[16] Florian Heinemann, Marcel Klatt, and Axel Munk. Kantorovich–rubinstein distance and 339
barycenter for finitely supported measures: Foundations and algorithms. Applied Mathematics 340
& Optimization , 87(1):4, 2023. 341
[17] Jan Lellmann, Dirk A Lorenz, Carola Schonlieb, and Tuomo Valkonen. Imaging with 342
kantorovich–rubinstein discrepancy. SIAM Journal on Imaging Sciences , 7(4):2833–2859, 343
2014. 344
[18] Lenaic Chizat, Gabriel Peyré, Bernhard Schmitzer, and François-Xavier Vialard. An interpolat- 345
ing distance between optimal transport and Fisher–Rao metrics. Foundations of Computational 346
Mathematics , 18(1):1–44, 2018. 347
10[19] Matthias Liero, Alexander Mielke, and Giuseppe Savare. Optimal entropy-transport problems 348
and a new Hellinger–Kantorovich distance between positive measures. Inventiones mathemati- 349
cae, 211(3):969–1117, 2018. 350
[20] Yogesh Balaji, Rama Chellappa, and Soheil Feizi. Robust optimal transport with applications 351
in generative modeling and domain adaptation. Advances in Neural Information Processing 352
Systems , 33:12934–12944, 2020. 353
[21] Quang Minh Nguyen, Hoang H Nguyen, Yi Zhou, and Lam M Nguyen. On unbalanced 354
optimal transport: Gradient methods, sparsity and approximation error. The Journal of Machine 355
Learning Research , 2023. 356
[22] Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On robust 357
optimal transport: Computational complexity and barycenter computation. Advances in Neural 358
Information Processing Systems , 34:21947–21959, 2021. 359
[23] Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. Generalized multidimensional 360
scaling: a framework for isometry-invariant partial surface matching. Proceedings of the 361
National Academy of Sciences , 103(5):1168–1172, 2006. 362
[24] Facundo Mémoli. Spectral gromov-wasserstein distances for shape matching. In 2009 IEEE 12th 363
International Conference on Computer Vision Workshops, ICCV Workshops , pages 256–263. 364
IEEE, 2009. 365
[25] Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph 366
partitioning and matching. Advances in neural information processing systems , 32, 2019. 367
[26] David A Edwards. The structure of superspace. In Studies in topology , pages 121–133. Elsevier, 368
1975. 369
[27] Mikhael Gromov. Structures métriques pour les variétés riemanniennes. Textes Math. , 1, 1981. 370
[28] Michael Gromov. Groups of polynomial growth and expanding maps (with an appendix by 371
jacques tits). Publications Mathématiques de l’IHÉS , 53:53–78, 1981. 372
[29] Dmitri Burago, Yuri Burago, Sergei Ivanov, et al. A course in metric geometry , volume 33. 373
American Mathematical Society Providence, 2001. 374
[30] Karl-Theodor Sturm. The space of spaces: curvature bounds and gradient flows on the space of 375
metric measure spaces , volume 290. American Mathematical Society, 2023. 376
[31] Marguerite Frank, Philip Wolfe, et al. An algorithm for quadratic programming. Naval research 377
logistics quarterly , 3(1-2):95–110, 1956. 378
[32] Simon Lacoste-Julien. Convergence rate of frank-wolfe for non-convex objectives. arXiv 379
preprint arXiv:1607.00345 , 2016. 380
[33] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in 381
neural information processing systems , 26, 2013. 382
[34] Nicolas Papadakis, Gabriel Peyré, and Edouard Oudet. Optimal transport with proximal splitting. 383
SIAM Journal on Imaging Sciences , 7(1):212–238, 2014. 384
[35] Jean-David Benamou, Brittany D Froese, and Adam M Oberman. Numerical solution of the 385
optimal transportation problem using the monge–ampère equation. Journal of Computational 386
Physics , 260:107–126, 2014. 387
[36] Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyré. Itera- 388
tive bregman projections for regularized transportation problems. SIAM Journal on Scientific 389
Computing , 37(2):A1111–A1138, 2015. 390
[37] Gabriel Peyré, Marco Cuturi, et al. Computational optimal transport: With applications to data 391
science. Foundations and Trends® in Machine Learning , 11(5-6):355–607, 2019. 392
11[38] Lenaic Chizat, Gabriel Peyré, Bernhard Schmitzer, and François-Xavier Vialard. Scaling algo- 393
rithms for unbalanced optimal transport problems. Mathematics of Computation , 87(314):2563– 394
2609, 2018. 395
[39] Nicolas Bonneel and David Coeurjolly. SPOT: sliced partial optimal transport. ACM Transac- 396
tions on Graphics , 38(4):1–13, 2019. 397
[40] Yikun Bai, Bernhard Schmitzer, Matthew Thorpe, and Soheil Kolouri. Sliced optimal partial 398
transport. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 399
Recognition , pages 13681–13690, 2023. 400
[41] Gabriel Peyré, Marco Cuturi, and Justin Solomon. Gromov-wasserstein averaging of kernel and 401
distance matrices. In International conference on machine learning , pages 2664–2672. PMLR, 402
2016. 403
[42] Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-wasserstein 404
learning for graph matching and node embedding. In International conference on machine 405
learning , pages 6932–6941. PMLR, 2019. 406
[43] Vayer Titouan, Nicolas Courty, Romain Tavenard, and Rémi Flamary. Optimal transport for 407
structured data with application on graphs. In International Conference on Machine Learning , 408
pages 6275–6284. PMLR, 2019. 409
[44] Thibault Séjourné, François-Xavier Vialard, and Gabriel Peyré. The unbalanced gromov 410
wasserstein distance: Conic formulation and relaxation. Advances in Neural Information 411
Processing Systems , 34:8766–8779, 2021. 412
[45] Laetitia Chapel, Mokhtar Z Alaya, and Gilles Gasso. Partial optimal tranport with applications 413
on positive-unlabeled learning. Advances in Neural Information Processing Systems , 33:2903– 414
2913, 2020. 415
[46] Nicolò De Ponti and Andrea Mondino. Entropy-transport distances between unbalanced metric 416
measure spaces. Probability Theory and Related Fields , 184(1-2):159–208, 2022. 417
[47] Weijie Liu, Chao Zhang, Jiahao Xie, Zebang Shen, Hui Qian, and Nenggan Zheng. Partial 418
gromov-wasserstein learning for partial graph matching. arXiv preprint arXiv:2012.01252 , 419
2020. 420
[48] Alexis Thual, Quang Huy Tran, Tatiana Zemskova, Nicolas Courty, Rémi Flamary, Stanislas 421
Dehaene, and Bertrand Thirion. Aligning individual brains with fused unbalanced gromov 422
wasserstein. Advances in Neural Information Processing Systems , 35:21792–21804, 2022. 423
[49] Cédric Villani. Topics in optimal transportation , volume 58. American Mathematical Soc., 424
2021. 425
[50] Benedetto Piccoli and Francesco Rossi. Generalized wasserstein distance and its application to 426
transport equations with source. Archive for Rational Mechanics and Analysis , 211(1):335–358, 427
2014. 428
[51] Florian Beier, Robert Beinert, and Gabriele Steidl. On a linear gromov–wasserstein distance. 429
IEEE Transactions on Image Processing , 31:7292–7305, 2022. 430
[52] Vayer Titouan, Nicolas Courty, Romain Tavenard, Chapel Laetitia, and Rémi Flamary. Optimal 431
transport for structured data with application on graphs. In Kamalika Chaudhuri and Ruslan 432
Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , 433
volume 97 of Proceedings of Machine Learning Research , pages 6275–6284, Long Beach, 434
California, USA, 09–15 Jun 2019. PMLR. 435
[53] Xinran Liu, Yikun Bai, Huy Tran, Zhanqi Zhu, Matthew Thorpe, and Soheil Kolouri. Ptlp: 436
Partial transport lpdistances. In NeurIPS 2023 Workshop Optimal Transport and Machine 437
Learning , 2023. 438
[54] Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY , 55(58- 439
63):94, 2015. 440
12[55] Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, 441
Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Léo 442
Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, 443
Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander 444
Tong, and Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research , 445
22(78):1–8, 2021. 446
[56] Jessa Bekker and Jesse Davis. Learning from positive and unlabeled data: A survey. Machine 447
Learning , 109:719–760, 2020. 448
[57] Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data. In 449
Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and 450
data mining , pages 213–220, 2008. 451
[58] Masahiro Kato, Takeshi Teshima, and Junya Honda. Learning from positive and unlabeled data 452
with a selection bias. In International conference on learning representations , 2018. 453
[59] Yu-Guan Hsieh, Gang Niu, and Masashi Sugiyama. Classification from positive, unlabeled 454
and biased negative data. In International Conference on Machine Learning , pages 2820–2829. 455
PMLR, 2019. 456
[60] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to 457
new domains. In Computer Vision–ECCV 2010: 11th European Conference on Computer Vision, 458
Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11 , pages 213–226. 459
Springer, 2010. 460
[61] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor 461
Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In 462
International conference on machine learning , pages 647–655. PMLR, 2014. 463
13A Notation and Abbreviations 464
• OT: Optimal Transport. 465
• POT: Partial Optimal Transport. 466
• GW: Gromov-Wasserstein. 467
• PGW: Partial Gromov-Wasserstein. 468
• FW: Frank-Wolfe. 469
• MPGW: Mass-Constrained Partial Gromov-Wasserstein. 470
•∥ · ∥: Euclidean norm. 471
•X2=X×X. 472
•M+(X): set of all positive (non-negative) Randon (finite) measures defined on X. 473
•P2(X): set of all probability measures defined on X, whose second moment is finite. 474
•R+: set of all non-negative real numbers. 475
•Rn×m: set of all n×mmatrices with real coefficients. 476
•Rn×m
+ (resp.Rn
+): set of all n×mmatrices (resp., n-vectors) with non-negative coefficients. 477
•Rn×m×n×m: set of all n×m×n×mtensors with real coefficients. 478
•1n,1n×m,1n×m×n×m: vector, matrix, and tensor of all ones. 479
• 1E: characteristic function of a measurable set E 480
1E(z) =1ifz∈E,
0otherwise.
•X,Y: metric measure spaces (mm-spaces): X= (X, d X, µ),Y= (Y, dY, ν). 481
•CX: given a discrete mm-space X= (X, d X, µ), where X={x1, . . . , x n}, the symmetric 482
matrix CX∈Rn×nis defined as CX
i,i′=dq
X(xi, x′
i). 483
•µ⊗2: product measure µ⊗µ. 484
•T#σ:T:X→Yis a measurable function and σis a measure on X.T#σis the push- 485
forward measure of σ, i.e., its is the measure on Ysuch that for all Borel set A⊂Y, 486
T#σ(A) =σ(T−1(A)). 487
•γ, γ1, γ2:γis a joint measure defined in a product space having γ1, γ2as its first and second 488
marginals, respectively. In the discrete setting, they are viewed as matrices and vectors, i.e., 489
γ∈Rn×m
+ , and γ1=γ1m∈Rn
+,γ2=γ⊤1n∈Rm
+. 490
•π1:X×Y→X, canonical projection mapping, with (x, y)7→x. Similarly, π2:X×Y→ 491
Yis canonical projection mapping, with (x, y)7→y. 492
•π1,2:S×X×Y→X×Y, canonical projection mapping, with (s, x, y )→(x, y). 493
Similarly, π0,1maps (s, x, y )to(s, x);π0,2maps (s, x, y )to(s, y). 494
•Γ(µ, ν), where µ∈ P2(X), ν∈ P2(Y)(where X, Y may not necessarily be the same set): 495
it is the set of all the couplings (transportation plans) between µandν, i.e., Γ(µ, ν) :={γ∈ 496
P2(X×Y) :γ1=µ, γ2=ν}. 497
•Γ(p,q): set of all the couplings between the discrete probability measures µ=Pn
i=1pX
iδxi498
andν=Pm
j=1qY
jδyjwith weight vectors 499
p = [pX
1, . . . , pX
n]⊤and q = [qY
1, . . . , qY
m]⊤. (20)
That is, Γ(p,q)coincides with Γ(µ, ν), but it is viewed as a subset of n×mmatrices 500
defined in (12). 501
•p, q: real numbers 1≤p, q < ∞. 502
•p,q: vectors of weights as in (20). 503
•p = [p1, . . . , p n]≤p′= [p′
1, . . . , p′
n]ifpj≤p′
jfor all 1≤j≤n. 504
•|p|=Pn
i=1piforp = [p1, . . . , p n]. 505
14•c(x, y) :X×Y→R+denotes the cost function used for classical and partial optimal 506
transport problems. lower-semi continuous function. 507
•OT(µ, ν): it is the classical optimal transport (OT) problem between the probability mea- 508
sures µandνdefined in (1). 509
•Wp(µ, ν): it is the p-Wasserstein distance between the probability measures µandνdefined 510
in (2), for 1≤p <∞. 511
•POT (µ, ν;λ): the Partial Optimal Transport (OPT) problem defined in (3). 512
•|µ|: total variation norm of the positive Randon (finite) measure µdefined on a measurable 513
space X, i.e.,|µ|=µ(X). 514
•µ≤σ: denotes that for all Borel set B⊆Xwe have that the measures µ, σ∈ M +(X) 515
satisfy µ(B)≤σ(B). 516
•Γ≤(µ, ν), where µ∈ M +(X), ν∈ M +(Y): set of all “partial transportation plans”
Γ≤(µ, ν) :={γ∈ M +(X×Y) :γ1≤µ, γ2≤ν}.
•Γ≤(p,q): set of all the “partial transportation plans” between the discrete probability 517
measures µ=Pn
i=1pX
iδxiandν=Pm
j=1qY
jδyjwith weight vectors p = [ pX
1, . . . , pX
n] 518
andq = [ qY
1, . . . , qY
m]. That is, Γ≤(p,q)coincides with Γ≤(µ, ν), but it is viewed as a 519
subset of n×mmatrices defined in (13). 520
•λ >0: positive real number. 521
•ˆ∞: auxiliary point. 522
•ˆX=X∪ {ˆ∞}. 523
•ˆµ,ˆν: given in (4). 524
•ˆ p,ˆ q: given in (53). 525
•ˆγ: given in (6). 526
•ˆc(·,·) :ˆX×ˆY→R+: cost as in (5). 527
•L:R×R→R: cost function for the GW problems. 528
•D:R×R→R: generic distance on Rused for GW problems. 529
•GWL(·,·): GW optimization problem given in (7). 530
•GWp(·,·): GW optimization problem given in (7) when L(a, b) =|a−b|p. 531
•GWL
q(·,·): general GW optimization problem for g≥1given in (33). 532
•GWp
q(·,·): general GW optimization problem for q≥1andL(a, b) =|a−b|pgiven in 533
(34). 534
•GWp
λ,q(·,·): generalized GW problem given in (39). 535
•dGW: GW-variant problem given in (51) for the general case, and in (55) for the discrete 536
setting. 537
•ˆL: cost given in (16) for the GW-variant problem. 538
•d:ˆX×ˆX→R+∪ {∞} : “generalized” metric given in (50) for ˆX. 539
•X∼Y: equivalence relation in for mm-spaces, X∼Yif and only if they have the same 540
total mass and GWp
q(X,Y) = 0 . 541
•PGWL
λ,q(·,·): partial GW optimization problem given in (9) or, equivalently, in (10). 542
•PGWp
λ,q(·,·): partial GW optimization problem given in (10) when L(a, b) =|a−b|p. 543
•PGW λ(·,·): is is the PGW problem PGWp
λ,q(·,·)for the case when p= 2 = q. 544
•µ(ϕ): given a measure µand a function ϕ,
µ(ϕ) :=Z
ϕ(x)dµ(x).
15•C(γ;λ, µ, ν ): the transportation cost induced by transportation plan γ∈Γ≤(µ, ν)in the
Partial GW problem 10,
C(γ;λ, µ, ν ) :=γ⊗2(L(dq
X, dq
Y)) +λ(|µ|2+|ν|2−2|γ|2).
•L: functional for the optimization problem PGW λ(·,·). 545
•M,˜M, and ˆM: see (14), and (54). Notice that, (M−2λ)i,i′,j,j′:=Mi,i′,j,j′−2λ. 546
•⟨·,·⟩F: Frobenius inner product for matrices, i.e., ⟨A, B⟩F= trace( A⊤B) = 547Pn,m
i,jAi,jBi,jfor all A, B∈Rn×m. 548
•M◦γ: product between the tensor Mand the matrix γ. 549
•∇: gradient. 550
•[1 :n] ={1, . . . , n }. 551
•α: step size based on the line search method. 552
•γ(1): initialization of the algorithm. 553
•γ(k),γ(k)′: previous and new transportation plans before and after step 1 in the k−th 554
iteration of version 1 of our proposed FW algorithm. 555
•ˆγ(k),ˆγ(k)′: previous and new transportation plans before and after step 1 in the k−th 556
iteration of version 2 of our proposed FW algorithm. 557
•G= 2˜M◦γ,ˆG= 2ˆM◦ˆγ: Gradient of the objective function in version 1 and version 2, 558
respectively, of our proposed FW algorithm for solving the discrete version of partial GW 559
problem. 560
•(δγ, a, b )and(δˆγ, a, b ): given in (18) and(56) for versions 1 and 2 of the algorithm, 561
respectively. 562
•C1-function: continuous and with continuous derivatives. 563
•MPGW ρ(·,·): Mass-Constrained Partial Gromov-Wasserstein defined in (73) 564
•Γρ
≤(µ, ν): set transport plans defined in (74) for the Mass-Constrained Partial Gromov- 565
Wasserstein problem. 566
•ΓPU,π(p,q): defined in (87). 567
B Proof of Proposition 3.2 568
The idea of the proof is inspired by the proof of Proposition 1 in [50]. 569
The goal is to verify that 570
PGWL
λ,q(X,Y)
:= inf
γ∈M +(X,Y)Z
(X×Y)2L(dq
X(x, x′), dq
Y(y, y′))dγ⊗2
| {z }
transport GW cost+λ 
|µ⊗2−γ⊗2
1|+|ν⊗2−γ⊗2
2|
| {z }
mass penalty
= inf
γ∈Γ≤(µ,ν)Z
(X×Y)2L(dq
X(x, x′), dq
Y(y, y′))dγ⊗2+λ 
|µ⊗2−γ⊗2
1|+|ν⊗2−γ⊗2
2|
.(21)
Consider γ∈ M +(X×Y)such that γ1≤µdoes not hold. Then we can write the Lebesgue
decomposition of γ1with respect to µ:
γ1=fµ+µ⊥,
where f≥0is the Radon-Nikodym derivative of γ1with respect to µ, andµ⊥, µare mutually singular,
that is, there exist measurable sets A, B such that A∩B=∅,X=A∪Bandµ⊥(A) = 0 , µ(B) = 0 .
Without loss of generality, we can assume that the support of flies on A, since
γ1(E) =Z
E∩Af(x)dµ(x) +µ⊥(E∩B)∀E⊆Xmeasurable .
16Define A1={x∈A:f(x)>1}, A2={x∈A:f(x)≤1}(both are measurable, since fis
measurable), and define ¯µ= min {f,1}µ. Then,
¯µ≤µ and ¯µ≤fµ≤fµ+µ⊥=γ1.
There exists a ¯γ∈ M +(X×Y)such that ¯γ1= ¯µ,¯γ≤γ, and ¯γ2≤γ2. Indeed, we can construct ¯γ
in the following way: First, let {γx}x∈Xbe the set of conditional measures (disintegration) such that
for every measurable (test) function ψ:X×Y→Rwe have
Z
ψ(x, y)dγ(x, y) =Z
XZ
Yψ(x, y)dγx(y)dγ1(x).
Then, define ¯γas
¯γ(U) :=Z
XZ
Y1U(x, y)dγx(y)d¯µ(x)∀U⊆X×YBorel .
Then, ¯γverifies that ¯γ1= ¯µ, and since ¯µ≤γ1, we also have that ¯γ≤γ, which implies ¯γ2≤γ2. 571
Since|γ1|=|γ2|and|¯γ1|=|¯γ2|, then we have |γ⊗2
1−¯γ⊗2
1|=|γ⊗2
2−¯γ⊗2
2|. 572
We claim that 573
|µ⊗2−γ⊗2
1| ≥ |µ⊗2−¯γ⊗2
1|+|γ⊗2
1−¯γ⊗2
1|. (22)
•Left-hand side of (22): Since {A, B}is a partition of X, we first spit the left-hand side of 574
(22) as 575
|µ⊗2−γ⊗2
1|= (µ⊗2−γ⊗2
1)(A×A)| {z }
(I)+ (µ⊗2−γ⊗2
1)(A×B) + (µ⊗2−γ⊗2
1)(B×A)| {z }
(II)
+ (µ⊗2−γ⊗2
1)(B×B)| {z }
(III).
Then we have 576
(III) = (µ⊗2−γ⊗2
1)(B×B) =µ⊥⊗µ⊥(B×B) =|µ⊥|2,
(II) = (µ⊗2−γ⊗2
1)(A×B) + (µ⊗2−γ⊗2
1)(B×A) = 2|µ⊥|(µ−γ1)(A).
Since γ1=fµinA, then ¯γ1=γ1inA2and¯γ1=µinA1, so we have 577
(µ−γ1)(A) = (µ−γ1)(A1) + (µ−γ1)(A2) = (γ1−¯γ1)(A1) + (µ−¯γ1)(A2)
= (γ1−¯γ1)(A) + (µ−¯γ1)(A).
Thus, 578
(II) = 2|µ⊥|((γ1−¯γ1)(A) + (µ−¯γ1)(A)),
and we also get that 579
(I) = (µ⊗2−γ⊗2
1)(A×A)
= (µ⊗2−γ⊗2
1)(A1×A1) + (µ⊗2−γ⊗2
1)(A2×A2) + (µ⊗2−γ⊗2
1)(A1×A2)
+ (µ⊗2−γ⊗2
1)(A2×A1)
= (γ⊗2
1−¯γ⊗2
1)(A1×A1) + (µ⊗2−¯γ⊗2
1)(A2×A2)+
+|¯γ1⊗µ−γ1⊗¯γ1|(A1×A2) +|µ⊗¯γ1−¯γ1⊗γ1|(A2×A1)
= (γ⊗2
1−¯γ⊗2
1)(A1×A1) + (µ⊗2−¯γ⊗2
1)(A2×A2) + 2(¯ γ1−γ1)(A1)(µ−¯γ1)(A2)
= (γ⊗2
1−¯γ⊗2
1)(A×A) + (µ⊗2−¯γ⊗2
1)(A×A) + 2(¯ γ1−γ1)(A1)(µ−¯γ1)(A2)| {z }
≥0.
•Right-hand side of (22):First notice that 580
(γ1−¯γ1)(B) = (γ1−¯γ1)(B)≤γ1(B) =|µ⊥|,
17and since ¯γ1≤µandµ(B) = 0 , we have 581
(µ−¯γ1)(B) = 0 .
Then, 582
|µ⊗2−¯γ⊗2
1|+|γ⊗2
1−¯γ⊗2
1|=
= (µ⊗2−¯γ⊗2
1)(A×A) + (γ⊗2
1−¯γ⊗2
1)(A×A) + (µ⊗2−¯γ⊗2
1)(B×B)
+ (γ⊗2
1−¯γ⊗2
1)(B×B) + (µ⊗2−¯γ⊗2
1)(A×B) + (γ⊗2
1−¯γ⊗2
1)(A×B)
+ (µ⊗2−¯γ⊗2
1)(B×A) + (γ⊗2
1−¯γ⊗2
1)(B×A)
≤(µ⊗2−¯γ⊗2
1)(A×A) + (γ⊗2
1−¯γ⊗2
1)(A×A)| {z }
≤(I)+|µ⊥|2
|{z}
=(III)+ 2|µ⊥|(γ1−¯γ1)(A)| {z }
=(II).
Thus, (22) holds. 583
We finish the proof of the proposition by noting that 584
|µ⊗2−¯γ⊗2
1|+|ν⊗2−¯γ⊗2
2| ≤ |µ⊗2−γ⊗2
1| − |γ⊗2
1−¯γ⊗2
1|+|ν⊗2−¯γ⊗2
2|
=|µ⊗2−γ⊗2
1| − |γ⊗2
2−¯γ⊗2
2|+|ν⊗2−¯γ⊗2
2|
≤ |µ⊗2−γ⊗2
1|+|ν⊗2−γ⊗2
2|
where the first inequality follows from (22), and the second inequality holds from the fact the total 585
variation norm | · |satisfies triangular inequality. Therefore ¯γinduces a smaller transport GW cost 586
thanγ(since ¯γ≤γ), and also ¯γdecreases the mass penalty in comparison that corresponding to 587
γ. Thus, ¯γis a better GW transportation plan, which satisfies ¯γ1≤µ. Similarly, we can further 588
construct ¯γ′based on ¯γsuch that ¯γ′
1≤µ,¯γ′
2≤ν. Therefore, we can restrict the minimization in (9) 589
fromM+(X×Y)toΓ≤(µ, ν). Thus, the equality (21) is satisfied. 590
Proof of Remark 3.1. Given γ∈Γ≤(µ, ν), since γ1≤µ,γ2≤ν, and γ1(X) =|γ1|=|γ|= 591
|γ2|=γ2(Y), we have 592
|µ⊗2−γ⊗2
1|+|ν⊗2−γ⊗2
2|=µ⊗2(X2)−γ⊗2
1(X2) +ν⊗2(Y2)−γ⊗2
2(Y2)
=|µ|2+|ν|2−2|γ|2,
and so the transportation cost in partial GW problem (10) becomes 593
C(γ;λ, µ, ν )
:=Z
(X×Y)2L(dq
X(x, x′), dq
Y(y, y′))dγ(x, y)dγ(x′, y′) +λ 
|µ⊗2−γ⊗2
1|+|ν⊗2−γ⊗2
2|
=Z
(X×Y)2L(dq
X(x, x′), dq
Y(y, y′))dγ(x, y)dγ(x′, y′) +λ 
|µ|2+|ν|2−2|γ|2
=Z
(X×Y)2(L(dq
X(x, x′), dq
Y(y, y′)−2λ)dγ(x, y)dγ(x′, y′) +λ 
|µ|2+|ν|2
|{z }
does not depend on γ. (23)
594
C Proof of Proposition 3.3 595
In this section, we discuss the minimizer of the Partial GW problem (9). Trivially, Γ≤(µ, ν)⊆ 596
M+(X×Y)and by using Proposition 3.2 it is enough to show that a minimizer for problem (10) 597
exists. 598
We refer the reader to [8, Chapters 5 and 10] for similar ideas. 599
18C.1 Formal Statement of Proposition 3.3 600
Suppose X, Y are compact sets, then exists compact set [0, β]⊂R, such that
d(x, x′), d(y, y′)∈[0, β],∀x, x′∈X, y, y′∈Y
LetA= [0, βq]. LetLA2denote the restriction of LonA2, i.e.LA2:A2→RwithLA2(r1, r2) = 601
L(r1, r2),∀r1, r2∈A. Suppose Lsatisfies the following: there exists 0< K < ∞such that for 602
every r1, r′
1, r2, r′
2∈A, 603
|LA2(r1, r2)−LA2(r′
1, r2)| ≤K|r1−r′
1|,|LA2(r1, r2)−LA2(r1, r′
2)| ≤K|r2−r′
2|(24)
(i.e.,LA2is Lipschitz on each variable). Then PGWL
λ(·,·)admits a minimizer. 604
Note, the condition (24) contains the case L(r1, r2) =|r1−r2|pas a special case: 605
Lemma C.1. IfL(r1, r2) =|r1−r2|p, for1≤p <∞, then Lsatisfies the condition (24). 606
Proof. Assume that Lis defined on an interval of the form [0, M], for some M > 0. Consider 607
r1, r′
1, r2, r′
2∈[0, M]. Ifp= 1, by triangle inequality we have 608
|L(r1, r2)−L(r′
1, r2)|=||r1−r2| − |r′
1−r2|| ≤ |r1−r′
1|
and similarly, 609
|L(r1, r2)−L(r1, r′
2)| ≤ |r2−r′
2|.
From [ 8, page 473], since for 1≤p <∞, the function t7→tp, fort∈[0, M], is Lipschitz with 610
constant bounded by pMp−1, we have 611
|L(r1, r2)−L(r′
1, r2)| ≤pMp−1|r1−r′
1|.
and similarly, 612
|L(r1, r2)−L(r1, r′
2)| ≤pMp−1|r2−r′
2|.
613
Lemma C.2. Given q≥1, consider β >0. Then [0, β]∋c7→cq∈[0, βq]is a Lipschitz function. 614
Proof. Given c1, c2∈[0, β], we have 615
|cq
1−cq
2| ≤qβq−1|c1−c2| (25)
Thus, c7→cqis a Lipschitz function. 616
C.2 Convergence Auxiliary Result 617
If a sequence {γn}converges weakly to γ, we write γnw⇀ γ. In this setting, if γnw⇀ γ, it does not 618
imply that (γn)⊗2w⇀ γ⊗2. Thus, the technique used in classical OT for proving the existence of a 619
minimizer for the optimal transport optimization problem as a consequence of the Stone-Weierstrass 620
theorem does not apply directly in the Gromov-Wasserstein context. 621
Inspired by [8], we introduce the following lemma. 622
Lemma C.3. Given metric space (Z, dZ), suppose ϕ:R2→Ris a Lipschitz continuous function
with respect to (Z2, d+
Z), where
d+
Z((z1, z2),(z′
1, z′
2)) := dZ(z1, z′
1) +dZ(z2, z′
2),∀(z1, z2),(z′
1, z′
2)∈Z2.
Given γ∈ M +(Z), and a sequence {γn}n≥1∈ M +(Z)such that converges weakly to γ,
γnw⇀ γ (n→ ∞ ).
Finally, consider the mapping
Z∋z7→γ(ϕ(z,·)) :=Z
Zϕ(z, z′)dγ(z′)∈R.
Then we have the following results: 623
19(1)γn(ϕ(z,·))→γ(ϕ(z,·))uniformly (when n→ ∞ ). 624
(2)(γn)⊗2(ϕ(·,·))→γ⊗2(ϕ(·,·))(when n→ ∞ ). 625
(3)IfM ⊂ M +(Z)is compact for the weak convergence, then infγ∈Mγ⊗2(ϕ(·,·))admits a 626
minimizer. 627
Proof. The main idea of the proof is similar to [ 8, Lemma 10.3]: we extend it from P+(Z)to 628
M+(Z). 629
(1)Since γnw⇀ γ, andZis compact, we have |γn| → |γ|. Then, given ϵ >0, fornsufficiently 630
large we have |γn| ≤ |γ|+ϵ. 631
Let us denote by ∥ϕ∥Lipthe Lipschitz constant of ϕ. For any z1, z2∈Z, we have: 632
|γn(ϕ(z1,·))−γn(ϕ(z2,·))| ≤Z
Z|ϕ(z1, z)−ϕ(z2, z)|γn(z)
≤max
z∈Z|ϕ(z1, z)−ϕ(z2, z)|(|γ|+ϵ)
≤(|γ|+ϵ)∥ϕ∥LipdZ(z1, z2) =KdZ(z1, z2),
where K= (|γ|+ϵ)∥ϕ∥Lipis a finite positive value. Note that the above inequality also 633
holds if we replace γnbyγ. 634
Since (Z, dZ)is compact, Z=SN
i=1B(zi, ϵ/K )for some z1, . . . , z N∈Z, where
B(zi, ϵ/3K) ={z∈Z:dZ(z, zi)≤ϵ/3K}is the closed ball centered at zi, with
radius ϵ/K. By definition of weak convergence, when nis sufficiently large,
|γn(ϕ(zi,·))−γ(ϕ(zi,·))|< ϵ/3, for each i∈[1 :N].
Given z∈Z, then z∈B(zi)for some zi. For sufficiently large n, we have: 635
|γn(ϕ(z,·))−γ(ϕ(z,·))|
≤ |γn(ϕ(z,·))−γn(ϕ(zi,·))|+|γn(ϕ(zi,·))−γ(ϕ(zi,·))|+|γ(ϕ(zi,·))−γ(ϕ(z,·))|
≤Kd(z, zi) +ϵ/3 +Kd(z, zi) =ϵ/3 +ϵ/3 +ϵ/3 =ϵ. (26)
Thus we prove the first statement. 636
(2) We recall that we do not have (γn)⊗2w⇀ γ⊗2. 637
Consider an arbitrary ϵ >0. We have, 638
0≤lim sup
n→∞|(γn)⊗2(ϕ)−(γ)⊗2(ϕ)| (27)
≤lim sup
n→∞|(γn⊗γn)(ϕ)−(γ⊗γn)(ϕ)|| {z }
An+ lim sup
n→∞|(γn⊗γ)(ϕ)−(γ⊗γ)(ϕ)|| {z }
Bn.
For the first term, when nis sufficiently large, by statement (1), we have: 639
An=Z
(γn(ϕ(z,·))−γ(ϕ(z,·))dγn(z)
≤max
z|γn(ϕ(z,·))−γ(ϕ(z,·)||γn|
≤ϵ(|γ|+ϵ) (28)
Thus, lim supnA= lim nA= 0. 640
Similarly, for the second term, when nis sufficiently large, we have 641
Bn:=Z
(γn(ϕ(z,·))−γ(ϕ(z,·)))dγ(z)≤ϵ|γ|. (29)
Thus, lim supnBn= lim nBn= 0. 642
Therefore, from (27), (28) and (29), we obtain 643
lim sup
n→∞|(γn)⊗2(ϕ)−(γ)⊗2(ϕ)|= lim
n→∞|(γn)⊗2(ϕ)−(γ)⊗2(ϕ)|= 0. (30)
20(3)Letγn∈ M be a sequence such that (γn)⊗2(ϕ)(weakly) converges to infγ∈Mγ⊗2(ϕ).
SinceMis compact, there exists a sub-sequence γnkw⇀ γ for some γ∈ M . Then, by
statement (2), we have:
γ⊗2(ϕ) = lim
k(γnk)⊗2(ϕ) = inf
γ∈Mγ⊗2(ϕ),
and we complete the proof. 644
645
C.3 Proof of the Formal Statement for Proposition 3.3 646
The proof follows the ideas of [8, Corollary 10.1]. 647
Define (Z, dZ)asZ:=X×Y, with dZ((x, y),(x′, y′)) := dX(x, x′) +dY(y, y′). 648
We claim that the following mapping 649
(X×Y)2=Z2→R
((x, y),(x′, y′))7→ϕ((x, y),(x′, y′)) := L(dq
X(x, x′), dq
Y(y, y′))−2λ
is a Lipschitz function with respect to d+
Z, where Lsatisfies (24). Indeed, given 650
((x1, y1),(x′
1, y′
1)),((x2, y2),(x′
2, y′
2))∈Z2, we have: 651
|ϕ((x1, y1),(x′
1, y′
1))−ϕ((x2, y2),(x′
2, y′
2))|
=|L(dX(x1, x′
1), dY(y1, y′
1))−L(dX(x2, x′
2), dY(y2, y′
2))|
≤ |L(dX(x1, x′
1), dY(y1, y′
1))−L(dX(x2, x′
2), dY(y1, y′
1))|
+|L(dX(x2, x′
2), dY(y1, y′
1))−L(dX(x2, x′
2), dY(y2, y′
2))|
≤K|dq
X(x1, x′
1)−dq
X(x2, x′
2)|+K|dq
Y(y1, y′
1)−dq
Y(y2, y′
2)|
≤K′|dX(x1, x′
1)−dX(x2, x′
2)|+K′|dY(y1, y′
1)−dY(y2, y′
2)| (31)
≤K′(dX(x1, x′
2) +dX(x′
1, x′
2)) +K′(dY(y1, y2) +dY(y′
1, y′
2)) (32)
=K′[((dX(x1, x2) +dY(y1, y2)) + (( dX(x′
1, x′
2) +dY(y′
1, y′
2))]
=K′[dZ((x1, y1),(x2, y2)) +dZ((x′
1, y′
1),(x′
2, y′
2))]
=K′d+
Z(((x1, y1),(x2, y2)),((x1, y1),(x2, y2)))
where in (31),K′=qβq−1K; the inequality holds by lemma C.2; The inequality (32) follows from 652
the triangle inequality: 653
dX(x1, x′
1)−dX(x2, x′
2)≤dX(x1, x2) +dX(x2, x′
2) +dX(x′
2, x′
1)−dX(x2, x′
2)
=dX(x1, x2) +dX(x′
1, x′
2),
and similarly,
dX(x2, x′
2)−dX(x1, x′
1)≤dX(x1, x2) +dX(x′
1, x′
2).
LetM= Γ≤(µ, ν). From [ 53, Proposition B.1], we have that Γ≤(µ, ν)is a compact set with respect 654
to the weak convergence topology. 655
By Lemma (C.3) part (3), we have the PGW problem, which can be written as 656
inf
γ∈Γ≤(µ,ν)γ⊗2(ϕ) +λ(|µ|2+|ν|2)
admits a solution, i.e., a minimizer γ∈Γ≤(µ, ν). Therefore, we end the proof of Proposition 3.3. 657
D Proof of Proposition 3.4: Metric Property of Partial GW 658
LetL(r1, r2) =Dp(r1, r2)for a metric DonR, and since all the metrics in Rare equivalent, for 659
simplicity, consider D(r1, r2) =|r1−r2|. (Notice that this satisfies the hypothesis of Proposition 660
H.1 used in the experiments). 661
21Consider the GW problem, for q≥1, 662
GWL
q(X,Y) := inf
γ∈Γ(µ,ν)Z
(X×Y)2L(dq
X(x, x′), dq
Y(y, y′))dγ⊗2, (33)
or, in particular, 663
GWp
q(X,Y) := inf
γ∈Γ(µ,ν)Z
(X×Y)2|dq
X(x, x′)−dq
Y(y, y′)|pdγ⊗2. (34)
For probability mm-spaces we have the equivalence relation X∼Yif and only if GWp
q(X,Y) = 0 . 664
By [8, Chapter 5], X∼Yis equivalent to the following: there exists a bijective isometry mapping 665
ϕ:X→Y, such that 666
dX(x, x′)−dY(ϕ(x), ϕ(x′)) = 0 , µ⊗2−a.s.
ϕ#µ=ν.
Remark D.1. In the literature, the case where q= 1is the most frequently considered problem. In 667
particular, in [ 8] it is stated the equivalence relation X∼Yif and only if there exists ϕ:X→Y 668
such that ϕ#µ=νanddX(x, x′) =dY(ϕ(x), ϕ(x′))µ⊗2−a.s.if and only if GWp
1(X,Y) = 0 . 669
Thus,X∼Yis also equivalent to have ϕ:X→Ysuch that ϕ#µ=νanddX(x, x′) =dY(y, y′) 670
γ⊗2−a.s.where γis a minimizer for GWp
1(X,Y). So, in this situation we also have dq
X(x, x′) = 671
dq
Y(y, y′)γ⊗2−a.s.for any given q≥1. Therefore, X∼Yif and only if GWp
q(X,Y) = 0 . 672
D.1 Formal Statement of Proposition 3.4 673
We first introduce the formal statement of Proposition 3.4. To do so, we extend the equivalence relation 674
∼to all mm-spaces (not only probability mm-spaces): Given arbitrary mm-spaces X= (X, d X, µ), 675
Y= (Y, dY, ν), where X, Y are compact and µ∈ M +(X),ν∈ M +(Y), we write X∼Yif and 676
only if they have the same total mass (i.e., |µ|=µ(X) =ν(Y) =|ν|) and GWp
q(X,Y) = 0 . 677
Formal statement of Proposition 3.4: Given λ >0,1≤p, q < ∞, then (PGWp
λ,q(·,·))1/pdefines 678
a metric among mm-spaces under taking quotient with respect to the equivalence relation ∼. 679
Next, we discuss its proof. 680
D.2 Non-Negativity and Symmetry Properties 681
It is straightforward to verify PGWp
λ,q(X,Y)≥0, and that PGWp
λ,q(X,Y) =PGWp
λ,q(Y,X). In 682
what follows, we will concentrate on proving PGWp
λ,q(X,Y) = 0 if and only if X∼Y: 683
IfX∼Y, then|µ|=|ν|, and we have
0≤PGWp
λ,q(X,Y)≤GWp
q(X,Y) = 0 ,
where the inequality follows from the fact Γ(µ, ν)⊆Γ≤(µ, ν). Thus, PGWp
λ,q(X,Y) = 0 . 684
For the other direction, suppose that PGWp
λ,q(X,Y) = 0 . We claim that |µ|=|ν|and that there exist 685
an optimal plan γforPGWp
λ,q(X,Y)such that |µ|=|γ|=|ν|. Let us prove this by contradiction. 686
Assume |µ|<|ν|. For convenience, suppose |µ|2≤ |ν|2−ϵ, for some ϵ >0. Then, for each 687
γ∈Γ≤(µ, ν), we have |γ⊗2| ≤ |µ|2≤ |ν|2−ϵ, and so 688
PGWp
λ,q(X,Y)≥λ(|µ|2+|ν|2−2|γ|2)≥λ(|ν2| − |γ|2)≥λϵ > 0.
Thus, PGWp
λ,q(X,Y)>0, which is a contradiction. So, |µ|=|ν|. In addition, if γ∈Γ≤(µ, ν) 689
is optimal for PGWp
λ,q(X,Y), we have |γ|=|µ|=|ν|, thus γ∈Γ(µ, ν). Therefore, since 690
PGWp
λ,q(X,Y) = 0 , and for such optimal γwe have |γ|=|µ|=|ν|, we obtain 691
Z
(X×Y)2|dq
X(x, x′)−dq
Y(y, y′)|pdγ⊗2= 0.
As a result, dq
X(x, x′) =dq
Y(y, y′)γ⊗2−a.s., which implies that GWp
q(X,Y) = 0 , and so X∼Y. 692
22D.3 Triangle Inequality – Strategy: Convert the PGW Problem into a GW Problem 693
Consider three arbitrary mm-spaces S= (S, dS, σ),X= (X, d X, µ),Y= (Y, dY, ν). We define 694
ˆS= (ˆS, d ˆS,ˆσ),ˆX= (ˆX, d ˆX,ˆµ),ˆY= (ˆY , d ˆY,ˆν)in a similar way to that of Proposition G.1 but now 695
aiming to have new spaces with equal total mass: 696
First, introduce auxiliary points ˆ∞0,ˆ∞1,ˆ∞2and set 697


ˆS=S∪ {ˆ∞0,ˆ∞1,ˆ∞2},
ˆX=X∪ {ˆ∞0,ˆ∞1,ˆ∞2},
ˆY=Y∪ {ˆ∞0,ˆ∞1,ˆ∞2}.
Define ˆσ,ˆµ,ˆνas follows: 698

ˆσ=σ+|µ|δˆ∞1+|ν|δˆ∞2,
ˆµ=µ+|σ|δˆ∞0+|ν|δˆ∞2,
ˆν=ν+|σ|δˆ∞0+|µ|δˆ∞1.(35)
Note that ˆσis not supported on point ˆ∞0, similarly, ˆµis not supported on ˆ∞1,ˆνis not supported 699
onˆ∞2. In addition, we have |ˆµ|=|ˆν|=|ˆσ|=|µ|+|ν|+|σ|. (For a similar idea in classical 700
unbalanced optimal transport see, for example, [16].) 701
Finally, define dˆS:ˆS2→R∪ {∞} as follows: 702
dˆS(s, s′) =dS(s, s′)if(s, s′)∈S2,
∞ elsewhere.(36)
Note, dˆS(·,·)is not a rigorous metric in ˆSsince we allow dˆS=∞. Similarly, define dˆX, dˆY. As a 703
result, we have constructed new spaces 704
ˆS= (ˆS, d ˆS,ˆσ),ˆX= (ˆX, d ˆX,ˆµ),ˆY= (ˆY , d ˆY,ˆν). (37)
We define the following mapping Dλ: (R∪ {∞} )×(R∪ {∞} )→R+: 705
Dp
λ(r1, r2) =

|r1−r2|pifr1, r2<∞,
λ ifr1=∞, r2<∞or vice versa ,
0 ifr1=r2=∞.(38)
Note that Dλis not a rigorous metric since it may sometimes violate triangle inequality. See the 706
following lemma for a detailed and precise explanation. 707
Lemma D.2. LetDλ(·,·)denote the function defined in (38). For any r0, r1, r2∈R∪ {∞} , we 708
have the following: 709
•Dλ(r1, r2)≥0.Dλ(r1, r2) = 0 if and only if r1=r2, where r1=r2denotes that 710
r1=r2∈Rorr1=r2=∞. 711
•Except the case r1, r2∈R, r0=∞, for all other cases, we have
Dλ(r1, r2)≤Dλ(r1, r0) +Dλ(r2, r0).
Proof of Lemma D.2. It is straightforward to verify Dλ(·,·)≥0. 712
Now, consider r0, r1, r2∈R∪ {∞} . Ifr1=r2∈Rorr1=r2=∞, we have Dλ(r1, r2) = 0 . 713
Otherwise, Dλ(r1, r2)>0. So,Dλ(r1, r2) = 0 if and only if r1=r2. 714
For the second item, we have the following cases: 715
Case 1: r1, r2, r0∈R, 716
Dλ(r1, r2) =|r1−r2|
≤ |r1−r2|+|r2−r0|
=Dλ(r0, r1) +Dλ(r0, r2)
Case 2: r1, r2∈R, r0=∞. We do not need to verify the inequality in this case. 717
23Case 3: r1∈R, r2, r0=∞, orr1=∞, r2∈R, r0=∞. In this case, we have 718
Dλ(r1, r2) =Dλ(r1, r0) =√
λ, D λ(r2, r0) = 0
and it is straightforward to verify the inequality. 719
Case 4: r1, r2=∞, r3∈R. In this case, we have Dλ(r1, r2) = 0≤Dλ(r0, r1) +Dλ(r0, r2). 720
Case 5: r1, r2, r0=∞. In this case, we have 721
Dλ(r1, r2) =Dλ(r1, r0) =Dλ(r2, r0) = 0
and it is straightforward to verify the inequality. 722
We construct the following generalized GW problem : 723
GWp
λ,q(ˆX,ˆY) := inf
ˆγ∈Γ(ˆµ,ˆν)Z
(ˆX×ˆY)2Dp
λ(dq
ˆX(x, x′), dq
ˆY(y, y′))dˆγ⊗2
| {z }
ˆC(ˆγ;λ,ˆµ,ˆν). (39)
Similarly, we define GWp
λ,q(ˆX,ˆS), and GWp
λ,q(ˆS,ˆY). 724
The mapping (6) is modified as: 725
Γ≤(σ, µ)∋γ017→ˆγ01∈Γ(ˆσ,ˆµ),
ˆγ01:=γ01+ (σ−γ01
1)⊗δˆ∞0+δˆ∞1⊗(µ−γ01
2) +|γ|δˆ∞1,ˆ∞0+|ν|δˆ∞2,ˆ∞2;
Γ≤(σ, ν)∋γ027→ˆγ02∈Γ(ˆσ,ˆν),
ˆγ02:=γ02+ (σ−γ02
1)⊗δˆ∞0+δˆ∞2⊗(ν−γ02
2) +|γ|δˆ∞2,ˆ∞0+|µ|δˆ∞1,ˆ∞1;
Γ≤(µ, ν)∋γ127→ˆγ12∈Γ(ˆµ,ˆν),
ˆγ12:=γ12+ (µ−γ12
1)⊗δˆ∞1+δˆ∞2⊗(ν−γ12
2) +|γ|δˆ∞2,ˆ∞1+|µ|δˆ∞0,ˆ∞0. (40)
It is straightforward to verify the above mappings are well-defined. In addition, we can observe that, 726
for each γ01∈Γ≤(σ, µ), γ02∈Γ≤(σ, ν), γ12∈Γ≤(µ, ν), 727
ˆγ01({ˆ∞2} ×X) = ˆγ01(S× {ˆ∞2}) = 0 , (41)
ˆγ02({ˆ∞1} ×Y) = ˆγ02(S× {ˆ∞1}) = 0 , (42)
ˆγ12({ˆ∞0} ×Y) = ˆγ12(X× {ˆ∞0}) = 0 .
Proposition D.3. Ifγ12∈Γ≤(µ, ν)is optimal in PGW problem PGWp
λ,q(X,Y), then ˆγ12defined
in(40) is optimal in generalized GW problem GWp
λ,q(ˆX,ˆY). Furthermore, ˆC(ˆγ12;λ,ˆµ,ˆν) =
C(γ12;λ, µ, ν ),and thus,
PGWp
λ,q(X,Y) =GWp
λ,q(ˆX,ˆY).
Proof of Proposition D.3. For each γ∈Γ≤(µ, ν), define ˆγby (40). 728
Note that if we merge the points ˆ∞1,ˆ∞2,ˆ∞3asˆ∞, i.e.
ˆ∞= ˆ∞1= ˆ∞2= ˆ∞3,
the value ˆC(ˆγ;λ,ˆµ,ˆν)will not change. Thus, we merge these three auxiliary points. 729
24We have: 730
ˆC(ˆγ;λ,ˆµ,ˆν) =Z
(ˆX×ˆY)2Dp
λ(dq
ˆX(x, x′), dq
ˆY(x, x′))dˆγ⊗2
=Z
(X×Y)2|dq
X(x, x′)−dq
Y(y, y′)|pdˆγ⊗2+Z
({ˆ∞}× Y)2λdˆγ⊗2+Z
(X×{ˆ∞})2λˆγ⊗2
+ 2Z
({ˆ∞}× Y)×(X×Y)λdˆγ⊗2+ 2Z
(X×{ˆ∞})×(X×Y)λdˆγ⊗2+Z
({ˆ∞}×{ ˆ∞})2Dp
λ(∞,∞)dˆγ⊗2
+ 2Z
({ˆ∞}× Y)×(X×{ˆ∞})Dp
λ(∞,∞)dˆγ⊗2+ 2Z
({ˆ∞}×{ ˆ∞})×(X×Y)Dp
λ(∞,∞)dˆγ⊗2
+ 2Z
({ˆ∞}×{ Y})×{ˆ∞}2Dp
λ(∞,∞)dˆγ⊗2+ 2Z
(X×{ˆ∞})×{ˆ∞}2Dp
λ(∞,∞)dˆγ⊗2
=Z
(X×Y)2|dq
X(x, x′)−dq
Y(y, y′)|pdγ⊗2
+ 2λ(|ν| − |γ|)|γ|+λ(|ν| − |γ|)2+ 2λ(|µ| − |γ|)|γ|+λ(|µ| − |γ|)2
=Z
(X×Y)2|dq
X(x, y′)−dq
Y(y, y′)|pdγ⊗2) +λ(|ν2|+|µ|2−2|γ|2) =C(γ;λ, µ, ν ).
As we merged the points ˆ∞1,ˆ∞2,ˆ∞3, by [ 40, Proposition B.1.], the mapping γ7→ˆγdefined in (40)
is a bijection. Then, if γ∈Γ≤(µ, ν)is optimal for the PGW problem PGWp
λ,q(X,Y)(defined in
(10)),ˆγ∈Γ(ˆµ,ˆν)is optimal for generalized GW problem GWp
λ,q(ˆX,ˆY)(defined in (39)). Therefore,
GWp
λ,q(ˆX,ˆY) =PGWp
λ,q(X,Y).
731
Proposition D.4 (Triangle inequality for GWp
λ,q(·,·)).Consider the generalized GW problem (39).
Then, for any p∈[1,∞), we have
GWp
λ,q(ˆX,ˆY)≤GWp
λ,q(ˆS,ˆX) +GWp
λ,q(ˆS,ˆY).
Proof of Proposition D.4. We prove the case p= 2. For general p≥1, it can be proved similarly. 732
Choose an optimal γ12∈Γ≤(µ, ν)forPGW2
λ,q(X,Y), an optimal γ01∈Γ≤(σ, µ)for 733
PGW2
λ,q(S,X), and an optimal γ02∈Γ≤(σ, ν)forPGW2
λ,q(S,Y). Construct ˆγ12,ˆγ01,ˆγ02by 734
(40). 735
By Proposition D.3, we have that ˆγ12,ˆγ01,ˆγ02are optimal for GW2
λ,q(ˆX,ˆY),GW2
λ,q(ˆS,ˆX), 736
GW2
λ,q(ˆS,ˆY), respectively. 737
Define canonical projection mapping 738
π0,1:(ˆS×ˆX×ˆY)→(ˆS×ˆX)
(s, x, y )7→(s, x).
Similarly, we define π0,2, π1,2. 739
Bygluing lemma (see Lemma 5.5 [ 54]), there exists ˆγ∈ M +(ˆS×ˆX×ˆY), such that (π0,1)#ˆγ= 740
ˆγ01,(π0,2)#ˆγ= ˆγ02. Thus, (π1,2)#ˆγis a coupling between ˆµ,ˆν. We have 741
GW2
λ,q(X,Y) =Z
(ˆX×ˆY)2D2
λ(dq
ˆX(x, x′), dq
ˆY(y, y′))d(ˆγ12)⊗2
≤Z
(ˆS×ˆX×ˆY)2D2
λ(dq
ˆX(x, x′), dq
ˆY(y, y′))dˆγ⊗2. (43)
The inequality holds since (π1,2)#ˆγ,ˆγ12∈Γ(ˆµ,ˆν), and ˆγ12is optimal. 742
25Next, we will show that 743
Z
(ˆS×ˆX×ˆY)2D2
λ(dq
ˆX(x, x′), dq
ˆY(y, y′))dˆγ⊗2
≤Z
(ˆS×ˆX×ˆY)2(Dλ(dq
ˆS(s, s′), dq
ˆX(x, x′)) +Dλ(dq
ˆS(s, s′), dq
ˆY(y, y′)))2dˆγ⊗2.
Let((s, x, y ),(s′, x′, y′))∈(ˆS,ˆX,ˆY)2, and assume that 744
Dλ(d2
ˆX(x, x′), d2
ˆY(y, y′))> D λ(d2
ˆS(s, s′), d2
ˆX(x, x′)) +Dλ(d2
ˆS(s, s′), d2
ˆY(y, y′)). (44)
By Lemma D.2, (44) implies dˆX(x, x′), dˆY(y, y′)∈R, dˆS(s, s′) =∞. Thus, by definition (36), it 745
also implies 746
(x, x′)∈X2,(y, y′)∈Y2,(s, s′)∈ˆS2\S2. (45)
Define the following sets: 747
Aα=ˆS×X×Y,
A0={ˆ∞0} ×X×Y,
A1={ˆ∞1} ×X×Y,
A2={ˆ∞2} ×X×Y.
Notice that, (44) =⇒(45) is equivalent to 748
(44)=⇒((s, x, y ),(s, x′, y′))∈A:=2[
i=0(Ai×Aα)∪2[
i=0(Aα×Ai). (46)
Next, we will show ˆγ⊗2(A) = 0 . Indeed, 749
ˆγ(A0)≤ˆγ({∞0} ×ˆX×ˆY) = ˆσ({∞0}) = 0 by definition (35) of ˆσ,
ˆγ(A1)≤ˆγ({∞1} ×ˆX×Y) = ˆγ02({ˆ∞1×Y}) = 0 by (42) ,
ˆγ(A2)≤ˆγ({∞2} ×X×ˆY) = ˆγ01({ˆ∞2×X}) = 0 by (41) .
Thus, ˆγ⊗2(A) = 0 . By considering B= (ˆS×ˆX×Y)2\A,we obtain 750
Z
(ˆS×ˆX×ˆY)2D2
λ(dq
ˆX(x, x′), dq
ˆY(y, y′))dγ⊗2
=Z
BD2
λ(dq
ˆX(x, x′), dq
ˆY(y, y′))dγ⊗2since γ⊗2(A) = 0
≤Z
B
Dλ(dq
ˆS(s, s′), dq
ˆX(x, x′) +Dλ(dq
ˆS(s, s′), dq
ˆY(y, y′))2
dγ⊗2by (46)
≤Z
(ˆS×ˆX×ˆY)2
Dλ(dq
ˆS(s, s′), dq
ˆX(x, x′) +Dλ(dq
ˆS(s, s′), dq
ˆY(y, y′))2
dγ⊗2. (47)
26Following (43) and (47), we have 751
GW2
λ,q(ˆX,ˆY)≤ Z
(ˆS×ˆX×ˆY)2D2
λ(dq
ˆX(x, x′), dq
ˆY(y, y′))dˆγ⊗2!1/2
≤ Z
(ˆS×ˆX×ˆY)2
Dλ(dq
ˆS(s, s′), dq
ˆX(x, x′)) +Dλ(dq
ˆS(s, s′), dq
ˆY(y, y′))2
dγ⊗2!1/2
≤ Z
(ˆS×ˆX×ˆY)2D2
λ(dq
ˆS(s, s′), dq
ˆX(x, x′))dγ⊗2!1/2
+ Z
(ˆS×ˆX×ˆY)2D2
λ(dq
ˆS(s, s′), dq
ˆY(y, y′))dγ⊗2!1/2
(48)
= Z
(ˆS×ˆX×ˆY)2D2
λ(dq
ˆS(s, s′), dq
ˆX(x, x′))d(γ01)⊗2!1/2
+ Z
(ˆS×ˆX×ˆY)2D2
λ(dq
ˆS(s, s′), dq
ˆY(y, y′))d(γ02)⊗2!1/2
=GW2
λ,q(ˆS,ˆX) +GW2
λ,q(ˆS,ˆY),
where in the third inequality (48) we used the Minkowski inequality in L2((ˆS×ˆX×ˆY)2,ˆγ⊗2). 752
Now, we can complete the proof of Proposition 3.4: By the Propositions D.3, we have
PGWp
λ,q(X,Y) =GWp
λ,q(ˆX,ˆY)
and similarly for PGWp
λ,qand(S,X), PGWp
λ,q(S,Y). By the Proposition D.4, GWp
λ,q(·,·)satisfies 753
the triangle inequality, thus we complete the proof: 754
PGWp
λ,q(X,Y) =GWp
λ,q(ˆX,ˆY)
≤GWp
λ,q(ˆS,ˆX) +GWp
λ,q(ˆS,ˆY)
=PGWp
λ,q(S,X) +PGWp
λ,q(S,Y).
E Proof of Proposition 3.5: PGW converges to GW as λ→ ∞ . 755
In the main text, we set λ∈R. In this section, we discuss the limit case that when λ→ ∞ . 756
Lemma E.1. Suppose |µ| ≤ |ν|, for each γ∈Γ≤(µ, ν), there exists γ′∈Γ≤(µ, ν)such that γ≤γ′757
and(π1)#γ′=µ. 758
Proof. Letγ∈Γ≤(µ, ν). 759
If|γ|=|µ|, then we have (π1)#γ=µ. 760
If|γ|<|µ|, letµr=µ−(π1)#γ, νr=ν−(π2)#γ. We have that µr, νrare non-negative measures, 761
with|µr|=|µ| − |γ|>0. If we define 762
γ′:=γ+1
|ν| − |γ|µr⊗νr,
we obtain γ≤γ′. In addition, we have: 763
(π1)#γ′= (π1)#γ+µr|νr|
|ν| − |γ|= (π1)#γ+µr=µ,
(π2)#γ′= (π2)#γ+νr|µr|
|ν| − |γ|≤(π2)#γ+νr|νr|
|ν| − |γ|=ν.
Thus, γ′∈Γ≤(µ, ν)and(π1)#γ′=µ. 764
27Lemma E.2. Given general mm-spaces X= (X, d X, µ),Y= (Y, dY, ν), where µ, νare supported 765
on bounded sets (in general, it is assumed that XandYare compact, and that supp(µ) =X, 766
supp(ν) = Y), consider the problem the problem PGWL
λ,q(X,Y)withL(r1, r2)a continuous 767
functions. If λis sufficiently large, for all optimal γ∈Γ≤(µ, ν)we have |γ|= min( |µ|,|ν|). 768
Proof. We prove it for q= 1, for a general q≥1, it can be proved similarly. 769
Without loss of generality, suppose |µ| ≤ |ν|. 770
Since µ, νare supported on bounded sets, there exists A= [0, M]such that dX(x, x′), dY(y, y′)∈A 771
for all x, x′∈supp(µ), y, y′∈supp(ν). 772
Thus, the restriction of LonA2, denoted as LA2, is continuous on A2, and thus it is bounded. So,
consider
m := max
r1,r2∈A(L(r1, r2))≥L(dX(x, x′), dY(y, y′)),∀x, x′∈supp(µ), y, y′∈supp(ν).
Suppose 2λ≥m + 1 , and assume that there exists a optimal γ∈Γ≤(µ, ν)such that |γ|<|µ|. By 773
Lemma E.1, there exists γ′such that γ≤γ′,(π1)#γ′=µ. Thus, we have 774
C(γ′;λ, µ, ν )−C(γ;λ, µ, ν ) =Z
(X×Y)L(dX(x, x′), dY(y, y′))−2λ d((γ′)⊗2−(γ)⊗2)
≤Z
(X×Y)m−2λ d((γ′)⊗2−(γ)⊗2)
=−(|γ′|2− |γ|2) =−(|µ|2− |γ|2)<0,
which is contradiction since γis optimal, and so we have completed the proof. 775
Lemma E.3. Consider probability mm-spaces X= (X, d X, µ),Y= (Y, dY, ν), that is, with 776
|µ|=|ν|= 1. Then, for each λ >0, we have 777
PGWL
λ,q(X,Y)≤GWL
q(X,Y).
Proof. In this setting, we have Γ(µ, ν)⊂Γ≤(µ, ν), and thus 778
PGWL
λ,q(X,Y)
= inf
Γ∈Γ≤(µ,ν)Z
(X×Y)2L(dq
X(x, x′), dq
Y(y, y′))dγ⊗2+λ(|µ|2+|ν|2−2|γ|2)
≤inf
γ∈Γ(µ,ν)Z
(X×Y)2L(dq
X(x, x′), dq
Y(y, y′)) +λ(|µ|2+|ν|2−2|γ|2)dγ⊗2
= inf
γ∈Γ(µ,ν)Z
(X×Y)2L(dq
X(x, x′), dq
Y(y, y′))dγ⊗2
=GWL
q(X,Y).
779
Based on the above properties, we can now prove Proposition 3.5: 780
Proposition E.4 (Generalization of Proposition 3.5) .Consider general probability mm-spaces
X= (X, d X, µ),Y= (Y, dY, ν), that is, with |µ|=|ν|= 1, where X, Y are bounded. Assume that
Lis continuous. Then
lim
λ→∞PGWL
λ,q(X,Y) =GWL
q(X,Y).
Proof. When λis sufficiently large, by Lemma E.2, for each optimal γλ∈Γ≤(µ, ν)of the minimiza- 781
tion problem PGWL
λ,q(X,Y), we have |γλ|= min( |µ|,|ν|) = 1 . That is, γλ∈Γ(µ, ν). Plugging 782
28γλintoC(γλ;λ, µ, ν ), we obtain: 783
PGWL
λ,q(X,Y) =Z
(X×Y)2L(dq
X(x, x′), dq
Y(y, y′))dγ⊗2
λ+λ(12+ 12−2·12)
=Z
(X×Y)2L(dq
X(x, x′), dq
Y(y, y′))dγ⊗2
λ≥GW(X,Y).
By Lemma E.3, we also have PGWL
λ,q(X,Y)≤GWL
q(X,Y)and we complete the proof. 784
F Tensor Product Computation 785
Lemma F.1. Given a tensor M∈Rn×m×n×nandγ, γ′∈Rn×m, the tensor product operator 786
M◦γsatisfies the following: 787
(i) The mapping γ7→M◦γis linear with respect to γ. 788
(ii) If Mis symmetric, in particular, Mi,j,i′,j′=Mi′,j′,i,j,∀i, i′∈[1 :n], j, j′∈[1 :m], then
⟨M◦γ, γ′⟩F=⟨M◦γ′, γ⟩F.
Proof. 789
(i)For the first part, consider γ, γ′∈Rn×mandk∈R. For each i, j∈[1 :n]×[1 :m], we 790
have we have 791
(M◦(γ+γ′))ij=X
i′,j′Mi,j,i′,j′(γ+γ′)i′j′
=X
i′,j′Mi,j,i′,j′γi′j′+X
i′,j′Mi,j,i′,j′γ′
i′j′
= (M◦γ)ij+ (M◦γ)i′j′,
(M◦(kγ))ij=X
i′,j′Mi,j,i′,j′(kγ)ij
=kX
i′,j′Mi,j,i′,j′γij
=k(M◦γ)ij.
Thus, M◦(γ+γ′) =M◦γ+M◦γ′andM◦(kγ) =kM◦γ. Therefore, γ7→M◦γis 792
linear. 793
(ii) For the second part, we have 794
⟨M◦γ, γ′⟩F=X
iji′j′Mi,j,i′,j′,γijγ′
i′j′
=X
i,j,i′,j′Mi′,j′,i,jγi′,j′γi,j (49)
=⟨Mγ′, γ⟩
where (49) follows from the fact that Mis symmetric. 795
796
G Another Algorithm for Computing PGW Distance – Solver 2 797
Our Algorithm 2 for solving the proposed PGW problem is based on a theoretical result that relates 798
GW and PGW. The details of our computational method, as well as the proof of Proposition G.1 stated 799
below, are provided in Appendix G.1. Based on such proposition, we extend the PGW problem to a 800
discrete GW-variant problem (55), leading to a solution for the original PGW problem by truncating 801
the GW-variant solution. 802
29Proposition G.1. LetX= (X, d X, µ)be a mm-space. Consider an auxiliary point ˆ∞and let 803
ˆX= (ˆX, d ˆX,ˆµ), where ˆX=X∪ {ˆ∞},ˆµis constructed by (4), and considering ∞as an auxiliary 804
point to Rsuch that x≤ ∞ for every x∈R, we extend dXintodˆX:ˆX2→R∪ {∞} and define 805
Lλ:R∪ {∞} → Ras follows: 806
dˆX(x, x′) =dX(x, x′)ifx, x′∈X
∞ otherwise, Lλ(r1, r2) :=L(r1, r2)−2λifr1, r2∈R
0 elsewhere.(50)
Consider the following GW-variant2problem: 807
dGWLλ(ˆX,ˆY) = inf
ˆγ∈Γ(ˆµ,ˆν)ˆγ⊗2(Lλ(dq
ˆX, dq
ˆY)) (51)
Then, when considering the bijection γ7→ˆγdefined in (6)we have that γis optimal for PGW 808
problem (10) if and only if ˆγis optimal for the GW-variant problem (51). 809
Proof. The mapping Fdefined by (6) well-defined bijection, as shown in[40, 12]. 810
Given γ∈Γ≤(µ, ν), we have ˆγ=F(γ)∈Γ(ˆµ,ˆν). Let ˆC(ˆγ;µ, ν)denote the transportation cost in 811
the GW-variant problem (51), that is, 812
ˆC(ˆγ;µ, ν) :=Z
(ˆX×ˆY)2Lλ(dq
ˆX(x, x′), dq
ˆY(y, y′))dˆγ(x, y)dˆγ(x′, y′)
Then, we have 813
C(γ;λ, µ, ν )
=Z
(X×Y)2(L(dq
X(x, x′), dq
Y(y, y′))−2λ)dγ⊗2+λ(|µ|+|ν|)|{z}
does not depend on γ
=Z
(X×Y)2(L(dq
X(x, x′), dq
Y(y, y′))−2λ)dˆγ⊗2+λ(|µ|+|ν|) ( since ˆγ|X×Y=γ)
=Z
(X×Y)2(L(dq
ˆX(x, x′), dq
ˆY(y, y′))−2λ)dˆγ⊗2+λ(|µ|+|ν|) (asdˆX|X×X=dX, dˆY|Y×Y=dY)
=Z
(X×Y)2Lλ(dq
ˆX(x, x′), dq
ˆY(y, y′))dˆγ⊗2+λ(|µ|+|ν|) (since ˆL|R×R(·,·) = (L(·,·)−2λ))
=Z
(ˆX×ˆY)2Lλ(dq
ˆX(x, x′), dq
ˆY(y, y′))dˆγ⊗2+λ(|µ|+|ν|)|{z}
does not depend on ˆγ.(since ˆLassigns 0toˆ∞)
Combining this with the fact that F:γ7→ˆγis a bijection, we have that γis optimal for (10) if 814
and only if ˆγis optimal for (51). Under the assumptions of Proposition 3.3, there exists an optimal 815
γ∈Γ≤(µ, ν)for the PGW problem exists, and so we have: 816
arg min
ˆγ∈Γ(ˆµ,ˆν)ˆC(ˆγ;µ, ν) = arg min
γ∈Γ≤(µ,ν)C(γ;λ, µ, ν ). (52)
817
Remark G.2. Both algorithms (Algorithm 1, and 2) are mathematically and computationally 818
equivalent, owing to the equivalence between the POT problem in Solver 1 and the OT problem in 819
Solver 2. 820
G.1 Frank-Wolfe for the PGW Problem – Solver 2 821
Similarly to the discrete PGW problem (15), consider the discrete version of (4): 822
ˆ p = [p; |q|]∈Rn+1,ˆ q = [q; |p|]∈Rm+1, (53)
2dGWLλ(ˆX,ˆY)is not a rigorous GW problem since dˆX=∞is possible, thus it is not a metric. Also, X,Y
are not necessarily probability mm-spaces
30Algorithm 2: Frank-Wolfe Algorithm for partial GW, ver 2
Input: µ=Pn
i=1pX
iδxi, ν=Pm
j=1qY
jδyj, γ(1)
Output: γ(final )
Compute CX, CY,ˆ p,ˆ q,ˆγ(1)
fork= 1,2, . . .do
ˆG(k)←2ˆM◦ˆγ(k)// Compute gradient
ˆγ(k)′←arg min ˆγ∈Γ(ˆ p,ˆ q)⟨ˆG(k),ˆγ⟩F// Solve the OT problem
Compute α(k)∈[0,1]via (56), (18) // Line search
ˆγ(k+1)←(1−α(k))ˆγ(k)′+αˆγ(k)// Update ˆγ
if convergence, break
end for
γ(final )←ˆγ(k)[1 :n,1 :m]
and, in a similar fashion, we define ˆM∈R(n+1)×(m+1)×(n+1)×(m+1)as 823
ˆMi,j,i′,j′=˜Mi,j,i′,j′ifi, i′∈[1 :n], j, j′∈[1 :m],
0 elsewhere .(54)
Then, the GW-variant problem (51) can be written as 824
dGW(ˆX,ˆY) = min
ˆγ∈Γ(ˆ p,ˆ q)LˆM(ˆγ). (55)
Based on Proposition G.1 (which relates PGWL
λ(·,·)withdGW(·,·)), we propose two versions of 825
the Frank-Wolfe algorithm [ 31] that can solve the PGW problem (15). Apart from Algorithm 1 in 826
[45], which solves a different formulation of partial GW, and Algorithm 1 in [ 44], which applies the 827
Sinkhorn algorithm to solve an entropic regularized version of (8), to the best of our knowledge, a 828
precise computational method for the discrete PGW problem (15) has not been studied. 829
Here, we discuss another version of the FW Algorithm for solving the PGW problem (15). The main 830
idea relies on solving first the GW-variant problem (51), and, at the end of the iterations, by using 831
Proposition G.1, convert the solution of the GW-variant problem to a solution for the original partial 832
GW problem (15). 833
First, construct ˆ p,ˆ q,ˆMas described in Proposition G.1. Then, for each iteration k, perform the 834
following three steps. 835
Step 1: Computation of gradient and optimal direction . Solve the OT problem: 836
ˆγ(k)′←arg min
ˆγ∈Γ(ˆ p,ˆ q)⟨LˆM(ˆγ(k)),ˆγ⟩F.
The gradient LˆM(γ(k))can be computed in a similar way as described in Lemma H.2. We refer to 837
Section H for details. 838
Step 2: Line search method . Find optimal step size α(k):
α(k)= arg min
α∈[0,1]{LˆM((1−α)ˆγ(k)+αˆγ(k)′)}.
Similar to Solver 1, let 839


δˆγ(k)= ˆγ(k)′−ˆγ(k),
a=⟨ˆM◦δˆγ(k), δˆγ(k)⟩F,
b= 2⟨ˆM◦δˆγ(k),ˆγ(k)⟩F.(56)
Then the optimal α(k)is given by formula (18). See Appendix J for a detailed discussion. 840
Step 3 . Update ˆγ(k+1)←(1−α(k))ˆγ(k)+α(k)ˆγ(k)′. 841
31H Gradient Computation in Algorithms 1 and 2 842
In this section, we discuss the computation of Gradient ∇L ˜M(γ)in Algorithm 1 and ∇L ˆM(ˆγ)in 843
Algorithm 2. 844
Proposition H.1 (Proposition 1 [41]) .If the cost function can be written as 845
L(r1, r2) =f1(r1) +f2(r2)−h1(r1)h2(r2) (57)
then 846
M◦γ=u(CX, CY, γ)−h1(CX)γh2(CY)⊤, (58)
where u(CX, CY, γ) :=f1(CX)γ11⊤
m+ 1nγ⊤
2f2(CY). 847
Additionally, the following lemma builds the connection between ˜M◦γandM◦γ. 848
Lemma H.2. For any γ∈Rn×m, we have: 849
˜M◦γ=M◦γ−2λ|γ|1n,m. (59)
Proof. For any γ∈Rn×m, we have 850
˜M◦γ= (M1n,n,m,m −2λ)◦γ
= (M−2λ1n,n,m,m )◦γ
=M◦γ−2λ1n,m,n,m ◦γ
=M◦γ−2(⟨1n,m, γ⟩F)1n,m
=M◦γ−2λ|γ|1n,m
where the second equality follows from Lemma F.1. 851
Next, in the setting of Algorithm 2, for any ˆγ∈R(n+1)×(m+1), we have 852
∇L ˆM(ˆγ) = 2 ˆM◦ˆγ (60)
andˆM◦ˆγcan be computed by the following lemma. 853
Lemma H.3. For each ˆγ∈R(n+1)×(m+1), we have ˆM◦ˆγ∈R(n+1)×(m+1)with the following: 854
(ˆM◦ˆγ)ij=(˜M◦ˆγ[1 :n,1 :m])ijifi∈[1 :n], j∈[1 :m]
0 elsewhere. (61)
Proof. Recall the definition of ˆMis given by (54), choose i∈[1 :n], j∈[1 :m], we have 855
(ˆM◦ˆγ)ij=nX
i′=1mX
j′=1ˆMi,j,i′,j′ˆγi′,j′+mX
j′=1ˆMi,j,n+1,jˆγn+1,j′+nX
i′=1ˆMi,j,i′,m+1ˆγi,m+1
+ˆMi,j,n+1,m+1ˆγn+1,m+1
=nX
i′=1mX
j′=1ˆMi,j,i′,j′ˆγi′,j′+ 0 + 0 + 0 =nX
i′=1mX
j′=1˜Mi,j,i′,j′ˆγi′,j′
= (˜M◦(ˆγ[1 :n,1 :m]))ij
Ifi=n+ 1, we have 856
(ˆM◦ˆγ)n+1,j=n+1X
i′=1m+1X
j′=1ˆMn+1,j,i′,j′ˆγi′,j′= 0
Similarly, (ˆM◦ˆγ)i,m+1= 0. Thus, we complete the proof. 857
32I Line Search in Algorithm 1 858
In this section, we discuss the derivation of the line search algorithm. 859
We observe that in the partial GW setting, for each γ∈Γ≤(µ, ν), the marginals of γare not fixed. 860
Thus, we can not directly apply the classical algorithm (e.g. [43]). 861
In iteration k, letγ(k), γ(k)′be the previous and new transportation plans from step 1 of the algorithm. 862
For convenience, we denote them as γ,γ′, respectively. 863
The goal is to solve the following problem: 864
min
α∈[0,1]L(˜M,(1−α)γ+αγ′) (62)
where L(˜M, γ) =⟨˜M◦γ, γ⟩F. By denoting δγ=γ′−γ, we have
L(˜M,(1−α)γ+αγ′) =L(˜M, γ +αδγ).
Then, 865
⟨˜M◦(γ+αδγ),(γ+αδγ)⟩F
=⟨˜M◦γ, γ⟩F+α
⟨˜M◦γ, δγ⟩F+⟨˜M◦δγ, γ⟩F
+α2⟨˜M◦δγ, δγ⟩F
Let 866
a=⟨˜M◦δγ, δγ⟩F,
b=⟨˜M◦γ, δγ⟩F+⟨˜M◦δγ, γ⟩F= 2⟨˜M◦γ, δγ⟩F, (63)
c=⟨˜M◦γ, γ⟩F,
where the second identity in (63) follows from Lemma F.1 and the fact that ˜M=M1n,n,m,m − 867
2λ1n,m,n,m is symmetric. 868
Therefore, the above problem (62) becomes
min
α∈[0,1]aα2+bα+c.
The solution is the following: 869
α∗=

1 ifa≤0, a+b≤0,
0 ifa≤0, a+b >0,
clip(−b
2a,[0,1]) ifa >0,(64)
where
clip(−b
2a,[0,1]) = min
1,max{0,−b
2a}
=

−b
2aif−b
2a∈[0,1],
0 if−b
2a<0,
1 if−b
2a>1.
We can further discuss the difference in computation of aandbin PGW setting and the classical GW 870
setting. If the assumption in Proposition H.1 holds, by (58) and (59), we have 871
a=⟨˜M◦δγ, δγ⟩F
=⟨(M◦δγ−2λ|δγ|In,m), δγ⟩F
=⟨M◦δγ, δγ⟩F−2λ|δγ|2(65)
=
u(CX, CY, δγ)−h1(CX)δγh2(CY)⊤, δγ
F−2λ|δγ|2,
b= 2⟨˜M◦γ, δγ⟩F
= 2⟨M◦γ−2λ|γ|In,m, δγ⟩
= 2(⟨M◦γ, δγ⟩F−2λ|δγ||γ|) (66)
Note that in the classical GW setting [ 43], the term u(CX, CY, δγ) = 0 n×mand|δγ|= 0. Therefore, 872
in such line search algorithm (Algorithm 2 in [ 43]), the terms u(CX, CY, δγ),2λ|δγ|1n×mare not 873
required. In addition, in equation (66),M◦γ,2λ|γ|have been computed in the gradient computation 874
step, thus these two terms can be directly applied in this step. 875
33J Line Search in Algorithm 2 876
Similar to the previous section, in iteration k, letˆγ(k),ˆγ(k)′denote the previous transportation plan 877
and the updated transportation plan. For convenience, we denote them as ˆγ,ˆγ′, respectively. 878
Letδˆγ= ˆγ−ˆγ′. 879
The goal is to find the following optimal α: 880
α= arg min
α∈[0,1]L(ˆM,(1−α)ˆγ, αˆγ′) = arg min
α∈[0,1]L(ˆM, αδ ˆγ+ ˆγ), (67)
where ˆM∈R(n+1)×(m+1)×(n+1)×(m+1), with ˆM[1 :n,1 :m,1 :n,1 :m] = ˜M=M− 881
2λ1n×m×n×m. 882
Similar to the previous section, let 883
a=⟨ˆM◦δˆγ, δˆγ⟩F,
b=⟨ˆM◦δˆγ,ˆγ⟩F+⟨ˆM◦ˆγ, δˆγ⟩F= 2⟨ˆM◦δˆγ,ˆγ⟩F, (68)
c=⟨ˆM◦ˆγ,ˆγ⟩F,
where (68) holds since ˆMis symmetric. Then, the optimal αis given by (64). 884
It remains to discuss the computation. By Lemma F.1, we set γ= ˆγ[1 :n,1 :m], δγ=δˆγ[1 :n,1 : 885
m]. Then, 886
a=⟨(ˆM◦δˆγ)[1 :n,1 :m], δγ⟩F=⟨(˜M◦δγ, δγ⟩F,
b=⟨(ˆM◦δˆγ)[1 :n,1 :m], γ⟩F=⟨(˜M◦δγ, γ⟩F.
Thus, we can apply (65),(66) to compute a, bin this setting by plugging in γ= ˆγ[1 :n,1 :m]and 887
δγ=δˆγ[1 :n,1 :m]. 888
K Convergence 889
As in [ 45] we will use the results from [ 32] on the convergence of the Frank-Wolfe algorithm for 890
non-convex objective functions. 891
Consider the minimization problems 892
min
γ∈Γ≤(p,q)L˜M(γ) and min
ˆγ∈Γ(ˆ p,ˆ q)LˆM(ˆγ) (69)
that corresponds to the discrete partial GW problem, and the discrete GW-variant problem (used in 893
version 2), respectively. The objective functions γ7→ L ˆM(γ) =˜Mγ⊗2(where ˜M=M−2λ1n,m 894
for a fixed matrix M∈Rn×mandλ > 0), and ˆγ7→ L ˆM(ˆγ) = ˆMˆγ⊗2(where ˆMis given by 895
(54)) are non-convex in general (for λ > 0, the matrices ˜Mand ˆMsymmetric but not positive 896
semi-definite), but the constraint sets Γ≤(p,q)andΓ(ˆ p,ˆ q)are convex and compact on Rn×m(see 897
Proposition B.2 [53]) and on R(n+1)×(m+1), respectively. 898
From now on we will concentrate on the first minimization problem in (69) and the convergence 899
analysis for the second one will be analogous. 900
Consider the Frank-Wolfe gap ofL˜Mat the approximation γ(k)of the optimal plan γ: 901
gk= min
γ∈Γ≤(p,q)⟨∇L ˜M(γ(k)), γ(k)−γ⟩F. (70)
It provided a good criterion to measure the distance to a stationary point at iteration k. Indeed, a plan 902
γ(k)is a stationary transportation plan for the corresponding constrained optimization problem in 903
(69) if and only if gk= 0. Moreover, gkis always non-negative ( gk≥0). 904
From Theorem 1 in [ 32], after Kiterations we have the following upper bound for the minimal 905
Frank-Wolf gap: 906
˜gK:= min
1≤k≤Kgk≤max{2L1, DL}√
K, (71)
34where
L1:=L˜M(γ(1))−min
γ∈Γ≤(p,q)L˜M(γ)
is the initial global suboptimal bound for the initialization γ(1)of the algorithm, and DL:= Lip · 907
(diam(Γ≤(p,q)))2, where Lipis the Lipschitz constant of ∇L ˜Manddiam(Γ≤(p,q))is the∥ · ∥F 908
diameter of Γ≤(p,q)inRn×m. 909
The important thing to notice is that the constant max{2L1, DL}does not depend on the iteration 910
stepk. Thus, according to Theorem 1 in [ 32], the rate on ˜gKisO(1/√
K). That is, the algorithm 911
takes at most O(1/ε2)iterations to find an approximate stationary point with a gap smaller than ε. 912
Finally, we adapt Lemma 1 in Appendix B.2 in [ 45] to our case characterizing the convergence 913
guarantee, precisely, determining such a constant max{2L1, DL}in(71). Essentially, we will 914
estimate upper bounds for the Lipschitz constant Lipand for the diameter diam (Γ≤(p,q)). 915
•Let us start by considering the diameter of the couplings of Γ≤(p,q)with respect to the 916
Frobenious norm ∥ · ∥F. By definition, 917
diam(Γ≤(p,q)) := sup
γ,γ′∈Γ≤(p,q)∥γ−γ′∥F.
For any γ∈Γ≤(p,q), since γ1≤pandγ2≤q, we obtain that, in particular, |γ1| ≤ |p|
and|γ2| ≤ |q|. Thus, since |γ1|=|γ|=|γ2|(recall that γ1=π1#γandγ2=π2#γ) we
have
|γ| ≤min{|p|,|q|}=:√s∀γ∈Γ≤(p,q).
Thus, given γ, γ′∈Γ≤(p,q), we obtain 918
∥γ−γ′∥2
F≤2∥γ∥2
F+ 2∥γ′∥2
F= 2X
i,j(γi,j)2+ 2X
i,j(γ′
i,j)2
≤2
X
i,j|γi,j|
2
+ 2
X
i,j|γ′
i,j|
2
= 2|γ|2+ 2|γ′|2≤4s
(essentially, we used that ∥ · ∥Fis the 2-norm for matrices viewed as vectors, that | · |is the 919
1-norm for matrices viewed as vectors, and the fact that ∥ · ∥ 2≤ ∥ · ∥ 1). As a result, 920
diam(Γ≤(p,q))≤2√s, (72)
where sonly depends on pandqthat are fixed weight vectors in Rn
+andRm
+, respectively. 921
•Now, let us analyze the Lipschitz constant of ∇L ˆMwith respect to ∥ · ∥F. For any γ, γ′∈ 922
Γ≤(p,q)we have, 923
∥∇L ˜M(γ)− ∇L ˜M(γ′)∥2
F
=∥˜M◦γ−˜M◦γ′∥2
F
=∥[M−2λ]◦(γ−γ′)∥2
F
=⟨[M−2λ]◦(γ−γ′),[M−2λ]◦(γ−γ′)⟩F
=X
i,j
[(M−2λ)◦(γ−γ′)]i,j2
=X
i,j
X
i′,j′(Mi,j,i′,j′−2λ)(γi′,j′−γ′
i′,j′)
2
≤
max
i,j,i′,j′{Mi,j,i′,j′−2λ}2
n,mX
i,j
n,mX
i′,j′(γi′,j′−γ′
i′,j′)
2

= (max( M)−2λ)2
n,mX
i,j∥γ−γ′∥2
F

≤nm(max( M)−2λ)2∥γ−γ′∥2
F.
35Hence, the Lipschitz constant of the gradient of L˜Mis by 924
Lip≤√nmmax
i,j,i′,j′{Mi,j,i′,j′} −2λ.
In the particular case where L(r1, r2) =|r1−r2|2we have Mi,j,i′,j′=|CX
i,i′−CY
j,j′|2(as in (14)) 925
where CX,CYare given n×nandm×mnon-negative symmetric matrices defined in (11), that 926
depend on the given discrete mm-spaces XandY. Here, we obtain 927
max
i,j,i′,j′{Mi,j,i′,j′}= max
i,j,i′,j′{|CX
i,i′−CY
j,j′|2} ≤
(max
i,i′{CX
i,i′})2+ (max
j,j′{CY
j,j′})2
and so the Lipschitz constant verifies 928
Lip≤√nm((max( CX)2+ max( CY)2)−2λ
Combining all together, we obtain that after Kiterations, the minimal Frank-Wolf gap verifies 929
˜gK= min
1≤k≤Kgk≤max{2L1,4s√nm|max i,j,i′,j′{Mi,j,i′,j′} −2λ|}√
K
≤2max{L1,2s√nm(max( CX)2+ max( CY)2)−2λ}√
K(ifMis as in (14) )
where L1dependents on the initialization of the algorithm. 930
Finally, we mention that there is a dependence in the constant max{2L1, DL}on the number of 931
points ( nandm) of our discrete spaces X={x1, . . . x n}andY={y1, . . . , y m}which was not 932
pointed out in [45]. 933
L Related Work: Mass-Constrained Partial Gromov-Wasserstein 934
Partial Gromov-Wasserstein is first introduced in [ 45]. To distinguish the PGW problem in [ 45] and 935
the PGW problem in this paper, we call the former one the Mass-Constrained Gromov-Wasserstein 936
problem (MPGW): 937
MPGW ρ(X,Y) := inf
γ∈Γρ
≤(µ,ν)γ⊗2(L(dq
X, dq
Y)), (73)
where ρ∈[0,min{|µ|,|ν|}], and 938
Γρ
≤(µ, ν) :={γ∈ M +(X×Y) :γ1≤µ, γ 2≤ν,|γ|=ρ}. (74)
Unlike the relation between Partial OT and OT, it is not rigorous to say that the PGW and the MPGW 939
problems are equivalent, since the objective function 940
γ7→Z
(X×Y)2L(d2
X(x, x′), d2
Y(y, y′))dγ⊗2(75)
is not a convex function even if (r1, r2)7→L(r1, r2)is convex [ 37]: (If the problems were convex, 941
MPGW, as the ‘Lagrangian formulation’ of PGW—adding the constraint of PGW in the functional 942
à laLagrange Multipliers — would be equivalent to PGW. However, since these problems are not 943
convex, we cannot claim that they are equivalent in principle.) 944
We can still investigate their relation by the following lemma, based on which we design the wall-clock 945
time experiment in Section O. 946
Proposition L.1. Suppose γ∈Γ≤(µ, ν)is optimal for PGW λ(X,Y). Let ρ=|γ|, we have γis 947
also optimal in MPGW ρ(X,Y). 948
Proof. Pickγ′∈Γρ
≤(µ, ν)⊂Γ≤(µ, ν), since γis optimal in PGW λ(µ, ν), we have 949
0≤C(γ;λ, µ, ν )−C(γ′;λ, µ, ν )
=Z
(X×Y)2L(d2
X(x, x′), d2
Y(y, y′))d(γ⊗2−γ′⊗2)
Thus, γis optimal in Γρ
≤(µ, ν)forMPGW ρ(X,Y)and we complete the proof. 950
36At first glance, the formulations of the MPGW (73) and the PGW (10) problems could be thought to 951
be equivalent since tuning the hyper-parameter λfor controlling the total mass in the PGW problem 952
is quite similar in spirit to the approach in [ 45] (MPGW) which instead constrains the total mass of γ 953
by the hyper-parameter ρ. However, since classical GW and its variants (e.g. UPGW, PGW, MPGW) 954
are not convex problems, mathematically this equivalence relation is not verified. 955
We first notice that the "Lagrangian form" of the MPGW problem (73) is our PGW formulation 956
(10) by considering 2λbe the "Lagrange variable" of constraint −|γ|2+ρ2≤0. However, as said 957
before, the equivalence is not direct as the cost functional (75) is not convex. In fact, he MPGW 958
problem does not give rise to a metric, while our PGW formulation gives rise to a metric as shown in 959
Proposition 3.4. We will show this through the following example. In fact, we will see that by using 960
the MPGW formulation we cannot distinguish different mm-spaces, while with our PGW we can 961
discriminate different mm-spaces. 962
Example: Consider the following three mm-spaces
X1= (R3,∥ · ∥,1000X
i=1αδxi),X2= (R3,∥ · ∥,800X
i=1αδxi),X3= (R3,∥ · ∥,400X
i=1αδxi),
where α >0is the mass of each point. For numerical stability reasons, we set α= 1/1000 . On the 963
one hand, if we compute MPGW, the mass is fixed to be a value ρ∈[0,0.4], since the total mass in 964
X3is0.4. For our experiment, we set ρ= 0.4, and we observe: 965
MPGW ρ(X1,X2;ρ= 0.4) = MPGW ρ(X2,X3;ρ= 0.4) = MPGW ρ(X1,X3;ρ= 0.4) = 0
On the other hand, if we compute our PGW, considering any λ >0, (in particular, we set λ= 10 ), 966
we obtain 967
PGW λ(X1,X2;λ= 10) = 3 .6
PGW λ(X2,X3;λ= 10) = 4 .8
PGW λ(X1,X3;λ= 10) = 8 .4
In particular, one can verify the triangular inequality. 968
As a conclusion, in this example, MPGW can not describe the dissimilarity of any two datasets taken 969
from{X1,X2,X3}. They are three distinct datasets, but MPGW returns zero for each pair. On the 970
contrary, our PGW can measure dissimilarity. 971
In addition, the discrepancy provided by our PGW formulation is consistent with the follow- 972
ing intuitive observation: One expects the dissimilarity between X1andX3to be larger than 973
the difference X1andX2, and than the difference between X1andX2. This is because we 974
are considering discrete measures, with the same mass at each point concentrated on the sets 975
{x1, . . . , x 400} ⊂ { x1, . . . , x 400, . . . , x 800} ⊂ { x1, . . . , x 400, . . . , x 800, . . . , x 1000}for the datasets 976
X3,X2,X1, respectively. 977
M Partial Gromov-Wasserstein Barycenter 978
We first introduce the classical Gromov-Wasserstein problem [ 41]: Consider finite discrete probability 979
measures µ1, . . . , µK, where µk=Pnk
i=1pk
iδxk
iand each xk
i∈Rdkfor some dk∈N. Let 980
Ck= [∥xk
i−xk
i′∥2]i,i′∈[1:nk]andpk= [pk
1, . . . , pk
nk]⊤. Given p∈Rn
+with|p|= 1for some n∈N 981
andξ1, . . . , ξ K≥0withPK
k=1ξk= 1, the GW barycenter problem is defined by: 982
min
C,γkKX
k=1ξk⟨L(C, Ck)◦γk, γk⟩, (76)
where the minimization is over all matrices C∈Rn×n, γk∈Γ(p,pk),∀k∈[1 :K]. 983
Similarly, we can extend the above definition into PGW setting. In particular, we relax the assumptions 984
|p|= 1and|pk|= 1for each k∈[1 :K]. Given λ1, . . . , λ K>0, the PGW barycenter is the follow 985
problem: 986
min
C,γkX
kξk⟨M(C, Ck)◦γk, γk⟩ −2λk|γk|2(77)
37where each γk∈Γ≤(p,pk). 987
The problem (77) can be solved iterative by two steps: 988
Minimization with respect to C: For each k, we solve the PGW problem
min
γk∈Γ≤(p,pk)⟨M(C, Ck)◦γk, γk⟩ −2λk|γk|2
via solver 1 or 2. 989
Minimization with respect to {γk}k: 990
min
CX
kξk⟨M(C, Ck)◦γk, γk⟩ (78)
Note, we can ignore the −2λk|γk|2terms as γkis fixed in this case. 991
It has closed form solution due to the following lemma and proposition: 992
Lemma M.1. Given matrices A∈Rn,m, B∈Rm,l, C∈Rn,l, let
L=⟨AB, C ⟩,
thendL
dA=CB⊤. 993
Proof. For any i∈[1 :n], j∈[1 :m], we have 994
dL
dAij:=X
i′,j′d
dAijCi′,j′(AB)i′,j′
=X
i′,j′Ci′,j′d(P
kAi′,kBk,j′)
dAij
=X
j′Ci,j′Bk,j′= (CB⊤)ij.
995
Proposition M.2. IfLsatisfies (57), and f′
1/h′
1is invertible, then (78) can be solved by 996
C=f′
1
h′
1−1P
kξkγkh2(Ck)(γk)⊤
P
kξkγk
1(γk
1)⊤
, (79)
where
A
B=Aij
Bij
ij,with convention0
0= 0.
Special case: if |p| ≤ |pk|,∀k, when λis sufficiently large, (79) and [41, Proposition 3] coincide. 997
Proof. From Proposition H.1, the objective in (78) becomes 998
L=X
kξk⟨f1(C)γ1
11⊤
nk+ 1n(γk
2)⊤f2(Ck)−h1(C)γkh2(Ck)⊤, γk⟩
=X
kξk⟨f1(C)γ1
11⊤
nk, γk⟩+X
kξk⟨1n(γk
2)⊤f2(Ck), γk⟩
| {z }
constant−X
kξk⟨h1(C)γkh2(Ck)⊤, γk⟩
38We setdL
dC= 0. From Lemma M.1, we have: 999
0 =dL
dC
=X
kξkf′
1(C)⊙γk1nk(γk
1)⊤−X
kξkh′
1(C)⊙γkh2(Ck)(γk)⊤
=f′
1(C)⊙X
kξkγk1nk(γk
1)⊤−h′
1(C)⊙X
kξkγkh2(Ck)(γk)⊤
=f′
1(C)⊙X
kξkγk
1(γk
1)⊤
|{z }
B−h′
1(C)⊙X
kξkγkh2(Ck)(γk)⊤
| {z }
A. (80)
We claimA
Bis well-defined, i.e., if Bij= 0, then Aij= 0. 1000
For each i, j∈[1 :n], ifBij= 0, we have two cases: 1001
Case 1: ∀k∈[1 :K], we have γk
1[i] = 0 . 1002
Thus, γk[i,:] = 0⊤
nk. SoA[i,:] = ( γkh2(Ck)(γk)⊤)[i,:] = 0⊤
nk. 1003
Case 2: ∀k∈[1 :K], we have γk
1[j] = 0 . 1004
It implies (γk)⊥[:, j] = 0 n, thus A[:, j] = (γkh2(Ck))(γk)⊤[:, j] = 0 nk. Therefore, Aij= 0. 1005
ThusA
Bis well-defined. 1006
In addition, in these two cases, if we change the value Ck
ij,Lwill not change. 1007
From (80), we have: 1008
f′
1
h′
1(C)
ij= P
kξkγkh2(Ck)(γk)⊤
ij P
kξkγk
1(γk
1)⊤
ij
ifBij>0. In addition, if Bij= 0, there is no constraint for Cij. 1009
Combining it with the fact that if Bi,j= 0, then Ci,jhas no effect on L. Thus, 1010
we have the following is a solution:
C=f′
1
h′
1−1P
kξkγkh2(Ck)(γk)⊤
P
kξkγk
1(γk
1)⊤
.
In particular case: |p| ≤ |pk|,∀k, suppose λ >max{c2:c∈S
kCk∪C}, by lemma E.1, we have 1011
for each k,|γk|= min( |p|,|p|k) =|p|, that is γk
1= p. 1012
Thus, 1013
X
kξkγk
1(γ1
1)⊤=X
kξkγk
1(γk
1)⊤=X
kξkpp⊤= pp⊤
Thus, C=
f′
1
h′
1−1P
kξkγkh2(Ck)(γk)⊤
pp⊤
. 1014
Remark M.3. Inl2loss case, i.e. L(r1, r2) =|r1−r2|2,(79) becomes 1015
C=P
kξkγkCk(γk)⊤
P
kξkγk
1(γk
1)⊤. (81)
Since in this case, we can set
f1(x) =x2, f2(y) =y2, h1(x) = 2 x, h2(y) =y.
Thusf′
1
h′
1(x) =2x
2=xand
f′
1
h′
1−1
(x) =x. Therefore, (79) becomes (81). 1016
39Algorithm 3: Partial Gromov-Wasserstein Barycenter
Input: {Ck,pk, λk}K
k=1,p
Output: C
Initialize C.
fori= 1,2, . . .do
compute γk←arg min γ∈Γ≤(p,pk)⟨L(C, Ck)−2λk, γ⟩,∀k∈[1 :K].
Update Cby (79).
if convergence, break
end for
Algorithm 4: Mass-Constrained Partial Gromov-Wasserstein Barycenter
Input: {Ck,pk, λk}K
k=1,p
Output: C
Initialize C.
fori= 1,2, . . .do
compute γk←arg minγ∈Γρk
≤(p,pk)⟨L(C, Ck), γ⟩,∀k∈[1 :K].
Update Cby (79).
if convergence, break
end for
Similarly, we can also extend the above PGW Barycenter into the MPGW setting:
min
C,γkKX
k=1ξk⟨L(C, Ck)◦γk, γk⟩,
where, for each k∈[1 :K],ρk∈[0,min(|p|,|pk|)], and the optimization is over C∈Rnand 1017
γk∈Γρk
≤(p,pk)fork∈[1 :K]. 1018
It can be solved by the following algorithm 4. 1019
Figure 4: We visualize the dataset in point cloud interpolation. The first row is the original images in
Link. The second row is the point clouds obtained by the k-mean method, where k= 1024 .
40Figure 5: We test interpolation tasks in 3 scenarios: source data is clean, target data is selected from
three cases as described in section dataset and data processing . In each scenario, we test η=
5%,10% respectively. In the first column, we present the source and target point cloud visualization
in each task. In columns 2-9, we present GW, PGW barycenter for t= 0/7,1/7, . . . , 7/7.
41M.1 Details of Point Cloud Interpolation Experiment 1020
Dataset and data processing. We apply the dataset in [ 41] with download link. The original data are 1021
images, which we convert into a point cloud using the k-mean algorithm, where k= 1024 (see the 1022
second row of Figure 4). 1023
Suppose D ⊂R2is a region that contains these point clouds. Let R ⊂R2denote another region. In 1024
R, we randomly select and add nηnoise points to these point clouds. In particular, we consider noise 1025
corruption in the following three cases: 1026
Case 1: Ris a rectangle region which is disjoint to D. See the third row in Figure 4. 1027
Case 2: R=R1∪ R 2, where R1,R2are rectangles which are disjoint to D. See the fourth row in 1028
Figure 4. 1029
Case 3: Rcontains D. See the fifth row in Figure 4. 1030
GW Barycenter and PGW Barycenter methods . We select t1, . . . , t Kwith0 =t1< t2< . . . < 1031
tK= 1. For each t∈ {t1, . . . , t K}, we compute the GW Barycenter 1032
arg min
C,γ1,γ2(1−t)⟨L(C, C1)◦γ1, γ1⟩+t⟨L(C, C2)◦γ2, γ2⟩, (82)
where γ1∈Γ(p,p1), γ2∈Γ(p,p2). Apply Smacof-MDS to the minimizer C, the resulting 1033
embedding, denoted as Xt∈Rn×2(where n= 1024 ) is the GW-based interpolation. 1034
Replacing the GW Barycenter with the PGW Barycenter 1035
arg min
C,γ1,γ2(1−t)(⟨L(C, C1)◦γ1, γ1⟩+λ1|γ1|2) +t(⟨L(C, C2)◦γ2, γ2⟩+λ2|γ2|), (83)
where λ1, λ2>0, γ1∈Γ≤(p,p1), γ2∈Γ≤(p,p2). Then we obtain PGW-based interpolation. 1036
Problem setup . We select one point cloud from the clean dataset denoted as X={xi}n
i=1(source 1037
point cloud), n= 1024 . 1038
Next, we select one noise-corrupted point cloud, as described in Case 1, Case 2, and Case 3, 1039
respectively. In these three scenarios, we test η= 0.5%andη= 10% where ηis the noise level. 1040
Therefore, we test 3∗2 = 6 different interpolation tasks for these two methods. The size of the target 1041
point cloud is then m=n+nη. See Figure 5 for details. 1042
Numerical details. In the GW-barycenter method, because of the balanced mass setting, we set
p1=1
n1n,p2=1
m1m,p =1
n1n.
In PGW-barycenter, we set
p1=1
n1n,p2=1
n1m,p =1
n1n.
In addition, we set λ1, λ2such that 2λ1,2λ2≥max(max( C1)2,max( C2)2). We compute GW/PGW 1043
barycenter for t= 0/7,1/7, . . . , 7/7. 1044
In both GW and PGW barycenter algorithms, we set the largest number of iterations to be 100. The 1045
threshold for convergence is set to be 1e-5. 1046
Performance analysis. Each interpolation task is essentially unbalanced: the source point cloud 1047
contains clean data, while the target point cloud contains clean and noise points. We observe that in 1048
the first two scenarios, the interpolation derived from GW is clearly disturbed by the noise data points. 1049
For example, in rows 1,3,5,7, columns t= 1/7,2/7,3/7, we see that the point clouds reconstructed 1050
by MDS have significantly different width-height ratios from those of the source and target point 1051
clouds. 1052
In contrast, PGW is significantly less disturbed, and the interpolation is more natural. The width- 1053
height ratio of the point clouds generated by the PGW barycenter is consistent with that of the 1054
source/target point clouds. 1055
In the third scenario, the noise data is uniformly selected from a large region that contains the domain 1056
of all clean point clouds. In this case, we observe that the GW and PGW barycenters perform similarly. 1057
42However, at t= 1/7,2/7,4/7, GW-barycenters present more noise points than PGW-barycenters in 1058
the same truncated region. 1059
Limitations and future work . The main issue of the above GW/PGW techniques arises from the 1060
MDS method: 1061
Given minimizer C∈Rn×nof GW/PGW barycenter problem (82) (or(83)), MDS studies the 1062
following problem: 1063
min
X∈Rn×dnX
i,i′=1C1/2
i,i′− ∥Xi−Xi′∥2
(84)
LetO(n)denote the set of all n×northonormal matrices. Suppose X∗is a minimizer, then RX∗is 1064
also a minimizer for the above problem for all R∈O(n). 1065
In practice, this means manually setting suitable rotation and flipping matrices for each method at 1066
each step, especially for the GW method. 1067
However, we understand that this issue stems from the inherent properties of the GW/PGW method. 1068
GW can be seen as a tool that describes the similarity between two graphs, which are rotation-invariant 1069
and flipping-invariant. Therefore, the GW/PGW barycenter essentially describes the interpolation 1070
between two graphs rather than two point clouds. 1071
M.2 Details of Point Cloud Matching 1072
Dataset setup . In the Moon dataset (see link), we apply n= 200 and set Gaussian variance to be 0.2. 1073
The outliers are sampled from region [[−2,−1.5]×[−3.5,−3]]. 1074
In the second experiment, the circle data is uniformly sampled from 2D circle
S1={s∈R2:∥s∥2= 1}
and spherical data is uniformly sampled from 3D sphere
S2={s+ [0,0,4]∈R2:∥s∥2= 1},
where the shift [0,0,4]is applied for visualization. 1075
We set sample size n= 200 for both 2D and 3D samples. 1076
In both experiment, the number of outliers is ηn= 0.2n= 40 . 1077
Numerical details . In GW, we normalize the two point clouds as
X= (X, d X,nX
i=11
nδxi),Y= (Y, dY,n+nηX
j=11
n+nηδyj).
In PGW, MPGW, UGW, we define the point clouds as 1078
X= (X, d X,nX
i=11
nδxi),Y= (Y, dY,n+nηX
j=11
nδyj).
In PGW, we choose λsuch that λ≥max(max(( CX)2),max(( CY)2)), in particular, λ= 10.0. 1079
In MPGW, we set ρ= 1.0. 1080
In UGW, we set ρ1=ρ2= 10.0,ϵ= 0.05. 1081
N Details of Shape Retrieval Experiment 1082
Dataset details. We test two datasets in this experiment, which we refer to as Dataset I and Dataset 1083
II. We visualize Dataset I in Figure 6a and Dataset II in Figure 6b. The complete datasets can be 1084
accessed from the supplementary materials. 1085
43bone
 goblet
 star
 horseshoe
rectangle
 trapezoid
 disk
 annulus(a) Dataset I
rectangle
 arrow
 semicircle
house
 double arrow
 circle (b) Dataset II
Figure 6: Visualization of a representative shape from each class of the two datasets.
Numerical details. We represent the shapes in each dataset as mm-spaces Xi= 1086
R2,∥ · ∥ 2, µi=Pni
k=1αiδxi
k
. We use αi=1
nito compute the GW distances for the balanced 1087
mass constraint setting. For the remaining distances, we set α=1
N, where Nis the median number 1088
of points across all shapes in the dataset. For the SVM experiments, we use exp(−σD)as the kernel 1089
for the SVM model, and we set σ= 10 for all distances. Moreover, we normalize the matrix Dto 1090
facilitate a fair comparison of each distance used, since the considered distance may have different 1091
scales. We note that the resulting kernel matrix is not necessarily positive semidefinite. 1092
In computing the pairwise distances, for the PGW method, we set λsuch that λ≤λmax = 1093
max i(|Ci|2). In particular, we compute λmax for each dataset and use λ=1
5λmax for each 1094
experiment. For UGW, we use ε= 10−1andρ1=ρ2= 1for both experiments. Finally, for MPGW, 1095
we set the mass-constrained term to be ρ= min( |µi|,|µj|)when computing the similarity between 1096
shapeXiandXj. 1097
Performance analysis. The pairwise distance matrices are visualized for each dataset in Figure 7, and 1098
the confusion matrices computed with each dataset are given in Figure 8. Finally, the classification 1099
accuracy with the SVM experiments is reported in Table 1a. The results indicate that the PGW 1100
distance is able to consistently obtain high performance across both datasets. 1101
In addition, from Figure 7, we observe that PGW qualitatively admits a more reasonable similarity 1102
measure compared to other methods. For example, in Dataset I, class “bone” and “rectangle” should 1103
have relatively smaller distance than “bone” and “annulus”. Ideally, a reasonable distance should 1104
satisfy the following: 1105
0< d(bone,rectangle )< d(bone,anulus ).
However, we do not observe this relation in GW and UGW3, and for the MPGW method, 1106
MPGW (bone,rectangle )≈0, which is also undesirable. For PGW, however, we do observe 1107
this relation. Additionally, we report the wall-clock time comparison in Table 1b. 1108
3For UGW, this is due to the Sinkhorn regularization term.
44bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusbone
rectangle
goblet
trapezoid
star
disk
horseshoe
annulusGW
bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusMPGW
bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusUGW
bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusPGW (ours)
0.00.20.40.60.81.0(a) Dataset I
rectanglehouse arrow
double arrowsemicirclecirclerectangle
house
arrow
double arrow
semicircle
circleGW
rectanglehouse arrow
double arrowsemicirclecircleMPGW
rectanglehouse arrow
double arrowsemicirclecircleUGW
rectanglehouse arrow
double arrowsemicirclecirclePGW (ours)
0.00.20.40.60.81.0
(b) Dataset II
Figure 7: Pairwise distance matrices computed for each dataset.
bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusbone
rectangle
goblet
trapezoid
star
disk
horseshoe
annulusGW
bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusMPGW
bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusUGW
bone
rectanglegoblet
trapezoidstardisk
horseshoeannulusPGW (ours)
0.00.20.40.60.81.0
(a) Dataset I
rectanglehouse arrow
double arrowsemicirclecirclerectangle
house
arrow
double arrow
semicircle
circleGW
rectanglehouse arrow
double arrowsemicirclecircleMPGW
rectanglehouse arrow
double arrowsemicirclecircleUGW
rectanglehouse arrow
double arrowsemicirclecirclePGW (ours)
0.00.20.40.60.81.0
(b) Dataset II
Figure 8: Confusion matrices computed from nearest neighbor classification experiments.
45O Wall-Clock Time Comparison for Partial GW Solvers 1109
In this section, we present the wall-clock time comparison between our method Algorithms 1, 2, 1110
the Frank-Wolf algorithm proposed in [ 45], and its Sinkhorn version [ 41,45]. Note that these two 1111
baselines solve a mass constraint version of the PGW problem, which we refer to as the “MPGW” 1112
problem. The proposed PGW formulation in this paper can be regarded as a “Lagrangian formulation” 1113
of MPGW4formulation to the PGW problem defined in (10). In this paper, we call these two baselines 1114
as “MPGW algorithm” and “Sinkhorn PGW algorithm”. 1115
Numerical details. The data is generated as follows: let µ=Unif([0,2]2)andν= 1116
Unif([0,2]3), we select i.i.d. samples {xi∼µ}n
i=1,{yj∼ν}m
j=1, where nis selected from 1117
[10,50,100,150, ...,10000] andm=n+ 100 ,p = 1 n/m,q = 1 m/m. For each n, we set 1118
λ= 0.2,1.0,10.0. The mass constraint parameter for the algorithm in [ 45], and Sinkhorn is com- 1119
puted by the mass of the transportation plan obtained by Algorithm 1 or 2. The runtime results are 1120
shown in Figure 9. 1121
Regarding the acceleration technique, for the POT problem in step 1, our algorithms and the MPGW 1122
algorithm apply the linear programming solver provided by Python OT package [ 55], which is written 1123
in C++. The Sinkhorn algorithm from Python OT does not have an acceleration technique. Thus, we 1124
only test its wall-clock time for n≤2000 . The data type is 64-bit float number. 1125
From Figure 9, we can observe the Algorithms 1, 2 and MPGW algorithm have a similar order of 1126
time complexity. However, using the column/row-reduction technique for the POT computation 1127
discussed in previous sections, and the fact the convergence behaviors of Algorithms 1 and 2 are 1128
similar to the MPGW algorithm, we observe that the proposed algorithms 1, 2 admits a slightly faster 1129
speed than MPGW solver. 1130
0 2000 4000 6000 8000 10000
n: size of p103
102
101
100101102wall-clock timev1,=0.2
v1,=10.0
v2,=0.2
v2,=10.0
m,=0.2
m,=10.0
s,=0.2
s,=10.0
Figure 9: We test the wall-clock time of our Algorithm 1 and Algorithm 2, the MPGW solver
(Algorithm 1 in [ 45]) , and the Sinkhorn algorithm [ 41]. We denote these methods as v1, v2, m, s
respectively. The linear programming solver applied in the first three methods is from POT [ 55],
which is written in C++. The maximum number of iterations for all the methods is set to be 1000 .
The maximum iteration for OT/OPT solvers is set to be 300n. The maximum Sinkhorn iteration is
set to be 1000 . The convergence tolerance for the Frank-Wolfe algorithm and the Sinkhorn algorithm
are set to be 1e−5. To achieve their best performance, the number of dummy points is set to be 1 for
MPGW and PGW.
4Due to the non-convexity of GW, we do not have a strong duality in some of the GW representations. Thus,
the Lagrangian form is not a rigorous description.
46P Positive Unlabeled Learning Problem 1131
P.1 Problem setup. 1132
Positive unlabeled (PU) learning [ 56,57,58] is a semi-supervised binary classification problem for 1133
which the training set only contains positive samples. In particular, suppose there exists a fixed 1134
unknown overall distribution over triples (x, o, l ), where xis data, l∈ {0,1}is the label of x, 1135
o∈ {0,1}where o= 1,o= 0 denote that lis observed or not, respectively. In the PU task, the 1136
assumption is that only positive samples’ labels can be observed, i.e., Prob(o= 1|x, l= 0) = 0 . 1137
Consider training labeled data Xpu={(xpu
i, l)}n
i=1⊂ {x:o= 1}and testing data Xun= 1138
{xun
j}m
j=1⊂ {x:o= 0}, where xipX
i∈Rd1, xu
j∈Rd2. In the classical PU learning setting, 1139
d2=d1. However, in [ 44] this assumption is relaxed. The goal is to leverage Xpto design a classifier 1140
ˆl:xu→ {0,1}to predict l(xu)for all xu∈Xu.51141
Following [ 57,45,44], in this experiment, we assume that the “select completely at random” (SCAR) 1142
assumption holds: Prob(o= 1|x, l= 1) = Prob(o= 1|l= 1) . In addition, we use π=Prob(l= 1143
1)∈[0,1]to denote the ratio of positive samples in testing set6. Following the PU learning setting in 1144
[58, 59, 45, 44], we assume πis known. In all the PU learning experiments, we fix π= 0.2. 1145
P.2 Our method. 1146
Similar to [ 45] our method is designed as follows: We set p∈Rn,q∈RmaspX
i=π
n, i∈[1 : 1147
n];qY
j=1
m, j∈[1 :m].LetXp= (Xp,∥ · ∥d1,Pn
i=1pX
iδxi),Xu= (Xu,∥ · ∥d2,Pn
j=1qY
jδyj). 1148
We solve the partial GW problem PGW λ(Xp,Xu)and suppose γis a solution. Let γ2=γ⊤1n. The 1149
classifier ˆlis defined by the indicator function 1150
ˆlγ(xu) = 1{xu:γ2(xu)≥quantile }, (85)
where quantile is the quantile value of γ2according to 1−π. 1151
Regarding the initial guess γ(1), [45] proposed a POT-based approach when XandYare sampled 1152
from the same domain, i.e., d1=d2, which we refer to as “POT initialization.” 1153
When X, Y are sampled from different spaces, that is, d1̸=d2, the above technique (86) is not 1154
well-defined. Inspired by [ 8,44], we propose the following “first lower bound-partial OT” (FLB-POT) 1155
initialization: 1156
γ(1)= arg min
γ∈Γ≤(p,q)Z
X×Y|sX,2(x)−sY,2(y)|2dγ(x, y) +λ(|p−γ1|+|q−γ2|),
where sX,2(x) =R
X|x−x′|2dµ(x)andsY,2is defined similarly. The above formula is analog to 1157
Eq. (7) in [ 44], which is designed for the unbalanced GW setting. To distinguish them, in this paper 1158
we call the Eq. (7) in [44] as “FLB-UOT initilization”. 1159
P.3 Dataset. 1160
The datasets include MNIST, EMNIST, and the following three domains of Caltech Office: Amazon 1161
(A), Webcam (W), and DSLR (D) [ 60]. For each domain, we select the SURF features [ 60] and 1162
DECAF features [ 61]. For MNIST and EMNIST, we train an auto-encoder, respectively, and the 1163
embedding space dimension is 4and6, respectively. See Figure 10 for the TSNE visualization of 1164
these datasets. 1165
P.4 Initial methods. 1166
In this experiment, we employ three distinct initial methods: “POT”, “FLB-UOT”, “FLB-POT”. 1167
5In the classical setting, the goal is to learn a classifier for all x. In this experiment, we follow the setting in
[44].
6In the classical setting, the prior distribution πis the ratio of positive samples of the original dataset. For
convenience, we ignore the difference between this ratio in the original dataset and the test dataset.
47(a) MNIST
 (b) EMNIST
(c) Surf(A)
 (d) Decaf(A)
(e) Surf(D)
 (f) Decaf(D)
(g) Surf(W)
 (h) Decaf(w)
Figure 10: TSNE visulization for datasets MNIST,EMNIST,Caltech Office.
48“POT initialization” is firstly introduced in [ 45].When X1, X2are in the same dimensional space, 1168
i.e.d1=d2. The initial guess, γ(1)is given by the following partial OT variant problem: 1169
γ(1)= arg min
γ∈ΓPU,π(p,q)⟨L(X, Y), γ⟩F, (86)
where L(X, Y)∈Rn×m,(L(X, Y))ij=∥xi−yj∥2and 1170
ΓPU,π(p,q) :={γ∈Rn×m
+ : (γ⊤1n)j∈ {qY
j,0},∀j;γ1m≤p,|γ|=π}. (87)
The above problem can be solved by a Lasso ( L1norm) regularized OT solver. 1171
When d1̸=d2, the above technique can not be applied since the problem (86) (in particular L(X, Y)) 1172
is not well-defined. 1173
The second method “FLB-UOT” is induced in [44]: 1174
γ(1)= arg min
γ∈Γ≤(p,q)Z
X×Y|sX,2(x)−sY,2(y)|2dγ(x, y) +λ(DKL(γ1,p) +DKL(γ2,q)),(88)
where sX,2(x) =R
X|x−x′|2dµ(x)andsY,2is defined similarly. The problem (88) is called 1175
Hellinger Kantorovich, which is a classical unbalanced optimal transport problem. It can be solved 1176
by the Sinkhorn solver [38]. 1177
Analog to the above method, we propose the third method, called “ FLB-POT ” (first lower bound- 1178
partial optimal transport) 1179
γ(1)= arg min
γ∈Γ≤(p,q)Z
X×Y|sX,2(x)−sY,2(y)|2dγ(x, y) +λ(|p−γ1|+|q−γ2|). (89)
The above problem is a partial OT problem and can be solved by classical linear programming [12]. 1180
P.5 Numerical details and performance. 1181
Accuracy Comparison. In Table 2 and 4, we present the accuracy results for the MPGW, UGW, and 1182
the proposed PGW methods when using three different initialization methods: POT, FLB-UOT, and 1183
FLB-POT. 1184
Following [ 45], in the MPGW and PGW methods, we incorporate the prior knowledge πinto the 1185
definition of pandq. Thus it is sufficient to set mass =πfor MPGW and choose a sufficiently 1186
large value for λin the PGW method. This configuration ensures that the mass matched in the target 1187
domain Yis exactly equal to π. However, in the UGW method [ 44], the setting is p=1
n1nand 1188
q=1
m1m. Therefore, in each experiment, we test different parameters (ρ, ρ2, ϵ)and select the ones 1189
that result in transported mass close to π. 1190
Overall, all methods show improved performance in MNIST and EMNIST datasets. One possible 1191
reason for this could be the better separability of the embeddings in MNIST and EMNIST, as 1192
DATASET INITMETHOD INITACCURACY MPGW UGW PGW ( OURS )
M→M POT 100% 100% 95% 100%
M→M FLB-U 75% 96% 95% 96%
M→M FLB-P 75% 99% 95% 99%
M→EM FLB-U 78% 94% 95% 94%
M→EM FLB-P 78% 94% 95% 94%
EM→M FLB-U 75% 97% 96% 97%
EM→M FLB-P 75% 97% 96% 97%
EM→EM POT 100% 100% 95% 100%
EM→EM FLB-U 78% 94% 95% 94%
EM→EM FLB-P 78% 95% 95% 95%
Table 2: Accuracy comparison of the MPGW, UGW, and the proposed PGW method on PU learning.
Here, ‘M’ denotes MNIST, and ‘EM’ denotes EMNIST.
49illustrated in Figure 10. Additionally, since MPGW and PGW incorporate information from rinto 1193
their formulations, they exhibit slightly better accuracy in many experiments. 1194
Numerical details. In this experiment, to prevent unexpected convergence to local minima in the 1195
Frank-Wolf algorithms, we manually set α= 1during the line search step for both MPGW and PGW 1196
methods. 1197
For the convergence criteria, we set the tolerance term for Frank-Wolfe convergence and the main 1198
loop in the UGW algorithm to be 1e−5. Additionally, the tolerance for Sinkhorn convergence in 1199
UGW was set to 1e−6. The maximum number of iterations for the POT solver in PGW and MPGW 1200
was set to 500n. In addition, for MPGW, we set mass = 0.2and for PGW method, based on lemma 1201
E.2, we set λto be constant such that 2λ≥(max(|CX|)2+ max( |CY|)2). 1202
Regarding data types, we used 64-bit floating-point numbers for MPGW and PGW, and 32-bit 1203
floating-point numbers for UGW. 1204
For the MNIST and EMNIST datasets, we set n= 1000 andm= 5000 . In the Surf(A) and Decaf(A) 1205
datasets, each class contained an average of 100 samples. To ensure the SCAR assumption, we set 1206
n= 1/2∗100 = 50 andm= 250 . Similarly, for the Surf(D) and Decaf(D) datasets, we set n= 15 1207
andm= 75 . Finally, for Surf(W) and Decaf(W), we used n= 20 andm= 100 . 1208
Wall-clock time In Table 3, we provide a comparison of wall-clock times for the MNIST and 1209
EMNIST datasets. 1210
SOURCE TARGET INITMETHOD INITTIME MPGW UGW PGW ( OURS )
M(1000) M(5000) POT 0.5 7.2 152.0 7.4
M(1000) M(5000) FLB-U 0.02 30.5 152.6 27.8
M(1000) M(5000) FLB-P 0.5 27.8 144.9 26.9
EM(1000) EM(5000) POT 0.5 7.3 157.3 7.5
EM(1000) EM(5000) FLB-U 0.02 30.0 181.8 29.9
EM(1000) EM(5000) FLB-P 0.5 22.2 155.1 22.3
M(1000) EM(5000) FLB-U 0.02 34.0 157.9 34.4
M(1000) EM(5000) FLB-P 0.5 34.9 155.5 35.0
EM(1000) M(5000) FLB-U 0.02 24.3 139.3 22.2
EM(1000) M(5000) FLB-P 0.5 32.0 162.7 29.9
M(2000) M(10000) POT 1.7 31.1 1384.8 32.1
M(2000) M(10000) FLB-U 0.1 209.0 1525.8 192.5
M(2000) M(10000) FLB-P 1.7 208.0 1418.4 192.1
M(2000) EM(10000) FLB-U 0.1 165.1 1606.1 164.2
M(2000) EM(10000) FLB-P 1.7 224.1 1420.7 223.7
EM(2000) M(10000) FLB-U 0.1 149.1 1426.5 138.1
EM(2000) M(10000) FLB-P 1.7 113.9 1407.6 103.9
EM(2000) EM(10000) POT 1.6 32.4 1445.9 33.4
EM(2000) EM(10000) FLB-U 0.1 233.0 1586.3 233.9
EM(2000) EM(10000) FLB-P 1.8 142.1 1620.6 142.1
Table 3: In this table, we present the wall-clock time for the MPGW, UGW, and the proposed PGW
method, as well as three different initialization methods (POT, FLB-UOT, FLB-POT). In the “Source”
(or “Target”) columm, M (or EM) denotes the MNIST (or EMNIST) dataset, the value 1000 (or5000 )
denotes the sample size of X(orY). The units of all reported wall-clock times is seconds.
50DATASET INITMETHOD INITACCURACY MPGW UGW PGW ( OURS )
SURF (A)→SURF (A) POT 81.2% 74.7% 66.5% 74.7%
SURF (A)→SURF (A) FLB-U 64.9% 65.7% 66.5% 65.7%
SURF (A)→SURF (A) FLB-P 63.3% 66.5% 66.5% 66.5%
DECAF (A)→DECAF (A) POT 95.1% 95.1% 60.8% 95.1%
DECAF (A)→DECAF (A) FLB-U 78.0% 67.4% 83.7% 67.4%
DECAF (A)→DECAF (A) FLB-P 78.0% 74.7% 88.6% 74.7%
SURF (D)→SURF (D) POT 100% 100% 89.3% 100%
SURF (D)→SURF (D) FLB-U 62.7% 73.3% 84.0% 73.3%
SURF (D)→SURF (D) FLB-P 60.0% 60.0% 78.7% 60.0%
DECAF (D)→DECAF (D) POT 100% 100% 100% 100%
DECAF (D)→DECAF (D) FLB-U 76.0% 68.0% 70.7% 68.0%
DECAF (D)→DECAF (D) FLB-P 73.3% 73.3% 86.7% 73.3%
SURF (W)→SURF (W) POT 100.0% 100.0% 81.3% 100.0%
SURF (W)→SURF (W) FLB-U 76.0% 70.7% 81.3% 70.7%
SURF (W)→SURF (W) FLB-P 73.3% 68.0% 78.7% 68.0%
DECAF (W)→DECAF (W) POT 100% 100% 100% 100%
DECAF (W)→DECAF (W) FLB-U 73.3% 68.0% 62.7% 68.0%
DECAF (W)→DECAF (W) FLB-P 70.7% 70.7% 73.3% 70.7%
SURF (A)→DECAF (A) FLB-U 73.9% 83.7% 91.8% 83.7%
SURF (A)→DECAF (A) FLB-P 73.9% 83.7% 87.8% 83.7%
DECAF (A)→SURF (A) FLB-U 67.3% 67.3% 69.0% 67.3%
DECAF (A)→SURF (A) FLB-P 67.3% 68.2% 71.4% 68.2%
SURF (D)→DECAF (D) FLB-U 76.0% 76.0% 65.3% 76.0%
SURF (D)→DECAF (D) FLB-P 76.0% 76.0% 65.3% 76.0%
DECAF (D)→SURF (D) FLB-U 73.3% 62.7% 73.3% 62.7%
DECAF (D)→SURF (D) FLB-P 73.3% 73.3% 73.3% 73.3%
SURF (W)→DECAF (W) FLB-U 70.7% 70.7% 76.0% 70.7%
SURF (W)→DECAF (W) FLB-P 70.7% 70.7% 76.0% 70.7%
DECAF (W)→SURF (W) FLB-U 68.0% 68.0% 65.3% 68.0%
DECAF (W)→SURF (W) FLB-P 68.0% 68.0% 70.7% 68.0%
Table 4: In this table, we present the accuracy comparison of the MPGW, UGW, and the proposed
PGW method. We report the initialization method and its accuracy, followed by the accuracy of each
of the methods MPGW, UGW, and PGW. The prior distribution π=p(l= 1) is set to be 0.2 in all
experiments. To guarantee the SCAR assumption, for Surf(A) and Decaf(A), we set n= 50 , which is
the half of the total number of data in one single class. mis set to be 250. Similarly, we set suitable
n, m for Surf(D), Decaf(D), Surf(W), Decaf(W).
51DATASET INITMETHOD INITTIME MPGW UGW PGW ( OURS )
SURF (A)→SURF (A) POT 1.4 E-3 1.9 E-2 3.8 2.0 E-2
SURF (A)→SURF (A) FLB-U 2.2 E-3 1.8 E-2 3.6 1.9 E-2
SURF (A)→SURF (A) FLB-P 1.7 E-3 1.8 E-2 3.8 1.5 E-2
DECAF (A)→DECAF (A) POT 1.7 E-3 1.9 E-2 7.3 1.9 E-2
DECAF (A)→DECAF (A) FLB-U 9.6 E-3 1.8 E-2 6.8 1.5 E-2
DECAF (A)→DECAF (A) FLB-P 2.0 E-3 1.8 E-2 6.7 1.6 E-2
SURF (D)→SURF (D) POT 2.9 E-4 5.8 E-4 3.1 3.8 E-4
SURF (D)→SURF (D) FLB-U 1.4 E-3 3.0 E-3 5.4 2.2 E-3
SURF (D)→SURF (D) FLB-P 3.1 E-4 2.9 E-3 5.4 2.1 E-3
DECAF (D)→DECAF (D) POT 3.1 E-4 6.0 E-4 3.3 3.6 E-4
DECAF (D)→DECAF (D) FLB-U 1.4 E-3 2.9 E-3 5.8 2.1 E-3
DECAF (D)→DECAF (D) FLB-P 3.4 E-4 2.8 E-3 5.3 2.0 E-3
SURF (W)→SURF (W) POT 3.0 E-4 6.0 E-4 5.2 3.6 E-4
SURF (W)→SURF (W) FLB-U 1.3 E-3 2.9 E-3 5.1 2.1 E-3
SURF (W)→SURF (W) FLB-P 3.3 E-4 2.9 E-3 5.1 2.1 E-3
DECAF (W)→DECAF (W) POT 3.3 E-4 6.2 E-4 3.3 3.4 E-4
DECAF (W)→DECAF (W) FLB-U 1.2 E-3 2.9 E-3 5.8 2.1 E-3
DECAF (W)→DECAF (W) FLB-P 3.3 E-4 2.8 E-3 5.4 2.0 E-3
SURF (A)→DECAF (A) FLB-U 1.1 E-1 2.8 E-2 6.7 2.6 E-2
SURF (A)→DECAF (A) FLB-P 1.9 E-3 2.2 E-2 0.2 2.1 E-2
DECAF (A)→SURF (A) FLB-U 0.1 5 E-2 6.7 4 E-2
DECAF (A)→SURF (A) FLB-P 2 E-3 1.8 6.8 1.5
SURF (D)→DECAF (D) FLB-U 1.8 E-3 5.3 E-3 6.0 2.3 E-3
SURF (D)→DECAF (D) FLB-P 3.5 E-4 3.9 E-4 5.9 3.8 E-4
DECAF (D)→SURF (D) FLB-U 1.8 E-3 0.296 5.6 0.165
DECAF (D)→SURF (D) FLB-P 3.3 E-4 0.218 5.6 0.170
SURF (W)→DECAF (W) FLB-U 1.8 E-3 5.3 E-3 5.0 2.3 E-3
SURF (W)→DECAF (W) FLB-P 3.4 E-4 4.1 E-4 5.0 3.9 E-4
DECAF (W)→SURF (W) FLB-U 1.8 E-3 5.1 E-3 5.8 2.1 E-3
DECAF (W)→SURF (W) FLB-P 3.4 E-4 2.9 E-3 5.6 2.2 E-3
Table 5: In this table, we present the wall-clock time comparison of the MPGW, UGW, and the
proposed PGW method. We report the initialization method and its wall-clock time, followed by the
wall-clock time of each of the methods MPGW, UGW, and PGW. The units of all reported wall-clock
times is seconds. The prior distribution π=p(l= 1) is set to be 0.2 in all experiments. To guarantee
the SCAR assumption, for Surf(A) and Decaf(A), we set n= 50 , which is the half of the total number
of data in one single class. mis set to be 250. Similarly, we set suitable n, m for Surf(D), Decaf(D),
Surf(W), Decaf(W).
52Q Limitations 1211
Compatibility Between Linear Search and Frank-Wolf Solver 1212
In practice, we have found that in some experiments, the linear search algorithm (see Sections I, J) 1213
may cause the Frank Wolfe algorithms (1, 2) to stop running earlier than expected. This may hurt the 1214
performance observed in the PU learning experiments (see Appendix P). As such, we disable line 1215
search in these experiments. 1216
However, in other experiments, for example PGW barycenter (Appendix M.1), we do not find a 1217
significant effect of the linear search algorithm on the results. 1218
MDS in Point Cloud Interpolation Experiment 1219
In the point cloud interpolation experiment (see Appendix M), for the classical GW barycenter method 1220
[41] or our PGW barycenter method, the last step is the same: applying MDS on the barycenter 1221
minimizer Cto construct interpolation point cloud Xt. However, such construction is not unique. 1222
As a consequence, for each constructed Xt, we need to manually set up the rotation and flipping 1223
matrices. 1224
This problem follows from the fact that the GW and PGW formulations cannot distinguish the data 1225
from its rotated (and flipped) version. We refer to Section M.1 for details. 1226
R Compute Resources 1227
All experiments presented in this paper are conducted on a computational machine with an AMD 1228
EPYC 7713 64-Core Processor, 8 ×32GB DIMM DDR4, 3200 MHz, and a NVIDIA RTX A6000 1229
GPU. 1230
S Impact Statement 1231
The work presented in this paper aims to advance the field of machine learning, particularly the 1232
supplementary theoretical developments and explorations of computational optimal transport. There 1233
are many potential societal consequences of our work, none of which we feel must be specifically 1234
highlighted here. 1235
53NeurIPS Paper Checklist 1236
1.Claims 1237
Question: Do the main claims made in the abstract and introduction accurately reflect the 1238
paper’s contributions and scope? 1239
Answer: [Yes] 1240
Justification: In the Abstract, we briefly introduce our main contributions, and in the 1241
Introduction (Section 1) we explain our main contributions in detail. These contributions 1242
are reflected by the theoretical and experimental results provided in the remainder of the 1243
main text and appendices. 1244
Guidelines: 1245
•The answer NA means that the abstract and introduction do not include the claims 1246
made in the paper. 1247
•The abstract and/or introduction should clearly state the claims made, including the 1248
contributions made in the paper and important assumptions and limitations. A No or 1249
NA answer to this question will not be perceived well by the reviewers. 1250
•The claims made should match theoretical and experimental results, and reflect how 1251
much the results can be expected to generalize to other settings. 1252
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 1253
are not attained by the paper. 1254
2.Limitations 1255
Question: Does the paper discuss the limitations of the work performed by the authors? 1256
Answer: [Yes] 1257
Justification: We explain the limitations in Appendix Q. 1258
Guidelines: 1259
•The answer NA means that the paper has no limitation while the answer No means that 1260
the paper has limitations, but those are not discussed in the paper. 1261
• The authors are encouraged to create a separate "Limitations" section in their paper. 1262
•The paper should point out any strong assumptions and how robust the results are to 1263
violations of these assumptions (e.g., independence assumptions, noiseless settings, 1264
model well-specification, asymptotic approximations only holding locally). The authors 1265
should reflect on how these assumptions might be violated in practice and what the 1266
implications would be. 1267
•The authors should reflect on the scope of the claims made, e.g., if the approach was 1268
only tested on a few datasets or with a few runs. In general, empirical results often 1269
depend on implicit assumptions, which should be articulated. 1270
•The authors should reflect on the factors that influence the performance of the approach. 1271
For example, a facial recognition algorithm may perform poorly when image resolution 1272
is low or images are taken in low lighting. Or a speech-to-text system might not be 1273
used reliably to provide closed captions for online lectures because it fails to handle 1274
technical jargon. 1275
•The authors should discuss the computational efficiency of the proposed algorithms 1276
and how they scale with dataset size. 1277
•If applicable, the authors should discuss possible limitations of their approach to 1278
address problems of privacy and fairness. 1279
•While the authors might fear that complete honesty about limitations might be used by 1280
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 1281
limitations that aren’t acknowledged in the paper. The authors should use their best 1282
judgment and recognize that individual actions in favor of transparency play an impor- 1283
tant role in developing norms that preserve the integrity of the community. Reviewers 1284
will be specifically instructed to not penalize honesty concerning limitations. 1285
3.Theory Assumptions and Proofs 1286
Question: For each theoretical result, does the paper provide the full set of assumptions and 1287
a complete (and correct) proof? 1288
54Answer: [Yes] 1289
Justification: In each theorem, we clearly specify the details of conditions and assumptions 1290
along with complete proof. 1291
Guidelines: 1292
• The answer NA means that the paper does not include theoretical results. 1293
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 1294
referenced. 1295
•All assumptions should be clearly stated or referenced in the statement of any theorems. 1296
•The proofs can either appear in the main paper or the supplemental material, but if 1297
they appear in the supplemental material, the authors are encouraged to provide a short 1298
proof sketch to provide intuition. 1299
•Inversely, any informal proof provided in the core of the paper should be complemented 1300
by formal proofs provided in appendix or supplemental material. 1301
• Theorems and Lemmas that the proof relies upon should be properly referenced. 1302
4.Experimental Result Reproducibility 1303
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 1304
perimental results of the paper to the extent that it affects the main claims and/or conclusions 1305
of the paper (regardless of whether the code and data are provided or not)? 1306
Answer: [Yes] 1307
Justifications: In Sections M.1,M.2,N, subsection “numerical details”, we explain the 1308
detailed parameter settings for each method in order to reproduce our results. 1309
Guidelines: 1310
• The answer NA means that the paper does not include experiments. 1311
•If the paper includes experiments, a No answer to this question will not be perceived 1312
well by the reviewers: Making the paper reproducible is important, regardless of 1313
whether the code and data are provided or not. 1314
•If the contribution is a dataset and/or model, the authors should describe the steps taken 1315
to make their results reproducible or verifiable. 1316
•Depending on the contribution, reproducibility can be accomplished in various ways. 1317
For example, if the contribution is a novel architecture, describing the architecture fully 1318
might suffice, or if the contribution is a specific model and empirical evaluation, it may 1319
be necessary to either make it possible for others to replicate the model with the same 1320
dataset, or provide access to the model. In general. releasing code and data is often 1321
one good way to accomplish this, but reproducibility can also be provided via detailed 1322
instructions for how to replicate the results, access to a hosted model (e.g., in the case 1323
of a large language model), releasing of a model checkpoint, or other means that are 1324
appropriate to the research performed. 1325
•While NeurIPS does not require releasing code, the conference does require all submis- 1326
sions to provide some reasonable avenue for reproducibility, which may depend on the 1327
nature of the contribution. For example 1328
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 1329
to reproduce that algorithm. 1330
(b)If the contribution is primarily a new model architecture, the paper should describe 1331
the architecture clearly and fully. 1332
(c)If the contribution is a new model (e.g., a large language model), then there should 1333
either be a way to access this model for reproducing the results or a way to reproduce 1334
the model (e.g., with an open-source dataset or instructions for how to construct 1335
the dataset). 1336
(d)We recognize that reproducibility may be tricky in some cases, in which case 1337
authors are welcome to describe the particular way they provide for reproducibility. 1338
In the case of closed-source models, it may be that access to the model is limited in 1339
some way (e.g., to registered users), but it should be possible for other researchers 1340
to have some path to reproducing or verifying the results. 1341
5.Open access to data and code 1342
55Question: Does the paper provide open access to the data and code, with sufficient instruc- 1343
tions to faithfully reproduce the main experimental results, as described in supplemental 1344
material? 1345
Answer: [Yes] 1346
Justification: We provide the data and code as supplementary material. 1347
Guidelines: 1348
• The answer NA means that paper does not include experiments requiring code. 1349
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 1350
public/guides/CodeSubmissionPolicy ) for more details. 1351
•While we encourage the release of code and data, we understand that this might not be 1352
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 1353
including code, unless this is central to the contribution (e.g., for a new open-source 1354
benchmark). 1355
•The instructions should contain the exact command and environment needed to run to 1356
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 1357
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 1358
•The authors should provide instructions on data access and preparation, including how 1359
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 1360
•The authors should provide scripts to reproduce all experimental results for the new 1361
proposed method and baselines. If only a subset of experiments are reproducible, they 1362
should state which ones are omitted from the script and why. 1363
•At submission time, to preserve anonymity, the authors should release anonymized 1364
versions (if applicable). 1365
•Providing as much information as possible in supplemental material (appended to the 1366
paper) is recommended, but including URLs to data and code is permitted. 1367
6.Experimental Setting/Details 1368
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 1369
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 1370
results? 1371
Answer: [Yes] 1372
Justification: We refer to the subsections “experiment setup” in Sections 5, M.1, M.2, N, P. 1373
Guidelines: 1374
• The answer NA means that the paper does not include experiments. 1375
•The experimental setting should be presented in the core of the paper to a level of detail 1376
that is necessary to appreciate the results and make sense of them. 1377
•The full details can be provided either with the code, in appendix, or as supplemental 1378
material. 1379
7.Experiment Statistical Significance 1380
Question: Does the paper report error bars suitably and correctly defined or other appropriate 1381
information about the statistical significance of the experiments? 1382
Answer: [Yes] 1383
Justification: We calculate accuracy in experiments N, P, which are the only statistics 1384
reported in this paper. These values are classification accuracies for each tested dataset. 1385
Thus, error bar/variance are not involved in this work. 1386
Guidelines: 1387
• The answer NA means that the paper does not include experiments. 1388
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 1389
dence intervals, or statistical significance tests, at least for the experiments that support 1390
the main claims of the paper. 1391
•The factors of variability that the error bars are capturing should be clearly stated (for 1392
example, train/test split, initialization, random drawing of some parameter, or overall 1393
run with given experimental conditions). 1394
56•The method for calculating the error bars should be explained (closed form formula, 1395
call to a library function, bootstrap, etc.) 1396
• The assumptions made should be given (e.g., Normally distributed errors). 1397
•It should be clear whether the error bar is the standard deviation or the standard error 1398
of the mean. 1399
•It is OK to report 1-sigma error bars, but one should state it. The authors should 1400
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 1401
of Normality of errors is not verified. 1402
•For asymmetric distributions, the authors should be careful not to show in tables or 1403
figures symmetric error bars that would yield results that are out of range (e.g. negative 1404
error rates). 1405
•If error bars are reported in tables or plots, The authors should explain in the text how 1406
they were calculated and reference the corresponding figures or tables in the text. 1407
8.Experiments Compute Resources 1408
Question: For each experiment, does the paper provide sufficient information on the com- 1409
puter resources (type of compute workers, memory, time of execution) needed to reproduce 1410
the experiments? 1411
Answer: [Yes] 1412
Justification: See Appendix R. 1413
Guidelines: 1414
• The answer NA means that the paper does not include experiments. 1415
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 1416
or cloud provider, including relevant memory and storage. 1417
•The paper should provide the amount of compute required for each of the individual 1418
experimental runs as well as estimate the total compute. 1419
•The paper should disclose whether the full research project required more compute 1420
than the experiments reported in the paper (e.g., preliminary or failed experiments that 1421
didn’t make it into the paper). 1422
9.Code Of Ethics 1423
Question: Does the research conducted in the paper conform, in every respect, with the 1424
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 1425
Answer: [Yes] 1426
Justification: The authors have reviewed the NeurIPS Code of Ethics and all the imported 1427
code has been properly cited. 1428
Guidelines: 1429
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 1430
•If the authors answer No, they should explain the special circumstances that require a 1431
deviation from the Code of Ethics. 1432
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 1433
eration due to laws or regulations in their jurisdiction). 1434
10.Broader Impacts 1435
Question: Does the paper discuss both potential positive societal impacts and negative 1436
societal impacts of the work performed? 1437
Answer: [Yes] 1438
Justification: See Appendix S. 1439
Guidelines: 1440
• The answer NA means that there is no societal impact of the work performed. 1441
•If the authors answer NA or No, they should explain why their work has no societal 1442
impact or why the paper does not address societal impact. 1443
57•Examples of negative societal impacts include potential malicious or unintended uses 1444
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 1445
(e.g., deployment of technologies that could make decisions that unfairly impact specific 1446
groups), privacy considerations, and security considerations. 1447
•The conference expects that many papers will be foundational research and not tied 1448
to particular applications, let alone deployments. However, if there is a direct path to 1449
any negative applications, the authors should point it out. For example, it is legitimate 1450
to point out that an improvement in the quality of generative models could be used to 1451
generate deepfakes for disinformation. On the other hand, it is not needed to point out 1452
that a generic algorithm for optimizing neural networks could enable people to train 1453
models that generate Deepfakes faster. 1454
•The authors should consider possible harms that could arise when the technology is 1455
being used as intended and functioning correctly, harms that could arise when the 1456
technology is being used as intended but gives incorrect results, and harms following 1457
from (intentional or unintentional) misuse of the technology. 1458
•If there are negative societal impacts, the authors could also discuss possible mitigation 1459
strategies (e.g., gated release of models, providing defenses in addition to attacks, 1460
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 1461
feedback over time, improving the efficiency and accessibility of ML). 1462
11.Safeguards 1463
Question: Does the paper describe safeguards that have been put in place for responsible 1464
release of data or models that have a high risk for misuse (e.g., pretrained language models, 1465
image generators, or scraped datasets)? 1466
Answer: [NA] 1467
Justification: This paper does not pose such risks. 1468
Guidelines: 1469
• The answer NA means that the paper poses no such risks. 1470
•Released models that have a high risk for misuse or dual-use should be released with 1471
necessary safeguards to allow for controlled use of the model, for example by requiring 1472
that users adhere to usage guidelines or restrictions to access the model or implementing 1473
safety filters. 1474
•Datasets that have been scraped from the Internet could pose safety risks. The authors 1475
should describe how they avoided releasing unsafe images. 1476
•We recognize that providing effective safeguards is challenging, and many papers do 1477
not require this, but we encourage authors to take this into account and make a best 1478
faith effort. 1479
12.Licenses for existing assets 1480
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 1481
the paper, properly credited and are the license and terms of use explicitly mentioned and 1482
properly respected? 1483
Answer: [Yes] 1484
Justification: In Sections M.1, M.2, N, P, subsection “dataset”, we provide the citations of 1485
all datasets from other literature. We also cite all code adapted from other sources. 1486
Guidelines: 1487
• The answer NA means that the paper does not use existing assets. 1488
• The authors should cite the original paper that produced the code package or dataset. 1489
•The authors should state which version of the asset is used and, if possible, include a 1490
URL. 1491
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 1492
•For scraped data from a particular source (e.g., website), the copyright and terms of 1493
service of that source should be provided. 1494
58•If assets are released, the license, copyright information, and terms of use in the 1495
package should be provided. For popular datasets, paperswithcode.com/datasets 1496
has curated licenses for some datasets. Their licensing guide can help determine the 1497
license of a dataset. 1498
•For existing datasets that are re-packaged, both the original license and the license of 1499
the derived asset (if it has changed) should be provided. 1500
•If this information is not available online, the authors are encouraged to reach out to 1501
the asset’s creators. 1502
13.New Assets 1503
Question: Are new assets introduced in the paper well documented and is the documentation 1504
provided alongside the assets? 1505
Answer: [NA] 1506
Justification: This paper does not release new assets. 1507
Guidelines: 1508
• The answer NA means that the paper does not release new assets. 1509
•Researchers should communicate the details of the dataset/code/model as part of their 1510
submissions via structured templates. This includes details about training, license, 1511
limitations, etc. 1512
•The paper should discuss whether and how consent was obtained from people whose 1513
asset is used. 1514
•At submission time, remember to anonymize your assets (if applicable). You can either 1515
create an anonymized URL or include an anonymized zip file. 1516
14.Crowdsourcing and Research with Human Subjects 1517
Question: For crowdsourcing experiments and research with human subjects, does the paper 1518
include the full text of instructions given to participants and screenshots, if applicable, as 1519
well as details about compensation (if any)? 1520
Answer: [NA] 1521
Justification: This paper does not involve crowdsourcing nor research with human subjects. 1522
Guidelines: 1523
•The answer NA means that the paper does not involve crowdsourcing nor research with 1524
human subjects. 1525
•Including this information in the supplemental material is fine, but if the main contribu- 1526
tion of the paper involves human subjects, then as much detail as possible should be 1527
included in the main paper. 1528
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 1529
or other labor should be paid at least the minimum wage in the country of the data 1530
collector. 1531
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 1532
Subjects 1533
Question: Does the paper describe potential risks incurred by study participants, whether 1534
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 1535
approvals (or an equivalent approval/review based on the requirements of your country or 1536
institution) were obtained? 1537
Answer: [NA] 1538
Justification: This paper does not involve crowdsourcing nor research with human subjects. 1539
Guidelines: 1540
•The answer NA means that the paper does not involve crowdsourcing nor research with 1541
human subjects. 1542
•Depending on the country in which research is conducted, IRB approval (or equivalent) 1543
may be required for any human subjects research. If you obtained IRB approval, you 1544
should clearly state this in the paper. 1545
59•We recognize that the procedures for this may vary significantly between institutions 1546
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1547
guidelines for their institution. 1548
•For initial submissions, do not include any information that would break anonymity (if 1549
applicable), such as the institution conducting the review. 1550
60