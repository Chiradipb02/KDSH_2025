Rethinking Open-set Noise in Learning with Noisy
Labels
Anonymous Author(s)
Affiliation
Address
email
Abstract
To reduce reliance on labelled data, learning with noisy labels (LNL) has gained 1
increasing attention. However, prevailing works typically assume that such datasets 2
are primarily affected by closed-set noise (where the true/clean labels of noisy 3
samples come from another known category), and ignore therefore the ubiqui- 4
tous presence of open-set noise (where the true/clean labels of noisy samples 5
may not belong to any known category). In this paper, we formally refine the 6
LNL problem setting considering the presence of open-set noise. We theoret- 7
ically analyze and compare the effects of open-set noise and closed-set noise, 8
as well as the effects between different open-set noise modes. We also analyze 9
common open-set noise detection mechanisms based on prediction entropy values. 10
To empirically validate the theoretical results, we construct two open-set noisy 11
datasets - CIFAR100-O/ImageNet-O and introduce a novel open-set test set for 12
the widely used WebVision benchmark. Our work suggests that open-set noise 13
exhibits qualitatively and quantitatively distinct characteristics, and how to fairly 14
and comprehensively evaluate models in this condition requires more exploration. 15
1 Introduction 16
In recent years, the tremendous success of machine learning often relies on the assumption that data 17
labels are accurate and free from noise. However, in real-world scenarios, label noise caused by 18
factors such as annotation errors and label ambiguity is ubiquitous, posing a pervasive challenge to 19
the performance and generalization of models. To address this challenge, various methods have been 20
proposed to learn with noisy labels, including noise transition matrix [ 7,23], label correction [ 17,3], 21
robust loss functions [6, 29, 19], and recently dominant sample selection-based approaches [11, 2]. 22
Most current efforts, however, primarily focus on closed-set noise, where the true labels of noisy 23
samples belong to another known class. This includes common noise models like symmetric noise 24
(assuming that the labels of samples are randomly flipped with a certain probability to any other 25
known classes) or asymmetric noise model (assuming that the probability of label confusion is 26
influenced by the classes, such as ’cat’ being more likely to be confused with ’dog’ than with 27
’airplane’). Recent advancements have also explored instance-dependent noise models [ 4,26], where 28
label confusion depends directly on individual instances. 29
Unfortunately, unlike the in-depth exploration of closed-set noise, there is noticeably limited research 30
on open-set noise, where the true labels of noisy samples may not belong to any known category. 31
This gap becomes particularly crucial when considering one of the primary motivations for learning 32
with noisy labels: learning with datasets obtained through web crawling. Examining one of the most 33
commonly used benchmarks - the WebVision dataset [ 12], we validate the prevalence of open-set 34
noise (fig. 1). In fact, the ‘open-world’ assumption involving open-set samples has received more 35
attention in other weakly supervised learning problems, such as open-set recognition and outlier 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Tench?Figure 1: Example images of class “Tench" from WebVision dataset. Clean samples are marked in
extcolorgreenGreen, closed-set noise is marked in Blue and open-set noise is marked in Red. See
appendix F for more details.
detection, but lacks enough exploration in the context of LNL. To this end, we focus on a thorough 37
theoretical analysis of open-set noise in this paper. Specifically: 38
•Considering the presence of open-set noise, we introduce the concept of a complete noise 39
transition matrix and reformulate the LNL problem and label noise definition in this context. 40
•To enable offline analysis, we consider two pragmatic cases: fitted case , that the model 41
perfectly fits the noisy distribution, and memorized case , that the model completely memorises 42
the noisy labels. 43
•We analyze and compare the open-set noise vs.closed-set noise on closed-set classification 44
accuracy and suggest that open-set noise has a less negative impact in both cases. We also 45
analyze and compare the ‘hard’ open-set noise vs.‘easy’ open-set noise, but find that these 46
two different noise modes show opposite trends in two different cases. 47
•Since closed-set classification evaluation may be insufficient to fully reflect model perfor- 48
mance, we consider introducing an additional open-set detection task and conduct preliminary 49
experiments. 50
•We derive and analyze the open-set noise detection mechanism based on the entropy values 51
of model predictions and suggest that it may be effective only for ‘easy’ open-set noise. We 52
also consider two representative LNL methods and combine them with such open-set noise 53
detection mechanism for further experiments. 54
•For controlled experiments, we construct two novel synthetic open-set noise datasets: 55
CIFAR100-O and ImageNet-O. Additionally, we introduce a new open-set test set to the 56
WebVision dataset for the open-set detection task. 57
2 Related works 58
Methods for learning with noisy labels can be roughly categorized into two main directions. The first 59
direction typically focuses on estimating noise transition matrix [ 4,26,23,7] or designing robust 60
loss functions [ 29,19,6], aiming to achieve theoretically risk-consistent or probabilistic-consistent 61
models. However, most of these works often assume an ideal scenario where the model can learn to 62
fit the sampled distribution well, overlooking the over-fitting issues arising from excessive model 63
capacity and insufficient data in practical situations. In this paper, we introduce the concept of 64
complete noise transition matrix considering the presence of open-set noise and conduct theoretical 65
analyses and experimental validations for both ideal case and over-fitting case, namely fitted case 66
and memorized case .The second type is often based on sample selection strategies, involving also 67
different regularization terms and off-the-shelf techniques such as semi-supervised learning and 68
model co-training, to achieve the state-of-the-art performance. Most sample selection methods are 69
based on the model’s current predictions, such as the popular ‘small loss’ mechanism [ 2,11,8,28, 70
10, 17, 13, 27, 24, 30], or model’s feature space [21, 22, 15, 5]. 71
Especially, the investigation on open-set noise is relatively scarce. Wang et al. [18] utilize Local 72
Outlier Factor algorithm to identify open-set noise in feature space, Wu et al. [22] propose to identify 73
open-set noise with subgraph connectivity, while both Sachdeva et al. [16] and Albert et al. [1]try to 74
identify open-set noise based on entropy-related dynamics. Instead, Feng et al. [5]do not identify 75
open-set noise explicitly while avoid relabelling and including open-set noise in the training. More 76
closely related to our work, Xia et al. [25] also investigates noise transition matrices involving open- 77
set noise but considering all open-set noise belonging to a single meta-class. In this paper, we consider 78
2that open-set noise may originate from different classes, and based on this premise, we analyze two 79
distinct open-set noise modes. Wei et al. [20] propose leveraging open-set noise to mitigate the 80
impact of closed-set noise, as it helps alleviating the model’s over-fitting tendency. Instead, we focus 81
on a thorough theoretical analysis of the effects with different noise modes, including open-set noise 82
versus closed-set noise, and different open-set noise versus each other. 83
3 Methodology 84
In section 3.1, we briefly introduce the traditional problem formulation of LNL. In section 3.2, we 85
reformulate the LNL problem considering open-set noise. In section 3.3, we formalize how label 86
noise influences model generalization, particularly, on the proposed error rate inflation metric. In 87
section 3.4, we analyze and compare the impact of open-set vs.closed-set noise, as well as ‘easy’ 88
open-set noise vs.‘hard’ open-set noise. In section 3.5, we scrutinize the open-set noise detection 89
mechanism based on model prediction entropy values. 90
3.1 Traditional formulation of LNL 91
Supervised classification learning typically assumes that we sample a certain number of independently 92
and identically distributed training samples {xk, yk}K
k=1from a joint distribution P(x,y;y∈ Yin), 93
i.e., the so-called train set. By default, here all the possible values for ykin the discrete label space 94
Yin:{1,2, ..., A}(referred here as inlier classes ), are known in advance. With a certain loss function, 95
given the train set {xk, yk}K
k=1we aim to train a model f:x→ywhose predictions can achieve the 96
minimum error rate under the whole clean distribution P(x,y;y∈ Yin). 97
Under LNL problem setting, we believe that the joint distribution P(x,y;y∈ Yin)has been perturbed 98
toPn(x,y;y∈ Yin); especially, the conditional distribution Pn(y|x;y∈ Yin)changes — normally 99
we assume the sampling prior is free of the label noise ( P(x;y∈ Yin) =Pn(x;y∈ Yin)), leading 100
to the presence of noisy labels yn
kin the noisy train set {xk, yn
k}K
k=1that do not conform to the clean 101
conditional distribution P(y|x;y∈ Yin). 102
3.2 Revisiting LNL considering open-set noise 103
We here formally revisit the problem formulation of learning with noisy labels considering the 104
existence of open-set noise. Instead of assuming all the possible classes are known ( y∈ Yin), we 105
consider samples from some unknown outlier classes may also exist in the train set. Let us denote 106
these classes as outlier classes Yout:{A+ 1, A+ 2, ..., A +B}withBas the number of possible 107
outlier classes. Then, we expand the support of joint distribution to contain both inlier and outlier 108
classes, denoted as P(x,y;y∈ Yin∪ Yout)andPn(x,y;y∈ Yin∪ Yout)for the clean and noisy 109
ones, respectively. For brevity, we denote as Yall≜Yin∪ Yout. Similarly as above, we still assume 110
the noisy labelling will not affect the sampling prior ( P(x;y∈ Yall) =Pn(x;y∈ Yall)). For 111
subsequent analysis, we first define below complete noise transition matrix: 112
Definition 3.1 (Complete noise transition matrix) .For a specific sample x, we define as T(sample 113
index omitted here for simplicity) the complete noise transition matrix1: 114
T="
Tin
A×A0A×B
Tout
B×A0B×B#
.
Tincorresponds to the confusion process between inlier classes Yin:{1,2, ..., A}, and Tout115
corresponds to the confusion process from outlier classes Yout:{A+ 1, A+ 2, ..., A +B}to inlier 116
classes Yin:{1,2, ..., A}. 117
For brevity, we denote as Tij≜P(yn=j|y=i,x=x;yn,y∈ Yall). We have further 118PA+B
j=1Tij= 1fori∈ {1, ..., A +B}- noise transition from each clean class sums to 1 over all 119
possible noisy classes. With such a complete noise transition matrix T, we can connect the clean 120
1The right part of the transition matrix is all-zero as we assume in the noisy labelling process all outlier
classes are confused into inlier classes, i.e., all of its samples been labelled as one of the inlier classes.
3conditional distribution P(y|x=x;y∈ Yall)with the noisy conditional distribution Pn(y|x= 121
x;y∈ Yall)as below: 122
Pn(y=j|x=x;y∈ Yall) =A+BX
l=1P(y=l|x=x;y∈ Yall)·Tlj (1)
Label noise Recent works usually discriminate label noise into closed-set noise and open-set noise. 123
Before continuing with the further discussion, we feel it is necessary to elucidate these two concepts 124
here clearly to avoid any ambiguities, as we will try to comparably discriminate and analyze them 125
later. Specifically, most of recent works define open-set noise as ‘a sample with its true label from 126
unknown classes but mislabelled with a known label’. Formally, we have: 127
Definition 3.2 (Label noise) .For sample xwith clean label yand noisy label yn: 128
• When y=yn,(x, y, yn)is a clean sample; 129
• When y̸=ynandy∈ Yin,(x, y, yn)is a closed-set noise; 130
• When y̸=ynandy∈ Yout,(x, y, yn)is an open-set noise. 131
Specifically, we have y∼P(y=y|x=x;y∈ Yall)while yn∼Pn(y=yn|x=x;y∈ Yall). 132
However, we can only identify label noise type with (x, y, yn)—y, ynyet to be sampled even with 133
known conditional probability. To enable sample-wise analysis on the impact of different label noise, 134
we further introduce below (Ox, Cx) label noise : 135
Definition 3.3 ((Ox, Cx) label noise) .For sample xwith clean conditional probability P(y|x= 136
x;y∈ Yall)and complete noise transition matrix T: 137
Ox=A+BX
i=A+1AX
j=1TijP(y=i|x=x;y∈ Yall) =A+BX
i=A+1P(y=i|x=x;y∈ Yall),
Cx=AX
i=1AX
j=1,j̸=iTijP(y=i|x=x;y∈ Yall).(2)
Here, Oxis the expected open-set noise ratio, Cxis the expected closed-set noise ratio. We then 138
define sample xas an (Ox, Cx) label noise. Intuitively speaking, sample xis expected to be an 139
open-set noise with probability as Oxand to be a closed-set noise with probability Cx. 140
With Definition 3.3, we formalize the concept of noise ratio for the whole distribution, as the 141
accumulated ( Ox, Cx) label noise at all sample points x∈ X: 142
N=Z
x(Ox+Cx)·P(x=x;y∈ Yall)dx (3)
3.3 Analyzing classification error rate inflation in LNL 143
In this section, we try to analyze the impact of different label noise. Please note, while the reformulated 144
LNL setting encompasses outlier classes Yout, in both the training and evaluation stage, they are 145
unknown (agnostic); the learned model fis still tailored for the classification of inlier classes Yin. 146
That is to say, the default classification evaluation protocol is still concerned with the classification 147
error rate over the inlier conditional probability, denoted as Pf(y|x=x;y∈ Yin). 148
Error rate inflation With Pf(y|x=x;y∈ Yin), in the evaluation phase, for specific sample x 149
we have its prediction as: yf= arg maxkPf(y=k|x=x;y∈ Yin)∈ Yin, and the corresponding 150
expected classification error rate as: 151
Ex=X
y̸=yfP(x,y;y∈ Yin) = (1 −P(y=yf)|x;y∈ Yin))·P(x;y∈ Yin).(4)
Specifically, we have the Bayes error rate corresponds to the Bayes optimal model f∗: 152
E∗
x= (1−max kP(y=k|x=x;y∈ Yin))·P(x=x;y∈ Yin). (5)
4To measure the negative impacts of noisy labels, we care about how much extra errors have been 153
introduced, measured by the error rate inflation of learned model fcompared to the Bayes optimal 154
model f∗: 155
Definition 3.4 (Error rate inflation) .WithE∗
xas the Bayes error rate, we define the error rate inflation 156
for sample xas:∆Ex=Ex−E∗
x. 157
Two pragmatic cases However, Pf(y|x=x;y∈ Yin), as the prediction of the final learned 158
model f, is affected by many factors (model capacity/dataset size/training hyperparameters such 159
as training epochs, etc.), which is non-trivial to determine its specific value for an offline analysis2. 160
Thus, we consider two specific pragmatic cases: 161
•Fitted case : the model perfectly fits the noisy distribution: Pf(y|x=x;y∈ Yin) =Pn(y|x= 162
x;y∈ Yin); 163
•Memorized case : the model completely memorises the noisy labels: Pf(y|x=x;y∈ Yin) = 164
Pyn(y|x=x;y∈ Yin); Here Pyndenotes the one-hot encoding of the noisy label yn. 165
Nonetheless, these two cases are very realistic and important; Empirically, it is highly possible that 166
thememorized case can correspond to scenarios such as scratch training based on a single-label 167
dataset with a normal deep neural network - as normally such model has enough capacity to memorize 168
all the labels, while the fitted case can correspond to scenarios such as fine-tuning a linear classifier 169
with a pre-trained model - as the pre-trained model already captures good sample representations and 170
the capacity of a linear classifier is limited. 171
3.4 Error rate inflation analysis w.r.t different label noise 172
In this section, we focus on analyzing the error rate inflation of different label noise. Let us recall the 173
clean conditional distribution as P(y|x;y∈ Yall). For ease of analysis, we contemplate a simple 174
scenario, wherein the entire clean conditional distribution remains unchanged, except only one of the 175
sample points, say x, is afflicted by label noise: 176
Pn(y|x̸=x;y∈ Yall) =P(y|x̸=x;y∈ Yall), Pn(y|x=x;y∈ Yall)̸=P(y|x=x;y∈ Yall).(6)
In this condition, we can simplify analyzing the impact of label noise on the whole distribution to 177
analyzing the error rate inflation of a single sample x. Specifically, we consider two specific sample 178
points x1andx2, corresponding to two in our later comparative analysis. Let us denote its clean 179
conditional probability as P(y|x=x1;y∈ Yall) = [ p1
1, ..., p1
A, ..., p1
A+B]andP(y|x=x2;y∈ 180
Yall) = [p2
1, ..., p2
A, ..., p2
A+B], and noise transition matrix as T1andT2, respectively. We further 181
assume: 182
Ox1+Cx1=Ox2+Cx2=δ. (7)
We compare the error rate inflation ( ∆Ex1vs∆Ex2) with different label noise given same/fixed 183
noise ratio for a strictly fair comparison. Note we assume that x1andx2hold the same sampling 184
prior probability: P(x=x1;y∈ Yall) =P(x=x2;y∈ Yall)); so that, we assure that the whole 185
noise ratio Nis fixed, and more importantly, sample x1andx2can be considered as probabilistic 186
exchangeable in the dataset collection process. 187
For better clarity, we depict the derivation relations for ∆xin fig. 2. Specifically, for our two 188
interested cases above, we have corresponding error rate inflation for sample x(sample subscript 189
omitted for simplicity) as: 190
•Fitted case : 191
∆Ex= max[ p1, ..., p A]−parg max[PA+B
i=1piTi1,...,PA+B
i=1piTiA] (8)
•Memorized case : 192
∆Ex= max[ p1, ..., p A]−AX
i=1(pi·A+BX
j=1pjTji) (9)
We notice that ∆xin both cases are only affected by clean conditional probability P(y|x=x1;y∈ 193
Yall)and complete noise transition matrix T. 194
2The reader may refer to [14] for more discussions about related topics such as model generalization.
5Closed-set Noise Ratio:
Clean conditional probability:
Noisy conditional probability:
 Complete Noise Transition Matrix: 
Noisy conditional probability over
inlier classes:
Clean conditional probability over
inlier classes:
 Error Rate Inflation: 
Open-set Noise Ratio:
Figure 2: All-in-one derivation flowchart. Full details in appendix C.
3.4.1 How does open-set noise compare to closed-set noise? 195
We first try to elucidate the difference between open-set noise and closed-set noise. Without loss of 196
generality, we consider: 197
Ox1> Ox2, Cx1< Cx2. (10)
Intuitively speaking, we consider sample x1to be more prone to open-set noise compared to sample 198
x2, thus corresponding to the ‘more open-set noise’ scenario. However, without extra regularizations, 199
there exist infinite T1andT2fulfilling eq. (7) and eq. (10) given specific P(y|x=x1;y∈ Yall)and 200
P(y|x=x2;y∈ Yall)(see toy example below), the analysis on ∆Ex1vs∆Ex2is thus infeasible. 201
Toy example about agnostic TAssuming a ternary classification, with two known inlier
classes (“0" and “1") and one unknown outlier class “2". Say, we have sample x1with clean
conditional probability as [0.1,0.2,0.7]. Assuming two different noise transition matrices for
T1below:
[0.55,0.45,0.0] = [0 .1,0.2,0.7]"0.5 0 .50
0.75 0 .250
0.5 0 .50#
[0.45,0.55,0.0] = [0 .1,0.2,0.7]"0 1 0
0.5 0.50
0.5 0.50#
We have Ox1= 0.7, Cx1= 0.2in both conditions but we arrive at different noisy conditional
probability, similarly for sample x2.
202
We thus consider a class concentration assumption — in most classification datasets, the majority of 203
samples belong to specific class exclusively with high probability. In this condition, we have proved: 204
Theorem 3.5 (Open-set noise vsclosed-set noise) .Let us consider sample x1,x2fulfilling eq. (7)
and eq. (10) - compared to x2,x1is considered as more prone to open-set noise. Let us denote
a= arg max iP(y=i|x=x1;y∈ Yall)andb= arg max iP(y=i|x=x2;y∈ Yall), we
assume (with a high probability): p1
a→1,{p1
i→0}i̸=aandp2
b→1,{p2
b→0}i̸=b. Then, we have:
∆Ex1<∆Ex2
in both Fitted case andMemorized case . 205
Please refer to appendix D.1 for detailed proof. To summarize, we validate that in most conditions, 206
open-set noise is less harmful than closed-set noise in both fitted case andmemorized case . 207
3.4.2 How does different open-set noise compare to each other? 208
We further study how different open-set noise affect the model. Specifically, we consider: 209
Ox1=Ox2, Cx1=Cx2= 0. (11)
Intuitively speaking, we focus on the impacts of different open-set noise modes given the same/fixed 210
open-set noise ratio, while excluding the effect of closed-set noise. In this section, we assume 211
sample x1and sample x2holds the same clean conditional probability: [p1
1, ..., p1
A, ..., p1
A+B] = 212
[p2
1, ..., p2
A, ..., p2
A+B], to only focus on the impact of different open-set noise modes with the same 213
original sample. It is straightforward that Ox1=Ox2always holds sincePA+B
i=A+1p1
i=PA+B
i=A+1p2
i. 214
To ensure Cx1=Cx2= 0, we simply set T1
in=T2
in=I. 215
6Thus, we have the flexibility to explore various forms of Tout— corresponding to different open-set 216
noise modes. Specifically, we consider two distinct open-set noise modes: ‘easy’ open-set noise 217
when the transition from outlier classes to inlier classes involves completely random flipping, and 218
‘hard’ open-set noise when there exists an exclusive transition between the outlier class and specific 219
inlier class. We denote as Teasyfor ‘easy’ open-set noise and Thardfor ‘hard’ open-set noise, with 220
intuitive explanations below: 221
Teasy=
1
A...1
A... ... ...
1
A...1
A

B×A(12)
and 222
Thard="0...1
... ... ...
1...0#
B×A(13)
Especially, for Teasy, we have Tij=1
Aeverywhere; for Thard, we denote as Hi:{argj(Thard
ji = 223
1)}A
i=1the set of corresponding outlier classes j∈ Youtconfused to inlier class i∈ Yin. Without 224
loss of generality, we consider x1with ‘easy’ open-set noise Teasyandx2with ‘hard’ open-set 225
noise Thard. Please note, that we no longer require class concentration assumption here as the noise 226
transition matrix is already known. In this condition, we have proved: 227
Theorem 3.6 (‘Hard’ open-set noise vs‘easy’ open-set noise) .Let us consider sample x1,x2 228
fulfilling eq. (7)and eq. (11). We set the corresponding noise transition matrix as T1
out= 229
Teasy, T2
out=Thard, T1
in=T2
in=Iand denote [p1
1, ..., p1
A, ..., p1
A+B] = [p2
1, ..., p2
A, ..., p2
A+B] = 230
[p1, ..., p A, ..., p A+B]. Then, we have: 231
•Fitted case :
∆Ex1≤∆Ex2.
•Memorized case :
∆Ex1−∆Ex2=AX
i=1aibi.
Here, ai=pi, bi=P
j∈Hipj−1
APA+B
i=A+1pi. 232
Please refer to appendix D.2 for detailed proof. Specifically, we further discuss about memo- 233
rized case here. SincePA
i=1bi= 0,PA
i=1ai= 1, we can easily infer max(∆ Ex1−∆Ex2)≥ 234
0,min(∆ Ex1−∆Ex2)≤0. With theorem D.3, we know when the ranking of {p1
i}A
i=1is completely 235
in agreement with the ranking {P
j∈Hip1
j}A
i=1(constant term −1
APA+B
i=A+1p1
iomitted here), we 236
reach its maximum value with ∆Ex1−∆Ex2≥0. Intuitively speaking, this implies a scenario that 237
the ‘hard’ open-set noise tends to confuse a sample into the inlier class it primarily belongs to (with 238
higher semantic similarity), as indicated by its higher probability (the higher the p1
ithe higher the 239P
j∈Hip1
j). For example, an outlier ‘tiger’ image is wrongly included as a ‘cat’ rather than a ‘dog’ in 240
a ‘cat vsdog’ binary classification dataset. As this is more consistent with the common intuition, we 241
default to such noise mode for ‘hard’ open-set noise — assuming the ranking of {p1
i}A
i=1is of high 242
agreement with the ranking of {P
j∈Hip1
j}A
i=1. 243
To summarize, unlike the general comparison between open-set noise and closed-set noise, the ‘hard’ 244
open-set noise and the ‘easy’ open-set noise exhibit an opposite trend in two different cases. In the 245
fitted case , ‘easy’ open-set noise appears to be less harmful, while in the memorized case , the impact 246
of ‘hard’ open-set noise is comparatively smaller. 247
3.5 Rethinking open-set noise detection 248
In this section, we try to investigate a commonly used open-set noise identification mechanism based 249
on entropy dynamics. Within the sample selection paradigm, several methods [ 1,16] have proposed 250
to further identify open-set noise, based on the empirical phenomenon that samples with relatively 251
in-confident predictions are usually open-set samples, characterized by its high prediction entropy. 252
Specifically, we consider original sample xwithout noise transition, xwithThardandxwithTeasy253
7as a clean sample, a ‘hard’ open-set noise and an ‘easy’ open-set noise, respectively. For simplicity, 254
we omit the subscript. 255
Empirically, most sample selection method starts from the early training stages after certain epochs 256
of warm-up training, expecting the model to learn meaningful information before over-fitting. To 257
analyze the entropy dynamics, we thus consider the model predictions in the fitted case as a pragmatic 258
proxy. Let us denote as Heasy,Hhard andHclean the prediction entropy corresponds to these three 259
conditions, we have3: 260
Hclean =H([p1PA
i=1pi, ...,pAPA
i=1pi])
=H([p1+p1PA
i=1piA+BX
i=A+1pi, ..., p A+pAPA
i=1piA+BX
i=A+1pi]),
Heasy=H([p1+1
AA+BX
i=A+1pi, ..., p A+1
AA+BX
i=A+1pi]),
Hhard=H([p1+X
j∈H1pj, ..., p A+X
j∈HApj]).(14)
We note Heasy≥ H clean4. However, comparing Hhard andHclean is non-trivial without specific 261
values for each entry. Thus, we suggest open-set noise detection based on the prediction entropy may 262
only be effective for ‘easy’ open-set noise . 263
4 Experiments 264
In this section, we try to validate our theoretical findings. In section 4.1, we validate the theoretical 265
comparisons of different label noise. In section 4.2, we validate the entropy dynamics with different 266
label noise. Moreover, in appendix E.1, we revisit the performance of two existing LNL methods 267
involving open-set noise. To conduct more controllable, fair and accurate experiments, we propose 268
two synthetic open-set noisy datasets — CIFAR100-O and ImageNet-O, respectively based on 269
the CIFAR100 and ImageNet datasets. We also consider closed-set noise in some experiments, 270
particularly, the symmetric closed-set noise. Please refer to appendix A for more dataset and 271
implementation details and also details about open-set detection protocol. 272
4.1 Empirical validation on previous probabilistic findings 273
In this section, we conduct experiments to validate the theorem 3.5 and theorem 3.6. Since most deep 274
models have sufficient capacity, we consider direct supervised learning from scratch on the noisy 275
dataset and consider the final model as the memorized case - as evidenced by nearly 100% train set 276
accuracy. Conversely, obtaining a model that perfectly fits the data distribution is often challenging; 277
here, we consider training a single-layer linear classifier upon a frozen pretrained encoder. Due to the 278
limited capacity of the linear layer, we expect to roughly approach the fitted case . 279
We show classification accuracy on CIFAR100-O and ImageNet-O datasets under different noise 280
ratios, as shown in fig. 3(a/b). We find that: 1) in both cases, the presence of open-set noise has 281
a significantly smaller impact on classification accuracy compared to closed-set noise. 2) ‘hard’ 282
open-set noise and ‘easy’ open-set noise show opposite trends in the two different scenarios. These 283
results align perfectly with our theoretical analysis. 284
In addition to closed-set classification accuracy, we also report the model’s open-set detection 285
performance using the maximum prediction value as the indicator [ 9]) in fig. 3(c/d). We find that, in 286
both cases, the presence of open-set noise leads to a degraded open-set detection performance, while 287
conversely, the presence of closed-set noise can often even enhance open-set detection performance. 288
In light of this contrasting trend, we propose that the open-set detection task, in addition to the default 289
closed-set classification, may help to offer a more comprehensive evaluation of LNL methods. 290
3Please refer to appendix D.2 for full derivation, specifically the eq. (36) and eq. (37).
4Please note, empirically the relative minority of open-set samples can also lead to low-confidence predictions,
which is beyond the scope of this work. We leave it to interested readers.
8(a) Closed-set classification in fitted case (b) Closed-set classification in memorized case
(c) Open-set detection in fitted case (d) Open-set detection in memorized caseFigure 3: Direct supervised training with different noise modes/ratios.
4.2 Inspecting entropy-based open-set noise detection mechanism 291
In section 3.5, we briefly analyze the open-set detection mechanism based on the entropy values of 292
model predictions and find that it may be effective only for ‘easy’ open-set noise. Here, we again 293
utilize the CIFAR100-O and ImageNet-O datasets for validation experiments with different open-set 294
noise ratios and modes. Specifically, we adopt the common warm-up idea used in existing LNL 295
methods - training with the entire dataset for a certain number of epochs. We report the model’s 296
predicted entropy values for each sample at the {5th,10th,20th,30th}epoch in fig. 4. 297
Epoch 5 Epoch 10 Epoch 20 Epoch 30
Epoch 5 Epoch 10 Epoch 20 Epoch 30
Epoch 5 Epoch 10 Epoch 20 Epoch 30
Epoch 5 Epoch 10 Epoch 20 Epoch 30
(a) CIFAR100-O with 20% 'easy' open-set noise
(c) ImageNet-O with 40% 'easy' open-set noise(b) CIFAR100-O with 20% 'hard' open-set noise
(d) ImageNet-O with 40% 'hard' open-set noise
Figure 4: Entropy dynamics w.r.t different datasets/noise modes/noise ratios.
We validate that the entropy dynamics is a more effective indicator for ‘easy’ open-set noise compared 298
to ‘hard’ open-set noise ((a) vs (b), (c) vs (d) in fig. 4). However, even for ‘easy’ open-set noise, we 299
also notice that the warm-up epoch matters a lot — too early ( 5thepoch in fig. 4(c)) or too late ( 30th 300
epoch in fig. 4(c)) also make open-set noise difficult to distinguish. We also test with mixed noise 301
including both open-set noise and closed-set noise, please refer to appendix B for more discussions. 302
5 Conclusions 303
This paper focuses on exploring how open-set label noise affects the performance of models. While 304
the ‘open world’ setting involving open-set samples has been widely discussed in several other weakly 305
supervised learning settings, its application in the context of learning with noisy labels has been 306
understudied. In light of this, we reconsider the LNL problem, specifically focusing on the impact of 307
open-set noise compared to closed-set noise, and different types of open-set noise compared to each 308
other, on the evaluation performance. In light of the challenges existing testing frameworks face in 309
handling open-set noise, we explore the open-set detection task to address the deficiencies in model 310
evaluation for open-set noise and conducted preliminary experiments. Additionally, we look into 311
the common mechanism for detecting open-set noise based on the model’s prediction entropy. Both 312
theoretical and empirical results highlight the urgent need for a deeper exploration of open-set noise 313
and its complex impact on model performance. 314
9References 315
[1]Paul Albert, Diego Ortego, Eric Arazo, Noel E O’Connor, and Kevin McGuinness. Addressing 316
out-of-distribution label noise in webly-labelled data. In Proceedings of the IEEE/CVF Winter 317
Conference on Applications of Computer Vision , pages 392–401, 2022. 2, 7 318
[2]Eric Arazo, Diego Ortego, Paul Albert, Noel O’Connor, and Kevin McGuinness. Unsupervised 319
label noise modeling and loss correction. In International Conference on Machine Learning , 320
pages 312–321. PMLR, 2019. 1, 2 321
[3]Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling: 322
Revisiting pseudo-labeling for semi-supervised learning. In Proceedings of the AAAI Conference 323
on Artificial Intelligence , volume 35, pages 6912–6920, 2021. 1 324
[4]Pengfei Chen, Junjie Ye, Guangyong Chen, Jingwei Zhao, and Pheng-Ann Heng. Beyond 325
class-conditional assumption: A primary attempt to combat instance-dependent label noise. In 326
Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pages 11442–11450, 327
2021. 1, 2 328
[5]Chen Feng, Georgios Tzimiropoulos, and Ioannis Patras. Ssr: An efficient and robust framework 329
for learning with unknown label noise. In 33rd British Machine Vision Conference 2022, BMVC 330
2022, London, UK, November 21-24, 2022 . BMV A Press, 2022. 2, 19, 20 331
[6]Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep 332
neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 31, 333
2017. 1, 2 334
[7]Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adapta- 335
tion layer. 2016. 1, 2 336
[8]Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi 337
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. 338
arXiv preprint arXiv:1804.06872 , 2018. 2 339
[9]Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution 340
examples in neural networks. In International Conference on Learning Representations , 2016. 341
8, 13 342
[10] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning 343
data-driven curriculum for very deep neural networks on corrupted labels. In International 344
Conference on Machine Learning , pages 2304–2313. PMLR, 2018. 2, 12 345
[11] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as 346
semi-supervised learning. arXiv preprint arXiv:2002.07394 , 2020. 1, 2, 12, 19, 20 347
[12] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: 348
Visual learning and understanding from web data. arXiv preprint arXiv:1708.02862 , 2017. 1, 349
12 350
[13] Eran Malach and Shai Shalev-Shwartz. Decoupling" when to update" from" how to update". 351
arXiv preprint arXiv:1706.02613 , 2017. 2 352
[14] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning . 353
MIT press, 2018. 5 354
[15] Diego Ortego, Eric Arazo, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Multi- 355
objective interpolation training for robustness to label noise. In Proceedings of the IEEE/CVF 356
Conference on Computer Vision and Pattern Recognition , pages 6606–6615, 2021. 2, 12 357
[16] Ragav Sachdeva, Filipe R Cordeiro, Vasileios Belagiannis, Ian Reid, and Gustavo Carneiro. 358
Evidentialmix: Learning with combined open-set and closed-set noisy labels. In Proceedings of 359
the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 3607–3615, 2021. 360
2, 7, 12 361
10[17] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. Selfie: Refurbishing unclean samples for robust 362
deep learning. In International Conference on Machine Learning , pages 5907–5915. PMLR, 363
2019. 1, 2 364
[18] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao 365
Xia. Iterative learning with open-set noisy labels. In Proceedings of the IEEE conference on 366
computer vision and pattern recognition , pages 8688–8696, 2018. 2 367
[19] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross 368
entropy for robust learning with noisy labels. In Proceedings of the IEEE/CVF International 369
Conference on Computer Vision , pages 322–330, 2019. 1, 2 370
[20] Hongxin Wei, Lue Tao, Renchunzi Xie, and Bo An. Open-set label noise can improve robustness 371
against inherent label noise. Advances in Neural Information Processing Systems , 34:7978–7992, 372
2021. 3 373
[21] Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, and Chao Chen. A 374
topological filter for learning with label noise. Advances in neural information processing 375
systems , 33:21382–21393, 2020. 2 376
[22] Zhi-Fan Wu, Tong Wei, Jianwen Jiang, Chaojie Mao, Mingqian Tang, and Yu-Feng Li. Ngc: A 377
unified framework for learning with open-world noisy data. arXiv preprint arXiv:2108.11035 , 378
2021. 2, 12 379
[23] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi 380
Sugiyama. Are anchor points really indispensable in label-noise learning? Advances in neural 381
information processing systems , 32, 2019. 1, 2 382
[24] Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi 383
Sugiyama. Sample selection with uncertainty of losses for learning with noisy labels. arXiv 384
preprint arXiv:2106.00445 , 2021. 2 385
[25] Xiaobo Xia, Bo Han, Nannan Wang, Jiankang Deng, Jiatong Li, Yinian Mao, and Tongliang 386
Liu. Extended T: Learning with mixed closed-set and open-set noisy labels. IEEE Transactions 387
on Pattern Analysis and Machine Intelligence , 45(3):3047–3058, 2022. 2 388
[26] Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. Estimating 389
instance-dependent bayes-label transition matrix using a deep neural network. In International 390
Conference on Machine Learning , pages 25302–25312. PMLR, 2022. 1, 2 391
[27] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. 392
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 393
pages 7017–7025, 2019. 2 394
[28] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does 395
disagreement help generalization against label corruption? In International Conference on 396
Machine Learning , pages 7164–7173. PMLR, 2019. 2 397
[29] Zhilu Zhang and Mert R Sabuncu. Generalized cross entropy loss for training deep neural 398
networks with noisy labels. arXiv preprint arXiv:1805.07836 , 2018. 1, 2 399
[30] Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Robust curriculum learning: from clean label de- 400
tection to noisy label self-correction. In International Conference on Learning Representations , 401
2020. 2 402
11A Experiment details 403
A.1 Dataset details 404
Previous works involving open-set noise also try to build synthetic noisy datasets, typically treating 405
different datasets as open-set noise for each other to construct synthetic noisy dataset [ 16,22]. In 406
this scenario, potential domain gaps could impact a focused analysis of open-set noise. In this 407
work, we propose selecting inlier/outlier classes from the same dataset to avoid this issue. Besides, 408
in previous works, the consideration of open-set noise patterns often focused on random flipping 409
from outlier classes to all possible inlier classes, which is indeed the ‘easy’ open-set noise adopted 410
here. However, both our theoretical analysis and experimental findings demonstrate that ‘easy’ 411
open-set noise and ‘hard’ open-set noise exhibit distinct characteristics. Therefore, relying solely 412
on experiments with ‘Easy’ open-set noise is insufficient, emphasizing the necessity to explore and 413
understand the complexities associated with different types of open-set noise. We also evaluate with 414
closed-set noise in some experiments, by default, we consider the common symmetric closed-set 415
noise in this work. 416
CIFAR100-O For the original CIFAR100 dataset, in addition to the commonly-used 100 fine 417
classes, there exist 20 coarse classes each consisting of 5 fine classes. To build CIFAR100-O, we 418
select one fine class from each coarse class as an inlier class (20 classes in total) while considering 419
the remaining classes as outlier classes (80 classes in total). Then, we consider ‘Hard’ and ‘Easy’ 420
open-set noise as below: 421
•‘Hard’: Randomly selected samples from the same coarse category as the target category 422
were introduced as open-set noise. 423
•‘Easy’: Regardless of the target category, samples from the remaining categories were 424
randomly introduced as open-set noise. 425
ImageNet-O For a more challenging benchmark, we consider ImageNet-1K datasets - consisting of 426
1,000 classes. Specifically, we randomly select 20 classes and artificially identify another 20 classes 427
similar to each of them: 428
inliers= [’tench’, ’great white shark’, ’cock’, ’indigo bunting’, ’European fire salamander’, ’African 429
crocodile’, ’barn spider’, ’macaw’, ’rock crab’, ’golden retriever’, ’wood rabbit’, ’gorilla’, ’abaya’, 430
’beer bottle’, ’bookcase’, ’cassette player’, ’coffee mug’, ’shopping basket’, ’trifle’, ’meat loaf’] 431
outliers= [’goldfish’, ’tiger shark’, ’hen’, ’robin’, ’common newt’, ’American alligator’, ’garden 432
spider’, ’sulphur-crested cockatoo’, ’king crab’, ’Labrador retriever’, ’Angora’, ’chimpanzee’, 433
’academic gown’, ’beer glass’, ’bookshop’, ’CD player’, ’coffeepot’, ’shopping cart’, ’ice cream’, 434
’pizza’] 435
Then, we consider ‘Hard’ and ‘Easy’ open-set noise as below: 436
•‘Hard’: Randomly select samples from the corresponding similar outlier class as the target 437
category were introduced as open-set noise. 438
•‘Easy’: Samples from the remaining categories were randomly introduced as open-set noise. 439
For open-set detection, we directly use the corresponding test sets of these classes from the original 440
datasets. 441
WebVision WebVision [ 12] is an extensive dataset comprising 1,000 classes of images obtained 442
through web crawling, which thus contains a large amount of open-set noise. In line with previous 443
studies [ 10,11,15], we evaluate our methods using the first 50 classes from the Google Subset of 444
WebVision. To test the performance of open-set detection on the WebVision dataset, we collect a 445
separate test set consisting of open-set images, following the same collection process as the WebVision 446
dataset. Specifically, we utilize the Google search engine with the class names as keywords and 447
identify those open-set samples that haven’t been included in the train set for this test set. 448
12A.2 Implementation details 449
Here, we provide detailed implementation specifications for the fitted case andmemorized case in 450
section 4.1. We also briefly the applied open-set detection protocol. 451
Fitted case For the fitted case , we train a randomly initialized classifier - a single linear layer based 452
on the encoder of the ResNet18 model with pretrained weights. In the case of the CIFAR100-O 453
dataset, a weak augmentation strategy involving image padding and random cropping is applied 454
during training, with a batch size of 512. The weight decay (wd) is set to 0.0005, and the model 455
undergoes training for 100 epochs, utilizing a learning rate (lr) of 0.02. The learning rate schedule 456
follows a cosine annealing strategy. 457
For the ImageNet-O dataset, no augmentation is applied during training. The batch size is maintained 458
at 512, with a weight decay (wd) of 0.01. The model is trained for 100 epochs, employing a learning 459
rate (lr) of 0.02. The learning rate schedule for this case also adheres to a cosine annealing strategy. 460
Memorized case In this case, we train a PreResNet18 model from scratch. For both datasets, a 461
weak augmentation strategy involving image padding and random cropping is applied during training, 462
with a batch size of 128. The weight decay (wd) is set to 0.0005, and the model undergoes training 463
for 200 epochs, utilizing a learning rate (lr) of 0.02. The learning rate schedule also follows a cosine 464
annealing strategy. 465
Open-set detection protocol We use the maximum softmax probability in [ 9] for the open-set 466
detection task. Specifically, assume the trained model foutputs a softmax vector pifor each sample 467
xi. We then choose a threshold value tbetween 0 and 1. For evaluation, we consider binary labels 468
indicating whether a sample belongs to a known class (closed-set) or the open-set and convert the 469
open-set detection task into a binary classification problem. Samples with a maximum softmax value 470
pmax
i below the threshold are considered potential open-set samples. This is because a low maximum 471
value indicates the model is less confident in any specific class for that sample. 472
B Entropy dynamics for mixed label noise 473
In addition to the open-set noise only scenario, we also inspect the entropy dynamics with mixed 474
label noise in fig. 5. Here, we use the notation ‘0.2all_0.5easy’ to represent a scenario where the 475
total noise ratio is 0.2, and within this, half of them are ’easy’ open-set noise. In the presence of 476
mixed label noise, the existence of closed-set noise severely interferes with identifying open-set 477
noise. For example, in fig. 5(d), the entropy values of open-set noise even exceed those of clean 478
samples. Though not theoretically analyzed, this further suggests that entropy dynamics based on 479
model predictions, may be fragile, and we need to handle open-set noise more cautiously.
Epoch 5 Epoch 10 Epoch 20 Epoch 30
(a) CIFAR100-O 0.2all_0.5easy
Epoch 5 Epoch 10 Epoch 20 Epoch 30
 (b) CIFAR100-O 0.2all_0.5hard
Epoch 5 Epoch 10 Epoch 20 Epoch 30
(c) ImageNet-O 0.4all_0.5easy
Epoch 5 Epoch 10 Epoch 20 Epoch 30
 (d) ImageNet-O 0.4all_0.5hard
Figure 5: Entropy dynamics w.r.t mixed label noise.
480
C Error rate inflation in two different cases 481
In this section, we present the computation details of error rate inflation in two interested cases - fitted 482
case andmemorized case . Specifically, we have: 483
13•Fitted case : 484
Ex= (1−P(y= arg maxkPn(y=k|x=x;y∈ Yin)|x=x;y∈ Yin))·P(x=x;y∈ Yin).
(15)
•Memorized case : 485
Ex= (1−P(y= arg maxkPyn(y=k|x=x;y∈ Yin)|x=x;y∈ Yin))·P(x=x;y∈ Yin)
=X
yn∈Yin(1−P(y=yn|x=x;y∈ Yin))Pn(y=yn|x=x;y∈ Yin)·P(x=x;y∈ Yin)
= [1−X
yn∈YinP(y=yn|x=x;y∈ Yin)Pn(y=yn|x=x;y∈ Yin)]·P(x=x;y∈ Yin)
(16)
While E∗
xdenotes the Bayes optimal error rate: 486
E∗
x= (1−max kP(y=k|x=x;y∈ Yin))·P(x=x;y∈ Yin). (17)
We thus have ∆Exin both cases as: 487
•Fitted case : 488
∆Ex= [max kP(y=k|x=x;y∈ Yin)−P(y= arg maxkPn(y=k|x=x;y∈ Yin)|x=x;y∈ Yin)]
·P(x=x;y∈ Yin).(18)
•Memorized case : 489
∆Ex= [max kP(y=k|x=x;y∈ Yin)−X
yn∈YinP(y=yn|x=x;y∈ Yin)Pn(y=yn|x=x;y∈ Yin)]
·P(x=x;y∈ Yin).(19)
Details on the derivation of error rate inflation (fig. 2) Then, we describe the essential concepts 490
depicted in fig. 2 in detail. For better clarity, we here restate the notations in section 3.4. We explicitly 491
consider two specific sample points x1andx2being perturbed independently, corresponding to two 492
different label noise modes. Let us assume its clean conditional probability as: 493
P(y|x=x1;y∈ Yall) = [p1
1, ..., p1
A, ..., p1
A+B],
P(y|x=x2;y∈ Yall) = [p2
1, ..., p2
A, ..., p2
A+B],(20)
and denote its noise transition matrix as T1={T1
ij}A+B
i,j=1andT2={T2
ij}A+B
i,j=1, respectively. Here, 494
{T1
ij= 0},{T2
ij= 0}for all j > A . 495
With eq. (1), we compute the corresponding noisy conditional probability for both samples as: 496
Pn(y|x=x1;y∈ Yall) = [A+BX
i=1p1
iT1
i1, ...,A+BX
i=1piT1
iA,0, ...,0],
Pn(y|x=x2;y∈ Yall) = [A+BX
i=1p2
iT2
i1, ...,A+BX
i=1p2
iT2
iA,0, ...,0].(21)
Note that the error rate inflation is dependent on the clean conditional probability over inlier classes , 497
noisy conditional probability over inlier classes andsampling prior over inlier classes as shown in 498
eq. (18) and eq. (19). 499
14Specifically, for sample x1, we have: 500
P(y=k|x=x1;y∈ Yin) =P(y=k|x=x1;y∈ Yall)P
i∈YinP(y=i|x=x1;y∈ Yall)=p1
kPA
i=1p1
i,
Pn(y=k|x=x1;y∈ Yin) =Pn(y=k|x=x1;y∈ Yall)P
i∈YinPn(y=i|x=x1;y∈ Yall)=A+BX
i=1p1
iT1
ik,
P(x=x1;y∈ Yin) =P
y∈YinP(x=x1,y=y;y∈ Yall)RP
y∈YinP(x=x,y=y;y∈ Yall)dx
∝X
y∈YinP(x=x1,y=y;y∈ Yall)
∝X
y∈YinP(y=y|x=x1;y∈ Yall)P(x=x1;y∈ Yall)
P(x=x1;y∈Yall)=P(x=x2;y∈Yall)=δ− − − − − − − − − − − − − − − − − − − − − − − − →
∝X
y∈YinP(y=y|x=x1;y∈ Yall) =AX
i=1p1
i.(22)
Simply changing the subscript leads us to the formulations for sample x2. To summarize, wrapping 501
the above together, we have: 502
P(y|x=x;y∈ Yin) = [p1PA
i=1pi, ...,pAPA
i=1pi],
Pn(y|x=x;y∈ Yin) = [A+BX
i=1piTi1, ...,A+BX
i=1piTiA],
P(x=x1;y∈ Yin) =AX
i=1pi.(23)
We here omit the sample subscript and abbreviate the proportional symbol for simplicity. With 503
eq. (18), eq. (19) and eq. (23), we can then compute and compare ∆Exin both fitted case and 504
memorized case : 505
∆Ex= max[ p1, ..., p A]−parg max[PA+B
i=1piTi1,...,PA+B
i=1piTiA](Fitted case ) (24)
506
∆Ex= max[ p1, ..., p A]−AX
i=1(pi·A+BX
j=1pjTji) (Memorized case ) (25)
D Full proof of theorem 3.5 and theorem 3.6 507
Error rate inflation comparison s.t.same noise ratio To ensure a fair comparison, in this work, 508
we focus on the impact of different label noise given the same noise ratio - modifying OxandCx 509
while analyzing the trend of ∆Ex. Specifically, for above mentioned x1andx2, we further assume: 510
Ox1+Cx1=Ox2+Cx2=δ. (26)
which leads us to: 511
A+BX
i=A+1p1
i+AX
i=1AX
j=1,j̸=iT1
ijp1
i=A+BX
i=A+1p2
i+AX
i=1AX
j=1,j̸=iT2
ijp2
i−→AX
i=1T1
iip1
i=AX
i=1T2
iip2
i(27)
Please note, here the clean conditional probability is considered as known and fixed, while eq. (27) 512
restricts the values of the noise transition matrix T1andT2, given specific clean conditional 513
probability. We then analyze and compare the error rate inflation in both conditions. 514
15D.1 Proof of theorem 3.5 — Open-set noise vs Closed-set noise 515
In this section, we try to compare open-set noise and closed-set noise. Without loss of generality, we 516
consider: 517
Ox1> Ox2. (28)
Intuitively speaking, sample x1is more affected by open-set noise compared to sample x2, thus 518
corresponding to the interested ‘open-set noise’. 519
As clarified by the toy example in section 3.4.1, without extra regularizations, the noise transition 520
matrix is not identifiable. We thus consider a simple compromise situation - in most classification 521
problems, the majority of samples (with a high probability) belong to a specific class exclusively with 522
high probability. 523
Let us denote:
a= arg max
iP(y=i|x=x1;y∈ Yall)
and
b= arg max
iP(y=i|x=x2;y∈ Yall).
We assume :
p1
a→1,{p1
i→0}i̸=a, p2
b→1,{p2
i→0}i̸=b,
and we have:
Ox1=A+BX
i=A+1p1
i, Ox2=A+BX
i=A+1p2
i.
With eq. (28), we easily infer that: a∈ Youtwhile b∈ Yin. Intuitively speaking, x1is an open-set 524
noise, with its clean conditional probability concentrated on one of the outlier classes, and vice versa 525
forx2. 526
With eq. (27), we further have:
AX
i=1T1
iip1
i≈AX
i=1T1
ii×0≈0,
AX
i=1T2
iip2
i≈AX
i=1,i̸=bT2
ii×0 +T2
bb×1≈T2
bb.
Thus we have: T2
bb≈0, which enables us to analyze and compare ∆Ex1and∆Ex2: 527
Fitted case In this case, according to eq. (24), we have: 528
∆Ex1= max[ p1
1, ..., p1
A]−parg max[PA+B
i=1p1
iT1
i1,...,PA+B
i=1p1
iT1
iA]
<max[p1
1, ..., p1
A]−min[p1
1, ..., p1
A]
p1
a→1,{p1
i→0}i̸=a,a∈Yout
− − − − − − − − − − − − − − − − →
≈0,(29)
529
∆Ex2= max[ p2
1, ..., p2
A]−parg max[PA+B
i=1p2
iT2
i1,...,PA+B
i=1p2
iT2
iA]
[PA+B
i=1p2
iT2
i1,...,PA+B
i=1p2
iT2
iA]≈[T2
a1,T2
a2,...,bz}|{
0,...,T2
aA]− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − →
=p2
b−p2
n
p2
b→1,{p2
i→0}i̸=b,b∈Yin,n̸=b− − − − − − − − − − − − − − − − − − − →
≈1.(30)
16Memorized case In this case, according to eq. (25), we similarly have: 530
∆Ex1= max[ p1
1, ..., p1
A]−AX
i=1(p1
i·A+BX
j=1p1
jT1
ji)≈0, (31)
∆Ex2= max[ p2
1, ..., p2
A]−AX
i=1(p2
i·A+BX
j=1p2
jT2
ji)≈1. (32)
We wrap up above for theorem D.2: 531
Theorem D.1 (Open-set noise vsClosed-set noise) .Let us consider sample x1,x2fulfilling eq. (26)
and eq. (28) - compared to x2,x1is considered as more prone to open-set noise. Let us denote
a= arg max iP(y=i|x=x1;y∈ Yall)andb= arg max iP(y=i|x=x2;y∈ Yall), we
assume (with a high probability): p1
a→1,{p1
i→0}i̸=aandp2
b→1,{p2
b→0}i̸=b. Then, we have:
∆Ex1<∆Ex2
in both fitted case andmemorized case . 532
D.2 Derivation of theorem 3.5 — ‘hard’ open-set noise vs ‘easy’ open-set noise 533
In this part, we try to analyze and compare ‘hard’ open-set noise with ‘easy’ open-set noise. For 534
better clarification, we repeat here the essential statements: 535
T1
out=Teasy=
1
A...1
A... ... ...
1
A...1
A

B×A(33)
and 536
T2
out=Thard="0...1
... ... ...
1...0#
B×A(34)
and 537
T1
in=T2
in=I. (35)
Especially, for Teasy, we have Tij=1
Aeverywhere; for Thard, we denote as Hi:{argj(Thard
ji =
1)}A
i=1the set of corresponding outlier classes j∈ Youtconfused to inlier class i∈ Yin. We also
have:
[p1
1, ..., p1
A, ..., p1
A+B] = [p2
1, ..., p2
A, ..., p2
A+B]
. 538
Fitted case In this case, according to eq. (24), for sample x1with ‘easy’ open-set noise, we have: 539
∆Ex1= max[ p1
1, ..., p1
A]−parg max[PA+B
i=1p1
iT1
i1,...,PA+B
i=1p1
iT1
iA]
= max[ p1
1, ..., p1
A]−parg max[ p1
1+1
APA+B
i=A+1p1
i,...,p1
A+1
APA+B
i=A+1p1
i]
= 0,(36)
and, for sample x2with ‘hard’ open-set noise, we have: 540
∆Ex2= max[ p2
1, ..., p2
A]−parg max[PA+B
i=1p2
iT2
i1,...,PA+B
i=1p2
iT2
iA]
= max[ p2
1, ..., p2
A]−parg max[ p2
1+P
b∈H1p2
b,...,p2
A+P
b∈HAp2
b]
∈[0,max[p2
1, ..., p2
A]−min[p2
1, ..., p2
A]].(37)
17Memorized case In this case, according to eq. (25), for sample x1with ‘easy’ open-set noise, we 541
have: 542
∆Ex1= max[ p1
1, ..., p1
A]−AX
i=1(p1
i·A+BX
j=1p1
jT1
ji)
= max[ p1
1, ..., p1
A]−AX
i=1p1
i(p1
i+1
AA+BX
i=A+1p1
i).(38)
and, for sample x2with ‘hard’ open-set noise, we have: 543
∆Ex2= max[ p2
1, ..., p2
A]−AX
i=1(p2
i·A+BX
j=1p2
jT2
ji)
= max[ p2
1, ..., p2
A]−AX
i=1p2
i(p2
i+X
j∈Hip2
j)(39)
We further have:
∆Ex1−∆Ex2=AX
i=1p1
i(X
j∈Hip1
j−1
AA+BX
i=A+1p1
i).
Letai=p1
i, bi=P
j∈Hip1
j−1
APA+B
i=A+1p1
i, we have:
∆Ex1−∆Ex2=AX
i=1aibi.
To summarize, we wrap up the above together: 544
Theorem D.2 (‘Hard’ open-set noise vs‘easy’ open-set noise) .Let us consider sample x1,x2 545
fulfilling eq. (26) and eq. (11). We set the corresponding noise transition matrix as in eq. (33), eq. (34) 546
and eq. (35). We further assume [p1
1, ..., p1
A, ..., p1
A+B] = [p2
1, ..., p2
A, ..., p2
A+B]. Then, we have: 547
∆Ex1≤∆Ex2
infitted case ,
∆Ex1−∆Ex2=AX
i=1aibi
inmemorized case . Here, ai=p1
i, bi=P
j∈Hip1
j−1
APA+B
i=A+1p1
i. 548
Theorem D.3 (Rearrangement Inequality) .For the sequences a1, a2, . . . , a nandb1, b2, . . . , b n,
where a1≤a2≤. . .≤anandb1≤b2≤. . .≤bn, the rearrangement inequality is given by:
a1·b1+a2·b2+. . .+an·bn≥a1·bσ(1)+a2·bσ(2)+. . .+an·bσ(n)≥a1·bn+a2·bn−1+. . .+an·b1
Here, σdenotes a permutation of the indices 1,2, . . . , n . The leftmost expression corresponds to the 549
case where σ(i) =i(identity permutation), and the rightmost expression corresponds to the case 550
where σ(i) =n+ 1−i(reverse permutation). 551
E Revisiting LNL methods 552
E.1 Revisiting existing LNL methods with open-set noise 553
In this section, we further investigate the learning effectiveness of existing LNL methods on previously 554
discussed open-set label noise, especially the dominant ones based on sample selection - these methods 555
often integrate different regularization terms and off-the-shelf techniques, resulting in state-of-the-art 556
performance. In essence, such methods typically include a sample selection module along with a 557
18robust training module. Here, we briefly denote the clean subset selected by the original method 558
asXclean and denote the entire dataset as Xall. Moreover, we consider integrating the previously 559
mentioned open-set detection mechanism into current LNL methods - we denote as Xinan inlier 560
subset based on entropy dynamics. Then, maintaining the robust training module unchanged, we 561
consider below three different variants (the involved LNL method abbreviated as X, the inlier subset 562
detection method abbreviated as EntSel ): 563
• X: Robust training using Xclean , i.e., the original method; 564
• EntSel: Robust training using Xin; 565
• X + EntSel: Robust training using Xin∩Xclean . 566
Specifically, we test with two representative LNL methods with well-maintained open-source imple- 567
mentations: SSR [ 5] and DivideMix [ 11]. Please refer to appendix E.2 for more details. In fig. 6, we 568
show results on CIFAR100-O and ImageNet-O. 569
0.2 0.2 0.4 0.4 0.2 0.2 0.4 0.4
Easy Hardraro
0.2 0.2 0.4 0.4 0.2 0.2 0.4 0.4
Easy Hardraro
0.2 0.2 0.4 0.4 0.2 0.2 0.4 0.4
Easy Hardraro
0.2 0.2 0.4 0.4 0.2 0.2 0.4 0.4
Easy Hardraro
0.2 0.2 0.4 0.4 0.2 0.2 0.4 0.4
Easy Hardraro
0.2 0.2 0.4 0.4 0.2 0.2 0.4 0.4
Easy Hardraro
0.2 0.2 0.4 0.4 0.2 0.2 0.4 0.4
Easy Hardraro
0.2 0.2 0.4 0.4 0.2 0.2 0.4 0.4
Easy Hardraro
Figure 6: Evaluation of directly supervised training with different noise modes/ratios. First row:
Closed-set classification accuracy; Second row: Open-set detection ROC AUC.
First, focusing on the classification accuracy of the model, we observe that 1) using EntSel instead of 570
the original method leads to a reduction in classification accuracy in the mixed noise scenario (SSR 571
vs EntSel and DivideMix vs EntSel); in pure open-set noise only scenarios, there are no obvious 572
trends showing differences in different variant models. 2) the classification accuracy for mixed noise 573
is significantly lower than that of only open-set noise at the same noise ratio, which further confirms 574
that closed-set noise is more harmful than open-set noise. 575
Furthermore, we demonstrate the performance of this model in detecting open-set samples - the 576
introduction of EntSel significantly enhances the effectiveness of open-set detection, especially 577
when the open-set noise is set to ‘easy’ mode. This also further confirms our theoretical analysis in 578
section 3.5 and experimental results in section 4.2.
Table 1: Results on WebVision dataset.
Method Accuracy (%) ROC AUC (%)
SSR 77.48 80.84
EntSel 77.08 85.43
SSR + EntSel 76.04 79.90
DivideMix 74.08 86.39
EntSel 62.96 81.66
DivideMix + EntSel 58.94 83.85
579
19We report results for the WebVision dataset in table 1, reaffirming that combining ‘EntSel’ with ‘SSR’ 580
significantly enhances open-set detection performance. Notably, most open-set noise in WebVision 581
seems to arise from factors like text co-occurrence rather than semantic similarity, categorizing 582
it more as ‘easy’ open-set noise. This may explain why EntSel effectively improves open-set 583
detection in this context. However, when combining EntSel with DivideMix, both classification 584
accuracy and open-set detection decrease, indicating that the robustness of the EntSel method itself is 585
questionable. Additionally, simply merging SSR/DivideMix with EntSel using subset intersection (X 586
+ EntSel) also leads to a decrease in both classification accuracy and open-set detection performance. 587
Finally, it’s worth mentioning that, despite having lower classification accuracy than SSR, DivideMix 588
outperforms SSR in open-set detection ROC AUC scores. All above illustrates that simply evaluating 589
the classification accuracy may be one-sided. 590
E.2 Details of involved methods 591
DivideMix [11] Denoting as L={li}N
i=1the losses of all samples, DivideMix proposes to model 592
it (after min-max normalization) with a Gaussian Mixture Model. The probabilities {pi}N
i=1of each 593
sample belonging to the component with a smaller mean value are then extracted. Samples with 594
probability pigreater than the threshold θare then identified as a “clean" subset. Link to code: 595
https://github.com/LiJunnan1992/DivideMix . 596
SSR [5] In contrast to DivideMix, SSR extracts features for each sample and constructs a neigh- 597
bourhood graph. By computing the nearest neighbour labels for each sample, a pseudo-label 598
distribution pis obtained through a KNN voting process. The consistency c=py/pmax between 599
this voted distribution and the given noisy label y(logit label) is then calculated. Samples with 600
consistency cgreater than the threshold θare identified as part of the “clean" subset. Link to code: 601
https://github.com/MrChenFeng/SSR_BMVC2022 . 602
EntSel We also provide a concise overview of the steps involved in EntSel, following a methodology 603
similar to DivideMix. Denoting as E={ei}N
i=1the entropy of all samples’ predictions, we similarly 604
model it (after min-max normalization) with a Gaussian Mixture Model. The probabilities {pi}N
i=1605
of each sample belonging to the component with a smaller mean value are then extracted. Samples 606
with probability pigreater than the threshold θ′are then identified as “inlier" subset. 607
Generally, we have a closed-set classifier g and an encoder f, and we use it for training based on 608
the selected subset. Existing sample selection methods usually rely on an estimated prediction 609
and a threshold to help filter clean samples. Our proposed OpenAdaptor focuses on the difference 610
between open-set and closed-set samples. When integrating them, we propose two different strategies: 611
absorption and exclusion. 612
E.3 Implementation details 613
Experiment details For both SSR and DivideMix, we employ model and optimization configu- 614
rations on the same dataset. Specifically, for CIFAR100-O and ImageNet-O, we utilize the Pres- 615
ResNet18 model, trained for 300 epochs with a batch size of 128 and a learning rate of 0.02, and a 616
cosine annealing schedule was implemented. For the WebVision dataset, we utilize the ResNet18 617
model, training for 120 epochs with a reduced batch size of 32. The learning rate is set to 0.01 and 618
controlled by a cosine annealing scheduler too. Additionally, a warm-up training phase of 10 epochs 619
is implemented in the CIFAR100-O and ImageNet-O experiments, while a 5-epoch warm-up training 620
phase is utilized in the WebVision experiment. 621
Hyperparameters In all experiments, we set the sample selection threshold θ′= 0.5for EntSel. 622
For SSR, we employ a sample selection threshold θ= 1.0in all experiments. For DivideMix, 623
the sample selection threshold remains constant at θ= 0.5across all experiments. Both SSR and 624
DivideMix incorporate MixUp, and we adhere to the original paper’s choices by setting the MixUp 625
coefficient to 4 for experiments on CIFAR100-O and ImageNet-O and to 0.5 for experiments on 626
WebVision. Please note, as exploring and comparing these methods are not our focus, we believe 627
there exist better hyperparameter settings. 628
20Robustness of EntSel A smaller θ′for EntSel leads to better performance on WebVision - especially 629
when EntSel is used with DivideMix. If we set θ′= 0.2, our classification accuracy increases from 630
62.96% to 67.2%, ROC AUC increases from 0.8166 to 0.8599 (table 1). However, we are keen to use 631
fixed hyperparameters in all experiments as we emphasize that the hyperparameter robustness is also 632
critical for LNL methods. 633
F More examples of open-set noise in WebVision dataset 634
In this section, we present additional examples of open-set noise within the ‘Tench’ class of the 635
WebVision dataset. We trace the origin of web pages containing some open-set noise images. 636
Remarkably, we identify that the appearance of the term ‘Tench’ or related keywords is prevalent 637
on the web pages hosting these open-set noise images. We posit that this occurrence is attributed 638
to the data collection process on the web. Specifically, in the course of keyword searches and 639
crawling for images, instances were inadvertently included due to the presence of keywords in image 640
descriptions or accompanying text, such as people with ‘tench’ in the name, or related fishing tools. 641
As highlighted earlier, the prevalent belief in the current LNL community is that real-world noise 642
primarily arises from confusion induced by semantic similarity. Consequently, numerous recent 643
studies have concentrated on instance-dependent noise and related theoretical analysis. However, our 644
findings here indicate that in real-world scenarios, particularly in web-crawled datasets, noise may 645
be unrelated to semantics but instead caused by other latent high-dimensional information, such 646
as accompanying text here. Addressing such real-world noise requires increased attention and 647
further exploration.
0lv77S5PaW5vlM.jpg
 http://ukscblog.com/ussc-v-uksc/
fIfzvcQnea4zsM.jpg
m59l4cCfDgox6M.jpg
X051MdLtrztWqM.jpg
usK0NyB0Q9VsVM.jpg
https://charlyanderic.travellerspoint.com/67/https://www.drennantackle.com/drennan-distance-specialist-tench-bream-12ft-2lb-rod/
https://en.wikipedia.org/wiki/You_Should_Be_So_Lucky
http://pete777-pete777.blogspot.com/2012/04/tench-rigs-27th-april.htmlImage Source
Figure 7: Open-set noise examples in class ‘Tench’ of WebVision dataset with path:
/google/q0001/ . The source images are resized to fit the layout. Please note that the web links
here are obtained in May 2024, and there is no guarantee that they will always be valid in the future.
648
21NeurIPS Paper Checklist 649
1.Claims 650
Question: Do the main claims made in the abstract and introduction accurately reflect the 651
paper’s contributions and scope? 652
Answer: [Yes] 653
Justification: We clearly and briefly describe our method and our contributions in the abstract 654
and introduction sections. 655
Guidelines: 656
•The answer NA means that the abstract and introduction do not include the claims 657
made in the paper. 658
•The abstract and/or introduction should clearly state the claims made, including the 659
contributions made in the paper and important assumptions and limitations. A No or 660
NA answer to this question will not be perceived well by the reviewers. 661
•The claims made should match theoretical and experimental results, and reflect how 662
much the results can be expected to generalize to other settings. 663
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 664
are not attained by the paper. 665
2.Limitations 666
Question: Does the paper discuss the limitations of the work performed by the authors? 667
Answer: [Yes] 668
Justification: In section 5 We specifically discuss the potential and limitations of current 669
LNL method in learning with open-set noise. 670
Guidelines: 671
•The answer NA means that the paper has no limitation while the answer No means that 672
the paper has limitations, but those are not discussed in the paper. 673
• The authors are encouraged to create a separate "Limitations" section in their paper. 674
•The paper should point out any strong assumptions and how robust the results are to 675
violations of these assumptions (e.g., independence assumptions, noiseless settings, 676
model well-specification, asymptotic approximations only holding locally). The authors 677
should reflect on how these assumptions might be violated in practice and what the 678
implications would be. 679
•The authors should reflect on the scope of the claims made, e.g., if the approach was 680
only tested on a few datasets or with a few runs. In general, empirical results often 681
depend on implicit assumptions, which should be articulated. 682
•The authors should reflect on the factors that influence the performance of the approach. 683
For example, a facial recognition algorithm may perform poorly when image resolution 684
is low or images are taken in low lighting. Or a speech-to-text system might not be 685
used reliably to provide closed captions for online lectures because it fails to handle 686
technical jargon. 687
•The authors should discuss the computational efficiency of the proposed algorithms 688
and how they scale with dataset size. 689
•If applicable, the authors should discuss possible limitations of their approach to 690
address problems of privacy and fairness. 691
•While the authors might fear that complete honesty about limitations might be used by 692
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 693
limitations that aren’t acknowledged in the paper. The authors should use their best 694
judgment and recognize that individual actions in favor of transparency play an impor- 695
tant role in developing norms that preserve the integrity of the community. Reviewers 696
will be specifically instructed to not penalize honesty concerning limitations. 697
3.Theory Assumptions and Proofs 698
Question: For each theoretical result, does the paper provide the full set of assumptions and 699
a complete (and correct) proof? 700
22Answer: [Yes] 701
Justification: We conduct theoretical analysis on the impact of open-set noise and provided 702
a complete proof in appendix D. 703
Guidelines: 704
• The answer NA means that the paper does not include theoretical results. 705
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 706
referenced. 707
•All assumptions should be clearly stated or referenced in the statement of any theorems. 708
•The proofs can either appear in the main paper or the supplemental material, but if 709
they appear in the supplemental material, the authors are encouraged to provide a short 710
proof sketch to provide intuition. 711
•Inversely, any informal proof provided in the core of the paper should be complemented 712
by formal proofs provided in appendix or supplemental material. 713
• Theorems and Lemmas that the proof relies upon should be properly referenced. 714
4.Experimental Result Reproducibility 715
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 716
perimental results of the paper to the extent that it affects the main claims and/or conclusions 717
of the paper (regardless of whether the code and data are provided or not)? 718
Answer: [Yes] 719
Justification: All the dataset and implementation details are included in appendix A. 720
Guidelines: 721
• The answer NA means that the paper does not include experiments. 722
•If the paper includes experiments, a No answer to this question will not be perceived 723
well by the reviewers: Making the paper reproducible is important, regardless of 724
whether the code and data are provided or not. 725
•If the contribution is a dataset and/or model, the authors should describe the steps taken 726
to make their results reproducible or verifiable. 727
•Depending on the contribution, reproducibility can be accomplished in various ways. 728
For example, if the contribution is a novel architecture, describing the architecture fully 729
might suffice, or if the contribution is a specific model and empirical evaluation, it may 730
be necessary to either make it possible for others to replicate the model with the same 731
dataset, or provide access to the model. In general. releasing code and data is often 732
one good way to accomplish this, but reproducibility can also be provided via detailed 733
instructions for how to replicate the results, access to a hosted model (e.g., in the case 734
of a large language model), releasing of a model checkpoint, or other means that are 735
appropriate to the research performed. 736
•While NeurIPS does not require releasing code, the conference does require all submis- 737
sions to provide some reasonable avenue for reproducibility, which may depend on the 738
nature of the contribution. For example 739
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 740
to reproduce that algorithm. 741
(b)If the contribution is primarily a new model architecture, the paper should describe 742
the architecture clearly and fully. 743
(c)If the contribution is a new model (e.g., a large language model), then there should 744
either be a way to access this model for reproducing the results or a way to reproduce 745
the model (e.g., with an open-source dataset or instructions for how to construct 746
the dataset). 747
(d)We recognize that reproducibility may be tricky in some cases, in which case 748
authors are welcome to describe the particular way they provide for reproducibility. 749
In the case of closed-source models, it may be that access to the model is limited in 750
some way (e.g., to registered users), but it should be possible for other researchers 751
to have some path to reproducing or verifying the results. 752
5.Open access to data and code 753
23Question: Does the paper provide open access to the data and code, with sufficient instruc- 754
tions to faithfully reproduce the main experimental results, as described in supplemental 755
material? 756
Answer: [No] 757
Justification: The complete codes will be released upon acceptance. 758
Guidelines: 759
• The answer NA means that paper does not include experiments requiring code. 760
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 761
public/guides/CodeSubmissionPolicy ) for more details. 762
•While we encourage the release of code and data, we understand that this might not be 763
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 764
including code, unless this is central to the contribution (e.g., for a new open-source 765
benchmark). 766
•The instructions should contain the exact command and environment needed to run to 767
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 768
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 769
•The authors should provide instructions on data access and preparation, including how 770
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 771
•The authors should provide scripts to reproduce all experimental results for the new 772
proposed method and baselines. If only a subset of experiments are reproducible, they 773
should state which ones are omitted from the script and why. 774
•At submission time, to preserve anonymity, the authors should release anonymized 775
versions (if applicable). 776
•Providing as much information as possible in supplemental material (appended to the 777
paper) is recommended, but including URLs to data and code is permitted. 778
6.Experimental Setting/Details 779
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 780
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 781
results? 782
Answer: [Yes] 783
Justification: All the dataset and implementation details are included in appendix A. 784
Guidelines: 785
• The answer NA means that the paper does not include experiments. 786
•The experimental setting should be presented in the core of the paper to a level of detail 787
that is necessary to appreciate the results and make sense of them. 788
•The full details can be provided either with the code, in appendix, or as supplemental 789
material. 790
7.Experiment Statistical Significance 791
Question: Does the paper report error bars suitably and correctly defined or other appropriate 792
information about the statistical significance of the experiments? 793
Answer: [Yes] 794
Justification: We conduct multiple runs and report averaged results in most experiments. 795
Guidelines: 796
• The answer NA means that the paper does not include experiments. 797
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 798
dence intervals, or statistical significance tests, at least for the experiments that support 799
the main claims of the paper. 800
•The factors of variability that the error bars are capturing should be clearly stated (for 801
example, train/test split, initialization, random drawing of some parameter, or overall 802
run with given experimental conditions). 803
•The method for calculating the error bars should be explained (closed form formula, 804
call to a library function, bootstrap, etc.) 805
24• The assumptions made should be given (e.g., Normally distributed errors). 806
•It should be clear whether the error bar is the standard deviation or the standard error 807
of the mean. 808
•It is OK to report 1-sigma error bars, but one should state it. The authors should 809
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 810
of Normality of errors is not verified. 811
•For asymmetric distributions, the authors should be careful not to show in tables or 812
figures symmetric error bars that would yield results that are out of range (e.g. negative 813
error rates). 814
•If error bars are reported in tables or plots, The authors should explain in the text how 815
they were calculated and reference the corresponding figures or tables in the text. 816
8.Experiments Compute Resources 817
Question: For each experiment, does the paper provide sufficient information on the com- 818
puter resources (type of compute workers, memory, time of execution) needed to reproduce 819
the experiments? 820
Answer: [Yes] 821
Justification: All experiments are conducted on a private server with 3 RX6000 GPUs. 822
Guidelines: 823
• The answer NA means that the paper does not include experiments. 824
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 825
or cloud provider, including relevant memory and storage. 826
•The paper should provide the amount of compute required for each of the individual 827
experimental runs as well as estimate the total compute. 828
•The paper should disclose whether the full research project required more compute 829
than the experiments reported in the paper (e.g., preliminary or failed experiments that 830
didn’t make it into the paper). 831
9.Code Of Ethics 832
Question: Does the research conducted in the paper conform, in every respect, with the 833
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 834
Answer: [Yes] 835
Justification: We confirm that the conducted reserach conform with the NeurIPS Code of 836
Ethics. 837
Guidelines: 838
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 839
•If the authors answer No, they should explain the special circumstances that require a 840
deviation from the Code of Ethics. 841
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 842
eration due to laws or regulations in their jurisdiction). 843
10.Broader Impacts 844
Question: Does the paper discuss both potential positive societal impacts and negative 845
societal impacts of the work performed? 846
Answer: [NA] 847
Justification: Not applicable. 848
Guidelines: 849
• The answer NA means that there is no societal impact of the work performed. 850
•If the authors answer NA or No, they should explain why their work has no societal 851
impact or why the paper does not address societal impact. 852
•Examples of negative societal impacts include potential malicious or unintended uses 853
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 854
(e.g., deployment of technologies that could make decisions that unfairly impact specific 855
groups), privacy considerations, and security considerations. 856
25•The conference expects that many papers will be foundational research and not tied 857
to particular applications, let alone deployments. However, if there is a direct path to 858
any negative applications, the authors should point it out. For example, it is legitimate 859
to point out that an improvement in the quality of generative models could be used to 860
generate deepfakes for disinformation. On the other hand, it is not needed to point out 861
that a generic algorithm for optimizing neural networks could enable people to train 862
models that generate Deepfakes faster. 863
•The authors should consider possible harms that could arise when the technology is 864
being used as intended and functioning correctly, harms that could arise when the 865
technology is being used as intended but gives incorrect results, and harms following 866
from (intentional or unintentional) misuse of the technology. 867
•If there are negative societal impacts, the authors could also discuss possible mitigation 868
strategies (e.g., gated release of models, providing defenses in addition to attacks, 869
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 870
feedback over time, improving the efficiency and accessibility of ML). 871
11.Safeguards 872
Question: Does the paper describe safeguards that have been put in place for responsible 873
release of data or models that have a high risk for misuse (e.g., pretrained language models, 874
image generators, or scraped datasets)? 875
Answer: [NA] 876
Justification: Not applicable. 877
Guidelines: 878
• The answer NA means that the paper poses no such risks. 879
•Released models that have a high risk for misuse or dual-use should be released with 880
necessary safeguards to allow for controlled use of the model, for example by requiring 881
that users adhere to usage guidelines or restrictions to access the model or implementing 882
safety filters. 883
•Datasets that have been scraped from the Internet could pose safety risks. The authors 884
should describe how they avoided releasing unsafe images. 885
•We recognize that providing effective safeguards is challenging, and many papers do 886
not require this, but we encourage authors to take this into account and make a best 887
faith effort. 888
12.Licenses for existing assets 889
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 890
the paper, properly credited and are the license and terms of use explicitly mentioned and 891
properly respected? 892
Answer: [Yes] 893
Justification: We include all essential information and references to the used datasets in this 894
work. 895
Guidelines: 896
• The answer NA means that the paper does not use existing assets. 897
• The authors should cite the original paper that produced the code package or dataset. 898
•The authors should state which version of the asset is used and, if possible, include a 899
URL. 900
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 901
•For scraped data from a particular source (e.g., website), the copyright and terms of 902
service of that source should be provided. 903
•If assets are released, the license, copyright information, and terms of use in the 904
package should be provided. For popular datasets, paperswithcode.com/datasets 905
has curated licenses for some datasets. Their licensing guide can help determine the 906
license of a dataset. 907
•For existing datasets that are re-packaged, both the original license and the license of 908
the derived asset (if it has changed) should be provided. 909
26•If this information is not available online, the authors are encouraged to reach out to 910
the asset’s creators. 911
13.New Assets 912
Question: Are new assets introduced in the paper well documented and is the documentation 913
provided alongside the assets? 914
Answer: [NA] 915
Justification: Not applicable. 916
Guidelines: 917
• The answer NA means that the paper does not release new assets. 918
•Researchers should communicate the details of the dataset/code/model as part of their 919
submissions via structured templates. This includes details about training, license, 920
limitations, etc. 921
•The paper should discuss whether and how consent was obtained from people whose 922
asset is used. 923
•At submission time, remember to anonymize your assets (if applicable). You can either 924
create an anonymized URL or include an anonymized zip file. 925
14.Crowdsourcing and Research with Human Subjects 926
Question: For crowdsourcing experiments and research with human subjects, does the paper 927
include the full text of instructions given to participants and screenshots, if applicable, as 928
well as details about compensation (if any)? 929
Answer: [NA] 930
Justification: Not applicable. 931
Guidelines: 932
•The answer NA means that the paper does not involve crowdsourcing nor research with 933
human subjects. 934
•Including this information in the supplemental material is fine, but if the main contribu- 935
tion of the paper involves human subjects, then as much detail as possible should be 936
included in the main paper. 937
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 938
or other labor should be paid at least the minimum wage in the country of the data 939
collector. 940
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 941
Subjects 942
Question: Does the paper describe potential risks incurred by study participants, whether 943
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 944
approvals (or an equivalent approval/review based on the requirements of your country or 945
institution) were obtained? 946
Answer: [NA] 947
Justification: Not applicable. 948
Guidelines: 949
•The answer NA means that the paper does not involve crowdsourcing nor research with 950
human subjects. 951
•Depending on the country in which research is conducted, IRB approval (or equivalent) 952
may be required for any human subjects research. If you obtained IRB approval, you 953
should clearly state this in the paper. 954
•We recognize that the procedures for this may vary significantly between institutions 955
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 956
guidelines for their institution. 957
•For initial submissions, do not include any information that would break anonymity (if 958
applicable), such as the institution conducting the review. 959
27