Interpretable Lightweight Transformer via Unrolling
of Learned Graph Smoothness Priors
Tam Thuc Do
York University
Toronto, Canada
dtamthuc@yorku.caParham Eftekhar
York University
Toronto, Canada
eftekhar@yorku.caSeyed Alireza Hosseini
York University
Toronto, Canada
ahoseini@yorku.ca
Gene Cheung
York University
Toronto, Canada
genec@yorku.caPhilip A. Chou
packet.media
Seattle, USA
pachou@ieee.org
Abstract
We build interpretable and lightweight transformer-like neural networks by un-
rolling iterative optimization algorithms that minimize graph smoothness priors—
the quadratic graph Laplacian regularizer (GLR) and the ℓ1-norm graph total
variation (GTV)—subject to an interpolation constraint. The crucial insight is that
a normalized signal-dependent graph learning module amounts to a variant of the
basic self-attention mechanism in conventional transformers. Unlike “black-box”
transformers that require learning of large key, query and value matrices to compute
scaled dot products as affinities and subsequent output embeddings, resulting in
huge parameter sets, our unrolled networks employ shallow CNNs to learn low-
dimensional features per node to establish pairwise Mahalanobis distances and
construct sparse similarity graphs. At each layer, given a learned graph, the target
interpolated signal is simply a low-pass filtered output derived from the minimiza-
tion of an assumed graph smoothness prior, leading to a dramatic reduction in
parameter count. Experiments for two image interpolation applications verify the
restoration performance, parameter efficiency and robustness to covariate shift of
our graph-based unrolled networks compared to conventional transformers.
1 Introduction
Focusing on the self-attention mechanism [ 1] as the basic building block—where the affinity between
two input tokens is computed as a transformed dot product— transformers [2] learn large parameter
sets to achieve state-of-the-art (SOTA) performance in a wide range of signal prediction/classification
problems [ 3,4], outperforming convolutional neural nets (CNNs) and recurrent neural nets (RNNs).
However, there are shortcomings: i) lack of mathematical interpretability to characterize general
performance1, ii) requiring substantial training datasets to train sizable parameters [ 12], and iii)
fragility to covariate shift —when training and testing data have different distributions [13].
Orthogonally, algorithm unrolling [14] implements iterations of a model-based algorithm as a
sequence of neural layers to build a feed-forward network, whose parameters can be learned end-
to-end via back-propagation from data. A classic example is the unrolling of the iterative soft-
1While works exist to analyze existing transformer architectures [ 5,6,7,8,9], only [ 10,11] characterized
the performance of a single self-attention layer and a shallow transformer, respectively. In contrast, we build
transformer-like networks by unrolling graph-based algorithms, so that each layer is interpretable by construction.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).thresholding algorithm (ISTA) in sparse coding into Learned ISTA (LISTA) [ 15]. Recently, [ 16]
showed that by unrolling an iterative algorithm minimizing a sparse rate reduction (SRR) objective, it
can lead to a family of “white-box” transformer-like deep neural nets that are 100% mathematically
interpretable. Inspired by [ 16], in this work we also seek to build white-box transformers via algorithm
unrolling, but from a unique graph signal processing (GSP) perspective [17, 18, 19].
Over the last decade and a half, GSP studies spectral analysis and processing of discrete signals
on structured data kernels described by finite graphs. Specifically, by assuming that the sought
signal is smooth (low-pass) with respect to (w.r.t.) a particular graph, a plethora of graph-based
restoration algorithms can be designed for practical applications, including image denoising [ 20],
JPEG dequantization [ 21], interpolation [ 22], 3D point cloud denoising [ 23,24] and super-resolution
[25]. At the heart of GSP is the construction of a similarity graph that captures pairwise similarities
between signal samples on two connected nodes. We first demonstrate that signal-dependent similarity
graph learning with normalization is akin to affinity computation in the self-attention mechanism .
Thus, our first contribution is to show that unrolling of a graph-based iterative algorithm2with
normalized graph learning results in an interpretable transformer-like feed-forward network .
Second, computation of a positive weight wi,j= exp( −d(i, j))of an edge (i, j)connecting two
graph nodes iandjoften employs Mahalanobis distance d(i, j) = (fi−fj)⊤M(fi−fj)between
representative ( e.g., CNN-computed) feature vectors fiandfj, where fi,fj∈RDreside in low-
dimensional space [ 27,28]. Hence, unlike a conventional transformer that requires large key and
query matrices, KandQ, to compute transformed dot products, the similarity graph learning module
can be more parameter-efficient. Moreover, by adding a graph smoothness prior such as graph
Laplacian regularizer (GLR) [ 18,20] orgraph total variation (GTV) [ 29,30,31] in the optimization
objective, once a graph Gis learned, the target signal is simply computed as the low-pass filtered
output derived from the minimization of the assumed graph smoothness prior. Thus, a large value
matrix Vto compute output embeddings typical in a transformer is also not needed. Our second
contribution is to demonstrate that a lightweight transformer with fewer parameters can be built via
unrolling of a graph-based restoration algorithm with a chosen graph signal smoothness prior .
Specifically, focusing on the signal interpolation problem, we first derive linear-time graph-based
algorithms by minimizing GLR or GTV , via conjugate gradient (CG) [ 32] or a modern adaptation of
alternative method of multipliers (ADMM) for sparse linear programming (SLP) [ 33], respectively.
In each iteration, given a learned graph, each algorithm deploys a low-pass graph filter to interpolate
the up-sampled observation vector into the target signal. We intersperse unrolled algorithm iterations
with graph learning modules into a compact and interpretable neural network. We demonstrate its
restoration performance, parameter efficiency ( 3%of SOTA’s parameters in one case), and robustness
to covariate shift for two practical applications: image demosaicking, and image interpolation.
Notation: Vectors and matrices are written in bold lowercase and uppercase letters, respectively. The
(i, j)element and the j-th column of a matrix Aare denoted by Ai,jandaj, respectively. The i-th
element in the vector ais denoted by ai. The square identity matrix of rank Nis denoted by IN, the
M-by-Nzero matrix is denoted by 0M,N, and the vector of all ones / zeros of length Nis denoted
by1N/0N, respectively. Operator ∥ · ∥pdenotes the ℓ-pnorm.
2 Preliminaries
2.1 GSP Definitions
A graph G(N,E,W)is defined by a node set N={1, . . . , N }and an edge set Eof size |E|=M,
where (i, j)∈ Emeans nodes i, j∈ N are connected with weight wi,j=Wi,j∈R. In this paper, we
consider only positive graphs Gwith no self-loops, i.e.,wi,j≥0,∀i, j, andwi,i= 0,∀i. We assume
edges are undirected, and thus adjacency matrix W∈RN×Nis symmetric. The combinatorial
graph Laplacian matrix is defined as L≜D−W∈RN×N, where D≜diag(W1N)is the degree
matrix , and diag(v)returns a diagonal matrix with valong its diagonal. Lfor a positive graph Gis
provably positive semi-definite (PSD), i.e., all its eigenvalues λi’s are non-negative [19].
2While there exist works on unrolling of graph-based algorithms [ 26], they typically assume a fixed graph
and optimize other parameters. In contrast, the key point in our work is that the normalized graph learning
module is akin to the basic self-attention mechanism in conventional transformers.
2We define also the incidence matrix C=RM×N: each k-th row of Ccorresponds to the k-th edge
(i, j)∈ E, where Ck,i=wi,j,Ck,j=−wi,j, andCk,l= 0,∀l̸=i, j. Since our assumed graph Gis
undirected, the polarities of Ck,iandCk,jare arbitrary, as long as they are opposite.
2.2 Graph Laplacian Regularizer
Given a positive connected graphGwithNnodes and Medges, we first define smoothness of a
signal x∈RNw.r.t.Gusing the graph Laplacian regularizer (GLR) [18, 20] as
∥x∥G,2=x⊤Lx=X
(i,j)∈Ewi,j(xi−xj)2(1)
where Lis a combinatorial Laplacian matrix specifying graph G. GLR (1)is non-negative for a
positive graph, and thus is suitable as a signal prior for minimization problems [20, 21].
2.3 Graph Total Variation
Instead of GLR (1), we can alternatively define graph signal smoothness using graph total variation
(GTV) [29, 30, 31] ∥x∥G,1for signal x∈RNas
∥x∥G,1=∥Cx∥1(a)=X
(i,j)∈Ewi,j|xi−xj| (2)
where (a)is true since Gis positive. GTV is also non-negative for positive G, and has been used as a
signal prior for restoration problems such as image deblurring [34].
3 Problem Formulation & Optimization using GLR
3.1 Problem Formulation
We first assume a positive, sparse andconnected graphGwithNnodes and Medges specified by
graph Laplacian matrix L. By sparse, we mean that MisO(N)and not O(N2). By connected, we
mean that any node jcan be traversed from any other node i. Given G, we first derive a linear-time
iterative algorithm to interpolate signal xby minimizing GLR given observed samples y. In Section 4,
we derive an algorithm by minimizing GTV instead given y. In Section 5, we unroll iterations of
one of two derived algorithms into neural layers, together with strategically inserted graph learning
modules, to construct graph-based lightweight transformer-like neural nets.
We first employ GLR [ 20] as the objective to reconstruct x∈RNgiven partial observation y∈RK,
where K < N . Denote by H∈ {0,1}K×Nasampling matrix defined as
Hi,j=
1if node jis the i-th sample
0o.w.(3)
that picks out Ksamples from signal x. The optimization is thus
min
xx⊤Lx, s.t.Hx=y (4)
where L∈RN×Nis a graph Laplacian matrix corresponding to a positive graph G[19]. PSD L
implies that x⊤Lx≥0,∀x, and thus (4)has a convex objective with a linear interpolation constraint.
3.2 Optimization
We solve (4)via a standard Lagrangian approach [ 35] and write its corresponding unconstrained
Lagrangian function f(x,µ)as
f(x,µ) =x⊤Lx+µ⊤(Hx−y) (5)
where µ∈RKis the Lagrange multiplier vector. To minimize f(x,µ)in(5), we take the derivative
w.r.t.xandµseparately and set them to zero, resulting in the following linear system:
2L H⊤
H 0 K,K
|{z }
P
x
µ
=
0N,N
y
. (6)
3Given that the underlying graph Gis positive and connected, coefficient matrix Pis provably full-rank
and thus invertible (see Appendix A.1 for a proof). Hence, (6) has a unique solution x∗.
Suppose we index the sampled nodes Sinxbefore the non-sampled nodes ¯S,i.e.,x= [xS;x¯S].
ThenH= [IK0K,N−K], and the second block row in (6)implies xS=y. Suppose we write
L= [LS,SLS,¯S;L¯S,SL¯S,¯S]in blocks also. For the first block row in (6), consider only the
non-sampled rows:

2L H⊤
x
µ
¯S= 2(L¯S,SxS+L¯S,¯Sx¯S) =0N−K (7)
where 
H⊤µ
¯S=0N−Ksince the non-sampled rows of H⊤(the non-sampled columns of H) are
zeros. Thus, x¯Scan be computed via the following system of linear equations:
L¯S,¯Sx¯S=−L¯S,Sy (8)
where L¯S,¯Sis a symmetric, sparse, and provably positive definite (PD) matrix (see Appendix A.2 for
a proof). Thus, there exists a unique solution x¯Sin (8).
Complexity : For notation simplicity, let L=L¯S,¯S. Linear system (8)can be solved efficiently using
conjugate gradient (CG), an iterative descent algorithm with complexity O(nnz(L)p
κ(L)/log(ϵ)),
where nnz(L)is the number of non-zero entries in matrix L,κ(L) =λmax(L)
λmin(L)is the condition
number ofL,λmax(L)andλmin(L)are the respective largest and smallest eigenvalues of L, and ϵ
is the convergence threshold of the gradient search [ 32]. Because Lis sparse by graph construction
(O(N)edges), L=L¯S,¯Sis also sparse, i.e.,nnz(L) =O(N). Assuming κ(L)can be reasonably
lower-bounded for PD Landϵis reasonably chosen, the complexity of solving (8)using CG is O(N).
Interpretation : To elicit a signal filtering interpretation from (6), we assume for now that Lis PD3
and thus invertible. Recall that the block matrix inversion formula [36] is
P−1=
A B
C D−1
=
A−1+A−1B(P/A)CA−1−A−1B(P/A)
−(P/A)CA−1(P/A)
(9)
where P/A= (D−CA−1B)−1is the Schur complement of block Aof matrix P. Solution x∗can
thus be computed as:
x∗=L−1H⊤ 
HL−1H⊤−1y
=L−1H⊤ 
(L−1)S−1y=L−1H⊤L#
Sy (10)
where L#
S≜ 
(L−1)S−1and(L−1)Sdenotes the rows and columns of L−1corresponding to the
sampled nodes. L#
Sis a high-pass filter similar to LS. Thus, we can interpret x∗as alow-pass filtered
output of up-sampled H⊤L#
Sy—with low-pass filter response r(Λ) =Λ−1where L=VΛV⊤,
Λ=diag([λ1, . . . , λ N]), is eigen-decomposible with frequencies λk’s and Fourier modes vk’s.
4 Problem Formulation & Optimization using GTV
4.1 Problem Formulation
Given a positive connected graph Gspecified by incidence matrix C∈RM×Nand partial observation
y, we now employ instead GTV as the objective to interpolate target signal x, resulting in
min
x∥Cx∥1, s.t.Hx=y, (11)
(11) is alinear program (LP), since both the objective and the lone constraint are linear. Thus, while
minimizing GLR leads to a linear system (6), minimizing GTV leads to a linear program (11).
3A combinatorial graph Laplacian matrix for a positive graph with at least one additional positive self-loop is
provably PD; see proof in Appendix A.2.
44.1.1 LP in Standard Form
First, we rewrite LP (11) in standard form as follows. Define upper-bound variable z∈RMwith a
pair of linear constraints z≥ ±Cx. This enables a linear objective 1⊤
Mzfor a minimization problem
(thus ensuring the upper bound is tight), i.e.,z=∥Cx∥1. Second, we introduce non-negative
slack variables q1,q2∈RMto convert inequality constraints z≥ ±Cxto equality constraints
z=Cx+q1andz=−Cx+q2. Thus, LP (11) can be rewritten as
min
z,x,q1⊤
Mz,s.t."IM−C−(IM0M,M)
IM C−(0M,MIM)
0K,M H 0 K,2M#
| {z }
A"z
x
q#
="0M
0M
y#
|{z}
b,q≥02M (12)
where q= [q1;q2]∈R2M.
4.2 Optimization Algorithm
Because coefficient matrix Ais sparse, (12) is a sparse linear program (SLP). We solve SLP
(12) efficiently by adopting an ADMM approach for SLP in [ 33]. We first define a convex but
non-differentiable (non-smooth) indicator function :
g(q) =
0 ifqj≥0,∀j
∞ o.w.. (13)
We next introduce auxiliary variable ˜q∈R2Mand equality constraint ˜q=q. We now rewrite (12)
with a single equality constraint as
min
z,x,q,˜q1⊤
Mz+g(˜q),s.t.
A
02M,M +NI2M
| {z }
B"z
x
q#
=
b
˜q
. (14)
We can now rewrite (14) into an unconstrained version using the augmented Lagrangian method as
min
z,x,q,˜q1⊤
Mz+g(˜q) +µ⊤ 
B"z
x
q#
−
b
˜q!
+γ
2B"z
x
q#
−
b
˜q2
2(15)
where µ∈R4M+Kis a Lagrange multiplier vector, and γ >0is a scalar parameter. In the sequel,
we write µ= [µa;µb;µc;µd;µe], where µa,µb,µd,µe∈RMandµc∈RK.
4.2.1 Optimizing Main Variables
As typically done in ADMM, we minimize the unconstrained objective (15) alternately as follows.
At iteration t, when ˜qtandµtare fixed, the optimization for zt+1,xt+1andqt+1becomes
min
z,x,q1⊤
Mz+ (µt)⊤ 
B"z
x
q#
−
b
˜q!
+γ
2B"z
x
q#
−
b
˜q2
2. (16)
The solution to this convex and smooth quadratic optimization is a system of linear equations,
zt+1=−1
γ1M−1
2γ 
µt
a+µt
b+µt
d+µt
e
+1
2(˜qt
1+˜qt
2) (17)
(C⊤C+H⊤H)xt+1=1
2γC⊤ 
µt
a−µt
b+µt
d−µt
e
−1
γH⊤µt
c−1
2C⊤(˜qt
1−˜qt
2) +H⊤y (18)
qt
1=1
2 
zt+1−Cxt+1
+1
2γ(µt
a−µt
d+γ˜qt
1)
qt
2=1
2 
zt+1+Cxt+1
+1
2γ(µt
b−µt
e+γ˜qt
2) (19)
See the Appendix A.3 for a derivation.
5Linear system (18) is solvable if the coefficient matrix L≜L+H⊤H, where L≜C⊤Cis a PSD
graph Laplacian for a positive graph, is invertible. See Appendix A.4 for a proof that Lis PD and
thus invertible .
Complexity : Linear system (18) again can be solved efficiently using CG with complexity
O(nnz(L)p
κ(L)/log(ϵ))[32]. Because Cis sparse by graph construction ( O(N)edges) and
His sparse by definition, Lis also sparse, i.e.,nnz(L) =O(N). Thus, assuming κ(L)is also
upper-bounded and ϵis reasonably chosen, the complexity of solving (18) using CG is O(N).
Interpretation :(18) can be interpreted as follows. H⊤yis the up-sampled version of observation
y.L=VΛV⊤,Λ = diag([λ1, . . . , λ N]), is an eigen-decomposible generalized Laplacian matrix —
Laplacian matrix C⊤Cplus self-loops of weight 1at sampled nodes due to diagonal matrix H⊤H,
and like a graph Laplacian Lwithout self-loops, can be interpreted as a high-pass spectral filter
[37].L−1=VΛ−1V⊤is thus a low-pass spectral filter with frequency response r(Λ) =Λ−1to
interpolate output L−1H⊤y. We interpret the remaining terms on the right-hand side of (18) as bias.
4.2.2 Optimizing Auxiliary Variable
Fixing zt+1,xt+1andqt+1, the optimization for ˜qt+1for (15) simplifies to
min
˜qg(˜q) + (µt
d)⊤ 
qt+1−˜q
+γ
2qt+1−˜q2
2. (20)
The solution for optimal ˜qt+1is term-by-term thresholding:
˜qt+1
i=
qt+1
i+1
γµt
d,i ifqt+1
i+1
γµt
d,i≥0
0 o.w.,∀i. (21)
See Appendix A.5 for a derivation.
4.2.3 Updating Lagrange Multiplier
The Lagrange multiplier µt+1can be updated in the usual manner in an ADMM framework [38]:
µt+1=µt+γ 
B"z
x
q#
−
b
˜q!
. (22)
Algorithm Complexity : Given that the number of iterations until ADMM convergence is not a
function of input size, it is O(1). The most time-consuming step in each ADMM iteration is the
solving of linear system (18) via CG in O(N). Thus, we conclude that solving SLP (12) using the
aforementioned ADMM algorithm is O(N).
Algorithm Comparison : Comparing the CG algorithm used to solve linear system (8)and the
ADMM algorithm developed to solve SLP (12), we first observe that, given a similarity graph G
specified by Laplacian or incidence matrix, LorC, both algorithms compute the interpolated signal
x∗as a low-pass filtered output of the up-sampled input H⊤yin (10) and (18), respectively. This is
intuitive, given the assumed graph smoothness priors, GLR and GTV . We see also that the ADMM
algorithm is more intricate: in each iteration, the main variables are computed using CG, while the
auxiliary variable is updated via ReLU-like thresholding. As a result, the ADMM algorithm is more
amenable to deep algorithm unrolling with better performance in general (see Section 6 for details).
5 Graph Learning & Algorithm Unrolling
We now discuss how a similarity graph Gcan be learned from data, specified by graph Laplacian
Lfor GLR minimization (4)or incidence matrix Cfor GTV minimization (11), so that the two
proposed graph-based interpolations can take place. Moreover, we show how a normalized graph
learning module4performs comparable operations to the self-attention mechanism in conventional
transformers. Thus, unrolling sequential pairs of graph-based iterative algorithm and graph learning
module back-to-back leads to an interpretable “white-box” transformer-like neural net.
4While estimating a precision (graph Laplacian) matrix from an empirical covariance matrix computed from
data is another graph learning approach [39, 40, 41], we pursue a feature-based approach here [27, 28].
65.1 Self-Attention Operator in Transformer
We first review the self-attention operator in a conventional transformer architecture, defined using
a transformed dot product and a softmax operation [ 1]. Specifically, first denote by xi∈REan
embedding for token iofNtokens. Affinity e(i, j)between tokens iandjis defined as the dot product
between linear-transformed embeddings KxiandQxj, where Q,K∈RE×Eare the query andkey
matrices, respectively. Using softmax, a non-linear function that maps a vector of real numbers to a
vector of positive numbers that sum to 1,attention weight ai,jis computed as
ai,j=exp(e(i, j))PN
l=1exp(e(i, l)), e (i, j) = (Qxj)⊤(Kxi). (23)
Given self-attention weights ai,j, output embedding yifor token iis computed as
yi=NX
l=1ai,lxlV (24)
where V∈RE×Eis avalue matrix. “Self-attention” here means that input embeddings are weighted
to compute output embeddings. A transformer is thus a sequence of embedding-to-embedding
mappings via different learned self-attention operations defined by Q,KandVmatrices. Multi-head
attention is possible when multiple query and key matrices Q(m)andK(m)are used to compute
different attention weights a(m)
i,j’s for the same input embeddings xiandxj, and the output embedding
yiis computed using an average of these multi-head attention weights a(m)
i,l’s.
5.2 Computation of Graph Edge Weights
Consider now how edge weights wi,j’s can be computed from data to specify a finite graph G[27,28].
A low-dimensional feature vector fi∈RDcan be computed for each node ifrom embedding
xi∈REvia some (possibly non-linear) function fi=F(xi), where typically D≪E. Edge weight
wi,jbetween nodes iandjin a graph Gcan then be computed as
wi,j= exp ( −d(i, j)), d (i, j) = (fi−fj)⊤M(fi−fj) (25)
where d(i, j)is the squared Mahalanobis distance given PSD metric matrix Mthat quantifies the
difference between nodes iandj.Medge weights {wi,j}compose a graph G, specified by the
Laplacian matrix Lfor GLR minimization (4)and the incidence matrix CM×Nfor GTV minimization
(11). Because wi,j≥0,∀i, j, constructed graph Gis positive.
As a concrete example, consider bilateral filter (BF) weights commonly used in image filtering
[42], where feature ficontains the 2D grid location liand color intensity piof pixel i, and metric
M=diag([1/σ2
d; 1/σ2
r])is a diagonal matrix with weights to specify the relative strength of the
domain andrange filters in BF. Because BF uses input pixel intensities pl’s to compute weighted
output pixel intensities pi’s, BF is signal-dependent , similar to self-attention weights in transformers.
Edge weights are often first normalized before being used for filtering.
Normalization : For normalization, the symmetric normalized graph Laplacian Lnis defined as
Ln≜D−1/2LD−1/2, so that the diagonal entries of Lnare all ones (assuming Gis connected and
positive) [18]. We assume normalized Lnis used for Laplacian Lin GLR minimization in (4).
Alternatively, the asymmetric random walk graph Laplacian Lrwis defined as Lrw≜D−1L, so
that the sum of each row of Lrwequals to zero [ 18]. Interpreting Lrwas a Laplacian matrix to a
directed graph, the weight sum of edges leaving each node iis one, i.e.,P
l|(i,l)∈E¯wi,l= 1,∀i. To
accomplish this, undirected edges weights {wi,j}are normalized to {¯wi,j}via
¯wi,j=exp(−d(i, j))P
l|(i,l)∈Eexp(−d(i, l)). (26)
For GTV minimization in (11), we normalize edge weights in incidence matrix Cinstead using (26).
This results in normalized ¯C∈R2M×Nfor a directed graph with 2Mdirected edges. Subsequently,
7Figure 1: Unrolling of GTV-based signal interpolation algorithm.
we define symmetric graph Laplacian ¯L=¯C⊤¯Cand generalized graph Laplacian ¯L=¯L+H⊤H.
Note that ∥¯C1∥1=PN
l=1¯wi,l|1−1|= 0after normalization, as expected for a total variation term
on a constant signal 1. Further, note that while ¯Cis an incidence matrix for a directed graph with 2M
edges, ¯Lis a graph Laplacian for an undirected graph with Medges. See Fig. 3 in Appendix A.6 for
an example of incidence matrix C, normalized incidence matrix ¯C, and graph Laplacian matrix ¯L.
Comparison to Self-Attention Operator : We see how the definitions of edge weights (25) and
normalization (26) are similar to attention weights in (23). Specifically, interpreting the negative
squared Mahalanobis distance −d(i, j)as affinity e(i, j), normalized edge weights ¯wi,jin(25) are
essentially the same as attention weights ai,jin(23). There are subtle but important differences: i)
how non-negative Mahalanobis distance d(i, j)is computed in (25) using features fi=F(xi)and
metric Mversus how real-valued affinity is computed via a transformed dot product in (23), and ii)
how the normalization term is computed in a one-hop neighborhood from node iin(26) versus how
it is computed using all Ntokens in (23). The first difference conceptually means that edge weight
based on Mahalanobis distance d(i, j)is symmetric ( i.e.,¯wi,j= ¯wj,i), while attention weight ai,jis
not. Both differences have crucial complexity implications, which we will revisit in the sequel.
Further, we note that, given a graph G, the interpolated signal x∗is computed simply as a low-pass
filtered output of the up-sampled input observation H⊤yvia(10) or(18), depending on the assumed
graph smoothness prior, GLR or GTV , while the output embedding yiin a conventional transformer
requires value matrix Vin (24). This also has a complexity implication.
5.3 Deep Algorithm Unrolling
We unroll Tsequential pairs of an iterative interpolation algorithm (GLR- or GTV-based) with a
graph learning module into an interpretable neural net. See Fig. 1a for an illustration of the GTV-
based algorithm unrolling, where the t-th pair of ADMM block and the graph learning module
have respective parameters ΘtandΦtthat are learned from back-propagation via a defined loss
function. Φtinclude parameters used to define feature function F(·)and metric matrix Min(25),
so the module can construct a graph Gspecified by incidence matrix Ct+1given signal xt. In our
implementation, we employ a shallow CNN to map a neighborhood of pixels centered at pixel ito a
low-dimensional feature fi, with a parameter size smaller than query and key matrices, QandK, in a
conventional transformer. See Section 6.1 for details.
An ADMM block contains multiple ADMM layers that are unrolled iterations of the iterative ADMM
algorithm described in Section 4.2. Each ADMM layer updates the main variables x,z,q, auxiliary
variable ˜q, and Lagrange multiplier µin turn using (17) to(22). ADMM weight parameter γ, as
well as parameters in CG used to compute linear system (18), are learned via back-propagation.
Specifically, two CG parameters αandβthat represent step size and momentum during the conjugate
gradient descent step are learned. See Appendix A.7 for details.
8Figure 2: Demosaicking performance vs. training
size for different models.MethodMcM
PSNR
σ= 10 σ= 20 σ= 30 σ= 50
RST-B 28.01 22.7 19.34 15.03
uGLR 28.24 22.84 19.49 15.203
uGTV 28.31 22.89 19.56 15.38
Table 1: Demosaicking performance in noisy sce-
nario, where models are trained on noiseless dataset.
6 Experiments
6.1 Experimental Setup
All models were developed using Python 3.11. We leveraged PyTorch to implement all models and
trained them using NVIDIA GeForce RTX 2080 Ti. To train each learned model, we used the DIV2K
dataset, which contains 800 and 100 high-resolution (HR) training and validation images, respectively.
Since the images are HR, we patchified the images into small images and used only about 1to4%of
the patches for training and validation sets. We randomly sampled patches of 64×64pixels to train
the model. To test a model, we used the McM [ 43], Kodak [ 44], and Urban100 [ 45] datasets, running
each model on the whole images. See Appendix A.8 for more implementation details.
We tested model performance in two imaging applications: demosaicking and image interpolation.
Demosaicking reconstructs a full-color image (each pixel contains RGB colors) from a Bayer-
patterned image, where each pixel location has only one of Red, Green, or Blue. Interpolation
reconstructs empty pixels missing all three colors in an image. To create input images, for the
first application, we started from a full-color image and then removed color components per pixel
according to the Bayer pattern. For the second application, we directly down-sampled horizontally
and vertically a HR image by a factor of 2to get the corresponding low-resolution (LR) image without
any anti-aliasing filtering. This is equivalent to keeping every four pixels in the HR image.
6.2 Experimental Results
For the first application, we evaluated our graph-based models against two variants of RSTCANet
[46], RSTCANet-B and RSTCANet-S (RST-B and RST-S for short), a SOTA framework that employs
a swin transformer architecture, Menon [ 47], Malvar [ 48] and bicubic interpolation. Menon [ 47]
employs a directional approach combined with an a posteriori decision, followed by an additional re-
finement step. Malvar [ 48] uses a linear filtering technique that incorporates inter-channel information
across all channels for demosaicking.
The baselines for our second application are MAIN [ 49], a multi-scale deep learning framework for
image interpolation, SwinIR [ 50], and bicubic interpolation. SwinIR consists of three stages: shallow
feature extraction, deep feature extraction, and a final reconstruction stage; see [ 50] for details. We
use Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) [ 51] as our
evaluation metrics, common in image quality assessment.
Method Params#McM
PSNR SSIMKodak
PSNR SSIMUrban100
PSNR SSIM
Bicubic - 29.01 0.8922 26.75 0.8299 22.95 0.7911
MAIN [49] 10942977 32.72 0.822 28.23 0.728 25.46 0.806
SwinIR-lightweight [50] 904744 32.24 0.9354 28.62 0.8794 25.08 0.8553
iGLR - 28.53 0.8537 26.71 0.8005 22.87 0.7549
iGTV - 30.41 0.887 28.05 0.832 24.26 0.7855
uGLR 319090 33.31 0.9431 29.10 0.8870 25.94 0.8777
uGTV 319115 33.36 0.9445 29.08 0.8888 26.12 0.8801
Table 3: Interpolation performance for different models, trained on 10k sample dataset.
9Method Params#McM
PSNR SSIMKodak
PSNR SSIMUrban100
PSNR SSIM
Bilinear - 29.71 0.9304 28.22 0.8898 24.18 0.8727
RST-B [46] 931763 34.85 0.9543 38.75 0.9857 32.82 0.973
RST-S [46] 3162211 35.84 0.961 39.81 0.9876 33.87 0.9776
Menon [47] - 32.68 0.9305 38.00 0.9819 31.87 0.966
Malvar [48] - 32.79 0.9357 34.17 0.9684 29.00 0.9482
iGLR - 29.39 0.8954 27.50 0.8487 23.13 0.8406
iGTV - 30.43 0.8902 28.66 0.8422 24.91 0.8114
uGLR 323410 36.09 0.9650 37.88 0.9821 33.60 0.9772
uGTV 323435 36.59 0.9665 39.11 0.9855 34.01 0.9792
Table 2: Demosaicking performance for different models, trained on 10k sample dataset.
Table 2 shows the demosaicking performance for different models, where all models were trained on
the same dataset and the same number of epochs ( 30), using a subset of DIV2K dataset containing
10Kof64×64patches. We observe that our unrolled GTV model (uGTV) achieved the best overall
performance, while the unrolled GLR model (uGLR) and RST-S performed similarly. Both our models
(uGTV and uGLR) performed better than RST-B while employing significantly fewer parameters.
Crucially, we observe that although our normalized edge weight ¯wi,jbased on Mahalanobis distance
is symmetric while the self-attention weight ai,jis not (due to query and key matrices QandK
not being the same in general), the directionality in the self-attention mechanism does not appear to
help improve performance of the conventional transformer further, at least for image interpolation
tasks. The iterative GTV algorithm (iGTV) without parameter optimization performed the worst,
demonstrating the importance of parameter learning.
In Fig. 2, we see the demosaicking performance of different models versus training data size. We see
that for small data size, our models (uGTV and uGLR) performed significantly better than RST-B.
This is intuitive, since a model with more parameters requires more training data in general. See
Appendix A.9 for example visual results.
Next, we test robustness to covariate shift by testing models trained on noiseless data using a
dataset artificially injected with Gaussian noise. Table 1 shows the demosaicking performance versus
different noise variances. We observe that our models outperformed RST-B in all noisy scenarios.
For image interpolation, we interploated a LR image to a corresponding HR image. We trained all
models on the same dataset as the first application with the same number of epochs ( 15). Table 3
shows that under the same training conditions, our proposed models (uGTV and uGLR) outperformed
MAIN in interpolation performance in all three benchmark datasets by about 0.7dB. Note that for this
application we only interpolated Y-channel from YCbCr color space for PSNR computation. Similar
to the first application, our models achieved slight performance gain while employing drastically
fewer parameters; specifically, uGTV employed only about 3%of the parameters in MAIN.
7 Conclusion
By unrolling iterative algorithms that minimize one of two graph smoothness priors— ℓ2-norm graph
Laplacian regularizer (GLR) or ℓ1-norm graph total variation (GTV)—we build interpretable and
light-weight transformer-like neural nets for the signal interpolation problem. The key insight is
that the normalized graph learning module is akin to the self-attention mechanism in a conventional
transformer architecture. Moreover, the interpolated signal in each layer is simply the low-pass
filtered output derived from the assumed graph smoothness prior, eliminating the need for the value
matrix. Experiments in two imaging applications show that interpolation results on par with SOTA
can be achieved with a fraction of the parameters used in conventional transformers.
References
[1]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural machine translation by
jointly learning to align and translate,” CoRR , vol. abs/1409.0473, 2014.
[2]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” Advances in neural information
10processing systems , vol. 30, 2017.
[3]Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo, “Swin transformer: Hierarchical vision transformer using shifted windows,” in 2021
IEEE/CVF International Conference on Computer Vision (ICCV) , 2021, pp. 9992–10002.
[4]Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weissenborn, Georg Heigold, Jakob Uszkoreit,
Lucas Beyer, Matthias Minderer, Mostafa Dehghani, Neil Houlsby, Sylvain Gelly, Thomas
Unterthiner, and Xiaohua Zhai, “An image is worth 16x16 words: Transformers for image
recognition at scale,” in The Ninth International Conference on Learning Representations
(ICLR) , 2021.
[5]James Vuckovic, Aristide Baratin, and Rémi Tachet des Combes, “A mathematical theory of
attention,” ArXiv , vol. abs/2007.02876, 2020.
[6]Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi,
Sanjiv Kumar, and Suvrit Sra, “Why are adaptive methods good for attention models?,” in
Advances in Neural Information Processing Systems , H. Larochelle, M. Ranzato, R. Hadsell,
M.F. Balcan, and H. Lin, Eds. 2020, vol. 33, pp. 15383–15393, Curran Associates, Inc.
[7]Charles Burton Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt, “Approximating how
single head attention learns,” ArXiv , vol. abs/2103.07601, 2021.
[8]Colin Wei, Yining Chen, and Tengyu Ma, “Statistically meaningful approximation: a case
study on approximating turing machines with transformers,” in Advances in Neural Information
Processing Systems , S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds.
2022, vol. 35, pp. 12071–12083, Curran Associates, Inc.
[9]Hyunjik Kim, George Papamakarios, and Andriy Mnih, “The lipschitz constant of self-attention,”
inProceedings of the 38th International Conference on Machine Learning , Marina Meila and
Tong Zhang, Eds. 18–24 Jul 2021, vol. 139 of Proceedings of Machine Learning Research , pp.
5562–5571, PMLR.
[10] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang, “Inductive biases and
variable creation in self-attention mechanisms,” in Proceedings of the 39th International
Conference on Machine Learning , Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesvari, Gang Niu, and Sivan Sabato, Eds. 17–23 Jul 2022, vol. 162 of Proceedings of
Machine Learning Research , pp. 5793–5831, PMLR.
[11] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen, “A theoretical understanding of
shallow vision transformers: Learning, generalization, and sample complexity,” in The Eleventh
International Conference on Learning Representations , 2023.
[12] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin
Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin,
Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos
Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd Van
Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine
Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader
Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran,
Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil
Houlsby, “Scaling vision transformers to 22 billion parameters,” in Proceedings of the 40th
International Conference on Machine Learning , Andreas Krause, Emma Brunskill, Kyunghyun
Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, Eds. 23–29 Jul 2023, vol. 202 of
Proceedings of Machine Learning Research , pp. 7480–7512, PMLR.
[13] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett, “Trained transformers learn linear models
in-context,” Journal of Machine Learning Research , vol. 25, no. 49, pp. 1–55, 2024.
[14] Vishal Monga, Yuelong Li, and Yonina C. Eldar, “Algorithm unrolling: Interpretable, efficient
deep learning for signal and image processing,” IEEE Signal Processing Magazine , vol. 38, no.
2, pp. 18–44, 2021.
[15] Karol Gregor and Yann LeCun, “Learning fast approximations of sparse coding,” in Proceed-
ings of the 27th International Conference on International Conference on Machine Learning ,
Madison, WI, USA, 2010, ICML’10, p. 399–406, Omnipress.
11[16] Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin
Haeffele, and Yi Ma, “White-box transformers via sparse rate reduction,” in Advances in Neural
Information Processing Systems , A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine, Eds. 2023, vol. 36, pp. 9422–9457, Curran Associates, Inc.
[17] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, “The emerging field
of signal processing on graphs: Extending high-dimensional data analysis to networks and other
irregular domains,” in IEEE Signal Processing Magazine , May 2013, vol. 30, no.3, pp. 83–98.
[18] A. Ortega, P. Frossard, J. Kovacevic, J. M. F. Moura, and P. Vandergheynst, “Graph signal
processing: Overview, challenges, and applications,” in Proceedings of the IEEE , May 2018,
vol. 106, no.5, pp. 808–828.
[19] G. Cheung, E. Magli, Y . Tanaka, and M. Ng, “Graph spectral image processing,” in Proceedings
of the IEEE , May 2018, vol. 106, no.5, pp. 907–930.
[20] J. Pang and G. Cheung, “Graph Laplacian regularization for inverse imaging: Analysis in the
continuous domain,” in IEEE Transactions on Image Processing , April 2017, vol. 26, no.4, pp.
1770–1785.
[21] X. Liu, G. Cheung, X. Wu, and D. Zhao, “Random walk graph Laplacian based smoothness
prior for soft decoding of JPEG images,” IEEE Transactions on Image Processing , vol. 26, no.2,
pp. 509–524, February 2017.
[22] Fei Chen, Gene Cheung, and Xue Zhang, “Manifold graph signal restoration using gradient
graph Laplacian regularizer,” IEEE Transactions on Signal Processing , vol. 72, pp. 744–761,
2024.
[23] Jin Zeng, Jiahao Pang, Wenxiu Sun, and Gene Cheung, “Deep graph Laplacian regularization
for robust denoising of real images,” in 2019 IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops (CVPRW) , 2019, pp. 1759–1768.
[24] Chinthaka Dinesh, Gene Cheung, and Ivan V . Baji ´c, “Point cloud denoising via feature graph
laplacian regularization,” IEEE Transactions on Image Processing , vol. 29, pp. 4143–4158,
2020.
[25] Chinthaka Dinesh, Gene Cheung, and Ivan V . Baji ´c, “Point cloud video super-resolution via
partial point coupling and graph smoothness,” IEEE Transactions on Image Processing , vol. 31,
pp. 4117–4132, 2022.
[26] Yongyi Yang, Tang Liu, Yangkun Wang, Jinjing Zhou, Quan Gan, Zhewei Wei, Zheng Zhang,
Zengfeng Huang, and David Wipf, “Graph neural networks inspired by classical iterative
algorithms,” in Proceedings of the 38th International Conference on Machine Learning , Marina
Meila and Tong Zhang, Eds. 18–24 Jul 2021, vol. 139 of Proceedings of Machine Learning
Research , pp. 11773–11783, PMLR.
[27] Wei Hu, Xiang Gao, Gene Cheung, and Zongming Guo, “Feature graph learning for 3D point
cloud denoising,” IEEE Transactions on Signal Processing , vol. 68, pp. 2841–2856, 2020.
[28] Cheng Yang, Gene Cheung, and Wei Hu, “Signed graph metric learning via Gershgorin disc
perfect alignment,” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 44,
no. 10, pp. 7219–7234, 2022.
[29] Abderrahim Elmoataz, Olivier Lezoray, and SÉbastien Bougleux, “Nonlocal discrete regulariza-
tion on weighted graphs: A framework for image and manifold processing,” IEEE Transactions
on Image Processing , vol. 17, no. 7, pp. 1047–1060, 2008.
[30] Camille Couprie, Leo Grady, Laurent Najman, Jean-Christophe Pesquet, and Hugues Talbot,
“Dual constrained tv-based regularization on graphs,” SIAM Journal on Imaging Sciences , vol.
6, no. 3, pp. 1246–1273, 2013.
[31] Peter Berger, Gabor Hannak, and Gerald Matz, “Graph signal recovery via primal-dual algo-
rithms for total variation minimization,” IEEE Journal of Selected Topics in Signal Processing ,
vol. 11, no. 6, pp. 842–855, 2017.
[32] J. R Shewchuk, “An introduction to the conjugate gradient method without the agonizing pain,”
Tech. Rep., USA, 1994.
[33] Sinong Wang and Ness Shroff, “A new alternating direction method for linear programming,”
inAdvances in Neural Information Processing Systems , I. Guyon, U. V on Luxburg, S. Bengio,
12H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. 2017, vol. 30, Curran Associates,
Inc.
[34] Y . Bai, G. Cheung, X. Liu, and W. Gao, “Graph-based blind image deblurring from a single
photograph,” IEEE Transactions on Image Processing , vol. 28, no.3, pp. 1404–1418, 2019.
[35] S. Boyd and L. Vandenberghe, Convex Optimization , Cambridge, 2004.
[36] G. Golub and C. F. Van Loan, Matrix Computations (Johns Hopkins Studies in the Mathematical
Sciences) , Johns Hopkins University Press, 2012.
[37] E. BrianDavies, Graham M. L. Gladwell, Josef Leydold, Peter F. Stadler, and Peter F. Stadler,
“Discrete nodal domain theorems,” Linear Algebra and its Applications , vol. 336, pp. 51–60,
2000.
[38] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical
learning via the alternating direction method of multipliers,” in Foundations and Trends in
Optimization , 2011, vol. 3, no.1, pp. 1–122.
[39] H. Egilmez, E. Pavez, and A. Ortega, “Graph learning from data under Laplacian and structural
constraints,” in IEEE Journal of Selected Topics in Signal Processing , July 2017, vol. 11, no.6,
pp. 825–841.
[40] Xiaowen Dong, Dorina Thanou, Michael Rabbat, and Pascal Frossard, “Learning graphs from
data: A signal representation perspective,” IEEE Signal Processing Magazine , vol. 36, no. 3, pp.
44–63, 2019.
[41] Saghar Bagheri, Tam Thuc Do, Gene Cheung, and Antonio Ortega, “Spectral graph learning
with core eigenvectors prior via iterative GLASSO and projection,” IEEE Transactions on
Signal Processing , vol. 72, pp. 3958–3972, 2024.
[42] C. Tomasi and R. Manduchi, “Bilateral filtering for gray and color images,” in Proceedings of
the IEEE International Conference on Computer Vision , Bombay, India, 1998.
[43] Lei Zhang, Xiaolin Wu, Antoni Buades, and Xin Li, “Color demosaicking by local directional
interpolation and nonlocal adaptive thresholding,” Journal of Electronic imaging , vol. 20, no. 2,
pp. 023016–023016, 2011.
[44] Eastman Kodak, “Kodak lossless true color image suite (photocd pcd0992),” URL http://r0k.
us/graphics/kodak , vol. 6, pp. 2, 1993.
[45] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja, “Single image super-resolution from
transformed self-exemplars,” in Proceedings of the IEEE conference on computer vision and
pattern recognition , 2015, pp. 5197–5206.
[46] Wenzhu Xing and Karen Egiazarian, “Residual swin transformer channel attention network
for image demosaicing,” in 2022 10th European Workshop on Visual Information Processing
(EUVIP) . IEEE, 2022, pp. 1–6.
[47] Daniele Menon, Stefano Andriani, and Giancarlo Calvagno, “Demosaicing with directional
filtering and a posteriori decision,” IEEE Transactions on Image Processing , vol. 16, no. 1, pp.
132–141, 2007.
[48] H.S. Malvar, Li wei He, and R. Cutler, “High-quality linear interpolation for demosaicing of
bayer-patterned color images,” in 2004 IEEE International Conference on Acoustics, Speech,
and Signal Processing , 2004, vol. 3, pp. iii–485.
[49] Jiahuan Ji, Baojiang Zhong, and Kai-Kuang Ma, “Image interpolation using multi-scale
attention-aware inception network,” IEEE Transactions on Image Processing , vol. 29, pp.
9413–9428, 2020.
[50] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte, “Swinir:
Image restoration using swin transformer,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV) Workshops , October 2021, pp. 1833–1844.
[51] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli, “Image quality assessment: from
error visibility to structural similarity,” IEEE Transactions on Image Processing , vol. 13, no. 4,
pp. 600–612, 2004.
[52] Yurii Nesterov, Introductory lectures on convex optimization: A basic course , vol. 87, Springer
Science & Business Media, 2013.
13A Appendix
A.1 Full-Rankness of Matrix Pin(4)
Given that the underlying graph Gis positive and connected, we prove that coefficient matrix P
in(4)is full-rank and thus invertible. We prove by contradiction: suppose Pisnotfull-rank, and
there exists a vector v= [x;µ]such that Pv=0N+K. Suppose we order Ksampled entries
xSbefore N−Knon-sampled entries x¯Sinx,i.e.,x= [xS;x¯S]. First, given sampling matrix
H= [IK0K,N−K]∈ {0,1}K×N, focusing on the second block row of P,[H 0 K,K]v=0K
means Hx= [Ik0K,N−K][xS;x¯S] =0K. Thus, sampled entries of xmust be zeros, i.e.,xS=0K.
Second, suppose we write Laplacian Lin blocks, i.e.,L= [LS,SLS,¯S;L¯S,SL¯S,¯S]. Then, the
non-sampled rows of the first block row of Mv are

[2L H⊤]
x
µ
¯S= 2 
L¯S,SxS+L¯S,¯Sx¯S
(27)
where (H⊤µ)¯S=0N−Ksince non-sampled rows of H⊤= [IK;0N−K,K]are all zeros. From
above, we know xS=0K, and thus ([2L H⊤]v)¯S=0N−Kimplies that we require L¯S,¯Sx¯S=0N.
However, since L¯S,¯Sis a combinatorial Laplacian matrix for a positive sub-graph connecting non-
sampled nodes plus at least one strictly positive self-loop (representing an edge from a non-sampled
node to a sample node), given Gis a connected positive graph, L¯S,¯Smust be PD (see Appendix A.2
below). Thus, ∄x¯S̸=0s.t.L¯S,¯Sx¯S=0N−K, a contradiction. Therefore, we can conclude that P
must be full-rank.
A.2 Positive Definiteness of Matrix L¯S,¯Sin(8)
Given that the underlying graph Gis positive and connected, we prove that coefficient matrix L¯S,¯S
in(8)is PD. By definition L≜diag(W1N)−W, where Wis an adjacency matrix for a positive
graph without self-loops, i.e.,Wi,j≥0,∀i̸=jandWi,i= 0,∀i. Thus, Li,i=P
jWi,j,∀i, and
Li,j=−Wi,j≤0,∀i̸=j. For sub-matrix L¯S,¯S,Li,i=P
j∈¯SWi,j+P
j∈SWi,j,∀i∈¯S. Define
L′
¯S,¯Sas a graph Laplacian matrix for nodes in ¯Sconsidering only edges between nodes in ¯S,i.e.,
Li,i=P
j∈¯SWi,j,∀i∈¯S. Define D′
¯S,¯Sas a diagonal degree matrix for nodes in ¯Sconsidering
only edges between ¯SandS,i.e.,D′
i,i=P
j∈SWi,j,∀i∈¯S. Note that D′
i,i≥0,∀i. We can now
writeL¯S,¯Sas
L¯S,¯S=L′
¯S,¯S+D′
¯S,¯S. (28)
L′
¯S,¯Sis a combinatorial graph Laplacian for a positive graph without self-loops, and thus is provably
PSD [ 19].D′
¯S,¯Sis a non-negative diagonal matrix, and thus is also PSD. By Weyl’s inequality, L¯S,¯S
is also PSD.
We prove by contradiction: suppose L¯S,¯Sis not PD, and ∃x̸=0such that x⊤L¯S,¯Sx= 0.
x⊤L¯S,¯Sx= 0 iffx⊤L′
¯S,¯Sx= 0 andx⊤D′
¯S,¯Sx= 0 simultaneously. Denote by ¯S1and¯S2the
indices of nodes in ¯Swith and without connections to nodes in S, respectively. ¯S1̸=∅, since Gis
connected. Suppose first ¯S2=∅. Then D′
¯S,¯Shas strictly positive diagonal entries and is PD, and
there is no x̸=0s.t.x⊤D′
¯S,¯Sx=0, a contradiction.
Suppose now ¯S2̸=∅. First, x⊤D′
¯S,¯Sx= 0implies x¯S1=0. Then,
x⊤L′
¯S,¯Sx=X
i,j∈¯SWi,j(xi−xj)2≥X
i∈¯S1,j∈¯S2Wi,j(xi−xj)2. (29)
Since each term in the sum is non-negative, the sum is zero only if for each (i, j)∈ Ewhere i∈¯S1
andj∈¯S2,0 =xi=xj. For nodes k∈¯S2connected only to nodes j∈¯S2connected to i∈¯S1,
xk=xj=xi= 0necessarily, and for nodes l∈¯S2connected to k∈¯S2must have xl=xk= 0,
and so on. Thus, x=0, a contradiction. Thus, we can conclude that L¯S,¯Sis PD.
14A.3 Derivation of (17),(18) and(19)
We define ϕϕϕ= [z;x;q1;q2]and rewrite the objective (16) to
min
ϕϕϕ
1M
0N+2M⊤
ϕϕϕ+ (µt)⊤
A
02M,M +NI2M
ϕϕϕ−
b
˜qt
+γ
2
A
02M,M +NI2M
| {z }
Bϕϕϕ−
b
˜qt2
2. (30)
(30) is a convex quadratic objective, and so we take the derivative w.r.t. ϕϕϕand set it to 0:

1M
0N+2M
+
A⊤0M+N,2M
I2M
(µt)
+γ
2
2
A⊤0M+N,2M
I2M
A
02M,M +NI2M
ϕϕϕ−2
A⊤0M+N,2M
I2M
b
˜qt
=0.(31)
Given that Bis the following matrix:
B=
IM −C−IM 0M
IM C 0 M−IM
0K,M H 0 K,M 0K,M
0M,M 0M,N IM 0M
0M,M 0M,N 0M IM
(32)
Hence, B⊤Bis
B⊤B=
2IM 0M,N −IM−IM
0N,M 2C⊤C+H⊤H C⊤−C⊤
−IM C 2IM0M,M
−IM −C 0 M,M 2IM
. (33)
Note that adding two of row 1to rows 3and4, we get [2IM0M,N0M,M0M,M].
Solving for ϕϕϕin (31), we get
γB⊤Bϕϕϕ=−
1M
0N
0M
0M
−B⊤

µt
a
µt
b
µt
c
µt
d
µt
e
−γ
0M
0M
y
˜qt
1
˜qt
2

(34)
=
−1M−µt
a−µt
b
C⊤µt
a−C⊤µt
b−H⊤µt
c+γH⊤y
µt
a−µt
d+γ˜qt
1
µt
b−µt
e+γ˜qt
2
 (35)
We can solve for zt+1directly; by adding two of row 1to rows 3and4of (35), we get
2γzt+1=−2(1M)−µt
a−µt
b−µt
d−µt
e+γ(˜qt
1+˜qt
2)
zt+1=−1
γ1M−1
2γ 
µt
a+µt
b+µt
d+µt
e
+1
2(˜qt
1+˜qt
2). (36)
Subtracting row 4from row 3of (35), we get
2γCxt+1+ 2γ(qt+1
1−qt+1
2) =µt
a−µt
b−µt
d+µt
e+γ(˜qt
1−˜qt
2)
γ(qt+1
1−qt+1
2) =−γCxt+1+1
2 
µt
a−µt
b−µt
d+µt
e
+γ
2(˜qt
1−˜qt
2).(37)
15Thus, row 2 can be rewritten as
γ(2C⊤C+H⊤H)xt+1+C⊤γ(qt+1
1−qt+1
2) =C⊤µt
a−C⊤µt
b−H⊤µt
c+γH⊤y
γ(C⊤C+H⊤H)xt+1=1
2C⊤ 
µt
a−µt
b+µt
d−µt
e
−H⊤µt
c
−γ
2C⊤(˜qt
1−˜qt
2) +γH⊤y. (38)
Finally, from rows 3and4,qt
1andqt
2can be computed as
qt
1=1
2 
zt+1−Cxt+1
+1
2γ(µt
a−µt
d+γ˜qt
1)
qt
2=1
2 
zt+1+Cxt+1
+1
2γ(µt
b−µt
e+γ˜qt
2). (39)
A.4 Invertibility of L=C⊤C+H⊤H
Clearly L=C⊤C+H⊤His real, symmetric, and positive semi-definite. Thus its eigenvalues are
real and non-negative. To show that it is invertible, it suffices to show that its minimum eigenvalue
λmin(L)is strictly greater than zero. But λmin(L) = min x:||x||=1x⊤Lx. Hence it suffices to show
thatx⊤Lx= 0implies x=0. Now observe that x⊤Lx=P
(i,j)∈Ew2
i,j(xi−xj)2+P
i∈Sx2
i,
where Sis the set of nodes with constrained values. If x⊤Lx= 0 then all terms must be zero,
meaning that all xiinSare zero, and hence all of their neighbors, and all of their neighbors, and so
on. Thus, Lis invertible if there exists at least one node with a constrained value (i.e., a self-loop) in
each connected component of the graph.
A.5 Derivation of (21)
We derive the solution to optimization (20). Ignoring the first convex but non-smooth term g(˜q), the
remaining two terms in the objective are convex and smooth. Taking the derivative w.r.t. variable ˜q
and setting it to 0, we get
−µt
d−γqt+1+γ˜q∗=0M
˜q∗=qt+1+1
γµt
d. (40)
This solution is valid iff g(˜q∗) = 0 ; otherwise the first term g(˜q∗)dominates and ˜q∗=0M. Given
that (40) can be computed entry-by-entry separately, (21) follows.
A.6 Example of Edge Weight Normalization for the Incidence Matrix
In Fig. 3(top), we show an example three-node undirected graph with three edge weights w1,2= 1/2,
w1,3= 1/2, andw2,3= 1/3, and the corresponding incidence matrix C. Normalizing edge weights
using (26), we see in Fig. 3(middle) a directed graph with six edges where the sum of normalized
edge weights leaving a node is 1, resulting in normalized incidence matrix ¯C. Finally, we see in
Fig. 3(bottom) an undirected graph with three edges corresponding to graph Laplacian ¯L=¯C⊤¯C.
Note that ∥¯C13∥1=06as expected.
A.7 Parameters Learning in Conjugate Gradient Algorithm (CG)
The linear systems that we need to solve— (8)for GLR minimization and (18) for GTV minimization—
have the follow form,
Lx=b. (41)
GivenLis PD, we consider the minimization problem,
min
xQ(x) =1
2x⊤Lx−b⊤x (42)
with gradient
δQ(x)
δx=Lx−b. (43)
16C="1/2−1/2 0
0 1 /3−1/3
1/2 0 −1/3#
¯C=
1/2−1/2 0
−3/5 3 /5 0
0 2 /5−2/5
0−2/5 2 /5
1/2 0 −1/2
−3/5 0 3 /5

¯L=
1
2+s2
4+s2
9−1+s2
4−1
4−s2
9
−1+s2
41+s2
4+2s2
9−2s2
9
−1
4−s2
9−2s2
91
4+s2
3

Figure 3: 3-node graph for incidence matrix C(top), 3-node graph for normalized incidence matrix
¯C(middle), 3-node graph for graph Laplacian ¯L=¯C⊤¯Cwhere s=6
5(bottom).
Thus, the simple gradient descent has the following update rules with αtcommonly known as learning
rate,
gt=Lxt−b=gt−1−αtLgt−1(44)
xt+1=xt−αtgt. (45)
Next, a momentum term βtand the cumulative gradients term vtare added, resulting in the well-
known Accelerated Gradient Descent Algorithm [ 52], also known as Conjugated Gradient Descent.
The new update rules are
gt=gt−1−αtLvt−1(46)
vt=gt+βtvt−1(47)
xt+1=xt−αtvt(48)
where both αtandβtin each iteration tare considered trainable parameters.
A.8 Experimental Details
Initially, we developed our unrolled ADMM model without training any parameters. The target signal
xwas estimated using linear interpolation of known values in 5×5pixel neighborhood. RGB values
and pixel locations were combined to form feature vectors for computing edge weights wi,j, which
were shared across the three channels. The metric matrix Mwas initialized as a diagonal matrix
with all entries set to 1.5. Vectors µa,µb,µc,µd, andµewere initialized with all entries equal to 0.1.
The parameters γ,α, andβin CG were set to 10,0.5, and 0.3, respectively. For the learned ADMM
block, the parameters γ,α,β, and the metric matrix Mwere learned during training. A training
dataset was created consisting of 5000, 10000, or 20000 image patches, each of size 64×64, to train
the model.
All matrix multiplications in our models are implemented to take advantage of the sparsity of the
constructed graphs, which were restricted to be a window of size 5×5 = 25 nearest neighbors of
each pixel. This ensures that our graphs are always connected andsparse . We stacked vertically 4
Graph Learning modules coupled with ADMM block, so that we can learn multiple graphs in parallel.
This is commonly known as multi-head in the transformer architecture. We also stacked 5graph
learning modules and ADMM blocks horizontally to further learn more graphs. In all ADMM blocks,
we set the number of ADMM iterations to 5and the number of CG iterations to 10.
To extract high-level features, a shallow CNN was employed, consisting of four convolutional layers
with 48 feature maps (12 features for each graph learning head). After each convolutional layer,
a ReLU activation function was applied. Convolutional layers utilized 3×3kernels without any
down-sampling, generating 48 features for each pixel. The feature maps were divided into four sets,
17(a) Urban100: image062.png
(b) Original
 (c) iGTV
 (d) uGTV
(e) uGLR
 (f) RST-B
 (g) RST-S
Figure 4: Visual demosaicking results for image Urban062 .
with each set serving as the input for a Graph Learning module, allowing for the parallel production
of four graphs. A simple weighted average scheme was then used to combine the outputs into a
single output xt. Additionally, skip connections were introduced between the convolutional layers to
facilitate the training process.
A.9 Additional Experimental Results
Fig. 4 shows visual results for a test image for all models, including our uGTV , uGLR and the
baselines. Two variants of RSTCANet, uGTV and uGLR are trained on 10000 images patches of size
64×64for30epochs. We observe that our two models, especially uGTV , has better performance
compared to RST-B and comparable performance with RST-S in selected high-frequency area.
18NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Main claims are clearly stated in the abstract and Introduction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We state the limitation of our edge weight computation, as compared to
self-attention weights in conventional transformers, in Section 5.2.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
19Justification: All proofs and derivations are included in the Appendices.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Details of our experiments are explained in Section 6.1 and Appendix A.8.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
20Answer: [No]
Justification: We used open source data: McM [ 43], Kodak [ 44], and Urban100 [ 45] datasets.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We described the experiments setup in Section 6.1 and Appendix A.8.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: We did provide various metrics for many datasets to compare our models with
the baseline models.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
21•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We described the experimental setup in Section 6.1 and Appendix A.8.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: As authors, we have made every effort to conform to the NeurIPS Code of
Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [No]
Justification: The performed work is purely computational and does not have direct societal
impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
22•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
23•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
24