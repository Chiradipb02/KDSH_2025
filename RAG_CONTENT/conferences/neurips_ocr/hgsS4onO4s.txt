Inverse M-Kernels for Linear Universal
Approximators of Non-Negative Functions
Hideaki Kim
NTT Corporation
hideaki.kin@ntt.com
Abstract
Kernel methods are widely utilized in machine learning ﬁeld to learn, from train-
ing data, a latent function in a reproducing kernel Hilbert space. It is well
known that the approximator thus obtained usually achieves a linear represen-
tation, which brings various computational beneﬁts, while maintaining great rep-
resentation power (i.e., universal approximation). However, when non-negativity
constraints are imposed on the function’s outputs, the literature usually takes the
kernel method-based approximators as offering linear representations at the ex-
pense of limited model ﬂexibility or good representation power by allowing for
their nonlinear forms. The main contribution of this paper is to derive a sufﬁcient
condition for a positive deﬁnite kernel so that it may construct ﬂexible and lin-
ear approximators of non-negative functions. We call a kernel function that offers
these attributes an inverse M-kernel ; it is a generalization of the inverse M-matrix.
Furthermore, we show that for a one-dimensional input space, universal exponen-
tial/Abel kernels are inverse M-kernels and construct linear universal approxima-
tors of non-negative functions. To the best of our knowledge, it is the ﬁrst time
that the existence of linear universal approximators of non-negative functions has
been elucidated. We conﬁrm the effectiveness of our results by experiments on the
problems of non-negativity-constrained regression, density estimation, and inten-
sity estimation. Finally, we discuss issues and perspectives on multi-dimensional
input settings.
1 Introduction
Non-parametric estimation of latent functions continues to be of theoretical and practical impor-
tance in a wide spectrum of disciplines such as signal/image processing [ 13,32], system control
[12], geostatistics [ 3], bioinformatics [ 28], and clinical research [ 4]. Kernel method, one of the most
established techniques, learns ﬂexible function approximators by embedding data points into higher
dimensional reproducing kernel Hilbert spaces (RKHSs) [ 26,29]. For a broad class of learning
problems, kernel methods invoke the representer theorem [ 27,35] and recast the inﬁnite-dimensional
functional problems as their ﬁnite-dimensional counterparts, where the obtained approximators have
linear representation, i.e., ﬁnite linear combinations of kernel functions evaluated on data points.
Signiﬁcant computational beneﬁts are attained by the linear representation such as convex optimiza-
tion and cheap evaluation/integration of approximators.
In recent years, great attention has been paid to kernel methods with non-negativity constraints
on function outputs [ 6,17,17,31]; crucial applications include non-negativity-constrained regres-
sion, density estimation, and intensity estimation. Compared to unconstrained alternatives, non-
negativity-constrained kernel methods developed to date are faced with a problematic trade-off be-
tween linearity and ﬂexibility: the obtained approximators either can have linear representations at
the expense of degraded representation power [ 17], or achieve good representation power (i.e., uni-
38th Conference on Neural Information Processing Systems (NeurIPS 2024).versal approximation) by accepting nonlinear forms [ 17,21], which incurs substantial computation
costs. To the best of our knowledge, no non-negativity-constrained kernel method has been proposed
that combines linear representation with good representation power.
In this paper, we propose the ﬁrst linear universal approximator of non-negative functions for one-
dimensional input spaces. First, we derive a sufﬁcient condition so that the kernel can construct a
linear approximator of a non-negative function. We call a kernel that satisﬁes this novel condition an
inverse M-kernel ; it is a generalization of inverse M-matrix [ 8]. Next, we show that exponential/Abel
kernels, which have the universal approximating property [ 19,30], are inverse M-kernel functions
and can construct linear universal approximators of non-negative functions for one-dimensional in-
put spaces. It is worth noting that the most popular Gaussian kernels do not satisfy the condition de-
manded by the inverse M-kernel. Our results shed light on exponential kernels, which have received
less attention in the literature, as universal kernels for non-negativity-constrained approximators on
one-dimensional input spaces.
In Section 2, we outline related works and introduce some known results on M-matrix theory used
throughout the paper. In Section 3, we introduce the inverse M-kernel and construct linear universal
approximators of non-negative functions. In Section 4, we show some important applications of
our results, which include non-negativity-constrained regression, density estimation, and intensity
estimation, and evaluate the effectiveness of our proposal on synthetic data1. Finally, Section 5states
our conclusions and discuss future works on multi-dimensional input settings.
2 Background
2.1 Kernel Method-Based Linear Approximator
LetXbe a prescribed input space and k:X × X → Rbe a positive semi-deﬁnite kernel. Then
there exists a unique reproducing kernel Hilbert space (RKHS) Hk[26,29] associated with kernel
k(·,·). Given a set of Npoints{xn∈ X}N
n=1and a regularized learning problem:
min
f∈HkL(f(x1), . . . , f (xN)) + Ω( ||f||2
Hk), (1)
where L:RN→Ris a loss function, and Ωis a non-decreasing function of the squared RKHS
norm of f. It is well known that the solution of ( 1) invokes the representer theorem [27,35] and has
the representation,
f∗(·) =NX
n=1αnk(xn,·). (2)
For simplicity, the linear regularizer, Ω(||f||2
Hk) =r||f||2
Hkforr≥0, is assumed in this paper. The
inﬁnite-dimensional optimization problem ( 1) can be reduced to a ﬁnite-dimensional one in terms
of coefﬁcients α:= (α1, . . . , α N)⊤∈RNas follows:
min
α∈RNL(Kα) +r α⊤Kα, K:= [k(xn, xn′)]nn′∈RN×N. (3)
Under universal kernels [ 19,30], where RKHSs are dense in the space of all continuous functions of
X, the linear approximator ( 2) can approximate any continuous function on X, which is a property
known as the universal approximation . Each evaluation of the objective function ( 3) and the linear
approximator ( 2) needs the computation of O(N2)andO(N), respectively.
When non-negativity constraints are imposed on target function f≥0, the literature considers that
the linear approximator ( 2) cannot be applied directly because it generally has negative values, even
under non-negative kernel k(·,·)≥0. Conventional approaches to address the problem are listed
below (Section 2.2-2.3).
1Code and data to reproduce the results are available at https://github.com/HidKim/IM-Kernel.
22.2 Linear Approximators with Non-Negativity Constraints
Non-negative coefﬁcients Model (NCM) [ 17] enforces the linear approximator ( 2) to be non-
negative by using non-negative coefﬁcients and kernel functions,
fNCM(·) =NX
n=1αnk(xn,·), α 1, . . . , α N≥0, k(·,·)≥0, (4)
where coefﬁcients α∈RNare obtained by solving the optimization problem ( 3) with constraint
αn≥0. Here non-negative kernels include popular kernels such as Gaussian kernels e−||x−x′||2,
exponential kernels e−||x−x′||, and Cauchy kernels (1 +||x−x′||2)−1. Although NCM enjoys great
computational beneﬁts from its linear representation such as the preservation of loss functional’s
convexity and cheap evaluations/integrations, it suffers from low representation power due to the
strong non-negativity constraint on coefﬁcients, αn≥0(for details, see Appendix B).
Note that linear approaches with partial non-negativity constraints [ 5,17,22], which require non-
negativity only on a ﬁnite number of points (the points are not necessarily data points {xn}N
n=1), do
not guarantee the non-negativity at locations other than the points; these approaches are out of scope
of this paper.
2.3 Nonlinear Approximators with Non-Negativity Constraints
An elegant quadratic form of the non-negative model (QNM) [ 17] has recently been proposed that
exploits the non-negativity inherent in positive semi-deﬁnite operators:
fQNM(·) =NX
n,n′=1Bnn′k(xn,·)k(xn′,·),B∈RN×N,B⪰0, (5)
where⪰0represents the positive semi-deﬁnite constraint. QNM has the following beneﬁcial prop-
erties: it preserves the convexity of the loss functionals; it can be integrated in a closed form if
we know how to integrate kernel functions; under mild conditions on kernels, it is a universal ap-
proximator for non-negative functions. The coefﬁcient matrix Bis obtained efﬁciently by solving
anN-dimensional optimization problem, which naively costs O(N3)for each evaluation of the
objective function (for details, see Appendix A). An evaluation of the approximator ( 5) needs the
computation of O(N2).
Generalized linear models (GLMs), another nonlinear approach to constructing a non-negative func-
tion, use nonlinear transformation of a linear model [ 21]. GLMs are so ﬂexible that they can repre-
sent a wide class of non-negative functions, but they generally do not preserve the convexity of loss
functionals where they are used and cannot be integrated in closed form. However, most recently,
a promising model called squared neural family (SNF) [ 34], which was speciﬁcally designed for
density estimation, has been proposed that uses a quadratic transformation,
fSNF(·)dx=dµ(·)
z(Θ)g(t(·);Θ)2, g (t;Θ) =Vσ(Wt+b),Θ= (V,W, b), (6)
where dµ(·)is a non-negative measure, g(·|Θ)is a neural network of one hidden layer with ac-
tivation function σ(·)and parameter Θ,t(·)is a sufﬁcient statistic, and z(Θ)is the normalizing
constant, z(Θ) =R
X||g(t(x);Θ)||2dµ(x). Thanks to [[MEANING IS UNCLEAR the theory of
the neural network, -» ?? Gaussian process kernels [ 20],]] the integrations of SNF (i.e., z(Θ)) can be
executed in a closed form under various dµ(·),t(·), and σ(·). But SNF has some possible drawbacks:
it cannot preserve the convexity of the loss functionals, and so can yield many local optima; it has
many hyper-parameters to be determined such as σ(·)and the size of parameter Θ, on which model
performance largely depends. In this paper, we adopted the same size of Θas in [ 34]:V∈R1×30,
W∈R30×dim(X),b∈ R30, and t(x) =x.
2.4 M-Matrix Theory
Here we introduce some existing results on M-matrix theory, which are then used to clarify a sufﬁ-
cient kernel condition so that it may construct a linear approximator of non-negative functions.
3Then-by-nreal matrix A∈Rn×nis called an M-matrix [23] if it has the form γIn−C, in which
In∈Rn×nis the identity matrix of size n,C∈Rn×nis an entry-wise non-negative matrix, and
γ > ρ (C), the spectral radius of C; this is equivalent to Awith non-positive off-diagonal entries
that is invertible and having an entry-wise non-negative inverse. An entry-wise non-negative matrix
that occurs as the inverse of an M-matrix is called an inverse M-matrix [8]. Note that the inverse
of an M-matrix is always entry-wise non-negative, while the reverse is not always true. We denote
the classes of M-matrices and inverse M-matrices by MandM−1, respectively. Lemma 1below
shows useful properties on the entry-wise sign patterns of partitioned inverse M-matrices (for the
proof, see Theorem 8 in [ 8]).
Lemma 1. Suppose that an n-by-nsymmetric inverse M-matrix Q∈ M−1is partitioned as
Q=
Q1Q⊤
3
Q3Q2
,Q1∈Rn1×n1,Q2∈Rn2×n2,Q3∈Rn2×n1,
where n1andn2are positive integers satisfying n=n1+n2. Then the following inequalities hold:
0⩽Q⊤
3Q−1
2, 0⩽Q1−Q⊤
3Q−1
2Q3∈ M−1,
where⩽(⩾)represents the entry-wise inequality of matrices/vectors.
Proof. Suppose that the inverse of Q, denoted by ¯Q∈ M , is partitioned as
¯Q=¯Q1¯Q⊤
3¯Q3¯Q2
, ¯Q1∈Rn1×n1,¯Q2∈Rn2×n2,¯Q3∈Rn2×n1.
Then ¯Q1and ¯Q⊤
3may be expressed by using Schur’s complements as
¯Q1= (Q1−Q⊤
3Q−1
2Q3)−1,¯Q⊤
3=−(Q1−Q⊤
3Q−1
2Q3)−1Q⊤
3Q−1
2.
Because ¯Q1, a principal submatrix of M-matrix Q, is an M-matrix (Corollary 3 in [ 8]),Q1−
Q⊤
3Q−1
2Q3=¯Q−1
1is an inverse M-matrix and entry-wise positive. Furthermore, Q⊤
3Q−1
2=−(Q1
−Q⊤
3Q−1
2Q3)¯Q⊤
3is an entry-wise positive matrix because (Q1−Q⊤
3Q−1
2Q3)∈ M−1and−¯Q⊤
3
are both entry-wise positive matrices. This completes the proof. ■
3 Inverse M-Kernels
In this section, we deﬁne a new class of kernels and show that it plays an essential role in constructing
a linear and ﬂexible approximator of non-negative functions.
Deﬁnition 1 (Inverse M-kernels) .Letk:X × X → R+be a positive semi-deﬁnite kernel that
outputs non-negative values, K:= [k(xn, xn′)]nn′be a gram matrix constructed for any set of N
points (x1, x2, . . . , x N), and s:N→R+be a non-negative function of data size N. We call k(·,·)
an inverse M-kernel if K+s(N)IN∈ M−1. We denote the class of inverse M-kernels by Fs(N)
M−1.
Also, if s(N) = 0 orK∈ M−1, we call the kernel a strict inverse M-kernel, which we denote by
k(·,·)∈ FM−1.
The non-negative function s(N)may generally exhibit various scalings with respect to N, but as
will be discussed later, smaller scalings offer greater advantages. In this paper, we will focus solely
on examples with scalings of O(1)andO(N).
3.1 Inverse M-Kernel Models
We now consider the following linear approximator with an inverse M-kernel:
fIMK(·) =NX
n=1αnk(xn,·) =k(·)⊤α, (K+s(N+ 1)IN)α⩾0, k(·,·)∈ Fs(N)
M−1,(7)
where{xn}N
n=1is the Ndata points, α:= (α1, . . . , α N)⊤, andk(x):= (k(x1, x), . . . , k (xN, x))⊤.
Coefﬁcients αare obtained by solving the optimization problem ( 3) with constraint (K+s(N+
1)IN)α⩾0. We call this approximator an inverse M-kernel model (IMK). Then Theorem 1below
guarantees the non-negativity of the approximator ( 7).
4Theorem 1 (Non-negativity of inverse M-kernel models) .The inverse M-kernel models fIMK(x)
deﬁned by ( 7) are non-negative for any input point x∈ X.
Proof. Consider the ( N+1)-by-( N+1) gram matrix for the data points {xn}N
n=1and any point x∈ X
such that
Q=
k(x, x)k(x)⊤
k(x)K
+s(N+ 1)IN+1.
Gram matrix Qis an inverse M-matrix because k(·,·)∈ Fs(N)
M−1, and according to the ﬁrst inequality
in Lemma 1, the following relation holds: k(x)⊤(K+s(N+ 1)IN)−1⩾0. Let β∈RN
+be a
non-negative vector, β⩾0, then the following inner product of non-negative vectors completes the
proof:k(x)⊤(K+s(N+ 1)IN)−1β=k(x)⊤α≥0, forβ= (K+s(N+ 1)IN)α⩾0.■
It should be emphasized here that the comparison between the two linear models, fIMKin (7) and
fNCMin (4), suggests that our proposed fIMKshould have substantially greater representation power
thanfNCM:fIMK’s constraint on coefﬁcient, (K+s(N+ 1)IN)α⩾0, is much weaker than fNCM’s,
α⩾0. Here, the discrepancy between the two models is controlled by s(N+ 1), and fIMKreduces
tofNCMfors(N+1)→ ∞ :(K+s(N+1)IN)α⩾0⇔(s(N+1)−1K+IN)α⩾0s→∞=α⩾0.
Clearly, fIMKhas greatest representation power when s(N+ 1) is equal to zero, and we can derive
a sufﬁcient condition on fIMKso that it may be a universal approximator of non-negative function.
Theorem 2 (Condition for linear universal approximation) .The inverse M-kernel model fIMKde-
ﬁned by ( 7) is a universal approximator of non-negative functions if the kernel is universal and a
strict inverse M-kernel.
Proof. Letk:X × X → Rbe a universal kernel, and let Zbe a compact subset of X. Then the
corresponding RKHS is equal to the space of all continuous functions from Z, denoted by C(Z),
which is equipped with maximum norm || · ||C(Z). Suppose that we have a set of data points,
{(xn, g(xn))}N
n=1, for non-negative target function g:Z → R+inC(Z), and k(·,·)is a strict
inverse M-kernel k(·,·)∈ FM−1. Then we can rewrite the inverse M-kernel model ( 7) in the form
of noise-free kernel ridge regression (KRR) as:
fIMK(x) =k(x)⊤K−1˜g, ˜g:= (g(x1), . . . , g (xN))⊤⩾0.
Because constraint ˜g⩾0is satisﬁed for any {xn}N
n=1due to the non-negativity of g(·), we can
apply the generalization error bound of a normal KRR (Proposition 1 in [ 15]) to it:
||fIMK(·)−g(·)||C(Z)<sup
x∈Zq
k(x, x)−k(x)⊤K−1k(x)·Γ,
where Γis a constant. The upper bound goes to zero if N→ ∞ and{xn}N
n=1is aligned appropri-
ately, indicating that given ϵ >0, there exists {xn}N
n=1such that ||fIMK(·)−g(·)||C(Z)≤ϵ, which
completes the proof. ■
3.2 Equivalent Inverse M-Kernels for Permanental Processes
As a by-product of inverse M-kernels, we can address a well-known nodal line problem [7] on Pois-
son intensity estimation with reproducing kernels [ 6]: Given a set of Npoints{xn}N
n=1observed
for compact domain T, intensity function λ(x), an instantaneous probability of events occurring at
each point on T, is estimated by a linear model with the equivalent kernel function h(·,·)so that
λ(x) =f2(x), f (x) =NX
n=1h(x, xn)v∗
n, v∗= arg min
v∈RN−NX
n=1logf(xn) +r v⊤Hv, (8)
whereH:= [h(xn, xn′)]nn′, and h(·,·)solves an integral equation constructed by kernel function
k(·,·)ash(x, x′) + 2 /rR
Tk(x, s)h(s, x′)ds=k(x, x′);f(·)generally may have negative values,
which causes many local modes since ±f(·)can lead to similar intensity λ(·) =f2(·), resulting in
artiﬁcial zero crossings of f(·), especially on locations where the intensity is low. If h(·,·)is a strict
inverse M-kernel, then the linear model of f(·)can constitute an inverse M-kernel model ( 7), which
is non-negative at any x∈ T under a weak constraint of coefﬁcient, Hv⩾0.
5We now consider solving the integral equation for h(·,·)with the naive approach [ 10], which ap-
proximates the integral operator by J-point numerical integration, resulting in
h(x, x′) =k(x, x′)−kJ(x)⊤ 
wIJ+KJ−1kJ(x′), w =rJ/(2|T |), (9)
wherekJ(x):= (k(x, q 1), . . . , k (x, qJ))⊤,KJ:= [k(qj, qj′)]jj′,|T |:=R
Tdx, and (q1, . . . , q J)
is the regularly aligned evaluation points. Theorem 3below shows a sufﬁcient condition on the
equivalent kernel such that it may be a strict inverse M-kernel.
Theorem 3 (Equivalent inverse M-kernels) .The equivalent kernel h(·,·)deﬁned by ( 9) is a strict
inverse M-kernel if the corresponding kernel k(·,·)is a strict inverse M-kernel.
Proof. Consider the ( J+2)-by-( J+2) gram matrix for the points {qj}J
j=1and any pair of different
points (x, z̸=x)∈ T such that
Q=U+E,U=0
@k(x, x)k(x, z)kJ(x)⊤
k(z, x)k(z, z)kJ(z)⊤
kJ(x)kJ(z)KJ1
A,E=diag 
0,0, w, . . . , w
.
Ifk(·,·)∈ FM−1, thenU∈ M−1andQ=U+E∈ M−1because of the additive diagonal
closure of inverse M-matrices (Theorem 3 in [ 8]). Applying the second inequality in Lemma 1toQ
leads to the relation: 0≤k(x, z)−kJ(x)⊤ 
wIJ+KJ−1kJ(z)∈ FM−1for any pair of points
(x, z), which completes the proof. ■
Gaussian Cox processes (GCPs) are the gold standard for intensity estimation, and the intensity
estimator ( 8) is the MAP solution of the permanental process [ 18], a variant of GCP where the
square root of the intensity function is assumed to be generated from a Gaussian process. In the
literature on GCPs, the advantage of permanental processes over other GCPs has been considered as
the efﬁcient estimation algorithm, and the equivalent kernels constructed by inverse M-kernels may
improve the predictive performance of the fast-to-compute permanental processes by weakening the
coefﬁcient constraints.
3.3 Construction of Inverse M-Kernels
In the former sections, we deﬁned a new class of kernel or inverse M-kernel, and showed some
beneﬁcial results obtained from its unique properties. Now we need to tackle a practical problem
of how to construct the inverse M-kernels. Our conclusion in this paper is that for one-dimensional
input space ( X ⊆ R), we can ﬁnd some strict inverse M-kernels, which include a well-known
universal kernel called exponential/Abel/Laplace kernel; For a multi-dimensional input space, we
can ﬁnd some inverse M-kernels, but strict ones have yet to be discovered. In the following sections,
we focus on the scenario of a one-dimensional input space, which is followed by discussions of
issues with and perspectives on multi-dimensional input setting.
Corollary 1 (Examples of strict inverse M-kernels) .Exponential kernel kexp(x, x′) =e−|x−x′|/τ
and intersection kernel kint(x, x′) = min( x, x′)−γ, deﬁned on one-dimensional space x, x′∈R,
are strict inverse M-kernels. Here, τandγare the hyperparameters of exponential and intersection
kernels, respectively.
Proof. Given a set of points (x1, . . . , x N)sorted in ascending order xn< xn′forn < n′, the inverse
gram matrices of exponential and intersection kernels, denoted by K−1
expandK−1
int, respectively, are
of tridiagonal form:
(K−1
exp)nn′=8
<
:pn
n−1pn+1
n/pn+1
n−1:|n−n′|= 0
−p
pn′
n(pn′
n−1) :|n−n′|= 1
0 :|n−n′|>1, pn′
n=(
1
1−e−2|xn−xn′|/τ:n, n′∈NN
1 :otherwise,
(K−1
int)nn′=8
<
:qn
n−1qn+1
n/qn+1
n−1:|n−n′|= 0
−qn+1
n :|n−n′|= 1
0 :|n−n′|>1, qn′
n=8
><
>:1
xn′−xn:n, n′∈NN
1
xn′−γ:n= 0
1 : n′=N+ 1,
6Table 1: Results on KdV data across 100 trials with standard errors. l2is the integrated squared
error between the approximator and the ground truth, and cpuis the CPU time in second.
NCM QNM Our Model
σ l2cpu(sec) l2cpu(sec) l2cpu(sec)
0.1 .078 ±.043 .002 ±.002 .034 ±.011 .634 ±.409 .047 ±.012 .002 ±.001
0.01 .074 ±.044 .001 ±.001 .002 ±.002 3.49 ±1.04 .011 ±.001 .003 ±.001
NCM QNM Our Model
Ground truthEstimation
Data points
Figure 1: Estimated non-negative functions on KdV data with small noise, σ= 0.01.
where NN={1,2, . . . , N },τ > 0, and γ < x 1. The results show that K−1
expandK−1
inthave
non-positive off-diagonal entries while their inverses are entry-wise non-negative due to the cor-
responding kernels’ properties, indicating that Kexp,Kint∈ M−1for any {xn}N
n=1. Therefore,
kexp, kint∈ FM−1. ■
Because exponential kernel kexp(·,·)is a universal kernel as well as a strict inverse M-kernel, Theo-
rem2suggests that an inverse M-kernel model with kexp(·,·)constitutes a linear universal approx-
imator for one-dimensional input spaces. It should be emphasized here that more popular kernels
such as Gaussian and Matérn kernels are not inverse M-kernels, and thus they cannot be used to con-
struct linear universal approximators with non-negativity constraints, which highlights an important
beneﬁt of kexp(·,·)that has been overlooked in the literature. However, linear models with kexp(·,·)
are generally not smooth at data points, which is a possible disadvantage given that conventional
nonlinear models can employ smooth kernels. It remains to be clariﬁed whether there exists an
inverse M-kernel that is more smooth than kexp(·,·).
It is easily veriﬁed that linear models with intersection kernels [ 16] constitute piece-wise linear
splines. By exploiting the fact that piece-wise linear splines, whose ﬁnite set of knot values are non-
negative, have non-negative values globally, Maatouk and Bay [ 14] proposed a Gaussian process (or
equivalently a kernel method) model with non-negativity constraints, which our result re-conﬁrms
from the perspective of (strict) inverse M-kernels.
We derived the tridiagonal formulae in Corollary 1by using some known properties of symmetric
Toeplitz matrices (e.g., see [ 33]) as a reference. It is clear that the derived tridiagonal formula is a
straightforward generalization of the inverse of Toeplitz matrices, but we cannot ﬁnd any references
that explicitly mention the tridiagonal formulae of one-dimensional exponential and intersection
kernels.
4 Experiments
We examined the validity of our proposal by comparing it with conventional linear and nonlinear
models on synthetic data. Here, we considered the three problems of non-negativity-constrained
regression, density estimation, and intensity estimation. As benchmark models, we adopted non-
negative coefﬁcients model (NCM) in ( 4) and quadratic form of non-negative model (QNM) in
(5) for non-negativity-constrained regression; we adopted NCM, QNM, and squared neural fam-
ily (SNF) in ( 6) for density estimation; we adopted the intensity estimator with Gaussian kernels
(IEK) [ 6] and the structured variational Bayesian approach with sigmoidal Gaussian Cox processes
(STVB) [ 1] for intensity estimation. As our proposal, we adopted inverse M-kernel model (IMK) in
(7) for non-negativity-constrained regression and density estimation, and intensity estimator with in-
7Table 2: Results on density estimation across 100 trials with standard errors. kKLis the Kullback–
Leibler distance between the estimation and the ground truth (the lower, the better); cpuis the CPU
time in seconds.
NCM SNF QNM Our Model
dKL cpu (sec) dKL cpu (sec) dKL cpu (sec) dKL cpu (sec)
.163±.154 .024 ±.007 .452 ±.291 2.99 ±.970 .163 ±.064 .483 ±.136 .123 ±.049 .053 ±.021
NCM QNM Our Model SFN
Ground truthEstimation
Data points
Figure 2: Estimated density functions.
verse M-kernels for intensity estimation. We employed Gaussian kernel k(x, x′) =e−|x−x′|2/τ2for
NCM and QNM, and exponential kernel k(x, x′) =e−|x−x′|/τfor our IMK. The hyper-parameters
for each model were optimized through three-fold cross validation on a grid: for NCM, QNM,
and IMK, the grid is (τ, r)∈ C ⊗ C forC={0.1,0.2,0.5,1,2,5,10}; for SNF, the number of
components for Gaussian mixture measure dµ(·)was selected from {1,2,3}. We implemented all
compared models by using Python-3.10.8 (SciPy-1.11, fnnls-1.0 (MIT License))1. A MacBook Pro
with 12-core CPU (Apple M2 Max) was used.
4.1 Non-Negativity-Constrained Regression
We considered a standard regression problem with the squared loss functional, L=1
σ2PN
n=1(yn−
f(xn))2, which makes the optimization problems in ( 3andA1) convex. Here x= (x1, . . . , x N)⊤
andy= (y1, . . . , y N)⊤are the observed input and target values, respectively, and σ2is the variance
of observation noise. For NCM and IMK, each of the convex problems of coefﬁcients αcan be
recast to non-negative least squares as
NCM : min α≥0||Cα−z||2,C= [K/σ;√rU⊤], z= [y/σ; 0N],
IMK : min β≥0||Cβ−z||2,C= [I/σ;√rU−1], z= [y/σ; 0N], α=K−1β,(10)
whereUis the lower triangular matrix of the Cholesky decomposition of the gram matrix, K=
UU⊤,0Nis the N-dimensional vector with zero entries, and [a;b]represents concatenation. We
solved ( 10) with the fast nonnegative least squares [ 2]. For QNM, we solved the convex problem of
coefﬁcients Bby using the sequential least squares programming (SLSQP) [ 11].
In accordance with [ 22], we considered approximating a non-negative 2-soliton solution of the
Korteweg-de Vries (KdV) equation [ 25],g(x, t = 1) =12([3+4 cosh(2 x−8t)+cosh(4 x−64t)])
8[3 cosh( x−28t)+cosh(3 x−36t)]2, where
the posterior means of unconstrained Gaussian process (equivalently, kernel method regressions)
tend to violate non-negativity of the function [ 22]. We sampled N=40data points equidistantly
from the KdV solution with small noise σ= 0.01and large noise σ= 0.1scenarios, each of
which was conducted 100times. Then we measured the predictive performances of the models
based on the integrated squared error between the result f∗(·)and the ground truth g(x), deﬁned as
l2=Rb
a|f∗(x)−g(x)|2dxfor(a, b) = (−20,5).
Table 1displays the predictive performances achieved by the compared methods on KdV data. The
comparison between the two linear models shows that our model (IMK) outperformed NCM in both
noise scenarios, which is clearly illustrated by Figure 1: our model succeeded in approximating
the two modes with the different scales appearing in the KdV solution, while NCM failed due to
its limited representation power. See also an additional experiment in Appendix Bto illustrate
the difference in representation power between the two linear models. QNM achieved the best
performance among the models, because the underlying function is smooth and consistent with
8Table 3: Results on intensity estimation across 100 trials with standard errors. l2is the integrated
squared error between the approximator and the ground truth, and cpuis the CPU time in seconds.
IEK STVB Our Model
l2(×103) cpu(sec) l2(×103) cpu(sec) l2(×103) cpu(sec)
7.50±3.06 6.36 ±3.46 1.74 ±0.504 1006 ±54.5 2.01 ±0.678 4.53 ±3.15
Our Model
Ground truthEstimation
Data pointsIEK
* * *
STVB
Figure 3: Estimated intensity functions. Asterisks * represent nodal points.
the Gaussian kernel that QNM adopted. The difference in performance between QNM and our
model tends to shrink when observations are noisy. The advantage of our method over QNM is its
computation efﬁciency: the linearity of the model can be fully exploited to achieve learning that is
hundreds of times faster than QNM.
4.2 Density Estimation
We considered a density estimation problem with the loss functional, L=−PN
n=1logf(xn),
where x= (x1, . . . , x N)⊤are the observed samples and the optimization problems in ( 3andA1)
are convex. Approximators of density functions are required to satisfy the normalization condition,
which can be recast as a linear constraint in the linear models (NCM and IMK): h⊤α= 1 for
(h)n=R
Xk(x, xn)dx. We trained NCM, SNF, QNM, and IMK by using SLSQP [ 11].
We created 100 sets of 50 samples generated from a Gaussian mixture model: g(x) = 0 .5[N(x|0,1)
+N(x|4,0.3)], where N(·|a, b)represents a normal distribution with mean aand standard
deviation b. The predictive performances were evaluated using the Kullback–Leibler distance
(the lower, the better) between the result f∗(x)and the ground truth g(x), deﬁned as dKL=Rb
ag(x) log( g(x)/f∗(x))dxfor(a, b) = (−5,5). Table 2lists the results, which show that our
IMK achieved better performance than NCM and comparable performance while being substan-
tially faster than QNM. Table 2also shows that SNF did not perform well, which might be due to
overﬁtting to the training data, as illustrated by Figure 2. This time we assumed a small training
data set ( N= 50 ), where neural network-based models are likely to overﬁt. More careful tuning of
the hyperparameters would improve SNF’s performances, while the robustness against data size is
generally a great advantage of kernel methods.
4.3 Intensity Estimation
We considered an intensity estimation problem ( 8), where SLSQP [ 11] was used to optimize IEK
and our model. We implemented STVB with the TensorFlow code [ 1], where the number of inducing
points was set as regularly aligned 100 points within the observation domain. We created 100
sets of event sequences generated from the following intensity function: λ(x) = 50 sin2(x) + 60
9forx∈[0,5], where John and Hensman [ 7] reported that the nodal line problem was likely to
happen. The predictive performances were evaluated using the integrated squared error between the
result λ∗(x)and the ground truth λ(x), deﬁned as l2=Rb
a|λ∗(x)−λ(x)|2dxfor(a, b) = (0 ,5).
Table 3lists the results which show that our model with an equivalent inverse M-kernel achieved
better performance than the naive intensity estimator with Gaussian kernel (IEK) and comparable
performance to STVB while being substantially faster. Figure 3illustrates that IEK allowed some
artiﬁcial zero crossings ofp
λ∗(x), while our model did not.
5 Discussions
We have proposed a novel class of kernel function, called inverse M-kernel function , with which
we may construct ﬂexible and linear approximators of non-negative functions. We showed that
exponential kernels, which are known as universal kernels, are inverse M-kernel functions, and they
can construct linear universal approximators of non-negative functions for one-dimensional input
settings. We conﬁrmed the potential beneﬁts of our proposal experimentally on three problems:
non-negative function regression, density estimation, and intensity estimation.
Future Work and Limitations To the best of our knowledge, this study is the ﬁrst to clarify the ex-
istence of linear universal approximators of non-negative functions, although the result is limited to
one-dimensional input spaces. Constructing linear and ﬂexible (or universal, if possible) approxima-
tors with non-negativity constraints for multi-dimensional input space is equivalent to ﬁnding inverse
M-kernels for multi-dimensional input spaces, the difﬁculty of which can be exempliﬁed as follows.
Letk0:R×R→Rbe a positive semi-deﬁnite kernel, and consider the construction of a kernel for
a two-dimensional input space by a popular multiplicative approach: k(z, z′) =k0(x, x′)k0(y, y′)
forz= (x, y). A gram matrix of k(·,·), denoted by K, evaluated over a Cartesian grid of input loca-
tions, (x1,···, xnx)⊗(y1,···, yny), will give rise to a matrix that can be written as the Kronecker
product of two smaller gram matrices, each of which are formed by evaluating k0(·,·)over each
input location [ 24]:K=Kx⊗Ky. Ifk0(·,·)is a strict inverse M-kernel function k0∈ FM−1,
thenK−1
x,K−1
y∈ M , butK−1=K−1
x⊗K−1
y/∈ M , that is, k(z, z′)/∈ FM−1: For example,
some off-diagonal entries of K−1are non-negative, (K−1)1(nx+2)= (K−1
x)12(K−1
y)12≥0.
A possible solution to the above difﬁculty is to select a scalar ηlarge enough to satisfy the condition
of inverse M-kernel (Deﬁnition 1):K+ηI∈ M−1. For general kernel functions, the condition
is not always satisﬁed even under a very large η. However, if gram matrix Ksatisﬁes a speciﬁc
condition called strict path product condition [9], then a lower bound of η(i.e., s(N)in Deﬁnition
1) that satisﬁes K+ηI∈ M−1can be evaluated as follows.
Theorem (Theorem 4 in [ 9]).LetA= (aij)be an n-by-nentry-wise non-negative matrix with
normalized unit diagonals, n≥3. ThenA+ηI∈ M−1for all η≥n−3ifAsatisﬁes the strict
path product condition, aijajk< aik, for all distinct indices i, j, k such that 1≤i, j, k≤n.
Actually, it is easily veriﬁed that gram matrices of multiplicative exponential kernels, kexp(x, s) =Q
de−|xd−sd|/τd, satisfy the strict path product condition regardless of the dimensionality of the
input space. Therefore, the multiplicative exponential kernels are inverse M-kernels with s(N) =
N−3, that is, kexp(·,·)∈ FN−3
M−1, for multi-dimensional input spaces, which suggests that the
following inverse M-kernel model (IMK) is valid:
fIMK(x) =NX
n=1αnk(xn, x) =k(x)⊤α,(K+(N−2)I)α⩾0, k(x, s)=DY
d=1e−|xd−sd|/τd,(11)
where D≥1is the input dimensionality. However, as discussed in Section 3.1, the discrepancy
between NCM and IMK becomes small if s(N+ 1) for the condition (K+s(N+ 1)I)α⩾0is
large, and thus s(N+ 1) = N−2implies that improvements of IMK ( 11) against NCM ( 4) should
exist but are likely to be marginal for N≳10. Because the lower bound η≥N−3in the theorem
above is not tight, a pressing need is to develop a method to ﬁnd a smaller value of ηsatisfying
(K+ηI)∈ M−1given kernel k(·,·)and input points (x1, . . . , x N).
10References
[1]Virginia Aglietti, Edwin V. Bonilla, Theodoros Damoulas, and Sally Cripps. Structured vari-
ational inference in continuous Cox process models. In Advances in Neural Information Pro-
cessing Systems 32 , 2019.
[2]Rasmus Bro and Sijmen De Jong. A fast non-negativity-constrained least squares algorithm.
Journal of Chemometrics: A Journal of the Chemometrics Society , 11(5):393–401, 1997.
[3]Jean-Paul Chiles and Pierre Delﬁner. Geostatistics: Modeling Spatial Uncertainty , volume
713. John Wiley & Sons, 2012.
[4]David Collett. Modelling Survival Data in Medical Research . Chapman and Hall/CRC, 2023.
[5]Sébastien Da Veiga and Amandine Marrel. Gaussian process modeling with inequality con-
straints. In Annales de la Faculté des sciences de Toulouse: Mathématiques , volume 21, pages
529–555, 2012.
[6]Seth Flaxman, Yee Whye Teh, and Dino Sejdinovic. Poisson intensity estimation with repro-
ducing kernels. In Artiﬁcial Intelligence and Statistics , pages 270–279. PMLR, 2017.
[7]S. T. John and James Hensman. Large-scale Cox process inference using variational Fourier
features. In International Conference on Machine Learning , volume 80, pages 2362–2370.
PMLR, 2018.
[8]Charles R. Johnson. Inverse M-matrices. Linear Algebra and its Applications , 47:195–216,
1982.
[9]Charles R. Johnson and Ronald L. Smith. Positive, path product, and inverse M-matrices.
Linear Algebra and Its Applications , 421(2-3):328–337, 2007.
[10] Hideaki Kim, Taichi Asami, and Hiroyuki Toda. Fast Bayesian estimation of point process
intensity as function of covariates. In Advances in Neural Information Processing Systems 35 ,
2022.
[11] Dieter Kraft. A software package for sequential quadratic programming. Forschungsbericht-
Deutsche Forschungs- und Versuchsanstalt fur Luft- und Raumfahrt , 1988.
[12] Miao Liu, Girish Chowdhary, Bruno Castra Da Silva, Shih-Yuan Liu, and Jonathan P. How.
Gaussian processes for learning and control: A tutorial with examples. IEEE Control Systems
Magazine , 38(5):53–86, 2018.
[13] Weifeng Liu, Jose C. Principe, and Simon Haykin. Kernel Adaptive Filtering: A Comprehen-
sive Introduction . John Wiley & Sons, 2011.
[14] Hassan Maatouk and Xavier Bay. Gaussian process emulators for computer experiments with
inequality constraints. Mathematical Geosciences , 49:557–582, 2017.
[15] Emilio Tanowe Maddalena, Paul Scharnhorst, and Colin N. Jones. Deterministic error bounds
for kernel-based learning techniques under bounded noise. Automatica , 134:109896, 2021.
[16] Subhransu Maji, Alexander C. Berg, and Jitendra Malik. Classiﬁcation using intersection
kernel support vector machines is efﬁcient. In 2008 IEEE Conference on Computer Vision and
Pattern Recognition , pages 1–8. IEEE, 2008.
[17] Ulysse Marteau-Ferey, Francis Bach, and Alessandro Rudi. Non-parametric models for non-
negative functions. In Advances in Neural Information Processing Systems 33 , 2020.
[18] Peter McCullagh and Jesper Møller. The permanental process. Advances in Applied Probabil-
ity, 38(4):873–888, 2006.
[19] Charles A. Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. Journal of
Machine Learning Research , 7(12), 2006.
[20] Radford M. Neal. Bayesian Learning for Neural Networks , volume 118. Springer Science &
Business Media, 2012.
11[21] John Ashworth Nelder and Robert W.M. Wedderburn. Generalized linear models. Journal of
the Royal Statistical Society Series A: Statistics in Society , 135(3):370–384, 1972.
[22] Andrew Pensoneault, Xiu Yang, and Xueyu Zhu. Nonnegativity-enforced Gaussian process
regression. Theoretical and Applied Mechanics Letters , 10(3):182–187, 2020.
[23] Robert J. Plemmons. M-matrix characterizations. I–nonsingular M-matrices. Linear Algebra
and its Applications , 18(2):175–188, 1977.
[24] Yunus Saatçi. Scalable Inference for Structured Gaussian Process Models . PhD thesis, Uni-
versity of Cambridge, 2011.
[25] Nicolas Schalch. The Korteweg-de Vries Equation. Proseminar: Algebra, Topology and Group
Theory in Physics , 2018.
[26] Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Support Vector Ma-
chines, Regularization, Optimization, and Beyond . MIT press, 2018.
[27] Bernhard Schölkopf, Ralf Herbrich, and Alex J. Smola. A generalized representer theorem. In
International Conference on Computational Learning Theory , pages 416–426. Springer, 2001.
[28] Bernhard Schölkopf, Koji Tsuda, and Jean-Philippe Vert. Kernel Methods in Computational
Biology . MIT press, 2004.
[29] John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis . Cambridge
University Press, 2004.
[30] Bharath K. Sriperumbudur, Kenji Fukumizu, and Gert R.G. Lanckriet. Universality, character-
istic kernels and RKHS embedding of measures. Journal of Machine Learning Research , 12
(7), 2011.
[31] Laura P. Swiler, Mamikon Gulian, Ari L. Frankel, Cosmin Safta, and John D. Jakeman. A sur-
vey of constrained Gaussian process regression: Approaches and implementation challenges.
Journal of Machine Learning for Modeling and Computing , 1(2), 2020.
[32] Hiroyuki Takeda, Sina Farsiu, and Peyman Milanfar. Kernel regression for image processing
and reconstruction. IEEE Transactions on Image Processing , 16(2):349–366, 2007.
[33] William F. Trench. Properties of some generalizations of Kac-Murdock-Szegö Matrices. Con-
temporary Mathematics , 281:233–245, 2001.
[34] Russell Tsuchida, Cheng Soon Ong, and Dino Sejdinovic. Squared neural families: A new
class of tractable density models. In Advances in Neural Information Processing Systems 36 ,
2023.
[35] Grace Wahba. Spline Models for Observational Data , volume 59. SIAM, 1990.
12NCM QNM Our Model
Ground truthEstimation
Data pointsN=20 N=100 N=20 N=100 N=20 N=100Figure B1: Estimation results of regression problem on ground truth g(x) =e−|x|2by using ref-
erence models (NCM and QNM) with Gaussian kernel e−|x−x′|2/4and our model with inverse
M-kernel e−|x−x′|/2.
A Details of QNM
Marteau-Ferey et al. [ 17] considered the following problem of positive semi-deﬁnite operator A,
inf
AL(fA(x1), . . . , f A(xN)) +r||A||∗+r2||A||2
F, (A1)
where||·||∗and||·||2
Fare the nuclear norm and the squared Frobenius norm, respectively, and fA(·)
is deﬁned as fA(·) =ϕ(·)⊤Aϕ(·)forϕ(·)the feature map of A. Then they showed that the solution
of (A1) holds for a representer theorem, which leads to the quadratic form of approximator ( 5). The
coefﬁcient matrix Bin (5) is obtained efﬁciently by solving the N-dimensional dual problem,
α∗= arg sup
α∈RN−L∗(α)−1
2r2||
Vdiag(α)V⊤+rI
−||2
F, (A2)
B=r−1
2V−1
Vdiag(α∗)V⊤+rI
−V−⊤, (A3)
where [A]−represents the negative part of A(for details, see [ 17]),Vis the Cholesky de-
composition of K:= [k(xn, xn′)]nn′, i.e.,K=V⊤V,L∗(α) =PN
n=1l∗(αn)represents
the Fenchel conjugate of the loss functional L(z1, . . . , z N) =PN
n=1l(zn). More concretely,
l∗(αn) = ynαn+1
2σ2α2
nforl(zn) =1
2σ2(zn−yn)2, and l∗(αn) =−(1 + log( −αn))for
l(zn) =−log(zn).
B Additional Results
To clearly discern the difference in representation power between the two linear models, NCM and
IMK, we conducted experiments on N=20andN=100data points sampled equidistantly from
g(x) =e−|x|2with noise σ= 0.01, where we set the scale parameter τof kernel function to be twice
as large as the ground truth, τ= 2, for all models. Figure B1displays the results: NCM failed to
recover the functional form of the ground truth even with a large number of training points, while our
IMK, which invokes the universal approximation under a strict inverse M-kernel function, achieved
good estimation results. As emphasized by Marteau-Ferey et al. [ 17], NCM cannot approximate a
function with a scale strictly smaller than the used kernel’s scale well, which raises a problematic
trade-off when the underlying function has different scales of components: adjusting τto the smaller
scale components causes overﬁtting in the larger scale ones, while adjusting τto the larger scale ones
fails to recover the smaller scale ones. The experiments in Section 4.1replicated this problematic
situation.
13NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reﬂect the
paper’s contributions and scope?
Answer: [Yes]
Justiﬁcation: Section 3is the main contribution of this paper, which is claimed in the
abstract and introduction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reﬂect how
much the results can be expected to generalize to other settings.
•It is ﬁne to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justiﬁcation: The limitations are clearly discussed in Sections 3.3and5.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
•The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-speciﬁcation, asymptotic approximations only holding locally). The au-
thors should reﬂect on how these assumptions might be violated in practice and what
the implications would be.
•The authors should reﬂect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reﬂect on the factors that inﬂuence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
•The authors should discuss the computational efﬁciency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be speciﬁcally instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
14Justiﬁcation: The theoretical results are numbered and proved in the main paper.
Guidelines:
•The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
•Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justiﬁcation: All the information needed to reproduce the experimental results are in Sec-
tion4. Also, we will release the code and synthetic data to reproduce the results at github
page.
Guidelines:
•The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or veriﬁable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
15Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justiﬁcation: We will release the code and synthetic data to reproduce the results at github
page. We submitted the code and synthetic data to reviewers.
Guidelines:
•The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so ʠNoʡis an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justiﬁcation: All the training and test details are in Section 4.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Signiﬁcance
Question: Does the paper report error bars suitably and correctly deﬁned or other appropri-
ate information about the statistical signiﬁcance of the experiments?
Answer: [Yes]
Justiﬁcation: All predictive/computational performances in Section 4are reported with
standard errors.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, conﬁ-
dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
16•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
•The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not veriﬁed.
•For asymmetric distributions, the authors should be careful not to show in tables or
ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding ﬁgures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufﬁcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justiﬁcation: Information on the computer resources is stated in the ﬁrst paragraph of Sec-
tion4.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justiﬁcation: We read the NeurIPS Code of Ethics, and conﬁrmed that our research conform
with it.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justiﬁcation: This work is not tied to particular applications, and does not present any
foreseeable societal consequence.
Guidelines:
•The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
17•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
ciﬁc groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efﬁciency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justiﬁcation: Synthetic data used in this paper is not tied to particular applications.
Guidelines:
•The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety ﬁlters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justiﬁcation: We used python package fnnls, of which version and license are mentioned
in Section 4.
Guidelines:
•The answer NA means that the paper does not use existing assets.
•The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
•The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
18•If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [Yes]
Justiﬁcation: We will also release code to reproduce the dataset used in the paper.
Guidelines:
•The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip ﬁle.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
•Including this information in the supplemental material is ﬁne, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
•Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
19•We recognize that the procedures for this may vary signiﬁcantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
20