P2C2Net: P DE-Preserved Coarse Correction Network
for Efficient Prediction of Spatiotemporal Dynamics
Qi Wang1, Pu Ren2, Hao Zhou1, Xin-Yang Liu3, Zhiwen Deng4, Yi Zhang4, Ruizhi Chengze4,
Hongsheng Liu4, Zidong Wang4, Jian-Xun Wang3, Ji-Rong Wen1, Hao Sun1,∗, Yang Liu5,∗
1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China
2Department of Civil and Environmental Engineering, Northeastern University, Boston, MA, USA
3Department of Aerospace and Mechanical Engineering, University of Notre Dame, Notre Dame, IN, USA
4Huawei Technologies, Shenzhen, China
5School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China
Emails: qi_wang@ruc.edu.cn (Q.W.); haosun@ruc.edu.cn (H.S.); liuyang22@ucas.ac.cn (Y .L.)
Abstract
When solving partial differential equations (PDEs), classical numerical methods
often require fine mesh grids and small time stepping to meet stability, consis-
tency, and convergence conditions, leading to high computational cost. Recently,
machine learning has been increasingly utilized to solve PDE problems, but they
often encounter challenges related to interpretability, generalizability, and strong
dependency on rich labeled data. Hence, we introduce a new P DE-Preserved Coarse
Correction Network (P2C2Net) to efficiently solve spatiotemporal PDE problems
on coarse mesh grids in small data regimes. The model consists of two synergistic
modules: (1) a trainable PDE block that learns to update the coarse solution (i.e.,
the system state), based on a high-order numerical scheme with boundary condition
encoding, and (2) a neural network block that consistently corrects the solution on
the fly. In particular, we propose a learnable symmetric Conv filter, with weights
shared over the entire model, to accurately estimate the spatial derivatives of PDE
based on the neural-corrected system state. The resulting physics-encoded model
is capable of handling limited training data (e.g., 3–5 trajectories) and accelerates
the prediction of PDE solutions on coarse spatiotemporal grids while maintaining a
high accuracy. P2C2Net achieves consistent state-of-the-art performance with over
50% gain (e.g., in terms of relative prediction error) across four datasets covering
complex reaction-diffusion processes and turbulent flows.
1 Introduction
Complex spatiotemporal dynamical systems are pivotal and commonly seen in numerous fields such
biology, meteorology, fluid mechanics, etc. The behaviors of these systems are primarily governed by
partial differential equations (PDEs), conventionally solved by numerical methods [ 1–4]. However,
direct numerical simulation (DNS) demands in-depth knowledge of the underlying physics, and
the efficacy of numerical solutions is intricately linked to the resolution of mesh grids and time
steps. High-resolution spatiotemporal grids are essential for accurate and convergent calculations,
yet leading to substantial computational costs. For instance, simulating the flow field around a large
aircraft [ 5,6] involves creating over millions of mesh nodes and may consume vast simulation time
even on high performance computing. Additionally, any changes in initial and boundary conditions
(I/BCs) or design parameters necessitate recalculations, further compounding the complexity.
∗Corresponding authors
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Recently, tremendous efforts have been placed on machine learning for data-driven simulation of
these systems [ 7–9], demonstrating promising potential. These methods do not require the a priori
knowledge of physics and, meanwhile, help bypass some traditional constraints, e.g., the smallest size
of mesh grid and time step to guarantee solution accuracy, stability and convergence [ 10]. However,
they typically face issues of poor interpretability, weak generalizability and strong dependency of
rich labeled data. Their performance deteriorate significantly particularly in small data regimes.
Embedding prior physics knowledge into the learning process has demonstrated effective to overcome
the aforementioned issues. A brute-force way lies in creating regularizers (e.g., the residual form of
PDEs and I/BCs) as “soft” penalty in the loss function, e.g., the family of physics-informed neural
works (PINNs) [ 11–17]. However, such a strategy has limited scalability and generalizability, and the
solution accuracy relies largely on a proper selection of loss weight hyperparameters. Embedding
physics explicitly into the network architecture, which imposes “hard” constraints such as physics-
encoded recurrent convolutional neural network (PeRCNN) [ 18,19], possesses better generalizability
as well as offers better convergence and flexibility for model training without the need of fine-tuning
hyperparameters. Nevertheless, existing methods fail to handle coarse mesh grids and suffer from
instability issues especially for long-range prediction of dynamics. Hybridizing classical numerical
schemes and neural networks, e.g., the learned interpolation (LI) model [ 20], can enable accelerated
simulation on coarse mesh grids with satisfied accuracy. Yet, since the numerical part is non-trainable,
such models still require rich labeled data to retain accuracy.
To tackle these critical challenges, we introduce the P2C2Net model for efficient prediction of
spatiotemporal dynamics on coarse mesh grids in small training data regimes. Specifically, a trainable
PDE block (a white box) is designed to learn the coarse solution at low resolution, where the temporal
marching of system states is handled by a fourth-order Runge-Kutta (RK4) scheme. We also propose
a learnable symmetric Conv filter for more accurate estimation of spatial derivatives on coarse grids,
as required in PDE block. A neural network (NN) block, which serves as a correction module, is
further introduced to correct the coarse solution, restoring information lost due to reduced resolution.
We also encode BC into the solution via a padding strategy. Our primary contributions are threefold:
•We propose a new physics-encoded correction learning model (P2C2Net) to efficiently
predict complex spatiotemporal dynamics on coarse mesh grids. The model requires only a
small set of training data and possesses plausible generalizability.
•We introduce a structured Conv filter that preserves symmetry to improve the estimation
accuracy of coarse spatial derivatives required in the solution updating process, which makes
the PDE block trainable with flexibility of handling coarse grids.
•P2C2Net achieves consistent state-of-the-art performance with at least 50% gain (e.g., in
terms of relative prediction error) across four datasets covering complex reaction-diffusion
(RD) processes and turbulent flows, simultaneously retaining accuracy and efficiency.
2 Related work
Numerical methods. Conventionally, PDE systems are solved by classical numerical methods
such as finite difference [ 1], finite volume [ 2], finite element [ 3], and spectral methods [ 4]. These
methods often require fine mesh grids and reasonable time stepping to meet stability, consistency
and convergence conditions. When dealing with large scale simulations or inverse analyses, the
computational costs remain remarkably high.
Deep learning methods. Given sufficient labeled training data, deep learning has been recently
applied to solve PDE problems. Representative approaches include Conv-based NN models [ 9,21],
U-Net [ 22], ResNet [ 23], graph neural networks [ 24,25], and Transformer-based models [ 26–28]. In
addition, neural operators such as DeepONet [ 7], multiwavelet-based model (MWT) [ 29], Fourier
neural operator (FNO) [ 8], and their variants [ 30–32] have been designed to directly learn mappings
between function spaces, making them particularly well-suited for modeling PDE systems. Diffusion
models have also been employed for prediction of spatiotemporal dynamics [33].
Physics-aware learning methods. Recently, physics-aware deep learning has demonstrated great
potential in modeling spatiotemporal dynamics under conditions of small training data. This paradigm
can be divided into two categories based on the way of embedding PDE information: (1) physics-
informed , and (2) physics-encoded . The former formalizes PDE soft constraints including equations
2and I/BCs via loss regularization on point-wise or mesh-based NNs (e.g., PINN [ 11–13,17], Phy-
GeoNet [ 34], PhyCRNet [ 35], PhySR [ 36], etc.), while the latter imposes hard constraints via
encoding PDE structures (e.g., equations, I/BCs, law of thermodynamics, symmetry) into NN archi-
tectures such as PeRCNN [ 18,19], TiGNN [ 37], and EquNN [ 38]. Other related works include the
PDE-Net models [ 39,40] with designed convolution kernels approximating differential operators,
thereby modeling the dynamics of the system.
Hybrid learning methods. The hybrid learning scheme represents a novel research direction that
has emerged in recent years, which integrates numerical methods with NNs. The resulting solver can
leverage a variety of classical numerical methods such as finite difference (e.g., PPNN [ 41], numerical
discretization learning [ 42]), finite volume (e.g., LI [ 20] and TSM [ 43], and spectral methods (e.g.,
machine-learning-augmented spectral solver [ 44]). These approaches can operate on coarse grids,
enabling faster simulations compared with traditional numerical solvers while retaining accuracy.
However, since the numerical part is non-trainable, such models generally require rich labeled data.
3 Methodology
3.1 Problem formulation
Let us consider a spatiotemporal dynamical system governed by the general form of PDEs:
∂u
∂t− F(u,u2,···,∇u,∇2u,···;µ) =f, (1)
where u(x, t)∈Rndenotes the n-dimensional physical state within spatiotemporal domain Ω×[0, T];
∂u/∂tthe first-order time derivative; Fa linear/nonlinear function; ∇∈Rnthe Nabla operator; µ
the PDE parameters (e.g., the Reynolds number Re);fthe external force (e.g., f=0for source-free
cases). Besides that, we define the initial condition (IC) as I(u,ut;x∈Ω, t= 0) = 0and the
boundary condition (BC) as B(u,∇u,···;x∈∂Ω) =0, where ∂Ωdenotes the boundary of Ω.
Our aim is to develop a learnable coarse model that accelerates the simulation and prediction of
spatiotemporal dynamics based on a minimal set of sparse data (e.g., low-resolution data down-
sampled across space and time). The learned model is expected to achieve high solution accuracy and
superior generalizability over various PDE scenarios, including ICs, force terms, and PDE parameters.
3.2 Network architecture
Herein, we introduce the P2C2Net architecture, as shown in Figure 1, taking the simulation of
Navier-Stokes (NS) flows as an example. The model is composed of four blocks, namely, the state
variable correction block, the learnable PDE block, the Poisson block, and the NN block. Note that
the network architecture is flexible and features a Poisson block that solves for the pressure term p,
which is absent in other cases.
3.2.1 The flow of data
In Figure 1 ( a), the network architecture includes two paths: the upper path computes the coarse
solution using a learnable PDE block, while the lower path is incorporated into the network to correct
the solution on a coarse grid with a Poisson block and a NN block. The data flow operates as follows:
(1) the network accepts ukas input and processes it by the PDE block on the upper path, where the
PDE block computes the residual of the governing equation. A filter bank, defined as a learnable filter
with symmetry constraints, calculates the derivative terms based on the corrected solution (produced
by the correction block). These terms are combined into an algebraic equation (a learnable form of
F). This process is incorporated into the RK4 integrator for solution update. (2) In the lower path, uk
is first corrected by the correction block, and pkis computed by the Poisson block. Inputs, including
solution states {uk, pk}and their derivative terms, forcing term, and Reynolds number, are fed into
the NN block. The output from this block serves as a correction for the upper path. (3) The final
result uk+1is obtained by combining the outputs from both the upper and lower paths. During the
gradient back-propagation process, the NN block learns to correct the coarse solution output of the
PDE block on the fly, and ensures that their combined results more closely approximate the ground
truth solution.
3d ba
e fg
Symmetry
 ProjectionCollect feature terms
 
Neural
Network
             - Fast Fourier Transform
             - Inverse Fast Fourier Transform- Wavenumbers in the    and    directionAssemble
Poisson
Solverc
Equation
Residue BlockPoisson
Block
Filter
BankFilter
BankFilter
BankPDE Block Poisson Block NN Block
Poisson SolverCorrection
Block
Collect 
derivatives
Notation
- external force- Reynold number
- pressure field
- coarse solution of velocity field at 
- corrected solution of velocity field at
- learned Nabla operator 
- PDE parameters 
PDE Block
Correction
BlockCopy
NN
 BlockPoisson
Block
O OI I I
?
? ?Figure 1: Schematic of P2C2Net for learning Navier-Stokes flows. ( a), Overall model architecture. ( b), Poisson
block. ( c), learnable PDE block. ( d), NN block. ( e), Poisson solver. ( f), Symbol notations. ( g), Conv filter with
symmetric constraint.
3.2.2 RK4 integration scheme
Similar to standard numerical solvers, the goal is to predict the solution at every time step satisfying
the underlying PDEs given specific I/BCs. Here, we aim to address the challenge of spatiotemporal
dynamics evolution on coarse grids (e.g., low resolution). Given the coarse solution at timestep tk,
denoted by uk, we expect the model yielding an accurate prediction of uk+1,v.i.z.,
uk+1=uk+Ztk+1
tk
H 
u(˜x, τ),u2(˜x, τ),···,∇u(˜x, τ),∇2u(˜x, τ),···;µ
+f(τ)
dτ (2)
where H(the PDE block) denotes a learnable form of F, which is discussed in Section 3.2.3; ˜x
represents the coarse grid coordinates. We herein employ the RK4 scheme as the integrator for time
marching of the dynamics, offering the fourth-order accuracy ( O(δt4)), where δt=tk+1−tkis the
coarse time step. More details on RK4 can be found in Appendix Section B.3.
3.2.3 Learnable PDE Block
We assume that the PDE formulation in Eq. (1) is given, e.g., the explicit expression of Fis known.
With coarse mesh grids and large time stepping, numerical methods such as finite difference (FD) tend
to diverge. This issue becomes more pronounced with greater grid coarsening. Hence, we propose a
learnable PDE block (depicted in Figure 1( c)), denoted by H, to approximate Fon coarse grids, so
as to enable the coarse simulation simultaneously retaining efficiency and accuracy, expressed as
H 
uk,u2
k,···,∇uk,∇2uk,···;µ
← F
uk,u2
k,···,ˆ∇ˆuk,ˆ∇2ˆuk,···;µ
(3)
where ukdenotes the coarse solution at time tk;ˆukthe corresponding neural-corrected coarse
solution state, obtained by the correction block shown in Figure 1( a) and ( c), used for estimation of
spatial derivatives, e.g., ˆuk=NN(uk). Here, ˆ∇represents a learnable Nabla operator composed of
a Conv filter with symmetric constraint (e.g., an enhanced FD kernel for numerical approximation
of spatial derivatives). Through the RK4 integration, we are then able to predict the coarse solution
for the next time step. Note that the PDE parameters µcan be set as trainable if unknown. Despite
information loss at the low resolution, such a learnable PDE block allows for more accurate derivative
estimation on coarse grids and ensures better adherence of the updated coarse solution to underlying
PDEs. Clearly, such a block adds a fully interpretable “white box” to the overall network architecture.
4Correction block: The correction block takes a NN to correct the coarse solution. In particular,
we choose FNO [ 8] as the neural correction operator performed on the coarse solution field in
such a block. In the Fourier space, FNO decomposes the input data into components with specific
frequencies, processes each component separately, and then applies Fourier transform to restore the
updated spectral features back to the physical domain. The layer-wise update can be expressed as:
vl+1(˜x) =σ 
Wlvl(˜x) + 
K(ϕ)vl
(˜x)
(4)
where, vl(˜x)denotes the l-th layer latent feature map on the coarse grids ˜x. Note that v0(˜x) =P(uk),
in which Pis a local transformation function that lifts ukto a higher dimensional representation. Here,
K(ϕ)(z) =iFFT(Rϕ·FFT(z))denotes a kernel integral transformation of the latent feature map z,
which encompasses Fourier transform, spectral filtering and convolution in the frequency domain Rϕ,
and inverse Fourier transform; ϕthe network parameters; σ(·)the GELU activation function; Wl
the weights of a linear transformation. Given an L-layer FNO, the corrected coarse solution reads
ˆuk=Q 
vL(˜x)
, where Qis a local projection function. Details of the FNO correction block are
found in Appendix Section B.1.
Conv filter with symmetric constraint: Based on FD schemes, spatial derivatives can be ap-
proximated by central difference stencils represented by convolution kernels [ 35,39–41]. Such an
approach often requires fine mesh grids to ensure accuracy; otherwise on coarse grids, there exists
solution divergence issue. To this end, we leverage our understanding of FD stencils and propose a
Conv filter with symmetric constraints to improve the accuracy of spatial derivative approximation on
coarse mesh grids. As shown in Figure 1( g), the filter weights are transformed into a 5×5matrix
gwith 7 learnable parameters (Table S1 demonstrates the significant differences in results across
various kernel sizes when applied to the Burgers dataset.), which satisfies symmetry, to estimate the
first-order derivative along the horizontal direction (e.g., the vertical direction takes gT),v.i.z.,
g=
a1 a4 a7−a4−a1
a2 a5−2a7−a5−a2
a3 a6 0−a6−a3
−a2−a52a7 a5 a2
−a1−a4−a7a4 a1
(5)
Although the number of learnable parameters in the Conv kernel is limited, the coarse derivatives
can still be accurately approximated after the model is trained (see the ablation study in Section 4.2).
The structured filter is designed for Conv operation which satisfies the Order of Sum Rules stated in
Lemma 1 [39] (see Appendix Section A for more details).
Lemma 1: A 2D filter gm×mhas sum rules of order ι= (ι1, ι2), where ι∈Z2
+, provided that
m−1
2X
k1=−m−1
2m−1
2X
k2=−m−1
2kα1
1kα2
2g[k1, k2] = 0 (6)
for all α= (α1, α2)∈Z2
+with|α|:=α1+α2<|ι|and for all α∈Z2
+with|α|=|ι|butα̸=ι. If
this holds for all α∈Z2
+with|α|< A except for α̸= ˇαwith certain ˇα∈Z2
+and|α|=B < A ,
then we say gto have total sum rules of order A\ {B+ 1}.
Corollary 1: The filter gwe designed in Eq. (5) has the sum rules of order (1, 0). By adjusting the
parameters in g, e.g., a7→0,a6+ 8a5→0, it satisfies the total sum rules of order 5\ {2}, and
achieves an approximation to the first-order derivative with the fourth-order accuracy. For example,
for a smooth function ω(x, y)and small perturbation ε >0, we have [39]:
1
εm−1
2X
k1=−m−1
2m−1
2X
k2=−m−1
2g[k1, k2]ω(x+εk1, y+εk2) =C∂ω(x, y)
∂x+O(ε4),asε→0.(7)
The proof of Corollary 1 can be found in Appendix Section A. Given that the order of sum rules
is closely related to the order of vanishing moments in the wavelet theory, Lemma 1 ensures that
the filter not only maintains high sensitivity to local changes in the feature but also effectively
suppresses irrelevant low-frequency components, thereby enhancing the estimation accuracy of
the first derivative [ 39]. Furthermore, Corollary 1 further guarantees that the designed filter can
approximate the first-order derivative with up to the fourth-order accuracy by properly adjusting the
trainable parameters.
581 82 83 84 85 86 87 88 89 90
91 92 93 94 95 96 97 98 99100
1 2 3 4 5 6 7 8 910
1112 1314 15 16 17 18 19 20
81 82 83 84 85 86 87 88 89 90
91 92 93 94 95 96 97 98 99100
1 2 3 4 5 6 7 8 910
1112 13 14 15 16 17 18 19 20
Filter Padding Nodes Internal NodesFigure 2: Periodic BC padding.BC encoding: To ensure that the predicted solution com-
plies with the given BCs, while also retaining the shape
of the feature map after Conv operations, we employ a
BC hard encoding method [ 19]. In particular, we consider
periodic BCs in this work and apply padding padding, as
shown in Figure 2, on the predicted solutions. Such an
encoding technique not only ensures the compliance of
the predicted solution with the BCs, but also enhances the
solution’s accuracy.
3.2.4 Poisson block
When solving NS equations, there exists the pressure term
pin the governing PDEs. However, for incompressible
flows, the pressure can be inferred by solving a Poisson
equation. We designed this block to achieve this aim, the
poisson equation namely, ∆p=ψ(u,f), where ψ(u,f) =
−2 (uyvx−uxvy)+∇·ffor 2D problems and the subscripts denote spatial differentiation. Hence, we
leverage the spectral method to estimate the pressure quantity (see Appendix Section B.2), as depicted
in Figure 1( e), which updates pkbased on ψ(uk,fk). The resulting Poisson block (Figure 1( b))
efficiently derives the pressure field from the velocity field and external force on the fly, streamlining
computations without the need for labeled training data of pressure.
3.2.5 NN block
It is noted that the predictions by the learnable PDE block on coarse grids may encounter instability
issue due to error accumulation over time, especially in scenarios involving long-range rollout
prediction. Hence, we propose an additional NN block to consistently correct the coarse solution
predicted by the PDE block on the fly, restoring information lost due to reduced resolution. This
module can be any trainable NN model, such as FNO [ 8], DenseCNN [ 41], or UNet [ 13,45]. In
particular, we consider FNO as the NN block, with more details found in Appendix Section B.1.
However, the NN block is not always necessary. For cases of simpler systems, such as the Burgers
equation, the learnable PDE block alone can perform well where the NN block can be omitted.
3.2.6 Model generalization
We herein introduce the setup of the model’s generalizability over various PDE scenarios, including
ICs, force terms, and PDE parameters (e.g., the Reynolds number Re). The time marching in our
network design inherently integrates ICs, automatically ensuring generalization over ICs given a
well trained model. We embed the force term in the learnable PDE Block (naturally in the PDE
formulation) shown in Figure 1( c), and meanwhile incorporate it as a feature map into the NN Block
as part of the input (see Figure 1( c)). This enables the joint learning of force feature variations for
generalization. In addition, the Reynolds number ( Re) is incorporated by creating a 2D feature
map embedding (e.g., for the 2D case) by introducing two trainable vectors aandb, namely,
Reembb= 1/Re·(a⊗b), in both the PDE Block and the the NN Block. Such an approach can
correct the error propagation of the diffusion term in the PDE Block caused by coarse grids, and
enhance the model’s ability of generalization across different Re’s.
4 Experiments
We evaluate the performance of P2C2Net against several baseline models across diverse PDE
systems, including fluid flows and RD processes. The results have demonstrated the supe-
riority of our model in terms of solution accuracy and generalizability thanks to the unique
design of embedding PDEs into the network. The source codes and data are found at
https://github.com/intell-sci-comput/P2C2Net (PyTorch) and https://gitee.com/
intell-sci-comput/P2C2Net.git (MindSpore).
6Table 1: Summary of datasets and training implementations. Note that “ →” denotes the downsampling process
from the original resolution (simulation) to the low resolution (training).
Dataset Numerical Spatial Time Steps # of Training # of Testing Rollout
Method Grid (Temporal Grid) Trajectories Trajectories Steps
Burgers FD 1002→252400 5 10 20
FN FD 1282→6425500→1375 3 10 32
GS FD 1282→3224000→1000 3 10 50
NS FV 20482→642153600 →4800 5 10 32
RMSE RMSE
UNet
 0.00d
ModelsError distribution Error propagation Ref. FNO LI
Snapshots at t = 7 s 5.00
 0.00
-5.00P2C2Net
 PeRCNN
 18.0
-18.0RMSEUNet
 Ref. FNO DeepONet
Snapshots at t = 1900 sb P2C2Net
 PeRCNN
NaN
NaN
Error distribution
Error distribution c Error propagation UNet FNO Ref. DeepONet
Snapshots at t = 10 sP2C2Net
 PeRCNN
Ref. FNO UNet PeRCNN
Snapshots at t = 1.4 sa
 DeepONet
ModelsError distributionRMSEP2C2Net
Models
Models
Error propagation
Error propagation
-0.36-0.27
-0.46
-0.11-0.02
-0.21
 0.60 1.03
 0.17
 0.29 0.61
-0.04
 0.15 0.42
-0.12-0.05 1.02
-1.12
NaN
NaN
NaN
Figure 3: An overview of the comparison between our P2C2Net and baseline models, including error distributions
(left), error propagation curves (middle), and predicted solutions (right). ( a)-(d) show the qualitative results on
Burgers, GS, FN, and NS equations, respectively. These PDE systems are trained with grid sizes of 25 ×25,
32×32, 64×64, and 64 ×64 accordingly.
4.1 Setup
Datasets. We consider four PDE datasets, including 2D Burgers, FitzHugh-Nagumo (FN), Gray-
Scott (GS), and NS equations. Each dataset exhibits intricate spatiotemporal patterns, presenting
significant challenges for prediction on coarse grids. High-order FD and finite volume (FV) methods
are utilized to generate these datasets. We segment the training samples based on different rollout
lengths. Additionally, the simulated high-resolution data u∈Rnt×n×H×Wis down-sampled across
space and time to low-resolution counterparts ˜u∈Rn′
t×n×H′×W′where n′
t< n t,H′< H ,
W′< W . The low-resolution data serves as labels for training. Moreover, for each PDE case, only
3∼5 sets of data trajectories are used for training while 10 trajectories are applied to test the model
performance. The summary of datasets and training configuration is shown in Table 1. More details
regarding datasets and their simulations can be found in the Appendix (see Table S2 and Section E).
7Evaluation metrics. Four metrics are used to assess the model’ performance: Root Mean Square
Error (RMSE), Mean Absolute Error (MAE), Mean Normalized Absolute Difference (MNAD), and
High Correction Time (HCT). The definition of each metric is listed in Appendix Section D.
Model training. Given the low-resolution training data, we aim to learn the evolution of spatiotem-
poral dynamics on coarse grids with bigger time stepping. This learning task is formulated as an
auto-regressive rollout problem, where the models are only constrained by a low-resolution data loss.
The loss function is defined as J(θ) =1
BNPB
i=1PN
j=1MSE
˘Sij,Sij
, where ˘Sijdenotes the
rollout-predicted coarse solution for the j-th sample in the i-th batch, Sijthe corresponding label,
BandNthe number of batches and the batch size, and θthe trainable parameters. The detailed
network parameters and training configurations are provided in Appendix Section B.
Baseline models. To validate the superiority of the proposed P2C2Net model, we conducted
comparisons with various baseline models, including FNO [ 8], UNet [ 45], PeRCNN [ 19], DeepONet
[7], and Learned Interpolation (LI) [ 20]. The descriptions and training setup of the baseline models
are provided in Appendix Section C.
4.2 Main resultsTable 2: Quantitative results of our model and baselines. For the
case of Burgers, GS, and FN, our model inferred the test set’s
upper time limits of 1.4 s, 2000 s, and 10 s, respectively, as the
trajectories of dynamics get stabilized. We take these limits in
HCT to facilitate evaluation metrics calculation.
Case Model RMSE MAE MNAD HCT (s)
BurgersFNO 0.0980 0.0762 0.062 0.3000
UNet 0.3316 0.2942 0.2556 0.0990
DeepONet 0.2522 0.2106 0.1692 0.0020
PeRCNN 0.0967 0.1828 0.1875 0.4492
P2C2Net (Ours) 0.0064 0 .0046 0 .0037 1 .4000
Promotion ( ↑) 93.4% 94.0% 94.0% 211.7%
GSFNO NaN NaN NaN 354
UNet NaN NaN NaN 4
DeepONet 0.3921 0.2670 0.2670 852
PeRCNN 0.1586 0.0977 0.0976 954
P2C2Net (Ours) 0.0135 0 .0062 0 .0062 2000 .0
Promotion ( ↑) 91.5% 93.7% 93.6% 109.6%
FNFNO 0.8935 0.5447 0.2593 3.5000
UNet 0.1730 0.0988 0.0470 6.5000
DeepONet 0.5474 0.3737 0.1779 0.5128
PeRCNN 0.5703 0.2258 0.1075 5.3750
P2C2Net (Ours) 0.0390 0 .0149 0 .0071 10 .000
Promotion ( ↑) 77.5% 84.9% 84.9% 53.8%
NSFNO 1.0100 0.7319 0.0887 2.5749
UNet 0.8224 0.5209 0.0627 3.9627
LI NaN NaN NaN 3.5000
PeRCNN 1.2654 0.9787 0.1192 0.6030
P2C2Net (Ours) 0.3533 0 .1993 0 .0235 7 .1969
Promotion ( ↑) 57.0% 61.7% 62.5% 81.6%Figure 3 presents the results of compar-
ison between P2C2Net and baselines, in-
cluding error distribution, error propaga-
tion, and predicted trajectories. More-
over, Table 2 provides the quantitative
model performance results.
Burgers Equation. Generally, our
P2C2Net and baseline models (PeRCNN
and FNO) all capture the evolving dy-
namics and provide acceptable results as
shown in the right part of Figure 3( a).
However, our method shows a notably
lower error level compared with base-
lines, as emphasized in the error distri-
bution and error propagation presented
in the left and middle parts of Fig-
ure 3( a). Moreover, Table 2 substantiates
this observation, revealing a significant
improvement in our model’s performance
compared with the baseline models from
a minimum of 93.4% to a maximum of
211.7%. We further conducted boundary
condition generalization tests, as detailed
in Appendix Section G.1.
GS Equation. The primary challenge of this dataset lies in its sparsity and the intricate patterns
it presents, as depicted in Figure 3( b) (right). Each baseline model struggles to accurately predict
the trajectories, especially the UNet model demonstrating significant divergence. Nevertheless, our
method exhibits superior stability and is capable of learning the underlying dynamics. This is further
validated by the error analysis presented in Figure 3( b), where our P2C2Net model outperforms
the baselines by one to two orders of magnitude. Table 2 shows a comprehensive summary of the
performance metrics, with our model achieving an improvement of over 91% across all evaluations
for the GS equation.
FN Equation. This RD system is another classic but challenging case due to its two-scale fast-slow
evolving patterns. As illustrated in the predicted snapshots of Figure 3( c), the baseline models
can capture the global patterns but struggle with the local details. Our method shows superiority
in learning the multi-scale features. Moreover, the error analysis demonstrates that our P2C2Net
achieves errors one to two orders of magnitude lower than those of the baseline models, with an
improvement ranging from 53.8% to 77.5% compared to the best baseline (see Table 2).
8NS Equation. We evaluate our method on an NS dataset with a Reynolds number of 1000, a
benchmark dataset known for its significant challenges. As shown in Figure 3( d), the snapshots
produced by the baseline models at t= 7 s exhibit incorrect dynamical patterns, especially the LI
model showing divergence. The average test error of our model is at least an order of magnitude
Scaled energy spectrum E( k)
Wavenumber  kk5
Figure 4: Energy spectra.lower than those of the baselines. Furthermore, P2C2Net
consistently outperforms the baselines throughout the er-
ror propagation process. Table 2 further validates the
superior performance of our model, with improvements
across all metrics of at least 57%. Additionally, we exam-
ine the physical properties of the learned fluid dynamics,
such as energy spectra. As illustrated in Figure 4, the
energy spectra curve of P2C2Net closely aligns with that
of the ground truth, demonstrating its effectiveness in
capturing high-frequency features.
Generalization test. Taking the NS equation as an ex-
ample, P2C2Net is able to generalize to different external
forces fand Reynolds numbers Re. Our model is trained
withf=sin(4y)nx−0.1uandRe= 1000 , where
nx= [1,0]T. We conduct the generalization test under
six external force scenarios and four distinct Reynolds
numbers Re={200,500,800,2000}. As depicted in Figure 5( a), the error distribution presents a
stable performance with errors generally below 0.1 across six distinct external forces. Figure 5( b)
shows the acceptable error propagation with a gradual upward trend, which aligns with the long-
term rollout outcomes of our model. For the generalization test on Reynolds numbers, Figure 5( c)
demonstrates satisfactory and robust error levels for all Reynolds numbers, and smaller errors are
observed when the Reynolds numbers are closer to those in the training set, as validated by the error
propagation curves shown in Figure 5( d). Overall, our P2C2Net model exhibits robust generalization
capabilities and stable performance across test samples for external forces, and Reynolds numbers.
Further details on the snapshots are provided in Appendix Figure S3.
Robustness to noisy/incomplete data. Using the Burgers equation as an example, we introduced
Gaussian noise of varying scales during the training process and observed its minimal impact on the
results. The results are presented in Table S3, which indicate that our model is robust to the training
data noise and maintains the HCT (high correlation time) metric without reduction.
RMSEa Error distribution Error propagation
External forcings
b
RMSEError propagation Error distribution c d
Reynolds numbers
Figure 5: The error distribution and propagation of P2C2Net for generalization
over different external forces ( a,b) and Reynolds numbers ( c,d).Moreover, for the Burgers
case, the time steps of the
training data consist of only
28% of the inference steps
in the test data. Namely,
our training data itself is
incomplete, and the miss-
ing data accounts for 72%
of the test set. Based on
this sparse dataset, we fur-
ther randomly reduced the
training data by 20% to sim-
ulate data incompleteness
(e.g., randomly deleting data
according to the rollout step
size to make the trajectory in-
complete, where the specific
size can be found in Table 1
in the paper). The results of
this experiment are shown in
Table S4, where values are
averaged over 10 test sets.
We can observe that, after making the data sparser, our model performance slightly decreases,
which means that our model is capable of handling scenarios with incomplete data.
9Ablation study. To assess the impact of different components on model performance, we design and
conduct a series of ablation experiments based on the Burgers equation. The experimental setups
include: (1) Model 1, replacing symmetric convolutions with regular convolutions; (2) Model 2,
using convolution kernels with finite difference stencils instead of symmetric convolutions; (3) Model
3, removing the Correction Block; (4) Model 4, substituting RK4 integration with first-order Euler
methods; and (5) the full P2C2Net architecture. All experiments are performed using the same
training data and model hyperparameters as previously defined.
Table 3: Results for the ablation study of P2C2Net.
Ablated Model RMSE MAE MNAD HCT (s)
Model 1 0.1450 0.1100 0.0924 0.1073
Model 2 NaN NaN NaN 0.2013
Model 3 0.1463 0.1141 0.0977 0.1100
Model 4 0.0322 0.0241 0.0193 1.1860
Full P2C2Net 0.0064 0 .0046 0 .0037 1 .4000As shown in Table 3, the network performs
poorly when the Fourier block is removed, with
error levels two orders of magnitude higher com-
pared to using the complete P2C2Net architec-
ture. This emphasizes the critical role of the
physics-encoded variable correction learning
method. Configurations using traditional finite
difference stencils as kernels also exhibit simi-
larly poor performance due to the large numerical errors when approximating derivatives caused
by coarse grids. Moreover, removing symmetry constraints leads to a significant increase in errors,
emphasizing the importance of symmetric features for model convergence, especially in scenarios
with limited data. Using the Euler scheme for time stepping instead of RK4 results in reduced stability
and increased error accumulation, leading to a decrease in performance compared to the complete
P2C2Net. Overall, the results confirm that the physics-encoded variable correction learning method
and convolution filters satisfying symmetry are indispensable components of the network framework.
5 Conclusion
GS FN NS
Equations0100200300400500600700800Inference time cost (s)654
5711x543
1830x203
2010xNumerical Solver
P2C2Net
Figure 6: Computational time for comparison.This paper presents a physics-encoded variable cor-
rection learning method designed to embed prior
knowledge on coarse grids for solving nonlinear dy-
namic systems. This approach enables the model to
focus on fitting equations, ensuring excellent general-
ization capability and interpretability. It introduces a
convolutional filter that follows symmetry constraints,
requiring only seven learnable parameters to adap-
tively compute the derivatives of system state vari-
ables corresponding to the flow field on a coarse grid. This allows the model to learn spatiotemporal
dynamics with limited data. Our model has been trained and tested on four different nonlinear
dynamic systems, achieving the SOTA results. Even with minimal data, it can generalize to different
initial conditions, external force terms, and PDE parameters. In summary, our model effectively
adapts to various nonlinear dynamic systems. Moreover, the trained model shows remarkable speedup
for simulation under the same condition of accuracy, shown in Figure 6.
Despite demonstrating excellent generalization ability, the model faces two challenges. Firstly, the
model is based on regular grids with periodic boundaries, limiting its ability to solve problems on
irregular grids. We will explore graph structures or coordinate transformations to handle irregular
grids and incorporate special boundary treatment methods to adapt to various boundary conditions.
Secondly, we expect to expand our research from 2D problems to 3D dynamical systems.
Acknowledgement
The work is supported by the National Natural Science Foundation of China (No. 62276269 and
No. 92270118), the Beijing Natural Science Foundation (No. 1232009), and the Strategic Priority
Research Program of the Chinese Academy of Sciences (No. XDB0620103). In addition, H.S and
Y .L. would like to acknowledge the support from the Fundamental Research Funds for the Central
Universities (No. 202230265 and No. E2EG2202X2). Q.W. acknowledges the support by the
Interdisciplinary-Innovative Research Program of the Institute of Interdisciplinary Sciences, Renmin
University of China. P.R. would like to disclose that he was involved in this work when he was at
Northeastern University, who has not been supported by Huawei Technologies.
10Impact statement
The aim of this work is to develop a novel physics-encoded learning scheme to accelerate predictions
and simulations of spatiotemporal dynamical systems. This method can be applied to various fields,
including weather forecasting, turbulent flow prediction, and other simulation tasks. Our research is
solely intended for scientific purposes and poses no potential ethical risks.
References
[1]John David Anderson and John Wendt. Computational fluid dynamics , volume 206. Springer,
1995.
[2]Fadl Moukalled, Luca Mangani, Marwan Darwish, F Moukalled, L Mangani, and M Darwish.
The finite volume method . Springer, 2016.
[3]Olek C Zienkiewicz, Robert L Taylor, and Jian Z Zhu. The finite element method: its basis and
fundamentals . Elsevier, 2005.
[4]George Karniadakis and Spencer J Sherwin. Spectral/hp element methods for computational
fluid dynamics . Oxford University Press, USA, 2005.
[5]Nashat N Ahmad. Numerical simulation of the aircraft wake vortex flowfield. In 5th AIAA
Atmospheric and Space Environments Conference , page 2552, 2013.
[6]Konrad A Goc, Oriol Lehmkuhl, George Ilhwan Park, Sanjeeb T Bose, and Parviz Moin. Large
eddy simulation of aircraft at affordable cost: a milestone in computational fluid dynamics.
Flow , 1:E14, 2021.
[7]Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning
nonlinear operators via DeepONet based on the universal approximation theorem of operators.
Nature Machine Intelligence , 3(3):218–229, 2021.
[8]Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik
Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric
partial differential equations. In International Conference on Learning Representations , 2021.
[9]Kimberly Stachenfeld, Drummond B Fielding, Dmitrii Kochkov, Miles Cranmer, Tobias Pfaff,
Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, and Alvaro Sanchez-Gonzalez. Learned
coarse models for efficient turbulence simulation. In International Conference on Learning
Representations , 2022.
[10] Joe D Hoffman and Steven Frankel. Numerical methods for engineers and scientists . CRC
press, 2018.
[11] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics , 378:686–707, 2019.
[12] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning
velocity and pressure fields from flow visualizations. Science , 367(6481):1026–1030, 2020.
[13] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physics-
informed deep learning for turbulent flow prediction. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining , pages 1457–1466, 2020.
[14] Ruiyang Zhang, Yang Liu, and Hao Sun. Physics-informed multi-lstm networks for meta-
modeling of nonlinear structures. Computer Methods in Applied Mechanics and Engineering ,
369:113226, 2020.
[15] Zhao Chen, Yang Liu, and Hao Sun. Physics-informed learning of governing equations from
scarce data. Nature Communications , 12(1):6136, 2021.
11[16] Chengping Rao, Hao Sun, and Yang Liu. Physics-informed deep learning for computational
elastodynamics without labeled data. Journal of Engineering Mechanics , 147(8):04021043,
2021.
[17] Kejun Tang, Jiayu Zhai, Xiaoliang Wan, and Chao Yang. Adversarial adaptive sampling: Unify
pinn and optimal transport for the approximation of pdes. In International Conference on
Learning Representations , 2024.
[18] Chengping Rao, Pu Ren, Yang Liu, and Hao Sun. Discovering nonlinear pdes from scarce data
with physics-encoded learning. In International Conference on Learning Representations , 2022.
[19] Chengping Rao, Pu Ren, Qi Wang, Oral Buyukozturk, Hao Sun, and Yang Liu. Encoding
physics to learn reaction–diffusion processes. Nature Machine Intelligence , 5(7):765–779,
2023.
[20] Dmitrii Kochkov, Jamie A Smith, Ayya Alieva, Qing Wang, Michael P Brenner, and Stephan
Hoyer. Machine learning–accelerated computational fluid dynamics. Proceedings of the
National Academy of Sciences , 118(21):e2101784118, 2021.
[21] Yohai Bar-Sinai, Stephan Hoyer, Jason Hickey, and Michael P. Brenner. Learning data-driven
discretizations for partial differential equations. Proceedings of the National Academy of
Sciences , 116(31):201814058, 2019.
[22] Jayesh K Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized
PDE modeling. Transactions on Machine Learning Research , 2023.
[23] Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural networks:
Bridging deep architectures and numerical differential equations. In International Conference
on Machine Learning , pages 3276–3285, 2018.
[24] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and
Peter Battaglia. Learning to simulate complex physics with graph networks. In International
Conference on Machine Learning , pages 8459–8468, 2020.
[25] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning mesh-
based simulation with graph networks. In International Conference on Learning Representations ,
2021.
[26] Steeven Janny, Aurélien Beneteau, Madiha Nadri, Julie Digne, Nicolas Thome, and Christian
Wolf. Eagle: Large-scale learning of turbulent fluid dynamics with mesh Transformers. In
International Conference on Learning Representations , 2023.
[27] Zijie Li, Dule Shu, and Amir Barati Farimani. Scalable transformer for pde surrogate modeling.
Advances in Neural Information Processing Systems , 36, 2024.
[28] Haixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, and Mingsheng Long. Transolver: A
fast transformer solver for pdes on general geometries. arXiv preprint arXiv:2402.02366 , 2024.
[29] Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for
differential equations. Advances in Neural Information Processing Systems , 34:24048–24062,
2021.
[30] Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural
operators. In The Eleventh International Conference on Learning Representations , 2022.
[31] Zongyi Li, Nikola Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Otta, Moham-
mad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, et al.
Geometry-informed neural operator for large-scale 3D PDEs. Advances in Neural Information
Processing Systems , 36, 2024.
[32] Somdatta Goswami, Katiana Kontolati, Michael D Shields, and George Em Karniadakis. Deep
transfer operator learning for partial differential equations under conditional shift. Nature
Machine Intelligence , 4(12):1155–1164, 2022.
12[33] Phillip Lippe, Bas Veeling, Paris Perdikaris, Richard Turner, and Johannes Brandstetter. Pde-
refiner: Achieving accurate long rollouts with neural pde solvers. Advances in Neural Informa-
tion Processing Systems , 36, 2024.
[34] Han Gao, Luning Sun, and Jian-Xun Wang. PhyGeoNet: Physics-informed geometry-adaptive
convolutional neural networks for solving parameterized steady-state pdes on irregular domain.
Journal of Computational Physics , 428:110079, 2021.
[35] Pu Ren, Chengping Rao, Yang Liu, Jian-Xun Wang, and Hao Sun. PhyCRNet: Physics-informed
convolutional-recurrent network for solving spatiotemporal pdes. Computer Methods in Applied
Mechanics and Engineering , 389:114399, 2022.
[36] Pu Ren, Chengping Rao, Yang Liu, Zihan Ma, Qi Wang, Jian-Xun Wang, and Hao Sun. Physr:
Physics-informed deep super-resolution for spatiotemporal data. Journal of Computational
Physics , 492:112438, 2023.
[37] Quercus Hernández, Alberto Badías, Francisco Chinesta, and Elías Cueto. Thermodynamics-
informed neural networks for physically realistic mixed reality. Computer Methods in Applied
Mechanics and Engineering , 407:115912, 2023.
[38] Rui Wang, Robin Walters, and Rose Yu. Incorporating symmetry into deep dynamics models
for improved generalization. In International Conference on Learning Representations , 2021.
[39] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In
International conference on machine learning , pages 3208–3216, 2018.
[40] Zichao Long, Yiping Lu, and Bin Dong. Pde-net 2.0: Learning pdes from data with a numeric-
symbolic hybrid deep network. Journal of Computational Physics , 399:108925, 2019.
[41] Xin-Yang Liu, Min Zhu, Lu Lu, Hao Sun, and Jian-Xun Wang. Multi-resolution partial differ-
ential equations preserved learning framework for spatiotemporal dynamics. Communications
Physics , 7(1):31, 2024.
[42] Jiawei Zhuang, Dmitrii Kochkov, Yohai Bar-Sinai, Michael P Brenner, and Stephan Hoyer.
Learned discretizations for passive scalar advection in a two-dimensional turbulent flow. Physi-
cal Review Fluids , 6(6):064605, 2021.
[43] Zhiqing Sun, Yiming Yang, and Shinjae Yoo. A neural PDE solver with temporal stencil
modeling. In International Conference on Machine Learning , pages 33135–33155, 2023.
[44] Gideon Dresdner, Dmitrii Kochkov, Peter Christian Norgaard, Leonardo Zepeda-Nunez, Jamie
Smith, Michael Brenner, and Stephan Hoyer. Learning to correct spectral methods for simulating
turbulent flows. Transactions on Machine Learning Research , 2023.
[45] Jayesh K Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized
pde modeling. arXiv preprint arXiv:2209.15616 , 2022.
[46] Johannes Brandstetter, Rianne van den Berg, Max Welling, and Jayesh K Gupta. Clifford
Neural Layers for PDE Modeling. In The Eleventh International Conference on Learning
Representations , 2022.
[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks
for biomedical image segmentation. In Medical image computing and computer-assisted
intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9,
2015, proceedings, part III 18 , pages 234–241. Springer, 2015.
13APPENDIX
A Design of the filter with symmetric constraint
f Firstly, inspired by the central finite difference method, we design a filter gwith the size of m×m,
which meets the symmetric constraint. For example, when m= 5,gis given by
g=
a1 a4 a7−a4−a1
a2 a5 a8−a5−a2
a3 a6 0−a6−a3
−a2−a5−a8a5 a2
−a1−a4−a7a4 a1
. (S1)
Taking α= (1,0), we have:
m−1
2X
k1=−m−1
2m−1
2X
k2=−m−1
2kα1
1kα2
2g[k1, k2] =−4a7+ 2a8. (S2)
In order to satisfy the sum rules of order α= (1,0), we can get a8=−2a7.Then, the filter gcan be
re-written as
g=
a1 a4 a7−a4−a1
a2 a5−2a7−a5−a2
a3 a6 0−a6−a3
−a2−a52a7 a5 a2
−a1−a4−a7a4 a1
. (S3)
Corollary 1 Proof: Consider the filter we designed above with α= (0 ,1),α= (0 ,3)and
α= (3,0), we have:
m−1
2X
k1=−m−1
2m−1
2X
k2=−m−1
2k0
1k1
2g[k1, k2] =−4a5−2a6 (S4)
m−1
2X
k1=−m−1
2m−1
2X
k2=−m−1
2k0
1k3
2g[k1, k2] =−16a5−2a6 (S5)
m−1
2X
k1=−m−1
2m−1
2X
k2=−m−1
2k3
1k0
2g[k1, k2] =−12a7 (S6)
Satisfying the sum rules of order α= (0,3)andα= (3,0)leads to strictly −16a5−2a6=
0,−12a7= 0. By adjusting the trainable parameters, we have a6+ 8a5→0,a7→0, and
−4a5−2a6̸= 0. According to Lemma1, the resulting filter gsatisfies the total sum rules of order
5\ {2}.
For any smooth function ω, we perform a convolution operation on it with the filter g. Applying a
Taylor expansion to this process, we obtain:
2X
k1,k2=−2g[k1, k2]ω(x+k1δx, y +k2δy)
=2X
k1,k2=−2g[k1, k2]4X
i,j=0δxiδyj
i!j!∂i+jω
∂xi∂yj(x, y) +o(|δx|4+|δy|4)
=4X
i,j=0ri,jδxiδyj∂i+jω
∂xi∂yj(x, y) +o(|δx|4+|δy|4).(S7)
14Fourier
layer    Fourier
layer    Fourier
layer    …… O IFigure S1: The architecture of FNO Model
Then, if the filter ghas total sum rules of order 5\ {2}, for a smooth function ω(x, y)and small
perturbation ε >0, we have:
1
εm−1
2X
k1=−m−1
2m−1
2X
k2=−m−1
2g[k1, k2]ω(x+εk1, y+εk2) =C∂ω(x, y)
∂x+O(ε4),asε→0. (S8)
If there is no such setting, for a6+ 8a5→0,a7→0, through continuous training, we iteratively
optimize the network and adjust the values of parameter a1toa7, such that:
4X
i,j=0ri,jδxiδyj∂i+jω
∂xi∂yj(x, y) = 0 ∀i, j\ {i= 1, j= 0} (S9)
In this case, Eq. (S8) still holds. Hence, by employing a filter gwith enhanced symmetry constraints
and through the learning process of the network, we ensure that fourth-order accuracy is preserved.
B Implementation Details
B.1 FNO Model
As illustrated in Figure S1, the network structure primarily consists of three key components: P(lift
operation), Q(projection operation), and Fourier layers. Both PandQare convolutional operations
designed for channel transformation. Every Fourier layer employs the Fast Fourier Transform ( FFT)
and the inverse Fast Fourier Transform ( iFFT ) for frequency domain transformations. Additionally,
Rϕrepresents spectral filtering and convolution in the frequency domain, and Wldenotes the local
linear transformation specific to the l-th layer. σthe activation function GeLU.
Our FNO model omits normalization schemes due to their detrimental impact on performance [ 46].
When used in the Correction Block, we set L= 2. For the Burgers equation, we configure modes =
12, width = 12, nd the projection operation from channel 12 to channel 50. Both the FHN and GS
equations share the same configuration: modes = 12, width = 20, with the projection operation from
channel 20 to channel 50. The NS equation requires a distinct configuration: modes = 25 width =
20, and the projection operation from channel 20 to channel 128. In particular, when it appears as a
module of Neural Network, the hyperparameters are set differently: L= 5,modes =grid/ 2−2,
and width to be the same as modes.
B.2 Poisson Solver
The objective of the Poisson solver is to determine the state quantities of a system within a two-
dimensional spatial domain using the spectral method for a specified Laplacian term. The Laplace
equation in two dimensions is represented as:
∆p=ψ(u,f). (S10)
15Add
Equation
Residual Block+ + +
Add+Equation
Residual Block
Add+Equation
Residual Block
Add+Equation
Residual BlockFigure S2: RK4 integration scheme
where ψ(u,f) =−2 (uyvx−uxvy) +∇·ffor 2D problems. Applying Fast Fourier Transform
(FFT) on Eq. (S10), we have
−(η2
x+η2
y)p∗=ψ∗(u,f), (S11)
where ηxandηyare the wavenumbers along the xandyaxes, respectively, assuming η2
x+η2
y̸= 0to
avoid division by zero. In the frequency domain, we can get
p∗=ψ∗(u,f)
−(η2x+η2y). (S12)
Next, we transform the field from the frequency domain to the spatial using inverse Fast Fourier
Transform ( iFFT ):
iFFT [p∗] =iFFTψ∗(u,f)
−(η2x+η2y)
. (S13)
By applying the above process to ψ(u,f), we can efficiently decouple the pressure field without any
labeled data or training process.
B.3 RK4 integration scheme
The general numerical integration method for time marching from utktoutk+1can be written as:
uk+1=uk+Ztk+1
tkH[uk(x, τ)]dτ. (S14)
Among them, uk+1andukare solutions at time k+ 1andk. As shown in Figure S2, RK4 is a
high-order integration scheme, which divides the time interval into multiple equally spaced small
time steps to approximate the integral. The final updating of the above state change can be written as:
s1=H[uk, tk],
s2=H
uk+δt
2×s1, tk+δt
2
,
s3=H
uk+δt
2×s2, tk+δt
2
,
s4=H[uk+δt×s3, tk+δt],
uk+1=uk+1
6δt(s1+ 2s2+ 2s3+s4).(S15)
where δtdenotes the step size and s1,s2,s3,s4represent four intermediate variables (slopes). The
global error is proportional to the step size to the fourth power, i.e., O(δt4).
C Baseline models
To evaluate the performance of our proposed method, we compare it against multiple state-of-the-art
(SOTA) baseline models. The detailed descriptions of each model are presented below, and the
training configurations are listed in Section F.
16Fourier Neural Operator (FNO). The FNO [ 8] combines neural networks with Fourier transforms
and consists of two major parts. The first part primarily involves performing Fourier transforms
on system state quantities, convolving in the frequency domain, and then inversely transforming to
extract global features. The second part includes linear transformations through convolutions on the
system state quantities to extract local features. Finally, activation functions are applied, and the
outputs from both parts are combined to produce the final result.
UNet. UNet [ 47] employs an encoder-decoder structure, where the encoder extracts multi-scale
features through multiple downsampling operations and the decoder recovers the original image size
through multiple upsampling steps. Skip connections are utilized during the decoding process to fuse
feature maps from corresponding layers, preserving both local details and capturing global features.
PeRCNN. PeRCNN [ 19] is a physics-encoded learning approach with physical laws embedded into
the neural networks. It comprises multiple parallel CNNs, leveraging the simulation of polynomial
equations through feature map multiplication. This approach enhances the model’s capacity for
extrapolation and generalization.
DeepONet. The aim of DeepONet [ 7] is to use neural networks to learn end-to-end mapping from
input data to target output by approximating operators. It includes a trunk net and a branch net, which
enables effectively capturing intricate functional relationships.
Learned Interpolation (LI). LI [20] is based on a finite volume method scheme. It leverages neural
networks to replace the traditional numerical approach that uses polynomial interpolation for the
velocity tensor product. This network learns how to dynamically adjust the interpolation function
based on the characteristics of the local flow field. Therefore, the LI method facilitates the predictions
of dynamics on coarser grids.
D Evaluation Metrics
In this paper, we use several metrics to evaluate our model, including RMSE, Mean Absolute Error
(MAE), Mean Normalized Absolute Difference (MNAD), and High Correction Time (HCT). RMSE
measures the average magnitude of the error between predicted and observed values, reflecting the
model’s accuracy. MAE evaluates the average absolute difference between predicted and observed
values, indicating the actual magnitude of the errors. MNAD is a crucial metric for assessing the
accuracy of model outputs over time. MNAD calculates the average discrepancy across a series of
temporal data points, offering a normalized measure of prediction error relative to the range of the
actual data. HCT quantifies the model’s ability to make accurate long-term predictions. These metrics
are defined as follows:
RMSE =vuut1
nnX
i=1∥Si−˘Si∥2, (S16)
MAE =1
nnX
i=1Si−˘Si (S17)
MNAD =1
nnX
i=1∥Si−˘Si∥
∥Si∥max− ∥Si∥min, (S18)
HCT =NX
i=1∆t·1(PCC (Si,˘Si)>0.8), (S19)
where
PCC (Si,˘Si) =cov(Si,˘Si)
σSiσ˘Si, (S20)
Here ndenotes the number of trajectories; Sithe ground truth of trajectories; ˆSithe spatiotemporal
sequence predicted by the model. “cov” is the covariance function and “ σ” is the standard deviation
of the given sequence. 1(·)is the indicator function that takes a value of 1 when the condition
(PCC (Si,˘Si)>0.8)is true, and 0 otherwise. Nis the total number of time steps.
17E Dataset Informations
To facilitate a comprehensive evaluation of our proposed method, we employ datasets generated
from various well-known equations that model complex systems. These datasets cover a wide range
of applications from fluid dynamics to reaction-diffusion systems, ensuring a robust assessment of
model performance.
For fairness of testing datasets, we randomly select 10 seeds from the range of [1, 10,000], which are
used to generate different Gaussian noise disturbances, which are then applied to the initial velocity
field to create varying ICs. We also perform a warm-up phase during data generation to ensure that
the variance and mean of the trajectories are closely aligned, thereby maintaining fairness. More
details about dataset generation can be found in Table S2.
Burgers. This equation describes the velocity flow in a viscous fluid, which combines compressibility
and nonlinear effects. It has wide applications in many scientific fields, including weather and climate
simulation, the petroleum industry, acoustics, and quantum field theory. The equation is given by:
∂u
∂t=ν∇2u−u·∇u, t∈[0, T] (S21)
where u={u, v} ∈R2represents the fluid velocities, νis the viscosity coefficient set to 0.002, and
∇2is the Laplacian operator.
To generate the dataset, we use the FD method with periodic boundary conditions on the spatial
domain x∈[0,1]. The data is initially simulated on a 1002grid and then downsampled to 252for
numerical experiments. The simulation timestep is set to 1×10−3seconds and the time duration is
T= 1.4s. We use five trajectories for training, each of which contains 400 snapshots. For testing,
we utilize ten groups of trajectories, each with 1400 snapshots.
GS. The GS equation models the interaction between two chemical species and has been widely
used to simulate various chemical reactions under different conditions. It helps to explore dynamic
phenomena such as self-organized patterns, chemical waves, reaction-diffusion patterns, collective
behavior, natural pattern formation, cell growth, and other areas in physics. The GS equation is
described as:
∂u
∂t=Du∇2u−uv2+F(1−u),
∂v
∂t=Dv∇2v+uv2−(F+κ)v,(S22)
where uandvrepresent the concentrations of two different chemical substances, DuandDvare the
diffusion coefficients, and Fandκdenote the growth and death rates of the substances, respectively.
In our case, we set Du= 2.0×10−5,Dv= 5.0×10−6,F= 0.04, and κ= 0.06. We create the
dataset using the FD method on a 1282grid under periodic boundary conditions, with the spatial
domain x∈[0,1]2. The simulation timestep is 0.5s, and the total simulation duration is 1900 s. The
data is then downsampled to a 322grid and the timestep is coarsened to 2s to establish the ground
truth. We leverage three training trajectories, each containing 1000 snapshots. Additionally, we
employ ten separate testing sets with diffenent ICs.
FN. This case is a widely recognized reaction-diffusion system and is used to simulate the propagation
of neural impulses. The governing equations are given by:
∂u
∂t=γ∇2u+M(u), t∈[0, T] (S23)
where u={u, v} ∈R2are the two interactive components, and γis the diffusion coefficient. The
reaction source terms M(u)are defined as:
Mu(u, v) =u−u3−v+α,
Mv(u, v) =β(u−v),(S24)
withα= 0.01denoting the external stimulus and β= 0.25as the reaction coefficient.
We generated the dataset using the FD method on a 1282grid and a timestep of 2.0×10−3s. The
spatial domain was set as x∈[0,128]2with periodic boundary conditions applied. The simulation
18DNS 2048 P2C2NetT = 0 T = 300 T = 600 T = 900 T = 1200 a
DNS 2048 P2C2Net
0
-10
--
-
--5510P2C2NetT = 0 T = 300 T = 600 T = 900 T = 1200 b
P2C2Net DNS 2048 DNS 2048Re = 2000 Re = 500
Vorticity
0
-10
--
-
--5510
VorticityFigure S3: Generalization across multiple Re’s and forces.
time is T= 10 s. This data is subsequently downsampled to 642with a timestep of 8.0×10−3s
to serve as the ground truth. For training purposes, we employ three groups of trajectories, each
comprising 1375 snapshots. For our testing phase, we leverage ten distinct groups of trajectories,
each with unique ICs.
NS. The NS equations are fundamental to the study of fluid dynamics, which govern the behavior
of fluid motion. In this paper, we consider a 2-dimensional incompressible Kolmogorov flow with
periodic boundary conditions in velocity-pressure form, which is written as
∂u
∂t+ (u·∇)u=1
Re∇2u−∇p+f, t∈[0, T],
∇·u= 0,(S25)
where u={u, v} ∈R2denotes the fluid velocity vector, p∈Ris the pressure, Rerepresentes
the Reynolds number that characterizes the flow regime. The Reynolds number serves as a scaling
parameter in the NS equation, resulting in a balance between the inertial forces (captured by the
advection term (u·∇)u) and the viscous forces(captured by the Laplacian term ∇2u). Thus, for
19Table S1: Impact of different kernel sizes.
Filter RMSE MAE MNAD HCT
3×3 0.1274 0.0926 0.0773 0.1947s
5×5 0.0064 0.0046 0.0037 1.4s
7×7 NaN NaN NaN 0.1673s
Table S2: Dataset generate details
Parameters / Case Burgers GS FN NS
DNS Method FD FD FD FV
Spatial Domain [0, 1]2[0, 1]2[0, 128]2[0,2π]2
Calculate Grid 10021282128220482
Training Grid 252322642642
Simulation δt(s) 1.00×10−32.00×10−35.00×10−12.19×10−4
Warmup (s) 0.1 0 9 40
Training data group 5 3 3 5
Testing data group 10 10 10 10
Spatial downsample 16× 16× 4× 1024×
Temporal downsample 1× 4× 4× 32×
low Reynolds numbers (i.e., when Reis small), the viscous forces are dominant, and the fluid flow is
mostly laminar and smooth. On the other hand, at high Reynolds numbers (i.e., when Reis large),
the inertial forces are dominant, and the fluid flow becomes more turbulent and chaotic.
In this paper, we generate the training data for a Re= 1000, using periodic boundary conditions
within the spatial domain x∈[0,2π]2. The data generation employs the FV method on a 20482grid
with a simulation timestep of 2.19×10−4s. Subsequently, this data is downsampled to 642with
a time step of 7.0×10−3to produce the ground truth. Five groups of trajectories, each containing
4800 snapshots, are employed for training. Ten groups of trajectories with diverse ICs are used for
each of the tests. Specifically, to further test the model’s generalization ability, we expand the test set:
(1) generating data corresponding to Revalues of 200, 500, 800, 1000, and 2000, and (2) creating
data with different external forces:
f1= cos(4 y)nx−0.1u,
f2= sin(4 y)nx−0.4u,
f3= cos(2 y)nx−0.1u,
f4= sin(2 y)nx−0.1u,
f5= cos(4 y)nx−0.4u,
f6= 0(S26)
F Training Details
All experiments were conducted on a single 80GB Nvidia A100 GPU, using an Intel(R) Xeon(R)
Platinum 8380 CPU (2.30GHz, 64 cores). We only give some of the changed parameters here, and
the other hyperparameters remain the same as the default settings.
P2C2Net. The architecture of P2C2Net, as shown in Figure 1, utilizes the Adam optimizer with
a learning rate set at 5×10−3. The model is trained with a batch size of 16 over 500 epochs. The
rollout timestep settings are detailed in Table 1. We employ the StepLR scheduler scaling the learning
rate by 0.96 every 200 steps.
FNO. The network architecture of FNO remains largely consistent with the original paper [ 8],
with the primary modification being the adaptation of the training method for this model to an
autoregressive approach. We utilize the Adam optimizer, with a learning rate of 1×10−3and a
20Table S3: Impact of noise on P2C2Net performance
Training RMSE MAE MNAD HCT (s)
+ 1% noise 0.0092 0.0088 0.0062 1.4
+ 0.5% noise 0.0078 0.0057 0.0047 1.4
w/o Noise noise 0.0064 0.0046 0.0037 1.4
Table S4: Impact of sparser Burgers dataset on P2C2Net performance
Training RMSE MAE MNAD HCT (s)
reduce 20% 0.0073 0.0052 0.0050 1.4
5×400 snapshots (in paper) 0.0064 0.0046 0.0037 1.4
batch size of 20. The training spans 1000 epochs, and the rollout timestep is aligned with that of the
P2C2Net.
UNet. We employ the modern UNet [ 22] architecture with the default setting, with the rollout
timestep consistent with that of P2C2Net. The scheduler utilized is StepLR with a step size of 100
and a gamma of 0.96. The optimizer employed is Adam, with a learning rate of 1×10−3and a batch
size of 10. The training is conducted over 1000 epochs.
DeepONet. We adopt the default architecture of DeepONet [ 7], with the Adam optimizer. The
learning rate is set at 5×10−4, with a decay applied every 5000 steps scaling at 0.9. The training is
conducted with a batch size of 16 and spans 20000 epochs.
PeRCNN. We utilize an architecture that is identical to the standard PeRCNN configuration [ 19].
The optimization is performed using Adam, with a StepLR scheduler that reduces the learning rate
by a factor of 0.96 every 100 steps. The initial learning rate is set at 0.01. The training regimen spans
1000 epochs with a batch size of 32.
LI. We utilizes its default network architecture and parameter configurations [ 20]. The optimizer
chosen is Adam with β1= 0.9andβ2= 0.99. The batch size is set to 8, with a global gradient norm
clipping of 0.01. The learning rate is 1×10−3, and the weight decay is 1×10−6.
G Additional Results
G.1 Applicability to different BCs
To verify the applicability of our model to different BCs, we use the Burgers equation as an example
and set the left boundary as Dirichlet, the right boundary as Neumann, and the top/bottom boundaries
as Periodic. Here, we denote this case as Complex Boundary Conditions (CBC). The rest of the data
generation setup (e.g., ICs, mesh grids) remains the same as used in the paper. We generated 10 CBC
test datasets resulting from different random ICs.
We then directly tested the model previously trained based on Periodic BC datasets reported in the
paper, meanwhile processing the boundaries using the BC encoding strategy [ 19] during inference.
The quantitative results (average over 10 datasets) are presented in Table S5 where we also list the
predicted snapshots at 1.4 s for two random ICs in Figure S4 . We can see that our model is capable
of generalizing over different BCs.
G.2 Generalization Results
For the NS equation, we also evaluate the generalization capability of P2C2Net with respect to
multiple Re’s and forces. Please refer to Figure S3 for details.
21Table S5: Generalization of P2C2Net over different boundaries
on the Burgers example for 10 trajectories.
BC Type RMSE MAE MNAD HCT(s)
Complex 0.0202 0.0103 0.0087 1.4
Periodic 0.0064 0.0046 0.0037 1.4
Ref. P2C2Net
-0.170.21
-0.54
-0.190.36
-0.73
Ref. P2C2Net
-0.12 0.28
-0.51
-0.34 0.15
-0.82
Figure S4: P2C2Net generalization over complex BCs (left Dirichlet, right Neumann, top/bottom Periodic) on
the Burgers example for two random ICs. Snapshots at t= 1.4s.
22NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In the abstract and introduction, our content covers the challenges to be
addressed by the paper, contributions, and a detailed overview of the applications.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In the conclusion, we discuss the limitations of this work, including the focus
on 2D problems.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
23Answer: [Yes]
Justification: We state the lemma and corollary in the main paper, with detailed derivations
provided in the Appendix Section A.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: As detailed in the paper, our model is easily reproducible, with key training
details provided in the Appendix, and the code can help readers reproduce the results.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
24Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: https://github.com/intell-sci-comput/P2C2Net
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide detailed descriptions of our experimental methods, comprehensive
code for replication, and further information on additional results, proofs, and supplementary
content in the appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We visualize our experimental results with informative figures and tables.
These include violin plots for error distribution, error propagation curves, and key metrics
like RMSE, MNAD, and MAE.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
25•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We offer sufficient computational resources and comprehensive information to
reproduce the experiments, which can be found in Appendix Section F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Our research is solely intended for scientific purposes, conforms in every
respect with the NeurIPS Code of Ethics, and poses no potential ethical risks.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: This method has the potential to revolutionize fields like weather forecasting,
turbulent flow prediction, and various other simulation tasks with its ability to accelerate
computations. We emphasize that our work is intended for positive societal impact, with
detailed discussions provided in the Appendix.
26Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [Yes]
Justification: Our model addresses scientific computing problems and has been used by other
researchers or institutions, such as Microsoft, with relevant data, which does not involve
high-risk scenarios
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Our code and data are developed in-house, so they are approved by the creators.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
27• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We are pleased to provide the necessary documentation, data, and code to
facilitate the reproduction of our results, ensuring convenience for all users.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our research subjects spatiotemporal dynamical systems and does not involve
humans or human subjects
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our paper is limited to PDE issues, complies with national laws and regulations,
and has no potential risks.
Guidelines:
28•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
29