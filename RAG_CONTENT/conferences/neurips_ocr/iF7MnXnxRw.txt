Understanding the Differences in Foundation Models:
Attention, State Space Models, and Recurrent Neural
Networks
Jerome Sieber∗
ETH Zurich
Zurich, Switzerland
jsieber@ethz.chCarmen Amo Alonso∗
ETH Zurich
Zurich, Switzerland
camoalonso@ethz.chAlexandre Didier
ETH Zurich
Zurich, Switzerland
adidier@ethz.ch
Melanie N. Zeilinger
ETH Zurich
Zurich, Switzerland
mzeilinger@ethz.chAntonio Orvieto
ELLIS Institute Tübingen
Tübingen, Germany
antonio@tue.ellis.eu
Abstract
Softmax attention is the principle backbone of foundation models for various artifi-
cial intelligence applications, yet its quadratic complexity in sequence length can
limit its inference throughput in long-context settings. To address this challenge,
alternative architectures such as linear attention, State Space Models (SSMs), and
Recurrent Neural Networks (RNNs) have been considered as more efficient al-
ternatives. While connections between these approaches exist, such models are
commonly developed in isolation and there is a lack of theoretical understanding of
the shared principles underpinning these architectures and their subtle differences,
greatly influencing performance and scalability. In this paper, we introduce the
Dynamical Systems Framework (DSF), which allows a principled investigation
of all these architectures in a common representation. Our framework facilitates
rigorous comparisons, providing new insights on the distinctive characteristics of
each model class. For instance, we compare linear attention and selective SSMs,
detailing their differences and conditions under which both are equivalent. We
also provide principled comparisons between softmax attention and other model
classes, discussing the theoretical conditions under which softmax attention can be
approximated. Additionally, we substantiate these new insights with empirical vali-
dations and mathematical arguments. This shows the DSF’s potential to guide the
systematic development of future more efficient and scalable foundation models.
1 Introduction
Foundation models serve as the backbone for a wide range of tasks across Artificial Intelligence
due to their ability to learn complex interactions in large datasets [ 1]. In recent years, the attention
mechanism [ 2] has been the dominating token-mixing strategy in foundation models. However,
its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a
challenge to scaling and deploying these models beyond moderate context lengths [ 3]. In order to
mitigate these issues, attention-free architectures have been proposed: prominent examples of these
are the novel State Space Models (SSMs) [ 4–8], as well as recent efforts to enhance Recurrent Neural
Networks (RNNs) [ 9–12]. Although these models show great promise in boosting efficiency, efforts
∗These authors contributed equally; ordered randomly.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).to provide a rigorous theoretical comparison are scarce, and current comparisons with attention are
merely empirical (see Section 5 for an in-depth discussion). Despite the prevalence and ubiquity
of foundation models, a principled understanding of the similarities and differences among these
different design strategies is currently lacking.
In order to close this gap, we introduce the Dynamical Systems Framework (DSF) – a theoretical
framework based on a control theoretic perspective – that allows us to evaluate the similarities
and differences between different foundation models in a principled manner. The DSF serves as a
powerful tool for approaching theoretical research questions about foundation models, enabling direct
comparisons – both theoretical and experimental – across architectures such as attention mechanisms,
SSMs, and RNNs. We believe that the DSF provides new insights on the most relevant features found
in current architectures, and can inform a systematic development of future hybrid models. The DSF
further simplifies identification of existing computational algorithms to apply to newly developed
models. Rather than providing an exhaustive list of insights, the results included below are meant to
exemplify important questions that the DSF can answer and guide future research. Specifically, we
explore the following questions:
•How are attention mechanisms, SSMs, and RNNs related?
TL;DR: All three model classes can be represented as recurrent models, which can be
compared using the proposed DSF.
•Can softmax attention be expressed as a recurrent model?
TL;DR: Softmax attention translates to a recurrent model within the DSF, however the
hidden state dimension needs to be infinite.
•Why does state expansion help to improve performance of RNNs and SSMs?
TL;DR: This is related to the second question: state expansion increases the dimension of
the hidden state thus allowing for an increased expressivity of the model (Lemma 2).
•Why do SSMs significantly outperform attention on the LRA benchmark?
TL;DR: The performance gap can be explained by the recurrent normalization strategy
(discretization step) used by selective SSMs as discussed in Section 4.2.
•How closely are linear attention, S6 (i.e. Mamba) related?
TL;DR: The common feature is the coupling of state transition and input matrix via a single
(normalization) parameter in their recurrent representation. However, the models differ in
the parameterization of this parameter, which we analyze experimentally.
•What do selective SSMs teach us about improving RNN architectures?
TL;DR: Replacing the state transition in a RNN variant - qLSTM - with the state transition
of S6 improves performance of the RNN.
We note that the main contribution of our paper is the introduction of the DSF, which is a unifying
framework for analysis of attention mechanisms, SSMs, and RNNs. To the best of our knowledge,
this is the first unified framework that allows analysis of all three model classes in the same parame-
terization and thus allows to identify differences in the models that lead to significant performance
improvements. While some of the provided results already exist in the literature (e.g that increased
state size improves performance), we also provide novel insights unique to the DSF framework in a
comprehensive way that enables further analysis with control theoretical tools.
Notation: We use Latin letters in the following way: Nis the size of the hidden state in the DSF,
nthe state expansion, dthe embedding size or model size, and Lthe sequence length. A visual
representation of these dimensions is given in Appendix A. We use superscripts, e.g. ·d, to denote
the elements or block-elements of a matrix and a block-matrix. We use subscripts, e.g. ·i, to denote
the time index (or input dependency). Specifically, virepresents the value of vector vat time i. We
use bold notation to indicate sequences, i.e., vi= [v1, . . . , v i]. We use σ(·)to denote the sigmoid
function. The products ⊙and⊗denote the Hadamard (element-wise) product and the Kronecker
(block-wise) product, respectively. Indenotes the identity matrix of size Rn×n. Generally, we omit
stating the bias term for weight matrices unless stating the bias term helps with clarity.
22 Preliminaries
In this section, we introduce the key architectural components studied in this work: attention, SSMs,
and RNNs. We remark that these components are often the central block - considered to be the
backbone - within a complex architecture composed of other blocks and skip connections (see for
instance [ 13]). In what follows, we review exclusively the backbone block, which we denote as f(·)
iny=f(u), where u∈RL×dandy∈RL×dare the input and output sequences, respectively.
2.1 Attention
The standard self-attention block [ 2] consists of three matrices: WQ, WK,andWV, which are the
learnt parameters of the model. These matrices, when multiplied with the input u, yield the queries
q∈Rdk, keys k∈Rdk, and values v∈Rdv, respectively:
q=uWQ,k=uWK,v=uWV. (1)
Keys, queries, and values are then combined in the attention block to produce the output
y=ζqk⊤
√dk
v, (2)
where ζ(·)is a map RL→RLand is applied row-wise. For standard self-attention, the softmax
function is used, i.e. ζ(·) = softmax( ·), but given the limitations of the softmax function, alternative
formulations have been proposed. We consider two formulations of attention: softmax attention (2)
and linear attention [ 14]. We focus on masked attention formulations, i.e., the attention matrix ζ(qk⊤)
has a lower-triangular structure, and to simplify the derivations, we drop the scaling factor√dk.
2.2 State Space Models
Architectures based on a state space parametrization compute the output ythrough a dynamic
recurrence of input signals at each time step i, i.e.,
hi=Aihi−1+Biui (3a)
yi=Cihi+Diui, (3b)
where hiis the hidden state of the system, and the dynamic matrices of appropriate dimensions
Ai, Bi, Ci, Diare the learnt model parameters. Different time-varying and time-invariant parameteri-
zations for Ai, Bi, Ci, Dihave been proposed in the literature (an overview is given in [ 15]). Here
we discuss the most prominent one.
S6. The first selective SSM parametrization (S6) was introduced together with the Mamba architec-
ture [7]. The S6 block parametrizes the recurrence as
Ai=e−∆iA, B i= ∆ iWBui, C i=WCui, D i=WDui, (4)
with ∆i= softplus( W∆(Wuui) +b∆)for every i,W∆,Wu,WB,WC,WD, and Aare learnt
matrices of appropriate dimensions, and b∆is a learnt bias. While SSM models allow for complex-
valued matrices Ai, Bi, Ci, Di, here we restrict ourselves to real-valued matrices as in [7].
2.3 Recurrent Neural Networks
Similar to SSMs, RNNs also parameterize the input-output relationship via a recurrent computation,
commonly given by the long short-term memory (LSTM) [16], i.e., at each time step i
xi=fi⊙xi−1+ii⊙¯ui, (5a)
yi=oi⊙tanh( xi), (5b)
where ¯uirepresents the pre-processed raw input ui, i.e.,
¯ui= tanh( Wuui+Uuyi−1), (6)
andfi,ii, and oiare the forget gate, the input gate, and the output gate, respectively,
fi=σ(Wfui+Ufyi−1), i i=σ(Wiui+Uiyi−1), o i=σ(Woui+Uoyi−1), (7)
where Wf, Wi, WoandUf, Ui, Uoare the learnt gate parameters. In this paper, we focus on two
variants: quasi LSTMs (qLSTM) [ 9], which removes the output dependence of the gates, and
RG-LRU [10], which attempts to integrate ideas from SSMs into RNNs.
3qLSTM. The qLSTM model is parameterized by recurrence (5)with pre-processed input ¯uiand
gates fi,ii,oi:
¯ui= tanh( Wuui), f i=σ(Wfui), i i=σ(Wiui), o i=σ(Woui). (8)
RG-LRU. The RG-LRU model presents a hybrid between a qLSTM and a SSM using the recurrence
xi=ai⊙xi−1+q
1−a2
i⊙(ii⊙ui) (9a)
yi=xi, (9b)
with the following gates and no pre-processing of ui:
ri=σ(Waui), i i=σ(Wuui), a i=e−cri⊙softplus(Λ). (10)
3 Dynamical Systems Framework for Architecture Comparison
In this section, we introduce the Dynamical Systems Framework (DSF) that allows in-depth analysis
of the architectural features of attention, SSMs, and RNNs from a dynamical systems perspective. We
use this to rewrite the parametrizations in a common framework and provide detailed comparisons.
3.1 Dynamical Systems Framework (DSF)
The DSF relies on a dynamical systems representation of the architectures. A dynamical system
models how a system’s state, here denoted by h, evolves over time according to a difference or
differential equation. Dynamical systems often evolve under the evolution of some input u, and
the observable is an output y. These systems capture time-dependent processes, rendering them
suitable for understanding the behavior of sequence models. Here, we choose a recurrent state space
representation. This choice is motivated by the widespread use of state space model representations
for dynamical systems. Moreover, we show in later sections that this representation encompasses
attention, RNNs, and SSMs in a suitable fashion that allows for further analysis. In particular, a linear
structured time-varying (LTV) dynamical system is defined by the recurrence
hi= Λihi−1+Biui (11a)
yi=Cihi+Diui, (11b)
where hi∈RNis the hidden state initialized with h−1= 0,Λi∈RN×Nis the diagonal state
transition matrix, Bi∈RN×dandCi∈Rd×Nare the input and output matrices, respectively, and
Di∈Rd×dis a scaled skip connection. Dynamical system (11) can alternatively be written in its
convolutional representation, i.e., y=Φu, where the convolutional kernel Φis defined as
Φ=
C0B0+D0
C1Λ1B0 C1B1+D1
.........
CLQL
k=1ΛkB0 . . . C LΛLBL−1CLBL+DL
. (12)
Note that the convolution kernel Φis of the same dimension as the attention matrix ζ(qk⊤)and that
these matrices are equivalent, up to the scaling factor WVused in self-attention.
Remark 1. This recurrent view yields a causal convolution kernel by definition. However, certain
models (e.g. non-masked attention) also use non-causal kernels. This can be incorporated in the
DSF (11) by modifying the state update (11a) . For the sake of simplicity and consistency with the
recent literature, we stick with causal models in the following.
3.2 Architecture Reformulation
In the following, we show how popular architectures based on attention, SSMs, and RNNs can be
rewritten into the DSF. To do this, all models will be reformulated into recurrence (11), i.e., all
resulting DSF representations will have hidden state dimension N.2Although the parametrization
2The dimensions used in the following are visualized in Appendix A for clarity.
4of models commonly found in the literature is conductive to efficient computation, here we depart
from this convention. The goal of the DSF reformulation is to establish a theoretical framework
that leads us to mathematical insights on the design of these models. The presented formulations
are not intended to be computationally implemented in DSF form, however the framework can be
used to identify computational algorithms for new architectures. For instance, the convolutional
form of linear attention (12) is efficiently implemented via flash linear attention [ 17]. However,
using the recurrent form derived below it can also be implemented via scan algorithms [ 18], e.g.,
parallel scan [ 5,6] or accelerated scan [ 19]. Given that the structural requirements on the model
parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to
new models even if the algorithm was designed for another model class.
3.2.1 Attention
In the following, we assume that we can separate the nonlinear map in (2) as
ζ(q⊤
ikj) =ϕ(qi)⊤ψ(kj)
η(qi,ki), (13)
where ϕ(·) :Rm→Rn,ψ(·) :Rm→Rn, and η(·,·) :Rm×Rm×(i+1)→R, which is the case
for all the considered architectures in this paper. Note that if ζ(·)is a kernel function, the proposed
separability is satisfied by construction, as it holds that ϕ=ψandη= 1. This allows us to write the
self-attention input-output relationship as
yi=iX
j=0ϕ(qi)⊤ψ(kj)
η(qi,ki)WVuj, (14)
withqi=WQui∈Rm,kj=WVuj∈Rm, andWQ∈Rm×d, WK∈Rm×d,WV∈Rd×d. Hence,
equation (14) can be reformulated into the DSF (11) as a dynamical system of dimension N=nd,
i.e., with hidden state hi∈Rnd, and dynamic matrices
Λi=η(qi−1,ki−1)
η(qi,ki)Ind∈Rnd×nd, (15a)
Bi=1
η(qi−1,ki−1)Id⊗ψ(kj)
WV∈Rnd×d, (15b)
Ci=Id⊗ϕ(qi)⊤∈Rd×nd. (15c)
We note that for the recurrence (11), the matrix Λiis given as an nd×ndmatrix, where nis the
number of features in ϕandψ, and dis the input dimension. However, due to the scalar structure
ofΛiin(15a) , it can be implemented as the scalar multiplicationη(qi−1,ki−1)
η(qi,ki)hi−1in(11). Hence,
the hidden state is never materialized as such in the computation of the attention scores. Interested
readers are referred to Appendix B for a detailed derivation.
Linear Attention. In the case of linear attention , both maps ϕ(·)andψ(·)in the DSF parametriza-
tion (15) are separable and we use the kernel proposed in [14], i.e.,
ϕ(qi) =elu(qi) + 1, ψ(kj) =elu(kj) + 1, η(qi,ki) = ( elu(qi) + 1)iX
j=0(elu(kj) + 1) ,(16)
where elu (·)is the exponential linear unit.
Generalized Linear Attention. We also study generalized linear attention, where we require that
the maps ϕ(·),ψ(·)are linear, but allow for general nonlinear normalization functions η(qi,ki), i.e.,
ϕ(qi) =qi, ψ(kj) =kj, η(qi,ki). (17)
Softmax Attention. Softmax attention also satisfies the assumption of separability (13). However, it
holds that the feature vector representation of the transformed Gaussian kernel in the softmax function,
i.e.,eq⊤
ikj, is infinite dimensional. Hence, the DSF representation (15) of softmax attention (2)and its
corresponding hidden state dimension Nwould also be infinite dimensional. This insight gives further
motivation to approximations of the softmax function by using, e.g., a Taylor series approximation
such as in [20], to render the feature vector finite-dimensional.
5Lemma 1. Softmax attention (2)can be expressed by separable attention (13) with
ϕ(qi)⊤ψ(kj) =ϕ(qi)⊤ϕ(kj) =eq⊤
ikj, η(qi,ki) =iX
j=0eq⊤
ikj, (18)
where ϕ(qi) :=c·h
1, qi,N2
j=1qi,N3
j=1qi, . . .i
is an infinite-dimensional feature vector and cis a
matrix of constant coefficients.
Proof. The exponential in softmax attention eq⊤
ikjcan be expressed in terms of its Taylor expansion,
which consists of an infinite sum of polynomial kernels of increasing degree p, decomposable through
the vectors of monomialsNp
j=1qi. See Appendix C for a complete proof.
The work in [ 21] analyzes softmax attention as a kernel smoother and [ 14] shows that a kernel-based
formulation can lead to linear complexity in sequence length for finite dimensional kernels. In [ 22], a
kernel-based formulation of softmax is used to propose orthogonal random features to model softmax
attention with linear complexity. In [ 20] a Taylor approximation of softmax attention is proposed, also
leading to linear complexity. Finally, [ 23] relates transformer decoders to dynamical systems with
increasing state size arising from the masked upper triangular part of the attention matrix. Compared
to these works, we analyze how the proposed formulations compare in the recurrence (11) allowing
us to compare to SSMs and RNNs in the following sections.
3.2.2 State Space Models
SSM models are straightforward to rewrite in the DSF given their intrinsic recurrent linear representa-
tion. However, similarly to attention, we slightly rewrite the standard representation introduced in the
literature to reveal deeper insights obscured by the standard representation focused on computational
efficiency. The detailed derivation can be found in Appendix E.
S6. The S6 parametrization can be written in the DSF (11) as
Λi=e−(∆i⊗In)⊙A∈Rnd×nd, (19a)
Bi= ∆ i⊗bi∈Rnd×d, (19b)
Ci=Id⊗c⊤
i∈Rd×nd, (19c)
with∆i=diag(softplus (W∆(Wuui) +b∆))∈Rd×d,bi=WBui∈Rn, ci=WCui∈Rn, and
Wu∈Rp×d, W∆∈Rd×pare weight matrices with p < d , and b∆∈Rdis a bias. Note that in
formulation (19) the dimensions of the matrices are Λi∈Rnd×nd,Bi∈Rnd×d,Ci∈Rd×nd, i.e., n
is the state dimension and dis the input dimension in the original formulation (4).
3.2.3 Recurrent Neural Networks
Given their recurrent nature, one can express LSTMs (5)in the DSF with some basic algebraic
manipulations (see Appendix F for details). Once again, we slightly rewrite the standard representation
since our goal is to obtain mathematical insights as opposed to computational efficiency.
qLSTM. In order to write the qLSTM formulation (8)in the DSF (11), a small modification is
needed. In particular, the tanh functions in the input pre-processing (8)and output gate (5b) need to
be dropped. Hence, the reformulated qLSTM in the DSF (11) writes as
Λi= diag( σ(Wfui))∈Rd×d, (20a)
Bi= diag( σ(Wiui))⊙Wu∈Rd×d, C i= diag( σ(Woui))∈Rd×d, (20b)
where Wf, Wi, Wo, Wu∈Rd×dare the learnt parameters in (8). It is important to note that here the
dimension of the hidden state hiis equal to the number of input channels d, whereas in attention and
SSMs the dimension of the hidden state hiin the DSF (11) isnd. For qLSTMs n= 1, which will
become relevant in further discussions; we refer to the fact that n >1asstate expansion .
6RG-LRU. Given the similarities of RG-LRU [ 10] and SSMs, it is straightforward to reformulate it
into the DSF (11) without the need for modifications besides simple algebraic manipulations. Hence,
the RG-LRU can be expressed in the DSF as
Λi=e−cri⊙softplus( A)∈Rd×d, B i=q
1−Λ2
i⊙diag( σ(WBui))∈Rd×d, C i=Id,(21a)
where ri= diag( σ(WRui)), and the functionp
1−Λ2
iis applied elementwise to Λi. Similar to the
qLSTM and in contrast with the other models, RG-LRU does not have state expansion, i.e. n= 1.
4 Architecture Comparison: Theoretical and Experimental Results
In this section, we use the DSF to explore some of the long-standing questions between attention,
SSMs, and RNNs. We provide theoretical results and/or numerical experiments to substantiate
our claims. The experiments presented below are performed on the multi-query associate recall
(MQAR) [ 24] and long range arena (LRA) [ 3] benchmarks using the code bases3provided with
the benchmarks. The complete experimental setup and computational resources used are detailed in
Appendices J and K, respectively, and a statistical analysis is provided in Appendix L.
4.1 Softmax Attention vs. Separable Attention.
Separable attention is used to avoid computation of the query-key matrix qk⊤. It allows to compute
k⊤vbefore multiplying the queries q, which reduces the computational complexity from quadratic to
linear in sequence length. While the DSF shows how separable attention, and in particular kernelized
attention can be rewritten as a recurrence (11), such a reformulation is only practical for a finite
state dimension. However, in the case of softmax (·), an infinite-dimensional kernel is needed, i.e.,
in the DSF, softmax attention requires n=∞. This insight can mathematically explain why the
good performance observed for softmax attention can only be approximated by separable attention
mechanisms, SSMs, or RNNs; but no other architecture is equivalent. The DSF predicts that softmax
can be better approximated by growing n, which we show in the following theoretical result.
Lemma 2. For two dynamical systems (11) with hidden state dimensions Nand¯NwithN≤¯N,
the dynamical system of state dimension ¯Ncan always recover the dynamical system with state
dimension N.
Proof. The result follows from the fact that the first Nstates and the output in (11) can be chosen to
be independent of the additional states. The full proof is given in Appendix D.
32 64 128 256
State expansion /u1D45B0.00.51.0Accuracy
L: 256, KV-pairs: 16
32 64 128 256
State expansion /u1D45B
L: 512, KV-pairs: 64
Linear attention (16)
Softmax attention (2)
Figure 1: Comparison of linear attention and softmax attention on two MQAR tasks {(L=
256,KV-pairs = 16) ,(L= 512 ,KV-pairs = 64)}, fixed model size d= 512 , and varying state
expansion n. We report the best result from a learning rate sweep in np.logspace (−4,−2,4).
Therefore, it holds that the expressivity of a model is non-decreasing with increasing state expansion
n(and state dimension N=nd), if the rest of the architecture is held constant. As the softmax
attention has an infinite hidden state dimension, i.e. n=∞(Lemma 1), we investigate empirically
how its performance compares to linear attention (16), with increasing state dimension on the MQAR.
Figure 1 shows that with larger nlinear attention converges to the performance of softmax attention,
which achieves perfect accuracy throughout.
3https://github.com/HazyResearch/zoology; https://github.com/google-research/long-range-arena
764 128 256 512
Model size/u1D4510.00.51.0Accuracy
Figure 2: Model accuracy with increasing model size
dfor different models: softmax, linear, and normal-
ized attention, S6, and SSD. The MQAR task is
(L= 512 ,KV-pairs = 64) , we fix n= 128 , and re-
port the best performance of a learning rate sweep in
np.logspace (−4,−2,4).
64128256512Model size30.000.250.500.751.00AccuracySoftmax att. (2)Linear att. (16)Normalized att. (21)SSD [Dao and Gu, 2024]S6 [Gu and Dao, 2023]
64128256512Model size30.000.250.500.751.00AccuracySoftmax att. (2)Linear att. (16)Normalized att. (21)SSD [Dao and Gu, 2024]S6 [Gu and Dao, 2023]
64128256512Model size30.000.250.500.751.00AccuracySoftmax att. (2)Linear att. (16)Normalized att. (21)SSD [Dao and Gu, 2024]S6 [Gu and Dao, 2023]•Normalization strategy vs Discretization step.In attention(15), a normalization map⌘(·)252is used. This map enters as a fraction in⇤iand also as a denominator inBi. Given that253this map is a scalar, these two cancel out when computing the output, as one can see in254the convolution representation(12). Hence, in attentionQik=j⇤kBjevaluates to1⌘(qi,ki).255Notice that this does not occur in S6(18), since the only shared parameter–discretization256step i–does not cancel out in⇤iandBigiven their different structure. This impacts the257selectivity of the matrices on the input, since some input-dependent features are normalized258differently in the two architectures.259We remark that multi-headed attention increases the number of parameters in⇤ifrom1to the number260of headsh; for more details see Appendix F. While the number of parameters in the state transition261 idoes play a role in increasing performance, e.g., multi-headed attention typically performs better262than single-headed attention [Vaswani et al., 2017], the results in [Dao and Gu, 2024] suggest that263this role is small. The larger inﬂuence thus lies in the recursive structure of⇤iandBi. To further264investigate this and the role of the normalization in attention, we compare S6 and softmax attention to265linear attention [Katharopoulos et al., 2020] andnormalized attentionon the multi-query associative266recall (MQAR) benchmark [Arora et al., 2023]. Inspired by S6, we deﬁnenormalized attentionas267the attention function268 (qi)=qi, (kj)=kj,⌘(ui)=eWuui,(21)whereWu2R1⇥dis an additional learnt parameter. This comparison is shown in Figure xy, where269we notice ...270Additionally, we train the three attention-based methods on the WikiText-103 dataset [Merity et al.,2712016] and report the training perplexity in Table xy.272
64128256512Model size 0.000.250.500.751.00AccuracySoftmax att. (2)Linear att. (16)Normalized att. (21)SSD [Dao and Gu, 2024]S6 [Gu and Dao, 2023]
Figure 2: Todo.Table 1: Training perplexity score fordifferent Attention architectures (70Mparams) on the WikiText-103 corpus.Model PerplexityLinear Attention (16)17.42Norm. Attention (21)16.43Softmax Attention (2)13.15273
4.3 RNNs vs. S6274In the comparison of RNNs and S6 it is immediate to observe several similarities. In particular, as275shown in Appendix G, the state transition matrix⇤iin S6(18)can be rewritten when assuming276A=a·Indas277⇤i=diag( rev(¯W ui)a)⌦In.(22)Notice that when no state expansion is considered, i.e.,n=1andIn=1, this expression almost278coincides with the qLSTM state transition(19a), with the only difference that (I) it uses the reversed279sigmoid function instead of a sigmoid for the forget gate, and (II) there is an additional learnt280parametersain the exponent. As shown in the recent xLSTM paper [Beck et al., 2024], state281expanded LSTMs3can yield similar performance to S6. This aligns with Lemma 2, and further282highlights the importance of state expansion for expressivity. In the case of qLSTM and RG-283LRU, state expansion can be easily incorporated by changing the dimensions of the projections2843The presented state expanded LSTM versions, cannot be directly translated into the DSF framework, sincethe gates not only depend on the inputsuibut also on past outputsyi 1. However, the used state expansion isessentiallyn=d, hence leading to a DSF system of sized2.8Model LRA WikiText
Linear Att. (16) 53.52 17.42
Norm. Att. (22) 58.08 16.43
Softmax Att. (2) 55.96 13.15
S6 (4) 66.84 N/A
Table 1: Average accuracy on the LRA
benchmark and training perplexity score
for different attention architectures (70M
params) on the WikiText-103 corpus.
4.2 Generalized Linear Attention vs. S6.
By comparing the DSF expressions for both generalized linear attention (15) and S6 (19), we notice
that the S6 parameters bi=WBui∈Rn, ci=WCui∈Rndirectly correspond to the keys and
queries in attention, i.e. ki=biandqi=ci. Moreover, the state expansion nin S6 is the same as the
hidden dimension in attention. However, while this leads to an equivalent output matrix Ciin the
DSF parametrization for both architectures, there are remarkable differences between the two:
•Number of parameters. The transition matrix Λihasdparameters in S6 (19) and only 1
in attention. In attention (15),η(qi,k)is the only parameter in Λi, and it is by definition
a scalar. In S6, the parameters in Λiare determined by ∆i∈Rd, which has ddifferent
parameters. However, it was shown in [ 8] that the number of parameters in Λican be reduced
to1– similar to attention – without compromising performance. Note that multi-headed
attention increases the number of parameters in Λifrom 1to the number of heads s; for
more details see Appendix G.
•Normalization strategy vs Discretization step. In attention (15), a normalization map η(·)
is used. This map enters as a fraction in Λiand also as a denominator in Bi. Given that
this map is a scalar, these two cancel out when computing the output, as one can see in
the convolution representation (12). Hence, in attentionQi
k=jΛkBjevaluates to1
η(qi,ki).
Notice that this does not occur in S6 (19), since the only shared parameter – discretization
step∆i– does not cancel out in ΛiandBigiven their different structure. This impacts the
selectivity of the matrices on the input, since some input-dependent features are normalized
differently in the two architectures.
While the number of parameters in the state transition Λidoes play a role in increasing performance
(multi-headed attention typically performs better than single-headed attention [2]), the results in [8]
suggest that this role is small. The larger influence thus lies in the recursive structure of Λiand
Biand/or the parameterization of normalization η(·). To further investigate this and the role of
normalization in attention, we compare S6 and softmax attention to SSD [ 8], linear attention [ 14], and
normalized attention on the MQAR [ 24] and LRA [ 3] benchmarks and train the three attention-based
methods on WikiText-103. Inspired by S6, we define normalized attention as the attention function
ϕ(qi) =qi, ψ(kj) =kj, η(ui) =eWηui, (22)
where Wη∈R1×dis an additional learnt parameter. In Appendix H we discuss two alternatives
to(22). The MQAR results are shown in Figure 2 and the average accuracy on the LRA and the
training perplexity on WikiText-103 in Table 1. The MQAR results suggest that proper normalization,
i.e., using normalization (22), improves the performance of linear attention schemes. This is further
supported by the performance of S6 and SSD on the MQAR benchmark, since these two methods
also employ input-dependent normalization. Additionally, normalized attention closes part of the
gap to softmax attention on the WikiText-103 dataset (Table 1). However on LRA, SSM models
(S6) still achieve considerably higher performance than attention-based models. While normalized
8attention outperforms linear and softmax attention on the LRA, it performs significantly worse than
S6. This result suggests that while the S6 inspired normalization helps to improve performance, the
remaining performance gap is possibly explained by the recurrent normalization strategy employed by
selective SSM models. Overall these results warrant further research into normalization strategies for
attention-based models to explain the performance difference to SSMs. The complete experimental
results on the MQAR and LRA benchmarks are detailed in Appendices L and M, respectively.
4.3 RNNs vs. S6
Comparing RNNs and S6, it is immediate to observe several similarities. In particular, as shown in
Appendix I, the state transition matrix Λiin S6 (19) can be rewritten (assuming A=a·Ind) as
Λi=diag(σrev(¯W∆ui)a)⊗In. (23)
Notice that when no state expansion is considered, i.e., n= 1andIn= 1, this expression almost
coincides with the qLSTM state transition (20a) , with the only difference that (I) it uses the reversed
sigmoid function instead of a sigmoid for the forget gate, and (II) there is an additional learnt
parameter ain the exponent. Inspired by the subtle difference in the state transition, we compare the
original qLSTM state transition (8)and the S6-inspired state transition 23 on the MQAR benchmark.
The performance of both models is shown in Figure 3. We note that the reversed sigmoid state
transition outperforms the original state transition on all three benchmark tasks, i.e., the performance
of qLSTMs can be improved by insights from S6. Considering state expansion ( n >1) for RNNs,
the recent xLSTM paper [ 12] shows that state expanded LSTMs4can yield similar performance to S6.
This aligns with Lemma 2 and further highlights the importance of state expansion for expressivity.
In qLSTM and RG-LRU, state expansion can be easily incorporated by changing the dimensions of
the projections Wf, Wi, Wo, where the ⊙operation in RG-LRU would be replaced by blockwise
operations ⊗. Finally, the most apparent difference between the two RNN variants – qLSTM and
RG-LRU – and S6 is the parameter coupling in ΛiandBi. While qLSTM does not use a coupling,
the couplings in RG-LRU and S6 are performed with different nonlinearities, which is discussed in
more detail in [10, Appendix A].
64 128 256 512
Model size/u1D4510.800.850.900.951.00Accuracy
L: 64, KV-pairs: 4
qLSTM
qLSTM w/ (23)
64 128 256 512
Model size/u1D4510.60.81.0Accuracy
L: 128, KV-pairs: 8
64 128 256 512
Model size/u1D4510.000.250.500.751.00Accuracy
L: 256, KV-pairs: 16
Figure 3: Comparison of qLSTM (8)and a qLSTM variant where the original state transition Λiis
replaced by (23).
5 Related Work
State-space models emerged from the S4 architecture by Gu et al. [25], who developed a new theoreti-
cally principled approach to sequence modeling rooted in polynomial approximation theory [ 26]. The
result is a transformer-like architecture [ 2], where attention is replaced by a linear recurrent neural
network with special reparametrization. The design of S4 got later simplified in [ 4,27], achieving
state-of-the-art performance on the long-range arena (LRA) [ 3] with a highly efficient recurrent
mechanism leveraging convolutional views [28], or parallel scans [5, 6].
The high efficiency of SSMs (linear processing) makes them particularly appealing when compared
to softmax attention-based transformers, where both inference time and memory suffer quadratically
from sequence length. The S4 architecture found first successful applications in reinforcement
4The presented state expanded LSTM versions, cannot be directly translated into the DSF framework, since
the gates not only depend on the inputs uibut also on past outputs yi−1. However, the used state expansion is
essentially n=d, hence leading to a DSF system of size d2.
9learning [ 29], vision [ 30], audio [ 31] as well as online learning [ 32]. Initial attempts in language
modeling [ 33,34], supported by theoretical investigations [ 35,36] hint at some necessary architectural
improvements to unlock the NLP domain. Leveraging in-context learning arguments, a few works [ 37–
39] started incorporating input selectivity mechanisms [ 40] into SSMs. These efforts culminated
in the Mamba architecture [ 7], which proposed a highly efficient and light (in terms of parameters)
input selectivity strategy, with drastic improvements when comparing to earlier variants (H3 [ 33] and
Hyena [ 41]) on text. This approach was also shown to be effective at byte level [ 42]. Beyond text,
Mamba was recently applied to the vision domain [ 43,44] – with outstanding results compared to
ViTs [ 45] both in terms of performance and efficiency. Other applications include e.g. genetics [ 46],
and point clouds [47]. Further, improvements on architectural aspects were proposed in [8, 11, 48].
The design of Mamba is also strongly supported by theoretical evidence showing precisely its superior
expressive power compared to S4 [ 49]. This boost in computational power is due to Mamba’s novel
input selectivity mechanism resembling gating, which unlocks content-dependent reasoning [ 7,40].
Interestingly, input selectivity brings SSMs closer to attention: in particular, Ali et al. [50] showed
that the particular parametrization of Mamba can be linked to a non-normalized softmax operator.
This finding is also supported by evidence from language theory – Mamba and Attention can solve a
similar class of problems [ 51]. Beyond Ali et al. [50] the connection between linear attention and
linear RNNs has been illustrated a few times in the literature [14, 23, 52, 53]. Connections between
these architectures have also been carried out using tools from communication complexity in [ 54,55].
Compared to these works and to Ali et al. [50], this paper offers a more careful comparison identifying
some precise distinctions between SSMs, linear, and softmax attention – which play a nontrivial role
in practice and can help bring to light interesting architectural improvements.
6 Conclusion
In this paper we presented the DSF, a framework based on dynamical-systems theory that allows
analysis of different deep learning architectures by writing them as linear recurrences in state space.
We first showed how to reformulate different architectures into the DSF, and then explored (theoretical
and experimental) insights resulting from this analysis, thereby answering the questions posed in
the introduction. For instance, we showed that with proper normalization the performance of linear
attention can be significantly increased (see Fig. 2). We also show, that the DSF allows to integrate
insights from one architecture to another as exemplified by Section 4.2. Additionally, the DSF
naturally allows analysis of the eigenvalues of the state transition matrix A, which are linked to
the exploding/vanishing gradient problem [ 6]. In the case of SSMs and RNNs, the eigenvalues are
constrained to be stable by construction, for attention-based models this is not the case and stability
needs to be obtained via normalization. The eigenvalues together with the state expansion also affect
a model’s long-term memory [ 6]. Both of these aspects can be analyzed via the DSF and should
be further investigated in future work. While the training dynamics (especially convergence) can
be studied empirically using experiments, the DSF also allows a theoretical analysis. As discussed
in Example 2 of [ 56], a gradient-based optimization algorithm (e.g. SGD) can be interpreted and
written as a dynamical system. Using this viewpoint together with the DSF allows interpretation of
the training dynamics as two interacting dynamical systems. Therefore, the training dynamics can be
theoretically analyzed using tools from control theory, e.g., via Lyapunov theory for convergence and
stability of the training. However, we believe this question requires an in-depth investigation and
additional empirical validation of the theoretical findings. We expect that the DSF can serve as a tool
for principled analysis and design of deep learning architectures.
Limitations. In terms of limitations, it is important to highlight that, while the DSF parametrization
allows for a principled comparison between frameworks, architectures written in the DSF do not
necessarily enjoy an efficient implementation unless their specific structure can leverage some of the
existing algorithms (parallel scan, etc.). In terms of experiments, the insights mentioned above are
only verified on two synthetic tasks (MQAR/LRA) and a smaller language task (WikiText-103). To
strengthen the insights, a more detailed analysis is needed on larger and more complex tasks.
Acknowledgments and Disclosure of Funding
Carmen Amo Alonso was partially supported by an ETH AI Center Postdoctoral Fellowship.
10References
[1]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von
Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the
opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.
[2]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural
Information Processing Systems , volume 30. Curran Associates, Inc., 2017.
[3]Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng
Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena : A Benchmark for
Efficient Transformers. In International Conference on Learning Representations (ICLR) , 2021.
[4]Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. On the Parameterization and
Initialization of Diagonal State Space Models, 2022. URL https://arxiv.org/abs/2206.
11893 .
[5]Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified State Space Layers
for Sequence Modeling. In The Eleventh International Conference on Learning Representations ,
2023.
[6]Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan
Pascanu, and Soham De. Resurrecting Recurrent Neural Networks for Long Sequences. In
Proceedings of the 40th International Conference on Machine Learning , volume 202, pages
26670–26698. PMLR, 23–29 Jul 2023.
[7]Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces,
2023. URL https://arxiv.org/abs/2312.00752 .
[8]Tri Dao and Albert Gu. Transformers are SSMs: Generalized Models and Efficient Algorithms
with Structured State Space Duality. In ICML 2024 , 2024.
[9]Aleksandar Stani ´c, Dylan Ashley, Oleg Serikov, Louis Kirsch, Francesco Faccio, Jürgen
Schmidhuber, Thomas Hofmann, and Imanol Schlag. The Languini Kitchen: Enabling Language
Modelling Research at Different Scales of Compute, 2023.
[10] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru,
Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume
Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas,
and Caglar Gulcehre. Griffin: Mixing Gated Linear Recurrences with Local Attention for
Efficient Language Models, 2024. URL https://arxiv.org/abs/2402.19427 .
[11] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong.
HGRN2: Gated Linear RNNs with State Expansion. arXiv preprint arXiv:2404.07904 , 2024.
[12] Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova,
Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xLSTM:
Extended Long Short-Term Memory. arXiv preprint arXiv:2405.04517 , 2024.
[13] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
[14] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers
are RNNs: fast autoregressive transformers with linear attention. In Proceedings of the 37th
International Conference on Machine Learning , ICML’20. JMLR.org, 2020.
[15] Carmen Amo Alonso, Jerome Sieber, and Melanie N Zeilinger. State space models as foundation
models: A control theoretic overview. arXiv preprint arXiv:2403.16899 , 2024.
[16] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):
1735–1780, 1997.
11[17] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations
of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/
flash-linear-attention .
[18] Guy E Blelloch. Prefix sums and their applications. 1990.
[19] V olodymyr Kyrylov. Accelerated Scan, January 2024. URL https://github.com/proger/
accelerated-scan .
[20] Tobias Christian Nauen, Sebastian Palacio, and Andreas Dengel. Taylorshift: Shifting the
complexity of self-attention from squared to linear (and back) using taylor-softmax. arXiv
preprint arXiv:2403.02920 , 2024.
[21] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan
Salakhutdinov. Transformer dissection: a unified understanding of transformer’s attention via
the lens of kernel. arXiv preprint arXiv:1908.11775 , 2019.
[22] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,
Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking
attention with performers. arXiv preprint arXiv:2009.14794 , 2020.
[23] Matanel Oren, Michael Hassid, Yossi Adi, and Roy Schwartz. Transformers are multi-state
rnns. arXiv preprint arXiv:2401.06104 , 2024.
[24] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri
Rudra, and Christopher Ré. Zoology: Measuring and Improving Recall in Efficient Language
Models. arXiv:2312.04927 , 2023.
[25] Albert Gu, Karan Goel, and Christopher Ré. Efficiently Modeling Long Sequences with
Structured State Spaces. In The International Conference on Learning Representations (ICLR) ,
2022.
[26] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. HiPPO: Recurrent Memory
with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems ,
volume 33, pages 1474–1487. Curran Associates, Inc., 2020.
[27] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as
structured state spaces. In Advances in Neural Information Processing Systems , volume 35,
pages 22982–22994. Curran Associates, Inc., 2022.
[28] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolu-
tional models great on long sequence modeling? arXiv preprint arXiv:2210.09298 , 2022.
[29] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh,
and Feryal Behbahani. Structured state space models for in-context reinforcement learning.
Advances in Neural Information Processing Systems , 2023.
[30] Eric Nguyen, Karan Goel, Albert Gu, Gordon W. Downs, Preey Shah, Tri Dao, Stephen A.
Baccus, and Christopher Ré. S4nd: Modeling images and videos as multidimensional signals
using state spaces. Advances in Neural Information Processing Systems , 2022.
[31] Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. It’s raw! audio generation with
state-space models. arXiv preprint arXiv:2202.09729 , 2022.
[32] Nicolas Zucchet, Robert Meier, Simon Schug, Asier Mujika, and João Sacramento. Online
learning of long-range dependencies, 2023.
[33] Daniel Y . Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Ré.
Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL
https://arxiv.org/abs/2212.14052 .
[34] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without
attention. arXiv preprint arXiv:2212.10544 , 2022.
12[35] Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L Smith. Univer-
sality of linear recurrences followed by non-linear projections: Finite-width guarantees and
benefits of complex eigenvalues. International Conference on Machine Learning , 2024.
[36] Shida Wang and Beichen Xue. State-space models with layer-wise nonlinearity are universal
approximators with exponential decaying memory. Advances in Neural Information Processing
Systems , 2023.
[37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin
Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV , et al. Rwkv: Reinventing rnns for
the transformer era. arXiv preprint arXiv:2305.13048 , 2023.
[38] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang,
and Furu Wei. Retentive network: A successor to transformer for large language models, 2023.
[39] Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2023.
[40] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning
and induction heads. arXiv preprint arXiv:2209.11895 , 2022.
[41] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua
Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional
language models. In International Conference on Machine Learning , pages 28043–28078.
PMLR, 2023.
[42] Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, and Alexander M Rush. Mambabyte:
Token-free selective state space model. arXiv preprint arXiv:2401.13660 , 2024.
[43] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and
Yunfan Liu. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 , 2024.
[44] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang.
Vision mamba: Efficient visual representation learning with bidirectional state space model.
2024.
[45] Hugo Touvron, Matthieu Cord, and Herve Jegou. Deit iii: Revenge of the vit. arXiv preprint
arXiv:2204.07118 , 2022.
[46] Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and V olodymyr Kuleshov.
Caduceus: Bi-directional equivariant long-range dna sequence modeling. arXiv preprint
arXiv:2403.03234 , 2024.
[47] Jiuming Liu, Ruiji Yu, Yian Wang, Yu Zheng, Tianchen Deng, Weicai Ye, and Hesheng Wang.
Point mamba: A novel point cloud backbone based on state space model with octree-based
ordering strategy. arXiv preprint arXiv:2403.06467 , 2024.
[48] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear
attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635 , 2023.
[49] Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, and Terry Lyons.
Theoretical foundations of deep selective state-space models. arXiv preprint arXiv:2402.19047 ,
2024.
[50] Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. arXiv
preprint arXiv:2403.01590 , 2024.
[51] William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space
models. arXiv preprint arXiv:2404.08819 , 2024.
[52] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast
weight programmers. In International Conference on Machine Learning , pages 9355–9366.
PMLR, 2021.
13[53] Nicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes V on Oswald, Maxime Larcher,
Angelika Steger, and Joao Sacramento. Gated recurrent neural networks discover attention.
arXiv preprint arXiv:2309.01775 , 2023.
[54] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou,
Atri Rudra, and Christopher Re. Simple linear attention language models balance the recall-
throughput tradeoff. In Forty-first International Conference on Machine Learning , 2024.
[55] Satwik Bhattamishra, Michael Hahn, Phil Blunsom, and Varun Kanade. Separations in
the representational capabilities of transformers and recurrent architectures. arXiv preprint
arXiv:2406.09347 , 2024.
[56] Florian Dörfler, Zhiyu He, Giuseppe Belgioioso, Saverio Bolognani, John Lygeros, and Michael
Muehlebach. Towards a systems theory of algorithms. IEEE Control Systems Letters , 2024.
[57] Amnon Shashua. Introduction to machine learning: Class notes 67577. arXiv preprint
arXiv:0904.3664 , 2009.
[58] Daniel López-Sánchez, Angélica González Arrieta, and Juan M Corchado. Data-independent
random projections from the feature-space of the homogeneous polynomial kernel. Pattern
Recognition , 82:130–146, 2018.
[59] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In International
Conference on Learning Representations , 2019. URL https://openreview.net/forum?
id=Bkg6RiCqY7 .
[60] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel
Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In
H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in
Neural Information Processing Systems , volume 33, pages 1877–1901. Curran Associates,
Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
[61] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien,
Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward
Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In
International Conference on Machine Learning , pages 2397–2430. PMLR, 2023.
14A Visual Representation of the matrix dimensions
Figure 4 represents the dimensions of the recurrence expressed by the linear structured time-varying
(LTV) dynamical system described in (11):
hi= Λihi−1+Biui,
where hi∈RNis the hidden state, Λi∈RN×Nis the diagonal state transition matrix, and
Bi∈RN×dis the input matrix. We highlight the role of the state expansion, where u∈Rdand
h∈RN=Rnd.
ui<latexit sha1_base64="04hCVQiYkcZDOnIjeK+Z/w8zMe4=">AAAB6nicdVBNS8NAEJ3Ur1q/qh69LBbBU0lKattb0YvHivYD2lA22027dLMJuxuhhP4ELx4U8eov8ua/cdNWUNEHA4/3ZpiZ58ecKW3bH1ZubX1jcyu/XdjZ3ds/KB4edVSUSELbJOKR7PlYUc4EbWumOe3FkuLQ57TrT68yv3tPpWKRuNOzmHohHgsWMIK1kW6TIRsWS3bZcR3baSBDLmpOxTWkWnUrtTpyyvYCJVihNSy+D0YRSUIqNOFYqb5jx9pLsdSMcDovDBJFY0ymeEz7hgocUuWli1Pn6MwoIxRE0pTQaKF+n0hxqNQs9E1niPVE/fYy8S+vn+ig7qVMxImmgiwXBQlHOkLZ32jEJCWazwzBRDJzKyITLDHRJp2CCeHrU/Q/6VRMUOXGjVtqXq7iyMMJnMI5OFCDJlxDC9pAYAwP8ATPFrcerRfrddmas1Yzx/AD1tsnwaqOJQ==</latexit>
d<latexit sha1_base64="ZKt3GAg/5ze2BEY7GFOxAInTe4k=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjy2YGuhDWWzmbRrN5uwuxFK6S/w4kERr/4kb/4bt20O2vpg4PHeDDPzglRwbVz32ymsrW9sbhW3Szu7e/sH5cOjtk4yxbDFEpGoTkA1Ci6xZbgR2EkV0jgQ+BCMbmf+wxMqzRN5b8Yp+jEdSB5xRo2VmmG/XHGr7hxklXg5qUCORr/81QsTlsUoDRNU667npsafUGU4Ezgt9TKNKWUjOsCupZLGqP3J/NApObNKSKJE2ZKGzNXfExMaaz2OA9sZUzPUy95M/M/rZia68idcpplByRaLokwQk5DZ1yTkCpkRY0soU9zeStiQKsqMzaZkQ/CWX14l7YuqV6teN2uV+k0eRxFO4BTOwYNLqMMdNKAFDBCe4RXenEfnxXl3PhatBSefOYY/cD5/AMvJjPQ=</latexit>
state<latexit sha1_base64="j797/Tsul6SAiFBlnYCFcQ+LuAs=">AAAB83icbVBNS8NAEN3Ur1q/qh69LBbBU0mkoN6KXjxWsB/QhLLZTtulm03YnYgl9G948aCIV/+MN/+N2zYHbX0w8Hhvhpl5YSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCoZeJUc2jyWMa6EzIDUihookAJnUQDi0IJ7XB8O/Pbj6CNiNUDThIIIjZUYiA4Qyv5PsITZgYZwrRXrrhVdw66SrycVEiORq/85fdjnkagkEtmTNdzEwwyplFwCdOSnxpIGB+zIXQtVSwCE2Tzm6f0zCp9Ooi1LYV0rv6eyFhkzCQKbWfEcGSWvZn4n9dNcXAVZEIlKYLii0WDVFKM6SwA2hcaOMqJJYxrYW+lfMQ042hjKtkQvOWXV0nrourVqtf3tUr9Jo+jSE7IKTknHrkkdXJHGqRJOEnIM3klb07qvDjvzseiteDkM8fkD5zPH9M4kjg=</latexit>
expansion<latexit sha1_base64="gTlemHBlFUnEgl4iCMVoiRKSWGg=">AAAB+XicbVDLSsNAFJ3UV62vqEs3wSK4KokU1F3RjcsK9gFtKJPpTTt0MgkzN6Ul9E/cuFDErX/izr9x2mahrQcGDuecy71zgkRwja77bRU2Nre2d4q7pb39g8Mj+/ikqeNUMWiwWMSqHVANgktoIEcB7UQBjQIBrWB0P/dbY1Cax/IJpwn4ER1IHnJG0Ug92+4iTDCDSULlPDTr2WW34i7grBMvJ2WSo96zv7r9mKURSGSCat3x3AT9jCrkTMCs1E01JJSN6AA6hkoagfazxeUz58IofSeMlXkSnYX6eyKjkdbTKDDJiOJQr3pz8T+vk2J442dcJimCZMtFYSocjJ15DU6fK2AopoZQpri51WFDqihDU1bJlOCtfnmdNK8qXrVy+1gt1+7yOorkjJyTS+KRa1IjD6ROGoSRMXkmr+TNyqwX6936WEYLVj5zSv7A+vwBixmURQ==</latexit>
+<latexit sha1_base64="5gUpEueemIOpL4kdq4Bq/9+7w9Q=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXAuot6MVjAuYByRJmJ73JmNnZZWZWCCFf4MWDIl79JG/+jZNkD5pY0FBUddPdFSSCa+O6305ubX1jcyu/XdjZ3ds/KB4eNXWcKoYNFotYtQOqUXCJDcONwHaikEaBwFYwupv5rSdUmsfywYwT9CM6kDzkjBor1S96xZJbducgq8TLSAky1HrFr24/ZmmE0jBBte54bmL8CVWGM4HTQjfVmFA2ogPsWCpphNqfzA+dkjOr9EkYK1vSkLn6e2JCI63HUWA7I2qGetmbif95ndSE1/6EyyQ1KNliUZgKYmIy+5r0uUJmxNgSyhS3txI2pIoyY7Mp2BC85ZdXSfOy7FXKN/VKqXqbxZGHEziFc/DgCqpwDzVoAAOEZ3iFN+fReXHenY9Fa87JZo7hD5zPH3VljLs=</latexit>
Bi<latexit sha1_base64="Y1JszqZ8OmJNll0qcDppJ1QHUuk=">AAAB6nicdVDJSgNBEK2JW4xb1KOXxiB4GnriSJJbiBePEc0CyRB6Oj1Jk56F7h4hDPkELx4U8eoXefNv7CyCij4oeLxXRVU9PxFcaYw/rNza+sbmVn67sLO7t39QPDxqqziVlLVoLGLZ9YligkespbkWrJtIRkJfsI4/uZr7nXsmFY+jOz1NmBeSUcQDTok20m1jwAfFErYd7Jaxi7CNa5WLsmOIW8WXFYwcGy9QghWag+J7fxjTNGSRpoIo1XNwor2MSM2pYLNCP1UsIXRCRqxnaERCprxsceoMnRlliIJYmoo0WqjfJzISKjUNfdMZEj1Wv725+JfXS3VQ9TIeJalmEV0uClKBdIzmf6Mhl4xqMTWEUMnNrYiOiSRUm3QKJoSvT9H/pF22Hdeu3bilemMVRx5O4BTOwYEK1OEamtACCiN4gCd4toT1aL1Yr8vWnLWaOYYfsN4+AWRCjec=</latexit>
⇤i<latexit sha1_base64="F4ma3iMEEJ2En+7gthhygjGuIao=">AAAB8HicdVDLSgMxFM3UV62vqks3wSK4GjJ1pO2u6MaFiwr2Ie1QMplMG5pkhiQjlNKvcONCEbd+jjv/xvQhqOiBwOGcc8m9J0w50wahDye3srq2vpHfLGxt7+zuFfcPWjrJFKFNkvBEdUKsKWeSNg0znHZSRbEIOW2Ho8uZ376nSrNE3ppxSgOBB5LFjGBjpbvetY1GuM/6xRJyPeSXkQ+Ri2qVs7JniV9F5xUEPRfNUQJLNPrF916UkExQaQjHWnc9lJpggpVhhNNpoZdpmmIywgPatVRiQXUwmS88hSdWiWCcKPukgXP1+8QEC63HIrRJgc1Q//Zm4l9eNzNxNZgwmWaGSrL4KM44NAmcXQ8jpigxfGwJJorZXSEZYoWJsR0VbAlfl8L/Savser5bu/FL9YtlHXlwBI7BKfBABdTBFWiAJiBAgAfwBJ4d5Tw6L87rIppzljOH4Aect0/gRZB+</latexit>
hi<latexit sha1_base64="EAMXoSihWgrgzsUbFtIq+OIYQg0=">AAAB6nicdVDLSgNBEOz1GeMr6tHLYBA8LbNxJckt6MVjRPOAZAmzk0kyZHZ2mZkVwpJP8OJBEa9+kTf/xslDUNGChqKqm+6uMBFcG4w/nJXVtfWNzdxWfntnd2+/cHDY1HGqKGvQWMSqHRLNBJesYbgRrJ0oRqJQsFY4vpr5rXumNI/lnZkkLIjIUPIBp8RY6XbU471CEbse9kvYR9jF1fJ5ybPEr+CLMkaei+cowhL1XuG9249pGjFpqCBadzycmCAjynAq2DTfTTVLCB2TIetYKknEdJDNT52iU6v00SBWtqRBc/X7REYirSdRaDsjYkb6tzcT//I6qRlUgozLJDVM0sWiQSqQidHsb9TnilEjJpYQqri9FdERUYQam07ehvD1KfqfNEuu57vVG79Yu1zGkYNjOIEz8KAMNbiGOjSAwhAe4AmeHeE8Oi/O66J1xVnOHMEPOG+fniaODQ==</latexit>
d<latexit sha1_base64="ZKt3GAg/5ze2BEY7GFOxAInTe4k=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjy2YGuhDWWzmbRrN5uwuxFK6S/w4kERr/4kb/4bt20O2vpg4PHeDDPzglRwbVz32ymsrW9sbhW3Szu7e/sH5cOjtk4yxbDFEpGoTkA1Ci6xZbgR2EkV0jgQ+BCMbmf+wxMqzRN5b8Yp+jEdSB5xRo2VmmG/XHGr7hxklXg5qUCORr/81QsTlsUoDRNU667npsafUGU4Ezgt9TKNKWUjOsCupZLGqP3J/NApObNKSKJE2ZKGzNXfExMaaz2OA9sZUzPUy95M/M/rZia68idcpplByRaLokwQk5DZ1yTkCpkRY0soU9zeStiQKsqMzaZkQ/CWX14l7YuqV6teN2uV+k0eRxFO4BTOwYNLqMMdNKAFDBCe4RXenEfnxXl3PhatBSefOYY/cD5/AMvJjPQ=</latexit>
n<latexit sha1_base64="EwlY8lBbFjnTUZZCPzKnslbsyV4=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjy2YGuhDWWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDDPzgkRwbVz32ymsrW9sbhW3Szu7e/sH5cOjto5TxbDFYhGrTkA1Ci6xZbgR2EkU0igQ+BCMb2f+wxMqzWN5byYJ+hEdSh5yRo2VmrJfrrhVdw6ySrycVCBHo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/IzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ+6Lq1arXzVqlfpPHUYQTOIVz8OAS6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8f2vGM/g==</latexit>
n<latexit sha1_base64="EwlY8lBbFjnTUZZCPzKnslbsyV4=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjy2YGuhDWWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDDPzgkRwbVz32ymsrW9sbhW3Szu7e/sH5cOjto5TxbDFYhGrTkA1Ci6xZbgR2EkU0igQ+BCMb2f+wxMqzWN5byYJ+hEdSh5yRo2VmrJfrrhVdw6ySrycVCBHo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/IzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ+6Lq1arXzVqlfpPHUYQTOIVz8OAS6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8f2vGM/g==</latexit>
n<latexit sha1_base64="EwlY8lBbFjnTUZZCPzKnslbsyV4=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjy2YGuhDWWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDDPzgkRwbVz32ymsrW9sbhW3Szu7e/sH5cOjto5TxbDFYhGrTkA1Ci6xZbgR2EkU0igQ+BCMb2f+wxMqzWN5byYJ+hEdSh5yRo2VmrJfrrhVdw6ySrycVCBHo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/IzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ+6Lq1arXzVqlfpPHUYQTOIVz8OAS6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8f2vGM/g==</latexit>
N<latexit sha1_base64="TuI4H4uSJQt41qP5CqwsvE8ICww=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKQL0FvXiSBMwDkiXMTnqTMbOzy8ysEEK+wIsHRbz6Sd78GyfJHjSxoKGo6qa7K0gE18Z1v53c2vrG5lZ+u7Czu7d/UDw8auo4VQwbLBaxagdUo+ASG4Ybge1EIY0Cga1gdDvzW0+oNI/lgxkn6Ed0IHnIGTVWqt/3iiW37M5BVomXkRJkqPWKX91+zNIIpWGCat3x3MT4E6oMZwKnhW6qMaFsRAfYsVTSCLU/mR86JWdW6ZMwVrakIXP198SERlqPo8B2RtQM9bI3E//zOqkJr/wJl0lqULLFojAVxMRk9jXpc4XMiLEllClubyVsSBVlxmZTsCF4yy+vkuZF2auUr+uVUvUmiyMPJ3AK5+DBJVThDmrQAAYIz/AKb86j8+K8Ox+L1pyTzRzDHzifP6pxjN4=</latexit>
N<latexit sha1_base64="TuI4H4uSJQt41qP5CqwsvE8ICww=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKQL0FvXiSBMwDkiXMTnqTMbOzy8ysEEK+wIsHRbz6Sd78GyfJHjSxoKGo6qa7K0gE18Z1v53c2vrG5lZ+u7Czu7d/UDw8auo4VQwbLBaxagdUo+ASG4Ybge1EIY0Cga1gdDvzW0+oNI/lgxkn6Ed0IHnIGTVWqt/3iiW37M5BVomXkRJkqPWKX91+zNIIpWGCat3x3MT4E6oMZwKnhW6qMaFsRAfYsVTSCLU/mR86JWdW6ZMwVrakIXP198SERlqPo8B2RtQM9bI3E//zOqkJr/wJl0lqULLFojAVxMRk9jXpc4XMiLEllClubyVsSBVlxmZTsCF4yy+vkuZF2auUr+uVUvUmiyMPJ3AK5+DBJVThDmrQAAYIz/AKb86j8+K8Ox+L1pyTzRzDHzifP6pxjN4=</latexit>
Figure 4: Visual representation of the dimensions considered in the DSF.
B Derivation of Separable Attention into DSF
We consider the layer
yi=iX
j=0f(qi, kj,ki)WVuj,
where we define the sequence of keys as
ki={k0, . . . , k i}.
We show that this layer can be equivalently written as the LTV system (11), i.e.,
yi=iX
j=0Ci
iY
k=j+1Λk
Bjuj,
withΛi∈Rnd×nd,Bi∈Rnd×dandCi∈Rd×nd, if the function f(·,·)is separable as follows
f(qi,ki) =ϕ(qi)⊤ψ(kj)
η(qi,ki),
where ϕ(qi)∈Rn,ψ(kj)∈Rn, andη(qi,ki)∈Rcan be considered to be a normalization function.
If the unnormalized part of f(·,·)is a kernel function, it holds that ψ(kj) =ϕ(kj)for some, possibly
infinite dimensional, feature vector ϕ.
We can compare the output formulations
y0=C0B0u0
and
y0=f(q0, k0,k0)WVu0
=ϕ(q0)⊤ 1
η(q0,k0)ψ(k0)WVu0,
15resulting in
B0=1
η(q0,k0)Id⊗ψ(k0)
WV∈Rnd×d, C 0=Id⊗ϕ(q0)⊤∈Rd×nd
Simlarly, we have
y1=C1B1u1+C1Λ1B0u0
y1=f(q1, k1,k1)WVu1+f(q1, k0,k1)WVu0
=ϕ(q1)⊤ 1
η(q1,k1)ψ(k1)WVu1+ϕ(q1)⊤ 1
η(q1,k1)ψ(k0)WVu0
⇒B1=1
η(q1,k1)Id⊗ψ(k1)
WV, C1=Id⊗ϕ(q1)⊤andΛ1=η(q0,k0)
η(q1,k1)Ind
and
y2=C2B2u2+C2Λ2B1u1+C2Λ2Λ1B0u0
y2=f(q2, k2,k2)WVu2+f(q2, k1,k2)WVu1+f(q2, k0,k2)WVu0
y2=ϕ(q2)⊤ 1
η(q2,k2)ψ(k2)WVu2+ϕ(q2)⊤ 1
η(q2,k2)ψ(k1)WVu1+ϕ(q2)⊤ 1
η(q2,k2)ψ(k0)WVu0
⇒B2=1
η(q2,k2)Id⊗ψ(k2)
WV, C2=Id⊗ϕ(q2)⊤andΛ2=η(q1,k1)
η(q2,k2)Ind.
Plugging this back in, we observe
y2=C2B2u2+C2Λ2B1u1+C2Λ2Λ1B0u0
=Id⊗ϕ(q2)⊤1
η(q2,k2)Id⊗ψ(k2)
WVu2
+Id⊗ϕ(q2)⊤η(q1,k1)
η(q2,k2)Ind1
η(q1,k1)Id⊗ψ(k1)
WVu1
+Id⊗ϕ(q2)⊤η(q1,k1)
η(q2,k2)Indη(q0,k0)
η(q1,k1)Ind1
η(q0,k0)Id⊗ψ(k0)
WVu0
=ϕ(q2)⊤ 1
η(q2,k2)ψ(k2)WVu2+ϕ(q2)⊤ 1
η(q2,k2)ψ(k1)WVu1+ϕ(q2)⊤ 1
η(q2,k2)ψ(k0)WVu0
=f(q2, k2,k2)WVu2+f(q2, k1,k2)WVu1+f(q2, k0,k2)WVu0
This generalizes the dynamical system matrices to
Bi=1
η(qi,ki)Id⊗ψ(ki)
WV,
Ci=Id⊗ϕ(qi)⊤,
Λi=η(qi−1,ki−1)
η(qi,ki)Ind.
C Proof of Lemma 1: Derivation of Softmax Attention into DSF
The softmax function Rn×m→(0,1]n×mis defined through row-wise normalization and is given by
softmax (z) :=

ez0,0Pm−1
j=0ez0,j. . .ez0,mPm−1
j=0ez0,j
.........
ezn,0Pm−1
j=0ezn,j . . .ezn,mPm−1
j=0ezn,j

.
The attention block is given as
yi=iX
j=0ζi,j(q⊤k)vj=iX
j=0softmax i,j(uWQW⊤
Ku⊤)WVuj,
16where softmax i,j(·)refers to the element (i, j)of the corresponding matrix. Note that eq⊤
ikjis a
kernel, implying that softmax attention can be brought into the separable form (13). In order to
provide the separable form of eq⊤
ikj, we first consider the Taylor expansion of the kernel, which is
given by
eq⊤
ikj=∞X
p=0(q⊤
ikj)p
p!.
Each polynomial (q⊤
ikj)prepresents itself a homogeneous polynomial kernel and its decomposition
into a feature vector of
n+p−1
p
monomials, as shown in [57], is given by
˜ϕp(x) ="r
p!
n1!n2!···nn!xn1
1···xnn
n#
ni≥0,P
ini=p. (24)
The feature representation of the exponential kernel is therefore
eq⊤
ikj=h
1, qi,q
1
2!˜ϕ2(qi)⊤,q
1
3!˜ϕ3(qi)⊤, . . .ih
1, kj,q
1
2!˜ϕ2(kj)⊤,q
1
3!˜ϕ3(kj)⊤, . . .i⊤
(25)
:=ϕ(qi)⊤ϕ(kj).
Note that to attain the monomials in (24) for a given p, one can also useNp
j=1x, as given in, e.g., [ 58],
which is equivalent up to the constant coefficients, such that we can useq
1
p!˜ϕp(x) =cpNp
j=1x,
where cpis a matrix of the respective coefficients multiplying each monomial.
D Proof of Lemma 2
Given two dynamical systems of the form (11), we denote the system of hidden state dimension N
with the state hN
iand the matrices ΛN
i,BN
i,CN
iandDN
iand correspondingly, the system of hidden
state dimension ¯Nusing the state h¯N
iand the matrices Λ¯N
i,B¯N
i,C¯N
iandD¯N
i. We show that the
system of dimension ¯N≥Ncan recover the system of dimension Nby selecting the following
system matrices:
Λ¯N
i=ΛN
i0
¯Λ ˆΛ
, B¯N
i=
BN
i¯B
,
C¯N
i=
CN
i0
, D¯N
i=DN
i.
It can be seen that the Nfirst states of the system with dimension ¯Npropagate equivalently to the
states of the system of dimension N. The additional states evolve independently given any matrices
¯Λ,ˆΛ,¯Bof appropriate dimension and do not affect the output, such that both systems are equivalent.
The two outputs are then equivalent by setting the corresponding entries of the C¯N
imatrix to 0.
E Derivation of S6 into DSF
While Ain(4)is represented as a dense matrix of size n×dpurely for computational reasons,
mathematically Ais a diagonal matrix of size nd×nd. This is evident from the fact that S6
parameterizes a different submatrix Ad∈Rn×nfor each embedding dimension d, leading to
A=
A1
...
Ad
.
To compute Λiin(11), the matrix Ais multiplied with the selective discretization time ∆i∈Rd×d,
which is computed as
∆i=diag(softplus (W∆(Wuui) +b∆)), (26)
17where Wu∈Rp×d, W∆∈Rd×pare weight matrices with p < d , and b∆∈Rdis a bias. Note
that we embed the computed discretization times in a diagonal d×dmatrix to simplify the next
reformulations. The product of ∆iandAis performed along the embedding dimension axis, i.e.,

∆1
iA1
...
∆d
iAd
= (∆ i⊗In)⊙A. (27)
To arrive at the DSF formulation (19), it only remains to take the exponential function of (27) and
stateBi,Cias in (4) with the appropriate dimensions.
F Derivation of LSTMs into DSF
F.1 RG-LRU
In order to replace the abundant elementwise operations ⊙in LSTMs with more suitable matrix-vector
multiplications for SSMs, we rely on the following observation for ai∈Rd, ui∈Rd:
σ(ai)⊙ui=
σ(a1
i)u1
i...
σ(ad
i)ud
i
=
σ(a1
i)
...
σ(ad
i)

u1
i...
ud
i
= diag( σ(ai))ui.
As with S6 in Appendix E, we reformulate some quantities for easier presentation, namely we embed
the input-dependent vectors σ(WRui)∈Rd, σ(WBui)∈Rd, where σ(·)denotes the sigmoid
function, in a diagonal matrix, i.e.,
ri= diag( σ(WRui)) =
r1
i
...
rd
i
, b i= diag( σ(WBui)) =
b1
i
...
bd
i
.
The DSF representation (21) is then obtained in a straightforward manner.
F.2 qLSTM
We start by using the same reformulation of the gates as in RG-LRU above:
fi= diag( σ(Wfui)), i i= diag( σ(Wiui)), o i= diag( σ(Woui)),
where fiis commonly called the forget gate, iiandoiare called the input and output gates, re-
spectively, and Wf, Wi, Wo∈Rd×d. By removing the tanh activation function, we effectively
eliminated the input activation gate, which now serves as a standard input to the recurrence (11), i.e.,
we reformulated the standard qLSTM (8) to the LTV
xi=fixt−1+ (ii⊙Wu)ui
yi=oihi.
G Dynamical System Derivation of Multi-headed Separable Attention
As in Appendix B, we assume a separable attention function, i.e.,
f(qi, kj,ki) =ϕ(qi)⊤ψ(kj)
η(qi,ki).
Additionally, we consider the multi-headed setting introduced in [ 2, Section 3.2.2], i.e., sdifferent
attention operations are performed in parallel. Due to the right multiplication of the input (instead
of left multiplication as in the original paper), the output of the different heads is stacked row-wise
instead of column-wise. Additionally, we assume there is no output mapping after the attention
18operation. This yields the simplified multi-headed layer
yi=iX
j=0
[ϕ(qi)⊤ψ(kj)
η(qi,ki)vj]1
...
[ϕ(qi)⊤ψ(kj)
η(qi,ki)vj]s
=iX
j=0
[ϕ(qi)⊤ψ(kj)
η(qi,ki)]1
...
[ϕ(qi)⊤ψ(kj)
η(qi,ki)]s

[vj]1
...
[vj]s

=iX
j=0
[ϕ(qi)⊤]1
...
[ϕ(qi)⊤]s

[1
η(qi,ki)]1·In/s
...
[1
η(qi,ki)]s·In/s

·
[ψ(kj)]1
...
[ψ(kj)]s

[vj]1
...
[vj]s
,
where·sdenotes the head index. As is standard for multi-headed attention, we reduce the dimensions
of the queries, keys, and values by the number of heads, i.e., qi∈Rm/s,kj∈Rm/s, andvj∈Rd/s.
Since the sdifferent values [vj]sare stacked, this is equivalent to the single headed version, i.e.,

[vj]1
...
[vj]s
=vj=WVuj.
Above observation is also valid for the queries and keys, i.e., we can e.g. write [ϕ(qi)]susing an
indicator function Is(·)on the single headed query qi, i.e.,
[ϕ(qi)]s=ϕ(Is(qi)) =ϕ(Is(WQui)).
This shows the intuition behind multi-headed attention, which essentially compares parts of the
single-headed queries and keys in parallel. Therefore, we can use Appendix B to write multi-headed
separable attention in the DSF as
Λi=diag[η(qi−1,ki−1)]1
[η(qi,ki)]1Id/s, . . . ,[η(qi−1,ki−1)]s
[η(qi,ki)]sId/s
⊗In∈Rnd×nd, (28a)
Bi=
diag1
[η(qi−1,ki−1)]1Id/s, . . . ,1
[η(qi−1,ki−1)]sId/s
⊗ψ(Is(ki))
WV∈Rnd×d,
(28b)
Ci=Id⊗ϕ(Is(qi))⊤∈Rd×nd. (28c)
Multiple heads therefore extend the single scalar in Λi(in the single-headed case) to sdifferent
scalars, however these only act upon a part of the queries qiand keys kjdue to the indicator function.
H Alternative Normalization Schemes
For all experiments in Section 4.2, we use the normalization scheme in (22). The exponential
normalization function η(ui)is inspired by softmax attention and S6, which both use exponential
functions for normalization (see (18) and(19)). However, other normalization functions can also be
considered e.g.
η(ui) =softplus (Wηui), (29)
η(ui) =σ(Wηui), (30)
where σ(·)denotes the sigmoid function. Table 2 shows an experimental comparison of the exponen-
tial normalization function (22) and the two alternatives (29),(30) on the LRA Image and MQAR
(L= 512 ,KV-pairs = 64) tasks. All three normalization schemes perform similarly on both tasks,
however the exponential normalization (22) yields the best performance, which is the reason we
choose it for normalized attention throughout the paper.
19Table 2: Accuracy of the three normalization functions (22),(29),(30) on LRA Image and MQAR
(L= 512 ,KV-pairs = 64)
Normalization FunctionTask [%]
LRA Image MQAR (L= 512 ,KV-pairs = 64)
Exponential (22) 35.96 85.9
Softplus (29) 35.27 84.3
Sigmoid (30) 35.80 84.7
I S6 uses reversed sigmoid in state transition matrix
In the following, we show that the state transition matrix Λiin S6 is essentially a reversed sigmoid of
the projected input. To show this, we assume for simplicity that AinΛi=e−(∆i⊗In)⊙Ais a scalar,
i.e.,A=a·Ind. This assumption simplifies Λito
Λi=e−a(∆i⊗In)=
e−a∆1
i·In
...
e−a∆d
i·In
, (31)
where each e−a∆j
i·Initself is a diagonal matrix with n-times e−a∆j
ion its diagonal. In order to
analyze this expression, we simplify the computation of ∆iby fusing the two projection matrices
with out loss of generality, i.e.,
∆i=diag(softplus (W∆(Wuui) +b∆))
=diag(softplus (¯W∆ui)) =
softplus (¯W1,:
∆ui)
...
softplus (¯Wd,:
∆ui)
,
where ¯Wj,:
∆denotes the jthrow of ¯W∆. Above reformulation is valid since the softplus (·)function
is applied elementwise and we note that ∆j
i=softplus (¯Wj,:
∆ui)in(31). Using the definition of the
softplus (·)function, we can show that
e−a∆1
i=e−asoftplus (¯Wj,:
∆ui)= (1 + e¯Wj,:
∆ui)−a=σrev(¯Wj,:
∆ui)a, (32)
where σrev(·)is the reversed sigmoid, i.e., σrev(x) =1
1+ex. Since the reversed sigmoid is again
applied elementwise to a vector or a matrix, we can write the S6 state transition matrix as
Λi=diag(σrev(¯W∆ui)a)⊗In,
where the power ais applied elementwise. The assumption A=a·Indwe made in the beginning, can
be relaxed to any diagonal matrix, however the resulting Λiwill have a more complex representation.
J Experimental Details
The experimental results provided in Section 4 are performed on the multi-query associative re-
call (MQAR) benchmark [ 24], the long range arena (LRA) benchmark [ 3], and the WikiText-103
dataset. To obtain the MQAR and LRA results, we modified the Zoology5and LRA6code bases and
added the normalized attention model and the selective SSM models, respectively. The code for both
benchmarks is provided on GitHub for MQAR7and for LRA8separately.
5https://github.com/HazyResearch/zoology
6https://github.com/google-research/long-range-arena
7https://github.com/IntelligentControlSystems/dsf-mqar
8https://github.com/jsie7/ssm-benchmark
20J.1 MQAR experiments
Training Details We evaluate the following three architecture classes:
1.Attention: softmax attention [ 2], linear attention [ 14], normalized attention (22). For
all three attention functions, we use a standard GPT-2 style multi-headed Transformer
architecture, where we replace the attention block with the respective attention function.
The three attention functions are defined in Section 2.1. For all MQAR runs we use a single
attention head.
2.State space model: S6 [7], SSD [ 8]. For both SSM variants, we use a standard GPT-2
style single-headed Transformer architecture, where we replace the attention block with
the respective SSM variant. This means for S6 and SSD, we do not implement the pre-
convolution on the input or the SiLU activations; but just the S6 and SSD blocks. We do
this to ensure a fair comparison of the backbones (sequence mixers) irrespective of the
higher-level architecture. The S6 and SSD blocks are defined in Section 2.2 and we use the
provided code base9to implement it.
3.RNN: qLSTM [ 9], modified qLSTM. We embed both qLSTM variants in a standard GPT-2
style single-headed Transformer architecture, where we replace the attention block with
the qLSTM. We do this to ensure a fair comparison of the backbones (sequence mixers)
irrespective of the higher-level architecture. The standard qLSTM is defined in Section 2.3
and the modified qLSTM is the same as the standard qLSTM but with modified state
transition (23), i.e., a modified forget gate.
For all MQAR runs, we use the following training protocol:
•Optimizer and schedule: Weight decay of 0.1, linear warmup with duration of
10%, AdamW optimizer [ 59]. For each run, we sweep the learning rates in
np.logspace (−4,−2,4)and train for 64 epochs. This is the same setup as in [24].
•Training duration: We use a global batch size of 512, which we reduce to 256if sequence
length L≥128, to128if sequence length L≥256, and to 64if sequence length L≥512.
We do this to keep the memory consumption approximately constant over different tasks.
•Width and depth: For all runs, we use two layers (each with a sequence model and a MLP,
interleaved with layer normalization). The model dimensions d, state expansion n, sequence
length L, and number of KV pairs are swept according to the experiment (see Section 4).
This is the same setup as in [24].
•Position information: Positional embeddings [ 60] are used for the attention and RNN
architecture classes, but not for the SSM architecture classes. This is the same setup as
in [24].
•Data: Each model is trained on 100,000 datapoints and evaluated on 3,000 datapoints. The
data and its order are constant for all runs. This is the same setup as in [24].
Performed Experiments We run the three attention models and the two state space models on
four different MQAR tasks, i.e., {L= 64 ,KV-pairs = 4},{L= 128 ,KV-pairs = 8},{L=
256,KV-pairs = 16}, and{L= 512 ,KV-pairs = 64}, which progressively increase in complexity.
For each model and task, we sweep both the model size d= [64 ,128,256,512] and the state
expansion n= [32 ,64,128,256],10resulting in a total of 320experiments. We only report the results
of the best performing learning rate; the full results of all experiments are stated in Appendix L.
We run the two qLSTM variants on three different MQAR tasks, i.e., {L= 64,KV-pairs = 4},
{L= 128 ,KV-pairs = 8}, and{L= 256 ,KV-pairs = 16}. For both variants we sweep the model
sized= [64 ,128,256,512], resulting in a total of 24 experiments. We only report the results of the
best performing learning rate; the full results are reported in Figure 3.
9https://github.com/state-spaces/mamba
10We did not increase nfurther, since the selective scan CUDA kernel provided for S6 and SSD is capped at
n= 256 ; for more information see https://github.com/state-spaces/mamba .
21J.2 LRA experiments
Training Details We evaluate the following two architecture classes:
1.Attention: softmax attention [ 2], linear attention [ 14], normalized attention (22). For
all three attention functions, we use a standard GPT-2 style multi-headed Transformer
architecture, where we replace the attention block with the respective attention function. The
three attention functions are defined in Section 2.1. To ensure a fair comparison, we keep all
hyperparameters of the three attention models constant except the attention function.
2.State space model: S6 [7]. We use a standard GPT-2 style single-headed Transformer
architecture, where we replace the attention block with the S6 block. This means, we do not
implement the pre-convolution on the input or the SiLU activations; but just the S6 block.
We do this to ensure a fair comparison of the backbones (sequence mixers) irrespective of
the higher-level architecture. The S6 block is defined in Section 2.2 and we use the provided
code base11to implement it.
For all LRA runs, we use the following training protocol:
•Optimizer and schedule: Linear warmup with duration of 10% and AdamW optimizer [ 59].
•Position information: Positional embeddings [ 60] are used for the attention architecture
classes, but not for the SSM architecture classes.
•Data: Each model is trained on the standard datasets provided with the LRA benchmark.
The exact hyperparameters for each LRA task and each model are reported in the publicly available
code base.12Note that we do not optimize the hyperparameters, i.e., the reported accuracies might be
lower than in the original LRA paper [3].
Performed Experiments We run the three attention models and the S6 models on the LRA tasks
ListOps ,Text ,Retrieval ,Image , and Pathfinder-32 , which are summarized below; for the
full details we refer to [3].
1.List Operations ( ListOps ): This task evaluates a model’s ability to capture hierarchical
dependencies over long contexts. The goal is to predict the result of a mathematical
operation consisting of nested mean ,median ,max, and minoperations,13The task is a
ten-way classification task with maximal input lengths of 2k.
2.Text Classification ( Text ): This task evaluates a model’s ability to capture the tone of long
tokenized texts. The dataset consists of IMDb movie reviews, which need to be classified
as negative or positive in tone. The task is a binary classification task with maximal input
lengths of 4k.
3.Document Retrieval ( Retrieval ): This task evaluates a model’s ability to compress long
sequences into representations that are suitable for similarity matching. The dataset consists
of tokenized papers published by the American Academy of Neurology (AAN), which need
to be classified in having a citation link or not. The task is a binary classification task with
maximal input lengths of 8k.
4.Image Classification ( Image ): This task evaluates a model’s ability to learn 2D spatial
relations from a 1D vector. The dataset consists of vectorized images, which depict one
of ten possible classes, e.g. a horse or a car. The task is a ten-way classification task with
maximal input lengths of 1k.
5.Long-Range Spacial Dependency ( Pathfinder-32 ): This task evaluates a model’s ability
to learn spacial dependencies in a vectorized image. The dataset consists of images, which
depict two circles and multiple dashed paths. The goal is to evaluate whether the two circles
are connected by any of the present paths or not. The task is therefore a binary classification
task with maximal input lengths of 2k.
11https://github.com/state-spaces/mamba
12https://github.com/jsie7/ssm-benchmark
13For instance, input: max(4, min(5,6, mean(9, 4, 5))) ,output: 5 .
22J.3 WikiText-103 experiments
Training Details We use the 70M parameter Pythia architecture (Pythia70M) [ 61].14For softmax
attention we use the standard Pythia attention block, while for linear attention [ 14] and normalized
attention (22) we replace the attention block in the Pythia architecture with the respective attention
functions defined in Sections 2.1 and 4.2.
For all training runs on WikiText-103, we use the following protocol:
•Optimizer and schedule: Weight decay of 0.1, linear warmup with duration of 10%,
AdamW optimizer [ 59] with β= (0.9,0.95), and gradient clipping = 1. For each run, we
sweep the learning rates in [0.0003,0.001,0.003,0.01]and train for 50 epochs.
•Training duration: We use a batch size of 128and train for 50 epochs.
•Width and depth: We use a context length of 1024 and the standard Pythia70M configura-
tion, i.e., model size of 512,8heads, and 6layers.
•Position information: Positional embeddings [60] are used as in standard Pythia.
Performed Experiments We train the three attention functions on WikiText-103 and sweep the
learning rates [0.0003,0.001,0.003,0.01]. For all three attention functions learning rate 0.003
performed best and the corresponding results are reported in Table 1.
K Computational Resources
All experiments (MQAR, LRA, and WikiText-103) were run on a cluster with 11nodes with the
following GPU and CPU specifications:
GPU Model Nr. of nodes memory/GPU GPUs/node CPUs/node
NVIDIA GTX 1080 Ti 1 11 GB 8 20
NVIDIA GTX 2080 Ti 2 11 GB 8 64
NVIDIA GTX 3090 1 24 GB 8 128
NVIDIA GTX 4090 1 24 GB 8 128
NVIDIA TITAN RTX 1 24 GB 8 128
NVIDIA Quadro RTX 6000 1 24 GB 8 128
NVIDIA V100 2 32 GB 8 44
NVIDIA A100 1 40 GB 8 48
NVIDIA A100 1 80 GB 10 48
The MQAR and LRA training and test runs were parallelized and assigned to the best available GPU
node, while the parallelized training on WikiText-103 was exclusively run on the NVIDIA A100
(80GB) node.
For each learning rate sweep of the MQAR runs described in Appendix J we estimate the average
runtime to be 1h,15leading to a total unparallelized runtime of 54 days for all MQAR tasks. There
where approximately 20runs for debugging and training purposes, which were terminated after a few
minutes, thus we did not include them in the time estimate.
For each task of the LRA benchmark, we estimate the average runtime to be 2h for Image , 5h for Text ,
6h for ListOps , 30h for Retrieval , and 45h for Pathfinder , leading to a total unparallelized
runtime of 31 days for all LRA tasks. Note that to obtain better hyperparameters, they would
need to be tuned for each task, which would significantly increase the total runtime. There where
approximately 30runs for debugging and training purposes, which were terminated after a few
minutes, thus we did not include them in the time estimate.
14https://github.com/EleutherAI/pythia
15Obviously, larger model sizes and MQAR tasks with larger sequence length took longer than smaller models
and tasks with shorter sequence length. However, a more accurate time estimate is hard to obtain due to the
cluster setup with multiple different GPU models and the fact that we terminated tasks early if the 99% accuracy
threshold was achieved.
23For the training on WikiText-103, each learning rate sweep took approximately 14h, leading to a total
parallelized runtime of 42h for all three attention models. We estimate a total of 1h of runtime for
tuning runs, bringing the total runtime to 43h.
L Extended Results on MQAR
Figures 5 and 6 show the MQAR results of Figures 2 and 3 but for multiple runs over 10 different
seeds.
64 128 256 512
Model size/u1D4510.00.20.40.60.81.0Accuracy
Softmax att. (2)
Linear att. (16)
Normalized att. (21)SSD [Dao and Gu, 2024]
S6 [Gu and Dao, 2023]
Figure 5: Model accuracy with increasing model size dfor different models: softmax, linear, and
normalized attention, S6, and SSD. The MQAR task is (L= 512 ,KV-pairs = 64) , we fix n= 128 ,
and report the best performance of a learning rate sweep in np.logspace (−4,−2,4). Solid lines
are the average accuracy over 10 different seeds, while the shaded area show the standard deviation.
64 128 256 512
Model size/u1D4510.800.850.900.951.00Accuracy
L: 64, KV-pairs: 4
qLSTM
qLSTM w/ (22)
64 128 256 512
Model size/u1D4510.60.81.0Accuracy
L: 128, KV-pairs: 8
64 128 256 512
Model size/u1D4510.000.250.500.751.00Accuracy
L: 256, KV-pairs: 16
Figure 6: Comparison of qLSTM (8)and a qLSTM variant where the original state transition Λiis
replace by (23). Solid lines are the average accuracy over 10 different seeds, while the shaded area
show the standard deviation.
In Figure 7 we report the complete results of all MQAR experiments detailed in Appendix J. A
selected subset of these are already presented in Figure 1 and Figure 2 in the main text.
The effect of state expansion can not only be observed for linear attention (Figure 1) but also for
normalized attention (22), S6, and SSD for the task {L= 512 ,KV-pairs = 64}. Contrary to this,
for small model sizes dand larger tasks (e.g. normalized attention, task {L= 512 ,KV-pairs = 64},
d= 64 ) the performance decreases with increased state expansion nor shows erratic behaviour.
Since this behavior only occurs for small d, we hypothesis that this effect is due to the model being to
small to accurately learn the task.
2464 128 256 512d>99>99>99>99
>99>99>99>99
>99>99>99>99
>99>99>99>99L: 64, KV-pairs: 4Softmax att.>99>99>99>99
>99>99>99>99
>99>99>99>99
>99>99>99>99L: 128, KV-pairs: 8
>99>99>99>99
>99>99>99>99
>99>99>99>99
>99>99>99>99L: 256, KV-pairs: 16
1.0>99>99>99
>99>99>99>99
>99>99>99>99
>99>99>99>99L: 512, KV-pairs: 6464 128 256 512d96.5>99>99>99
>99>99>99>99
>99>99>99>99
>99>99>99>99Linear att.96.8 98.9>99>99
77.6>99>99>99
15.6>99>99>99
>99>99>99>9992.7 95.4 86.0 80.5
>9995.9>99>99
28.0>99>99>99
45.7 88.9>99>999.7 0.9 21.8 0.9
1.6 1.6 1.6 1.6
4.2 2.1 3.7 96.2
2.1 89.4 96.6 96.464 128 256 512d>99>99>99>99
>99>99>99>99
>99>99>99>99
>99>99>99>99Normalized att.>99>99>99>99
>99>99>99>99
>99>99>99>99
>99>99>99>99>9993.2 97.1 91.4
>99>99>99>99
>99>99>99>99
>99>99>99>9937.4 17.5 19.1 1.6
55.9 85.9 90.6>99
>99>99>99>99
>99>99>99>9964 128 256 512d>99>99>99>99
>99>99>99>99
>99>99>99>99
>99>99>99>99SSD>99>99>99>99
>99>99>99>99
>99>99>99>99
>99>99>99>9995.1 92.6 92.2>99
88.7>99>99>99
>99>99>99>99
>99>99>99>990.5 0.0 0.0 39.9
98.3 97.3 98.9>99
>99>99>99>99
>99>99>99>99
32 64 128 256
n64 128 256 512d>99>99>99>99
>99>99>99>99
>99>99>99>99
>99>99>99>99S6
32 64 128 256
n94.0>99>99>99
>99>99>99>99
>99>99>99>99
>99>99>99>99
32 64 128 256
n90.7 97.2 88.8>99
>99>99>99>99
>99>99>99>99
>99>99>99>99
32 64 128 256
n0.7 0.4 0.8 2.2
68.7 81.5 87.8>99
62.3 81.0>99>99
95.1>99>99>990.000.250.500.751.00
0.000.250.500.751.00
0.000.250.500.751.00
0.000.250.500.751.00
0.000.250.500.751.00
Figure 7: Results for softmax attention [ 2], linear attention [ 14], normalized attention (22), S6 [ 7],
and SSD [ 8] on four different, progressively harder MQAR tasks {L= 64,KV-pairs = 4},{L=
128,KV-pairs = 8},{L= 256 ,KV-pairs = 16}, and{L= 512 ,KV-pairs = 64}. We sweep the
model size d= [64 ,128,256,512] and the state expansion n= [32 ,64,128,256] for each model and
task. We only report the best performance from a learning rate sweep in np.logspace (−4,−2,4)
measured as accuracy on the MQAR task. The accuracy is denoted in % in the grid in the figure.
While Figure 2 shows that normalized attention outperforms standard linear attention [14], Figure 7
shows an even more significant performance gap. Additionally, we note that SSD outperforms S6,
which was already hinted at in [ 8], and that normalized attention performs on par with S6. Together
these results hint at the importance of normalization both for attention and SSMs. The comparison of
S6 and SSD shows that reducing the number of parameters in the state transition Λifromdto a scalar
does not hurt performance, which is further supported by the findings in [ 8]. These experiments
25also suggest that the recursive structure in ΛiandBi(present in S6 and SSD but not in normalized
attention or linear attention) is less important than proper normalization of the attention scores.
Additionally, the results on WikiText-103 (Table 1) show that better normalization can close 25% of
the perplexity gap between linear attention and softmax attention. Together, these results warrant
more investigations into new and better normalization techniques for attention-based models.
Finally, we remark that softmax attention performs perfectly accross all sweeps except {L=
512,KV-pairs = 64},d= 64 , and n= 32 , which is most likely due to a too small model or too
small learning rate.
M Extended Results on LRA
In Table 3 we report the complete results of all LRA experiments detailed in Appendix J. The average
performance over all tasks is reported in Table 1 in the main text.
While normalized attention (22) performs slightly better on LRA than the other two attention-based
models, there is a significant performance gap to the S6 model (a selective SSM variant). The
reason for this gap potentially lies in the recurrent normalization employed in selective SSMs (see
Section 4.2). Interestingly, S6 only achieves significantly higher accuracy on the tasks Text and
Image , showing that selective SSM models are particularly suited for long-range classification of
text and image modalities.
Table 3: Model performance in terms of test accuracy on the LRA benchmark. The first entry
(Random) represents the performance of random guessing on the task, i.e., indicating the baseline
above which a model is considered to have learned a meaningful representation.
ModelLRA Task [%]
ListOps Text Retrieval Image Pathfinder avg
Random 10.00 50.00 50.00 10.00 50.00 34.00
Softmax Attention [3] 35.72 63.10 77.46 34.22 69.32 55.96
Linear Attention [14] 16.12 64.41 76.44 37.97 72.64 53.52
Normalized Attention (22) 38.24 64.96 79.68 35.96 71.56 58.08
S6 [7] 38.02 81.34 80.50 65.08 69.26 66.84
26NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our theoretical analysis and experimental results substantiate the claims made
in the abstract and introduction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Limitations of the proposed framework as well as the computational experi-
ments are discussed in Section 6.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
27Answer: [Yes]
Justification: Lemmas 1 and 2 are properly stated, and their respective proofs are provided
in Appendix C and Appendix D.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We state the complete experimental details in Appendix J and provide a link to
the full code base.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
28Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide a link to the full code base on GitHub.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All training details are provided in Appendix J.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Some of the experimental results presented in this paper are accompanied
by error bars. Most experiments were performed for fixed seeds and only the best results
of ablated hyper-parameters are reported. For Figures 2 and 3 we provide error bars over
10 different seeds in Appendix L. However, obtaining statistical significance data for all
performed experiments would have drastically increased the amount of compute and runtime
needed, which were both not feasible for us.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
29•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The used computational resources are stated in Appendix K.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes] .
Justification: Ethical considerations in accordance with NeurIPS Code of Ethics have been
respected throughout the research process. Potential limitations are discussed in Section 6.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The paper has not direct societal impacts. If the presented theoretical frame-
work is used to design a new foundation model, there might be societal impacts of the model
derived from this work. However, at this stage the societal impact is not assessable.
Guidelines:
30• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper does not release a new dataset or high risk model.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The used code is properly cited in Appendix J and the licences are appropriately
given in the code base accompanying this paper.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
31•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not introduce any new assets. The paper analyzes exist-
ing models and improvements to those, which are guided by the introduced theoretical
framework.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA] .
Justification: No human subjects or crowdsourcing were used to conduct this research.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: Answer: [NA] .
Justification: No human subjects or crowdsourcing were used to conduct this research.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
32•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
33