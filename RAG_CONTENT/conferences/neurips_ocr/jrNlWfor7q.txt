Kronecker-Factored Approximate Curvature for
Physics-Informed Neural Networks
Felix Dangel∗
Vector Institute
Toronto
Canada
fdangel@vectorinstitute.aiJohannes Müller∗
Chair of Mathematics of Information Processing
RWTH Aachen University
Aachen, Germany
mueller@mathc.rwth-aachen.de
Marius Zeinhofer∗
Seminar for Applied Mathematics, ETH Zürich,
Department of Nuclear Medicine, University Hospital Freiburg
marius.zeinhofer@uniklinik-freiburg.de
Abstract
Physics-informed neural networks (PINNs) are infamous for being hard to train.
Recently, second-order methods based on natural gradient and Gauss-Newton
methods have shown promising performance, improving the accuracy achieved by
first-order methods by several orders of magnitude. While promising, the proposed
methods only scale to networks with a few thousand parameters due to the high
computational cost to evaluate, store, and invert the curvature matrix. We propose
Kronecker-factored approximate curvature (KFAC) for PINN losses that greatly
reduces the computational cost and allows scaling to much larger networks. Our
approach goes beyond the established KFAC for traditional deep learning problems
as it captures contributions from a PDE’s differential operator that are crucial for
optimization. To establish KFAC for such losses, we use Taylor-mode automatic
differentiation to describe the differential operator’s computation graph as a forward
network with shared weights. This allows us to apply KFAC thanks to a recently
developed general formulation for networks with weight sharing. Empirically, we
find that our KFAC-based optimizers are competitive with expensive second-order
methods on small problems, scale more favorably to higher-dimensional neural
networks and PDEs, and consistently outperform first-order methods and LBFGS.
1 Introduction
Neural network-based approaches to numerically solve partial differential equations (PDEs) are
growing at an unprecedented speed. The idea to train network parameters to minimize the residual of
a PDE traces back to at least Dissanayake & Phan-Thien [15], Lagaris et al. [28], but was only recently
popularized under the name deep Galerkin method (DGM) and Physics-informed neural networks
(PINNs) through the works of Sirignano & Spiliopoulos [52], Raissi et al. [50]. PINNs are arguably
one of the most popular network-based approaches to the numerical solution of PDEs as they are easy
to implement, seamlessly incorporate measurement data, and promise to work well in high dimensions.
Despite their immense popularity, PINNs are notoriously difficult to optimize [ 57] and fail to provide
satisfactory accuracy when trained with first-order methods, even for simple problems [ 64,41].
Recently, second-order methods that use the function space geometry to design preconditioners
∗Equal contribution
38th Conference on Neural Information Processing Systems (NeurIPS 2024).have shown remarkable promise in addressing the training difficulties of PINNs [ 64,41,14,24,42].
However, these methods require solving a linear system in the network’s high-dimensional parameter
space at cubic computational iteration cost, which prohibits scaling such approaches. To address
this, we build on the idea of Kronecker-factored approximate curvature (KFAC) and apply it to
Gauss-Newton matrices of PINN losses which greatly reduces the computational cost:
•We use higher-order forward (Taylor) mode automatic differentiation to interpret the compu-
tation graph of a network’s input derivatives as a larger net with weight sharing (§3.1).
•We use this weight sharing view to propose KFAC for Gauss-Newton matrices of objectives
with differential operators, like PINN losses (§3.3 and eq. (14)). Thanks to the generality of
Taylor-mode and KFAC for weight sharing layers [17], our approach is widely applicable.
•We show that, for specific differential operators, the weight sharing in Taylor-mode can be
further reduced by absorbing the reduction of partial derivatives into the forward propagation,
producing a more efficient scheme. For the prominent example of the Laplace operator, this
recovers and generalizes the forward Laplacian framework [29] (§3.2 and eq. (9)).
•Empirically, we find that our KFAC-based optimizers are competitive with expensive second-
order methods on small problems, scale more favorably to higher-dimensional neural
networks and PDEs, and consistently outperform first-order methods and LBFGS (§4).
Related work Various approaches were developed to improve the optimization of PINNs such
as adaptive re-weighting of loss terms [ 57,56,59], different sampling strategies for discretizing
the loss [ 34,43,13,63,58,61], and curriculum learning [ 26,58]. While LBFGS is known to
improve upon first-order optimizers [ 35], recently, other second-order methods that design meaningful
preconditioners that respect the problem’s geometry have significantly outperformed it [ 64,41,14,
33,24,7,62]. Müller & Zeinhofer [42] provide a unified view on these approaches which greatly
improve the accuracy of PINNs, but come with a significant per-iteration cost as one needs to solve a
linear system in the network’s high-dimensional parameter space, which is only feasible for small
networks when done naively. One approach is to use matrix-free methods to approximately compute
Gauss-Newton directions by introducing an inner optimization loop, see [ 51,36] for supervised
learning problems and [ 64,4,24,62] for PINNs. Instead, our KFAC-based approach uses an explicit
structured curvature representation which can be updated over iterations and inverted more cheaply.
We build on the literature on Kronecker-factored approximate curvature (KFAC), which was initially
introduced in Heskes [22], Martens [36] as an approximation of the per-layer Fisher matrix to perform
approximate natural gradient descent. Later, KFAC was extended to convolutional [ 20], recurrent [ 39],
attention [ 47,44,21], and recently to general linear layers with weight sharing [ 17]. These works do
not address preconditioners for losses with contributions from differential operators, as is the case for
PINN losses. Our interpretation via Taylor-mode makes the computation graph of such losses explicit,
and allows us to establish KFAC based on its generalization to linear weight sharing layers [17].
2 Background
For simplicity, we present our approach for multi-layer perceptrons (MLPs) consisting of fully-
connected and element-wise activation layers. However, the generality of Taylor-mode automatic
differentiation and KFAC for linear layers with weight sharing allows our KFAC to be applied to such
layers (e.g. fully-connected, convolution, attention) in arbitrary neural network architectures.
Flattening & Derivatives We vectorize matrices using the first-index-varies-fastest convention,
i.e. column-stacking (row index varies first, column index varies second) and denote the corresponding
flattening operation by vec. This allows to reduce derivatives of matrix- or tensor-valued objects
back to the vector case by flattening a function’s input and output before differentiation. The
Jacobian of a vector-to-vector function a7→b(a)has entries [Jab]i,j=∂bi/∂aj. For a matrix-to-
matrix function A7→B(A), the Jacobian is JAB= JvecAvecB. A useful property of vecis
vec(AXB ) = (B⊤⊗A) vecXfor matrices A,X,Bwhich implies JX(AXB ) =B⊤⊗A.
Sequential neural nets Consider a sequential neural network uθ=fθ(L)◦fθ(L−1)◦. . .◦fθ(1)
of depth L∈N. It consists of layers fθ(l):Rh(l−1)→Rh(l),z(l−1)7→z(l)=fθ(l)(z(l−1))with
2trainable parameters θ(l)∈Rp(l)that transform an input z(0):=x∈Rd:=h(0)into a prediction
uθ(x) =z(L)∈Rh(L)via intermediate representations z(l)∈Rh(l). In the context of PINNs,
we use networks with scalar outputs ( h(L)= 1) and denote the concatenation of all parameters by
θ= (θ(1)⊤, . . . ,θ(L)⊤)⊤∈RD. A common choice is to alternate fully-connected and activation
layers. Linear layers map z(l−1)7→z(l)=W(l)z(l−1)using a weight matrix W(l)= vec−1θ(l)∈
Rh(l)×h(l−1)(bias terms can be added as an additional column and by appending a 1to the input).
Activation layers map z(l−1)7→z(l)=σ(z(l−1))element-wise for a (typically smooth) σ:R→R.
2.1 Energy Natural Gradients for Physics-Informed Neural Networks
Let us consider a domain Ω⊆Rdand the partial differential equation
Lu=finΩ, u =gon∂Ω,
with right-hand side f, boundary data gand a differential operator L, e.g. the negative Laplacian
−Lu= ∆ xu=Pd
i=1∂2
xiu. We parametrize uwith a neural net and train its parameters θto
minimize the loss
L(θ) =1
2NΩNΩX
n=1(Luθ(xn)−f(xn))2+1
2N∂ΩN∂ΩX
n=1(uθ(xb
n)−g(xb
n))2
=:LΩ(θ) +L∂Ω(θ)(1)
with points {xn∈Ω}NΩ
n=1from the domain’s interior, and points {xb
n∈∂Ω}N∂Ω
n=1on its boundary.2
First-order optimizers like gradient descent and Adam struggle at producing satisfactory solutions
when used to train PINNs [ 9]. Instead, function space-inspired second-order methods have lately
shown promising results [ 42]. We focus on energy natural gradient descent (ENGD [ 41])which—
applied to PINN objectives like (1)—corresponds to the Gauss-Newton method [ 6, Chapter 6.3].
ENGD mimics Newton’s method in function space up to a projection onto the model’s tangent space
and a discretization error that vanishes quadratically in the step size, thus providing locally optimal
residual updates. Alternatively, the Gauss-Newton method can be motivated from the standpoint
of operator preconditioning, where the Gauss-Newton matrix leads to optimal conditioning of the
problem [14].
Natural gradient methods perform parameter updates via a preconditioned gradient descent scheme
θ←θ−αG(θ)+∇L(θ), where G(θ)+denotes the pseudo-inverse of a suitable Gramian matrix
G(θ)∈RD×Dandαis a step size. ENGD for the PINN loss (1) uses the Gramian
G(θ) =1
NΩNΩX
n=1(JθLuθ(xn))⊤JθLuθ(xn) +1
N∂ΩN∂ΩX
n=1 
Jθuθ(xb
n)⊤Jθuθ(xb
n)
=:GΩ(θ) +G∂Ω(θ).(2)
(2)is the Gauss-Newton matrix of the residual r(θ) = ( rΩ(θ)⊤/√NΩ,r∂Ω(θ)⊤/√N∂Ω)⊤∈RNΩ+N∂Ω
with interior and boundary residuals rΩ,n(θ) =Luθ(xn)−f(xn)andr∂Ω,n(θ) =uθ(xb
n)−g(xb
n).
2.2 Kronecker-factored Approximate Curvature
We review Kronecker-factored approximate curvature (KFAC) which was introduced by Heskes
[22], Martens & Grosse [38] in the context of maximum likelihood estimation to approximate the
per-layer Fisher information matrix by a Kronecker product to speed up approximate natural gradient
descent [ 1]. The Fisher associated with the loss 1/2NPN
n=1∥uθ(xn)−yn∥2
2with targets yn∈Ris
F(θ) =1
NNX
n=1(Jθuθ(xn))⊤Jθuθ(xn) =1
NNX
n=1(Jθun)⊤Jθun∈RD×D, (3)
where un=uθ(xn), and it coincides with the classical Gauss-Newton matrix [ 37]. The established
KFAC approximates (3). While the boundary Gramian G∂Ω(θ)has the same structure as F(θ), the
interior Gramian GΩ(θ)does not as it involves derivative rather than function evaluations of the net.
2The second regression loss can also include other constraints like measurement data.
3KFAC tackles the Fisher’s per-layer block diagonal, F(θ)≈diag(F(1)(θ), . . . ,F(L)(θ))with
F(l)(θ) = 1/NPN
n=1(Jθ(l)un)⊤Jθ(l)un∈Rp(l)×p(l). For a fully-connected layer’s block, let’s
examine the term Jθ(l)uθ(x)from Equation (3) for a fixed data point. The layer parameters θ(l)=
vecW(l)enter the computation via z(l)=W(l)z(l−1)and we have JW(l)z(l)=z(l−1)⊤⊗I[e.g.
10]. Further, the chain rule gives the decomposition JW(l)u= (Jz(l)u)JW(l)z(l)=z(l−1)⊤⊗Jz(l)u.
Inserting into F(l)(θ), summing over data points, and using the expectation approximationP
nAn⊗
Bn≈N−1(P
nAn)⊗(P
nBn)from Martens & Grosse [38], we obtain the KFAC approximation
for linear layers in supervised square loss regression with a network’s output,
F(l)(θ)≈ 
1
NNX
n=1z(l−1)
nz(l−1)
n⊤!
| {z }
=:A(l)∈Rh(l−1)×h(l−1)⊗ 
1
NNX
n=1(Jz(l)un)⊤Jz(l)un!
| {z }
=:B(l)∈Rh(l)×h(l). (4)
It is cheap to store and invert by inverting the two Kronecker factors.
3 Kronecker-Factored Approximate Curvature for PINNs
ENGD’s Gramian is a sum of PDE and boundary Gramians, G(θ) =GΩ(θ) +G∂Ω(θ). We
will approximate each Gramian separately with a block diagonal matrix with Kronecker-factored
blocks, G•(θ)≈diag(G(1)
•(θ), . . . ,G(L)
•(θ))for• ∈ { Ω, ∂Ω}withG(l)
•(θ)≈A(l)
•⊗B(l)
•.
For the boundary Gramian G∂Ω(θ), we can re-use the established KFAC from Equation (4) as its
loss corresponds to regression over the network’s output. The interior Gramian GΩ(θ), however,
involves PDE terms in the form of network derivatives and therefore cannot be approximated with
the existing KFAC. It requires a new approximation that we develop here for the running example of
the Poisson equation and more general PDEs (Equations (9) and (14)). To do so, we need to make
the dependency between the weights and the differential operator Luexplicit. We use Taylor-mode
automatic differentiation to express this computation of higher-order derivatives as forward passes of
a larger net with shared weights, for which we then propose a Kronecker-factored approximation,
building on KFAC’s recently-proposed generalization to linear layers with weight sharing [17].
3.1 Higher-order Forward Mode Automatic Differentiation as Weight Sharing
Here, we review higher-order forward mode, also known as Taylor-mode , automatic differentiation [ 19,
18,3, tutorial in §C]. Many PDEs only incorporate first- and second-order partial derivatives and we
focus our discussion on second-order Taylor-mode for MLPs to keep the presentation light. However,
one can treat higher-order PDEs and arbitrary network architectures completely analogously.
Taylor-mode propagates directional (higher-order) derivatives. We now recap the forward propagation
rules for MLPs consisting of fully-connected and element-wise activation layers. Our goal is to
evaluate first-and second-order partial derivatives of the form ∂xiu, ∂2
xi,xjufori, j= 1, . . . , d . At the
first layer, set z(0)=x∈Rd, ∂xiz(0)=ei∈Rd, i.e., the i-th basis vector and ∂2
xi,xjz(0)=0∈Rd.
For a linear layer fθ(l)(z(l−1)) =W(l)z(l−1), applying the chain rule yields the propagation rule
z(l)=W(l)z(l−1)∈Rh(l), (5a)
∂xiz(l)=W(l)∂xiz(l−1)∈Rh(l), (5b)
∂2
xi,xjz(l)=W(l)∂2
xi,xjz(l−1)∈Rh(l). (5c)
The propagation rule through a nonlinear element-wise activation layer z(l−1)7→σ(z(l−1))is
z(l)=σ(z(l−1))∈Rh(l), (6a)
∂xiz(l)=σ′(z(l−1))⊙∂xiz(l−1)∈Rh(l), (6b)
∂2
xi,xjz(l)=∂xiz(l−1)⊙σ′′(z(l−1))⊙∂xjz(l−1)+σ′(z(l−1))⊙∂2
xi,xjz(l−1)∈Rh(l).(6c)
4Forward Laplacian For differential operators of special structure, we can fuse the Taylor-mode
forward propagation of individual directional derivatives in Equations (5) and (6) and obtain a more
efficient computation. E.g., to compute not the full Hessian but only the Laplacian, we can simplify
the forward pass, which yields the forward Laplacian framework of Li et al. [29]. To the best of our
knowledge, this connection has not been pointed out in the literature. Concretely, by summing (5c)
and (6c) over i=j, we obtain the Laplacian forward pass for linear and activation layers
∆xz(l)=W(l)∆xz(l−1)∈Rh(l), (7a)
∆xz(l)=σ′(z(l−1))⊙∆xz(l−1)+dX
i=1σ′′(z(l−1))⊙(∂xiz(l−1))⊙2∈Rh(l). (7b)
This reduces computational cost, but is restricted to PDEs that involve second-order derivatives only
via the Laplacian, or a partial Laplacian over a sub-set of input coordinates (e.g. heat equation, §4).
For a more general second-order linear PDE operator L=Pd
i,j=1ci,j∂2
xi,xj, the forward pass for a
linear layer is Lz(l)=W(l)Lz(l−1)∈Rh(l), generalizing (7a), and similarly for Equation (7b)
Lz(l)=σ′(z(l−1))⊙ Lz(l−1)+dX
i,j=1ci,jσ′′(z(l−1))⊙∂xiz(l−1)⊙∂xjz(l−1)∈Rh(l),
see §C.3 for details. This is different from [ 30], which transforms the input space such that the
coefficients are diagonal with entries {0,±1}, reducing the computation to two forward Laplacians.
Importantly, the computation of higher-order derivatives for linear layers boils down to a forward pass
through the layer with weight sharing over the different partial derivatives (Equation (5)), and weight
sharing can potentially be reduced depending on the differential operator’s structure (Equation (7a)).
Therefore, we can use the concept of KFAC in the presence of weight sharing to derive a principled
Kronecker approximation for Gramians containing differential operator terms.
3.2 KFAC for Gauss-Newton Matrices with the Laplace Operator
Let’s consider the Poisson equation’s interior Gramian block for a linear layer (suppressing ΩinNΩ)
G(l)
Ω(θ) =1
NNX
n=1(JW(l)∆xun)⊤JW(l)∆xun.
Because we made the Laplacian computation explicit through Taylor-mode autodiff (§3.1, specifically
Equation (7a)), we can stack all output vectors that share the layer’s weight into a matrix Z(l)
n∈
Rh(l)×SwithS=d+ 2and columns Z(l)
n,1=z(l)
n,Z(l)
n,2=∂x1z(l)
n, . . . ,Z(l)
n,1+d=∂xdz(l)
n, and
Z(l)
n,2+d= ∆xz(l)
n(likewise Z(l−1)
n∈Rh(l−1)×Sfor the layer inputs), then apply the chain rule
JW(l)∆xun= (JZ(l)
n∆xun)JW(l)Z(l)
n=SX
s=1Z(l−1)
n,s|{z}
∈Rh(l−1)⊤⊗JZ(l)
n,s∆xun
|{z}
=:g(l)
n,s∈Rh(l),
which has a structure similar to the Jacobian in §2.2, but with an additional sum over the Sshared
vectors. With that, we can now express the exact interior Gramian for a layer as
G(l)
Ω(θ) =1
NNX
n=1SX
s=1SX
s′=1Z(l−1)
n,sZ(l−1)⊤
n,s′⊗g(l)
n,sg(l)⊤
n,s′. (8)
Next, we want to approximate Equation (8) with a Kronecker product. To avoid introducing a new
convention, we rely on the KFAC approximation for linear layers with weight sharing developed by
Eschenhagen et al. [17]—specifically, the approximation called KFAC-expand . This drops all terms
withs̸=s′, then applies the expectation approximation from §2.2 over the batch and shared axes:
KFAC for the Gauss-Newton matrix of a Laplace operator
G(l)
Ω(θ)≈ 
1
NSN,SX
n,s=1Z(l−1)
n,sZ(l−1)
n,s⊤!
⊗ 
1
NN,SX
n,s=1g(l)
n,sg(l)
n,s⊤!
=:A(l)
Ω⊗B(l)
Ω (9)
53.3 KFAC for Generalized Gauss-Newton Matrices Involving General PDE Terms
To generalize the previous section, let’s consider the general M-dimensional PDE system of order k,
Ψ(u, Dxu, . . . , Dk
xu) =0∈RM, (10)
where Dm
xucollects all partial derivatives of order m. For m∈ {0, . . . , k }there are Sm= d+m−1
d−1
independent partial derivatives and the total number of independent derivatives is
S:=Pk
m=0Sm= d+k
k
.Ψis a smooth mapping from all partial derivatives to RM,
Ψ:RS→RM. To construct a PINN loss for Equation (10), we feed the residual rΩ,n(θ):=
Ψ(uθ(xn), Dxuθ(xn), . . . , Dk
xuθ(xn))∈RMwhere Dm
xuθ(xn)∈Rd×Sminto a smooth convex
criterion function ℓ:RM→R,
LΩ(θ):=1
NNX
n=1ℓ(rΩ,n(θ)). (11)
The generalized Gauss-Newton (GGN) matrix [ 51] is the Hessian of LΩ(θ)when the residual is
linearized w.r.t. θbefore differentiation. It is positive semi-definite and has the form
GΩ(θ):=1
NNX
n=1(JθrΩ,n(θ))⊤Λ(rΩ,n) (JθrΩ,n(θ)), (12)
withΛ(r):=∇2
rℓ(r)∈RM×M≻0the criterion’s Hessian, e.g. ℓ(r) = 1/2∥r∥2
2andΛ(r) =IM.
Generalizing the second-order Taylor-mode from §3.1 to higher orders for the linear layer, we find
Dm
xz(l)=W(l)Dm
xz(l−1)∈Rh(l)×Sm(13)
for any m. Hence, we can derive a forward propagation for the required derivatives where a
linear layer processes at most Svectors3, i.e. the linear layer’s weight is shared over the ma-
trices D0
xz(l−1):=z(l−1), D1
xz(l−1), . . . , Dk
xz(l−1). Stacking them into a matrix Z(l−1)
n =
(z(l−1), D1
xz(l−1), . . . , Dk
xz(l−1))∈Rh(l−1)×S(andZ(l)
nfor the outputs), the chain rule yields
G(l)
Ω(θ) =1
NNX
n=1
JW(l)Z(l)
n⊤
JZ(l)
nrΩ,n⊤
Λ(rΩ,n)
JZ(l)
nrΩ,n
JW(l)Z(l)
n
=1
NN,S,SX
n,s,s′=1
JW(l)Z(l)
n,s⊤
JZ(l)
n,srΩ,n⊤
Λ(rΩ,n)
JZ(l)
n,s′rΩ,n
JW(l)Z(l)
n,s′
=1
NN,S,SX
n,s,s′=1Z(l−1)
n,sZ(l−1)⊤
n,s′⊗
JZ(l)
n,srΩ,n⊤
Λ(rΩ,n)
JZ(l)
n,s′rΩ,n
where Z(l−1)
n,s∈Rh(l−1)denotes the s-th column of Z(l−1)
n . Following the same steps as in §3.2, we
apply the KFAC-expand approximation from [17] to obtain the generalization of Equation (9):
KFAC for the GGN matrix of a general PDE operator
G(l)
Ω(θ)≈ 
1
NSN,SX
n,s=1Z(l−1)
n,sZ(l−1)⊤
n,s′!
⊗ 
1
NN,SX
n,s=1
JZ(l)
n,srΩ,n⊤
Λ(rΩ,n)
JZ(l)
n,srΩ,n!
=:A(l)
Ω⊗B(l)
Ω(14)
To bring this expression even closer to Equation (9), we can re-write the second Kronecker factor
using an outer product decomposition Λ(rΩ,n) =PM
m=1ln,mln,mwithln,m∈RM, then introduce
g(l)
n,s,m := (JZ(l)
n,srΩ,n)⊤ln,m∈Rh(l)and write the second term as 1/NPN,S,M
n,s,m =1g(l)
n,s,mg(l)⊤
n,s,m ,
similar to the Kronecker-factored low-rank (KFLR) approach of Botev et al. [5].
3Depending on the linear operator, one may reduce weight sharing, as demonstrated for the Laplacian in §3.1.
6KFAC for variational problems Our proposed KFAC approximation is not limited to PINNs and
can be used for variational problems of the form
min
uZ
Ωℓ(u, ∂xu, . . . , ∂k
xu)dx, (15)
where ℓ:RK→Ris a convex function. We can perceive this as a special case of the setting above
withΨ = id and hence the KFAC approximation (14) remains meaningful. In particular, it can be
used for the deep Ritz method and other variational approaches to solve PDEs [16].
3.4 Algorithmic Details
To design an optimizer based on our KFAC approximation, we re-use techniques from the original
KFAC [ 38] & ENGD [ 41] algorithms. §B shows pseudo-code for our method on the Poisson equation.
At iteration t, we approximate the per-layer interior and boundary Gramians using our derived
Kronecker approximation (Equations (9) and (14)), G(l)
Ω,t≈A(l)
Ω,t⊗B(l)
Ω,tandG(l)
∂Ω,t≈A(l)
Ω,t⊗B(l)
Ω,t.
Exponential moving average and damping For preconditioning, we accumulate the Kronecker
factors A(l)
•,t,B(l)
•,tover time using an exponential moving average ˆA(l)
•,t=βˆA(l)
•,t−1+ (1−β)A(l)
•,t
of factor β∈[0,1)(identically for ˆB(l)
•,t), similar to the original KFAC. Moreover, we apply the same
constant damping of strength λ >0to all Kronecker factors, ˜A(l)
•,t=ˆA(l)
•,t+λIand˜B(l)
•,t=ˆB(l)
•,t+λI
such that the curvature approximation used for preconditioning at step tis
G•,t≈diag
˜A(1)
Ω,t⊗˜B(1)
Ω,t, . . . , ˜A(L)
Ω,t⊗˜B(L)
Ω,t
+ diag
˜A(1)
∂Ω,t⊗˜B(1)
∂Ω,t, . . . , ˜A(L)
∂Ω,t⊗˜B(L)
∂Ω,t
.
Gradient preconditioning Given layer l’s mini-batch gradient g(l)
t=∂L(θt)/∂θ(l)
t∈Rp(l), we
obtain an update direction ∆(l)
t=−(˜A(l)
Ω,t⊗˜B(l)
Ω,t+˜A(l)
∂Ω,t⊗˜B(l)
∂Ω,t)−1g(l)
t∈Rp(l)using the trick
of [38, Appendix I] to invert the Kronecker sum via eigen-decomposing all Kronecker factors.
Learning rate and momentum From the preconditioned gradient ∆t∈RD, we consider two
different updates θt+1=θt+δtwe call KFAC andKFAC* . KFAC uses momentum over previous
updates, ˆδt=µδt−1+∆t, and µis chosen by the practitioner. Like ENGD, it uses a logarithmic
grid line search, selecting δt=α⋆ˆδtwithα⋆= arg minαL(θt+αˆδt)where α∈ {2−30, . . . , 20}.
KFAC* uses the automatic learning rate and momentum heuristic of the original KFAC optimizer. It
parametrizes the iteration’s update as δt+1(α, µ) =α∆t+µδt, then obtains the optimal parameters
by minimizing the quadratic model m(δt+1) =L(θt) +δ⊤
t+1gt+1/2δ⊤
t+1(G(θt) +λI)δt+1with
the exact damped Gramian. The optimal learning rate and momentum arg minα,µm(δt+1)are

α⋆
µ⋆
=−
∆⊤
tG(θt)∆t+λ∥∆t∥2∆⊤
tG(θt)δt+λ∆⊤
tδt
∆⊤
tG(θt)δt+λ∆⊤
tδtδ⊤
tG(θt)∆t+λ∥δt∥2−1
∆⊤
tgt
δ⊤
tgt
(see [ 38, Section 7] for details). The computational cost is dominated by the two Gramian-vector
products with ∆tandδt. By using the Gramian’s outer product structure [ 12,45], we perform them
with autodiff [48, 51] using one Jacobian-vector product each, as recommended in [38].
Computational complexity Inverting layer l’s Kronecker approximation of the Gramian requires
O(h(l)3+h(l+1)3)time and O(h(l)2+h(l+1)2)storage, where h(l)is the number of neurons in
thel-th layer, whereas inverting the exact block for layer would require O(h(l)3h(l+1)3)time and
O(h(l)2h(l+1)2)memory. In general, the improvement from the Kronecker factorization depends on
how close to square the weight matrices of a layer are, and therefore on the architecture. In practise,
the Kronecker factorization usually significantly reduces memory and run time. Further improvements
can be achieved by using structured Kronecker factors, e.g. (block-)diagonal matrices [32].
We use the forward Laplacian framework in our implementation, which we found to be significantly
faster and more memory efficient than computing batched Hessian traces, see §C.4.
7100101102103
Time [s]10−510−2101L2errorD= 257
100101102103
Time [s]10−510−310−1D= 9,873
100101102103
Time [s]10−510−310−1D= 116,097
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*Figure 1: Performance of different optimizers on the 2d Poisson equation (16) measured in relative
L2error against wall clock time for architectures with different parameter dimensions D.
100101102103
Time [s]10−510−310−1L2errorD= 449
100101102103
Time [s]10−410−2100D= 10,065
100101102103
Time [s]10−410−2100D= 116,865
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
Figure 2: Performance of different optimizers on the (4+1)d heat equation (17) measured in relative
L2error against wall clock time for architectures with different parameter dimensions D.
4 Experiments
We implement KFAC, KFAC*, and ENGD with either the per-layer or full Gramian in PyTorch [ 46].
As a matrix-free version of ENGD, we use the Hessian-free optimizer [ 36] which uses truncated
conjugate gradients (CG) with exact Gramian-vector products to precondition the gradient. We
chose this because there is a fully-featured implementation from Tatzel et al. [55] which offers many
additional heuristics like adaptive damping, CG backtracking, and backtracking line search, allowing
this algorithm to work well with little hyper-parameter tuning. As baselines, we use SGD with tuned
learning rate and momentum, Adam with tuned learning rate, and LBFGS with tuned learning rate
and history size. We tune hyper-parameters using Weights & Biases [ 60] (see §A.1 for the exact
protocol). For random/grid search, we run an initial round of approximately 50 runs with generous
search spaces, then narrow them down and re-run for another 50 runs; for Bayesian search, we assign
the same total compute to each optimizer. We report runs with lowest L2error estimated on a held-out
data set with the known solution to the studied PDE. To be comparable, all runs are executed on
a compute cluster with RTX 6000 GPUs (24 GiB RAM) in double precision, and we use the same
computation time budget for all optimizers on a fixed PINN problem. All search spaces and best run
hyper-parameters, as well as training curves over iteration count rather than time, are in §A.
Pedagogical example: 2d Poisson equation We start with a low-dimensional Poisson equation
from Müller & Zeinhofer [41] to reproduce ENGD’s performance (Figure 1). It is given by
−∆u(x, y) = 2 π2sin(πx) sin(πy)for(x, y)∈[0,1]2
u(x, y) = 0 for(x, y)∈∂[0,1]2.(16)
We choose a fixed data set of same size as the original paper, then use random/grid search to evaluate
the performance of all optimizers for different tanh -activated MLPs, one shallow and two with five
fully-connected layers of different width (all details in §A.2). We include ENGD whenever the
network’s parameter space is small enough to build up the Gramian.
For the shallow net (Figure 1, left), we can reproduce the results of [ 41], where exact ENGD achieves
high accuracy. In terms of computation time, our KFACs are competitive with full-ENGD for a long
8100101102103
Time [s]10−310−1101L2error5d Poisson,D= 116,865
100101102103
Time [s]10−310−110d Poisson, D= 118,145
100102104
Time [s]10−210−1100100d Poisson, D= 1,325,057
SGD Adam Hessian-free LBFGS KFAC KFAC*Figure 3: Optimizer performance on Poisson equations in high dimensions and different boundary
conditions measured in relative L2error against wall clock time for networks with Dparameters.
phase, outperforming the first-order and quasi-Newton baselines. In contrast to ENGD, which runs
out of memory for networks with more than 10 000 parameters, KFAC scales to larger networks
(Figure 1, center and right) and is competitive with other second-order optimizers like Hessian-free,
which uses more sophisticated heuristics. We make similar observations on a small (1+1)d heat
equation with the same models, see §A.7 and fig. A10.
An evolutionary problem: (4+1)d heat equation To demonstrate that our methods can also be
applied to other problems than the Poisson equation, we consider a four-dimensional heat equation
∂tu(t,x)−κ∆xu(t,x) = 0 fort∈[0,1],x∈[0,1]4,
u(0,x) =4X
i=1sin(2xi)forx∈[0,1]4,
u(t,x) = exp( −t)4X
i=1sin(2xi)fort∈[0,1],x∈∂[0,1]4,(17)
with diffusivity constant κ=1/4, similar to that studied in [ 41] (see §A.6 for the heat equation’s
PINN loss). We use the previous architectures with same hidden widths and evaluate optimizer
performance with random/grid search (all details in §A.8), see Figure 2. To prevent over-fitting, we
use mini-batches and sample a new batch each iteration. We noticed that KFAC improves significantly
when batches are sampled less frequently and hypothesize that it might need more iterations to make
similar progress than one iteration of Hessian-free or ENGD on a batch. Consequently we sample
a new batch only every 100 iterations for KFAC. To ensure that this does not lead to an unfair
advantage for KFAC, we conduct an additional experiment for the MLP with D= 116 864 where
we tune batch sizes, batch sampling frequencies, and all hyper-parameters with generous search
spaces using Bayesian search (§A.10). We find that this does not significantly boost performance of
the other methods (compare Figures 2 and A14). Again, we observe that KFAC offers competitive
performance compared to other second-order methods for networks with prohibitive size for ENGD
and consistently outperforms SGD, Adam, and LBFGS. We confirmed these observations with
another 5d Poisson equation on the same architectures, see §A.3 and fig. A7.
High-dimensional Poisson equations To demonstrate scaling to high-dimensional PDEs and even
larger neural networks, we consider three Poisson equations ( d= 5,10,100) with different boundary
conditions used in [16, 41], which admit the solutions
u⋆(x) =5X
i=1cos(πxi)forx∈[0,1]5,
u⋆(x) =5X
k=1x2k−1x2kforx∈[0,1]10,
u⋆(x) =∥x∥2
2forx∈[0,1]100.(18)
We use the same architectures as before, but with larger intermediate widths and parameters up to
a million (Figure 3). Due to lacking references for training such high-dimensional problems, we
9select all hyper-parameters via Bayesian search, including batch sizes and batch sampling frequencies
(details in §A.5). We see a similar picture as before with KFAC consistently outperforming first-
order methods and LBFGS, offering competitive performance with Hessian-free. To account for the
possibility that the Bayesian search did not properly identify good hyper-parameters, we conduct a
random/grid search experiment for the 10d Poisson equation (Figure 3, middle), using similar batch
sizes and same batch sampling frequencies as for the (4 + 1) d heat equation (details in §A.4). In
this experiment, KFAC also achieved similar performance than Hessian-free and outperformed SGD,
Adam, and LBFGS (Figure A8).
(9+1)d Fokker-Planck equation To show the applicability to nonlinear PDEs, we consider a
Fokker-Planck equation in logarithmic space. PINN formulations of the Fokker-Planck equation have
been considered in [23, 54]. Concretely, we are solving a nine-dimensional equation of the form
∂tq(t,x)−d
2−1
2∇q(t,x)·x− ∥∇ q(t,x)∥2−∆q(t,x) = 0 , q(0) = log( p∗(0)), (19)
100101102103
Time [s]10−1100101L2errorSGD
Adam
Hessian-free
LBFGS
KFAC
KFAC*
Figure 4: Performance of different optimizers on
a (9+1)d logarithmic Fokker-Planck equation in
relative L2error against wall clock time.withd= 9,t∈[0,1]andx∈R9, where in
practice R9is replaced by [−5,5]9. The solution
isq∗= log( p∗)andp∗is given by p∗(t,x)∼
N(0,exp(−t)I+(1−exp(−t))2I). We model
the solution with a medium sized tanh-activated
MLP with D= 118 145 parameters, batch sizes
areNΩ= 3 000 ,N∂Ω= 1 000 , and we assign
each run a computation time budget of 6 000 s.
As in previous experiments, the batches are re-
sampled every iteration for all optimizers except
for KFAC and KFAC*, which use the same batch
for ten steps (details in §A.11). Figure 4 reports
theL2error over training time. Again, KFAC is
among the best performing optimizers offering
competitive performance to Hessian-free and
clearly outperforming all first-order methods.
5 Discussion and Conclusion
We extended the concept of Kronecker-factored approximate curvature (KFAC) to Gauss-Newton
matrices of Physics-informed neural network (PINN) losses that involve derivatives, rather than
function evaluations, of the neural net. This greatly reduces the computational cost of approximate
natural gradient methods, which are known to work well on PINNs, and allows them to scale to much
larger nets. Our approach goes beyond the established KFAC for traditional supervised problems
as it captures contributions from a PDE’s differential operator that are crucial for optimization.
To establish KFAC for such losses, we use Taylor-mode autodiff to view the differential operator’s
compute graph as a forward net with shared weights, then apply the recently-developed formulation of
KFAC for linear layers with weight sharing. Empirically, we find that our KFAC-based optimizers are
competitive with expensive second-order methods on small problems and scale to high-dimensional
neural networks and PDEs while consistently outperforming first-order methods and LBFGS.
Limitations & future directions While our implementation currently only supports MLPs and
the Poisson and heat equations, the concepts we use to derive KFAC (Taylor-mode, weight sharing)
apply to arbitrary architectures and PDEs, as described in §3.3. We are excited that our current
algorithms show promising performance when compared to second-order methods with sophisticated
heuristics. In fact, the original KFAC optimizer itself [ 38] relies heavily on such heuristics that are
said to be crucial for its performance [ 8]. Our algorithms borrow components, but we did not explore
all bells and whistles, e.g. adaptive damping and heuristics to distribute damping over the Kronecker
factors. We believe our current algorithm’s performance can further be improved, e.g. by exploring
(1) updating the KFAC matrices less frequently, as is standard for traditional KFAC, (2) merging the
two Kronecker approximations for boundary and interior Gramians into a single one, (3) removing
matrix inversions [ 31], (4) using structured Kronecker factors [ 32], (5) computing the Kronecker
factors in parallel with the gradient [ 11], (6) using single or mixed precision training [ 40], and (7)
studying cheaper KFAC flavours based on the empirical Fisher [ 27] or input-based curvature [ 2,49].
10Acknowledgments and Disclosure of Funding
The authors thank Runa Eschenhagen for insightful discussions on KFAC for linear weight sharing
layers. FD would like to thank Luca Thiede for his adamant questions about Taylor mode and
forward Laplacians. Resources used in preparing this research were provided, in part, by the
Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the
Vector Institute. JM acknowledges funding by the Deutsche Forschungsgemeinschaft (DFG, German
Research Foundation) under the project number 442047500 through the Collaborative Research
Center Sparsity and Singular Structures (SFB 1481). MZ acknowledges support from an ETH
Postdoctoral Fellowship for the project “Reliable, Efficient, and Scalable Methods for Scientific
Machine Learning”.
References
[1]Amari, S.-I. Natural gradient works efficiently in learning. Neural computation , 10(2):251–276,
1998.
[2]Benzing, F. Gradient Descent on Neurons and its Link to Approximate Second-order Optimiza-
tion. In International Conference on Machine Learning (ICML) , 2022.
[3]Bettencourt, J., Johnson, M. J., and Duvenaud, D. Taylor-mode automatic differentiation
for higher-order derivatives in JAX. In Advances in Neural Information Processing Systems
(NeurIPS); Workhop on Program Transformations for ML , 2019.
[4]Bonfanti, A., Bruno, G., and Cipriani, C. The Challenges of the Nonlinear Regime for Physics-
Informed Neural Networks. arXiv preprint arXiv:2402.03864 , 2024.
[5]Botev, A., Ritter, H., and Barber, D. Practical Gauss-Newton optimisation for deep learning. In
International Conference on Machine Learning (ICML) , 2017.
[6]Bottou, L., Curtis, F. E., and Nocedal, J. Optimization methods for large-scale machine learning.
SIAM Review (SIREV) , 60, 2016.
[7]Chen, Z., McCarran, J., Vizcaino, E., Solja ˇci´c, M., and Luo, D. TENG: Time-Evolving Natural
Gradient for Solving PDEs with Deep Neural Net. arXiv preprint arXiv:2404.10771 , 2024.
[8] Clarke, R. M., Su, B., and Hernández-Lobato, J. M. Adam through a second-order lens. 2023.
[9]Cuomo, S., Di Cola, V . S., Giampaolo, F., Rozza, G., Raissi, M., and Piccialli, F. Scientific
machine learning through physics–informed neural networks: Where we are and what’s next.
Journal of Scientific Computing , 92(3):88, 2022.
[10] Dangel, F., Harmeling, S., and Hennig, P. Modular Block-diagonal Curvature Approximations
for Feedforward Architectures. In International Conference on Artificial Intelligence and
Statistics (AISTATS) , 2020.
[11] Dangel, F., Kunstner, F., and Hennig, P. BackPACK: Packing more into Backprop. In Interna-
tional Conference on Learning Representations (ICLR) , 2020.
[12] Dangel, F., Tatzel, L., and Hennig, P. ViViT: Curvature access through the generalized gauss-
newton’s low-rank structure. Transactions on Machine Learning Research (TMLR) , 2022.
[13] Daw, A., Bu, J., Wang, S., Perdikaris, P., and Karpatne, A. Rethinking the importance of
sampling in physics-informed neural networks. arXiv preprint arXiv:2207.02338 , 2022.
[14] De Ryck, T., Bonnet, F., Mishra, S., and de Bézenac, E. An operator preconditioning perspective
on training in physics-informed machine learning. arXiv preprint arXiv:2310.05801 , 2023.
[15] Dissanayake, M. and Phan-Thien, N. Neural-network-based approximations for solving partial
differential equations. communications in Numerical Methods in Engineering , 10(3):195–201,
1994.
[16] E, W. and Yu, B. The deep ritz method: a deep learning-based numerical algorithm for solving
variational problems. Communications in Mathematics and Statistics , 6(1):1–12, 2018.
11[17] Eschenhagen, R., Immer, A., Turner, R. E., Schneider, F., and Hennig, P. Kronecker-Factored
Approximate Curvature for Modern Neural Network Architectures. In Advances in Neural
Information Processing Systems (NeurIPS) , 2023.
[18] Griewank, A. and Walther, A. Evaluating derivatives: principles and techniques of algorithmic
differentiation . SIAM, 2008.
[19] Griewank, A., Juedes, D., and Utke, J. Algorithm 755: ADOL-C: A package for the automatic
differentiation of algorithms written in C/C++. ACM Transactions on Mathematical Software
(TOMS) , 22(2):131–167, 1996.
[20] Grosse, R. and Martens, J. A Kronecker-Factored Approximate Fisher Matrix for Convolution
Layers. In International Conference on Machine Learning (ICML) , 2016.
[21] Grosse, R., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini, A., Steiner, B., Li, D., Durmus, E.,
Perez, E., et al. Studying large language model generalization with influence functions. arXiv
preprint arXiv:2308.03296 , 2023.
[22] Heskes, T. On “natural” learning and pruning in multilayered perceptrons. Neural Computation ,
12(4):881–901, 2000.
[23] Hu, Z., Zhang, Z., Karniadakis, G. E., and Kawaguchi, K. Score-based physics-informed neural
networks for high-dimensional fokker-planck equations. arXiv preprint arXiv:2402.07465 ,
2024.
[24] Jnini, A., Vella, F., and Zeinhofer, M. Gauss-Newton Natural Gradient Descent for Physics-
Informed Computational Fluid Dynamics. arXiv preprint arXiv:2402.10680 , 2024.
[25] Johnson, M. J., Bettencourt, J., Maclaurin, D., and Duvenaud, D. Taylor-made higher-order au-
tomatic differentiation. 2021. URL https://github.com/google/jax/files/6717197/
jet.pdf . Accessed January 03, 2024.
[26] Krishnapriyan, A., Gholami, A., Zhe, S., Kirby, R., and Mahoney, M. W. Characterizing
possible failure modes in physics-informed neural networks. Advances in Neural Information
Processing Systems , 34:26548–26560, 2021.
[27] Kunstner, F., Hennig, P., and Balles, L. Limitations of the empirical Fisher approximation for
natural gradient descent. Advances in neural information processing systems , 32, 2019.
[28] Lagaris, I. E., Likas, A., and Fotiadis, D. I. Artificial neural networks for solving ordinary and
partial differential equations. IEEE transactions on neural networks , 9(5):987–1000, 1998.
[29] Li, R., Ye, H., Jiang, D., Wen, X., Wang, C., Li, Z., Li, X., He, D., Chen, J., Ren, W., et al.
Forward Laplacian: A New Computational Framework for Neural Network-based Variational
Monte Carlo. 2023.
[30] Li, R., Wang, C., Ye, H., He, D., and Wang, L. DOF: Accelerating high-order differential
operators with forward propagation. In International Conference on Learning Representations
(ICLR), Workshop on AI4DifferentialEquations In Science , 2024.
[31] Lin, W., Duruisseaux, V ., Leok, M., Nielsen, F., Khan, M. E., and Schmidt, M. Simplifying
Momentum-based Riemannian Submanifold Optimization. 2023.
[32] Lin, W., Dangel, F., Eschenhagen, R., Neklyudov, K., Kristiadi, A., Turner, R. E., and Makhzani,
A. Structured inverse-free natural gradient descent: Memory-efficient & numerically-stable
KFAC. In International Conference on Machine Learning (ICML) , 2024.
[33] Liu, S., Su, C., Yao, J., Hao, Z., Su, H., Wu, Y ., and Zhu, J. Preconditioning for physics-informed
neural networks. arXiv preprint arXiv:2402.00531 , 2024.
[34] Lu, L., Meng, X., Mao, Z., and Karniadakis, G. E. DeepXDE: A deep learning library for
solving differential equations. SIAM Review , 63(1):208–228, 2021.
[35] Markidis, S. The old and the new: Can physics-informed deep-learning replace traditional
linear solvers? Frontiers in big Data , 4:669097, 2021.
12[36] Martens, J. Deep learning via Hessian-free optimization. In International Conference on
Machine Learning (ICML) , 2010.
[37] Martens, J. New insights and perspectives on the natural gradient method, 2020.
[38] Martens, J. and Grosse, R. Optimizing Neural Networks with Kronecker-factored Approximate
Curvature. In International Conference on Machine Learning (ICML) , 2015.
[39] Martens, J., Ba, J., and Johnson, M. Kronecker-factored Curvature Approximations for Recur-
rent Neural Networks. In International Conference on Learning Representations , 2018. URL
https://openreview.net/forum?id=HyMTkQZAb .
[40] Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston,
M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. 2017.
[41] Müller, J. and Zeinhofer, M. Achieving high accuracy with PINNs via energy natural gradient
descent. In International Conference on Machine Learning , pp. 25471–25485. PMLR, 2023.
[42] Müller, J. and Zeinhofer, M. Optimization in SciML–A Function Space Perspective. arXiv
preprint arXiv:2402.07318 , 2024.
[43] Nabian, M. A., Gladstone, R. J., and Meidani, H. Efficient training of physics-informed neural
networks via importance sampling. Computer-Aided Civil and Infrastructure Engineering , 36
(8):962–977, 2021.
[44] Osawa, K., Li, S., and Hoefler, T. Pipefisher: Efficient training of large language models using
pipelining and Fisher information matrices. Proceedings of Machine Learning and Systems , 5,
2023.
[45] Papyan, V . Measurements of three-level hierarchical structure in the outliers in the spectrum of
deepnet Hessians. In International Conference on Machine Learning (ICML) , 2019.
[46] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,
Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani,
A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An Imperative
Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing
Systems (NeurIPS) . 2019.
[47] Pauloski, J. G., Huang, Q., Huang, L., Venkataraman, S., Chard, K., Foster, I., and Zhang, Z.
Kaisa: an adaptive second-order optimizer framework for deep neural networks. In Proceedings
of the International Conference for High Performance Computing, Networking, Storage and
Analysis , pp. 1–14, 2021.
[48] Pearlmutter, B. A. Fast Exact Multiplication by the Hessian. Neural Computation , 1994.
[49] Petersen, F., Sutter, T., Borgelt, C., Huh, D., Kuehne, H., Sun, Y ., and Deussen, O. ISAAC
Newton: Input-based Approximate Curvature for Newton’s Method. In International Conference
on Learning Representations (ICLR) , 2023.
[50] Raissi, M., Perdikaris, P., and Karniadakis, G. E. Physics-informed neural networks: A
deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational physics , 378:686–707, 2019.
[51] Schraudolph, N. N. Fast curvature matrix-vector products for second-order gradient descent.
Neural Computation , 2002.
[52] Sirignano, J. and Spiliopoulos, K. Dgm: A deep learning algorithm for solving partial differential
equations. Journal of computational physics , 375:1339–1364, 2018.
[53] Skorski, M. Chain rules for hessian and higher derivatives made easy by tensor calculus. arXiv
preprint arXiv:1911.13292 , 2019.
[54] Sun, J., Berner, J., Richter, L., Zeinhofer, M., Müller, J., Azizzadenesheli, K., and Anandku-
mar, A. Dynamical measure transport and neural pde solvers for sampling. arXiv preprint
arXiv:2407.07873 , 2024.
13[55] Tatzel, L., Hennig, P., and Schneider, F. Late-Phase Second-Order Training. In Advances in
Neural Information Processing Systems (NeurIPS), Workshop Has it Trained Yet? , 2022.
[56] van der Meer, R., Oosterlee, C. W., and Borovykh, A. Optimally weighted loss functions for
solving PDEs with neural networks. Journal of Computational and Applied Mathematics , 405:
113887, 2022.
[57] Wang, S., Teng, Y ., and Perdikaris, P. Understanding and mitigating gradient flow pathologies in
physics-informed neural networks. SIAM Journal on Scientific Computing , 43(5):A3055–A3081,
2021.
[58] Wang, S., Sankaran, S., and Perdikaris, P. Respecting causality is all you need for training
physics-informed neural networks. arXiv preprint arXiv:2203.07404 , 2022.
[59] Wang, S., Yu, X., and Perdikaris, P. When and why PINNs fail to train: A neural tangent kernel
perspective. Journal of Computational Physics , 449:110768, 2022.
[60] Weights and Biases. Experiment Tracking with Weights and Biases, 2020. URL https:
//www.wandb.ai/ . Software available from wandb.ai.
[61] Wu, C., Zhu, M., Tan, Q., Kartha, Y ., and Lu, L. A comprehensive study of non-adaptive and
residual-based adaptive sampling for physics-informed neural networks. Computer Methods in
Applied Mechanics and Engineering , 403:115671, 2023.
[62] Zampini, S., Zerbinati, U., Turkiyyah, G., and Keyes, D. PETScML: Second-order solvers for
training regression problems in Scientific Machine Learning. arXiv preprint arXiv:2403.12188 ,
2024.
[63] Zapf, B., Haubner, J., Kuchta, M., Ringstad, G., Eide, P. K., and Mardal, K.-A. Investigating
molecular transport in the human brain from MRI with physics-informed neural networks.
Scientific Reports , 12(1):1–12, 2022.
[64] Zeng, Q., Kothari, Y ., Bryngelson, S. H., and Schäfer, F. Competitive physics informed networks.
arXiv preprint arXiv:2204.11144 , 2022.
14Kronecker-Factored Approximate Curvature for
Physics-Informed Neural Networks (Supplementary
Material)
A Experimental Details and Additional Results 15
A.1 Hyper-Parameter Tuning Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.2 2d Poisson Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 5d Poisson Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.4 10d Poisson Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A.5 5/10/100-d Poisson Equations with Bayesian Search . . . . . . . . . . . . . . . . . 24
A.6 PINN Loss for the Heat Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
A.7 1+1d Heat Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
A.8 4+1d Heat Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
A.9 Robustness Under Model Initialization for 4+1d Heat Equation . . . . . . . . . . . 34
A.10 4+1d Heat Equation with Bayesian Search . . . . . . . . . . . . . . . . . . . . . . 34
A.11 9+1-d Logarithmic Fokker-Planck Equation with Random Search . . . . . . . . . . 36
B Pseudo-Code: KFAC for the Poisson Equation 38
C Taylor-Mode Automatic Differentiation & Forward Laplacian 39
C.1 Taylor-Mode Automatic Differentiation . . . . . . . . . . . . . . . . . . . . . . . 39
C.2 Forward Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
C.3 Generalization of the Forward Laplacian to Weighted Sums of Second Derivatives . 42
C.4 Comparison of Forward Laplacian and Autodiff Laplacian . . . . . . . . . . . . . 43
D Backpropagation Perspective of the Laplacian 43
D.1 Hessian Backpropagation and Backward Laplacian . . . . . . . . . . . . . . . . . 44
D.2 Parameter Jacobian of the Backward Laplacian . . . . . . . . . . . . . . . . . . . 46
D.3 Gramian of the Backward Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . 46
A Experimental Details and Additional Results
A.1 Hyper-Parameter Tuning Protocol
In all our experiments, we tune the following optimizer hyper-parameters and otherwise use the
PyTorch default values:
•SGD: learning rate, momentum
15•Adam: learning rate
•Hessian-free: type of curvature matrix (Hessian or GGN), damping, whether to adapt
damping over time (yes or no), maximum number of CG iterations
•LBFGS: learning rate, history size
•ENGD: damping, factor of the exponential moving average applied to the Gramian, initial-
ization of the Gramian (zero or identity matrix)
•KFAC: factor of the exponential moving average applied to the Kronecker factors, damping,
momentum, initialization of the Kronecker factors (zero or identity matrix)
•KFAC*: factor of the exponential moving average applied to the Kronecker factors, damping,
initialization of the Kronecker factors (zero or identity matrix)
Depending on the optimizer and experiment we use grid, random, or Bayesian search from Weights
& Biases to determine the hyper-parameters. Each individual run is executed in double precision and
allowed to run for a given time budget, and we rank runs by the final L2error on a fixed evaluation
data set. To allow comparison, all runs are executed on RTX 6000 GPUs with 24 GiB of RAM. For
grid and random searches, we use a round-based approach. First, we choose a relatively wide search
space and limit to approximately 50 runs. In a second round, we narrow down the hyper-parameter
space based on the first round, then re-run for another approximately 50 runs. We will release the
details of all hyper-parameter search spaces, as well as the hyper-parameters for the best runs in our
implementation.
A.2 2d Poisson Equation
Setup We consider a two-dimensional Poisson equation −∆u(x, y) = 2 π2sin(πx) sin(πy)on
the unit square (x, y)∈[0,1]2with sine product right-hand side and zero boundary conditions
u(x, y) = 0 for(x, y)∈∂[0,1]2. We choose a single set of training points with NΩ= 900 , N∂Ω=
120. The L2error is evaluated on a separate set of 9 000 data points using the known solution
u⋆(x, y) = sin( πx) sin(πy). Each run is limited to a compute time of 1 000 s. We compare three
MLP architectures of increasing size, each of whose linear layers are Tanh-activated except for
the final one: a shallow 2→64→1MLP with D= 257 trainable parameters, a five layer
2→64→64→48→48→1MLP with D= 9 873 trainable parameters, and a five layer
2→256→256→128→128→1MLP with D= 116 097 trainable parameters. For the biggest
architecture, full and per-layer ENGD lead to out-of-memory errors and are thus not tested in the
experiments. Figure A5 visualizes the results, and Figure A6 illustrates the learned solutions over
training for all optimizers on the shallow MLP.
Best run details The runs shown in Figure A5 correspond to the following hyper-parameters:
•2→64→1MLP with D= 257
– SGD: learning rate: 1.805 015 ·10−2, momentum: 9.9·10−1
– Adam: learning rate: 1.692 339 ·10−3
–Hessian-free: curvature matrix: GGN , initial damping: 500, constant damping: no,
maximum CG iterations: 300
– LBFGS: learning rate: 5·10−1, history size: 150
–ENGD (full): damping: 1·10−10, exponential moving average: 3·10−1, initialize
Gramian to identity: yes
–ENGD (layer-wise): damping: 0, exponential moving average: 9·10−1, initialize
Gramian to identity: yes
–KFAC: damping: 1.544 099 ·10−12, momentum: 5.117 575 ·10−1, exponential moving
average: 4.496 490 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 1.215 640 ·10−10, exponential moving average: 9.263 314 ·10−1,
initialize Kronecker factors to identity: yes
•2→64→64→48→48→1MLP with D= 9 873
– SGD: learning rate: 3.758 303 ·10−3, momentum: 9·10−1
– Adam: learning rate: 2.052 448 ·10−4
16(a)
100101102103
Time [s]10−510−2101L2errorD= 257
100101102103
Time [s]10−510−310−1D= 9,873
100101102103
Time [s]10−510−310−1D= 116,097
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
100101102103
Time [s]10−1010−610−2102LossD= 257
100101102103
Time [s]10−810−4100D= 9,873
100101102103
Time [s]10−710−3101D= 116,097
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
(b)
101103105
Iteration10−510−2101L2errorD= 257
101103105
Iteration10−510−310−1D= 9,873
101103105
Iteration10−510−310−1D= 116,097
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
101103105
Iteration10−1010−610−2102LossD= 257
101103105
Iteration10−810−4100D= 9,873
101103105
Iteration10−710−3101D= 116,097
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
Figure A5: Training loss and evaluation L2error for learning the solution to a 2d Poisson equation
over (a) time and (b) steps. Columns are different neural networks.
–Hessian-free: curvature matrix: GGN , initial damping: 1·10−1, constant damping:
no, maximum CG iterations: 350
– LBFGS: learning rate: 1·10−1, history size: 200
–ENGD (full): damping: 1·10−10, exponential moving average: 6·10−1, initialize
Gramian to identity: no
–ENGD (layer-wise): damping: 1·10−6, exponential moving average: 3·10−1, initialize
Gramian to identity: no
–KFAC: damping: 2.640 390 ·10−11, momentum: 9.995 595 ·10−2, exponential moving
average: 5.556 664 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 2.989 247 ·10−13, exponential moving average: 6.258 340 ·10−1,
initialize Kronecker factors to identity: yes
•2→256→256→128→128→1MLP with D= 116 097
– SGD: learning rate: 2.478 674 ·10−3, momentum: 9·10−1
– Adam: learning rate: 6.406 108 ·10−4
–Hessian-free: curvature matrix: GGN , initial damping: 50, constant damping: no,
maximum CG iterations: 350
– LBFGS: learning rate: 1·10−1, history size: 225
17Optimizer First step 0.1% trained 1% trained 10% trained True solution
SGD
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 5.3·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 335, Loss: 2.8·10−1,L2loss: 2.4·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 3299 , Loss: 1.7·10−4,L2loss: 3.6·10−3
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 32494 , Loss: 5.6·10−6,L2loss: 1.8·10−4
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 5.3·10−1
Adam
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 3.8·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 335, Loss: 5.6·100,L2loss: 4.1·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 3299 , Loss: 1.4·10−2,L2loss: 4.1·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 32494 , Loss: 9.8·10−6,L2loss: 7.7·10−4
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 3.8·10−1
LBFGS
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 3.4·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 50, Loss: 4.0·10−5,L2loss: 1.1·10−3
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 491, Loss: 1.0·10−6,L2loss: 1.8·10−4
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 4831 , Loss: 1.0·10−6,L2loss: 1.8·10−4
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 3.4·10−1
Hessian-free
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 3.1·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 4, Loss: 3.9·101,L2loss: 1.8·100
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 35, Loss: 1.0·10−4,L2loss: 3.1·10−3
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 335, Loss: 2.1·10−8,L2loss: 1.8·10−5
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 3.1·10−1
ENGD (full)
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 1.5·100
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 6, Loss: 1.1·100,L2loss: 3.4·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 55, Loss: 7.8·10−1,L2loss: 1.3·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 594, Loss: 5.1·10−3,L2loss: 1.9·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 1.5·100
ENGD
(layer-wise)
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 3.7·100
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 6, Loss: 1.7·100,L2loss: 3.7·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 61, Loss: 8.0·10−3,L2loss: 3.3·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 594, Loss: 3.1·10−11,L2loss: 6.6·10−7
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 3.7·100
KFAC
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 1.7·100
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 20, Loss: 3.4·10−1,L2loss: 3.3·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 208, Loss: 4.9·10−6,L2loss: 1.5·10−3
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 2049 , Loss: 8.4·10−10,L2loss: 1.1·10−5
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 1.7·100
KFAC*
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 4.0·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 35, Loss: 7.2·10−2,L2loss: 7.5·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 335, Loss: 5.7·10−5,L2loss: 2.1·10−3
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 3629 , Loss: 1.8·10−9,L2loss: 4.4·10−6
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Normalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
x1Normalized true solutionStep: 0, Loss: 4.9·101,L2loss: 4.0·10−1
Figure A6: Visual comparison learned and true solutions while training with different optimizers for
the 2d Poisson equation using a two-layer MLP (corresponding to the curves in Figure 1 left). All
functions are shown on the unit square (x, y)∈Ω = [0; 1]2and normalized to the unit interval.
–KFAC: damping: 1.710 269 ·10−13, momentum: 8.484 996 ·10−1, exponential moving
average: 9.636 460 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 1.232 407 ·10−13, exponential moving average: 9.488 207 ·10−1,
initialize Kronecker factors to identity: yes
Search space details The runs shown in Figure A5 were determined to be the best via a search with
approximately 50 runs on the following search spaces which were obtained by refining an initially
wider search ( Udenotes a uniform, and LUa log-uniform distribution):
•2→64→1MLP with D= 257
18–SGD: learning rate: LU([1·10−3; 1·10−1]), momentum: U({0,3·10−1,6·10−1,9·
10−1,9.9·10−1})
– Adam: learning rate: LU([5·10−4; 1·10−1])
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({100,1,1·
10−2,1·10−4,1·10−6}), constant damping: U({no,yes}), maximum CG iterations:
U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({75,100,125,150,175,200,225,250})
–ENGD (full): damping: U({1·10−6,1·10−8,1·10−10,1·10−12,0}), exponential
moving average: U({0,3·10−1,6·10−1,9·10−1,9.9·10−1}), initialize Gramian to
identity: U({no,yes})
–ENGD (layer-wise): damping: U({1·10−4,1·10−6,1·10−8,1·10−10,0}), exponential
moving average: U({0,3·10−1,6·10−1,9·10−1,9.9·10−1}), initialize Gramian to
identity: U({no,yes})
–KFAC: damping: LU([1·10−13; 1·10−7]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([0; 9.9·10−1]), initialize Kronecker factors to identity: yes
–KFAC*: damping: LU([1·10−13; 1·10−7]), exponential moving average: U([0; 9.9·
10−1]), initialize Kronecker factors to identity: yes
•2→64→64→48→48→1MLP with D= 9 873
–SGD: learning rate: LU([1·10−3; 1·10−2]), momentum: U({0,3·10−1,6·10−1,9·
10−1})
– Adam: learning rate: LU([1·10−4; 5·10−1])
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({1,1·
10−1,1·10−2,1·10−3,1·10−4}), constant damping: U({no,yes}), maximum CG
iterations: U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({50,75,100,125,150,175,200,225})
–ENGD (full): damping: U({1·10−8,1·10−9,1·10−10,1·10−11,1·10−12,0}),
exponential moving average: U({0,3·10−1,6·10−1,9·10−1}), initialize Gramian
to identity: U({no,yes})
–ENGD (layer-wise): damping: U({1·10−2,1·10−3,1·10−4,1·10−5,1·10−6}),
exponential moving average: U({0,3·10−1,6·10−1,9·10−1,9.9·10−1}), initialize
Gramian to identity: U({no,yes})
–KFAC: damping: LU([1·10−13; 1·10−7]), momentum: U([0; 9.9·10−1]), expo-
nential moving average: U([0; 9.9·10−1]), initialize Kronecker factors to identity:
U({no,yes})
–KFAC*: damping: LU([1·10−13; 1·10−7]), exponential moving average: U([0; 9.9·
10−1]), initialize Kronecker factors to identity: U({no,yes})
•2→256→256→128→128→1MLP with D= 116 097
–SGD: learning rate: LU([1·10−3; 1·10−2]), momentum: U({0,3·10−1,6·10−1,9·
10−1})
– Adam: learning rate: LU([1·10−4; 5·10−1])
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({1,1·
10−1,1·10−2,1·10−3,1·10−4}), constant damping: U({no,yes}), maximum CG
iterations: U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({50,75,100,125,150,175,200,225})
–KFAC: damping: LU([1·10−13; 1·10−7]), momentum: U([0; 9.9·10−1]), expo-
nential moving average: U([0; 9.9·10−1]), initialize Kronecker factors to identity:
U({no,yes})
–KFAC*: damping: LU([1·10−13; 1·10−7]), exponential moving average: U([0; 9.9·
10−1]), initialize Kronecker factors to identity: U({no,yes})
19A.3 5d Poisson Equation
Setup We consider a five-dimensional Poisson equation −∆u(x) =π2P5
i=1cos(πxi)on the
five-dimensional unit square x∈[0,1]5with cosine sum right-hand side and boundary conditions
u(x) =P5
i=1cos(πxi)forx∈∂[0,1]5. We sample training batches of size NΩ= 3 000 , N∂Ω=
500and evaluate the L2error on a separate set of 30 000 data points using the known solution
u⋆(x) =P5
i=1cos(πxi). All optimizers except for KFAC sample a new training batch each iteration.
KFAC only re-samples every 100 iterations because we noticed significant improvement with multiple
iterations on a fixed batch. To make sure that this does not lead to an unfair advantage of KFAC, we
conduct an additional experiment where we also tune the batch sampling frequency, as well as other
hyper-parameters; see §A.5. The results presented in this section are consistent with this additional
experiment (compare the rightmost column of Figure A7 and the leftmost column of Figure A9).
Each run is limited to 3000 s. We compare three MLP architectures of increasing size, each of whose
linear layers are Tanh-activated except for the final one: a shallow 5→64→1MLP with D= 449
trainable parameters, a five layer 5→64→64→48→48→1MLP with D= 10 065 trainable
parameters, and a five layer 5→256→256→128→128→1MLP with D= 116 864 trainable
parameters. For the biggest architecture, full and layer-wise ENGD lead to out-of-memory errors and
are thus not tested in the experiments. Figure A7 visualizes the results.
(a)
100101102103
Time [s]10−510−2101L2errorD= 449
100101102103
Time [s]10−310−1D= 10,065
100101102103
Time [s]10−310−1101D= 116,865
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
100101102103
Time [s]10−810−3102LossD= 449
100101102103
Time [s]10−510−2101D= 10,065
100101102103
Time [s]10−4100104D= 116,865
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
(b)
101103105
Iteration10−510−2101L2errorD= 449
101103105
Iteration10−310−1D= 10,065
101103
Iteration10−310−1101D= 116,865
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
101103105
Iteration10−810−3102LossD= 449
101103105
Iteration10−510−2101D= 10,065
101103
Iteration10−4100104D= 116,865
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
Figure A7: Training loss and evaluation L2error for learning the solution to a 5d Poisson equation
over (a) time and (b) steps. Columns are different neural networks.
20Best run details The runs shown in Figure A7 correspond to the following hyper-parameters:
•5→64→1MLP with D= 449
– SGD: learning rate: 4.829 757 ·10−3, momentum: 9·10−1
– Adam: learning rate: 1.527 101 ·10−3
–Hessian-free: curvature matrix: GGN , initial damping: 5, constant damping: no,
maximum CG iterations: 350
– LBFGS: learning rate: 5·10−2, history size: 125
–ENGD (full): damping: 1·10−11, exponential moving average: 6·10−1, initialize
Gramian to identity: yes
–ENGD (layer-wise): damping: 1·10−6, exponential moving average: 6·10−1, initialize
Gramian to identity: yes
–KFAC: damping: 3.030 734 ·10−13, momentum: 4.410 155 ·10−1, exponential moving
average: 3.260 663 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 9.835 853 ·10−13, exponential moving average: 7.714 287 ·10−1,
initialize Kronecker factors to identity: yes
•5→64→64→48→48→1MLP with D= 10 065
– SGD: learning rate: 1.007 555 ·10−3, momentum: 9·10−1
– Adam: learning rate: 6.999 994 ·10−4
–Hessian-free: curvature matrix: GGN , initial damping: 1·10−1, constant damping:
no, maximum CG iterations: 350
– LBFGS: learning rate: 1·10−1, history size: 225
–ENGD (full): damping: 1·10−8, exponential moving average: 3·10−1, initialize
Gramian to identity: no
–ENGD (layer-wise): damping: 1·10−6, exponential moving average: 3·10−1, initialize
Gramian to identity: no
–KFAC: damping: 1.183 063 ·10−12, momentum: 9.058 900 ·10−1, exponential moving
average: 9.588 846 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 2.829 461 ·10−10, exponential moving average: 9.001 393 ·10−1,
initialize Kronecker factors to identity: yes
•5→256→256→128→128→1MLP with D= 116 865
– SGD: learning rate: 1.924 173 ·10−3, momentum: 9·10−1
– Adam: learning rate: 5.416 376 ·10−4
–Hessian-free: curvature matrix: GGN , initial damping: 5, constant damping: no,
maximum CG iterations: 350
– LBFGS: learning rate: 2·10−2, history size: 225
–KFAC: damping: 1.844 213 ·10−11, momentum: 7.528 559 ·10−1, exponential moving
average: 9.307 849 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 2.183 605 ·10−12, exponential moving average: 9.563 992 ·10−1,
initialize Kronecker factors to identity: yes
Search space details The runs shown in Figure A7 were determined to be the best via a search with
approximately 50 runs on the following search spaces which were obtained by refining an initially
wider search ( Udenotes a uniform, and LUa log-uniform distribution):
•5→64→1MLP with D= 449
–SGD: learning rate: LU([1·10−3; 1·10−2]), momentum: U({0,3·10−1,6·10−1,9·
10−1})
– Adam: learning rate: LU([1·10−4; 5·10−1])
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({1,1·
10−1,1·10−2,1·10−3,1·10−4}), constant damping: U({no,yes}), maximum CG
iterations: U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({50,75,100,125,150,175,200,225})
21–ENGD (full): damping: U({1·10−8,1·10−9,1·10−10,1·10−11,1·10−12,0}),
exponential moving average: U({0,3·10−1,6·10−1,9·10−1}), initialize Gramian
to identity: U({no,yes})
–ENGD (layer-wise): damping: U({1·10−2,1·10−3,1·10−4,1·10−5,1·10−6}),
exponential moving average: U({0,3·10−1,6·10−1,9·10−1,9.9·10−1}), initialize
Gramian to identity: U({no,yes})
–KFAC: damping: LU([1·10−13; 1·10−7]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([0; 9.9·10−1]), initialize Kronecker factors to identity: yes
–KFAC*: damping: LU([1·10−13; 1·10−7]), exponential moving average: U([0; 9.9·
10−1]), initialize Kronecker factors to identity: yes
•5→64→64→48→48→1MLP with D= 10 065
–SGD: learning rate: LU([1·10−3; 1·10−2]), momentum: U({0,3·10−1,6·10−1,9·
10−1})
– Adam: learning rate: LU([1·10−4; 5·10−1])
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({1,1·
10−1,1·10−2,1·10−3,1·10−4}), constant damping: U({no,yes}), maximum CG
iterations: U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({50,75,100,125,150,175,200,225})
–ENGD (full): damping: U({1·10−8,1·10−9,1·10−10,1·10−11,1·10−12,0}),
exponential moving average: U({0,3·10−1,6·10−1,9·10−1}), initialize Gramian
to identity: U({no,yes})
–ENGD (layer-wise): damping: U({1·10−2,1·10−3,1·10−4,1·10−5,1·10−6}),
exponential moving average: U({0,3·10−1,6·10−1,9·10−1,9.9·10−1}), initialize
Gramian to identity: U({no,yes})
–KFAC: damping: LU([1·10−13; 1·10−7]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([0; 9.9·10−1]), initialize Kronecker factors to identity: yes
–KFAC*: damping: LU([1·10−12; 1·10−6]), exponential moving average: U([0; 9.9·
10−1]), initialize Kronecker factors to identity: yes
•5→256→256→128→128→1MLP with D= 116 865
–SGD: learning rate: LU([1·10−3; 1·10−2]), momentum: U({0,3·10−1,6·10−1,9·
10−1})
– Adam: learning rate: LU([1·10−4; 5·10−1])
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({1,1·
10−1,1·10−2,1·10−3,1·10−4}), constant damping: U({no,yes}), maximum CG
iterations: U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({50,75,100,125,150,175,200,225})
–KFAC: damping: LU([1·10−13; 1·10−7]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([0; 9.9·10−1]), initialize Kronecker factors to identity: yes
–KFAC*: damping: LU([1·10−12; 1·10−6]), exponential moving average: U([0; 9.9·
10−1]), initialize Kronecker factors to identity: yes
A.4 10d Poisson Equation
Setup We consider a 10-dimensional Poisson equation −∆u(x) = 0 on the 10-dimensional
unit square x∈[0,1]5with zero right-hand side and harmonic mixed second order polynomial
boundary conditions u(x) =Pd/2
i=1x2i−1x2iforx∈∂[0,1]d. We sample training batches of size
NΩ= 3 000 , N∂Ω= 1000 and evaluate the L2error on a separate set of 30 000 data points using the
known solution u⋆(x) =Pd/2
i=1x2i−1x2i. All optimizers except for KFAC sample a new training
batch each iteration. KFAC only re-samples every 100 iterations because we noticed significant
improvement with multiple iterations on a fixed batch. Each run is limited to 6 000 s. We use a
10→256→256→128→128→1MLP with D= 118 145 MLP whose linear layers are
Tanh-activated except for the final one. Figure A8 visualizes the results.
22(a)
100101102103
Time [s]10−410−310−210−1100L2errorSGD
Adam
Hessian-free
LBFGS
KFAC
100101102103
Time [s]10−810−610−410−2100Loss
(b)
100101102103104105
Iteration10−410−310−210−1100L2error
100101102103104105
Iteration10−810−610−410−2100Loss
Figure A8: Training loss and evaluation L2error for learning the solution to a 10d Poisson equation
over (a) time and (b) steps.
Best run details The runs shown in Figure A8 correspond to the following hyper-parameters:
•SGD: learning rate: 6.550 109 ·10−3, momentum: 9·10−1
•Adam: learning rate: 1.359 480 ·10−4
•Hessian-free: curvature matrix: GGN , initial damping: 1·10−3, constant damping: no,
maximum CG iterations: 250
•LBFGS: learning rate: 2·10−1, history size: 200
•KFAC: damping: 1.056 857 ·10−13, momentum: 7.160 131 ·10−1, exponential moving
average: 9.622 372 ·10−1, initialize Kronecker factors to identity: yes
•KFAC*: damping: 7.978 934 ·10−11, exponential moving average: 8.950 193 ·10−1,
initialize Kronecker factors to identity: yes
Search space details The runs shown in Figure A8 were determined to be the best via a Bayesian
search on the following search spaces which each optimizer given approximately the same total
computational time ( Udenotes a uniform, and LUa log-uniform distribution):
•SGD: learning rate: LU([1·10−3; 1·10−2]), momentum: U({0,3·10−1,6·10−1,9·10−1})
•Adam: learning rate: LU([5e-05 ; 5·10−3])
•Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({1,1·10−1,1·
10−2,1·10−3,1·10−4}), constant damping: U({no,yes}), maximum CG iterations:
U({50,250})
•LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}), history
size:U({50,75,100,125,150,175,200,225})
•KFAC: damping: LU([1·10−14; 1·10−8]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([5·10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
•KFAC*: damping: LU([1·10−12; 1·10−6]), exponential moving average: U([0; 9.9·10−1]),
initialize Kronecker factors to identity: yes
23A.5 5/10/100-d Poisson Equations with Bayesian Search
Setup Here, we consider three Poisson equations −∆u(x) =f(x)with different right-hand sides
and boundary conditions on the unit square x∈[0,1]d:
•d= 5 with cosine sum right-hand side f(x) =π2Pd
i=1cos(πxi), boundary conditions
u(x) =Pd
i=1cos(πxi)forx∈∂[0,1]d, and known solution u⋆(x) =Pd
i=1cos(πxi).
We assign each run a budget of 3 000 s.
•d= 10 with zero right-hand side f(x) = 0 , harmonic mixed second order polynomial
boundary conditions u(x) =Pd/2
i=1x2i−1x2iforx∈∂[0,1]d, and known solution u⋆(x) =Pd/2
i=1x2i−1x2i. We assign each run a budget of 6 000 s.
•d= 100 with constant non-zero right-hand side f(x) =−2d, square norm boundary
conditions u(x) =∥x∥2
2forx∈∂[0,1]d, and known solution u⋆(x) =∥x∥2
2. We assign
each run a budget of 10 000 s.
We tune the optimizer-hyperparameters described in §A.1, as well as the batch sizes NΩ, N∂Ω, and
their associated re-sampling frequencies using Bayesian search. We use five layer MLP architectures
with varying widths whose layers are Tanh-activated except for the final layer. These architectures
are too large to be optimized by ENGD. Figure A9 visualizes the results.
Best run details The runs shown in Figure A9 correspond to the following hyper-parameters:
• 5d Poisson equation, 5→256→256→128→128→1MLP with D= 116 865
–SGD: learning rate: 2.686 653 ·10−4, momentum: 9.878 243 ·10−1,NΩ:606,N∂Ω:
2 001 , batch sampling frequency: 1 570
–Adam: learning rate: 6.111 767 ·10−5,NΩ:534,N∂Ω:1 021 , batch sampling fre-
quency: 8 220
–Hessian-free: curvature matrix: GGN , initial damping: 1.541 954 ·101, constant
damping: no, maximum CG iterations: 358,NΩ:1 084 ,N∂Ω:3 837 , batch sampling
frequency: 230
–LBFGS: learning rate: 1.749 124 ·10−1, history size: 339,NΩ:5 391 ,N∂Ω:4 768 ,
batch sampling frequency: 155
–KFAC: damping: 4.251 462 ·10−10, momentum: 9.198 986 ·10−1, exponential moving
average: 9.737 093 ·10−1, initialize Kronecker factors to identity: yes,NΩ:4 690 ,
N∂Ω:2 708 , batch sampling frequency: 2 369
–KFAC*: damping: 2.240 865 ·10−12, exponential moving average: 8.522 194 ·10−1,
initialize Kronecker factors to identity: yes,NΩ:3 149 ,N∂Ω:3 801 , batch sampling
frequency: 1 393
• 10d Poisson equation, 10→256→256→128→128→1MLP with D= 118 145
–SGD: learning rate: 5.805 516 ·10−2, momentum: 9.715 522 ·10−1,NΩ:537,N∂Ω:
1 173 , batch sampling frequency: 1 083
–Adam: learning rate: 1.337 679 ·10−4,NΩ:115,N∂Ω:1 960 , batch sampling fre-
quency: 4 975
–Hessian-free: curvature matrix: GGN , initial damping: 8.963 629 ·10−1, constant
damping: no, maximum CG iterations: 143,NΩ:3 736 ,N∂Ω:961, batch sampling
frequency: 3
–LBFGS: learning rate: 1.695 334 ·10−1, history size: 338,NΩ:342,N∂Ω:765, batch
sampling frequency: 845
–KFAC: damping: 6.575 415 ·10−4, momentum: 9.772 500 ·10−1, exponential moving
average: 2.745 481 ·10−1, initialize Kronecker factors to identity: yes,NΩ:1 284 ,
N∂Ω:2 258 , batch sampling frequency: 455
–KFAC*: damping: 7.530 350 ·10−12, exponential moving average: 9.648 138 ·10−1,
initialize Kronecker factors to identity: yes,NΩ:1 090 ,N∂Ω:1 930 , batch sampling
frequency: 2 454
• 100d Poisson equation, 100→768→768→512→512→1MLP with D= 1 325 057
24(a)
100101102103
Time [s]10−310−1101L2error5d Poisson,D= 116,865
100101102103
Time [s]10−310−110d Poisson, D= 118,145
100102104
Time [s]10−210−1100100d Poisson, D= 1,325,057
SGD Adam Hessian-free LBFGS KFAC KFAC*
100101102103
Time [s]10−510−1103Loss5d Poisson,D= 116,865
100101102103
Time [s]10−1010−610−210d Poisson, D= 118,145
100102104
Time [s]10−510−310−1101100d Poisson, D= 1,325,057
SGD Adam Hessian-free LBFGS KFAC KFAC*
(b)
101103105
Iteration10−310−1101L2error5d Poisson,D= 116,865
101103105
Iteration10−310−110d Poisson, D= 118,145
101103
Iteration10−210−1100100d Poisson, D= 1,325,057
SGD Adam Hessian-free LBFGS KFAC KFAC*
101103105
Iteration10−510−1103Loss5d Poisson,D= 116,865
101103105
Iteration10−1010−610−210d Poisson, D= 118,145
101103
Iteration10−510−310−1101100d Poisson, D= 1,325,057
SGD Adam Hessian-free LBFGS KFAC KFAC*
Figure A9: Training loss and evaluation L2error for learning the solution to high-dimensional
Poisson equations over (a) time and (b) steps using Bayesian search.
–SGD: learning rate: 1.450 764 ·10−3, momentum: 9.747 671 ·10−1,NΩ:177,N∂Ω:
2 422 , batch sampling frequency: 519
–Adam: learning rate: 7.894 685 ·10−5,NΩ:100,N∂Ω:601, batch sampling frequency:
19
–Hessian-free: curvature matrix: GGN , initial damping: 7.705 318 ·10−4, constant
damping: no, maximum CG iterations: 263,NΩ:108,N∂Ω:2 372 , batch sampling
frequency: 55
–LBFGS: learning rate: 1.797 096 ·10−1, history size: 112,NΩ:2 115 ,N∂Ω:1 852 ,
batch sampling frequency: 23
–KFAC: damping: 9.724 117 ·10−3, momentum: 5.015 715 ·10−1, exponential moving
average: 9.200 952 ·10−1, initialize Kronecker factors to identity: yes,NΩ:124,N∂Ω:
2 332 , batch sampling frequency: 322
–KFAC*: damping: 1.236 763 ·10−7, exponential moving average: 8.302 663 ·10−1,
initialize Kronecker factors to identity: yes,NΩ:175,N∂Ω:2 086 , batch sampling
frequency: 16
25Search space details The runs shown in Figure A9 were determined to be the best via a Bayesian
search on the following search spaces which each optimizer given approximately the same total
computational time ( Udenotes a uniform, and LUa log-uniform distribution):
• 5d Poisson equation, 5→256→256→128→128→1MLP with D= 116 865
–SGD: learning rate: LU([1·10−6; 1]), momentum: U([0; 9.9·10−1]),NΩ:
U({100,101,. . .,10 000 }),N∂Ω:U({50,51,. . .,5 000}), batch sampling frequency:
U({0,1,. . .,10 000 })
–Adam: learning rate: LU([1·10−6; 1]),NΩ:U({100,101,. . .,10 000 }),N∂Ω:
U({50,51,. . .,5 000}), batch sampling frequency: U({0,1,. . .,10 000 })
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping:
LU([1·10−15; 1]), constant damping: U({no,yes}), maximum CG iterations:
U({1,2,. . .,500}),NΩ:U({100,101,. . .,10 000 }),N∂Ω:U({50,51,. . .,5 000}),
batch sampling frequency: U({0,1,. . .,10 000 })
–LBFGS: learning rate: LU([1·10−6; 1]), history size: U({5,6,. . .,500}),NΩ:
U({100,101,. . .,10 000 }),N∂Ω:U({50,51,. . .,5 000}), batch sampling frequency:
U({0,1,. . .,10 000 })
–KFAC: damping: LU([1·10−15; 1·10−2]), momentum: U([0; 9.9·10−1]), expo-
nential moving average: U([0; 9.9·10−1]), initialize Kronecker factors to identity:
U({no,yes}),NΩ:U({100,101,. . .,10 000 }),N∂Ω:U({50,51,. . .,5 000}), batch
sampling frequency: U({0,1,. . .,10 000 })
–KFAC*: damping: LU([1·10−15; 1·10−2]), exponential moving average:
U([0; 9.9·10−1]), initialize Kronecker factors to identity: U({no,yes}),NΩ:
U({100,101,. . .,10 000 }),N∂Ω:U({50,51,. . .,5 000}), batch sampling frequency:
U({0,1,. . .,10 000 })
• 10d Poisson equation, 10→256→256→128→128→1MLP with D= 118 145
–SGD: learning rate: LU([1·10−6; 1]), momentum: U([0; 9.9·10−1]),NΩ:
U({100,101,. . .,5 000}),N∂Ω:U({50,51,. . .,2 500}), batch sampling frequency:
U({0,1,. . .,5 000})
–Adam: learning rate: LU([1·10−6; 1]),NΩ:U({100,101,. . .,5 000}),N∂Ω:
U({50,51,. . .,2 500}), batch sampling frequency: U({0,1,. . .,5 000})
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping:
LU([1·10−15; 1]), constant damping: U({no,yes}), maximum CG iterations:
U({1,2,. . .,500}),NΩ:U({100,101,. . .,5 000}),N∂Ω:U({50,51,. . .,2 500}),
batch sampling frequency: U({0,1,. . .,5 000})
–LBFGS: learning rate: LU([1·10−6; 1]), history size: U({5,6,. . .,500}),NΩ:
U({100,101,. . .,5 000}),N∂Ω:U({50,51,. . .,2 500}), batch sampling frequency:
U({0,1,. . .,5 000})
–KFAC: damping: LU([1·10−15; 1·10−2]), momentum: U([0; 9.9·10−1]), expo-
nential moving average: U([0; 9.9·10−1]), initialize Kronecker factors to identity:
U({no,yes}),NΩ:U({100,101,. . .,5 000}),N∂Ω:U({50,51,. . .,2 500}), batch
sampling frequency: U({0,1,. . .,5 000})
–KFAC*: damping: LU([1·10−15; 1·10−2]), exponential moving average:
U([0; 9.9·10−1]), initialize Kronecker factors to identity: U({no,yes}),NΩ:
U({100,101,. . .,5 000}),N∂Ω:U({50,51,. . .,2 500}), batch sampling frequency:
U({0,1,. . .,5 000})
• 100d Poisson equation, 100→768→768→512→512→1MLP with D= 1 325 057
–SGD: learning rate: LU([1·10−6; 1]), momentum: U([0; 9.9·10−1]),NΩ:
U({100,101,. . .,5 000}),N∂Ω:U({50,51,. . .,2 500}), batch sampling frequency:
U({0,1,. . .,1 000})
–Adam: learning rate: LU([1·10−6; 1]),NΩ:U({100,101,. . .,5 000}),N∂Ω:
U({50,51,. . .,2 500}), batch sampling frequency: U({0,1,. . .,1 000})
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping:
LU([1·10−15; 1]), constant damping: U({no,yes}), maximum CG iterations:
U({1,2,. . .,500}),NΩ:U({100,101,. . .,5 000}),N∂Ω:U({50,51,. . .,2 500}),
batch sampling frequency: U({0,1,. . .,1 000})
26–LBFGS: learning rate: LU([1·10−6; 1]), history size: U({5,6,. . .,500}),NΩ:
U({100,101,. . .,5 000}),N∂Ω:U({50,51,. . .,2 500}), batch sampling frequency:
U({0,1,. . .,1 000})
–KFAC: damping: LU([1·10−15; 1·10−2]), momentum: U([0; 9.9·10−1]), expo-
nential moving average: U([0; 9.9·10−1]), initialize Kronecker factors to identity:
U({no,yes}),NΩ:U({100,101,. . .,5 000}),N∂Ω:U({50,51,. . .,2 500}), batch
sampling frequency: U({0,1,. . .,1 000})
–KFAC*: damping: LU([1·10−15; 1·10−2]), exponential moving average:
U([0; 9.9·10−1]), initialize Kronecker factors to identity: U({no,yes}),NΩ:
U({100,101,. . .,5 000}),N∂Ω:U({50,51,. . .,2 500}), batch sampling frequency:
U({0,1,. . .,1 000})
A.6 PINN Loss for the Heat Equation
Consider the (˜d+ 1) -dimensional homogeneous heat equation
∂tu(t,˜x)−κ∆˜xu(t,˜x) = 0
with spatial coordinates ˜x∈Ω⊆R˜dand time coordinate t∈T⊆Rwhere Tis a time interval
andκ >0denotes the heat conductivity. In this case, our neural network processes a (d=˜d+ 1)-
dimensional vector x= (t,˜x⊤)⊤∈Rdand we can re-write the heat equation as
∂x1u(x)−κdX
d=2∆xdu(x) = 0 .
In the following, we consider the unit time interval T = [0; 1] , the unit square Ω = [0; 1]˜dand set
κ=1/4. There are two types of constraints we need to enforce on the heat equation in order to
obtain unique solutions: initial conditions and boundary conditions. As our framework for the KFAC
approximation assumes only two terms in the loss function, we combine the contributions from the
boundary and initial values into one term.
To make this more precise, consider the following example solution of the heat equation, which will
be used later on as well. As initial conditions, we use u0(˜x) =u(0,˜x) =Q˜d
i=1sin(π˜xi)for˜x∈Ω.
For boundary conditions, we use g(t,˜x) = 0 for(t,˜x)∈T×∂Ω. The manufactured solution is
u⋆(t,˜x) = exp 
−π2˜dt
4!˜dY
i=1sin(π[˜xi]).
The PINN loss for this problem consists of three terms: a PDE term, an initial value condition term,
and a spatial boundary condition term,
L(θ) =1
NΩNΩX
n=1
∂tuθ(xΩ
n)−1
4∆˜xnuθ(xΩ
n)2
+1
N∂ΩN∂ΩX
n=1 
uθ(x∂Ω
n)−g(x∂Ω
n)2
+1
N0N0X
n=1 
uθ(0,x0
n)−u0(x0
n)2
withxΩ
n∼T×Ω, andx∂Ω
n∼T×∂Ω, andx0
n∼ {0}×Ω. To fit this loss into our framework which
assumes two loss terms, each of whose curvature is approximated with a Kronecker factor, we combine
the initial value and boundary value conditions into a single term. Assuming N∂Ω=N0=Ncond/2
without loss of generality, we write
L(θ) =1
NΩNΩX
n=1∂tuθ(xΩ
n)−1
4∆˜xnuθ(xΩ
n)−yΩ
n2
2| {z }
LΩ(θ)+1
NcondNcondX
n=1uθ(xcond
n)−ycond
n2
2
| {z }
Lcond(θ)
27with domain inputs xΩ
n∼T×Ωand targets yΩ
n= 0, boundary and initial condition targets
ycond
n=u⋆(xcond
n)with initial inputs xcond
n∼ {0} ×Ωforn= 1, . . . , Ncond/2and boundary inputs
xcond
n∼T×∂Ωforn=Ncond/2+ 1, . . . , N cond. This loss has the same structure as the PINN loss in
Equation (1).
A.7 1+1d Heat Equation
Setup We consider a 1+1-dimensional heat equation ∂tu(t, x)−κ∆xu(t, x) = 0 withκ=1/4on
the unit square and unit time interval, x, t∈[0,1]×[0,1]. The equation has zero spatial boundary
conditions and the initial values are given by u(0, x) = sin( πx)forx∈[0,1]. We sample a
single training batch of size NΩ= 900 , N∂Ω= 120 (N∂Ω/2points for the initial value and spatial
boundary conditions each) and evaluate the L2error on a separate set of 9 000 data points using
the known solution u⋆(t, x) = exp( −π2t/4) sin(πx). Each run is limited to 1 000 s. We compare
three MLP architectures of increasing size, each of whose linear layers are Tanh-activated except
for the final one: a shallow 2→64→1MLP with D= 257 trainable parameters, a five layer
2→64→64→48→48→1MLP with D= 9 873 trainable parameters, and a five layer
2→256→256→128→128→1MLP with D= 116 097 trainable parameters. For the biggest
architecture, full and layer-wise ENGD lead to out-of-memory errors and are thus not part of the
experiments. Figure Figure A10 summarizes the results, and Figure A11 illustrates the learned
solutions over training for all optimizers on the shallow MLP
Best run details The runs shown in Figure A10 correspond to the following hyper-parameters:
•2→64→1MLP with D= 257
– SGD: learning rate: 1.752 752 ·10−2, momentum: 9.9·10−1
– Adam: learning rate: 8.629 006 ·10−4
–Hessian-free: curvature matrix: GGN , initial damping: 1·10−4, constant damping:
no, maximum CG iterations: 350
– LBFGS: learning rate: 1·10−1, history size: 125
–ENGD (full): damping: 1·10−12, exponential moving average: 9·10−1, initialize
Gramian to identity: no
–ENGD (layer-wise): damping: 1·10−10, exponential moving average: 3·10−1,
initialize Gramian to identity: no
–KFAC: damping: 1.273 754 ·10−8, momentum: 7.562 617 ·10−1, exponential moving
average: 3.611 724 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 1.968 427 ·10−9, exponential moving average: 9.703 638 ·10−1,
initialize Kronecker factors to identity: yes
•2→64→64→48→48→1MLP with D= 9 873
– SGD: learning rate: 9.276 977 ·10−2, momentum: 9.9·10−1
– Adam: learning rate: 2.551 515 ·10−3
–Hessian-free: curvature matrix: GGN , initial damping: 1·10−3, constant damping:
no, maximum CG iterations: 200
– LBFGS: learning rate: 2·10−1, history size: 125
–ENGD (full): damping: 1·10−6, exponential moving average: 9·10−1, initialize
Gramian to identity: no
–ENGD (layer-wise): damping: 1·10−8, exponential moving average: 6·10−1, initialize
Gramian to identity: no
–KFAC: damping: 3.169 186 ·10−13, momentum: 7.075 879 ·10−1, exponential moving
average: 8.860 410 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 5.035 695 ·10−14, exponential moving average: 9.815 164 ·10−1,
initialize Kronecker factors to identity: yes
•2→256→256→128→128→1MLP with D= 116 097
– SGD: learning rate: 5.709 474 ·10−2, momentum: 9.9·10−1
– Adam: learning rate: 6.716 485 ·10−4
28(a)
100101102103
Time [s]10−610−410−2L2errorD= 257
100101102103
Time [s]10−610−410−2D= 9,873
100101102103
Time [s]10−710−510−310−1D= 116,097
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC
100101102103
Time [s]10−1110−710−3LossD= 257
100101102103
Time [s]10−1110−710−3D= 9,873
100101102103
Time [s]10−1210−810−4D= 116,097
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC
(b)
101103105
Iteration10−610−410−2L2errorD= 257
101103105
Iteration10−610−410−2D= 9,873
101103105
Iteration10−710−510−310−1D= 116,097
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC
101103105
Iteration10−1110−710−3LossD= 257
101103105
Iteration10−1110−710−3D= 9,873
101103105
Iteration10−1210−810−4D= 116,097
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC
Figure A10: training loss and evaluation L2error for learning the solution to a 1+1-dimensional heat
equation over (a) time and (b). each column corresponds to a different neural network.
–Hessian-free: curvature matrix: GGN , initial damping: 1·10−2, constant damping:
no, maximum CG iterations: 300
– LBFGS: learning rate: 2·10−1, history size: 125
–KFAC: damping: 2.576 488 ·10−13, momentum: 2.043 395 ·10−2, exponential moving
average: 9.727 829 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 7.343 493 ·10−11, exponential moving average: 9.765 844 ·10−1,
initialize Kronecker factors to identity: yes
Search space details The runs shown in Figure A10 were determined to be the best via a search
with approximately 50 runs on the following search spaces which were obtained by refining an
initially wider search ( Udenotes a uniform, and LUa log-uniform distribution):
•2→64→1MLP with D= 257
–SGD: learning rate: LU([1·10−3; 1·10−1]), momentum: U({0,3·10−1,6·10−1,9·
10−1,9.9·10−1})
– Adam: learning rate: LU([5·10−4; 1·10−1])
29Optimizer First step 0.1% trained 1% trained 10% trained True solution
SGD
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.6·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 277, Loss: 2.5·10−2,L2loss: 1.0·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 3000 , Loss: 6.1·10−4,L2loss: 1.4·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 29540 , Loss: 2.2·10−5,L2loss: 1.4·10−3
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.6·10−1
Adam
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.7·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 277, Loss: 5.5·10−2,L2loss: 1.7·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 2727 , Loss: 7.1·10−4,L2loss: 1.4·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 26855 , Loss: 6.8·10−6,L2loss: 7.3·10−4
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.7·10−1
LBFGS
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.1·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 55, Loss: 1.2·10−5,L2loss: 1.0·10−3
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 594, Loss: 2.8·10−6,L2loss: 2.7·10−4
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 5845 , Loss: 2.8·10−6,L2loss: 2.7·10−4
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.1·10−1
Hessian-free
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 3.4·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 1, Loss: 1.2·10−1,L2loss: 3.5·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 11, Loss: 7.5·10−3,L2loss: 8.6·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 107, Loss: 4.6·10−8,L2loss: 7.8·10−5
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 3.4·10−1
ENGD (full)
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 3.5·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 4, Loss: 1.1·10−1,L2loss: 3.2·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 42, Loss: 2.3·10−3,L2loss: 2.1·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 446, Loss: 4.9·10−12,L2loss: 6.6·10−7
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 3.5·10−1
ENGD
(layer-wise)
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.6·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 4, Loss: 1.0·10−1,L2loss: 2.7·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 42, Loss: 4.2·10−2,L2loss: 1.5·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 446, Loss: 1.8·10−4,L2loss: 1.1·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.6·10−1
KFAC
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.1·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 14, Loss: 1.8·10−2,L2loss: 7.5·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 143, Loss: 2.5·10−3,L2loss: 2.5·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 1400 , Loss: 4.7·10−7,L2loss: 3.2·10−4
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.1·10−1
KFAC*
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.7·10−1
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 34, Loss: 1.4·10−2,L2loss: 7.6·10−2
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 335, Loss: 1.5·10−5,L2loss: 7.9·10−4
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 3299 , Loss: 5.3·10−9,L2loss: 2.3·10−5
0.0 0.2 0.4 0.6 0.8 1.0
x0.00.20.40.60.81.0tNormalized learned solution
0.0 0.2 0.4 0.6 0.8 1.0
xNormalized true solutionStep: 0, Loss: 1.4·10−1,L2loss: 2.7·10−1
Figure A11: Visual comparison learned and true solutions while training with different optimizers for
the 1+1d heat equation using a two-layer MLP (corresponding to the curves in Figure A10 left). All
functions are shown on the unit square (x, t)∈Ω = [0; 1]2and normalized to the unit interval.
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({100,1,1·
10−2,1·10−4,1·10−6}), constant damping: U({no,yes}), maximum CG iterations:
U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({75,100,125,150,175,200,225,250})
–ENGD (full): damping: U({1·10−6,1·10−8,1·10−10,1·10−12,0}), exponential
moving average: U({0,3·10−1,6·10−1,9·10−1,9.9·10−1}), initialize Gramian to
identity: U({no,yes})
30–ENGD (layer-wise): damping: U({1·10−4,1·10−6,1·10−8,1·10−10,0}), exponential
moving average: U({0,3·10−1,6·10−1,9·10−1,9.9·10−1}), initialize Gramian to
identity: U({no,yes})
–KFAC: damping: LU([1·10−13; 1·10−7]), momentum: U([5·10−1; 9.9·10−1]),
exponential moving average: U([0; 9.9·10−1]), initialize Kronecker factors to identity:
yes
–KFAC*: damping: LU([1·10−13; 1·10−7]), exponential moving average: U([5·
10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
•2→64→64→48→48→1MLP with D= 9 873
–SGD: learning rate: LU([1·10−3; 1·10−1]), momentum: U({0,3·10−1,6·10−1,9·
10−1,9.9·10−1})
– Adam: learning rate: LU([5·10−4; 1·10−1])
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({100,1,1·
10−2,1·10−4,1·10−6}), constant damping: U({no,yes}), maximum CG iterations:
U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({75,100,125,150,175,200,225,250})
–ENGD (full): damping: U({1·10−6,1·10−8,1·10−10,1·10−12,0}), exponential
moving average: U({0,3·10−1,6·10−1,9·10−1,9.9·10−1}), initialize Gramian to
identity: U({no,yes})
–ENGD (layer-wise): damping: U({1·10−4,1·10−6,1·10−8,1·10−10,0}), exponential
moving average: U({0,3·10−1,6·10−1,9·10−1,9.9·10−1}), initialize Gramian to
identity: U({no,yes})
–KFAC: damping: LU([1·10−13; 1·10−7]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([5·10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
–KFAC*: damping: LU([1·10−15; 1·10−9]), exponential moving average: U([5·
10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
•2→256→256→128→128→1MLP with D= 116 097
–SGD: learning rate: LU([1·10−3; 1·10−1]), momentum: U({0,3·10−1,6·10−1,9·
10−1,9.9·10−1})
– Adam: learning rate: LU([5·10−4; 1·10−1])
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({100,1,1·
10−2,1·10−4,1·10−6}), constant damping: U({no,yes}), maximum CG iterations:
U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({75,100,125,150,175,200,225,250})
–KFAC: damping: LU([1·10−14; 1·10−7]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([5·10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
–KFAC*: damping: LU([1·10−14; 1·10−6]), exponential moving average: U([5·
10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
A.8 4+1d Heat Equation
Setup We consider a 4+1-dimensional heat equation ∂tu(t,x)−κ∆xu(t,x) = 0 withκ=1/4
on the four-dimensional unit square and unit time interval, x, t∈[0,1]4×[0,1]. The equation has
spatial boundary conditions u(t, x) = exp( −t)P4
i=1sin(2xi)fort,x∈[0,1]×∂[0,1]4throughout
time, and initial value conditions u(0,x) =P4
i=1sin(2xi)forx∈[0,1]4. We sample training
batches of size NΩ= 3 000 , N∂Ω= 500 (N∂Ω/2points for the initial value and spatial boundary
conditions each) and evaluate the L2error on a separate set of 30 000 data points using the known
solution u⋆(t,x) = exp( −t)P4
i=1sin(2xi). All optimizers except for KFAC sample a new training
batch each iteration. KFAC only re-samples every 100 iterations because we noticed significant
improvement with multiple iterations on a fixed batch. To make sure that this does not lead to
an unfair advantage of KFAC, we conduct an additional experiment where we also tune the batch
sampling frequency, as well as other hyper-parameters; see §A.10. The results presented in this
section are consistent with this additional experiment (compare the rightmost column of Figure A12
31and Figure A14). Each run is limited to 3000 s. We compare three MLP architectures of increasing
size, each of whose linear layers are Tanh-activated except for the final one: a shallow 5→64→1
MLP with D= 449 trainable weights, a five layer 5→64→64→48→48→1MLP with
D= 10 065 trainable weights, and a five layer 5→256→256→128→128→1MLP with
D= 116 864 trainable weights. For the biggest architecture, full and layer-wise ENGD lead to
out-of-memory errors and are thus not tested. Figure A12 visualizes the results.
(a)
100101102103
Time [s]10−510−310−1L2errorD= 449
100101102103
Time [s]10−410−2100D= 10,065
100101102103
Time [s]10−410−2100D= 116,865
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
100101102103
Time [s]10−910−510−1LossD= 449
100101102103
Time [s]10−710−3101D= 10,065
100101102103
Time [s]10−810−510−2D= 116,865
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
(b)
101103105
Iteration10−510−310−1L2errorD= 449
101103105
Iteration10−410−2100D= 10,065
101103
Iteration10−410−2100D= 116,865
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
101103105
Iteration10−910−510−1LossD= 449
101103105
Iteration10−710−3101D= 10,065
101103
Iteration10−810−510−2D= 116,865
SGD Adam Hessian-free LBFGS ENGD (full) ENGD (layer-wise) KFAC KFAC*
Figure A12: Training loss and evaluation L2error for learning the solution to a 4+1-d heat equation
over (a) time and (b) steps. Columns are different neural networks.
Search space details The runs shown in Figure A12 were determined to be the best via a search
with approximately 50 runs on the following search spaces which were obtained by refining an
initially wider search ( Udenotes a uniform, and LUa log-uniform distribution):
•5→64→1MLP with D= 449
– SGD: learning rate: 7.737 742 ·10−3, momentum: 9·10−1
– Adam: learning rate: 3.708 460 ·10−3
–Hessian-free: curvature matrix: GGN , initial damping: 2·10−1, constant damping:
no, maximum CG iterations: 300
– LBFGS: learning rate: 2·10−1, history size: 175
–ENGD (full): damping: 1·10−10, exponential moving average: 6·10−1, initialize
Gramian to identity: yes
32–ENGD (layer-wise): damping: 1·10−6, exponential moving average: 0, initialize
Gramian to identity: yes
–KFAC: damping: 1.000 288 ·10−9, momentum: 9.474 108 ·10−1, exponential moving
average: 7.783 519 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 2.965 060 ·10−8, exponential moving average: 9.574 717 ·10−1,
initialize Kronecker factors to identity: yes
•5→64→64→48→48→1MLP with D= 10 065
– SGD: learning rate: 9.357 973 ·10−3, momentum: 9·10−1
– Adam: learning rate: 7.801 748 ·10−4
–Hessian-free: curvature matrix: GGN , initial damping: 5·10−3, constant damping:
no, maximum CG iterations: 400
– LBFGS: learning rate: 1·10−1, history size: 225
–ENGD (full): damping: 1·10−8, exponential moving average: 0, initialize Gramian
to identity: yes
–ENGD (layer-wise): damping: 1·10−6, exponential moving average: 3·10−1, initialize
Gramian to identity: no
–KFAC: damping: 4.143 385 ·10−14, momentum: 7.660 303 ·10−1, exponential moving
average: 9.821 414 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 1.955 740 ·10−10, exponential moving average: 9.821 778 ·10−1,
initialize Kronecker factors to identity: yes
•5→256→256→128→128→1MLP with D= 116 865
– SGD: learning rate: 7.192 473 ·10−3, momentum: 9·10−1
– Adam: learning rate: 5.266 284 ·10−4
–Hessian-free: curvature matrix: GGN , initial damping: 2·10−3, constant damping:
no, maximum CG iterations: 250
– LBFGS: learning rate: 2·10−1, history size: 200
–KFAC: damping: 8.581 322 ·10−13, momentum: 8.501 747 ·10−1, exponential moving
average: 9.803 115 ·10−1, initialize Kronecker factors to identity: yes
–KFAC*: damping: 3.405 440 ·10−14, exponential moving average: 8.445 471 ·10−1,
initialize Kronecker factors to identity: yes
Search space details The runs shown in Figure A12 were determined to be the best via a search
with approximately 50 runs on the following search spaces which were obtained by refining an
initially wider search ( Udenotes a uniform, and LUa log-uniform distribution):
•5→64→1MLP with D= 449
–SGD: learning rate: LU([1·10−3; 1·10−2]), momentum: U({0,3·10−1,6·10−1,9·
10−1})
– Adam: learning rate: LU([5·10−4; 1·10−1])
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({1,1·
10−1,1·10−2,1·10−3,1·10−4}), constant damping: U({no,yes}), maximum CG
iterations: U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({50,75,100,125,150,175,200,225})
–ENGD (full): damping: U({1·10−8,1·10−9,1·10−10,1·10−11,1·10−12,0}),
exponential moving average: U({0,3·10−1,6·10−1,9·10−1}), initialize Gramian
to identity: U({no,yes})
–ENGD (layer-wise): damping: U({1·10−2,1·10−3,1·10−4,1·10−5,1·10−6}),
exponential moving average: U({0,3·10−1,6·10−1,9·10−1,9.9·10−1}), initialize
Gramian to identity: U({no,yes})
–KFAC: damping: LU([1·10−12; 1·10−6]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([5·10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
–KFAC*: damping: LU([1·10−13; 1·10−7]), exponential moving average: U([5·
10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
33•5→64→64→48→48→1MLP with D= 10 065
–SGD: learning rate: LU([1·10−3; 1·10−2]), momentum: U({0,3·10−1,6·10−1,9·
10−1})
– Adam: learning rate: LU([5·10−4; 1·10−1])
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({1,1·
10−1,1·10−2,1·10−3,1·10−4}), constant damping: U({no,yes}), maximum CG
iterations: U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({50,75,100,125,150,175,200,225})
–ENGD (full): damping: U({1·10−8,1·10−9,1·10−10,1·10−11,1·10−12,0}),
exponential moving average: U({0,3·10−1,6·10−1,9·10−1}), initialize Gramian
to identity: U({no,yes})
–ENGD (layer-wise): damping: U({1·10−2,1·10−3,1·10−4,1·10−5,1·10−6}),
exponential moving average: U({0,3·10−1,6·10−1,9·10−1,9.9·10−1}), initialize
Gramian to identity: U({no,yes})
–KFAC: damping: LU([1·10−14; 1·10−8]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([5·10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
–KFAC*: damping: LU([1·10−14; 1·10−8]), exponential moving average: U([5·
10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
•5→256→256→128→128→1MLP with D= 116 865
–SGD: learning rate: LU([1·10−3; 1·10−2]), momentum: U({0,3·10−1,6·10−1,9·
10−1})
– Adam: learning rate: LU([5·10−4; 1·10−1])
–Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({1,1·
10−1,1·10−2,1·10−3,1·10−4}), constant damping: U({no,yes}), maximum CG
iterations: U({50,250})
–LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}),
history size: U({50,75,100,125,150,175,200,225})
–KFAC: damping: LU([1·10−14; 1·10−8]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([5·10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
–KFAC*: damping: LU([1·10−14; 1·10−8]), exponential moving average: U([5·
10−1; 9.9·10−1]), initialize Kronecker factors to identity: yes
A.9 Robustness Under Model Initialization for 4+1d Heat Equation
Here we study the robustness of our results from §A.8 for the 4+1d heat equation when initializing
the neural network differently. We choose the MLP with D= 10 065 parameters from Figure 2’s
middle panel which is bigger than the two-layer toy model, while still allowing to run ENGD. Using
the same hyper-parameters, we re-run all optimizers with 10 different model initializations. The
results are shown in Figure A13. We observe that all optimizers perform similar to Figure 2, except
for LBFGS which diverges for some runs.
A.10 4+1d Heat Equation with Bayesian Search
Setup We consider the same heat equation as in §A.8 and use the 5→256→256→128→
128→1MLP with D= 116 865 . We tune all optimizer hyper-parameters as described in §A.1 and
also tune the batch sizes NΩ, N∂Ω, as well as their re-sampling frequencies. Figure A14 summarizes
the results.
Best run details The runs shown in Figure A14 correspond to the following hyper-parameters:
•SGD: learning rate: 1.614 965 ·10−2, momentum: 9.899 167 ·10−1,NΩ:527,N∂Ω:2 157 ,
batch sampling frequency: 543
•Adam: learning rate: 2.583 569 ·10−4,NΩ:472,N∂Ω:3 018 , batch sampling frequency:
177
34100101102103
Time [s]10−510−410−310−210−1100101L2error4d Heat (mlp-tanh-64-64-48-48, D= 10065 )
SGD
Adam
Hessian-free
LBFGS
ENGD (full)
ENGD (layer-wise)
ENGD (diagonal)
KFAC
KFAC*Figure A13: Best runs from the MLP with 10 065 parameters on the 4+1d heat equation from Figure 2
middle repeated over 10 different model initializations. All optimizers perform similarly, except for
LBFGS which diverges for some runs.
(a)
100101102103
Time [s]10−510−310−1L2errorSGD
Adam
Hessian-free
LBFGS
KFAC
KFAC*
100101102103
Time [s]10−1110−810−510−2101Loss
(b)
100101102103104105
Iteration10−510−310−1L2error
100101102103104105
Iteration10−1110−810−510−2101Loss
Figure A14: Training loss and evaluation L2error for learning the solution to a 4+1-dimensional heat
equation over (a) time and (b) using Bayesian search.
•Hessian-free: curvature matrix: GGN , initial damping: 5.077 634 ·10−4, constant damping:
yes, maximum CG iterations: 163,NΩ:1 172 ,N∂Ω:1 637 , batch sampling frequency:
5 440
35•LBFGS: learning rate: 1.029 194 ·10−1, history size: 488,NΩ:582,N∂Ω:2 038 , batch
sampling frequency: 315
•KFAC: damping: 8.435 180 ·10−14, momentum: 9.718 645 ·10−1, exponential moving
average: 9.800 744 ·10−1, initialize Kronecker factors to identity: yes,NΩ:2 525 ,N∂Ω:
2 663 , batch sampling frequency: 7 916
•KFAC*: damping: 8.837 871 ·10−15, exponential moving average: 9.887 596 ·10−1,
initialize Kronecker factors to identity: yes,NΩ:2 563 ,N∂Ω:2 873 , batch sampling
frequency: 9 647
Search space details The runs shown in Figure A14 were determined to be the best via a Bayesian
search on the following search spaces which each optimizer given approximately the same total
computational time ( Udenotes a uniform, and LUa log-uniform distribution):
•SGD: learning rate: LU([1·10−6; 1]), momentum: U([0; 9.9·10−1]),NΩ:
U({100,101,. . .,10 000 }),N∂Ω:U({50,51,. . .,5 000}), batch sampling frequency:
U({0,1,. . .,10 000 })
•Adam: learning rate: LU([1·10−6; 1]),NΩ:U({100,101,. . .,10 000 }),N∂Ω:
U({50,51,. . .,5 000}), batch sampling frequency: U({0,1,. . .,10 000 })
•Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: LU([1·10−15; 1]),
constant damping: U({no,yes}), maximum CG iterations: U({1,2,. . .,500}),NΩ:
U({100,101,. . .,10 000 }),N∂Ω:U({50,51,. . .,5 000}), batch sampling frequency:
U({0,1,. . .,10 000 })
•LBFGS: learning rate: LU([1·10−6; 1]), history size: U({5,6,. . .,500}),NΩ:
U({100,101,. . .,10 000 }),N∂Ω:U({50,51,. . .,5 000}), batch sampling frequency:
U({0,1,. . .,10 000 })
•KFAC: damping: LU([1·10−15; 1·10−2]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([0; 9.9·10−1]), initialize Kronecker factors to identity: U({no,yes}),
NΩ:U({100,101,. . .,10 000 }),N∂Ω:U({50,51,. . .,5 000}), batch sampling frequency:
U({0,1,. . .,10 000 })
•KFAC*: damping: LU([1·10−15; 1·10−2]), exponential moving average: U([0; 9.9·10−1]),
initialize Kronecker factors to identity: U({no,yes}),NΩ:U({100,101,. . .,10 000 }),
N∂Ω:U({50,51,. . .,5 000}), batch sampling frequency: U({0,1,. . .,10 000 })
A.11 9+1-d Logarithmic Fokker-Planck Equation with Random Search
For a given drift µ: [0,1]×Rd→Rdand diffusivity σ: [0,1]→Rd×dthe Fokker-Planck equation
with initial probability density p0is given by
∂tp+ div( µp)−1
2Tr(σσ⊤∇2p) = 0 , p(0) = p0,
which is posed on [0,1]×Rd. Note that p(t,·)is a probability density on Rdfor all t∈[0,1]. We
transform the above equation into logarithmic space via q= log( p). Then qsolves
∂tq+ div( µ) +∇q·µ−1
2∥σ⊤∇q∥2−1
2tr(σσ⊤∇2q) = 0 , q(0) = log p0.
For the concrete example of the main text, we set µ(t,x) =−1
2xandσ=√
2I∈Rd×d. We
consider a 9+1 dimensional Fokker-Planck equation in logarithmic space and replace the unbounded
domain by [0,1]×[−5,5]d. Precisely, we aim to solve the equation
∂tq(t,x)−d
2−1
2∇q(t,x)·x− ∥∇ q(t,x)∥2−∆q(t,x) = 0 , q(0) = log( p∗(0)),
where d= 9,t∈[0,1]andx∈[−5,5]. The solution q∗= log( p∗)is given as p∗(t,x)∼
N(0,exp(−t)I+ (1−exp(−t))2I). The PINN loss includes the PDE residual and the initial
conditions. We model the solution with a medium sized tanh-activated MLP with D= 118 145
and the layer structure 10→256→256→128→128→1and use batch sizes of NΩ= 3 000 ,
N∂Ω= 1 000 . Each run is assigned a budget of 6 000 s. Figure A15 visualizes the results.
36(a)
100101102103
Time [s]10−1100101L2errorSGD
Adam
Hessian-free
LBFGS
KFAC
KFAC*
100101102103
Time [s]10−2100102Loss
(b)
100101102103104
Iteration10−1100101L2error
100101102103104
Iteration10−2100102Loss
Figure A15: Training loss and evaluation L2error for learning the solution to a (9+1)d log Fokker-
Planck equation over (a) time and (b) steps.
Search space details The runs shown in Figure A15 were determined to be the best via a random
search on the following search spaces which each optimizer given approximately the same total
computational time ( Udenotes a uniform, and LUa log-uniform distribution):
•SGD: learning rate: LU([1·10−3; 1·10−2]), momentum: U({0,3·10−1,6·10−1,9·10−1})
•Adam: learning rate: LU([5e-05 ; 5·10−3])
•Hessian-free: curvature matrix: U({GGN,Hessian }), initial damping: U({1,1·10−1,1·
10−2,1·10−3,1·10−4}), constant damping: U({no,yes}), maximum CG iterations:
U({50,250})
•LBFGS: learning rate: U({5·10−1,2·10−1,1·10−1,5·10−2,2·10−2,1·10−2}), history
size:U({50,75,100,125,150,175,200,225})
•KFAC: damping: LU([1·10−11; 1·10−5]), momentum: U([0; 9.9·10−1]), exponential
moving average: U([9.99·10−1; 9.999·10−1]), initialize Kronecker factors to identity: yes
•KFAC*: damping: LU([1·10−11; 1·10−5]), exponential moving average: U([9.99·
10−1; 9.999·10−1]), initialize Kronecker factors to identity: yes
We found that KFAC* requires very large exponential moving averages to work well.
37B Pseudo-Code: KFAC for the Poisson Equation
Algorithm 1 KFAC for the Poisson equation.
Require:
MLP uθwith parameters θ0= (θ(1)
0, . . . ,θ(L)
0) = (vec W(1)
0, . . . , vecW(L)
0),
interior data {(xn, yn)}NΩ
n=1,
boundary data {(xb
n, yb
n)}N∂Ω
n=1
exponential moving average β, momentum µ, Damping λ, number of steps T
0) Initialization
forl= 1, . . . , L do
A(l)
Ω,B(l)
Ω,A(l)
∂Ω,B(l)
∂Ω←0orI ▷Initialize Kronecker factors
end for
fort= 0, . . . , T −1do
1) Compute the interior loss and update its approximate curvature
(Z(0)
n. . . ,Z(L)
n,∆un)←∆uθt(xn)n= 1, . . . , N Ω ▷Forward Laplacian wit intermediates
Compute layer output gradients g(l)
n,s:=∂∆un/∂Z(l)
n,swith autodiff in one backward pass
(g(1)
n,s, . . . ,g(L)
n,s)←grad(∆un,(Z(1)
n,s, . . . ,Z(L)
n,s))n= 1, . . . , N Ω,s= 1, . . . , S :=d+ 2
for all l= 1, . . . , L do ▷Update Kronecker factors of the interior loss
ˆA(l)
Ω←βˆA(l)
Ω+ (1−β)1
NΩSPNΩ
n=1Z(l−1)
n,sZ(l−1)⊤
n,s
ˆB(l)
Ω←βˆB(l)
Ω+ (1−β)1
NΩPNΩ
n=1g(l)
n,sg(l)⊤
n,s
end for
LΩ(θt)←1
2NΩPNΩ
n=1(∆un−yn)2▷Compute interior loss
2) Compute the boundary loss and update its approximate curvature
(z(0)
n. . . ,z(L)
n, un)←uθt(xb
n)n= 1, . . . , N ∂Ω ▷Forward pass with intermediates
Compute layer output gradients g(l)
n:=∂un/z(l)
nwith autodiff in one backward pass
(g(1)
n. . . , ∂g(L)
n)←grad(un,(z(0)
n. . . ,z(L)
n))n= 1, . . . , N ∂Ω
for all l= 1, . . . , L do ▷Update Kronecker factors of the boundary loss
ˆA(l)
∂Ω←βˆA(l)
∂Ω+ (1−β)1
N∂ΩPN∂Ω
n=1z(l−1)
nz(l−1)⊤
n
ˆB(l)
∂Ω←βˆB(l)
∂Ω+ (1−β)1
N∂ΩPN∂Ω
n=1g(l)
ng(l)⊤
n
end for
L∂Ω(θt)←1
2N∂ΩPN∂Ω
n=1(un−yb
n)2▷Compute boundary loss
3) Update the preconditioner (use inverse of Kronecker sum trick)
for all l= 1, . . . , L do
C(l)←h
(ˆA(l)
Ω+λI)⊗(ˆB(l)
Ω+λI) + ( ˆA(l)
∂Ω+λI)⊗(ˆB(l)
∂Ω+λI)i−1
end for
4) Compute the gradient using autodiff, precondition the gradient
(g(1), . . . ,g(L))←grad(LΩ(θt) +L∂Ω(θt),(θ(1)
t, . . . ,θ(L)
t)) ▷Gradient with autodiff
for all l= 1, . . . , L do ▷Precondition gradient
∆t← −C(l)g(l)▷Proposed update direction
ˆδ(l)
t←µδ(l)
t−1+∆(l)
tift >0else∆(l)
t ▷Add momentum from previous update
end for
5) Given the direction ˆδ(1)
t, . . . , ˆδ(L)
t, choose learning rate αby line search & update
forl= 1, . . . , L do ▷Parameter update
δ(l)
t←αˆδ(l)
t
θ(l)
t+1←θ(l)
t+αδ(l)
t
end for
end for
return Trained parameters θT
38C Taylor-Mode Automatic Differentiation & Forward Laplacian
PINN losses involve differential operators of the neural network, for instance the Laplacian. Recently,
Li et al. [29] proposed a new computational framework called forward Laplacian to evaluate the
Laplacian and the neural network’s prediction in one forward traversal. To establish a Kronecker-
factorized approximation of the Gramian, which consists of the Laplacian’s gradient, we need to
know how a weight matrix enters its computation. Here, we describe how the weight matrix of a
linear layer inside a feed-forward net enters the Laplacian’s computation when using the forward
Laplacian framework. We start by connecting the forward Laplacian framework to Taylor-mode
automatic differentiation [ 18,3], both to make the presentation self-contained and to explicitly point
out this connection which we believe has not been done previously.
C.1 Taylor-Mode Automatic Differentiation
The idea of Taylor-mode is to forward-propagate Taylor coefficients, i.e. directional derivatives,
through the computation graph. We provide a brief summary based on its description in [3].
Taylor series and directional derivatives Consider a function f:Rd→Rand its K-th order
Taylor expansion at a point x∈Rdalong a direction αv∈Rdwithα∈R,
ˆf(α) =f(x+αv) =f(x) +α∂f(x)
∂x⊤
v+α2
2!v⊤∂2f(x)
∂x2
v
+α3
3!X
i1,i2i3∂3f(x)
∂x3
i1,i2,i3vi1vi2vi3
+. . .
+αK
K!X
i1,...,iK∂Kf(x)
∂xK
i1,...,iKvi1···viK.
We can unify this expression by introducing the K-th order directional derivative of fatxalongv,
∂Kf(x) [v, . . . ,v]|{z}
Ktimes:=X
i1,...,iK∂Kf(x)
∂xK
i1,...,iKvi1. . . v iK.
This simplifies the uni-directional Taylor expansion to
ˆf(α) =f(x+αv) =f(x) +α∂f(x)[v] +α2
2!∂2f(x)[v,v] +α3
3!∂3f(x)[v,v,v]
+. . .+αK
K!∂Kf(x)[v, . . . ,v]
=:KX
k=1αk
k!∂kf(x)
⊗kv
=:KX
k=1wf
kαk
where we have used the notation ⊗kvto indicate kcopies of v, and introduced the k-th order
Taylor coefficient wf
k∈Roff. This generalizes to vector-valued functions: If f’s output was
vector-valued, say f(x)∈Rc, we would have Taylor-expanded each component individually and
grouped coefficients of same order into vectors wf
k∈Rcsuch that [wf
k]iis the k-th order Taylor
coefficient of the ith component of f.
A note on generality: In this introduction to Taylor-mode, we limit the discussion to the compu-
tation of higher-order derivatives along a single direction v, i.e.∂Kf(x)[v, . . . ,v]. This is limited
though, e.g. if we set K= 2 then we can compute ∂2f(x)[v,v] =v⊤(∂2f(x)/∂x2)v. We can set
v=eito the i-th standard basis vector to compute the i-th diagonal element of the Hessian. But
we cannot evaluate off-diagonal elements, as this would require multi-directional derivatives, like
∂2f(x)[ei,ej̸=i]. A more general description of Taylor-mode for multi-directional Taylor series
along Mdirections, ˆf(α1, . . . , α M) =f(x+α1v1+···+αMvM), which require more general
directional derivatives ∂Kf(x)[v1, . . . ,vK](each vector can be different) are discussed in [ 25]. We
will use this formulation later to generalize the forward Laplacian scheme to more general weighted
sums of second-order derivatives in §C.3.
39Composition rule Next, we consider the case where f=g◦his a composition of two functions.
Starting from the Taylor coefficients wh
0, . . .wh
Kofˆh(α) =h(x+αv), the Taylor coefficients
wf
0, . . . ,wf
Kofˆf(α) =f(x+αv)follow from Faà di Bruno’s formula [18, 3]:
wf
k=X
σ∈part( k)1
n1!. . . n K!∂|σ|g(wh
0)
⊗s∈σwh
s
(C20)
In the above, part( k)is the set of all integer partitionings of k; a set of sets. |σ|denotes the length of
a setσ∈part( k),niis the count of integer iinσ, andwh
0=h(x).
Second-order Taylor-mode Our goal is the computation of second-order derivatives of fw.r.t.x. So
let’s work out Equation (C20) up to order K= 2. The zeroth and first order are simply the forward
pass and the forward-mode gradient chain rule. For the second-order term, we need the integer
partitioning of 2, given by part(2) = {{1,1},{2}}. This results in
wf
0=g(wh
0), (C21a)
wf
1=∂g(wh
0)[wh
1], (C21b)
wf
2=1
2∂2g(wh
0)[wh
1,wh
1] +∂g(wh
0)[wh
2]. (C21c)
We can also express wf
1,wf
2in terms of Jacobian- and Hessian-vector products of g,
wf
1=
Jwh
0g(wh
0)
wh
1, (C22a)
wf
2=1
2
wh
1⊤∂2[g(wh
0)]1
∂wh
02wh
1
...
wh
1⊤∂2[g(wh
0)]D
∂wh
02wh
1
+
Jwh
0g(wh
0)
wh
2. (C22b)
Note that first-order Taylor-mode (Equation (C22a)) corresponds to the standard forward-mode
autodiff which pushes forward error signals through Jacobian-vector products.
C.2 Forward Laplacian
Our goal is to compute the Laplacian of f:Rd→Rc(in practise, c= 1),
∆xf(x) =dX
i=1
∂2[f(x)]1[ei,ei]
...
∂2[f(x)]c[ei,ei]
:= 2dX
i=1wf
2,i∈Rc, (C23)
where eiis the i-th standard basis vector, [f(x)]jis the j-th component of f(x), and we have
introduced the second-order Taylor coefficients wf
2,ioffalongei. The Laplacian requires computing,
then summing, the second-order Taylor coefficients of dTaylor approximations {f(x+ei)}i=1,...,d.
Naive approach We can use Taylor-mode differentiation to compute all these components in one
forward traversal. Adding the extra loop over the Taylor expansions we want to compute in parallel,
we obtain the following scheme from Equation (C21),
wf
0=g(wh
0), (C24a)n
wf
1,io
i=1,...,d=
∂g(wh
0)[wh
1,i]	
i=1,...,d, (C24b)
n
wf
2,io
i=1,...,d=1
2∂2g(wh
0)[wh
1,i,wh
1,i] +∂g(wh
0)[wh
2,i]
i=1,...,d. (C24c)
Forward Laplacian framework Computing the Laplacian via Equation (C24) first computes,
then sums, the diagonal second-order derivatives {wf
2,i}i=1,...,d. Note that we can pull the sum
40inside the forward propagation scheme, specifically Equation (C24c), and push-forward the summed
second-order coefficients. This simplifies Equation (C24) to
wf
0=g(wh
0), (C25a)n
wf
1,io
i=1,...,d=
∂g(wh
0)[wh
1,i]	
i=1,...,d, (C25b)
dX
i=1wf
2,i
|{z}
1/2∆xf(x)= 
1
2dX
i=1∂2g(wh
0)[wh
1,i,wh
1,i]!
+∂g(wh
0)"dX
i=1wh
2,i#
|{z}
1/2∆xg(x). (C25c)
Equation (C25) is the forward Laplacian framework from Li et al. [29] for computing the Laplacian
of a neural network. Here, we have derived it from Taylor-mode automatic differentiation. Note that
Equation (C25) requires less computations and memory than Equation (C24) because we can pull the
summation from the Laplacian into the forward propagation scheme.
C.2.1 Forward Laplacian for Elementwise Activation Layers
We now describe Equation (C25) for the case where g:Rc→Rcacts element-wise via σ:R→R.
We will write σ(•), σ′(•), σ′′(•)to indicate element-wise application of σ, its first derivative σ′, and
second derivative σ′′to all elements of •. Further, let ⊙denote element-wise multiplication, and
(•)⊙2element-wise squaring. With that, we can write the Jacobian as Jh(x)g(x) = diag( σ(h(x)))
where diag(•)embeds a vector •into the diagonal of a matrix. The Hessian of component iis
∂2[g(h(x))]i/∂h(x)2= [σ′′(h(x))]ieie⊤
i. Inserting Equation (C22) into Equation (C25) and using
the Jacobian and Hessian expressions of the element-wise activation function yields the following
forward Laplacian forward propagation:
wf
0=σ(wh
0), (C26a)n
wf
1,io
=
σ′(wh
0)⊙wh
1,i	
i=1,...,d, (C26b)
dX
i=1wf
2,i=1
2σ′′(wh
0)⊙ dX
i=1 
wh
1,i⊙2!
+σ′(wh
0)⊙ dX
i=1wh
2,i!
. (C26c)
C.2.2 Forward Laplacian for Linear Layers
Now, let g:RDin→RDoutbe a linear layer with weight matrix W∈RDout×Dinand bias vector
b∈RDout. Its Jacobian is Jh(x)(Wh(x) +b) =Wand the second-order derivative is zero. Hence,
Equation (C25) for linear layers becomes
wf
0=Wwh
0+b, (C27a)n
wf
1,io
i=1,...,d=
Wwh
1,i	
i=1,...,d, (C27b)
dX
i=1wf
2,i=W dX
i=1wh
2,i!
. (C27c)
We can summarize Equation (C27) in a single equation by grouping all quantities that are multiplied
byWinto a single matrix, and appending a single row of ones or zeros to account for the bias:

wf
0wf
1,1. . .wf
1,dPD
i=1wf
2,i
| {z }
:=Tf∈RDout×(d+2)= (W b )
wh
0wh
1,1. . .wh
1,dPd
i=1wh
2,i
1 0 . . . 0 0
| {z }
:=Th∈R(Din+1)×(d+2),
or, in compact form,
Tf=˜WTh. (C28)
Equation (C28) shows that the weight matrix ˜W(l)= (W(l)b(l))of a linear layer f(l)inside a
neural network f(L)◦. . .◦f(1)is applied to a matrix T(l−1)∈RDin×(d+2)during the computation
of the net’s prediction and Laplacian via the forward Laplacian framework and yields another matrix
T(l)∈RDout×(d+2).
41C.3 Generalization of the Forward Laplacian to Weighted Sums of Second Derivatives
The Laplacian is of the form ∆xf=P
i∂2f(x)[ei,ei]and we previously described the forward
Laplacian framework of Li et al. [29] as a consequence of pulling the summation into Taylor-mode’s
forward propagation. Here, we derive the forward propagation to more general operators of the formP
i,jci,j∂2f(x)[ei,ej], which contain the Laplacian for ci,j=δi,j.
As mentioned in §C.1, this requires a generalization of Taylor-mode which computes derivatives of
the form ∂Kf(x)[v, . . . ,v], where the directions vmust be identical. We start with the formulation
in [25] which expresses the K-th multi-directional derivative of a function f=g◦hthrough the
composites’ derivatives (all functions can be vector-to-vector)
∂Kf(x)[v1, . . . ,vK] =X
σ∈part({1,...,K})∂|σ|g(h(x))h
⊗η∈σ∂|η|h(x) [⊗l∈ηvl]i
. (C29)
Here, part({1, . . . , K })denotes the set of all set partitions of {1, . . . , K }(σis a set of sets). E.g.,
part({1}) ={{{1}}},
part({1,2}) ={{{1,2}},{{1},{2}}},
part({1,2,3}) ={{{1,2,3}},{{1},{2,3}},{{1,2},{3}},{{1,3},{2}},{{1},{2},{3}}}.
To make this more concrete, let’s consider Equation (C29) for first- and second-order derivatives,
∂f(x)[v] =∂g(h(x))[∂h(x)[v]], (C30a)
∂2f(x)[v1,v2] =∂g2(h(x))[∂h(x)[v1], ∂h(x)[v2]] +∂g(h(x))[∂h2(x)[v1,v2]]. (C30b)
From Equation (C30), we can see that if we want to compute a weighted sum of second-order
derivativesP
i,jci,j∂2f(x)[vi,vj], we can pull the sum inside the second equation,
X
i,jci,j∂2f(x)[vi,vj] =X
i,jci,j∂2g(h(x))[∂h(x)[vi], ∂h(x)[vj]]
+∂g(h(x))
X
i,jci,j∂2h(x)[vi,vj]
.(C31)
Hence, we can propagate the collapsed second-order derivatives, together with all first-order deriva-
tives along v1,v2, . . .. The only difference to the forward Laplacian is how second-order effects of
an operation are incorporated (first term in Equation (C31)).
We now specify Equations (C29) and (C31) for linear layers and element-wise activation functions.
For a linear layer g:h(x)7→Wh(x) +b, we have ∂m>1g(h(x))[v1, . . . ,vm] =0, and thus
∂f(x)[v] =W∂h(x)[v], (C32a)
∂2f(x)[v1,v2] =W∂2h(x)[v1,v2], (C32b)
∂Kf(x)[v1, . . . ,vK] =W∂Kh(x)[v1, . . . ,vK]. (C32c)
The last equation is because only the set partition {1, . . . , K }contributes to Equation (C29).
For elementwise activations g:h(x)7→σ(h(x))withσ:R→Rapplied component-wise, we have
the structured derivative tensor [∂mg(h(x))]i1,...,im=∂mσ(h(x)i1)δi1,...,imand multi-directional
derivative ∂Kg(h(x))[v1, . . . ,vK] =∂Kσ(x)⊙v1⊙ ··· ⊙ vK. Equation (C30) becomes
∂f(x)[v] =σ′(h(x))⊙∂h(x)[v], (C33a)
∂2f(x)[v1,v2] =σ′′(h(x))⊙∂h(x)[v1]⊙∂h(x)[v2] +σ′(h(x))⊙∂2h(x)[v1,v2].(C33b)
As shown in Equation (C30b), for both Equations (C32) and (C33), we can pull the summation inside
the propagation scheme. Specifically, to computeP
i,jci,j∂2f(x)[ei,ej], we have for linear layers
f(x) =g(h(x)), (C34a)
∂f(x)[ei] =W∂h(x)[ei], i = 1, . . . , d , (C34b)
X
i,jci,j∂2f(x)[ei,ej] =W
X
i,jci,j∂2h(x)[ei,ej]
. (C34c)
42and for activation layers
f(x) =σ(h(x)), (C34d)
∂f(x)[ei] =σ′(h(x))⊙∂h(x)[ei], i = 1, . . . , d , (C34e)
X
i,jci,j∂2f(x)[ei,ej] =X
i,jci,jσ′′(h(x))⊙∂h(x)[ei]⊙∂h(x)[ej]
+σ′(h(x))⊙
X
i,jci,j∂2h(x)[ei,ej]
.(C34f)
(the summed second-order derivatives that are forward-propagated are highlighted). This propagation
reduces back to the forward Laplacian Equations (C26) and (C27) when we set ci,j=δi,j. In contrast
to other attempts to compute such a weighted sum of second-order derivatives by reducing it to
(multiple) partial standard forward Laplacians [ 30], we do not need to diagonalize the coefficient
matrix and can compute the linear operator in one forward propagation.
C.4 Comparison of Forward Laplacian and Autodiff Laplacian
Setup We compare the efficiency of the forward Laplacian, that we use in all our experiments, to
an off-the shelve solution. We consider two Laplacian implementations:
1.Autodiff Laplacian. Computes the Laplacian with PyTorch’s automatic differentiation
(functorch ) by computing the batched Hessian trace (via torch.func.hessian and
torch.func.vmap ). This is the standard approach in many PINN implementations.
2.Forward Laplacian. Computes the Laplacian via the forward Laplacian framework. We used
this approach for all PDEs and optimizers, except ENGD, presented in the experiments.
We use the biggest network from our experiments (the DΩ→768→768→512→512→1MLP
with tanh-activations from Figure 3), then measure run time and peak memory of computing the net’s
Laplacian on a mini-batch of size N= 1024 with varying values of DΩ. To reduce measurement
noise, we repeat each run over five independent Python sessions and report the smallest value (using
the same GPU as in all other experiments, an NVIDIA RTX 6000 with 24 GiB memory).
Results The following tables compare run time and peak memory between the two approaches:
DΩAutodiff Forward
Laplacian [s] Laplacian [s]
1 0.051 (1.6x) 0.033 (1.0x)
10 0.20 (2.0x) 0.10 (1.0x)
100 1.7 (2.0x) 0.84 (1.0x)DΩAutodiff Forward
Laplacian [GiB] Laplacian [GiB]
1 0.21 (0.96x) 0.22 (1.0x)
10 0.98 (1.6x) 0.61 (1.0x)
100 8.8 (1.9x) 4.6 (1.0x)
We observe that the forward Laplacian is roughly twice as fast as the functorch Laplacian, and that
it uses significantly less memory for large input dimensions, up to only one half when DΩ= 100 . We
visualized both tables using more values for DΩ, see Figure C16. In the shown regime, we find that
the MLP’s increasing cost in DΩ(due to the growing first layer) is negligible as we observe linear
scaling in both memory and run time. For extremely large DΩ, it would eventually become quadratic.
D Backpropagation Perspective of the Laplacian
Here, we derive the computation graphs for the Laplacian and its associated Gramian when using
reverse-mode AD, aka backpropagation. In contrast to the Taylor-mode perspective, the resulting
expressions cannot be interpreted as simple weight-sharing. This complicates defining a Kronecker-
factored approximation for the Gramian without introducing new approximations that are different
from Eschenhagen et al. [17], rendering the Taylor-mode perspective advantageous.
We start by deriving the Laplacian ∆u:= Tr(∇2
xu)of a feed-forward NN (see §2.1), assuming
a single data point for simplicity (see §D.1) and abbreviating uθasu. The goal is to make the
Laplacian’s dependence w.r.t. a weight W(i)in one layer of the network explicit. Then, we can write
43Figure C16: Time (left) and memory (right) required with the forward Laplacian used in our
implementation and the functorch implementation.
down the Jacobian JW(i)∆u(see §D.2) which is required for the Gramian in Equation (2) (see §D.3).
We do this based on the concept of Hessian backpropagation [10, HBP,] which yields a recursion for
the Hessian ∇2
xu. The Laplacian follows by taking the trace of the latter. Finally, we use the chain
rule express the Laplacian’s Jacobian JW(i)∆uin terms of W(i)’s children in the compute graph.
D.1 Hessian Backpropagation and Backward Laplacian
Gradient backpropagation describes a recursive procedure to compute gradients by backpropagating a
signal via vector-Jacobian products (VJPs). A similar procedure can be derived to compute Hessians
w.r.t. nodes in a graph ( z(i)orθ(i)). We call this recursive procedure Hessian backpropagation [10].
Gradient backpropagation As a warm-up, let’s recall how to compute the gradient ∇θu=
(∇θ(1)u, . . . , ∇θ(L)u). We start by setting ∇z(L)u=∇uu= 1(assuming uis scalar for simplicity),
then backpropagate the error via VJPs according to the recursion
∇z(i−1)u=
Jz(i−1)z(i)⊤
∇z(i)u ,
∇θ(i)u=
Jθ(i)z(i)⊤
∇z(i)u(D35)
fori=L, . . . , 1. This yields the gradients of uw.r.t. all intermediate representations and parameters.
Hessian backpropagation Just like gradient backpropagation, we can derive a recursive scheme
for the Hessian. Recall the Hessian chain rule
∇2(f◦g) = (J g)⊤∇2f(g)(Jg) +X
k(∇gf)k· ∇2gk, (D36)
where gidenotes the individual components of g, see [ 53]. The recursion for computing Hessians of
uw.r.t. intermediate representations and parameters starts by initializing the recursion with ∇2
z(L)u=
∇2
uu= 0, and then backpropagating according to (see Dangel et al. [10] for details)
∇2
z(i−1)u=
Jz(i−1)z(i)⊤
∇2
z(i)u
Jz(i−1)z(i)
+h(i)X
k=1
∇2
z(i−1)[z(i)]k
[∇z(i)u]k,
∇2
θ(i)u=
Jθ(i)z(i)⊤
∇2
z(i)u
Jθ(i)z(i)
+h(i)X
k=1
∇2
θ(i)[z(i)]k
[∇z(i)u]k(D37)
fori=L, . . . , 1. The first term takes the incoming Hessian (w.r.t. a layer’s output) and sandwiches it
between the layer’s Jacobian. It can be seen as backpropagating curvature from downstream layers.
The second term adds in curvature introduced by the current layer. It is only non-zero if the layer is
nonlinear. For linear layers, convolutional layers, and ReLU layers, it is zero.
Following the Hessian backpropagation procedure of Equation (D37) yields the per-layer parameter
and feature Hessians ∇2
z(i)u,∇2
θ(i)u. In Figure D17 we depict the dependencies of intermediate
gradients and Hessians for computing ∇2
xu=∇2
z(0)u:
44Parameters θ(1) . . .θ(i−1)θ(i) . . .θ(L)
Forward x z(1) . . .z(i−1)z(i) . . . u
Backward ∇xu ∇z(1)u . . . ∇z(i−1)u ∇z(i)u . . . ∇uu
Hess. backward ∆u ∇2
xu ∇2
z(1)u . . . ∇2
z(i−1)u ∇2
z(i)u . . . ∇2
uu
Figure D17: Computation graph of a sequential neural network’s Laplacian ∆uwhen using (Hessian)
backpropagation. Arrows indicate dependencies between intermediates. Note that z(0):=x,
z(L):=u,∇uu= 1, and∇2
uu=0. For the Gramian, we are interested in how the neural network
parameters enter the Laplacian’s computation. Each parameter is used three times: during (i) the
forward pass, (ii) the backward pass for the gradient, and (iii) the backward pass for the Hessian.
•∇z(i−1)udepends on ∇z(i)udue to the recursion in Equation (D35), and on z(i−1),θ(i)due
to the Jacobian Jz(i−1)z(i)in the gradient backpropagation Equation (D35).
•∇2
z(i−1)udepends on ∇2
z(i)uand∇z(i)udue to the recursion in Equation (D37), and on
z(i−1),θ(i)due to the Jacobian Jz(i−1)z(i)and Hessian ∇2
z(i−1)[z(i)]kin the Hessian back-
propagation Equation (D35).
The Laplacian ∆ufollows by taking the trace of ∇2
xufrom above, and is hence recursively defined.
To make these expressions more concrete, we now recap the HBP equations for fully-connected
layers and element-wise nonlinear activations.
Hessian backpropagation through nonlinear layers We mostly consider nonlinear layers without
trainable parameters and consist of a componentwise nonlinearity z7→σ(z)for some σ:R→R.
The Jacobian of such a nonlinear layer is given by Jz(i−1)z(i)= diag( σ′(z(i−1)))and the Hessian
terms are given by ∇2
z(i−1)[z(i)]k=σ′′(z(i−1)
k)eke⊤
kwhere ekis the unit vector along coordinate k.
With these two identities we can backpropogate the input Hessian through such layers via
∇2
z(i−1)u=
diag( σ′(z(i−1)))⊤
∇2
z(i)u
diag( σ′(z(i−1)))
+h(i)X
k=1σ′′(z(i−1)
k)eke⊤
k[∇z(i)u]k.(D38)
Hessian backpropagation through a linear layer To de-clutter the dependency graph of Fig-
ure D17, we will now consider the dependency of ∆uw.r.t. the weight of a single layer. We assume
this layer ito be a linear layer with parameters W(i)such that θ(i)= vec( W(i)),
z(i)=W(i)z(i−1). (D39)
For this layer, the second terms in Equation (D37) disappears because the local Hessians are zero,
that is ∇2
z(i−1)[z(i)]k=0and∇2
W(i)[z(i)]k=0. Also, the Jacobians are JW(i)z(i)=z(i−1)⊤⊗I
andJz(i−1)z(i)=W(i)and hence only depend on one of the two layer inputs. This simplifies the
computation graph. Figure D18 shows the dependencies of W(i)on the Laplacian, highlighting its
three direct children,
z(i)=W(i)z(i−1),
∇z(i−1)u=W(i)⊤(∇z(i)u),
∇2
z(i−1)u=W(i)⊤ 
∇2
z(i)u
W(i).(D40)
45. . .W(i) . . .
. . .z(i−1)z(i) . . .
. . . ∇z(i−1)u ∇z(i)u . . .
∆u . . . ∇2
z(i−1)u ∇2
z(i)u . . .Figure D18: Direct dependencies of a linear
layer’s weight matrix W(i)in the Laplacian’s
computation graph. There are three direct children:
(i) the layer’s output from the forward pass, (ii) the
Laplacian’s gradient w.r.t. the layer’s input from
the gradient backpropagation, and (iii) the Lapla-
cian’s Hessian w.r.t. the layer’s input from the Hes-
sian backpropagation. The Jacobians JW(i)∆u
required for the Gramian are the vector-Jacobian
products accumulated over those children.
D.2 Parameter Jacobian of the Backward Laplacian
Recall that the entries of the Gramian are composed from parameter derivatives of the input Laplacian,
see Equation (2). We have identified the direct children of W(i)in the Laplacian’s compute graph,
see Equation (D40). This allows us to compute the Jacobian JW(i)∆uby the chain rule, i.e. by
accumulating the Jacobians over all direct children,
JW(i)∆u=P
•∈n
z(i),∇z(i−1)u,∇2
z(i−1)uo(JW(i)•)⊤∇•∆u
=
JW(i)z(i)⊤
∇z(i)∆u
+ (JW(i)∇z(i−1)u)⊤∇∇z(i−1)u∆u
+ 
JW(i)∇2
z(i−1)u⊤∇∇2
z(i−1)u∆u .(D41)
The terms ∇•∆ucan be computed with gradient backpropagation to the respective intermediates.
D.3 Gramian of the Backward Laplacian
With the Laplacian’s Jacobian from Equation (D41), we can now write down the Gramian block of
the interior loss (up to summation over the data) for W(i)as
G(i)
Ω= (JW(i)∆u) (JW(i)∆u)⊤
=P
•,•∈n
z(i),∇z(i−1)u,∇2
z(i−1)uo(JW(i)•)⊤h
(∇•∆u) (∇•∆u)⊤i
(JW(i)•)
| {z }
=:G(i)
Ω,•,•. (D42)
The Gramian consists of nine different terms, see Figure D19 for a visualization which shows not only
the diagonal blocks G(i)
Ω, but also the full Gramian GΩwhich decomposes in the same way. The terms
∇•∆uare automatically computed when computing the gradient of the loss via backpropagation. We
will now proceed and simplify the terms by inserting the Jacobians into Equation (D41) and studying
the Gramian’s block diagonal, which is approximated by KFAC, in more detail.
Computing JW(i)•Let us first compute the Jacobians JW(i)•in Equation (D41). The Jacobian of
the linear layer’s forward pass is
JW(Wx) =x⊤⊗I. (D43a)
The Jacobian from the gradient backpropagation is
JW 
W⊤x
=I⊗x⊤, (D43b)
and the Jacobian from the Hessian backpropagation is
JW 
W⊤XW
=I⊗W⊤X+K 
I⊗W⊤X⊤
, (D43c)
where K∈Rdim(Z)×dim(Z)(denoting Z:=W⊤XW ) is a permutation matrix that, when multi-
plied onto a vector whose basis corresponds to that of the flattened output Z, modifies the order from
first-varies-fastest to last-varies-fastest, i.e.
Kvec(Z) = vec( Z⊤).
46Full interior Gramian
(forward, forward) (forward, gradient) (forward, Hessian)
(gradient, forward) (gradient, gradient) (gradient, Hessian)
(Hessian, forward) (Hessian, gradient) (Hessian, Hessian)
Figure D19: Contributions GΩ,•,•to the Laplacian’s Gramian GΩfrom different children in the
computation graph on a synthetic toy problem. We use a 4→3→2→1sigmoid-activated MLP
and 10 randomly generated inputs. The contributions are highlighted as in Equation (D42).
Re-introducing the layer indices, the expressions in Equation (D40) become
JW(i)z(i)=z(i−1)⊤⊗I
JW(i)∇z(i−1)u ,=I⊗ ∇z(i)u
JW(i)∇2
z(i−1)u ,=I⊗h
W(i)⊤ 
∇2
z(i)ui
+K
I⊗h
W(i)⊤ 
∇2
z(i)u⊤i
.(D44)
We will now use symmetries in the objects used during Hessian backpropagation to simplify this
further. At a first glance, it looks like the Gramian consists of 16 terms, as there are 4 summands
from the Jacobians in Equation (D43). However, we can simplify into 9 terms:
47First,∇2
z(i)uis symmetric, that is
JW(i)
W(i)⊤ 
∇2
z(i)u
W(i)
=I⊗h
W(i)⊤ 
∇2
z(i)ui
+K
I⊗h
W(i)⊤ 
∇2
z(i)ui
,
and the transposed Jacobian is
I⊗h 
∇2
z(i)u
W(i)i
+
I⊗h 
∇2
z(i)u
W(i)i
K⊤.
Second, we multiply the transpose Jacobian onto ∇∇2
z(i−1)u∆u, which inherits symmetry from the
Hessian, [∇∇2
z(i−1)u∆u]j,k= [∇∇2
z(i−1)u∆u]k,j. Due to this symmetry, the action of K(orK⊤)
does not alter it,
K⊤
∇∇2
z(i−1)u∆u
=∇∇2
z(i−1)u∆u .
In other words, it does not matter how we flatten (first- or last-varies-fastest). This simplifies the VJP
(last line in Equation (D42)) to

I⊗h 
∇2
z(i)u
W(i)i
∇∇2
z(i−1)u∆u+
I⊗h 
∇2
z(i)u
W(i)i
K⊤∇∇2
z(i−1)u∆u
= 2
I⊗h 
∇2
z(i)u
W(i)i
∇∇2
z(i−1)u∆u .
We can now write down the simplified Jacobian from Equation (D41), whose self-outer product forms
the Gramian block for a linear layer’s weight matrix,
JW(i)∆u=
z(i−1)⊤⊗I⊤
∇z(i)∆u
| {z }
(1)
+ (I⊗ ∇z(i)u)⊤∇∇z(i−1)u∆u
| {z }
(2)
+ 2
I⊗h 
∇2
z(i)u
W(i)i
∇∇2
z(i−1)u∆u
| {z }
(3),(D45)
where (1) is the contribution from the forward pass, (2) is the contribution from the gradient back-
propagation, and (3) is the contribution from the Hessian backpropagation. The Jacobians from
Equation (D43) allow to express the Gramian in terms of Kronecker-structured expressions consisting
of 9 terms in total. Figure D19 shows the 6 contributions from different children pairs.
Conclusion One problem of computing the Laplacian and its Jacobian with backpropagation ac-
cording to Equation (D45) is that if we write out the Gramian’s block G(i)
Ω= JW(i)∆u(JW(i)∆u)⊤,
we obtain 9 terms of different structure. Defining a single Kronecker product approximation would
involve introducing new approximations on top of those employed by Eschenhagen et al. [17]. There-
fore, the forward Laplacian, or Taylor-mode, perspective we choose in the main text is advantageous
as it allows to define KFAC without introducing new approximations.
48NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We list our contributions as bullet points in §1 and provide references to the
parts of the paper that present them.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See the paragraph dedicated to limitations in §5.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]
49Justification: Whereas we provide a derivation of the proposed Kronecker-factored approxi-
mation, our work does not provide any rigorous mathematical statements.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide a detailed description of the general experimental protocol in
§A.1 and details for each individual experiment in §A, including all hyper-parameter search
spaces and hyper-parameters of the best runs. We also show pseudo-code for our algorithm
in §B.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
505.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We will open-source our KFAC implementations, as well as the code to fully
reproduce all experiments and the original data presented in this manuscript.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We list all details in §A.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: While we do not show mean and standard deviations over different model
initializations for training curves, we aimed to use a thorough tuning protocol to avoid
artifacts from insufficient hyper-parameter tuning and also conducted experiments with
alternative hyper-parameter search methods (Bayesian versus random/grid) to ensure the
consistency of our results. To further probe the robustness of our results, we analyzed their
stability w.r.t. random seed in for one experiment in §A.9.
Guidelines:
• The answer NA means that the paper does not include experiments.
51•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We describe the hardware in §4. The total computation time can be inferred
from the description of the tuning protocol in combination with the assigned time budget
per run.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have read the Code of Ethics and believe that our submission conforms to
it.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [No]
52Justification: It is the goal of this paper to improve the accuracy of neural network-based
PDE solvers. As the numerical solution of PDEs is a classic topic in applied mathematics
and engineering, we believe there is no immediate negative societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper does not release any data or models that have a high risk for misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use any existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
53•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release any new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
54•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
55