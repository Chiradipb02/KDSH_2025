Small steps no more: Global convergence of stochastic
gradient bandits for arbitrary learning rates
Jincheng Mei1Bo Dai1,3Alekh Agarwal2Sharan Vaswani5Anant Raj6
Csaba Szepesvári1 4Dale Schuurmans1 4
1Google DeepMind2Google Research3Georgia Institute of Technology4University of Alberta
5Simon Fraser University6Indian Institute of Science
{jcmei,bodai,alekhagarwal,szepi,schuurmans}@google.com
vaswani.sharan@gmail.com anantraj@iisc.ac.in
Abstract
We provide a new understanding of the stochastic gradient bandit algorithm by
showing that it converges to a globally optimal policy almost surely using anycon-
stant learning rate. This result demonstrates that the stochastic gradient algorithm
continues to balance exploration and exploitation appropriately even in scenarios
where standard smoothness and noise control assumptions break down. The proofs
are based on novel findings about action sampling rates and the relationship be-
tween cumulative progress and noise, and extend the current understanding of how
simple stochastic gradient methods behave in bandit settings.
1 Introduction
The stochastic gradient method has been ubiquitous in the field of machine learning for decades [ 4].
When applied to reinforcement learning (RL), a representative instantiation of stochastic gradient
is the well known policy gradient [ 32] (or REINFORCE [ 34]) algorithm, where in each iteration
an online sample is gathered using the current policy, from which a gradient estimate is obtained
to conduct parameter updates. In the simplest setting of a stochastic bandit problem [ 16], where
decisions matter only for one step, the REINFORCE policy gradient method becomes equivalent to
the stochastic gradient bandit algorithm [ 31, Section 2.8]. Compared to other statistical methods,
such as the upper confidence bound algorithm (UCB, [ 14,3]), and Thompson sampling (TS, [ 33,2]),
the stochastic gradient bandit algorithm is conceptually simpler and more computationally efficient,
as it does not calculate exploration bonuses nor posterior distributions. Moreover, the stochastic
gradient method is highly scalable and naturally applicable to large scale neural networks [29, 30].
However, unlike UCB or TS, the stochastic gradient bandit algorithm does not have an equivalently
well established and comprehensive theoretical footing. Given its pervasive success and widespread
application in RL [ 30] and fine-tuning for large language models [ 26,27]), it remains an important
question to understand the success of stochastic gradient based algorithms in bandit-like settings, not
only to bridge the gap between theory and practice, but also to identify more effective and robust
variants. In this paper, we make a significant contribution to the theoretical understanding of the
stochastic gradient bandit algorithm, bringing its justification closer to that of other less scalable but
theoretically well established methods. In particular, we establish the surprising result that:
For any constant learning rate η >0, the stochastic gradient bandit algorithm is guaranteed to
converge to the globally optimal policy almost surely.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Since learning rate is the only tuning parameter in the stochastic gradient bandit algorithm, this result
offers a remarkable robustness for the method, that it converges to a near optimal policy, irrespective
of the value of this hyperparameter! Analysis of this algorithm is challenging because it requires
techniques for simultaneously handling non-convex optimization, stochastic approximation, and the
exploration-exploitation trade-off. Prior theoretical work on the stochastic gradient algorithm has pri-
marily focused on non-convex convex optimization and stochastic approximation, but understanding
the simultaneous effect on exploration has been largely lacking.
Recently, significant progress has been made in establishing global convergence results for policy
gradient (PG) methods. For example, it has been shown that using exact gradients, Softmax PG
converges to a globally optimal policy asymptotically as the number of iterations tgoes to infinity
[1]. Subsequent work has demonstrated that the asymptotic rate of convergence is O(1/t)[23], albeit
with problem and initialization dependent constants [ 22,17]. The rate and constant dependence in
the true gradient setting have been improved via several techniques, including entropy regularization
[23], normalization [21], and using natural gradient (mirror descent) [1, 6, 15].
Unfortunately, in the online stochastic setting, where the policy gradient has to be estimated using
the current policy to collect samples, these accelerated methods all obtain worse asymptotic results
than the standard Softmax PG [ 20], failing to converge to a global optimum without careful design
choices [ 19]. Yet in the same setting, standard Softmax PG has been shown to succeed in its simplest
form, provided only that a sufficiently small learning constant rate η∈Θ(1) is used [24].
For stochastic gradient based methods, decaying or sufficiently small learning rates are used by almost
all current approaches, motivated by classical convergence analyses from stochastic optimization
[28,12,39,38,9,37,36,24,8]. Stationary point convergence is guaranteed for learning rates
sufficiently small with respect to the smoothness of the objective function, while also decaying to
zero at a precise rate if noise in the gradient estimator persists. In addition to appropriate learning rate
control, many other techniques have been developed to control the effects of gradient noise, including
regularization [ 38,9], variance reduction [ 37], and carefully considering growth conditions [ 36,24].
The technical challenges we face in the current study can be understood in the following aspects:
(1)Using an arbitrarily large constant learning rate for online stochastic gradient optimization
immediately renders the smoothness and noise control techniques mentioned above inapplicable.
(2)With any constant learning rate η >0, the question of whether oscillation or convergence will
ultimately occur needs to be addressed before even considering whether any convergence is to a
global optimum. This additional level of complexity arises because the optimization objective is not
necessarily improved monotonically in expectation. Finally, (3)The gradient bandit algorithm does
not use any exploration bonus, which means that new techniques are required to demonstrate that it
adequately balances the exploration-exploitation trade-off.
In this paper, we resolve the above difficulties by uncovering intriguing exploration properties of
stochastic gradient when using any constant learning rate. In particular, we establish the following.
(i)In the stochastic online setting, with probability 1, the stochastic gradient bandit algorithm will
not keep sampling any single action forever, implying that it will exhibit a minimal form of
exploration without any further modification. This asymptotic event (as t→ ∞ ) happens with
probability 1and holds for any constant learning rate η >0.
(ii)This result can then be leveraged to show that, as a consequence, given any constant learning rate,
the stochastic gradient bandit algorithms will converge to the globally optimal policy as t→ ∞ ,
with probability 1. That is, the probability of sub-optimal actions decays to 0, even though some
of them are taken infinitely often asymptotically.
2 Setting and Background
We consider the stochastic multi-armed bandit problem [ 16], specified by Kactions and a true mean
reward vector r∈RK, where for each action a∈[K]:={1,2, . . . , K },
r(a) =ZRmax
−Rmaxx·Pa(x)µ(dx), (1)
where Rmax>0is the reward range, µis a finite measure over [−Rmax, Rmax], and Pa(x)≥0is
the probability density function with respect to µ. We use Rato denote the reward distribution for
2Algorithm 1 Gradient bandit algorithm (without baselines)
Input: initial parameters θ1∈RK, learning rate η >0.
Output: policies πθt= softmax( θt).
while t≥1do
Sample an action at∼πθt(·)and observe reward Rt(at)∼Pat.
foralla∈[K]do
ifa=atthen
θt+1(a)←θt(a) +η·(1−πθt(a))·Rt(at).
else
θt+1(a)←θt(a)−η·πθt(a)·Rt(at).
end if
end for
end while
action adefined by the density Paand base measure µ. The goal is to find a policy πθ∈[0,1]Kto
achieve high expected reward,
max
θ∈RKπ⊤
θr, (2)
where πθis parameterized by θ∈RK.
The gradient bandit algorithm. A natural idea to optimize Eq. (2) is to use stochastic gradient
ascent, which is shown in Algorithm 1 and known as the gradient bandit algorithm [ 31, Section 2.8].
In Algorithm 1, in each iteration t≥1, the probability of pulling arm a∈[K]is given as
πθt(a) = [softmax( θt)](a):=exp{θt(a)}P
a′∈[K]exp{θt(a′)},for all a∈[K], (3)
where θt∈RKis the parameter vector to be updated. The following proposition shows that
Algorithm 1 is an instance of stochastic gradient ascent with an unbiased gradient estimator [ 31,24].
Proposition 1 (Proposition 2.3 of [24]) .Algorithm 1 is equivalent to the following update,
θt+1←θt+η·d π⊤
θtˆrt
dθt=θt+η· 
diag(πθt)−πθtπ⊤
θt
ˆrt, (4)
whereEthd π⊤
θtˆrt
dθti
=d π⊤
θtr
dθt, andEt[·]is defined with respect to randomness from on-policy sampling
at∼πθt(·)and reward sampling Rt(at)∼Pat. The Jacobian of θ7→πθ:= softmax( θ)is d πθ
dθ⊤=diag(πθ)−πθπ⊤
θ∈RK×K, and ˆrt(a):=I{at=a}
πθt(a)·Rt(a)for all a∈[K]is the
importance sampling (IS) estimator, and we set Rt(a) = 0 for all a̸=at.
Known results on the convergence of the gradient bandit algorithm. Since Eq. (2) corresponds to
a smooth non-concave maximization problem over θ∈RK[23], using Algorithm 1 with decaying
learning rates is sufficient to guarantee convergence to a stationary point [ 28,25,12,39]. However,
this is insufficient to ensure the globally optimal solution of Eq. (2) is reached, since there exist
multiple stationary points. More recently, guarantees of convergence to a globally optimal policy have
been developed for PG methods in the true gradient setting [ 1,23], where the algorithm has access
to exact mean rewards. These results were later extended to achieve global convergence guarantees
(almost surely) in the stochastic setting [ 38,9,37,36,24,18]. However, these extended results
have required decaying or sufficiently small learning rates, motivated by exploiting smoothness and
combating the inherent noise in stochastic gradients.
Despite these previous assumptions, there exists empirical and theoretical evidence that using a
large learning rate in the stochastic gradient bandit algorithm is a viable option. For example, it
has been observed that softmax policies learn even with extremely large learning rates such as 214
[11]. For logistic regression on linearly separable data, the objective has an exponential tail and the
minimizer is unbounded, yet it has been shown that gradient descent with iteration dependent learning
rateη∈Θ(t)achieves accelerated O(1/t2)convergence [ 35]. Though the objective in Eq. (2) has
3similar properties, unlike logistic regression, the problem we are considering is non-concave, so the
same techniques cannot be directly applied. The most related results are from [ 24], which proved
that with a small problem specific constant learning rate, Algorithm 1 achieves convergence to a
globally optimal policy almost surely. However, as mentioned, the learning rate choices in [24] rely
on assumptions of (non-uniform) smoothness and noise growth conditions (their Lemmas 4.2, 4.3,
and 4.6), which cannot be directly applied here for a large learning rate.
Consequently, the use of large learning rates appear to render existing results and techniques inapplica-
ble. Furthermore, with a large constant learning rate, it is unclear whether Algorithm 1 will converge
to any stationary point, or the iterates will keep oscillating. If the algorithm does converge, it is also
not clear what effect large step-sizes have on exploration, and whether the algorithm will converge to
the optimal arm in such cases. Resolving these questions requires new results that characterize the
behavior of Algorithm 1, since the classical optimization and stochastic approximation convergence
theories are no longer applicable, as explained.
3 Asymptotic Global Convergence of Gradient Bandit Algorithm
We have seen that solving the non-concave maximization problem Eq. (2) using Algorithm 1 with any
constant (potentially large) learning rate requires ideas beyond classical optimization theory. Here,
we take a different perspective to investigate how Algorithm 1 samples actions. For analysis, we
make the following assumption about the reward distribution.
Assumption 1 (True mean reward has no ties) .For all i, j∈[K], ifi̸=j, then r(i)̸=r(j).
Remark 1. Removing Assumption 1 remains an open question for future work, while we believe that
Algorithm 1 works without Assumption 1. One piece of evidence to support this conjecture is that
even in the exact gradient setting, the set of initializations where Softmax PG approaches non-strict
one-hot policies has zero measure.
3.1 Failure Mode of Aggressive Updates
It has been observed that several accelerated PG methods in the true gradient setting, including natural
PG [13,1] and normalized PG [ 21], obtain worse results than standard softmax PG if combined with
online sampling at∼πθt(·)using constant learning rates [ 20]. The failure mode in these cases is
that the update is too aggressive and commits to a sub-optimal arm without sufficiently exploring
all arms. This results in a non-trivial probability of sampling one action forever, i.e., there exists
a potentially sub-optimal action a∈[K], such that with some constant probability, at=afor all
t≥1. Such an outcome implies that πθt(a)→1ast→ ∞ [20, Theorem 3]. Since a∈[K]could
be a sub-optimal action with r(a)< r(a∗) = max a∈[K]r(a), this results in a lack of exploration,
and consequently, methods such as natural PG and normalized PG are not guaranteed to converge to
the optimal action a∗:= arg maxa∈[K]r(a)with probability 1.
3.2 Stochastic Gradient Automatically Avoids Lack of Exploration
Our first key finding is that Algorithm 1 does not keep sampling one action forever, no matter how
large the constant learning rate is. This property avoids the problem of a lack of exploration, in the
sense that Algorithm 1 will at least explore more than one action infinitely often. At first glance, this
might not seem like a strong property, since the algorithm might somehow explore only sub-optimal
actions forever. However, we will argue below that this property coupled with additional arguments
is sufficient to guarantee convergence to the globally optimal policy.
Let us now formally prove the above property. By Algorithm 1, for all a∈[K], for all t≥1,
θt+1(a)←θt(a) +η·(1−πθt(a))·Rt(a),ifat=a,
−η·πθt(a)·Rt(at),otherwise .(5)
(6)
We define Nt(a)as the number of times action a∈[K]is sampled up to iteration t≥1, i.e.,
Nt(a):=tX
s=1I{as=a}, (7)
4and its asymptotic limit N∞(a):= lim t→∞Nt(a), which could possibly be infinity. For all a∈[K],
we have either N∞(a) =∞orN∞(a)<∞, meaning that a∈[K]is sampled infinitely often or
only finitely many times asymptotically. First, we prove the following Lemma 1, which shows that if
an action a∈[K]is sampled only finitely many times as t→ ∞ , then the parameter corresponding
to action ais also finite, i.e., supt≥1|θt(a)|<∞.
Lemma 1. Using Algorithm 1 with any constant η∈Θ(1) , ifN∞(a)<∞for an action a∈[K],
then we have, almost surely,
sup
t≥1θt(a)<∞,andinf
t≥1θt(a)>−∞. (8)
Lemma 1 will be used multiple times in the subsequent convergence arguments.
Proof sketch. Since we assume action a∈[K]is sampled finitely many times, the update given
in the case depicted by Eq. (5) happens finitely many times. Each update is bounded since the
sampled reward is in [−Rmax, Rmax]by Eq. (1), and the learning rate is a constant, i.e., η∈Θ(1) .
In Algorithm 1, θt(a)is still updated even when at̸=a, with the corresponding update given
by the case depicted by Eq. (6). Therefore, whether θt(a)is bounded depends on the cumulative
probabilityPt
s=1πθs(a)being summable as t→ ∞ . According to the extended Borel-Cantelli
lemma (Lemma 3), we have, almost surely,
nX
t≥1πθt(a) =∞o
={N∞(a) =∞}, (9)
which implies (by taking complements) thatP
t≥1πθt(a)<∞if and only if N∞(a)<∞. There-
fore, if a∈[K]is sampled finitely often, θt(a)will be updated in a bounded manner (using Eqs. (5)
and (6)) as t→ ∞ , hence establishing Lemma 1. Detailed proofs for this lemma, as well as for all
other results in this paper can be found in the appendix.
Given Lemma 1, we can then establish the above-mentioned finding about the exploration effect
of Algorithm 1 in Lemma 2.
Lemma 2 (Avoiding a lack of exploration) .Using Algorithm 1 with any η∈Θ(1) , there exists at
least a pair of distinct actions i, j∈[K]andi̸=j, such that, almost surely,
N∞(i) =∞,andN∞(j) =∞. (10)
Proof sketch. The argument for the existence of one such action is straightforward, since by the
pigeonhole principle, if there are finitely many actions, i.e., K <∞, there must be at least one action
i∈[K]that is sampled infinitely often as t→ ∞ .
The argument for the existence of a second such action is by contradiction. Suppose that all the
other actions j∈[K]withj̸=iare sampled only finitely many times as t→ ∞ . According
to Lemma 1, their corresponding parameters must remain finite, i.e., supt≥1|θt(j)|<∞for all
j∈[K]withj̸=i. Now consider θt(i). By assumption, the second update case for this parameter,
Eq. (6), happens only finitely often, since Eq. (6) can only occur when at̸=i. Therefore, the key
question is whether the cumulative probabilityPt
s=1(1−πθs(i))involved in the first case of the
update, Eq. (5), is summable as t→ ∞ . Note thatPt
s=1(1−πθs(i)) =Pt
s=1P
j̸=iπθs(j), which
is indeed summable as t→ ∞ , by the assumption and Eq. (9). This implies that action i, which is
sampled infinitely often, achieves a parameter magnitude, supt≥1|θt(i)|<∞, that remains bounded
ast→ ∞ . Using the softmax parameterization Eq. (3) in the above argument, we conclude that for
alla∈[K],inft≥1πθt(a)>0, i.e., every action’s probability remains bounded away from zero, and
hence is not summable. Using Eq. (9), this implies that every action is sampled infinitely often, which
contradicts the assumption that only action iis sampled infinitely often as t→ ∞ .
Discussion. Lemma 2 implies that Algorithm 1 is not an aggressive method in the sense of [ 20],
no matter how large the learning rate is, as long as it is constant, i.e., η∈Θ(1) . According to [ 20,
Theorem 7], even if we fix the sampling in Algorithm 1 to a sub-optimal action a∈[K]forever, i.e.,
at=afor all t≥1, its probability will not approach 1faster than O(1/t), i.e., 1−πθt(a)∈Ω(1/t).
This means that there must be at least one another action a′∈[K]witha′̸=a, such that a′will also
be sampled infinitely often. A more intuitive explanation is that the (1−πθt(a))term in Eq. (5) will
5be near 0, which slows the speed of committing to a deterministic policy on awhenever πθt(a)is
close to 1, which encourages exploration. Such natural exploratory behavior arises in Algorithm 1
because of the softmax Jacobian diag(πθ)−πθπ⊤
θin the update shown in Proposition 1, which
determines the growth order of θt(a)for all a∈[K]ast→ ∞ , making the effect of a constant
learning rate η∈Θ(1) asymptotically inconsequential.
3.3 Warm up: Global Asymptotic Convergence when K= 2
We now consider the simplest case, where we have only two possible actions. According to Lemma 2,
each of the two actions must be sampled infinitely often as t→ ∞ . We now illustrate the second
key result, that for both actions a∈[K], the random sequence {θt(a)}t≥1follows the direction of
the expected gradient for sufficiently large t≥1almost surely. The proof uses a technique that has
been previously used in [ 19,24] for small learning rates, but here we observe that the same technique
continues to work for Algorithm 1 no matter how large the learning rate is, as long as η∈Θ(1) .
Theorem 1. LetK= 2andr(1)> r(2). Using Algorithm 1 with any η∈Θ(1) , we have, almost
surely, πθt(a∗)→1ast→ ∞ , where a∗:= arg maxa∈[K]r(a)(equal to Action 1in this case).
Proof sketch. According to Lemma 2, N∞(1) = N∞(2) = ∞. Denote the the reward gap as
∆:=r(a∗)−max a̸=a∗r(a)>0, which becomes ∆ = r(1)−r(2)for two actions. Since the
stochastic gradient is unbiased (Proposition 1), we have, for all t≥1(detailed calculations omitted),
Et[θt+1(a∗)] =θt(a∗) +η·πθt(a∗)· 
r(a∗)−π⊤
θtr
(11)
=θt(a∗) +η·πθt(a∗)·∆·(1−πθt(a∗))> θt(a∗). (12)
A similar calculation shows that,
Et[θt+1(2)] = θt(2)−η·πθt(2)·∆·(1−πθt(2))< θt(2), (13)
which means that θt(a∗)is monotonically increasing in expectation and θt(2)is monotonically
decreasing in expectation. In other words, {θt(a∗)}t≥1is a sub-martingale, while {θt(2)}t≥1
is a super-martingale. However, since θt∈RKis unbounded, Doob’s martingale convergence
results cannot be directly applied, so we pursue a different argument. Following [ 19,24], given an
action a∈[K], we define Pt(a):=Et[θt+1(a)]−θt(a)as the “progress”, and define Wt(a):=
θt(a)−Et−1[θt(a)]as the “noise”, where θt(a) =Wt(a) +Pt−1(a) +θt−1(a). By recursion we
can determine that,
θt(a) =E[θ1(a)] +tX
s=1Ws(a) +t−1X
s=1Ps(a), (14)
i.e.,θt(a)is the result of “cumulative progress” and “cumulative noise”. According to [ 24, Theorem
C.3], the cumulative noise term can be bounded by using martingale concentration, where the order of
the corresponding confidence interval is smaller than the order of the cumulative progress. Therefore,
the summation will always be determined by the cumulative progress as t→ ∞ . According
to the calculations in Eqs. (12) and (13), we have Pt(a∗)>0andPt(2)<0, both of which
are not summable. As a result, θt(a∗)→ ∞ andθt(2)→ −∞ ast→ ∞ , which implies that
πθt(a∗)
πθt(2)= exp{θt(a∗)−θt(a)} → ∞ , hence πθt(a∗)→1ast→ ∞ .
3.4 Global Asymptotic Convergence for all K≥2
The illustrative two-action case shows that if π⊤
θtr∈(r(2), r(a∗))and if both actions are sampled
infinitely often, then we have, almost surely θt(a∗)→ ∞ andθt(2)→ −∞ ast→ ∞ . However, the
question at the beginning of Section 3.2 remains: when K > 2, if the two actions sampled infinitely
often in Lemma 2 are both sub-optimal, will that result in a similar failure mode to the one described
in Section 3.1? The answer is no, which follows from our third key finding, which is based on another
contradiction-based argument that establishes almost sure convergence to a globally optimal policy in
the general K > 2case.
Theorem 2. Given K≥2, using Algorithm 1 with any η∈Θ(1) , we have, almost surely, πθt(a∗)→
1ast→ ∞ , where a∗= arg maxa∈[K]r(a)is the optimal action.
6Proof sketch. We consider two cases: N∞(a∗)<∞andN∞(a∗) =∞, corresponding to
whether the optimal action is sampled finitely or infinitely often as t→ ∞ . We argue that the
first case ( N∞(a∗)<∞) is impossible, while for the second case ( N∞(a∗) =∞) we prove that
θt(a∗)−θt(a)→ ∞ for all a∈[K]withr(a)< r(a∗), which implies πθt(a∗)→1ast→ ∞ .
First case. Suppose that N∞(a∗)<∞. We argue that this is impossible via contradiction. Given the
assumption and Lemma 2 we know there must be at least two other sub-optimal actions i1, i2∈[K],
i1̸=i2, such that N∞(i1) =N∞(i2) =∞. In particular, let i1= arg mina∈[K],N∞(a)=∞r(a)and
i2= arg maxa∈[K],N∞(a)=∞r(a), hence r(i1)< r(i2)< r(a∗). By Lemma 6 (see Appendix) we
will also have r(i1)< π⊤
θtr < r (i2)for sufficiently large t≥1, which implies for action i1,
Et[θt+1(i1)] =θt(i1) +η·πθt(i1)· 
r(i1)−π⊤
θtr
< θt(i1), (15)
for sufficiently large t≥1, which further implies that supt≥1θt(i1)<∞. Meanwhile, for the
optimal action a∗, the assumption and Lemma 1 imply that inft≥1θt(a∗)>−∞. Combining these
two observations gives,
sup
t≥1πθt(i1)
πθt(a∗)= sup
t≥1exp{θt(i1)−θt(a∗)}<∞. (16)
On the other hand, since N∞(i1) =∞andN∞(a∗)<∞(by assumption), we then have
supt≥1exp{θt(i1)−θt(a∗)}=∞by Lemma 5 (see Appendix), which contradicts Eq. (16).
Second case. Suppose that N∞(a∗) =∞. We will argue that πθt(a∗)→1ast→ ∞ almost surely.
First, according to Lemma 2, there exists at least one sub-optimal action i1∈[K],i1̸=a∗, such
thatN∞(i1) =∞. Let i1= arg mina∈[K],N∞(a)=∞r(a). By Lemma 6 and the definition of a∗,
we have r(i1)< π⊤
θtr < r (a∗)for all sufficiently large t≥1. Since N∞(i1) =∞, using similar
calculations to Eqs. (13) and (14) in Theorem 1, we have, θt(i1)→ −∞ ast→ ∞ . We also have,
inft≥1θt(a∗)>−∞ ast→ ∞ . Hence,πθt(a∗)
πθt(i1)= exp{θt(a∗)−θt(i1)} → ∞ ast→ ∞ .
Define A∞:={a∈[K]|N∞(a) =∞}as the set of actions that are sampled infinitely often, and
note that |A∞| ≥2by Lemma 2. Sort the action indices in A∞according to their expected reward
values in descending order, i.e.,
r(a∗)> r(i|A∞|−1)> r(i|A∞|−2)>···> r(i2)> r(i1). (17)
Assumption 1 is used here to prevent two arms from having the same reward and thus guarantee the
inequalities are strict in Eq. (17). Next, using similar calculations as in Lemma 6, we have,
π⊤
θtr−r(i2)> πθt(a∗)·
r(a∗)−r(i2)−X
a−∈A−(i2)πθt(a−)
πθt(a∗)·(r(i2)−r(a−))
,(18)
whereA−(i2):={a−∈[K] :r(a−)< r(i2)}is the set of actions that have lower mean reward than
i2∈[K], and note that i1∈ A−(i2). Using the above definitions, we can conclude that i1is the only
arm in A−(i2)that has been sampled infinitely often. According to Lemma 5, for all a−∈ A−(i2)
witha−̸=i1, we have,πθt(a∗)
πθt(a−)→ ∞ ast→ ∞ , since N∞(a∗) =∞(by assumption) and
N∞(a−)<∞(by Eq. (17)). Therefore, for all sufficiently large t, the probability ratio in Eq. (18)
πθt(a∗)
πθt(a−)→ ∞ for all a−∈ A−(i2), which implies that, for all sufficiently large t≥1,
π⊤
θtr−r(i2)>0.5·πθt(a∗)· 
r(a∗)−r(i2)
>0. (19)
We have thus shown that π⊤
θtr > r (i2). Recall that we had previously proved that π⊤
θtr > r (i1).
Hence, we will apply this argument recursively: after this point, i2∈[K]will become the new
“i1∈[K]”, and a similar inequality to Eq. (13) will then hold for i2∈[K]from similar calculations
to Eqs. (13) and (14) in Theorem 1, establishing θt(i2)→ −∞ ast→ ∞ . This will imply that for
all sufficiently large t≥1,
π⊤
θtr−r(i3)>0.5·πθt(a∗)· 
r(a∗)−r(i3)
>0. (20)
Continuing the recursive argument, we can conclude for all actions a∈ A∞witha̸=a∗that
πθt(a∗)
πθt(a)→ ∞ ast→ ∞ . Meanwhile, for all actions a̸∈ A∞, Lemma 5 also shows thatπθt(a∗)
πθt(a)→ ∞
ast→ ∞ . Combining these two results yields the conclusion that for all sub-optimal actions a∈[K]
withr(a)< r(a∗)we haveπθt(a∗)
πθt(a)→ ∞ ast→ ∞ , which implies πθt(a∗)→1ast→ ∞ . Thus,
we have established almost sure convergence to the globally optimal policy.
7Discussion. Lemma 2 is important to prove that the optimal arm will be sampled infinitely often.
In particular, Lemma 2 guarantees |A∞| ≥2and the existence of i1in Eq. (15), which can then be
used to construct the contradiction in Eq. (16). Without Lemma 2, |A∞|might be equal to 1and
the failure mode in Section 3.1 can occur, resulting in Algorithm 1 not sampling the optimal action
infinitely often as t→ ∞ .
4 Simulation Study
To validate and enhance the theoretical findings, we ran experiments with a four action bandit
environment ( K= 4) with a true mean reward vector of r= (0.2,0.05,−0.1,−0.4)⊤∈R4. The
reward distribution Pafor arm 1≤a≤4is Gaussian, centered at r(a)and with a standard deviation
of0.1. The environment is chosen to illustrate various phenomenon, which we discuss after presenting
the results. The algorithm is Algorithm 1 with θ1=0∈RK.
For comparison, assuming that the random rewards belong to the [−1,1]interval, the only result
for the stochastic gradient bandit algorithm [ 24, Lemma 4.6] that allowed a constant learning rate
required that the learning rate be less than ηc=∆2
40·K3/2·R3max=9
128000≈0.00007 , where we used
∆ = 0 .15,K= 4, and Rmax= 1. While technically, the result does not apply to our case where
the reward distributions have unbounded support, the probability of the reward landing outside of
[−1,1]is in the order of 10−9. Choosing Rmaxto be larger, this probability falls extremely quickly,
which suggests that the above threshold is generous. For the experiments we use the learning rates
η∈ {1,10,100,1000}, that are several orders of magnitudes larger than ηc. For each learning rate,
we plot the outcome of 10runs, corresponding to different random seeds. Each run lasts 106(≈e14)
iterations. The log-suboptimality gaps for the 4×10cases are shown on Figures 1a-1d, where they
are plotted against the logarithm of time. Additional results for K= 2arms are shown in Appendix C.
Note that in this example small sub-optimality implies that the optimal arm is chosen with high
probability. In what follows, we discuss the results in the plots.
Asymptotic convergence. For the smaller learning rates of η= 1and10, all10seeds rapidly and
steadily converge, reaching a sub-optimality of e−14or less. For η= 100 and1000 , most of the runs
reach even small error even faster, but some runs are “stuck” even after 106steps. Note that this
does not contradict the theoretical result; nor do we suspect numerical issues. As seen for the case of
η= 100 , even after a long phase with little to no progress, a run can “recover” (see the grey curve).
In fact, it is reasonable to expect that the price of increasing the learning rate is larger variance; as
seen in these plots (subplots (a) and (b) are also attesting to this). Differences between learning rates
are further discussed below.
Non-monotone objective value. Using a very small learning rate guarantees monotonic improve-
ment (in expectation) in the policy’s expected reward [ 24]. Conversely, a large learning rate results in
non-monotonic evolution of the expected rewards {π⊤
θtr}t≥1, even in the final stages of convergence,
as can be seen clearly for the learning rates of η= 1and10in Figures 1a and 1b. For larger η, the
non-monotone behavior happens over longer periods and is less visible in the plots. This is because
using large learning rates causes the policy to rapidly increase the parameters for some action, after
which the gradient becomes small, limiting further progress.
Rate of convergence. This work establishes almost sure convergence to the globally optimal policy
ast→ ∞ without giving any particular rate of convergence. Figures 1a and 1b, where the log-log plot
has a slope of nearly −1, give some evidence that an O(1/t)asymptotic rate is achieved. In general,
such a rate cannot be improved in terms of t[14]. However, more work is needed to rigorously
quantify the asymptotic convergence rate.
Different learning rates. Two observations can be made from Figure 1 regarding the effect of
using different ηvalues: First , during the final stage of convergence when r(a∗)−π⊤
θtr≈0, using
larger ηresults in faster convergence on average. As ηincreases, the order of log (r(a∗)−π⊤
θtr)
also changes from e−14(η= 1), toe−20(η= 100 ), and e−200(η= 1000 ). We conjecture that the
asymptotic rate of convergence has an O(1/η)dependence. Second , using larger learning rates can
take a longer time to enter the final stage of convergence. When η= 1or10, all curves quickly enter
the final stage of r(a∗)−π⊤
θtr≈0. However, for larger ηvalues, 1/10runs ( η= 100 ) and 3/10runs
80 2 4 6 8 10 12 14
log(t)14
12
10
8
6
4
2
Log sub-optimality gap(a)η= 1.
0 2 4 6 8 10 12 14
log(t)20.0
17.5
15.0
12.5
10.0
7.5
5.0
2.5
Log sub-optimality gap (b)η= 10 .
0 2 4 6 8 10 12 14
log(t)30
25
20
15
10
5
0Log sub-optimality gap
(c)η= 100 .
0 2 4 6 8 10 12 14
log(t)300
250
200
150
100
50
0Log sub-optimality gap (d)η= 1000 .
Figure 1: Log sub-optimality gap, log (r(a∗)−π⊤
θtr), plotted against the logarithm of time, logt, in
a4-action problem with various learning rates, η. Each subplot shows a run with a specific learning
rate. The curves in a subplot correspond to 10 different random seeds. Theory predicts that essentially
all seeds will lead to a curve converging to zero ( −∞ in these plots). For a discussion of the results,
see the text.,
(η= 1000 ) result in r(a∗)−π⊤
θtrvalues far from 0even after 106iterations. These runs take orders
of magnitude more iterations to eventually achieve r(a∗)−π⊤
θtr≈0. These situations correspond to
the policy πθtgetting stuck near sub-optimal corners of the simplex, meaning that πθt(i)≈1for a
sub-optimal action i∈[K]withr(i)< r(a∗). In such cases, even Softmax PG with the true gradient
can remain stuck on a sub-optimal plateau for an extremely long time [ 23]. However, the reason why
larger learning rates lead to longer plateaus in the stochastic setting remains unclear.
Trade-offs and multi-stage chracterizations of convergence. Given the above observations, there
appears to exist a trade-off for η: larger ηvalues result in faster convergence during the final stage
where r(a∗)−π⊤
θtr≈0, but at the the cost of taking far longer to enter this final stage of convergence.
Since asymptotic convergence results are insufficient for explaining these subtleties in a satisfactory
manner, a more refined analysis that considers the different stages of convergence is required.
5 Conclusions and Future Directions
This work refines our understanding of stochastic gradient bandit algorithms by proving that it
converges to a globally optimal policy almost surely with anyconstant learning rate. Our new proof
strategy based on the asymptotics of sample counts opens new directions for better characterizing
exploration effects of stochastic gradient methods, while also suggesting interesting new questions.
Immediate next steps are to characterize the asymptotic rate of convergence under large constant
learning rates. Characterizing the multiple stages of convergence remains another interesting future
direction. One interesting possibility is that there might exist an optimal time-dependent scheme
forincreasing the learning rate (such as η∈O(logt)) to accelerate convergence, rather than use a
constant η∈O(1). This is corroborated by our experiments: As seen in Figure 1, small learning
rates perform better during the early stages of optimization, while larger learning rates achieve faster
convergence during the final stage. Other directions include extending our bandit results to the more
general RL setting [ 34], as well as extending our results for the softmax tabular parameterization to
handle function approximation [1].
9Limitations: While this work establishes a surprising asymptotic convergence result for any constant
learning rate, it does not shed light on the convergence rate, nor on the effect of different learning
rates on the convergence. Moreover, our analysis is limited to multi-armed bandits, and does not
immediately extend to the general RL setting. These aspects are the main limitations of this paper.
Broader impact: This is primarily theoretical work on a fundamental algorithm that is used broadly
in RL applications. We expect these results to improve the research community’s understanding of
the basic stochastic gradient bandit method.
Acknowledgments and Disclosure of Funding
Jincheng Mei would like to thank Ramki Gummadi for providing feedback on a draft of this
manuscript. Csaba Szepesvári and Dale Schuurmans gratefully acknowledge funding from the
Canada CIFAR AI Chairs Program, Amii and NSERC. Sharan Vaswani acknowledges the support
from the NSERC Discovery Grant (RGPIN-2022-04816).
References
[1]Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient
methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research ,
22(98):1–76, 2021.
[2]Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In
Conference on learning theory , pages 39–1. JMLR Workshop and Conference Proceedings, 2012.
[3]Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine learning , 47(2):235–256, 2002.
[4]Léon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP-
STAT’2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010
Keynote, Invited and Contributed Papers , pages 177–186. Springer, 2010.
[5] Leo Breiman. Probability . SIAM, 1992.
[6]Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural
policy gradient methods with entropy regularization. Operations Research , 70(4):2563–2578, 2022.
[7]Nicolò Cesa-bianchi and Claudio Gentile. Improved risk tail bounds for on-line algorithms. In Y . Weiss,
B. Schölkopf, and J. Platt, editors, Advances in Neural Information Processing Systems , volume 18. MIT
Press, 2005.
[8]Denis Denisov and Neil Walton. Regret analysis of a markov policy gradient algorithm for multi-arm
bandits. arXiv preprint arXiv:2007.10229 , 2020.
[9]Yuhao Ding, Junzi Zhang, and Javad Lavaei. Beyond exact gradients: Convergence of stochastic soft-max
policy gradient methods with entropy regularization. arXiv preprint arXiv:2110.10117 , 2021.
[10] David A. Freedman. On Tail Probabilities for Martingales. The Annals of Probability , 3(1):100 – 118,
1975.
[11] Shivam Garg, Samuele Tosatto, Yangchen Pan, Martha White, and A Rupam Mahmood. An alternate
policy gradient estimator for softmax policies. arXiv preprint arXiv:2112.11622 , 2021.
[12] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization , 23(4):2341–2368, 2013.
[13] Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems , pages
1531–1538, 2002.
[14] Tze Leung Lai, Herbert Robbins, et al. Asymptotically efficient adaptive allocation rules. Advances in
applied mathematics , 6(1):4–22, 1985.
[15] Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling
complexity, and generalized problem classes. Mathematical programming , 198(1):1059–1106, 2023.
[16] Tor Lattimore and Csaba Szepesvári. Bandit algorithms . Cambridge University Press, 2020.
10[17] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Softmax policy gradient methods can take
exponential time to converge. In Conference on Learning Theory , pages 3107–3110. PMLR, 2021.
[18] Michael Lu, Matin Aghaei, Anant Raj, and Sharan Vaswani. Towards principled, practical policy gradient
for bandits and tabular mdps. arXiv preprint arXiv:2405.13136 , 2024.
[19] Jincheng Mei, Wesley Chung, Valentin Thomas, Bo Dai, Csaba Szepesvari, and Dale Schuurmans. The
role of baselines in policy gradient optimization. Advances in Neural Information Processing Systems ,
35:17818–17830, 2022.
[20] Jincheng Mei, Bo Dai, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. Understanding the effect of
stochasticity in policy optimization. Advances in Neural Information Processing Systems , 34:19339–19351,
2021.
[21] Jincheng Mei, Yue Gao, Bo Dai, Csaba Szepesvari, and Dale Schuurmans. Leveraging non-uniformity in
first-order non-convex optimization. In International Conference on Machine Learning , pages 7555–7564.
PMLR, 2021.
[22] Jincheng Mei, Chenjun Xiao, Bo Dai, Lihong Li, Csaba Szepesvári, and Dale Schuurmans. Escaping the
gravitational pull of softmax. Advances in Neural Information Processing Systems , 33:21130–21140, 2020.
[23] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of
softmax policy gradient methods. In International Conference on Machine Learning , pages 6820–6829.
PMLR, 2020.
[24] Jincheng Mei, Zixin Zhong, Bo Dai, Alekh Agarwal, Csaba Szepesvari, and Dale Schuurmans. Stochastic
gradient succeeds for bandits. arXiv preprint arXiv:2402.17235 , 2024.
[25] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic ap-
proximation approach to stochastic programming. SIAM Journal on optimization , 19(4):1574–1609,
2009.
[26] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. arXiv preprint arXiv:2203.02155 , 2022.
[27] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.
Direct preference optimization: Your language model is secretly a reward model. Advances in Neural
Information Processing Systems , 36, 2024.
[28] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics , pages 400–407, 1951.
[29] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy
optimization. In International conference on machine learning , pages 1889–1897, 2015.
[30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
[31] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction . MIT Press, 2018.
[32] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. Advances in neural information processing systems ,
12, 1999.
[33] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika , 25(3-4):285–294, 1933.
[34] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning , 8:229–256, 1992.
[35] Jingfeng Wu, Peter L Bartlett, Matus Telgarsky, and Bin Yu. Large stepsize gradient descent for logistic
loss: Non-monotonicity of the loss improves optimization efficiency. arXiv preprint arXiv:2402.15926 ,
2024.
[36] Rui Yuan, Robert M Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla
policy gradient. In International Conference on Artificial Intelligence and Statistics , pages 3332–3380.
PMLR, 2022.
11[37] Junyu Zhang, Chengzhuo Ni, Csaba Szepesvari, Mengdi Wang, et al. On the convergence and sample
efficiency of variance-reduced policy gradient method. Advances in Neural Information Processing Systems ,
34:2228–2240, 2021.
[38] Junzi Zhang, Jongho Kim, Brendan O’Donoghue, and Stephen Boyd. Sample efficient reinforcement
learning with reinforce. arXiv preprint arXiv:2010.11364 , 2020.
[39] Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar. Global convergence of policy gradient methods
to (almost) locally optimal policies. SIAM Journal on Control and Optimization , 58(6):3586–3612, 2020.
12A Proofs
Lemma 1. Using Algorithm 1 with any constant η∈Θ(1) , ifN∞(a)<∞for an action a∈[K],
then we have, almost surely,
sup
t≥1θt(a)<∞,andinf
t≥1θt(a)>−∞. (21)
Proof. Suppose N∞(a)<∞for an action a∈[K]. According to Algorithm 1,
θt+1(a)←θt(a) +η·(1−πθt(a))·Rt(a),ifat=a ,
−η·πθt(a)·Rt(at), otherwise ,(22)
and let
It(a):=1,ifat=a ,
0,otherwise .(23)
According to Eq. (22), we have, for all t≥1,
θt(a)−θ1(a) =t−1X
s=1Is(a)·η·(1−πθs(a))·Rs(a) +t−1X
s=1(1−Is(a))·(−η)·πθs(a)·Rs(as).
(24)
Using triangle inequality, we have,
|θt(a)−θ1(a)| ≤t−1X
s=1Is(a)·η·(1−πθs(a))·Rs(a)+t−1X
s=1(1−Is(a))·(−η)·πθs(a)·Rs(as)
(25)
≤η·Rmax·t−1X
s=1Is(a) +η·Rmax·t−1X
s=1πθs(a) (26)
=η·Rmax·
Nt−1(a) +t−1X
s=1πθs(a)
. (27)
By assumption, we have,
N∞(a):= lim
t→∞Nt(a)<∞. (28)
According to the extended Borel-Cantelli Lemma 3, we have, almost surely,
∞X
t=1πθt(a):= lim
t→∞tX
s=1πθs(a)<∞. (29)
Combining Eqs. (25), (28) and (29), we have, almost surely,
sup
t≥1|θt(a)−θ1(a)|<∞, (30)
which implies that, almost surely,
sup
t≥1|θt(a)| ≤sup
t≥1|θt(a)−θ1(a)|+|θ1(a)|<∞.
Lemma 2 (Avoiding lack of exploration) .Using Algorithm 1 with any η∈Θ(1) , there exists at least
a pair of distinct actions i, j∈[K]andi̸=j, such that, almost surely,
N∞(i) =∞,andN∞(j) =∞. (31)
13Proof. First, we have, for all t≥1,
t=tX
s=1X
a∈[K]Is(a)X
a∈[K]It(a) = 1 for all t≥1
(32)
=X
a∈[K]tX
s=1Is(a) (33)
=X
a∈[K]Nt(a). (34)
By pigeonhole principle, there exists at least one action i∈[K], such that, almost surely,
N∞(i):= lim
t→∞Nt(i) =∞. (35)
We argue the existence of another action by contradiction. Suppose for all the other actions j∈[K]
andj̸=i, we have N∞(j)<∞. According to the extended Borel-Cantelli Lemma 3, we have,
almost surely,
∞X
t=1πθt(j):= lim
t→∞tX
s=1πθs(j)<∞. (36)
According to the update Eq. (22), we have, for all t≥1,
θt(i)−θ1(i) =t−1X
s=1Is(i)·η·(1−πθs(i))·Rs(i) +t−1X
s=1(1−Is(i))·(−η)·πθs(i)·Rs(as).
(37)
By triangle inequality, we have,
|θt(i)−θ1(i)| ≤t−1X
s=1Is(i)·η·(1−πθs(i))·Rs(i)+t−1X
s=1(1−Is(i))·(−η)·πθs(i)·Rs(as)
(38)
≤η·Rmax·t−1X
s=1(1−πθs(i)) +η·Rmax·t−1X
s=1(1−Is(i)) (39)
=η·Rmax·t−1X
s=1X
j̸=iπθs(j) +t−1X
s=1X
j̸=iIs(j)
(40)
=η·Rmax·X
j̸=it−1X
s=1πθs(j) +X
j̸=iNt−1(j)
. (41)
Combing Eqs. (36) and (38) and the assumption of N∞(j)<∞for all j̸=i, we have, almost surely,
sup
t≥1|θt(i)| ≤sup
t≥1|θt(i)−θ1(i)|+|θ1(i)|<∞. (42)
Since N∞(j)<∞for all j̸=iby assumption, and according to Lemma 1, we have, almost surely,
sup
t≥1|θt(j)|<∞. (43)
Combining Eqs. (42) and (43), we have, for all action a∈[K],
sup
t≥1|θt(a)|<∞, (44)
which implies that, there exists c >0andc∈O(1), such that, for all a∈[K],
inf
t≥1πθt(a) = inf
t≥1exp{θt(a)}P
a′∈[K]exp{θt(a′)}≥c >0. (45)
14Therefore, for all action a∈[K],
∞X
t=1πθt(a):= lim
t→∞tX
s=1πθs(a) (46)
≥lim
t→∞tX
s=1c (47)
= lim
t→∞t·c (48)
=∞. (49)
According to the extended Borel-Cantelli Lemma 3, we have, almost surely, for all action a∈[K],
N∞(a) =∞, (50)
which is a contradiction with the assumption of N∞(j)<∞for all the other actions j∈[K]with
j̸=i. Therefore, there exists another action j∈[K]withj̸=i, such that N∞(j) =∞.
Theorem 1. LetK= 2andr(1)> r(2). Using Algorithm 1 with any η∈Θ(1) , we have, almost
surely, πθt(a∗)→1ast→ ∞ , where a∗:= arg maxa∈[K]r(a)(equal to Action 1in this case).
Proof. The proof uses the same notations as of [24, Theorem 5.1].
According to Lemma 2, we have, N∞(1) = ∞andN∞(2) = ∞, i.e., both of the two actions are
sampled for infinitely many times as t→ ∞ .
LetFtbe the σ-algebra generated by a1,R1(a1),···,at−1,Rt−1(at−1):
Ft=σ({a1, R1(a1),···, at−1, Rt−1(at−1)}). (51)
Note that θtandIt(defined by Eq. (23)) are Ft-measurable for all t≥1. LetEt[·]denote the
conditional expectation with respect to Ft:Et[X] =E[X|Ft]. Define the following notations,
Wt(a):=θt(a)−Et−1[θt(a)], (“noise” ) (52)
Pt(a):=Et[θt+1(a)]−θt(a). (“progress” ) (53)
For each action a∈[K], fort≥2, we have the following decomposition,
θt(a) =Wt(a) +Pt−1(a) +θt−1(a). (54)
By recursion we can determine that,
θt(a) =E[θ1(a)] +tX
s=1Ws(a) +t−1X
s=1Ps(a), (55)
while we also have,
θ1(a) =θ1(a)−E[θ1(a)]| {z }
W1(a)+E[θ1(a)], (56)
andE[θ1(a)]accounts for potential randomness in initializing θ1∈RK. According to Proposition 1,
we have,
Pt(a) =Et[θt+1(a)]−θt(a) =η·πθt(a)· 
r(a)−π⊤
θtr
, (57)
which implies that, for all t≥1,
Pt(a∗)>0> Pt(2). (58)
Next, we show thatPt
s=1Ps(a∗)→ ∞ ast→ ∞ by contradiction. Suppose that,
∞X
t=1Pt(a∗)<∞. (59)
15For the optimal action a∗= 1,
tX
s=1Ps(a∗) =tX
s=1η·πθs(a∗)·(r(a∗)−π⊤
θsr) (60)
=η·∆·tX
s=1πθs(a∗)·(1−πθs(a∗)), (61)
where ∆:=r(a∗)−max a̸=a∗r(a) =r(1)−r(2)>0is the reward gap. Denote that, for all t≥1,
Vt(a∗):=5
18·t−1X
s=1πθs(a∗)·(1−πθs(a∗)). (62)
According to Lemma 8 (using Eqs. (58) to (60) and (62)), we have, almost surely,
sup
t≥1|θt(a∗)|<∞. (63)
For the sub-optimal action (Action 2), we have,
tX
s=1Ps(2) =tX
s=1η·πθs(2)·(r(2)−π⊤
θsr) (64)
=−η·∆·tX
s=1πθs(2)·(1−πθs(2)) (65)
=−tX
s=1Ps(a∗), (66)
and also denote, for all t≥1,
Vt(2):=5
18·t−1X
s=1πθs(2)·(1−πθs(2)) = Vt(a∗). (67)
According to Lemma 8 (using Eqs. (58), (59), (64) and (67)), we have, almost surely,
sup
t≥1θt(2)<∞. (68)
Combining Eqs. (63) and (68), there exists c >0andc∈O(1), such that, for all a∈ {a∗,2},
inf
t≥1πθt(a) = inf
t≥1exp{θt(a)}P
a′∈[K]exp{θt(a′)}≥c >0, (69)
which implies that,
∞X
t=1πθt(a∗)·(1−πθt(a∗)):= lim
t→∞tX
s=1πθs(a∗)·(1−πθs(a∗)) (70)
≥lim
t→∞tX
s=1c·c (71)
= lim
t→∞t·c2(72)
=∞, (73)
which is a contradiction with the assumption of Eq. (59). Therefore, we have,
tX
s=1Ps(a∗)→ ∞ ,ast→ ∞ . (74)
According to Lemma 9 (using Eqs. (58), (60), (62) and (74)), we have, almost surely,
θt(a∗)→ ∞ ,ast→ ∞ . (75)
16Similarly, according to Lemma 10 (using Eqs. (58), (64), (67) and (74)), we have, almost surely,
lim
t→∞θt(2) =−∞. (76)
Note that,
πθt(a∗) =πθt(a∗)
πθt(2) + πθt(a∗)(77)
=1
exp{θt(2)−θt(a∗)}+ 1. (78)
Combining Eqs. (75) to (77), we have, almost surely,
πθt(a∗)→1,ast→ ∞ .
Theorem 2. Under Assumption 1, given K≥2, using Algorithm 1 with any η∈Θ(1) , we have,
almost surely, πθt(a∗)→1ast→ ∞ , where a∗= arg maxa∈[K]r(a)is the optimal action.
Proof. Similar to the proof of Theorem 1, we define Ftto be the σ-algebra generated by a1,R1(a1),
···,at−1,Rt−1(at−1)i.e.
Ft=σ({a1, R1(a1),···, at−1, Rt−1(at−1)}). (79)
Note that θtandIt(defined by Eq. (23)) are Ft-measurable for all t≥1. LetEt[·]denote the
conditional expectation with respect to Ft:Et[X] =E[X|Ft]. Define the following notations,
Wt(a):=θt(a)−Et−1[θt(a)], (“noise” ) (80)
Pt(a):=Et[θt+1(a)]−θt(a). (“progress” ) (81)
For each action a∈[K], fort≥2, we have the following decomposition,
θt(a) =Wt(a) +Pt−1(a) +θt−1(a). (82)
By recursion we can determine that,
θt(a) =E[θ1(a)] +tX
s=1Ws(a) +t−1X
s=1Ps(a), (83)
while we also have,
θ1(a) =θ1(a)−E[θ1(a)]| {z }
W1(a)+E[θ1(a)], (84)
andE[θ1(a)]accounts for potential randomness in initializing θ1∈RK. For an action a∈[K],
A+(a):=
a+∈[K] :r(a+)> r(a)	
, (85)
A−(a):=
a−∈[K] :r(a−)< r(a)	
. (86)
Define A∞as the set of actions which are sampled for infinitely many times as t→ ∞ , i.e.,
A∞:={a∈[K]|N∞(a) =∞}. (87)
First part. We show that N∞(a∗) =∞by contradiction.
Suppose a∗̸∈ A∞i.e.N∞(a∗)<∞. According to Lemma 2, we have, |A∞| ≥2. Since a∗̸∈ A∞
by assumption, there must be at least two other sub-optimal actions i1, i2∈[K],i1̸=i2, such that,
N∞(i1) =N∞(i2) =∞. (88)
In particular, define
i1:= arg min
a∈[K],
N∞(a)=∞r(a), (89)
i2:= arg max
a∈[K],
N∞(a)=∞r(a). (90)
17Using the update for arm i1, we know that, for all s≥1,
Ps(i1) =η·πθs(i1)·(r(i1)−π⊤
θsr) (91)
By definition and because of Assumption 1, we have that r(i1)< r(i2)< r(a∗). Furthermore,
according to Lemma 6, we have, for sufficiently large τ≥1,
r(i1)< π⊤
θtr < r (i2), (92)
which implies that after some large enough τ≥1,
tX
s=τPs(i1) =tX
s=τη·πθs(i1)·(r(i1)−π⊤
θsr) (93)
<0. (by Eq. (92) ) (94)
r(i1)−π⊤
θsr=X
a̸=i1πθs(a) [r(i1)−r(a)]
=−X
a+∈A+(i1)πθs(a+)·(r(a+)−r(i1)) +X
a−∈A−(i1)πθs(a−)·(r(i1)−r(a−)).
(95)
From Eq. (89), we have,
A∞⊆ A+(i1)∪ {i1}, (96)
which implies that, for all a−∈ A−(i1), we have,
N∞(a−)<∞. (97)
Note that, by definition, i2∈ A+(i1), and
N∞(i2) =∞. (98)
In order to bound Eq. (95) using the above relations, note that,
X
a−∈A−(i1)πθs(a−)P
a+∈A+(i1)πθs(a+)·(r(a+)−r(i1))·(r(i1)−r(a−)) (99)
<X
a−∈A−(i1)πθs(a−)
πθs(i2)·(r(i2)−r(i1))·(r(i1)−r(a−))
(Fewer terms in the denominator)
By Lemma 5, for large enough τ≥1, for all a−,πθs(a−)
πθs(i2)≤1
|A−(i1)|·r(i2)−r(i1)
r(i1)−r(a−). Hence,
≤X
a−∈A−(i1)1
2·1
|A−(i1)|·r(i2)−r(i1)
r(i1)−r(a−)·r(i1)−r(a−)
r(i2)−r(i1)(100)
=1
2(101)
=⇒X
a−∈A−(i1)πθs(a−)·(r(i1)−r(a−))≤1
2X
a+∈A+(i1)πθs(a+)·(r(a+)−r(i1)).(102)
Combining Eqs. (94), (95) and (102), we have,
tX
s=τPs(i1) =tX
s=τη·πθs(i1)·(r(i1)−π⊤
θsr) (103)
≤ −η
2·tX
s=τπθs(i1)X
a+∈A+(i1)πθs(a+)·(r(a+)−r(i1)) (104)
≤ −η·∆
2·tX
s=τπθs(i1)X
a+∈A+(i1)πθs(a+), (105)
18where
∆:= min
i,j∈[K], i̸=j|r(i)−r(j)|>0. (by Assumption 1 ) (106)
Bounding the variance for arm i1, we have that for all large enough τ≥1,
Vt(i1):=5
18·t−1X
s=τπθs(i1)·(1−πθs(i1)) (107)
=5
18·t−1X
s=τπθs(i1)·
X
a−∈A−(i1)πθs(a−) +X
a+∈A+(i1)πθs(a+)
 (108)
Using similar calculations as for the progress term, we have,
X
a−∈A−(i1)πθs(a−)P
a+∈A+(i1)πθs(a+)<X
a−∈A−(i1)πθs(a−)
πθs(i2)(Fewer terms in the denominator)
By Lemma 5, for large enough τ≥1, for all a−,πθs(a−)
πθs(i2)≤13
51
|A−(i1)|. Hence,
≤X
a−∈A−(i1)13
5·1
|A−(i1)|(109)
=13
5(110)
=⇒X
a−∈A−(i1)πθs(a−)≤13
5X
a+∈A+(i1)πθs(a+) (111)
Combining Eqs. (108) and (111), we have that for large enough τ≥1,
Vt(i1) =5
18·t−1X
s=τπθs(i1)·(1−πθs(i1)) (112)
≤t−1X
s=τπθs(i1)X
a+∈A+(i1)πθs(a+). (113)
According to Lemma 12 (using Eqs. (94), (105) and (113)), we have, almost surely,
sup
t≥1θt(i1)<∞. (114)
Since N∞(a∗)<∞by assumption, and according to Lemma 1, we have, almost surely,
inf
t≥1θt(a∗)>−∞. (115)
Combining Eqs. (114) and (115), we have,
sup
t≥1πθt(i1)
πθt(a∗)= sup
t≥1exp{θt(i1)−θt(a∗)}<∞. (116)
On the other hand, by Lemma 5, we have,
sup
t≥1πθt(i1)
πθt(a∗)=∞ (117)
which contradicts Eq. (116). Hence, the assumption that N∞(a∗)<∞cannot hold. This completes
the proof by contradiction, and implies that N∞(a∗) =∞.
19Second part. With a∗∈ A∞i.e.N∞(a∗) =∞, we now argue that πθt(a∗)→1ast→ ∞ almost
surely.
According to Lemma 2, we have, |A∞| ≥2. Since a∗∈ A∞by assumption, there must be at least
one sub-optimal action i1∈[K]withr(i1)< r(a∗), such that,
N∞(i1) =∞. (118)
In particular, define
i1:= arg min
a∈[K],
N∞(a)=∞r(a). (119)
According to Lemma 6, we have, for all sufficiently large τ≥1,
r(i1)< π⊤
θtr < r (a∗). (120)
Using the same arguments as in the first part (except that i2in Eq. (98) is replaced with a∗), both
Eqs. (105) and (113) hold.
Next, we argue thatPt
s=τπθs(i1)P
a+∈A+(i1)πθs(a+)→ ∞ ast→ ∞ by contradiction.
Suppose
∞X
t=τπθt(i1)X
a+∈A+(i1)πθt(a+)<∞. (121)
According to Lemma 8 (using Eqs. (94), (105), (113) and (121)), we have,
sup
t≥1|θt(i1)|<∞. (122)
Calculating the progress and variance for arm a∗, fort≥1,
Pt(a∗) =η·πθt(a∗)·(r(a∗)−π⊤
θtr) (123)
≥η·∆·πθt(a∗)· 
1−πθt(a∗)
. 
∆:=r(a∗)−max
a̸=a∗r(a)
(124)
≥0. (125)
Denote that, for all t≥1,
Vt(a∗):=5
18·t−1X
s=1πθs(a∗)·(1−πθs(a∗)). (126)
According to Lemma 11 (using Eqs. (123) and (126)), we have,
inf
t≥1θt(a∗)>−∞. (127)
Combining Eqs. (122) and (127), we have,
sup
t≥1πθt(i1)
πθt(a∗)= sup
t≥1exp{θt(i1)−θt(a∗)}<∞, (128)
For all t≥1,|θt(i1)|<∞and since there is at least one arm ( a∗) s.t. inft≥1θt(a)>−∞, there
exists ϵ >0andϵ∈O(1), such that,
sup
t≥1πθt(i1)<1−2ϵ. (129)
According to Eq. (118), we know that N∞(i1) =∞and for all a−∈ A−(i1),N∞(a−)<∞.
Using Lemma 5, we have, for all large enough t≥1,πθt(a−)
πθt(i1)<ϵ
|A−(i1)|.
πθt(i1) +X
a+∈A+(i1)πθt(a+) = 1−πθt(i1)X
a−∈A−(i1)πθt(a−)
πθt(i1)>1−πθt(i1)ϵ(130)
=⇒πθt(i1) +X
a+∈A+(i1)πθt(a+)≥1−ϵ. (131)
20Combining Eqs. (129) and (131), we have, for all large enough t≥1,
X
a+∈A+(i1)πθt(a+)≥ϵ, (132)
which implies that,
∞X
s=τπθs(i1)X
a+∈A+(i1)πθs(a+)≥ϵ·∞X
s=τπθs(i1) (133)
=∞, (since N∞(i1) =∞and by Lemma 3 ) (134)
which contradicts the assumption of Eq. (121). This completes the proof by contradiction, and
therefore, we have,
tX
s=τπθs(i1)X
a+∈A+(i1)πθs(a+)→ ∞ ,ast→ ∞ . (135)
According to Lemma 10 (using Eqs. (105), (113) and (135)), we have, almost surely,
θt(i1)→ −∞ ,ast→ ∞ . (136)
Combining Eqs. (127) and (136), we have, almost surely,
πθt(a∗)
πθt(i1)= exp{θt(a∗)−θt(i1)} → ∞ ,ast→ ∞ . (137)
Hence, we have proved that if N∞(i1) =∞andr(i1)< π⊤
θtr < r (a∗)for sufficiently large τ≥1,
then,πθt(a∗)
πθt(i1)→ ∞ ,ast→ ∞ .
In order to use this argument recursively, consider sorting the action indices in A∞according to their
descending expected reward values,
r(a∗)> r(i|A∞|−1)> r(i|A∞|−2)>···> r(i2)> r(i1). (138)
We know that,
π⊤
θtr−r(i2) =X
a̸=i2πθt(a)·(r(a)−r(i2)) (139)
=X
a−∈A−(i2)πθt(a−)·(r(a−)−r(i2)) +X
a+∈A+(i2)πθt(a+)·(r(a+)−r(i2))
(140)
> πθt(a∗)·(r(a∗)−r(i2))−X
a−∈A−(i2)πθt(a−)·(r(i2)−r(a−)) (141)
=πθt(a∗)·
r(a∗)−r(i2)−X
a−∈A−(i2)πθt(a−)
πθt(a∗)·(r(i2)−r(a−))
, (142)
According to Lemma 5, for all a−∈ A−(i2)witha−̸=i1, we have,
πθt(a∗)
πθt(a−)→ ∞ ,ast→ ∞ . (143)
Combining Eqs. (137) and (143), we have, for all a−∈ A−(i2),
πθt(a∗)
πθt(a−)→ ∞ ,ast→ ∞ . (144)
Hence, for all sufficiently large τ≥1, for all a−∈ A−(i2),
πθt(a−)
πθt(a∗)≤1
2|A−(i2)|r(a∗)−r(i2)
r(i2)−r(a−). (145)
21Combining Eqs. (142) and (145), we have, for all sufficiently large t≥1,
π⊤
θtr−r(i2)> πθt(a∗)·r(a∗)−r(i2)
2>0. (146)
Hence we have, for all sufficiently large τ≥1,
r(i2)< π⊤
θtr < r (a∗) (147)
Comparing Eqs. (120) and (147), we can use a similar argument for i2and conclude that, for
sufficiently large τ≥1, then,πθt(a∗)
πθt(i2)→ ∞ ,ast→ ∞ . This further implies that
r(i3)< π⊤
θtr < r (a∗). (148)
Continuing this recursive argument, we have, for all actions a∈ A∞witha̸=a∗,
πθt(a∗)
πθt(a)→ ∞ ,ast→ ∞ . (149)
Meanwhile, according to Lemma 5, we have, for all actions a̸∈ A∞,
πθt(a∗)
πθt(a)→ ∞ ,ast→ ∞ . (150)
Combining Eqs. (149) and (150), we have, for all sub-optimal actions a∈[K]withr(a)< r(a∗),
πθt(a∗)
πθt(a)→ ∞ ,ast→ ∞ . (151)
Finally, note that,
πθt(a∗) =πθt(a∗)P
a∈[K]:r(a)<r(a∗)πθt(a) +πθt(a∗)(152)
=1
P
a∈[K]:r(a)<r(a∗)πθt(a)
πθt(a∗)+ 1. (153)
Combining Eqs. (151) and (153), we have, almost surely,
πθt(a∗)→1,ast→ ∞ .
22B Miscellaneous Extra Supporting Results
Lemma 3 (Extended Borel-Cantelli Lemma, Corollary 5.29 of [ 5]).Let(Fn)n≥1be a filtration,
An∈ Fn. Then, almost surely,
{ω:ω∈Aninfinitely often }=(
ω:∞X
n=1P(An|Fn))
. (154)
Lemma 4 (Freedman’s inequality [ 10,7], Theorem C.3 of [ 24]).LetX1, X2, . . . be a sequence of
random variables, such that for all t≥1,|Xt| ≤1/2. Define
Sn:=nX
t=1E[Xt|X1, . . . , X t−1]−Xtand Vn:=nX
t=1Var[Xt|X1, . . . , X t−1]. (155)
Then, for all δ >0,
Pr 
∃n:Sn≥6s
Vn+4
3
logVn+ 1
δ
+ 2 log1
δ
+4
3log 3!
≤δ. (156)
Lemma 5. Using Algorithm 1, for any two different actions i, j∈[K]withi̸=j, ifN∞(i) =∞
andN∞(j)<∞, then we have, almost surely,
sup
t≥1πθt(i)
πθt(j)=∞. (157)
Proof. We prove the result by contradiction. Suppose
c:= sup
t≥1πθt(i)
πθt(j)<∞. (158)
According to the extended Borel-Cantelli Lemma 3, we have, for all a∈[K], almost surely,
nX
t≥1πθt(j) =∞o
={N∞(j) =∞}. (159)
Hence, taking complements, we have,
nX
t≥1πθt(j)<∞o
={N∞(j)<∞} (160)
also holds almost surely, which implies that,
∞X
t=1πθt(i) =∞,and (161)
∞X
t=1πθt(j)<∞. (162)
Therefore, we have,
∞X
t=1πθt(i) =∞X
t=1πθt(j)·πθt(i)
πθt(j)(163)
≤c·∞X
t=1πθt(j) (164)
<∞, (165)
which is a contradiction with Eq. (161).
23Lemma 6. Using Algorithm 1, we have, almost surely, for all large enough t≥1,
r(i1)< π⊤
θtr < r (i2), (166)
where i1, i2∈[K]andi1̸=i2are the action indices defined as,
i1:= arg min
a∈[K],
N∞(a)=∞r(a), (167)
i2:= arg max
a∈[K],
N∞(a)=∞r(a). (168)
Proof. Define A∞as the set of actions which are sampled for infinitely many times as t→ ∞ , i.e.,
A∞:={a∈[K]|N∞(a) =∞}. (169)
According to Lemma 2, we have |A∞| ≥2, which implies that i1̸=i2.
Given any sub-optimal action i∈[K]withr(i)< r(a∗), we partition the remaining actions into two
parts using r(i).
A+(i):=
a+∈[K] :r(a+)> r(i)	
, (170)
A−(i):=
a−∈[K] :r(a−)< r(i)	
. (171)
By definition, we have,
A∞⊆ A+(i1)∪ {i1},and (172)
A∞⊆ A−(i2)∪ {i2}. (173)
First part. r(i1)< π⊤
θtr.
Ifr(i1) = min a∈[K]r(a), i.e., i1is the “worst action”, then r(i1)< π⊤
θtrholds trivially. Otherwise,
suppose r(i1)̸= min a∈[K]r(a). We have,
π⊤
θtr−r(i1) =X
a+∈A+(i1)πθt(a+)·(r(a+)−r(i1))−X
a−∈A−(i1)πθt(a−)·(r(i1)−r(a−)).
(174)
Consider the non-empty set A∞∩ A+(i1). Pick an action j1∈ A∞∩ A+(i1), and ignore all the
other actions a+∈ A+(i1)witha+̸=j1in the above equation. We have,
π⊤
θtr−r(i1)> πθt(j1)·(r(j1)−r(i1))−X
a−∈A−(i1)πθt(a−)·(r(i1)−r(a−)) (175)
=πθt(j1)·
r(j1)−r(i1)−X
a−∈A−(i1)πθt(a−)
πθt(j1)·(r(i1)−r(a−))
, (176)
where the first inequality is because of r(a+)−r(i1)>0for all a+∈ A+(i1)by Eq. (170). Note
thatN∞(j1) =∞andN∞(a−)<∞. According to Lemma 5, we have, for all large enough t≥1,
X
a−∈A−(i1)πθt(a−)
πθt(j1)·(r(i1)−r(a−)) =X
a−∈A−(i1)(r(i1)−r(a−)).πθt(j1)
πθt(a−)(177)
<X
a−∈A−(i1)(r(i1)−r(a−))·1A−(i1)·r(j1)−r(i1)
r(i1)−r(a−)·1
2(178)
=r(j1)−r(i1)
2. (179)
Combining Eqs. (175) and (177), we have,
π⊤
θtr−r(i1)> πθt(j1)·r(j1)−r(i1)
2>0, (180)
24where the last inequality is because j1∈ A+(i1).
Second part. π⊤
θtr < r (i2).
The arguments are similar to the first part. If r(i2) =r(a∗), i.e., i2is the optimal action, then
π⊤
θtr < r (i2)holds trivially. Otherwise, suppose r(i2)̸=r(a∗). We have,
r(i2)−π⊤
θtr=X
a−∈A−(i2)πθt(a−)·(r(i2)−r(a−))−X
a+∈A+(i2)πθt(a+)·(r(a+)−r(i2)).
(181)
Consider the non-empty set A∞∩ A−(i2). Pick an action j2∈ A∞∩ A−(i2), and ignore all the
other actions a−∈ A−(i2)witha−̸=j2in the above equation. We have,
r(i2)−π⊤
θtr≥πθt(j2)·(r(i2)−r(j2))−X
a+∈A+(i2)πθt(a+)·(r(a+)−r(i2)) (182)
=πθt(j2)·
r(i2)−r(j2)−X
a+∈A+(i2)πθt(a+)
πθt(j2)·(r(a+)−r(i2))
, (183)
where the first inequality is because of r(i2)−r(a−)>0for all a−∈ A−(i2)by Eq. (170). Note
thatN∞(j2) =∞andN∞(a+)<∞. According to Lemma 5, we have, for all large enough t≥1,
X
a+∈A+(i2)πθt(a+)
πθt(j2)·(r(a+)−r(i2)) =X
a+∈A+(i2)(r(a+)−r(i2)).πθt(j2)
πθt(a+)(184)
<X
a+∈A+(i2)(r(a+)−r(i2))·1A+(i2)·r(i2)−r(j2)
r(a+)−r(i2)·1
2(185)
=r(i2)−r(j2)
2. (186)
Combining Eqs. (182) and (184), we have,
r(i2)−π⊤
θtr≥πθt(j2)·r(i2)−r(j2)
2>0, (187)
where the last inequality is because j2∈ A−(i2).
For the following lemmas, we will use the notation defined in the proofs for Theorems 1 and 2.
Lemma 7 (Concentration of noise) .Given an action a∈[K]. We have, with probability at least
1−δ,
∀t:tX
s=1Ws+1(a)≤36η Rmaxs
(Vt(a) + 4/3) logVt(a) + 1
δ
+ 12 η Rmaxlog(1/δ) + 8 η Rmaxlog 3,
(188)
where
Vt(a):=5
18·tX
s=1πθs(a)·(1−πθs(a)), (189)
and
Wt(a):=θt(a)−Et−1[θt(a)] (190)
is originally defined by Eq. (52).
Proof. First, note that,
Et[Wt+1(a)] = 0 ,for all t≥0. (191)
25Using the update Eq. (22), we have,
Wt+1(a) =θt+1(a)−Et[θt+1(a)] (192)
=θt(a) +η·(It(a)−πθt(a))·Rt(at)− 
θt(a) +η·πθt(a)· 
r(a)−π⊤
θtr
(193)
=η·(It(a)−πθt(a))·Rt(at)−η·πθt(a)· 
r(a)−π⊤
θtr
. (194)
According to Eq. (1), we have,
|Wt+1(a)| ≤3η·Rmax. (195)
The conditional variance of noise is,
Var[Wt+1(a)|Ft]:=Et[(Wt+1(a))2] (196)
≤2η2·Et[(It(a)−πθt(a))2·Rt(at)2] + 2 η2·πθt(a)2· 
r(a)−π⊤
θtr2, (197)
where the inequality is by (a+b)2≤2a2+ 2b2. Next, we have,
Et[(It(a)−πθt(a))2·Rt(at)2] =πt(a)·(1−πt(a))2·r(a)2+X
a′̸=aπθt(a′)·πt(a)2·r(a′)2
(198)
≤R2
max·
πt(a)·(1−πt(a))2+ (1−πt(a))·πt(a)2
(199)
=R2
max·πθt(a)·(1−πθt(a)), (200)
and,
r(a)−π⊤
θtr=X
a′̸=aπθt(a′)·(r(a)−r(a′))(201)
≤X
a′̸=aπθt(a′)· |r(a)−r(a′)| (202)
≤2Rmax·X
a′̸=aπθt(a′) (203)
= 2Rmax·(1−πθt(a)). (204)
Combining Eqs. (196), (198) and (201), we have,
Var[Wt+1(a)|Ft]≤2η2·R2
max·πθt(a)·(1−πθt(a)) + 8 η2·R2
max·πθt(a)2·(1−πθt(a))2
(205)
≤10η2·R2
max·πθt(a)·(1−πθt(a)). (206)
LetXt+1(a):=Wt+1(a)
6η·Rmax. Then we have,
|Xt+1(a)| ≤1/2,and (207)
Var[Xt+1(a)|Ft]≤5
18·πθt(a)·(1−πθt(a)). (208)
According to Lemma 4, there exists an event E1such that Pr(E1)≥1−δ, and when E1holds,
∀t:tX
s=1Xs+1(a)≤6s
(Vt(a) + 4/3) logVt(a) + 1
δ
+ 2 log(1 /δ) +4
3log 3,(209)
which implies that,
∀t:tX
s=1Ws+1(a)≤36η Rmaxs
(Vt(a) + 4/3) logVt(a) + 1
δ
+ 12 η Rmaxlog(1/δ) + 8 η Rmaxlog 3,
(210)
where Vt(a):=5
18·Pt
s=1πθs(a)·(1−πθs(a)).
26Lemma 8 (Bounded progress) .For any action a∈[K]withN∞(a) =∞, if there exists c >0and
τ <∞, such that, for all t≥τ,
tX
s=1Ps(a)≥c·Vt(a), (211)
where Vt(a)is defined in Eq. (189) , and if also
∞X
t=1Pt(a)<∞, (212)
then we have, almost surely,
sup
t≥1|θt(a)|<∞. (213)
Proof. According to Eq. (55) and triangle inequality, we have,
θt(a)≤E[θ1(a)]+tX
s=1Ws(a)+t−1X
s=1Ps(a). (214)
According to Lemma 7, there exists an event E1, such that Pr(E1)≥1−δ, and when E1holds, we
have, for all t≥τ+ 1,
tX
s=1Ws(a)≤36η Rmaxs
(Vt−1(a) + 4/3)·logVt−1(a) + 1
δ
+ 12 η Rmaxlog(1/δ) + 8 η Rmaxlog 3.
(215)
According to Eq. (212), as t→ ∞ ,
t−1X
s=1Ps(a)≤t−1X
s=1Ps(a)<∞. (216)
According to Eqs. (211), (215) and (216), we have,
tX
s=1Ws(a)<∞. (217)
Combining Eqs. (214), (216) and (217), we have, as t→ ∞ ,
|θt(a)|<∞. (218)
Take any ω∈ E:={N∞(a) =∞}. Because P(E \(E ∩ E 1))≤P(Ω\ E1)≤δ→0asδ→0, we
have that P-almost surely for all ω∈ Ethere exists δ >0such that ω∈ E ∩ E 1while Eq. (215) also
holds for this δ. Take such a δ. We have, almost surely,
sup
t≥1|θt(a)|<∞.
Lemma 9 (Unbounded positive progress) .For any action a∈[K]withN∞(a) =∞, if there exists
c >0andτ <∞, such that, for all t≥τ,
Pt(a)>0,and (219)
tX
s=τPs(a)≥c·Vt(a), (220)
where Vt(a)is defined in Eq. (189) , and if also,
∞X
t=τPt(a) =∞, (221)
then we have, almost surely,
θt(a)→ ∞ ,ast→ ∞ . (222)
27Proof. According to Eq. (55),
θt(a) =E[θ1(a)] +tX
s=1Ws(a) +t−1X
s=1Ps(a). (223)
According to Lemma 7, there exists an event E1, such that Pr(E1)≥1−δ, and when E1holds, we
have, for all t≥τ+ 1,
tX
s=1Ws(a)≥ −36η Rmaxs
(Vt−1(a) + 4/3)·logVt−1(a) + 1
δ
| {z }
♡−12η Rmaxlog(1/δ)−8η Rmaxlog 3.
(224)
By Eq. (221), as t→ ∞ ,
t−1X
s=1Ps(a)→ ∞ , (225)
and the speed ofPt−1
s=1Ps(a)→ ∞ is strictly faster than ♡ → ∞ , according to Eq. (220). This
implies that θt(a)→ ∞ , as a result of “cumulative progress” dominates “cumulative noise”.
Take any ω∈ E:={N∞(a) =∞}. Because P(E \(E ∩ E 1))≤P(Ω\ E1)≤δ→0asδ→0, we
have that P-almost surely for all ω∈ Ethere exists δ >0such that ω∈ E ∩ E 1while Eq. (224) also
holds for this δ. Take such a δ. We have, almost surely,
θt(a)→ ∞ ,ast→ ∞ .
Lemma 10 (Unbounded negative progress) .For any action a∈[K]withN∞(a) =∞, if there
exists c >0andτ <∞, such that, for all t≥τ,
Pt(a)<0,and (226)
−tX
s=τPs(a)≥c·Vt(a), (227)
where Vt(a)is defined in Eq. (189) , and if also,
∞X
t=τPt(a) =−∞, (228)
then we have, almost surely,
θt(a)→ −∞ ,ast→ ∞ . (229)
Proof. The proof follows almost the same arguments for Lemma 9.
Lemma 11 (Positive progress) .For any action a∈[K]withN∞(a) =∞, if there exists c >0and
τ <∞, such that, for all t≥τ,
Pt(a)>0,and (230)
tX
s=τPs(a)≥c·Vt(a), (231)
where Vt(a)is defined in Eq. (189) , then we have, almost surely,
inf
t≥1θt(a)>−∞. (232)
Proof. First case: ifP∞
t=1Pt(a)<∞, then according to Lemma 8, we have, almost surely,
sup
t≥1|θt(a)|<∞, (233)
which implies Eq. (232).
Second case: ifP∞
t=1Pt(a) =∞, then according to Lemma 9, we have, almost surely,
θt(a)→ ∞ ,ast→ ∞ , (234)
which also implies Eq. (232).
28Lemma 12 (Negative progress) .For any action a∈[K]withN∞(a) =∞, if there exists c >0and
τ <∞, such that, for all t≥τ,
Pt(a)<0,and (235)
−tX
s=τPs(a)≥c·Vt(a), (236)
where Vt(a)is defined in Eq. (189) , then we have, almost surely,
sup
t≥1θt(a)<∞. (237)
Proof. First case: if−P∞
t=1Pt(a)<∞, then according to Lemma 8, we have, almost surely,
sup
t≥1|θt(a)|<∞, (238)
which implies Eq. (237).
Second case: if−P∞
t=1Pt(a) =∞, then according to Lemma 10, we have, almost surely,
θt(a)→ −∞ ,ast→ ∞ , (239)
which also implies Eq. (237).
C Additional simulation results
0.0 0.2 0.4 0.6 0.8 1.0
t 1e6107
106
105
104
103
102
101
100Probability of optimal action
(a)πθt(a∗),η= 100 .
0.0 0.2 0.4 0.6 0.8 1.0
t 1e60.0000.0250.0500.0750.1000.1250.1500.1750.200Sub-optimality gap (b)r(a∗)−π⊤
θtr,η= 100 .
0 2 4 6 8 10 12 14
log(t)18
16
14
12
10
8
6
4
2
Log sub-optimality gap (c)log (r(a∗)−π⊤
θtr),η= 10 .
Figure 2: Visualization in a two-action stochastic bandit problem. Here the rewards are defined as
(−0.05,−0.25). Other details are same as for Figure 1. Figures 2a and 2a are based on a single run,
while Figure 2c averages across 10 runs. Note that log (r(a∗)−π⊤
θtr)≈10−33at the final stages on
Figure 2b.
29NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In our view, the abstract and introduction summarize the main results, as well
as the technical novelty in obtaining these results.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See Section 5.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
30Justification: We describe the setting and assumptions clearly. The main results are backed
by proof sketches in the main text and detailed proofs in the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Our experiments consist of simple simulations, for which all the details are
included in Section 4. The algorithm studied here is remarkably simple and well-known, so
the experiments should be straightforward to reproduce with these details, without any code.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
31Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: Given that we are doing simulations with a very classical algorithm, for which
we give all simulation details and hyperparameters, we do not believe that code is necessary
to reproduce the results. Our data is simulated, and hence easily reproduced, given the
distribution which we explicitly describe.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Please see the details in Section 4.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: There are no comparisons for which significance needs to be demonstrated,
but we provide evidence for the convergence by repeating 10 independent runs, which are
presented in the plots.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
32•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: Our experiments are simply run on a single laptop or Python notebook, without
requiring any particular infrastructure.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research does not involve human subjects or introduces any new datasets.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: See Section 5.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
33•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: No data or models are released.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: No existing assets are used.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
34•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No human subjects or crowdsourcing is involved in this work.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No human subjects or crowdsourcing is involved in this work.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
35•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
36