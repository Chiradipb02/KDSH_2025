Soft Tensor Product Representations for Fully
Continuous, Compositional Visual Representations
Bethia Sun
UNSW, Sydney
bethia.sun@unsw.edu.auMaurice Pagnucco
UNSW, Sydney
morri@unsw.edu.auYang Song
UNSW, Sydney
yang.song1@unsw.edu.au
Abstract
Since the inception of the classicalist vs. connectionist debate, it has been argued
that the ability to systematically combine symbol-like entities into compositional
representations is crucial for human intelligence. In connectionist systems, the
field of disentanglement has emerged to address this need by producing repre-
sentations with explicitly separated factors of variation (FoV). By treating the
overall representation as a string-like concatenation of the inferred FoVs, however,
disentanglement provides a fundamentally symbolic treatment of compositional
structure, one inherently at odds with the underlying continuity of deep learning
vector spaces. We hypothesise that this symbolic-continuous mismatch produces
broadly suboptimal performance in deep learning models that learn or use such
representations. To fully align compositional representations with continuous
vector spaces, we extend Smolensky’s Tensor Product Representation (TPR) and
propose a new type of inherently continuous compositional representation, Soft
TPR, along with a theoretically-principled architecture, Soft TPR Autoencoder ,
designed specifically for learning Soft TPRs. In the visual representation learning
domain, our Soft TPR confers broad benefits over symbolic compositional repre-
sentations: state-of-the-art disentanglement and improved representation learner
convergence, along with enhanced sample efficiency and superior low-sample
regime performance for downstream models, empirically affirming the value of our
inherently continuous compositional representation learning framework.
1 Introduction
Compositional structure, capturing the property of being decomposable into a set of constituent
parts, is ubiquitous in our surroundings – from the recursive application of syntax in language,
to the parsing of richly complex visual scenes into their constituent parts. Given the central role
such structure plays in our understanding of the world, it is highly intuitive that deep learning
representations also embody compositional structure. Indeed, empirical evidence highlights the
usefulness of explicitly compositional representations, showcasing a multitude of benefits, including
increased interpretability [10, 12], reduced sample complexity [30, 33], increased fairness [20, 25,
41], and improved performance in out-of-distribution generalisation [33, 48, 50].
We consider the following, intuitive notion of compositional representations. A represen-
tation of compositionally-structured data is a compositional representation if it has a struc-
ture that faithfully reflects the compositional structure of the represented data [49]. In
the visual representation learning domain, data is clearly compositionally-structured , as
images can be decomposed into a set of constituent factors of variation (FoVs), e.g.,
{magenta floor ,orange wall ,aqua object colour ,oblong object shape }for the image in Figure 1.
Code is available at https://github.com/gomb0c/soft_tpr/
38th Conference on Neural Information Processing Systems (NeurIPS 2024).A widely explored representation learning framework producing explicitly compositional represen-
tations is that of disentanglement . We adopt the conventional [5, 25, 33, 43], intuitive definition
of adisentangled representation , which states that a representation, ψ(x), is disentangled if each
of the underlying FoVs can be cleanly separated into a distinct dimension (or contiguous subset of
dimensions) of ψ(x), or, in other words, if each FoV has a 1-1 correspondence with a distinct part
of the representation [43]. Framed in this way, it is apparent that disentangled representations are
explicitly compositional by nature. The majority of state-of-the-art disentanglement approaches use a
variational autoencoder backbone, and rely on weak supervision [13, 22, 31, 33, 35], or a penalisation
of the aggregate posteriorR
q(z|x)p(x)dx[10, 14, 17, 24, 26, 32] to promote disentanglement. More
recent approaches depart from the restrictive assumptions of a variational framework, and instead use
standard autoencoding [47], or energy-function based optimisation [36], with additional inductive
biases to encourage disentanglement. Despite the diversity of methods characterising existing work,
we make a crucial observation which unifies them together: by enforcing the 1-1 correspondence
between FoVs and distinct parts of the representation, existing approaches [10, 13, 14, 17, 22, 24, 26,
31, 32, 33, 35, 36, 47] essentially produce compositional representations corresponding to a concate-
nation of scalar-valued or vector-valued FoV tokens, as illustrated in Figure 1a. This concatenative
approach enforces a rigid, slot-based representational structure that constraints how information can
be represented and combined, and mirrors symbolic systems, where distinct symbols occupy discrete
slots within a representation. We argue that this fundamentally symbolic approach creates a deep
incompatibility with the inherent continuity of the vector spaces underlying deep learning for the
following reasons:
1.Misalignment with Gradient-Based Learning : The symbolic approach’s introduction
of discrete representational slots for each FoV introduces non-differentiable boundaries
between FoV slots, challenging the efficacy of gradient-based learning, which thrives on
smooth and continuous transformations. For example, when modifying a given FoV , this
symbolic structure restricts gradient propagation to the dimensions associated solely to the
corresponding representational slot, inhibiting the smooth permeation of gradients across
the entire vector space. Managing these discrete, slot-based boundaries thus fragments
gradient flow across the vector space, producing abrupt, discontinuous transitions that may
potentially complicate learning.
2.Restrictive / Incompatible Structure : By allocating distinct representational slots for each
FoV , the symbolic approach imposes a rigid representational structure that prevents the
representation from exploiting the expressivity inherent in continuous vector spaces. More
concretely, this slot-based framework prevents the encoding of FoVs as flexible combinations
of basis vectors that span the entire representational space – an approach that is not only
more intuitive, but also critical for capturing rich interactions and complex dependencies
among FoVs. By failing to allow for this flexibility, the symbolic approach prevents the
representation from leveraging the full expressivity of its underlying vector space.
Critically, we hypothesise that the fundamental incompatibility between the symbolic treatment of
compositional structure provided by disentanglement, and the continuous vector spaces of deep
learning produces suboptimal behaviour in the models that learn or use these representations. This
hypothesis prompts the following question: can we instead represent compositional structure in
an inherently continuous manner? A continuous compositional representation would yield the
representation, ψ(x), bycontinuously combining the FoVs, rather than maintaining a discrete, slot-
based separation, as in the symbolic approach. The continuous approach to representing compositional
structure is thus, a more mathematically intuitive framework in the context of deep learning.
Pioneered by Smolensky, the Tensor Product Representation [3] is a specific representational form
that encodes compositional structure in an inherently continuous manner. At the crux of it, TPRs
are formed by continuously blending the FoVs together into the overall representation, in a manner
analogous to superimposing multiple waves together to produce a complex waveform, as illustrated in
Figure 1b. For a representation to qualify as a TPR, it must adhere to a highly specific mathematical
form, which confers upon the TPR valuable theoretical properties (elaborated on in Section 3.2), but
also imposes two major limitations (see B.1 for further details). First, as depicted by the stars in Figure
1c, only a discrete subset of points in the underlying representational space, ˜V, satisfies the stringent
mathematical criteria to qualify as TPRs. Consequently, to learn TPRs, representation learners must
map from the data manifold onto this discrete subset , which constitutes a highly constrained and
inherently challenging learning task. Second, the TPR specification enforces a strict, algebraic
2Figure 1: (a) Disentangled representations can be conceptualised as a concatenation of FoV tokens (coloured
blocks), effectively enforcing a string-like, symbolic compositional structure, where each FoV is allocated to a
discrete slot in the representation. We instead, consider a continuous representation of compositional structure,
(b), where the FoVs (first 6 waves) are continuously superimposed together to produce the overall representation,
ψ(x)(in red). (c) Only a subset of points (stars) in the underlying representational space (rainbow manifold)
satisfy the TPR specification. The Soft TPR relaxes this, capturing larger, continuous regions of the underlying
representational space (the translucent circles), while preserving the TPR’s key properties.
definition of compositional structure, limiting the TPR’s ability to faithfully represent real-world
data which is often quasi -compositional, only approximately adhering to a rigid, formal definition
of compositionality. Historically, these limitations have confined TPR learning to formal domains
characterised by explicit, algebraic structure – as evidenced by the near exclusive deployment of
TPRs in language [19, 23, 28, 34, 52] – and, to contexts where strong supervision from highly
structured downstream tasks is available to steer the representation learning process [23, 28, 38,
51]. To negate these drawbacks and extend continuous compositional representations to weakly
supervised, non-formal domains, we propose Soft TPR , a new, inherently continuous compositional
representation that can be thought of as a continuous relaxation of the traditional TPR, as illlustrated
by the translucent circular regions in Figure 1c. At its core, the Soft TPR is designed to promote
representational flexibility and ease of learning while simultaneously preserving the structural and
mathematical integrity of the traditional TPR. We additionally introduce Soft TPR Autoencoder , a
theoretically-principled weakly-supervised architecture for learning Soft TPRs, which we use to
operationalise the Soft TPR framework in the visual representation learning domain.
Our main contributions are threefold: i) We propose a novel compositional representation learning
framework, introducing the inherently continuous Soft TPR compositional form, alongside a dedicated,
weakly-supervised architecture, Soft TPR Autoencoder , for learning this form. ii) Our framework is
the first to learn continuous compositional representations in the non-formal domain of vision . iii) We
empirically affirm the far-reaching benefits of enhanced vector space alignment produced by the Soft
TPR framework, demonstrating that Soft TPRs achieve state-of-the-art disentanglement, accelerate
representation learner convergence, and provide downstream models with enhanced sample efficiency
and superior low-sample regime performance.
2 Related Work
Disentanglement : In aiming to produce explicitly compositional representations without strong
supervision, our work shares the same objective as disentangled representation learning. Prior
to the highly influential work of [25], which proved the impossibility of learning disentangled
representations without supervision or other inductive biases, disentangled representations were
learnt in a completely unsupervised fashion [8, 10, 14, 17, 24, 26, 37]. Our use of weak supervision
is inspired by the work [13, 22, 31, 33, 35] relating to this highly influential impossibility result.
In particular, we leverage the type of weak supervision termed ‘match pairing’ [35], where pairs,
(x, x′), differing in values for a subset of known FoVs are presented to the model, to incentivise
disentanglement. Our work, however, fundamentally diverges from all disentanglement work we
are aware of, by adopting an inherently continuous representation of compositional structure, which
contrasts with the symbolic representations of compositional structure characterising existing work.
TPR-based Work : Existing TPR-based approaches generate continuous representations of compo-
sitional structure by producing an element with the explicit mathematical form of a TPR. To learn
this highly specific form, these approaches rely on the algebraic characterisation of compositionality
present in formal domains, such as mathematics [38], or language [19, 23, 28, 34, 52] in addition to
strong supervision signals from highly structured downstream tasks, such as part-of-speech tagging
[23], and answering structured language [28, 51] or mathematics questions [38]. In contrast, our Soft
3TPR eases these stringent constraints by offering a relaxed specification of inherently continuous
compositional structure. This allows our approach to extend continuous representations of composi-
tional structure to an orthogonal and less structured domain, that of visual representation learning,
while also reducing reliance on annotated data by instead using weak supervision to learn this relaxed
representational form.
3 Preliminaries
3.1 A Formal Framework for Compositional Representations
We adopt a generalised, non-generative version of the definition of compositional representations
from [49]. Data x∈Xiscompositionally-structured if there exists a decomposition function β:
X→A1×. . .×Andecomposing xinto constituent parts, i.e. β(x) ={a1, . . . , a n}, where ai∈Ai.
A map ψ:X→VFproduces a compositional representation ifψ(x) =C(ψ1(a1), . . . , ψ n(an))
where ψi:Ai→Videnote component functions that independently embed the parts of xinto
vector spaces, and C:V1×. . .×Vn→VFdenotes a composition function that combines the
embedded parts of xtogether to form the overall representation. Intuitively, this definition enforces a
faithful structural correspondence between the constituency structure of the data, x(i.e., the parts,
{a1, . . . , a n}) and the constituency structure of the representation, ψ(x)(i.e., the embedded parts,
{ψ1(a1), . . . , ψ n(an)}) (assuming Cis invertible).
We formalise a symbolic compositional representation, ψs(x), as a compositional representation
where Cis a concatenation operation. Thus, ψs(x) = 
ψ1(a1)T, . . . , ψ n(an)TTfor any symbolic
compositional representation ,ψs(x). Clearly, the aforementioned disentanglement methods [8, 10,
13, 14, 17, 22, 24, 26, 31, 32, 33, 35, 36, 37, 47] all fit this framework. While we observe that the
fundamentally symbolic definition of Cas concatenation produces an inherent misalignment with the
continuous vector spaces of deep learning, it has one notable benefit: the embedded FoVs, {ψi(ai)},
can be easily recovered from the representation, ψs(x), by simply partitioning ψs(x).
It is highly intuitive that for any compositional representation, ψ(x), to be broadly useful, the FoVs,
{ai}, should be easily recoverable from ψ(x)(here we assume the ψi’s are invertible, and so, that this
property corresponds to being able to recover the representational components, {ψi(ai)}fromψ(x)).
We thus explore whether an alternative Cexists that simultaneously 1) combines the embedded
FoVs into the overall representation in an inherently continuous manner and 2) preserves the direct
recoverability of the embedded FoVs.
3.2 The TPR Framework
TPR [3] is a specific type of representation that is compositional ,continuous , and under certain
conditions, ensures the direct recoverability of the embedded parts {ψi(ai)}from the overall rep-
resentation. We briefly introduce essential aspects of the framework, deferring further details and
formal proofs to Appendix A. The TPR framework views compositionally-structured objects as
possessing a number of (potentially infinite) roles1, where each role is bound to a corresponding filler.
It thus defines the constituent parts {ai}of any compositionally-structured object as a set of role-filler
bindings . This role-filler binding formalism has predominantly been applied in the natural language
domain [19, 28, 34, 52], with fillers often corresponding to words and roles to grammatical categories
(e.g., the word catas a filler, and the the category noun as a role). We translate this formalism into
the domain of visual representation learning by informally equating roles as FoV types , and fillers as
FoV values , e.g.,{floor colour ,wall colour ,object colour ,object size ,object shape ,orientation }
and{blue,magenta ,orange ,green ,small ,medium ,large,oblong ,cube, . . .}respectively for the
Shapes3D domain of Figure 1. The binding of a filler, f, from a set FofNFfillers, to a role,
r, from a set RofNRroles, such as the filler magenta to the role object colour conveys a sort
of filler-specific, role-modulated semantic content, and is denoted by f/r. The compositional
structure of the image, x, in Figure 1 would thus correspond to the following set of role-filler bind-
ings: β(x)={magenta /floor colour ,orange /wall colour ,aqua/object colour ,large/object size ,
oblong /object shape }.
1We assume a finite set of NRroles, an intuitive assumption to make in the context of visual representation
learning.
4To produce the TPR, the roles and fillers for each binding in xare independently embedded using role
and filler embedding functions, ξR:R→VR, ξF:F→VFrespectively. To produce an embedding
of the binding f/r, a tensor product, which we denote by ⊗, is then taken over the embedded role,
ξR(r), and embedded filler, ξF(f), comprising the binding. Finally, a summation is performed over
allembedded bindings in xto produce the overall representation. More concretely, the TPR, ψtpr(x),
is defined as:
ψtpr(x) :=X
iξF(fm(i))⊗ξR(ri), (1)
where m:{1, . . . , N R} → { 1, . . . , N F}is a matching function associating each of the roles to the
unique filler it binds to in the decomposition of x2.
We now place the TPR in the formal framework of Section 3.1, by observing that the TPR defines the
component functions as ψi(fm(i), ri) :=ξF(fm(i))⊗ξR(ri), and the composition function, C, as
ordinary vector space addition. As the TPR does not concatenate , but rather, additively superimposes
all representational components {ψi(fm(i), ri)}together to produce the overall representation, it
instantiates an inherently continuous representation of compositional structure. Defining Cas ordinary
addition, however, prompts the question of whether the summed up representational components
{ψi(fm(i), ri)}can be recovered from the representation, ψtpr(x). Remarkably, due to the special
form of the TPR, each representational component ψi(fm(i), ri)corresponding to an embedded
role-filler binding, ξF(fm(i))⊗ξR(ri)can be faithfully recovered from the TPR through a process
referred to as unbinding (see A.2 for more details). More concretely, provided that all role embedding
vectors {ξR(ri)}are linearly independent, the embedded filler bound to the i-th role can be unbound
from the representation, ψtpr(x), by taking a (tensor) inner product between ψtpr(x)and the i-th
unbinding vector, ui:
ψtpr(x)ui= X
iξF(fm(i))⊗ξR(ri)!
ui=ξF(fm(i)), (2)
where uiis a vector corresponding to the i-th column of UT, the (left) inverse of the matrix formed by
taking all (linearly independent) role embeddings as columns. By repeating the unbinding procedure
using each of the NRunbinding vectors, the embedded fillers bound to each role, and hence, the
embeddings (ξF(fm(i)), ξR(ri))comprising each binding embedding, ξF(fm(i))⊗ξR(ri), can be
recovered from the overall representation, ψtpr(x). Thus, provided that the linear independence
condition is satisfied, the TPR represents a continuous compositional representation that retains the
key benefit of symbolic compositional representations: the direct recoverability of the representational
parts,{ψi(fm(i), ri)}.
4 Methods
4.1 Soft TPR: An Extension to the TPR Framework
The TPR’s highly specific representational form, ψtpr(x) :=P
iξF(fm(i))⊗ξR(ri), can only be
satisfied by a discrete subset of points in the underlying representational space, VF⊗VR. This
imposes an arduous learning task on representation learners: to parameterise the highly constrained
map from the data manifold to a discrete subset of points. This representational form additionally
assumes a strict algebraic definition of compositionality that corresponds to a set of bindings, where
each binding comprises a single role and a single filler, precluding the TPR from representing quasi -
compositional objects that only approximately satisfy this strict, algebraic definition of compositional
structure (e.g., French liaison consonants, where a weighted sum of multiple fillers, rather than a
single filler, bind to a role [9]). Our primary insight is that both these drawbacks can be mitigated
bycontinuously relaxing the TPR specification (see B.1). This relaxation allows for a wider variety
of mappings within a ‘cloud’ around each TPR (represented by the translucent circular regions in
Figure 1c), which eases the difficulty of representation learning. Furthermore, it relaxes the rigid
role-filler based specification of compositional structure into a softer , less rigid notion, enabling the
representation of nuanced, quasi-compositional data. We thus introduce the Soft TPR , a continuously
relaxed, less stringently defined variant of the explicit TPR that simultaneously retains the TPR’s key
2We omit the dependence of monxfor notational clarity.
5properties of 1) continuous compositional structure, and 2) direct recoverability of representational
parts{ 
ξF(fm(i)), ξR(ri)
}from the overall representation.
Consider an element zinVF⊗VR, the vector space underlying TPRs produced by an arbitrary role
embedding function ξR:R→VRand an arbitrary filler embedding function ξF:F→VF. Ifzis
sufficiently ‘close’ to some TPR, ψtpr3, quantified by ||z−ψtpr||F< ϵ, where ||A||Fdenotes the
Frobenius norm of the rank-2 tensor, A, andϵis some small, scalar valued quantity, then this distance
metric induces the approximation z≈ψtpr. By performing unbinding of the i-th filler to both sides
of the approximation, we have:
zui≈ψtprui= X
iξF(fm(i))⊗ξR(ri)!
ui=ξF(fm(i)), (3)
=ξF(fm(i)) +ϵi=:˜fi,where ϵi=zui−ξF(fm(i)). (4)
Thus, performing unbinding on an element zinVF⊗VRwhere the sufficient closeness condition
holds recovers a soft filler embedding, ˜fi, that approximates the true filler embedding, ξF(fm(i)),
of the filler bound to role riforψtprwith approximation error ϵi. We define such elements as
Soft TPRs , noting that these elements both 1) softly approximate the continuous compositional
structure captured by some explicit TPR, ψtpr, aszis sufficiently ‘close’ to ψtprbased on the chosen
distance metric, and 2) approximately preserve the recoverability of the representational components
{(ξF(fm(i)), ξR(ri))}of the explicit TPR, ψtpr, they approximate, with the only difference being that
softfiller embeddings, ˜fiare returned in place of the actual filler embeddings, ξF(fm(i)). Defining
Soft TPRs in this way 1) provides a less restrictive representational specification that can be satisfied
by any arbitrary element from VF⊗VRprovided that, for some explicit TPR, ψtpr, the sufficient
closeness requirement, ||z−ψtpr||F< ϵ, holds, and 2) allows learned representations to embody a
more flexible, relaxed notion of compositional structure.
4.2 Soft TPR Autoencoder: A Concrete Implementation of Learning Soft TPRs
We define our vector spaces of interest over the reals as VF:=RDFandVR:=RDRwhere DF, DR
denote the dimensionality of the filler and role embedding spaces. The main insight underlying our
method is that, as the Soft TPR is effectively any arbitrary element from a vector space4RDF·DR
that is sufficiently close to some explicit TPR, any (DF·DR)-dimensional vector produced by an
encoder in a standard autoencoding framework can be treated as a Soft TPR candidate . This suggests
that a simple autoencoding framework only needs to be slightly modified to produce Soft TPRs.
Briefly speaking, our Soft TPR Autoencoder contains a standard encoder, E, the TPR decoder, and a
standard decoder, D, where the encoder output, z, corresponds to the Soft TPR. At a high level, our
framework aims to ensure two properties: 1) representational form, and 2) representational content.
Representational form requires that the encoder output, z, has the desired Soft TPR form (i.e. that
||z−ψtpr||F< ϵfor some TPR, ψtpr). However, having a Soft TPR form alone is insufficient; the
representation produced by the autoencoder must also reflect the truerole-filler content of the data
to be a good representation, as required by the aim of representational content . These 2 properties
are (mostly) respectively achieved using the unsupervised and weakly supervised components of our
method.
Representational Form : to encourage the autoencoder to produce elements that are Soft TPRs, we
penalise the Euclidean distance ||z−ψ∗
tpr||2between the encoder output, z, and the explicit TPR,
ψ∗
tpr, that zbest approximates. To obtain ψ∗
tpr, needed to penalise the above distance, we derive an
explicit analytical form for ψ∗
tprand construct elements satisfying this analytical form using a role
embedding matrix, MξRcontaining NRDR-dimensional role embedding vectors, {ξR(ri)}, and a
filler embedding matrix, MξFcontaining NFDF-dimensional filler embedding vectors, {ξF(fi)}.
To define an explicit analytical expression for ψ∗
tpr, we are guided by the intuition that the TPR that z
best approximates should have an explicit dependency on the soft filler bindings ofz, and so, elect to
3We occasionally omit the dependence on xfor notational clarity.
4Due to isomorphism of vector spaces RDF⊗RDR∼=RDF·DR, we henceforth use vectors from RDF·DR
in place of rank-2 tensors from RDF⊗RDR, and the Euclidean norm instead of the Frobenius norm to align the
Soft TPR framework more seamlessly with the autoencoding framework.
6Encoder(E)Quantisation ModuleUnbound ‘soft’ fillersUnbinding ModuleTPR DecoderTPR ConstructorVector QuantisationTPR operatorDecoder(D)Quantised fillersUnbinding operator
Filler embedding matrixRole embedding matrix
Figure 2: Diagram illustrating the Soft TPR Autoencoder. We encourage the encoder E’s output, z, to have the
form of a Soft TPR by penalising its distance with the greedily defined, explicit TPR, ψ∗
tprof Equation 5 that z
best approximates. ψ∗
tpris recovered using a 3 step process performed by our TPR decoder (center rectangle): 1)
unbinding, 2) quantisation, and 3) TPR construction. The decoder, D, reconstructs the input image using ψ∗
tpr.
use the following, greedily optimal definition:
ψ∗
tpr:=X
iξF(fm(i))⊗ξR(ri),where m(i) := arg min
j||˜fk−ξF(fj)||2, and ˜fk:=zui. (5)
That is, we define ψ∗
tpras the TPR constructed from explicit filler embeddings ξF(fj)with the
smallest Euclidean distance to the softfiller embeddings ˜fkofz. To construct elements satisfying (5),
we use a 3-step process carried out by the novel TPR decoder we introduce, visible in Figure 2: 1)
Unbinding : The unbinding module consists of a fixed semi-orthogonal role embedding matrix MξR
and recovers the soft filler embeddings {˜fi}associated with each encoder output, z, by performing
the TPR unbinding operation. We elaborate on our theoretically-informed reason for this choice
of role embedding matrix, how the unbinding vectors are obtained, and for not backpropagating
gradient to MξRin B.3.1. 2) Quantisation : The quantisation module, containing a learnable filler
embedding matrix MξF, employs the VQ-V AE vector quantisation algorithm [11] to both 1) learn
the explicit filler embeddings, and 2) quantise the soft filler embeddings {˜f1, . . . , ˜fNR}produced by
the unbinding module into the explicit filler embeddings {ξF(f1), . . . , ξ F(fNR)}with the smallest
Euclidean distances. 3) TPR Construction : The TPR construction module recovers ψ∗
tprby simply
performing the TPR operationP
iξF(fm(i))⊗ξR(ri)over the fixed, semi-orthogonal role embedding
vectors with the corresponding explicit filler embeddings produced by the quantisation module. To
ensure the quantised filler embeddings {ξF(fm(i))}depend explicitly on the reconstructed image,
we pass the output of the TPR decoder, ψ∗
tprto the image decoder, D, for reconstruction. The overall
unsupervised loss, Lu, is thus given by the following, where sdenotes the stop-gradient operator, βa
hyperparameter, and Lrany suitable image-based reconstruction loss (we use L2):
Lu:=||z−ψ∗
tpr||2
2|{z}
Soft TPR penalty+Lr(x, D(ψ∗
tpr))
|{z }
reconstruction loss+X
i1
NR
||s[ξF(fm(i))]−˜fi||2
2+β||ξF(fm(i))−s[˜fi]||2
2
| {z }
VQ-V AE quantisation loss,
(6)
Representational Content : While the unsupervised loss Luencourages the autoencoder to produce
encodings zwith the desired Soft TPR form, it may not ensure that the content of zaccurately
reflects the true role-filler semantics of the represented data. To address this, we introduce a weakly
supervised loss to ensure that the explicit TPR, z∗
tpr, which zbest approximates, reflects the ground-
truth semantics of the image. We employ a match-pairing context similar to [13, 22, 33, 35], where
image pairs (x, x′)share the same role-filler bindings for all but one of the roles, ri, and the identity
ofriis known, but not any of the fillers, or role-filler bindings. Our intuition is that, for the role-filler
binding embeddings of zto reflect the semantics of the represented image, the Euclidean distance
between the quantised fillers of xandx′bound to role rishould be maximal, relative to the distances
between the pairs of filler embeddings for all other roles rj, j̸=i. To encourage this, we apply the
cross entropy loss corresponding to the 3rd term in Eq 7, where ∆qdenotes the NR-dimensional
vector with each dimension (∆q)kpopulated by the Euclidean distance between the quantised fillers
ofxandx′for role rk, and ldenotes the one-hot vector of dimension NRwith the index for riset
7to 1. Additionally, we apply a reconstruction loss using the TPRs, ψs
tpr(x)andψs
tpr(x′), which are
constructed by swapping the quantised filler embeddings of xandx′bound to role ri, to reconstruct
x′andxrespectively.
Our final loss, L, is a weighted sum over the unsupervised and weakly supervised loss components,
where λ1andλ2are hyperparameters:
L:=Lu+λ11
2Lr(x, D(ψs
tpr(x′))) +1
2Lr(x′, D(ψs
tpr(x)))
+λ2CE(∆ q, l), (7)
5 Results
To assess the compositional representations produced by our Soft TPR framework, we perform
evaluation along three dimensions: 1) Compositional Structure / Disentanglement : What is the
degree to which Soft TPR representations achieve explicitly compositional structure ?2) Represen-
tation Learner Convergence Rate : Can representation learners learn the inherently continuous
compositional structure embodied in the Soft TPR faster than symbolic alternatives ?3) Downstream
Models : Does the enhanced vector space alignment produced by the Soft TPR facilitate benefits for
downstream models using compositional representations?
We benchmark against a suite of weakly supervised disentanglement baselines: Ada-GV AE [33],
GV AE [22], ML-V AE [13], SlowV AE [39], and the GAN-based model of [35], which we henceforth
refer to as ‘Shu’. These models all produce symbolic compositional representations corresponding to
a concatenation of scalar-valued FoV tokens. Like our model, Ada-GV AE, GV AE, ML-V AE, and
Shu are trained with paired samples (x, x′)sharing values for all but a subset of FoVs types (roles), I.
ML-V AE, GV AE, and Shu assume access to I, the FoV types (roles) that differ between xandx′,
matching our model’s level of supervision, while Ada-GV AE does not. We thus modify Ada-GV AE
(method detailed in Appendix C.2.2) for more direct comparability, denoting our modification by
Ada-GV AE-k. In contrast, SlowV AE is trained with pairs of samples where allunderlying FoV values
change, and also assumes that this change can be characterised as a sample from a Laplacian. We
additionally benchmark against 2 baselines producing vector-tokened compositional representations:
COMET [36], and Visual Concept Tokeniser (VCT) [47]. These vector-tokened models cannot
be directly compared to our model as they are fully unsupervised, however, we include them for
completeness. We train 5 instances of each representation learning model using 5 random seeds for
200,000 iterations across all datasets, and report results averaged over the 5 random runs.
5.1 Compositional Structure / Disentanglement
To evaluate the degree to which Soft TPRs achieve explicitly compositional structure, we quantify
representational disentanglement using standard disentanglement datasets Cars3D [4], MPI3D [21],
and Shapes3D [24] (see C.4.1 for further details). As can be seen in Table 1, our model achieves
state-of-the-art disentanglement for all datasets, with notable DCI metric increases of 29% and74%
on the 2 more challenging datasets of Cars3D and MPI3D respectively. To rule out the possibility
that the improvement in disentanglement produced by our model is due to a slight increase of 13,568
(Cars3D), 1,824 (Shapes3D), 1,600 (MPI3D) learnable parameters produced by the addition of the
filler embedding matrix, MξF, to a standard (variational) encoder-decoder framework, we modify
models with fewer parameters to have an identical number of parameters as ours. We note that
the modifications are applicable only for the scalar-tokened baselines, as our model has tens of
millions less parameters than COMET and VCT. In line with [40], we observe that the performance
of scalar-tokened disentanglement models remains fairly consistent, or even deteriorates, when the
number of learnable parameters increases, so we defer control experiment results, and our associated
method to Appendix C.2.4.
Key Implication : The Soft TPR’s superior level of explicit compositionality (as quantified by the dis-
entanglement metrics) in a controlled, parameter-count environment, suggests that its fundamentally
continuous representational form is potentially easier for deep learning models to learn compared to
symbolic representational forms characterising existing disentanglement work.
85.2 Representation Learner Convergence Rate
To evaluate whether the inherently continuous compositional form of the Soft TPR can be learned
more quickly than symbolic alternatives, we consider representations produced at 100, 1,000, 10,000,
100,000 and 200,000 iterations of training, and evaluate 1) their disentanglement, and 2) their utility,
as quantified by the performance of downstream models using these representations. For downstream
model performance, we consider two commonly used [30, 46, 33] tasks: the classification-based
abstract visual reasoning task of [30] and a regression task involving the prediction of continuous
FoV values for the disentanglement datasets. Downstream models are evaluated on a fixed, held-out
test set for both tasks. While our framework does not promote faster disentanglement convergence
(results in Appendix C.4.1), it interestingly, promotes accelerated learning of useful representations
forboth downstream tasks compared to baselines. The downstream performance improvements are
particularly pronounced in the low iteration regime of 100 iterations of representation learner training,
as demonstrated by the improvements of 10%,10%, and 31% in Table 2 and the 27% improvement
in Table 3. To ensure fair comparison, we embed baseline representations of both higher, and lower
dimensionality into the same space as our model. For each baseline model, we take the best result
from either the original, or the modified model (denoted by †), and present the full suite of results,
and details of our embedding method and downstream model setup in Appendix C.
Key Implication : The representation learning convergence results suggests that while our model
may not learn the explicit compositional structure captured by disentanglement metrics more quickly
than baselines (though in the limit, the greatest possible disentanglement is higher), it learns useful
information for downstream tasks more quickly than baselines, where that useful information is
encoded in the relaxed compositional structure of the Soft TPR.
Table 1: FactorV AE and DCI scores. Additional results in Section C.3.3
ModelsCars3D Shapes3D MPI3D
FactorV AE score DCI score FactorV AE score DCI score FactorV AE score DCI score
Symbolic scalar-tokened compositional representations
Slow-V AE 0.902 ±0.035 0.509 ±0.027 0.950 ±0.032 0.850 ±0.047 0.455 ±0.083 0.355 ±0.027
Ada-GV AE-k 0.947 ±0.064 0.664±0.167 0.973 ±0.006 0.963±0.077 0.496±0.095 0.343 ±0.040
GV AE 0.877 ±0.081 0.262 ±0.095 0.921 ±0.075 0.842 ±0.040 0.378 ±0.024 0.245 ±0.074
ML-V AE 0.870 ±0.052 0.216 ±0.063 0.835 ±0.111 0.739 ±0.115 0.390 ±0.026 0.251 ±0.029
Shu 0.573 ±0.062 0.032 ±0.014 0.265 ±0.043 0.017 ±0.006 0.287 ±0.034 0.033 ±0.008
Symbolic vector-tokened compositional representations
VCT 0.966±0.029 0.382±0.080 0.957 ±0.043 0.884 ±0.013 0.689±0.035 0.475 ±0.005
COMET 0.339 ±0.008 0.024 ±0.026 0.168 ±0.005 0.002 ±0.000 0.145 ±0.024 0.005 ±0.001
Fully continuous compositional representations
Ours 0.999±0.001 0.863 ±0.027 0.984 ±0.012 0.926±0.028 0.949±0.032 0.828 ±0.015
Table 2: FoV regression R2scores (100 iterations of repre-
sentation learner training).
ModelsCars3D Shapes3D MPI3D
Symbolic scalar-tokened compositional representations
Slow-V AE 0.233 ±0.048 0.600 ±0.048 0.557±0.012
Ada-GV AE-k 0.307 ±0.084 0.684 ±0.059 0.519 ±0.023
GV AE 0.319 ±0.073 0.565 ±0.034 0.511 ±0.037
ML-V AE 0.317 ±0.058 0.551 ±0.059 0.504 ±0.016
Shu†0.012±0.007 0.461 ±0.075 0.299 ±0.040
Symbolic vector-tokened compositional representations
VCT 0.080 ±0.001 0.886±0.033 0.316±0.016
COMET†0.484±0.477 0.474±0.062 0.240 ±0.010
Fully continuous compositional representations
Ours 0.531±0.054 0.981 ±0.003 0.732 ±0.012Table 3: Abstract visual reasoning accuracy (100
iterations of representation learner training).
ModelsAbstract visual reasoning dataset
Symbolic scalar-tokened
Slow-V AE†0.552±0.035
Ada-GV AE-k†0.631±0.037
GV AE†0.554±0.031
ML-V AE†0.550±0.025
Shu 0.208 ±0.052
Symbolic vector-tokened
VCT 0.440 ±0.033
COMET†0.348±0.069
Fully continuous
Ours 0.804±0.016
5.3 Downstream Models
To evaluate whether the enhanced alignment between compositional representations and continuous
vector spaces produced by our Soft TPR benefits downstream models, we examine downstream
1) sample efficiency, and 2) raw performance in the low sample regime. We use the previously
mentioned tasks of abstract visual reasoning and FoV regression. To quantify sample efficiency, in
line with [25], we use a ratio-based metric obtained by dividing the performance of the downstream
model when trained using a restricted number of samples (100, 250, 500, 1,000 and 10,000 samples),
by its performance when trained using all samples (between 19,104-1,036,800 samples depending on
the task). As illustrated in Table 4, our model has superior sample efficiencies compared to baselines,
9especially in the most restrictive case where downstream models have access to only 100 samples
produced by representation learners, achieving a 93% improvement. The Soft TPR representations
produced by our model additionally produce substantial raw performance increases in the low sample
regime, as evidenced in Table 4, where its performance in the low-sample regimes of 100 and 200
samples constitutes a respective 138% and168% improvement, and in Table 5, a 30% improvement.
Table 4: Downstream FoV R2scores (odd columns) and sample
efficiencies (even columns) on the MPI3D dataset.
Models100 samples 100 samples/all 250 samples 250 samples/all
Symbolic scalar-tokened compositional representations
Slow-V AE 0.127 ±0.050 0.130 ±0.051 0.152 ±0.011 0.155 ±0.011
Ada-GV AE-k 0.206±0.031 0.270±0.037 0.213 ±0.023 0.279 ±0.026
GV AE 0.181 ±0.030 0.234 ±0.035 0.217 ±0.023 0.282 ±0.027
ML-V AE 0.182 ±0.013 0.236 ±0.019 0.222±0.024 0.288±0.030
Shu 0.151 ±0.016 0.343±0.024 0.211±0.026 0.482±0.075
Symbolic vector-tokened compositional representations
VCT 0.086 ±0.051 0.189 ±0.107 0.119 ±0.070 0.246 ±0.137
COMET -0.051 ±0.015 0.000 ±0.000 -0.042 ±0.018 0.000 ±0.000
Fully continuous compositional representations
Ours 0.490±0.068 0.556 ±0.078 0.594 ±0.056 0.665 ±0.067Table 5: Abstract visual reasoning ac-
curacy in the low-sample regime of 500
samples.
Models Symbolic scalar-tokened
Slow-V AE 0.196 ±0.028
Ada-GV AE-k 0.203 ±0.007
GV AE 0.182 ±0.013
ML-V AE 0.193 ±0.012
Shu 0.200 ±0.010
Symbolic vector-tokened
VCT 0.277±0.039
COMET 0.259 ±0.016
Fully continuous
Ours 0.360±0.033
Table 6: Sample efficiencies for FoV regression.
Representational form Cars3D Shapes3D MPI3D
TPR (100/all samples) 0.403 ±0.029 0.428 ±0.101 0.569±0.054
Soft TPR (100/all samples) 0.705±0.023 0.464 ±0.071 0.555±0.078
TPR (250/all samples) 0.639 ±0.055 0.634 ±0.123 0.759±0.089
Soft TPR (250/all samples) 0.889±0.042 0.730 ±0.038 0.665±0.067Table 7: Effect of model properties on disen-
tanglement performance (MPI3D dataset).
Property DCI Score
- Weak supervision 0.225 ±0.034
- Explicit filler dependency 0.718 ±0.051
- Semi orthogonality 0.756 ±0.039
Full 0.828 ±0.015
Key Implication : The consistent performance improvement we observe in generic downstream
models with no a priori knowledge of the Soft TPR’s representational form, suggests the relaxed , in-
herently continuous representation of compositional structure embodied by our Soft TPR can be more
efficiently leveraged by downstream models compared to symbolic compositional representations,
benefiting both sample efficiency, and raw performance in the low sample regime.
5.4 More Ablation Studies
We additionally repeat our full suite of experiments using the explicit TPR, ψ∗
tpr, produced by the
TPR decoder, in place of our Soft TPR, z. These experiments, a subset of which is presented in Table
6, empirically demonstrate that the Soft TPR’s continuously relaxed specification of compositional
structure confers exclusive benefits for both the representation learner and the downstream models
not captured by the traditional TPR (see Appendix C.6.1). Note for MPI3D, the explicit TPR has a
lower raw R2score when fully trained (0.785 ±0.019 vs 0.882 ±0.016), contributing to its higher
sample efficiency in Table 6. We additionally examine the importance of the following properties of
our model in producing explicitly compositional Soft TPR representations: 1) the presence of weak
supervision, by setting λ1=λ2= 0in Equation 7, 2) the explicit dependency between the quantised
filler embeddings and the decoder output, by instead using the Soft TPR to reconstruct the input
image, and 3) the semi-orthogonality of the role embedding matrix, MξRby removing this constraint
in the random initialisation of MξR, with the results of these ablations illustrated in Table 7.
6 Conclusion
In this work, we address a longstanding issue in the connectionist approach to compositionality: the
fundamental mismatch between disentangled representations and the inherently continuous nature of
deep learning vector spaces. To overcome this, we introduce Soft TPR , a novel, inherently continu-
ouscompositional representational form that extends Smolensky’s Tensor Product Representation,
together with the Soft TPR Autocoder , a theoretically-principled architecture designed for learning
Soft TPRs. Our flexible ,continuous framework yields substantial improvements in the visual domain,
enhancing compositional structure, accelerating convergence in representation learners, and boosting
efficiency in downstream models. These wide-ranging empirical benefits underscore the importance
of rethinking compositional representations to honour deep learning’s continuous foundations. Future
work will extend our continuous framework to hierarchical forms of compositionality, enabling bound
fillers themselves to decompose into role-filler bindings for enhanced representational expressivity.
10Acknowledgements
This research has been supported by an UNSW University Postgraduate Award given to B.S. We
thank the anonymous reviewers for their valuable feedback, and B. Sphear for insightful discussions.
References
[1] Noam Chomsky. Syntactic Structures . The Hague: Mouton, 1957.
[2] Jerry A. Fodor. The Language of Thought: A Theory of Mental Representation . Cambridge, MA: Harvard
University Press, 1975.
[3] Paul Smolensky. “Tensor product variable binding and the representation of symbolic structures in
connectionist systems”. In: Artificial Intelligence 46.1 (1990), pp. 159–216.
[4] Sanja Fidler, Sven Dickinson, and Raquel Urtasun. “3D Object Detection and Viewpoint Estimation with
a Deformable 3D Cuboid Model”. In: Advances in Neural Information Processing Systems . 2012.
[5] Yoshua Bengio. Deep Learning of Representations: Looking Forward . 2013. arXiv: 1305 . 0445
[cs.LG] .
[6] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. “Neural Module Networks”. In:
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015), pp. 39–48. URL:
https://api.semanticscholar.org/CorpusID:5276660 .
[7] Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merriënboer, Armand Joulin,
and Tomas Mikolov. Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks . 2015.
arXiv: 1502.05698 [cs.AI] .URL:https://arxiv.org/abs/1502.05698 .
[8] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. “InfoGAN:
Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets”. In:
Advances in Neural Information Processing Systems . Ed. by D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett. V ol. 29. Curran Associates, Inc., 2016.
[9] Paul Smolensky and Matthew A. Goldrick. “Gradient Symbolic Representations in Grammar: The case
of French Liaison”. In: 2016. URL:https://api.semanticscholar.org/CorpusID:36953611 .
[10] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir
Mohamed, and Alexander Lerchner. “beta-V AE: Learning Basic Visual Concepts with a Constrained
Variational Framework”. In: International Conference on Learning Representations . 2017.
[11] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu koray. “Neural Discrete Representation
Learning”. In: Advances in Neural Information Processing Systems . Ed. by I. Guyon, U. V on Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. V ol. 30. Curran Associates, Inc.,
2017.
[12] Tameem Adel, Zoubin Ghahramani, and Adrian Weller. “Discovering Interpretable Representations for
Both Deep Generative and Discriminative Models”. In: Proceedings of the 35th International Conference
on Machine Learning . V ol. 80. Proceedings of Machine Learning Research. PMLR, 2018, pp. 50–59.
[13] Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. “Multi-Level Variational Autoencoder:
Learning Disentangled Representations From Grouped Observations”. In: Proceedings of the AAAI
Conference on Artificial Intelligence 32.1 (2018).
[14] Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and
Alexander Lerchner. Understanding disentangling in β-VAE . 2018. arXiv: 1804.03599 [stat.ML] .
[15] Ricky T. Q. Chen, Xuechen Li, Roger Grosse, and David Duvenaud. “Isolating Sources of Disentangle-
ment in Variational Autoencoders”. In: Advances in Neural Information Processing Systems . 2018.
[16] Cian Eastwood and Christopher K. I. Williams. “A Framework for the Quantitative Evaluation of
Disentangled Representations”. In: International Conference on Learning Representations . 2018.
[17] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. “Variational Inference of Disentangled
Latent Concepts from Unlabeled Observations”. In: 6th International Conference on Learning Represen-
tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings .
2018.
[18] Adam Santoro, Felix Hill, David Barrett, Ari Morcos, and Timothy Lillicrap. “Measuring abstract
reasoning in neural networks”. In: International conference on machine learning . 2018, pp. 4477–4486.
[19] Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul Smolensky, Kenneth D. Forbus, and Jianfeng Gao.
“Natural- to formal-language generation using Tensor Product Representations”. In: CoRR abs/1910.02339
(2019). arXiv: 1910.02339 .URL:http://arxiv.org/abs/1910.02339 .
[20] Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi,
and Richard Zemel. “Flexibly Fair Representation Learning by Disentanglement”. In: Proceedings of the
36th International Conference on Machine Learning . 2019.
11[21] Muhammad Waleed Gondal, Manuel Wüthrich, undefinedor ¯de Miladinovi ´c, Francesco Locatello, Martin
Breidt, Valentin V olchkov, Joel Akpo, Olivier Bachem, Bernhard Schölkopf, and Stefan Bauer. “On
the transfer of inductive bias from simulation to the real world: a new disentanglement dataset”. In:
Proceedings of the 33rd International Conference on Neural Information Processing Systems . 2019.
[22] Haruo Hosoya. “Group-based learning of disentangled representations with generalizability for novel
contents”. In: Proceedings of the 28th International Joint Conference on Artificial Intelligence . 2019,
pp. 2506–2513.
[23] Qiuyuan Huang, Li Deng, Dapeng Wu, Chang Liu, and Xiaodong He. “Attentive Tensor Product
Learning”. In: Proceedings of the AAAI Conference on Artificial Intelligence 33.01 (2019), pp. 1344–
1351.
[24] Hyunjik Kim and Andriy Mnih. Disentangling by Factorising . 2019. arXiv: 1802.05983 [stat.ML] .
[25] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf,
and Olivier Bachem. “Challenging Common Assumptions in the Unsupervised Learning of Disentangled
Representations”. In: Proceedings of the 36th International Conference on Machine Learning . V ol. 97.
Proceedings of Machine Learning Research. PMLR, 2019, pp. 4114–4124.
[26] Emile Mathieu, Tom Rainforth, N Siddharth, and Yee Whye Teh. “Disentangling Disentanglement in
Variational Autoencoders”. In: Proceedings of the 36th International Conference on Machine Learning .
2019.
[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,
Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie
Bai, and Soumith Chintala. “PyTorch: An Imperative Style, High-Performance Deep Learning Library”.
In:Advances in Neural Information Processing Systems 32 . Curran Associates, Inc., 2019, pp. 8024–
8035. URL:http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-
performance-deep-learning-library.pdf .
[28] Imanol Schlag and Jürgen Schmidhuber. “Learning to Reason with Third-Order Tensor Products”. In:
Advances in Neural Processing Information Systems . 2019.
[29] Imanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic, Jürgen Schmidhuber, and Jianfeng
Gao. “Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving”. In:
ArXiv abs/1910.06611 (2019). URL:https://api.semanticscholar.org/CorpusID:204575948 .
[30] Sjoerd van Steenkiste, Francesco Locatello, Jürgen Schmidhuber, and Olivier Bachem. “Are disentan-
gled representations helpful for abstract visual reasoning?” In: Proceedings of the 33rd International
Conference on Neural Information Processing Systems . 2019.
[31] Junxiang Chen and Kayhan Batmanghelich. “Weakly Supervised Disentanglement by Pairwise Sim-
ilarities”. In: Proceedings of the AAAI Conference on Artificial Intelligence 34.04 (2020), pp. 3495–
3502.
[32] Zheng Ding, Yifan Xu, Weijian Xu, Gaurav Parmar, Yang Yang, Max Welling, and Zhuowen Tu. “Guided
Variational Autoencoder for Disentanglement Learning”. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) . 2020.
[33] F. Locatello, B. Poole, G. Rätsch, B. Schölkopf, O. Bachem, and M. Tschannen. “Weakly-Supervised
Disentanglement Without Compromises”. In: Proceedings of the 37th International Conference on
Machine Learning (ICML) . V ol. 119. Proceedings of Machine Learning Research. PMLR, 2020, pp. 6348–
6359.
[34] R. Thomas McCoy, Tal Linzen, Ewan Dunbar, and Paul Smolensky. “Tensor Product Decomposition
Networks: Uncovering Representations of Structure Learned by Neural Networks”. In: Proceedings
of the Society for Computation in Linguistics 2020 . Association for Computational Linguistics, 2020,
pp. 277–278. URL:https://aclanthology.org/2020.scil-1.34 .
[35] Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. “Weakly Supervised Disentan-
glement with Guarantees”. In: International Conference on Learning Representations . 2020.
[36] Yilun Du, Shuang Li, Yash Sharma, B. Joshua Tenenbaum, and Igor Mordatch. “Unsupervised Learning
of Compositional Energy Concepts”. In: Advances in Neural Information Processing Systems . 2021.
[37] Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, and Gunhee Kim. “IB-GAN: Disentangled Representa-
tion Learning with Information Bottleneck Generative Adversarial Networks”. In: Proceedings of the
AAAI Conference on Artificial Intelligence 35.9 (May 2021), pp. 7926–7934. DOI:10.1609/aaai.
v35i9.16967 .URL:https://ojs.aaai.org/index.php/AAAI/article/view/16967 .
[38] Yichen Jiang, Asli Celikyilmaz, Paul Smolensky, Paul Soulos, Sudha Rao, Hamid Palangi, Roland
Fernandez, Caitlin Smith, Mohit Bansal, and Jianfeng Gao. “Enriching Transformers with Structured
Tensor-Product Representations for Abstractive Summarization”. In: Proceedings of the 2021 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies . Online: Association for Computational Linguistics, 2021, pp. 4780–4793. DOI:10.18653/
v1/2021.naacl-main.381 .URL:https://aclanthology.org/2021.naacl-main.381 .
12[39] David A. Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge,
and Dylan Paiton. “Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding”.
In:International Conference on Learning Representations . 2021. URL:https://openreview.net/
forum?id=EbIDjBynYJ8 .
[40] Milton Llera Montero, Casimir JH Ludwig, Rui Ponte Costa, Gaurav Malhotra, and Jeffrey Bowers. “The
role of Disentanglement in Generalisation”. In: International Conference on Learning Representations .
2021. URL:https://openreview.net/forum?id=qbH974jKUVy .
[41] Sungho Park, Sunhee Hwang, Dohyung Kim, and Hyeran Byun. “Learning Disentangled Representation
for Fair Facial Attribute Classification via Fairness-aware Information Alignment”. In: Proceedings of
the AAAI Conference on Artificial Intelligence 35 (2021), pp. 2403–2411. DOI:10.1609/aaai.v35i3.
16341 .URL:https://ojs.aaai.org/index.php/AAAI/article/view/16341 .
[42] L. Schott, J. von Kügelgen, F. Träuble, P. Gehler, C. Russell, M. Bethge, B. Schölkopf, F. Locatello, and
W. Brendel. “Visual Representation Learning Does Not Generalize Strongly Within the Same Domain”.
In:ICLR 2021 - Workshop on Generalization beyond the training distribution in brains and machines .
2021.
[43] Frederik Träuble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal,
Bernhard Schölkopf, and Stefan Bauer. “On Disentangled Representations Learned from Correlated
Data”. In: Proceedings of the 38th International Conference on Machine Learning . V ol. 139. Proceedings
of Machine Learning Research. 2021, pp. 10401–10412.
[44] Milton Montero, Jeffrey Bowers, Rui Ponte Costa, Casimir Ludwig, and Gaurav Malhotra. “Lost in
Latent Space: Examining failures of disentangled models at combinatorial generalisation”. In: Advances
in Neural Information Processing Systems . Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,
K. Cho, and A. Oh. V ol. 35. Curran Associates, Inc., 2022, pp. 10136–10149.
[45] Zoltán Gendler Szabó. “Compositionality”. In: The Stanford Encyclopedia of Philosophy . Ed. by Edward
N. Zalta and Uri Nodelman. Fall 2022. Metaphysics Research Lab, Stanford University, 2022.
[46] Tao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, and Nanning Zheng. “Towards Building A
Group-based Unsupervised Representation Disentanglement Framework”. In: International Conference
on Learning Representations . 2022. URL:https://openreview.net/forum?id=YgPqNctmyd .
[47] Tao Yang, Yuwang Wang, Yan Lu, and Nanning Zheng. “Visual Concepts Tokenization”. In: Advances in
Neural Information Processing Systems . 2022.
[48] H. Zhang, Y .-F. Zhang, W. Liu, A. Weller, B. Schölkopf, and E. Xing. “Towards Principled Disentangle-
ment for Domain Generalization”. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) . 2022, pp. 8024–8034. URL:https://openaccess.thecvf.com/
content/CVPR2022/papers/Zhang_Towards_Principled_Disentanglement_for_Domain_
Generalization_CVPR_2022_paper.pdf .
[49] Thaddäus Wiedemer, Prasanna Mayilvahanan, Matthias Bethge, and Wieland Brendel. “Compositional
Generalization from First Principles”. In: Thirty-seventh Conference on Neural Information Processing
Systems . 2023. URL:https://openreview.net/forum?id=LqOQ1uJmSx .
[50] Haoyang Li, Xin Wang, Zeyang Zhang, Haibo Chen, Ziwei Zhang, and Wenwu Zhu. “Disentangled
Graph Self-supervised Learning for Out-of-Distribution Generalization”. In: Forty-first International
Conference on Machine Learning . 2024. URL:https://openreview.net/forum?id=OS0szhkPmF .
[51] Taewon Park, Inchul Choi, and Minho Lee. “Attention-based Iterative Decomposition for Tensor Product
Representation”. In: The Twelfth International Conference on Learning Representations . 2024. URL:
https://openreview.net/forum?id=FDb2JQZsFH .
[52] Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, and Dapeng Wu. “Tensor Product Generation
Networks for Deep NLP Modeling”. In: Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long Papers) . Association for Computational Linguistics.
13Appendix
A TPR Framework 14
A.1 Additional Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.2 Formal Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B Soft TPR Framework 16
B.1 Shortcomings of the TPR and How Soft TPR Helps . . . . . . . . . . . . . . . . . 16
B.2 Alternative Formulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.3 Soft TPR Autoencoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.4 Model Hyperparameters and Hyperparameter Tuning . . . . . . . . . . . . . . . . 22
C Results 23
C.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.2 Baseline Implementations and Experimental Settings . . . . . . . . . . . . . . . . 23
C.3 Disentanglement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
C.4 Representation Learning Convergence . . . . . . . . . . . . . . . . . . . . . . . . 27
C.5 Downstream Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
C.6 More Ablation Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
D Limitations and Future Work 76
D.1 Extension to Linguistic Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
D.2 Need for Weak Supervision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
D.3 Downstream Utility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
D.4 Dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
D.5 Computational Cost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
A TPR Framework
In this section, we provide additional details regarding Smolensky’s TPR framework [3], as well as
formal proofs of presented results. We refer interested readers to [3] for a more comprehensive dive
into this fascinating framework.
A.1 Additional Details
By defining the constituent components of any compositional object as a set of role-filler bindings,
the TPR defines the decomposition function, β, of Section 3.1 that maps from a set of compositional
objects, X, to a set of parts, more explicitly as follows [3]:
β:X→2F×R;x→ {(f, r)|f/r}, (8)
where Fdenotes a set of fillers, and Rdenotes a set of roles. Note that in contrast to the formal
definition of βwe use in Section 3.1, which assumes each x∈Xis decomposable into a set of
nparts, the above decomposition allows objects to be decomposed into a variably-sized set of
role-filler bindings, with this set corresponding to an element in the powerset of F×R. For the
visual representation learning domain we consider, all considered disentanglement datasets clearly
have the property of being decomposable into a fixed size set of role-filler bindings, as all images in
14these datasets contain the same number of FoV types and each FoV type is bound to a FoV token .
Due to this property, we can take βas a special subcase of the generalised definition in Equation 8:
β:X→ A :x→ {(fm(i), ri)|fm(i)/ri}. (9)
where m:{1, . . . , N R} → { 1, . . . , N F}denotes a matching function that associates each role ri
with the filler it binds to in β(x)(we again drop the dependence of monxfor ease of notation), and
Adenotes the set of all possible bindings produced by binding a filler to each of the NRroles, with
size NF+NR−1
NR
(we assume the same filler can bind to multiple roles).
A.2 Formal Proofs
We now formally prove that the TPR with βdefined in 9 has form ψtpr(x) =C(ψ1(a1), . . . , ψ n(an))
and thus corresponds to the definition of a compositional representation in Section 3.1.
Proof. We denote the role embedding and filler embedding functions as ξR:R→VR, ξF:F→VF
respectively.
By definition of the TPR in Eq 1, we have that:
ψtpr(x) :=X
iξF(fm(i))⊗ξR(ri).
and hence,
ψtpr(x) =X
iψi(fm(i), ri),
where ai:= (fm(i), ri)∈β(x),ψi:F×R→VF⊗VR; (fm(i), ri)→ξF(fm(i))⊗ξR(ri), andC
is ordinary vector space addition. Hence, almost trivially, ψtpr(x)clearly has the required form to be
acompositional representation .
Now, we prove the recoverability of the embedded components {ψi(fm(i), ri)}from the TPR,
ψtpr(x), provided that the set of all role embedding vectors, {ξR(ri)}, are linearly independent.
Similar variants of this proof can be found in [3], [19].
Proof. Assume the set of all role embedding vectors {ξR(ri)}are linearly independent. Then, the
role embedding matrix, MR:= (ξR(r1). . . ξR(rNR))formed by taking the role embedding vectors
as columns, has a left inverse, U, such that:
UMξR=INR×NR.
Hence, we have that (UMξR)ij=Ui:MξR:j=Iij.
For ease of notation, let uidenote the i-th column of UT, and note that ξR(rj)clearly corresponds to
MξR:j. So,Ui:MξR:j= (UT
:i)TMξR:j=uT
iξR(rj) =Iij.
Hence, we have that:
uT
iξR(rj) =δij=1i=j
0otherwise.
Using the definition of ψtpr(x),ψtpr(x) =P
iξF(fm(i))⊗ξR(ri), we apply the (tensor) inner
product of ψtpr(x)withui:
15ψtpr(x) = X
iξF(fm(i))⊗ξR(ri)!
ui
= X
iξF(fm(i))ξR(ri)T!
ui
=X
iξF(fm(i))δji
=ξF(fm(i)).
Thus, the embedding of the filler, fm(i), bound to each role, ri, can be recovered through use of a
tensor inner product with the unbinding vector, ui, corresponding to the i-th column of UT. Note
that the representational components of ψtpr(x), i.e., the embedded bindings, ψi(fm(i), ri)are fully
determined by the embedding of the role, ξR(ri), and filler, ξF(fm(i)), comprising the binding, as
ξF(fm(i), ri)simply corresponds to their tensor product. Thus, recovering (ξ(fm(i)), ξR(ri))for
each binding in β(x)corresponds to recovering the representational component, ψi(fm(i), ri). So,
provided the set of role embeddings are linearly independent, and they can be obtained (e.g. through
a look-up table of role embeddings), all representational components, ψi(fm(i), ri)can be directly
recovered from the overall TPR representation, ψtpr(x).
B Soft TPR Framework
In this section, we provide additional information regarding our Soft TPR framework, more ex-
tensively detailing our theoretically-informed rationale behind certain design decisions, as well as
providing all necessary model architecture information for our Soft TPR Autoencoder to replicate
our results (note that code is available at https://github.com/gomb0c/soft_tpr/ ).
B.1 Shortcomings of the TPR and How Soft TPR Helps
In this subsection, we provide a more detailed explanation regarding the fundamental limitations
produced by the TPR’s stringent specification (Eq 1). For concreteness, we use an example. We
consider the following set of roles and fillers, R={shape ,colour }, F={red,blue,square }.
Additionally, we define the role ξR:R→R2and filler ξF:F→R3embedding functions as
follows:
ξR(shape) =
1
0
,
ξR(colour) =
1
1
,
ξF(red) ="1
2
3#
,
ξF(blue) ="2
2
3#
,
ξF(square) ="0
0
1#
,
1.Discrete Mapping : Consider the set of possible TPRs, T, that can be produced by
R, F, ξ R, ξFas defined above. We have:
16ψtpr(xred_square ) =ξF(red)⊗ξR(colour) + ξF(square) ⊗ξR(shape)
="1
2
3#
⊗
1
1
+"0
0
1#
⊗
1
0
∼=
1
2
3
1
2
3
+
0
0
1
0
0
0

=
1
2
4
1
2
3

ψtpr(xblue_square ) =ξF(blue) ⊗ξR(colour) + ξF(square) ⊗ξR(shape)
="2
2
3#
⊗
1
1
+"0
0
1#
⊗
1
0
∼=
2
2
3
2
2
3
+
0
0
1
0
0
0

=
2
2
4
2
2
3

These 2 possible TPRs form a discrete, 2 element subset of the underlying representational
space,R6,T={(2 2 4 2 2 3)T,(1 2 4 1 2 3)T}. Relaxing the TPR specification to the Soft
TPR allows any point, z, in the underlying representational space, R6to qualify as a Soft
TPR provided that for some explicit TPR, ψtpr∈T, the sufficient closeness requirement
holds: ||z−ψtpr||2< ϵ. So, if we consider the set of possible Soft TPRs, TS, we have =
TS:={(2 2 4 2 2 3)T+α,(1 2 4 1 2 3)T+α:|α|< ϵ}. Hence, TSclearly has strictly more
points than T, so there should be more functions parameterising the map from the observed
data to TScompared to T. Furthermore, in contrast to T, which contains discrete (singular)
points scattered around in R6,TScontains continuous cloud-like regions centered at each
ψtpr∈T. Both these factors should make the Soft TPR representation potentially easier to
learn and extract information from, compared to the TPR. This is reflected in our empirical
results in Section C.6.1, where, compared to the traditional TPR, the Soft TPR demonstrates
1) greater representation learner convergence, 2) superior sample efficiency for downstream
tasks, and 3) superior raw downstream performance in the low sample regime.
2.Quasi-Compositional Structure : The traditional TPR enforces a strict algebraic definition
of compositionality (i.e.P
iξF(fm(i))⊗ξR(ri)). Even explicitly algebraically-structured
domains such as natural language, do not always adhere to this rigid specification of
compositional structure (e.g. French liaison consonants, which consist of a weighted sum of
fillers bound to a single role [9]). The Soft TPR’s continuous relaxation of this constraint
allows it to represent structures that only approximately satisfy the TPR’s strict definition
of compositionality. A more relevant example could be the construction of compositional
representations of coloured squares where the colour is a weighted combination of fillers blue
andred(i.e., varying shades of purple ). In this case, even if purple has not been seen by
the TPR Autoencoder previously (i.e., there is no quantised filler representing ξF(purple) ),
it may be possible for the TPR Autoencoder to produce a Soft TPR corresponding to
purple square if there is a suitable ϵ-level relaxation of any of the explicit TPRs (i.e.,
red square ,blue square ) that represents a purple square .
173.Serial Construction : Building explicit TPRs requires that the embedded fillers and roles
comprising a binding (i.e. (ξF(fm(i)), ξR(ri))) are first tokened before the entire compo-
sitional representation, ψtpr(x)can be produced [19, 23, 28, 34, 38, 51, 52]. This sort of
sequential approach, where constituents must be tokened before the compositional represen-
tation can be formed, is a key characteristic of the symbolic representation of compositional
structure [45]. In contrast, the Soft TPR allows the Encoder to produce any arbitrary element
ofVF⊗VR(in this case ∼=R6), provided that the sufficient closeness requirement holds.
Thus, once the Encoder is trained, it is theoretically possible for the Soft TPR Autoencoder to
exploit vector space continuity to generate approximately compositional representations by
mapping directly from the data to a Soft TPR, without needing to token any representational
constituents.
B.2 Alternative Formulations
In contrast to the greedily optimal analytic form of ψ∗
tprthat we derive in Equation 5, it is possible
to instead derive a globally optimal TPR that the (DR·DF)-dimensional vector, z, produced by
the encoder, E, best approximates with reference to the Frobenius norm metric of ||z−ψtpr||F.
We fix the set of fillers, F, and roles, R, as well as the embedding functions, ξF:F→RDFand
ξR:R→RDRto be arbitrary sets and embedding functions respectively. Note that with F,R,
ξF, and ξRfixed, the only degree of freedom in defining the space of possible TPRs is given by
defining M∗, the set of role-filler matching functions that define the permissible role-filler binding
decompositions of x∈X. The globally optimal TPR that zbest approximates, ψopt
tpr, is given by:
ψopt
tpr:= arg min
ψtpr∈T||z−ψtpr||F, (10)
where T:={P
iξF(fm(i))⊗ξR(ri)}m∈M∗denotes the set of all possible TPRs given the choices
ofF,R,ξF,ξRandM∗. Again, we drop the dependency of both ψtprandmonxfor ease of
notation.
IfM∗is a subset of the entire set of matching functions, M, i.e., if there are some constraints to what
fillers can be bound to what roles, then, solving for 10 is NP-complete. If M∗however corresponds
to the entire set of possible matching functions, i.e., M∗=M, then, 10 can be solved by simply
using any method that solves the (one-to-many) assignment problem. Noting the poor worst case
time complexity of O(n3)for such solutions, and guided by our intuition that there should be a
more explicit dependency between ψ∗
tprand the structure of z, we thus propose the greedily optimal
definition of ψ∗
tprin 5, which creates an explicit dependency between the unbound softfillers of z,
{˜fi}, and ψ∗
tpr.
B.3 Soft TPR Autoencoder
Note that defining masm(i) := arg minj||˜fk−ξF(fj)||2in Equation 5 effectively per-
mits any filler, fj, to bind to a role, ri. This is somewhat inevitable given the weakly
supervised framework we are situated in, as without knowledge of the ground-truth FoV
type-token bindings, it is impossible to define M∗as a subset of all possible matching func-
tions, M, that produces the ground-truth set of role-filler binding decompositions, i.e. we
cannot know
m∈ M|{ (ξF(fm(i)), ξR(ri))}=β(x)	
x∈X. We observe, however, that the
size of Mis combinatorially large, i.e., |M| = NF+NR−1
NR
, and contains role-filler bind-
ing decompositions that are clearly misaligned with the ground-truth semantics of images, e.g.,
{cube/object colour ,purple /object shape ,green /orientation ,blue/wall colour ,2π/floor colour }
for the image in Figure 2. We thus, decide to design our Soft TPR Autoencoder with 2 orthogonal
and separately achieved aims: 1) representational form , to ensure the produced representation has the
form of a Soft TPR, and 2) representational content , to ensure that mproduces sensible role-filler
bindings, and hence, that the Soft TPR, z, which best approximates ψ∗
TPR reflects the ground-truth
FoV type-token bindings of the image.
18B.3.1 Representational Form
We now provide additional details on how our method achieves the representational form property.
Recall from Section 4.2, that we penalise the Euclidean distance between the encoder, E’s output,
z, and the explicit TPR ψ∗
tprthatzbest approximates with the analytic form of Equation 5. Such a
penalisation should ideally encourage the Soft TPR Autoencoder to produce encodings of inputted
images that have the form of a Soft TPR, as all of E’s outputs are directly penalised to satisfy the
sufficient closeness property of ||z−ψtpr||25for the TPR, ψtpr=ψ∗
tpr. To recover ψ∗
tpr, we use
our TPR decoder. As specified in Section 4.2, the TPR decoder consists of 3 main modules, 1)
Unbinding , where the soft fillers of z,{˜fi}are unbound from z, 2)Quantisation , where the explicit
TPR filler embeddings {ξF(fm(i))}with the closest Euclidean distances to each soft filler of zare
obtained, and 3) TPR Construction , where the required operations are applied to the embedding
vectors of the role-filler bindings produced from previous modules, to construct the explicit TPR,P
iξF(fm(i))⊗ξR(ri)with the analytic form of 5. We now provide additional details on these 3
modules.
1) Unbinding : While FoV types (roles) can be bound to the same FoV value (fillers), e.g.
object colour /magenta ,floor colour /magenta , each FoV type (role) clearly represents an inde-
pendent visual concept type, and so, it is reasonable to use linearly independent embedding vectors
for the FoV types. Thus, within our considered domain of visual representation learning, we can
reasonably satisfy the linear independence requirement that ensures recoverability of the embedded
representational components (ξF(fm(i)), ξR(ri))from the TPR representation. Note that any ran-
domly initialised role embedding matrix MξR∈RDR×NRis likely (though not guaranteed) to consist
ofNRlinearly independent role embedding vectors provided DR>> N R, and furthermore, that
we can encourage that the linear independence property to be preserved during all stages of training
by adding orthogonality regularisation, ||MT
ξRMξR−INR×NR||F. There is one pertinent problem,
however, if we simply let the role embedding matrix, MξR, consist of any set of arbitrary, linearly
independent role embedding vectors, {ξR(ri)}. That is, for any choices of NR, DR, where these
values are large, it is computationally expensive to obtain U, the (left) inverse of MξRrequired to
produce the unbinding vectors, {ui}, needed for unbinding. We thus, decide to leverage the properties
of (left-invertible) semi-orthogonal matrices, A∈Rd×n, for which ATA=In×n. In this case,
UT=MξR, and so, the i-th unbinding vector, uisimply corresponds to the i-the role embedding
vector, ξR(ri). Thus, we can simply unbind the soft filler embedding from z’bound’ to role riby
taking the (tensor) inner product between zandξR(ri), thei-th column of MξR.
To accomplish this in practice, our unbinding module randomly initialises the role embedding matrix,
MξRas a semi-orthogonal matrix, through use of torch .nn.utils.parameterizations .orthogonal ,
and fixes MξRduring all stages of training (i.e., gradient is notbackpropagated to MξR), to ensure
that the semi-orthogonal property is preserved during all stages of training. Fixing MξRmay prompt
concerns that that role embedding vectors do not capture semantically informative content. However,
for the visual representation learning domain we consider, it is intuitive that role embeddings
themselves do not need to convey much semantic content, as the majority of an image’s semantic
content can be conveyed through filler embeddings and the bindings of fillers to roles. More concretely,
filler embeddings, being learnable, can convey semantic information by being close (in some norm) to
one another, e.g., the filler embeddings for {green ,magenta ,blue}may be close in Euclidean distance
to one another, and thus convey the semantic information that they correspond to values for the same
(colour) role. Additionally, as the decoder, D, receives information from all embedded bindings in
the form of the TPR, ψ∗
tpr, to perform image reconstruction, Dshould intuitively learn through the
reconstruction loss term of Equation 6, the semantics associated with a role. For example, given
the embedding for binding (oblong/object shape ),Dshould learn from the reconstruction loss, the
mapping between the randomly initialised role embedding vector for object colour and the semantics
of the corresponding FoV type (i.e., that this role changes the colour of the object in the image).
Now, we proceed to formally prove the aforementioned semi-orthogonality properties.
We first prove that for a (left-invertible) semi-orthogonal matrix, A∈Rd×n,ATA=In×n.
5We again assume we are working with vector spaces over the reals, and use the vector space isomorphism
property of RDF⊗RDR∼=RDF·DRto align the TPR framework more seamlessly with the autoencoding
framework.
19Proof. LetAdenote a (left-invertible) semi-orthogonal matrix of dimension Rd×n. Writing Aout
using its columns, {a1, . . . , a n}, we have:
ATA=
aT
1...
aT
n
(a1···an)
As the columns {a1, . . . , a n}are orthonormal, we have that: ai·aj=δij, where ·denotes the dot
product, and
δij=1i=j
0otherwise,
from which the result clearly follows.
For completeness, we also prove that uicorresponds to the i-th role embedding vector, though this
follows rather trivially from the properties of semi-orthogonal matrices, and the definition of ui.
Proof. LetMξRbe a (left-invertible) semi-orthogonal matrix. Then, from the properties of semi-
orthogonal matrices proved earlier, we have that MT
ξRMξR=I, and so, UT=
MT
ξRT
=MξR.
Hence, the i-th column of UT, which corresponds to the unbinding vector, ui, by definition, is simply
(MξR):i=ξR(ri), thei-th role embedding vector.
2) Quantisation
The quantisation module relies on the VQ-V AE vector quantisation algorithm [11] to learn the filler
embedding vectors of the filler embedding matrix, MξF, and to quantise the soft filler embeddings
{˜fi}into the explicit filler embeddings {ξF(fi)}with the closest Euclidean distances. Briefly
speaking, to perform vector quantisation, the VQ-V AE algorithm simply quantises each soft filler
˜fiinto the embedding vector from MξFwith the smallest Euclidean distance [11]. This clearly
corresponds with the definition of min Equation 5 (i.e. for each soft filler embedding, ˜fi,m
matches an explicit filler embedding with the smallest Euclidean distance to ˜fito produce ψ∗
tpr). As
this quantisation operation corresponds to an argmax and is thus non-differentiable, the VQ-V AE
algorithm uses a simple L2loss, the codebook loss , to move the embedding vectors ξR(ri)towards the
soft filler embeddings, as captured by the first term of the VQ-V AE quantisation loss term in Equation
6. To prevent the embedding space from growing arbitrarily, a commitment loss , corresponding to
the final term of the VQ-V AE quantisation loss in 6 is added, to ensure the encoder commits to an
embedding.
We refer interested readers to [11] for more details on how the quantisation algorithm works.
3) TPR Construction
The TPR construction module contains no learnable parameters, and simply deterministically applies
the operations required to produce a TPR from the quantised filler embeddings, {ξF(fm(i))}, with
the corresponding role embeddings, {ξR(ri)}, from the role embedding matrix, MξR.
B.3.2 Representational Content
In this section, we provide an explicit form for the ‘swapped’ TPRs, ψs
tpr(x)andψs
tpr(x′), used in
our weakly supervised reconstruction loss corresponding to the second term of 7.
Given a pair of images, (x, x′)where xandx′share the same role-filler bindings for all roles but ri,
and the identity of ri, but not any of the role-filler bindings are known, we construct the ‘swapped’
TPRs for xandx′as follows:
ψs
tpr(x) :=X
j̸=iξF(fm(j))⊗ξR(rj) +ξF(fm′(i))⊗ξR(ri) (11)
ψs
tpr(x′) :=X
j̸=iξF(fm′(j))⊗ξR(rj) +ξF(fm(i))⊗ξR(ri), (12)
20where mandm′denote the matching functions for xandx′respectively. That is, we construct
‘swapped’ TPRs by simply swapping the quantised filler embedding associated with role riin
constructing new TPR representations for xandx′. Note that in contrast to a similar operation that
might be applied to scalar-tokened orvector-tokened compositional representations, swapping the
quantised filler between the representations for xandx′in this case produces a global (not local)
effect on the resulting representation.
B.3.3 Model Architecture
We now provide concrete details of our model architecture. Our model consists of a standard Conv-
based encoder, E, (Table 8) our TPR decoder, (Table 9) and a standard Conv-transpose based decoder,
D(Table 10). The only learnable component of the TPR decoder corresponds to the filler embedding
matrix, MξF, and so, our Soft TPR Autoencoder only adds an additional DF·NFparameters to a
standard (variational) autoencoding framework consisting of the encoder and decoder, where DFand
NFcorrespond to the dimensionality of the filler embedding space, and number of filler embedding
vectors respectively.
For all our experiments, we use the same general architecture for the encoder, E, the TPR decoder,
and the decoder, D.
Table 8: Encoder (E)architecture.
Encoder
Input 64×64×c
Conv 32,4×4,stride = 2 ,padding = 1
BatchNorm 32
ReLU 32
Conv 64,4×4,stride = 2 ,padding = 1
BatchNorm 64
ReLU 64
Conv 256,4×4,stride = 2 ,padding = 1
BatchNorm 256
ReLU 256
Conv 512,4×4,stride = 2 ,padding = 1
BatchNorm 512
ReLU 512
Flatten 512×4×4
Linear 1024
ReLU 1024
Linear 512
ReLU 512
Linear 512
ReLU 512
Linear DF·DR
Table 9: TPR Decoder architecture.
TPR Decoder
Role embedding matrix ( fixed )DR×NR
Filler embedding matrix DF×NF
21Table 10: Decoder (D)architecture.
Decoder
Input DF·DR
ConvTranspose 512,4×4,stride = 2 ,padding = 1
BatchNorm 512
ReLU 512
ConvTranspose 256,4×4,stride = 2 ,padding = 1
BatchNorm 256
ReLU 256
ConvTranspose 64,4×4,stride = 2 ,padding = 1
BatchNorm 64
ReLU 64
ConvTranspose 32,4×4,stride = 2 ,padding = 1
BatchNorm 32
ReLU 32
ConvTranspose 3,4×4,stride = 2 ,padding = 1
B.4 Model Hyperparameters and Hyperparameter Tuning
The tunable hyperparameters of the Soft TPR Autoencoder are the following following: 1) archi-
tectural hyperparameters NR, NF, DR, and DFcorresponding to the number of role (respectively
filler) embedding vectors and the dimensionalities of their respective embedding spaces, and 2) loss
function hyperparameters β(Equation 6), λ1(Equation 7) and λ2(Equation 7).
In line with VQ-V AE [11], we set β, the coefficient for the VQ-V AE commitment loss to0.5. AsNR
corresponds to the number of FoV types , to ensure fair comparisons with scalar-tokened generative
baselines, and COMET, which all assume that the number of FoVs equals 10, (VCT assumes 20),
we fix NRto be 10. We tune remaining hyperparameters by running hyperparameter optimisation
using the open-source hyperparameter sweep framework of Weights and Biases (WandB). We set
the search method as Bayesian search, and the optimisation criterion to be the fully unsupervised
MSE reconstruction loss corresponding to the second term in Equation 6. During hyperparameter
optimisation, we train all models for between 50,000-100,000 iterations. The obtained hyperparameter
values for the Soft TPR Autoencoder are listed in Table 11. See Section C.6.2 for ablation experiments
demonstrating our model’s robustness to hyperparameter configurations.
Table 11: Hyperparameter values.
HyperparameterCars3D Shapes3D MPI3D
Architectural hyperparameters
DR 12 16 12
NR(fixed) 10 10 10
DF 128 32 32
NF 106 57 50
Loss function hyperparameters
λ1 0.00024 0.00091 0.0000
λ2 0.02200 0.00228 1.16050
β(fixed) 0.5 0.5 0.5
In practice, we find that the Soft TPR penalty of Equation 6 is negligible, most likely because
thecodebook loss of VQ-V AE, which pushes the quantised fillers {ξF(fm(i))}and the soft filler
embeddings {˜fi}ofztogether, is already sufficient to push ztoψ∗
tpr. To prevent redundantly
calculating this term, we remove it from the overall loss in the implementation of our model.
Our model is implemented in Pytorch [27] and trained using the Adam optimiser on the loss
corresponding to 7. We use a learning rate of 1e−4, and the default setting of (β1, β2) = (0 .9,0.999)
across all instances of model training.
22C Results
In this section, we provide additional details about our experiments, and present additional results.
C.1 Datasets
For disentanglement, we consider the standard disentanglement datasets: Cars3D [4], Shapes3D
[24], and MPI3D [21], where for MPI3D, we consider the ‘real’ variant of the dataset, (i.e., not the
‘realistic’, ‘toy’, or ‘complex’ variants of the dataset). These datasets contain 3,6, and 7ground-truth
FoVs respectively, and are procedurally generated by taking the Cartesian product of all possible FoV
values for each FoV type. We provide metadata in Tables 12, 13 and 14 and denote all continuous-
valued FoV types by the symbol‡. As the colours in the Shapes3D dataset correspond to 10 linearly
spaced values with a natural ordering (e.g. red, with value 0, is perceptually more similar to orange,
with value 1, compared to aqua, with value 5), we treat all FoV colour types in the Shapes3D dataset
as continuous-valued variables, in line with [42].
Table 12: Cars3D dataset
FoV types FoV values
Car type 199 types
Rotation‡24 angles
Camera elevation‡4 typesTable 13: Shapes3D dataset
FoV types FoV values
Floor colour‡10 colours
Wall colour‡10 colours
Object colour‡10 colours
Object size‡8 sizes
Object type 4 shapes
Azimuth‡15 anglesTable 14: MPI3D dataset
FoV types FoV values
Object colour 6 colours
Object shape 6 shapes
Object size‡2 sizes
Camera height 3 heights
Background colour 3 colours
Horizontal axis‡40 values
Vertical axis‡40 values
For the downstream task of regression, we simply use the above disentanglement datasets, regressing
on each dataset’s continuous-valued FoVs, as indicated by the‡symbol in Tables 12, 13 and 14. For
the downstream task of abstract visual reasoning [30], we use the dataset from [30]. Each sample
from this dataset consists of a Raven’s Progressive Matrix (RPM) style question (samples of the
dataset can be found in [30]). Each RPM-style question has a set of 8 context panels, and 6 answer
panels, where each panel corresponds to an image from the Shapes3D dataset. For the model to select
the correct answer out of a selection of 6possible answer panels, it must correctly infer the abstract
visual relation that each row of the context panels share.
C.2 Baseline Implementations and Experimental Settings
C.2.1 Experiment Compute Resources
For the generative weakly supervised, scalar-valued baselines (i.e., AdaGV AE-k, GV AE, MLV AE,
SlowV AE, the Shu model), and our model, we perform model training on a single Nvidia RTX4090
GPU. We also perform all associated experiments for these models (i.e., downstream model training,
downstream model evaluation, disentanglement evaluation, etc.) on this single Nvidia RTX4090
GPU. Our model takes approximately 1.5 hours to fully train (i.e., run 200,000 iterations) on the
Cars3D and Shapes dataset, and approximately 4.0 hours to fully train on the MPI3D dataset.
For the vector-valued baselines of COMET and VCT, we perform model training, and all associated
experiments on a single Nvidia V100 GPU.
C.2.2 Disentanglement Models
For Ada-GV AE, GV AE, MLV AE and SlowV AE, use the Pytorch-based, open-source implementation
of [42], which was verified by authors to reproduce official reported results for each model. For
COMET, and VCT, we use official open-source implementations published by the authors of the
corresponding models [36, 47]. For the GAN-based model of Shu, we convert the official Tensorflow-
based implementation of [35] to Pytorch and verify that our implementation reproduces official results.
For all baseline models, we use suggested hyperparameters where dataset-specific hyperparameters
are given, otherwise, we perform hyperparameter tuning using an identical WandB hyperparameter
sweep setup as our model.
23As mentioned in the first paragraph of Section 5, for the weakly supervised baselines of Ada-GV AE,
GV AE, MLV AE, SlowV AE and Shu, all models with exception to Ada-GV AE assume access to I,
the FoV that differ between each pair in an observed sample (x, x′), and thus have identical levels of
weak supervision as our model. Ada-GV AE, however, does not assume access to I, so we make the
following modification to make Ada-GV AE more comparable with our model. Ada-GV AE adaptively
estimates Iusing a method that relies on the estimation of k:=|I|, i.e., the number of FoVs that
have changed in each observed sample. Thus, we simply amend Ada-GV AE by providing it access
to the ground-truth value for k(note that if we instead provide Ada-GV AE with knowledge of the
FoVs comprising Iitself, it becomes identical to GV AE). We empirically verify that our modification,
which we denote by Ada-GV AE-k, produces superior or in worst case, comparable results to the
original Ada-GV AE model. We train all models, with exception to SlowV AE, which assumes all
FoV values in observed pairs (x, x′)change, with k= 1(i.e., only 1 FoV changes between xandx′).
This ensures all models are trained in an identical setting as our framework.
For the selected hyperparameter configuration associated with each model, including our own, we
obtain 5trained models using 5random seeds, and collate results over these 5seeds. All models are
trained for 200,000 iterations on all datasets.
C.2.3 Downstream Models
For the task of FoV regression, in line with [42], we use a simple, generic MLP model, with the
architecture listed in Table 15. For each disentanglement dataset, the MLP regression model receives
representations of images produced by a representation learner (i.e., a disentanglement model), and is
trained to predict the corresponding ground-truth FoV values in a supervised fashion. We evaluate
regression performance by computing the R2score on a held out, randomly selected test set of 1,000
samples.
For each of the 5 instances of a representation learning model produced by the 5 random seeds, we
obtain 2 MLPs, resulting in a total of 10 MLP models for each representation learner. We obtain each
MLP by uniformly sampling the number of output nodes in the first, second, and fifth layers of the
MLP, as indicated in 15. All reported results are averaged over these 10 MLP models. Note that all
MLP regression models have noa priori knowledge of the representational form (i.e. scalar-tokened
symbolic compositional representation, vector-tokened symbolic compositional representation, fully
continuous compositional representation) that they will receive during training as we do not provide
any inductive biases to the MLP models or optimise them in any representation-learner specific way
(apart from through using the supervised, MSE-based training loss).
For each MLP model, we use the Adam optimiser on the MSE loss between predicted and ground-truth
FoV values, with a learning rate of 1e−4, and the default setting of (β1, β2) = (0 .9,0.999) .
Table 15: MLP architecture
MLP
Linear d1, d1∈[256,512]
ReLU
Linear d2, d2∈[256,512]
ReLU
Linear dim(z)
ReLU
Linear dim(z)
ReLU
Linear d3, d3∈[128,256]
ReLU
Linear k
For the abstract visual reasoning task, in line with [30], we use the Wild Relation Network (WReN)
model [18] on representations obtained by all representation learners to predict the ground-truth
answer for each RPM matrix. For each of the 5 instances of a model produced by the 5 random seeds,
we randomly sample 2 possible configurations of the WReN model, producing 10 WReN models for
each representation learning model. In line with [30], for the edge MLP, g, in the WReN model, we
uniformly sample either 256 or 512 hidden units. Similarly, we uniformly sample either 128 or 256
hidden units for the graph MLP, f. We however, fix the number of hidden layers in gto 2, and fin 1,
24representing the smallest possible number of hidden layers, to constrain the capacity of the WReN
model. We refer readers to [18] for further details on the WReN architecture. All WReN models
are trained using Adam optimisation on the BCE loss between the predicted logits and ground-truth
labels, with a learning rate of 1e−5and the default setting of (β1, β2) = (0 .9,0.999) .
C.2.4 Experimental Controls
Recall that our main hypothesis is that neural networks can both learn explicit compositional structure,
and leverage it more easily when that structure is instantiated in a fully continuous way, e.g., when
embodied by our Soft TPRs. To ensure that the experimental results indeed provide empiricial support
for this hypothesis, we apply a series of controls to rule out the contribution of any confounding
variables to the empirical results. We detail these controls below.
Disentanglement Controls
To ensure that any performance boosts on the disentanglement metrics (shown in Tables 16 and
17) are not attributable to the slight increase in learnable parameters of our model (note that our
model only adds one learnable component, i.e., the filler embedding matrix, MξFon top of a
standard (variational) autoencoding framework, with this corresponding to 13,568, 1,824, and 1,600
additional parameters for the Cars3D, Shapes3D, and MPI3D datasets respectively), we perform a
control to fix the number of learnable parameters in baseline models with fewer learnable parameters
compared to our model. The baseline models with fewer learnable parameters than our model are
SlowV AE, Ada-GV AE-k, GV AE, ML-V AE, and the Shu model. VCT and COMET are substantially
more parameter hungry (10+ million) than our model, so we do not perform the controls on these
models. We increase the number of parameters by increasing the number of filters in the convolution
(and transpose-convolution) layers of the generative baseline models, and/or increasing the number
of convolution (and transpose-convolution) layers until the modified model has at least as many
parameters as our model, repeating this process for each of the disentanglement datasets. We denote
these parameter-controlled models in Tables 16 and 17 with the symbol∗. In line with [40] and as
shown in Tables 16 and 17, we do not observe any increase in disentanglement performance conferred
by increasing the number of learnable parameters in these generative, scalar-tokened baselines, so we
apply representation learning convergence experiments, and downstream model experiments using
the original models, and not their parameter-controlled variants.
Downstream Task Controls
To ensure that any performance boosts on the downstream tasks of FoV regression and abstract
visual reasoning are not attributable to our representation’s increased dimensionality, we post-
process representations produced by allmodels (including models producing higher-dimensional
representations) to match the dimensionality of our representation. Note that all generative, weakly
supervised models producing scalar-tokened representations produce representations with a dimension
of 10 for all datasets, which contrasts with the representational dimensions of 1536 (Cars3D), 512
(Shapes3D), and 320 (MPI3D) of our Soft TPRs, highlighting the necessity of such a control.
To perform this control, we apply separate methods for the scalar-tokened and vector-tokened models.
For scalar-tokened models, we multiply each dimension kof the latent vector, lkwith an random
embedding vector ek∈Rdfrom an fixed, randomly initialised, semi-orthogonal embedding matrix
E∈Rd×10, and subsequently concatenate all multiplied random embedding vectors together to
form the dimensionality-controlled representation, (l1ek, . . . , l 10e10)∈R10d.dis a dataset specific
integer that ensures that the size of the dimensionality-controlled representation is at least as small
as the dimensionality of the Soft TPR representation for the given dataset, i.e., d:=⌈dim(z)⌉/10,
where zis the Soft TPR representation produced by a given dataset. Note that we choose a semi-
orthogonal embedding matrix with the intuition that this ensures the maximal distinguishability of
each continguous subset of dimensions corresponding to liei.
For COMET and VCT, which both produce vector-tokened representations, we consider alternative
methods of performing dimensionality-control. COMET’s representations have a dimensionality
of 640 for all datasets, and so, the model produces lower-dimensional representations than ours
for Shapes3D and Cars3D, but not MPI3D. VCT, on the other hand, produces representations with
dimension 5120, and so, produces higher-dimensional representations than ours for all datasets. For
any vector-tokened representation with higher dimensionality than our representation, we apply PCA-
based postprocessing to the model representation, reducing the dimensionality of each vector-tokened
25value in the representation to the required dimensionality, d, where d:=⌈dim(z)⌉/dim(zbaseline ),
before concatenating all PCA-reduced vectors together to produce the modified representation. For
any vector-tokened representation produced by COMET with lower dimensionality than ours, we
apply a simple matrix multiplication between the representation, and a randomly initialised matrix of
required dimensionality to embed the representation into the same-dimensional space as ours.
We denote the results produced by all such postprocessing by†in relevant tables, and indicate such
postprocessing in the captions of relevant plots.
C.3 Disentanglement
In reporting the disentanglement metric results for baseline models, we use published results where
applicable, i.e., we use the results published in [47] for VCT and COMET and the published results
for SlowV AE [39]. For GV AE, MLV AE, and Ada-GV AE-K, we evaluate disentanglement using the
Pytorch-based implementation of disentanglement metrics, [42], which corresponds to a Pytorch-
based implementation of the official disentanglement lib of [25]. We also use this implementation to
evaluate the disentanglement of representations produced by all models.
C.3.1 Disentanglement Metrics
In line with [47], we consider the following 4 disentanglement metrics: the FactorV AE score [24],
the DCI Disentanglement score [16] (we refer to DCI Disentanglement as DCI), the BetaV AE score
[10], and the MIG score [15]. We provide a brief overview of all 4 disentanglement metrics, and
refer interested readers to the papers these metrics are introduced in for further details, as well as the
Appendix of [25] for more details on how the metrics are implemented in [42] and [25].
FactorV AE Metric : A randomly selected FoV of the dataset is fixed, and a mini-batch of observations
is subsequently randomly sampled. The representation learner produces representations for the
samples. Disentanglement is quantified using the accuracy of a majority vote classifier that predicts
the index of the ground-truth fixed FoV based on the index of the representation vector with the
smallest variance.
DCI Metric : A Gradient Boosted Tree (GBT) is trained to predict the ground-truth FoV values from
representations produced by a representation learner. The predictive importance of the dimensions
of a representation is obtained using the model’s feature importances. For each sample, a score is
computed that corresponds to one minus the entropy of the probability that a dimension of the learned
representation is useful for FoV prediction, weighted by the relative entropy of the corresponding
dimension. An average of these scores over the mini-batch of samples is taken to produce the final
score.
BetaV AE Metric : This metric quantifies disentanglement by predicting the index of a fixed FoV from
representations produced by a representation learner, similar to the FactorV AE metric. In contrast to
the FactorV AE metric, however, the BetaV AE metric uses a linear classifier on difference vectors
to predict the index of the fixed FoV . Each difference vector is produced by taking the difference
between representations produced for a pair of samples (x, x′)with one underlying fixed FoV .
MIG Metric : For each FoV , the MIG metric computes the mutual information between each
dimension in the representation, and the corresponding FoV . The score is obtained by computing the
average, normalised difference between the highest and second highest mutual information of each
FoV with the dimensions of the representation.
C.3.2 Evaluating the Disentanglement of Soft TPRs
Since disentanglement metrics are typically computed under the assumption that the representation
corresponds to a concatenation of scalar-valued FoV tokens, we now detail how we compute
the above disentanglement metrics on the Soft TPR, a continuous compositional representation.
Similar amendments have been made in COMET and VCT, so that the disentanglement of their
representations, corresponding to a concatenation of vector-valued FoV tokens, can be computed.
FactorV AE metric : To compute the FactorV AE score of our Soft TPR, we produce a NR-dimensional
vector, v, for each Soft TPR, where NRcorresponds to the number of roles, i.e. the FoV types . We
produce vby simply populating each dimension, vi, with the index of the quantised filler that role ri
26is bound to, i.e. we set vitom(i). We use the resulting v’s to compute the variances required by the
FactorV AE metric, noting that if the FoV type corresponding to role iis fixed, and this is reflected in
our representation, dimension iof the vvectors produced in a mini-batch should clearly have the
smallest variance, as all fillers, fm(i), that role ribinds to, will have the same identity across the
mini-batch.
DCI metric : As the DCI relies on computing the ground-truth FoV values from NR-dimensional
representations produced by the representation learner, we again, follow a similar procedure as the
above, in converting our (DF·DR)-dimensional Soft TPR representation to a NR-dimensional
representation. That is, we simply consider a NR-dimensional vector, v, where each dimension, vi, is
populated by the index of the quantised filler, m(i)that role riis bound to, and use this vector to
compute the corresponding DCI result.
BetaV AE metric : For the BetaV AE metric, as each sample used to train the linear classifier consists
of aNR-dimensional difference vector obtained by computing the difference between the NR-
dimensional scalar-tokened compositional representations, we obtain the following difference vector,
d, for our Soft TPR representations. For each role i∈ {1, . . . , N R}, we obtain the corresponding
quantised filler embeddings for each sample x, x′in the pair, i.e., we obtain ξF(fm(i)), ξF(fm′(i)).
For each pair of quantised filler embeddings, we obtain a scalar distance measure corresponding to
the cosine similarity between the the pair. The i-th dimension of dis populated using this value. We
use difference vectors obtained in this way to compute the BetaV AE metric.
MIG metric : For the MIG metric, which relies on a discretisation of the values in each dimension i
of the NR-dimensional scalar-tokened compositional representations to compute the discrete mutual
information, we apply the same postprocessing technique as in the FactorV AE, and DCI metric, to
evaluate MIG on our Soft TPRs. That is, we produce the same NR-dimensional vector, v, noting that
this choice of postprocessing also performs the discretisation required by the MIG computation.
C.3.3 Full Results
We now present unabridged results for all considered disentanglement metrics, denoting the learnable
parameter control modification of each relevant baseline with the symbol∗. As can be seen in
Tables 16 and 17, our Soft TPR representations are explicitly more compositional (as quantified by
the disentanglement metric scores) compared to all considered baselines, with especially notable
performance increases on the more challenging datasets of Cars3D and MPI3D.
Table 16: FactorV AE and DCI scores
ModelsCars3D Shapes3D MPI3D
FactorV AE score DCI FactorV AE score DCI FactorV AE score DCI
Symbolic scalar-tokened compositional representations
Slow-V AE 0.902 ±0.035 0.509 ±0.027 0.950 ±0.032 0.850 ±0.047 0.455 ±0.083 0.355 ±0.027
Slow-V AE∗0.872±0.038 0.518 ±0.039 0.961 ±0.028 0.867 ±0.028 0.421 ±0.091 0.317 ±0.039
Ada-GV AE-k 0.947 ±0.064 0.664±0.167 0.973 ±0.006 0.963±0.077 0.496±0.095 0.343 ±0.040
Ada-GV AE-k∗0.931±0.051 0.641 ±0.179 0.932 ±0.012 0.923 ±0.108 0.451 ±0.129 0.319 ±0.056
GV AE 0.877 ±0.081 0.262 ±0.095 0.921 ±0.075 0.842 ±0.040 0.378 ±0.024 0.245 ±0.074
GV AE∗0.841±0.123 0.219 ±0.012 0.881 ±0.129 0.810 ±0.102 0.341 ±0.067 0.216 ±0.109
ML-V AE 0.870 ±0.052 0.216 ±0.063 0.835 ±0.111 0.739 ±0.115 0.390 ±0.026 0.251 ±0.029
ML-V AE∗0.881±0.041 0.220 ±0.051 0.823 ±0.091 0.714 ±0.091 0.401 ±0.019 0.240 ±0.043
Shu 0.573 ±0.062 0.032 ±0.014 0.265 ±0.043 0.017 ±0.006 0.287 ±0.034 0.033 ±0.008
Shu∗0.551±0.062 0.019 ±0.021 0.297 ±0.031 0.020 ±0.006 0.219 ±0.057 0.029 ±0.010
Symbolic vector-tokened compositional representations
VCT 0.966±0.029 0.382±0.080 0.957 ±0.043 0.884 ±0.013 0.689±0.035 0.475 ±0.005
COMET 0.339 ±0.008 0.024 ±0.026 0.168 ±0.005 0.002 ±0.000 0.145 ±0.024 0.005 ±0.001
Fully continuous compositional representations
Ours 0.999±0.001 0.863 ±0.027 0.984 ±0.012 0.926±0.028 0.949±0.032 0.828 ±0.015
C.4 Representation Learning Convergence
We additionally examine representation learning convergence by evaluating the representations
produced at 100, 1,000, 10,000, 100,000, and 200,000 iterations of model training, where the latter
stage of 200,000 iterations corresponds to fully trained models. To quantify representation learning
27Table 17: BetaV AE and MIG scores
ModelsCars3D Shapes3D MPI3D
BetaV AE score MIG BetaV AE score MIG BetaV AE score MIG
Symbolic scalar-tokened compositional representations
Slow-V AE 1.000±0.000 (=) 0.104±0.018 1.000±0.000 (=) 0.615±0.045 0.666±0.069 0.329±0.026
Slow-V AE∗1.000±0.000 (=) 0.071±0.013 1.000±0.000 (=) 0.655±0.067 0.629±0.079 0.287 ±0.045
Ada-GV AE-k 1.000±0.000 (=) 0.395±0.095 1.000 ±0.000 (=) 0.556±0.064 0.750 ±0.053 0.213 ±0.064
Ada-GV AE-k∗1.000±0.000 (=) 0.321±0.102 1.000±0.000 (=) 0.498±0.073 0.783 ±0.061 0.241 ±0.079
GV AE 1.000±0.000 (=) 0.096±0.036 0.998 ±0.004 0.251 ±0.072 0.704 ±0.072 0.145 ±0.074
GV AE∗1.000±0.000 (=) 0.100±0.052 1.000±0.000 0.203±0.089 0.681 ±0.061 0.109 ±0.087
MLV AE 1.000±0.000 (=) 0.088±0.020 0.976 ±0.038 0.354 ±0.165 0.703 ±0.039 0.142 ±0.062
MLV AE∗1.000±0.000 (=) 0.050±0.058 0.921 ±0.052 0.298 ±0.091 0.689 ±0.041 0.096 ±0.071
Shu 0.912 ±0.022 0.025 ±0.012 0.498 ±0.064 0.005 ±0.003 0.353 ±0.040 0.013 ±0.007
Shu∗0.923±0.031 0.015 ±0.009 0.512 ±0.071 0.006 ±0.002 0.327 ±0.059 0.009 ±0.011
Symbolic vector-tokened compositional representations
VCT 1.000±0.000 (=) 0.117±0.045 0.999 ±0.0004 0.525 ±0.028 0.844±0.038 0.227±0.048
COMET 0.343 ±0.006 0.000 ±0.000 0.166 ±0.004 0.0002 ±0.000 0.144 ±0.005 0.000 ±0.0001
Fully continuous compositional representations
Ours 1.000±0.000 (=) 0.348±0.0124 1.000±0.000 (=) 0.471±0.088 1.000±0.000 0.620 ±0.067
convergence, we evaluate both 1) the explicit compositionality of representations produced at these
stages of training (as quantified by disentanglement metric performance), and 2) the usefulness of
these representations for the downstream tasks of FoV regression and abstract visual reasoning.
As mentioned in Section 5.1, the representation learning convergence of our model as measured
by disentanglement performance is comparable with baselines, however, our model consistently
converges faster than baselines in producing representations that can be effectively leveraged for
both downstream tasks, as indicated by higher downstream model performance across the majority
of representation learner training stages. We now present the full suite of unabridged results, first
presenting disentanglement results, and subsequently downstream results.
In all line plots, we plot the mean, and indicate the standard deviation by the shaded regions. We use
the same legend for all plots, where 0(grey) denotes SlowV AE, 1(orange) denotes AdaGV AE-k, 2
(green) denotes GV AE, 3(red) denotes MLV AE, 4(purple) denotes Shu, 5(pink) denotes VCT, 6
(brown) denotes COMET, and 7(blue) denotes our model, Soft TPR Autoencoder.
C.4.1 Disentanglement
We first present line plots of representation learner convergence for each of the four considered
disentanglement metrics (i.e., the FactorV AE, DCI, BetaV AE and MIG scores) for all three disentan-
glement datasets (Cars3D, Shapes3D, MPI3D). A series of tables containing the values associated
with these line plots is presented following the plots. As the disentanglement results produced
by our learnable parameter controls for the models of Ada-GV AE-k, GV AE, ML-V AE and Shu,
do not achieve superior disentanglement results compared to the original models, we only present
disentanglement convergence results for the original variants of all baseline models.
28102103104105
Iteration count0.00.20.40.60.81.0Factor score
Cars3D 0
1
2
3
4
5
6
7Figure 3: Factor score convergence on the Cars3D dataset
102103104105
Iteration count0.00.20.40.60.8DCI score
Cars3D 0
1
2
3
4
5
6
7
Figure 4: DCI score convergence on the Cars3D dataset
29102103104105
Iteration count0.40.50.60.70.80.91.0Beta score
Cars3D 0
1
2
3
4
5
6
7Figure 5: BetaV AE score convergence on the Cars3D dataset
102103104105
Iteration count0.000.050.100.150.200.250.300.350.40MIG score
Cars3D 0
1
2
3
4
5
6
7
Figure 6: MIG score convergence on the Cars3D dataset
30102103104105
Iteration count0.00.20.40.60.81.0Factor score
Shapes3D 0
1
2
3
4
5
6
7Figure 7: Factor score convergence on the Shapes3D dataset
102103104105
Iteration count0.00.20.40.60.81.0DCI score
Shapes3D 0
1
2
3
4
5
6
7
Figure 8: DCI score convergence on the Shapes3D dataset
31102103104105
Iteration count0.20.40.60.81.0Beta score
Shapes3D 0
1
2
3
4
5
6
7Figure 9: BetaV AE score convergence on the Shapes3D dataset
102103104105
Iteration count0.00.10.20.30.40.50.60.7MIG score
Shapes3D 0
1
2
3
4
5
6
7
Figure 10: MIG score convergence on the Shapes3D dataset
32102103104105
Iteration count0.00.20.40.60.81.0Factor score
MPI3D 0
1
2
3
4
5
6
7Figure 11: Factor score convergence on the MPI3D dataset
102103104105
Iteration count0.00.20.40.60.8DCI score
MPI3D 0
1
2
3
4
5
6
7
Figure 12: DCI score convergence on the MPI3D dataset
33102103104105
Iteration count0.20.40.60.81.0Beta score
MPI3D 0
1
2
3
4
5
6
7Figure 13: BetaV AE score convergence on the MPI3D dataset
102103104105
Iteration count0.00.10.20.30.40.50.60.7MIG score
MPI3D 0
1
2
3
4
5
6
7
Figure 14: MIG score convergence on the MPI3D dataset
34Table 18: Representation learner convergence on the Cars3D dataset (Factor score)
ModelsFactor score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.455 ±0.020 0.546 ±0.043 0.624 ±0.050 0.713 ±0.054 0.743 ±0.046
Ada-GV AE-k 0.460 ±0.024 0.642 ±0.031 0.755 ±0.042 0.873 ±0.077 0.904 ±0.082
GV AE 0.478 ±0.026 0.560 ±0.036 0.641 ±0.052 0.701 ±0.044 0.722 ±0.035
MLV AE 0.481 ±0.022 0.575 ±0.059 0.652 ±0.041 0.721 ±0.035 0.719 ±0.031
Shu 0.000 ±0.000 0.294 ±0.162 0.229 ±0.194 0.000 ±0.000 0.069 ±0.138
Symbolic vector-tokened compositional representations
VCT 0.618±0.046 0.887±0.074 0.907±0.036 0.918 ±0.037 0.915 ±0.028
COMET 0.531 ±0.024 0.531 ±0.024 0.531 ±0.024 0.531 ±0.024 0.531 ±0.024
Fully continuous compositional representations
Ours 0.639±0.045 0.898 ±0.038 0.999 ±0.001 0.999 ±0.001 0.999 ±0.001
Table 19: Representation learner convergence on the Cars3D dataset (DCI score)
ModelsDCI score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.047 ±0.012 0.115 ±0.021 0.154 ±0.031 0.261 ±0.041 0.308 ±0.073
Ada-GV AE-k 0.054 ±0.011 0.165 ±0.012 0.338±0.042 0.558 ±0.167 0.694 ±0.140
GV AE 0.071 ±0.024 0.098 ±0.021 0.147 ±0.025 0.216 ±0.053 0.238 ±0.061
MLV AE 0.062±0.022 0.100±0.018 0.175 ±0.050 0.197 ±0.062 0.238 ±0.077
Shu 0.007 ±0.005 0.013 ±0.011 0.018 ±0.012 0.038 ±0.036 0.026 ±0.015
Symbolic vector-tokened compositional representations
VCT 0.055 ±0.044 0.240±0.043 0.227±0.024 0.226 ±0.020 0.230 ±0.044
COMET 0.020 ±0.013 0.015 ±0.011 0.011 ±0.006 0.008 ±0.003 0.009 ±0.003
Fully continuous compositional representations
Ours 0.379±0.117 0.381 ±0.026 0.812 ±0.033 0.843 ±0.024 0.832 ±0.048
Table 20: Representation learner convergence on the Cars3D dataset (BetaV AE score)
ModelsBetaV AE score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.937 ±0.025 0.999 ±0.002 (=) 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=)
Ada-GV AE-k 0.971±0.011 1.000 ±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=)
GV AE 0.958±0.027 0.999 ±0.001 (=) 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=)
MLV AE 0.964 ±0.011 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=)
Shu 0.534 ±0.164 0.742 ±0.072 0.728±0.105 0.433±0.016 0.449 ±0.099
Symbolic vector-tokened compositional representations
VCT 0.889 ±0.044 1.000±0.001 (=) 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=)
COMET 0.592 ±0.027 0.592 ±0.028 0.592 ±0.028 0.592±0.028 0.592 ±0.028
Fully continuous compositional representations
Ours 0.953 ±0.042 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000
35Table 21: Representation learner convergence on the Cars3D dataset (MIG score)
ModelsMIG score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.008 ±0.004 0.042 ±0.017 0.061 ±0.024 0.071 ±0.028 0.081 ±0.031
Ada-GV AE-k 0.011±0.005 0.068 ±0.013 0.092 ±0.035 0.241 ±0.095 0.312 ±0.085
GV AE 0.014 ±0.006 0.031 ±0.015 0.056 ±0.008 0.084 ±0.031 0.095 ±0.032
MLV AE 0.012 ±0.007 0.037 ±0.024 0.057 ±0.018 0.085 ±0.021 0.087 ±0.017
Shu 0.007 ±0.004 0.014 ±0.005 0.019 ±0.010 0.002 ±0.001 0.001 ±0.001
Symbolic vector-tokened compositional representations
VCT 0.001 ±0.001 0.034 ±0.016 0.033 ±0.021 0.033 ±0.022 0.032 ±0.015
COMET 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000
Fully continuous compositional representations
Ours 0.016±0.010 0.136 ±0.009 0.344 ±0.011 0.349 ±0.010 0.349 ±0.010
Table 22: Representation learner convergence on the Shapes3D dataset (Factor score)
ModelsFactor score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.270 ±0.025 0.392 ±0.095 0.547 ±0.138 0.762 ±0.156 0.923 ±0.091
Ada-GV AE-k 0.296±0.047 0.577 ±0.023 0.815 ±0.057 0.960±0.057 0.961±0.070
GV AE 0.263 ±0.036 0.469 ±0.063 0.674 ±0.086 0.890 ±0.058 0.879 ±0.077
MLV AE 0.166 ±0.084 0.457 ±0.045 0.621 ±0.078 0.746 ±0.080 0.765 ±0.084
Shu 0.000 ±0.000 0.000 ±0.000 0.242 ±0.072 0.194 ±0.021 0.227 ±0.016
Symbolic vector-tokened compositional representations
VCT 0.263 ±0.000 0.984±0.031 1.000 ±0.000 0.976±0.033 0.967±0.044
COMET 0.268 ±0.022 0.265 ±0.039 0.334 ±0.028 0.384 ±0.040 0.440 ±0.071
Fully continuous compositional representations
Ours 0.356±0.011 0.435±0.048 0.975±0.040 0.995±0.005 0.960±0.053
Table 23: Representation learner convergence on the Shapes3D dataset (DCI score)
ModelsDCI score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.056±0.015 0.192±0.060 0.353 ±0.111 0.616 ±0.147 0.749 ±0.118
Ada-GV AE-k 0.053 ±0.018 0.331±0.022 0.662±0.042 0.944±0.063 0.964 ±0.067
GV AE 0.042 ±0.013 0.198 ±0.046 0.438 ±0.064 0.749 ±0.053 0.835 ±0.038
MLV AE 0.026 ±0.003 0.183 ±0.036 0.407 ±0.051 0.668 ±0.084 0.739 ±0.103
Shu 0.030 ±0.007 0.028 ±0.012 0.032 ±0.006 0.019 ±0.010 0.018 ±0.006
Symbolic vector-tokened compositional representations
VCT 0.044 ±0.000 0.817±0.114 0.909 ±0.010 0.887±0.026 0.880±0.022
COMET 0.017 ±0.006 0.031 ±0.005 0.062 ±0.017 0.173 ±0.064 0.233 ±0.066
Fully continuous compositional representations
Ours 0.057±0.018 0.183±0.077 0.860±0.052 0.887±0.068 0.924±0.027
36Table 24: Representation learner convergence on the Shapes3D dataset (BetaV AE score)
ModelsBetaV AE score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.539 ±0.061 0.694 ±0.012 0.949 ±0.037 0.989±0.020 1.000±0.000 (=)
Ada-GV AE-k 0.553 ±0.050 0.699 ±0.016 0.999±0.001 1.000±0.000 (=) 1.000±0.000 (=)
GV AE 0.505 ±0.031 0.683 ±0.019 0.974 ±0.025 1.000±0.000 (=) 0.998±0.004
MLV AE 0.484 ±0.015 0.684 ±0.013 0.970 ±0.029 0.949 ±0.041 0.974 ±0.036
Shu 0.311 ±0.204 0.357 ±0.050 0.495 ±0.040 0.379 ±0.056 0.403 ±0.058
Symbolic vector-tokened compositional representations
VCT 0.889±0.044 1.000 ±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=)
COMET 0.395 ±0.000 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=)
Fully continuous compositional representations
Ours 0.638±0.015 0.710 ±0.035 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=)
Table 25: Representation learner convergence on the Shapes3D dataset (MIG score)
ModelsMIG score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.018 ±0.009 0.080 ±0.053 0.163 ±0.107 0.297 ±0.156 0.386 ±0.169
Ada-GV AE-k 0.023±0.012 0.189 ±0.071 0.288±0.138 0.520±0.149 0.555 ±0.152
GV AE 0.019 ±0.012 0.074 ±0.053 0.153 ±0.039 0.225 ±0.052 0.243 ±0.064
MLV AE 0.010 ±0.004 0.082 ±0.042 0.179 ±0.076 0.305 ±0.126 0.354 ±0.148
Shu 0.025±0.007 0.008±0.005 0.013 ±0.005 0.010 ±0.003 0.008 ±0.004
Symbolic vector-tokened compositional representations
VCT 0.000 ±0.000 0.405±0.103 0.466 ±0.023 0.469±0.025 0.448 ±0.065
COMET 0.000 ±0.000 0.004 ±0.003 0.013 ±0.010 0.037 ±0.022 0.044 ±0.025
Fully continuous compositional representations
Ours 0.015 ±0.005 0.043 ±0.018 0.343±0.104 0.412±0.066 0.402 ±0.091
Table 26: Representation learner convergence on the MPI3D dataset (Factor score)
ModelsFactor score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.240 ±0.008 0.300 ±0.015 0.325 ±0.014 0.458 ±0.095 0.471 ±0.109
Ada-GV AE-k 0.246±0.012 0.275±0.011 0.321 ±0.011 0.314 ±0.050 0.318 ±0.050
GV AE 0.236 ±0.020 0.253 ±0.017 0.277 ±0.023 0.237 ±0.020 0.232 ±0.023
MLV AE 0.235 ±0.008 0.257 ±0.014 0.261 ±0.018 0.229 ±0.021 0.240 ±0.019
Shu 0.000 ±0.000 0.061 ±0.075 0.173 ±0.089 0.093 ±0.077 0.124 ±0.056
Symbolic vector-tokened compositional representations
VCT 0.220 ±0.024 0.360±0.017 0.441±0.037 0.678 ±0.125 0.678 ±0.045
COMET 0.227 ±0.019 0.233 ±0.016 0.233 ±0.016 0.243 ±0.005 0.233 ±0.015
Fully continuous compositional representations
Ours 0.334±0.079 0.350±0.049 0.750±0.065 0.937 ±0.034 0.951 ±0.083
37Table 27: Representation learner convergence on the MPI3D dataset (DCI score)
ModelsDCI score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.090 ±0.013 0.125 ±0.014 0.200 ±0.014 0.341 ±0.095 0.381 ±0.081
Ada-GV AE-k 0.097 ±0.008 0.117 ±0.009 0.174 ±0.005 0.319 ±0.038 0.338 ±0.038
GV AE 0.101 ±0.019 0.111 ±0.011 0.142 ±0.022 0.247 ±0.062 0.258 ±0.063
MLV AE 0.097 ±0.015 0.103 ±0.010 0.137 ±0.013 0.252 ±0.031 0.266 ±0.026
Shu 0.083 ±0.000 0.065 ±0.030 0.063 ±0.018 0.036 ±0.012 0.050 ±0.012
Symbolic vector-tokened compositional representations
VCT 0.123±0.021 0.220±0.029 0.535±0.048 0.580 ±0.046 0.611 ±0.035
COMET 0.039 ±0.016 0.036 ±0.011 0.033 ±0.008 0.029 ±0.003 0.032 ±0.007
Fully continuous compositional representations
Ours 0.172±0.051 0.174±0.037 0.670±0.060 0.813 ±0.010 0.799 ±0.078
Table 28: Representation learner convergence on the MPI3D dataset (BetaV AE score)
ModelsBetaV AE score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.359 ±0.004 0.484 ±0.019 0.629 ±0.038 0.833 ±0.035 0.851 ±0.045
Ada-GV AE-k 0.362±0.006 0.397±0.028 0.587 ±0.040 0.741 ±0.047 0.748 ±0.041
GV AE 0.352 ±0.012 0.378 ±0.015 0.506 ±0.041 0.683 ±0.071 0.703 ±0.062
MLV AE 0.343 ±0.021 0.394 ±0.022 0.507 ±0.015 0.672 ±0.038 0.697 ±0.035
Shu 0.170 ±0.057 0.294 ±0.026 0.325 ±0.038 0.269 ±0.023 0.278 ±0.021
Symbolic vector-tokened compositional representations
VCT 0.889±0.044 1.000 ±0.001 1.000 ±0.000 1.000 ±0.000 1.000 ±0.000
COMET 0.277 ±0.011 0.277 ±0.011 0.277 ±0.011 0.270 ±0.007 0.278 ±0.010
Fully continuous compositional representations
Ours 0.302 ±0.011 0.577±0.021 0.973 ±0.048 0.980 ±0.022 0.976 ±0.024
Table 29: Representation learner convergence on the MPI3D dataset (MIG score)
ModelsMIG score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.014 ±0.004 0.059±0.042 0.084±0.061 0.206 ±0.120 0.214 ±0.125
Ada-GV AE-k 0.016±0.005 0.039±0.011 0.113 ±0.051 0.213 ±0.062 0.214 ±0.057
GV AE 0.014 ±0.004 0.022 ±0.003 0.058 ±0.035 0.140 ±0.061 0.147 ±0.066
MLV AE 0.010 ±0.004 0.032 ±0.022 0.044 ±0.015 0.136 ±0.051 0.144 ±0.054
Shu 0.005 ±0.000 0.010 ±0.006 0.024 ±0.019 0.009 ±0.005 0.013 ±0.005
Symbolic vector-tokened compositional representations
VCT 0.000 ±0.000 0.013 ±0.003 0.216±0.055 0.240 ±0.050 0.248 ±0.057
COMET 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000
Fully continuous compositional representations
Ours 0.037±0.020 0.046±0.018 0.393±0.063 0.612 ±0.068 0.514 ±0.202
C.4.2 Downstream Performance
We additionally evaluate the representation learning convergence by examining the usefulness of
representations produced at different stages of training (i.e., 100, 250, 500, 1,000, 10,000, 100,000
38and 200,000 iterations of training). To quantify ‘usefulness’, we consider performance of downstream
models on the tasks of FoV regression, and abstract visual reasoning when trained using represen-
tations produced by each stage of training. For each task, we present line plots, and additionally
tables with values corresponding to each of the plots. We use the same legend as in the previous
section, where 0(grey) denotes SlowV AE, 1(orange) denotes AdaGV AE-k, 2(green) denotes GV AE,
3(red) denotes MLV AE, 4(purple) denotes Shu, 5(pink) denotes VCT, 6(brown) denotes COMET,
and7(blue) denotes our model, Soft TPR Autoencoder. We additionally provide all results for
the dimensionality-control setting, where the dimensionality of representations produced by all
representation learners is held constant following the approach detailed in C.2.4, denoting this clearly
in plot captions, and by the symbol†in the tables.
FoV Regression
As clearly visible in the tables and plots, generic regression models are able to more effectively use
representations produced by our Soft TPR Autoencoder produced across almost all stages of training
for all disentanglement datasets. Improvements are most notable in the low-iteration regime of 102
iterations, and across most stages of training for the more challenging task of FoV regression on the
MPI3D dataset (Figures 19 and 20).
39102103104105
Iteration count0.00.20.40.60.81.0RSq
Cars3D 0
1
2
3
4
5
6
7Figure 15: Convergence of representation learners as measured by FoV regression on the Cars3D dataset (original
setting)
102103104105
Iteration count1.00
0.75
0.50
0.25
0.000.250.500.751.00RSq
Cars3D 0
1
2
3
4
5
6
7
Figure 16: Convergence of representation learners as measured by FoV regression on the Cars3D dataset
(dimensionality-controlled setting)
40102103104105
Iteration count0.20.30.40.50.60.70.80.91.0RSq
Shapes3D 0
1
2
3
4
5
6
7Figure 17: Convergence of representation learners as measured by FoV regression on the Shapes3D dataset
(original setting)
102103104105
Iteration count0.20.30.40.50.60.70.80.91.0RSq
Shapes3D 0
1
2
3
4
5
6
7
Figure 18: Convergence of representation learners as measured by FoV regression on the Shapes3D dataset
(dimensionality-controlled setting)
41102103104105
Iteration count0.20.30.40.50.60.70.80.91.0RSq
MPI3D 0
1
2
3
4
5
6
7Figure 19: Convergence of representation learners as measured by FoV regression on the MPI3D dataset (original
setting)
102103104105
Iteration count0.20.30.40.50.60.70.80.91.0RSq
MPI3D 0
1
2
3
4
5
6
7
Figure 20: Convergence of representation learners as measured by FoV regression on the MPI3D dataset
(dimensionality-controlled setting)
42Table 30: Convergence of representation learners as measured by FoV regression on the Cars3D dataset
ModelsR2score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.233 ±0.048 0.816 ±0.017 0.892 ±0.007 0.925 ±0.005 0.935 ±0.004
Slow-V AE†0.203±0.074 0.797 ±0.009 0.891 ±0.006 0.917 ±0.006 0.937 ±0.001
Ada-GV AE-k 0.307 ±0.084 0.821 ±0.016 0.896 ±0.009 0.944 ±0.007 0.957±0.005
Ada-GV AE-k†0.264±0.088 0.805 ±0.015 0.888 ±0.009 0.940 ±0.007 0.951±0.008
GV AE 0.319±0.073 0.831±0.012 0.901±0.009 0.942 ±0.004 0.947±0.005
GV AE†0.282±0.056 0.821 ±0.008 0.895 ±0.009 0.936 ±0.007 0.943 ±0.004
MLV AE 0.317 ±0.058 0.820 ±0.007 0.904 ±0.007 0.945±0.004 0.948±0.007
MLV AE†0.26±0.061 0.808 ±0.005 0.900 ±0.009 0.939 ±0.004 0.948 ±0.007
Shu -0.016 ±0.016 0.003 ±0.019 0.343 ±0.037 0.076 ±0.058 0.075 ±0.051
Shu†0.012±0.007 -0.009 ±0.02 0.343 ±0.042 0.074 ±0.070 0.094 ±0.055
Symbolic vector-tokened compositional representations
VCT 0.080 ±0.001 0.829±0.015 0.877±0.007 0.832 ±0.010 0.813 ±0.014
VCT†0.038±0.014 0.243 ±0.03 0.655 ±0.011 0.631 ±0.029 0.623 ±0.036
COMET -0.029 ±0.011 -0.035 ±0.023 -0.035 ±0.023 -0.035 ±0.023 -0.035 ±0.023
COMET†-0.484 ±0.477 -0.495 ±0.512 -0.495 ±0.512 -0.515 ±0.47 -0.715 ±0.295
Fully continuous compositional representations
Ours 0.531±0.054 0.855 ±0.012 0.920 ±0.009 0.914±0.008 0.910 ±0.010
Table 31: Convergence of representation learners as measured by FoV regression on the Shapes3D dataset
ModelsR2score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.597 ±0.048 0.927 ±0.007 0.960 ±0.032 0.999±0.000 (=) 1.000±0.000 (=)
Slow-V AE†0.564±0.060 0.920 ±0.011 0.955 ±0.037 0.999±0.000 (=) 0.999±0.000
Ada-GV AE-k 0.684 ±0.068 0.974 ±0.007 0.997 ±0.000 1.000±0.000 (=) 1.000±0.000 (=)
Ada-GV AE-k†0.698±0.063 0.973 ±0.007 0.997 ±0.000 1.000±0.000 (=) 1.000±0.000 (=)
GV AE 0.565 ±0.034 0.951 ±0.015 0.997 ±0.000 1.000±0.000 (=) 1.000±0.000 (=)
GV AE†0.565±0.022 0.951 ±0.014 0.998±0.001 1.000±0.000 (=) 1.000±0.000 (=)
MLV AE 0.551 ±0.059 0.959 ±0.012 0.997 ±0.000 0.999±0.000 (=) 1.000±0.000 (=)
MLV AE†0.556±0.055 0.960 ±0.010 0.996 ±0.000 0.999±0.000 (=) 1.000±0.000 (=)
Shu 0.327 ±0.13 0.497 ±0.106 0.631 ±0.087 0.471 ±0.027 0.427 ±0.082
Shu†0.461±0.075 0.525 ±0.107 0.632 ±0.088 0.471 ±0.008 0.419 ±0.082
Symbolic vector-tokened compositional representations
VCT 0.886±0.033 0.997±0.000 0.995±0.001 0.97 ±0.013 0.954 ±0.008
VCT†0.255±0.049 0.819 ±0.077 0.896 ±0.021 0.888 ±0.028 0.843 ±0.04
COMET 0.395 ±0.063 0.772 ±0.016 0.968 ±0.011 1.000±0.000 (=) 1.000±0.000 (=)
COMET†0.474±0.062 0.809 ±0.013 0.970 ±0.011 1.000±0.000 (=) 1.000±0.000 (=)
Fully continuous compositional representations
Ours 0.981±0.003 0.989±0.007 1.000±0.000 (=) 1.000±0.000 (=) 1.000±0.000 (=)
43Table 32: Convergence of representation learners as measured by FoV regression on the MPI3D dataset
ModelsR2score
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.557±0.012 0.634 ±0.015 0.818 ±0.016 0.960 ±0.013 0.980 ±0.005
Slow-V AE†0.512±0.009 0.612 ±0.023 0.809 ±0.025 0.964±0.008 0.981 ±0.004
Ada-GV AE-k 0.519 ±0.023 0.611 ±0.028 0.694 ±0.007 0.750 ±0.008 0.762 ±0.014
Ada-GV AE-k†0.524±0.015 0.595 ±0.015 0.704 ±0.025 0.753 ±0.012 0.774 ±0.003
GV AE 0.511 ±0.037 0.602 ±0.021 0.724 ±0.012 0.748 ±0.013 0.771 ±0.018
GV AE†0.508±0.028 0.614 ±0.020 0.717 ±0.013 0.773 ±0.024 0.764 ±0.019
MLV AE 0.504 ±0.016 0.605 ±0.013 0.717 ±0.020 0.780 ±0.016 0.773 ±0.031
MLV AE†0.497±0.019 0.602 ±0.008 0.714 ±0.012 0.766 ±0.010 0.790 ±0.027
Shu 0.270 ±0.041 0.498 ±0.095 0.592 ±0.082 0.470 ±0.036 0.441 ±0.041
Shu†0.299±0.040 0.519 ±0.062 0.568 ±0.079 0.474 ±0.036 0.438 ±0.037
Symbolic vector-tokened compositional representations
VCT 0.316 ±0.016 0.316 ±0.011 0.516 ±0.02 0.543 ±0.000 0.456 ±0.061
VCT†0.251±0.04 0.396 ±0.041 0.546 ±0.027 0.508 ±0.016 0.561 ±0.051
COMET 0.266 ±0.004 0.266 ±0.004 0.266 ±0.004 0.266 ±0.004 0.266 ±0.004
COMET†0.240±0.010 0.236 ±0.008 0.236 ±0.008 0.236 ±0.008 0.236 ±0.008
Fully continuous compositional representations
Ours 0.732±0.073 0.741 ±0.058 0.914 ±0.012 0.891±0.011 0.882 ±0.016
Abstract Visual Reasoning
We present now present our full suite of results for the downstream task of abstract visual reasoning.
As demonstrated in Table 33 and the corresponding Figures 21 and 22, representations produced
by our model at only102iterations of training are able to be leveraged by downstream models to
achieve a 80.04 %accuracy for the challenging abstract visual reasoning task, in contrast to the value
of 63.1 %obtained by the best performing baseline, representing a 26.78 %performance increase.
This again provides strong empirical support for our hypothesis that the approximate , continuously-
instantiated compositional structure embodied by our Soft TPR can be learnt more quickly by
representation learners than alternative, symbolic representations of compositional structure, thereby
allowing downstream models to effectively leverage these representations despite a small number of
representation learner training iterations.
44102103104105
n epochs0.20.40.60.81.0Accuracy
Abstract visual reasoning dataset 0
1
2
3
4
5
6
7Figure 21: Convergence of representation learners as measured by classification performance on the abstract
visual reasoning dataset (original setting)
102103104105
n epochs0.20.40.60.81.0Accuracy
Abstract visual reasoning dataset 0
1
2
3
4
5
6
7
Figure 22: Convergence of representation learners as measured by classification performance on the abstract
visual reasoning dataset (dimensionality-controlled setting)
45Table 33: Convergence of representation learners as measured by classification performance on the abstract
visual reasoning dataset
ModelsClassification accuracy
102iterations 103iterations 104iterations 105iterations 2×105iterations
Symbolic scalar-tokened compositional representations
Slow-V AE 0.332 ±0.022 0.450 ±0.023 0.460 ±0.031 0.429 ±0.038 0.396 ±0.022
Slow-V AE†0.552±0.035 0.791 ±0.011 0.904 ±0.020 0.949±0.012 0.959±0.009
Ada-GV AE-k 0.397 ±0.021 0.375 ±0.038 0.409 ±0.029 0.455 ±0.047 0.472 ±0.037
Ada-GV AE-k†0.631±0.037 0.845±0.010 0.892 ±0.015 0.943±0.017 0.954±0.009
GV AE 0.346 ±0.029 0.424 ±0.034 0.431 ±0.020 0.444 ±0.057 0.478 ±0.032
GV AE†0.554±0.031 0.815 ±0.008 0.894 ±0.011 0.894 ±0.055 0.936 ±0.008
MLV AE 0.336 ±0.016 0.404 ±0.013 0.431 ±0.038 0.466 ±0.052 0.432 ±0.023
MLV AE†0.550±0.025 0.824 ±0.021 0.878 ±0.014 0.919 ±0.034 0.925 ±0.024
Shu 0.208 ±0.052 0.273 ±0.012 0.331 ±0.007 0.317 ±0.025 0.310 ±0.038
Shu†0.207±0.048 0.399 ±0.072 0.462 ±0.024 0.433 ±0.023 0.444 ±0.013
Symbolic vector-tokened compositional representations
VCT 0.440 ±0.033 0.932±0.062 0.769±0.087 0.695 ±0.049 0.641 ±0.039
VCT†0.432±0.043 0.723 ±0.065 0.731 ±0.017 0.479 ±0.023 0.458 ±0.040
COMET 0.369 ±0.077 0.683 ±0.037 0.848 ±0.049 0.501 ±0.251 0.984±0.006
COMET†0.348±0.069 0.545 ±0.197 0.829 ±0.051 0.856 ±0.256 0.532 ±0.348
Fully continuous compositional representations
Ours 0.804±0.016 0.864±0.011 0.952±0.008 0.884±0.012 0.869 ±0.024
C.5 Downstream Performance
We present our full suite of experimental results that empirically demonstrate the utility of our Soft
TPR representation from the perspective of downstream models, by considering the sample efficiency,
and low-sample regime performance of downstream models on the tasks of FoV regression, and
abstract visual reasoning.
C.5.1 Sample Efficiency Results
For sample efficiency, as mentioned in Section 5.2, and in line with [25], we compute a ratio-based
metric obtained by dividing the performance of the downstream model when trained using a restricted
number of 100, 250, 500, 1,000 and 10,000 samples, by its performance when trained using all
samples. The total number of samples corresponds to 19,104, 480,000, 1,036,800 and 100,000 for the
tasks of regression on the Cars3D, Shapes3D, MPI3D datasets, and the abstract visual reasoning task
respectively. As this metric is dependent on the performance of downstream models when trained
using all samples, we do not compute this metric for representation learners where the corresponding
downstream models achieve an R2score of less than 0.5for regression, as this may produce sample
efficiency scores with very little semantic meaning (e.g. a model that achieves a sample efficiency
score of 0.9 when its final R2score is 0.1). This corresponds to removing the Shu model from
Shapes3D sample efficiency calculations, and COMET and Shu from the Cars3D sample efficiency
calculations.
As many models for the abstract visual reasoning task have low classification accuracies on the
held-out test set following training with the maximal number of 100,000 samples, we do not compute
sample efficiencies for this task, and instead refer readers to results in Section C.5.2 for the raw
classification accuracies associated with each model.
Note that for all box plots, we follow standard convention, and display the median in each box with a
solid line, where the box shows the quartiles of the corresponding values, and the whiskers extend
to1.5times the interquartile range. We again, use the same legend, where grey denotes SlowV AE,
orange denotes AdaGV AE-k, green denotes GV AE, red denotes MLV AE, purple denotes Shu, pink
denotes VCT, brown denotes COMET, and blue denotes our model, Soft TPR Autoencoder.
460 1 2 3 4 50.20.40.60.81.0Rsq ratioSample efficiency (100/all)
0 1 2 3 4 50.00.20.40.60.81.0Rsq ratioSample efficiency (250/all)0
1
2
3
4
5Figure 23: Downstream regression model sample efficiency on the Cars3D dataset (original setting).
0 1 2 3 4 50.00.20.40.60.81.0Rsq ratioSample efficiency (500/all)
0 1 2 3 4 50.00.20.40.60.81.0Rsq ratioSample efficiency (1,000/all)
0 1 2 3 4 50.00.20.40.60.81.0Rsq ratioSample efficiency (10,000/all)0
1
2
3
4
5
Figure 24: Downstream regression model sample efficiency on the Cars3D dataset (original setting).
470 1 2 3 4 5 6 70.00.20.40.60.81.0Rsq ratioSample efficiency (100/all)
0 1 2 3 4 5 6 70.20.40.60.81.0Rsq ratioSample efficiency (250/all)0
1
2
3
4
5
6
7Figure 25: Downstream regression model sample efficiency on the Cars3D dataset (dimensionality-controlled
setting).
0 1 2 3 4 5 6 70.50.60.70.80.91.0Rsq ratioSample efficiency (500/all)
0 1 2 3 4 5 6 70.20.30.40.50.60.70.80.91.0Rsq ratioSample efficiency (1,000/all)
0 1 2 3 4 5 6 70.20.40.60.81.0Rsq ratioSample efficiency (10,000/all)0
1
2
3
4
5
6
7
Figure 26: Downstream regression model sample efficiency on the Cars3D dataset (dimensionality-controlled
setting).
480 1 2 3 4 5 60.00.10.20.30.40.50.60.70.8Rsq ratioSample efficiency (100/all)
0 1 2 3 4 5 60.00.20.40.60.8Rsq ratioSample efficiency (250/all)0
1
2
3
4
5
6Figure 27: Downstream regression model sample efficiency on the Shapes3D dataset (original setting).
0 1 2 3 4 5 60.20.40.60.81.0Rsq ratioSample efficiency (500/all)
0 1 2 3 4 5 60.20.40.60.81.0Rsq ratioSample efficiency (1,000/all)
0 1 2 3 4 5 60.30.40.50.60.70.80.91.0Rsq ratioSample efficiency (10,000/all)0
1
2
3
4
5
6
Figure 28: Downstream regression model sample efficiency on the Shapes3D dataset (original setting).
490 1 2 3 4 5 6 70.00.10.20.30.40.50.60.70.8Rsq ratioSample efficiency (100/all)
0 1 2 3 4 5 6 70.00.20.40.60.8Rsq ratioSample efficiency (250/all)0
1
2
3
4
5
6
7Figure 29: Downstream regression model sample efficiency on the Shapes3D dataset (dimensionality-controlled
setting).
0 1 2 3 4 5 6 70.00.20.40.60.81.0Rsq ratioSample efficiency (500/all)
0 1 2 3 4 5 6 70.00.20.40.60.81.0Rsq ratioSample efficiency (1,000/all)
0 1 2 3 4 5 6 70.20.30.40.50.60.70.80.91.0Rsq ratioSample efficiency (10,000/all)0
1
2
3
4
5
6
7
Figure 30: Downstream regression model sample efficiency on the Shapes3D dataset (dimensionality-controlled
setting).
500 1 2 3 4 5 6 70.00.10.20.30.40.50.60.7Rsq ratioSample efficiency (100/all)
0 1 2 3 4 5 6 70.00.10.20.30.40.50.60.7Rsq ratioSample efficiency (250/all)0
1
2
3
4
5
6
7Figure 31: Downstream regression model sample efficiency on the MPI3D dataset (original setting).
0 1 2 3 4 5 6 70.00.20.40.60.8Rsq ratioSample efficiency (500/all)
0 1 2 3 4 5 6 70.00.20.40.60.8Rsq ratioSample efficiency (1,000/all)
0 1 2 3 4 5 6 70.00.20.40.60.81.0Rsq ratioSample efficiency (10,000/all)0
1
2
3
4
5
6
7
Figure 32: Downstream regression model sample efficiency on the MPI3D dataset (original setting).
510 1 2 3 4 5 6 70.00.10.20.30.40.50.60.7Rsq ratioSample efficiency (100/all)
0 1 2 3 4 5 6 70.00.10.20.30.40.50.60.7Rsq ratioSample efficiency (250/all)0
1
2
3
4
5
6
7Figure 33: Downstream regression model sample efficiency on the MPI3D dataset (dimensionality-controlled
setting).
0 1 2 3 4 5 6 70.00.20.40.60.8Rsq ratioSample efficiency (500/all)
0 1 2 3 4 5 6 70.00.20.40.60.8Rsq ratioSample efficiency (1,000/all)
0 1 2 3 4 5 6 70.00.20.40.60.8Rsq ratioSample efficiency (10,000/all)0
1
2
3
4
5
6
7
Figure 34: Downstream regression model sample efficiency on the MPI3D dataset (dimensionality-controlled
setting).
52Table 34: Downstream regression model sample efficiency on the Cars3D dataset
ModelsR2ratio
102/all samples 2.5×102/all samples 5×103/all samples 104/all samples 105/all samples
Symbolic scalar-tokened compositional representations
Slow-V AE 0.273 ±0.052 0.537 ±0.012 0.833 ±0.025 0.882 ±0.018 0.994±0.004
Slow-V AE†0.166±0.055 0.292 ±0.014 0.866 ±0.005 0.895 ±0.010 0.993 ±0.002
Ada-GV AE-k 0.610 ±0.131 0.834 ±0.040 0.881±0.026 0.922 ±0.019 0.989±0.005
Ada-GV AE-k†0.452±0.156 0.821 ±0.028 0.879 ±0.029 0.920 ±0.021 0.992 ±0.007
GV AE 0.412 ±0.092 0.791 ±0.013 0.855 ±0.012 0.900 ±0.008 0.986 ±0.006
GV AE†0.314±0.066 0.799±0.010 0.849±0.014 0.895 ±0.014 0.987 ±0.004
MLV AE 0.477 ±0.069 0.698 ±0.027 0.844 ±0.015 0.893 ±0.016 0.984 ±0.010
MLV AE†0.305±0.071 0.648 ±0.072 0.854 ±0.013 0.896 ±0.012 0.989 ±0.008
Symbolic vector-tokened compositional representations
VCT 0.558 ±0.028 0.694 ±0.034 0.808 ±0.013 0.837 ±0.023 0.973 ±0.024
VCT†0.197±0.151 0.640 ±0.113 0.640 ±0.113 0.725 ±0.038 1.000±0.000 (=)
Fully continuous compositional representations
Ours 0.705±0.023 0.889 ±0.042 0.926 ±0.009 0.942 ±0.018 1.000 ±0.000 (=)
Table 35: Downstream regression model sample efficiency on the Shapes3D dataset
ModelsR2ratio
102/all samples 2.5×102/all samples 5×103/all samples 104/all samples 105/all samples
Symbolic scalar-tokened compositional representations
Slow-V AE 0.021 ±0.016 0.085 ±0.035 0.183 ±0.051 0.563 ±0.070 0.994 ±0.002
Slow-V AE†0.003±0.008 0.021 ±0.025 0.066 ±0.036 0.419 ±0.107 0.995 ±0.002
Ada-GV AE-k 0.480±0.155 0.510±0.160 0.717 ±0.206 0.855 ±0.103 0.997 ±0.001
Ada-GV AE-k†0.133±0.088 0.199 ±0.058 0.643 ±0.231 0.809 ±0.164 0.998±0.000 (=)
GV AE 0.417 ±0.056 0.594 ±0.079 0.688 ±0.052 0.893 ±0.024 0.997 ±0.000
GV AE†0.13±0.047 0.234 ±0.038 0.479 ±0.076 0.871 ±0.029 0.998±0.000 (=)
MLV AE 0.371 ±0.041 0.515 ±0.093 0.590 ±0.080 0.855 ±0.059 0.997 ±0.000
MLV AE†0.123±0.048 0.235 ±0.069 0.413 ±0.035 0.808 ±0.141 0.999±0.000
Symbolic vector-tokened compositional representations
VCT 0.020 ±0.025 0.216 ±0.139 0.377 ±0.100 0.591 ±0.079 0.884 ±0.008
VCT†0.000±0.000 0.000 ±0.000 0.000 ±0.000 0.009 ±0.018 0.470 ±0.071
COMET 0.352 ±0.036 0.699 ±0.038 0.772 ±0.028 0.812 ±0.025 0.997 ±0.001
COMET†0.752±0.039 0.810 ±0.034 0.870 ±0.029 0.916 ±0.031 0.996±0.001
Fully continuous compositional representations
Ours 0.464 ±0.071 0.730±0.038 0.804 ±0.075 0.889 ±0.035 0.996±0.001
53Table 36: Downstream regression model sample efficiency on the MPI3D dataset
ModelsR2ratio
102/all samples 2.5×102/all samples 5×103/all samples 104/all samples 105/all samples
Symbolic scalar-tokened compositional representations
Slow-V AE 0.130 ±0.051 0.155 ±0.011 0.608±0.082 0.692 ±0.081 0.881 ±0.022
Slow-V AE†0.107±0.027 0.095 ±0.011 0.541 ±0.079 0.668 ±0.117 0.877 ±0.017
Ada-GV AE-k 0.270±0.037 0.279±0.026 0.500 ±0.016 0.541 ±0.020 0.703 ±0.017
Ada-GV AE-k†0.053±0.053 0.120 ±0.023 0.442 ±0.018 0.504 ±0.015 0.680 ±0.012
GV AE 0.234 ±0.035 0.282 ±0.027 0.481 ±0.032 0.524 ±0.035 0.686 ±0.033
GV AE†0.077±0.073 0.157 ±0.037 0.415 ±0.043 0.443 ±0.034 0.648 ±0.025
MLV AE 0.236 ±0.019 0.288 ±0.030 0.462 ±0.051 0.497 ±0.017 0.663 ±0.012
MLV AE†0.065±0.042 0.114 ±0.049 0.387 ±0.024 0.469 ±0.034 0.659 ±0.030
Shu 0.343 ±0.024 0.482±0.075 0.549±0.091 0.601 ±0.047 0.750 ±0.058
Shu†0.143±0.103 0.427 ±0.035 0.493 ±0.073 0.547 ±0.070 0.714 ±0.067
Symbolic vector-tokened compositional representations
VCT 0.189 ±0.107 0.246 ±0.137 0.294 ±0.145 0.312 ±0.14 0.418 ±0.180
VCT†0.039±0.088 0.168 ±0.082 0.230 ±0.110 0.279 ±0.127 0.502 ±0.052
COMET 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.823 ±0.139
COMET†0.000±0.000 0.000 ±0.000 0.000 ±0.000 0.000 ±0.000 0.187 ±0.374
Fully continuous compositional representations
Ours 0.556±0.078 0.665 ±0.067 0.721 ±0.089 0.787 ±0.078 0.858 ±0.040
C.5.2 Low Sample Regime Results
To evaluate the utility of our Soft TPR representation from the perspective of downstream models,
we additionally evaluate the raw performance of downstream models as a function of the number of
samples they have been trained on (again considering 100, 250, 500, 1,000, 10,000 and all samples).
We find that our Soft TPR representation contributes to a substantial performance boost in the
downstream model’s performance in a low-sample regime where the downstream model has been
trained with 100, 250, 500, and 1,000 samples. We present our full suite of experimental results,
and highlight the particular performance differentials conferred by our representational form in the
low-sample regime.
FoV Regression
54012345670.00.10.20.30.40.50.60.7RSqDownstream performance (100)
012345670.00.20.40.60.8RSqDownstream performance (250)
012345670.00.20.40.60.8RSqDownstream performance (500)0
1
2
3
4
5
6
7Figure 35: Downstream regression model R2scores on the Cars3D dataset (original setting).
012345670.00.20.40.60.8RSqDownstream performance (1,000)
012345670.00.20.40.60.81.0RSqDownstream performance (10,000)
012345670.00.20.40.60.81.0RSqDownstream performance (all)0
1
2
3
4
5
6
7
Figure 36: Downstream regression model R2scores on the Cars3D dataset (original setting).
55012345670.00.10.20.30.40.50.60.7RSqDownstream performance (100)
012345670.00.20.40.60.8RSqDownstream performance (250)
012345670.00.20.40.60.8RSqDownstream performance (500)0
1
2
3
4
5
6
7Figure 37: Downstream regression model R2scores on the Cars3D dataset (dimensionality-controlled setting).
012345670.00.20.40.60.8RSqDownstream performance (1,000)
012345670.00.20.40.60.81.0RSqDownstream performance (10,000)
012345670.00.20.40.60.81.0RSqDownstream performance (all)0
1
2
3
4
5
6
7
Figure 38: Downstream regression model R2scores on the Cars3D dataset (dimensionality-controlled setting).
56012345670.00.10.20.30.40.50.60.70.8RSqDownstream performance (100)
012345670.00.20.40.60.8RSqDownstream performance (250)
012345670.00.20.40.60.81.0RSqDownstream performance (500)0
1
2
3
4
5
6
7Figure 39: Downstream regression model R2scores on the Shapes3D dataset (original setting).
012345670.00.20.40.60.81.0RSqDownstream performance (1,000)
012345670.20.40.60.81.0RSqDownstream performance (10,000)
012345670.30.40.50.60.70.80.91.0RSqDownstream performance (all)0
1
2
3
4
5
6
7
Figure 40: Downstream regression model R2scores on the Shapes3D dataset (original setting).
57012345670.00.10.20.30.40.50.60.70.8RSqDownstream performance (100)
012345670.00.20.40.60.8RSqDownstream performance (250)
012345670.00.20.40.60.81.0RSqDownstream performance (500)0
1
2
3
4
5
6
7Figure 41: Downstream regression model R2scores on the Shapes3D dataset (dimensionality-controlled setting).
012345670.00.20.40.60.81.0RSqDownstream performance (1,000)
012345670.20.40.60.81.0RSqDownstream performance (10,000)
012345670.30.40.50.60.70.80.91.0RSqDownstream performance (all)0
1
2
3
4
5
6
7
Figure 42: Downstream regression model R2scores on the Shapes3D dataset (dimensionality-controlled setting).
58012345670.00.10.20.30.40.50.6RSqDownstream performance (100)
012345670.00.10.20.30.40.50.60.7RSqDownstream performance (250)
012345670.00.10.20.30.40.50.60.7RSqDownstream performance (500)0
1
2
3
4
5
6
7Figure 43: Downstream regression model R2scores on the MPI3D dataset (original setting).
012345670.00.10.20.30.40.50.60.70.8RSqDownstream performance (1,000)
012345670.00.20.40.60.8RSqDownstream performance (10,000)
012345670.30.40.50.60.70.80.91.0RSqDownstream performance (all)0
1
2
3
4
5
6
7
Figure 44: Downstream regression model R2scores on the MPI3D dataset (original setting).
59012345670.00.10.20.30.40.50.6RSqDownstream performance (100)
012345670.00.10.20.30.40.50.60.7RSqDownstream performance (250)
012345670.00.10.20.30.40.50.60.7RSqDownstream performance (500)0
1
2
3
4
5
6
7Figure 45: Downstream regression model R2scores on the MPI3D dataset (dimensionality-controlled setting).
012345670.00.10.20.30.40.50.60.70.8RSqDownstream performance (1,000)
012345670.00.20.40.60.8RSqDownstream performance (10,000)
012345670.20.30.40.50.60.70.80.91.0RSqDownstream performance (all)0
1
2
3
4
5
6
7
Figure 46: Downstream regression model R2scores on the MPI3D dataset (dimensionality-controlled setting).
60Table 37: Downstream regression model performance on the Cars3D dataset
ModelsR2score
102samples 2.5×102samples 5×103samples 104samples 105samples all samples
Symbolic scalar-tokened compositional representations
Slow-V AE 0.255 ±0.05 0.502 ±0.009 0.779 ±0.023 0.825 ±0.016 0.929 ±0.003 0.935 ±0.004
Slow-V AE†0.155±0.052 0.274 ±0.013 0.811 ±0.005 0.838 ±0.010 0.930 ±0.002 0.935 ±0.004
Ada-GV AE-k 0.584±0.125 0.798 ±0.040 0.843 ±0.028 0.883±0.020 0.947 ±0.009 0.957 ±0.005 (=)
Ada-GV AE-k†0.430±0.149 0.784 ±0.033 0.836 ±0.028 0.875±0.018 0.944 ±0.010 0.957±0.005 (=)
GV AE 0.390 ±0.087 0.750 ±0.011 0.810 ±0.009 0.852 ±0.006 0.934 ±0.006 0.947±0.005 (=)
GV AE†0.296±0.061 0.754 ±0.009 0.801 ±0.016 0.845 ±0.015 0.931 ±0.007 0.947±0.005 (=)
MLV AE 0.452 ±0.063 0.662 ±0.029 0.800 ±0.018 0.847 ±0.010 0.933 ±0.007 0.948 ±0.007
MLV AE†0.289±0.066 0.614 ±0.067 0.809 ±0.011 0.849 ±0.014 0.938 ±0.006 0.948 ±0.007
Shu 0.044 ±0.045 0.055 ±0.044 0.061 ±0.047 0.067 ±0.050 0.085 ±0.056 0.075 ±0.051
Shu†0.010±0.042 0.065 ±0.022 0.079 ±0.031 0.079 ±0.030 0.101 ±0.046 0.075 ±0.051
Symbolic vector-tokened compositional representations
VCT 0.454 ±0.021 0.565 ±0.033 0.657 ±0.009 0.681 ±0.019 0.792 ±0.018 0.813 ±0.014
VCT†0.117±0.087 0.340 ±0.073 0.390 ±0.059 0.444 ±0.019 0.643 ±0.034 0.813 ±0.014
COMET -0.058 ±0.027 -0.098 ±0.054 -0.074 ±0.036 -0.147 ±0.101 -0.036 ±0.028 -0.035 ±0.023
COMET†-0.379 ±0.303 -0.258 ±0.085 -0.966 ±0.470 -0.306 ±0.103 -0.739 ±0.751 -0.035 ±0.023
Fully continuous compositional representations
Ours 0.642±0.023 0.809 ±0.037 0.843 ±0.010 0.858±0.019 0.917 ±0.013 0.910 ±0.010
Table 38: Downstream regression model performance on the Shapes3D dataset
ModelsR2score
102samples 2.5×102samples 5×103samples 104samples 105samples all samples
Symbolic scalar-tokened compositional representations
Slow-V AE 0.019 ±0.017 0.085 ±0.035 0.183 ±0.051 0.562 ±0.070 0.993 ±0.002 1.000±0.000 (=)
Slow-V AE†-0.030 ±0.034 0.018 ±0.028 0.066 ±0.036 0.418 ±0.107 0.994 ±0.003 0.999±0.000
Ada-GV AE-k 0.480 ±0.155 0.510 ±0.160 0.666 ±0.170 0.854 ±0.103 0.997±0.001 (=) 1.000±0.000 (=)
Ada-GV AE-k†0.133±0.088 0.199 ±0.058 0.563 ±0.209 0.809 ±0.164 0.997±0.001 (=) 1.000±0.000 (=)
GV AE 0.417 ±0.056 0.593 ±0.079 0.687 ±0.052 0.893±0.024 0.997 ±0.000 (=) 1.000±0.000 (=)
GV AE†0.130±0.047 0.234 ±0.038 0.478 ±0.076 0.870 ±0.029 0.998±0.000 1.000 ±0.000 (=)
MLV AE 0.370 ±0.041 0.515 ±0.093 0.590 ±0.080 0.854 ±0.059 0.996 ±0.000 1.000±0.000 (=)
MLV AE†0.123±0.048 0.234 ±0.069 0.412 ±0.035 0.807 ±0.141 0.997±0.000 (=) 1.000±0.000 (=)
Shu 0.062 ±0.086 0.117 ±0.075 0.146 ±0.073 0.157 ±0.078 0.245 ±0.096 0.427 ±0.082
Shu†-0.029 ±0.076 0.093 ±0.08 0.118 ±0.077 0.131 ±0.079 0.228 ±0.085 0.419 ±0.082
Symbolic vector-tokened compositional representations
VCT -0.201 ±0.289 0.202 ±0.136 0.359 ±0.093 0.564 ±0.075 0.844 ±0.012 0.954 ±0.008
VCT†-4.183 ±1.679 -1.464 ±0.492 -0.552 ±0.307 -0.205 ±0.134 0.396 ±0.063 0.843 ±0.040
COMET 0.352 ±0.036 0.699 ±0.038 0.772 ±0.029 0.812 ±0.025 0.996 ±0.001 1.000±0.000 (=)
COMET†0.751±0.039 0.870 ±0.029 0.996 ±0.001 0.915 ±0.031 0.996±0.001 1.000±0.000 (=)
Fully continuous compositional representations
Ours 0.464±0.071 0.730±0.038 0.804 ±0.075 0.889±0.035 0.995 ±0.002 1.000±0.000 (=)
61Table 39: Downstream regression model performance on the MPI3D dataset
ModelsR2score
102samples 2.5×102samples 5×103samples 104samples 105samples all samples
Symbolic scalar-tokened compositional representations
Slow-V AE 0.127 ±0.059 0.152 ±0.011 0.596±0.083 0.678 ±0.082 0.863±0.023 0.980±0.005
Slow-V AE†0.105±0.026 0.093 ±0.011 0.531 ±0.077 0.655 ±0.113 0.860±0.016 0.981±0.004
Ada-GV AE-k 0.206±0.031 0.213±0.023 0.381 ±0.014 0.412 ±0.015 0.536 ±0.013 0.762 ±0.014
Ada-GV AE-k†0.037±0.045 0.093 ±0.018 0.342 ±0.014 0.390 ±0.012 0.527 ±0.010 0.774 ±0.003
GV AE 0.181 ±0.030 0.217 ±0.023 0.371 ±0.026 0.404 ±0.028 0.530 ±0.035 0.771 ±0.018
GV AE†0.056±0.057 0.120 ±0.029 0.316 ±0.027 0.338 ±0.019 0.495 ±0.020 0.764 ±0.019
MLV AE 0.182 ±0.013 0.222±0.024 0.357±0.042 0.384 ±0.023 0.513 ±0.025 0.773 ±0.031
MLV AE†0.051±0.032 0.089 ±0.037 0.305 ±0.018 0.370 ±0.023 0.520 ±0.033 0.790 ±0.027
Shu 0.151 ±0.016 0.211 ±0.026 0.238 ±0.019 0.264 ±0.018 0.330 ±0.033 0.441 ±0.041
Shu†0.057±0.048 0.186 ±0.012 0.214 ±0.017 0.237 ±0.013 0.311 ±0.020 0.438 ±0.037
Symbolic vector-tokened compositional representations
VCT 0.047 ±0.139 0.110 ±0.087 0.124 ±0.104 0.105 ±0.175 0.164 ±0.183 0.455 ±0.071
VCT†-0.005 ±0.058 0.084 ±0.062 0.106 ±0.092 0.153 ±0.072 0.276 ±0.036 0.550 ±0.046
COMET -0.051 ±0.015 -0.042 ±0.018 -0.037 ±0.012 -0.053 ±0.024 0.218 ±0.036 0.266 ±0.004
COMET†-0.118 ±0.045 -0.089 ±0.050 -0.074 ±0.039 -0.179 ±0.098 0.027 ±0.099 0.236 ±0.008
Fully continuous compositional representations
Ours 0.490±0.068 0.594±0.056 0.635 ±0.070 0.693 ±0.060 0.757±0.03 0.882 ±0.016
b) Abstract Visual Reasoning
62012345670.150.200.250.300.35AccuracyDownstream performance (100)
012345670.150.200.250.300.35AccuracyDownstream performance (250)
012345670.150.200.250.300.350.40AccuracyDownstream performance (500)0
1
2
3
4
5
6
7Figure 47: Downstream WReN model classification accuracy on the abstract visual reasoning dataset (original
setting).
012345670.150.200.250.300.350.400.450.50AccuracyDownstream performance (1,000)
012345670.20.30.40.50.6AccuracyDownstream performance (10,000)
012345670.20.30.40.50.60.70.80.91.0AccuracyDownstream performance (100,000)0
1
2
3
4
5
6
7
Figure 48: Downstream WReN model classification accuracy on the abstract visual reasoning dataset (original
setting).
63012345670.000.050.100.150.200.250.300.35AccuracyDownstream performance (100)
012345670.100.150.200.250.300.35AccuracyDownstream performance (250)
012345670.100.150.200.250.300.350.40AccuracyDownstream performance (500)0
1
2
3
4
5
6
7Figure 49: Downstream WReN model classification accuracy on the abstract visual reasoning dataset
(dimensionality-controlled setting).
012345670.150.200.250.300.350.400.450.50AccuracyDownstream performance (1,000)
012345670.20.30.40.50.60.7AccuracyDownstream performance (10,000)
012345670.20.30.40.50.60.70.80.91.0AccuracyDownstream performance (100,000)0
1
2
3
4
5
6
7
Figure 50: Downstream WReN model classification accuracy on the abstract visual reasoning dataset
(dimensionality-controlled setting).
64Table 40: Downstream WReN model performance on the abstract visual reasoning dataset
ModelsClassification accuracy
102samples 2.5×102samples 5×102samples 103samples 104samples 105samples
Symbolic scalar-tokened compositional representations
Slow-V AE 0.178 ±0.008 0.196 ±0.024 0.196 ±0.028 0.228 ±0.030 0.361 ±0.028 0.396 ±0.022
Slow-V AE†0.146±0.084 0.149 ±0.034 0.163 ±0.039 0.237 ±0.023 0.345 ±0.021 0.959±0.009
Ada-GV AE-k 0.188 ±0.023 0.198 ±0.018 0.203 ±0.007 0.194 ±0.008 0.295 ±0.028 0.472 ±0.037
Ada-GV AE-k†0.182±0.007 0.234±0.024 0.285 ±0.025 0.319±0.004 0.662±0.023 0.954±0.009
GV AE 0.171 ±0.009 0.189 ±0.008 0.182 ±0.013 0.188 ±0.020 0.319 ±0.038 0.478 ±0.032
GV AE†0.180±0.022 0.237 ±0.018 0.287 ±0.014 0.306 ±0.020 0.684±0.020 0.936±0.008
MLV AE 0.171 ±0.016 0.188 ±0.009 0.193 ±0.012 0.194 ±0.015 0.293 ±0.012 0.432 ±0.023
MLV AE†0.177±0.012 0.221 ±0.023 0.277 ±0.023 0.325 ±0.017 0.644 ±0.026 0.925 ±0.024
Shu 0.175 ±0.009 0.184 ±0.018 0.200 ±0.010 0.202 ±0.014 0.288 ±0.038 0.310 ±0.038
Shu†0.177±0.014 0.230 ±0.045 0.285 ±0.028 0.302 ±0.034 0.444 ±0.013 0.444 ±0.013
Symbolic vector-tokened compositional representations
VCT 0.228±0.036 0.229±0.049 0.277 ±0.039 0.326±0.042 0.371±0.040 0.641 ±0.039
VCT†0.191±0.017 0.226 ±0.031 0.259 ±0.009 0.282 ±0.017 0.319 ±0.018 0.458 ±0.040
COMET 0.174 ±0.010 0.241 ±0.010 0.259 ±0.016 0.283 ±0.009 0.221 ±0.012 0.983±0.006
COMET†0.190±0.012 0.214 ±0.004 0.236 ±0.013 0.213 ±0.023 0.532 ±0.348 0.532 ±0.348
Fully continuous compositional representations
Ours 0.273±0.033 0.312 ±0.027 0.360 ±0.033 0.412 ±0.066 0.560±0.103 0.869 ±0.024
C.6 More Ablation Experiments
C.6.1 Soft TPR vs TPR
To verify that our Soft TPR confers exclusive benefits compared to TPRs, we repeat all our experi-
ments using the explicit TPR that our TPR decoder produces, which represents the Soft TPR’s greedily
optimal explicit TPR counterpart, ψ∗
tpr. In all plots, we use the same legend, denoting the explicit
TPR as yellow (0) and the Soft TPR as blue (1). Across all considered cases: i.e., 1) convergence rate
of representation learning (as measured by the downstream model’s ability to effectively leverage
representations produced at different stages of training), 2) sample efficiency of downstream models,
and 3) raw performance of downstream models in the low sample regime, the Soft TPR confers
differential performance boosts compared to the explicit TPR. This offers strong empirical evidence
for our hypothesis that embodying a more approximate form of explicitly compositional structure
helps representation learners by alleviating the strigent requirement of having to produce explicit
TPRs, and additionally provides representation learners with more knowledge of the entire manifold
underlying compositional structure, which TPRs cannot fully capture either due to representation
learner deficits, or the inherent quasi-compositional structure of represented objects.
Convergence Rate of Representation Learning
65102103104105
Iteration count0.00.20.40.60.8RSq
Cars3D 0
1Figure 51: Convergence of Soft TPR (0) vs TPR (1) as measured by FoV regression on the Cars3D dataset
102103104105
Iteration count0.600.650.700.750.800.850.900.951.00RSq
Shapes3D 0
1
Figure 52: Convergence of Soft TPR (0) vs TPR (1) as measured by FoV regression on the Shapes3D dataset
66102103104105
Iteration count0.30.40.50.60.70.80.9RSq
MPI3D 0
1Figure 53: Convergence of Soft TPR (0) vs TPR (1) as measured by FoV regression on the MPI3D dataset
102103104105
n epochs0.700.750.800.850.900.95Accuracy
Abstract visual reasoning dataset 0
1
Figure 54: Convergence of Soft TPR (0) vs TPR (1) as measured by classification performance on the abstract
visual reasoning dataset
Sample Efficiency of Downstream Models
670 10.40.50.60.7Rsq ratioSample efficiency (100/all)
0 10.550.600.650.700.750.800.850.900.95Rsq ratioSample efficiency (250/all)0
1Figure 55: Downstream regression model sample efficiency on the Cars3D dataset using TPR representations (0)
vs Soft TPR representations (1)
0 10.840.860.880.900.920.940.960.981.00Rsq ratioSample efficiency (500/all)
0 10.880.900.920.940.960.981.00Rsq ratioSample efficiency (1,000/all)
0 10.960.981.001.021.04Rsq ratioSample efficiency (10,000/all)0
1
Figure 56: Downstream regression model sample efficiency on the Cars3D dataset using TPR representations (0)
vs Soft TPR representations (1)
680 10.200.250.300.350.400.450.500.550.60Rsq ratioSample efficiency (100/all)
0 10.40.50.60.70.8Rsq ratioSample efficiency (250/all)0
1Figure 57: Downstream regression model sample efficiency on the Shapes3D dataset using TPR representations
(0) vs Soft TPR representations (1)
0 10.600.650.700.750.800.850.90Rsq ratioSample efficiency (500/all)
0 10.750.800.850.900.95Rsq ratioSample efficiency (1,000/all)
0 10.940.960.981.001.021.04Rsq ratioSample efficiency (10,000/all)0
1
Figure 58: Downstream regression model sample efficiency on the Shapes3D dataset using TPR representations
(0) vs Soft TPR representations (1)
690 10.450.500.550.600.650.70Rsq ratioSample efficiency (100/all)
0 10.550.600.650.700.750.800.850.90Rsq ratioSample efficiency (250/all)0
1Figure 59: Downstream regression model sample efficiency on the MPI3D dataset using TPR representations (0)
vs Soft TPR representations (1)
0 10.550.600.650.700.750.800.850.90Rsq ratioSample efficiency (500/all)
0 10.650.700.750.800.850.900.95Rsq ratioSample efficiency (1,000/all)
0 10.800.850.900.951.00Rsq ratioSample efficiency (10,000/all)0
1
Figure 60: Downstream regression model sample efficiency on the MPI3D dataset using TPR representations (0)
vs Soft TPR representations (1)
700 10.2000.2250.2500.2750.3000.3250.3500.3750.400Accuracy ratioSample efficiency (100/100,000)
0 10.250.300.350.400.45Accuracy ratioSample efficiency (250/100,000)0
1Figure 61: Downstream WReN model sample efficiency on the abstract visual reasoning dataset using TPR
representations (0) vs Soft TPR representations (1)
0 10.250.300.350.400.45Accuracy ratioSample efficiency (500/100,000)
0 10.300.350.400.450.500.550.60Accuracy ratioSample efficiency (1,000/100,000)
0 10.350.400.450.500.550.600.650.700.75Accuracy ratioSample efficiency (10,000/100,000)0
1
Figure 62: Downstream WReN model sample efficiency on the abstract visual reasoning dataset using TPR
representations (0) vs Soft TPR representations (1)
Low Sample-Regime Performance of Downstream Models
710 10.300.350.400.450.500.550.600.650.70RSqDownstream performance (100)
0 10.500.550.600.650.700.750.800.85RSqDownstream performance (250)0
1Figure 63: Downstream regression model R2scores on the Cars3D dataset using TPR representations (0) vs
Soft TPR representations (1)
Figure 64: Downstream regression model R2scores on the Cars3D dataset using TPR representations (0) vs
Soft TPR representations (1)
720 10.200.250.300.350.400.450.500.550.60RSqDownstream performance (100)
0 10.40.50.60.70.8RSqDownstream performance (250)0
1Figure 65: Downstream regression model R2scores on the Shapes3D dataset using TPR representations (0) vs
Soft TPR representations (1)
0 10.750.800.850.900.95RSqDownstream performance (1,000)
0 10.940.960.981.001.021.04RSqDownstream performance (10,000)
0 10.960.981.001.021.04RSqDownstream performance (all)0
1
Figure 66: Downstream regression model R2scores on the Shapes3D dataset using TPR representations (0) vs
Soft TPR representations (1)
730 10.350.400.450.500.550.60RSqDownstream performance (100)
0 10.450.500.550.600.650.70RSqDownstream performance (250)0
1Figure 67: Downstream regression model R2scores on the MPI3D dataset using TPR representations (0) vs
Soft TPR representations (1)
0 10.550.600.650.700.750.80RSqDownstream performance (1,000)
0 10.650.700.750.800.85RSqDownstream performance (10,000)
0 10.750.800.850.90RSqDownstream performance (all)0
1
Figure 68: Downstream regression model R2scores on the MPI3D dataset using TPR representations (0) vs
Soft TPR representations (1)
740 10.1750.2000.2250.2500.2750.3000.3250.350AccuracyDownstream performance (100)
0 10.2000.2250.2500.2750.3000.3250.3500.375AccuracyDownstream performance (250)0
1Figure 69: Downstream WReN model classification accuracy on the abstract visual reasoning dataset using TPR
representations (0) vs Soft TPR representations (1)
0 10.250.300.350.400.450.50AccuracyDownstream performance (1,000)
0 10.300.350.400.450.500.550.600.65AccuracyDownstream performance (10,000)
0 10.7750.8000.8250.8500.8750.9000.9250.950AccuracyDownstream performance (100,000)0
1
Figure 70: Downstream WReN model classification accuracy on the abstract visual reasoning dataset using TPR
representations (0) vs Soft TPR representations (1)
C.6.2 Robustness to Hyperparameter Choices
We perform an additional experiment to empiricially verify that our model is robust to different
hyperparameter choices. For the MPI3D dataset, which disentanglement models experience the
greatest difficulty in producing disentangled representations for (see Table 1), we randomly pick
another set of hyperparameters, shown in Table 41, from the top 5 models based on the MSE loss
criterion. For this randomly chosen hyperparameter configuration, we evaluate the disentanglement
of the representations on the MPI3D dataset produced by the resulting model.
75Table 41: Hyperparameter values of ablation setting
HyperparameterMPI3D
Architectural hyperparameters
DR 16
NR(fixed) 10
DF 64
NF 50
Loss function hyperparameters
λ1 0.0000
λ2 0.25378
β(fixed) 0.5
Table 42: Disentanglement metric scores on the MPI3D dataset
Hyperparameter configuration FactorV AE score DCI score BetaV AE score MIG score
Original 0.949 ±0.032 0.828 ±0.015 1.000 ±0.000 0.620 ±0.067
Ablation 0.911 ±0.029 0.798 ±0.031 1.000 ±0.000 0.590 ±0.047
D Limitations and Future Work
D.1 Extension to Linguistic Domains
Applying the Soft TPR to the TPR’s typical domain of language is an intriguing future direction,
especially as language can deviate from strict algebraic compositionality – for instance, idiomatic
expressions such as ‘spill the beans’ cannot be understood as a function of their constituents alone.
Soft TPR’s more flexible specification allows it to capture approximate forms of compositionality
precluded from the TPR’s strict algebraic definition (Eq 1), thereby potentially providing the Soft
TPR the ability to better handle the nuance and complexity of language.
To adapt our framework to language, we replace our Conv/Deconv encoder/decoders with simple
RNNs, retrain our TPR decoder, and remove the semi-supervised loss, using Eq 6 as the full loss.
Preliminary results in Table 43 on the BaBI [7] dataset are compared with TPR baselines from AID
[51]. Our Soft TPR Autoencoder does not presently surpass AID, but notable points include:
1.Our use of simpler RRN-based encoders and an MLP-based downstream network, unlike
the more sophisticated architectures of [51].
2.Soft TPR retains its performance improvement above the corresponding explicit TPR it can
be quantised into.
3.The smaller gap between systematic vs non-systematic dataset splits in our model compared
to TPR-RNN (+AID) and FWM.
4.We train our representation learner using self-supervision (reconstruction loss) alone, only
employing supervision on the downstream prediction network, while the baselines employ
strong supervision and end-to-end training to produce representations.
Table 43: Mean word error rate [%] on the sys-bAbI task [51]
Model w/o sys diff w/ sys diff Gap
TPR-RNN 0.79 ±0.16 8.74 ±3.74 7.95
TPR-RNN + AID 0.69±0.08 5.61±1.78 4.92
FWM 0.79 ±0.14 2.85±1.61 2.06
FWM + AID 0.45±0.16 1.21 ±0.66 0.76
Ours (TPR) 4.54 ±0.61 6.51 ±1.23 1.97
Ours (Soft TPR) 2.79 ±0.24 3.64 ±0.69 0.85
76D.2 Need for Weak Supervision
To produce a compositional representation, ψ(x) =C(ψ1(a1), . . . , ψ n(an))(as in Section 3.1), each
representational constituent, ψi(ai), must map 1-1 to a data constituent, ai. Without supervision
(i.e., access only to observational data, {xi}), this is challenging, because the data constituents {ai}
underlying each object, x, are unknown and cannot be identified.
We can frame the above intuition in a more mathematically rigorous way, using the generative
framework. Essentially, as formally proved in [25], it is impossible to identify the true distribution
for the data constituents (generative factors), p(a), using observational data, {xi}, alone as there
are infinitely many bijective functions f: supp( a)→asuch that: 1) aandf(a)are completely
entangled (i.e. non-diagonal Jacobian) and2) the marginal distributions of aandf(a)are identical
(meaning the marginal distributions of the observations are also identical, i.e.,R
p(x|a)p(a)da=R
p(x|f(a))p(f(a))da). Thus, without inductive biases, it is impossible to infer the data constituents
{ai}of any observation, x, from observational data {xi}alone.
To combat this non-identifiability result, in line with [13, 22, 31, 33, 35], we use weak supervision,
presenting the model with data pairs (x, x′)where xandx′differ in a subset of FoVs, e.g. β(x) =
{shape /cube,colour /purple, size/large},β(x′) = {shape /cube,colour /cyan, size/large},
where the FoV types corresponding to the different FoV values are known to the model. Note
that this weak supervision is minimal, only providing the model to access to the differing FoV types in
an index set, I, (i.e., I:={colour }) and not anyof the FoV values (i.e., {cube,purple ,cyan,large}
are all not known by the model).
Some possible future extensions to reduce this level of weak supervision, or alternative forms of
weak supervision include:
1.Embodied Learning : In the visual domain, some roles, e.g. object position correspond to
affordances. Embodied agents may be able to reduce the need for explicit supervision by
collecting (x, x′)andIthrough interaction with their environment.
2.Pretrained Filler Embeddings : Initialising the filler embedding matrix, MξFwith em-
beddings learnt by a pre-trained vision model could impart knowledge of domain-agnostic
fillers (e.g., colours, shapes), reducing the need to explicitly provide (x, x′)andIto the
model.
3.Segmentation Masks : Segmentation masks for each object may potentially reduce the need
to explicitly provide (x, x′)andIto the model.
D.3 Downstream Utility
Our investigation of downstream utility centers on two selected tasks – FoV regression/classification
and abstract visual reasoning, which aligns with the standard framework for assessing the quality and
downstream utility of compositional representations [26, 30, 33, 40, 44, 46]. While existing work [30,
33] demonstrates that explicitly compositional representations enhance downstream sample efficiency
compared to non-compositional representations, a result we improve upon (C.5.1), the broader utility
of compositional representations remains a topic of ongoing exploration [21, 40, 42, 44].
Theoretical perspectives [1, 2] argue that explicitly compositional representations are fundamental
in the production of productive, systematic, and inferentially coherent thought – 3 key properties
characterising human cognition. Investigating how explicitly compositional representations can
yield empirical benefits across these dimensions represents an essential avenue for future research.
Although preliminary studies [40, 42, 44] do not find strong evidence that explicitly compositional
representations improve compositional generalisation (a key aspect of systematicity), [44] suggests
that this finding is because compositional representations are necessary, but not sufficient to induce
systematicity; an explicitly compositional processing approach [6] is also required.
Future work could extend our theoretical framework with the hope of producing empirical results
consistent with the theoretical arguments of [1, 2]. In particular, as our unbinding module is designed
to provably and efficiently recover structured role-filler constituents from the Soft TPR, it may
be possible to exploit this module to systematically reconfigure roles and fillers from existing
representations to create representations of novel combinations of role-filler bindings (i.e., novel
77compositional data). This type of ability could prove extremely beneficial in areas such as concept
learning and compositional generalisation.
D.4 Dimensionality
In this subsection, we denote the dimensions of the role and filler embedding spaces as DFandDR
respectively. We also denote the number of fillers as NFand the number of roles as NR.
The Soft TPR belongs to VF⊗VR, which is a DF×DRdimensional space, which grows mul-
tiplicatively in DF,DR. Several factors, however, mitigate scalability concerns in light of this
fact:
1.Independence of Embedding Space Dimensionality : We note that the dimensionality
of the role and filler embedding spaces ( DR, DFrespectively) are properties of the corre-
sponding embedding functions ( ξR:R→VRandξF:F→VF) and thus, can be fixed
independently of the number of roles, NR, the number of fillers, NF, or the number of
total role-filler bindings (which we denote by n) within a domain. Thus, it is possible to
fix the Soft TPR’s dimensionality ( DF×DR) to be smaller than NF×NR(the number
of roles/FoV types multipled by the number of fillers/FoV tokens) or n(the number of
bindings), which all may be large in complex visual domains. As illustrated in Table 44,
the dimensionality of the TPR is smaller than NF×NRin both the Shapes3D and MPI3D
domains.
2.Relaxing Orthogonality : While DFcan be set a priori with no regard to NR, NForn,
we require DR≥NRfor semi-orthogonality of the role-embedding matrix MξR, which
guarantees faithful (see proof 2 of A.2) and computationally efficient (see ‘Unbinding’
heading of B.3.1) recoverability of constituents. It is, however, possible to relax this
constraint (i.e., to have DR< N R) to further reduce dimensionality. In this case, semi-
orthogonality of MξRis impossible and hence the recoverability of constituents cannot be
guaranteed, however, there are some less stringent guarantees on the outcome of unbinding
that can still be derived (see p291 of [3] for more details).
We also more explicitly compare the dimensionality of the Soft TPR with baselines in Table 45.
Scalar-tokened symbolic representations have a low dimensionality of 10(NR) at the expense of
representational expressivity (each representational constituent ψi(ai)is scalar-valued). In contrast,
Soft TPR has vector-valued representational constituents (i.e. ≈ξF(fm(i))⊗ξR(ri)), similar to VCT
and COMET. When compared to these models, the Soft TPR has significantly lower dimensionality
compared to VCT and is comparable with COMET.
Table 44: Comparison of multiplicative dimensionality
ParameterDataset
Cars3D Shapes3D MPI3D
DR 12 16 12
NR 10 10 10
DF 128 32 32
NF 106 57 50
DF·DR(TPR dimension ) 1536 512 384
NF·NR 1060 570 500Table 45: Comparison of dimensionality of representa-
tions
ModelCars3D Shapes3D MPI3D
Representational dimension
Symbolic scalar-tokened compositional representations
SlowV AE 10 10 10
Ada-GV AE-k 10 10 10
GV AE 10 10 10
ML-V AE 10 10 10
Shu 10 10 10
Symbolic vector-tokened compositional representations
VCT 5120 5120 5120
COMET 640 640 640
Fully continuous compositional representations
Ours (Soft TPR) 1536 512 384
D.5 Computational Cost
In the Soft TPR Autoencoder, the expensive tensor product operation is employed to generate ψ∗
tpr.
Given the computational cost of the tensor product, we more concretely examine the computational
78cost of training the Soft TPR Autoencoder by computing the FLOPs for a single forward pass
on a batch size of 16 using the open-source implementation of fvcore https://github.com/
facebookresearch/fvcore/tree/main/docs , visible in Table 46. This data demonstrates that,
despite the tensor product’s computational cost, the mathematically-informed derivation of our model
allows it to obtain compositional representations with vector-valued representational constituents at a
significantly lower cost compared to relevant, vector-tokened baselines (2 orders of magnitude less
than VCT, and 4 orders of magnitude less than COMET).
Future work could explore the use of tensor contraction techniques to reduce computational expense.
For instance [29] uses a Hadamard product based tensor product compression technique. This reduces
computational cost from n2(tensor product of 2 vectors) to n(Hadamard product), but comprises
the theoretical guarantees on constituent recoverability. We believe developing tensor contraction
techniques within the TPR framework is an important direction for future research, to ensure efficient
TPR-based representations with provable recoverability of constituents.
Table 46: Comparison of FLOPs required for a forward pass of batch size 16
ModelCars3D Shapes3D MPI3D
Representational dimension
Symbolic scalar-tokened compositional representations
SlowV AE 1.47×1081.47×1081.47×108
Ada-GV AE-k 1.47×1081.47×1081.47×108
GV AE 1.47×1081.47×1081.47×108
ML-V AE 1.47×1081.47×1081.47×108
Shu 1.45×1081.45×1081.45×108
Symbolic vector-tokened compositional representations
VCT 2.53×10112.53×10112.53×1011
COMET 5.12×10135.12×10135.12×1013
Fully continuous compositional representations
Ours (Soft TPR) 3.21×1092.93×1092.89×109
79NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: All claims accurately reflect the paper’s contributions and scope. We provide
theoretical proofs in the Appendix, as well as our complete suite of experimental results.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We explicitly state in the last paragraph of the paper that future work is to be
done in developing hierarchical Soft TRP representations.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ’Limitations’ section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
80Answer: [Yes]
Justification: We include complete proofs for all theoretical results in the Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide all information (specification of model architecture, computing
resources, hyperparameters) to replicate experimental results in the Appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
81Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: All datasets are open-access. We provide sufficient instructions in our Ap-
pendix to reproduce experimental results. Furthermore, we provide the main code for our
Soft TPR model architecture with this submission. We plan to release a streamlined version
of our entire code base used to conduct all experiments if the paper is accepted.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide these details in the Appendix, and additionally, in the code.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We present results with standard deviations, as well as the IQR and whiskers
extending to 1.5 times the IQR for all box plots.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer ’Yes’ if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the
main claims of the paper.
82•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We specify the specific GPUs we use for all experiments in the Appendix,
as well as the average amount of time it takes to train the Soft TPR Autoencoder for each
dataset.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Our research conforms to the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: As the central aim of our work relates to the representational form of composi-
tional representations, our research is theoretical in nature and has no immediate societal
impact.
83Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our work poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: In the main paper, as well as the Appendix, we cite the papers associated
with each dataset we use. We additionally make explicit references in the Appendix to
all externally created code that we use in our experiments. Furthermore, if the paper is to
be accepted, we will clearly provide licenses and attribution to the original authors where
applicable in the code files.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
84•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The code we submit in our submission corresponds with our Soft TPR Au-
toencoder architecture, and is thus reasonably straightforward to understand, especially
when supplemented by this paper. If this paper is accepted, however, we will include more
extensive written documentation for the official, more extensive codebase we release.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our research does not involve crowdsourcing nor human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
85Justification: Not applicable.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
86