Be Confident in What You Know: Bayesian Parameter
Efficient Fine-Tuning of Vision Foundation Models
Deep Shankar Pandey†Spandan Pyakurel†Qi Yu∗
Rochester Institute of Technology
{dp7972,sp1468,qi.yu}@rit.edu
Abstract
Large transformer-based foundation models have been commonly used as pre-
trained models that can be adapted to different challenging datasets and settings
with state-of-the-art generalization performance. Parameter efficient fine-tuning
(PEFT ) provides promising generalization performance in adaptation while incur-
ring minimum computational overhead. However, adaptation of these foundation
models through PEFT leads to accurate but severely underconfident models, espe-
cially in few-shot learning settings. Moreover, the adapted models lack accurate
fine-grained uncertainty quantification capabilities limiting their broader applicabil-
ity in critical domains. To fill out this critical gap, we develop a novel lightweight
Bayesian Parameter Efficient Fine-Tuning (referred to as Bayesian-PEFT ) frame-
work for large transformer-based foundation models. The framework integrates
state-of-the-art PEFT techniques with two Bayesian components to address the
under-confidence issue while ensuring reliable prediction under challenging few-
shot settings. The first component performs base rate adjustment to strengthen the
prior belief corresponding to the knowledge gained through pre-training, making
the model more confident in its predictions; the second component builds an ev-
idential ensemble that leverages belief regularization to ensure diversity among
different ensemble components. Our thorough theoretical analysis justifies that the
Bayesian components can ensure reliable and accurate few-shot adaptations with
well-calibrated uncertainty quantification. Extensive experiments across diverse
datasets, few-shot learning scenarios, and multiple PEFT techniques demonstrate
the outstanding prediction and calibration performance by Bayesian-PEFT .
1 Introduction
Transformer-based foundation models have been developed as general models with state-of-the-art
generalization performance [ 32,66,52,28]. These models leverage the rich meta-knowledge acquired
during the pre-training stage to effectively adapt to complex downstream tasks [ 32], where pre-
training is usually performed on massive-scale annotated datasets ( e.g., [35,55]) through supervised
learning[ 32,66,28] or self-supervised learning [ 52]. To achieve effective adaption, various parameter-
efficient fine-tuning ( PEFT ) approaches have been developed [ 38,27,9,54] that introduce a small
number of tunable parameters either within or outside of the backbone architecture to ensure good
generalization capability while incurring little computational overhead because most parts of (or
the entire) backbone architecture is frozen during fine-tuning [ 25,59,67]. Bias-fine tuning [ 9], a
representative partial tuning-based PEFT , only fine-tunes the bias parameters to downstream tasks.
Adapter [ 51] and side-tune [ 72] fine-tuning techniques are instances of extra module-based PEFT that
introduce extra parameterized modules and fine-tune them based on the downstream tasks. Visual
Prompt-tuning [ 32] (VPT) follows the popular prompt learning paradigm by introducing a learnable
∗Corresponding author,†equal contribution
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Transformer Encoder Layer
Transformer Encoder Layer
Transformer Encoder Layer
Head
Learnable Prompts
Multihead Attention
Add & NormLayerNormLayerNormAdd & Norm
Bias Fine TuningFrozenLearnable
Head
Side NetworkAdapterBackbone
InputweightbiasECE Vs. Accuracy from different PEFT methodsperforming few (1-20)-shot adaption 
(a)(b)
Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods
prompt that is fine-tuned on the downstream task knowledge keeping the pre-trained backbone frozen.
Despite the attractive generalization performance, most foundation models adapted to downstream
few-shot tasks through PEFT exhibit a somewhat surprising and undesirable behavior that may prevent
them from being applied to many critical domains. Figure 1 (b) shows the predictive accuracy versus
the Expected Calibration Error (ECE) of a foundation model after performing few-shot adaptation on
CIFAR-100 using a series of representative PEFT methods, including VPT [ 32], Adapter [ 51], Bias
Fine Tuning [ 71], and Side-tune [ 72]. It is clear that the adapted model is able to provide accurate
predictions even after fine-tuned on limited training samples. For example, all PEFT methods help
to boost the model’s accuracy to over 75% using just 5-shot fine-tuning and the accuracy reaches
80% after 10-shot fine-tuning. However, the adapted model is very poorly calibrated as shown by
the large ECE scores, which are consistently over 0.3 across all the fine-tuning methods. Increasing
the fine-tuning size does not show clear improvement and sometimes even hurts the calibration
performance. While one may expect the poor ECE to be caused by over-confidence as we fine-tune
a large foundation model using very limited training samples, the detailed ECE plots as shown in
Figure 2 (a)-(c) reveal that the model is in fact severely under-confident. For example, after adapting
to the 1-shot training dataset, the VPT fine-tuned model can already achieve a test accuracy close
to50% but is under-confident in almost all its predictions leading to an ECE score over 0.45. The
under-confidence issue is observed for all representative PEFT methods across different datasets and
various few-shot learning settings as evidenced by our experiments.
The under-confident few-shot adaptation behavior of foundation models closely mimics how human
experts with rich domain knowledge in their own disciplines tend to make conservative decisions
when facing new tasks that deviate from their own expertise. Analogous to their human counterparts,
the rich prior knowledge gained through the pre-training stage of foundational models overshadows
the relatively limited knowledge obtained through few-shot fine-tuning, which prevents them from
making more confident predictions in the downstream tasks. Unreliable uncertainty ( i.e.,confidence)
quantification makes the predictions provided by these models less trustworthy, which may limit
applying the promising “pre-train-then-finetune” paradigm to many critical domains. As shown in
Figure 2 (a)-(c), the fine-tuned model seldom makes any predictions with confidence over 80%,
making it difficult to leverage these predictions in any high-stakes decision-making process.
The need to balance between the rich prior knowledge gained through pre-training and the new
knowledge obtained through few-shot adaptation inspires us to investigate the under-confidence issue
from the Bayesian perspective. In particular, we propose to look into the predictive behavior of the
few-shot adapted foundation model through the lenses of evidential learning [ 56], which is built upon
Bayesian theorem and subjective logic (SL) theory [ 33]. As part of the recent advances in modern
Bayesian modeling, evidential learning provides a cost-effective way to perform Bayesian inference
with the capability to quantify fine-grained second-order uncertainty [ 57]. By leveraging the important
20.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 49.155, ECE: 0.451(a) VPT
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 47.435, ECE: 0.396 (b) Adapter
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 34.863, ECE: 0.327 (c) Bias
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 49.155, ECE: 0.064 (d)B-PEFT
Figure 2: PEFT on the 1-shot CIFAR100 dataset: all existing PEFT techniques exhibit severe under-
confidence while the proposed B-PEFT reduces the ECE by almost an order of magnitude.
theoretical connection between fine-grained uncertainty and model accuracy [ 46] , we unveil the
underlying reason that supports the good predictive performance of a few-shot adapted foundation
model and the root cause for the under-confident behavior. Drawing from this important insight
as outlined above, we propose to integrate the modern PEFT techniques into a novel lightweight
Bayesian framework, referred to as Bayesian-PEFT (B-PEFT ), aiming to achieve highly reliable and
accurate few-shot adaptations with well-calibrated and trustworthy uncertainty quantification.
The proposed Bayesian framework offers two important components to address the under-confidence
issue while ensuring reliable prediction under challenging few-shot settings. The first component
makes novel adjustments to the base rates introduced by the SL theory to strengthen the prior belief
corresponding to the knowledge gained through pre-training. Meanwhile, the adjustment does not
change the relative order of the belief assigned to different classes, which ensures that the model
accuracy is maintained. Our theoretical analysis shows that the proposed base rate adjustment strategy
leads to more confident predictions (by increasing the gaps between the belief assigned to the ground-
truth class and the rest) without compromising the model’s accuracy. Figure 2d shows that B-PEFT
significantly improves the model calibration. To further enhance the reliability of both prediction
accuracy and uncertainty quantification when performing few-shot adaptation, the second component
performs Bayesian model averaging by building a diversity-inducing evidential ensemble. In addition
to using different random initialization of the PEFT components, diversity is further enhanced through
incorrect belief regularization that penalizes a model for assigning a high belief to a non-ground-truth
class. By controlling the strength of belief regularization, different ensemble components are guided
to learn from diverse features in the data space, where a light penalty allows an ensemble component
to learn the common discriminative features while a heavy one will force an ensemble component to
learn rare features to avoid errors on the difficult data samples. A deeper theoretical analysis of the
proposed diversity-induced evidential ensemble is equivalent to Stein Variational Gradient Descent
(SVGD) based ensembles [ 13]. Experiments on multiple challenging few-shot learning tasks justify
the superior performance of B-PEFT over state-of-the-art PEFT baselines, in terms of both prediction
accuracy and uncertainty calibration performance. Our contributions can be summarized as follows:
•We identify the severe under-confidence issue of pre-trained foundation models after perform-
ing parameter-efficient fine-tuning over few-shot datasets. The fine-grained uncertainty analysis
through evidential learning and SL theory reveals the root cause for their good predictive perfor-
mance while being under-confident.
•We develop a novel lightweight Bayesian framework that integrates state-of-the-art PEFT tech-
niques with two Bayesian components to address the under-confidence issue while ensuring reliable
prediction under challenging few-shot settings. The first component performs base rate adjustment
to strengthen the prior belief corresponding to the knowledge gained through pre-training, making
the model more confident in its predictions; the second component builds an evidential ensemble
that leverages belief regularization to ensure diversity among different ensemble components.
•We perform thorough theoretical analysis to justify why the proposed Bayesian components can
ensure reliable and accurate few-shot adaptations with well-calibrated uncertainty quantification.
•We carry out experiments with 4 benchmark datasets, 5 different few-shot settings, and 3 parameter
efficient fine-tuning techniques that demonstrate the effectiveness of the developed model.
2 Related Works
Parameter Efficient Fine Tuning of Foundation Models. Transformer-based foundation models
[63,15] have been developed as an improvement over traditional convolution-based architectures
3[29,31] for computer vision tasks. The transformer-based models achieve strong generalization
performance [ 40] after training on large datasets. Moreover, the pre-trained transformers can be
fine-tuned in limited data settings leading to state-of-the-art performance [ 32,27]. As a computation,
memory, and parameter-efficient alternative to full fine-tuning of such large pre-trained transformers,
different Parameter Efficient Fine Tuning (PEFT) approaches have been developed. PEFT techniques
freeze most of the large transformer backbone, fine-tune the remaining backbone parameters and/or
introduce lightweight extra modules for adapting to the downstream task. Existing approaches can
broadly be categorized as extra-module-based [ 72,54], partial-tuning-based [ 71,9], and visual prompt
tuning-based [68, 68, 28] methods. Extra-module-based methods ( e.g., Adapter [51], side-tune[72])
introduce small additional learnable modules and keep the pre-rained backbone frozen. Partial-tuning-
based methods ( e.g., Bias [ 9]) keep a large portion of the backbone frozen, and fine-tune only part of
the foundation model to downstream tasks. Visual prompt-based PEFT methods (VPT) introduce
a learnable prompt variable along with a learnable classification head over the fixed pre-trained
backbone to be adapted to the downstream task. VPT [ 32] has shown significant improvement over
other PEFT techniques, and can even outperform full fine-tuning in multiple datasets/settings [28].
Calibration in Deep Learning Models Calibration methods have been increasingly explored to
achieve trustworthy deep-learning models. Post-hoc calibration methods [ 26,43,73] aim to learn
a calibration map for a standard trained deep learning model such that the map can transform the
poorly calibrated probabilities to calibrated probabilities. Regularization-based calibration approaches
introduce explicit regularization (such as with L 2regularization [ 26], entropy regularization [ 50]),
or implicit regularization (such as with focal loss [ 39]) during training to ensure that the trained
model is calibrated. Data augmentation methods such as Label smoothing [ 44], and mixup training
[60][74] have also been explored for developing calibrated deep learning models. Recent survey [ 65]
provides a discussion of the most relevant works towards developing calibrated deep learning models.
Most existing calibration methods are designed to tackle the over-confidence issue, which is more
commonly observed for large models trained from limited data due to overfitting. We observe that
fine-tuned foundation models exhibit severe under-confidence in their predictions, where existing
calibration techniques are less effective. To this end, we propose a lightweight Bayesian framework
that fills this critical gap.
Few-Shot Adaptation and Relationship with Meta-Learning. In this work, we consider few-shot
adaption with a focus on N-way K-shot classification [ 64,22], where the model is presented with a
few-shot training set with N-class, each having Kexamples. For instance, 1-shot Cifar100 training
set consists of 1 sample from each of the 100 classes. The model is then evaluated on the test set,
which is identical to the query set in the meta-testing tasks [ 58]. It is worth to note that meta-learning
(e.g., matching networks [ 64], MAML [ 16], VERSA [ 23], PLATIPUS [ 17]) leverages an episodic
learning paradigm to achieve few-shot adaptation, where both meta-training and meta-testing are
done on the task level in an episodic fashion [ 69,37] with a large number of N-way K-shot training
tasks. In this work, we consider more challenging few-shot adaptation tasks ( e.g., 100-way 1-shot
in Cifar100 and 102-way 1-shot in Flowers102) compared to the commonly used 5-way 1-shot
meta-learning tasks. We leverage the power of the pre-trained foundation models, which eliminates
the need of task based episodic meta-training. From a meta-learning perspective, the pre-training
phase for the foundation model could be viewed as performing meta-knowledge acquisition similar
to meta-training. The pre-trained model can be seen as an expert equipped with the meta-knowledge,
and parameter-efficient fine-tuning performs quick adaptation to the downstream tasks, analogous to
the support-set based adaptation done in meta-testing.
3 Bayesian Parameter-Efficient Fine-Tuning of Foundation Models
We start by introducing some fundamental concepts from evidential learning, which will serve as
key building blocks in the proposed Bayesian-PEFT framework. We then detail the two Bayesian
components: base rate adjustment to address under-confidence and diversity-inducing evidential
ensemble to improve the reliability on both prediction accuracy and uncertainty quantification.
3.1 Preliminaries
Evidential Deep Learning (EDL) [ 56] introduces a computationally efficient framework to transform
deterministic deep learning (DL) models into uncertainty-aware models. The key idea is to introduce
a higher-order conjugate prior distribution over the predicted likelihood distribution and train the DL
model to output parameters of the higher-order distribution. Towards classification, EDL models
4[56,11] introduce Dirichlet prior distribution for the multinomial likelihood distribution. Specifically,
the output softmax layer of the DL model is replaced by a monotonic, non-negative transformation
function ( e.g.,ReLU, SoftPlus, or exp) to obtain the evidence for different classes that are transformed
into the Dirichlet parameters. Mathematically, for a DL model fθ(·), and an input sample x, we have
ei=E 
fθ(x)
iαi=ei+ai×W (1)
where eiis the output evidence for the ithclass from the model fθ(·)and input sample x,aiis
the base rate for the ithclass, Wis the non-informative prior weight usually set to the number of
classes, Eis the non-negative transformation function, and αiparameterizes a Dirichlet distribution.
Existing EDL works usually adopt a non-informative base rate of ai=1
N∀i∈[1, N]. Furthermore,
a multinomial distribution Mult(y|p)over labels is parameterized as E[pi] =αi
S,where the total
Dirichlet Strength S=PN
i=1αi.
An evidential model can be trained via a Type-II Maximum Likelihood-based evidential loss
LLog(x,y)[56] with KL regularization that penalizes evidence assigned to non-ground-truth classes:
Levid(x,y) =LLog(x,y) +λKL 
Dir(p|˜α)||Dir(p|1)
(2)
where ˜α=y+ (1−y)⊙α. Once trained, the evidential model can predict an evidence vector e=
(e1, e2, ...eN)⊤for a given test sample x. From the predicted evidence, we obtain the model’s belief
(b) over different classes, the correct belief bcor, incorrect belief binc, and vacuity uas
b=e
S, b cor=X
y⊙b, b inc=X
(1−y)⊙b, u =N
S, (3)
where vacuity uis a second-order uncertainty [ 33] that captures the model’s lack of knowledge in its
prediction; bcorandbincquantify model accuracy and error, respectively. However, neither bcor
norbinccan be evaluated without the ground-truth label, which is not available in the testing phase.
Existing theoretical work has established an important connection between bincand dissonance [ 46],
which is another second-order uncertainty [ 33] that can be quantified without the ground-truth label.
More specifically, dissonance discan be evaluated as
dis=NX
n=1
bnP
j̸=nbjBal(bj, bn)P
j̸=nbj
, Bal (bj, bn) =(
2min(bj,bn)
bj+bn,ifbibj>0
0, otherwise(4)
where Bal(·,·)is the relative mass balance function between two belief masses. The dissonance
essentially captures the conflicting belief assigned to different classes [57].
3.2 Strengthening the prior belief through base rate adjustment
To gain a deeper understanding of the under-confident few-shot adaptation behavior of foundation
models, we perform fine-grained uncertainty analysis using the predicted evidence from an evidential
model. To this end, we replace the softmax layer in a VPT fine-tuned transformer model with
an exponential-based evidential head that outputs non-negative evidence for different classes. We
analyze the output evidence from the evidential model that reveals some interesting insights.
Why is the model accurate? First, we observe that the relative order of the evidence assigned to
different classes is accurate. This implies that the model outputs relatively greater evidence for the
correct class compared to all other classes that ensure the model’s strong predictive performance. To
more precisely quantify the model’s accuracy, we adapt the lower bound of incorrect belief theorem
developed for meta-learning [46] to evaluate the model accuracy through its predicted dissonance:
Theorem 1. Consider an evidential model that outputs incorrect belief of bincand the dissonance in
the beliefs is dis. Then, the incorrect belief of the model will be at least half of the dissonance for all
predictions from the evidential model.
1
2dis≤binc where 0≤dis≤1 & 0 ≤binc≤1 (5)
Figure 3a shows the test accuracy vs. dissonance curve, which is aligned with the relationship
between the incorrect belief and the dissonance given in the theorem above, where a low dissonance
implies a low binc(or a high accuracy). From all the testing samples, we observe relatively low
dissonance and the highest is only slightly above 0.7 as shown in the figure. We further evaluate the
Area Under the Curve of the Accuracy vs. (1−dis)and obtain an AUC of 0.82 as shown in Figure
3b. This implies the model is able to clearly discriminate the ground-truth label from the rest without
much confusion ( i.e.,low dissonance) which ensures its good prediction accuracy.
5(a) Dissonance-Accuracy
 (b) AUC Curve
0 20 40 60 80 100
Class ID012345EvidenceConfidence: 0.025, Vacuity: 0.875
Ground Truth Class (c) Evidence of classes
0.0 0.2 0.4 0.6 0.8 1.0
Vacuity02000400060008000Number of SamplesVacuity Count (d) Vacuity Distribution
Figure 3: 1-shot Cifar10 results and evidence vacuity trends
Why is the model under-confident? Despite being able to assign relatively more evidence to the
correct label over the rest, it is also interesting to observe that the model generally assigns very low
evidence to all the labels, including the correct one. Figure 3c shows the evidence distribution of one
representative test data sample from Cifar100. As can be seen, most classes are assigned very low
evidence that is close to zero. The ground-truth class is assigned higher evidence, but it is far from
sufficient to make the prediction confident. The resultant confidence is only 0.025 while the vacuity
is extremely high at 0.875, implying that the model believes it has very limited knowledge of the
data sample despite it correctly identifying the correct label. Figure 3d shows the predicted vacuity
over all the test samples, most of which are assigned a very high vacuity. This confirms the overly
conservative behavior of the model, where the low confidence is primarily due to the insufficient
allocation of evidence in its predictions. On the other hand, since the model is fairly accurate, it
is reasonable to believe that the model underestimates the contribution of the rich prior knowledge
gained through pre-training.
Base rate adjustment to strength the prior belief. The fine-grained uncertainty analysis through
the lenses of evidential learning not only explains the good predictive performance of the few-shot
adapted model through PEFT but also unveils the root cause for its under-confident behavior, which is
under-estimation of the contribution from the prior knowledge to the downstream task. While the
classical Bayes’ theorem offers a principal idea to address the issue, which is to strengthen the prior
belief, there is a lack of practical way to achieve this. To this end, we propose to leverage the base rate
introduced by the subjective logic theory [ 33] as an effective vehicle to adjust the prior belief gained
through pre-training. According to (1), adjusting the base rate has the effect of changing the Dirichlet
parameter α, which will change the confidence for the prediction given by max iE[pi]. However, base
rate adjustment needs to meet two key requirements: (1) the relative order of the Dirichlet parameters
assigned to different classes should be preserved so that the predictive performance of the model
remains unaffected, (2) the gap between the Dirichlet parameters for different classes is transformed
such that the model becomes more confident in its predictions, making it well-calibrated. To meet
these requirements, we we propose a transformation function Amto the model’s output evidence
such that the model is well calibrated without any compromise in the generalization performance:
α=Am 
fθ(xi)
=e+Wχ, χ i=aadj
i=ei−emin
eminm
(6)
where χ= (χ1, χ2, ...χ N)⊤is the adjusted base rate, and m≥1controls the base rate transformation.
The adjusted base rate χconsiders evidence of all classes as a reference via emin, and transforms the
gap between different class evidences such that the model is well calibrated.
Lemma 2. The base-rate adjusted model that uses learnable base rate χ= (χ1, χ2, ..., χ N)⊤has the
same generalization performance compared to the model using fixed base rate of ai=1
N∀i∈[1, N]
Theorem 3. For any m≥1, the transformation function Amtransforms the base rate for the class
with the highest evidence emaxand class with the second highest evidence e2ndsuch that the gap in
Dirichlet parameters between the two classes is non-decreasing.
Remark. Theorem 3 ensures that the expected probability E[pi]for the predicted class ihas an
increased gap with the rest of the classes, which results in an increase of the model’s confidence.
Therefore, if the prediction is accurate, the model’s calibration performance will be improved.
Meanwhile, Lemma 2 ensures that the good prediction accuracy of the model is maintained by the
proposed base rate adjustment strategy. The detailed proofs are given in Appendix D.
3.3 Building A Diversity Induced Evidential Ensemble
The second Bayesian component of the proposed B-PEFT framework aims to further improve the
reliability of both prediction accuracy and uncertainty quantification when performing few-shot adap-
6DirichletParametersDirichletParametersDirichletParametersBase-rateadjustmentBase-rateadjustmentBase-rateadjustment
EnsemblePredictionsAccuracyECE
Input
Observed VariableLatent Variable
(a)(b)
Ensemble E
Ensemble 2
Ensemble 1Figure 4: (a) Schematic diagram and (b) Graphical model of the B-PEFT model
tation. It performs Bayesian model averaging by building a diversity-inducing evidential ensemble.
The ensemble of deep learning models ( i.e.,deep ensemble) [ 20,36] can effectively improve the gen-
eralization performance of deep learning models. Moreover, deep ensembles can capture the model
uncertainty [ 36,53] via the agreement-disagreement between the ensemble components. Model
uncertainty essentially captures the uncertainty in the model parameters, which is denoted as θof the
graphical model of B-PEFT as shown in Figure 4(b). The schematic diagram of B-PEFT is shown
in Figure 4(a). The model uncertainty can be leveraged to evaluate the reliability of fine-grained
uncertainty output by the evidential model.
The effectiveness of the ensembles has been empirically demonstrated across multiple datasets/settings
[30] with theoretical guarantees [ 2]. However, standard deep ensembling leads to limited diversity
among the ensemble components as it only considers the random initialization of components. We
propose a novel diversity-inducing ensembling scheme for the evidential models. Similar to the
deep ensemble [ 36], we also consider randomly initialized evidential models. We additionally train
each ensemble component with different strengths for incorrect evidence regularization along with
evidential loss objective. The overall objective for each ensemble component is identical to (2).
However, each ensemble component is trained with different incorrect evidence (or belief) regulariza-
tion strengths ( i.e.,different components place different priorities for the minimization of incorrect
evidence over the maximization of correct evidence) which leads to diversity among the components.
Since each component’s priority for minimizing the incorrect evidence is different, the components
focus on different attributes/features in the data that help the model avoid overfitting to an identical
set of discriminative features. As a result, the proposed evidential ensembling scheme implicitly
pushes the ensemble components away from each other, making it equivalent to the repulsive force in
the Stein Variational Gradient Descent (SVGD) [12, 13].
Lemma 4. For given incorrect evidence regularization Linc
reg, and E ensemble components with
regularization strengths λp, p∈[1, P], the ensemble components in the evidence space are implicitly
pushed away from each other by a force λp∇Linc
regthat acts identical to the repulsive force in Stein
Variational Gradient Descent (SVGD) based ensembles.
Figure 5: Illustration of ensemble diversity achieved through
incorrect belief regularization with different strengthRemark. The detailed proof is
given in the Appendix. We
present an intuitive visualization
of the update of the eviden-
tial ensemble model for different
strengths ( λ1< λ 2< ... < λ P)
of incorrect evidence regulariza-
tion for different seeds in Figure 5.
Each ensemble component aims
to maximize the likelihood (direc-
tion− →A) and minimize incorrect
7Table 1: Prediction accuracy and ECE performance on few-shot adaptation
K (Shot)Cifar10 Cifar100 Food101 Flowers102
Accuracy ↑ ECE↓ Accuracy ↑ ECE↓ Accuracy ↑ ECE↓ Accuracy ↑ ECE↓
(a) Standard Model
1-Shot 69.578±1.3510.437±0.01048.637±0.7570.393±0.00835.702±1.0950.263±0.009 88.161±0.910.61±0.004
2-Shot 81.771±1.3330.400±0.01664.501±0.3030.494±0.00253.954±0.659 0.39±0.004 93.462±1.072 0.55±0.006
5-Shot 88.707±0.4230.255±0.00876.758±0.5250.517±0.00165.586±0.1970.424±0.00297.363±0.1650.472±0.013
10-Shot 91.061±0.2170.212±0.00580.720±0.3290.501±0.00371.566±0.0690.444±0.00398.244±0.1140.439±0.018
20-Shot 92.678±0.370.166±0.00482.608±0.2660.487±0.00474.914±0.1780.460±0.00398.431±0.1000.425±0.017
(b) Evidential Model
1-Shot 70.197±1.0130.557±0.01151.127±0.4350.499±0.00436.297±1.4070.349±0.014 89.225±1.030.846±0.004
2-Shot 81.613±1.736 0.553±0.0165.545±0.3390.620±0.00452.855±0.5510.485±0.00595.071±0.4130.874±0.006
5-Shot 88.764±0.8960.391±0.01577.561±0.7160.744±0.006 65.135±0.270.536±0.00597.602±0.199 0.686±0.02
10-Shot 92.014±0.3530.388±0.00681.561±0.2910.765±0.00270.863±0.2610.673±0.00398.326±0.2330.444±0.008
20-Shot 93.029±0.2390.360±0.01583.100±0.1840.782±0.00172.060±0.3090.599±0.00398.708±0.0140.411±0.013
(c) Base-rate adjusted Evidential Model (Calibrated Evidential Model)
1-Shot 70.197±1.0130.027±0.00251.127±0.4350.077±0.00436.297±1.4070.081±0.011 89.225±1.030.025±0.004
2-Shot 81.613±1.7360.040±0.01365.545±0.339 0.08±0.003 52.855±0.5510.063±0.00695.071±0.4130.023±0.003
5-Shot 88.764±0.8960.028±0.00677.561±0.7160.044±0.00265.135±0.2700.037±0.00397.602±0.1990.015±0.002
10-Shot 92.014±0.3530.019±0.00181.561±0.2910.034±0.00270.863±0.2610.054±0.00298.326±0.2330.023±0.003
20-Shot 93.029±0.2390.016±0.00283.100±0.1840.031±0.00172.060±0.3090.050±0.00298.708±0.0140.021±0.000
(d)B-PEFT Model (Ours)
1-Shot 74.674±0.9680.024±0.00252.335±0.6100.067±0.00138.745±0.1840.021±0.00190.238±0.1010.023±0.001
2-Shot 83.865±0.7350.022±0.00267.563±0.2720.056±0.00154.661±0.0170.020±0.00195.715±0.0200.021±0.002
5-Shot 90.556±0.1600.017±0.00180.081±0.0670.036±0.00066.548±0.1100.034±0.00197.807±0.0660.014±0.002
10-Shot 92.956±0.0860.014±0.00083.038±0.0450.031±0.00071.661±0.2120.038±0.00298.050±0.0410.011±0.001
20-Shot 93.833±0.0210.014±0.00183.748±0.0650.030±0.00175.495±0.1280.043±0.00198.193±0.0200.010±0.001
evidence (direction− →B). The strengths of incorrect evidence regularization (direction− →B) are different
for each ensemble component that acts as an implicit repulsive force among the ensemble components,
ensuring that they are diverse from each other. Different from the SVGD-based ensemble, in our
proposed model, the particles do not need to explicitly communicate with each other making our
proposed approach computationally efficient, scalable, and generalizable.
4 Experiments and Results
Experiment setup, datasets, and baselines. We consider K-shot adaptation ( i.e.,the dataset has
Kexamples per class in the training set) with Cifar10 [ 1], Cifar100 [ 1], Food101 [ 8], and Flowers102
[45] datasets. For instance, the 2-shot Cifar100 dataset has 2examples per class leading to a total of
200 labeled training samples. For all datasets and experiments, the training set is a few-shot dataset,
and the evaluation is done on the standard test set available with benchmark datasets. Details of the
few-shot training datasets along with additional experiment details are presented in the Appendix E.
We consider large pre-trained vision transformer with ViT backbone [ 15] and consider Visual Prompt
Tuning (VPT) [ 32], along with bias fine-tuning [ 9] and adapter fine-tuning [ 72] as the PEFT techniques
(We use VPT as the representative PEFT where not specified due to its superior performance). We
consider accuracy-preserving post-hoc calibration techniques including Temperature Scaling (TS)
[26], Parameterized Temperature Scaling (PTS) [ 61], and Isotonic Regression (IR-MC) [ 6] as the
baseline calibration techniques.
Prediction and calibration performance. We first consider standard cross-entropy (CE)-based
PEFT of the supervised pre-trained ViT model on few-shot datasets. We present the accuracy and
calibration results of VPT in Table 1 (a). We observe that the straightforward adaption of the models
leads to accurate but under-confident models as indicated by a high ECE. The evidential models as
shown in Table 1 (b) have comparable or better generalization performance across the datasets/settings.
However, these models are also severely under-confident similar to CE-based models indicated by
high ECE and accuracy-confidence trends (see Figure 11a, 11b in the Appendix). The overall
performance of the calibrated evidential model using base-rate adjustment is presented in Table 1 (c).
As can be seen, the accuracy remains the same as the adjusted base rate expands the gap between
evidence of the class and preserves the relative order in the predicted class evidence. It effectively
tackles the under-confidence issue, which leads to a significant improvement in the overall ECE
performance across the datasets and settings (also see Figure 11c in the Appendix). Table 1 (d) shows
8the results of the proposed B-PEFT model that introduces a diversity-enforcing ensemble of calibrated
evidential models trained with different strengths of incorrect evidence regularization. performance.
Table 2: Adapter and bias fine tuning results
K (Shot)Bias Adapter
Accuracy ↑ ECE↓ Accuracy ↑ ECE↓
(a) Standard Model
1-Shot 35.514±2.4200.296±0.02346.150±1.1500.386±0.010
2-Shot 55.098±4.9320.384±0.03666.789±0.5140.513±0.003
5-Shot 74.203±0.4670.383±0.00278.738±0.0320.503±0.000
10-Shot 79.141±0.2330.336±0.00281.589±0.0310.470±0.000
(b) Evidential Model
1-Shot 36.243±4.1130.3498±0.04147.391±1.4210.463±0.014
2-Shot 58.258±3.8840.516±0.03267.523±0.6740.654±0.006
5-Shot 75.643±0.6980.509±0.00679.875±0.0510.670±0.001
10-Shot 80.158±0.2840.454±0.00182.674±0.0440.731±0.001
(c) Base-rate adjusted Evidential Model
1-Shot 36.243±4.1130.061±0.01147.391±1.4210.081±0.005
2-Shot 58.258±3.8840.077±0.00467.523±0.6740.070±0.001
5-Shot 75.643±0.6980.069±0.00279.875±0.0510.057±0.000
10-Shot 80.158±0.2840.063±0.00182.674±0.0440.052±0.001
(d)B-PEFT Model (Ours)
1-Shot 37.825±0.3440.050±0.00248.732±0.2250.076±0.002
2-Shot 62.796±1.0800.065±0.00569.187±0.1530.068±0.002
5-Shot 77.181±0.1950.062±0.00179.918±0.0100.051±0.001
10-Shot 80.788±0.0640.059±0.00882.748±0.0160.049±0.001Such a learning signal helps the model avoid
overfitting (addressing the potential overcon-
fidence issue) and leads to further improve-
ment in generalization and the calibration.
We further carry out experiments with addi-
tional PEFT techniques of bias and adapter
fine-tuning and on the K-shot Cifar100
dataset in Table 2. We observe that these
PEFT techniques lead to a lower generaliza-
tion performance (as indicated by a lower
test accuracy) compared to the VPT-based
technique. Nevertheless, the same under-
confidence issue remains as shown in Table 2
(a) for standard cross-entropy trained model
performance, and in Table 2 (b) for their ev-
idential extensions. We then carry out ex-
periments using base-rate adjusted evidential
model and our proposed B-PEFT model. The
results are shown in Table 2 (c) and Table
2 (d). Base-rate adjusted evidential model
andB-PEFT are equally effective with these
PEFT techniques, and they address the under-
confidence issue, which leads to improve-
ment in both generalization and calibration.
Table 3: ECE performance comparison
Model 1Shot 2Shot 5Shot 10Shot
CE Model [32] 0.393 0 .494 0 .517 0 .501
Evidential Model [56] 0.499 0 .620 0 .744 0 .765
TS [26] 0.092 0 .074 0 .043 0 .036
PTS [61] 0.145 0 .129 0 .096 0 .083
IR-MC [6] 0.091 0 .104 0 .103 0 .085
BR-Evid (Ours) 0.077 0 .080 0 ,044 0 .034
B-PEFT (Ours) 0.067 0 .056 0 .036 0 .031Comparison with existing calibration tech-
niques. We compare our proposed eviden-
tial calibra tion technique with existing calibra-
tion techniques using a representative Cifar100
dataset on different few-shot classification set-
tings. We consider Isotonic Regression (IR-
MC)[ 6], Temperature Scaling (TS) [ 26], and
Parameterized Temperature Scaling (PTS) [ 61]
as baselines. Overall results are presented in
Table 3. The baseline model and its evidential
extension are poorly calibrated. All calibration
techniques transform the logits to address the under-confidence issue of the model. Our proposed
Base-rate adjusted Evidential model (BR-Evid) addresses the under-confidence issue in the evidential
model leading to significant improvement in the ECE. B-PEFT model further improves on the BR-
Evid model as it effectively addresses both the under-confidence issue (via the base rate adjustment)
and overconfidence issue (via the Bayesian ensembling) that leads to superior calibration results as
indicated by the lowest ECE in all few-shot settings.
Uncertainty quantification results. We investigate the uncertainty quantification performance of
our proposed calibrated evidential ensemble model. The developed model can reflect the model uncer-
tainty ( i.e.,the model’s confidence in its predictions) through the ensemble agreement/disagreement
and the distributional uncertainty through the sharpness of evidential prior distribution ( i.e.,the
vacuity). For the analysis, we use Cifar10 as in-distribution (ID) dataset for different shots, and
Cifar100 as OOD dataset. As shown in Figure 6, we plot vacuity and variance distribution for 1
and 5 shots. A single model outputs the vacuity that can be used to detect OOD samples. We can
see ID samples are in the lower vacuity region and OOD samples are in the higher vacuity region.
As the number of shots increases, the region becomes more separate. However, for 1 shot, there
are some OOD samples in the lower vacuity region as well. Since we can not trust vacuity alone
for uncertainty, we utilize the variance of ensemble components to quantify model uncertainty. As
we see in Figure 6(c), the variance of OOD samples mostly lies in the high variance region. As we
increase the number of shots, the variance shifts towards the lower region. High variance indicates
that model-predicted vacuity can not be totally trusted. This behavior is qualitatively shown in Figure
90.000 0.200 0.400 0.600 0.800 1.000
Vacuity010002000300040005000Number of Samples1 shot In-Distribution:Cifar10
In Distribution: Correct
In-Distribution: Incorrect
OOD: Cifar100(a) Vacuity for 1 shot
0.000 0.200 0.400 0.600 0.800 1.000
Vacuity010002000300040005000600070008000Number of Samples5 shot In-Distribution:Cifar10
In Distribution: Correct
In-Distribution: Incorrect
OOD: Cifar100 (b) Vacuity for 5 shot
0.0000.0050.0100.0150.0200.0250.0300.035
Variance0100020003000400050006000Number of Samples1 shot In-Distribution:Cifar10
In Distribution: Correct
In-Distribution: Incorrect
OOD: Cifar100 (c) Variance for 1 shot
0.000 0.010 0.020 0.030 0.040
Variance010002000300040005000600070008000Number of Samples5 shot In-Distribution:Cifar10
In Distribution: Correct
In-Distribution: Incorrect
OOD: Cifar100 (d) Variance for 5 shot
Figure 6: (a-b): Vacuity distribution of a single model and (c-d): variance distribution of ensemble
models for 1/5 shots cifar10 as In-Distribution and Cifar100 as Out-of-Distribution dataset
Component 1 vacuity:  0.379
Component 2 vacuity:  0.811
Component 3 vacuity:  0.587
Ensemble Variance:  0.013Component 1 vacuity:  0.308
Component 2 vacuity:  0.780
Component 3 vacuity:  0.601
Ensemble Variance:  0.018Component 1 vacuity:  0.857
Component 2 vacuity:  0.882
Component 3 vacuity:  0.868
Ensemble Variance:  0.002
(a) High Variance (b) High Variance (c) Low Variance
Figure 7: Qualitative analysis of OOD samples for 1-shot adaptation of Cifar10 as ID dataset and
Cifar100 as OOD dataset
7. We observe that when only a single component outputs a low vacuity for the OOD samples in
Figure 7 (a-b), the variance of the ensemble is high, implying a high model uncertainty. When all
the components output a high vacuity to the OOD sample in Figure 7 (c), the variance is also low,
implying a low model uncertainty.
0.5 1.0 1.5 2.0 2.5 3.0
m0.100.150.200.250.300.350.400.45ECE
1 shot Cifar100
kl=0.0
kl=0.1
kl=1.0
kl=10.0
kl=100.0
kl=1000.0
Figure 8: Impact of mAblation study. We carry out ablation with 1-shot Cifar100
dataset to study the impact of the base rate transformation order
m(Section 3.2) for different strengths of incorrect evidence reg-
ularization on the calibration performance. As we increase m,
the probability gap between classes improves, leading to more
confident predictions. However, the model starts to become
overconfident for large mvalues (see Figure 8).Moreover, with
an increase in incorrect evidence regularization strength, the
optimal mvalue decreases to a smaller value ( e.g., optimal m
= 2.0 for λ= 0.1, and optimal m= 1.5forλ= 10). Choosing
a proper mleads to the best-calibrated evidential model. We
present additional results studying the impact of min Appendix F.1.
Limited by space, we provide results comparing meta-learning methods on standard few-shot tasks
in Appendix F.4, discuss impact of various components (number of classes, data size, and unfrozen
parameters) in Appendix F.5, and include additional experiments and comparisons, including OOD
settings, in Appendix F.6. Moreover, we carry out additional ablation experiments to study the impact
of incorrect evidence regularization strength (Appendix F.2), and the impact of ensemble components
(Appendix F.3). We further carry out experiments and discuss applying PEFT to foundation models
pre-trained in a self-supervised fashion (Appendix G). We discuss the societal impact and limitations
of the work in Appendices H and I, respectively.
5 Conclusion
In this work, we focus on transformer-based large vision foundation models and investigate different
parameter-efficient fine-tuning techniques for effective few-shot adaptation. We observe that the
existing models are severely under-confident, especially in challenging datasets and settings. More-
over, existing models lack fine-grained uncertainty quantification capabilities. We extend the models
to uncertainty-aware evidential models, and resort to the evidential framework to develop a novel
Bayesian parameter efficient fine-tuning ( B-PEFT ) framework that integrates evidence-based base rate
adjustment to addresses the under-confidence and a diversity inducing evidential ensemble technique
to further improve the reliability in model prediction and uncertainty quantification. The B-PEFT
framework possesses theoretically sound properties to ensure its superior generalization capability
and robust calibration behavior. We carry out intensive experiments across different benchmark
datasets and diverse few-shot settings that demonstrate the outstanding performance of B-PEFT .
10Acknowledgments
This research was supported in part by an NSF IIS award IIS-1814450. The views and conclusions
contained in this paper are those of the authors and should not be interpreted as representing any
funding agency. We would like to thank the anonymous reviewers for their constructive comments.
References
[1]Krizhevsky Alex. Learning multiple layers of features from tiny images. https://www. cs.
toronto. edu/kriz/learning-features-2009-TR. pdf , 2009.
[2]Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation
and self-distillation in deep learning. arXiv preprint arXiv:2012.09816 , 2020.
[3]Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential
regression. Advances in Neural Information Processing Systems , 33:14927–14937, 2020.
[4]Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls
of in-domain uncertainty estimation and ensembling in deep learning. arXiv preprint
arXiv:2002.06470 , 2020.
[5]Wentao Bao, Qi Yu, and Yu Kong. Evidential deep learning for open set action recognition.
InProceedings of the IEEE/CVF International Conference on Computer Vision , pages 13349–
13358, 2021.
[6]Eugene Berta, Francis Bach, and Michael Jordan. Classifier calibration with roc-regularized
isotonic regression. In International Conference on Artificial Intelligence and Statistics , pages
1972–1980. PMLR, 2024.
[7]Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty
in neural network. In International conference on machine learning , pages 1613–1622. PMLR,
2015.
[8]Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative
components with random forests. In European Conference on Computer Vision , 2014.
[9]Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for
efficient on-device learning. Advances in Neural Information Processing Systems , 33:11285–
11297, 2020.
[10] Bertrand Charpentier, Oliver Borchert, Daniel Zügner, Simon Geisler, and Stephan Günne-
mann. Natural posterior network: Deep bayesian predictive uncertainty for exponential family
distributions. In International Conference on Learning Representations , 2022.
[11] Bertrand Charpentier, Daniel Zügner, and Stephan Günnemann. Posterior network: Uncer-
tainty estimation without ood samples via density-based pseudo-counts. Advances in Neural
Information Processing Systems , 33:1356–1367, 2020.
[12] Francesco D’Angelo and Vincent Fortuin. Repulsive deep ensembles are bayesian. Advances in
Neural Information Processing Systems , 34:3451–3465, 2021.
[13] Francesco D’Angelo, Vincent Fortuin, and Florian Wenzel. On stein variational neural network
ensembles. arXiv preprint arXiv:2106.10760 , 2021.
[14] Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer,
and Philipp Hennig. Laplace redux-effortless bayesian deep learning. Advances in Neural
Information Processing Systems , 34:20089–20103, 2021.
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020.
11[16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70 , pages 1126–1135. JMLR. org, 2017.
[17] Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In
Advances in Neural Information Processing Systems , pages 9516–9527, 2018.
[18] Gianni Franchi, Olivier Laurent, Maxence Leguéry, Andrei Bursuc, Andrea Pilzer, and Angela
Yao. Make me a bnn: A simple strategy for estimating bayesian uncertainty from pre-trained
models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pages 12194–12204, 2024.
[19] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning , pages 1050–1059.
PMLR, 2016.
[20] Mudasir A Ganaie, Minghui Hu, Ashwani Kumar Malik, Muhammad Tanveer, and Ponnuthu-
rai N Suganthan. Ensemble deep learning: A review. Engineering Applications of Artificial
Intelligence , 115:105151, 2022.
[21] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li,
and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International
Journal of Computer Vision , 132(2):581–595, 2024.
[22] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting.
InProceedings of the IEEE conference on computer vision and pattern recognition , pages
4367–4375, 2018.
[23] Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E Turner.
Meta-learning probabilistic inference for prediction. arXiv preprint arXiv:1805.09921 , 2018.
[24] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting
gradient-based meta-learning as hierarchical bayes. In International Conference on Learning
Representations , 2018.
[25] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. Ppt: Pre-trained prompt tuning for
few-shot learning. arXiv preprint arXiv:2109.04332 , 2021.
[26] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International conference on machine learning , pages 1321–1330. PMLR, 2017.
[27] Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi, and Dongfang
Liu. Eˆ 2vpt: An effective and efficient approach for visual prompt tuning. arXiv preprint
arXiv:2307.13770 , 2023.
[28] Cheng Han, Qifan Wang, Yiming Cui, Wenguan Wang, Lifu Huang, Siyuan Qi, and Dongfang
Liu. Facing the elephant in the room: Visual prompt tuning or full finetuning? arXiv preprint
arXiv:2401.12902 , 2024.
[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
[30] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger.
Snapshot ensembles: Train 1, get m for free. arXiv preprint arXiv:1704.00109 , 2017.
[31] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 4700–4708, 2017.
[32] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan,
and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision , pages
709–727. Springer, 2022.
12[33] Audun Jøsang. Subjective logic , volume 3. Springer, 2016.
[34] Audun Josang, Jin-Hee Cho, and Feng Chen. Uncertainty characteristics of subjective opinions.
In2018 21st International Conference on Information Fusion (FUSION) , pages 1998–2005.
IEEE, 2018.
[35] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset,
Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images
dataset v4: Unified image classification, object detection, and visual relationship detection at
scale. International journal of computer vision , 128(7):1956–1981, 2020.
[36] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. Advances in neural information
processing systems , 30, 2017.
[37] Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang, and
Sung Ju Hwang. Learning to balance: Bayesian meta-learning for imbalanced and out-of-
distribution tasks. In Eighth International Conference on Learning Representations, ICLR 2020 .
ICLR, 2020.
[38] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient
prompt tuning. arXiv preprint arXiv:2104.08691 , 2021.
[39] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense
object detection. In Proceedings of the IEEE international conference on computer vision ,
pages 2980–2988, 2017.
[40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
of the IEEE/CVF international conference on computer vision , pages 10012–10022, 2021.
[41] Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. Advances
in neural information processing systems , 31, 2018.
[42] Aryan Mobiny, Pengyu Yuan, Supratik K Moulik, Naveen Garg, Carol C Wu, and Hien
Van Nguyen. Dropconnect is effective in modeling uncertainty of bayesian deep networks.
Scientific reports , 11(1):5458, 2021.
[43] Azadeh Sadat Mozafari, Hugo Siqueira Gomes, Wilson Leão, Steeven Janny, and Christian
Gagné. Attended temperature scaling: a practical approach for calibrating deep neural networks.
arXiv preprint arXiv:1810.11586 , 2018.
[44] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help?
Advances in neural information processing systems , 32, 2019.
[45] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large
number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image
processing , pages 722–729. IEEE, 2008.
[46] Deep Shankar Pandey and Qi Yu. Multidimensional belief quantification for label-efficient
meta-learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 14391–14400, June 2022.
[47] Deep Shankar Pandey and Qi Yu. Evidential conditional neural processes. In Proceedings of
the AAAI Conference on Artificial Intelligence , volume 37, pages 9389–9397, 2023.
[48] Deep Shankar Pandey and Qi Yu. Learn to accumulate evidence from all training samples:
Theory and practice. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference
on Machine Learning , volume 202 of Proceedings of Machine Learning Research , pages
26963–26989. PMLR, 23–29 Jul 2023.
[49] Tim Pearce, Felix Leibfried, and Alexandra Brintrup. Uncertainty in neural networks: Approxi-
mately bayesian ensembling. In International conference on artificial intelligence and statistics ,
pages 234–244. PMLR, 2020.
13[50] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Reg-
ularizing neural networks by penalizing confident output distributions. arXiv preprint
arXiv:1701.06548 , 2017.
[51] Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vuli ´c, Sebastian Ruder,
Kyunghyun Cho, and Iryna Gurevych. Adapterhub: A framework for adapting transformers.
arXiv preprint arXiv:2007.07779 , 2020.
[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning ,
pages 8748–8763. PMLR, 2021.
[53] Rahul Rahaman et al. Uncertainty quantification and deep ensembles. Advances in Neural
Information Processing Systems , 34:20063–20075, 2021.
[54] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains
with residual adapters. Advances in neural information processing systems , 30, 2017.
[55] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining
for the masses. arXiv preprint arXiv:2104.10972 , 2021.
[56] Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify
classification uncertainty. Advances in neural information processing systems , 31, 2018.
[57] Weishi Shi, Xujiang Zhao, Feng Chen, and Qi Yu. Multifaceted uncertainty estimation for
label-efficient deep learning. Advances in neural information processing systems , 33, 2020.
[58] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
editors, Advances in Neural Information Processing Systems , volume 30. Curran Associates,
Inc., 2017.
[59] Zhao Song, Ke Yang, Naiyang Guan, Junjie Zhu, Peng Qiao, and Qingyong Hu. Vppt: Visual
pre-trained prompt tuning framework for few-shot image classification. In ICASSP 2023 - 2023
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages
1–5, 2023.
[60] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah
Michalak. On mixup training: Improved calibration and predictive uncertainty for deep neural
networks. Advances in Neural Information Processing Systems , 32, 2019.
[61] Christian Tomani, Daniel Cremers, and Florian Buettner. Parameterized temperature scaling for
boosting the expressive power in post-hoc uncertainty calibration. In In European Conference
on Computer Vision (ECCV) , 2022.
[62] Dennis Ulmer. A survey on evidential deep learning for single-pass uncertainty estimation.
arXiv preprint arXiv:2110.03051 , 2021.
[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
[64] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks
for one shot learning. Advances in neural information processing systems , 29, 2016.
[65] Cheng Wang. Calibration in deep learning: A survey of the state-of-the-art. arXiv preprint
arXiv:2308.01222 , 2023.
[66] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang,
Yonghong Tian, and Wen Gao. Large-scale multi-modal pre-trained models: A comprehensive
survey. Machine Intelligence Research , 20(4):447–482, 2023.
14[67] Jingyuan Wen, Yutian Luo, Nanyi Fei, Guoxing Yang, Zhiwu Lu, Hao Jiang, Jie Jiang, and
Zhao Cao. Visual prompt tuning for few-shot text classification. In Proceedings of the 29th
International Conference on Computational Linguistics , pages 5560–5570, 2022.
[68] Liqi Yan, Cheng Han, Zenglin Xu, Dongfang Liu, and Qifan Wang. Prompt learns prompt: ex-
ploring knowledge-aware generative prompt collaboration for video captioning. In Proceedings
of international joint conference on artificial intelligence (IJCAI) , pages 1622–1630, 2023.
[69] Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. In Advances in Neural Information Processing Systems ,
pages 7332–7342, 2018.
[70] Yu Yu, Chao-Han Huck Yang, Jari Kolehmainen, Prashanth G Shivakumar, Yile Gu, Sungho
Ryu Roger Ren, Qi Luo, Aditya Gourav, I-Fan Chen, Yi-Chieh Liu, et al. Low-rank adaptation
of large language model rescoring for parameter-efficient speech recognition. In 2023 IEEE
Automatic Speech Recognition and Understanding Workshop (ASRU) , pages 1–8. IEEE, 2023.
[71] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient
fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199 ,
2021.
[72] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning:
a baseline for network adaptation via additive side networks. In Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16 , pages
698–714. Springer, 2020.
[73] Jize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. Mix-n-match: Ensemble and compositional
methods for uncertainty calibration in deep learning. In International conference on machine
learning , pages 11117–11128. PMLR, 2020.
[74] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves
calibration. In International Conference on Machine Learning , pages 26135–26160. PMLR,
2022.
[75] Xujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. Uncertainty aware semi-supervised
learning on graph data. Advances in Neural Information Processing Systems , 33:12827–12836,
2020.
[76] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning
for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 16816–16825, 2022.
[77] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for
vision-language models. International Journal of Computer Vision , 130(9):2337–2348, 2022.
15Supplementary Material
Appendix
Table of Contents
A Organization of the Appendix 17
B Summary of the Symbols 17
C Further Discussion on Uncertainty-Aware Deep Learning 17
C.1 Existing Uncertainty Quantification Methods in Deep Learning . . . . . . . . . 17
C.2 Evidential Deep Learning Models for Classification . . . . . . . . . . . . . . . 18
C.3 Evidential models vs. Standard Bayesian Models . . . . . . . . . . . . . . . . . 19
D Proofs of Theoretical Results 19
D.1 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
D.2 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
D.3 Connection with SVGD-based Bayesian Ensembling and Proof of Lemma 4 . . . 20
E Dataset and Implementation Details 21
F Additional Experiments 22
F.1 Impact of mon Expected Calibration Error . . . . . . . . . . . . . . . . . . . . 22
F.2 Impact of Incorrect Evidence Regularization Strength ( λ) . . . . . . . . . . . . 25
F.3 Impact of Ensemble Components . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.4 Few Shot Learning Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.5 Impact of Different Components . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.6 Additional Experiments and Comparison . . . . . . . . . . . . . . . . . . . . . 27
G Calibration Behavior of Self-Supervised Model 28
H Societal Impact 28
I Limitations and Future Work 29
16A Organization of the Appendix
• In Section B, we present the table with summary of the key symbols used in this work.
•In Section C, we present related works in uncertainty-aware deep learning, describe eviden-
tial deep learning for classification in details, and discuss some key advantages of using
evidential models to address the under-confidence issue over standard Bayesian models.
• In Section D, we present proof of all our theoretical claims.
• In Section E, we present the details of hyperparameters and additional experiment details.
•In Section F, we present additional experiment results including the impact of m for calibra-
tion performance, comparison results with meta-learning methods, the impact of the number
of classes, data size, and unfrozen parameters, comparison in OOD settings, the impact of
incorrect evidence regularization strength, the results of additional PEFT methods, and the
impact of ensemble components.
•In Section G, we discuss the calibration and accuracy behavior for PEFT of large foundation
models pre-trained in a self-supervised fashion.
• In Section H, we discuss societal impact of our work.
• In Section I, we discuss the limitations of our work and present potential future direction.
The source code for the experiments carried out in this work is attached in the supplementary materials
and is available at the link: https://github.com/ritmininglab/B-PEFT
B Summary of the Symbols
Table 4: Summary of the symbols and their definitions
Symbol Definition
x Input sample vector
y Ground truth label as one hot vector
e, ei The evidence vector, and the evidence for class i
ai Fixed base rate for class i, usually set to ai=1
N
αi Dirichlet parameter value for class i
S=PN
i=1αi The Dirichlet Strength
(b, bcor, binc) The belief vector, Correct belief, and the Incorrect belief
N Number of classes
bi Belief for class i
u Vacuity output by the model
dis Dissonance output by the model
E Non-negative evidential transformation function (we use exp)
χi Learnable base rate for class i
χ Learnable base rate vector
λ Incorrect evidence regularization strength
⊙ Element wise multiplication between two vectors
W Non-informative prior weight
C Further Discussion on Uncertainty-Aware Deep Learning
C.1 Existing Uncertainty Quantification Methods in Deep Learning
Accurate quantification of predictive uncertainty is essential for the development of trustworthy
Deep Learning (DL) models. To this end, DL models have been augmented to become uncertainty-
aware using a variety of approaches such as ensemble-based approaches [ 36,49], bayesian neural
networks based approaches [ 42,19,7], and deterministic neural network based approaches [ 56,
11,3]. Deep ensemble techniques [ 36,49] construct an ensemble of neural networks, and the
agreement/disagreement across the ensemble components is used to quantify different uncertainties.
Alternatively, Bayesian neural networks [ 19][7][42] have been developed that consider a Bayesian
formalism ( e.g., bayes-by-backprop [7], dropout during test [19]) to quantify different uncertainties.
17ABNN [ 18] introduces Bayesian normalization layers after training of deep learning models, and
requires additional training of these layers in a post-hoc manner. Deterministic neural network-based
approaches [ 57,41,10] extend the existing neural network to become uncertainty-aware and enable
the networks to quantify fine-grained uncertainties with a single forward pass through the network.
Evidential deep learning models [ 56,5,75,10,10,62], an instance of deterministic approaches,
introduce a conjugate higher-order evidential prior for the likelihood distribution to enable the model
to express the fine-grained uncertainties in both classification[ 56,11] and regression problems [ 3,47].
Towards classification, evidential models [ 56,5,75] introduce higher-order evidential Dirichlet
prior to the multinomial likelihood that enables the deterministic neural network model to capture
different uncertainty characteristics. In what follows, we first provide some additional details on using
evidential learning model to perform classification. We then highlight some important advantage of
using evidential models over standard Bayesian models in uncertainty quantification.
C.2 Evidential Deep Learning Models for Classification
Evidential Deep Learning models, based on Subjective Logic theory [ 33], aim to train the model
such that for any new input sample, the model can make predictions, as well as output fine-grained
uncertainty information (via vacuity [ 56] and dissonance [ 34]). Towards capturing fine-grained
uncertainty for classification problems, EDL models assume that the label for each sample is obtained
from a generative process with a Dirichlet prior and a multinomial likelihood. The parameters for
the Dirichlet prior express the vacuity and belief masses for uncertainty estimation. The conjugacy
between the Dirichlet prior and the multinomial likelihood is explored, and different evidential losses
are introduced for model training and inference [ 48]. In this work, we consider Type-II Maximum
Likelihood-based evidential loss LLog(x,y)[56] with incorrect evidence regularization Linc
reg(x,y)
given by [56]
Levid(x,y) =LLog(x,y) +λ× Linc
reg(x,y) (7)
We replace the softmax layer in the head of the VPT model with expactivation function. To avoid zero
evidence regions , we also include the correct evidence regularization Lcor(x,y) =−λcorlog(αgt−1)
(λcoris the magnitude of the model’s vacuity) in model training objective. The evidential model
outputs evidence vector e=(e1, e2, ...eN)for a given input xand corresponding ground truth label of
y. Based on the evidence, Dirichlet parameters are obtained as αi=ei+ 1. The Type-II Maximum
likelihood-based evidential loss is given by
LLog(x,y) =−lnZ
Mult(y|p)Dir(p|α)dp= log S−KX
k=1yklogαkS=KX
k=1αk (8)
The incorrect evidence regularization guides the model to minimize the evidence for all classes other
than the ground truth class and can take one of the following forms
1. KL-based incorrect evidence regularization term as in EDL [56]
LEDL
reg(x,y) =KL 
Dir(p|˜α)||Dir(p|1)
= logΓPK
k=1˜αk
Γ(K)QK
k=1Γ˜αk
+KX
k=1(˜αk−1)
ψ(˜αk)−ψKX
j=1˜αj(9)
Where ˜α=y+ (1−y)⊙α= (˜α1,˜α2, ...˜αN)parameterize a dirichlet distribution,
˜αi=gt= 1,˜αi=αi∀i̸=gt, and⊙represents element-wise product. Here, the KL
regularization term encourages the Dirichlet distribution based on the incorrect evidence i.e.,
Dir(p|˜α)to be flat which is possible when there is no incorrect evidence.
2. Incorrect evidence sum based regularization as in ADL [57]
LADL
reg(x,y) =KX
k=1 
e⊙(1−y)
k=KX
k=1ek×(1−yk) (10)
3. Incorrect belief-sum based regularization as in Units-ML [46]
LUnits
reg(x,y) =KX
k=1 e
S⊙(1−y)
k=KX
k=1ek
S×(1−yk) (11)
18All three regularizations guide the model to minimize the incorrect evidence(ideally close to zero). In
our experiments, we consider KL-based incorrect evidence regularization.
C.3 Evidential models vs. Standard Bayesian Models
As compared with the Bayesian-inspired models, evidential learning offers two key properties that
allow us to formulate a principled solution to address the unique under-confident behavior of the
PEFT methods. First, thanks to its evidence-based fine-grained uncertainty decomposition capability,
we can separate two distinct sources of second-order uncertainty, including vacuity and dissonance.
Different from the commonly used first-order uncertainty (e.g., entropy), these two second-order
uncertainty serve as a key tool to understand why PEFT methods are both accurate (with a low
dissonance) while being under-confident (with a high vacuity). This key insight suggests that these
methods systematically under-estimate the contribution from the prior knowledge to the downstream
task. While the classical Bayes’ theorem offers a principal idea to address the issue, which is to
strengthen the prior belief, there is a lack of practical way to achieve this. As the second key property,
evidential learning allows us to leverage the base rate, which is rooted in the subjective logic theory
as an effective vehicle to adjust the prior belief gained through pre-training. To this end, we propose
a transformation function in Eq. (6) to adjust the base rate that leads to the increase of the model
confidence while maintaining the predictive accuracy of the model as guaranteed by our theoretical
results in Lemma 2 and Theorem 3. Furthermore, we develop belief-based diversity for ensemble
of evidential models leading to the the B-PEFT model. In theory, evidential deep learning model
could be augmented with the Bayesian normalization layers [ 18] or Bayesian neural networks [ 7] as
an alternative to belief-based diversity of B-PEFT . We leave exploration of different techniques for
diversity for Bayesian evidential model as a potential future work.
D Proofs of Theoretical Results
In this section, we provide the proofs of the major theoretical results presented in the main paper.
D.1 Proof of Lemma 2
Proof. Consider an input sample xfor which the model outputs the evidence (e1, e2, ..., e N)⊤. Let
emax= max( e1, e2, ..., e N), and emin= min( e1, e2, ..., e N). Here, emax≥ei≥emin∀i∈[1, N].
For the evidential model with fixed base rate of ai=1
N∀i∈[1, N], the model’s predicted class is
given by cpred= arg max( e1+a1×W, e 2+a2×W, ..., e N+aN×W) = arg max( e1+ 1, e2+
1, ..., e N+ 1) = Index (emax).For the calibrated model with learnable χ= (χ1, χ2, ..., χ N)⊤, the
model’s predicted class is given by cnew
pred= arg max( α1, α2, ..., α N) = arg max( e1+χ1×W, e 2+
χ2×W, ..., e N+χN×W) = arg max( e1+N(e1
emin)−N, e 2+N(e2
emin)−N, ..., e N+N(eN
emin)−N).
Since emax≥ei≥emin∀i∈[1, N],αmax≥αi≥αmin∀i∈[1, N], and cnew
pred=cpred
D.2 Proof of Theorem 3
Proof. Consider an input sample xfor which the model outputs the evidence (e1, e2, ..., e N)⊤.
Letemax= max( e1, e2, ...eN),emin= min( e1, e2, ...eN), and emax≥e2nd≥, ...,≥emin. For the
evidential model with a fixed base rate of ai=1
N∀i∈[1, N], the difference between the Dirichlet
parameters for class with maximum evidence and class with the second maximum evidence is given
byαmax−α2nd=emax+amaxW−e2nd−a2ndW=emax−e2ndasai=1
N∀i∈[1, N].
For the calibrated model with learnable χ= (χ1, χ2, ..., χ N), the difference between the Dirichlet
parameters for class with maximum evidence and class with the second maximum evidence is given
byαmax−α2nd=emax+χmaxW−e2nd−χ2ndW= (emax−e2nd) + (χmax−χ2nd)W. Now,
χmax=emax−emin
eminm
=emax
emin−1m
&χ2nd=e2nd−emin
eminm
=e2nd
emin−1m
(12)
Or, 
χmax−χ2nd
=emax
emin−1m
−e2nd
emin−1m
(13)
Sinceei
emin≥1∀i∈[1, N], and emax≥e2nd≥, ...,≥emin,(χmax−χ2nd)≥0∀m > 0. For
emax> e 2nd,&m > 0,χmax−χ2nd>0. Thus, with the proposed learnable base rate, the gap between
19the two largest Dirichlet parameters is maintained whenever m= 0and/or emax=e2nd. Moreover,
whenever m≥1andemax> e 2nd, the Dirichlet parameter gap between the two classes is increased
by a factor of
emax
emin−1m
−
e2nd
emin−1m
.
D.3 Connection with SVGD-based Bayesian Ensembling and Proof of Lemma 4
We first carry out an analysis of the update in Stein Variational Gradient Descent (SVGD) based
ensembling [ 12,13] that reveals the repulsive force acting among the ensemble components that
pushes the particles away and introduces diversity. We then consider ensemble components with
different strengths of incorrect evidence regularization Linc
regand analyze the update to the evidential
model in the evidence space that reveals a repulsive diversity-enforcing force acting identical to the
SVGD based ensemble.
SVGD update involves randomly initializing the particles and iteratively updating the particles to
match the target distribution, which is summarized below.
Algorithm 1 SVGD Update
Input: {x0
i}N
i=1: A set of initial parameters, and target distribution density function p(x)
ForLiterations , iteratively update the particles as
•xl+1
i=xl
i+ϵlˆϕ∗(xl
i),where ˆϕ∗(x) =1
EPE
e=1k(xl
e, x)∇xlelogp(xl
e)+∇xlek(xl
e, x)
Here, ϵlis the step size at iteration l, and k(·,·)is the kernel function that measures similarity.
Output: {x0
i}N
i=1: A set of initial parameters, and target distribution density function p(x)
For given incorrect evidence regularization Linc
reg, and P ensemble components with regularization
strengths λp, p∈[1, P], the ensemble components in the evidence space are implicitly pushed away
from each other by a force λp∇Linc
regthat acts identical to the repulsive force in Stein Variational
Gradient Descent (SVGD) based ensembles.
Proof. For simplicity, consider RBF kernel for k(·,·)i.e.,k(a, b) = exp −1
h(a−b)2
, two particles
x1, x2, and analyze their updates in SVGD based ensembling. At iteration l, the update to particle x2
is given by
ˆϕ∗(xl
2) =1
2
k(xl
2, xl
1)∇xl
1logp(xl
1) +k(xl
2, xl
2)∇xl
2logp(xl
2) +∇xl
1k(xl
1, xl
2) +∇xl
2k(xl
1, xl
2)
In the above update,− →Q=k(xl
2, xl
1)∇xl
1logp(xl
1) +k(xl
2, xl
2)∇xl
2logp(xl
2)aims to guide the
particles in the direction that maximizes the likelihood, and the update direction− →R=∇xl
1k(xl
1, xl
2)+
∇xl
2k(xl
1, xl
2)acts as the repulsive force. Considering the repulsive force
− →R=∇xl
1k(xl
1, xl
2) +∇xl
2k(xl
1, xl
2) =∇xl
1exp−1
h(xl
1−xl
2)2
+∇xl
2exp−1
h(xl
1−xl
2)2
=2
h(xl
2−xl
1)k(xl
1, xl
2)
As can be seen, the repulsive force− →Rpushes the particle xl
2in the direction away from particle xl
1
that introduces diversity. With more particles, each particle is updated in the direction that maximizes
the likelihood, and the particle is pushed away from all other particles (by force R).
Next, consider ensemble components with different strengths of incorrect evidence regularization Linc
reg
to analyze the update to the evidential model in the evidence space. For simplicity, we consider the
incorrect evidence sum-based regularization similar to ADL without correct evidence regularization
(The analysis is valid for all incorrect evidence regularization and for models with correct evidence
regularization). For a model with incorrect evidence regularization, the overall evidential loss is given
by:
Levid(x,y) =LLog(x,y) +λ× LADL
reg(x,y) = log S−KX
k=1yklogαk+λ×KX
k=1ek×(1−yk)
20The gradient of the loss with respect to logits (the output head layer, where ek= exp( ok)) is given by
gradk=∂LLog(x,y)
∂ok+∂LADL
reg(x,y)
∂ok=1
S−yk
αk∂ek
∂ok+λ×(1−yk)×∂ek
∂ok(14)
=1
S−yk
αk+λ(1−yk)
ek=1
S−yk
αk+λ(1−yk)
ek (15)
Consider Kclass classification problem. The gradient update to the logit layer for the evidential
model is given by
gradk=ek×
1
S−y1
α11
S−y2
α2...
1
S−yK
αK
+ek×λ×
1−y1
1−y2
...
1−yK
 (16)
=− →A+λ×− →B (17)
Here, yk∈[0,1], yk= 1ifk=gt,and yk= 0 otherwise . Moreover, the λvalue is varied,
and different λvalues lead to different evidential models.
The update force− →Apushes the evidential model in the direction that maximizes the likelihood
(similar to− →Qin SVGD-based update), and the force− →Bimplicitly pushes the ensemble components
away from each other (similar to the repulsive force− →Rin SVGD-based update). Each component
moves in− →Bdirection with a different force determined by the incorrect evidence regularization
strength λthat ensures that the ensemble components are diverse. Due to the different strengths
of incorrect evidence regularization, each ensemble component places a different priority level for
minimization of incorrect evidence over acquiring correct evidence, which ensures that the ensemble
components remain diverse.
E Dataset and Implementation Details
We consider ViT model backbone [ 15] that is pre-trained in a supervised fashion, and 4 benchmark
datasets of Cifar10 [ 1], Cifar100 [ 1], Food101 [ 8], and Flowers102 [ 45]. We consider few-shot
adaptation with K-shot classification problem (We experiment with Kvalues of 1,2,5,10, and
20). The few-shot training set is constructed by randomly selecting Ksamples per class from the
training set of the benchmark datasets. We consider 2-shot validation set for all datasets and settings.
We train the model on the few-shot training set, use the 2-shot validation set for hyperparameter
tuning, and evaluate all models on the benchmark test set with all the test set samples. We augment
the few-shot training set and the few-shot validation set with resize, random horizontal flip, and
cropping. The dataset details are also presented in Table 5. We train all the models for 50epochs
on the few-shot training dataset with a batch size of 64samples at each iteration and evaluate the
model on the benchmark test set. The evidence is in the range [0, infinity], and some stability issues
could potentially arise in extreme cases when the logit output is extremely low (i.e. close to negative
infinity). In our experiments, we did not observe the stability issue. Still, the issue can arise in some
extreme cases for which a small delta in the denominator could be introduced or the network’s logits
could be bounded to be greater than a small negative value. For the calibration baseline model of
Parameterized Temperature Scaling (PTS) [ 61], we consider a 2-layer neural network with 128 nodes
in the hidden layer (we carry out hyperparameter tuning with 1-layer, 2-layer, and 3-layer networks
and select the best model), and train with a learning rate of 0.00001. For Temperature Scaling [ 26],
we optimize for the temperature hyperparameter using Adam optimizer, and a learning rate of 0.01
(We also experiment with SGD optimizer, and learning rates of 0.1, 1.0, 0.01, and 0.0001 to select
the best performing model). The evidential models use incorrect evidence regularization strength
of (0,0.1,1.0,10.0,100.0,and1000.0). For Isotonic Regression [ 6], we consider multi-class setting
and sklearn package. We use VPT [ 32] as the representative PEFT where not specified due to its
superior performance. The key model performance results are averaged across 5 different runs to
present the mean and the standard deviation. The experiments use Pytorch and are carried out on a
workstation with NVIDIA RTX A6000 GPU.
21Dataset Name Number of Classes Training Samples Validation Samples Test Samples
Cifar10 [1] 10 10 ×K 20 10 ,000
Cifar100 [1] 100 100 ×K 200 10,000
Food101 [8] 101 101 ×K 202 25,250
Flowers102 [45] 102 102 ×K 204 6 ,149
Table 5: Dataset Details for K-Shot Classification problem
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 49.155, ECE: 0.272
(a) 1 Shot, KL = 0.1, m = 1.0
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 49.155, ECE: 0.103 (b) 1 Shot, KL = 0.1, m = 1.5
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 49.155, ECE: 0.070 (c) 1 Shot, KL = 0.1, m = 2.0
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 50.905, ECE: 0.214
(d) 1 Shot, KL = 100.0, m =
1.0
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 50.905, ECE: 0.083(e) 1 Shot, KL = 100.0, m =
1.5
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 50.905, ECE: 0.124(f) 1 Shot, KL = 100.0, m =
2.0
Figure 9: Visualization of the impact of m for 1-Shot Cifar100 dataset using reliability plots
F Additional Experiments
F.1 Impact of mon Expected Calibration Error
The developed B-PEFT model introduces the post-hoc calibration technique that adjusts the base
rate in the evidential model with one additional hyperparameter m(see Section 3.2 for details).
We carry out a grid search using the few-shot validation dataset and select the optimal mfor the
evidential models. In this section, we carry out detailed ablation to study the impact of mwith
few-shot Cifar100 datasets. We consider mvalues of (0.5,1.0,1.5,2.0,2.5,3.0), incorrect evidence
regularization strengths of (0.0,0.1,1.0,10.0,100.0,1000.0), and few-shot values of (1,2,5,10,20)
(see Table 6 and Figure 9, Figure 10). Across all the experiments, we observe that as we increase m,
the model transforms the evidence to maximize the evidence gap making the model more confident on
its knowledge. With increased confidence, the model’s calibration performance improves. However,
a large increase in mvalues starts to make the model overconfident leading to increased ECE and
poor calibration. The strength of incorrect evidence regularization also impacts the optimal value
ofm. For large values of incorrect evidence regularization, a smaller mvalue suffices to make the
model well-calibrated. The trend is seen across all few-shot classification settings.
22Table 6: Impact of m
Reg. Strength ( λ) (Eqn 2) m = 0.5 m = 1.0 m =1.5 m =2.0 m =2.5 m =3.0
1 Shot
λ=0.0 0.401±0.0070.257±0.0090.094±0.0110.07±0.0040.147±0.0080.214±0.008
λ=0.1 0.423±0.0080.261±0.0070.092±0.0080.079±0.0030.156±0.0070.22±0.008
λ=1.0 0.434±0.0090.252±0.0090.082±0.0090.091±0.0050.169±0.0070.23±0.007
λ=10.0 0.429±0.0040.231±0.0050.077±0.0070.107±0.0050.186±0.0050.245±0.005
λ=100.0 0.433±0.0090.212±0.0070.08±0.0080.125±0.0070.199±0.0080.253±0.009
λ=1000.0 0.422±0.0060.182±0.0030.077±0.0050.148±0.0030.221±0.0020.271±0.003
2 Shot
λ=0.0 0.533±0.0050.334±0.0090.120±0.0110.041±0.0020.100±0.0090.154±0.008
λ=0.1 0.559±0.0060.328±0.0100.103±0.0100.060±0.0050.113±0.0060.160±0.005
λ=1.0 0.565±0.0050.308±0.0050.086±0.0040.072±0.0030.125±0.0030.167±0.003
λ=10.0 0.560±0.0050.282±0.0050.074±0.0040.081±0.0040.136±0.0050.177±0.004
λ=100.0 0.550±0.0020.264±0.0030.066±0.0020.087±0.0030.146±0.0020.186±0.002
λ=1000.0 0.547±0.0020.257±0.0010.064±0.0010.090±0.0020.147±0.0020.186±0.002
5 Shot
λ=0.0 0.597±0.0100.354±0.0170.125±0.0170.031±0.0040.073±0.0110.117±0.010
λ=0.1 0.639±0.0070.349±0.0070.109±0.0050.042±0.0040.084±0.0020.119±0.003
λ=1.0 0.634±0.0020.340±0.0030.111±0.0020.047±0.0060.085±0.0030.122±0.002
λ=10.0 0.645±0.0040.362±0.0050.138±0.0030.049±0.0040.071±0.0010.107±0.002
λ=100.0 0.652±0.0060.357±0.0010.119±0.0010.042±0.0020.075±0.0030.109±0.003
λ=1000.0 0.642±0.0060.314±0.0030.081±0.0020.051±0.0020.091±0.0040.122±0.005
10 Shot
λ=0.0 0.604±0.0170.325±0.0070.090±0.0040.041±0.0030.095±0.0050.134±0.007
λ=0.1 0.629±0.0090.314±0.0050.077±0.0050.054±0.0040.100±0.0040.134±0.004
λ=1.0 0.657±0.0060.367±0.0030.136±0.0020.053±0.0030.068±0.0030.102±0.004
λ=10.0 0.685±0.0040.404±0.0020.151±0.0010.034±0.0020.057±0.0010.090±0.002
λ=100.0 0.673±0.0070.363±0.0040.109±0.0040.041±0.0010.075±0.0010.104±0.003
λ=1000.0 0.651±0.0040.335±0.0050.099±0.0060.049±0.0010.082±0.0010.113±0.001
20 Shot
λ=0.0 0.413±0.0030.141±0.0080.060±0.0100.162±0.0110.226±0.0110.270±0.010
λ=0.1 0.409±0.0020.099±0.0050.090±0.0060.188±0.0070.248±0.0060.288±0.006
λ=1.0 0.399±0.0020.091±0.0020.086±0.0020.183±0.0020.244±0.0020.284±0.002
λ=10.0 0.387±0.0020.075±0.0040.106±0.0030.202±0.0030.259±0.0020.297±0.002
λ=100.0 0.401±0.0120.097±0.0240.093±0.0210.196±0.0160.259±0.0120.299±0.009
230.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 71.497, ECE: 0.346(a) 5 Shot, KL = 0.1, m = 1.0
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 71.497, ECE: 0.108 (b) 5 Shot, KL = 0.1, m = 1.5
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 71.497, ECE: 0.041 (c) 5 Shot, KL = 0.1, m = 2.0
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 74.147, ECE: 0.357
(d) 5 Shot, KL = 100.0, m =
1.0
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 74.147, ECE: 0.116(e) 5 Shot, KL = 100.0, m =
1.5
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 74.147, ECE: 0.041(f) 1 Shot, KL = 100.0, m =
2.0
Figure 10: Visualization of the impact of m for 5-Shot Cifar100 dataset using reliability plots
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 49.845, ECE: 0.406
(a) Standard CE model
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 50.965, ECE: 0.494 (b) Evidential Model
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 50.965, ECE: 0.079 (c) Calibrated Evidential
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 52.825, ECE: 0.068 (d)B-PEFT (Ours)
Figure 11: Accuracy-Confidence trends in 1-shot Cifar100 Results
24F.2 Impact of Incorrect Evidence Regularization Strength ( λ)
Evidential deep learning models introduce incorrect evidence regularization to minimize the evidence
of classes other than the ground truth class. In this work, we use KL divergence-based incorrect
evidence regularization (see Section C.2), and introduce a hyperparameter λthat controls the priority
the model places on minimizing the incorrect class evidence over maximizing the correct class
evidence. In this section, we study the impact of the hyperparameter λon the model performance
withK-shot Cifar100 and Flowers102 experiments (We experiment with Kvalues of (1,2,5,10,20)).
We observe that the model’s calibration performance (the ECE) is optimal when no incorrect evidence
regularization is used i.e.,λ= 0 (see Table 7 and Table 8). However, no incorrect evidence
regularization hurts the model’s generalization performance. With the increase in incorrect evidence
regularization, the model’s generalization performance (indicated by accuracy) improves albeit with
ECE tradeoff. However, very large incorrect evidence regularization misguides the model to only
focus on minimizing incorrect class evidence hurting generalization. The optimal λvalue leads to
the best generalization performance while hurting the ECE performance. Moreover, with a larger
number of shots in training, the optimal λvalue is generally smaller (For instance, in 1-shot Cifar100,
optimal λ= 1000 .0, in2-shot Cifar100, optimal λ= 10.0, and in 5-shot Cifar100, optimal λ= 0.1).
Once the optimal λvalue is determined, we can resort to our evidential base-rate adjustment that
leads to a calibrated evidential model with good generalization performance.
Table 7: Different Shot Classification – Accuracy and ECE in Cifar100
Shots KL 0.0 KL 0.1 KL1.0 KL10.0 KL100.0 KL1000.0
(a) Accuracy ↑
1 45.502±0.49247.707±0.72748.754±0.67249.967±0.56950.127±0.96251.127±0.435
2 60.494±1.56764.006±0.83464.820±0.58665.545±0.33965.439±0.29565.531±0.301
5 74.230±1.09977.391±1.05377.238±0.69476.760±0.65277.561±0.71677.525±0.685
10 80.566±0.36881.512±0.16381.055±0.18581.561±0.29181.559±0.253 81.344±0.2
20 82.111±0.684 82.967±0.283.012±0.12383.100±0.18483.014±0.103 81.966±0.7
(b) ECE ↓
1 0.404±0.005 0.440±0.006 0.460±0.007 0.479±0.005 0.487±0.009 0.499±0.004
2 0.480±0.010 0.562±0.007 0.596±0.005 0.620±0.004 0.631±0.003 0.637±0.003
5 0.513±0.006 0.632±0.007 0.686±0.003 0.715±0.005 0.744±0.006 0.756±0.006
10 0.499±0.004 0.644±0.004 0.712±0.001 0.765±0.002 0.787±0.002 0.740±0.007
20 0.483±0.003 0.647±0.001 0.733±0.001 0.782±0.001 0.804±0.001 0.490±0.007
Table 8: Different Shot Classification – Accuracy and ECE in Flowers102
Shots KL 0.0 KL 0.1 KL1.0 KL10.0 KL100.0 KL1000.0
(a) Accuracy ↑
1 Shot 84.314±1.57186.434±0.43788.481±0.9289.225±1.0389.475±1.04589.882±0.592
2 Shot 91.416±1.29693.795±0.55794.575±0.50494.899±0.209 94.8±0.409 95.071±0.413
5 Shot 97.471±0.13597.51±0.27997.602±0.19997.139±0.18596.953±0.1997.235±0.248
10 Shot 98.326±0.23397.964±0.19897.804±0.09397.847±0.06498.093±0.1498.034±0.111
20 Shot 98.708±0.01498.37±0.07297.953±0.11598.086±0.17698.406±0.16797.75±0.804
(b) ECE ↓
1 Shot 0.662±0.015 0.722±0.008 0.766±0.008 0.801±0.008 0.826±0.009 0.846±0.004
2 Shot 0.608±0.005 0.696±0.003 0.747±0.008 0.794±0.008 0.835±0.009 0.874±0.006
5 Shot 0.487±0.014 0.598±0.013 0.686±0.020.731±0.005 0.83±0.019 0.896±0.004
10 Shot 0.444±0.008 0.57±0.026 0.641±0.008 0.733±0.013 0.827±0.018 0.908±0.01
20 Shot 0.411±0.013 0.544±0.009 0.634±0.012 0.75±0.046 0.831±0.023 0.898±0.007
25F.3 Impact of Ensemble Components
49 50 51 52
Accuracy0.0780.0800.0820.0840.0860.088ECEEnsemble Components
1 components
2 components
3 components
4 components
5 components
Figure 12: Impact of ensemble components as
number of component increasesWe present the experiment to study the impact
of ensemble components in Figure 12. The ex-
periment is performed on 1 one-shot Cifar100
dataset using prompt-based adaption for vision
transformer. There is a performance gain of both
accuracy and ECE with the use of all ensemble
components. We also note that as we increase the
number of components from 1to3, both the gen-
eralization and calibration performance increase
significantly. For instance, the ECE with a single
ensemble component is 0.088 which improves to
0.077 with 3 ensemble components while the ac-
curacy improves by almost 3%.. However, with a
further increase in the number of ensemble compo-
nents, the generalization/calibration performance
improvement is not significant. Thus, for our B-PEFT model, we carry out an ensemble of 3evi-
dential models that is a good balance between the number of ensemble components and the gain in
generalization/calibration performance.
F.4 Few Shot Learning Results
In this set of experiments, we apply our model to the 5-way 1-shot mini-ImageNet testing tasks
and compare it with representative meta-learning models. The results are summarized in Table
9. Benefiting from the knowledge acquired during pre-training, the proposed PEFT-based model
outperforms the episodic meta-learning models by a large margin, demonstrating the potential of
large vision foundation models for effective few-shot learning. However, the VPT model is under-
confident as shown in Table 10 and Figure 13. Our B-PEFT model improves on the VPT model’s
generalization and calibration leading to promising few-shot adaptation results.
Table 9: 5-Way 1-Shot Mini-ImageNet
Model Accuracy
MAML [16] 48.70
Matching Networks [64] 43.56
LLAMA [24] 49.40
VERSA [23] 53.40
PLATIPUS [17] 50.13
Bayesian-MAML [69] 53.30
Bayesian-TAML [37] 71.46
VPT 89.50
B-PEFT (Ours) 90.09
Table 10: Calibration Results
Model Accuracy ECE
VPT 89.50 0.418
B-PEFT (Ours) 90.09 0.065
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyFigure 13: VPT reliability plot for 5-Way 1-
Shot Mini-ImageNet
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0Accuracy
Figure 14: Full fine tuning reliability plot for
100-way 1-Shot Cifar100
F.5 Impact of Different Components
In this section, we study the under-confidence behavior of PEFT methods w.r.t. data size, number of
classes, and number of unfrozen parameters.
26Data size. We observe that the model’s accuracy increases with more training samples (see Table
1 where we vary shots from 1 to 20). The under-confidence remains, even with further increase in
training samples. To this end, we conduct additional experiments on Cifar100 by increasing the
training samples per class to 500 and observe the under-confidence issue despite the increase in the
accuracy. The trend is summarized in Figure 15 (a-b). We see an increase in accuracy and a decrease
in ECE. However, even with 500 samples per class, the under-confidence issue remains. Further,
we report the accuracy and ECE of the fully fine-tuned model (fine-tuning of all the parameters)
for 1 shot cifar100 in Table 11 where we observe a decrease in accuracy while the calibration issue
remains. We observe that full fine-tuning leads to overconfidence behavior, hurting the generalization
performance, as seen in Table 11 and reliability plot as presented in Figure 14.
0 100 200 300 400 500
Number of Shots5055606570758085Accuracy
Accuracy Trend
(a) Data size trend
0 100 200 300 400 500
Number of Shots0.400.420.440.460.480.500.52ECE
ECE Trend (b) Data size trend
2 4 6 8 10
Number of Prompt4042444648Accuracy
1 shot Cifar100
Shallow Prompt
Deep Prompt (c) Prompt size trend
2 4 6 8 10
Number of Prompt0.340.360.380.400.42ECE
1 shot Cifar100
Shallow Prompt
Deep Prompt (d) Prompt size trend
Figure 15: (a-b) Accuracy-EEC trend with the number of parameters, (c-d): Accuracy-ECE Trend
with data size
Number of classes: To study the impact of number of classes, we formulate 5-way 1 shot, 10-way 1
shot, and 100-way 1 shot tasks using Cifar100. The results are presented in Table 12. As we decrease
the number of shots from 100 to 5, we see an increase in accuracy and a decrease in ECE. We observe
that the model is more accurate as tasks become easier (indicated by fewer classes i.e.,lower Nvalue
in Table 12). However, the under-confidence issue remains.
Number of unfrozen parameters: We conduct additional experiments on Cifar100 100-way 1-shot
tasks by varying the number of prompts for 1) shallow prompt: prompt added to the input only and
2) deep prompt: prompt added to all Transformer encoder layers’ input as well. The accuracy and
ECE trends are presented in Figure 15 (c-d). As can be seen, with the increase in the number of
prompts for both shallow and deep prompts, there are fluctuations in accuracy and ECE performance.
However, the under-confidence issue persists for all the cases.
F.6 Additional Experiments and Comparison
In this section, we carry out additional experiments to study the OOD performance of our model, and
compare our model with additional methods present in literature.
OOD Performance: The current work, being an instance of fine-grained uncertainty quantification
works, could potentially help in OOD detection. To this end, we present the OOD results of Cifar10
as in-distribution dataset and Cifar100 as out-of-distribution dataset with AUROC, FPR95, AUPR
metrics for our model on 100-way 1-shot and 100-way 5-shot Cifar100 tasks in Table 13. As
seen, B-PEFT performs better than PEFT, and with more training data, the model’s OOD detection
capabilities improve. Even with only 5 samples/class (i.e. 100-way 5-shot Cifar100 task), the model
can achieve an AUROC of 92.58.
Model Comparison: We first carry out experiments with cosine classifier [ 22] without training
for the 100-way 1-shot task on Cifar100. The cosine classifier has comparable generalization
performance (accuracy) in comparison to VPT based model (see Table 14). However, looking at
the ECE, the miscalibration issue is even higher than VPT based model. Hence, the simple solution
(cosine classifier) does not ensure calibrated predictions. We also carry out experiments with test
time augmentation [ 4] and VPT fine tuning with LoRA [ 70], on 100-way 1-shot Cifar100 dataset.
Towards comparison with Bayesian inspired methods [ 14], we use Laplace approximation on last
layer of the model using Kronecker Product and Diagonalization represented by KronLaplace and
DiagLaplace in Table 14. As can be seen, these methods also suffer from the under-confidence
issue when straightforwardly extended to the VPT. B-PEFT achieves much better generalization and
calibration performance than these baselines.
27Table 11: Full fine-tuning results
Model Accuracy ECE
Full Fine Tuning 25.75 0.118
FT + Base rate adj. 25.75 0.037
VPT 48.63 0.393
B-PEFT 52.34 0.067Table 12: N-Way 1-Shot Cifar100 calibration
Task Accuracy ECE
5 way 1 shot 63.60 0.324
10 way 1 shot 56.35 0.370
100 way 1 shot 48.63 0.393
Table 13: 100-way Cifar100 OOD experiments
PEFT AUROC AUPR FPR95
1 shot 79.53 80.09 70.58
5 shot 90.93 90.75 39.82
(B-PEFT) AUROC AUPR FPR95
1 shot 81.24 81.98 68.15
5 shot 92.58 92.85 35.24Table 14: Comparison with baselines
Model Accuracy ECE
Cosine Classifier 47.99 0.493
ViT + LoRA 48.19 0.243
Test Time Aug. 50.10 0.271
VPT + KronLaplace 50.26 0.475
VPT + DiagLaplace 50.20 0.474
VPT 48.63 0.393
B-PEFT 52.34 0.067
G Calibration Behavior of Self-Supervised Model
012 5 10 20
Shots202530354045505560Accuracy
Num of Shots vs Accuracy
zero-shot
prompt
adapter
Figure 16: Accuracy trends of CLIP on few-
shot adaptationIn this work, we focus on vision foundation models
that are pre-trained in a supervised learning paradigm.
These models have shown remarkable effectiveness
in a wide range of areas including image classifi-
cation, video understanding, and visual recognition
among others. Alternatively, foundation models have
been developed that pre-train in a self-supervised
fashion ( e.g., CLIP [ 52]). These models demonstrate
good zero-shot performance on various datasets. As
a representative of the self-supervised models, we
use CLIP in our experiments. Parameter-efficient
methods [ 77,76,21] have been proposed to adapt
the CLIP model for downstream tasks. We use the
popular methods: adapter and prompt for few-shot
adaptation to study the calibration behavior. The re-
liability plot of different shot adaptations (1,2 and 5
shots per class) for cifar100 along with accuracy and ECE is presented in Figure 17. The accuracy
behavior as we increase the number of shots from 1to20is shown in Figure 16. In both methods,
we observe for few-shot adaptations, the accuracy is either lower or comparable to zero-shot perfor-
mance. More specifically, for adapter-based adaptation, even with 5 shots per class, the accuracy
does not reach zero-shot accuracy. On the contrary, the parameter-efficient fine-tuning of supervised
foundation models shows consistent improvement in accuracy as we increase the number of shots.
Towards calibration performance, we observe that prompt-based adaptation, along with zero-shot
generally show good calibration performance with some degree of overfitting (see Figure 17). In
contrast, prompt adaptation for supervised models that are severe, and prompt adaptation for CLIP
models have no such issue. However, the adapter-based adaptation is mostly over-confident and hence
has a higher ECE value than our proposed model. Considering these results for parameter-efficient
few-shot adaption of pre-trained self-supervised models, the calibration and uncertainty behavior of
such pre-trained models pose an interesting direction for further investigation.
H Societal Impact
We study the parameter-efficient fine-tuning techniques for large pre-trained vision foundation
models, where we identify two key issues: the under-confidence of the fine-tuned models in their
280.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 41.960, ECE: 0.091(a) 1 Shot using prompt
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 44.370, ECE: 0.076 (b) 2 Shot using prompt
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 49.050, ECE: 0.056 (c) 5 Shot using prompt
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 21.370, ECE: 0.225
(d) 1 Shot using adapter
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 29.150, ECE: 0.212 (e) 2 Shot using adapter
0.0 0.2 0.4 0.6 0.8 1.0
Confidence0.00.20.40.60.81.0AccuracyAccuracy: 43.340, ECE: 0.152 (f) 5 Shot using adapter
Figure 17: Different Shot adaptation: Calibration performance of CLIP based model on cifar100
dataset using prompt and adapter-based fine-tuning
predictions, and lack of fine-grained uncertainty quantification capabilities. We develop a novel
Bayesian Evidential model: B-PEFT that addresses the weaknesses of existing PEFT for pre-trained
foundation models. Being an instance of the PEFT, our developed model enables the large foundation
models to be adapted to challenging few-shot problems in a parameter-efficient and computationally
efficient manner with limited memory requirements and energy footprint. Moreover, the developed
model improves the generalization performance and the model’s predictions are calibrated ensuring
trustworthiness. Finally, the model has fine-grained uncertainty quantification capabilities which are
highly desirable when applying these models in real-world safety-critical scenarios. Overall, our
developed B-PEFT is expected to have a strong positive societal impact.
I Limitations and Future Work
In this work, we investigate the calibration of transformer-based foundation models under few-shot
adaptation using various parameter-efficient fine-tuning methods. We focus on fine-tuning supervised
pre-trained models for few-shot learning. We note that there are other self-supervised pre-trained
models that show promising results for various benchmark datasets. We investigate the calibration
of CLIP, a representative method, under few-shot adaptation using the two most popular parameter-
efficient fine-tuning methods: prompt and adapter. Our preliminary results demonstrate that the
few-shot performance does not consistently increase in comparison to zero-shot. Similarly, prompt-
based fine-tuning has relatively better calibration than adapter-based fine-tuning. As an extension
of this work, we will investigate the calibration performance of self-supervised foundation models.
Additionally, it could be interesting to study the calibration performance of PEFT for tasks beyond
image classification, e.g., to other modalities such as audio and language foundation models. For
instance, the ideas developed in this work could potentially be used in situations where the PEFT
leads to mis-calibrated models and the developed model requires trustworthy fine-grained uncertainty
quantification capabilities. If these data modalities are also modeled using transformers and follow
the parameter-efficient fine-tuning paradigm in performing downstream tasks, we expect the proposed
approach can benefit them in a similar way.
29NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our abstract and introduction present the paper’s contribution and scope, and
match the theoretical and empirical results in the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations of the proposed work have been discussed in Section I.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Complete proof of all the theoretical claims is presented.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Details for reproducibility along with link to the code is provided.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Only benchmark datasets and publicly available models are used for training
and evaluation.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Experimental setting and details are provided.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: All the key results present the mean and standard deviation of five trials.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Details of all compute resources used in experiments is provided
9.Code Of Ethics
30Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The work confirms to the NeurIPS Code of Ethics.
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: The societal impact is discussed in Section H.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The work poses no obvious high risk for misuse.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: References and citations are provided as required.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The link to the source code and resources is provided.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing and research with human subjects is involved in this research
work.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No crowdsourcing and research with human subjects is involved in this research
work.
31