Theoretical guarantees in KL for Diffusion Flow
Matching
Marta Gentiloni Silveri
École polytechnique
Route de Saclay, 91120 Palaiseau, France
marta.gentiloni-silveri@polytechnique.edu
Giovanni Conforti
Università degli Studi di Padova
Via Trieste, 63, 35131 Padova, Italia
giovanni.conforti@math.unipd.itAlain Durmus
École polytechnique
Route de Saclay, 91120 Palaiseau, France
alain.durmus@polytechnique.edu
Abstract
Flow Matching (FM) (also referred to as stochastic interpolants or rectified flows)
stands out as a class of generative models that aims to bridge in finite time the
target distribution ν⋆with an auxiliary distribution µ, leveraging a fixed coupling π
and a bridge which can either be deterministic or stochastic. These two ingredients
define a path measure which can then be approximated by learning the drift of its
Markovian projection. The main contribution of this paper is to provide relatively
mild assumptions on ν⋆,µandπto obtain non-asymptotics guarantees for Diffu-
sion Flow Matching (DFM) models using as bridge the conditional distribution
associated with the Brownian motion. More precisely, we establish bounds on the
Kullback-Leibler divergence between the target distribution and the one generated
by such DFM models under moment conditions on the score of ν⋆,µandπ, and a
standard L2-drift-approximation error assumption.
1 Introduction
A significant task in statistics and machine learning currently revolves around generating samples
from a target distribution that is only accessible via a dataset. To tackle this challenge, generative
models have become prominent as effective computational tools for learning to simulate new data.
Essentially, these models involve learning a generator capable of mapping a source distribution into
new approximate samples from the target distribution.
One of the most productive approaches to generative modeling is based on deterministic and stochas-
tic transport dynamics, that connect a target distribution with a base distribution. Typically, the target
distribution represents the data set from which we want to generate new samples, while the base dis-
tribution is one that can be easily simulated or sampled. Regarding the dynamics, they correspond to
SDEs Stochastic Differential Equations (SDEs) or Ordinary Differential Equations (ODEs), where the
drift (for SDEs) or velocity field (for ODEs) is determined by solving a regression problem. This re-
gression problem is usually addressed with appropriate neural networks and related training techniques
[SDWMG15a, OFLR21, FJNO20, DBMP19, CLT22, DBTHD21, SE19, LYB+23, GCBD19].
Among these methods, score-base generative models (SGMs) and in particular diffusion models based
on score matching [ SDWMG15b ,HJA20 ,SE20 ,SE19 ] was an important milestone. In a nutshell,
these models involve transforming an arbitrary density into a standard Gaussian model and consists
in learning the drift of the corresponding reversal process. More precisely, the idea is to first consider
38th Conference on Neural Information Processing Systems (NeurIPS 2024).an Ornstein-Uhlenbeck (OU) process (XOU
t)t∈[0,T], over a time interval [0, T],
dXOU
t=−(1/2)XOU
tdt+ dBt, XOU
0∼ν⋆, (1)
where (Bt)t⩾0is ad-dimensional Brownian motion. Then, the reversal process (← −XOU
t)t∈[0,T], which
is defined from the non-homogeneous SDE [And82]:
d← −XOU
t={(1/2)← −XOU
t+∇logpOU
T−t(← −XOU
t)}dt+ dBt, t∈[0, T],with← −XOU
0∼ν⋆POU
T,
(2)
allows the law of XOU
Tto be transported to ν⋆: [And82 , Equations 3.11, 3.12] show that← −XOU
T
has distribution ν⋆. The initialization ν⋆POU
Tin(2)is the distribution of XOU
Tdefined in (1). The
drift in (2)can be decomposed as the sum of a linear function and the score associated with the
density of XOU
T−twith respect to the Lebesgue measure, denoted by pOU
T−t. From the Tweedie identity,
this score is the solution to a regression problem that can be solved efficiently by score matching
[HD05 ,Hyv05 ,Vin11 ]. Learning this score at different times can also be formulated as a sequence
of denoising problems. Once the drift of the reversal process is learned or equivalently the scores
(∇logpOU
t)t∈[0,T], score-based generative models consist in following the reversal dynamics over
[0, T]or, more commonly, a discretization of it, starting with a sample from N(0,Id). The final
sample at time Tis then approximatively distributed according to ν⋆. Note that an approximation
is made even if the reversal dynamics were simulated exactly, because for full accuracy, the model
would need to start from a sample of ν⋆POU
T. However, it is well known that for sufficiently large T,
ν⋆POU
Tis (exponentially) close to N(0,Id).
When exploring diffusion models, it has been realized that the generation of approximate data samples
could also be achieved using an ODE instead of the reversal diffusion:
d← −xOU
t/dt= (1/2)(← −xOU
t+∇logpOU
T−t(← −xOU
t)).
Similarly to the drift of the reversal diffusion, the velocity fields at time t∈[0, T]associated with
this ODE is the sum of a linear function and the score of the density of XOU
T−t. This observation
has prompted the introduction of the Probability Flow ODE implementation of diffusion models
[CCL+23a]. SGMs in their standard and probability flow ODE implementations have achieved
notable success in a range of applications; see e.g., [RBL+22, RDN+22, PVG+21].
While diffusion-based methods have now become popular generative models, they can suffer from
two limitations. First, there is a trade-off in selecting the time horizon Tand second, they rely solely
on Gaussian distributions as base distributions, in general. Therefore, there remains considerable
interest in developing methods that consider a more general base distribution µand that accomplish
the transport between ν⋆andµrelying on dynamics defined on a fixed finite time interval. Defining a
generative process in finite time by means of a coupling and an interpolating process, [ Pel22 ], laid the
foundation for Flow Matching (FM) models [ A VE22 ,ABVE23 ,LCBH+23,Liu22 ,LGL23 ], finally
addressing this problem.
In its simplest form, the main strategy employed by FMs to bridge two distributions, involves a fixed
coupling πbetween ν⋆andµand the use of a bridge, i.e., a conditional distribution on the path space
C([0,1],Rd)of a reference process (Rt)t∈[0,1]given its starting point R0and end point R1. In case
Rtis a deterministic function of R0andR1, we say that the bridge is deterministic and stochastic
otherwise. As (Rt)t∈[0,1]corresponds to the solution of a stochastic differential equation, we coin
the term Diffusion Flow Matching (DFM) to distinguish the latter case from the former one and
focus on it. Then, this bridge and the coupling πbetween ν⋆andµdefine an interpolated process
(XI
t)t∈[0,1], referred to as an interpolant, defined as (XI
0, XI
1)∼πand(XI
t)t∈[0,1]given (XI
0, XI
1)
has the same conditional distribution as (Rt)t∈[0,1]given (R0, R1). However, (XI
t)t∈[0,1]does not
correspond in general to the distribution of a diffusion or even to the one of a Markov process. This
characteristic poses a challenge when dealing with potential stochastic sampling procedures: indeed,
similarly to SGMs, FMs and DFMs aim to design a Markov process that approximatively transport µ
toν⋆. To address this issue, most works proposing FM and DFM models [ SBCD23 ,LWYql23 ] rely
on mimicking the marginal flow of the interpolated process (XI
t)t∈[0,1]through a diffusion process
known as the Markovian projection. A remarkable feature of this diffusion lies in the fact that its
drift is also a solution of a regression problem that can be approximatively solved using only samples
from the interpolant (XI
t)t∈[0,1]. Then, an approximate samples from ν⋆is obtained by following a
discretization of the dynamics associated with the considered Markovian projection, starting with a
sample of µ.
2While there exists now an important literature on theoretical guarantees for SGMs [ CDS23 ,CLL23 ,
CCL+23b,PMM23 ,LLT23 ,Bor22a ], only a few works have been considering FMs. In addition, up
to our knowledge, these works on FMs only consider deterministic interpolants [ A VE22 ,BDD23 ,
GHJZ24 ]. The main objective of this paper is to fill this gap and to analyze DFMs using the
d-dimensional Brownian motion as reference process, in which case the bridge is simply the d-
dimensional standard Brownian bridge. We provide theoretical guarantees, upper bounding the
Kullback Leibler divergence between the target distribution and the one resulting from the DFM. Our
results consider the two sources of error coming from the DFM model, namely drift-estimation and
time-discretization. This pursuit underscores the significance of comprehending and quantifying the
factors influencing the performance of DFMs, paving the way for further advancements in generative
modeling techniques.
Our contribution. In this work, we analyze a DFM model using as bridge the d-dimensional
Brownian bridge and examine how it performs in two distinct scenarios: one without early-stopping
and another with early-stopping. In our first main contribution Theorem 2, we establish an explicit
and simple bound on the KLdivergence between the data distribution and the distribution at the
final time of the DFM model. We achieve our bound without early stopping, by assuming only
(1) moment conditions on the target ν⋆and the base µ(H1); (2) integrability conditions on the
scores associated with the data distribution ν⋆, the base distribution µand the coupling π(H2); (3) a
L2-drift-approximation error ( H3) (an assumption commonly considered in previous works). Note
that condition (2) implies that ν⋆necessarily admits a density. We relax this condition in our second
contribution. In Theorem 3, we establish an explicit bound on the KLdivergence between a smoothed
version of the target distribution and an early stopped version of the DFM model, assuming (1) and
(3), but replacing the condition (2) by assuming (4) π=µ⊗ν⋆and integrability conditions only on
the score associated with µ.
To the best of our knowledge, our paper provides the first convergence analysis for diffusion-type
FMs, that tackles all the sources of error, i.e., the drift-approximation-error and the time-discretization
error. In addition, previous studies concerning FMs and Probability Flow ODEs, with deterministic
or mixed sampling procedure, either rely on at least some Lipschitz regularity of the flow velocity
field or its estimator and/or do not take the time-discretization error into consideration. Also, in the
context of SGMs with constant step-size, most of existing works without early-stopping are obtained
assuming either the score (or its estimator) to be Lipschitz or the data distribution to satisfy some
additional conditions (e.g., manifold hypothesis, bounded support, etc.); the unique exception being
[CDS23]. We refer to Section 3.2 for a more in depth literature comparison.
Notation. Given a measurable space (E,E), we denote by P(E)the set of probability measures of
E. Also, given a topological space (E, τ), we use B(E)to denote the Borel σ-algebra on E. Given two
random variables Y,˜Y, we write Y⊥ ⊥˜Yto say that Yand˜Yare independent. Denote by (Bt)t∈[0,1]
ad-dimensional Brownian motion. We denote by Lebdthe Lebesgue measure on Rd. Given two real
numbers u, v∈R, we write u≲v(resp. u≳v) to mean u≤Cv(resp. u≥Cv) for a universal
constant C >0. Also, we denote by ∥x∥the Euclidean norm of x∈Rd, by⟨x, y⟩the scalar product
between x, y∈Rd, and by xTthe transpose of x. Given a matrix A∈Rd×s, we denote by ∥A∥op
the operator norm of A. Forf: [0,1]×Rd→Rregular enough, we denote by ∇xf(t, x),∇2
xf(t, x)
and∆xf(t, x)respectively the gradient, hessian and laplacian of f, defined for t, x∈[0,1]×Rdby
∇xf(t, x) := ( ∂xif(t, x))i,∇2
xf(t, x) := ( ∂xi∂xjf(t, x))i,j,∆f(t, x) :=Pd
i=1∂2
xif(t, x), where
∂xjdenotes the partial derivative with respect to the j-th variable. For F: [0,1]×Rd→Rdregular
enough, we denote by DxF,divxFand∆xFrespectively, the Jacobian matrix, the divergence
and the vectorial laplacian of F, defined for t, x∈[0,1]×RdbyDxF(t, x) = ( ∂xjFi(t, x))i,j,
divxF(t, x) :=Pd
j=1∂xjFj(t, x),∆xF(t, x) = (∆ xF1(t, x), ...,∆xFd(t, x)).
2 Diffusion Flow Matching.
Given a target distribution ν⋆∈ P(Rd)and a base distribution µ∈ P(Rd), the idea at the core
of FM models is intuitively to construct a path between these two by considering two ingredients
(1) a coupling πand (2) a bridge (or an interpolant following [ A VE22 ]) between µandν⋆(or
more precisely a bridge with foundations π). More formally, here we say that πis a coupling
3between µandν⋆if for any A∈ B(Rd),π(A×Rd) =µ(A)andπ(Rd×A) =ν⋆(A), and
denote by Π(µ, ν⋆)the set of coupling between µandν⋆. Then, based on a probability measure
onW= C([0 ,1],Rd)the set of continuous functions from [0,1]toRd, we define the bridge
bQassociated with Qas the Markov kernel bQonR2d×W, such that, for any A∈ B(W),
Q(A) =R
Q0,1d(x0, x1)bQ((x0, x1),A)(see e.g., [ Kle13 , Theorem 8.37] for the existence of this
kernel), where for any I={t1, . . . , t n} ⊂[0,1],t1<···< tn,QIis the I-marginal distribution
ofQ,i.e., the pushforward measure of Qby(xt)t∈[0,1]7→(xt1, . . . , x tn). From a probabilistic
perspective, it implies that if (Wt)t∈[0,1]∼Q, then bQis a conditional distribution of (Wt)t∈[0,1]∼
Qgiven (W0, W1): for any bounded and measurable function on W,E[f((Wt)t∈[0,1])|X0, X1] =R
f((wt)t∈[0,1])bQ((W0, W1),d(wt)t∈[0,1]).
2.1 Definition of the interpolated process
Consider now a coupling πand a bridge bQβassociated to Qβ∈ P(W). We suppose here that Qβis
the distribution of (Yt)t∈[0,1]solution of the stochastic differential equation
dYt=β(Yt)dt+√
2dBt, t∈[0,1], Y 0∼µ∈ P(Rd), (3)
where (Bt)t∈R+is a standard d-dimensional Brownian motion. In addition, we suppose β∈
C∞(Rd,Rd)for simplicity and that (3)admits a unique strong solution. Consider now, the interpo-
lated measure I(π,Qβ)1corresponding to the distribution of the process defined by (XI
0, XI
1)∼π
and(XI
t)t∈[0,1]|(XI
0, XI
1)∼bQβ((XI
0, XI
1),·). In [ ABVE23 ],(XI
t)t∈[0,1]is referred to as a stochas-
tic interpolant. Denote by (s, t, x, y )7→pY
t|s(y|x)the conditional density of Ytgiven Yswith respect
to the Lebesgue measure and by (pI
t)t∈[0,1]the time marginal densities of (XI
t)t∈[0,1]with respect
to the Lebesgue measure, that is P(Yt∈A|Ys) =R
Apt|s(x|Ys)dxandP(XI
t∈A) =R
ApI
t(x)dx,
forA∈ B(Rd)ands, t∈[0,1],s̸=t. Then, note that, as a straightforward consequence of the
definition of (XI
t)t∈[0,1], it holds
pI
t(x) =Z
R2dpY
t|0(x|x0)pY
1|t(x1|x)˜π(dx0,dx1), (4)
where
˜π(dx0,dx1) =π(dx0,dx1)
pY
1|0(x1|x0).
An example that we will focus on in this paper is Qβ=Bthe distribution of the Brownian motion
(√
2Bt)t∈Rsolution of (3)withβ≡0. Then, it is well known that bBis then the Markov kernel
associated with the Brownian bridge and the resulting stochastic interpolant satisfies for any t∈[0,1]
XI
tdist= (1−t)XI
0+tXI
1+p
2t(1−t)Z,Z∼N(0,Id), (5)
wheredist=denotes the equality in distribution.
It is well-known that for any x0, x1∈Rd, the distribution bQβ((x0, x1),·)is diffusion-like under
appropriate conditions. More precisely, bQβ((x0, x1),·)is the distribution of (Yt)t∈[0,1]solution to
dYt={β(Yt) + 2∇ϕx1
t(Yt)}dt+√
2dBt, t∈[0,1],Y0=x0, (6)
where ϕx1
t(y) = log pY
1|t(x1|y). For instance, for x0, x1∈Rd, the bridge bB((x0, x1),·)associated
toBis the distribution of a Brownian bridge (¯Bx0,x1
t)t∈[0,1]solution to the SDE
d¯Bx0,x1
t =x1−¯Bx0,x1
t
1−tdt+√
2dBt, t∈[0,1],¯Bx0,x1
0 =x0.
From (6), it turns out that (XI
t)t∈[0,1]given (XI
0, XI
1)is therefore solution to
dXI
t={β(XI
t) + 2∇ϕXI
1
t(XI
t)}dt+√
2dBt, t∈[0,1], XI
0∼µ . (7)
1It corresponds to π⊗bQβthe tensor product between πand bQβ:I(π,Qβ)(A) =R
bQβ((x0, x1),A)dπ(x0, x1),A∈ B(Rd). In other words, π-mixture of Qβ-bridges.
4Note that the drift coefficient in (7) depends on XI
1, and therefore, (XI
t)t∈[0,1]is not Markov, which
is a natural property if we are interested in constructing a generative process. To circumvent this
issue, based on I(π,Qβ), we aim to define a distribution M(π,Qβ)such that it has the same one-
dimensional time marginals as I(π,Qβ)and corresponds to a diffusion, i.e., if(XI
t)t∈[0,1]∼I(π,Qβ)
and(XM
t)t∈[0,1]∼M(π,Qβ), for any t∈[0,1],XI
tdist=XM
tand(XM
t)∈Nis solution of a diffusion
with Markov coefficient. This can be done trough the Markovian projection.
2.2 Markovian projection and Diffusion Flow Matching
Markovian projection. The idea of Markovian projection originally dates back to [ Gyö86 ] and
[Kry]. Its main idea is in essence to define a diffusion Markov process which “mimics” the time-
marginal of an Itô process:
dXt=btdt+√
2dBt, t∈[0,1],X0∼µ ,
for some adapted process (bt)t∈[0,1]and initial distribution µ. Under appropriate conditions (see
[BS13 , Corollary 3.7]), this diffusion process, denoted by (XM
t)t∈[0,1], exists and is solution to an
SDE with a (relatively) simple modification of the drift bt, namely
dXM
t=˜bt(XM
t)dt+√
2dBt, t∈[0,1],XM
0∼µ ,
where ˜bt(XM
t) =E[bt|XI
t]. This result can be applied to the non-Markov process (XI
t)t∈[0,1]
solution of (7): its Markovian projection is solution of
dXM
t=˜βt(XM
t)dt+√
2dBt, t∈[0,1],
for some function ˜β: [0,1]×Rd→Rd. It turns out that we can identify ˜β, relying on the family
of conditional densities (pY
t|s)0≤s≤t≤1and marginal densities (pI
t)0≤t≤1. This is the content of the
following result.
Theorem 1. Consider a π∈Π(µ, ν⋆)andQβassociated with (3). Consider the drift field
˜βY
t(x) =β(x) + 2R
∇xlogpY
1|t(x1|x)pY
t|0(x|x0)pY
1|t(x1|x)˜π(dx0,dx1)
pI
t(x). (8)
Under appropriate conditions (see Appendix A.1), the Markov process (XM
t)t∈[0,1]solution of
dXM
t=˜βY
t(XM
t)dt+√
2dBt, t∈[0,1), XM
0∼µ , (9)
mimics the one-dimensional time marginals of I(π,Qβ), i.e., for any t∈[0,1),XI
tdist=XM
t.
This result is well-know, but, for sake of completeness, we provide the proof in Appendix A.1.
The process (9)is known as the Markovian projection ofI(π,Qβ), and its drift (8)asmimicking drift.
In what follows, we denote by M(π,Qβ)the distribution of (XM
t)t∈[0,1]onW.
Remark 1.It can be easily shown by continuity that XM
t=XI
t⇒XI
1fort→1, where ⇒denotes
the convergence in distribution. As Law(XI
1) =ν⋆,the Markovian projection therefore gives a an
ideal generative model which would consist in following the SDE (9) with initial distribution µ.
Remark 2.Note that, because of (4), the mimicking drift (8) rewrites as
˜βY
t(XI
t) =Eh
2∇xlogpY
1|t(XI
1|XI
t) +β(XI
t)|XI
ti
.
Diffusion Flow Matching. Eventually, as pointed out in Remark 1, the Markovian projection
gives a an ideal generative model. However, a) the mimicking drift (8)is intractable and b) the
continuous-time SDE (9)can not be numerically simulated. Thus, in order to implement the proposed
theoretical idea, we first need to address and overcome the aforementioned computational challenges.
To circumvent a), observe that, because of Remark 2 and [ Kle13 , Corollary 8.17], we can approximate
the mimicking drift via solving
min
θ∈ΘEhsY
θ(t, XI
t)−˜βY
t(XI
t)2i
, (10)
5for a properly chosen class of neural networks {(t, x)7→sY
θ(t, x)}θ∈Θ, and replace ˜βYin(9)
withsY
θ⋆, where θ⋆∈Θdenotes a minimizer of (10). To deal with b), we simply make use of
the Euler-Maruyama scheme, i.e., for a choice of sequence of step sizes {hk}N
k=1,N⩾1, and
the corresponding time discretization tk=Pk
i=1hi, such that t0= 0 andtN= 1, we define the
continuous process (Xθ⋆
t)t∈[0,1]recursively on the intervals [tk, tk+1]by
dXθ⋆
t=sY
θ⋆(tk, Xθ⋆
tk)dt+√
2dBt, t∈[tk, tk+1],with Xθ⋆
0∼µ . (11)
(11) is the DFM generative model we are going to analyze.
3 Main results
In this section, we provide convergence guarantees in Kullback-Leibler divergence for the Diffusion
Flow Matching model (11), under mild assumptions on the µ, ν⋆, πandsθ⋆, either within a non-
early-stopping regime or within a early-stopping regime.
From now on, we consider the case β≡0,i.e.,Qβ=B. We show in Appendix A.2, Remark 9,
that, under out set of assumptions, the conditions of Theorem 1 hold for this setup. Moreover, in this
case, for any s, t∈[0,1],s < t andx, y∈Rd, the conditional density pY
t|s(y|x)≡pt−s(y|x)where
(t, x, y )7→pt(y|x)is the heat kernel:
pt(y|x) =1
(4πt)d/2exp
−∥y−x∥2
4t
, t∈(0,1]. (12)
In the following, we set ˜βY≡˜βandsY
θ⋆≡sθ⋆.
3.1 Convergence Bounds.
We assume moment conditions on the probability measures µandν⋆, and mild integrability
conditions on the probability distributions µ, ν⋆and on the coupling π.
Forp⩾1, we denote for ζ∈ P(Rd),
mp[ζ] =Z
∥x∥pdζ(x).
H1. The probability distributions µ, ν⋆satisfy m8[µ] +m8[ν⋆]<+∞.
H2. The probability distributions ν⋆, µandπare absolutely continuous with respect to the Lebesgue
measure on RdandR2drespectively, and satisfy
(i)The functions log dµ/dLebdandlog dν⋆/dLebdare continuously differentiable and satisfy
∥∇logν⋆∥8
L8(ν⋆)+∥∇logµ∥8
L8(µ)<+∞where for ζ∈ {ν⋆, µ},
∥∇logζ∥8
L8(ζ)=Z∇logdζ
dLebd
(x0)8
dζ(x0).
(ii)The function log dπ/dLeb2dis continuously differentiable and satisfies ∥∇log ˜π∥8
L8(π)<+∞
where
∥∇log ˜π∥8
L8(π)=Z
∥∇log (˜π) (x0, x1)∥8dπ(x0, x1),˜π(x0, x1) =1
p1(x1|x0)dπ
dLeb2d(x0, x1),
(13)
andp1is defined in (12).
Remark 3.Under H1, note that ∥∇log ˜π∥8
L8(π)<+∞is equivalent by (12) and π∈Π(µ, ν⋆)to
Z
∥∇log(d π/dLeb2d)(x0, x1)∥8dπ(x0, x1)<+∞.
6Remark 4.We can relax the condition that log dµ/dLebd,log dν⋆/dLebdandlog dπ/dLeb2dare
continuously differentiable assuming that8q
dµ/dLebd,8q
dν⋆/dLebdand8q
dπ/dLeb2dbelongs
to some Sobolev space, but, for ease of presentation, we prefer not to delve into these technical
details.
Moreover, we assume to have estimated the mimicking drift with an ε2-precision, for some ε2>0
sufficiently small.
H3. There exist θ⋆∈Θandε2>0such that
N−1X
k=0hk+1Ehsθ⋆(tk, XM
tk)−˜βtk(XM
tk)2i
≤ε2.
Remark 5.To be coherent with the previous section, observe that, as a consequence of Theorem 1,
for any k= 0,···, N, it holds
Ehsθ⋆(tk, XM
tk)−˜βtk(XM
tk)2i
=Ehsθ⋆(tk, XI
tk)−˜βtk(XI
tk)2i
.
Under such assumptions, we derive an upper bound on the KL divergence between the data distribution
ν⋆and the output of the DFM (11):
Theorem 2. Consider a uniform partition of [0,1]with a constant stepsize hk≡h,h= 1/Nh>0,
forNh∈N∗and consider the corresponding process (Xθ⋆
t)t∈[0,1]defined in (11). Assume H1 to 3.
Denoting by νθ⋆
1the distribution of Xθ⋆
1, we have that
KL(ν⋆|νθ⋆
1)≲ε2+h(h1/8+ 1)
d4+m8[µ] +m8[ν⋆] +∥∇log ˜π∥8
L8(π)
+∥∇logµ∥8
L8(µ)+∥∇logν⋆∥8
L8(ν⋆)
.(14)
Remark 6.Under almost the same conditions as Theorem 2, i.e.,H1H2,H3, replacing ˜πin(13) by
˜πT(x0, x1) =1
pT(x1|x0)dπ
dLeb2d(x0, x1), (15)
our proofs apply also to DFM using a time horizon T > 0and the Brownian bridge on [0, T]. In
particular, we would have obtained similar bounds as the ones derived in Theorem 2 but with a factor
max(1 , T8)in front of the second addend.
Remark 7.Choosing, in Theorem 2,
Nh=d4+m8[µ] +m8[ν⋆] +∥∇log ˜π∥8
L8(π)+∥∇logµ∥8
L8(µ)+∥∇logν⋆∥8
L8(ν⋆)
ε2
makes the approximation error of order O(ε2)and the complexity of order O(ε−2).
Using an early-stopping procedure, we obtain in the case πis the independent coupling:
Theorem 3. Fix0< δ < 1/2. Consider a uniform partition of [0,1]with a constant stepsize
hk≡h,h= 1/Nh>0, forNh∈N∗and consider the corresponding process (Xθ⋆
t)t∈[0,1]defined
in(11). Assume H1,H3 and π=µ⊗ν⋆to be the independent coupling. Suppose in addition that
µis absolutely continuously with respect to the Lebesgue measure, log dµ/dLebdis continuously
differentiable and satisfies ∥∇logµ∥8
L8(µ)<+∞.
Then, denoting by ν⋆
1−δandνθ⋆
1−δthe distribution of XM
1−δandXθ⋆
1−δrespectively, we have that
KL(ν⋆
1−δ|νθ⋆
1−δ)≲ε2+h(h1/8+ 1)d4
δ4+m8[µ] +m8[ν⋆]1
δ8+∥∇logµ∥8
L8(µ)
.
Corollary 1. Fixδ=O(√ε). Consider a uniform partition of [0,1]with a constant stepsize
h=O(min( ε4/d4, ε6)), and consider the corresponding process (Xθ⋆
t)t∈[0,1]defined in (11).
Assume H1,H3 and π=µ⊗ν⋆to be the independent coupling. Suppose in addition that µ
7is absolutely continuously with respect to the Lebesgue measure, log dµ/dLebdis continuously
differentiable and satisfies ∥∇logµ∥8
L8(µ)<+∞.
Then, denoting by ν⋆
1−δandνθ⋆
1−δthe distribution of XM
1−δandXθ⋆
1−δrespectively, we have that
W2
2,FM(ν⋆
1−δ|νθ⋆
1−δ)≲ε2,
where W2,FMdenotes the Fortet- Mourier distance of order 2, i.e.
W2
2,FM(µ, ν) = inf
π∈Π(µ,ν)Z
min{∥x−y∥2,1}dπ(x, y).
3.2 Related works and comparison with existing literature.
FMs stand at the forefront of innovation in generative modeling, offering a practical solution to
the longstanding challenge of bridging two arbitrary distributions within a finite time interval.
Their consequent immense potential has prompted substantial research efforts aimed at providing
a theoretical explanation for their effectiveness and has put SGMs and Probability Flow ODEs
all in perspective. In this section we report and discuss previous researches on FMs, SGMs and
Probability Flow ODEs with the purpose of highlighting the links and differences with our work and
contextualizing our contribution.
Non-early-stopping setting. In the context of FMs, [ A VE22 ] and [ BDD23 ] seek convergence guar-
antees in 2-Wasserstein distance. Both consider more general designs for the stochastic interpolant
and a deterministic sampling procedure, rather than a stochastic one. However, both works rely on
some regularity condition on the approximated flow velocity filed, i.e., that it is Lipschitz. Namely,
[A VE22 ] works under a K-Lipschitz (uniform in time and space) assumption on the estimator of
the exact flow velocity field. How such assumption pertains to the flow matching framework for
generative modeling is not articulated and remains unclear. On the other hand, [ BDD23 ] assumes the
estimator of the exact flow velocity field to be Lt-Lipschtz for any t∈[0,1]and discuss in [ BDD23 ,
Theorem 2] how such assumption relate to the setting : under the additional assumption [ BDD23 ,
Assumption 4], the true flow velocity field is proven to be Lt-Lipschitz in space for any t∈[0,1].
Therefore, [ BDD23 ] enhances the findings of [ A VE22 ]. However, [ BDD23 , Assumption 4] is not an
usual conditions considered in papers about convergence guarantees for SGM and it is unclear which
type of distributions satisfy [ BDD23 , Assumption 4]. Moreover, both works [ BDD23 ,A VE22 ] do
not take into account the discretization error in their analysis.
In the context of Probability Flow ODEs, [ LWCC24 ] and [ GZ24 ] investigate the performance of such
models in Total Vartiation distance and 2-Wasserstein distance. In contrast to [ BDD23 ] and [ A VE22 ],
[LWCC24 ] and [ GZ24 ] examine the error coming from the (prerequisite when implementing an
algorithm) introduction of a time-discretization scheme. However, once again, the provided bounds
work under smoothness assumptions either on the score or on its estimator. More precisely, the
result reported in [ LWCC24 ] depends on a small L2-Jacobian-estimation error assumption, besides a
classical small L2-score-estimation error assumption. As for [ GZ24 ], they assume the score to be
Lipschitz in time and the data to be smooth and log-concave. In contrast, our result do not make such
assumptions. Finally, to the best of our knowledge, the recent work [ CDS23 ] represents the state
of art in the context of SGMs without early-stopping procedure: [ CDS23 , Theorem 2.1] provides
a sharp bound in KLdivergence between the data distribution and the law of the SGM both in
the overdamped and kinetic setting under the sole assumptions of an L2-score-approximation error
and that the data distribution has finite Fisher information with respect to the standard Gaussian
distribution. All previous results are obtained assuming either some Lipschitz condition on the
score and/or its estimator ([ CCL+23b], [CLL23 ]) or a manifold hypothesis on the data distribution
([Bor22b ]). However, we underline that the FM framework enables to consider a significantly wider
range of interpolating paths compared to SGMs and to avoid the trade-off concerning the time horizon
Twhich is inevitable when dealing with SGMs.
Early-stopping procedure. The recent work [ GHJZ24 ] establishes convergence guarantees in
2-Wasserstein distance for FMs based on a deterministic sampling procedure. However, the results of
[GHJZ24 ] requires to interpolate with a Gaussian distribution and applies only to data distributions
which either have a bounded support, are strongly log-concave, or are the convolution between a
Gaussian and an other probability distribution supported on an Euclidean Ball. In contrast, for our
bound to hold true we only need the data distribution ν⋆and its score to have finite eight-order
8moment. Furthermore, even if [ GHJZ24 ] goes into depth when dealing with the statistical analysis
of the estimator and the L2-estimation error, the entire investigation therein pursued depends on the
choice of ReLUnetworks with Lipschitz regularity control to approximate the velocity field. They
motivate such choice by proving (see [ GHJZ24 , Theorem 5.1]) Lipschitz properties in time and space
of the true velocity field under the aforementioned assumptions on the data. On the contrary, we
do not assume any regularity on the estimator of the mimicking drift. In the context of Probability
Flow ODEs, [ CCL+23a] provides bound in Total Variation distance, but assuming both the score
and its estimator to be Lipschitz in space. So, also in the early-stopping regime, our bound improves
previously obtained one.
To conclude, in the context of SGMs, [ CLL23 ,CDS23 ,BDBDD23 ] are able to cover any data
distribution with bounded second moment at the cost of using exponentially decreasing step-sizes.
However, [ CDS23 , Corollary 2.4] and [ BDBDD23 ] improves upon [ CLL23 , Theorem 2.2]: the term
that takes track of the time-discretization error is linearly dependent on the dimension din the former
works, whereas quadratically dependent on din the latter.
3.3 The proposed methodology.
In what follows, we provide a sketch of the proofs of Theorem 2 and Theorem 3 in order to outline
and delineate our methodology.
The starting point of our proof of Theorem 2 is the following (by now) standard [ CCL+23b,CLL23 ,
CDS23] decomposition of the KL divergence which is derived from Girsanov theorem:
KL(ν⋆|νθ⋆
1)≤KL(M(π,B)|Law( Xθ⋆
[0,1]))≲N−1X
k=0Ztk+1
tkEhsθ⋆(tk, XM
tk)−˜βt(XM
t)2i
dt
≲ε2+N−1X
k=0Ztk+1
tkEh˜βtk(XM
tk)−˜βt(XM
t)2i
dt ,
where, for the first inequality, we used the data processing inequality [ Nut21 , Lemma 1.6] and the
last inequality follows from the triangle inequality and the assumption H3. In order to conclude,
we should bound the L2norm of the adjoint process in the Pontryagin system associated with the
Markovian projection of the interpolant. We do so by introducing a novel quantity in the generative
model literature (see [ Kre97 ]), namely the so-called reciprocal characteristic of the mimicking drift,
i.e.,
(∂t+LM
t)˜βt,
where LMdenotes the generator of (XM
t)t∈[0,1]. This quantity may be viewed as some sort of mean
acceleration field and guides the time evolution of the mimicking drift, as
d˜βt(XM
t) = (∂t+LM
t)˜βt(XM
t)dt+√
2Dx˜βt(XM
t)dBt, t∈[0,1].
The main efforts in our proof are directed towards bounding the L2norm of the reciprocal charac-
teristic whose representation in terms of either conditional moments or higher-order logarithmic
derivatives of conditional densities is quite intricate, see (39). Trying to bound each of these terms
separately requires strong assumptions on the initial distributions and couplings leading to sub-
optimal results. However, using integration by parts both in time and space and a double change of
measure argument, and profiting from symmetry properties of the heat kernel, we managed to bound
these terms under assumptions comparable to the minimal ones required in the analysis of SGMs.
Note that the analysis of the reciprocal characteristic is not required for SGMs (it is always 0) and
that controlling it also requires new tricks and ideas, since its representation contains up to three
logarithmic derivatives of conditional distributions, whereas the analysis of SGMs requires at most
two such derivatives to be analyzed.
Regarding the proof of Theorem 3, we consider the interpolated process (XI
t)t∈[0,δ]restricted to
[0,1−δ]. Denoting by π1−δ, the coupling between µandν⋆
1−δcorresponding to the distribution
of the couple (XI
0, XI
1−δ). By the property of the Brownian bridge, (XI
t)t∈[0,1−δ]is a stochastic
interpolant resulting from π1−δand the Brownian bridge on [0,1−δ]. Therefore based on Remark 6,
9we only have to show ν⋆
1−δandπ1−δsatisfy H1 and H2, replacing ˜πinH2 by˜π1−δdefined in (15).
More precisely, we show that they hold and that
∥∇log ˜π1−δ∥8
L8(π1−δ)⩽∥∇logµ∥8
L8(µ)+m8[µ]1
(1−δ)8+m8[ν⋆]1
δ8+d4 1
δ4(1−δ)4
∇logν⋆
1−δ8
L8(ν⋆
1−δ)⩽m8[µ]1
(1−δ)8+m8[ν⋆]1
δ8+d4 1
δ4(1−δ)4.
4 Conclusion
In this work, we have investigated a Diffusion Flow Matching model built around the Markovian
projection of the d-dimensional Brownian bridge between the data distribution ν⋆and the base
distribution µ. In particular, we have derived convergence guarantees in Kullback-Leibler divergence,
which take account of all the sources of error - time-discretization error and drift-estimation error -
that arise when implementing the model, and which hold under mild moments conditions on µ,ν⋆,
the scores of µ,ν⋆and the score of the coupling πbetween µandν⋆. However, there are several
questions remaining open. First, it would be worthy to understand if we could lower the order of
integrability of the score associated with µ, νandπ. Second, it would be interesting to complement
our analysis by a statistical analysis of DFM (11), similarly to what have been achieved in [ GHJZ24 ]
for a particular deterministic FM model. Finally, it would be valuable to obtain better dimension
dependence with respect to the space dimension dwhen applying early-stopping procedure.
Acknowledgments and Disclosure of Funding
The work of Marta Gentiloni-Silveri has been supported by the Paris Ile-de-France Région in the
framework of DIM AI4IDF. The work by Alain Durmus is partially funded by the European Union
(ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the
author(s) only and do not necessarily reflect those of the European Union or the European Research
Council Executive Agency. Neither the European Union nor the granting authority can be held
responsible for them.
References
[ABVE23] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic
interpolants: A unifying framework for flows and diffusions. arXiv preprint
arXiv:2303.08797 , 2023.
[And82] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes
and their Applications , 12(3):313–326, 1982.
[A VE22] Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with
stochastic interpolants. arXiv preprint arXiv:2209.15571 , 2022.
[BB17] Paolo Baldi and Paolo Baldi. Stochastic calculus . Springer, 2017.
[BDBDD23] Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Linear
convergence bounds for diffusion models via stochastic localization. arXiv preprint
arXiv:2308.03686 , 2023.
[BDD23] Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow
matching methods. arXiv preprint arXiv:2305.16860 , 2023.
[BKRS15] Vladimir I. Bogachev, Nicolai V . Krylov, Michael Röckner, and Stanislav V . Sha-
poshnikov. Fokker-planck-kolmogorov equations. 2015.
[Bor22a] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold
hypothesis. Transactions on Machine Learning Research , 2022. Expert Certification.
[Bor22b] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold
hypothesis. Transactions on Machine Learning Research , 2022. Expert Certification.
10[BS13] Gerard Brunick and Steven Shreve. Mimicking an Itô process by a solution of a
stochastic differential equation. The Annals of Applied Probability , 23(4):1584 –
1628, 2013.
[CCL+23a] Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim.
The probability flow ODE is provably fast. In Thirty-seventh Conference on Neural
Information Processing Systems , 2023.
[CCL+23b] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R. Zhang.
Sampling is as easy as learning the score: theory for diffusion models with minimal
data assumptions. International Conference on Learning Representations , 2023.
[CDS23] Giovanni Conforti, Alain Durmus, and Marta Gentiloni Silveri. Score diffusion
models without early stopping: finite fisher information is all you need. arXiv
preprint arXiv:2308.12240 , 2023.
[CLL23] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based
generative modeling: User-friendly bounds under minimal smoothness assumptions.
InInternational Conference on Machine Learning , pages 4735–4763. PMLR, 2023.
[CLT22] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood train-
ing of schrödinger bridge using forward-backward SDEs theory. In International
Conference on Learning Representations , 2022.
[DBMP19] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline
flows. Advances in neural information processing systems , 32, 2019.
[DBTHD21] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion
schrödinger bridge with applications to score-based generative modeling. Advances
in Neural Information Processing Systems , 34:17695–17709, 2021.
[FJNO20] Chris Finlay, Jörn-Henrik Jacobsen, Levon Nurbekyan, and Adam Oberman. How to
train your neural ode: the world of jacobian and kinetic regularization. In Interna-
tional conference on machine learning , pages 3154–3164. PMLR, 2020.
[GCBD19] Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, and David Duvenaud. Scalable
reversible generative models with free-form continuous dynamics. In International
Conference on Learning Representations , page 7, 2019.
[GHJZ24] Yuan Gao, Jian Huang, Yuling Jiao, and Shurong Zheng. Convergence of con-
tinuous normalizing flows for learning probability distributions. arXiv preprint
arXiv:2404.00551 , 2024.
[Gyö86] István Gyöngy. Mimicking the one-dimensional marginal distributions of processes
having an itô differential. Probability theory and related fields , 71(4):501–516, 1986.
[GZ24] Xuefeng Gao and Lingjiong Zhu. Convergence analysis for general probability flow
odes of diffusion models in wasserstein distances. arXiv preprint arXiv:2401.17958 ,
2024.
[HD05] Aapo Hyvärinen and Peter Dayan. Estimation of non-normalized statistical models
by score matching. Journal of Machine Learning Research , 6(4), 2005.
[HJA20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models,
2020.
[Hyv05] Aapo Hyvärinen. Estimation of non-normalized statistical models by score matching.
Journal of Machine Learning Research , 6(24):695–709, 2005.
[Kle13] Achim Klenke. Probability theory: a comprehensive course . Springer Science &
Business Media, 2013.
[Kre97] Arthur J Krener. Reciprocal diffusions in flat space. Probability theory and related
fields , 107(2):243–281, 1997.
11[Kry] N. V . Krylov. On the relation between differential operators of second order and the
solutions of stochastic differential equations. Steklov Seminar .
[LCBH+23]Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew
Le. Flow matching for generative modeling. In The Eleventh International Confer-
ence on Learning Representations , 2023.
[LGL23] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to
generate and transfer data with rectified flow. International Conference on Learning
Representations (ICLR) , 2023.
[Liu22] Qiang Liu. Rectified flow: A marginal preserving approach to optimal transport.
arXiv preprint arXiv:2209.14577 , 2022.
[LLT23] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative
modeling for general data distributions. In International Conference on Algorithmic
Learning Theory , pages 946–985. PMLR, 2023.
[LWCC24] Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards non-asymptotic conver-
gence for diffusion-based generative models. In The Twelfth International Conference
on Learning Representations , 2024.
[LWYql23] Xingchao Liu, Lemeng Wu, Mao Ye, and qiang liu. Learning diffusion bridges
on constrained domains. In The Eleventh International Conference on Learning
Representations , 2023.
[LYB+23]Sungbin Lim, EUN BI YOON, Taehyun Byun, Taewon Kang, Seungwoo Kim,
Kyungjae Lee, and Sungjoon Choi. Score-based generative modeling through
stochastic evolution equations in hilbert spaces. In A. Oh, T. Naumann, A. Glober-
son, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information
Processing Systems , volume 36, pages 37799–37812. Curran Associates, Inc., 2023.
[Nut21] Marcel Nutz. Introduction to entropic optimal transport. Lecture notes, Columbia
University , 2021.
[OFLR21] Derek Onken, Samy Wu Fung, Xingjian Li, and Lars Ruthotto. Ot-flow: Fast and
accurate continuous normalizing flows via optimal transport. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 35, pages 9223–9232, 2021.
[Pel22] Stefano Peluchetti. Non-denoising forward-time diffusions, 2022.
[PMM23] Francesco Pedrotti, Jan Maas, and Marco Mondelli. Improved convergence of score-
based diffusion models via prediction-correction. arXiv preprint arXiv:2305.14164 ,
2023.
[PVG+21]Vadim Popov, Ivan V ovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudi-
nov. Grad-tts: A diffusion probabilistic model for text-to-speech. In International
Conference on Machine Learning , pages 8599–8608. PMLR, 2021.
[RBL+22]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer. High-resolution image synthesis with latent diffusion models. In Proceed-
ings of the IEEE/CVF conference on computer vision and pattern recognition , pages
10684–10695, 2022.
[RDN+22]Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
Hierarchical text-conditional image generation with clip latents. arXiv preprint
arXiv:2204.06125 , 1(2):3, 2022.
[RY13] Daniel Revuz and Marc Yor. Continuous martingales and Brownian motion , volume
293. Springer Science & Business Media, 2013.
[SBCD23] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion
schrödinger bridge matching. In Thirty-seventh Conference on Neural Information
Processing Systems , 2023.
12[SDWMG15a] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep
unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and
David Blei, editors, Proceedings of the 32nd International Conference on Machine
Learning , volume 37 of Proceedings of Machine Learning Research , pages 2256–
2265, Lille, France, 07–09 Jul 2015. PMLR.
[SDWMG15b] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli.
Deep unsupervised learning using nonequilibrium thermodynamics, 2015.
[SE19] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the
data distribution. Advances in neural information processing systems , 32, 2019.
[SE20] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the
data distribution, 2020.
[VEH14] Tim Van Erven and Peter Harremos. Rényi divergence and kullback-leibler diver-
gence. IEEE Transactions on Information Theory , 60(7):3797–3820, 2014.
[Vin11] Pascal Vincent. A connection between score matching and denoising autoencoders.
Neural Computation , 23:1661–1674, 2011.
13A Postponed proofs
A.1 The Markovian Projection
First, we introduce the set of assumptions under which Theorem 1 holds true.
H4. Fort > s ,(s, t, x, y )7→pY
t|s(y|x)is continuously differentiable in the tandsvariables and
twice continuously differentiable in the xandyvariables. Furthermore, ∂tpY
t|s(y|x),∂spY
t|s(y|x),
∇xpY
t|s(y|x),∇ypY
t|s(y|x),∇2
xpY
t|s(y|x),∇2
ypY
t|s(y|x)are bounded.
H4 ensures that (s, t, x, y )7→pY
t|s(y|x)is enough regular to allow a series of algebraic manipulations.
H5.˜βYis a locally bounded Borel vector field on Rd×(0,1)such that, for at least one probability
solution µtto the Fokker-Planck equation
∂tµt+ div( ˜βY
tµt)−∆µt= 0, t∈(0,1), µ 0=µ , (16)
it holdsR
∥˜βY
t(x)∥µt(dx)dt <+∞.
H5 provides uniqueness of the solution to the Fokker Planck equation with drift field ˜βY, see
[BKRS15, Theorem 9.4.3].
We are now ready to rigorously state and prove Theorem 1:
Theorem 4. Consider a π∈Π(µ, ν⋆),Qβassociated with (3)and the drift field defined in (8).
UnderH4 and 5, the Markov process (XM
t)t∈[0,1]solution of (9)is such that, for any t∈[0,1),
XI
tdist=XM
t.
Proof of Theorem 4: We start by reminding that (s, t, x, y )7→pY
t|s(y|x)satisfies for any x, y∈Rd
ands, t∈[0,1]withs < t both the Fokker-Planck equation
∂tpY
t|s(y|x) + div y(pY
t|s(y|x)β(x))−∆ypY
t|s(y|x) = 0 ,
and the Kolmogorov backward equation
∂spY
t|s(y|x) +⟨β(x),∇xpY
t|s(y|x)⟩+ ∆ xpY
t|s(y|x) = 0 .
If we exploit these well-known results, (4) and H4, we get that for t∈(0,1)
∂tpI
t(x) =∂tZ
R2dpY
t|0(x|x0)pY
1|t(x1|x)˜π(dx0,dx1)
=Z
R2d
∆xpY
t|0(x|x0)pY
1|t(x1|x)−pY
t|0(x|x0)∆xpY
1|t(x1|x)
˜π(dx0,dx1)
−Z
R2ddivx(pY
t|0(x|x0)β(x))pY
1|t(x1|x)˜π(dx0,dx1)
−Z
R2d⟨β(x),∇xpY
1|t(x1|x)⟩pY
t|0(x|x0)˜π(dx0,dx1)
=Z
R2d
∆xpY
t|0(x|x0)pY
1|t(x1|x) +∇xpY
t|0(x|x0)∇xpY
1|t(x1|x)
˜π(dx0,dx1)
−Z
R2d
∇xpY
t|0(x|x0)∇xpY
1|t(x1|x) +pY
t|0(x|x0)∆xpY
1|t(x1|x)
˜π(dx0,dx1)
− ⟨β(x),∇xpI
t(x)⟩ −divxβ(x)pI
t(x)
= div xZ
R2d{∇xpY
t|0(x|x0)pY
1|t(x1|x)−pY
t|0(x|x0)∇xpY
1|t(x1|x)}˜π(dx0,dx1)
−divx(β(x)pI
t(x))
= div x
−β(x)pI
t(x)
+Z
R2d{∇xlogpY
t|0(x|x0)− ∇ xlogpY
1|t(x1|x)}pY
t|0(x|x0)pY
1|t(x1|x)˜π(dx0,dx1)
.
14Therefore, if we set
vY
t(x) =R
R2d{∇xlogpY
1|t(x1|x)− ∇ xlogpY
t|0(x|x0)}pY
t|0(x|x0)pY
1|t(x1|x)˜π(dx0,dx1)
pI
t(x)+β(x),
we have proven that pI
t(x)satisfies the following continuity equation
∂tpI
t(x) + div x(vY
t(x)pI
t(x)) = 0 , t∈(0,1), x∈Rd.
Consequently, if we define
bY
t(x) =vY
t(x) +∇xlogpI
t(x),
we have that, under H4,pI
t(x)satisfies the following Fokker-Planck equation
∂tpI
t(x) + div x(bY
t(x)pI
t(x))−∆xpI
t(x) = 0 , t∈(0,1), x∈Rd. (17)
So,bt(x)is a mimicking drift. It remains to show that bY≡˜βY: the thesis will then follows from
the uniqueness of the solution to the Fokker Planck equation (17) under H5, see [ BKRS15 , Theorem
9.4.3]. To this aim, note that
∇xlogpI
t(x) =R
R2d{∇xpY
t|0(x|x0)pY
1|t(x1|x) +pY
t|0(x|x0)∇xpY
1|t(x1|x)}˜π(dx0,dx1)
pI
t(x)
=R
R2d{∇xlogpY
t|0(x|x0) +∇xlogpY
1|t(x1|x)}pY
t|0(x|x0)pY
1|t(x1|x)˜π(dx0,dx1)
pI
t(x).
Therefore, for any t∈[0,1)andx∈Rd, we have that
bY
t(x) = 2R
R2d∇xlogpY
1|t(x1|x)pY
t|0(x|x0)pY
1|t(x1|x)˜π(dx0,dx1)
pI
t(x)+β(x) =˜βY
t(x).
A.2 Preliminary results
We begin this section with two remarks aiming at highlighting some of the properties of the heat
kernels (12) and some of their important consequences.
Remark 8.It is well-known that (s, x, y )7→ps(y|x)defined in (12) is twice continuously differen-
tiable in the space variables xandyand satisfies for x, y∈Rd, s∈(0,1],
∇xps(y|x) =−x−y
2sps(y|x) =−∇yps(y|x), (18)
∇2
xps(y|x) =−1
2sps(y|x) Id +(x−y)(x−y)T
4s2ps(y|x) =∇2
yps(y|x), (19)
∆xps(y|x) =−d
2sps(y|x) +x−y
2s2
ps(y|x) = ∆ yps(y|x). (20)
Moreover (12) satisfies the heat equation, i.e.,
∂sps(y|x) = ∆ xps(y|x), s∈(0,1], x, y∈Rd. (21)
Thus, in particular, (s, x, y )7→ps(y|x)is continuously differentiable in the time variable s.
Remark 9.In the case β≡0,i.e.,Qβ=B, under H1, the conditions of Theorem 1 hold. Indeed,
as highlighted in Remark 8, (s, x, y )7→ps(y|x)defined in (12) is continuously differentiable in the
time variable sand twice continuously differentiable in the space variables xandy. Also, using
Equation (18), Equation (19) and (12), it’s straightforward to verify that ∇xpY
t|s(y|x),∇ypY
t|s(y|x),
∇2
xpY
t|s(y|x),∇2
ypY
t|s(y|x)are bounded. Additionally, using (20) and(21), it’s easy to argue that also
∂tpY
t|s(y|x)and∂spY
t|s(y|x)are bounded. So H4 is verified and pI
tsolves the Fokker-Planck equation
15(16). Moreover ˜βis clearly locally bounded on Rd×(0,1)and, as a consequence of (18),(5), and
Jensen inequality, it satisfies uniformly in time
Z
Rd˜βt(x)2
pI
t(x)dx=Z
Rd1
pI
t(x)Z
R2dx1−x
1−tpt(x|x0)p1−t(x1|x)˜π(dx0,dx1)2
pI
t(x)dx
=E"E"
XI
1−XI
t
1−tXI
t#2#
=E"E"
XI
1−XI
0−√
2t√1−tZXI
t#2#
≲E"XI
1−XI
0−√
2tZ2#
≲m2[µ] +m2[ν⋆] +d ,
which is finite under H1. It follows thatR
∥˜βY
t(x)∥pI
t(x)dxdt <+∞.So,H5 is verified.
Additionally, hereunder, we state three lemmas that will be crucial in the derivation of the bounds
provided in Theorems 2 and 3.
Lemma 1. For any p≥1, they hold
E[XI
s−XI
02p]≲s2pm2p[µ] +s2pm2p[ν⋆] +dpsp(1−s)p,
and
E[XI
1−XI
s2p]ds≲(1−s)2pm2p[µ] + (1−s)2pm2p[ν⋆] +dpsp(1−s)p.
Proof of Lemma 1: As a direct consequence of (5) and Young inequality, it holds
E[XI
s−XI
02p] =Ehs(XI
1−XI
0) +p
2s(1−s)Z2pi
≲s2pE[XI
0−XI
12p] + (2 s(1−s))pE[∥Z∥2p]
≲s2pm2p[µ] +s2pm2p[ν⋆] +dpsp(1−s)p.
A similar argument holds for E[XI
1−XI
s2p].
We preface the next lemma with a definition.
Definition 1. Consider Q∈ P(W). The reverse time measure QR∈ P(W)ofQis defined as
follows: for any A∈ B(W)
QR(A) =Q(AR), ,
where AR:={t7→ω(1−t) :ω∈A}.
Lemma 2. Assume π≪Leb2dandµ, ν⋆≪Lebd. Then (XI
t)t∈[0,1]solves weakly
d− →Xt= 2− →bt(− →X0,− →Xt)dt+√
2d− →Bt, t∈[0,1],− →X0∼µ . (22)
with(− →Bt)t∈[0,1]d-dimensional Brownian motion independent of− →X0and
− →bt(x0, x) =∇xψx0
t(x),
where ψx0
t(x)solves
∂tψx0
t+ ∆ xψx0
t+∥∇xψx0
t∥2= 0, t∈[0,1), ψx0
1= log ˜ πx0
0, (23)
with
˜πx0
0(x) :=dπ(x0, x)
dLeb2d1
p1(x|x0)dµ(x0)
dLebd, (24)
16andp1(x|x0)defined in (12).
Similarly, ((XI
t)t∈[0,1])Rsolves weakly
d← −Xt= 2← −bt(← −X0,← −Xt)dt+√
2d← −Bt, t∈[0,1],← −X0∼ν⋆.
with(← −Bt)t∈[0,1]d-dimensional Brownian motion independent of← −X0and
← −bt(x1, x) =∇xφx1
1−t(x),
where φx1
t(x)solves
−∂tφx1
t+ ∆ xφx1
t+∥∇xφx1
t∥2= 0, t∈[0,1), φx1
0= log ˜ πx1
1,
with
˜πx1
1(x) :=dπ(x, x1)
dLeb2d1
p1(x1|x)dν⋆(x1)
dLebd, (25)
andp1(x1|x)defined in (12).
Proof of Lemma 2: We just show that (XI
t)t∈[0,1]solves weakly (22). The argument for
((XI
t)t∈[0,1])Ris similar and therefore omitted.
For a fixed x0∈Rd, we denote by (x0,A)7→Iπ,B(x0,A)the conditional distribution of (XI
t)t∈[0,1]
given XI
0(see e.g. [ Kle13 , Theorem 8.37] for the existence of this conditional distribution) and by
(Bx0
t)t∈[0,1]the solution to
dBx0
t=√
2dBt, t∈[0,1], Bx0
0=x0.
Also, we denote by (Wt)t∈[0,1]the canonical process on the Wiener space Wand by (Ft)t∈[0,1]the
corresponding natural filtration. Note that, as a consequence of the very definition of (XI
t)t∈[0,1],
(23) and Ito’s formula applied to (ψx0
t(Bx0
t))t∈[0,1], it holds
dIπ,B(x0,·)
dLaw(( Bx0
t)t∈[0,1])((Bx0
t)t∈[0,1])
= ˜πx0
0(Bx0
1)
= exp
ψx0
1(Bx0
1)
= exp
ψx0
1(Bx0
1)−ψx0
0(x0)
= exp
ψx0
1(Bx0
1)−ψx0
0(Bx0
0)−Z1
0n
∂tψx0
t+ ∆ xψx0
t+∥∇xψx0
t∥2o
(Bx0
t)dt
= expZ1
0∇xψx0
t(Bx0
t)dBx0
t−Z1
0∥∇xψx0
t(Bx0
t)∥2dt
.
Hence, for any t∈[0,1]we have that
dIπ,B(x0,·)
dLaw(( Bx0
t)t∈[0,1])
Ft=Dt,
where
Dt= expZt
0∇xψx0
t(Ws)dWs−Z1
0∥∇xψx0
t(Ws)∥2ds
is continuous Law(( Bx0
t)t∈[0,1])- almost surely and is such that
dDt=Dt∇xψx0
s(Ws)dWt.
Therefore, being (Wt)t∈[0,1]a martingale under Law(( Bx0
t)t∈[0,1]), as a consequence of Girsanov
theorem (see [RY13, Theorem 1.4]), we have that
 
Wt− 
D−1⟨W, D⟩
t
t∈[0,1]=
Wt−2Zt
0∇xψx0
s(Ws)ds
t∈[0,1]
17is a((Ft)t,Iπ,B(x0,·))- martingale with bracket√
2t.It follows that, under Iπ,B(x0,·),(Wt)t∈[0,1]
solves
dWt= 2∇xψx0
t(Ws)dt+√
2dBt, t∈[0,1], W 0=x0.
The thesis is now a direct consequence of the fact that
I(π,B)(A) =Z
RdP(XI∈A|XI
0=x0)µ(dx0) =Z
RdIπ,B(x0,A)µ(dx0),
for any measurable A∈W.
Lemma 3. Assume H2. Then, almost surely, it holds
− →bt(XI
0, XI
t) =E"
∇˜πXI
0
0(XI
1)
˜πXI
0
0(XI
1)(XI
0, XI
t)#
, (26)
where ˜πx0
0is defined as in (24). Moreover, for any u∈[0,1−s],s∈[0,1]andp∈ {2,4,8}it holds
E"Zu+s
u− →br(− →X0,− →Xr)drp#
≲sp
∥∇log ˜π∥p
Lp(π)+∥∇logµ∥p
Lp(µ)
. (27)
Similarly, almost surely it holds
← −bt(XI
0, XI
t) =E"
∇˜πXI
1
1(XI
0)
˜πXI
1
1(XI
0)(XI
0, XI
t)#
, (28)
where ˜πx1
1is defined as in (25). Moreover, for any u∈[0,1−s],s∈[0,1]andp∈ {2,4,8}it holds
E"Zu+s
u← −br(← −X0,← −Xr)drp#
≲sp
∥∇log ˜π∥p
Lp(π)+∥∇logν⋆∥p
Lp(ν⋆)
. (29)
Proof of Lemma 3: We just show (26) and(27). The proof of (28) and(29) is similar and therefore
omitted. First of all, note that (26) is trivial for t= 1. We therefore focus on t∈[0,1). It’s well
known that the solution ψx0
t(x)to the Hamilton-Jacobi-Bellman equation (23) is given by
ψx0
t(x) = logZ
˜πx0
0(x1)p1−t(x1|x)dx1
, t∈[0,1).
Therefore, we have that for t∈[0,1)
− →bt(x0, x) =∇xψx0
t(x) =R
˜πx0
0(x1)∇xp1−t(x1|x)dx1R
˜πx0
0(˜x1)p1−t(˜x1|x)d˜x1.
Using (18) and integrating by parts, we get for t∈[0,1)that
− →bt(x0, x) =−R
˜πx0
0(x1)∇x1p1−t(x1|x)dx1R
˜πx0
0(˜x1)p1−t(˜x1|x)d˜x1=R
(∇˜πx0
0(x1)/˜πx0
0(x1))˜πx0
0(x1)p1−t(x1|x)dx1R
˜πx0
0(˜x1)p1−t(˜x1|x)d˜x1.
But then, it suffices to prove that for t∈[0,1)it holds
˜πx0
0(x1)p1−t(x1|x)R
˜πx0
0(˜x1)p1−t(˜x1|x)d˜x1=pI
1|0,t(x1|x0, x),
to conclude. To do so, we simply use (4).
pI
1|0,t(x1|x0, x) =pI
0,1,t(x0, x1, x)R
pI
0,1,t(x0,˜x1, x)d˜x1
=π(x0, x1)pt(x|x0)p1−t(x1|x)
p1(x1|x0)Zπ(x0,˜x1)pt(x|x0)p1−t(˜x1|x)
p1(˜x1|x0)d˜x1
=π(x0, x1)p1−t(x1|x)
p1(x1|x0)Zπ(x0,˜x1)p1−t(˜x1|x)
p1(˜x1|x0)d˜x1
=π(x0, x1)p1−t(x1|x)
µ(x0)p1(x1|x0)Zπ(x0,˜x1)p1−t(˜x1|x)
µ(x0)p1(˜x1|x0)d˜x1
=˜πx0
0(x1)p1−t(x1|x)R
˜πx0
0(˜x1)p1−t(˜x1|x)d˜x1.
18We are left with the bounds (27).
We prove (27) only for p= 1. The other two cases are analogous. To this aim, we simply make use
of (22), Hölder and Jensen inequalities and the properties of the conditional expectation.
E"Zu+s
u− →br(− →X0,− →Xr)dr2#
=E"Zu+s
uE"
∇˜πXI
0
0(XI
1)
˜πXI
0
0(XI
1)(XI
0, XI
r)#
dr2#
≲sE"Zu+s
uE"
∇˜πXI
0
0(XI
1)
˜πXI
0
0(XI
1)(XI
0, XI
r)#2
dr#
≲sE"Zu+s
uE"∇˜πXI
0
0(XI
1)
˜πXI
0
0(XI
1)2(XI
0, XI
r)#
dr#
=sE"Zu+s
u∇˜πXI
0
0(XI
1)
˜πXI
0
0(XI
1)2
dr#
≲s2Z
∥∇log ˜πx0
0(x1)∥2dπ(x0, x1)
≲s2
∥∇log ˜π∥2
L2(π)+∥∇logµ∥2
L2(µ)
,
where, in the last inequality, we have used the very definition of ˜πx0
0given in (24).
A.3 Main results
We preface the proofs of our main results with some extra notation. For sake of brevity, we denote by
− →ft
u= 2Zt+u
u− →br(− →X0,− →Xr)dr ,− →gt
u=− →Bt+u−− →Bu,
and similarly
← −ft
u= 2Zt+u
u← −br(← −X0,← −Xr)dr ,← −gt
u=← −Bt+u−← −Bu.
Remark 10.With this new notation, according to Lemma 2, we have that
(XI
t)t∈[0,1]dist= (− →Xt)t∈[0,1],(XI
t)t∈[0,1]dist= ((← −Xt)t∈[0,1])R,
and that for any u∈[0,1]andt∈[0,1−u],
XI
t+u−XI
udist=− →ft
u+− →gt
u, XI
1−(t+u)−XI
1−udist=← −ft
u+← −gt
u.
Furthermore, according to Lemma 3, under H2, for any u∈[0,1],t∈[0,1−u],p∈ {2,4,8}, we
have that
Eh− →ft
upi
≲tp(∥∇logµ∥p
Lp(µ)+∥∇log ˜π∥p
Lp(π)),
and
Eh← −ft
upi
≲tp(∥∇logν⋆∥p
Lp(ν⋆)+∥∇log ˜π∥p
Lp(π)).
Remark 11.Note that for any u∈[0,1]andt∈[0,1−u]
− →gt
u⊥ ⊥(− →Xr)r≤u,← −gt
u⊥ ⊥(← −Xr)r≤u.
This fact is an almost immediate consequence of the Markov property of (− →Bt)t∈[0,1], see [ BB17 ,
Theorem 3.3]: consider the filtration (Ft)t∈[0,1]defined by Ft=σ(− →X0,(− →Bu)u≤t). Then, being
(− →Bt)t∈[0,1]⊥ ⊥− →X0,(− →Bt)t∈[0,1]is(Ft)t∈[0,1]-adapted and, as a consequence of the Markov property
of(− →Bt)t∈[0,1],(− →gt
u)t∈[0,1−u]= (− →Bt+u−− →Bu)t∈[0,1−u]⊥ ⊥ F u. On the other hand, it’s well
known that (− →Xt)t∈[0,1]is(Ft)t∈[0,1]-adapted. It therefore follows that− →gt
u⊥ ⊥(− →Xr)r≤u. A similar
arguments holds for (← −Xt)t∈[0,1].
19A.4 Proof of Theorem 2
We fix 0< ϵ < min{h,1/2}and, for any t∈[0,1−ϵ], we denote by ν⋆
t=Law(XM
t)and
νθ⋆
t=Law(Xθ⋆
t).
First, using the data processing inequality [ Nut21 , Lemma 1.6], the standard decomposition of the
KL divergence [ CCL+23b,CLL23 ,CDS23 ] based on Girsanov theorem, triangle inequality and H3,
we bound the KL divergence between ν⋆
1−ϵandνθ⋆
1−ϵas follows
KL(ν⋆
1−ϵ|νθ⋆
1−ϵ)
≤KL(Law((XM
t)t∈[0,1−ϵ])|Law(( Xθ⋆
t)t∈[0,1−ϵ]))
≲N−2X
k=0Ztk+1
tkEhsθ⋆(tk, XM
tk)−˜βt(XM
t)2i
dt+Z1−ϵ
1−hEhsθ⋆(1−h, XM
1−h)−˜βt(XM
t)2i
dt
≲ε2+N−2X
k=0Ztk+1
tkEh˜βtk(XM
tk)−˜βt(XM
t)2i
dt+Z1−ϵ
1−hEh˜β1−h(XM
1−h)−˜βt(XM
t)2i
dt .
(30)
Second, we aim at bounding the RHS of (30) uniformly in ϵ. Indeed, if we assume to be able to
bound it with a constant Aindependent of ϵ, then, using the weak convergence of XM
1−ϵtoXI
1(whose
law is given by ν⋆) asϵ→0, the continuity of (Xθ⋆
t)t∈[0,1], hence the weak convergence of Xθ⋆
1−ϵto
Xθ⋆
1(whose law is given by νθ⋆
1) asϵ→0, and the lower semi-continuity of the KL-divergence with
respect to the weak convergence [VEH14, Theorem 19], we will get
KL(ν⋆|νθ⋆
1)≤lim inf
ϵ→0KL(ν⋆
1−ϵ|νθ⋆
1−ϵ)≲lim inf
ϵ→0A=A . (31)
Let us therefore bound the RHS of (30). We will do so by using stochastic calculus tools, and, more
precisely, Ito’s formula. To this aim let us introduce the generator of (XM
t)t∈[0,1−ϵ], which is defined
for any t∈[0,1−ϵ]andρ∈C2(Rd)as
LM
tρ:=⟨∇xρ,˜βt⟩+ ∆ xρ .
Using Ito’s formula, we get that
d˜βt(XM
t) = (∂t+LM
t)˜βt(XM
t)dt+√
2Dx˜βt(XM
t)dBt, t∈[0,1−ϵ].
So, applying Young inequality and Ito’s isometry, we have that, for any k= 0,···, N−1
Eh˜βtk(XM
tk)−˜βt(XM
t)2i
=E"Zt
tk(∂s+LM
s)˜βs(XM
s)ds+√
2Zt
tkDx˜βs(XM
s)dBs2#
≲E"Zt
tk(∂s+LM
s)˜βs(XM
s)ds2#
+ 2Zt
tkEhDx˜βs(XM
s)2i
ds .
We now bound separately the two upper addends. To do so, we introduce the auxiliary measures
λh
k(ds)∈ P([tk, tk+1])fork= 0, ..., N −2andλh
N−1(ds)∈ P([1−h,1−ϵ])which will help us,
via a double change of measure argument, to mitigate the bad behaviour at t= 0andt= 1of the
reciprocal characteristic of the mimicking drift ( i.e.,∂s+LM
s), which is the trickiest addend. Namely,
fork= 0, ..., N −2we consider the measures λh
k(ds)∈ P([tk, tk+1])defined as
λh
k(ds) =ρ(s)−1
Zk1[tk,tk+1]ds ,
with
ρ(s)−1=s−7/81{s≤1/2}+ (1−s)−7/81{s>1/2},
and
Zk=Zmin{tk+1,1/2}
min{tk,1/2}r−7/8dr+Zmax{tk+1,1/2}
max{tk,1/2}(1−r)−7/8dr .
20Whereas, for k=N−1, we consider the measures λh
N−1(ds)∈ P([1−h,1−ϵ])defined as
λh
N−1(ds) =ρ(s)−1
ZN−11[1−h,1−ϵ]ds ,
withρ(s)−1as above and
ZN−1=Zmin{1−ϵ,1/2}
min{1−h,1/2}r−7/8dr+Zmax{1−ϵ,1/2}
max{1−h,1/2}(1−r)−7/8dr .
Note that, for any s∈[0,1−ϵ]and for any k= 0, ..., N −1, they hold
ρ(s)≲1, Z k≲h1/8,Z1−ϵ
0ρ−1(s)ds= 161
21/8
−8ϵ1/8≲1. (32)
We start by bounding the first addend, that is the one that involves the reciprocal characteristic of the
mimicking drift. With a first change of measure argument, we get for any k= 0, ..., N −1,
E"Zt
tk(∂s+LM
s)˜βs(XM
s)ds2#
=Z2
kE"Zt
tk(∂s+LM
s)˜βs(XM
s)ρ(s)λh
k(ds)2#
,
where, in the last inequality, we used (32). But then, if we apply Jensen inequality and use an other
change of measure argument, we get
E"Zt
tk(∂s+LM
s)˜βs(XM
s)ds2#
≤Z2
kE"Zt
tk(∂s+LM
s)˜βs(XM
s)2
ρ(s)2λh
k(ds)#
≤ZkZt
tkEh(∂s+LM
s)˜βs(XM
s)2i
ρ(s)ds
≲h1/8Zt
tkEh(∂s+LM
s)˜βs(XM
s)2i
ρ(s)ds .(33)
Let us now focus on the second addend. Remarkably, this addend can be bounded via the reciprocal
characteristic of ˜β: because of Ito’s formula, for t∈[0,1−ϵ], it holds true
d˜βt(XM
t)2
=n
2⟨˜βt,(∂t+LM
t)˜βt⟩+ 2Dx˜βt2o
(XM
t)dt+ 2√
2⟨˜βt, Dx˜βt⟩(XM
t)dBt.
Consequently, if we assume that the process (Rs
0⟨˜βt, Dx˜βt⟩(XM
t)dBt)s∈[0,1−ϵ]is a true martingale
(see Lemma 4 below), we have that
2Zt
tkEhDx˜βs(XM
s)2i
ds
≤Eh˜βt(XM
t)2i
−Eh˜βtk(XM
tk)2i
+ 2Zt
tkE[⟨˜βs(XM
s),(∂s+LM
s)˜βs(XM
s)⟩]ds.
But then, using, as before, a double change of measure argument and applying Cauchy-Schwartz
inequality, we can bound the above expression as follows
2Zt
tkEhDx˜βs(XM
s)2i
ds
≤Eh˜βt(XM
t)2i
−Eh˜βtk(XM
tk)2i
+ 2ZkZt
tkE[⟨˜βs(XM
s),(∂s+LM
s)˜βs(XM
s)⟩]ρ(s)λh
k(ds)
≤Eh˜βt(XM
t)2i
−Eh˜βtk(XM
tk)2i
+ZkZt
tkEh˜βs(XM
s)2i
λh
k(ds)
+ZkZt
tkEh(∂s+LM
s)˜βs(XM
s)2i
ρ(s)2λh
k(ds)
=Eh˜βt(XM
t)2i
−Eh˜βtk(XM
tk)2i
+Zt
tkEh˜βs(XM
s)2i
ρ(s)−1ds
+Zt
tkEh(∂s+LM
s)˜βs(XM
s)2i
ρ(s)ds .
21Plugging this bound and (33) in (30), we get
KL(ν⋆
1−ϵ|νθ⋆
1−ϵ)≲ε2+hEh˜β1−ϵ(XM
1−ϵ)2i
+hZ1−ϵ
0Eh˜βs(XM
s)2i
ρ(s)−1ds
+h(h1/8+ 1)Z1−ϵ
0Eh(∂s+LM
s)˜βs(XM
s)2i
ρ(s)ds . (34)
We now compute explicitly and upper bound each term appearing in the RHS of (34), recalling that,
because of Theorem 1, for any s∈[0,1−ϵ],ν⋆
s=Law(XI
s). We start with
Eh˜β1−ϵ(XM
1−ϵ)2i
≲Z
RdR
R2dp1−ϵ(x|x0)∇xpϵ(x1|x)˜π(x0, x1)dx1dx0
pI
1−ϵ(x)2
pI
1−ϵ(x)dx .
First we use (18), second we integrate by part, third we apply Jensen inequality and last we rely on H
2.
Eh˜β1−ϵ(XM
1−ϵ)2i
≲Z
RdR
R2dp1−ϵ(x|x0)pϵ(x1|x)(∇x1˜π(x0, x1)/˜π(x0, x1))˜π(x0, x1)dx0dx1
pI
1−ϵ(x)2
pI
1−ϵ(x)dx
=E"E"
∇x1˜π
˜π(XI
0, XI
1)XI
1−ϵ#2#
≤E"
E"∇x1˜π
˜π(XI
0, XI
1)2XI
1−ϵ##
=E"∇x1˜π
˜π(XI
0, XI
1)2#
=∇x1˜π
˜π2
L2(π)≤ ∥∇ log ˜π∥2
L2(π).
(35)
In the very same way we deal with the third term of the RHS of (34).
Z1−ϵ
0Eh˜βs(XM
s)2i
ρ(s)−1ds
≲Z1−ϵ
0ρ(s)−1Z
RdR
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)2
pI
s(x)dxds
=Z1−ϵ
0ρ(s)−1Z
Rd1
pIs(x)Z
R2dps(x|x0)p1−s(x1|x)∇x1˜π
˜π(x0, x1)˜π(x0, x1)dx0,dx12
pI
s(x)dx
=Z1−ϵ
0ρ(s)−1E"E"
∇x1˜π
˜π(XI
0, XI
1)XI
s#2#
ds
≤Z1−ϵ
0ρ(s)−1E"
E"∇x1˜π
˜π(XI
0, XI
1)2XI
s##
ds
=Z1−ϵ
0ρ(s)−1E"∇x1˜π
˜π(XI
0, XI
1)2#
ds
=Z1−ϵ
0ρ(s)−1ds∥∇log ˜π∥2
L2(π)≲∥∇log ˜π∥2
L2(π),
(36)
where, in the last inequality, we used (32). We now turn to the last term, i.e., to
Z1−ϵ
0Eh(∂s+LM
s)˜βs(XM
s)2i
ρ(s)ds=Z1−ϵ
0Z
Rd(∂s+LM
s)˜βs(x)2
ρ(s)pI
s(x)dxds .
22Some computations and (21) lead to
∂s˜βs(x)
= 2R
R2d∆xps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
−2R
R2dps(x|x0)∇x∆xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
−2R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1R
R2d∆xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
(pIs(x))2
+ 2R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1R
R2dps(x|x0)∆xp1−s(x1|x)˜π(x0, x1)dx0dx1
(pIs(x))2;
Dx˜βs(x)
= 2R
R2d∇xp1−s(x1|x)(∇xps(x|x0))T˜π(x0, x1)dx0dx1
pIs(x)
+ 2R
R2dps(x|x0)∇2
xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
−2R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)R
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)T
−2R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)T
,
(37)
hence
Dx˜βs(x)˜βs(x)
= 4R
R2d∇xp1−s(x1|x)(∇xps(x|x0))T˜π(x0, x1)dx0dx1
pIs(x)R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
+ 4R
R2dps(x|x0)∇2
xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
−4 R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)! R
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)!T
·R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
−4R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)2
;
(38)
23∆x˜βs(x)
= 2R
R2d∆xps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
+ 4R
R2d∇2
xp1−s(x|x0)∇xps(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
−4R
R2d∇xp1−s(x1|x)(∇xps(x|x0))T˜π(x0, x1)dx0dx1
pIs(x)
·R
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
−4R
R2d∇xp1−s(x1|x)(∇xps(x|x0))T˜π(x0, x1)dx0dx1
pIs(x)
·R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
−4R
R2d⟨∇xp1−s(x1|x),∇xps(x|x0)⟩˜π(x0, x1)dx0dx1
pIs(x)
·R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
+ 2R
R2dps(x|x0)∇x∆xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
−4R
R2dps(x|x0)∇2
xp1−s(x1|x)˜π(x0, x1)dx0dx1R
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
(pIs(x))2
−4R
R2dps(x|x0)∇2
xp1−s(x1|x)˜π(x0, x1)dx0dx1R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
(pIs(x))2
−2R
R2d∆xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
(pIs(x))2
−2R
R2dps(x|x0)∆xp1−s(x1|x)˜π(x0, x1)dx0dx1R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
(pIs(x))2
+ 8R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x) R
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)!T
·R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
+ 4R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)2
·R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
+ 4R
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)2
·R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x).
So, we get
(∂s+LM
s)˜βs(x) =6X
k=1Ak
s(x), (39)
where we have defined
A1
s(x) =−4R
R2d∇xp1−s(x1|x)(∇xps(x|x0))T˜π(x0, x1)dx0dx1
pIs(x)
·R
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x),
24A2
s(x) =−4R
R2d⟨∇xp1−s(x1|x),∇xps(x|x0)⟩˜π(x0, x1)dx0dx1
pIs(x)
·R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x),
A3
s(x) = 4R
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx12
(pIs(x))2
·R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x),
A4
s(x) = 4R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
· R
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)!TR
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x).
A5
s(x) = 4R
R2d∆xps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)
−4R
R2d∆xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1R
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
(pIs(x))2,
A6
s(x) = 4R
R2d∇2
xp1−s(x1|x)∇xps(x|x0)˜π(x0, x1)dx0dx1
pIs(x)
−4R
R2dps(x|x0)∇2
xp1−s(x1|x)˜π(x0, x1)dx0dx1R
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
(pIs(x))2.
Therefore
Z1−ϵ
0Eh(∂s+LM
s)˜βs(XM
s)2i
ρ(s)ds≲6X
k=1Z1−ϵ
0Z
RdAk
s(x)2ρ(s)pI
s(x)dxds .
We now bound each term Ak
sin the sum.
Using (32) and Young inequality, we have
Z1−ϵ
0Z
RdA1
s(x)2ρ(s)pI
s(x)dxds
≲Z1−ϵ
0Z
RdR
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)4
pI
s(x)dxds
+Z1−ϵ
0Z
RdR
R2d∇xps(x|x0)(∇xp1−s(x1|x))T˜π(x0, x1)dx0dx1
pIs(x)4
oppI
s(x)dxds .
To bound the first term we proceed as in (36), that is, we first exploit (18) and the integration by part
formula and we then use Jensen inequality and the properties of the conditional expectation.
Z1−ϵ
0Z
RdR
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)4
pI
s(x)dxds≤ ∥∇ log ˜π∥4
L4(π).
(40)
25To bound the second term we first split the time interval [0,1−ϵ]in two, [0,1/2]and[1/2,1−ϵ],
we second make either ∇xps(x|x0)or∇xp1−s(x1|x)explicit and we last proceed as before, i.e., we
exploit (18), we integrate by parts and we use Young and Jensen inequalities.
Z1−ϵ
0Z
RdR
R2d∇xp1−s(x1|x)(∇xps(x|x0))T˜π(x0, x1)dx0dx1
pIs(x)4
oppI
s(x)dxds
=Z1/2
0Z
RdR
R2d∇xps(x|x0)(∇xp1−s(x1|x))T˜π(x0, x1)dx0dx1
pIs(x)4
oppI
s(x)dxds
+Z1−ϵ
1/2Z
RdR
R2d∇xp1−s(x1|x)(∇xps(x|x0))T˜π(x0, x1)dx0dx1
pIs(x)4
oppI
s(x)dxds
≲Z1/2
0Z
Rd1
pIs(x)Z
R2d∇x0˜π
˜π(x0, x1)(x1−x)T
1−sps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx14
op
·pI
s(x)dxds
+Z1−ϵ
1/2Z
Rd1
pIs(x)Z
R2d∇x1˜π
˜π(x0, x1)(x−x0)T
sps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx14
op
·pI
s(x)dxds
≲Z1/2
0E"∇x0˜π
˜π(XI
0, XI
1)
(XI
1−XI
s)T4
op#
ds
+Z1
1/2E"∇x1˜π
˜π(XI
0, XI
1)
(XI
s−XI
0)T4
op#
ds
≲Z1/2
0EhXI
1−XI
s8i
ds+Z1
1/2EhXI
s−XI
08i
ds+∥∇log ˜π∥8
L8(π)
≲d4+m8[µ] +m8[ν⋆] +∥∇log ˜π∥8
L8(π),
where, in the last inequality, we have used Lemma 1.
In a similar way we get
Z1−ϵ
0Z
RdA2
s(x)2ρ(s)pI
s(x)dxds≲d4+m8[µ] +m8[ν⋆] +∥∇log ˜π∥8
L8(π).
The argument to bound A2
sresembles the one used to bound A1
sand we therefore omit it.
We now focus on A3
s. To bound such term, we first use (32) and Young inequality and we second
proceed as in (40) and (36).
Z1−ϵ
0Z
RdA3
s(x)2ρ(s)pI
s(x)dxds
≲Z1−ϵ
0Z
RdR
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)8
pI
s(x)dxds
+Z1−ϵ
0Z
RdR
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)4
pI
s(x)dxds
≲E"∇x0˜π
˜π(XI
0, XI
1)8#
+E"∇x1˜π
˜π(XI
0, XI
1)4#
≲∥∇log ˜π∥8
L8(π)+∥∇log ˜π∥4
L4(π).
26Proceeding in a similar way (we omit the argument, as it is almost a duplication of the previous one),
we get
Z1−ϵ
0Z
RdA4
s(x)2ρ(s)pI
s(x)dxds≲∥∇log ˜π∥8
L8(π)+∥∇log ˜π∥4
L4(π).
We now turn to A5
s. Because of (20), A5
srewrites as
A5
s(x) =1
pIs(x)Z
R2dn−d
2s+∥x−x0∥2
4s2ox1−x
2(1−s)ps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
−1
pIs(x)Z
R2dn−d
2s+∥x−x0∥2
4s2o
ps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
·1
pIs(x)Z
R2dx1−x
2(1−s)ps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
≲1
pIs(x)Z
R2d∥x−x0∥2
s2x1−x
1−sps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
−1
pIs(x)Z
R2d∥x−x0∥2
s2ps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
·1
pIs(x)Z
R2dx1−x
1−sps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
.
Therefore, we have that
Z1−ϵ
0Z
RdA5
s(x)2ρ(s)pI
s(x)dxds≲Z1−ϵ
0E"E"XI
s−XI
02
s2XI
1−XI
s
1−sXI
s#
−E"XI
s−XI
02
s2XI
s#
E"
XI
1−XI
s
1−sXI
s#2#
ρ(s)ds .
We now split the time interval [0,1−ϵ]in two, [0,1/2],[1/2,1−ϵ]and we focus on the first one.
So we look at s∈[0,1/2]and we try to bound the integrand. Using Lemma 1, Lemma 2, Lemma 3
and standard and well-known inequalities (Cauchy-Schwarz, Young and Jensen inequalities), we get
27that for s∈[0,1/2]
E"E"XI
s−XI
02
s2XI
1−XI
s
1−sXI
s#
−E"XI
s−XI
02
s2XI
s#
E"
XI
1−XI
s
1−sXI
s#2#
s7/8
=E"E"← −X1−s−← −X12
s2← −X0−← −X1−s
1−s← −X1−s#
−E"← −X1−s−← −X12
s2← −X1−s#
E"← −X0−← −X1−s
1−s← −X1−s#2#
s7/8
=E"E"← −fs
1−s2
+ 2⟨← −fs
1−s,← −gs
1−s⟩+← −gs
1−s2
s2(← −X0−← −X1−s)← −X1−s#
−E"← −fs
1−s2
+ 2⟨← −fs
1−s,← −gs
1−s⟩+← −gs
1−s2
s2← −X1−s#
Eh← −X0−← −X1−s← −X1−si2#
s7/8
=E"E"← −fs
1−s2
+ 2⟨← −fs
1−s,← −gs
1−s⟩
s2(← −X0−← −X1−s)← −X1−s#
−E"← −fs
1−s2
+ 2⟨← −fs
1−s,← −gs
1−s⟩
s2← −X1−s#
Eh← −X0−← −X1−s← −X1−si2#
s7/8
≲E"E"← −fs
1−s2
+ 2⟨← −fs
1−s,← −gs
1−s⟩
s2(← −X0−← −X1−s)← −X1−s#2#
s7/8
+E"E"← −fs
1−s2
+ 2⟨← −fs
1−s,← −gs
1−s⟩
s2← −X1−s#
Eh← −X0−← −X1−s← −X1−si2#
s7/8
≲E"← −fs
1−s2
+ 2⟨← −fs
1−s,← −gs
1−s⟩
s2(← −X0−← −X1−s)2#
s7/8
+E"E"← −fs
1−s2
+ 2⟨← −fs
1−s,← −gs
1−s⟩
s2← −X1−s#4#
s7/4+E"Eh← −X0−← −X1−s← −X1−si4#
≲E"← −fs
1−s2
+ 2⟨← −fs
1−s,← −gs
1−s⟩
s24#
s7/4+Eh← −X0−← −X1−s4i
≲E"← −fs
1−s8
s8#
+E"
⟨← −fs
1−s,← −gs
1−s⟩4
s8#
s7/4+Eh← −X0−← −X1−s4i
28≲E"← −fs
1−s8
s8#
+E"← −gs
1−s8
s8#
s7/2+Eh← −X0−← −X1−s4i
≲E"← −fs
1−s8
s8#
+d4s−1/2+EhXI
1−XI
s4i
≲∥∇log ˜π∥8
L8(π)+∥∇logν⋆∥8
L8(ν⋆)+d4s−1/2+d2+m4[µ] +m4[ν⋆].
But then, we obtain that
Z1/2
0Z
RdA5
s(x)2ρ(s)pI
s(x)dxds≲d4+m4[µ] +m4[ν⋆] +∥∇log ˜π∥8
L8(π)
+∥∇logν⋆∥8
L8(ν⋆).
We now focus on the second time interval, that is we look at s∈[1/2,1−ϵ]and try to bound the
integrand. Using (32) and proceeding in a similar way, we get that
E"E"XI
s−XI
02
s2XI
1−XI
s
1−sXI
s#
−E"XI
s−XI
02
s2XI
s#
E"
XI
1−XI
s
1−sXI
s#2#
ρ(s)
≲E"E"− →Xs−− →X02− →f1−s
s+− →g1−s
s
1−s− →Xs#
−Eh− →Xs−− →X02− →Xsi
E"− →f1−s
s+− →g1−s
s
1−s− →Xs#2#
=E"E"− →Xs−− →X02− →f1−s
s
1−s− →Xs#
−Eh− →Xs−− →X02− →Xsi
E"− →f1−s
s
1−s− →Xs#2#
≲Eh− →Xs−− →X08i
+E"− →f1−s
s4
(1−s)4#
=EhXI
s−XI
08i
+E"− →f1−s
s4
(1−s)4#
≲d4+m8[µ] +m8[ν⋆] +∥∇log ˜π∥4
L4(π)+∥∇logµ∥4
L4(µ).
But then, we have that
Z1−ϵ
1/2Z
RdA5
s(x)2ρ(s)pI
s(x)dxds≲d4+m8[µ] +m8[ν⋆] +∥∇log ˜π∥8
L8(π)
+∥∇logµ∥8
L8(µ).
To conclude, we can bound A5
sas follows
Z1−ϵ
0Z
RdA5
s(x)2ρ(s)pI
s(x)dxds≲d4+m8[µ] +m8[ν⋆] +∥∇log ˜π∥8
L8(π)
+∥∇logµ∥8
L8(µ)+∥∇logν⋆∥8
L8(ν⋆).
29We are left with A6
s. Using (19), we can rewrite A6
sas follows
A6
s(x)
=1
pIs(x)Z
R2dn−1
2(1−s)Id +(x1−x)(x1−x)T
4(1−s)2ox−x0
2sps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
−1
pIs(x)Z
R2dn−1
2(1−s)Id +(x1−x)(x1−x)T
4(1−s)2o
ps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
·1
pIs(x)Z
R2dx−x0
2sps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
≲(1
pIs(x)Z
R2d(x1−x)(x1−x)T
(1−s)2x−x0
sps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
−1
pIs(x)Z
R2d(x1−x)(x1−x)T
(1−s)2ps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
·1
pIs(x)Z
R2dx−x0
sps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
.
It follows that
Z1−ϵ
0Z
RdA6
s(x)2ρ(s)pI
s(x)dxds
≲Z1−ϵ
0E"E"
(XI
1−XI
s)(XI
1−XI
s)T
(1−s)2XI
s−XI
0
sXI
s#
−E"
(XI
1−XI
s)(XI
1−XI
s)T
(1−s)2XI
s#
E"
XI
s−XI
0
sXI
s#2#
ρ(s)ds
At this point, we proceed as for A5
s, that is we split the time interval in two and we use Lemma 1,
Lemma 2 and Lemma 3. By doing so and by using (32), we get that for s∈[0,1/2]
E"E"
(XI
1−XI
s)(XI
1−XI
s)T
(1−s)2XI
s−XI
0
sXI
s#
−E"
(XI
1−XI
s)(XI
1−XI
s)T
(1−s)2XI
s#
E"
XI
s−XI
0
sXI
s#2#
ρ(s)
≲E"E"
(← −X0−← −X1−s)(← −X0−← −X1−s)T← −X1−← −X1−s
s← −X1−s#
−Eh
(← −X0−← −X1−s)(← −X0−← −X1−s)T← −X1−si
E"← −X1−← −X1−s
s← −X1−s#2#
=E"E"
(← −X0−← −X1−s)(← −X0−← −X1−s)T← −fs
1−s
s← −X1−s#
−Eh
(← −X0−← −X1−s)(← −X0−← −X1−s)T← −X1−si
E"← −fs
1−s
s← −X1−s#2#
≲Eh(← −X0−← −X1−s)(← −X0−← −X1−s)T4
opi
+E"← −fs
1−s
s4#
=E[XI
1−XI
s8] +E"← −fs
1−s
s4#
≲d4+m8[µ] +m8[ν⋆] +∥∇log ˜π∥8
L8(π)+∥∇logν⋆∥8
L8(ν⋆).
30Whereas, for s∈[1/2,1−ϵ], we get
E"E"
(XI
1−XI
s)(XI
1−XI
s)T
(1−s)2XI
s−XI
0
sXI
s#
−E"
(XI
1−XI
s)(XI
1−XI
s)T
(1−s)2XI
s#
E"
XI
s−XI
0
sXI
s#2#
ρ(s)
≲E"E"
(− →X1−− →Xs)(− →X1−− →Xs)T
(1−s)2(− →Xs−− →X0)− →Xs#
−E"
(− →X1−− →Xs)(− →X1−− →Xs)T
(1−s)2− →Xs#
Eh− →Xs−− →X0− →Xsi2#
(1−s)7/8
= (1−s)7/8E"E"− →f1−s
s(− →f1−s
s)T+− →f1−s
s(− →g1−s
s)T+− →g1−s
s(− →f1−s
s)T
(1−s)2(− →Xs−− →X0)− →Xs#
−E"− →f1−s
s(− →f1−s
s)T+− →f1−s
s(− →g1−s
s)T+− →g1−s
s(− →f1−s
s)T
(1−s)2− →Xs#
Eh− →Xs−− →X0− →Xsi2#
≲E"− →f1−s
s(− →f1−s
s)T+− →f1−s
s(− →g1−s
s)T+− →g1−s
s(− →f1−s
s)T4
op
(1−s)8#
(1−s)7/4+Eh− →Xs−− →X04i
≲E"− →f1−s
s8
(1−s)8#
+E"− →g1−s
s8
(1−s)8#
(1−s)7/2+ +EhXI
s−XI
04i
≲∥∇log ˜π∥8
L8(π)+∥∇logµ∥8
L8(µ)+d4(1−s)−1/2+d2+m4[µ] +m4[ν⋆].
Consequently, we have that
Z1−ϵ
0Z
RdA6
s(x)2ρ(s)pI
s(x)dxds≲d4+m8[µ] +m8[ν⋆] +∥∇log ˜π∥8
L8(π)
+∥∇logµ∥8
L8(µ)+∥∇logν⋆∥8
L8(ν⋆).
Putting together the bounds on the {Ak
s}6
k=1derived so far, we eventually obtain
Z1−ϵ
0Eh(∂s+LM
s)˜βs(− →Xs)2i
ρ(s)ds
≲d4+m8[µ] +m8[ν⋆] +∥∇log ˜π∥8
L8(π)+∥∇logµ∥8
L8(µ)+∥∇logν⋆∥8
L8(ν⋆).(41)
Plugging (41), (35) and (36) into (34), we get
KL(ν⋆
1−ϵ|νθ⋆
1−ϵ)≲ε2+h(h1/8+ 1)
d4+m8[µ] +m8[ν⋆] +∥∇log ˜π∥8
L8(π)
+∥∇logµ∥8
L8(µ)+∥∇logν⋆∥8
L8(ν⋆)
.
The estimate (14) then follows from the above estimate and (31).
However, for (14) to hold true, we still need to prove that
Lemma 4. (Rs
0⟨˜βt, Dx˜βt⟩(XM
t)dBt)s∈[0,1−ϵ]is a martingale.
Proof of Lemma 4. If we show that for any s∈[0,1−ϵ],it holds E[∥⟨˜βs, Dx˜βs⟩(XM
t)∥2]< C, for
some C >0independent of time, then by Fubini’s theorem and [BB17, Theorem 7.3] we are done.
To do so, by the Cauchy-Schwarz inequality, we just need to show that E[∥˜βs(XM
s)∥4]and
31E[∥Dx˜βs(XM
s)∥4
opi
are bounded from above by constants which are independent of time. To
this aim, note that, as a direct consequence of (8), Theorem 1, (18), Jensen inequality and H2, it holds
Eh˜βs(XM
s)4i
≲Z
RdR
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)4
pI
s(x)dx
=Z
Rd1
pIs(x)Z
R2dps(x|x0)p1−s(x1|x)∇x1˜π
˜π(x0, x1)˜π(x0, x1)dx0,dx14
pI
s(x)dx
=E"E"
∇x1˜π
˜π(XI
0, XI
1)XI
s#4#
≤E"
E"∇x1˜π
˜π(XI
0, XI
1)4XI
s##
=E"∇x1˜π
˜π(XI
0, XI
1)4#
=∥∇log ˜π∥4
L4(π).(42)
Similarly, recalling (38), we have that
EhDx˜βs(XM
s)4
opi
≲Z
RdR
R2d∇xp1−s(x1|x)(∇xps(x|x0))T˜π(x0, x1)dx0dx1
pIs(x)4
oppI
s(x)dx
+Z
RdR
R2dps(x|x0)∇2
xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)4
oppI
s(x)dx
+Z
RdR
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)8
pI
s(x)dx
+Z
RdR
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)8
pI
s(x)dx .
To bound the first term of the RHS of the above expression, we integrate by parts and use Lemma 1
andH2.
Z
RdR
R2d∇xp1−s(x1|x)(∇xps(x|x0))T˜π(x0, x1)dx0dx1
pIs(x)4
oppI
s(x)dx
=Z
RdR
R2d∇xps(x|x0)(∇xp1−s(x1|x))T˜π(x0, x1)dx0dx1
pIs(x)4
oppI
s(x)dx
=Z
Rd1
pIs(x)Z
R2d∇x0˜π
˜π(x0, x1)(x1−x)T
1−sps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx14
oppI
s(x)dx
≲E"∇x0˜π
˜π(XI
0, XI
1)(XI
1−XI
s)T
ϵ4
op#
≲1
ϵ(d4+m8[µ] +m8[ν⋆]) +∥∇log ˜π∥8
L8(π).
32To bound the second term, we integrate by parts and proceed as before.
Z
RdR
R2dps(x|x0)∇2
xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)4
oppI
s(x)dx
=Z
RdR
R2dps(x|x0)(∇x1˜π(x0, x1)/˜π(x0, x1))(∇xp1−s(x1|x))T˜π(x0, x1)dx0dx1
pIs(x)4
op
pI
s(x)dx
=Z
Rd1
pIs(x)Z
R2dps(x|x0)p1−s(x1|x)∇x1˜π
˜π(x0, x1)(x−x1)T
1−s˜π(x0, x1)dx0dx14
op
pI
s(x)dx
≲1
ϵ(d4+m8[µ] +m8[ν⋆]) +∥∇log ˜π∥8
L8(π).
To bound the third and last term, we proceed as in (42), getting
Z
RdR
R2dps(x|x0)∇xp1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)8
pI
s(x)dx≲∥∇log ˜π∥8
L8(π),
and
Z
RdR
R2d∇xps(x|x0)p1−s(x1|x)˜π(x0, x1)dx0dx1
pIs(x)8
pI
s(x)dx≲∥∇log ˜π∥8
L8(π).
A.5 Proof of Theorem 3
Fixδ >0. Then, because of Theorem 1, (5) and H1, it holds
m8[ν⋆
1−δ] =E[XI
1−δ8]≲δ8m8[µ] + (1−δ)8m8[ν⋆] +d4δ4(1−δ)4<+∞. (43)
Moreover ν⋆
1−δ≪Lebdwith density pI
1−δand
∇logdν⋆
1−δ
dLebd
∈L8(ν⋆
1−δ). (44)
Indeed because of (18), it holds
∇logdν⋆
1−δ
dLebd(x1−δ)
=∇R
R2dp1−δ(x1−δ|x0)pδ(x1|x1−δ)˜π(dx0,dx1)
pI
1−δ(x1−δ)
=1
pI
1−δ(x1−δ)Z
R2dnx1−δ−x0
1−δ+x1−x1−δ
δo
p1−δ(x1−δ|x0)pδ(x1|x1−δ)˜π(dx0,dx1)
=1
pI
1−δ(x1−δ)Z
R2d(2δ−1)x1−δ−δx0+ (1−δ)x1
δ(1−δ)p1−δ(x1−δ|x0)pδ(x1|x1−δ)˜π(dx0,dx1)
≲E"
XI
1−δ−δXI
0+ (1−δ)XI
1
δ(1−δ)XI
1−δ=x1−δ#
.
33But then, using Jensen inequality we obtain that
Z
Rd∇x1−δlogdν⋆
1−δ
dLebd8
dν⋆
1−δ=E"E"
XI
1−δ−δXI
0+ (1−δ)XI
1
δ(1−δ)XI
1−δ#8#
≲E"XI
1−δ−δXI
0+ (1−δ)XI
1
δ(1−δ)8#
≲m8[ν⋆
1−δ]1
δ8(1−δ)8+m8[µ]1
(1−δ)8+m8[ν⋆]1
δ8
≲m8[µ]1
(1−δ)8+m8[ν⋆]1
δ8+d4 1
δ4(1−δ)4.
Also, consider
π1−δ(x0, x1−δ) =µ(x0)Z
RdpI
1−δ|0,1(x1−δ|x0, x1)ν⋆(dx1),
where (x0, x1, x1−δ)7→pI
1−δ|0,1(x1−δ|x0, x1)denotes the density of XI
1−δgiven (XI
0, XI
1)with
respect to the Lebesgue measure. Then π1−δ∈Π(µ, ν⋆
1−δ)andπ1−δ≪Leb2d. Moreover
∇log1
p1−δdπ1−δ
dLeb2d
∈L8(π1−δ). (45)
Indeed, because of (5),
pI
1−δ|0,1(x1−δ|x0, x1) =1
(4πδ(1−δ))d/2exp 
−∥x1−δ−δx0−(1−δ)x1∥2
4δ(1−δ)!
.
Therefore
∇x0pI
1−δ|0,1(x1−δ|x0, x1) =x1−δ−δx0−(1−δ)x1
2(1−δ)pI
1−δ|0,1(x1−δ|x0, x1),
and
∇x1−δpI
1−δ|0,1(x1−δ|x0, x1) =−x1−δ−δx0−(1−δ)x1
2δ(1−δ)pI
1−δ|0,1(x1−δ|x0, x1).
Furthermore
pI
1−δ|0,1(x1−δ|x0, x1)ν⋆(dx1)R
RdpI
1−δ|0,1(x1−δ|x0,˜x1)ν⋆(d˜x1)=pI
1−δ|0,1(x1−δ|x0, x1)µ(x0)ν⋆(dx1)R
RdpI
1−δ|0,1(x1−δ|x0,˜x1)µ(x0)ν⋆(d˜x1)
=pI
1|0,1−δ(x1|x0, x1−δ)dx1.
Consequently, we have that
∇x0π1−δ
π1−δ(x0, x1−δ)
=∇µ(x0)R
RdpI
1−δ|0,1(x1−δ|x0, x1)ν⋆(dx1) +µ(x0)R
Rd∇x0pI
1−δ|0,1(x1−δ|x0, x1)ν⋆(dx1)
µ(x0)R
RdpI
1−δ|0,1(x1−δ|x0,˜x1)ν⋆(d˜x1)
=∇µ(x0)
µ(x0)+Z
Rdx1−δ−δx0−(1−δ)x1
2(1−δ)pI
1|0,1−δ(x1|x0, x1−δ)dx1,
and that
∇x1−δπ1−δ
π1−δ(x0, x1−δ) =µ(x0)R
Rd∇x1−δpI
1−δ|0,1(x1−δ|x0, x1)ν⋆(dx1)
µ(x0)R
RdpI
1−δ|0,1(x1−δ|x0,˜x1)ν⋆(d˜x1)
=−Z
Rdx1−δ−δx0−(1−δ)x1
2δ(1−δ)pI
1|0,1−δ(x1|x0, x1−δ)dx1.
34But then, if we use Jensen inequality and (43), we get
Z
R2d∇x0logdπ1−δ
dLeb2d8
dπ1−δ≲Z
Rd∇x0logdµ
dLebd8
dµ
+E"E"
XI
1−δ−δXI
0−(1−δ)XI
1
1−δ(XI
0, XI
1−δ)#8#
≲∥∇logµ∥8
L8(µ)+E"XI
1−δ−δXI
0−(1−δ)XI
1
1−δ8#
≲∥∇logµ∥8
L8(µ)+m8[ν⋆
1−δ]1
(1−δ)8+m8[µ]δ8
(1−δ)8+m8[ν⋆]
≲∥∇logµ∥8
L8(µ)+m8[µ]δ8
(1−δ)8+m8[ν⋆] +d4δ4
(1−δ)4
≲∥∇logµ∥8
L8(µ)+m8[µ]1
(1−δ)8+m8[ν⋆]1
δ8+d4 1
δ4(1−δ)4,
and (similarly)
Z
R2d∇x1−δlogdπ1−δ
dLeb2d8
dπ1−δ≲E"E"
XI
1−δ−δXI
0−(1−δ)XI
1
δ(1−δ)(XI
0, XI
1−δ)#8#
≲m8[µ]1
(1−δ)8+m8[ν⋆]1
δ8+d4 1
δ4(1−δ)4.
Additionally, because of Remark 8 and (43), they hold
Z
R2d∥∇x0logp1−δ(x1−δ|x0)∥8dπ1−δ(x0, x1−δ)
=Z
R2d∇x1−δlogp1−δ(x1−δ|x0)8dπ1−δ(x0, x1−δ)
=E"XI
1−δ−XI
0
1−δ8#
≲m8[ν⋆
1−δ]1
(1−δ)8+m8[µ]1
(1−δ)8
≲m8[µ]δ8
(1−δ)8+m8[ν⋆] +d4δ4
(1−δ)4+m8[µ]1
(1−δ)8
≲m8[µ]1
(1−δ)8+m8[ν⋆]1
δ8+d4 1
δ4(1−δ)4.
It follows from H1,H2(i), (43),(44) and(45) that the probability distributions µ, ν⋆
1−δand the cou-
pling π1−δ∈Π(µ, ν⋆
1−δ)satisfy H1. The bound in Theorem 3 is now a straightforward consequence
of Theorem 2 and the bounds on the scores derived so far.
35NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The claims made in the abstract and introduction make clear the goals attained
by the paper and match the results therein provided. Additionally, a discussion on the
contributions made by the paper and the comparison with the literature is held.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The paper discusses the limitations of the work performed in Section 4.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
36Answer: [Yes]
Justification: The paper provides the full set of assumptions for either Theorem 2 and
Theorem 3, see Section 3.1. Moreover, it includes the detailed proofs of all the stated
theorems in Appendix A and a sketch of the proof of our main contributions Theorem 2,
Theorem 3 in Section 3.3.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: Since this work is of theoretical nature, we do not provide experimental data,
hence our answer.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
375.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: Since this work is of theoretical nature, we do not provide experimental data,
hence our answer.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: Since this work is of theoretical nature, we do not provide experimental data,
hence our answer.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: Since this work is of theoretical nature, we do not provide experimental data,
hence our answer.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
38•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: Since this work is of theoretical nature, we do not provide experimental data,
hence our answer.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We confirm that the research conducted our paper conform, in every respect,
with the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Since this work is of theoretical nature, it does not present societal impacts up
to our knowledge.
Guidelines:
39• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Since this work is of theoretical nature, it does not present such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: In the present work, we do not use any assets to be credited.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
40•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: In the present work, we do not introduce new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The present work does not include any experiment involving human subjects,
hence our answer.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The present work does not include any experiment involving human subjects,
hence our answer.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
41•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
42