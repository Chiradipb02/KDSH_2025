Reinforcement Learning under Latent Dynamics:
Toward Statistical and Algorithmic Modularity
Philip Amortila∗
philipa4@illinois.eduDylan J. Foster
dylanfoster@microsoft.comNan Jiang
nanjiang@illinois.edu
Akshay Krishnamurthy
akshaykr@microsoft.comZakaria Mhammedi
mhammedi@google.com
Abstract
Real-world applications of reinforcement learning often involve environments
where agents operate on complex, high-dimensional observations, but the
underlying (“latent”) dynamics are comparatively simple. However, outside
of restrictive settings such as small latent spaces, the fundamental statistical
requirements and algorithmic principles for reinforcement learning under latent
dynamics are poorly understood.
This paper addresses the question of reinforcement learning under general latent
dynamics from a statistical and algorithmic perspective. On the statistical side,
our main negative result shows that most well-studied settings for reinforcement
learning with function approximation become intractable when composed
with rich observations; we complement this with a positive result, identifying
latent pushforward coverability as a general condition that enables statistical
tractability. Algorithmically, we develop provably efficient observable-to-latent
reductions—that is, reductions that transform an arbitrary algorithm for the latent
MDP into an algorithm that can operate on rich observations—in two settings:
one where the agent has access to hindsight observations of the latent dynamics
[LADZ23], and one where the agent can estimate self-predictive latent models
[SAGHCB20]. Together, our results serve as a first step toward a unified statistical
and algorithmic theory for reinforcement learning under latent dynamics.
1 Introduction
Many application domains for reinforcement learning (RL) require the agent to operate on rich,
high-dimensional observations of the environment, such as images or text [WSD15; LFDA16;
KFPM21; NRKFG22; Bak+22; Bro+22]. However, the environment itself can often be summarized
bylatent dynamics for a low-dimensional or otherwise simple latent state space. The decoupling
of latent dynamics from the complex observation process naturally suggests a modular framework
for algorithm design: first learn a representation that decodes the latent state from observations, then
apply a reinforcement learning algorithm for the latent dynamics on top of the learned representation.
This paper investigates the algorithmic and statistical foundations of this framework. We ask: Can
we take existing algorithms and sample complexity guarantees for reinforcement learning in the
latent state space and lift them to the observation space in a modular fashion?
There is a growing body of theoretical and empirical work developing algorithms that combine
representation learning and reinforcement learning to develop scalable algorithms. On the empirical
side, a plethora of representation learning objectives have been deployed to varying degrees of
success [PAED17; Tan+17; ZMCGL21; LSA20; YFK21; Lam+24; Guo+22; HPBL23], but we lack
∗The full (author-recommended) version of this paper can be found at: https://arxiv.org/pdf/2410.17904.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).a mathematical framework to systematically compare these objectives and understand when one
might be preferred to another. On the theoretical side, all existing approaches suffer from three
primary drawbacks: (a) they are tailored to restricted classes of latent dynamics models (tabular
MDPs [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23], LQR [DR21; Mha+20], or factored
MDPs [MLJL21]), limiting generality; (b) the analyses, despite focusing on restrictive settings, are
unwieldy, limiting progress in algorithm development; and (c) they are not modular , in the sense
that the representation learning procedures are specialized to specific choices of latent reinforcement
learning algorithm, limiting ease of use.
1.1 Contributions
We address the aforementioned limitations by introducing a new framework, reinforcement learning
under general latent dynamics .
Reinforcement learning under general latent dynamics (Section 2). In our framework, the agent
performs control based on high-dimensional observations, but the dynamics of the environment are
governed by an unobserved latent state space. Following prior work (particularly the so-called Block
MDP formulation [DKJADL19]), we assume that the latent states can be uniquely decoded from
observations , but that the true decoder is unknown and must be learned. To aid in the decoding
process, we supply the learner with a class of representations that is realizable in the sense that it is
powerful enough to represent the true decoder. Our point of departure from prior theoretical works is
that we do not assume specific structure (e.g., tabular or linear dynamics) on the Markov decision
process (MDP) that governs the latent dynamics. Instead, we make the minimal assumption that the
latent dynamics belong to a base MDP class which is statistically tractable , in the sense that when
the latent states are directly observed there exists some reinforcement learning algorithm with low
sample complexity that is capable of learning a near-optimal policy for every MDP in the class. We
take the first steps toward building a unified and modular theory for reinforcement learning in this
setting.
Contributions: Statistical modularity (Section 3). A central consideration for reinforcement
learning under latent dynamics is that representation learning and exploration must be intertwined:
an accurate decoder is required to explore the latent state space, but exploration is required to learn
an accurate decoder. To develop provable sample complexity guarantees, one must prevent errors
from compounding during this interleaving process, a challenging statistical problem which prior
work addresses through strong structural assumptions on the base MDP [KAL16; DKJADL19;
MHKL20; ZSUWAS22; MFR23; DR21; Mha+20; MLJL21]. For the general latent-dynamics setting
we consider, it is unclear whether similar techniques can be applied, or whether the setting is even
statistically tractable, ignoring computational considerations. Thus, our first contribution considers
the question of statistical modularity :2
If a base MDP class is tractable when observed directly, is the corresponding latent-dynamics
problem tractable?
Statistical modularity adopts a minimax perspective by assuming that the base MDP lies in a given
class, and demands that the sample complexity of the latent-dynamics setting is controlled by a
natural bound on the sample complexity of the base MDP class. We show, perhaps surprisingly,
thatmost well-studied reinforcement learning settings involving function approximation [RVR13;
JKALS17; SJKAL19; MJTS20; AJSWY20; Li09; DVRZ19; WSY20; ZGS21; Du+21; JLM21;
FKQR21] do not admit statistical modularity (Theorem 3.1). In other words, statistical tractability
of an MDP class does not extend to the latent-dynamics setting . We complement these negative
findings with a positive result, identifying pushforward coverability as a general structural condition
on the latent dynamics that enables sample efficiency (Theorem 3.2).
Contributions: Algorithmic modularity (Section 4). Beyond developing a modular understanding
of the statistical landscape, we investigate modular algorithm design principles for RL under general
latent dynamics. Specifically, we consider the question of observable-to-latent reductions , whereby
RL under latent dynamics can be reduced to the simpler problem of RL with latent states directly
observed:
Can we generically lift algorithms for a base MDP class to solve the corresponding latent-dynamics
problem?
2This question and associated definitions are restated formally in Section 3.1.
2This property, which we refer to as algorithmic modularity , enables modular, greatly simplified
algorithm design, allowing one to use an arbitrary base algorithm for the base MDP class to solve the
corresponding latent-dynamics problem. Algorithmic modularity is a stronger property than mere
statistical modularity, and thus is subject to our statistical lower bound. Accordingly, we consider two
settings that sidestep the lower bound through additional feedback and modeling assumptions. Our
first algorithmic result considers hindsight observability [LADZ23], where latent states are revealed
during training, but not at deployment (Theorem 4.1). Our second considers stronger function
approximation conditions that enable the estimation of self-predictive latent models [SAGHCB20]
through representation learning (Theorem A.1). Both results are fully modular : they transform
anysample-efficient algorithm for the base MDP class into a sample-efficient algorithm for the
latent-dynamics setting. Thus, they constitute the first general-purpose algorithms for RL under
latent dynamics.
Together, we believe our results can serve as a foundation for further development of practical,
general-purpose algorithms for RL under latent dynamics. To this end, we highlight a number of
fascinating and challenging open problems for future research (Section 5).
2 Reinforcement Learning under General Latent Dynamics
In this section we formally introduce our framework, reinforcement learning under general latent
dynamics.
MDP preliminaries. We consider an episodic finite-horizon online reinforcement learn-
ing setting. With Hdenoting the horizon, a Markov decision process (MDP) M⋆= 
X,A,{P⋆
h}H
h=0,{R⋆
h}H
h=1, H	
consists of a state space X, an action space A, a reward distribution
R⋆
h:X × A → ∆([0,1])(with expectation r⋆
h(x, a)), and a transition kernel P⋆
h:X × A → ∆(X)
(with the convention that P⋆
0(· | ∅)is the initial state distribution).3
At the beginning of the episode, the learner selects a randomized, non-stationary policy π=
(π1, . . . , π H), where πh:X → ∆(A); we let Πrnsdenote the set of all such policies. The episode
evolves through the following process; beginning from x1∼P⋆
0(· | ∅), the MDP generates a trajec-
tory(x1, a1, r1), . . . , (xH, aH, rH)viaah∼πh(xh),rh∼R⋆
h(xh, ah), andxh+1∼P⋆
h(· |xh, ah).
We let PM⋆,πdenote the law under this process, and let EM⋆,πdenote the corresponding expectation,
and likewise let PM,πandEM,πdenote the analogous laws and expectations in another MDP M. We
assume thatPH
h=1rh∈[0,1]almost surely for any trajectory in M⋆.
For a policy πand MDP M, the expected reward for πis given by JM(π):=EM,πPH
h=1rh
,
and the value functions are given by VM,π
h(x):=EM,πPH
h′=hrh′|xh=x
, and
QM,π
h(x, a):=EM,πPH
h′=hrh′|xh=x, ah=a
.We let πM={πM,h}H
h=1denote an
optimal deterministic policy of M, which maximizes VM,π(over π) at all states (and in particular,
satisfies πM∈arg maxπ∈ΠrnsJM(π)), and write QM,⋆:=QM,πM. For f:X × A → R, we
write πf(x):= arg maxaf(x, a)as well as Vf(x) = max af(x, a). For MDP M, horizon
h∈[H], and g:X → R, we let TM
hdenote the Bellman (optimality) operator defined
via [TM
hg](x, a) =EM[rh+g(xh+1)|xh=x, ah=a],and we overload notation by letting
[TM
hf](x, a) = [ TM
hVf](x, a). We also let TM,π
h denote the Bellman evaluation operator
defined via [TM,π
hf](x, a) =EM
rh+Ea′∼πh+1(·|xh+1)[f(xh+1, a′)]|xh=x, ah=a
,for any
π∈Πrns. We define the occupancy measures for layer hvia dM,π
h(x) =PM,π[xh=x]and
dM,π
h(x, a) =PM,π[xh=x, ah=a].
Online reinforcement learning. In online reinforcement learning, the learning algorithm ALG
repeatedly interacts with an unknown MDP M⋆by executing a policy and observing the resulting
trajectory. After Trounds of interaction, the algorithm outputs a final policy bπ, with the goal of
minimizing their risk, defined via
Risk(T,ALG, M⋆):=JM⋆(πM⋆)−JM⋆(bπ). (1)
3To simplify presentation, we assume that XandAare countable; our results extend to handle continuous
variables with an appropriate measure-theoretic treatment.
3Framework: Reinforcement learning under general latent dynamics. Inreinforcement learning
under general latent dynamics , we consider MDPs M⋆where the dynamics are governed by the evolu-
tion of an unobserved latent state sh, while the agent observes and acts on observations xhgenerated
from these latent states. Formally, a latent-dynamics MDP consists of two ingredients: a base MDP
Mlat={S,A,{Plat,h}H
h=0,{Rlat,h}H
h=1, H}defined over a latent state space S, and a decodable
emission process ψ:={ψh:S → ∆(X)}H
h=1, which maps each latent state to a distribution over
observations. The former is an arbitrary MDP defined over S, while the latter is defined as follows.
Definition 2.1 (Emission process) .Anemission process is any function ψ:={ψh:S → ∆(X)}H
h=1,
and is said to be decodable if
∀h,∀s′̸=s∈ S: supp ψh(s)∩supp ψh(s′) =∅. . (2)
When ψ={ψh}H
h=1is decodable, we let ψ−1:={ψ−1
h:X → S}H
h=1denote the associated decoder.
With this, we can formally introduce the notion of a latent-dynamics MDP.
Definition 2.2 (Latent-dynamics MDP) .For a base MDP Mlat =
{S,A,{Plat,h}H
h=0,{Rlat,h}H
h=1, H}, and a decodable emission process ψ, the latent-dynamics
MDP ⟪Mlat, ψ⟫:=
X,A,{Pobs,h}H
h=0,{Robs,h}H
h=1, H	
is defined as the MDP where the latent
dynamics evolve based on the agent’s action ah∈ A via the process sh+1∼Plat,h(sh, ah)and
rh∼Rlat,h(sh, ah). The latent state is not observed directly, and instead the agent observes
xh∈ X generated by the emission process xh∼ψh+1(sh).4
Note that under these dynamics, the decoder ψ−1associated with ψensures that ψ−1
h(xh) =sh
almost surely for all h∈[H]. That is, the latent states can be uniquely decoded from the observations.
To emphasize the distinction between the latent-dynamics MDP ⟪Mlat, ψ⟫(which operates on the
observable state space X) and the MDP Mlat(which operates on the latent state space S), we refer
to the latter as a base MDP rather than, for example, a “latent MDP”, and apply a similar convention
to other latent objects whenever possible.5
Departing from prior work, we do not place any inherent restrictions on the base MDP, and in
particular do not assume that the latent space is small (i.e., tabular). Rather, we aim to understand—in
a unified fashion—what structural assumptions on the base MDP Mlatare required to enable
learnability under latent dynamics. To this end, it will be useful to considers specific classes (i.e.,
subsets) of base MDPs Mlatand the classes of latent-dynamics MDPs they induce.
Definition 2.3 (Latent-dynamics MDP class) .Given a set of base MDPs Mlatand a set of decoders
Φ⊂ {X → S} , we let
⟪Mlat,Φ⟫:={⟪Mlat, ψ⟫:Mlat∈ M lat, ψis decodable , ψ−1∈Φ} (3)
denote the class of induced latent-dynamics MDPs.
Stated another way, ⟪Mlat,Φ⟫is the set of all latent-dynamics MDPs ⟪Mlat, ψ⟫where (i) the
base MDP Mlatlies in Mlat, and (ii), the emission process ψis decodable, with the corresponding
decoder belonging to Φ. The class Mlatrepresents our prior knowledge about the underlying
MDP Mlat; concrete classes considered in prior work include tabular MDPs [KAL16; DKJADL19;
MHKL20; ZSUWAS22; MFR23], linear dynamical systems [DMRY20; DR21; Mha+20], and
factored MDPs [MLJL21]. In particular, the class Mlatmay itself warrant using function
approximation. At the same time, the class Φrepresents our prior knowledge or inductive bias
about the emission process, enabling representation learning. In what follows, we investigate
what conditions on Mlatmake the induced class ⟪Mlat,Φ⟫tractable, both statistically (statistical
modularity; Section 3) and via reduction (algorithmic modularity; Section 4).
3 Statistical Modularity: Positive and Negative Results
This section presents our main statistical results. We begin by formally defining the notion of
statistical modularity introduced in Section 1, present our main impossibility result (lower bound) and
its implications (Section 3.2), then give positive results for the general class of pushforward-coverable
MDPs (Section 3.3).
4Equivalently the dynamics can be described via Robs,h(xh, ah) =Rlat(ψ−1
h(xh), ah)andPobs,h(xh+1|
xh, ah) =Plat,h(ψ−1
h+1(xh+1)|ψ−1
h(xh), ah)·ψh+1(xh+1|ψ−1
h+1(xh+1)).
5For example, in Section 4 we will be concerned with reductions from observation-space algorithms to “base
algorithms” that operate on the latent state space.
43.1 Statistical modularity: A formal definition
We first define the statistical complexity for a MDP class (or, model class )M.
Definition 3.1 (Statistical complexity) .We say that an MDP class Mcan be learned up to ε-
optimality using comp(M, ε, δ)samples if there exists an algorithm ALGwhich, for every M∈ M ,
attains
Risk(T,ALG, M)≤ε
with probability at least 1−δ, after T=comp(M, ε, δ)rounds of online interaction in M.
We say that a base MDP class Mlatadmits statistically modularity if, for any decoder class Φ, the
induced latent-dynamics MDP class ⟪Mlat,Φ⟫can be learned with statistical complexity that is
polynomial in: (i) the statistical complexity for the base class, and (ii) the capacity of the decoder class.
Definition 3.2 (Statistical modularity) .We say the MDP class Mlatisstatistically modular under
complexity comp(Mlat, ε, δ)if, for every decoder class Φ, we have
comp(⟪Mlat,Φ⟫, ε, δ) =poly(comp(Mlat, ε, δ),log|Φ|). (4)
We say that Mlatadmits strong statistical modularity if Eq. (4) holds when comp(Mlat, ε, δ)is the
minimax sample complexity for Mlat.
In the sequel, we examine well-studied MDP classes Mlat(e.g., those which admit low Bellman
rank [JKALS17]) and choose comp(Mlat, ε, δ)based on natural upper bounds on their optimal
sample complexity; in this case we will simply say they are (or are not) statistical modular, leaving
the complexity upper bound comp implicit. Following prior work [KAL16; DKJADL19; MHKL20;
ZSUWAS22; MFR23; DR21; Mha+20; MLJL21], we use log|Φ|as a proxy for the statistical
complexity of supervised learning with the decoder class Φ.6
The two most notable examples of statistical modularity covered by prior work are: (i) taking
Mlatas the set of tabular MDPs admits strong statistical modularity [DKJADL19; MHKL20;
MFR23], and (ii) taking Mlatas the set of linear MDPs admits statistical modularity with complexity
poly(d, H,|A|, ε−1,log 
δ−1
)[AKKS20; UZS22; MCKJA24; MBFR23].7Interestingly, the latter
does not admit strong statistical modularity, because the optimal rate for Mlatdoes not scale with
|A|, but the rate for ⟪Mlat,Φ⟫necessarily does [LS20; HLSW21]. The results of Mhammedi et al.;
Misra et al.; Song et al. [Mha+20; MLJL21; SWFK24] can also be viewed as instances of statistical
modularity for other base MDP classes.
3.2 Lower bounds: Impossibility of statistical modularity
Our main result in this section is to show that for most MDP classes Mlatconsidered in the
literature on sample-efficient reinforcement learning with function approximation [RVR13; JKALS17;
SJKAL19; MJTS20; AJSWY20; Li09; DVRZ19; WSY20; ZGS21; Du+21; JLM21; FKQR21],
statistical modularity (under the natural complexity upper bound for the class of interest) is impossible .
Our central technical result is the following lower bound, which shows that statistical modularity
can be impossible even when the base MDP is known to the learner a-priori . The lower bound is a
significant generalization of the result from Song et al. [SWFK24]; we first state the lower bound,
then discuss implications.
Theorem 3.1 (Impossibility of statistical modularity) .For every N≥4, there exists a decoder class
Φwith|Φ|=Nand a family of base MDPs Mlatsatisfying (i) |M lat|= 1, (ii)H≤ O(log(N)),
(iii)|S|=|X| ≤ N2, (iv)|A|= 2, and such that
1. For all ε, δ > 0, we have comp(Mlat, ε, δ) = 0 .
2. For an absolute constant c >0,comp(⟪Mlat,Φ⟫, c, c)≥Ω(N/log(N)).
In other words, even when the base dynamics are fully known, strong statistical modularity
(in this case, poly(log|Φ|)complexity) is impossible; any algorithm will require at least
min{√
S,2Ω(H)/H,|Φ|/log|Φ|}episodes to learn a near-optimal policy for a latent-dynamics MDP
⟪Mlat, ψ⟫∈⟪Mlat,Φ⟫.
6Our main results easily extend to infinite classes through standard arguments.
7In the latter case, the latent-dynamics class ⟪Mlat,Φ⟫may be seen to be a set of low-rank MDPs (that
is, linear MDPs with unknown features), so that low-rank MDP algorithms may be applied directly on the
observations (Appendix E.2).
5Base MDP class MlatStatistical
Modularity?
Tabular ✓
Contextual Bandits ✓
Low-Rank MDP ✓
Known Deterministic MDP ( |M lat|= 1) ✓
Low State Occupancy ( ∀π:S → ∆(A)) ✓
Model Class + Pushforward Coverability ✓
Linear CB/MDP ✗⋆
Model Class + Coverability ( ∀πM:M∈ M ) ✗
Known Stochastic MDP ( |M lat|= 1) ✗
Bellman Rank ( Q-type or V-type) ✗
Eluder Dimension + Bellman Completeness ✗
Q⋆-Irrelevant State Abstraction ✗
Linear Mixture MDP ✗
Linear Q⋆/V⋆✗
Low State/State-Action Occupancy ( ∀πM:M∈ M) ✗
Bisimulation ?
Low State-Action Occupancy ( ∀π:S → ∆(A)) ?⋆
Model Class + Coverability ( ∀π:S → ∆(A)) ?Figure 1: Summary of statistical
modularity (SM) results.
✓:SM is possible for a nat-
ural choice of comp(·)(e.g.,
poly(|S|,|A|, H, ε−1,log 
δ−1
)
for tabular MDPs).
✗:SM is not possible with natural
choices of comp(·).
?:open.
⋆:SM is possible if willing to pay
for (suboptimal) |A|complexity.
See Appendix E.2 for precise
descriptions of each setting and
our choices for their complexities.
Intuition for lower bound. The intuition behind the lower bound in Theorem 3.1 is as follows: the
unobserved latent state space consists of N=|Φ|binary trees (indexed from 1toN), each with N
leaf nodes. The starting distribution is uniform over the roots of the Ntrees, and the agent receives a
reward of 1if and only if they navigate to the leaf node that corresponds to the index of their current
tree. The observed state space is identical to the latent state space, but the emission process shifts the
index of the tree by an amount which is unknown to the agent. Despite the base MDP being known
and the decoder class satisfying realizability, the agent requires near-exhaustive search to identify the
value of the shift and recover a near-optimal policy.
A taxonomy of statistical modularity. As a corollary, we prove that many (but not all) well-studied
function approximation settings do not admit statistical modularity by embedding them into the
lower bound construction of Theorem 3.1 (as well as a variant of the result, Theorem E.1). Our
results are summarized in Figure 1. Our impossibility results highlight the following phenomenon:
many MDP classes Mlatthat place structural assumptions via the value functions (e.g., MDPs
with linear- Q⋆/V⋆[Du+21] or MDPs with a Bellman complete value function class of bounded
eluder dimension [JLM21; WSY20]) become intractable under latent dynamics. Intuitively, this
is because it is not possible to take advantage of structure in value functions without learning a
good representation, and, simultaneously, these assumptions are too weak by themselves to enable
learning such a representation. Meanwhile, MDP classes Mlatthat place structural assumptions on
the transition distribution (e.g., MDPs with low state occupancy complexity [Du+21] or low-rank
MDPs [AKKS20]) are sometimes (but not always) tractable under latent dynamics.8
We point to Appendix E.2 for background on all the settings in Figure 1 and proofs that they are (or
are not) statistically modular. We remark that it is fairly straightforward to embed most of the MDP
classes of Figure 1 into the construction of Theorem 3.1 since it only uses only a single base MDP
Mlat, and we expect that many other base MDP classes can similarly be shown to be intractable.
However, proving the positive results in Figure 1 requires establishing several new results showing
that certain base classes are tractable under latent dynamics; most notably, we next discuss the case
ofpushforward coverability .
3.3 Upper bounds: Pushforward-coverable MDPs are statistically modular
Our main postive result concerning statistical modularity is to highlight pushforward coverability
[XJ21; AFK24; MFR24]—a strengthened version of the coverability parameter introduced in Xie
et al. [XFBJK23]—as a general structural parameter that enables sample-efficient reinforcement
learning under latent dynamics.
8If one is willing to pay for suboptimal |A|factors, then more (but not all) classes become statistically
tractable (e.g., linear MDPs [JYWJ20] and MDPs with low state-action occupancy [Du+21]).
6Definition 3.3 (Pushforward coverability) .The pushforward coverability coefficient Cpushfor an
MDP Mlatwith transition kernel Platis defined by
Cpush(Mlat) = max
h∈[H]inf
µ∈∆(S)sup
(s,a,s′)∈S×A×SPlat,h−1(s′|s, a)
µ(s′). (5)
Concrete examples [AFK24; MFR24] include: (i) tabular MDPs Mlatadmit Cpush(Mlat)≤ |S| ; and
(ii) Low-Rank MDPs Mlat(with or without known features) in dimension dadmit Cpush(Mlat)≤d.
Further examples include analytically sparse Low-Rank MDPs [GMR24] and Exogenous Block
MDPs with weakly correlated noise [MFR24]. Our main result is as follows.
Theorem 3.2 (Pushforward-coverable MDPs are statistically modular) .LetMlatbe a base MDP
class such that each Mlat∈ M lathas pushforward coverability bounded by Cpush(Mlat)≤Cpush.
Then, for any decoder class Φ, we have:
1.comp(Mlat, ε, δ)≤poly(Cpush,|A|, H,log|M lat|, ε−1,log 
δ−1
), and
2.comp(⟪Mlat,Φ⟫, ε, δ)≤poly(Cpush,|A|, H,log|M lat|,log|Φ|, ε−1,log 
δ−1
,log log |S|).
Theorem 3.2 shows that, modulo a term that is doubly-logarithmic in |S|, latent pushforward cover-
ability enables statistical modularity. That is, when the base (latent) dynamics satisfy pushforward
coverability, there exists an algorithm for the latent-dynamics setting which scales with the statistical
complexity of the base MDP class and log|Φ|. We suspect that the additional log log |S|factor is not
essential and can be removed with a more sophisticated analysis. We note that the complexity comp
chosen above is not the minimax complexity for Mlat, since every set of pushforward coverable
MDPs is also a set of coverable MDPs with a potentially smaller coverability parameter [AFK24].
Let us provide some intuition for this result. We firstly note that when M⋆
lathas pushforward cover-
ability parameter Cpush, it holds that for any emission process ψ⋆, the observation-level MDP M⋆
obs:=
⟪M⋆
lat, ψ⋆⟫also satisfies pushforward coverability with the same parameter Cpush(Lemma D.5). Yet,
despite access to realizable base MDP class Mlatand decoder class Φ, it is unclear whether the latent-
dynamics MDP M⋆
obssatisfies any of the observation-level function approximation conditions required
by existing approaches that provide sample complexity guarantees under pushforward coverability.
In particular, known algorithms for this setting either require a Bellman-complete value function
class [XFBJK23], a class realizing certain density ratios [AFJSX24; AFK24], or a realizable model
class [AFK24], and it is highly nontrivial to construct these for the latent-dynamics MDP M⋆
obs=
⟪M⋆
lat, ψ⋆⟫given only the base MDP class Mlatand the decoder class Φ. Intuitively, this is because
the former observation-level function approximation classes capture properties of the observation-
level dynamics which cannot be obtained without some knowledge of the emission process.
Our main technical contribution is to establish a new structural property for pushforward-coverable
MDPs (Lemma F.1): low-dimensional linear embeddings of their latent models can approximate the
Bellman updates for an arbitrary set of test functions (as long as the set is not too large). We use
this property to construct low-dimensional linear features that can approximate Bellman backups
inobservation-space , allowing us to (approximately) satisfy the Bellman completeness assumption
required to apply GOLF[JLM21] to the latent-dynamics MDP. A fascinating open question is whether
a similar approach can be used to establish that standard (as opposed to pushforward) coverable
MDPs are statistically modular, which would encompass all other known positive cases of statistical
modularity (cf. Figure 1). We refer interested readers to a more detailed technical overview in
Appendix F.1, as well as the full proof in Appendix F.2.
4 Algorithmic Modularity
We now turn our attention to algorithmic modularity . Specifically, we aim for observable-to-latent
reductions , whereby—via representation learning—RL under latent dynamics can be efficiently
reduced to the simpler problem of RL with latent states directly observed. Since algorithmic
modularity is a stronger property than statistical modularity, we sidestep the previous lower bounds in
Section 3 through additional feedback and modeling assumptions. Our main result for this section is a
new meta-algorithm, O2L , which, under these assumptions (and when equipped with an appropriately
designed representation learning oracle), acts as a universal reduction in the sense that, whenever the
representation learning oracle has low risk, the reduction transforms anysample-efficient algorithm
foranybase MDP class into a sample-efficient algorithm for the induced latent-dynamics MDP class.
7Algorithm 1 O2L: Observable-to-Latent Reduction
1:input : Epochs T, episodes K, decoder set Φ, rep. learning oracle REPLEARN , base alg. ALGlat.
2:fort= 1,2,···, Tdo
3: REPLEARN chooses a representation bϕ(t):X → S ∈ Φbased on data collected so far.
4: Initialize new instance of A LGlat.
5: fork= 1,2,···, Kdo //ALGlatplays Krounds in the “ bϕ(t)-compressed dynamics.”
6: ALGlatchooses policy π(t,k)
lat:S ×[H]→∆(A).
7: Deploy πlat◦bϕ(t)to collect trajectory {x(t,k)
h, a(t,k)
h, r(t,k)
h}H
h=1.
8: Update A LGlatwith compressed trajectory {bϕ(t)
h(x(t,k)
h), a(t,k)
h, r(t,k)
h}H
h=1.
9: end for
10: ALGlatreturns final policy bπ(t):S×[H]→∆(A), deploy bπ(t)◦bϕ(t)to collect one trajectory.
11:end for
12:return bπ=Unif(bπ(1)◦bϕ(1), . . . ,bπ(T)◦bϕ(T)).
Setup and O2L meta-algorithm. For the results in this section, we denote the (unknown) latent-
dynamics MDP of interest by M⋆
obs:=⟪M⋆
lat, ψ⋆⟫, and use ϕ⋆:= (ψ⋆)−1to denote the true decoder.
TheO2L meta-algorithm (Algorithm 1) learns a near-optimal policy for M⋆
obsby alternating between
performing representation learning and executing a black-box “base” RL algorithm (designed for the
base MDP) on the learned representation; this approach is inspired by empirical methods that blend
representation learning and RL in the latent space (e.g., [GKBNB19; SAGHCB20; Ni+24]).
Concretely, the algorithm takes as input a representation learning oracle REPLEARN and a base RL
algorithm ALGlatthat operates in the latent space. In each epoch t∈[T],REPLEARN produces a new
representation bϕ(t):X → S based on data observed so far (potentially using additional side informa-
tion, which we will elaborate on in the sequel). Then, the reduction invokes ALGlat, usingbϕ(t)to sim-
ulate access to the true latent states. In particular, ALGlatruns for Kepisodes, where at each episode
k: (i)ALGlatproduces a latent policy πlat(t,k):S×[H]→∆(A), (ii) the latent policy is transformed
into an observation-level policy via composition with bϕ(t), i.e.πlat(t,k)◦bϕ(t), which is then deployed
to produce a trajectory {x(t,k)
h, a(t,k)
h, r(t,k)
h}H
h=1, and (iii) the trajectory is compressed through bϕ(t)
and used to update ALGlatvia{bϕ(t)
h(x(t,k)
h), a(t,k)
h, r(t,k)
h}H
h=1(cf. Line 8 of Algorithm 1).9After the
Krounds conclude, ALGlatproduces a final latent policy bπ(t)
lat:S ×[H]→∆(A). The final policy
bπchosen by the O2L algorithm is a uniform mixture of bπ(t)
lat◦bϕ(t)over all the epochs.
The central assumption behind O2L is that the base algorithm ALGlatcan achieve low-risk in
the underlying base MDP M⋆
latif given access to the true latent states sh=ϕ⋆(xh).Beyond this
assumption, we require that the representation learning oracle REPLEARN can learn a sufficiently
high-quality representation. In our applications, this will be made possible by assuming access
to a realizable decoder class Φand two distinct assumptions: hindsight observability (Section 4.1)
and conditions enabling self-predictive representation learning (Section 4.2). We will show that
under these conditions, we can instantiate a representation learning oracle such that O2L inherits
the sample complexity guarantee for A LGlat, thereby achieving algorithmic modularity.
4.1 Algorithmic modularity via hindsight observability
Our first algorithmic result bypasses the hardness in Section 3 by considering the setting of hindsight
observability , which has garnered recent interest in the context of POMDPs [LADZ23; GCWXWB24;
SLS23; LXJZV24]. Here, we assume that at training time (but not during deployment), the algorithm
has access to additional feedback in the form of the true latent states, which are revealed at the end of
each episode.
Assumption 4.1 (Hindsight Observability [LADZ23]) .The latent states (ϕ⋆
1(x1), . . . , ϕ⋆
H(xH))are
revealed to the learner after each episode (x1, a1, r1, . . . , x H, aH, rH)concludes.
We emphasize that in the hindsight observability framework, the learner must still execute observation-
space policies πobs:X×[H]→∆(A), as the latent states are only revealed at the end of each episode .
Under hindsight observability, we can instantiate the representation learning oracle in O2L so that the
9Note that, if bϕis inaccurate, the compressed trajectory cannot necessarily be viewed as being generated by a
latent MDP, and must instead be viewed as coming from a Partially Observed MDP (Appendix I.1.1).
8reduction achieves low risk for any choice of black-box base algorithm ALGlat.In particular, we make
use of online classification oracles, which use the revealed latent states to achieve low classification
loss with respect to ϕ⋆under adaptively generated data. We first state a guarantee based on generic
classification oracles, then instantiate it to give a concrete end-to-end sample complexity bound.
Formally, at each step t, the online classification oracle, denoted via REPclass, is given the
states and hindsight observations collected so far and produces a deterministic estimate bϕ(t)=
REPclass({x(i)
h, ϕ⋆
h(x(i)
h)}i<t,h≤H)for the true decoder ϕ⋆. We measure the regret of the oracle via
the0/1loss for classification:
Regclass(T):=TX
t=1HX
h=1Eπ(t)∼p(t)Eπ(t)h
Ibϕ(t)
h(xh)̸=ϕ⋆
h(xh)	i
,
where p(t)represents a randomization distribution over the policy π(t). Our reduction succeeds under
the assumption that the oracle has low expected regret.
Assumption 4.2. For any (possibly adaptive) sequence π(t), with π(t)∼p(t), the online classification
oracle REPclass has expected regret bounded by
E[Regclass(T)]≤Est class(T),
where Est class(T)is a known upper bound.
We apply such an oracle within O2L as follows: at the end of each iteration t∈[T]in
O2L , we sample k∼[K]uniformly, and update the classification oracle with the trajectory
(x(t,k)
1, a(t,k)
1, r(t,k)
1), . . . , (x(t,k)
Ha(t,k)
H, r(t,k)
H); see the proof of Theorem 4.1 for details. We let
Risk obs(TK)denote the risk of the O2L reduction when run for Tepochs of Kepisodes, and
we let Risk ⋆(K):=E[Risk(K,ALGlat, M⋆
lat)]denote the expected risk of ALGlatwhen executed
onM⋆
latwith access to the true latent states sh=ϕ⋆(xh)forKepisodes.
Theorem 4.1 (Risk bound for O2L under hindsight observability) .LetALGlatbe a base algorithm
with base risk Risk ⋆(K), and REPclass a representation learning oracle satisfying Assumption 4.2.
Then Algorithm 1, with inputs T, K, Φ,REPclass, and ALGlat, has expected risk
E[Risk obs(TK)]≤Risk ⋆(K) +2K
TEst class(T).
This result shows that we can achieve sublinear risk under latent dynamics as long as (i) the base
algorithm achieves sublinear risk Risk ⋆(K)given access to the true latent states, and (ii) the classifi-
cation oracle achieves sublinear regret Est class(T). Notably, the result is fully modular, meaning we
require no explicit conditions on the latent dynamics or the base algorithm, and is computationally
efficient whenever the base algorithm and classification oracle are efficient.
To make Theorem 4.1 concrete, we next provide a representation learning oracle ( EXPWEIGHTS .DR;
Algorithm 3 in Appendix G.1) based on a derandomization of the classical exponential weights
mechanism, which satisfies Assumption 4.2 with Est class≲Hlog|Φ|whenever it has access to a
class Φthat satisfies decoder realizability.
Lemma 4.1 (Online classification via EXPWEIGHTS .DR).Under decoder realizability (ϕ⋆∈Φ),
EXPWEIGHTS .DR(Algorithm 3) satisfies Assumption 4.2 with10
Est class(T) =eO(Hlog|Φ|).
Instantiating Theorem 4.1 with the above representation learning oracle, we obtain the following
algorithmic modularity result.
Corollary 4.1 (Algorithmic modularity under hindsight observability) .For any base algorithm
ALGlat, under decoder realizability (ϕ⋆∈Φ),O2L with inputs T, K, Φ,EXPWEIGHTS .DR,and
ALGlatachieves
E[Risk obs(TK)]≲Risk ⋆(K) +HKlog|Φ|
T.
Consequently, for any ALGlat, setting T≈KHlog|Φ|/Risk ⋆(K)achieves E[Risk obs(TK)]≲
Risk ⋆(K)with a number of trajectories TK=eO(K2Hlog|Φ|/Risk⋆(K)).
10In this section, the notations eO,≈,and≲ignore only constants and logarithmic factors of H.
9Beyond achieving algorithmic modularity, this result shows that under hindsight observability, we
can achieve strong statistical modularity (modulo possible Hfactors) for every base MDP class
Mlat, an important result in its own right.11As an example, suppose that Risk ⋆(K) =O(K−1/2),
which is satisfied by many standard algorithms of interest [JKALS17; JYWJ20; JLM21; FKQR21].
Then, setting Taccording to Corollary 4.1 obtains an expected risk bound of εusingO(Hlog|Φ|/ε5)
trajectories.
Remark 4.1 (Online versus offline oracles) .Theorem 4.1 critically uses that assumption that REPclass
satisfies an online classification error bound to handle the fact that data is generated adaptively based
on the estimators bϕ(1), . . . ,bϕ(T)it produces, which is by now a relatively standard technique in the
design of interactive decision making algorithms [FR20; FKQR21; FR23]. We note that under
coverability and other exploration conditions, online oracles for classification can be directly obtained
from offline (i.e. supervised) classification oracles [XFBJK23; BRS24; FHQR24].
4.2 Algorithmic modularity via self-predictive estimation
We complement the above results by studying the general online RL setting without hindsight
observations. To address this more challenging setting, we design an optimistic self-predictive
estimation objective (Eq. (7)), which learns a representation by jointly fitting a decoder together
with a latent model. We prove that any representation learning oracle that attains low regret with
respect to this objective can be used in O2L to obtain observable-to-latent reductions for any low-risk
base algorithm ALGlat(for a formal statement, see Theorem A.1). We provide a (computationally
inefficient) estimator ( SELFPREDICT .OPT; Algorithm 4 in Appendix H.1) which we show attains
low optimistic self-regret under certain statistical conditions (namely, coverability of the base MDP
and a function approximation condition enabling us to express the self-prediction target as a latent
model, see Lemma A.1 for a formal statement), thereby obtaining an end-to-end reduction for the
general online RL setting. For lack of space, these results are deferred to Appendix A.
5 Discussion
Our work initiates the study of statistical and algorithmic modularity for reinforcement learning under
general latent dynamics. Our positive and negative results serve as a first step toward a unified theory
for reinforcement learning in the presence of high-dimensional observations. To this end, we close
with some important future directions and open problems.
Statistical modularity. Can we obtain a unified characterization for the statistical complexity of RL
under latent dynamics with a given class of base MDPs Mlat? Our results in Section 3 suggest that
this will require new tools that go beyond existing notions of statistical complexity. Toward resolving
this problem, concrete questions that are not yet understood include: (i) Is coverability [XFBJK23]
(as opposed to pushforward coverability) sufficient for learnability under latent dynamics? (ii) Is the
Exogenous Block MDP problem [EMKAL22; MFR24]—a special case of our general framework—
statistically tractable? Lastly, are there additional types of feedback that are weaker than hindsight
observability, yet suffice to bypass the hardness results in Section 3?
Algorithmic modularity. Can we derive a unified representation learning objective that enables
algorithmic modularity whenever statistical modularity is possible? Ideally, such an objective would
be computationally tractable. Alternatively, can we show that algorithmic modularity fundamentally
requires stronger modeling assumptions than statistical modularity? Toward addressing the problems
above, a first step might be to understand: (i) What are the minimal statistical assumptions under
which we can minimize the self-predictive objective in Section 4.2? (ii) How can we encourage
finding good representations via self-prediction beyond the use of optimism over the base (latent)
models; and (iii) when can we minimize self-prediction in a computationally efficient fashion?
Acknowledgements
Nan Jiang acknowledges funding support from NSF IIS-2112471, NSF CAREER IIS-2141781,
Google Scholar Award, and Sloan Fellowship.
11Formally, while we have defined the statistical modularity condition in terms of high-probability risk bounds,
it is straightforward to extend it to instead consider expected risk bounds.
10References
[ACK24] Philip Amortila, Tongyi Cao, and Akshay Krishnamurthy. “Mitigating Covariate
Shift in Misspecified Regression With Applications to Reinforcement Learning”.
In:Conference on Learning Theory . 2024.
[AFJSX24] Philip Amortila, Dylan J. Foster, Nan Jiang, Ayush Sekhari, and Tengyang Xie.
“Harnessing Density Ratios for Online Reinforcement Learning”. In: International
Conference on Learning Representations . 2024.
[AFK24] Philip Amortila, Dylan J Foster, and Akshay Krishnamurthy. “Scalable Online
Exploration via Coverability”. In: International Conference on Machine Learning .
2024.
[AHKLLS14] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert
Schapire. “Taming the monster: A fast and simple algorithm for contextual bandits”.
In:International Conference on Machine Learning . 2014.
[AJKS22] Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learn-
ing: Theory and algorithms .https://rltheorybook.github.io/ . Version:
January 31, 2022. 2022.
[AJSWY20] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. “Model-
Based Reinforcement Learning with Value-Targeted Regression”. In: International
Conference on Machine Learning . 2020.
[AKKS20] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. “FLAMBE:
Structural Complexity and Representation Learning of Low Rank MDPs”. In:
Neural Information Processing Systems . 2020.
[AOM17] Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. “Minimax Regret
Bounds for Reinforcement Learning”. In: International Conference on Machine
Learning . 2017.
[AZ22] Alekh Agarwal and Tong Zhang. “Model-based RL with Optimistic Posterior
Sampling: Structural Conditions and Sample Complexity”. In: Neural Information
Processing Systems . 2022.
[Bak+22] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet,
Brandon Houghton, Raul Sampedro, and Jeff Clune. “Video PreTraining (VPT):
Learning to Act by Watching Unlabeled Online Videos”. In: Neural Information
Processing Systems . 2022.
[BLM13] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequali-
ties: A nonasymptotic theory of independence . Oxford university press, 2013.
[Bro+22] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis,
Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine
Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil
J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei
Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch,
Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell
Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin
Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran,
Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun
Xu, Tianhe Yu, and Brianna Zitkovich. “RT-1: Robotics Transformer for Real-
World Control at Scale”. In: arXiv:2212.06817 . 2022.
[BRS24] Adam Block, Alexander Rakhlin, and Abhishek Shetty. “On the Performance of
Empirical Risk Minimization with Smoothed Data”. In: Conference on Learning
Theory . 2024.
[CBL06] Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, Learning, and Games . Cam-
bridge university press, 2006.
[CJ19] Jinglin Chen and Nan Jiang. “Information-Theoretic Considerations in Batch
Reinforcement Learning”. In: International Conference on Machine Learning .
2019.
[DKJADL19] Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik,
and John Langford. “Provably Efficient RL With Rich Observations via Latent
State Decoding”. In: International Conference on Machine Learning . 2019.
11[DMKV21] Omar Darwiche Domingues, Pierre Ménard, Emilie Kaufmann, and Michal Valko.
“Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Re-
visited”. In: Algorithmic Learning Theory . 2021.
[DMRY20] Sarah Dean, Nikolai Matni, Benjamin Recht, and Vickie Ye. “Robust Guarantees
for Perception-Based Control”. In: Learning for Dynamics and Control . 2020.
[DR21] Sarah Dean and Benjamin Recht. “Certainty Equivalent Perception-Based Control”.
In:Learning for Dynamics and Control . 2021.
[Du+21] Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen
Sun, and Ruosong Wang. “Bilinear Classes: A Structural Framework for Provable
Generalization in RL”. In: International Conference on Machine Learning . 2021.
[DVRZ19] Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. “Provably Efficient Rein-
forcement Learning With Aggregated States”. In: arXiv:1912.06366 . 2019.
[EFMKL22] Yonathan Efroni, Dylan J Foster, Dipendra Misra, Akshay Krishnamurthy, and
John Langford. “Sample-Efficient Reinforcement Learning in the Presence of
Exogenous Information”. In: Conference on Learning Theory . 2022.
[EMKAL22] Yonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, and
John Langford. “Provable RL With Exogenous Distractors via Multistep Inverse
Dynamics”. In: International Conference on Learning Representations . 2022.
[FGH23] Dylan J Foster, Noah Golowich, and Yanjun Han. “Tight Guarantees for Interactive
Decision Making with the Decision-Estimation Coefficient”. In: Conference on
Learning Theory . 2023.
[FGQRS23] Dylan J Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, and Ayush Sekhari.
“Model-Free Reinforcement Learning with the Decision-Estimation Coefficient”.
In:Neural Information Processing Systems . 2023.
[FHQR24] Dylan J Foster, Yanjun Han, Jian Qian, and Alexander Rakhlin. “Online Es-
timation via Offline Estimation: An Information-Theoretic Framework”. In:
arXiv:2404.10122 . 2024.
[FKQR21] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. “The Statisti-
cal Complexity of Interactive Decision Making”. In: arXiv:2112.13487 . 2021.
[FR20] Dylan J Foster and Alexander Rakhlin. “Beyond UCB: Optimal and Efficient
Contextual Bandits With Regression Oracles”. In: International Conference on
Machine Learning . 2020.
[FR23] Dylan J Foster and Alexander Rakhlin. “Foundations of Reinforcement Learning
and Interactive Decision Making”. In: arXiv:2312.16730 . 2023.
[FWYDY20] Fei Feng, Ruosong Wang, Wotao Yin, Simon S Du, and Lin Yang. “Provably
Efficient Exploration for Reinforcement Learning Using Unsupervised Learning”.
In:Neural Information Processing Systems . 2020.
[GCWXWB24] Jiacheng Guo, Minshuo Chen, Huan Wang, Caiming Xiong, Mengdi Wang, and
Yu Bai. “Sample-Efficient Learning of POMDPs with Multiple Observations In
Hindsight”. In: International Conference on Learning Representations . 2024.
[Gee00] S. A. van de Geer. Empirical Processes in M-Estimation. Cambridge University
Press, 2000.
[GKBNB19] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Belle-
mare. “DeepMDP: Learning Continuous Latent Space Models for Representation
Learning”. In: International Conference on Machine Learning . 2019.
[GMR24] Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. “Exploring and Learning in
Sparse Linear MDPs Without Computationally Intractable Oracles”. In: Symposium
on Theory of Computing . 2024.
[Guo+22] Zhaohan Guo, Shantanu Thakoor, Miruna Pîslar, Bernardo Avila Pires, Florent
Altché, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill,
Yunhao Tang, Michal Valko, Rémi Munos, Mohammad Gheshlaghi Azar, and
Bilal Piot. “BYOL-Explore: Exploration by Bootstrapped Prediction”. In: Neural
Information Processing Systems . 2022.
[Haf+19] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak
Lee, and James Davidson. “Learning Latent Dynamics for Planning From Pixels”.
In:International Conference on Machine Learning . 2019.
12[HLBN19] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. “Dream to
Control: Learning Behaviors by Latent Imagination”. In: International Conference
on Learning Representations . 2019.
[HLNB21] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. “Master-
ing Atari With Discrete World Models”. In: International Conference on Learning
Representations . 2021.
[HLSW21] Botao Hao, Tor Lattimore, Csaba Szepesvári, and Mengdi Wang. “Online Sparse
Reinforcement Learning”. In: International Conference on Artificial Intelligence
and Statistics . 2021.
[HPBL23] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. “Mastering
Diverse Domains Through World Models”. In: arXiv:2301.04104 . 2023.
[Jia24] Nan Jiang. “A Note on Loss Functions and Error Compounding in Model-based
Reinforcement Learning”. In: arXiv:2404.09946 . 2024.
[JKALS17] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E
Schapire. “Contextual Decision Processes With Low Bellman Rank Are PAC-
Learnable”. In: International Conference on Machine Learning . 2017.
[JLM21] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. “Bellman Eluder Dimension: New
Rich Classes of RL Problems, and Sample-Efficient Algorithms”. In: Neural
Information Processing Systems . 2021.
[JYWJ20] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. “Provably Efficient
Reinforcement Learning With Linear Function Approximation”. In: Conference
on Learning Theory . 2020.
[KAL16] Akshay Krishnamurthy, Alekh Agarwal, and John Langford. “PAC Reinforcement
Learning With Rich Observations”. In: Neural Information Processing Systems .
2016.
[KFPM21] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. “RMA: Rapid
Motor Adaptation for Legged Robots”. In: Robotics: Science and Systems . 2021.
[LADZ23] Jonathan Lee, Alekh Agarwal, Christoph Dann, and Tong Zhang. “Learning in
POMPDs Is Sample-Efficient With Hindsight Observability”. In: International
Conference on Machine Learning . 2023.
[Lam+24] Alex Lamb, Riashat Islam, Yonathan Efroni, Aniket Rajiv Didolkar, Dipendra
Misra, Dylan J Foster, Lekan P Molu, Rajan Chari, Akshay Krishnamurthy, and
John Langford. “Guaranteed Discovery of Control-Endogenous Latent States with
Multi-Step Inverse Models”. In: Transactions on Machine Learning Research .
2024.
[LFDA16] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. “End-To-End
Training of Deep Visuomotor Policies”. In: The Journal of Machine Learning
Research . 2016.
[Li09] Lihong Li. A Unifying Framework for Computational Reinforcement Learning
Theory . Rutgers, The State University of New Jersey, 2009.
[LS20] Tor Lattimore and Csaba Szepesvári. Bandit Algorithms . Cambridge University
Press, 2020.
[LSA20] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. “CURL: Contrastive Unsu-
pervised Representations for Reinforcement Learning”. In: International Confer-
ence on Machine Learning . 2020.
[LXJZV24] Michael Lanier, Ying Xu, Nathan Jacobs, Chongjie Zhang, and Yevgeniy V orobey-
chik. “Learning Interpretable Policies in Hindsight-Observable POMDPs through
Partially Supervised Reinforcement Learning”. In: arXiv:2402.09290 . 2024.
[MBFR23] Zak Mhammedi, Adam Block, Dylan J Foster, and Alexander Rakhlin. “Efficient
Model-Free Exploration in Low-Rank MDPs”. In: Neural Information Processing
Systems . 2023.
[MCKJA24] Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal.
“Model-Free Representation Learning and Exploration in Low-Rank Mdps”. In:
Journal of Machine Learning Research . 2024.
13[MFR23] Zakaria Mhammedi, Dylan J Foster, and Alexander Rakhlin. “Representation
Learning With Multi-Step Inverse Kinematics: An Efficient and Optimal Approach
to Rich-Observation RL”. In: International Conference on Machine Learning .
2023.
[MFR24] Zakaria Mhammedi, Dylan J Foster, and Alexander Rakhlin. “The Power of Resets
in Online Reinforcement Learning”. In: arXiv:2404.15417 . 2024.
[Mha+20] Zakaria Mhammedi, Dylan J Foster, Max Simchowitz, Dipendra Misra, Wen Sun,
Akshay Krishnamurthy, Alexander Rakhlin, and John Langford. “Learning the
Linear Quadratic Regulator From Nonlinear Observations”. In: Neural Information
Processing Systems . 2020.
[MHKL20] Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. “Kine-
matic State Abstraction and Provably Efficient Rich-Observation Reinforcement
Learning”. In: International Conference on Machine Learning . 2020.
[MJTS20] Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. “Sample Complexity
of Reinforcement Learning Using Linearly Combined Model Ensembles”. In:
International Conference on Artificial Intelligence and Statistics . 2020.
[MLJL21] Dipendra Misra, Qinghua Liu, Chi Jin, and John Langford. “Provable Rich Observa-
tion Reinforcement Learning With Combinatorial Latent States”. In: International
Conference on Learning Representations . 2021.
[Ni+24] Tianwei Ni, Benjamin Eysenbach, Erfan Seyedsalehi, Michel Ma, Clement
Gehring, Aditya Mahajan, and Pierre-Luc Bacon. “Bridging State and History
Representations: Understanding Self-Predictive RL”. In: arXiv:2401.08898 . 2024.
[NRKFG22] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav
Gupta. “R3M: A Universal Visual Representation for Robot Manipulation”. In:
arXiv:2203.12601 . 2022.
[OVR16] Ian Osband and Benjamin Van Roy. “On Lower Bounds for Regret in Reinforce-
ment Learning”. In: arXiv:1608.02732 . 2016.
[PAED17] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. “Curiosity-
Driven Exploration by Self-Supervised Prediction”. In: International Conference
on Machine Learning . 2017, pp. 2778–2787.
[RH23] Philippe Rigollet and Jan-Christian Hütter. “High-dimensional statistics”. In:
arXiv:2310.19244 . 2023.
[RVR13] Daniel Russo and Benjamin Van Roy. “Eluder Dimension and the Sample Com-
plexity of Optimistic Exploration”. In: Neural Information Processing Systems .
2013.
[SAGHCB20] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville,
and Philip Bachman. “Data-Efficient Reinforcement Learning with Self-Predictive
Representations”. In: International Conference on Learning Representations . 2020.
[Sch+20] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Lau-
rent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore
Graepel, Timothy Lillicrap, and David Silver. “Mastering Atari, Go, Chess and
Shogi by Planning With a Learned Model”. In: Nature . 2020.
[SJKAL19] Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
“Model-Based RL in Contextual Decision Processes: PAC Bounds and Exponential
Improvements Over Model-Free Approaches”. In: Conference on Learning Theory .
2019.
[SLS23] Ming Shi, Yingbin Liang, and Ness Shroff. “Theoretical Hardness and Tractability
of POMDPs in RL with Partial Hindsight State Information”. In: arXiv:2306.08762 .
2023.
[SWFK24] Yuda Song, Lili Wu, Dylan J Foster, and Akshay Krishnamurthy. “Rich-
Observation Reinforcement Learning with Continuous Latent Dynamics”. In:
International Conference on Machine Learning . 2024.
[SZSBKS23] Yuda Song, Yifei Zhou, Ayush Sekhari, J Andrew Bagnell, Akshay Krishnamurthy,
and Wen Sun. “Hybrid RL: Using Both Offline and Online Data Can Make RL
Efficient”. In: International Conference on Learning Representations . 2023.
14[Tan+17] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen,
Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. “# Exploration: A
Study of Count-Based Exploration for Deep Reinforcement Learning”. In: Neural
Information Processing Systems . 2017.
[Tan+23] Yunhao Tang, Zhaohan Daniel Guo, Pierre Harvey Richemond, Bernardo Avila
Pires, Yash Chandak, Remi Munos, Mark Rowland, Mohammad Gheshlaghi Azar,
Charline Le Lan, Clare Lyle, András György, Shantanu Thakoor, Will Dabney,
Bilal Piot, Daniele Calandriello, and Michal Valko. “Understanding Self-Predictive
Learning for Reinforcement Learning”. In: International Conference on Machine
Learning . 2023.
[UZS22] Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. “Representation Learning for
Online and Offline RL in Low-rank MDPs”. In: The Tenth International Conference
on Learning Representations . 2022.
[WSD15] Niklas Wahlström, Thomas B Schön, and Marc Peter Deisenroth. “From Pixels
to Torques: Policy Learning With Deep Dynamical Models”. In: International
Conference on Machine Learning . 2015.
[WSY20] Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. “Reinforcement Learning
with General Value Function Approximation: Provably Efficient Approach via
Bounded Eluder Dimension”. In: Neural Information Processing Systems . 2020.
[WYDW21] Tianhao Wu, Yunchang Yang, Simon Du, and Liwei Wang. “On Reinforcement
Learning With Adversarial Corruption and Its Application to Block MDP”. In:
International Conference on Machine Learning . 2021.
[XFBJK23] Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M Kakade. “The Role
of Coverage in Online Reinforcement Learning”. In: International Conference on
Learning Representations . 2023.
[XJ21] Tengyang Xie and Nan Jiang. “Batch Value-Function Approximation With Only
Realizability”. In: International Conference on Machine Learning . 2021.
[YFK21] Denis Yarats, Rob Fergus, and Ilya Kostrikov. “Image Augmentation Is All You
Need: Regularizing Deep Reinforcement Learning From Pixels”. In: International
Conference on Learning Representations . 2021.
[ZGS21] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. “Nearly Minimax Optimal
Reinforcement Learning for Linear Mixture Markov Decision Processes”. In:
Conference on Learning Theory . 2021.
[Zha06] Tong Zhang. “From ϵ-entropy to KL-entropy: Analysis of minimum information
complexity density estimation”. In: The Annals of Statistics . V ol. 34. 5. Institute of
Mathematical Statistics, 2006, pp. 2180–2210.
[Zha22] Tong Zhang. “Feel-Good Thompson Sampling for Contextual Bandits and Rein-
forcement Learning”. In: SIAM Journal on Mathematics of Data Science . 2022.
[ZMCGL21] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine.
“Learning Invariant Representations for Reinforcement Learning Without Recon-
struction”. In: International Conference on Learning Representations . 2021.
[ZSUWAS22] Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal,
and Wen Sun. “Efficient Reinforcement Learning in Block MDPs: A Model-Free
Representation Learning Approach”. In: International Conference on Machine
Learning . 2022.
15Contents
AOmitted Results from Section 4: Algorithmic Modularity via Self-predictive Estimation 17
A.1 Self-predictive estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.2 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.3 Instantiating the self-predictive estimation oracle . . . . . . . . . . . . . . . . . . 19
B Additional Discussion of Related Work 22
C Technical Tools 23
D Structural Properties of Coverability and Mismatch Functions 24
E Proofs and Additional Results for Section 3.2: Impossibility Results 28
E.1 Additional Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
E.2 Details for Figure 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
E.3 Proofs for Lower Bounds (Theorems 3.1 and E.1) . . . . . . . . . . . . . . . . . . 34
F Proofs for Section 3.3: Positive Results 40
F.1 Technical Overview: Low-dimensional embeddings for pushforward-coverable MDPs. 40
F.2 Proofs for Latent Model Class + Pushforward Coverability (Theorem 3.2) . . . . . 41
G Proofs and Additional Information for Section 4.1: Hindsight RL 52
G.1 Pseudocode and Proofs for E XPWEIGHTS .DR(Lemma 4.1) . . . . . . . . . . . . 52
G.2 Proofs for O2L Under Hindsight Observability (Theorem 4.1) . . . . . . . . . . . 53
H Proofs for Appendix A: Self-Predictive Estimation 56
H.1 Pseudocode and Proofs for S ELFPREDICT .OPT(Lemma A.1) . . . . . . . . . . . . 56
H.2 Proofs for Main Risk Bound (Theorem A.1) . . . . . . . . . . . . . . . . . . . . . 60
I Additional Results for Appendix A: Self-Predictive Estimation 63
I.1 O2L with Self-predictive Estimation and CorruptionRobust Base Algorithms . . 63
I.2 Proofs for Appendix I.1.2: Properties of ϕ-compressed POMDPs . . . . . . . . . . 66
I.3 Proofs for Appendix I.1.3: Risk Bound Under CorruptionRobustness (Theorem H.1) 71
I.4 Proofs for Appendix I.1.4: Examples of CorruptionRobust Algorithms . . . . . . 73
16A Omitted Results from Section 4: Algorithmic Modularity via
Self-predictive Estimation
In this section, we remove the assumption of hindsight observability used in Section 4.1 and instantiate
O2L in the general online RL setting. Rather than assume access to additional side-information,
we adopt a model-based representation learning approach, and augment our ability to perform
representation learning by equipping the representation learning algorithm with a set of base MDPs
Mlatin addition to the decoder class Φ. We will learn a representation by jointly fitting a decoder
and the base (latent) dynamics, which is a common approach in practice [GKBNB19; HLBN19;
Haf+19; HLNB21; Sch+20; SAGHCB20; Guo+22]. We firstly present in Appendix A.1 a new notion
ofoptimistic self-predictive regret which combines self-predictive representation learning with a form
of optimism over a learned latent model. We then show in Appendix A.2 that any representation
learning oracle that attains low regret, when used within O2L (Algorithm 1), leads to observable-to-
latent reductions that ensure low risk for anybase algorithm A LGlat, thereby achieving algorithmic
modularity. Lastly, in Appendix A.3, we instantiate this oracle under natural structural and function
approximation conditions, yielding end-to-end modularity and sample complexity guarantees.
A.1 Self-predictive estimation
Our self-predictive representation learning oracles learn to fit a representation ϕsuch that the induced
latent transitions (ϕh(xh)toϕh+1(xh+1)) can be accurately modeled by some base (latent) MDP
Mlat∈ M lat. To describe the objective, let us first introduce some notation. For a given MDP M
over either S(resp.X), we write Mh(rh, sh+1|sh, ah)(resp. Mh(rh, xh+1|xh, ah)) for the joint
conditional distribution over rewards and next states. Next, for any ϕ∈Φ, we define the pushforward
model forM⋆
obs,hinduced by ϕvia:

ϕh+1♯M⋆
obs,h
(r, s′|x, a):=X
x′:ϕh+1(x′)=s′M⋆
obs,h(r, x′|x, a). (6)
Thepushforward model forϕcaptures the forward probability of the estimated latent state ϕ(x′)
given a current observation x. To measure distance between models, we will use squared Hellinger
distance (e.g, Foster et al. [FKQR21]), defined via D2
H(P,Q) =R q
dP
dν−q
dQ
dν2dνfor a common
dominating measure ν. Then, for a base model Mlatand a decoder ϕ, the self-predictive error of
(Mlat, ϕ), at state-action pair xh, ah, is given by
[∆h(Mlat, ϕ)](xh, ah):=D2
H 
M lat,h(ϕh(xh), ah),
ϕh+1♯M⋆
obs,h
(xh, ah)
.
This term captures the ability of Mlat,h(ϕh(xh), ah)to predict the next latent state ϕh+1(xh+1)
which is obtained by the pushforward model
ϕh+1♯M⋆
obs,h
(xh, ah). Formally, in our model-based
representation learning setup, we consider oracles which, for each iteration twithin O2L , take as
input the trajectories collected so far and produce an estimate (cM(t)
lat,bϕ(t))for the decoder and base
model. The representation learning oracle’s self-predictive regret , for the sequence (cM(t)
lat,bϕ(t)), is
then defined as
Regself(T) =TX
t=1HX
h=0Eπ(t)∼p(t)Eπ(t)h
[∆h(cM(t)
lat,bϕ(t))](xh, ah)i
,
where p(t)represents a randomization distribution over the policy π(t).
On its own, minimizing this regret may lead to degenerate solutions, a widely observed phenomenon
in practice [Tan+23]. For example, in a standard combination lock MDP (e.g., Agarwal et al.; Misra
et al. [AJKS22; MHKL20]), a degenerate decoder-model pair that maps all observations to a single
latent state will have zero self-predictive loss until we reach the goal, which can take exponentially
long.12We address this via the notion of optimistic estimation used in Zhang; Foster et al. [Zha22;
FGQRS23], which biases the objective towards latent models with high return. This leads to the
12This is similar to the observation that naive value function approximation methods, such as Fitted Q-Iteration,
can fail to explore in online RL without optimism. We expect that given access to additional exploratory data
(e.g., in the Hybrid RL setting of Song et al. [SZSBKS23]), the latent optimism term can be removed.
17following optimistic self-predictive regret , defined for a parameter γ >0, via
Regself;opt(T, γ) =TX
t=1HX
h=0Eπ(t)∼p(t)Eπ(t)h
[∆h(cM(t)
lat,bϕ(t))](xh, ah)i
+γ−1(JM⋆
lat(πM⋆
lat)−JcM(t)
lat(πcM(t)
lat)). (7)
We assume going forward that REPself;optobtains low optimistic self-predictive regret; in Ap-
pendix A.3 we provide a maximum-likelihood-type estimator and conditions under which this holds.
Assumption A.1. For a parameter γ >0and any (possibly adaptive) sequence π(t), with π(t)∼p(t),
the online representation learning oracle REPself;optis proper (i.e. outputs cM(t)
lat∈ M latfor all
t∈[T]) and satisfies
E
Regself;opt(T, γ)
≤Est self;opt(T, γ),
where Est self;opt(T, γ)is a known upper bound.
We note that only the decoder bϕ(t)is used within O2L ; the model cM(t)
latis only used for analysis (and
possibly within the representation learner R EPself;opt).
A.2 Main result
We now state the main guarantee for O2L with self-predictive representation learning. Recall
thatRisk obs(TK)denotes the risk of the O2L reduction. Compared to the hindsight-observable
setting, we require a slightly stronger performance guarantee from the base algorithm ALGlat: our
result scales with the worst-case expected risk for ALGlatover all Mlat∈ M lat, defined via
Risk base(K):= supMlat∈M latE[Risk(K,ALGlat, M lat))].
Theorem A.1 (Risk bound for O2L under self-predictive estimation) .Suppose REPself;optsatisfies
Assumption A.1 with parameter γ >0. Then Algorithm 1, with inputs T, K, Φ,REPself;opt, and
ALGlathas expected risk
E[Risk obs(TK)]≤c1·Risk base(K) +c2γ·K
TEst self;opt(T, γ) +c3γ−1·KH,
for absolute constants c1, c2, c3>0.
Theorem A.1 achieves sublinear risk as long as (i) the latent algorithm achieves sublinear risk
Risk base(K)given access to the true states, and (ii) the self-predictive representation learning oracle
achieves sublinear regret Est self;opt(T, γ)for an appropriate choice of γ.13Intuitively, our result
scales with Risk base(K)instead of Risk ⋆(K)due to potential symmetries in the self-predictive
objective. For example, there might be a representation-model pair (cMlat,bϕ)that is identical
to(M⋆
lat, ϕ⋆)up to permutations of the latent state space; these cannot be distinguished by a
representation learning oracle that does not observe the latent states directly, and thus the base
algorithm may be tasked with solving either of these base MDPs. As with Theorem 4.1, this
result achieves algorithmic modularity (since O2L inherits the risk of the base algorithm), and is
computationally efficient whenever the base algorithm and self-predictive representation learning
oracle are efficient.
Let us provide some intuition behind the proof of Theorem A.1. Recall that, within the inner loop
ofO2L , the latent algorithm ALGlatinteracts with the bϕ(t)-compressed dynamics generated by
compressing the observations xh, ahthrough the current decoder bϕ(t)
h(Line 8). The crux of the
analysis is the following observation: by the self-predictive representation learning guarantee, these
dynamics, despite being possibly non-Markovian and generated from a POMDP (Definition I.1), are
well approximated in squared Hellinger distance by the base model cM(t)
latestimated by REPself;opt(cf.
Lemma I.2). We can then show that ALGlat, when given data from the bϕ(t)-compressed dynamics,
has risk (for solving cM(t)
lat) that is proportional to: i) its base risk if it were to observe states from
cM(t)
lat, and ii) the Hellinger distance between cM(t)
latand the process induced by its bϕ(t)-compressed
13For example, in our estimator of Appendix A.3, we can first set γ≈KH/ Risk base(K)so that the third
term matches Risk base(K), and then set Tso that the second term does.
18dynamics. The last ingredient is the use of latent optimism in Eq. (7), through which the risk on M⋆
lat
is upper bounded by the risk on cM(t)
lat.
In the above, showing that ALGlatobtains low risk for cM(t)
lat(despite given data from a different
process) is done by establishing a certain form of corruption robustness (Definition I.2). Indeed,
Theorem A.1 is a special case of a more general theorem (Theorem H.1), which provides a bound
that adapts to ALGlat’s level of robustness. We obtain Theorem A.1 by showing that any algorithm
satisfies the property we require (for a suitably slow rate), but we further show that tighter rates can
be achieved by analyzing the specifics of various algorithms of interest (Appendix I.1.4).
A.3 Instantiating the self-predictive estimation oracle
We now present an algorithm, SELFPREDICT .OPT(Algorithm 4 in Appendix H.1), which satisfies
Assumption A.1 under additional technical conditions, allowing us to instantiate Theorem A.1 to give
end-to-end guarantees. Before stating the main guarantee, we highlight a few technical difficulties
regarding obtaining finite-sample guarantees for (online) self-predictive estimation, and use them to
motivate our statistical assumptions and algorithm design.
The statistics of (online) self-predictive estimation. The first challenge is a realizability issue:
when ϕ̸=ϕ⋆, we may not even be able to represent the objective ϕ♯M⋆
obsas a latent model using
only decoder and latent model realizability. Since we can never guarantee that ϕ=ϕ⋆exactly in the
presence of statistical errors, we must introduce a modelling assumption which lets us capture the
pushforward models ϕ♯M⋆
obs. To this end, we introduce the mismatch functions, which are defined
as follows.
Definition A.1 (Mismatch functions) .For a decodable emission process ψ⋆and decoder ϕ∈Φ,
themismatch function forϕ,Γϕ={Γϕ,h:S → ∆(S)}H
h=1, is defined, for every h∈[H], as the
probability kernel
Γϕ,h(s′
h|sh):=Pxh∼ψ⋆
h(sh)(ϕh(xh) =s′
h).
In the context of self-prediction, we show that the following mismatch completeness assumption
suffices to capture the pushforward models ϕ♯M⋆
obs.
Assumption A.2 (Mismatch completeness) .We have a model class Lsuch that, for each ϕ∈Φ, and
Mlat∈ M lat, we have Γϕ◦Mlat∈ L, where
[Γϕ◦Mlat]h(rh, sh+1|sh, ah):=X
s′
h+1∈SMlat,h(rh, s′
h+1|sh, ah)Γϕ,h+1(sh+1|s′
h+1).
In particular, Lemma D.8 establishes that
[ϕh+1♯M⋆
obs,h](· |x, a) = [Γ ϕ◦M⋆
lat]h(· |ϕ⋆
h(x), a).
Accordingly, we view this assumption as a minimal way to realize the pushforward models ϕ♯M⋆
obs.
The second challenge is a double-sampling issue, which appears because the decoders in Eq. (7)
are coupled at different horizons. We address this with a novel “debiased” maximum likelihood
procedure that subtracts a form of excess risk (cf. Eq. (60)) to recover an unbiased estimator [Jia24].
Our debiased estimator and the mismatch completeness assumption can be viewed as analogous
to the techniques and assumptions that are required for squared Bellman error minimization in the
context of value function approximation [CJ19; JLM21].
The last issue stems from seeking an online estimation guarantee: the policies chosen by the latent
algorithm are a function of the estimated decoders, which precludes the use of randomized estimators
(e.g. exponential weights). We bypass this issue by appealing to the structural condition of coverability
[XFBJK23], which allows us to restrict our attention to estimators that achieve low offline estimation
error (via Lemma C.7).14
Definition A.2 (State Coverability) .The state coverability coefficient for an MDP Mand a policy
class Πdefined over a state space Z,Ccov,st(M,Π), is given by
Ccov,st(M,Π):= max
h∈[H]min
µ∈∆(Z)max
π∈Πmax
z∈ZdM,π
h(z)
µ(z)
. (8)
14More generally, we expect that our results can be extended to any “decoupling coefficient” [Zha22; AZ22].
19We require coverability in M⋆
obsover the set of (observation-space) policies played by the O2L
reduction (cf. Line 7). Again appealing to the mismatch functions, we can express this as an
assumption about the base dynamics M⋆
lat; we show (Lemma D.1) that the latter is equivalent to
assuming coverability in M⋆
latover the set of stochastic policies
ΓΦ◦Πlat:=(
[Γϕ◦πlat]h(a|s) =X
s′∈SΓϕ,h(s′|s)πlat,h(a|s′)|ϕ∈Φ, πlat∈Πlat)
,(9)
where Πlatdenotes the set of policies that ALGlatmay execute. While this set may appear compli-
cated, it is sufficient to assume coverability over the set of all deterministic non-stationary policies on
M⋆
lat.15
Guarantee for our self-predictive estimation oracle. With these prerequisites, the main guarantee
for our estimator, S ELFPREDICT .OPT(Algorithm 4), is as follows.
Lemma A.1 (Optimistic self-predictive estimation via SELFPREDICT .OPT).LetΠlatdenote the
set of policies played by ALGlat, and Ccov,st=Ccov,st(M⋆
lat,ΓΦ◦Πlat)be the state coverability
parameter on M⋆
latover the set of stochastic policies ΓΦ◦Πlat(Eq. (9)). Then, for any γ >0,
under decoder realizability (ϕ⋆∈Φ), base model realizability (M⋆
lat∈ M lat), and mismatch
function completeness with class Llat(Assumption A.2), the estimator in Algorithm 4 with inputs
Φ,Mlat,Llat,andγsatisfies Assumption A.1 with16
Est self;opt(T, γ) =eOq
HC cov,st|A|Tlog(|M lat||Llat||Φ|)
.
Instantiating Theorem A.1 with the above representation learning oracle, we obtain the following
algorithmic modularity result.
Corollary A.1 (Algorithmic modularity via SELFPREDICT .OPT).Under the same conditions as in
Lemma A.1, and for any base algorithm ALGlat,O2L with inputs T, K, Φ,SELFPREDICT .OPT,and
ALGlatachieves
E[Risk obs(TK)]≲c1·Risk base(K)+c2γ·K√
Tq
HC cov,st|A|log(|M lat||Llat||Φ|)+c3γ−1·KH,
for absolute constants c1, c2, c3. Consequently, for any ALGlatwith base risk Risk base(K), setting
γandTappropriately gives
E[Risk obs(TK)]≲Risk base(K),
with a number of trajectories TK=eO(K5H3Ccov,st|A|log2(|M lat||L lat||Φ|)/(Risk base(K))4).
For example, if ALGlatis a base algorithm with Risk base(K) = O(K−1/2), setting γ
andTappropriately gives an expected risk of εwith a number of trajectories TK =
eO 
H3Ccov,st|A|(log(|M lat||L lat||Φ|))2/ε14
. This result shows that statistical modularity can be achieved
up to log(|Llat|)factors for every base MDP class Mlatwhich is subsumed by coverability, in-
cluding tabular MDPs and low-rank MDPs.17Compared to our positive result for the case of
pushforward coverability (Section 3.3), this imposes less dynamics assumptions (since coverability
is implied by pushforward coverability) but requires more representational assumptions (namely,
access to the mismatch-complete class Llat). We further remark that the mismatch completeness
assumption always holds for i) the Block MDP setting, since we can always construct Llatsuch that
log(|Llat|) =O(HS2), and ii) every MDP class Mlatwhenever we also have a realizable set of
emission processes ( ψ⋆∈Ψ), since we can construct Llatsuch that log(|Llat|) = log( |Φ||M lat||Ψ|).
However, the mismatch completeness assumption may be more general than either of these settings.
15This follows from Lemma D.3 by noting that each maximum on the right hand side of Eq. (13) is attained
by a deterministic non-stationary policy.
16In this section, the notations eOand≲ignores constants and logarithmic factors of: H, C cov,st,|A|, T,and
log(|M lat||L lat||Φ|).
17This provides a partial answer to the “Model Class + Coverability” open question of Figure 1.
20Our results can be viewed as providing a theoretical justification for self-predictive representation
learning, which has been widely used in empirical works [GKBNB19; SAGHCB20]. We consider
self-prediction’s ability to obtain universal observable-to-latent reductions as a strong indicator that it
merits further theoretical study. In particular, many empirical works propose heuristics to alleviate the
degeneracy/non-uniqueness issues inherent with self-prediction [GKBNB19; SAGHCB20; HPBL23;
Tan+23]. Our methods provide a principled way to address these, and it would be interesting to
investigate whether this is also empirically effective. In general, however, it is unclear whether our
loss admits a computationally efficient implementation, due to the presence of optimism. Towards
this, a fascinating direction for future work is understanding how self-predictive estimation can be
used to obtain algorithmic modularity without the addition of optimism over the base (latent) models.
21B Additional Discussion of Related Work
In this section, we discuss aspects of related work not already covered in greater detail.
Reinforcement learning under latent dynamics (or, with rich observations). Reinforcement
learning under latent dynamics (or, with rich observations) has received extensive investigation in
recent years, however most works have been focused on the Block MDP model in which the latent
state space is tabular/finite [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23] (see also the
the closely related framework of Low-Rank MDPs [AKKS20; MCKJA24; ZSUWAS22; UZS22;
MBFR23]). Beyond tabular spaces, Dean et al.; Dean et al.; Mhammedi et al. [DMRY20; DR21;
Mha+20] consider continuous linear dynamics, Misra et al. [MLJL21] considers factored (but discrete)
latent dynamics, Efroni et al.; Efroni et al.; Mhammedi et al. [EMKAL22; EFMKL22; MFR24]
consider the Exogenous Block MDP problem in which a tabular latent state space is augmented with
a non-controllable (“exogenous”) factor, and Song et al. [SWFK24] consider Lipshitz continuous
dynamics. To our knowledge, our work is the first to: i) explore reinforcement learning under general
latent dynamics, in particular in settings where the latent space itself admits function approximation,
and ii) take a more modular approach (cf. the taxonomy of Section 3).
On the algorithmic side, the works of Uehara et al. [UZS22] and Zhang et al. [ZSUWAS22], which
consider Low-Rank MDPs and Block MDPs respectively, can be viewed as interleaving representation
learning with “latent” reinforcement learning algorithms that assume access to a good representation,
and were an inspiration for this work. However, the algorithmic details and analyses are highly
specialized to Block/Low-Rank MDPs, and unlikely to be directly applicable to reinforcement
learning under general latent dynamics. Other works with a modular flavor include:
•Feng et al. [FWYDY20] solve tabular Block MDPs by combining a black-box latent algorithm
with an “unsupervised learning oracle” for representation learning. This approach only leads to
guarantees for tabular Block MDPs, and it is unclear whether the unsupervised learning oracle their
approach requires can be constructed in natural settings.
•Wu et al. [WYDW21] solve tabular block MDPs by combining a corruption-robust latent algorithm
with a representation learning procedure based on clustering. Again, this work is restricted to the
tabular setting, and requires a separation condition which may not be satisfied in general.
General complexity measures for reinforcement learning. Another line of research provides
general complexity measures that enable sample-efficient reinforcement learning, including Bellman
rank [JKALS17; SJKAL19; Du+21; JLM21], eluder dimension [RVR13], coverability [XFBJK23],
and the Decision-Estimation Coefficient (DEC) [FKQR21; FGH23; FGQRS23]. Bellman rank and
other complexity measures based on average Bellman error [JKALS17; SJKAL19; Du+21; JLM21]
are insufficient to characterize learnability under general latent dynamics, as there are classes Mlat
that are known to be learnable, yet do not have bounded Bellman rank or Bellman-Eluder dimension
[EMKAL22; XFBJK23]. Meanwhile, variants of Bellman rank based on squared Bellman error
or related notions of error can [XFBJK23; AFJSX24] address this problem for some settings, but
satisfying the modeling/realizability assumptions (e.g., Bellman completeness) required by these
methods in the latent-dynamics setting is non-trivial. For example, the crux of our sample complexity
bounds under latent pushforward coverability in Section 3 (Theorem 3.2) is to prove a rather involved
structural result which shows that Bellman completeness can indeed be satisfied under this assumption,
but it is unclear whether these techniques can be applied to more general latent dynamics classes. We
expect that it is possible to bound the Decision-Estimation Coefficient [FKQR21; FGH23; FGQRS23]
for the framework, but deriving efficient algorithms using this framework is non-trivial.
22C Technical Tools
Lemma C.1. For any sequence of real-valued random variables (Xt)t≤Tadapted to a filtration
(Ft)t≤T, it holds that with probability at least 1−δ,
TX
t=1Xt≤TX
t=1log 
Et−1
eXt
+ log 
δ−1
.
Lemma C.2 (Freedman’s inequality (e.g., Agarwal et al. [AHKLLS14])) .Let(Xt)t≤Tbe a real-
valued martingale difference sequence adapted to a filtration (Ft)t≤T. If|Xt| ≤Ralmost surely,
then for any η∈(0,1/R), with probability at least 1−δ,
TX
t=1Xt≤ηTX
t=1Et−1
X2
t
+log 
δ−1
η.
Lemma C.3 (Corollary of Lemma C.2) .Let(Xt)t≤Tbe a sequence of random variables adapted to
a filtration (Ft)t≤T. If0≤Xt≤Ralmost surely, then with probability at least 1−δ,
TX
t=1Xt≤3
2TX
t=1Et−1[Xt] + 4Rlog 
2δ−1
,
and
TX
t=1Et−1[Xt]≤2TX
t=1Xt+ 8Rlog 
2δ−1
.
Lemma C.4 (Lemma D.2 of Foster et al. [FHQR24]) .Let(X1,F1), . . . , (Xn,Fn)be a sequence of
measurable spaces, and let X(i)=Qi
t=1XtandF(i)=⊗i
t=1Ft. For each i, letP(i)andQ(i)be
probability kernels from (X(i−1),F(i−1))to(Xi,Fi). LetPandQbe the laws of X1, . . . , X nunder
Xi∼P(i)(· |X1:i−1)andXi∼Q(i)(· |X1:i−1), respectively. Then it holds that
D2
H(P, Q)≤7EP"nX
i=1D2
H(P(i)(· |X1:i−1), Q(i)(· |X1:i−1))#
Lemma C.5 (Lemma A.11 of Foster et al. [FKQR21]) .LetPandQbe probability measures on
(X,F). For all h:X →Rwith0≤h(X)≤Ralmost surely under PandQ, we have
EP[h(X)]≤3EQ[h(X)] + 4 RD2
H(P,Q).
Lemma C.6 (Lemma 1 of Jiang et al. [JKALS17]) .For any f:X ×A → [0,1],π:S×[H]→∆(A),
we have
Ex1[f(x1, π(x1))]−J(π) =HX
h=1Eπ[f(xh, ah)− Tπf(xh, ah)].
Lemma C.7 (Offline-to-online conversion under coverability [XFBJK23; FHQR24]) .LetMbe an
MDP over state space Z,Πbe a policy set, and Ccov=Ccov(M,Π)be the (state-action) coverability
coefficient for MandΠ(Definition D.3). Let p(t)∈∆(Π) be a sequence of distributions over Π, and
g(t)
h:Z × A → [0,1]be a sequence of functions. Then we have that
TX
t=1HX
h=1Eπ(t)∼p(t)Eπ(t)
g(t)
h(xh, ah)
≤ O
vuutHC covlog(T)TX
t=1HX
h=1t−1X
i=1Eπ(i)∼p(i)Eπ(i)
g(t)
h(xh, ah)
+HC cov
.
23D Structural Properties of Coverability and Mismatch Functions
This appendix contains structural results regarding coverability and the mismatch functions. We
firstly recall the definition of the mismatch functions.
Definition D.1 (Mismatch functions) .For decodable emission process ψ⋆, decoder ϕ∈Φand
h∈[H], we define the mismatch function forϕ,Γϕ,h:S → ∆(S), as the probability kernel
Γϕ,h(s′
h|sh):=Pxh∼ψ⋆
h(sh)(ϕh(xh) =s′
h).
We also recall the definition of state coverability.
Definition D.2 (State Coverability) .The coverability coefficient for an MDP Mand a policy class Π
defined over a state space Z,Ccov,st(M,Π), is given by
Ccov,st(M,Π):= max
h∈[H]min
µ∈∆(Z)max
π∈Πmax
z∈ZdM,π
h(z)
µ(z)
. (10)
We also define the related notion of state-action coverability.
Definition D.3 (State-Action Coverability) .The coverability coefficient for an MDP Mand a policy
class Πdefined over a state space Zand action space A,Ccov(M,Π), is given by
Ccov(M,Π):= max
h∈[H]min
µ∈∆(Z×A )max
π∈Πmax
z,a∈Z×AdM,π
h(z, a)
µ(z, a)
. (11)
In the remainder of the section, we let Πlat⊆ {S × [H]→∆(A)}denote an arbitrary set of latent
policies, and
ΓΦ◦Πlat=(
[Γϕ◦πlat]h(a|s):=X
s′∈SΓϕ,h(s′|s)πlat,h(a|s′)|ϕ∈Φ, πlat∈Πlat)
.(12)
Lemma D.1 (State coverability is invariant to rich observations) .LetM⋆
obs=⟪M⋆
lat, ψ⋆⟫. Then, we
have
Ccov,st(M⋆
obs,Πlat◦Φ) = Ccov,st(M⋆
lat,ΓΦ◦Πlat).
Furthermore, letting {µlat,h∈∆(S)}h∈[H]denote the distribution which witnesses the right-hand-
side, the left-hand-side is witnessed by the distribution
µobs,h(x) =ψ⋆
h(x|ϕ⋆
h(x))µlat,h(ϕ⋆
h(x)).
The lemma follows from the following two observations.
Lemma D.2. Let{Γϕ}ϕ∈Φdenote the mismatch functions for emission ψ⋆, and let Mobs=
⟪Mlat, ψ⋆⟫. Then, for any πlat∈Πlat,ϕ∈Φ,h∈[H],x∈ X, we have
dMobs,πlat◦ϕ
h(x) =ψ⋆
h(x|ϕ⋆
h(x))dMlat,Γϕ◦πlat
h(ϕ⋆
h(x)).
Proof of Lemma D.2. Below, we write sh=ϕ⋆(xh). We proceed by induction, simply writing
dobs,h:=dMobs,πlat◦ϕ
h anddlat,h:=dMlat,Γϕ◦πlat
h . The base case (h= 1) is obtained by noting that
dlat,1(s) =Plat,1(s| ∅)while dobs,1(x) =Pobs,1(x| ∅) =ψ⋆
1(x|s)Plat,1(s| ∅). For the general
case, via the Bellman flow equations, we have
dobs,h(xh) =X
xh−1,ah−1∈X×APobs,h(xh|xh−1, ah−1)dobs,h−1(xh−1)πlat(ah−1|ϕ(xh−1))
=ψ(xh|sh)X
xh−1,ah−1∈X×APlat,h(sh|sh−1, ah−1)dlat,h−1(sh−1)ψ(xh−1|sh−1)
×πlat(ah−1|ϕ(xh−1))
=ψ(xh|sh)X
sh−1,ah−1∈S×APlat,h(sh|sh−1, ah−1)dlat,h−1(sh−1)
×X
xh−1:ϕ⋆(xh−1)=sh−1ψ(xh−1|sh−1)πlat(ah−1|ϕ(xh−1)).
24The result is obtained by noting that
Γϕ◦πlat(ah−1|sh−1) =X
s′∈SΓϕ(s′|sh−1)πlat(ah−1|s′)
=X
s′∈SX
xh−1:ϕ⋆(xh−1)=sh−1ψ(xh−1|sh−1)I{ϕ(xh−1) =s′}πlat(ah|s′)
=X
xh−1:ϕ⋆(xh−1)=sh−1ψ(xh−1|sh−1)πlat(ah−1|ϕ(xh−1)),
where the second line follows from the definition of the mismatch functions.
Lemma D.3 (Equivalence of state coverability and cumulative state reachability) .LetMbe an MDP
defined over a state space Z. The following definition is equivalent to Definition D.2:
Ccov,st(M,Π):= max
h∈[H]X
z∈Zmax
π∈ΠdM,π
h(z). (13)
Proof of Lemma D.3. Straightforward adaptation of the proof of Lemma 3 from Xie et al.
[XFBJK23].
Proof of Lemma D.1. Using Lemma D.2 and Lemma D.3, we have
Ccov,st(Mobs,Πlat◦Φ) = max
h∈[H]X
x∈Xmax
πlat,ϕdπlat◦ϕ
obs (x)
= max
h∈[H]X
x∈Xmax
πlat,ϕψ⋆(x|ϕ⋆(x))dΓϕ◦πlat
lat (ϕ⋆(x))
= max
h∈[H]X
s∈SX
x:ϕ⋆(x)=smax
πlat,ϕψ⋆(x|s)dΓϕ◦πlat
lat (s)
= max
h∈[H]X
s∈Smax
πlat,ϕdΓϕ◦πlat
lat (s)X
x:ϕ⋆(x)=sψ⋆(x|s)
=Ccov,st(Mlat,ΓΦ◦Πlat).
Lastly, we show that state-action coverability is bounded by state coverability times the size of the
action set.
Lemma D.4 (State-action coverability bound) .For any MDP Mand policy set Π, we have
Ccov(M,Π)≤Ccov,st(M,Π)|A|.
Proof of Lemma D.4. Letµs∈∆(Z)witness Ccov,st(M,Π). Fixh∈[H], which we omit below
for cleanliness. Then, we have
min
µs,a∈∆(Z×A )max
π∈Πmax
z,a∈Z×AdM,π(z, a)
µs,a(z, a)
≤max
π∈Πmax
z,a∈Z×AdM,π(z)π(a|z)
µs(z)1/|A|
≤ |A| max
π∈Πmax
z∈ZdM,π(z)
µs(z)
=Ccov,st(M,Π)|A|.
Lemma D.5 (Pushforward coverability is invariant to rich observations) .LetCpush(M)denote the
pushforward coverability parameter for an MDP M(Definition 3.3), and M⋆
obs:=⟪M⋆
lat, ψ⋆⟫. Then,
we have
Cpush(M⋆
obs) =Cpush(M⋆
lat).
25Furthermore, letting {µlat,h∈∆(S)}h∈[H]denote the distribution which witnesses the right-hand-
side, the left-hand-side is witnessed by the distribution
µobs,h(x) =ψ⋆
h(x|ϕ⋆
h(x))µlat,h(ϕ⋆
h(x)).
This follows from an analogous equivalence of pushforward coverability and cumulative conditional
reachability .
Lemma D.6 (Equivalence of pushforward coverability and cumulative conditional reachability) .Let
Mbe an MDP defined over a state space Zwith transition kernel P. The following definition is
equivalent to pushforward coverability (Definition 3.3):
Cpush(M):= max
h∈[H]X
z′∈Zmax
z,a∈Z×APh(z′|z, a).
Proof of Lemma D.6. Fixh∈[H], whose dependence we omit below. For the first direction, letting
µdenote the pushforward coverability distribution, we have:
X
z′∈Zmax
z,a∈Z×AP(z′|z, a) =X
z′∈Zmax
z,a∈Z×AP(z′|z, a)
µ(z′)µ(z′)≤CpushX
z′∈Zµ(z′) =Cpush.
For the second direction, taking µ(z′)∝max z,aP(z′|z, a), we have
min
µ∈∆(Z)max
z,a,z′∈Z×A×ZP(z′|z, a)
µ(z′)≤ max
z,a,z′∈Z×A×ZP(z′|z, a)
max ˜z,˜aP(z′|˜z,˜a)X
˜z′max
˜z,˜aP(˜z′|˜z,˜a)
≤X
z′max
z,aP(z′|z, a).
Proof of Lemma D.5. This result follows by Lemma D.6 since,
Cpush(Mobs) =X
x′∈Xmax
x,aPobs(x′|x, a)
=X
s′∈SX
x′:ϕ⋆(x′)=s′max
x,aψ⋆(x′|s′)Plat(s′|ϕ⋆(x), a)
=X
s′∈Smax
x,aPlat(s′|ϕ⋆(x), a)X
x′:ϕ⋆(x′)=s′ψ⋆(x′|s′)
=X
s′∈Smax
s,aPlat(s′|s, a) =Cpush(Mlat).
We next show that the mismatch functions can be used to express the observation-level backups for
any function of the decoders. For any g:S →R,h∈[H], we define the function [Γϕ,h◦g] :S →R
[Γϕ,h◦g](s):=X
s′∈SΓϕ,h(s′|s)g(s′).
We further overload the Bellman operator notation and define, for any g:S →RandMlat=
(rlat, Plat),
[TMlat
hg](s, a) =rlat(s, a) +Es′∼Plat(s,a)[g(s′)].
Lemma D.7. LetMobs=⟪Mlat, ψ⋆⟫,ϕ⋆:= (ψ⋆)−1,ϕ∈Φ, and Γϕbe the mismatch function for
emission ψ⋆(Definition D.1). Then, for any flat:S × A → R,h∈[H], and (x, a)∈ X × A , we
have h
TMobs
h(flat◦ϕh+1)i
(x, a) =h
TMlat
h(Γϕ,h+1◦Vflat)i
(ϕ⋆
h(x), a).
26Proof of Lemma D.7. Letf:=flat,h∈[H], and (x, a)∈ X × A be given. Then, we have:
h
TMobs
h(f◦ϕh+1)i
(x, a)
=rlat,h(ϕ⋆
h(x), a) +Esh+1∼Plat,h(ϕ⋆
h(x),a)Exh+1∼ψ⋆
h+1(sh+1)[Vf(ϕ(xh+1))]
=rlat,h(ϕ⋆
h(x), a) +Esh+1∼Plat,h(ϕ⋆
h(x),a)
X
xh+1∈Xψ⋆(xh+1|sh+1)Vf(ϕ(xh+1))

=rlat,h(ϕ⋆
h(x), a) +Esh+1∼Plat,h(ϕ⋆
h(x),a)"X
s′∈SΓϕ(s′|sh+1)Vf(s′)#
=rlat,h(ϕ⋆
h(x), a) +Esh+1∼Plat,h(ϕ⋆
h(x),a)[Γϕ◦Vf(sh+1)]
=h
TMlat
h(Γϕ◦Vf)i
(ϕ⋆
h(x), a),
where the third line follows from the definition of the mismatch function Γϕ.
We next show that the mismatch functions can be used to realize the pushforward dynamics ϕ♯M⋆
obs,
which we recall are defined as:

ϕ♯M⋆
obs,h
(r, s′|x, a) =X
x′:ϕ(x′)=s′M⋆
obs,h(r, x′|x, a). (14)
We also recall the notation [Γϕ,h+1◦Mlat]h, defined via:
[Γϕ◦Mlat]h(rh, sh+1|sh, ah):=X
s′
h+1∈SMlat,h(rh, s′
h+1|sh, ah)Γϕ,h+1(sh+1|s′
h+1).
Lemma D.8 (Pushforward model realizability via mismatch functions) .For all ϕ∈Φ,h∈[H], we
have:
[ϕh+1♯M⋆
obs,h](· |x, a) =
[Γϕ◦M⋆
lat]h◦ϕ⋆
h
(· |x, a) (15)
Proof of Lemma D.8. Note that Γϕcan alternatively be written as:
Γϕ,h(s′
h|sh) =X
xh:ϕ(xh)=s′
hψ⋆
h(xh|sh).
We have
ϕh+1♯M⋆
obs,h(rh+1, sh+1|xh, ah)
=X
xh+1:ϕh+1(xh+1)=sh+1M⋆
obs,h(rh+1, xh+1|xh, ah)
=X
xh+1:ϕh+1(xh+1)=sh+1
X
r,s′∈R×SM⋆
lat,h(r, s′|ϕ⋆
h(xh), ah)ψ⋆
h+1(xh+1|s′)

=X
r,s′∈R×SM⋆
lat,h(r, s′|ϕ⋆
h(xh), ah)X
xh+1:ϕh+1(xh+1)=sh+1ψ⋆
h+1(xh+1|s′)
=X
r,s′∈R×SM⋆
lat,h(r, s′|ϕ⋆
h(xh), ah)Γϕ,h+1(s′|sh+1)
= [Γ ϕ◦M⋆
lat]h(r, sh+1|ϕ⋆
h(xh), ah),
as desired.
27E Proofs and Additional Results for Section 3.2: Impossibility Results
This section contains additional information and proofs related to our impossibility results regarding
statistical modularity (Section 3.2), and is organized as follows:
•Appendix E.1 contains the statement for an additional lower bound that is useful for establishing
the impossibility results of Figure 1.
• Appendix E.2 contains details for each entry of Figure 1.
•Appendix E.3 contains for proofs for our main lower bound (Theorem 3.1) and the additional lower
bound (Theorem E.1).
E.1 Additional Lower Bound
Theorem E.1 (Alternative lower bound) .For every N≥4, there exists an emission class Ψand a
decoder class Φwith|Ψ|=|Φ|=Nand a family of latent MDPs Mlatsatisfying (i) |M lat|= 1,
(ii)H= 1, (iii)|S|=|X|=N, (iv)|A|=N, and such that
1. For all ε, δ > 0, we have comp(Mlat, ε, δ) = 0 .
2. For an absolute constant c >0,comp(⟪Mlat,Φ⟫, c, c)≥Ω(N/log(N)).
Proof of Theorem E.1. See Appendix E.3.2.
E.2 Details for Figure 1
Below, we provide details on each entry in Figure 1. More precisely, for each latent class Mlat, we
will give a (brief) description of the MDP class Mlat, give our choice of latent complexity comp for
Mlat, and prove that the class is or is not statistically modular for that choice of latent complexity.
We view our choices of latent complexities as natural complexities for the respective classes.
Tabular MDPs ( ✓).
• Latent class Mlat: Tabular MDPs Mlat= (S,A, Plat, Rlat, H). [AOM17]
•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(|S|,|A|, H, ε−1,logδ−1), which is
attainable, for example, via the U CB-VIalgorithm of Azar et al. [AOM17]
•Statistical modularity ( ✓): Known Block MDP algorithms (e.g. MUSIK [MFR23], BRIEE
[ZSUWAS22]) have sample complexities of poly(|S|,|A|, H, ε−1,logδ−1,log|Φ|).
Contextual Bandits ( ✓).
•Latent class Mlat: Contextual bandits with context space S, action space A, reward function
r⋆
lat:S × A → [0,1]and a finite realizable function class satisfying r⋆∈ F lat.
•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(|A|,log|Flat|, ε−1,logδ−1), attain-
able via, e.g., the S QUARE -CBalgorithm [FR20].
•Statistical modularity ( ✓): We note that Flat◦Φ ={[f◦ϕ]|f∈ F, ϕ∈Φ}is a realizable
function class for the observation-level reward function r⋆
obs, since r⋆
obs= [r⋆
lat◦ϕ⋆]∈ F lat◦Φ.
Thus, applying the SQUARE -CBalgorithm directly on the observations x(t), a(t), r(t)will give
complexity poly(|A|log(|Flat||Φ|), ε−1,logδ−1) =poly(|A|,log|Flat|,log|Φ|, ε−1,logδ−1).
Low-rank MDP ( ✓).
•Latent class Mlat: MDPs Mlat= (S,A, H, P lat, rlat)such that there exists µ⋆
lat,h∈Rd,
θ⋆
lat,h∈Rd, and a known set of features Ξlat=n
ξlat=
ξlat,h:S × A → Rd	H
h=1o
such that
for all h∈[H]we have rlat(sh, ah) =⟨ξ⋆
lat,h(sh, ah), θ⋆
lat,h⟩as well as
Plat,h(sh+1|sh, ah) =⟨ξ⋆
lat,h(sh, ah), µ⋆
lat,h+1(sh+1)⟩ (16)
for some ξ⋆
lat∈Ξlat.
•Latent complexity comp : We take comp(Mlat, ε, δ) = poly(d,|A|, H,log|Ξlat|, ε−1,logδ−1),
which is attainable via the V OX algorithm of Mhammedi et al. [MBFR23].
28•Statistical modularity ( ✓): This is obtained by noting that the observation-level dynamics also
satisfy the low-rank property with the same dimension. Formally, letting Pobsbe the transition
kernel for ⟪Mlat, ψ⋆⟫andϕ⋆= (ψ⋆)−1, we have
Pobs,h(xh+1|xh, ah) =X
sh+1∈SPlat,h(sh+1|ϕ⋆
h(xh), ah)ψ⋆
h+1(xh+1|sh+1)
=X
sh+1∈S
ξ⋆
lat,h(ϕ⋆
h(x), a), µ⋆
lat,h+1(sh+1)
ψ⋆
h+1(xh+1|sh+1)
=*
ξ⋆
lat,h(ϕ⋆
h(x), a),X
sh+1∈Sµ⋆
lat,h+1(sh+1)ψ⋆
h+1(xh+1|sh+1)+
.
Thus, the transition kernel Pobs is a low-rank MDP with µobs,h+1(xh+1):=P
sh+1µ⋆
lat,h+1(sh+1)ψ⋆
h+1(xh+1|sh+1)and feature class
Ξlat◦Φ =n
ξlat◦ϕ={ξh◦ϕh:x, a7→ξh(ϕh(x), a)}H
h=1|ξlat∈Ξlat, ϕ∈Φo
.
Lastly, since robs= [rlat◦ϕ⋆], the reward function is also linear with the same unknown feature
class. Thus we can apply VOXdirectly on top of the observations, with the feature class Ξlat◦Φ,
which will achieve a complexity poly(d,|A|, H,log|Ξlat|,log|Φ|, ε−1,log 
δ−1
).
Known Deterministic MDP ( |M lat|= 1) (✓).
•Latent class Mlat:Mlat={Mlat= (S,A, Plat, Rlat, H)}is a set of MDPs of size 1 with both
deterministic rewards and deterministic transitions.
•Latent complexity comp : We take comp(Mlat, ε, δ) = 0 , which is attainable as Mlatis known and
we can simply deploy its optimal policy.
•Statistical modularity ( ✓): We note that, due to determinism, the latent optimal policy can be
chosen to be open-loop without loss of generality, and thus will always experience the same
trajectory (s⋆
1, a⋆
1, . . . , s⋆
H, a⋆
H). We can define the observation-level policy which commits to this
same sequence of actions, i.e. πobs,h(xh) =a⋆
hfor all xh. This will be an optimal policy for any
Mobs=⟪Mlat, ψ⟫, and can also be learned in 0samples.
Low State Occupancy ( ∀π:S → ∆(A))(✓).
•Latent class Mlat:Mlat={Mlat= (S,A, Plat, Rlat, H)}is a set of MDPs for which
we have a realizable value function class, and such that there exists a feature map ζlat=
ζlat,h:S →Rd	H
h=1such that for all π:S → ∆(A)and for all Mlat∈ M lat, we have
∀h∈[H]∃θMlat,π
h:dMlat,π
h(s) =D
ζlat,h(s), θMlat,π
hE
.
Note that the feature map does not need to be known.
•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(d,|A|, H,log|Flat|, ε−1,log 
δ−1
),
which is attainable by the BILIN-UCBalgorithm of Du et al., since i) MDPs with this property have
Bilinear rank bounded by d|A|(see Definition 4.3 and Lemma 4.6 of Du et al. [Du+21]), and ii)
one can construct the value function class Flat={QMlat,⋆|Mlat∈ M lat}, which is realizable
and has size log|Flat|= log|M lat|.
•Statistical modularity ( ✓): We firstly note that one can construct a realizable value function class
for the set ⟪Mlat,Φ⟫, via the set Fobs=
QMlat,⋆◦ϕ|Mlat∈ M lat, ϕ∈Φ	
. This is realizable
since, for any Mobs:=⟪Mlat, ψ⟫, letting ϕ⋆=ψ−1, we have QMobs,⋆=QMlat,⋆◦ϕ⋆, and that this
class has size log|M lat||Φ|. We can then show that the occupancies dMobs,πfobs, forfobs∈ F obs,
can also be expressed as d-dimensional linear function for an appropriate choice of features,
which will imply that the BILIN-UCBalgorithm run directly on Mobswill attain a complexity of
poly(d,|A|, H,logMlat,log Φ, ε−1,log 
δ−1
). To obtain this, we recall the following lemma:
Lemma D.2. Let{Γϕ}ϕ∈Φdenote the mismatch functions for emission ψ⋆, and let Mobs=
⟪Mlat, ψ⋆⟫. Then, for any πlat∈Πlat,ϕ∈Φ,h∈[H],x∈ X, we have
dMobs,πlat◦ϕ
h(x) =ψ⋆
h(x|ϕ⋆
h(x))dMlat,Γϕ◦πlat
h(ϕ⋆
h(x)).
29Thanks to the above lemma, we have
dπf◦ϕ
obs(xh) =ψ(xh|ϕ⋆(xh))dΓϕ◦πf
lat (ϕ⋆(xh))
=ψ(xh|ϕ⋆(xh))D
[ζlat,h◦ϕ⋆
h](xh), θMlat,Γϕ◦πf
hE
=D
ψ(xh|ϕ⋆(xh))[ζlat,h◦ϕ⋆
h](xh), θMlat,Γϕ◦πf
hE
and so dπf◦ϕ
obs is linear with feature mapping ψ(xh|ϕ⋆(xh))[ζlat,h◦ϕ⋆
h]and parameter θMlat,Γϕ◦πf.
Recall that the feature map need not be known, so that BILIN-UCBcan still be applied despite not
knowing ψandϕ⋆.
Model class + Pushforward Coverability ( ✓).
•Latent class Mlat:Mlat={Mlat= (S,A, Plat, Rlat, H)}is a set of MDPs that all satisfy
pushforward coverability Cpush(Mlat)≤Cpush(cf. Eq. (28) for the definition).
•Latent complexity comp : We take comp(Mlat, ε, δ) =
poly(Cpush,|A|, H,log|M lat|, ε−1,log 
δ−1
), which is attainable by the GOLF algorithm
via the results of Xie et al. [XFBJK23] (see also Lemma F.3). We obtain this by noting that i)
Ccov≤Cpush|A|, where Ccovis defined in Definition 2 of Xie et al. [XFBJK23], and ii) a realizable
model class can be used to construct a realizable value function class Fand a Bellman-complete
value function helper class Gwith sizes log|F|= log|M| andlog|G|=O(log|M|).
• Statistical modularity ( ✓): This is obtained via Theorem 3.2.
Linear CB/MDP ( ✗⋆).
•Latent class Mlat: MDPs Mlat= (S,A, Plat, Rlat, H)that are linear with respect to a known
feature map ξ⋆
lat:S × A → Rd(i.e. such that Eq. (16) holds for ξ⋆
lat).
•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(d, H, ε−1,log 
δ−1
), which is attain-
able via the LSVI-UCBalgorithm of Jin et al. [JYWJ20]. Note that this guarantee does not depend
on the number of actions.
•Statistical intractability ( ✗): The latent model used in the construction of Theorem E.1 is a set (of
size 1) of linear MDPs with d= 1. In particular, that construction was a contextual bandit so we
only have to realize a reward function, and since there is only one latent model so we can trivially
embed this with d= 1viaξ⋆
lat(s, a) =rlat(s, a), where rlatis the reward function of the MDP
used in Theorem E.1.
•Statistical modularity with additional |A|-dependence: As in the Low-rank MDP case above,
⟪Mlat, ψ⟫is low-rank with unknown feature set Φ′={ξ⋆
lat◦ϕ|ϕ∈Φ}. Thus, by the same
conclusion, a the VOXalgorithm will have complexity poly(d,|A|, H,log|Φ|), which is of the
desired form if we allow suboptimal dependence on |A|.
Model class + Coverability ( ∀πM:M∈ M ) (✗).
•Latent assumption: Mlat={Mlat= (S,A, Plat, Rlat, H)}is a set of MDPs that all satisfy
coverability with respect to the policy class ΠM={πM|M∈ M} , i.e. we have
∀Mlat∈ M lat:Ccov(Mlat) = inf
µh∈∆(S×A )sup
h∈[H]sup
π∈ΠMdMlat,π
h
µh
∞<∞
•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(Ccov, H,log|M lat|, ε−1,log 
δ−1
),
which is attainable by the GOLF algorithm via the results of Xie et al. [XFBJK23] (see also
Lemma F.3). We obtain this by noting that a realizable model class can be used to construct a
realizable value function class Fand a complete value function class Gof sizes log|F|= log|M|
andlog|G|=O(log|M|).
•Statistical intractability ( ✗): The latent models used in the construction of Theorem 3.1 are a set of
coverable MDPs – in particular, these are trivially coverable with Ccov= 1since there is a single
latent model and we can take µ=dM⋆
lat,πM⋆
lat. We remark that it is an interesting open question
whether this impossibility result continues to hold if we require coverability with respect to the
class Πof all possible latent policies.
30Known Stochastic MDP ( |M lat|= 1) (✗).
• Latent class Mlat:Mlat={Mlat= (S,A, Plat, Rlat, H)}is a set of MDPs of size 1.
•Latent complexity comp : We take comp(Mlat, ε, δ) = 0 , which is attainable as Mlatis known and
we can simply deploy its optimal policy.
•Statistical intractability ( ✗): This is precisely the setting of Theorem 3.1, which shows that at least
Ω(N/log(N))samples will be needed, where N=|Φ|.
Bellman rank ( Q-type or V-type) ( ✗)
•Latent assumption: Mlat={Mlat= (S,A, Plat, Rlat, H)}is a set of latent models such that
eachMlat∈ M lathasQ-type Bellman rank dorV-type Bellman rank d[JLM21]. Letting Fbe a
realizable value function class for Mlat, in the Q-type case, this means that the |ΠF| × |F| matrix
EQ
h(π, f) =Eπh
fh(sh, ah)−rh−max
a′fh+1(sh+1, a′)i
,
admits a rank dfactorization. In the V-type case, the matrix
EV
h(π, f) =Esh∼dπ
h,ah∼πfh
fh(sh, ah)−rh−max
a′fh+1(sh+1, a′)i
admits a rank- dmatrix factorization.
•Latent complexity comp : We take comp(Mlat, ε, δ) = poly(d, H,|A|log|F|, ε−1,log 
δ−1
)
for the V-type Bellman rank case, which is achievable by the OLIVE algorithm of Jiang et al.
[JKALS17], and comp(Mlat, ε, δ) =poly(d, H, log|F|, ε−1,log 
δ−1
)forQ-type Bellman rank,
which is achievable by the B ILIN-UCBalgorithm of Du et al. [Du+21].
•Statistical intractability ( ✗): We note that the construction in Theorem 3.1 has |M lat|= 1,
which trivially has Bellman rank equal to 1, so Theorem 3.1 precludes statistical modularity with
complexity comp .
Eluder dimension + Bellman Completeness ( ✗)
•Latent class Mlat:Mlat={Mlat= (S,A, Plat, Rlat, H)}is a set of MDPs such that there is a
function class Flatsatisfying
∀flat∈ F lat, M lat∈ M lat:TMlatflat∈ F lat.
Furthermore, each Mlat∈ M lathas Bellman-Eluder dimension bounded by d(see Definition 8 of
[JLM21]).
•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(d, H, log|F|, ε−1,log 
δ−1
), which
is attainable by the G OLF algorithm of Jin et al. [JLM21].
•Statistical intractability ( ✗): As in the Bellman rank case, the construction in Theorem 3.1 has
|M lat|= 1, so we can take Flat={QMlat,⋆|Mlat∈ M lat}which is evidently complete
forTMlat, and has Eluder dimension 1, so Theorem 3.1 precludes statistical modularity with
complexity comp .
Q⋆-irrelevant State Abstraction ( ✗)
•Latent class Mlat:Mlat= (S,A, Plat, Rlat, H)such that there is a known state abstraction
function ζlat:S → Z such that ζlat(s) =ζlat(s′)implies that QMlat,⋆(s, a) =QMlat,⋆(s′, a)for
alla∈ A.
•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(|Z|,|A|, H, ε−1,log 
δ−1
)which is
attainable by the O LIVE algorithm of Jiang et al. [JKALS17].
•Statistical intractability ( ✗): We take Mlat={Mlat}as the MDP class from the construction of
Theorem 3.1. Let Q⋆
lat:=QMlat,⋆. Note that we have Q⋆
lat(s, a)∈ {0,1}for all s, a, so we can take
a latent abstract state space Z={(0,0),(0,1),(1,0),(1,1)}and a state abstraction function ζlat
such that ζlat(s) = (i, j)ifQ⋆
lat(s,0) = iandQ⋆
lat(s,1) = j. This satisfies the property of a Q⋆-
irrelevant abstraction, since ζlat(s) =ζlat(s′) = (i, j)implies that Q⋆
lat(s,0) = Q⋆
lat(s′,0) = i
andQ⋆
lat(s,1) = Q⋆
lat(s′,1) = j. This has a constant-sized abstract space ( |Z|= 4) and|A|= 2,
so Theorem 3.1 precludes statistical modularity with complexity comp .
31Linear Mixture MDP ( ✗).
•Latent class Mlat: MDPs Mlat= (S,A, Plat, Rlat, H)such that there is a known feature map
ζlat={ζlat,h:s′, s, a7→Rd}H
h=1such that
∀h∈[H],∃θh∈Rd:Plat,h(s′|s, a) =⟨ζlat,h(s′|s, a), θh⟩
•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(d, H, ε−1,log 
δ−1
), which is attain-
able by the U CRL-VTR+algorithm of Zhou et al. [ZGS21]
•Statistical intractability ( ✗): We take Mlat={Mlat}to be the construction of Theorem 3.1. Here,
there is a single latent model, so this is trivially embeddable with ζlat,h(s′|s, a) =P⋆
lat,h(s′|
s, a)∈R1. This has dimension d= 1, so Theorem 3.1 precludes statistical modularity with
complexity comp .
Linear Q⋆/V⋆(✗).
•Latent class Mlat: MDPs Mlat= (S,A, Plat, Rlat, H)such that there are known features
maps αlat:S × A → Rdandβlat:S → Rdsuch that for all Mlat∈ M lat, there exists
unknown parameters θQ, θV∈Rdsuch that QMlat,⋆(s, a) =⟨αlat(s, a), θQ⟩andVMlat,⋆(s) =
⟨βlat(s), θV⟩.
•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(d, H, ε−1,log 
δ−1
), which is attain-
able by the B ILIN-UCBalgorithm of Du et al. [Du+21].
•Statistical intractability ( ✗): We can take Mlatto be the latent MDP class from the construction
of Theorem 3.1. Since there is a single latent model, this is trivially embeddable with dimension
1, i.e. we can take ζlat(s, a) =Q⋆
lat(s, a)andβlat(s) =V⋆
lat(s). This has dimension d= 1, so
Theorem 3.1 precludes statistical modularity with complexity comp .
Low State or State-Action Occupancy ( ∀πM:M∈ M)(✗).
•Latent class Mlat: In the Low State Occupancy model, Mlat={Mlat= (S,A, Plat, Rlat, H)}
is a set of MDPs such that there exists a feature map ζV
lat=
ζlat,h:S →Rd	H
h=1such that for
allπ∈ {πMlat|Mlat∈ M lat}and for all Mlat∈ M lat, we have
∀h∈[H]∃θMlat,π
h:dMlat,π
h(s) =D
ζV
lat,h(s), θMlat,π
hE
.
For the State-Action Occupancy model, we have that there exists a feature map ζQ
lat=
ζlat,h:S × A → Rd	H
h=1such that for all π∈ {πMlat|Mlat∈ M lat}and for all Mlat∈
Mlat, we have
∀h∈[H]∃θMlat,π
h:dMlat,π
h(s, a) =D
ζQ
lat,h(s, a), θMlat,π
hE
.
Note that the feature map does not need to be known in either case.
•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(d,|A|, H,log|Flat|, ε−1,log 
δ−1
)
for the state occupancy case and comp(Mlat, ε, δ) =poly(d, H, log|M lat|, ε−1,log 
δ−1
). Both
are attainable by the BILIN-UCBalgorithm of Du et al., since i) MDPs with this property have
Bilinear rank bounded by d|A|anddrespectively (see Definition 4.3 and Lemma 4.6 of [Du+21]),
and ii) one can construct the value function class Flat={QMlat,⋆|Mlat∈ M lat}which is
realizable and has size log|Flat|= log|M lat|.
•Intractability: We can take the construction of Theorem 3.1, which has |M lat|= 1 and thus is
trivially embeddable with dimension 1, i.e. we can take ζV
lat(s) =dMlat,πMlat(s)andζQ
lat(s, a) =
dMlat,πMlat(s, a).
Bisimulation (?)
•Latent class Mlat: MDPs Mlat= (S,A, Plat, Rlat, H)such that there is a known state abstraction
function ζlat:S → Z such that ζlat(s) =ζlat(es)implies that Rlat(s, a) =Rlat(es, a)for all
a∈ A as well asP
s′:ζlat(s′)=z′Plat(s′|s, a) =P
s′:ζlat(s′)=z′Plat(s′|es, a)for all z′.
32•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(|Z|,|A|, H, ε−1,log 
δ−1
)which is
attainable by the O LIVE algorithm of [JKALS17].
•Openness ( ?): A negative result does not follow from existing constructions, since the dynamics
from the tree-based construction of Theorem 3.1 are not bisimilar unless |Z|=|S|, which allows for
the application of tabular methods. At the same time, a positive result does not follow from existing
methods, since it is non-trivial to extend existing Block MDP methods to use the bisimulation state
abstraction in a way that only pays for |Z|.
Low State-Action Occupancy ( ∀π:S → ∆(A)) (?⋆)
•Latent class Mlat:Mlat={Mlat= (S,A, Plat, Rlat, H)}is a set of MDPs such that there
exists a feature map ζQ
lat=
ζlat,h:S × A → Rd	H
h=1such that for all π:S → ∆(A)and for
allMlat∈ M lat, we have
∀h∈[H]∃θMlat,π
h:dMlat,π
h(s, a) =D
ζQ
lat,h(s, a), θMlat,π
hE
.
Note that the feature map does not need to be known.
•We take comp(Mlat, ε, δ) = poly(d, H, log|M lat|, ε−1,log 
δ−1
), which is attainable by the
BILIN-UCBalgorithm of Du et al., since i) MDPs with this property have Bilinear rank bounded
byd(see Definition 4.3 and Lemma 4.6 of [Du+21]), and ii) one can construct a realizable value
function class of size log|F|= log|M|.
•Openness ( ?): A negative result does not follow from existing constructions, since the dynamics
from the tree-based construction of Theorem 3.1 do not have linear occupancies for all π:S →
∆(A)unless d=|S|, which allows for the application of tabular methods, and the dynamics from
the bandit-based construction Theorem E.1 do not have linear occupancies for all π:S → ∆(A)
unless d=|A|. At the same time, unlike the low state occupancy case, a positive result does not
follow as it is unclear if we can express the observation-space occupancies linearly.
•Statistical tractability with additional (suboptimal) |A|-dependence ( ✓): Note that we can reduce
to the Low State Occupancy case ( ✓), since
dπ(s) =X
a∈Adπ(s, a) =*
θπ,X
a∈AζQ
lat(s, a)+
:=
θπ, ζV
lat(s)
.
However, this blows up the feature norm bound of the feature map ζV
lat(s)by a factor of |A|, which
will appear logarithmically in the bound obtained by B ILIN-UCB.
Model class + Coverability ( ∀π:S → ∆(A)) (?).
•Latent class Mlat:Mlat={Mlat= (S,A, Plat, Rlat, H)}is a set of MDPs that all satisfy
coverability with respect to all policies πlat:S → ∆(A), i.e. we have
∀Mlat∈ M lat:Ccov(Mlat) = inf
µh∈∆(S×A )sup
h∈[H]sup
π:S→∆(A)dMlat,π
h
µh
∞<∞
•Latent complexity comp : We take comp(Mlat, ε, δ) =poly(Ccov, H,log|M lat|, ε−1,log 
δ−1
),
which is attainable by the GOLFalgorithm via the results of Xie, Foster, Bai, Jiang, and Kakade (see
also Lemma F.3). We obtain this by noting that a realizable model class can be used to construct a
realizable value function class Fand a complete value function class Gof sizes log|F|= log|M|
andlog|G|=O(log|M|).
•Openness ( ?): A negative result does not follow from the existing constructions. The tree-based
construction of Theorem 3.1 satisfies coverability with Ccov= exp(Ω( H))and the bandit-based
construction of Theorem E.1 satisfies coverability with Ccov=|A|. In both cases, the lower bounds
cannot be used to rule out statistical modularity with the above latent complexity. Similarly, it
unclear how to obtain a positive result for the latent-dynamics class ⟪Mlat,Φ⟫.
33E.3 Proofs for Lower Bounds (Theorems 3.1 and E.1)
E.3.1 Main lower bound (Theorem 3.1)
We will prove the following result.
Theorem 3.1 (Impossibility of statistical modularity) .For every N≥4, there exists a decoder class
Φwith|Φ|=Nand a family of base MDPs Mlatsatisfying (i) |M lat|= 1, (ii)H≤ O(log(N)),
(iii)|S|=|X| ≤ N2, (iv)|A|= 2, and such that
1. For all ε, δ > 0, we have comp(Mlat, ε, δ) = 0 .
2. For an absolute constant c >0,comp(⟪Mlat,Φ⟫, c, c)≥Ω(N/log(N)).
Proof. LetNbe given and assume without loss of generality that it is a power of 2. We first
construct the class of latent-dynamics MDPs, following Song et al. [SWFK24].
Latent MDP. The construction has a single “known” latent MDP Mlat, so that the only uncertainty
in the family of latent-dynamics MDPs we construct arises from the emission processes. We set
Mlat={Mlat}. Set H= log2(N) + 1 andA={0,1}. We define the state space and latent
transition dynamics as follows.
• The state space can be partitioned as S=S1, . . . ,SN.
•Each block Sicorresponds to a standard depth- Hbinary tree MDP with deterministic dynamics
(e.g., Osband et al.; Domingues et al. [OVR16; DMKV21]). There is a single “root” node at layer
h= 1, which we denote by si
root, andN“leaf” nodes at layer H, which we denote by
si,j
leaf	
j∈[N].
For each h= 1, . . . , H −1, choosing action 0leads to the left successor of the current state
deterministically, and choosing action 1leads to the right sucessor; this process continues until
we reach a leaf node at layer H.
• The initial state distribution is Plat,1(∅) =Unif(s1
root, . . . , sN
root).
• There are no rewards for layers 1, . . . , H −1. For layer H, the reward is
RH(si,j
leaf,·) =I{j=i}. (17)
This construction can summarized as follows. At layer 1, we draw the index of one of Nbinary trees
uniformly at random, and initialize into the root of the tree. From here, we receive a reward of 1if
we successfully navigate to the leaf node whose index agrees with the index of the tree itself, and
receive a reward of 0otherwise.
Note that the total number of latent states in this construction is |S|=N· |S1|=N(2N−1)
Observation space and decoder class. Let us introduce some additional notation. For each
blockSi, letSi
h:={si,j
h}j∈[2h−1]denote the states in block ithat are reachable at layer h, so that
Si
1=
si
root	
andSi
H={si,j
leaf}j∈[N]. We define X=Sso that |X| ≤ 4N2, and consider a class of
emission processes corresponding to deterministic maps. Let Σdenote the set of cyclic permutations
onNelements, excluding the identity permutation. That is, each σi∈Σtakes the form
σi:k7→k+imod N fori∈ {1, . . . , N }.
For each σ∈Σ, we consider the emission process
ψσ
h(· |s(i,j)
h) =Is(σ(i),j)
h.
That is, ψσshifts the index of the binary tree containing s(i,j)
haccording to σ. LetΨ ={ψσ|σ∈Σ}.
Consider the decoder class
Φ = Ψ−1:=
si7→sψ−1(i)|ψ∈Ψ	
,
which has |Φ|=N. We consider the class of rich-observation MDPs given by
⟪Mlat,Φ⟫:=
Mi:=⟪Mlat, ψσi⟫|σi∈Σ	
. (18)
It is clear that this class of rich-observation MDPs satisfies the decodability assumption for emissions
Ψ.
34Sample complexity lower bound. To lower bound the sample complexity, we prove a lower bound
on the constrained PAC Decision-Estimation Coefficient (DEC) of [FGH23]. For an arbitrary MDP
M(defined over the space X) and ε∈[0,21/2], define18
decε(M,M) = inf
p,q∈∆(Π)sup
M∈M
Eπ∼p[JM(πM)−JM(π)]|Eπ∼q
D2
H 
M(π),M(π)
≤ε2	
,
where M(π)denotes the law over trajectories (x1, a1, r1), . . . , (xH, aH, rH)induced by executing
the policy πin the MDP M,JM(π)denotes the expected reward for policy πunder M, and πM
denotes the optimal policy for M. We further define
decε(M) = sup
Mdecε(M,M),
where the supremum ranges over all MDPs defined over XandA. We now appeal to the following
technical lemma.
Lemma E.1. For all ε2≥4/N, we have that decε(⟪Mlat,Φ⟫)≥1
2.
In light of Lemma E.1, it follows from Theorem 2.1 in Foster et al. [FGH23]19that any PAC RL
algorithm that uses Tepisodes of interaction for Tlog(T)≤c·Nmust have E[JM(πM)−JM(bπ)]≥
c′for a worst-case MDP in M, where c, c′>0are absolute constants. This implies that any PAC RL
which has E[JM(πM)−JM(bπ)]≤c′must have Tlog(T)≥c·Nand thus T≥c·N/log(N).
Proof of Lemma E.1. Define Mlatas the latent-space MDP that has identical dynamics to Mlatbut,
has zero reward in every state, and define M:=⟪Mlat,id⟫as the rich-observation MDP obtained
by composing Mlatwith the “identity” emission process idthat sets xh=sh. Observe that Mand
Mi, induce identical dynamics in observation space if rewards are ignored: For all policies π,
PM,π[(x1, a1), . . . , (xH, aH) =·] =PMi,π[(x1, a1), . . . , (xH, aH) =·]. (19)
It follows that for each i, for all policies π, we have
D2
H 
Mi(π),M(π)
=D2
H 
(⟪Mlat, ψi⟫)(π),(⟪Mlat,id⟫)(π)
=NX
j=1PM,π
xH=s(ψi(j),j)
leaf
·D2
H(I1,I0)
= 2NX
j=1PM,π
xH=s(ψi(j),j)
leaf
=2
NNX
j=1PM,π
xH=s(ψi(j),j)
leaf|x1=s(ψi(j))
root
, (20)
=2
NNX
j=1PM,πh
xH=s(j,ψ−1
i(j))
leaf|x1=s(j)
rooti
, (21)
since the learner receives identical feedback in the MDPs MiandMunless they reach the observation
xH=s(ψi(j),j)
leaf for some j(corresponding to latent state s(j,j)
leafinMi), in which case they receiver
reward 1inMibut reward 0inM. We now claim that for any q∈∆(Π) , there exists a set of at least
N/2indices Iq⊂[N]such that
Eπ∼q
D2
H 
Mi(π),M(π)
≤4
N(22)
18For measures PandQ, we define squared Hellinger distance by D2
H(P,Q) =R
(√
dP−√dQ)2.
19Theorem 2.1 in Foster et al. [FGH23] is stated with respect to supM∈conv(M)decε(M,M), but the actual
proof (Section 2.2) gives a stronger result that scales with supMdecε(M,M).
35for all i∈ Iq. To see this, note that by Eq. (21), we have
Ei∼Unif([N])Eπ∼q
D2
H 
Mi(π),M(π)
≤Eπ∼q
2
NNX
j=11
NNX
i=1PM,πh
xH=s(j,ψ−1
i(j))
leaf|x1=s(j)
rooti

≤Eπ∼q
2
NNX
j=11
N
=2
N,
where the second inequality uses thatPN
i=1PM,πh
xH=s(j,ψ−1
i(j))
leaf|x1=s(j)
rooti
≤1, as the events
in the sum are mutually exclusive (and the event we condition on does not depend on i). We conclude
by Markov’s inequality that Pi∼Unif([N])
Eπ∼q
D2
H 
Mi(π),M(π)
≥4/N
≤1/2, giving Iq≥
N/2.
From Eq. (26), we conclude that for all ε2≥4/N,
decε(M,M)≥inf
q∈∆(Π)inf
p∈∆(Π)sup
i∈Iqn
Eπ∼ph
JMi(πMi)−JMi(π)io
.
To lower bound this quantity, observe that for any index iand any policy π, we have
JMi(πMi)−JMi(π) =1
NNX
j=1PM(i),π
xH̸=s(ψi(j),j)
leaf|x1=s(ψi(j))
root
= 1−1
NNX
j=1PM(i),π
xH=s(ψi(j),j)
leaf|x1=s(ψi(j))
root
= 1−1
NNX
j=1PM,π
xH=s(ψi(j),j)
leaf|x1=s(ψi(j))
root
= 1−1
NNX
j=1PM,πh
xH=s(j,ψ−1
i(j))
leaf|x1=s(j)
rooti
,
where the third inequality uses Eq. (19). We conclude that for any distribution p, q∈∆(Π) ,
sup
i∈Iqn
Eπ∼ph
JMi(πMi)−JMi(π)io
≥Ei∼Unif(Iq)n
Eπ∼ph
JMi(πMi)−JMi(π)io
≥1−1
NNX
j=1Ei∼Unif(Iq)PM,πh
xH=s(j,ψ−1
i(j))
leaf|x1=s(j)
rooti
= 1−1
NNX
j=11
|Iq|X
i∈IqPM,πh
xH=s(j,ψ−1
i(j))
leaf|x1=s(j)
rooti
≥1−1
|Iq|≥1
2
as long as N≥4, where the second-to-last inequality uses that for all j, the events
xH=s(j,ψ−1
i(j))
leaf|
x1=s(j)
root	
are disjoint for all i. Since this lower bound holds uniformly for all q, p∈∆(Π) , we
conclude that
decε(⟪Mlat,Φ⟫,M)≥1
2.
E.3.2 Proof of alternative lower bound (Theorem E.1)
We will prove the following result.
36Theorem E.1 (Alternative lower bound) .For every N≥4, there exists an emission class Ψand a
decoder class Φwith|Ψ|=|Φ|=Nand a family of latent MDPs Mlatsatisfying (i) |M lat|= 1,
(ii)H= 1, (iii)|S|=|X|=N, (iv)|A|=N, and such that
1. For all ε, δ > 0, we have comp(Mlat, ε, δ) = 0 .
2. For an absolute constant c >0,comp(⟪Mlat,Φ⟫, c, c)≥Ω(N/log(N)).
Proof of Theorem E.1. We repeat more or less repeat the same proof as Theorem 3.1, but with the
appropriate modifications to translate from the contextual tree-based construction in Theorem 3.1
to the contextual bandit-based construction in the theorem statement. Let Nbe given and assume
without loss of generality that it is a power of 2.
Latent MDP. Our construction has a single “known” latent MDP Mlat; that is, the only uncertainty
in the family of rich-observation MDPs we construct arises from the emission processes. Set
Mlat={Mlat}. SetH= 1andA= [N]. We define the state space and latent transition dynamics
as follows.
• The state space can be partitioned as S=S1, . . . ,SN.
• Each block Sicorresponds to a single state siwithNactions denoted by ai,i∈[N].
• The initial state distribution is Plat,1(∅) =Unif(s1, . . . , sN).
• The reward function is
R1(si, aj) =I{j=i}. (23)
Informally, this construction can summarized as a contextual bandit (with uniform context distribu-
tion), with a reward of 1if and only if we play the action corresponding to the index of the context
drawn.
Note that the total number of latent states in this construction is |S|=Nand the number of actions
is|A|=N.
Observation space and decoder class. We define X=Sso that |X|=|S|, and consider a class of
emission processes corresponding to deterministic maps. Let Σdenote the set of cyclic permutations
onNelements, excluding the identity permutation. That is, each σi∈Σtakes the form
σi:k7→k+imod N, fori∈ {1, . . . , N }.
For each σ∈Σ, we consider the emission process
ψσ(· |si) =Isσ(i)(·)
That is, ψσshifts the context siaccording to σ. LetΨ ={ψσ|σ∈Σ}. Consider the decoder class
Φ = Ψ−1:=
si7→sψ−1(i)|ψ∈Ψ	
,
which has |Φ|=N. We consider the class of rich-observation MDPs given by
⟪Mlat,Φ⟫:=
Mi:=⟪Mlat, ψσi⟫|σi∈Σ	
. (24)
It is clear that this class of rich-observation MDPs satisfies the decodability assumption for emissions
Ψ.
Sample complexity lower bound. To lower bound the sample complexity, we prove a lower bound
on the constrained PAC Decision-Estimation Coefficient (DEC) of [FGH23]. For an arbitrary MDP
M(defined over the space X) and ε∈[0,21/2], define20
decε(M,M) = inf
p,q∈∆(Π)sup
M∈M
Eπ∼p[JM(πM)−JM(π)]|Eπ∼q
D2
H 
M(π),M(π)
≤ε2	
,
where M(π)denotes the law over observations (x1, a1, r1)induced by executing the policy πin the
MDP M,JM(π)denotes the expected reward for policy πunder M, and πMdenotes the optimal
policy for M. We further define
decε(M) = sup
Mdecε(M,M),
where the supremum ranges over all MDPs defined over XandA. We now appeal to the following
technical lemma.
20For measures PandQ, we define squared Hellinger distance by D2
H(P,Q) =R
(√
dP−√dQ)2.
37Lemma E.2. For all ε2≥4/N, we have that supMdecε(M,M)≥1
2.
In light of Lemma E.2, it follows from Theorem 2.1 in Foster et al. [FGH23]21that any PAC RL algo-
rithm that uses Tepisodes of interaction for Tlog(T)≤c·Nmust have E[JM(πM)−JM(bπ)]≥c′
for a worst-case MDP in M, where c, c′>0are absolute constants. This implies that any PAC RL
which has E[JM(πM)−JM(bπ)]≤c′must have Tlog(T)≥c·Nand thus T≥c·N/log(N).
Proof of Lemma E.2. Define Mlatas the latent-space MDP that has identical dynamics to Mlat
but, has zero reward for every state-action pair, and define M:=⟪Mlat,id⟫as the rich-observation
MDP obtained by composing Mlatwith the identity emission process that sets xh=sh. In the rest
of the proof, we use the shorthand ψi:=ψσi. Observe that MandMi, induce identical dynamics in
observation space if rewards are ignored, i.e. for all policies π:X → ∆(A),
PM,π[(x1, a1) =·] =PMi,π[(x1, a1) =·]. (25)
It follows that for each i, for all policies π, we have
D2
H 
Mi(π),M(π)
=D2
H 
(⟪Mlat, ψi⟫)(π),(⟪Mlat,id⟫)(π)
=NX
j=1PM,πh
x1=sψi(j), a1=aji
·D2
H(I1,I0)
= 2NX
j=1PM,πh
x1=sψi(j), a1=aji
=2
NNX
j=1PM,πh
a1=aj|x1=sψi(j)i
=2
NNX
j=1PM,πh
a1=aψ−1
i(j)|x1=sji
since the learner receives identical feedback in the MDPs MiandMunless they play the action
a1=ajgiven observation x1=sψi(j)(corresponding to latent state siinMi), in which case they
receiver reward 1inMibut reward 0inM. We now claim that for any q∈∆(Π) , there exists a set
of at least N/2indices Iq⊂[N]such that
Eπ∼q
D2
H 
Mi(π),M(π)
≤4
N(26)
for all i∈ Iq. To see this, note that by Eq. (21), we have
Ei∼Unif([N])Eπ∼q
D2
H 
Mi(π),M(π)
≤Eπ∼q
2
NNX
j=11
NNX
i=1PM,πh
a1=aψ−1
i(j)|x1=ji

≤Eπ∼q
2
NNX
j=11
N
=2
N.
We conclude by Markov’s inequality that Pi∼Unif([N])
Eπ∼q
D2
H 
Mi(π),M(π)
≥4/N
≤1/2,
giving Iq≥N/2.
From Eq. (26), we conclude that for all ε2≥4/N,
decε(⟪Mlat,Φ⟫,M)≥inf
q∈∆(Π)inf
p∈∆(Π)sup
i∈Iqn
Eπ∼ph
JMi(πMi)−JMi(π)io
.
21Theorem 2.1 in Foster et al. [FGH23] is stated with respect to supM∈conv(M)decε(M,M), but the actual
proof (Section 2.2) gives a stronger result that scales with supMdecε(M,M).
38To lower bound this quantity, observe that for any index iand any policy π, we have
JMi(πMi)−JMi(π) = 1−1
NNX
j=1PM(i),π[a1=a(j)|x1=s(ψi(j))]
= 1−1
NNX
j=1PM,π[a1=a(j)|x1=s(ψi(j))]
= 1−1
NNX
j=1PM,πh
a1=a(ψ−1
i(j))|x1=s(j)i
,
where the third inequality uses Eq. (25). We conclude that for any distribution p, q∈∆(Π) ,
sup
i∈Iqn
Eπ∼ph
JMi(πMi)−JMi(π)io
≥Ei∼Unif(Iq)n
Eπ∼ph
JMi(πMi)−JMi(π)io
≥1−1
NNX
j=1Ei∼Unif(Iq)PM,πh
a1=a(ψ−1
i(j))|x1=s(j)i
= 1−1
NNX
j=11
|Iq|X
i∈IqPM,πh
a1=a(ψ−1
i(j))|x1=s(j)i
≥1−1
|Iq|≥1
2
as long as N≥4. Since this lower bound holds uniformly for all q, p∈∆(Π) , we conclude that
decε(⟪Mlat,Φ⟫,M)≥1
2.
39F Proofs for Section 3.3: Positive Results
This section is dedicated to our upper bound establishing that pushforward-coverable MDPs are
statistically modular (Theorem 3.2). We provide a technical overview in Appendix F.1, and provide a
full proof in Appendix F.2.
F.1 Technical Overview: Low-dimensional embeddings for pushforward-coverable MDPs.
The idea behind our positive result is to show that under the conditions of Theorem 3.2, it is possible
to construct an (approximately) Bellman-complete value function class for the latent-dynamics MDP
M⋆
obs, at which point we can apply the GOLFalgorithm of Jin et al. [JLM21]. We achieve this via two
technical contributions. The first is the introduction of the mismatch functions Γϕ, formally defined
as follows.
Definition F.1 (Mismatch functions) .For a decodable emission process ψ⋆and decoder ϕ∈Φ,
themismatch function forϕ,Γϕ={Γϕ,h:S → ∆(S)}H
h=1, is defined, for every h∈[H], as the
probability kernel
Γϕ,h(s′
h|sh):=Pxh∼ψ⋆
h(sh)(ϕh(xh) =s′
h).
The mismatch functions allow us to express functions of the decoders as latent objects, and we revisit
them in the context of self-predictive estimation (Appendix A). For the present result, we show
(Lemma D.7) that the mismatch functions can capture the observation-level Bellman backups for
any function of the decoders. That is, for any xh, ah, letting sh= (ψ⋆)−1(xh)denote the true latent
state, we have that for any flat:S × A → Randϕ∈Φ:
[TM⋆
obs
h(flat◦ϕh+1)](xh, ah) = [TM⋆
lat
h(Γϕ,h+1◦Vflat)](sh, ah). (27)
That is, the Bellman update of flat◦ϕh+1in the latent-dynamics MDP M⋆
obscan be expressed as
a Bellman update in the base MDP M⋆
latfor a different (latent) function Γϕ,h+1◦Vflat(sh+1):=P
s′
h+1Γϕ,h+1(s′
h+1|sh+1) max a′flat(s′
h+1, a′).
However, the mismatch functions Γϕembed some knowledge of the emission process, and (with only
decoder and base model realizability) are unknown to the learner. Our second technical contribution
bypasses this by establishing a new structural property for pushforward-coverable MDPs (Lemma F.1):
there exist low-dimensional linear embeddings of their transition kernels which can approximate
Bellman backups for an arbitrary and potentially unknown set of functions, as long as the set is not
too large.
Lemma F.1 (Pushforward-coverable MDPs admit low-dimensional embeddings) .LetMbe a
known MDP with reward function r, transition kernel P, and pushforward coverability parameter
Cpush. Let µ={µh}h∈[H]denote its pushforward coverability distribution (i.e. the minimizer of
Definition 3.3) and F ⊆ (S ×[H]→[0,1])be an arbitrary class of functions. Suppose that we
sample W∈ {± 1}d×Sas a matrix of independent Rademacher random variables, and define
ψh(s, a) =rh(s, a)⊕1√
dW
Ph(· |s, a)/µ1/2
h(·)
·∈S∈Rd+1.
and
wf,h= 1⊕1√
dW
µ1/2
h(·)fh+1(·)
·∈S∈Rd+1.
Then for any εapx∈(0,1), as long as we set
d≥29Cpushlog 
16|F|Hδ−1/εapx
εapx,
we have that for all f∈ F andh∈[H], with probability at least 1−δ:
Eµh⊗Unif(A)h 
clip[0,2][⟨wf,h, ψh(s, a)⟩]− Thfh+1(s, a)2i
≤εapx,
as well as max s,a,h∥ψh(s, a)∥2
2≤Cpush(16 log( |S||A| H) + 11) andmax f,h∥wf,h∥2
2≤
16 log( |F|H) + 11 . We emphasize that the feature map ψ={ψh}H
h=1is oblivious to F, in the
sense that it can be computed directly from Mwithout any knowledge of F.
We use this property, in conjunction with latent model realizability, to construct linear features that
can approximate the right-hand-side of Eq. (27), thus yielding an (approximately) Bellman-complete
value function class for the latent-dynamics MDP M⋆
obs.
40F.2 Proofs for Latent Model Class + Pushforward Coverability (Theorem 3.2)
In this section, we establish positive results under latent MDP classes which satisfy pushforward
coverability. We assume that every model in Mlatsatisfies pushforward coverability , defined as
follows:
Definition F.2 (Pushforward coverability) .The pushforward coverability coefficient Cpushfor an
MDP Mwith transition kernel Pis defined by
Cpush(M) = max
h∈[H]inf
µ∈∆(S)sup
(s,a,s′)∈S×A×SPh−1(s′|s, a)
µ(s′). (28)
The pushforward coverability coefficient for an MDP class Mis defined by
Cpush(M) = max
M∈MCpush(M).
Note that for any MDP Mwe always have
Ccov(M,Πrns)≤Cpush(M)|A|, (29)
where Ccovis the state-action coverability coefficient (Definition D.3). Thus, an MDP with low
pushforward coverability is also an MDP with low state-action coverability for all policies (upto a
dependence on |A|).
We will show the show the following result.
Theorem 3.2 (Pushforward-coverable MDPs are statistically modular) .LetMlatbe a base MDP
class such that each Mlat∈ M lathas pushforward coverability bounded by Cpush(Mlat)≤Cpush.
Then, for any decoder class Φ, we have:
1.comp(Mlat, ε, δ)≤poly(Cpush,|A|, H,log|M lat|, ε−1,log 
δ−1
), and
2.comp(⟪Mlat,Φ⟫, ε, δ)≤poly(Cpush,|A|, H,log|M lat|,log|Φ|, ε−1,log 
δ−1
,log log |S|).
The proof comes in three parts. We will firstly show that MDP that satisfies pushforward coverabil-
ity admit low-dimensional feature maps that can approximate Bellman backups (Appendix F.2.1),
then establish that a regret bound for the GOLF algorithm [XFBJK23] under misspecification (Ap-
pendix F.2.2), and then combine these ingredients (Appendix F.2.3).
F.2.1 A structural result: Pushforward-coverable MDPs are approximately low-rank
Our central technical result for this section is Lemma F.1, which is based on a variant of the Johnson-
Lindenstrauss lemma and establishes that under pushforward coverability, we can define a linear
feature class which satisfies an approximate form of Bellman completeness. We define the clipping
operator via
clip[0,2](x):= max {min{x,2},0}.
We prove the following lemma.
Lemma F.1 (Pushforward-coverable MDPs admit low-dimensional embeddings) .LetMbe a
known MDP with reward function r, transition kernel P, and pushforward coverability parameter
Cpush. Let µ={µh}h∈[H]denote its pushforward coverability distribution (i.e. the minimizer of
Definition 3.3) and F ⊆ (S ×[H]→[0,1])be an arbitrary class of functions. Suppose that we
sample W∈ {± 1}d×Sas a matrix of independent Rademacher random variables, and define
ψh(s, a) =rh(s, a)⊕1√
dW
Ph(· |s, a)/µ1/2
h(·)
·∈S∈Rd+1.
and
wf,h= 1⊕1√
dW
µ1/2
h(·)fh+1(·)
·∈S∈Rd+1.
Then for any εapx∈(0,1), as long as we set
d≥29Cpushlog 
16|F|Hδ−1/εapx
εapx,
41we have that for all f∈ F andh∈[H], with probability at least 1−δ:
Eµh⊗Unif(A)h 
clip[0,2][⟨wf,h, ψh(s, a)⟩]− Thfh+1(s, a)2i
≤εapx,
as well as max s,a,h∥ψh(s, a)∥2
2≤Cpush(16 log( |S||A| H) + 11) andmax f,h∥wf,h∥2
2≤
16 log( |F|H) + 11 . We emphasize that the feature map ψ={ψh}H
h=1is oblivious to F, in the
sense that it can be computed directly from Mwithout any knowledge of F.
Proof of Lemma F.1. Fixh∈[H], whose dependence we omit for cleanliness. We begin by verifying
that, in expectation, ⟨wf, ψ(s, a)⟩is equal to Tf(s, a). For this, note that
⟨wf, ψ(s, a)⟩
=r(s, a) +1
ddX
i=1 X
s′∈SWi,s′P(s′|s, a)
µ1/2(s′)! X
s′′∈SWi,s′′µ1/2(s′′)f(s′′)!
=r(s, a) +X
s′∈SP(s′|s, a)f(s′) +1
ddX
i=1X
s′∈SX
s′′∈S
s′′̸=s′Wi,s′P(s′|s, a)
µ1/2(s′)Wi,s′′µ1/2(s′′)f(s′′).
Consequently, we have
|Tf(s, a)− ⟨wf, ψ(s, a)⟩|=1
ddX
i=1X
s′∈SX
s′′∈S
s′′̸=s′Wi,s′P(s′|s, a)
µ1/2(s′)Wi,s′′µ1/2(s′′)f(s′′).(30)
Note that this remaining noise term is zero-mean – we will show in the sequel that it can be made
small by picking dappropriately. We next examine the norms of the vectors ψ(s, a)andwf. Note
that we have
∥ψ(s, a)∥2
2=1
ddX
i=1 X
s′∈SWi,s′P(s′|s, a)
µ1/2(s′)!2
=X
s′∈SP2(s′|s, a)
µ(s′)+1
ddX
i=1X
s′∈SX
s′′∈S
s′′̸=s′Wi,s′Wi,s′′P(s′|s, a)
µ1/2(s′)P(s′′|s, a)
µ1/2(s′′)
≤Cpush+1
ddX
i=1X
s′∈SX
s′′∈S
s′′̸=s′Wi,s′Wi,s′′P(s′|s, a)
µ1/2(s′)P(s′′|s, a)
µ1/2(s′′), (31)
where we have used that
X
s′∈SP2(s′|s, a)
µ(s′)≤CpushX
s′∈SP(s′|s, a) =Cpush
by definition of pushforward coverability. Further note that we have
∥wf∥2
2=1
ddX
i=1 X
s′∈SWi,s′µ1/2(s′)f(s′)!2
=Es′∼µ[f(s′)] +1
ddX
i=1X
s′∈SX
s′′∈S
s′′̸=s′Wi,s′Wi,s′′µ1/2(s′)f(s′)·µ1/2(s′′)f(s′′)
≤1 +1
ddX
i=1X
s′∈SX
s′′∈S
s′′̸=s′Wi,s′Wi,s′′µ1/2(s′)f(s′)·µ1/2(s′′)f(s′′). (32)
We will now appeal to the following technical lemma to upper bound Eq. (30), Eq. (31), and Eq. (32)
by establishing that the Rademacher noise terms concentrate to their expectations. The proof of the
lemma will be given in the sequel.
42Lemma F.2. Letu, v∈Rn, and let W∈ {± 1}d×nhave independent Rademacher entries. Then
with probability at least 1−δ,
1
dX
i∈[d]X
j∈[n]X
k∈[n]
k̸=jWi,jWi,kujvk≤ ∥u∥2∥v∥2·r
32 log(2 δ−1)
d+∥u∥2
2∥v∥2
2·64 log 
2δ−1
d.(33)
Furthermore, for any set of vectors V ⊂Rn, we also have
1
dmax
v∈VX
i∈[d]X
j∈[n]X
k∈[n]
k̸=jWi,jWi,kvjvk
≤max
v∈V∥v∥2
2(16 log |V|+ 9) + max
v∈V∥v∥2
2·r
32 log(2 δ−1)
d+ max
v∈V∥v∥4
2·64 log 
2δ−1
d.
Let(s, a)∈ S ×A andf∈ F. To bound |⟨ψ(s, a), wf⟩−T f(s, a)|(cf. Eq. (30)), we apply the first
bound of Lemma F.2 with u= 
P(s′|s, a)/µ1/2(s′)
s′∈Sandv= 
µ1/2(s′)f(s′)
s′∈S, which
gives
|⟨ψ(s, a), wf⟩ − T f(s, a)| ≤r
32Cpushlog(2δ−1)
d+ 64Cpushlog 
2δ−1
d:=ε(δ−1), (34)
where we have again used that ∥u∥2
2=P
s′∈SP2(s′|s,a)
µ(s′)≤Cpushand also that ∥v∥2
2= 1 since
∥f∥∞≤1for all f∈ F . To bound Eq. (31), we apply the second bound of Lemma F.2 with
V=
Ph−1(s′|s,a)
µ1/2
h(s′)
s′∈S
s,a∈S×A
h×[H], which gives
max
s,a∈S×A ,h∈[H]∥ψh(s, a)∥2
2≤Cpush(16 log |S||A| H+ 9) + Cpushr
32 log(2 δ−1)
d+C2
push64 log 
2δ−1
d.
Lastly, to bound Eq. (32), we take V=
µ1/2
h(s′)fh(s′)
s′∈S
f∈F
h∈[H]in Lemma F.2, which
establishes that
max
f∈F,h∈[H]∥wf,h∥2
2≤9 + 16 log |F|H+r
32 log(2 δ−1)
d+64 log 
2δ−1
d.
Note that Eq. (34) establishes that the Bellman backup Tf(s, a)is well-approximated by
⟨ψ(s, a), wf⟩only at a single state-action pair (s, a). We can obtain an L∞-approximation guar-
antee by taking a union bound over SandA, which would incur a dependence on log|S|in
the final sample complexity. Here, we bypass this by instead requiring only an approximation
guarantee under the L2(µ⊗Unif(A))norm. Via (pushforward) coverability, this will ensure
thatEπh
(⟨wf, ψ(s, a)⟩ − T f(s, a))2i
is well-controlled for all policies π, which will be suffi-
cient for our downstream sample-complexity analysis of GOLF. However, directly establishing
anL2(µ⊗Unif(A))approximation guarantee is technically challenging since it would require
establishing a fourth-order (rather than second-order) equivalent of Eq. (33). The remainder of the
proof will obtain an L2(µ⊗Unif(A))approximation guarantee by instead sampling a dataset of size
nfrom µ⊗Unif(A)and taking a union bound over that dataset to ensure a uniform bound on all
state-action pairs in that dataset. Via an additional concentration bound, this will ensure that the error
is well-behaved under the L2(µ⊗Unif(A))norm.
For each h∈[H], sample a dataset D={(s(i)
h, a(i)
h)}n
i=1i.i.d. from µh⊗Unif(A). By a union
bound over n,F, and H, we have that
∀i∈[n], f∈ F, h∈[H] :
ψh(s(i)
h, a(i)
h), wf,h
− Thfh+1(s(i)
h, a(i)
h)≤ε(n|F|Hδ−1),(35)
where we recall the definition of ε(·)from Eq. (34). Now, let
Xf,h(s, a):= 
clip[0,2][⟨ψh(s, a), wf,h⟩]− Thfh+1(s, a)2.
43Note that |Xf,h(s, a)| ≤4and
Xf,h(s, a)≤(⟨ψh(s, a), wf,h⟩ − T hfh+1(s, a))2,
sinceThfh+1(s, a)∈[0,2]and the clipping operator is 1-Lipshitz. Note that
E(s,a)∼µh⊗Unif(A)[Xf,h(s, a)]:=Eµh⊗Unif(A)h 
clip[0,2][⟨ψh(s, a), wf⟩]− Thfh+1(s, a)2i
,
where this expectation is only over the sampling of the data point (s, a)(and not the Rademacher
matrix W). Let
Xi,f,h:=Xf,h(s(i)
h, a(i)
h).
By boundedness of Xf,h(s, a)and Hoeffding’s inequality, we have that with probability at least
1−δ: 1
nnX
i=1Xi,f,h−Eµ⊗Unif(A)[Xf,h(s, a)]≤4r
log(2δ−1)
n.
Taking another union bound over FandHas well as the event in Eq. (35) gives that
∀f∈ F, h∈[H] :1
nnX
i=1Xi,f,h−Eµ⊗Unif(A)[Xf,h(s, a)]≤4r
log(2|F|Hδ−1)
n,
(36)
and∀i∈[n], f∈ F, h∈[H] :Xi,f,h≤ε2(n|F|Hδ−1), (37)
recalling the definition of ε(·)from Eq. (34). Then, re-arranging Eq. (36) gives us that
Eµ⊗Unif(A)h 
clip[0,2][⟨ψh(sh, ah), wf⟩]− Thfh+1(sh, ah)2i
≤1
nnX
i=1Xi,f,h+ 4r
log(2|F|Hδ−1)
n
≤ε2(n|F|Hδ−1) + 4r
log(2|F|Hδ−1)
n, (38)
We now conclude the proof by picking nanddappropriately to ensure that the right-hand-side is
bounded by εapx, which will ensure the desired claim that
Eµ⊗Unif(A)h 
clip[0,2][⟨ψh(sh, ah), wf⟩]− Thfh+1(sh, ah)2i
≤εapx.
For convenience, we introduce absolute constants candc′whose precise values may change from
line to line. We pick n= 64 log 
2|F|Hδ−1
/ε2
apx. Plugging this into (38) gives
Eµ⊗Unif(A)h 
clip[0,2][⟨ψh(sh, ah), wf⟩]− Thfh+1(sh, ah)2i
≤ε2(n|F|Hδ−1) +c·ε(39)
Noting that n≤128|F|Hδ−1
ε2apxand plugging this into ε(Eq. (34)) gives
ε(n|F|Hδ−1)≤C1/2
pushr
64 log(16 |F|Hδ−1/εapx)
d+Cpush128 log 
16|F|Hδ−1/εapx
d.(40)
Setting
d≥29Cpushlog 
16|F|Hδ−1/εapx
εapx
ensures that
ε2(n|F|Hδ−1)≤ε(n|F|Hδ−1)≤εapx
2(41)
by Eq. (40). Combining Eq. (38) and Eq. (41), we get
Eµ⊗Unif(A)h 
clip[0,2][⟨ψh(sh, ah), wf⟩]− Thfh+1(sh, ah)2i
≤εapx, (42)
44as desired. It only remains to establish the concentration results of Lemma F.2.
Proof of Lemma F.2. We establish the first claim. Let i∈[d]be fixed, and consider the random
variable
Zi:=X
j∈[n]X
k∈[n]
k̸=jWi,jWi,kvjuk.
Note that E[Zi] = 0 by independence of Wi,jandWi,kfor every j̸=k. By Exercise 6.9 of
Boucheron et al. [BLM13], we have that
logE[exp( λZi)]≤16λ2
2(1−64∥u∥2
2∥v∥2
2λ)∥u∥2
2∥v∥2
2.
Since Ziare independent, it follows that
logE"
exp 
λdX
i=1Zi!#
≤16λ2
2(1−64∥u∥2
2∥v∥2
2λ)∥u∥2
2∥v∥2
2d.
Hence,Pd
i=1Ziis a sub-Gamma random variable with parameters ν= 16∥u∥2
2∥v∥2
2dandc=
64∥u∥2
2∥v∥2
2, and it follows from Equation (2.5) on page 29 of Boucheron et al. [BLM13] that for all
ε >0,
P dX
i=1Zi≥ ∥u∥2∥v∥2√
32dε+ 64∥u∥2
2∥v∥2
2ε!
≤e−ε.
Taking a union bound, and using that the random variable is symmetric, we obtain the desired claim.
We now establish the second claim. Let V ⊂Rnbe a subset of vectors. Let i∈[d]be fixed, and
re-consider the random variable
Zi:= max
v∈VX
j∈[n]X
k∈[n]
k̸=jWi,jWi,kvjvk.
Again appealing to Exercise 6.9 of Boucheron et al. [BLM13], we have that
logE[exp( λ(Zi−E[Zi]))]≤16λ2
2(1−64Bλ)E
max
v∈VX
j∈[n]X
k∈[n]
k̸=jWi,jWi,kv2
jv2
k

≤16λ2
2(1−64Bλ)E
max
v∈VnX
j,k=1v2
jv2
k

=16λ2
2(1−64Bλ)max
v∈V∥v∥4
2
where B:= max v∈V∥v∥4
2. Since Ziare independent, it follows that
logE"
exp 
λdX
i=1(Zi−E[Zi])!#
≤16λ2
2(1−64Bλ)max
v∈V∥v∥4
2d.
Hence,Pd
i=1Ziis a sub-Gamma random variable with parameters ν= 16 max v∈V∥v∥4
2dand
c= 64 max v∈V∥v∥4
2, and it follows from Equation (2.5) on page 29 of Boucheron et al. [BLM13]
that for all ε >0,
P 
1
ddX
i=1Zi≥E[Zi] + max
v∈V∥v∥2
4r
32ε
d+ 64 max
v∈V∥v∥4
2ε
d!
≤e−ε.
45To conclude, it remains only to show the bound E[Zi]≤max v∥v∥2
2(16 log |V|+ 9) . This follows by
a standard log-sum-exp approach. Below, we abbreviate ρj:=Wi,j. We can observe that for any
λ >0:
E[Zi] =E
max
v∈VX
j∈[n]X
k∈[n]
k̸=jρjρkvjvk

≤1
λlog
X
v∈VE
exp
λX
j∈[n]X
k∈[n]
k̸=jρjρkvjvk



≤1
λlog
X
v∈VE
exp
λ
nX
j=1ρjvj
2


 (43)
Note that X:=P
jρjvjis subGaussian with parameter ∥v∥2
2, since:
E
exp
λnX
j=1ρjvj

=nY
j=1E[exp( λρjvj)]≤nY
j=1exp 
λ2v2
j
2!
= expλ2
2∥v∥2
2
.
Then, it follows (e.g. Lemma 1.12 of Rigollet et al. [RH23]) that X2−E[X2]satisfies a sub-
exponential MGF bound with parameter 16∥v∥2
2, i.e.
E[exp 
λ(X2−E[X2])
]≤exp256
2λ2∥v∥4
2
∀|λ| ≤1
16∥v∥2
2.
We also note that
E[X2] =nX
i,j=1vivjE[εiεj] =∥v∥2
2.
Adding and subtracting E[X2]in Eq. (43) gives
≤1
λlog X
v∈VE
exp 
λ 
X2− ∥v∥2
2
+λ∥v∥2
2!
=1
λlog X
v∈VE
exp 
λ 
X2− ∥v∥2
2
exp 
λ∥v∥2
2!
≤1
λlog X
v∈Vexp 
128λ2∥v∥4
2+λ∥v∥2
2!
∀|λ| ≤1
16 max v∥v∥2
2
≤1
λlog|V|+ max
v128λ∥v∥4
2+ max
v∥v∥2
2∀|λ| ≤1
16 max v∥v∥2
2
Picking λ=1
16 max v∥v∥2
2concludes the proof.
F.2.2 G OLF with on-policy misspecification
Consider the version of GOLF [JLM21] in Algorithm 2. We have the following guarantee for the
regret of G OLF, which extends Jin et al. [JLM21] to allow for on-policy misspecification.
Lemma F.3. Suppose that QM⋆
obs,⋆∈ F andGsatisfies εapx-completeness in the sense that for
allh∈[H]andf∈ F h+1, there exists g∈ G hsuch that Eπ
g− TM⋆
obs
hf2
≤ε2
apxfor all
π∈ΠF:={πf:f∈ F} . LetCcov:=Ccov(M⋆
obs,ΠF)(Definition D.3). Then for an appropriate
choice of β, Algorithm 2 ensures that
Reg≤Hp
CcovTlog(|F||G| HT/δ ) +HTp
Ccovlog(T)εapx.
46Algorithm 2 GOLF [JLM21]
input: Function classes FandG, confidence width β >0.
initialize: F(0)← F ,D(0)
h← ∅ ∀ h∈[H].
1:forepisode t= 1,2, . . . , T do
2: Select policy π(t)←πf(t), where f(t):= arg maxf∈F(t−1)f(x1, πf,1(x1)).
3: Execute π(t)for one episode and obtain trajectory (x(t)
1, a(t)
1, r(t)
1), . . . , (x(t)
H, a(t)
H, r(t)
H).
4: Update dataset: D(t)
h← D(t−1)
h∪ 
x(t)
h, a(t)
h, x(t)
h+1	
∀h∈[H].
5: Compute confidence set:
F(t)←
f∈ F:L(t)
h(fh, fh+1)−min
gh∈GhL(t)
h(gh, fh+1)≤β∀h∈[H]
,
where L(t)
h(f, f′):=X
(x,a,r,x′)∈D(t)
h
f(x, a)−r−max
a′∈Af′(x′, a′)2
,∀f, f′∈ F.
6:end for
7:Output bπ=Unif(π(1:T)).
Proof of Lemma F.3. For each fh+1 ∈ F h+1, let apx[fh] =
arg mingh∈Ghsupπ∈ΠEπh
(gh− Thfh+1)2i
. Let
δ(t)
h(·,·):=f(t)
h(·,·)− Tff(t)
h+1(·,·) & eδ(t)
h(·,·):=f(t)
h(·,·)−apx
f(t)
h+1
(·,·),
and note that by Jensen’s inequality we have that for all π,Eπ
δ(t)
h(·,·)
≤Eπh
eδ(t)
h(·,·)i
+εapx.
We further adopt the shorthand d(t)
h(x, a):=dπ(t)
h(x, a)and˜d(t)
h(x, a):=P
i<td(t)
h(x, a). As
a consequence of realizability ( Q⋆
obs,h∈ F h) and approximate Bellman completeness, standard
concentration arguments (proved in the sequel) lead to the following result.
Lemma F.4 (Optimism and small in-sample squared Bellman errors) .With probability at least 1−δ,
by taking β=clog(TH|F||G| /δ) +Tεapx, we have that for all t∈[T],
(i)Q⋆
obs,h∈ F(t),and (ii)X
x,a˜d(t)
h(x, a)
eδ(t)
h(x, a)2
≤ O(β).
The rest of the proof proceeds similarly to the analysis of Section 3.2 in Xie et al. [XFBJK23].
Namely, by optimism (Lemma F.4) and a standard Bellman error decomposition (Lemma C.6) we
have
Reg≤TX
t=1HX
h=1Ed(t)
h
δ(t)
h(x, a)
≤TH·εapx+TX
t=1HX
h=1Ed(t)
hh
eδ(t)
h(x, a)i
.
Let us defining the burn-in time
τh(x, a) = min {t|˜d(t)
h(x, a)≥Ccovµ⋆
h(x, a)},
where µ⋆
his the coverability distribution for the set of policies ΠF(i.e., the distribution µ⋆
hthat
achieves the minimum in the coverability definition). Using the same decomposition into “burn-in
phase” and “stable phase” in Xie et al. [XFBJK23], we have:
TX
t=1HX
h=1Ed(t)
hh
eδ(t)
h(x, a)i
≤2HC cov+TX
t=1HX
h=1Ed(t)
hh
eδ(t)
h(x, a)I{t≥τh(x, a)}i
.
47Applying a change of measure argument on the second term then gives:
TX
t=1HX
h=1Ed(t)
hh
eδ(t)
h(x, a)I{t≥τh(x, a)}i
≤HvuutTX
t=1X
x,a 
I{t≥τh(x, a)}d(t)
h(x, a)2
˜d(t)
h(x, a)
| {z }
(A)
×vuutTX
t=1X
x,a˜d(t)
h(x, a)
eδ(t)
h(x, a)2
| {z }
(B)
By the same reasoning as in Xie et al. [XFBJK23], we have (A)≤ O(p
Ccovlog(T)), and by
Lemma F.4 we have (B)≤ O(√βT). Using that β= log( TH|F|/δ) +Tε2
apxgives the desired
result. It remains to establish the concentration results of Lemma F.4.
Proof of Lemma F.4. For any function f, define a random variable
Xt(h, f) = 
fh(s(t)
h, a(t)
h)−r(t)
h−fh+1(s(t)
h+1)2− 
Thfh+1(s(t)
h, a(t)
h)−r(t)
h−fh+1(s(t)
h+1)2.
LetFt,h={s(i)
1, a(i)
1, r(i)
1, . . . , s(i)
H, a(i)
H, r(i)
H}i<t. Note that
E
r(t)
h+fh+1(s(t)
h+1)|Ft,h
=Eπ(t)[Thf(sh, ah)]. (44)
and thus that
E[Xt(h, f)|Ft,h] =Eπ(t)h
(fh(sh, ah)− Thfh(sh, ah))2i
.
Next, note that
Var[Xt(h, f)|Ft,h]≤Eh
(Xt(h, f))2|Ft,hi
≤Eh 
fh(s(t)
h, a(t)
h)− Thfh(s(t)
h, a(t)
h)2 
fh(s(t)
h, a(t)
h) +Thfh(s(t)
h, a(t)
h) + 2 
r(t)
h−fh+1(s(t)
h+1)2|Ft,hi
≤16Eh 
fh(s(t)
h, a(t)
h)− Thfh(s(t)
h, a(t)
h)2|Ft,hi
= 16E[Xt(h, f)|Ft,h].
By Freedman’s inequality (Lemma C.2, Lemma C.3), we have that with probability at least 1−δ:
X
i<tXi(h, f)−X
i<tE[Xi(h, f)|Fi,h]≤ O
s
log(1/δ)X
i<tE[Xi(h, f)|Fi,h] + log(1 /δ)

Taking a union bound over [T]×[H]×F, we have that for all t, h, f , with probability at least 1−δ:
X
i<tXi(h, f)−X
i<tEπ(i)h
(fh(sh, ah)− Thfh(sh, ah))2i(45)
≤ O
s
ιX
i<tEπ(i)h
(fh(sh, ah)− Thfh(sh, ah))2i
+ι
, (46)
where ι= log( |F|HT/δ ). We now show that
X
i<tXi(h, f(t))≤β+O 
Tε2
apx+ι
=O(β), (47)
which will imply, from Eq. (46), that
X
i<tEπ(t)h
(fh(sh, ah)− Thfh(sh, ah))2i
≤ O(ι+β) =O(β),
as desired. To see Eq. (47), let
∆t=X
i<t 
apx
Thf(t)
h+1
(s(i)
h, a(i)
h)−r(i)
h−f(t)
h+1(s(i)
h+1)2− 
Thf(t)
h(s(i)
h, a(i)
h)−r(i)
h−f(t)
h+1(s(i)
h+1)2
48and then note that:X
i<tXi(h, f(t)) =X
i<t 
f(t)
h(s(i)
h, a(i)
h)−r(i)
h−f(t)
h+1(s(i)
h+1)2− 
Thf(t)
h(s(i)
h, a(i)
h)−r(i)
h−f(t)
h+1(s(i)
h+1)2
=X
i<t 
f(t)
h(s(i)
h, a(i)
h)−r(i)
h−f(i)
h+1(s(i)
h+1)2
−X
i<t 
apx
Thf(t)
h+1
(s(i)
h, a(i)
h)−r(i)
h−f(t)
h+1(s(i)
h+1)2+ ∆ t
≤X
i<t 
f(t)
h(s(i)
h, a(i)
h)−r(i)
h−f(t)
h+1(s(i)
h+1)2
−inf
gh∈GhX
i<t 
g(s(i)
h, a(i)
h)−r(i)
h−f(t)
h+1(s(i)
h+1)2+ ∆ t
≤β+ ∆ t.
where the second-to-last line follows from apx
Thf(t)
h+1
∈ G and the last line follows from the
definition of the confidence set. It remains to show that ∆t≤ O(Tε2
apx+ι), which we do via a
similar concentration argument. Namely, let
Yt(h, f) = 
apx[Thfh+1](s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2− 
Thfh(s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2,
and note that, as before,
E[Yt(h, f)|Ft,h] =Eπ(t)h
(apx[Thfh+1](sh, ah)− Thfh(sh, ah))2i
,
and
Var[Yt(h, f)|Ft,h]≤16E[Yt(h, f)|Ft,h],
by the same calculation as earlier. Thus, by Freedman’s inequality and a union bound, we have that,
with probability at least 1−δ,
X
i<tYt(h, f)−X
i<tEπ(t)h
(apx[Thfh+1](sh, ah)− Thfh(sh, ah))2i(48)
≤ O
s
ιX
i<tEπ(t)h
(apx[Thfh+1](sh, ah)− Thfh(sh, ah))2i
+ι
, (49)
where ι= log( |F|HT/δ ). Recalling the misspecification assumption, this implies that
X
i<tYt(h, f)≤ O 
tε2
apx+ι
,
for all h, f, t , with high probability. This concludes the result for (ii). For(i), this follows identically
to the proof of Lemma 40 in Jin et al. [JLM21], since this only uses the property that Q⋆∈ F.
F.2.3 Sample-efficient latent-dynamics RL under pushforward coverability
We conclude by combining the previous two results to obtain the main result for this section.
Theorem 3.2 (Pushforward-coverable MDPs are statistically modular) .LetMlatbe a base MDP
class such that each Mlat∈ M lathas pushforward coverability bounded by Cpush(Mlat)≤Cpush.
Then, for any decoder class Φ, we have:
1.comp(Mlat, ε, δ)≤poly(Cpush,|A|, H,log|M lat|, ε−1,log 
δ−1
), and
2.comp(⟪Mlat,Φ⟫, ε, δ)≤poly(Cpush,|A|, H,log|M lat|,log|Φ|, ε−1,log 
δ−1
,log log |S|).
Proof of Theorem 3.2. LetM⋆
obs:=⟪M⋆
lat, ψ⋆⟫∈⟪Mlat,Φ⟫be the unknown latent-dynamics
MDP. Define observation-level value functions
F={QMlat,⋆◦ϕ|Mlat∈ M lat, ϕ∈Φ},
49so that QM⋆
obs,⋆=QM⋆
lat,⋆◦ϕ⋆∈ F via decoder and model realizability, and log|Fh| ≤log|M lat||Φ|.
Consider any function class L ⊆ {S → [0,1]}and MDP Mlat= (rlat, Plat). For a given value
εapx>0, setting daccording to Lemma F.1 implies that there exists a d-dimensional feature map
φMlat,h(s, a)∈Rd+1such that for all ℓ∈ L andh∈[H], there exists wℓ,h∈Rd+1such that
EµMlat⊗Unif(A)
clip[0,2]
φMlat,h(s, a), wℓ,h
− TMlat
hℓh+1(s, a)2
≤εapx, (50)
where µMlatis the pushforward coverability distribution for Mlat. Moreover, the map φhis explicitly
computed as a function of Mlatby a randomized algorithm with success probability 1−δ, with no
knowledge of the class Lrequired. We consider the class
L=(
Γϕ◦QMlat,⋆(s, a):=X
s′∈SΓϕ(s′|s)QMlat,⋆(s′, a)|ϕ∈Φ, M lat∈ M lat)
, (51)
where Γϕ:S → ∆(S)is the mismatch function for decoder ϕand emission ψ⋆, defined in
Definition F.1. Note that Lhas size log|L| ≤ log|M lat||Φ|, and that we have
TM⋆
obs
h(QMlat,⋆
h◦ϕh)(x, a) =TM⋆
lat
h(Γϕ,h+1◦VMlat,⋆
h)(ϕ⋆
h(x), a)
by Lemma D.7. By Lemma D.1 we have that µM⋆
obs,h(x) =ψ⋆
h(x|ϕ⋆
h(x))µM⋆
lat,h(ϕ⋆
h(x))is the
coverability distribution for MDP M⋆
obs, and
EµM⋆
lat⊗Unif(A)[f(s, a)] =EµM⋆
obs⊗Unif(A)[f(ϕ⋆(x), a)].
Now, define
GMlat,h=n
(x, a)7→clip[0,2][⟨φMlat,h(ϕ(x), a), w⟩]|ϕ∈Φ,∥w∥2
2≤11 + 16 log( |M lat||Φ|H)o
.
Recall the definition of wf(forf:S × A → [0,1]) from Lemma F.1, and note that by the
norm bound max ℓ∈L∥wℓ∥2
2≤11 + 16 log( |M lat||Φ|H)given by Lemma F.1, we have (x, a)7→
⟨φMlat,h(ϕ(x), a), wℓ⟩ ∈ G hfor every ℓ∈ L. Next, note that by the norm bound max s,a∥ψ(s, a)∥2
2≤
Cpush(11 + 16 log( |S||A| H)), given by Lemma F.1, we have every gh∈ GMlat,hsatisfies ∥gh∥∞≤
cC1/2
pushlog(|M lat||Φ||S||A| H):=Bfor some absolute constant c. Therefore, GMlat,hhas size
log|GMlat,h| ≤eO(d·log(B) + log |Φ|) =eO(dlog log( |S|) + log |Φ|), where the eOnotation ignores
logarithmic factors of Cpush,|A|,log|M lat|, and log|Φ|.22Define Gh=∪Mlat∈M latGMlat,h, which
has size log|Gh| ≤log|Mlat|+(eO(dlog log( |S|) + log |Φ|)). Together, these results with Lemma F.1
imply that for all fh+1∈ Fh+1, there exists gh∈ Ghsuch that
EµM⋆
obs,h⊗Unif(A)
gh(xh, ah)−h
TM⋆
obs
hfh+1i
(xh, ah)2
≤εapx.
This, in turn, implies that for all πobs∈Πrnswe have
Eπobs
gh(xh, ah)−h
TM⋆
obs
hfh+1i
(xh, ah)2
≤Cpush|A|εapx,
since µM⋆
obs,h⊗Unif(A)satisfies coverability (Definition D.3) with parameter Ccov(M⋆
obs,Πrns)≤
Cpush|A|(Eq. (29)).
Then, it follows by Lemma F.3 that if we run Algorithm 2 with the classes FandGwe will get
Reg≤Hq
Cpush|A|Tlog(|M lat||Φ|HT/δ )(dlog log( |S|) + log |Φ|) +HTq
C2
push|A|2log(T)εapx
≤Hs
C5
push|A|Tlog(|M lat||Φ|HT/δ )log 
C2
push|M lat||Φ|2Hδ−1/εapx
log log( |S|)
εapx
+HTq
C2
push|A|2log(T)εapx
22Formally, this requires a standard covering number argument; we omit the details.
50Choosing εapx=1√
Tto balance leads to
Reg≲HT3/4q
C5
push|A|log(|M lat||Φ|HT/δ ) log 
C2
push|M lat||Φ|2Hδ−1T
log log( |S|)
+HT3/4q
C2
push|A|2log(T)
≲HT3/4q
C5
push|A|2log(|M lat||Φ|HT/δ ) log 
TC2
push|M lat||Φ|2H/δ
log log( |S|),
which gives a risk bound of
Risk≲1
T1/4Hq
C5
push|A|2log(|M lat||Φ|HT/δ ) log 
TC2
push|M lat||Φ|2H/δ
log log( |S|).
Equating this to εgives a sample complexity of
T=poly(Cpush, A, H, log|M lat|,log|Φ|, ε−1,log 
δ−1
,log log( |S|)),
as desired. Note that we have not made much effort to optimize the rate; in particular, a faster rate is
likely possible by using the GOLF.DBRalgorithm of Amortila et al. [ACK24], which improves over
the G OLF algorithm under the presence of misspecification.
51G Proofs and Additional Information for Section 4.1: Hindsight RL
This appendix contains additional information and proofs related to algorithmic modularity under
hindsight observations (Section 4.1), and is organized as follows:
•Appendix G.1 contains the pseudocode and proofs related to the online representation learning
oracle E XPWEIGHTS .DR(Lemma 4.1).
•Appendix G.2 contains the proof for our risk bound of the O2L algorithm under hindsight observ-
ability (Theorem 4.1).
G.1 Pseudocode and Proofs for EXPWEIGHTS .DR(Lemma 4.1)
Algorithm 3 Derandomized Exponential Weights (E XPWEIGHTS .DR)
input : Decoder set Φ
fort= 1,2,···, Tdo
Get dataset
x(i)
h, ϕ⋆(x(i)
h)	
i∈[t−1],h∈[H]
forh= 1, . . . , H do
Forϕ∈Φ, compute
q(t)
h(ϕh)∝exp 
−t−1X
i=1I
ϕh(x(i)
h)̸=ϕ⋆
h(x(i)
h)!
,
and set
¯ϕ(t)
h(x) = arg max
s∈SPϕh∼q(t)
h(ϕh(x) =s). (52)
end for
Return ¯ϕ(t)={¯ϕ(t)
h}H
h=1.
end for
The main result for this estimator is the following.
Lemma 4.1 (Online classification via EXPWEIGHTS .DR).Under decoder realizability (ϕ⋆∈Φ),
EXPWEIGHTS .DR(Algorithm 3) satisfies Assumption 4.2 with23
Est class(T) =eO(Hlog|Φ|).
Proof of Lemma 4.1. For each h∈[H], consider the realizable online classification problem where
x(t)
h∼dπ(t)
h, forπ(t)chosen adversarially, and y(t)
h=ϕ⋆
h(x(t)
h). Consider the exponential weights
estimator
q(t)
h(ϕ)∝exp 
−t−1X
i=1I
ϕ(x(i)
h)̸=ϕ⋆
h(x(i)
h)!
.
For every sequence (x(t)
h)T
t=1, these distributions satisfy the deterministic regret bound
TX
t=1Ebϕ(t)
h∼q(t)
hh
Ih
bϕ(t)
h(x(t)
h)̸=ϕ⋆
h(x(t)
h)ii
≤2 log|Φ|,
by Corollary 2.3 of Cesa-Bianchi et al. [CBL06]. Taking conditional expectations over x(t)
h∼dπ(t)
h
and using Lemma C.3 gives that with probability at least 1−δ:
TX
t=1Ebϕ(t)
h∼q(t)
hEπ(t)h
Ih
bϕ(t)
h(xh)̸=ϕ⋆
h(xh)ii
≤4 log|Φ|+ 8 log 
2δ−1
.
Taking a union bound over h∈[H]and summing over h∈[H]we obtain that with probability at
least1−δ:
TX
t=1HX
h=1Ebϕ(t)
h∼q(t)
hEπ(t)h
Ih
bϕ(t)
h(xh)̸=ϕ⋆
h(xh)ii
≤4Hlog|Φ|+ 8Hlog 
2Hδ−1
.
23In this section, the notations eO,≈,and≲ignore only constants and logarithmic factors of H.
52Now, recall that at each time t, we define the improper decoder ¯ϕ(t)
hvia:
¯ϕ(t)
h(x) = arg max
s∈SPϕ(t)
h∼q(t)
h(ϕ(t)
h(x) =s) (53)
Letℓh(xh, q(t)
h) =Pϕ(t)
h∼q(t)
h(ϕ(t)
h(xh)̸=ϕ⋆
h(xh)). Note that ℓsatisfies
TX
t=1HX
h=1Eϕ(t)
h∼q(t)
hEπ(t)
I
ϕ(t)
h(xh)̸=ϕ⋆
h(xh)
=TX
t=1HX
h=1Eπ(t)Eϕ(t)
h∼q(t)
h
I
ϕ(t)
h(xh)̸=ϕ⋆
h(xh)
(54)
=TX
t=1HX
h=1Eπ(t)[ℓh(xh, q(t)
h)]. (55)
By abuse of notation we also denote ℓh(xh,¯ϕh) =I¯ϕh(x)̸=ϕ⋆(x)
. We will show that
∀x, t, h :ℓh(xh,¯ϕ(t)
h)≤2ℓh(xh, q(t)
h), (56)
from which we will obtain that with probability at least 1−δ:
Regclass(T) =TX
t=1HX
h=1Eπ(t)
I¯ϕ(t)
h(xh)̸=ϕ⋆
h(xh)
≤8Hlog|Φ|+ 16Hlog 
2Hδ−1
.
Integrating the high-probability regret bound gives
E[Regclass(T)] =O(Hlog(H|Φ|)),
as desired. Towards establishing Eq. (56), let us fix xand let smaxdenote the argmax in Eq. (53).
There are two cases:
•Pϕ(t)
h∼q(t)
h(ϕ(t)
h(x) =smax)≥1
2:
→Ifsmax=ϕ⋆(x),ℓ(x,¯ϕ(t)
h) = 0 so we are done.
→Otherwise, smax̸=ϕ⋆(x)and we have ℓ(x,¯ϕ(t)
h) = 1 . However, since ϕ⋆(x)̸=smaxwe have
ϕ(t)
h(x) =smax=⇒ϕ(t)
h(x)̸=ϕ⋆
h(x)and so
Pϕ(t)
h∼q(t)
h(ϕ(t)
h(x)̸=ϕ⋆
h(x))≥Pϕ(t)
h∼q(t)
h(ϕ(t)
h(x) =smax)≥1
2=1
2ℓ(x,¯ϕ(t)
h).
•Pϕ(t)
h∼q(t)
h(ϕ(t)
h(x) =smax)<1
2:
→Ifsmax=ϕ⋆
h(x),ℓ(x,¯ϕ(t)
h) = 0 so we are done.
→Otherwise, smax̸=ϕ⋆(x)and we have ℓ(x,¯ϕ(t)
h) = 1 . However, by definition of smaxas the
mode we also have
Pϕ(t)
h∼q(t)
h(ϕ(t)
h(x) =ϕ⋆
h(x))≤Pϕ(t)
h∼q(t)
h(ϕ(t)
h(x) =smax)<1
2,
so in particular we have
ℓ(x, q(t)
h) =Pϕ(t)
h∼q(t)
h(ϕ(t)
h(x)̸=ϕ⋆
h(x))>1
2=1
2ℓ(x,¯ϕ(t)
h).
G.2 Proofs for O2L Under Hindsight Observability (Theorem 4.1)
Theorem 4.1 (Risk bound for O2L under hindsight observability) .LetALGlatbe a base algorithm
with base risk Risk ⋆(K), and REPclass a representation learning oracle satisfying Assumption 4.2.
Then Algorithm 1, with inputs T, K, Φ,REPclass, and ALGlat, has expected risk
E[Risk obs(TK)]≤Risk ⋆(K) +2K
TEst class(T).
53Proof of Theorem 4.1. Let(bϕ(t))t∈[T]denote the decoders chosen by REPclass, and let ρ(t)denote
the distribution over decoders induced at time tfrom the interaction of REPclass,ALGlat,andM⋆
obs.
Letπ(t,k)
obs:=π(t,k)
lat◦bϕ(t)andp(t,k)
obsdenote the distribution over (observation-space) policies played
at epoch tand episode k, induced by the interaction of REPclass,ALGlat, and M⋆
obs. We adopt
the notation π(t,K+1)
lat:=bπ(t)
lat∼p(t,K+1)
lat for the final policy output by ALGlatin epoch tand
(x(t,K+1)
h, a(t,K+1)
h, r(t,K+1)
h)for the trajectory collected from that (observation-level) policy bπ(t)
lat◦bϕ(t).
We firstly note that by assumption, we have the guarantee
E"TX
t=1K+1X
k=1HX
h=1Eπ(t,k)
obs∼p(t,k)
obsEπ(t,k)
obsh
Ih
bϕ(t)
h(xh)̸=ϕ⋆
h(xh)ii#
≤(K+ 1) Est class(T)
≤2KEst class(T). (57)
which follows by applying Assumption 4.2 to the distributions ¯p(t)
obs=1
(K+1)PK+1
k=1p(t,k)
obsand noting
that
TX
t=1HX
h=11
K+ 1K+1X
k=1Eπ(t,k)
obs∼p(t,k)
obsEπ(t,k)
obsh
Ih
bϕ(t)
h(xh)̸=ϕ⋆
h(xh)ii
=TX
t=1HX
h=1E¯π(t)
obs∼¯p(t)
obsE¯π(t)
obsh
Ih
bϕ(t)
h(xh)̸=ϕ⋆
h(xh)ii
≤Est class(T).
LetRisk(K,ALGlat, ϕ, M⋆
obs) =JM⋆
obs(π⋆
M⋆
obs)−JM⋆
obs(bπlat◦ϕ)be the random variable denot-
ing the risk of the final policy output by ALGlatafter Krounds of interaction with M⋆
obswhen
given feature ϕin any epoch t. For any ϕ:X → S , letEϕdenote the law over trajectories
(x(k)
h, a(k)
h, r(k)
h)k∈[K+1],h∈[H]and policies (π(k)
lat◦ϕ)k∈[K+1]generated after Krounds of interaction
when ALGlatis given feature ϕin any epoch. (Recall that, for all of the above definitions, a new
instance of ALGlatis initialized at every epoch, so we do not have to specify which epoch it is , only
the current feature ϕ). Finally, let Gtbe the “good” event
Gt=n
∀k∈[K+ 1],∀h∈[H] :bϕ(t)
h(x(t,k)
h) =ϕ⋆
h(x(t,k)
h)o
.
Recall that, in any round t,ALGlatonly observes the latent (“compressed”) trajectories
(bϕ(t)
h(x(t,k)
h), a(t,k)
h, r(t,k)
h)as history for choosing policies. We can therefore conclude that, when
bϕ(t)(x(t,k)
h) =ϕ⋆(x(t,k)
h)for all k∈[K+ 1], h∈[H], the distribution over final policies bπ(t)
latchosen
by A LGlatwill be identical as if we had chosen ϕ⋆as our decoder. In particular, this implies
Ebϕ(t)h
I{Gt}Risk(K,ALGlat,bϕ(t), M⋆
obs)i
=Eϕ⋆[I{Gt}Risk(K,ALGlat, ϕ⋆, M⋆
obs)]
≤Risk ⋆(K), (58)
where the second line simply follows by removing the indicator function, recalling that Risk ⋆(K) =
E[Risk(K,ALGlat, M⋆
lat)], and using that Risk(K,ALGlat, ϕ⋆, M⋆
obs) =Risk(K,ALGlat, M⋆
lat).
Then, we have:
E[Risk obs(TK)] =1
TTX
t=1Ebϕ(t)∼ρ(t)h
Ebϕ(t)h
Risk(K,ALGlat,bϕ(t), M⋆
obs)ii
≤1
TTX
t=1Ebϕ(t)∼ρ(t)h
Ebϕ(t)h
I{Gt}Risk(K,ALGlat,bϕ(t), M⋆
obs)ii
+1
TTX
t=1Ebϕ(t)∼ρ(t)h
Ebϕ(t)[I{¬Gt}]i
≤1
TTX
t=1Risk ⋆(K) +1
TTX
t=1P(¬Gt)
=Risk ⋆(K) +1
TTX
t=1P(¬Gt),
54where the first equality applies the tower rule for conditional expectation, the second equality applies
linearity of conditional expectations and the upper bound Risk(K,ALGlat,bϕ(t), M⋆
obs)≤1, and the
third lines applies the upper bound Eq. (58). It remains to bound the last term. Here, note that by a
union bound,
P(¬Gt)≤E"K+1X
k=1HX
h=1Eπ(t,k)∼p(t,k)Eπ(t,k)In
bϕ(t)(x(t,k)
h)̸=ϕ⋆(x(t,k)
h)o#
,
where we have used that trajectory kin round tis sampled from policy π(t,k), which is in turn sampled
from p(t,k). Summing over tand using the bound in Eq. (57) concludes the proof.
55H Proofs for Appendix A: Self-Predictive Estimation
This appendix contains additional information and proofs related to algorithmic modularity under
self-predictive estimation (Appendix A), and is organized as follows:
•Appendix H.1 contains the pseudocode and proofs related to the online representation learning
oracle S ELFPREDICT .OPT(Lemma A.1).
•Appendix H.2 contains the proof for our risk bound of the O2L algorithm under self-predictive
estimation (Theorem A.1).
H.1 Pseudocode and Proofs for SELFPREDICT .OPT(Lemma A.1)
The pseudocode for our self-predictive estimation procedure is given in Algorithm 4.
Algorithm 4 Optimistic Self-Predictive Latent Model Estimation (S ELFPREDICT .OPT)
1:input : Decoder set Φ, Latent model class Mlat, Mismatch-complete class Llat, Optimism
parameter γ
2:Setβ:=1
2p
CcovHlog(T)/T
3:fort= 1,2,···, Tdo
4: Get dataset D(t)={x(i)
h, a(i)
h, r(i)
h, x(i)
h+1}i∈[t−1],h∈[H]
5: Compute
(cM(t),bϕ(t)) = arg max
(M,ϕ)∈(M lat,Φ)(
(γβ)−1JM(πM) +HX
h=1nX
i=1log 
Mh(r(i)
h, ϕh+1(x(i)
h+1)|ϕh(x(i)
h), a(i)
h)
(59)
− max
(M′,ϕ′)∈(Llat,Φ)nX
i=1log 
M′
h(r(i)
h, ϕh+1(x(i)
h+1)|ϕ′
h(x(i)
h), a(i)
h))
.
(60)
6: Return bϕ(t)=n
bϕ(t)
ho
h∈[H].
7:end for
Our main result concerning the SELFPREDICT .OPTestimator for online optimistic self-predictive
estimation is the following. We recall our notation for the instantaneous self-prediction error
[∆h(Mlat, ϕ)](xh, ah):=D2
H 
Mlat,h(ϕh(xh), ah),
ϕh+1♯M⋆
obs,h
(xh, ah)
.
Lemma A.1 (Optimistic self-predictive estimation via SELFPREDICT .OPT).LetΠlatdenote the
set of policies played by ALGlat, and Ccov,st=Ccov,st(M⋆
lat,ΓΦ◦Πlat)be the state coverability
parameter on M⋆
latover the set of stochastic policies ΓΦ◦Πlat(Eq. (9)). Then, for any γ >0,
under decoder realizability (ϕ⋆∈Φ), base model realizability (M⋆
lat∈ M lat), and mismatch
function completeness with class Llat(Assumption A.2), the estimator in Algorithm 4 with inputs
Φ,Mlat,Llat,andγsatisfies Assumption A.1 with24
Est self;opt(T, γ) =eOq
HC cov,st|A|Tlog(|M lat||Llat||Φ|)
.
Proof of Lemma A.1. We will firstly establish that the algorithm obtains low offline estimation error.
Lemma H.1 (SELFPREDICT .OPTattains low offline estimation error) .For any γ >0, under decoder
realizability ( ϕ⋆∈Φ), model realizability ( M⋆
lat∈ M lat), and mismatch function completeness with
classLlat(Assumption A.2), the estimator in Algorithm 4 with inputs Φ,Mlat,Llat, and γsatisfies
24In this section, the notations eOand≲ignores constants and logarithmic factors of: H, C cov,st,|A|, T,and
log(|M lat||L lat||Φ|).
56that for all t∈[T], with probability at least 1−δ,
HX
h=0t−1X
i=1Eπ(i)∼p(i)Eπ(i)h
[∆h(cM(t),bϕ(t))](xh, ah)i
+γ−1
JM⋆
lat(πM⋆
lat)−JcM(t)(πcM(t))
≤ O 
log 
|M lat||Llat||Φ|HTδ−1
. (61)
Given this result, we can appeal to offline-to-online conversions to establish the final result. Let
Ccov:=Ccov(M⋆
obs,Πlat◦Φ)denote the (state-action) coverability coefficient in M⋆
obsover the
set of policies Πlat◦Φ. Note that by Lemma D.1 we have Ccov,st(M⋆
obs,Πlat◦Φ) = Ccov,stand
therefore by Lemma D.4 we have Ccov(M⋆
obs,Πlat◦Φ)≤Ccov,st|A|. Let η >0be a parameter
to be chosen later, and βoff=O 
log 
|M lat||Llat||Φ|HTδ−1
be the offline estimation error
guaranteed by Lemma H.1. We abbreviate α:=p
CcovHlog(T),Ep(t)[·]:=Eπ(t)∼p(t)Eπ(t)[·], and
Eep(t):=Pt−1
i=1Eπ(i)∼p(i)Eπ(i)[·]. Then, we have:
TX
t=1HX
h=1Ep(t)h
[∆h(cM(t),bϕ(t))](xh, ah)i
+γ−1
JM⋆
lat(πM⋆
lat)−JcM(t)(πcM(t))
≤αvuutTX
t=1HX
h=1Eep(t)h
[∆h(cM(t),bϕ(t))](xh, ah)i
+γ−1TX
t=1
JM⋆
lat(πM⋆
lat)−JcM(t)(πcM(t))
+O(HC cov)
≤α 
η
2TX
t=1HX
h=1Eep(t)h
[∆h(cM(t),bϕ(t))](xh, ah)i
+1
2η!
+γ−1TX
t=1
JM⋆
lat(πM⋆
lat)−JcM(t)(πcM(t))
+O(HC cov)
where in the first inequality we have used Lemma C.7 with g(t)
h= ∆ h(cM(t),bϕ(t))and in the second
inequality we have used the AM-GM inequality with parameter η. Collecting terms, we proceed via:
=αη
2TX
t=1 HX
h=1Eep(t)h
∆h(cM(t),bϕ(t))(xh, ah)i
+ (γηα
2)−1
JM⋆
lat(πM⋆
lat)−JcM(t)(πcM(t))!
+α
2η+O(HC cov)
≤αη
2Tβoff+α
2η+O(HC cov)
≤ Oq
Ccov,st|A|Hlog(T)Tβoff+HC cov,st|A|
≤ Oq
HC cov,st|A|Tlog(T) log 
|M lat||Llat||Φ|HTδ−1
,
where in the first inequality we have used Lemma H.1 and the definition of γin Algorithm 4 (cf. Eq.
(59)) and in the second inequality we have chosen η=1/√
Tto balance the terms and used the bound
Ccov≤Ccov,st|A|. We convert to an expected regret bound by picking δappropriately, which gives
the final result. It remains to show Lemma H.1.
Proof of Lemma H.1. Fix an iteration t∈[T], and abbreviate cM:=cM(t)andbϕ:=bϕ(t). We
follow the analysis of maximum likelihood estimation from Geer; Zhang; Agarwal et al. [Gee00;
Zha06; AKKS20]. In particular, we quote Lemma 24 of [AKKS20], which in an abstract conditional
estimation framework with density class Fstates the following.
Lemma H.2 (Lemma 24 of Agarwal et al. [AKKS20]) .LetD={(xi, yi)}be a dataset collected with
xi∼p(i)(x1:i−1, y1:i−1)andyi∼f⋆(· |xi),L(f, D) =Pn
i=1ℓ(f,(xi, yi))be any loss function
that decomposes additively, bf:D→ F be an estimator, D′be a tangent sequence D′={(exi,eyi)}
57sampled independently via exi∼p(i)(x1:i−1, y1:i−1)andeyi∼f⋆(· |exi). Then, with probability at
least1−δ, we have
−logED′exp
L(bf(D), D′)
≤ −L(bf(D), D) + log 
|F|δ−1
, (62)
For our purposes, we have that F=Mlat◦Φ, the data distribution is collected adaptively (for each
h∈[H]) via π(i)∼p(i),x(i)
h, a(i)
h∼dM⋆
obs,π(i)
h , and r(i)
h, x(i)
h+1∼M⋆
obs(· |x(i)
h, a(i)
h). For the loss
function L, we take
L((M, ϕ), D) =−HX
h=0tX
i=1log 
M⋆
obs(r(i)
h+1, ϕh+1(x(i)
h+1)|x(i)
h, a(i)
h)
[Mh◦ϕh](r(i)
h, ϕh+1(x(i)
h+1)|x(i)
h, a(i)
h)!
−γ−1
2(JM⋆
lat(πM⋆
lat)−JM(πM)).
We begin by upper bounding the quantity −L((cM,bϕ)(D), D)appearing on the right-hand side of
Eq. (62), or equivalently lower bounding L((cM,bϕ)(D), D). Let us abbreviate bV=JcM(πcM)and
V⋆=JM⋆
lat(πM⋆
lat). Towards this, note that
L((cM,bϕ)(D), D) =HX
h=0tX
i=1logh
cMh◦bϕhi
(r(i)
h,bϕh+1(x(i)
h+1)|x(i)
h, a(i)
h)
−HX
h=0tX
i=1log
M⋆
obs(r(i)
h,bϕh+1(x(i)
h+1)|x(i)
h, a(i)
h)
+γ−1
2(bV−V⋆)
≥HX
h=0tX
i=1logh
cMh◦bϕhi
(r(i)
h,bϕh+1(x(i)
h+1)|x(i)
h, a(i)
h)
−HX
h=0max
[M′◦ϕ′]∈L lat◦ΦtX
i=1log
[M′
h◦ϕ′
h](r(i)
h,bϕh+1(x(i)
h+1)|x(i)
h, a(i)
h)
+γ−1
2(bV−V⋆)
≥HX
h=0tX
i=1log 
M⋆
lat,h◦ϕ⋆
h
(r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)
−HX
h=0max
[M′◦ϕ′]∈L lat◦ΦtX
i=1log 
[M′
h◦ϕ′
h](r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)
+γ−1
2(V⋆−V⋆)
=HX
h=0tX
i=1log 
M⋆
lat,h◦ϕ⋆
h
(r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)
−HX
h=0max
[M′◦ϕ′]∈L lat◦ΦtX
i=1log 
[M′
h◦ϕ′
h](r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)
,
where in the second line we have used Lemma D.8 with Assumption A.2 and in the third line we
have used the ERM property of cM◦bϕtogether with decoder and model realizability. We claim that
this implies
L((cM,bϕ)(D), D)≥ −log 
|Llat◦Φ|Hδ−1
(63)
by concentration. Indeed, for each h∈[H], i∈[t],and[M′◦ϕ′]∈ L lat◦Φ, let
Z[M′◦ϕ′]
i,h=−1
2log 
M⋆
obs(r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)
[M′◦ϕ′](r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)!
58Applying Lemma C.1, we have that
tX
i=1log 
M⋆
obs(r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)
[M′◦ϕ′](r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)!
≥tX
i=1−2 log 
Eπ(i)∼p(i)Eπ(i)"
exp 
−1
2log 
M⋆
obs(r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)
[M′◦ϕ′](r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)!!#!
−log 
δ−1
, (64)
with probability at least 1−δ, where we have recalled that data is gathered adaptively according to
π(i)∼p(i).We now quote the following lemma from Zhang; Agarwal et al. [Zha06; AKKS20].
Lemma H.3 (Lemma 25 of Agarwal et al. [AKKS20]) .For any D ∈∆(X)andp, q∈[X → ∆(Y)],
we have
−2 logEx∼D,y∼q(·|x)exp
−1
2log(q(y|x)/p(y|x))
≥Ex∼D
D2
H(q(· |x), p(· |x))
Proof of Lemma H.3. We include the proof for completeness. The result follows via the following
steps.
−2 logEx∼D,y∼q(·|x)exp
−1
2log(q(y|x)/p(y|x))
=−2 logEx∼D,y∼q(·|x)p
p(y|x)/q(y|x)
≥2
1−Ex∼D,y∼q(·|x)p
p(y|x)/q(y|x)
(∀x: log( x)≤x−1)
=Ex∼Dh
2
1−Ey∼q(·|x)p
p(y|x)/q(y|x)i
=Ex∼D
D2
H(p(· |x), q(· |x))
By Lemma H.3, we have that the right-hand-side of Eq. (64) is further lower bounded by
tX
i=1log 
M⋆
obs(r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)
[M′◦ϕ′](r(i)
h, ϕ⋆
h+1(x(i)
h+1)|x(i)
h, a(i)
h)!
≥tX
i=1Eπ(i)∼p(i)Eπ(i)
D2
H 
ϕ⋆
h+1♯M⋆
obs(· |x(i)
h, a(i)
h),[M′◦ϕ′](· |x(i)
h, a(i)
h)
−log 
δ−1
≥ −log 
δ−1
,
where the last line follows from the non-negativity of squared Hellinger. Taking a union bound over
M′◦ϕ′∈ L lat◦Φandh∈[H]gives the desired lower bound in Eq. (63).
To conclude the proof, it remains to lower bound the left-hand side in Eq. (62). Here, note that:
−logED′exp
L((cM,ˆϕ)(D), D′)
+γ−1
2(V⋆−bV)
=−logED′
exp
−1
2HX
h=1tX
i=1log
M⋆
obs(er(i)
h,bϕh+1(ex(i)
h+1)|x(i)
h, a(i)
h)h
cMh◦bϕhi
(r(i)
h,bϕh+1(x(i)
h+1)|x(i)
h, a(i)
h)



=−HX
h=1tX
i=1logEπ(i)∼p(i)Eπ(i)
exp
−1
2log
M⋆
obs(r(i)
h,bϕh+1(x(i)
h+1)|x(i)
h, a(i)
h)h
cMh◦bϕhi
(r(i)
h,bϕh+1(x(i)
h+1)|x(i)
h, a(i)
h)


,
(65)
59where we have used that in the “tangent sequence” D′the current sample (er(i)
h,ex(i)
h+1)is independent
of(r(i)
h, x(i)
h+1). To bound this term, we again appeal to Lemma H.3, concluding that
−HX
h=1tX
i=1logEπ(i)∼p(i)Eπ(i)
exp
−1
2log
M⋆
obs(r(i)
h,bϕh+1(x(i)
h+1)|x(i)
h, a(i)
h)h
cMh◦bϕhi
(r(i)
h,bϕh+1(x(i)
h+1)|x(i)
h, a(i)
h)



≥1
2HX
h=1tX
i=1Eπ(i)∼p(i)Eπ(i)h
D2
H
[cMh◦bϕh](xh, ah),bϕh+1♯M⋆
obs(xh, ah)i
Combining everything, we have:
1
2 HX
h=1tX
i=1Eπ(i)∼p(i)Eπ(i)h
D2
Hh
cMh◦bϕhi
(xh, ah),bϕh+1♯M⋆
obs(xh, ah)i
+γ−1(V⋆−bV)!
≤log 
|Llat||Φ|Hδ−1
+ log 
|M lat||Φ|δ−1
Taking an additional union bound over t∈[T], we have that with probability at least 1−δ:
HX
h=1tX
i=1Eπ(i)∼p(i)
hEπ(i)h
D2
Hh
cMh◦bϕhi
(xh, ah),bϕh+1♯M⋆
obs(xh, ah)i
+γ−1
JM⋆
lat(πM⋆
lat)−JM(t)(πM(t))
≤ O 
log 
|M lat||Llat||Φ|HTδ−1
,
for all t∈[T], as desired.
Corollary A.1 (Algorithmic modularity via SELFPREDICT .OPT).Under the same conditions as in
Lemma A.1, and for any base algorithm ALGlat,O2L with inputs T, K, Φ,SELFPREDICT .OPT,and
ALGlatachieves
E[Risk obs(TK)]≲c1·Risk base(K)+c2γ·K√
Tq
HC cov,st|A|log(|M lat||Llat||Φ|)+c3γ−1·KH,
for absolute constants c1, c2, c3. Consequently, for any ALGlatwith base risk Risk base(K), setting
γandTappropriately gives
E[Risk obs(TK)]≲Risk base(K),
with a number of trajectories TK=eO(K5H3Ccov,st|A|log2(|M lat||L lat||Φ|)/(Risk base(K))4).
Proof of Corollary A.1. The first inequality simply follows by plugging the bound
ofEst self;optfrom Lemma A.1 into Theorem A.1. For the second inequality, let ∆ =
c2p
HC cov,st|A|log(|M lat||Llat||Φ|). The result follows by setting γs.t. c3γ−1HK =
Risk base(K)i.e.γ=c3KH
Risk base(K), and Tsuch thatγK∆√
T=Risk base(K)i.e.T=K4∆2γ2
Risk base(K)2=
K4∆2H2
(Risk base(K))4. Then the result follows by direct substitution and by noting thatK
T≤1since
Risk base(K)≤1.
H.2 Proofs for Main Risk Bound (Theorem A.1)
Our main risk bound (Theorem A.1) follows as a special case of a more general theorem (Theo-
rem H.1), which holds for algorithm that satisfies a property we refer to as CorruptionRobust -ness
(Definition I.2). We now state the more general theorem, postponing its proof (and a formal definition
of corruption robustness) until Appendix I.
60Theorem H.1 (Risk bound for O2L under self-predictive estimation and CorruptionRobustness ).
Assume REPself;optsatisfies Assumption A.1 with parameter γ >0and that Mlatis realizable (i.e.
M⋆
lat∈ M lat). Furthermore, let ALGlatbeCorruptionRobust (Definition I.2) with parameter α.
Then, O2L (Algorithm 1) with inputs T, K, Φ,ALGlat, and REPself;opthas expected risk
E[Risk obs(TK)]≤c1·Risk base(K) +c2γ·K
TEst self;opt(T, γ) +c3γ−1· 
α2+H
(66)
for absolute constants c1, c2, c3>0.
Our main risk bound (Theorem A.1) follows from the following lemma, which establishes that
any ALGlatisCorruptionRobust in the sense of Definition I.2 for a sufficiently large cor-
ruption robustness parameter. Below, for any POMDP fMover state-action space S × A , we
writefM(s1:h, a1:h)for the conditional probability over reward rhandsh+1given s1:h, a1:h, i.e.
fMh(s1:h, a1:h) =fMh(rh, sh+1=· |s1:h, a1:h).
Lemma H.4. LetM⋆be any reference MDP and fMbe any POMDP with the same state and action
space. Then for any algorithm ALGlat, we have
EfM,ALGlat[Risk M⋆(K)]≤c1EM⋆,ALGlat[Risk M⋆(K)]
+c2EfM,ALGlat"KX
k=1HX
h=1EfM,π(k)h
D2
H
M⋆
h(sh, ah),fMh(s1:h, a1:h)i#
,
where c1, c2>0are absolute constants. In particular, ALGlatisCorruptionRobust (Definition I.2)
withα=c2√
KH.
Proof of Lemma H.4. Let us abbreviate ALG:=ALGlat. Fori∈[K], letτ(i)denote the trajectory
(s(i)
1, a(i)
1, r(i)
1, . . . , s(i)
H, a(i)
H, r(i)
H). LetP:=PM⋆,ALGdenote the law of {(π(i), τ(i))}i∈[K]under ALG
in the true MDP M⋆, andQ:=PfM,ALGdenote the law of {(π(i), τ(i))}i∈[K]under ALGunder the
POMDP fM. Let us write M⋆(π)andfM(π)for the laws of trajectory τsampled from policy πin
M⋆orfMrespectively. Let bπdenote the policy output by the algorithm after Krounds of interaction
with the environment. By Lemma C.5 we have
EfM,ALGh
JM⋆(πM⋆)−JM⋆(bπ)i
≤3EM⋆,ALGh
JM⋆(πM⋆)−JM⋆(bπ)i
+4D2
H
PM⋆,ALG,PfM,ALG
.
By the subadditivity property for squared Hellinger distance (Lemma C.4) applied to the sequence
π(1), τ(1), . . . , π(K), τ(K), we have
D2
H
PM⋆,ALG,PfM,ALG
≤7EfM,ALG"KX
k=1D2
H(P(π(k)|π(1:k−1), τ(1:k−1)),Q(π(k)|π(1:k−1), τ(1:k−1)))+
D2
H(P(τ(k)|π(1:k), τ(1:k−1)),Q(τ(k)|π(1:k), τ(1:k−1)))#
= 7EfM,ALG"KX
k=1D2
H(P(τ(k)|π(1:k), τ(1:k−1)),Q(τ(k)|π(1:k), τ(1:k−1)))#
= 7EfM,ALG"KX
k=1D2
H
M⋆(π(k)),fM(π(k))#
≤49EfM,ALG"KX
k=1HX
h=1EfM,π(k)h
D2
H
M⋆
h(sh, ah),fMh(s1:h, a1:h)i#
where in the second step we have used that P(π(k)|π(1:k), τ(1:k−1)) =Q(π(k)|π(1:k), τ(1:k−1))
since the histories are equivalent, in the third step we have used that the trajectories are gener-
ated by the MDP/PODMP M⋆andfM, respectively, in the fourth step we have again applied
the subadditivity property of the squared Hellinger distance (Lemma C.4) to the sequence
(s1, a1, r1, . . . , s H, aH, rH).
61Theorem A.1 (Risk bound for O2L under self-predictive estimation) .Suppose REPself;optsatisfies
Assumption A.1 with parameter γ >0. Then Algorithm 1, with inputs T, K, Φ,REPself;opt, and
ALGlathas expected risk
E[Risk obs(TK)]≤c1·Risk base(K) +c2γ·K
TEst self;opt(T, γ) +c3γ−1·KH,
for absolute constants c1, c2, c3>0.
Proof of Theorem A.1. This follows from Theorem H.1 as well as Lemma H.4, by taking
α=c2√
KH and simplifying.
62I Additional Results for Appendix A: Self-Predictive Estimation
This section contains a more general result for algorithmic modularity under self-predictive estimation
(Theorem H.1), from which our main result is derived as a special case, along with associated
background, applications, and proofs. This section is organized as follows.
•Appendix I.1 presents: definitions for the ϕ-compressed POMDP and CorruptionRobust algo-
rithms (Appendix I.1.1), statements for properties of the ϕ-compressed dynamics (Appendix I.1.2).
The risk bound for O2L under self-predictive estimation and CorruptionRobustness (Theo-
rem H.1) is given in Appendix I.1.3, and a statement that the GOLFalgorithm is CorruptionRobust
(Appendix I.1.4).
• Appendix I.2 presents for the proofs for the properties of the ϕ-compressed POMDPs.
•Appendix I.3 presents a proof for the risk bound of O2L under self-predictive estimation and
CorruptionRobustness .
• Appendix I.4 presents a proof that the G OLF algorithm is CorruptionRobust .
I.1 O2L with Self-predictive Estimation and CorruptionRobust Base Algorithms
I.1.1 Definitions: ϕ-compressed POMDP and CorruptionRobustness
Consider iteration k∈[K]of epoch t∈[T]within O2L . Suppose that REPLEARN has
chosen decoder ϕ=ϕ(t):X → S . Then, the latent algorithm has observed the data
D(t,k)={ϕ(x(t,k)
h), a(t,k)
h, r(t,k)
h, ϕ(x(t,k)
h+1)}collected from the preceding policies in the epoch:
π(t,1)
lat◦ϕ(t), . . . , π(t,k−1)
lat◦ϕ(t)(Line 8). Due to possible inaccuracies in the decoder ϕ, the dataset
D(t,k)may not be generated from a Markovian process and must instead be viewed as being generated
from a PODMP, formally defined as follows.
Definition I.1 (ϕ-compressed POMDP) .Theϕ-compressed POMDP fM⋆
ϕinduced by M⋆
obsandϕ
is defined by:
1. Latent state space X
2. Action space A
3. Observation state space S
4. Latent reward functions R⋆
obs,h:X × A → [0,1]
5. Latent dynamics P⋆
obs,h:X × A → ∆(X)
6. (Deterministic) observation function Oh:X → S defined by Oh(x) =ϕh(x),
7. Horizon H
8. Initial latent distribution P⋆
obs(x0| ∅)
Note that the latent space for the POMDP is the observation space of the latent-dynamics MDP M⋆
obs,
and vice-versa; we adopt this terminology because—from the perspective of the base algorithm, the
observations xhcan be viewed as a Markovian (yet partially observed process) that generates the
learned states ϕ(xh)on which the algorithm acts. We write ePπlat
ϕ:=PfM⋆
ϕ,πlatfor the probability
distribution over trajectories (xh, sh, ah, rh)h∈[H]in the ϕ-compressed POMDP when playing policy
πlat:S×[H]→∆(A), where xh∈ X are the POMDP’s latent states, sh∈ Sare the observed states,
andah∈ A are the actions. We let eEπlat
ϕ:=EfM⋆
ϕ,πlatdenote the corresponding expectation. We write
ePϕ,h(sh+1|s1:h, a1:h) =ePπlat
ϕ(sh+1|s1:h, a1:h)and˜rϕ,h(rh|s1:h, a1:h) =ePπlat
ϕ(rh|s1:h, a1:h)
for the conditional distributions of next states and rewards given the first hstate-action pairs, which
are policy-independent. We also write fM⋆
ϕ(rh, sh+1|s1:h, a1:h) = ˜rϕ,h(rh|s1:h, a1:h)ePϕ,h(sh+1|
s1:h, a1:h)for the joint one-step probability. We will abbreviate fM⋆
ϕ(s1:h, a1:h):=fM⋆
ϕ(rh, sh+1=
· |s1:h, a1:h).
63Note that for any πlat,ePπlat
ϕ,h(sh+1|sh, ah)is a well-defined (Markovian, policy-dependent) proba-
bility kernel, which is equivalent to
ePπlat
ϕ,h(sh+1|sh, ah) =X
s1:h−1,a1:h−1ePπlat
ϕ,h(s1:h−1, a1:h−1|sh, ah)ePϕ,h(sh+1|s1:h, a1:h)(67)
=eEπlat
ϕh
ePϕ,h(sh+1|s1:h, a1:h)|sh, ahi
(68)
Similarly, ˜rπlat
ϕ,h(rh|sh, ah)is a Markovian and policy-dependent reward distribution which is
equivalent to
˜rπlat
ϕ,h(rh|sh, ah) =X
s1:h−1,a1:h−1ePπlat
ϕ,h(s1:h−1, a1:h−1|sh, ah)˜rϕ,h(rh|s1:h, a1:h) (69)
=eEπlat
ϕ[˜rϕ,h(rh|s1:h, a1:h)|sh, ah]. (70)
Finally, we let
fMπlat,⋆
ϕ,h(rh, sh+1|sh, ah) =eEπlat
ϕh
fM⋆
ϕ(rh, sh+1|s1:h, a1:h)|sh, ahi
(71)
denote the associated one-step model over joint rewards and transitions.
OurCorruptionRobustness condition asserts that the agent—when observing data from the ϕ(t)-
compressed dynamics fM⋆
ϕ—attains a risk bound for Mlatwhich is proportional to its risk when
observing data from Mlatitself, plus a term that captures the degree of misspecification between fM⋆
ϕ
andMlat.
Definition I.2 (CorruptionRobust algorithm) .We say that ALGlatisCorruptionRobust with
parameters αandRisk baseif there exists a constant c1such that, for any (ϕ, M lat)∈Φ× M lat, we
have
EfM⋆
ϕ,ALGlat[Risk(K,ALGlat, M lat)]≤c1·Risk base(K)
+αEfM⋆
ϕ,ALGlat
vuutKX
k=1HX
h=1Eπ(k)
lat∼p(k)eEπ(k)
lat
ϕh
D2
H
Mlat,h(sh, ah),fM⋆
ϕ,h(s1:h, a1:h)i
,
where we recall the definition of the random variable Risk(K,ALGlat, M lat)from Eq. (1), the
expectation EfM⋆
ϕ,ALGlatdenotes the interaction protocol of ALGlatin the ϕ-compressed dynamics
fM⋆
ϕ, and p(k)denotes the randomization distribution over latent policies that ALGlatplays.
I.1.2 Basic properties of the ϕ-compressed dynamics (Definition I.1)
We establish a number of basic properties for the ϕ-compressed POMDP and their relation to the
self-prediction guarantee obtained by REPself;opt. These properties are proved in Appendix I.2.
Firstly, we have the following change-of-measure lemma:
Lemma I.1 (Change of measure lemma) .For any ϕ∈Φ,f∈[S × A → [0,1]],h∈[H], and
πlat∈[S ×[H]→∆(A)], we have:
eEπlat
ϕ[f(sh, ah)] =Eπlat◦ϕ[[f◦ϕ](xh, ah)]. (72)
The next lemma states that the kernels of the ϕ-compressed POMDP are well-approximated by the
(Markovian) latent model fit by R EPself;opt. We recall the instantaneous self-prediction error
[∆h(Mlat, ϕ)](xh, ah):=D2
H 
Mlat,h(ϕh(xh), ah),
ϕh+1♯M⋆
obs,h
(xh, ah)
.
Lemma I.2 (Near-markovianity of the ϕ-compressed dynamics) .For any decoder ϕ, base model
Mlat, and policy πlat:S ×[H]→∆(A), we have:
HX
h=0eEπlat
ϕh
D2
H
Mlat,h(sh, ah),fM⋆
ϕ,h(s1:h, a1:h)i
≤HX
h=0Eπlat◦ϕ[[∆h(Mlat, ϕ)](xh, ah)].(73)
64Furthermore, we also have
HX
h=0eEπlat
ϕh
D2
H
Mlat,h(sh, ah),fM⋆,π lat
ϕ,h(sh, ah)i
≤HX
h=0Eπlat◦ϕ[[∆h(Mlat, ϕ)](xh, ah)].(74)
A corollary is the following lemma establishing errors between expectations under Mlat, the model
estimated by R EPself;opt, and those under the ϕ-compressed POMDP fM⋆
ϕ.
Lemma I.3 (Simulation lemma) .For any latent model Mlatwith Markovian transition kernel
{Plat,h}h∈[H], latent policy πlat:S ×[H]→∆(A), and decoder ϕ∈Φ, we have that for all
f:S × A → [0,1]:
|EMlat,πlat[f(sh, ah)]−eEπlat
ϕ[f(sh, ah)]|
≤X
h′<hEπlat◦ϕh[Plat◦ϕ]h(xh′, ah′)−ϕh+1♯P⋆
obs,h(xh′, ah′)
tvi
, (75)
and thus for any sequence of policies π(t)
lat, latent models M(t)
lat, and decoders ϕ(t), we have:
TX
t=1HX
h=0|EM(t)
lat,π(t)
lat[f(sh, ah)]−eEπ(t)
lat
ϕ(t)[f(sh, ah)]| (76)
≤H√
THvuutTX
t=1HX
h=0Eπ(t)
lat◦ϕ(t)
[∆h(M(t)
lat, ϕ(t))](xh, ah)
. (77)
I.1.3 Risk bound for O2L under CorruptionRobustness
We state the main risk bound for O2L under self-predictive estimation and the above definition of
corruption robustness.
Theorem H.1 (Risk bound for O2L under self-predictive estimation and CorruptionRobustness ).
Assume REPself;optsatisfies Assumption A.1 with parameter γ >0and that Mlatis realizable (i.e.
M⋆
lat∈ M lat). Furthermore, let ALGlatbeCorruptionRobust (Definition I.2) with parameter α.
Then, O2L (Algorithm 1) with inputs T, K, Φ,ALGlat, and REPself;opthas expected risk
E[Risk obs(TK)]≤c1·Risk base(K) +c2γ·K
TEst self;opt(T, γ) +c3γ−1· 
α2+H
(66)
for absolute constants c1, c2, c3>0.
I.1.4 Examples of CorruptionRobust algorithms
In this section, we establish that the GOLF algorithm satisfies the CorruptionRobust definition
(Definition I.2) with a parameter α≈K−1/2. This improves upon the rate that would be obtained
by invoking the generic guarantee in Lemma H.4. We expect that several other algorithms can
be analyzed in a similar way, thereby leading to tight rates in the same fashion. We restate the
pseudocode in Algorithm 5 for convenience.
LetMlat= (rlat, Plat)be given, and we let Q⋆
lat:=QMlat,⋆, andTlat,hf(s, a):=rlat,h(s, a) +
Es′∼Plat,h(s,a)[Vf(s′)]. We assume that the algorithm has a latent function class Falgwhich realizes
Q⋆
lat, as well as a helper class Galgwhich is Tlat-complete for Falg.
Assumption I.1 (Tlat-completeness) .We have:
Q⋆
lat∈ F alg,andTlatFalg⊆ G alg.
For our analysis of G OLF, it is most natural to quantify the corruption levels in the following way.
Assumption I.2 (Corruption levels of MlatandfM⋆
ϕ).Letε2
repbe such that, for any sequence of
policies π(k)
latplayed by the algorithm when interacting with the ϕ-compressed POMDP , we have
KX
k=1HX
h=1Eπ(k)
lat∼p(k)
lateEπ(k)
lat
ϕ
(rlat,h(sh, ah)−˜rπ(k)
ϕ,h(sh, ah))2+Plat,h(sh, ah)−ePπ(k)
ϕ,h(sh, ah)2
tv
≤ε2
rep. (78)
65Algorithm 5 GOLF [JLM21]
input: Function classes FandG, confidence width β >0.
initialize: F(0)← F ,D(0)
h← ∅ ∀ h∈[H].
1:forepisode t= 1,2, . . . , T do
2: Select policy π(t)←πf(t), where f(t):= arg maxf∈F(t−1)f(x1, πf,1(x1)).
3: Execute π(t)for one episode and obtain trajectory (x(t)
1, a(t)
1, r(t)
1), . . . , (x(t)
H, a(t)
H, r(t)
H).
4: Update dataset: D(t)
h← D(t−1)
h∪ 
x(t)
h, a(t)
h, x(t)
h+1	
∀h∈[H].
5: Compute confidence set:
F(t)←
f∈ F:L(t)
h(fh, fh+1)−min
gh∈GhL(t)
h(gh, fh+1)≤β∀h∈[H]
,
where L(t)
h(f, f′):=X
(x,a,r,x′)∈D(t)
h
f(x, a)−r−max
a′∈Af′(x′, a′)2
,∀f, f′∈ F.
6:end for
7:Output bπ=Unif(π(1:T)).
We note that
ε2
rep≲KX
k=1HX
h=1eEπ(k)
lat
ϕ
D2
H
Mlat,h(sh, ah),fM⋆,π(k)
lat
ϕ,h(sh, ah)
≤KX
k=1HX
h=1eEπ(k)
lat
ϕh
D2
H
Mlat,h(sh, ah),fM⋆
ϕ,h(s1:h, a1:h)i
by the data-processing inequality (cf. Eq. (90) and Eq. (80)) and the inequality ∥p−q∥2
tv≤D2
H(p, q),
and thus a CorruptionRobustness bound in terms of εrepimplies a CorruptionRobustness bound
in the sense of Definition I.2.
Theorem I.1 (Latent GOLF is CorruptionRobust ).Under Assumption I.1 and Assumption I.2,
Algorithm 5 with β=c 
log 
|F||G| KHδ−1
+εrep
, has regret
KX
k=1JMlat(πMlat)−JMlat(π(k))≤ O
Hp
CcovKlog(K) log( |F||G| HK/δ)
+O
H3/2q
KC covlog(K)ε2rep
,
and consequently is CorruptionRobust (Definition I.2) with parameters
α=H3/2
√
Kp
Ccovlog(K)andRisk base(K) =OH√
Kp
Ccovlog(K) log(|F||G| HK)
.
Corollary I.1 (GOLF applied in O2L ).Let us suppose that the appropriate assump-
tions for the estimator in Algorithm 4 to have regret bounded by Est self(T, γ) =
O √HC covTlog(Ccov|M lat||Llat||Φ|HT)
(Lemma A.1) hold. Then, we can take γ≈K−1/2
andT≈K4, and the bound Theorem H.1 gives an expected risk of εwith a number of trajecto-
riesTK=poly(Ccov, H,log|M lat|,log|Φ|,log|Llat|)·1/ε10, improving over the 1/ε14rate of the
universal result (Corollary A.1).
I.2 Proofs for Appendix I.1.2: Properties of ϕ-compressed POMDPs
Lemma I.1 (Change of measure lemma) .For any ϕ∈Φ,f∈[S × A → [0,1]],h∈[H], and
πlat∈[S ×[H]→∆(A)], we have:
eEπlat
ϕ[f(sh, ah)] =Eπlat◦ϕ[[f◦ϕ](xh, ah)]. (72)
Proof of Lemma I.1. Recall that ePπlat
ϕdenotes the law of (xh, sh, ah)h∈[H]in the ϕ-compressed
POMDP when playing policy πlat. For clarity, and to differentiate a random variable from its realiza-
tion, in the proofs below we will use upper-case notation such as {Sh=sh, Ah=ah, Xh=xh}to
indicate realizations of random variables in the POMDP.
66Let˜dπlat
h(s, a) =ePπlat
ϕ(Sh=s, Ah=a)be the marginalized occupancy measure for in the ϕ-
compressed POMDP fM⋆
ϕ. We write dπlat◦ϕ
h:=dM⋆
obs,πlat◦ϕ
h . The left-hand side in Eq. (72) is equal
to:
eEπlat
ϕ[f(sh, ah)] =X
s∈S,a∈A˜dπlat
h(s, a)f(s, a),
Meanwhile, the right-hand side is equal to:
Eπlat◦ϕ
h[[f◦ϕ](xh, ah)] =X
s∈S,a∈Af(s, a)X
x:ϕ(x)=sdπlat◦ϕ
h(x, a).
So it only remains to show that, for each s∈ S anda∈ A , we have ˜dπlat
h(s, a) =P
x:ϕ(x)=sdπlat◦ϕ
h(x, a). Firstly, note that it is enough to show thatP
xh:ϕ(xh)=shdπlat◦ϕ
h(xh) =
˜dπlat
h(sh), since ˜dπlat
h(sh, ah) = ˜dπlat
h(sh)πlat(ah|sh)andP
xh:ϕ(xh)=shdπlat◦ϕ
h(xh, ah) =P
xh:ϕ(xh)=shdπlat◦ϕ
h(xh)πlat(ah|ϕ(xh)) = πlat(ah|sh)P
xh:ϕ(xh)=shdπlat◦ϕ
h(xh). Toward
this, we have:
X
xh:ϕ(xh)=shdπlat◦ϕ
h(xh) =X
xh:ϕ(xh)=shX
xh−1,ah−1∈X×Adπlat◦ϕ
h−1(xh−1, ah−1)P⋆
obs,h(xh|xh−1, ah−1)
=X
xh−1,ah−1∈X×Adπlat◦ϕ
h−1(xh−1, ah−1)X
xh:ϕ(xh)=shP⋆
obs,h(xh|xh−1, ah−1)
=X
xh−1,ah−1∈X×Adπlat◦ϕ
h−1(xh−1, ah−1)P⋆
obs,h(ϕ(xh) =sh|xh−1, ah−1)
At the same time,
˜dπlat
h(sh) =ePπlat
ϕ(Sh=sh)
=X
ex,eaePπlat
ϕ(Xh−1=ex, A h−1=ea)ePπlat
ϕ(Sh=sh|Xh−1=ex, A h−1=ea)
=X
ex,eaePπlat
ϕ(Xh−1=ex, A h−1=ea)P⋆
obs(ϕ(xh) =sh|xh−1, ah−1),
where in the second equality we have used the definition of the observation function sh=O(xh) =
ϕ(xh).
To conclude, it remains to show that for all h, we have:
dπlat◦ϕ
h(xh, ah) =ePπlat
ϕ(Xh=xh, Ah=ah).
We do this by induction. Again, note that it is sufficient to establish dπlat◦ϕ
h(xh) =ePπlat
ϕ(Xh=xh).
The case h= 1is clear. For the general case, we have:
dπlat◦ϕ
h(xh) =X
xh−1,ah−1∈X×Adπlat◦ϕ
h−1(xh−1, ah−1)P⋆
obs(xh|xh−1, ah−1)
=X
xh−1,ah−1∈X×AePπlat
ϕ(Xh=xh−1, Ah−1=ah−1)P⋆
obs(xh|xh−1, ah−1)
=X
xh−1,ah−1∈X×AePπlat
ϕ(Xh=xh−1, Ah−1=ah−1)
×ePπlat
ϕ(Xh=xh|Xh−1=xh−1, Ah−1=ah−1)
=ePπlat
ϕ(Xh=xh).
67Lemma I.2 (Near-markovianity of the ϕ-compressed dynamics) .For any decoder ϕ, base model
Mlat, and policy πlat:S ×[H]→∆(A), we have:
HX
h=0eEπlat
ϕh
D2
H
Mlat,h(sh, ah),fM⋆
ϕ,h(s1:h, a1:h)i
≤HX
h=0Eπlat◦ϕ[[∆h(Mlat, ϕ)](xh, ah)].(73)
Furthermore, we also have
HX
h=0eEπlat
ϕh
D2
H
Mlat,h(sh, ah),fM⋆,π lat
ϕ,h(sh, ah)i
≤HX
h=0Eπlat◦ϕ[[∆h(Mlat, ϕ)](xh, ah)].(74)
Proof of Lemma I.2. We begin with the first event. Note that, for any πlat, the PODMP kernel
fM⋆
ϕ,h(rh, sh+1=· |s1:h, a1:h)can be written as:
fM⋆
ϕ,h(rh, sh+1=· |s1:h, a1:h) =X
xh,ah∈X×AePπlat
ϕ(rh, sh+1=· |xh, ah, s1:h, a1:h)
×ePπlat
ϕ(xh, ah|s1:h, a1:h)
=X
xh,ah∈X×AePπlat
ϕ(rh, sh+1=· |xh, ah)ePπlat
ϕ(xh, ah|s1:h, a1:h),
where we have used fM(rh, sh+1=· |s1:h, a1:h) =ePπlat
ϕ(rh, sh+1=· |s1:h, a1:h), the law of total
probability, and that xh, ahis a sufficient statistic for rhandsh+1. We further note that
ePπlat
ϕ(rh, sh+1=· |xh, ah) =M⋆
obs,h(rh, ϕh+1(xh+1) =· |xh, ah), (79)
since sh+1=Oh+1(xh+1) =ϕh+1(xh+1)is a deterministic function of xh+1andrh, xh+1∼
M⋆
obs,h(xh, ah). Thus, for a fixed handt, and omitting the hindices on the decoder ϕfor cleanliness,
the expectation in equation Eq. (73) becomes:
eEπlat
ϕh
D2
H
Mlat,h(sh, ah),fM⋆
ϕ,h(rh, sh+1=· |s1:h, a1:h)i
≤X
s1:h,a1:h∈(S×A )hePπlat
ϕ(s1:h, a1:h)X
xh,ahePπlat
ϕ(xh, ah|s1:h, a1:h)
×D2
H
Mlat,h(sh, ah),ePπlat
ϕ(rh, sh+1=· |xh, ah)
(Jensen)
=X
s1:h,a1:h∈(S×A )h
xh,ah∈X×AePπlat
ϕ(s1:h, a1:h)ePπlat
ϕ(xh, ah|s1:h, a1:h)
×D2
H
Mlat,h(ϕ(xh), ah),ePπlat
ϕ(rh, ϕ(xh+1) =· |xh, ah)
=X
xh,ah∈X×AePπlat
ϕ(xh, ah)D2
H 
Mlat(ϕ(xh), ah), M⋆
obs,h(rh, ϕ(xh+1) =· |xh, ah)
(Simplifying & Eq. (79))
=Eπlat◦ϕ
D2
H 
Mlat(ϕ(xh), ah), M⋆
obs,h(rh, ϕ(xh+1) =· |xh, ah)
(Change of measure (Lemma I.1))
=Eπlat◦ϕ
D2
H 
Mlat(ϕ(xh), ah), ϕ♯M⋆
obs,h(xh, ah)
, (By definition of ϕ♯M⋆
obs)
as desired. Summing over h∈[H]we obtain the desired bound. The bound Eq. (74) is a
consequence of Eq. (73) and the data-processing inequality. Namely, using the definition of fM⋆,π lat
ϕ,h
from Eq. (71) and the joint convexity of the squared Hellinger distance we have:
D2
H
Mlat,h(· |sh, ah),fM⋆,π lat
ϕ,h(· |sh, ah)
≤eEπlat
ϕh
D2
H
Mlat,h(· |sh, ah),fM⋆
ϕ,h(· |s1:h, a1:h)
|sh, ahi
. (80)
68Thus, we have
Eπlat
ϕh
D2
H
Mlat,h(· |sh, ah),fM⋆,π lat
ϕ,h(· |sh, ah)i
≤Eπlat
ϕh
Eπlat
ϕh
D2
H
Mlat,h(· |sh, ah),fM⋆
ϕ,h(· |s1:h, a1:h)
|sh, ahii
=Eπlat
ϕh
D2
H
Mlat,h(· |sh, ah),fM⋆
ϕ,h(· |s1:h, a1:h)i
,
as desired.
Lemma I.3 (Simulation lemma) .For any latent model Mlatwith Markovian transition kernel
{Plat,h}h∈[H], latent policy πlat:S ×[H]→∆(A), and decoder ϕ∈Φ, we have that for all
f:S × A → [0,1]:
|EMlat,πlat[f(sh, ah)]−eEπlat
ϕ[f(sh, ah)]|
≤X
h′<hEπlat◦ϕh[Plat◦ϕ]h(xh′, ah′)−ϕh+1♯P⋆
obs,h(xh′, ah′)
tvi
, (75)
and thus for any sequence of policies π(t)
lat, latent models M(t)
lat, and decoders ϕ(t), we have:
TX
t=1HX
h=0|EM(t)
lat,π(t)
lat[f(sh, ah)]−eEπ(t)
lat
ϕ(t)[f(sh, ah)]| (76)
≤H√
THvuutTX
t=1HX
h=0Eπ(t)
lat◦ϕ(t)
[∆h(M(t)
lat, ϕ(t))](xh, ah)
. (77)
Proof of Lemma I.3. Firstly note that, from Lemma I.1, the left-hand-side of Eq. (75) is equivalent
to
|EMlat,πlat[f(sh, ah)]−eEπlat
ϕ[f(sh, ah)]|=|EMlat,πlat[f(sh, ah)]−EM⋆
obs,πlat◦ϕ[[f◦ϕ](xh, ah)]|
(81)
For any πlat:S ×[H]→∆(A), letdπlat
lat,h=dMlat,πlat
hdenote the occupancy in Mlat, and similarly
for any πobs:X ×[H]→∆(A)letdπobs
obs,h(xh, ah) =dM⋆
obs,πobs
h(xh, ah)denote the occupancy in
M⋆
obs. We overload notation by letting dπlat◦ϕ
obs,h(s, a):=P
x:ϕ(x)=sdπ◦ϕ
obs,h(x, a). We will establish the
stronger result that
dπlat
lat,h(·)−dπlat◦ϕ
obs,h(·)
tv≤X
h′<hEπlat◦ϕ
∥[Plat◦ϕ](xh′, ah′)−ϕ♯P⋆
obs(xh′, ah′)∥tv
, (82)
where the tvnorm on the left-hand-side is over S × A . Note that this implies the desired bound on
Eq. (81) by Holder’s inequality. We prove this by induction over h. For the base case ( h= 0), we
have:
X
s1,a1dπlat
lat,1(s1, a1)−dπlat◦ϕ
obs (s1, a1)
=X
s1,a1Plat,0(s1| ∅)πlat(a1|s1)−X
x1=ϕ(x1)=s1dπlat◦ϕ
obs (x1, a1)
=X
s1,a1Plat,0(s1| ∅)πlat(a1|s1)−X
x1=ϕ(x1)=s1P⋆
obs,0(x1| ∅)πlat(a1|ϕ(x1))
=X
s1Plat,0(s1| ∅)−ϕ1♯P⋆
obs,0(s1| ∅)X
a1πlat(a1|s1)
=Plat,0(∅)−ϕ1♯P⋆
obs,0(∅)
tv.
69For the general case, let us further overload notation by letting dπ◦ϕ
obs,h(sh) =P
ahdπ◦ϕ
obs,h(sh, ah)
andP⋆
obs(sh|xh−1, ah−1) =ϕ♯P⋆
obs(sh|xh−1, ah−1) =P
xh:ϕ(xh)=shP⋆
obs(xh|xh−1, ah−1).
Let us also abbreviate π:=πlat. Firstly note that it is sufficient to establish the result forP
sh∈Sdπ
lat,h(sh)−dπ◦ϕ
obs,h(sh), since
X
sh,ah∈S×Adπ
lat,h(sh, ah)−dπ◦ϕ
obs,h(sh, ah)=X
sh,ah∈S×Adπ
lat,h(sh)−dπ◦ϕ
obs,h(sh)π(ah|sh)
=X
sh∈Sdπ
lat,h(sh)−dπ◦ϕ
obs,h(sh).
Below, all summations over sh(resp. xh) with domain unspecified are over S(resp.X), and likewise
for summations over sh, ahorxh, ah. We have:
X
shdπ
lat,h(sh)−dπ◦ϕ
obs,h(sh)
=X
shdπ
lat,h(sh)−X
xh:ϕ(xh)=shdπ◦ϕ
obs,h(xh)
=X
shX
sh−1,ah−1dπ
lat,h(sh−1, ah−1)Plat,h(sh|sh−1, ah−1)
−X
xh:ϕ(xh)=shX
xh−1,ah−1dπ◦ϕ
obs,h(xh−1, ah−1)P⋆
obs,h(xh|xh−1, ah−1)
=X
shX
sh−1,ah−1dπ
lat,h(sh−1, ah−1)Plat,h(sh|sh−1, ah−1)
−X
xh−1,ah−1dπ◦ϕ
obs,h(xh−1, ah−1)P⋆
obs,h(sh|xh−1, ah−1)
=X
shX
sh−1,ah−1dπ
lat,h(sh−1, ah−1)Plat,h(sh|sh−1, ah−1)
−X
xh−1,ah−1dπ◦ϕ
obs,h(xh−1, ah−1)Plat,h(sh|ϕ(xh−1), ah−1)
+X
xh−1,ah−1dπ◦ϕ
obs,h(xh−1, ah−1)Plat,h(sh|ϕ(xh−1), ah−1)
−X
xh−1,ah−1dπ◦ϕ
obs,h(xh−1, ah−1)P⋆
obs,h(sh|xh−1, ah−1)
≤X
sh−1,ah−1dπ
lat,h(sh−1, ah−1)−X
xh−1:ϕ(xh−1)=sh−1dπ◦ϕ
obs,h(xh−1, ah−1)X
shPlat,h(sh|sh−1, ah−1)
+X
shX
xh−1,ah−1dπ◦ϕ
obs,h(xh−1, ah−1) 
(Plat,h◦ϕ)(sh|xh−1, ah−1)−P⋆
obs,h(sh|xh−1, ah−1)
≤dπ
lat,h−1(·)−dπ◦ϕ
obs,h−1(ϕ−1(·))
tv
+X
xh−1,ah−1dπ◦ϕ
obs,h(xh−1, ah−1)X
sh(Plat,h◦ϕ)(sh|xh−1, ah−1)−P⋆
obs,h(sh|xh−1, ah−1)
≤dπ
lat,h−1(·)−dπ◦ϕ
obs,h−1(ϕ−1(·))
tv+Eπ◦ϕh[Plat,h◦ϕ](xh−1, ah−1)−ϕ♯P⋆
obs,h(xh−1, ah−1)
tvi
.
70From which it follows that, for each h, we have:dπ
lat,h(·)−dπ◦ϕ
obs,h(ϕ−1(·))
tv≤X
h′<hEπ◦ϕh[Plat◦ϕ]h′(xh′, ah′)−ϕh′+1♯P⋆
obs,h′(xh′, ah′)
tvi
≤X
h′∈[H]Eπ◦ϕh[Plat◦ϕ]h′(xh′, ah′)−ϕh′+1♯P⋆
obs,h′(xh′, ah′)
tvi
.
I.3 Proofs for Appendix I.1.3: Risk Bound Under CorruptionRobustness (Theorem H.1)
Theorem H.1 (Risk bound for O2L under self-predictive estimation and CorruptionRobustness ).
Assume REPself;optsatisfies Assumption A.1 with parameter γ >0and that Mlatis realizable (i.e.
M⋆
lat∈ M lat). Furthermore, let ALGlatbeCorruptionRobust (Definition I.2) with parameter α.
Then, O2L (Algorithm 1) with inputs T, K, Φ,ALGlat, and REPself;opthas expected risk
E[Risk obs(TK)]≤c1·Risk base(K) +c2γ·K
TEst self;opt(T, γ) +c3γ−1· 
α2+H
(66)
for absolute constants c1, c2, c3>0.
Proof of Theorem H.1. Let us write π(t,K+1)
lat =bπ(t)
latand, for any t, k∈[T]×[K+ 1],π(t,k)
obs:=
π(t,k)
lat◦ϕ(t). Let p(t,k)
obsdenote the distributions of played policies π(t,k)
obsinduced by the interaction
ofALGlatandREPself;optinside the O2L algorithm. Let us write the online sum of self-prediction
errors as
ε2
rep:=TX
t=1K+1X
k=1HX
h=0Eπ(t,k)
obs∼p(t,k)Eπ(t,k)
obs
D2
H 
[M(t)
lat◦ϕ(t)]h(xh, ah), ϕ(t)
h+1♯M⋆
obs,h(xh, ah)
(83)
Since the final output policy of O2L satisfies bπlat=Unif(bπ(1)
lat, . . . ,bπ(T)
lat)(Line 12), we have
E[Risk obs(TK)] =1
TTX
t=1Eh
JM⋆
obs(π⋆
obs)−JM⋆
obs(bπ(t)
obs)i
.
We take the following decomposition on the risk
JM⋆
obs(π⋆
obs)−JM⋆
obs(bπ(t)
obs) =JM⋆
lat(πM⋆
lat)−JM(t)
lat(πM(t)
lat) +JM(t)
lat(πM(t)
lat)−JM(t)
lat(bπ(t)
lat)
| {z }
At
+JM(t)
lat(bπ(t)
lat)−JM⋆
obs(bπ(t)
obs)| {z }
Bt. (84)
We will show that EhPT
t=1Ati
≲TRegbase(K) +α√
TE[εrep]and that EhPT
t=1Bti
≲
√
THE[εrep], then return to the first term JM⋆
lat(πM⋆
lat)−JM(t)
lat(πM(t)
lat)at the end of the proof.
To bound EhPT
t=1Ati
, we note that
TX
t=1E[At]≤c1TRisk base(K)+
αTX
t=1E
vuutKX
k=1HX
h=1Eπ(t,k)
lat∼p(t,k)
lateEπ(t,k)
lat
ϕ(t)h
D2
H
M(t)
lat,h(sh, ah),fM⋆
ϕ(t),h(s1:h, a1:h)i

≤c1TRisk base(K) +αTX
t=1E
vuutKX
k=1HX
h=1Eπ(t,k)
lat∼p(t,k)
latEπ(t,k)
lat◦ϕ(t)
[∆h(M(t)
lat, ϕ(t))](xh, ah)

≤c1TRisk base(K) +α√
TE
vuutTX
t=1KX
k=1HX
h=1Eπ(t,k)
obs∼p(t,k)
obsEπ(t,k)
obs
[∆h(M(t)
lat, ϕ(t))](xh, ah)

≤c1TRisk base(K) +α√
TE[εrep].
71where the first line follows from the CorruptionRobust definition (Definition I.2), the second line
follows from Lemma I.2, the third line follows by Cauchy-Schwartz, and the last line recalls the
definition of εrepfrom Eq. (83).
For the termPT
t=1Bt, for any πlat:S ×[H]→∆(A)we let Qπlat
lat(t),h=TM(t)
lat
hQπlat
lat(t),h+1be the
Qπlatfunction of the latent MDP M(t)
lat. Note that
TX
t=1n
JM(t)
lat(bπ(t)
lat)−Ebπ(t)
lat◦ϕ(t)h
[Qbπ(t)
lat(t)◦ϕ(t)]1(x1, a1)io
(85)
=TX
t=1EM(t)
lat,bπ(t)
lath
Qbπ(t)
lat(t),1(s1, a1)i
−Ebπ(t)
lat◦ϕ(t)h
[Qbπ(t)
lat(t)◦ϕ(t)]1(x1, a1)i
≤TX
t=1Ebπ(t)
lat◦ϕ(t)[P(t)
lat◦ϕ(t)]0(∅)−ϕ(t)
1♯P⋆
obs,0(∅)
tv
(by Lemma I.3)
≤TX
t=1HX
h=0Ebπ(t)
lat◦ϕ(t)[P(t)
lat◦ϕ(t)]h(xh, ah)−ϕ(t)
h+1♯P⋆
obs,h(xh, ah)
tv
≤√
THε rep, (by Cauchy-Schwartz)
so it is enough to bound
TX
t=1
Ebπ(t)
lat◦ϕ(t)
[Qbπ(t)
lat
lat(t)◦ϕ(t)]1(x1, a1)
−JM⋆
obs(bπ(t)
obs)
.
Fixtandh, whose indexing we omit below for cleanliness. Note that, for any πlat:S×[H]→∆(A),
we have:
Eπlat◦ϕ
[Qπlat
lat◦ϕ]h(xh, ah)− TM⋆
obs,πlat◦ϕ
h[Qπlat
lat◦ϕ]h+1(xh, ah)2
(86)
≤2Eπlat◦ϕh 
[rlat◦ϕ]h−r⋆
obs,h2(xh, ah)i
(87)
+ 2Eπlat◦ϕ
EPlat,h(ϕ(xh),ah)h
Qπlat
lat,h+1(·, πlat)i
−EP⋆
obs,h(xh,ah)
[Qπlat
lat◦ϕ]h+1(·, πlat)2
(88)
≤2Eπlat◦ϕh 
[rlat◦ϕ]h−r⋆
obs,h2(xh, ah) +Plat,h(ϕ(xh), ah)−ϕh+1♯P⋆
obs,h(xh, ah)2
tvi
(89)
≤4Eπlat◦ϕ
D2
H 
Mlat,h(ϕh(xh), ah), ϕh+1♯M⋆
obs,h(xh, ah)
, (90)
where the final line follows from two applications of the data-processing inequality (since
Mlat,h(rh, sh+1|ϕh(xh), ah) = Rlat,h(rh|ϕh(xh), ah)Plat,h(sh+1|ϕh(xh), ah)and
ϕh+1♯M⋆
obs,h(rh, sh+1|xh, ah) =R⋆
obs,h(rh|xh, ah)ϕh+1♯P⋆
obs,h(sh+1|xh, ah)) as well as
the bound ∥p−q∥2
tv≤D2
H(p, q). Summing this over t, hand using a standard decomposition for
72regret (Lemma C.6) gives:
TX
t=1
Ebπ(t)
lat◦ϕ(t)
[Qbπ(t)
lat
lat(t)◦ϕ(t)]1(x1, a1)
−JM⋆
obs(bπ(t)
obs)
=TX
t=1HX
h=1Ebπ(t)
lat◦ϕ(t)
[Qbπ(t)
lat
lat(t)◦ϕ(t)]h(xh, ah)− TM⋆
obs,bπ(t)
lat◦ϕ(t)
h[Qbπ(t)
lat
lat(t)◦ϕ(t)]h+1(xh, ah)
(Lemma C.6)
≤√
THvuutTX
t=1HX
h=1Ebπ(t)
lat◦ϕ(t)"
[Qbπ(t)
lat
lat(t)◦ϕ(t)]h(xh, ah)− TM⋆
obs,bπ(t)
lat◦ϕ(t)
h[Qbπ(t)
lat
lat(t)◦ϕ(t)]h+1(xh, ah)2#
≤√
4THvuutTX
t=1HX
h=1Ebπ(t)
lat◦ϕ(t)h
D2
Hh
M(t)
lat,h◦ϕ(t)
hi
(xh, ah), ϕ(t)
h+1♯M⋆
obs,h(xh, ah)i
(By Eq. (90))
≤√
4THε rep.
Returning to the decomposition of Eq. (84) and combining everything gives:
E[Risk obs]≤1
T(TX
t=1Eh
JM⋆
lat(πM⋆
lat)−JM(t)
lat(πM(t)
lat)i)
+1
T
α√
T+ 4√
TH
E[εrep]
+c1·Risk base(K)
≤1
T(TX
t=1Eh
J(π⋆)−JM(t)
lat(πM(t)
lat) +γε2
repi)
+γ−1
T
α√
T+ 4√
TH2
+c1·Risk base(K)
≤γ2K
TEst self;opt(T, γ) + 2γ−1 
α2+ 16H
+c1·Risk base(K),
where the second inequality follows by AM-GM applied to the middle term and the third
inequality follows from: i) Jensen’s inequality, ii) Assumption A.1 applied to the distributions
¯p(t)
obs=1
KPK
k=1p(t,k)
obs, iii) the bound K+1≤2K, and iv) the inequality (x+y)2≤2(x2+y2).
I.4 Proofs for Appendix I.1.4: Examples of CorruptionRobust Algorithms
Theorem I.1 (Latent GOLF is CorruptionRobust ).Under Assumption I.1 and Assumption I.2,
Algorithm 5 with β=c 
log 
|F||G| KHδ−1
+εrep
, has regret
KX
k=1JMlat(πMlat)−JMlat(π(k))≤ O
Hp
CcovKlog(K) log( |F||G| HK/δ)
+O
H3/2q
KC covlog(K)ε2rep
,
and consequently is CorruptionRobust (Definition I.2) with parameters
α=H3/2
√
Kp
Ccovlog(K)andRisk base(K) =OH√
Kp
Ccovlog(K) log(|F||G| HK)
.
Proof of Theorem I.1. Recall that the agent is observing data from the ϕ-compressed POMDP
fM⋆
ϕ, and thus the datasets are of the form D(k)
h=D(k)
ϕ,h={ϕ(x(i)
h), a(i)
h, r(i)
h, ϕ(x(i)
h+1)}k−1
i=1. For any
πlat∈Πlat, we define
eTπlat
ϕ,hf(sh, ah) = ˜rπlat
ϕ,h(sh, ah) +Es′∼ePπlat
ϕ,h(sh,ah)[f(s′)],
where ˜rπlat
ϕ,handePπlat
ϕ,hare the policy-dependent Markov operators defined in Eq. (67) and Eq. (69).
As a consequence, we observe the following misspecification guarantee for Tlat.
73Lemma I.4 (Misspecification guarantee for Tlat).
∀f:S × A → [0,1] :KX
k=1HX
h=1eEπ(k)
ϕ
Tlat,hf(sh, ah)−eTπ(k)
ϕ,hf(sh, ah)2
≤ O(ε2
rep).
Proof of Lemma I.4. Follows from Assumption I.2 and the definitions of eTπ(k)
ϕ,h andTlat,h.
We begin with the following lemmas, which will be proved in the sequel.
Lemma I.5 (Optimism) .For the choice of βin Theorem I.1, with probability at least 1−δ, we have
that for all k∈[K]:
Q⋆
lat∈ F(k).
Lemma I.6 (Small in-sample squared Bellman errors) .With probability at least 1−δ, we have that
for all k∈[K],h∈[H], and f∈ F(k):
k−1X
i=1eEπ(i)
ϕ
f(sh, ah)−eTπ(i)
ϕ,hf(sh, ah)2
≤ O(β).
Let us write π(k)
obs:=π(k)◦ϕ. Let us introduce the shorthand ˜d(k)
obs,h:=Pk−1
i=1dπ(k)
obs
obs,h, where dπ
obsis
the occupancy for M⋆
obs, and also the burn-in time
κh(x, a):= min(
k:k−1X
i=1dπ(k)
obs,h(x, a)≥Ccovµ⋆
h(x, a))
.
Let us recall, from the analysis of [XFBJK23], that for any h∈[H]andf:S × A → [0,1]we have
KX
k=1Eπ(k)[f(sh, ah)I{k < κ h(sh, ah)}]≤2Ccov, (91)
as well as
HX
h=1KX
k=1X
s,a(dπ(k)
obs
h(x, a)I{k≥κh(x, a)})2
˜d(k)
h(x, a)≤O(HC covlog(K)). (92)
74X
kJMlat(πMlat)−JMlat(π(k))≤KX
k=1HX
h=1EMlat,π(k)[f(k)(sh, ah)− T latf(k)(sh, ah)]
(Optimism (Lemma I.5))
≤KX
k=1HX
h=1eEπ(k)
ϕ[f(k)(sh, ah)− T latf(k)(sh, ah)] +H3/2q
Kε2rep
(Simulation Lemma Lemma I.3)
=KX
k=1HX
h=1Eπ(k)◦ϕ[[(f(k)− T latf(k))◦ϕ](xh, ah)] +H3/2q
Kε2rep
(Change of measure Lemma I.1)
≤KX
k=1HX
h=1Eπ(k)◦ϕ[[(f(k)− T latf(k))◦ϕ](xh, ah)I{k≥κh(xh, ah)}]
+ 2HC cov+H3/2q
Kε2rep (Burn-in time Eq. (91))
≤KX
k=1HX
h=1Eπ(k)◦ϕhh
(f(k)−eTπ(k)
ϕ,hf(k))◦ϕi
(xh, ah)I{k≥κh(xh, ah)}i
| {z }
(I)
+KX
k=1HX
h=1Eπ(k)◦ϕhh
(eTπ(k)
ϕ,hf(k)− T lat,hf(k))◦ϕi
(xh, ah)i
| {z }
(II)
+ 2HC cov+H3/2q
Kε2rep
Note that, by change of measure (Lemma I.1) and the misspecification guarantee (Lemma I.4), the
second term is bounded by:
(II) =KX
k=1HX
h=1eEπ(k)
ϕh
(eTπ(k)
ϕ,hf(k)− T lat,hf(k))(sh, ah)i
≤q
KHε2rep.
Turning to the first term, we have:
HX
h=1KX
k=1Eπ(k)
obshh
(f(k)−eTπ(k)
ϕ,hf(k))◦ϕi
(xh, ah)I{k≥κh(xh, ah)}i
(93)
≤vuutHX
h=1KX
k=1X
x,a(dπ(k)
obs
h(x, a)I{k≥κh(x, a)})2
˜d(k)
h(x, a)vuutHX
h=1KX
k=1E˜d(k)
obs
(f(k)−eTπ(k)
ϕ,hf(k))◦ϕ2
(xh, ah)
(94)
≤p
HC covlog(K)vuutHX
h=1KX
k=1E˜d(k)
obs
(f(k)−eTπ(k)
ϕ,hf(k))◦ϕ2
(xh, ah)
(coverability potential Eq. (92))
=p
HC covlog(K)vuutHX
h=1KX
k=1k−1X
i=1eEπ(i)
ϕ
f(k)(sh, ah)−eTπ(k)
ϕ,hf(k)(sh, ah)2
(change of measure, Lemma I.1)
≤ O
Hp
CcovKlog(K)β
, (95)
75where we have used that, from Lemma I.6, we have:
HX
h=1KX
k=1k−1X
i=1eEπ(i)
ϕ
f(k)(sh, ah)−eTπ(i)
ϕf(k)(sh, ah)2
≤ O(βHK ).
This gives an upper bound on the regret of
KX
k=1JMlat(πMlat)−JMlat(π(k))≤ O
Hp
CcovKlog(K)β+H3/2q
Kε2rep
.
Using that β=O
log
|F||G| HK
δ
+εrep
and simplifying gives
KX
k=1JMlat(πMlat)−JMlat(π(k))≤ O
Hp
CcovKlog(K) log( |F||G| HK/δ)
+O
H3/2q
KC covlog(K)ε2rep
,
as desired. It only remains to establish the concentrations results.
Concentration analysis. We establish the concentration results of Lemma I.5 and Lemma I.6.
Proof of Lemma I.6. Let
Xk(h, f) = 
fh(s(k)
h, a(k)
h)−r(k)
h−fh+1(s(k)
h+1)2−
eTπ(k)
ϕfh(s(k)
h, a(k)
h)−r(k)
h−fh+1(s(k)
h+1)2
.
LetFk,h={s(i)
1, a(i)
1, r(i)
1, . . . , s(i)
H, a(i)
H, r(i)
H}k
i=1. Note that
E
r(k)
h+fh+1(s(k)
h+1)|Fk,h
=E
r(k)
h+fh+1(s(k)
h+1)|π(k)
=E
E
r(k)
h+fh+1(s(k)
h+1)|s(k)
h, a(k)
h, π(k)
|π(k)
=Eh
eTπ(k)
ϕf(s(k)
h, a(k)
h)|π(k)i
=eEπ(k)
ϕh
eTπ(k)
ϕf(sh, ah)i
,
and thus that
E[Xk(h, f)|Fk,h] =eEπ(k)
ϕ
fh(sh, ah)−eTπ(k)
ϕfh(sh, ah)2
.
Next, note that
Var[Xk(h, f)|Fk,h]≤Eh
(Xk(h, f))2|Fk,hi
≤16E
fh(s(k)
h, a(k)
h)−eTπ(k)
ϕfh(s(k)
h, a(k)
h)2
|Fk,h
= 16E[Xk(h, f)|Fk,h].
By Freedman’s inequality (Lemma C.2, Lemma C.3), we have that with probability at least 1−δ:
X
t<kXt(h, f)−X
t<kE[Xt(h, f)|Ft,h]≤ O
s
log(1/δ)X
t<kE[Xt(h, f)|Ft,h] + log(1 /δ)

Taking a union bound over [K]×[H]×F, we have that for all k, h, f , with probability at least 1−δ:
X
t<kXt(h, f)−X
t<keEπ(k)
ϕ
fh(sh, ah)−eTπ(k)
ϕfh(sh, ah)2(96)
≤ O s
ιX
t<keEπ(k)
ϕ
fh(sh, ah)−eTπ(k)
ϕfh(sh, ah)2
+ι!
, (97)
where ι= log( |F|HK/δ ). We now show that
X
t<kXt(h, f(k))≤β+O(εrep+ι) =O(β), (98)
76which will imply, from Eq. (96), that
X
t<keEπ(k)
ϕ
fh(sh, ah)−eTπ(k)
ϕfh(sh, ah)2
≤ O(ι+β) =O(β),
as desired. To see Eq. (98), let
∆k=X
t<k 
Tlatf(k)
h(s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2−
eTπ(t)
ϕf(k)
h(s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2
and then note that:
X
t<kXt(h, f(k)) =X
t<k 
f(k)
h(s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2−
eTπ(t)
ϕf(k)
h(s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2
=X
t<k 
f(k)
h(s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2
−X
t<k 
Tlatf(k)
h(s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2+ ∆ k
≤X
t<k 
f(k)
h(s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2
−inf
gh∈GhX
t<k 
g(s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2+ ∆ k
≤β+ ∆ k.
where the second-to-last line follows from TlatF ⊆ G and the last line follows from the definition of
the confidence set. It remains to show that ∆k≤ O(εrep+ι), which we do via a similar concentration
argument. Namely, let
Yt(h, f) = 
Tlatfh(s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2−
eTπ(t)
ϕfh(s(t)
h, a(t)
h)−r(t)
h−f(k)
h+1(s(t)
h+1)2
,
and note that, as before,
E[Yt(h, f)|Ft,h] =eEπ(t)
ϕ
Tlatfh(sh, ah)−eTπ(t)
ϕfh(sh, ah)2
,
and
Var[Yt(h, f)|Ft,h]≤16E[Yt(h, f)|Ft,h],
by the same calculation as earlier. Thus, by Freedman’s inequality and a union bound, we have that,
with probability at least 1−δ,
X
t<kYt(h, f)−X
t<keEπ(k)
ϕ
Tlatfh(sh, ah)−eTπ(k)
ϕfh(sh, ah)2(99)
≤ O s
ιX
t<keEπ(k)
ϕ
Tlatfh(sh, ah)−eTπ(k)
ϕfh(sh, ah)2
+ι!
, (100)
where ι= log( |F|HK/δ ). Recalling the misspecification assumption Lemma I.4, this implies that
X
t<kYt(h, f)≤ O(εrep+ι),
for all h, f, k , with high probability. Applying this to f=f(k)concludes the result.
Proof of Lemma I.5. We use similar arguments to the preceding lemma. Let Q⋆
lat,h:=Q⋆
Mlat,h. The
aim is to show that, for all h∈[H], k∈[K], g∈ G, we have:
X
t<k 
g(s(t)
h, a(t)
h)−r(t)
h−Q⋆
lat,h+1(s(t)
h+1)2− 
Q⋆
lat,h(s(t)
h, a(t)
h)−r(t)
h−Q⋆
lat,h(s(t)
h+1)2≥ −β,
77from which the conclusion will follow. We show that
X
t<k 
g(s(t)
h, a(t)
h)−r(t)
h−Q⋆
lat,h+1(s(t)
h+1)2−
eTπ(t)
ϕQ⋆
lat,h(s(t)
h, a(t)
h)−r(t)
h−Q⋆
lat,h(s(t)
h+1)2
| {z }
:=Wt(h,g)≥ −β/2,
(101)
and also that
X
t<k
eTπ(t)
ϕQ⋆
lat,h(s(t)
h, a(t)
h)−r(t)
h−Q⋆
lat,h(s(t)
h+1)2
− 
Q⋆
lat,h(s(t)
h, a(t)
h)−r(t)
h−Q⋆
lat,h(s(t)
h+1)2
| {z }
:=Vt(h)≥ −β/2.
(102)
For Eq. (101), note that
E[Wt(h, g)| Ft,h] =eEπ(t)
ϕ
gh(sh, ah)−eTπ(t)
ϕ,hQ⋆
lat,h(sh, ah)2
, (103)
and that Var[Wt(h, g)| Ft,h]≤16E[Wt(h, g)| Ft,h]. By Freedman, this gives
X
t<kWt(h, g)−X
t<kE[Wt(h, g)|Ft,h]≤ O
s
ιX
t<kE[Wt(h, g)| Ft,h] +ι

≤1
2E[Wt(h, g)| Ft,h] +O(ι),
or in other words
X
t<kWt(h, g)≥1
2X
t<kE[Wt(h, g)|Ft,h]− O(ι)≥ −O (ι),
using the non-negativity of Eq. (103). For Eq. (102), note that
E[Vt(h)|Ft,h] =−eEπ(t)
ϕ
Tlat,hQ⋆
lat,h−eTπ(t)
ϕ,hQ⋆
lat,h2
≥ −εrep, (104)
and that Var[Vt(h)|Ft,h]≤16eEπ(t)
ϕ
Tlat,hQ⋆
lat,h−eTπ(t)
ϕ,hQ⋆
lat,h2
. By Freedman, this gives
X
t<kVt(h)−X
t<kE[Vt(h)|Ft,h](105)
≤ O s
ιX
t<keEπ(t)
ϕ
TlatQ⋆
lat,h+1(sh, ah)−eTπ(t)
ϕ,hQ⋆
lat,h+1(sh, ah)2
+ι!
(106)
=O(εrep+ι), (107)
or in other words
X
t<kVt(h)≥X
t<kE[Vt(h)|Ft,h]− O(εrep+ι)≥ −O (εrep+ι),
where we have used Eq. (104).
78NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope?
Answer: [Yes]
Justification: All results in this paper are of a theoretical nature, and the stated contributions in the
abstract and introduction are given in a precise, formal language.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims made in the
paper.
•The abstract and/or introduction should clearly state the claims made, including the contributions
made in the paper and important assumptions and limitations. A No or NA answer to this
question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how much the
results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals are not
attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: All results in this paper are of a theoretical nature – we precisely state the conditions
under which our results hold.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that the
paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to violations of
these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,
asymptotic approximations only holding locally). The authors should reflect on how these
assumptions might be violated in practice and what the implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was only tested
on a few datasets or with a few runs. In general, empirical results often depend on implicit
assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach. For
example, a facial recognition algorithm may perform poorly when image resolution is low or
images are taken in low lighting. Or a speech-to-text system might not be used reliably to
provide closed captions for online lectures because it fails to handle technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms and how
they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to address
problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by reviewers
as grounds for rejection, a worse outcome might be that reviewers discover limitations that
aren’t acknowledged in the paper. The authors should use their best judgment and recognize
that individual actions in favor of transparency play an important role in developing norms that
preserve the integrity of the community. Reviewers will be specifically instructed to not penalize
honesty concerning limitations.
793.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and a
complete (and correct) proof?
Answer: [Yes]
Justification: Each theoretical result is stated with all necessary assumptions and is accompanied
by complete (and correct) proofs.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if they appear
in the supplemental material, the authors are encouraged to provide a short proof sketch to
provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented by
formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main experi-
mental results of the paper to the extent that it affects the main claims and/or conclusions of the
paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived well by the
reviewers: Making the paper reproducible is important, regardless of whether the code and data
are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken to make
their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways. For
example, if the contribution is a novel architecture, describing the architecture fully might
suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary
to either make it possible for others to replicate the model with the same dataset, or provide
access to the model. In general. releasing code and data is often one good way to accomplish
this, but reproducibility can also be provided via detailed instructions for how to replicate the
results, access to a hosted model (e.g., in the case of a large language model), releasing of a
model checkpoint, or other means that are appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submissions
to provide some reasonable avenue for reproducibility, which may depend on the nature of the
contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how to
reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe the
architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should either
be a way to access this model for reproducing the results or a way to reproduce the model
(e.g., with an open-source dataset or instructions for how to construct the dataset).
80(d)We recognize that reproducibility may be tricky in some cases, in which case authors are
welcome to describe the particular way they provide for reproducibility. In the case of
closed-source models, it may be that access to the model is limited in some way (e.g.,
to registered users), but it should be possible for other researchers to have some path to
reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instructions to
faithfully reproduce the main experimental results, as described in supplemental material?
Answer: [NA]
Justification: The paper does not include experiments requiring code.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be possible,
so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless
this is central to the contribution (e.g., for a new open-source benchmark).
•The instructions should contain the exact command and environment needed to run to reproduce
the results. See the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how to access
the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new proposed
method and baselines. If only a subset of experiments are reproducible, they should state which
ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized versions (if
applicable).
•Providing as much information as possible in supplemental material (appended to the paper) is
recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail that is
necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
81• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the main claims
of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for example,
train/test split, initialization, random drawing of some parameter, or overall run with given
experimental conditions).
•The method for calculating the error bars should be explained (closed form formula, call to a
library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error of the
mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should preferably
report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of
errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or figures
symmetric error bars that would yield results that are out of range (e.g. negative error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how they were
calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the experi-
ments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual experimental
runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute than the
experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it
into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS
Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conforms with the NeurIPS Code of Etichs.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a deviation
from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consideration due
to laws or regulations in their jurisdiction).
10.Broader Impacts
82Question: Does the paper discuss both potential positive societal impacts and negative societal
impacts of the work performed?
Answer: [NA]
Justification: This is a primarily theoretical work.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal impact or
why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses (e.g.,
disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-
ment of technologies that could make decisions that unfairly impact specific groups), privacy
considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied to par-
ticular applications, let alone deployments. However, if there is a direct path to any negative
applications, the authors should point it out. For example, it is legitimate to point out that
an improvement in the quality of generative models could be used to generate deepfakes for
disinformation. On the other hand, it is not needed to point out that a generic algorithm for
optimizing neural networks could enable people to train models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is being used
as intended and functioning correctly, harms that could arise when the technology is being used
as intended but gives incorrect results, and harms following from (intentional or unintentional)
misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation strategies
(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for
monitoring misuse, mechanisms to monitor how a system learns from feedback over time,
improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible release of
data or models that have a high risk for misuse (e.g., pretrained language models, image generators,
or scraped datasets)?
Answer: [NA]
Justification: This is a purely theoretical work, and as such poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with necessary
safeguards to allow for controlled use of the model, for example by requiring that users adhere
to usage guidelines or restrictions to access the model or implementing safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors should
describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do not require
this, but we encourage authors to take this into account and make a best faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the
paper, properly credited and are the license and terms of use explicitly mentioned and properly
respected?
Answer: [NA]
Justification: The paper does not use existing assets.
83Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of service of
that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package should
be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for
some datasets. Their licensing guide can help determine the license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of the derived
asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to the asset’s
creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their sub-
missions via structured templates. This includes details about training, license, limitations,
etc.
•The paper should discuss whether and how consent was obtained from people whose asset is
used.
•At submission time, remember to anonymize your assets (if applicable). You can either create
an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as well as
details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsouring nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
•Including this information in the supplemental material is fine, but if the main contribution of
the paper involves human subjects, then as much detail as possible should be included in the
main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
labor should be paid at least the minimum wage in the country of the data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub-
jects
84Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals
(or an equivalent approval/review based on the requirements of your country or institution) were
obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent) may be
required for any human subjects research. If you obtained IRB approval, you should clearly
state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions and
locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for
their institution.
•For initial submissions, do not include any information that would break anonymity (if applica-
ble), such as the institution conducting the review.
85