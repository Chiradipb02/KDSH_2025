Provable Benefits of Complex Parameterizations
for Structured State Space Models
Yuval Ran-MiloIEden LumbrosoIEdo Cohen-KarlikI
Raja GiryesIAmir GlobersonI IINadav CohenI
Abstract
Structured state space models (SSMs), the core engine behind prominent neural
networks such as S4 and Mamba, are linear dynamical systems adhering to a
specified structure, most notably diagonal. In contrast to typical neural network
modules, whose parameterizations are real, SSMs often use complex parameter-
izations. Theoretically explaining the benefits of complex parameterizations for
SSMs is an open problem. The current paper takes a step towards its resolution,
by establishing formal gaps between real and complex diagonal SSMs. Firstly,
we prove that while a moderate dimension suffices in order for a complex SSM
to express all mappings of a real SSM, a much higher dimension is needed for a
real SSM to express mappings of a complex SSM. Secondly, we prove that even if
the dimension of a real SSM is high enough to express a given mapping, typically,
doing so requires the parameters of the real SSM to hold exponentially large values,
which cannot be learned in practice. In contrast, a complex SSM can express
any given mapping with moderate parameter values. Experiments corroborate our
theory, and suggest a potential extension of the theory that accounts for selectivity,
a new architectural feature yielding state of the art performance.1
1 Introduction
Structured state space models (SSMs ) are the core engine behind prominent neural network architec-
tures such as S4 [ 21], Mamba [ 20], LRU [ 41], Mega [ 37], S5 [ 50] and more [ 23,31,38,34]. In their
typical form, SSMs can be thought of as single-input single-output linear dynamical systems, wherein
the state transition matrix has a specified structure, most notably diagonal [ 22,23,41,50,37,20]. A
salient characteristic of SSMs is that their parameterizations are often complex (take values in C), in
contrast to typical neural network modules whose parameterizations are conventionally real (take
values in R).
There has been mixed evidence regarding benefits of complex parameterizations over real param-
eterizations for SSMs. Some prior works have demonstrated that complex parameterizations are
essential for strong performance [ 22,41], whereas others have shown that in various settings real
parameterizations lead to comparable (and in some cases better) performance [ 37,20]. It was conjec-
tured [ 20] that in the context of diagonal SSMs (namely, SSMs with diagonal state transition matrix),
complex parameterizations are preferable for continuous data modalities ( e.g., audio, video), whereas
for discrete data modalities ( e.g., text, DNA) real parameterizations suffice. Unfortunately, to date,
formal support for this conjecture is lacking. The extent to which complex parameterizations benefit
diagonal SSMs remains to be an open question.
In this paper, we take a step towards theoretically addressing the foregoing question. Specifically, we
provide two theoretical contributions establishing provable benefits of complex parameterizations
ITel Aviv UniversityIIGoogle. Correspondence to: Yuval Milo <yuvalmilo@mail.tau.ac.il>
1Due to lack of space, a portion of the paper is deferred to the appendices. We refer the reader to [ 44] for a
self-contained version of the text.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).for diagonal SSMs. Our first contribution establishes that, although both real and complex diagonal
SSMs are universal —in the sense that both can precisely express any linear time-invariant (LTI)
mapping up to any time twhen their dimensions are equal to or greater than t—there is a strong
separation between the SSMs in terms of expressiveness. Namely, denoting the dimensions of the
real and complex SSMs by nRandnC, respectively, we prove that for any nC, there are various
oscillatory mappings expressible by the complex SSM which cannot be approximately expressed
up to time tby the real SSM unless nRis on the order of t, which may be arbitrarily larger than nC.
This is in stark contrast to the fact that all mappings expressible by the real SSM can be precisely
expressed (up to any time) by the complex SSM whenever nC≥nR.
Given the prevalence of overparameterization in machine learning, one may question how conse-
quential the above separation (between the real and complex SSMs) is in practice. Indeed, in an
overparameterized regime where a given LTI mapping is to be approximated up to a given time t
andnR, nC≥t, universality implies that both the real and complex SSMs can precisely express the
mapping up to time t. Accordingly, in this overparameterized regime, it is a priori unclear whether
the complex SSM offers an advantage over the real SSM. Our second contribution shows that it does.
Specifically, we prove a surprising result by which, if the given mapping satisfies a mild condition,
then in order to approximately express the mapping up to time t, the real SSM must have dimension
or parameter magnitude exponential in t. This is in stark contrast to the complex SSM, which can
precisely express the given mapping up to time twith dimension and parameter magnitudes that are
at most linear in t. The aforementioned mild condition is satisfied by the canonical copy mapping,
by a basic oscillatory mapping, and with high probability by a random (generic) mapping. In such
important cases, practical learning of the given mapping necessitates using a complex SSM.
Our theory is corroborated by controlled experiments, demonstrating that complex parameterizations
for SSMs significantly improve performance. We also evaluate SSMs with selectivity —a new
architectural feature yielding state of the art performance [ 20,31,4,57]. Our experiments with
selectivity portray a more nuanced picture: complex parameterizations are beneficial for some tasks,
whereas for others, selectivity allows real parameterizations to achieve comparable (and in some
cases better) performance. These findings align with the mixed evidence reported in the literature.
Moreover, they suggest a potential extension of our theory that accounts for selectivity and may
elucidate this evidence, thereby fully delineating the benefits of complex parameterizations for SSMs.
2 Preliminaries
2.1 Notations
We use non-boldface lowercase letters for denoting scalars ( e.g.α∈R,c∈C,n∈N), boldface
lowercase letters for denoting vectors ( e.g.x∈Rn,v∈Cn), and non-boldface uppercase letters
for denoting matrices ( e.g.A∈Rn,m,B∈Cn,m). Series (finite or infinite) of scalars, vectors or
matrices are viewed as functions of time and denoted accordingly ( e.g.(x(t)∈Rn)t∈N,(A(t)∈
Cn,m)t=1,2,...,k). For series of scalars, we also use as notation boldface uppercase letters ( e.g.S=
(s(t)∈R)t∈N,I= (i(t)∈C)t=1,2,...,k). Given k∈Nand a series of scalars Swhose length is
greater than or equal to k, we use Skto denote the kth element of S, andS:kto denote the truncation
ofSto length k(i.e.the series comprising the first kelements of S), allowing ourselves to regard
this truncated series as a vector of dimension k. For k∈N∪ {∞} , we use [k]as shorthand for
the set {1,2, . . . , k }. Given a complex number c∈C, we denote its magnitude by |c| ∈R≥0, its
phase by arg(c)∈[0,2π), its real part by ℜ(c)∈R, and its imaginary part by ℑ(c)∈R(meaning
c=|c|exp(iarg(c)) =ℜ(c) +iℑ(c)). We let 0and1stand for vectors whose entries are all zeros
and all ones, respectively, with dimension to be inferred from context. The Hadamard (element-wise)
product, defined between vectors or matrices of the same size, and between scalar series of the same
length, is denoted by ⊙. The convolution operator, defined between two (finite or infinite) scalar
series, is denoted by ∗. Namely, given two scalar series S= (s(t))t∈[k],¯S= (¯s(t))t∈[¯k]of lengths
k,¯k∈N∪ {∞} respectively, S∗¯Sis the scalar series of length k+¯k−1whose mth element, for
m∈[k+¯k−1], is given byPmin{m,k}
t=max{m−¯k+1,1}s(t)¯s(m−t+ 1) .
2.2 Structured State Space Models
LetK=RorK=C. Astructured state space model (SSM)of dimension n∈Nis parameterized by
three matrices: A∈Kn,n, astate transition matrix , which adheres to a predefined structure ( e.g.is
2constrained to be diagonal); B∈Kn,1, aninput matrix ; andC∈K1,n, anoutput matrix .23Given the
values of A,BandC, the SSM realizes a mapping ϕn,(A,B,C ):RN→RNwhich receives as input a
real scalar series (u(t))t∈N, and produces as output a real scalar series (y(t))t∈Ndefined through the
following recursive formula:
x(t) =Ax(t−1) +Bu(t), y(t) =ℜ 
Cx(t)
, t∈N, (1)
where (x(t)∈Kn)t∈Nis a vector series of states , andx(0) = 0∈Kn. IfK=Rwe say that the
SSM is real, and if K=Cwe say that it is complex .4We refer to the SSM as stable if all eigenvalues
ofAhave magnitude strictly smaller than one; otherwise we refer to the SSM as unstable . For
convenience, we often identify an SSM with the triplet (A, B, C )holding its parameter matrices, and
regard the (single column) matrices BandC⊤as vectors.
Perhaps the most prominent form of structure imposed on SSMs is stable diagonality ,i.e.stability
combined with diagonality [ 22,23,41,37,20]. Accordingly, unless stated otherwise, we assume that
the state transition matrix Aof an SSM is diagonal and has entries with magnitude strictly smaller
than one.
2.3 Linear Time-Invariant Mappings
Letϕ:RN→RNbe a mapping from the space of (infinite) real scalar series to itself. We say that
ϕ(·)islinear if for all α∈RandS,¯S∈RNit holds that ϕ(αS+¯S) =αϕ(S) +ϕ(¯S). For every
k∈N, define the kstep delay δk:RN→RNto be the operator that adds kpreceding zeros to the
series it receives as input.5We say that the mapping ϕ(·)islinear time-invariant (LTI) if it is linear,
and it commutes with δk(·)(meaning ϕ(δk(·)) =δk(ϕ(·))) for every k∈N. It is well known [ 40]
that if ϕ(·)is LTI then it is given by ϕ(S) =S∗ϕ(I), where I:= (1 ,0,0, . . .)∈RNis the impulse
series, and ϕ(I)is referred to as the impulse response ofϕ(·). Conversely, for any R∈RN, the
mapping defined by S7→S∗Ris LTI, and its impulse response is R.
We will identify LTI mappings with their impulse responses. More specifically, for any k∈N,
we identify an LTI mapping up to time k, with the truncation of its impulse response to length k.
Accordingly, for any LTI mappings ϕ(·),¯ϕ(·)and any ϵ∈R≥0, we say that ¯ϕ(·)ϵ-approximates ϕ(·)
up to time kif∥ϕ(I):k−¯ϕ(I):k∥1≤ϵ. If the latter inequality holds with ϵ= 0, we also say that ¯ϕ(·)
matches ϕ(·)up to time k.
Let(A, B, C )be an SSM of dimension n∈N, realizing the mapping ϕn,(A,B,C ):RN→RN(see
Section 2.2). It is straightforward to see that ϕn,(A,B,C )(·)is LTI, and that its impulse response is
given by:
ϕn,(A,B,C )(I) = 
ℜ(CB),ℜ(CAB ),ℜ(CA2B), . . .
. (2)
For real and complex settings ( i.e.forK=RandK=C), we will study the extent to which
varying (A, B, C )as well as n, can lead ϕn,(A,B,C )(·)toϵ-approximate different LTI mappings up
to different times.
3 Theoretical Analysis
Throughout this section, we consider a real SSM (AR, BR, CR)of dimension nRrealizing the map-
pingϕnR,(AR,BR,CR)(·), and a complex SSM (AC, BC, CC)of dimension nCrealizing the mapping
ϕnC,(AC,BC,CC)(·)(see Section 2.2).
2Various SSMs include discretization [20,21,23,16], which amounts to replacing parameter matrices by
certain transformations that depend on an additional parameter ∆∈R>0(e.g., replacing Aby(I−∆/2·
A)−1(I+ ∆/2·A)[23,22]). With slight modifications, our main theoretical results apply to SSMs with
common discretizations—see Appendix A.1 for details.
3Some SSMs include an additional feedthrough parameter D∈K1,1[41]. With feedthrough, the expression
fory(t)in Equation (1) becomes ℜ(Cx(t) +Du(t)). Our theory essentially applies as is to SSMs with
feedthrough—see Appendix A.2 for details.
4It is possible to consider a hybrid setting where Ais allowed to be complex while BandCare restricted to
be real. This setting enjoys all provable benefits of the complex setting ( K=C)—see Appendix A.3 for details.
5That is, for any S= (s(t))t∈N∈RN,δk(S)∈RNis the series whose mth element, for m∈N, equals
s(m−k)ifm > k and0otherwise.
33.1 Universality
It is known (see, e.g., [14]) that the real SSM is universal , in the sense that it can precisely express
any LTI mapping up to any time twhen its dimension is equal to or greater than t. Trivially, this
implies the same for the complex SSM. Proposition 1 below formalizes these facts for completeness.
Proposition 1. Letϕ:RN→RNbe an arbitrary LTI mapping, and let t∈N. Then, the following
holds for both K=RandK=C. IfnK≥t, there exist assignments for (AK, BK, CK)with which
ϕnK,(AK,BK,CK)(·)matches ϕ(·)up to time t.6
Proof sketch (proof in Appendix D.1). Beginning with the real SSM ( K=R), the proof shows that
ϕnR,(AR,BR,CR)(·)matches ϕ(·)up to time tifV(AR)(C⊤
R⊙BR) =ϕ(I):t, where V(AR)is a Van-
dermonde matrix that has full rank when the diagonal entries of ARare distinct. Assigning ARthis
way, there must exist v∈RnRwith which V(AR)v=ϕ(I):t. Assigning BR=vandC⊤
R=1
concludes the proof for the real SSM. The complex SSM ( K=C) can be treated analogously.
3.2 Separation in Expressiveness
Proposition 2 and Theorem 1 below together establish that although both the real and complex SSMs
are universal (see Section 3.1), there is a strong separation between the two in terms of expressiveness.
Proposition 2 formalizes an obvious fact: all mappings expressible by the real SSM can be precisely
expressed (up to any time) by the complex SSM whenever nC≥nR(i.e., whenever the dimension of
the complex SSM is equal to or greater than the dimension of the real SSM). Theorem 1 proves a much
less obvious result: for any nC, there are various oscillatory mappings ( i.e.mappings with oscillatory
impulse responses) expressible by the complex SSM which cannot be approximately expressed up to
timetby the real SSM unless nRis on the order of t, which may be arbitrarily larger than nC.
Proposition 2. Consider an arbitrary assignment for (AR, BR, CR), and assume that nC≥nR.
Then, there exist assignments for (AC, BC, CC)with which ϕnC,(AC,BC,CC)(·) =ϕnR,(AR,BR,CR)(·).
Proof. It suffices to prove the sought after result for nC=nR, since one can effectively reduce the
dimension of the complex SSM by zeroing out entries of BC(orCC). Assuming that nC=nR, we
may assign to (AC, BC, CC)the values of (AR, BR, CR). Under this assignment ϕnC,(AC,BC,CC)(·) =
ϕnR,(AR,BR,CR)(·), as required.
Theorem 1. Lett∈Nandϵ∈R≥0. Assume without loss of generality that nC= 1,7in which
caseAC,BCandCCcan be regarded as scalars. Suppose |sin(arg( AC))| ≥0.2,|AC| ≥0.51/t
and|BC·CC| ≥1. Then, if ϕnR,(AR,BR,CR)(·)ϵ-approximates ϕnC,(AC,BC,CC)(·)up to time t, it must
be that nR≥ ⌊t/9⌋ −1−4ϵ.
Proof sketch (proof in Appendix D.2). The idea behind the proof is as follows. The complex SSM
realizes an oscillatory mapping, in the sense that elements 1,3, . . . , 2⌈t/2⌉−1of its impulse response
alternate Θ(t)times between being greater than or equal to 1/4, and being smaller than or equal
to−1/4. The real SSM on the other hand is limited in its ability to realize oscillations, insofar as
elements 1,3, . . . , 2⌈t/2⌉ −1of the impulse response it gives rise to are a linear combination of
decaying exponentials, and therefore can change sign at most O(nR)times. Combining these two
observations leads to the desired result.
3.3 Separation in Practical Learnability
Letϕ:RN→RNbe an LTI mapping with bounded impulse response, which we would like to
ϵ-approximate up to time tfor some ϵ∈R≥0andt∈N. Assume that the dimensions of the real
and complex SSMs are greater than or equal to t. By Proposition 1, both the real and complex
SSMs can express mappings that match ϕ(·)up to time t, and in particular that achieve the desired
approximation. The current subsection establishes that despite this parity in terms of expressiveness,
there is a strong separation between the real and complex SSMs in terms of practical learnability .
6It is possible to strengthen this result, e.g., by showing that the requirement nK≥tcan be replaced by
nK≥ ⌈(t+ 1)/2⌉whenK=C. We omit details, as the current form of the result suffices for our purposes.
7This does not limit generality since one can effectively reduce the dimension of the complex SSM by
zeroing out entries of BC(orCC).
4Section 3.3.1 proves that under a mild condition on ϕ(·), in order for the real SSM to achieve the
desired approximation, either its dimension or the magnitude of its parameters must be exponential in t.
Section 3.3.2 then explains that such exponentiality impedes practical learning via gradient descent.
Finally, Section 3.3.3 proves that in stark contrast to the real SSM, the complex SSM can achieve the
desired approximation with dimension and parameter magnitudes that are at most linear in t.
3.3.1 Real Parameterizations Suffer from Exponentiality
Definition 1 below formalizes the notion of forward difference for a real scalar series—a discrete
analogue of derivative for a differentiable real function. Our main theoretical result, Theorem 2, then
establishes that if forward differences associated with ϕ(I)—the impulse response of ϕ(·)—satisfy a
certain condition, then in order for the real SSM to express a mapping that ϵ-approximates ϕ(·)up to
timet, either the dimension of the real SSM nRor the magnitude of its parameters (BR, CR)must be
exponential in t. Roughly speaking, the aforementioned condition on forward differences associated
withϕ(I)is that there exists some d∈Θ(t)such that the dth forward difference of the restriction
ofϕ(I)to either odd or even elements has magnitude greater than 2dϵ. Perhaps surprisingly, this
condition is especially mild, as the magnitude of the dth forward difference of a real scalar series
typically scales exponentially with d. Several important cases where the condition is satisfied are
presented below.
Definition 1. LetSbe a real scalar series of length k∈N∪ {∞} . The forward difference ofS,
denoted S(1), is the scalar series of length k−1whose mth element, for m∈[k−1], is given by
Sm+1−Sm. Ford∈ {2,3, . . . , k −1}, thedth forward difference ofS, denoted S(d), is recursively
defined to be the forward difference of S(d−1).
Theorem 2. Suppose ϕnR,(AR,BR,CR)(·)ϵ-approximates ϕ(·)up to time t. Then:
nR∥CR⊤⊙BR∥∞≥ max
d,m∈N, d+m≤⌊t/2⌋
σ∈{odd,even}n
2d+2 min {d,m}
2−d(ϕ(I)|σ)(d)
m−ϵo
, (3)
where: ϕ(I)|oddandϕ(I)|even are the restrictions of the impulse response ϕ(I)to odd and even
elements, respectively; and (ϕ(I)|odd)(d)
mand(ϕ(I)|even)(d)
mstand for the mth element of the dth
forward difference of ϕ(I)|oddandϕ(I)|even, respectively.
Proof sketch (proof in Appendix D.3). The idea behind the proof is as follows. The restrictions of
the impulse response of ϕnR,(AR,BR,CR)(·)to odd and even elements— i.e.,ϕnR,(AR,BR,CR)(I)|oddand
ϕnR,(AR,BR,CR)(I)|even, respectively—are each a linear combination of nRdecaying exponentials,
where the coefficients of the linear combination have absolute value no greater than ∥CR⊤⊙BR∥∞.
Forward differences of decaying exponentials are exponentially small. Therefore, by linearity
of forward differences, requiring ϕnR,(AR,BR,CR)(I)|oddorϕnR,(AR,BR,CR)(I)|even to have a for-
ward difference that is not exponentially small implies an exponentially large lower bound on
nR∥CR⊤⊙BR∥∞. When ϕnR,(AR,BR,CR)(·)ϵ-approximates ϕ(·)up to time t, forward differences
ofϕnR,(AR,BR,CR)(I)|oddandϕnR,(AR,BR,CR)(I)|even are close to those of ϕ(I)|oddandϕ(I)|even,
respectively. We thus conclude that if ϕ(I)|oddorϕ(I)|even has a forward difference that is not
especially small, then nR∥CR⊤⊙BR∥∞must be exponentially large. This conclusion is formalized
via Equation (3), the sought after result.
Special cases. Theorem 2 implies that the real SSM suffers from exponentiality (namely, its
dimension nRor the magnitude of its parameters (BR, CR)must be exponential in tin order for
it to express a mapping that ϵ-approximates ϕ(·)up to time t) in various important cases. Indeed,
Corollaries 1 and 2 below respectively show that the real SSM suffers from exponentiality when ϕ(·)is
a canonical copy (delay) mapping, and with high probability when ϕ(·)is a random (generic) mapping.
In light of Theorem 1 (namely, of the inability of the real SSM to compactly approximate various
oscillatory mappings expressible by the complex SSM), it is natural to ask if the real SSM suffers from
exponentiality in cases where ϕ(·)is oscillatory, i.e.where its impulse response oscillates. Corollary 3
below shows that exponentiality indeed transpires in a case where ϕ(·)is a basic oscillatory mapping.
On the other hand, there are simple cases where ϕ(·)is oscillatory yet exponentiality does not take
5place, e.g.the case where the impulse response of ϕ(·)is(+1,−1,+1,−1, . . .).8Precise delineation
of the type of oscillations that lead to exponentiality is deferred to future work (see Section 6).
Corollary 1. Suppose t≥9andϕ(·) =δ⌊(t−1)/2⌋(·), where as defined in Section 2.3, δ⌊(t−1)/2⌋(·)
is the⌊(t−1)/2⌋step delay mapping. Assume also that ϵ≤1/ 
8√
t
. Then, if ϕnR,(AR,BR,CR)(·)
ϵ-approximates ϕ(·)up to time t, it must hold that:
nR∥C⊤
R⊙BR∥∞≥2t/2/ 
32√
t
. (4)
Proof sketch (proof in Appendix D.4). The proof computes forward differences associated with
δ⌊(t−1)/2⌋(I)(impulse response of δ⌊(t−1)/2⌋(·)), and plugs them into Theorem 2 (Equation (3)).
Corollary 2. Letα∈R>0, and let R∈RNbe generated by a random process where each element
ofRis independently drawn from a uniform distribution over the interval [−α, α]. Suppose that t≥8
and that ϕ(·)is the mapping whose impulse response is R(i.e.,ϕ(·)is defined by ϕ(S) =S∗R). Let
p∈(0,1), and assume that ϵ≤αp
p/t. Then, with probability at least 1−p, ifϕnR,(AR,BR,CR)(·)
ϵ-approximates ϕ(·)up to time t, it must hold that:
nR∥C⊤
R⊙BR∥∞≥2t/2α√p/ 
8√
t
. (5)
Proof sketch (proof in Appendix D.5). The proof derives lower bounds (holding with probability at le-
ast1−p) on forward differences associated with R, and plugs them into Theorem 2 (Equation (3)).
Corollary 3. Suppose that ϕ(I) = (+1 ,0,−1,0,+1,0,−1,0, . . .)andϵ≤0.5. Then, if
ϕnR,(AR,BR,CR)(·)ϵ-approximates ϕ(·)up to time t, it must hold that:
nR∥C⊤
R⊙BR∥∞≥23t/4−4. (6)
Proof sketch (proof in Appendix D.6). The proof computes forward differences associated with the
series (+1,0,−1,0,+1,0,−1,0, . . .), and plugs them into Theorem 2 (Equation (3)).
3.3.2 Exponentiality Impedes Practical Learning
For any value of tthat is not especially small, exponentiality in tfor the real SSM as put forth in
Section 3.3.1— i.e., exponentiality in tof the dimension of the real SSM nRor the magnitude of its
parameters (BR, CR)—impedes practical learning. This impediment is obvious in the case where nR
is exponential in t(in this case, it is impractical to even store the parameters of the real SSM, let alone
learn them). Appendix B treats the complementary case, i.e.it shows that learning is impractical when
the required values for the parameters (BR, CR)are exponential in t(this is deferred to an appendix
due to space constraints). The results of Section 3.3.1 therefore imply that the real SSM cannot
practically learn a mapping that ϵ-approximates ϕ(·)up to time tunder important choices of ϕ(·).
3.3.3 Complex Parameterizations Do Not Suffer from Exponentiality
Section 3.3.1 established that under a mild condition on ϕ(·), in order for the real SSM to express
a mapping that ϵ-approximates ϕ(·)up to time t, either the dimension of the real SSM nRor the
magnitude of its parameters (BR, CR)must be exponential in t. Proposition 3 below proves that in
stark contrast, for any choice of ϕ(·)(whose impulse response ϕ(I)is bounded), the complex SSM
can express mappings that match ϕ(·)up to time twith dimension nCand magnitude of parameters
(BC, CC)that are at most linear in t.
Proposition 3. For any choice of nCgreater than or equal to t, there exist assignments for
(AC, BC, CC)with which ϕnC,(AC,BC,CC)(·)matches ϕ(·)up to time t, and wherein: (i)∥BC∥2≤
2∥ϕ(I):t∥2; and (ii)∥C⊤
C∥2≤1.9
8To see that exponentiality does not take place when ϕ(I) = (+1 ,−1,+1,−1, . . .), note that in this case,
with any nR,ϕnR,(AR,BR,CR)(·)ϵ-approximates ϕ(·)up to time twhenever: CR⊤⊙BRholds one in its first
entry and zeros elsewhere; and ARholds min{0,(−1 +ϵ/t2)}in its first diagonal entry.
9It is possible to develop a variant of this result that only requires nC≥ ⌈(t+ 1)/2⌉. We omit details, as the
current form of the result suffices for our purposes.
6Proof sketch (proof in Appendix D.7). The proof employs the theory of discrete Fourier transform
(DFT ). It begins by assigning (scaled versions of) the tth roots of unity to the diagonal entries of AC.
Then, it uses the inverse DFT formula to derive assignments for BCandCCleading ϕnC,(AC,BC,CC)(·)
to match ϕ(·)up to time t. Finally, the proof applies Plancheral theorem to show that the derived
assignments for BCandCCsatisfy the desired criteria.
4 Experiments
This section presents controlled experiments corroborating our theory. Section 4.1 demonstrates that
complex parameterizations significantly improve performance of SSMs in the theoretically analyzed
setting. Section 4.2 shows that this improvement extends to a real-world setting beyond our theory.
Finally, Section 4.3 evaluates SSMs with selectivity —a new architectural feature yielding state of
the art performance [ 20,31,4,57]. The experiments with selectivity portray a nuanced picture:
complex parameterizations are beneficial for some tasks, whereas for others, selectivity allows real
parameterizations to achieve comparable (and in some cases better) performance. These findings align
with mixed evidence reported in the literature (see Section 1). Moreover, they suggest a potential
extension of our theory that accounts for selectivity and may elucidate this evidence, thereby fully
delineating the benefits of complex parameterizations for SSMs.
For conciseness, we defer some of the details behind our implementation to Appendix F.
Code for reproducing our experiments is available at https://github.com/edenlum/
SSMComplexParamBenefits .
4.1 Theoretically Analyzed Setting
To empirically demonstrate our theoretical findings, we trained the analyzed real and complex SSMs
(see Section 2.2) to approximate up to time tthe mapping ϕ(·)(see Section 2.3), with tvarying
and with the following choices for ϕ(·): the canonical copy (delay) mapping from Corollary 1; the
random (generic) mapping from Corollary 2; and the basic oscillatory mapping from Corollary 3.
Throughout, the dimension of the real or complex SSM ( nRornC, respectively) was set to at least t,
which, by universality (Section 3.1), implies that the SSM can express a mapping that precisely
matches ϕ(·)up to time t. Our theory (Section 3.3) establishes that despite this parity between the
real and complex SSMs in terms of expressiveness, there is a strong separation between the SSMs
in terms of practical learnability. In particular, with the above choices of ϕ(·), there are exponential
barriers that apply only to the real SSM, and prevent its training from being able to yield a mapping
that closely approximates ϕ(·)up to time t. Tables 1 and 2 present results obtained with the real and
complex SSMs, respectively. They confirm the predictions of our theory.
Table 1: In accordance with our theory, the analyzed real SSM (see Section 2.2) cannot practically learn to
closely approximate ϕ(·)up to time tunder important choices of ϕ(·), even when tis moderate. This table
reports the approximation error attained by the real SSM, i.e.the minimum ϵwith which a mapping learned
by the real SSM ϵ-approximates ϕ(·)up to time t(see Section 2.3), when t= 32 andϕ(·)varies over the
following possibilities: the canonical copy (delay) mapping from Corollary 1; the random (generic) mapping
from Corollary 2; and the basic oscillatory mapping from Corollary 3. Learning was implemented by applying
one of three possible gradient-based optimizers—Adam [ 29], AdamW [ 36] or RAdam [ 33]—to a loss as in
our theory (see Appendix C). For each choice of ϕ(·), reported approximation errors are normalized (scaled)
such that a value of one is attained by the trivial zero mapping. Each configuration was evaluated with five
random seeds, and its reported approximation error is the minimum (best) that was attained. The dimension of
the real SSM ( nR) was set to 1024 ; other choices of dimension led to qualitatively identical results. For further
implementation details see Appendix F.1.
Optimizer Approx. of Copy Approx. of Random Approx. of Oscillatory
Adam 0.767 0 .536 0 .812
AdamW 0.772 0 .541 0 .809
RAdam 0.767 0 .538 0 .811
4.2 Real-World Setting
To empirically demonstrate the benefits of complex parameterizations for SSMs in settings beyond our
theory, we evaluated the prominent S4 neural network architecture [ 21] on the real-world sequential
7Table 2: In contrast to the analyzed real SSM, and in alignment with our theory, the analyzed complex SSM
(see Section 2.2) can practically learn to closely approximate ϕ(·)up to time tunder important choices of ϕ(·)
and various choices of t. This table reports approximation errors attained by the complex SSM. It adheres to
the description of Table 1, with the following exceptions (all designed to stress the superiority of the complex
SSM over the real SSM): (i)only Adam optimizer was used; (ii)in addition to 32,talso took the values 64,128
and256;(iii)for each configuration, the reported approximation error is the maximum (worst) that was achieved
across the random seeds; and (iv)the dimension of the complex SSM ( nC) was set to t(higher dimensions led to
qualitatively identical results). For further implementation details see Appendix F.1.
t Approx. of Copy Approx. of Random Approx. of Oscillatory
32 1 .6×10−56.3×10−51.6×10−4
64 1 .7×10−51.6×10−53.7×10−4
128 4 .9×10−54.8×10−52.6×10−3
256 6 .8×10−47.6×10−41.7×10−3
CIFAR-10 dataset from the widely recognized Long Range Arena benchmark [ 52]. Our imple-
mentation is based on the official S4 repository,10where unless stated otherwise, hyperparameters
(pertaining to the neural network architecture and its training) were kept at their default values. A
single run with complex parameterization yielded a test accuracy of 89.10%, significantly higher than
the highest test accuracy of 78.27% attained with real parameterization across three random seeds.
Modifying the optimizer and initialization scheme with the real parameterization did not improve the
test accuracy—see Appendix F.2 for details. The overarching conclusion from our theory—namely,
that SSMs benefit from complex parameterizations—thus extends to this real-world setting.
4.3 Selectivity
A new architectural feature for SSMs that yields state of the art performance is selectivity [20,31,
4,57]. In its original form—proposed as part of the Mamba neural network architecture [ 20]—
selectivity amounts to replacing the parameters BandC(see Section 2.2), as well as an additional
discretization parameter ∆∈R>0,2by certain functions of the input (u(t))t∈N. We empirically
study the impact of complex parameterizations on SSMs with selectivity by evaluating a Mamba
neural network on two synthetic tasks regarded as canonical in the SSM literature [ 27,20]:(i) copy ,
which was shown by our theory (Section 3.3) and earlier experiments (Section 4.1) to pose difficulties
for real parameterizations in SSMs with no selectivity; and (ii) induction-head , which can be seen
as a generalization of copy in which the delay is input-specified (rather than being fixed). Our
implementation is based on a widely adopted Mamba repository easily amenable to modification.11
Unless stated otherwise, repository hyperparameters (pertaining to the neural network architecture
and its training) were kept at their default values. Further details concerning our implementation,
including detailed descriptions of the copy and induction-head tasks, can be found in Appendix F.3.
Our first experiment with the Mamba neural network compared real and complex parameterizations
for the underlying SSMs, with selectivity included. On the copy task, across three random seeds
for each configuration, the highest accuracy attained with the real parameterization was 80.17%,
whereas the lowest accuracy attained with the complex parameterization was 93.05%. This gap
in performance in favor of the complex parameterization aligns with our theoretical and empirical
findings for SSMs without selectivity. In stark contrast, on the induction-head task, there is no such
gap (in fact, there is a slight advantage to the real parameterization): across three random seeds
for each configuration, accuracies attained with the real parameterization ranged between 97.35%
and98.3%, whereas those attained with the complex parameterization ranged between 93.93% and
97.64%. These results align with mixed evidence reported in the SSM literature, by which complex
parameterizations are essential for strong performance on some tasks [ 22,41,22], whereas on others,
real parameterizations lead to comparable (and in some cases better) performance [37, 20].
To gain insight into the induction-head task not benefiting from the complex parameterization, we
conducted an ablation experiment with partial versions of selectivity ( i.e., where not all of the
parameters B,Cand∆were replaced by functions of the input). The results of this experiment,
reported in Table 3, reveal that when selectivity is fully or partially removed (more precisely, when
10https://github.com/state-spaces/s4 (Apache-2.0 license).
11https://github.com/alxndrTL/mamba.py (MIT license).
8Table 3: Ablation experiment demonstrating that real parameterizations can compare (favorably) to complex
parameterizations for SSMs with selectivity, but complex parameterizations become superior when selectivity
is fully or partially removed. This table reports test accuracies attained by a Mamba neural network [ 20] on a
synthetic induction-head task regarded as canonical in the SSM literature [ 27,20]. Evaluation included multiple
configurations for the SSMs underlying the neural network. Each configuration corresponds to either real or
complex parameterization, and to a specific partial version of selectivity— i.e., to a specific combination of
parameters that are selective (replaced by functions of the input), where the parameters that may be selective are:
the input matrix B; the output matrix C; and a discretization parameter ∆. For each configuration, the highest
and lowest accuracies attained across three random seeds are reported. Notice that when both BandCare
selective, the real parameterization compares (favorably) to the complex parameterization, whereas otherwise,
the complex parameterization is superior. For further implementation details see Appendix F.3.
Selective B Selective C Selective ∆ Real Accuracy (%) Complex Accuracy (%)
Yes Yes Yes 97.35to98.3 93 .93to97.64
Yes Yes No 97.9to98.62 90 .18to95.86
Yes No Yes 61.82to71.28 91 .93to96.77
Yes No No 49.91to52.5 58 .93to73.77
No Yes Yes 56.78to69.54 92 .52to96.91
No Yes No 41.01to43.89 57 .44to64.67
No No Yes 15.48to26.33 68 .54to79.71
No No No 23.86to29.62 37 .61to50.19
B,Cor both are not replaced by functions of the input), the complex parameterization regains its
advantage. This suggests that selectivity, which is not covered by our theory, may be the key factor
enabling real parameterizations to perform as well as complex parameterizations for SSMs on certain
tasks. In other words, selectivity may be the dominant factor behind the aforementioned evidence
in the SSM literature being mixed. In Section 7 we outline a potential extension of our theory that
accounts for selectivity and may elucidate this evidence, thereby fully delineating the benefits of
complex parameterizations for SSMs.
5 Related Work
SSMs are closely related to linear dynamical systems—a classic object of study in areas such as
systems theory [ 3] and control theory [ 51]. Although there exists extensive literature concerning
properties of real and complex linear dynamical systems [ 10,5,6,9], this literature does not readily
establish benefits of complex parameterizations for SSMs, primarily due to the following reasons:
(i)the output of a complex SSM is turned real by disregarding imaginary components (see Section 2.2),
therefore it differs from a complex linear dynamical system; and (ii)the structures typically imposed
on state transition matrices of SSMs ( e.g.stable diagonality; see Section 2.2) are generally uncommon
in the literature on linear dynamical systems.
SSMs can be viewed as a special case of recurrent neural networks [ 48], which received significant
theoretical attention [ 46,39,25,11,53]. In this context, several works focused specifically on
SSMs [ 42,30,24,2,28,58,12,32,55]. However, to our knowledge, the only prior work to formally
and explicitly treat benefits of complex parameterizations for SSMs is [ 42]. The treatment of [ 42]
(see Section 4.1 therein) can be viewed as a special case of ours. Indeed, [ 42] considered a task that,
using our notation (see Section 2.2), amounts to linearly reconstructing an input element u(t)from
the state x(t′)of an SSM, where t, t′∈N, t′≥t. This is equivalent to assigning the output matrix
of the SSM Csuch that the mapping realized by the SSM ϕn,(A,B,C )(·)is a canonical copy (delay)
mapping. Roughly speaking, [ 42] showed that this task requires linear operations with exponential
parameters if the SSM is real, whereas linear operations with moderate parameters suffice if the SSM
is complex. The same result follows from our Corollary 1 and Proposition 3. We stress that our theory
goes far beyond this result, for example in that it covers various mappings beyond copy, including a
random (generic) mapping—see Section 3.3 for details.
With regards to related empirical work, the literature includes several experimental comparisons
between real and complex parameterizations for SSMs [ 20,22,42]. Nonetheless, to our knowledge,
the controlled experiments we conducted (see Section 4) are reported herein for the first time.
96 Limitations
While this paper offers meaningful contributions regarding benefits of complex parameterizations for
SSMs, it is important to acknowledge several of its limitations. First, while we establish a separation
between real and complex parameterizations in terms of expressiveness (Section 3.2), we do not
quantify how prevalent this separation is, i.e., what proportion of the mappings expressible with
complex parameterizations cannot be compactly approximated with real parameterizations. Second,
while we prove that real parameterizations suffer from exponentiality that impedes practical learning
(Sections 3.3.1 and 3.3.2), and that complex parameterizations do not suffer from exponentiality
(Section 3.3.3), we do not formally establish practical learnability with complex parameterizations—
our evidence for this is purely empirical (Section 4). Third, our theory does not treat all fundamental
aspects of learning where real and complex parameterizations may differ, for example it does not
treat implicit bias of gradient-based optimization [ 49]. Fourth, our experiments (Section 4) include
only a single real-world setting. Finally, while we establish that a separation between real and
complex parameterizations in terms of practical learnability takes place in three important cases (see
Corollaries 1 to 3), these cases are likely far from being exhaustive. Indeed, Theorem 2 provides a
mild sufficient condition for separation in terms of practical learnability—namely, that certain forward
differences are not especially small—and we believe it is possible to apply analytical tools ( e.g., the
Nørlund-Rice integral [ 17]) for translating this condition into interpretable properties satisfied in
various important cases beyond those considered. Pursuing the latter direction, and more broadly,
addressing the aforementioned limitations, are regarded as important directions for future work.
7 Discussion
The extent to which complex parameterizations benefit SSMs is an important open question in
machine learning. Evidence in the literature is mixed: while some works demonstrate that complex
parameterizations are essential for strong performance, others show that in various settings, real pa-
rameterizations lead to comparable (and in some cases better) performance. It was conjectured by Gu
and Dao [20] that complex parameterizations are preferable for continuous data modalities ( e.g., audio,
video), whereas for discrete data modalities ( e.g., text, DNA) real parameterizations suffice.
Since a complex SSM includes twice as many parameters as a real SSM of the same dimension, a
priori, one might expect that a real SSM would benefit from becoming complex similarly to how
it would benefit from doubling its dimension. Our theory showed that this is not the case, and in
fact the former benefit far exceeds the latter. Indeed, we established separations between real and
complex SSMs, by which a real SSM can only match a complex SSM if either the dimension of the
real SSM or the number of iterations required for its training is exponentially large. Experiments
corroborated our theory, and suggested that selectivity—a new architectural feature yielding state
of the art performance—may be the dominant factor behind the aforementioned evidence in the
literature being mixed.
We now outline a potential extension of our theory that accounts for selectivity. Roughly speaking,
the separations we established between real and complex SSMs arise from a gap in their ability to
express oscillations, i.e., to express frequency components in their impulse response: while a complex
SSM can easily express any frequency, a real SSM struggles to do so. Adding selectivity to a real
SSM makes its parameters input-dependent, resulting in what can be viewed as an input-dependent
impulse response. We hypothesize that this dependence allows importing frequency components
from the input to the impulse response. If confirmed, this hypothesis would imply that when the input
data is sufficiently rich in frequency content, selectivity can endow real SSMs with all the benefits
we proved for complex SSMs. Such an outcome aligns with the conjecture of Gu and Dao [20]:
continuous data modalities often consist of only low frequencies, whereas discrete data modalities
typically have a “whiter spectrum,” i.e., a more uniform mix of frequencies [54].
We believe that extending our theory as described may elucidate the mixed evidence in the literature,
thereby fully delineating the benefits of complex parameterizations for SSMs.
Acknowledgments and Disclosure of Funding
We thank Itamar Zimerman for illuminating discussions. This work was supported the European
Research Council (ERC) grants HOLI 819080 and NN4C 101164614, a Google Research Scholar
Award, a Google Research Gift, Meta, the Yandex Initiative in Machine Learning, the Israel Science
10Foundation (ISF) grant 1780/21, the Tel Aviv University Center for AI and Data Science, the Adelis
Research Fund for Artificial Intelligence, Len Blavatnik and the Blavatnik Family Foundation, and
Amnon and Anat Shashua.
References
[1]Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay
Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu,
and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL
https://www.tensorflow.org/ . Software available from tensorflow.org.
[2] Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models, 2024.
[3]Kathleen T Alligood, Tim D Sauer, James A Yorke, and David Chillingworth. Chaos: an introduction to
dynamical systems. SIAM Review , 40(3):732–732, 1998.
[4]Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts
for state-space models, 2024.
[5]Panos J. Antsaklis. Linear systems / Panos J. Antsaklis, Anthony N. Michel. McGraw-Hill„ New York,
1997. ISBN 0070414335 (alk. paper).
[6] Masanao Aoki. State space modeling of time series . Springer Science & Business Media, 2013.
[7]Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on edge of stability
in deep learning, 2022.
[8]Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf,
and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba, 2024. URL
https://arxiv.org/abs/2406.14528 .
[9] Roger W Brockett. Finite dimensional linear systems . SIAM, 2015.
[10] John L Casti. Linear dynamical systems . Academic Press Professional, Inc., 1986.
[11] Minshuo Chen, Xingguo Li, and Tuo Zhao. On generalization bounds of a family of recurrent neural
networks, 2019. URL https://arxiv.org/abs/1910.12947 .
[12] Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, and Terry Lyons. Theoretical
foundations of deep selective state-space models, 2024. URL https://arxiv.org/abs/2402.19047 .
[13] Jeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, and Ameet Talwalkar. Gradient descent on
neural networks typically occurs at the edge of stability. ArXiv , abs/2103.00065, 2021. URL https:
//api.semanticscholar.org/CorpusID:232076011 .
[14] Edo Cohen-Karlik, Itamar Menuhin-Gruman, Raja Giryes, Nadav Cohen, and Amir Globerson. Learning
low dimensional state spaces with overparameterized recurrent neural nets. International Conference on
Learning Representations , 2023.
[15] Alexandru Damian, Eshaan Nichani, and Jason D. Lee. Self-stabilization: The implicit bias of gradient
descent at the edge of stability. ArXiv , abs/2209.15594, 2022. URL https://api.semanticscholar.
org/CorpusID:252668622 .
[16] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through
structured state space duality, 2024. URL https://arxiv.org/abs/2405.21060 .
[17] Philippe Flajolet and Robert Sedgewick. Mellin transforms and asymptotics: Finite differences and rice’s
integrals. Theoretical Computer Science , 144(1):101–124, 1995. ISSN 0304-3975. doi: https://doi.org/
10.1016/0304-3975(94)00281-M. URL https://www.sciencedirect.com/science/article/pii/
030439759400281M .
[18] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry
hungry hippos: Towards language modeling with state space models. In The Eleventh International
Conference on Learning Representations , 2022.
11[19] Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. It’s raw! audio generation with state-space
models, 2022.
[20] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.
[21] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state
spaces, 2022.
[22] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. On the parameterization and initialization of
diagonal state space models, 2022.
[23] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state
spaces, 2022.
[24] Joshua Hanson and Maxim Raginsky. Universal approximation of input-output maps by temporal convolu-
tional nets, 2019.
[25] Joshua Hanson and Maxim Raginsky. Universal simulation of stable dynamical systems by recurrent neural
nets. In Alexandre M. Bayen, Ali Jadbabaie, George Pappas, Pablo A. Parrilo, Benjamin Recht, Claire
Tomlin, and Melanie Zeilinger, editors, Proceedings of the 2nd Conference on Learning for Dynamics and
Control , volume 120 of Proceedings of Machine Learning Research , pages 384–392. PMLR, 10–11 Jun
2020. URL https://proceedings.mlr.press/v120/hanson20a.html .
[26] IEEE. 754-2019 - ieee standard for floating-point arithmetic, July 2019.
[27] Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat after me: Transformers
are better than state space models at copying, 2024. URL https://arxiv.org/abs/2402.01032 .
[28] Sekitoshi Kanai, Yasutoshi Ida, Kazuki Adachi, Mihiro Uchida, Tsukasa Yoshida, and Shin’ya Yamaguchi.
Evaluating time-series training dataset through lens of spectrum in deep state space models, 2024. URL
https://arxiv.org/abs/2408.16261 .
[29] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
[30] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and optimization theory for linear
continuous-time recurrent neural networks. Journal of Machine Learning Research , 23(42):1–85, 2022.
URL http://jmlr.org/papers/v23/21-0368.html .
[31] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked
Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman,
Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor
Zusman, and Yoav Shoham. Jamba: A hybrid transformer-mamba language model, 2024.
[32] Fusheng Liu and Qianxiao Li. From generalization analysis to optimization designs for state space models,
2024. URL https://arxiv.org/abs/2405.02670 .
[33] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.
On the variance of the adaptive learning rate and beyond, 2021. URL https://arxiv.org/abs/1908.
03265 .
[34] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan
Liu. Vmamba: Visual state space model, 2024.
[35] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017. URL
https://arxiv.org/abs/1608.03983 .
[36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https://arxiv.
org/abs/1711.05101 .
[37] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and
Luke Zettlemoyer. Mega: Moving average equipped gated attention, 2023.
[38] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke
Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with
unlimited context length, 2024. URL https://arxiv.org/abs/2404.08801 .
[39] Yuichi Nakamura and Masahiro Nakagawa. Approximation capability of continuous time recurrent neural
networks for non-autonomous dynamical systems. In International Conference on Artificial Neural
Networks , 2009. URL https://api.semanticscholar.org/CorpusID:36553223 .
12[40] Alan V . Oppenheim and Alan S. Willsky with S. Hamid Nawab. Signals and Systems . Prentice Hall, Upper
Saddle River, NJ, 2 edition, 1997. ISBN 978-0138147570.
[41] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and
Soham De. Resurrecting recurrent neural networks for long sequences, 2023.
[42] Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L. Smith. Universality of
linear recurrences followed by non-linear projections: Finite-width guarantees and benefits of complex
eigenvalues, 2024.
[43] PyTorch Contributors. Pytorch documentation, 2023. URL https://pytorch.org/docs/stable/ .
Accessed: 2024-09-22.
[44] Yuval Ran-Milo, Eden Lumbroso, Edo Cohen-Karlik, Raja Giryes, Amir Globerson, and Nadav Cohen.
Provable benefits of complex parameterizations for structured state space models, 2024. URL https:
//arxiv.org/abs/2410.14067 .
[45] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics , pages 400–407, 1951.
[46] Anton Maximilian Schäfer and Hans Georg Zimmermann. Recurrent neural networks are universal
approximators. In Stefanos D. Kollias, Andreas Stafylopatis, Włodzisław Duch, and Erkki Oja, editors,
Artificial Neural Networks – ICANN 2006 , pages 632–640, Berlin, Heidelberg, 2006. Springer Berlin
Heidelberg. ISBN 978-3-540-38627-8.
[47] Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and V olodymyr Kuleshov. Caduceus:
Bi-directional equivariant long-range dna sequence modeling, 2024. URL https://arxiv.org/abs/
2403.03234 .
[48] Alex Sherstinsky. Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm)
network. Physica D: Nonlinear Phenomena , 404:132306, March 2020. ISSN 0167-2789. doi: 10.1016/j.
physd.2019.132306. URL http://dx.doi.org/10.1016/j.physd.2019.132306 .
[49] Yonatan Slutzky, Yotam Alexander, Noam Razin, and Nadav Cohen. The implicit bias of structured state
space models can be poisoned with clean labels. arxiv preprint , 2024.
[50] Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for
sequence modeling, 2023.
[51] Eduardo D Sontag. Mathematical control theory: deterministic finite dimensional systems , volume 6.
Springer Science & Business Media, 2013.
[52] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers, 2020.
URL https://arxiv.org/abs/2011.04006 .
[53] Zhuozhuo Tu, Fengxiang He, and Dacheng Tao. Understanding generalization in recurrent neural networks.
InInternational Conference on Learning Representations , 2020. URL https://api.semanticscholar.
org/CorpusID:214346647 .
[54] Martin Vetterli, Jelena Kova ˇcevi´c, and Vivek K Goyal. Foundations of Signal Processing . Cambridge
University Press, 2014.
[55] Shida Wang and Beichen Xue. State-space models with layer-wise nonlinearity are universal approximators
with exponential decaying memory, 2023. URL https://arxiv.org/abs/2309.13414 .
[56] Bernard Widrow and István Kollár. Quantization Noise: Roundoff Error in Digital Computation, Signal
Processing, Control, and Communications . Cambridge University Press, 2008.
[57] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision
mamba: Efficient visual representation learning with bidirectional state space model, 2024.
[58] Nicolas Zucchet and Antonio Orvieto. Recurrent neural networks: vanishing and exploding gradients are
not the end of the story, 2024. URL https://arxiv.org/abs/2405.21064 .
13A Extensions of Theoretical Analysis
A.1 Discretization
Various SSMs incorporate discretization [20,21,23,16], which involves replacing parameter ma-
trices with specific transformations that depend on an additional parameter ∆∈R>0. With slight
modifications, our main theoretical results, Theorem 1 and Theorem 2, apply directly to SSMs with
common discretizations. Below, we exemplify this for one of the most common discretizations. Other
discretizations can be treated similarly.
One of the most common discretizations is the bilinear discretization [ 21,22], in which an SSM
(ĎAK,ĎBK,ĎCK)is parameterized in the following way:
ĎAK= (I−∆/2·AK)−1(I+ ∆/2·AK),ĎBK= (I−∆/2·AK)−1∆BK,ĎCK=CK,
where ∆∈R>0, the matrices AK, BKandCKshare the same dimensions and are defined over the
same fields as ĎAK,ĎBKandĎCK, respectively, and the real parts of the diagonal entries of AKare
negative. Since the set of functions expressible by a real or complex SSM remains unchanged under
this discretization, Theorem 1 holds without modification. Furthermore, Theorem 2 applies to this
discretization if the left-hand side of Equation (3) is replaced by nR∆∥CR⊤⊙BR∥∞. This follows
directly from Lemma 13, which shows that for any i∈[nR], we have |∆ (BR)i| ≥ | ĎBR
i|. The
latter inequality justifies replacing ĎBRwith∆BRon the left-hand side of Equation (3), leading to the
desired result.
A.2 Feedthrough
Some SSMs include an additional feedthrough parameter D∈K1,1[41]. With feedthrough, the
expression for y(t)in Equation (1) becomes ℜ(Cx(t) +Du(t)). Our theory essentially applies as is
to SSMs with feedthrough, as explained below.
Obviously, an SSM of dimension nwithout a feedthrough parameter can be emulated by an SSM of
dimension nwith a feedthrough parameter Dby setting D= 0. Conversely, an SSM (A, B, C )of
dimension nwith a feedthrough parameter Dcan be emulated by an SSM (A′, B′, C′)of dimension
n+ 1without a feedthrough parameter. This is achieved, for example, by setting the diagonal entries
ofA′to be those of Afollowed by zero, the entries of B′to be those of Bfollowed by one, and the
entries of C′to be those of Cfollowed by D. For any result where an SSM without feedthrough is
assumed, a corresponding result for an SSM with feedthrough holds. Such a result can be attained by
either emulating an SSM with feedthrough by an SSM without (while increasing dimension from nto
n+1) or by emulating an SSM without feedthrough by an SSM with feedthrough, (while maintaining
the dimension) depending on the context.
A.3 Complex Awith Real BandC
It is possible to consider hybrid SSMs where Ais allowed to be complex while BandCare restricted
to be real. Below we show that such hybrid SSMs enjoy all provable benefits of complex SSMs ( i.e.,
of SSMs where A, B andCcan all be complex).
There are four results concerning complex SSMs: Proposition 1, Proposition 2, Theorem 1 and
Proposition 3. The first three results can be trivially extended to the setting where only Ais complex,
as their proofs do not assume complex BorC. An extension of Proposition 3 is also straightforward.
Indeed, the proof of Proposition 3 utilizes the fact that for a complex SSM (AC, BC, CC)of dimension
nC, the impulse response can be any linear combination of nCarbitrary decaying sine and cosine
waves, which, by Fourier theory, can approximate any sequence of length nC. If only ACis allowed
to be complex, the impulse response can consist of any linear combination of nCarbitrary decaying
cosine waves, which can still represent any sequence of length nCvia the discrete cosine transform.
Thus, the setting remains fundamentally similar, allowing for the extension of Proposition 3 to hybrid
SSMs.
B Exponentiality Impedes Practical Learning
In this appendix, we show that learning the parameters of the real SSM, i.e.(AR, BR, CR), is
impractical when the required values for (BR, CR)are exponential in t. We account for two aspects of
this impracticality: the number of iterations required by gradient descent; and the precision (number
14of bits) required for representing the parameters. Note that we will not preclude the possibility of
overcoming the impracticality through development of new techniques ( e.g.new parameterizations
forBRandCR). We believe our account herein may assist in such developments.
Exponential number of iterations. For the sake of illustration, suppose that training the real SSM
to realize a mapping ϕnR,(AR,BR,CR)(·)thatϵ-approximates ϕ(·)up to time t, is implemented via
gradient descent over a loss function ℓ(·)that measures the squared error of the output at time t,
where input elements are drawn independently from the standard normal distribution:
ℓ(AR, BR, CR) :=EU∈RN,U1,U2,...i.i.d.∼ N(0,1)
(ϕnR,(AR,BR,CR)(U)t−ϕ(U)t)2
. (7)
By a simple derivation (see Appendix C) ℓ(AR, BR, CR) =∥ϕnR,(AR,BR,CR)(I):t−ϕ(I):t∥2
2, therefore
sufficient minimization of the loss ℓ(·)(namely, minimization of ℓ(·)to or below the value ϵ2/t)
indeed guarantees that ϕnR,(AR,BR,CR)(·)ϵ-approximates ϕ(·)up to time t. Assume the following
regularity conditions on gradient descent: (i)the step sizes (learning rates) for all iterations are upper
bounded by a constant (independent of t);(ii)the step size for each iteration is stable , in the sense
that it is upper bounded by 2/λmax, where λmax represents the maximum eigenvalue of the Hessian
ofℓ(·)at the respective iteration [ 13,7,15]; and (iii)the values of ℓ(·)throughout all iterations are
upper bounded by a constant (independent of t) times the value of ℓ(·)at initialization. Proposition 4
below establishes that under said regularity conditions, during gradient descent, the magnitude of
the parameters (BR, CR)grows at most linearly in the number of iterations. Accordingly, if the
required values for (BR, CR)are exponential in tand the initialization of (BR, CR)is not, attaining
the required values necessitates an exponential (in t) number of iterations.
Proposition 4. Consider an application of gradient descent to the loss ℓ(·)in Equation (7):
 
A(i)
R, B(i)
R, C(i)
R
= 
A(i−1)
R, B(i−1)
R, C(i−1)
R
−η(i)∇ℓ 
A(i−1)
R, B(i−1)
R, C(i−1)
R
, i∈N,
where 
A(0)
R, B(0)
R, C(0)
R
is a chosen initialization, and η(i)∈R>0represents the step size selected
for iteration i∈N. Assume ∃c1∈R>0such that ∀i∈N:η(i)≤c1. Assume also:
∀i∈N:η(i)≤2.
max
λmax 
∇2ℓ 
A(i−1)
R, B(i−1)
R, C(i−1)
R
,0	
,
where λmax(M), for a symmetric matrix M, is the maximum eigenvalue of M. Finally, assume:
∃c2∈R>0such that ∀i∈N:ℓ 
A(i)
R, B(i)
R, C(i)
R
≤c2·ℓ 
A(0)
R, B(0)
R, C(0)
R
.
Then:
∀i∈N: maxnB(i)
R
∞,(C(i)
R)⊤
∞o
≤maxnB(0)
R
∞,(C(0)
R)⊤
∞o
+i· 
4c1c2t·ℓ 
A(0)
R, B(0)
R, C(0)
R0.5.
Proof sketch (proof in Appendix D.8). Taking into account the form of ℓ(·)(see Appendix C) and the
assumptions made, the proof shows that at each iteration of gradient descent, each entry in BRorCR
changes by at most (4c1c2t·ℓ(A(0)
R, B(0)
R, C(0)
R))0.5. This readily leads to the desired result.
Prohibitive precision. With conventional floating-point representation, a real number ρis rep-
resented as s·k·2m, where: s∈ {− 1,1}(the sign) is represented by one bit; k∈N∪ {0}(the
significand) is represented by bk∈Nbits; and m∈Z(the exponent) is represented by bm∈N
bits. The precision of the floating-point representation is the total number of bits used for s,k
andm,i.e.it is1 +bk+bm. For example, with the widespread IEEE 754 standard [ 26]: single
precision corresponds to 32bits with bk= 23, bm= 8; and double precision corresponds to 64bits
withbk= 52, bm= 11 . It is customary to model quantization error as an additive random vari-
able uniformly distributed over the interval [−ξ/2, ξ/2], where ξ∈R>0is the quantization step
size [ 56]. With the floating-point representation of ρ, the best (lowest) achievable quantization step
size is on the order of |ρ| ·2−bk, thus we may model quantization error as a multiplicative random
variable uniformly distributed over [1−q/2,1 +q/2], where q= Θ(2−bk). Definition 2 below
employs this model to formalize the notion of robustness to quantization for values of (AR, BR, CR)
with which ϕnR,(AR,BR,CR)(·)ϵ-approximates ϕ(·)up to time t. Specifically, Definition 2 defines
robustness to q-quantization for such values to be the probability that ϕnR,(AR,BR,CR)(·)continues
15toϵ-approximate ϕ(·)up to time t, when the values are multiplied by random variables uniformly
distributed over [1−q/2,1 +q/2]. Proposition 5 below then proves that robustness to q-quantization
is (at most) inversely proportional to qtimes the magnitude of (BR, CR). Recalling that q= Θ(2−bk),
we conclude that if the values of (BR, CR)are exponential in t, a non-negligible robustness to quanti-
zation necessitates having bk= Ω(t), meaning a floating-point precision that scales (at least) linearly
int. The latter requirement is prohibitive, since virtually all computing systems implementing neural
networks entail fixed options for floating-point precision (typically 16,32and64[1,43]), all much
smaller than what the time tcan be (tens of thousands or more [20, 47, 19, 8])
Definition 2. Letq∈[0,1], and let QAR,QBRandQCRbe random matrices of the same sizes
asAR,BRandCR, respectively, wherein each entry is independently drawn from the uniform
distribution over [1−q/2,1 +q/2]. Let (A∗
R, B∗
R, C∗
R)be values for (AR, BR, CR)with which
ϕnR,(AR,BR,CR)(·)ϵ-approximates ϕ(·)up to time t. The robustness to q-quantization of(A∗
R, B∗
R, C∗
R)
is the probability that ϕnR,(AR,BR,CR)(·)ϵ-approximates ϕ(·)up to time twhen (AR, BR, CR)take
the values (A∗
R⊙QAR, B∗
R⊙QBR, C∗
R⊙QCR).
Proposition 5. Suppose that ϕnR,(AR,BR,CR)(·)ϵ-approximates ϕ(·)up to time t. Then, for any
q∈[0,1], the robustness to q-quantization of the values held by (AR, BR, CR)is at most:
2ϵ 
q∥C⊤
R⊙BR∥∞
.
Proof sketch (proof in Appendix D.9). Denote by (A∗
R, B∗
R, C∗
R)the values held by (AR, BR, CR),
and let QAR,QBRandQCRbe random matrices as in Definition 2. When (AR, BR, CR)take the
values (A∗
R⊙QAR, B∗
R⊙QBR, C∗
R⊙QCR), the first entry of the impulse response of ϕnR,(AR,BR,CR)(·)
equals (C∗
R⊙QCR)(B∗
R⊙QBR). It suffices to restrict attention to this first entry, and show that
|(C∗
R⊙QCR)(B∗
R⊙QBR)−ϕ(I)1| ≤ϵwith probability at most 2ϵ/(q∥C⊤
R⊙BR∥∞). The proof
establishes the latter anti-concentration result.
C Loss Functions
Consider a loss function as in our theory (see Equation (7)), applied to both real and complex SSMs:
ℓ(AK, BK, CK) =EU∈RN,U1,U2,...i.i.d.∼ N(0,1)h 
ϕnK,(AK,BK,CK)(U)t−ϕ(U)t2i
,
where K=RorK=C. Below we prove that:
ℓ(AK, BK, CK) =ϕnK,(AK,BK,CK)(I):t−ϕ(I):t2
2.
Indeed, as discussed in Section 2.3, for any U∈RN:
ϕnK,(AK,BK,CK)(U)t= 
U∗ϕnK,(AK,BK,CK)(I)
t, ϕ(U)t= (U∗ϕ(I))t.
Thus:
ϕnK,(AK,BK,CK)(U)t−ϕ(U)t=tX
j=1Uj 
ϕnK,(AK,BK,CK)(I)t−j+1−ϕ(I)t−j+1
.
It holds that:
ℓ(AR, BR, CR) =EU∈RN,U1,U2,...i.i.d.∼ N(0,1)

tX
j=1Uj 
ϕnK,(AK,BK,CK)(I)t−j+1−ϕ(I)t−j+1
2

=tX
j=1 
ϕnK,(AK,BK,CK)(I)t−j+1−ϕ(I)t−j+12EUj∼N(0,1)
U2
j
=ϕnK,(AK,BK,CK)(I):t−ϕ(I):t2
2,
as required.
16D Deferred Proofs
D.1 Proof of Proposition 1
Beginning with the real SSM ( K=R), assume that nR≥t. We would like to show that there exist
assignments for (AR, BR, CR)with which:
ϕ(I):t= (CRBR, CRARBR, CRA2
RBR, . . . , C RAt−1
RBR),
where ϕ(I):tstands for the first telements of the impulse response of ϕ(·). Regarding ϕ(I):t,BR
andC⊤
Ras (column) vectors, we may write the sought after equality as V(AR)(C⊤
R⊙BR) =
ϕ(I):t, where V(AR)∈Rt,nRis a matrix whose (i, j)th entry holds the jth diagonal entry of AR
exponentiated by i−1. Notice that V(AR)is a Vandermonde matrix comprising powers of the
diagonal entries of AR. Any assignment for ARwith distinct diagonal entries leads the Vandermonde
matrix to have full rank, which in turn means that there exists a vector v∈RnRsatisfying V(AR)v=
ϕ(I):t. Assigning BR=vandC⊤
R=1concludes the proof for the real SSM. The complex
SSM ( K=C) can be treated analogously, while assigning real values to AC,BCandCCin
order to allow disregarding the fact that only real parts of outputs are taken ( i.e., disregarding the
operator ℜ(·)in Equation (2)).
D.2 Proof of Theorem 1
To facilitate the proof of the theorem, we first outline essential definitions and lemmas to quanitify a
lower bound for the number of times the impulse response alternates between being greater than or
equal to 1/4, and being smaller than or equal to −1/4. Building on this groundwork, we then unpack
the main theorem’s proof in Appendix D.2.2.
D.2.1 Setup for the proof
Definition 3. For an angle θinRwe will say that θis in the mostly-positive region if
θmod 2 π∈h
0,π
3i
∪5π
3,2π
and we say that θis in the mostly-negative region if
θmod 2 π∈2π
3,4π
3
Remark 1. Under this definition, a non-zero complex number xhas that its argument is in the
mostly-positive region if and only if R(x)≥|x|
2. Conversely, xhas that its argument is in the
mostly-negative region if and only if R(x)≤ −|x|
2.
Definition 4. We say that a set is balanced if at least a sixth of its elements are in the mostly-positive
region and at least a sixth of its elements are in the mostly-negative region.
Definition 5. Thebalancing number of a complex number xwith argument θis defined as the
minimum p∈Nsuch that for any non-zero real number bthe set {b+θ, b+ 2θ, . . . , b +pθ}is
balanced. If no such pexists, we say that the balancing number of xis infinity. We denote the
balancing number of xbyβ(x).
We will begin by establishing an upper bound on β(x)forxin a specific region in the complex plane.
Subsequently, we generalize this result to give an upper bound of the balancing number for almost all
complex numbers.
Lemma 1. For any natural number p≥4, if a non-zero complex number xhas an argument θsuch
thatθis in[2π
p,2π
3], then β(x)≤p. Moreover, for any natural number p≥9, ifxhas an argument θ
such that θis in[π+2π
p−1,4π
3]thenβ(x)≤p.
Proof. Since the mostly-positive and mostly-negative regions are defined by looking at the angle of a
complex number mod 2π, to prove that β(x)≤p, it is enough to show there exists some number
k≤psuch that for any real bthe following set
{(b+θ) mod 2 π,(b+ 2·θ) mod 2 π, . . . , (b+k·θ) mod 2 π}
is balanced.
Indeed, let there be some real b.
171.Ifθis in[2π
p,2π
3]andp≥4, Letkbe the natural number such that 2π < θk ≤2π+θ. It
follows that 4≤k≤pand that θ≤2π
k−1. By Lemma 5 we have that the set
{(b+θ) mod 2 π,(b+ 2·θ) mod 2 π, . . . , (b+k·θ) mod 2 π}
has at least a single point in any continuous interval (in mod 2π) of length θ. As such, any
continuous interval mod 2πof length2π
3must contain at least
2π
3
θ
≥$
2π
3
π
k−1%
=k−1
3
≥k
6
points, where the last inequality is true for any k≥4. Since the mostly-positive and
mostly-negative regions are continuous intervals (mod 2π) of length2π
3, we have that the set
{(b+θ) mod 2 π,(b+ 2·θ) mod 2 π, . . . , (b+k·θ) mod 2 π}
has at least a sixth of its points in the mostly-positive and mostly-negative regions, and is
therefore balanced, as needed.
2.Ifθis in[π
2+2π
p−1,4π
3]andp≥9, then 2θmod πis in the intervalh
2π
p−1
2,2π
3i
which is
contained in the interval
2π
⌊p
2⌋,2π
3
. Therefore, by the previous section, there exists some
k≤p
2
such that the following set:
{b+ 2θmod 2 π, . . . , b +k·2θmod 2 π}
and the set
{(b−θ) + 2θmod 2 π, . . . , (b−θ) +k·2θmod 2 π}
are both balanced. Overall, their union, the set
{(b+θ) mod 2 π,(b+ 2·θ) mod 2 π, . . . , (b+ 2k·θ) mod 2 π}
is also balanced, showing that the β(x)≤2k≤pas needed.
Lemma 2. For any natrual numbers r, θthe following holds:
β 
er+iθ
=β 
er−iθ
Proof. For any rational numbers θandb, and for any natural number p, we will define the set Sθ,b,p
in the following way:
Sθ,b,p={(b+θ) mod 2 π,(b+ 2·θ) mod 2 π, . . . , (b+p·θ) mod 2 π}
We will also define the set Sθ,pas
Sθ,p={Sθ,b,p|b∈C}.
Given this notation, we have that β 
er−iθ
=pif and only if pis the minimum natural number such
that every set in Sθ,pis balanced. For any rational numbers θandband for any natural number p, we
have
S−θ,−b,p=−Sθ,b,p
(Where minus is taken element-wise mod 2π). This immediately gives:
S−θ,p=−Sθ,p.
Since the mostly-positive and mostly-negative regions are invariant to taking a minus mod 2π, we
have that all the sets in Sθ,pare balanced if and only if so are all the sets of S−θ,p. This directly
implies that β 
er+iθ
=β 
er−iθ
as needed.
Corollary 4. Ifx∈Chas that arg(x) mod π∈[2π
p−1, π−2π
p−1]for some p≥9thenβ(x)≤p
18Proof. Denote θ≡arg(x). From Lemma 1 if the angle θlies within the interval
I=2π
p,2π
3
∪
π+2π
p−1,4π
3
thenβ(x)≤p. Additionally, Lemma 2 indicates that the same holds if θis in the interval −I(where
operations are element-wise mod 2π). Hence, we have that if θis contained in interval I′, where
I′=I∪(−I)
=2π
p,2π
3
∪
π+2π
p−1,4π
3
∪4π
3,2π−2π
p
∪2π
3, π−2π
p−1
=2π
p, π−2π
p−1
∪
π+2π
p−1,2π−2π
p
thenβ(x)≤p. Overall, this means that if θhas that
θmod π∈2π
p−1, π−2π
p−1
thenβ(x)≤p, which gives the Corollary.
D.2.2 The Proof
Letϵ >0be any positive number. For any sequence X={X1, X2, . . . , X t}, letX|1denote the
subsequence consisting of elements at odd indices, i.e., X|1={X1, X3, . . .}.
First, we notice that from the assumption that |sin(arg( AC))| ≥0.2we have that arg(AC)∈π
16, π−π
16
, and so
arg(A2
C) mod π= 2 arg( AC) mod π∈2·hπ
16, π−π
16i
mod π=2π
9−1, π−2π
9−1.
This implies, by Corollary 4, that the balancing number of A2
Cis less than 9. Now, if ϕnR,(AR,BR,CR)(·)
ϵ-approximates ϕnC,(AC,BC,CC)(·)up to time t, then, by definition,
∥ϕnR,(AR,BR,CR)(I):t−ϕnC,(AC,BC,CC)(I):t∥1≤ϵ.
This implies
∥ϕnR,(AR,BR,CR)(I):t|1−ϕnC,(AC,BC,CC)(I):t|1∥1≤ϵ.
Now, we have that 
ϕ1,(AC,BC,CC)(I):t|1
k= 
ϕ1,(AC,BC,CC)(I):t
2k−1
=ℜ 
BCCCA2k−2
C
and because the balancing of A2
Cis at most 9, this implies that there exists a subsequence of
1,2, . . . ,⌊t/2⌋with increasing indices kjof length at leastt
9
−1such that if jis even then
BCCC 
A2
Ckj−1is in the mostly-positive region, which implies
ℜ
BCCC 
A2
Ckj−1
≥ |BCCC| 
A2
Ckj−1
2≥1
4,
where the first inequality is due to the definition of the mostly-positive region and the second is
implied from the assumption that |BC·CC| ≥1and|AC| ≥0.51/t. Conversely, if jis odd it is in
the mostly-negative region, which implies
ℜ
BCCC 
A2
Ckj−1
≤ −|BCCC| 
A2
Ckj−1
2≤ −1
4.
Next, we will show that the impulse response of any real system, when restricted to even indices, can
change its sign at most nR−1times. This will prove the theorem since
1
4t
9
−nR−1
≤ ∥ϕnR,(AR,BR,CR)(I):t|1−ϕnC,(AC,BC,CC)(I):t|1∥1≤ϵ.
19The first inequality results from the fact that there would be at leastj
t
pk
−nR−1indices where
1
4≤ 
ϕ1,(AC,BC,CC)(I):t|1
kandϕnR,(AR,BR,CR)(I):t|1andϕ1,(AC,BC,CC)(I):t|1disagree on sign,
which yields the theorem.
Indeed, let us upper bound the number of sign changes of the real impulse response when restricted
to odd indices. We have that
 
ϕnR,(AR,BR,CR)(I):t|1
k= 
ϕnR,(AR,BR,CR)(I):t
2k−1
=ℜ 
CRA2k−2
RBR
=CRA2k−2
RBR
=nRX
j=1(BR)j(CR)j(AR)2k−2
j,j.
By the mean value theorem, the number of times ϕnR,(AR,BR,CR)(I):t|1changes signs is bounded by
the number of zeros of the following continuous function:
f(x) =nRX
j=1(BR)j(CR)j
(AR)2
j,jx
.
This function is a linear combination of nRexponential functions with a positive base, and so
by Lemma 4, it has at most nR−1zeros, as needed.
D.3 Proof of Theorem 2
Letd, m be natural numbers such that d+m≤ ⌊t/2⌋, and let σbe in{odd,even}. We will show
that
nR∥C⊤
R⊙BR∥∞≥2d+2 min {d,m}
2−d(ϕ(I)|σ)(d)
m−ϵ
,
thus proving the theorem.
LetϕnR,(AR,BR,CR)(·)be a real system that ϵ-approximates ϕ(·)up to time t. For convenience, we
will denote ϕnR,(AR,BR,CR)(I):tbyYandϕ(I):tbyT. Then, by definition, we have that
∥T−Y∥1≤ϵ,
which implies that
|T|σ−Y|σ|∞≤ϵ.
Lemma 9 guarantees that there exist some parameters
˜AR,˜BR,˜CR
where ˜ARis non-negative and
the following two things hold:
∥CT
R⊙BR∥∞≥ ∥˜CRT⊙˜BR∥∞
Y|σ=ϕnR,(˜AR,˜BR,˜CR)(I):t.
Because ˜ARis non-negative, Y|σcan be viewed as a linear combination of decaying exponentials
with positive coefficients. Since the forward difference is linear, Lemma 6 ensures that the absolute
value of the dth forward difference of Y|σat index mis upper bounded by
m
d+mmd
d+md
·nR∥˜CRT⊙˜BR∥∞.
Using Lemma 12, we get
(Y|σ)(d)
m≤nR∥˜CRT⊙˜BR∥∞
22 min( m,d).
20Lemma 8 implies that
∥T|σ−Y|σ∥∞≥T|(d)
σ−Y|(d)
σ
∞
2d
≥(T|σ)(d)
m−(Y|σ)(d)
m
2d
≥(T|σ)(d)
m
2d−nR∥˜CRT⊙˜BR∥∞
2d+2 min( m,d).
Plugging in |T|σ−Y|σ|∞≤ϵandT|σ, we get
ϵ≥2−d(ϕ(I)|σ)(d)
m−nR∥˜CRT⊙˜BR∥∞
2d+2 min( m,d).
Reordering this equation yields the desired result.
D.4 Proof of Corollary 1
For simplicity of notation, we will denotet−1
2
bykand assume without loss of generality that kis
odd. Theorem 2 indicates that
nR∥C⊤
R⊙BR∥∞≥max d,m∈N, d+m≤⌊t/2⌋, σ∈{odd,even}n
2d+2 min {d,m}
2−d(ϕ(I)|σ)(d)
m−ϵo
Substituting d=k+1
2,m=k+1
4
,σ=even, and ϵ≤1
8√
tyields:
n∥CT⊙B∥∞≥2k−1
(ϕ(I)|even)(k+1
2)
⌊k+1
4⌋
2k+1
2−1
8√
t
(8)
In the subsequent analysis, we will demonstrate that(ϕ(I)|even)(k+1
2)
⌊k+1
4⌋≥2k+1
2
4√
t.
Once established, the remainder of the proof will naturally follow since plugging this forthcoming
result into Equation (8) will ensure that:
n∥CT⊙B∥∞≥2k−1
8√
t=2k
16√
t=2⌊(t−1)/2⌋
16√
t≥2(t/2)−1
16√
t=2(t/2)
32√
t
satisfying the requirements for our conclusion. Indeed, we have that for any j≤k
(ϕ(I)|even)j= (ϕ(I))2j=δk(I)2j= 1(2j=k+ 1) = 1
j=k+ 1
2
.
Lemma 7 implies that
(ϕ(I)|even)(k+1
2)
⌊k+1
4⌋=k+1
2X
j=0(ϕ(I)|even)⌊k+1
4⌋+j(−1)k+1
2−jk+1
2
j
=k+1
2X
j=01
j+k+ 1
4
=k+ 1
2
(−1)k+1
2−jk+1
2
j
= (−1)⌊k+1
4⌋k+1
2
k+1
2−k+1
4
= (−1)⌊k+1
4⌋k+1
2k+1
4
.
21By Lemma 11, we can assert:
(ϕ(I)|even)(k+1
2)
⌊k+1
4⌋≥2k+1
2
4q
2k+1
2≥2k+1
2
4√
t,
which completes the proof.
D.5 Proof of Corollary 2
Theorem 2 indicates that
nR∥C⊤
R⊙BR∥∞≥max d,m∈N, d+m≤⌊t/2⌋, σ∈{odd,even}n
2d+2 min {d,m}
2−d(R:t|σ)(d)
m−ϵo
.
For any σ∈ {odd,even}, plugging in m=t
8
,d=t
4
, and ϵ≤α√
δ√
tyields:
n∥CT⊙B∥∞≥2t
2−3
(R:t|σ)(⌊t
4⌋)
⌊t
8⌋
2⌊t
4⌋−α√
δ√
t
. (9)
In the subsequent analysis, we will demonstrate that for any σ∈ {odd,even}, the following holds
with a probability smaller than√
δ:
(R:t|σ)(⌊t
4⌋)
⌊t
8⌋≤2⌊t
4⌋·2α√
δ√
t.
Once established, the remainder of the proof will naturally follow, as plugging this forthcoming result
into Equation (9) will ensure that for any σ∈ {odd,even}, with probability 1−√
δwe have that:
n∥CT⊙B∥∞≥2t
2−3α√
δ√
t=2t
2α√
δ
8√
t,
satisfying the requirements for our conclusion. Since R:t|evenandR:t|oddare independent random
variables, we have that the previous statement holds with probability higher than 1−δas required.
Now let us proceed with proving the aforementioned result. Assume without loss of generality that
σ=even. Lemma 7 implies that
(R|even)(⌊t
4⌋)
⌊t
8⌋=⌊t
4⌋X
j=0(R|even)⌊t
8⌋+j(−1)⌊t
4⌋−jt
4
j
=⌊t
4⌋X
j=0(R)2⌊t
8⌋+2j(−1)⌊t
4⌋−jt
4
j.
For each j∈t
4
, define the random variable Xjas follows:
Xj= (R)2⌊t
8⌋+2j(−1)⌊t
4⌋−jt
4
j
,
and denote its PDF by pXj. We have that
Xj∼t
4
j
U(−α, α) =U(−αt
4
j
, αt
4
j
),
which means that
pXj(x) =

1
2α(⌊t
4⌋
j)if−α≤x≤α
0 otherwise.
22LetXbe the sum of these random variables X=P⌊t
4⌋
j=0Xjand denote its PDF by pX. We note that
P(R|even)(⌊t
4⌋)
⌊t
8⌋≤ϵ0
=P[−ϵ0< X < ϵ 0]≤2ϵ0max
x∈RpX(x).
As a sum of i.i.d. random variables, the maximum of pXis upper bounded by the maximum of the
PDF of each of the random variables it is a sum of. Which means that
max
x∈RpX(x)≤max
x∈RpX⌊t
8⌋(x) =1
2α ⌊t/4⌋
⌊t/8⌋≤q
⌊t
8⌋
α2⌊t
4⌋≤√
t
2α2⌊t
4⌋
(where the second inequality results from Lemma 10). Overall we have that
P(R|even)(⌊t
4⌋)
⌊t
8⌋≤ϵ0
≤ϵ0√
t
2α2⌊t
4⌋.
Plugging in ϵ0= 2⌊t
4⌋·2α√
δ√
twe have that
P"(R|even)(⌊t
4⌋)
⌊t
8⌋≤2⌊t
4⌋·2α√
δ√
t#
≤√
δ
which concludes the proof.
D.6 Proof of Corollary 3
Theorem 2 demonstrated that
nR∥C⊤
R⊙BR∥∞≥max d,m∈N, d+m≤⌊t/2⌋, σ∈{odd,even}n
2d+2 min {d,m}
2−d(ϕ(I)|σ)(d)
m−ϵo
.
Plugging in σ=odd, we have that for any m∈[⌊t/2⌋]the following holds:
(ϕ(I)|σ)m= (−1)m−1.
Using simple induction, it is easy to show that for any d, m∈Nsuch that d+m≤ ⌊t/2⌋,
(ϕ(I)|σ)(d)
m= (−1)(m+d−1)2d.
Plugging this into Theorem 2 with σ=odd,d=⌊t/4⌋,m=⌊t/4⌋, and ϵ= 0.5yields:
nR∥C⊤
R⊙BR∥∞≥2⌊t/4⌋+2⌊t/4⌋(1−0.5) = 23⌊t/4⌋−1≥23t/4−4,
as required.
D.7 Proof of Proposition 3
It is sufficient to prove the proposition for nC=tsince the dimension of a diagonal SSM can always
be effectively reduced by zeroing out elements of BC.
According to Section 2.3, it is enough to show that there exist assignments for (AC, BC, CC)such
that the following conditions hold:
ϕt,(AC,BC,CC)(I):t=ϕ(I):t,∥BC∥2= 2∥ϕ(I):t∥2,∥CT
C∥2= 1.
For convenience, we will denote AC, BC, CCbyA, B, C .
We begin by utilizing the theory of discrete Fourier transform (DFT ) to allow us to write the
truncated impulse response of ϕof length tas a linear combination of tdecaying sine and cosine
waves. Specifically, defining
˜ϕ(I):t
k=(ϕ(I):t)k
αk−1, where α∈(0,1)is some constant, and denoting
the DFT of ˜ϕ(I):tbya+bi∈Ctwe get that
∀k∈[t],(ϕ(I):t)k
αk−1=1
tt−1X
j=0aj+1cos
2π(k−1)j
t
+t−1X
j=0bj+1sin
2π(k−1)j
t
.
23Now, we will derive assignments for the SSM’s parameters so that its impulse response will equate
this sum. Indeed, we fix the diagonal entries of Ato be a scaled version of the t roots of unity, and C
to be the inverse of the square root of tas follows:
Aj,j=αe2πij−1
t, C j=1√
t
We know (see Section 2.3) that the impulse response of a complex system at index kis given by:
 
ϕt,(A,B,C )(I):t
k=ℜ
tX
j=1Ak−1
j,jBjCj
.
Which implies that
ℜ
tX
j=1Ak−1
j,jBjCj
=tX
j=1ℜ(CjAk−1
j,j)ℜ(Bj)− ℑ(CjAk−1
j,j)ℑ(Bj)
=αk−11√
t
t−1X
j=0cos
2π(k−1)j
t
ℜ(Bj)−t−1X
j=0sin
2π(k−1)j
t
ℑ(Bj)
.
By the Plancherel theorem the following holds:
∥a+bi∥2=√
t˜ϕ(I):t
2≤√
t∥ϕ(I):t∥2
αt−1.
Therefore, if we define B=a−ib√
t, we get
ℜ
tX
j=1Ak−1
j,jBjCj
=αk−1
t
t−1X
j=0aj+1cos
2π(k−1)j
t
+t−1X
j=0bj+1sin
2π(k−1)j
t

= (ϕ(I):t)k
and
∥B∥2=a−bi√
t
2≤∥ϕ(I):t∥2
αt−1,∥C∥2=s
t·1√
t2
= 1,
and by choosing α= 1
21
t−1we get the required assignment.
D.8 Proof of Proposition 4
For simplicity of notation, let A(i)
R, B(i)
R, C(i)
Rbe denoted by A(i), B(i), C(i), respectively, and
ℓ(A(i), B(i), C(i))byℓ(i). We begin by writing the loss explicitly (see Appendix C):
ℓ(A, B, C ) =tX
j=1 nRX
k=1Aj−1
kBkCk−ϕ(I)j!2
.
Fixing any k∈[nR], we have
dℓ(A, B, C )
dBk= 2tX
j=1Aj−1
kCk nRX
m=1Aj−1
mBmCm−ϕ(I)j!
,
which, combined with the fact that |Am| ≤1, leads to
dℓ(A, B, C )
dBk≤2|Ck|tX
j=1nRX
m=1Aj−1
mBmCm−ϕ(I)j
= 2|Ck|ϕn,(A,B,C )(I):t−ϕ(I):t
1
≤2√
t|Ck|ϕn,(A,B,C )(I):t−ϕ(I):t
2
≤2√
t·max (∥B∥∞,∥C∥∞)p
ℓ(A, B, C ).
24This implies
∥B(i)∥∞≤ ∥B(i−1)∥∞+η(i)·2p
t·ℓ(i−1)maxB(i−1)
∞,C(i−1)
∞
.
Next, we will demonstrate that
η(i)maxB(i−1)
∞,C(i−1)
∞
≤√c1,
which concludes the proof since this aforementioned result will establish that for all i∈N, the
following holds:
∥B(i)∥∞≤ ∥B(i−1)∥∞+ 2p
tℓ(i−1)c1
≤ ∥B(i−1)∥∞+ 2p
tc2ℓ(0)c1 (1)
≤ ∥B(0)∥∞+ 2ip
tc2ℓ(0)c1. (2)
Where (1) holds because we assume ℓ(i−1)≤c2ℓ(0)and (2) is by recursion.
Repeating the same argument for C(i)completes the proof.
Finally, let us show that η(i)max B(i−1)
∞,C(i−1)
∞
≤√c1. Indeed, Lemma 3 combined
with the assumptions that
∀i∈N:η(i)≤2
max
λmax∇2ℓ(i−1),0	
and∀i∈N:η(i)≤c1gives that
η(i)≤min(
2
2 maxB(i−1)
∞,C(i−1)
∞	2, c1)
.
Considering the cases where maxB(i−1)
∞,C(i−1)
∞	
is bigger and smaller than 1/√c1we
immediately get the required result.
Lemma 3. In the setting of Proposition 4 we have that for any AR, BR, CRthe following holds:
λmax 
∇2ℓ 
AR, BR, CR
≥2 max {∥BR∥∞,∥CR∥∞}2
Proof. For simplicity of notation, let AR, BR, CRbe denoted by A, B, C , respectively. We will
assume, without loss of generality, that ∥C∥∞≥ ∥B∥∞, and denote i= arg max iCiand by eithe
one-hot vector with 1 at index iand zero elsewhere. We will also denote the following function
Ii(x) =ϕnR,(A,B+x·ei,C)(I):t
byIi(x), and the following loss function:
˜ℓi(x) =ℓ(Ii(x)) = ˜ℓ(A, B +x·ei, C)
by˜ℓi. Next, We notice that by definition of the Hessian, we have that
 
∇2ℓ(A,·, C)[B]
i,i=˜ℓ′′
i(0).
Therefore, λmax 
∇2ℓ
atA, B, C is lower-bounded by ˜ℓ′′
i(0). It is therefore sufficient to show that
|˜ℓ′′
i(0)| ≥2∥C∥2
∞. Indeed, by Appendix C, the following holds
25|˜ℓ′′
i(0)|=dℓ(A, B, C )
(dBi)2
=dPt
j=1PnR
m=1Aj−1
mBmCm−ϕ(I)j2
(dBi)2
=tX
j=1d
dBi  nRX
m=1Aj−1
mBmCm−ϕ(I)j!
·2Aj−1
iCi!
=tX
j=12Aj−1
iCi·Aj−1
iCi
≥2∥C∥2
∞,
which concludes the proof.
D.9 Proof of Proposition 5
For simplicity of notation, we will denote AR, BR, CR, nRbyA, B, C, n . The robustness to q-
quantization is by definition the probability:
P
∥ϕn,(A⊙QA,B⊙QB,C⊙QC)(I):t−ϕ(I):t∥1≤ϵ
≤Pϕn,(A⊙QA,B⊙QB,C⊙QC)(I)1−ϕ(I)1≤ϵ
.
Let us denote the random variable ϕn,(A⊙QA,B⊙QB,C⊙QC)(I)1byX, and its PDF by pX. The
probability P[|X−Y| ≤ϵ]is equal to the integralRY+ϵ
Y−ϵpX(x)dx≤2ϵmax x∈RpX(x). We get
that
P
∥ϕn,(A⊙QA,B⊙QB,C⊙QC)(I)−ϕ(I)∥1≤ϵ
≤2ϵmax
x∈RpX(x).
and so, to prove the theorem, it suffices to show that
max
x∈RpX(x)≤1
q∥B⊙C∥∞.
Let us investigate the random variable X:
X=nX
i=1(B⊙QB)i(C⊙QC)i=nX
i=1BiCi(1 +qBi)(1 + qCi)
=nX
i=1BiCi(1 +qBi+qCi+qBiqCi),
where qBi, qCi∼ U(−q/2, q/2). As a sum of independent random variables, the maximum of
pXis upper bounded by the maximum of the PDF of each of the random variables it is a sum of.
Specifically, for any i∈[n], it is bounded by the maximum of the PDF of U(−BiCiq/2, BiCiq/2)
which is equal to 1/(BiCiq). Overall, this means that
max
x∈RpX(x)≤min
i1
BiCiq=1
q∥B⊙C∥∞,
as needed.
26E Auxiliary Lemmas
Lemma 4. For any nnon-negative numbers a1, . . . , a n, andnreal numbers b1, . . . , b n, the function
fdefined below
f(x) =nX
i=1biaix
can have at most n−1zeros.
Proof. We will show this by induction. The claim is obvious for n= 1, let us assume the induction
claim is true for n=kand we will show that it is true for n=k+ 1. Indeed, let fbe the function
defined by
f(x) =k+1X
i=1biaix.
Ifb1= 0ora1= 0we can immediately use the induction step on f. Otherwise, we can write fin
the following way:
f(x) =b1ax
1 
1 +k+1X
i=2bi
b1ai
a1x!
.
Since b1ax
1is always non-zero, the amount of roots of fis equal to the number of roots of the function
gdefined by
g(x) = 1 +n+1X
i=2bi
b1ai
a1x
.
By Rolle’s theorem, the number of roots of a function is bounded by the number of roots of its
derivative plus one. We have that g′(x)is given by:
g′(x) =k+1X
i=2bi
b1ai
a1x
lnai
a1
.
Which, by the induction step, can have at most kroots, as needed.
Lemma 5. Letθbe a real number in the open interval (0,2π), letbbe a real number in [0,2π],
and let pbe a natural number such that θ·p≥2π. Then the set
{(b+θ) mod 2 π,(b+ 2θ) mod 2 π, . . . , (b+p·θ) mod 2 π}
contains at least one point in any closed and continuous-mod- 2πinterval of length θwithin [0,2π].
Proof. Letxk= (b+kθ)fork= 1,2, . . . , p . Define the intervals
Ik= [xk−1, xk) mod 2 πfork= 1,2, . . . , p .
Each interval Ikhas a length of θ. Since pθ≥2πwe have that
p[
k=1Ik= [0,2π].
Now, consider any closed and continuous-mod- 2πinterval J⊂[0,2π)of length θ. SinceSp
k=1Ik=
[0,2π], the interval Jmust intersect at least one of the intervals Ik. That is,
J∩Ik∗̸=∅for some k∗∈ {1,2, . . . , p }.
Since Jhas length θand overlaps with Ik∗(which also has length θ), and since both are continuous-
mod-2πtheir intersection must include at least one of the endpoints of Ik∗which is in the set
{(b+θ) mod 2 π,(b+ 2θ) mod 2 π, . . . , (b+p·θ) mod 2 π}.
This completes the proof.
27Lemma 6. Letαbe a real number within the interval [0,1], and let n, m be natural numbers with t
being a natural number such that t≥n+m. The absolute value of the mth forward difference of
the sequence X={1, α, α2, . . . , αt−1}at index n, as defined in Definition 1, is bounded above by
m
n+mmn
n+mn
.
Proof. By induction, the mth forward difference of Xat index n(when we start at index 0), for
n≤t−m, isαn(α−1)m. Forα∈[0,1], this absolute value simplifies to:
|αn(α−1)m|=αn(1−α)m.
To find the maximum value of this expression within [0,1], we differentiate with respect to αand set
the derivative to zero:
d
dα(αn(1−α)m) =nαn−1(1−α)m−mαn(1−α)m−1.
Solving for α, we find that the critical points are at α∈ {0,1}or:
n(1−α)−mα= 0 = ⇒α=n
n+m.
A second derivative test confirms that α=n
n+mis a maximum within [0,1]. Substituting αmax=
n
n+minto our original expression, we obtain:
|(X(m))n| ≤n
n+mn
1−n
n+mm
=m
n+mmn
n+mn
,
thereby completing the proof.
Lemma 7. Given a sequence A={a0, a1, . . . , a n−1}and two natural numbers m < n , themth
forward difference of A(Definition 1) at index n, denoted 
A(m)
n, is given by

A(m)
n=mX
j=0an+j(−1)m−jm
j
Proof. We will show this by induction over m. For m= 0This is obvious. Let’s assume that the
Lemma is true for m=kand prove that it is also true for m=k+ 1< n.
Indeed

A(m+1)
n=
A(m)
n+1−
A(m)
n
=mX
j=0(−1)m−jan+j+1m
j
−mX
j=0(−1)m−jan+jm
j
=an+m+1−(−1)man+m−1X
j=0(−1)m−jan+j+1m
j
+m
j+ 1
=an+m+1−(−1)man+m−1X
j=0(−1)m−jan+j+1m+ 1
j+ 1
=an+m+1−(−1)man+mX
j=1(−1)m−j+1an+jm+ 1
j
=m+1X
j=0(−1)m+1−jan+jm+ 1
j
as needed.
28Lemma 8. Given two sequences A={a0, a1, . . . , a n−1}andB={b0, b1, . . . , b n−1}of length n,
and for any natural number m < n , it holds that
∥A−B∥∞≥∥A(m)−B(m)∥∞
2m
where A(m)andA(m)are the mth forward difference (Definition 1) of AandBrespectively
Proof. We aim to demonstrate that
2m∥A−B∥∞≥ ∥A(m)−B(m)∥∞
which directly supports the statement of the lemma.
It suffices to verify this for m= 1, as the general case can then be established by induction, showing
that:
∥A(m)−B(m)∥∞≤2∥A(m−1)−B(m−1)∥∞≤ ··· ≤ 2m∥A−B∥∞
Indeed, for m= 1, we have that for any i < n−1
|(A(1)−B(1))i|=|ai+1−ai−bi+1+bi| ≤ |ai+1−bi+1|+|ai−bi| ≤2∥A−B∥∞
Lemma 9. LetϕnR,(AR,BR,CR)(I):tbe a truncated impulse response of a real SSM denoted by Y.
Each of the following two sequences
Y|odd= (Y1,Y3, . . .)
and
Y|even= (Y2,Y4, . . .)
are impluse responses of some other real SSM of dimension nRwith parameters (˜AR,˜BR,˜CR)where
˜ARis non-negative, and
∥CT
R⊙BR∥∞≥ ∥˜CRT⊙˜BR∥∞
Proof. We observe that (see Section 2.3)
(Y|odd)i=CRAR2(i−1)BR
setting (˜AR,˜BR,˜CR)=(A2
R, BR, CR)yields that Y|oddis the impulse response of the real SSM
ϕnR,(˜AR,˜BR,˜CR).
For the even case:
(Y|even)i=CRAR2i−1BR
setting (˜AR,˜BR,˜CR)=(A2
R, BR, CRAR)yields that Y|evenis the impulse response of the real SSM
ϕnR,(˜AR,˜BR,˜CR). In both cases, ˜ARis non-negative, and
∥CT
R⊙BR∥∞≥ ∥˜CRT⊙˜BR∥∞
since in the even case (˜BR,˜CR)=(BR, CR)and in the odd case |˜CRj| ≤ |CRj|because |ARj,j| ≤1
for all j.
Lemma 10. For any positive integer n, the following inequality holds:
2n
n
≥22n
2√n
29Proof. Using the Stirling’s formula for factorials, we know that n!is bounded by:
√
2πnn
en
e(1
12n−1
360n3)< n!<√
2πnn
en
e1
12n.
which can be simplified to:
√
2πnn
en
< n!<√
2πnn
en
e1
12n.
We can plug the bound in the factorial form of the binomial coefficients:
2n
n
=(2n)!
n!·n!>√
2π2n 2n
e2n
(√
2πn)2 n
e2ne1
6n=22n
√πne1
6n>22n
2√n
Where the last inequality is due to√πe1
6n<2for all n≥2. To complete the proof for all positive
integers notice we get equality for n= 1.
Lemma 11. For any positive integer n, the following inequalitie hold:n
⌊n
2⌋
≥2n
4√
2n
Proof. Utilizing Lemma 10
we have 2m
m
≥22m
2√m.
To validate the lemma, we consider cases based on the parity of n. For even n,n
⌊n
2⌋
=n
n
2
≥2n
2pn
2>2n
4√
2n,
and for odd n,n
⌊n
2⌋
=n
n−1
2
≥n−1
n−1
2
≥2n−2
2q
n−1
2≥2n
4√
2n,
thereby proving the lemma as required.
Lemma 12. For any positive integers nandm, it holds thatm
n+mmn
n+mn
≤1
22 min( m,n)
Proof. We establish this inequality through direct analysis:
m
n+mmn
n+mn
≤m
n+mmin(m,n)n
n+mmin(m,n)
= 
mn
(n+m)2!min(m,n)
≤mn
4nmmin(m,n)
=1
22 min( m,n)
This last inequality leverages the fact that (n+m)2−4nm= (n−m)2>0.
Lemma 13. For any complex number x=a+biwitha≤0, it holds that1
1−x≤1.
Proof. It suffices to show that |1−x| ≥1. Indeed, we have
|1−x|=p
(1−a)2+b2≥p
(1−a)2=|1−a|= 1−a≥1,
as required.
30Input: ··· cX1X2··· Xh··· cX1···Xh−1
Output: ··· X1X2··· Xh2h
Figure 1: Illustration of the induction-head task. See Appendix F.3 for details.
F Implementation Details
Appendices F.1 to F.3 below present implementation details omitted from Sections 4.1 to 4.3,
respectively. Source code for reproducing the results of Sections 4.1 and 4.3 can be found at
https://github.com/edenlum/SSMComplexParamBenefits . The results of Section 4.2 were
obtained using the official S4 repository, available at https://github.com/state-spaces/s4
(Apache-2.0 license). All experiments were conducted on a single NVIDIA A6000 GPU.
F.1 Theoretically Analyzed Setting
In all experiments, we parameterized state transition matrices in a manner that ensures stability,
similarly to the LRU architecture [ 41]. For real SSMs, we performed a grid search for each optimizer,
varying learning rates and initialization schemes. Namely, we evaluated learning rates of 1×10−4,1×
10−5and1×10−6, and randomly initialized the diagonal elements of ARuniformly in [−1,1]or in
[−1,0.99]∪[0.99,1]. For complex SSMs, we used a learning rate of 1×10−5and initialized the
diagonal elements of ACsimilarly to [ 41], by sampling uniformly from the complex ring with radii
0.99to1. For all SSMs, we employed a cosine learning rate scheduler [ 35] and trained for half a
million steps.
F.2 Real-World Setting
Our implementation is based on the official S4 repository, available at https://github.com/
state-spaces/s4 (Apache-2.0 license). Unless stated otherwise, repository hyperparameters
(pertaining to the neural network architecture and its training) were kept at their default values. The
SSM powering the architecture was adapted to a diagonal structure for the state transition matrix A.
For real SSMs, we began with the same default configuration as for complex SSMs, and extended
the configuration by performing a grid search over different optimizers (Adam [ 29], AdamW [ 36],
SGD [ 45]) and initialization schemes (diagonal-real, diagonal-linear, and legs, as defined in the
official S4 repository). We ran three random seeds with the default configuration, and a single random
seed with all others.
F.3 Selectivity
Copy task. Our version of copy task has a delay of t= 64 . It entails input sequences of length
2t, where tokens are sampled independently and uniformly as one-hot vectors of dimension 16.
The corresponding output sequences are generated by shifting the input sequences tpositions to
the right, while introducing zero padding. That is, given an input sequence [a1, a2, . . . , a 2t], the
corresponding output sequence is [0,0, . . . , 0, a1, a2, . . . , a t]. Training and evaluation (measurement
of test accuracy) in this task are based on randomly generated examples, where the input is sampled
from a uniform distribution. In training, a fresh batch of examples is generated for each iteration, and
the loss corresponding to an example is the cross-entropy loss averaged over the last ttokens of the
output. In evaluation, the zero-one loss is averaged over the last ttokens of the output across one
thousand freshly generated examples.
Induction-head task. In the induction-head task [ 18], the goal is to teach a model to identify and
copy a specific subsequence of the input. Figure 1 illustrates this task, which in our case has an
induction head size of h= 128 . The task entails input sequences of length 3h,i.e., input sequences
with3htokens. The first (less than h) tokens in an input sequence are sampled independently and
uniformly as one-hot vectors of dimension 16. These tokens are followed by a special copy token ,
31c=0. After the copy token comes a sequence of htokens, sampled as before and denoted X. The
2hth token is another copy token c, after which the first to penultimate tokens of the sequence X
repeat. The tokens between the first appearance of Xand the second appearance of care irrelevant
for the task. The output sequence corresponding to the above-described input sequence is equal to the
input sequence shifted one position to the left (with the last token of Xplaced on the right). Training
and evaluation (measurement of test accuracy) in this task are based on randomly generated examples,
where the following aspects of the input are sampled from uniform distributions: the location of the
first copy token c, the sequence X, and the irrelevant tokens. In training, a fresh batch of examples
is generated for each iteration, and the loss corresponding to an example is the cross-entropy loss
averaged over the last htokens of the output. In evaluation, the zero-one loss is averaged over the last
htokens of the output across one thousand freshly generated examples.
Hyperparameters. Our implementation is based on a widely adopted Mamba repository, available
athttps://github.com/alxndrTL/mamba.py (MIT license). Unless stated otherwise, repository
hyperparameters (pertaining to the neural network architecture and its training) were kept at their
default values. The Mamba neural network was configured to have two layers with a hidden dimension
of64. We trained the network with a batch size of 8and a learning rate of 1×10−3. Training
comprised 1,000,000steps on the copy task, and 250,000steps on the induction-head task (the
latter task warranted a shorter run-time since it was experimented with much more). For complex
parameterization, we changed the state transition matrix A, the input matrix Band the output
matrix Cto be complex, while keeping the discretization parameter ∆real. To maintain parameter
count, the dimension of SSMs with complex parameterization was half the dimension with real
parameterization, namely, 8as opposed to 16.
32NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction clearly differentiate between formal contributions
and informal discussions.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The paper includes a dedicated limitations section (Section 6).
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Every theoretical result is formally stated (as a lemma, proposition, theorem or
corollary) and proven.
4.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The paper provides (in Section 4 and Appendix F) references to code for
reproducing experiments.
5.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The paper provides (in Section 4 and Appendix F) a detailed account of
implementation details.
6.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The paper reports (in Section 4 and Appendix F) minimum and maximum
values across multiple random seeds, while specifying the number of seeds.
7.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The paper specifies (in Appendix F) the type of hardware used for compute, as
well as details determining the computational load of each experiment (number of training
steps, neural network architecture etc.).
8.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
33Answer: [Yes]
Justification: The paper is theoretical in nature, with experiments on synthetic or standard
datasets designed to corroborate the theory. As such, it inherently conforms with the NeurIPS
Code of Ethics.
9.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The paper is theoretical in nature, with experiments on synthetic or standard
datasets designed to corroborate the theory. As such, its societal impacts are self explanatory.
10.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA] .
Justification: The paper does not offer data or models that have a high risk for misuse.
11.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The paper specifies (in Section 4 and Appendix F) all assets used, along with
applicable license details.
12.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The paper is accompanied by code for reproducing experiments ( https://
github.com/edenlum/SSMComplexParamBenefits ), and this code is well documented.
13.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not include crowdsourcing experiments or research with human
subjects.
14.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not include research with human subjects.
34