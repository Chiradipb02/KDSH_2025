Onf-Divergence Principled Domain Adaptation: An
Improved Framework
Ziqiao Wang
Tongji University
Shanghai, China
ziqiaowang@tongji.edu.cnYongyi Mao
University of Ottawa
Ottawa, Canada
ymao@uottawa.ca
Abstract
Unsupervised domain adaptation (UDA) plays a crucial role in addressing distribu-
tion shifts in machine learning. In this work, we improve the theoretical founda-
tions of UDA proposed in Acuna et al. (2021) by refining their f-divergence-based
discrepancy and additionally introducing a new measure, f-domain discrepancy
(f-DD). By removing the absolute value function and incorporating a scaling pa-
rameter, f-DD obtains novel target error and sample complexity bounds, allowing
us to recover previous KL-based results and bridging the gap between algorithms
and theory presented in Acuna et al. (2021). Using a localization technique, we
also develop a fast-rate generalization bound. Empirical results demonstrate the
superior performance of f-DD-based learning algorithms over previous works in
popular UDA benchmarks.
1 Introduction
Machine learning often faces the daunting challenge of domain shift, where the distribution of data in
the testing environment differs from that used in training. Unsupervised Domain Adaptation (UDA)
arises as a solution to this problem. In UDA, models are allowed to access to labelled source domain
data and unlabelled target domain data, while the ultimate goal is to find a model that performs well
on the target domain. The mainstream theoretical foundations of UDA, and more broadly, domain
adaptation [ 2], primarily rely on the seminal works of discrepancy-based theory [ 3,4]. In particular,
[3,4] characterize the error gap between two domains using a hypothesis class-specified discrepancy
measure e.g., H∆H-divergence. While these works initially focus on binary classification tasks and
zero-one loss, [ 5] extend the theory to a more general setting. Subsequently, this kind of theoretical
framework was extended by various works [ 6–10,1,11], all sharing some common properties such
as the ability to estimate the proposed discrepancy from finite unlabeled samples. Importantly, these
theoretical results often inspire the design of new algorithms, such as domain-adversarial training of
neural networks (DANN) [ 12] and Margin Disparity Discrepancy (MDD) [ 9], directly motivated by
H∆H-divergence.
Recently, [ 1] proposes an f-divergence-based domain learning framework, which generalizes various
previous frameworks (e.g., those based on H∆H-divergence) and have demonstrated great empirical
successes. However, we argue that this framework has potential limitations, at least in three aspects.
First, their discrepancy measure is based on the variational representation of f-divergence in [ 13]
(cf. Lemma 2.1). Although this variational formula is commonly adopted, its weakness has been
pointed out in several works [ 14,15]. For example, from this formula, one cannot recover the
well-known Donsker and Varadhan’s (DV) representation of KL divergence [ 16]. This reveals
a second limitation: some existing domain adaptation or transfer learning theories are based on
the DV representation of KL, such as [ 17–20], and the framework by [ 1], although including KL
as a special case of f-divergence, fail to unify those theories. Furthermore and more critically,
38th Conference on Neural Information Processing Systems (NeurIPS 2024).the discrepancy measure proposed by [ 1] contains an absolute value function, while the original
variational representation does not. Notably, this absolute value function is necessary in their
derivation of a target error upper bound, and the fundamental reason behind this is still due to the
weak version of the variational representation relied upon. Specifically, a variational representation
of an f-divergence is a lower bound of the divergence, but using a weak lower bound creates
technical difficulties in proving an upper bound for target domain error. [ 1] chooses to add an
absolute value function accordingly, potentially leading to an overestimation of the corresponding
f-divergence. In fact, this absolute value function is removed in their proposed algorithm, termed
f-Domain Adversarial Learning ( f-DAL). While f-DAL outperforms both DANN and MDD in
standard benchmarks, this choice exhibits a clear gap between their theory and algorithm.
In this work, to overcome these limitations and explore the full potential of f-divergence, we present
an improved framework for f-divergence-based domain learning theory. Specifically, we apply a
more advanced variational representation of f-divergence (cf. Lemma 2.2), independently developed
by [21,22] and [ 23]. After introducing some preliminaries, the organization of the remainder of the
paper and our main contributions are summarized below.
•In Section 3, we revisit the theoretical analysis in [ 1], where we refine their f-divergence-based
discrepancy by Lemma 2.2 while retaining the absolute value function in the definition. The resulting
target error bound (cf. Lemma 3.1) and the KL-based generalization bound (cf. Theorem 3.1)
complement the theoretical framework of [1].
•In Section 4, we design a novel f-divergence-based domain discrepancy measure, dubbed f-
DD. Specifically, we eliminate the absolute value function from the definition used in Section 3,
incorporating a scaling parameter instead. We then derive an upper bound for the target error
(cf. Theorem 4.1) and the sample complexity bound for our f-DD. The generalization bound based
on empirical f-DD naturally follows from these results. Notably, the obtained target error bound
allows us to recover the previous KL-based result in [19] (cf. Corollary 4.1).
•In Section 5, to improve the convergence rate of our f-DD-based bound, we sharpen the bound using
a localization technique [ 24,10]. The localized f-DD allows for a crisp application of the local
Rademacher complexity-based concentration results [ 24], while also enabling us to achieve a fast-
rate target error bound (cf. Theorem 5.1). As a concrete example, we present a generalization bound
based on localized KL-DD (cf. Theorem 5.2), where our proof techniques are directly connected to
fast-rate PAC-Bayesian bounds [ 25,26] and fast-rate information-theoretic generalization bounds
[27–29].
•In Section 6, we conduct empirical studies based on our f-DD framework. We show f-DD
outperforms the original f-DAL in three popular UDA benchmarks, with the best performance
achieved by Jeffereys-DD. Additionally, we note that the training objective in f-DAL aligns more
closely with our theory than with [ 1] (cf. Proposition 1). We also show that adding the absolute
value function leads to overestimation and that optimizing the scaling parameter in f-DD may not
be necessary in practical algorithms.
2 Preliminaries
Notations and UDA Setup LetXandYbe the input space and the label space. Let Hbe the
hypothesis space, where each h∈ H is a hypothesis mapping XtoY. Consider a single-source
domain adaptation setting, where µandνare two unknown distributions on X × Y , characterizing
respectively the source domain and the target domain. Let S={Xi, Yi}n
i=1∼µ⊗nbe a labeled
source-domain sample and T={Xj}m
j=1∼ν⊗m
Xbe an unlabelled target-domain sample, where
νXis the marginal distribution of Xin the target domain. We use ˆµandˆνto denote the empirical
distributions on Xcorresponding to SandT, respectively. The objective of UDA is to find a
hypothesis h∈ H based on SandTthat “works well” on the target domain.
Letℓ:Y × Y → R+
0be a symmetric loss (e.g., ℓ(y, y) = 0 for all y∈ Y). The population risk for
eachh∈ H in the target domain (i.e. target error) is defined as Rν(h)≜E(X,Y)∼ν[ℓ(h(X), Y)],
and the population risk in the source domain, Rµ(h), is defined in the same way. Since µandνare
unknown to the learner, one often uses recourse to the empirical risk in the source domain, which, for
a given S, is defined as Rˆµ(h)≜E(X,Y)∼ˆµ[ℓ(h(X), Y)] =1
nPn
i=1ℓ(h(Xi), Yi). Furthermore, let
fνandfµbe the ground truth labeling functions for the target domain and source domain, respectively,
i.e.fν(x) = arg max y∈Yν(Y=y|x)andfµ(x) = arg max y∈Yµ(Y=y|x).
2With a little abuse of the notation, we will simply use ℓ(h, h′)to represent ℓ(h(x), h′(x))when
the same xis evaluated, serving as the “disagreement” for handh′onx. Additionally, following
the conventional literature on DA theory [ 5], we assume that the loss function satisfies the triangle
property1. For the readers’ convenience, a summary of all notations is provided in Table 5 in the
Appendix.
Background on f-divergence The family of f-divergence is defined as follows.
Definition 2.1 (f-divergence [ 30]).LetPandQbe two distributions on Θ. Let ϕ:R+→R
be a convex function with ϕ(1) = 0 . IfP≪Q2, then f-divergence is defined as Dϕ(P||Q)≜
EQh
ϕ
dP
dQi
,wheredP
dQis a Radon-Nikodym derivative.
Thef-divergence family contains many popular divergences. For example, letting ϕ(x) =xlogx
(orxlogx+c(x−1)for any constant c) recovers the definition of KL divergence.
Thef-divergence discrepancy measure by [ 1] is motivated by the variational formula of f-divergence
that utilizes the Legendre transformation (LT).
Lemma 2.1 ([13]) .Letϕ∗be the convex conjugate3ofϕ, andG={g: Θ→dom( ϕ∗)}. Then
Dϕ(P||Q) = sup
g∈GEθ∼P[g(θ)]−Eθ∼Q[ϕ∗(g(θ))].
However, it is well-known that the variational formula in Lemma 2.1 does not recover the the Donsker
and Varadhan’s (DV) representation of KL divergence (cf. Lemma B.1). We will elaborate on this later.
Recently, [ 23] and [ 21,22] concurrently introduce a novel variational representation for f-divergence,
which is also implicitly stated in [31, Theorem 4.2], as given below.
Lemma 2.2 ([21, Corollary 3.5]) .The variational formula of f-divergence is
Dϕ(P||Q) = sup
g∈GEθ∼P[g(θ)]−inf
α∈R{Eθ∼Q[ϕ∗(g(θ) +α)]−α}.
This variational representation is a “shift transformation” of Lemma 2.1 (i.e. g→g+α). Note
that this representation shares the same optimal solution as Lemma 2.1 (clearly identified as the
corresponding f-divergence), but Lemma 2.2 is considered tighter in the sense that the representation
in Lemma 2.2 is flatter around the optimal solution. [ 23] provides a comprehensive study to justify
this, and they also show that Lemma 2.2 can accelerate numerical estimation of f-divergences.
Here, to illustrate the advantage of Lemma 2.2, we use KL divergence as an example. Specifically,
letϕ(x) =xlogx−x+ 1, then its conjugate function is ϕ∗(y) =ey−1. Substituting ϕ∗into
Lemma 2.1, we have
DKL(P||Q) = sup
g∈GEP[g(θ)]−EQh
eg(θ)−1i
. (1)
This representation is usually called LT-based KL. On the other hand, with the optimal α∗=
−logEQ
eg(θ)
, Lemma 2.2 will give us
DKL(P||Q)= sup
g∈GEP[g(θ)]−inf
α∈Rn
EQh
eg(θ)+αi
−1−αo
= sup
g∈GEP[g(θ)]−logEQh
eg(θ)i
.(2)
Notice that Eq. (2) immediately recovers the DV representation of KL. Since log(x)≤x−1for
x >0, it is evident that, as a lower bound of KL divergence, Eq. (2) is pointwise tighter than Eq. (1).
In Appendix C, we also show the variational representations of some other divergences obtained from
Lemma 2.2.
In the context of UDA, it may be tempting to think that using a point-wise smaller quantity (in
Lemma 2.1), as the key component of an upper bound for target error, is essentially desired. However,
1The triangle property of loss function indicates that ℓ(y1, y2)≤ℓ(y1, y3)+ℓ(y3, y2)for any y1, y2, y3∈ Y.
2We say that Pis absolutely continuous with respect to Q, written P≪Q, ifQ(A) = 0 = ⇒P(A) = 0
for all measurable sets A⊆Θ.
3For a function f:X →R∪ {−∞ ,+∞}, its convex conjugate is f∗(y)≜supx∈dom( f)⟨x, y⟩ −f(x).
3neither Lemma 2.1 nor Lemma 2.2 is able to directly give such an upper bound. To elaborate,
asϕ∗(x)≥xwhen ϕ(1) = 0 (cf. Lemma B.2), both Lemma 2.1 and Lemma 2.2 imply that
Dϕ(P||Q)≤supgEP[g(θ)]−EQ[g(θ)]. Bearing this in mind, UDA typically requires an upper
bound for the quantity suphEν[ℓ◦h(X)]−Eµ[ℓ◦h(X)], and simply restricting Gin Lemma 2.1
and Lemma 2.2 to a composition of Handℓcan only provide a lower bound for such a quantity.
Before we propose an improved discrepancy measure, we first revisit the absolute discrepancy in [ 1]
by using Lemma 2.2 instead. This also serves as a review of the common developments in the DA
theory.
3 Warm-Up: Refined Absolute f-Divergence Domain Discrepancy
Based on Lemma 2.2, we refine the discrepancy measure of [1, Definition 3] as follows.
Definition 3.1. For a given h∈ H, theeDh,H
ϕdiscrepancy from µtoνis defined as
eDh,H
ϕ(µ||ν)≜sup
h′∈HEµ[ℓ(h, h′)]−Ih
ϕ,ν(ℓ◦h′),
where Ih
ϕ,ν(ℓ◦h′) = inf α∈R{Eν[ϕ∗(ℓ(h, h′) +α)]−α}.
Remark 3.1. Removing the absolute value function in eDh,H
ϕ(µ||ν)does not alter its non-negativity.
To see this, consider h′=h. By the definitions of ϕandϕ∗,infαϕ∗(α)−α=ϕ(1) = 0 , we have
Eµ[ℓ(h, h′)] =Ih
ϕ,ν(ℓ◦h′) = 0 . Consequently, since hexists in H,eDh,H
ϕ(µ||ν)≥0holds even
without the absolute value function. In addition, due to the absolute value function, the relation
between eDh,H
ϕ(µ||ν)with the one in [1] is no longer clear.
While the absolute value function is not required for ensuring non-negativity, it is crucial for deriving
the subsequent error bound for the target domain.
Lemma 3.1. Letλ∗= min h∗∈HRµ(h∗) +Rν(h∗), then for any h∈ H, we have
Rν(h)≤Rµ(h) +eDh,H
ϕ(µ||ν) +λ∗.
Remark 3.2 (Regarding λ∗).This error bound shares similarities with the previous works [ 3,4,9,10,
1]. For example, the third term λ∗is the ideal joint risk for the DA problem. As widely discussed in
prior studies, this term captures the inherent challenge in the DA task and might be inevitable [ 4,32].
However, it has also been pointed out that this λ∗term can be significantly pessimistic, particularly
in the case of conditional shift [ 33]. In fact, similar to the λ∗-free bound in [ 33, Theorem 4.1], it is a
simple matter to replace λ∗with the cross-domain error term min{Rν(fµ), Rµ(fν)}in Lemma 3.1
(and all the other target error bounds in this paper). See Appendix D.3 for details. Given that [ 1]
also uses λ∗, we use λ∗in the bounds for consistency in the remainder of this paper.
In the sequel, we will give a Rademacher complexity based generalization bound for the target error.
LetˆRS(F)denote the empirical Rademacher complexity of function class F={f:X → R}
for some sample S[34]. Notice that a shift transformation of a function class will not change
its Rademacher complexity, so the generalization bound based on eDh,H
ϕclosely resembles the one
presented in [ 1, Theorem 3], which contains a Lipschitz constant of ϕ∗. Here, we give a generalization
bound specialized for eDh,H
KL, wherein the Lipschitz constant of ϕ∗can be explicitly determined.
Theorem 3.1. Letℓ(·,·)∈[0,1]. Then, for any h∈ H, with probability at least 1−δ, we have
Rν(h)≤Rˆµ(h) +eDh,H
KL(ˆµ||ˆν) + 2eˆRS(Hℓ) + 4 ˆRT(Hℓ) +λ∗+O r
log(1/δ)
n+r
log(1/δ)
m!
,
where Hℓ={x7→ℓ(h(x), h′(x))|h, h′∈ H} .
[1] also applies Rademacher complexity-based bound to further upper bound λ∗by its empirical
version, namely ˆλ∗= min h∗∈HRˆµ(h∗) +Rˆν(h∗). However, since there is no target label available,
Rˆν(h∗)is still not computable, invoking ˆλ∗here has no clear advantage.
44 New f-Divergence-Based DA Theory
WhileeDh,H
ϕserves as a valid domain discrepancy measure in DA theory, it exaggerates the do-
main difference without appropriate control. It’s noteworthy that [ 1] attempts to demonstrate their
discrepancy with the absolute value function is upper bounded by Dϕ[1, Lemma 1], but this is
problematic; as supU≥0does not imply supU= sup |U|when Uis not a positive function.
Note that the error in [ 1, Lemma 1] is also identified in [ 35]. Furthermore, when designing their
f-DAL algorithm, they drop the absolute value function in their hypothesis-specified f-divergence
(see Eq. (5)). Consequently, the remarkable performance of f-DAL reveals a significant gap from
their theoretical foundation.
To bridge this gap, we introduce a new hypothesis-specific f-divergence-based DA framework. Our
new discrepancy measure is dubbed f-domain discrepancy, or f-DD, defined without the absolute
value function and with an affine transformation.
Definition 4.1 (f-DD) .For a given h∈ H, thef-DD measure Dh,H
ϕis defined as
Dh,H
ϕ(ν||µ)≜sup
h′∈H,t∈REν[t·ℓ(h, h′)]−Ih
ϕ,µ(tℓ◦h′),
where Ih
ϕ,µ(tℓ◦h′) = inf α{Eµ[ϕ∗(t·ℓ(h, h′) +α)]−α}.
Remark 4.1. IfHℓ=G, then Dh,H
ϕ(ν||µ)is an affine transformation of Lemma 2.1 (i.e. g→
tg+α) and a scaling transformation of Lemma 2.2. Importantly, unlike eDh,H
ϕ, it’s easy to see that
Dh,H
ϕ(ν||µ)≤Dϕ(ν||µ).
OurDh,H
ϕ(ν||µ)retains some common properties of the discrepancies defined in the DA theory
literature. First, as t= 0 leads to Eν[t·ℓ(h, h′)] = Ih
ϕ,µ(tℓ◦h′) = 0 , the non-negativity of
Dh,H
ϕ(ν||µ)is immediately justified. In addition, its asymmetric property is also preferred in DA, as
discussed in the previous works [ 9,10]. Moreover, when µ=ν, by the definition of ϕ∗, we have
Dh,H
ϕ(ν||µ) = 0 .
To present an error bound, the routine development, as in Lemma 3.1, is insufficient; we require a
general version of the “change of measure” inequality, as given below.
Lemma 4.1. Letψ(x)≜ϕ(x+ 1) , and ψ∗is its convex conjugate. For any h′, h∈ H andt∈R,
define Kh′,µ(t)≜infαEµ[ψ∗(t·ℓ(h, h′) +α)]. Let Kµ(t) = suph′∈HKh′,µ(t), then for any
h, h′∈ H,
K∗
µ(Eν[ℓ(h, h′)]−Eµ[ℓ(h, h′)])≤Dh,H
ϕ(ν||µ),
where K∗
µis the convex conjugate of Kµ.
It is worth reminding that KµandK∗
µboth depend on h, although we ignore hin the notations to
avoid cluttering. We are now ready to give a target error bound based on our f-DD.
Theorem 4.1. For any h∈ H, we have
Rν(h)≤Rµ(h) + inf
t≥0Dh,H
ϕ(ν||µ) +Kµ(t)
t+λ∗. (3)
Furthermore, let ℓ∈[0,1], ifϕis twice differentiable and ϕ′′is monotone, then
Rν(h)≤Rµ(h) +s
2
ϕ′′(1)Dh,H
ϕ(ν||µ) +λ∗. (4)
The following is an application of Eq. (4), where we consider the case of KL, namely Dh,H
KL(ν||µ).
Corollary 4.1. Letℓ∈[0,1], then for any h∈ H, we have Rν(h)≤Rµ(h) +q
2Dh,H
KL(ν||µ) +λ∗.
AsDh,H
KL(ν||µ)≤DKL(ν||µ), the bound in [ 19, Theorem 4.2] can be recovered by Corollary 4.1 for
the same bounded loss function. We also remark that the boundedness assumption in Corollary 4.1
can be further relaxed by applying the same sub-Gaussian assumption as in [19, Theorem 4.2].
5To give a generalization bound, the next step involves obtaining a concentration result for f-DD, as
given below.
Lemma 4.2. Letℓ∈[0,1]and let t0be the optimal tachieving the superum in Dh,H
ϕ(ν||µ). Assume
ϕ∗isL-Lipschitz, then for any given h, with probability at least 1−δ, we have
Dh,H
ϕ(ν||µ)≤Dh,H
ϕ(ˆν||ˆµ) + 2|t0|ˆRT(Hℓ) + 2L|t0|ˆRS(Hℓ) +O r
log(1/δ)
n+r
log(1/δ)
m!
.
Remark 4.2. The function ϕ∗needs not to be globally Lipschitz continuous; it can be locally Lipschitz
for a bounded domain. For example, in the case of KL, ϕ∗ise-Lipschitz continuous when ℓ∈[0,1].
Moreover, although the distribution-dependent quantity t0might not always have a closed-form
expression, in the case of χ2-DD, we know that t0=2(Eν[ℓ(h,h′∗)]−Eµ[ℓ(h,h′∗)])
Varµ(ℓ(h,h′∗)), where h′∗is the
corresponding optimal hypothesis.
It is also straightforward to obtain concentration results for Kµ(t)−Kˆµ(t)andRµ(h)−Rˆµ(h).
Substituting these results into Eq. (3) obtains the final generalization bound based on f-DD. Or
alternatively, one can directly substitute Lemma 4.2 into Eq. (4) (See Appendix D.7). However, in
this case, the empirical f-DD and other terms in Lemma 4.2 will feature a square root, slowing down
the convergence rate. We address this limitation in the following section.
5 Sharper Bounds via Localization
A localization technique in DA theory is recently studied in [ 10]. We now incorporate it into our
framework with some novel applications. First, we define a localized hypothesis space, formally
referred to as the (true) Rashomon set [36–38].
Definition 5.1 (Rashomon set) .Given a data distribution µ, a hypothesis space Hand a loss function
ℓ, for a Rashomon parameter r≥0, the Rashomon set Hris an r-level subset of Hdefined as:
Hr≜{h∈ H|Rµ(h)≤r}.
Notice that the Rashomon set Hrimplicitly depends on the data distribution. In this paper, we
specifically define Hrby the source domain distribution µ. Then, we define our localized f-DD:
Definition 5.2 (Localized f-DD) .For a given h∈ H r1, the localized f-DD from µtoνis defined as
Dh,Hr
ϕ(ν||µ)≜ sup
h′∈Hr,t≥0Eν[tℓ(h, h′)]−Ih
ϕ,µ(tℓ◦h′).
Remark 5.1. Compared with f-DD, localized f-DD restricts htoHr1andh′toHr, where r1and
rmay or may not be equal. In addition, the scaling parameter tis now restricted to R+
0instead of R.
Clearly, Dh,Hr
ϕ(µ||ν)is non-decreasing when rincreases, and it is upper bounded by Dh,H
ϕ(µ||ν).
As also mentioned at the end of the previous section, Eq. (4) of Theorem 4.1 (and Corollary 4.1),
involves a square root function for f-DD, potentially indicative of a slow-rate bound (e.g., if Dh,H
ϕ∈
O(1/n), then the bound decays with O(1/√n)). We now show that how the localized f-DD achieves
a fast-rate error bound.
Theorem 5.1. For any h∈ H r1and constants C1, C2∈(0,+∞)satisfying Kh′,µ(C1)≤
C1C2Eµ[ℓ(h, h′)]for any h′∈ H r, the following holds:
Rν(h)≤Rµ(h) +1
C1Dh,Hr
ϕ(ν||µ) +C2Rr
µ(h) +λ∗
r,
where λ∗
r= min h∗∈HrRµ(h∗) +Rν(h∗)andRr
µ(h) = suph′∈HrEµ[ℓ(h, h′)].
Remark 5.2. By the triangle property, Rr
µ(h)≤r+r1. In this case, a small r1will reduce both the
first term and the third term in the bound. However, determining the optimal value for ris intricate.
On the one hand, we hope ris small so that Dh,Hr
ϕ(ν||µ)andRr
µ(h)are both small. On the other
hand, if ris too small, specifically if r < λ∗, then it’s possible that λ∗
r> λ∗because the optimal
hypothesis minimizing the joint risk may not exist in Hr. Additionally, if both r1andrare too small,
the effective space for optimizing C1andC2may also be limited. Therefore, the value of rinvolves a
complex trade-off among the three terms.
6Overall, we expect r1to be as small as possible, aligning with the principle of empirical risk
minimization for the source domain in practice. We may let r > λ∗so that the optimal hypothesis is
guaranteed to exist in the Rashomon set Hr. Furthermore, if r+r1is unavoidably large, we prefer a
small C2so that C2R′
µ(h)is small. If λ∗itself is negligible, we use a vanishing r+r1. In this case,
one can focus on minimizing 1/C1while allowing C2to be large.
Combining Theorem 5.1 with Lemma 4.2 and following routine steps will obtain the generalization
bound based on f-DD, where the local Rademacher complexity [ 24] will be involved. However,
one may feel unsatisfied without an explicit clue for the condition Kh′,µ(C1)≤C1C2Eµ[ℓ(h, h′)]
in Theorem 5.1. In fact, exploring concentration results under this condition is a central theme in
obtaining fast-rate PAC-Bayesian generalization bounds [ 25,39–41,26] and the information-theoretic
generalization bounds [ 27–29,42]. Building upon similar ideas from these works, we now establish
a sharper generalization result for our localized KL-DD measure, where the fast-rate condition is
more explicit. One key ingredient is the following result.
Lemma 5.1. Letℓ∈[0,1], and let the constants C1>0andC2∈(0,1)satisfy the condition 
eC1−C1−1 
1−min{r1+r,1}+C2
2min{r1+r,1}
≤C1C2. Then, for any h∈ H r1and
h′∈ H r, we have
Eν[ℓ(h, h′)]≤inf
C1,C2Dh,Hr
KL(ν||µ)
C1+ (1 + C2)Eµ[ℓ(h, h′)].
Remark 5.3. As an extreme case, if r+r1→0(implying Eµ[ℓ(h, h′)] = 0 ), then let C2→
1, the condition in the lemma indicates that C1<1.26. Hence, the optimal bound becomes
Eν[ℓ(h, h′)]≤0.79Dh,Hr
KL(ν||µ). This bound remains valid even without r+r1→0. It holds when
the Rashomon set Hris “consistent” with a given h, meaning all hypotheses in Hrhave similar
predictions to hon the source domain data. As an another case, if r+r1≥1andsuph′Eµ[ℓ(h, h′)]
is also large, we may prefer a small C2, such as setting C2= 0.1. In this case, the condition
becomes 
eC1−C1−1
C2≤C1, suggesting that C1<3.74. This results in the optimal bound
Eν[ℓ(h, h′)]−Eµ[ℓ(h, h′)]≤0.27Dh,Hr
KL(ν||µ) + 0.1Eµ[ℓ(h, h′)]. The condition in Lemma 5.1 is
common in many fast-rate bound literature, such as [28, Theorem 3].
We are now in a position to give a fast-rate generalization bound for localized KL-DD.
Theorem 5.2. Under the conditions in Lemma 5.1. For any h∈ H r1, with probability at least 1−δ,
we have
Rν(h)≤Rˆµ(h) +Dh,Hr
KL(ˆν||ˆµ)
C1+C2Rr
µ(h) +O
ˆRT 
Hℓ
r
+ˆRS
Hℓ
max{r,r1}
+Olog(1/δ)
n+log(1/δ)
m
+O r
(r1+r) log(1 /δ)
n+r
rlog(1/δ)
m!
+λ∗
r.
Remark 5.4. Due to the non-negativity of f-DD, a similar generalization bound also applies to the
Jeffereys divergence (or symmetrized KL divergence) [ 43] counterpart, which is simply the sum of KL
divergence and reverse KL divergence (i.e. DKL(ˆµ||ˆν) + D KL(ˆν||ˆµ)). Furthermore, considering the
fact that DKL(ν||µ)≤log(1 + χ2(ν||µ))≤χ2(ν||µ)[30], one might anticipate a similar bound for
χ2-DD, which we defer to Appendix D.11.
This generalization bound suggests that when r+r1is small, not only are the first four terms
(including the local Rademacher complexities) reduced, but it also causes O 1
n+1
m
to dominate
the convergence rate of the bound. In practice, when empirical risk of the source domain is always
minimized to zero (i.e. the realizable case), then r1itself may have a fast vanishing rate (e.g.,
O(1/n)). In Appendix D.12, we provide a concrete example to further illustrate the superiority of
localized f-DD.
6 Algorithms and Experimental Results
6.1 Domain Adversarial Learning Algorithm
In a practical algorithm, the hypothesis space consists of two components: the representation part,
denoted as Hrep={hrep:X → Z} , where Zis the representation space (e.g., the hidden output
7of a neural network), and the classification part, denoted as Hcls={hcls:Z → Y} . Therefore,
the entire hypothesis space is given by H={hcls◦hrep|hrep∈ H rep, hcls∈ H cls}. The training
objective in f-DAL [1] is
min
h∈Hmax
h′∈H′Rˆµ(h) +η˜dˆµ,ˆν(h, h′). (5)
Here, ˜dˆµ,ˆν(h, h′) =Eˆνh
ˆℓ(h, h′)i
−Eˆµh
ϕ∗
ˆℓ(h, h′)i
, where ηis a trade-off parameter and ˆℓis
the surrogate loss used to evaluate the disagreement between handh′, which may or may not be the
same as ℓ. Note that, to better align with our framework, we change the order of ˆµandˆνin˜dˆµ,ˆν
in the original f-DAL. This modification is minor, as in either case, its optimal value belongs to
f-divergence (such as KL and reverse KL, χ2and reverse χ2).
Table 1: Accuracy (%) on the Office-31 benchmark.
Method A →W D →W W →D A →D D →A W →A Avg
ResNet-50 [44] 68.4 ±0.2 96.7 ±0.1 99.3 ±0.1 68.9 ±0.2 62.5 ±0.3 60.7 ±0.3 76.1
DANN [12] 82.0 ±0.4 96.9 ±0.2 99.1 ±0.1 79.7 ±0.4 68.2 ±0.4 67.4 ±0.5 82.2
MDD [9] 94.5 ±0.3 98.4 ±0.1 100.0±.0 93.5 ±0.2 74.6 ±0.3 72.2 ±0.1 88.9
KL [45] 87.9 ±0.4 99.0 ±0.2 100.0±.0 85.6 ±0.6 70.1 ±1.1 69.3 ±0.7 85.3
f-DAL [1] 95.4±0.7 98.8 ±0.1 100.0±.0 93.8 ±0.4 74.9 ±1.5 74.2 ±0.5 89.5
Ours (KL-DD) 94.9 ±0.7 98.7 ±0.1 100.0±.0 95.9±0.6 74.6 ±0.9 74.6 ±0.7 89.8
Ours ( χ2-DD) 95.3 ±0.2 98.7 ±0.1 100.0±.0 95.0 ±0.4 73.7 ±0.5 75.6±0.2 89.7
Ours (Jeffreys-DD) 94.9 ±0.7 99.1±0.2 100.0±.0 95.9±0.6 76.0±0.5 74.6 ±0.7 90.1
Table 2: Accuracy (%) on the Office-Home benchmark.
Method Ar →Cl Ar →Pr Ar →Rw Cl →Ar Cl →Pr Cl →Rw Pr →Ar Pr →Cl Pr →Rw Rw →Ar Rw →Cl Rw →Pr Avg
ResNet-50 [44] 34.9 50.0 58.0 37.4 41.9 46.2 38.5 31.2 60.4 53.9 41.2 59.9 46.1
DANN [12] 45.6 59.3 70.1 47.0 58.5 60.9 46.1 43.7 68.5 63.2 51.8 76.8 57.6
MDD [9] 54.9 73.7 77.8 60.0 71.4 71.8 61.2 53.6 78.1 72.5 60.2 82.3 68.1
f-DAL [1] 54.7 71.7 77.8 61.0 72.6 72.2 60.8 53.4 80.0 73.3 60.6 83.8 68.5
Ours (KL-DD) 55.3 70.8 78.6 62.6 73.8 73.6 62.7 53.4 80.9 75.2 61.3 84.2 69.4
Ours ( χ2-DD) 55.2 68.9 79.0 62.3 73.7 73.4 62.5 53.6 81.3 74.8 61.0 84.1 69.2
Ours (Jeffereys-DD) 55.5 74.9 79.5 64.3 73.8 73.9 63.9 54.7 81.3 75.2 61.6 84.2 70.2
Eq. (5) results in an adversarial training strategy. Specifically, the outer optimization spans the
entire hypothesis space. Meanwhile, within the inner optimization, given a h=hrep◦hcls, the
representation component hrepis shared for h′. In other words, the optimization is carried out for
h′inH′=Hcls◦hrep≜{hcls◦hrep|hcls∈ H cls}. The overall training framework of our f-DD is
illustrated in Figure 2 in Appendix.
Clearly, as also discussed previously, max h′˜dˆµ,ˆν(h, h′)≤max h′˜dˆµ,ˆν(h, h′), which presents a
clear gap between the theory and algorithms in [ 1]. In contrast, this training objective aligns more
closely with our Dh,H
ϕ. Formally, we have the following result.
Proposition 1. Letdˆµ,ˆν(h, h′) =Eˆνh
ˆℓ(h, h′)i
−Ih
ϕ,ˆµ(ˆℓ◦h′). Assume His sufficiently large s.t.
ˆℓ:X →R, we have max h′˜dˆµ,ˆν(h, h′) = max h′dˆµ,ˆν(h, h′) = Dh,H′
ϕ(ˆν||ˆµ)≤Dϕ(ˆν||ˆµ).
In our algorithm, we use dˆµ,ˆν(h, h′)to replace ˜dˆµ,ˆν(h, h′)in Eq. (5). Proposition 1 implies that either
the optimal ˜dˆµ,ˆν(h, h′)or the optimal dˆµ,ˆν(h, h′)coincides with f-DD. Moreover, as ˆℓis typically
unbounded in practice (e.g., cross-entropy loss), considering the unbounded nature of t, Proposition 1
suggests an equivalence between optimizing ˆℓ(h, h′)through h′and optimizing tℓ(h, h′)through
bothtandh′. In this sense, f-DAL has already considered the scaling transformation. Later on we
will empirically investigate whether explicitly optimizing tis necessary.
Furthermore, we highlight that the training objective in our algorithm, namely Eq. (5) with ˜dˆµ,ˆν(h, h′)
replaced by dˆµ,ˆν(h, h′), is de facto motivated by the insights from Section 5. In that section, we
demonstrate that when the source domain error is small, the f-DD term without the square-root
function provides a more accurate reflection of generalization (see, for instance, Remark 5.3). Given
that the empirical risk of the source domain is always minimized during training, we expect the final
hypothesis to fall within the subset of H(i.e. Rashomon set with r1).
86.2 Experiments
The goals of our experiments unfold in three aspects: 1) demonstrating that utilizing the f-DD
measure (i.e., using dˆµ,ˆν(h, h′)as the training objective) leads to superior performance on the
benchmarks; 2) confirming that the absolute discrepancy (i.e.˜dˆµ,ˆν(h, h′)) leads to a degradation in
performance; 3) showing that optimizing over tmay be unnecessary in practical scenarios.
Table 3: Accuracy (%) on the Digits datasets
Method M →U U→M Avg
DANN [12] 91.8 94.7 93.3
f-DAL [1] 95.3 97.3 96.3
Ours (KL-DD) 95.7 98.1 96.9
Ours ( χ2-DD) 95.4 97.3 96.4
Ours (Jeffereys-DD) 95.9 98.3 97.1Dataset We use three benchmark datasets:
1) the Office31 dataset [ 46], which comprises
4,652images across 31categories. This dataset
includes three domains: Amazon ( A), Web-
cam ( W) and DSLR ( D); 2) the Office-Home
dataset [ 47], consisting of 15,500images dis-
tributed among four domains: Artistic images
(Ar), Clip Art ( Cl), Product images ( Pr), and
Real-world images ( Rw); and 3) two Digits
datasets, MNIST and USPS [ 48] with the as-
sociated domain adaptation tasks MNIST →
USPS ( M→U) and USPS →MNIST ( U→M).
We follow the splits and evaluation protocol established by [ 49], where MNIST and USPS have
60,000and7,291training images, as well as 10,000and2,007test images, respectively.
Discrepancy Measures In our algorithms, we mainly focus on three specific discrepancy mea-
sures: KL-DD, χ2-DD and the weighted Jeffereys-DD. Specifically, the weighted Jeffereys-DD is
γ1DKL(ˆµ||ˆν) +γ2DKL(ˆν||ˆµ), where γ1andγ2are tunable hyper-parameters. We note that Jeffereys
divergence is not explored in [ 1] while it is also an f-divergence with ϕ(x) = ( x−1) log x, and
advantages of Jeffereys divergence are studied in [18, 45, 19].
Baselines and Implementation Details The primary baseline for our study is the preceding f-DAL
[1]. Note that, with the exception of Digits, the results reported by [ 1] forf-DAL rely on maximum
values obtained from their χ2-divergence and weighted Jensen-Shannon divergence across individual
sub-tasks. In our comparison, we also include DANN [ 12] and MDD [ 9] as they may be interpreted
as special cases of f-DAL. Furthermore, we compare the results reported by [ 45] on the Office-31
dataset, although denoted as “KL”, their method incorporates Jeffereys divergence in their algorithms.
0 1K 2K 3K 4K
Iterations05K10K15K20K25K30KEstimated f-divergence
W/ Abs.
W/O Abs.
3K 4K 4K 5K 5K 6K 6K
Iterations05K10K15K20K25K30K35K40KEstimated f-divergence
W/ Abs.
W/O Abs.
Figure 1: Failure of absolute discrepancy. The y-
axis is the estimated f-divergence.Our implementation closely follows [ 1]. For
Office-31 and Office-Home, we utilize a pre-
trained ResNet-50 [ 44] as the backbone network,
while both the primary classifier in hand the
auxiliary classifier in h′consist of two-layer
Leaky-ReLU networks. In the case of Digits, we
use LeNet [ 50] as the backbone network and a
two-layer ReLU network with Dropout ( 0.5) for
the classifiers. Other hyper-parameter settings
and the evaluation protocol remain consistent
with [ 1], and the reported results represent aver-
age accuracies over 3different random seeds.
Table 4: Comparison between KL-DD and OptKL-
DD
Method Office-31 Office-Home Digits
KL-DD 89.8 69.4 96.9
OptKL-DD 89.6 69.2 96.5Boosted Benchmark Performance by f-DD
Tables 1-3 collectively demonstrate the superior
performance of our weighted Jeffereys-DD com-
pared to other methods across the three bench-
marks. Notably, the most significant improve-
ment over f-DAL is observed on Office-Home
(70.2%vs.68.5%). Remarkably, this perfor-
mance even surpasses the combination of f-
DAL with a sampling-based implicit alignment
approach [ 51] (See Table 6 in Appendix), specifically designed to handle the label shift issues. In
addition, unlike findings in [ 1], where χ2outperforms other methods on nearly all tasks, our use of a
9tighter variational representation-based discrepancy reveals that χ2is no longer superior to KL. In
fact, our KL-DD slightly outperforms χ2-DD in all three benchmarks.
Failure of Absolute Discrepancy We also perform experiments on A→DandAr→Clusing the
absolute discrepancy (i.e., max˜dˆµ,ˆν(h, h′)). Specifically, we compare the χ2-based discrepancy
with (w/) and without (w/o) the absolute value function. Figure 1 illustrates that such a discrepancy can
easily explode during training, demonstrating its tendency to overestimate f-divergence. Additional
results for KL are given in Figure 3 in Appendix.
Optimizing over tIn the paragraph following Proposition 1, we discuss the observation that
optimizing over tmay not be necessary. Empirical evidence indicates that setting t= 1 with
hyper-parameter tuning (e.g., through η) obtains satisfactory performance. Now, let’s investigate the
selection of tfor KL-DD. Instead of using a stochastic gradient-based optimizer for updating t, we
invoke a quadratic approximation for the optimal t, as studied in [ 23]. Specifically, we define a Gibbs
measure dµ′=eℓ(h,h′)dµ
Eµ[eℓ(h,h′)], then the optimal t∗≈1 + ∆ t∗, where ∆t∗=Eν[ℓ(h,h′)]−Eµ′[ℓ(h,h′)]
Varµ′(ℓ(h,h′)). In-
terested readers can find a detailed derivation of this approximation in [ 23, Appendix B]. Substituting
t= 1 + ∆ t∗, we have the training objective for approximately optimal KL-DD (OptKL-DD). Table 4
presents an empirical comparison between OptKL-DD and the original KL-DD, where tis simply set
to 1. The results indicate that OptKL-DD does not provide any improvement on these benchmarks.
Similar observations also hold for χ2, in which case optimal thas an analytic form (see Appendix E),
suggesting that using t= 1, at least for KL and χ2, might be sufficient in practice.
Additional experimental results, including an ablation study on η, t-SNE [ 52] visualizations and other
comparisons, can be found in Appendix E.
7 Other Related Works
Domain Adaptation Apart from those mentioned in the introduction, various other discrepancy
measures are explored in DA theories and algorithms. These include the Wasserstein distance
[53–55], Maximum Mean Discrepancy [ 56,57], second-order statistics alignment [ 58,59], transfer
exponents [ 60,61], Integral Probability Metrics [ 62] and so on. Notably, [ 62] defines a general
Integral Probability Metrics (IPMs)-based discrepancy measure. It’s noteworthy that the intersection
of the IPMs family and the f-divergence family results in the total variation. Consequently, both
corresponding discrepancy measures can consider H∆H-divergence as a special case. Additionally,
one of our baseline models [ 45] diverges from the adversarial training strategy. Instead, they directly
minimize the KL divergence between two isotropic Gaussian distributions (source domain Gaussian
and target domain Gaussian) in the representation space. Here, the Gaussian means and variances
correspond to the hidden outputs of the representation network. For further literature on DA theory,
readers are directed to a recent survey by [63].
f-divergence Moreover, the combination of f-divergence and adversarial training schemes has
been extensively studied in generative models, including f-GAN [ 64,65],χ2-GAN [ 66] and others.
In the DA context, [ 67] introduce a f-divergence-based discrepancy measure while still relying on
Lemma 2.1 and focusing solely on the Jensen-Shannon case. Additionally, [ 68] investigates α-Rényi
divergence for multi-source DA, and [ 69] provides some intriguing interpretations of χ2-divergence-
based generalization bound for covariate shifts.
8 Conclusion and Future Work
In this work, we present an improved approach for integrating f-divergence into DA theory. Theoreti-
cal contributions include novel DA generalization bounds, including fast-rate bounds via localization.
On the practical front, the revised f-divergence-based discrepancy improves the benchmark perfor-
mance. Several promising future directions emerge from our work. Firstly, beyond its usefulness for
local Rademacher complexity, the Rashomon set Hralso relates to another generalization measure,
Rashomon ratio [ 37], which may give an alternative perspective on generalization in DA. Additionally,
exploring transfer component-based analysis [ 60] for tight minimax rates in DA, invoking a power
transformation instead of the affine transformation in f-DD, holds promise.
10Acknowledgments and Disclosure of Funding
This work is supported partly by an NSERC Discovery grant. The authors would like to thank Loïc
Simon for bringing reference [ 35] to their attention and for the insightful feedback provided on this
work. The authors also thank Fanshuang Kong for the extensive experimental guidance provided
throughout this project. Furthermore, the authors are thankful to the anonymous AC and reviewers
for their careful reading and valuable suggestions.
References
[1]David Acuna, Guojun Zhang, Marc T Law, and Sanja Fidler. f-domain adversarial learning:
Theory and algorithms. In International Conference on Machine Learning , pages 66–75. PMLR,
2021.
[2]Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence.
Dataset Shift in Machine Learning . The MIT Press, 2009.
[3]Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. Advances in neural information processing systems , 19, 2006.
[4]Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jen-
nifer Wortman Vaughan. A theory of learning from different domains. Machine Learning , 79
(1-2):151–175, 2010.
[5]Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning
bounds and algorithms. In The 22nd Conference on Learning Theory , 2009.
[6]Corinna Cortes, Mehryar Mohri, and Andrés Muñoz Medina. Adaptation algorithm and theory
based on generalized discrepancy. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining , pages 169–178, 2015.
[7]Pascal Germain, Amaury Habrard, François Laviolette, and Emilie Morvant. A pac-bayesian
approach for domain adaptation with specialization to linear classifiers. In International
conference on machine learning , pages 738–746. PMLR, 2013.
[8]Pascal Germain, Amaury Habrard, François Laviolette, and Emilie Morvant. A new pac-
bayesian perspective on domain adaptation. In International conference on machine learning ,
pages 859–868. PMLR, 2016.
[9]Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm
for domain adaptation. In International Conference on Machine Learning , pages 7404–7413.
PMLR, 2019.
[10] Yuchen Zhang, Mingsheng Long, Jianmin Wang, and Michael I Jordan. On localized discrepancy
for domain adaptation. arXiv preprint arXiv:2008.06242 , 2020.
[11] Changjian Shui, Qi Chen, Jun Wen, Fan Zhou, Christian Gagné, and Boyu Wang. A novel
domain adaptation theory with jensen–shannon divergence. Knowledge-Based Systems , 257:
109808, 2022.
[12] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural
networks. The journal of machine learning research , 17(1):2096–2030, 2016.
[13] XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence func-
tionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information
Theory , 56(11):5847–5861, 2010.
[14] Avraham Ruderman, Mark D Reid, Darío García-García, and James Petterson. Tighter varia-
tional representations of f-divergences via restriction to probability measures. In Proceedings
of the 29th International Coference on International Conference on Machine Learning , pages
1155–1162, 2012.
11[15] Jiantao Jiao, Yanjun Han, and Tsachy Weissman. Dependence measures bounding the explo-
ration bias for general measurements. In 2017 IEEE International Symposium on Information
Theory (ISIT) , pages 1475–1479. IEEE, 2017.
[16] Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov
process expectations for large time. iv. Communications on pure and applied mathematics , 36
(2):183–212, 1983.
[17] Xuetong Wu, Jonathan H Manton, Uwe Aickelin, and Jingge Zhu. Information-theoretic
analysis for transfer learning. In 2020 IEEE International Symposium on Information Theory
(ISIT) , pages 2819–2824. IEEE, 2020.
[18] Yuheng Bu, Gholamali Aminian, Laura Toni, Gregory W Wornell, and Miguel Rodrigues.
Characterizing and understanding the generalization error of transfer learning with gibbs
algorithm. In International Conference on Artificial Intelligence and Statistics , pages 8673–
8699. PMLR, 2022.
[19] Ziqiao Wang and Yongyi Mao. Information-theoretic analysis of unsupervised domain adapta-
tion. In International Conference on Learning Representations , 2023.
[20] Fanshuang Kong, Richong Zhang, Ziqiao Wang, and Yongyi Mao. On unsupervised domain
adaptation: Pseudo label guided mixup for adversarial prompt tuning. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 38, pages 18399–18407, 2024.
[21] Rohit Agrawal and Thibaut Horel. Optimal bounds between f-divergences and integral prob-
ability metrics. In International Conference on Machine Learning , pages 115–124. PMLR,
2020.
[22] Rohit Agrawal and Thibaut Horel. Optimal bounds between f-divergences and integral proba-
bility metrics. The Journal of Machine Learning Research , 22(1):5662–5720, 2021.
[23] Jeremiah Birrell, Markos A Katsoulakis, and Yannis Pantazis. Optimizing variational repre-
sentations of divergences and accelerating their statistical estimation. IEEE Transactions on
Information Theory , 68(7):4553–4572, 2022.
[24] Peter Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. Annals
of Statistics , 33(4):1497–1537, 2005.
[25] O Catoni. Pac-bayesian supervised classification: the thermodynamics of statistical learning.
Vol. 56. Lecture Notes - Monograph Series. Institute of Mathematical Statistics , 2007.
[26] Pierre Alquier. User-friendly introduction to pac-bayes bounds. arXiv preprint
arXiv:2110.11216 , 2021.
[27] Fredrik Hellström and Giuseppe Durisi. Fast-rate loss bounds via conditional information
measures with applications to neural networks. In 2021 IEEE International Symposium on
Information Theory (ISIT) , pages 952–957. IEEE, 2021.
[28] Fredrik Hellström and Giuseppe Durisi. A new family of generalization bounds using sample-
wise evaluated CMI. In Advances in Neural Information Processing Systems , 2022.
[29] Ziqiao Wang and Yongyi Mao. Tighter information-theoretic generalization bounds from
supersamples. In International Conference on Machine Learning . PMLR, 2023.
[30] Yury Polyanskiy and Yihong Wu. Information Theory: From Coding to Learning . Cambridge
university press, 2023.
[31] Aharon Ben-Tal and Marc Teboulle. An old-new concept of convex risk measures: The
optimized certainty equivalent. Mathematical Finance , 17(3):449–476, 2007.
[32] Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility of
unlabeled target samples. In Algorithmic Learning Theory: 23rd International Conference, ALT
2012, Lyon, France, October 29-31, 2012. Proceedings 23 , pages 139–153. Springer, 2012.
12[33] Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning , pages
7523–7532. PMLR, 2019.
[34] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds
and structural results. Journal of Machine Learning Research , 3(Nov):463–482, 2002.
[35] Rodrigue Siry, Ryan Webster, Loïc Simon, and Julien Rabin. On the theoretical equivalence of
several trade-off curves assessing statistical proximity. Journal of Machine Learning Research ,
24:185, 2024.
[36] Leo Breiman. Statistical modeling: The two cultures (with comments and a rejoinder by the
author). Statistical science , 16(3):199–231, 2001.
[37] Lesia Semenova, Cynthia Rudin, and Ronald Parr. On the existence of simpler machine
learning models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and
Transparency , pages 1827–1858, 2022.
[38] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. All models are wrong, but many are
useful: Learning a variable’s importance by studying an entire class of prediction models
simultaneously. J. Mach. Learn. Res. , 20(177):1–81, 2019.
[39] Yevgeny Seldin, François Laviolette, Nicolo Cesa-Bianchi, John Shawe-Taylor, and Peter Auer.
Pac-bayesian inequalities for martingales. IEEE Transactions on Information Theory , 58(12):
7086–7093, 2012.
[40] Ilya O Tolstikhin and Yevgeny Seldin. Pac-bayes-empirical-bernstein inequality. Advances in
Neural Information Processing Systems , 26, 2013.
[41] Jun Yang, Shengyang Sun, and Daniel M Roy. Fast-rate pac-bayes generalization bounds via
shifted rademacher processes. Advances in Neural Information Processing Systems , 32, 2019.
[42] Ziqiao Wang and Yongyi Mao. Sample-conditioned hypothesis stability sharpens information-
theoretic generalization bounds. In Advances in Neural Information Processing Systems , 2023.
[43] Harold Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings
of the Royal Society of London. Series A. Mathematical and Physical Sciences , 186(1007):
453–461, 1946.
[44] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
[45] A. Tuan Nguyen, Toan Tran, Yarin Gal, Philip Torr, and Atilim Gunes Baydin. KL guided
domain adaptation. In International Conference on Learning Representations , 2022.
[46] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to
new domains. In Proceedings of the 11th European conference on Computer vision: Part IV ,
pages 213–226, 2010.
[47] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan.
Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages 5018–5027, 2017.
[48] Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on
pattern analysis and machine intelligence , 16(5):550–554, 1994.
[49] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. Advances in neural information processing systems , 31, 2018.
[50] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
13[51] Xiang Jiang, Qicheng Lao, Stan Matwin, and Mohammad Havaei. Implicit class-conditioned
domain alignment for unsupervised domain adaptation. In International Conference on Machine
Learning , pages 4816–4827. PMLR, 2020.
[52] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research , 9(11), 2008.
[53] Rémi Flamary, Nicholas Courty, Davis Tuia, and Alain Rakotomamonjy. Optimal transport for
domain adaptation. IEEE Trans. Pattern Anal. Mach. Intell , 1(1-40):2, 2016.
[54] Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution
optimal transportation for domain adaptation. Advances in neural information processing
systems , 30, 2017.
[55] Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation
learning for domain adaptation. In Thirty-second AAAI conference on artificial intelligence ,
2018.
[56] Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Schölkopf, and Alex Smola.
Correcting sample selection bias by unlabeled data. Advances in neural information processing
systems , 19, 2006.
[57] Boqing Gong, Kristen Grauman, and Fei Sha. Connecting the dots with landmarks: Discrimina-
tively learning domain-invariant features for unsupervised domain adaptation. In International
conference on machine learning , pages 222–230. PMLR, 2013.
[58] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation.
InEuropean conference on computer vision , pages 443–450. Springer, 2016.
[59] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In
Proceedings of the AAAI conference on artificial intelligence , 2016.
[60] Steve Hanneke and Samory Kpotufe. On the value of target data in transfer learning. Advances
in Neural Information Processing Systems , 32, 2019.
[61] Steve Hanneke, Samory Kpotufe, and Yasaman Mahdaviyeh. Limits of model selection under
transfer learning. arXiv preprint arXiv:2305.00152 , 2023.
[62] Sofien Dhouib and Setareh Maghsudi. Connecting sufficient conditions for domain adaptation:
source-guided uncertainty, relaxed divergences and discrepancy localization. arXiv preprint
arXiv:2203.05076 , 2022.
[63] Ievgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Younes Bennani. A survey
on domain adaptation theory. arXiv preprint arXiv:2004.11829 , 2020.
[64] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural
samplers using variational divergence minimization. Advances in neural information processing
systems , 29, 2016.
[65] Jiaming Song and Stefano Ermon. Bridging the gap between f-gans and wasserstein gans. In
International Conference on Machine Learning , pages 9078–9087. PMLR, 2020.
[66] Chenyang Tao, Liqun Chen, Ricardo Henao, Jianfeng Feng, and Lawrence Carin Duke. χ2
generative adversarial network. In International conference on machine learning , pages 4887–
4896. PMLR, 2018.
[67] Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. Domain adaptation with
asymmetrically-relaxed distribution alignment. In International conference on machine learning ,
pages 6872–6881. PMLR, 2019.
[68] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Multiple source adaptation and
the rényi divergence. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
Intelligence , pages 367–374, 2009.
14[69] David Bruns-Smith, Alexander D’Amour, Avi Feller, and Steve Yadlowsky. Tailored overlap for
learning under distribution shift. In NeurIPS 2022 Workshop on Distribution Shifts: Connecting
Methods and Applications , 2022.
[70] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning .
MIT press, 2018.
[71] Colin McDiarmid. Concentration , pages 195–248. Springer Berlin Heidelberg, 1998.
[72] Stéphane Boucheron, Olivier Bousquet, and Gábor Lugosi. Theory of classification: A survey
of some recent advances. ESAIM: probability and statistics , 9:323–375, 2005.
[73] Gavin E. Crooks. Inequalities between the jenson-shannon and jeffreys divergences. In Tech.
Note 004 , 2008.
[74] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition , pages 248–255, 2009.
[75] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. In International Conference on Learning Representations , 2021.
[76] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
of the IEEE/CVF international conference on computer vision , pages 10012–10022, 2021.
[77] Tao Sun, Cheng Lu, Tianshuo Zhang, and Haibin Ling. Safe self-refinement for transformer-
based domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 7191–7200, 2022.
[78] Jinjing Zhu, Haotian Bai, and Lin Wang. Patch-mix transformer for unsupervised domain
adaptation: A game perspective. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 3561–3571, 2023.
15Appendices
The structure of Appendix is outlined as follows: Section A provides a table summarizing the
notations used throughout the paper. In Section B, we present a collection of technical lemmas
crucial to our analysis. Additional variational representations for certain f-divergences are explored
in Section C, where we also further discuss on why using a tighter variational representation of
f-divergence is important. Section D restates our theoretical results, provides detailed proofs, and
introduces supplementary theoretical findings. For more details about experiments and additional
empirical results, refer to Section E.
A Summary of Notations
For easy reference, Table 5 summarizes the key notations used in this paper.
Table 5: Summary of notations.
Notation Definition
X,Y,H input, label and hypothesis space
µ,ν source domain distribution and target domain distribution
S,T source sample S={(Xi, Yi)}n
i=1and target sample T={Xi}m
i=1
ˆµ,ˆν empirical source distribution and empirical target distribution
ℓ(h(x), h′(x))orℓ(h, h′) loss between the predictions returned by handh′
Rν(h),Rµ(h) Rν(h)≜Eν[ℓ(h(X), Y)],Rµ(h)≜Eµ[ℓ(h(X), Y)]
Rˆµ(h)1
nPn
i=1ℓ(h(X), Y)
Dϕ(P||Q) EQh
ϕ
dP
dQi
;f-divergence between PandQ
eDh,H
ϕ(µ||ν) suph′∈HEµ[ℓ(h, h′)]−Ih
ϕ,ν(ℓ◦h′)
λ∗minh∗∈HRµ(h∗) +Rν(h∗)
ˆRS(F) Eε1:n
supf∈F1
nPn
i=1εif(Zi)
; (empirical) Rademacher complexity
Hℓ{x7→ℓ(h(x), h′(x))|h, h′∈ H}
Dh,H
ϕ(ν||µ) suph′∈H,t∈REν[t·ℓ(h, h′)]−Ih
ϕ,µ(tℓ◦h′);f-DD
Ih
ϕ,µ(tℓ◦h′) inf α{Eµ[ϕ∗(t·ℓ(h, h′) +α)]−α}
ψ(x) ϕ(x+ 1)
Kh′,µ(t) inf αEµ[ψ∗(t·ℓ(h, h′) +α)]
Kµ(t) suph′∈HKh′,µ(t)
t0 optimal tachieving the superum in Dh,H
ϕ(ν||µ)
Hr {h∈ H|Rµ(h)≤r}
Dh,Hr
ϕ(ν||µ) suph′∈Hr,t≥0Eν[tℓ(h, h′)]−Ih
ϕ,µ(tℓ◦h′)
λ∗
r minh∗∈HrRµ(h∗) +Rν(h∗)
Rr
µ(h) suph′∈HrEµ[ℓ(h, h′)]
Hrep,Hcls {hrep:X → Z} and{hcls:Z → Y}
˜dˆµ,ˆν(h, h′) Eˆµh
ˆℓ(h, h′)i
−Eˆνh
ϕ∗
ˆℓ(h, h′)i
ˆℓ surrogate loss used in practical algorithms
dˆµ,ˆν(h, h′) Eˆνh
ˆℓ(h, h′)i
−Ih
ϕ,ˆµ(ˆℓ◦h′)
B Some Technical Lemmas
The well-known Donsker-Varadhan representation of KL divergence is given below.
Lemma B.1 (Donsker and Varadhan’s variational formula) .LetQ,Pbe probability measures on
Θ, for any bounded measurable function f: Θ→R, we have DKL(Q||P) = supfEθ∼Q[f(θ)]−
logEθ∼P[expf(θ)].
16The following lemma is largely used.
Lemma B.2. Letϕ∗be the Fenchel conjugate of ϕand let ψ(x) =ϕ(x+1), then ψ∗(x) =ϕ∗(x)−x.
Furthermore, if ϕsatisfies ϕ(1) = 0 , we have ψ∗(x)≥0, or equivalently ϕ∗(x)≥x.
Proof. By definition, ψ∗(x) = suptxt−ϕ(t+ 1) . Lett′=t+ 1, then ψ∗(x) = supt′x(t′−1)−
ϕ(t′) =ϕ∗(x)−x. Ifϕ(1) = 0 , then ϕ∗(x) = suptxt−ϕ(t)≥x−ϕ(1) = x. This completes the
proof.
Definition B.1 (Empirical Rademacher Complexity [ 34]).For any function class F={f:Z →R},
the empirical Rademacher complexity is defined as ˆRS(F)≜Eε1:n
supf∈F1
nPn
i=1εif(Zi)
,
where S={Zi}n
i=1andε1:nis a sequence of i.i.d. Rademacher variables.
The Rademacher complexity-based generalization bound is given below.
Lemma B.3 ([70, Theorem 3.3]) .LetFbe a family of functions mapping from Zto[0,1]and let
i.i.d. sample S={Zi}n
i=1, we have ES
supf∈F 
EZ[f(Z)]−1
nPn
i=1f(Zi)
≤2ESh
ˆRS(F)i
.
Then for any f∈ F, with probability at least 1−δover the draw of S, we have
E[f(Z)]≤1
nnX
i=1f(Zi) + 2 ˆRS(F) +O r
log(1/δ)
n!
.
The following result is from [ 21, Proposition 4.1.], and the corresponding detailed proof is given in
[22, Corollary 6.3.11].
Lemma B.4. Assume that ϕis twice differentiable on its domain and ϕ′′is monotone. Let ψ(x) =
ϕ(x+ 1) and let f:X → [a, b], then for each t∈R, we have infαEX[ψ∗(tf(X) +α)]≤
(b−a)2
2ϕ′′(1)·t2.
The following two lemmas are used for deriving fast-rate bound.
Lemma B.5 ([71, Lemma 2.8]) .LetB(x) =ex−x−1
x2be the Bernstein function. If a random variable
Xsatisfies E[X] = 0 andX≤b, thenE
eX
≤eB(b)E[X2].
Proof. It’s easy to verify that B(x)is an increasing function for x >0. Thus, B(x)≤B(b)for
x≤b. Then,
ex=x+ 1 + x2B(x)≤x+ 1 + x2B(b).
For the bounded random variable Xwith zero mean, we have
E
eX
≤E[X] + 1 + E
X2B(b)
≤eB(b)E[X2]. (6)
The last inequality is by ex≥x+ 1. This completes the proof.
Lemma B.6 (Talagrand’s inequality [ 72, Theorem 5.4]) .Letb >0and let Fbe a class of functions
fromX →R. Assume that supf∈FE[f(X)]−f(x)≤bfor any x. Then, with probability at least
1−δ,
sup
f∈FE[f(X)]−1
nnX
i=1f(Xi)≤2E"
sup
f∈FE[f(X)]−1
nnX
i=1f(Xi)#
+r
2Var( f(X)) log(1 /δ)
n+4blog(1/δ)
3n.
C Variational Representations Beyond KL Divergence
C.1 χ2-Divergence
Forχ2-divergence, let ϕ(x) = (x−1)2forx >0, then ϕ∗(y) =y2
4+y.
Simply plugging ϕ∗into Lemma 2.1 will give us
χ2(P||Q) = sup
gEP[g(θ)]−EQ[g(θ)]−EQh
(g(θ))2i
4. (7)
17Similarly, simply plugging ϕ∗into Lemma 2.2 will give us
χ2(P||Q) = sup
gEP[g(θ)]−EQ[g(θ)]−VarQ(g(θ))
4, (8)
where the optimal α=EQ[g(θ)]in Lemma 2.2.
Notice that VarQ(g(θ))≤EQh
(g(θ))2i
, we can see that, as a lower bound of χ2, Eq. (8) is tighter
than Eq. (7).
If we further consider the affine transformation of Lemma 2.1 and the scaling transformation of
Lemma 2.2, we can recover Hammersley-Chapman-Robbins lower bound and the Cramér-Rao and
van Trees lower bounds [30].
More precisely, let g(·) =ag(·) +bbe substituted to Eq. (7), where a, b∈R, it is easy to see that
χ2(P||Q) = sup
g,a,bEP[ag(θ) +b]−EQ"
(ag(θ) +b)2
4+ag(θ) +b#
= sup
g(EP[g(θ)]−EQ[g(θ)])2
VarQ(g(θ)),
(9)
where the optimal a∗=2(EP[g(θ)]−EQ[g(θ)])
VarQ(g(θ))andb∗=−a∗EQ[g(θ)].
Letg(·) =tg(·)be substituted to Eq. (8), where t∈R, then we have
χ2(P||Q) = sup
g,tEP[tg(θ)]−EQ[tg(θ)]−t2VarQ(g(θ))
4= sup
g(EP[g(θ)]−EQ[g(θ)])2
VarQ(g(θ)),
where the optimal t∗=a∗=2(EP[g(θ)]−EQ[g(θ)])
VarQ(g(θ)). Therefore, Lemma 2.2 recovers Eq. (9).
C.2 Reverse KL Divergence
The reverse KL divergence DKL(Q||P)can be simply obtained by exchanging the orders of Pand
Qin the KL divergence DKL(P||Q). In this case, the DV representation of reverse KL is
DKL(Q||P) = sup
g∈GEQ[g(θ)]−logEPh
eg(θ)i
. (10)
To obtain this from Lemma 2.2, let ϕ(x) =−log(x), then plugging ϕ∗(y) =−1−log(−y)into
Lemma 2.2, we have
DKL(Q||P) = sup
g,αEQ[g(θ) +α] + 1 + EP[log(−g(θ)−α)].
Now reparameterizng g+α→ −eg+α, we have
DKL(Q||P) = sup
g,α−EQh
eg(θ)+αi
+ 1 + EP[g(θ) +α] = sup
gEQ[g(θ)]−logEPh
eg(θ)i
.
This recovers Eq. (10).
C.3 Jeffereys Divergence
Jeffreys divergence, a member of the f-divergence family with ϕ(x) = ( x−1) log x, is the sum
of KL divergence and reverse KL divergence. In our algorithm implementation, we obtain the
variational formula for Jeffreys divergence by simply combining the variational representations of
KL and reverse KL, namely
sup
g∈GEP[g(θ)]−logEQh
eg(θ)i
+ sup
g∈GEQ[g(θ)]−logEPh
eg(θ)i
.
Moreover, there is a tight relationship between Jensen-Shannon (JS) divergence and Jeffreys diver-
gence [ 73]:DJS(P||Q)≤n
1
4DJeffereys (P||Q),log2
1+e−DJeffereys(P||Q)o
.Although we don’t directly
use JS divergence in our algorithms, minimizing Jeffreys divergence simultaneously minimizes JS
divergence.
18C.4 Understanding the Importance of a Tighter Variational Representation
In simplified terms, Lemma 2.1 gives a variational representation expressed as:
Dϕ(P||Q) = sup
g∈GV1(g),
while Lemma 2.2 provides a variational representation as:
Dϕ(P||Q) = sup
g∈GV2(g).
where we have expressed the quantities to be maximized in Lemma 2.1 and Lemma 2.2 respectively
asV1(g)andV2(g). It is clear that for each specific g∈ G, we have V2(g)≥V1(g)(since Lemma 2.1
is a special case of Lemma 2.2 where α= 0). This is when we say V2is a (point-wise) tighter
variational representation than V1.
In the two optimization problems above, the optimal solution in both cases gives rise to same
divergence. That is, when the two optimization problems can be solved perfectly, there is no
advantage of one representation over the other. However, when the global optimal of the optimization
problems is not attainable, any gobtained in an optimization effort gives rise to an estimate of the
divergence, namely as V1(g)in the first case and as V2(g)in the second. It is then clear that due
toV2being a tighter variational representation, V2consistently provides a closer approximation to
Dϕ(P||Q)thanV1for the same g.
This virtue carries over to the min-max training strategy of UDA. In that case, gis reparameterized
bytℓ◦h′. We desire the inner maximization, whether of V2or ofV1, to give rise to a good estimate
of a similar divergence while the outer minimization aims to reduce this divergence. If the inner
maximization is perfect, then the choice between V1andV2makes no difference. However, if the
inner maximization isn’t perfect, V2gives a point-wise superior result compared to V1, leading to a
flatter optimization region around the optimal solution. In other words, using V2as the maximization
objective gives a better estimate of the corresponding f-divergence than V1.
D Omitted Proofs and Additional Results
D.1 Proof of Lemma 3.1
Lemma 3.1. Letλ∗= min h∗∈HRµ(h∗) +Rν(h∗), then for any h∈ H, we have
Rν(h)≤Rµ(h) +eDh,H
ϕ(µ||ν) +λ∗.
Proof. For a given h∈ H, letα∈R,
Rν(h)−Rµ(h)≤Eν[ℓ(h, h∗)] +Eν[ℓ(h∗, fν)]−Eµ[ℓ(h, fµ)] (11)
= inf
αEν[ℓ(h, h∗) +α]−α+Eν[ℓ(h∗, fν)]−Eµ[ℓ(h, fµ)]
≤inf
αEν[ϕ∗(ℓ(h, h∗) +α)]−α+Eν[ℓ(h∗, fν)]−Eµ[ℓ(h, fµ)] (12)
≤inf
αEν[ϕ∗(ℓ(h, h∗) +α)]−α−Eµ[ℓ(h, h∗)] +Eν[ℓ(h∗, fν)] +Eµ[ℓ(h∗, fµ)]
(13)
≤inf
αEν[ϕ∗(ℓ(h, h∗) +α)]−α−Eµ[ℓ(h, h∗)]+λ∗
≤eDh,H
ϕ(µ||ν) +λ∗,
where Eq. (11) and (13) are by the triangle property of loss function, Eq. (12) is by Lemma B.2, and
the last inequality is by the definition of f-DD. This completes the proof.
D.2 Proof of Theorem 3.1
Theorem 3.1. Letℓ(·,·)∈[0,1]. Then, for any h∈ H, with probability at least 1−δ, we have
Rν(h)≤Rˆµ(h) +eDh,H
KL(ˆµ||ˆν) + 2eˆRS(Hℓ) + 4 ˆRT(Hℓ) +λ∗+O r
log(1/δ)
n+r
log(1/δ)
m!
,
where Hℓ={x7→ℓ(h(x), h′(x))|h, h′∈ H} .
19Proof. We first prove a concentration result for the eDh,H
KL(µ||ν)measure.
Lemma D.1. Assume ℓ∈ [0,1]. Let βbe a constant such that β≤
minn
Eνh
eℓ(h,h′)i
,Eˆνh
eℓ(h,h′)io
for any ˆν, h, h′, then for any given h, with probability at
least1−δ, we have
eDh,H
KL(µ||ν)−eDh,H
KL(ˆµ||ˆν)≤2ˆRS(Hℓ) +2
βˆRT(exp◦Hℓ) +O r
log(1/δ)
n+r
log(1/δ)
m!
.
Proof of Lemma D.1. For a fixed h∈ H, we have
eDh,H
KL(µ||ν)−eDh,H
KL(ˆµ||ˆν)
= sup
h′∈HEµ[ℓ(h, h′)]−logEνh
eℓ(h,h′)i−sup
h′∈HEˆµ[ℓ(h, h′)]−logEˆνh
eℓ(h,h′)i
≤sup
h′∈HEµ[ℓ(h, h′)]−logEνh
eℓ(h,h′)i
−
Eˆµ[ℓ(h, h′)]−logEˆνh
eℓ(h,h′)i (14)
≤sup
h′∈H|Eµ[ℓ(h, h′)]−Eˆµ[ℓ(h, h′)]|+ sup
h′∈HlogEνh
eℓ(h,h′)i
−logEˆνh
eℓ(h,h′)i
≤2ˆRS(Hℓ) +O r
log(1/δ)
n!
+ sup
h′∈HlogEˆνh
eℓ(h,h′)i
−logEνh
eℓ(h,h′)i
| {z }
A, (15)
where Eq. (14) is by sup|U| −sup|V| ≤sup|U| − |V| ≤sup||U| − |V|| ≤ sup|U−V|and
Eq. (15) is by Lemma B.3.
Letβbe a constant such that β≤minn
Eνh
eℓ(h,h′)i
,Eˆνh
eℓ(h,h′)io
for any ˆν, h, h′. For a given
h′, W.L.O.G. assume that Eˆνh
eℓ(h,h′)i
≥Eνh
eℓ(h,h′)i
, then
A= logEˆνh
eℓ(h,h′)i
Eν
eℓ(h,h′)=log
1 +Eˆνh
eℓ(h,h′)i
Eν
eℓ(h,h′)−1

≤Eˆνh
eℓ(h,h′)i
Eν
eℓ(h,h′)−1
=1
Eν
eℓ(h,h′)
Eˆνh
eℓ(h,h′)i
−Eνh
eℓ(h,h′)i
≤1
βEˆνh
eℓ(h,h′)i
−Eνh
eℓ(h,h′)i, (16)
where the first inequality is by log(x+ 1)≤xforx >0, and the last inequality is by the definition
ofβ.
Plugging the inequality above into Eq. (15), we have
eDh,H
KL(µ||ν)−eDh,H
KL(ˆµ||ˆν)
≤2ˆRS(Hℓ) +O r
log(1/δ)
n!
+ sup
h′∈H1
βEˆνh
eℓ(h,h′)i
−Eνh
eℓ(h,h′)i
≤2ˆRS(Hℓ) +2
βˆRT(exp◦Hℓ) +O r
log(1/δ)
n+r
log(1/δ)
m!
, (17)
where the last inequality is again by using Lemma B.3. This completes the proof.
20Recall Lemma 3.1, we have
Rν(h)≤Rµ(h) +eDh,H
ϕ(µ||ν) +λ∗
≤Rˆµ(h) + 2 ˆRS(Hℓ) +O r
log(1/δ)
n!
+eDh,H
ϕ(µ||ν) +λ∗(18)
≤Rˆµ(h) + 4 ˆRS(Hℓ) +2
βˆRT(exp◦Hℓ) +eDh,H
ϕ(ˆµ||ˆν) +λ∗+O r
log(1/δ)
n+r
log(1/δ)
m!
(19)
≤Rˆµ(h) + 4 ˆRS(Hℓ) +2e
βˆRT(Hℓ) +eDh,H
ϕ(ˆµ||ˆν) +λ∗+O r
log(1/δ)
n+r
log(1/δ)
m!
,
(20)
where Eq. (18) and Eq. (19) are by Lemma B.3 and Lemma D.1, respectively. For the last inequality,
notice that the loss is bounded between [0,1]so the exponential function is e-Lipschitz in this domain,
then we apply the Talagrand’s lemma [70, Lemma 5.7], i.e. ˆRT(exp◦Hℓ)≤eˆRT(Hℓ).
Finally, due to the boundedness of the loss, there always exists a constant β∈[1, e]such that
β≤minn
Eνh
eℓ(h,h′)i
,Eˆνh
eℓ(h,h′)io
, and a simple choice is β= 1. Plugging β= 1into Eq. (20)
will complete the proof.
D.3 Joint Error-Free Target Error Bound
[33] presents a λ∗-free target error bound by using the cross-domain error min{Rν(fµ), Rµ(fν)}.
We now illustrate that it is possible to incorporate this term into all of our target error bounds.
In particular, the λ∗term appears due to the following triangle inequalities:
Rν(h)−Rµ(h)≤Eν[ℓ(h, h∗)] +Eν[ℓ(h∗, fν)]−Eµ[ℓ(h, fµ)]
≤Eν[ℓ(h, h∗)] +Eν[ℓ(h∗, fν)]−Eµ[ℓ(h, h∗)] +Eµ[ℓ(h∗, fµ)]
=Eν[ℓ(h, h∗)]−Eµ[ℓ(h, h∗)]| {z }
A1+λ∗.
To avoid λ∗, we use fµandfνinstead as the middle hypothesis for the application of triangle
inequalities:
Rν(h)−Rµ(h)≤Eν[ℓ(h, fν)]−Eµ[ℓ(h, fν)]| {z }
A2+Eµ[ℓ(fν, fµ)], (21)
Rν(h)−Rµ(h)≤Eν[ℓ(h, fµ)]−Eµ[ℓ(h, fµ)]| {z }
A3+Eν[ℓ(fµ, fν)]. (22)
If|A2|and|A3|are upper bounded by some hypothesis class-specific divergence dH(µ, ν), then
combining Eq (21) and Eq. (22) will give us:
Rν(h)≤Rµ(h) +dH(µ, ν) + min {Rν(fµ), Rµ(fν)},
where Rν(fµ) =Eν[ℓ(fµ, fν)]andRµ(fν) =Eµ[ℓ(fν, fµ)]. Clearly, if His large enough such
thatfµandfνare all achievable, |A1|,|A2|and|A3|are all upper bounded by our f-DD defined in
this paper. In fact, as long as fµandfνare close enough to H, ourf-DD can upper bound |A2|and
|A3|, with similar adaptations used in [33, Lemma 4.1].
D.4 Proof of Lemma 4.1
Lemma 4.1. Letψ(x)≜ϕ(x+ 1) , and ψ∗is its convex conjugate. For any h′, h∈ H andt∈R,
define Kh′,µ(t)≜infαEµ[ψ∗(t·ℓ(h, h′) +α)]. Let Kµ(t) = suph′∈HKh′,µ(t), then for any
h, h′∈ H,
K∗
µ(Eν[ℓ(h, h′)]−Eµ[ℓ(h, h′)])≤Dh,H
ϕ(ν||µ),
where K∗
µis the convex conjugate of Kµ.
21Proof. By Lemma B.2, we know that
Kh′,µ(t) = inf
αEµ[ψ∗(tℓ(h, h′) +α)]
= inf
αEµ[ϕ∗(tℓ(h, h′) +α)]−α−tEµ[ℓ(h, h′)] =Ih
ϕ,µ(tℓ◦h′)−tEµ[ℓ(h, h′)].
According to the definition of Kµ, for each t, we have
Kh′,µ(t) =Ih
ϕ,µ(tℓ◦h′)−tEµ[ℓ(h, h′)]≤Kµ(t).
This indicates that
tEν[ℓ(h, h′)]−tEν[ℓ(h, h′)] +Ih
ϕ,µ(tℓ◦h′)−tEµ[ℓ(h, h′)]≤Kµ(t).
Since this inequality holds for any t, by rearranging terms, we have
sup
tt(Eν[ℓ(h, h′)]−Eµ[ℓ(h, h′)])−Kµ(t)≤sup
ttEν[ℓ(h, h′)]−Ih
ϕ,µ(tℓ◦h′)≤Dh,H
ϕ(ν||µ).
(23)
Notice that the most left hand side is the definition of K∗
µ, we thus have
K∗
µ(Eν[ℓ(h, h′)]−Eµ[ℓ(h, h′)])≤Dh,H
ϕ(ν||µ).
This completes the proof.
D.5 Proof of Theorem 4.1
Theorem 4.1. For any h∈ H, we have
Rν(h)≤Rµ(h) + inf
t≥0Dh,H
ϕ(ν||µ) +Kµ(t)
t+λ∗. (3)
Furthermore, let ℓ∈[0,1], ifϕis twice differentiable and ϕ′′is monotone, then
Rν(h)≤Rµ(h) +s
2
ϕ′′(1)Dh,H
ϕ(ν||µ) +λ∗. (4)
Proof. We first follow the similar developments in Lemma 3.1.
Rν(h)−Rµ(h)≤Eν[ℓ(h, h∗)] +Eν[ℓ(h∗, fν)]−Eµ[ℓ(h, fµ)]
≤Eν[ℓ(h, h∗)] +Eν[ℓ(h∗, fν)]−Eµ[ℓ(h, h∗)] +Eµ[ℓ(h∗, fµ)]
=Eν[ℓ(h, h∗)]−Eµ[ℓ(h, h∗)]| {z }
A+λ∗.
By Lemma 4.1, we know that K∗
µ(A)≤Dh,H
ϕ(ν||µ). This indicates that
Dh,H
ϕ(ν||µ)≥sup
t∈RtA−Kµ(t)≥sup
t≥0tA−Kµ(t).
We hint that with more careful handling of the aforementioned step, one can obtain a bound for the
absolute mean deviation (i.e. |A|) in the end. However, the current development suffices for our
immediate objectives.
Notice that when t= 0, this holds trivially. When t >0, the above inequality is equivalent to
A≤inf
t>01
t
Kµ(t) + Dh,H
ϕ(ν||µ)
.
Hence, we have
A≤inf
t≥01
t
Kµ(t) + Dh,H
ϕ(ν||µ)
.
22Plugging the bound for Ainto the inequality at the beginning of the proof, we obtain the first desired
result
Rν(h)−Rµ(h)≤inf
t≥01
t
Kµ(t) + Dh,H
ϕ(ν||µ)
+λ∗.
For the second part, we apply Lemma B.4 here, then it is easy to see that Kµ(t)≤t2
2ϕ′′(1). Therefore,
Rν(h)−Rµ(h)≤inf
tt
2ϕ′′(1)+Dh,H
ϕ(ν||µ)
t+λ∗=s
2Dh,H
ϕ(ν||µ)
ϕ′′(1)+λ∗.
where the last equality is obtained by letting t=q
2ϕ′′(1)Dh,H
ϕ(ν||µ). This completes the proof.
D.6 Proof of Corollary 4.1
Corollary 4.1. Letℓ∈[0,1], then for any h∈ H, we have Rν(h)≤Rµ(h) +q
2Dh,H
KL(ν||µ) +λ∗.
Proof. In the case of KL, ϕ(x) =xlogx−x+ 1, and 1/ϕ′′(x) =x. Hence, this corollary can be
directly obtained from Eq. (4) in Theorem 4.1 by substituting ϕ′′(1) = 1 .
D.7 Proof of Lemma 4.2 and Generalization Bounds for f-DD
Lemma 4.2. Letℓ∈[0,1]and let t0be the optimal tachieving the superum in Dh,H
ϕ(ν||µ). Assume
ϕ∗isL-Lipschitz, then for any given h, with probability at least 1−δ, we have
Dh,H
ϕ(ν||µ)≤Dh,H
ϕ(ˆν||ˆµ) + 2|t0|ˆRT(Hℓ) + 2L|t0|ˆRS(Hℓ) +O r
log(1/δ)
n+r
log(1/δ)
m!
.
Proof of Lemma 4.2. For a fixed h,
Dh,H
ϕ(ν||µ)−Dh,H
ϕ(ˆν||ˆµ)
= sup
h′sup
αEν[t0ℓ(h, h′)] +Eµ[α−ϕ∗(t0ℓ(h, h′) +α)]− 
sup
h′,tsup
αEˆν[tℓ(h, h′)] +Eˆµ[α−ϕ∗(tℓ(h, h′) +α)]!
≤sup
h′sup
αEν[t0ℓ(h, h′)] +Eµ[α−ϕ∗(t0ℓ(h, h′) +α)]−(Eˆν[t0ℓ(h, h′)] +Eˆµ[α−ϕ∗(t0ℓ(h, h′) +α)])
≤sup
h′|Eν[t0ℓ(h, h′)]−Eˆν[t0ℓ(h, h′)]|+ sup
h′sup
α|Eµ[ϕ∗(t0ℓ(h, h′) +α)]−Eˆµ[ϕ∗(t0ℓ(h, h′) +α)]|
≤2|t0|ˆRT(Hℓ) +O r
log(1/δ)
m!
+ 2ˆRS(ϕ∗◦ Ht0ℓ) +O r
log(1/δ)
n!
≤2|t0|ˆRT(Hℓ) + 2L|t0|ˆRS(Hℓ) +O r
log(1/δ)
n+r
log(1/δ)
m!
,
where the last two inequalities are by the scaling property of Rademacher complexity and Talagrand’s
lemma [70]. This completes the proof.
The generalization bound for f-DD is then given as follows, which clearly shows a slow convergence
rate.
Theorem D.1. Under the conditions in Lemma 4.2. Let ϕbe twice differentiable and ϕ′′is monotone,
for any h∈ H, with probability at least 1−δ, we have
Rν(h)≤Rˆµ(h) +λ∗+Oq
Dh,H
ϕ(ˆν||ˆµ) +q
ˆRT(Hℓ) +ˆRS(Hℓ)
+O
ˆRS(Hℓ)
+O
sr
log(1/δ)
n+r
log(1/δ)
m
+O r
log(1/δ)
n!
.
23Proof of Theorem D.1. The concentration result for Rµ(h)−Rˆµ(h)is exactly the same as Eq. (18)
in the proof of Theorem 4.1. Then putting everything together and bypPn
i=1xi≤Pn
i=1√xiwill
complete the proof.
D.8 Proof of Theorem 5.1
Theorem 5.1. For any h∈ H r1and constants C1, C2∈(0,+∞)satisfying Kh′,µ(C1)≤
C1C2Eµ[ℓ(h, h′)]for any h′∈ H r, the following holds:
Rν(h)≤Rµ(h) +1
C1Dh,Hr
ϕ(ν||µ) +C2Rr
µ(h) +λ∗
r,
where λ∗
r= min h∗∈HrRµ(h∗) +Rν(h∗)andRr
µ(h) = suph′∈HrEµ[ℓ(h, h′)].
Proof. For a given h∈ H r1, leth∗
r= arg min h∗∈HrRµ(h∗) +Rν(h∗), then following similar steps
in Lemma 3.1 and Theorem 4.1, we first have
Rν(h)−Rµ(h)≤Eν[ℓ(h, h∗
r)]−Eµ[ℓ(h, h∗
r)] +λ∗
r.
Then, we can apply Lemma 4.1 for bounding Eν[ℓ(h, h∗
r)]−Eµ[ℓ(h, h∗
r)]here, which gives us
Rν(h)−Rµ(h)≤inf
t1
t
Kr
µ(t) + Dh,Hr
ϕ(ν||µ)
+λ∗
r, (24)
where Kr
µ(t) = suph′∈HrKh′,µ(t).
The condition for C1andC2to exist indicates that
Kr
µ(C1) = sup
h′∈HrKh′,µ(C1)≤C1C2Rr
µ(h).
Replacing tbyC1and plugging the inequality above into Eq. (24) give us
Rν(h)−Rµ(h)≤inf
C1,C21
C1Dh,Hr
ϕ(ν||µ) +C2Rr
µ(h) +λ∗
r,
which completes the proof.
D.9 Proof of Lemma 5.1
Lemma 5.1. Letℓ∈[0,1], and let the constants C1>0andC2∈(0,1)satisfy the condition 
eC1−C1−1 
1−min{r1+r,1}+C2
2min{r1+r,1}
≤C1C2. Then, for any h∈ H r1and
h′∈ H r, we have
Eν[ℓ(h, h′)]≤inf
C1,C2Dh,Hr
KL(ν||µ)
C1+ (1 + C2)Eµ[ℓ(h, h′)].
Proof. LetC′= 1 + Cwhere C∈(0,1)and let g(·) =ℓ(h(·), h′(·)), then we aim to bound the
weighted gap: Eν[g(X)]−C′Eµ[g(X)].
By definition,
Dh,Hr
KL(ν||µ) = sup
g,ttEν[g(X)]−logEµh
etg(X)i
= sup
g,ttEν[g(X)]−C′tEµ[g(X)]−logEµh
et(g(X)−C′Eµ[g(X)])i
(25)
Then, recall that ℓ≤1, and we use Lemma B.54to obtain that
logEµh
et(g(X)−C′Eµ[g(X)])i
≤t(1−C′)Eµ[g(X)] +B(t)t2Eµh
(g(X)−C′Eµ[g(X)])2i
=B(t)t2Eµ
(g(X))2
+B(t)t2(C′2−2C′) (Eµ[g(X)])2−t(C′−1)Eµ[g(X)]
=B(t)t2Eµ
(g(X))2
+B(t)t2(C2−1) (Eµ[g(X)])2−tCEµ[g(X)], (26)
4Note that, in this context, the random variable has a non-zero mean. Therefore, the first expectation term in
Eq. (6) within the proof of Lemma B.5 should be retained.
24where the function B(·)is the Bernstein function defined in Lemma B.5.
Since 0≤Eµ[g(X)]≤min{r1+r,1}andg(·)∈[0,1], we have
B(t)tEµ
(g(X))2
+B(t)t(C2−1) (Eµ[g(X)])2−CEµ[g(X)]
≤B(t)tEµ[g(X)] +B(t)t(C2−1) min {r1+r,1}Eµ[g(X)]−CEµ[g(X)].
Our theme here is to obtain logEµh
et(g(X)−C′Eµ[g(X)])i
≤0, so having the following inequality
hold is sufficient
B(t)tEµ[g(X)] +B(t)t(C2−1) min {r1+r,1}Eµ[g(X)]−CEµ[g(X)]≤0.
Therefore, logEµh
et(g(X)−C′Eµ[g(X)])i
≤0will hold if the following satisfies for tandC,
B(t)t+B(t)t(C2−1) min {r1+r,1} −C≤0.
Equivalently, substituting the expression of the Bernstein function B(·)gives us
 
et−t−1 
1−min{r1+r,1}+C2min{r1+r,1}
−tC≤0, (27)
Now let t=C1andC=C2satisfy Eq. (27), then for any h, h′, by re-arranging terms in Eq. (25),
we have
Eν[ℓ(h, h′)]−Eµ[ℓ(h, h′)]≤1
C1
Dh,Hr
KL(ν||µ) + log Eµh
eC1(g(X)−(1+C2)Eµ[g(X)])i
+C2Eµ[ℓ(h, h′)]
≤1
C1Dh,Hr
KL(ν||µ) +C2Eµ[ℓ(h, h′)],
where the last inequality holds because logEµ
eC1(g(X)−(1+C2)Eµ[g(X)])
≤0when Eq. (27) is
satisfied.
This completes the proof.
D.10 Proof of Theorem 5.2
Theorem 5.2. Under the conditions in Lemma 5.1. For any h∈ H r1, with probability at least 1−δ,
we have
Rν(h)≤Rˆµ(h) +Dh,Hr
KL(ˆν||ˆµ)
C1+C2Rr
µ(h) +O
ˆRT 
Hℓ
r
+ˆRS
Hℓ
max{r,r1}
+Olog(1/δ)
n+log(1/δ)
m
+O r
(r1+r) log(1 /δ)
n+r
rlog(1/δ)
m!
+λ∗
r.
Proof. We first apply both Lemma B.6 and Lemma B.3 for bounding Rµ(h)with local Rademacher
Complexity ˆRS(Hr1) =Eε1:nh
suph∈Hr11
nPn
i=1εiℓ(h, fµ)i
,
sup
hRµ(h)−Rˆµ(h)≤2ES∼µ⊗n
sup
hRµ(h)−Rˆµ(h)
+O r
r1log(1/δ)
n+log(1/δ)
n!
≤2ˆRS(Hr1) +O r
r1log(1/δ)
n+log(1/δ)
n!
, (28)
where we use the fact that Varµ(ℓ(h, fµ))≤Rµ(h)≤r1forℓ∈[0,1].
Similar to Lemma D.1, let t=t0achieve the supreme in Dh,Hr
KL(ν||µ). Let β≤
minn
Eνh
eℓ(h,h′)i
,Eˆνh
eℓ(h,h′)io
for any ˆνandh, h′∈ H (e.g., β= 1), then the concentration
25result of localized KL-DD is derived below,
Dh,Hr
KL(ν||µ)−Dh,Hr
KL(ˆν||ˆµ)
≤sup
h′|Eν[t0ℓ(h, h′)]−Eˆν[t0ℓ(h, h′)]|+ sup
h′logEµh
et0ℓ(h,h′)i
−logEˆµh
et0ℓ(h,h′)i
≤sup
h′|Eν[t0ℓ(h, h′)]−Eˆν[t0ℓ(h, h′)]|+ sup
h′1
βEµh
et0ℓ(h,h′)i
−Eˆµh
et0ℓ(h,h′)i (29)
≤2|t0|ˆRT(Hℓ
r) +O r
rlog(1/δ)
m+log(1/δ)
m!
+ 2e
β|t0|ˆRS(Hℓ
r) +O r
rlog(1/δ)
n+log(1/δ)
n!
,
(30)
where Eq. (29) follows from the similar developments in Eq. (16).
Putting Eq. (30), Eq. (28), Lemma 5.1 and Theorem 5.1 all together, we have
Rν(h)≤Rˆµ(h) + inf
C1,C2Dh,Hr
KL(ˆν||ˆµ)
C1+C2Rr
µ(h) +O
ˆRS 
Hℓ
r1
+ˆRS 
Hℓ
r
+ˆRT 
Hℓ
r
+Olog(1/δ)
n+log(1/δ)
m
+O r
r1log(1/δ)
n+r
rlog(1/δ)
n+r
rlog(1/δ)
m!
+λ∗
r.
Finally, using√a+√
b≤2√
a+bandO
ˆRS 
Hℓ
r1
+ˆRS 
Hℓ
r
=O
ˆRS
Hℓ
max{r1,r}
will conclude the proof.
D.11 Generalization Bounds based on χ2-DD
Theorem D.2. Letℓ(·,·)∈[0,1]. For any h∈ H r1,C1>0andC2∈(0,1)satisfying
C1Varµ(ℓ(h,h′))
4≤C2Eµ[ℓ(h, h′)]for any h′∈ H r, with probability at least 1−δ, we have
Rν(h)≤Rˆµ(h) +Dh,Hr
χ2(ˆν||ˆµ)
C1+C2Rr
µ(h) +O
ˆRT 
Hℓ
r
+ˆRS
Hℓ
max{r,r1}
+λ∗
r
+Olog(1/δ)
n+log(1/δ)
m
+O r
(r1+r) log(1 /δ)
n+r
rlog(1/δ)
m!
.
Proof. We first give a similar result to Lemma 5.1 based on χ2-DD.
Again, let g(·) =ℓ(h, h′). By Eq. (8), we know that our localized χ2-DD is
Dh,Hr
χ2(ν||µ) = sup
h′,tt(Eν[g(X)]−Eµ[g(X)])−t2Varµ(g(X))
4.
LetC′= 1 + Cwhere C∈(0,1). Then, for any h, h′,
Eν[g(X)]−C′Eµ[g(X)]≤inf
t≥01
tDh,Hr
χ2(ν||µ) +tVarµ(g(X))
4−CEµ[g(X)].
When tandCsatisfytVarµ(g(X))
4−CEµ[g(X)]≤0, we have Eν[g(X)]−Eµ[g(X)]≤
inft,C1
tDh,Hr
χ2(ν||µ) +CEµ[g(X)].
26For the concentration result,
Dh,Hr
χ2(ν||µ)−Dh,Hr
χ2(ˆν||ˆµ)
≤sup
h′,tt(Eν[g(X)]−Eµ[g(X)])−t2Varµ(g(X))
4−t(Eˆν[g(X)]−Eˆµ[g(X)]) +t2Varˆµ(g(X))
4
= sup
h′,tt(Eν[g(X)]−Eˆν[g(X)]) + t(Eˆµ[g(X)]−Eµ[g(X)]) +t2
4(Var ˆµ(g(X))−Varµ(g(X)))
≤2|t0|ˆRS(Hr) + 2|t0|ˆRT(Hr) +O r
r1log(1/δ)
n+log(1/δ)
n+r
r1log(1/δ)
m+log(1/δ)
m!
+ sup
h′t2
0
4(Var ˆµ(g(X))−Varµ(g(X))).
Notice that
sup
h′t2
0
4(Var ˆµ(g(X))−Varµ(g(X)))
= sup
h′t2
0
4
Eˆµ
g2(X)
−Eµ
g2(X)
+ (Eµ[g(X)])2−(Eˆµ[g(X)])2
= sup
h′t2
0
4 
Eˆµ
g2(X)
−Eµ
g2(X)
+ (Eµ[g(X)]−Eˆµ[g(X)]) (Eµ[g(X)] +Eˆµ[g(X)])
≤sup
h′t2
0
4Eˆµ
g2(X)
−Eµ
g2(X)
+ sup
h′t2
0
2(Eµ[g(X)]−Eˆµ[g(X)])
≤t2
0ˆRS(Hr) +O r
rlog(1/δ)
n+log(1/δ)
n!
,
where we use the fact that g2is2-Lipschitz in g∈[0,1].
Notice that as mentioned in Remark 4.2, t0=2(Eν[ℓ(h,h′∗)]−Eµ[ℓ(h,h′∗)])
Varµ(ℓ(h,h′∗)). Putting everything together
and following the last several steps as in Theorem 5.2 will complete the proof.
D.12 Threshold Learning Example
Consider a popular threshold learning example.
Example 1 (Threshold Learning) .LetX=R. A threshold function, denoted as hc, is defined s.t.
hc(x) = 0 ifx < c andhc(x) = 1 ifx≥c. Let the source domain µbe an uniform distribution on
[0,1]and let the target domain νbe another uniform distribution on [0,2]. LetH={hc|c∈[0,1
2]}
and assume the ground-truth hypothesis is h∗
1
2for the both the source and target domains. Let ℓbe
the zero-one loss.
Recall that Dh,H
KL(ν||µ) = suph′,ttEν[ℓ(h, h′)]−logEµetℓ(h,h′). Hence,
Dh1
2,H
KL(ν||µ) = sup
tZ1
2
0t
2·1dx−log Z1
2
0etdx+Z1
1
2e0dx!
= sup
tt
4−log1
2+et
2
= 0.131.
Now let r= 1/4, soHr={hc|c∈[1
4,1
2]}, by a similar calculation we have
Dh1
2,H1
4
KL (ν||µ) = sup
tt
8−log(3
4+et
4) = 0 .048andRr
µ=1
4.
Based on Remark 5.3, we can set C2= 0.1andC1= 3.74. Hence, we have
0.27·Dh1
2,H1
4
KL (ν||µ) + 0.1·Rr
µ= 0.038≤r
Dh1
2,H
KL(ν||µ) = 0 .36.
27Since h∗
1
2∈ H r⊂ H , we have λ∗=λ∗
r= 0in this case. This example justifies that the localization
technique can significantly tighten the bound. Note that C2= 0.1andC1= 3.74are not the optimal
choice so localized bound can be even tighter.
In addition, as a more extreme case, let r→0soHr={h1
2}. Then by a similar calculation, we have
Dh1
2,H0
KL (ν||µ) =Rr
µ= 0, while Dh1
2,H
KL(ν||µ)remains unchanged. Notice that Rν(h1
2)−Rµ(h1
2) =
0so localized f-DD gives a tightest target error bound in this case.
D.13 Proof of Proposition 1
Proposition 1. Letdˆµ,ˆν(h, h′) =Eˆνh
ˆℓ(h, h′)i
−Ih
ϕ,ˆµ(ˆℓ◦h′). Assume His sufficiently large s.t.
ˆℓ:X →R, we have max h′˜dˆµ,ˆν(h, h′) = max h′dˆµ,ˆν(h, h′) = Dh,H′
ϕ(ˆν||ˆµ)≤Dϕ(ˆν||ˆµ).
Proof. Since ˆℓ:X → R,tℓ:X → Randtℓ+α:X → R, then it is straightforward to have
max h′˜dˆµ,ˆν(h, h′) = max h′dˆµ,ˆν(h, h′) = Dh,H′
ϕ(ˆµ||ˆν). Additionally, as tℓ◦H′⊆ G, they are upper
bounded by Dϕ(ˆµ||ˆν).
E More Experiment Details and Additional Results
We adopt the experimental setup from [ 1] and build upon their publicly available
code, accessible at https://github.com/nv-tlabs/fDAL/tree/main. Additionally, we lever-
age some settings from the implementations of [ 9] and [ 49], which can be found at
https://github.com/thuml/MDD/tree/master and https://github.com/thuml/CDAN, respectively. Our
code is available at https://github.com/ZiqiaoWangGeothe/f-DD.
ℎ𝑟𝑒𝑝ℎ𝑐𝑙𝑠′ℎ𝑐𝑙𝑠ℎ(𝑥𝑠)
ℎ′(𝑥𝑡)ℎ′(𝑥𝑠)𝑓-DD:maxℎ′𝑑ෝ𝜇ෝ,𝜈(ℎ,ℎ′)ERM:minℎ𝑅ෝ𝜇(ℎ)𝑥𝑠~𝜇𝑥𝑡~𝜈Minimizing classification error of labeled source data
Approximating 𝑓 -divergence-based domain discrepancy𝜇𝜈min𝑓-DD
Figure 2: Illustration of the adversarial training framework for f-DD-based UDA. The framework
includes the representation network ( hrep), the main classifier ( hcls), and the auxiliary classification
network ( h′
cls). It jointly minimizes the empirical risk on the source domain and the approximated
f-DD between the source and target domains.
Specifically, for experiments on Office-31 and Office-Home, we utilize a pretrained ResNet-50
backbone on ImageNet [ 74]. Our f-DD is trained for 40 epochs using SGD with Nesterov Momentum,
setting the momentum to 0.9, the learning rate to 0.004, and the batch size to 32. Hyperparameter
settings and training protocols closely align with [ 9] and [ 49]. Particularly, on Office-31, we vary
the trade-off parameter ηfor our KL-DD within [3,4.5,5.75], and for our χ2-DD within [1,1.75,2].
For Jeffreys-DD, we choose ηγ1, ηγ2from [0,1,1.2,3,3.75,4.5,5,5.75]. On Office-Home, ηfor
KL-DD is chosen from [3,3.75,4.5],ηforχ2-DD from [3,3.75], andηγ1, ηγ2for Jeffreys-DD from
[0,1,1.2,3,3.75,4.5]. For the Digits datasets, we train our f-DD for 30 epochs with SGD and
28Nesterov Momentum, setting the momentum to 0.9 and the learning rate to 0.01. The batch size is
set to 128 for M→Uand 64 for U→M. Other hyperparameters closely follow those used by [ 1] and
[49]. The trade-off parameter ηfor both KL-DD and χ2-DD is selected from [0.75,1], and ηγ1, ηγ2
for Jeffreys-DD from [0,0.01,0.5,0.55]. All experiments are conducted on NVIDIA V100 (32GB)
GPUs.
Comparison with f-DAL + Implicit Alignment [1] also investigate the empirical performance of
combining f-DAL with a sampling-based implicit alignment technique from [ 51], enhancing their
original f-DAL. However, our findings reveal that our f-DD consistently outperforms f-DAL with
implicit alignment, as detailed in Table 6. This observation suggests that the performance gain can
be achieved by simply adopting a more tightly defined variational representation. Furthermore, in
Table 6, the entry labeled f-DD (Best) corresponds to the average of the maximum accuracy across
KL-DD, χ2-DD, and Jeffereys-DD for each subtask. Notably, this aggregated result shows slight
improvement on Office-31 when compared to the individual performance of Jeffereys-DD.
Table 6: Comparison between f-DD and f-DAL
Method Office-31 Office-Home
f-DAL 89.5 68.5
f-DAL+Imp. Align. 89.2 70.0
Jeffereys-DD 90.1 70.2
f-DD (Best) 90.3 70.2
Ablation Study We adjust the trade-off hyper-parameter ηin KL-DD and present the outcomes for
Office-31 and Office-Home. As depicted in Table 7 and Table 8, we can see that the performance
exhibits relatively low sensitivity to changes in η. Hence, the tuning process need not be overly
meticulous.
Table 7: Ablation Study on Office-31 for KL-DD
η 3.75 4.5 5.5 5.75
A→D 95.5±0.7 95.4 ±0.4 95.4 ±0.7 95.9 ±0.6
W→A 74.5±0.1 74.6 ±0.7 74.6 ±0.4 74.5 ±0.5
Table 8: Ablation Study on Office-Home for KL-DD
η 3 3.75 4.5
Ar→Cl 55.3±0.4 54.9 ±0.2 55.3 ±0.1
Pr→Rw 80.7±0.1 80.8 ±0.2 80.9 ±0.1
Additional Results for Absolute Divergence In addition to Figure 1, we present the performance
of the absolute KL discrepancy in Figure 3(a-b). The consistent observations persist, indicating
that the absolute version of the discrepancy tends to overestimate the f-divergence, leading to a
breakdown in the training process.
Table 9: Comparison between χ2-DD and Opt χ2-DD
Method A→D Ar →Cl
χ2-DD 95.0 ±0.4 55.2 ±0.3
Optχ2-DD 93.1 ±0.3 53.9 ±0.3
290 500 1K 1K 2K
Iterations0100200300400Estimated f-divergence
KL W/ Abs.
KL W/O Abs.(a) KL (Office-31)
0 500 1K 1K 2K 2K
Iterations050100150200250300Estimated f-divergence
KL W/ Abs.
KL W/O Abs. (b) KL (Office-Home)
0 1K 2K 3K 4K
Iterations05K10K15K20K25K30KEstimated f-divergence
Chi-square W/ Abs.
Chi-square W/O Abs. (c)χ2(Office-31)
3K 4K 4K 5K 5K 6K 6K
Iterations05K10K15K20K25K30K35K40KEstimated f-divergence
Chi-square W/ Abs.
Chi-square W/O Abs. (d)χ2(Office-Home)
Figure 3: Comparison between Dh,H
ϕandeDh,H
ϕ. The y-axis is the estimated corresponding f-
divergence and the x-axis is the number of iterations.
Details and Additional Results for Optimal f-DD As outlined in Section 6, instead of invoking a
stochastic gradient-based optimizer (e.g., SGD) to update t, we utilize a quadratic approximation for
the optimal tas presented in [23]. The approximately optimal KL-DD (OptKL-DD) is expressed as
follows:
Dh,H
KL(ν||µ) = sup
h′(1 + ∆ t∗)Eν[ℓ(h, h′)]−logEµh
e(1+∆ t∗)ℓ(h,h′)i
.
Here, a Gibbs measure is defined as dµ′≜eℓ(h,h′)dµ
Eµ[eℓ(h,h′)], and ∆t∗is determined by the formula
∆t∗=Eν[ℓ(h, h′)]−Eµ′[ℓ(h, h′)]
Varµ′(ℓ(h, h′)).
This approximation is obtained by [ 23] through a Taylor expansion around t= 1, with additional
details provided in their Appendix B. In our implementation of OptKL-DD, similar to KL-DD, we
useˆℓto replace ℓ.
(a)χ2inf-DAL
 (b)χ2inf-DD
 (c) KL-DD
 (d) Jeffereys-DD
Figure 4: Visualization results of representations obtained by using t-SNE. The source domain (blue
points) is Uand the target domain (orange points) is M.
For the optimal χ2-DD (Opt χ2-DD), we have its analytic form, as shown in Appendix C:
Dh,H
χ2(ν||µ) = sup
h′(Eν[ℓ(h, h′)]−Eµ[ℓ(h, h′)])2
Varµ(ℓ(h, h′)).
By replacing ℓbyˆℓ, we utilize the above as the training objective in our implementation. Table 9
illustrates that the optimal form does not improve performance in two sub-tasks, namely A→Dand
Ar→Cl, and in fact, may even degrade performance. Consequently, when the surrogate loss itself is
unbounded, optimizing over tmay not be necessary, at least for χ2-DD and KL-DD.
Visualization Results To visualize model representations (output of hrep) trained with f-DAL,
χ2-DD, KL-DD, and Jeffereys-DD, we leverage t-SNE [ 52]. In Figure 4, we present visualization
results using USPS ( U) as the source domain and MNIST ( M) as the target domain. Notably, while
the original f-DAL already provides satisfactory results, our f-DD achieves further improvements in
representation alignment.
30More Advance Network Architecture While our empirical study primarily aims to compare with
[1] and improve their algorithms, in this section, we also evaluate our f-DD on more complicated
models, which can serve as a valuable reference for practitioners.
In particular, we update our original backbone, ResNet-50, with pretrained transformer-based models.
Specifically, we use the pretrained Vision transformer (ViT) base model (vit-base-patch16-224) [ 75]
and pretrained Swin-based transformer (swin-base-patch4-window7-224) [ 76], while keeping all
other settings in our algorithms unchanged. The results on Office-31 are presented in Table 10.
Table 10: Accuracy (%) on the Office-31 benchmark.
Method A →W D →W W →D A →D D →A W →A Avg
ViT-based [75] 91.2 99.2 100.0 90.4 81.1 80.6 91.1
SSRT-ViT [77] 97.7 99.2 100.0 98.6 83.5 82.2 93.5
PMTrans-ViT [78] 99.1 99.6 100.0 99.4 85.7 86.3 95.0
Ours-ViT 98.0 ±0.2 99.2 ±0.0 100 ±0.0 98.6 ±0.2 83.5 ±0.5 83.9 ±0.4 93.9
Swin-based [76] 97.0 99.2 100.0 95.8 82.4 81.8 92.7
PMTrans-Swin [78] 99.5 99.4 100.0 99.8 86.7 86.5 95.3
Ours-Swim 98.7 ±0.2 99.2 ±0.0 100 ±0.0 99.2 ±0.2 86.1 ±0.1 85.6 ±0.3 94.8
Results on models other than ours are taken directly from [ 78, Table 2]. Here ViT-based and
Swin-based refer to source-only training using ViT and Swin transformers, respectively, and “Ours”
particularly refers to our weighted-Jeffereys discrepancy. While our method does not outperform the
current SOTA, PMTrans, (especially for ViT), but close to it, this should demonstrate the practical
utility of the proposed approach. Notably, our approach not only significantly improves the respective
backbones but also outperforms the strongest baseline, SSRT-ViT [77], as compared in [78].
The fact the our approach does not outperform PMTrans should come at no surprise. In particular,
PMTrans and all other compared models all include while our approach not only neglected these
ingredients but also simply use hyperparameter based on our earlier ResNet-50 settings without
further tuning.
One notable difference between our method and other SOTA approaches is the absence of any pseudo-
labeling strategy in our framework. Specifically, our theoretical framework is constructed under
the assumption of zero knowledge of target labels. In a strategy that successfully exploits pseudo-
labelling, usually explicit or implicit prior knowledge are available (e.g., for the purpose of selecting
hyperparameters relating to pseudo labeling). For example, such knowledge may present itself as a
labeled validation set in the target domain. Such knowledge, if available, may change our problem
setup and require a refinement of our theory. We anticipate that combining our method with advanced
pseudo-labeling strategies, data augmentation techniques like Mixup (which itself inherently relies
on pseudo-labels), and label smoothing will improve our results, possibly approaching the current
SOTA performance.
31NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We highlight the motivations and contributions of this work in the abstract and
introduction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In the last section of the Appendix, we discuss how the lack of pseudo-labeling
techniques makes our framework slightly weaker than the current state-of-the-art algorithm
in certain tasks.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
32Answer: [Yes]
Justification: All assumptions are given in the theorem statements or in the preliminary
section, and all the technical details are provided in Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Our implementation closely follows the previous works, as pointed out in the
experiments section, with our source code repository referenced in Appendix E. Additionally,
we explained all the additional details that needed for reproducing our results in main text
and Appendix E.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
335.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We only use some well-known public DA datasets, which can be easily found,
and we provide our source code in the supplementary materials and point out the source
code repository in Appendix E.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: See Appendix E
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We provide the standard deviations of our experimental results in the tables,
except for Office-Home. Although we obtained the standard deviations for Office-Home,
we excluded them from the table because previous baselines did not report them.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
34•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: See Appendix E
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Our work conforms the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: We do not see any societal impacts with our paper, as it is mainly theoretical
in nature.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
35•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: There is no such risk.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We cite the datasets and models properly.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
36•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: No new asset.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
37•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
38