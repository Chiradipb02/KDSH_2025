PiCO: Peer Review in LLMs based on the Consistency
Optimization
Anonymous Author(s)
Affiliation
Address
email
Abstract
Existing large language models (LLMs) evaluation methods typically focus on test- 1
ing the performance on some closed-environment and domain-specific benchmarks 2
with human annotations. In this paper, we explore a novel unsupervised evalua- 3
tion direction , utilizing peer-review mechanisms to measure LLMs automatically 4
without any human feedback. In this setting, both open-source and closed-source 5
LLMs lie in the same environment, capable of answering unlabeled questions and 6
evaluating each other, where each LLM’s response score is jointly determined 7
by other anonymous ones. To obtain the ability hierarchy among these models, 8
we assign each LLM a learnable capability parameter to adjust the final ranking. 9
We formalize it as a constrained optimization problem, intending to maximize the 10
consistency of each LLM’s capabilities and scores. The key assumption behind is 11
that high-level LLM can evaluate others’ answers more accurately than low-level 12
ones, while higher-level LLM can also achieve higher response scores. Moreover, 13
we propose three metrics called PEN, CIN, and LIS to evaluate the gap in aligning 14
human rankings. We perform experiments on multiple datasets with these metrics, 15
validating the effectiveness of the proposed approach. 16
1 Introduction 17
Goodhart’s Law: “When a measure becomes a target, it ceases to be a good 18
measure. ” 19
Large language models (LLMs)[ 11,2,12,43] have achieved remarkable success across a variety 20
of real-world applications [ 54,32,36,52]. With the increasingly widespread application of these 21
models, there is an urgent need for an effective evaluation method to ensure that their performance 22
and usability meet the growing demands. To assess the ability level of LLMs, a large number of 23
evaluation benchmarks have been proposed by using some small and domain-specific datasets with 24
human-curated labels, such as MMLU [ 26], HELM [ 30], Big-Bench[ 39], GLUE[ 45]. However, these 25
benchmarks can only measure LLMs’ core capability on a confined set of tasks (e.g. multi-choice 26
knowledge or retrieval questions), which fails to assess their alignment with human preference in 27
open-ended tasks adequately [ 16,28,34]. On the other hand, these evaluations may suffer from 28
benchmark leakage issue, referring that the evaluation data is unknowingly used for model training, 29
which can also lead to misleading evaluations [ 49,56]. Therefore, blindly improving scores on 30
these public benchmarks cannot always yield a large language model that truly satisfies human 31
requirements. 32
For assessing human preferences, recent studies have focused on building crowdsourced battle 33
platforms with human ratings as the primary evaluation metric. Typical platforms include Chatbot 34
Arena [ 55], MT-Bench [ 55], and AlpacaEval [ 29]. It constructs anonymous battles between chatbots 35
in real-world scenarios, where users engage in conversations with two chatbots at the same time and 36
rate their responses based on personal preferences. While human evaluation is the gold standard for 37
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same
environment, capable of answering unlabeled questions and evaluating each other, where each LLM’s response
score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to
optimize the score ranking based on the consistency assumption , while reducing the entropy of the peer-review
evaluation system. The consistency optimization aims to find a final score ranking that all LLMs “agree” it.
measuring human preferences, it is exceptionally slow and costly[ 55]. In addition, adding a new 38
LLM to the crowdsourced battle platforms also poses a cold-start issue [ 15]. Thus, a fundamental 39
question arises: can we construct an unsupervised LLMs evaluation system without relying on any 40
human feedback ? 41
Actually, in real human evaluation systems, people build their ability hierarchy based on different 42
empirical assumptions. For example, majority voting [ 22,10,40] and rating voting [ 5] methods 43
are widely used during the decision-making process, which are based on the wisdom of the crowds 44
[40,13,50] and have been proven to lead to better results than that of an individual. Moreover, in 45
the established practice of peer-review in academic research, scholars evaluate their academic level 46
rankings based on the consistency assumption ,i.e., scholars with stronger abilities have stronger 47
persuasiveness for evaluating others, and can also obtain higher achievements. This paper attempts to 48
explore whether similar phenomena exist in the LLMs evaluation systems. 49
In this work, we propose PiCO , aPeer review approach in LLMs based on Consistency Optimization. 50
In this setting, LLMs themselves act as “reviewers”, engaging in mutual assessments to achieve 51
comprehensive, efficient, and performance evaluations without relying on manually annotated data. 52
This method aims to address the limitations of existing evaluation approaches and provide insights 53
into LLMs’ real-world capabilities. As shown in Figure 1, both open-source and closed-source 54
LLMs lie in the same environment and answer the open-ended questions from an unlabeled dataset. 55
Then, we construct anonymous answer pairs, while randomly selecting other LLMs as “reviewers” to 56
evaluate both responses with a learnable confidence weight w. Finally, we employ this weight and 57
calculate the response scores Gfor each LLM based on the weighted joint evaluation. It is worth 58
noting that the whole peer-review process works in an unsupervised way, and our goal is to optimize 59
the confidence weights that re-rank the LLMs to be closer to human rankings. 60
To achieve this, we formalize it as a constrained optimization based on the consistency assumption. We 61
maximize the consistency of each LLM’s capability wand score Gwhile adjusting the final ranking 62
to align with human preference more closely. The key assumption behind this is that high-level LLM 63
can evaluate others’ answers more accurately (confidence) than low-level ones, while higher-level 64
LLM can also achieve higher answer-ranking scores. As a result, the entropy (controversy) of the 65
whole peer-review evaluation system can be minimized. In other words, the consistency optimization 66
aims to find a final score ranking that all LLMs have no “disputes” regarding. 67
To evaluate the gap in aligning human rankings, we propose three metrics called PEN ( Permutation 68
Entropy), CIN ( Count Inversions), LIS ( Longest Increasing Subsequence). The experiments are 69
conducted on multiple crowdsourcing datasets and validated on these three metrics. The experimental 70
results demonstrate that the proposed PiCO framework can effectively obtain a large language models’ 71
leaderboard closer to human preferences. 72
2Figure 2: Preference alignment metric. Three metrics for evaluating the gap with human preferences called PEN,
CIN, and LIS, respectively
The contributions of this paper can be summarized as follows. 73
•We explore a novel unsupervised LLM evaluation direction without human feedback, uti- 74
lizing peer-review mechanisms to measure LLMs automatically. All LLMs can answer 75
unlabeled questions and evaluate each other. 76
•A constrained optimization based on the consistency assumption is proposed to re-rank the 77
LLMs to be closer to human rankings. 78
•We propose three metrics called PEN, CIN, and LIS on the PiCO framework for evaluating 79
the gap with human preferences. 80
•The experiments with these metrics on three crowdsourcing datasets validate the effective- 81
ness of the proposed approach. 82
2 The Proposed Approach 83
In this section, we first describe the problem definition and preference alignment evaluation, and then 84
introduce the proposed PiCO framework in detail. 85
2.1 Definition and Metrics 86
Problem Definition. In this subsection, we aim to measure the ability level of LLMs automatically 87
without relying on human annotations. Thus we consider an unsupervised LLM evaluation scenario 88
with an unlabeled dataset Qconsisting of nopen-ended questions, where Q={Qi}n
i=1. In addition, 89
we have a large language model pool M={Mj}m
j=1, which includes both open-source and closed- 90
source models. Write M1≻M2to indicate that the LLM M1has stronger capabilities than the LLM 91
M2. Thus, we can assume that the ground-truth ranking R∗alignment with human preferences, 92
R∗:= [M1≻M2≻M3≻...≻Mm], (1)
and assume that the learned ranking ˆRby different evaluation methods is as follows, 93
ˆR:= [M3≻M1≻M2≻...≻Mm]. (2)
The goal is to build an LLM ranking ˆRthat aligns with human ranking R∗, making the loss Lof the 94
both rankings tend towards 0,i.e.,L(ˆR,R∗)→0 95
Preference Alignment Metrics. Before building LLM rankings, we first need to discuss how to 96
evaluate aligned human rankings. Intuitively, the metrics we want mainly describe the differences 97
between two arrays composed of ranking indices. Assuming that human ranking R∗is defined as 98
being well-ranked in ascending order ( [1,2,3, ..., m ]) as shown in Eq 1. Thus the metric is to quantify 99
the randomness of the learned ranking array ( [3,1,2, ..., m ]) as shown in Eq 2. Based on this, we 100
propose three metrics called PEN, CIN, and LIS, respectively. 101
PEN ( Permutation Entropy). Permutation entropy [ 8] is a concept used to quantify the complexity or 102
randomness of time series data. It provides a measure of the irregularity or unpredictability of the 103
order of values in a sequence. We thus utilize it to measure the gap with human rankings as follows, 104
LPEN(ˆR,R∗) :=−X
p(π) logp(π), (3)
where 105
p(π) =#{t|0≤t≤m−k,(Mt+1, ..., M t+k)∈π}
m−k+ 1.
3Figure 3: The pipeline of the PiCO. It is mainly composed of two components: the peer-review and consistency
optimization stages. Specifically, in the peer-review stage, the unlabeled dataset Qand the LLMs pool Mare
given. Then, we let all LLMs answer each unlabeled question to obtain the response set A. We shuffle the set
and construct anonymous answer pairs, while randomly selecting other LLMs to evaluate both responses with a
learnable confidence w. As a result, we can obtain the answer-ranking data Dwhich is a quadruple that records
the partial order between two answers and the evaluator’s confidence weight. In the consistency optimization
stage, we update the parameter wby maximizing the consistency of each LLM’s capability and score, while
re-ranking the LLMs to be closer to human rankings.
πdenotes different permutations, kis a hyper-parameter recommended to be set to 3 to 7, and we 106
setk= 3in this paper. Intuitively, it samples some subsequences and calculates the entropy for all 107
permutation types. And the lower the permutation entropy in the learned LLM rankings, the closer it 108
is to the ground-truth human rankings. 109
CIN ( Count Inversions). Counting inversions [ 27] aims to measure the degree of disorder or 110
"invertedness" in an array or sequence of elements. We thus define it as follows, 111
LCIN(ˆR,R∗) :=X
Mi,Mj∼M1{Mi≻Mj∧i < j}. (4)
Where 1{·}is the indicator function that the value is 1 when the condition is met, otherwise it is 0. 112
Intuitively, the fewer inverse pairs in the learned LLM rankings, the closer it is to the ground-truth 113
human rankings. 114
LIS ( Longest Increasing Subsequence). The longest increasing subsequence aims to find the length 115
of the longest subsequence in a given sequence of elements, where the subsequence is in increasing 116
order. We utilize it to measure the degree of match with human rankings as follows, 117
LLIS(ˆR,R∗) := max {dp[i]|1≤i≤m}, (5)
where 118
dp[i] = 1 + max {dp[j]|1≤j < i∧Mj≺Mi}.
dp[i]represents the length of the longest increasing subsequence that ends with Mi. LIS allows for 119
a nuanced understanding of the degree to which the learned ranking aligns with the ideal human 120
ranking, with a higher LIS length indicating greater alignment. 121
2.2 Algorithm Details 122
The PiCO framework, depicted in Figure 3, involves peer-review and consistency optimization stages. 123
In the peer-review stage, we first collect an unlabeled dataset Qconsisting of open-ended questions, 124
and construct a large language model pool Mthat includes both open-source and closed-source 125
LLMs. Then, we let all LLMs answer each unlabeled question to obtain the response set A. We 126
shuffle the set and construct anonymous answer pairs, while randomly selecting other LLMs as 127
“reviewers” to evaluate both responses with a learnable confidence w. Finally, we can obtain the 128
answer-ranking data Dand calculate the response score Gfor each large language model. In the 129
consistency optimization phase, we maximize the consistency of each LLM’s capability wand score 130
Gwith constrained optimization, while re-ranking the LLMs to be closer to human rankings. 131
2.2.1 Peer Review Stage 132
Data Collection and LLMs Pool Construction. Benefiting from the creation of crowdsourced 133
battle platforms, we accessed open assessment datasets from Chatbot Arena[ 55], MT-Bench[ 55], 134
4and AlpacaEval[ 29]. These open datasets include critical fields such as "question _id" and 135
"question _content." Utilizing the Chatbot Arena dataset, which features pairwise data from twenty 136
LLMs with human preference annotations, we assembled an LLM pool M={Mj}m
j=1. Leveraging 137
33K human-annotated interactions from this dataset, we established a ground-truth ranking R∗and 138
gathered responses A={{Aj
i}n
i=1}m
j=1for our dataset Q={Qi}n
i=1. 139
Answer-Ranking Data Construction Based on Peer Review. After obtaining the responses set A, 140
we aim to generate answer-ranking data Dthrough the peer-review mechanism. Specifically, for the 141
same question Qi∈ Q, we randomly construct a battle pair < Aj
i, Ak
i>for review. Each battle pair 142
will be randomly assigned five models (“reviewers”) to determine the winners or declare ties. Note 143
that the model may evaluate its own answers, but the entire process is anonymous. As a result, we 144
can obtain the quadruples (Aj
i, Ak
i, > ws), indicating the “reviewer” Msbelieves that the answer Aj
i145
is better than answer Ak
iwith a confidence ws. Therefore, the answer-ranking data Dcan be defined 146
as follows, 147
D=n
(Aj
i, Ak
i, >, ws)o
i∼Q,j,k,s∼M, (6)
where idenotes the question index, and j, k, s indicate the model indices. wsis a learnable confidence 148
of model Ms, and>is a partial order relationship from {>, <, =}. 149
2.2.2 Consistency Optimization Stage 150
As shown in Eq 6, following the peer-review mechanism, we construct anonymous answer pairs and 151
randomly select other LLMs as “reviewers” to evaluate both responses with a learnable confidence w. 152
Next, we expect to optimize the confidence wand re-rank the LLMs to be closer to human rankings. 153
We thus propose the consistency assumption, i.e., high-level LLM can evaluate others’ answers 154
more accurately (confidence) than low-level ones, while higher-level LLM can also achieve higher 155
answer-ranking scores. Formally, we maximize the consistency of each LLM’s capability wand 156
score Gwith constrained optimization as follows, 157
argmax
wConsistency (G, w) (7)
s.t.Gj=X
(Aj
i,Ak
i,>,ws)∼D1{Aj
i> Ak
i} ∗ws,
where 1{·}is the indicator function that the value is 1 when the condition is met, otherwise, it is 0. 158
Gjdenotes the response score of model Mj, which is calculated by joint evaluation of other models. 159
Moreover, we employ Pearson correlation [ 38] to measure the consistency between wandG. Note 160
that we only introduce this straightforward implementation to validate our idea of PiCO. Other more 161
advanced strategies may be employed to further improve the performance. 162
Discussion: It is worth noting that the whole process (Eq. 6 and 7) works in an unsupervised way. 163
The only thing we do is to adaptively assign each LLM a score that matches its abilities. An intuitive 164
example is as follows: in a real peer-review system, if the academic level of three scholars a,b, andc 165
satisfies the following relationship, wa> wb> wc. So, in the ultimate ideal scenario, the ranking 166
of the scores submitted by these three scholars should also be, Ga> G b> G c. In other words, the 167
sorting of Gandwsatisfies high consistency. On the other hand, scholars with stronger abilities ( i.e., 168
scholar a) evaluate Ab> Achave stronger persuasiveness, so scholar bshould also receive higher 169
weighted scores 1∗wa. 170
Reviewer Elimination Mechanism. Realizing that not all LLMs have sufficient ability to evaluate 171
the responses of other models. We thus introduce an unsupervised elimination mechanism to remove 172
those LLMs that have low scores. It iteratively removes the lowest-scoring LLM from the “reviewer 173
queue” for the next consistency optimization stage, until 60% of models are eliminated. The whole 174
process of the approach is summarized in Algorithm 1, and the details can be found in Appendix D. 175
3 Experiments 176
Datasets. To validate the effectiveness of the proposed approach, we perform experiments on Chatbot 177
Arena[ 55], MT-Bench[ 55], and AlpacaEval[ 29]. The MT-Bench dataset assesses six LLMs’ responses 178
to 80 multi-category questions. The Chatbot Arena Conversations Dataset, with 33K conversations 179
from 13K IPs during April-June 2023, evaluates real dialogue performance. AlpacaEval dataset 180
5Table 1: Comparison of all methods on three datasets under data volumes of 1, 0.7 and 0.4, where the top value
is highlighted by blod font. Lower PEN and CIN scores indicate better performance, while a higher LIS score
signifies improved performance.
Datasets Chatbot Arena MT-Bench AlpacaEval
Methods 1 0.7 0.4 1 0.7 0.4 1 0.7 0.4
PEN (↓)
Majority V oting [40] 1.27±0.051.30±0.031.36±0.061.37±0.031.30±0.061.27±0.041.26±0.021.28±0.031.29±0.03
Rating V oting [5] 1.39±0.021.43±0.031.42±0.071.32±0.031.35±0.041.38±0.041.34±0.031.37±0.031.34±0.08
GPTScore(flan-t5-xxl)[23] 1.68±0.011.68±0.021.65±0.021.72±0.021.70±0.021.68±0.031.55±0.021.57±0.031.60±0.01
GPTScore(davinci-002)[23] 1.54±0.021.64±0.021.68±0.051.51±0.021.61±0.011.61±0.041.25±0.021.23±0.081.26±0.14
PandaLM[46] 1.65±0.011.64±0.021.63±0.051.55±0.031.59±0.051.52±0.081.56±0.011.58±0.011.64±0.05
PRD[28] 1.15±0.041.12±0.051.13±0.061.15±0.051.17±0.061.23±0.041.21±0.041.22±0.061.23±0.07
PRE[17] 1.07±0.011.03±0.031.06±0.041.17±0.041.13±0.051.19±0.051.18±0.031.21±0.041.15±0.05
PiCO (Ours) 0.94±0.020.96±0.040.95±0.081.01±0.071.02±0.111.06±0.241.17±0.021.17±0.081.13±0.05
CIN(↓)
Majority V oting [40] 22.00±0.0023.25±1.0925.00±2.5523.00±0.0020.50±0.8721.00±1.0020.00±0.0021.25±1.3022.25±1.30
Rating V oting [5] 24.00±0.0024.50±1.2925.00±1.1522.00±0.0022.50±1.0024.25±0.5022.00±0.0022.50±0.5822.50±1.00
GPTScore(flan-t5-xxl)[23] 67.00±0.0066.50±0.5068.25±1.0953.00±0.0055.75±2.7754.50±2.2935.00±0.0036.00±0.7137.75±1.60
GPTScore(davinci-002)[23] 42.00±0.0045.50±1.1251.00±5.6133.00±0.0035.00±0.7136.25±1.6421.00±0.0020.25±2.8621.50±4.39
PandaLM[46] 37.00±0.0036.25±1.7936.00±3.7432.00±0.0033.00±3.3231.50±6.3431.00±0.0032.25±1.3035.50±2.60
PRD[28] 17.00±0.0016.25±0.4317.50±1.5017.00±0.0017.75±1.0919.50±1.5019.00±0.0019.25±1.4819.50±0.87
PRE[17] 15.00±0.0014.25±0.8314.75±1.0917.00±0.0017.00±1.0018.25±1.3019.00±0.0019.25±1.0917.75±1.30
PiCO (Ours) 12.00±0.0012.50±0.5012.25±1.0914.50±0.5014.75±1.6416.00±6.3617.00±0.0018.00±1.8717.25±1.09
LIS(↑)
Majority V oting [40] 7.00±0.006.75±0.436.75±0.437.00±0.008.25±0.438.50±1.128.00±0.007.50±0.507.50±0.50
Rating V oting [5] 7.00±0.007.50±0.587.75±0.507.00±0.007.25±0.507.25±0.508.00±0.008.00±0.008.00±0.00
GPTScore(flan-t5-xxl)[23] 5.00±0.005.00±0.004.00±0.714.00±0.004.50±0.504.75±0.436.00±0.006.00±0.006.00±0.00
GPTScore(davinci-002)[23] 8.00±0.006.25±0.436.00±0.716.00±0.006.50±0.506.25±0.438.00±0.008.25±0.838.25±1.48
PandaLM[46] 5.00±0.005.50±0.506.00±0.007.00±0.007.00±0.717.25±0.436.00±0.005.75±0.435.50±0.50
PRD[28] 8.00±0.008.75±0.439.25±0.838.00±0.008.25±0.437.75±0.838.50±0.008.25±0.838.25±0.43
PRE[17] 9.00±0.0010.25±0.4310.00±0.878.00±0.008.50±0.508.25±0.838.00±0.008.00±0.008.25±0.43
PiCO (Ours) 10.00±0.0010.25±0.7110.50±0.438.75±0.438.75±0.879.00±1.229.00±0.008.75±0.438.50±0.50
integrates 805 evaluations from diverse tests (e.g., Self-Instruct[ 48], OASST, Anthropic’s helpful[ 7], 181
Vicuna[ 16] and Koala[ 25] test sets) to align evaluations real-world interactions[ 21]. These datasets 182
are collected by crowdsourcing platforms from human feedback, so they have a ground-truth ranking 183
LLMs R∗aligned with human preferences. 184
LLMs Pool. In our experiments, we employ 15 LLMs with diverse architectures to construct the 185
LLMs pool, including GPT-3.5-Turbo[ 35], WizardLM-13B[ 51], Guanaco-33B[ 1], Vicuna-7B[ 16], 186
Vicuna-13B[ 16], Koala-13B[ 24], Mpt-7B[ 42], gpt4all-13B[ 6], ChatGLM-6B[ 53], Oasst-sft-4-pythia- 187
12B[ 19], FastChat-T5-3B[ 55], StableLM-7B[ 3], Dolly-12B[ 18], LLaMA-13B[ 43], Alpaca-13B[ 41]. 188
All models use the same evaluation template, they can be found in Appendix B 189
Baselines. To validate the effectiveness of the proposed PiCO approach, we compare the following 190
methods in the experiments. 191
•The wisdom of the crowds : The two methods that perform LLMs evaluation based on the 192
wisdom of the crowds [ 40,13,50] are compared in this experiment. 1)Majority Voting 193
[40]: Multiple review models vote for the better answer for the same response pair, and the 194
model with the most votes gets 1 score; 2)Rating Voting [5]: Multiple review models also 195
vote on the same response pair, and the number of votes obtained is the score. 196
•State-of-the-art methods : The four recent SOTA methods of using either single or multiple 197
models for self-evaluation are compared in this experiment. PandaLM[ 46]:It is a fine-tuned 198
language model based on Llama-7b designed for the preference judgment tasks to evaluate 199
and optimize LLMs. GPTScore[ 23]:It employs generative pre-trained models to assess the 200
quality of generated text. It calculates the likelihood that the text was generated in response 201
to specific instructions and context, indicative of high quality. In our implementation, GPT-3 202
(davinci-002) and flan-t5-xxl serve as the base models. PRD[ 28]:It transforms the LLMs 203
win rates into weights for competitive ranking, while evaluating each LLM based on its 204
preference for all possible pairs of answers, enabling a tournament-style ranking system. 205
PRE[ 17]:It employs a supervised process to evaluate LLMs using a qualification exam, 206
aggregates their scores based on accuracy, and assigns weights accordingly. PiCO (Ours) : 207
the proposed approach in this paper. 208
Metrics. For all experiments, we employ three metrics to evaluate the aforementioned experimental 209
setups and our Peer Review method: PEN, CIN, and LIS. Moreover, we perform the experiments for 210
4 runs and record the average results over 4 seeds ( seed = 1,2,3,4). 211
6(a) ChatBot Arena (PG)
 (b) MT-Bench (PG)
 (c) AlpacaEval (PG)
(d) ChatBot Arena (weighted PG)
 (e) MT-Bench (weighted PG)
 (f) AlpacaEval (weighted PG)
Figure 4: Heatmap distribution of preference gap (PG) metric among seven LLMs across three datasets. Higher
values (above 0) indicate greater evaluation bias[ 17]. The first row shows original PG values in three datasets,
while the second row displays PG values re-weighted using our learned confidence weights.
3.1 Performance Comparison 212
We validate the effectiveness of the proposed PiCO method on three datasets by comparing the 213
following two types of methods, i.e., the wisdom of the crowds and recent SOTA LLMs evaluation 214
methods. The average results of PEN, CIN and LIS are demonstrated in Table 1. The ratios of 215
response sets Dare 1, 0.7, and 0.4, respectively. 216
The results presented in Table 1 illustrate the proposed PiCO method consistently surpasses com- 217
peting approaches across the majority of evaluated metrics Notably, PiCO achieves performance 218
improvements of 0.1, 2.5, and 0.92 on the PEN, CIN, and LIS metrics, respectively, compared to the 219
Runner-up. These results underscore the superiority of aggregating evaluations from multiple models, 220
such as Majority V oting, Rating V oting, PRD, and PRE, as opposed to relying solely on single-model 221
methods like GPTScore and PandaLM. This collective model approach, leveraging ’the wisdom of 222
the crowds’, more accurately aligns with human rankings in our open-question evaluation framework. 223
In comparison with existing peer review evaluation methods( i.e.,PRD and PRE), it is evident that 224
PiCO exhibits improvements across various evaluation metrics. Despite PRD’s adjustment of model 225
weights based on their win rates and PRE’s reliance on supervised human feedback data to assign 226
weights through a qualification exam, neither method achieves performance superior to the fully 227
unsupervised PiCO approach. These methods rely on predefined criteria and human feedback, 228
potentially leading to biases or suboptimal performance. In contrast, PiCO leverages unsupervised 229
learning techniques, allowing it to autonomously adapt and discover patterns in the data without 230
explicit human intervention. 231
It is important to highlight that PandaLM, a language model equipped with 7 billion parameters, was 232
fine-tuned using labels generated by GPT-3.5-turbo as the ground truth, achieving stable performance 233
across various datasets. However, in our unsupervised, open-ended experimental setup, which focuses 234
on ranking-based metrics, GPTScore exhibits less robustness regardless of whether the base model is 235
GPT-3 (davinci-002) or flan-t5-xx. 236
3.2 Exploring the Role of Confidence Weight 237
In this subsection, we will show that the confidence weight wlearned by our consistency optimization 238
can reduce the system evaluation bias. Specifically, we first study whether the “review” model would 239
7Figure 5: Performance comparison of the PiCO (Ours) and PRE[ 17] methods on the Chatbot Arena, MT-Bench,
and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is CIN, where lower
values indicate better performance.
prefer a particular model’s response. Following [ 17], we employ the preference gap (PG) to evaluate 240
the bias as follows, 241PG(i, j) =Pi(i > j )−Pj(i > j ), (8)
where Pi(i > j )represents the winning rate of model ias the “reviewer” believes that idefeated 242
j. The heatmap distribution of the PG value PG(i, j)among seven LLMs across three datasets is 243
demonstrated in the first row of Figure 4. It can be observed that the evaluation system exhibits severe 244
bias. Especially on ChatGLM-6B and Mpt-7B models, they often believe that their results are better 245
than other ones, as their PG values are greater than 0 across three datasets. 246
After the consistency optimization , we assign the learned confidence weight wto the corresponding 247
model and ultimately obtain the re-weighting PG value ˆPG(i, j)as follows, 248
ˆPG(i, j) =wi×Pi(i > j )−wj×Pj(i > j ). (9)
The results of the re-weighting PG value ˆPG(i, j)are displayed on the second row of Figure 4. It can 249
be observed that the learned confidence weight wcan significantly mitigate the preference gaps of the 250
whole evaluation system. In our consistency optimization, LLMs such as ChatGLM-6B and Mpt-7B 251
have lower weights, and reducing their confidence can effectively alleviate the system evaluation bias. 252
3.3 Study of Elimination Mechanism 253
The PiCO and PRE[ 17] methods both employ elimination mechanisms to remove those weakest 254
LLMs from the “reviewer queue” during the evaluation process. As shown in Figure 5, the x-axis 255
quantifies the number of reviewers eliminated, and the y-axis measures the CIN, where lower scores 256
denote higher performance. Due to space limitations, more results on PEN and LIS metrics can be 257
found in Appendix E. It can be observed that both PiCO and PRE exhibit better performance with 258
an increasing number of eliminated “reviewers”. The proposed PiCO approach can achieve better 259
performance than PRE in most cases. It is worth noting that the PRE method employs the accuracy 260
of “qualification exams” to eliminate weak LLMs, and this process requires human annotation [ 17]. 261
On the contrary, the elimination process of our PiCO method is unsupervised and can still achieve 262
better evaluation results than PRE. 263
3.4 Validation of Consistency Assumption 264
In this subsection, we conduct the ablation study to validate the effectiveness of the consistency 265
assumption . Specifically, we first manually construct three methods: Forward Weight V oting, 266
Uniform Weight V oting, and Reverse Weight V oting. That is, the ability weights of the model are 267
respectively weighted forward ( w= [1,0.9, ...,0]), uniformly ( w= [1,1, ...,1]), and backward 268
(w= [0,0.1, ...,1]) according to the ground-truth human ranking. Then, we randomly initialize the 269
ability weights and employ our consistency optimization to adjust the weight. In addition, we also 270
collect the average performance of “reviewer queue”, i.e., employing a single LLM as the “reviewer” 271
to evaluate all response pairs and then calculate the average results of all LLMs. 272
As shown in Table 2, it can be observed that the Forward Weight V oting achieves better results than 273
the Uniform and Backward ones in all cases, while the Backward one achieves worse results. It 274
validates that assigning larger weights to those models with stronger capabilities can obtain better 275
8Table 2: Ablation study comparing Backward, Uniform, Forward weight voting, and Consistency Optimization
methods with the Average Performance of Reviewer Queue across three datasets.
MethodsMT-Bench Chatbot Arena AlpacaEval
PEN (↓) CIN(↓) PEN (↓) CIN(↓) PEN (↓) CIN(↓)
Average Performance of Reviewer Queue 1.49±0.2834.87±14.681.49±0.2638.80±19.281.50±0.2333.13±13.97
Backward Weight V oting 1.43±0.0425.00±0.001.43±0.0526.00±0.001.36±0.0324.00±0.00
Uniform Weight V oting 1.34±0.2322.00±0.001.39±0.0224.00±0.001.34±0.0322.00±0.00
Forward Weight V oting 1.32±0.0321.00±0.001.33±0.0323.00±0.001.30±0.0521.00±0.00
Random Weight + Consistency Optimization 1.17±0.0617.50±0.501.20±0.0818.00±1.221.21±0.0419.00±0.00
results. Most importantly, employing our consistency optimization algorithm to assign weights to 276
different review models can further improve the performance of the evaluation system, i.e., lower PEN 277
and CIN, as well as higher LIS in all cases. Moreover, it is worth noting that the average performance 278
of the “reviewer queue” is very poor, even worse than the Backward Weight V oting. This means 279
that the answer-ranking data Dcontains a lot of evaluation noise, while the proposed approach can 280
still optimize weights and obtain better ranking results. In summary, the above experimental results 281
validate the effectiveness of the consistency assumption from various perspectives. 282
4 Related Work 283
Evaluation Benchmarks for Diversity. LLMs are designed to handle a variety of tasks, necessitat- 284
ing comprehensive benchmarks[ 15]. Notable benchmarks include GLUE[ 45] and SuperGLUE[ 44], 285
which simulate real-world scenarios across tasks such as text classification, translation, reading 286
comprehension, and dialogue generation. HELM[ 30] provides a holistic evaluation of LLMs, as- 287
sessing language understanding, generation, coherence, and reasoning. BIG-bench[ 39] pushes LLM 288
capabilities with 204 diverse tasks. MMLU[ 26] measures multitask accuracy across domains like 289
mathematics and law. However, these evaluations can be compromised by benchmark leakage, where 290
evaluation data inadvertently used for training leads to inflated performance metrics[4, 56]. 291
Human Evaluation. Human evaluation provides reliable feedback that closely aligns with real- 292
world applications[ 15]. Liang et al.[ 30] evaluated summary and misinformation scenarios across 293
multiple models. Ziems et al.[ 57] involved experts to assess model outputs in various domain-specific 294
tasks. Bang et al.[ 9] examined ChatGPT’s performance in summarization, translation, and reasoning 295
using human-annotated datasets. The LMSYS initiative introduced platforms like Chatbot Arena[ 55], 296
relying on human ratings as the primary evaluation metric. Despite its effectiveness, human evaluation 297
is costly and subject to bias and cultural differences[37]. 298
Large Language Models for Evaluation. The development of open-source LLMs has led to the 299
use of LLMs as evaluators. GPTScore[ 23] uses models like GPT-3 to assign probabilities to high- 300
quality content through multidimensional evaluation. Bubeck et al.[ 12] tested GPT-4, finding it 301
rivaling human capabilities. Lin and Chen introduced LLM-EV AL[ 31] for evaluating dialogue quality 302
with single prompts. PandaLM[ 46] employs LLMs as "judges" for evaluating instruction tuning. 303
However, reliance on a single model can introduce biases such as positional[20], verbosity[47], and 304
self-favoring biases[ 33,55]. ChatEval[ 14] proposes a multi-agent framework to simulate human 305
evaluation processes. Similarly, PRE[ 17] and PRD[ 28] use LLMs as evaluators, combining multiple 306
evaluation outcomes for automated assessment. However, the PRE method, which relies on human 307
feedback for supervised evaluation throughout the process, still incurs relatively high costs. 308
5 Conclusion 309
In this paper, we propose the novel Peer Review method based on the Consistency Optimization 310
(PiCO) to automatically evaluate Large Language Models (LLMs) without relying on human feedback. 311
PiCO utilizes peer-review mechanisms to autonomously assess LLMs in a shared environment, where 312
both open-source and closed-source models can respond to unlabeled questions and evaluate each 313
other. In this setup, each LLM’s response score is determined collectively by other anonymous 314
models, aiming to maximize consistency across capabilities and scores. We propose three metrics, 315
i.e.,PEN, CIN, and LIS, to quantify the disparity from human preferences. The extensive experiment 316
results across multiple datasets and metrics demonstrate that PiCO effectively generates an LLM 317
leaderboard that aligns closely with human preferences. In the future, we plan to extend the peer- 318
review mechanism to evaluate the capabilities of multi-modality large models. 319
9References 320
[1]Guanaco - generative universal assistant for natural-language adaptive context-aware omnilin- 321
gual outputs. https://guanaco-model.github.io/ , 2023. Accessed: 15 April 2024. 322
[2]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni 323
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 324
technical report. arXiv preprint arXiv:2303.08774 , 2023. 325
[3]Stability AI. Stablelm-tuned-alpha-7b: A fine-tuned language model for diverse applications. 326
https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b , 2023. Accessed: 327
15 April 2024. 328
[4]Rachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-Yeol Ahn. Can we trust the evaluation 329
on chatgpt?, 2023. 330
[5]Mohammad Allahbakhsh and Aleksandar Ignjatovic. Rating through voting: An iterative 331
method for robust rating. arXiv preprint arXiv:1211.0390 , 2012. 332
[6]Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. 333
Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. 334
https://github.com/nomic-ai/gpt4all , 2023. 335
[7]Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn 336
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless 337
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 338
2022. 339
[8]Christoph Bandt and Bernd Pompe. Permutation entropy: a natural complexity measure for 340
time series. Physical review letters , 88(17):174102, 2002. 341
[9]Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Love- 342
nia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation 343
of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023 , 344
2023. 345
[10] Robert S Boyer and J Strother Moore. Mjrty—a fast majority vote algorithm. In Automated 346
reasoning: essays in honor of Woody Bledsoe , pages 105–117. Springer, 1991. 347
[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, 348
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are 349
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020. 350
[12] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece 351
Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general 352
intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023. 353
[13] David V Budescu and Eva Chen. Identifying expertise to extract the wisdom of crowds. 354
Management science , 61(2):267–280, 2015. 355
[14] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, 356
and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. 357
arXiv preprint arXiv:2308.07201 , 2023. 358
[15] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, 359
Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language 360
models. ACM Transactions on Intelligent Systems and Technology , 2023. 361
[16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, 362
Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot 363
impressing gpt-4 with 90% chatgpt quality. https://vicuna.lmsys.org , 2023. Accessed: 364
15 April 2024. 365
10[17] Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu. Pre: A peer review based large 366
language model evaluator. arXiv preprint arXiv:2401.15641 , 2024. 367
[18] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick 368
Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world’s first truly open 369
instruction-tuned llm, 2023. 370
[19] Open-Assistant Contributors. Oasst-sft-4-pythia-12b: A supervised fine-tuning 371
model for language understanding. https://huggingface.co/OpenAssistant/ 372
oasst-sft-4-pythia-12b-epoch-3.5 , 2023. Accessed: 15 April 2024. 373
[20] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient 374
finetuning of quantized llms. Advances in Neural Information Processing Systems , 36, 2024. 375
[21] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos 376
Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for 377
methods that learn from human feedback. arXiv preprint arXiv:2305.14387 , 2023. 378
[22] Allan M. Feldman. Majority voting. SpringerLink , 2006. 379
[23] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. 380
arXiv preprint arXiv:2302.04166 , 2023. 381
[24] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, 382
and Dawn Song. Koala-13b: Dialogue model for effective human-ai interaction. https: 383
//bair.berkeley.edu/blog/2023/04/03/koala/ , 2023. Accessed: 15 April 2024. 384
[25] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and 385
Dawn Song. Koala: A dialogue model for academic research. Blog post, April , 1, 2023. 386
[26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and 387
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint 388
arXiv:2009.03300 , 2020. 389
[27] Charles Eric Leiserson, Ronald L Rivest, Thomas H Cormen, and Clifford Stein. Introduction 390
to algorithms , volume 3. MIT press Cambridge, MA, USA, 1994. 391
[28] Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language 392
model based evaluations. arXiv preprint arXiv:2307.02762 , 2023. 393
[29] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy 394
Liang, and Tatsunori B Hashimoto. Alpacaeval: An automatic evaluator of instruction-following 395
models, 2023. 396
[30] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, 397
Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of 398
language models. arXiv preprint arXiv:2211.09110 , 2022. 399
[31] Yen-Ting Lin and Yun-Nung Chen. Llm-eval: Unified multi-dimensional automatic evaluation 400
for open-domain conversations with large language models. arXiv preprint arXiv:2305.13711 , 401
2023. 402
[32] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 403
Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language 404
processing. ACM Computing Surveys , 55(9):1–35, 2023. 405
[33] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: 406
Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634 , 407
2023. 408
[34] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo- 409
pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted 410
question-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021. 411
11[35] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt , 2022. Accessed: 412
[insert date here]. 413
[36] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, 414
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to 415
follow instructions with human feedback. Advances in Neural Information Processing Systems , 416
35:27730–27744, 2022. 417
[37] Kaiping Peng, Richard E Nisbett, and Nancy YC Wong. Validity problems comparing values 418
across cultures and possible solutions. Psychological methods , 2(4):329, 1997. 419
[38] Philip Sedgwick. Pearson’s correlation coefficient. Bmj, 345, 2012. 420
[39] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, 421
Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 422
Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 423
arXiv preprint arXiv:2206.04615 , 2022. 424
[40] James Surowiecki. The wisdom of crowds . Anchor, 2005. 425
[41] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy 426
Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. 427
https://github.com/tatsu-lab/stanford_alpaca , 2023. 428
[42] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially 429
usable llms, 2023. Accessed: 2023-05-05. 430
[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- 431
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open 432
and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. 433
[44] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix 434
Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose 435
language understanding systems. Advances in neural information processing systems , 32, 2019. 436
[45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 437
Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv 438
preprint arXiv:1804.07461 , 2018. 439
[46] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya 440
Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark 441
for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087 , 2023. 442
[47] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, 443
David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? 444
exploring the state of instruction tuning on open resources. Advances in Neural Information 445
Processing Systems , 36, 2024. 446
[48] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, 447
and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc- 448
tions. arXiv preprint arXiv:2212.10560 , 2022. 449
[49] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng 450
Cheng, Weiwei Lü, Rui Hu, et al. Skywork: A more open bilingual foundation model. arXiv 451
preprint arXiv:2310.19341 , 2023. 452
[50] Susan C Weller. Cultural consensus theory: Applications and frequently asked questions. Field 453
methods , 19(4):339–368, 2007. 454
[51] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and 455
Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions, 456
2023. 457
[52] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. Llm lies: Hallucinations 458
are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469 , 2023. 459
12[53] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, 460
Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv 461
preprint arXiv:2210.02414 , 2022. 462
[54] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, 463
Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv 464
preprint arXiv:2303.18223 , 2023. 465
[55] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, 466
Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 467
Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. 468
[56] Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, 469
Ji-Rong Wen, and Jiawei Han. Don’t make your llm an evaluation benchmark cheater. arXiv 470
preprint arXiv:2311.01964 , 2023. 471
[57] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large 472
language models transform computational social science? arXiv preprint arXiv:2305.03514 , 473
2023. 474
A Dataset Format 475
Focusing on the MT-Bench dataset, we demonstrate the ensuing data format utilizing dataset Q. 476
As Figure 6 illustrates, the Question dataset Qcontains "Question id," "Category," "Question," 477
and "Reference." In categories with definitive answers like "reasoning" or "math," the "Reference" 478
field is populated with standard answers; otherwise, it remains blank. Each model M in our pool 479
processes the Question dataset Qto generate the LLMs answer data A, consisting of "Question 480
id," "Answer id," "Model id," and "Answer." Finally, we combine pairs in Aand appoint judges to 481
evaluate, creating the Answer-Ranking data D, featuring "Question id," "Model 1," "Model 2," "G1 482
winner," "G2 winner," and "Judge." Here, "G1 winner" and "G2 winner" indicate the outcomes of 483
inputting reversed order responses of Model 1 and Model 2 into the judge model, a method employed 484
to mitigate biases stemming from models’ preferences for input order. 485
Figure 6: Format of the Question dataset Q, LLMs responses data A, and the Answer-Ranking data Dfor Peer
Review
B Detailed Prompt for Reviewers 486
The evaluation prompts, as detailed in Section 2.2.1, are employed during the Peer Review Stage. 487
These prompts are provided to the Reviewer Language Model Systems (LLMs), enabling them to 488
generate evaluative preferences. In our experimental framework, we devised four distinct prompt 489
settings. For each setting, a tailored prompt template was meticulously crafted as illustrated below: 490
Template for Single-Turn Interaction: This template is designed for single-turn interactions 491
between users and LLMs, where there is no predetermined correct answer. It facilitates open-ended 492
dialogue, allowing for a wide range of user inquiries without the expectation of specific responses. 493
Referenced Template for Single-Turn Interaction: Tailored for single-turn dialogues between 494
users and LLMs, this template incorporates predefined correct answers. It is particularly suited for 495
13interactions involving factual inquiries, such as mathematics or logic problems, where accuracy and 496
reference to correct information are paramount. 497
Template for Multi-Turn Interaction: This template caters to multi-turn conversations between 498
users and LLMs, without predefined answers. It supports extended interactions, enabling users to 499
explore topics in depth through a series of interconnected questions and responses. 500
Referenced Template for Multi-Turn Interaction: Designed for multi-turn dialogues with prede- 501
fined correct answers, this template is ideal for complex inquiries requiring sequential reasoning or 502
problem-solving, such as mathematical computations or logical deductions. 503
Each template is carefully constructed to match its intended use-case, providing a structured frame- 504
work that guides the interaction between users and LLMs towards achieving desired outcomes, 505
whether for open-ended exploration or precise problem-solving. 506
Template for Single-Turn Answer
System prompt: Please act as a judge and evaluate the quality of the responses provided by
two AI assistants to the user question displayed below. You do not need to explain, just give
your judgment. Output your final verdict by strictly following this format: "[[A]]" if assistant
A is better, "[[B]]" if assistant B is better, and "[[C]]" for a tie.
User Question: {question}
Assistant A’s Answer: {answer a}
Assistant B’s Answer: {answer b}
507
Referenced Template for Single-Turn Answer
System prompt: Please act as a judge and evaluate the quality of the responses provided
by two AI assistants to the user question displayed below, with reference to the provided
reference answers. You do not need to explain, just give your judgment. Output your final
verdict by strictly following this format: "[[A]]"if assistant A is better, "[[B]]" if assistant B is
better, and "[[C]]" for a tie.
User Question: {question}
Reference Answer: {reference answer}
Assistant A’s Answer: {answer a}
Assistant B’s Answer: {answer b}
508
Template for Multi-Turn Answer
System prompt: Please act as a judge and evaluate the quality of the responses provided by
two AI assistants to the user question displayed below. You do not need to explain, just give
your judgment. Output your final verdict by strictly following this format: "[[A]]" if assistant
A is better, "[[B]]" if assistant B is better, and "[[C]]" for a tie
Assistant A’s Conversation with User:
User: {question 1}
Assistant A: {answer a1}
User: {question 2}
Assistant A: {answer a2}
Assistant B’s Conversation with User:
User: {question 1}
Assistant B: {answer b1}
User: {question 2}
Assistant B: {answer b2}
509
14Referenced Template for Multi-Turn Answer
System prompt: Please act as a judge and evaluate the quality of the responses provided
by two AI assistants to the user question displayed below, in comparison to the reference
answers. You do not need to explain, just give your judgment. Output your final verdict by
strictly following this format: "[[A]]"if assistant A is better, "[[B]]" if assistant B is better,
and "[[C]]" for a tie.
Reference Answer
User: {question 1}
Reference answer: {ref answer 1}
User: {question 2}
Reference answer: {ref answer 2}
Assistant A’s Conversation with User:
User: {question 1}
Assistant A: {answer a1}
User: {question 2}
Assistant A: {answer a2}
Assistant B’s Conversation with User:
User: {question 1}
Assistant B: {answer b1}
User: {question 2}
Assistant B: {answer b2}
510
C Scoring Methodology 511
In Section 2.2.2, Equation 7 delineates the methodology for optimizing scores. Within this frame- 512
work, the function 1{Aj
i> Ak
i}is more precisely defined as f(Aj
i, Ak
i). Additionally, the function 513
f(Aj
i, Ak
i)is not fixed and can be implemented using various computational strategies. We introduce 514
two distinct methodologies in this context: the Elo mechanism and the Rank mechanism. 515
Within the framework of the Elo mechanism, as specified by Equation 10, the BASE value is set to 516
10, and the SCALE factor is determined to be 400. This approach facilitates a dynamic adjustment 517
of scores based on the outcomes of pairwise comparisons, allowing for a nuanced reflection of 518
performance variations among models. 519
Conversely, in the context of the Rank mechanism, as outlined by Equation 11, rank (j)signifies the 520
current ranking of model j, with the constant Kassigned a value of 200. This mechanism employs 521
a model’s ranking within a predefined hierarchy as a pivotal factor in score calculation, thereby 522
providing a straightforward, yet effective, method for evaluating comparative model performance. 523
f(Aj
i, Ak
i) =

1−1
1+BASE((G(k)−G(j))/SCALE ) ifAj
i> Ak
i
0.5−1
1+BASE((G(k)−G(j))/SCALE )ifAj
i=Ak
i
0−1
1+BASE((G(k)−G(j))/SCALE ) ifAj
i< Ak
i(10)
f(Aj
i, Ak
i) =

1 + (rank (j)−rank (k))/K ifAj
i> Ak
i
0.5 ifAj
i=Ak
i
0 ifAj
i< Ak
i(11)
D Overall Algorithm of Peer Review 524
The overall algorithm, as delineated in Algorithm 1, encapsulates the comprehensive process outlined 525
in Section 2.2. This sequence commences with "Data Collection and LLMs Pool Construction," 526
progresses through "Answer-Ranking Data Construction Based on Peer Review," advances to "Con- 527
sistency Optimization," and culminates with the "Unsupervised Elimination Mechanism." 528
15Algorithm 1 Overall Framework Algorithm of Peer Review
Require: Unlabeled dataset Q, Pool of LLMs M, Active LLM pool M∗=M
Ensure: Consistency-optimized ranking of LLMs R∗
1:Initialize response matrix A← ∅
2:foreach question qi∈ Q do
3: Initialize response vector for question qi,Ai← ∅
4: foreach model mj∈ M do
5: Ai
j←response of model mjto question qi
6: Ai←Ai∪ {Ai
j}
7: end for
8: Shuffle Aito obtain permuted response vector Ai
9: A←A∪ {Ai}
10:end for
11:Initialize answer-ranking data D← ∅
12:Initialize model weights vector wwith Gaussian distribution
13:foreach permuted response vector Aido
14: foreach pair of responses (Aj
i, Ak
i)inAido
15: fors←1to5do ▷Randomly select 5 models for evaluation
16: Evaluate the pair (Aj
i, Ak
i)with model ms
17: D←D∪ {(Aj
i, Ak
i, > ws)}
18: end for
19: end for
20:end for
21:Initialize scores Gjfor each model mj∈ M to the Elo initial score
22:repeat
23: while not converged do
24: foreach model mj∈ M do
25: Compute Gjusing updated formula:
26: Gj=P
iP
k̸=jP
s̸=k,s̸=j1{Aj
i, Ak
i} ×ws(Aj
i, Ak
i, > ws, s∈ M∗)∈D
27: end for
28: Update weight vector wto maximize the consistency of wandG
29: end while
30: SortM∗byGjto identify Mmin, the lowest-scoring model
31: ifsize of M∗> threshold then
32: Remove MminfromM∗
33: end if
34:until size of M∗< threshold
35:Compute the final ranking R∗based on the optimized scores Gj
36:return R∗
E Complete Experimental Results 529
In Section 3.4, we both employ elimination mechanisms to cull the weakest LLMs from the ’reviewer 530
queue’ during the evaluation process. In Figures 7 and 8, we present the results for the PEN and 531
LIS metrics, where lower PEN scores indicate better performance, and higher LIS scores denote 532
superior performance. It is evident that both the ’PiCO’ and PRE approaches demonstrate enhanced 533
performance as the number of eliminated ’reviewers’ increases. In most cases, the proposed ’PiCO’ 534
method outperforms PRE. 535
In Section 3.5, we validate the effectiveness of the consistency assumption and compare it with the 536
Average Performance of the Reviewer Queue, i.e., employing a single LLM as the ’reviewer’ to 537
evaluate all response pairs and then calculating the average results of all LLMs. The comprehensive 538
results compared with the Reviewer Queue are illustrated in Table3, Figure 9, 10 and 11, revealing 539
that in the full Reviewer Queue, the performance of the vast majority of LLMs is very poor, indicating 540
that the evaluations from most LLMs are noise. However, our ’PiCO’ approach nearly matches the 541
evaluative prowess of the pool’s most capable LLM, GPT-3.5. Remarkably, given its unsupervised 542
nature, the ’PiCO’ method demonstrates the capability to mitigate the influence of noise, reaching the 543
16Figure 7: Performance comparison of the PiCO (Ours) and PRE[ 17] methods on the MT-Bench, Chatbot Arena,
and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is PEN, where lower
values indicate better performance.
Figure 8: Performance comparison of the PiCO (Ours) and PRE[ 17] methods on the MT-Bench, Chatbot Arena,
and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is LIS, where upper
values indicate better performance.
Table 3: Comparison of performance across three datasets using Unsupervised methods versus using single
models in reviewer queue.
MethodsMT-Bench Chatbot Arena AlpacaEval
PEN (↓)CIN(↓)LIS(↑)PEN (↓)CIN(↓)LIS(↑)PEN (↓)CIN(↓)LIS(↑)
Gpt-3.5 0.97 12 .00 10 .000.85 11 .00 11 .001.15 16 .00 9 .00
Guanaco-33B 1.25 21 .00 8 .00 1.50 28 .00 7 .00 1.26 20 .00 9 .00
Vicuna-13B 1.31 20 .00 7 .00 1.27 23 .00 8 .00 1.20 17 .00 8 .00
WizardLM-13B 1.15 17 .00 9 .00 1.27 19 .00 8 .00 1.17 17 .00 9 .00
Vicuna-7B 1.27 21 .00 8 .00 1.30 20 .00 7 .00 1.34 23 .00 8 .00
Koala-13B 1.67 43 .00 6 .00 1.34 23 .00 8 .00 1.54 31 .00 7 .00
gpt4all-13B 1.74 45 .00 6 .00 1.60 35 .00 6 .00 1.73 42 .00 6 .00
Mpt-7B 1.67 39 .00 6 .00 1.72 52 .00 6 .00 1.63 34 .00 7 .00
Oass-pythia-12B 1.77 50 .00 5 .00 1.74 42 .00 5 .00 1.70 47 .00 6 .00
Alpaca-13B 1.77 49 .00 7 .00 1.60 73 .00 4 .00 1.63 34 .00 7 .00
FastChat-T5-3B 1.45 29 .00 7 .00 1.53 30 .00 7 .00 1.30 22 .00 7 .00
ChatGLM-6B 1.59 33 .00 7 .00 1.71 55 .00 5 .00 1.63 34 .00 6 .00
StableLM-7B 1.68 63 .00 5 .00 1.75 44 .00 5 .00 1.72 56 .00 4 .00
Dolly-12B 1.76 46 .00 6 .00 1.57 71 .00 6 .00 1.75 54 .00 6 .00
LLaMA-13B 1.60 35 .00 7 .00 1.76 56 .00 6 .00 1.70 50 .00 5 .00
Average Performance of All Review LLMs 1.51 34 .87 6 .93 1.50 38 .80 6 .60 1.50 33 .13 6 .93
PRD[28] 1.15 17 .00 8 .00 1.15 17 .00 8 .00 1.21 19 .00 9 .00
PRE[17] 1.17 17 .00 8 .00 1.07 15 .00 9 .00 1.18 19 .00 8 .00
PiCO (Ours) 1.01 14.508.75 0.94 12.0010.00 1.17 17.009.00
evaluation upper bound (the strongest LLM) within any given unknown LLM pool M, even in the 544
absence of prior ranking information. 545
17Figure 9: Comparison of performance on the CIN metric across three datasets using Unsupervised methods
versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The
dotted line represents the average value using single models.
Figure 10: Comparison of performance on the PEN metric across three datasets using Unsupervised methods
versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The
dotted line represents the average value using single models.
F Selected Models and Optimized Ranking 546
For our analysis, we meticulously selected 15 LLMs spanning a variety of architectures, encompassing 547
both open-source and closed-source models, as detailed in the subsequent table. Our curated selection 548
features prominent LLMs including the closed-source "gpt-3.5-turbo," "chatglm" which is predicated 549
on the encoder-decoder framework, "fastchat-t5-3b" that leverages Google’s T5 (Text-to-Text Transfer 550
Transformer) architecture, and "llama-13b" founded on the GPT architectural principles. 551
We have comprehensively detailed the ranking outcomes across three distinct datasets for our 552
comparative analysis, incorporating the optimized model rankings, names, and their respective scores. 553
18Figure 11: Comparison of performance on the LIS metric across three datasets using Unsupervised methods
versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The
dotted line represents the average value using single models.
As delineated in Appendix C, the PiCO (Ours) is capable of employing various scoring mechanisms, 554
thereby facilitating the presentation of ranking outcomes on three datasets utilizing both the Elo and 555
Rank mechanisms. Furthermore, we have also enumerated the ranking results for PRD and PRE 556
methodologies across the three datasets, offering a holistic view of the competitive landscape. 557
19F.1 PiCO 558
Grade-Elo-Chatbot
#1Gpt-3.5 | Grade: 9205.162109375
#2WizardLM-13B | Grade: 9143.46875
#3Guanaco-33B | Grade: 5886.92626953125
#4Vicuna-7B | Grade: 5368.9462890625
#5Vicuna-13B | Grade: 5216.79541015625
#6Koala-13B | Grade: 3545.1171875 | Eliminated
#7Mpt-7B | Grade: 962.99462890625 | Eliminated
#8Gpt4all-13B | Grade: 652.4602661132812 | Eliminated
#9Chatglm-6B | Grade: 417.1375427246094 | Eliminated
#10Oasst-pythia-12B | Grade: -898.2676391601562 | Eliminated
#11Fastchat-t5-3B | Grade: -1251.7183837890625 | Eliminated
#12StableLM-7B | Grade: -2232.66943359375 | Eliminated
#13Dolly-12B | Grade: -3163.540283203125 | Eliminated
#14Llama-13B | Grade: -3648.37841796875 | Eliminated
#15Alpaca-13B | Grade: -14204.3984375 | Eliminated
559
Grade-Elo-AlpacaEval
#1WizardLM-13B | Grade: 8662.7158203125
#2Vicuna-13B | Grade: 5586.46630859375
#3Guanaco-33B | Grade: 5445.341796875
#4Vicuna-7B | Grade: 5374.2314453125
#5Gpt-3.5 | Grade: 4845.91552734375
#6Koala-13B | Grade: 4338.77783203125 | Eliminated
#7Chatglm-6B | Grade: 2293.4208984375 | Eliminated
#8Gpt4all-13B | Grade: 2080.511962890625 | Eliminated
#9Mpt-7B | Grade: 1694.4945068359375 | Eliminated
#10Fastchat-t5-3B | Grade: 1371.94287109375 | Eliminated
#11Oasst-pythia-12B | Grade: -665.8685302734375 | Eliminated
#12StableLM-7B | Grade: -1343.5838623046875 | Eliminated
#13Dolly-12B | Grade: -5377.13427734375 | Eliminated
#14Llama-13B | Grade: -5847.59130859375 | Eliminated
#15Alpaca-13B | Grade: -13459.6162109375 | Eliminated
560
Grade-Elo-MT _Bench
#1WizardLM-13B | Grade: 2178.10302734375
#2Vicuna-13B | Grade: 1720.1114501953125
#3Guanaco-33B | Grade: 1704.1832275390625
#4Vicuna-7B | Grade: 1659.2799072265625
#5Gpt-3.5 | Grade: 1535.8819580078125
#6Mpt-7B | Grade: 1338.5235595703125 | Eliminated
#7Koala-13B | Grade: 1267.9747314453125 | Eliminated
#8Chatglm-6B | Grade: 1011.7701416015625 | Eliminated
#9Gpt4all-13B | Grade: 976.5963745117188 | Eliminated
#10Oasst-pythia-12B | Grade: 779.3573608398438 | Eliminated
#11StableLM-7B | Grade: 512.1678466796875 | Eliminated
#12Alpaca-13B | Grade: 334.9879455566406 | Eliminated
#13Fastchat-t5-3B | Grade: 303.5980529785156 | Eliminated
#14Dolly-12B | Grade: 72.63818359375 | Eliminated
#15Llama-13B | Grade: -395.19921875 | Eliminated
561
20Grade-Rank-Chatbot
#1WizardLM-13B | Grade: 0.30809280276298523
#2Gpt-3.5 | Grade: 0.293962299823761
#3Guanaco-33B | Grade: 0.28587597608566284
#4Vicuna-7B | Grade: 0.28212910890579224
#5Vicuna-13B | Grade: 0.27900218963623047
#6Koala-13B | Grade: 0.2672431766986847 | Eliminated
#7Mpt-7B | Grade: 0.2500302195549011 | Eliminated
#8Gpt4all-13B | Grade: 0.24746862053871155 | Eliminated
#9Chatglm-6B | Grade: 0.2466953843832016 | Eliminated
#10Oasst-pythia-12B | Grade: 0.23637069761753082 | Eliminated
#11Fastchat-t5-3B | Grade: 0.2350562959909439 | Eliminated
#12StableLM-7B | Grade: 0.22843806445598602 | Eliminated
#13Dolly-12B | Grade: 0.22219440340995789 | Eliminated
#14Llama-13B | Grade: 0.2165679931640625 | Eliminated
#15Alpaca-13B | Grade: 0.13975904881954193 | Eliminated
562
Grade-Rank-AlpacaEval
#1WizardLM-13B | Grade: 0.4019235074520111
#2Vicuna-13B | Grade: 0.36745429039001465
#3Guanaco-33B | Grade: 0.3664878010749817
#4Vicuna-7B | Grade: 0.36541733145713806
#5Gpt-3.5 | Grade: 0.36000365018844604
#6Koala-13B | Grade: 0.3544933795928955 | Eliminated
#7Chatglm-6B | Grade: 0.3319571018218994 | Eliminated
#8Gpt4all-13B | Grade: 0.3306528627872467 | Eliminated
#9Mpt-7B | Grade: 0.32641729712486267 | Eliminated
#10Fastchat-t5-3B | Grade: 0.32173293828964233 | Eliminated
#11Oasst-pythia-12B | Grade: 0.2999681532382965 | Eliminated
#12StableLM-7B | Grade: 0.2932431995868683 | Eliminated
#13Dolly-12B | Grade: 0.24777530133724213 | Eliminated
#14Llama-13B | Grade: 0.24381506443023682 | Eliminated
#15Alpaca-13B | Grade: 0.16114839911460876
563
Grade-Rank-MT _Bench
#1WizardLM-13B | Grade: 0.2994651198387146
#2Vicuna-13B | Grade: 0.2809261679649353
#3Guanaco-33B | Grade: 0.2767307460308075
#4Vicuna-7B | Grade: 0.2758147716522217
#5Gpt-3.5 | Grade: 0.27261608839035034
#6Mpt-7B | Grade: 0.26338690519332886 | Eliminated
#7Koala-13B | Grade: 0.2613368630409241 | Eliminated
#8Gpt4all-13B | Grade: 0.24908888339996338 | Eliminated
#9Chatglm-6B | Grade: 0.24898234009742737 | Eliminated
#10Oasst-pythia-12B | Grade: 0.2415400892496109 | Eliminated
#11StableLM-7B | Grade: 0.2299075722694397 | Eliminated
#12Alpaca-13B | Grade: 0.22171474993228912 | Eliminated
#13Fastchat-t5-3B | Grade: 0.221677765250206 | Eliminated
#14Dolly-12B | Grade: 0.21185410022735596 | Eliminated
#15Llama-13B | Grade: 0.192665234208107 | Eliminated
564
21F.2 PRD 565
PRD-Chatbot
#1WizardLM-13B | Grade: 5565.28271484375
#2Gpt-3.5 | Grade: 4613.22900390625
#3Guanaco-33B | Grade: 3423.588134765625
#4Vicuna-7B | Grade: 2985.4892578125
#5Vicuna-13B | Grade: 2972.15673828125
#6Koala-13B | Grade: 2237.70751953125
#7Chatglm-6B | Grade: 875.373779296875
#8Mpt-7B | Grade: 602.46923828125
#9Gpt4all-13B | Grade: 356.06243896484375
#10Fastchat-t5-3B | Grade: 184.89663696289062
#11Dolly-12B | Grade: 52.10746765136719
#12Oasst-pythia-12B | Grade: -307.49908447265625
#13StableLM-7B | Grade: -691.4453735351562
#14Llama-13B | Grade: -848.1654052734375
#15Alpaca-13B | Grade: -7020.923828125
566
PRD-AlpacaEval
#1WizardLM-13B | Grade: 5469.75634765625
#2Guanaco-33B | Grade: 3707.014892578125
#3Vicuna-13B | Grade: 3618.63427734375
#4Vicuna-7B | Grade: 3569.389892578125
#5Gpt-3.5 | Grade: 3197.755615234375
#6Koala-13B | Grade: 2893.642578125
#7Chatglm-6B | Grade: 1847.1300048828125
#8Fastchat-t5-3B | Grade: 1585.66943359375
#9Gpt4all-13B | Grade: 1561.145751953125
#10Mpt-7B | Grade: 1332.3753662109375
#11StableLM-7B | Grade: -33.00855255126953
#12Oasst-pythia-12B | Grade: -92.68387603759766
#13Dolly-12B | Grade: -3013.588623046875
#14Llama-13B | Grade: -3211.0302734375
#15Alpaca-13B | Grade: -7432.3701171875
567
PRD-MT _Bench
#1WizardLM-13B | Grade: 1811.64697265625
#2Vicuna-13B | Grade: 1537.8084716796875
#3Guanaco-33B | Grade: 1481.1739501953125
#4Vicuna-7B | Grade: 1401.5194091796875
#5Gpt-3.5 | Grade: 1272.8072509765625
#6Mpt-7B | Grade: 1186.5518798828125
#7Chatglm-6B | Grade: 1166.6246337890625
#8Koala-13B | Grade: 1124.2513427734375
#9Gpt4all-13B | Grade: 871.2874755859375
#10Oasst-pythia-12B | Grade: 855.3653564453125
#11StableLM-7B | Grade: 782.702880859375
#12Fastchat-t5-3B | Grade: 636.966064453125
#13Alpaca-13B | Grade: 414.9374694824219
#14Dolly-12B | Grade: 377.5018005371094
#15Llama-13B | Grade: 78.90127563476562
568
22F.3 PRE 569
PRE-Chatbot
#1WizardLM-13B | Grade: 1113.7034715479742
#2Gpt-3.5 | Grade: 1076.1116664199608
#3Guanaco-33B | Grade: 1067.441581415147
#4Vicuna-13B | Grade: 1057.702184441485
#5Vicuna-7B | Grade: 1043.4840340151043
#6Koala-13B | Grade: 1030.4455842017508 | Eliminated
#7Chatglm-6B | Grade: 1012.4487557424748 | Eliminated
#8Mpt-7B | Grade: 1000.487230109001 | Eliminated
#9Gpt4all-13B | Grade: 1000.4111397038492 | Eliminated
#10Fastchat-t5-3B | Grade: 992.3732179832363 | Eliminated
#11Oasst-pythia-12B | Grade: 977.5217305871272 | Eliminated
#12StableLM-7B | Grade: 970.3665926795535 | Eliminated
#13Llama-13B | Grade: 929.6268868888149 | Eliminated
#14Dolly-12B | Grade: 929.1943463130976 | Eliminated
#15Alpaca-13B | Grade: 798.6815779514078 | Eliminated
570
PRE-AlpacaEval
#1WizardLM-13B | Grade: 1127.822808841937
#2Vicuna-7B | Grade: 1077.1823389450524
#3Vicuna-13B | Grade: 1075.4338443616266
#4Guanaco-33B | Grade: 1074.8043135229418
#5Gpt-3.5 | Grade: 1065.305736105376
#6Gpt4all-13B | Grade: 1039.4091630861865 | Eliminated
#7Koala-13B | Grade: 1038.205749976473 | Eliminated
#8Mpt-7B | Grade: 1032.2893401162178 | Eliminated
#9Chatglm-6B | Grade: 1027.1937496918501 | Eliminated
#10Fastchat-t5-3B | Grade: 992.3481168791307 | Eliminated
#11StableLM-7B | Grade: 979.3894141445692 | Eliminated
#12Oasst-pythia-12B | Grade: 940.6438439723215 | Eliminated
#13Dolly-12B | Grade: 886.1412110662756 | Eliminated
#14Llama-13B | Grade: 880.0797724297793 | Eliminated
#15Alpaca-13B | Grade: 763.7505968602533 | Eliminated
571
PRE-MT _Bench
#1WizardLM-13B | Grade: 1065.5843776639435
#2Vicuna-13B | Grade: 1062.3934138040302
#3Guanaco-33B | Grade: 1052.2206466556906
#4Vicuna-7B | Grade: 1035.1112817247572
#5Gpt-3.5 | Grade: 1029.8316754711038
#6Koala-13B | Grade: 1024.9307662983267 | Eliminated
#7Chatglm-6B | Grade: 1020.5238960907612 | Eliminated
#8Mpt-7B | Grade: 1014.0683255081057 | Eliminated
#9Gpt4all-13B | Grade: 991.7142639623017 | Eliminated
#10StableLM-7B | Grade: 979.8443261256327 | Eliminated
#11Oasst-pythia-12B | Grade: 977.9930430111322 | Eliminated
#12Fastchat-t5-3B | Grade: 953.0776159143571 | Eliminated
#13Alpaca-13B | Grade: 949.129770731626 | Eliminated
#14Dolly-12B | Grade: 928.511065779112 | Eliminated
#15Llama-13B | Grade: 915.0655312591185 | Eliminated
572
23NeurIPS Paper Checklist 573
1.Claims 574
Question: Do the main claims made in the abstract and introduction accurately reflect the 575
paper’s contributions and scope? 576
Answer: [Yes] 577
Justification: We clearly state our claims in the abstract and introduction, such as a novel 578
unsupervised LLM evaluation method and a consistency-based constrained optimization 579
approach. These are substantiated in Section 3, demonstrating the alignment between our 580
theoretical contributions and empirical results. 581
Guidelines: 582
•The answer NA means that the abstract and introduction do not include the claims 583
made in the paper. 584
•The abstract and/or introduction should clearly state the claims made, including the 585
contributions made in the paper and important assumptions and limitations. A No or 586
NA answer to this question will not be perceived well by the reviewers. 587
•The claims made should match theoretical and experimental results, and reflect how 588
much the results can be expected to generalize to other settings. 589
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 590
are not attained by the paper. 591
2.Limitations 592
Question: Does the paper discuss the limitations of the work performed by the authors? 593
Answer: [No] 594
Justification: Although this paper does not have a separate ’Limitations’ section, the con- 595
sistency assumptions on which the work is based are clearly stated in the introduction, and 596
their validity is experimentally verified in Section 3.5. Moreover, the limitations of our work 597
are discussed in the conclusion, noting that the current study is conducted solely within a 598
text-based llm evaluation environment, and exploring the potential for future expansion into 599
multimodal large model assessments. 600
Guidelines: 601
•The answer NA means that the paper has no limitation while the answer No means that 602
the paper has limitations, but those are not discussed in the paper. 603
• The authors are encouraged to create a separate "Limitations" section in their paper. 604
•The paper should point out any strong assumptions and how robust the results are to 605
violations of these assumptions (e.g., independence assumptions, noiseless settings, 606
model well-specification, asymptotic approximations only holding locally). The authors 607
should reflect on how these assumptions might be violated in practice and what the 608
implications would be. 609
•The authors should reflect on the scope of the claims made, e.g., if the approach was 610
only tested on a few datasets or with a few runs. In general, empirical results often 611
depend on implicit assumptions, which should be articulated. 612
•The authors should reflect on the factors that influence the performance of the approach. 613
For example, a facial recognition algorithm may perform poorly when image resolution 614
is low or images are taken in low lighting. Or a speech-to-text system might not be 615
used reliably to provide closed captions for online lectures because it fails to handle 616
technical jargon. 617
•The authors should discuss the computational efficiency of the proposed algorithms 618
and how they scale with dataset size. 619
•If applicable, the authors should discuss possible limitations of their approach to 620
address problems of privacy and fairness. 621
•While the authors might fear that complete honesty about limitations might be used by 622
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 623
limitations that aren’t acknowledged in the paper. The authors should use their best 624
24judgment and recognize that individual actions in favor of transparency play an impor- 625
tant role in developing norms that preserve the integrity of the community. Reviewers 626
will be specifically instructed to not penalize honesty concerning limitations. 627
3.Theory Assumptions and Proofs 628
Question: For each theoretical result, does the paper provide the full set of assumptions and 629
a complete (and correct) proof? 630
Answer: [Yes] 631
Justification: We thoroughly detail the Consistency Assumption which underpins our the- 632
oretical results and provide a complete proof in Section 3.5. Furthermore, we ensure that 633
all necessary assumptions are explicitly stated and each theorem and proof is carefully 634
numbered and cross-referenced for clarity and accessibility. 635
Guidelines: 636
• The answer NA means that the paper does not include theoretical results. 637
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 638
referenced. 639
•All assumptions should be clearly stated or referenced in the statement of any theorems. 640
•The proofs can either appear in the main paper or the supplemental material, but if 641
they appear in the supplemental material, the authors are encouraged to provide a short 642
proof sketch to provide intuition. 643
•Inversely, any informal proof provided in the core of the paper should be complemented 644
by formal proofs provided in appendix or supplemental material. 645
• Theorems and Lemmas that the proof relies upon should be properly referenced. 646
4.Experimental Result Reproducibility 647
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 648
perimental results of the paper to the extent that it affects the main claims and/or conclusions 649
of the paper (regardless of whether the code and data are provided or not)? 650
Answer: [Yes] 651
Justification: We provide detailed pseudocode of our new LLM evaluation algorithm in 652
Appendix D and have made all relevant data and code publicly accessible on GitHub, 653
ensuring anonymity during the review process. This comprehensive disclosure allows other 654
researchers to reproduce our experimental results, fully aligning with our paper’s claims and 655
enhancing the credibility of our findings. 656
Guidelines: 657
• The answer NA means that the paper does not include experiments. 658
•If the paper includes experiments, a No answer to this question will not be perceived 659
well by the reviewers: Making the paper reproducible is important, regardless of 660
whether the code and data are provided or not. 661
•If the contribution is a dataset and/or model, the authors should describe the steps taken 662
to make their results reproducible or verifiable. 663
•Depending on the contribution, reproducibility can be accomplished in various ways. 664
For example, if the contribution is a novel architecture, describing the architecture fully 665
might suffice, or if the contribution is a specific model and empirical evaluation, it may 666
be necessary to either make it possible for others to replicate the model with the same 667
dataset, or provide access to the model. In general. releasing code and data is often 668
one good way to accomplish this, but reproducibility can also be provided via detailed 669
instructions for how to replicate the results, access to a hosted model (e.g., in the case 670
of a large language model), releasing of a model checkpoint, or other means that are 671
appropriate to the research performed. 672
•While NeurIPS does not require releasing code, the conference does require all submis- 673
sions to provide some reasonable avenue for reproducibility, which may depend on the 674
nature of the contribution. For example 675
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 676
to reproduce that algorithm. 677
25(b)If the contribution is primarily a new model architecture, the paper should describe 678
the architecture clearly and fully. 679
(c)If the contribution is a new model (e.g., a large language model), then there should 680
either be a way to access this model for reproducing the results or a way to reproduce 681
the model (e.g., with an open-source dataset or instructions for how to construct 682
the dataset). 683
(d)We recognize that reproducibility may be tricky in some cases, in which case 684
authors are welcome to describe the particular way they provide for reproducibility. 685
In the case of closed-source models, it may be that access to the model is limited in 686
some way (e.g., to registered users), but it should be possible for other researchers 687
to have some path to reproducing or verifying the results. 688
5.Open access to data and code 689
Question: Does the paper provide open access to the data and code, with sufficient instruc- 690
tions to faithfully reproduce the main experimental results, as described in supplemental 691
material? 692
Answer: [Yes] 693
Justification: All necessary data and code have been made publicly available on GitHub, 694
with detailed instructions for installation, environment setup, and execution commands. This 695
includes all raw, pre-processed, intermediate, and generated data needed to reproduce our 696
experimental results. The repository is anonymous during the review process to ensure 697
compliance with double-blind requirements. This thorough documentation ensures that 698
other researchers can faithfully replicate our study. 699
Guidelines: 700
• The answer NA means that paper does not include experiments requiring code. 701
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 702
public/guides/CodeSubmissionPolicy ) for more details. 703
•While we encourage the release of code and data, we understand that this might not be 704
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 705
including code, unless this is central to the contribution (e.g., for a new open-source 706
benchmark). 707
•The instructions should contain the exact command and environment needed to run to 708
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 709
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 710
•The authors should provide instructions on data access and preparation, including how 711
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 712
•The authors should provide scripts to reproduce all experimental results for the new 713
proposed method and baselines. If only a subset of experiments are reproducible, they 714
should state which ones are omitted from the script and why. 715
•At submission time, to preserve anonymity, the authors should release anonymized 716
versions (if applicable). 717
•Providing as much information as possible in supplemental material (appended to the 718
paper) is recommended, but including URLs to data and code is permitted. 719
6.Experimental Setting/Details 720
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 721
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 722
results? 723
Answer: [Yes] 724
Justification: We have detailed the data processing and training procedures in Sections 2.2 725
and Appendices A, B, and D. For comprehensive understanding, additional information 726
such as hyperparameters, optimizer types, and detailed data splits are provided alongside 727
the code due to space constraints in the paper. 728
Guidelines: 729
• The answer NA means that the paper does not include experiments. 730
26•The experimental setting should be presented in the core of the paper to a level of detail 731
that is necessary to appreciate the results and make sense of them. 732
•The full details can be provided either with the code, in appendix, or as supplemental 733
material. 734
7.Experiment Statistical Significance 735
Question: Does the paper report error bars suitably and correctly defined or other appropriate 736
information about the statistical significance of the experiments? 737
Answer: [Yes] 738
Justification: We conducted each experiment four times using different seeds ( seed = 739
1,2,3,4) to ensure robustness. The results, presented as averages, are accompanied by 740
standard deviations as error bars in Tables 1 and 2. This approach captures the variability 741
due to different initializations and confirms the reproducibility of our results. The standard 742
deviations used help clarify the extent of variability in the experiments, ensuring that our 743
statistical analysis aligns with best practices for empirical research. 744
Guidelines: 745
• The answer NA means that the paper does not include experiments. 746
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 747
dence intervals, or statistical significance tests, at least for the experiments that support 748
the main claims of the paper. 749
•The factors of variability that the error bars are capturing should be clearly stated (for 750
example, train/test split, initialization, random drawing of some parameter, or overall 751
run with given experimental conditions). 752
•The method for calculating the error bars should be explained (closed form formula, 753
call to a library function, bootstrap, etc.) 754
• The assumptions made should be given (e.g., Normally distributed errors). 755
•It should be clear whether the error bar is the standard deviation or the standard error 756
of the mean. 757
•It is OK to report 1-sigma error bars, but one should state it. The authors should 758
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 759
of Normality of errors is not verified. 760
•For asymmetric distributions, the authors should be careful not to show in tables or 761
figures symmetric error bars that would yield results that are out of range (e.g. negative 762
error rates). 763
•If error bars are reported in tables or plots, The authors should explain in the text how 764
they were calculated and reference the corresponding figures or tables in the text. 765
8.Experiments Compute Resources 766
Question: For each experiment, does the paper provide sufficient information on the com- 767
puter resources (type of compute workers, memory, time of execution) needed to reproduce 768
the experiments? 769
Answer: [No] 770
Justification: Although we did not detail the exact compute resources for each experimental 771
setup in the paper, we used NVIDIA A6000 graphics cards for open-source models and API 772
calls for proprietary models. To facilitate reproducibility, we have provided all necessary 773
data, ensuring that the experiments can be replicated on consumer-grade computers. This 774
approach allows readers to reproduce the results without requiring high-end computational 775
resources. 776
Guidelines: 777
• The answer NA means that the paper does not include experiments. 778
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 779
or cloud provider, including relevant memory and storage. 780
•The paper should provide the amount of compute required for each of the individual 781
experimental runs as well as estimate the total compute. 782
27•The paper should disclose whether the full research project required more compute 783
than the experiments reported in the paper (e.g., preliminary or failed experiments that 784
didn’t make it into the paper). 785
9.Code Of Ethics 786
Question: Does the research conducted in the paper conform, in every respect, with the 787
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 788
Answer: [Yes] 789
Justification: The research conducted in this paper complies with the NeurIPS ethics 790
guidelines in all respects. 791
Guidelines: 792
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 793
•If the authors answer No, they should explain the special circumstances that require a 794
deviation from the Code of Ethics. 795
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 796
eration due to laws or regulations in their jurisdiction). 797
10.Broader Impacts 798
Question: Does the paper discuss both potential positive societal impacts and negative 799
societal impacts of the work performed? 800
Answer: [Yes] 801
Justification: In the introduction, we discuss the potential positive impact of our novel 802
unsupervised LLM evaluation approach, which could significantly advance the field of LLM 803
evaluation. However, we also recognize potential negative societal impacts, such as the 804
misuse of this technology to unfairly or inaccurately assess LLM systems, which might 805
lead to biased or misleading outcomes. We suggest potential mitigation strategies, such as 806
implementing robust validation protocols and ethical guidelines to govern the application of 807
this evaluation methodology. 808
Guidelines: 809
• The answer NA means that there is no societal impact of the work performed. 810
•If the authors answer NA or No, they should explain why their work has no societal 811
impact or why the paper does not address societal impact. 812
•Examples of negative societal impacts include potential malicious or unintended uses 813
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 814
(e.g., deployment of technologies that could make decisions that unfairly impact specific 815
groups), privacy considerations, and security considerations. 816
•The conference expects that many papers will be foundational research and not tied 817
to particular applications, let alone deployments. However, if there is a direct path to 818
any negative applications, the authors should point it out. For example, it is legitimate 819
to point out that an improvement in the quality of generative models could be used to 820
generate deepfakes for disinformation. On the other hand, it is not needed to point out 821
that a generic algorithm for optimizing neural networks could enable people to train 822
models that generate Deepfakes faster. 823
•The authors should consider possible harms that could arise when the technology is 824
being used as intended and functioning correctly, harms that could arise when the 825
technology is being used as intended but gives incorrect results, and harms following 826
from (intentional or unintentional) misuse of the technology. 827
•If there are negative societal impacts, the authors could also discuss possible mitigation 828
strategies (e.g., gated release of models, providing defenses in addition to attacks, 829
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 830
feedback over time, improving the efficiency and accessibility of ML). 831
11.Safeguards 832
Question: Does the paper describe safeguards that have been put in place for responsible 833
release of data or models that have a high risk for misuse (e.g., pretrained language models, 834
image generators, or scraped datasets)? 835
28Answer: [NA] 836
Justification: This paper introduces a new approach for unsupervised LLM evaluation and 837
does not involve the release of pre-trained models, image generators, or newly collected 838
datasets. Therefore, there are no direct risks associated with misuse or dual-use of such 839
resources, making safeguards for controlled release irrelevant to this study. 840
Guidelines: 841
• The answer NA means that the paper poses no such risks. 842
•Released models that have a high risk for misuse or dual-use should be released with 843
necessary safeguards to allow for controlled use of the model, for example by requiring 844
that users adhere to usage guidelines or restrictions to access the model or implementing 845
safety filters. 846
•Datasets that have been scraped from the Internet could pose safety risks. The authors 847
should describe how they avoided releasing unsafe images. 848
•We recognize that providing effective safeguards is challenging, and many papers do 849
not require this, but we encourage authors to take this into account and make a best 850
faith effort. 851
12.Licenses for existing assets 852
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 853
the paper, properly credited and are the license and terms of use explicitly mentioned and 854
properly respected? 855
Answer: [Yes] 856
Justification: This paper utilizes the FastChat project’s code, along with several other pre- 857
trained models and datasets. The FastChat project adheres to the Apache License 2.0. In 858
compliance with the licensing requirements, we have included the original project’s licensing 859
information in all derivative works and have clearly marked any modifications made to the 860
code. Additionally, we have ensured that all utilized pre-trained models and datasets are 861
appropriately cited. 862
Guidelines: 863
• The answer NA means that the paper does not use existing assets. 864
• The authors should cite the original paper that produced the code package or dataset. 865
•The authors should state which version of the asset is used and, if possible, include a 866
URL. 867
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 868
•For scraped data from a particular source (e.g., website), the copyright and terms of 869
service of that source should be provided. 870
•If assets are released, the license, copyright information, and terms of use in the 871
package should be provided. For popular datasets, paperswithcode.com/datasets 872
has curated licenses for some datasets. Their licensing guide can help determine the 873
license of a dataset. 874
•For existing datasets that are re-packaged, both the original license and the license of 875
the derived asset (if it has changed) should be provided. 876
•If this information is not available online, the authors are encouraged to reach out to 877
the asset’s creators. 878
13.New Assets 879
Question: Are new assets introduced in the paper well documented and is the documentation 880
provided alongside the assets? 881
Answer: [NA] 882
Justification: The paper does not release new assets. 883
Guidelines: 884
• The answer NA means that the paper does not release new assets. 885
•Researchers should communicate the details of the dataset/code/model as part of their 886
submissions via structured templates. This includes details about training, license, 887
limitations, etc. 888
29•The paper should discuss whether and how consent was obtained from people whose 889
asset is used. 890
•At submission time, remember to anonymize your assets (if applicable). You can either 891
create an anonymized URL or include an anonymized zip file. 892
14.Crowdsourcing and Research with Human Subjects 893
Question: For crowdsourcing experiments and research with human subjects, does the paper 894
include the full text of instructions given to participants and screenshots, if applicable, as 895
well as details about compensation (if any)? 896
Answer: [NA] 897
Justification: This paper focuses on an unsupervised evaluation method for LLMs that 898
does not require human feedback or interaction. Consequently, there is no involvement of 899
crowdsourcing or research with human subjects, making details about participant instructions 900
and compensation irrelevant. 901
Guidelines: 902
•The answer NA means that the paper does not involve crowdsourcing nor research with 903
human subjects. 904
•Including this information in the supplemental material is fine, but if the main contribu- 905
tion of the paper involves human subjects, then as much detail as possible should be 906
included in the main paper. 907
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 908
or other labor should be paid at least the minimum wage in the country of the data 909
collector. 910
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 911
Subjects 912
Question: Does the paper describe potential risks incurred by study participants, whether 913
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 914
approvals (or an equivalent approval/review based on the requirements of your country or 915
institution) were obtained? 916
Answer: [NA] 917
Justification: The paper does not involve crowdsourcing nor research with human subjects. 918
Guidelines: 919
•The answer NA means that the paper does not involve crowdsourcing nor research with 920
human subjects. 921
•Depending on the country in which research is conducted, IRB approval (or equivalent) 922
may be required for any human subjects research. If you obtained IRB approval, you 923
should clearly state this in the paper. 924
•We recognize that the procedures for this may vary significantly between institutions 925
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 926
guidelines for their institution. 927
•For initial submissions, do not include any information that would break anonymity (if 928
applicable), such as the institution conducting the review. 929
30