Are More LM Calls All You Need? Towards the
Scaling Properties of Compound AI Systems
Lingjiao Chen1, Jared Davis1, Boris Hanin3, Peter Bailis1,
Ion Stoica2, Matei Zaharia2, James Zou1
Stanford University1
University of California, Berkeley2
Princeton University3
Abstract
Many recent state-of-the-art results in language tasks were achieved using com-
pound systems that perform multiple Language Model (LM) calls and aggregate
their responses. However, there is little understanding of how the number of LM
calls – e.g., when asking the LM to answer each question multiple times and tak-
ing a majority vote – affects such a compound system’s performance. In this paper,
we initiate the study of scaling properties of compound inference systems. We an-
alyze, theoretically and empirically, how the number of LM calls affects the per-
formance of Vote and Filter-Vote, two of the simplest compound system designs,
which aggregate LM responses via majority voting, optionally applying LM ﬁlters.
We ﬁnd, surprisingly, that across multiple language tasks, the performance of both
Vote and Filter-Vote can ﬁrst increase but then decrease as a function of the num-
ber of LM calls. Our theoretical results suggest that this non-monotonicity is due
to the diversity of query difﬁculties within a task: more LM calls lead to higher
performance on “easy” queries, but lower performance on “hard” queries, and non-
monotone behavior can emerge when a task contains both types of queries. This
insight then allows us to compute, from a small number of samples, the number
of LM calls that maximizes system performance, and deﬁne an analytical scaling
model for both systems. Experiments show that our scaling model can accurately
predict the performance of Vote and Filter-Vote systems and thus ﬁnd the optimal
number of LM calls to make.
1 Introduction
Compound AI systems that perform multiple Language Model (LM) calls and aggregate their re-
sponses are increasingly leveraged to solve language tasks [ ZKC+24,DLT+23,TAB+23,TWL+24,
WWS+22]. For example, Google’s Gemini Ultra achieved state-of-the-art results on MMLU using a
CoT@32 voting strategy: the LM is called 32 times, and then the majority vote of the 32 responses is
used in the ﬁnal response [ TAB+23]. Other compound systems ﬁlter responses using an LM before
selecting one [ Alp23 ].
A natural question is, thus, how does scaling the number of LM calls affect the performance of
such compound systems? This question is under-explored in research, but characterizing the scaling
dynamics is crucial for researchers and practitioners to estimate how many LM calls are needed for
their applications and allocate computational resources aptly. Understanding these scaling dynamics
is also helpful in recognizing the limits of compound inference strategies.
As a ﬁrst step towards answering this question, we study the scaling properties of two popular
compound system designs, Vote and Filter-Vote. Vote aggregates multiple proposed answers via
majority vote, as in Gemini’s CoT@32. Filter-Vote leverages a ﬁlter before performing a majority
38th Conference on Neural Information Processing Systems (NeurIPS 2024).(c) GPQA (b) TRUTHFULQA (d) A VERITEC (a) MMLU PHYSICS
Figure 1: The scaling behavior of Vote and Filter-Vote. Interestingly, their performance is often a
non-monotonic function of the number of LM calls. For example, as the number LM calls increases,
Vote’s performance initially increases but then decreases, while Filter-Vote’s performance initially
decreases but then increases on the MMLU PHYSICS dataset.
vote, similar to AlphaCode 2 [ Alp23 ]. While the inference system design space is broad, we focus
on Vote and Filter-Vote for two reasons. First, they have already been used in real applications,
such as Gemini’s CoT@32 strategy. Second, despite their simplicity, they exhibit nontrivial scaling
properties. Speciﬁcally, although one might expect their performance to monotonically increase as
more LM calls are invoked, we have identiﬁed a surprising phenomenon, across multiple language
tasks, exhibited by these systems: growing the number of LM calls initially improves performance
but then degrades it, as shown in Figure 1.
This surprising effect motivates our theoretical study of Vote and Filter-Vote, which explains this non-
monotone effect through the diversity of query difﬁculty in a given task (see Figure 2and Theorem
2). At a high level, our results show that more LM calls continuously lead to better performance
on “easy” queries and worse performance on “hard” queries. When a task is some mixture of
“easy” and “hard” queries, the non-monotone aggregate behavior emerges. Formally, a query is easy
if a compound system with inﬁnitely many LM calls gives a correct answer and hard otherwise.
We provide mathematical conditions under which a query is easy or difﬁcult for Vote or Filter-
Vote. We also derive a performance scaling model for both systems that explicitly models query
difﬁculties, and present an algorithm that lets users ﬁt the scaling law’s parameters using a small
number of samples. In experiments with GPT-3.5, we show that these algorithms can let us estimate
the scaling behavior for various problems and identify the optimal number of calls to make to the
LM to maximize accuracy. Our main contributions are:
Main Contributions
Non-monotonic Scaling Behavior. We ﬁnd empirically that the performance of Vote and
Filter-Vote is often non-monotonic in the number of LM calls.
Query Difﬁculty-based Explanation. We propose a formal notion of query difﬁculty. We
then argue empirically and show in a simple analytical model that the diversity of query
difﬁculty can explain the scaling behavior of Vote and Filter-Vote. Additional LM calls
improve performance on easy queries and degrades performance on difﬁcult queries. We
provide precise conditions under which the non-monotone scaling behavior emerges on
datasets containing both easy and difﬁcult queries in our analytical model.
Heuristic for Optimal Number of LM Calls. We empirically validate predictions for the
optimal number of LM calls from our analytical model for Vote and Filter-Vote, which can
be estimated from a small number of queries.
In a nutshell, our work shows that more LM calls do not necessarily improve the performance of
compound AI systems and that it is possible , at least in some cases, to predict how the number
of LM calls affects AI systems’ performance and thus decide the optimal number of LM calls for
a given task . Our work focuses on tasks with a fairly small number of possible responses (e.g.,
multiple-choice questions) that support a majority vote, but tasks with many valid outputs, such as
2(b) V ote easy (53%) (a) V ote overall (100%) (c) Vote dif ficult (47%)
(e) Filter-V ote easy (56%) (d) Filter-V ote overall (100%) (f) Filter-V ote dif ficult (44%)
Figure 2: Performance breakdown on MMLU PYHSICS. As the number of LM calls increases, Vote
and Filter-Vote perform increasingly better on easy queries but increasingly worse on difﬁcult ones.
chat, remain under-explored. We have released the code and datasets1used in this paper, and hope
to excite more research regarding the scaling properties of compound AI systems.
2 Related Work
Neural Scaling Laws. There has been extensive research on how the training parameters affect
the performance of neural language models [ KMH+20,SGS+22,BDK+21,MLP+23,IPH+24].
Among others, the model loss has been empirically shown to follow a power law as the number
of model parameters and training tokens [ KMH+20]. Researchers have also proposed theories to
explain the empirical scaling laws [ BDK+21]. By contrast, recent work has found that scaling up
the model parameters leads to performance decay on certain tasks [ MLP+23]. To the best of our
knowledge, there is no study on how the number of LM calls affects the performance of a compound
system, which is complementary to scaling laws on training parameters and data set sizes.
Compound Systems using LMs. Many inference strategies that perform multiple model calls
have been developed to advance performance on various language processing tasks [ XCG+23,
DLT+23,TAB+23,TWL+24,WWS+22,CZZ23 ,ZKAW23 ,ŠPW23 ]. For example, Gemini
reaches state-of-the-art performance on MMLU via its CoT@32 majority voting scheme [ TAB+23].
Self-consistency [ WWS+22] boosts the performance of chain-of-thought via a majority vote scheme
over multiple reasoning paths generated by PaLM-540B. Finally, AlphaCode 2 [ Alp23 ] matches the
85th percentile of humans in a coding contest regime by generating up to one million samples per
problem via an LM and then ﬁltering the answer set down. While these approaches are empiri-
cally compelling, there has been little systematic study of how the number of LM calls affects these
systems’ performance, and how it should be tuned for a given application.
Compound System Evaluation. Compound AI systems are increasingly evaluated in tradi-
tional benchmarks [ CKB+21,HBK+21,TVCM18 ,NWD+19] as well as domain-speciﬁc new
datasets [ KBGA24 ,MMA+24,KCM+23] and the agent environments [ LYZ+23,SYC+20,
JYW+24]. We refer the interested readers to a survey for more details [ CWW+24]. Existing pa-
pers focus on obtaining a single metric of a given system, while our goal is to understand how the a
compound system’s performance is affected by its parameters (e.g., # of LM calls).
1https://github.com/lchen001/CompoundAIScalingLaws
3Algorithm 1: Vote.
Input: A user query x, # gen responses K
Output: A response ˆy
1Sample θ1, θ2,· · ·, θKi.i.d. from Θ;
2Generate zk=G(x, θk), k= 1,· · ·, K;
3SetˆyK=V(z1,· · ·, zK);
4Return ˆyKAlgorithm 2: Filter-Vote.
Input: A user query x, # gen responses K
Output: A response ˆy
1Sample θ1, θ2,· · ·, θKi.i.d. from Θ;
2Generate zk, ek=G(x, θk), k= 1,· · ·, K;
3Generate wk= Φ(x, zk, ek), k= 1,· · ·, K;
4ifmax kwk= 0then
5 SetˆyK=V(z1,· · ·, zK);
6else
7 SetˆyK=V({ |zk|wk= 1| });
8Return ˆyK
3 Inference System Designs
In this paper, we focus on two simple and natural inference system designs: Vote and Filter-
Vote. Vote is inspired by and resembles several real-world compound AI systems, such as self-
consistency [ WWS+22], Medprompt [ NKM+23], and Gemini CoT@32 strategy [ TAB+23], while
Filter-Vote represent many other real-world compound AI systems including AlphaCode 2 [ Alp23 ]
and AlphaGeometry [ TWL+24]. Note that this paper focuses on tasks with a small number of
possible answers.
Building Blocks. Vote and Filter-Vote rely on three building blocks, a generator G(·,·), a majority
voter V(·), and a ﬁlter Φ(·,·,·). The generator G(·,·)takes a user query xandθ∈Θas inputs
and produces a candidate answer and an explanation. Here, instantiations of Θare a design choice
of users and can encode many generation strategies. For example, even with a single ﬁxed LM,
diverse generations may be achieved by using a non-zero temperature and different prompt word-
ings or few-shot examples for each call to the LM. If Θcontains different LMs, then this system
deﬁnition can also represent LM ensembles. The majority voter Vreturns the mode of its input, i.e.,
V(z1, z2,· · ·, zK)≜arg max a∈A∑K
k=1/x31zk=a, and breaks ties arbitrarily. Here, Ais the space of
all possible answers. Finally, the ﬁlter Φ(·,·)takes the user query and multiple candidate answers
as input, and only returns the subset that an LM believes is correct.
Vote. Given a user query x, Vote (i) ﬁrst creates Kcandidate answers by calling the generator G,
and then (ii) uses the majority voter Vto choose one as the ﬁnal response ˆyK. The details are given
in Algorithm 1.
Filter-Vote. Given a user query, Filter-Vote (i) ﬁrst generates multiple candidate answers, (ii) re-
moves a few candidate answers by the ﬁlter Φ, and (iii) then uses the majority voter Vto choose one
from the remaining answers as the ﬁnal response. If all answers are removed by the ﬁlter, then Vis
applied on the original candidate answers. Algorithm 2gives the formal description.
4 Analytical Model of Scaling Behavior
Now we present our analytical performance model of Vote and Filter-Vote strategies. Speciﬁcally,
we are interested in understanding the behavior of F(K;D)≜ /x45[ˆyK=y], where the expectation
is over Dand the candidate responses. All notations are summarized in Table 1.
4.1 When do more LM calls lead to an increase or decrease in performance?
Our ﬁrst key insight is that individual query difﬁculty is crucial in LM calls’ effects. To see this, let
us ﬁrst introduce query difﬁculty indicator.
Deﬁnition 1. Given a user query x,d(x)is called an query difﬁculty indicator if
lim
K→∞F(K, x) ={0iffd(x)>0,
1iffd(x)<0
4Table 1: Notations.
Symbol Meaning
x an input query
y the correct answer
K the number of LM calls
zk the output by one LM call
ˆyK the output by an inference system using KLM calls
D/D Tr test dataset/train dataset
A answer space
α fraction of easy queries
p1 probability of zkbeing correct for easy queries
p2 probability of zkbeing correct for difﬁcult queries
F(K;D)Accuracy of an Inference System with KLM calls per query on D
G(K;D) Analytical Performance Model (to approximate F(K;D))
0 0.2 0.4 0.6 0.8 100.20.40.60.81
Subset 1 Difficult ySubset 2 Difficult y
0 0.2 0.4 0.6 0.8 100.20.40.60.81
Decrease
Inverse U shape
U shape
Increase
Subset 1 Difficult ySubset 2 Difficult y
0 0.2 0.4 0.6 0.8 100.20.40.60.81
Subset 1 Difficult ySubset 2 Difficult y
Figure 3: How the query difﬁculties shape the landscape of a one-layer Voting Inference System’s
performance. Informally, if the overall task is “easy” ( p1+p2>1), but the fraction of “hard”
queries is large ( α < 1−1
t), then as the number of LM calls increases, the Voting Inference
Systems’ performance increases ﬁrst but then decreases. We call such a landscape a “inverse U
shape”. Similarly, if the overall task is “hard” ( p1+p2<1), but the fraction of “hard” queries is
small ( α > 1−1
t), then enlarging the number of LM calls leads an initial decrease and then increase.
Such a landscape is called a “U shape”. When αis large, the U-shape is less likely to occur while
the inverse U-shape becomes more common. Smaller αleads to an opposite trend.
Intuitively, a positive query difﬁculty indicator implies the query is difﬁcult, i.e., inﬁnitely many LM
calls lead to an incorrect ﬁnal answer, and a negative value implies the query is easy, i.e., inﬁnitely
many LM calls eventually give a correct ﬁnal answer. For simplicity, we assume that the limit of
F(K, x)is always either 0 or 1 for Vote and Filter-Vote, i.e., eventually the answer is correct or
incorrect. Also note that d(x)is scale-invariant, i.e., if d(x)is an item difﬁculty indicator, then for
any positive scalar γ > 0,γ×d(x)is also a difﬁculty indicator. We will call a query xdifﬁcult
(easy) if d(x)>0(d(x)<0). We give two concrete instantiations as follows.
Lemma 1. For Vote, dV(x)≜max a̸=yPr[G(x, θ) = a]−Pr[G(x, θ) = y]is an
query difﬁculty indicator. For Filter-Vote, denote G(x, θ) = [ G1(x, θ), G2(x, θ)]. Then
dF(x)≜max a̸=yPr[G1(x, θ) = a|Φ(x, G 1(x, θ), G2(x, θ)) = 1] −Pr[G1(x, θ) =
y|Φ(x, G 1(x, θ), G2(x, θ)) = 1] is a query difﬁculty indicator.
5Here, dV(x)>0indicates that an incorrect answer is more likely to be generated than the correct
one, hence the query is difﬁcult. Similarly, dF(x)>0implies that an incorrect answer is more
likely to be kept in the ﬁltered answer set.
More LM calls elicit higher performance on easy queries, but lower performance on difﬁcult queries.
Therefore, the performance, F(K;D), is more difﬁcult to characterize when the data set Dcontains
both easy and difﬁcult queries. Here, we study Vote on a special case of Dto understand how the
difﬁculty impacts the performance function F(K;D).
A case study on a speciﬁc dataset. Let us consider a speciﬁc dataset Dα,p1,p2with answer space
cardinality |A|= 2. Here, α∈[0,1]queries in Darex1such that Pr[G(x1, θ) =y] =p1>1
2, and
1−αqueries are x2such that Pr[G(x2, θ) =y] =p2<1
2. The following theorem qualitatively
characterizes the performance of Vote on this dataset.
Theorem 2. Lett≜p2(1−p2)(1
2−p2)
p1(1−p1)(p1−1
2)+ 1. Ifp1+p2̸= 1andKis odd, then F(K;Dα,p1,p2)
•increases monotonically, if p1+p2>1andα≥1−1
t
•decreases monotonically, if p1+p2<1orα≤1−1
t
•increases and then decreases, if p1+p2>1andα < 1−1
t
•decreases and then increases, if p1+p2<1andα > 1−1
t
Theorem 2precisely connects the query difﬁculty with the performance landscape. Here, tis a
constant that only depends on p1andp2, i.e., the probability of an LM’s generation being correct
on easy and hard queries, respectively. Intuitively, tquantiﬁes the difﬁculty similarity between the
easy and hard queries: it becomes larger if the easy queries are more difﬁcult ( p1is smaller) or the
hard queries are less difﬁcult ( p2is larger). Interestingly, it suggests that, for some query difﬁculty
distribution, a non-monotone effect of the number of LM calls is expected. Informally, if the overall
task is “easy” ( p1+p2>1), but the fraction of “hard” queries is large ( α < 1−1
t), then as the
number of LM calls increases, the Voting Inference Systems’ performance increases ﬁrst but then
decreases. We call such a landscape a “inverse U shape”. Similarly, if the overall task is “hard”
(p1+p2<1), but the fraction of “hard” queries is small ( α > 1−1
t), then enlarging the number of
LM calls leads an initial decrease and then increase. Such a landscape is called a “U shape”. This
well explains the U-shape of Inference Systems’ performance shown in Figure 1.Figure 3visualizes
the effects of query difﬁculty on the performance landscape in more detail.
4.2 What is the analytical scaling model?
Now we derive an analytical scaling model for both Vote and Filter-Vote. Noting that the perfor-
mance is the average of F(K, x)for each xin the dataset, the key challenge is identifying the shape
ofF(K, x)for easy and difﬁcult queries. Let us ﬁrst consider the special case |A|= 2, where we
can obtain a close form result for Vote.
Theorem 3. If|A|= 2 , then on any query x, the performance of Vote is F(K, x) =
I1−dV(x)
2(K+1
2,K+1
2), where Ix(a, b)≜∫x
0ta−1(1−t)b−1dt/∫1
0ta−1(1−t)b−1dtis the regu-
larized incomplete beta function.
Thus, for Vote with |A|= 2,F(K, D ) = /x45x∼D[I1−dV(x)
2(K+1
2,K+1
2)].
How about Filter-Vote and the general answer space? Admittedly, an exact scaling model is chal-
lenging to obtain. Instead, we give an approximation model inspired by the special case. We ﬁrst
note that F(K, x)should be treated separately for difﬁcult and easy queries: after all, as a function
ofa, the incomplete beta function Ix(a, a)monotonically increases/decreases if x >1
2(x <1
2).
Second, Ix(a, a)grows roughly exponentially in x, and this trend should hold for general answer
space and for both Vote and Filter-Vote. Hence, we propose the following scaling model
G(K, x)≜{
e−c1(x)K−c2(x)√
K+c3(x), ifd(x)>0,
1−e−c1(x)K−c2(x)√
K+c3(x),ifd(x)<0
6where constants c1(x)>0, c2(x)>0, c3(x)do not depend on the number of LM calls K. There-
fore, our analytical performance scaling model is G(K, D ) = /x45x∼D[G(K, x)]. In practice, one
can use a training dataset DTrto ﬁt the parameters in G(K, D ). Note that given a query x, the pa-
rameters ci(x)can be different for Vote and Filter-Vote. In particular, if the ﬁlter is of high quality,
then the performance should converge quickly, and thus the constants ci(·)are likely to be larger.
Otherwise, the performance should scale slower, and thus the constants ci(·)should be smaller. We
will show in the experiments that G(K, D )matches the empirical performance F(K, D )accurately.
4.3 How to optimize the number of LM calls?
In general, one can always (i) ﬁt the analytical scaling model G(K, D ), and (ii) then use
max KG(K, D )to obtain the optimal number of LM calls. Interestingly, we show that for a special
case, we can derive the optimal number of LM calls.
Theorem 4. Ifp1+p2>1andα < 1−1
t, then the number of LM calls K∗that maximizes
F(K, D α,p1,p2)for Vote (up to rounding) is
K∗= 2logα
1−α2p1−1
1−2p2
logp2(1−p2)
p1(1−p1)
The optimal number of LM calls depends on the query difﬁculty. For example, K∗will be larger if
αgrows (up to 1−1
t). That is, if there are more “easy” queries than “difﬁcult” queries, then more
LM calls should be adopted.
5 Experiments
We compare the empirical performance of Vote and Filter-Vote and the performance predicted by
our analytical scaling model. Our goal is three-fold: (i) validate that there are cases where more LM
calls do not monotonically improve the performance of these Inference Systems, (ii) justify that the
number of LM calls has opposite effects on easy and difﬁcult queries, and (iii) explore whether our
analytical scaling model can accurately predict the performance of Vote and Filter-Vote, and thus
guide the design of Inference Systems such as optimizing number of LM calls.
Datasets and LM. To understand the scaling properties of Vote and Filter-Vote, we conduct sys-
tematical experiments on both (i) real-world datasets and (ii) synthetic datasets with controlled query
difﬁculties. Speciﬁcally, the real-world datasets include MMLU PHYSICS [ HBB+20], TRUTH-
FULQA [ LHE21 ], GPQA [ RHS+23], and A VERITEC [ SGV24 ]. MMLU PHYSICS contains high
school physics questions extracted from the original MMLU dataset. TRUTHFULQA measures
whether a language model is truthful in generating answers to questions. GPQA queries are gener-
ated by experts in biology, physics, and chemistry. Each query in A VERITEC is a claim and the
goal is to verify its correctness. Each query is prompted as a multiple-choice question for objective
evaluation. The details of these datasets and prompts can be found in the Appendix. The synthetic
dataset is Dα,p1,p2as introduced in Section 3, and we study the scaling behavior by varying the
parameters. We use GPT-3.5-turbo-0125 on the real-world datasets. All experiments are averaged
over 1,000 runs.
Non-monotonic Scaling Behavior. We start by understanding how the number of LM calls affects
the performance of Vote and Filter-Vote empirically. As shown in Figure 1, we observe a non-
monotonic behavior: more LM calls can sometimes lead to a drop in performance! This underscores
the importance of scaling performance modeling.
A case study on A VERITEC. Now let us perform a case study on the A VERITEC dataset
to understand the intriguing behavior better. In particular, we use the deployment partition of
A VERITEC [ SGV24 ], which contains 500 fact veriﬁcation questions. The goal is to determine
if a given claim should be (A) refused, (B) supported, or there is (C) conﬂicting evidence or (D)
not enough evidence. We evaluate the performance of both Vote and Filter-Vote on A VERITEC. In
addition, we ﬁt our analytical scaling model with 2, 5, 10, 20, 50, 100 LM calls. Then we use it to
predict the performance of Vote and Filter-Vote using 100 randomly drawn number of LM calls.
7(c) Scaling model prediction (a) Overall performance (b) Performance breakdown
36% 64%
38% 62%
(A) 
34% (D) 
56%
(C) 
10%
(D) 
29%(B) 
67%(C) 
4%Claim: Forty percent of Iowa’ s energy
resources are from renewables  
Question: The above claim is?  
(A) Refuse (B) Support (C) Conﬂicting
Evidence (D) Not Enough Evidence
Claim: All USA Ballots Arriving After
Election Day Will Be Thrown Out  
Question: The above claim is?  
(A) Refuse (B) Support (C) Conﬂicting
Evidence (D) Not Enough EvidenceAn easy query
A difficult queryOne LLM call
(d) Examples of easy and hard queriesMore LLM calls
More LLM calls One LLM call
Figure 4: A case study on the A VERITEC dataset. (a) As more LM calls are invoked, the overall
performance of Vote and Filter-Vote both initially increases but then decreases. (b) This U-shape
can be perfected explained by the opposite effects on easy and difﬁcult queries: More LM calls lead
to higher performance on easy queries, but lower performance on difﬁcult ones. (c) Our analytical
scaling model accurately predicts the empirical performance. (d) Examples of an easy query and a
difﬁcult one. One LM call gives the correct answer with probability higher than any other answers
(67%), and thus Vote with more calls eventually gives the correct answer. For the difﬁcult query, the
probability of the correct answer (34%) is lower than that of an incorrect answer (56%). Thus, Vote
with more LM calls eventually always generates a wrong answer.
As shown in Figure 4, there are several intriguing behaviors. First, more LM calls do not always lead
to better performance. In fact, the performance of both Vote and Filter-Vote increases ﬁrst but then
decreases as the number of LM calls increases from 2 to 1000 (Figure 4(a)). The performance break-
down (shown in Figure 4(b)) gives a natural explanation. More LM calls lead to higher performance
when a query is relatively difﬁcult ( dF(x)>0for Filter-Vote and dV(x)>0for Vote), but lower
performance when a query is relatively easy ( dF(x)<0for Filter-Vote and dV(x)<0for Vote).
We also observe a high correlation between the performance predicted by our proposed analytical
scaling model and the empirical performance, as depicted in Figure 4(c). This implies the optimal
number of LM calls can also be accurately predicted by our scaling model. Finally, we also give
examples of one easy query and one difﬁcult query in Figures 4(d). On the easy example, one LM
call gives the correct answer with a probability higher than any other answer (67%). Thus, more LM
calls eventually give the correct answer. On the difﬁcult query, the probability of the correct answer
(34%) is lower than that of an incorrect answer (56%). Thus, Vote with more LM calls eventually
always generates a wrong answer.
Scaling model performance on real-world datasets. Next we study how our analytical scaling
model generalizes to other real-world datasets. In particular, we ﬁt our analytical model with 2, 5, 10,
50, and 100 LM calls, and then use the scaling model to predict the performance on 100 randomly
drawn number of LM calls.
Figure 5shows the correlation between the predicted and empirical performance on three real-world
datasets, namely, MMLU PHYSICS, TRUTHFULQA, and GPQA. We ﬁrst note that the best per-
formance of the Filter-Vote system is not necessarily better than that of the Vote system. Indeed,
8(c) TRUTHFULQA (a) MMLU PYHSICS (b) GPQA
Figure 5: How the proposed analytical scaling model performs on other real-world datasets, namely,
(a) MMLU PHYSICS, (b) TRUTHFULQA, and (c) GPQA. Here, we ﬁt the scaling model with 2,
5, 10, 50, and 100 LM calls, and then use the scaling model to predict the performance on 100
randomly drawn number of LM calls. Overall, we observe that our analytical model can predict the
performance of the two compound systems accurately. Interestingly, the prediction accuracy on the
Vote system is relatively higher than that on the Filter-Vote system. This is expected as the Filter-
Vote system generalizes the Vote system and thus its scaling behavior can be more complex.
on TRUTHFULQA, the Vote system’s best performance is higher than that of the Filter-Vote sys-
tem. This further highlights the importance of performance prediction, even if there is no budget
constraint. Second, we observe that our scaling model can predict the performance of both Vote
and Filter-Vote systems accurately across all these real-world datasets. This is because our scaling
model carefully takes into account both difﬁcult and easy queries and thus reﬂects the non-monotone
behavior. Interestingly, the prediction accuracy on the Vote system is higher than that on the Filter-
Vote system. This is perhaps because the Filter-Vote system generalizes the Vote system and thus its
scaling behavior can be more complex.
Difﬁculty distribution determines whether more LM calls help. To systematically understand
how the query difﬁculty affects Inference Systems’ performance landscape, we synthesize a bi-level
difﬁcult dataset Dα,p1,p2, vary (i) the fraction of the easy subset αand (ii) the query difﬁculty p1, p2,
and then study the scaling performance of Vote. When it is clear from the context, we may also call
(p1, p2)query difﬁculty.
As shown in Figure 6, we observe that query difﬁculty plays an important role in the number of
LM calls’ effects. For example, when the difﬁculty parameter is (0.85,0.1)and the fraction of easy
queries is α= 0.6, Vote’s performance is monotonically increasing as the number of LM calls
grows. However, adding more hard queries by changing the fraction α= 0.4changes the trend: the
performance goes down ﬁrst for small call numbers and then goes up for larger numbers of calls. It
is also interesting to notice an inverse “U”-shape. For example, when query difﬁculty is (0.85,0.4),
there is a clear U-shape performance. Overall, this justiﬁes that (i) there are cases where more LM
calls is not beneﬁcial, and (ii) the diversity of query difﬁculty critically determines these cases.
Analytical scaling model predicts the optimal number of LM calls. Identifying the optimal
number of calls is an important implication of the scaling properties. Here, we compare the optimal
number of calls for Vote predicted by our analytical model and the optimal numbers empirically
observed on the bi-level difﬁcult dataset Dα,p1,p2. As summarized in Table 2(in the Appendix),
the predicted optimal number is exactly the observed optimal number of LM calls, for all query
difﬁculties evaluated. This validates the assumptions made by Theorem 4.
6 Conclusion
In this paper, we systematically study how the number of LM calls affects the performance of two
natural inference strategy designs: majority voting (Vote) and majority voting after ﬁltering results
with an LM (Filter-Vote). We ﬁnd that increasing the number of LM calls can lead to non-monotone
behavior, e.g., ﬁrst increasing performance and then decreasing it. We offer theoretical analysis
910203040500.50.60.7
10203040500.40.50.6
10203040500.40.450.50.550.6
10203040500.50.6
10203040500.40.450.50.550.6
10203040500.40.50.6
10203040500.450.50.550.6
10203040500.40.450.50.550.6
10203040500.30.40.50.6
𝛼 (easy fraction) =0.4 𝛼 (easy fraction) =0.5 𝛼 (easy fraction) =0.6Number of LLM Calls Number of LLM Calls Number of LLM Calls
Number of LLM Calls Number of LLM Calls Number of LLM Calls
Number of LLM Calls Number of LLM Calls Number of LLM CallsPerformance
Performance
PerformancePerformance
Performance
PerformancePerformance
Performance
PerformanceItem Difficulty=(0.85,0.4) Item Difficulty=(0.85,0.3) Item Difficulty=(0.85,0.1)
Item Difficulty=(0.75,0.4) Item Difficulty=(0.75,0.3) Item Difficulty=(0.75,0.1)
Item Difficulty=(0.65,0.4) Item Difficulty=(0.65,0.3) Item Difficulty=(0.65,0.1)Figure 6: Overall performance of one-layer Voting Inference Systems on synthetic datasets with
bi-level difﬁculty. Overall, we observe that increasing the number of LM calls does not necessar-
ily lead to performance improvements. For example, when the query difﬁculty is (0.85,0.4), the
performance increases ﬁrst and then decreases as the number of LM calls grows. When the query
difﬁculty is (0.65,0.1), there is a reverse trend: the performance goes down ﬁrst and then goes up.
This validates our empirical observation that a larger number of LM calls and thus more resources
may not necessarily result in better performance.
that attributes this phenomenon to the diversity of query difﬁculties within a task, and conduct
experiments to validate our analysis. Furthermore, we show how to estimate the optimal number
of LM calls to make for a given task using a small number of queries to ﬁt the parameters of our
analytical model, thus helping practitioners optimize their system designs. Overall, our study shows
that more LM calls are not necessarily better and underscores the importance of compound system
design. To stimulate further research, we have released our code and datasets, available at https:
//github.com/lchen001/CompoundAIScalingLaws . We hope our ﬁndings and analysis will
inspire more research into maximizing the effectiveness of inference time compute.
Acknowledgement
This work was supported in part by a Google PhD Fellowship, a Sloan Fellowship, NSF
CCF 1763191, NSF CAREER AWARD 1651570 and 1942926, NIH P30AG059307, NIH
U01MH098953, grants from the Chan-Zuckerberg Initiative, Sutherland, and afﬁliate members and
other supporters of the Stanford DAWN project and UC Berkeley SKY Lab, including Google, IBM,
Intel, Lacework, Meta, Microsoft, Mohamed Bin Zayed University of Artiﬁcial Intelligence, Nexla,
Samsung SDS, Uber, and VMware. BH is supported by a 2024 Sloan Fellowship in Mathematics,
NSF CAREER grant DMS-2143754, and NSF grants DMS-1855684 and DMS-2133806. We also
thank anonymous reviewers for helpful discussion and feedback.
10References
[Alp23] AlphaCode. Alphacode 2 technical report. https: // storage. googleapis. com/
deepmind-media/ AlphaCode2/ AlphaCode2_ Tech_ Report. pdf , 2023.
[BDK+21]Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Ex-
plaining neural scaling laws. arXiv preprint arXiv:2102.06701 , 2021.
[CKB+21]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Train-
ing veriﬁers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
[CWW+24]Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao
Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation
of large language models. ACM Transactions on Intelligent Systems and Technology ,
15(3):1–45, 2024.
[CZZ23] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large lan-
guage models while reducing cost and improving performance. arXiv preprint
arXiv:2305.05176 , 2023.
[DLT+23]Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Im-
proving factuality and reasoning in language models through multiagent debate. arXiv
preprint arXiv:2305.14325 , 2023.
[HBB+20]Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song,
and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv
preprint arXiv:2009.03300 , 2020.
[HBK+21]Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,
Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the
math dataset. arXiv preprint arXiv:2103.03874 , 2021.
[IPH+24]Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassil-
vitskii, and Sanmi Koyejo. Scaling laws for downstream task performance of large
language models. arXiv preprint arXiv:2402.04177 , 2024.
[JYW+24]Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Oﬁr Press,
and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world
github issues? In The Twelfth International Conference on Learning Representations ,
2024.
[KBGA24] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo.
Gpt-4 passes the bar exam. Philosophical Transactions of the Royal Society A ,
382(2270):20230254, 2024.
[KCM+23]Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon,
Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James
Maningo, et al. Performance of chatgpt on usmle: potential for ai-assisted medical
education using large language models. PLoS digital health , 2(2):e0000198, 2023.
[KMH+20]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Re-
won Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for
neural language models. arXiv preprint arXiv:2001.08361 , 2020.
[LHE21] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models
mimic human falsehoods. arXiv preprint arXiv:2109.07958 , 2021.
[LYZ+23]Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu,
Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as
agents. arXiv preprint arXiv:2308.03688 , 2023.
[MLP+23]Ian R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller,
Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et al. Inverse
scaling: When bigger isn’t better. arXiv preprint arXiv:2306.09479 , 2023.
11[MMA+24]Nikita Mehandru, Brenda Y Miao, Eduardo Rodriguez Almaraz, Madhumita Sushil,
Atul J Butte, and Ahmed Alaa. Evaluating large language models as agents in the
clinic. NPJ digital medicine , 7(1):84, 2024.
[NKM+23]Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz.
Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375 ,
2023.
[NWD+19]Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe
Kiela. Adversarial nli: A new benchmark for natural language understanding. arXiv
preprint arXiv:1910.14599 , 2019.
[RHS+23]David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe
Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level
google-proof q&a benchmark. arXiv preprint arXiv:2311.12022 , 2023.
[SGS+22]Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos.
Beyond neural scaling laws: beating power law scaling via data pruning. Advances in
Neural Information Processing Systems , 35:19523–19536, 2022.
[SGV24] Michael Schlichtkrull, Zhijiang Guo, and Andreas Vlachos. Averitec: A dataset for
real-world claim veriﬁcation with evidence from the web. Advances in Neural Infor-
mation Processing Systems , 36, 2024.
[ŠPW23] Marija Šakota, Maxime Peyrard, and Robert West. Fly-swat or cannon? cost-effective
language model choice via meta-modeling. arXiv preprint arXiv:2308.06077 , 2023.
[SYC+20]Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler,
and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for
interactive learning. arXiv preprint arXiv:2010.03768 , 2020.
[TAB+23]Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,
Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gem-
ini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 ,
2023.
[TVCM18] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.
Fever: a large-scale dataset for fact extraction and veriﬁcation. arXiv preprint
arXiv:1803.05355 , 2018.
[TWL+24]Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad
geometry without human demonstrations. Nature , 625(7995):476–482, 2024.
[WWS+22]Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,
Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought
reasoning in language models. arXiv preprint arXiv:2203.11171 , 2022.
[XCG+23]Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large
language model based agents: A survey. arXiv preprint arXiv:2309.07864 , 2023.
[ZKAW23] Jieyu Zhang, Ranjay Krishna, Ahmed H Awadallah, and Chi Wang. Ecoassistant:
Using llm assistant more affordably and accurately. arXiv preprint arXiv:2310.03046 ,
2023.
[ZKC+24]Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller,
Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, and Ali Gh-
odsi. The shift from models to compound ai systems. https://bair.berkeley.
edu/blog/2024/02/18/compound-ai-systems/ , 2024.
12NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reﬂect the
paper’s contributions and scope?
Answer: [Yes]
Justiﬁcation: We claim an interesting scaling behavior of simple compound AI systems
and propose a possible explanation by query difﬁcult diversity, which are both justiﬁed by
theoretical analysis and empirical results.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reﬂect how
much the results can be expected to generalize to other settings.
•It is ﬁne to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justiﬁcation: Please see Section Bin the appendix.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
•The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-speciﬁcation, asymptotic approximations only holding locally). The au-
thors should reﬂect on how these assumptions might be violated in practice and what
the implications would be.
•The authors should reﬂect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reﬂect on the factors that inﬂuence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
•The authors should discuss the computational efﬁciency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be speciﬁcally instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
13Answer: [Yes]
Justiﬁcation: All proofs can be found in Section Cin the Appendix.
Guidelines:
•The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
•Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justiﬁcation: We provide all experiment details in Section 5and Section Din the appendix.
Guidelines:
•The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or veriﬁable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a language model), releasing of a model checkpoint, or other means that
are appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a language model), then there should either
be a way to access this model for reproducing the results or a way to reproduce the
model (e.g., with an open-source dataset or instructions for how to construct the
dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
14Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justiﬁcation: We will release the data and code once the paper is accepted.
Guidelines:
•The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not
be possible, so No is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justiﬁcation: Please see Section 5and Section Din the appendix for details.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Signiﬁcance
Question: Does the paper report error bars suitably and correctly deﬁned or other appropri-
ate information about the statistical signiﬁcance of the experiments?
Answer: [No]
Justiﬁcation: Our experiments are conducted on large-scale datasets (a few hundred to a
thousand samples).
Guidelines:
•The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, conﬁ-
dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
15•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
•The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not veriﬁed.
•For asymmetric distributions, the authors should be careful not to show in tables or
ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding ﬁgures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufﬁcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justiﬁcation: Please refer to Section 5and Section Dfor details.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justiﬁcation: We have read and followed the Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justiﬁcation: Please refer to Section Ain the Appendix.
Guidelines:
•The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
ciﬁc groups), privacy considerations, and security considerations.
16•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efﬁciency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justiﬁcation: No new dataset is released or collected, and thus safeguards are not necessary.
Guidelines:
•The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety ﬁlters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justiﬁcation: We have cited all datasets used in this paper and also explicitly included their
licenses in Section Din the appendix.
Guidelines:
•The answer NA means that the paper does not use existing assets.
•The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
•The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
17•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [NA]
Justiﬁcation: No new data is released in this paper.
Guidelines:
•The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip ﬁle.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justiﬁcation: This paper does not use crowdsourcing or human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
•Including this information in the supplemental material is ﬁne, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
•Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
•We recognize that the procedures for this may vary signiﬁcantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
18A Broader Impacts
Many inference strategies are often computationally and ﬁnancially expensive to deploy due to mul-
tiple calls of language models. Our study suggests not blindly scaling up, but determining the
resources allocated to these inference strategies carefully. More broadly, our paper paves the way
for data-centric inference strategy designs.
B Limitations
In this paper, we focus our analysis and experiments on the scaling behaviors for two instances
of compound AI systems, but there are many other types of compound AI systems. For ease of
evaluation, our experiments are conducted on relatively objective language tasks, but it remains
open to study how the performance scales on subjective tasks such as generating poems and writing
essays. Another interesting open problem is how to predict query difﬁcult without querying the
LMs. Note that in this work, we do not discuss the cost of LM calls; this is an important dimension
to weigh in practice, and in future work we will investigate how to balance cost, performance, and
latency scaling.
C Missing Proofs
We give all missing proofs here.
C.1 Useful Lemmas
Lemma 5. Suppose Dis2-level difﬁcult with (α, p1, p2)and∥A∥= 2, and W.L.O.G, Kis odd.
Then F(K;D) = αIp1(K+1
2,K+1
2) + (1 −α)Ip2(K+1
2,K+1
2), where Ix(a, b)≜∫x
0ta−1(1−
t)b−1dt/∫1
0ta−1(1−t)b−1dtis the regularized incomplete beta function.
Proof. To analyze how the performance scales as the network size increases, let us ﬁrst expand the
performance of Inference System by the law of total expectation
F(K;D) = /x45[ˆy=y] =2∑
i=1/x45[ˆy=y|r(x) =pi] Pr[r(x) =pi] (1)
Now consider /x45[ˆy=y|r(x) =p]for any p. Note that there are in total 2 possible generations (by
∥A∥= 2), so the estimated generation ˆyis correct if and only if more than half of the generations
is correct. That is to say, /x45[ˆy=y|r(x) =p] = Pr[∑K
k=1/x31zk=y>K
2] = 1 −Pr[∑K
k=1/x31zk=y≤
K
2]. For ease of notation, denote Z≜∑K
k=1/x31zk=y. Note that /x31zk=yis a Bernoulli variable
with parameter pand thus Zis a Binomial variable with parameter (K, p). Therefore, we have
Pr[Z≤w] =I1−p(K−w, w + 1), where Ix(a, b)≜∫x
0ta−1(1−t)b−1dtis the incomplete beta
function. Thus, the above gives us
/x45[ˆy=y|r(x) =p] = 1 −I1−p(K− ⌊K
2⌋,⌊K
2⌋+ 1) = Ip(⌊K
2⌋+ 1, K− ⌊K
2⌋)
IfKis odd, we can write it as /x45[ˆy=y|r(x) =p] =Ip(K+1
2,K+1
2)We can now plug this back
into equation ( 1) to obtain
F(K;D) =2∑
i=1/x45[ˆy=y|r(x) =pi] Pr[r(x) =pi] =2∑
i=1Ipi(K+ 1
2,K+ 1
2) Pr[r(x) =pi]
=αIp1(K+ 1
2,K+ 1
2) + (1 −α)Ip2(K+ 1
2,K+ 1
2)
which completes the proof.
Lemma 6. Ip(M+ 1, M+ 1) = Ip(M, M ) +pM(1−p)M
MB(M,M )(2p−1), where B(a, b)≜∫1
0ta−1(1−
t)b−1dtis the beta function.
19Proof. Noting that Ip(a+ 1, b) =Ip(a, b)−pa(1−p)b
aB(a,b), we have
Ip(M+ 1, M+ 1) = Ip(M, M + 1)−pM(1−p)M+1
MB(M, M + 1)
ByIp(a, b+ 1) = Ip(a, b) +pa(1−p)b
bB(a,b), we have
Ip(M, M + 1) = Ip(M, M ) +pM(1−p)M
MB(M, M )
Thus, Ip(M+ 1, M+ 1) =pM(1−p)M
MB(M,M )−pM(1−p)M+1
MB(M,M +1). The Pascal’s identity implies that B(a, b+
1) =a
a+bB(a, b)and thus B(M, M + 1) = B(M, M )/2. Thus, we have
Ip(M+ 1, M+ 1) =pM(1−p)M
MB(M, M )−pM(1−p)M+1
MB(M, M + 1)=pM(1−p)M
MB(M, M )−2pM(1−p)M+1
MB(M, M )
Extracting the common factors gives
pM(1−p)M
MB(M, M )−2pM(1−p)M+1
MB(M, M )=pM(1−p)M
MB(M, M )[1−2(1−p)] =pM(1−p)M
MB(M, M )(2p−1)
That is, Ip(M+ 1, M+ 1) = Ip(M, M ) +pM(1−p)M
MB(M,M )(2p−1), which completes the proof.
C.2 Proof of Lemma 1
Proof. Let us ﬁrst consider Vote.
We want to ﬁrst note that limK→∞Pr[ˆyK=y∗] = 1 , where y∗≜arg max a∈APr[G(x, θ) =a].
To see this, we can ﬁrst write Pr[ˆyK=y∗] = Pr[max a̸=y∗∑K
i=1/x31zi=a<∑K
i=1/x31zi=y∗] =
Pr[∑K
i=1/x31zi=a−∑K
i=1/x31zi=y∗<0,∀a̸=y∗]. Subtracting 1 from both sides, we have 1−Pr[ˆyK=
y∗] = Pr[ ∪a̸=y∗∑K
i=1/x31zi=a−∑K
i=1/x31zi=y∗>0]. Now by union bound, we can bound the right
hand side by∑
a̸=yPr[∑K
i=1 /x31zi=a− /x31zi=y∗>0]. Now, note that each term within the inner
summation is i.i.d. Furthermore, observe that the expectation is /x45[ /x31zi=a− /x31zi=y∗] = Pr[ G(x, θ) =
a]−Pr[G(x, θ) =y∗]<0. Therefore, by law of large numbers, we have limKPr[∑K
i=1/x31zi=a−
/x31zi=y∗>0] = 0 . Since there are only ﬁnite number of a∈A, the sum of this quantity over all a
should also be 0. Therefore, we have
lim
K→∞1−Pr[ˆyK=y∗] = lim
K→∞Pr[∪a̸=y∗K∑
i=1/x31zi=a−K∑
i=1/x31zi=y∗>0] = 0
Thus, limK→∞Pr[ˆyK=y∗] = 1 .
Now, dV(x)>0implies y=y∗and thus limK→∞F(K, D ) = 1 .dV(x)>0implies y̸=y∗and
thuslimK→∞F(K, D ) = 0 .
Now let us consider Filter-Vote.
Abusing the notation a little, we claim that again limK→∞Pr[ˆyK=y∗] = 1 , where
y∗≜arg max a∈APr[G1(x, θ) = a|Φ(x, G 1(x, θ), G2(x, θ)) = 1] , where recall that
G1(x, θ), G2(x, θ)≜G(x, θ). To see this, we can ﬁrst write Pr[ˆyK=y∗] =
Pr[max a̸=y∗∑K
i=1/x31zi=awi<∑K
i=1/x31zi=y∗wi] = Pr[∑K
i=1/x31zi=awi−∑K
i=1/x31zi=y∗wi<0,∀a̸=
y∗]. Subtracting 1 from both sides, we have 1−Pr[ˆyK=y∗] = Pr[ ∪a̸=y∗∑K
i=1 /x31zi=awi−∑K
i=1/x31zi=y∗wi>0]. Now by union bound, we can bound the right hand side by∑
a̸=yPr[∑K
i=1/x31zi=awi− /x31zi=y∗wi>0]. Now, note that each term within the inner summa-
tion is i.i.d. Furthermore, observe that the expectation is /x45[ /x31zi=awi− /x31zi=y∗wi] = Pr[ G1(x, θ) =
a,Φ(x, G 1(x, θ), G2(x, θ)) = 1] −Pr[G(x, θ) =y∗,Φ(x, G 1(x, θ), G2(x, θ)) = 1] <0. Therefore,
20by law of large numbers, we have limKPr[∑K
i=1 /x31zi=awi− /x31zi=y∗wi>0] = 0 . Since there are
only ﬁnite number of a∈A, the sum of this quantity over all ashould also be 0. Therefore, we have
lim
K→∞1−Pr[ˆyK=y∗] = lim
K→∞Pr[∪a̸=y∗K∑
i=1/x31zi=awi− /x31zi=y∗wi>0] = 0
Thus, limK→∞Pr[ˆyK=y∗] = 1 .
Now, dF(x)>0implies y=y∗and thus limK→∞F(K, D ) = 1 .dF(x)>0implies y̸=y∗and
thuslimK→∞F(K, D ) = 0 .
Hence, we have shown that dV(x)anddF(x)are difﬁculty indicator for Vote and Filter-Vote, re-
spectively.
C.3 Proof of Theorem 2
Proof. We construct a recurrent relation on F(K;D)by Applying Lemma 5
F(K+ 2;D)−F(K;D)
=αIp1(K+ 3
2,K+ 3
2) + (1 −α)Ip2(K+ 3
2,K+ 3
2)−[αIp1(K+ 1
2,K+ 1
2) + (1 −α)Ip2(K+ 1
2,K+ 1
2)]
=α(Ip1(K+ 3
2,K+ 3
2)−Ip1(K+ 1
2,K+ 1
2)) + (1 −α)(Ip2(K+ 3
2,K+ 3
2)−Ip2(K+ 1
2,K+ 1
2))
For ease of notation, let us denote M=K+1
2. Applying Lemma 6leads to
F(K+ 2;D)−F(K;D)
=α[pM
1(1−p1)M
MB(M, M )(2p1−1)] + (1 −α)[pM
2(1−p2)M
MB(M, M )(2p2−1)]
=1
MB(M, M )[α·pM
1(1−p1)M(2p1−1) + (1 −α)·pM
2(1−p2)M(2p2−1)]
Now rearranging the terms gives
F(K+ 2;D)−F(K;D)
=1
MB(M, M )·(1−α)pM
2(1−p2)M(1−2p2)·[α(2p1−1)
(1−α)(1−2p2)[p1(1−p1)
p2(1−p2)]M−1]
For ease of notation, denote ∆F(M)≜α(2p1−1)
(1−α)(1−2p2)p1(1−p1)
p2(1−p2)]M−1. Note that ∆F(M)is mono-
tonically increasing or decreasing, depending on the parameters p1, p2. It is also easy to show that
α≥1−1
tif and only if ∆F(1)≥0. Now, we are ready to derive the main results by analyzing
∆F(M).
•p1+p2>1andα≥1−1
t: Nowp1(1−p1)
p2(1−p2)>1, and thus ∆F(M)is monotonically in-
creasing. Furthermore, it is easy to see limM→∞∆F(M) =∞. Furthermore, ∆F(1)≥0.
That is, for any given M,∆F(M)is non-negative and thus F(K;D)must be monotoni-
cally increasing
•p1+p2>1andα < 1−1
t: Nowp1(1−p1)
p2(1−p2)>1, and thus ∆F(M)is monotonically
increasing. Furthermore, it is easy to see limM→∞∆F(M) =∞. Furthermore, ∆F(1)<
0. That is, as Kand thus Mincreases, ∆F(M)is negative and then becomes positive.
Therefore, F(K;D)must decrease and then increase
•p1+p2<1andα > 1−1
t: Nowp1(1−p1)
p2(1−p2)<1, and thus ∆F(M)is monotonically decreas-
ing. Furthermore, it is easy to see limM→∞∆F(M) =−1. Furthermore, ∆F(1)>0.
That is, as Kand thus Mincreases, ∆F(M)is positive and then becomes negative. There-
fore,F(K;D)must increase and then decrease
21•p1+p2<1andα≤1−1
t: Nowp1(1−p1)
p2(1−p2)<1, and thus ∆F(M)is monotonically decreas-
ing. Furthermore, it is easy to see limM→∞∆F(M) =−1. Furthermore, ∆F(1)≤0.
That is, for any given M,∆F(M)is non-positive and thus F(K;D)must be monotoni-
cally decreasing
Thus, we complete the proof.
C.4 Proof of Theorem 3
Proof. Let us ﬁrst consider F(K, x) = Pr[ˆ yK=y]. Note that there are in total 2 possible genera-
tions (by ∥A∥= 2), so the estimated generation ˆyis correct if and only if more than half of the gener-
ations is correct. That is to say, /x45[ˆyK=y] = Pr[∑K
k=1/x31zk=y>K
2] = 1 −Pr[∑K
k=1/x31zk=y≤K
2].
For ease of notation, denote Z≜∑K
k=1/x31zk=y. Note that /x31zk=yis a Bernoulli variable with param-
eterpand thus Zis a Binomial variable with parameter (K, p). Therefore, we have Pr[Z≤w] =
I1−p(K−w, w + 1), where Ix(a, b)≜∫x
0ta−1(1−t)b−1dt/∫1
0ta−1(1−t)b−1dtis the incomplete
beta function. Speciﬁcally, we can set w=K
2and get F(K, x) = Pr[ˆ yK=y] =I1−p(K+1
2,K+1
2).
Noting that by deﬁnition, dV(x) = 1 −2p, we can rewrite this as F(K, x) =I1−dV(x)
2(K+1
2,K+1
2).
Taking the integral over xcompletes the proof.
C.5 Proof of Theorem 4
Proof. Note that the optimal number of LM calls must ensure that the incremental component
∆F(M) = 0 . That is, the optimal value M∗must satisfy ∆F(M∗) =α(2p1−1)
(1−α)(1−2p2)[p1(1−p1)
p2(1−p2)]M∗−
1 = 0 .Solving the above equation gives us a unique solution
M∗= 2logα
1−α2p1−1
1−2p2
logp2(1−p2)
p1(1−p1)
Noting that K∗=⌈2M⌉]completes the proof.
D Additional Experiment Details
D.1 Datasets
We evaluate Vote and Filter-Vote on four real-world datasets, namely, GPQA [ RHS+23],
A VERITEC [ SGV24 ], TRUTHFULQA [ LHE21 ], and MMLU PHYSICS [ HBB+20]. A VERITEC
is a fact veriﬁcation dataset, where each query is a claim (e.g., “All USA ballots Arriving after elec-
tion day will be thrown out”), and the goal is to determine if this claim should be refused, supported,
or there is conﬂicting evidence or not enough conﬁdence. We use the “development” partition
offered in the original paper, which contains 500 queries. GPQA is a dataset consisting of multiple-
choice questions written by domain experts in biology, physics, and chemistry. We use the diamond
partition, which contains 198 queries, and as reported by the original paper, enjoys the highest ex-
pert accuracy. TRUTHFULQA is another widely used multiple-choice question-answering dataset.
We uses the validation parititon, which contains 817 questions spanning 38 categories, including
health, law, ﬁnance, and politics. MMLU PHYSICS is the high school physics subset of the MMLU
dataset, which contains 151 questions. All queries are prompted as multiple-choice questions for
ease of evaluation.
Liscense. GPQA is released under the MIT License. A VERITEC uses Creative Commons
Attribution-NonCommercial 4.0 International License. TRUSTFULQA and PHYSICS (as part of
MMLU) both adopt the Apache License 2.0
D.2 Prompting for generators and ﬁlters
We provide the prompts used for the generators and ﬁlters in the following boxes.
22Table 2: Optimal number of LM calls prediction. For each data distribution with speciﬁc 2-level
difﬁculty parameters α, p1, p2, we sample 100 data points, employ a simulated Voting Inference
System with 1000 number of LM calls, estimate the parameters by their empirical mean and adopt
the analytical optimal number of LM calls predictor. Across evaluated query difﬁculties, our pre-
dicted optimal number of LM calls exactly matches the optimal number empirically observed.
α p1 p2 ˆα ˆp1 ˆp2Optimal Number of LM Calls
Analytical/Empirical
0.4 0.85 0.4 0.42 0.84 0.40 3
0.4 0.85 0.3 0.38 0.85 0.30 1
0.4 0.75 0.4 0.41 0.74 0.40 4
0.4 0.75 0.3 0.44 0.75 0.30 1
0.4 0.65 0.4 0.41 0.65 0.40 1
0.5 0.85 0.4 0.50 0.84 0.40 4
0.5 0.85 0.3 0.50 0.85 0.30 2
0.5 0.75 0.4 0.51 0.75 0.40 7
0.5 0.75 0.3 0.49 0.75 0.30 4
0.5 0.65 0.4 0.48 0.65 0.40 15
0.6 0.85 0.4 0.59 0.84 0.39 5
0.6 0.85 0.3 0.63 0.85 0.30 4
0.6 0.75 0.4 0.61 0.75 0.40 11
0.6 0.75 0.3 0.59 0.75 0.30 11
0.6 0.65 0.4 0.62 0.65 0.40 30
Prompt for the generator G()
Please answer the following question. You should ﬁrst analyze it step by step. Then generate
your ﬁnal answer by the answer is (X).
Q: {Query}
A:
Prompt for the ﬁlter Φ()
[User Question]: {query}
[Answer]:{answer}
Instruction: Review your previous answer and ﬁnd problems with your answer. Finally,
conclude with either [[correct]] if the above answer is correct or [[wrong]] if it is incorrect.
Think step by step.
Verdict:
D.3 Generation Setup
We set up the temperature as 0.1 since all the evaluation tasks are objective and have clear correct
answers. For each question, we query GPT-3.5-turbo-0125 400 times. Then for each F(K, D ), we
randomly sample Kanswers from the 400 answers with replace to simulate the performance once,
and report the average over 1000 runs.
D.4 The optimal number of LM calls predicted by the scaling model
Table 2compares the optimal number of LM calls predicted by the scaling model and the ground-
truth value on the bi-level dataset Dα,p1,p2. Overall, We observe that they match very well.
230.65 0.75 0.85
p10.1 0.3 0.4p21.86e-04 2.38e-05 1.62e-06
9.41e-05 8.36e-05 1.04e-04
3.87e-04 5.51e-04 5.65e-04
 0.00010.00020.00030.00040.0005(a)α= 0.4
0.65 0.75 0.85
p10.1 0.3 0.4p22.32e-04 2.99e-05 2.34e-06
1.53e-04 6.11e-05 8.63e-05
2.52e-04 4.54e-04 4.71e-04
0.00010.00020.00030.0004 (b)α= 0.5
0.65 0.75 0.85
p10.1 0.3 0.4p22.79e-04 3.60e-05 3.09e-06
2.14e-04 3.95e-05 6.85e-05
1.25e-04 3.57e-04 3.76e-04
0.000050.000100.000150.000200.000250.000300.00035 (c)α= 0.6
Figure 7: Mean square error of the performance predicted by our proposed scaling law on synthe-
sized datasets with varying bi-level difﬁculties. Here, we ﬁt the scaling law by performance evalu-
ated at K= 1,2,3,4,5, and evaluate its performance for Kfrom 1 to 100. Overall, we observe that
the predicted performance accurately matches the empirical evaluation.
D.5 Performance Estimation via Our Proposed Scaling Law
Can our proposed scaling model predict the empirical performance of Vote accurately? To answer
this, we apply it to each dataset considered in Figure 6. In particular, we feed our estimator with
the Inference Systems’ performance with the number of LM calls being 1,2,3,4,5, and then use
it to predict the performance for LM calls within the range from 1 to 100. Note that this is quite
challenging, as the estimator needs to extrapolate, i.e., predict the performance of a Voting Inference
System whose number of LM calls is much larger than any number seen in the training data.
As shown in Figure 7, the performance predicted by our estimator accurately matches the empirical
observation. Across all query difﬁculty parameters, the mean square error ranges from 1e−6to
1e−4. This suggests that predicting how the number of LM calls affects a Voting Inference System
is feasible and also indicates that our scaling law captures the key performance trend effectively.
24