Space-Time Continuous PDE Forecasting using
Equivariant Neural Fields
David M. Knigge∗,1, David R. Wessels∗,1, Riccardo Valperga1, Samuele Papa1, Jan-Jakob Sonke2,
Efstratios Gavves†,1, Erik J. Bekkers†,1
1University of Amsterdam2Netherlands Cancer Institute
d.m.knigge@uva.nl ,d.r.wessels@uva.nl
Abstract
Recently, Conditional Neural Fields (NeFs) have emerged as a powerful modelling
paradigm for PDEs, by learning solutions as flows in the latent space of the
Conditional NeF. Although benefiting from favourable properties of NeFs such
as grid-agnosticity and space-time-continuous dynamics modelling, this approach
limits the ability to impose known constraints of the PDE on the solutions – e.g.
symmetries or boundary conditions – in favour of modelling flexibility. Instead, we
propose a space-time continuous NeF-based solving framework that - by preserving
geometric information in the latent space - respects known symmetries of the PDE.
We show that modelling solutions as flows of pointclouds over the group of interest
Gimproves generalization and data-efficiency. We validated that our framework
readily generalizes to unseen spatial and temporal locations, as well as geometric
transformations of the initial conditions - where other NeF-based PDE forecasting
methods fail - and improve over baselines in a number of challenging geometries.
1 Introduction
Partial Differential Equations (PDEs) are a foundational tool in modelling and understanding spatio-
temporal dynamics across diverse scientific domains. Classically, PDEs are solved using numerical
methods such as finite elements, finite volumes, or spectral methods. In recent years, Deep Learning
(DL) methods have emerged as promising alternatives due to abundance of observed and simulated
data as well as the accessibility to computational resources, with applications ranging from fluid
simulations and weather modelling [51, 7] to biology [33].
* shared first author, †shared lead advising
38th Conference on Neural Information Processing Systems (NeurIPS 2024).
Figure 1: We propose to solve an equivariant PDE in function space by solving an equivariant ODE in
latent space. Through our proposed framework, which leverages Equivariant Neural Fields fθ, a field
νtis represented by a set of latents zν
t={(pν
i,cν
i)}N
i=1consisting of a posepiand context vector ci.
Using meta-learning, the initial latent zν
0is fit in only 3 SGD steps, after which an equivariant neural
ODE Fψmodels the solution as a latent flow.The systems modelled by PDEs often have underlying symmetries. For example, heat diffusion
or fluid dynamics can be modeled with differential operators which are rotation equivariant, e.g.,
given a solution to the system of PDEs, its rotation is also a valid solution1. In such scenarios it is
sensible, and even desirable, to design neural networks that incorporate and preserve such symmetries
to improve generalization and data-efficiency [12, 48, 4].
Crucially, DL-based approaches often rely on data sampled on a regular grid, without the inherent
ability to generalize outside of it, which is restrictive in many scenarios [ 40]. To this end, [ 51] propose
to use Neural Fields (NeFs) for modelling and forecasting PDE dynamics. This is done by fitting a
neural ODE [ 11] to the conditioning variables of a conditional Neural Field trained to reconstruct
states of the PDE [13]. However, this approach fails to leverage aforementioned known symmetries
of the system. Furthermore, using neural fields as representations has proved difficult due to the
non-linear nature of neural networks [ 13,3,35], limiting performance in more challenging settings.
We posit that NeF-based modelling of PDE dynamics benefits from representations that account
for the symmetries of the system as this allows for introducing inductive biases into the model that
ought to be reflected in solutions. Furthermore, we show that through meta-learning [ 28,45] the NeF
backbone improves performance for complex PDEs by further structuring the NeF’s latent space,
simplifying the task of the neural ODE.
We introduce a framework for space-time continuous equivariant PDE solving , by adapting a class
ofSE(n) -Equivariant Neural Fields (ENFs) to PDE-specific symmetries. We leverage the ENF as
representation for modelling spatiotemporal dynamics. We solve PDEs by learning a flow in the
latent space of the ENF - starting at a point z0corresponding to the initial state of the PDE - with an
equivariant graph-based neural ODE [ 11] we develop from previous work [ 5]. We extend the ENF
to equivariances beyond SE(n) , by extending its weight-sharing scheme to equivalance classes for
specific symmetries relevant to our setting. Furthermore, we show how meta-learning [ 14,28,45,13],
can not only significantly reduce inference time of the proposed framework, but also substantially
simplify the structure of the latent space of the ENF, thereby simplifying the learning process of the
latent dynamics for the neural ODE model. We present the following contributions:
•We introduce a framework for spatio-temporally continuous PDE solving that respects
known symmetries of the PDE through equivariance constraints.
•We show that correctly chosen equivariance constraints as inductive bias improves per-
formance of the solver - in terms of MSE - in spatio-temporally continuous settings, i.e.
evaluated offthe training grid and beyond the training horizon.
•We show how meta-learning improves the structure of the latent space of the ENF, simplify-
ing the learning process, leading to better performance in solving PDEs.
We structure the paper as follows: in Sec. 2 we provide an overview of the mathematical preliminaries
and describe the problem setting. Our proposed framework is introduced in Sec. 3. We validate
our framework on different PDEs defined over a variety of geometries in Sec. 4, with differing
equivariance constraints, showing competitive performance over other neural PDE solvers.We provide
an in-depth positioning of our approach in relation to other work in Appx. A.
2 Mathematical background and problem setting
Continuous spatiotemporal dynamics forecasting. The setting considered is data-driven learning
of the dynamics of a system described by continuous observables. In particular, we consider flows of
fields, denoted with ˆν:Rd×[0, T]→Rc. We use ˆνtas a shorthand for ˆν(·, t). We assume the flow
is governed by a PDE, and consider the Initial Value Problem (IVP) of predicting ˆνtfrom a given ν0.
The dataset consists of field snapshots ν:X × JTK→Rc, in which JTK:= 1,2, . . . , T denotes the
set of time points on which the flow is sampled and X ⊂Rdis a set of coordinate values. For each
time point we are given a set of input-output pairs [X, ν(X)]where ν(X)⊂Rcare the values of the
field at those coordinates. Importantly, the location at which the field is sampled need not be regular,
i.e., we do not require the training data to be on a grid or to be regularly spaced in time, nor need
coordinate values be identical for train and test sets. Following [ 51], we distinguish between tin-
referring to values within the training time horizon [0, T]- and tout- analogously to values beyond T.
1Assuming boundary conditions are symmetric, i.e. they transform according to the relevant group action.
2Neural Fields in dynamics modelling. Conditional Neural fields (NeFs) are a class of coordinate-
based neural networks, often trained to reconstruct discretely-sampled input continuously. More
specifically, a conditional neural field fθ:Rn→Rdis a field –parameterized by a neural network
with parameters θ– that maps input coordinates x∈Rnin the data domain alongside conditioning
latents ztod-dimensional signal values ν(x)∈Rd. By associating a conditioning latent zν∈Rc
to each signal ν, a single conditional NeF fθ:Rn×Rc→Rdcan learn to represent families Dof
continuous signals such that ∀ν∈ D:f(x)≈fθ(x;zν). [51] propose to use conditional NeFs for
PDE modelling by learning a continuous flow in the latent space of a conditional neural field. In
particular, a set of latents {zν
i}T
i=1are obtained by fitting a conditional neural field to a given set
of observations {νi}T
i=1at timesteps 1, ..., T ; simultaneously, a neural ODE [ 11]Fψis trained to
map pairs of temporally contiguous latents s.t. solutions correspond to the trajectories traced by the
learned latents. Though this approach yields impressive results for sparse and irregular data in planar
PDEs, we show it breaks down on complex geometries. We hypothesize that this is due to lack of
a latent space that preserves relevant geometric transformations that characterize the symmetries
of the systems we are modelling, and as such propose an extension of this framework where such
symmetries are preserved.
Symmetries and weight sharing. Given a group Gwith identity element e∈G, and a set X, a
group action is a map T:G×X→X. For simplicity, we denote the action of g∈Gonx∈X
asgx:=T(g, x), and call G-space a smooth manifold equipped with a Gaction. A group action
is homomorphic to Gwith its group product, namely it is such that ex=xand(gh)x=g(hx).
As an example, we are interested in the Special Euclidean group SE(n)= Rn⋊SO(n): group
elements of SE(n) are identified by a translation t∈Rnand rotations R∈SO(n)with group
operation gg′= (t,Rθ) (t′,Rθ′) = (Rx′+x,RRθ′); We denote by Lgthe left action of Gon
function spaces defined as Lgf(x′) =f(g−1x′) =f(R−1
θ(x′−x)). Many PDEs are defined by
equivariant differential operators such that for a given state ν:LgN[ν] =N[Lgν]. If the boundary
conditions do not break the symmetry, namely if the boundary is symmetric with respect to the same
group action, then a G-transformed solution to the IVP for some ν0corresponds to the solution
for the G-transformed initial value. For example, laws of physics do not depend on the choice
of coordinate system, this implies that many PDEs are defined by SE(n) -equivariant differential
operators. The geometric deep learning literature shows that models can benefit from leveraging the
inherent symmetries or invariances present in the data by constraining the searchable function space
through weight sharing [9,25,5]. Recall that in our framework we model flows of fields, solutions to
PDEs defined by equivariant differential operators, with ordinary differential equations in the latent
space of conditional neural fields. We leverage the symmetries of the system for two key aspects
of the proposed method: first by making the relation between signals and corresponding latents
equivariant; second, by using equivariant ODEs, namely ODEs defined by equivariant vector fields: if
dz
dτ=F(z)is such that F(gz) =gF(z), then solutions are mapped to solutions by the group action.
3 Method
We adapt the work of [51], and consider the following optimization problem2:
min
θ,ψ,z τEν∈D,x∈X,t∈JTK∥νt(x)−fθ(x;zν
t)∥2
2,where zν
t=zν
0+Zt
0Fψ(zν
τ)dτ , (1)
withfθ(x;zν
t)a decoder tasked with reconstructing state νtfrom latent zν
tandFψa neural ODE that
maps a latent to its temporal derivative:dzν
τ
dτ=Fψ(zν
τ), modelling the solution as flow in latent space
starting at the initial latent zν
0- see Fig. 1 for a visual intuition.
Equivariant space-time continuous dynamics forecasting. A PDE defined by a G-equivariant
differential operator - for which LgN[ν] =N[Lgν]- are such that solutions are mapped to other
solutions by the group action if the boundary conditions are symmetric. We would like to leverage
this property, and constrain the neural ODE Fψsuch that the solutions it finds in latent space can be
mapped onto each other by the group action. Our motivation for this is twofold: (1) it is natural for
2We highlight that [ 51] optimize latents zν
t, neural field fθ, and ODE Fψusing two separate objectives. We
instead found that our framework is more stable under single-objective optimization.
3our model to have, by construction, the geometric properties that the modelled system is known to
posses - (2) to get more structured latent representations and facilitate the job of the neural ODE. To
achieve this we first need the latent space Zto be equipped with a well-defined group action with
respect to which ∀g∈G, z∈Z:Fψ(gz) =gFψ(z), and, most importantly, we need the relation
between the reconstructed field and the corresponding latent to be equivariant, i.e.,
∀g∈G , x∈ X:Lgfθ(x;zν
t) =fθ(g−1x;zν
t) =fθ(x;gzν
t). (2)
Note that, somewhat imprecisely, we call this condition equivariance to convey the idea even though
it is not, strictly speaking, the commonly used definition of equivariance for general operators. If we
consider the decoder as a mapping from latents to fields, we can make the notion of equivariance of
this mapping more precise. Namely
f(x) =Dθ(z), Dθ(z) :zν
t7→fθ(·;zν
t), f(g−1x) =Dθ(gz), Dθ(gz) :g zν
t7→fθ(g−1·;zν
t).(3)
Figure 2: The proposed framework respects pre-
defined symmetries of the PDE: a rotated solution
LgνTmay be obtained either by solving from la-
tentzν
0(top-left) and transforming the solution zν
T
(top-right) to gzν
T(bottom-right) or transforming
zν
0togzν
0(bottom-left) and solving this.In Sec. 3.1 we describe the Equivariant Neu-
ral Field (ENF)-based decoder, which satisfies
equation (2). Second, in Sec. 3.2 we outline the
graph-based equivariant neural ODE. Sec. 3.3
explains the motivation for- and use of- meta-
learning for obtaining the ENF backbone param-
eters. We show how the combination of equiv-
ariance and meta-learning produce much more
structured latent representations of continuous
signals (Fig. 3).
3.1 Representing
PDE states with Equivariant Neural Fields
We briefly recap ENFs here, referring the reader
to [49] for more detail. We extend ENFs to
symmetries for PDEs over varying geometries.
ENFs as cross-attention over bi-invariant attributes. Attention-based conditional neural fields
represent a signal ν∈ D with a corresponding latent set zν[52]. This class of conditional neural fields
obtain signal-specific reconstructions ν(x)≈fθ(x;zν)through a cross-attention operation between
the latent set zνand input coordinates x. ENFs [ 49] extend this approach by imposing equivariance
constraints w.r.t a group G⊆SE(n) on the relation between the neural field and the latents such
that transformations to the signal νcorrespond to transformation of the latent zν(Eq. (2)). For this
condition to hold, we need a well-defined action on the latent space Zoffθ. To this end, ENFs define
elements of the latent set zνas tuples of posepi∈Gandcontext ci∈Rd,zν:={(pi,ci)}N
i=1.
The latent space is then equipped with a group action defined as gz={(gpi,ci)}N
i=1. To achieve
equivariance over transformations ENFs follow [ 5] where equivariance is achieved with convolutional
weight-sharing over equivalence classes of points pairs x, x′. ENFs instead extend weight-sharing to
cross-attention over bi-invariant attributes of z, xpairs.
Weight-sharing over bi-invariant attributes of z, xis motivated by Eq. 2, by which we have:
fθ(x;z) =fθ(gx;gz). (4)
Intuitively, the above equation says that a transformation gon the domain of fθ, i.e. g−1x, can
be undone by also acting with gonz. In other words, the output of the neural field fθshould be
bi-invariant tog−transformations of the pair z, x. For a specific pair (zi, xm)∈Z×X, the term
bi-invariant attribute ai,mdescribes a function a: (zi, xm)7→a(zi, xm)such that a(zi, xm) =
a(gzi, gxm). Thorughout the paper we use ai,mas shorthand for a(zi, xm).
To parameterize fθ, we can accordingly choose any function that is bi-invariant to G−transformations
ofz, x. In particular, for an input coordinate xmENFs choose to make fθa cross-attention operation
between attributes ai,mand the invariant context vectors ci:
fθ(xm, z) = cross _attn(a:,m,c:,c:) (5)
As an example, for SE(n) -equivariance, we can define the bi-invariant simply using the group action:
aSE(n)
i,m =p−1
ixm=RT
i(xm−xi), which is bi-invariant by:
∀g∈SE(n) : ( pi, x)7→(g pi, g x)⇔p−1
ix7→(g pi)−1g x=p−1
ig−1g x=p−1
ix . (6)
4Bi-invariant attributes for PDE solving. As explained above, ENF is equivariant to SE(n) -
transformations by defining fθas a function of an SE(n)−bi-invariant attribute aSE(n). Although
many physical processes adhere to roto-translational symmetries, we are also interested in solving
PDEs that - due to the geometry of the domain, their specific formulation, and/or their boundary
conditions - are not fully SE(n)−equivariant. As such, we are interested in extending ENFs to
equivariances that are not strictly (subsets of) SE(n) , which we show we can achieve by finding
bi-invariants that respect these particular transformations. Below, we provide two examples, the other
invariants we use in the experiments - including a "bi-invariant" a∅that is not actually bi-invariant to
any geometric transformations, which we use to ablate over equivariance constraints - are in Appx. D.
The flat 2-torus. When the physical domain of interest is continuous and extends indefinitely, periodic
boundary conditions are often used, i.e. the PDE is defined over a space topologically equivalent
to that of the 2-torus. Such boundary conditions break SO(2) symmetries; assuming the domain
has periodicity πand none of the terms of this PDE depend on the choice of coordinate frame,
these boundary conditions imply that the PDE is equivariant to periodic translations: the group of
translations modulo π:T2≡R2/Z2. In this case, periodic functions over x, ywith periods πwould
work as a bi-invariant, i.e. using poses p∈T2,aT2= cos(2 π(x0−p0)) + cos(2 π(x1−p1))- which
happens to be bi-invariant to rotations byπ
2as well. Instead, since we do not assume any rotational
symmetries to exist on the torus, we opt for a non-rotationally symmetric function:
aT2
i,m= cos(2 π(x0
i−p0
i))⊕cos(2 π(x1
i−p1
i)), (7)
where ⊕denotes concatenation. This bi-invariant is used in experiments on Navier-Stokes over the
flat 2-Torus.
The 2-sphere. In some settings a PDE may be symmetric only to rotations along a certain axes. An
example is that of the global shallow-water equations on the two-sphere - used to model geophysical
processes such as atmospheric flow [ 16], which are characterised by rotational symmetry only along
the earth’s axis of rotation due to inclusion of a term for Coriolis acceleration that breaks full SO(3)
equivariance. We use poses p∈SO(3) parametrised by Euler angles ϕ, θ, γ , and spherical coordinates
ϕ, θforx∈S2. We make the first two Euler angles coincide with the spherical coordinates and
define a bi-invariant for rotations around the axis θ=π.
aSW
i,m= ∆ϕpi,xm⊕θpi⊕γpi⊕θxm, (8)
where ∆ϕpi,xm=ϕpi−ϕxm−2πifϕpi−ϕxm> π and∆ϕpi,xm=ϕpi−ϕxm+2πifϕpi−ϕxm<−π,
to adjust for periodicity.
In summary, to parameterize an ENF equivariant with respect to a specific group we are simply
required to find attributes that are bi-invariant with respect to the same group. In general we achieve
this by using group-valued poses and their action on the PDE domain.
3.2 PDE solution as latent space flow
Letzν
0be a latent set that faithfully reconstructs the initial state ν0. We want to define a neural
ODE Fψthat map latents zν
tto their temporal derivativesdzν
τ
dτ=Fψ(zν
τ)that is equivariant with
respect to the group action: gFψ(zν
τ)=Fψ(gzν
τ). To this end, we use a message passing neural
network (MPNN) to learn a flow of poses piand contexts ciover time. We base our architecture
on PΘNITA [ 5], which employs convolutional weight-sharing over bi-invariants for SE(n) . For an
in-depth recap of message-passing frameworks, we refer the reader to Appx. A. Since Fψis required
to be equivariant w.r.t. the group action, any updates to the poses pishould also be equivariant. [ 41]
propose to parameterize an equivariant node position update by using a basis spanned by relative
node positions xj−xi. In our setting, poses piare points on a manifold Mequipped with a group
action. As such, we analogously propose parameterizing pose updates by a weighted combination
of logarithmic maps logpi(pj), which intuitively describe the relative position between pi, pjin the
tangent space TpiM, or the displacement from pitopj. We integrate the resulting pose update over
the manifold through the exponential map exppi. In the euclidean case logpi(pj)=xj−xiand we
get back node position updates per [ 41]. In short, the message passing layers we use consist of the
5(a)
 (b)
 (c)
Figure 3: We show the impact of meta-learning and equivariance on the latent space of the ENF when
representing trajectories of PDE states. Fig. 3a shows a T-SNE plot of the latent space of fθwhen zν
t
is optimized with autodecoding, and no weight sharing over bi-invariants is enforced. Fig. 3b shows
the latent space when meta-learning is used, but no weight sharing is enforced. Fig. 3c shows the
latent space when zν
tare obtained using meta-learning and fθshares weights over aSE(n).
following update functions:
cl+1
i=X
(pj,cj)∈zν,lkcontext(al
i,j)cl
j, pl+1
i= exppl
i1
NX
(pl
j,cl
j)∈zν,lkpose(al
i,j)cl
jlogpl
i(pl
j)
,
(9)
withkcontext, kposemessage functions weighting the incoming context and pose updates, parameterized
by a two-layer MLP as a function of the respective bi-invariant.
3.3 Obtaining the initial latent zν
0
Until now we’ve not discussed how to obtain latents corresponding to the initial condition zν
0. An
approach often used in conditional neural field literature is that of autodecoding [ 36], where latents zν
are optimized for reconstruction of the input signal νwith SGD. Optimizing a NeF for reconstruction
does not necessarily lead to good quality representations [ 35], i.e. using MSE-based autodecoding to
obtain latents zν
t- as is proposed by [ 51] - may complicate the latent space, impeding optimization
of the neural ODE Fψ. Moreover, autodecoding requires many optimization steps at inference (for
reference, [ 51] use 300-500 steps). [ 13] propose meta-learning as a way to overcome long inference
times, as it allows for fitting latents in a few steps - typically three or four. We hypothesize that
meta-learning may also structure the latent space - similar to the impact of equivariance constraints,
since the very limited number of optimization steps requires efficient organization of latents zν
t
around the (shared) initialization, forcing together the latent representation of contiguous states. To
this end, we propose to use meta-learning for obtaining the initial latent zν
0, which is then unrolled by
the neural ode Fψto find solutions zν
t.
3.4 Equivariance and meta-learning structure the latent space Z
As a first validation of the hypotheses that both equivariance constraints and meta-learning introduce
structure to the latent space of fθ, we visualize latent spaces of different variants of the ENF. We fit
ENFs to a dataset consisting of solutions to the heat equation for various initial conditions (details
in Appx. E). For each sample νt, we obtain a set of latents zν
t, which we average over the invariant
context vectors ci∈Rcto obtain a single vector in Rcinvariant to a group action according to the
chosen bi-invariant. Next, we apply T-SNE [ 47] to the resulting vectors in Rc. We use three setups:
(a) no meta-learning, θand latents zν
toptimized for every νtseparately using autodecoding [ 36],
and no equivariance imposed (per Eq. 15), shown in Fig. 3a. (b) meta-learning is used to obtain
θ,zν
t, but no equivariance imposed, shown in Fig. 3b and (c) meta-learning is used to obtain θ,zν
tand
SE(2) -equivariance is imposed by weight-sharing over aSE(n)bi-invariants, shown in Fig. 3c. The
results confirm our intuition that both meta-learning and equivariance improve latent-space structure.
Recap: optimization objective. We use a meta-learning inner-loop [ 28,13] to obtain the initial
latent zν
0under supervision of coordinate-value pairs (x, ν(x)0)x∈Xfrom ν0. This latent is unrolled
forttraintimesteps using Fψ. The obtained latents are used to reconstruct states zν
talong the trajectory
6ofν, and parameters of fθ, Fψare optimised for reconstruction MSE, as shown in the left-hand side
of Eq. 1. See Appx. B for detailed pseudocode of this process.
4 Experiments
We intend to show the impact of symmetry-preservation in continuous PDE solving. To this end we
perform a range of experiments assessing different qualities of our model on tasks with different
symmetries. First, we investigate the equivariance properties of our framework by evaluating it
against unseen geometric transformations of the initial conditions. Next, we assess generalization
andextrapolation capabilities w.r.t. unseen spatial locations and time horizons inside and outside the
time ranges seen during training respectively, robustness to partial test-time observations, and data-
efficiency . As the continuous nature of NeF-based PDE solving allows, we verify these properties for
PDEs defined over challenging geometries : the plane R2, 2-torus T2and the sphere S2and the 3D
ballB3. Architectural details and hyperparameters are in Appx. E. Code is available on GitHub .
Additionally, we validate our model on a benchmark of PDEs that exhibits no transformation
symmetries : the CFDBench [ 32] benchmark. We include details on parameter counts, memory usage
and runtimes of our model compared to baselines in Appx. F.
4.1 Datasets and evaluation
Table 1: MSE ↓for heat equation on R2.
tINTRAIN tOUT TRAIN tINTEST tOUT TEST
DINo [51] 5.92E-04 2.40E-04 3.85E-03 5.12E-03
Oursa∅6.23±1.01E-06 4.90 ±20.1E-06 2.19±0.32E-03 5.08 ±13.2E-04
OursaSE(2)1.18±0.45E-05 2.53 ±3.50E-05 1.50±0.77E-05 2.53 ±3.43E-05
Figure 4: A train and test sample from the planar
diffusion dataset. Initial conditions for train and
test are spikes in disjoint subsets of R2.All datasets are obtained by randomly sampling
disjoint sets of initial conditions for train and
test sets, and solving them using numerical meth-
ods. Dataset-specific details on generation can
be found in Appx E. •Heat equation on R2and
S2.The heat equation describes diffusion over
a surface:dc
dt=D∇2c, where cis a scalar field,
andDis the diffusivity coefficient. We solve it
on the 2D plane where ∇2c=∂2c
∂x1+∂2c
∂x2- and
on the 2-sphere S2where in spherical coordi-
nates:∇2c=
1
sinθ∂
∂θ 
sinθ∂c
∂θ
+1
sin2θ∂2c
∂ϕ2
.
Although a relatively simple PDE, we find that
defining it over a non-trivial geometry such
as the sphere proves hard for non-equivariant
methods. •Navier-Stokes on T2.We solve 2D
Navier Stokes [ 43] for an incompressible fluid
with dynamicsdv
dt=−u∇v+v∆µ+f, v=
∇ ×u,∇u= 0, where uis the velocity field, v
the vorticity, µthe viscosity and fa forcing term (see Appx. E). We create a dataset of solutions
for the vorticity using Gaussian random fields as initial conditions. Due to the incompressibility
condition, it is natural to solve this PDE with periodic boundary conditions corresponding to the
topology of a 2-Torus T2- implying equivariance to periodic translation. •Shallow-water on S2.
The global shallow-water equations model large-scale oceanic and atmospheric flow on the globe,
derived from Navier-Stokes under assumption of shallow fluid depth. The global shallow-water
equations (see Appx. E) include terms for Coriolis accelleration, which makes this problem equivari-
ant to rotation along the globe’s axis of rotation. We follow the IVP specified by [ 16], and create
a dataset of paired vorticity-fluid height solutions. •Internally-heated convection in a 3D ball.
We solve the Boussinesq equation for internally heated convection in a ball, a model relevant for
example in the context of the Earth’s mantle convection. It involves continuity equations for mass
conservation, momentum equations for fluid flow under pressure, viscous forces and buoyancy, and a
term modelling heat transfer. We generate initial conditions varying the internal temperature using
N(0,1)noise and obtain solutions for the temperature defined over a regular spherical ϕ, θ, r grid.
•CFDBench [32] consists of a set of one-step solutions (pairs of input and output states νt, νt+1)
for classic computational fluid dynamics (CFD) problems, with varying fluid properties, boundary
conditions and geometries. The goal of this dataset is to assess generalizability of DL-based PDE
solvers over problem parameters. This dataset does not exhibit transformation symmetries because of
the absolute position of obstacles in the geometry and orientation of the flow.
7Figure 5: A Navier-Stokes test sample (top) and corre-
sponding predictions from our model (bottom). We visu-
alize predictions in the train horizon tin= [0, ...,9], tout=
[10, ...,20]and beyond. The model remains stable well be-
yond the train horizon, but due to accumulated errors fails to
capture dynamics beyond t >40.
Figure 6: Test MSE tinfor increasing
training set sizes for the heat equa-
tion over the sphere. Equivariant im-
proves over non-equivariant. For ref-
erence we show performance of DINo
[51] trained on 256 trajectories.
Evaluation. All reported MSE values are for predictions obtained given only the initial condition v0,
with std over 3 runs. We evaluate two settings for train and test sets both: generalization setting with
time evolution happening within the seen horizon during training ( tin); and, extrapolation setting
with the time evolution happening outside the seen horizon during training ( tout). For both cases we
measure the mean-squared error (MSE). To position our work relative to competitive data-driven
PDE solvers, on the 2D-Navier-Stokes experiment we provide comparisons with a range of baselines.
In most other settings these models cannot straightforwardly be applied, and we only compare to
[51], to our knowledge the only other fully continuous PDE solving method in literature. For the
Navier-Stokes and Internally-Heated Convection experiments, we compare with Transolver [ 50],
which has shown SOTA results as DL-based PDE solving method for general geometries.
Equivariance properties - heat equation on the plane. To verify our framework respects the
posed equivariance constraints, we create a dataset of solutions to the heat equation that requires
a neural solver to respect equivariance constraints to achieve good performance. Specifically,
for initial conditions we randomly insert a pulse of variable intensity in x= (x1, x2)∈R2s.t.
−1<x1<1,0<x2<1for the training data and −1<x1<1,−1<x2<0for the test data. Intuitively,
Table 2: MSE ↓for Navier-Stokes on T2.
tINTRAIN tOUT TRAIN tINTEST tOUT TEST
100% OFν0OBSERVED
CNODE [2] 6.02E-02 3.35E-01 5.48E-02 3.17E-01
FNO 9.43E-05 2.11E-03 8.44E-05 1.60E-03
G-FNO 3.13E-05 3.49E-04 3.15E-05 3.52E-04
Transolver [50] 1.80E-02 4.85E-01 1.85E-02 4.90E-01
DINo [51] 8.20E-03 6.85E-02 1.11E-02 9.08E-02
Ours AD,aT2/π5.60±0.43E-02 0.37 ±0.34E-01 6.75 ±0.62E-02 4.00 ±0.38E-01
Oursa∅1.41±1.83E-02 1.67 ±1.27E-01 2.60 ±3.16E-02 2.14 ±1.46E-01
OursaT2/π1.45±0.08E-03 9.14 ±0.36E-03 1.57 ±0.09E-03 1.16 ±0.14E-02
50% OFν0OBSERVED
CNODE [2] 1.38E-01 6.33E-01 1.52E-01 6.76E-01
FNO 3.31E-02 1.39E-01 3.20E-02 1.47E-01
G-FNO 2.75E-02 1.17E-01 2.32E-02 1.01E-01
Transolver [50] 4.69E-01 0.99E-01 4.76E-01 0.99E-01
DINo [51] 3.67E-02 2.81E-01 3.74E-02 2.83E-01
Ours AD,aT2/π6.89±2.68E-02 3.95 ±2.18E-01 7.01 ±3.56E-02 4.01 ±2.29E-01
Oursa∅1.05±0.04E-02 1.45 ±0.01E-01 2.60 ±3.16E-02 2.14 ±1.46E-01
OursaT2/π1.50±0.17E-03 8.97 ±1.57E-03 5.75 ±2.58E-03 5.03 ±2.63E-02
5% OFν0OBSERVED
CNODE [2] 1.23E+00 2.14E+00 1.20E+00 4.35E+00
FNO 4.13E-01 7.70E-01 3.84E-01 7.07E-01
G-FNO 3.56E-01 7.09E-01 3.40E-01 6.47E-01
Transolver [50] 8.48E-01 1.23E+00 8.28E-01 1.24E+00
DINo [51] 3.67E-02 2.81E-01 3.94E-02 2.91E-01
Ours AD,aT2/π6.89±2.68E-02 3.95 ±2.18E-01 7.01 ±3.56E-02 4.01 ±2.29E-01
Oursa∅7.31±1.37E-02 2.97 ±2.42E-01 7.96 ±1.65E-02 3.35 ±3.41E-01
OursaT2/π3.19±1.07E-02 1.33 ±0.35E-01 3.44 ±1.43E-02 1.61 ±4.93E-01
Table 3: Zero-shot temporal superresolution on
Navier-Stokes.
tINTEST tOUT TEST
dτ=1.0(train) dτ=0.5dτ=0.25 dτ=1.0(train) dτ=0.5dτ=0.25
DINo [51] 1.19E-02 3.85E-02 3.98E-02 8.82E-02 2.19E-01 2.22E-01
Ours 1.76E-03 1.86E-03 1.91E-03 1.42E-02 1.49E-02 1.52E-02train and test sets contain spikes under different
disjoint sets of roto-translations (see Fig. 4). We
train variants of our framework with ( aSE(2), Eq.
6) and without ( a∅, Eq. 15) equivariance con-
straints. In this dataset, we set tin= [0, ...,9],
and evaluation horizon tout= [10 , ...,20]. Re-
sults in Tab. 1 show that the non-equivariant
model, as well as the baseline [ 51] are un-
able to successfully solve test initial conditions,
whereas the equivariant model performs well.
Robustness to subsampling & time-horizons
- Navier-Stokes on the 2-Torus. We perform
an experiment assessing the impact of equivari-
ance constraints and meta-learning on robust-
ness to sparse test-time observations of the ini-
tial condition. To this end, we train a model
with (aT2, Eq. 7), without ( a∅, Eq. 15) equivari-
ance constraints, and one with equivariance con-
straints and without meta-learning (AD aT2, Eq.
7), on a fully-observed train set. The training
horizon tin= [0, ...,9], and evaluation horizon
tout= [10 , ...,20]. Subsequently, we apply the
trained model to the problem of solving from
sparse initial conditions v0, with observation
rates where 50% and5%of the initial condition is observed (Tab. 2). Approaches operating on
discrete (CNODE [ 2]) and regular grids (FNO [ 29], G-FNO [ 20]) perform very well when evaluated
on fully-observed regular grids, outperforming continuous approaches (ours, [ 51]). However, we note
8that all discrete/regular models greatly deteriorate in performance when observation rates decrease.
Equivariance constraints and meta-learning clearly improve performance overall, achieving best
perfomance in all sparse settings. Our proposed framework performs competitively to discrete base-
lines and other NeF based PDE solving methods [ 51] in the fully observed setting. To qualitatively
assess long-term stability well-beyond the train horizon, we visualizate test trajectory and the solution
found by our model for tin= [0, ...,9], tout= [10 , ...,20]and beyond in Fig. 5. We show error
accumulation for solutions on 100% observed and 50% observed initial states in Appx. F. To validate
the time-continuous nature of our model, we train a model with supervision at a resolution of dτ=1.0,
and subsequently evaluate on resolutions dτ=0.5anddτ=0.25(Tab. 3), showing that our framework
does not accumulate significant error when deploying on higher temporal resolution.
Data-efficiency - Diffusion on the sphere. To assess the impact of equivariance on data efficiency,
we vary the size of the training set of heat equation solutions from 16 to 64 trajectories and apply a
model with ( aSO(3), Eq. 13) and without ( a∅, Eq. 15) equivariance constraints. In this dataset, we set
tin= [0, ...,9], and evaluation horizon tout= [10 , ...,20]. We visualize tintest- and train MSE in Fig.
6. These results show the non-equivariant model overfitting the training set for smaller numbers of
trajectories while unable to solve the PDE satisfactorily, whereas the equivariant model generalizes
well even with only 16 training trajectories.
Table 4: MSE ↓on Shallow-Water equations on
the sphere.
tINTRAIN tOUT TRAIN tINTEST tOUT TEST
TRAIN RESOLUTION
DINo [51] 1.75E-04 1.36E-03 2.01E-04 1.37E-03
OursaSW9.94±0.41E-05 1.89±0.03E-03 1.09±1.14E-04 1.87±0.04E-03
ZERO -SHOT 2X SUPER -RESOLUTION
DINo [51] 3.03E-04 2.03E-03 3.37E-04 2.03E-03
OursaSW1.58±0.02E-04 1.96 ±0.02E-03 1.61 ±0.01E-04 1.93 ±0.02E-03
Figure 7: Test samples at train resolution (top),
2×train resolution (middle) and corresponding
predictions from our equivariant model ( aSWEq.
8 (bottom). The model does not produce significant
upsampling artefacts, but fails to capture dynamics
outside the training horizon.
Table 5: MSE ↓on Internally-Heated Convection
in the ball.
tINTRAIN tOUT TRAIN tINTEST tOUT TEST
100% OFν0OBSERVED
Transolver [50] 3.89E-04 1.88E-02 4.13E-04 2.09E-02
DINo [51] 2.94E-03 7.56E-02 3.06E-03 7.78E-02
OursaB35.79±0.17E-04 7.72±0.55E-03 5.99±0.15E-04 7.97±0.46E-03
50% OFν0OBSERVED
Transolver [50] 4.39E-01 4.99E-01 4.38E-01 5.00E-01
DINo [51] 3.01E-03 1.06E-01 3.02E-03 1.13E-01
OursaB36.27E-04 7.76E-03 6.63E-04 8.21E-03Super-resolution - Shallow-Water on the
sphere. Due to their continuous nature, NeF-
based approaches inherently support zero-shot
super-resolution. In this setting, we generate
a set of solutions for the global shallow-water
equations over S2at2×resolution, and apply
mean-pooling with a kernel size of 2 to obtain
a low-resolution dataset. We train a model that
respects rotational symmetries along the rota-
tion axis of the globe ( aSW, Eq. 8) at train
resolution, and evaluate the model by solving
initial conditions at 2×resolution (Tab. 4, Fig.
7). In this dataset, we set tin= [0, ...,9], and
evaluation horizon tout= [10 , ...,14]. First, we
note that our model has difficulty capturing the
dynamics near tout- and beyond the training
horizon, i.e. t=>9- we suspect because of
accumulation of reconstruction errors impacting
the ability of Fψto model the relatively volatile
dynamics of these equations. This points to a
drawback of NeF-based solvers: error accumu-
lation starts with the reconstruction error on the
initial condition. Ranging over our experiments,
we found that this error can be reduced by in-
creasing model capacity, at steep cost of com-
putational complexity attributable to the global
attention operator in the ENF backbone. Regard-
ing super-resolution; the model is able to solve
the high-resolution initial conditions without in-
ducing significantly increased MSE - it does not
produce significant artefacts in the process.
Challenging geometries - Internally heated
convection in 3D ball. We show the value of
inductive biases in modelling over a challenging
geometry. We apply an equivariant model ( aB3, Eq. 14) to a set of solutions to Boussinesq internally
heated convection in a ball defined over a regular ϕ, θ, r -grid, where we set tin= [0, ...,9], and
evaluation horizon tout= [10 , ...,14]. Results (Tab. 6, Fig. 8) for our equivariant model show good
generalization compared to a non-equivariant baseline [ 51]. The significantly larger Transolver [ 50]
model (see Appx. F) obtains better performance within the train time horizon on train and test
9sets, but overfits to this time horizon, generalizing poorly beyond. We additionally show results
for sparsely observed input states, showing the limitations of non NeF-based solvers to generalize
over irregular changing observation grids. We interpret these results as an indication of a marked
reduction in solving-complexity and improved generalization when correctly accounting for a PDE’s
symmetries.
Figure 8: Test samples (top) and corresponding
predictions from our model equivariant to S2-
rotations in the ball. (Eq. 14)
Table 6: Test MSE ↓for CFDBench auto-
regressive one forward propagation considering all
properties for the cavity, dam and cylinder flows.
Baselines taken from [32].
CAVITY DAM CYLINDER
Identity 6.42E-02 1.69E-03 7.54E-02
Auto-FFN 6.42E-02 1.68E-03 7.54E-02
Auto-DeepONet 6.39E-02 1.64E-03 7.53E-02
Auto-EDeepONet 6.45E-02 1.49E-03 7.43E-02
Auto-DeepONetCNN 6.33E-02 1.68E-03 7.52E-02
FNO 2.61E-02 8.75E-05 1.15E-03
U-Net 1.58E-02 1.70E-03 5.49E-05
OursaR2 1.10E-02 8.19E-05 1.42E-05Non-symmetric PDEs Lastly, we evaluate our
model in the setting when solving PDEs that
do not exhibit any global symmetries to assess
whether the preservation of symmetries in the
latent space of our model precludes application
to non-symmetric PDEs. We train a translation-
equivariant ( aR2) model, i.e. one with equivari-
ance properties identical to a CNN, on the Cav-
ity, Dam and Cylinder flows from CFDBench
[32]. Comparing to the baselines set by the
dataset authors, we see our method improves sig-
nificantly over a number of classical baselines,
despite no global transformational symmetries
being present in the problems being solved.
5 Limitations & Future work
We’re interested in exploring the application
of our ENF-based PDE solving framework to
larger-scale, more complex problems. Through-
out our experiments with ENFs we noticed that
more complex signals, e.g. higher resolution
PDEs, may be fit easily either by increasing the
ENF hidden size or by increasing the number of
latents used. However, either of these changes
significantly impacts computational complexity, due to the calculation of attention coefficients in
the latent space of the ENF for every latent-input coordinate pair. A possible way of addressing this
is detailed in [ 49]; we can approximate the output of the attention operation through limiting the
number of latents attended to (using k-nearest neighbours). This may open the door to modelling
more complex, larger-scale dynamics than learned in present experiments.
6 Conclusion
We introduce a novel equivariant space-time continuous framework for solving partial differential
equations (PDEs). Uniquely - our method handles sparse or irregularly sampled observations of
the initial state while respecting symmetry-constraints and boundary conditions of the underlying
PDE. We clearly show the benefit of symmetry-preservation over a range of challenging tasks, where
existing methods fail to capture the underlying dynamics.
References
[1]Ilze Amanda Auzina, Ça ˘gatay Yıldız, Sara Magliacane, Matthias Bethge, and Efstratios Gavves.
Modulated neural odes. Advances in Neural Information Processing Systems , 36, 2024.
[2]Ibrahim Ayed, Emmanuel De Bezenac, Arthur Pajot, and Patrick Gallinari. Learning the spatio-
temporal dynamics of physical processes from partial observations. In ICASSP 2020-2020
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages
3232–3236. IEEE, 2020.
[3]Matthias Bauer, Emilien Dupont, Andy Brock, Dan Rosenbaum, Jonathan Richard Schwarz,
and Hyunjik Kim. Spatial functa: Scaling functa to imagenet classification and generation.
arXiv preprint arXiv:2302.03130 , 2023.
10[4]Erik J Bekkers. B-spline cnns on lie groups. In International Conference on Learning Repre-
sentations , 2019.
[5]Erik J Bekkers, Sharvaree Vadgama, Rob D Hesselink, Putri A van der Linden, and David W
Romero. Fast, expressive se (n)equivariant networks through weight-sharing in position-
orientation space. arXiv preprint arXiv:2310.02970 , 2023.
[6]Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling.
Geometric and physical quantities improve e (3) equivariant message passing. arXiv preprint
arXiv:2110.02905 , 2021.
[7]Johannes Brandstetter, Rianne van den Berg, Max Welling, and Jayesh K Gupta. Clifford neural
layers for pde modeling. arXiv preprint arXiv:2209.04934 , 2022.
[8] Johannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural pde solvers.
arXiv preprint arXiv:2202.03376 , 2022.
[9]Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veli ˇckovi ´c. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478 , 2021.
[10] Keaton J. Burns, Geoffrey M. Vasil, Jeffrey S. Oishi, Daniel Lecoanet, and Benjamin P. Brown.
Dedalus: A flexible framework for numerical simulations with spectral methods. Physical
Review Research , 2(2):023068, April 2020. doi: 10.1103/PhysRevResearch.2.023068.
[11] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. Advances in neural information processing systems , 31, 2018.
[12] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International
conference on machine learning , pages 2990–2999. PMLR, 2016.
[13] Emilien Dupont, Hyunjik Kim, SM Eslami, Danilo Rezende, and Dan Rosenbaum. From
data to functa: Your data point is a function and you can treat it like one. arXiv preprint
arXiv:2201.12204 , 2022.
[14] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In International conference on machine learning , pages 1126–1135.
PMLR, 2017.
[15] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing
convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In
International Conference on Machine Learning , pages 3165–3176. PMLR, 2020.
[16] Joseph Galewsky, Richard K Scott, and Lorenzo M Polvani. An initial-value problem for testing
numerical models of the global shallow-water equations. Tellus A: Dynamic Meteorology and
Oceanography , 56(5):429–440, 2004.
[17] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International conference on machine learning ,
pages 1263–1272. PMLR, 2017.
[18] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. Advances
in neural information processing systems , 32, 2019.
[19] Xiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady flow ap-
proximation. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining , pages 481–490, 2016.
[20] Jacob Helwig, Xuan Zhang, Cong Fu, Jerry Kurtin, Stephan Wojtowytsch, and Shuiwang Ji.
Group equivariant fourier neural operators for partial differential equations. Proceedings of
the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202,
2023. , 2023.
[21] Quercus Hernández, Alberto Badías, David González, Francisco Chinesta, and Elías Cueto.
Structure-preserving neural networks. Journal of Computational Physics , 426:109950, 2021.
11[22] Pengzhan Jin, Zhen Zhang, Aiqing Zhu, Yifa Tang, and George Em Karniadakis. Sympnets:
Intrinsic structure-preserving symplectic networks for identifying hamiltonian systems. Neural
Networks , 132:166–179, 2020.
[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[24] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907 , 2016.
[25] David M Knigge, David W Romero, and Erik J Bekkers. Exploiting redundancy: Separable
group convolutional networks on lie groups. In International Conference on Machine Learning ,
pages 11359–11386. PMLR, 2022.
[26] Miltiadis Miltos Kofinas, Erik Bekkers, Naveen Nagaraja, and Efstratios Gavves. Latent field
discovery in interacting dynamical systems with neural fields. Advances in Neural Information
Processing Systems , 36, 2023.
[27] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya,
Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function
spaces. arXiv preprint arXiv:2108.08481 , 2021.
[28] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for
few-shot learning. arXiv preprint arXiv:1707.09835 , 2017.
[29] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya,
Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differen-
tial equations. arXiv preprint arXiv:2010.08895 , 2020.
[30] Yongtuo Liu, Sara Magliacane, Miltiadis Kofinas, and Efstratios Gavves. Graph switching
dynamical systems. In International Conference on Machine Learning , pages 21867–21883.
PMLR, 2023.
[31] Yongtuo Liu, Sara Magliacane, Miltiadis Kofinas, and Efstratios Gavves. Amortized equation
discovery in hybrid dynamical systems, 2024.
[32] Yining Luo, Yingfa Chen, and Zhen Zhang. Cfdbench: A comprehensive benchmark for
machine learning methods in fluid dynamics. arXiv preprint arXiv:2310.05963 , 2023.
[33] Philipp Moser, Wolfgang Fenz, Stefan Thumfart, Isabell Ganitzer, and Michael Giretzlehner.
Modeling of 3d blood flows with physics-informed neural networks: Comparison of network
architectures. Fluids , 8(2):46, 2023.
[34] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms.
arXiv preprint arXiv:1803.02999 , 2018.
[35] Samuele Papa, David M Knigge, Riccardo Valperga, Nikita Moriakov, Miltos Kofinas, Jan-
Jakob Sonke, and Efstratios Gavves. Neural modulation fields for conditional cone beam neural
tomography. arXiv preprint arXiv:2307.08351 , 2023.
[36] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.
Deepsdf: Learning continuous signed distance functions for shape representation. In Proceed-
ings of the IEEE/CVF conference on computer vision and pattern recognition , pages 165–174,
2019.
[37] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film:
Visual reasoning with a general conditioning layer. In Proceedings of the AAAI conference on
artificial intelligence , volume 32, 2018.
[38] Adeel Pervez, Francesco Locatello, and Efstratios Gavves. Mechanistic neural networks for
scientific machine learning. arXiv preprint arXiv:2402.13077 , 2024.
[39] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning
mesh-based simulation with graph networks. arXiv preprint arXiv:2010.03409 , 2020.
12[40] Michael Prasthofer, Tim De Ryck, and Siddhartha Mishra. Variable-input deep operator
networks. arXiv preprint arXiv:2205.11404 , 2022.
[41] Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural
networks. In International conference on machine learning , pages 9323–9332. PMLR, 2021.
[42] Vincent Sitzmann, Eric Chan, Richard Tucker, Noah Snavely, and Gordon Wetzstein. Metasdf:
Meta-learning signed distance functions. Advances in Neural Information Processing Systems ,
33:10136–10147, 2020.
[43] George Gabriel Stokes et al. On the effect of the internal friction of fluids on the motion of
pendulums. 1851.
[44] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,
Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let
networks learn high frequency functions in low dimensional domains. Advances in neural
information processing systems , 33:7537–7547, 2020.
[45] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T
Barron, and Ren Ng. Learned initializations for optimizing coordinate-based neural repre-
sentations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2846–2855, 2021.
[46] Riccardo Valperga, Kevin Webster, Dmitry Turaev, Victoria Klein, and Jeroen Lamb. Learning
reversible symplectic dynamics. In Proceedings of The 4th Annual Learning for Dynamics
and Control Conference , volume 168 of Proceedings of Machine Learning Research , pages
906–916. PMLR, 23–24 Jun 2022.
[47] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research , 9(11), 2008.
[48] Maurice Weiler and Gabriele Cesa. General e (2)-equivariant steerable cnns. Advances in neural
information processing systems , 32, 2019.
[49] David R Wessels, David M Knigge, Samuele Papa, Riccardo Valperga, Efstratios Gavves, and
Erik J Bekkers. Grounding continuous representations in geometry: Equivariant neural fields.
ArXiv Preprint arXiv: , 2024.
[50] Haixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, and Mingsheng Long. Transolver: A
fast transformer solver for pdes on general geometries. arXiv preprint arXiv:2402.02366 , 2024.
[51] Yuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain Rakotomamonjy, and Patrick
Gallinari. Continuous pde dynamics forecasting with implicit neural representations. arXiv
preprint arXiv:2209.14855 , 2022.
[52] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape
representation for neural fields and generative diffusion models. ACM Transactions on Graphics
(TOG) , 42(4):1–16, 2023.
[53] Maksim Zhdanov, David Ruhe, Maurice Weiler, Ana Lucic, Johannes Brandstetter, and Patrick
Forré. Clifford-steerable convolutional neural networks. arXiv preprint arXiv:2402.14730 ,
2024.
[54] David Zwicker. py-pde: A python package for solving partial differential equations. Journal
of Open Source Software , 5(48):2158, 2020. doi: 10.21105/joss.02158. URL https://doi.
org/10.21105/joss.02158 .
13A Related work
DL approaches to dynamics modelling In recent years, the learning of spatiotemporal dynamics has been
receiving significant attention, either for modelling interacting systems [ 31,30], scientific Machine Learning
[51,8,7,38,26,53], or even videos [ 1]. Most DL methods for solving PDEs attempt to directly replace solvers
with mappings between finite-dimensional Euclidean spaces, i.e. through the use of CNNs [ 19,2] or GNNs
[39,8] often applied autoregressively to an observed (discretized) PDE state. Instead, the Neural Operator (NO)
[27] paradigm attempts to learn infinite-dimensional operators, i.e. mappings between function spaces, with
limited success. Fourier Neural Operator (FNO) [ 29] extends this method by performing convolutions in the
spectral domain. FNO obtains much improved performance, but due to its reliance on FFT is limited to data on
regular grids.
Inductive biases in DL and dynamics modelling Geometric Deep Learning aims to improve model
generalization and performance by constraining/designing a model’s space of learnable functions based on
geometric principles. Prominent examples include Group Equivariant Convolutional Networks and Steerable
CNNs [ 12,4], generalizations of CNNs that respect symmetries of the data - such as dilations and continuous
rotations [ 48,15,25]. Analogously, Graph Neural Networks (GNNs) [ 24] or Message Passing Neural Networks
(MPNNS) [ 17] are a variant of neural network that respects set-permutations naturally found in graph data. They
are typically formulated for graphs G= (V,E), with nodes i∈ V and edges E. Typically nodes are embedded
into a node vector f0
i, which is subsequently updated over multiple layers of message passing . Message passing
consists of (1) computing messages mi,jover edges i, jfrom node jtoiwith the message function (taking into
account edge attributes ai,j:mi,j=ϕm(fl
i, fl
j, ai,j)(2) aggegating incoming messages: mi=P
j∈N(i)mi,j,
(3) computing updated node features fl+1
i=ϕu(fl
i, mi).
Recently, such methods have also been adapted for sparse physical data, e.g. for molecular property prediction
[41,6] - where the GNN is additionally required to respect transformation symmetries. [ 5] unifies these
approaches to equivariance under the guise of weight sharing over equivalence classes defined by bi-invariant
attributes of pairs of nodes i, j, a viewpoint we leverage in constructing the equivariant conditioning latent zν
t
corresponding to a PDE state νt. In the context of dynamics modelling, equivariant architectures have been
employed to incorporate various properties of physical systems in the modelling process, examples of such
properties are the symplectic structure [22], discrete symmetries such as reversing symmetries [46] and energy
conservation [18, 21].
Neural Fields in dynamics modelling Conditional Neural fields (NeFs) are a class of coordinate-based
neural networks, often trained to reconstruct discretely-sampled input continuously. More specifically, a
conditional neural field fθ:Rn→Rdis a field –parameterized by a neural network with parameters θ– that
maps input coordinates x∈Rnin the data domain alongside conditioning latents ztod-dimensional signal
values f(x)∈Rd. By associating a conditioning latent zν∈Rcto each signal ν, a single conditional NeF fθ:
Rn×Rc→Rdcan learn to represent families Dof continuous signals such that ∀ν∈ D:f(x)≈fθ(x;zν).
[13] showed the viability of using the latents zias representations for downstream tasks (e.g. classification,
generation) proposing a framework for learning on neural fields . This framework inherits desirable properties of
neural fields, such as inherent support for sparsely and/or irregularly sampled data, and independence to signal
resolution. [ 51] propose to use conditional NeFs for PDE modelling by learning a continuous flow in the latent
space of a conditional neural field. In particular, a set of latents {zν
i}T
i=1are obtained by fitting a conditional
neural field to a given set of observations {νi}T
i=1at timesteps 1, ..., T ; simultaneously, a neural ODE [11] Fψ
is trained to map pairs of temporally continuous latents s.t. solutions correspond to the trajectories traced by the
learned latents. Though this approach yields impressive results for sparse and irregular data in planar PDEs, we
show it breaks down on more challenging geometries. We hypothesize that this is due to a lack of a latent space
that preserves relevant geometric transformation with respect to which systems we are modelling are symmetric,
and as such propose an extension of this framework where such symmetries are preserved.
Obtaining Neural Fields representations Most NeF-based approach to representation or reconstruction
use SGD to optimize (a subset of) the parameters of the NeF, inevitably leading to significant overhead in
inference; conditional NeFs require optimizing a (set of) latents from initialization to reconstruct for a novel
sample. Accordingly, research has explored ways of addressing this limitation. [ 42,45] propose using Meta-
Learning [ 14,34] to optimize for an initialization for the NeF from which it is possible to reconstruct for a
novel sample in as few as 3 gradient descent steps. [ 13] propose to meta-learn the NeF backbone, but fix the
initialization for the latent zand instead optimize the learning rate used in its optimization using Meta-SGD
[28]. Recently, work has also explored the relation between initialization/optimization of a NeF and its value as
downstream representation; [ 35] show that (1) using a shared NeF initialization and (2) limiting the number
of gradient updates to the NeF improves performance in downstream tasks, as this simplifies the complex
relation between a NeFs parameter space and its output function space. We combine these insights and make
Meta-Learning part of our equivariant PDE solving pipeline, as it enables fast inference and we show it to
simplify the latent space of the ENF, improving performance of the neural ODE solver.
14B Pseudocode for optimization objective
See Alg. 1 for pseudocode of the training loop that we use, written for a single datasample for simplicity of
notation. For simplicity, we further assume we’re using an euler stepper to solve the neural ODE, but this can be
replaced by any solver. For inference, this stratagem is identical, except we do not perform gradient updates to
θ, ψ.
Algorithm 1 Optimization objective
Randomly initialize neural field fθ
Randomly initialize neural ode Fψ
while not done do
Sample initial states and coordinates ν0.
Initialize latents zν
0← {(pi,ci)}N
i=1.
for all step∈Ninitial state opt = 3do
zν
0←zν
0−ϵ∇zν
0Lmse 
fθ(·, zν
0), ν0)
end for
for all t∈[1, ..., t in]do
zν
t←zν
0+Rt
0Fψ(zν
τ)dτ
end for
Update θ, ψ per:
θ←θ−η∇θL′
mse,ψ←ψ−η∇ψL′
msewithL′
mse= (
fθ(·, zν
t), νt	tin
t=0
end while
C Equivariant Neural Fields
ENF to reconstruct PDE states For ease of notation we denote PandCthe matrices containing poses
and corresponding appearances stacked row-wise, i.e. Pi,:=pT
iandCi,:=cT
i. Furthermore, we denote Aas
the matrix containing all bi-invariants ai,mstacked row-wise, i.e. Ai,:=aT
i,m:
fθ(x;zνt) := softmaxQ(A)KT(C)√dk+G(A)
V(C;A), (10)
where the softmax is applied over the latent set and with dkthe hidden dimensionality of the ENF. The query
matrix Qis constructed as Q=WqγT
q(A),γqa Gaussian RFF embedding [ 44], followed by a linear layer Wq,
i.e.Qconsists of the RFF embedded bi-invariants of the input coordinate xmand each of the latent poses
pistacked row-wise. The key matrix is given by a learnable linear transformation Wkof the context vectors
ci:K=WkCT. The attention coefficients which result from the inner product of Q,Kare weighted by a
Gaussian window Gwhose magnitude is conditioned on a distance measure on the relative distance between
latent poses and input coordinates as: Gi=σatt(||pi−x||2), with σatta hyperparameter which determines
thelocality of each of the latents. Finally the value matrix is calculated as a learnable linear transformation
Wvof the appearances A, conditioned through FiLM modulation [ 37] by a second RFF embedding of the
relative poses split into scale- and shift modulations: V=WvA⊙Wvαγvα(A) +Wvβγvβ(A). The latents
zν
tare optimized for a single state νt, whereas the parameters θof the ENF backbone - which consist of all the
learnable parameters of the linear layers Wq,Wk,Wv,Wvα,Wvβused to construct Q,K,V- are shared
over all states.
The overall architecture consists of a linear layer WRc→Rdapplied to ci∈Rc, followed by a layernorm.
After this, the cross attention listed above is applied, followed by three d-dim linear layers, the final one mapping
to the output dimension Rout.
Equivariance follows from sharing Q,K,Vover equivalence classes Note that the latent space of
the ENF is equipped with a group action as: gzν
t={(gpi,ai)}N
i=1. As an example, SE(2) -equivariance of the
ENF follows from bi-invariance of the quantity aused to construct Qunder the group action:
∀g∈SE(n) : (pi,x)7→(g pi, gx)⇔p−1
ix7→(g pi)−1gx=p−1
ig−1gx=p−1
ig−1g . (11)
And so, constructing the matrix containing the relative poses of bi-transformed poses and coordinates (gP)−1gx
as((gP)−1gx)i,:=p−1
ig−1gx=p−1
ix, we trivially have:
∀g∈SE(n) : (pi,x)7→(g pi, gx)⇔Q(A)7→Q(gA) =Q(A). (12)
15D Defining additional bi-invariant attributes
Other examples of the bi-invariants attributes that are used in the experiments section are listed here.
Full rotation symmetries on the 2-sphere For the global shallow water equations we defined aSWas an attribute
that is bi-invariant only to rotations over globe’s axis, i.e. rotations over ϕ. In our experiments we also solve
diffusion over the sphere, which is fully SO(3) rotationally symmetric. To achieve equivariance to full 3d
rotations, we take poses p∈SO(3) parameterized by euler angles which act on points x∈S2parameterized by
3D unit vectors xthrough 3D-rotation matrices, allowing us to calculate the bi-invariant p−1x:
aSO(3)
i,m =Rixm. (13)
This bi-invariant is used in our experiments for diffusion on the 2-sphere.
The 3D ball B3. We experiment with Boussinesq equation for internally heated convection in a ball. The PDE is
fully rotationally symmetric, but since the heat source Kis at a fixed point (the center of the ball resp.), it is not
symmetric to translations of the initial conditions within the ball. As such, we let p∈SO(3) ×Rwithϕ, θ, γ, r
s.t.0< r < 1. The PDE is defined over spherical coordinates (ϕ, θ, r ), which we map to vectors in x∈R3.
We then use the following bi-invariant, which is only symmetric to rotations in SO(3) :
aB3
i,m=Rixm⊕rpi⊕rxm. (14)
No transformation symmetries . A simple "bi-invariant" for this setting that preserves all geometric information
is given by simply concatenating coordinates pwith coordinates x:
a∅
i,m=pi⊕xm (15)
Parameterizing the cross-attention operation in Eq. 5 as function of this bi-invariant results in a framework
without any equivariance constraints. We use this in experiments to ablate over equivariance constraints and its
impact on performance.
E Experimental Details
E.1 Dataset creation
For creating the dataset of PDE solutions we used py-pde [ 54] for Navier-Stokes and the diffusion equation on
the plane. For the shallow-water equation and the diffusion equation on the sphere, as well as the internally
heated convection in a 3D ball we used Dedalus [10].
Diffusion on the plane. For the diffusion equation on the plane we use as initial conditions narrow spikes
centred at random locations in the left half of the domain for the train set, and in the right half of the domain for
the test set. States are defined on a 64 ×64 grid ranging from -3 to 3. Initial conditions are randomly sampled
uniformly between -2 and 2 for xand 0 and 2 for yin the training set and between -2 and 2 for xand -2 and 0
fory. A random value uniformly sampled between 5.0 and 5.5 is inserted at the randomly sampled location.
We solve the equation with an Euler solver for 27 steps, discarding the first 7, with a timestep dt= 0.01. We
generate 1024 training and 128 test trajectories.
Navier-Stokes on the flat 2-torus. For Navier-Stokes on the flat 2-torus we use Gaussian random fields
as initial conditions and solve the PDE using a Cranck-Nicholson method with timestep dt= 1.0for 20 steps.
The PDE isdv
dt=−u∇v+v∆µ+f, v=∇ ×u,∇u= 0, where uis the velocity field, vthe vorticity, µthe
viscosity and fa forcing term
dv
dt=−u∇v+v∆µ+f
v=∇ ×u
∇u= 0,
where uis the velocity field, vthe vorticity, µthe viscosity and fa forcing term. We set viscosity to 1E−3,
resulting with our setup in a Reynolds number of ∼1
v= 1000 . States are defined on a 64 ×64 grid. We
generate 8192 training and 512 test trajectories.
Diffusion on the 2-sphere. For the diffusion dataset on the sphere, states are defined over a 128×64
ϕ, θgrid. Initial conditions are generated as a gaussian peak inserted at a random point on the sphere with
σ= 0.25. The equation is solved for 20 timesteps with RK4 and dt= 1.0. We generate 256 training and 64 test
trajectories.
16Spherical whallow-water equations [16]. The global shallow-water equations are
du
dt=−fk×u−g∇h+ν∆u
dh
dt=−h∇ ·u+ν∆h,
whered
dtis the material derivative, kis the unit vector orthogonal to the surface of the sphere, uis the velocity
field that is tangent to the spherical surface and and his the thickness of the fluid layer. The rest are constant
parameters of the Earth (see [ 16] for details). As initial conditions we follow [ 16] and use basic zonal flow,
representing a mid-latitude tropospheric jet, with a correspondingly balanced height field.
u(ϕ) =

0 forϕ≤ϕ0
umax
enexph
1
(ϕ−ϕ0)(ϕ−ϕ1)i
forϕ0< ϕ < ϕ 1
0 forϕ≥ϕ1
Where umax= 80ms−1,ϕ0=π/7, ϕ1=π/2−ϕ1, and en=exp[−4(ϕ1−ϕ0)2]. With this initial zonal
flow, we numerically integrate the balance equation
gh(ϕ) =gh0−Zϕ
au(ϕ′)
f+tan(ϕ′)
au(ϕ′)
dϕ′,
to obtain the height h. We then randomly generate small un-balanced perturbations h′to the height field
h′(θ, ϕ) =ˆhcos(ϕ)e−(θ2−θ/α)2e−[(ϕ2−ϕ)/β]2
by uniformly sampling α, β, ˆh, θ2,andϕ2within a neighbourhood of the values use in [ 16]. States are defined
on a 192 ×96 grid for the high-resolution dataset, which is subsequently downsampled by 2×2mean pooling
to a96×48grid. We generate 512 training trajectories and 64 test trajectories.
Internally-heated convection in the ball. The equations for the internally-heated convection system are
listed here, they include thermal diffusivity ( κ) and kinematic viscosity ( ν), given by:
κ= (Ra·Pr)−1/2
ν=Ra
Pr−1/2
We set Ra = 1e−6and Pr = 1.
1. Incompressibility condition (continuity equation):
∇ ·u+τp= 0
2. Momentum equation (Navier-Stokes equation):
∂u
∂t−ν∇2u+∇p−rT+lift(τu) =−u×(∇ ×u)
3. Temperature equation:
∂T
∂t−κ∇2T+lift(τT) =−u· ∇T+κTsource
4. Shear stress boundary condition (stress-free condition):
Shear Stress = 0on the boundary
5. No penetration boundary condition (radial component of velocity at r= 1):
radial (u(r= 1)) = 0
6. Thermal boundary condition (radial gradient of temperature at r= 1):
radial (∇T(r= 1)) = −2
7. Pressure gauge condition:Z
p dV = 0
The boundary conditions imposed are stress-free and no-penetration for the velocity field and a constant thermal
flux at the outer boundary. These conditions are enforced using penalty terms ( τ) that are lifted into the domain
using higher-order basis functions.
States are defined over a 64×24×24ϕ, θ, r grid. We use a SBDF2 solver which we constrain by dtmin= 1e−4
anddtmax= 2e−2. We evolve the PDE for 26 timesteps, discarding the first 6. We generate 512 training
trajectories and 64 test trajectories.
17E.2 Training details
We provide hyperparameters per experiment. We optimize the weights of the neural field fθ, and neural ODE
Fψwith Adam [ 23] with a learning rate of 1E-4 and 1E-3 respectively. We initialize the inner learning rate
that we use in Meta-SGD [ 28] for learning zνat 1.0 for pand 5.0 for c. For the neural ODE Fψ, we use 3 of
our message passing layers in the architecture specified in [ 5], with a hidden dimensionality of 128. The std
parameter of the RFF embedding functions γq, γvα, γvβ(see Appx. C), is chosen per experiment. We run all
experiments on a single A100. All experiments are ran 3 times.
Diffusion on the plane. We use 4 latents with c∈R16. We set the hidden dim of the ENF to 64 and use 2
attention heads. We train the model for 1000 epochs. We set γq= 0.05, γvα= 0.01, γvβ= 0.01. We use a
batch size of 8. The model takes approximately 8 hours to train.
Navier-Stokes on the flat 2-torus. We use 4 latents with c∈R16. We set the hidden dim of the ENF to
64 and use 2 attention heads. We train the model for 2000 epochs. We set γq= 0.05, γvα= 0.2, γvβ= 0.2.
We use a batch size of 4. The model takes approximately 48 hours to train.
Diffusion on the 2-sphere. We use 18 latents with c∈R4. We set the hidden dim of the ENF to 16 and
use 2 attention heads. We train the model for 1500 epochs. We set γq= 0.01, γvα= 0.01, γvβ= 0.01. We use
a batch size of 2. The model takes approximately 12 hours to train.
Spherical whallow-water equations [ 16].We use 8 latents with c∈R32. We set the hidden dim of
the ENF to 128, and use 2 attention heads. We train the model for 1500 epochs. We set γq= 0.05, γvα=
0.2, γvβ= 0.2. We use a batch size of 2. The model takes approximately 24 hours to train.
Internally-heated convection in the ball We use 8 latents with c∈R32. We set the hidden dim of
the ENF to 128, and use 2 attention heads. We train the model for 1500 epochs. We set γq= 0.05, γvα=
0.2, γvβ= 0.2. We use a batch size of 2. The model takes approximately 24 hours to train.
CFDBench [ 32]We use 25 latents with c∈R16. We set the hidden dim of the ENF to 128, and use 1
attention head. We set γq= 0.05, γvα= 0.1, γvβ= 0.1. We train the model for 100 epochs on a single A100
GPU, taking about 16 hours.
Baselines As baseline models on Navier-Stokes we train FNO and GFNO [ 29] with 8 modes and 32 channels
for 700 epochs (until convergence). We train CNODE [ 2] with 4 layers of size 64 for 300 epochs (until
convergence). We train DINo on all experiments for 2000 epochs with an architecture as specified in [ 51]. For
the IHC and shallow-water experiments, we increase the latent dim from 100 to 200, the number of layers for
the neural ODE from 3 to 5, and the latent dim of the neural field decoder from 64 to 256, as per [51].
For the Navier-Stokes and Internally-Heated Convection experiments we additionally train the Transolver
[50] model. We adapt the Transolver model in PyTorch from the official github repository, and use the same
hyperparameter settings as in the original paper (resulting in a 7.1M parameter model) - modifying the training
objective to be identical to the autoregressive one we use in our experiments (i.e. mapping from frame to frame).
During training in the Navier-Stokes task we observed noticeable instabilities, with the method producing
high-frequency artefacts in rollouts in this autoregressive setting. For sparsely subsampled initial conditions,
performance deteriorates further - highlighting the benefit of NeF-based continuous PDE solving.
For experiments on internally-heated convection, due to the large size of the input frames, we were required to
scale down the Transolver model size in order to be able to fit the model on our A100 GPU, from 256 to 64
hidden units, resulting in a 1.2M parameter model - somewhat comparable in size to the model we use in our
experiments (889K). We train the model for 2000 epochs on an A100 GPU, taking approximately 30 hours.
F Additional results
Parameter counts, memory and time efficiency In order to compare parameter, memory and time
efficiency of our method to other baselines, we provide details of our models compared to [ 51] in Tab. 7, and
memory usage/inference time compared to the models we used as baselines in the Navier-Stokes experiments
in 8. We note that although our method is not the most memory efficient - meta-learning requires a significant
computational overhead - it is the fastest in inference settings when unrolling an unseen state. This is attributable
to (1) the fact that we’re using a latent-space solver which operates on a drastically compressed representation of
the PDE state, and use (2) meta-learning over auto-decoding, which means we only require 3 gradient descent
steps to obtain the initial state compared to 300-500 typically used in auto-decoding [51].
18Table 7: Parameter count for model used in
Navier-Stokes, Shallow-Water and IHC exper-
iments.
#PARAMS fθ#PARAMS Fψ TOT #PARAMS
NAVIER -STOKES
DINo [51] 333k 502k 835k
Ours 354k 531k 885k
SHALLOW -WATER
DINo [51] 639k 902k 1,5M
Ours 356k 533k 889k
INTERNALLY -HEATED CONVECTION
DINo [51] 639k 702k 1,3M
Ours 356k 533k 889kTable 8: Parameter count and runtimes for for models
used in Navier-Stokes experiments. Note that training
runtimes are measured per epoch, inference runtimes
are measured for unrolling a single 20-step trajectory.
GPU memory allocations are measured per trajectory.
#PARAMS RUNTIME /EPGPU MEM RUNTIME /TRAJ GPU MEM
TRAINING INFERENCE
FNO 541k 40.7s 0.35Gb 54.0ms 0.20Gb
GFNO 3.8M 200.6s 2.08Gb 423.1ms 1.28Gb
CNODE 1.8M 1480.0s 1.45Gb 170.4ms 0.71Gb
DINo [51] 835k 25.8s 0.23Gb 61.7ms 0.14Gb
Transolver [50] 7.1M 619.2s 9.42Gb 43.6ms 5.19Gb
Ours 885k 70.0s 0.70Gb 9.5ms 0.61Gb
Figure 9: Error accumulation
over long rollouts for Navier-
Stokes test set with ν0100%
observed, train horizon tinis
marked in green.
Figure 10: Error accumulation
over long rollouts for Navier-
Stokes test set with ν050%
observed, train horizon tinis
marked in green.
Figure 11: Visualization of an
80-step rollout for a Navier-
Stokes test sample.
Error accumulation on long rollouts for Navier-Stokes In order to better empirically assess error
accumulation of our method for long-term extrapolation, we provide experimental results for unrolling of
Navier-Stokes for 80 timesteps. We apply our model both in a setting with fully observed initial conditions and
sparsely observed (50%) initial conditions, and provide plots of accumulated MSE along these trajectories in
Figs. 9,10, and a visualization of the solution and error in Fig. 11. We compare against DINo [ 51] and FNO [ 29],
and show that we consistently improve over DINo in terms of long-term extrapolation. FNO achieves better
extrapolation limits due to reduced error accumulation in the fully observed setting, but very rapidly deteriorates
even within the train horizon with sparsely observed initial conditions, whereas our proposed approach loses
very little in terms of extrapolation performance in this sparse setting.
19NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope?
Answer: [Yes]
Justification: We feel the abstract and introduction accurately reflect the contributions made in the
paper, the experiments in Sec. 4 clearly show that our framework achieves improved performance
exactly because of the equivariance constraints.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims made in the
paper.
•The abstract and/or introduction should clearly state the claims made, including the contributions
made in the paper and important assumptions and limitations. A No or NA answer to this
question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how much the
results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals are not
attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss some limitations of the model’s performance, namely the inability to
generalize well on toutfor the global shallow-water equitions, and our hypothesis for the cause of this;
namely limited reconstruction accuracy of the decoder on complex states ν.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that the paper
has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to violations of
these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,
asymptotic approximations only holding locally). The authors should reflect on how these
assumptions might be violated in practice and what the implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was only tested
on a few datasets or with a few runs. In general, empirical results often depend on implicit
assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach. For
example, a facial recognition algorithm may perform poorly when image resolution is low or
images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide
closed captions for online lectures because it fails to handle technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms and how
they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to address problems
of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by reviewers
as grounds for rejection, a worse outcome might be that reviewers discover limitations that
aren’t acknowledged in the paper. The authors should use their best judgment and recognize
that individual actions in favor of transparency play an important role in developing norms that
preserve the integrity of the community. Reviewers will be specifically instructed to not penalize
honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and a complete
(and correct) proof?
Answer: [Yes]
Justification: The main claim of the paper; that we obtain equivarance by weight-sharing over bi-
invariances is proven in Eq. 4.
Guidelines:
20• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if they appear in
the supplemental material, the authors are encouraged to provide a short proof sketch to provide
intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented by
formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main experimental
results of the paper to the extent that it affects the main claims and/or conclusions of the paper
(regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The results should be reproducible based only on the contents of this paper and the
references made in it. Code will be made available for a camera-ready version, which also includes
code for generating the datasets of solutions and experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived well by the
reviewers: Making the paper reproducible is important, regardless of whether the code and data
are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken to make
their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways. For
example, if the contribution is a novel architecture, describing the architecture fully might suffice,
or if the contribution is a specific model and empirical evaluation, it may be necessary to either
make it possible for others to replicate the model with the same dataset, or provide access to
the model. In general. releasing code and data is often one good way to accomplish this, but
reproducibility can also be provided via detailed instructions for how to replicate the results,
access to a hosted model (e.g., in the case of a large language model), releasing of a model
checkpoint, or other means that are appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submissions
to provide some reasonable avenue for reproducibility, which may depend on the nature of the
contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how to
reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe the
architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should either be
a way to access this model for reproducing the results or a way to reproduce the model (e.g.,
with an open-source dataset or instructions for how to construct the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case authors are
welcome to describe the particular way they provide for reproducibility. In the case of
closed-source models, it may be that access to the model is limited in some way (e.g.,
to registered users), but it should be possible for other researchers to have some path to
reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instructions to
faithfully reproduce the main experimental results, as described in supplemental material?
Answer: [Yes]
Justification: Code is added to the submission.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
21•While we encourage the release of code and data, we understand that this might not be possible,
so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless
this is central to the contribution (e.g., for a new open-source benchmark).
•The instructions should contain the exact command and environment needed to run to reproduce
the results. See the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how to access
the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new proposed
method and baselines. If only a subset of experiments are reproducible, they should state which
ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized versions (if
applicable).
•Providing as much information as possible in supplemental material (appended to the paper) is
recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [Yes]
Justification: These details are listed in Appx. E.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail that is
necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate informa-
tion about the statistical significance of the experiments?
Answer: [Yes]
Justification: std is given for experimental results over 3 runs.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the main claims
of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for example,
train/test split, initialization, random drawing of some parameter, or overall run with given
experimental conditions).
•The method for calculating the error bars should be explained (closed form formula, call to a
library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error of the
mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report
a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is
not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or figures
symmetric error bars that would yield results that are out of range (e.g. negative error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how they were
calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
Answer: [Yes]
Justification: We provide details on the hardware on which we run the experiments, the approximate
runtime per run. This should be enough information to reproduce.
22Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual experimental
runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute than the
experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into
the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code
of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We respect the code of ethics.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a deviation
from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consideration due
to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative societal impacts
of the work performed?
Answer: [Yes]
Justification: The paper provides a way for improving PDE forecasting. This can have many positive
societal impacts, e.g. improving weather forecasting for early warning, material sciences. However, the
framework could provide erroneous predictions, without any indication of its confidence, which could
have major negative consequences. Future work should look into associating confidence intervals to
predictions, to get indications of the viability of results.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal impact or
why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses (e.g.,
disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-
ment of technologies that could make decisions that unfairly impact specific groups), privacy
considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied to particular
applications, let alone deployments. However, if there is a direct path to any negative applications,
the authors should point it out. For example, it is legitimate to point out that an improvement in
the quality of generative models could be used to generate deepfakes for disinformation. On the
other hand, it is not needed to point out that a generic algorithm for optimizing neural networks
could enable people to train models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is being used
as intended and functioning correctly, harms that could arise when the technology is being used
as intended but gives incorrect results, and harms following from (intentional or unintentional)
misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation strategies
(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitor-
ing misuse, mechanisms to monitor how a system learns from feedback over time, improving the
efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible release of
data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or
scraped datasets)?
Answer: [NA]
23Justification: We want the paper to reproducibel and as such we provide code. There are no major
risks for misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with necessary
safeguards to allow for controlled use of the model, for example by requiring that users adhere to
usage guidelines or restrictions to access the model or implementing safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors should
describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do not require
this, but we encourage authors to take this into account and make a best faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,
properly credited and are the license and terms of use explicitly mentioned and properly respected?
Answer: [Yes]
Justification: Authors are original owners, or credit relevant owners as required.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of service of
that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package should
be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for
some datasets. Their licensing guide can help determine the license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of the derived
asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to the asset’s
creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation provided
alongside the assets?
Answer: [NA]
Justification: Paper does not provide new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their sub-
missions via structured templates. This includes details about training, license, limitations,
etc.
•The paper should discuss whether and how consent was obtained from people whose asset is
used.
•At submission time, remember to anonymize your assets (if applicable). You can either create an
anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper include
the full text of instructions given to participants and screenshots, if applicable, as well as details about
compensation (if any)?
Answer: [NA]
Justification: Not applicable.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
24•Including this information in the supplemental material is fine, but if the main contribution of the
paper involves human subjects, then as much detail as possible should be included in the main
paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
labor should be paid at least the minimum wage in the country of the data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects
Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an
equivalent approval/review based on the requirements of your country or institution) were obtained?
Answer: [NA]
Justification: Not applicable.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent) may be
required for any human subjects research. If you obtained IRB approval, you should clearly state
this in the paper.
•We recognize that the procedures for this may vary significantly between institutions and
locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for
their institution.
• For initial submissions, do not include any information that would break anonymity (if applica-
ble), such as the institution conducting the review.
25