Retentive Network
Anonymous Author(s)
Affiliation
Address
email
Abstract
In this work, we propose Retentive Network ( RETNET) as a foundation architecture 1
for large language models, simultaneously achieving training parallelism, low-cost 2
inference, and good performance. We theoretically derive the connection between 3
recurrence and attention. Then we propose the retention mechanism for sequence 4
modeling, which supports three computation paradigms, i.e., parallel, recurrent, 5
and chunkwise recurrent. Specifically, the parallel representation allows for training 6
parallelism. The recurrent representation enables low-cost O(1)inference, which 7
improves decoding throughput, latency, and GPU memory without sacrificing 8
performance. The chunkwise recurrent representation facilitates efficient long- 9
sequence modeling with linear complexity, where each chunk is encoded parallelly 10
while recurrently summarizing the chunks. Experimental results on language 11
modeling show that RETNETachieves favorable scaling results, parallel training, 12
low-cost deployment, and efficient inference. 13
1 Introduction 14
Transformer [ 51] has become the de facto architecture for large language models, which was initially 15
proposed to overcome the sequential training issue of recurrent models [ 25]. However, training 16
parallelism of Transformers is at the cost of inefficient inference, because of the O(N)complexity per 17
step and memory-bound key-value cache [ 42], which renders Transformers unfriendly to deployment. 18
The growing sequence length increases GPU memory consumption as well as latency and reduces 19
inference speed. Numerous efforts have continued to develop the next-generation architecture, aiming 20
at retaining training parallelism and competitive performance as Transformers while having efficient 21
O(1)inference. It is challenging to achieve the above goals simultaneously. 22
There have been three main strands of research. First, linearized attention [ 27,37] approximates 23
standard attention scores exp(q·k)with kernels ϕ(q)·ϕ(k), so that autoregressive inference can 24
be rewritten in a recurrent form. However, the modeling capability and performance are worse than 25
Transformers, which hinders the method’s popularity. The second strand returns to recurrent models 26
for efficient inference while sacrificing training parallelism. As a remedy, element-wise operators [ 36] 27
are used for acceleration, however, representation capacity and performance are harmed. The third 28
line explores replacing attention with other mechanisms, such as S4 [ 20], and its variants [ 11,38]. 29
None of the previous work can achieve strong performance and efficient inference at the same time 30
compared to Transformers. 31
In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long- 32
sequence modeling, Transformer-comparable performance, and parallel model training simultane- 33
ously. Specifically, we introduce a multi-scale retention mechanism to substitute multi-head attention, 34
which has three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent repre- 35
sentations. First, the parallel representation empowers training parallelism to utilize GPU devices 36
fully. Second, the recurrent representation enables efficient O(1)inference in terms of memory 37
and computation. The deployment cost and latency can be significantly reduced. Moreover, the 38
implementation is greatly simplified without key-value cache tricks. Third, the chunkwise recurrent 39
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.representation can perform efficient long-sequence modeling. We parallelly encode each local block 40
for computation speed while recurrently encoding the global blocks to save GPU memory. 41
We compare RetNet with Transformer and its variants. Experimental results on language modeling 42
show that RetNet is consistently competitive in terms of both scaling curves and in-context learning. 43
Moreover, the inference cost of RetNet is length-invariant. For a 7B model and 8k sequence 44
length, RetNet decodes 8.4 ×faster and saves 70% of memory than Transformers with key-value 45
caches. During training, RetNet also achieves 3 ×acceleration than standard Transformer with 46
highly-optimized FlashAttention-2 [ 10]. Besides, RetNet’s inference latency is insensitive to batch 47
size, allowing enormous throughput. The intriguing properties make RetNet a potential candidate to 48
replace Transformer for large language models. 49
2 Retentive Network 50
Retentive network (RetNet) is stacked with Lidentical blocks, which follows a similar layout (i.e., 51
residual connection, and pre-LayerNorm) as in Transformer [ 51]. Each RetNet block contains two 52
modules: a multi-scale retention (MSR) module, and a feed-forward network (FFN) module. We 53
introduce the MSR module in the following sections. Given an input sequence x=x1···x|x|, 54
RetNet encodes the sequence in an autoregressive way. The input vectors {xi}|x|
i=1is first packed 55
intoX0= [x1,···,x|x|]∈R|x|×dmodel, where dmodel is hidden dimension. Then we compute 56
contextualized vector representations Xl= RetNet l(Xl−1), l∈[1, L]. 57
2.1 Retention 58
In this section, we introduce the retention mechanism that has a dual form of recurrence and 59
parallelism. So we can train the models in a parallel way while recurrently conducting inference. 60
Consider a sequence modeling problem that maps v(n)7→o(n)through states sn. Letvn, ondenote 61
v(n), o(n)for simplicity. We formulate the mapping in a recurrent manner: 62
sn=Asn−1+K⊺
nvn, A∈Rd×d, K n∈R1×d
on=Qnsn=nX
m=1QnAn−mK⊺
mvm, Q n∈R1×d(1)
where we map vnto the state vector sn, and then implement a linear transform to encode sequence 63
information recurrently. Next, we make the projection Qn, Kncontent-aware: 64
Q=XW Q, K =XW K (2)
where WQ, WK∈Rd×dare learnable matrices. 65
We diagonalize the matrix A= Λ( γeiθ)Λ−1, where γ, θ∈Rd. Then we obtain An−m= 66
Λ(γeiθ)n−mΛ−1. By absorbing ΛintoWQandWK, we can rewrite Equation (1) as: 67
on=nX
m=1Qn(γeiθ)n−mK⊺
mvm
=nX
m=1(Qn(γeiθ)n)(Km(γeiθ)−m)⊺vm(3)
where Qn(γeiθ)n, Km(γeiθ)−mis known as xPos [ 45], i.e., a relative position embedding proposed 68
for Transformer. We further simplify γas a scalar, Equation (3) becomes: 69
on=nX
m=1γn−m(Qneinθ)(Kmeimθ)†vm (4)
where†is the conjugate transpose. The formulation is easily parallelizable within training instances. 70
In summary, we start with recurrent modeling as shown in Equation (1), and then derive its parallel 71
formulation in Equation (4). We consider the original mapping v(n)7→o(n)as vectors and obtain 72
the retention mechanism as follows. 73
2𝑋𝐾𝑄 𝑉𝑂
GN
𝑄𝐾⊺⊙𝐷𝑉(a) Parallel representation.
𝑋𝑛𝑆𝑛−1 𝑆𝑛𝛾
𝐾𝑛 𝑉𝑛 𝑄𝑛𝑂𝑛Recurrent
State
InputOutput
GN (b) Recurrent representation.
Figure 1: RetNet has three equivalent computation paradigms, i.e., parallel, recurrent, and chunkwise
recurrent representations. Given the same input, three paradigms obtain the same output. “GN” is
short for GroupNorm.
The Parallel Representation of Retention As shown in Figure 1a, the retention layer is defined as: 74
Q= (XW Q)⊙Θ, K = (XW K)⊙Θ, V =XW V
Θn=einθ, D nm=γn−m, n≥m
0, n < m
Retention( X) = (QK⊺⊙D)V(5)
where D∈R|x|×|x|combines causal masking and exponential decay along relative distance as one 75
matrix, and Θis the complex conjugate of Θ. In practice, we map Q, K∈Rd→Cd/2, add the 76
complex position embedding Θ, then map them back to Rd, following the implementation trick as in 77
LLaMA [48, 44]. Similar to self-attention, the parallel representation enables us to train the models 78
with GPUs efficiently. 79
The Recurrent Representation of Retention As shown in Figure 1b, the proposed mechanism can 80
also be written as recurrent neural networks (RNNs), which is favorable for inference. For the n-th 81
timestep, we recurrently obtain the output as: 82
Sn=γSn−1+K⊺
nVn
Retention( Xn) =QnSn, n = 1,···,|x|(6)
where Q, K, V, γ are the same as in Equation (5). 83
The Chunkwise Recurrent Representation of Retention A hybrid form of parallel representation 84
and recurrent representation is available to accelerate training, especially for long sequences. We 85
divide the input sequences into chunks. Within each chunk, we follow the parallel representation 86
(Equation (5)) to conduct computation. In contrast, cross-chunk information is passed following the 87
recurrent representation (Equation (6)). Specifically, let Bdenote the chunk length. We compute the 88
retention output of the i-th chunk via: 89
Q[i]=QBi:B(i+1), K [i]=KBi:B(i+1), V [i]=VBi:B(i+1)
Ri=K⊺
[i](V[i]⊙ζ) +γBRi−1, ζ ij=γB−i−1
Retention( X[i]) = (Q[i]K⊺
[i]⊙D)V[i]| {z }
Inner-Chunk+ (Q[i]Ri−1)⊙ξ|{z}
Cross-Chunk, ξ ij=γi+1(7)
where [i]indicates the i-th chunk, i.e., x[i]= [x(i−1)B+1,···, xiB]. The proof of the equivalence 90
between recurrent representation and chunkwise recurrent representation is described in Appendix B. 91
2.2 Gated Multi-Scale Retention 92
We use h=dmodel/dretention heads in each layer, where dis the head dimension. The heads use 93
different parameter matrices WQ, WK, WV∈Rd×d. Moreover, multi-scaleretention (MSR) assigns 94
3def ParallelRetention(
q, k, v, # bsz ∗num_head ∗len ∗qkv_dim
decay_mask): # num_head ∗len ∗len
retention = q @ k.transpose( −1,−2)
retention = retention ∗decay_mask
output = retention @ v
output = group_norm(output)
return output
def RecurrentRetention(
q, k, v, # bsz ∗num_head ∗qkv_dim
past_kv, # bsz ∗num_head ∗qk_dim ∗v_dim
decay): # num_head ∗1∗1
current_kv = decay ∗past_kv + k.unsqueeze( −1)∗v.
unsqueeze( −2)
output = torch.sum(q.unsqueeze( −1)∗current_kv,
dim= −2)
output = group_norm(output)
return output, current_kvdef ChunkwiseRetention(
q, k, v, # bsz ∗num_head ∗chunk_size ∗
qkv_dim
past_kv, # bsz ∗num_head ∗qk_dim ∗
v_dim
decay_mask, # num_head ∗chunk_size ∗
chunk_size
chunk_decay, # num_head ∗1∗1
inner_decay): # num_head ∗chunk_size
retention = q @ k.transpose( −1,−2)
retention = retention ∗decay_mask
inner_retention = retention @ v
cross_retention = (q @ past_kv) ∗
inner_decay
retention = inner_retention +
cross_retention
output = group_norm(retention)
current_kv = chunk_decay ∗past_kv + k.
transpose( −1,−2) @ v
return output, current_kv
Figure 2: Pseudocode for the three computation paradigms of retention. Parallel implementation
enables training parallelism to fully utilize GPUs. Recurrent paradigm enables low-cost inference.
Chunkwise retention combines the above advantages (i.e., parallel within each chunk and recurrent
across chunks), which has linear memory complexity for long sequences.
different γfor each head. For simplicity, we set γidentical among different layers and keep them 95
fixed. In addition, we add a swish gate [ 23,40] to increase the non-linearity of retention layers. 96
Formally, given input X, we define the layer as: 97
γ= 1−2−5−arange(0 ,h)∈Rh
head i= Retention( X, γ i)
Y= GroupNormh(Concat(head 1,···,head h))
MSR( X) = (swish( XW G)⊙Y)WO(8)
where WG, WO∈Rdmodel×dmodelare learnable parameters, and GroupNorm [53] normalizes the 98
output of each head, following SubLN proposed in [ 43]. Notice that the heads use multiple γscales, 99
which results in different variance statistics. So we normalize the head outputs separately. 100
The pseudocode of retention is summarized in Figure 2. 101
Retention Score Normalization We utilize the scale-invariant nature of GroupNorm to improve the 102
numerical precision of retention layers. Specifically, multiplying a scalar value within GroupNorm 103
does not affect outputs and backward gradients, i.e., GroupNorm( α∗head i) = GroupNorm(head i). 104
We implement three normalization factors in Equation (5). First, we normalize QK⊺asQK⊺/√
d. 105
Second, we replace Dwith ˜Dnm=Dnm/√Pn
i=1Dni. Third, let Rdenote the retention scores 106
R=QK⊺⊙D, we normalize it as ˜Rnm=Rnm/max(Pn
i=1|Rni|,1). Then the retention output 107
becomes Retention( X) =˜RV. The above tricks do not affect the final results while stabilizing the 108
numerical flow of both forward and backward passes, because of the scale-invariant property. 109
2.3 Overall Architecture of Retention Networks 110
For an L-layer retention network, we stack multi-scale retention (MSR) and feed-forward network 111
(FFN) to build the model. Formally, the input sequence {xi}|x|
i=1is transformed into vectors by a 112
word embedding layer. We use the packed embeddings X0= [x1,···,x|x|]∈R|x|×dmodelas the 113
input and compute the model output XL: 114
Yl= MSR(LN( Xl)) +Xl
Xl+1= FFN(LN( Yl)) +Yl(9)
where LN(·)is LayerNorm [ 3]. The FFN part is computed as FFN( X) = gelu( XW 1)W2, where 115
W1, W2are parameter matrices. 116
4Training We use the parallel (Equation (5)) and chunkwise recurrent (Equation (7)) representations 117
during the training process. The parallelization within sequences or chunks efficiently utilizes 118
GPUs to accelerate computation. More favorably, chunkwise recurrence is especially useful for 119
long-sequence training, which is efficient in terms of both FLOPs and memory consumption. 120
Inference The recurrent representation (Equation (6)) is employed during inference, which nicely 121
fits autoregressive decoding. The O(1)complexity reduces memory and inference latency while 122
achieving equivalent results. 123
3 Experiments 124
We perform language modeling experiments to evaluate RetNet. First, we present the scaling curves 125
of Transformer and RetNet. Second, we follow the training settings of StableLM-4E1T [ 50] to 126
compare with open-source Transformer models in downstream benchmarks. Moreover, for training 127
and inference, we compare speed, memory consumption, and latency. The training corpus is a curated 128
compilation of The Pile [16], C4 [14], and The Stack [29]. 129
3.1 Comparison with Transformer Variants 130
We compare RetNet with various efficient Transformer variants, including RWKV [ 36], H3 [ 11], 131
Hyena [ 38], and Mamba [ 19]. We use LLaMA [ 48] architecture, including RMSNorm [ 59] and 132
SwiGLU [ 40,7] module, as the Transformer backbone, which shows better performance and stability. 133
Consequently, other variants follow these settings. Specifically, Mamba does not have FFN layers so 134
we only implement RMSNorm. For RetNet, the FFN intermediate dimension is5
3dand the value 135
dimensions in WG, WV, WOare also5
3d, where the overall parameters are still 12d2. All models 136
have 400M parameters with 24 layers and a hidden dimension of 1024. For H3, we set the head 137
dimension to 8. For RWKV , we use the TimeMix module to substitute self-attention layers while 138
keeping FFN layers consistent with other models for fair comparisons. We train the models with 40k 139
steps with a batch size of 0.25M tokens. 140
Fine-Grained Language Modeling Evaluation As shown in Table 1, we first report the language 141
modeling perplexity of validation sets. Besides the overall validation set, following [ 2], we divide 142
perplexity into “AR-Hit” and “First Occur”. Specifically, AR-Hit contains the predicted tokens that 143
are previously seen bigrams in the previous context, which evaluates the associative recall ability. 144
“First Occur” has the predicted tokens that can not be recalled from the context. Among various 145
Transformer variants, RetNet outperforms previous methods on both “AR-Hit” and “First Occur” 146
splits, which is important for real-world use cases. 147
Knowledge-Intensive Tasks We also evaluate Massive Multitask Language Understanding 148
(MMLU; [ 24]) answer perplexity to evaluate models on knowledge-intensive tasks. We report 149
the average perplexity of the correct answers, i.e., given input [Question, “Answer:”, Correct 150
Answer] , we calculate the perplexity of the “ Correct Answer ” part. RetNet achieves competitive 151
results among the architectures. 152
Language Modeling MMLU
Valid. Set AR-Hit First-Occur STEMs Humanites Social-Sci. Others Avg
Transformer [51] 3.320 1.118 3.826 0.584 0.229 0.279 0.402 0.356
Transformer Variants
Hyena [38] 3.545 1.799 3.947 1.125 0.576 0.654 0.819 0.767
RWKV [36] 3.497 1.706 3.910 1.156 0.609 0.617 0.781 0.768
Mamba [19] 3.379 1.322 3.852 0.668 0.288 0.300 0.425 0.403
H3 [11] 3.563 1.722 3.986 1.169 0.532 0.637 0.792 0.752
RetNet 3.360 1.264 3.843 0.577 0.263 0.280 0.384 0.362
Table 1: Perplexity results on language modeling and MMLU [ 24] answers. We use the augmented
Transformer architecture proposed in LLaMA [ 48] for reference. For language modeling, we
report perplexity on both the overall validation set and fine-grained diagnosis sets [ 2], i.e., “AR-Hit”
evaluates the associative recall capability, and “First-Occur” indicates the regular language modeling
performance. Besides, we evaluate the answer perplexity of MMLU subsets.
53.2 Language Modeling Evaluation with Various Model Sizes 153
1.3B 2.7B 6.7B
Model Size681012141618Validation PPLTransformer
RetNet
Figure 3: Validation perplexity (PPL) de-
creases along with scaling up the model
size.We train language models with various sizes (i.e., 1.3B, 154
2.7B, and 6.7B) from scratch. The training batch size 155
is 4M tokens with 2048 maximal length. We train the 156
models with 25k steps. The detailed hyper-parameters are 157
described in Appendix E. We train the models with 512 158
AMD MI200 GPUs. 159
Figure 3 reports perplexity on the validation set for the 160
language models based on Transformer and RetNet. We 161
present the scaling curves with three model sizes, i.e., 162
1.3B, 2.7B, and 6.7B. RetNet achieves comparable results 163
with Transformers. More importantly, the results indicate 164
that RetNet is favorable in terms of size scaling. In addi- 165
tion to performance, RetNet training is quite stable in our 166
experiments. Experimental results show that RetNet is a 167
strong competitor to Transformer for large language mod- 168
els. Empirically, we find that RetNet starts to outperform 169
Transformer when the model size is larger than 2B. 170
3.3 Long-Context Evaluation 171
We evaluate long-context modeling on the ZeroSCROLLS [ 41] benchmark. We train a hybrid model 172
of size 2.7B, RetNet+, which stacks the attention and retention layers. Specifically, we insert one 173
attention layer after every 3 retention layers. We follow most configurations of the 2.7B model as in 174
Section 3.2. We scale the number of training tokens to 420B tokens. The batch size is 4M tokens. 175
We first train the model with 4K length and then extend the sequence length to 16K for the last 50B 176
training tokens. The rotation base scaling [55] is used for length extension. 177
Figure 4 reports the answer perplexity given various lengths of input document. It shows that both 178
Transformer and RetNet+ perform better with longer input documents. The results indicate that the 179
language models successfully utilize the long-distance context. Notice that the 12K and 16K results 180
in Qasper are similar because the lengths of most documents are shorter than 16K. Moreover, RetNet+ 181
obtains competitive results compared with Transformer for long-context modeling. Meanwhile, 182
retention has better training and inference efficiency. 183
4096 8192 12288 16384
Context Length1.51.61.7Answer PPL
GovReport
4096 8192 12288 16384
Context Length1.71.81.9Answer PPL
Qasper
4096 8192 12288 16384
Context Length3.253.503.754.004.25Answer PPL
NarrativeQA
Transformer RetNet+
Figure 4: Answer perplexity decreases along with longer input documents. Transformer and RetNet+
obtain comparable performance for long-context modeling on the ZeroSCROLLS [41] benchmark.
3.4 Inference Cost 184
As shown in Figure 5, we compare memory cost, throughput, and latency of Transformer and RetNet 185
during inference. Transformers reuse KV caches of previously decoded tokens. RetNet uses the 186
recurrent representation as described in Equation (6). We evaluate the 6.7B model on the A100-80GB 187
GPU. Figure 5 shows that RetNet outperforms Transformer in terms of inference cost. 188
Memory As shown in Figure 5a, the memory cost of Transformer increases linearly due to KV 189
caches. In contrast, the memory consumption of RetNet remains consistent even for long sequences, 190
62048 3072 4096 5120 6144 7168 8192
Sequence Length203040GPU Memory (GB)
Model Weights
RetNet
Transformer(a) GPU memory cost with varying
sequence length.
2048 3072 4096 5120 6144 7168 8192
Sequence Length50100150200250300Throughput (wps)
RetNet
Transformer(b) Inference throughput with vary-
ing sequence length.
2 4 6 8
Batch Size100200300Latency (ms)
Transformer (1024)
Transformer (2048)
Transformer (4096)
Transformer (8192)
RetNet (8192)(c) Inference latency with different
batch sizes.
Figure 5: Inference cost of Transformer and RetNet with a model size of 6.7B. RetNet outperforms
Transformers in terms of memory consumption, throughput, and latency.
requiring much less GPU memory to host RetNet. The additional memory consumption of RetNet is 191
almost negligible (i.e., about 3%) while the model weights occupy 97%. 192
Throughput As presented in Figure 5b, the throughput of Transformer drops along with the 193
decoding length increases. In comparison, RetNet has higher and length-invariant throughput during 194
decoding, by utilizing the recurrent representation of retention. 195
Latency Latency is an important metric in deployment that greatly affects the user experience. 196
We report the decoding latency in Figure 5c. Experimental results show that increasing batch size 197
renders the Transformer’s latency larger. Moreover, the latency of Transformers grows faster with 198
longer input. In order to make latency acceptable, we have to restrict the batch size, which harms the 199
overall inference throughput of Transformers. By contrast, RetNet’s decoding latency outperforms 200
Transformers and stays almost the same across different batch sizes and input lengths. 201
3.5 Training Throughput 202
8192 16384 32768 65536
Sequence Length400060008000100001200014000Throughput (wps)
Transformer (FlashAttn-2)
RetNet (Triton)
RetNet (PyT orch)
Figure 6: Training throughput (word
per second; wps) of Transformer with
FlashAttention-2 [10] and RetNet.Figure 6 compares the training throughput of Trans- 203
former and RetNet, where the training sequence lengths 204
range from 8192 to 65536. The model size is 3.5B, 205
where the hidden dimension is 3072 and the layer size 206
is 28. We use highly optimized FlashAttention-2 [ 10] 207
for Transformers. In comparison, we implement chunk 208
recurrent representation (Equation (7)) using Triton [ 46], 209
where the computation is both memory-friendly and 210
computationally efficient. The chunk size is set to 256. 211
We evaluate the results with eight Nvidia H100-80GB 212
GPUs because FlashAttention-2 is highly optimized for 213
H100 cards. 214
Experimental results show that RetNet has higher train- 215
ing throughput than Transformers. The acceleration ratio increases as the sequence length is longer. 216
When the training length is 64k, RetNet’s throughput is about 3 times than Transformer’s. 217
3.6 Zero-Shot and Few-Shot Evaluation on Downstream Tasks 218
We also compare the language models on a wide range of downstream tasks. We evaluate zero-shot 219
and 4-shot learning with the 6.7B models. As shown in Table 2, the datasets include HellaSwag 220
(HS; [ 57]), BoolQ [ 8], COPA [ 52], PIQA [ 6], Winograd, Winogrande [ 30], and StoryCloze (SC; [ 34]). 221
The accuracy numbers are consistent with language modeling perplexity presented in Figure 3. RetNet 222
achieves comparable performance with Transformer on zero-shot and in-context learning settings. 223
3.7 Ablation Studies 224
We ablate various design choices of RetNet and report the language modeling results in Table 3. The 225
evaluation settings and metrics are the same as in Section 3.1. 226
7HS BoolQ COPA PIQA Winograd Winogrande SC Avg
Zero-Shot Performance
Transformer 55.9 62.0 69.0 74.6 69.5 56.5 75.0 66.07
RetNet 60.7 62.2 77.0 75.4 77.2 58.1 76.0 69.51
Few-shot Performance (4-Shot)
Transformer 55.8 58.7 71.0 75.0 71.9 57.3 75.4 66.44
RetNet 60.5 60.1 78.0 76.0 77.9 59.9 75.9 69.76
Table 2: Zero-shot and few-shot learning performance. The language model size is 6.7B.
Architecture We ablate the swish gate and GroupNorm as described in Equation (8). Table 3 227
shows that the above two components improve performance. First, the gating module is essential 228
for enhancing non-linearity and improving model capability. Notice that we use the same parameter 229
allocation as in Transformers after removing the gate. Second, group normalization in retention 230
balances the variances of multi-head outputs, which improves training stability and language modeling 231
results. 232
Multi-Scale Decay Equation (8) shows that we use different γas the decay rates for the retention 233
heads. In the ablation studies, we examine removing γdecay (i.e., “ −γdecay”) and applying the 234
same decay rate across heads (i.e., “ −multi-scale decay”). Specifically, ablating γdecay is equivalent 235
toγ= 1. In the second setting, we set γ= 1−2−6.5for all heads. Table 3 indicates that both the 236
decay mechanism and using multiple decay rates can improve the language modeling performance. 237
Head Dimension As indicated by the recurrent perspective of Equation (1), the head dimension 238
implies the memory capacity of hidden states. In ablation, we reduce the default head dimension from 239
256to64, i.e.,64for queries and keys, and ⌊5
3×64⌋ ≈108for values. We keep the hidden dimension 240
dmodel the same. Accordingly, we adjust the multi-scale decay as γ= 1−2−5−arange (0,h)/4to keep 241
the same decay range. Table 3 shows that the larger head dimension achieves better performance. 242
Language Modeling MMLU
Valid. Set AR-Hit First-Occur STEMs Humanites Social-Sci. Others Avg
RetNet 3.360 1.264 3.843 0.577 0.263 0.280 0.384 0.362
−swish gate 3.509 1.366 4.002 0.599 0.285 0.315 0.421 0.390
−GroupNorm 3.367 1.302 3.843 0.630 0.295 0.327 0.438 0.406
−γdecay 3.920 2.122 4.334 0.958 0.566 0.571 0.694 0.681
−multi-scale decay 3.524 1.768 3.928 0.921 0.433 0.471 0.590 0.582
Reduce head dim. 3.397 1.331 3.872 0.637 0.272 0.294 0.393 0.384
Table 3: Perplexity results on language modeling and MMLU [ 24] answers. For language modeling,
we report perplexity on both the overall validation set and fine-grained diagnosis sets [ 2], i.e., “AR-Hit”
evaluates the associative recall capability, and “First-Occur” indicates the regular language modeling
performance. Besides, we evaluate the answer perplexity of the MMLU subsets.
3.8 Results on Vision Tasks 243
We also compare RetNet with vision Transformers [ 15,47] in Table 4, where bidirectional en- 244
coders are evaluated. Unlike causal language models, the vision encoders do not require recurrent 245
representations. Specifically, we use retention as follows: 246
Q= (XW Q)⊙Θ, K = (XW K)⊙Θ, V =XW V
Retention( X) = (QK⊺)V=Q(K⊺V)
where multi-scale decay is removed in bidirectional computation. Notice that we can compute 247
retention in different orders. Similar to linear attention [ 27], the Q(K⊺V)paradigm is an efficient 248
operator in bidirectional settings, especially for high-resolution images. 249
We perform experiments on ImageNet-1K classification [ 13], COCO object detection [ 32], and 250
ADE20K semantic segmentation [ 60]. We compare RetNet with DeiT [ 47] which is a well-tuned 251
vision Transformer. Besides, we follow [ 21] and plug in a depth-wise convolution in experiments. 252
We adopt the DeiT-M size, which has about 38M parameters. For ImageNet-1K image classification, 253
8ImageNet COCO ADE20K
Acc APbAPb
50 APb
75 mIoU mAcc
DeiT [47] 80.76 0.458 0.678 0.502 43.52 55.08
RetNet 81.57 0.457 0.669 0.488 44.13 56.12
Table 4: Results on vision tasks, i.e., image classification (ImageNet), object detection (COCO), and
semantic segmentation (ADE20K). RetNet achieves competitive performance with DeiT, which is a
well-tuned vision Transformer.
we use AdamW [ 33] for 300 epochs, and 20 epochs of linear warm-up. The learning rate is 1×10−3, 254
the batch size is 1024, and the weight decay is 0.05. For COCO object detection, we use Mask 255
R-CNN [ 22] as the task head, and the above models pre-trained on ImageNet as the backbone with 256
3x schedules. In ADE20K experiments, we use UperNet [ 54] as the segmentation head. The detailed 257
configuration can be found in Appendix H. 258
Table 4 shows the results across various vision tasks. RetNet is competitive compared with DeiT. 259
For classification and segmentation, RetNet is slightly better than DeiT, where RetNet achieves 260
0.81% accuracy improvement on ImageNet and 0.61% mIoU improvement on ADE20K. For object 261
detection, the results are comparable. 262
4 Related Work 263
Numerous efforts are focused on reducing the quadratic complexity of attention mechanisms. Linear 264
attention [ 27] uses various kernels ϕ(qi)ϕ(kj)/P|x|
n=1ϕ(qi)ϕ(kn)to replace the softmax function. In 265
contrast, we reexamine sequence modeling from scratch, rather than aiming at approximating 266
softmax . AFT [ 58] simplifies dot-product attention to element-wise and moves softmax to key 267
vectors. RWKV [ 36] replaces AFT’s position embeddings with exponential decay and runs the 268
models recurrently for training and inference. In comparison, retention preserves high-dimensional 269
states to encode sequence information, which contributes to expressive ability and better performance. 270
S4 [20] unifies convolution and recurrence format and achieves O(NlogN)training complexity 271
leveraging the FFT kernel. Unlike Equation (2), if QnandKnare content-unaware, the formulation 272
can be degenerated to S4 [ 20]. Hyena [ 38] generates the convolution kernels, achieving sub-quadratic 273
training efficiency but keeping O(N)complexity in single-step inference. Recently, most related 274
work has focused on modifying γin Equation (6) as a data-dependent variable, such as Mamba [ 19], 275
GLA [ 56], Gateloop [ 28], and xLSTM [ 4]. Another strand explores hybrid architectures [ 31,12] that 276
interleave the above components with attention layers. 277
In addition, we discuss the training and inference efficiency of some related methods. Let Ddenote 278
the hidden dimension, Hthe head dimension, and Nthe sequence length. For training, RWKV’s 279
token-mixing complexity is O(DN), and Mamba’s complexity is O(DHN )with optimized CUDA 280
kernels. Hyena’s is O(DNlogN)with Fast Fourier Transform acceleration. In comparison, the 281
chunk-wise recurrent representation is O(DN(B+H)), where Bis the chunk size, and we usually 282
setH= 256 , B≤512. However, chunk-wise computation is highly parallelized, enabling efficient 283
hardware usage. For large model size (i.e., larger D) or sequence length, the additional b+hhas 284
negligible effects. For inference, among the efficient architectures compared, Hyena has the same 285
complexity (i.e., O(N)per step) as Transformer, while the others can perform O(1)decoding. 286
5 Conclusion 287
We propose retentive networks (RetNet) for sequence modeling, which enables various representa- 288
tions, i.e., parallel, recurrent, and chunkwise recurrent. RetNet achieves significantly better inference 289
efficiency (in terms of memory, speed, and latency), favorable training parallelization, and competitive 290
performance compared with Transformers. The above advantages make RetNet an ideal successor to 291
Transformers for large language models, especially considering the deployment benefits brought by 292
theO(1)inference complexity. In the future, we are interested in deploying RetNet on various edge 293
devices, such as mobile phones. 294
9References 295
[1]J. Ainslie, J. Lee-Thorp, M. de Jong, Y . Zemlyanskiy, F. Lebrón, and S. Sanghai. GQA: Training 296
generalized multi-query Transformer models from multi-head checkpoints. arXiv preprint 297
arXiv:2305.13245 , 2023. 298
[2]S. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. Ré. Zoology: 299
Measuring and improving recall in efficient language models. arXiv preprint arXiv:2312.04927 , 300
2023. 301
[3]J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 302
2016. 303
[4]M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brand- 304
stetter, and S. Hochreiter. xLSTM: Extended long short-term memory. arXiv preprint 305
arXiv:2405.04517 , 2024. 306
[5]J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on Freebase from question- 307
answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural 308
Language Processing , pages 1533–1544, Seattle, Washington, USA, Oct. 2013. Association for 309
Computational Linguistics. 310
[6]Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi. Piqa: Reasoning about physical com- 311
monsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence , 312
2020. 313
[7]A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. 314
Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. B. Rao, 315
P. Barnes, Y . Tay, N. M. Shazeer, V . Prabhakaran, E. Reif, N. Du, B. C. Hutchinson, R. Pope, 316
J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, 317
S. Dev, H. Michalewski, X. García, V . Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, 318
D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, 319
A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. O. Moreira, R. Child, O. Polozov, K. Lee, 320
Z. Zhou, X. Wang, B. Saeta, M. Díaz, O. Firat, M. Catasta, J. Wei, K. S. Meier-Hellstern, 321
D. Eck, J. Dean, S. Petrov, and N. Fiedel. PaLM: Scaling language modeling with pathways. 322
ArXiv , abs/2204.02311, 2022. 323
[8]C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. BoolQ: 324
Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 325
Conference of the North American Chapter of the Association for Computational Linguistics , 326
pages 2924–2936, 2019. 327
[9]T. Computer. Redpajama-data: An open source recipe to reproduce llama training dataset, 2023. 328
URLhttps://github.com/togethercomputer/RedPajama-Data . 329
[10] T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. arXiv 330
preprint arXiv:2307.08691 , 2023. 331
[11] T. Dao, D. Y . Fu, K. K. Saab, A. W. Thomas, A. Rudra, and C. Ré. Hungry hungry hippos: 332
Towards language modeling with state space models. arXiv preprint arXiv:2212.14052 , 2022. 333
[12] S. De, S. L. Smith, A. Fernando, A. Botev, G. Cristian-Muraru, A. Gu, R. Haroun, L. Berrada, 334
Y . Chen, S. Srinivasan, G. Desjardins, A. Doucet, D. Budden, Y . W. Teh, R. Pascanu, N. D. 335
Freitas, and C. Gulcehre. Griffin: Mixing gated linear recurrences with local attention for 336
efficient language models. 2024. 337
[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical 338
image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 339
248–255. Ieee, 2009. 340
[14] J. Dodge, A. Marasovi ´c, G. Ilharco, D. Groeneveld, M. Mitchell, and M. Gardner. Documenting 341
large webtext corpora: A case study on the colossal clean crawled corpus. In Conference on 342
Empirical Methods in Natural Language Processing , 2021. 343
10[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, 344
M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for 345
image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020. 346
[16] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, 347
N. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv 348
preprint arXiv:2101.00027 , 2020. 349
[17] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, 350
A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, 351
H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A 352
framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/ 353
records/10256836 . 354
[18] X. Geng and H. Liu. Openllama: An open reproduction of llama, May 2023. URL https: 355
//github.com/openlm-research/open_llama . 356
[19] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv 357
preprint arXiv:2312.00752 , 2023. 358
[20] A. Gu, K. Goel, and C. Ré. Efficiently modeling long sequences with structured state spaces. 359
arXiv preprint arXiv:2111.00396 , 2021. 360
[21] D. Han, X. Pan, Y . Han, S. Song, and G. Huang. Flatten Transformer: Vision Transformer 361
using focused linear attention. In Proceedings of the IEEE/CVF International Conference on 362
Computer Vision , pages 5961–5971, 2023. 363
[22] K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE 364
international conference on computer vision , pages 2961–2969, 2017. 365
[23] D. Hendrycks and K. Gimpel. Gaussian error linear units (GELUs). arXiv: Learning , 2016. 366
[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring 367
massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020. 368
[25] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation , 9:1735–1780, 369
Nov. 1997. 370
[26] W. Hua, Z. Dai, H. Liu, and Q. Le. Transformer quality in linear time. In International 371
Conference on Machine Learning , pages 9099–9117. PMLR, 2022. 372
[27] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive 373
transformers with linear attention. In International Conference on Machine Learning , pages 374
5156–5165. PMLR, 2020. 375
[28] T. Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv 376
preprint arXiv:2311.01927 , 2023. 377
[29] D. Kocetkov, R. Li, L. Ben Allal, J. Li, C. Mou, C. Muñoz Ferrandis, Y . Jernite, M. Mitchell, 378
S. Hughes, T. Wolf, D. Bahdanau, L. von Werra, and H. de Vries. The Stack: 3TB of permissively 379
licensed source code. Preprint , 2022. 380
[30] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In Thirteenth 381
International Conference on the Principles of Knowledge Representation and Reasoning , 2012. 382
[31] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meirom, Y . Belinkov, 383
S. Shalev-Shwartz, et al. Jamba: A hybrid Transformer-Mamba language model. arXiv preprint 384
arXiv:2403.19887 , 2024. 385
[32] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. 386
Microsoft COCO: Common objects in context. In Computer Vision–ECCV 2014: 13th European 387
Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 , pages 740–755. 388
Springer, 2014. 389
11[33] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference 390
on Learning Representations , 2019. 391
[34] N. Mostafazadeh, M. Roth, A. Louis, N. Chambers, and J. Allen. Lsdsem 2017 shared task: The 392
story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential 393
and Discourse-level Semantics , pages 46–51, 2017. 394
[35] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting 395
recurrent neural networks for long sequences. ArXiv , abs/2303.06349, 2023. 396
[36] B. Peng, E. Alcaide, Q. G. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, 397
M. Grella, G. Kranthikiran, X. He, H. Hou, et al. RWKV: Reinventing RNNs for the Transformer 398
era.ArXiv , abs/2305.13048, 2023. 399
[37] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong. Random feature 400
attention. arXiv preprint arXiv:2103.02143 , 2021. 401
[38] M. Poli, S. Massaroli, E. Nguyen, D. Y . Fu, T. Dao, S. Baccus, Y . Bengio, S. Ermon, and 402
C. Ré. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint 403
arXiv:2302.10866 , 2023. 404
[39] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ questions for machine 405
comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in 406
Natural Language Processing , pages 2383–2392, Austin, Texas, Nov. 2016. Association for 407
Computational Linguistics. doi: 10.18653/v1/D16-1264. 408
[40] P. Ramachandran, B. Zoph, and Q. V . Le. Swish: a self-gated activation function. arXiv: Neural 409
and Evolutionary Computing , 2017. 410
[41] U. Shaham, M. Ivgi, A. Efrat, J. Berant, and O. Levy. ZeroSCROLLS: A zero-shot benchmark 411
for long text understanding. arXiv preprint arXiv:2305.14196 , 2023. 412
[42] N. M. Shazeer. Fast Transformer decoding: One write-head is all you need. ArXiv , 413
abs/1911.02150, 2019. 414
[43] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-LM: 415
Training multi-billion parameter language models using model parallelism. arXiv preprint 416
arXiv:1909.08053 , 2019. 417
[44] J. Su, Y . Lu, S. Pan, B. Wen, and Y . Liu. Roformer: Enhanced transformer with rotary position 418
embedding. arXiv preprint arXiv:2104.09864 , 2021. 419
[45] Y . Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V . Chaudhary, X. Song, and F. Wei. A 420
length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association 421
for Computational Linguistics (Volume 1: Long Papers) , pages 14590–14604, Toronto, Canada, 422
July 2023. Association for Computational Linguistics. 423
[46] P. Tillet and D. Cox. Triton: An intermediate language and compiler for tiled neural network 424
computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine 425
Learning and Programming Languages , pages 10–19, 2019. 426
[47] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou. Training data-efficient 427
image transformers & distillation through attention. In International conference on machine 428
learning , pages 10347–10357. PMLR, 2021. 429
[48] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, 430
E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv 431
preprint arXiv:2302.13971 , 2023. 432
[49] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, 433
P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv 434
preprint arXiv:2307.09288 , 2023. 435
[50] J. Tow, M. Bellagente, D. Mahan, and C. Riquelme. StableLM 3B 4E1T. https://aka.ms/ 436
StableLM-3B-4E1T , 2023. 437
12[51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and 438
I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 439
30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, 440
Long Beach, CA, USA , pages 6000–6010, 2017. 441
[52] A. Wang, Y . Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. 442
Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding 443
systems. arXiv preprint arXiv:1905.00537 , 2019. 444
[53] Y . Wu and K. He. Group normalization. In Proceedings of the European conference on computer 445
vision (ECCV) , pages 3–19, 2018. 446
[54] T. Xiao, Y . Liu, B. Zhou, Y . Jiang, and J. Sun. Unified perceptual parsing for scene understanding. 447
InProceedings of the European conference on computer vision (ECCV) , pages 418–434, 2018. 448
[55] W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. 449
Sankararaman, B. Oguz, et al. Effective long-context scaling of foundation models. arXiv 450
preprint arXiv:2309.16039 , 2023. 451
[56] S. Yang, B. Wang, Y . Shen, R. Panda, and Y . Kim. Gated linear attention transformers with 452
hardware-efficient training. arXiv preprint arXiv:2312.06635 , 2023. 453
[57] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi. Hellaswag: Can a machine really 454
finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for 455
Computational Linguistics , 2019. 456
[58] S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind. An attention 457
free transformer. arXiv preprint arXiv:2105.14103 , 2021. 458
[59] B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural 459
Information Processing Systems , 32, 2019. 460
[60] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba. Semantic 461
understanding of scenes through the ADE20k dataset. International Journal of Computer Vision , 462
127:302–321, 2019. 463
13A Scaling Up Number of Training Tokens 464
We scale up the number of training tokens to 350B for the 3B-size models. We compare with strong 465
Transformer checkpoints including OpenLLaMA [ 18] and StableLM [ 50]. Moreover, we reproduce a 466
Transformer language model (named Transformer Repro) for apple-to-apple comparison. 467
Our model RetNet+ follows the same configuration as in Section 3.3, which is a hybrid model. The 468
model’s hidden size is 3072, and the number of layers is 28. Without vocabulary embedding, the total 469
number of parameters is 3.17B, which is between StableLM-3B-4E1T (2.7B) and OpenLLaMA-3B- 470
v1 (3.19B). The batch size is 4M tokens. The training length is 4k. The learning rate is 3.2×10−4471
with 1000 warm-up steps and linear learning rate decay. The training corpus includes The Pile [ 16] 472
and RedPajama [9]. Transformer Repro follows the exact same setting. 473
Table 5 reports accuracy numbers on the Harness-Eval benchmark [ 17]. We directly follow the evalua- 474
tion protocol. The results show that RetNet+ achieves a performance comparable to Transformer Repro 475
on language tasks. Notice that OpenLLaMA-3B-v1 and StableLM-3B use different learning rate 476
schedules. The results of these two models are used for reference purposes. 477
Model ARC-C ARC-C norm ARC-E ARC-E norm Hellaswag Hellaswag norm
OpenLLaMA-3B-v1 0.303 0.323 0.641 0.599 0.449 0.608
StableLM-3B — — 0.649 0.610 — —
Transformer Repro 0.322 0.354 0.668 0.633 0.476 0.633
RetNet+ 0.321 0.347 0.675 0.613 0.478 0.639
Model OBQA OBQA norm PIQA PIQA norm Winogrande Avg
OpenLLaMA-3B-v1 0.222 0.348 0.713 0.724 0.594 0.502
StableLM-3B — — 0.759 0.763 0.608 —
Transformer Repro 0.258 0.358 0.746 0.755 0.612 0.529
RetNet+ 0.258 0.362 0.750 0.763 0.614 0.529
Table 5: Accuracy on the Harness-Eval benchmark. All models are trained with 350B tokens with a
batch size of 4M tokens. The results of OpenLLaMA-3B-v1 are taken from their official repository
(https://bit.ly/openllama-350b-results ), and StableLM-3B from their technical report
(https://bit.ly/StableLM-3B-4E1T ).
B Equivalence Between Chunk-wise Recurrent Representation and 478
Recurrent Representation 479
We illustrate the equivalence between the recurrent representation and the chunk-wise recurrent 480
representation. Specifically, let Bdenote the chunk length. For the output On,ncan be divided as 481
n=kB+rwhere Bis the chunk size. Following Equation 6, we have: 482
On=nX
m=1γn−mQnK⊺
mVm
= (QnK⊺
kB+1:n⊙Γ)VkB+1:n+ (Qnγr)k−1X
c=0BX
m=1(K⊺
m+cBVm+cBγB−m)γ(k−1−c)B
= (QnK⊺
kB+1:n⊙Γ)VkB+1:n+ (Qnγr)kX
c=1(K⊺
[c](V[c]⊙ζ))γ(k−c)B
= (QnK⊺
kB+1:n⊙Γ)VkB+1:n+ (Qnγr)Ri−1(10)
where Γi=γn−i,ζij=γB−m, and [i]indicates the i-th chunk, i.e., x[i]= [x(i−1)B+1,···, xiB]. 483
Then we write Rnas a recurrent function and compute the retention output of the i-th chunk via: 484
Ri=K⊺
[i](V[i]⊙ζ) +γBRi−1
ζij=γB−i, ξ ij=γi
Retention( X[i]) = (Q[i]K⊺
[i]⊙D)V[i]| {z }
Inner-Chunk+ (Q[i]⊙ξ)Ri−1|{z}
Cross-Chunk(11)
14Finally, we show that the chunkwise recurrent representation is equivalent to the other representations. 485
C Results with Different Context Lengths 486
As shown in Table 6, we report the results of language modeling with different context lengths. In 487
order to make the numbers comparable, we use 2048 text chunks as evaluation data and only compute 488
the perplexity for the last 128 tokens. Experimental results show that RetNet performs comparably 489
with Transformer in different context lengths. 490
Model 512 1024 2048
Transformer 13.55 12.56 12.35
RetNet 13.09 12.14 11.98
Table 6: Language modeling perplexity of RetNet and Transformer with different context length. The
results show that RetNet has a consistent advantage across sequence length.
D Hyperparameters Used in Section 3.1 491
We use LLaMA [ 48] architecture, including RMSNorm [ 59] and SwiGLU [ 40,7] module, as 492
the Transformer backbone, which shows better performance and stability. The weights of word 493
embedding and softmax projection are shared. Consequently, other variants follow these settings. 494
For RetNet, the FFN intermediate dimension is5
3dand the value dimensions in WG, WV, WOare 495
also5
3d, where the overall parameters are still 12d2. 496
For H3, we set the head dimension to 8. For RWKV , we use the TimeMix module to substitute 497
self-attention layers while keeping FFN layers consistent with other models for fair comparisons. 498
For Mamba, we follow all the details in the paper [ 19], where double-SSM layers are implemented 499
instead of “SSM + SwiGLU”. In addition to RetNet and Mamba, the FFN intermediate dimension is 500
all8
3d. All models have 400M parameters, 24 layers, and a hidden dimension of 1024. We train the 501
models with 40k steps and a batch size of 0.25M tokens. 502
Params Values
Layers 24
Hidden size 1024
V ocab size 100,288
Heads 24
Adam β (0.9, 0.98)
LR 1.5×10−4
Batch size 0.25M
Warmup steps 375
Weight decay 0.05
Dropout 0.0
Table 7: Hyperparamters used for the architecture comparison in Section 3.1.
E Hyperparameters Used in Section 3.2 503
We re-allocate the parameters in MSR and FFN for fair comparisons. Let ddenote dmodel for simplicity 504
here. In Transformers, there are about 4d2parameters in self-attention where WQ, WK, WV, WO∈ 505
Rd×d, and 8d2parameters in FFN where the intermediate dimension is 4d. In comparison, RetNet 506
has8d2parameters in retention, where WQ, WK∈Rd×d, WG, WV∈Rd×2d, WO∈R2d×d. Notice 507
that the head dimension of Vis twice Q, K , similar to GAU [ 26]. The widened dimension is 508
projected back to dbyWO. In order to keep the parameter number the same as Transformer, the FFN 509
intermediate dimension in RetNet is 2d. Meanwhile, we set the head dimension to 256, i.e., 256for 510
15queries and keys, and 512for values. For fair comparison, we keep γidentical among different model 511
sizes, where γ= 1−elinspace(log 1/32,log1/512,h)∈Rhinstead of the default value in Equation (8). 512
Hyperparameters 1.3B 2.7B 6.7B
Layers 24 32 32
Hidden size 2048 2560 4096
FFN size 4096 5120 8192
Heads 8 10 16
Learning rate 6×10−43×10−43×10−4
LR scheduler Linear decay
Warm-up steps 375
Tokens per batch 4M
Adam β (0.9, 0.98)
Training steps 25,000
Gradient clipping 2.0
Dropout 0.1
Weight decay 0.05
Table 8: Hyperparamters used for language modeling in Section 3.2.
F Results on Open-Ended Generation Tasks 513
Table 9 presents one-shot performance on two open-ended question-answering tasks, including 514
SQUAD [ 39] and WebQS [ 5], with 6.7B models as follows. We report the recall metric in the table, 515
i.e., whether the answers are contained in the generated response. 516
Dataset SQUAD WebQS
Transformer 67.7 36.4
RetNet 72.7 40.4
Table 9: Answer recall of RetNet and Transformer on open-ended question answering.
G Inference Cost of Grouped-Query Retention 517
We compare with grouped-query attention [ 1] and evaluate the method in the context of RetNet. 518
Grouped-query attention makes a trade-off between performance and efficiency, which has been 519
successfully verified in LLaMA2 34B/70B [ 49]. The method reduces the overhead of key/value cache 520
during inference. Moreover, the performance of grouped-query attention is better than multi-query 521
attention [42], overcoming the quality degradation brought by using one-head key value. 522
As shown in Table 10, we compare the inference cost with grouped-query attention and apply the 523
method for RetNet. For the LLaMA2 70B model, the number of key/value heads is reduced by 8 ×, 524
where the query head number is 64 while the key/value head number is 8. For RetNet-70B, the 525
parameter allocation is identical to LLaMA [ 48], where the dimension is 8192, and the head number 526
is 32 for RetNet. For RetNet-70B-GQ2, the key-value head number is 16, where grouped-query 527
retention is applied. We run the inference with four A100 GPUs without quantization. 528
When the batch size is 256, LLaMA2 runs out of memory while RetNet without group query still 529
has a high throughput. When equipped with grouped-query retention, RetNet-70B achieves 38% 530
acceleration and saves 30% memory. 531
We evaluate LLaMA2 under 2k and 8k lengths separately. The batch size is reduced to 8 so that 532
LLaMA2 can run without out of memory. Table 10 shows that the inference cost of Transformers 533
increases with the sequence length. In contrast, RetNet is length-invariant. Moreover, RetNet-70B- 534
GQ2 achieves better latency, throughput, and GPU memory than LLaMA2-70B-2k/8k equipped 535
16with grouped-query attention. Notice that the evaluation metrics are averaged over positions of 536
different sequence lengths for a fair comparison, rather than only considering the inference cost of 537
the maximum length. 538
Model Batch Size Latency (ms) ↓Throughput (wps) ↑Memory (GB) ↓
LLaMA2-70B-2k 256 — — OOM
LLaMA2-70B-8k 256 — — OOM
RetNet-70B 256 639.1 410.19 72.469
RetNet-70B-GQ2 256 461.8 567.66 52.726
LLaMA2-70B-2k 8 184.5 44.42 33.374
LLaMA2-70B-8k 8 277.7 29.50 37.386
RetNet-70B-GQ2 8 106.2 77.02 32.301
Table 10: Inference cost of RetNet and LLaMA2-70B with difference batch size and length. LLaMA2-
70B is equipped with grouped-query attention, reducing key/value heads by 8 ×. “-GQ2” means
grouped-query retention, which reduces half of key/value heads. “-2k” and “-8k” indicate sequence
length for LLaMA2, while RetNet is length-invariant. RetNet is capable of large-batch inference and
is favourable in terms of latency, throughput, and GPU memory.
H Hyperparameters Used in Section 3.8 539
Hyperparameters DeiT RetNet
Layers 12 12
Hidden size 512 512
Patch size 16 16
FFN size 2048 1024
Heads 8 2
Learning rate 1×10−3
LR scheduler Cosine decay
Batch size 1024
Epochs 300
Warmup epochs 5
Smoothing 0.1
Weight decay 0.05
Drop path 0.3
Table 11: Hyperparamters used for the ImageNet experiments in Section 3.8.
17NeurIPS Paper Checklist 540
1.Claims 541
Question: Do the main claims made in the abstract and introduction accurately reflect the 542
paper’s contributions and scope? 543
Answer: [Yes] 544
Justification: The abstract and introduction is carefully written. 545
Guidelines: 546
•The answer NA means that the abstract and introduction do not include the claims 547
made in the paper. 548
•The abstract and/or introduction should clearly state the claims made, including the 549
contributions made in the paper and important assumptions and limitations. A No or 550
NA answer to this question will not be perceived well by the reviewers. 551
•The claims made should match theoretical and experimental results, and reflect how 552
much the results can be expected to generalize to other settings. 553
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 554
are not attained by the paper. 555
2.Limitations 556
Question: Does the paper discuss the limitations of the work performed by the authors? 557
Answer: [Yes] 558
Justification: Limitations are discussed in the paper. 559
Guidelines: 560
•The answer NA means that the paper has no limitation while the answer No means that 561
the paper has limitations, but those are not discussed in the paper. 562
• The authors are encouraged to create a separate "Limitations" section in their paper. 563
•The paper should point out any strong assumptions and how robust the results are to 564
violations of these assumptions (e.g., independence assumptions, noiseless settings, 565
model well-specification, asymptotic approximations only holding locally). The authors 566
should reflect on how these assumptions might be violated in practice and what the 567
implications would be. 568
•The authors should reflect on the scope of the claims made, e.g., if the approach was 569
only tested on a few datasets or with a few runs. In general, empirical results often 570
depend on implicit assumptions, which should be articulated. 571
•The authors should reflect on the factors that influence the performance of the approach. 572
For example, a facial recognition algorithm may perform poorly when image resolution 573
is low or images are taken in low lighting. Or a speech-to-text system might not be 574
used reliably to provide closed captions for online lectures because it fails to handle 575
technical jargon. 576
•The authors should discuss the computational efficiency of the proposed algorithms 577
and how they scale with dataset size. 578
•If applicable, the authors should discuss possible limitations of their approach to 579
address problems of privacy and fairness. 580
•While the authors might fear that complete honesty about limitations might be used by 581
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 582
limitations that aren’t acknowledged in the paper. The authors should use their best 583
judgment and recognize that individual actions in favor of transparency play an impor- 584
tant role in developing norms that preserve the integrity of the community. Reviewers 585
will be specifically instructed to not penalize honesty concerning limitations. 586
3.Theory Assumptions and Proofs 587
Question: For each theoretical result, does the paper provide the full set of assumptions and 588
a complete (and correct) proof? 589
Answer: [NA] 590
18Justification: There is no theoretical result in this paper. 591
Guidelines: 592
• The answer NA means that the paper does not include theoretical results. 593
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 594
referenced. 595
•All assumptions should be clearly stated or referenced in the statement of any theorems. 596
•The proofs can either appear in the main paper or the supplemental material, but if 597
they appear in the supplemental material, the authors are encouraged to provide a short 598
proof sketch to provide intuition. 599
•Inversely, any informal proof provided in the core of the paper should be complemented 600
by formal proofs provided in appendix or supplemental material. 601
• Theorems and Lemmas that the proof relies upon should be properly referenced. 602
4.Experimental Result Reproducibility 603
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 604
perimental results of the paper to the extent that it affects the main claims and/or conclusions 605
of the paper (regardless of whether the code and data are provided or not)? 606
Answer: [Yes] 607
Justification: The experiment can be easily reproduced based on the model description, 608
hyperparameter, and any well-known pre-training corpus. 609
Guidelines: 610
• The answer NA means that the paper does not include experiments. 611
•If the paper includes experiments, a No answer to this question will not be perceived 612
well by the reviewers: Making the paper reproducible is important, regardless of 613
whether the code and data are provided or not. 614
•If the contribution is a dataset and/or model, the authors should describe the steps taken 615
to make their results reproducible or verifiable. 616
•Depending on the contribution, reproducibility can be accomplished in various ways. 617
For example, if the contribution is a novel architecture, describing the architecture fully 618
might suffice, or if the contribution is a specific model and empirical evaluation, it may 619
be necessary to either make it possible for others to replicate the model with the same 620
dataset, or provide access to the model. In general. releasing code and data is often 621
one good way to accomplish this, but reproducibility can also be provided via detailed 622
instructions for how to replicate the results, access to a hosted model (e.g., in the case 623
of a large language model), releasing of a model checkpoint, or other means that are 624
appropriate to the research performed. 625
•While NeurIPS does not require releasing code, the conference does require all submis- 626
sions to provide some reasonable avenue for reproducibility, which may depend on the 627
nature of the contribution. For example 628
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 629
to reproduce that algorithm. 630
(b)If the contribution is primarily a new model architecture, the paper should describe 631
the architecture clearly and fully. 632
(c)If the contribution is a new model (e.g., a large language model), then there should 633
either be a way to access this model for reproducing the results or a way to reproduce 634
the model (e.g., with an open-source dataset or instructions for how to construct 635
the dataset). 636
(d)We recognize that reproducibility may be tricky in some cases, in which case 637
authors are welcome to describe the particular way they provide for reproducibility. 638
In the case of closed-source models, it may be that access to the model is limited in 639
some way (e.g., to registered users), but it should be possible for other researchers 640
to have some path to reproducing or verifying the results. 641
5.Open access to data and code 642
Question: Does the paper provide open access to the data and code, with sufficient instruc- 643
tions to faithfully reproduce the main experimental results, as described in supplemental 644
material? 645
19Answer: [Yes] 646
Justification: Code will be released in camera-ready version. All of the data we use is 647
public-available. 648
Guidelines: 649
• The answer NA means that paper does not include experiments requiring code. 650
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 651
public/guides/CodeSubmissionPolicy ) for more details. 652
•While we encourage the release of code and data, we understand that this might not be 653
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 654
including code, unless this is central to the contribution (e.g., for a new open-source 655
benchmark). 656
•The instructions should contain the exact command and environment needed to run to 657
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 658
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 659
•The authors should provide instructions on data access and preparation, including how 660
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 661
•The authors should provide scripts to reproduce all experimental results for the new 662
proposed method and baselines. If only a subset of experiments are reproducible, they 663
should state which ones are omitted from the script and why. 664
•At submission time, to preserve anonymity, the authors should release anonymized 665
versions (if applicable). 666
•Providing as much information as possible in supplemental material (appended to the 667
paper) is recommended, but including URLs to data and code is permitted. 668
6.Experimental Setting/Details 669
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 670
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 671
results? 672
Answer: [Yes] 673
Justification: Hyperparameters are attached in the appendix. 674
Guidelines: 675
• The answer NA means that the paper does not include experiments. 676
•The experimental setting should be presented in the core of the paper to a level of detail 677
that is necessary to appreciate the results and make sense of them. 678
•The full details can be provided either with the code, in appendix, or as supplemental 679
material. 680
7.Experiment Statistical Significance 681
Question: Does the paper report error bars suitably and correctly defined or other appropriate 682
information about the statistical significance of the experiments? 683
Answer: [No] 684
Justification: For large language models, the variance between different runs is negligible. 685
Moreover, the evaluation pipeline is deterministic. 686
Guidelines: 687
• The answer NA means that the paper does not include experiments. 688
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 689
dence intervals, or statistical significance tests, at least for the experiments that support 690
the main claims of the paper. 691
•The factors of variability that the error bars are capturing should be clearly stated (for 692
example, train/test split, initialization, random drawing of some parameter, or overall 693
run with given experimental conditions). 694
•The method for calculating the error bars should be explained (closed form formula, 695
call to a library function, bootstrap, etc.) 696
• The assumptions made should be given (e.g., Normally distributed errors). 697
20•It should be clear whether the error bar is the standard deviation or the standard error 698
of the mean. 699
•It is OK to report 1-sigma error bars, but one should state it. The authors should 700
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 701
of Normality of errors is not verified. 702
•For asymmetric distributions, the authors should be careful not to show in tables or 703
figures symmetric error bars that would yield results that are out of range (e.g. negative 704
error rates). 705
•If error bars are reported in tables or plots, The authors should explain in the text how 706
they were calculated and reference the corresponding figures or tables in the text. 707
8.Experiments Compute Resources 708
Question: For each experiment, does the paper provide sufficient information on the com- 709
puter resources (type of compute workers, memory, time of execution) needed to reproduce 710
the experiments? 711
Answer: [Yes] 712
Justification: The corresponding resources are stated in the paper. 713
Guidelines: 714
• The answer NA means that the paper does not include experiments. 715
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 716
or cloud provider, including relevant memory and storage. 717
•The paper should provide the amount of compute required for each of the individual 718
experimental runs as well as estimate the total compute. 719
•The paper should disclose whether the full research project required more compute 720
than the experiments reported in the paper (e.g., preliminary or failed experiments that 721
didn’t make it into the paper). 722
9.Code Of Ethics 723
Question: Does the research conducted in the paper conform, in every respect, with the 724
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 725
Answer: [Yes] 726
Justification: We follow the NeurIPS Code of Ethics in the research. 727
Guidelines: 728
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 729
•If the authors answer No, they should explain the special circumstances that require a 730
deviation from the Code of Ethics. 731
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 732
eration due to laws or regulations in their jurisdiction). 733
10.Broader Impacts 734
Question: Does the paper discuss both potential positive societal impacts and negative 735
societal impacts of the work performed? 736
Answer: [NA] 737
Justification: We work on fundamental research that has no direct societal impact. 738
Guidelines: 739
• The answer NA means that there is no societal impact of the work performed. 740
•If the authors answer NA or No, they should explain why their work has no societal 741
impact or why the paper does not address societal impact. 742
•Examples of negative societal impacts include potential malicious or unintended uses 743
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 744
(e.g., deployment of technologies that could make decisions that unfairly impact specific 745
groups), privacy considerations, and security considerations. 746
21•The conference expects that many papers will be foundational research and not tied 747
to particular applications, let alone deployments. However, if there is a direct path to 748
any negative applications, the authors should point it out. For example, it is legitimate 749
to point out that an improvement in the quality of generative models could be used to 750
generate deepfakes for disinformation. On the other hand, it is not needed to point out 751
that a generic algorithm for optimizing neural networks could enable people to train 752
models that generate Deepfakes faster. 753
•The authors should consider possible harms that could arise when the technology is 754
being used as intended and functioning correctly, harms that could arise when the 755
technology is being used as intended but gives incorrect results, and harms following 756
from (intentional or unintentional) misuse of the technology. 757
•If there are negative societal impacts, the authors could also discuss possible mitigation 758
strategies (e.g., gated release of models, providing defenses in addition to attacks, 759
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 760
feedback over time, improving the efficiency and accessibility of ML). 761
11.Safeguards 762
Question: Does the paper describe safeguards that have been put in place for responsible 763
release of data or models that have a high risk for misuse (e.g., pretrained language models, 764
image generators, or scraped datasets)? 765
Answer: [NA] 766
Justification: The paper does not pose safety risks. 767
Guidelines: 768
• The answer NA means that the paper poses no such risks. 769
•Released models that have a high risk for misuse or dual-use should be released with 770
necessary safeguards to allow for controlled use of the model, for example by requiring 771
that users adhere to usage guidelines or restrictions to access the model or implementing 772
safety filters. 773
•Datasets that have been scraped from the Internet could pose safety risks. The authors 774
should describe how they avoided releasing unsafe images. 775
•We recognize that providing effective safeguards is challenging, and many papers do 776
not require this, but we encourage authors to take this into account and make a best 777
faith effort. 778
12.Licenses for existing assets 779
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 780
the paper, properly credited and are the license and terms of use explicitly mentioned and 781
properly respected? 782
Answer: [Yes] 783
Justification: We carefully follow the licenses of open-source code, data, and models. 784
Guidelines: 785
• The answer NA means that the paper does not use existing assets. 786
• The authors should cite the original paper that produced the code package or dataset. 787
•The authors should state which version of the asset is used and, if possible, include a 788
URL. 789
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 790
•For scraped data from a particular source (e.g., website), the copyright and terms of 791
service of that source should be provided. 792
•If assets are released, the license, copyright information, and terms of use in the 793
package should be provided. For popular datasets, paperswithcode.com/datasets 794
has curated licenses for some datasets. Their licensing guide can help determine the 795
license of a dataset. 796
•For existing datasets that are re-packaged, both the original license and the license of 797
the derived asset (if it has changed) should be provided. 798
22•If this information is not available online, the authors are encouraged to reach out to 799
the asset’s creators. 800
13.New Assets 801
Question: Are new assets introduced in the paper well documented and is the documentation 802
provided alongside the assets? 803
Answer: [NA] 804
Justification: The paper does not release new assets. 805
Guidelines: 806
• The answer NA means that the paper does not release new assets. 807
•Researchers should communicate the details of the dataset/code/model as part of their 808
submissions via structured templates. This includes details about training, license, 809
limitations, etc. 810
•The paper should discuss whether and how consent was obtained from people whose 811
asset is used. 812
•At submission time, remember to anonymize your assets (if applicable). You can either 813
create an anonymized URL or include an anonymized zip file. 814
14.Crowdsourcing and Research with Human Subjects 815
Question: For crowdsourcing experiments and research with human subjects, does the paper 816
include the full text of instructions given to participants and screenshots, if applicable, as 817
well as details about compensation (if any)? 818
Answer: [NA] 819
Justification: The paper does not involve crowdsourcing nor research with human subjects. 820
Guidelines: 821
•The answer NA means that the paper does not involve crowdsourcing nor research with 822
human subjects. 823
•Including this information in the supplemental material is fine, but if the main contribu- 824
tion of the paper involves human subjects, then as much detail as possible should be 825
included in the main paper. 826
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 827
or other labor should be paid at least the minimum wage in the country of the data 828
collector. 829
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 830
Subjects 831
Question: Does the paper describe potential risks incurred by study participants, whether 832
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 833
approvals (or an equivalent approval/review based on the requirements of your country or 834
institution) were obtained? 835
Answer: [NA] 836
Justification: The paper does not involve crowdsourcing nor research with human subjects. 837
Guidelines: 838
•The answer NA means that the paper does not involve crowdsourcing nor research with 839
human subjects. 840
•Depending on the country in which research is conducted, IRB approval (or equivalent) 841
may be required for any human subjects research. If you obtained IRB approval, you 842
should clearly state this in the paper. 843
•We recognize that the procedures for this may vary significantly between institutions 844
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 845
guidelines for their institution. 846
•For initial submissions, do not include any information that would break anonymity (if 847
applicable), such as the institution conducting the review. 848
23