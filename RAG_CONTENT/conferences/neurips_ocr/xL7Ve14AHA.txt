Regularized Adaptive Momentum Dual
Averaging with an Efficient Inexact Subproblem
Solver for Training Structured Neural Network
Zih-Syuan Huang
Department of Computer Science and Information Engineering
National Taiwan University
Taipei 106, Taiwan
r11922210@ntu.edu.tw
Ching-pei Lee
Department of Advanced Data Science
Institute of Statistical Mathematics
Tachikawa, Tokyo 190-8562, Japan
chingpei@ism.ac.jp
Abstract
We propose a Regularized Adaptive Momentum Dual Averaging ( RAMDA)
algorithmfortrainingstructuredneuralnetworks. Similartoexistingregular-
ized adaptive methods, the subproblem for computing the update direction
ofRAMDA involves a nonsmooth regularizer and a diagonal preconditioner,
and therefore does not possess a closed-form solution in general. We thus
also carefully devise an implementable inexactness condition that retains
convergence guarantees similar to the exact versions, and propose a compan-
ion efficient solver for the subproblems of both RAMDA and existing methods
to make them practically feasible. We leverage the theory of manifold iden-
tification in variational analysis to show that, even in the presence of such
inexactness, the iterates of RAMDA attain the ideal structure induced by the
regularizer at the stationary point of asymptotic convergence. This structure
is locally optimal near the point of convergence, so RAMDA is guaranteed
to obtain the best structure possible among all methods converging to the
same point, making it the first regularized adaptive method outputting
models that possess outstanding predictive performance while being (lo-
cally) optimally structured. Extensive numerical experiments in large-scale
modern computer vision, language modeling, and speech tasks show that
the proposed RAMDA is efficient and consistently outperforms state of the
art for training structured neural network. Implementation of our algorithm
is available at https://www.github.com/ismoptgroup/RAMDA/ .
1 Introduction
Since the recent emergence of ChatGPT, large language models (LLMs) and other huge
deep learning models have garnered much attention and popularity, even among the public
who are unfamiliar with machine learning. A challenge with such gigantic neural network
models is their vast number of model parameters, reaching hundreds of billions, resulting in
expensive storage and inference. It thus becomes crucial to find ways to exploit structures in
38th Conference on Neural Information Processing Systems (NeurIPS 2024).trained models to reduce their spatial and prediction costs without degrading the prediction
performance. An active line of research is to explicitly add a nonsmooth regularization term
to the training objective function and apply proximal stochastic (sub)gradient methods,
with or without a diagonal preconditioner for adaptiveness, to induce a pre-specified type
of desirable structure in the final model [ 49,51,9]. Unfortunately, although the added
regularizer indeed induces some desirable structures at the stationary points of the training
objective function, the iterates of these methods only converge to those stationary points
asymptotically, but never really attain such a point at any iteration. Therefore, whether
the output model of these algorithms, which is also an iterate that is only close enough to
a stationary point, indeed possesses the ideal structure at the nearby stationary point is
unknown, and theoretical analyses of these algorithms do not cover any guarantees regarding
the obtained structure. Indeed, [ 16] oberserd empirically that the structures obtained by
those methods are highly suboptimal and unstable over iterations. They then proposed a
regularized dual averaging method called RMDA, and proved that after a finite number of
steps, the iterates of RMDAcan stably identify the locally optimal structure induced by the
regularizer at the stationary point of asymptotic convergence.1This is up to our knowledge
the only method with such structure guarantees for training structured neural networks.
With this property, their experiments demonstrated that their method also empirically
outperforms existing methods on modern computer vision tasks. However, since RMDAdoes
not incorporate adaptiveness and their experiments are conducted only on medium-scale
image classification problems, its usefulness beyond computer vision is in doubt.
For a wide range of tasks in deep learning such as language modeling and speech recognition,
researchers have developed numerous architectures to achieve state-of-the-art prediction
performance, including the transformer [ 44] and the LSTM [ 15]. The transformer is also
gaining prominence in computer vision for achieving exceptional performance [ 29]. Therefore,
it is becoming increasingly important to devise methods that attain satisfactory performance
for training these network architectures with structure. For such modern architectures,
adaptive methods like Adam[20] that iteratively rescale the stochastic gradient update
directions via a coordinate-wise/diagonal preconditioner are known to outperform their
non-adaptive counterparts and thus considered state-of-the-art [ 10,1,52,28,22]. It is hence
expected that the non-adaptive RMDAof [16] might not lead to promising results for such
widely-used architectures and tasks.
This work aims to fill this gap to propose a practical regularized adaptive method with guar-
antees for both convergence and structure identification. Since RMDAalready has structure
guarantees, it might look like we just need to combine it with an arbitrary preconditioner
for adaptiveness. However, this seemingly easy extension actually requires deliberation in
two aspects. First, except for few exceptions, combination of even a simple diagonal precon-
ditioner and a nonsmooth regularizer makes the training subproblem complicated with no
closed-form solution. This is totally different from adaptive methods with no regularization,
whose subproblem optimal solution can be easily computed by coordinate-wise divisions.
Therefore, in the regularized case, the best we can hope for is to apply an iterative approach
to approximately solve the subproblem. This calls for careful design and control for the
measure and the degree of the inexactness in the approximate subproblem solution. The
second aspect is the need of an appropriate preconditioner that provides not only outstanding
empirical performance but also desirable theoretical properties. The interplay between the
inexactness and the preconditioner makes it particularly difficult to address the following
three challenges simultaneously. (i) Convergence : Proving convergence of a new algorithm
with even just one added component is always a nontrivial task. For example, although
convergence of SGD has been well-studied for decades, similar guarantees for its adaptive
correspondence, Adagrad, is not established until very recently [ 8]. We are dealing with both
a preconditioner that changes the whole algorithm, just like from SGD to Adagrad, and the
inevitable inexact subproblem solutions that could nullify many useful properties (regard-
ing the subdifferential) commonly used in convergence proofs. (ii) Structure : Theoretical
guarantees for structure identification is another critical aim of this work. Inexactness alone
already makes this goal difficult; see Example 1 of [ 24] for a simple instance such that even
1See the first paragraph in Section 1 and Appendix B of [ 16] for a discussion about why the
structure at the point of convergence is locally optimal.
2infinitesimal inexactness could hinder structure identification. Even without inexactness,
finding a preconditioner that leads to structure identification guarantees is already difficult
because no adaptive algorithm, even in the much simpler deterministic and exact setting, is
known to have such a guarantee. (iii) Subproblem solver : Our goal is a practical algorithm, so
we need to solve the subproblem efficiently. This requires the inexact measure be checkable
and the degree quickly attainable by a well-designed solver, and the preconditioner should
make the subproblem well-conditioned and cannot complicate the computation of the solver.
To tackle these difficulties, we start from considering structure identification. We leverage
the theory of manifold identification in variational analysis and nonlinear optimization
to design a method that leads to finite-iteration structure identification guarantees. As
discussed by [ 37,16], the key to such guarantees for stochastic algorithms is to ensure the
variance in the stochastic estimations decreases to zero. Due to the standard practice of data
augmentation in deep learning, the training loss in the objective function is essentially the
expected value of the training loss over a certain probability distribution instead of a finite
sum. We thus draw inspirations from [ 25,16] to consider a dual-averaging-type approach
[32,47] with momentum to attain variance reduction in this setting for the stochastic gradient
estimation. However, we also need variance reduction for the preconditioner, so we carefully
select a preconditioner whose update is in a manner similar to dual averaging, and prove
that its variance also decreases to zero. We then conceive an implementable and practical
subgradient-norm-based inexactness measure compatible with the structure identification
theory. Further requirements are then added to the inexactness degree and the preconditioner
to ensure convergence, and we also safeguard the preconditioner to keep the subproblems
well-conditioned and the computation simple. We then propose to solve the subproblem by a
proximal gradient (PG) solver that provably achieves our inexactness requirement efficiently.
This leads to our Regularized Adaptive Momentum Dual Averaging ( RAMDA) algorithm.
We summarize our main contributions as follows.
1.An adaptive algorithm for finding locally optimal structures :RAMDA is the first
regularized adaptive method guaranteed to find the locally optimal structure possessed
by the stationary point to which its iterates converge. It thus produces models that are
more structured while retaining the superb prediction performance of adaptive methods.
2.Efficient subproblem solver for regularized adaptive methods : We propose an
implementable inexactness condition and a companion efficient subproblem solver for
regularized adaptive methods (including ours and existing ones) whose subproblems have
no closed-form solution. We show that the induced inexactness does not affect convergence
or structure identification guarantees. This condition and subproblem solver thus also
serve as a key step for realizing existing frameworks for regularized adaptive methods.
3.A method with outstanding empirical performance : Experiments on training mod-
ern neural networks in computer vision (ImageNet), language modeling (Transformer-XL),
and speech (Tacotron2) with structured sparsity show that RAMDA steadily outper-
forms state of the art by achieving higher structured sparsity ratio and better prediction
performance simultaneously.
2 Related Work
Dual Averaging for Deep Learning. Our method is motivated by [ 16] that adapted
the famous regularized dual averaging [ 47,25] approach with momentum to train structured
neural network models with data augmentation. They selected dual averaging for the gradient
estimation to achieve variance reduction for structure guarantees, but their algorithm does not
allow for adaptiveness. Inspired by this approach, we also take dual-averaging-like updates
for the diagonal preconditioner in the subproblem for adaptiveness. Our preconditioner
design also borrows ideas from the empirically successful MADGRAD of [7] for training
non-regularized neural networks. RAMDA can thus also be seen as a generalization of
MADGRAD to the regularized setting. Since no regularizer is present, unlike RAMDA, the
subproblem of MADGRAD has a closed-form solution and no structure is expected. Moreover,
[7] only analyzed convergence rates of the objective value when the problem is convex. Our
analysis of (i) variance reduction in the preconditioner, (ii) convergence in the nonconvex
nonsmooth regularized case, and (iii) structure identification guarantees are novel and closer
3to properties desirable in practice. The first two items are also applicable when no regularizer
is present, so our theory also expands guarantees for MADGRAD .
Regularized Stochastic Algorithms for Deep Learning. Other than RMDA, there are
several works on training structured neural networks through regularization and its proximal
operator, but none have structure guarantees. [ 49] considered a simple regularized SGD
method with momentum, but their convergence analysis is only for the nonadaptive case.
[51] studied a general regularized adaptive framework ProxGen that incorporates diagonal
preconditioners, and showed that the subgradient of the objective function can decrease
to the reciprocal of the batch size, but their result does not guarantee further convergence
to stationary points. Moreover, they do not allow inexactness in the subproblem, so their
frameworkcanberealizedforonlyasmallclassofproblems. [ 9]proposed ProxSSIthatextends
ProxGentothecaseofgroup-sparsityregularizers, whosecorrespondingsubproblemindeedhas
no closed-form solution. They applied the Newton-Raphson method to obtain nearly-optimal
subproblem solutions, and proposed a seemingly mild inexactness condition. Unfortunately,
their condition is not checkable, and their corresponding convergence guarantee requires the
regularizer to be locally smooth around each iterate, which excludes most regularizers that
induce meaningful structures. On the other hand, we will show that with our implementable
inexactness condition, ProxGen still possesses the same convergence guarantees in [ 51] without
any additional requirement on the regularizer. Moreover, we will see in Section 6 that the
time cost of the subproblem solver of ProxSSIis prohibitively high.
Structure and Manifold Identification. The major tool for our structure guarantees is
the theory of manifold identification [ 12,13,27,24] in variational analysis and nonlinear
optimization. This theory shows that points possessing the same structure induced by the
regularizer at a stationary point form a smooth manifold around this stationary point, and
with properties from the regularizer, if a sequence of points converges to this stationary
point with their corresponding subgradients decreasing to zero, this sequence is guaranteed
to eventually stay in this manifold, thus identifying the structure. [ 25,37,16] have leveraged
this tool to show manifold identification for various stochastic algorithms, and the common
key, as pointed out by [ 37], is variance reduction. Our analysis uses a result given in [ 40] to
prove so for both the gradient estimator and the preconditioner.
3 Problem Setting and Algorithm
As described in Section 1, we consider the case in which the training objective function is
the expectation over a probability distribution as follows.
minW∈EF(W):=Eξ∼D[fξ(W)] +ψ(W), (1)
whereEis a Euclidean space with inner product ⟨·,·⟩and its induced norm ∥·∥,Dis a
distribution over a space Ωrepresenting all possible data modifications, fξis differentiable
almost everywhere for any ξ, and the possibly nonsmooth regularizer ψ(W)is for promoting
a desirable structure in the optimal solutions.
Our algorithm can be seen as a double-dual averaging method that incorporates momentum,
a proximal operation for the regularization, and dual averaging for updating both the
stochastic gradient estimation and the preconditioner. For ease of description, we assume
without loss of generality that E=Rnin this section. At the tth iteration with learning
rateηtand iterate Wt−1, we first draw an independent and identically distributed sample
ξt∼D, compute the stochastic (sub)gradient Gt:=∇fξt(Wt−1)of the loss function at the
current point Wt−1with respect to ξt, and then update the weighted sum Vtof historical
stochastic gradients and the weighted sum Utof their squared norms using the value st:
/braceleftbiggV0:= 0, Vt:=Vt−1+stGt,∀t>0,
U0:= 0, Ut:=Ut−1+stGt◦Gt,∀t>0,st:=ηt√
t, (2)
where◦denotestheHadamard(pointwise)productin E. Wethenconstructthepreconditioner
Ptand the weight sum αtby
Pt:= Diag(3√
Ut+ϵ), αt:=/summationdisplayt
k=1sk, (3)
4Algorithm 1 RAMDA (W0,T,T 2,ϵ,{ηt},{ct},{ϵt})
V0←0, U0←0, α 0←0
fort= 1,...,Tdo
Sampleξt∼D, st←ηt√
t, αt←αt−1+st, Gt←∇fξt(Wt−1)
ComputeVt,Utby (2) and construct Ptby (3), and θt←max(diag(Pt))−1
Compute ˆWtin (4) by PG(Wt−1,W0,α−1
tVt,α−1
tPt,αtθt,T2,ϵt)
UpdateWtby (5)
output:WT
whereϵ>0is a (usually small) constant for numerical stability and Diag(·)is the diagonal
matrix whose diagonal entries are the elements of the input vector. The update direction is
then obtained by (approximately) solving the following subproblem.
ˆWt≈arg min
W/parenleftbig
Qt(W):=αtψ(W) +⟨Vt, W⟩+1
2⟨W−W0, Pt(W−W0)⟩/parenrightbig
,(4)
whereW0is the initial point. Details regarding (4) and how to solve it are deferred to
Section 4. The iterate is then updated by averaging ˆWtandWt−1with somect∈[0,1]:
Wt= (1−ct)Wt−1+ctˆWt. (5)
The choice of Ptin (3) that uses the accumulated square of the stochastic gradient norm as
the preconditioner is the key to adaptivity and is widely seen in adaptive methods such as
Adagrad [ 11], while the choice of the cubic root instead of the square root is motivated by
the impressive numerical performance of MADGRAD of [7] for smooth problems without a
regularization term. The averaging step in (5) with ct̸= 1can be interpreted as incorporating
a momentum term in the non-regularized non-adaptive case [43, 19].
4 Subproblem Solver
Given an iterate Wt−1, a momentum term mt, a preconditioner Pt, and a stepsize ηt,
existing regularized adaptive stochastic gradient algorithms for (1) can be summarized in
the following form [51]:
Wt= arg min
W/parenleftbigˆQt(W):=⟨mt, W⟩+1
2ηt⟨W−Wt−1, Pt(W−Wt−1)⟩+ψ(W)/parenrightbig
,(6)
whose form is similar to (4). When the preconditioner Ptis a multiple of the identity matrix
like in the case of [ 16], the exact subproblem solution of (4) can be efficiently computed
through the proximal operator associated with the regularizer. However, a major difficulty for
realizing regularized adaptive methods, including the proposed RAMDA and the framework
of [51] whose preconditioners are not a multiple of the identity, is that except for few special
regularizers, the subproblem usually has no closed-form solution. We therefore consider
using approximate solutions of the subproblem.
We propose to apply a few iterations of proximal gradient (PG) [see, e.g.,5,33] to approxi-
mately solve the subproblems in (4) and (6) when no closed-form solution is available, and
we will show theoretically and empirically in the following sections that the inexactness of
such approximate solutions has barely any effects on the theoretical guarantees and the final
model quality. For the inexactness of the approximate solution in (4), we require
min
s∈∂Qt(ˆWt)∥s∥≤ϵt, Qt(ˆWt)≤Qt(Wt−1), (7)
for some pre-specified ϵt, where∂Qt(Wt+1)is the (limiting) subdifferential [see, e.g.,38,
Definition 8.3]. This condition can be easily checked using information available in the PG
iterations. For the sake of time efficiency, we also impose an upper limit for the number of
PG iterations. Likewise, when applying our subproblem solver to (6), we enforce (7) but
withQtreplaced by ˆQtand ˆWtbyWt. We focus on the case of diagonal and positive Pt,
5Algorithm 2 PG(Z0,W0,V,P,θ,T 2,ˆϵ)
ifψis nonconvex thenθ←θ/2
forj= 1,...,T 2do
Zj←proxψ(Zj−1−θ(V+P(Zj−1−W0)))
if(7)holds withϵt= ˆϵand ˆWt=ZjthenZT2←Zj, and break
output:ZT2
and thus the largest eigenvalue max(diag(Pt)), where diag(·)is the vector formed by the
diagonal entries of the input matrix, can be calculated easily and used to compute a step
size guaranteeing sufficient objective decrease. For cases in which this value is difficult to
obtain, one can apply a simple backtracking linesearch for the subproblem to find a suitable
step size efficiently. This PG subproblem solver is summarized in Algorithm 2. To guarantee
convergence for both our algorithm and the framework of [ 51], our analysis in Section 5
requires that{ϵt}satisfy
¯ϵ:=/summationdisplay∞
t=0ϵ2
t<∞. (8)
We will show in Section 5 that (7) holds after at most O(ϵ−2
t)iterations of Algorithm 2.
5 Analysis
This section discusses theoretical guarantees for RAMDA and the proposed subproblem solver
in Algorithm 2. We also prove convergence guarantees for applying PGto approximately
solve (6) for the framework of [ 51]. All proofs are in the appendices. Some of our results
are inspired by [ 16], but with the added inexactness in (4) and the adaptiveness for the
preconditioner, the analysis is nontrivial. Recall that we assume that fξis differentiable only
almost everywhere but not everywhere, which conforms with widely-used network structures
like ReLU-type activations.
We first show that (7) can be attained by Algorithm 2 and that the point of convergence of
RAMDA is almost surely a stationary point.
Theorem 1. Assume that (4)and(6)has at least one optimal solution with a finite optimal
objective value. Given ϵt>0, the number of iterations Algorithm 2takes to satisfy (7)for
both(4)and(6)isO(log(ϵ−1
t))whenψis convex and O(ϵ−2
t)whenψis nonconvex.
Theorem 2. Consider{ˆWt}generated by Algorithm 1for(1), with(7)and{ct}and{ϵt}
satisfying/summationtextct=∞and(8). Assume there is L≥0such that for any ξ,fξis almost surely
L-Lipschitz-continuously-differentiable, so the expectation is also L-Lipschitz-continuously-
differentiable, there is C≥0such that Eξt∼D/vextenddouble/vextenddouble∇fξt/parenleftbig
Wt−1/parenrightbig/vextenddouble/vextenddouble4≤Cfor allt, and that the set
of stationary points Z:={W|0∈∂F(W)}is nonempty. For any given W0, consider the
event that{ˆWt}converges to a point ¯W(each event corresponds to a different ¯W). If∂ψis
outer semicontinuous at ¯W, this event has a nonzero probability, and {ηt}satisfies
/summationdisplay
stα−1
t=∞,/summationdisplay/parenleftbig
stα−1
t/parenrightbig2<∞,/vextenddouble/vextenddoubleWt+1−Wt/vextenddouble/vextenddouble/parenleftbig
stα−1
t/parenrightbig−1a.s.−−→ 0,
then we have that ¯W∈Zwith probability one conditional on this event. Moreover, {Wt}
also converges to this stationary point ¯W.
Usually, convergence to a point requires some further regularity conditions like the Kurdyka–
Łojasiewicz condition and boundedness of the iterates. However, existing frameworks
regarding iterates convergence using such conditions also require the method analyzed to
have a subgradient-descent-like behavior and to be a descent algorithm. Neither of these
hold true even for the basic stochastic gradient algorithm, and we leave the analysis for this
part as a challenging future work.
Our next key result shows that after a finite number of iterations, iterates of RAMDA all
possess the same structure as that of the point of convergence ¯W. For this end, we first
need to introduce the notions of partial smoothness and prox-regularity, and impose these
assumptions on ψat¯W.
6Definition 1 (Partial Smoothness [ 26,12]).A function ψis partly smooth at a point ¯W
relative to a setM¯W∋¯Wif
1. Around ¯W,M¯Wis aC2-manifold and ψ|M¯WisC2.
2.ψis regular (finite and the Fréchet subdifferential coincides with the limiting Fréchet
subdifferential) at all points W∈M ¯Wnear ¯W, with∂ψ(W)̸=∅.
3. The affine span of ∂ψ(¯W)is a translate of the normal space to M¯Wat¯W.
4.∂ψis continuous at ¯Wrelative toM¯W.
We often callM¯Wthe active manifold at ¯W. Locally, this manifold represents all points
near ¯Wthat share the same structure induced by the regularized as ¯W. Therefore, finding
the active manifold is equivalent to finding the locally optimal structure.
Definition 2 (Prox-regularity [ 36]).A function ψis prox-regular at ¯WforV∗∈∂ψ(¯W)
ifψis locally lower semi-continuous around ¯W, finite at ¯W, and there is ρ>0such that
ψ(W1)≥ψ(W2)+⟨V, W 1−W2⟩−ρ
2∥W1−W2∥2for everyW1,W2near ¯Wwithψ(W2)close
toψ(¯W)andV∈∂ψ(W2)close toV∗.ψis prox-regular at ¯Wif it is prox-regular for all
V∈∂ψ(¯W).
Theorem 3. Consider Algorithm 1with the conditions in Theorem 2satisfied. Consider
the event of{ˆWt}converging to a certain point ¯Was in Theorem 2. If the probability of
this event is nonzero; ψis prox-regular and subdifferentially continuous at ¯Wand partly
smooth at ¯Wrelative to the active C2manifoldM¯W;∂ψis outer semicontinuous at ¯W;
and the nondegeneracy condition −∇f/parenleftbig¯W/parenrightbig
∈relative interior of ∂ψ/parenleftbig¯W/parenrightbig
holds at ¯W, then
conditional on this event, almost surely there is T0≥0such that
ˆWt∈M ¯W,∀t≥T0.
We note particularly that convex and weakly-convex [ 34] functions are all regular, prox-
regular, and subdifferentially continuous everywhere.
We also show that our subproblem solver and condition can be effectively applied to the
framework of [ 51] while retaining the same convergence guarantees. As mentioned in Section 2,
our result is much stronger than that of [ 9] for having no unrealistic smoothness requirement
onψand using an implementable inexactness condition.
Theorem 4. For the framework in [ 51] with the subproblem solved approximately by Algo-
rithm2such that (7)holds with{ϵt}satisfying (8). Then Theorem 1 of [ 51] still holds, but
with the constants {Qi}being also dependent on ¯ϵ.
6 Experiments
This section examines the practical performance of RAMDA for training structured neural
networks. As sparsity is arguably one of the most widely adopted structures in machine
learning, we follow [ 45] to consider structured sparsity as the representative structure in our
experiments. Particularly, we employ the group LASSO regularization [ 50] to encourage
group sparsity and disable weight decay in all experiments, except for the dense baselines.
We begin from examining the efficiency and effectiveness of PGfor both RAMDA and existing
regularized adaptive methods. We then consider tasks in computer vision, language modeling,
and speech to compare the following algorithms using Pytorch.
•RAMDA: The proposed Algorithm 1.
•RMDA[16]
•ProxSGD [49]
•ProxGen [51]: We follow their experiments to use AdamW and apply our PGas the
subproblem solver.
•ProxSSI[9]
These algorithms are introduced in Section 2 and also further summarized in Appendix A.
For each task, we also provide for reference a baseline that does not include a group LASSO
regularizer in the training (SGD with momentum ( MSGD) for computer vision, and Adam
7Table 1: Weighted group sparsity and validation accuracy of different subproblem stopping
criteria.
No early stopping Early stopping
Model/Data Algorithm Accuracy Sparsity Accuracy Sparsity
VGG19 / ProxGen 92.7±0.2% 88.8±0.0%92.7±0.1% 86.9±0.4%
CIFAR10 RAMDA 92.7±0.2% 86.7±0.3%92.9±0.2% 86.3±0.4%
ResNet50 / ProxGen 73.6±0.1% 74.7±0.6%74.0±0.1% 67.6±3.1%
CIFAR100 RAMDA 69.9±1.5% 69.5±2.1%71.2±1.4% 67.5±1.6%
for the other two), but our comparison is only among those for training structured models.
Our code for reproducing the experiments and the hyperparameter settings are available
athttps://github.com/ismoptgroup/ramda_exp/ . Additional details of the stability of
the structure (level of structured sparsity here) over epochs of RAMDA are available in
Appendix D.
We use two criteria for comparison: 1. Model predictive ability, and 2. Structured sparsity
level. The former is task-dependent and thus specified in each experiment. Regarding the
latter, sparsifying neural networks while preserving its performance requires prior knowledge
of model design. A common approach is retaining certain parameters during the training
process, and we adhere to this convention such that the bias, batch normalization [ 17], layer
normalization [ 3], and embedding layers do not have any sparsity-inducing regularization
imposed on them [ 9,35]. For the rest, we adopt channel-wise grouping for convolutional
layers, input-wise grouping for fully-connected and LSTM layers during the training phase.
For evaluation, our structured sparsity is calculated using the weighted group sparsity with
the weights proportional to the number of parameters in each group.
We run each experiment with three different random initializations and show the mean and
standard deviation of the validation predictive performance and the structured sparsity of
the final model of all methods.
Subproblem We start from showing the effectiveness of our proposed subproblem solver
forRAMDA andProxGen. For both approaches, we use Theorem 2 of [ 9] to safely screen
out a portion of groups that will be zero at the optimal subproblem solution, and opt
for the PGalgorithm to solve the remaining parts. We consider two practical stopping
criteria for PG: 1. Running until it reaches the maximum iterations (no early stopping),
and 2. Terminate when the subproblem objective improvement is small (early stopping).
For the former, we set the maximum to 100. For the latter, we terminate PGearly if
(Qt(Zj−1)−Qt(Zj))/(|Qt(Zj|+ 1)<10−8is reached. Moreover, to ensure incorporation
of the preconditioner into ProxGen, we set its minimum PG iterations to 2. We examine
how these stopping criteria affect the final model of RAMDA and ProxGen using image
classification problems of a smaller scale. From Table 1, we see that early stopping does not
affect the outcome much. Given that early stopping is more efficient, we will adopt it in all
subsequent experiments.
Next, we compare ProxGen with ProxSSI(these two only differ in the subproblem solver)
to examine the efficiency and performance differences between solving the subproblems
approximately and (almost) exactly in Table 2. We see that our solver is around 3X
faster than ProxSSI, and the model qualities are similar. We thus exclude ProxSSIfrom
our comparisons in the following experiments due to its excessively lengthy running time,
especially for large-scale tasks.
Image Classification We conduct a classical computer vision task of training ResNet50
[14] with the ILSVRC 2012 ImageNet dataset [ 39]. The result in Table 3 shows that RAMDA
attains the best validation accuracy and structured sparsity simultaneously.
Language Modeling For language modeling, we train Transformer-XL (base) [ 6] using the
WikiText-103 dataset [ 31]. Transformer-XL is comprised of embedding and non-embedding
layers, and in the PyTorch implementation, the non-embedding layers are built using linear
8Table 2: Weighted group sparsity, validation accuracy and time/epoch of ProxSSIandProxGen
for CIFAR10/CIFAR100. We report the average time/epoch using one NVIDIA V100 GPU.
Algorithm Accuracy Sparsity Time Accuracy Sparsity Time
VGG19/CIFAR10 VGG19/CIFAR100
ProxSSI 92.8±0.1% 88.4±0.2% 79s 67.3±0.1% 78.6±0.3% 79s
ProxGen 92.8±0.0% 86.6±0.1% 24s 68.1±0.4% 75.5±0.2% 26s
ResNet50/CIFAR10 ResNet50/CIFAR100
ProxSSI 94.0±0.1% 83.7±0.6% 260s 73.7±0.4% 70.4±0.7% 251s
ProxGen 94.1±0.1% 80.4±0.4% 70s 73.6±0.4% 65.5±3.6% 74s
Table 3: Weighted group sparsity and validation accuracy on ImageNet/ResNet50.
Algorithm Accuracy Sparsity
MSGD 77.14±0.04% -
RAMDA 74.53±0.10% 29.19±0.94%
ProxSGD 73.50±0.20% 17.54±1.26%
ProxGen 74.17±0.08% 20.29±0.22%
RMDA 74.47±0.08% 25.20±1.69%
and layer-normalization layers. We apply group LASSO regularization to the linear layers,
and present in Table 4 the perplexity and the weighted group sparsity of the models trained.
We see that RAMDA gives the best perplexity and structured sparsity simultaneously.
Speech Synthesis We consider Tacotron2 [ 41] for speech synthesis on the LJSpeech
dataset [ 18]. We apply regularization to the convolutional, LSTM, and linear layers of
Tacotron2 and show the results in Table 5. Clearly, RAMDA gives the lowest validation loss
and the highest group sparsity.
Time Efficiency In Tables 4 and 5, we see that although RAMDA andProxGen have more
difficult subproblems without a closed-form solution to solve, our proposed PGsolver is
highly efficient such that the running time of them is still close to other approaches, making
these regularized adaptive approaches practically feasible.
Summary In summary, thanks to its adaptive nature (for better predictive performance)
and its ability of manifold identification (for higher structured sparsity), RAMDA is superior
to state of the art on modern language modeling and speech synthesis tasks as well as the
ImageNet problem. We also observe from the plots in the appendices that it is possible to
further improve the sparsity level of RAMDA if we run it for more epochs.
7 Conclusions
In this work, we proposed a regularized dual averaging method with adaptiveness, RAMDA,
for training structured neural networks. Our method outperforms state of the art on modern
Table4: WeightedgroupsparsityandvalidationperplexityonTransformer-XLwithWikiText-
103.
Alg. Perplexity Sparsity Time/epoch
Adam 23.00±0.05 - 6261 ±21s
RAMDA 26.97±0.10 36.2±0.3% 6954±30s
ProxSGD 27.42±0.02 33.1±1.5% 6167±12s
ProxGen 27.49±0.19 30.5±0.6% 6652±21s
RMDA 27.10±0.08 36.0±2.7% 6184±20s
9Table 5: Weighted group sparsity and validation loss on Tacotron2 with LJSpeech.
Alg. Loss Sparsity Time/epoch
Adam 0.39±0.02 - 431 ±2s
RAMDA 0.44±0.01 52.9±1.6% 443±1s
ProxSGD 0.50±0.00 34.3±1.6% 431±0s
ProxGen 0.45±0.01 45.6±0.9% 438±2s
RMDA 0.46±0.01 45.9±1.7% 431±2s
architectures including LSTM and transformers as well as the ImageNet problem. We
also proposed a subroutine with strong convergence guarantees to approximately solve the
regularized subproblem of both our method and an existing framework efficiently. Extensive
experiments on group sparsity showed that our subproblem solver can greatly reduce the
training time for existing methods, and our proposed RAMDA achieves simultaneously
higher structured sparsity ratio and better prediction performance than existing methods.
Implementation of our method is available at https://www.github.com/zhisyuan1214/
RAMDA/.
Acknowledgement
Ching-pei’s research is supported in part by the JSPS Grant-in-Aid for Research
Activity Start-up 23K19981 and Grant-in-Aid for Early-Career Scientists 24K20845.
References
[1]Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive
optimization. In Advances in Neural Information Processing Systems , 2019. 2
[2]Hedy Attouch, Jérôme Bolte, and Benar Fux Svaiter. Convergence of descent methods
for semi-algebraic and tame problems: proximal algorithms, forward–backward splitting,
and regularized Gauss–Seidel methods. Mathematical programming , 137(1-2):91–129,
2013. 15
[3]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. Technical
report, 2016. arXiv:1607.06450. 8
[4]Amir Beck. First-Order Methods in Optimization . SIAM - Society for Industrial and
Applied Mathematics, Philadelphia, PA, United States, 2017. 15
[5]Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for
linear inverse problems. SIAM Journal on Imaging Sciences , 2(1):183–202, 2009. 5
[6]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan" Salakhut-
dinov. Transformer-XL: Attentive language models beyond a fixed-length context. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,
2019. 8
[7]Aaron Defazio and Samy Jelassi. Adaptivity without compromise: A momentumized,
adaptive, dual averaged gradient method for stochastic optimization. Journal of Machine
Learning Research , 23(144):1–34, 2022. 3, 5, 14
[8]Alexandre Défossez, Leon Bottou, Francis Bach, and Nicolas Usunier. A simple conver-
gence proof of adam and adagrad. Transactions on Machine Learning Research , 2022.
ISSN 2835-8856. 2
[9]Tristan Deleu and Yoshua Bengio. Structured sparsity inducing adaptive optimizers for
deep learning. Technical report, 2021. arXiv:2102.03869. 2, 4, 7, 8
10[10]Michael Denkowski and Graham Neubig. Stronger baselines for trustable results in
neural machine translation. Technical report, 2017. arXiv:1706.09733. 2
[11]John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online
learning and stochastic optimization. Journal of machine learning research , 12(7), 2011.
5
[12]WarrenL.HareandAdrianS.Lewis. Identifyingactiveconstraintsviapartialsmoothness
and prox-regularity. Journal of Convex Analysis , 11(2):251–266, 2004. 4, 7
[13]Warren L. Hare and Adrian S. Lewis. Identifying active manifolds. Algorithmic
Operations Research , 2(2):75–75, 2007. 4
[14]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for
image recognition. In IEEE conference on computer vision and pattern recognition , 2016.
8, 20
[15]SeppHochreiterandJürgenSchmidhuber. Longshort-termmemory. Neural computation ,
9(8):1735–1780, 1997. 2
[16]Zih-Syuan Huang and Ching-pei Lee. Training structured neural networks through
manifold identification and variance reduction. In International Conference on Learning
Representations , 2022. 2, 3, 4, 5, 6, 7, 14
[17]Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network
training by reducing internal covariate shift. In International conference on machine
learning, pages 448–456, 2015. 8
[18] Keith Ito and Linda Johnson. The LJ speech dataset, 2017. 9
[19]Samy Jelassi and Aaron Defazio. Dual averaging is surprisingly effective for deep learning
optimization. Technical report, 2020. arXiv:2010.10502. 5, 14
[20]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In
International Conference on Learning Representations , 2015. 2
[21]Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,
2009. 20
[22]Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt.
Noise is not the main factor behind the gap between sgd and adam on transformers,
but sign descent might be. In International Conference on Learning Representations ,
2023. 2
[23]Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998. 20
[24]Ching-pei Lee. Accelerating inexact successive quadratic approximation for regularized
optimization through manifold identification. Mathematical Programming , 2023. 2, 4, 17
[25]Sangkyun Lee and Stephen J. Wright. Manifold identification in dual averaging for
regularized stochastic online learning. Journal of Machine Learning Research , 13:
1705–1744, 2012. 3, 4
[26]Adrian S. Lewis. Active sets, nonsmoothness, and sensitivity. SIAM Journal on
Optimization , 13(3):702–725, 2002. 7
[27]Adrian S. Lewis and Shanshan Zhang. Partial smoothness, tilt stability, and generalized
hessians. SIAM Journal on Optimization , 23(1):74–94, 2013. 4
[28]Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding
the difficulty of training transformers. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP) , 2020. 2
11[29]Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows.
InProceedings of the IEEE/CVF international conference on computer vision , 2021. 2,
22
[30]Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Interna-
tional Conference on Learning Representations , 2019. 14
[31]Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel
mixture models. In International Conference on Learning Representations , 2017. 8
[32]Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical
programming , 120(1):221–259, 2009. 3
[33]Yurii Nesterov. Gradient methods for minimizing composite functions. Mathematical
Programming , 140(1):125–161, 2013. 5
[34]Evgeni Alekseevich Nurminskii. The quasigradient method for the solving of the
nonlinear programming problems. Cybernetics , 9(1):145–150, 1973. 7
[35]Alexandra Peste, Eugenia Iofinova, Adrian Vladu, and Dan Alistarh. Ac/dc: Alternating
compressed/decompressed training of deep neural networks. In Advances in neural
information processing systems , 2021. 8
[36]René Poliquin and R Rockafellar. Prox-regular functions in variational analysis. Trans-
actions of the American Mathematical Society , 348(5):1805–1838, 1996. 7
[37]Clarice Poon, Jingwei Liang, and Carola-Bibiane Schönlieb. Local convergence proper-
ties of SAGA/prox-SVRG and acceleration. In International Conference on Machine
Learning , 2018. 3, 4
[38]R. Tyrrell Rockafellar and Roger J-B Wets. Variational analysis , volume 317. Springer
Science & Business Media, 2009. 5
[39]Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.
Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. International
journal of computer vision , 115(3):211–252, 2015. 8
[40]Andrzej Ruszczyński. Feasible direction methods for stochastic programming problems.
Mathematical Programming , 19:220–229, 1980. 4, 16
[41]Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng
Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, Rif A. Saurous, Yannis
Agiomyrgiannakis, and Yonghui Wu. Natural tts synthesis by conditioning wavenet
on mel spectrogram predictions. In 2018 IEEE international conference on acoustics,
speech and signal processing (ICASSP) , 2018. 9
[42]Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-
scale image recognition. In International Conference on Learning Representations , 2015.
20
[43]Wei Tao, Zhisong Pan, Gaowei Wu, and Qing Tao. Primal averaging: A new gradient
evaluation step to attain the optimal individual convergence. IEEE transactions on
cybernetics , 50(2):835–845, 2018. 5
[44]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in
neural information processing systems , 2017. 2
[45]Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured
sparsity in deep neural networks. Advances in neural information processing systems ,
pages 2074–2082, 2016. 7
12[46]Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. Technical report, 2017. arXiv:1708.07747.
22
[47]Lin Xiao. Dual averaging methods for regularized stochastic learning and online
optimization. Journal of Machine Learning Research , 11(88):2543–2596, 2010. 3
[48]Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai,
and Han Hu. SimMIM: A simple framework for masked image modeling. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
9653–9663, 2022. 22
[49]Yang Yang, Yaxiong Yuan, Avraam Chatzimichailidis, Ruud JG van Sloun, Lei Lei, and
Symeon Chatzinotas. ProxSGD: Training structured neural networks under regulariza-
tion and constraints. In International Conference on Learning Representations , 2019. 2,
4, 7
[50]Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped
variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology) ,
68(1):49–67, 2006. 7
[51]Jihun Yun, Aurélie C Lozano, and Eunho Yang. Adaptive proximal gradient methods
for structured neural networks. In Advances in Neural Information Processing Systems ,
2021. 2, 4, 5, 6, 7, 18
[52]Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank
Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention
models? In Advances in Neural Information Processing Systems , 2020. 2
Appendices
Table of Contents
A More Experiment Details 13
A.1 Implementation and Hyperparameter Setting of RAMDA . . . . . . . . . 14
A.2 Details of the Algorithms Compared . . . . . . . . . . . . . . . . . . . . 14
A.3 Computational Resource . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
B Proofs 14
B.1 Convergence Result for ProxGen . . . . . . . . . . . . . . . . . . . . . . . 18
C Additional Experiments for Computer Vision 20
D Plots of Sparsity Level and Validation Accuracy over Epochs 21
E Experiment with Nuclear-norm Regularization 22
A More Experiment Details
This section describes details of our implementation of RAMDA and the setting of the
experiments conducted in Section 6.
13A.1 Implementation and Hyperparameter Setting of RAMDA
Similar to [ 16], we introduce a restart strategy to the implementation of RAMDA. During each
stage of the training, the learning rate ηtand the momentum factor ctare fixed. Once the
epoch count enters the next stage, we reset the counter tto1and use the output parameter
WTfrom the previous round as the new input parameter W0to the same algorithm, set
αt,VtandUtto0, but keep the scheduling for ηandcgoing without resetting them, and
decreaseϵby a factor. We initialize ctas either 0.1or0.01, depending on the problems, and
use a constant ctuntil the final stage, where we gradually increase it by
ct= min(c√
i,1),
whereicounts the training steps executed at this final stage. This momentum strategy is
applied to both RAMDA andRMDAin our experiments.
A.2 Details of the Algorithms Compared
In Table 6, we summarize details of the algorithms compared in Section 6.
Table 6: Algorithms used in the experiments.
Algorithm Unregularized counterpart Subproblem
RAMDA MADGRAD [7] PG
RMDA MDA[19] Closed-form solution
ProxSGD MSGD Closed-form solution
ProxGen AdamW[30] PG
ProxSSI AdamW[30] Newton-Raphson
A.3 Computational Resource
We conduct all experiments utilizing NVIDIA TESLA V100 (32 GB) GPUs. We employ
eight V100 GPUs for each run of the ImageNet experiments. For all other experiments, we
utilize a single V100 GPU per run.
B Proofs
This section provides proofs of the theoretical results stated in Section 5. We restate these
results and provide their corresponding proofs right after each statement.
Theorem 1. Assume that (4)and(6)has at least one optimal solution with a finite optimal
objective value. Given ϵt>0, the number of iterations of Algorithm 2takes to satisfy (7)for
both(4)and(6)isO(ϵ−1
t)whenψis convex and O(ϵ−2
t)whenψis nonconvex.
Proof.We use the notation
¯Qt(Z) =ft(Z) +ψ(Z)
to unify the two objective function Qt/αtand ˆQt, whereftis the smooth part and we define
the Lipschitz constant of ∇ftasL.
At each iteration, PGsolves the following subproblem
Zj+1∈arg min
Z⟨∇ft(Zj), Z−Zj⟩+1
2θt/vextenddouble/vextenddoubleZ−Zj/vextenddouble/vextenddouble2+ψ(Z),
and thus from the first-order optimality conditions, clearly we have
∇ft(Zj+1)−∇ft(Zj)−1
θt/parenleftbig
Zj+1−Zj/parenrightbig
∈∂¯Qt(Zj+1).
We thus have from the Lipschitz continuity of ∇ftthat
min
s∈∂¯Qt(Zj+1)∥s∥≤/vextenddouble/vextenddouble∇ft(Zj+1)−∇ft(Zj)/vextenddouble/vextenddouble+1
θt/vextenddouble/vextenddoubleZj+1−Zj/vextenddouble/vextenddouble≤/parenleftbig
L+θ−1
t/parenrightbig/vextenddouble/vextenddoubleZj+1−Zj/vextenddouble/vextenddouble.
(9)
14Note that ¯Qtis lower bounded, say by ¯Q∗
t, and has at least one solution Z∗(unique when ψ
is convex).
In the case that ψis convex, we know that θt= 1/L, and (3) clearly shows that the
subproblem objective ¯Qisϵ-strongly convex. Therefore, standard analysis of proximal
gradient [see, for example, 4, Lemma 10.4 and iTheorem 10.29] gives that
L
2/vextenddouble/vextenddoubleZj+1−Zj/vextenddouble/vextenddouble≤¯Qt(Zj)−¯Qt(Zj+1),∀j≥0, (10)
¯Qt(Zj)−¯Qt(Z∗)≤L
2/parenleftig
1−ϵ
L/parenrightigj/vextenddouble/vextenddoubleZ0−Z∗/vextenddouble/vextenddouble2,∀j≥1. (11)
The combination of (9)–(11) shows that it takes O(logϵ−1
t)iterations for PGto reach the
required precision of ϵt.
Whenψis nonconvex, we have that θt= 1/(2L)and standard analysis [ 2, Section 5.1] gives
min
k=0,1,...,j/vextenddouble/vextenddoubleZj+1−Zj/vextenddouble/vextenddouble≤C√j(12)
for some constant Cdepending on Land ¯Qt(Wt)−¯Q∗
t. Therefore, (12) and (9) show that it
takesO(ϵ−2
t)iterations to reach the desired precision.
Theorem 2. Consider{ˆWt}generated by Algorithm 1for(1), with(7)and{ct}and{ϵt}
satisfying/summationtextct=∞and(8). Assume there is L≥0such that for any ξ,fξis almost surely
L-Lipschitz-continuously-differentiable, so the expectation is also L-Lipschitz-continuously-
differentiable, there is C≥0such that Eξt∼D/vextenddouble/vextenddouble∇fξt/parenleftbig
Wt−1/parenrightbig/vextenddouble/vextenddouble4≤Cfor allt, and that the set
of stationary points Z:={W|0∈∂F(W)}is nonempty. For any given W0, consider the
event that{ˆWt}converges to a point ¯W(each event corresponds to a different ¯W). If∂ψis
outer semicontinuous at ¯W, this event has a nonzero probability, and {ηt}satisfies
/summationdisplay
stα−1
t=∞,/summationdisplay/parenleftbig
stα−1
t/parenrightbig2<∞, (13)
/vextenddouble/vextenddoubleWt+1−Wt/vextenddouble/vextenddouble/parenleftbig
stα−1
t/parenrightbig−1a.s.−−→ 0, (14)
then we have that ¯W∈Zwith probability one conditional on this event. Moreover, {Wt}
also converges to this stationary point ¯W.
Proof.First, we prove that when {ˆWt}converges to ¯W,{Wt}also converges to ¯W. From
(5), we have that
/vextenddouble/vextenddoubleWt−¯W/vextenddouble/vextenddouble≤(1−ct)/vextenddouble/vextenddoubleWt−1−¯W/vextenddouble/vextenddouble+ct/vextenddouble/vextenddouble/vextenddoubleˆWt−¯W/vextenddouble/vextenddouble/vextenddouble. (15)
Since ˆWt→¯W, for anyϵ>0we can find an integer tϵ≥0such that∥ˆWt−¯W∥≤ϵfor all
t≥tϵ. Therefore, by deducting ϵfrom both sides of (15), we get
/vextenddouble/vextenddoubleWt−¯W/vextenddouble/vextenddouble−ϵ≤/parenleftiggt/productdisplay
k=tϵ(1−ct)/parenrightigg
/parenleftbig/vextenddouble/vextenddoubleWtϵ−1−¯W/vextenddouble/vextenddouble−ϵ/parenrightbig
≤exp/parenleftigg
−t/summationdisplay
k=tϵct/parenrightigg
/parenleftbig/vextenddouble/vextenddoubleWtϵ−1−¯W/vextenddouble/vextenddouble−ϵ/parenrightbig
,∀t≥tϵ.
By lettingtapproach infinity and noting that/summationtextct=∞, we see that
lim
t→∞/vextenddouble/vextenddoubleWt−¯W/vextenddouble/vextenddouble≤ϵ.
Becauseϵis arbitrary, we see that ∥Wt−¯W∥→0, and hence{Wt}converges to ¯W.
Next, consider the update of α−1
tUt, we can see from (2) that
Ut
αt=αt−1
αtUt−1
αt−1+st∇fξt(Wt−1)
αt=/parenleftbigg
1−st
αt/parenrightbiggUt−1
αt−1+st
αt∇fξt(Wt−1).(16)
15Moreover, the assumptions on ηtsatisfies all the required conditions of Lemma 1 of [ 40]. We
therefore apply Lemma 1 of [40] to conclude that
Ut
αta.s.−−→Eξ∼D/bracketleftbig
∇fξ/parenleftbig
Wt/parenrightbig
◦∇fξ/parenleftbig
Wt/parenrightbig/bracketrightbig
. (17)
The update for α−1
tVthas a form analogous to (16), and we have from Jensen’s inequality
that
Eξt∼D/vextenddouble/vextenddouble∇fξt/parenleftbig
Wt−1/parenrightbig/vextenddouble/vextenddouble2≤/radicalig
Eξt∼D∥∇fξt(Wt−1)∥4≤√
C,
implying that the second moment is also bounded in expectation. We can therefore also
apply Lemma 1 of [40] to α−1
tVtand conclude that
Vt
αta.s.−−→∇ Eξ∼D/bracketleftbig
fξ/parenleftbig
Wt/parenrightbig/bracketrightbig
. (18)
We further notice that the union of two events that happens almost surely is still an event
that happens almost surely.
From (4) and (7), we can see that there is a sequence {zt}such that
−/parenleftbiggVt
αt+zt
αt+Pt
αt(ˆWt−W0)/parenrightbigg
∈∂ψ(ˆWt),∥zt∥≤ϵt. (19)
Our assumption in (14) implies that αt→∞, which together with (8) leads to
zt
αt→0. (20)
From (18), that∇Eξ∼D[fξ(W)]is Lipschitz continuous, and that Wt→¯W(which we have
proven in the first part), we see that
Vt
αta.s.−−→∇ Eξ∼D/bracketleftbig
fξ(¯W)/bracketrightbig
. (21)
For the third term, we have from (3) and (17) that
Pt
αt=α−2
3
tDiag/parenleftigg
3/radicaligg
Ut
αt/parenrightigg
+ϵ
αtI.
Again since αt→∞, the second term of the equation above converges to 0. Therefore, by
(17), we obtain that
Pt
αta.s.−−→α−2
3
tDiag/parenleftbigg
3/radicalig
Eξ∼D[∇fξ(Wt)◦∇fξ(Wt)]/parenrightbigg
.
Again from the continuity of ∇Eξ∼D[fξ(W)]and thatαt→∞, we conclude that
Pt
αta.s.−−→α−2
3
tDiag/parenleftbigg
3/radicalig
Eξ∼D/bracketleftbig
∇fξ/parenleftbig¯W/parenrightbig
◦∇fξ/parenleftbig¯W/parenrightbig/bracketrightbig/parenrightbigg
a.s.−−→0. (22)
Finally, using the outer semicontinuity of ∂ψ(W)at¯W, we conclude from (19)–(22) that
0∈∇Eξ∼D/bracketleftbig
fξ/parenleftbig¯W/parenrightbig/bracketrightbig
+ lim
t→∞ψ(ˆWt)⊆∇Eξ∼D/bracketleftbig
fξ/parenleftbig¯W/parenrightbig/bracketrightbig
+ψ(¯W) =∂F(¯W)
with probability one, showing that ¯Wis a stationary point almost surely.
Theorem 3. Consider Algorithm 1with the conditions in Theorem 2satisfied. Consider the
event of{ˆWt}converging to a certain point ¯Was in Theorem 2. If the probability of this
event is nonzero; ψis prox-regular and subdifferentially continuous at ¯Wand partly smooth
at¯Wrelative to the active C2manifoldM¯W;∂ψis outer semicontinuous at ¯W; and the
nondegeneracy condition
−∇f/parenleftbig¯W/parenrightbig
∈relative interior of ∂ψ/parenleftbig¯W/parenrightbig
(23)
holds at ¯W, then conditional on this event, almost surely there is T0≥0such that
ˆWt∈M ¯W,∀t≥T0. (24)
In other words, the active manifold at ¯Wis identified by the iterates of Algorithm 1after a
finite number of iterations almost surely.
16Proof.From (19), there exists a sequence {Yt}such that
Yt∈∂ψ(ˆWt),Vt
αt+zt
αt+Pt
αt(ˆWt−W0) +Yt= 0,∀t. (25)
For notational ease, we denote
f(W):=Eξ∼D[fξ(W)]. (26)
From (25), we then get
∇f(ˆWt)−Vt
αt−zt
αt−Pt
αt(ˆWt−W0)∈∂F(ˆWt). (27)
We aim to show that
dist(0,∂F(ˆWt)):= min
Y∈∂F(ˆWt)∥Y∥
converges to 0almost surely. From (27), we have
dist(0,∂F(ˆWt))≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇f(ˆWt)−Vt
αt−zt
αt−Pt
αt(ˆWt−W0)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇f(ˆWt)−Vt
αt/vextenddouble/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddouble/vextenddoublezt
αt/vextenddouble/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddouble/vextenddoublePt
αt(ˆWt−W0)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇f(ˆWt)−Vt
αt/vextenddouble/vextenddouble/vextenddouble/vextenddouble+ϵt
αt+/vextenddouble/vextenddouble/vextenddouble/vextenddoublePt
αt(ˆWt−W0)/vextenddouble/vextenddouble/vextenddouble/vextenddouble, (28)
where we get the first inequality from the triangle inequality and the second from (19).
According to (18) and (22), there are {At}and{Bt}such that
/braceleftiggVt
αt=∇f(Wt) +At,∥At∥a.s.−−→0
Pt
αt=α−2
3
tDiag/parenleftig
3/radicalbig
∇f(Wt)◦∇f(Wt)/parenrightig
+Bt,∥Bt∥a.s.−−→0.(29)
Substituting the above two equations back to (28), we obtain
dist(0,∂F(ˆWt))
≤/vextenddouble/vextenddouble/vextenddouble∇f(ˆWt)−∇f(Wt)/vextenddouble/vextenddouble/vextenddouble+∥At∥+ϵt
αt+/parenleftig
α−2
3
t/vextenddouble/vextenddouble/vextenddouble3/radicalbig
∇f(Wt)◦∇f(Wt)/vextenddouble/vextenddouble/vextenddouble
∞+∥Bt∥/parenrightig/vextenddouble/vextenddouble/vextenddoubleˆWt−W0/vextenddouble/vextenddouble/vextenddouble
≤L/vextenddouble/vextenddouble/vextenddoubleˆWt−Wt/vextenddouble/vextenddouble/vextenddouble+∥At∥+ϵt
αt+/parenleftig
α−2
3
t/vextenddouble/vextenddouble/vextenddouble3/radicalbig
∇f(Wt)◦∇f(Wt)/vextenddouble/vextenddouble/vextenddouble
∞+∥Bt∥/parenrightig/vextenddouble/vextenddouble/vextenddoubleˆWt−W0/vextenddouble/vextenddouble/vextenddouble.
(30)
From Theorem 2, we know that ˆWtandWtboth converge to ¯W, so/vextenddouble/vextenddouble/vextenddoubleˆWt−Wt/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddoubleˆWt−¯W/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddoubleWt−¯W/vextenddouble/vextenddouble→0.
From (8) and (14), we know that ϵt/αt→0. Because ˆWt→¯W, we also have that/vextenddouble/vextenddouble/vextenddoubleˆWt−W0/vextenddouble/vextenddouble/vextenddouble→/vextenddouble/vextenddouble¯W−W0/vextenddouble/vextenddouble<∞.
FromWt→¯W, we have that
/vextenddouble/vextenddouble/vextenddouble3/radicalbig
∇f(Wt)◦∇f(Wt)/vextenddouble/vextenddouble/vextenddouble
∞→/vextenddouble/vextenddouble/vextenddouble/vextenddouble3/radicalig
∇f(¯W)◦∇f(¯W)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞<∞.
Combining these results with (30), we conclude that
L/vextenddouble/vextenddouble/vextenddoubleˆWt−Wt/vextenddouble/vextenddouble/vextenddouble+∥At∥+ϵt
αt+/parenleftig
α−2
3
t/vextenddouble/vextenddouble/vextenddouble3/radicalbig
∇f(Wt)◦∇f(Wt)/vextenddouble/vextenddouble/vextenddouble
∞+∥Bt∥/parenrightig/vextenddouble/vextenddouble/vextenddoubleˆWt−W0/vextenddouble/vextenddouble/vextenddoublea.s.−−→0,
proving that
dist(0,∂F(ˆWt))a.s.−−→0.
On the other hand, since fis continuous and ψis subdifferentially continuous at ¯W(which
impliesFis also subdifferentially contnuous at ¯W),ˆWt→¯W, and that∇f(ˆWt) +Yta.s.−−→
0∈∂F(¯W)(from Theorem 2), we know that F(ˆWt)a.s.−−→F(¯W)as well. Therefore, we can
apply Lemma 1 of [ 24] to conclude that (24) indeed holds for some T0<∞almost surely.
17Algorithm 3 ProxGen (W0,T,T 2,{ηt},{ρt},{ct},{ϵt},{bt},δI)
m0←0
fort= 1,...,Tdo
Sampleξt∼Dwith batch size bt
Gt←∇fξt(Wt−1)
mt←ρtmt−1+ (1−ρt)Gt
ConstructPtsatisfyingPt⪰δI
θt←1/∥Pt∥2
ComputeWt+1by roughly solving (6) that satisfies (7) with Qtreplaced by ˆQtand ˆWt
replaced by Wt+1, using PG(Wt,Wt,mt,η−1
tPt,θt,T2,ϵt)
output:WT
B.1 Convergence Result for ProxGen
We next discuss the convergence result for the framework of [ 51] with inexactness added. For
consistency, we first use our notations to introduce their framework, with our inexactness
condition added, in Algorithm 3.
In their analysis, [ 51] made the following four assumptions, and we will follow these assump-
tions using the notation (26).
(C-1)The expected loss function fisL-Lipschitz-continuously-differentiable and lower-
bounded for some L≥0.
(C-2)The stochastic gradient Gt=∇fξt(Wt−1)is an unbiased estimator of ∇f(Wt−1)
with bounded variance.
Eξt∼D[Gt] =∇f(Wt−1), Eξt∼D/bracketleftig/vextenddouble/vextenddoubleGt−∇f(Wt−1)/vextenddouble/vextenddouble2/bracketrightig
≤σ2
bt,∀t≥0,
wherebtis the batch size of ξtandσ≥0is a constant.
(C-3)There are some ρ0,µ∈[0,1)andD,G> 0such that/vextenddouble/vextenddoubleWt+1−Wt/vextenddouble/vextenddouble≤D,∥Gt∥≤G,
andρt=ρ0µt−1for allt.
(C-4)There is some γ >0such that/vextenddouble/vextenddoubleη−1
tPt/vextenddouble/vextenddouble
2≤1/γ <∞for allt.
(C-5)There isδ>0such that
Pt≥δ, ηt≤δ
3L,∀t≥0. (31)
Theorem 4. For the framework in [ 51] with the subproblem solved approximately by Algo-
rithm2such that (7)holds with{ϵt}satisfying (8). Then Theorem 1 of [ 51] still holds, but
with the constants {Qi}being also dependent on ¯ϵ.
Proof.The major flow of our proof follows that of [ 51] but with suitable modifications to
accommodate the inexactness condition in the subproblem solving.
It is clear from [51, Lemma 1] that
∥mt∥≤G,∀t≥0. (32)
By the update rule for mt, (6) and (7), we have that there is ztsuch that
0∈zt+ (1−ρt)gt+ρtmt−1+∂ψ(Wt) +1
ηt(Pt)(Wt−Wt−1),∥zt∥≤ϵt,∀t≥0,
leading to
∇f(Wt)−zt−(1−ρt)gt−ρtmt−1−1
ηt(Pt)(Wt−Wt−1)∈∂F(Wt).(33)
18We thus have from (33) and (C- 4) that
dist(0,∂F(Wt))2
≤/vextenddouble/vextenddouble/vextenddoublezt+ (1−ρt)gt−∇f(Wt) +ρtmt−1+ (Wt−Wt−1) +1
ηt(Pt)(Wt−Wt−1)−(Wt−Wt−1)/vextenddouble/vextenddouble/vextenddouble2
≤4/vextenddouble/vextenddouble/vextenddouble(1−ρt)gt−∇f(Wt) +ρtmt−1+ (Wt−Wt−1)/vextenddouble/vextenddouble/vextenddouble2
+ 4ϵ2
t+ 4/vextenddouble/vextenddouble/vextenddouble1
ηt(Pt)(Wt−Wt−1)/vextenddouble/vextenddouble/vextenddouble2
+ 4/vextenddouble/vextenddouble/vextenddouble(Wt−Wt−1)/vextenddouble/vextenddouble/vextenddouble2
≤4/vextenddouble/vextenddouble/vextenddouble(1−ρt)gt−∇f(Wt) +ρtmt−1+ (Wt−Wt−1)/vextenddouble/vextenddouble/vextenddouble2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T1+4/parenleftig1
γ2+ 1/parenrightig
∥Wt−Wt−1∥2+ 4ϵ2
t.
(34)
We will separately bound the quantities T1and/vextenddouble/vextenddoubleWt−Wt−1/vextenddouble/vextenddouble2below.
From the subproblem objective requirement in (7), we get
/angbracketleftbig
(1−ρt)gt+ρtmt−1,Wt−Wt−1/angbracketrightbig
+ψ(Wt)+1
2ηt⟨Wt−Wt−1, Pt(Wt−Wt−1)⟩≤ψ(Wt−1).
(35)
From (C- 1), we have
f(Wt)≤f(Wt−1) +⟨∇f(Wt−1),Wt−Wt−1⟩+L
2∥Wt−Wt−1∥2. (36)
Summing (35) and (36) gives
/angbracketleftbig
(1−ρt)gt−∇f(Wt−1)+ρtmt−1,Wt−Wt−1/angbracketrightbig
+/vextenddouble/vextenddoubleWt−Wt−1/vextenddouble/vextenddouble2
Pt
2ηt−L
2I≤F(Wt−1)−F(Wt).
(37)
Note thatη−1
tPt−LI⪰0from (31), so the second term in (37) is nonnegative. (37) together
with (C- 3) then leads to
∥Wt−Wt−1∥2
Pt
2ηt−L
2I
≤F(Wt−1)−F(Wt)−/angbracketleftbig
gt−∇f(Wt−1),Wt−Wt−1/angbracketrightbig
+⟨ρtgt,Wt−Wt−1⟩−⟨ρtmt−1,Wt−Wt−1⟩
≤F(Wt−1)−F(Wt) +1
2L∥gt−∇f(Wt−1)∥2+L
2∥Wt−Wt−1∥2+ρ2
t
2L∥gt∥2+L
2∥Wt−Wt−1∥2
+ρt∥mt−1∥∥Wt−Wt−1∥
≤F(Wt−1)−F(Wt) +1
2L∥gt−∇f(Wt−1)∥2+ +L∥Wt−Wt−1∥2+ρ2
0µ2(t−1)G2
2L+ρ0µt−1DG.
Summing it over t= 1,2,...,Tand utilizing the assumption that the step sizes are non-
increasing then give
/parenleftigδ
2η0−3
2L/parenrightigT/summationdisplay
t=1∥Wt−Wt−1∥2≤∆ +C1+1
2LT/summationdisplay
t=1∥gt−∇f(Wt−1)∥2,
where
∆:=F(W0)−min
WF(W), C 1:=ρ0DG
1−µ+ρ2
0G2
2L(1−µ2).
From the inequality above, we obtain
T/summationdisplay
t=1∥Wt−Wt−1∥2≤H1+H2T/summationdisplay
t=1∥gt−∇f(Wt−1)∥2(38)
for some constants H1,H2depending on L,∆,δ,η 0, andC1. From (37), we also have/angbracketleftig
(1−ρt)gt−∇f(Wt) +ρtmt−1,Wt−Wt−1/angbracketrightig
≤F(Wt−1)−F(Wt)−/angbracketleftbig
∇f(Wt)−∇f(Wt−1),Wt−Wt−1/angbracketrightbig
−/vextenddouble/vextenddoubleWt−Wt−1∥2
1
2ηt(Pt)−L
2I
≤F(Wt−1)−F(Wt)−/angbracketleftbig
∇f(Wt)−∇f(Wt−1),Wt−Wt−1/angbracketrightbig
.
19Therefore, we obtain
T1=∥(1−ρt)gt−∇f(Wt) +ρtmt−1∥2+∥Wt−Wt−1∥2+ 2/angbracketleftig
(1−ρt)gt−∇f(Wt) +ρtmt−1,Wt−Wt−1/angbracketrightig
≤∥(1−ρt)gt−∇f(Wt−1) +∇f(Wt−1)−∇f(Wt) +ρtmt−1∥2+∥Wt−Wt−1∥2
+ 2/parenleftbig
F(Wt−1)−F(Wt)−/angbracketleftbig
∇f(Wt)−∇f(Wt−1),Wt−Wt−1/angbracketrightbig/parenrightbig
≤4∥gt−∇f(Wt−1)∥2+ 4L2∥Wt−Wt−1∥2+ 4ρ2
t(∥mt−1∥2+∥gt∥2) +∥Wt−Wt−1∥2
+ 2/parenleftbig
F(Wt−1)−F(Wt) +L∥Wt−Wt−1∥2/parenrightbig
≤2/parenleftbig
F(Wt−1)−F(Wt)/parenrightbig
+ 8ρ2
0µ2(t−1)G2+/parenleftig
1 + 2L+ 4L2/parenrightig
∥Wt−Wt−1∥2+ 4∥gt−∇f(Wt−1)∥2.
(39)
LetC2:= 2 + 2L+ 4L2+γ−2and insert (39) into (34), we get
dist(0,∂F(Wt))2
≤4/parenleftigg
2/parenleftbig
F(Wt−1)−F(Wt)/parenrightbig
+ 8ρ2
0µ2(t−1)G2+C2∥Wt−Wt−1∥2+ 4∥gt−∇f(Wt−1)∥2+ϵ2
t/parenrightigg
.
(40)
Therefore, we have from (8) and (40) and (C- 2) that
Ea,ξ1,...,ξT[dist(0,∂F(Wa))2]
≤1
TT/summationdisplay
t=1E/bracketleftig/vextenddouble/vextenddouble(1−ρt)gt−∇f(Wt) +zt+ρtmt−1+1
ηt(Pt)(Wt−Wt−1)/vextenddouble/vextenddouble2/bracketrightig
≤4
T/parenleftig
2∆ +8ρ2
0G2
1−µ2+ 4T/summationdisplay
t=1E∥gt−∇f(Wt−1)∥2+C2T/summationdisplay
t=1E∥Wt−Wt−1∥2+T/summationdisplay
t=1ϵ2
t/parenrightig
≤4
T/parenleftig
2∆ +8ρ2
0G2
1−µ2+ 4σ2T/summationdisplay
t=11
bt+C2(H1+H2σ2T/summationdisplay
t=11
bt) + ¯ϵ/parenrightig
≤Q1
TT/summationdisplay
t=11
bt+Q2∆
T+Q3
T,
for some constants Q1,Q2,Q3dependent on{η0,δ,∆,L,D,G,ρ 0,µ,γ, ¯ϵ}, but not on T. This
proves our theorem.
C Additional Experiments for Computer Vision
In this section, we compare RAMDA with other methods on image classification with smaller
datasets. They are:
1. Logistic regression (neural network with no hidden layer) with the MNIST dataset
[23].
2. A modified VGG19 [42] with the CIFAR10 dataset [21].
3. The same VGG19 with the CIFAR100 dataset [21].
4. A modified ResNet50 [14] with the CIFAR10 dataset.
5. The same ResNet50 with the CIFAR100 dataset.
The results are shown in Table 7. In the logistic regression problem, we only perform a single
run, with the initial point being the origin, as it is a convex problem. Moreover, in this
problem, when dealing with ProxSSI,ProxGen, and ProxSGD whose sparsity levels are highly
unstable over iterations, we report their highest weighted group sparsity over all epochs, but
for all other problems, we report the group sparsity level of the final output.
Experiments in this subsection show that RAMDA might sometimes perform worse than
existing methods on smaller problems like CIFAR10/100. Fortunately, for such smaller
problems, the training cost is not very significant, and one can afford to try more algorithms.
20Table 7: Group sparsity and validation accuracy of different methods on image classification
with smaller datasets.
Algorithm Validation accuracy Group sparsity
Logistic Regression/MNIST
ProxSGD 91.31% 39.29%
ProxSSI 91.31% 39.92%
ProxGen 91.31% 39.92%
RMDA 91.34% 57.02%
RAMDA 91.35% 57.40%
VGG19/CIFAR10
MSGD 93.95±0.14% -
ProxSGD 92.82±0.09% 82.76 ±5.42%
ProxSSI 92.81±0.15% 88.40 ±0.23%
ProxGen 92.83±0.05% 86.64 ±0.12%
RMDA 93.13±0.10% 90.22 ±0.06%
RAMDA 92.89±0.13% 86.31 ±0.31%
VGG19/CIFAR100
MSGD 74.07±0.05% -
ProxSGD 71.96±0.15% 72.34±11.9%
ProxSSI 67.29±0.06% 78.58 ±0.34%
ProxGen 68.13±0.36% 75.46 ±0.17%
RMDA 71.96±0.31% 80.88 ±0.11%
RAMDA 70.47±0.25% 65.19 ±0.77%
ResNet50/CIFAR10
MSGD 95.54±0.19% -
ProxSGD 92.36±0.05% 82.18 ±2.67%
ProxSSI 94.04±0.12% 83.67 ±0.63%
ProxGen 94.07±0.12% 80.45 ±0.45%
RMDA 95.11±0.11% 85.64 ±0.12%
RAMDA 93.85±0.10% 81.99 ±1.26%
ResNet50/CIFAR100
MSGD 79.49±0.49% -
ProxSGD 74.54±0.58% 49.29 ±5.91%
ProxSSI 73.65±0.39% 70.38 ±0.74%
ProxGen 73.63±0.43% 65.51 ±3.58%
RMDA 75.62±0.19% 79.97 ±0.27%
RAMDA 69.23±0.86% 68.65 ±1.83%
D Plots of Sparsity Level and Validation Accuracy over Epochs
We provide in Fig. 1 the plots of predictive ability and structured sparsity over epochs of
some representative experiments we have conducted. These experiments are:
1. ResNet50 with the ILSVRC 2012 ImageNet dataset.
2. Transformer-XL with the WikiText-103 dataset.
3. Tacotron2 with the LJSpeech dataset.
4. Logistic Regression with the MNIST dataset.
5. A modified VGG19 with the CIFAR10 dataset.
6. The same VGG19 with the CIFAR100 dataset.
7. A modified ResNet50 with the CIFAR10 dataset.
8. The same ResNet50 with the CIFAR100 dataset.
In the plot for Transformer-XL, one step processes ten batches, and for our batch size
of 64, one epoch consists of 8,401 batches. We further observe in the zoomed-in sparsity
21plots in Fig. 2 that the sparsity level of RAMDA is stable at the final epochs. These
plots corroborates our theory that RAMDA is indeed capable of manifold identification
while achieving competitive prediction performance. On the other hand, in the absence
of manifold identification guarantees, the sparsity levels of ProxSGD ,ProxSSIandProxGen
exhibit oscillations that are sometimes drastic. We note that for the largest problems
Tacotron2 and Transformer-XL, the sparsity levels of RAMDA are still gradually increasing
even at the final epochs. This suggests that if we are willing to run the algorithm for longer,
it is possible that the structured sparsity level could be further improved.
E Experiment with Nuclear-norm Regularization
We further conduct some preliminary experiments with a different regularizer to showcase
that the proposed RAMDA can be applied to structures beyond sparsity. We consider the
structure such that each layer of the neural network is low-rank, induced by imposing one
nuclear-norm regularizer per layer individually by treating each layer as a matrix. Given
a matrixX∈Rm×nof rankr≤min{m,n}with its singular value decomposition (SVD)
X=UΣV⊤, whereU∈Rm×r,V∈Rn×rare orthogonal and the positive definite diagonal
matrix Σ∈Rr×rrepresents the nonzero singular values of X, the nuclear norm of Xis
computed by
∥X∥∗=r/summationdisplay
i=1Σi,i,
and the corresponding proximal operator for λ>0is
proxλ∥·∥∗(X) =UˆΣV⊤,where ˆΣi,i= max{0,Σi,i−λ}, i= 1,...,r.
Given a point X∗with rankr∗, the active manifold of the nuclear norm at X∗is
M(X∗) ={Y|rank(Y) =r∗}.
Using low-rankness to condense neural networks is itself an interesting research topic, but
conducting full SVDs could be rather time-consuming, so applying this structure to larger
problems is challenging but potentially useful. How to exploit this structure for prediction
acceleration and to make the training more efficient, possibly using iterative methods to
compute approximate SVDs, is an interesting topic we plan to investigate in the near future.
Instead, the purpose of the preliminary experiment here is merely for showing that our
method is also applicable to other structures.
We first consider training a simple neural network with six fully-connected layers using
the FashionMNIST dataset [ 46]. Since this is a rather small-scale problem and this is a
image classification problem, we do not expect RAMDA to outperform non-adaptive methods,
especially the RMDAmethod that is also able to identify the active manifold. The goal of
this experiment is just to demonstrate the possibilities of structures beyond sparsity. The
results are shown in Table 8. As we have anticipated, RAMDA is indeed slightly worse than
RMDAregarding the low-rank level and the prediction accuracy, but it is still competitive
and outperforms ProxGen andProxSGD . This exemplifies the potential of RAMDA as well as
RMDAfor training neural networks with other useful structures.
We also conduct an experiment on pretraining a modified vision transformer model [ 29] for
masked image modeling [ 48] using the CIFAR10 dataset. Following the standard practice of
this task, we select the model that gives the lowest validation loss among the last 50 epochs
as the final output. The results are shown in Table 9. We can see that RAMDA attains the
lowest validation loss and has a low-rank level almost identical to that of RMDA. On the
other hand, ProxSGD andProxGen have worse low-rank levels.
NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately
reflect the paper’s contributions and scope?
220 20 40 60 80 100 120 140 160
Epochs0.00.20.40.60.8Validation Accuracy
ResNet50 on ImageNet
MSGD
ProxSGD
ProxGen
RMDA
RAMDA
0 20 40 60 80 100 120 140 160
Epochs0.00.10.20.3Weighted Structured Sparsity
(a) ResNet50 on ImageNet
0 10 20 30 40 50 60 70
Every 5000 Steps20406080100Validation Perplexity
Transformer-XL on WikiT ext-103
Adam
ProxSGD
ProxGen
RMDA
RAMDA
0 10 20 30 40 50 60 70
Every 5000 Steps0.00.10.20.30.4Weighted Structured Sparsity
 (b) Transformer-XL on WikiText-103
0 200 400 600 800
Epochs0.500.751.00Validation Loss
T acotron2 on LJSpeech
Adam
ProxSGD
ProxGen
RMDA
RAMDA
0 200 400 600 800
Epochs0.00.20.40.6Weighted Structured Sparsity
(c) Tacotron2 on LJSpeech
0 100 200 300 400 500
Epochs0.860.880.900.92Validation Accuracy
Logistic Regression on MNIST
ProxSGD
ProxSSI
ProxGen
RMDA
RAMDA
0 100 200 300 400 500
Epochs0.00.20.40.6Weighted Structured Sparsity
 (d) Logistic Regression on MNIST
0 200 400 600 800 1000
Epochs0.000.250.500.751.00Validation Accuracy
VGG19 on CIFAR10
MSGD
ProxSGD
ProxSSI
ProxGen
RMDA
RAMDA
0 200 400 600 800 1000
Epochs0.000.250.500.75Weighted Structured Sparsity
(e) VGG19 on CIFAR10
0 200 400 600 800 1000
Epochs0.00.20.40.60.8Validation Accuracy
VGG19 on CIFAR100
MSGD
ProxSGD
ProxSSI
ProxGen
RMDA
RAMDA
0 200 400 600 800 1000
Epochs0.000.250.500.75Weighted Structured Sparsity
 (f) VGG19 on CIFAR100
0 200 400 600 800 1000
Epochs0.000.250.500.751.00Validation Accuracy
ResNet50 on CIFAR10
MSGD
ProxSGD
ProxSSI
ProxGen
RMDA
RAMDA
0 200 400 600 800 1000
Epochs0.000.250.500.75Weighted Structured Sparsity
(g) ResNet50 on CIFAR10
0 200 400 600 800 1000
Epochs0.000.250.500.75Validation Accuracy
ResNet50 on CIFAR100
MSGD
ProxSGD
ProxSSI
ProxGen
RMDA
RAMDA
0 200 400 600 800 1000
Epochs0.000.250.500.75Weighted Structured Sparsity
 (h) ResNet50 on CIFAR100
Figure 1: Group sparsity level and validation prediction performance v.s epochs. In the plot
for Transformer-XL, one step processes ten batches, and for our batch size of 64, one epoch
consists of 8,401 batches.
23130 140 150
Epochs0.150.200.250.300.35Weighted Structured Sparsity
ResNet50 on ImageNet
ProxSGD
ProxGen
RMDA
RAMDA(a) ResNet50 on ImageNet
60 65 70 75
Every 5000 Steps0.280.300.320.340.360.38Weighted Structured Sparsity
Transformer-XL on WikiT ext-103
ProxSGD
ProxGen
RMDA
RAMDA (b) Transformer-XL on WikiText-103
800 850 900 950
Epochs0.300.350.400.450.500.55Weighted Structured Sparsity
T acotron2 on LJSpeech
ProxSGD
ProxGen
RMDA
RAMDA
(c) Tacotron2 on LJSpeech
450 460 470 480 490 500
Epochs0.00.10.20.30.40.50.6Weighted Structured Sparsity
Logistic Regression on MNIST
ProxSGD
ProxSSI
ProxGen
RMDA
RAMDA (d) Logistic Regression on MNIST
900 920 940 960 980 1000
Epochs0.40.50.60.70.80.9Weighted Structured Sparsity
VGG19 on CIFAR10
ProxSGD
ProxSSI
ProxGen
RMDA
RAMDA
(e) VGG19 on CIFAR10
900 920 940 960 980 1000
Epochs0.600.650.700.750.800.85Weighted Structured Sparsity
VGG19 on CIFAR100
ProxSGD
ProxSSI
ProxGen
RMDA
RAMDA (f) VGG19 on CIFAR100
900 920 940 960 980 1000
Epochs0.800.820.840.86Weighted Structured Sparsity
ResNet50 on CIFAR10
ProxSGD
ProxSSI
ProxGen
RMDA
RAMDA
(g) ResNet50 on CIFAR10
900 920 940 960 980 1000
Epochs0.40.50.60.70.80.9Weighted Structured Sparsity
ResNet50 on CIFAR100
ProxSGD
ProxSSI
ProxGen
RMDA
RAMDA (h) ResNet50 on CIFAR100
Figure 2: Group sparsity level at the last epochs.
24Table 8: Low-rank level and validation accuracy of different methods on training a six-layer
fully-connected neural network with the FashionMNIST dataset for image classification.
Algorithm Validation accuracy Low-rank level
MSGD 89.95±0.29% -
ProxSGD 87.54±0.52% 78.00±0.77%
ProxGen 86.66±0.33% 87.46±4.19%
RMDA 88.19±0.23% 91.88±0.12%
RAMDA 87.99±0.24% 89.59±0.42%
Table 9: Low-rank level and validation loss of different methods on pretraining a modified
vision transformer model using the CIFAR10 dataset for masked image modeling.
Algorithm Validation loss Low-rank level
AdamW 0.0865±0.0001 -
ProxSGD 0.1042±0.0003 82.60±0.34%
ProxGen 0.1120±0.0019 82.64±2.47%
RMDA 0.1054±0.0031 86.23±0.41%
RAMDA 0.1035±0.0016 86.20±0.35%
Answer: [Yes]
Justification: Our claims accurately reflect the paper’s contributions and scope.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the
authors?
Answer: [Yes]
Justification: Limitations of our work are discussed in Section 5 (after Theorem 2)
and Appendix C.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assump-
tions and a complete (and correct) proof?
Answer: [Yes]
Justification: All assumptions are stated clearly in each theorem statement, and all
detailed proofs are provided in Appendix B.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce
the main experimental results of the paper to the extent that it affects the main
claims and/or conclusions of the paper (regardless of whether the code and data are
provided or not)?
Answer: [Yes]
Justification: Algorithm details are all given in the paper, and the parameter settings
are all available in the supplementary materials.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient
instructions to faithfully reproduce the main experimental results, as described in
supplemental material?
Answer: [Yes]
Justification: Our code is provided in the supplementary materials, our data are
public data sets, and sufficient instructions are given in the README in the
supplementary materials.
256.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits,
hyperparameters, how they were chosen, type of optimizer, etc.) necessary to
understand the results?
Answer: [Yes]
Justification: All details are given in either the main paper or the supplementary
materials.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other
appropriate information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We report the mean and standard deviation of the comparison criteria
over three different random initializations.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the
computer resources (type of compute workers, memory, time of execution) needed
to reproduce the experiments?
Answer: [Yes]
Justification: Time of execution is reported in Section 6, and details of the computer
resources are given in Appendix A.3.
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with
the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conform, in every respect, with
the NeurIPS Code of Ethics.
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and
negative societal impacts of the work performed?
Answer: [NA]
Justification: This is a fundamental research work and there is no foreseeable negative
societal impact.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for
responsible release of data or models that have a high risk for misuse (e.g., pretrained
language models, image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models),
used in the paper, properly credited and are the license and terms of use explicitly
mentioned and properly respected?
Answer: [Yes]
Justification: We do cite all of the papers that proposed the models, datasets, and
code we used.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the
documentation provided alongside the assets?
26Answer: [Yes]
Justification: This paper will introduce new open-source software, and we have
provided a README file for documentation of the package.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does
the paper include the full text of instructions given to participants and screenshots,
if applicable, as well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human
subjects.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research
with Human Subjects
Question: Does the paper describe potential risks incurred by study participants,
whether such risks were disclosed to the subjects, and whether Institutional Review
Board (IRB) approvals (or an equivalent approval/review based on the requirements
of your country or institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human
subjects.
27