Dimension-free Private Mean Estimation for
Anisotropic Distributions
Yuval Dagan
School of Computer Science
Tel Aviv University
ydagan@tauex.tau.ac.ilMichael I. Jordan
Department of EECS and Statistics
University of California Berkeley
michael_jordan@berkeley.edu
Xuelin Yang
Department of Statistics
University of California Berkeley
xuelin@berkeley.eduLydia Zakynthinou
Department of EECS
University of California Berkeley
lydiazak@berkeley.edu
Nikita Zhivotovskiy
Department of Statistics
University of California Berkeley
zhivotovskiy@berkeley.edu
Abstract
We present differentially private algorithms for high-dimensional mean estima-
tion. Previous private estimators on distributions over Rdsuffer from a curse of
dimensionality, as they require Ω(d1/2)samples to achieve non-trivial error, even
in cases where O(1)samples suffice without privacy. This rate is unavoidable
when the distribution is isotropic, namely, when the covariance is a multiple of the
identity matrix. Yet, real-world data is often highly anisotropic, with signals con-
centrated on a small number of principal components. We develop estimators that
are appropriate for such signals—our estimators are (ε, δ)-differentially private and
have sample complexity that is dimension-independent for anisotropic subgaussian
distributions. Given nsamples from a distribution with known covariance-proxy Σ
and unknown mean µ, we present an estimator ˆµthat achieves error , ∥ˆµ−µ∥2≤α,
as long as n≳tr(Σ) /α2+ tr(Σ1/2)/(αε). We show that this is the optimal sam-
ple complexity for this task up to logarithmic factors. Moreover, for the case
of unknown covariance, we present an algorithm whose sample complexity has
improved dependence on the dimension, from d1/2tod1/4.
1 Introduction
Machine learning is increasingly deployed in real-world settings to learn about properties of pop-
ulations, both large and small. When the data comes from human populations, it is essential that
algorithm design allows inferring properties of populations without revealing potentially sensitive
information about specific individuals in the population. That sensitive information can be revealed,
inadvertently or adversarially, has been demonstrated in numerous ways, including via reconstruction
attacks [ 21,24], as well as membership-inference attacks [ 56], often targeting sensitive genomic
data [ 34,55,64]. To mitigate the risk of privacy violations in general database theory, Dwork, McSh-
erry, Nissim, and Smith [ 25] proposed the rigorous guarantee of differential privacy (DP), which has
∗Authors ordered alphabetically.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).been widely adopted in industry [ 29,11,33,62,53,4] and government [ 30,1,2]. Algorithms that
are differentially private are guaranteed to not leak too much information about the individuals in a
database.
In the machine learning setting, there is a tension between differential privacy and inferential and
predictive accuracy. It is an ongoing challenge to capture that tension mathematically, in a way
that is applicable to a wide variety of problems and is sufficiently quantitative so as to provide a
guide for real users and real systems designers. A particularly salient theoretical challenge is to
obtain results that capture dimension-dependence—given that machine learning data are often of
high dimensionality and involve significant correlation among dimensions, and given that privacy
is difficult to guarantee in high dimensions, particularly so when there are correlations. Indeed,
differentially private inference suffers from a curse of dimensionality —the sample size nthat is
required to obtain a non-trivial DP learner is often polynomial in the dimension dof the data.
Significant progress has been made in addressing this challenge in recent years by focusing on a
relatively simple inferential task, that of high-dimensional mean estimation. Formally, given a data
set of npoints, X= (X(1), . . . , X(n))∈Rd×ndrawn i.i.d. from a multivariate distribution Pwith
unknown mean µ∈Rd, the goal is to learn µ.
Obtaining low-error private mean estimators in the high-dimensional regime is not always possible.
For example, consider a Gaussian distribution P=N(µ, σ2Id), where Idis the d×didentity
matrix. Here, the sample complexity of any private estimator ˆµachieving error ∥ˆµ−µ∥2≤αis
n= Ω(dσ2/α2+dσ/(αε))[39], where εis the privacy parameter.1The first term corresponds to
the non-private sample complexity and the second term to the additional samples required due to
privacy. Although both depend on d, note that for non-trivial error α= 0.01σ√
dandε= 0.1, the
non-private term is O(1), whereas the dimension-dependence persists in the cost of privacy which is
O(√
d).
In spite of this lower bound, there is still hope for obtaining better dependence on the dimension in
certain cases. This is due to the fact that the lower bound instance assumes that the covariance is
isotropic: a multiple of the identity matrix. However, real-world data are far from being isotropic.
Often, the signal is concentrated in a few directions, while it is significantly weaker in others, as can
be revealed via Singular Value Decomposition (SVD). In these cases, there are several examples of
non-private estimators for a variety of tasks which exploit the structure of the data to achieve lower
sample complexity. Specifically for mean estimation of Gaussian distributions, as in our example
above, only n=O(tr(Σ) /α2)samples are required [ 48] (this number of samples is sufficient even
for robust estimators under the strong contamination model [ 50]). This bound is instance-adaptive, as
the trace of the covariance matrix tr(Σ) equals its upper bound, d∥Σ∥2, in the isotropic case, but can
be much smaller for anisotropic data. Exploiting the non-isotropic structure of the covariance matrix
is also central to the covariance estimation problem with respect to the operator norm (namely, when
the error between the true covariance matrix Σand its estimate ˆΣis measured in terms of ∥ˆΣ−Σ∥2)
[43,65]. A more recent focus is on overparametrized linear regression [ 9], where again the highly
non-isotropic structure of the covariance matrix allows for inference under certain assumptions on the
decay of eigenvalues of the covariance matrix. In all the mentioned results, non-private estimation is
possible when n≪d, including even infinite-dimensional Hilbert spaces.
Returning to private estimation, prior work has obtained optimal bounds for learning the mean of
high-dimensional (sub)Gaussian distributions in the affine-invariant Mahalanobis distance [ 18,39,3,
46,13,47]. These imply an upper bound for learning the mean in Euclidean distance in the order
ofn=O(d∥Σ∥2/α2+dp
∥Σ∥2/(αε)), which is optimal for isotropic, but loose for anisotropic
cases. A folklore estimator, based on [ 41] achieves n= Ω(p
dtr(Σ) /(αε)), while [ 8] are the first to
focus on the anisotropic case and obtain improved bounds for diagonal covariance, achieving error
n= Ω(tr(Σ1/2)/(αε) +√
d/ε). Thus, all previous work requires that the sample complexity is at
leastΩ(√
d), which excludes the high-dimensional scenario we are interested in. We are led to pose
the following question.:
1We focus on approximate (ε, δ)-DP, as opposed to pure(ε,0)-DP. However, we omit any dependence on δ
in the introduction.
2Question 1. Is it possible to obtain good private mean estimators with a sample size that grows
slower with the dimension, or is even dimension-independent, when the covariance of the data is far
from isotropic? What is the optimal sample complexity in the case of known and unknown covariance?
1.1 Our contributions
First, note that no improved bounds are possible for pure DP, as follows directly from the so-called
packing technique [ 32,18] and specifically applying [ 18, Lemma 5.1]: any ε-DP algorithm which
estimates the mean of a Gaussian distribution up to constant accuracy requires n= Ω ( d/ε)samples.
This negative result motivates us to focus on (ε, δ)-differential privacy.
In order to make progress, one would like to utilize the fact that when the covariance is far from
being isotropic, the data is closer to being low-dimensional. Concretely, let Σbe the covariance
matrix of Pandσ2
1≥. . .≥σ2
dits singular values. If the covariance is far from isotropic, there
are only few directions with non-trivial variance. For illustration, if σ1=···=σk= 1, whereas
σk+1=···=σd= 1/d, then the distribution is, in some sense, close to being k-dimensional. Here,
we would like our sample complexity to be of order krather than√
d.
We start by presenting a result in the case where the covariance matrix is known. Here, the bound
depends only onPd
i=1σi= tr(Σ1/2), a quantity allowing less contribution from small singular
values:
Theorem 1.1 (Upper bound, known covariance, informal) .Setε, δ∈(0,1),α > 0. Let X∼
N(µ,Σ)nwith known covariance. There exists an (ε, δ)-differentially private algorithm which, with
probability 0.99, returns an estimate ˆµsuch that ∥ˆµ−µ∥2≤α, and has sample complexity
n=˜O 
tr(Σ)
α2+tr(Σ1/2)p
log(1/δ)
αε+log(1/δ)
ε!
. (1)
The first term corresponds to the non-private sample complexity, whereas the remaining two terms
are due to privacy. The result extends to subgaussian distributions. In the example illustrated above,
this bound indeed yields a dimension-independent complexity of n=˜Oδ(k/α2+k/(αε)).
We show that the sample complexity of Theorem 1.1 is nearly optimal. Indeed, the first summand is
optimal due to [ 19, Theorem 4], while the last summand is optimal by a lower bound in the univariate
case [41]. We show the optimality of the intermediate summand in (1) up to polylogarithmic terms.
Theorem 1.2 (Lower bound, informal) .Any(ε, δ)-DP algorithm which estimates the mean µ∈
[−1,1]dof a Gaussian up to αwith probability 0.99has sample complexity n= Ω
tr(Σ1/2)
αεlog2(d)
.
We now move to the case of unknown covariance. A first approach would be to learn the covariance
approximately, namely, find a matrix Asuch that A⪯Σ⪯CA, for some C > 1, and then use
Ainstead of Σin our known-covariance estimator. However, learning such a matrix Aprivately
requires sample size n= Θ( d3/2)[39,40]. Another approach would be to learn only the diagonal
elements of the covariance [ 41] this would require n=O(√
d/ε)samples. Below, we obtain a
sample complexity whose dependence in the dimension is d1/4, together with a dependendence on
the diagonal elements of the covariance matrix:
Theorem 1.3 (Upper bound, unknown covariance, informal) .Let parameters ε, δ∈(0,1). Let
X∼ N (µ,Σ)nwith unknown covariance Σ. There exists an (ε, δ)-DP algorithm which, with
probability 0.99, returns an estimate ˆµsuch that ∥ˆµ−µ∥2≤α, and has sample complexity
n=˜O
tr(Σ)
α2+Pd
i=1Σ1/2
iip
log(1/δ)
αε+d1/4qPd
i=1Σ1/2
iilog(1/δ)
√αε
. (2)
In general, tr(Σ1/2)≤Pd
i=1Σ1/2
ii, and if Σis diagonal, the two quantities coincide. Our theorem
is in fact more adaptable to easier cases of covariance structure. As a special case, when the
covariance is diagonal and the singular values exhibit an exponential decay, that is, σi=σ1e−(i−1),
thenn=˜O
tr(Σ)
α2+tr(Σ1/2)√
log(1/δ)
αε+log5/3(d) log3/2(1/δ)
ε
samples suffice even under unknown
covariance.
31.2 Techniques
Known covariance. A folklore (ε, δ)-DP algorithm, based on techniques for the univariate case
developed by [ 41], is to filter outliers by privately estimating each individual coordinate of the mean,
µi, up to an additive error of ˜O(Σ1/2
ii)for all i, clipping any sample point to within that range, and out-
putting the mean of the modified data set with added spherical Gaussian noise N(0,tr(Σ) Id/(ε2n2)).
A standard analysis of this procedure yields a sample complexity of n≳tr(Σ)
α2+√
d
ε+√
dtr(Σ)
αε,
where the dependence on δis omitted for clarity. For constant ε, the folklore estimator achieves
privacy for free , that is, the error due to privacy is lower than the error of statistical estimation, when
n≳d.
An improvement to this simple analysis, proposed recently by Aumüller et al. [ 8] for matrices of
diagonal covariance, suggests adding noise N 
0,tr(Σ1/2)Σ1/2/(ε2n2)
instead, which introduces
more noise in the directions of larger variance. Slightly simplifying their result and additionally
ignoring logarithmic factors in d, and the range of µ, their sample complexity is n≳tr(Σ)
α2+√
d
ε+
tr(Σ1/2)
αε.This estimator achieves privacy for free as long as n≳max{∥σ∥2
1/∥σ∥2
2,√
d}, where σ2
denotes the vector of singular values of Σ. While this removes the dimension dependence in the third
term compared to the naïve sample complexity, the second term still requires Ω(√
d)samples. This is
due to the first step of the algorithm (inherited from [ 37]), which performs dindependent estimation
tasks. In both approaches, the pre-processing step is a form of coarse mean estimation which ensures
that the data will not include outliers, and it is the source of sample-inefficiency.
Thus, in our work, we remove outliers, namely vectors too far away from the true mean in one
of the coordinates, using only n=˜O(1/ε)samples, thus completely removing the dependence
ondin the final sample complexity bounds. (Indeed, our estimator achieves privacy for free for
n≳∥σ∥2
1/∥σ∥2
2.) Next, we generalize the approach of [ 8] to general covariance, rather than diagonal.
Finally, we show that the sample complexity is nearly optimal. Specifically:
Our pre-processing is realized by using a polynomial-time filtering algorithm of Tsfadia et al. [ 63].
Given a predicate computed for two data points, so-called FriendlyCore returns a subset X′of the
input, such that all pairs of the remaining, unfiltered data points satisfy the predicate. Its sample
complexity is ˜O(1/ε)foranypredicate, hence it has the potential to yield a dimension-independent
bound. For our purposes, X′needs to satisfy some sensitivity properties. It follows from our analysis
that the filtering should be such that for any two points X(j), X(ℓ)∈X′,∥Σ−1/4(X(j)−X(ℓ))∥2
2≤
˜O 
tr(Σ1/2)
.
The lower bound for (ε, δ)-DP is an application of the standard fingerprinting [ 15,28,39,40]
technique for isotropic Gaussians. A straightforward modification of the technique to anisotropic
covariance Σgives a weaker bound than Theorem 1.2. Instead one needs to choose an appropriate set
of almost-isotropic coordinates whose size scales with tr(Σ1/2), and apply the technique to that set.
Unknown covariance. Moving to the case of unknown covariance, for illustration, we focus on the
simpler, yet fundamental, case where the covariance matrix is diagonal, so that Σ = diag( σ2
1, . . . , σ2
d).
First, the folklore algorithm described in the known-covariance setting, which adds spherical Gaussian
noise, does not require knowledge of the covariance but only of its trace. The trace can be privately
learned with n=˜O(1/ε)samples. Second, we note that with n=˜O(√
d/ε)samples, it is possible
to learn each σiup to a multiplicative constant [ 41]. This allows us to apply the algorithm with known
covariance from Theorem 1.1. However, the first step in this approach still requires Ω(√
d)samples.
Our approach is to combine these two methods. We privately learn the largest k≈ε2n2variances, and
their indices. This is done using the sparse vector technique [ 23] and can be achieved with nsamples.
We use the known-covariance algorithm to estimate the mean in these top kcoordinates, with the
same error bound as in the known-covariance setting. For the mean at the remaining coordinates,
we use the algorithm that only requires knowledge of the trace of the covariance. The error of the
latter estimate is αbot≈√
d∥σbot∥2/(nε), where σbotis the vector containing the lowest d−k
variances. The first observation is that σcontains at least kentries as large as ∥σbot∥∞, hence,
∥σ∥1≥k∥σbot∥∞. Then, by Hölder’s inequality, ∥σbot∥2≤p
∥σbot∥1∥σbot∥∞. Substituting k
yields αbot≈√
d∥σ∥1/(ε2n2), which implies the desired sample complexity bound in Theorem 1.3.
41.3 Related work
Differentially private Gaussian mean estimation. Smith [ 58] proposed estimators for asymp-
totically normal statistics with optimal convergence rates under a certain range of parameters. The
optimal sample complexity for learning the mean of a Gaussian with known covariance in Maha-
lanobis norm under (ε, δ)-DP is n≳d/α2+d/(αε) + log(1 /δ)/εand has been established in a
series of works [ 18,39,3,46], starting from [ 41] in the univariate setting. Given the covariance
matrix Σ, the Mahalanobis distance between the estimate ˜µand the true mean µis defined as:
∥˜µ−µ∥Σ=∥Σ−1/2(˜µ−µ)∥2.When Σ =Id, the Mahalanobis and Euclidean norms coincide. The
Mahalanobis distance yields an affine-invariant accuracy guarantee, and ∥˜µ−µ∥Σ≤αimmediately
implies ∥˜µ−µ∥2≤αp
∥Σ∥2. However, the power of the Mahalanobis guarantee is overshadowed
by the fact that even for α=√
d, a large sample size, namely n= Ω(√
d), is required, which
excludes the high-dimensional scenario we are interested in.2Furthermore, confidence sets induced
by guarantees in the Euclidean distance have the pleasant property of being more easily constructible.
Beyond global sensitivity. There are several lines of work within differential privacy which aim to
satisfy some form of instance-adaptive accuracy guarantee, as we do. General purpose frameworks
which aim to privately estimate a statistic of the data, with error which adapts to “good” data
sets, include propose-test-release [ 22], smooth-sensitivity [ 52], and Lipschitz extensions [ 12,42].
Our method follows the same high-level structure as propose-test-release. The latter has been
combined with robust estimators to yield optimal private learners for several tasks [ 13,47]. Even
more generally, [ 6,35] give a black-box method which transforms robust estimators to private ones
via the inverse-sensitivity mechanism [ 5] (see [ 59] for a discussion on inverse-sensitivity). As there
exist optimal robust estimators for the mean of anisotropic Gaussians [ 50], this would be a viable
approach, but the volumetric analysis of the transformation involves terms which depend on the
dimension. Tsfadia et al. [ 63] propose a filtering method which yields private aggregators whose error
adapts to the diameter of the input data set. It is their method that we utilize for our upper bounds.
A series of works formalize instance-optimality for private estimation of empirical [ 5,37,20] or
population [ 49,7] quantities. These are all generally well-suited to our setting but either do not adapt
to high dimensions, or a direct application would require n≳p
dtr(Σ) /(αε).
Nikolov and Tang [ 51] study instance-optimality specifically for Gaussian noise mechanisms, albeit
for data that belong in a bounded convex set. Although this is not the case for Gaussian data, it is
worth noting that our error rates match those of [ 51], which hold for arbitrary distributions over K,
when the bounded set is K=µ+ Σ1/2Bd(1). Privately learning Khowever would require more
samples.
Privately learning nuisance parameters. Karwa and Vadhan [ 41] learn (a constant multiple of) the
variance of a univariate Gaussian using n=˜O(log(1 /δ)/ε)samples. In high dimensions, privately
learning the covariance matrix of a Gaussian in spectral norm requires n≳d3/2samples [ 39,40],
which is more than one needs to learn the mean under known covariance. Brown et al. [ 13] avoid the
bottleneck of private covariance estimation, showing that the sample complexity of Gaussian mean
estimation under known covariance with respect to Mahalanobis distance can in fact be matched,
even when the covariance is unknown. Their tools also follow the propose-test-release approach
and could be modified to fit our setting, but the privacy analysis would still require n≳d. Singhal
and Steinke [ 57] learn a subspace in which the majority of the data lie, which could be used as a
pre-processing step, followed by projection. However, to recover the set of top keigenvectors, they
require that there exists a large gap between the two consecutive variances, that is, σk≥poly( d)σk+1.
Comparison with [ 8].The paper by Aumüller et al. [ 8] is the closest work to ours, aiming to
find sample-efficient mean estimators with respect to the Euclidean norm in the anisotropic case.
Their work focuses on the less general case of diagonal, (almost) known covariance. The sample
complexity of their estimator requires n≳√
d, whereas our estimator for the known covariance case
is dimension-independent, and, as we prove, optimal. However, the focus in [ 8] is on estimators
that satisfy the stricter privacy guarantee of ρ-zCDP, which forces the need for dimension-dependent
sample size. This is the key contrast with our dimension-free philosophy. As an interesting distinction,
2This limitation is due to the fact that the Mahalanobis distance equalizes the variance across all directions
and forces us to make inferences even in directions where the distribution has particularly small variance.
5Aumüller et al. [ 8] provide accuracy guarantees with respect to the ℓpnorm (the upper bounds) for
slightly more general classes of so-called well-concentrated distributions, which include subgaussians.
It would be interesting to establish optimal private mean estimation bounds with respect to general
ℓpnorms. In fact, the optimal non-private sample complexity of Gaussian mean estimation, with
matching upper and lower bounds, with respect to general norms has been established only recently,
and it depends on the Gaussian mean width of the set induced by the unit dual ball of the norm [ 19].
2 Preliminaries
We write [n] ={1, . . . , n },logdenotes the natural logarithm, and Bd(c, r)denotes the d-dimensional
Euclidean ball with radius rand center c. We omit cifc= 0.
We introduce differential privacy here. We say that X, X′areneighboring data sets if either
∃j∈[|X|]such that X′=X\X(j)or∃j∈[|X′|]such that X=X′\X′(j).3Differentially private
algorithms have indistinguishable output distributions on neighboring data sets.
Definition 2.1 ((ε, δ)-indistinguishability) .Two distributions P, Q over domain Ware(ε, δ)-
indistinguishable, denoted by P≈ε,δQ, if for any measurable subset W⊆ W ,
Pr
w∼P[w∈W]≤eεPr
w∼Q[w∈W] +δand Pr
w∼Q[w∈W]≤eεPr
w∼P[w∈W] +δ.
Definition 2.2 (Differential Privacy [ 25]).A randomized algorithm A:X∗→ W is(ε, δ)-
differentially private if for all neighboring data sets X, X′we have A(X)≈ε,δA(X′). We say that
algorithm Asatisfies pure differential privacy if it satisfies the definition for δ= 0.
Differential privacy satisfies several useful properties, such as post-processing and composition [25,
27]. For further details and guarantees of standard DP mechanisms, see Section A.
Our estimators will use the BasicFilter procedure of Tsfadia et al. [ 63], whose detailed definition is
presented in Section 3 . They provide a framework which allows us to extend an algorithm which
is private only for “easy” pairs of data sets , to an algorithm that is private for any worst-case pair.
“Easy” pairs of data sets are modelled with respect to a predicate fbetween two data points:
Definition 2.3 (f-friendly, Def. 1.1 [ 63]).LetXbe a data set over Xand let f:X2→ {0,1}
be a predicate. We say Xisf-friendly if for all x, y∈Xthere exists z∈ X such that f(x, z) =
f(z, y) = 1 .
Definition 2.4 (f-friendly DP, Def. 1.3 [ 63]).An algorithm Ais called f-friendly (ε, δ)-DP if for
any neighboring data sets X, X′, such that X∪X′isf-friendly, A(X)≈ε,δA(X′).
Theorem 2.5 (Theorem 4.11 [ 63]).LetAbe an f-friendly (ε, δ)-DP algorithm. Given data set
X, letv= BasicFilter( X, f, α = 0) andC(X) ={X(j)}{j:vj=1}. Then B(X) :=A(C(X))is
(2(eε−1)ε,2eε+2(eε−1)δ)-DP .
We assume data are drawn from subgaussian distributions, which include Gaussians.
Definition 2.6 (Subgaussian distributions) .The random vector Xwith mean µis subgaussian with a
p.s.d. covariance matrix proxy Σif for any λand any v∈Rd,Eeλ⟨X−µ,v⟩≤eλ2v⊤Σv/2.
Lemma 2.7 (Norm of the subgaussian vector [ 36,65]).LetX= (X(1), . . . , X(n))be drawn i.i.d.
from the subgaussian distribution with mean µand covariance-proxy Σ. With probability at least
1−β,∥1
nPn
j=1X(j)−µ∥2≤p
tr(Σ) /n+p
2∥Σ∥2log(1/β)/n.
3 Nearly-matching upper and lower bounds under known covariance
Algorithm 1 proceeds in two simple steps. The first step filters out outliers so that all remaining
pairs of data points satisfy the re-scaled distance predicate distM,λand, assuming enough data points
remain, the second step releases their empirical mean along with appropriate Gaussian noise.
We retrieve the folklore result, by taking M=Id, λ≈p
tr(Σ) , which is known (otherwise, can
be easily privately estimated as in Section 4). The filtering then guarantees that all pairs of points
3This is the so-called add/remove model of DP, which will be convenient for our use of prior work. The same
privacy guarantees will also hold for the swap model, where dHam(X, X′)≤1, up to constant factors.
6are within distancep
tr(Σ) , and adds spherical Gaussian noise with covariance tr(Σ) Id/(ε2n2). To
retrieve the optimal bound, take M= Σ, which splits the privacy budget unevenly among coordinates.
Then, λ≈p
tr(Σ1/2)and the Gaussian noise has covariance tr(Σ1/2)Σ1/2/(ε2n2), as in [8].
Algorithm 1 Private Re-scaled Averaging: AvgM,λ,ε,δ (X)
Require: Data set X= (X(1), . . . , X(n))T∈Rn×d. Privacy parameters: ε, δ > 0. Failure
probability β >0. Symmetric invertible matrix M. Parameter λ.
1:LetdistM,λ(x, y) = 1{∥M−1/4(x−y)∥2≤λ}.
2:v= BasicFilter( X,distM,λ, α= 0) .
3:LetC={X(j)}{j:vj=1}.
4:Compute ˆnC=|C| −log(1/δ)
ε+zwhere z∼Lap(1
ε).
5:if|C|= 0orˆnC≤0then
6: return ⊥.
7:return ˆµ=1
|C|P
x∈Cx+ηwhere η∼ N
0,8 log(1 .25/δ)λ2
ε2ˆn2
CM1/2
.
8:procedure BasicFilter (X, f, α ) ▷Algorithm 4.3 from [63].
9: forj= 1, . . . , n do
10: Letzj=Pn
k=1f(X(j), X(k))−n/2.
11: Sample vj= Bern( pj), where pj=

0, ifzj≤0,
1, ifzj≥(1/2−α)n,
zj
(1/2−α)n,otherwise.
12: return v= (v1, . . . , v n)
Theorem 3.1. Letε∈(0,10), δ∈(0,1), α > 0, β∈(0,1). Algorithm 1 is (ε, δ)-differentially
private. Let Xbe a data set of size n, drawn from a subgaussian distribution with covariance-proxy
Σand mean µ. Given M= Σ,λ≥p
2 tr(M−1/4ΣM−1/4) + 2q
2∥M−1/4ΣM−1/4∥2log(n
β),
with probability at least 1−β, Algorithm 1 returns ˆµsuch that ∥ˆµ−µ∥2≤α, as long as
n=˜Ω 
tr(Σ) + ∥Σ∥2log1
β
α2+tr(Σ1/2)q
log1
δ
αε+q
∥Σ∥2log1
δlog1
β
αε+log1
δβ
ε!
,(3)
where ˜Ωhides constants and a log factor of the third term multiplied with itself.
The theorem holds more generally for any symmetric invertible M, andλsatisfying the assumptions.
We sketch the proof of Theorem 3.1 next. All remaining details are in Appendix B.
Proof sketch. We start with the accuracy analysis. First we show that the original dataset Xpasses
through BasicFilter (i.e., C=X) with high probability. It suffices to show that each pair j̸=
k∈[n], satisfies distM,λ(X(j), X(k)) = 1 with probability 1−β/n2. Observe that for j̸=k,
M−1/4(X(j)−X(k))is subgaussian with mean 0 and covariance proxy 2M−1/4ΣM−1/4. By
Lemma 2.7 and our setting of λ, indeed ∥M−1/4(X(j)−X(k))∥2≤λfor each pair with probability
1−β/n2. We condition on C=X. With high probability by the CDF of the Laplace distribution
and since |C|=n= Ω(log(1 /δβ)/ε),ˆnC= Ω( n). Thus, the algorithm does not abort, and
returns estimate ˆµ. It remains to upper bound the total error of ˆµ. This is at most the error of the
empirical mean plus the error due to noise ∥η∥2. By Lemma 2.7, with high probability, the former is
˜O(p
tr(Σ) /n)and the latter ˜O(λp
tr(M1/2)/(εn)). Substituting M= Σ, and the value for λ, the
total error becomes ˜O(p
tr(Σ) /n+ tr(Σ1/2)/(εn)), which yields the stated sample complexity.
The privacy analysis follows the steps of [ 63, Claim 3.4]. By Theorem 2.5, it suffices to show that lines
4-7 of Algorithm 1, namely, A, aredistΣ,β-friendly DP. Consider neighboring inputs X, X′, differing
in the n-th data point w.l.o.g., that is X′=X\X(n). Since ||X| − |X′||= 1, by the guarantees of
the Laplace mechanism, the r.v.s ˆnX,ˆnX′in Line 4 are (ε,0)-indistinguishable. Moreover, they both
satisfy ˆnX,ˆn′
X<|X|with probability 1−δ/2>1/2. Conditioning on this event for the remainder of
the sketch, we can fix ˆnX= ˆn′
X= ˆn <|X|, for some value ˆn. Ifˆn≤0, both runs abort. Otherwise,
7it suffices to show that Line 7 adds sufficient noise to maintain privacy. By post-processing, since
Mis not data-dependent, this is equivalent to ensuring that N(M−1/41
|X|P|X|
i=1X(i), v2Id)≈ε,δ
N(M−1/41
|X|−1P|X|−1
i=1X(i), v2Id)where v= (2λ/ˆn)(p
2 log(1 .25/δ)/ε). This is true by the
guarantees of the Gaussian mechanism applied to f(X) = M−1/4P|X|
i=1X(i)/|X|, whose ℓ2-
sensitivity for distΣ,λ-friendly X, X′can be upper bounded by 2λ/|X| ≤2λ/ˆn(since 0<ˆn <|X|,
by assumption). By composition, Aindeed satisfies distM,λ-friendly (O(ε), O(δ))-DP.
We show that the sample complexity of Theorem 3.1 is optimal. We briefly explain our lower bound
construction here. All remaining details are in Appendix C.
Proof Sketch of Theorem 1.2. LetΣ = diag( σ2). Assume w.l.o.g. that σ2
1≥. . .≥σ2
d. Partition
the set of coordinates into buckets Sk={i∈[d] :σi∈σ1·(2−k,2−k+1]},∀k∈[log(d)]
andSlog(d)+1= [d]\S
k∈[log(d)]Sk. We have thatP
k∈[log(d)+1]P
i∈Skσi=∥σ∥1. Consider
the bucket Swhich contributes the most to this sum and let σSbe the maximum variance in this
bucket. It must be that |S| ≥∥σ∥1
(log(d)+1)σS.The lower bound of [ 39, Theorem 6.5] for isotropic
Gaussians, implies that any (ε, δ)-private mean estimator which returns, with constant probability,
an estimate ˆµSwith error αfor the coordinates in S(note that they are all within a factor of 2),
requires n= Ω(|S|σS/(αεlog(d))) = Ω( ∥σ∥1/(αεlog2(d)))samples. As an estimator for the
d-dimensional Gaussian mean restricted to S, would give us such a ˆµS, the statement follows.
4 Handling unknown covariance
In this section we consider the case of unknown covariance. First, recall that Ω(d3/2)samples are
required to privately learn the covariance matrix in spectral norm [ 40], which is prohibitive. The
lower bound instance is an almost-isotropic Gaussian, which means that anisotropic distributions may
circumvent it. Still, the superlinear dependence on dimplies that this approach will yield suboptimal
sample complexity for mean estimation. Avoiding private covariance estimation, Brown et al. [ 13]
propose a “covariance-aware” private mean estimator which returns the mean with Gaussian noise
which scales with the empirical covariance matrix of the data set ΣX, asN(0, λ2
MΣX/(ε2n2))for
appropriate factor λ2
M. Since adding data-dependent noise can break privacy, a pre-processing step is
required to ensure that no outliers exist in the data set with respect to the empirical covariance, roughly
ensuring that ∥Σ−1/2
X(X(k)−X(j))∥2≤λM, for all j̸=k∈[n]. In our case, to maintain the
accuracy guarantee of the known-covariance case, the Gaussian noise should be N(0, λ2Σ1/2
X/(ε2n2))
and all data points should satisfy ∥Σ−1/4
X(X(k)−X(j))∥2≤λ. Note that n≥tr(Σ) /∥Σ∥2samples
suffice for the empirical covariance to be close to the true covariance Σin spectral norm [ 43], so
applying the algorithm from [ 13] could maintain accuracy while still allowing a dimension-free
sample complexity. Unfortunately, we still cannot use this approach because n≥dsamples are
required for the privacy analysis to go though, namely, for neighboring data sets X, X′it holds that
N(0,Σ1/2
X)≈ε,δN(0,Σ1/2
X′)forε≈d/n, which forces us to take n≥d/εsamples. The same is
true for the follow-up works of [ 14,44] which give polynomial-time versions of this algorithm with
slightly better statistical guarantees.
Luckily, our accuracy guarantee does not require the variance estimate in all directions to be accurate.
For example, consider all directions with variance at most ∥Σ∥2/d. Adding spherical Gaussian noise
to these directions maintains a dimension-free error, without requiring tighter estimates for their
variance. Thus, on a high level, our approach for mean estimation in the unknown covariance case is
to identify and estimate as many of the top variances as our sample size allows, which turns out to be
k≈ε2n2, while adding spherical Gaussian noise to the remaining ones.
We sketch the proof of the following theorem. All remaining details are in Appendix D.
Theorem 4.1. Let parameters ε, δ∈(0,1). Let X∼ N (µ,Σ)nwith unknown covariance Σ.
Algorithm 2 is (ε, δ)-differentially private and, with probability 1−β, returns an estimate ˆµsuch
8Algorithm 2 Private Re-scaled Averaging with Unknown Covariance
Require: Data set X= (X(1), . . . , X(2n))T∈R2n×d. Privacy parameters: ε, δ > 0. Failure
probability β >0.
1:Require n= Ω
log2(d) + log(1
δβ)q
log(1
δ) log( d)/ε
.
2:Letk←ε2n2/ 
log2(d) log(1 /δ) log2(1/δβ) + log( εn)
andℓ←Θ(log( d)).
3:Split the dataset into two equal halves: XvarandXmean.
4:SplitXvarintom=⌊n
2ℓ⌋groups of size 2ℓ. Define X(j,r)as the r-th sample in the j-th group.
5:foreach group j= 1tomand each dimension i= 1toddo
6: Define V(j)
i=1
2ℓPℓ
r=1(X(j,2r−1)
i −X(j,2r)
i)2.
7:ˆR←FindKthLargestVarianceε,δ(V, k).
8:Itop←TopVarε,δ(V,ˆR/8, k)andIbot←[d]\Itop.
9:foreachi∈Itopdo
10: Estimate ˆΣii←VarianceSum ε′,δ′,β′(V,{i})forε′←ε√
klog(1/δ),δ′←δ
k,β′←β
k.
11:Compute ˆSbot←VarianceSum ε,δ,β(V, Ibot).
12:ˆµtop←AvgM,λ,ε,δ (Xmean[Itop]), where M= diag( {ˆΣii}Itop),λ=˜ΘqP
i∈ItopˆΣ1/2
ii
.
13:ˆµbot←AvgM,λ,ε,δ (Xmean[Ibot]), where M=Id,λ=˜Θ(pˆSbot).
14:return (ˆµtop,ˆµbot)
that∥ˆµ−µ∥2≤α, as long as
n=˜Ω
log2(d) +tr(Σ)
α2+Pd
i=1Σ1/2
iiq
log1
δ
αε+d1/4qPd
i=1Σ1/2
iilog5/4(1
δ) log( d)
√αε
,(4)
where the symbol ˜Ωhides multiplicative logarithmic factors in 1/β.
Next, we describe Algorithm 2 and introduce some of its subroutines along with their guarantees.
All omitted proofs are in Appendix D. Our algorithm receives a data set X(1), . . . , X(n), where each
X(i)is ad-dimensional vector distributed as N(µ,Σ). The algorithm starts by splitting the dataset
intom=⌊n/(2ℓ)⌋groups each of size 2ℓ, where ℓ= Θ(log d). Denote the elements of each group
jbyX(j,1), . . . , X(j,2ℓ).Within each group j, for each coordinate i, we compute an estimate V(j)
i
forΣii:V(j)
i=1
2ℓPℓ
r=1(X(j,2r−1)
i −X(j,2r)
i)2.For convenience, we define what it means for the
V(j)
ivariables to provide a good estimate of the set of {Σii}i∈[d].
Definition 4.2. Given variances Σ11, . . . , Σddand given a set of estimates, V={V(j)
i}j∈[m],i∈[d],
we say that Visvalid if|{j:∀i∈[d],Σii/2≤V(j)
i≤2Σii}| ≥4m/5.
Proof sketch of Theorem 4.1. For ease of notation, we assume that Σ = diag( σ2). We start from
the accuracy analysis. Assume nsatisfies the sample complexity bound of Eq. (4). By Chernoff
bound, since ℓ= Θ(log( d))andm= Ω(log(1 /β)), with probability 1−β,Vis valid. We use the
estimates V(j)
ias inputs to private procedures: FindKthLargestVariance( V, k)(compute the k-th
largest variance up to a multiplicative constant), VarianceSum( V, I)(compute the sumP
i∈Iσ2
i
up to a multiplicative constant), and TopVar( V, R, k )(identify the indices of the at-most- klargest
variances σ2
i≥R). The first two tasks can be implemented by the Stable Histogram algorithm [ 16]
ifm= Ω(log(1 /δ)/ε).TopVar can be implemented via the Sparse Vector technique [ 26,54,31],
ifm= Ω(p
klog(1/δ) log( d/β)/√εn). Both are satisfied for the given m=n/2ℓ. With these
procedures, the algorithm privately learns the k-th largest variance R, identifies the top kcoordinates,
Itop, and learns estimates {ˆσi}Itopthat are accurate up to a multiplicative constant.
Next, we estimate the mean µin the coordinates Itopseparately from Ibot:= [d]\Itop, denoted
byµtopandµbot, respectively. Denote vector σtop= ({σi}i∈Itop)andσbot= ({σi}i∈Ibot).
To estimate µtop, Algorithm 1, given input vectors X(1), . . . , X(n), restricted to coordinates Itop,
9diagonal matrix M, with Mii= ˆσ2
i(assume that the rows and columns of Mare indexed by Itop),
andλ≈p
∥σtop∥1, returns an estimate ˆµtopwith error α(since the sample complexity of Eq. (4)is
larger than the one required by Theorem 3.1).
To estimate µbot, we do not know the variances, so we use the naïve approach. We first call
VarianceSum once again, to provide an estimate ˆtof∥σbot∥2up to a multiplicative constant.
Given ˆt, we again call Algorithm 1, now for a (d−k)-dimensional estimation problem. Given
input vectors X(1), . . . , X(n), restricted to coordinates Ibot, matrix M=Id−k, and λ≈ˆt, Algo-
rithm 1 returns an estimate ˆµbotwith error αas long as n=˜Ω(√
d∥σbot∥2/(αε)). By Hölder’s
inequality, ∥σbot∥2≤p
∥σbot∥1∥σbot∥∞≤p
∥σ∥1∥σbot∥∞.By the guarantees of TopVar and
FindKthLargestVariance ,∥σbot∥∞is smaller than the k-th largest variance of Σup to a multi-
plicative constant, which, in turn, must be smaller than ∥σ∥1/k. Substituting this above, we obtain
that it suffices for the stated sample complexity to additionally satisfy n=˜Ω(√
d∥σ∥1/(√
kαε)),
which can be confirmed by substituting the definition for k.
The privacy guarantee follows directly by composition of O(1) (ε, δ)-DP mechanisms.
Remark 1.We note that the sample complexity of Algorithm 2 in fact depends on the decay of the
diagonal elements of Σ, and can yield improved bounds for easier instances. In particular, the error
of the algorithm due to privacy is in the order of ∥σItop∥1/(εn) +p
|Ibot|∥σIbot∥2/(εn). Thus, if
σfollows an exponential decay, i.e., the i-th largest variance is proportional to e−(i−1), or all σbot
variances are smaller than ∥σ∥1/d, then it suffices to learn only the top k= log( d)variances and the
error almost matches that of the known-covariance case, up to additional logarithmic factors in d,
1/δ. Moreover, identifying easier instances is possible by computing a private histogram over log(d)
buckets of the form (2−j,2−j+1]∥σ∥∞, given n=˜O(log(d)/ε)samples [16, 41].
Thus, we can determine special cases where the decay of Σallows us to achieve the optimal rate of
Theorem 1.1 even with unknown diagonal covariance. But without further assumptions, our algorithm
has sample complexity that depends on d1/4. The question of the optimal sample complexity for
mean estimation in the case of unknown covariance, which captures anisotropic distributions, remains
open.
5 Conclusion and future work
We present (ε, δ)-differentially private mean estimators for subgaussian distributions with error
αas measured in Euclidean distance, with high probability, as long as the sample size is n=
˜Θ 
tr(Σ) /α2+ tr(Σ1/2)/(αε)
. The sample complexity is thus dimension-independent when the
covariance is highly anisotropic. We show that this is the optimal sample complexity for this task
up to logarithmic factors. We also present an algorithm in the more challenging case of unknown
covariance, whose sample complexity has improved dependence on the dimension, that is, d1/4.
In the known covariance case, the dependence on log(1/δ)could possibly be decoupled from the
tr(Σ1/2)/(αε)term. This is an artifact of the Gaussian noise added for privacy and can possibly be
avoided using mean estimators based on the exponential mechanism, as in the spherical Gaussian
case [ 13,3,35], but the volumetric arguments involved in their analysis incur factors dependent on d,
which seem hard to overcome.
A more interesting direction for future work is the case of unknown covariance. We can determine
special cases where the decay of Σallows us to achieve the optimal rate of Theorem 1.1 with unknown
diagonal covariance. What is the appropriate norm in which one needs to learn Σfor the current
known-covariance approach to be accurate, and how many samples are needed for this task privately?
More generally, the optimal sample complexity of mean estimation in the unknown (even diagonal)
covariance case for anisotropic distributions (possibly achieved by an algorithm which doesn’t follow
the same structure) is an open question.
Acknowledgments and Disclosure of Funding
We thank NeurIPS reviewers for suggestions on improving the clarity of this manuscript. This work
was done while both LZ and YD were postdoctoral fellows in the Simons Institute for the Theory
10of Computing, funded by FODSI. We also wish to acknowledge funding from the European Union
(ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the
author(s) only and do not necessarily reflect those of the European Union or the European Research
Council Executive Agency. Neither the European Union nor the granting authority can be held
responsible for them.
References
[1]John M. Abowd. The US Census Bureau adopts differential privacy. In ACM International
Conference on Knowledge Discovery & Data Mining , KDD ’18, pages 2867–2867, 2018.
[2]John M. Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Micah Heineck,
Christine Heiss, Robert Johns, Daniel Kifer, Philip Leclerc, Ashwin Machanavajjhala, Brett
Moran, William Sexton, Matthew Spence, and Pavel Zhuravlev. The 2020 Census Disclosure
Avoidance System TopDown Algorithm. Harvard Data Science Review , Special Issue 2, June
2022.
[3]Ishaq Aden-Ali, Hassan Ashtiani, and Gautam Kamath. On the sample complexity of privately
learning unbounded high-dimensional Gaussians. In Proceedings of the 32nd International
Conference on Algorithmic Learning Theory , ALT ’21, March 2021.
[4]Apple Differential Privacy Team. Learning with privacy at scale. Apple Machine Learning Jour-
nal, 1(8), 2017. https://docs-assets.developer.apple.com/ml-research/papers/
learning-with-privacy-at-scale.pdf .
[5]Hilal Asi and John C. Duchi. Instance-optimality in differential privacy via approximate inverse
sensitivity mechanisms. In Advances in Neural Information Processing Systems 33 , NeuRIPS
’20, pages 14106–14117, Dec 2020.
[6]Hilal Asi, Jonathan Ullman, and Lydia Zakynthinou. From robustness to privacy and back.
InProceedings of the 40th International Conference on Machine Learning , ICML ’23, pages
1121–1146. PMLR, Jul 2023.
[7]Hilal Asi, John C. Duchi, Saminul Haque, Zewei Li, and Feng Ruan. Universally instance-
optimal mechanisms for private statistical estimation. In Proceedings of 37th Conference on
Learning Theory , COLT ’24, pages 221–259. PMLR, Jul 2024.
[8]Martin Aumüller, Christian Janos Lebeda, Boel Nelson, and Rasmus Pagh. PLAN: Variance-
aware private mean estimation, June 2023. https://arxiv.org/abs/2306.08745 .
[9]Peter L Bartlett, Philip M Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in
linear regression. Proceedings of the National Academy of Sciences , 117(48):30063–30070,
2020.
[10] Amos Beimel, Hai Brenner, Shiva Kasiviswanathan, and Kobbi Nissim. Bounds on the sample
complexity for private learning and private data release. Machine Learning , 94:401–437, 2014.
doi: 10.1007/s10994-013-5404-1.
[11] Andrea Bittau, Úlfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghunathan, David
Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes, and Bernhard Seefeld. PROCHLO:
Strong privacy for analytics in the crowd. In ACM Symposium on Operating Systems Principles ,
SOSP ’17, pages 441–459, 2017.
[12] Jeremiah Blocki, Avrim Blum, Anupam Datta, and Or Sheffet. Differentially private data
analysis of social networks via restricted sensitivity. In 4th ACM Conference on Innovations in
Theoretical Computer Science , ITCS ’13, pages 87–96, 2013.
[13] Gavin Brown, Marco Gaboardi, Adam Smith, Jonathan Ullman, and Lydia Zakynthinou.
Covariance-aware private mean estimation without private covariance estimation. In Advances
in Neural Information Processing Systems 34 , NeurIPS ’21, pages 7950–7964, 2021.
[14] Gavin Brown, Samuel B. Hopkins, and Adam Smith. Fast, sample-efficient, affine-invariant
private mean and covariance estimation for subgaussian distributions. In Proceedings of the
36th Conference on Learning Theory , COLT ’23, pages 5578–5579. PMLR, Jul 2023.
11[15] Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approxi-
mate differential privacy. In ACM Symposium on the Theory of Computing , STOC ’14, pages
1–10, 2014.
[16] Mark Bun, Kobbi Nissim, and Uri Stemmer. Simultaneous private learning of multiple concepts.
InProceedings of the 7th ACM Conference on Innovations in Theoretical Computer Science ,
ITCS ’16, pages 369–380. ACM, 2016.
[17] Mark Bun, Thomas Steinke, and Jonathan Ullman. Make up your mind: The price of online
queries in differential privacy. In Proceedings of the 28th Annual ACM-SIAM Symposium on
Discrete Algorithms , SODA ’17, pages 1306–1325. SIAM, 2017.
[18] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypothesis
selection. In Advances in Neural Information Processing Systems 32 , NeurIPS ’19, pages
156–167, 2019.
[19] Jules Depersin and Guillaume Lecué. Optimal robust mean and location estimation via convex
programs with respect to any pseudo-norms. Probability Theory and Related Fields , 183(3):
997–1025, 2022.
[20] Travis Dick, Alex Kulesza, Ziteng Sun, and Ananda Theertha Suresh. Subset-based instance
optimality in private estimation. In Proceedings of the 40th International Conference on
Machine Learning , ICML’23. JMLR, 2023.
[21] Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy. In Proceedings
of the 22nd ACM Symposium on Principles of Database Systems , PODS ’03, pages 202–210.
ACM, 2003.
[22] Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In Proceedings of the
41st ACM Symposium on Theory of Computing , STOC ’09, pages 371–380. ACM, 2009.
[23] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Founda-
tions and Trends in Theoretical Computer Science , 9(3-4):211–407, 2014.
[24] Cynthia Dwork and Sergey Yekhanin. New efficient attacks on statistical disclosure control
mechanisms. In Annual International Cryptology Conference , pages 469–480. Springer, 2008.
[25] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Conference on Theory of Cryptography , TCC ’06, pages
265–284, 2006.
[26] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N. Rothblum, and Salil P. Vadhan. On the
complexity of differentially private data release: efficient algorithms and hardness results. In
Proceedings of the 41st ACM Symposium on Theory of Computing , STOC ’09, pages 381–390.
ACM, 2009.
[27] Cynthia Dwork, Guy N. Rothblum, and Salil P. Vadhan. Boosting and differential privacy. In
Proceedings of the 51st IEEE Annual Symposium on Foundations of Computer Science , FOCS
’10, pages 51–60. IEEE, 2010.
[28] Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan. Robust
traceability from trace amounts. In Proceedings of the 56th IEEE Annual Symposium on
Foundations of Computer Science , FOCS ’15, 2015.
[29] Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: Randomized aggregatable
privacy-preserving ordinal response. In ACM Conference on Computer and Communications
Security , CCS ’14, 2014.
[30] Samuel Haney, Ashwin Machanavajjhala, John M Abowd, Matthew Graham, Mark Kutzbach,
and Lars Vilhuber. Utility cost of formal privacy for releasing national employer-employee
statistics. In Proceedings of the 2017 ACM International Conference on Management of Data ,
SIGMOD ’17, pages 1339–1354. ACM, 2017.
12[31] Moritz Hardt and Guy Rothblum. A multiplicative weights mechanism for privacy-preserving
data analysis. In IEEE Symposium on Foundations of Computer Science , FOCS ’10, pages
61–70, 2014.
[32] Moritz Hardt and Kunal Talwar. On the geometry of differential privacy. In Proceedings of the
42nd ACM Symposium on Theory of Computing , STOC ’10, 2010.
[33] Florian Hartmann and Peter Kairouz. Distributed differential pri-
vacy for federated learning, 2023. https://research.google/blog/
distributed-differential-privacy-for-federated-learning/ .
[34] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill
Muehling, John V . Pearson, Dietrich A. Stephan, Stanley F. Nelson, and David W. Craig.
Resolving individuals contributing trace amounts of DNA to highly complex mixtures using
high-density SNP genotyping microarrays. PLoS Genetics , 4(8):e1000167, 2008.
[35] Samuel B. Hopkins, Gautam Kamath, Mahbod Majid, and Shyam Narayanan. Robustness
implies privacy in statistical estimation. In Proceedings of the 55th Annual ACM Symposium on
Theory of Computing , STOC ’23, page 497–506, 2023.
[36] Daniel Hsu, Sham Kakade, and Tong Zhang. A tail inequality for quadratic forms of subgaussian
random vectors. Electronic Communications in Probability , 17:1 – 6, 2012.
[37] Ziyue Huang, Yuting Liang, and Ke Yi. Instance-optimal mean estimation under differential
privacy. In Advances in Neural Information Processing Systems 34 , pages 25993–26004, 2021.
[38] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential
privacy. In Proceedings of the 32nd International Conference on Machine Learning , ICML ’15,
pages 1376–1385, 2015.
[39] Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan Ullman. Privately learning high
dimensional distributions. In Proceedings of the 32nd Annual Conference on Learning Theory ,
COLT ’19. JMLR, 2019.
[40] Gautam Kamath, Argyris Mouzakis, and Vikrant Singhal. New lower bounds for private estima-
tion and a generalized fingerprinting lemma. In Advances in Neural Information Processing
Systems 35 , NeurIPS ’23, pages 24405–24418, 2022.
[41] Vishesh Karwa and Salil Vadhan. Finite sample differentially private confidence intervals. In
Proceedings of the 9th Conference on Innovations in Theoretical Computer Science , ITCS ’18,
pages 44:1–44:9, 2018.
[42] Shiva P. Kasiviswanathan, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Analyzing
graphs with node differential privacy. In 10th IACR Theory of Cryptography Conference , TCC
’13, pages 457–476. Springer, 2013.
[43] Vladimir Koltchinskii and Karim Lounici. Concentration inequalities and moment bounds for
sample covariance operators. Bernoulli , pages 110–133, 2017.
[44] Rohith Kuditipudi, John C. Duchi, and Saminul Haque. A pretty fast algorithm for adaptive
private mean estimation. In Proceedings of the 36th Conference on Learning Theory , COLT
’23, pages 2511–2551. PMLR, Jul 2023.
[45] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model
selection. Annals of Statistics , pages 1302–1338, 2000.
[46] Xiyang Liu, Weihao Kong, Sham Kakade, and Sewoong Oh. Robust and differentially private
mean estimation. In Advances in Neural Information Processing Systems 34 , NeurIPS ’21,
pages 3887–3901, 2021.
[47] Xiyang Liu, Weihao Kong, and Sewoong Oh. Differential privacy and robust statistics in high
dimensions. In Proceedings of the 35th Annual Conference on Learning Theory , COLT ’22,
pages 1167–1246. PMLR, Jul 2022.
13[48] Gábor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed
distributions: A survey. Foundations of Computational Mathematics , 19(5):1145–1190, 2019.
[49] Audra McMillan, Adam Smith, and Jon Ullman. Instance-optimal differentially private estima-
tion, 2022. https://arxiv.org/abs/2210.15819 .
[50] Arshak Minasyan and Nikita Zhivotovskiy. Statistically optimal robust mean and covariance
estimation for anisotropic Gaussians, 2023. https://arxiv.org/abs/2301.09024 .
[51] Aleksandar Nikolov and Haohua Tang. General Gaussian noise mechanisms and their optimality
for unbiased mean estimation. In 15th ACM Conference on Innovations in Theoretical Computer
Science , ITCS ’24, 2024.
[52] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in
private data analysis. In Proceedings of the 30th ACM Symposium on Theory of Computing,
STOC , STOC ’07, pages 75–84, 2007.
[53] Ryan Rogers, Subbu Subramaniam, Sean Peng, David Durfee, Seunghyun Lee, Santosh Kumar
Kancha, Shraddha Sahay, and Parvez Ahammad. LinkedIn’s audience engagements API: A
privacy preserving data analytics system at scale, 2020. https://arxiv.org/abs/2002.
05839 .
[54] Aaron Roth and Tim Roughgarden. Interactive privacy via the median mechanism. In Proceed-
ings of the 42nd ACM Symposium on Theory of Computing , STOC ’10, pages 765–774. ACM,
June 2010.
[55] Sriram Sankararaman, Guillaume Obozinski, Michael I. Jordan, and Eran Halperin. Genomic
privacy and limits of individual detection in a pool. Nature Genetics , 41(9):965–967, 2009.
[56] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference
attacks against machine learning models. In IEEE Symposium on Security and Privacy (S&P),
Oakland , 2017.
[57] Vikrant Singhal and Thomas Steinke. Privately learning subspaces. In Advances in Neural
Information Processing Systems 34 , NeurIPS ’21, pages 1312–1324, 2021.
[58] Adam Smith. Privacy-preserving statistical estimation with optimal convergence rates. In
Proceedings of the 43rd Annual ACM Symposium on the Theory of Computing , STOC ’11,
pages 813–822. ACM, 2011.
[59] Thomas Steinke. Beyond global sensitivity via inverse sensitivity. DifferentialPrivacy.org, Sept
2023. https://differentialprivacy.org/inverse-sensitivity/ .
[60] Thomas Steinke and Jonathan Ullman. Interactive fingerprinting codes and the hardness of
preventing false discovery. In Proceedings of the 28th Annual Conference on Learning Theory ,
COLT ’15, 2015.
[61] Thomas Steinke and Jonathan Ullman. Tight lower bounds for differentially private selection.
InProceedings of the 58th IEEE Symposium on Foundations of Computer Science , FOCS ’17,
2017.
[62] David Tastuggine and Ilya Mironov. Introducing Opacus: A high-
speed library for training PyTorch models with differential pri-
vacy. Facebook AI Blog, 2020. https://ai.facebook.com/blog/
introducing-opacus-a-high-speed-library-for-training-pytorch-models-
with-differential-privacy/ .
[63] Eliad Tsfadia, Edith Cohen, Haim Kaplan, Yishay Mansour, and Uri Stemmer. FriendlyCore:
Practical differentially private aggregation. In Proceedings of the 39th International Conference
on Machine Learning , ICML ’22, pages 21828–21863. PMLR, Jul 2022.
[64] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine
learning: Analyzing the connection to overfitting. In IEEE Computer Security Foundations
Symposium , CSF ’18, pages 268–282, 2018.
[65] Nikita Zhivotovskiy. Dimension-free bounds for sums of independent matrices and simple
tensors via the variational principle. Electronic Journal of Probability , 29:1–28, 2024.
14A Standard DP mechanisms and properties
Definition A.1 (Laplace distribution) .Forv≥0, letLap(v)denote the Laplace distribution over R,
which has probability density function p(z) =1
2σe−|z|/v. From the CDF of the Laplace distribution,
we get that Prz∼Lap(v)[z≥vlog(1/2β)] =β.
Definition A.2 (Laplace Mechanism, [ 25]).Letf:X∗→R, data set XoverX, and privacy
parameter ε. The Laplace Mechanism returns
˜f(X) =f(X) + Lap( v),where v= ∆ f/ε
and∆f= max
X∼X′|f(X)−f(X′)|.
Lemma A.3 ([25]) .The Laplace Mechanism is ε-differentially private.
Definition A.4 (Gaussian Mechanism, [ 25]).Letf:X∗→Rd, data set XoverX, and privacy
parameters ε, δ. The Gaussian Mechanism returns
˜f(X) =f(X) +N(0, v2Id),where v= ∆ fp
2 log(1 .25/δ)/ε
and∆f= max
X∼X′∥f(X)−f(X′)∥2is the global ℓ2-sensitivity off.
Lemma A.5 ([25]) .The Gaussian Mechanism is (ε, δ)-differentially private.
Differential privacy is maintained under post-processing and degrades mildly under composition.
Lemma A.6 (Composition, [ 25,27,38]).LetMbe an adaptive composition of M1, . . . , M T, that is,
on input X,M(X) :=MT(X, M T−1(X, . . . , M 2(X, M 1(X)))). Then
1.(Basic composition) If M1, . . . , M Tare(ε1, δ1), . . . , (εT, δT)-differentially private respec-
tively, then Mis(ε, δ)-differentially private for ε=PT
t=1εtandδ=PT
t=1δt.
2.(Advanced composition) Let εt>0,δt∈[0,1]fort∈ {1, . . . , T }, and ˜δ∈[0,1]. If
M1, . . . , M Tare(ε1, δ1), . . . , (εT, δT)-differentially private respectively, then Mis(˜ε˜δ,˜δ+PT
t=1δt)-differentially private where ˜ε˜δis given by:
˜ε˜δ=kX
ℓ=1(eεℓ−1)εℓ
eεℓ+ 1+vuutkX
ℓ=1ε2
ℓlog1
˜δ
.
Fact A.7 (Fact 2.17 [ 63] reduced to pure DP) .LetY≈εY′random variables over Yand let the
event E⊆ Y be such that Pr[Y∈E],Pr[Y′∈E]≥q. Then Y|E≈ε/qY′
|E.
B Omitted details of Section 3
We state here again the general theorem which holds for any M.
Theorem B.1. Letε∈(0,10), δ∈(0,1), α > 0, β∈(0,1).4Algorithm 1 is (ε, δ)-differentially
private. Let Xbe a data set of size n, drawn from a subgaussian distribution with covariance-proxy
Σand mean µ. Given M= Σ,λ≥p
2 tr(M−1/4ΣM−1/4) + 2q
2∥M−1/4ΣM−1/4∥2log(n
β),
with probability at least 1−β, Algorithm 1 returns ˆµsuch that ∥ˆµ−µ∥2≤α, as long as
n≥C
tr(Σ) + ∥Σ∥2log1
β
α2+λq
tr(M1/2) +r
∥M1/2∥2log1
βq
log1
δ
αε+log1
δβ
ε
,
(5)
4We require εto be smaller than some constant, due to approximations we take in the privacy analysis. The
theorem may hold for ε >10but we did not optimize the choice of constant, as this range is already wide.
15for some universal constant C. IfΣis known, choosing M= Σ and substituting λ, the sample
complexity becomes
n≥C 
tr(Σ) + ∥Σ∥2log1
β
α2+tr(Σ1/2)q
log1
δ
αε
+q
∥Σ∥2log1
δlog1
β
αεlog 
∥Σ∥2log1
δlog1
β
αε!
+log1
δβ
ε!
. (6)
Although the theorem holds for any M, λ , choosing M= Σ gives us the optimal bound.5
Remark 2.IfMis such that Σ⪯M, we may assume without loss of generality that Mis invertible.
Indeed, if this is not the case, then we know that the distribution of the data is supported on a
lower-dimensional subspace along with its mean µ. Using M, we can project onto this subspace. In
this context, we can refocus our analysis on the scenario where Mis an invertible matrix.
Computational complexity We note that Algorithm 1 has time complexity O(n2), which can be
further improved to O(nlogn)as in [63].
The guarantees of Theorem B.1 follow by combining Theorem B.3 and Theorem B.5 below and
re-scaling parameters ε, δ, β with appropriate constants.
B.1 Accuracy analysis
We start with the accuracy analysis. We first prove that for subgaussian data sets, all data points pass
the filter with high probability.
Lemma B.2. LetX= (X(1), . . . , X(n))be a data set drawn from a subgaussian distribution with
covariance proxy Σ. Letβ∈(0,1),Minvertible matrix and some λ≥p
2 tr(M−1/4ΣM−1/4) +
2p
2∥M−1/4ΣM−1/4∥2log(n/β)given as inputs to Algorithm 1. Then the BasicFilter procedure
outputs C=X, with probability 1−β.
Proof. It suffices to show that in BasicFilter we have pj= 1forj∈[n](so that vj= 1and thus
C=X). For each j, k∈[n], we want to show distM,λ(X(j), X(k)) = 1 with probability at least
1−β/n2. For j=k, it is trivial. What is left is to show for j̸=k,
∥M−1/4(X(j)−X(k))∥2≤q
2 tr(M−1/4ΣM−1/4) + 2q
2∥M−1/4ΣM−1/4∥2log(n/β),(7)
with probability at least 1−β/n2. First observe that −X(k)is a subgaussian vector independent
ofX(j)with mean −µand covariance proxy Σ. Hence, X(j)−X(k)is a subgaussian vector with
mean 0 and covariance proxy 2Σ, and so M−1/4(X(j)−X(k))is subgaussian with mean zero and
covariance proxy 2M−1/4ΣM−1/4. By Lemma 2.7, with probability 1−β/n2,
∥M−1/4(X(j)−X(k))∥2≤q
2 tr(M−1/4ΣM−1/4) + 2q
2∥M−1/4ΣM−1/4∥2log(n/β).
Then we union bound all n(n−1)pairs of j, k∈[n], j̸=ksuch that Eq. (7)holds with probability
of at least 1−β.
5For intuition, consider adding noise N(0, c2
i)to each coordinate i. (It is clear that the privacy budget should
be distributed unevenly among coordinates.) We would like to minimize the ℓ2norm of this noise, which is
approximatelyPd
i=1c2
i. The average sensitivity of each coordinate of a Gaussian is σi, so to achieve total privacy
lossε, by advanced composition, we require the ci’s to satisfyPd
i=1σ2
i/c2
i≤ε2. Solving this optimization
problem, we find that ci∝p
∥σ∥1σi, which corresponds to noise N(0,∆2Σ1/2), for∆2≈∥σ∥1
ε2n2. The same
reasoning can be applied to the case of the Mahalanobis error metric [ 13] where adding noise N(0,∆2
MΣ)for
∆2
M≈d
ε2n2gives us the optimal bound. Here the minimization objective is roughly Σd
i=1c2
i/σ2
iso the optimal
solution requires ci∝σi.
16Theorem B.3. Letε > 0, δ∈(0,1), α > 0, β∈(0,1). Suppose n≥2 log(1 /δβ)/ε. Let
X= (X(1), . . . , X(n))be a data set drawn from a subgaussian distribution with mean µand
covariance proxy Σ. Then, given invertible matrix Mandλ≥p
2 tr(M−1/4ΣM−1/4) +
2p
2∥M−1/4ΣM−1/4∥2log(n/β), Algorithm 1, with probability 1−7
2β, returns ˆµsuch that
∥ˆµ−µ∥2≤p
tr(Σ)√n+p
2∥Σ∥2log(1/β)√n
+4p
2 log(1 .25/δ)λ
εnq
tr(M1/2) +q
2∥M1/2∥2log(1/β)
.
Proof. LetµC, µXbe the sample mean of CandX, respectively. By the triangle inequality, we
decompose it into
∥ˆµ−µ∥2≤ ∥ˆµ−µC∥2+∥µC−µX∥2+∥µX−µ∥2. (8)
By Lemma B.2, C=Xwith probability 1−β. Condition on this event for the rest of the proof.
Then, ˆnC=n−log(1/δ)
ε+zsatisfies ˆnC≥0.5n >0with probability 1−β/2because
Pr
z <log(1/δ)
ε−0.5n
≤Pr
z <log(1/δ)
ε−log(1/δβ)
ε
= Pr
z <−log(1/β)
ε
=1
2β
by Definition A.1 and our assumption that n≥2 log(1 /δβ)/ε. Conditioning on this assumption, we
do not abort and with probability 1−β, by Lemma 2.7,
∥ˆµ−µC∥2=∥η∥2≤4p
2 log(1 .25/δ)λ
εnq
tr(M1/2) +q
2∥M1/2∥2log(1/β)
.
Again, by Lemma 2.7, with probability 1−β,
∥µX−µ∥2=1
nnX
j=1X(j)−µ
2≤p
tr(Σ)√n+p
2∥Σ∥2log(1/β)√n.
Moreover, since C=X, it holds that µX=µC. Combining these results into Eq. (8), the algorithm
does not abort and we retrieve the stated error bound, with probability 1−7
2β.
B.2 Privacy analysis
We now move to the privacy analysis.
Lemma B.4. In Algorithm 1, ˆnC<|C|, with probability 1−δ/2.
Proof. It follows that by Definition A.1,
Pr [ˆnC≥ |C|] = Pr [ |C| −log(1/δ)/ε+z≥ |C|] = Pr [ z≥log(1/δ)/ε] =1
2δ.
Note that this holds regardless of whether CisdistΣ,β-friendly or whether Xis subgaussian.
The privacy analysis follows the steps of [63, Claim 3.4].
Theorem B.5. Letε∈(0,1/2), δ∈(0,1/2). For any input parameters M, λ , Algorithm 1 satisfies
(21ε, e10δ)-DP .
Proof. It suffices show that lines 4-7 of Algorithm 1 are distΣ,β-friendly (ε′, δ′)-DP, such that by
Theorem 2.5, Algorithm 1 is (2(eε′−1)ε′,2eε′+2(eε′−1)δ′)-DP.
Denote lines 4-7 of Algorithm 1 as algorithm A. Consider neighboring inputs X, X′such that X∪X′
isdistΣ,β-friendly. Assume without loss of generality X′=X\X(j)so that |X′|=|X| −1. Let
A(X),A(X′)represent the outputs of two independent executions of Aand let bNX,bNX′be the
random variable in line 4of the algorithm. We want to show A(X)≈ε′,δ′A(X′).
17Note that |X|>0. If|X′|= 0, then |X|= 1 andPr [A(X′) =⊥] = 1 . We then show that
Pr [A(X) =⊥]≥1−eεδ/2. This holds since by Definition A.1,
Pr[ˆNX≤0] = Pr [ z≤log(1/δ)/ε−1] = Pr [ z≤log(1/(eεδ))/ε] = 1−eεδ
2.
Therefore, in this case, A(X)≈0,eεδ/2A(X′). That is, if ε≤1/2,Ais(0, δ)-DP.
Now consider |X′|>0. By Lemma B.4, We know Pr[ˆNX<|X|] = Pr[ ˆNX′<|X′|] = 1−δ/2.
Hence, Pr[ˆNX′<|X|] = Pr[ ˆNX′<|X′|+ 1]≥1−δ/2. Then what is left is to compare
A(X)|ˆNX<|X|,A(X′)|ˆNX′<|X|.
By Lemma A.3, ˆNX≈ε,0ˆNX′as|X| − |X′|= 1. By Fact A.7, ˆNX|ˆNX<|X|≈ε/(1−δ/2),0
ˆNX′|ˆNX′<|X|. In order to perform composition by Lemma A.6, we now show that for each fixed
ˆn <|X|,A(X)|ˆNX=ˆn≈ε,δA(X′)|ˆNX′=ˆnas follows:
Choose ˆn <|X|. Ifˆn≤0, then A(X)|bNX=ˆn=A(X′)|bNX=ˆn=⊥and we are done. If 0<
ˆn <|X|, it suffices to show N(1
|X|P|X|
i=1X(i), v2M1/2)≈ε,δN(1
|X|−1P|X|
i=1,i̸=jX(i), v2M1/2),
where v2=8 log(1 .25/δ)λ2
ε2ˆn2 , which, by post-processing, is equivalent to
N
M−1/41
|X||X|X
i=1X(i), v2Id
≈ε,δN
M−1/41
|X| −1|X|X
i=1,i̸=jX(i), v2Id
. (9)
Define vector D=1
|X|P|X|
i=1X(i)−1
|X|−1P|X|
i=1,i̸=jX(i). AsX∪X′isdistΣ,β-friendly, for every
i∈[|X|]\ {j}, there exists some Y(i)∈Rdsuch that distΣ,β(X(i), Y(i)) = dist Σ,β(X(j), Y(i)) =
1.We have
∥M−1/4D∥2=1
|X|(|X| −1)M−1/4

|X|X
i=1,i̸=jX(i)
−X(j)(|X| −1)

2
≤1
|X|(|X| −1)|X|X
i=1,i̸=jM−1/4
X(i)−Y(i)
2+M−1/4
Y(i)−X(j)
2
(by triangle inequality)
≤1
|X|(|X| −1)|X|X
i=1,i̸=j2λ=2λ
|X|≤2λ
ˆn.
(bydistΣ,β-friendly assumption and since 0<ˆn <|X|)
We know Equation (9)holds by applying the guarantees of the Gaussian mechanism (Lemma A.5)
where we set f(X) =M−1/41
|X|P|X|
i=1X(i)and∆f= 2λ/ˆn.
Combining these results, we have Ais(ε+ε
1−δ/2, δeε/(1−δ/2)+δ
2)-DP in this case. For ε≤
1/2, δ≤1/2, this becomes at most (3ε,2δ)-DP.
Therefore, overall, by Theorem 2.5, Algorithm 1 is (2(eε′−1)ε′,2eε′+2(eε′−1)δ′)-DP with ε′=
3ε, δ′= 2δ. So for ε≤1/2, the algorithm is (21ε, e10δ)-DP overall.
C Lower bounds
C.1 Dimension-dependent lower bound under pure DP
The so-called packing lower bound technique [ 32,10] implies a lower bound on the order of dfor the
number of samples required by any pure DP algorithm learning the mean of a Gaussian distribution,
even in the anisotropic case we consider in this paper.
18There exist several statements in prior works which establish the lower bound for learning a Gaussian
distribution with known covariance in TV distance, which is equivalent to learning the mean in
Mahalanobis distance, or to learning the mean in ℓ2norm in the isotropic case [ 18, Lemma 5.1]. It
is trivial to observe that the dependence on the dimension dpersists in the anisotropic case, yet we
include the proof here for completeness.
Theorem C.1. For any α < R/ 2, any ε-DP algorithm which estimates the mean µ∈ Bd(R)of a
Gaussian distribution with known covariance Σ, up to accuracy αinℓ2norm with probability 9/10,
requires n≥dlog(R/2α)
εsamples.
Proof. Consider a 2α-packing of the d-dimensional R-radius ball, denoted by P2α⊂ Bd(R).
That is, ∀u, v∈ P,∥u−v∥2>2α, so that the balls with centers u, vand radius αare disjoint:
Bd(u, α)∩ Bd(v, α) =∅. We consider the family of Gaussian distributions {N(u,Σ)}u∈P2α.
Suppose Ais anε-DP algorithm with the stated accuracy requirement. This implies that ∀u∈ P2α:
Pr
A,X∼N(u,Σ)n[A(X)∈ Bd(u, α)]≥9/10. (10)
At the same time, for any pair of samples X, X 0of size n, and any measurable set B⊂range( A),
by the privacy guarantee, PrA[A(X)∈B]≤eεnPrA[A(X0)∈B]. This implies specifically that
foru0, u∈ P2α,
Pr
A,X∼N(u,Σ)n[A(X)∈B]≤eεnPr
A,X0∼N(u0,Σ)n[A(X0)∈B]. (11)
We have
1≥ Pr
A,X0∼N(u0,Σ)n[A(X0)∈[
u∈P2αBd(u, α)]
=X
u∈P2αPr
A,X0∼N(u0,Σ)n[A(X0)∈ Bd(u, α)] ({Bd(u, α)}u∈P2αdisjoint)
≥X
u∈P2αe−εnPr
A,X∼N(u,Σ)n[A(X)∈ Bd(u, α)] (by Eq. (11))
≥ |P 2α|e−εn·9
10. (by Eq. (10))
We conclude that n≥log|P2α|
ε. Since |P2α| ≥ R
αd, it follows that n≥dlog(R/2α)
ε.
This lower bound makes ε-DP prohibitive for the regime we consider in our setting. To compare with
our upper bounds for (ε, δ)-DP, suppose that we want to learn µwith accuracy cσ1< R, where c >0
is a small constant. Then our main result implies that this is achievable with n≤C∥σ∥1
εσ1samples for
some constant C >0, whereas under ε-DP, we would need at least n≥d
ε≫∥σ∥1
εσ1for the regime we
consider in this paper.
C.2 Lower bound for approximate DP
The so-called tracing orfingerprinting lower-bound technique [ 15,60,17,61,28] is the main
technique used to yield lower bounds for mean estimation under (ε, δ)-DP. Kamath et al. [ 39,40]
apply it to give lower bounds for the problem of learning a Gaussian in TV distance (which is
equivalent to learning the Gaussian in Mahalanobis distance for the known covariance case, or to the
isotropic case).
Theorem C.2 (Theorem 6.5 [ 39]).IfA:Rd×n→[−Rσ, Rσ ]dis(ε, δ)-DP for δ=˜O(√
d
Rn), and
for every Gaussian distribution with mean µ∈[−Rσ, Rσ ]dand known covariance matrix σ2Id, with
probability 2/3,∥A(X)−µ∥ ≤α≤√
dσR/ 3, then n≥dσ
24αεlog(dR).
Following exactly the same steps as the proof of the theorem under the slightly more general case
of known covariance Σ = diag( σ2)gives us a weak lower bound for our setting, on the order of
n≥∥σ∥2
2
24εασ2
1log(dR).
However, a more careful application of the same theorem directly gives us the following stronger
lower bound, which implies that our algorithm for the known covariance case is near-optimal.
19Theorem C.3. IfA:Rd×n→Rdis(ε, δ)-DP for δ=O((np
log(n))−1), and for every Gaussian
distribution with mean µ∈[−1,1]dand known covariance proxy Σ = diag( σ2), with probability
2/3,∥A(X)−µ∥ ≤α=O(∥σ∥1/log(d)), then n= Ω
∥σ∥1
αεlog2(d)
.
Proof. Assume w.l.o.g. that σ2
1≥. . .≥σ2
d. Consider a partition of the set of coordinates [d]into
buckets Sk={i∈[d] :σi∈(σ1
2k,σ1
2k−1]},∀k∈[log(d)]andSlog(d)+1= [d]\S
k∈[log(d)]Sk. We
have thatPlog(d)+1
k=1P
i∈Skσi=∥σ∥1. Consider the bucket Smwhich contributes the most to this
sum, that is m= arg maxP
i∈Smσi. LetσSm= max {σi:i∈Sm}. It must be that
|Sm| ≥∥σ∥1
(log(d) + 1) σSm.
Otherwise, ∥σ∥1=Plog(d)+1
k=1P
i∈Skσi≤(log(d) + 1)|Sm|σSm<∥σ∥1, which is a contradiction.
All the variances of the coordinates in Smare within a factor of two from σSm. We apply Theorem C.2
to the |Sm|-dimensional Gaussian with R= 1. Consider the Gaussian distribution with mean
µSm∈[−1,1]|Sm|and known covariance matrix σ2
SmId. We have that any (ε, δ)-DP algorithm
forδ=O
1
n√
log(n)
which returns, with probability 2/3, an estimate ˆµSmwith error ∥ˆµSm−
µSm∥2≤α≤p
|Sm|σSm/3, requires
n≥|Sm|σSm
24αεlog(d)≥∥σ∥1
48αεlog2(d)(12)
samples.
Now assume that there exists (ε, δ)-DP algorithm A:Rd×n→Rdforδ=O
1
n√
log(n)
, such that,
for every Gaussian distribution with mean µ∈[−1,1]dand known covariance proxy Σ = diag( σ2),
with probability 2/3,∥A(X)−µ∥ ≤α≤∥σ∥1
3(log( d)+1). Restricting the output A(X)to the coordinates
inSm, would give us a mean estimate for Smwith error at most α. Combined with Eq. (12), this
completes the proof of the theorem.
D Omitted details of Section 4
We state the main theorem in more detail.
Theorem D.1. Let parameters ε, δ∈(0,1). LetX∼ N(µ,Σ)nwith unknown covariance Σ. There
exists an (ε, δ)-differentially private algorithm which, with probability 1−β, returns an estimate ˆµ
such that ∥ˆµ−µ∥2≤α, as long as
n= Ω
log2(d) +log(d) log(1
δβ)q
log1
δ
ε
,
n= Ω 
tr(Σ) + ∥Σ∥2log1
β
α2!
, (13)
n=˜Ω
Pd
i=1Σ1/2
iiq
log1
δ
αε
, (14)
and
n=˜Ω
d1/4qPd
i=1Σ1/2
iilog5/4(1
δ) log( d)
√αε
, (15)
where the symbol ˜Ωhides multiplicative logarithmic factors in 1/βand the term in parentheses.
20Recall the definition of set V. Within each group j, for each coordinate i, we compute an estimate
V(j)
iforΣiias follows:
V(j)
i=1
2ℓℓX
r=1
X(j,2r−1)
i −X(j,2r)
i2
. (16)
We show that the variance estimates are valid in the subsequent sections.
Lemma D.2. LetX(1), . . . , X(n)bed-dimensional i.i.d. samples from N(µ,Σ). Let
{V(j)
i}j∈[m],i∈[d]be the estimates defined in Eq. (16). Then, there exist universal constants C, C′>1
such that if ℓ≥Clogdandm≥C′log(1/β), with probability at least 1−β, the set Vof estimates
is valid.
Next, we use the estimates V(j)
ias inputs to multiple procedures. We introduce the following
estimation tasks.
Definition D.3. For a covariance matrix Σ∈Rd×dconsider the following:
1.k-th largest variance: Approximate the k-th largest value among the diagonal of Σ, namely,
thek-th largest value among (Σ11, . . . , Σdd).
2.Sum of variances: given a subset I⊆[d], approximate the sumP
i∈IΣii.
We have the following algorithms for these tasks. The proofs of Lemmas D.4, D.5, and D.6 are in the
subsequent subsections.
Lemma D.4. Letε, δ, β ∈(0,1/2). There exists an algorithm FindKthLargestVarianceε,δ, which
receives variance estimates V(1), . . . , V(m)∈Rdand an integer k∈[d], and satisfies the following,
provided that
m≥Ω1
εlog1
δβ
.
•Privacy: FindKthLargestVarianceε,δis(ε, δ)-DP with respect to changing each input
vector V(j).
•Accuracy: denote the k-th largest entry of {Σ11, . . . , Σdd}byQand the algorithm’s output
byˆQ. If the estimates (V(1), . . . , V(m))are valid wrt Σ, then there exists a universal
constant C >0such that with probability at least 1−β,
Q/8≤ˆQ≤8Q .
Lemma D.5. Letε, δ, β ∈(0,1). There exists an algorithm VarianceSum ε,δ, which receives
variance estimates V(1), . . . , V(m)∈Rdand a subset I⊆[d]. It has the exact same guarantees
asFindKthLargestVariance from Lemma D.4, except that it provides an estimate forP
i∈IΣii
instead of an estimate for the k-th largest diagonal entry of Σ.
Assume for now that the estimates V(j)
iare valid. With these procedures at hand, we first compute R
such that (by rescaling) Q/64≤R≤Q, where Qis the k-th largest diagonal entry of Σ. Then, we
call a procedure that finds kentries isuch that Σii≥R. Its guarantees are listed below:
Lemma D.6. Letε, δ, β ∈(0,1). There exists an (ε, δ)-DP algorithm TopVarε,δ(V, R), such that, if
Vis valid,
m≥Ω r
klog(1/δ)
εnlogd
β!
,
and|{i: Σii≥R}| ≥ k, then the algorithm outputs a set Itopof size ksuch that for all i∈Itop,
Σii≥R/4.
At the next step, we would like to find, up to a constant factor, the variances corresponding to these
coordinates: the values Σiifori∈Itop. We use the algorithm VarianceSum ktimes, providing the
sets{i}fori∈Itop. We obtain estimates ˆΣiithat approximate Σiiup to a constant factor.
Next, we estimate the mean µin the coordinates Itop, denoted µtop, separately from Ibot:= [d]\Itop,
denoted µbot: since we approximately know the variances in Itop, we can obtain a better estimate.
21Both for estimating µtop, and for estimating µbot, we use AvgM,λ,ε,δ (Algorithm 1 from Section 3)
with appropriate choices of parameters M, λ . Recall that Algorithm 1 satisfies the guarantees of
Theorem B.1.
For estimating µtop, we use AvgM,λ,ε,δ for estimating the mean of a k-dimensional Gaussian,
with input vectors restricted to coordinates Itop,X(1)
Itop, . . . , X(n)
Itop, thek×k-dimensional diagonal
matrix M, with Mii=ˆΣii(we assume that the rows and columns of Mare indexed by Itop),
andλ=OqP
i∈ItopˆΣ1/2
iilogn
β
. Denote the output by ˆµtop. Theorem B.1 shows that with
probability 1−β,∥ˆµtop−µtop∥2≤α, if
n≥˜Ω
tr(Σ)
α2+p
log(1/δ)P
i∈ItopΣ1/2
ii
αε+log(1/δ)
ε
, (17)
where ˜Ωhides multiplicative logarithmic factors in 1/βand the second term.
For estimating Ibot, we do not know the variances. In order to perform the estimation, we first
call the algorithm VarianceSum to provide an estimate ˆSbotsuch that1
CP
i∈IbotΣii≤ˆSbot≤
CP
i∈IbotΣiifor a constant C. Given that estimate, we again will call AvgM,λ,ε,δ , now for a (d−k)-
dimensional estimation problem. We input the samples X(1)
Ibot, . . . , X(n)
Ibot, replace the matrix Mwith
the identity of dimension (d−k)×(d−k), and let λ=Oq
ˆSbotlogn
β
.6Denote the output by
ˆµbot. The guarantees of Theorem B.1 provide that with probability 1−β,∥ˆµbot−µbot∥2≤α, if,
additionally to Eq. (17), we have
n≥˜Ω
q
dlog1
δP
i∈IbotΣii
αε
, (18)
where ˜Ωhides multiplicative logarithmic factors of 1/βand of the term in parentheses. As we prove
below, combining these guarantees would yield the desired result. Additionally, we note that in order
for the proof to go through, we split the sample into two groups. One group is used for estimating
the variances and the other group is given as an input to the two invocations of VarianceSum . We
provide the formal pseudocode in Algorithm 2.
Accuracy analysis. We put together the statements of the lemmas above, to establish the overall
accuracy guarantee of Algorithm 2. By Lemma D.2, the estimates V(j)
iare valid (i.e., at least
4m/5of the groups have approximation up to 2for every coordinate), with probability 1−βas
long as m= Ω(log1
β). Consequently, Lemma D.4 implies that as long as m= Ω(1
εlog1
δβ),
FindKthLargestVariance outputs an estimate of the k-th largest variance, which is accurate up to a
constant factor C= 8, with probability 1−β. By scaling, we can assume that, ˆR/8, is upper bounded
by the k-th largest variance. Under this assumption, and as long as m= Ωq
klog(1/δ)
εnlogd
β
,
Lemma D.6 implies that w.p. 1−β, the output of TopVar ,Itop, is a set of size k, containing indices
of elements whose variances are at least ˆR/32. By Lemma D.5, as long as m= Ω
1
ε′log1
δ′β′
=
Ω√
klog(1/δ)
εlogk
δβ
, the estimates ˆΣiito the variances in the indices in Itopare accurate up to a
constant factor, with a failure probability of β/k for each invocation of this lemma, which sums up to
a failure probability of β. Similarly, the estimate ˆSbothas the same guarantee. If nis large enough to
satisfy the requirement of Line 1, then all previous constraints on mare satisfied.
Lastly, the two estimates from AvgM,λ,ε,δ suffer an approximation of α, each with a failure probability
ofβ, provided that the conditions on the sample complexity n, that are given in Eq. (17) and Eq. (18),
6We could additionally privately learn the largest variance among Ibot, denoted by ˆsand set λ=
OpˆSbot+q
ˆslogn
β
to decouple ˆSbotfrom the logarithmic factor, but we choose not to for simplicity, and
since we did not optimize for logarithmic factors overall.
22hold. By assumption on the sample complexity (Eq. (13),(14)), the guarantee of Eq. (17) indeed holds.
It remains to prove that the guarantee of Eq. (18) holds as well. We analyze the termqP
i∈IbotΣii.
Denote vector σbot= ({σi}i∈Ibot)where σi= Σ1/2
ii. ThenqP
i∈IbotΣii=∥σbot∥2. By Hölder’s
inequality,
∥σbot∥2≤p
∥σbot∥1∥σbot∥∞≤p
∥σ∥1∥σbot∥∞.
By the guarantees of TopVar andFindKthLargestVariance , except for a failure probability of
O(β), there exists a universal constant C >1such that
max
i∈IbotΣ1/2
ii≤CˆR1/2.
Further, by assumption, ˆRis up to a constant the k-th largest diagonal element of Σ, hence,
CˆR1/2≤1
kdX
i=1Σ1/2
ii.
Substituting this above, we obtain that
sX
i∈IbotΣii≤1√
kdX
i=1Σ1/2
ii.
Thus, it suffices for the stated sample complexity to additionally satisfy n=
˜Ω√
dlog(1/δ)Pd
i=1Σ1/2
ii√
kαε
. Substituting the definition for k, we obtain Eq. (15), which
completes the proof.
Privacy analysis. Notice that the output of the algorithm is obtained by composing multiple
differentially private mechanisms. Some of these mechanisms access the estimates V(1), . . . , V(m)
instead of the original dataset. Yet, since each input datapoint X(i)influences only one vector V(j),
this implies that any DP guarantees for algorithms that use the V(j)estimates, directly translate to
DP guarantees on the original input dataset.
Notice that the algorithm has O(1)calls to (ε, δ)-DP mechanisms, and kcalls to (ε′, δ′)-DP mecha-
nisms: these are the calls to VarianceSum . By Lemma A.6 (advanced composition), the concate-
nation of all the calls to VarianceSum are together, (O(ε), O(δ)). By basic composition of the
same lemma, composing the resulting composion with the other calls to DP mechanisms, yields an
(O(ε), O(δ))-DP mechanism.
D.1 The variance estimates are valid: Proof of Lemma D.2
The random variable (X(j,2r−1)
i −X(j,2r)
i)/√2Σiiis standard normal. Thus,
ℓX
r=1(X(j,2r−1)
i −X(j,2r)
i)2
2Σii
follows a chi-squared distribution with ℓdegrees of freedom. We use the following concentration
property of a Chi-squared random variable [ 45, Lemma 1]: if Zis Chi-squared with ℓdegrees of
freedom,
Pr[E[Z]/2≤Z≤2E[Z]]≥1−2e−cℓ,
for some constant c >0. Consequently,
Pr"
ℓ
2≤ℓX
r=1(X(j,2r−1)
i −X(j,2r)
i)2
2Σii≤2ℓ#
≥1−2e−cℓ,
for some constant c >0.
By a union bound over all dimensions i, the probability that all dimensions’ variance estimates fall
within the specified bounds in a single group jis at least 1−2dexp(−cℓ). Assuming ℓ≥Clogd
ensures this probability is very high (e.g., at least 7/8for suitable constant C).
Using the Chernoff bound for the binomial distribution, if each group independently satisfies the
variance bounds with probability at least 7/8, then the probability that at least 4/5of the groups
satisfy the variance bounds is at least 1−βform= Ω(log(1 /β)).
23D.2 Finding the indices of the largest variances: Proof of Lemma D.6
We propose an algorithm which, receives estimates V(j)
ifor the variances and a threshold R, and
outputs kindices i∈[d]whose variance is at least R/C for some universal constant C. To do so, we
use the sparse vector algorithm, which receives a dataset D, queries Q1(D), . . . , Q d(D), a threshold
Tand a natural number k. It outputs kindices isuch that Qi(D)≥T(approximately). In order
to use the sparse vector to identify the largest variances, our dataset Dwill be V, the collection of
estimates. The query Qi(V)will capture whether the i’th variance is Ω(R). We define the query
Qi(V) =1
mn
j:V(j)
i≥R/2o,
and the threshold T= 1/2. Intuitively, if Qi(V)≥1/2this means that at least half of the values of
j,V(j)
i≥R/2, which implies that Σii≥Ω(R), provided that the estimates Vare valid. Otherwise,
it implies Σii≤R.
We now formally define the sparse vector algorithm [ 26,54,31], and review its guarantees. See [ 23,
Section 3.6] for a detailed analysis of the sparse vector technique.
Algorithm 3 Sparse( D,{Qi}, T, d, ε, δ ), from [23]
Require: Input is a private database D, an adaptively chosen stream of sensitivity 1/nqueries
Q1, . . ., a threshold T, a cutoff point k, and privacy parameters ε, δ.
1:ˆT←T+Lap 2
εn
2:σ←q
32kln(1/δ)
εn
3:count←0
4:I← ∅
5:foreach query ido
6: vi←Lap(σ)
7: ifQi(D) +vi≥ˆTthen
8: I←I∪ {i}
9: count←count + 1
10: ifcount≥kthen
11: return I
12:return I
Lemma D.7 (Sparse guarantees) .Sparse (Algorithm 3) is (ε, δ)-differentially private. Let β∈(0,1)
and define
α= 2σ
logd+ log2
β
=r
128kln(1/δ)
εn
logd+ log2
β
.
For any sequence of dqueries Q1, . . . , Q dif there are at least kqueries isuch that Qi(D)≥T+α,
then the following holds with probability 1−β: the output of Algorithm 3, I, is a set of size k, and
for each i∈I,Qi(D)≥T−α.
Next, we formally define the algorithm TopVar , to find the indices of the largest variances.
Algorithm 4 TopVarε,δ(V, R, k )
Require: Variance estimates V={V(j)
i}j∈[m],i∈[d], threshold R∈R, privacy parameters ε, δ∈
(0,1), number of indices k∈N.
1:Define queries Qi(D)for each i∈[d]as:
Qi(D) =1
mn
j:V(j)
i≥R/2o
2:T←1/2
3:return Sparse( V,{Qi}, T, k, δ )
24The privacy guarantees of TopVar follow directly from the guarantees of the sparse vector. Next,
we describe how to derive the accuracy guarantees. Notice that if the V(j)
iare valid, then, for any i
such that Σii≥R: for at least 4m/5values of j, it holds that V(j)
i≥R/2, hence, Qi(D)≥4/5.
Further, for any isuch that Σii< R/ 4, for at least 4m/5values of jit holds that V(j)
i< R/ 2, hence
Qi(D)≤1/5. Hence, if we set the threshold at T= 1/2, andα= 1/4, then, for any ioutput by the
algorithm, Σii≥R/4. Further, if there are at least kindices isuch that Qii≥R, the algorithm will
output kindices.
D.3 Finding the k-th largest variance: Proof of Lemma D.4
We propose an algorithm, Algorithm 5, that receives pre-computed variance estimates V(j)
ifor each
group jand coordinate i. The algorithm uses them to compute an estimate for the k-th largest
variance for each V(j):
Mj:=k-th largest ofn
V(j)
1, . . . , V(j)
do
i∈[d].
Our algorithm combines all of these estimates in a differentially private manner, using a stable
histogram: Algorithm 6. That algorithm splits the real line into buckets, {Bb}b∈Z∪{−∞} . It receives
the estimates M1, . . . , M m∈Rand outputs the index bof the bucket that contains the largest number
of estimates Mj(approximately).
In our application, we would like to estimate the k-th largest variance up to a multiplicative constant
factor, hence, we define the buckets as
Bb=[4b,4b+1)b∈Z
{0} b=−∞.
Denote by b∗index of the bucket that contains the k-th largest diagonal entry of Σ. If the estimates
V(1), . . . , V(m)are valid then, by definition of validity (Definition 4.2), it follows that at least 4m/5
of the estimates Mjfall into the union Bb∗−1∪Bb∗∪Bb∗+1. Under this assumption, Algorithm 6 is
guaranteed to output one of b∗−1,b∗orb∗+ 1, with probability 1−δ.
The algorithm for k-th largest variance, Algorithm 5, is presented here:
Algorithm 5 FindKthLargestVarianceε,δ({V(j)
i}i∈[d],j∈[m], k)
Require: Pre-computed variance estimates V(j)
ifor each group jand each coordinate i. Privacy
parameters ε, δ > 0. Integer k≤d. Number of groups m.
1:forj∈[m]do
2: Mj←k-th largest value among {V(j)
1, V(j)
2, . . . , V(j)
d}
3:Define bins {Bb}b∈Z∪{−∞} by:
Bb=[4b,4b+1)b∈Z
{0} b=−∞
4:b←StableHistogramε,δ({Mj}j∈[m],{Bb})
5:return ˆM= 4b
We proceed by defining StableHistogram as introduced in [ 16] and providing its guarantees, and
then we conclude with the proof of Lemma D.4. The presentation of StableHistogram is from [ 13].
25Algorithm 6 StableHistogramε,δ({Mi},{Bb}), from [16]
Require: Items M1, . . . , M m∈ U. Bins{Bb}b∈Z. Privacy parameters ε, δ > 0.
1:forb∈Zdo
2: cb← |{i:zi∈Bb}|
3:forbwithcb>0do
4: ˜cb←cb+ Lap(2 /ε)
5:τ←1 +2 log(1 /δ)
ε
6:Letbmax= arg max b˜cb, with arbitrary tie breaks
7:if˜cbmax≥τthen
8: return bmax
9:else
10: return ⊥
We use its privacy and accuracy guarantees, proved as Lemma C.1 in [13]:
Lemma D.8 (Stable Histogram Guarantees) .StableHistogramε,δ(Algorithm 6) is (ε, δ)-
differentially private. Suppose that there exists b∗∈Zsuch that
|{M1, . . . , M m} ∩(Bb∗−1∪Bb∗∪Bb∗+1)| ≥3m/4.
There exists a constant C >0such that, for all 0< ε, β, δ < 1, if
m≥C
εlog1
δβ,
then with probability at least 1−β, the algorithm’s output lies in {b−1, b, b+ 1}.
The privacy guarantees of Algorithm 5 follow directly from the privacy guarantees of Algorithm 6.
For the accuracy guarantees, notice that if the estimates V(j)are valid then at least 4m/5of the
values Mjfall into the bucket Bb∗that contains the true value of the k-th largest entry of the diagonal
ofΣ. Under this assumption, Algorithm 6 is guaranteed to output, with probability 1−β, one of
b∗−1,b∗orb∗+ 1. This implies that the output of Algorithm 5 is approximates the target quantity
up to a constant, as required.
D.4 Finding a sum of variances: Proof of Lemma D.5
We propose an algorithm that is similar to Algorithm 5, with a single difference: given each estimate
V(j), the algorithm computes
Mj=X
i∈IV(j)
i.
The algorithm is summarized below:
Algorithm 7 VarianceSum ε,δ({V(j)
i}i∈[d],j∈[m], I)
Require: Pre-computed variance estimates V(j)
ifor each group jand each coordinate i. Privacy
parameters ε, δ > 0. Subset I⊆[d]. Number of groups m.
1:forj∈[m]do
2: Mj←P
i∈IV(j)
i
3:Define bins {Bb}b∈Z∪{−∞} by:
Bb=[4b,4b+1)b∈Z
{0} b=−∞
4:b←StableHistogramε,δ({Mj}j∈[m],{Bb})
5:return ˆM= 4b
The proof is identical to the proof of Lemma D.4. In order to carry that proof, one has to notice that
ifb∗is the bucket that containsP
i∈IΣiiand if the estimates V(j)are valid, then at least 4m/5of
the estimates Mjfall within Bb∗−1∪Bb∗∪Bb∗+1.
26NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The introduction includes informal theorem statements for each of our main
claims.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss limitations of this work in the introduction, in Section 4, and when
outlining directions for future work.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
27Answer: [Yes]
Justification: All theorems in the technical Sections 3, 4, as well as the Appendix include
their assumptions in the theorem statement.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
28Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
29• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We believe that the presented research conforms with the NeurIPS Code of
Ethics, as it is theoretical and does not enable harmful consequences.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Our paper is theoretical and contributes to a line of work that aims to design
privacy-preserving statistical estimators, which use few samples, even in high-dimensional
settings. We believe that in the long-term, the societal impact of this line of work will be
positive, in the sense that it will enable the use of privacy-preserving methods, and possibly
even mitigate the need for collection of large data sets.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
30•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper does not contribute data or models with high risk of misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
31•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
32