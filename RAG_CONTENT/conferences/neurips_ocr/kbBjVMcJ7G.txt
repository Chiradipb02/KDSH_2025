Operator World Models for Reinforcement Learning
Pietro Novelli
Istituto Italiano di Tecnologia
pietro.novelli@iit.itMarco Pratticò
Istituto Italiano di Tecnologia
marco.prattico@iit.it
Massimiliano Pontil
Istituto Italiano di Tecnologia
AI Centre, University College London
massimiliano.pontil@iit.itCarlo Ciliberto
AI Centre, University College London
c.ciliberto@ucl.ac.uk
Abstract
Policy Mirror Descent (PMD) is a powerful and theoretically sound methodology
for sequential decision-making. However, it is not directly applicable to Reinforce-
ment Learning (RL) due to the inaccessibility of explicit action-value functions.
We address this challenge by introducing a novel approach based on learning a
world model of the environment using conditional mean embeddings. Leveraging
tools from operator theory we derive a closed-form expression of the action-value
function in terms of the world model via simple matrix operations. Combining
these estimators with PMD leads to POWR, a new RL algorithm for which we
prove convergence rates to the global optimum. Preliminary experiments in finite
and infinite state settings support the effectiveness of our method1.
1 Introduction
In recent years, Reinforcement Learning (RL) [ 1] has seen significant progress, with methods capable
of tackling challenging applications such as robotic manipulation [ 2], playing Go [ 3] or Atari games
[4] and resource management [ 5] to name but a few. The central challenge in RL settings is to
balance the trade-off between exploration and exploitation, namely to improve upon previous policies
while gathering sufficient information about the environment dynamics. Several strategies have been
proposed to tackle this issue, such as Q-learning-based methods [ 4], policy optimization [ 6,7] or
actor-critics [ 8] to name a few. In contrast, when full information about the environment is available,
sequential decision-making methods need only to focus on exploitation. Here, strategies such as
policy improvement or policy iteration [ 9] have been thoroughly studied from both the algorithmic
and theoretical standpoints. Within this context, the understanding of Policy Mirror Descent (PMD)
methods has recently enjoyed a significant step forward, with results guaranteeing convergence to a
global optimum with associated rates [10, 11, 12].
In their original formulation, PMD methods require explicit knowledge of the action-value func-
tions for all policies generated during the optimization process. This is clearly inaccessible in RL
applications. Recently, [ 12] showed how PMD convergence rates can be extended to settings in
which inexact estimators of the action-value function are used (see [ 13] for a similar result from a
regret-based perspective). The resulting convergence rates, however, depend on uniform norm bounds
on the approximation error, usually guaranteed only under unrealistic and inefficient assumptions
such as the availability of a (perfect) simulator to be queried on arbitrary state-action pairs. Moreover,
these strategies require repeating this sampling/learning process for any policy generated by the
PMD algorithm, which is computationally expensive and demands numerous interactions with the
1Code available at: github.com/CSML-IIT-UCL/powr
38th Conference on Neural Information Processing Systems (NeurIPS 2024).environment. A natural question, therefore, is whether PMD approaches can be efficiently deployed
in RL settings while enjoying the same strong theoretical guarantees.
In this work, we address these issues by proposing a novel approach to estimating the action-value
function. Unlike previous methods that directly approximate the action-value function from samples,
we first learn the transition operator and reward function associated with the Markov decision
process (MDP). To model the transition operator, we adopt the Conditional Mean Embedding (CME)
framework [ 14,15]. We then leverage an operatorial characterization of the action-value function
to express it in terms of these estimated quantities. This strategy draws a peculiar connection with
world model methods and can be interpreted as world model learning via CMEs. The notion of world
models for RL has been popularized by Ha and Schmidhuber in [ 16] distilling ideas from the early
nineties [ 17,18]. Traditional world model methods such as [ 16,19] emphasize learning an implicit
model of the environment in the form of a simulator. The simulator can be sampled directly in the
latent representation space, which is usually of moderate dimension, resulting in a compressed and
high-throughput model of the environment. This approach, however, requires extensive sampling
for application to PMD and incurs into two sources of error in estimating the action-value function:
model and sampling error. In contrast, CMEs can be used to estimate expectations without sampling
and incur only in model error, for which learning bounds are available [ 20,21]. One of our key
results shows that by modeling the transition operator as a CME between suitable Sobolev spaces, we
can compute estimates of the action-value function of any sufficiently smooth policy in closed form
via efficient matrix operations.
Combining our estimates of the action-value function with the PMD framework we obtain a novel RL
algorithm that we dub Policy mirror descent with Operator World-models for Reinforcement learning
(POWR) . A byproduct of adopting CMEs to model the transition operator is that we can naturally
extend PMD to infinite state space settings. We leverage recent advancements in characterizing the
sample complexity of CME estimators to prove convergence rates for the proposed algorithm to the
global maximum of the RL Problem. Our approach is similar in spirit to [ 22], which proposed a
value iteration strategy based on CMEs. We extend these ideas to PMD strategies and refine previous
results on convergence rates. Learning the transition operator with a least-squares based estimator
was also recently considered in [ 23], which proposed an optimistic strategy to prove near-optimal
regret bounds in linear mixture MDP settings [ 24]. In contrast, in this work, we cast our problem
within a linear MDP setting with possibly infinite latent dimension. We validate our approach on
simple environments from the Gymlibrary [ 25] both in finite and infinite state settings, reporting
promising evidence in support of our theoretical analysis.
Contributions . The main contributions of this paper are: i)a CME-based world model framework,
which enables us to generate estimators for the action-value function of a policy in closed form via
matrix operations. ii)An (inexact) PMD algorithm combining the learned CMEs world models
with mirror descent update steps to generate improved policies. iii)Showing that the algorithm is
well-defined when learning the world model as an operator between a suitable family of Sobolev
spaces. iv)Showing convergence rates of the proposed approach to the global maximum of the RL
problem, under regularity assumptions on the MDP. v)Empirically testing the proposed approach in
practice, comparing it with well-established baselines.
2 Problem Formulation and Policy Mirror Descent
We consider a Markov Decision Process (MDP) over a state space Xand action space A, with
transition kernel τ. We assume XandAto be Polish, τ: Ω→ P(X)to be a Borel measurable
function from the joint space Ω =X × A to the space P(X)of Borel probability measures on X.
We define a policy to be a Borel measurable function π:X → P (A). When A(respectively X) is
a finite set, the space P(A) = ∆( A)⊆R|A|(respectively P(X) = ∆( X)⊆R|X|) corresponds to
the probability simplex. Given a discount factor γ >0, an initial state distribution ν∈ P(X)and a
Borel measurable bounded and non-negative reward2function r: Ω→Rwe denote by
J(π) =Eν,π,τ"∞X
t=0γtr(Xt, At)#
(1)
2All the discussion in this work can be extended to the case where also the rewards are random and
τ: Ω→ P (X ×R)takes values in the space of joint distributions over states and rewards (Xt+1, R t)
2the (discounted) expected return of the policy πapplied to the MDP, yielding the Markov process
(Xt, At)t∈N, where X0is distributed according to νand for each t∈Nthe action Atis distributed
according to π(·|Xt)andXt+1according to τ(·|Xt, At).
In sequential decision settings, the goal is to find the optimal policy π∗maximizing (1) over the space
of all measurable policies. In reinforcement learning, one typically assumes that knowledge of the
transition τ, the reward r, and (possibly) the starting distribution νis not available. It is only possible
to gather information about these quantities by interacting with the MDP to sample state-action pairs
(xt, at)and corresponding rewards r(xt, at)and transitions xt+1.
Policy Mirror Descent (PMD) . In so-called tabular settings – in which both XandAare finite sets –
the policy optimization problem amounts to maximizing (1) over the space Π = ∆( A)⊗R|X|of
column substochastic matrices, namely matrices M∈R|A|×|X|with non-negative entries and whose
columns sum up to one, namely M∗1A=1X, with 1denoting the vector with all entries equal to one
on the appropriate space. Borrowing from the convex optimization literature – where mirror descent
algorithms offer a powerful approach to minimize a convex functional over a convex constraint set
[26,27] – recent work proposed to adopt mirror descent also for policy optimization, a strategy known
aspolicy mirror descent (PMD) [10]. Even though the objective in (1) is not convex (or concave,
since we are maximizing it), it turns out that mirror ascent can nevertheless enjoy global convergence
to the maximum, with sublinear [ 11] or even linear rates [ 12], at the cost of dimension-dependent
constants.
Starting from an initial policy π0, PMD generates a sequence (πt)t∈Naccording to the update step
πt+1(·|x) = argmin
p∈∆(A)−η⟨qπt(·, x), p⟩+D(p, πt(·|x)), (2)
for any x∈ X, with η >0a step size, Da suitable Bregman divergence [ 27] and qπ: Ω→Rthe
so-called action-value function of a policy π, see also [12, Sec. 4]. The action-value function
qπ(x, a) =E"∞X
t=0γtr(Xt, At)X0=x, A0=a#
(3)
is the discounted return obtained by taking action a∈ A in state x∈ X and then following the
policy π. The solution to (2) crucially depends on the choice of D. For example, in [ 10] the authors
observed that if Dis the Kullback-Leibler divergence, PMD corresponds to the Natural Policy
Gradient originally proposed in [ 28] while [ 12] showed that if Dis the squared euclidean distance,
PMD recovers the Projected Policy Gradient method from [11].
PMD in Reinforcement Learning . A clear limitation to adopting PMD in RL settings is that (3)
needs exact knowledge of the action-value functions qπtassociated to each iterate πtof the algorithm.
This requires evaluating the expectation in (3), which is not possible in RL where we do not know the
reward rand MDP transition distribution τin advance. While sampling strategies can be adopted to
estimate qπt, a key question is how the approximation error affects PMD.
The work in [ 12] provides an answer to this question, extending the analysis of PMD to the case
where estimates ˆqπtare used in place of the true action-value function in (2). We recall here an
informal version of the result for the case of sublinear convergence rates for PMD. We postpone a
more rigorous statement of the theorem and its assumptions to Sec. 5, where we extend it to infinite
state spaces X.
Theorem 1 (Inexact PMD (Sec. 5 in [ 12]) – Informal) .In the tabular setting, let (πt)t∈Nbe a
sequence of policies obtained by applying the PMD update in (2)with functions ˆqπt: Ω→Rin
place of qπtandDa suitable Bregman divergence. For any T∈Nandε >0, if∥ˆqπt−qπt∥∞≤ε
for all t= 1, . . . , T , then
max
πJ(π)−J(πT)≤O(ε+ 1/T). (4)
Thm. 1 implies that inexact PMD retains the convergence rates of its exact counterpart, provided that
the approximation error for each action-value function is of order 1/Tin uniform norm ∥·∥∞. While
this result supports estimating the action-value function in RL, implementing this strategy in practice
poses two main challenges, even in tabular settings. First, approximating the expectation in (3) in
∥·∥∞norm via sampling requires “starting” the MDP from each state x∈ X, multiple times. This
3is often not possible in RL, where we do not have control over the starting distribution ν. Second,
repeating this sampling process to learn a ˆqπtfor each policy πtcan become extremely expensive in
terms of both the number of computations and interactions with the environment.
In this work, we propose a new strategy to tackle the problems above. Instead of re-sampling the
MDP to estimate ˆqπtat each iteration t, we learn estimators ˆrandˆτfor the reward and transition
distribution, respectively. For any policy π, we then leverage the relation between these quantities in
(3) to generate an estimator ˆqforqπ. This approach tackles the above challenges since 1) it enables us
to control the approximation error on any action-value function in terms of the approximation error of
ˆrandˆτ; 2) it does not require sampling the MDP to learn a new ˆqπtfor each πtgenerated by PMD.
3 Operator World Models
In this section, we present an operator-based formulation of the problem introduced in Sec. 2 (see
also [ 11]). This will be instrumental in extending the PMD theory to arbitrary state spaces X, to
quantify the approximation error of the action-value function in terms of the approximation error of
the reward and transition distribution, and to motivate conditional mean embeddings as the tool to
learn these latter quantities.
Conditional Expectation Operators . We start by defining the transfer operator Tassociated
with the MDP transition distribution τ. LetBb(X)denote the space of bounded Borel measurable
functions on a space X. Formally, T:Bb(X)→Bb(Ω)is the linear operator such that, for any
f∈Bb(X)
(Tf)(x, a) =Z
Xf(x′)τ(dx′|x, a) =E[f(X′)|x, a] for all (x, a)∈Ω, (5)
where X′is sampled according to τ(·|x, a). Note that Tis the Markov operator [ 29, Ch. 19] encoding
the dynamics of the MDP and its conjugate T∗:M(Ω)→ M (X)is the operator mapping signed
Borel measures µ∈ M (Ω)to their transition via τas(T∗µ)(B) =R
B×Ωτ(dx′|x, a)µ(dx, da )for
any measurable B ⊆ X . For any policy πwe define the operator Pπ:Bb(Ω)→Bb(X)such that for
allg∈Bb(Ω)
(Pπg)(x) =Z
Ag(x, a)π(da|x) =E[g(X, A)|X=x]for all x∈ X, (6)
where the expectation is taken over the action Asampled according to π(·|x). Also Pπis a Markov
operator and its conjugate P∗
π:M(X)→ M (Ω)is the operator mapping any ν∈ M(X)to its joint
measure with π, namely (P∗
πν)(C) =R
Cπ(da|x)ν(dx)for any measurable C ⊆Ω.
Operator Formulation of RL . With these two operators in place, we can characterize the expected re-
ward after a single interaction between a policy πand the MDP as (TPπr)(x, a) =E[r(X′, A′)|X0=
x, A0=a]. This observation can be applied recursively, yielding the operatorial characterization of
the action-value function from (3)
qπ(x, a) =∞X
t=0γtE[r(Xt, At)|X0=x, A0=a] =∞X
t=0(γTPπ)tr= (Id−γTPπ)−1r, (7)
where the last equality follows from TandPπbeing Markov operators [ 29, Ch. 19] ( ∥T∥=∥Pπ∥=
1), making the Neumann series convergent. Analogously, we can reformulate the RL objective
introduced in (1) as the pairing
J(π) =
Pπ(Id−γTPπ)−1r, ν
=⟨Pπqπ, ν⟩, (8)
forν∈ P(X)a starting distribution. In both (7) and (8) the operatorial formulation encodes the
cumulative reward collected through the (possibly infinitely many) interactions of the policy with the
MDP in closed form, as the inversion (Id−γTPπ)−1r. This characterization motivates us to learn T
andrfrom data and then express any action-value function as the interaction of these two terms with
the policy πas in (7), rather than learning each qπindependently for any π.
Learning the World Model via Conditional Mean Embeddings . Conditional Mean Embeddings
(CME) offer an effective tool to model and learn conditional expectation operators from data [ 15].
They cast the problem of learning Tby studying the restriction of its action on a suitable family
4of functions. Let φ:X → F andψ: Ω→ G two feature maps with values into the Hilbert
spaces FandG. With some abuse of notation (which is justified by them being Hilbert spaces), we
interpret FandGas subspaces of functions in Bb(X)andBb(Ω)of the form f(x) =⟨f, φ(x)⟩and
g(x, a) =⟨g, ψ(x, a)⟩for any f∈ F andg∈ G and any (x, a)∈Ω. We say that the linear MDP
assumption holds with respect to (φ, ψ)if
Assumption 1 (Linear MDP – Well-specified CME) .The restriction of TtoFis a Hilbert-Schmidt
operator T|F∈HS(F,G).
In CME settings, the assumption above is known as requiring the CME of τto be well-specified . The
following result, proved in Appendix A.2, clarifies this aspect and establishes the relation of Asm. 1
with the standard definition of linear MDP.
Proposition 2 (Well-specified CME) .Under Asm. 1, (T|F)∗= (T∗)|Gand, for any (x, a)∈Ω
(T|F)∗ψ(x, a) =Z
Xφ(x′)τ(x′|x, a) =E[φ(X′)|X=x, A=a]. (9)
Proposition 2 shows that (9) is equivalent to the standard linear MDP assumption [ 30, Ch. 8] when
Xis a finite set (taking φthe one-hot encoding) while being weaker in infinite settings. From the
CME perspective, the proposition characterizes the action of (T|F)∗as sending evaluation vectors
inGto the conditional expectation of evaluation vectors in Fwith respect to τ, the definition of
conditional mean embedding of τ[31,15]. This characterization also suggests a learning strategy:
(9) characterizes the action of Tas evaluating the conditional expectation of a vector φ(X′)given
(x, a). Given a set of points (xi, ai)n
i=1and corresponding x′sampled from τ(·|xi, ai), this can be
learned by minimizing the squared loss, yielding the estimator (see [15, Sec 4.2])
Tn= argmin
T∈HS(F,G)1
nnX
i=1∥φ(x′
i)−T∗ψ(xi, ai)∥2
F+λ∥T∥2
HS=S∗
nK−1
λZn. (10)
When FandGare finite dimensional, SnandZnare matrices with nrows, each corresponding
respectively to the vectors ψ(xi, ai)andφ(x′
i)fori= 1, . . . , n . In the infinite setting, they generalize
to operators Sn:G → RnandZn:F → Rn. The matrix Kλ=SnS∗
n+nλIdn∈Rn×nis the
regularized Gram (or kernel) matrix with (i, j)-th entry corresponding to
 
Kλ
ij=⟨ψ(xi, ai), ψ(xj, aj)⟩+nλδij. (11)
We conclude our discussion on learning world models via CMEs by noting that in most RL settings,
the reward function is unknown, too. Analogously to what we have described for Tnand following
the standard practice in supervised settings, we can learn an estimator for rsolving a problem akin to
(10). This yields a function of the form rn=S∗
nb=Pn
i=1biψ(xi, ai)as the linear combination of
the embedded training points with the entries of the vector b=K−1
λywhere y∈Rnis the vector
with entries yi=r(xi, ai).
Estimating the Action-value Function qπ. We now propose our strategy to generate an estimator
for the action-value function qπof a given policy πin terms of an estimator for the reward rand a
world model for Tlearned in terms of the restriction to GandF. To this end, we need to introduce
the notion of compatibility between a policy πand the pair (G,F).
Definition 1 ((G,F)-compatibility) .A policy π:X → P (A)is compatible with two subspaces
F ⊆Bb(X)andG ⊆Bb(Ω)if the restriction PπtoGis a bounded linear operator with range ⊆ F ,
that is (Pπ)|G:G → F .
Definition 1 is analogous to the linear MDP Asm. 1 in that it requires the restriction of PπtoGto
take values in the associated space F. However, it is slightly weaker since it requires this restriction
to be bounded (and linear) rather than being an HS operator. We will discuss in Sec. 4 how this
difference will allow us to show that a wide range of policies (in particular those generated by our
POWR method) is (G,F)-compatible for our choice of function spaces. Definition 1 is the key
condition that enables us to generate an estimator for qπ, as characterized by the following result.
5Algorithm 1 POWR: POLICY MIRROR DESCENT WITH OPERATOR WORLD -MODELS FOR RL
Input: Dataset (xi, ai, x′
i, ri)n
i=1, discount factor γ∈(0,1), step size η >0, kernel function
k(x, x′) =⟨ϕ(x), ϕ(x′)⟩withϕ:X → H as in Proposition 4, initial weights C0= 0∈Rn×|A|.
/* World Model Learning */
letE∈Rn×|A|with rows Ei=ONEHOT|A|(ai).
letKλ∈Rn×nsuch that Kij=k(xi, xj)δai=aj+nλδij ▷Eq. (11)
letH∈Rn×nsuch that Hij=k(x′
i, xj)
compute K−1
λandb=K−1
λywithy= (r1, . . . , r n)∈Rn▷Eq. (10)
/* Policy Mirror Descent */
fort= 0,1, . . . , T −1do:
πt+1=SOFTMAX (ηHC t)∈Rn×|A|▷PMD Step (15)
Mπt+1=H⊙(πt+1E⊤)∈Rn×n▷Proposition 3, Eq. (13)
Ct+1=Ct+diag(c)Ewithc= (Id−γK−1
λMπt+1)−1b ▷ Proposition 3, Eq. (12)
end for
return πT:X → ∆(A)such that πT(x) =SOFTMAX (η HxCT)withHx= (k(x, xi))n
i=1∈Rn.
Proposition 3. LetTn=S∗
nBZn∈HS(F,G)andrn=S∗
nb∈ Gfor respectively a B∈Rn×nand
b∈Rn. Letπbe(G,F)-compatible. Then,
ˆqπ= (Id−γTnPπ)−1rn=S∗
n(Id−γBM π)−1b (12)
where Mπ=ZnPπS∗
n∈Rn×nis the matrix with entries
 
Mπ
ij=⟨φ(x′
i),Pπψ(xj, aj)⟩=Z
A⟨ψ(x′
i, a), ψ(xj, aj)⟩π(da|x′
i). (13)
Proposition 3 leverages a kernel trick argument to express the estimator for the action-value function
ofπas the linear combination ˆqπ=Pn
i=1ciψ(xi, ai)of the (embedded) training points ψ(xi, ai)
and the entries ciof the vector c= (Id−γBM π)−1b∈Rn. We prove the result in Appendix A.4.
We note that in (12) both BandMπaren×nmatrices and therefore the characterization of ˆqπ
amounts to solving a n×nlinear system. For settings where nis large, one can adopt random
projection methods such as Nyström approximation to learn Tnandrn[32]. These strategies have
been recently shown to significantly reduce the computational load of learning while retaining the
same empirical and theoretical performance as their non-approximated counterparts [33, 34].
We conclude this section noting how (13) implies that we only need to be able to evaluate π, but we
do not need explicit knowledge of Pπas operator. As we shall see, this property will be instrumental
to prove generalization bounds for our proposed PMD algorithm in Sec. 4.
4 Proposed Algorithm: POWR
We are ready to describe our algorithm for world model-based PMD. In the following, we restrict to
the case where |A|<∞is a finite set. As introduced in Sec. 2, policy mirror descent methods are
mainly characterized by the choice of Bregman divergence Dused for the mirror descent update and,
in the case of inexact methods, the estimator ˆqπtof the action-value function qπtfor the intermediate
policies generated by the algorithm.
In POWR, we combine the CME world model presented in Sec. 3 with mirror descent steps using
the Kullback-Leibler divergence DKL(p;p′) =P
a∈Apalog(pa/p′
a)in the update of (2). It was
shown in [ 10] that in this case PMD corresponds to Natural Policy Gradient [ 28]. As showed in [ 35,
Example 9.10], the solution to (2) can be written in closed form for any x∈ X as
πt+1(·|x) =πt(·|x)eηˆqπt(x,·)
P
a∈Aπt(a|x)eηˆqπt(x,a), (14)
where we used the estimated action-value function ˆqπfrom Proposition 3. Additionally, the formula
above can be applied recursively, expressing πt+1as the softmax operator applied to the discounted
6sum of the action-value functions up to the current iteration
πt+1(·|x) = SOFTMAX 
log(π0(·|x)) +ηtX
s=0ˆqπs(x,·)!
. (15)
Choice of the Feature Maps . A key question to address before adopting the action-value estimators
introduced in Sec. 3 is choosing the two spaces FandGto perform world model learning. Specifically,
to apply Proposition 3 and obtain proper estimators ˆqπt, we need to guarantee that all policies
generated by the PMD update (14) are(G,F)-compatible (Definition 1). The following result
describes a suitable family of such spaces.
Proposition 4 (Separable Spaces) .Letϕ:X → H be a feature map into a Hilbert space H.
LetF=H ⊗ H andG=R|A|⊗ H with feature maps respectively φ(x) =ϕ(x)⊗ϕ(x)and
ψ(x, a) =ϕ(x)⊗ea, with ea∈R|A|the one-hot encoding of action a∈ A. Letπ:X → ∆(A)be
a policy such that π(a|·) =⟨pa, ϕ(·)⟩withpa∈ H for any a∈ A. Then, πis(G,F)-compatible.
Proposition 4 (proof in Appendix A.5) states that for the specific choice of function spaces F=H⊗H
andG=R|A|⊗H,we can guarantee (G,F)-compatibility, provided that His rich enough to “contain”
allπ(a|·)fora∈ A.We postpone the discussion on identifying a suitable spaces Hfor PMD to
Sec. 5, since (G,F)-compatibility is not needed to mechanically apply Proposition 3 and obtain an
estimator ˆqπ. This is because (12) exploits a kernel-trick to bypass the need to know Pπexplicitly
and rather requires only to be able to evaluate πon the training data. The latter is possible for πt+1,
thanks to the explicit form of the PMD update in (14). We can therefore present our algorithm.
POWR . Alg. 1 describes Policy mirror descent with Operator World-models for Reinforcement
learning (POWR) . Following the intuition of Proposition 4, the algorithm assumes to work with
separable spaces. During an initial phase, we learn the world model Tn=S∗
nK−1
λZnand the reward
rn=S∗
nbfitting the conditional mean embedding described in (10) on a dataset (xi, ai, x′
i, ri)n
i=1
(e.g. obtained via experience replay [ 36]). Once the world model has been learned, we optimize
the policy and perform PMD iterations via (15). In this second phase, we first evaluate the past
(cumulative) action-value estimatorsPt
s=0ˆqπsto obtain the policy πt+1(·|xi)by (inexact) PMD
via the softmax operator in (15). We use the newly obtained policy to compute the matrix Mπt+1
defined in (13), which is a key component to obtain the estimator ˆqπt+1forqπt+1. We note that
in the case of the separable spaces of Proposition 4, this matrix reduces to the n×nmatrix with
entries k(x′
i, xj)πt+1(aj|x′
i), where k(x′
i, xj) =⟨ϕ(x′
i), ϕ(xj)⟩is the kernel matrix between initial
and evolved states. Finally, we obtain c= (Id−γK−1
λM)−1band model ˆqπt+1=S∗
ncaccording to
Proposition 3.
Clearly, the world model learning and PMD phases can be alternated in POWR, essentially finding
a trade-off between exploration and exploitation. This could possibly lead to a refinement of the
world model as more observations are integrated into the estimator. While in this work we do not
investigate the effects of such alternating strategy, Thm. 7 offers relevant insights in this sense. The
result characterizes the behavior of the PMD algorithm when combined with a varying (possibly
increasing) accuracy in the estimation of the action-valued function (see Sec. 5 for more details).
5 Theoretical Analysis
We now show that POWR converges to the global maximizer of the RL problem in (1). To this end,
we first identify a family of function spaces guaranteed to be compatible with the policies generated
by Alg. 1. Then, we provide an extension of the result in [ 12] for inexact PMD to infinite state spaces
X, showing the impact of the action-value approximation error on the convergence rates. For the
estimator introduced in Proposition 3, we relate this error to the approximation errors of Tnandrn
leveraging the Simulation Lemma A.6. Finally, we use recent advancements in the characterization
of CMEs’ fast learning rates to bound the sample complexity of these latter quantities, yielding error
bounds for POWR.
POWR is Well-defined . To properly apply Proposition 3 to estimate the action-value function of
any PMD iterate πt, we need to guarantee that every iterate belongs to the space Haccording to
Proposition 4. The following result provides such a family of spaces.
7Theorem 5. LetX ⊂Rdbe a compact set and let H=W2,s(X)be the Sobolev space of smoothness
s >0(see e.g. [ 37]). Let πt(a|·)andˆqπt(·, a)belong to Hfor any a∈ A andπt(a|x)>0for any
x∈ X. Then the policy πt+1solution to the PMD update in (14) belongs to H.
According to Thm. 5, Sobolev spaces offer a viable choice for compatibility with PMD-generated
policies. This observation is further supported by the fact that Sobolev spaces of smoothness s > d/ 2
are so-called reproducing kernel Hilbert spaces (rkhs) (see e.g. [ 38, Ch. 10]). We recall that rkhs are
always naturally equipped with a ϕ:X → H such that the inner product ⟨ϕ(x), ϕ(x′)⟩=k(x, x′)
defines a so-called reproducing kernel, namely a positive definite function that is (usually) efficient
to evaluate, even if ϕ(x)is high or infinite dimensional. For example, H=W2,s(X)withs=⌈d
2⌉
has associated kernel k(x, x′) =e−∥x−x′∥/σwith bandwidth σ >0[38]. By applying Thm. 5 to the
iterates generated by Alg. 1 we have the following result.
Corollary 6. With the hypothesis of Proposition 4 let H=W2,s(X)withs > d/ 2. Let Tn∈
HS(F,G)andrn∈ G characterized as in Proposition 3. Let π0(a|·)∝eηq0(·,a)forq0such that
q0(·, a)∈ H anya∈ A. Then, for any t∈Nthe PMD iterates πtgenerated by Alg. 1 are such that
πt(a|·)∈ H and hence are (G,F)-compatible.
The above corollary guarantees us that if we are able to learn our estimates for the action-value
function in a suitably regular Sobolev space H, then POWR is well-defined. This is a necessary
condition to then being able to study its theoretical behavior in our main result. We report the proofs
of Thm. 5 and Cor. 6 in Appendix C.1.
Inexact PMD Converges . We now present a more rigorous version of the characterization of the
convergence rates of the inexact PMD algorithm discussed informally in Thm. 1.
Theorem 7 (Convergenge of Inexact PMD) .Let(πt)t∈Nbe a sequence of policies generated by
Alg.1that are all (G,F)-compatible. If the action-value functions ˆqπtare estimated with an error
∥qπt−ˆqπt∥∞≤εt, the iterates of Alg. 1 converge to the optimal policy as
J(π∗)−J(πT)≤εT+O 
1
T+1
TT−1X
t=0εt!
, (16)
where π∗:X → ∆(A)is a measurable maximizer of (8).
Thm. 7 shows that inexact PMD can behave comparably to its exact version provided that 1) the
action value functions ˆqπtare estimated with increasing accuracy, and 2) that the sequence of
policies is (G,F)-compatible, for example in the Sobolev-based setting of Cor. 6. Specifically, if
∥qπt−ˆqπt∥∞≤O(1/t)for any t∈N, the convergence rate of inexact PMD is of order O(logT/T),
only a logarithmic factor slower than exact PMD. This means that we do not necessarily need a good
approximation of the world model from the beginning but rather a strategy to improve upon such
approximation as we perform more PMD iterations. This suggests adopting an alternating strategy
between exploration (world model learning) and exploitation (PMD steps), as suggested in Sec. 4.
We do not investigate this question in this work.
The demonstration technique used to prove Thm. 7 follows closely [ 12, Thm. 8 and 13]. We provide
a proof in Appendix B.1 since the original result did not allow for a decreasing approximation error
but rather assumed a constant one. Moreover, extending it to the case of infinite Xrequires taking
care of additional details related to potential measurability issues.
Action-value approximation error in terms of World Model estimates . Thm. 7 highlights the
importance of studying the error of the estimator for the action-value functions produced by Alg. 1.
These objects are obtained via the formula described in Proposition 3 in terms of Tn,rnandPπ. The
exact qπhas an analogous closed-form characterization in terms of T,randPπas expressed in (7),
and motivating our operator-based approach. The following result compares these quantities in terms
of the approximation errors of the world model and the reward function.
Lemma 8 (Implications of the Simulation Lemma) .LetTnandrnthe empirical estimators of
the transfer operator Tand reward function ras defined in Proposition 3, respectively. If T
satisfies Asm. 1, r∈ G, and γTn< γ′<1, then, for every (G,F)-compatible policy π
ˆqπ−qπ
∞≤1
1−γ′"
const ψrn−r
G+γr
∞
1−γT|F−Tn
HS#
.
8In the result above, when applied to a function in G, such as rn, the uniform norm is to be interpreted
as the uniform norm of the evaluation of such function, namely ∥rn∥∞= sup(x,a)∈Ω|⟨rn, ψ(x, a)⟩|,
and analogously for Tn. The proof, reported in Appendix C.2, follows by first decomposing the
difference qπ−ˆqπwith the simulation lemma [ 30, Lemma 2.2] and then applying the triangular
inequality for the uniform norm.
POWR converges . We are ready to state the convergence result for Alg. 1. We consider the setting
where the dataset used to learn Tn(andrn) is made of i.i.d. triplets (xi, ai, x′
i)with(xi, ai)sampled
from a distribution ρ∈ P(Ω)supported on all Ω(such as the state occupancy measure (see e.g.
[11] or Appendix A.3) of the uniform policy π(·|x) = 1 /|A|) and x′
isampled from τ(·|xi, ai). To
guarantee bounds in uniform norm, the result makes a further regularity assumption, of the transfer
operator (and the reward function)
Assumption 2 (Strong Source Condition) .Letρ∈ P(Ω) andCρthe covariance operatorP
a∈AR
Xρ(dx, a)ψ(x, a)⊗ψ(x, a). The transition operator Tand the reward function rare
such that T|F∈HS(F,G)andr∈ G. Further,(T|F)∗C−β
ρ
HS<∞andC−β
ρr
G<∞for
some β >0.
Asm. 2 imposes a strong requirement to the so-called source condition , a quantity that describes
how well the target objective of the learning process (here Tandr) “interact” with the sampling
distribution. The assumption is always satisfied when the hypothesis space is finite dimensional
(e.g. in the tabular RL setting) and imposes additional smoothness on Tandrwhen belonging to a
Sobolev space. Equipped with this assumption, we can now state the convergence theorem for Alg. 1.
Theorem 9. Let(πt)t∈Nbe a sequence of policies generated by Alg. 1 in the same setting of Cor. 6. If
the action-value functions ˆqπtare estimated from a dataset (xi, ai;x′
i)n
i=1with(xi, ai)∼ρ∈ P(Ω)
such that Asm. 2 holds with parameter β, the iterates of Alg. 1 converge to the optimal policy as
J(π∗)−J(πT)≤O1
T+δ2n−α
with probability not less than 1−4e−δ. Here, α∈
β
2+2β,β
1+2β
andπ∗:X → ∆(A)is a
measurable maximizer of (8).
The proof of Thm. 9 is reported in Appendix C and combines the results discussed in this section
with fast convergence rates for the least-squares [ 39] and CME [ 21] estimators. In particular we first
use Thm. 5 to guarantee that the policies produced by Alg. 1 are all (G,F)-compatible and therefore
that applying Proposition 3 to obtain an estimator for the action-value function is well-defined. Then,
we use Lemma 8 to study the approximation error of these estimators in terms of our estimates for
the world model and the reward function. Bounds on these quantities are then used in the result for
inexact PMD convergence in Thm. 7. We note here that since the latter results require convergence in
uniform norm, we cannot leverage standard results for least-squares and CME convergence, which
characterize convergence in L2(Ω, µ)and would only require Asm. 1 (Linear MDP) to hold. Rather,
we need to impose Asm. 2 to guarantee faster rates in uniform norm.
6 Experimental results
We empirically evaluated POWR on classical Gym environments [ 25], ranging from discrete
(FrozenLake-v1 ,Taxi-v3 ) to continuous state spaces ( MountainCar-v0 ). To ensure balancing
between exploration and exploitation of our method, we alternated between running the environment
with the current policy to collect samples for world model learning and running Alg. 1 for a number
of steps to generate a new policy. Appendix D provides implementation details regarding this process
as well as additional results.
Fig. 1 compares our approach with the performance of well-established baselines including A2C [ 40],
DQN [ 4], TRPO [ 7], and PPO [ 6]. The figure reports the average cumulative reward obtained by the
models on test environments with respect to the number of interactions with the MDP ( timesteps
in log scale in the figure) across 7 different training runs. In all plots, the horizontal dashed line
represents the “success” threshold for the corresponding environment, according to official guidelines.
We observe that our method outperforms all competitors by a significant margin in terms of sample
complexity, that is, the reward achieved after a given number of timesteps. In the case of the Taxi-v3
9103104105
Timestep (logscale)0.00.20.40.60.81.0Reward
A2C
DQN
TRPO
PPO
POWR (Ours)(a)FrozenLake-v1
103104105
Timestep (logscale)800
700
600
500
400
300
200
100
0Reward (b)Taxi-v3
103104105106
Timestep (logscale)200
180
160
140
120
100
Reward (c)MountainCar-v0
Figure 1: The plots show the average cumulative reward in different environments with respect to the
timesteps (i.e. number of interactions with MDP). The dark lines represent the mean of the cumulative
reward and the shaded area is the minimum and maximum values reached across 7independent runs.
The horizontal dashed lines represent the reward threshold proposed by the Gym library [25].
environment, it avoids converging to a local optimum, in contrast every other method with the
exception of DQN. On the downside, we note that our method exhibits less stability than other
approaches, particularly during the initial stages of the training process. This is arguably due to a
sub-optimal interplay between exploration and exploitation, which will be the subject of future work.
7 Conclusions and Future Work
Motivated by recent advancements in policy mirror descent (PMD), this work introduced a novel
reinforcement learning (RL) algorithm leveraging these results. Our approach operates in two,
possibly alternating, phases: learning a world model and planning via PMD. During exploration,
we utilize conditional mean embeddings (CMEs) to learn a world model operator, showing that this
procedure is well-posed when performed over suitable Sobolev spaces. The planning phase involves
PMD steps for which we guarantee convergence to a global optimum at a polynomial rate under
specific MDP regularities.
Our analysis opens avenues for further exploration. Firstly, extending PMD to infinite action spaces
remains a challenge. While we introduced the operatorial perspective on RL for infinite state space
settings, the PMD update with KL divergence requires approximation methods (e.g., Monte Carlo)
whose impact on convergence requires investigation. Secondly, scalability to large environments
requires adopting approximated yet efficient CME estimators like Nystrom [ 34] or reduced-rank
regressors [ 41,42]. Thirdly, a question we touched upon only empirically, is whether alternating world
model learning with inexact PMD updates benefits the exploration-exploitation trade-off. Studying
this strategy’s impact on convergence is a promising future direction. Finally, a crucial question is
generalizing our policy compatibility results beyond Sobolev spaces. Ideally, a representation learning
process would identify suitable feature maps that guarantee compatibility with the PMD-generated
policies while allowing for added flexibility in learning the world model.
Acknowledgments and Disclosure of Funding
We acknowledge financial support from NextGenerationEU and MUR PNRR project PE0000013
CUP J53C22003010006 “Future Artificial Intelligence Research (FAIR)”, EU grant ELISE (GA no
951847) and EU Project ELIAS (GA no 101120237).
References
[1]Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press,
2018.
[2]OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob Mc-
Grew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning
dexterous in-hand manipulation. The International Journal of Robotics Research , 39(1):3–20,
2020.
10[3]David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van
Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc-
tot, et al. Mastering the game of go with deep neural networks and tree search. Nature ,
529(7587):484–489, 2016.
[4]V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou,
Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning.
arXiv1312.5602 , 2013.
[5]Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. Resource man-
agement with deep reinforcement learning. In Proceedings of the 15th ACM workshop on hot
topics in networks , pages 50–56, 2016.
[6]John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv1707.06347 , 2017.
[7]John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust
region policy optimization, 2017.
[8]Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor, 2018.
[9] Dimitri Bertsekas. Dynamic Programming and Optimal Control: Volume I , volume 4. Athena
scientific, 2012.
[10] Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization:
Global convergence and faster rates for regularized mdps. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence , volume 34, pages 5668–5675, 2020.
[11] Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of
policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine
Learning Research , 22(98):1–76, 2021.
[12] Lin Xiao. On the convergence rates of policy gradient methods. Journal of Machine Learning
Research , 23(282):1–36, 2022.
[13] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In International Conference on Machine Learning , pages 2160–2169. PMLR, 2019.
[14] Kenji Fukumizu, Francis R. Bach, and Michael I. Jordan. Dimensionality reduction for super-
vised learning with reproducing kernel Hilbert spaces. Journal of Machine Learning Research ,
5:73–99, 2004.
[15] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Schölkopf, et al. Kernel
mean embedding of distributions: A review and beyond. Foundations and Trends ®in Machine
Learning , 10(1-2):1–141, 2017.
[16] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. Ad-
vances in neural information processing systems , 31, 2018.
[17] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM
Sigart Bulletin , 2(4):160–163, 1991.
[18] Jürgen Schmidhuber. Making the world differentiable: on using self supervised fully recurrent
neural networks for dynamic reinforcement learning and planning in non-stationary environ-
ments , volume 126. Inst. für Informatik, 1990.
[19] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains
through world models. arXiv preprint arXiv:2301.04104 , 2023.
[20] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A general framework for consistent
structured prediction with implicit loss embeddings. Journal of Machine Learning Research ,
21(98):1–67, 2020.
11[21] Zhu Li, Dimitri Meunier, Mattes Mollenhauer, and Arthur Gretton. Optimal rates for regularized
conditional mean embedding learning. Advances in Neural Information Processing Systems ,
35:4433–4445, 2022.
[22] Steffen Grünewälder, Guy Lever, Luca Baldassarre, Massimiliano Pontil, and Arthur Gretton.
Modelling transition dynamics in mdps with rkhs embeddings. In Proceedings of the 29th
International Conference on International Conference on Machine Learning , pages 1603—-
1610, 2012.
[23] Antoine Moulin and Gergely Neu. Optimistic planning by regularized dynamic programming.
InInternational Conference on Machine Learning , pages 25337–25357. PMLR, 2023.
[24] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based rein-
forcement learning with value-targeted regression. In International Conference on Machine
Learning , pages 463–474. PMLR, 2020.
[25] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym. arxiv. arXiv preprint arXiv:1606.01540 , 10, 2016.
[26] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters , 31(3):167–175, 2003.
[27] Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends ®
in Machine Learning , 8(3-4):231–357, 2015.
[28] Sham M. Kakade. A natural policy gradient. Advances in Neural Information Processing
Systems , 14, 2001.
[29] C.D. Aliprantis and K.C. Border. Infinite Dimensional Analysis: A Hitchhiker’s Guide . Studies
in Economic Theory. Springer, 1999.
[30] Alekh Agarwal, Nan Jiang, Sham M. Kakade, and Wen Sun. Reinforcement learning: Theory
and algorithms. 2021. URL https://rltheorybook.github.io , 2022.
[31] Steffen Grünewälder, Guy Lever, Luca Baldassarre, Sam Patterson, Arthur Gretton, and Mas-
similano Pontil. Conditional mean embeddings as regressors. In Proceedings of the 29th
International Conference on International Conference on Machine Learning , pages 1803–1810,
2012.
[32] Christopher Williams and Matthias Seeger. Using the Nyström method to speed up kernel
machines. Advances in Neural Information Processing Systems , 13, 2000.
[33] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nyström computa-
tional regularization. Advances in Neural Information Processing Systems , 28, 2015.
[34] Giacomo Meanti, Antoine Chatalic, Vladimir R. Kostic, Pietro Novelli, Massimiliano Pontil,
and Lorenzo Rosasco. Estimating Koopman operators with sketching to provably learn large
scale dynamical systems. Advances in Neural Information Processing Systems , 36, 2023.
[35] Amir Beck. First-Order Methods in Optimization . Society for Industrial and Applied Mathe-
matics, 2017.
[36] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature , 518(7540):529–533, 2015.
[37] Robert A. Adams and John J. F. Fournier. Sobolev Spaces . Elsevier, 2003.
[38] Holger Wendland. Scattered data approximation , volume 17. Cambridge University Press,
2004.
[39] Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares
algorithms. Journal of Machine Learning Research , 21(205):1–38, 2020.
12[40] V olodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lill-
icrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. arXiv1602.01783 , 2016.
[41] Vladimir R. Kostic, Pietro Novelli, Andreas Maurer, Carlo Ciliberto, Lorenzo Rosasco, and
Massimiliano Pontil. Learning dynamical systems via Koopman operator regression in reproduc-
ing kernel Hilbert spaces. Advances in Neural Information Processing Systems , 35:4017–4031,
2022.
[42] Giacomo Turri, Vladimir Kostic, Pietro Novelli, and Massimiliano Pontil. A randomized
algorithm to solve reduced rank operator regression. arXiv preprint arXiv:2312.17348 , 2023.
[43] Gene H. Golub and Charles F. Van Loan. Matrix Computations . Johns Hopkins University
Press, 2013.
[44] O. Kallenberg. Foundations of Modern Probability . Probability and Its Applications. Springer
New York, 2002.
[45] Giulia Luise, Saverio Salzo, Massimiliano Pontil, and Carlo Ciliberto. Sinkhorn barycenters
with free support via frank-wolfe algorithm. Advances in Neural Information Processing
Systems , 32, 2019.
[46] Vladimir R. Kostic, Karim Lounici, Pietro Novelli, and Massimiliano Pontil. Sharp spectral
rates for koopman operator learning. Advances in Neural Information Processing Systems , 36,
2023.
[47] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah
Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of
Machine Learning Research , 22(268):1–8, 2021.
13Appendix
The appendices are organized as follows:
•Appendix A discuss the operatorial formulation of RL and show how to derive the operator-
based results in this work.
•Appendix B focuses on policy mirror descent (PMD) and its convergence rate in the inexact
setting.
• Appendix C proves the main result of this work, namely the theoretical analysis of POWR.
• Appendix D provide details on the experiments reported in this work.
A Operatorial Results
A.1 Auxiliary Lemma
We recall here a corollary of the Sherman-Woodbury identity [43].
Lemma A.1. LetAandBtwo conformable linear operators such that (I+AB)−1is invertible.
Then (I+AB)−1A=A(I+BA)−1
Proof. The result is obvious if Ais invertible. More generally, we consider the following two
applications of the Sherman-Woodbury [43] formula
(I+AB)−1=I−A(I+BA)−1B (A.1)
and
(I+BA)−1=I−(I+BA)−1BA. (A.2)
Multiplying the two equations by Arespectively to the right and to the left, we obtain the desired
result.
A.2 Markov operators and their properties
We recall here the notion of Markov operators, which is central for a number of results in the
following. We refer to [29, Chapter 19] for more details on the topic.
Definition A.1 (Markov operators) .LetXandYbe Polish spaces. A bounded linear operator
L(Bb(X), Bb(Y))is a Markov operator if is positive and maps the unit function to itself, that is:
a.f≥0∈Bb(X) =⇒Pf≥0∈Bb(Y),
b.P1X=1Y,
where 1X:X → R(respectively 1Y) denotes the function taking constant value equal to 1onX
(respectively Y).
We recall that Markov operators are a convex subset of L(Bb(X), Bb(Y)). Here we denote this space
asLM(Bb(X), Bb(Y)). Direct inspection of (5) and (6) shows that the transfer operator Tassociated
to an MDP and the policy operator Pπassociated to a policy πare both Markov operators.
Markov Operators and Policy Operators . In (6) we defined the policy operator Pπassociated to a
policy π. It turns out that the converse is also true, namely that any such Markov operator is a policy
operator.
Proposition A.2. LetP∈ LM(Bb(X), Bb(Ω)) be a Markov operator. Then there exists πP, such
that the associated policy operator corresponds to P, namely PπP=P.
Proof. Define the map πP:X → M (A)taking value in the space of bounded Borel measures over
Asuch that, for any x∈ X and any B ⊆ A Borel measurable subset
πP(B|x) = (P1X×B)(x). (A.3)
14We need to guarantee that for every x∈ X the function πP(·|x)is a signed measure. To show this,
first note that the operation defined by πPis well-defined, since for any measurable set Bthe function
1X×B is also measurable, making πP(B|x)well defined as well. Moreover, since 1∅(a) = 0 for any
a∈ A, it implies that 1X×∅= 0 and therefore πP(∅|x) = 0 for any x∈ X. Finally, σ-additivity
follows from the definition of indicator functions, namely 1S∞
i=1Bi=P∞
i=11Bifor any family of
pair-wise disjoint sets (Bi)n
i=1, which implies πP(S∞
i=1Bi|x) =P∞
i=1πP(Bi|x)for any x∈ X.
We now apply the two properties of Markov operators to show that πPtakes values in P(A), namely
it is a non-negative measure that sums to 1. Since Markov operators map non-negative functions in
non-negative functions and since 1X×B≥0for any B ⊆ X , we have π(·|x)≥0as well for any
x∈ X. Moreover, since Ω =X × A andP1Ω=1X, we have
π(A|x) = (P1Ω)(x) =1X(x) = 1 , (A.4)
for any x∈ X. Therefore πP(·|x)is a probability measure for any x∈ X. Direct application of (6)
shows that the associated policy operator corresponds to P, namely PπP=Pas desired.
Given the correspondence between policies and their Markov operator according to (6) and Proposi-
tion A.2, in the following we will denote the policy operator associated to a policy πonlyPwhere
clear from context.
With the definition of the Markov operator in place, we can now prove the following result introduced
in the main paper.
Proposition 2 (Well-specified CME) .Under Assumption 1, (T|F)∗= (T∗)|Gand, for any (x, a)∈Ω
(T|F)∗ψ(x, a) =Z
Xφ(x′)τ(x′|x, a) =E[φ(X′)|X=x, A=a]. (9)
Proof. Recall that since they are Hilbert spaces F∼=F∗andG∼=G∗are isometric to their dual and
therefore we can interpret any f∈ F as the function f(·) =⟨f, φ(·)⟩with some abuse of notation,
where clear from context. By Assumption 1 we have that T|Ftakes values in G. This means that
(T|Ff)∈ Gor, in other words
⟨T|Ff, ψ(x, a)⟩= (T|Ff)(x, a)
=Z
Ff(x′)τ(dx′|x, a)
=Z
F⟨f, φ(x′)⟩τ(dx′|x, a)
=
f,Z
φ(x′)τ(dx′|x, a)
,
from which we obtain
⟨f,(T|F)∗ψ(x, a)⟩=
f,Z
φ(x′)τ(dx′|x, a)
.
Since the above equality holds for any f∈ F (9) holds, as desired.
We note that the result can be extended to the setting where T|F(F)⊆ G, namely the image of T|F
is contained in G, namely a sort of (F,G)-compatibility for the transition operator (see Definition 1).
A.3 The operatorial formulation of RL
According to the operatorial characterization in (7), the action value function of a policy πis directly
related to the action of the associated policy operator P. To highlight this relation, we will adopt the
following notation:
•Action-value (or Q-)function.
q(P) = (Id−γTP)−1r. (A.5)
15•Value function.
v(P) =Pq(P). (A.6)
•Cumulative reward. The RL objective functional
J(P) =D
P(Id−γTP)−1r, νE
=⟨Pq(P), ν⟩=⟨v(P), ν⟩. (A.7)
•State visitation (or State occupancy) measure. By the characterization of the adjoints of
PandT(see discussion in Section 3 we can represent the evolution of a state distribution νt
at time tto the next state distribution as νt+1=T∗P∗νt. Applying this relation recursively,
we recover the state visitation probability associated to the starting state distribution ν0=
ν∈ P(X), the MDP with transition Tand the policy Pas
dν(P) = (1 −γ)∞X
t=0γt(T∗P∗)tν= (1−γ) (Id−γPT)−∗ν, (A.8)
where the (1−γ)γtis a normalizing factor to guarantee that the series corresponds to a
convex combination of the probability distributions νt, hence guaranteeing dν(P)to be
well-defined (namely it belongs to P(X)).
Previous well-known RL results in operator form . Under the operatorial formulation of RL, we
can recover several well-known results from the reinforcement literature with concise proofs. We
recall here a few of these results that will be useful in the following.
Remark A.1. Algebraic manipulation of the cumulative expected reward J(P)implies
J(P) =D
P(Id−γTP)−1r, νE
=D
Pr,(Id−γPT)−∗νE
=1
1−γ⟨Pr, dν(P)⟩,
where we used Lemma A.1 and dν(P)is the state visitation distribution starting from νand following
the policy P.
The following result, known as Performance Difference Lemma [see e.g. 11, Lemma 1.16], will be
instrumental to prove the convergence rates for PMD in Theorem 7.
Lemma A.3 (Performance difference) .LetP1,P2two policy operators. The following equality holds
J(P1)−J(P2) =1
1−γ⟨(P1−P2)q(P2), dν(P1)⟩. (A.9)
Proof. Using the definition of J(P1)and Lemma A.1 one gets
J(P1)−J(P2) =D
P1(Id−γTP1)−1r, νE
−D
P2(Id−γTP2)−1r, νE
=D
(Id−γP1T)−1P1r, νE
−D
P2(Id−γTP2)−1r, νE
=D
(Id−γP1T)−1P1(Id−γTP2) (Id−γTP2)−1r, νE
−D
(Id−γP1T)−1(Id−γP1T)P2(Id−γTP2)−1r, νE
=D
(Id−γP1T)−1[P1(Id−γTP2)−(Id−γP1T)P2] (Id−γTP2)−1r, νE
=D
(Id−γP1T)−1[P1−P2] (Id−γTP2)−1r, νE
=1
1−γ⟨(P1−P2)q(P2), dν(P1)⟩.
A direct consequence of the operator formulation of the performance difference lemma is the following
operator-based characterization of the differential behavior of the RL objective. The result can be
found in [ 11] for the case of finite state and action spaces, however here the operatorial formulation
allows for a much more concise proof.
16Corollary A.4 (Directional derivatives) .For any two Markov P1,P2:Bb(X)→Bb(Ω), we have
that the directional derivative in P1towards P2is
lim
h→0J(P1+h(P2−P1))−J(P1)
h=1
1−γ⟨(P2−P1)q(P1), dν(P1)⟩. (A.10)
Proof. The result follows by recalling that the space of Markov operators is convex, namely for
anyh∈[0,1]the term Ph=P1+h(P2−P1)is still a Markov operator. Therefore, we can apply
Lemma A.3 to obtain
J(Ph)−J(P1) =⟨(Ph−P1)q(P1), dν(Ph)⟩ (A.11)
=h⟨(P2−P1)q(P1), dν(Ph)⟩. (A.12)
We can therefore divide the above quantity by hand send h→0. The result follows by observing
thatdν(Ph) = (Id−γTPh)−∗ν→(Id−γTP1)−∗ν=dν(P1)forh→0, since ∥Ph∥= 1for any
h∈[0,1]and the function M7→(I−γTM)−1is continuous on the open ball of radius 1/γ > 1in
L(Bb(Ω), Bb(X))with respect to the operator norm.
Properties of (Id−γPT)−1. The quantity (Id−γPT)−1(note, not (Id−γTP)−1) plays a central
role in the study of POWR. We prove here a few properties that will be useful in the following.
Lemma A.5 (Properties of Id−γPT).The following facts are true:
1. For any f≥0∈Bb(X)it holds (Id−γPT)−1f≥f.
2. The operator (1−γ)(Id−γPT)−1is a Markov operator.
3.For any positive measure ν∈ M(Bb(X))it holds (1−γ)∥(Id−γPT)−∗ν∥TV=∥ν∥TV.
4. For any positive measure ν∈ M(Bb(X))it holds ∥P∗ν∥TV=∥ν∥TV.
5.For any bounded linear operator X, policy operator Pand discount factor γ <∥X∥, it holds(Id−γXP)−1
∞≤1/(1−γ∥X∥).
Proof. Since both TandPare Markov operators by construction, it immediately follows that their
composition is a Markov operator as well. Using the Neumann series representation of (Id−γPT)−1
it follows that for all f≥0∈Bb(X)
(Id−γPT)−1f=∞X
t=0γt(PT)tf=f+∞X
t=1γt(PT)tf≥f≥0,
proving (1). Further,
(Id−γPT)−11X=∞X
t=0γt(PT)t1X=∞X
t=0γt1X=1X
1−γ
showing that (1−γ)(Id−γPT)−11X=1Xand proving (2). Finally, since (1−γ)(Id−γPT)−1
andPare Markov operators, (3) and (4) follow from the direct application of [ 29, Theorem 19.2].
For the last point (5), let f∈Bb(Ω). AsPis a conditional expectation operator, it holds that
∥XPf∥∞≤ ∥X∥P(1Ω∥f∥∞) =∥X∥∥f∥∞
Where the inequality is just the conditional version of Jensen’s inequality [ 44, Chapter 5] applied on
the (convex) ∥·∥∞function, while the equality comes from the fact that TPis a Markov operator.
Then, we have
sup
∥f∥∞=1(Id−γXP)−1f
∞= sup
∥f∥∞=1∞X
t=0(γXP)tf
∞
≤sup
∥f∥∞=1∞X
t=0γt(XP)tf
∞
≤sup
∥f∥∞=1∞X
t=0γtXtf
∞
(∥f∥∞= 1) =1
1−γX.
17Simulation Lemma . We report here the Simulation lemma, since it will be key to bridging the gap
between Policy Mirror Descent and Conditional Mean Embeddings in Theorem 9 through Lemma 8.
Lemma A.6 (Simulation Lemma [ 30]-Lemma 2.2) .Letγ >0and let T1,T2two linear operators
with operator norm strictly less than γ. Let Pbe a policy operator. Denote by q(P,T) = ( Id−
γTP)−1rthe (generalized) action-value function associated to these terms and v(P,T) =Pq(P,T)
the corresponding value function. Then the following equality holds
q(P,T1)−q(P,T2) =γ(Id−γT1P)−1(T2−T1)v(P,T2) (A.13)
Proof. Using the same technique of the proof of Lemma A.3 one has
q(P,T1)−q(P,T2) = (Id−γT1P)−1r−(Id−γT2P)−1r
=γ(Id−γT1P)−1(T2−T1)P(Id−γT2A)−1r
=γ(Id−γT1P)−1(T2−T1)v(P,T2)
where we have used fact that for any two invertible operators MandPit holds M−1−P−1=
M−1(P−M)P−1for the second equation and applied the operatorial characterization of the value
function to conclude the proof.
We then have the following result, which hinges on a generalization of the standard Simulation lemma
in [30, Lemma 2.2] where we account also for the reward function to vary.
Corollary A.7. Letγ >0and let T1,T2two linear operators with operator norm strictly less than γ.
Letr1andr2be two reward functions and Pa policy operator. Denote by q(P,T, r) = (Id−γTP)−1r
the (generalized) action-value function associated to these terms and v(P,T, r) =Pq(P,T, r)the
corresponding value function. Then the following equality holds
q(P,T1, r1)−q(P,T2, r2) = (Id−γT1P)−1(r1−r2) +γ(Id−γT1P)−1(T2−T1)v(P,T2, r2).
Proof. The difference between action-value functions can be written as
q(P,T1, r1)−q(P,T2, r2) = (Id−γT1P)−1r1−(Id−γT2P)−1r2
= (Id−γT1P)−1(r1−r2) +
(Id−γT1P)−1−(Id−γT2P)−1
r2
where we added and removed a term (Id−γT1P)−1r2. The result follows by plugging in the
Simulation Lemma A.6 for the second term of the right hand side.
The corollary above will be useful in Appendix C to control the approximation error of the estimates
ˆqπtappearing in the convergence rates for inexact PMD in Theorem 7.
A.4 Action-value Estimator for (G,F)-compatible Policies
We can leverage the notation introduced in this section to prove the following form for the world
model-based estimator of the action-value function.
Proposition 3. LetTn=S∗
nBZn∈HS(F,G)andrn=S∗
nb∈ Gfor respectively a B∈Rn×nand
b∈Rn. Letπbe(G,F)-compatible. Then,
ˆqπ= (Id−γTnPπ)−1rn=S∗
n(Id−γBM π)−1b (12)
where Mπ=ZnPπS∗
n∈Rn×nis the matrix with entries
 
Mπ
ij=⟨φ(x′
i),Pπψ(xj, aj)⟩=Z
A⟨ψ(x′
i, a), ψ(xj, aj)⟩π(da|x′
i). (13)
Proof. By hypothesis
ˆqπ= (Id−γTnPπ)−1rn= (Id−γS∗
nBZnPπ)−1S∗
nb. (A.14)
18Eq. (12) follows by applying Lemma A.1. Eq. (13) can be verified by direct calculation. Denote by
(ei)m
i=1the vectors of the canonical basis in Rn. Then, for any i, j= 1, . . . , n
(Mπ)ij=⟨ei, Mπej⟩=⟨ei, ZnPπS∗
nej⟩=⟨Z∗
nei,PπS∗
nej⟩. (A.15)
Now, we recall that the two operators Sn:G →RnandZn:F →Rnare the evaluation operators
for respectively the points (xi, ai)n
i=1and(x′
i)n
i=1. Namely, for any vector v∈Rn
S∗
nv=nX
i=1viψ(xi, ai) and Z∗
nv=nX
i=1viφ(x′
i). (A.16)
This implies that
(Mπ)ij=⟨Z∗
nei,PπS∗
nej⟩=⟨φ(x′
i),Pπψ(xj, aj)⟩ (A.17)
Since Pπis(G,F)-compatible by hypothesis, we can leverage the same reasoning used in Proposi-
tion 2 to show that
(Pπ|G)∗φ(x′) =Z
Aψ(x′, a)π(da|x′) (A.18)
for any x′∈ X. By plugging this equation in the previous characterization for (Mπ)ijwe have
⟨φ(x′
i),Pπψ(xj, aj)⟩=⟨P∗
πφ(x′
i), ψ(xj, aj)⟩ (A.19)
=Z
Aψ(x′
i, a)π(da|x′
i), ψ(xj, aj)
(A.20)
=Z
A⟨ψ(x′
i, a), ψ(xj, aj)⟩π(da|x′
i), (A.21)
as required.
A.5 Separable Spaces
We show here the sufficient condition for (G,F)-compatibility of a policy in the case of the separable
spaces introduced in Section 4.
Proposition 4 (Separable Spaces) .Letϕ:X → H be a feature map into a Hilbert space H.
LetF=H ⊗ H andG=R|A|⊗ H with feature maps respectively φ(x) =ϕ(x)⊗ϕ(x)and
ψ(x, a) =ϕ(x)⊗ea, with ea∈R|A|the one-hot encoding of action a∈ A. Letπ:X → ∆(A)be
a policy such that π(a|·) =⟨pa, ϕ(·)⟩withpa∈ H for any a∈ A. Then, πis(G,F)-compatible.
Proof. The proposition follows from observing that for any v∈R|A|andh∈ H , applying Pπ
according to (6) to the function g(x, a) =⟨h, ϕ(x)⟩⟨v, De a⟩yields
(Pπg)(x) =X
a∈Ag(x, a)π(a|x) =⟨h, ϕ(x)⟩X
a∈A⟨v, De a⟩π(a|x) (A.22)
=⟨h, ϕ(x)⟩X
a∈A⟨v, De a⟩⟨pa, ϕ(x)⟩ (A.23)
=*
h⊗X
a∈A⟨v, De a⟩pa, ϕ(x)⊗ϕ(x)+
(A.24)
Hence (Pπg)(x) =⟨f, φ(x)⟩withf=h⊗h′∈ H ⊗ H =Fandh′=P
a∈A⟨v, De a⟩pa∈ H.
Therefore, the restriction of PπtoGtakes value in Fas desired.
B Policy Mirror Descent
In this section we briefly review the tools needed to formulate the PMD method and discuss the
convergence rates for inexact PMD. Most of the discussion follows the presentation in [ 12] formulated
within the notation used in this work.
19LetD: ∆(A)×rint∆(A)→Ra Bregman divergence [ 35, Definition 9.2] over the probability
simplex, where rint∆(A)denotes the relative interior of ∆(A). In the following, for any t∈Nwe
will denote by πtthe policy produced at iteration tby a PMD algorithm according to the update (2)
(with either the exact action-value function or an estimator, as discussed in Section 3) with divergence
Dand step-size η >0. We denote Pt=Pπtthe associated operator. We recall here the PMD update
from (2), highlighting the dependency on the policy operator Ptvia the action-value function q(Pt).
πt+1(·|x)∈argmin
p∈∆(A)(
−ηX
a∈Aq(Pt)(x, a)pa+D(p;πt(·|x)))
for all x∈ X. (B.1)
While this point-wise characterization is sufficient to define the updated policy πt+1:X → ∆(A)
from the previous πtand its action-value function q(Pt), we need to guarantee that πt+1is measurable.
If that were not the case, we would not be able to guarantee the existence of a Pt+1associated with it,
possibly affecting the well-definiteness of iteratively applying the mirror descent update (B.1). The
following result addresses this issue.
Lemma B.1 (Measurability of the Mirror Descent updates) .LetD: ∆(A)×rint ∆( A)→Rbe a
Bregman divergence continuous in its first argument. There exists a measurable policy πt+1:X →
∆(A)that satisfies (B.1) for all x∈ X.
Proof. The proof follows from the Measurable Maximum Theorem [ 29, Theorem 18.19]. Let us
denote ft:X ×∆(A)→Rthe function
ft(x, p) :=−ηX
a∈Aq(Pt)(x, a)pa+D(p;πt(x)). (B.2)
Let also κ:X↠∆(A)be the constant correspondance x7→∆(A)for all x∈ X.κclearly has
nonempty compact values, and it is also weakly measurable since for any open set G⊂∆(A), its
lower inverse κℓ(G) :={x∈ X:κ(x)∩G̸=∅}=Xbelongs to the Borel sigma-algebra of X.
Finally, since q(Pt)∈Bb(Ω), and by assumption Dis continuous in its first argument, then we have
thatftis a Carathéodory function. Then, by [ 29, Theorem 18.19] we have that the correspondence of
minimizers µ:X↠∆(A)defined as
µ(x) :=
p∗∈κ(x) :ft(x, p∗) = min
p∈κ(x)ft(x, p)
admits a measurable selector, which we denote πt+1:X → ∆(A), proving the statement of the
Lemma.
The previous Lemma is the key technical step enabling us to extend the convergence rates of Mirror
Descent proved in [ 12] to non-tabular settings. We now state and prove fa ew Lemmas instrumental
to prove Theorem 7.
Lemma B.2 (Three-points lemma) .Letπt+1:X → ∆(A)a measurable minimizer of (B.2) and
Pt+1its associated operator. For every measurable policy π:X → ∆(A)(alongside its associated
operator P) it holds
η[(Pt+1−P)q(Pt)] (x)≥D(π(x);πt+1(x))−D(π(x);πt(x)) (B.3)
Proof. The function ft(x, p)in (B.2) is convex and differentiable in pas it is a sum of a linear
function and a (strictly convex) Bregman divergence. By the first-order optimality condition [ 35,
Corollay 3.68], a minimizer p∗offt(x,·)satisfies, for all p∈∆(A)
⟨∇ft(x, p∗), π(x)−p∗⟩ ≥0. (B.4)
Since πt+1(x)is a minimizer of ft(x,·)by assumption, letting p∗=πt+1(x), the first order
optimality condition (B.4) becomes
η[(Pt+1−P)q(Pt)] (x)− ⟨∇ ψ(πt(x))− ∇ψ(πt+1(x)), π(x)−πt+1(x)⟩ ≥0 = ⇒
η[(Pt+1−P)q(Pt)] (x)≥D(π(x);πt+1(x))−D(π(x);πt(x)) +D(πt+1(x);πt(x)) = ⇒
η[(Pt+1−P)q(Pt)] (x)≥D(π(x);πt+1(x))−D(π(x);πt(x))
20Where in the first line we used the definition of Bregman divergence [ 35, Definition 9.2] D(p;q) :=
ψ(p)−ψ(q)− ⟨∇ ψ(q), p−q⟩for a suitable Legendre function ψ: ∆(A)→R, the first implication
follows from the three-points property of Bregman divergences [ 35, Lemma 9.11], and the last
implication from the positivity of D(πt+1(x);πt(x)).
Corollary B.3 (MD Iterations are monotonically increasing) .This Corollary is essentially a restate-
ment of [ 12, Lemma 7]. Let (Pt)t∈Nbe the sequence of policy operators associated to the measurable
minimizers of (B.1) for all t∈N. For all x∈ X it holds
[(Pt+1−Pt)q(Pt)] (x)≥0 (B.5)
and
J(Pt+1)−J(Pt)≥0 (B.6)
i.e. the objective function is always increased by a mirror descent iteration. Further, if ˜q(Pt)∈Bb(Ω)
is such that ∥q(Pt)−˜q(Pt)∥∞≤εt, then (B.5) holds inexactly on ˜q(Pt)as
[(Pt+1−Pt)˜q(Pt)] (x)≥ −2εt. (B.7)
Proof. By setting π(x) =πt(x)in (B.3), and recalling that D(p;q)≥0with equality if and only if
p=q, it follows that
η[(Pt+1−Pt)q(Pt)] (x)≥D(πt(x);πt+1(x))≥0,
giving (B.5). Integrating (B.5) over (Id−γPt+1T)−∗νand using the Performance Differ-
ence Lemma A.3 one gets (B.6). Finally, we get (B.7) from
[(Pt+1−Pt)˜q(Pt)] (x) = [( Pt+1−Pt)q(Pt)] (x) + [( Pt+1−Pt)(˜q(Pt)−q(Pt))] (x)
≥[(Pt+1−Pt)(˜q(Pt)−q(Pt))] (x)
≥ −(Pt+1−Pt)(˜q(Pt)−q(Pt))
∞
≥ −Pt+1−Pt˜q(Pt)−q(Pt)
∞
≥ −2εt.(B.8)
Where the first inequality follows from (B.5), and the latter from the fact that policy operators are
Markov operators and have norm 1, andPt+1−Pt≤Pt+1+Pt= 2.
B.1 Convergence rates of PMD
We are finally ready to prove the convergence rates for the Policy Mirror Descent algorithm (B.1).
The proof technique is loosely based on [ 12, Theorem 8, Lemma 12], and extends them to the case of
general state spaces through the key Lemma B.1 and using a fully operatorial formalism.
Theorem 7 (Convergenge of Inexact PMD) .Let(πt)t∈Nbe a sequence of policies generated by
Algorithm 1that are all (G,F)-compatible. If the action-value functions ˆqπtare estimated with an
error∥qπt−ˆqπt∥∞≤εt, the iterates of Algorithm 1 converge to the optimal policy as
J(π∗)−J(πT)≤εT+O 
1
T+1
TT−1X
t=0εt!
, (16)
where π∗:X → ∆(A)is a measurable maximizer of (8).
Proof. As usual, in this proof we denote the estimated and exact action-value functions as qn(Pt) :=
ˆqπt= (Id−γTnPt)−1rnandq(Pt) := qπt= (Id−γTPt)−1r, respectively. From hypothesis,
Algorithm 1 is well-defined since all policies it generates are (G,F)-compatible. The resulting
sequence of policies (πt)t∈Nare generated via the update rule (14) on the inexact action-value
functions qn(Pt), as defined in (12). As the update rule (14) is a (measurable) minimizer of (B.1)
when Dequals the Kullback-Leibler divergence, the three-points Lemma B.2 with π(x) =π∗(x)
yields
[(P∗−Pt+1)qn(Pt)] (x)≤1
ηD(π∗(x);πt(x))−1
ηD(π∗(x);πt+1(x)).
21Adding and subtracting the term [(P∗−Pt+1)q(Pt)] (x), and bounding the remaining difference as
[(P∗−Pt+1)(q(Pt)−qn(Pt))] (x)≤2εt– see the derivation of (B.8) – one gets
[(P∗−Pt+1)q(Pt)] (x)≤2εt+1
ηD(π∗(x);πt(x))−1
ηD(π∗(x);πt+1(x)).
Adding and subtracting Ptq(Pt)on the left side gives
[(P∗−Pt)q(Pt)] (x)≤[(Pt+1−Pt)q(Pt)] (x)+2εt+1
ηD(π∗(x);πt(x))−1
ηD(π∗(x);πt+1(x)),
and integrating with respect to the positive measure (Id−γP∗T)−∗νand using the performance
difference Lemma A.3 on the left hand side one has
J(P∗)−J(Pt)≤1
1−γ⟨(Pt+1−Pt)q(Pt) + 2εt, dν(P∗)⟩
1
η(1−γ)⟨D(π∗;πt)−D(π∗;πt+1), dν(P∗)⟩,(B.9)
where we used (A.8) on the right-hand-side terms. Since (Pt+1−Pt)q(Pt) + 2 εt≥0because
of (B.7), we can use fact (1) from Lemma A.5 with (Id−γPt+1T)−1and the performance differ-
ence Lemma A.3 to get
⟨(Pt+1−Pt)q(Pt) + 2εt, dν(P∗)⟩ ≤
(Id−γPt+1T)−1[(Pt+1−Pt)q(Pt) + 2εt], dν(P∗)
=⟨Pt+1q(Pt+1), dν(P∗)⟩ − ⟨Ptq(Pt), dν(P∗)⟩+2εt
1−γ.
Substituting this bound in (B.9) and summing from t= 0. . . T−1one gets to
T−1X
t=0J(P∗)−J(Pt)≤1
1−γ(⟨PTq(PT), dν(P∗)⟩ − ⟨P0q(P0), dν(P∗)⟩) +2
(1−γ)2T−1X
t=0εt
1
η(1−γ)⟨D(π∗;π0)−D(π∗;πT), dν(P∗)⟩.
Using facts (3) and (4) from Lemma A.5 we have that the terms ⟨Pq(P), dν(P∗)⟩on the right hand
side can be bounded as
⟨Pq(P), dν(P∗)⟩=
P(Id−γTP)−1r, dν(P∗)
=
(Id−γPT)−1Pr, dν(P∗)
=
r,P∗(Id−γPT)−∗dν(P∗)
(Duality) ≤ ∥r∥∞P∗(Id−γPT)−∗dν(P∗)
TV
(Lemma A.5 and ∥ν∥TV= 1)≤∥r∥∞
1−γ,
while−⟨D(π∗;πT), dν(P∗)⟩can be dropped due to the positivity of Bregman divergences yielding
T−1X
t=0J(P∗)−J(Pt)≤2
(1−γ)2 
r
∞+T−1X
t=0εt!
+1
η(1−γ)⟨D(π∗;π0), dν(P∗)⟩.(B.10)
Now notice that for all t < T it holds
J(Pt) =⟨Ptq(Pt), ν⟩
=⟨Ptqn(Pt), ν⟩+⟨Pt(q(Pt)−qn(Pt)), ν⟩
(Equation (B.6) )≤ ⟨PTqn(PT), ν⟩+⟨Pt(q(Pt)−qn(Pt)), ν⟩
=⟨PTqn(PT), ν⟩+⟨Pt(q(Pt)−qn(Pt)), ν⟩+⟨PT(qn(PT)−q(PT)), ν⟩
≤J(PT) +εt+εT,
so that
J(P∗)−J(PT)≤εT+1
TT−1X
t=0J(P∗)−J(Pt) +εt.
Combining this with (B.10) we obtain
J(P∗)−J(PT)≤εT+1
T"
1 +2
(1−γ)2T−1X
t=0εt+2r
∞
(1−γ)2+1
η(1−γ)⟨D(π∗;π0), dν(P∗)⟩#
,
leading to the desired bound.
22C POWR Convergence Rates
In this section, we prove the convergence of POWR. To do so, we need to first show that under
the choice of spaces FandGproposed in this work, the resulting PMD iterations are well defined.
Then, we need to bound the approximation error of the estimates for the action-value functions of the
iterates produced by the inexact PDM algorithm, which appear in the rates of Theorem 7.
C.1 POWR is Well-defined
In order to guarantee that the iterations of POWR generate policies πtfor which we can compute
an estimator according to the formula in Proposition 3, we need to guarantee that all such policies
are(G,F)-compatible. In particular, we restrict to the case of the separable spaces introduced in
Proposition 4, for which it turns out that it is sufficient to show that all policies belong to the space
Hcharacterizing F=H ⊗ H andG=R|A|⊗ H. The following results provide a candidate for
choosing such a space.
Theorem 5. LetX ⊂Rdbe a compact set and let H=W2,s(X)be the Sobolev space of smoothness
s >0(see e.g. [ 37]). Let πt(a|·)andˆqπt(·, a)belong to Hfor any a∈ A andπt(a|x)>0for any
x∈ X. Then the policy πt+1solution to the PMD update in (14) belongs to H.
Proof. We recall that Sobolev spaces [ 37] over a compact subset XofRDare closed with respect
to the operations of sum, multiplication, exponentiation or inversion (if the function is supported
on the entire domain X), namely for any two f, f′∈ H,f+f′, ff′, ef∈ H and, if f(x)>0for
allx∈ X,1/f∈ H. This follows by applying the chain rule and the boundedness of derivatives
over the compact X(see for instance [ 45, Lemma E.2.2]). The proof follows by observing that the
one-step update πt+1in (14) is expressed precisely in terms of these operations and the hypothesis
thatπt(a|·)andˆqπt(·, a)belong to Hfor any a∈ A.
Combining the choice of space Haccording to the above result and combining with the PMD
iterations of Algorithm 1 we have the following corollary.
Corollary 6. With the hypothesis of Proposition 4 let H=W2,s(X)withs > d/ 2. Let Tn∈
HS(F,G)andrn∈ G characterized as in Proposition 3. Let π0(a|·)∝eηq0(·,a)forq0such that
q0(·, a)∈ H anya∈ A. Then, for any t∈Nthe PMD iterates πtgenerated by Algorithm 1 are such
thatπt(a|·)∈ H and hence are (G,F)-compatible.
Proof. We proceed by induction. Since ¯q(·, a)∈ H we can apply the same reasoning in Theorem 5
to guarantee that π0(a|·)∈ H for any a∈ A. Moreover, π0(a|cot)>0for any a∈ A since it is
the (normalized) exponential of a function. Hence π0is(G,F)-compatible. Therefore, ˆq0obtained
according to Proposition 3 is well defined and belongs to G, implying ˆq0(·, a)∈ H for any a∈ A.
Now, assume by the inductive hypothesis that the policy πt(a|·)generated by POWR at time tand
the corresponding estimator ˆqπt(·, a)of the action value function belong to Hand that πt(a|x)>0
for any (x, a)∈Ω. Then, by Theorem 5 we have that also πt+1the solution to the PMD update in
(14) belongs to H(and is therefore (G,F)-compatible). Additionally, since πt+1can be expressed
as the softmax of a (finite) sum of functions in H, we have also πt+1(a|x)>0for al (x, a)∈Ω,
proving the inductive hypothesis and concluding the proof.
The above corollary guarantees us that if we are able to learn our estimates for the action-value
function in Ha suitably regular Sobolev space, then POWR is well-defined. This is a necessary
condition to then being able to study its theoretical behavior in our main result.
C.2 Controlling the Action-value Estimation Error
We now show how to control the estimation error for the action-value function. We start by considering
the following application of the (generalized) Simulation lemma in Corollary A.7.
Lemma 8 (Implications of the Simulation Lemma) .LetTnandrnthe empirical estimators of
the transfer operator Tand reward function ras defined in Proposition 3, respectively. If T
23satisfies Assumption 1, r∈ G, and γTn< γ′<1, then, for every (G,F)-compatible policy π
ˆqπ−qπ
∞≤1
1−γ′"
const ψrn−r
G+γr
∞
1−γT|F−Tn
HS#
.
Proof. Recall that in the notation of these appendices, the action value of a policy and its estimator
via the world model CME framework are denoted qπ=q(P)andˆqπ=qn(P)respectively. We can
apply Corollary A.7 to obtain
qn(P)−q(P) = (Id−γTnP)−1(rn−r) +γ(Id−γTnP)−1(T−Tn)v(P).
Then, by Lemma A.5, point 5, we have
qn(P)−q(P)
∞≤1
1−γ′hrn−r
∞+γ(T−Tn)v(P)
∞i
,
where v(P) := P(Id−γTP)−1ris the value function of the MDP, and we used that γ∥Tn∥<
γ′. Because of Assumption 1, r∈ G, and Pbeing (G,F)-compatible, it holds that v(P)∈ F ,
while Proposition 3 implies rn∈ G, and(T−Tn)v(P)∈ Gas well. Therefore, using the reproducing
property
rn−r
∞= sup
(x,a)∈Ω|⟨ψ(x, a), rn−r⟩G| ≤ ∥rn−r∥Gsup
(x,a)∈Ω∥ψ(x, a)∥G=Cψ∥rn−r∥G
where we assumed a bounded kernel ⟨ψ(x, a), ψ(x, a)⟩ ≤Cψfor all (x, a)∈Ω. Similarly, for the
term depending on Tn−Twe have
(T−Tn)v(P)
∞= sup
(x,a)∈Ω|[(T|F−Tn)v(P)](x, a)|
= sup
(x,a)∈Ω|⟨ψ(x, a),(T|F−Tn)v(P)⟩G|
= sup
(x,a)∈ΩTr[(v(P)⊗ψ(x, a))(T|F−Tn)]
≤T|F−Tn
HSsup
(x,a)∈Ω|v(P)(x, a)|
≤∥r∥∞
1−γT|F−Tn
HS.
Combining the previous two bounds, we get to
qn(P)−q(P)
∞≤1
1−γ′"
Cψrn−r
G+γr
∞
1−γT|F−Tn
HS#
,
as desired.
According to the result above, we can control the approximation error for the action value function in
terms of the approximation errors ∥rn−r∥GandT|F−Tn
HS. This can be done by leveraging
state-of-the-art statistical learning rates for the ridge regression and CME estimators from [ 39,21,46].
The following lemma connects Assumption 2 with the notation used in [ 39] which enables us to use
the required result.
Lemma C.1 (Relation between (A.8) and [39]’s definition) .The following two facts are equivalent
1.g∈ Gsatisfies the strong source condition Assumption 2 with parameter βon the probability
distribution ρ.
2.g∈[G]1+2β
ρ as in the notation of [39].
Proof. Using the same notations as in [39], we have
g∈[G]1+2β
ρ⇐⇒ g=X
i∈Naiµ1
2+β
ieiand(ai)i∈N
ℓ2<∞.
24For1 =⇒2we have that that for g∈[G]1+2β
ρ
C−β
ρg=X
i∈Naiµ1
2
iei (C.1)
whose G-norm isC−β
ρg
G=(ai)i∈N
ℓ2<∞.
For2 =⇒1, letg=P
i∈Nbiµ1
2
ieiwith(bi)i∈N
ℓ2<∞(since). Now,C−β
ρg
G<∞is
equivalent to(biµ−β
i)i∈N
ℓ2<∞. By letting bi=µβ
iaiwe have that(ai)i∈N
ℓ2<∞and that
g=X
i∈Naiµ1
2+β
iei,
that is g∈[G]1+2β
ρ .
With the connection between [ 39] and Assumption 2 in place we can characterize the bound on the
approximation error for the world model-based estimation of the action-value function.
Proposition C.2. LetTnandrnthe empirical estimators of the transfer operator Tand reward
function ras defined in Proposition 3, respectively. When Pis a(G,F)-compatible policy as
in Definition 1 and the strong source condition Assumption 2 is attained with parameter β, it holdsqn(P)−q(P)
∞≤O(δ2n−α), (C.2)
with rates α∈
β
2+2β,β
1+2β
and probability not less than 1−4e−δ.
Proof. We use Lemma C.1 to apply Theorem 3.1 (ii) from [ 39] to show that under Assumption 2
with parameter βit holds, with probability not less than 1−4e−δ,rn−r
G≤δ2crn−αr. (C.3)
The rate αr∈
β
2+2β,β
1+2β
is determined by the properties of the inclusion G,→Bb(Ω), and
the constant cr>0is independent of nandδ. Similarly, point (2.) of [ 21, Theorem 2] shows that
under Assumption 2 T|F−Tn
HS(F,G)≤δ2cTn−αT(C.4)
again with probability not less than 1−4e−δ, rates αT∈
β
2+2β,β
1+2β
and with cT>0independent
ofnandδ. Combining every bound and denoting α:= min( αr, αT), we conclude
qn(P)−q(P)
∞≤δ2
1−γ′"
Cψcr+γcTr
∞
1−γ#
n−α=O(δ2n−α). (C.5)
as required.
C.3 Convergence Rates for POWR
With a bound on the estimation error of the action-value function by Algorithm 1, we are finally ready
to state the complexity bounds for POWR.
Theorem 9. Let(πt)t∈Nbe a sequence of policies generated by Algorithm 1 in the same setting
of Corollary 6. If the action-value functions ˆqπtare estimated from a dataset (xi, ai;x′
i)n
i=1with
(xi, ai)∼ρ∈ P(Ω)such that Assumption 2 holds with parameter β, the iterates of Algorithm 1
converge to the optimal policy as
J(π∗)−J(πT)≤O1
T+δ2n−α
with probability not less than 1−4e−δ. Here, α∈
β
2+2β,β
1+2β
andπ∗:X → ∆(A)is a
measurable maximizer of (8).
Proof. Since the setting of Corollary 6 implies that Ptare(G,F)-compatible for all t, and Assump-
tion 2 is holding, then q(Pt)andqn(Pt)belong to Gfor all (Pt)t∈N. This assures that we can use the
statistical learning bounds Proposition C.2 into Theorem 7, yielding the final bound.
25D Experimental details
D.1 Additional Results
In Fig. 2 we show the average timestep at which a reward threshold is met during the training phase.
The testing environments are the same as introduced previously, with reward thresholds being the
standard ones given in [ 25], except for the Taxi-v3 environment, where it is marginally lower.
Interestingly, in this environment, only DQN and our algorithm are capable of achieving the original
threshold within 1.5×106timesteps during the training. On the other hand, the new lower threshold
is also reached by the PPO algorithm.
Our approach attain the desired reward quicker than the competing algorithms. Furthermore, the
timestep at which POWRreaches the threshold exhibits a lower variance compared to other techniques.
This implies that our approach requires a stable amount of timesteps to learn how to solve a specific
environment.
(a)FrozenLake-v1
 (b)Taxi-v3
 (c)MountainCar-v0
Figure 2: Mean timestep at which various algorithms attain a specified reward threshold during
their training. The reward targets are set at 0.8forFrozenLake-v1 ,6forTaxi-v3 , and−110for
MountainCar-v0 . The absence of a box indicates that the corresponding algorithm was unable to
meet the reward threshold within the training process.
D.2 Other methods
We compare the performance of our algorithm with several baselines. In particular, we considered
A2C [ 40], DQN [ 4], TRPO [ 7] and PPO [ 6], which we implemented using the stable baselines library
[47]. We used the standard hyperparameters in [47].
26NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: we provide theoretical and experimental proofs of our claims.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: see the conclusion section.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
27Justification: We give all the proofs in the Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: In the paper, we provided the algorithm description and analysis, including the
hyperparameters of the baselines and our proposed algorithm.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
28Answer: [Yes]
Justification: We provided the link to the code and all the hyperparameters we tested.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provided a description of the open-source environments, in which we
tested our POWR and the baselines.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We showed seven independent runs for each experiment we reported to prove
the effectiveness of our method. Moreover, in the plot, we can observe the bands based on
the minimum and maximum values reached by the different runs.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
29• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: We provided the code.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We respected the ethics guidelines for the development of this work.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: We believe that this work doesn’t have any societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
30•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This work doesn’t have such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We implemented our approach from scratch. We mentioned the baselines and
the environments for testing involved in this work, that are open-source
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
31•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We provided the code of our algorithm.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This work does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA] .
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
32